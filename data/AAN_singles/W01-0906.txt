Verification and Validation of Language Processing Systems: Is ItEvaluation?Valerie B. BarrDepartment of Computer ScienceHofstra UniversityHempstead, NY 11549-1030  USAvbarr@hofstra.eduJudith L. KlavansCenter for Research on Information AccessColumbia University535 West 114th Street, MC 1103New York, NY  10027 USAklavans@cs.columbia.eduAbstractIf Natural Language Processing(NLP) systems are viewed asintelligent systems then we should beable to make use of verification andvalidation (V&V) approaches andmethods that have been developed inthe intelligent systems community.This paper addresses languageengineering infrastructure issues byconsidering whether standard V&Vmethods are fundamentally differentthan the evaluation practicescommonly used for NLP systems, andproposes practical approaches forapplying V&V in the context oflanguage processing systems.
Weargue that evaluation, as it isperformed in the NL community, canbe improved by supplementing it withmethods from the V&V community.1 NLP Systems as IntelligentSystemsLanguage engineering research is carried out inareas such as speech recognition, naturallanguage understanding, natural languagegeneration, speech synthesis, informationretrieval, information extraction, and inference(Jurafsky  & Martin, 2000).
In practice thismeans building systems which model humanactivities in various language processing tasks.Therefore, we can quite clearly view languageprocessing systems as forms of intelligentsystems.
This view allows us to draw on workthat has been done within the intelligent systemscommunity within computer science onverification and validation of systems.
It alsoallows us to consider V&V in the context of NLsystems, and evaluation, as carried out on NLsystems, in the context of software engineeringmethodologies.
This research extends the firstauthor?s earlier work on software testingmethodologies in the context of expert systems(Barr, 1995; Barr, 1999).2 Verification and Validation ofIntelligent SystemsThe area of verification and validation ofsoftware systems has suffered from amultiplicity of definitions (Barr, 2001; Gonzalezand Barr, 2000).
However, the most commonlyused definitions are :?
Verification ?
ensuring that softwarecorrectly implements specific functions,that it satisfies its specification.?
Validation ?
determining that the systemsatisfies customer requirements.These definitions have been re-examined inorder to account for the differences between?conventional?
software and intelligent systems.An intelligent system is built based on aninterpretation of the problem domain, with theexpectation that the system will behave in afashion that is equivalent to the behavior of anexpert in the domain.
It follows that humanperformance is often the benchmark we use toevaluate an intelligent system.The usual definitions of verification andvalidation can be applied to intelligent systemswith slight modifications to take into accountthe presence of a knowledge base and thenecessity of comparing system performance tothat of humans in the problem domain.
The coreissue in validation and verification of anintelligent system boils down to one simpleobjective: ensuring that the resulting system willprovide an answer, solution or behaviorequivalent to what an expert in the field wouldsay if given the same inputs.Therefore the definitions of verification andvalidation have been refined (Gonzalez andBarr, 2000) in order to account for the differingaspects of intelligent systems :?
Verification ?
the process of ensuring 1)that the intelligent system conforms tospecifications, and 2) its knowledgebase is consistent and complete withinitself.?
Validation ?
the process of ensuring thatthe output of the intelligent system isequivalent to those of human expertswhen given the same inputs.Consistency of the knowledge base means thatthere are no redundancies, conflicts or cycles.Completeness means that all facts are used, thereare no unreachable conclusions, missing rules(in the rule-based expert systems context), thereare no dangling conditions.
These definitions ofverification and validation retain the standarddefinitions used in software engineering, whilealso requiring that the knowledge base be free ofinternal errors, and letting human performancebe the standard for ?customer requirements?.In the realm of language processing, the?expert?
can often be any user of language in thecontext for which the system has beendeveloped.3 Evaluation of NLP SystemsThe previous section presented definitions forV&V.
In this section the general paradigms forevaluation of NLP system is presented.3.1 Taxonomies of Evaluation withinNLPOur review of  the evaluation literature indicatesthat NLP systems have largely been evaluatedusing a black-box, functional, approach.Evaluation is often subdivided into formativeevaluation and summative evaluation (SparckJones & Galliers, 1996).
The former determinesif the system meets the objectives that were setfor it.
It can be diagnostic, indicating areas inwhich the system does not perform well, orpredictive, indicating the performance that canbe expected in actual use.
Summativeevaluation is a comparison of different systemsor approaches for solving a single problem.In a somewhat different taxonomy(Hirschman and Thompson, 1998), evaluation issubdivided into?
Adequacy evaluation ?
determinationof the fitness of a system for its intendedpurpose.
Will it do what is required bythe user, how well, and at what cost??
Diagnostic evaluation ?
exposure ofsystem failures and production of asystem performance profile.?
Performance evaluation ?measurement of system performance inone or more specific areas.
Can be usedto compare alternative implementationsor successive generations of a system.We can see that performance evaluationoverlaps with summative evaluation, whileadequacy evaluation corresponds to formativeevaluation.While the evaluation process must considerthe results generated by an NLP system, it alsoconsiders the usability of the system(Hirschman and Thompson, 1998; White andTaylor, 1998), its features, and how easily it canbe enhanced.
For example, a translation systemmay appear to work well in a testbed situation,but may not function well when embedded intoa larger system.
Or it may perform acceptablywhen its output is intended for a generalaudience, but not when an expert uses theoutput.Sparck Jones and Galliers (1996) discusshow the evaluation process should take intoaccount whether the NLP task is part of a largersystem with both linguistic and non-linguisticcomponents, and determine the impact onoverall performance of each of the subparts.
Wecall this component performance evaluation.Additional complexity arises in the evaluation ofcomponent performance within multi-facetedsystems, such as embodied conversationalagents, where  assessment of how well thesystem works is based on more than strictlanguage aspects, considering also more subtlefeatures such as gesture and tone (Cassell et al,2000).
Furthermore, whether or not a systemresponse is considered to be correct oracceptable may depend on who is judging it.In general, NLP systems for various kinds oftasks require differing views of the evaluationprocess, with different criteria, measures, andmethods.
For example, consider the ways inwhich evaluation of machine translation (MT)systems is carried out.
Notice that not allaspects of  validation and verification, asdiscussed in section 2, are represented.Evaluation of machine translation (MT) systemshas to consider the pre-processing of input andthe post-editing of output.
Black-box evaluationof MT systems can measure the percentage ofwords that are incorrect in the entire output text(based on how post-editing changes raw outputtext to fix it).
But whether or not a word isconsidered incorrect in the output may dependon the task of the system.
So functionalevaluation of an MT system may have to beaugmented by a subjective determination ofwhether the output text carries the sameinformation as the input text, and whether theoutput is intelligible (Sparck Jones and Galliers1996).
Another example is the case of speechinterfaces and spoken dialogue systems.
theevaluation process typically focuses on theaccuracy, coverage, and speed of the system,with increasing attention paid to usersatisfaction (James et al 2000, Walker andHirschman 1999).
Notice that in just these twoexamples, various kinds of evaluation are calledinto play.
We will argue in section 4 that V&Vtechniques extend these evaluation methods,providing system coverage analysis that assessescompleteness and consistency.3.2 Factors which impact evaluationEvaluation of NLP systems must also takeinto account the kinds of inputs we expect asystem to work on after its testing phase iscomplete.
Wacholder (1997)  demonstrates theextent to which the linguistic complexity ofdocuments is one of the factors responsible forthe weakness of applications that process naturallanguage texts.
The ability to categorize testdata by complexity will help distinguishbetween a failure of an NLP system that resultsfrom extraordinary document complexity(beyond that of the data on which the systemwas tested) and a failure that results frominadequate testing of the NLP tool.
The formershould be predictable, while the latter shouldrarely happen if a system has been adequatelytested.
It is certainly possible that a tool may bevery well tested, functionally and with regard toconsistency and completeness, on text of certaindegree of complexity, but still fail on text that ismore complex or from a different domain.3.3 Comparative evaluation effortsThere are NLP evaluation methods that,although in a different problem domain, closelymirror the approach typically used with expertsystems, comparing machine results to humanresults.
For example, the TAUM-AVIATIONmachine translation system was evaluated in1980, in part by comparing the raw translationproduced by the system to several humantranslations.
Then revised and post-editedtranslations (human and machine) were ratedand ranked by a number of potential users(Sparck Jones and Galliers, 1996).
This isessentially the same testing method that wasused for the MYCIN expert system (Yu, 1985)and many additional systems.
However, withinthe expert systems area several methods havebeen developed in subsequent years that addressthe weaknesses of strictly functional evaluationapproaches (e.g.
Barr, 1999; Grossner, 1993).There are also well-known evaluationefforts such as EAGLES (Sparck Jones andGalliers, 1996) and the Paradise evaluationframework (Walker et al, 1997).
In addition,many researchers have participated in thecomparative evaluation efforts characterized bythe Text Retrieval Conferences (TREC)1, theMessage Understanding Conferences (MUC)2and Document Understanding Conferences(DUC)3, the Cross-Language Evaluation Forum(CLEF)4, and the summarization evaluationeffort (SUMMAC) (for a very comprehensivelist of evaluation related links, seehttp://www.limsi.fr/TLP/CLASS/prj.eval.links.html).Evaluation of NLP systems is aided by thefact that there is considerable test data available.There are substantial repositories of data, suchas the TREC collection that includes, amongother data, Associated Press wire feeds;Department of Energy documents; FederalRegister documents; Wall Street Journal fulltexts; and sources from Ziff-Davis Publishing.1http://trec.nist.gov/2http://www.itl.nist.gov/iaui/894.02/related_projects/tipster/muc.htm3http://www-nlpir.nist.gov/projects/duc/index.html4http://www.iei.i.cnr.it/DELOS/CLEF/It is important to note that the DARPA/ARPAsponsored conferences (MUC, TIPSTER, andTREC, for example), while making considerabledata available, promote functional testing bystressing black-box performance of a system.The metrics used in the MUC program areoriented toward functional testing, focusing onthe number of spots in a template that arecorrectly filled in by a particular MUC system,along with various error-based measures.
Fordatabase query systems the emphasis has beenon functional testing, supplemented withevaluations of the system by users, given thedesire to create marketable systems.An issue that arises in comparativeevaluation efforts, particularly because there isso much test data available, is what it means tocompare the behavior of two systems designedto carry out the same task, based on theirperformance on a common set of test data.Allen (1995) argues that evaluation results forindividual systems, and any comparison ofresults across systems, should not be givenmuch credence until they reach ?somereasonably high level of performance.
?Certainly the MUC and TREC programs arebased on comparing performance of multiplesystems on a common task.
One of the purposesof our research is to show that withoutassessment of consistency and completeness, thequality of the functional testing alone may notbe sufficient for predicting reliability of an NLPsystem and V&V methods will improve thesituation.3.4 Additional comments on functionaltestingWe have referred to functional testing inprior paragraphs in the context of variousaspects of evaluation.
Recent literature (Declerket al, 1998; Rubio et al, 1998; Klavans et al,1998; Jing et al, 1998) shows that functionaltesting is still very much in use for evaluation ofNLP systems and larger systems of which NLPcomponents are a part.
Where other evaluationmechanisms are in use, they are still based onthe behavior of the system under test, not basedon an analysis of how test case executionexercises the system.
For example, White andTaylor (1998) propose evaluating machinetranslation (MT) systems based on what kind oftext handling tasks could be supported by theoutput of the MT system.
They examine the texthandling tasks (publishing, gisting, extraction,triage, detection, filtering) to determine howgood a translation has to be in order for it to beuseful for each task.
They then rank the texthandling tasks in such a way that if an MTsystem?s output can facilitate a task, it can alsofacilitate tasks lower on the scale, but is unlikelyto facilitate tasks higher on the scale.
This kindof evaluation is functional in nature, though theassessment of the quality of the MT system?soutput is based not on an examination of theoutput but on a subsequent use of the output.The notion of functional glass-box testingdoes not assess coverage of the system itself, butis essentially an assessment of how well asystem carries out its task.
It relies on theprogrammer or tester?s idea of how a componentshould carry out its task for a particular testinput (Sparck Jones and Galliers 1996).
At thesame time, black-box evaluation is a veryimportant and powerful testing approach,particularly because it works from theperspective of the user, without concern forimplementation.4 Applying V&V to NLP ?
Is itEvaluation?In the previous section we outlined manydifferent types of evaluation that are performedon NL systems.
Our claim at the beginning ofthe paper was that evaluation, as it is performedin the NL community, can be improved byadopting V&V approaches.
In this section weshow specifically what the relationship isbetween V&V, as it is typically applied insoftware development, and evaluation as it iscarreid out in the context of NLP systems.In considering whether V&V and evaluationare equivalent, we need to consider whether theevaluation process achieves the goals ofverification and validation.
That is, does theevaluation process demonstrate that?
the system is correct and conforms to itsspecification?
the knowledge inherent in the system isconsistent and complete?
the output is equivalent to that of human?experts?.It is apparent that summative, adequacy anddiagnostic evaluation are all in some wayequivalent to validation.
The evaluation stepsinvolve black-box exercise of test data throughthe system, which then allows for a comparisonof actual results to expected results.
Thisfacilitates an assessment of whether the output isequivalent to that of human experts (whoprovide the expected results).The usual evaluation processes, throughformative evaluation, also facilitate one aspectof verification, in that they allow us to determineif a system conforms to its specification.
Thatis, based on the specification for a system, adomain-based test set can be constructed forevaluation which will then demonstrate whetheror not a system meets the specification.It is the second aspect of verification,determining whether the knowledge representedwithin the system is consistent and complete,that seems not to be taken into account by theevaluation processes in NLP.
The difficulty liesin the fact that a domain based test set can nevercompletely test the actual system as built.Rather, it tests the linguistic assumptions thatmotivated construction of the system.
A domainbased test set can determine if the systembehaves correctly over the test data, but may notadequately test the full system.
In particular,any inconsistencies in the knowledgerepresented within the system, or missingknowledge, may not be identified by anevaluation process that relies on domain-basedtest data.To address this issue, we need to applyadditional testing techniques, based on coverageof the actual system, in order to achieve the fullbreadth of verification activities on a languageprocessing system.
Furthermore, we may notneed larger test sets, but we may need differenttest cases in the test set.5 Applying V&V to NLP ?
How DoWe Do It?In our research we are in the early stages ofexperiments wherein we apply existing V&Vtools to a number of NL systems for indexingand significant topics detection.
We expect theresults of these experiments will support ourclaim that V&V techniques will positivelyenhance the evaluation process.The actual software testing tools wehave chosen are based on the implementationparadigms that are used in the specific NLsystems.
For example, for a C based system forautomatic indexing (Wacholder et al, 2001), wehave selected the Panorama C/C++ package5.Various features of this tool facilitate testing ofthe system as built, based on code coveragerather than domain coverage.
This approacheffectively tests the knowledge base forconsistency and completeness, which we cannotdo as successfully with a domain based test set.Regular expressions are frequently usedto implement components of NL systems.
Weare studying a component (Evans et al, 2000) ofa significant topics identification system thatuses regular expressions.
The software testingcommunity has not yet developed tools foraddressing coverage (completeness andconsistency) testing of regular expressions.
Inthis case we will construct a tool, building ontheoretical work (Yannakakis and Lee, 1995)that has been carried out in the network protocolresearch community for testing finite-statemachines.Clearly we propose adding additionalsteps to the testing process.
However, this doesnot necessarily imply that huge amounts ofadditional test data will be necessary.
In atypical testing protocol, a developer can start theV&V phase with a domain based test set.Additional test cases are then addedincrementally as needed until the test set isadequate for coverage of the system as built, andfor assessment of the consistency andcompleteness  of the system?s knowledge.6 Open QuestionsThe question we intend to address in futureresearch is whether different natural languageapplication areas can profitably benefit fromdifferent techniques utilized in the softwaretesting/V&V world.
For example, the issuesinvolved with grammars and parsers areundoubtedly quite different from those thatcome into play in machine translation systems.With grammars and parsers it is quite temptingto test the grammar by running the parser andvice versa.
Yet the grammar and parser areessentially built concurrently, and an error inone would easily carry over as an error in theother.
Typical testing strategies make it quite5http://www.softwareautomation.comdifficult to expose these sorts of errors.
Atesting approach is necessary which will helpexpose errors or incompletenesses that exist inboth a grammar and its parser.
An effort byBr?ker (2000) applies code instrumentationtechniques to grammar analysis.
However, thekinds of errors that may occur in a translationsystem or a language generation system are of adifferent nature and will require different testingstrategies to expose.7 AcknowledgementsThis research was partially supported by theNational Science Foundation under NSFPOWRE grant #9973855.
We also thankBonnie Webber, of the University of Edinburgh,and the Columbia University Natural LanguageProcessing Group, particularly Kathy McKeownand Nina Wacholder, for their helpfuldiscussions.ReferencesAllen, James (1995).
Natural LanguageUnderstanding.
Benjamin/Cummings, RedwoodCity, CA.Barr, Valerie (1995).
TRUBAC:  A Tool forTesting Expert Systems with Rule-Base CoverageMeasures.
Proceedings of the 13th Annual PacificNorthwest Software Quality Conference, Portland,OR.Barr, Valerie  (1999).
Applications of Rule-BaseCoverage Measures to Expert System Evaluation.Journal of Knowledge Based Systems, Volume 12(1999), pp.
27-35.Barr, Valerie (2001).
A quagmire of terminology.Proceedings of Florida Artificial IntelligenceResearch Symposium 2001.Br?ker, Norbert (2000).
The use of instrumentationin grammar engineering.
Proceedings of COLING2000.Cassell, Justine, Joseph Sullivan, Scott Prevost,and Elizabeth Churchill (2000).
EmbodiedConversational Agents.
MIT Press, Cambridge, MA.Declerk, Thierry et al (1998).
Evaluation of theNLP Components of an Information ExtractionSystem for German.
Proceedings of the 1stInternational Conference on Language Resourcesand Evaluation, Granada, Spain, May 1998, pgs.293-297.Evans, David K. et al (2000).
DocumentProcessing with LinkIT,  Proceedings of RIAO 2000(Recherche d?Informations Assistee par Ordinateur),Paris.Gonzalez, Avelino and Valerie Barr (2000).Validation and verification of intelligent systems ?what are they and how are they different?
Journal ofExperimental and Theoretical Artificial Intelligence,12(4).Grossner, C. et al (1993).
Exploring the structureof rule based systems.
Proceedings, AAAI-93,Washington, D.C., pp.
704-709.Hirschman, Lynette and Henry S. Thompson(1998).
Overview of Evaluation in Speech andNatural Language Processing in Survey of the Stateof the Art in Human Language Technology, GiovanniVarile and Antonio Zampolli, eds., CambridgeUniversity Press, New York.James, Frankie et al (2000).
Accuracy, Coverage,and Speed:  What Do They Mean to Users?
Seehttp://www.riacs.edu/doc/2000/htmlo/chi_nl_workshop.htmlJing, Hongyan et al (1998).
SummarizationEvaluation Methods: Experiments and Analysis.AAAI Symposium on Intelligent Summarization,March 1998, Stanford University.Jurafsky, Daniel and James Martin (2000).
Speechand Language Processing.
Prentice-Hall, NJ.Klavans, Judith L., Kathleen McKeown, Min-YenKan, and Susan Lee (1998).
Resources forEvaluation of Summarization Techniques.Proceedings of the First International Conference onLanguage Resources and Evaluation, Granada,Spain, 1998, pgs.
899-902.Rubio, A. et.al.
(1998).
On the Comparison ofSpeech Recognition Tasks.
Proceedings of the FirstInternational Conference on Language Resourcesand Evaluation, Granada, Spain, 1998.Sparck Jones, Karen and Julia Galliers (1996).Evaluating Natural Language Processing Systems.Springer-Verlag, Berlin.Wacholder, Nina.
(1997).
POWRE:Computationally Tractable Methods for DocumentAnalysis.
CS Report, Dept.
of Computer Science,Columbia University (NSF funded project).Wacholder, Nina,  et al (2001).
AutomaticGeneration of Indexes for Digital Libraries.
IEEE-ACM Joint Conference on Digital Libraries.Walker, M., et.al.
(1997).
PARADISE:  AFramework for evaluating spoken dialogue agents.Proceedings of Association of ComputationalLinguists 35th Annual Meeting.Walker, M. and L. Hirschman (1999).
DARPACommunicator Evaluation Proposal.www.research.att.com/~walker/eval/evalplan6.rtfWhite, John S. and Kathryn B. Taylor (1998).
ATask-Oriented Evaluation Metric for MachineTranslation.
Proceedings of the First InternationalConference on Language Resources and Evaluation,Granada, Spain, May 1998, pgs.
21-25.Yannakakis, Mihalis and David Lee (1995).Testing Finite State Machines : Fault Detection .Journal of Computer and Systems Sciences, Volume50, pages 209-227.Yu, V.L.
et.al.
(1985).
An evaluation of MYCIN?sadvice.
In Rule-Based Expert Systems, BruceBuchanan and Edward Shortliffe (Eds.
), Addison-Wesley, Reading, MA.
