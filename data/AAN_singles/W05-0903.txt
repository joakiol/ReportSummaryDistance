Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translationand/or Summarization, pages 17?24, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsPreprocessing and Normalizationfor Automatic Evaluation of Machine TranslationGregor Leusch and Nicola Ueffing and David Vilar and Hermann NeyLehrstuhl fu?r Informatik VIRWTH Aachen UniversityD-52056 Aachen, Germany,{leusch,ueffing,vilar,ney}@i6.informatik.rwth-aachen.deAbstractEvaluation measures for machine trans-lation depend on several common meth-ods, such as preprocessing, tokenization,handling of sentence boundaries, and thechoice of a reference length.
In thispaper, we describe and review somenew approaches to them and comparethese to state-of-the-art methods.
Weexperimentally look into their impact onfour established evaluation measures.
Forthis purpose, we study the correlationbetween automatic and human evaluationscores on three MT evaluation corpora.These experiments confirm that the to-kenization method, the reference lengthselection scheme, and the use of sentenceboundaries we introduce will increase thecorrelation between automatic and humanevaluation scores.
We find that ignoringcase information and normalizing evalu-ator scores has a positive effect on thesentence level correlation as well.1 IntroductionMachine translation (MT), as any other natural lan-guage processing (NLP) research subject, dependson the evaluation of its results.
Unfortunately,human evaluation of MT system output is a timeconsuming and expensive task.
This is why auto-matic evaluation is preferred to human evaluation inthe research community.Over the last years, a manifold of automatic evalu-ation measures has been proposed and studied.
Thisunderlines the importance, but also the complexityof finding a suitable evaluation measure for MT.We will give a short overview of some measures insection 2 of this paper.Although most of these measures share similarideas and foundation, we observe that researcherstend to approach problems common to severalmeasures differently from each other.
A noteworthyexample here is the determination of a translationreference length.In section 3, we will have a look onto structuralsimilarities and differences among several measures,focussing on common steps.
We will show thatdecisions taken about them can be as important tothe outcome of an evaluation, as the choice of theevaluation measure itself.To this end, we will study the performanceof each error measure and setting by comparisonwith human evaluation on three different evaluationtasks in section 4.
These experiments will showthat sophisticated tokenization as well as addingsentence boundaries and a good choice for thereference lengths will improve correlation betweenautomatic and human evaluation significantly.
Casenormalization and evaluator normalization are help-ful only when evaluating on sentence level; systemlevel evaluation is not affected by these methods.After a discussion of these results in section 5, wewill conclude this paper in section 6.2 Automatic evaluation measuresThe majority of MT evaluation approaches are basedon the distance or similarity of MT candidate outputto a set of reference translations, i.e.
to sentenceswhich are known to be correct.
The lower thisdistance is, or the higher the similarity, the better the17candidate translations are considered to be, and thusthe better the MT system.2.1 Evaluation measures studiedOut of the vast amount of measures, we will focuson the following measures that are widely used inresearch and in evaluation campaigns: WER, PER,BLEU, and NIST.Let a test set consist of k = 1, .
.
.
,K candidatesentences Ek generated by an MT system.
Foreach candidate sentence Ek, we have a set of r =1, .
.
.
, Rk reference sentences E?r,k.
Let Ik denotethe length, and I?k the reference length for eachsentence Ek.
We will explain in section 3.3 how thereference length is calculated.With this, we write the total candidate length overthe corpus as I?
:=?k Ik, and the total referencelength as I??
:=?k I?k .Let nem1 ,k denote the count of the m-gram em1within the candidate sentence Ek; similarly letn?em1 ,r,k denote the same count within the referencesentence E?r,k.
The total m-gram count over thecorpus is then n?m :=?k?em1 ?Eknem1 ,k.2.1.1 WERThe word error rate is defined as the Levenshteindistance dL(Ek, E?r,k) between a candidate sentenceEk and a reference sentence E?r,k, divided by thereference length I?k for normalization.For a whole candidate corpus with multiplereferences, we define the WER to be:WER :=1I??
?kminrdL(Ek, E?r,k)Note that the WER of a single sentence can becalculated as the WER for a corpus of size K = 1.2.1.2 PERThe position independent error rate (Tillmann etal., 1997) ignores the ordering of the words withina sentence.
Independent of the word position, theminimum number of deletions, insertions, andsubstitutions to transform the candidate sentenceinto the reference sentence is calculated.
Usingthe counts ne,r, n?e,r,k of a word e in the candidatesentence Ek, and the reference sentence E?r,k, wecan calculate this distance asdPER(Ek, E?r,k):=12(??Ik?I?k??+?e??ne,k?n?e,r,k??
)This distance is then normalized into an error rate,the PER, as described in section 2.1.1.A promising approach is to compare bigram orarbitrary m-gram count vectors instead of unigramcount vectors only.
This will take into account theordering of the words within a sentence implicitly,although not as strong as the WER does.2.1.3 BLEUBLEU (Papineni et al, 2001) is a precisionmeasure based on m-gram count vectors.
Theprecision is modified such that multiple referencesare combined into a single m-gram count vector,n?e,k := maxr n?e,r,k.
Multiple occurrences of anm-gram in the candidate sentence are counted ascorrect only up to the maximum occurrence countwithin the reference sentences.
Typically, m =1, .
.
.
, 4.To avoid a bias towards short candidate sentencesconsisting of ?safe guesses?
only, sentences shorterthan the reference length will be penalized with abrevity penalty.BLEU := lpBLEU ?
gmm{1sm+n?m?
(sm+?k?em1 ?Ekmin(nem1 ,k , n?em1 ,k))}with the geometric mean gm and a brevity penaltylpBLEU := min(1 , exp(1 ?I??I?
))In the original BLEU definition, the smoothingterm sm is zero.
To allow for sentence-wiseevaluation, Lin and Och (2004) define the BLEU-Smeasure with s1 := 1 and sm>1 := 0.
We haveadopted this technique for this study.2.1.4 NISTThe NIST score (Doddington, 2002) extendsthe BLEU score by taking information weights ofthe m-grams into account.
The NIST informationweight is defined asInfo(em1 ) := ?
(log2 ?
?nem1 ?
log2?
?nem?11)with ?
?nem1 :=?k,rn?en1 ,k,r.Note that the weight of a phrase occurringin many references sentence for a candidate isconsidered to be lower than the weight of a phraseoccurring only once!18The NIST score is the sum over all informationcounts of the co-occurring m-grams, summed upseparately for each m = 1, .
.
.
, 5 and normalizedby the total m-gram count.NIST := lpNIST ??m(1n?m?
?k?em1 ?Ekmin(nem1 ,k , n?em1 ,k)?
Info(em1 ))As in BLEU, there is a brevity penalty to avoid abias towards short candidates:lpNIST := exp(?
?
log22 min(1 ,I?I??
))where ?
:= ?
log2 2log22 3Due to the information weights, the value of theNIST score depends highly on the selection of thereference corpus.
This must be taken into accountwhen comparing NIST scores of different evaluationcampaigns.2.2 Other measuresLin and Och (2004) introduce a family of threemeasures named ROUGE.
ROUGE-S is a skip-bigram F-measure.
ROUGE-L and ROUGE-W aremeasures based on the length of the longest commonsubsequence of the sentences.
ROUGE-S has astructure similar to the bigram PER presented here.We expect ROUGE-L and ROUGE-W to have similarproperties to WER.In (Leusch et al, 2003), we have describedINVWER, a word error rate enhanced by blocktransposition edit operations.
As structure andscores of INVWER are similar to WER, we haveomitted INVWER experiments in this paper.3 Preprocessing and normalizationAlthough the general idea is clear, there are stillseveral details to be specified when implementingand using an automatic evaluation measure.
We aregoing to investigate the following problems:The first detail we have to state more precisely isthe term ?word?
in the above formulae.
A commonapproach for western languages is to consider spacesas separators of words.
The role of punctuationmarks in tokenization is arguable though.
Apunctuation mark can separate words, it can be partof a word, and it can be a word of its own.
Equallyit can be irrelevant at all for evaluation.On the same lines it is to be specified whetherwe consider words to be equal if they differ onlywith respect to upper and lower case.
For theIWSLT evaluation, (Paul et al, 2004) give anintroduction to how the handling of punctuationand case information may affect automatic MTevaluation.Also, a method to calculate the ?referencelength?
must specified if there are multiple referencesentences of different length.Since we want to compare automatic evaluationwith human evaluation, we have to clarify somequestions about assessing human evaluation as well:Large evaluation tasks are usually distributed toseveral human evaluators.
To smooth evaluationnoise, it is common practice to have each candidatesentence evaluated by at least two human judges in-dependently.
Therefore there are several evaluationscores for each candidate sentence.
We require asingle score for each system, though.
Consequently,we have to specify how to combine the evaluatorscores into sentence scores and then the sentencescores into a system score.Different definitions of this will have a significantimpact on automatic and human evaluation scores.3.1 Tokenization and punctuationThe importance of punctuation as well as thestrictness of punctuation rules depends on thelanguage.
In most western languages, correctpunctuation can vastly improve the legibility oftexts.
Marks like full stop or comma separate words.Other marks like apostrophes and hyphens can beused to join words, forming new words by this.
Forexample, the spelling ?There?s?
is a contraction of?There is?.Similar phenomena can be found in other lan-guages, although the set of critical characters mayvary.
Even when evaluating English translations, thecandidate sentences may contain source languageparts like proper names which should thus be treatedaccording to the source language.From the viewpoint of an automatic evaluationmeasure, we have to decide which units we wouldconsider to be words of their own.We have studied four tokenization methods.
Thesimplest method is keeping the original sentences,and considering only spaces as word separators.Moreover, we can consider all punctuation marks toseparate words but remove them completely then.The mteval tool (Papineni, 2002) improves this19Table 1: Tokenization methods studied?
Original candidatePowell said: "We?d not bealone; that?s for sure."?
Remove punctuationPowell said We d not be alonethat s for sure?
Tokenization of punctuation (mteval)Powell said : " We?d not bealone ; that?s for sure .
"?
Tokenization and treatment of abbreviationsand contractionsPowell said : " we would not bealone ; that is for sure .
"scheme by keeping all punctuation marks as separatewords except for decimal points and hyphensjoining composita.
We have extended this schemeby implementing a treatment of common Englishcontractions.
Table 1 illustrates these methods.3.2 Case sensitivityIn western languages, maintaining correct upperand lower case can improve the readability of atext.
Unfortunately, though the case of a worddepends on the word class, classification is notalways unambiguous.
What is more, the first wordin a sentence is always written in upper case.
Thislowers the significance of case information in MTevaluation, as even a valid reordering of wordsbetween candidate and reference sentence may leadto conflicting cases.
Consequently, we investigatedif and how case information can be exploited forautomatic evaluation.3.3 Reference lengthEach automatic evaluation measure we have takeninto account depends on the calculation of a refer-ence length: WER, PER, and ROUGE are normalizedby it, whereas NIST or BLEU incorporate it forthe determination of the brevity penalty.
In MTevaluation practise, there are multiple referencesentences for each candidate sentence, with differentlengths each.
It is thus not intuitively clear what the?reference length?
is.A simple choice here is the average length of thereference sentences.
Though this is modus operandifor NIST, it is problematic with brevity penalty or F-measure based scores, as even candidate sentencesthat are identical to a shorter-than-average referencesentence ?
which we would intuitively consider to be?optimal?
?
will then receive a sub-optimal score.BLEU incorporates a different method for thedetermination of the reference length in its defaultimplementation: Reference length here is thereference sentence length which is closest to thecandidate length.
If there is more than one theshortest of them is chosen.For measures based on the comparison of singlesentences such as WER, PER, and ROUGE, at leasttwo more methods deserve consideration:?
The average length of the sentences with thelowest absolute distance or highest similarityto the candidate sentence.
We call this method?average nearest-sentence length?.?
The length of the sentence with the lowestrelative error rate or the highest relativesimilarity.
We call this method ?best length?.Note that when using this method, not theminimum absolute distance is used for theerror rate, but the distance that leads tominimum relative error.Other strategies studied by us, e.g.
minimumlength of the reference sentences, did not showany theoretical or experimental advantage over themethods mentioned here.
Thus we will not discussthem in this paper.3.4 Sentence boundariesThe position of a word within a sentence can be quitesignificant for the correctness of the sentence.WER, INVWER, and ROUGE-L take into accountthe ordering explicitly.
This is not the case with n-PER, BLEU, or NIST, although the positions of innerwords are regarded implicitly by m-gram overlap.To model the position of words at the initial or theend of a sentence, one can enclose the sentence withartificial sentence boundary words.
Although thisis a common approach in language modelling, ithas to our knowledge not yet been applied to MTevaluation.3.5 Evaluator normalizationFor human evaluation, it has to be specified how tohandle evaluator bias, and how to combine sentencescores into system scores.Regarding evaluator bias, even accurate evalua-tion guidelines will not prevent a measurable dis-crepancy between the scores assigned by differenthuman evaluators.The 2003 TIDES/MT evaluation may serve asan example here: Since the candidate sentences of20543210.00.20.40.60.81.0Relative assessment countEvaluatorFigure 1: Distribution of adequacy assessments foreach human evaluator.
TIDES CE corpus.the participating systems were randomly distributedamong ten human evaluators, one would expect theassessed scores to be independent of the evaluator.Figure 1 indicates that this is indeed not the case,as the evaluators can clearly be distinguished by theamount of good and bad marks they assessed.
(0, 1) evaluator normalization overcomes thisbias: For each human evaluator the average sentencescore given by him or her and its variance arecalculated.
These assignments are then normalizedto (0, 1) expectation and standard deviation (Dod-dington, 2003), separately for each evaluator.Evaluator normalization should be unnecessaryfor system evaluation, as the evaluator biasestend to cancel out over the large amount ofcandidate sentences if the alignment of evaluatorsand systems is random enough.
Moreover, with(0, 1) normalization the calculated system scores arerelative, not absolute scores.
As such they can onlybe compared with scores out of the same evaluation.Whereas the assessments by the human evaluatorsare given on the sentence level, our interest maylie on the evaluation of whole candidate systems.Depending on the number of assessments percandidate sentence, different combination methodsfor the sentence scores can be considered for this,e.g.
mean or median.
As our data consisted onlyof two or three human assessments per sentence, wehave only applied the mean in our experiments.It has to be defined how a system score iscalculated from the sentence scores.
All of theautomatic evaluation measures implicitly weight thecandidate sentences by their length.
Consequently,we applied for the human evaluation scores aweighting by length on sentence level as well.Table 2: Corpus statisticsTIDES CE TIDES AE BTEC CESource language Chinese Arabic ChineseTarget language English English EnglishSentences 919 663 500Running words 25784 17763 3632Punctuation marks 3760 2698 610Ref.
translations 4 4 16Avg.
ref.
length 28.1 26.8 7.3Candidate systems 7 6 114 Experimental resultsTo assess the impact of the mentioned preprocessingsteps, we calculated scores for several automaticevaluation measures with varying preprocessing,reference length calculation, etc.
on three eval-uation test sets from international MT evaluationcampaigns.
We then compared these automatic eval-uation results with human evaluation of adequacyand fluency by determining a correlation coefficientbetween human and automatic evaluation.
Wechose Pearson?s r for this.
Although all evaluationmeasures were calculated using length weighting,we did not do any weighting when calculating thesentence level correlation.Regarding the m-gram PER, we had studied m-gram lengths of up to 8 both separately and in com-bination with shorter m-gram lengths in previousexperiments.
However, an m-gram length of greaterthan 4 did not show noteworthy correlation.
For this,we will leave out these results in this paper.For the sake of clarity, we will also leaveout measures that behave very similarly to akinmeasures e.g.
INVWER and WER, 2-PER and 1-PER, or BLEU and BLEU-S.Since WER and PER are error measures, whereasBLEU and NIST are similarity measures, thecorrelation coefficients with human evaluation willhave opposite signs.
For convenience, we will lookat the absolute coefficients only.4.1 CorporaFrom the 2003 TIDES evaluation campaign weincluded both the Chinese-English and the Arabic-English test corpus in our experiments.
Both wereprovided with adequacy and fluency scores between1 and 5 for seven and six candidate sets respectively.As we wanted to perform experiments on a corpuswith a larger amount of MT systems, we alsoincluded the IWSLT BTEC 2004 Chinese-English21evaluation (Akiba et al, 2004).
We restricted ourexperiments to the eleven MT systems that had beentrained on a common training corpus.Corpus statistics can be found in table 2.4.2 Experimental baselineIn our first experiment we studied the correlationof the different evaluation measures with humanevaluation at ?baseline?
conditions.
These includedno sentence boundaries, but tokenization withtreatment of abbreviations, see table 1.
Forsentence evaluation, conditions included evaluatornormalization.
Case information was removed.
Weused these settings in the other experiments, too, ifnot stated otherwise.Figure 2 shows the correlation between automaticand human scores.
On the TIDES corpora thesystem level correlation is particularly high, at amoderate sentence level correlation.
We assumethe latter is due to the poor sentence inter-annotatoragreement on these corpora, which is then smoothedout on system level.
On the BTEC corpusa high sentence level correlation accompanies asignificantly lower system level correlation.
Notethat due to the much lower number of samples onthe system level (e.g.
5 vs. 5500), small changesin the sentence level correlation are more likely tobe significant than such changes on system level.We have verified these effects by inspecting the rankcorrelation on both levels, as well as by experimentson other corpora.
Although these experimentssupport our findings, we have omitted results hereWERPER BLEUSNIST l AdequacyFluencyTIDESCE TIDESAE BTECCE0.00.20.40.6l l l ll l l lllllTIDESCE TIDESAE BTECCE0.00.20.40.60.81.0l l l ll l l l llllFigure 2: Pearson correlation coefficient betweenautomatic and human evaluation.
Bars indicatecorrelation with adequacy, circles with fluencyscore.Left: sentence, right: system level correlation.W WERP PER B BLEUS ll no normalizationnormalization0.00.20.40.6l l l l ll l l l l l lTIDESCE TIDESAEW P B W P B0.00.20.40.60.81.0 l l l l l l l l l l l lTIDESCE TIDESAEW P B W P BFigure 3: Effect of evaluator normalization.Left: sentence, right: system level correlation.W WERP PER B BLEUS ll use caseignore case0.00.20.40.6ll ll llll ll ll llll llTIDESCE TIDESAE BTECCEW P B W P B W P B0.00.20.40.60.81.0 llllllll ll ll llllllTIDESCE TIDESAE BTECCEW P B W P B W P BFigure 4: Effect of case normalization.Left: sentence, right: system level correlation.for the sake of clarity.4.3 Evaluator normalizationWe studied the effect of (0, 1)-normalization ofscores assigned by human evaluators.
The NISTmeasure showed a behavior very similar to that ofthe other measures and is thus left out in the graph.The correlation of all automatic measures both withfluency and with adequacy increases significantlyat sentence level (figure 3).
We do not notice apositive effect on system level, which confirms theassumption stated in section 3.5.4.4 Tokenization and case normalizationThe impact of case information was analyzed in ournext experiment.
Figure 4 (again without the NISTmeasure as it shows a similar behavior to the othermeasures) indicates that it is advisable to disregardcase information when looking into adequacy onsentence level.
Surprisingly, this also holds for22W WERB BLEUS l AdequacyFluency ll llkeepremove tokenizetok+treat.0.00.20.40.6llll llll llll llllTIDESCE TIDESAEW B W B0.00.20.40.60.81.0 llll llll llll llllTIDESCE TIDESAEW B W BFigure 5: Effect of different tokenization steps.Left: sentence, right: system level correlation.fluency.
We do no find a clear tendency on whetheror not to regard case information at system level.Figure 5 indicates that the way of handlingpunctuation we proposed does pay off when eval-uating adequacy.
For fluency our results werecontradictory: A slight decrease on the Arabic-English corpus is accompanied by a slight decay onthe Chinese-English corpus.
We did not investigatethe BTEC corpus here as most systems sticked to thetokenization guidelines for this evaluation.4.5 Reference lengthThe dependency of evaluation measures on theselection of reference lengths is rarely covered inthe literature.
However, as we can see in figure 6,our experiments indicate a significant impact.
Theselected three methods here are the default forWER/PER, NIST, and BLEU, respectively.
For thedistance based evaluation measures, represented byW WERB BLEUS N NIST ll laveragenearest best0.00.20.40.6lllllllllllllllllllllTIDESCE TIDESAE BTECCEW B N W B N W B N0.00.20.40.60.81.0lllllllllllllllllllllTIDESCE TIDESAE BTECCEW B N W B N W B NFigure 6: Effect of different reference lengths.Left: sentence, right: system level correlation.P 2PERB BLEUS N NIST ll llnoneinitial endboth0.00.20.40.6llllllllllll llllllllllllTIDESCE TIDESAEP B N P B N0.00.20.40.60.81.0 llllllllllll llllllllllllTIDESCE TIDESAEP B N P B NFigure 7: Effect of sentence boundaries.Left: sentence, right: system level correlation.WER here, taking the length of the sentence leadingto the best score leads to the best correlation withboth fluency and adequacy.
Taking the averagelength instead seems to be the worst choice.For brevity penalty based measures, the effect isnot as clear: On both TIDES corpora there is nosignificant difference in correlation between usingthe average length and the nearest length.
Onthe BTEC corpus, choosing the nearest sentencelength leads to a significantly higher correlation thanchoosing the average length.
We assume this is dueto the high number of reference sentences on thiscorpus.4.6 Sentence boundariesAs sentence boundaries will only influence m-gramcount vector based measures, we have restrictedour experiments to bigram PER, BLEU-S, and NISThere.
Including sentence boundaries (figure 7)has a positive effect on correlation with fluencyand adequacy for both bigram PER and BLEU-S.Sentence initials seem to be more important thansentence ends here.
For the NIST measure, we donot find any significant effect.5 DiscussionIn a perfect MT world, any dependency of anevaluation on case information or tokenizationshould be inexistent, as MT systems already haveto deal with both in the translation process, andcould be designed to produce output according toevaluation campaign guidelines.
Once all translationsystems stick to the same specifications, no furtherpreprocessing steps should be necessary.In practice there will be some systems that step23out of line.
If we then choose strict rules regardingcase information and punctuation, automatic errormeasures will penalize these systems rather hard,whereas penalty is rather low if we choose lax ones.In this situation case information will have alarge effect on the correlation between automaticand human evaluation, depending on whether theinvolved candidate systems will have a good or a badhuman evaluation.
It is vital to keep this in mindwhen drawing conclusions here regarding systemevaluation, despite the obvious importance of caseinformation in natural languages.These considerations also hold for the treatmentof punctuation marks, as a special care should beunnecessary if all systems sticked to tokenizationspecifications.
In practise, MT systems differin the way they generate and handle punctuationmarks.
Therefore, appropriate preprocessing stepsare advisable.Our experiments suggest that sentence boundariesincrease correlation between automatic scores andadequacy both on sentence and on system level.For fluency, the improvement is less significant, andmainly depends on the sentence initials.For length penalty based measures, we have foundthat choosing the nearest sentence length yields thehighest correlation with human evaluation.
Fordistance based measures instead, it seems advisableto choose the sentence that leads to the best relativescore as the one that determines the reference length.6 ConclusionWe have described several MT evaluation measures.We have pointed out common preprocessing stepsand auxiliary methods which have not been studiedin detail so far in spite of their importance forthe MT evaluation process.
Particularly, we haveintroduced a novel method for determining thereference length of an evaluation candidate sentence,and a simple method to incorporate sentenceboundary information to m-gram based evaluationmeasures.We then have performed several experimentson these methods on three evaluation corpora.The results indicate that both our new referencelength algorithm and the use of sentence boundariesimprove the correlation of the studied automaticevaluation measures with human evaluation.
Fur-thermore, we have learned that case informationshould be removed when performing automaticsentence evaluation.
On sentence level, evaluatornormalization can improve the correlation betweenautomatic and human evaluation.AcknowledgementsThis work was partially funded by the DeutscheForschungsgemeinschaft (DFG) under the project?Statistische Textu?bersetzung?
(Ne572/5) and by theEuropean Union under the integrated project TC-STAR ?
Technology and Corpora for Speech toSpeech Translation (IST-2002-FP6-506738).ReferencesY.
Akiba, M. Federico, N. Kando, H. Nakaiwa, M. Paul,and J. Tsujii.
2004.
Overview of the IWSLT04evaluation campaign.
In Proc.
IWSLT, pp.
1?12,Kyoto, Japan, September.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics.
In Proc.
ARPA Workshop on Human LanguageTechnology.G.
Doddington.
2003.
NIST MT Evaluation Workshop.Personal communication, July.G.
Leusch, N. Ueffing, and H. Ney.
2003.
A novelstring-to-string distance measure with applications tomachine translation evaluation.
In Proc.
MT SummitIX, pp.
240?247, New Orleans, LA, September.C.
Y. Lin and F. J. Och.
2004.
Orange: a method forevaluation automatic evaluation metrics for machinetranslation.
In Proc.
COLING 2004, pp.
501?507,Geneva, Switzerland, August.K.
A. Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
Technical Report RC22176 (W0109-022),IBM Research Division, Thomas J. Watson ResearchCenter, September.K.
A. Papineni.
2002.
The NIST mteval scor-ing software.
http://www.itl.nist.gov/iad/894.01/tests/mt/resources/scoring.htm.M.
Paul, H. Nakaiwa, and M. Federico.
2004.
Towardsinnovative evaluation methodologies for speech trans-lation.
In Working Notes of the NTCIR-4 Meeting,volume 2, pp.
17?21.C.
Tillmann, S. Vogel, H. Ney, A. Zubiaga, andH.
Sawaf.
1997.
Accelerated DP based search forstatistical translation.
In European Conf.
on SpeechCommunication and Technology, pp.
2667?2670,Rhodes, Greece, September.24
