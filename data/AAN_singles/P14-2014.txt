Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 81?86,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsDescending-Path Convolution Kernel for Syntactic StructuresChen Lin1, Timothy Miller1, Alvin Kho1, Steven Bethard2,Dmitriy Dligach1, Sameer Pradhan1and Guergana Savova1,1Children?s Hospital Boston Informatics Program and Harvard Medical School{firstname.lastname}@childrens.harvard.edu2Department of Computer and Information Sciences, University of Alabama at Birminghambethard@cis.uab.eduAbstractConvolution tree kernels are an efficientand effective method for comparing syntac-tic structures in NLP methods.
However,current kernel methods such as subset treekernel and partial tree kernel understate thesimilarity of very similar tree structures.Although soft-matching approaches can im-prove the similarity scores, they are corpus-dependent and match relaxations may betask-specific.
We propose an alternative ap-proach called descending path kernel whichgives intuitive similarity scores on compa-rable structures.
This method is evaluatedon two temporal relation extraction tasksand demonstrates its advantage over richsyntactic representations.1 IntroductionSyntactic structure can provide useful features formany natural language processing (NLP) taskssuch as semantic role labeling, coreference resolu-tion, temporal relation discovery, and others.
How-ever, the choice of features to be extracted from atree for a given task is not always clear.
Convolu-tion kernels over syntactic trees (tree kernels) offera potential solution to this problem by providingrelatively efficient algorithms for computing sim-ilarities between entire discrete structures.
Thesekernels use tree fragments as features and countthe number of common fragments as a measure ofsimilarity between any two trees.However, conventional tree kernels are sensitiveto pattern variations.
For example, two trees in Fig-ure 1(a) sharing the same structure except for oneterminal symbol are deemed at most 67% similarby the conventional tree kernel (PTK) (Moschitti,2006).
Yet one might expect a higher similaritygiven their structural correspondence.The similarity is further attenuated by trivialstructure changes such as the insertion of an ad-jective in one of the trees in Figure 1(a), whichwould reduce the similarity close to zero.
Suchan abrupt attenuation would potentially propel amodel to memorize training instances rather thangeneralize from trends, leading towards overfitting.In this paper, we describe a new kernel oversyntactic trees that operates on descending pathsthrough the tree rather than production rules asused in most existing methods.
This representationis reminiscent of Sampson?s (2000) leaf-ancestorpaths for scoring parse similarities, but here it isgeneralized over all ancestor paths, not just thosefrom the root to a leaf.
This approach assigns morerobust similarity scores (e.g., 78% similarity in theabove example) than other soft matching tree ker-nels, is faster than the partial tree kernel (Moschitti,2006), and is less ad hoc than the grammar-basedconvolution kernel (Zhang et al, 2007).2 Background2.1 Syntax-based Tree KernelsSyntax-based tree kernels quantify the similaritybetween two constituent parses by counting theircommon sub-structures.
They differ in their defini-tion of the sub-structures.Collins and Duffy (2001) use a subset tree (SST)representation for their sub-structures.
In the SSTrepresentation, a subtree is defined as a subgraphwith more than one node, in which only full pro-duction rules are expanded.
While this approach iswidely used and has been successful in many tasks,the production rule-matching constraint may be un-necessarily restrictive, giving zero credit to rulesthat have only minor structural differences.
Forexample, the similarity score between the NPs inFigure 1(b) would be zero since the production ruleis different (the overall similarity score is above-zero because of matching pre-terminals).The partial tree kernel (PTK) relaxes the defi-nition of subtrees to allow partial production rule81a)NPDTaNNcatNPDTaNNdogb)NPDTaNNcatNPDTaJJfatNNcatc)SADVPRBhereNPPRPsheVPVBZcomesSNPPRPsheVPVBZcomesADVPRBhereFigure 1: Three example tree pairs.matching (Moschitti, 2006).
In the PTK, a subtreemay or may not expand any child in a productionrule, while maintaining the ordering of the childnodes.
Thus it generates a very large but sparsefeature space.
To Figure 1(b), the PTK generatesfragments (i) [NP [DT a] [JJ fat]]; (ii) [NP [DT a][NN cat]]; and (iii) [NP [JJ fat] [NN cat]], amongothers, for the second tree.
This allows for partialmatching ?
substructure (ii) ?
while also generatingsome fragments that violate grammatical intuitions.Zhang et al (2007) address the restrictivenessof SST by allowing soft matching of productionrules.
They allow partial matching of optionalnodes based on the Treebank.
For example, therule NP ?
DT JJ NN indicates a noun phraseconsisting of a determiner, adjective, and commonnoun.
Zhang et al?s method designates the JJ asoptional, since the Treebank contains instances ofa reduced version of the rule without the JJ node(NP ?
DT NN ).
They also allow node match-ing among similar preterminals such as JJ, JJR, andJJS, mapping them to one equivalence class.Other relevant approaches are the spectrum tree(SpT) (Kuboyama et al, 2007) and the route kernel(RtT) (Aiolli et al, 2009).
SpT uses a q-gram?
a sequence of connected vertices of length q ?as their sub-structure.
It observes grammar rulesby recording the orientation of edges: a?b?c isdifferent from a?b?c.
RtT uses a set of routes asbasic structures, which observes grammar rules byNPDTaNNcatl=0: [NP],[DT],[NN]l=1: [NP-DT],[NP-NN],[DT-a],[NN-cat]l=2: [NP-DT-a],[NP-NN-cat]Figure 2: A parse tree (left) and its descendingpaths according to Definition 1 (l - length).recording the index of a neighbor node.2.2 Temporal Relation ExtractionAmong NLP tasks that use syntactic informa-tion, temporal relation extraction has been draw-ing growing attention because of its wide applica-tions in multiple domains.
As subtasks in Tem-pEval 2007, 2010 and 2013, multiple systemswere built to create labeled links from eventsto events/timestamps by using a variety of fea-tures (Bethard and Martin, 2007; Llorens et al,2010; Chambers, 2013).
Many methods exist forsynthesizing syntactic information for temporalrelation extraction, and most use traditional treekernels with various feature representations.
Mir-roshandel et al (2009) used the path-enclosed tree(PET) representation to represent syntactic informa-tion for temporal relation extraction on the Time-Bank (Pustejovsky et al, 2003) and the AQUAINTTimeML corpus1.
The PET is the smallest subtreethat contains both proposed arguments of a relation.Hovy et al (2012) used bag tree structures to rep-resent the bag of words (BOW) and bag of part ofspeech tags (BOP) between the event and time inaddition to a set of baseline features, and improvedthe temporal linking performance on the TempEval2007 and Machine Reading corpora (Strassel etal., 2010).
Miller at al.
(2013) used PET tree, bagtree, and path tree (PT, which is similar to a PETtree with the internal nodes removed) to representsyntactic information and improved the temporalrelation discovery performance on THYME data2(Styler et al, 2014).
In this paper, we also usesyntactic structure-enriched temporal relation dis-covery as a vehicle to test our proposed kernel.3 MethodsHere we decribe the Descending Path Kernel(DPK).1http://www.timeml.org2http://thyme.healthnlp.org82Definition 1 (Descending Path): Let T be aparse tree, v any non-terminal node in T , dv adescendant of v, including terminals.
A descendingpath is the sequence of indexes of edges connectingv and dv, denoted by [v ?
?
?
?
?
dv].
The length lof a descending path is the number of connectingedges.
When l = 0, a descending path is the non-terminal node itself, [v].
Figure 2 illustrates a parsetree and its descending paths of different lengths.Suppose that all descending paths of a tree T areindexed 1, ?
?
?
, n, and pathi(T ) is the frequencyof the i-th descending path in T .
We represent T asa vector of frequencies of all its descending paths:?
(T ) = (path1(T ), ?
?
?
, pathn(T )).The similarity between any two trees T1and T2can be assessed via the dot product of their respec-tive descending path frequency vector representa-tions: K(T1, T2) = ??(T1),?
(T2)?.Compared with the previous tree kernels, ourdescending path kernel has the following advan-tages: 1) the sub-structures are simplified so thatthey are more likely to be shared among trees,and therefore the sparse feature issues of previouskernels could be alleviated by this representation;2) soft matching between two similar structures(e.g., NP?DT JJ NN versus NP?DT NN) havehigh similarity without reference to any corpus orgrammar rules;Following Collins and Duffy (2001), we derivea recursive algorithm to compute the dot productof the descending path frequency vector represen-tations of two trees T1and T2:K(T1, T2) = ??(T1),?
(T2)?=?ipathi(T1) ?
pathi(T2)=?n1?N1?n2?N2?iIpathi(n1) ?
Ipathi(n2)=?n1?N1n2?N2C(n1, n2)(1)where N1and N2are the sets of nodes in T1andT2respectively, i indexes the set of possible paths,Ipathi(n) is an indicator function that is 1 iff thedescending pathiis rooted at node n or 0 other-wise.
C(n1, n2) counts the number of commondescending paths rooted at nodes n1and n2:C(n1, n2) =?iIpathi(n1) ?
Ipathi(n2)C(n1, n2) can be computed in polynomial time bythe following recursive rules:Rule 1: If n1and n2have different labels (e.g.,?DT?
versus ?NN?
), then C(n1, n2) = 0;Rule 2: Else if n1and n2have the same labelsand are both pre-terminals (POS tags), thenC(n1, n2) = 1 +{1 if term(n1) = term(n2)0 otherwise.where term(n) is the terminal symbol under n;Rule 3: Else if n1and n2have the same labelsand they are not both pre-terminals, then:C(n1, n2) = 1 +?ni?children(n1)nj?children(n2)C(ni, nj)where children(m) are the child nodes of m.As in other tree kernel approaches (Collins andDuffy, 2001; Moschitti, 2006), we use a discountparameter ?
to control for the disproportionatelylarge similarity values of large tree structures.Therefore, Rule 2 becomes:C(n1, n2) = 1 +{?
if term(n1) = term(n2)0 otherwise.and Rule 3 becomes:C(n1, n2) = 1 + ?
?ni?children(n1)nj?children(n2)C(ni, nj)Note that Eq.
(1) is a convolution kernel underthe kernel closure properties described in Haus-sler (1999).
Rules 1-3 show the equivalence be-tween the number of common descending pathsrooted at nodes n1and n2, and the number ofmatching nodes below n1and n2.In practice, there are many non-matching nodes,and most matching nodes will have only a fewmatching children, so the running time, as in SST,will be approximated by the number of matchingnodes between trees.3.1 Relationship with other kernelsFor a given tree, DPK will generate significantlyfewer sub-structures than PTK, since it does notconsider all ordered permutations of a productionrule.
Moreover, the fragments generated by DPKare more likely to be shared among different trees.For the number of corpus-wide fragments, it is83Kernel ID #Frag Sim N(Sim)SST a 9 3 0.50O(?|N1||N2|)b 15 2 0.25c 63 7 0.20DPK a 11 7 0.78O(?2|N1||N2|)b 13 9 0.83c 31 22 0.83PTK a 20 10 0.67O(?3|N1||N2|)b 36 15 0.65c 127 34 0.42Table 1: Comparison of the worst case computa-tional complexicity (?
- the maximum branchingfactor) and kernel performance on the 3 examplesfrom Figure 1.
#Frag is the number of fragments,N(Sim) is the normalized similarity.
Please seethe online supplementary note for detailed frag-ments of example (a).possible that DPK?
SST?
PTK.
In Table 1, given?
= 1, we compare the performance of 3 kernelson the three examples in Figure 1.
Note that formore complicated structures, i.e., examples b andc, DPK generates fewer fragments than SST andPTK, with more shared fragments among trees.The complexity for all three kernels are at leastO(|N1||N2|)since they share the pairwise summa-tion at the end of Equation 1.
SST, due to its re-quirement of exact production rule matching, onlytakes one pass in the inner loop which adds a factorof ?
(the maximum branching factor of any pro-duction rule).
DPK does a pairwise summationof children, which adds a factor of ?2to the com-plexity.
Finally, the efficient algorithm for PTKis proved by Moschitti (2006) to contain a con-stant factor of ?3.
Table 1 orders the tree kernelsaccording by their listed complexity.It may seem that the value of DPK is strictly in itsability to evaluate all paths, which is not explicitlyaccounted for by other kernels.
However, anotherview of the DPK is possible by thinking of it ascheaply calculating rule production similarity bytaking advantage of relatively strict English wordordering.
Like SST and PTK, the DPK requiresthe root category of two subtrees to be the samefor the similarity to be greater than zero.
UnlikeSST and PTK, once the root category comparisonis successfully completed, DPK looks at all pathsthat go through it and accumulates their similarityscores independent of ordering ?
in other words, itwill ignore the ordering of the children in its pro-duction rule.
This means, for example, that if therule production NP?
NN JJ DT were ever foundin a tree, to DPK it would be indistinguishable fromthe common production NP?
DT JJ NN, despitehaving inverted word order, and thus would havea maximal similarity score.
SST and PTK wouldassign this pair a much lower score for having com-pletely different ordering, but we suggest that casessuch as these are very rare due to the relativelystrict word ordering of English.
In most cases, thedeterminer of a noun phrase will be at the front, thenouns will be at the end, and the adjectives in themiddle.
So with small differences in productionrules (one or two adjectives, extra nominal modifier,etc.)
the PTK will capture similarity by compar-ing every possible partial rule completion, but theDPK can obtain higher and faster scores by justcomparing one child at a time because the orderingis constrained by the language.
This analysis doeslead to a hypothesis for the general viability of theDPK, suggesting that in languages with freer wordorder it may give inflated scores to structures thatare syntactically dissimilar if they have the sameconstituent components in different order.Formally, Moschitti (2006) showed that SST isa special case of PTK when only the longest childsequence from each tree is considered.
On the otherend of the spectrum, DPK is a special case of PTKwhere the similarity between rules only considerschild subsequences of length one.4 EvaluationWe applied DPK to two published temporal relationextraction systems: (Miller et al, 2013) in theclinical domain and Cleartk-TimeML (Bethard,2013) in the general domain respectively.4.1 Narrative Container DiscoveryThe task here as described by Miller et al (2013) isto identify the CONTAINS relation between a timeexpression and a same-sentence event from clinicalnotes in the THYME corpus, which has 78 notesof 26 patients.
We obtained this corpus from theauthors and followed their linear composite kernelsetting:KC(s1, s2) = ?P?p=1KT(tp1, tp2)+KF(f1, f2) (2)where siis an instance object composed of flat fea-tures fiand a syntactic tree ti.
A syntactic tree ti84can have multiple representations, as in Bag Tree(BT), Path-enclosed Tree (PET), and Path Tree(PT).
For the tree kernel KT, subset tree (SST) ker-nel was applied on each tree representation p. Thefinal similarity score between two instances is the?
-weighted sum of the similarities of all representa-tions, combined with the flat feature (FF) similarityas measured by a feature kernel KF(linear or poly-nomial).
Here we replaced the SST kernel withDPK and tested two feature combinations FF+PETand FF+BT+PET+PT.
To fine tune parameters, weused grid search by testing on the default develop-ment data.
Once the parameters were tuned, wetested the system performance on the testing data,which was set up by the original system split.4.2 Cleartk-TimeMLWe tested one sub-task from TempEval-2013 ?the extraction of temporal relations between anevent and time expression within the same sen-tence.
We obtained the training corpus (Time-Bank + AQUAINT) and testing data from the au-thors (Bethard, 2013).
Since the original featuresdidn?t contain syntactic features, we created a PETtree extractor for this system.
The kernel settingwas similar to equation (2), while there was onlyone tree representation, PET tree, P=1.
A linearkernel was used as KFto evaluate the exact sameflat features as used by the original system.
Weused the built-in cross validation to do grid searchfor tuning the parameters.
The final system wastested on the testing data for reporting results.4.3 Results and DiscussionResults are shown in Table 2.
The top sectionshows THYME results.
For these experiments,the DPK is superior when a syntactically-rich PETrepresentation is used.
Using the full feature set ofMiller et al (2013), SST is superior to DPK andobtains the best overall performance.
The bottomsection shows results on TempEval-2013 data, forwhich there is little benefit from either tree kernel.Our experiments with THYME data show thatDPK can capture something in the linguisticallyricher PET representation that the SST kernel can-not, but adding BT and PT representations decreasethe DPK performance.
As a shallow representation,BT does not have much in the way of descendingpaths for DPK to use.
PT already ignores the pro-duction grammar by removing the inner tree nodes.DPK therefore cannot get useful information andmay even get misleading cues from these two rep-Features KTP R FTHYMEFF+PET DPK 0.756 0.667 0.708SST 0.698 0.630 0.662FF+BT+ DPK 0.759 0.626 0.686PET+PT SST 0.754 0.711 0.732TempEvalFF+PET DPK 0.328 0.263 0.292SST 0.325 0.263 0.290FF - 0.309 0.266 0.286Table 2: Comparison of tree kernel performancefor temporal relation extraction on THYME andTempEval-2013 data.resentations.
These results show that, while DPKshould not always replace SST, there are represen-tations in which it is superior to existing methods.This suggests an approach in which tree representa-tions are matched to different convolution kernels,for example by tuning on held-out data.For TempEval-2013 data, adding syntactic fea-tures did not improve the performance significantly(comparing F-score of 0.290 with 0.286 in Ta-ble 3).
Probably, syntactic information is not astrong feature for all types of temporal relations onTempEval-2013 data.5 ConclusionIn this paper, we developed a novel convolutiontree kernel (DPK) for measuring syntactic similar-ity.
This kernel uses a descending path represen-tation in trees to allow higher similarity scores onpartially matching structures, while being simplerand faster than other methods for doing the same.Future work will explore 1) a composite kernelwhich uses DPK for PET trees, SST for BT and PT,and feature kernel for flat features, so that differenttree kernels can work with their ideal syntactic rep-resentations; 2) incorporate dependency structuresfor tree kernel analysis 3) applying DPK to otherrelation extraction tasks on various corpora.6 AcknowledgementsThanks to Sean Finan for technically supporting theexperiments.
The project described was supportedby R01LM010090 (THYME) from the NationalLibrary Of Medicine.85ReferencesFabio Aiolli, Giovanni Da San Martino, and Alessan-dro Sperduti.
2009.
Route kernels for trees.
InProceedings of the 26th Annual International Con-ference on Machine Learning, pages 17?24.
ACM.Steven Bethard and James H Martin.
2007.
Cu-tmp:temporal relation classification using syntactic andsemantic features.
In Proceedings of the 4th Inter-national Workshop on Semantic Evaluations, pages129?132.
Association for Computational Linguis-tics.Steven Bethard.
2013.
Cleartk-timeml: A minimalistapproach to TempEval 2013.
In Second Joint Con-ference on Lexical and Computational Semantics (*SEM), volume 2, pages 10?14.Nate Chambers.
2013.
Navytime: Event and time or-dering from raw text.
In Second Joint Conferenceon Lexical and Computational Semantics (*SEM),Volume 2: Proceedings of the Seventh InternationalWorkshop on Semantic Evaluation (SemEval 2013),pages 73?77, Atlanta, Georgia, USA, June.
Associa-tion for Computational Linguistics.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Neural InformationProcessing Systems.David Haussler.
1999.
Convolution kernels on discretestructures.
Technical report, University of Califor-nia in Santa Cruz.Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Pat-wardhan, and Chris Welty.
2012.
When did thathappen?
: linking events and relations to timestamps.In Proceedings of the 13th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 185?193.
Association for Compu-tational Linguistics.Tetsuji Kuboyama, Kouichi Hirata, Hisashi Kashima,Kiyoko F Aoki-Kinoshita, and Hiroshi Yasuda.2007.
A spectrum tree kernel.
Information and Me-dia Technologies, 2(1):292?299.Hector Llorens, Estela Saquete, and Borja Navarro.2010.
Tipsem (english and spanish): EvaluatingCRFs and semantic roles in TempEval-2.
In Pro-ceedings of the 5th International Workshop on Se-mantic Evaluation, pages 284?291.
Association forComputational Linguistics.Timothy Miller, Steven Bethard, Dmitriy Dligach,Sameer Pradhan, Chen Lin, and Guergana Savova.2013.
Discovering temporal narrative containersin clinical text.
In Proceedings of the 2013 Work-shop on Biomedical Natural Language Processing,pages 18?26, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.Seyed Abolghasem Mirroshandel, M Khayyamian, andGR Ghassem-Sani.
2009.
Using tree kernels forclassifying temporal relations between events.
Proc.of the PACLIC23, pages 355?364.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In Machine Learning: ECML 2006, pages 318?329.Springer.James Pustejovsky, Patrick Hanks, Roser Sauri, An-drew See, Robert Gaizauskas, Andrea Setzer,Dragomir Radev, Beth Sundheim, David Day, LisaFerro, et al 2003.
The TimeBank corpus.
In Cor-pus linguistics, volume 2003, page 40.Geoffrey Sampson.
2000.
A proposal for improvingthe measurement of parse accuracy.
InternationalJournal of Corpus Linguistics, 5(1):53?68.Stephanie Strassel, Dan Adams, Henry Goldberg,Jonathan Herr, Ron Keesing, Daniel Oblinger,Heather Simpson, Robert Schrag, and JonathanWright.
2010.
The DARPA machine readingprogram-encouraging linguistic and reasoning re-search with a series of reading tasks.
In LREC.William Styler, Steven Bethard, Sean Finan, MarthaPalmer, Sameer Pradhan, Piet de Groen, Brad Er-ickson, Timothy Miller, Lin Chen, Guergana K.Savova, and James Pustejovsky.
2014.
Temporalannotations in the clinical domain.
Transactionsof the Association for Computational Linguistics,2(2):143?154.Min Zhang, Wanxiang Che, Ai Ti Aw, Chew Lim Tan,Guodong Zhou, Ting Liu, and Sheng Li.
2007.
Agrammar-driven convolution tree kernel for seman-tic role classification.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics, pages 200?207.86
