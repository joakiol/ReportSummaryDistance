Acquiring entailment pairs across languages and domains:A data analysisManaal FaruquiDept.
of Computer Science and EngineeringIndian Institute of TechnologyKharagpur, Indiamanaal.iitkgp@gmail.comSebastian Pad?Seminar f?r ComputerlinguistikUniversit?t HeidelbergHeidelberg, Germanypado@cl.uni-heidelberg.deAbstractEntailment pairs are sentence pairs of a premise and a hypothesis, where the premise textuallyentails the hypothesis.
Such sentence pairs are important for the development of Textual Entailmentsystems.
In this paper, we take a closer look at a prominent strategy for their automatic acquisitionfrom newspaper corpora, pairing first sentences of articles with their titles.
We propose a simplelogistic regression model that incorporates and extends this heuristic and investigate its robustnessacross three languages and three domains.
We manage to identify two predictors which predictentailment pairs with a fairly high accuracy across all languages.
However, we find that robustnessacross domains within a language is more difficult to achieve.1 IntroductionSemantic processing has become a major focus of attention in NLP.
However, different applicationssuch as Question Answering, Information Extraction and Machine Translation often adopt very different,task-specific semantic processing strategies.
Textual entailment (TE) was introduced by Dagan et al(2006) as a ?meta-task?
that can subsume a large part of the semantic processing requirements of suchapplications by providing a generic concept of inference that corresponds to ?common sense?
reasoningpatterns.
Textual Entailment is defined as a relation between two natural language utterances (a PremiseP and a Hypothesis H) that holds if ?a human reading P would infer that H is most likely true?.
See,e.g., the ACL ?challenge paper?
by Sammons et al (2010) for further details.The successive TE workshops that have taken place yearly since 2005 have produced annotation forEnglish which amount to a total of several thousand entailing Premise-Hypothesis sentence pairs, whichwe will call entailment pairs:(1) P: Swedish bond yields end 21 basis points higher.H: Swedish bond yields rose further.From the machine learning perspective assumed by many approaches to TE, this is a very small numberof examples, given the complex nature of entailment.
Given the problems of manual annotation, therefore,Burger and Ferro (2005) proposed to take advantage of the structural properties of a particular type ofdiscourse ?
namely newspaper articles ?
to automatically harvest entailment pairs.
They proposed to pairthe title of each article with its first sentence, interpreting the first sentence as Premise and the title asHypothesis.
Their results were mixed, with an average of 50% actual entailment pairs among all pairsconstructed in this manner.
SVMs which identified ?entailment-friendly?
documents based on their bagsof words lead to an accuracy of 77%.
Building on the same general idea, Hickl et al (2006) applied asimple unsupervised filter which removes all entailment pair candidates that ?did not share an entity (oran NP)?.
They report an accuracy of 91.8% on a manually evaluated sample ?
considerably better Burgerand Ferro.
The article however does not mention the size of the original corpus, and whether ?entity?
is to95be understood as named entity, so it is difficult to assess what its recall is, and whether it presupposes ahigh-quality NER system.In this paper, we model the task using a logistic regression model that allows us to synchronouslyanalyse the data and predict entailment pairs, and focus on the question of how well these results generalizeacross domains and languages, for many of which no entailment pairs are available at all.
We make threemain contributions: (a), we define an annotation scheme based on semantic and discourse phenomena thatcan break entailment and annotate two datasets with it; (b), we idenfiy two robust properties of sentencepairs that correlate strongly with entailment and which are robust enough to support high-precisionentailment pair extraction; (c), we find that cross-domain differences are actually larger than cross-lingualdifferences, even for languages as different as German and Hindi.Plan of the paper.
Section 2 defines our annotation scheme.
In Section 3, we sketch the logisticregression framework we use for analysis, and motivate our choice of predictors.
Sections 4 and 5 presentthe two experiments on language and domain comparisons, respectively.
We conclude in Section 6.2 A fine-grained annotation scheme for entailment pairsThe motivation of our annotation scheme is to better understand why entailment breaks down betweentitles and first sentences of newswire articles.
We subdivide the general no entailment category of earlierstudies according to an inventory of reasons for non-entailment that we collected from an informalinspection of some dozen articles from an English-language newspaper.
Additionally, we separate outsentences that are ill-formed in the sense of not forming one proposition.2.1 Subtypes of non-entailmentNo-par (Partial entailment).
The Premise entails the Hypothesis almost, but not completely, in one oftwo ways: (a), The Hypothesis is a conjunction and the Premise entails just one conjunct; or (b),Premise and Hypothesis share the main event, but the Premise is missing an argument or adjunctthat forms part of the Hypothesis.
Presumably, in our setting, such information is provided by theother sentences in the article than the first one.
In Ex.
(1), if P and H were switched, this would bethe case for the size of the rise.No-pre (Presupposition): The Premise uses a construction which can only be understood with informa-tion from the Hypothesis, typically a definite description or an adjunct.
This category arises becausethe title stands before the first sentence and is available as context.
In the following example, thePremise NP ?des Verbandes?
can only be resolved through the mention of ?VDA?
(the German carmanufacturer?s association) in the Hypothesis.
(2) P: HerzogHerzogwirdwillinindemthevierk?pfigenfour-headF?hrungsgremiummanagement boarddesof theVerbandsassociationf?rfordietheTeile-partsundandZubeh?rindustrieaccessory businesszust?ndigresposiblesein.be.H: MartinMartinHerzogHerzogwirdbecomesVDA-Gesch?ftsf?hrer.VDA manager.No-con (Contradiction): Direct contradiction of Premise and Hypothesis.
(3) P: WieHowdietheinnerebiologicalUhrclock[...][...]funktioniert,works,istisnochstillweitgehendmostlyunbekannt.unknown.H: LichtLightstelltregulatesdietheinnerebiologicalUhr.clock.96No-emb (Embedding): The Premise uses an embedding that breaks entailment (e.g., modal adverbials ornon-factural embedding verb).
In the following pair, the proposition in the Hypothesis is embeddedunder ?expect?.
(4) P: An Arkansas gambling amendment [...] is expected to be submitted to the state SupremeCourt Monday for a rehearing, a court official said.H: Arkansas gaming petition goes before court again MondayNo-oth (Other): All other negative examples where Premise and Hypothesis are well-formed, and whichcould not be assigned to a more specific category, are included under this tag.
In this sense, ?Other?is a catch-all category.
Often, Premise and Hypothesis, taken in isolation, are simply unrelated:(5) P: Victor the Parrot kept shrieking "Voda, Voda" ?
"Water, Water".H: Thirsty jaguar procures water for Bulgarian zoo.2.2 Ill-formed sentence pairsErr (Error): These cases arise due to errors in sentence boundary detection: Premise or Hypothesis maybe cut off in the middle of the sentence.Ill (Ill-formed): Often, the titles are not single grammatical sentences and can therefore not be interpretedsensibly as the Hypothesis of an entailment pair.
They can be incomplete proposition such as NPsor PPs (?Beautiful house situated in woods?
), or, frequently, combinations of multiple sentences(?RESEARCH ALERT - Mexico upped, Chile cut.?
).3 Modeling entailment with logistic regressionWe will model the entailment annotation labels on candidate sentence pairs using a logistic regressionmodel.
From a machine learning point of view, logistic regression models can be seen as a rather simplestatistical classifier which can be used to acquire new entailment pairs.
From a linguistic point of view,they can be used to explain the phenomena in the data, see e.g., Bresnan et al (2007).Formally, logistic regression models assume that datapoints consist of a set of predictors x and abinary response variable y.
They have the formp(y = 1) = 11 + e?z with z =?i?ixi (1)where p is the probability of a datapoint x, ?i is the coefficient assigned to the linguistically motivatedfactor xi.
Model estimation sets the parameters ?
so that the likelihood of the observed data is maximized.From the linguistics perspective, we are most interested in analysing the importance of the differentpredictors: for each predictor xi, the comparison of the estimated value of its coefficient ?i can becompared to its estimated standard error, and it is possible to test the hypothesis that ?i = 0, i.e., thepredictor does not significantly contribute to the model.
Furthermore, the absolute value of ?i can beinterpreted as the log odds ?
that is, as the change in the probability of the response variable being positivedepending on xi being positive.e?i = P (y = 1|x = 1, .
.
.
)/P (y = 0|x = 1, .
.
.
)P (y = 1|x = 0, .
.
.
)/P (y = 0|x = 0, .
.
. )
(2)The fact that z is just a linear combination of predictor weights encodes the assumption that the log oddscombine linearly among factors.From the natural language processing perspective, we would like to create predictions for newobservations.
Note, however, that simply assessing the significance of predictors on some dataset, as97provided by the logistic regression model, corresponds to an evaluation of the model on the training set,which is prone to the problem of overfitting.
We will therefore in our experiments always apply the modelsacquired from one dataset on another to see how well they generalize.3.1 Choice of PredictorsNext, we need a set of plausible predictors that we can plug into the logistic regression framework.
Thesepredictors should ideally be language-independent.
We analyse the categories of our annotation, as aninventory of phenomena that break entailment, to motivate a small set of robust predictors.Following early work on textual entailment, we use word overlap as a strong indicator of entail-ment (Monz and de Rijke, 2001).
Our weighted overlap predictor uses the well-known tf/idf weightingscheme to compute the overlap between P and H (Manning et al, 2008):weightedOverlap(T,H,D) =?w?T?H tf-idf(w,D)?w?H tf-idf(w,D)(3)where we treat each article as a separate document and the whole corpus as document collection D. Weexpect that No-oth pairs have generally the lowest weighted overlap, followed by No-par pairs, while Yespairs have the highest weighted overlap.
We also use a categorical version of this observation in the formof our strict noun match predictor.
This predictor is similar in spirit to the proposal by Hickl et al (2006)mentioned in Section 1.
The boolean strict noun match predictor is true if all Hypothesis nouns are presentin the Premise, and is therefore a predictor that is geared at precision rather than recall.
A third predictorthat was motivated by the No-par and No-oth categories was the number of words in the article: No-othsentence pairs often come from long articles, where the first sentence provides merely an introduction.
Forthis predictor, log num words, we count the total number of words in the article and logarithmize it.1 Theremaining subcategories of No were more difficult to model.
No-pre pairs should be identifiable by testingwhether the Premise contains a definite description that cannot be accommodated, a difficult problemthat seems to require world knowledge.
Similarly, the recognition of contradictions, as is required to findNo-con pairs, is very difficult in itself (de Marneffe et al, 2008).
Finally, No-emb requires the detectionof a counterfactual context in the Premise.
Since we do not currently see robust, language-independentways of modelling these phenomena, we do not include specific predictors to address them.The situation is similar with regard to the Err category.
While it might be possible to detect incompletesentences with the help of a parser, this again involves substantial knowledge about the language.
The Illcategory, however, appears easier to target: at least cases of Hypotheses consisting of multiple phrasescase be detected easily by checking for sentence end markers in the middle of the Hypothesis (full stop,colon, dash).
We call this predictor punctuation.4 Experiment 1: Analysis by Language4.1 Data sources and preparationThis experiment performs a cross-lingual comparison of three newswire corpora.
We use English, German,and Hindi.
All three belong to the Indo-European language family, but English and German are moreclosely related.For English and German, we used the Reuters RCV2 Multilingual Corpus2.
RCV2 contains over487,000 news stories in 13 different languages.
Almost all news stories cover the business and politicsdomains.
The corpus marks the title of each article; we used the sentence splitter provided by Treetag-ger (Schmid, 1995) to extract the first sentences.
Our Hindi corpus is extracted from the text collectionof South Asian languages prepared by the EMILLE project (Xiao et al, 2004)3.
We use the Hindi1This makes the coefficiently easier to interpret.
The predictive difference is minimal.2http://trec.nist.gov/data/reuters/reuters.html3http://www.elda.org/catalogue/en/text/W0037.html98No.
of sentence pairs English German HindiOriginal 473,874 (100%) 112,259 (100%) 20,209 (100%)Filtered 264.711 (55.8%) 50.039 (44.5%) 10.475 (51.8%)Table 1: Pair extraction statisticsCorpus err ill no-con no-emb no-oth no-par no-pre yesEnglish Reuters 3.5 2.9 0 0.2 3.7 7.4 0 82.3German Reuters 2.1 11.0 0.4 0.2 4.3 2.1 0.2 79.7Hindi Emille 1.1 2.5 0 0.3 14.7 5.7 0 75.7Table 2: Exp.1: Distribution of annotation categories (in percent)monolingual data, which was crawled from Webdunia,4 an Indian daily online newspaper.
The articlesare predominantly political, with a focus on Indo-Pakistani and Indo-US affairs.
We identify sentenceboudaries with the Hindi sentence marker (?|?
), which is used exclusively for this purpose.We preprocessed the data by extracting the title and the first sentence, treating the first sentence asPremise and the title as Hypothesis.
We applied a filter to remove pairs where the chance of entailmentwas impossible or very small.
Specifically, our filter keeps only sentence pairs that (a) share at least onenoun and where (b) both sentences include at least one verb and are not questions.
Table 1 shows thecorpus sizes before and after filtering.
Note that the percentage of selected sentences across the languagesare all in the 45%-55% range.
This filter could presumably be improved by requiring a shared namedentity, but since language-independent NER is still an open research issue, we did not follow up on thisavenue.
We randomly sampled 1,000 of the remaining sentence pairs per language for manual annotation.4.2 Distribution of annotation categoriesFirst, we compared the frequencies of the annotation categories defined in Section 3.1.
The results areshown in Table 2.
We find our simple preprocessing filter results in an accuracy of between 75 and 82%.This is still considerably below the results of Hickl et al, who report 92% accuracy on their English data.5Even though the overall percentage of ?yes?
cases is quite similar among languages, the details of thedistribution differ.
One fairly surprising observation was the fairly large number of ill-formed sentencepairs.
As described in Section 2, this category comprises cases where the Hypothesis (i.e., a title) is not agrammatical sentence.
Further analysis of the category shows that the common patterns are participleconstructions (Ex.
(6)) and combinations of multiple statements (Ex.
(7)).
The participle construction isparticularly prominent in German.
(6) Glencoe Electric, Minn., rated single-A by Moody?s.
(7) WiederAgainK?mpfefightsininS?dlibanonSouthern Lebanon--IsraeliIsraeliget?tet.killed.The ?no?-categories make up a total of 11.3% (English), 6.6% (German), and 20.7% (Hindi).
The ?other?and ?partial?
categories clearly dominate.
This is to be expected, in particular the high number of partialentailments.
The ?other?
category mostly consists of cases where the title summarizes the whole article,but the first sentence provides only a gentle introduction to the topic:(8) P: One automotive industry analyst has dubbed it the ?Lincoln Town Truck?.H: Ford hopes Navigator will lure young buyers to Lincoln.As regards the high ratio of ?no-other?
cases in the Hindi corpus, we found a high number of instanceswhere the title states the gist of the article too differently from the first sentence to preserve entailment:4http://www.webdunia.com5We attribute the difference to the filtering scheme which is difficult to reconstruct from Hickl et al (2006).99Predictor German sig English sig Hindi sigweighted overlap -0.77 ** -2.30 *** -3.35 ***log num words -0.05 ?
-0.03 ?
-0.17 ?punctuation -1.04 *** -0.43 ** -0.35 **strict noun match -0.12 ?
-0.19 ?
-0.38 **Table 3: Exp.
1: Predictors in the logreg model (*: p<0.05; **: p<0.01; ***: p<0.001)(9) P: aAj BF E?\ss XAynA kF lokE?ytA km nhF\ h I h{ .Even today, Princess Diana?s popularity has not decreased.H: E?\ss XAynA k p/ aOr kAXnFlAm ho\g .Bidding on Princess Diana?s letter and cards would take place.The remaining error categories (embedding, presupposition, contradiction) were, disappointingly, almostabsent.
Another sizable category is formed by errors, though.
We find the highest percentage for English,where our sentence splitter misinterpreted full stops in abbreviations as sentence boundaries.4.3 Modelling the dataWe estimated logistic regression models on each dataset, using the predictors from Section 3.1.
Consider-ing the eventual goal of extracting entailment pairs, we use the decision yes vs. everything else as ourresponse variable.
The analysis was performed with R, using the rms6 and ROCR7 packages.Analysis of predictors.
The coefficients for the predictors and their significances are shown in Table 3.There is considerable parallelism between the languages.
In all three languages, weighted overlap betweenH and P is a significant predictor: high overlap indicates entailment, and vice versa.
Its effect size is largeas well: Perfect overlap increases the probability of entailment for German by a factor of e0.77 = 2.16, forEnglish by 10, and for Hindi even by 28.
Similarly, the punctuation predictor comes out as a significantnegative effect for all three languages, presumably by identifying ill-formed sentence pairs.
In contrast,the length of the article (log num words) is not a significant predictor.
This is a surprising result, givenour hypothesis that long articles often involve an ?introduction?
which reduces the chance for entailmentbetween the title and the first sentence.
The explanation is that the two predictors, log num words andweighted overlap, are highly significantly correlated in all three corpora.
Since weighted overlap is thepredictive of the two, the model discards article length.Finally, strict noun match, which requires that all nouns match between H and P, is assigned apositive coefficient for each language, but only reaches significance for Hindi.
This is the only genuinecross-lingual difference: In our Hindi corpus, the titles are copied more verbatim from the text than forEnglish and German (median weighted overlap: Hindi 0.76, English 0.72, German 0.69).
Consequently,in English and German the filter discards too many entailment instances.
For all three languages, though,the coefficient is small ?
for Hindi, where it is largest, it increases the odds by a factor of e0.39 ?
1.4.Evaluation.
We trained models on the three corpora, using only the two predictors that contributedsignificantly in all languages (weighted overlap and punctuation), in order to avoid overfitting on theindividual datasets.8 We applied each model to each dataset.
How such models should be evaluateddepends on the intended purpose of the classification.
We assume that it is fairly easy to obtain largecorpora of newspaper text, which makes precision an issue rather than recall.
The logistic regressionclassifier assigns a probability to each datapoint, so we can trade off recall and precision.
We fix recall ata reasonable value (30%) and compare precision values.6http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/Design7http://rocr.bioinf.mpi-sb.mpg.de/8Subsequent analysis of ?full?
models (with all features) showed that they did not generally improve over two-feature models.100PPPPPPPPDataModels German model English model Hindi modelGerman data 91.6 88.8 88.8English data 93.2 94.3 94.6Hindi data 98.7 98.7 99.1Table 4: Exp.
1: Precision for the class ?yes?
(entailment) at 30% RecallOur expectation is that each model will perform best on its own corpus (since this is basically thetraining data), and worse on the other languages.
The size of the drop for the other languages reflects thedifferences between the corpora as well as the degree of overfitting models show to their training data.The actual results are shown in Table 4.3.
The precision is fairly high, generally over 90%, and wellabove the baseline percentage of entailment pairs.
The German data is modelled best by the Germanmodel, with the two other models performing 3 percent worse.
The situation is similar, although lesspronounced, on Hindi data, where the Hindi-trained model is 0.4% better than the two other models.
ForEnglish, the Hindi model even outperforms the English model by 0.3%9, which in turn works about 1%better than the German model.
In sum, the logistic regression models can be applied very well acrosslanguages, with little loss in precision.
The German data with its high ratio of ill-formed headlines (cf.Table 2) is most difficult to model.
Hindi is simplest, due to the tendency of title and first sentence to bealmost identical (cf.
the large weight for the overlap predictor).5 Experiment 2: Analysis by Domain of German corpora5.1 DataThis experiment compares three German corpora from different newspapers to study the impact of domaindifferences: Reuters, ?Stuttgarter Zeitung?, and ?Die Zeit?.
These corpora differ in domain and in style.The Reuters corpus was already described in Section 4.1.
?Stuttgarter Zeitung?
(StuttZ) is a daily regionalnewspaper which covers international business and politics like Reuters, but does not draw its materialcompletely from large news agencies and gives more importance to regional and local events.
Its style istherefore less consistent.
Our corpus covers some 80,000 sentences of text from StuttZ.
The third corpuscomprises over 4 million sentences of text from ?Die Zeit?, a major German national weekly.
The text ispredominantly from the 2000s, plus selected articles from the 1940s through 1990s.
?Die Zeit?
focuses onop-ed pieces and general discussions of political and social issues.
It also covers arts and science, whichthe two other newspapers rarely do.5.2 Distribution of annotation categoriesWe extracted and annotated entailment pair candidates in the same manner as before (cf.
Section 4.1).The new breakdown of annotation categories in Table (10) shows, in comparison to the cross-lingualresults in Table 2, a higher incidence of errors, which we attribute to formatting problems of these corpora.Compared to the German Reuters corpus we considered in Exp.
1, StuttZ and Die Zeit contain considerablyfewer entailment pairs, most notably Die Zeit, where the percentage of entailment pairs is just 21.6% inour sample, compared to 82.3% for Reuters.
Notably, there are almost no cases where the first sentencerepresents a partial entailment; in contrast, for more than one third of the examples (33.9%), there is noentailment relation between the title and the first sentence.
This seems to be a domain-dependent, or evenstylistic, effect: in ?Die Zeit?, titles are often designed solely as ?bait?
to interest readers in the article:(10) P: Sat.1Sat.1sahwatched[...][...]DorisDorisdabei zu ,,wiehowsieshe[...][...]Auto fahrento drivelernte.learned.9The English model outperforms the Hindi model at higher recall levels, though.101Corpus err ill no-con no-emb no-oth no-par no-pre yesReuters 3.5 2.9 0 0.2 3.7 7.4 0 82.3StuttZ 6.2 3.6 0.5 2.8 12.4 3.0 0.6 70.7Die Zeit 2.3 39.0 0.5 1.8 33.9 0.9 0.0 21.6Table 5: Exp.
2: Distribution of annotation categories on German corpora (in percent)Predictor Reuters sig StuttZ sig Die Zeit sigweighted overlap -0.77 ** -1.82 *** -2.60 ***log num words -0.05 ?
-0.24 ?
-0.20 ?punctuation -1.04 *** -0.01 ?
-1.21 ***strict noun match -0.12 ?
-0.20 ?
-0.01 ?Table 6: Exp.
2: Predictors in the logreg model (*: p<0.05; **: p<0.01; ***: p<0.001)PPPPPPPPDataModels Reuters StuttZ Die ZeitReuters 91.6 85.4 91.6StuttZ 83.0 83.0 82.6Die Zeit 45.2 45.2 46.7Table 7: Exp.
2: Precision for the class ?yes?
at 30% recallH: Doris,Doris,esitistisgr?n!green!Other titles are just noun or verb phrases, which accounts for the large number (39%) of ill-formed pairs.5.3 Modelling the dataPredictors and evaluation.
The predictors of the logistic regression models for the three Germancorpora are shown in Table 6.
The picture is strikingly similar to the results of Exp.
1 (Table 3): weightedoverlap and punctuation are highly significant predictors for all three corpora (except punctuation, whichis insignificant for StuttZ); even the effect sizes are roughly similar.
Again, neither sentence lengthnor strict noun match are significant.
This indicates that the predictors we have identified work fairlyrobustly.
Unfortunately, this does not imply that they always work well.
Table 6 shows the precision ofthe predictors in Exp.
2, again at 30% Recall.
Here, the difference to Exp.
1 (Table 4.3) is striking.
First,overfitting of the predictors is worse across domains, with losses of 5% on Reuters and Die Zeit when theyare classified with models trained on other corpora even though use just two generic features.
Second, andmore seriously, it is much more difficult to extract entailment pairs from the Stuttgarter Zeitung corpusand, especially, the Die Zeit corpus.
For the latter, we can obtain a precision of at most 46.7%, comparedto >90% in Exp.
1.We interpret this result as evidence that domain adaptation may be an even greater challenge thanmultilinguality in the acquisition of entailment pairs.
More specifically, our impression is that the heuristicof pairing title and first sentence works fairly well for a particular segment of newswire text, but nototherwise.
This segment consists of factual, ?no-nonsense?
articles provided by large news agencies suchas Reuters, which tend to be simple in their discourse structure and have an informative title.
In domainswhere articles become longer, and the intent to entertain becomes more pertinent (as for Die Zeit), theheuristic fails very frequently.
Note that the weighted overlap predictor cannot recover all negative cases.Example (10) is a case in point: one of the two informative words in H, ?Doris?
and ?gr?n?, is in fact in P.Domain specificity.
The fact that it is difficult to extract entailment pairs from some corpora is seriousexactly because, according to our intuition, the ?easier?
news agency corpora (like Reuters) are domain-102Corpus D( ?
| deWac) words w with highest P (w)/Q(w)Reuters 0.98 H?ndler (trader), B?rse (exchange), Prozent (per cent), erkl?rte (stated)StuttZ 0.93 DM (German Mark), Prozent (per cent), Millionen (millions), Gesch?fts-jahr (fiscal year), Milliarden (billions)Die Zeit 0.64 hei?t (means), wei?
(knows), l?
?t (leaves/lets)Table 8: Exp.
2: Domain specificity (KL distance from deWac); typical content wordsspecific.
We quantify this intuition with an approach by Ciaramita and Baroni (2006), who proposeto model the representativeness of web-crawled corpora as the KL divergence between their Laplace-smoothed unigram distribution P and that of a reference corpus, Q (w ?
W are vocabulary words):D(P,Q) =?w?WP (w) log P (w)Q(w) (4)We use the deWac German web corpus (Baroni et al, 2009) as reference, making the idealizing assumptionthat it is representative for the German language.
We interpret a large distance from deWac as domainspecificity.
The results in Table 8 bear out our hypothesis: Die Zeit is less domain specific than StuttZ,which in turn is less specific than Reuters.
The table also lists the content words (nouns/verbs) that aremost typical for each corpus, i.e., which have the highest value of P (w)/Q(w).
The lists bolster theinterpretation that Reuters and StuttZ concentrate on the economical domain, while the typical terms ofDie Zeit show an argumentative style, but no obvious domain bias.
In sum, domain specificity is inverselycorrelated with the difficulty of extracting entailment pairs: from a representativity standpoint, we shoulddraw entailment pairs from Die Zeit.6 ConclusionIn this paper, we have discussed the robustness of extracting entailment pairs from the title and firstsentence of newspaper articles.
We have proposed a logistic regression model and have analysed itsperformance on two datasets that we have created: a cross-lingual one a cross-domain one.
Our cross-lingual experiment shows a positive result: despite differences in the distribution of annotation categoriesacross domains and languages, the predictors of all logistic regression models look remarkably similar.
Inparticular, we have found two predictors which are correlated significantly with entailment across (almost)all languages and domains.
These are (a), a tf/idf measure of word overlap between the title and the firstsentence; and (b), the presence of punctuation indicating that the title is not a single grammatical sentence.These predictors extract entailment pairs from newswire text at a precision of > 90%, at a recall of 30%,and represent a simple, cross-lingually robust method for entailment pair acquisition.The cross-domain experiment, however, forces us to qualify this positive result.
On two other Germancorpora from different newspapers, we see a substantial degradation of the model?s performance.
It mayseem surprising that cross-domain robustness is a larger problem than cross-lingual robustness.
Ourinterpretation is that the limiting factor is the degree to which the underlying assumption, namely thatfirst sentence entails the title, is true.
If the assumption is true only for a minority of sentences, ourpredictors cannot save the day.
This assumption holds well in the Reuters corpora, but less so for theother newspapers.
Unfortunately, we also found that the Reuters corpora are at the same time thematicallyconstrained, and therefore only of limited use for extracting a representative corpus of entailment pairs.
Asecond problem is that the addition of features we considered beyond the two mentioned above threatensto degrade the classifier due to overfitting, at least across domains.Given these limitation of the present headline-based approach, other approaches that are moregenerally applicable may need to be explored.
Entailment pairs have for example been extracted fromWikipedia (Bos et al, 2009).
Another direction is to build on methods to extract paraphrases fromcomparable corpora (Barzilay and Lee, 2003), and extend them to capture asymmetrical pairs, whereentailment holds in one, but not the other, direction.103Acknowledgments.
The first author would like to acknowledge the support of a WISE scholarshipgranted by DAAD (German Academic Exchange Service).ReferencesBaroni, M., S. Bernardini, A. Ferraresi, and E. Zanchetta (2009).
The wacky wide web: A collectionof very large linguistically processed web-crawled corpora.
Journal of Language Resources andEvaluation 43(3), 209?226.Barzilay, R. and L. Lee (2003).
Learning to paraphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of HLT/NAACL, Edmonton, AL, pp.
16?23.Bos, J., M. Pennacchiotti, and F. M. Zanzotto (2009).
Textual entailment at EVALITA 2009.
InProceedings of IAAI, Reggio Emilia.Bresnan, J., A. Cueni, T. Nikitina, and H. Baayen (2007).
Predicting the dative alternation.
In G. Bouma,I.
Kraemer, and J. Zwarts (Eds.
), Cognitive Foundations of Interpretation, pp.
69?94.
Royal NetherlandsAcademy of Science.Burger, J. and L. Ferro (2005).
Generating an entailment corpus from news headlines.
In Proceedings ofthe ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pp.
49?54.Ciaramita, M. and M. Baroni (2006).
A figure of merit for the evaluation of web-corpus randomness.
InProceedings of EACL, Trento, Italy, pp.
217?224.Dagan, I., O. Glickman, and B. Magnini (2006).
The PASCAL recognising textual entailment challenge.In Machine Learning Challenges, Volume 3944 of Lecture Notes in Computer Science, pp.
177?190.Springer.de Marneffe, M.-C., A. N. Rafferty, and C. D. Manning (2008).
Finding contradictions in text.
InProceedings of the ACL, Columbus, Ohio, pp.
1039?1047.Hickl, A., J. Williams, J. Bensley, K. Roberts, B. Rink, and Y. Shi (2006).
Recognizing textual entailmentwith LCC?s Groundhog system.
In Proceedings of the Second PASCAL Challenges Workshop.Manning, C. D., P. Raghavan, and H. Sch?tze (2008).
Introduction to Information Retrieval (1st ed.
).Cambridge University Press.Monz, C. and M. de Rijke (2001).
Light-weight entailment checking for computational semantics.
InProceedings of ICoS, Siena, Italy, pp.
59?72.Sammons, M., V. Vydiswaran, and D. Roth (2010).
?Ask Not What Textual Entailment Can Do for You...?.In Proceedings of ACL, Uppsala, Sweden, pp.
1199?1208.Schmid, H. (1995).
Improvements in part-of-speech tagging with an application to german.
In Proceedingsof the SIGDAT Workshop at ACL, Cambridge, MA.Xiao, Z., T. McEnery, P. Baker, and A. Hardie (2004).
Developing Asian language corpora: Standardsand practice.
In In Proceedings of the Fourth Workshop on Asian Language Resources, Sanya, China,pp.
1?8.104
