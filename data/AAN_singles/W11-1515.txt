Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 115?123,Portland, OR, USA, 24 June 2011. c?2011 Association for Computational LinguisticsAuthor Age Prediction from Text using Linear RegressionDong Nguyen Noah A. Smith Carolyn P. Rose?Language Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{dongn,nasmith,cprose}@cs.cmu.eduAbstractWhile the study of the connection betweendiscourse patterns and personal identificationis decades old, the study of these patterns us-ing language technologies is relatively recent.In that more recent tradition we frame authorage prediction from text as a regression prob-lem.
We explore the same task using threevery different genres of data simultaneously:blogs, telephone conversations, and online fo-rum posts.
We employ a technique from do-main adaptation that allows us to train a jointmodel involving all three corpora together aswell as separately and analyze differences inpredictive features across joint and corpus-specific aspects of the model.
Effective fea-tures include both stylistic ones (such as POSpatterns) as well as content oriented ones.
Us-ing a linear regression model based on shallowtext features, we obtain correlations up to 0.74and mean absolute errors between 4.1 and 6.8years.1 IntroductionA major thrust of research in sociolinguistics is tounderstand the connection between the way peo-ple use language and their community membership,where community membership can be construedalong a variety of dimensions, including age, gen-der, socioeconomic status and political affiliation.
Aperson is a member of a multiplicity of communi-ties, and thus the person?s identity and language areinfluenced by many factors.In this paper we focus on the relationship betweenage and language use.
Recently, machine learningmethods have been applied to determine the age ofpersons based on the language that they utter.
Stud-ies of the stylistic and content-based features thatpredict age or other personal characteristics yieldnew insights into the connection between discourseand identity.
However, that connection is known tobe highly contextual, such as whether the data werecollected synchronously or asynchronously, throughtyped or spoken interaction, or whether participantscan see one another or not.
Recent work in the areaof domain adaptation raises awareness about the ef-fect of contextual factors on the generality of textprediction models.Our first contribution to this literature is an in-vestigation of age prediction using a multi-corpusapproach.
We present results and analysis acrossthree very different corpora: a blog corpus (Schleret al, 2006), a transcribed telephone speech corpus(Cieri et al, 2004) and posts from an online forumon breast cancer.
By using the domain adaptationapproach of Daume?
III (2007), we train a model onall these corpora together and separate the globalfeatures from corpus-specific features that are asso-ciated with age.A second contribution is the investigation of ageprediction with age modeled as a continuous vari-able rather than as a categorical variable.
Mostprior research on age prediction has framed this as atwo-class or three-class classification problem (e.g.,Schler et al, 2006 and Garera and Yarowsky, 2009).In our work, modeling age as a continuous variableis interesting not only as a more realistic representa-tion of age, but also for practical benefits of jointmodeling of age across corpora since the bound-115aries for discretizing age into a categorical variablein prior work have been chosen heuristically and ina corpus-dependent way, making it hard to compareperformance across different kinds of data.In the remainder of the paper, we first discuss re-lated work and present and compare the differentdatasets.
We then outline our approach and results.We conclude with discussion and future work.2 Related workTime is an important factor in sociolinguistic analy-sis of language variation.
While a thorough reviewof this work is beyond the scope of this paper, Eckert(1996) gives an overview of the literature on age asa sociolinguistic variable.
Linguistic variation canoccur as an individual moves through life, or as a re-sult of changes in the community itself as it movesthrough time.
As an added complexity, Argamon etal.
(2007) found connections between language vari-ation and age and gender.
Features that were usedwith increasing age were also used more by malesfor any age.
Features that were used with decreas-ing age were used more by females.
In other work,the same features that distinguish male and femalewriting also distinguish non-fiction and fiction (Arg-amon et al, 2003).
Thus, the separate effects of age,time period, gender, topic, and genre may be diffi-cult to tease apart in naturalistic data where many ofthese variables are unknown.Recently, machine learning approaches have beenexplored to estimate the age of an author or speakerusing text uttered or written by the person.
Thishas been modeled as a classification problem, in asimilar spirit to sociolinguistic work where age hasbeen investigated in terms of differences in distri-butions of characteristics between cohorts.
In thesociolinguistic literature, cohorts such as these aredetermined either etically (arbitrary, but equal agespans such as decades) or emically (related to lifestage, such as adolescence etc.).
In machine learn-ing research, these cohorts have typically been deter-mined for practical reasons relating to distribution ofage groups within a corpus, although the boundariessometimes have also made sense from a life stageperspective.
For example, researchers have mod-eled age as a two-class classification problem withboundaries at age 40 (Garera and Yarowsky, 2009)or 30 (Rao et al, 2010).
Another line of work haslooked at modeling age estimation as a three-classclassification problem (Schler et al, 2006; Goswamiet al, 2009), with age groups of 13-17, 23-27 and33-42.
In addition to machine learning experiments,other researchers have published statistical analysesof differences in distribution related to age and lan-guage and have found similar patterns.As an example of one of these studies, Pen-nebaker and Stone (2003) analyzed the relationshipbetween language use and aging by collecting datafrom a large number of previous studies.
Theyused LIWC (Pennebaker et al, 2001) for analysis.They found that with increasing age, people tend touse more positive and fewer negative affect words,more future-tense and less past-tense, and fewerself-references.
Furthermore, a general pattern ofincreasing cognitive complexity was seen.
Barbieri(2008) uses key word analysis to analyze languageand age.
Two groups (15?25 and 35?60) were com-pared.
Analysis showed that younger speakers?
talkis characterized by slang and swear words, indica-tors of speaker stance and emotional involvement,while older people tend to use more modals.Age classification experiments have been con-ducted on a wide range of types of data, in-cluding blogs (Schler et al, 2006; Goswami etal., 2009), telephone conversations (Garera andYarowsky, 2009), and recently Twitter (Rao et al,2010).
Effective features were both content fea-tures (such as unigrams, bigrams and word classes)as well as stylistic features (such as part-of-speech,slang words and average sentence length).
Theseseparate published studies present some common-alities of findings.
However, based on these re-sults from experiments conducted on very differentdatasets, it is not possible to determine how gener-alizable the models are.
Thus, there is a need for aninvestigation of generalizability specifically in themodeling of linguistic variation related to age, whichwe present in this paper.Age classification from speech data has been ofinterest for many years.
Recently, age regression us-ing speech features has been explored (Spiegl et al,2009).
Spiegel?s system obtained a mean absoluteerror of approximately 10 years using support vec-tor regression.
Van Heerden et al (2010) explorecombining regression estimates to improve age clas-116sification.
As far as we are aware, we are the first topublish results from a regression model that directlypredicts age using textual features.3 Data descriptionWe explore three datasets with different characteris-tics.
The data was divided into a training, develop-ment and test set.
Statistics are listed in Table 1.3.1 Blog corpusIn August 2004 Schler et al (2006) crawled blogsfrom blogger.com.
Information such as gen-der and age were provided by the users in their re-spective profiles.
Users were divided into three agegroups, and each group had an equal number of fe-male and male bloggers.
In our experiments, ev-ery document consists of all posts from a particularblogger.3.2 Fisher telephone corpusThe Fisher corpus (Cieri et al, 2004) contains tran-scripts of telephone conversations.
People were ran-domly assigned to pairs, and for (almost) every per-son, characteristics such as gender and age wererecorded.
Furthermore, for each conversation a topicwas assigned.
The data was collected beginning De-cember 2002 and continued for nearly one year.
Inour experiments, we aggregate the data for each per-son.3.3 Breast cancer forumWe drew data from one of the most active online fo-rums for persons with breast cancer.1 All posts anduser profiles of the forum were crawled in January2011.
Only a small proportion of users had indicatedtheir age in their profile.
We manually annotated theage of approximately 200 additional users with lesscommon ages by looking manually at their posts.
Anauthor?s age can often be annotated because userstend to make references to their age when they intro-duce themselves or when telling their treatment his-tory (e.g., I was diagnosed 2 years ago when I wasjust 38).
Combining this with the date of the specificpost, a birth year can be estimated.
Because a per-son?s data can span multiple years, we aggregate allthe data per year for each person.
Each person was1http://community.breastcancer.orgFigure 1: Comparison of age frequency in datasets.0500100015002000250010 20 30 40 50 60 70 80 90FrequencyAgeBlogsFisherCancerassigned randomly to one of the data splits, to makesure all documents representing the same person ap-peared in only one split.
The dataset contains postsfrom October 2002 until January 2011.3.4 Dataset comparison and statisticsThe datasets differ in several respects: specificity(general topics versus breast cancer), modality of in-teraction (telephone conversations versus online fo-rum versus blog post), age distribution, and amountof data per person.
The blog and Fisher dataset con-tain approximately equal amounts of males and fe-males, while the breast cancer dataset is heavily bi-ased towards women.A comparison of the age distributions of the threecorpora is given in Figure 1.
The Fisher datasethas the most uniform distribution across the ages,while the blog data has a lot of young persons andthe breast cancer forum has a lot of older people.The youngest person in our dataset is 13 years oldand the oldest is 88.
Note that our blog corpus con-tains gaps between different age categories, whichis an artifact of the experimental approach used bythe people who released this dataset (Schler et al,2006).Because all datasets were created between 2002and 2011, we are less likely to observe results due tocohort effects (changes that occur because of collec-tive changes in culture, such as use of the Internet).117Table 1: Datasets statistics.Blogs Fisher CancerData #docs avg #tokens #docs avg #tokens #docs avg #tokens #personsTraining 9,660 13,042 5,957 3,409 2,330 22,719 1,269Development 4,830 13,672 2,977 3,385 747 32,239 360Test 4,830 13,206 2,980 3,376 797 26,952 3684 Experimental setup4.1 Linear regressionGiven an input vector x ?
Rm, where x1, .
.
.
, xmrepresent features (also called independent variablesor predictors), we find a prediction y?
?
R for the ageof a person y ?
R using a linear regression model:y?
= ?0 + x>?
where ?0 and ?
are the parame-ters to estimate.
Usually, the parameters are learnedby minimizing the sum of squared errors.
In orderto strive for a model with high explanatory value,we use a linear regression model with Lasso (alsocalled L1) regularization (Tibshirani, 1996).
Thisminimizes the sum of squared errors, but in additionadds a penalty term ?
?mj=1 |?j |.
?
is a constant andcan be found by optimizing over the developmentdata.
As a result, this method delivers sparse mod-els.
We use OWLQN to optimize the regularizedempirical risk (Andrew and Gao, 2007; Gao et al,2007).
We evaluate the models by reporting the cor-relation and mean absolute error (MAE).4.2 Joint modelTo discover which features are important acrossdatasets and which are corpus-specific, we train amodel on the data of all corpora using the featurerepresentation proposed by Daume?
III (2007).
Usingthis model, the original feature space is augmentedby representing each individual feature as 4 new fea-tures: a global feature and three corpus-specific fea-tures, specifically one for each dataset.
Thus for ev-ery feature f , we now have fglobal , fblogs , ffisher andfcancer .
For every instance, only the global and theone specific corpus feature are set.
For example fora particular feature value xj for the blog dataset wewould have ?xj , xj , 0, 0?.
If it would appear in thecancer dataset we would have ?xj , 0, 0, xj?.
Becausethe resulting model using L1 regression only selectsa small subset of the features, some features mayonly appear either as global features or as corpus-specific features in the final model.4.3 Overview different modelsBesides experimenting with the joint model, we arealso interested in the performance using only the dis-covered global features.
This can be achieved by ap-plying the weights for the global features directly aslearned by the joint model, or retraining the modelon the individual datasets using only the global fea-tures.
In summary, we have the following models:?
INDIV: Models trained on the three corpora in-dividually.?
JOINT: Model trained on all three corpora withfeatures represented as in Daume?
III (2007).?
JOINT-Global: Using the learned JOINTmodel but only keeping the global features.?
JOINT-Global-Retrained: Using the discov-ered global features by the JOINT model, butretrained on each specific dataset.4.4 Features4.4.1 Textual featuresWe explore the following textual features; all fea-tures are frequency counts normalized by the length(number of tokens) of the document.?
Unigrams.?
POS unigrams and bigrams.
Text is tagged us-ing the Stanford POS tagger (Toutanova et al,2003).?
LIWC (Pennebaker et al, 2001).
This is a wordcounting program that captures word classessuch as inclusion words (LIWC-incl: ?with,??and,?
?include,?
etc.
), causation words (LIWC-cause: ?because,?
?hence,?
etc.
), and stylis-tic characteristics such as percentage of wordslonger than 6 letters (LIWC-Sixltr).118Figure 2: Scatterplot of true and predicted age.-20-10010203040506070809010 20 30 40 50 60 70 80 90PredictedageTrue age4.4.2 GenderBecause the gender of a person also influenceshow age is reflected in a person?s text or speech (e.g.Argamon et al (2007) ), we add a binary feature forthe gender of the person (Male = 1, Female = 0).This feature is only known for the blog and Fisherdataset.
For the breast cancer dataset the gender isnot known, but we assume they are all women.5 Results and discussionAs discussed, we experiment with four differentmodels.
We explore three different feature sets: onlyunigrams, only POS, and the full feature set.
The re-sults are presented in Table 2.
The most importantfeatures using the JOINT model with the full featureset (condition 10) are presented in Table 3.5.1 Quantitative analysisOverall, similar performance is obtained on theFisher and blog datasets.
The highest correlationswere achieved on the Fisher dataset, with a best cor-relation of r = 0.742.
This gives an r2 value of0.551, indicating that 55% of the variance can beexplained by the model.
However, a higher meanabsolute error (MAE) was observed compared tothe blog dataset.
This may be caused by the largerspread in distribution of ages in the Fisher dataset.The lowest correlations were observed on the cancerdataset.
This is probably caused by the small amountof training instances, the noisy text, and the fact thatthe ages lie very close to each other.Overall, the joint model using all features per-formed best (condition 10).
In Figure 2 a plot ispresented that relates the true and predicted ages forthis condition.
We find that for the high ages thereare more instances with high errors, probably causedby the small amount of training data for the extremeages.We find the correlation metric to be very sensitiveto the amount of data.
For example, when comput-ing the correlation over the aggregated results of allcorpora, we get a much higher correlation (0.830),but the MAE (5.345) is closer to that computed overthe individual datasets.
However, the MAE is de-pendent on the age distributions in the corpus, whichcan be observed by contrasting the MAE on the runsof the Fisher and cancer dataset.
This thus suggeststhat these two measures are complementary and bothare useful as evaluation metrics for this task.For most experiments the joint models show im-provement over the individual models.
Returningto our question of generality, we can make severalobservations.
First, performance decreases signif-icantly when only using the global features (com-paring JOINT and JOINT-Global-retrained), con-firming that corpus-specific features are important.Second, learned weights of global features are rea-sonably generalizable.
When using the full featureset, retraining the global features on the corpora di-rectly only gives a slight improvement (e.g.
com-pare conditions 11 and 12).
Third, the bias term(?0) is very corpus-specific and has a big influenceon the MAE.
For example, when comparing condi-tions 11 and 12, the correlations are very similar butthe MAEs are much lower when the model is re-trained.
This is a result of adjusting the bias termto the specific dataset.
For example the bias term ofthe model trained on only the blog dataset is 22.45,compared to the bias of 46.11 when trained on thecancer dataset.In addition, we observe better performance in thecancer dataset when retraining the model using onlythe global features compared to the initial featureset.
This suggests that using the global featuresmight have been an effective method for feature se-lection to prevent overfitting on this small dataset.119Table 2: Results on the test set, reported with Pearson?s correlation (r) and mean absolute error (MAE).Blogs Fisher CancerID Model #Features r MAE r MAE r MAEUnigrams1 INDIV 56,440 0.644 4.236 0.715 7.145 0.426 7.0852 JOINT 56,440 0.694 4.232 0.723 7.066 0.530 6.5373 JOINT-Global 656 0.605 5.800 0.628 10.370 0.461 16.6324 JOINT-Global-retrained 656 0.658 4.409 0.675 7.529 0.498 6.797POS5 INDIV 4,656 0.519 5.095 0.553 8.635 0.150 7.6996 JOINT 4,656 0.563 4.899 0.549 8.657 0.035 8.4497 JOINT-Global 110 0.495 6.332 0.390 12.232 0.151 19.4548 JOINT-Global-retrained 110 0.519 5.095 0.475 9.187 0.150 7.699All features9 INDIV 61,416 0.699 4.144 0.731 6.926 0.462 6.94310 JOINT 61,416 0.696 4.227 0.742 6.835 0.535 6.54511 JOINT-Global 510 0.625 5.295 0.650 11.982 0.459 17.47212 JOINT-Global-retrained 510 0.629 4.633 0.651 7.862 0.490 6.8765.2 Feature analysisThe most important features using the JOINT modelwith the full feature set (condition 10) are presentedin Table 3.
Features associated with a young agehave a negative weight, while features associatedwith old age have a positive weight.
For almost allruns and evaluation metrics the full feature set givesthe best performance.
However, looking at the per-formance increase, we observe that the unigram onlybaseline gives strong results.
Overall, both stylisticas well as content features are important.
For con-tent features, we see that references to family (e.g.,?granddaughter?
versus ?son?)
as well as to dailylife (e.g., ?school?
versus ?job?)
are very predictive.Although the performance using only POS tagsis lower, reasonable correlations are obtained usingonly POS tags.
In Table 3 we see many POS featuresassociated with old age.
This is confirmed when an-alyzing the whole feature set selected by the JOINTmodel (condition 10).
In this model 510 features arenonzero, 161 of which are POS patterns.
Of these,43 have a negative weight, and 118 have a positiveweight.
This thus again suggests that old age is char-acterized more by syntactic effects than young age.Most important features are consistent with obser-vations from previous research.
For example, in theFisher dataset, similar to findings from classificationexperiments by Garera and Yarowsky (2009), theword ?well?
is most predictive of older age.
?Like?has the highest association with younger age.
Thisagrees with observations by Barbieri (2008).
Aswas also observed by others, ?just?
is highly associ-ated with young persons.
Consistent with literaturethat males generally ?sound older?
than they trulyare (Argamon et al, 2007, and others), our malespeaker feature has a high negative weight.
And, inagreement with previous observations, younger peo-ple use more swear words and negative emotions.The differences between the corpora are reflectedin the features that have the most weight.
The effec-tive features in the Fisher dataset are more typicalof conversational settings and effective features inthe cancer dataset are about being pregnant and hav-ing kids.
Features associated with the blog datasetare typical of the story telling nature of many blogposts.Comparing the extracted corpus-specific featureswith the features selected when training on the indi-vidual corpora, we do see evidence that the JOINTmodel separates general versus specific features.For example, the most important features associ-ated with young people in the cancer dataset whenonly training on the cancer dataset (condition 9)are: LIWC - Emoticons, LIWC - Pronoun, definitely,120Table 3: Most important features in the JOINT model with all features (condition 10).
(a) Features for younger people.Global Blogs Fisher Cancerlike -1.295 you -0.387 actually -0.457 LIWC-Emotic.
-0.188gender-male -0.539 went -0.310 mean -0.343 young -0.116LIWC-School -0.442 fun -0.216 everyone -0.273 history -0.092just -0.354 school -0.192 definitely -0.273 mom -0.087LIWC-Anger -0.303 but -0.189 mom -0.230 ultrasound -0.083LIWC-Cause -0.290 LIWC-Comma -0.152 student -0.182 kids -0.071mom -0.290 go -0.142 pretty -0.137 age -0.069so -0.271 POS-vbp nn -0.116 POS-lrb cd -0.135 mum -0.069definitely -0.263 thats -0.115 LIWC-Swear -0.134 POS-sym rrb -0.069LIWC-Negemo -0.256 well -0.112 huge -0.126 discharge -0.063(b) Features for older people.Global Blogs Fisher Canceryears 0.601 LIWC - Job 0.514 well 1.644 POS - dt 0.713POS - dt 0.485 son 0.267 LIWC - WC 0.855 POS - md vb 0.450LIWC - Incl 0.483 kids 0.228 POS - uh prp 0.504 POS - nn 0.369POS - prp vbp 0.337 years 0.178 retired 0.492 LIWC - Negate 0.327granddaughter 0.332 work 0.147 POS - prp vbp 0.430 POS - nn vbd 0.321grandchildren 0.293 wife 0.142 said 0.404 POS - nnp 0.304had 0.277 husband 0.137 POS - cc fw 0.358 us 0.287daughter 0.272 meds 0.112 son 0.353 all 0.266grandson 0.245 dealing 0.096 subject 0.319 good 0.248ah 0.243 weekend 0.094 POS - cc cc 0.316 POS - cc nn 0.222mom, mum, really, LIWC - Family, LIWC - Humans,thank, and she.
The difference in age distribution isreflected in the feature weights.
In the JOINT model,the bias term is 24.866.
Because most of the personsin the cancer dataset are older, the features associ-ated with young age in the cancer dataset have muchlower weights compared to the other datasets.Because our goal is to compare features acrossthe corpora, we have not exploited corpus-specificfeatures.
For example, thread or subforum featurescould be used for the breast cancer corpus, and forthe Fisher dataset, one could add features that ex-ploit the conversational setting of the data.5.3 ExamplesWe present examples of text of younger and olderpersons and connect them to the learned model.The examples are manually selected to illustratestrengths and weaknesses of the model.5.3.1 Younger peopleWe first present some examples of text by youngpersons.
The following is an example of a 17-yearold in the blog dataset, the system predicted this tobe from a 16.48-year-old:I can?t sleep, but this time I have schooltommorow, so I have to try I guess.
Myparents got all pissed at me today becauseI forgot how to do the homework [...].
Re-ally mad, I ended it pissing off my momand [...] NOTHING!
Damn, when I?m atmy cousin?s I have no urge to use the com-puter like I do here, [...].This example matches with important features de-termined by the system, containing references toschool and parents, and usage of swearing and angerwords.121The following are selected turns (T) by a 19-yearold (system prediction: 17.37 years) in a conversa-tion in the Fisher dataset.T: yeah it?s too i just just freaked out [...]T: that kinda sucks for themT: they were they were like going crazy[...]T: it?s like against some law to likeThe text has many informal words such as ?kinda?and well as many occurrences of the word ?like.
?This example is from a 19-year old from the can-cer dataset.
The system?s prediction was far off, es-timating an age of 35.48.Im very young and an athlete and I reallydo not want to look disfigured, especiallywhen I work so hard to be fit.
I know itsounds shallow, but Im young and hopeto [...] my husband one day :) [...] Mygrandmother died of breast cancer at 51,and my mother is currently dealing with acancerous tumor on her ovaries.Besides explicit references to being ?very young,?the text is much more formal than typical texts, mak-ing it a hard example.5.3.2 Older peopleThe following is a snippet from a 47-year-old(system prediction: 34.42 years) in the blog dataset.[...
]In the weeks leading up to this meet-ing certain of the managers repeatedly as-serted strong positions.
[...] their previous(irresponsible yet non-negotiable) opin-ions[...] Well, today?s my first Father?sday [...].
Bringing a child into this worldis quite a responsibility especially with allthe fears and challenges we face.
[...]This matches some important features such as ref-erences to jobs, as well as having kids.
The manyreferences to the word ?father?
in the whole textmight have confused the model.
The following areselected turns (T) by a 73-year old (system predic-tion: 73.26 years) in a conversation in the Fisherdataset.T: ah thoughts i?m retired right nowT: i i really can?t ah think of anyth- thinkof i would ah ah change considerably ahi?m i?m very i?ve been very happily mar-ried and i have ah three children and sixgrandchildrenT: yeah that?s right well i i think i would dothings more differently fair- fairly recentlythan a long time agoThis example contains references to being retiredand having grandchildren, as well as many usagesof ?ah?.
The following is an example of a 70-yearold (system prediction: 71.53 years) in the cancerdataset.[...]
I was a little bit fearful of havingsurgery on both sides at once (reductionand lift on the right, tissue expander onthe left) [...] On the good side, my sonand family live near the plastic surgeon?soffice and the hospital, [...], at least frommy son and my granddaughter [...]6 ConclusionWe presented linear regression experiments to pre-dict the age of a text?s author.
As evaluation metrics,we found correlation as well as mean absolute er-ror to be complementary and useful measures.
Weobtained correlations up to 0.74 and mean absoluteerrors between 4.1 and 6.8 years.
In three differentcorpora, we found both content features and stylis-tic features to be strong indicators of a person?s age.Even a unigram only baseline already gives strongperformance and many POS patterns are strong in-dicators of old age.
By learning jointly from all ofthe corpora, we were able to separate generally ef-fective features from corpus-dependent ones.AcknowledgmentsThe authors would like to thank the anonymous review-ers for feedback, Michael Heilman for the regressioncode, and other members of the ARK group for help run-ning the experiments.
This work was funded by NSFgrants CAREER IIS-1054319 to N.A.S.
and IIS-0968485to C.P.R.122ReferencesGalen Andrew and Jianfeng Gao.
2007.
Scalable train-ing of l1-regularized log-linear models.
In Proc.
ofICML.Shlomo Argamon, Moshe Koppel, Jonathan Fine, andAnat R. Shimoni.
2003.
Gender, genre, and writingstyle in formal written texts.
Text, 23(3):321?346.Shlomo Argamon, Moshe Koppel, James Pennebaker,and Jonathan Schler.
2007.
Mining the blogosphere:age, gender, and the varieties of self-expression.Federica Barbieri.
2008.
Patterns of age-based linguisticvariation in American English.
Journal of Sociolin-guistics, 12(1):58?88.Christopher Cieri, David Miller, and Kevin Walker.2004.
The Fisher corpus: a resource for the next gen-erations of speech-to-text.
In Proc.
of LREC, pages69?71.Hal Daume?
III.
2007.
Frustratingly easy domain adapta-tion.
In Proc.
of ACL.Penelope Eckert.
1996.
Age as a sociolinguistic variable.In The Handbook of Sociolinguistics.
Oxford: Black-well.Jianfeng Gao, Galen Andrew, Mark Johnson, andKristina Toutanova.
2007.
A comparative study of pa-rameter estimation methods for statistical natural lan-guage processing.
In Proc.
of ACL.Nikesh Garera and David Yarowsky.
2009.
Modeling la-tent biographic attributes in conversational genres.
InProc.
of ACL-IJCNLP.Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.2009.
Stylometric analysis of bloggers?
age and gen-der.
In Proc.
of ICWSM.James W. Pennebaker and Lori D. Stone.
2003.
Wordsof wisdom: Language use over the lifespan.
Journalof Personality and Social Psychology, 85:291?301.James W. Pennebaker, Roger J. Booth, and Martha E.Francis, 2001.
Linguistic Inquiry and Word Count(LIWC): A Computerized Text Analysis Program.Delip Rao, David Yarowsky, Abhishek Shreevats, andManaswi Gupta.
2010.
Classifying latent user at-tributes in Twitter.
In Proc.
of SMUC.Jonathan Schler, Moshe Koppel, Shlomo Argamon, andJames Pennebaker.
2006.
Effects of age and genderon blogging.
In Proceedings of the AAAI Spring Sym-posia on Computational Approaches to Analyzing We-blogs.Werner Spiegl, Georg Stemmer, Eva Lasarcyk, VaradaKolhatkar, Andrew Cassidy, Blaise Potard, StephenShum, Young Chol Song, Puyang Xu, Peter Beyer-lein, James Harnsberger, and Elmar No?th.
2009.
Ana-lyzing features for automatic age estimation on cross-sectional data.
In Proc.
of INTERSPEECH.Robert Tibshirani.
1996.
Regression shrinkage and se-lection via the lasso.
Journal of the Royal StatisticalSociety Series B (Methodological), 58(1):267?288.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Proc.
ofNAACL-HLT.Charl van Heerden, Etienne Barnard, Marelie Davel,Christiaan van der Walt, Ewald van Dyk, MichaelFeld, and Christian Muller.
2010.
Combining re-gression and classification methods for improving au-tomatic speaker age recognition.
In Proc.
of ICASSP.123
