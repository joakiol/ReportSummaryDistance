Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 401?408Manchester, August 2008Using Hidden Markov Random Fields to Combine Distributional andPattern-based Word ClusteringNobuhiro Kaji and Masaru KitsuregawaInstitute of Industrial Science, University of Tokyo4-6-1 Komaba, Meguro-ku, Tokyo 153-8505 Japan{kaji,kitsure}@tkl.iis.u-tokyo.ac.jpAbstractWord clustering is a conventional and im-portant NLP task, and the literature hassuggested two kinds of approaches to thisproblem.
One is based on the distribu-tional similarity and the other relies onthe co-occurrence of two words in lexico-syntactic patterns.
Although the two meth-ods have been discussed separately, it ispromising to combine them since they arecomplementary with each other.
This pa-per proposes to integrate them using hid-den Markov random fields and demon-strates its effectiveness through experi-ments.1 IntroductionWord clustering is a technique of grouping similarwords together, and it is important for various NLPsystems.
Applications of word clustering includelanguage modeling (Brown et al, 1992), text clas-sification (Baker and McCallum, 1998), thesaurusconstruction (Lin, 1998) and so on.
Furthermore,recent studies revealed that word clustering is use-ful for semi-supervised learning in NLP (Miller etal., 2004; Li and McCallum, 2005; Kazama andTorisawa, 2008; Koo et al, 2008).A well-known approach to grouping similarwords is to use distribution of contexts in whichtarget words appear.
It is founded on the hypothe-sis that similar words tend to appear in similar con-texts (Harris, 1968).
Based on this idea, some stud-ies proposed probabilistic models for word cluster-ing (Pereira et al, 1993; Li and Abe, 1998; Roothc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.et al, 1999; Torisawa, 2002).
Others proposeddistributional similarity measures between words(Hindle, 1990; Lin, 1998; Lee, 1999; Weeds et al,2004).
Once such similarity is defined, it is trivialto perform clustering.On the other hand, some researchers utilizedco-occurrence for word clustering.
The idea be-hind it is that similar words tend to co-occur incertain patterns.
Considerable efforts have beendevoted to measure word similarity based on co-occurrence frequency of two words in a window(Church and Hanks, 1989; Turney, 2001; Terra andClarke, 2003; Matsuo et al, 2006).
In addition tothe classical window-based technique, some stud-ies investigated the use of lexico-syntactic patterns(e.g., X or Y) to get more accurate co-occurrencestatistics (Chilovski and Pantel, 2004; Bollegala etal., 2007).These two approaches are complementary witheach other, because they are founded on differenthypotheses and utilize different corpus statistics.Consider to cluster a set of words based on the dis-tributional similarity.
It is likely that some wordsare difficult to cluster due to the data sparseness orsome other problems, while we can still expect thatthose words are correctly classified using patterns.This consideration leads us to combine distribu-tional and pattern-based word clustering.
In thispaper we propose to combine them using mixturemodels based on hidden Markov random fields.This model was originally proposed by (Basu etal., 2004) for semi-supervised clustering.
In semi-supervised clustering, the system is provided withsupervision in the form of pair-wise constraintsspecifying data points that are likely to belong tothe same cluster.
These constraints are directly in-corporated into the clustering process as a priorknowledge.
Our idea is to view the co-occurrence401of two words in lexico-syntactic patterns as con-straints, and incorporate them into distributionalword clustering.In summary, this paper discusses the problemof integrating multiple approaches for word clus-tering.
We consider that the clustering results areimproved if multiple approaches are successfullycombined and if they are complementary with eachother.
Our contribution is to provide a proba-bilistic framework for this problem.
Although ourproposal aims at combining the distributional andpattern-based approaches, it is also applicable tocombine other approaches like (Lin et al, 2003),as we will discuss in Section 5.4.2 Distributional ClusteringThis and next section describe distributional andpattern-based word clustering respectively.
Sec-tion 4 will explain how to combine them.2.1 Probabilistic modelIn distributional word clustering, similarity be-tween words (= nouns) is measured by the distribu-tion of contexts in which they appear.
As a context,verbs that appear in certain grammatical relationswith the target nouns are typically used.
Using thedistribution of such verbs, we can express a nounn by a feature vector ?(n):?
(n) = (fnv1, fnv2, ...fnvV)where fnvidenotes the frequency of noun-verbpair (n, vi), and V denotes the number of distinctverbs.
The basic idea of using the distribution forclustering is to group n and n?
together if?
(n) and?(n?)
are similar.Let us consider a soft clustering model.
We hy-pothesize that ?
(n) is a mixture of multinomial,and the probability of n is defined by1p(n) =Z?z=1p(z)p(?(n)|z)=Z?z=1?zfn!?vfnv!
?v?fnvvzwhere Z is the number of mixture components,?zis the mixing coefficient (?z?z= 1), fn=?vfnvis the total number of occurrence of n, and1We ignored p(fn) by assuming that it is independent ofhidden variables.
See (McCallum and Nigam, 1998) for detaildiscussion.
?vzis the parameter of the multinomial distribu-tion (?v?vz= 1).
In this model the hidden vari-ables can be interpreted as semantic class of nouns.Now consider a set of nouns n = {ni}Ni=1.
Letz = {zi}Ni=1be a set of hidden variables corre-sponding to n. Assuming that the hidden variablesare independent and niis also independent of othernouns given the hidden variables, the probability ofn is defined byp(n) =?zp(z)p(n|z)wherep(z) =N?i=1p(zi)p(n|z) =N?i=1p(ni|zi).Hereafter, we use p(n|z) instead of p(?
(n)|z) tokeep the notation simple.
p(n|z) is the conditionaldistribution on all nouns given all the hidden vari-ables, and p(z) is the prior distribution on the hid-den variables.
Computing the log-likelihood of thecomplete data (n, z), we foundlog p(n, z) =N?i=1log p(zi)p(ni|zi).
(1)2.2 Parameter estimationThe parameters can be estimated by the EM algo-rithm.
In the E-step, p(zi|ni) is computed basedon current parameters.
It is computed byp(zi= k|ni) =p(zi= k)p(ni|zi= k)?zp(z)p(ni|z)=?k?v?fnivvk?z?z?v?fnivvz.In the M-step, the parameters are re-estimated byusing the result of the E-step:??k=?
+?ifni?p(zi= k|ni)?V +?v?ifnivp(zi= k|ni)?k=?
+?ip(zi= k|ni)?Z +?z?ip(zi= z|ni)where ?
is a smoothing factor.2 Both steps arerepeated until a convergence criteria is satisfied.The important point to note is that the E-step canbe computed using the above equation because thehidden variables are independent.2?=1.0 in our experiment.402X ya Y X mo Y mo X to Y to X, Y nado(X or Y) (X and Y) (X and Y) (X, Y etc.
)Table 1: Four lexico-syntactic patterns, where Xand Y are extracted as co-occurring words.
Notethat ya, mo, and to are Japanese postpositions, andthey correspond to or or and in English.3 Pattern-based ClusteringA graph-based algorithm was employed in order tocluster words using patterns.3.1 Graph ConstructionWe first construct the graph in which verticesand edges correspond to words and their co-occurrences in patterns respectively (Figure 1).
Weemployed four lexico-syntactic patterns (Table 1)to extract co-occurrence of two words from cor-pus.
Note that we target Japanese in this paper al-though our proposal is independent of languages.The edges are weighted by the strength of co-occurrence that is computed by the Point-wise Mu-tual Information (PMI):PMI(ni, nj) = logf(ni, nj)f(?, ?
)f(ni, ?
)f(?, nj)where f(ni, nj) is the co-occurrence frequencyof two nouns, and ???
means summation over allnouns.
If PMI is less than zero, the edge is re-moved.3.2 Graph PartitioningAssuming that similar words tend to co-occur inthe lexico-syntactic patterns, it is reasonable toconsider that a dense subgraph is a good cluster(Figure 1).
Following (Matsuo et al, 2006), weexploit the Newman clustering (Newman, 2004) topartition the graph into such dense subgraphs.We start by describing Newman?s algorithm forunweighted graphs and we will generalize it toweighted graphs later.
The Newman clustering isan algorithm that divides a graph into subgraphsbased on connectivity.
Roughly speaking, it di-vides a graph such that there are a lot of edges be-tween vertices in the same cluster.
In the algorithmgoodness of clustering is measured by score Q:Q =?i(eii?
a2i)ramendumplingpastasteakJapan U.S.A.GermanyChinaFranceFigure 1: An example of the graph consisting oftwo dense subgraphs.whereeij=# of edges between two vertices in cluster i and j# of all edgesai=?keik.The term eijis the fraction of edges between clus-ter i and j. aiis the sum of eikover all clusters,and a2irepresents the expected number of fractionof edges within the cluster i when edges are givenat random.
See (Newman, 2004) for the detail.The Newman clustering optimizes Q in an ag-glomerative fashion.
At the beginning of the algo-rithm every vertex forms a singleton cluster, andwe repeatedly merge two clusters so that the joinresults in the largest increase in Q.
The change inQ when cluster i and j are merged is given by?Q = eij+ eji?
2aiaj= 2(eij?
aiaj).The above procedure is repeated until Q reacheslocal maximum.The algorithm can be easily generalized toweighted graphs by substituting ?sum of weightsof edges?
for ?# of edges?
in the definition of eij.The other part of the algorithm remains the same.4 Integration based on Hidden MarkovRandom FieldsThis section represents how to integrate the distri-bution and pattern for word clustering.4.1 Background and ideaClustering has long been discussed as an unsu-pervised learning problem.
In some applications,however, it is possible to provide some form ofsupervision by hand in order to improve the clus-tering result.
This motivated researchers to inves-tigate semi-supervised clustering, which uses notonly unlabeled data but supervision in the form ofpair-wise constraints (Basu et al, 2004).
In this403framework, the clustering system is provided witha set of pair-wise constraints specifying data pointsthat are likely to belong to the same cluster.
Theseconstraints are directly incorporated into the clus-tering process as a prior knowledge.Our idea is to view the co-occurrence of twowords in lexico-syntactic patterns as constraints,and incorporate them into the distributional clus-tering.
The rest of this section describes how to ex-tend the distributional clustering so as to incorpo-rate the constraints, and how to generate the con-straints using the patterns.4.2 Probabilistic modelLet C be a set of pair-wise constraints, and con-sider to incorporate the constraints into the distri-butional clustering (Section 2).
In what follows weassume each constraint ?i, j?
?
C represents thatziand zjare likely to have the same value, and it isassociated with a weight wij(> 0) correspondingto a penalty for constraint violation.It is easy to extend the distributional cluster-ing algorithm so as to incorporate the constraints.This is done by just changing the prior distributionon hidden variables p(z).
Following (Basu et al,2004), we construct the Markov random field onthe hidden variables so as to incorporate the con-straints.
The new prior distribution is defined asp(z) =N?i=1p(zi) ?1Gexp{???i,j??C?
(zi6= zj)wij}where ?(?)
is the delta function.
?
(zi6= zj) takesone if the constraint ?i, j?
is violated and otherwisezero.
G is the normalization factor of the Markovrandom field (the second term).By examining the log-likelihood of the completedata, we can see how violation of constraints is pe-nalized.
Using the new prior distribution, we getlog p(n, z) =N?i=1log p(zi)p(ni|zi)???i,j??C?
(zi6= zj)wij?
log G.The first term in the right-hand side is equal to thelog-likelihood of the multinomial mixture, namelyequation (1).
The second term can be interpretedas the penalty for constraint violation.
The lastterm is a constant.It is worth pointing out that the resulting algo-rithm makes a soft assignment and polysemouswords can belong to more than one clusters.4.3 Parameter estimationThe parameters are estimated by the EM algo-rithm.
The M-step is exactly the same as discussedin Section 2.2.
The problem is that the hidden vari-ables are no longer independent and the E-step re-quires the calculation ofp(zi|n) =?z?ip(z?i, zi|n)?
?z?ip(z?i, zi)p(n|z?i, zi)where z?imeans all hidden variables but zi.
Thecomputation of the above equation is intractablebecause the summation in it requires O(ZN?1) op-erations.Instead of exactly computing p(zi|n), we ap-proximate it by using the mean field approximation(Lange et al, 2005).
In the mean field approxima-tion, p(z|n) is approximated by a factorized distri-bution q(z), in which all hidden variables are inde-pendent:q(z) =N?i=1qi(zi).
(2)Using q(z) instead of p(z|n), computation of theE-step can be written as follows:p(zi|n) '?z?iq(z?i, zi) = qi(zi).
(3)The parameters of q(z) are determined such thatthe KL divergence between q(z) and p(z|n) isminimized.
In other words, the approximate dis-tribution q(z) is determined by minimizing?zq(z) logq(z)p(z|n)(4)under the condition that?kqi(zi= k) = 1for all i.
This optimization problem can be re-solved by introducing Lagrange multipliers.
Be-cause we cannot get the solution in closed form, aniterative method is employed.
Taking the deriva-tive of equation (4) with respect to a parameterqik= qi(zi= k) and setting it to zero, we getthe following updating formula:q(t+1)ik?
p(ni, k) exp{?
?j?Ni(1 ?
q(t)jk)wij} (5)404where Ni= {j|?i, j?
?
C} and q(t)ikis the value ofqikat t-th iteration.
The derivation of this formulais found in Appendix.4.4 Generation of constraintsIt is often pointed out that even small amounts ofmisspecified constraints significantly decrease theperformance of semi-supervised clustering.
Thisis because the error of misspecified constraints ispropagated to the entire transitive neighborhoodsof the constrained data (Nelson and Cohen, 2007).As an example, consider that we have two con-straints ?i, j?
and ?j, k?.
If the former is misspeci-fied one, the error propagate to k through j.To tackle this problem, we propose a techniqueto put an upper bound ?
on the size of the transitiveneighborhoods.
Our constraint generation processis as follows.
To begin with, we modified the New-man clustering so that the maximum cluster sizedoes not exceed ?.
This can be done by prohibit-ing such merge that results in larger cluster than?.
Given the result of the modified Newman clus-tering, it is straightforward to generate constraints.Constraints are generated between two nouns inthe same cluster if they co-occur in the lexico-syntactic patterns at least one time.
The penaltyfor constraint violation wijwas set to PMI(ni, nj).This procedure obviously ensures that the size ofthe transitive neighborhoods is less than ?.5 Experiments5.1 Data setsWe parsed 15 years of news articles by KNP3 soas to obtain data sets for the distributional andpattern-based word clustering (Table 2).
The num-ber of distinct nouns in total was 297,719.
Notethat, due to the computational efficiency, we re-moved such nouns that appeared less than 10 timeswith verbs and did not appear at all in the patterns.A test set was created using manually tailoredJapanese thesaurus (Ikehara et al, 1997).
We ran-domly selected 500 unambiguous nouns from 25categories (20 words for each category).5.2 BaselinesFor comparison we implemented the followingbaseline systems.?
The multinomial mixture (Section 2).?
The Newman clustering (Newman, 2004).3http://nlp.kuee.kyoto-u.ac.jp/nl-resource/nouns 208,934verbs 64,954noun-verb pairs 4,804,715nouns 245,465noun-noun pairs 633,302Table 2: Data sets statistics.
The first and secondrow shows the number of distinct words (and wordpairs) used for the distributional and pattern-basedword clustering respectively.?
Three K-means algorithms using differentdistributional similarity or dissimilarity mea-sures: cosine, ?-skew divergence (Lee,1999)4, and Lin?s similarity (Lin, 1998).?
The CBC algorithm (Lin and Pantel, 2002;Pantel and Lin, 2002).5.3 Evaluation procedureAll the nouns in the data set were clustered by theproposed and baseline systems.5 For the mixturemodels and K-means, the number of clusters wasset to 1,000.
The parameter ?
was set to 100.The result was assessed by precision and recallusing the test data.
The precision and recall werecomputed by the B-CUBED algorithm as follows(Bagga and Baldwin, 1998).
For each noun niinthe test data, precisioniand recalliare defined asprecisioni=|Si?
Ti||Si|recalli=|Si?
Ti|| Ti|where Siis the system generated cluster contain-ing niand Tiis the goldstandard cluster containingni.
The precision and recall are defined as an av-erage of precisioniand recallifor all the nouns inthe test data respectively.
The result of soft clus-tering models cannot be directly evaluated by theprecision and recall.
In such cases, each noun isassigned to the cluster that maximizes p(z|n).5.4 The result and discussionTable 3 shows the experimental results.
The bestresults for each statistic are shown in bold.
For themixture models and K-means, the precision and re-call are an average of 10 trials.Table 3 demonstrates the impact of combiningdistribution and pattern.
Our method outperformed4?
= 0.99 in our experiment.5Our implementation is available fromhttp://www.tkl.iis.u-tokyo.ac.jp/?kaji/clustering.405P R F1proposed .383 .437 .408multinomial mixture .360 .374 .367Newman (2004) .318 .353 .334cosine .603 .114 .192?-skew divergence (Lee, 1999) .730 .155 .255Lin?s similarity (Lin, 1998) .691 .096 .169CBC (Lin and Pantel, 2002) .981 .060 .114Table 3: Precision, recall, and F-measure.all the baseline systems.
It was statistically signif-icantly better than the multinomial mixture (P <0.01, Mann-Whitney U-test).
Note that it is possi-ble to improve some baseline systems, especiallyCBC, by tuning the parameters.
For CBC we sim-ply used the same parameter values as reported in(Lin and Pantel, 2002).Compared with the multinomial mixture, oneadvantage of our method is that it has broad cov-erage.
Our method can successfully handle un-known words, which do not appear with verbs atall (i.e., fn= 0 and ?
(n) is zero vector), if theyco-occur with other words in the lexico-syntacticpatterns.
For unknown words, the hidden variablesare determined based only on p(z) because p(n|z)takes the same value for all hidden variables.
Thismeans that our method clusters unknown wordsusing pair-wise constraints.
On the other hand,the multinomial mixture assigns all the unknownwords to the cluster that maximizes p(z).The test set included 51 unknown words.6 Wesplit the test set into two parts: fn= 0 and fn6= 0,and calculated precision and recall for each subset(Table 4).
Although the improvement is especiallysignificant for the unknown words, we can clearlyconfirm the improvement for both subsets.
For theNewman clustering we can discuss similar things(Table 5).
Different from the Newman clustering,our method can handle nouns that do not co-occurwith other nouns if 0 < fn.
In this case the test setincluded 64 unknown words.It is interesting to point out that our frameworkcan further incorporate lexico-syntactic patternsfor dissimilar words (Lin et al, 2003).
Namely,we can use patterns so as to prevent distribution-ally similar but semantically different words (e.g.,ally and supporter (Lin et al, 2003)) from being as-signed to the same cluster.
This can be achieved byusing cannot-link constraints, which specify datapoints that are likely to belong to different clus-6The baseline systems assigned the unknown words to adefault cluster as the multinomial mixture does.fn= 0 fn6= 0P R F1P R F1proposed .320 .632 .435 .412 .450 .430multi.
.099 1.000 .181 .402 .394 .398Table 4: Detail comparison with the multinomialmixture.f(ni, ?)
= 0 f(ni, ?)
6= 0P R F1P R F1proposed .600 .456 .518 .380 .479 .424Newman .071 1.000 .133 .354 .412 .381Table 5: Detail comparison with the Newman clus-tering.ters (Basu et al, 2004).
The remaining problemis which patterns to use so as to extract dissimilarwords.
Although this problem has already beendiscussed by (Lin et al, 2003), they mainly ad-dressed antonyms.
We believe that a more exhaus-tive investigation is required.
In addition, it is stillunclear whether dissimilar words are really usefulto improve clustering results.One problem that we did not examine is how todetermine optimal number of clusters.
In the ex-periment, the number was decided with trial-and-error through our initial experiment.
We leave itas our future work to test methods of automat-ically determining the cluster number (Pedersenand Kulkarni, 2006; Blei and Jordan, 2006).6 Related workAs far as we know, the distributional and pattern-based word clustering have been discussed inde-pendently (e.g., (Pazienza et al, 2006)).
One ofthe most relevant work is (Bollegala et al, 2007),which proposed to integrate various patterns in or-der to measure semantic similarity between words.Although they extensively discussed the use of pat-terns, they did not address the distributional ap-proach.Mirkin (2006) pointed out the importance ofintegrating distributional similarity and lexico-syntactic patterns, and showed how to combine thetwo approaches for textual entailment acquisition.Although their work inspired our research, we dis-cussed word clustering, which is related to but dif-ferent from entailment acquisition.Lin (2003) also proposed to use both distribu-tional similarity and lexico-syntactic patterns forfinding synonyms.
However, they present an oppo-site viewpoint from our research.
Their proposalis to exploit patterns in order to filter dissimilar406words.
As we have already discussed, the integra-tion of such patterns can also be formalized usingsimilar probabilistic model to ours.A variety of studies discussed determining po-larity of words.
Because this problem is ternary(positive, negative, and neutral) classification ofwords, it can be seen as one kind of word clus-tering.
The literature suggested two methods ofdetermining polarity, and they are analogous to thedistributional and co-occurrence-based approachesin word clustering (Takamura et al, 2005; Hi-gashiyama et al, 2008).
We consider it is alsopromising to integrate them for polarity determi-nation.7 ConclusionThe distributional and pattern-based word cluster-ing have long been discussed separately despitethe potentiality for their integration.
In this paper,we provided a probabilistic framework for com-bining the two approaches, and demonstrated thatthe clustering result is significantly improved.Our important future work is to extend currentframework so as to incorporate patterns for dissim-ilar words using cannot-link constraints.
We con-sider such patterns further improve the clusteringresult.Combining distribution and pattern is importantfor other NLP problems as well (e.g., entailmentacquisition, polarity determination).
Although thispaper examined word clustering, we consider apart of our idea can be applied to other problems.AcknowledgementThis work was supported by the ComprehensiveDevelopment of e-Society Foundation Softwareprogram of the Ministry of Education, Culture,Sports, Science and Technology, Japan.ReferencesBagga, Amit and Breck Baldwin.
1998.
Entity-basedcross-document coreferencing using the vector spacemodel.
In Proceedings of ACL, pages 79?85.Baker, L. Douglas and Andrew Kachites McCallum.1998.
Distributional clustering of words for textclassification.
In Proceedings of SIGIR, pages 96?103.Basu, Sugato, Mikhail Bilenko, and Raymond J.Mooney.
2004.
A probabilistic framework for semi-supervised clustering.
In Proceedings of SIGKDD,pages 59?68.Blei, David M. and Michael I. Jordan.
2006.
Vari-ational inference for Dirichlet process mixtures.Bayesian Analysis, 1(1):121?144.Bollegala, Danushka, Yutaka Matsuo, and MitsuruIshizuka.
2007.
An integrated approach to mea-suring semantic similarity between words using in-formation available on the web.
In Proceedings ofNAACL, pages 340?347.Brown, Peter F., Vincent J. Della Pietra, Peter V. deS-ouza, Jenifer C. Lai, and Rober L. Mercer.
1992.Class-based n-gram models of natural language.Computational Linguistics, 18(4):467?479.Chilovski, Timothy and Patrick Pantel.
2004.
VER-BOCEAN: Mining the web for fine-grained semanticverb relations.
In Proceedings of EMNLP, pages 33?40.Church, KennethWard and Patrick Hanks.
1989.
Wordassociation norms, mutual information, and lexicog-raphy.
In Proceedings of ACL, pages 76?83.Harris, Zellig.
1968.
Mathematical Structure of Lan-guage.
New York: Wiley.Higashiyama, Masahiko, Kentaro Inui, and Yuji Mat-sumoto.
2008.
Learning polarity of nouns by se-lectional preferences of predicates (in Japanese).
InProceedings of the Association for NLP, pages 584?587.Hindle, Donald.
1990.
Noun classification frompredicate-argument structure.
In Proceedings ofACL, pages 268?275.Ikehara, Satoru, Masahiro Miyazaki, Satoshi Shirai,Akio Yokoo, Hiromi Nakaiwa, Kentarou Ogura,and Yoshifumi Oyama Yoshihiko Hayashi, editors.1997.
Japanese Lexicon.
Iwanami Publishing.Kazama, Jun?ichi and Kentaro Torisawa.
2008.
Induc-ing gazetteers for named entity recognition by large-scale clustering of dependency relations.
In Pro-ceedings of ACL, pages 407?415.Koo, Terry, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proceedings of ACL, pages 595?603.Lange, Tilman, Martin H.C. Law, Anil K. Jain, andJoachim M. Buhmann.
2005.
Learning with con-strained and unlabelled data.
In Proceedings ofCVPR, pages 731?738.Lee, Lillian.
1999.
Measures of distributional similar-ity.
In Proceedings of ACL, pages 25?32.Li, Hang and Naoki Abe.
1998.
Word clustering anddisambiguation based on co-occurrence.
InProceed-ings of ACL-COLING, pages 749?755.Li, Wei and Andrew McCallum.
2005.
Semi-supervised sequence modeling with syntactic topicmodels.
In Proceedings of AAAI, pages 813?818.407Lin, Dekang and Patrick Pantel.
2002.
Concept discov-ery from text.
In Proceeodings of COLING, pages577?583.Lin, Dekang, Shaojun Zhao, Lijuan Qin, and MingZhou.
2003.
Identifying synonyms among distri-butionally similar words.
In Proceedings of IJCAI,pages 1492?1493.Lin, Dekang.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of ACL-COLING,pages 768?774.Matsuo, Yutaka, Takeshi Sakaki, Koki Uchiyama, andMitsuru Ishizuka.
2006.
Graph-based word cluster-ing using a web search engine.
In Proceedings ofEMNLP, pages 542?550.McCallum, Andrew and Kamal Nigam.
1998.
A com-parison of event models for naive Bayes text classifi-cation.
In Proceedings of AAAI Workshop on Learn-ing for Text Categorization, pages 41?48.Miller, Scott, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrim-inative training.
In Proceedings of NAACL, pages579?586.Mirkin, Shachar, Ido Dagan, andMaayan Geffet.
2006.Integrating pattern-based and distributional similar-ity methods for lexical entailment acquisition.
InProceedings of COLING-ACL Poster Sessions, pages579?586.Nelson, Blaine and Ira Cohen.
2007.
Revisiting prob-abilistic models for clustering with pair-wise con-straints.
In Proceedings of ICML, pages 673?680.Newman, Mark.
2004.
Fast algorithm for detectingcommunity structure in networks.
In Phys.
Rev.
E69.Pantel, Patrick and Dekang Lin.
2002.
Discoveringword senses from text.
In Proceedings of SIGKDD,pages 613?619.Pazienza, Maria Teresa, Marco Pennacchiotti, andFabio Massimo Zanzotto.
2006.
Discoveringverb relations in corpora: Distributional versusnon-distributional approaches.
In Proceedings ofIEA/AIE, pages 1042?1052.Pedersen, Ted and Anagha Kulkarni.
2006.
Automaticcluster stopping with criterion functions and the gapstatistic.
In Proceedings of HLT/NAACL, Compan-ion Volume, pages 276?279.Pereira, Fernando, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of english words.
InProceedings of ACL, pages 183?190.Rooth, Mats, Stefan Riezler, Detlef Prescher, GlennGarrroll, and Franz Beil.
1999.
Inducing a semanti-cally annotated lexicon via EM-based clustering.
InProceedings of ACL, pages 104?111.Takamura, Hiroya, Takashi Inui, and Manabu Oku-mura.
2005.
Extracting semantic orientations ofwords using spin model.
In Proceedings of ACL,pages 133?140.Terra, Egidio and C.L.A.
Clarke.
2003.
Frequency es-timates for statistical word similarity measures.
InProceedings of NAACL, pages 165?172.Torisawa, Kentaro.
2002.
An unsupervised learningmethod for associative relationships between verbphrases.
In Proceedings of COLING, pages 1009?1015.Turney, Peter.
2001.
Mining the web for synonyms:PMI-IR versus LSA on TOEFL.
In Proceedings ofECML, pages 491?502.Weeds, Julie, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributionalsimilarity.
In Proceedings of COLING, pages 1015?1021.Appendix.
Derivation of the updatingformulaWe can rewrite equation (4) as follows:(4) =?zq(z) log q(z) (6)?
?zq(z)N?i=1log p(ni, zi) (7)+?zq(z)??i,j??C?
(zi6= zj)wij(8)+ const (9)where we made use of the fact that log p(z|n) =log p(n|z)p(z) + const.
Taking the derivative ofequation (6), (7), and (8) with respect to qik, wefound?
(6)?qik= log qik+ const?
(7)?qik= ?
log p(ni, k) + const?(8)?qik=?z?iq(z?i)?j?Ni?
(zj6= k)wij+ const=?j?Ni?z?iq(z?i)?
(zj6= k)wij+ const=?j?Ni(1 ?
qjk)wij+ constwhere const denotes terms independent of k. Mak-ing use of these results, the updating formula canbe derived by taking the derivative of equation (4)with respect to qikand setting it to zero.408
