Evaluating Answers to Definition QuestionsEllen M. VoorheesNational Institute of Standards and TechnologyGaithersburg, MD 20899ellen.voorhees@nist.govAbstractThis paper describes an initial evaluation ofsystems that answer questions seeking defini-tions.
The results suggest that humans agreesufficiently as to what the basic concepts thatshould be included in the definition of a par-ticular subject are to permit the computation ofconcept recall.
Computing concept precision ismore problematic, however.
Using the lengthin characters of a definition is a crude approxi-mation to concept precision that is nonethelesssufficient to correlate with humans?
subjectiveassessment of definition quality.The TREC question answering track has sponsoreda series of evaluations of systems?
abilities to answerclosed class questions in many domains (Voorhees,2001).
Closed class questions are fact-based, short an-swer questions.
The evaluation of QA systems for closedclass questions is relatively simple because a responseto such a question can be meaningfully judged on a bi-nary scale of right/wrong.
Increasing the complexity ofthe question type even slightly significantly increases thedifficulty of the evaluation because partial credit for re-sponses must then be accommodated.The ARDA AQUAINT1 program is a research initia-tive sponsored by the U.S. Department of Defense aimedat increasing the kinds and difficulty of the questions au-tomatic systems can answer.
A series of pilot evaluationshas been planned as part of the research agenda of theAQUAINT program.
The purpose of each pilot is to de-velop an effective evaluation methodology for systemsthat answer a certain kind of question.
One of the firstpilots to be implemented was the Definitions Pilot, a pi-lot to develop an evaluation methodology for questionssuch as What is mold?
and Who is Colin Powell?.1See http:///www.ic-arda.org/InfoExploit/aquaint/index.html.This paper presents the results of the pilot evaluation.The pilot demonstrated that human assessors generallyagree on the concepts that should appear in the definitionfor a particular subject, and can find those concepts in thesystems?
responses.
Such judgments support the compu-tation of concept recall, but do not support concept pre-cision since it is not feasible to enumerate all conceptscontained within a system response.
Instead, the lengthof a response is used to approximate concept precision.An F-measure score combining concept recall and lengthis used as the final metric for a response.
Systems rankedby average F score correlate well with assessors?
subjec-tive opinions as to definition quality.1 The TaskThe systems?
task in the pilot was as follows.
For eachof 25 questions the system retrieved a list of text frag-ments such that each fragment was a component of thedefinition.
The list was assumed to be ordered such thatthe more important elements in the definition appearedearlier in the list.
There were no limits placed on ei-ther the length of an individual fragment or on the num-ber of items in a list, though systems knew they wouldbe penalized for retrieving extraneous information.
SixAQUAINT contractors submitted eight runs to the pilot.The eight runs are labeled A?H in the discussion below.The questions were developed by NIST assessors whosearched a set of news articles for definition targets.
Theresult of question development was a question phrased aseither ?Who is.
.
.
?
or ?What is.
.
.
?, plus their own defi-nition of the target.
In general, these definitions consistedof one or two paragraphs of English prose.2 Assessing System ResponsesEach system response was independently judged by twodifferent assessors.
In what follows, the ?author?
asses-sor is the assessor who originally created the question; the?other?
assessor is the second assessor to judge the ques-tion.
Each assessor performed two rounds of assessingper question.In the first round of assessing, the assessor assignedtwo scores to the response from a system.
One score wasfor the content of the response and the other for its or-ganization, with each score on a scale of 0?10.
A highcontent score indicated that the response contained mostof the information it should contain and little misleadinginformation.
A high organization score indicated the re-sponse ranked the more important information before theless important information, and contained little or no ir-relevant information.
The final score for a question was afunction of the organization and content scores, with thecontent score receiving much more emphasis.The ranking of systems when using the question au-thor to assign scores was FADEBGCH; the ranking wasFAEGDBHC when using scores assigned by the other as-sessor.
The final scores for the systems varied across as-sessors largely due to different interpretations of the or-ganization score.
Different assessors used different de-fault scores when there was only one entry in the sys-tem response; organization scores also appeared to bestrongly correlated with content scores.
Despite these dif-ferences, the judgments do provide some guidance as tohow a more quantitative scoring metric should rank sys-tems.
The assessors preferred the responses from systemF over those from system A, which in turn was preferredover the remainder of the systems.
Responses from sys-tems C and H were the least preferred.The goal of the second round of assessing was to sup-port a more quantitative evaluation of the system re-sponses.
In this round of assessing, an assessor first cre-ated a list of ?information nuggets?
about the target us-ing all the system responses and the question author?sdefinition.
An information nugget was defined as a factfor which the assessor could make a binary decision asto whether a response contained the nugget.
The asses-sor then decided which nuggets were vital?nuggets thatmust appear in a definition for that definition to be good.Finally, the assessor went through each of the system re-sponses and marked where each nugget appeared in theresponse.
If a system returned a particular nugget morethan once, it was marked only once.Figure 1 shows an example of how one response wasjudged for the question Who is Christopher Reeve?.
Theleft side of the figure shows the concept list developed bythe assessor, with vital concepts marked with a star.
Theright side of the figure shows a system response with theconcepts underlined and tagged with the concept number.In Figure 1, each list entry has at most one conceptmarked.
However, that was not generally the case.
Manylist entries contained multiple concepts while others con-tained none.
Thus, using the list entry as the unit for eval-uation is not sensible.
Instead, we should calculate mea-sures in terms of the concepts themselves.
Computingconcept recall is straightforward given these judgments;it is the ratio of the number of correct concepts retrievedto the number of concepts in the assessor?s list.
But thecorresponding measure of concept precision, the ratio ofthe number of correct concepts retrieved to the total num-ber of concepts retrieved, is problematic since the correctvalue for the denominator is unknown.
A trial evalua-tion prior to the pilot showed that assessors found enu-merating all concepts represented in a response to be sodifficult as to be unworkable.
For example, how manyconcepts are contained in ?stars on Sunday in ABC?s re-make of ?Rear Window??
Using only concept recall asthe final score is not workable either, since systems wouldnot be rewarded for being selective: retrieving the entiredocument collection would get a perfect score for everyquestion.Borrowing from the evaluation of summarization sys-tems (Harman and Over, 2002), we can use length asa (crude) approximation to precision.
A length-basedmeasure captures the intuition that users would preferthe shorter of two definitions that contain the same con-cepts.
The length-based measure used in the pilot givesa system an allowance of 100 (non-white-space) char-acters for each correct concept it retrieves.
The pre-cision score is set to one if the response is no longerthan this allowance.
If the response is longer than theallowance, the precision score is downgraded using thefunction precision   length  allowancelength .Remember that the assessors marked some concepts asvital and the remainder are not vital.
The non-vital con-cepts act as a ?don?t care?
condition.
That is, systemsshould be penalized for not retrieving vital concepts, andpenalized for retrieving items that are not on the asses-sor?s concept list at all, but should be neither penalizednor rewarded for retrieving a non-vital concept.
To imple-ment the don?t care condition, concept recall is computedonly over vital concepts, while the character allowance inthe precision computation is based on both vital and non-vital concepts.
The recall for the example in Figure 1 isthus 2/3, and the character allowance is 300.The final score for a response was computed using theF-measure, a function of both recall (R) and precision (P).The general version of the F-measure isF  RPPRwhereis a parameter signifying the relative importanceof recall and precision.
The main evaluation in the pi-lot used a value of 5, indicating that recall is 5 times asimportant as precision.
The value of 5 is arbitrary, but re-flects both the emphasis given to content in the first round1 * actor2 * accident3 * treatment/therapy4 spinal cord injury activist5 written an autobiography6 human embryo research activista) list of concepts  Actor   the actor who was paralyzed when he fell off his horse  the name attraction  stars on Sunday in ABC?s remake of ?Rear Window  was injured in a show jumping accident andhas become a spokesman for the cause b) system responseFigure 1: Assessor annotation of a sample response for Who is Christopher Reeve?author otherF 0.688 F 0.757A 0.606 A 0.687D 0.568 G 0.671G 0.562 D 0.669E 0.555 E 0.657B 0.467 B 0.522C 0.349 C 0.384H 0.330 H 0.365Table 1: Average F scores per system per assessor type.of assessing and acknowledges the crudeness of the pre-cision approximation.Table 1 gives the average F scores for the pilot runsas evaluated using both assessors?
judgments.
As can beseen from the table, the rankings of systems are stableacross different assessors in that the only difference inthe rankings are for two runs whose scores are extremelysimilar (D and G).
While the absolute value of the scoresis different when using different assessors, the magnitudeof the difference between scores is generally preserved.For example, there is a large gap between the scores forsystems F and A, and a much smaller gap for systems Cand H. The rankings also obey the ordering constraintssuggested by the first round of assessing.The different systems in the pilot took different ap-proaches to producing their definitions.
System H alwaysreturned a single text snippet as a definition.
System Breturned a set of complete sentences.
System G tended tobe relatively terse, while F and A were more verbose.
Theaverage length of a response for each system is A: 1121.2,B: 1236.5, C: 84.7, D: 281.8, E: 533.9, F: 935.6, G: 164.5,and H: 33.7.
The differences in the systems are reflectedin their relative scores when differentvalues are used.For example, when evaluated using  and the au-thors?
judgments, the system ranking is GFDAECHB; for   the ranking is GDFAECHB.
Thus as expected, asprecision gains in importance, system G rises in the rank-ings, system B falls quickly, and system F also sinks.3 ConclusionThe AQUAINT pilot evaluations are designed to explorethe issues surrounding new evaluation methodologies forquestion answering systems using a small set of systems.If a pilot is successful, the evaluation will be transferredto the much larger TREC QA track.
The definition pi-lot demonstrated that relative F scores based on conceptrecall and adjusted response length are stable when com-puted using different human assessor judgments, and re-flect intuitive judgments of quality.
The main measureused in the pilot strongly emphasized recall, but varyingthe F measure?sparameter allows different user prefer-ences to be accommodated as expected.
Definition ques-tions will be included as a part of the TREC 2003 QAtrack where they will be evaluated using this methodol-ogy.AcknowledgmentsThis paper was informed by the discussion that tookplace on the AQUAINT definition question mailing list.My thanks to the AQUAINT contractors who submittedresults to the pilot evaluation, and especially to RalphWeischedel and Dan Moldovan who coordinated the def-inition pilot.ReferencesDonna Harman and Paul Over.
2002.
The DUC sum-marization evaluations.
In Proceedings of the Interna-tional Conference on Human Language Technology.Ellen M. Voorhees.
2001.
The TREC question answer-ing track.
Journal of Natural Language Engineering,7(4):361?378.
