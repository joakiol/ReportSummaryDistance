Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 35?39,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsRelaxCor Participation in CoNLL Shared Task on Coreference ResolutionEmili Sapena, Llu?
?s Padro?
and Jordi Turmo?TALP Research CenterUniversitat Polite`cnica de CatalunyaBarcelona, Spain{esapena, padro, turmo}@lsi.upc.eduAbstractThis paper describes the participation ofRELAXCOR in the CoNLL-2011 sharedtask: ?Modeling Unrestricted Coreference inOntonotes?.
RELAXCOR is a constraint-basedgraph partitioning approach to coreferenceresolution solved by relaxation labeling.
Theapproach combines the strengths of groupwiseclassifiers and chain formation methods in oneglobal method.1 IntroductionThe CoNLL-2011 shared task (Pradhan et al, 2011)is concerned with intra-document coreference reso-lution in English, using Ontonotes corpora.
The coreof the task is to identify which expressions (usuallyNPs) in a text refer to the same discourse entity.This paper describes the participation of RELAX-COR and is organized as follows.
Section 2 de-scribes RELAXCOR, the system used in the task.Next, Section 3 describes the tuning needed by thesystem to adapt it to the task issues.
The same sec-tion also analyzes the obtained results.
Finally, Sec-tion 4 concludes the paper.2 System descriptionRELAXCOR (Sapena et al, 2010a) is a coreferenceresolution system based on constraint satisfaction.It represents the problem as a graph connecting any?Research supported by the Spanish Science and Innova-tion Ministry, via the KNOW2 project (TIN2009-14715-C04-04) and from the European Community?s Seventh FrameworkProgramme (FP7/2007-2013) under Grant Agreement number247762 (FAUST)pair of candidate coreferent mentions and applies re-laxation labeling, over a set of constraints, to decidethe set of most compatible coreference relations.This approach combines classification and cluster-ing in one step.
Thus, decisions are taken consider-ing the entire set of mentions, which ensures consis-tency and avoids local classification decisions.
TheRELAXCOR implementation used in this task is animproved version of the system that participated inthe SemEval-2010 Task 1 (Recasens et al, 2010).The knowledge of the system is represented as aset of weighted constraints.
Each constraint has anassociated weight reflecting its confidence.
The signof the weight indicates that a pair or a group of men-tions corefer (positive) or not (negative).
Only con-straints over pairs of mentions were used in the cur-rent version of RELAXCOR.
However, RELAXCORcan handle higher-order constraints.
Constraints canbe obtained from any source, including a trainingdata set from which they can be manually or auto-matically acquired.The coreference resolution problem is representedas a graph with mentions in the vertices.
Mentionsare connected to each other by edges.
Edges are as-signed a weight that indicates the confidence that themention pair corefers or not.
More specifically, anedge weight is the sum of the weights of the con-straints that apply to that mention pair.
The largerthe edge weight in absolute terms, the more reliable.RELAXCOR uses relaxation labeling for the res-olution process.
Relaxation labeling is an iterativealgorithm that performs function optimization basedon local information.
It has been widely used tosolve NLP problems.
An array of probability values35is maintained for each vertex/mention.
Each valuecorresponds to the probability that the mention be-longs to a specific entity given all the possible enti-ties in the document.
During the resolution process,the probability arrays are updated according to theedge weights and probability arrays of the neighbor-ing vertices.
The larger the edge weight, the strongerthe influence exerted by the neighboring probabilityarray.
The process stops when there are no morechanges in the probability arrays or the maximumchange does not exceed an epsilon parameter.2.1 Attributes and ConstraintsFor the present study, all constraints were learnedautomatically using more than a hundred attributesover the mention pairs in the training sets.
Usual at-tributes were used for each pair of mentions (mi,mj)?where i < j following the order of the document?, like those in (Sapena et al, 2010b), but bina-rized for each possible value.
In addition, a setof new mention attributes were included such asSAME SPEAKER when both mentions have thesame speaker1 (Figures 1 and 2).A decision tree was generated from the train-ing data set, and a set of constraints was extractedwith the C4.5 rule-learning algorithm (Quinlan,1993).
The so-learned constraints are conjunctionsof attribute-value pairs.
The weight associated witheach constraint is the constraint precision minus abalance value, which is determined using the devel-opment set.
Figure 3 is an example of a constraint.2.2 Training data selectionGenerating an example for each possible pair ofmentions produces an unbalanced dataset wheremore than 99% of the examples are negative (notcoreferent), even more considering that the mentiondetection system has a low precision (see Section3.1).
So, it generates large amounts of not coref-erent mentions.
In order to reduce the amount ofnegative pair examples, a clustering process is runusing the positive examples as the centroids.
Foreach positive example, only the negative exampleswith distance equal or less than a threshold d areincluded in the final training data.
The distance iscomputed as the number of different attribute values1This information is available in the column ?speaker?
ofthe corpora.Distance and position:Distance between mi and mj in sentences:DIST SEN 0: same sentenceDIST SEN 1: consecutive sentencesDIST SEN L3: less than 3 sentencesDistance between mi and mj in phrases:DIST PHR 0, DIST PHR 1, DIST PHR L3Distance between mi and mj in mentions:DIST MEN 0, DIST MEN L3, DIST MEN L10APPOSITIVE: One mention is in apposition with the other.I/J IN QUOTES: mi/j is in quotes or inside a NPor a sentence in quotes.I/J FIRST: mi/j is the first mention in the sentence.Lexical:STR MATCH: String matching of mi and mjPRO STR: Both are pronouns and their strings matchPN STR: Both are proper names and their strings matchNONPRO STR: String matching like in Soon et al (2001)and mentions are not pronouns.HEAD MATCH: String matching of NP headsTERM MATCH: String matching of NP termsI/J HEAD TERM: mi/j head matches with the termMorphological:The number of both mentions match:NUMBER YES, NUMBER NO, NUMBER UNThe gender of both mentions match:GENDER YES, GENDER NO, GENDER UNAgreement: Gender and number of both mentions match:AGREEMENT YES, AGREEMENT NO, AGREEMENT UNClosest Agreement: mi is the first agreement foundlooking backward from mj : C AGREEMENT YES,C AGREEMENT NO, C AGREEMENT UNI/J THIRD PERSON: mi/j is 3rd personI/J PROPER NAME: mi/j is a proper nameI/J NOUN: mi/j is a common nounANIMACY: Animacy of both mentions match (person, object)I/J REFLEXIVE: mi/j is a reflexive pronounI/J POSSESSIVE: mi/j is a possessive pronounI/J TYPE P/E/N: mi/j is a pronoun (p), NE (e) or nominal (n)Figure 1: Mention-pair attributes (1/2).inside the feature vector.
After some experimentsover development data, the value of d was assignedto 5.
Thus, the negative examples were discardedwhen they have more than five attribute values dif-ferent than any positive example.
So, in the end,22.8% of the negative examples are discarded.
Also,both positive and negative examples with distancezero (contradictions) are discarded.2.3 Development processThe current version of RELAXCOR includes a pa-rameter optimization process using the developmentdata sets.
The optimized parameters are balance andpruning.
The former adjusts the constraint weightsto improve the balance between precision and re-call as shown in Figure 4; the latter limits the num-ber of neighbors that a vertex can have.
Limiting36Syntactic:I/J DEF NP: mi/j is a definite NP.I/J DEM NP: mi/j is a demonstrative NP.I/J INDEF NP: mi/j is an indefinite NP.NESTED: One mention is included in the other.MAXIMALNP: Both mentions have the same NP parentor they are nested.I/J MAXIMALNP: mi/j is not included in any other NP.I/J EMBEDDED: mi/j is a noun and is not a maximal NP.C COMMANDS IJ/JI: mi/j C-Commands mj/i.BINDING POS: Condition A of binding theory.BINDING NEG: Conditions B and C of binding theory.I/J SRL ARG N/0/1/2/X/M/L/Z: Syntactic argument of mi/j .SAME SRL ARG: Both mentions are the same argument.I/J COORDINATE: mi/j is a coordinate NPSemantic:Semantic class of both mentions match(the same as (Soon et al, 2001))SEMCLASS YES, SEMCLASS NO, SEMCLASS UNOne mention is an alias of the other:ALIAS YES, ALIAS NO, ALIAS UNI/J PERSON: mi/j is a person.I/J ORGANIZATION: mi/j is an organization.I/J LOCATION: mi/j is a location.SRL SAMEVERB: Both mentions have a semantic rolefor the same verb.SRL SAME ROLE: The same semantic role.SAME SPEAKER: The same speaker for both mentions.Figure 2: Mention-pair attributes (2/2).DIST SEN 1 & GENDER YES & I FIRST &I MAXIMALNP & J MAXIMALNP &I SRL ARG 0 & J SRL ARG 0 &I TYPE P & J TYPE PPrecision: 0.9581Training examples: 501Figure 3: Example of a constraint.
It applies when the distancebetween mi and mj is exactly 1 sentence, their gender match,both are maximal NPs, both are argument 0 (subject) of theirrespective sentences, both are pronouns, and mi is not the firstmention of its sentence.
The final weight will be weight =precision?
balance.the number of neighbors reduces the computationalcost significantly and improves overall performancetoo.
Optimizing this parameter depends on proper-ties like document size and the quality of the infor-mation given by the constraints.The development process calculates a grid giventhe possible values of both parameters: from 0 to 1for balance with a step of 0.05, and from 2 to 14for pruning with a step of 2.
Both parameters wereempirically adjusted on the development set for theevaluation measure used in this shared task: the un-weighted average of MUC (Vilain et al, 1995), B3(Bagga and Baldwin, 1998) and entity-based CEAF(Luo, 2005).Figure 4: Development process.
The figure shows MUC?s pre-cision (red), recall (green), and F1 (blue) for each balance valuewith pruning adjusted to 6.3 CoNLL shared task participationRELAXCOR has participated in the CoNLL task inthe Closed mode.
All the knowledge required by thefeature functions is obtained from the annotationsof the corpora and no external resources have beenused with the exception of WordNet (Miller, 1995),gender and number information (Bergsma and Lin,2006) and sense inventories.
All of them are allowedby the task organization and available in their web-site.There are many remarkable features that makethis task different and more difficult but realisticthan previous ones.
About mention annotation, itis important to emphasize that singletons are not an-notated, mentions must be detected by the systemand the mapping between system and true mentionsis limited to exact matching of boundaries.
More-over, some verbs have been annotated as coreferingmentions.
Regarding the evaluation, the scorer usesthe modification of (Cai and Strube, 2010), unprece-dented so far, and the corpora was published very re-cently and there are no published results yet to use asreference.
Finally, all the preprocessed informationis automatic for the test dataset, carring out somenoisy errors which is a handicap from the point ofview of machine learning.Following there is a description of the mention de-tection system developed for the task and an analysisof the obtained results in the development dataset.373.1 Mention detection systemThe mention detection system extracts one mentionfor every NP found in the syntactic tree, one for ev-ery pronoun and one for every named entity.
Then,the head of every NP is determined using part-of-speech tags and a set of rules from (Collins, 1999).In case that some NPs share the same head, thelarger NP is selected and the rest discarded.
Also themention repetitions with exactly the same bound-aries are discarded.
In addition, nouns with capitalletters and proper names not included yet, that ap-pear two or more times in the document, are also in-cluded.
For instance, the NP ?an Internet business?is added as a mention, but also ?Internet?
itself isadded in the case that the word is found once againin the document.As a result, taking into account that just exactboundary matching is accepted, the mention detec-tion achieves an acceptable recall, higher than 90%,but a low precision (see Table 1).
The most typ-ical error made by the system is to include ex-tracted NPs that are not referential (e.g., predicativeand appositive phrases) and mentions with incorrectboundaries.
The incorrect boundaries are mainlydue to errors in the predicted syntactic column andsome mention annotation discrepancies.
Further-more, verbs are not detected by this algorithm, somost of the missing mentions are verbs.3.2 Results analysisThe results obtained by RELAXCOR can be foundin Tables 1 and 2.
Due to the lack of annotated sin-gletons, mention-based metrics B3 and CEAF pro-duce lower scores ?near 60% and 50% respectively?than the ones typically achieved with different anno-tations and mapping policies ?usually near 80% and70%.
Moreover, the requirement that systems useautomatic preprocessing and do their own mentiondetection increase the difficulty of the task which ob-viously decreases the scores in general.The measure which remains more stable on itsscores is MUC given that it is link-based and nottakes singletons into account anyway.
Thus, it is theonly one comparable with the state of the art rightnow.
The results obtained with MUC scorer show animprovement of RELAXCOR?s recall, a feature thatneeded improvement given the previous publishedMeasure Recall Precision F1Mention detection 92.45 27.34 42.20mention-based CEAF 55.27 55.27 55.27entity-based CEAF 47.20 40.01 43.31MUC 54.53 62.25 58.13B3 63.72 73.83 68.40(CEAFe+MUC+B3)/3 - - 56.61Table 1: Results on the development data setMeasure Recall Precision F1mention-based CEAF 53.51 53.51 53.51entity-based CEAF 44.75 38.38 41.32MUC 56.32 63.16 59.55B3 62.16 72.08 67.09BLANC 69.50 73.07 71.10(CEAFe+MUC+B3)/3 - - 55.99Table 2: Official test resultsresults with a MUCs recall remarkably low (Sapenaet al, 2010b).4 ConclusionThe participation of RELAXCOR to the CoNLLshared task has been useful to evaluate the systemusing data never seen before in a totally automaticcontext: predicted preprocessing and system men-tions.
Many published systems typically use thesame data sets (ACE and MUC) and it is easy to un-intentionally adapt the system to the corpora and notjust to the problem.
This kind of tasks favor com-parisons between systems with the same frameworkand initial conditions.The obtained performances confirm the robust-ness of RELAXCOR and a recall improvement.
Andthe overall performance seems considerably goodtaking into account the unprecedented scenario.However, a deeper error analysis is needed, speciallyin the mention detection system with a low precisionand the training data selection process which maybe discarding positive examples that could help im-proving recall.AcknowledgmentsThe research leading to these results has received funding from theEuropean Community?s Seventh Framework Programme (FP7/2007-2013) under Grant Agreement number 247762 (FAUST), and fromthe Spanish Science and Innovation Ministry, via the KNOW2 project(TIN2009-14715-C04-04).38ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In Proceedings of the Lin-guistic Coreference Workshop at LREC 98, pages 563?566, Granada, Spain.S.
Bergsma and D. Lin.
2006.
Bootstrapping path-basedpronoun resolution.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand the 44th annual meeting of the Association forComputational Linguistics, pages 33?40.
Associationfor Computational Linguistics.Jie Cai and Michael Strube.
2010.
Evaluation met-rics for end-to-end coreference resolution systems.
InProceedings of SIGDIAL, pages 28?36, University ofTokyo, Japan.M.
Collins.
1999.
Head-driven statistical models fornatural language parsing.
Ph.D. thesis, University ofPennsylvania.Xiaoqiang Luo.
2005.
On coreference resolution per-formance metrics.
In Proceedings of the Joint Con-ference on Human Language Technology and Empir-ical Methods in Natural Language Processing (HLT-EMNLP 2005, pages 25?32, Vancouver, B.C., Canada.G.A.
Miller.
1995.
WordNet: a lexical database for En-glish.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
Conll-2011 shared task: Modeling unrestrictedcoreference in ontonotes.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning (CoNLL 2011), Portland, Oregon,June.J.R.
Quinlan.
1993.
C4.5: Programs for Machine Learn-ing.
Morgan Kaufmann.Marta Recasens, Llu?
?s Ma`rquez, Emili Sapena,M.
Anto`nia Mart?
?, Mariona Taule?, Ve?roniqueHoste, Massimo Poesio, and Yannick Versley.
2010.Semeval-2010 task 1: Coreference resolution in multi-ple languages.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 1?8, Up-psala, Sweden, July.
Association for ComputationalLinguistics.Emili Sapena, Llu?
?s Padro?, and Jordi Turmo.
2010a.
AGlobal Relaxation Labeling Approach to CoreferenceResolution.
In Proceedings of 23rd International Con-ference on Computational Linguistics, COLING, Bei-jing, China, August.Emili Sapena, Llu?
?s Padro?, and Jordi Turmo.
2010b.
Re-laxCor: A Global Relaxation Labeling Approach toCoreference Resolution.
In Proceedings of the ACLWorkshop on Semantic Evaluations (SemEval-2010),Uppsala, Sweden, July.W.M.
Soon, H.T.
Ng, and D.C.Y.
Lim.
2001.
AMachine Learning Approach to Coreference Resolu-tion of Noun Phrases.
Computational Linguistics,27(4):521?544.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of the Sixth Message Understanding Conference(MUC-6), pages 45?52.39
