Proceedings of the 5th Workshop on Important Unresolved Matters, pages 25?32,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsExploiting Semantic Information for HPSG Parse SelectionSanae Fujita,?
Francis Bond,?
Stephan Oepen,?
Takaaki Tanaka??
{sanae,takaaki}@cslab.kecl.ntt.co.jp, ?
bond@ieee.org, ?
oe@ifi.uio.no?
NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation?
National Institute of Information and Communications Technology (Japan)?
University of Oslo, Department of Informatics (Norway)AbstractIn this paper we present a framework forexperimentation on parse selection usingsyntactic and semantic features.
Resultsare given for syntactic features, depen-dency relations and the use of semanticclasses.1 IntroductionIn this paper we investigate the use of semantic in-formation in parse selection.Recently, significant improvements have beenmade in combining symbolic and statistical ap-proaches to various natural language processingtasks.
In parsing, for example, symbolic grammarsare combined with stochastic models (Oepen et al,2004; Malouf and van Noord, 2004).
Much of thegain in statistical parsing using lexicalized modelscomes from the use of a small set of function words(Klein and Manning, 2003).
Features based on gen-eral relations provide little improvement, presum-ably because the data is too sparse: in the Penntreebank standardly used to train and test statisti-cal parsers stocks and skyrocket never appear to-gether.
However, the superordinate concepts capi-tal (?
stocks) and move upward (?
sky rocket) fre-quently appear together, which suggests that usingword senses and their hypernyms as features may beusefulHowever, to date, there have been few combina-tions of sense information together with symbolicgrammars and statistical models.
We hypothesizethat one of the reasons for the lack of success isthat there has been no resource annotated with bothsyntactic and semantic information.
In this paper,we use a treebank with both syntactic information(HPSG parses) and semantic information (sense tagsfrom a lexicon) (Bond et al, 2007).
We use this totrain parse selection models using both syntactic andsemantic features.
A model trained using syntacticfeatures combined with semantic information out-performs a model using purely syntactic informationby a wide margin (69.4% sentence parse accuracyvs.
63.8% on definition sentences).2 The Hinoki CorpusThere are now some corpora being built with thesyntactic and semantic information necessary to in-vestigate the use of semantic information in parseselection.
In English, the OntoNotes project (Hovyet al, 2006) is combining sense tags with the Penntreebank.
We are using Japanese data from the Hi-noki Corpus consisting of around 95,000 dictionarydefinition and example sentences (Bond et al, 2007)annotated with both syntactic parses and senses fromthe same dictionary.2.1 Syntactic AnnotationSyntactic annotation in Hinoki is grammar basedcorpus annotation done by selecting the best parse(or parses) from the full analyses derived by a broad-coverage precision grammar.
The grammar is anHPSG implementation (JACY: Siegel and Bender,2002), which provides a high level of detail, mark-ing not only dependency and constituent structurebut also detailed semantic relations.
As the gram-mar is based on a monostratal theory of grammar(HPSG: Pollard and Sag, 1994), annotation by man-ual disambiguation determines syntactic and seman-tic structure at the same time.
Using a grammar25helps treebank consistency ?
all sentences anno-tated are guaranteed to have well-formed parses.The flip side to this is that any sentences which theparser cannot parse remain unannotated, at least un-less we were to fall back on full manual mark-up oftheir analyses.
The actual annotation process usesthe same tools as the Redwoods treebank of English(Oepen et al, 2004).A (simplified) example of an entry is given in Fig-ure 1.
Each entry contains the word itself, its partof speech, and its lexical type(s) in the grammar.Each sense then contains definition and examplesentences, links to other senses in the lexicon (suchas hypernym), and links to other resources, suchas the Goi-Taikei Japanese Lexicon (Ikehara et al,1997) and WordNet (Fellbaum, 1998).
Each contentword of the definition and example sentences is an-notated with sense tags from the same lexicon.There were 4 parses for the definition sentence.The correct parse, shown as a phrase structure tree,is shown in Figure 2.
The two sources of ambigu-ity are the conjunction and the relative clause.
Theparser also allows the conjunction to combine \densha and 0 hito.
In Japanese, relative clausescan have gapped and non-gapped readings.
In thegapped reading (selected here), 0 hito is the subjectof ?U unten ?drive?.
In the non-gapped readingthere is some underspecified relation between themodifee and the verb phrase.
This is similar to thedifference in the two readings of the day he knewin English: ?the day that he knew about?
(gapped)vs ?the day on which he knew (something)?
(non-gapped).
Such semantic ambiguity is resolved byselecting the correct derivation tree that includes theapplied rules in building the tree (Fig 3).The semantic representation is Minimal Recur-sion Semantics (Copestake et al, 2005).
We sim-plify this into a dependency representation, furtherabstracting away from quantification, as shown inFigure 4.
One of the advantages of the HPSG signis that it contains all this information, making it pos-sible to extract the particular view needed.
In or-der to make linking to other resources, such as thesense annotation, easier predicates are labeled withpointers back to their position in the original sur-face string.
For example, the predicate densha n 1links to the surface characters between positions 0and 3:\.UTTERANCENPVP NPP VNPPPN CONJ N CASE-P V V\ ?
?
k ?U 2d 0densha ya jidousha o unten suru hitotrain or car ACC drive do person?U31 ?chauffeur?
: ?a person who drives a train or car?Figure 2: Syntactic View of the Definition of ?U31 untenshu ?chauffeur?e2:unknown<0:13>[ARG x5:_hito_n]x7:densha_n_1<0:3>[]x12:_jidousha_n<4:7>[]x13:_ya_p_conj<0:4>[LIDX x7:_densha_n_1,RIDX x12:_jidousha_n]e23:_unten_s_2<8:10>[ARG1 x5:_hito_n]e23:_unten_s_2<8:10>[ARG2 x13:_ya_p_conj]Figure 4: Simplified Dependency View of the Defi-nition of ?U31 untenshu ?chauffeur?2.2 Semantic AnnotationThe lexical semantic annotation uses the sense in-ventory from Lexeed (Kasahara et al, 2004).
Allwords in the fundamental vocabulary are taggedwith their sense.
For example, the wordd& ookii?big?
(of example sentence in Figure 1) is tagged assense 5 in the example sentence, with the meaning?elder, older?.The word senses are further linked to semanticclasses in a Japanese ontology.
The ontology, Goi-Taikei, consists of a hierarchy of 2,710 semanticclasses, defined for over 264,312 nouns, with a max-imum depth of 12 (Ikehara et al, 1997).
We showthe top 3 levels of the Goi-Taikei common noun on-tology in Figure 5.
The semantic classes are prin-cipally defined for nouns (including verbal nouns),although there is some information for verbs and ad-jectives.3 Parse SelectionCombining the broad-coverage JACY grammar andthe Hinoki corpus, we build a parse selection modelon top of the symbolic grammar.
Given a set of can-26????????????????
?INDEX ?U3 untenshuPOS nounSENSE 1???????????
?DEFINITION[\1 ?
?
1 k?U1 2d04 a person who drives trains and cars]EXAMPLE[d&(5 C<8b\1 G?U31 Dod6 G%?3 2I dream of growing up and becoming a train driver]HYPERNYM 04 hito ?person?SEM.
CLASS ?292:driver?
(?
?4:person?
)WORDNET motorman1????????????????????????????
?Figure 1: Dictionary Entry for ?U31 untenshu ?chauffeur?frag-nprel-cl-sbj-gaphd-complement noun-lehd-complement v-lighthd-complementhd-complement case-p-acc-lenoun-le conj-le noun-le vn-trans-le v-light-le\ ?
?
k ?U 2d 0densha ya jidousha o unten suru hitotrain or car ACC drive do person?U31 ?chauffeur?
: ?a person who drives a train or car?Figure 3: Derivation Tree of the Definition of ?U31 untenshu ?chauffeur?Phrasal nodes are labeled with identifiers of grammar rules, and (pre-terminal) lexical nodes with class names for types of lexicalentries.Lvl 0 Lvl 1 Lvl 2 Lvl 3humanagent organizationfacilityconcreteplace regionnatural placeobject animateinanimateabstractthingmental statenoun actionhuman activityevent phenomenonnatural phen.abstractexistencesystemrelationshippropertyrelation stateshapeamountlocationtimeFigure 5: Top 3 levels of the GoiTaikei Ontologydidate analyses (for some Japanese string) accordingto JACY, the goal is to rank parse trees by their prob-ability: training a stochastic parse selection modelon the available treebank, we estimate statistics ofvarious features of candidate analyses from the tree-bank.
The definition and selection of features, thus,is a central parameter in the design of an effectiveparse selection model.3.1 Syntactic FeaturesThe first model that we trained uses syntactic fea-tures defined over HPSG derivation trees as summa-rized in Table 1.
For the closely related purpose ofparse selection over the English Redwoods treebank,Toutanova et al (2005) train a discriminative log-linear model, using features defined over derivationtrees with non-terminals representing the construc-tion types and lexical types of the HPSG grammar.The basic feature set of our parse selection modelfor Japanese is defined in the same way (correspond-ing to the PCFG-S model of Toutanova et al (2005)):each feature capturing a sub-tree from the deriva-27# sample features1 ?0 rel-cl-sbj-gap hd-complement noun-le?1 ?1 frag-np rel-cl-sbj-gap hd-complement noun-le?1 ?2 ?
frag-np rel-cl-sbj-gap hd-complement noun-le?2 ?0 rel-cl-sbj-gap hd-complement?2 ?0 rel-cl-sbj-gap noun-le?2 ?1 frag-np rel-cl-sbj-gap hd-complement?2 ?1 frag-np rel-cl-sbj-gap noun-le?3 ?1 conj-le ya?3 ?2 noun-le conj-le ya?3 ?3  noun-le conj-le ya?4 ?1 conj-le?4 ?2 noun-le conj-le?4 ?3  noun-le conj-le?Table 1: Example structural features extracted fromthe derivation tree in Figure 3.
The first columnnumbers the feature template corresponding to eachexample; in the examples, the first integer valueis a parameter to feature templates, i.e.
the depthof grandparenting (types #1 and#2) or n-gram size(types #3 and #4).
The special symbols ?
and denote the root of the tree and left periphery of theyield, respectively.tion limited to depth one.
Table 1 shows examplefeatures extracted from our running example (Fig-ure 3 above) in our MaxEnt models, where the fea-ture template #1 corresponds to local derivation sub-trees.
We will refer to the parse selection model us-ing only local structural features as SYN-1.3.1.1 Dominance FeaturesTo reduce the effects of data sparseness, featuretype #2 in Table 1 provides a back-off to deriva-tion sub-trees, where the sequence of daughters isreduced to just the head daughter.
Conversely, tofacilitate sampling of larger contexts than just sub-trees of depth one, feature template #1 allows op-tional grandparenting, including the upwards chainof dominating nodes in some features.
In our ex-periments, we found that grandparenting of up tothree dominating nodes gave the best balance of en-larged context vs. data sparseness.
Enriching our ba-sic model SYN-1 with these features we will hence-forth call SYN-GP.3.1.2 N-Gram FeaturesIn addition to these dominance-oriented featurestaken from the derivation trees of each parse tree,our models also include more surface-oriented fea-tures, viz.
n-grams of lexical types with or withoutlexicalization.
Feature type #3 in Table 1 definesn-grams of variable size, where (in a loose anal-ogy to part-of-speech tagging) sequences of lexicaltypes capture syntactic category assignments.
Fea-ture templates #3 and #4 only differ with regard tolexicalization, as the former includes the surface to-ken associated with the rightmost element of eachn-gram (loosely corresponding to the emission prob-abilities in an HMM tagger).
We used a maximumn-gram size of two in the experiments reported here,again due to its empirically determined best overallperformance.3.2 Semantic FeaturesIn order to define semantic parse selection features,we use a reduction of the full semantic representa-tion (MRS) into ?variable-free?
elementary depen-dencies.
The conversion centrally rests on a notionof one distinguished variable in each semantic rela-tion.
For most types of relations, the distinguishedvariable corresponds to the main index (ARG0 in theexamples above), e.g.
an event variable for verbal re-lations and a referential index for nominals.
Assum-ing further that, by and large, there is a unique re-lation for each semantic variable for which it servesas the main index (thus assuming, for example, thatadjectives and adverbs have event variables of theirown, which can be motivated in predicative usagesat least), an MRS can be broken down into a set ofbasic dependency tuples of the form shown in Fig-ure 4 (Oepen and L?nning, 2006).All predicates are indexed to the position of theword or words that introduced them in the input sen-tence (<start:end>).
This allows us to link themto the sense annotations in the corpus.3.2.1 Basic Semantic DependenciesThe basic semantic model, SEM-Dep, consists offeatures based on a predicate and its arguments takenfrom the elementary dependencies.
For example,consider the dependencies for densha ya jidousha-wo unten suru hito ?a person who drives a train orcar?
given in Figure 4.
The predicate unten ?drive?has two arguments: ARG1 hito ?person?
and ARG2jidousha ?car?.From these, we produce several features (See Ta-ble 2).
One has all arguments and their labels (#20).We also produce various back offs: #21 introduces28# sample features20 ?0 unten s ARG1 hito n 1 ARG2 ya p conj?20 ?0 ya p conj LIDX densha n 1 RIDX jidousha n 1?21 ?1 unten s ARG1 hito n 1?21 ?1 unten s ARG2 jidousha n 1?21 ?1 ya p conj LIDX densha n 1?21 ?1 ya p conj RIDX jidousha n 1?22 ?2 unten s hito n 1 jidousha n 1?23 ?3 unten s hito n 1?23 ?3 unten s jidousha n 1?.
.
.Table 2: Example semantic features (SEM-Dep) ex-tracted from the dependency tree in Figure 4.only one argument at a time, #22 provides unlabeledrelations, #23 provides one unlabeled relation at atime and so on.Each combination of a predicate and its relatedargument(s) becomes a feature.
These resemble thebasic semantic features used by Toutanova et al(2005).
We further simplify these by collapsingsome non-informative predicates, e.g.
the unknownpredicate used in fragments.3.2.2 Word Sense and Semantic ClassDependenciesWe created two sets of features based only on theword senses.
For SEM-WS we used the sense anno-tation to replace each underspecified MRS predicateby a predicate indicating the word sense.
This usedthe gold standard sense tags.
For SEM-Class, we usedthe sense annotation to replace each predicate by itsGoi-Taikei semantic class.In addition, to capture more useful relationships,conjunctions were followed down into the left andright daughters, and added as separate features.
Thesemantic classes for \1densha ?train?
and  ?1jidousha ?car?
are both ?988:land vehicle?,while ?U1 unten ?drive?
is ?2003:motion?
and04 hito ?person?is ?4:human?.
The sample featuresof SEM-Class are shown in Table 3.These features provide more specific information,in the case of the word sense, and semantic smooth-ing in the case of the semantic classes, as words arebinned into only 2,700 classes.3.2.3 Superordinate Semantic ClassesWe further smooth these features by replacing thesemantic classes with their hypernyms at a givenlevel (SEM-L).
We investigated levels 2 to 5.
Pred-# sample features40 ?0 unten s ARG1 C4 ARG2 C988?40 ?1 C2003 ARG1 C4 ARG2 C988?40 ?1 C2003 ARG1 C4 ARG2 C988?40 ?0 ya p conj LIDX C988 RIDX C988?41 ?2 unten s ARG1 C4?41 ?2 unten s ARG2 C988?.
.
.Table 3: Example semantic class features (SEM-Class).icates are binned into only 9 classes at level 2, 30classes at level 3, 136 classes at level 4, and 392classes at level 5.For example, at level 3, the hypernym classfor ?988:land vehicle?
is ?706:inanimate?,?2003:motion?
is ?1236:human activity?and ?4:human?
is unchanged.
So we used?706:inanimate?
and ?1236:human activity?to make features in the same way as Table 3.An advantage of these underspecified semanticclasses is that they are more robust to errors in wordsense disambiguation ?
fine grained sense distinc-tions can be ignored.3.2.4 Valency Dictionary CompatabilityThe last kind of semantic information we use isvalency information, taken from the Japanese sideof the Goi-Taikei Japanese-English valency dictio-nary as extended by Fujita and Bond (2004).This va-lency dictionary has detailed information about theargument properties of verbs and adjectives, includ-ing subcategorization and selectional restrictions.
Asimplified entry of the Japanese side for ?U2dunten-suru ?drive?
is shown in Figure 6.Each entry has a predicate and several case-slots.Each case-slot has information such as grammaticalfunction, case-marker, case-role (N1, N2, .
.
. )
andsemantic restrictions.
The semantic restrictions aredefined by the Goi-Taikei?s semantic classes.On the Japanese side of Goi-Taikei?s valencydictionary, there are 10,146 types of verbs giving18,512 entries and 1,723 types of adjectives giving2,618 entries.The valency based features were constructed byfirst finding the most appropriate pattern, and thenrecording how well it matched.To find the most appropriate pattern, we extractedcandidate dictionary entries whose lemma is the29PID:300513?
N1 <4:people> "%" ga?
N2 <986:vehicles> "k" o?
?U2d unten-suruFigure 6: ?U2dunten-suru ?N1 drive N2?.PID is the verb?s Pattern ID# sample features31 ?0 High?31 ?1 300513 High?31 ?2 2?31 ?3 R:High?31 ?4 300513 R:High?32 ?1 unten s High?32 ?4 unten s R:High?33 ?5 N1 C High?33 ?7 C?.
.
.Table 4: Example semantic features (SP)same as the predicate in the sentence: for exam-ple we look up all entries for ?U2d unten-suru ?drive?.
Then, for each candidate pattern, wemapped its arguments to the target predicate?s ar-guments via case-markers.
If the target predicatehas no suitable argument, we mapped to comitativephrase.
Finally, for each candidate patterns, we cal-culate a matching score1 and select the pattern whichhas the best score.Once we have the most appropriate pattern,we then construct features that record how goodthe match is (Table 4).
These include: the to-tal score, with or without the verb?s Pattern ID(High/Med/Low/Zero: #31 0,1), the number of filledarguments (#31 2), the fraction of filled argumentsvs all arguments (High/Med/Low/Zero: #31 3,4),the score for each argument of the pattern (#32 5)and the types of matches (#32 5,7).These scores allow us to use information aboutword usage in an exisiting dictionary.4 Evaluation and ResultsWe trained and tested on a subset of the dictionarydefinition and example sentences in the Hinoki cor-pus.
This consists of those sentences with ambigu-ous parses which have been annotated so that the1The scoring method follows Bond and Shirai (1997), anddepends on the goodness of the matches of the arguments.number of parses has been reduced (Table 5).
Thatis, we excluded unambiguous sentences (with a sin-gle parse), and those where the annotators judgedthat no parse gave the correct semantics.
This doesnot necessarily mean that there is a single correctparse, we allow the annotator to claim that two ormore parses are equally appropriate.Corpus # Sents Length Parses/Sent(Ave) (Ave)Definitions Train 30,345 9.3 190.1Test 2,790 10.1 177.0Examples Train 27,081 10.9 74.1Test 2,587 10.4 47.3Table 5: Data of Sets for EvaluationDictionary definition sentences are a differentgenre to other commonly used test sets (e.g news-paper text in the Penn Treebank or travel dialoguesin Redwoods).
However, they are valid examplesof naturally occurring texts and a native speaker canread and understand them without special training.The main differences with newspaper text is thatthe definition sentences are shorter, contain morefragments (especially NPs as single utterances) andfewer quoting and proper names.
The main differ-ences with travel dialogues is the lack of questions.4.1 A Maximum Entropy RankerLog-linear models provide a very flexible frame-work that has been widely used for a range of tasksin NLP, including parse selection and reranking formachine translation.
We use a maximum entropy/ minimum divergence (MEMD) modeler to trainthe parse selection model.
Specifically, we use theopen-source Toolkit for Advanced DiscriminativeModeling (TADM:2 Malouf, 2002) for training, us-ing its limited-memory variable metric as the opti-mization method and determining best-performingconvergence thresholds and prior sizes experimen-tally.
A comparison of this learner with the useof support vector machines over similar data foundthat the SVMs gave comparable results but were farslower (Baldridge and Osborne, 2007).
Because weare investigating the effects of various different fea-tures, we chose the faster learner.2http://tadm.sourceforge.net30Method Definitions ExamplesAccuracy Features Accuracy Features(%) (?1000) (%) (?1000)SYN-1 52.8 7 67.6 8SYN-GP 62.7 266 76.0 196SYN-ALL 63.8 316 76.2 245SYN baseline 16.4 random 22.3 randomSEM-Dep 57.3 1,189 58.7 675+SEM-WS 56.2 1,904 59.0 1,486+SEM-Class 57.5 2,018 59.7 1,669+SEM-L2 60.3 808 62.9 823+SEM-L3 59.8 876 62.8 879+SEM-L4 59.9 1,000 62.3 973+SEM-L5 60.4 1,240 61.3 1,202+SP 59.1 1,218 68.2 819+SEM-ALL 62.7 3,384 69.1 2,693SYN-SEM 69.5 2,476 79.2 2,126SEM baseline 20.3 random 22.8 randomTable 6: Parse Selection Results4.2 ResultsThe results for most of the models discussed in theprevious section are shown in Table 6.
The accuracyis exact match for the entire sentence: a model getsa point only if its top ranked analysis is the same asan analysis selected as correct in Hinoki.
This is astricter metric than component based measures (e.g.,labelled precision) which award partial credit for in-correct parses.
For the syntactic models, the base-line (random choice) is 16.4% for the definitions and22.3% for the examples.
Definition sentences areharder to parse than the example sentences.
Thisis mainly because they have fewer relative clausesand coordinate NPs, both large sources of ambigu-ity.
For the semantic and combined models, multiplesentences can have different parses but the same se-mantics.
In this case all sentences with the correctsemantics are scored as good.
This raises the base-lines to 20.3 and 22.8% respectively.Even the simplest models (SYN-1 and SEM-Dep)give a large improvement over the baseline.
Addinggrandparenting to the syntactic model has a largeimprovement (SYN-GP), but adding lexical n-gramsgave only a slight improvement over this (SYN-ALL).The effect of smoothing by superordinate seman-tic classes (SEM-Class), shows a modest improve-ment.
The syntactic model already contains a back-off to lexical-types, we hypothesize that the seman-tic classes behave in the same way.
Surprisingly, aswe add more data, the very top level of the seman-tic class hierarchy performs almost as well as the++ ++ + + + + + ++bcbcbcbcbcbcbcbcbcbcbcldldldldldldldldldldld0 20 40 60 80 100203040506070% of training data (30,345 sentences)Sent.Accuracy SYN-SEMSEM-ALLSYN-ALLFigure 7: Learning Curves (Definitions)more detailed levels.
The features using the valencydictionary (SP) also provide a considerable improve-ment over the basic dependencies.Combining all the semantic features (SEM-ALL)provides a clear improvement, suggesting that theinformation is heterogeneous.
Finally, combing thesyntactic and semantic features gives the best resultsby far (SYN-SEM: SYN-ALL + SEM-Dep + SEM-Class +SEM-L2 + SP).
The definitions sentences are hardersyntactically, and thus get more of a boost from thesemantics.
The semantics still improve performancefor the example sentences.The semantic class based sense features used hereare based on manual annotation, and thus show anupper bound on the effects of these features.
Thisis not an absolute upper bound on the use of senseinformation ?
it may be possible to improve furtherthrough feature engineering.
The learning curves(Fig 7) have not yet flattened out.
We can still im-prove by increasing the size of the training data.5 DiscussionBikel (2000) combined sense information and parseinformation using a subset of SemCor (with Word-Net senses and Penn-II treebanks) to produce a com-bined model.
This model did not use semantic de-pendency relations, but only syntactic dependen-cies augmented with heads, which suggests that thedeeper structural semantics provided by the HPSGparser is important.
Xiong et al (2005) achievedonly a very minor improvement over a plain syntac-tic model, using features based on both the corre-lation between predicates and their arguments, andbetween predicates and the hypernyms of their argu-ments (using HowNet).
However, they do not inves-tigate generalizing to different levels than a word?simmediate hypernym.31Pioneering work by Toutanova et al (2005) andBaldridge and Osborne (2007) on parse selection foran English HPSG treebank used simpler semanticfeatures without sense information, and got a far lessdramatic improvement when they combined syntac-tic and semantic information.The use of hand-crafted lexical resources such asthe Goi-Taikei ontology is sometimes criticized onthe grounds that such resources are hard to produceand scarce.
While it is true that valency lexiconsand sense hierarchies are hard to produce, they areof such value that they have already been created forall of the languages we know of which have largetreebanks.
In fact, there are more languages withWordNets than large treebanks.In future work we intend to confirm that we canget improved results with raw sense disambiguationresults not just the gold standard annotations and testthe results on other sections of the Hinoki corpus.6 ConclusionsWe have shown that sense-based semantic featurescombined with ontological information are effec-tive for parse selection.
Training and testing onthe definition subset of the Hinoki corpus, a com-bined model gave a 5.6% improvement in parse se-lection accuracy over a model using only syntacticfeatures (63.8% ?
69.4%).
Similar results (76.2%?
79.2%) were found with example sentences.ReferencesJason Baldridge and Miles Osborne.
2007.
Active learning andlogarithmic opinion pools for HPSG parse selection.
NaturalLanguage Engineering, 13(1):1?32.Daniel M. Bikel.
2000.
A statistical model for parsing andword-sense disambiguation.
In Proceedings of the Joint SIG-DAT Conference on Empirical Methods in Natural LanguageProcessing and Very Large Corpora, pages 155?163.
HongKong.Francis Bond, Sanae Fujita, and Takaaki Tanaka.
2007.
The Hi-noki syntactic and semantic treebank of Japanese.
LanguageResources and Evaluation.
(Special issue on Asian languagetechnology).Francis Bond and Satoshi Shirai.
1997.
Practical and efficientorganization of a large valency dictionary.
In Workshop onMultilingual Information Processing ?
Natural LanguageProcessing Pacific Rim Symposium ?97: NLPRS-97.
Phuket.Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag.2005.
Minimal Recursion Semantics.
An introduction.
Re-search on Language and Computation, 3(4):281?332.Christine Fellbaum, editor.
1998.
WordNet: An Electronic Lex-ical Database.
MIT Press.Sanae Fujita and Francis Bond.
2004.
A method of creatingnew bilingual valency entries using alternations.
In GillesSe?rasset, editor, COLING 2004 Multilingual Linguistic Re-sources, pages 41?48.
Geneva.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes: The90% solution.
In Proceedings of the Human LanguageTechnology Conference of the NAACL, Companion Volume:Short Papers, pages 57?60.
Association for ComputationalLinguistics, New York City, USA.
URL http://www.aclweb.org/anthology/N/N06/N06-2015.Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, AkioYokoo, Hiromi Nakaiwa, Kentaro Ogura, YoshifumiOoyama, and Yoshihiko Hayashi.
1997.
Goi-Taikei ?A Japanese Lexicon.
Iwanami Shoten, Tokyo.
5 vol-umes/CDROM.Kaname Kasahara, Hiroshi Sato, Francis Bond, TakaakiTanaka, Sanae Fujita, Tomoko Kanasugi, and ShigeakiAmano.
2004.
Construction of a Japanese semantic lexicon:Lexeed.
In IPSG SIG: 2004-NLC-159, pages 75?82.
Tokyo.
(in Japanese).Dan Klein and Christopher D. Manning.
2003.
Accurate un-lexicalized parsing.
In Erhard Hinrichs and Dan Roth, edi-tors, Proceedings of the 41st Annual Meeting of the Associ-ation for Computational Linguistics, pages 423?430.
URLhttp://www.aclweb.org/anthology/P03-1054.pdf.Robert Malouf.
2002.
A comparison of algorithms for maxi-mum entropy parameter estimation.
In CONLL-2002, pages49?55.
Taipei, Taiwan.Robert Malouf and Gertjan van Noord.
2004.
Wide cover-age parsing with stochastic attribute value grammars.
InIJCNLP-04 Workshop: Beyond shallow analyses - For-malisms and statistical modeling for deep analyses.
JSTCREST.
URL http://www-tsujii.is.s.u-tokyo.ac.jp/bsa/papers/malouf.pdf.Stephan Oepen, Dan Flickinger, Kristina Toutanova, andChristoper D. Manning.
2004.
LinGO redwoods: A rich anddynamic treebank for HPSG.
Research on Language andComputation, 2(4):575?596.Stephan Oepen and Jan Tore L?nning.
2006.
Discriminant-based MRS banking.
In Proceedings of the 5th InternationalConference on Language Resources and Evaluation (LREC2006).
Genoa, Italy.Carl Pollard and Ivan A.
Sag.
1994.
Head Driven Phrase Struc-ture Grammar.
University of Chicago Press, Chicago.Melanie Siegel and Emily M. Bender.
2002.
Efficient deep pro-cessing of Japanese.
In Proceedings of the 3rd Workshop onAsian Language Resources and International Standardiza-tion at the 19th International Conference on ComputationalLinguistics, pages 1?8.
Taipei.Kristina Toutanova, Christopher D. Manning, Dan Flickinger,and Stephan Oepen.
2005.
Stochastic HPSG parse disam-biguation using the redwoods corpus.
Research on Languageand Computation, 3(1):83?105.Deyi Xiong, Qun Liu Shuanglong Li and, Shouxun Lin, andYueliang Qian.
2005.
Parsing the Penn Chinese treebankwith semantic knowledge.
In Robert Dale, Jian Su Kam-FaiWong and, and Oi Yee Kwong, editors, Natural LanguageProcessing ?
IJCNLP 005: Second International Joint Con-ference Proceedings, pages 70?81.
Springer-Verlag.32
