Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 180?191,Paris, October 2009. c?2009 Association for Computational LinguisticsEffective Analysis of Causes and Inter-dependencies of Parsing ErrorsTadayoshi Hara1 Yusuke Miyao11Department of Computer Science, University of TokyoHongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, JAPAN2School of Computer Science, University of Manchester3NaCTeM (National Center for Text Mining){harasan,yusuke,tsujii}@is.s.u-tokyo.ac.jpJun?ichi Tsujii1,2,3AbstractIn this paper, we propose two methods foranalyzing errors in parsing.
One is to clas-sify errors into categories which grammardevelopers can easily associate with de-fects in grammar or a parsing model andthus its improvement.
The other is todiscover inter-dependencies among errors,and thus grammar developers can focus onerrors which are crucial for improving theperformance of a parsing model.The first method uses patterns of er-rors to associate them with categories ofcauses for those errors, such as errors inscope determination of coordination, PP-attachment, identification of antecedent ofrelative clauses, etc.
On the other hand,the second method, which is based on re-parsing with one of observed errors cor-rected, assesses inter-dependencies amongerrors by examining which other errorswere to be corrected as a result if a spe-cific error was corrected.Experiments show that these two meth-ods are complementary and by being com-bined, they can provide useful clues as tohow to improve a given grammar.1 IntroductionIn any kind of complex systems, analyzing causesof errors is a crucial step for improving its perfor-mance.
In recent sophisticated parsing technolo-gies, the step of error analysis has been becomingmore and more convoluted and time-consuming,if not impossible.
While common performanceevaluation measures such as F-values are useful tocompare the performance of systems or evaluateimprovement of a system, they hardly give usefulclues as to how to improve a system.
Evaluationmeasures usually assume uniform units such as thenumber of correctly or incorrectly recognized con-stituent boundaries and their labels, or in a similarvein, dependency links among words and their la-bels, and then compute single values such as the F-value.
These values do not give any insights as towhere the weaknesses exist in a parsing model.
Asa result, the improvement process takes the formof time consuming trial-error cycles.Once grammar developers know the actual dis-tribution of errors across different categories suchas PP-attachment, complement/adjunct distinc-tion, gerund/participle distinction, etc., they canthink of focused and systematic improvement ofa parsing model.Another problem of the F-value in terms ofuniform units is that it does not take inter-dependencies among errors into consideration.
Inparticular, for parsers based on grammar for-malisms such as LFG (Kaplan and Bresnan, 1995),HPSG (Pollard and Sag, 1994), or CCG (Steed-man, 2000), units (eg.
single predicate-argumentlinks) are inter-related through hierarchical struc-tures and structure sharing assumed by these for-malisms.
Single errors are inherently propagatedto other sets of errors.
This is also the case, thoughto a lesser extent, for parsing models in whichshallow parsing is followed by another componentfor semantic label assignment.In order to address these two issues, we proposetwo methods in this paper.
One is to recognizecause categories of errors and the other is to cap-ture inter-dependencies among errors.
The formermethod defines various patterns of errors to iden-tify categories of error causes.
The latter methodre-parses a sentence with a single target error cor-rected, and regards the errors which are correctedin re-parse as errors dependent on the target.Although these two methods are implementedfor a specific parser using HPSG (Miyao and Tsu-jii, 2005; Ninomiya et al, 2006), the same ideascan be applied to any type of parsing models.180PredicateSentence: John    has    comePredicative event 1:Predicative event 2:Word:  hasGrammatical nature:  auxiliary# of arguments:  2Argument 1 JohnArgument 2 comePredicateWord:  comeGrammatical nature:  verb# of arguments:  1Argument 1 JohnPredicate-argument relationsFigure 1: Predicate-argument relationsJohn aux_2argsARG1 ARG2verb_1argARG1has : come :Figure 2: Representation of predicate-argumentrelationsIn the following, Section 2 introduces a parserand its evaluation metrics, Section 3 illustrates dif-ficulties in analyzing parsing errors based on com-mon evaluation measures, and Section 4 proposesthe two methods for effective error analysis.
Sec-tion 5 presents experimental results which showhow our methods work for analyzing actual pars-ing errors.
Section 6 and Section 7 illustrate fur-ther application of these methods to related topics.Section 8 summarizes this research and indicatessome of future directions.2 A parser and its evaluationA parser is a system which interprets given sen-tences in terms of structures derived from syn-tactic or in some cases semantic viewpoints, andstructures constructed as a result are used as es-sential information for various tasks of natural lan-guage processing such as information extraction,machine translation, and so on.In this paper, we address issues involved in im-proving the performance of a parser which pro-duces structural representations deeper than sur-face constituent structures.
Such a parser is calleda ?deep parser.?
In many deep parsers, the outputstructure is defined by a linguistics-based gram-mar framework such as CFG, CCG (Steedman,2000), LFG (Kaplan and Bresnan, 1995) or HPSGAbbr.
Full Abbr.
Fullaux auxiliary conj conjunctionprep prepositional lgs logical subjectverb verb app appositioncoord coordination relative relativedet determiner Narg(s) takes N argumentsadj adjunction mod modifies a wordTable 1: Descriptions for predicate types(Pollard and Sag, 1994).
Alternatively, some deepparsing models assume staged processing in whicha stage of shallow parsing is followed by a stage ofsemantic role labeling, which assigns labels indi-cating semantic relationships between predicatesand their arguments.
In either case, we assume aparser to produce a single ?deep?
structural rep-resentation for a given sentence, which is chosenfrom a set of possible interpretations as the mostprobable one by a disambiguation model.For evaluation of the performance of a parser,various metrics have been introduced accordingto the structure captured by a given grammarformalism or a system of semantic labels.
Inmost cases, instead of examining correctness fora whole structure, a parser is evaluated in terms ofthe F-value which shows how correctly it recog-nizes relationships among words and assigns ?la-bels?
to the relationships in the structure.
In thispaper, we assume a certain type of ?predicate-argument relation.
?In this measurement, a structure given for asentence is decomposed into a set of predicativewords and their arguments.
A predicate takesother words as its arguments.
In our representa-tion, the arguments are labeled by semanticallyneutral labels such as ARGn(n = 1...5) andMOD.
In this representation, a basic unit is atriplet, such as<Predicate:PredicateType,ArgumentLabel,Argument>,where ?Predicate?
and ?Argument?
are surfacewords.
As shown in the examples in Section 4,?PredicateType?
bears extra information concern-ing the syntactic construction in which the tripletis embedded.
ARG1-ARG5 express relations be-tween a Head and its complement, while MOD ex-presses a relation between an Adjunct and its mod-ifiee.
Since all dependency relations are expressedby triplets, triplets contain not only semantic de-181I saw a girl with a telescope Correct answer:ARG1 ARG2ARG1 ARG2Parser output:ARG1 ARG2ARG1 ARG2CompareError (25%):ARG1ARG1Correct (75%):ARG1 ARG2ARG2ARG1 ARG2ARG2I saw a girl with a telescopeI saw a girl with a telescopeI saw a girl with a telescopeFigure 3: An example of parsing performanceevaluationspendencies but also many dependencies which areessentially syntactic in nature.
Figure 1 shows anexample used in Miyao and Tsujii (2005) and Ni-nomiya et al (2006).This example shows predicate-argument rela-tions for ?John has come.?
There are two pred-icates in this sentence, ?has?
and ?come?.
Theword ?has?, which is used as an auxiliary verb,takes two words, ?John?
and ?come?, as its ar-guments, and therefore two triplets of predicate-argument relation, <has ARG1 John> and <hasARG2 come>.
As for the predicative word?come?, we have one triplet <come ARG1 John>.Note that, in this HPSG analysis, the auxiliaryverb ?has?
is analyzed in such a way that it takesone NP as subject and one VP as complement,and that the subject of the auxiliary verb is sharedby the verb (?come?)
in VP as its subject (Fig-ure 2).
The fact that ?has?
in this sentence is anauxiliary verb is indicated by the ?PredicateType?,aux 2args.
A ?PredicateType?
consists of a typeand the number of arguments it takes (Table 1).3 Difficulties in analyzing parsing errorsFigure 3 shows an example of the evaluation ofthe parser based on these predicate-argument rela-tions.
Note that the predicate types are abbreviatedin this figure.
In the sentence ?I saw a girl with atelescope?, there should be four triplets for the twopredicates, ?saw?
and ?with,?
each of which takesError:They completed the sale of forARG1ARG1it to him $1,000ConflictAnalysis 2: (Impossible)They completed the sale of for ARG1ARG1it to him $1,000Analysis 1: (Possible) ARG1ARG1ARG1ARG1Can each error occur independently?They completed the sale of for ARG1 ARG1it to him $1,000ARG1ARG1Figure 4: Sketch of error propagationThe book on which read the shelf  I yesterdayARG1 ARG2ARG2ARG1Error:Figure 5: Parsing errors around one relative clauseattachmenttwo arguments.
Although the parser output doesindeed contain four triplets, the first argument of?with?
is not the correct one.
Thus, this output iserroneous, with the F-value of 75%.While the F-value thus computed is fine for cap-turing the performance of a parser, it does not offerany help for improving its performance.First, because it does not give any indica-tion on what portion of erroneous triplets are inPP-attachment, complement/adjunct distinction,gerund/participle distinction, etc., one cannot de-termine which part of a parsing model should beimproved.
In order to identify error categories, wehave to manually compare a parsing output witha correct parse and classify them.
Consider againthe example in Figure 3.
We can easily observethat ?ARG1?
of predicate ?with?
was mistaken.
Inthis case, the word linked via ?ARG1?
representsa modifiee of the prepositional phrase, and therebywe conclude that the error is in PP-attachment.While the process looks straightforward for thissimple sentence and error, to perform such a man-ual inspection for all sentences and more complextypes of errors is costly, and becomes inhibitivewhen the size of a test set of sentences is realisti-182cally large.Another problem with the F-value is that it ig-nores inter-dependencies among errors.
Since theF-value does not consider inter-dependencies, onecannot determine which errors are more crucialthan others in terms of the performance of the sys-tem as a whole.A simple example of inter-dependency is shownin Figure 4.
?ARG1?
of ?for?
and ?to?
were mis-taken by a parser, both of which can be classifiedas PP-attachments as in Figure 3.
However, thetwo errors are not independent.
The former errorcan occur by itself (Analysis 1) while the lattercannot because of the structural conflict with theformer (Analysis 2).
The occurrence of the lattererror thus forces the former.Moreover, inter-dependency in a deep parserbased on linguistics-based formalisms can becomplicated.
Error propagation is ingrained ingrammar itself.
Consider Figure 5.
In this exam-ple, a wrong decision on the antecedent of a rela-tive clause results in a wrong triplet of the predi-cate in the embedded clause with the antecedent.That is, the two erroneous triplets, one of the?ARG1?
of ?which?
and the other of the ?ARG2?of ?read,?
were caused by a single wrong deci-sion of the antecedent of a relative clause.
Sucha propagation of errors can be even more compli-cated, for example, when the predicate in the rela-tive clause is a control verb.In the following section we propose two meth-ods for analyzing errors.
Although both meth-ods are implemented for the specific parser Enju(Miyao and Tsujii, 2005; Ninomiya et al, 2006),the same ideas can be implemented for any parsingmodel.4 Methods for effective error analysis4.1 Recognizing categories of error causesWhile the Enju parser produces rich feature struc-tures as output, the performance is evaluated bythe F-value in terms of basic units of predicate-argment structure.
As we illustrated in Section 2,the basic unit is a triplet in the following form.<Predicate:PredicateType,ArgumentLabel,Argument>We illustrated in Section 2 how we can identifyerrors in PP-attachment simply by examining aThe  car  was  designed to : use   it  for ...Correct output:aux_2argsto :[ verb1 ] ?ARG3 [ verb2 ]Parser output:aux_mod_2argsMODto :ARG2Unknown subject ARG1 ARG1[ verb1 ] ?
[ verb2 ]aux_2argsExample:Parser output:Correct answer:ARG3The  car  was  designed to : use   it  for ...aux_mod_2argsMOD ARG2Unknown subject ARG1 ARG1Pattern:(Patterns of correct answer and parser output can be interchanged)Figure 6: Pattern for ?To-infinitive for modi-fier/argument of verb?triplet produced by the parser with the correspond-ing triplet in the gold standard parse.However, in more complex cases, we have toconsider a set of mismatched triplets collectivelyin order to map errors to meaningful error causes.The following are typical examples of error causesand pattern rules which identify them.
(1) Interpretation of Infinitival Clauses as Adjunctor ComplementTwo different types of interpretations of the in-finitival clauses are explicitly indicated by ?Predi-cateType.?
Consider the following two sentences.
(a) [Infinitival clause as an adjunct of the mainclause]The car was designed (by John) to use it forbusiness trips.
(b) [Infinitival clause as an argument of catena-tive verb]The car is designed to run fast.In both sentences, ?to?
is treated as a predicate torepresent the infinitival clauses in triplets.
How-ever, Enju marks the ?PredicateType?
of (a) as?aux-mod-2args,?
while it marks the predicatesimply as ?aux-2args?
in (b).
Furthermore, thelinkage between the main clause and the infinitivalclause is treated differently.
In (a), the infinitivalclause takes the main clause with relation MOD,while in (b) the main clause takes the infinitival183[ gerund ]: verb_Narg(s)Parser output: [ gerund ]: verb_mod_Narg(s)Correct answer:(Patterns of correct answer and parser output can be interchanged)Pattern:Example:The customers walk the doora   package   for   themexpecting: verb_mod_3argsyou to havein MODARG1ARG2 ARG3Parser output:Correct output:The customers walk the doora   package   for   themexpecting: verb_3argsyou to haveinNot existARG2 ARG3ARG1 (MOD)?
??
?Figure 7: Pattern for ?Gerund acts as modifier ornot?clause as ARG3.
Furthermore, in the catenativeverb interpretation of ?designed?, the deep object(the surface subject in this example) fills ARG1of the verb in the infinitival clause (complement),while in the adjunct interpretation, the deep sub-ject which is missing in this sentence occupiesthe same role.
Consequently, a single erroneouschoice between these two interpretations results ina set of mismatched triplets.We recognize such a set of mismatched tripletsby a pattern rule (Figure 6) and map them to thistype of error cause.
(2) Interpretation of Gerund-Participle interpreta-tionsA treatment similar to (1) is taken for differentinterpretations of Gerund.
Interpretation as Ad-junct of a main clause is signaled by the ?Predi-cateType?
verb-mod-*, while an interpretation asa modifier of a noun is represented by the ?Predi-cateType?
verb (Figure 7).
(3) Interpretation of ?by?A prepositional phrase with ?by?
in a passiveclause can be interpreted as a deep subject, whilethe same phrase can be interpreted as an ordinaryPP phrase that is used as an adjunct.
The first in-terpretation is marked by the ?PredicateType?
lgs(logical subject) which takes only one argument.The relationship between the passivized verb andthe deep subject is captured by ARG1 which goesExample:Pattern:Correct output:Parser output: prep_2argsUnknown subject[ verb1 ] ?ARG1ARG1 ?lgs_1arg ARG1[ verb1 ] ?
?ARG1A 50-state study released in September  by : Friends ?Unknown subject?
ARG1ARG1prep_2argsParser output:Correct answer:A 50-state study released in September  by : Friends ?
?ARG1ARG1lgs_1argARG1(Patterns of correct answer and parser output can be interchanged)Figure 8: Pattern for ?Subject for passive sentenceor not?Example:Pattern:relative_1argARG1Parser output: ARG1/2ErrorParser output:Correct answer:The  book on which : readARG1the  shelf  I yesterdayARG2The  book on which : readARG1the  shelf  I yesterdayARG2relative_1argrelative_1argError?
?Figure 9: Pattern for ?Relative clause attachment?from the verb to the noun phrase.
On the otherhand, in the interpretation as an ordinary PP, thepreposition as predicate links the main verb andNP via ARG1 and ARG2, respectively (Figure 8).Again, a set of mismatched triplets should bemapped to a single cause of errors via a patternrule.
(4) Antecedent of a Relative ClauseThis type of error is manifested by two mis-matched triplets with different predicates.
This isbecause a wrong choice of antecedent for a rela-tive clause results in a wrong link for the trace ofthe relative clause.Since a relative clause pronoun is treated as a184Cause categories Patterns[Argument selection]Prepositional attachment ARG1 of prep *Adjunction attachment ARG1 of adj *Conjunction attachment ARG1 of conj *Head selection for noun phrase ARG1 of det *Coordination ARG1/2 of coord *[Predicate type selection]Preposition/Adjunction prep * ?
adj *Gerund acts as modifier/not verb mod Narg(s)?
verb Narg(s)Coordination/conjunction coord * ?
conj *# of arguments for preposition prep Marg(s)?
prep Narg(s)Adjunction/adjunctive noun adj * ?
noun *[More structural errors]To-infinitive for see Figure 6modifier/argument of verbSubject for passive sentence/not see Figure 8[Others]Comma any error around ?,?Relative clause attachment see Figure 9Table 2: Defined patterns for cause categoriespredicate which takes the antecedent as its singleargument, identification of error type can be donesimply by looking at ARG1.
However, since theerrors usually propagate to the triplets that containtheir traces, we have to map them together to thesingle error (Figure 9).Table 2 shows the errors across different typeswhich our current version of pattern rules canidentify.4.2 Capturing inter-dependencies amongerrorsSome inter-dependencies among erroneoustriplets are ingrained in grammar, such as the caseof antecedent of a relative clause in (4) in Section4.1.
Some are caused by general constraints suchas the projection principle in dependency structure(Figure 4 in Section 2).Regardless of causes of dependencies, to recog-nize inter-dependencies among errors is a crucialstep of effective error analysis.Our method consists of the following four steps:[Step 1] Re-parsing a target sentence: A given sen-tence is re-parsed under the condition where an er-ror is forcibly corrected.
[Step 2] Forming a network of inter-dependenciesof errors: By comparing the new parse result (aset of triplets) with the initial parse result, thisstep creates a directed graph of errors in the ini-1Re-parse a sentence under the condition whereeach error is forcibly corrected123Correct 211Form inter-dependent error groups anderror propagation network4 1433CorrectCorrectCorrectdisappeardisappeardisappeardisappear,,,1 2 3 4ARG1our work force todayonErrors:ARG1 ARG1ARG2ARG2 ARG1 ARG1 ARG1It  has  no  bearing2 3 455 4Correct disappear1 32, , ,4,2 4,,2 3,,5PropagationResultant network:Inter-dependent error group Inter-dependent error group(a)(b)(c)Inter-dependency among errors:re-parsere-parsere-parsere-parsere-parseFigure 10: Schema of capturing inter-dependenciestial parse.
A directed link shows that correction ofthe error in the starting node produces a new parseresult in which the error in the receiving node ofthe link disappears.
[Step 3] Forming groups of inter-dependent errors:This step recognizes a group of inter-dependent er-rors which forms a directed circle in the networkcreated by [Step 2].
[Step 4] Forming a network of error propagation:This step creates a new network by reducing eachof inter-dependent error groups of [Step 3] to a sin-gle node.Figure 10 illustrates how these steps work.
Inthis example, while ?today?
should modify thenoun phrase ?our work force?, the initial parsewrongly takes ?today?
as the head noun of thewhole noun phrase.
As a result, there are five er-rors; three wrong outputs, ?ARG2?
of ?on?
(Er-ror 1), ?ARG1?
of ?our?
(Error 2) and ?ARG1?of ?work?
(Error 3).
There is an extra triplet for?ARG1?
of ?force?
(Error 4), and a triplet for?ARG1?
of ?today?
(Error 5) is missing (Figure10 (a)).Figure 10 (b) shows inter-dependencies amongthe errors recognized by [Step 2], and Figure 10185# ofCause categories of errors Errors LocationsClassified 2,078 1,671[Argument selection]Prepositional attachment 579 579Adjunction attachment 261 261Conjunction attachment 43 40Head selection for noun phrase 30 30Coordination 202 184[Predicate type selection]Preposition/Adjunction 108 54Gerund acts as modifier/not 84 31Coordination/conjunction 54 27# of arguments for preposition 51 17Adjunction/adjunctive noun 13 13[More structural errors]To-infinitive for 120 22modifier/argument of verbSubject for passive sentence/not 8 3[Others]Comma 444 372Relative clause attachment 102 38Unclassified 2,631 ?Total 4,709 ?Table 3: Errors classified into cause categories(c) shows what the resultant network looks like.An inter-dependent error group of 1, 2, 3 and 4 isrecognized by [Step 3] and represented as a singlenode.
Error 5 is propagated to this node in the finalnetwork.5 ExperimentsWe applied our methods to the analyses of actualerrors produced by Enju.
This version of Enju wastrained on the Penn Treebank (Marcus et al, 1994)Section 2-21.5.1 Observation of identified cause categoriesWe first parsed sentences in PTB Section 22, andbased on the observation of errors, we defined thepatterns in Section 4.
We then parsed sentences inSection 0.
The errors in Section 0 were mapped toerror cause categories by the pattern rules createdfor Section 22.Table 3 summarizes the distribution across thecauses of errors.
The left and right numbers in thetable show the number of erroneous triplets clas-sified into the categories and the frequency of thepatterns matched, respectively.
The table showsthat, with the 14 pattern rules, we successfully ob-served 1,671 hits and 2,078 erroneous triplets aredealt with by these hits.
This amounts to morethan 40% erroneous triplets (2,078/4,709).
Sincethis was the first attempt, we expect the coveragecan be easily improved by adding new patterns.Evaluated sentences (erroneous) 1,811 (1,009)Errors (Correctable) 4,709 (3,085)Inter-dependent error groups 1,978Correction propagations 501F-score (LP/LR) 90.69 (90.78/90.59)Table 4: Summary of inter-dependencies      Figure 11: Frequency of each size of inter-dependent error groupFrom the table, we can observe that a signif-icant portion of errors is covered by simple typesof error causes such as PP-attachment and Adjunctattachment.
They are simple in the sense that thenumber of erroneous triplets treated and the fre-quency of the pattern application coincide.
How-ever, their conceived significance may be over-rated.
These simple types may constitute parts ofmore complex error causes.
Furthermore, sincepattern rules for these simple causes are easy toprepare and have already been covered by the cur-rent version, most of the remaining 60% of the er-roneous triplets are likely to require patterns formore complex causes.On the other hand, patterns for complex causescollect more erroneous triplets once they are fired.This tendency is more noticeable in structural pat-terns of errors.
For example, in ?To-infinitive formodifier/argument of verb,?
there were only 22hits for the pattern, while the number of erroneoustriplets is 120.
This implies five triplets per hit.This is because, in a deep parser, a wrong choicebetween adjunct or complement interpretations ofa to-infinitival clause affects the interpretation ofimplicit arguments in the clause through control.Though expected, such detailed observations showhow differences between shallow and deep parsersmay affect evaluation methods and the methods ofanalyzing errors.5.2 Observation of inter-dependenciesIn the inter-dependency experiments we per-formed, some errors could not be forcibly cor-rected by our method.
This was because the parser186The  asbestos  fiber  ,  crocidolite ,  is  unusually  resilient  onceit  enters  the  lungs  ,  with  even  brief  exposures  to  it causingsymptoms  that  show  up  decades  later  ,  researchers  said  .
(a) (b)(c) (d)?
fiber      , : crocidolite ?app_2args?
fiber      , : crocidolite ?coord_2argsCorrect answer:Parser output:?
is   usually   resilient   ?
the   lungs      ,       with   ??
symptoms   that    show : up   decades   later  ?Parser output:Correct answer: verb_1arg?
symptoms   that    show : up   decades   later  ?verb_2args?
it   causing   symptoms   that   show   up   decades   later  ?Sentence:Inter-dependent error group (a):Inter-dependent error group (b):Inter-dependent error group (c):Inter-dependent error group (d):ARG1 ARG2ARG1 ARG2ARG1ARG1ARG1ARG1ARG1ARG1ARG1ARG2ARG1ARG1Figure 12: Obtained inter-dependent error groupswe use prunes less probable parse substructuresduring parsing.
In some cases, even if we gave alarge positive value to a triplet which should be in-cluded in the final parse, parsing paths which cancontain the triplet were pruned before.
In this re-search, we ignored such errors as ?uncorrectable?ones, and focused on the remaining ?correctable?errors.Table 4 shows a summary of the analysis.
As theprevious experiment, Enju produced 4,709 errorsfor Section 0, of which 3,085 were correctable.
Byapplying the method illustrated in Section 4.2, weobtained 1,978 inter-dependent error groups and501 correction propagation relationships amongthe groups.Figure 11 shows the frequency of the size ofinter-dependent error groups.
About half of thegroups contain only single errors which couldhave only one-way correction propagations withother errors or were completely independent ofother errors.Figure 12 shows an example of the extractedinter-dependent error groups.
For the sentenceshown at the top, Enju gave seven errors.
By ap-plying the method in Section 4.2, these errors weregrouped into four inter-dependent error groups (a)to (d), and no correction propagations were de-She  says  she  offered  Mrs.  Yeargin a  quiet  resignation  andthought  she  could  help  save  her  teaching  certificate .
(a) (b)Sentence:Inter-dependent error group (a):Inter-dependent error group (b):Correct answer:Parser output:?
she  could  help  save : her  teaching  certificate .verb_3args?
she  could  help  save : her  teaching  certificate .verb_2argsCorrect answer:Parser output:?
thought   she  could   help : save   ?verb_2args?
thought   she  could   help : save   ?aux_2argsCorrection propagation from (a) to (b)ARG2ARG2ARG2ARG2ARG1 ARG2ARG1 ARG2ARG3ARG1ARG1ARG2ARG2ARG1Figure 13: Correction propagation between ob-tained inter-dependent error groupstected among them.
Group (a) contains two errorson the comma?s local behavior as apposition or co-ordination.
Group (b) contains the errors on thewords which give almost the same attachment be-haviors.
Group (c) contains the errors on whetherthe verb ?show?
took ?decades?
as its object ornot.
Group (d) contains an error on the attachmentof the adverb ?later?.
Regardless of the overlapof the regions in the sentence for (c) and (d), ourapproach successfully grouped the errors into twoindependent groups.
The method shows that theerrors in each group are inter-dependent, but er-rors in one group are independent of those in an-other.
This enables us to concentrate on each ofthe co-occurring error groups separately, withoutminding the errors in other groups.Figure 13 shows another example.
In this ex-ample, eight errors for a sentence were classifiedinto two inter-dependent error groups (a) and (b).Moreover, it shows that the correction of group (a)results in correction of group (b).The errors in group (a) were related to thechoice as to whether ?help?
had an auxiliary ora pure verbal role.
The errors in group (b) wererelated with the choice as to whether ?save?
tookonly one object (?her teaching certificate?)
or twoobjects (?her?
and ?teaching certificate?).
Be-tween group (a) and (b), no ?structural?
con-187It  invests  heavily  in  dollar-denominated  securities  overseas  andis  currently  waiving  management  fees  ,  which  boosts  its yield .
(a)(b)Sentence:Inter-dependent error group (a):Inter-dependent error group (b):Adjunction attachmentCause categories:Comma, Relative clause attachmentCause categories:It  invests  heavily  in  ?
securities  overseas : ?adj_1argARG1 ARG1?
is  currently  waiving  management  fees   ,   which   boosts  ?ARG1ARG1ARG1ARG1ARG1ARG1Figure 14: Combining our two methods (1)flict could arise when correcting only each of thegroups.
We could then hypothesize that the cor-rection propagation between the two groups werecaused by the disambiguation model.By dividing the errors into minimal units andclarifying the effects of correcting a target error,we can conclude that the inter-dependent group(a) should be handled first for effective improve-ment of the parser.
In such a way, obtained inter-dependencies among errors can suggest an effec-tive strategy for parser improvement.5.3 Combination of the two methodsBy combining the two methods described in Sec-tion 4.1 and 4.2, we can see how each error causeaffects the performance of a parser.
The resultsare summarized in Table 5.
The leftmost columnin the table shows the numbers of errors in termsof triplets, which are the same as the leftmost col-umn in Table 3.The ?independence rate?
shows the ratio of er-roneous triplets in the category which are not af-fected by correction of other erroneous triplets.
Onthe other hand, the ?correction effect?
shows howmany erroneous triplets would be corrected if oneof the erroneous triplets in the category was cor-rected.
These two columns are computed by usingthe error propagation network constructed in Sec-tion 4.2.
That is, by using the network we obtainthe number of erroneous triplets to be corrected ifa given erroneous triplet in the category was cor-rected, sum up these numbers and then calculatethe average number of expected side-effect correc-Clark  J.  Vitulli was  named  senior  vice  president  and  generalmanager  of   this  U.S.  sales  and  marketing  arm of Japaneseauto Maker Mazda Motor Corp .
(b)(a)Sentence:Inter-dependent error group (a):Inter-dependent error group (b):Coordination (fragment)Head selection for noun phraseCause categories:?
president  ?
of  this  U.S.  sales   and : ?coord_2argsARG1ARG1of   this : U.S. sales and : marketing armdet_1arg coord_2argsARG2ARG1ARG2 ARG1ARG2 ARG1 ARG1 ARG1 ARG2Correction propagation from (a) to (b)Coordination (fragment)Cause categories:Figure 15: Combining our two methods (2)tion per erroneous triplet in the category.Figure 14 shows an example of independent er-rors.
For the sentence at the top, the parser pro-duced four errors.
The method in Section 4.2successfully discovered two inter-dependent errorgroups (a) and (b).
There was no error propaga-tion relation between the two groups.
On the otherhand, the method in Section 4.1 associated all ofthese four errors with the categories of ?Adjunc-tion attachment,?
?Comma?
and ?Relative clauseattachment,?
and the error for the ?Adjunction at-tachment?
corresponds to the inter-dependent er-ror group (a).
Because this group is not a receivingnode of any propagation in the network, the erroris regarded as an ?independent?
one.Independent errors mean that, if a new parsingmodel could correct them, the correction wouldnot be destroyed by other errors which remain inthe new parsing model.The correction effect shows the opposite anddesirable effect of the nature of the dependencyamong errors which the propagation network rep-resents.
This means that, if one of erroneoustriplets in the category was corrected, the correc-tion would be amplified through propagation, andas a result other errors would also be corrected.We show an example of the correction effect inFigure 15.
In the figure, the parser had six errors;three false outputs for ARG1 of ?and,?
?this?
and188Independence Correction Expected rangeCause categories of errors # of errorsrate (%) effect (%) of error correction[Argument selection]Prepositional attachment 579 74.8 144.3 625.0 - 835.5Adjunction attachment 261 56.6 179.6 265.3 - 468.8Conjunction attachment 43 36.4 239.4 37.5 - 102.9Head selection for noun phrase 30 0.0 381.8 0.0 - 114.5Coordination 202 42.5 221.2 189.9 - 446.8[Predicate type selection]Preposition/Adjunction 108 41.7 158.3 71.3 - 171.0Gerund acts as modifier/not 84 46.2 159.0 61.7 - 133.0Coordination/conjunction 54 44.4 169.4 40.6 - 91.5# of arguments for preposition 51 95.8 108.3 52.9 - 55.2Adjunction/adjunctive noun 13 75.0 125.0 12.2 - 16.3[More structural errors]To-infinitive for 120 36.0 116.0 50.1 - 139.2modifier/argument of verbSubject for passive sentence/not 8 37.5 112.5 3.4 - 9.0[Others]Comma 444 39.5 194.4 341.0 - 863.1Relative clause attachment 102 32.1 141.7 46.4 - 144.5Table 5: Correction propagations between errors for each cause category and the other errors?U.S.,?
two false outputs for ARG2 of ?of?
and?and,?
and a missing output for ARG1 of ?sales.
?Our method for inter-dependencies classified theseerrors into two inter-dependent error groups (a)and (b), and extracted an correction propagationfrom (a) to (b).
Our method for cause categories,on the other hand, associated two errors of ?and?with the category ?Coordination?
and one error of?this?
with the category ?Head selection for nounphrase.?
When we correct an error in the interde-pendent error group (a), the correction leads to notonly correction of the other errors in (a) but alsocorrection of the error in (b) via correction prop-agation from (a) to (b).
Therefore, a correctioneffect of an error in group (a) results in 6.0.On the basis of the above considerations, we es-timated the range of the effect which an error cor-rection in each category has.
The minimum of ex-pected correction range in Table 5 is given by theproduct of the number of erroneous triplets in thecategory, the independence rate and the correctioneffect.
On the other hand, the maximum is givenby the product of the number of erroneous tripletsin the category and the correction effect.
This as-sumes that all corrections made in the category arenot cancelled by other errors, while the figure inthe minimum are based on the assumption that allcorrections made in the category, except for the in-dependent ones, are cancelled by other errors.Table 5 would thus suggest which categoriesshould be resolved with high priority, from threepoints of view: the number of errors in the cat-egory, the number of independent errors, and thecorrection effect.6 Further applications of our methodsIn this section, as an example of the further ap-plication of our methods, we attempt to analyzeparsing behaviors in domain adaptation from theviewpoints of error cause categories.In Hara et al (2007), we proposed a method foradapting Enju to a target domain, and then suc-ceeded in improving the parser performance forthe GENIA corpus (Kim et al, 2003), a biomed-ical domain.
Table 6 summarizes the parsing re-sults for three types of settings respectively: pars-ing PTB with Enju (?Enju for PTB?
), parsing GE-NIA with Enju (?Enju for GENIA?
), and parsingGENIA with the adapted model (?Adapted for GE-NIA?).
We then analyzed the performance transi-tion among these settings from the viewpoint ofthe cause categories given in Section 4.1 (Table 7).In order to compare the error frequencies amongdifferent settings, we took the percentage of targeterrors in all of the evaluated triplets.
The signedvalues between the two settings show how muchthe errors increased when moving from the left set-tings to the right ones.When we focus on the transition from ?Enjufor PTB?
to ?Enju for GENIA,?
we can observethat the change in the domain resulted in a dif-ferent distribution of error causes.
The errors formost categories increased, and in particular, the er-rors for ?Prepositional attachment?
and ?Coordi-189Enju for PTB Enju for GENIA Adapted for GENIAEvaluated sentences 1,811 842 842Evaluated triplets 44,934 22,230 22,230Errors 4,709 3,120 2,229F-score (LP/LR) 90.69 (90.78/90.59) 87.41 (87.60/87.22) 90.93 (91.10/90.76)Table 6: Summary of parsing performances for domain and model variationsRate of errors against total examined relations in test set (%)Cause categories of errors Enju for PTB ??
Enju for GENIA ??
Adapted for GENIAClassified 4.62 +2.60?
7.22 ?1.80?
5.42[Argument selection]Prepositional attachment 1.29 +0.93?
2.22 ?0.64?
1.58Adjunction attachment 0.58 +0.38?
0.96 ?0.20?
0.76Conjunction attachment 0.10 ?0.04?
0.06 ?0.04?
0.02Head selection for noun phrase 0.07 +0.17?
0.24 ?0.06?
0.18Coordination 0.45 +0.59?
1.04 ?0.25?
0.79[Predicate type selection]Preposition/Adjunction 0.24 +0.08?
0.32 ?0.06?
0.26Gerund acts as modifier/not 0.19 ?0.07?
0.12 +0.01?
0.13Coordination/conjunction 0.12 ?0.00?
0.12 ?0.07?
0.05# of arguments for preposition 0.11 ?0.02?
0.09 ?0.00?
0.09Adjunction/adjunctive noun 0.03 +0.19?
0.22 ?0.08?
0.14[More structural errors]To-infinitive for 0.27 +0.02?
0.29 ?0.09?
0.20modifier/argument of verbSubject for passive sentence/not 0.02 +0.34?
0.36 +0.01?
0.37[Others]Comma 0.99 ?0.03?
0.96 ?0.31?
0.65Relative clause attachment 0.23 +0.05?
0.28 ?0.03?
0.25Unclassified 5.86 +0.96?
6.82 ?2.22?
4.60Total (Classified + Unclassified) 10.48 +3.56?
14.04 ?4.01?
10.03Table 7: Error distributions for domain and model variationsnation?
increased remarkably.
On the other hand,the transition from ?Enju for GENIA?
to ?Adaptedfor GENIA?
shows that their adaptation methodsucceeded in reducing the errors for most cate-gories to some extent.
However, for ?Preposi-tional attachment,?
?Coordination,?
and ?Subjectfor passive sentence or not,?
there were still no-ticeable gaps in error distribution between ?Enjufor PTB?
and ?Adapted for GENIA.?
This wouldmean that the adapted model requires further per-formance improvement if we expect the same levelof performances for those categories as the parseroriginally obtained in PTB.We could thus capture some biases of causecategories which occur in domain transition orin domain adaptation, which would not be clari-fied by F-score evaluation methods.
With inter-dependencies given by the method described inSection 4.2, the above analysis would be usefulfor effectively exploring further adaptation.7 Related worksAlthough there have been many researchers whoanalyzed errors in their own systems in the experi-ments, there has been little research which focusedon error analysis itself.In the field of parsing, McDonald and Nivre(2007) compared parsing errors between graph-based and transition-based parsers.
They consid-ered accuracy transitions from various points ofview, and the obtained statistical data suggestedthat error propagation seemed to occur in thegraph structures of parsing outputs.
Our researchproceeded one step further and attempted to re-veal the nature of the propagations.
In examin-ing the combination of the two types of parsing,they utilized approaches similar to our method forcapturing inter-dependencies of errors.
They al-lowed a parser to give only structures produced bythe parsers and utilized the ideas for evaluating theparser?s potentials, whereas we utilized it for ob-serving error propagations.Dredze et al (2007) showed that many of theparsing errors in domain adaptation tasks maycome from inconsistencies between the annota-tions of training resources.
This would sug-gest that just error comparisons without consider-ing the inconsistencies could lead to a misunder-190standing of what happens in domain transitions.The summarized error cause categories and inter-dependencies given by our methods would be use-ful clues for extracting such domain-dependent er-ror phenomena.When we look into other research areas in nat-ural language processing, Gime?nez and Ma`rquez(2008) proposed an automatic error analysis ap-proach in machine translation (MT) technologies.They developed a metric set which could capturefeatures in MT outputs at different linguistic lev-els with different levels of granularity.
Like weconsidered parsing systems, they explored ways toresolve costly and rewardless error analysis in theMT field.
One of their objectives was to enableresearchers to easily obtain detailed linguistic re-ports on the behavior of their systems, and to con-centrate on analyses for the system improvements.8 ConclusionWe proposed two methods for analyzing parsingerrors.
One is to assign errors to cause categories,and the other is to capture inter-dependenciesamong errors.
The first method defines error pat-terns to identify cause categories and then asso-ciates errors involved in the patterns with the cor-responding categories.
The second method re-parses a sentence with a target error corrected, andregards errors corrected together as dependent onthe target.In our experiments with an HPSG parser, wesuccessfully associated more than 40% of the er-rors with 14 cause categories, and captured 1,978inter-dependent error groups.
Moreover, the com-bination of our methods gave a more detailed erroranalysis for effective improvement of the parser.In our future work, we would give more pat-tern rules for classifying a large percentage of er-rors into cause categories, and incorporate uncor-rectable errors into inter-dependency analysis.
Af-ter improving the analytical facilities of our indi-vidual methods, we would explore the possibil-ity of combining the methods for obtaining morepowerful and detailed clues on how to improveparsing performance.AcknowledgmentsThis work was partially supported by Grant-in-Aidfor Specially Promoted Research (MEXT, Japan).ReferencesMark Dredze, John Blitzer, Partha Pratim Talukdar,Kuzman Ganchev, Joa?o V. Grac?a, and FernandoPereira.
2007.
Frustratingly hard domain adapta-tion for dependency parsing.
In Proceedings of theCoNLL Shared Task Session of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 1051?1055.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2008.
Towards het-erogeneous automatic mt error analysis.
In Proceed-ings of the Sixth International Language Resourcesand Evaluation (LREC?08), pages 1894?1901.Tadayoshi Hara, Yusuke Miyao, and Jun?ichi Tsujii.2007.
Evaluating impact of re-training a lexical dis-ambiguation model on domain adaptation of an hpsgparser.
In Proceedings of 10th International Confer-ence on Parsing Technologies (IWPT 2007), pages11?22.Ronald M. Kaplan and Joan Bresnan.
1995.
Lexical-functional grammar: A formal system for gram-matical representation.
Formal Issues in Lexical-Functional Grammar, pages 29?130.Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, andJun?ichi Tsujii.
2003.
GENIA corpus - a seman-tically annotated corpus for bio-textmining.
Bioin-formatics, 19(suppl.
1):i180?i182.Mitchell Marcus, Grace Kim, Mary AnnMarcinkiewicz, Robert Macintyre, Ann Bies,Mark Ferguson, Karen Katz, and Britta Schas-berger.
1994.
The Penn Treebank: Annotatingpredicate argument structure.
In Proceedings ofARPA Human Language Technology Workshop.Ryan McDonald and Joakim Nivre.
2007.
Charac-terizing the errors of data-driven dependency pars-ing models.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 122?131.Yusuke Miyao and Jun?ichi Tsujii.
2005.
Probabilis-tic disambiguation models for wide-coverage HPSGparsing.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics(ACL), pages 83?90.Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-ruoka, Yusuke Miyao, and Jun?ichi Tsujii.
2006.Extremely lexicalized models for accurate and fastHPSG parsing.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 155?163.Carl J. Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University of ChicagoPress.Mark Steedman.
2000.
The Syntactic Process.
THEMIT Press.191
