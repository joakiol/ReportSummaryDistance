Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 331?339,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Pilot Study of Opinion Summarization in ConversationsDong Wang Yang LiuThe University of Texas at Dallasdongwang,yangl@hlt.utdallas.eduAbstractThis paper presents a pilot study of opinionsummarization on conversations.
We createa corpus containing extractive and abstrac-tive summaries of speaker?s opinion towardsa given topic using 88 telephone conversa-tions.
We adopt two methods to perform ex-tractive summarization.
The first one is asentence-ranking method that linearly com-bines scores measured from different aspectsincluding topic relevance, subjectivity, andsentence importance.
The second one is agraph-based method, which incorporates topicand sentiment information, as well as addi-tional information about sentence-to-sentencerelations extracted based on dialogue struc-ture.
Our evaluation results show that bothmethods significantly outperform the baselineapproach that extracts the longest utterances.In particular, we find that incorporating di-alogue structure in the graph-based methodcontributes to the improved system perfor-mance.1 IntroductionBoth sentiment analysis (opinion recognition) andsummarization have been well studied in recentyears in the natural language processing (NLP) com-munity.
Most of the previous work on sentimentanalysis has been conducted on reviews.
Summa-rization has been applied to different genres, suchas news articles, scientific articles, and speech do-mains including broadcast news, meetings, conver-sations and lectures.
However, opinion summariza-tion has not been explored much.
This can be use-ful for many domains, especially for processing theincreasing amount of conversation recordings (tele-phone conversations, customer service, round-tablediscussions or interviews in broadcast programs)where we often need to find a person?s opinion orattitude, for example, ?how does the speaker thinkabout capital punishment and why??.
This kind ofquestions can be treated as a topic-oriented opin-ion summarization task.
Opinion summarizationwas run as a pilot task in Text Analysis Conference(TAC) in 2008.
The task was to produce summariesof opinions on specified targets from a set of blogdocuments.
In this study, we investigate this prob-lem using spontaneous conversations.
The problemis defined as, given a conversation and a topic, asummarization system needs to generate a summaryof the speaker?s opinion towards the topic.This task is built upon opinion recognition andtopic or query based summarization.
However, thisproblem is challenging in that: (a) Summarization inspontaneous speech is more difficult than well struc-tured text (Mckeown et al, 2005), because speechis always less organized and has recognition errorswhen using speech recognition output; (b) Senti-ment analysis in dialogues is also much harder be-cause of the genre difference compared to other do-mains like product reviews or news resources, as re-ported in (Raaijmakers et al, 2008); (c) In conversa-tional speech, information density is low and thereare often off topic discussions, therefore presentinga need to identify utterances that are relevant to thetopic.In this paper we perform an exploratory studyon opinion summarization in conversations.
Wecompare two unsupervised methods that have been331widely used in extractive summarization: sentence-ranking and graph-based methods.
Our system at-tempts to incorporate more information about topicrelevancy and sentiment scores.
Furthermore, inthe graph-based method, we propose to better in-corporate the dialogue structure information in thegraph in order to select salient summary utterances.We have created a corpus of reasonable size in thisstudy.
Our experimental results show that bothmethods achieve better results compared to the base-line.The rest of this paper is organized as follows.
Sec-tion 2 briefly discusses related work.
Section 3 de-scribes the corpus and annotation scheme we used.We explain our opinion-oriented conversation sum-marization system in Section 4 and present experi-mental results and analysis in Section 5.
Section 6concludes the paper.2 Related WorkResearch in document summarization has been wellestablished over the past decades.
Many tasks havebeen defined such as single-document summariza-tion, multi-document summarization, and query-based summarization.
Previous studies have usedvarious domains, including news articles, scientificarticles, web documents, reviews.
Recently thereis an increasing research interest in speech sum-marization, such as conversational telephone speech(Zhu and Penn, 2006; Zechner, 2002), broadcastnews (Maskey and Hirschberg, 2005; Lin et al,2009), lectures (Zhang et al, 2007; Furui et al,2004), meetings (Murray et al, 2005; Xie and Liu,2010), voice mails (Koumpis and Renals, 2005).In general speech domains seem to be more diffi-cult than well written text for summarization.
Inprevious work, unsupervised methods like MaximalMarginal Relevance (MMR), Latent Semantic Anal-ysis (LSA), and supervised methods that cast the ex-traction problem as a binary classification task havebeen adopted.
Prior research has also explored usingspeech specific information, including prosodic fea-tures, dialog structure, and speech recognition con-fidence.In order to provide a summary over opinions, weneed to find out which utterances in the conversa-tion contain opinion.
Most previous work in senti-ment analysis has focused on reviews (Pang and Lee,2004; Popescu and Etzioni, 2005; Ng et al, 2006)and news resources (Wiebe and Riloff, 2005).
Manykinds of features are explored, such as lexical fea-tures (unigram, bigram and trigram), part-of-speechtags, dependency relations.
Most of prior work usedclassification methods such as naive Bayes or SVMsto perform the polarity classification or opinion de-tection.
Only a handful studies have used conver-sational speech for opinion recognition (Murray andCarenini, 2009; Raaijmakers et al, 2008), in whichsome domain-specific features are utilized such asstructural features and prosodic features.Our work is also related to question answering(QA), especially opinion question answering.
(Stoy-anov et al, 2005) applies a subjectivity filter basedon traditional QA systems to generate opinionatedanswers.
(Balahur et al, 2010) answers some spe-cific opinion questions like ?Why do people criti-cize Richard Branson??
by retrieving candidate sen-tences using traditional QA methods and selectingthe ones with the same polarity as the question.
Ourwork is different in that we are not going to an-swer specific opinion questions, instead, we providea summary on the speaker?s opinion towards a giventopic.There exists some work on opinion summariza-tion.
For example, (Hu and Liu, 2004; Nishikawa etal., 2010) have explored opinion summarization inreview domain, and (Paul et al, 2010) summarizescontrastive viewpoints in opinionated text.
How-ever, opinion summarization in spontaneous conver-sation is seldom studied.3 Corpus CreationThough there are many annotated data sets for theresearch of speech summarization and sentimentanalysis, there is no corpus available for opinionsummarization on spontaneous speech.
Thus for thisstudy, we create a new pilot data set using a sub-set of the Switchboard corpus (Godfrey and Holli-man, 1997).1 These are conversational telephonespeech between two strangers that were assigned atopic to talk about for around 5 minutes.
They weretold to find the opinions of the other person.
Thereare 70 topics in total.
From the Switchboard cor-1Please contact the authors to obtain the data.332pus, we selected 88 conversations from 6 topics forthis study.
Table 1 lists the number of conversationsin each topic, their average length (measured in theunit of dialogue acts (DA)) and standard deviationof length.topic #Conv.
avg len stdevspace flight and exploration 6165.5 71.40capital punishment 24gun control 15universal health insurance 9drug testing 12universal public service 22Table 1: Corpus statistics: topic description, number ofconversations in each topic, average length (number ofdialog acts), and standard deviation.We recruited 3 annotators that are all undergrad-uate computer science students.
From the 88 con-versations, we selected 18 (3 from each topic) andlet al three annotators label them in order to studyinter-annotator agreement.
The rest of the conversa-tions has only one annotation.The annotators have access to both conversationtranscripts and audio files.
For each conversation,the annotator writes an abstractive summary of upto 100 words for each speaker about his/her opin-ion or attitude on the given topic.
They were told touse the words in the original transcripts if possible.Then the annotator selects up to 15 DAs (no mini-mum limit) in the transcripts for each speaker, fromwhich their abstractive summary is derived.
The se-lected DAs are used as the human generated extrac-tive summary.
In addition, the annotator is askedto select an overall opinion towards the topic foreach speaker among five categories: strongly sup-port, somewhat support, neutral, somewhat against,strongly against.
Therefore for each conversation,we have an abstractive summary, an extractive sum-mary, and an overall opinion for each speaker.
Thefollowing shows an example of such annotation forspeaker B in a dialogue about ?capital punishment?
:[Extractive Summary]I think I?ve seen some statistics that say that, uh, it?smore expensive to kill somebody than to keep them inprison for life.committing them mostly is, you know, either crimes ofpassion or at the momentor they think they?re not going to get caughtbut you also have to think whether it?s worthwhile onthe individual basis, for example, someone like, uh, jeffreydahlmer,by putting him in prison for life, there is still a possi-bility that he will get out again.I don?t think he could ever redeem himself,but if you look at who gets accused and who are theones who actually get executed, it?s very racially related?
and ethnically related[Abstractive Summary]B is against capital punishment except under certaincircumstances.
B finds that crimes deserving of capitalpunishment are ?crimes of the moment?
and as a resultfeels that capital punishment is not an effective deterrent.however, B also recognizes that on an individual basissome criminals can never ?redeem?
themselves.
[Overall Opinion]Somewhat againstTable 2 shows the compression ratio of the extrac-tive summaries and abstractive summaries as well astheir standard deviation.
Because in conversations,utterance length varies a lot, we use words as unitswhen calculating the compression ratio.avg ratio stdevextractive summaries 0.26 0.13abstractive summaries 0.13 0.06Table 2: Compression ratio and standard deviation of ex-tractive and abstractive summaries.We measured the inter-annotator agreementamong the three annotators for the 18 conversations(each has two speakers, thus 36 ?documents?
in to-tal).
Results are shown in Table 3.
For the ex-tractive or abstractive summaries, we use ROUGEscores (Lin, 2004), a metric used to evaluate auto-matic summarization performance, to measure thepairwise agreement of summaries from different an-notators.
ROUGE F-scores are shown in the tablefor different matches, unigram (R-1), bigram (R-2),and longest subsequence (R-L).
For the overall opin-ion category, since it is a multiclass label (not binarydecision), we use Krippendorff?s ?
coefficient tomeasure human agreement, and the difference func-tion for interval data: ?2ck = (c?
k)2 (where c, k arethe interval values, on a scale of 1 to 5 correspondingto the five categories for the overall opinion).We notice that the inter-annotator agreement forextractive summaries is comparable to other speech333extractive summariesR-1 0.61R-2 0.52R-L 0.61abstractive summariesR-1 0.32R-2 0.13R-L 0.25overall opinion ?
= 0.79Table 3: Inter-annotator agreement for extractive and ab-stractive summaries, and overall opinion.summary annotation (Liu and Liu, 2008).
Theagreement on abstractive summaries is much lowerthan extractive summaries, which is as expected.Even for the same opinion or sentence, annotatorsuse different words in the abstractive summaries.The agreement for the overall opinion annotationis similar to other opinion/emotion studies (Wil-son, 2008b), but slightly lower than the level rec-ommended by Krippendorff for reliable data (?
=0.8) (Hayes and Krippendorff, 2007), which showsit is even difficult for humans to determine whatopinion a person holds (support or against some-thing).
Often human annotators have different inter-pretations about the same sentence, and a speaker?sopinion/attitude is sometimes ambiguous.
Thereforethis also demonstrates that it is more appropriate toprovide a summary rather than a simple opinion cat-egory to answer questions about a person?s opiniontowards something.4 Opinion Summarization MethodsAutomatic summarization can be divided into ex-tractive summarization and abstractive summariza-tion.
Extractive summarization selects sentencesfrom the original documents to form a summary;whereas abstractive summarization requires genera-tion of new sentences that represent the most salientcontent in the original documents like humans do.Often extractive summarization is used as the firststep to generate abstractive summary.As a pilot study for the problem of opinion sum-marization in conversations, we treat this problemas an extractive summarization task.
This sectiondescribes two approaches we have explored in gen-erating extractive summaries.
The first one is asentence-ranking method, in which we measure thesalience of each sentence according to a linear com-bination of scores from several dimensions.
The sec-ond one is a graph-based method, which incorpo-rates the dialogue structure in ranking.
We choose toinvestigate these two methods since they have beenwidely used in text and speech summarization, andperform competitively.
In addition, they do not re-quire a large labeled data set for modeling training,as needed in some classification or feature basedsummarization approaches.4.1 Sentence RankingIn this method, we use Equation 1 to assign a scoreto each DA s, and select the most highly ranked onesuntil the length constriction is satisfied.score(s) = ?simsim(s,D) + ?relREL(s, topic)+?sentsentiment(s) + ?lenlength(s)?i?i = 1 (1)?
sim(s,D) is the cosine similarity between DAs and all the utterances in the dialogue fromthe same speaker, D. It measures the rele-vancy of s to the entire dialogue from the tar-get speaker.
This score is used to represent thesalience of the DA.
It has been shown to be animportant indicator in summarization for var-ious domains.
For cosine similarity measure,we use TF*IDF (term frequency, inverse docu-ment frequency) term weighting.
The IDF val-ues are obtained using the entire Switchboardcorpus, treating each conversation as a docu-ment.?
REL(s, topic) measures the topic relevance ofDA s. It is the sum of the topic relevance of allthe words in the DA.
We only consider the con-tent words for this measure.
They are identifiedusing TreeTagger toolkit.2 To measure the rel-evance of a word to a topic, we use PairwiseMutual Information (PMI):PMI(w, topic) = log2p(w&topic)p(w)p(topic)(2)2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html334where all the statistics are collected from theSwitchboard corpus: p(w&topic) denotes theprobability that word w appears in a dialogueof topic t, and p(w) is the probability of w ap-pearing in a dialogue of any topic.
Since ourgoal is to rank DAs in the same dialog, andthe topic is the same for all the DAs, we dropp(topic) when calculating PMI scores.
Be-cause the value of PMI(w, topic) is negative,we transform it into a positive one (denotedby PMI+(w, topic)) by adding the absolutevalue of the minimum value.
The final rele-vance score of each sentence is normalized to[0, 1] using linear normalization:RELorig(s, topic) =?w?sPMI+(w, topic)REL(s, topic) =RELorig(s, topic)?MinMax?Min?
sentiment(s) indicates the probability that ut-terance s contains opinion.
To obtain this,we trained a maximum entropy classifier witha bag-of-words model using a combinationof data sets from several domains, includingmovie data (Pang and Lee, 2004), news articlesfrom MPQA corpus (Wilson and Wiebe, 2003),and meeting transcripts from AMI corpus (Wil-son, 2008a).
Each sentence (or DA) in thesecorpora is annotated as ?subjective?
or ?objec-tive?.
We use each utterance?s probability ofbeing ?subjective?
predicted by the classifier asits sentiment score.?
length(s) is the length of the utterance.
Thisscore can effectively penalize the short sen-tences which typically do not contain muchimportant content, especially the backchannelsthat appear frequently in dialogues.
We alsoperform linear normalization such that the finalvalue lies in [0, 1].4.2 Graph-based SummarizationGraph-based methods have been widely used in doc-ument summarization.
In this approach, a documentis modeled as an adjacency matrix, where each noderepresents a sentence, and the weight of the edge be-tween each pair of sentences is their similarity (co-sine similarity is typically used).
An iterative pro-cess is used until the scores for the nodes converge.Previous studies (Erkan and Radev, 2004) showedthat this method can effectively extract importantsentences from documents.
The basic framework weuse in this study is similar to the query-based graphsummarization system in (Zhao et al, 2009).
Wealso consider sentiment and topic relevance infor-mation, and propose to incorporate information ob-tained from dialog structure in this framework.
Thescore for a DA s is based on its content similaritywith all other DAs in the dialogue, the connectionwith other DAs based on the dialogue structure, thetopic relevance, and its subjectivity, that is:score(s) = ?sim?v?Csim(s, v)?z?C sim(z, v)score(v)+?relREL(s, topic)?z?C REL(z, topic)+?sentsentiment(s)?z?C sentiment(z)+?adj?v?CADJ(s, v)?z?C ADJ(z, v)score(v)?i?i = 1 (3)where C is the set of all DAs in the dialogue;REL(s, topic) and sentiment(s) are the sameas those in the above sentence ranking method;sim(s, v) is the cosine similarity between two DAss and v. In addition to the standard connection be-tween two DAs with an edge weight sim(s, v), weintroduce new connections ADJ(s, v) to model di-alog structure.
It is a directed edge from s to v, de-fined as follows:?
If s and v are from the same speaker and withinthe same turn, there is an edge from s to v andan edge from v to s with weight 1/dis(s, v)(ADJ(s, v) = ADJ(v, s) = 1/dis(s, v)),where dis(s, v) is the distance between s andv, measured based on their DA indices.
Thisway the DAs in the same turn can reinforceeach other.
For example, if we consider that335one DA is important, then the other DAs in thesame turn are also important.?
If s and v are from the same speaker, andseparated only by one DA from anotherspeaker with length less than 3 words (usu-ally backchannel), there is an edge from s tov as well as an edge from v to s with weight 1(ADJ(s, v) = ADJ(v, s) = 1).?
If s and v form a question-answer pair from twospeakers, then there is an edge from question sto answer v with weight 1 (ADJ(s, v) = 1).We use a simple rule-based method to deter-mine question-answer pairs ?
sentence s hasquestion marks or contains ?wh-word?
(i.e.,?what, how, why?
), and sentence v is the im-mediately following one.
The motivation foradding this connection is, if the score of a ques-tion sentence is high, then the answer?s score isalso boosted.?
If s and v form an agreement or disagreementpair, then there is an edge from v to s withweight 1 (ADJ(v, s) = 1).
This is also de-termined by simple rules: sentence v containsthe word ?agree?
or ?disagree?, s is the previ-ous sentence, and from a different speaker.
Thereason for adding this is similar to the abovequestion-answer pairs.?
If there are multiple edges generated from theabove steps between two nodes, then we use thehighest weight.Since we are using a directed graph for the sen-tence connections to model dialog structure, the re-sulting adjacency matrix is asymmetric.
This is dif-ferent from the widely used graph methods for sum-marization.
Also note that in the first sentence rank-ing method or the basic graph methods, summariza-tion is conducted for each speaker separately.
Ut-terances from one speaker have no influence on thesummary decision for the other speaker.
Here in ourproposed graph-based method, we introduce con-nections between the two speakers, so that the adja-cency pairs between them can be utilized to extractsalient utterances.5 Experiments5.1 Experimental SetupThe 18 conversations annotated by all 3 annotatorsare used as test set, and the rest of 70 conversa-tions are used as development set to tune the param-eters (determining the best combination weights).
Inpreprocessing we applied word stemming.
We per-form extractive summarization using different wordcompression ratios (ranging from 10% to 25%).
Weuse human annotated dialogue acts (DA) as the ex-traction units.
The system-generated summaries arecompared to human annotated extractive and ab-stractive summaries.
We use ROUGE as the eval-uation metrics for summarization performance.We compare our methods to two systems.
Thefirst one is a baseline system, where we select thelongest utterances for each speaker.
This has beenshown to be a relatively strong baseline for speechsummarization (Gillick et al, 2009).
The secondone is human performance.
We treat each annota-tor?s extractive summary as a system summary, andcompare to the other two annotators?
extractive andabstractive summaries.
This can be considered asthe upper bound of our system performance.5.2 ResultsFrom the development set, we used the grid searchmethod to obtain the best combination weights forthe two summarization methods.
In the sentence-ranking method, the best parameters found on thedevelopment set are ?sim = 0, ?rel = 0.3, ?sent =0.3, ?len = 0.4.
It is surprising to see that the sim-ilarity score is not useful for this task.
The possiblereason is, in Switchboard conversations, what peo-ple talk about is diverse and in many cases only topicwords (except stopwords) appear more than once.
Inaddition, REL score is already able to catch the topicrelevancy of the sentence.
Thus, the similarity scoreis redundant here.In the graph-based method, the best parametersare ?sim = 0, ?adj = 0.3, ?rel = 0.4, ?sent = 0.3.The similarity between each pair of utterances isalso not useful, which can be explained with similarreasons as in the sentence-ranking method.
This isdifferent from graph-based summarization systemsfor text domains.
A similar finding has also beenshown in (Garg et al, 2009), where similarity be-3363843485358630.10.150.20.25compressionratioROUGE-1(%)max-lengthsentence-rankinggraphhuman(a) compare to reference extractive summary17192123252729310.10.150.20.25compressionratioROUGE-1(%)max-lengthsentence-rankinggraphhuman(b) compare to reference abstractive summaryFigure 1: ROUGE-1 F-scores compared to extractiveand abstractive reference summaries for different sys-tems: max-length, sentence-ranking method, graph-based method, and human performance.tween utterances does not perform well in conversa-tion summarization.Figure 1 shows the ROUGE-1 F-scores compar-ing to human extractive and abstractive summariesfor different compression ratios.
Similar patterns areobserved for other ROUGE scores such as ROUGE-2 or ROUGE-L, therefore they are not shown here.Both methods improve significantly over the base-line approach.
There is relatively less improvementusing a higher compression ratio, compared to alower one.
This is reasonable because when thecompression ratio is low, the most salient utterancesare not necessarily the longest ones, thus using moreinformation sources helps better identify importantsentences; but when the compression ratio is higher,longer utterances are more likely to be selected sincethey contain more content.There is no significant difference between the twomethods.
When compared to extractive referencesummaries, sentence-ranking is slightly better ex-cept for the compression ratio of 0.1.
When com-pared to abstractive reference summaries, the graph-based method is slightly better.
The two systemsshare the same topic relevance score (REL) andsentiment score, but the sentence-ranking methodprefers longer DAs and the graph-based methodprefers DAs that are emphasized by the ADJ ma-trix, such as the DA in the middle of a cluster ofutterances from the same speaker, the answer to aquestion, etc.5.3 AnalysisTo analyze the effect of dialogue structure we in-troduce in the graph-based summarization method,we compare two configurations: ?adj = 0 (only us-ing REL score and sentiment score in ranking) and?adj = 0.3.
We generate summaries using these twosetups and compare with human selected sentences.Table 4 shows the number of false positive instances(selected by system but not by human) and false neg-ative ones (selected by human but not by system).We use all three annotators?
annotation as reference,and consider an utterance as positive if one annotatorselects it.
This results in a large number of referencesummary DAs (because of low human agreement),and thus the number of false negatives in the systemoutput is very high.
As expected, a smaller compres-sion ratio (fewer selected DAs in the system output)yields a higher false negative rate and a lower falsepositive rate.
From the results, we can see that gen-erally adding adjacency matrix information is ableto reduce both types of errors except when the com-pression ratio is 0.15.The following shows an example, where the thirdDA is selected by the system with ?adj = 0.3, butnot by ?adj = 0.
This is partly because the weightof the second DA is enhanced by the the question-337?adj = 0 ?adj = 0.3ratio FP FN FP FN0.1 37 588 33 5810.15 60 542 61 5460.2 100 516 90 5110.25 137 489 131 482Table 4: The number of false positive (FP) and false neg-ative (FN) instances using the graph-based method with?adj = 0 and ?adj = 0.3 for different compression ratios.answer pair (the first and the second DA), and thussubsequently boosting the score of the third DA.A: Well what do you think?B: Well, I don?t know, I?m thinking about from one toten what my no would be.B: It would probably be somewhere closer to, uh, lesscontrol because I don?t see, -We also examined the system output and humanannotation and found some reasons for the systemerrors:(a) Topic relevance measure.
We use the statis-tics from the Switchboard corpus to measure the rel-evance of each word to a given topic (PMI score),therefore only when people use the same word indifferent conversations of the topic, the PMI score ofthis word and the topic is high.
However, since thesize of the corpus is small, some topics only con-tain a few conversations, and some words only ap-pear in one conversation even though they are topic-relevant.
Therefore the current PMI measure cannotproperly measure a word?s and a sentence?s topicrelevance.
This problem leads to many false neg-ative errors (relevant sentences are not captured byour system).
(b) Extraction units.
We used DA segments asunits for extractive summarization, which can beproblematic.
In conversational speech, sometimesa DA segment is not a complete sentence becauseof overlaps and interruptions.
We notice that anno-tators tend to select consecutive DAs that constitutea complete sentence, however, since each individualDA is not quite meaningful by itself, they are oftennot selected by the system.
The following segmentis extracted from a dialogue about ?universal healthinsurance?.
The two DAs from speaker B are notselected by our system but selected by human anno-tators, causing false negative errors.B: and it just can devastate ?A: and your constantly, -B: ?
your budget, you know.6 Conclusion and Future WorkThis paper investigates two unsupervised methodsin opinion summarization on spontaneous conver-sations by incorporating topic score and sentimentscore in existing summarization techniques.
In thesentence-ranking method, we linearly combine sev-eral scores in different aspects to select sentenceswith the highest scores.
In the graph-based method,we use an adjacency matrix to model the dialoguestructure and utilize it to find salient utterances inconversations.
Our experiments show that bothmethods are able to improve the baseline approach,and we find that the cosine similarity between utter-ances or between an utterance and the whole docu-ment is not as useful as in other document summa-rization tasks.In future work, we will address some issues iden-tified from our error analysis.
First, we will in-vestigate ways to represent a sentence?s topic rel-evance.
Second, we will evaluate using other ex-traction units, such as applying preprocessing to re-move disfluencies and concatenate incomplete sen-tence segments together.
In addition, it would beinteresting to test our system on speech recognitionoutput and automatically generated DA boundariesto see how robust it is.7 AcknowledgmentsThe authors thank Julia Hirschberg and AniNenkova for useful discussions.
This research issupported by NSF awards CNS-1059226 and IIS-0939966.ReferencesAlexandra Balahur, Ester Boldrini, Andre?s Montoyo, andPatricio Mart??nez-Barco.
2010.
Going beyond tra-ditional QA systems: challenges and keys in opinionquestion answering.
In Proceedings of COLING.Gu?nes Erkan and Dragomir R. Radev.
2004.
LexRank:graph-based lexical centrality as salience in text sum-marization.
Journal of Artificial Intelligence Re-search.338Sadaoki Furui, Tomonori Kikuchi, Yousuke Shinnaka,and Chior i Hori.
2004.
Speech-to-text and speech-to-speech summarization of spontaneous speech.
IEEETransactions on Audio, Speech & Language Process-ing, 12(4):401?408.Nikhil Garg, Benoit Favre, Korbinian Reidhammer, andDilek Hakkani Tu?r.
2009.
ClusterRank: a graphbased method for meeting summarization.
In Proceed-ings of Interspeech.Dan Gillick, Korbinian Riedhammer, Benoit Favre, andDilek Hakkani-Tur.
2009.
A global optimizationframework for meeting summarization.
In Proceed-ings of ICASSP.John J. Godfrey and Edward Holliman.
1997.Switchboard-1 Release 2.
In Linguistic Data Consor-tium, Philadelphia.Andrew Hayes and Klaus Krippendorff.
2007.
Answer-ing the call for a standard reliability measure for cod-ing data.
Journal of Communication Methods andMeasures, 1:77?89.Minqing Hu and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In Proceedings of ACMSIGKDD.Konstantinos Koumpis and Steve Renals.
2005.
Auto-matic summarization of voicemail messages using lex-ical and prosodic features.
ACM - Transactions onSpeech and Language Processing.Shih Hsiang Lin, Berlin Chen, and Hsin min Wang.2009.
A comparative study of probabilistic rankingmodels for chinese spoken document summarization.ACM Transactions on Asian Language InformationProcessing, 8(1).Chin-Yew Lin.
2004.
ROUGE: a package for auto-matic evaluation of summaries.
In Proceedings of ACLworkshop on Text Summarization Branches Out.Fei Liu and Yang Liu.
2008.
What are meeting sum-maries?
An analysis of human extractive summariesin meeting corpus.
In Proceedings of SIGDial.Sameer Maskey and Julia Hirschberg.
2005.
Com-paring lexical, acoustic/prosodic, structural and dis-course features for speech summarization.
In Pro-ceedings of Interspeech.Kathleen Mckeown, Julia Hirschberg, Michel Galley, andSameer Maskey.
2005.
From text to speech summa-rization.
In Proceedings of ICASSP.Gabriel Murray and Giuseppe Carenini.
2009.
Detectingsubjectivity in multiparty speech.
In Proceedings ofInterspeech.Gabriel Murray, Steve Renals, and Jean Carletta.
2005.Extractive summarization of meeting recordings.
InProceedings of EUROSPEECH.Vincent Ng, Sajib Dasgupta, and S.M.Niaz Arifin.
2006.Examining the role of linguistic knowledge sources inthe automatic identification and classification of re-views.
In Proceedings of the COLING/ACL.Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Mat-suo, and Genichiro Kikui.
2010.
Opinion summariza-tion with integer linear programming formulation forsentence extraction and ordering.
In Proceedings ofCOLING.Bo Pang and Lilian Lee.
2004.
A sentiment educa-tion: sentiment analysis using subjectivity summariza-tion based on minimum cuts.
In Proceedings of ACL.Michael Paul, ChengXiang Zhai, and Roxana Girju.2010.
Summarizing contrastive viewpoints in opinion-ated text.
In Proceedings of EMNLP.Ana-Maria Popescu and Oren Etzioni.
2005.
Extractingproduct features and opinions from reviews.
In Pro-ceedings of HLT-EMNLP.Stephan Raaijmakers, Khiet Truong, and Theresa Wilson.2008.
Multimodal subjectivity analysis of multipartyconversation.
In Proceedings of EMNLP.Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.2005.
Multi-perspective question answering using theOpQA corpus.
In Proceedings of EMNLP/HLT.Janyce Wiebe and Ellen Riloff.
2005.
Creating sub-jective and objective sentence classifiers from unan-notated texts.
In Proceedings of CICLing.Theresa Wilson and Janyce Wiebe.
2003.
Annotatingopinions in the world press.
In Proceedings of SIG-Dial.Theresa Wilson.
2008a.
Annotating subjective content inmeetings.
In Proceedings of LREC.Theresa Wilson.
2008b.
Fine-grained subjectivity andsentiment analysis: recognizing the intensity, polarity,and attitudes of private states.
Ph.D. thesis, Universityof Pittsburgh.Shasha Xie and Yang Liu.
2010.
Improving super-vised learning for meeting summarization using sam-pling and regression.
Computer Speech and Lan-guage, 24:495?514.Klaus Zechner.
2002.
Automatic summarization ofopen-domain multiparty dialogues in dive rse genres.Computational Linguistics, 28:447?485.Justin Jian Zhang, Ho Yin Chan, and Pascale Fung.
2007.Improving lecture speech summarization using rhetor-ical information.
In Proceedings of Biannual IEEEWorkshop on ASRU.Lin Zhao, Lide Wu, and Xuanjing Huang.
2009.
Usingquery expansion in graph-based approach for query-focused multi-document summarization.
Journal ofInformation Processing and Management.Xiaodan Zhu and Gerald Penn.
2006.
Summarization ofspontaneous conversations.
In Proceedings of Inter-speech.339
