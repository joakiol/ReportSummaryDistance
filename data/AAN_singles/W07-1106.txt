Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 41?48,Prague, June 2007. c?2007 Association for Computational LinguisticsPulling their Weight: Exploiting Syntactic Forms for the AutomaticIdentification of Idiomatic Expressions in ContextPaul Cook and Afsaneh Fazly and Suzanne StevensonDepartment of Computer ScienceUniversity of TorontoToronto, Canadafpcook,afsaneh,suzanneg@cs.toronto.eduAbstractMuch work on idioms has focused on typeidentification, i.e., determining whether a se-quence of words can form an idiomatic ex-pression.
Since an idiom type often has aliteral interpretation as well, token classifi-cation of potential idioms in context is criti-cal for NLP.
We explore the use of informa-tive prior knowledge about the overall syn-tactic behaviour of a potentially-idiomaticexpression (type-based knowledge) to de-termine whether an instance of the expres-sion is used idiomatically or literally (token-based knowledge).
We develop unsuper-vised methods for the task, and show thattheir performance is comparable to that ofstate-of-the-art supervised techniques.1 IntroductionIdentification of multiword expressions (MWEs),such as car park, make a decision, and kick thebucket, is extremely important for accurate naturallanguage processing (NLP) (Sag et al, 2002).
MostMWEs need to be treated as single units of mean-ing, e.g., make a decision roughly means ?decide?.Nonetheless, the components of an MWE can beseparated, making it hard for an NLP system to iden-tify the expression as a whole.
Many researchershave recently developed methods for the automaticacquisition of various properties of MWEs from cor-pora (Lin, 1999; Krenn and Evert, 2001; Baldwin etal., 2003; McCarthy et al, 2003; Venkatapathy andJoshi, 2005; Villada Moiro?n and Tiedemann, 2006;Fazly and Stevenson, 2006).
These studies lookinto properties, such as the collocational behaviourof MWEs, their semantic non-compositionality, andtheir lexicosyntactic fixedness, in order to distin-guish them from similar-on-the-surface literal com-binations.Most of these methods have been aimed at rec-ognizing MWE types; less attention has been paidto the identification of instances (tokens) of MWEsin context.
For example, most such techniques (ifsuccessful) would identify make a face as a poten-tial MWE.
This expression is, however, ambiguousbetween an idiom, as in The little girl made a funnyface at her mother, and a literal combination, as inShe made a face on the snowman using a carrot andtwo buttons.
Despite the common perception thatphrases that can be idioms are mainly used in theiridiomatic sense, our analysis of 60 idioms has shownotherwise.
We found that close to half of these id-ioms also have a clear literal meaning; and of the ex-pressions with a literal meaning, on average around40% of their usages are literal.
Distinguishing tokenphrases as MWEs or literal combinations of words isthus essential for NLP applications that require theidentification of multiword semantic units, such assemantic parsing and machine translation.Recent studies addressing MWE token classifi-cation mainly perform the task as one of wordsense disambiguation, and draw on the local con-text of an expression to disambiguate it.
Suchtechniques either do not use any information re-garding the linguistic properties of MWEs (Birkeand Sarkar, 2006), or mainly focus on their non-compositionality (Katz and Giesbrecht, 2006).
Pre-41vious work on the identification of MWE types,however, has found other properties of MWEs, suchas their syntactic fixedness, to be relevant to theiridentification (Evert et al, 2004; Fazly and Steven-son, 2006).
In this paper, we propose techniques thatdraw on this property to classify individual tokens ofa potentially idiomatic phrase as literal or idiomatic.We also put forward classification techniques thatcombine such information with evidence from thelocal context of an MWE.We explore the hypothesis that informative priorknowledge about the overall syntactic behaviour ofan idiomatic expression (type-based knowledge) canbe used to determine whether an instance of theexpression is used literally or idiomatically (token-based knowledge).
Based on this hypothesis, we de-velop unsupervised methods for token classification,and show that their performance is comparable tothat of a standard supervised method.Many verbs can be combined with one or more oftheir arguments to form MWEs (Cowie et al, 1983;Fellbaum, 2002).
Here, we focus on a broadly doc-umented class of idiomatic MWEs that are formedfrom the combination of a verb with a noun in its di-rect object position, as in make a face.
In the restof the paper, we refer to these verb+noun combi-nations, which are potentially idiomatic, as VNCs.In Section 2, we propose unsupervised methods thatclassify a VNC token as an idiomatic or literal usage.Section 3 describes our experimental setup, includ-ing experimental expressions and their annotation.In Section 4, we present a detailed discussion of ourresults.
Section 5 compares our work with similarprevious studies, and Section 6 concludes the paper.2 Unsupervised Idiom IdentificationWe first explain an important linguistic property at-tributed to idioms?that is, their syntactic fixedness(Section 2.1).
We then propose unsupervised meth-ods that draw on this property to automatically dis-tinguish between idiomatic and literal usages of anexpression (Section 2.2).2.1 Syntactic Fixedness and Canonical FormsIdioms tend to be somewhat fixed with respect tothe syntactic configurations in which they occur(Nunberg et al, 1994).
For example, pull one?sweight tends to mainly appear in this form whenused idiomatically.
Other forms of the expression,such as pull the weights, typically are only usedwith a literal meaning.
In their work on automati-cally identifying idiom types, Fazly and Stevenson(2006)?henceforth FS06?show that an idiomaticVNC tends to have one (or at most a small numberof) canonical form(s), which are its most preferredsyntactic patterns.
The preferred patterns can varyacross different idiom types, and can involve a num-ber of syntactic properties: the voice of the verb (ac-tive or passive), the determiner introducing the noun(the, one?s, etc.
), and the number of the noun (singu-lar or plural).
For example, while pull one?s weighthas only one canonical form, hold fire and hold one?sfire are two canonical forms of the same idiom, aslisted in an idiom dictionary (Seaton and Macaulay,2002).In our work, we assume that in most cases, id-iomatic usages of an expression tend to occur in asmall number of canonical form(s) for that idiom.We also assume that, in contrast, the literal usagesof an expression are less syntactically restricted, andare expressed in a greater variety of patterns.
Be-cause of their relative unrestrictiveness, literal us-ages may occur in a canonical idiomatic form forthat expression, but usages in a canonical form aremore likely to be idiomatic.
Usages in alternativesyntactic patterns for the expression, which we referto as the non-canonical forms of the idiom, are morelikely to be literal.
Drawing on these assumptions,we develop three unsupervised methods that deter-mine, for each VNC token in context, whether it hasan idiomatic or a literal interpretation.2.2 Statistical MethodsThe following paragraphs elaborate on our proposedmethods for identifying the idiomatic and literal us-ages of a VNC: the CForm method that uses knowl-edge of canonical forms only, and two Diff methodsthat draw on further contextual evidence as well.
Allthree methods draw on our assumptions describedabove, that usages in the canonical form for an id-iom are more likely to be idiomatic, and those inother forms are more likely to be literal.
Thus, forall three methods, we need access to the canonicalform of the idiom.
Since we want our token iden-tification methods to be unsupervised, we adopt the42unsupervised statistical method of FS06 for findingcanonical forms for an idiomatic VNC.
This methoddetermines the canonical forms of an expression tobe those forms whose frequency is much higher thanthe average frequency of all its forms.CForm: The underlying assumption of thismethod is that information about the canonicalform(s) of an idiom type is extremely informativein classifying the meaning of its individual instances(tokens) as literal or idiomatic.
Our CForm classi-fies a token as idiomatic if it occurs in the automat-ically determined canonical form(s) for that expres-sion, and as literal otherwise.Di : Our two Di methods combine local con-text information with knowledge about the canon-ical forms of an idiom type to determine if its to-ken usages are literal or idiomatic.
In developingthese methods, we adopt a distributional approachto meaning, where the meaning of an expression isapproximated by the words with which it co-occurs(Firth, 1957).
Although there may be fine-graineddifferences in meaning across the idiomatic usagesof an expression, as well as across its literal usages,we assume that the idiomatic and literal usages cor-respond to two coarse-grained senses of the expres-sion.
Since we further assume these two groupsof usages will have more in common semanticallywithin each group than between the two groups, weexpect that literal and idiomatic usages of an ex-pression will typically occur with different sets ofwords.
We will refer then to each of the literal andidiomatic designations as a (coarse-grained) mean-ing of the expression, while acknowledging thateach may have multiple fine-grained senses.
Clearly,the success of our method depends on the extent towhich these assumptions hold.We estimate the meaning of a set of usages of anexpression e as a word frequency vector ~vewhereeach dimension i of ~veis the frequency with whiche co-occurs with word i across the usages of e. Wesimilarly estimate the meaning of a single token ofan expression t as a vector ~vtcapturing that usage.To determine if an instance of an expression is literalor idiomatic, we compare its co-occurrence vector tothe co-occurrence vectors representing each of theliteral and idiomatic meanings of the expression.
Weuse a standard measure of distributional similarity,cosine, to compare co-occurrence vectors.In supervised approaches, such as that of Katz andGiesbrecht (2006), co-occurrence vectors for literaland idiomatic meanings are formed from manually-annotated training data.
Here, we propose unsuper-vised methods for estimating these vectors.
We useone way of estimating the idiomatic meaning of anexpression, and two ways for estimating its literalmeaning, yielding two methods for token classifica-tion.Our first Diff method draws further on our expec-tation that canonical forms are more likely idiomaticusages, and non-canonical forms are more likely lit-eral usages.
We estimate the idiomatic meaning ofan expression by building a co-occurrence vector,~vI -CF, for all uses of the expression in its auto-matically determined canonical form(s).
Since wehypothesize that idiomatic usages of an expressiontend to occur in its canonical form, we expect theseco-occurrence vectors to be largely representative ofthe idiomatic usage of the expression.
We similarlyestimate the literal meaning by constructing a co-occurrence vector, ~vL-NCF, of all uses of the expres-sion in its non-canonical forms.
We use the termDiI-CF;L-NCFto refer to this method.Our second Diff method also uses the vector~vI -CFto estimate the idiomatic meaning of an ex-pression.
However, this approach follows that ofKatz and Giesbrecht (2006) in assuming that literalmeanings are compositional.
The literal meaning ofan expression is thus estimated by composing (sum-ming and then normalizing) the co-occurrence vec-tors for its component words.
The resulting vec-tor is referred to as ~vL-Comp, and this method asDiI-CF;L-Comp.For both Diff methods, if the meaning ofan instance of an expression is determined tobe more similar to its idiomatic meaning (e.g.,cosine (~vt; ~vI-CF) > cosine (~vt; ~vL-NCF)), thenwe label it as an idiomatic usage.
Otherwise, it islabeled as literal.11We also performed experiments using a KNN classifierin which the co-occurrence vector for a token was comparedagainst the co-occurrence vectors for the canonical and non-canonical forms of that expression, which were assumed tobe idiomatic and literal usages respectively.
However, perfor-mance was generally worse using this method.43Note that all three of our proposed techniques fortoken identification depend on how accurately thecanonical forms of an expression can be acquired.FS06?s canonical form acquisition technique, whichwe use here, works well if the idiomatic usage ofa VNC is sufficiently frequent compared to its lit-eral usage.
In our experiments, we examine theperformance of our proposed classification methodsfor VNCs with different proportions of idiomatic-to-literal usages.3 Experimental Setup3.1 Experimental Expressions and AnnotationWe use data provided by FS06, which consists of alist of VNCs and their canonical forms.
From thisdata, we discarded expressions whose frequency inthe British National Corpus2 (BNC) is lower than20, in an effort to make sure that there would be lit-eral and idiomatic usages of each expression.
Thefrequency cut-off further ensures an accurate esti-mate of the vectors representing each of the lit-eral and idiomatic meanings of the expression.
Wealso discarded expressions that were not found in atleast one of two dictionaries of idioms (Seaton andMacaulay, 2002; Cowie et al, 1983).
This processresulted in the selection of 60 candidate expressions.For each of these 60 expressions, 100 sentencescontaining its usage were randomly selected fromthe automatically parsed BNC (Collins, 1999), usingthe automatic VNC identification method describedby FS06.
For an expression which occurs less than100 times in the BNC, all of its usages were ex-tracted.
Our primary judge, a native English speakerand an author of this paper, then annotated each useof each candidate expression as one of literal, id-iomatic, or unknown.
When annotating a token, thejudge had access to only the sentence in which it oc-curred, and not the surrounding sentences.
If thiscontext was insufficient to determine the class of theexpression, the judge assigned the unknown label.Idiomaticity is not a binary property, rather it isknown to fall on a continuum from completely se-mantically transparent, or literal, to entirely opaque,or idiomatic.
The human annotators were requiredto pick the label, literal or idiomatic, that best fit the2http://www.natcorp.ox.ac.ukusage in their judgment; they were not to use the un-known label for intermediate cases.
Figurative ex-tensions of literal meanings were classified as literalif their overall meaning was judged to be fairly trans-parent, as in You turn right when we hit the road atthe end of this track (taken from the BNC).
Some-times an idiomatic usage, such as had words in Iwas in a bad mood, and he kept pestering me, sowe had words, is somewhat directly related to itsliteral meaning, which is not the case for more se-mantically opaque idioms such as hit the roof.
Theabove sentence was classified as idiomatic since theidiomatic meaning is much more salient than the lit-eral meaning.Based on the primary judge?s annotations, we re-moved expressions with fewer than 5 instances ofeither of their literal or idiomatic meanings, leav-ing 28 expressions.
The remaining expressions werethen split into development (DEV) and test (TEST)sets of 14 expressions each.
The data was dividedsuch that DEV and TEST would be approximatelyequal with respect to the frequency, and proportionof idiomatic-to-literal usages, of their expressions.Before consensus annotation, DEV and TEST con-tained a total of 813 and 743 tokens, respectively.A second human judge, also a native English-speaking author of this paper, then annotated DEVand TEST.
The observed agreement and unweightedkappa score on TEST were 76% and 0:62 respec-tively.
The judges discussed tokens on which theydisagreed to achieve a consensus annotation.
Finalannotations were generated by removing tokens thatreceived the unknown label as the consensus anno-tation, leaving DEV and TEST with a total of 573 and607 tokens, and an average of 41 and 43 tokens perexpression, respectively.3.2 Creation of Co-occurrence VectorsWe create co-occurrence vectors for each expressionin our study from counts in the BNC.
We form co-occurrence vectors for the following items. Each token instance of the target expression The target expression in its automatically deter-mined canonical form(s) The target expression in its non-canonicalform(s)44 The verb in the target expression The noun in the target expressionThe co-occurrence vectors measure the frequencywith which the above items co-occur with each of1000 content bearing words in the same sentence.3The content bearing words were chosen to be themost frequent words in the BNC which are used asa noun, verb, adjective, adverb, or determiner.
Al-though determiners are often in a typical stoplist, wefelt it would be beneficial to use them here.
Deter-miners have been shown to be very informative inrecognizing the idiomaticity of MWE types, as theyare incorporated in the patterns used to automati-cally determine canonical forms (Fazly and Steven-son, 2006).43.3 Evaluation and BaselineOur baseline for comparison is that of always pre-dicting an idiomatic label, the most frequent classin our development data.
We also compare our un-supervised methods against the supervised methodproposed by Katz and Giesbrecht (2006).
In thisstudy, co-occurrence vectors for the tokens wereformed from uses of a German idiom manually an-notated as literal or idiomatic.
Tokens were classi-fied in a leave-one-out methodology using k-nearestneighbours, with k = 1.
We report results using thismethod (1NN) as well as one which considers a to-ken?s 5 nearest neighbours (5NN).
In all cases, wereport the accuracy macro-averaged across the ex-perimental expressions.4 Experimental Results and AnalysisIn Section 4.1, we discuss the overall performanceof our proposed unsupervised methods.
Section 4.2explores possible causes of the differences observedin the performance of the methods.
We examineour estimated idiomatic and literal vectors, and com-pare them with the actual vectors calculated from3We also considered 10 and 20 word windows on either sideof the target expression, but experiments on development dataindicated that using the sentence as a window performed better.4We employed singular value decomposition (Deerwester etal., 1990) to reduce the dimensionality of the co-occurrencevectors.
This had a negative effect on the results, likely be-cause information about determiners, which occur frequentlywith many expressions, is lost in the dimensionality reduction.Method %Acc (%RER)Baseline 61.9 -Unsupervised DiI -CF ; L-Comp67.8 (15.5)DiI -CF ; L-NCF70.1 (21.5)CForm 72.4 (27.6)Supervised 1NN 72.4 (27.6)5NN 76.2 (37.5)Table 1: Macro-averaged accuracy (%Acc) and relative errorreduction (%RER) over TEST.manually-annotated data.
Results reported in Sec-tions 4.1 and 4.2 are on TEST (results on DEV havevery similar trends).
Section 4.3 then examines theperformance of the unsupervised methods on ex-pressions with different proportions of idiomatic-to-literal usages.
This section presents results on TESTand DEV combined, as explained below.4.1 Overall PerformanceTable 4.1 shows the macro-averaged accuracy onTEST of our three unsupervised methods, as well asthat of the baseline and the two supervised methodsfor comparison (see Section 3.3).
The best super-vised performance and the best unsupervised perfor-mance are indicated in boldface.
As the table shows,all three unsupervised methods outperform the base-line, confirming that the canonical forms of an ex-pression, and local context, are both informative indistinguishing literal and idiomatic instances of theexpression.The table also shows that DiI -CF ;L-NCFper-forms better than DiI -CF ;L-Comp.
This suggeststhat estimating the literal meaning of an expressionusing the non-canonical forms is more accurate thanusing the composed vector, ~vL-Comp.
In Section 4.2we find more evidence for this.
Another interestingobservation is that CForm has the highest perfor-mance (among unsupervised methods), very closelyfollowed by DiI -CF ;L-NCF.
These results confirmour hypothesis that canonical forms?which reflectthe overall behaviour of a VNC type?are stronglyinformative about the class of a token, perhaps evenmore so than the local context of the token.
Im-portantly, this is the case even though the canonicalforms that we use are imperfect knowledge obtainedautomatically through an unsupervised method.Our results using 1NN, 72:4%, are comparable45Vectors cosine Vectors cosine~aidmand ~alit.55~vI -CFand ~alit.70 ~vI -CFand ~aidm.90~vL-NCFand ~alit.80 ~vL-NCFand ~aidm.60~vL-Compand ~alit.72 ~vL-Compand ~aidm.76Table 2: Average similarity between the actual vectors (~a) andthe estimated vectors (~v), for the idiomatic and literal meanings.to those of Katz and Giesbrecht (2006) using thismethod on their German data (72%).
However, theirbaseline is slightly lower than ours at 58%, andthey only report results for 1 expression with 67 in-stances.
Interestingly, our best unsupervised resultsare in line with the results using 1NN and not sub-stantially lower than the results using 5NN.4.2 A Closer Look into the Estimated VectorsIn this section, we compare our estimated idiomaticand literal vectors with the actual vectors for theseusages calculated from manually-annotated data.Such a comparison helps explain some of the differ-ences we observed in the performance of the meth-ods.
Table 4.2 shows the similarity between the esti-mated and actual vectors representing the idiomaticand literal meanings, averaged over the 14 TEST ex-pressions.
Actual vectors, referred to as ~aidmand~alit, are calculated over idiomatic and literal usagesof the expressions as determined by the human an-notations.
Estimated vectors, ~vI -CF, ~vL-CF, and~vL-Comp, are calculated using our methods describedin Section 2.2.For comparison purposes, the first row of Ta-ble 4.2 shows the average similarity between theactual idiomatic and literal vectors, ~aidmand ~alit.These vectors are expected to be very dissimilar,hence the low average cosine between them servesas a baseline for comparison.
We now look into therelative similarity of each estimated vector, ~vI -CF,~vL-CF, ~vL-Comp, with these two vectors.The second row of the table shows that, as de-sired, our estimated idiomatic vector, ~vI -CF, is no-tably more similar to the actual idiomatic vector thanto the actual literal vector.
Also, ~vL-NCFis moresimilar to the actual literal vector than to the actualidiomatic vector (third row).
Surprisingly, however,~vL-Compis somewhat similar to both actual literaland idiomatic vectors (in fact it is slightly more simi-lar to the latter).
These results suggest that the vectorcomposed of the context vectors for the constituentsof an expression may not always be the best estimateof the literal meaning of the expression.5 Given thisobservation, the overall better-than-baseline perfor-mance of DiI-CF;L-Compmight seem unjustified ata first glance.
However, we believe this performanceis mainly due to an accurate estimate of ~vI -CF.4.3 Performance Based on Class DistributionWe further divide our 28 DEV and TEST expres-sions according to their proportion of idiomatic-to-literal usages, as determined by the human annota-tors.
In order to have a sufficient number of expres-sions in each group, here we merge DEV and TEST(we refer to the new set as DT).
DTIhighcontains17 expressions with 65%?90% of their usages be-ing idiomatic?i.e., their idiomatic usage is domi-nant.
DTIlowcontains 11 expressions with 8%?58%of their occurrences being idiomatic?i.e., their id-iomatic usage is not dominant.Table 4.3 shows the average accuracy of all themethods on these two groups of expressions, withthe best performance on each group shown in bold-face.
On DTIhigh, both DiI -CF ;L-NCFand CFormoutperform the baseline, with CForm having thehighest reduction in error rate.
The two methods per-form similarly to each other on DTIlow, though notethat the error reduction of CForm is more in linewith its performance on DTIhigh.
These results showthat even for VNCs whose idiomatic meaning isnot dominant?i.e., those in DTIlow?automatically-acquired canonical forms can help with their tokenclassification.An interesting observation in Table 4.3 is theinconsistent performance of DiI -CF ;L-Comp: themethod has a very poor performance on DTIhigh, butoutperforms the other two unsupervised methods onDTIlow.
As we noted earlier in Section 2.2, the morefrequent the idiomatic meaning of an expression,the more reliable the acquired canonical forms forthat expression.
Since the performance of CFormand DiI -CF ;L-NCFdepends highly on the accu-racy of the automatically acquired canonical forms,it is not surprising that these two methods perform5This was also noted by Katz and Giesbrecht (2006) in theirsecond experiment.46Method DTIhighDTIlowBaseline 81.4 (-) 35.0 (-)Unsuper- DiI -CF ; L-Comp73.1 (-44.6) 58.6 (36.3)vised DiI -CF ; L-NCF82.3 (4.8) 52.7 (27.2)CForm 84.7 (17.7) 53.4 (28.3)Super- 1NN 78.3 (-16.7) 65.8 (47.4)vised 5NN 82.3 (4.8) 72.4 (57.5)Table 3: Macro-averaged accuracy over DEV and TEST, di-vided according to the proportion of idiomatic-to-literal usages.worse than DiI -CF ;L-Compon VNCs whose id-iomatic usage is not dominant.The high performance of the supervised meth-ods on DTIlowalso confirms that the poorer perfor-mance of the unsupervised methods on these VNCsis likely due to the inaccuracy of the canonical formsextracted for them.
Interestingly, when canonicalforms can be extracted with a high accuracy (i.e.,for VNCs in DTIhigh) the performance of the unsu-pervised methods is comparable to (or even slightlybetter than) that of the best supervised method.
Onepossible way of improving the performance of unsu-pervised methods is thus to develop more accuratetechniques for the automatic acquisition of canoni-cal forms.5 Related WorkVarious properties of MWEs have been exploitedin developing automatic identification methods forMWE types (Lin, 1999; Krenn and Evert, 2001; Fa-zly and Stevenson, 2006).
Much research has ad-dressed the non-compositionality of MWEs as animportant property related to their idiomaticity, andhas used it in the classification of both MWE typesand tokens (Baldwin et al, 2003; McCarthy et al,2003; Katz and Giesbrecht, 2006).
We also makeuse of this property in an MWE token classificationtask, but in addition, we draw on other salient char-acteristics of MWEs which have been previouslyshown to be useful for their type classification (Evertet al, 2004; Fazly and Stevenson, 2006).The idiomatic/literal token classification methodsof Birke and Sarkar (2006) and Katz and Giesbrecht(2006) rely primarily on the local context of a to-ken, and fail to exploit specific linguistic propertiesof non-literal language.
Our results suggest that suchproperties are often more informative than the localcontext, in determining the class of an MWE token.The supervised classifier of Patrick and Fletcher(2005) distinguishes between compositional andnon-compositional English verb-particle con-struction tokens.
Their classifier incorporateslinguistically-motivated features, such as the degreeof separation between the verb and particle.
Here,we focus on a different class of English MWEs,verb+noun combinations.
Moreover, by makinga more direct use of their syntactic behaviour, wedevelop unsupervised token classification methodsthat perform well.
The unsupervised token classifierof Hashimoto et al (2006) uses manually-encodedinformation about allowable and non-allowablesyntactic transformations of Japanese idioms?thatare roughly equivalent to our notions of canonicaland non-canonical forms.
The rule-based classifierof Uchiyama et al (2005) incorporates syntac-tic information about Japanese compound verbs(JCVs), a type of MWE composed of two verbs.In both cases, although the classifiers incorporatesyntactic information about MWEs, their manualdevelopment limits the scalability of the approaches.Uchiyama et al (2005) also propose a statisticaltoken classification method for JCVs.
This methodis similar to ours, in that it also uses type-basedknowledge to determine the class of each tokenin context.
However, their method is supervised,whereas our methods are unsupervised.
Moreover,Uchiyama et al (2005) evaluate their methods on aset of JCVs that are mostly monosemous.
Here, weintentionally exclude such cases from consideration,and focus on those MWEs that have two clear id-iomatic and literal meanings, and that are frequentlyused with either meaning.6 ConclusionsWhile a great deal of research has focused on prop-erties of MWE types, such as their compositional-ity, less attention has been paid to issues surround-ing MWE tokens.
In this study, we have developedtechniques for a semantic classification of tokens ofa potential MWE in context.
We focus on a broadlydocumented class of English MWEs that are formedfrom the combination of a verb and a noun in itsdirect object position, referred to as VNCs.
We an-notated a total of 1180 tokens for 28 VNCs accord-47ing to whether they are a literal or idiomatic usage,and we found that approximately 40% of the to-kens were literal usages.
These figures indicate thatautomatically determining whether a VNC token isused idiomatically or literally is of great importancefor NLP applications.
In this work, we have pro-posed three unsupervised methods that perform sucha task.
Our proposed methods incorporate automati-cally acquired knowledge about the overall syntacticbehaviour of a VNC type, in order to do token classi-fication.
More specifically, our methods draw on thesyntactic fixedness of VNCs?a property which hasbeen largely ignored in previous studies of MWEtokens.
Our results confirm the usefulness of thisproperty as incorporated into our methods.
All ourmethods outperform the baseline of always predict-ing the most frequent class.
Moreover, consideringour approach is unsupervised, our best accuracy of72:4% is not substantially lower than the accuracyof a standard supervised approach at 76:2%.ReferencesTimothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical model ofmultiword expression decomposability.
In Proceed-ings of the ACL-SIGLEX Workshop on Multiword Ex-pressions: Analysis, Acquisition and Treatment.Julia Birke and Anoop Sarkar.
2006.
A clustering ap-proach for nearly unsupervised recognition of nonlit-eral language.
In Proceedings of EACL-06, 329?336.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Anthony P. Cowie, Ronald Mackin, and Isabel R. Mc-Caig.
1983.
Oxford Dictionary of Current IdiomaticEnglish, volume 2.
Oxford University Press.Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society of Information Science,41(6):391?407.Stefan Evert, Ulrich Heid, and Kristina Spranger.
2004.Identifying morphosyntactic preferences in colloca-tions.
In Proceedings LREC-04.Afsaneh Fazly and Suzanne Stevenson.
2006.
Automat-ically constructing a lexicon of verb phrase idiomaticcombinations.
In Proceedings of EACL-06, 337?344.Christiane Fellbaum.
2002.
VP idioms in the lexicon:Topics for research using a very large corpus.
InS.
Busemann, editor, Proceedings of the KONVENS-02 Conference.John R. Firth.
1957.
A synopsis of linguistic theory1930?1955.
In Studies in Linguistic Analysis (specialvolume of the Philological Society), 1?32.
The Philo-logical Society, Oxford.Chikara Hashimoto, Satoshi Sato, and Takehito Utsuro.2006.
Japanese idiom recognition: Drawing a line be-tween literal and idiomatic meanings.
In Proceedingsof the COLING/ACL 2006 Main Conference PosterSessions, 353?360.Graham Katz and Eugenie Giesbrecht.
2006.
Auto-matic identification of non-compositional multi-wordexpressions using latent semantic analysis.
In Pro-ceedings of the ACL/COLING-06 Workshop on Multi-word Expressions: Identifying and Exploiting Under-lying Properties, 12?19.Brigitte Krenn and Stefan Evert.
2001.
Can we do betterthan frequency?
A case study on extracting PP-verbcollocations.
In Proceedings of the ACL-01 Workshopon Collocations.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of ACL-99,317?324.Diana McCarthy, Bill Keller, and John Carroll.
2003.Detecting a continuum of compositionality in phrasalverbs.
In Proceedings of the ACL-SIGLEX Workshopon Multiword Expressions: Analysis, Acquisition andTreatment.Geoffrey Nunberg, Ivan A.
Sag, and Thomas Wasow.1994.
Idioms.
Language, 70(3):491?538.Jon Patrick and Jeremy Fletcher.
2005.
Classifying verb-particle constructions by verb arguments.
In Proceed-ings of the Second ACL-SIGSEM Workshop on the Lin-guistic Dimensions of Prepositions and their use inComputational Linguistics Formalisms and Applica-tions, 200?209.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
Multiword ex-pressions: A pain in the neck for NLP.
In Proceedingsof CICLing-02, 1?15.Maggie Seaton and Alison Macaulay, editors.
2002.Collins COBUILD Idioms Dictionary.
HarperCollinsPublishers, second edition.Kiyoko Uchiyama, Timothy Baldwin, and Shun Ishizaki.2005.
Disambiguating Japanese compound verbs.Computer Speech and Language, Special Issue onMultiword Expressions, 19(4):497?512.Sriram Venkatapathy and Aravid Joshi.
2005.
Measur-ing the relative compositionality of verb-noun (V-N)collocations by integrating features.
In Proceedings ofHLT/EMNLP-05, 899?906.Begon?a Villada Moiro?n and Jo?rg Tiedemann.
2006.Identifying idiomatic expressions using automaticword-alignment.
In Proceedings of the EACL-06Workshop on Multiword Expressions in a MultilingualContext, 33?40.48
