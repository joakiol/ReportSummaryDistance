Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1158?1167,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsGenerating Subsequent Reference in Shared Visual Scenes:Computation vs. Re-UseJette Viethen1,2jette.viethen@mq.edu.au1TiCCUniversity of TilburgTilburg, The NetherlandsRobert Dale2robert.dale@mq.edu.au2Centre for Language TechnologyMacquarie UniversitySydney, AustraliaMarkus Guhe3m.guhe@ed.ac.uk3School of InformaticsUniversity of EdinburghEdinburgh, UKAbstractTraditional computational approaches to re-ferring expression generation operate in a de-liberate manner, choosing the attributes to beincluded on the basis of their ability to dis-tinguish the intended referent from its dis-tractors.
However, work in psycholinguis-tics suggests that speakers align their refer-ring expressions with those used previously inthe discourse, implying less deliberate choiceand more subconscious reuse.
This raises thequestion as to which is a more accurate char-acterisation of what people do.
Using a cor-pus of dialogues containing 16,358 referringexpressions, we explore this question via thegeneration of subsequent references in sharedvisual scenes.
We use a machine learning ap-proach to referring expression generation anddemonstrate that incorporating features thatcorrespond to the computational tradition doesnot match human referring behaviour as wellas using features corresponding to the processof alignment.
The results support the view thatthe traditional model of referring expressiongeneration that is widely assumed in work onnatural language generation may not in factbe correct; our analysis may also help explainthe oft-observed redundancy found in human-produced referring expressions.1 IntroductionComputational work on referring expression genera-tion (REG) has an extensive history, and a wide vari-ety of algorithms have been proposed, dealing withvarious facets of what is recognised to be a com-plex problem.
Almost all of this work sees the taskas being concerned with choosing those attributesof an intended referent that distinguish it from theother entities with which it might be confused (see,for example, Dale (1989), Dale and Reiter (1995),Krahmer et al (2003), van Deemter and Krahmer(2007), Gardent and Striegnitz (2007)).
Indepen-dently, an alternative way of thinking about refer-ence has arisen within the psycholinguistics com-munity: there is now a long tradition of work thatexplores how a dialogue participant?s forms of ref-erence are influenced by those previously used fora given entity.
Most recently, this line of work hasbeen discussed in terms of the notions of alignment(Pickering and Garrod, 2004) and conceptual pacts(Clark and Wilkes-Gibbs, 1986; Brennan and Clark,1996).We suspect that neither approach tells the fullstory, and so we are interested in exploring whetherthe two perspectives should be integrated.
Using alarge corpus of referring expressions in task-orienteddialogues, this paper presents a machine learningapproach that allows us to combine features corre-sponding to the two perspectives.
Our results showthat models based on the alignment perspective out-perform models based on traditional REG considera-tions, as well as a number of simpler baselines.The paper is structured as follows.
In Section 2,we outline the two perspectives on subsequent ref-erence, and summarise related work.
In Section 3,we describe the iMAP Corpus and the referring ex-pressions it contains.
In Section 4, we describe theapproach we take to learning models of referentialbehaviour using this data, and in Section 5 we dis-cuss the results of a number of experiments based1158on this approach, followed by an error analysis inSection 6.
Section 7 draws some conclusions anddiscusses future work.2 Related Work2.1 The Algorithmic ApproachWe use the term algorithmic approach here to re-fer to the perspective that is common to the consid-erable body of work within computational linguis-tics on the problem of referring expression gener-ation developed over the last 20 years.
Much ofthis work takes as its starting point the characterisa-tion of the problem expressed in (Dale, 1989).
Thiswork has focused on the design of algorithms whichtake into account the context of reference in order todecide what properties of an entity should be men-tioned in order to distinguish that entity from otherswith which it might be confused.
Early work wasconcerned with subsequent reference in discourse,inspired by Grosz and Sidner?s (1986) observationson how the attentional structure of a discourse madeparticular referents accessible at any given point.More recently, attention has shifted to initial ref-erence in visual domains, driven in large part bythe availability of the TUNA dataset and the sharedtasks that make use of it (Gatt et al, 2008).
The con-struction of distinguishing descriptions has consis-tently been a key consideration in this body of work.Scenarios that require the generation of referencesin multi-turn dialogues that concern visual scenesare likely to be among the first where we can ex-pect computational approaches to referring expres-sion generation to be practically useful.
Surpris-ingly, however, the more recent work on initial refer-ence in visual domains and the earlier work on sub-sequent reference in discourse remain somewhat dis-tinct and separate from each other, despite much thesame algorithms having been used in both.
Thereis very little work that brings these two strands to-gether by looking at both initial and subsequent ref-erences in dialogues that concern visual scenes.
Anexception here is the machine learning approach de-veloped by Stoia et al (2006), who aimed at buildinga dialogue system for a situated agent giving instruc-tions in a virtual 3D world.
However, their approachwas concerned with choosing the type of referenceto use (definite or indefinite, pronominal, bare ormodified head noun), and not with the content of thereference; and their data set consisted of only 1242referring expressions.2.2 The Alignment ApproachMeanwhile, starting with the early work of Carroll(1980), a quite distinct strand of research in psy-cholinguistics has explored how a speaker?s form ofreference to an entity is impacted by the way that en-tity has been previously referred to in the discourseor dialogue.
The general idea behind what we willcall the alignment approach is that a conversationalparticipant will often adopt the same semantic, syn-tactic and lexical alternatives as the other party in adialogue.
This perspective is most strongly associ-ated with the work of Pickering and Garrod (2004).With respect to reference in particular, speakers aresaid to form conceptual pacts in their use of lan-guage (Clark and Wilkes-Gibbs, 1986; Brennan andClark, 1996).
Although there is disagreement aboutthe exact mechanisms that enable alignment andconceptual pacts, the implication of much of thiswork is that one speaker introduces an entity bymeans of some description, and then (perhaps aftersome negotiation) both conversational participantsshare this form of reference, or a form of referencederived from it, when they subsequently refer to thatentity.Recent work by Goudbeek and Krahmer (2010)supports the view that subconscious alignment doesindeed take place at the level of content selection forreferring expressions.
The participants in their studywere more likely to use a dispreferred attribute todescribe a target referent if this attribute had recentlybeen used in a description by a confederate.There is some work within natural language gen-eration that attempts to model the process of align-ment (Buschmeier et al, 2009; Janarthanam andLemon, 2009), but this is predominantly concernedwith what we might think of as the ?lexical perspec-tive?, focussing on lexical choice rather than the se-lection of appropriate semantic content for distin-guishing descriptions.2.3 Combined ModelsThis paper is not the first to look at how the algorith-mic approach and the alignment approach might beintegrated in REG.
An early machine learning ap-1159Figure 1: An example pair of maps.proach to content selection was presented by Jor-dan and Walker (2000; 2005); they were also in-terested in an exploration of the validity of differ-ent psycholinguistic models of reference produc-tion, including Grosz and Sidner?s (1986) modelof discourse structure, the conceptual pacts modelof Clark and colleagues, and the intentional influ-ences model developed by Jordan (2000).
However,their data set consists of only 393 referring expres-sions, compared to our 16,358, and these expres-sions had functions other than identification; mostimportantly, the entities referred to were not part ofa shared visual scene as is the case in our data.Gupta and Stent (2005) instantiated Dale and Re-iter?s (1995) Incremental Algorithm with a prefer-ence ordering that favours the attributes that wereused in the previous mention of the same referent.
Ina second variant, they even require these attributesto be included in a subsequent reference.
Differ-ently from most other work on REG, they extendedthe task to include ordering of the attributes in thesurface form.
They therefore create a special evalu-ation metric that takes ordering into account, whichmakes it hard to compare the performance they re-port to that of any system that is not concerned withattribute ordering, such as ours.
Their evaluation setwas also considerably smaller than ours: they used1294 and 471 referring expressions from two differ-ent corpora, compared to our test set of 4947 refer-ring expressions.More recently in (Viethen et al, 2010), we pre-sented a rule-based system that addressed a specificinstance of the problem we consider here, using thesame corpus as we do: we singled out 2579 first ref-erences to landmarks by the second speaker (?secondspeaker initial references?)
and attempted to repro-duce these using a system based on Dale and Re-iter?s (1995) Incremental Algorithm.
Although thedata set was a subset of the one used here, the systemdid not reach the same performance (see Section 5).3 Referring Expressions in the iMAPCorpusThe iMAP Corpus (Louwerse et al, 2007) is a col-lection of 256 dialogues between 32 participant-pairs who contributed 8 dialogues each.
Both par-ticipants had a map of the same environment, butone participant?s map showed a route winding itsway between the landmarks on the map; see Fig-ure 1.
The task was for this participant (the in-struction giver, IG) to describe this route in such away that their partner (the instruction follower, IF)could draw it onto their map; this was complicatedby some discrepancies between the two maps, such1160as missing landmarks, the unavailability of colour insome regions due to ink stains, and small differencesbetween some landmarks.The landmarks differ from each other in type,colour, and one other attribute, which is differentfor each type of landmark.
For example, there aredifferent kinds of birds (eagle, ostrich, penguin .
.
.
);fish differ by their patterns (dotted, checkered, plain.
.
.
), aliens have different shapes (circular, hexago-nal .
.
.
), and bugs appear in small clusters of differ-ing numbers.
In addition to these inherent attributesof the landmarks, participants used spatial relationsto other items on the map.
Each referring expressionin the corpus is annotated with a unique identifiercorresponding to the landmark that it describes andthe semantic values of the attributes that it contains.This collection of annotations forms the basic datawe use in our experiments.For each landmarkR referred to in a dialogue, weview the sequence of references to this landmark asa coreference chain, notated ?R1, R2, .
.
.
, Rn?.
Byconvention, R1 is termed the initial reference, andall other references in the chain are subsequent ref-erences.
From the corpus as a whole we extracted34,127 referring expressions in 9558 chains.
The av-erage length of a chain is 4.74; and the longest coref-erence chain contains 43 references.
Referencesmay be contributed to a chain by either speaker, andcan be arbitrarily far apart: in the data, 4201 refer-ences are in the utterance immediately following thepreceding reference in the chain, but the distance be-tween references in a chain can be as high as 423utterances.We removed from the data any annotation thatwas not concerned with the four landmark attributes,type, colour, relation, or the landmark?s other dis-tinguishing attribute.
For example, ?semanticallyempty?
head nouns such as thing or set.
Ordi-nal numbers that were annotated as the use of thenumber attribute were re-tagged as spatial relations,as these usually described the position of the targetwithin a line of landmarks.As a result of the removal of annotations not per-taining to the use of the four landmark attributes,2785 referring expressions had no annotation left;we removed these instances from the final data set.We also do not attempt to replicate the remaining5552 plural referring expressions or the 3062 pro-Content Pattern Count Proportion?other?
5893 36.0%?other, type?
3684 22.5%?other, colour?
1630 10.0%?other, colour, type?
1021 6.2%?colour?
969 5.9%?relation?
777 4.7%?other, relation?
587 3.6%?type?
574 3.5%?colour, type?
434 2.7%?other, relation, type?
312 1.9%?relation, type?
236 1.4%?colour, relation?
99 0.6%?other, colour, relation?
81 0.5%?other, colour, relation, type?
44 0.3%?colour, relation, type?
17 0.1%Total 16,358Table 1: The 15 content patterns by frequency.nouns found in the corpus.1 However, we do in-clude all of these instances in the feature extractionstep, on the assumption that they might impact onthe content of subsequent references.
Similarly, wefilter out 6369 initial references after we have ex-tracted features from them, since we focus here onthe generation of subsequent reference only.
The re-maining 16,358 referring expressions form the datawhich we use in our experiments.Contrary to findings from other corpora, in whichcolour was used much more frequently (Gatt, 2007;Viethen and Dale, 2008), the colour attribute wasused in only 26.3% of the referring expressions inour data set.
This is probably due to the often lowreliability of colour in this task caused by the inkstains.
The proportion of referring expressions men-tioning the target?s type might, at 38.7%, also seemlow.
This can be explained by the fact that one quar-ter of the landmarks, namely birds and buildings, aremore likely to be described in terms of their specifickind than in terms of their generic type.
This alsohelps explain why the overall use of the other at-tribute, which for some landmarks was their kind,was used in 81.0% of all instances.
Spatial relationswere used in 13.16% of the referring expressions,comparable to other corpora in the literature.1The additional issues that arise in generating plural refer-ences and deciding when to use pronouns considerably compli-cate the problem; see (Gatt, 2007).1161We can think of each referring expression as be-ing a linguistic realisation of a content pattern: thisis the collection of attributes that are used in thatinstance.
The attributes can be derived from theproperty-level annotation given in the corpus.
So,for example, if a particular reference appears as thenoun phrase the blue penguin, annotated seman-tically as ?blue, penguin?, then the correspondingcontent pattern is ?colour, kind?.
Our aim is to repli-cate the content pattern of each referring expressionin the corpus.
Table 1 lists the 15 content patternsthat occur in our data set in order of frequency.4 Modelling Referential Behaviour4.1 The Two PerspectivesOur task is defined simply as follows: for each sub-sequent referenceR in the corpus, can we predict thecontent pattern that will be used in that reference?As we noted at the outset of the paper, the literaturewould appear to suggest two distinct approaches tothis problem.
What we have characterised as the al-gorithmic approach can be summarised thus:At the point where a reference is required,a speaker determines the relevant featuresof other entities in the context, then com-putes the content of a referring expressionwhich distinguishes the intended referentfrom the other entities.The alignment approach, on the other hand, can besummarised thus:Speakers align the forms of reference theyuse to be similar or identical to referencesthat have been used before.
In particular,once a form of reference to the intendedreferent has been established, they tend tore-use that form of reference, or perhapsan abbreviated version of it.The alignment approach would appear to be prefer-able on the grounds of computational cost: wewould expect that retrieving a previously-used refer-ring expression, or parts thereof, generally requiresless computation than building a new referring ex-pression from scratch.On the other hand, if the context has changedin any way, then a previously-used form of ref-erence may no longer be effective in identifyingMap FeaturesMain Map type most frequent type of LM on this mapMain Map other other attribute if the most frequent type of LMMixedness are other LM types present on this map?Ink Orderliness shape of the ink blot(s) on the IF?s mapLmprop Featuresother Att type of the other attribute of the target[att] Value value for each att of target[att] Difference was att of target different between the twomaps?Missing was target missing one of the maps?Inked Out was target inked] out on the IG?s map?Speaker FeaturesDyad ID ID of the pair of participant-pairSpeaker ID ID of the person who uttered this RESpeaker Role was the speaker the IG or the IF?Table 2: The Ind feature set.the intended referent, and recomputation may berequired.2 This is precisely the consideration onwhich the initial work on referring expression gen-eration was based, inspired by Grosz and Sidner?s(1986) observations about how the changing atten-tional structure of a discourse moves different en-tities in and out of focus.
However, a straightfor-ward recomputation of reference based on the cur-rrent context carries the risk that the most effectiveset of properties to use may change quite radically;if no account is taken of the history of previous ref-erences to the entity, it?s conceivable that one couldproduce a description that is so different from theprevious description that they are virtually unreco-gisable as descriptions of the same entity.
Ideally,what we want to do is modify a previous descriptionto do the job.These observations suggest that, in order tochoose the most appropriate form of reference for anentity, we need to simultaneously take account of:?
the other entities from which it must be distin-guished, both in the visual context and in thepreceding discourse (in other words, exactlythe information that traditional algorithmic ap-proaches consider);?
how this entity, and perhaps other entities, havebeen referred to in the past (precisely the infor-mation that the alignment approach considers).2Unfortunately, determining what counts as a change of con-text, especially in visual scenes, is fraught with difficulty.1162TradREG Features (Visual)Count Vis Distractors number of visual distractorsProp Vis Same [att] proportion of visual distractors withsame attDist Closest distance to the closest visual distrac-torClosest Same [att] has the closest distractor the sameatt?Dist Closest Same [att] distance to the closest distractor ofsame att as targetCl Same type Same [att] has the closest distractor of the sametype also the same att?TradREG Features (Discourse)Count Intervening LMs number of other LMs mentioned sincethe last mention of the targetProp Intervening [att] proportion of intervening LMs forwhich att was used AND which havethe same att as targetTable 3: The TradREG feature set.The set of features we describe next attempts to cap-ture these two aspects of the problem.4.2 FeaturesThe number of factors that can be hypothesised ashaving an impact on the form of a referring expres-sion in a dialogic setting associated with a visual do-main is very large.
Attempting to incorporate all ofthese factors into parameters for rule-based systems,and then experimenting with different settings forthese parameters, is prohibitively complex.
Instead,we here capture a wide range of factors as featuresthat can be used by a machine learning algorithm toautomatically induce from the data a classifier thatpredicts for a given set of features the attributes thatshould be used in a referring expression.The features we extracted from the data set arelisted in Tables 2?4.3 They fall into five subsets.Map Features capture design characteristics of themaps the current dialogue is about; Speaker Fea-tures capture the identity and role of the partici-pants; and LMprop Features capture the inherentvisual properties of the target referent.
For our ex-periments, we group the Map, LMprop and Speakerfeature sets into one theory-independent set (Ind).Most importantly for our present considerations,3In these tables, att is an abbreviatory variable that is instan-tiated once for each of the four attributes type, colour, relation,and the other distinguishing attribute of the landmark.
The ab-breviation LM stands for landmarkAlignment Features (Recency)Last Men Speaker Same who made the last mention of target?Last Mention [att] was att used in the last mention oftarget?Dist Last Mention Utts distance to the last mention of targetin utterancesDist Last Mention REs distance to the last mention of targetin REsDist Last [att] LM Utts distance in utterances to last use ofatt for targetDist Last [att] LM REs distance in REs to last use of att fortargetDist Last [att] Dial Utts distance in utterances to last use ofattDist Last [att] Dial REs distance in REs to last use of attDist Last RE Utts distance to last RE in utterancesLast RE [att] was att mentioned in the last RE?Alignment Features (Frequency)Count [att] Dial how often has att been used in the dialogue?Count [att] LM how often has att been used for target?Quartile quartile of the dialogue the RE was uttered inDial No number of dialogues already completed +1Mention No number of previous mentions of target +1Table 4: The Alignment feature set.TradREG Features capture factors that the tradi-tional computational approaches to referring expres-sion generation take account of, in particular prop-erties of the discourse and visual distractors; andAlignment Features capture factors that we wouldexpect to play a role in the psycholinguistic modelsof alignment and conceptual pacts.4.3 The ModelsFor the experiments described here, we used a 70?30split to divide the data into a training set (11,411 in-stances) and a test set (4,947 instances).
In additionto the main prediction class content pattern, the splitwas stratified for Speaker ID and Quartile to ensurethat training and test set contained the same pro-portion of descriptions from each speaker and eachquartile of the dialogues.
We used the J48 algorithmimplemented in the Weka toolkit (Witten and Frank,2005) to train decision trees with the task of judging,based on the given features, which content patternshould be used.First, we have three separate baseline models:HeadNounOnly generates only the property that isthe most likely head noun for the target, whichis kind for birds and buildings and type for all1163other landmarks.
This is a form of ?reducedreference?
strategy.RepeatLast represents a very simplistic alignmentapproach.
It generates the same content patternthat was used in the previous mention of thetarget referent.MajorityClass generates the content pattern mostcommonly used in the training set.We then have a number of models that use subsetsof the features described above:AllFeatures is a decision tree trained on all fea-tures;TradREG is a decision tree trained on theTradREG features only;Alignment is a decision tree trained on the Align-ment features only;Ind is a decision tree trained on the Ind featuresonly;Alignment+Ind is a decision tree trained on all butthe TradREG features;TradREG+Ind is a decision tree trained on all butthe Alignment features; andTradREG+Alignment is a decision tree trained onall but the Ind features.5 ResultsIn this section we report how the models describedin the previous section performed on the held-outtest set in comparison to each other and to the threebaselines.We use Accuracy and average DICE score for ourcomparisons; these are the most commonly usedmeasures in the REG literature (see, for example,Gatt et al, 2008).
Given two sets of attributes, Aand B, DICE is computed as(1) DICE = 2?
|A ?B||A|+ |B| .This gives some measure of the overlap between tworeferring expressions, assigning a partial score if thetwo sets share attributes but are not identical.
TheAccuracy of a system is the proportion of test in-stances for which it achieves a DICE score of 1, sig-nifying a perfect match.col other type rel Comb.
PatternAcc Acc Acc Acc Acc DICEHeadOnly n/a n/a n/a n/a 23.1 0.49RepLast n/a n/a n/a n/a 38.4 0.55Majority 73.8 81.0 61.7 86.8 36.0 0.65predicts no yes no no ?other?Trad 74.6 84.8 77.1 87.0 47.3 0.73Align 83.6 84.1 80.7 87.5 54.6 0.78Ind 81.9 82.8 81.4 88.0 52.7 0.78Align+Ind 86.1 85.3 82.4 88.7 58.2 0.81Trad+Ind 82.2 84.1 81.1 87.1 52.5 0.78Trad+Align 84.1 84.0 80.1 86.8 53.9 0.78AllFeatures 86.2 85.8 83.2 88.5 58.8 0.81Table 5: Performance of our models compared to thebaselines.
Model names are abbreviated for space rea-sons.
The Accuracy (given in %) of all models is signifi-cantly better than that of the highest performing baselineat p<.01 according to the ?2 statistic.We tested two different ways of generating con-tent patterns based on the different feature sets de-scribed above: PatternAtOnce builds a decisiontree that chooses one of the 15 content patterns thatoccur in our data set; whereas CombinedPatternbuilds attribute-specific decision trees (one for eachof the four attributes that occur in the data: colour,other, type, and relation), and then combines theirpredictions into a complete content pattern.
Wefound that CombinedPattern slightly outperformedPatternAtOnce, although the difference is not statis-tically significant for all feature sets.
For space rea-sons, we report in what follows only on the slightlybetter-performing CombinedPattern model.Table 5 compares the performances of the threebaselines and the decision trees based on the five fea-ture subsets for each of the individual attributes andfor the combined content pattern; note that the Head-NounOnly and RepeatLast baselines do not makeattribute-specific predictions.
The table shows thatthe learned systems outperform all three baselinesfor the individual attributes as well as for the com-bined content pattern.A comparison of the Alignment feature set andthe TradREG feature set shows that the former out-performs the latter for the attribute-specific treeswhich predict the use of the colour attribute and the1164use of relation, and that the combined patterns re-sulting from the Alignment trees are a better matchof the human-produced patterns both in terms of Ac-curacy (p<.01 for all three categories, using ?2) andDICE.
Interestingly, even the theory-independentInd features outperform the TradREG features.The comparison between TradREG+Ind andAlignment+Ind again shows a clear advantage forthe Alignment features: dropping them from thecomplete feature set significantly hurts performancecompared to AllFeatures (?2=80.5, p<.01), whiledropping the TradREG features has no significantimpact.
Also consistent with the results of the threeindividual feature sets, dropping the Ind featureshurts performance more than dropping the TradREGfeatures, but less than dropping the Alignment fea-tures.
Training on the complete feature set (All-Features) achieves the highest performance, whichis significantly better than that of all other featuressets (p<.01 using ?2) except Alignment+Ind.These results suggest that considerations at theheart of traditional REG approaches do not play asimportant a role as those postulated by alignment-based models for the selection of semantic contentfor subsequent referring expressions.We also note that the Accuracy scores achievedby our learned systems are similar to the best num-bers previously reported in the REG literature.
WhileJordan and Walker?s (2005) data set is not directlycomparable, they achieved a maximum of 59.9%Accuracy, against our 58.8%.
Stoia et al?s (2006)best Accuracy was 31.2%, albeit on a slightly dif-ferent task.
Even in the arguably much simplernon-dialogic domains of the REG competitions con-cerned with pure content selection, the best perform-ing system achieved only 53% Accuracy (see Gatt etal., 2008).
The most comparable approach, the rule-based system we presented in (Viethen et al, 2010)for a subset of the data used here, was not able tooutperform a RepeatLast baseline at 40.2% Accu-racy and an average DICE score of 0.67.6 Error AnalysisAn important question to ask is how wrong the mod-els really are when they do not succeed in perfectlymatching a human-produced reference in our testset.
It might be that they choose a completely dif-Acc Dice Super Sub Inter NooverTrad 47.3 0.75 14.4 22.2 5.5 10.5Align 54.6 0.78 16.0 16.1 3.9 9.4Ind 52.7 0.78 17.1 17.2 3.9 9.0Align+Ind 58.2 0.81 16.0 14.8 3.1 7.9Trad+Ind 52.5 0.78 17.4 17.5 3.8 8.8Trad+Align 53.9 0.78 17.1 15.6 4.3 9.0AllFeature 58.8 0.81 16.5 14.5 3.1 7.2Table 6: The proportions of test instances for which eachmodel produced a subset, a superset, some other form ofintersection or no-overlap to the human reference.ferent set of attributes from those included by thehuman speaker; however, the Accuracy score alsocounts as incorrect any set that only partly overlapswith the reference found in the test set.The DICE score gives us a partial answer to thisquestion, as it assigns a score that is based on thesize of the overlap between the attribute set cho-sen by the model and that included by the humanspeaker.
A DICE score that is equal to the Accuracyscore would mean that each referring expression waseither reproduced perfectly, or that a set of attributeswas chosen that did not overlap with the originalone at all.
The fact that all our models achievedDICE scores much higher than their Accuracy scoresshows that they only rarely got it completely wrong.Table 6 gives a more fine-grained picture by list-ing, for each model, what percentage of the refer-ring expressions it produced contained a subset ofthe attributes included in the human reference, whatpercentage were a superset, what percentage hadanother form of partial intersection, and what per-centage had no commonality with the human refer-ence.
Interestingly, a large number of the referringexpressions produced by the model trained only onTradREG features are subsets of the human refer-ence.
This indicates that human speakers tend to in-clude more attributes than are strictly speaking nec-essary to distinguish the landmark.4 The Alignmentmodel does not as often produce a subset of the goldstandard content pattern, suggesting that it might bealignment considerations that account for some of4That humans often produce ?redundant?
descriptions, in op-position to the target behaviour of some of the early REG algo-rithms, is of course an oft-observed fact.1165both both 1st 2nd either pot.corr.
wrong corr.
corr.
corr.
AccTrad vs Ind 1797 1794 545 811 3153 63.7Trad vs Align 1742 1647 600 958 3300 66.7Trad vs Align+Ind 1849 1574 493 1031 3373 68.2Align vs Trad+Ind 1908 1557 792 690 3390 68.5Align vs Ind 1872 1511 828 736 3436 69.5Ind vs Trad+Align 1840 1511 768 828 3436 69.5Table 7: Comparison of the predictions for the combinedcontent pattern between the models trained on mutuallyexclusive feature sets.the apparent redundancy that human-produced refer-ring expressions contain.A second important question is whether the differ-ent feature sets are doing the same work, or whetherthey complement each other.
Table 7 lists for thosepairings of our learned models which were based onmutually exclusive feature sets how many referringexpressions both models predicted correctly, howmany both failed to predict, and how many were pre-dicted correctly by either of the two models.Note the high numbers in the columns listing thecounts of instances which both models got eithercorrect or wrong: these show that there is con-siderable overlap between all pairings.
The small-est agreement lies at 3424 instances (68.2%) be-tween TradREG (the least successful model) andAlignment+Ind (the most successful model).
How-ever, they also each predict correct solutions that theother misses: 493 (10.0%) for TradREG and 1031(20.8%) for Alignment+Ind.The last two columns of Table 7 show the numberof instances that at least one of the two models ineach pairing got correct and the proportion out ofall test instances that this number represents.
Thisproportion is the maximum Accuracy that could beachieved by a model that combines the two modelsin a pairing and then correctly chooses which one touse in each instance.
The maximum Accuracies thatcould be achieved in this way on our data set lie justbelow 70%, significantly higher than any numbersreported in the literature on the task of generatingsubsequent reference.7 ConclusionsUsing the largest corpus of referring expressionsto date, we have shown how both the traditionalcomputational view of REG and the alternative psy-cholinguistic alignment approach can be capturedvia a large set of features for machine learning.
Ad-ditionally, we defined a number of theory indepen-dent features.
Using this approach we have pre-sented three main findings.First, we have demonstrated that a model using allthese features to predict content patterns in subse-quent references in shared visual scenes delivers anAccuracy of 58.8% and a DICE score of 0.81, out-performing models based only on features inspiredby one of the two approaches.
However, we foundthat the features based on traditional REG considera-tions do not contribute as much to this score as thosebased on the alignment approach, and that droppingthe traditional REG features does not significantlyhurt the performance of a model based on alignmentand theory-independent features.Second, our error analysis showed that the mainreason for the low performance of a model basedon traditional algorithmic features is that it oftenchooses too few attributes.
The fact that the modelbased on the alignment features does not make thismistake so frequently suggests that it may be thepsycholinguistic considerations incorporated in ouralignment features that lead people to add those ad-ditional attributes.Finally, while the different models make the samecorrect predictions about the content of referring ex-pressions in many cases, there are also a consider-able number of cases where the models based oneither the traditional algorithmic features (10.0%)or the alignment and independent features (20.8%)alone make correct predictions that the other getswrong; this suggests that a system with the abilityto choose the correct model in each of those cases(perhaps based on a hypothesis as to whether or notthe relevant context has changed) could reach an ac-curacy of almost 70% on our data set.
In future workwe plan to identify further features that will allow usto inform this choice so that we can move towardsthis level of performance.1166ReferencesSusan E. Brennan and Herbert H. Clark.
1996.
Concep-tual pacts and lexical choice in conversation.
Journalof Experimental Psychology: Learning, Memory, andCognition, 22:1482?1493.Hendrik Buschmeier, Kirsten Bergmann, and StefanKopp.
2009.
An alignment-capable microplanner fornatural language generation.
In Proceedings of the12th European Workshop on Natural Language Gen-eration, pages 82?89, Athens, Greece.John M. Carroll.
1980.
Naming and describing in socialcommunication.
Language and Speech, 23:309?322.Herbert H. Clark and Deanna Wilkes-Gibbs.
1986.
Re-ferring as a collaborative process.
Cognition, 22(1):1?39.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the Gricean maxims in the gener-ation of referring expressions.
Cognitive Science,19(2):233?263.Robert Dale.
1989.
Cooking up referring expressions.
InProceedings of the 27th Annual Meeting of the Associ-ation for Computational Linguistics, Vancouver B.C.,Canada.Claire Gardent and Kristina Striegnitz.
2007.
Generat-ing bridging definite descriptions.
In Harry C. Buntand Reinhard Muskens, editors, Computing Meaning,volume 3, pages 369?396.
Kluwer, Dordrecht, TheNetherlands.Albert Gatt, Anja Belz, and Eric Kow.
2008.
The TUNAChallenge 2008: Overview and evaluation results.
InProceedings of the 5th International Conference onNatural Language Generation, pages 198?206, SaltFork OH, USA.Albert Gatt.
2007.
Generating Coherent Reference toMultiple Entities.
Ph.D. thesis, University of Ab-erdeen, UK.Martijn Goudbeek and Emiel Krahmer.
2010.
Pref-erences versus adaptation during referring expressiongeneration.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 55?59, Uppsala, Sweden.Barbara J. Grosz and Candance L. Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.
Com-putational Linguistics, 12(3):175?204.Surabhi Gupta and Amanda Stent.
2005.
Automaticevaluation of referring expression generation usingcorpora.
In Proceedings of the Workshop on UsingCorpora for Natural Language Generation, pages 1?6, Brighton, UK.Srinivasan Janarthanam and Oliver Lemon.
2009.
Learn-ing lexical alignment policies for generating referringexpressions for spoken dialogue systems.
In Pro-ceedings of the 12th European Workshop on Natu-ral Language Generation (ENLG 2009), pages 74?81, Athens, Greece, March.
Association for Compu-tational Linguistics.Pamela W. Jordan and Marilyn Walker.
2000.
Learningattribute selections for non-pronominal expressions.In Proceedings of the 38th Annual Meeting on As-sociation for Computational Linguistics, Hong Kong,China.Pamela W. Jordan and Marilyn Walker.
2005.
Learningcontent selection rules for generating object descrip-tions in dialogue.
Journal of Artificial Intelligence Re-search, 24:157?194.Pamela W. Jordan.
2000.
Intentional Influences on Ob-ject Redescriptions in Dialogue: Evidence from anEmpirical Study.
Ph.D. thesis, University of Pitts-burgh, Pittsburgh PA, USA.Emiel Krahmer, Sebastiaan van Erk, and Andre?
Verleg.2003.
Graph-based generation of referring expres-sions.
Computational Lingustics, 29(1):53?72.Max M. Louwerse, Nick Benesh, Mohammed E. Hoque,Patrick Jeuniaux, Gwyneth Lewis, Jie Wu, and MeganZirnstein.
2007.
Multimodal communication in face-to-face computer-mediated conversations.
In Proceed-ings of the 28th Annual Conference of the CognitiveScience Society, pages 1235?1240.Martin J. Pickering and Simon Garrod.
2004.
Toward amechanistic psychology of dialogue.
Behavioral andBrain Sciences, 27(2):169?226.Laura Stoia, Darla M. Shockley, Donna K. Byron, andEric Fosler-Lussier.
2006.
Noun phrase generationfor situated dialogs.
In Proceedings of the 4th Interna-tional Conference on Natural Language Generation,pages 81?88, Sydney, Australia, July.Kees van Deemter and Emiel Krahmer.
2007.
Graphsand Booleans: On the generation of referring expres-sions.
In Harry C. Bunt and Reinhard Muskens, edi-tors, Computing Meaning, volume 3, pages 397?422.Kluwer, Dordrecht, The Netherlands.Jette Viethen and Robert Dale.
2008.
The use of spatialrelations in referring expression generation.
In Pro-ceedings of the 5th International Conference on Natu-ral Language Generation, pages 59?67, Salt Fork OH,USA.Jette Viethen, Simon Zwarts, Robert Dale, and MarkusGuhe.
2010.
Dialogue reference in a visual domain.In Proceedings of the 7th International Conference onLanguage Resources and Evaluation, Valetta, Malta.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques.
Mor-gan Kaufmann, San Francisco CA, USA.1167
