Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2205?2215,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsGrammatical Error Correction: Machine Translation and ClassifiersAlla RozovskayaDepartment of Computer ScienceVirginia TechBlacksburg, VA 24060alla@vt.eduDan RothDepartment of Computer ScienceUniversity of IllinoisUrbana, IL 61820danr@illinois.eduAbstractWe focus on two leading state-of-the-artapproaches to grammatical error correc-tion ?
machine learning classification andmachine translation.
Based on the com-parative study of the two learning frame-works and through error analysis of theoutput of the state-of-the-art systems, weidentify key strengths and weaknesses ofeach of these approaches and demonstratetheir complementarity.
In particular, themachine translation method learns fromparallel data without requiring further lin-guistic input and is better at correctingcomplex mistakes.
The classification ap-proach possesses other desirable charac-teristics, such as the ability to easily gener-alize beyond what was seen in training, theability to train without human-annotateddata, and the flexibility to adjust knowl-edge sources for individual error types.Based on this analysis, we develop analgorithmic approach that combines thestrengths of both methods.
We presentseveral systems based on resources usedin previous work with a relative improve-ment of over 20% (and 7.4 F score points)over the previous state-of-the-art.1 IntroductionFor the majority of English speakers today, En-glish is not the first language.
These writers makea variety of grammar and usage mistakes that arenot addressed by standard proofing tools.
Re-cently, there has been a spike in research on gram-matical error correction (GEC), correcting writingmistakes made by learners of English as a Sec-ond Language, including four shared tasks: HOO(Dale and Kilgarriff, 2011; Dale et al, 2012) andSystem Method PerformanceP R F0.5CoNLL-2014 top 3 MT 41.62 21.40 35.01CoNLL-2014 top 2 Classif.
41.78 24.88 36.79CoNLL-2014 top 1 MT, rules 39.71 30.10 37.33Susanto et al (2014) MT, classif.
53.55 19.14 39.39Miz.
& Mats.
(2016) MT 45.80 26.60 40.00This work MT, classif.
60.17 25.64 47.40Table 1: (Lack of) progress in GEC over the lastfew years.CoNLL (Ng et al, 2013; Ng et al, 2014).
Theseshared tasks facilitated progress on the problemwithin the framework of two leading methods ?machine learning classification and statistical ma-chine translation (MT).The top CoNLL system combined a rule-basedmodule with MT (Felice et al, 2014).
The secondsystem that scored almost as highly used machinelearning classification (Rozovskaya et al, 2014),and the third system used MT (Junczys-Dowmuntand Grundkiewicz, 2014).
Furthermore, Susantoet al (2014) showed that a combination of the twomethods is beneficial, but the advantages of eachmethod have not been fully exploited.Despite success of various methods and thegrowing interest in the task, the key differences be-tween the leading approaches have not been iden-tified or made explicit, which could explain thelack of progress on the task.
Table 1 shows ex-isting state-of-the-art since CoNLL-2014.
The topresults are close, suggesting that several groupshave competitive systems.
Two improvements (of<3 points) were published since then (Susanto etal., 2014; Mizumoto and Matsumoto, 2016).The purpose of this work is to gain a better un-derstanding of the values offered by each methodand to facilitate progress on the task, building onthe advantages of each approach.
Through bet-ter understanding of the methods, we exploit thestrengths of each technique and, building on exist-ing architecture, develop superior systems within2205each framework.
Further combination of thesesystems yields even more significant improve-ments over existing state-of-the-art.
We make thefollowing contributions:?
We examine two state-of-the-art approachesto GEC and identify strengths and weaknesses ofthe respective learning frameworks.
?We perform an error analysis of the output oftwo state-of-the-art systems, and demonstrate howthe methods differ with respect to the types of lan-guage misuse handled by each.?
We exploit the strengths of each framework:with classifiers, we explore the ability to learnfrom native data, i.e.
without supervision, and theflexibility to adjust knowledge sources to specificerror types; with MT, we leverage the ability tolearn without further linguistic input and to bet-ter identify complex mistakes that cannot be easilydefined in a classifier framework.
?As a result, we build several systems that com-bine the strengths of both frameworks and demon-strate substantial progress on the task.
Specif-ically, the best system outperforms the previousbest result by 7.4 F score points.Section 2 describes related work.
Section 3presents error analysis.
In Section 4, we developclassifier and MT systems that make use of thestrengths of each framework.
Section 5 shows howto combine the two approaches.
Section 6 con-cludes.2 Related WorkWe first introduce the CoNLL-2014 shared taskand briefly describe the state-of-the-art GEC sys-tems in the competition and beyond.
Next, anoverview of the two leading methods is presented.2.1 CoNLL-2014 shared task and approachesCoNLL-2014 training data (henceforth CoNLL-train) is a corpus of learner essays (1.2M words)written by students at the National University ofSingapore (Dahlmeier et al, 2013), corrected anderror-tagged.
The CoNLL-2013 test set was in-cluded in CoNLL-2014 and is used as develop-ment.
Both the development and the test sets arealso from the student population studying at thesame University but annotated separately.
We re-port results on the CoNLL-2014 test.The annotation includes specifying the relevantcorrection as well as the information about eacherror type.
The tagset consists of 28 categories.Table 2 illustrates the 11 most frequent errors inthe development data; errors are marked with anasterisk, and ?
denotes a missing word.
The ma-jority of these errors are related to grammar butalso include mechanical, collocation, and other er-rors.An F-based scorer, named M2, was used toscore the systems (Dahlmeier and Ng, 2012).
Themetric in CoNLL-2014 was F0.5, i.e.
weighingprecision twice as much as recall.
Two types ofannotations were used: original and revised.
Wefollow the recommendations of the organizers anduse the original data (Ng et al, 2014).The approaches varied widely: classifiers, MT,rules, hybrid systems.
Table 3 summarizes the topfive systems.
The top team used a hybrid systemthat combined rules and MT.
The second systemdeveloped classifiers for common grammatical er-rors.
The third system used MT.As for external resources, the top 1 and top 3teams used additional learner data to train theirMT systems, the Cambridge University PressLearners?
Corpus and the Lang-8 corpus (Mizu-moto et al, 2011), respectively.
Many teams alsoused native English datasets.
The most commonones are the Web1T corpus (Brants and Franz,2006), the CommonCrawl dataset, which is sim-ilar to Web1T, and the English Wikipedia.
Severalteams used off-the-shelf spellcheckers.In addition, Susanto et al (2014) made an at-tempt at combining MT and classifiers.
Theyused CoNLL-train and Lang-8 as non-native dataand English Wikipedia as native data.
We be-lieve that the reason this study did not yield sig-nificant improvements (Table 1) is that individualstrengths of each framework have not been fullyexploited.
Further, each system was applied sepa-rately and decisions were combined using a gen-eral MT combination technique (Heafield et al,2009).
Finally, Mizumoto and Matsumoto (2016)attempt to improve an MT system also trainedon Lang-8 with discriminative re-ranking usingpart-of-speech (POS) and dependency features butonly obtain a small improvement.
These resultssuggest that standard combination and re-rankingtechniques are not sufficient.2.2 Overview of the State-of-the-ArtThe statistical machine translation approach isbased on the noisy-channel model.
The best trans-lation for a foreign sentence f is:e?= argmaxep(e)p(f |e)2206Error type Rel.
freq.
(%) ExamplesArticle (ArtOrDet) 19.93*?/The government should help encourage *the/?
breakthroughs as wellas *a/?
complete medication system .Wrong collocation (Wci) 12.51Some people started to *think/wonder if electronic products can replacehuman beings for better performances .Noun number (Nn) 11.44There are many reports around the internet and on newspaper stating thatsome users ?
*iPhone/iPhones exploded .Preposition (Prep) 8.98 I do not agree *on/with this argument...Word form (Wform) 6.56...the application of surveillance technology serves as a warning to the*murders/murderers and they might not commit more murder .Orthography/punctuation (Mec) 5.75Even British Prime Minister , Gordon Brown *?/, has urged that all carsin *britain/Britain to be green by 2020 .Verb tense (Vt) 4.56Through the thousands of years , most Chinese scholars *are/{havebeen} greatly affected by Confucianism .Linking words/phrases (Trans) 4.10 *However/Although , video surveillance may be a great help .Local redundancy (Rloc-) 3.70Some solutions *{as examples}/?
would be to design plants/fertilizersthat give higher yield ...Subject-verb agreement (SVA) 3.58 However , tracking people *are/is different from tracking goods .Verb form (Vform) 3.52 Travelers survive in desert thanks to GPS *guide/guiding them .Table 2: Example errors.
In the parentheses, the error codes used in the shared task are shown.
Errorsexemplifying the relevant phenomena are marked; the sentences may contain other mistakes.Rank System F0.5 Approach External training data Externalname Native data Learner data error modules1 CAMB 37.33 Rules and MT Microsoft Web LMCambridge Corpus, Eng.Vocab ProfileCambridge ?Writeand Improve?2 CUUI 36.79 Classif.
; patterns Web1T3 AMU 35.01 MT Wikipedia, CommonCrawl Lang-84 POST 30.88 LM and rules Web1T PyEnchant Spell5 NTHU 29.92Rules, MT, clas-sif.Web1T, Gigaword, BNC,Google BooksSpellcheckers: As-pell, GingerItTable 3: The top 5 systems in CoNLL-2014.
The last column lists external proofing tools used.
LMstands for language models.The model consists of two components: a lan-guage model assigning a probability p(e) for anytarget sentence e, and a translation model that as-signs a conditional probability p(f |e).
The lan-guage model is learned using a monolingual cor-pus in the target language.
The parameters ofthe translation model are estimated from a par-allel corpus, i.e.
the set of foreign sentencesand their corresponding translations into the tar-get language.
In error correction, the task is castas translating from erroneous learner writing intocorrected well-formed English.
The MT approachrelies on the availability of a parallel corpus forlearning the translation model.
In case of errorcorrection, a set of learner sentences and their cor-rections functions as a parallel corpus.State-of-the-art MT systems are phrase-based,i.e.
parallel data is used to derive a phrase-basedlexicon (Koehn et al, 2003).
The resulting lexiconconsists of a list of pairs (seqf, seqe) where seqfis a sequence of one or more foreign words, seqeis a predicted translation.
Each pair comes withan associated score.
At decoding time, all phrasesfrom sentence f are collected with their corre-sponding translations observed in training.
Theseare scored together with the language modelingscores and may include other features.
The phrase-based approach by Koehn et al (2003) uses a log-linear model (Och and Ney, 2002), and the bestcorrection maximizes the following:e?= argmaxeP (e|f) (1)= argmaxeexp(M?m=1?mhm(e, f))where hmis a feature function, such as lan-guage model score and translation scores, and ?mcorresponds to a feature weight.The classifier approach is based on the context-sensitive spelling correction methodology (Gold-ing and Roth, 1996; Golding and Roth, 1999;Banko and Brill, 2001; Carlson et al, 2001; Carl-son and Fette, 2007) and goes back to earlier ap-proaches to article and preposition error correction(Izumi et al, 2003; Han et al, 2006; Gamon etal., 2008; Felice and Pulman, 2008; Tetreault etal., 2010; Gamon, 2010; Dahlmeier and Ng, 2011;Dahlmeier and Ng, 2012).
The classifier approachto error correction has been prominent for a longtime before MT, since building a classifier does notrequire having annotated learner data.2207Property MT Classifier(1a) Error coverage: ability to address a widevariety of error phenomena+All errors occurring in the train-ing data are automatically covered-Only errors covered by the classi-fiers; new errors need to be addedexplicitly(1b) Error complexity: ability to handle com-plex and interacting mistakes that go beyondword boundaries+Automatically through paralleldata, via phrase-based lexicons-Need to develop via specific ap-proaches(2) Generalizability: going beyond the errorconfusions observed in training-Only confusions observed intraining can be corrected+Easily generalizable via confu-sion sets and features(3) Supervision/Annotation: role of learnerdata in training the system-Required +Not required(4) System flexibility: adapting knowledgesources per error phenomena-Not easy to integrate error-specific knowledge resources+Flexible; phenomenon-specificknowledge sourcesTable 4: Summary of the key properties of the MT and the classifier-based approaches.
We use +and ?
to indicate a positive or a negative value with respect to each factor.Classifiers are trained individually for a specificerror type.
Because an error type needs to be de-fined, typically only well-defined mistakes can beaddressed in a straightforward way.
Given an errortype, a confusion set is specified and includes a listof confusable words.
For some errors, confusionsets are constructed using a closed list (e.g.
prepo-sitions).
For other error types, NLP tools are re-quired.
To identify locations where an article waslikely omitted incorrectly, for example, a phrasechunker is used.
Each occurrence of a confusableword in text is represented as a vector of featuresderived from a context window around the target.The problem is cast as a multi-class classificationtask.In the classifier paradigm, there are various al-gorithms ?
generative (Gamon, 2010; Park andLevy, 2011), discriminative (Han et al, 2006;Gamon et al, 2008; Felice and Pulman, 2008;Tetreault et al, 2010), and joint approaches(Dahlmeier and Ng, 2012; Rozovskaya and Roth,2013).
Earlier works trained on native data (dueto lack of annotation).
Later approaches incorpo-rated learner data in training in various ways (Hanet al, 2010; Gamon, 2010; Rozovskaya and Roth,2010a; Dahlmeier and Ng, 2011).3 Error Analysis of MT and ClassifiersThis section presents error analysis of the MTand classifier approaches.
We begin by identify-ing several key properties that distinguish betweenMT systems and classifier systems and that we useto characterize the learning frameworks and theoutputs of the systems:(1a) Error coverage denotes the ability of a sys-tem to identify and correct a variety of error types.
(1b) Error complexity indicates the capacity of asystem to address complex mistakes such as thosewhere multiple errors interact.
(2) Generalizibility refers to the ability of a sys-tem to identify mistakes in new unseen contextsand propose corrections beyond those observed intraining data.
(3) The role of supervision or having annotatedlearner data for training.
(4) System flexibility is a property of the systemthat allows it to adapt resources specially to correctvarious phenomena.
The two paradigms are sum-marized in Table 4.
We use + and ?
to indicatewhether a learning framework has desirable (+) orundesirable characteristic with regard to each fac-tor.The first three properties characterize systemoutput, while (3) and (4) arise from the systemframeworks.
Below we analyze the output of sev-eral state-of-the-art CoNLL-2014 systems in moredetail.1Section 4 explores (3) and (4) that relateto the learning frameworks.3.1 Error Coverage and ComplexityError coverage To understand how systems differwith respect to error coverage, we consider recallof each system per error type.
Error-type recallcan be easily computed using error tags and is re-ported in the CoNLL overview paper.The recall numbers show substantial variationsamong the systems.
If we consider error cat-egories that have non-negligible recall numbers(higher than 10%), classifier-based approacheshave a much lower proportion of error types forwhich 10% recall was achieved.
Among the 28 er-ror types, the top classifier systems ?
ColumbiaUniversity-University of Illinois (CUUI, top-2)and National Tsing Hua University (NTHU, top-5) ?
have a recall higher than 10% for 8 and 9error types, respectively.
In contrast, the two MT-based systems ?
Cambridge University (CAMB,1Outputs are available on the CoNLL-2014 website.2208(1) It is a concern that will be with us *{during our whole life}/{for our entire life} .
(2) The decision to inform relatives of *{such genetic disorder}/{such genetic disorders} will be dependent .
.
.
(3) .. we need to respect it and we have no right *{in saying}/{to say} that he must tell his relatives about it .
(4) ...and his family might be a *{genetically risked}/{genetic risk} family .
(5) ...he was *diagnosis/{diagnosed with} a kind of genetic disease which is very serious .
(6) The situation may become *worst/worse if the child has diseases like cancer or heart disease .
.
.Table 5: Complex and interacting mistakes that MT successfully addresses.
Output of the MT-basedAMU system.top-1) and the Adam Mickiewicz University sys-tem (AMU, top-3) ?
have 15 and 17 error types,respectively, for which the recall is at least 10%.These recall discrepancies indicate that the MTapproach has a better overall coverage, which isintuitive given that all types of confusions are au-tomatically added through phrase-based transla-tion tables in MT, while classifiers must explicitlymodel each error type.
Note, however, that thesenumbers do not necessarily indicate good type-based performance, since high recall may corre-spond to low precision.Error complexity In the MT approach, error con-fusions are learned automatically via the phrasetranslation tables extracted from the parallel train-ing data.
Thus, an MT system can easily handle in-teracting and complex errors where replacementsinvolve a sequence of words.
Table 5 illustratescomplex and interacting mistakes that the MT ap-proach is able to handle.
Example (1) contains aphrase-level correction that includes both a prepo-sition replacement and an adjective change.
(2) isan instance of an interacting mistake where thereis a dependency between the article and the nounnumber, and a mistake can be corrected by chang-ing one of the properties but not both.
(3), (4) and(5) require multiple simultaneous corrections onvarious words in a phrase.
(6) is an example of anincorrect adjectival form, an error that is typicallynot modeled with standard classifiers.3.2 GeneralizabilityBecause MT systems extract error/correction pairsfrom phrase-translation tables, they can only iden-tify erroneous surface forms observed in trainingand propose corrections that occurred with the cor-responding surface forms.
Crucially, in a standardMT scenario, any resulting translation consists of?matches?
mined from the translation tables, soa standard MT model lacks lexical abstractionsthat might help generalize, thus out-of-vocabularywords is a well-known problem in MT (Daumeand Jagarlamudi, 2011).
While more advancedMT models can abstract by adding higher-levelError AMU (MT) CUUI (Classif.
)type P R F0.5 P R F0.5Orthog./punc.
(Mec) 61.6 16.3 39.6 53.3 8.7 26.4Article (ArtOrDet) 38.0 10.9 25.4 31.8 47.9 34.0Preposition (Prep) 54.9 10.4 29.5 31.7 8.8 20.9Noun number (Nn) 49.6 43.2 48.2 42.5 46.2 43.2Verb tense (Vt) 30.2 9.3 20.8 61.1 5.4 19.9Subj.-verb agr.
(SVA) 48.3 14.9 33.3 57.7 57.7 57.7Verb form (Vform) 40.5 16.8 31.8 69.2 15.1 40.3Word form (Wform) 59.0 36.6 52.6 60.0 13.5 35.6Table 6: Performance of MT and classifier sys-tems from CoNLL-2014 on common errors.features such as POS, previous attempt yieldedonly marginal improvements (Mizumoto and Mat-sumoto, 2016), since one typically needs differenttypes of abstractions depending on the error type,as we show below.With classifiers, it is easy to generalize usinghigher-level information that goes beyond surfaceform and to adjust the abstraction to the error type.Many grammatical errors may benefit from gener-alizations based on POS or parse information; wecan thus expect that classifiers will do better onerrors that require linguistic abstractions.To validate this hypothesis, we evaluate type-based performance of two systems: a top-3 MT-based AMU system and a top-2 classifier-basedCUUI; we do not include the top-1 system, sinceit is a hybrid system that also uses rules.Unlike recall, estimating type-based precisionrequires knowing the type of the correction sup-plied by the system, which is not specified in theoutput.
We thus manually analyze the output ofthe AMU and CUUI systems for seven commonerror categories and assign to each correction anappropriate type to estimate precision and F0.5(Table 6).
The CUUI system addresses all of theseerrors, with the exception of mechanical (Mec), ofwhich it handles a small subset.
The AMU sys-tem does better on mechanical, preposition, wordform, and noun number.
CUUI does better on ar-ticles, verb agreement, and verb form.We now consider examples of errors that arecorrected by the classifier-based CUUI system inthese three categories but are missed by the MT-based AMU system (Table 7).
Examples (1) and2209Long-distance dependencies: verb agreement(1)As a result , in the case that when one of the members *happen/happens to feel uncomfortable or abnormal , he or sheshould be aware that .
.
.
(2)A study of New York University in 2010 shown that patients with family members around generally *recovers/recover2-4 days faster than those taken care by professional nurses .Confusions not found in training: verb agreement and verb form(3) Hence , the social media sites *serves/serve as a platform for the connection .
(4) After *came/coming back from the hospital , the man told his parents that the problem was that he carried .
.
.
(5) social media is the only resource they can approach to know everything *happened/happening in their country .
.
.Superfluous words: articles(6) For *an/?
example , if exercising is helpful, we can always look for more chances for the family to go exercise .
(7) .
.
.
as soon a person is made aware of his or her genetic profile , he or she has *a/?
knowledge about others .Omissions: articles(8) In this case , if one of the family members or close relatives is found to carry *?/a genetic risk .
.
.Table 7: Generalizing beyond surface form: Examples of mistakes that classifiers successfully address.Output of the classifier-based CUUI system.
(2) illustrate verb errors with long-distance sub-jects (?one?
and ?patients?).
This is handled inthe classification approach via syntactic features.An MT system misses these errors because it islimited to edits within short spans.
Examples (3),(4), and (5) illustrate verb mistakes for which thecorrect replacements were not observed in train-ing but that are nonetheless corrected by general-izing beyond surface form.
Finally, (6) and (7)illustrate omission and insertion errors, a major-ity of article mistakes.
The MT system is espe-cially bad at correcting such mistakes.
Notably,the classifier-based CUUI system correctly identi-fied twice as many omitted articles and more than20 times more superfluous articles than the MT-based AMU system.
This happens because an MTsystem is restricted to suggesting deletions and in-sertions in those contexts that were observed intraining, whereas a classifier uses shallow parse in-formation, which allows it to insert or delete an ar-ticle in front of every eligible noun phrase.
Theseexamples demonstrate that the ability of a systemto generalize beyond the surface forms is indeedbeneficial for long-distance dependencies, for ab-stracting away from surface forms when formu-lating confusion sets, and for mistakes involvingomitting or inserting a word.4 Developing New State-of-the-Art MTand Classifier SystemsIn this section, we explore the advantages of eachlearning approach, as identified in the previoussection, within each learning framework.
To thisend, drawing on the strengths of each framework,we develop new state-of-the-art MT and classifiersystems.2In the next section, we will use these2Implementation details can be found at cogcomp.cs.illinois.edu/page/publication view/793System Learner NativeCoNLL-trainLang-8 Eng.Wiki.Web1T1.2M 48M 2B 1TMT X X X -Classif.
X - - XTable 8: Data used in the experiments.
Corporasizes are in the number of words.MT and classifier components and show how toexploit the strengths of each framework in combi-nation.
Table 8 summarizes the data used.
Resultsare reported with respect to all errors in the testdata.
This is different from performance for indi-vidual errors in Table 6.4.1 Machine Translation SystemsA key advantage of the MT framework is that, un-like with classifiers, error confusions are learnedfrom parallel data automatically, without further(linguistic) input.
We build two MT systems thatdiffer only in the use of parallel data: the CoNLL-2014 training data and Lang-8.
Our MT systemsare trained using Moses (Koehn et al, 2007) andfollow the standard approach (Junczys-Dowmuntand Grundkiewicz, 2014; Susanto et al, 2014).Both systems use two 5-gram language models?
English Wikipedia and the corrected side ofCoNLL-train ?
trained with KenLM (Heafield etal., 2013).
Table 9 reports the performance ofthe systems.
As shown, performance increases bymore than 11 points when a larger parallel corpusis used.
The best MT system outperforms the topCoNLL system by 2 points.4.2 ClassifiersWe now present several classifier systems, explor-ing the two important properties of the classifica-tion framework ?
the ability to train without super-2210Parallel data PerformanceP R F0.5CoNLL-train 43.34 11.81 28.25Lang-8 66.15 15.11 39.48CoNLL-2014 top 1 39.71 30.10 37.33Table 9: MT systems trained in this work.vision and system flexibility (see Table 4).4.2.1 SupervisionSupervision in the form of annotated learner dataplays an important role in developing an error cor-rection system but is expensive.
Native data, incontrast, is cheap and available in large quantities.Therefore, the fact that, unlike with MT, it is pos-sible to build a classifier system without any anno-tated data, is a clear advantage of classifiers.Training without supervision is possible in theclassification framework, as follows.
For a givenmistake type, e.g.
preposition, a classifier istrained on native data that is assumed to be cor-rect; the classifier uses context words around eachpreposition as features.
The resulting model isthen applied to learner prepositions and will pre-dict the most likely preposition in a given con-text.
If the preposition predicted by the classi-fier is different from what the author used in text,this preposition is flagged as a mistake.
We referthe reader to Rozovskaya and Roth (2010b) andRozovskaya and Roth (2011) for a description oftraining classifiers with and without supervisionfor error correction tasks.
Below, we address twoquestions related to the use of supervision:?
Training with supervision: When training us-ing learner data, how does a classifier-based sys-tem compare against an MT system??
Training without supervision: How well canwe do by building a classifier system with nativedata only, compared to MT and classifier-basedsystems that use supervision?Our classifier system is based on the imple-mentation framework of the second CoNLL-2014system (Rozovskaya et al, 2014) and consists ofclassifiers for 7 most common grammatical errorsin CoNLL-train: article; preposition; noun num-ber; verb agreement; verb form; verb tense; wordform.
All modules take as input the corpus doc-uments pre-processed with a POS tagger3(Even-Zohar and Roth, 2001), a shallow parser4(Pun-3http://cogcomp.cs.illinois.edu/page/software view/POS4http://cogcomp.cs.illinois.edu/page/software view/ChunkerSystem PerformanceP R F0.5Classifiers (learner) 32.15 17.96 27.76Classifiers (native) 38.41 23.05 33.89MT 43.34 11.81 28.25CoNLL-2014 top 1 39.71 30.10 37.33CoNLL-2014 top 2 41.78 24.88 36.79CoNLL-2014 top 3 41.62 21.40 35.01Table 10: Classifier systems trained with andwithout supervision.
Learner data refers toCoNLL-train.
Native data refers to Web1T.
TheMT system uses CoNLL-train for parallel data.yakanok and Roth, 2001), a syntactic parser (Kleinand Manning, 2003) and a dependency converter(Marneffe et al, 2006).Classifiers are trained either on learner data(CoNLL-train) or native data (Web1T).
Classifiersbuilt on CoNLL-train are trained discriminativelywith the Averaged Perceptron algorithm (Rizzoloand Roth, 2010) and use rich POS and syntacticfeatures tailored to specific error types that arestandard for these tasks (Lee and Seneff, 2008;Han et al, 2006; Tetreault et al, 2010; Ro-zovskaya et al, 2011); Na?
?ve Bayes classifiers aretrained on Web1T with word n-gram features.
Adetailed description of the classifiers and the fea-tures used can be found in Rozovskaya and Roth(2014).
We also add several novel ideas that aredescribed below.Table 10 shows the performance of two classi-fier systems, trained with supervision (on CoNLL-train) and without supervision on native data(Web1T), and compares these to an MT approachtrained on CoNLL-train.
The first classifier systemperforms comparably to the MT system (27.76 vs.28.25), however, the native-trained classifier sys-tem outperforms both, and does not use any an-notated data.
The native-trained classifier systemwould place fourth in CoNLL-2014.4.2.2 FlexibilityWe now explore another advantage of theclassifier-based approach, that of allowing for aflexible architecture where we can tailor knowl-edge sources for individual phenomena.
In Sec-tion 4.2.1, we already took advantage of the factthat in the classifier framework it is easy to in-corporate features suited to individual error types.We now show that by adding supervision in a waytailored toward specific errors we can further im-prove the classifier-based approach.Adding Supervision in a Tailored Way There isa trade-off between training on native and learner2211Training Performancedata P R F0.5(1) Learner 32.15 17.96 27.76(2) Native 38.41 23.05 33.89(3) Tailored 57.07 14.74 36.26Table 11: Classifiers: supervision in a tailoredway.
Trained on (1) learner data (CoNLL-train);(2) native data (Web1T); (3) data sources tailoredper error type.data.
The advantage of training on native data isclearly the size, which is important for estimatingcontext parameters.
Learner data provides addi-tional information, such as learner error patternsand the manner of non-native writing.Instead of choosing to train on one data type,the classifier framework allows one to combinethe two data sources in various ways: voting (Ro-zovskaya et al, 2014), alternating structure op-timization (Dahlmeier and Ng, 2011), training ameta-classifier (Gamon, 2010), and extracting er-ror patterns (Rozovskaya and Roth, 2011).
Wecompare two approaches of adding supervision:(1) Learner error patterns: Error patterns are ex-tracted from learner data and ?injected?
into mod-els trained on native data (Rozovskaya and Roth,2011).
Learner data is used to estimate mistake pa-rameters; contextual cues are based on native data.
(2) Learner error patterns+native predictions:Classifiers are trained on native data.
Classifierpredictions are used as features in models trainedon learner data.
Learner data thus contributes boththe specific manner of learner writing and the mis-take parameters.
The native data contributes con-textual information.We found that (2) is superior to (1) for arti-cle, agreement, and preposition errors; (1) worksbetter on verb form and word form errors; andnoun number errors perform best when a classifieris trained on native data.
(Learner error patternswere found not to be beneficial for correcting nounnumber errors (Rozovskaya and Roth, 2014)).
Tai-lored supervision yields an improvement of almost3 points over the system trained on native data andalmost 9 points over the system trained on learnerdata (Table 11).Adding Mechanical Errors Finally, we addcomponents for mechanical errors: punctuation,spelling, and capitalization.
These are distin-guished from the grammatical mistakes, as theyare not specific to GEC and can be handled withexisting resources or simple methods.For capitalization and missing commas, weSystem/training data PerformanceP R F0.5Native 38.41 23.05 33.89Native+mechanical 42.72 27.69 38.54Tailored 57.07 14.74 36.26Best (tailored+mechanical) 60.79 19.93 43.11CoNLL-2014 top system 39.71 30.10 37.33Susanto et al (2014) 53.55 19.14 39.39Miz.
& Mats.
(2016) 45.80 26.60 40.00Table 12: Classifier systems in this work.
Com-parison to existing state-of-the-art.System PerformanceP R F0.5MT is trained on CoNLL-trainMT 43.34 11.81 28.25Spelling+MT 49.86 16.36 35.37Article+MT 45.11 13.99 31.22Verb agr.+MT 46.36 14.63 32.33Art.+Verb agr.+Spell+MT 52.07 20.89 40.10MT is trained on Lang-8MT 66.15 15.11 39.48Spelling+MT 65.87 16.94 41.75Article+MT 63.81 17.70 41.95Verb.
agr.+MT 66.09 18.01 43.08Art.+Verb agr.+Spell+MT 64.13 22.15 46.51Table 13: Pipelines: select classifiers and MT.compile a list of patterns using CoNLL trainingdata.
We also use an off-the-shelf speller (Flor,2012; Flor and Futagi, 2012).
Results are shown inTable 12.
Performance improves by almost 5 and7 points for the native-trained system and for thebest configuration of classifiers with supervision.Both systems also outperform the top CoNLL sys-tem, by 1 and 6 points, respectively.
The result of43.11 by the best classifier configuration substan-tially outperforms the existing state-of-the-art: acombination of two MT systems and two classi-fier systems, and MT with re-ranking (Susanto etal., 2014; Mizumoto and Matsumoto, 2016).5 Combining MT and Classifier SystemsSince MT and classifiers differ with respect tothe types of errors they can better handle, wecombine these systems in a pipeline architecturewhere the MT is applied to the output of classi-fiers.
Classifiers are applied first, since MT is bet-ter at handling complex phenomena.
First, we addthe speller and those classifier components thatperform substantially better than MT (articles andverb agreement), due to the ability of classifiers togeneralize beyond lexical information.
The addedclassifiers are part of the best system in Table 12.Results are shown in Table 13.
Adding classi-fiers improves the performance, thereby demon-2212System PerformanceP R F0.5MT (CoNLL-train) 43.34 11.81 28.25MT (Lang-8) 66.15 15.11 39.48Best classifier (Table 12) 60.79 19.93 43.11Best class.+MT (CoNLL-train) 51.92 25.08 42.77Best class.+MT (Lang-8) 60.17 25.64 47.40Table 14: Pipelines: the best classifier systemand MT systems.System PerformanceP R F0.5Best classifier (Table 12) 60.79 19.93 43.11Art.+Verb agr.+Spell+MT 64.13 22.15 46.51Best classifier+MT 60.17 25.64 47.40CoNLL-2014 top system 39.71 30.10 37.33Susanto et al (2014) 53.55 19.14 39.39Miz.
& Mats.
(2016) 45.80 26.60 40.00Table 15: Best systems in this work.
Comparisonto existing state-of-the-art.strating that the classifiers address a complemen-tary set of mistakes.
Adding all three modules im-proves the results from 28.25 to 40.10 and from39.48 to 46.51 for the MT systems trained onCoNLL-train and Lang-8, respectively.
Notably,the CoNLL-train MT system especially benefits,which shows that when the parallel data is small,it is particularly worthwhile to add classifiers.It should be stressed that even with a smallerparallel corpus, when the three modules are added,the resulting system is very competitive with pre-vious state-of-the-art that uses a lot more super-vision: Susanto et al (2014) and Mizumoto andMatsumoto (2016) use Lang-8.
These resultsshow that when one has an MT system, it is possi-ble to improve by investing effort into building se-lect classifiers for phenomena that are most chal-lenging for MT.Finally, Table 14 demonstrates that combiningMT with the best classifier system improves theresult further when the MT system is trained onLang-8, but not when the MT system is trained onCoNLL-train.
We also note that the CoNLL-trainMT system also has a much lower precision thanthe other systems.
We conclude that when only alimited amount of data is available, the classifierapproach on its own performs better.As a summary, Table 15 lists the best sys-tems developed in this work ?
a classifier sys-tem, a pipeline of select classifiers and MT, anda pipeline consisting of the best classifier and theMT systems ?
and compares to existing state-of-the-art.
Our classifier system is a 3-point improve-ment over the existing state-of-the-art, while thebest pipeline is a 7.4-point improvement (20% rel-ative improvement).6 Discussion and ConclusionsA recent surge in GEC research has producedtwo leading state-of-the-art approaches ?
machinelearning classification and machine translation.Based on the analysis of the methods and an er-ror analysis on the outputs of state-of-the-art sys-tems that adopt these approaches, we explainedthe differences and the key advantages of each.With respect to error phenomena, we showed thatwhile MT is better at handling complex mistakes,classifiers are better at correcting mistakes that re-quire abstracting beyond lexical context.
We fur-ther showed that the key strengths of the classifi-cation framework are its flexibility and the abilityto train without supervision.We built several systems that draw on thestrengths of each approach individually and ina pipeline.
The best classifier system and thepipelines outperform reported best results on thetask, often by a large margin.The purpose of this work is to gain a betterunderstanding of the advantages offered by eachlearning method in order to make further progresson the GEC task.
We showed that the values pro-vided by each method can be exploited within eachapproach and in combination, depending on the re-sources available, such as annotated learner data(MT), and additional linguistic resources (clas-sifiers).
As a result, we built robust systemsand showed substantial improvement over existingstate-of-the-art.For future work, we intend to study the problemin the context of other languages.
However, it isimportant to realize that the problem is far frombeing solved even in English, and the current workmakes very significant progress on it.AcknowledgmentsThe authors thank Michael Flor for his help with running thespelling system on the data and John Wieting for sharing theEnglish Wikipedia corpus.
The authors are also grateful toMark Sammons, Peter Chew, and the anonymous reviewersfor the insightful comments on the paper.
The work of DanRoth on this project was supported by DARPA under agree-ment number FA8750-13-2-0008.
Any opinions, findings,conclusions or recommendations are those of the authors anddo not necessarily reflect the view of the agencies.ReferencesM.
Banko and E. Brill.
2001.
Scaling to very very large cor-pora for natural language disambiguation.
In Proceedings2213of 39th Annual Meeting of the Association for Computa-tional Linguistics, pages 26?33, Toulouse, France, July.T.
Brants and A. Franz.
2006.
Web 1T 5-gram Version 1.Linguistic Data Consortium.A.
Carlson and I. Fette.
2007.
Memory-based context-sensitive spelling correction at web scale.
In Proceedingsof the IEEE International Conference on Machine Learn-ing and Applications (ICMLA).A.
J. Carlson, J. Rosen, and D. Roth.
2001.
Scaling up con-text sensitive text correction.
In IAAI.D.
Dahlmeier and H. T. Ng.
2011.
Grammatical error correc-tion with alternating structure optimization.
In Proceed-ings of ACL.D.
Dahlmeier and H.T Ng.
2012.
A beam-search decoder forgrammatical error correction.
In Proceedings of EMNLP-CoNLL.D.
Dahlmeier, H.T.
Ng, and S.M.
Wu.
2013.
Building a largeannotated corpus of learner English: The NUS corpus oflearner English.
In Proceedings of the NAACL Workshopon Innovative Use of NLP for Building Educational Appli-cations.R.
Dale and A. Kilgarriff.
2011.
Helping Our Own: TheHOO 2011 pilot shared task.
In Proceedings of the 13thEuropean Workshop on Natural Language Generation.R.
Dale, I. Anisimoff, and G. Narroway.
2012.
A reporton the preposition and determiner error correction sharedtask.
In Proceedings of the NAACL Workshop on Innova-tive Use of NLP for Building Educational Applications.H.
Daume and J. Jagarlamudi.
2011.
Domain adaptation formachine translation by mining unseen words.
In ACL.Y.
Even-Zohar and D. Roth.
2001.
A sequential model formulti class classification.
In Proceedings of EMNLP.R.
De Felice and S. Pulman.
2008.
A classifier-based ap-proach to preposition and determiner error correction inL2 English.
In Proceedings of the 22nd InternationalConference on Computational Linguistics (Coling 2008),pages 169?176, Manchester, UK, August.M.
Felice, Z. Yuan, ?.
Andersen, H. Yannakoudakis, andE.
Kochmar.
2014.
Grammatical error correction usinghybrid systems and type filtering.
In Proceedings of theEighteenth Conference on Computational Natural Lan-guage Learning: Shared Task.M.
Flor and Y. Futagi.
2012.
On using context for auto-matic correction of non-word misspellings in student es-says.
In Proceedings of the 7th Workshop on InnovativeUse of NLP for Building Educational Applications.
Asso-ciation for Computational Linguistics.M.
Flor.
2012.
Four types of context for automatic spellingcorrection.
Traitement Automatique des Langues (TAL).
(Special Issue: Managing noise in the signal: error han-dling in natural language processing), 3(53):61?99.M.
Gamon, J. Gao, C. Brockett, A. Klementiev, W. Dolan,D.
Belenko, and L. Vanderwende.
2008.
Using contextualspeller techniques and language modeling for ESL errorcorrection.
In Proceedings of IJCNLP.M.
Gamon.
2010.
Using mostly native data to correct errorsin learners?
writing.
In Proceedings of NAACL.A.
R. Golding and D. Roth.
1996.
Applying Winnow tocontext-sensitive spelling correction.
In ICML.A.
R. Golding and D. Roth.
1999.
A Winnow based ap-proach to context-sensitive spelling correction.
MachineLearning.N.
Han, M. Chodorow, and C. Leacock.
2006.
Detecting er-rors in English article usage by non-native speakers.
Jour-nal of Natural Language Engineering, 12(2):115?129.N.
Han, J. Tetreault, S. Lee, and J. Ha.
2010.
Using an error-annotated learner corpus to develop and ESL/EFL errorcorrection system.
In Proceedings of LREC.K.
Heafield, G. Hanneman, and A. Lavie.
2009.
Machinetranslation system combination with flexible word order-ing.
In Proceedings of the Fourth Workshop on StatisticalMachine Translation.
Association for Computational Lin-guistics.K.
Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.
2013.Scalable modified Kneser-Ney language model estima-tion.
In ACL.E.
Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isahara.2003.
Automatic error detection in the Japanese learners?English spoken data.
In Proceedings of ACL.M.
Junczys-Dowmunt and R. Grundkiewicz.
2014.
TheAMU system in the CoNLL-2014 shared task: Grammati-cal error correction by data-intensive and feature-rich sta-tistical machine translation.
In Proceedings of the Eigh-teenth Conference on Computational Natural LanguageLearning: Shared Task.D.
Klein and C. D. Manning.
2003.
Fast exact inferencewith a factored model for natural language parsing.
InProceedings of NIPS.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statistical phrase-based translation.
In ACL.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Fed-erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens,C.
Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007.Moses: Open source toolkit for statistical machine trans-lation.
In ACL.J.
Lee and S. Seneff.
2008.
An analysis of grammatical errorsin non-native speech in English.
In Proceedings of the2008 Spoken Language Technology Workshop.M.
Marneffe, B. MacCartney, and Ch.
Manning.
2006.
Gen-erating typed dependency parses from phrase structureparses.
In Proceedings of LREC.T.
Mizumoto and Y. Matsumoto.
2016.
Discriminativereranking for grammatical error correction with statisticalmachine translation.
In NAACL.
To appear.T.
Mizumoto, M. Komachi, M. Nagata, and Y. Matsumoto.2011.
Mining revision log of language learning SNS forautomated japanese error correction of second languagelearners.
In IJCNLP.H.
T. Ng, S. M. Wu, Y. Wu, Ch.
Hadiwinoto, and J. Tetreault.2013.
The CoNLL-2013 shared task on grammatical errorcorrection.
In Proceedings of CoNLL: Shared Task.2214H.
T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H. Su-santo, and C. Bryant.
2014.
The CoNLL-2014 sharedtask on grammatical error correction.
In Proceedings ofCoNLL: Shared Task.F.
J. Och and H. Ney.
2002.
Discriminative training andmaximum entropy models for statistical machine transla-tion.
In ACL.A.
Park and R. Levy.
2011.
Automated whole sentencegrammar correction using a noisy channel model.
In ACL,Portland, Oregon, USA, June.
Association for Computa-tional Linguistics.V.
Punyakanok and D. Roth.
2001.
The use of classifiers insequential inference.
In Proceedings of NIPS.N.
Rizzolo and D. Roth.
2010.
Learning Based Java forRapid Development of NLP Systems.
In Proceedings ofLREC.A.
Rozovskaya and D. Roth.
2010a.
Generating confusionsets for context-sensitive error correction.
In Proceedingsof EMNLP.A.
Rozovskaya and D. Roth.
2010b.
Training paradigms forcorrecting errors in grammar and usage.
In Proceedingsof NAACL.A.
Rozovskaya and D. Roth.
2011.
Algorithm selection andmodel adaptation for ESL correction tasks.
In Proceed-ings of ACL.A.
Rozovskaya and D. Roth.
2013.
Joint learning and infer-ence for grammatical error correction.
In Proceedings ofEMNLP.A.
Rozovskaya and D. Roth.
2014.
Building a State-of-the-Art Grammatical Error Correction System.
In Transac-tions of ACL.A.
Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011.University of Illinois system in HOO text correctionshared task.
In Proceedings of the European Workshopon Natural Language Generation (ENLG).A.
Rozovskaya, K.-W. Chang, M. Sammons, D. Roth, andN.
Habash.
2014.
The University of Illinois andColumbia system in the CoNLL-2014 shared task.
In Pro-ceedings of CoNLL Shared Task.R.
H. Susanto, P. Phandi, and H. T. Ng.
2014.
System com-bination for grammatical error correction.
In EMNLP.J.
Tetreault, J.
Foster, and M. Chodorow.
2010.
Using parsefeatures for preposition selection and error detection.
InProceedings of ACL.2215
