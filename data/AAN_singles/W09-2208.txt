Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 58?65,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Simple Semi-supervised Algorithm ForNamed Entity RecognitionWenhui Liao and Sriharsha VeeramachaneniResearch and Develpment,Thomson Reuters610 Opperman Drive, Eagan MN 55123{wenhui.liao, harsha.veeramachaneni}@thomsonreuters.comAbstractWe present a simple semi-supervised learningalgorithm for named entity recognition (NER)using conditional random fields (CRFs).
Thealgorithm is based on exploiting evidence thatis independent from the features used for aclassifier, which provides high-precision la-bels to unlabeled data.
Such independent ev-idence is used to automatically extract high-accuracy and non-redundant data, leading to amuch improved classifier at the next iteration.We show that our algorithm achieves an aver-age improvement of 12 in recall and 4 in pre-cision compared to the supervised algorithm.We also show that our algorithm achieves highaccuracy when the training and test sets arefrom different domains.1 IntroductionNamed entity recognition (NER) or tagging is thetask of finding names such as organizations, persons,locations, etc.
in text.
Since whether or not a word isa name and the entity type of a name are determinedmostly by the context of the word as well as by theentity type of its neighbors, NER is often posed as asequence classification problem and solved by meth-ods such as hidden Markov models (HMM) and con-ditional random fields (CRF).Automatically tagging named entities (NE) withhigh precision and recall requires a large amount ofhand-annotated data, which is expensive to obtain.This problem presents itself time and again becausetagging the same NEs in different domains usuallyrequires different labeled data.
However, in mostdomains one often has access to large amounts ofunlabeled text.
This fact motivates semi-supervisedapproaches for NER.Semi-supervised learning involves the utilizationof unlabeled data to mitigate the effect of insuf-ficient labeled data on classifier accuracy.
Onevariety of semi-supervised learning essentially at-tempts to automatically generate high-quality train-ing data from an unlabeled corpus.
Algorithms suchas co-training (Blum and Mitchell, 1998)(Collinsand Singer, 1999)(Pierce and Cardie, 2001) andthe Yarowsky algorithm (Yarowsky, 1995) make as-sumptions about the data that permit such an ap-proach.The main requirement for the automatically gen-erated training data in addition to high accuracy,is that it covers regions in the feature space withlow probability density.
Furthermore, it is neces-sary that all the classes are represented according totheir prior probabilities in every region in the fea-ture space.
One approach to achieve these goals isto select unlabeled data that has been classified withlow confidence by the classifier trained on the orig-inal training data, but whose labels are known withhigh precision from independent evidence.
Here in-dependence means that the high-precision decisionrule that classifies these low confidence instancesuses information that is independent of the featuresused by the classifier.We propose two ways of obtaining such inde-pendent evidence for NER.
The first is based onthe fact that multiple mentions of capitalized to-kens are likely to have the same label and occurin independently chosen contexts.
We call this the58multi-mention property.
The second is based on thefact that entities such as organizations, persons, etc.,have context that is highly indicative of the class,yet is independent of the other context (e.g.
com-pany suffixes like Inc., Co., etc., person titles likeMr., CEO, etc.).
We call such context high precisionindependent context.Let us first look at two examples.Example 1:1) said Harry You, CEO of HearingPoint ....2) For this year?s second quarter, You said thecompany?s ...The classifier tags ?Harry You?
as person (PER)correctly since its context (said, CEO) makes it anobvious name.
However, in the second sentence, theclassifier fails to tag ?You?
as a person since ?You?is usually a stopword.
The second sentence is ex-actly the type of data needed in the training set.Example 2:(1) Medtronic Inc 4Q profits rise 10 percent...(2) Medtronic 4Q profits rise 10 percent...The classifier tags ?Medtronic?
correctly in thefirst sentence because of the company suffix ?Inc?while it fails to tag ?Medtronic?
in the secondsentence since ?4Q profits?
is a new pattern and?Medtronic?
is unseen in the training data.
Thus thesecond sentence is what we need in the training set.The two examples have one thing in common.
Inboth cases, the second sentence has a new patternand incorrect labels, which can be fixed by usingeither multi-mention or high-precision context fromthe first sentence.
We actually artificially constructthe second sentence to be added to the training set inExample 2 although only the first sentence exists inthe unlabeled corpus.By leveraging such independent evidence, ouralgorithm can automatically extract high-accuracyand non-redundant data for training, and thus ob-tain an improved model for NER.
Specifically, ouralgorithm starts with a model trained with a smallamount of gold data (manually tagged data).
Thismodel is then used to extract high-confidence data,which is then used to discover low-confidence databy using other independent features.
These low-confidence data are then added to the training datato retrain the model.
The whole process repeatsuntil no significant improvement can be achieved.Our experiments show that the algorithm is not onlymuch better than the initial model, but also betterthan the supervised learning when a large amountof gold data are available.
Especially, even whenthe domain from which the original training data issampled is different from the domain of the testingdata, our algorithm still provides significant gains inclassification accuracy.2 Related WorkThe Yarowsky algorithm (Yarowsky, 1995), orig-inally proposed for word sense disambiguation,makes the assumption that it is very unlikely for twooccurrences of a word in the same discourse to havedifferent senses.
This assumption is exploited byselecting words classified with high confidence ac-cording to sense and adding other contexts of thesame words in the same discourse to the trainingdata, even if they have low confidence.
This allowsthe algorithm to learn new contexts for the sensesleading to higher accuracy.
Our algorithm also usesmulti-mention features.
However, the applicationof the Yarowsky algorithm to NER involves severaldomain-specific choices as will become evident be-low.Wong and Ng (Wong and Ng, 2007) use the sameidea of multiple mentions of a token sequence be-ing to the same named entity for feature engineer-ing.
They use a named entity recognition modelbased on the maximum entropy framework to tag alarge unlabeled corpus.
Then the majority tags ofthe named entities are collected in lists.
The modelis then retrained by using these lists as extra fea-tures.
This method requires a sufficient amount ofmanually tagged data initially to work.
Their papershows that, if the initial model has a low F-score,the model with the new features leads to low F-scoretoo.
Our method works with a small amount of golddata because, instead of constructing new features,we use independent evidence to enrich the trainingdata with high-accuracy and non-redundant data.The co-training algorithm proposed by Blum andMitchell (Blum and Mitchell, 1998) assumes thatthe features can be split into two class-conditionallyindependent sets or ?views?
and that each view issufficient for accurate classification.
The classifierbuilt on one of the views is used to classify a largeunlabeled corpus and the data classified with high-59confidence are added to the training set on whichthe classifier on the other view is trained.
This pro-cess is iterated by interchanging the views.
Themain reason that co-training works is that, becauseof the class-conditional independence assumptions,the high-confidence data from one view, in additionto being highly precise, is unbiased when added tothe training set for the other view.
We could notapply co-training for semi-supervised named entityrecognition because of the difficulty of finding infor-mative yet class-conditionally independent featuresets.Collins et al(Collins and Singer, 1999) proposedtwo algorithms for NER by modifying Yarowsky?smethod (Yarowsky, 1995) and the framework sug-gested by (Blum and Mitchell, 1998).
However, alltheir features are at the word sequence level, insteadof at the token level.
At the token level, the seedrules they proposed do not necessarily work.
In ad-dition, parsing sentences into word sequences is nota trivial task, and also not necessary for NER, in ouropinion.Jiao et al propose semi-supervised conditionalrandom fields (Jiao et al, 2006) that try to maxi-mize the conditional log-likelihood on the trainingdata and simultaneously minimize the conditionalentropy of the class labels on the unlabeled data.This approach is reminiscent of the semi-supervisedlearning algorithms that try to discourage the bound-ary from being in regions with high density of unla-beled data.
The resulting objective function is nolonger convex and may result in local optima.
Ourapproach in contrast avoids changing the CRF train-ing procedure, which guarantees global maximum.3 Named Entity RecognitionAs long as independent evidence exists for one typeof NE, our method can be directly applied to classifysuch NE.
As an example, we demonstrate how to ap-ply our method to classify three types of NEs: orga-nization (ORG), person (PER), and location (LOC)since they are the most common ones.
A non-NE isannotated as O.3.1 Conditional Random Fields for NERWe use CRF to perform classification in our frame-work.
CRFs are undirected graphical models trainedto maximize the conditional probability of a se-quence of labels given the corresponding input se-quence.
Let X , X = x1...xN , be an input sequence,and Y , Y = y1....yN , be the label sequence for theinput sequence.
The conditional probability of Ygiven X is:P (Y |X) = 1Z(X) exp(N?n=1?k?kfk(yn?1, yn, X, n))(1)where Z(X) is a normalization term, fk is a featurefunction, which often takes a binary value, and ?kis a learned weight associated with the feature fk.The parameters can be learned by maximizing log-likelihood ` which is given by` = ?ilogP (Yi|Xi)?
?k?2k2?2k(2)where ?2k is the smoothing (or regularization) pa-rameter for feature fk.
The penalty term, used forregularization, basically imposes a prior distributionon the parameters.It has been shown that ` is convex and thus aglobal optimum is guaranteed (McCallum, 2003).Inferring label sequence for an input sequence Xinvolves finding the most probable label sequence,Y ?
= argmaxYP (Y |X), which is done by theViterbi algorithm (Forney, 1973).3.2 Features for NEROne big advantage of CRF is that it can naturallyrepresent rich domain knowledge with features.3.2.1 Standard FeaturesPart of the features we used for our CRF classifierare common features that are widely used in NER(McCallum and Li, 2003), as shown below.1) Lexicon.
Each token is itself a feature.2) Orthography.
Orthographic information isused to identify whether a token is capitalized, oran acronym, or a pure number, or a punctuation, orhas mixed letters and digits, etc.3) Single/multiple-token list.
Each list is a collec-tion of words that have a common sematic meaning,such as last name, first name, organization, companysuffix, city, university, etc.604) Joint features.
Joint features are the conjunc-tions of individual features.
For example, if a tokenis in a last name list and its previous token is in atitle list, the token will have a joint feature called asTitle+Name.5) Features of neighbors.
After extracting theabove features for each token, its features are thencopied to its neighbors (The neighbors of a token in-clude the previous two and next two tokens) with aposition id.
For example, if the previous token of atoken has a feature ?Cap@0?, this token will have afeature ?Cap@-1?.3.2.2 Label FeaturesOne unique and important feature used in our al-gorithm is called Label Features.
A label featureis the output label of a token itself if it is known.We designed some simple high-precision rules toclassify each token, which take precedence over theCRF.
Specifically, if a token does not include anyuppercase letter, is not a number, and it is not in thenocap list (which includes the tokens that are notcapitalized but still could be part of an NE, such asal, at, in, -, etc), the label of this token is ?O?.Table 1: An example of extracted featuresTokens FeatureMonday W=Monday@0 O@0vice W=vice@0 O@0chairman W=chairman@0 title@0 O@0Goff W=Goff@0 CAP@0 Lastname@0W=chairman@-1 title@-1 O@-1W=vice@-2 O@-2W=said@1 O@1 W=it@2 O@2said W=said@0 O@0the W=it@0 O@0company W=company@0 O@0In addition, if a token is surrounded by ?O?
to-kens and is in a Stopword list, or in a Time list (acollection of date, time related tokens), or in a no-cap list, or a nonNE list (a collection of tokens thatare unlikely to be an NE), or a pure number, its labelis ?O?
as well.
For example, in the sentence ?Fordhas said there is no plan to lay off workers?, all thetokens except ?Ford?
have ?O?
labels.
More rulescan be designed to classify NE labels.
For example,if a token is in an unambiguousORG list, it has alabel ?ORG?.For any token with a known label, unless it is aneighbor of a token with its label unknown (i.e., notpretagged with high precision), its features includeonly its lexicon and its label itself.
No features willbe copied from its neighbors either.
Table 1 givesan example to demonstrate the features used in ouralgorithm.
For the sentence ?Monday vice chairmanGoff said the company ...?, only ?Goff?
includes itsown features and features copied from its neighbors,while most of the other tokens have only two fea-tures since they are ?O?
tokens based on the high-precision rules.Usually, more than half the tokens will be classi-fied as ?O?.
This strategy greatly saves feature ex-traction time, training time, and inference time, aswell as improving the accuracy of the model.
Mostimportantly, this strategy is necessary in the semi-supervised learning, which will be explained in thenext section.4 Semi-supervised Learning AlgorithmOur semi-supervised algorithm is outlined in Ta-ble 2.
We assume that we have a small amount oflabeled data L and a classifier Ck that is trained onL.
We exploit a large unlabeled corpus U from thetest domain from which we automatically and grad-ually add new training data D to L, such that Lhas two properties: 1) accurately labeled, meaningthat the labels assigned by automatic annotation ofthe selected unlabeled data are correct, and 2) non-redundant, which means that the new data is fromregions in the feature space that the original trainingset does not adequately cover.
Thus the classifier Ckis expected to get better monotonically as the train-ing data gets updated.Table 2: The semi-supervised NER algorithmGiven:L - a small set of labeled training dataU - unlabeled dataLoop for k iterations:Step 1: Train a classifier Ck based on L;Step 2: Extract new data D based on Ck;Step 3: Add D to L;At each iteration, the classifier trained on the pre-vious training data (using the features introduced inthe previous section) is used to tag the unlabeleddata.
In addition, for each O token and NE seg-ment, a confidence score is computed using the con-61strained forward-backward algorithm (Culotta andMcCallum, 2004), which calculates the LcX , the sumof the probabilities of all the paths passing throughthe constrained segment (constrained to be the as-signed labels).One way to increase the size of the training datais to add all the tokens classified with high confi-dence to the training set.
This scheme is unlikelyto improve the accuracy of the classifier at the nextiteration because the newly added data is unlikelyto include new patterns.
Instead, we use the highconfidence data to tag other data by exploiting inde-pendent features.?
Tagging ORGIf a sequence of tokens has been classified as?ORG?
with high confidence score (> T )1,we force the labels of other occurrences of thesame sequence in the same document, to be?ORG?
and add all such duplicate sequencesclassified with low confidence to the trainingdata for the next iteration.
In addition if a highconfidence segment ends with company suf-fix, we remove the company suffix and checkthe multi-mentions of the remaining segmentalso.
In addition to that, we reclassify the sen-tence after removing the company suffix andcheck if the labels are still the same with high-confidence.
If not, the sequence will be addedto the training data.
As shown in Example 4,?Safeway shares ticked?
is added to trainingdata because ?Safeway?
has low confidence af-ter removing ?Inc.
?.Example 4:High-confidence ORG: Safeway Inc. sharesticked up ...Low-confidence ORG:1) Safeway shares ticked up ...2) Wall Street expects Safeway to post earnings...?
Tagging PERIf a PER segment has a high confidence scoreand includes at least two tokens, both this seg-1Through the rest of the paper, a high confidence scoremeans the score is larger than T. In our experiments, T is setas 0.98.
A low confidence score means the score is lower than0.8.ment and the last token of this segment areused to find their other mentions.
Similarly,we force their labels to be PER and add themto the training data if their confidence score islow.
However, if these mentions are followedby any company suffix and are not classifiedas ORG, their labels, as well as the companysuffix are forced to be ORG (e.g., Jefferies &Co.).
We require the high-confidence PER seg-ment to include at least two tokens because theclassifier may confuse single-token ORG withPER due to their common context.
For ex-ample, ?Tesoro proposed 1.63 billion purchaseof...?, Tesoro has high-confidence based on themodel, but it represents Tesoro Corp in the doc-ument and thus is an ORG.In addition, the title feature can be used simi-larly as the company suffix features.
If a PERwith a title feature has a high confidence score,but has a low score after the title feature isremoved, the PER and its neighbors will beput into training data after removing the title-related tokens.Example 5:High-confidence PER:1)Investor AB appoints Johan Bygge as CFO...2)He is replacing Chief CEO Avallone...Low-confidence PER:1) Bygge is employed at...2) He is replacing Avallone ...(It is obvious for a human-being that Bygge isPER because of the existence of ?employed?.However, when the training data doesn?t in-clude such cases, the classifier just cannot rec-ognize it.)?
Tagging LOCThe same approach is used for a LOC segmentwith a high confidence score.
We force the la-bels of its other mentions to be LOC and addthem to the training data if their confidencescore is low.
Again, if any of these mentionsfollows or is followed by an ORG segment witha high confidence score, we force the labels tobe ORG as well.
This is because when a LOCis around an ORG, the LOC is usually treatedas part of an ORG, e.g., Google China.62Example 6:High-confidence LOC: The former Soviet re-public of Azerbaijan is...Low-confidence PER:Azerbaijan energy reserves better than...Change LOC to ORG: shareholders of theChicago Board of Trade...?
Tagging OSince all the NE segments added to the train-ing data have low confidence scores based onthe original model, and especially since manyof them were incorrectly classified before cor-rection, these segments form good training datacandidates.
However, all of them are positiveexamples.
To balance the training data, weneed negative examples as well.
If a token isclassified as ?O?
with high confidence scoreand does not have a label feature ?O?, this to-ken will be used as a negative example to beadded to the training data.Since the features of each token include the fea-tures copied from its neighbors, in addition to thoseextracted from the token itself, its neighbors need tobe added to the training set alo.
If the confidence ofthe neighbors are low, the neighbors will be removedfrom the training data after copying their features tothe token of interest.
If the confidence scores of theneighbors are high, we further extend to the neigh-bors of the neighbors until low-confidence tokensare reached.
We remove low-confidence neighborsin order to reduce the chances of adding training ex-amples with false labels.Table 3: Step 2 of the semi-supervised algorithmStep 2: Extract new data D based on Cki) Classify kth portion of U and compute confidencescores;ii) Find high-confidence NE segments and use themto tag other low-confidence tokensiii) Find qualified O tokensiv) Extract selected NE and O tokens as well astheir neighborsv) Shuffle part of the NEs in the extracted datavi) Add extracted data to DNow we have both negative and positive trainingexamples.
However, one problem with the positivedata is that the same NE may appear too many timessince the multi-mention property is used.
For ex-ample, the word ?Citigroup?
may appear hundredsof times in recent financial articles because of thesubprime crisis.
To account for this bias in the datawe randomly replace these NEs.
Specifically, wereplace a portion of such NEs with NEs randomlychosen from our NE lists.
The size of the portion isdecided by the ratio of the NEs that are not in ourNE list over all the NEs in the gold data.Table 3 summarizes the key sub-steps in Step 2of the algorithm.
At each step, more non-redundantand high-accuracy data is added into the training setand thus improves the model gradually.5 ExperimentsThe data set used in the experiments is explainedin Table 4.
Although we have 1000 labeled newsdocuments from the Thomson Financial (TF) Newssource, only 60 documents are used as the initialtraining data in our algorithm.
For the evaluation,the gold data was split into training and test sets asappropriate.
The toolbox we used for CRF is Mallet(McCallum, 2002).Table 4: Data source.
Tokens include words, punctuationand sentence breaks.Gold Data 1000 docs from TF news(around 330 tokens per doc)Unlabeled Corpus 100,000 docs from TF news0.5 0.6 0.7 0.8 0.9939495969798Confidence Score ThresholdAccuracyFigure 1: Token accuracy vs confidence score.We first investigated our assumption that a highconfidence score indicates high classification accu-racy.
Figure 1 illustrates how accuracy varies asCRF confidence score changes when 60 documents63are used as training data and the remaining are usedas testing data.
When the threshold is 0.98, the tokenaccuracy is close to 99%.
We believe this accuracy issufficiently high to justify using the high confidencescore to extract tokens with correct labels.Table 5: Precision and recall of the automatically ex-tracted training dataNE Precision% Recall% F-score%LOC 94.5 96.8 95.6ORG 96.6 93.4 94.9PER 95.0 89.6 92.2We wished to study the accuracy of our trainingdata generation strategy from how well it does on thegold data.
We treat the remaining gold data (exceptthe data trained for the initial model) as if they wereunlabeled, and then applied our data extraction strat-egy on them.
Table 5 illustrates the precision and re-call for the three types of NEs of the extracted data,which only accounts for a small part of the gold data.The average F-score is close to 95%.
Although theprecision and recall are not perfect, we believe theyare good enough for the training purpose, consider-ing that human tagged data is seldom more accurate.We compared the semi-supervised algorithm witha supervised algorithm using the same features.
Thesemi-supervised algorithm starts with 60 labeleddocuments (around 20,000 tokens) and ends witharound 1.5 million tokens.
We trained the supervisedalgorithm with two data sets: using only 60 docu-ments (around 20,000 tokens) and using 700 doc-uments (around 220,000 tokens) respectively.
Thereason for the choice of the training set size isthe fact that 20,000 tokens are a reasonably smallamount of data for human to tag, and 220,000 tokensare the amount usually used for supervised algo-rithms (CoNLL 2003 English NER (Sang and Meul-der, 2003) training set has around 220,000 tokens).Table 6 illustrates the results when 300 docu-ments are used for testing.
As shown in Table 6,starting with only 6% of the gold data, the semi-supervised algorithm achieves much better resultsthan the semi-supervised algorithm when the sameamount of gold data is used.
For LOC, ORG, andPER, the recall increases 5.5, 16.8, and 8.2 respec-tively, and the precision increases 2.4, 1.5, and 6.8respectively.
Even compared with the model trainedwith 220,000 tokens, the semi-supervised learningalgorithm is better.
Especially, for PER, the pre-cision and recall increase 2.8 and 4.6 respectively.Figure 2 illustrates how the classifier is improved ateach iteration in the semi-supervised learning algo-rithm.Table 6: Classification results.
P/R represents Preci-sion/Recall.
The numbers inside the parentheses are theresult differences when the model trained from 60 docs isused as baseline.Training Data P/R(LOC) P/R(ORG) P/R(PER)60 docs 88.1/85.6 86.0/64.2 74.5/81.2700 docs 91.2/88.2 90.5/76.6 78.3/84.8(3.1/3.6) (4.5/12.4) (3.8/3.6)semi-supervised 90.5/91.1 87.5/81.0 81.1/89.4(60 docs) (2.4/5.5) (1.5/16.8) (6.6/8.2)1 2 3 4 5 6 7 8 982838485IterationOverallF?ScoreFigure 2: Overall F-score vs iteration numbersTable 7 compares the results when the multi-mention property is also used in testing as a high-precision rule.
Comparing Table 7 to Table 6, wecan see that with the same training data, using multi-mention property helps improve classification re-sults.
However, this improvement is less than thatobtained by using this property to extract trainingdata thus improve the model itself.
(For a fair com-parison, the model used in the semi-supervised algo-rithm in Table 6 only uses multi-mention property toextract data.
)Our last experiment is to test how this method canbe used when the initial gold data and the testingdata are from different domains.
We use the CoNLL2003 English NER (Sang and Meulder, 2003) train-ing set as the initial training data, and automaticallyextract training data from the TF financial news cor-pus.
The CoNLL data is a collection of news wire64documents from the Reuters Corpus, while TF dataincludes financial-related news only.
Table 8 illus-trates the results.
As shown in the table, with onlyCoNLL data, although it contains around 220,000tokens, the results are not better than the resultswhen only 60 TF docs (Table 6) are used for train-ing.
This indicates that data from different domainscan adversely affect NER accuracy for supervisedlearning.
However, the semi-supervised algorithmachieves reasonably high accuracy.
For LOC, ORG,and PER, the recall increases 16, 20.3, and 4.7 re-spectively, and the precision increases 4.5, 5.5, and4.7 respectively.
Therefore our semi-supervised ap-proach is effective for situation where the test andtraining data are from different sources.Table 7: Classification results when multi-mention prop-erty (M) is used in testingTrainig Data P/R(LOC) P/R(ORG) P/R(PER)60 docs +M 89.9/87.6 82.4/71.4 78.2/87.3700 docs+M 91.2/89.1 90.2/78.3 79.4/91.1(1.3/1.5) (7.8/6.9) (1.2/3.8)semi-supervised 90.0/91.0 86.6/82.4 81.3/90.6+M (60 docs) (1.1/3.4) (4.2/11.0) (3.1/3.3)Table 8: Classification results trained on CoNLL data andtest on TF data.
Training data for the semi-supervisedalgorithm are automatically extracted using both multi-mention and high-precision context from TF corpus.Training Data P/R(LOC) P/R(ORG) P/R(PER)CoNLL 85.6/74.7 75.2/65.9 72.4/85.2Semi-supervised 90.1/90.7 81.7/86.2 77.1/90.5(CoNLL) (4.5/16) (5.5/20.3) (4.7/4.7)6 ConclusionWe presented a simple semi-supervised learning al-gorithm for NER using conditional random fields(CRFs).
In addition we proposed using high preci-sion label features to improve classification accuracyas well as to reduce training and test time.Compared to other semi-supervised learning al-gorithm, our proposed algorithm has several advan-tages.
It is domain and data independent.
Althoughit requires a small amount of labeled training data,the data is not required to be from the same domainas the one in which are interested to tag NEs.
It canbe applied to different types of NEs as long as in-dependent evidence exists, which is usually avail-able.
It is simple and, we believe not limited by thechoice of the classifier.
Although we used CRFs inour framework, other models can be easily incorpo-rated to our framework as long as they provide accu-rate confidence scores.
With only a small amount oftraining data, our algorithm can achieve a better NEtagging accuracy than a supervised algorithm with alarge amount of training data.ReferencesA.
Blum and T. Mitchell.
1998.
Combining labeled andunlabeled data with co-training.
Proceedings of theWorkshop on Computational Learning Theory, pages92?100.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
Proceedingsof the Joint SIGDAT Conference on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora.A.
Culotta and A. McCallum.
2004.
Confidence estima-tion for information extraction.
HLT-NAACL.G.
D. Forney.
1973.
The viterbi algorithm.
Proceedingsof the IEEE, 61(3):268?278.Feng Jiao, Shaojun Wang, Chi H. Lee, Russell Greiner,and Dale Schuurmans.
2006.
Semi-supervised condi-tional random fields for improved sequence segmen-tation and labeling.
In Proceedings of the 21st In-ternational Conference on Computational Linguistics,pages 209?216, July.Andrew McCallum and Wei Li.
2003.
Early resultsfor named entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.CoNLL.A.K.
McCallum.
2002.
Mallet: A machine learning forlanguage toolkit.
http://mallet.cs.umass.edu.Andrew McCallum.
2003.
Efficiently inducing featuresof conditional random fields.David Pierce and Claire Cardie.
2001.
Limitations ofco-training for natural language learning from largedatasets.
In EMNLP.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.Introduction to the conll-2003 shared task: Language-independent named entity recognition.
CoNLL, pages142?147.Yingchuan Wong and Hwee Tou Ng.
2007.
One classper named entity: Exploiting unlabeled text for namedentity recognition.
IJCAI, pages 1763?1768.David Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Meeting ofthe Association for Computational Linguistics, pages189?196.65
