Proceedings of the 1st Workshop on Speech and Multimodal Interaction in Assistive Environments, pages 8?12,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsIntegration of Multimodal Interaction as Assistance in VirtualEnvironments+Kiran Pala Ramnaresh Pothukuchi+Sachin Joshi +Suryakanth V Ganagashetty+International Institute of Information Technology Hyderabad, Hyderabad India{kiranap, rushtosachin, prnaresh.prnaresh} @ gmail.com, svg@iiit.ac.inAbstractThis paper discusses the significance of themultimodal interaction in virtualenvironments (VE) and the criticalitiesinvolved in integration and coordinationbetween modes during interaction.
Also,we present an architecture and design ofthe integration mechanism with respect toinformation access in second languagelearning.
In this connection, we haveconducted an experiential study on speechinputs to understand how far users?experience of information can beconsidered to be supportive to thisarchitecture.1 IntroductionIn the era of globalization education has taken adifferent path from the traditional space ofteaching and learning.
A nation?s commerce and itsmarket with respect to global changes, theimplications of global needs are all demanding topolicy makers for them to change educationalpolicies accordingly.In the above scenario, technology also has asignificant role to play.
Rapid development and useof new technologies have helped the humanlearning trajectory to take a complete shift from theclassrooms to communities, personalization etc.There the e-learning and learning throughtechnologies can be television and internettechnologies, gadgets, tablets etc.
E-learning has,with certainty, become a major entity in personaland community based learning.
In addition, thesedays most of the classrooms have adapted itself tothe concept of personalization with the help oftechnology assistive mechanisms in education, thatis, the education sector shapes their face as e-education.
Learning is a differently nuancedconcept from teaching and instruction.
Also,learning is a continuous interactive process; itcannot be a discretely developing process as wesee that the definition of learning has shifted to akind of entertainment activity.
As shown in Pala(2012a) the interaction can be active or passive.We know that environments play a moresignificant role in facilitating the interaction withthe learner as an interface between learners andcommunities.
A learner receives information fromenvironment through their senses such as visual,tactile and auditory with different activities whichcan directly affect their memory both declarativeand procedural (Ullman, 2001).
The activitiesblend with an interaction continuous with theenvironment.
The tremendous development ofinformation and communication technologies(ICTs) and its applications have made it possible toreplicate the real environments on virtualplatforms.
The virtual environments facilitate theinteraction for communication and informationprocessing more or less like real environments.Generally, whatever information is receivedthrough senses from the environment will beredirected to memory in the form of experienceand then it is modulated with respect to the form ofboth production and perception states of a learner(Miller, and Johnson, 1976).
But, whether thevirtual environments provide an experience to thelearners similar in these respects to the realenvironments is an answerable question to thecommunity.
Such experience is only possible whenthe multimodal interaction and assistance takeplace at the learner level from the environment.This communication, interaction and assistance canbe peer-to-peer or person-to-person or peer-to-person etc.
In any interaction or communication,assistance will be harnessed to rethink and rehearse8the information which has been received.
Since therehearsal process is directly related to memory, ithelps learner to be fluent and expert in the relateddomain.2 Assistance in Accessing of InformationThe assistive technologies played an important rolein the olden days and even today with emerginginformation technology it does play a significantrole.
The assistive technologies are used not justfor those who have physical or cognitivedifficulties, but even in areas of information accessand representation.
Some of the assistivetechnological devices include speech recognition,screen reader, touch screen, on-screen keyboard,word prediction, transliteration etc.
In the virtualenvironment, the resources considered are imagedatabase, text database, and video or action data(Bartle, 2004).
VE will support the learner in manyaspects and would boost learners?
abilities.
VEwould be helpful in many ways such as providingimmediate feedback, experimentation, grabbingfocus, furthering exploration, and would also suitthe learner requirements.Accessing information and assistance with aneye on the representation of the accessedinformation is highly interrelated in?understanding the meaning?.
For exampleconsider a sound-meaning relationship, if a na?velearner wants to learn the sounds of a newlanguage and listens to a sound like /a/.
Users maynot be able to immediately utter the same sound.For that we will use ?/a/ for /apple/?.
Sometimeswe need to show the picture of /apple/ also to makethe learner better ?understand the meaning?
i.e.pragmatic information of the condition orstatement like shown in figure-1.
This instanceeasily and naturally occurs in real environments.But it is possible in VE by integrating multimodalinteraction (tactile, visual, auditory) as assistancefor the purpose of representing the accessedcontent from the crawled database extracted fromthe web according to the level of the learner andrequirements like games or only content ormeaning etc.However, in the personalization of learning andfacilitation according to content representations,the expected naturalness is still far away from whatoccurs in real situations.
In this paper, we proposea na?ve architecture with the reference to Indianlanguages and the target group is second languagelearners (L2).Figure 1: An example, environment required forunderstanding of the meaning with assistance.3 ArchitectureHere we discuss the details of the proposedarchitecture with the reference to each module?sfunctions.
This architecture mainly focuses on theintegration of multimodal interaction as assistanceto individuals who are adult learners.
We haveconsidered in the designing of this architecturelearners?
behavioral profiles, cognitive abilities andtechnological traits to pave the way for a morepersonalized interaction with the environment.
Pala(2012a) has shown that these learners can be fromany age group after the stage of puberty includingeven those who do not have much experience inuse of virtual environments.Input Devices: All these input devices likeAutomatic Speech Recognition (ASR) touchscreen, mouse, keyboard etc.
are interconnected toeach other to ensure avoidance of information lossduring non-linear interaction as well.
Generally,adult individual learners move towardsmultitasking and non-linear interaction at a timeand it has been expected that it should be acontinuous activity.
For example, the learner cangive a speech input which is recognized by theASR, at the same time the learner can utilize touchscreen, keyboard and mouse to give another input.The input of the learner can be an alphabet or aword.
Here we are dealing with sound-meaningrelationships and conceptual structures and theirtypes in languages at the lexical level.
The multipleinput facilities will assist the learner to provideversatile inputs of their own choice.
It also has a9significant role in furthering or initializing learningin learners who have physical disorders.
Thiscombined interaction of the visual and tactilesenses is directly connected to the proceduralmemory (Christiansen and Chater, 2008;Tomasello, 2008).Figure 2: A Block diagram of Virtual Environmentwith Multimodal interaction as assistive.Lexical Indexer: It is a kind of database withthe linguistic categories and relations of eachlexeme as has been discussed in Pala (2012b).
Itconsists of a morphological analyzer and astemmer.
At the functional level it extracts the rootword from the given input and verifies it in theindexer for its category and relationships in orderto search for the same category-oriented examplesand images from the web through crawlers.Additionally, the same keywords will be indexedagain for ranking purposes of a specific learner.
Ifa keyword is not available with the indexer, itsends the keyword directly to the web with a newindex and later learns the relations and categorieswith the help of parts-of-speech taggers (POS) andshallow parses (Parser/Hindi, 2012; Akshar,Chaitanya, and Sangal, 1995).An Engine: This engine consists of webcrawlers for content resources, annotators,synthesizers (Text-to-Speech) and a predictivelearning algorithm which has been built on self-organizing maps.
Speech synthesizers receiveinformation from the text annotator.
The examplesare provided in the form of phones, lexical itemsand sentences, it converts them into a signal formto speak it aloud when the learner requests.Here annotators have a significant responsibilityin handling information.
In the process of buildingimage annotators, we have used regularexpressions for replacing the names.
In addition,we have used wavelet transforms to verify thequality i.e.
pixel depth, colors hue etc.
of theimage.
Some other parameters like size and weightof the image have also been taken into account.Similarly, according to Pala (2011a) the textannotators have been constructed with an eye onparameters like removal of punctuations andspecial symbols etc.
through an inclusion of theheuristic mechanism for anaphora references.
Theprojection of video for action-related lexical itemshas been dealt with in the post-processing section.Post-Processor: In this module we will have averification process at initial stages, i.e.,  in thedevelopmental state of the application a manualcheck up will be carried out along with autoverification process by the content developers whowill look into the pragmatic and semantic aspectsof example sentences, action videos and imagesvery carefully.
In the case of videos, the post-processing stage is more important in that when theinput keyword contains a verb, making the actionthrough image or text understandable is highlydifficult.
Thus, we have chosen the video form forlexical items related to action and motion.
Thiscategorical information will be received from thelexical indexer.
The video codecs, definition of thevideo or animations quality, the length of the videoand the mixture of audio clarity are very importantparameters in selection and building of such actionoriented contents.In this paper we are dealing with the contentrepresentation modes but there is a similarsignificant role that mediates having a ?kind ofcontent and presentation model for presenting to10understand the meaning?
in learning process.
Thiswill streamline the process of the constant reviewprocess by the domain experts as shown in Pala(2012b).Displayer: It is a space to interact with the useror the leaner, i.e., it is an interface between thelearner and the application.
It is embedded with allinteraction modes (input and output tools) whichwe have discussed above for the assistancepurpose.
It projects the output in all types ofmodes which affect different senses (visual andauditory) of the user on screen according to userinput requests.
The displayer is crucial as thelearners get distracted and lose interest in learningif the size of the screen, projection and the level ofpixel value are to be defined according to userrequirements.
This requires a meticulous design sothat the users?
attention and their rehearsal activitygravitate towards the learning content.Figure 3: Example for Bilingual environment(English to Telugu)Since this application is multilingual, the learnercan make a request in any language.
At thismoment we have built an application for two majorIndian languages and English.
If, for example, auser asks for a meaning and use of the lexical itemin English and their target language is Telugu, the?meaning?
and ?use?
of the lexical items will beshown in what we see in figure-3 below.
Nativespeakers generally look for the synonym for a?regular use?
of a lexical item.
We consider thisfactor to be of much importance and build adatabase which consists of the synonyms with their?regular use?
as shown in the figure-4 below.Figure 4: An example process of monolingualenvironment (Telugu to Telugu (Robert and Wyatt,1956))4 User Experience Study on MultimodalInteractionTo demonstrate the impact of multiple input modeson the quality of users?
experience we haveperformed an experiential study to elicit users?perceptual inference- through speech andkeyboard.
We have built an English ASR usingCMU Sphinx.
For this we have used 1000 isolatedwords for the testing of the ASR which is used fortraining.
The study was executed by providing theisolated words recorded by speakers.
In this studywe have passes since we would like to test userexperience after the integration of the multimodalinput mechanisms (here we have integrated akeyboard with ASR) to an individual computer.
Inthe first pass the spoken word was decoded usingthe entire vocabulary of 1000 words given to therecognizer.
Then the user was asked to type thefirst character of the spoken word.
The wordsstarting with that character were segregated.
Insecond pass, the spoken word was decoded withonly segregated words given as input vocabulary toASR.
As expected, the second pass decodingshowed a major accuracy improvement because ofreduction in search vocabulary size.
The relativeimprovement in accuracy was 36.61% percent.
Theentire procedure has been designed in such amanner that each lexical item will be selected froma bag of lexical items.
As the entire procedure isexecuted, significant parameters for evaluation ofthe responses from the participants are drawn upfor further analysis.
All users reported that they11were much more satisfied with multimodal itemsthan with using speech recognition alone, since thesystem performs better with a minimal additionaleffort of pressing a single key.
Not only accuracybut speed of the system was better.5 ImplicationsResults accrued from such a study are believedto have ramifications for the interface betweendecision making behavior at the level of theindividual and the organization in a morespecific sense.
Thus this observation shows thatmulti-modal interfaces can lead to better userexperience.
Human experience is labile andmalleable in that it can be harnessed in differentmodes and through different media with the addedadvantage that the same content can be harnessed,molded and manipulated for differentially orientedpurposes and tasks at hand.
This character ofexperience is fine-tuned for multimodal learning oflinguistic structures the underlying cognitivestructures of which can be observed to shape andbe reshaped by such experiences in VEs as thestudy has revealed.
This is extremely valuable forany study that aims at figuring out how cognitivestructures during learning can be seen to behave invivo.6 Future WorkThere are several limitations and problems with thecurrent study.
Language learning especially lexicallearning is a very complicated and multi-dimensional process requiring representationalityat several levels of architectural specification.
Thishas been attenuated by orders of magnitude for thesake of modeling and initialization of the processeswithin the architecture of the current VE.
Thisneeds a further elaboration within the currentarchitecture that will lead to multi-layered sub-architectures for lexical learning cutting acrosssyntactic, morphological, semantics/pragmatic andother cognitive levels of representation.ReferencesAkshar, B, Chaitanya, V and Sangal, R., 1995, NaturalLanguage Processing: A Paninian Perspective,Prentice-Hall of India, New Delhi, 65-106.Bartle, R.A., 2004, Designing virtual worlds, NewRiders Pub.Christiansen, M.H.
and Chater, N., 2008, Language asshaped by the brain.
Behav.
Brain Sci.
31, 489?509Miller, G, Johnson, L. P., 1976., Language andPerception.
Cambridge: Cambridge University Press.Pala.
K., and Gangashetty S.V., 2012a (In Press),Virtual Environments can Mediate ContinuousLearning, Technology Inclusive Learning.
IGI, USA.Pala K., Gangashetty S.V., 2012b (In press), Challengesand Opportunities in Automatically BuildingBilingual Lexicon from Web Corpus, inInterdisciplinary Journal on Linguistics, UniversityPress.Pala, K. and Begum, R., 2011a An Experiment onResolving Pronominal Anaphora in Hindi: UsingHeuristics, Journal on Information Systems forIndian Languages, 267-270, Springer.Pala, K. and Singh, A.K.
and Gangashetty, S.V., 2011b,Games for Academic Vocabulary Learning through aVirtual Environment, Asian Language Processing(IALP), 2011 International Conference on, 295-298,IEEEParser/Hindi, 2012, Hindi Shallow Parser source,Retrieved 1 March 2012 from, Hindi Shallow Parser-source,  http://ltrc.iiit.ac.in/analyzer/Robert, C. and Wyatt, JL, 1956, A ComparativeGrammar of the Dravidian or South Indian Family ofLanguages, Robert, Revised and edited by Rev, JLWyatt and T. Ramakrishna Pillai, Reprint ed.,(Madras:.
University of Madras, 1961)Tomasello, M., 2008.
The Origins of HumanCommunication, MIT PressUllman, M.T., 2001.
The Declarative/Procedural Modelof Lexicon and Grammar, Journal of PsycholinguisticResearch, 30(1).12
