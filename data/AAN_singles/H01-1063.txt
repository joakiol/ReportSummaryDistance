Scalability and Portability of a Belief Network-basedDialog Model for Different Application DomainsCarmen WaiThe Chinese University of Hong KongShatin, N.T., Hong KongSAR, ChinaTel:  +852 2609 8327cmwai@se.cuhk.edu.hkHelen M. MengThe Chinese University of Hong KongShatin, N.T., Hong KongSAR, ChinaTel:  +852 2609 8327hmmeng@se.cuhk.edu.hkRoberto PieracciniSpeechWorks International Ltd17 State StreetNew York, NY 1004Tel: +1.212.425.7200roberto.pieraccini@speechworks.comABSTRACTThis paper describes the scalability and portability of a BeliefNetwork (BN)-based mixed initiative dialog model acrossapplication domains.
The Belief Networks (BNs) are used toautomatically govern the transitions between a system-initiativeand a user-initiative dialog model, in order to produce mixed-initiative interactions.
We have migrated our dialog model froma simpler domain of foreign exchange to a more complexdomain of air travel information service.
The adapted processesinclude: (i) automatic selection of specified concepts in theuser?s query, for the purpose of informational goal inference; (ii)automatic detection of missing / spurious concepts based onbackward inference using the BN.
We have also enhanced ourdialog model with the capability of discourse contextinheritance.
To ease portability across domains, which oftenimplies the lack of training data for the new domain, we havedeveloped a set of principles for hand-assigning BNprobabilities, based on the ?degree of belief?
in therelationships between concepts and goals.
Application of ourmodel to the ATIS data gave promising results.1.
INTRODUCTIONSpoken dialog systems demonstrate a high degree of usability inmany restricted domains, and dialog modeling in such systemsplays an important role in assisting users to achieve their goals.The system-initiative dialog model assumes complete control inguiding the user through an interaction towards task completion.This model often attains high task completion rates, but the useris bound by many constraints throughout the interaction.Conversely, the user-initiative model offers maximum flexibilityto the user in determining the preferred course of interaction.However this model often has lower task completion ratesrelative to the system-initiative model, especially when theuser?s request falls beyond the system's competence level.
Tostrike a balance between these two models, the mixed-initiativedialog model allows both the user and the system to influencethe course of interaction.
It is possible to handcraft asophisticated mixed-initiative dialog flow, but the task isexpensive, and may become intractable for complex applicationdomains.We strive to reduce handcrafting in the design of mixed-initiative dialogs.
We propose to use Belief Networks (BN) toautomatically govern the transitions between a system-initiativeand a user-initiative dialog model, in order to produce mixed-initiative interactions.
Previous work includes the use ofsemantic interpretation rules for natural language understanding,where the rules are learnt by decision trees known as SemanticClassification Trees (SCTs) [6].
Moreover, there is alsoprevious effort that explores the use of machine learningtechniques to automatically determine the optimal dialogstrategy.
A dialog system can be described as a sequentialdecision process that has states and actions.
An optimal strategycan be obtained by reinforcement learning [7, 8].
While thesystem is interacting with users, it can explore the state spaceand thus learn different actions.Our BN framework was previously used for naturallanguage understanding [1,2].
We have extended this model fordialog modeling, and demonstrated feasibility in the CUFOREX (foreign exchange) [3,4] system, whose domain haslow complexity.
This work explores the scalability andportability of our BN-based dialog model to a more complexapplication.
We have chosen the ATIS (Air Travel InformationService) domain due to data availability.12.
BELIEF NETWORKS FOR MIXED-INITIATIVE DIALOG MODELING ?THE CU FOREX DOAMINWe have devised an approach that utilizes BNs for mixed-initiative dialog modeling, and demonstrated its feasibility inthe CU FOREX domain.
Details can be found in [4].
Weprovide a brief description here for the sake of continuity.CU FOREX is a bilingual (English and Cantonese)conversational hotline that supports inquiries regarding foreignexchange.
The domain is relatively simple, and can becharacterized by two query types (or informational goals ?Exchange Rate or Interest Rate); and five domain-specificconcepts (a CURRENCY pair, TIME DURATION, EXCHANGE RATEand INTEREST RATE).
Our approach involves two processes:2.1 Informational Goal InferenceA BN is trained for each informational goal.
Each BN receivesas input the concepts that are related to its corresponding goal.In CU FOREX, there are two BNs, each with five inputconcepts.
The pre-defined BN topology shown in Figure 1(without dotted arrow) incorporates the simplifying assumptionthat all concepts are dependent only on the goal, but areindependent of one another.
This topology can be enhanced by1 The ATIS data can be licensed from the Linguistic DataConsortium (www.ldc.upenn.edu).learning the inter-concept dependencies from training dataaccording to the Minimum Description Length (MDL) principle[2].
The resultant topology is illustrated in Figure 1.Figure 1.
The predefined topology of our BNs is enhanced bythe linkage (dotted arrow) learnt to capture dependenciesamong concepts.
The arrows of the acyclic graph are drawnfrom cause to effect.Given an input query, each trained BN will make a binarydecision (using pre-set threshold of 0.5)2 regarding the presenceor absence of its corresponding informational goal, based on thepresence or absence of its input concepts in the query.
Thedecisions across all BNs are combined to identify theinformational goal of the input query.
We labeled the query toa goal if the corresponding BN votes positive with themaximum aposteriori probability.
Alternatively, we may labelthe query with all goals for which the BNs vote positive.Should all BNs vote negative, the query is rejected as out-of-domain (OOD).2.2 Detection of Missing / SpuriousConceptsAutomatic detection of missing or spurious concepts isachieved by backward inference in the BN.
Given an identifiedgoal from the previous process, the goal node of thecorresponding BN is instantiated (i.e.
P(Gi) set to 1), andbackward inference updates the probability of each conceptP(Ci).
Comparison between P(Ci)  and a pre-set threshold ?
(=0.5) determines whether the concept should be present orabsent; and further comparison with the actual occurrence(s)determines whether the concept is missing or spurious.
In thisway, domain-specific constraints for database access is capturedand enforced in the BN, i.e.
an Exchange Rate inquiryrequires a currency pair, and an Interest Rate inquiry requiresspecifications of the currency and the duration.
A missingconcept will cause the dialog model to automatically trigger asystem prompt.
A spurious concept will cause automaticallytrigger a request for clarification.Table 1 provides an illustrative example from the CUFOREX domain.
The first process infers that the query ?Can Ihave the interest rate of the yen??
has the informational goal ofInterest Rate.
The second process of backward inferenceindicates that the concept <DURATION> should be present, but isabsent from the query.
Hence <DURATION> is a missing conceptand the dialog model prompts for the information.2  We choose threshold at 0.5 since P(G=1|C)+P(G=0|C)=1Table 1.
This table steps through our dialog modeling process.The input query is ?Can I have the interest rate of the yen?.Process 1 (informational goal inference) identifies that this is aninterest rate inquiry.
Process 2 performs backward inference tocompute the concept probabilities.
Thresholding with ?=0.5indicates whether the concept should be present or absent.Comparison between this binary decision and the actualoccurrence detects that the concept <DURATION> is missing.Hence the dialog model prompts for the missing information.Query:  Can I have the interest rate of the yen?Process 1:  Informational Goal InferenceBN for Interest RateP(Goal = Interest Rate | Query) = 0.801 ?
goal presentBN for Exchange RateP(Goal = Exchange Rate | Query) = 0.156 ?
goal absentHence, inferred goal is Interest Rate.Process 2:  Detection of Missing / Spurious ConceptsConcept Cj P(Cj) BinaryDecisionfor CjActualOccurrenceof CjCURRENCY1 0.91 present presentCURRENCY2 0.058 absent absentDURATION 0.77 present absentEXCHANGE_RATE 0.011 absent absentINTEREST RATE 0.867 present presentResponse:  How long would you like to deposit?3.
MIGRATION TO THE ATIS DOMAINOur experiments are based on the training and test sets of theAir Travel Information Service (ATIS) domain.
ATIS is acommon task in the ARPA (Advanced Research ProjectsAgency) Speech and Language Program in the US.
We used theClass A (context-independent) as well as Class D (context-dependent) queries of the ATIS-3 corpus.
The disjoint trainingand test sets consist of 2820, 773 (1993 test), 732 (1994 test)transcribed utterances respectively.
Each utterance isaccompanied with its corresponding SQL query for retrievingthe relevant information.We derive the informational goal for each utterance fromthe main attribute label of its SQL query.
Inspection of theClass A training data reveals that out of the 32 query types (orinformational goals, e.g.
flight identification, fare identification,etc.
), only 11 have ten or more occurrences.
These 11 goalscover over 95% of the training set, and 94.7% of the testing set(1993 test).
Consequently, we have developed 11 BNs tocapture the domain-specific constraints for each informationalgoal.
Also, with the reference to the attribute labels identified askey semantic concepts from the SQL query, we have designedour semantic tags for labeling the input utterance.
We have atotal of 60 hand-designed semantic tags, where both syntactic(e.g.
<PREPOSITION> <SUPERLATIVE>) and semantic concepts(e.g.
<DAY_NAME>, <FLIGHT_NUMBER>) are present.
Hence,ATIS presents increased domain complexity, which ischaracterized by 11 query types and total 60 domain-specificconcepts.?
represents goal node?
represents concept nodeInterest RatesCURR1CURR2 DURATION EX_RATEIN_RATE4.
SCALABILITY OF A BN-BASEDDIALOG MODEL4.1 Informational Goal InferenceThere is a total of 60 hand-designed3 semantic concepts in theATIS domain.
In order to constrain computation time for goalinference, we have limited the number of semantic concepts (N)that are indicative of each goal Gj.
The parameter N (=20) hasbeen selected using the Information Gain criterion to optimizeon overall goal identification accuracy on the Class A trainingutterances [1].We have also refined the pre-defined topology usingMinimum Description Length (MDL) principle to modelconcept dependencies.
Example of the BN is shown in Figure 2.Their inclusion brought performance improvements in goalidentification [2].Figure 2.
Topology of the BNs for the informational goalFlight_ID.Consequently, each BN has a classification-based networktopology ?
there are N (=20) input concept nodes (e.g.
airline,flight_number, etc.)
and a single output node.
To avoid the useof sparsely trained BNs, we have developed 11 BNs to capturethe domain-specific constraints for each informational goalusing Class A training data.
The remaining goals are thentreated as out-of-domain.A trained BN is then used to infer the presence / absenceof its corresponding informational goal, based on the inputconcepts.
According to the topology shown in Figure 1, thelearnt network is divided into sub-networks: {Flight_ID,CITY_1, CITY_2}, {Flight_ID, AIRLINE, CLASS}, {Flight_ID,TIME}, etc.
The updated joint probabilities are iterativelycomputed according to the Equation (1) by each sub-network,the aposteriori probability P*(Gi) is computed by themarginalization of the updated joint probability P*(Gi,C).
P*(Gi)is then compared to a threshold (?)
to make the binary decision.where P*(C) is instantiated according the presence or absenceof the concepts; P(Gi,C) is the joint probability obtained fromtraining and P*(Gi,C) is the updated joint probabilityThe binary decisions across all BNs are combined toidentify the informational goal of the input query.
We maylabel the query to a goal if the corresponding BN votes positivewith the highest aposteriori probability.
Alternatively, we maylabel the query with all the goals for which the BNs votespositive.
Should all BNs vote negative, the input query isrejected as out-of-domain (OOD).3 We have included the concepts/attributes needed for databaseaccess, as well as others that play a syntactic role for naturallanguage understanding.4.2 Detection of Missing / SpuriousConceptsHaving inferred the informational goal of the query, thecorresponding node (goal node) is instantiated, and we performbackward inference to test the networks' confidence in eachinput concept.
In this way, we can test for cases of spurious andmissing concepts, and generate the appropriate systemsresponse.When the goal node is instantiated for backward inference,the joint probability of P(C, Gi) will be updated for each sub-network by Equation 2:where P*(G) is updated and instantiated to 1, P(C|Gi) is theconditional probability obtained from training data andP*(C,Gi) is the updated joint probabilityBy marginalization, we can get P(Cj).
We have pre-setthreshold 0.5 for the CU FOREX domain to determine whetherthe concept should be present or absent.
However, when thedialog modeling using single threshold scheme is applied to theATIS domain, we often obtained several missing / spuriousconcepts for an input query.
For example, consider the query.Query: What type of aircraft is used in American airlinesflight number seventeen twenty three?Concepts: <WHAT> <TYPE> <AIRCRAFT> <AIRLINE_NAME><FLIGHT_NUMBER>Goal: Aircraft_CodeOur BN for AIRCRAFT_CODE performed backwardinference and the results in Table 2 using single thresholdscheme indicated that the concepts <ORIGIN> and<DESTINATION> are missing, while <FLIGHT_NUMBER> isspurious.
One reason is because in the training data, mostqueries with the goal Aircraft_Code provided the city pairinstead of the flight number, but both serve equally well as anadditional specification for database access.
If our dialogmodel followed through with these detected missing andspurious concepts, it would prompt the user for the city oforigin, then the city of destination; and then clarify that theflight number is spurious.
In order to avoid such redundancies,we defined two thresholds for backward inferencing, as follows:Hence concepts whose probabilities (from backwardinference) scores between ?upper and ?lower will not take effect inresponse generation (i.e.
prompting / clarification).
Conceptswhose scores exceed ?upper, and also correspond to an SQLattribute will be prompted if missing; and concepts whosescores scant ?lower, and correspond to an SQL attribute will beclarified if spurious.
By minimizing number of dialog turnsinteracting with the users in the training data, we haveempirically adopted 0.7 and 0.2 for ?upper and ?lower respectively.The double threshold scheme enables the dialog model toprompt for missing concepts that are truly needed, and clarifyfor spurious concepts that may confuse the query?sinterpretation.
(1)(2)<?upper and >=?lower?
Cj is optional in the given Gi query>=?upper ?
Cj  should be present in the given Gi queryP(Cj)< ?lower ?
Cj should be absent in the given Gi query)()(),(),()()|(),( **** CPCPCGPCGPCPCGPCGP iiiivvvvvvv=?=)()|(),( ** iii GPGCPGCPvv=???
represents  a goal node?
represents a concept nodeCITY_1Flight IDCITY_2AIRLINETIME     CLASSTable 2.
Aposteriori probabilities obtained from backwardinferencing using 0.5 as threshold for the query ?What type ofaircraft is used in american airlines flight number seventeentwenty three?
?Conceptj (Cj)(Part of concepts)P(Cj ) BinaryDecisionFor CjActualOccurrencefor CjAIRCRAFT 1.000 present presentCITY_NAME1 0.645 present absentCITY_NAME2 0.615 present absentDAY_NAME 0.077 absent absentFLIGHT_NUMBER 0.420 absent present4.3 Context InheritanceWe attempt to test our framework using ATIS-3 Class A and Dqueries.
As the Class D queries involve referencing discoursecontext derived from previous dialog turns, we have enhancedour BN-based dialog model with the capability of contextinheritance.
Since the additional concepts may affect our goalinference, we choose to invoke goal inference again (aftercontext inheritance) only if query was previously (prior tocontext inheritance) classified as OOD.
Otherwise, the originalinferred goal of the query is maintained.
This is illustrated inTable 3.
Context inheritance serves to fill in the conceptsdetected missing from the original query.
This is illustrated inTables 4 and 5.Table 3.
Examples of ATIS dialogs produced by the BN-baseddialog model.
It indicates that the OOD query is inferred againas Flight_ID query after the inheritance of discourse context.System What kind of flight information are you interestedin?User I'd like to fly from miami to chicago on americanairlines.
(Class A query)System Goal Inference: Flight_ID (Concepts pass thedomain constraints)User Which ones arrive around five p.m.?
(Class D)System Goal Inference: Flight_ID.
(System first infers thisquery as OOD, but it retrieves the concepts fromthe discourse context and infers again to getFlight _ ID.
)Table 4.
Examples of ATIS dialogs produced by the BN-baseddialog model with the capability of inheritance for the missingconcepts.System What kind of flight information are youinterested in?User Please list all the flights from Chicago to Kansascity on June seventeenth.
(Class A query)System Goal Inference: Flight_ID (Concepts pass thedomain constraints)User For this flight how much would a first classfare cost.
(Class D)System Goal Inference: Fare_ID.
(The missing concepts<CITY_NAME1> <CITY_NAME2> are automaticallyretrieved from the discourse context.
)Table 5.
Aposteriori probabilities obtained from backwardinferencing using double threshold scheme for the Class Dquery ?For this flight how much would a first class fare cost.
?in Table 4.
It indicates that the cities of origin and destinationare missing.Conceptj (Cj)(Part of concepts)P(Cj ) Decisionfor CjActualOccurrencefor CjAIRPORT_NAME 0.0000 absent absentCITY_NAME1 0.9629 present absentCITY_NAME2 0.9629 present absentCLASS_NAME 0.2716 optional presentFARE 0.8765 present presentWe inherit discourse context for all the Class D queries.Based on the training data, we have designed a few contextrefresh rules to ?undo?
context inheritance for several querytypes.
For example, if the goal of the Class D query is<Airline_Code>, it is obviously asking about an airline, hencethe concept <AIRLINE_NAME> will not be inherited.5.
PORTABILITY OF A BN-BASEDDIALOG MODELIn addition to scalability, this work conducts a preliminaryexamination of the portability of our BN-based dialog modelsacross different application domains.
Migration to a newapplication often implies the lack of domain-specific data totrain our BN probabilities.
At this stage, BN probabilities canbe hand-assigned to reflect the "degree of belief" of theknowledge domain expert.5.1 General Principles for ProbabilityAssignmentFor each informational goal, we have to identify the conceptsthat are related to the goal.
For example, the informational goalGround_Transportation is usually associated with the keyconcepts of <AIRPORT_NAME> <CITY_NAME> and<TRANSPORT_TYPE>.
After the identification of all concepts forthe 11 goals, 23 key concepts (more details below) are extractedfrom the total 60 concepts.
Each of the 11 handcrafted BNshence receives as input of the identical set of 23 concepts.13 semantic concepts (out of 23) (e.g.
<CITY_NAME>,<AIRPORT_NAME>, <AIRLINE_NAME>) correspond to the SQLattributes for database access, while the reminding 10correspond to syntactic/semantic concepts (e.g.
<AIRCRAFT>,<FARE>, <FROM>).
For the sake of simplicity, we assumedindependence among concepts in the BN (pre-defined topology),and we then hand-assigned the four probabilities for each of the11 BNs, namely P(Cj=1|Gi=1), P(Cj=0|Gi=1), P(Cj=1|Gi=0),P(Cj=0|Gi=0).
We avoid assigning the probabilities of 1 or 0since they are not supportive of probabilistic inference.
In thefollowing we describe the general principles for assigningP(Cj=1|Gi=1) and P(Cj=1|Gi=0).
The remaining P(Cj=0|Gi=1)and P(Cj=0|Gi=0) can be derived by the complement of theformer two probabilities.5.1.1 Probability Assignment for P(Cj=1|Gi=1)We assign the probabilities of P(Cj=1|Gi=1) based on theoccurrence of the concept Cj with the corresponding Gi query asshown in Table 6.Case 1.
Cj must occur given GiIf we identify a concept that is mandatory for a query of goal Gi,we will hand-assign a high probability  (0.95-0.99) forP(Cj=1|Gi=1).
For example, concept <FARE> (for words e.g.fare, price, etc.)
must occur in Fare_ID query.
(?what is thefirst class fare from detroit to las vegas?
and ?show me the firstclass and coach price").Case 2.
Cj often occurs given GiIf the concept often occurs with the Gi query, then we will lowerthe probabilities of P(Cj=1|Gi=1) to the range of 0.7-0.8.
Forexample, the Fare_ID query often comes with the concepts of<CITY_ORIGIN> and <CITY_DESTINATION>.Case 3.
Cj may occur given GiThis applies to the concepts that act as additional constraints fordatabase access.
Examples are <TIME_VALUE>, <DAY_NAME>,<PERIOD>specified in the user query.Case 4.
Cj seldom occurs given GiThe occurrence of this kind of concepts in the user query isinfrequent.
Example includes the concept <STOPS> whichspecify the nonstop flight for the Fare_ID query.Case 5.
Cj never occurs given GiThis kind of concepts usually provides negative evidence forgoal inference.
Examples include the concept<FLIGHT_NUMBER> in the Flight_ID query.
The presence of<FLIGHT_NUMBER> in the input query implies that the goalFlight_ID  is unlikely, because the aposteriori probability forthe BN Flight_ID is lowered.Table 6.
Conditions for assigning the probabilitiesP(Cj=1|Gi=1).Condition Probability of P(Cj=1|Gi=1)1.
Cj must occur given GI 0.95 ?
0.992.
Cj often occur given Gi 0.7 ?
0.83.
Cj may occur given GI 0.4 ?
0.64.
Cj seldom occur given Gi 0.2 ?
0.35.
Cj must not occur given Gi 0.01 ?
0.15.1.2 Probability Assignment for P(Cj=1|Gi=0)For assignment the probabilities of P(Cj=1|Gi=0) for BNi, wehave to consider the occurrence of  the concepts for goals otherthan Gi, i.e.
for goal Gm (where m ranges between 1 and 11  butis not equal to i).
The scheme for assigning P(Cj=1|Gi=0), i.e.probability of concept Cj being present while goal Gi is absent,is shown in Table 7.Case 1.
Cj always occurs for goals other than GiConsider the relationship between the concept <CITY> and thegoal Aircraft_Code.
Since <CITY> always occur for otherinformational goals, (e.g.
Flight_ID, Fare_ID, etc.
), we assignP(C<CITY>=1|G<Aircraft_Code>=0) in the range of 0.7-0.9.Case 2.
Cj sometimes occurs for goals other than GiConsider the relationship between the concept <CLASS> and thegoal Aircraft_Code.
Since <CLASS> sometimes occurs in theinformational goals other than Aircraft_Code, and acts as theadditional constraints for database access, we assignP(C<CLASS>=1|G<Aircraft_Code>=0) in the range of 0.2-0.5.Case 3.
Cj seldom occurs for goals other than GiThis applies to the concepts that are strongly dependent on aspecific goal and hence seldom appear for other goals.
Forexample, the concept <TRANSPORTATION> usually accompaniesthe goal Ground_Transportation only.
HenceP(C<TRANSPORTATION>=1|G<Ground_Transportation>=0)is set closed to 0.Table 7 Conditions for assigning the probabilities P(Cj=1|Gi=0)Condition Probability ofP(Cj=1|Gi=0)1.
Cj always occurs for goals other than Gi 0.7 ?
0.92.
Cj sometimes occurs for goals other than Gi 0.2 ?
0.53.
Cj seldom occurs for goals other than Gi 0.01 ?
0.15.2 EvaluationBNs with hand-assigned probabilities achieved a goalidentification accuracy of 80.9% for the ATIS-3 1993 test set(Class A and D sentences included).
This compares to 84.6%when they have been automatically trained on the training data.The availability of training data for the BNs enhancesperformance in goal identification.
Queries whose goals are notcovered by our 11 BNs are treated as OOD, and are consideredto be identified correctly if there are classified as such.We have compared the handcrafted probabilities with thetrained probabilities based on natural language understanding,where the evaluation metric is the sentence error rate.
Asentence is considered correct only if the inferred goal andextracted concepts in the generated semantic frame agrees withthose in the reference semantic frame (derived from the SQL inthe ATIS corpora).
The goal identification accuracies and thesentence error rates for the ATIS-3 1993 test set aresummarized in Table 8.
When we compare the our results withthe NL understanding results from the 10 ATIS evaluation sitesshown in Table 9, our performance falls within a reasonablerange.Table 8 Goal identification accuracies and the sentence errorrates of Class A and D queries of ATIS test 93 data for thehandcrafted probabilities and automatically trained probabilitiesrespectively.ClassBNs(handcraftedprobabilities)BNs(trainedprobabilities)A (448) 90.18% 91.74%D (325) 68.31% 74.78%Goal IDAccuracy A+D  80.98%  84.61%A (448) 12.05% 9.15%D (325) 40.92% 33.85%SentenceErrorRate A+D 24.19% 19.53%Table 9 Benchmark NL  results from the  10 ATIS evaluationsites [6].Class Sentence Error RateA (448) 6.0 ?
28.6%D (325) 13.8 ?
63.1%A+D (773) 9.3 ?
43.1%We observed that our strategy for context inheritance maybe too aggressive, which leads to concept insertion errors in thegenerated semantic frame.
This is illustrated in the example inTable 10.Table 10 The case frame for query 3 indicates our contextinheritance strategy may be too aggressive which leads to aconcept insertion error in the generated semantic frame.Query 1: List flights from oakland to salt lake city beforesix a m Thursday morning(Our system generates a correct semantic frame.
)Query 2: List delta flights before six a m (Class D)(Our system generates a correct semantic frame.
)Query 3: List all flights from twelve oh one a m until six am (Class D)(Our system detects missing concepts of<CITY_NAME>, which are inherited fromdiscourse)Case Frame SQL ReferenceGoal: Flight_ID Flight_IDCITY_NAME = oakland CITY_NAME = oaklandConcepts:CITY_NAME = salt lakecityDEPARTURE_TIME =twelve oh one a m untilsix a mAIRLINE_NAME = delta (aconcept insertion error)CITY_NAME = salt lake cityDEPARTURE_TIME = >=1&& <= 6006.
SUMMARY AND CONCLUSIONSThis paper describes the scalability and portability of the BN-based dialog model as we migrate from the foreign exchangedomain (CU FOREX) to the relatively more complex air traveldomain (ATIS).
The complexity of an application domain ischaracterized by the number of in-domain informational goalsand concepts.
The presence / absence of concepts are used toinfer the presence/absence of each goal, by means of the BN.When a large number of in-domain concepts are available, weused an information-theoretic criterion (Information Gain) toautomatically select the small set of concepts most indicative ofa goal, and do so for every in-domain goal.
Automaticdetection of missing / spurious concepts is achieved bybackward inference using the BN corresponding to the inferredgoal.
This detection procedure drives our mixed-initiativedialog model ?
the system prompts the user for missingconcepts, and asks for clarification if spurious concepts aredetected.
For the simpler CU FOREX domain, detection ofmissing / spurious concepts was based on a single probabilitythreshold.
However, scaling up to ATIS (which has many moreconcepts) shows that some concepts need to be present, othersshould be absent, but still others should be optional.
Hence weneed to use two levels of thresholding to decide if a conceptshould be present, optional or absent in the query.
We havealso enhanced our BN-based dialog model with the capabilityof context-inheritance, in order to handle the context-dependentuser queries in the ATIS domain.
Discourse context is inheritedfor the Class D queries, and we invoke goal inference againafter context inheritance if a query was previously classified asOOD.As regards portability, migration to a new applicationdomain often implies the lack of domain-specific training data.Hence we have proposed a set of general principles forprobability assignment to the BNs, as a reflection of our?degree of belief?
in the relationships between concepts andgoals.
We compared the goal identification performance, aswell as concept error rates between the use of hand-assignedprobabilities, and the probabilities trained from the ATIStraining set.
Results show that the hand-assigned probabilitiesoffer a decent starting performance to ease portability to a newdomain.
The system performance can be further improved ifdata is available to train the probabilities.7.
REFERENCES[1] Meng, H., W. Lam and C. Wai, ?To Believe is toUnderstand,?
Proceedings of Eurospeech, 1999.
[2] Meng, H., W. Lam and K. F. Low, ?Learning BeliefNetworks for Language Understanding,?
Proceedings ofASRU, 1999.
[3] Meng, H., S. Lee and C. Wai, ?CU FOREX:  A BilingualSpoken Dialog System for the Foreign Exchange Domain,?Proceedings of ICASSP, 2000.
[4] Meng, H., C. Wai, R. Pieraccini, ?The Use of BeliefNetworks for Mixed-Initiative Dialog Modeling,?Proceeding of ICSLP, 2000.
[5] Kuhn, R., and R. De Mori, ?The Application of SemanticClassification Trees for Natural Language Understanding,?IEEE Trans.
PAMI, Vol.
17, No.
5, pp.
449-460, May1995.
[6] Pallet, D., J. Fiscus, W. Fisher, J. Garofolo, B. Lund, andM.
Przybocki, ?1993 Benchmark Tests for the ARPASpoken Language Program,?
Proceedings of the SpokenLanguage Technology Workshop, 1994.
[7] Levin, E., Pieraccini, R., and Eckert, W., ?A StochasticModel of Human-Machine Interaction for LearningDialogue Strategies?, Speech and Audio Processing, IEEETransactions, Vol 8, pp.
11-23, Jan 2000.
[8] Walker, M., Fromer, J., Narayanan, S., ?Learning OptimalDialogue Strategies: A Case Study of a Spoken DialogueAgent for Email?, in Proceedings of ACL/COLING 98 ,1998.
