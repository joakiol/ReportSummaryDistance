Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 548?556,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsInducing Compact but Accurate Tree-Substitution GrammarsTrevor Cohn and Sharon Goldwater and Phil BlunsomSchool of InformaticsUniversity of Edinburgh10 Crichton Street, Edinburgh EH8 9ABScotland, United Kingdom{tcohn,sgwater,pblunsom}@inf.ed.ac.ukAbstractTree substitution grammars (TSGs) are a com-pelling alternative to context-free grammarsfor modelling syntax.
However, many popu-lar techniques for estimating weighted TSGs(under the moniker of Data Oriented Parsing)suffer from the problems of inconsistency andover-fitting.
We present a theoretically princi-pled model which solves these problems us-ing a Bayesian non-parametric formulation.Our model learns compact and simple gram-mars, uncovering latent linguistic structures(e.g., verb subcategorisation), and in doing sofar out-performs a standard PCFG.1 IntroductionMany successful models of syntax are based onProbabilistic Context Free Grammars (PCFGs)(e.g., Collins (1999)).
However, directly learning aPCFG from a treebank results in poor parsing perfor-mance, due largely to the unrealistic independenceassumptions imposed by the context-free assump-tion.
Considerable effort is required to coax goodresults from a PCFG, in the form of grammar en-gineering, feature selection and clever smoothing(Collins, 1999; Charniak, 2000; Charniak and John-son, 2005; Johnson, 1998).
This effort must be re-peated when moving to different languages, gram-mar formalisms or treebanks.
We propose that muchof this hand-coded knowledge can be obtained auto-matically as an emergent property of the treebankeddata, thereby reducing the need for human input incrafting the grammar.We present a model for automatically learning aProbabilistic Tree Substitution Grammar (PTSG),an extension to the PCFG in which non-terminalscan rewrite as entire tree fragments (elementarytrees), not just immediate children.
These large frag-ments can be used to encode non-local context, suchas head-lexicalisation and verb sub-categorisation.Since no annotated data is available providing TSGderivations we must induce the PTSG productionsand their probabilities in an unsupervised way froman ordinary treebank.
This is the same problem ad-dressed by Data Oriented Parsing (DOP, Bod et al(2003)), a method which uses as productions all sub-trees of the training corpus.
However, many of theDOP estimation methods have serious shortcomings(Johnson, 2002), namely inconsistency for DOP1(Bod, 2003) and overfitting of the maximum like-lihood estimate (Prescher et al, 2004).In this paper we develop an alternative means oflearning a PTSG from a treebanked corpus, with thetwin objectives of a) finding a grammar which ac-curately models the data and b) keeping the gram-mar as simple as possible, with few, compact, ele-mentary trees.
This is achieved using a prior to en-courage sparsity and simplicity in a Bayesian non-parametric formulation.
The framework allows us toperform inference over an infinite space of gram-mar productions in an elegant and efficient manner.The net result is a grammar which only uses the in-creased context afforded by the TSG when necessaryto model the data, and otherwise uses context-freerules.1 That is, our model learns to use larger ruleswhen the CFG?s independence assumptions do nothold.
This contrasts with DOP, which seeks to useall elementary trees from the training set.
While ourmodel is able, in theory, to use all such trees, in prac-tice the data does not justify such a large grammar.Grammars that are only about twice the size of a1While TSGs and CFGs describe the same string lan-guages, TSGs can describe context-sensitive tree-languages,which CFGs cannot.548treebank PCFG provide large gains in accuracy.
Weobtain additional improvements with grammars thatare somewhat larger, but still much smaller than theDOP all-subtrees grammar.
The rules in these gram-mars are intuitive, potentially offering insights intogrammatical structure which could be used in, e.g.,the development of syntactic ontologies and guide-lines for future treebanking projects.2 Background and related workA Tree Substitution Grammar2 (TSG) is a 4-tuple,G = (T,N, S,R), where T is a set of terminal sym-bols, N is a set of non-terminal symbols, S ?
N isthe distinguished root non-terminal and R is a setof productions (a.k.a.
rules).
The productions takethe form of elementary trees ?
tree fragments ofdepth ?
2, where each internal node is labelled witha non-terminal and each leaf is labelled with either aterminal or a non-terminal.
Non-terminal leaves arecalled frontier non-terminals and form the substitu-tion sites in the generative process of creating treeswith the grammar.A derivation creates a tree by starting with theroot symbol and rewriting (substituting) it with anelementary tree, then continuing to rewrite frontiernon-terminals with elementary trees until there areno remaining frontier non-terminals.
Unlike Con-text Free Grammars (CFGs) a syntax tree may notuniquely specify the derivation, as illustrated in Fig-ure 1 which shows two derivations using differentelementary trees to produce the same tree.A Probabilistic Tree Substitution Grammar(PTSG), like a PCFG, assigns a probability to eachrule in the grammar.
The probability of a derivationis the product of the probabilities of its componentrules, and the probability of a tree is the sum of theprobabilities of its derivations.As we mentioned in the introduction, work withinthe DOP framework seeks to induce PTSGs fromtreebanks by using all possible subtrees as rules, andone of a variety of methods for estimating rule prob-abilities.3 Our aim of inducing compact grammarscontrasts with that of DOP; moreover, we develop aprobabilistic estimator which avoids the shortcom-ings of DOP1 and the maximum likelihood esti-2A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003))without the adjunction operator.3TAG induction (Chiang and Bikel, 2002; Xia, 2002) alsotackles a similar learning problem.mate (Bod, 2000; Bod, 2003; Johnson, 2002).
Re-cent work on DOP estimation also seeks to addressthese problems, drawing from estimation theory tosolve the consistency problem (Prescher et al, 2004;Zollmann and Sima?an, 2005), or incorporating agrammar brevity term into the learning objective(Zuidema, 2007).
Our work differs from these pre-vious approaches in that we explicitly model a priorover grammars within a Bayesian framework.4Models of grammar refinement (Petrov et al,2006; Liang et al, 2007; Finkel et al, 2007) alsoaim to automatically learn latent structure underly-ing treebanked data.
These models allow each non-terminal to be split into a number of subcategories.Theoretically the grammar space of our model is asub-space of theirs (projecting the TSG?s elementarytrees into CFG rules).
However, the number of non-terminals required to recreate our TSG grammarsin a PCFG would be exorbitant.
Consequently, ourmodel should be better able to learn specific lexicalpatterns, such as full noun-phrases and verbs withtheir sub-categorisation frames, while theirs are bet-ter suited to learning subcategories with larger mem-bership, such as the terminals for days of the weekand noun-adjective agreement.
The approaches areorthogonal, and we expect that combining a categoryrefinement model with our TSG model would pro-vide better performance than either approach alone.Our model is similar to the Adaptor Grammarmodel of Johnson et al (2007b), which is alsoa kind of Bayesian nonparametric tree-substitutiongrammar.
However, Adaptor Grammars require thateach sub-tree expands completely, with only termi-nal symbols as leaves, while our own model permitsnon-terminal frontier nodes.
In addition, they disal-low recursive containment of adapted non-terminals;we impose no such constraint.3 ModelRecall the nature of our task: we are given a corpusof parse trees t and wish to infer a tree-substitutiongrammar G that we can use to parse new data.Rather than inferring a grammar directly, we gothrough an intermediate step of inferring a distri-bution over the derivations used to produce t, i.e.,4A similar Bayesian model of TSG induction has been de-veloped independently to this work (O?Donnell et al, 2009b;O?Donnell et al, 2009a).549(a)SNPNPGeorgeVPVhatesNPNPbroccoli(b)SNPGeorgeVPVVhatesNPbroccoliS?
NP (VP (V hates) NP)NP?
GeorgeNP?
broccoliS?
(NP George) (VP V (NP broccoli))V?
hatesFigure 1: Example derivations for the same tree,where arrows indicate substitution sites.
The ele-mentary trees used in (a) and (b) are shown belowas grammar productions in bracketed tree notation.a distribution over sequences of elementary trees ethat compose to form t. We will then essentially readthe grammar off the elementary trees, as describedin Section 5.
Our problem therefore becomes one ofidentifying the posterior distribution of e given t,which we can do using Bayes?
Rule:P (e|t) ?
P (t|e)P (e) (1)Since the sequence of elementary trees can be splitinto derivations, each of which completely specifiesa tree, P (t|e) is either equal to 1 (when t and eare consistent) or 0 (otherwise).
Therefore, the workin our model is done by the prior distribution overelementary trees.
Note that this is analogous to theBayesian model of word segmentation presented byGoldwater et al (2006); indeed, the problem of in-ferring e from t can be viewed as a segmentationproblem, where each full tree must be segmentedinto one or more elementary trees.
As in Goldwateret al (2006), we wish to favour solutions employinga relatively small number of elementary units (here,elementary trees).
This can be done using a Dirichletprocess (DP) prior.
Specifically, we define the distri-bution of elementary tree e with root non-terminalsymbol c asGc|?c, P0 ?
DP(?c, P0(?|c))e|c ?
GcwhereP0(?|c) (the base distribution) is a distributionover the infinite space of trees rooted with c, and ?c(the concentration parameter) controls the model?stendency towards either reusing elementary trees orcreating novel ones as each training instance is en-countered (and consequently, the tendency to inferlarger or smaller sets of elementary trees from theobserved data).
We discuss the base distribution inmore detail below.Rather than representing the distribution Gc ex-plicitly, we integrate over all possible values of Gc.The resulting distribution over ei, conditioned one<i = e1 .
.
.
ei?1 and the root category c is:p(ei|e<i, c, ?c, P0) = n<iei,c + ?cP0(ei|c)n<i?,c + ?c (2)where n<iei,c is the number number of times ei hasbeen used to rewrite c in e<i, and n<i?,c =?e n<ie,c isthe total count of rewriting c.As with other DP models, ours can be viewed as acache model, where ei can be generated in one oftwo ways: by drawing from the base distribution,where the probability of any particular tree is pro-portional to ?cP0(ei|c), or by drawing from a cacheof previous expansions of c, where the probability ofany particular expansion is proportional to the num-ber of times that expansion has been used before.This view makes it clear that the model embodiesa ?rich-get-richer?
dynamic in which a few expan-sions will occur with high probability, but many willoccur only once or twice, as is typical of natural lan-guage.
Our model is similar in this way to the Adap-tor Grammar model of Johnson et al (2007a).We still need to define P0, the base distributionover tree fragments.
We use two such distributions.The first, PM0 generates each elementary tree bya series of random decisions: whether to expand anon-terminal, how many children to produce andtheir identities.
The probability of expanding a non-terminal node labelled c is parameterised via a bino-mial distribution, Bin(?c), while all other decisionsare chosen uniformly at random.
The second basedistribution, PC0 , has a similar generative processbut draws non-terminal expansions from a treebank-trained PCFG instead of a uniform distribution.Both choices of P0 have the effect of biasing themodel towards simple rules with a small number ofinternal nodes.
The geometric increase in cost dis-courages the model from using larger rules; for thisto occur these rules must yield a large increase in thedata likelihood.
As PC0 incorporates PCFG probabil-550SNP,1GeorgeVP,0V,0hatesNP,1broccoliFigure 2: Gibbs state e specifying the derivation inFigure 1a.
Each node is labelled with its substitutionindicator variable.ities, it assigns higher relative probability to largerrules, compared to the more draconian PM0 .4 TrainingTo train our model we use Gibbs sampling (Gemanand Geman, 1984), a Markov chain Monte Carlomethod in which variables are repeatedly sampledconditioned on the values of all other variables inthe model.
After a period of burn-in, each sam-pler state (set of variable assignments) is a samplefrom the posterior distribution of the model.
In ourcase, we wish to sample from P (e|t, ?, ?
), where(?, ?)
= {?c, ?c} for all categories c. To do so,we associate a binary variable with each non-rootinternal node of each tree in the training set, indi-cating whether that node is a substitution point ornot.
Each substitution point forms the root of someelementary tree, as well as a frontier non-terminalof an ancestor node?s elementary tree.
Collectively,the training trees and substitution variables specifythe sequence of elementary trees e that is the currentstate of the sampler.
Figure 2 shows an example treewith its substitution variables, corresponding to theTSG derivation in Figure 1a.Our Gibbs sampler works by sampling the valueof each substitution variable, one at a time, in ran-dom order.
If d is the node associated with the sub-stitution variable s under consideration, then the twopossible values of s define two options for e: onein which d is internal to some elementary tree eM ,and one in which d is the substitution site con-necting two smaller trees, eA and eB .
In the ex-ample in Figure 2, when sampling the VP node,eM = (S NP (VP (V hates) NP)), eA = (S NP VP),and eB = (VP (V hates) NP).
To sample a value fors, we compute the probabilities of eM and (eA, eB),conditioned on e?
: all other elementary trees in thetraining set that share at most a root or frontier non-terminal with eM , eA, or eB .
This is easy to dobecause the DP is exchangeable, meaning that theprobability of a set of outcomes does not depend ontheir ordering.
Therefore, we can treat the elemen-tary trees under consideration as the last ones to besampled, and apply Equation 2, giving usP (eM |cM )=n?eM ,cM + ?cMP0(eM |cM )n?
?,cM + ?cM(3)P (eA, eB|cA)=n?eA,cA + ?cAP0(eA|cA)n?
?,cA + ?cA(4)?n?eB ,cB + ?
(eA, eB) + ?cBP0(eB|cB)n?
?,cB + ?
(cA, cB) + ?cBwhere cx is the root label of ex, x ?
{A,B,M},the counts n?
are with respect to e?, and ?
(?, ?)
isthe Kronecker delta function, which returns 1 whenits arguments are identical and 0 otherwise.
We haveomitted e?, t, ?
and ?
from the conditioning con-text.
The ?
terms in the second factor of (4) accountthe changes to n?
that would occur after observingeA, which forms part of the conditioning context foreB .
If the trees eA and eB are identical, then thecount n?eB ,cB would increase by one, and if the treesshare the same root non-terminal, then n?
?,cB wouldincrease by one.In the previous discussion, we have assumedthat the model hyperparameters, (?, ?
), are known.However, selecting their values by hand is extremelydifficult and fitting their values on heldout data is of-ten very time consuming.
For this reason we treatthe hyper-parameters as variables in our model andinfer their values during training.
We choose vaguepriors for each hyper-parameter, encoding our lackof information about their values.
We treat the con-centration parameters, ?, as being generated by avague gamma prior, ?c ?
Gamma(0.001, 1000).We sample a new value ?
?c using a log-normal dis-tribution with mean ?c and variance 0.3, which isthen accepted into the distribution p(?c|e, t, ?
?, ?
)using the Metropolis-Hastings algorithm.
We use aBeta prior for the binomial specification parameters,?c ?
Beta(1, 1).
As the Beta distribution is conju-gate to the binomial, we can directly resample the?
parameters from the posterior, p(?c|e, t, ?, ??
).Both the concentration and substitution parametersare resampled after every full Gibbs sampling itera-tion over the training trees.5515 ParsingWe now turn to the problem of using the modelto parse novel sentences.
This requires finding themaximiser ofp(t|w, t) =?p(t|w, e, ?, ?)
p(e, ?, ?|t) de d?
d?
(5)wherew is the sequence of words being parsed and tthe resulting tree, t are the training trees and e theirsegmentation into elementary trees.Unfortunately solving for the maximising parsetree in (5) is intractable.
However, it can approxi-mated using Monte Carlo techniques.
Given a sam-ple of (e, ?, ?
)5 we can reason over the space ofpossible trees using a Metropolis-Hastings sampler(Johnson et al, 2007a) coupled with a Monte Carlointegral (Bod, 2003).
The first step is to sample fromthe posterior over derivations, p(d|w, e, ?, ?).
Thisis achieved by drawing samples from an approxima-tion grammar, p?
(d|w), which are then accepted tothe true distribution using the Metropolis-Hastingsalgorithm.
The second step records for each sampledderivation the CFG tree.
The counts of trees consti-tute an approximation to p(t|w, e, ?, ?
), from whichwe can recover the maximum probability tree.A natural proposal distribution, p?
(d|w), is themaximum a posterior (MAP) grammar given the el-ementary tree analysis of our training set (analogousto the PCFG approximation used in Johnson et al(2007a)).
This is not practical because the approx-imation grammar is infinite: elementary trees withzero count in e still have some residual probabil-ity under P0.
In the absence of a better alternative,we discard (most of) the zero-count rules from MAPgrammar.
This results in a tractable grammar repre-senting the majority of the probability mass, fromwhich we can sample derivations.
We specificallyretain all zero-count PCFG productions observed inthe training set in order to provide greater robustnesson unseen data.In addition to finding the maximum probabilityparse (MPP), we also report results using the maxi-mum probability derivation (MPD).
While this couldbe calculated in the manner as described above, we5Using many samples of (e, ?, ?)
in a Monte Carlo inte-gral is a straight-forward extension to our parsing algorithm.
Wedid not observe a significant improvement in parsing accuracywhen using a multiple samples compared to a single sample,and therefore just present results for a single sample.S ?
A | BA?
A A | B B | (A a) (A a) | (B a) (B a)B ?
A A | B B | (A b) (A b) | (B b) (B b)Figure 3: TSG used to generate synthetic data.
Allproduction probabilities are uniform.found that using the CYK algorithm (Cocke, 1969)to find the Viterbi derivation for p?
yielded consis-tently better results.
This algorithm maximises anapproximated model, as opposed to approximatelyoptimising the true model.
We also present resultsusing the tree with the maximum expected count ofCFG rules (MER).
This uses counts of the CFG rulesapplied at each span (compiled from the derivationsamples) followed by a maximisation step to find thebest tree.
This is similar to the MAX-RULE-SUMalgorithm of Petrov and Klein (2007) and maximumexpected recall parsing (Goodman, 2003).6 ExperimentsSynthetic data Before applying the model tonatural language, we first create a synthetic problemto confirm that the model is capable of recoveringa known tree-substitution grammar.
We created 50random trees from the TSG shown in Figure 3.
Thisproduces binary trees with A and B internal nodesand ?a?
and ?b?
as terminals, such that the termi-nals correspond to their grand-parent non-terminal(A and a or B and b).
These trees cannot be mod-elled accurately with a CFG because expanding Aand B nodes into terminal strings requires knowingtheir parent?s non-terminal.We train the model for 100 iterations of Gibbssampling using annealing to speed convergence.Annealing amounts to smoothing the distributionsin (3) and (4) by raising them to the power of 1T .Our annealing schedule begins at T = 3 and lin-early decreases to reach T = 1 in the final iteration.The sampler converges to the correct grammar, withthe 10 rules from Figure 3.Penn-treebank parsing We ran our natural lan-guage experiments on the Penn treebank, using thestandard data splits (sections 2?21 for training, 22for development and 23 for testing).
As our model isparameter free (the ?
and ?
parameters are learnt intraining), we do not use the development set for pa-552rameter tuning.
We expect that fitting these param-eters to maximise performance on the developmentset would lead to a small increase in generalisationperformance, but at a significant cost in runtime.
Wereplace tokens with count?
1 in the training samplewith one of roughly 50 generic unknown word mark-ers which convey the token?s lexical features and po-sition in the sentence, following Petrov et al (2006).We also right-binarise the trees to reduce the branch-ing factor in the same manner as Petrov et al (2006).The predicted trees are evaluated using EVALB6 andwe report the F1 score over labelled constituents andexact match accuracy over all sentences in the test-ing sets.In our experiments, we initialised the sampler bysetting all substitution variables to 0, thus treatingevery full tree in the training set as an elementarytree.
Starting with all the variables set to 1 (corre-sponding to CFG expansions) or a random mix of0s and 1s considerably increases time until conver-gence.
We hypothesise that this is due to the samplergetting stuck in modes, from which a series of lo-cally bad decisions are required to escape.
The CFGsolution seems to be a mode and therefore startingthe sampler with maximal trees helps the model toavoid this mode.Small data sample For our first treebank exper-iments, we train on a small data sample by usingonly section 2 of the treebank.
Bayesian methodstend to do well with small data samples, while forlarger samples the benefits diminish relative to pointestimates.
The models were trained using Gibbssampling for 4000 iterations with annealing linearlydecreasing from T = 5 to T = 1, after whichthe model performed another 1000 iterations withT = 1.
The final training sample was used in theparsing algorithm, which used 1000 derivation sam-ples for each test sentence.
All results are the aver-age of five independent runs.Table 1 presents the prediction results on the de-velopment set.
The baseline is a maximum likeli-hood PCFG.
The TSG model significantly outper-forms the baseline with either base distribution PM0or PC0 .
This confirms our hypothesis that CFGs arenot sufficiently powerful to model syntax, but thatthe increased context afforded to the TSG can makea large difference.
This result is even more impres-sive when considering the difference in the sizes of6See http://nlp.cs.nyu.edu/evalb/.F1 EX # rulesPCFG 60.20 4.29 3500TSG PM0 : MPD 72.17 11.92 6609MPP 71.27 12.33 6609MER 74.25 12.30 6609TSG PC0 : MPD 75.24 15.18 14923MPP 75.30 15.74 14923MER 76.89 15.76 14923SM?=2: MPD 71.93 11.30 16168MER 74.32 11.77 16168SM?=5: MPD 75.33 15.64 39758MER 77.93 16.94 39758Table 1: Development results for models trained onsection 2 of the Penn tree-bank, showing labelledconstituent F1 and exact match accuracy.
Grammarsizes are the number of rules with count ?
1.grammar in the PCFG versus TSG models.
The TSGusing PM0 achieves its improvements with only dou-ble as many rules, as a consequence of the priorwhich encourages sparse solutions.
The TSG resultswith the CFG base distribution, PC0 , are more ac-curate but with larger grammars.7 This base distri-bution assigns proportionally higher probability tolarger rules than PM0 , and consequently the modeluses these additional rules in a larger grammar.Surprisingly, the MPP technique is not systemati-cally better than the MPD approach, with mixed re-sults under the F1 metric.
We conjecture that this isdue to sampling variance for long sentences, whererepeated samples of the same tree are exceedinglyrare.
The MER technique results in considerablybetter F1 scores than either MPD or MPP, with amargin of 1.5 to 3 points.
This method is less af-fected by sampling variance due to its use of smallertree fragments (PCFG productions at each span).For comparison, we trained the Berkeley split-merge (SM) parser (Petrov et al, 2006) on the samedata and decoded using the Viterbi algorithm (MPD)and expected rule count (MER a.k.a.
MAX-RULE-SUM).
We ran two iterations of split-merge training,after which the development F1 dropped substan-tially (in contrast, our model is not fit to the devel-opment data).
The result is an accuracy slightly be-low that of our model (SM?=2).
To be fairer to theirmodel, we adjusted the unknown word threshold totheir default setting, i.e., to apply to word types oc-7The grammar is nevertheless far smaller than the full DOPgrammar on this data set, which has 700K rules.5530 1 2 3 4 5 6 7 8countcount ofcounts0100200300400500 depthnodeslexemesvarsFigure 4: Grammar statistics for a TSG PM0 modeltrained on section 2 of the Penn treebank, show-ing a histogram over elementary tree depth, num-ber of nodes, terminals (lexemes) and frontier non-terminals (vars).curring fewer than five times (SM?=5).
We expectthat tuning the treatment of unknown words in ourmodel would also yield further gains.
The grammarsizes are not strictly comparable, as the Berkeley bi-narised grammars prohibit non-binary rules, and aretherefore forced to decompose each of these rulesinto many child rules.
But the trend is clear ?
ourmodel produces similar results to a state-of-the-artparser, and can do so using a small grammar.
Withadditional rounds of split-merge training, the Berke-ley grammar grows exponentially larger (200K rulesafter six iterations).Full treebank We now train the model usingPM0 on the full training partition of the Penn tree-bank, using sections 2?21.
We run the Gibbs samplerfor 15,000 iterations while annealing from T = 5 toT = 1, after which we finish with 5,000 iterationsat T = 1.
We repeat this three times, giving an av-erage F1 of 84.0% on the testing partition using themaximum expected rule algorithm and 83.0% usingthe Viterbi algorithm.
This far surpasses the ML-PCFG (F1 of 70.7%), and is similar to Zuidema?s(2007) DOP result of 83.8%.
However, it still wellbelow state-of-the art parsers (e.g., the Berkeleyparser trained using the same data representationscores 87.7%).
But we must bear in mind that theseparsers have benefited from years of tuning to thePenn-treebank, where our model is much simplerand is largely untuned.
We anticipate that carefuldata preparation and model tuning could greatly im-prove our model?s performance.NP?
(NNP Mr.) NNPCD (NN %)(NP CD (NN %)) (PP (IN of) NP)(NP ($ $) CD) (NP (DT a) (NN share))(NP (DT the) (N?P (NN company) POS)) N?P(NP QP (NN %)) (PP (IN of) NP)(NP CD (NNS cents)) (NP (DT a) (NN share))(NP (NNP Mr.) (N?P NNP (POS ?s))) NNQP (NN %)(NP (NN president)) (PP (IN of) NP)(NP (NNP Mr.) (N?P NNP (POS ?s))) N?PNNP (N?P NNP (NNP Corp.))NNP (N?P NNP (NNP Inc.))(NP (NN chairman)) (PP (IN of) NP)VP?
(VBD said) (SBAR (S (NP (PRP it)) VP))(VBD said) (SBAR (S NP VP))(VBD rose) (V?P (NP CD (NN %)) V?P)(VBP want) S(VBD said) (SBAR (S (NP (PRP he)) VP))(VBZ plans) S(VBD said) (SBAR S)(VBZ says) (SBAR (S NP VP))(VBP think) (SBAR S)(VBD agreed) (S (VP (TO to) (VP VB V?P)))(VBZ includes) NP(VBZ says) (SBAR (S (NP (PRP he)) VP))(VBZ wants) S(VBD closed) (V?P (PP (IN at) NP) (V?P , ADVP))Table 3: Most frequent lexicalised expansions fornoun and verb phrases, excluding auxiliary verbs.7 DiscussionSo what kinds of non-CFG rules is the model learn-ing?
Figure 4 shows the grammar statistics for aTSG model trained on the small data sample.
Thismodel has 5611 CFG rules and 1008 TSG rules.The TSG rules vary in depth from two to nine levelswith the majority between two and four.
Most rulescombine a small degree of lexicalisation and a vari-able or two.
This confirms that the model is learn-ing local structures to encode, e.g., multi-word units,subcategorisation frames and lexical agreement.
Thefew very large rules specify full parses for sentenceswhich were repeated in the training corpus.
Thesecomplete trees are also evident in the long tail ofnode counts (up to 27; not shown in the figure) andcounts for highly lexicalised rules (up to 8).To get a better feel for the types of rules beinglearnt, it is instructive to examine the rules in the re-554NP?
PP?
ADJP?DT N?P IN NP JJNNS (IN in) NP RB JJDT NN (TO to) NP JJ ( ?ADJP CC JJ)(DT the) N?P TO NP JJ PPJJ NNS (IN with) NP (RB very) JJNP (PP (IN of) NP) (IN of) NP RB ?ADJPNP PP (IN by) NP (RBR more) JJNP (N?P (CC and) NP) (IN at) NP JJ ?ADJPJJ N?P IN (NP (DT the) N?P) ADJP ( ?ADJP CC ADJP)NN NNS (IN on) NP RB VBN(DT the) NNS (IN from) NP RB ( ?ADJP JJ PP)DT (N?P JJ NN) IN (S (VP VBG NP)) JJ (PP (TO to) NP)NN IN (NP NP PP) ADJP (PP (IN than) NP)JJ NN (IN into) NP (RB too) JJ(NP DT NN) (PP (IN of) NP) (IN for) NP (RB much) JJRTable 2: Top fifteen expansions sorted by frequency (most frequent at top), taken from the final sample of amodel trained on the full Penn treebank.
Non-terminals shown with an over-bar denote a binarised sub spanof the given phrase type.sultant grammar.
Table 2 shows the top fifteen rulesfor three phrasal categories for the model trained onthe full Penn treebank.
We can see that many of theserules are larger than CFG rules, showing that theCFG rules alone are inadequate to model the tree-bank.
Two of the NP rules encode the prevalenceof preposition phrases headed by ?of?
within a nounphrase, as opposed to other prepositions.
Also note-worthy is the lexicalisation of the determiner, whichcan affect the type of NP expansion.
For instance,the indefinite article is more likely to have an ad-jectival modifier, while the definite article appearsmore frequently unmodified.
Highly specific tokensare also incorporated into lexicalised rules.Many of the verb phrase expansions have beenlexicalised, encoding the verb?s subcategorisation,as shown in Table 3.
Notice that each verb here ac-cepts only one or a small set of argument frames,indicating that by lexicalising the verb in the VP ex-pansion the model can find a less ambiguous andmore parsimonious grammar.The model also learns to use large rules to de-scribe the majority of root node expansions (we adda distinguished TOP node to all trees).
These rulesmostly describe cases when the S category is usedfor a full sentence, which most often include punc-tuation such as the full stop and quotation marks.
Incontrast, the majority of expansions for the S cat-egory do not include any punctuation.
The modelhas learnt to differentiate between the two differentclasses of S ?
full sentence versus internal clause ?due to their different expansions.8 ConclusionIn this work we have presented a non-parametricBayesian model for inducing tree substitution gram-mars.
By incorporating a structured prior over ele-mentary rules our model is able to reason over theinfinite space of all such rules, producing compactand simple grammars.
In doing so our model learnslocal structures for latent linguistic phenomena, suchas verb subcategorisation and lexical agreement.
Ourexperimental results show that the induced gram-mars strongly out-perform standard PCFGs, and arecomparable to a state-of-the-art parser on small datasamples.
While our results on the full treebank arewell shy of the best available parsers, we have pro-posed a number of improvements to the model andthe parsing algorithm that could lead to state-of-the-art performance in the future.ReferencesRens Bod, Remko Scha, and Khalil Sima?an, editors.2003.
Data-oriented parsing.
Center for the Study ofLanguage and Information - Studies in ComputationalLinguistics.
University of Chicago Press.Rens Bod.
2000.
Combining semantic and syntacticstructure for language modeling.
In Proceedings ofthe 6th International Conference on Spoken LanguageProcessing, Beijing, China.Rens Bod.
2003.
An efficient implementation of a newDOP model.
In Proceedings of the 10th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, Budapest, Hungary, April.555Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics, pages173?180, Ann Arbor, Michigan, June.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of 1st Meeting of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 132?139.David Chiang and Daniel M. Bikel.
2002.
Recoveringlatent information in treebanks.
In Proceedings of the19th International Conference on Computational Lin-guistics, pages 183?189, Taipei, Taiwan.John Cocke.
1969.
Programming languages and theircompilers: Preliminary notes.
Courant Institute ofMathematical Sciences, New York University.Michael John Collins.
1999.
Head-driven statisticalmodels for natural language parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia, PA.Jenny Rose Finkel, Trond Grenager, and Christopher D.Manning.
2007.
The infinite tree.
In Proceedings ofthe 45th Annual Meeting of the Association of Com-putational Linguistics, pages 272?279, Prague, CzechRepublic, June.Stuart Geman and Donald Geman.
1984.
Stochastic re-laxation, Gibbs distributions and the Bayesian restora-tion of images.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 6:721?741.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2006.
Contextual dependencies in unsupervisedword segmentation.
In Proceedings of COLING/ACL,Sydney.Joshua Goodman.
2003.
Efficient parsing of DOP withPCFG-reductions.
In Bod et al (Bod et al, 2003),chapter 8.Mark Johnson, Thomas Griffiths, and Sharon Goldwa-ter.
2007a.
Bayesian inference for PCFGs viaMarkov chain Monte Carlo.
In Proceedings of Hu-man Language Technologies 2007: The Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 139?146, Rochester,New York, April.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007b.
Adaptor grammars: A framework for spec-ifying compositional nonparametric Bayesian models.In Advances in Neural Information Processing Sys-tems 19.Mark Johnson.
1998.
PCFG models of linguistic treerepresentations.
Computational Linguistics, 24(4),December.Mark Johnson.
2002.
The DOP estimation method isbiased and inconsistent.
Computational Lingusitics,28(1):71?76, March.Aravind Joshi.
2003.
Tree adjoining grammars.
In Rus-lan Mikkov, editor, The Oxford Handbook of Computa-tional Linguistics, pages 483?501.
Oxford UniversityPress, Oxford, England.Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.2007.
The infinite PCFG using hierarchical Dirichletprocesses.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 688?697, Prague, CzechRepublic, June.Timothy J. O?Donnell, Noah D. Goodman, JesseSnedeker, and Joshua B. Tenenbaum.
2009a.
Com-putation and reuse in language.
In 31st Annual Con-ference of the Cognitive Science Society, Amsterdam,The Netherlands, July.
To appear.Timothy J. O?Donnell, Noah D. Goodman, and Joshua B.Tenenbaum.
2009b.
Fragment grammar: Exploringreuse in hierarchical generative processes.
TechnicalReport MIT-CSAIL-TR-2009-013, MIT.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Proceedings of Hu-man Language Technologies 2007: The Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 404?411, Rochester,New York, April.
Association for Computational Lin-guistics.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 433?440, Sydney, Aus-tralia, July.Detlef Prescher, Remko Scha, Khalil Sima?an, and An-dreas Zollmann.
2004.
On the statistical consistencyof dop estimators.
In Proceedings of the 14th Meet-ing of Computational Linguistics in the Netherlands,Antwerp, Belgium.Fei Xia.
2002.
Automatic grammar generation fromtwo different perspectives.
Ph.D. thesis, University ofPennsylvania.Andreas Zollmann and Khalil Sima?an.
2005.
A consis-tent and efficient estimator for data-oriented parsing.Journal of Automata, Languages and Combinatorics,10(2):367?388.Willem Zuidema.
2007.
Parsimonious data-orientedparsing.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 551?560, Prague, Czech Republic, June.556
