Fast Duplicate Document Detection using Multi-level Prefix-filterKenji Tateishi and Dai KusuiNEC CorporationTakayama, Ikoma, Nara, 630-0101, Japan{k-tateishi@bq, kusui@ct}.jp.nec.comAbstractDuplicate document detection is the problemof finding all document-pairs rapidly whosesimilarities are equal to or greater than agiven threshold.
There is a method pro-posed recently called prefix-filter that findsdocument-pairs whose similarities neverreach the threshold based on the numberof uncommon terms (words/characters) ina document-pair and removes them beforesimilarity calculation.
However, prefix-filtercannot decrease the number of similaritycalculations sufficiently because it leavesmany document-pairs whose similarities areless than the threshold.
In this paper, wepropose multi-level prefix-filter, which re-duces the number of similarity calculationsmore efficiently and maintains the advan-tage of prefix-filter (no detection loss, no ex-tra parameter) by applying multiple differentprefix-filters.1 IntroductionDuplicate Document Detection (DDD) is the prob-lem of finding all document-pairs rapidly whosesimilarities are equal to or greater than a giventhreshold.
DDD is often used for data cleaning ofcustomer databases, trend analysis of failure casedatabases in contact centers, and can be appliedfor spam filtering by detecting duplicate blog doc-uments.
After receiving target documents and thesimilarity threshold (ST), the Duplicate DocumentDetection System (DDDS) shows users all docu-ment pairs whose similarities are equal or greaterthan ST, or document groups these document pairsunify.
In the case of data cleaning, DDDS addition-ally requires users to confirm whether each docu-ment pair result is truly duplicated.The naive implementation of DDD requires simi-larity calculations of all document pairs, but it de-mands huge time according to the number of tar-get documents.
The current techniques apply thetwo-stage approach: (i) Reduce document pairs us-ing shallow filtering methods, and then (ii) calcu-late similarities between the remaining documentpairs.
Among them, prefix-filter(Sarawagi and Kir-pal, 2004)(Chaudhuri et al, 2006)(Bayardo et al,2007) is a filtering method that finds document-pairs whose similarities never reach the thresh-old based on the number of uncommon terms(words/characters) in a document-pair, and that re-moves them before similarity calculation.For example, suppose that a document pair iscomposed of 10 terms, and 80% similarity means8 terms are in common in the document pair.
In thiscase, if the similarity of a document pair is equal toor greater than 80% and 3 terms are selected fromone document, the other document must contain atleast one of the 3 terms.
Therefore, prefix-filter canremove document pairs where one document doesnot contain any of the 3 terms selected from theother.
It can be implemented rapidly by index files.Prefix-filter has two advantages compared with otherfiltering methods: (i) All document pairs equal toor greater than the similarity threshold (ST) are ob-tained without any detection loss, and (ii) no extraparameter for filtering is required other than ST.The problem with prefix-filter is that it cannot re-duce similarity calculations sufficiently because itleaves many document-pairs whose similarities areless than ST. Document-pairs that prefix-filter canremove depend on terms selected from each docu-ment (in the above example, which 3 terms are se-lected).
At worst, document pairs where only oneterm is in common might remain.
The processingtime of DDD can be approximated by the productof the number of similarity calculations and the pro-853cessing time of each similarity calculation.
In orderto identify the same document pairs correctly, a deepsimilarity function considering synonyms and vari-ants is essential.
Therefore, the number of similaritycalculations should decrease as mush as possible.In this paper, we propose multi-level prefix-filter,which reduces the number of similarity calcula-tions more efficiently and maintains the advantagesof prefix-filter (no detection loss, no extra param-eter) by applying multiple different prefix-filters.Each prefix-filter chooses terms from each docu-ment based on a different priority decision criterion,and removes different document-pairs.
It finally cal-culates the similarities of the document-pairs left byall of the prefix-filters.
We conducted an experimentwith a customer database composed of address andcompany name fields, and used edit-similarity forthe similarity calculation.
The result showed thatmulti-level prefix-filter could reduce the number ofsimilarity calculations to 1/4 compared with the cur-rent prefix-filter.2 Prefix-filterPrefix-filter finds document-pairs whose similaritiesnever reach the similarity threshold (ST) based onthe number of uncommon terms in a document-pair,and that removes them before the similarity calcu-lation.
A DDDS with prefix-filter processes the fol-lowing four steps.
1Step 1: Define x: the minimum proportion of com-mon terms in a document pair whose similarityis equal to or greater than ST (0 ?
ST ?
1).Step 2: Decide priorities of all terms on target doc-uments.Step 3: Select terms from each document accordingto the priorities in Step 2 until the proportion ofselected terms exceeds 1?
x.Step 4: Remove document pairs that share no termsselected in Step 3, and calculate the similaritiesof the remaining document pairs.Let us illustrate how prefix-filter works briefly.For example, a user inputs 6 documents as in Fig.11Here, we show the simplest prefix-filter of (Chaudhuri etal., 2006)and sets the similarity threshold at ST = 0.6 andchooses edit-similarity as the similarity function.Note that edit-similarity between document d1 anddocument d2, denoted as edit sim(d1, d2), is de-fined as follows.edit sim(d1, d2) = 1?
edit distance(d1, d2)max(|d1|, |d2|)Here, |d1| and |d2| denotes the length of d1 and d2respectively, and edit distance(d1, d2) representsthe minimum number of edit operations (insertion,deletion, and substitution) that convert d1 to d2.
Forexample, edit distance(d1, d5) in Fig.1 is 4: deleteE, H, and I, and insert M. Then, max(|d1|, |d5|) is9, derived from |d1| = 9 and |d5| = 7.
Therefore,edit sim(d1, d5) = 1?
(4/9) = 0.45.In the first step, when the similarity function isedit-similarity, the minimum proportion of commonterms (characters) in a document pair whose similar-ity is equal or greater than ST = 0.6 is x = 0.6.This means the similarity of a document pair inwhich the proportion of common terms is less than0.6 never reaches 0.6. x can be derived from thesimilarity function (see Appendix A).In step 2, DDDS decides the priorities of all termson target documents.
Fig.
1 (a) gives all terms con-tained in the 6 documents priorities from the lowestdocument frequency (if the same frequency, alpha-betical order).
Regardless of the priority decisioncriteria, the similarities of document pairs removedare always less than ST, but document pairs removeddiffer.
Empirically, it is known that giving high pri-ority from the term of the lowest frequency is effec-tive because the lower the frequency of a term, thelower the probability of a document pair containingthat term(Chaudhuri et al, 2006).In step 3, DDDS chooses terms from each docu-ment according to the priority decision criterion ofstep 2 in Fig.1 (a) until the proportion of selectedterms exceeds 1 ?
x = 0.4.
For example, the pro-portion is over 0.4 when DDDS selects 4 terms fromd1, composed of 9 terms.
DDDS selects 4 termsaccording to (a): {A,B,C, I}.
Fig.1 (b) shows se-lected terms using boldface and background color.Finally, DDDS removes document pairs that shareno terms selected in step 3, and calculates similari-ties of the remaining document pairs.
The similari-ties of document pairs with no common terms never854Figure 1: Overview of prefix-filter.reach 0.6 because the proportion of common terms isless than 0.6.
Prefix-filter can be implemented eas-ily using an index file, storing the relation of eachselected term and the list of document IDs includ-ing the term.
As a result, document d1 targets d3and d5 on similarity calculation.
Finally, the numberof similarity calculations can be reduced by 5 timeswhile naive solution requires (6*5)/2=15 times.3 Multi-level prefix-filterThe problem with prefix-filter is that it cannot re-duce similarity calculations sufficiently because itleaves many document-pairs whose similarities areless than ST. Document-pairs that prefix-filter canremove depend on terms selected from each docu-ment.
At worst, document pairs where only one termis in common might remain.
In the case of selectingterms according to priority decision criterion (a) inFig.1, for example, a document pair {d4,d6} on (b)remains although only K is in common.
In order toidentify the same document pairs correctly, a deepsimilarity function such as edit-similarity is essen-tial.
Therefore, the number of similarity calculationsshould be decreased as much as possible.We propose multi-level prefix-filter, which re-duces the number of similarity calculations more ef-ficiently by applying multiple different prefix-filters.Each prefix-filter chooses terms from each docu-ment based on different priority decision criteria,and removes different document-pairs.
It finally cal-culates the similarities of document-pairs left by allof the prefix-filters.
That is why multi-level prefix-filter can reduce the number of document pairs morecomprehensively than the current prefix-filter (with-out any detection loss).
Fig.2 illustrates an exam-ple of multi-level prefix-filter, applying prefix-filtertwice.
After DDDS changes priority decision crite-rion between the first and second prefix-filter, termsselected from each document vary.
As a result, doc-ument pairs filtered by each prefix-filter change aswell.
The product of document pairs each prefix-filter leaves leads to the reduction of similarity cal-culations by 3 times.Let us explain two kinds of priority decision cri-teria of terms in the following sections.3.1 Priority decision using Score(n,w)We define Score(n,w), the score of a term w on n-thprefix-filter, as follows, and give a higher priority toa smaller value of Score(n,w).Score(n,w) =????????
?df(w) n = 10.1 ?
df(w)+n?1?i=1sdf(i, w) n ?
2where df(w) is the document frequency of w overthe target documents, and sdf(i, w) denotes thenumber of documents in which w was selected on i-th prefix-filter.
The basic concept is to give a higherpriority to a term of smaller frequency.
As men-tioned before, this is effective because the lower thefrequency of a term, the lower the probability of adocument pair containing that term.
On the otherhand, it is expected that a multi-level prefix-filter be-comes more effective if each prefix-filter can filterdifferent document pairs.
Therefore, after the sec-ond prefix-filter (n ?
2), we give a higher prior-ity to a term whose frequency is small (first term)and which was not selected by previous prefix-filters(second term).Fig.3 illustrates the process of multi-level prefix-filter based on this creterion.
This multi-level prefix-filter can be implimented using two kinds of indexfiles (W INDEX, D INDEX) rapidly.
If PC withmultiple processers, it is easy to parallelize filteringprocess.3.2 Priority decision using Score(d, n, w)We define Score(d, n, w), the score of a term w con-tained in document d on n-th prefix-filter, as fol-855Figure 2: Overview of multi-level prefix-filter.lows, and give a higher priority to a smaller valueof Score(d, n, w).Score(d, n, w) ={df(w) n = 1|DSdn?1 ?DSSw| n ?
2where DSdn?1 is target documents of similarity cal-culation of d left after the n ?
1-th prefix-filter, andDSSw is documents containing a term w. The ba-sic concept is to give a higher priority to a term thatcan filter many document pairs.
It decides the pri-orities of terms on n-th prefix-filter after waiting forthe result of n?
1-th prefix-filter.4 Experiments4.1 Experimental methodWe compared multi-level prefix-filter with the cur-rent prefix-filter in order to clarify how much theproposed method could reduce the number of sim-ilarity calculations.
We used a customer databasein Japanese, composed of 200,000 records, and hadbeen used for data cleaning.
Each record has twofields, company name and address, averaging 11terms and 18 terms, respectively.
We selected edit-similarity as the similarity function, and set 80%as ST.
The database contains 86031 (43%) dupli-cated documents (records) in the company name,and 123068 (60%) in the address field when we as-sumed document pairs whose similarity was equalto or greater than 80%.
A DDDS with multi-levelprefix-filter ran on an NEC Express 5800 with Win-dows 2000, 2.6GHz Pentium Xeon and 3.4 GByte ofmemory.N: Number of applying prefix-filter, D: Target documents, ST: Similarity ThresholdIndex creation process:1. for(w?D)2.
Score(1,w) = df(w)3. end for4.
for(i=1; i?N; i++)5. for(j=0; j?|D|; j++)6.
W= terms chosen from w?di of the smallest Score(i,w)until the proportion of selected terms exceeds 1-x.7.
for(w?W)8. push(D_INDEX(i,dj), w)9. push(W_INDEX(i,w), dj)10. end for11.
end for12.
for(w?D)13.
Score(i+1,w)= 0.1 * df(w) + ?1?k?i sdf(k,w)14. end for15.
end forMatching process:16. for(i=0; i?|D|;i++)Filtering process:17.
DS = D18.
for(j=1; j?N; j++) {19. for(w?W_INDEX(j,di))20.
DSSw ={dk | dk?W_INDEX(j,w), k > i}21.
DSj = DSSw?
DSj22.
end for23.
DS = DS?
DSj24.
end forSimilarity calculation process:25. for(ds?DS)26. push(RESULT, {d,ds}) if (sim(d, ds)?ST)27. end for28.
end forFigure 3: Multi-level prefix-filter with Score(n,w).4.2 Experimental resultFig.4 (a) shows the comparison between multi-levelprefix-filter using Score(d, n, w) and Score(n,w)under the condition that the number of prefix-filtersis one or two.
The company name field was usedfor target documents.
Although multi-level prefix-filter using Score(n,w) succeeded in the reductionof processing time, Score(d,n,w) failed because oftoo many score calculations.
Therefore, we usedScore(n,w) in the following experiments.Fig.4 (b) shows the number of similarity calcu-lations when the number of applied prefix-filtersvaries.
In this figure, n = 1 means the cur-rent prefix-filter.
The number of similarity calcula-tions decreased most sharply in the case of applyingprefix-filters twice on both the company name andaddress fields, and converged in 10 times.
Multi-level prefix-filter reduced the number of similaritycalculations by 10 times, about to 1/4 (77% reduc-tion) in the company name field, and about to 1/3(69% reduction) in the address field.Fig.4 (c) shows total processing time when the856number of applied prefix-filters varies.
It representsthe sum of index creation/filtering time and similar-ity calculation time.
When the number of appliedprefix-filters increased, the latter decreased becausethe number of similarity calculations also decreased,but the former increased instead.
Note that we didnot parallelize the filtering process here.
Total pro-cessing time decreased most sharply in the case ofapplying prefix-filters 4 times on both the companyname (to be 43%) and address fields (to be 49%).Fig.4 (d) shows the reduction rate of the numberof similarity calculations and processing time whenprefix-filter was applied 4 times and the size of tar-get document sets varied.
Here, the reduction ratedenotes the proportion of the number of similaritycalculations or processing time of multi-level prefix-filter, applying prefix-filter 4 times, to those of thecurrent prefix-filter, applying prefix-filter once.
Thisresult reveals the effectiveness of multi-level prefix-filter does not change for the size of the target docu-ment set.4.3 DiscussionThe experimental results indicated that multi-levelprefix-filter could reduce the number of similaritycalculations up to 1/4, and that this effectiveness wasnot lost by changing the size of the target database.In addition, it showed that the optimal number of ap-plied prefix-filters did not depend on the target fieldor the size of the target database.
Therefore, multi-level prefix-filter proved to be more effective thanthe current prefix-filter without losing the advan-tages of the current prefix-filter (no detection loss,no extra parameter).The experimental results also indicated that thecompany name field was more effective than the ad-dress field.
As mentioned, the address field waslonger than that of the company name field on av-erage, and it contained more duplicated documents.Therefore, we expect that the proposed method iseffective in the following situation: (i) the lengthof each document (record) is short, (ii) the num-ber of duplicate documents has been reduced before-hand by simple filtering methods such as deletingexact match documents or documents different onlyin space, and (iii) detecting the remaining duplicatedocuments by using a deep similarity function suchas edit-similarity.5 Related workDuplicate Document Detection for databases hasbeen researched for a long time(Elmagarmid et al,2007).
The current techniques apply the two-stageapproach: (i) Reduce document pairs using shallowfiltering methods, and then (ii) calculate similaritybetween the remaining document pairs.
Multi-levelprefix-filter belongs to the first step (i).Current filtering methods were independent of thesimilarity function.
Jaro(Jaro, 1989) proposed Stan-dard Blocking, which created many record blocks inwhich each record shared the same first n terms, andcalculated the similarity of document-pairs includedin the same record block.
Hernandez(Hernandezand Stolfo, 1995) proposed the Sorted Neighbor-hood Method (SNM), which first sorted records bya given key function, and then grouped adjacentrecords within the given window size as a block.McCallum(McCallum et al, 2000) improved themby allowing a record to locate in plural blocks in or-der to avoid detection loss.However, the problems of these filtering methodsusing blocking are that the user needs trial and errorparameters such as first n terms for Standard Block-ing, and that these incur detection loss in spite ofimprovements being attempted, caused by two doc-uments of a correct document pair existing in dif-ferent blocks.
Prefix-filter solved these problems:(i) all document pairs equal or more than similar-ity threshold (ST) are obtained without any detectionloss, and (ii) any extra parameter for filtering is notrequired other than ST. As we clarified in Section 4,multi-level prefix-filter proved to be more effectivethan the current prefix-filter without losing these ad-vantages.Another filtering method without any detectionloss, called PARTENUM, has been proposed re-cently(Arasu et al, 2006).
However, it needs to ad-just two kinds of parameters (n1, n2) for obtainingoptimal processing time according to the size of tar-get document set or the similarity threshold.6 ConclusionIn this paper, we proposed multi-level prefix-filter,which reduces the number of similarity calculationsmore efficiently and maintains the advantage of thecurrent prefix-filter by applying multiple different857050001000015000200002500030000350004000045000n=1n=2n=3n=4n=5n=6n=7n=8n=9n=10The number of applied prefix-filtersSimularitycalculations[x10000]Company name0100002000030000400005000060000700008000090000100000n=1n=2n=3n=4n=5n=6n=7n=8n=9n=10The number of applied prefix-filtersSimilaritycalculations[x10000]Address020040060080010001200140016001800n=1 n=2 n=3 n=4 n=5 n=6Company name fieldProcessingtime[sec]Similarity calculationIndex creation and filtering0100020003000400050006000n=1 n=2 n=3 n=4 n=5 n=6Address fieldProcessingtime[sec]Similarity calculationIndex creation and filtering00.20.40.60.81100000 200000 300000 400000Document size (Company name field)ReductionrateProcessing timeThe number of similarity calculations00.20.40.60.81100000 200000 300000 400000Document size (Address field)ReductionrateProcessing timeThe number of similarity calclulations05001000150020002500300035004000score(d, n, w)n=1score(d, n ,w)n=2Processingtime[sec]Similarity calculationIndex creation and filtering05001000150020002500300035004000score(n, w)n=1score(n, w)n=2Processingtime[sec]Similarity calculationIndex creation and filteringFigure 4: Experimental result.prefix-filters.
Experiments with a customer databasecomposed of 200,000 documents and edit-distancefor similarity calculation showed that it could reducethe number of similarity calculations to 1/4 com-pared with the current prefix-filter.ReferencesArvind Arasu, Venkatesh Ganti, and Raghav Kaushik.2006.
Efficient exact set-similarity joins.
Proceedingsof the 32nd International Conference on Very LargeData Bases, pages 918?929.Roberto J. Bayardo, Yiming Ma, and RamakrishnanSrikant.
2007.
Scaling up all pairs similarity search.Proceedings of the 16th International Conference onWorld Wide Web, pages 131?140.Surajit Chaudhuri, Venkatesh Ganti, and RaghavKaushik.
2006.
A primitive operator for similarityjoins in data cleaning.
Proceedings of the 22nd Inter-national Conference on Data Engineering(ICDE?06),pages 5?16.Ahmed K. Elmagarmid, Panagiotis G. Ipeirotis, and Vas-silios S. Verykios.
2007.
Duplicate record detection:A survey.
IEEE Transactions on Knowledge and DataEngineering, vol.19, no.1, pages 1?15.Mauricio A. Hernandez and Salvatore J. Stolfo.
1995.The merge/purge problem for large databases.
Pro-ceedings of the 1995 ACM SIGMOD internationalconference on Management of data, pages 127?138.M.
A. Jaro.
1989.
Advances in record linkage methodol-ogy as applied to matching the 1985 census of tampa,florida.
Journal of the American Statistical Society, 84(406), pages 414?420.Andrew McCallum, Kamal Nigam, and Lyle H. Ungar.2000.
Efficient clustering of high-dimensional datasets with application to reference matching.
Proceed-ings of the sixth ACM SIGKDD international confer-ence on Knowledge discovery and data mining, pages169?178.Sunita Sarawagi and Alok Kirpal.
2004.
Efficient setjoins on similarity predicates.
Proceedings of the 2004ACM SIGMOD international conference on Manage-ment of data, pages 743?754.A The minimum proportion of commontermsHere, we explain how to obtain x of edit-similarity.First,edit distance(d1, d2) ?
max(|d1|, |d2|)?|d1?d2|(|d1 ?
d2| denotes the number of common terms inboth d1 and d2), andST ?
edit sim(d1, d2) ?
|d1 ?
d2|max(|d1|, |d2|)?
|d1 ?
d2||d1|.Therefore,x = min{|d1 ?
d2||d1|} = ST.858
