Using Argumentation to Retrieve Articles with Similar Citations fromMEDLINEImad Tbahriti1,2, Christine Chichester1, Fr?d?rique Lisacek1,3, Patrick Ruch41Geneva Bioinformatics (GeneBio) SA, 25, avenue de Champel, Geneva2Computer Science Dept., University of Geneva3Swiss Institute of Bioinformatics, Geneva4SIM, University Hospital of Geneva{imad.tbahriti;christine.chichester;frederique.lisacek}@genebio.com - patrick.ruch@epfl.chAbstractThe aim of this study is to investigate therelationships between citations and thescientific argumentation found in the abstract.We extracted citation lists from a set of 3200full-text papers originating from a narrowdomain.
In parallel, we recovered thecorresponding MEDLINE records for analysisof the argumentative moves.
Ourargumentative model is founded on fourclasses: PURPOSE, METHODS, RESULTS,and CONCLUSION.
A Bayesian classifiertrained on explicitly structured MEDLINEabstracts generates these argumentativecategories.
The categories are used to generatefour different argumentative indexes.
A fifthindex contains the complete abstract, togetherwith the title and the list of Medical SubjectHeadings (MeSH) terms.
To appraise therelationship of the moves to the citations, thecitation lists were used as the criteria fordetermining relatedness of articles,establishing a benchmark.
Our results showthat the average precision of queries with thePURPOSE and CONCLUSION features is thehighest, while the precision of the RESULTSand METHODS features was relatively low.
Alinear weighting combination of the moves isproposed, which significantly improvesretrieval of related articles.1 IntroductionNumerous techniques help researchers locaterelevant documents in an ever-growing mountainof scientific publications.
Among these techniquesis the analysis of bibliographic information, whichcan identify conceptual connections between largenumbers of articles.
Although helpful, most ofthese systems deliver masses of documents to theresearcher for analysis, which contain variousdegrees of similarity.
This paper introduces amethod to determine the similarity of abibliographic co-citation list, that is the list ofcitations that are shared between articles, and theargumentative moves of an abstract in an effort todefine novel similarity searches.Authors of biological papers develop argumentsand present the justification for their experimentsbased on previously documented results.
Theseresults are represented as citations to earlierscientific literature and establish the links betweenold and new findings.
The assumption is that themajority of scientific papers employing the samecitations depict related viewpoints.
The methoddescribed here is applied to improve retrieval ofsimilar articles based on co-citations, but otherapplications are foreseen.
Documents that shouldbe conceptually correlated due to bibliographicrelatedness but which propose different or novelarguments are often not easily located in themajority of bibliographically correlated articles.Our system can be tuned to identify thesedocuments.
Conversely, such a system could alsobe used as a platform to aid authors by means ofautomatic assembly or refinement of theirbibliographies through the suggestion of citationscoming from documents containing similararguments.The rest of this paper is structured as follows:section 2 describes the background related toexperiments using citations or argumentation thatcompare aspects connected to the logical contentof publications.
Section 3 details the method andthe generation of the different indexes used in ouranalyses, e.g.
the citation index, the fourargumentative indexes and the abstract index(abstract, title and keywords).
Section 4 presentsthe results of the evaluations we performed.Section 5 closes with a summary of thecontribution of this work, limitations and futurework.2 BackgroundDigital libraries aim at structuring their records tofacilitate user navigation.
Interfaces visualizing8overlapping relationships of the standard libraryfields such as author and title in documentcollections are usually the most accessible to theuser.
Beyond these well-known targets, researchers(see de Bruijn and Martin, 2002, or Hirschman andal.
2002, for a survey) interested in informationextraction and retrieval for biomedical applicationshave mostly focused on studying specificbiological interactions (Stapley and Benoit, 2000;N?dellec et al, 2002; Dobrokhotov et al, 2003)and related entities (Collier et al, 2000;Humphreys et al, 2000; Yu et al, 2002;Yamamoto et al, 2003; Albert et al, 2003) orusing terms in biomedical vocabularies (Nazarenkoet al, 2001; Ruch et al, 2004; Srinivasan andHristovski, 2004).
The use of bibliographical andargumentative information (McKnight andSrinivasan 2003) has been less well studied byresearchers interested in applying natural languageprocessing to biomedical texts.2.1 CitationsOriginating from bibliometrics, citation analysis(White, 2003) has been used to visualize a field viaa representative slice of its literature.
Co-citationtechniques make it possible to cluster documentsby scientific paradigm or hypothesis (Noyons etal., 1999).
Braam et al, (1991) have investigatedco-citation as a tool to map subject-matterspecialties.
They found that the combination ofkeyword analysis and co-citation analysis wasuseful in revealing the cognitive content ofpublications.
Peters et al, (1995) further exploredthe citation relationships and the cognitiveresemblance in scientific articles.
Word profilesimilarities of publications that werebibliographically coupled by a single, highly citedarticle were compared with publications that werenot bibliographically coupled to that specificarticle.
A statistically significant relationship hasbeen established between the content of articlesand their shared citations.
This result will serve asbasis to establish our benchmark without relevancejudgments (Wu and Crestani, 2003; Soborrof et al,2001).2.2 Argumentation in biomedical abstractsScientific research is often described as a problemsolving activity.
In full text scientific articles thisproblem-solution structure has been crystallized ina fixed presentation known as Introduction,Methods, Results and Conclusion.
This structure isoften presented in a much-compacted version inthe abstract and it has been clearly demonstratedby Schuemie et al, (2004) that abstracts contain ahigher information density than full text.Correspondingly, the 4-move problem-solvingstructure (standardized according to ISO/ANSIguidelines) has been found quite stable in scientificreports (Orasan, 2001).
Although theargumentative structure of an article is not alwaysexplicitly labeled, or can be labeled using slightlydifferent markers (as seen in Figure 1), a similarimplicit structure is common in most biomedicalabstracts (Swales, 1990).
Therefore, to find themost relevant argumentative status that describesthe content of the article, we employed aclassification method to separate the content densesentences of the abstracts into the argumentativemoves.INTRODUCTION: Chromophobe renal cellcarcinoma (CCRC) comprises 5% of neoplasms ofrenal tubular epithelium.
CCRC may have a slightlybetter prognosis than clear cell carcinoma, butoutcome data are limited.
PURPOSE: In this study,we analyzed 250 renal cell carcinomas to a)determine frequency of CCRC at our Hospital and b)analyze clinical and pathologic features of CCRCs.METHODS: A total of 250 renal carcinomas wereanalyzed between March 1990 and March 1999.Tumors were classified according to well-establishedhistologic criteria to determine stage of disease; thesystem proposed by Robson was used.
RESULTS: Of250 renal cell carcinomas analyzed, 36 wereclassified as chromophobe renal cell carcinoma,representing 14% of the group studied.
The tumorshad an average diameter of 14 cm.
Robson stagingwas possible in all cases, and 10 patients were stage1) 11 stage II; 10 stage III, and five stage IV.
Theaverage follow-up period was 4 years and 18 (53%)patients were alive without disease.
CONCLUSION:The highly favorable pathologic stage (RI-RII, 58%)and the fact that the majority of patients were aliveand disease-free suggested a more favorableprognosis for this type of renal cell carcinoma.Figure 1: Example of an explicitly structured abstract inMEDLINE.
The 4-class argumentation model issometimes split into classes that may carry slightlydifferent names, as illustrated in this example by theINTRODUCTION marker.3 MethodsWe established a benchmark based on citationanalysis to evaluate the impact of usingargumentation to find related articles.
Ininformation retrieval, benchmarks are developedfrom three resources: a document collection, aquery collection and a set of relevance rankingsthat relates each query to the set of documents.Existing information retrieval collections normallycontain user queries composed of only a fewwords.
These short queries are not suitable forevaluating a system tailored to retrieve articleswith similar citations.
Therefore, we have createdthe collection and tuned the system to accept longqueries such as abstracts (Figure 2).9Figure 2:Flowchart for the chain of experimental procedures.
The benchmark was assembled fromcitations shared between documents and compared to the document similarity ranking of EasyIR.3.1 Data acquisition and citation indexingAll the data used in these experiments wereacquired from MEDLINE using the PubMedinterface.Document Collection..
The document set wasobtained from PubMed by executing a set ofBoolean queries to recover articles related to smallactive peptides from many animal speciesexcluding humans.
These peptides hold thepromise of becoming novel therapeutics.
The setconsisted of 12500 documents, which werecomprised of abstract, title and MeSH terms.
For3200 of these documents we were able to recoverthe full text including the references for citationextraction and analysis.Queries..
Following statistical analysis confirmedby Buckley and Voorhees (2000), four sets of 25articles were selected from the 3200 full textarticles.
The title, abstract and MeSH terms fieldswere used to construct the queries.
For testing theinfluence the argumentative move, the specificsentences were extracted and tested either alone orin combination with the queries that contained thetitle, abstract and MeSH terms.Citation analysis.. Citation lists wereautomatically extracted from 3200 full-text articlesthat were correspondingly represented within thedocument set.
This automatic parsing of citationswas manually validated.
Each citation wasrepresented as a unique ID for comparisonpurposes.
Citation analysis of the entire collectiondemonstrated that the full-text articles possessed amean citation count of 28.30 + 24.15  (mean +S.D.)
with a 95% CI  = 27.47 ?
29.13.
Withinthese records the mean co-citation count was 7.79+ 6.99 (mean + S.D.)
with a 95% CI  = 7.55 ?
8.03.As would be expected in a document set whichcontains a variety of document types (reviews,journal articles, editorials), the standard deviationsof these values are quite large.Citation benchmark.. For each set of queries, abenchmark was generated from the 10 citedarticles that contained the greatest number of co-citations in common with the query.
For thebenchmark, the average number of cited articlesthat have more than 9 co-citations was 15.70+ 6.58 (mean + S.D.).
Query sets were checked toconfirm that at least one sentence in each abstractwas classified per argumentative class.3.2 MetricsThe main measure for assessing informationretrieval engines is mean average precision (MAP).MAP is the standard metric although it may tend tohide minor differences in ranking (Mittendorf andSch?uble, 1996).3.3 Text indexingFor indexing, we used the easyIR system1, whichimplements standard vector space IR schemes(Salton et al, 1983).
The term-weighting schema1 http://lithwww.epfl.ch/~ruch/softs/softs.html.10composed of combinations of term frequency,inverse document frequency and lengthnormalization was varied to determine the mostrelevant output ranking.
Table 1 gives the mostcommon term weighting factors (atc.atn, ltc.atn);the first letter triplet applies to the document, thesecond letter triplet applies to the query (Ruch,2002).Table 1.
Weighting parameters, following SMARTconventions.3.4 Argumentative classificationThe classifier segmented the abstracts into 4argumentative moves: PURPOSE, METHODS,RESULTS, and CONCLUSION.Figure 3: The classification results for the abstract shownin Figure 1.
In each box, the attributed class is first,followed by the score for the class, followed by theextracted text segment.
In this example, one of RESULTSsentences is misclassified as METHODSThe classification unit is the sentence which meansthat abstracts are preprocessed using an ad hocsentence splitter.
The confusion matrix for the fourargumentative moves generated by the classifier isgiven in Table 2.
This evaluation used explicitlystructured abstracts; therefore, the argumentativemarkers were removed prior to the evaluation.Figure 3 shows the output from the classifier, whenapplied to the abstract shown in Figure 1.
Afterextraction, each of the four types of argumentativemoves was then used for indexing, retrieval andcomparison tasks.Table 2.
Confusion matrices for each argumentative class.PURP METH RESU CONCPURP 93.55% 0% 3.23% 3%METH 8% 81% 8% 3%RESU 7.43% 5.31% 74.25% 13.01%CONC 2.27% 0% 2.27% 95.45%3.5 Argumentative combinationWe adjusted the weight of the four argumentativemoves, based on their location and then combinedthem to improve retrieval effectiveness.
The queryweights were recomputed as indicated in equation(1).Wnew = Wold * Sc * kc (1)c ?
{PURPOSE; METHODS; RESULTS;CONCLUSION}Wold: the feature weight as given by the queryweighting (ltc)S: the normalized score attributed by theargumentative classifier to each sentence in theabstract.
This score is attributed to each featureappearing in the considered segmentCONCLUSION |00160116| The highly favorable pathologicstage (RI-RII, 58%) and the fact that the majority ofpatients were alive and disease-free suggested a morefavorable prognosis for this type of renal cell carcinoma.METHODS |00160119| Tumors were classified according towell-established histologic criteria to determine stage ofdisease; the system proposed by Robson was used.METHODS |00162303| Of 250 renal cell carcinomasanalyzed, 36 were classified as chromophobe renal cellcarcinoma, representing 14% of the group studied.PURPOSE |00156456| In this study, we analyzed 250 renalcell carcinomas to a) determine frequency of CCRC at ourHospital and b) analyze clinical and pathologic features ofCCRCs.PURPOSE |00167817| Chromophobe renal cell carcinoma(CCRC) comprises 5% of neoplasms of renal tubularepithelium.
CCRC may have a slightly better prognosisthan clear cell carcinoma, but outcome data are limited.RESULTS |00155338| Robson staging was possible in allcases, and 10 patients were stage 1) 11 stage II; 10 stageIII, and five stage IV.k: a constant for each value of c. The value is setempirically using the tuning set (TS).
The initialvalue of k for each category is given by thedistribution observed in Table 4 (i.e., 0.625, 0.164,0.176, 0.560 for the classes, PURPOSE,METHODS, RESULTS and CONCLUSIONrespectively), and then an increment step (positiveand negative) is varied to get the most optimalcombination.This equation combines the score (Sc) attributed bythe original weighting (ltc) for each feature (Wold)found in the query with a boosting factor (kc).
Theboosting factor was derived from the scoreprovided by the argumentative classifier for eachclassified sentence.
For these experiments, theparameters were determined with a tuning set (TS),one of the four query sets, and the final evaluationwas done using the remaining three sets, thevalidation sets (VS).
The document feature factor(atn) remained unchanged.4 ResultsIn this section, we described the generation of thebaseline measure and the effects of differentconditions on this baseline.114.1 Comparison of text index parametersThe use of a domain specific thesaurus tends toimprove the MAP when compared to the citationbenchmark, 0.1528 vs. 0.1517 for ltc.atn and0.1452 vs. 0.1433 for atc.atn (Table 3).
The ltc.atnweighting schema in combination with thethesaurus produced the best results, therefore theseparameters were more likely to retrieve abstractsfound in the citation index and thus were used forall subsequent experiments.Table 3.
Mean average precision (MAP) for each query set(1,2,3, and 4) with different term weighting schemas.
Thelast column gives the average MAP.
T represents thethesaurus4.2 Argumentation-based retrievalFor demonstrating that argumentative features canimprove document retrieval, we first determinedwhich argumentative class was the most contentbearing.
Subsequently, we combined the fourargumentative classes to again improve documentretrieval.Table 4.
MAP results from querying the collection usingonly the argumentative move.To determine the value of each argumentativemove in the retrieval, the argumentativecategorizer first parses each query abstract,generating four groups each representing a uniqueargumentative class.
The document collection wasseparately queried with each group.
Table 4 givesthe MAP measures for each type of argumentation.Table 4 shows the sentences classified asPURPOSE provide the most useful content toretrieve similar documents.
Baseline precision of62.5% is achieved when using only this section ofthe abstract.
The CONCLUSION move is thesecond most valuable at 56% of the baseline.
TheMETHODS and RESULTS sections appear lesscontent bearing for retrieving similar documents,16.4% and 17.6%, respectively, of the baseline.Each argumentative set represents roughly aquarter of the textual content of the originalabstract.
Querying with the PURPOSE section,(25% of the available textual material) realizesalmost 2/3 of the average precision and for theCONCLUSION section, it is more than 50% of thebaseline precision.
In information retrieval queriesand documents are often seen as symmetricalelements.
This fact may imply the possible use ofthe argumentative moves as a technique to reducethe size of the indexed document collection or tohelp indexing pruning in large repositories (Carmeland al.
2001).4.3 Argumentative overweightingAs implied in Table 4, Table 5 confirms thatoverweighting the features of PURPOSE andCONCLUSION sentences results in a gain inaverage precision (respectively +3.39% and +3.98for CONCLUSION and PURPOSE) as measuredby citation similarity.
More specifically, Table 5demonstrates the use of PURPOSE andCONCLUSION as follows:Set 1 Set 2 Set 3 Set 4 Averageatc.atn 0.1402 0.1417 0.1438 0.1476 0.1433atc.atn + T 0.1440 0.1431 0.1477 0.1465 0.1452ltc.atn 0.1505 0.1528 0.1506 0.1529 0.1517ltc.atn + T 0.1524 0.1534 0.1530 0.1539 0.1532  ?
PURPOSE applies a boosting coefficient tofeatures classified as PURPOSE by theargumentative classifier;?
CONCLUSION applies a boosting coefficientto features classified as CONCLUSION by theargumentative classifier;?
COMBINATION applies two differentboosting coefficients to features classified asCONCLUSION and PURPOSE by theargumentative classifier.The results, in Table 5, from boosting PURPOSEand CONCLUSION features are given alongsidethe MAP and show an improvement of precision atthe 5 and 10 document level.
At the 5-documentlevel the advantage is with the PURPOSE features,but at the 10-document level boosting theCONCLUSION features is more effective.
Whilethe improvement brought by boosting PURPOSEand CONCLUSION features, when measured byMAP is modest (3-4%), the improvement observedby their optimal combination reached a significantimprovement: + 5.48%.
The various combinationsof RESULTS and METHODS sections did notlead to any improvement.PURP METH RESU CONC ltc.atn + TMAP 0.0958 (62.5%)0.0251(16.4%)0.0270(17.6%)0.0858(56.0%) 0.1532Argumentation has typically been studied inrelation to summarization (Teufel and Moens,2002).
Its impact on information retrieval is moredifficult to establish although recent experiments(Ruch et al, 2003) tend to confirm thatargumentation is useful for information extraction,as demonstrated by the extraction of genefunctions for LocusLink curation.
Similarly, usingthe argumentative structure of scientific articleshas been proposed to reduce noise (Camon et al,2004) in the assignment of Gene Ontology codesas investigated in the BioCreative challenge.
Inparticular, it was seen that the use of ?Material andMethods?
sentences should be avoided.
A factwhich is confirmed by our results with theMETHOD argumentative move.12Table 5.
Retrieval results for the argumentative classesPURPOSE and CONCLUSION, and the combination ofboth classes.5 Conclusion and Future workWe have reported on the construction of aninformation retrieval engine tailored to search fordocuments with similar citations in MEDLINEcollections.
The tool retrieves similar documentsby giving more weight to features located inPURPOSE and CONCLUSION segments.
TheRESULTS and METHODS argumentative movesare reported here as less useful for such a retrievaltask.
Evaluated on a citation benchmark, thesystem significantly improves retrievaleffectiveness of a standard vector-space engine.
Inthis context, it would be interesting to investigatehow argumentation can be beneficial to perform adhoc retrieval tasks in MEDLINE (Kayaalp et al,2003).Evidently using citation information to build ourbenchmark raises some questions.
Authors mayrefer to other work in many ways to benefit thetone of their argument.
Specifically, there are twomajor citation contexts, one where an article iscited negatively or contrastively and one where anarticle is cited positively, or the authors state thattheir own work originates from the cited work.
Inthis study we have not made a distinction betweenthese contexts but we consider this as an avenuefor building better representations of the citedarticles in future work.
Finally, we are nowexploring the use of the tool to detectinconsistencies between articles.
We hope to usecitation and content analysis to identify articlescontaining novel views so as to expose differencesin the consensus of the research area?s intellectualfocus.
The idea is to retrieve documents havingkey citation similarity but show some dissimilarityregarding a given argumentative category.Finally, we have observed that citation networks indigital libraries are analogous to hyperlinks in webrepositories.
Consequently using web-inspiredsimilarity measures may be beneficial for ourpurposes.
Of particular interest in relation toargumentation, is the fact that citations networks,like web pages, are hierarchically nested graphwith argumentative moves introducingintermediate levels (Bharat et al, 2001).MAP Precision at 5Precisionat 10ltc.atn + T 0.1532 0.2080 0.1840PURPOSE 0.1593 (+3.98%) 0.2240 0.1760CONCLUSION 0.1584 (+3.39%) 0.2160 0.1920COMBINATION 0.1616 (+5.48%) 0.2320 0.1960AcknowledgementsWe would like to thank Patrick Brechbiehl for hisassistance in organizing the computingenvironment and Ron Appel for his support.ReferencesS.
Albert, S. Gaudan, H. Knigge, A. Raetsch, A.Delgado, B. Huhse, H. Kirsch, M. Albers, D.Rebholz-Schuhmann and M. Koegl.
2003.Computer-assisted generation of a protein-interaction database for nuclear receptors.Journal of Molecular Endocrinology, 17(8):1555-1567.K.
Bharat, B. Chang, M. Rauch Henzinger, M.Ruhl: Who Links to Whom: Mining Linkagebetween Web Sites.
ICDM 2001: 51-58R.
R. Braam, H.F. Moed, and A.F.J.
van Raan.1991 Mapping of science by combined co-citation and word analysis, I: Structural Aspects,Journal of the American Society for InformationScience, 42 (4): 233-251.C.
Buckley and E. M. Voorhees.
2000.
Evaluatingevaluation measure stability, ACM SIGIR, p. 33-40.E.
Camon et al Personnal communication onBioCreative Task 2 Evaluation.
BMCBioinformatics Special Issue onBioCreative.2004.
To be submitted.D.
Carmel, E. Amitay, M. Herscovici, Y. Maarek,Y.
Petruschka and A. Soffer: Juru at TREC 10 -Experiments with Index Pruning.
TREC 2001N.
Collier, C. Nobata and  J.I.
Tsujii.
2000.Extracting the Names of Genes and GeneProducts with a Hidden Markov Model.COLING 2000.
201-207.B.
de Bruijn and J. Martin.
Getting to the (c)ore ofknowledge: mining biomedical literature.
2002.In International Journal of Medical Informatics,P Ruch and R Baud, eds., pages 7-18, Volume67, Issues 1-3, 4 , p. 7-18P.
B. Dobrokhotov, C. Goutte, A. L. Veuthey, and?.
Gaussier: Combining NLP and probabilisticcategorisation for document and term selectionfor Swiss-Prot medical annotation.
2003.
ISMB2003, 91-94.W.
Hersh, S. Moy, D. Kraemer, L. Sacherek andD.
Olson.
2003.
More Statistical Power Needed:The OHSU TREC 2002 Interactive TrackExperiments, TREC 2002.13L Hirschman, JC Park, JI Tsujii, L Wong, C Wu:Accomplishments and challenges in literaturedata mining for biology.
Bioinformatics 18(12):1553-1561 (2002)K. Humphreys, G. Demetriou and R. Gaizauskas.2000.
Two Applications of InformationExtraction to Biological Science JournalArticles: Enzyme Interactions and ProteinStructures In Proceedings of the Workshop onNatural Language Processing for Biology, heldat the Pacific Symposium on Biocomputing(PSB2000).M.
Kayaalp, A.R.
Aronson, S.M.
Humphrey, N.C.Ide, L.K.
Tanabe, L.H.
Smith, D. Demner, R.R.Loane, J.G.
Mork, and O. Bodenrieder.
2003.Methods for accurate retrieval of MEDLINEcitations in functional genomics.
In Notebook ofthe TREC-2003, pages 175-184, Gaithersburg,MD.L.
McKnight and P. Srinivasan.
2003.Categorization of Sentence Types in MedicalAbstracts.
Proceedings of the 2003 AMIAconference.H.
Mima,  S. Ananiadou, G. Nenadic, and J. Tsujii.A methodology for terminology-based knowledgeacquisition and integration, 2002.
COLING.Morgan Kaufmann.E Mittendorf and P Sch?uble.
1996.
Measuring theeffects of data corruption on informationretrieval.
SDAIR Proceedings.A.
Nazarenko, P. Zweigenbaum, B. Habert and J.Bouaud.
2001.
Corpus-based Extension of aTerminological Semantic Lexicon, RecentAdvances in Computational Terminology.
JohnBenjamins,  2001.C.
N?dellec, M. Vetah and P. Bessi?res.
2001.Sentence filtering for information extraction ingenomics, a classification problem.
InProceedings PKDD, pages 326-237, Springer-Verlag, Berlin.E.C.M.
Noyons, H.F. Moed, and M. Luwel.
1999.A Bibliometric Study Combining Mapping andCitation Analysis for Evaluative BibliometricPurposes.
Journal of the American Society forInformation.
Science, 50(2):115-131.C.
Orasan.
2001.
Patterns in scientific abstracts.In Proceedings of Corpus Linguistics, 433-445.H.
P. F. Peters, R.R.
Braam, and  A.F.J.
van Raan.1995.
Cognitive Resemblance and CitationRelations in Chemical Engineering Publications.Journal of the American Society for InformationScience, 46 (1): 9-21.J.C.
Reynar and A. Ratnaparkhi.. 1997.
Amaximum entropy approach to identifyingsentence boundaries.
In Proceedings of the FifthConference on Applied Natural LanguageProcessing, 16-19.P.
Ruch.
2002.
Using Contextual SpellingCorrection to Improve Retrieval Effectiveness inDegraded Text Collections.
COLING 2002.Morgan Kaufmann.P.
Ruch, R. Baud and A. Geissb?hler.
2003.Learning-free Text Categorization, AIME, MDojat, E Keravnou and P Barahona (Eds.).
199-208, LNAI 2780.
Springer.P.
Ruch, C Chichester, G Cohen, G Coray, FEhrler, H Ghorbel, H M?ller, and V Pallotta.2004.
Report on the TREC 2003 Experiment:Genomic Track, TREC.M.J.
Schuemie, M. Weeber, B.J.A Schijvenaars,E.M.
van Mulligen, C.C.
van der Eijk, R. JeliertB.
Mons, and J.
A. Kors.
2004.
Distribution ofinformation in biomedical abstracts and full textpublications.
Bioinformatics.
Submitted.I Soborrof, C. Nicholas and P. Cahan.
2001.Ranking Retrieval Systems without RelevanceJudgments.
SIGIR 2001: 66-73P.
Srinivasan and D. Hristovski.
2004.
DistillingConceptual Connections from MeSH Co-Occurrences.
MEDINFO 2004.
Submitted.B Stapley and G Benoir.
2000.
BioBibliometrics:information retrieval and visualisation from co-occurrences of gene names in MEDLINEabstracts.
Pac.
Symp.
Biocomp.
5:526-537.J.
Swales.
1990 Genre analysis: English inacademic and research settings.
CambridgeUniversity Press, UK.S.
Teufel and M. Moens: Summarizing ScientificArticles: Experiments with Relevance andRhetorical Status.
Computational Linguistics28(4): 409-445 (2002)H. White.
2003.
Pathfinder networks and authorcocitation analysis: a remapping of paradigmaticinformation scientists.
J.
Am.
Soc.
Inf.
Sci.Technol 54(5) 423-434.S.
Wu and F. Crestani.
2003.
Methods for RankingInformation Retrieval Systems WithoutRelevance Judgments.
SAC 2003: 811-816.ACM.K.
Yamamoto, T. Kudo, A. Konagaya and Y.Matsumoto.
2003.
Protein name tagging forbiomedical annotation in text.
ACL Workshopon Natural Language Processing in Biomedicine,pp.
65-72, July 2003.H.
Yu, V. Hatzivassiloglou, C. Friedman, I.H.Iossifov, A. Rzhetsky and W.J.
Wilbur.
2002.
Arule-based approach for automaticallyidentifying gene and protein names in MEDLINEabstracts: A proposal.
ISMB 2002.14
