Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 238?247,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsDon?t count, predict!
A systematic comparison ofcontext-counting vs. context-predicting semantic vectorsMarco Baroni and Georgiana Dinu and Germ?an KruszewskiCenter for Mind/Brain Sciences (University of Trento, Italy)(marco.baroni|georgiana.dinu|german.kruszewski)@unitn.itAbstractContext-predicting models (more com-monly known as embeddings or neurallanguage models) are the new kids on thedistributional semantics block.
Despite thebuzz surrounding these models, the litera-ture is still lacking a systematic compari-son of the predictive models with classic,count-vector-based distributional semanticapproaches.
In this paper, we performsuch an extensive evaluation, on a widerange of lexical semantics tasks and acrossmany parameter settings.
The results, toour own surprise, show that the buzz isfully justified, as the context-predictingmodels obtain a thorough and resoundingvictory against their count-based counter-parts.1 IntroductionA long tradition in computational linguistics hasshown that contextual information provides a goodapproximation to word meaning, since semanti-cally similar words tend to have similar contex-tual distributions (Miller and Charles, 1991).
Inconcrete, distributional semantic models (DSMs)use vectors that keep track of the contexts (e.g.,co-occurring words) in which target terms appearin a large corpus as proxies for meaning represen-tations, and apply geometric techniques to thesevectors to measure the similarity in meaning ofthe corresponding words (Clark, 2013; Erk, 2012;Turney and Pantel, 2010).It has been clear for decades now that raw co-occurrence counts don?t work that well, and DSMsachieve much higher performance when varioustransformations are applied to the raw vectors,for example by reweighting the counts for con-text informativeness and smoothing them with di-mensionality reduction techniques.
This vectoroptimization process is generally unsupervised,and based on independent considerations (for ex-ample, context reweighting is often justified byinformation-theoretic considerations, dimension-ality reduction optimizes the amount of preservedvariance, etc.).
Occasionally, some kind of indi-rect supervision is used: Several parameter set-tings are tried, and the best setting is chosen basedon performance on a semantic task that has beenselected for tuning.The last few years have seen the developmentof a new generation of DSMs that frame the vec-tor estimation problem directly as a supervisedtask, where the weights in a word vector are set tomaximize the probability of the contexts in whichthe word is observed in the corpus (Bengio et al,2003; Collobert and Weston, 2008; Collobert etal., 2011; Huang et al, 2012; Mikolov et al,2013a; Turian et al, 2010).
The traditional con-struction of context vectors is turned on its head:Instead of first collecting context vectors and thenreweighting these vectors based on various crite-ria, the vector weights are directly set to optimallypredict the contexts in which the correspondingwords tend to appear.
Since similar words occurin similar contexts, the system naturally learns toassign similar vectors to similar words.This new way to train DSMs is attractive be-cause it replaces the essentially heuristic stackingof vector transforms in earlier models with a sin-gle, well-defined supervised learning step.
At thesame time, supervision comes at no manual anno-tation cost, given that the context windows usedfor training can be automatically extracted froman unannotated corpus (indeed, they are the verysame data used to build traditional DSMs).
More-over, at least some of the relevant methods can ef-ficiently scale up to process very large amounts ofinput data.11The idea to directly learn a parameter vector based onan objective optimum function is shared by Latent Dirichlet238We will refer to DSMs built in the traditionalway as count models (since they initialize vectorswith co-occurrence counts), and to their training-based alternative as predict(ive) models.2Now,the most natural question to ask, of course, iswhich of the two approaches is best in empiricalterms.
Surprisingly, despite the long tradition ofextensive evaluations of alternative count DSMson standard benchmarks (Agirre et al, 2009; Ba-roni and Lenci, 2010; Bullinaria and Levy, 2007;Bullinaria and Levy, 2012; Sahlgren, 2006; Pad?oand Lapata, 2007), the existing literature containsvery little in terms of direct comparison of countvs.
predictive DSMs.
This is in part due to the factthat context-predicting vectors were first devel-oped as an approach to language modeling and/oras a way to initialize feature vectors in neural-network-based ?deep learning?
NLP architectures,so their effectiveness as semantic representationswas initially seen as little more than an interest-ing side effect.
Sociological reasons might also bepartly responsible for the lack of systematic com-parisons: Context-predictive models were devel-oped within the neural-network community, withlittle or no awareness of recent DSM work in com-putational linguistics.Whatever the reasons, we know of just threeworks reporting direct comparisons, all limited intheir scope.
Huang et al (2012) compare, in pass-ing, one count model and several predict DSMson the standard WordSim353 benchmark (Table3 of their paper).
In this experiment, the countmodel actually outperforms the best predictive ap-proach.
Instead, in a word-similarity-in-contexttask (Table 5), the best predict model outperformsthe count model, albeit not by a large margin.Blacoe and Lapata (2012) compare count andpredict representations as input to compositionfunctions.
Count vectors make for better inputsin a phrase similarity task, whereas the two repre-sentations are comparable in a paraphrase classifi-cation experiment.3Allocation (LDA) models (Blei et al, 2003; Griffiths et al,2007), where parameters are set to optimize the joint prob-ability distribution of words and documents.
However, thefully probabilistic LDA models have problems scaling up tolarge data sets.2We owe the first term to Hinrich Sch?utze (p.c.).
Predic-tive DSMs are also called neural language models, becausetheir supervised context prediction training is performed withneural networks, or, more cryptically, ?embeddings?.3We refer here to the updated results reported inthe erratum at http://homepages.inf.ed.ac.uk/s1066731/pdf/emnlp2012erratum.pdfFinally, Mikolov et al (2013d) compare theirpredict models to ?Latent Semantic Analysis?
(LSA) count vectors on syntactic and semanticanalogy tasks, finding that the predict models arehighly superior.
However, they provide very littledetails about the LSA count vectors they use.4In this paper, we overcome the comparisonscarcity problem by providing a direct evaluationof count and predict DSMs across many parametersettings and on a large variety of mostly standardlexical semantics benchmarks.
Our title alreadygave away what we discovered.2 Distributional semantic modelsBoth count and predict models are extracted froma corpus of about 2.8 billion tokens constructedby concatenating ukWaC,5the English Wikipedia6and the British National Corpus.7For both modeltypes, we consider the top 300K most frequentwords in the corpus both as target and context ele-ments.2.1 Count modelsWe prepared the count models using the DISSECTtoolkit.8We extracted count vectors from sym-metric context windows of two and five words toeither side of target.
We considered two weight-ing schemes: positive Pointwise Mutual Informa-tion and Local Mutual Information (akin to thewidely used Log-Likelihood Ratio scheme) (Ev-ert, 2005).
We used both full and compressed vec-tors.
The latter were obtained by applying the Sin-gular Value Decomposition (Golub and Van Loan,1996) or Non-negative Matrix Factorization (Leeand Seung, 2000), Lin (2007) algorithm, with re-duced sizes ranging from 200 to 500 in steps of100.
In total, 36 count models were evaluated.Count models have such a long and rich his-tory that we can only explore a small subset ofthe counting, weighting and compressing meth-ods proposed in the literature.
However, it isworth pointing out that the evaluated parametersubset encompasses settings (narrow context win-dow, positive PMI, SVD reduction) that have been4Chen et al (2013) present an extended empirical evalua-tion, that is however limited to alternative context-predictivemodels, and does not include the word2vec variant we usehere.5http://wacky.sslmit.unibo.it6http://en.wikipedia.org7http://www.natcorp.ox.ac.uk8http://clic.cimec.unitn.it/composes/toolkit/239found to be most effective in the systematic explo-rations of the parameter space conducted by Bul-linaria and Levy (2007; 2012).2.2 Predict modelsWe trained our predict models with the word2vectoolkit.9The toolkit implements both the skip-gram and CBOW approaches of Mikolov etal.
(2013a; 2013c).
We experimented only withthe latter, which is also the more computationally-efficient model of the two, following Mikolov etal.
(2013b) which recommends CBOW as moresuitable for larger datasets.The CBOW model learns to predict the word inthe middle of a symmetric window based on thesum of the vector representations of the words inthe window.
We considered context windows of2 and 5 words to either side of the central ele-ment.
We vary vector dimensionality within the200 to 500 range in steps of 100.
The word2vectoolkit implements two efficient alternatives to thestandard computation of the output word proba-bility distributions by a softmax classifier.
Hi-erarchical softmax is a computationally efficientway to estimate the overall probability distribu-tion using an output layer that is proportional tolog(unigram.perplexity(W )) instead of W (forW the vocabulary size).
As an alternative, nega-tive sampling estimates the probability of an out-put word by learning to distinguish it from drawsfrom a noise distribution.
The number of thesedraws (number of negative samples) is given bya parameter k. We test both hierarchical softmaxand negative sampling with k values of 5 and 10.Very frequent words such as the or a are not veryinformative as context features.
The word2vectoolkit implements a method to downsize their ef-fect (and simultaneously improve speed perfor-mance).
More precisely, words in the trainingdata are discarded with a probability that is pro-portional to their frequency (capturing the sameintuition that motivates traditional count vectorweighting measures such as PMI).
This is con-trolled by a parameter t and words that occur withhigher frequency than t are aggressively subsam-pled.
We train models without subsampling andwith subsampling at t = 1e?5(the toolkit pagesuggests 1e?3?
1e?5as a useful range based onempirical observations).In total, we evaluate 48 predict models, a num-9https://code.google.com/p/word2vec/ber comparable to that of the count models weconsider.2.3 Out-of-the-box modelsBaroni and Lenci (2010) make the vectors oftheir best-performing Distributional Memory (dm)model available.10This model, based on the sameinput corpus we use, exemplifies a ?linguisticallyrich?
count-based DSM, that relies on lemmasinstead or raw word forms, and has dimensionsthat encode the syntactic relations and/or lexico-syntactic patterns linking targets and contexts.
Ba-roni and Lenci showed, in a large scale evaluation,that dm reaches near-state-of-the-art performancein a variety of semantic tasks.We also experiment with the popular predictvectors made available by Ronan Collobert.11Fol-lowing the earlier literature, with refer to themas Collobert and Weston (cw) vectors.
These are100-dimensional vectors trained for two months(!)
on the Wikipedia.
In particular, the vectorswere trained to optimize the task of choosing theright word over a random alternative in the middleof an 11-word context window (Collobert et al,2011).3 Evaluation materialsWe test our models on a variety of benchmarks,most of them already widely used to test and com-pare DSMs.
The following benchmark descrip-tions also explain the figures of merit and state-of-the-art results reported in Table 2.Semantic relatedness A first set of semanticbenchmarks was constructed by asking humansubjects to rate the degree of semantic similarityor relatedness between two words on a numeri-cal scale.
The performance of a computationalmodel is assessed in terms of correlation betweenthe average scores that subjects assigned to thepairs and the cosines between the correspondingvectors in the model space (following the previ-ous art, we use Pearson correlation for rg, Spear-man in all other cases).
The classic data set ofRubenstein and Goodenough (1965) (rg) consistsof 65 noun pairs.
State of the art performanceon this set has been reported by Hassan and Mi-halcea (2011) using a technique that exploits theWikipedia linking structure and word sense dis-ambiguation techniques.
Finkelstein et al (2002)10http://clic.cimec.unitn.it/dm/11http://ronan.collobert.com/senna/240introduced the widely used WordSim353 set (ws)that, as the name suggests, consists of 353 pairs.The current state of the art is reached by Halawiet al (2012) with a method that is in the spiritof the predict models, but lets synonymy infor-mation from WordNet constrain the learning pro-cess (by favoring solutions in which WordNet syn-onyms are near in semantic space).
Agirre et al(2009) split the ws set into similarity (wss) and re-latedness (wsr) subsets.
The first contains tightertaxonomic relations, such as synonymy and co-hyponymy (king/queen) whereas the second en-compasses broader, possibly topical or syntag-matic relations (family/planning).
We report state-of-the-art performance on the two subsets from thework of Agirre and colleagues, who used differentkinds of count vectors extracted from a very largecorpus (orders of magnitude larger than ours).
Fi-nally, we use (the test section of) MEN (men), thatcomprises 1,000 word pairs.
Bruni et al (2013),the developers of this benchmark, achieve state-of-the-art performance by extensive tuning on ad-hoctraining data, and by using both textual and image-extracted features to represent word meaning.Synonym detection The classic TOEFL (toefl)set was introduced by Landauer and Dumais(1997).
It contains 80 multiple-choice questionsthat pair a target term with 4 synonym candidates.For example, for the target levied one must choosebetween imposed (correct), believed, requestedand correlated.
The DSMs compute cosines ofeach candidate vector with the target, and pick thecandidate with largest cosine as their answer.
Per-formance is evaluated in terms of correct-answeraccuracy.
Bullinaria and Levy (2012) achieved100% accuracy by a very thorough exploration ofthe count model parameter space.Concept categorization Given a set of nominalconcepts, the task is to group them into natural cat-egories (e.g., helicopters and motorcycles shouldgo to the vehicle class, dogs and elephants into themammal class).
Following previous art, we tacklecategorization as an unsupervised clustering task.The vectors produced by a model are clusteredinto n groups (with n determined by the gold stan-dard partition) using the CLUTO toolkit (Karypis,2003), with the repeated bisections with global op-timization method and CLUTO?s default settingsotherwise (these are standard choices in the liter-ature).
Performance is evaluated in terms of pu-rity, a measure of the extent to which each clustercontains concepts from a single gold category.
Ifthe gold partition is reproduced perfectly, purityreaches 100%; it approaches 0 as cluster qualitydeteriorates.
The Almuhareb-Poesio (ap) bench-mark contains 402 concepts organized into 21 cat-egories (Almuhareb, 2006).
State-of-the-art puritywas reached by Rothenh?ausler and Sch?utze (2009)with a count model based on carefully crafted syn-tactic links.
The ESSLLI 2008 Distributional Se-mantic Workshop shared-task set (esslli) contains44 concepts to be clustered into 6 categories (Ba-roni et al, 2008) (we ignore here the 3- and 2-way higher-level partitions coming with this set).Katrenko and Adriaans (2008) reached top per-formance on this set using the full Web as a cor-pus and manually crafted, linguistically motivatedpatterns.
Finally, the Battig (battig) test set intro-duced by Baroni et al (2010) includes 83 conceptsfrom 10 categories.
Current state of the art wasreached by the window-based count model of Ba-roni and Lenci (2010).Selectional preferences We experiment withtwo data sets that contain verb-noun pairs thatwere rated by subjects for the typicality of thenoun as a subject or object of the verb (e.g., peo-ple received a high average score as subject ofto eat, and a low score as object of the sameverb).
We follow the procedure proposed by Ba-roni and Lenci (2010) to tackle this challenge: Foreach verb, we use the corpus-based tuples theymake available to select the 20 nouns that are moststrongly associated to the verb as subjects or ob-jects, and we average the vectors of these nounsto obtain a ?prototype?
vector for the relevant ar-gument slot.
We then measure the cosine of thevector for a target noun with the relevant proto-type vector (e.g., the cosine of people with the eat-ing subject prototype vector).
Systems are eval-uated by Spearman correlation of these cosineswith the averaged human typicality ratings.
Ourfirst data set was introduced by Ulrike Pad?o (2007)and includes 211 pairs (up).
Top-performance wasreached by the supervised count vector system ofHerda?gdelen and Baroni (2009) (supervised in thesense that they directly trained a classifier on golddata, as opposed to the 0-cost supervision of thecontext-learning methods).
The mcrae set (McRaeet al, 1998) consists of 100 noun?verb pairs, withtop performance reached by the DepDM system ofBaroni and Lenci (2010), a count DSM relying on241syntactic information.Analogy While all the previous data sets are rel-atively standard in the DSM field to test traditionalcount models, our last benchmark was introducedin Mikolov et al (2013a) specifically to test pre-dict models.
The data-set contains about 9K se-mantic and 10.5K syntactic analogy questions.
Asemantic question gives an example pair (brother-sister), a test word (grandson) and asks to findanother word that instantiates the relation illus-trated by the example with respect to the test word(granddaughter).
A syntactic question is similar,but in this case the relationship is of a grammaticalnature (work?works, speak.
.
.
speaks).
Mikolovand colleagues tackle the challenge by subtract-ing the second example term vector from the first,adding the test term, and looking for the nearestneighbour of the resulting vector (what is the near-est neighbour of~brother?~sister+~grandson?
).Systems are evaluated in terms of proportion ofquestions where the nearest neighbour from thewhole semantic space is the correct answer (thegiven example and test vector triples are excludedfrom the nearest neighbour search).
Mikolov et al(2013a) reach top accuracy on the syntactic subset(ansyn) with a CBOW predict model akin to ours(but trained on a corpus twice as large).
Top ac-curacy on the entire data set (an) and on the se-mantic subset (ansem) was reached by Mikolovet al (2013c) using a skip-gram predict model.Note however that, because of the way the taskis framed, performance also depends on the sizeof the vocabulary to be searched: Mikolov et al(2013a) pick the nearest neighbour among vectorsfor 1M words, Mikolov et al (2013c) among 700Kwords, and we among 300K words.Some characteristics of the benchmarks we useare summarized in Table 1.4 ResultsTable 2 summarizes the evaluation results.
Thefirst block of the table reports the maximum per-task performance (across all considered parametersettings) for count and predict vectors.
The latteremerge as clear winners, with a large margin overcount vectors in most tasks.
Indeed, the predic-tive models achieve an impressive overall perfor-mance, beating the current state of the art in sev-eral cases, and approaching it in many more.
It isworth stressing that, as reviewed in Section 3, thestate-of-the-art results were obtained in almost allcases using specialized approaches that rely on ex-ternal knowledge, manually-crafted rules, parsing,larger corpora and/or task-specific tuning.
Ourpredict results were instead achieved by simplydownloading the word2vec toolkit and running itwith a range of parameter choices recommendedby the toolkit developers.The success of the predict models cannot beblamed on poor performance of the count mod-els.
Besides the fact that this would not explainthe near-state-of-the-art performance of the pre-dict vectors, the count model results are actuallyquite good in absolute terms.
Indeed, in severalcases they are close, or even better than those at-tained by dm, a linguistically-sophisticated count-based approach that was shown to reach top per-formance across a variety of tasks by Baroni andLenci (2010).Interestingly, count vectors achieve perfor-mance comparable to that of predict vectors onlyon the selectional preference tasks.
The up taskin particular is also the only benchmark on whichpredict models are seriously lagging behind state-of-the-art and dm performance.
Recall from Sec-tion 3 that we tackle selectional preference by cre-ating average vectors representing typical verb ar-guments.
We conjecture that this averaging ap-proach, that worked well for dm vectors, mightbe problematic for prediction-trained vectors, andwe plan to explore alternative methods to build theprototypes in future research.Are our results robust to parameter choices, orare they due to very specific and brittle settings?The next few blocks of Table 2 address this ques-tion.
The second block reports results obtainedwith single count and predict models that are bestin terms of average performance rank across tasks(these are the models on the top rows of tables3 and 4, respectively).
We see that, for both ap-proaches, performance is not seriously affected byusing the single best setup rather than task-specificsettings, except for a considerable drop in perfor-mance for the best predict model on esslli (due tothe small size of this data set?
), and an even moredramatic drop of the count model on ansem.
Amore cogent and interesting evaluation is reportedin the third block of Table 2, where we see whathappens if we use the single models with worstperformance across tasks (recall from Section 2above that, in any case, we are exploring a spaceof reasonable parameter settings, of the sort that an242name task measure source soarg relatedness Pearson Rubenstein and Goodenough Hassan and Mihalcea (2011)(1965)ws relatedness Spearman Finkelstein et al (2002) Halawi et al (2012)wss relatedness Spearman Agirre et al (2009) Agirre et al (2009)wsr relatedness Spearman Agirre et al (2009) Agirre et al (2009)men relatedness Spearman Bruni et al (2013) Bruni et al (2013)toefl synonyms accuracy Landauer and Dumais Bullinaria and Levy (2012)(1997)ap categorization purity Almuhareb (2006) Rothenh?ausler and Sch?utze(2009)esslli categorization purity Baroni et al (2008) Katrenko and Adriaans(2008)battig categorization purity Baroni et al (2010) Baroni and Lenci (2010)up sel pref Spearman Pad?o (2007) Herda?gdelen and Baroni(2009)mcrae sel pref Spearman McRae et al (1998) Baroni and Lenci (2010)an analogy accuracy Mikolov et al (2013a) Mikolov et al (2013c)ansyn analogy accuracy Mikolov et al (2013a) Mikolov et al (2013a)ansem analogy accuracy Mikolov et al (2013a) Mikolov et al (2013c)Table 1: Benchmarks used in experiments, with type of task, figure of merit (measure), original reference(source) and reference to current state-of-the-art system (soa).rg ws wss wsr men toefl ap esslli battig up mcrae an ansyn ansembest setup on each taskcnt 74 62 70 59 72 76 66 84 98 41 27 49 43 60pre 84 75 80 70 80 91 75 86 99 41 28 68 71 66best setup across taskscnt 70 62 70 57 72 76 64 84 98 37 27 43 41 44pre 83 73 78 68 80 86 71 77 98 41 26 67 69 64worst setup across taskscnt 11 16 23 4 21 49 24 43 38 -6 -10 1 0 1pre 74 60 73 48 68 71 65 82 88 33 20 27 40 10best setup on rgcnt (74) 59 66 52 71 64 64 84 98 37 20 35 42 26pre (84) 71 76 64 79 85 72 84 98 39 25 66 70 61other modelssoa 86 81 77 62 76 100 79 91 96 60 32 61 64 61dm 82 35 60 13 42 77 76 84 94 51 29 NA NA NAcw 48 48 61 38 57 56 58 61 70 28 15 11 12 9Table 2: Performance of count (cnt), predict (pre), dm and cw models on all tasks.
See Section 3 andTable 1 for figures of merit and state-of-the-art results (soa).
Since dm has very low coverage of the an*data sets, we do not report its performance there.243experimenter might be tempted to choose withouttuning).
The count model performance is severelyaffected by this unlucky choice (2-word window,Local Mutual Information, NMF, 400 dimensions,mean performance rank: 83), whereas the predictapproach is much more robust: To put its worst in-stantiation (2-word window, hierarchical softmax,no subsampling, 200 dimensions, mean rank: 51)into perspective, its performance is more than 10%below the best count model only for the an andansem tasks, and actually higher than it in 3 cases(note how on esslli the worst predict models per-forms much better than the best one, confirmingour suspicion about the brittleness of this smalldata set).
The fourth block reports performance inwhat might be the most realistic scenario, namelyby tuning the parameters on a development task.Specifically, we pick the models that work beston the small rg set, and report their performanceon all tasks (we obtained similar results by pick-ing other tuning sets).
The selected count modelis the third best overall model of its class as re-ported in Table 3.
The selected predict model isthe fourth best model in Table 4.
The overall countperformance is not greatly affected by this choice.Again, predict models confirm their robustness,in that their rg-tuned performance is always close(and in 3 cases better) than the one achieved by thebest overall setup.Tables 3 and 4 let us take a closer look atthe most important count and predict parame-ters, by reporting the characteristics of the bestmodels (in terms of average performance-basedranking across tasks) from both classes.
For thecount models, PMI is clearly the better weight-ing scheme, and SVD outperforms NMF as a di-mensionality reduction technique.
However, nocompression at all (using all 300K original dimen-sions) works best.
Compare this to the best over-all predict vectors, that have 400 dimensions only,making them much more practical to use.
For thepredict models, we observe in Table 4 that nega-tive sampling, where the task is to distinguish thetarget output word from samples drawn from thenoise distribution, outperforms the more costly hi-erarchical softmax method.
Subsampling frequentwords, which downsizes the importance of thesewords similarly to PMI weighting in count mod-els, is also bringing significant improvements.Finally, we go back to Table 2 to point out thepoor performance of the out-of-the-box cw model.window weight compress dim.
meanrank2 PMI no 300K 355 PMI no 300K 382 PMI SVD 500 422 PMI SVD 400 465 PMI SVD 500 472 PMI SVD 300 505 PMI SVD 400 512 PMI NMF 300 522 PMI NMF 400 535 PMI SVD 300 53Table 3: Top count models in terms of meanperformance-based model ranking across all tasks.The first row states that the window-2, PMI, 300Kcount model was the best count model, and, acrossall tasks, its average rank, when ALL models aredecreasingly ordered by performance, was 35.
SeeSection 2.1 for explanation of the parameters.We must leave the investigation of the parametersthat make our predict vectors so much better thancw (more varied training corpus?
window size?objective function being used?
subsampling?
.
.
.
)to further work.
Still, our results show that it?snot just training by context prediction that ensuresgood performance.
The cw approach is very popu-lar (for example both Huang et al (2012) and Bla-coe and Lapata (2012) used it in the studies we dis-cussed in Section 1).
Had we also based our sys-tematic comparison of count and predict vectorson the cw model, we would have reached oppositeconclusions from the ones we can draw from ourword2vec-trained vectors!5 ConclusionThis paper has presented the first systematic com-parative evaluation of count and predict vectors.As seasoned distributional semanticists with thor-ough experience in developing and using countvectors, we set out to conduct this study becausewe were annoyed by the triumphalist overtones of-ten surrounding predict models, despite the almostcomplete lack of a proper comparison to countvectors.12Our secret wish was to discover that it isall hype, and count vectors are far superior to theirpredictive counterparts.
A more realistic expec-12Here is an example, where word2vec is called the crownjewel of natural language processing: http://bit.ly/1ipv72M244win.
hier.
neg.
subsamp.
dim meansoftm.
samp.
rank5 no 10 yes 400 102 no 10 yes 300 135 no 5 yes 400 135 no 5 yes 300 135 no 10 yes 300 132 no 10 yes 400 132 no 5 yes 400 155 no 10 yes 200 152 no 10 yes 500 152 no 5 yes 300 16Table 4: Top predict models in terms of meanperformance-based model ranking across all tasks.See Section 2.2 for explanation of the parameters.tation was that a complex picture would emerge,with predict and count vectors beating each otheron different tasks.
Instead, we found that the pre-dict models are so good that, while the triumphal-ist overtones still sound excessive, there are verygood reasons to switch to the new architecture.However, due to space limitations we have onlyfocused here on quantitative measures: It remainsto be seen whether the two types of models arecomplementary in the errors they make, in whichcase combined models could be an interesting av-enue for further work.The space of possible parameters of countDSMs is very large, and it?s entirely possible thatsome options we did not consider would have im-proved count vector performance somewhat.
Still,given that the predict vectors also outperformedthe syntax-based dm model, and often approxi-mated state-of-the-art performance, a more profic-uous way forward might be to focus on parametersand extensions of the predict models instead: Af-ter all, we obtained our already excellent resultsby just trying a few variations of the word2vec de-faults.
Add to this that, beyond the standard lex-ical semantics challenges we tested here, predictmodels are currently been successfully applied incutting-edge domains such as representing phrases(Mikolov et al, 2013c; Socher et al, 2012) or fus-ing language and vision in a common semanticspace (Frome et al, 2013; Socher et al, 2013).Based on the results reported here and the con-siderations we just made, we would certainly rec-ommend anybody interested in using DSMs fortheoretical or practical applications to go for thepredict models, with the important caveat that theyare not all created equal (cf.
the big difference be-tween word2vec and cw models).
At the sametime, given the large amount of work that has beencarried out on count DSMs, we would like to ex-plore, in the near future, how certain questionsand methods that have been considered with re-spect to traditional DSMs will transfer to predictmodels.
For example, the developers of LatentSemantic Analysis (Landauer and Dumais, 1997),Topic Models (Griffiths et al, 2007) and relatedDSMs have shown that the dimensions of thesemodels can be interpreted as general ?latent?
se-mantic domains, which gives the correspondingmodels some a priori cognitive plausibility whilepaving the way for interesting applications.
An-other important line of DSM research concerns?context engineering?
: There has been for exam-ple much work on how to encode syntactic in-formation into context features (Pad?o and Lapata,2007), and more recent studies construct and com-bine feature spaces expressing topical vs. func-tional information (Turney, 2012).
To give justone last example, distributional semanticists havelooked at whether certain properties of vectors re-flect semantic relations in the expected way: e.g.,whether the vectors of hypernyms ?distribution-ally include?
the vectors of hyponyms in somemathematical precise sense.Do the dimensions of predict models also en-code latent semantic domains?
Do these modelsafford the same flexibility of count vectors in cap-turing linguistically rich contexts?
Does the struc-ture of predict vectors mimic meaningful seman-tic relations?
Does all of this even matter, or arewe on the cusp of discovering radically new waysto tackle the same problems that have been ap-proached as we just sketched in traditional distri-butional semantics?Either way, the results of the present investiga-tion indicate that these are important directions forfuture research in computational semantics.AcknowledgmentsWe acknowledge ERC 2011 Starting IndependentResearch Grant n. 283554 (COMPOSES).ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pasc?a, and Aitor Soroa.
2009.245A study on similarity and relatedness using distribu-tional and WordNet-based approaches.
In Proceed-ings of HLT-NAACL, pages 19?27, Boulder, CO.Abdulrahman Almuhareb.
2006.
Attributes in LexicalAcquisition.
Phd thesis, University of Essex.Marco Baroni and Alessandro Lenci.
2010.
Dis-tributional Memory: A general framework forcorpus-based semantics.
Computational Linguis-tics, 36(4):673?721.Marco Baroni, Stefan Evert, and Alessandro Lenci, ed-itors.
2008.
Bridging the Gap between SemanticTheory and Computational Simulations: Proceed-ings of the ESSLLI Workshop on Distributional Lex-ical Semantic.
FOLLI, Hamburg.Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-simo Poesio.
2010.
Strudel: A distributional seman-tic model based on properties and types.
CognitiveScience, 34(2):222?254.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for seman-tic composition.
In Proceedings of EMNLP, pages546?556, Jeju Island, Korea.David Blei, Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022.Elia Bruni, Nam Khanh Tran, and Marco Ba-roni.
2013.
Multimodal distributional seman-tics.
Journal of Artificial Intelligence Research.In press; http://clic.cimec.unitn.it/marco/publications/mmds-jair.pdf.John Bullinaria and Joseph Levy.
2007.
Extractingsemantic representations from word co-occurrencestatistics: A computational study.
Behavior Re-search Methods, 39:510?526.John Bullinaria and Joseph Levy.
2012.
Extractingsemantic representations from word co-occurrencestatistics: Stop-lists, stemming and SVD.
BehaviorResearch Methods, 44:890?907.Yanqing Chen, Bryan Perozzi, Rami Al-Rfou?, andSteven Skiena.
2013.
The expressive power ofword embeddings.
In Proceedings of the ICMLWorkshop on Deep Learning for Audio, Speech andLanguage Processing, Atlanta, GA.
Published on-line: https://sites.google.com/site/deeplearningicml2013/accepted_papers.Stephen Clark.
2013.
Vector space mod-els of lexical meaning.
In Shalom Lappinand Chris Fox, editors, Handbook of Contem-porary Semantics, 2nd ed.
Blackwell, Malden,MA.
In press; http://www.cl.cam.ac.uk/?sc609/pubs/sem_handbook.pdf.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of ICML, pages 160?167, Helsinki, Fin-land.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Katrin Erk.
2012.
Vector space models of word mean-ing and phrase meaning: A survey.
Language andLinguistics Compass, 6(10):635?653.Stefan Evert.
2005.
The Statistics of Word Cooccur-rences.
Ph.D dissertation, Stuttgart University.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions on Informa-tion Systems, 20(1):116?131.Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-gio, Jeff Dean, Marc?Aurelio Ranzato, and TomasMikolov.
2013.
DeViSE: A deep visual-semanticembedding model.
In Proceedings of NIPS, pages2121?2129, Lake Tahoe, Nevada.Gene Golub and Charles Van Loan.
1996.
MatrixComputations (3rd ed.).
JHU Press, Baltimore, MD.Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.2007.
Topics in semantic representation.
Psycho-logical Review, 114:211?244.Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, andYehuda Koren.
2012.
Large-scale learning ofword relatedness with constraints.
In Proceedingsof KDD, pages 1406?1414.Samer Hassan and Rada Mihalcea.
2011.
Semanticrelatedness using salient semantic analysis.
In Pro-ceedings of AAAI, pages 884?889, San Francisco,CA.Amac?
Herda?gdelen and Marco Baroni.
2009.
Bag-Pack: A general framework to represent semanticrelations.
In Proceedings of GEMS, pages 33?40,Athens, Greece.Eric Huang, Richard Socher, Christopher Manning,and Andrew Ng.
2012.
Improving word represen-tations via global context and multiple word proto-types.
In Proceedings of ACL, pages 873?882, JejuIsland, Korea.George Karypis.
2003.
CLUTO: A clustering toolkit.Technical Report 02-017, University of MinnesotaDepartment of Computer Science.246Sophia Katrenko and Pieter Adriaans.
2008.
Qualiastructures and their impact on the concrete nouncategorization task.
In Proceedings of the ESS-LLI Workshop on Distributional Lexical Semantics,pages 17?24, Hamburg, Germany.Thomas Landauer and Susan Dumais.
1997.
A solu-tion to Plato?s problem: The latent semantic analysistheory of acquisition, induction, and representationof knowledge.
Psychological Review, 104(2):211?240.Daniel Lee and Sebastian Seung.
2000.
Algorithms forNon-negative Matrix Factorization.
In Proceedingsof NIPS, pages 556?562.Chih-Jen Lin.
2007.
Projected gradient methods forNonnegative Matrix Factorization.
Neural Compu-tation, 19(10):2756?2779.Ken McRae, Michael Spivey-Knowlton, and MichaelTanenhaus.
1998.
Modeling the influence of the-matic fit (and other constraints) in on-line sentencecomprehension.
Journal of Memory and Language,38:283?312.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
http://arxiv.org/abs/1301.3781/.Tomas Mikolov, Quoc Le, and Ilya Sutskever.
2013b.Exploiting similarities among languages for Ma-chine Translation.
http://arxiv.org/abs/1309.4168.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado, and Jeff Dean.
2013c.
Distributed representa-tions of words and phrases and their compositional-ity.
In Proceedings of NIPS, pages 3111?3119, LakeTahoe, Nevada.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013d.
Linguistic regularities in continuous spaceword representations.
In Proceedings of NAACL,pages 746?751, Atlanta, Georgia.George Miller and Walter Charles.
1991.
Contex-tual correlates of semantic similarity.
Language andCognitive Processes, 6(1):1?28.Sebastian Pad?o and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.Ulrike Pad?o.
2007.
The Integration of Syntax andSemantic Plausibility in a Wide-Coverage Model ofSentence Processing.
Dissertation, Saarland Univer-sity, Saarbr?ucken.Klaus Rothenh?ausler and Hinrich Sch?utze.
2009.Unsupervised classification with dependency basedword spaces.
In Proceedings of GEMS, pages 17?24, Athens, Greece.Herbert Rubenstein and John Goodenough.
1965.Contextual correlates of synonymy.
Communica-tions of the ACM, 8(10):627?633.Magnus Sahlgren.
2006.
The Word-Space Model.Ph.D dissertation, Stockholm University.Richard Socher, Brody Huval, Christopher Manning,and Andrew Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of EMNLP, pages 1201?1211, Jeju Island, Ko-rea.Richard Socher, Milind Ganjoo, Christopher Manning,and Andrew Ng.
2013.
Zero-shot learning throughcross-modal transfer.
In Proceedings of NIPS, pages935?943, Lake Tahoe, Nevada.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceed-ings of ACL, pages 384?394, Uppsala, Sweden.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Peter Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.Journal of Artificial Intelligence Research, 44:533?585.247
