Proceedings of the 6th Workshop on Statistical Machine Translation, pages 145?151,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsCMU System Combination in WMT 2011Kenneth Heafield and Alon LavieCarnegie Mellon University5000 Forbes AvePittsburgh, PA, USA{heafield,alavie}@cs.cmu.eduAbstractThis paper describes our submissions,cmu-heafield-combo, to the ten tracksof the 2011 Workshop on Machine Transla-tion?s system combination task.
We show howthe combination scheme operates by flexiblyaligning system outputs then searching aspace constructed from the alignments.Humans judged our combination the best oneight of ten tracks.1 IntroductionWe participated in all ten tracks of the 2011 Work-shop on Machine Translation system combinationtask as cmu-heafield-combo.
This uses a sys-tem combination scheme that builds on our priorwork (Heafield and Lavie, 2010), especially withrespect to language modeling and handling non-English languages.
We present a summary ofthe system, describe improvements, list the dataused (all of the constrained monolingual data), andpresent automatic results in anticipation of humanevaluation by the workshop.2 Our Combination SchemeGiven single-best outputs from each system, thescheme aligns system outputs then searches a spacebased on these alignments.
The scheme is a contin-uation of our previous system (Heafield and Lavie,2010) so we describe unchanged parts of the sys-tem in less detail, preferring instead to focus on newcomponents.2.1 AlignmentWe run the METEOR matcher (Denkowski andLavie, 2010) on every pair of system outputs for agiven sentence.
It identifies exact matches, identi-cal stems (Porter, 2001) except for Czech, WordNetsynonym matches for English (Fellbaum, 1998), andautomatically extracted matches for all five targetlanguages.
The automatic matches come from piv-oting (Bannard and Callison-Burch, 2005) on con-strained data.
An example METEOR alignment isshown in Figure 1, though it need not be monotone.Twice that produced by nuclear plantsDouble that that produce nuclear power stationsFigure 1: Alignment generated by METEOR showingexact (that?that and nuclear?nuclear), stem (produced?produce), synonym (twice?double), and unigram para-phrase (plants?stations) alignments.2.2 SearchThe search space is unchanged from Heafield andLavie (2010), so we give a summary here.
The gen-eral idea is to generate a combined sentence oneword at a time, going from left to right.
As thescheme creates an output, it also steps through thesystem outputs from left to right.
Stepping throughsystems is synchronized with the partial output, sothat words to the left are already captured in the hy-pothesis and the next word from any of the systemsrepresents a meaningful extension of the partial out-put.
All of these options are considered by hypothe-sis branching.145Thus far, we have assumed that system outputs aremonotone: they agree on word order, so it is possi-ble to step through all of them simultaneously.
Onthe left are words captured in the partial output andon the right are the words whose meaning remainsto be captured in the output.
When systems disagreeon word order, the partial output corresponds to dis-joint pieces of a system?s output.
We still retain thatnotion that a word is either captured in the partialoutput or not captured, but do not have a single di-viding line between them.
In this case, we still pro-ceed from left to right, considering the first uncap-tured word for extension.
Then, we skip over partsof a system?s output that have already been captured.Here, we have used the informal notion of wordswhose meaning is ?captured?
or ?uncaptured?
by thepartial output.
The system interprets words alignedto the partial output as captured while those notaligned to the hypothesis are considered uncaptured.A heuristic also cleans up excess words in orderto keep the stepping process loosely synchronizedacross system outputs.2.3 FeaturesWe use three feature categories to guide search:Length The length of the hypothesis in tokens.Language Model Log probability and OOV countfrom an N -gram language model.
Details arein Section 4.1.Match Counts Counts of n-gram matches betweensystems outputs and the hypothesis.The match count features report n-gram matchesbetween each system and the hypothesis.
Specifi-cally, feature ms,n reports n-gram overlap betweenthe hypothesis and system s. We track n-gramcounts up to length N , typically 2 or 3, finding thattracking longer lengths adds little.
An example isshown in Figure 2.These match counts may be exact, in which caseevery word of the n-gram must be the same (upto case) or approximate, in which case any alignedword found by METEOR may be substituted.
Be-cause exact matches handle lexical choice and in-exact matches collect more votes that better handleword order, we use both sets of features.
However,the limit N may be different i.e.
Ne = 2 countsexact matches up to length 2 and Na = 3 countsinexact matches up to length 3.System 1: Supported Proposal of FranceSystem 2: Support for the Proposal of FranceCandidate: Support for Proposal of FranceUnigram Bigram TrigramSystem 1 4 2 1System 2 5 3 1Figure 2: Example match feature values with two systemsand matches up to length three.
Here, ?Supported?
countsbecause it aligns with ?Support?.3 Related WorkHypothesis selection (Hildebrand and Vogel, 2009)selects an entire sentence at a time instead of pickingand merging words.
This makes the approach lessflexible, in that it cannot synthesize new sentences,but also less risky by avoiding matching and relatedproblems entirely.While our alignment is based on METEOR, othertechniques are based on TER (Snover et al, 2006),Inversion Transduction Grammars (Narsale, 2010),and other alignment methods.
These use exactalignments and positional information to infer align-ments, ignoring the content-based method used byMETEOR.
This means they might align contentwords to function words, while we never do.
In prac-tice, using both signals would likely work better.Confusion networks (Rosti et al, 2010; Narsale,2010) are the dominant method for system combi-nation.
These base their word order on one system,dubbed the backbone, and have all systems vote onediting the backbone.
Word order is largely fixed tothat of one system; by contrast, ours can piece to-gether word orders taken from multiple systems.
Ina loose sense, our approach is a confusion networkwhere the backbone is permitted to switch after eachword.Interestingly, BBN (Rosti et al, 2010) this yearadded a novel-bigram penalty that penalizes bigramsin the output if they do not appear in one of the sys-146tem outputs.
This is the complement of our bigrammatch count features (and, since, we have a lengthfeature, the same up to rearranging weights).
How-ever, they threshold it to indicate whether the bigramappears at all instead of how many systems supportthe bigram.4 ResourcesThe resources we use are constrained to those pro-vided for the shared task.For the paraphrase matches described in Sec-tion 2.1, METEOR (Denkowski and Lavie, 2010)trains its paraphrase tables via pivoting (Bannardand Callison-Burch, 2005).
The phrase tables aretrained using parallel data from Europarl v6 (Koehn,2005) (fr-en, es-en, de-en, and es-de), news com-mentary (fr-en, es-en, de-en, and cz-en), United Na-tions (fr-en and es-en), and CzEng (cz-en) (Bojarand Z?abokrtsky?, 2009) sections 0?8.4.1 Language ModelingAs with previous versions of the system, we uselanguage model log probability as a feature to biastranslations towards fluency.
We add a second fea-ture per language model that counts OOVs, allow-ing MERT to independently tune the OOV penalty.Language models often have poor OOV estimatesfor translation because they come not from new textin the same language but from new text in a differ-ent language.
The distribution is even more biasedin system combination, where most systems have al-ready applied a language model.
The new OOV fea-ture replaces a previous feature that reported the av-erage n-gram length matched by the model.We added support for multiple language mod-els so that their probabilities, OOV penalties, andall other features are dynamically interpolated usingMERT.
This we use for the Haitian Creole-Englishtasks, where the first language model is a largemodel built on the monolingual data except SMSmessages and the second small language model isbuilt on the SMS messages.
The OOV features playan important role here because frequent anonymiza-tion markers such as ?[firstname]?
do not appear inthe large language model.To scale to larger language models, we useBigFatLM1, an open-source builder of large un-pruned models with modified Kneser-Ney smooth-ing.
Then, we filter the models to the system out-puts.
In order for an n-gram to be queried, all of thewords must appear in system outputs for the samesentence.
This enables a filtering constraint strongerthan normal vocabulary filtering, which permits n-grams supported only by words in different sen-tences.
Finally, we use KenLM (Heafield, 2011) forinference at runtime.Our primary use of data is for language model-ing.
We used essientially every constrained resourceavailable and appended them together to build onelarge model.
For every language, we used the pro-vided Europarl v6 (Koehn, 2005), News Crawl, andNews Commentary corpora.
In addition, we used:English Gigaword Fourth Edition (Parker et al,2009) and the English parts of United Na-tions documents, Giga-FrEn, and CzEng (Bojarand Z?abokrtsky?, 2009) sections 0?7.
For theHaitian Creole-English tasks, we built a sepa-rate language model on the SMS messages andused it alongside the large English model.Czech CzEng (Bojar and Z?abokrtsky?, 2009) sec-tions 0?7French Gigaword Second Edition (Mendonc?a etal., 2009a) and the French parts of Giga-FrEnand United Nations documents.German There were no additional corpora avail-able.Spanish Gigaword Second Edition (Mendonc?a etal., 2009b) and the Spanish parts of United Na-tions documents.4.2 PreprocessingMany corpora contained excessive duplicate text.We wrote a deduplicator that removes all but thefirst instance of each line.
Clean corpora generallyreduced line count by 10-25% when deduplicated,resulting from naturally-occuring duplicates such as?yes .?
We left the duplicate lines in these corpora.The News Crawl corpus showed a 72.6% reductionin line count due mainly to boilerplace, such as the1https://github.com/jhclark/bigfatlm147Reuters comment section header and Fark headlinesthat appear in a box on many pages.
We dedupli-cated the News Crawl corpus, United Nations docu-ments, and New York Times and LA Times portionsof English Gigaword.The Giga-FrEn corpus is noisy.
We removed linesfrom Giga-FrEn if any of the following conditionsheld:?
Invalid UTF8 or control characters.?
Less than 90% of characters are in the Latinalphabet (including diacritics) or punctuation.We did not count ?<?
and ?>?
as punctuationto limit the amount of HTML code.?
Less than half the characters are Latin letters.System outputs and language model training datawere normalized using the provided punctuationnormalization script, Unicode codepoint collaps-ing, the provided Moses (Koehn et al, 2007) to-kenizer, and several custom rules.
These removeformatting-related tokens from Gigaword, rejoinsome French words with internal apostrophes, andthreshold repetitive punctuation.
In addition, Ger-man words were segmented as explained in Section4.3.
Text normalization is more difficult for systemcombination because the system outputs, while theo-retically detokenized, contain errors that result fromdifferent preprocessing at each site.4.3 German SegmentationGerman makes extensive use of compounding, cre-ating words that do not cleanly align to English andhave less reliable statistics.
German-English trans-lation systems therefore typically segment Germancompounds as a preprocessing step.
In our case,we are concerned with combining translations intoGerman that may be segmented differently.
Thesecan be due to stylistic choices; for example both?jahrzehnte lang?
and ?jahrzehntelang?
appear withapproximately equal frequency as shown in Table 1.Translation systems add additional biases due to thevarious preprocessing approaches taken by individ-ual sites and inherent biases in models such as wordalignment.In order to properly align differently segmentedwords, we normalize by segmenting all system out-puts and our language model training data usingWords Separate Compoundedjahrzehnte lang 554 542klar gemacht 840 802unter anderem 49538 4wieder herzustellen 513 1532Table 1: Counts of separate or compounded versions ofselect words in the lowercased German monolingual data.Compounding can be optional or biased in either way.the single-best segmentation from cdec (Dyer etal., 2010).
Running our system therefore producessegmented German output.
Internally, we tunedtowards segmented references but for final outputit is desirable to rejoin compound words.
Sincethe cdec segmentation was designed for German-English translation, no corresponding desegmenterwas provided.We created a German desegmenter in the naturalway: segment German words then invert the map-ping to identify words that should be rejoined.
To doso, we ran every word from the German monolingualdata and system outputs through the cdec segmenter,counted both the compounded and segmented ver-sions in the monolingual data, and removed thosethat appear segmented more often.
Desegmenting isa mildly ambiguous process because n-grams to re-join may overlap.
When an n-gram compounded toone word, we gave that a score of n2.
The total scoreis a sum of these squares, favoring compounds thatcover more words.
Maximizing the score is a fastand exact dynamic programming algorithm.
Casingof unchanged words comes from equally-weightedsystem votes at the character level while casing ofrejoined words is based on the majority appearancein the corpus; this is almost always initial capital.We ran our desegmenter followed by the workshop?sprovided detokenizer to produce the submitted out-put.5 ResultsWe tried many variations on the scheme, such as se-lecting different systems, tuning to BLEU (Papineniet al, 2002) or METEOR (Denkowski and Lavie,2010), and changing the structure of the match countfeatures from Section 2.3.
To try these, we ranMERT 242 times, or about 24 times for each of theten tasks in which we participated.
Then we selected148the best performing systems on the tuning set andsubmitted them, with the secondary system chosento meaningfully differ from the primary while stillscoring well.
Once the evaluation released refer-ences, we scored against them to generate Table 2.On the featured Haitian Creole task, we show noand sometimes even negative improvement.
This weattribute to the gap between the top system, bm-i2r,and the second place system.
For htraw-en, wheretraining data is noisy, the bm-i2r is 3.65 BLEUhigher than the second place system at 28.53 BLEU.On htclean-en, the gap is 4.44 points to the secondplace cmu-denkowski-contrastive.The main tasks were quite competitive and manysystems were within a BLEU point of the top.
Thisis an ideal scenario for system combination, and weshow corresponding improvements.
The English-Czech task is difficult for our scheme because we donot properly handle Czech morpology in alignment.On Czech-English, online-B beat other systems bya substantial (6.21 BLEU) margin, so we see littlegain.
On English-German, the gain is small but thisis consistent with a general observation that moreimprovement is seen on higher-quality systems.
Fur-ther, strength in this year?s submission comes fromlanguage modeling, but only limited German datawas available; segmenting German improved ourscores.
Translations into Spanish and French showthe impact of Gigaword in those languages.The evaluation?s official metric is human rank-ing judgments.
On this metric, our submissionsscore highest on eight of ten tracks: Czech-English,German-English, English-Czech, English-German,English-Spanish, English-French, the clean HaitianCreole-English task, and the raw Haitian Creole-English task.
For Spanish-English, humans pre-ferred RWTH?s submission.
For French-English,humans preferred RWTH and BBN.
However, sys-tem combinations were ranked against other systemcombinations, but not against underlying systems,so we suspect that the bm-i2r submission still per-forms better than combinations on the Haitian Cre-ole tasks.
The human judges also preferred ourtranslations more than BLEU (where we lead onthree language pairs: English to German, Span-ish, and French).
We attribute this to the tendencyof confusion networks to drop words supported bymany systems due to position-based alignment er-Track Entry BLEU TER METhtraw-enprimary 32.30 56.57 61.05contrast 31.76 56.69 60.81bm-i2r 32.18 57.01 60.85htclean-enprimary 36.39 51.16 63.72contrast 36.49 51.15 63.78bm-i2r 36.97 51.06 64.01cz-enprimary 29.85 53.20 62.50contrast 29.88 53.19 62.40online-B 29.59 52.15 61.77de-enprimary 26.21 56.19 60.56contrast 26.11 56.42 60.54online-B 24.30 57.95 59.63es-enprimary 33.90 48.88 65.72contrast 33.47 49.41 66.41online-A 30.26 51.56 63.83fr-enprimary 32.41 48.93 65.72contrast 32.15 49.12 65.71kit 30.36 50.74 64.32en-czprimary 20.80 61.17 41.68contrast 20.74 61.29 41.69online-B 20.37 61.38 41.40en-deprimary 18.45 64.15 22.91contrast 18.27 64.48 22.75online-B 17.92 64.01 22.95en-esprimary 36.47 47.08 34.96contrast 35.82 47.52 34.64online-B 33.85 50.09 33.96en-frprimary 36.42 48.28 24.29contrast 36.31 48.56 24.12online-B 35.34 48.68 23.53Table 2: Automatic scores for our submissions.
For com-parison, the top individual system by BLEU is shownin the third row of each track.
Test data and referenceswere preprocessed prior to scoring.
Metrics are uncasedand METEOR 1.0 uses adequacy-fluency parameters.
Weshow improvement on all tasks except Haitian Creole-English.149rors; our content-based alignment method avoidsmany of these errors.
BLEU penalizes the missingword the same as missing punctuation while humanjudges will penalize heavily for missing content.
Forfull results, we refer to the simultaneously publishedWorkshop on Machine Translation findings paper.6 ConclusionWe participated in the all ten tracks of the sys-tem combination, prioritizing participation and lan-guage support over optimizing for one particularlanguage pair.
Nonetheless, we show improvementon several tasks, including wins by BLEU on threetracks.
The Haitian Creole and Czech-English tasksproved challenging due to the gap between top sys-tems.
However, other tracks show a variety ofhigh-performing systems that make our scheme per-form well.
Unlike most other system combinationschemes, our code is open source2 so that these re-sults may be replicated and brought to bear on simi-lar problems.AcknowledgementsJon Clark assisted with language model constructionand wrote BigFatLM.
This material is based uponwork supported by the National Science Founda-tion Graduate Research Fellowship under Grant No.0750271 and by the DARPA GALE program.ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings ACL.Ondr?ej Bojar and Zdene?k Z?abokrtsky?.
2009.
CzEng 0.9,building a large Czech-English automatic parallel tree-bank.
The Prague Bulletin of Mathematical Linguis-tics, (92):63?83.Michael Denkowski and Alon Lavie.
2010.
Meteor-nextand the meteor paraphrase tables: Improved evalua-tion support for five target languages.
In Proceed-ings of the Joint Fifth Workshop on Statistical Ma-chine Translation and MetricsMATR, pages 339?342,Uppsala, Sweden, July.
Association for ComputationalLinguistics.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,2http://kheafield.com/code/mtVladimir Eidelman, and Philip Resnik.
2010. cdec:A decoder, alignment, and learning framework forfinite-state and context-free translation models.
InProceedings of the ACL 2010 System Demonstrations,ACLDemos ?10, pages 7?12.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Kenneth Heafield and Alon Lavie.
2010.
CMU multi-engine machine translation for WMT 2010.
In Pro-ceedings of the Joint Fifth Workshop on Statistical Ma-chine Translation and MetricsMATR.
Association forComputational Linguistics.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, Edin-burgh, UK, July.
Association for Computational Lin-guistics.Almut Silja Hildebrand and Stephan Vogel.
2009.
CMUsystem combination for WMT?09.
In Proceedings ofthe Fourth Workshop on Statistical Machine Transla-tion, pages 47?50, Athens, Greece, March.
Associa-tion for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In AnnualMeeting of the Association for Computational Linguis-tics (ACL), Prague, Czech Republic, June.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of MTSummit.A?ngelo Mendonc?a, David Graff, and Denise DiPer-sio.
2009a.
French gigaword second edition.LDC2009T28.A?ngelo Mendonc?a, David Graff, and Denise DiPer-sio.
2009b.
Spanish gigaword second edition.LDC2009T21.Sushant Narsale.
2010.
JHU system combinationscheme for wmt 2010.
In Proceedings of the JointFifth Workshop on Statistical Machine Translationand MetricsMATR, pages 311?314, Uppsala, Sweden,July.
Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 311?318, Philadelphia, PA,July.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2009.
English gigaword fourth edi-tion.
LDC2009T13.150Martin Porter.
2001.
Snowball: A language for stem-ming algorithms.
http://snowball.tartarus.org/.Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, andRichard Schwartz.
2010.
BBN system description forwmt10 system combination task.
In Proceedings ofthe Joint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 321?326, Uppsala,Sweden, July.
Association for Computational Linguis-tics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human Anno-tation.
In Proceedings of the 7th Conference of theAssociation for Machine Translation in the Americas(AMTA-2006), pages 223?231, Cambridge, MA, Au-gust.151
