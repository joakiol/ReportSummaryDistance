Proceedings of the 5th Workshop on Important Unresolved Matters, pages 105?111,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsThe Spanish Resource Grammar: pre-processing strategy and lexical acquisi-tionMontserrat Marimon, N?ria Bel, Sergio Espeja, Natalia SeghezziIULA - Universitat Pompeu FabraPl.
de la Merc?, 10-1208002-Barcelona{montserrat.marimon,nuria.bel,sergio.espeja,natalia.seghezzi}@upf.eduAbstractThis paper describes work on the develop-ment of an open-source HPSG grammar forSpanish implemented within the LKB sys-tem.
Following a brief description of themain features of the grammar, we presentour approach for pre-processing and on-going research on automatic lexical acqui-sition.11 IntroductionIn this paper we describe the development of theSpanish Resource Grammar (SRG), an open-source 2  medium-coverage grammar for Spanish.The grammar is grounded in the theoreticalframework of HPSG (Head-driven Phrase Struc-ture Grammar; Pollard and Sag, 1994) and usesMinimal Recursion Semantics (MRS) for the se-mantic representation (Copestake et al 2006).
TheSRG is implemented within the Linguistic Knowl-edge Building (LKB) system (Copestake, 2002),based on the basic components of the grammarMatrix, an open?source starter-kit for the devel-opment of HPSG grammars developed as part ofthe LinGO consortium?s multilingual grammarengineering (Bender et al, 2002).The SRG is part of the DELPH-IN open-sourcerepository of linguistic resources and tools forwriting (the LKB system), testing (The [incrtsbd()]; Oepen and Carroll, 2000) and efficiently1 This research was supported by the Spanish Ministerio deEducaci?n y Ciencia: Project AAILE (HUM2004-05111-C02-01), Ramon y Cajal, Juan de la Cierva programmes and PTA-CTE/1370/2003 with Fondo Social Europeo.2 The Spanish Resource Grammar may be downloaded from:http://www.upf.edu/pdi/iula/montserrat.marimon/.processing HPSG grammars (the PET system;Callmeier, 2000).
Further linguistic resources thatare available in the DELPH-IN repository includebroad-coverage grammars for English, German andJapanese as well as  smaller grammars for French,Korean, Modern Greek, Norwegian andPortuguese .3The SRG has a full coverage of closed wordclasses and it contains about 50,000 lexical entriesfor open classes (roughtly: 6,600 verbs, 28,000nouns, 11,200 adjectives and 4,000 adverbs).These lexical entries are organized into a typehierachy of about 400 leaf types (defined by a typehierarchy of  around 5,500 types).
The grammaralso has 40 lexical rules to perform valencechanging operations on lexical items and 84structure rules to combine words and phrases intolarger constituents and to compositionally build upthe semantic representation.We have been developing the SRG sinceJanuary 2005.
The range of linguistic phenomenathat the grammar handles includes almost all typesof subcategorization structures, valencealternations,  subordinate clauses, raising andcontrol, determination,  null-subjects andimpersonal constructions, compound tenses,modification,  passive constructions, comparativesand superlatives, cliticization, relative andinterrogative clauses and sentential adjuncts,among others.Together with the linguistic resources (grammarand lexicon) we provide a set of controlled hand-constructed test suites.
The construction of the testsuites plays a major role in the development of theSRG, since test suites provide a fine-grained diag-3 See .
http://www.delph-in.net/105nosis of grammar performance and they allow us tocompare the SRG with other DELPH-IN gram-mars.
In building the test suites we aimed at (a)testing specific phenomena in isolation or in con-trolled interaction, (b) providing test cases whichshow systematic and exhaustive variations overeach phenomenon, including infrequent phenom-ena and variations, (c) avoiding irrelevant variation(i.e.
different instances of the same lexical type), (d)avoiding ambiguity, and (e) including negative orungrammatical data.
We have about 500 test caseswhich are distributed by linguistic phenomena (wehave 17 files).
Each test case includes a short lin-guistic annotation describing the phenomenon andthe number of expected results when more thanone analysis cannot be avoided (e.g.
testing op-tionality).Test suites are not the only source of data wehave used for testing the SRG.
Hand-constructedsentences were complemented by real corpus casesfrom: (a) the Spanish questions from the QuestionAnswering track at CLEF (CLEF-2003, CLEF-2004, CLEF-2005 and CLEF-2006), and (b) thegeneral sub-corpus of the Corpus T?cnic del?IULA (IULA?s Technical Corpus; Cabr?
andBach, 2004); this sub-corpus includes newspaperarticles and it has been set up for contrastivestudies.
CLEF cases include short queries showinglittle interaction of phenomena and an average of9.2 words; newspaper articles show a high level ofsyntactic complexity and interaction of phenomena,sentences are a bit longer, ranging up to 35 words.We are currently shifting to much more variedcorpus data of the Corpus T?cnic de l?IULA, whichincludes specialized corpus of written text in theareas of computer science, environment, law,medicine and economics, collected from severalsources, such as legal texts, textbooks, researchreports, user manuals, ?
In these texts sentencelength may range up to 70 words.The rest of the paper describes the pre-processing strategy we have adopted and on ouron-going research on lexical acquisition.2 Pre-processing in the SRGFollowing previous experiments within theAdvanced Linguistic Engineering Platform (ALEP)platform (Marimon, 2002), we have integrated ashallow processing tool, the FreeLing tool, as apre-processing module of the grammar.The FreeLing tool is an open-source4 languageanalysis tool suite (Atserias et al, 2006) perfomingthe following functionalities (thoughdisambiguation, named entity classification and thelast three functionalities have not been integrated):?
Text tokenization (including MWU andcontraction splitting).?
Sentence splitting.?
Morpho-syntactic analysis anddisambiguation.n.?
Named entity detection and classification.?
Date/number/currency/ratios/physicalmagnitude (speed, weight, temperature,density, etc.)
recognitio?
Chart-based shallow parsing.?
WordNet-based sense annotation.?
Dependency parsing.FreeLing also includes a guesser to deal withwords which are not found in the lexicon bycomputing the probability of each possible PoS taggiven the longest observed termination string forthat word.
Smoothing using probabilities of shortertermination strings is also performed.
Details canbe found in Brants (2000) and Samuelson (1993).Our system integrates the FreeLing tool bymeans of the LKB Simple PreProcessor Protocol(SPPP; http://wiki.delph-in.net/moin/LkbSppp),which assumes that a preprocessor runs as anexternal process to the LKB system, and uses theLKB inflectional rule component to convert thePoS tags delivered by the FreeLing tool into partialdescriptions of feature structures.2.1 The integration of PoS tagsThe integration of the morpho-syntactic analysis inthe LKB system using the SPPP protocol meansdefining inflectional rules that propagate the mor-pho-syntactic information associated to full-forms,in the form of PoS tags, to the morpho-syntacticfeatures of the lexical items.
(1) shows the rulepropagating the tag AQMS (adjective qualitativemasculine singular) delivered by FreeLing.
Note4 The FreeLing tool may be downloaded fromhttp://www.garraf.epsevg.upc.es/freeling/.106that we use the tag as the rule identifier (i.e.
thename of the inflectional rule in the LKB).
(1) aqms :=%suffix ()[SYNSEM.LOCAL[CAT adj,AGR.PNG[PN 3sg,GEN masc]]]In Spanish, when the verb is in non-finite form,such as infinitive or gerund, or it is in the impera-tive, clitics5 take the form of enclitics.
That is, theyare attached to the verb forming a unique word,e.g.
hacerlo (hacer+lo; to do it), gustarle (gus-tar+le; to like to him).
FreeLing does not splitverbs and pronouns, but uses complex tags thatappend the tags of each word.
Thus, the form ha-cerlo gets the tag VMN+PP3MSA (verb main in-finitive + personal pronoun 3rd masculine singularaccusative).
In order to deal with these complextags, the SRG includes a series of rules that buildup the same type of linguistic structure as that onebuilt up with the structure rules attaching affixes tothe left of verbal heads.
Since the application ofthese rules is based on the tag delivered by FreeL-ing, they are included in the set of inflectional rulesand they are applied after the set of rules dealingwith complement cliticization.Apart from avoiding the implementation of in-flectional rules for such a highly inflected lan-guage, the integration of the morpho-syntacticanalysis tags will allow us to implement defaultlexical entries (i.e.
lexical entry templates that areactivated when the system cannot find a particularlexical entry to apply) on the basis of the categoryencoded to the lexical tag delivered by FreeLing,for virtually unlimited lexical coverage.
62.2 The integration of multiword expressionsAll multiword expressions in FreeLing are storedin a file.
The format of the file is one multiwordper line, having three fields each: form, lemma andPoS.7 (2) shows two examples of multiword fixed5 Actually, Spanish weak pronouns are considered pronominalaffixes rather than pronominal clitics.6 The use of underspecified default lexical entries in ahighly lexicalized grammar, however, may increaseambiguity and overgeneration (Marimon and Bel,2004).7 FreeLing only handles continuous multiword expres-sions.expressions; i.e.
the ones that are fully lexicalizedand never show morpho-syntactic variation, atrav?s de (through) and a buenas horas (finally).
(2) a_trav?s_de a_trav?s_de SPS00a_buenas_horas a_buenas_horas RGThe multiword form field may admit lemmas inangle brackets, meaning that any form with thatlemma will be a valid component for the multi-word.
Tags are specified directly or as a referenceto the tag of some of the multiword components.
(3) builds a multiword with both singular and plu-ral forms  (apartado(s) de correos (P.O Box)).
Thetag of the multiform is that of its first form ($1)which starts with NC and takes the values fornumber depending on whether the form is singularor plural.
(3) <apartado>_de_correos apar-tado_de _correos \$1:NCBoth fixed expressions and semi-fixed expres-sions are integrated by means of the inflectionalrules that we have described in the previous sub-section and they are treated in the grammar asword complex with a single part of speech.2.3 The integration of messy details andnamed entitiesFreeLing identifies, classifies and, when appropri-ate, normalizes special text constructions that maybe considered peripheral to the lexicon, such asdates, numbers, currencies, ratios, physical magni-tudes, etc.
FreeLing also identifies and classifiesnamed entities (i.e.
proper names); however, we donot activate the classification functionality, sincehigh performance of that functionality is onlyachieved with PoS disambiguated contexts.To integrate these messy details and named enti-ties into the grammar, we require special inflec-tional rules and lexical entry templates for eachtext construction tag delivered by FreeLing.
Someof these tags are: W for dates, Z for numbers, Zmfor currencies, ...
In order to define one single en-try for each text construct, we identify the tag andthe STEM feature.
(4) shows the lexical entry fordates.88 Each lexical entry in the SRG consists of a unique identifier,a lexical type, an orthography and a semantic relation.107(4)date := date_le &[STEM <?w?>,SYNSEM.LKEY.KEYREL.PRED time_n_rel]The integration of these messy details allows usto release the analysis process from certain tasksthat may be reliably dealt with by shallow externalcomponents.3 Automatic Lexical AcquisitionWe have investigated Machine Learning (ML)methods applied to the acquisition of the informa-tion contained in the lexicon of the SRG.ML applied to lexical acquisition is a very activearea of work linked to deep linguistic analysis dueto the central role that lexical information has inlexicalized grammars and the costs of hand-crafting them.
Korhonen (2002), Carroll and Fang(2004), Baldwin (2005), Blunsom and Baldwin(2006), and Zhang and Kordoni (2006) are just afew examples of reported research work on deeplexical acquisition.The most successful systems of lexical acquisi-tion are based on the linguistic idea that the con-texts where words occur are associated to particu-lar lexical types.
Although the methods are differ-ent, most of the systems work upon the syntacticinformation on words as collected from a corpus,and they develop different techniques to decidewhether this information is relevant for type as-signment or it is noise, especially when there arejust a few examples.
In the LKB grammaticalframework, lexical types are defined as a combina-tion of grammatical features.
For our research, wehave looked at these morpho-syntactically moti-vated features that can help in discriminating thedifferent types that we will ultimately use to clas-sify words.
Thus, words are assigned a number ofgrammatical features, the ones that define the lexi-cal types.Table 1 and Table 2 show the syntactic featuresthat we use to characterize 6 types of adjectivesand 7 types of nouns in Spanish, respectively.9 Ascan be observed, adjectives are cross-classifiedaccording to their syntactic position within the NP,i.e.
(preN(ominal)) vs  postN(ominal), the possibil-ity of co-occurring in predicative constructions9 The SRG has 35 types for nouns and 44 types for adjectives.
(pred) and being modified by degree adverbs (G),and their subcategorization frame (pcomp);whereas lexical types for nouns are basically de-fined on the basis of the mass/countable distinctionand valence information.
Thus, an adjective likebonito (nice), belonging to the type a_qual_intr,may be found both in pre-nominal and post-nominal position or in predicative constructions, itmay also be modified by degree adverbs, this typeof adjectives, however, does not take comple-ments.
Nouns belonging to the type n_intr_count,like muchacha (girl), are countable intransitivenouns.TYPE/SF preN postN pred G pcompa_adv_int yes no no no noa_adv_event yes yes no no noa_rel_nonpred no yes no no noa_rel_pred no yes yes no noa_qual_intr yes yes yes yes noa_qual_trans yes yes yes yes yesTable 1.
Some adjectival types of the SRGTYPE/SF mass count intr trans pcompn_intr_mass yes no yes no non_intr_count no yes yes no non_intr_cnt-mssyes yes yes no non_trans_mass yes no no yes non_trans_count no yes no yes non_ppde_pcomp_countno yes no yes yesn_ppde_pcomp_mssyes no no yes yesTable 2.
Some nominal types of the SRGWe have investigated two methods to automati-cally acquire such linguistic information for Span-ish nouns and adjectives: a Bayesian model and adecision tree.
The aim of working with these twomethods was to compare their performance takinginto account that while the decision tree gets theinformation from previously annotated data, theBayesian method learns it from the linguistic ty-pology as defined by the grammar.
These methodsare described in the following subsections.3.1 A Bayesian model for lexical acquisitionWe have used a Bayesian model of inductive learn-ing for assigning grammatical features to wordsoccurring in a corpus.
Given a hypothesis space(the linguistic features of words according to itslexical type) and one or more occurrences of the108word to classify, the learner evaluates all hypothe-ses for word features and values by computingtheir posterior probabilities, proportional to theproduct of prior probabilities and likelihood.In order to obtain the likelihood, grammaticalfeatures are related to the expected contexts wheretheir instances might appear.
The linguistic typol-ogy provides likelihood information that is thelearner?s expectation about which contexts arelikely to be observed given a particular hypothesisof a word type.
This likelihood is used as a substi-tute of the computations made by observing di-rectly the data, which is what a supervised machinelearning method does.
As said, our aim was tocompare these two strategies.The decision on a particular word is determinedby averaging the predictions of all hypothesisweighted by their posterior probabilities.
Moretechnically, for each syntactic feature {sf1, sf2, ...,sfn} of the set SF (Syntactic Features) representedin the lexical typology, we define the goal of oursystem to be the assignment of a value, {no, yes},that maximizes the result of a function f: ??
SF,where ?
is the collection of its occurrences (?
={v1, v2, ..., vz}), each being a n-dimensional vector.The decision on value assignment is achieved byconsidering every occurrence as a cumulative evi-dence in favour or against of having each syntacticfeature.
Thus, our function Z?
(SF, ?
), shown in (5),will assess how much relevant information is gotfrom all the vectors.
A further function, shown in(8), will decide on the maximal value in order toassign sfi,x.
(5)  ?= zj jvxisfPxisfZ )|,(),,(' ?To assess P(sfi,x|vj), we use (6), which is the ap-plication of Bayes Rule for solving the estimationof the probability of a vector conditioned to a par-ticular feature and value.
(6)?=k kisfPkisfjvPxisfPxisfjvPjvxisfP ),(),|(),(),|()|,(For solving (6), the prior P(sfi,x) is computed onthe basis of a lexical typology too, assuming thatwhat is more frequent in the typology will corre-spondingly be more frequent in the data.
For com-puting the likelihood P(vj|sfi,x), as each vector ismade of m components, that is, the linguistic cuesvz = {lc1, lc2, ..., lcm}, we proceed as in (7) on thebasis of P(lcl|sfi,x); i.e.
the likelihood of finding theword in a particular context given a particular syn-tactic feature.
(7)  ?==ml xisfllcPxisfjvP 1),|(),|(Finally Z, as in (8), is the function that assignsthe syntactic features to ?
.10(8)???????????=>=?=>==noyesxisfZnoxisfZyesnoxisfZyesxisfZZ)|,(')|,(')|,(')|,('???
?For computing the likelihood, we count on theconditional probabilities of the correlations be-tween features as defined in the typology.
We usethese correlations to infer the expectation of ob-serving the linguistic cues associated to particularsyntactic features, and to make it to be conditionalto a particular feature and value.
However, linguis-tic cues and syntactic features are in two differentdimensions; syntactic features are properties oflexical items, while linguistic cues show the char-acteristics of actual occurrences.
As we assumethat each syntactic feature must have at least onecorresponding linguistic cue, we must tune theprobability to acknowledge the factors that affectlinguistic cues.
For such a tuning, we have consid-ered the following two issues: (i) to include in theassessments the known uncertainty of the linguisticcues that can be present in the occurrence or not;and (ii) to create a dummy variable to deal with thefact that, while syntactic features in the typologyare independent from one another, evidences intext are not so.We have also observed that the information thatcan be gathered by looking at all word occurrencesas a complex unit have a conclusive value.
Takefor instance the case of prepositions.
The observa-tion of a given prepositions in different occur-rences of the same word is a conclusive evidencefor considering it a bound preposition.
In order totake this into account, we have devised a functionthat acts as a dynamic weighting module.
Thefunction app_lc(sfi, ?)
returns the number of con-texts where the cue is found.
In the case that in a10 In the theoretical case of having the same probabilityfor yes and for no, Z is undefined.109particular signature there is no context with such alc, it returns ?1?.
Thus, app_lc is used to reinforcethis conclusive evidence in (5), which is now (9).
(9)),(_*)|,(),,(' ??
isflcappzj jvyesxisfPyesxisfZ ???????
===?
===zj jvnoxisfPnoxisfZ )|,(),,(' ?3.2 A Decision treeLinguistic motivated features have also beenevaluated using a C4.5 Decision Tree (DT) classi-fier (Quinlan, 1993) in the Weka implementation(Witten and Frank, 2005).
These features corre-spond to the expected contexts for the differentnominal and adjectival lexical types.We have trained the DT with all the vectors ofthe word occurrences that we had in the differentgold-standards, using their encoding for the super-vised experiment in a 10-fold cross-validation test-ing (Bel et al 2007).3.3 Evaluation and ResultsFor the evaluation, we have applied both methodsto the lexical acquisition of nouns and adjectives.We have worked with a PoS tagged corpus of1,091,314 words.
Datasets of 496 adjectives and289 nouns were selected among the ones that hadoccurrences in the corpus.
Some manual selectionhad to be done in order to have all possible typesrepresented but still it roughly corresponds to thedistribution of features in the existing lexicon.We evaluated by comparing with Gold-standard files; i.e.
the manually encoded lexicon ofthe SRG.
The usual accuracy measures as typeprecision (percentage of feature values correctlyassigned to all values assigned) and type recall(percentage of correct feature values found in thedictionary) have been used.
F1 is the usual scorecombining precision and recall.Table 3 shows the results in terms of F1 scorefor the different methods and PoS for feature as-signment.
From these data, we concluded that theprobabilistic information inferred from the lexicaltypology defined in our grammar is a good sourceof knowledge for lexical acquisition.PoS noun adjZ 0.88 0.87DT 0.89 0.9Table 3.
F1 for different methods and PoS.Table 4 shows more details of the results compar-ing between DT and Z for Spanish adjectives.SF = no SF = yesZ DT Z DTprep_a 0.98 0.97 0.72 0.44prep_en 0.98 0.99 0.27 0prep_con 0.99 0.99 0.60 0prep_para 0.98 0.99 0.51 0.53prep_de 0.88 0.97 0.34 0.42postN 0 0 0.99 0.99preN 0.75 0.83 0.44 0.80Pred 0.50 0.41 0.59 0.82G 0.85 0.80 0.75 0.72Sent 0.97 0.97 0.55 0.44Table 4.
F1 for Spanish adjectival features.Finally, Table 5 shows the results for 50 Spanishnouns with only one occurrence in the corpus.These results show that grammatical features canbe used for lexical acquisition of low frequencylexical items, providing a good hypothesis for en-suring grammar robustness and adding no over-generation to parsing results.DT Zprec.
rec.
F prec.
rec.
FMASS 0.50 0.16 0.25 0.66 0.25 0.36COUNT 0.97 1.00 0.98 1.00 0.96 0.98TRANS 0.75 0.46 0.57 0.68 0.73 0.71INTRANS 0.85 0.95 0.89 0.89 0.76 0.82PCOMP 0 0 0 0.14 0.20 0.16Table 5.
Results of 50 unseen nouns with a sin-gle occurrence.4 Future WorkWe have presented work on the development of anHPSG grammar for Spanish; in particular, we havedescribed our approach for pre-processing and on-going research on automatic lexical acquisition.Besides extending the coverage of the SRG andcontinuing research on lexical acquisition, the spe-cific aims of our future work on the SRG are:?
Treebank development.110?
To extend the shallow/deep architectureand integrate the structures generated bypartial parsing, to provide robust techniquesfor infrequent structural constructions.
Thecoverage of these linguistic structures bymeans of structure rules would increase bothprocessing time and ambiguity.?
To use ML methods for disambiguation;i.e.
for ranking possible parsings accordingto relevant linguistic features, thus enablingthe setting of a threshold to select the n-bestanalyses.?
The development of error mining tech-niques (van Noord, 2004) to identify errone-ous and incomplete information in the lin-guistic resources which cause the grammarto fail.ReferencesJ.
Atserias, B. Casas, E. Comelles, M. Gonz?lez, L. Pa-dr?
and M. Padr?.
2006.
FreeLing 1.3: Syntactic andsemantic services in an open-source NLP library.
5thInternational Conference on Language Resourcesand Evaluation.
Genoa, Italy.T.
Baldwin.
2005.
Bootstrapping Deep Lexical Re-sources: Resources for Courses, ACL-SIGLEX 2005.Workshop on Deep Lexical Acquisition.
Ann Arbor,Michigan.N.
Bel, S. Espeja, M. Marimon.
2007.
Automatic Ac-quisition of Grammatical Types for Nouns.
HumanLanguage Technologies: The Annual Conference ofthe North American Chapter of the Association forComputational Linguistics.
Rochester, NY, USA,E.M.
Bender, D. Flickinger and S. Oepen.
2002.
Thegrammar Matrix.
An open-source starter-kit for therapid development of cress-linguistically consistentbroad-coverage precision grammar.
Workshop onGrammar Engineering and Evaluation, 19th Interna-tional Conference on Computational Linguistics.Taipei, Taiwan.P.
Blunsom and T. Baldwin.
2006.
Multilingual DeepLexical Acquisition for HPSGs via Supertagging.Conference on Empirical Methods in Natural Lan-guage Processing.
Sydney, Australia.T.
Brants.
2000.
TnT: A statistical part-of-speech tag-ger.
6th Conference on Applied Natural LanguageProcessing.
Seattle, USA.T.
Cabr?
and C. Bach, 2004.
El corpus t?cnic del?IULA: corpus textual especializado pluriling?e.Panacea, V. 16, pages 173-176.U.
Callmeier.
2000.
Pet ?
a platform for experimenta-tion with efficient HPSG processing.
Journal ofNatural Language Engineering 6(1): Special Issueon Efficient Processing with HPSG: Methods, Sys-tem, Evaluation, pages 99-108.A.
Copestake, D. Flickinger, C. Pollard and I.A.
Sag.2006.
Minimal Recursion Semantics: An Introduc-tion.
Research on Language and Computation3.4:281-332.A.
Copestake.
2002.
Implementing Typed FeaturesStructure Grammars.
CSLI Publications.A.
Korhonen.
2002.
?Subcategorization acquisition?.
AsTechnical Report UCAM-CL-TR-530, University ofCambridge, UK.M.
Marimon.
2002.
Integrating Shallow LinguisticProcessing into a Unification-based Spanish Gram-mar.
9th International Conference on ComputationalLinguistics.
Taipei, Taiwan.M.
Marimon and N. Bel.
2004.
Lexical Entry Templatesfor Robust Deep Parsing.
4th International Confer-ence on Language Resources and Evaluation.
Lis-bon, Portugal.S.
Oepen and J. Carroll.
2000.
Performance Profiling forParser Engineering.
Journal of Natural LanguageEngineering 6(1): Special Issue on Efficient Process-ing with HPSG: Methods, System, Evaluation, pages81-97.C.J.
Pollard and I.A.
Sag.
1994.
Head-driven PhraseStructure Grammar.
The University of ChicagoPress, Chicago.R.J.
Quinlan 1993.
C4.5: Programs for Machine Learn-ing.
Series in Machine Learning.
Morgan Kaufman,San Mateo, CA.C.
Samuelson.
1993.
Morphological tagging based en-tirely on Bayesian inference.
9th Nordic Conferenceon Computational Linguistics.
Stockholm, Sweden.I.H.
Witten and E. Frank.
2005.
Data Mining: Practicalmachine learning tools and techniques.
MorganKaufmann, San Francisco.G.
van Noord.
2004.
Error mining for wide-coveragegrammar engineering.
42th Annual Meeting of theACL.
Barcelona, Spain.Y.
Zhang and V. Kordoni.
2006.
Automated deep lexi-cal acquisition for robust open text processing.
5thInternational Conference on Language Resourcesand Evaluation.
Genoa, Italy.111
