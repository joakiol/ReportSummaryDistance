Learning Surface Text Patternsfor a Question Answering SystemDeepak Ravichandran and Eduard HovyInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292-6695USA{ravichan,hovy}@isi.eduAbstractIn this paper we explore the power ofsurface text patterns for open-d mainquestion answering systems.
In order toobtain an optimal set of patterns, we havedeveloped a method for learning suchpatterns automatically.
A tagged corpusis built from the Internet in abootstrapping process by providing a fewhand-crafted examples of each questiontype to Altavista.
Patterns are thenautomatically extracted from the returneddocuments and standardized.
Wecalculate the precision of each pattern,and the average precision for eachquestion type.
These patterns are thenapplied to find answers to new questions.Using the TREC-10 question set, wereport results for two cases: answersdetermined from the TREC-10 corpusand from the web.1 IntroductionMost of the recent open domain question-answering systems use external knowledgeand tools for answer pinpointing.
These mayinclude named entity taggers, WordNet,parsers, hand-tagged corpora, and ontologylists (Srihari and Li, 00; Harabagiu et al, 01;Hovy et al, 01; Prager et al, 01).
However, atthe recent TREC-10 QA evaluation(Voorhees, 01), the winning system used justone resource: a fairly extensive list of surfacepatterns (Soubbotin and Soubbotin, 01).
Theapparent power of such patterns surprisedmany.
We therefore decided to investigatetheir potential by acquiring patternsautomatically and to measure their accuracy.It has been noted in several QA systemsthat certain types of answer are expressedusing characteristic phrases (Lee et al, 01;Wang et al, 01).
For example, forBIRTHDATEs (with questions like ?Whenwas X born??
), typical answers are?Mozart was born in 1756.?
?Gandhi (1869?
1948)?
?These examples suggest that phrases like?<NAME> was born in <BIRTHDATE>?
?<NAME> (<BIRTHDATE>?
?when formulated as regular expressions, canbe used to locate the correct answer.In this paper we present an approach forautomatically learning such regularexpressions (along with determining theirprecision) from the web, for given types ofquestions.
Our method uses the machinelearning technique of bootstrapping to build alarge tagged corpus starting with only a fewexamples of QA pairs.
Similar techniqueshave been investigated extensively in the fieldof information extraction (Riloff, 96).
Thesetechniques are greatly aided by the fact thatthere is no need to hand-tag a corpus, whilethe abundance of data on the web makes iteasier to determine reliable statisticalestimates.Our system assumes each sentence to be asimple sequence of words and searche  forrepeated word orderings as evidence forComputational Linguistics (ACL), Philadelphia, July 2002, pp.
41-47.Proceedings of the 40th Annual Meeting of the Association foruseful answer phrases.
We use suffix treesfor extracting substrings of optimal length.We borrow the idea of suffix trees fromcomputational biology (Gusfield, 97) where itis primarily used for detecting DNAsequences.
Suffix trees can be processed intime linear on the size of the corpus and, moreimportantly, they do not restrict the length ofsubstrings.
We then test the patterns learnedby our system on new unseen questions fromthe TREC-10 set and evaluate their results todetermine the precision of the patterns.2 Learning of PatternsWe describe the pattern-lear ing algorithmwith an example.
A table of patterns isconstructed for each individual question typeby the following procedure (Algorithm 1).1.
Select an example for a given questiontype.
Thus for BIRTHYEAR questions weselect ?Mozart 1756?
(we refer to?Mozart?
as the question term and ?1756?as the answer term).2.
Submit the question and the answer termas queries to a search engine.
Thus, wegive the query +?Mozart?
+?1756?
toAltaVista (http://www.altavista.com).3.
Download the top 1000 web documentsprovided by the search engine.4.
Apply a sentence breaker to thedocuments.5.
Retain only those sentences that containboth the question and the answer term.Tokenize the input text, smooth variationsin white space characters, and remove htmland other extraneous tags, to allow simpleregular expression matching tools such asegrep to be used.6.
Pass each retained sentence through asuffix tree construc or.
This finds allsubstrings, of all lengths, along with theircounts.
For example consider thesentences ?The great composer Mozart(1756?
1791) achieved fame at a youngage?
?Mozart (1756?
1791) was a genius?,and ?The whole world would always beindebted to the great music of Mozart(1756?
1791)?.
The longest matchingsubstring for all 3 sentences is ?Mozart(1756?
1791)?, which the suffix tree wouldextract as one of the outputs along with thescore of 3.7.
Pass each phrase in the suffix tree througha filter to retain only those phrases thatco tain both the question and the answerterm.
For the example, we extract onlythose phrases from the suffix tree thatcontain the words ?Mozart?
and ?1756?.8.
Replace the word for the question term bythe tag ?<NAME>?
and the word for theanswer term by the term ?<ANSWER>?.This procedure is repeated for differentexamples of the same question type.
ForBIRTHDATE we also use ?Gandhi 1869?,?Newton 1642?, etc.For BIRTHDATE, the above stepsproduce the following output:a. born in <ANSWER> , <NAME>b.
<NAME> was born on <ANSWER> ,c. <NAME> ( <ANSWER> -d. <NAME> ( <ANSWER -  )...These are some of the most commonsubstrings of the extracted sentences thatcontain both <NAME> and <ANSWER>.Since the suffix tree records all substrings,partly overlapping strings such as c and d areseparately saved, which allows us to obtainseparate counts of their occurrencefr quencies.
As will be seen later, this allowsus to differentiate patterns such as d (whichrecords a still living person, and is quiteprecise) from its more general substring c(which is less precise).Algorithm 2: Calculating the precision of eachpattern.1.
Query the search engine by using only thequestion term (in the example, only?Mozart?).2.
Download the top 1000 web documentsprovided by the search engine.3.
As before, segment these documents intoindividual sentences.4.
Retain only those sentences that containthe question term.5.
For each pattern obtained from Algorithm1, check the presence of each pattern in thesentence obtained from above for twoinstances:i) Presence of the pattern with<ANSWER> tag matched by anyword.ii) Presence of the pattern in the sentencewith <ANSWER> tag matched by thecorrect answer term.In our example, for the pattern ?<NAME>was born in <ANSWER>?
we check thepresence of the following strings in theanswer sentencei) Mozart was born in <ANY_WORD>ii) Mozart was born in 1756Calculate the precision of each pattern bythe formula P = Ca / o whereCa = total number of patterns with theanswer term presentCo = total number of patterns presentwith answer term replaced by any word6.
Retain only the patterns matching asufficient number of examples (we choosethe number of examples > 5).We obtain a table of regular expressionpatterns for a given question type, along withthe precision of each pattern.
This precisionis the probability of each pattern containingthe answer and follows directly from theprinciple of maximum likelihood estimation.For BIRTHDATE the following table isobtained:1.0  <NAME>( <ANSWER> -  )0.85  <NAME> was born on <ANSWER>,0.6  <NAME> was born in <ANSWER>0.59  <NAME> was born <ANSWER>0.53  <ANSWER> <NAME> was born0.50  ?
<NAME> ( <ANSWER>0.36 <NAME> ( <ANSWER> -For a given question type a good range ofpatterns was obtained by giving the system asfew as 10 examples.
The rather long list ofpatterns obtained would have been verydifficult for any human to come up withmanually.The question term could appear in thedocuments obtained from the web in variousways.
Thus ?Mozart?
could be written as?Wolfgang Amadeus Mozart?, ?Mozart,Wolfgang Amadeus?, ?Amadeus Mozart?
or?Mozart?.
To learn from such variations, instep 1 of Algorithm 1 we specify the variousways in which the question term could bespecified in the text.
The presence of any ofthese names would cause it to be tagged as theoriginal question term ?Mozart?.The same arrangement is also done for theanswer term so that presence of any variant ofthe answer term would cause it to be treatedexactly like the original answer term.
Whileeasy to do for BIRTHDATE, this step can beproblematic for question types such asDEFINITION, which may contain variousacceptable answers.
In general the inputexample terms have to be carefully selectedso that the questions they represent do nothave a long list of possible answers, as thiswould affect the confidence of the precisionscores for each pattern.
All the answers needto be enlisted to ensure a high confidence inthe precision score of each pttern, in thepresent framework.The precision of the patterns obtainedfrom one QA-pair example in algorithm 1 iscalculated from the documents obtained inalgorithm 2 for other examples of the samequestion type.
In other words, the precisionscores are calculated by cross-checking thepatterns across various examples of the sametyp .
This step proves to be very significantas it helps to eliminate dubious patterns,which may appear because the contents oftwo or more websites may be the same, or thesame web document reappears in the searchengine output for algorithms 1 and 2.Algorithm 1 does not explicitly specifyany particular question type.
Judicious choiceof the QA example pair therefore allows it tobe used for many question types withoutchange.3 Finding AnswersUsing the patterns to answer a new questionwe employ the following algorithm:1.
Determine the question type of the newquestion.
We use our existing QA system(Hovy et al, 2002b; 2001) to do so.2.
The question term in the question isidentified, also using our existing system.3.
Create a query from the question term andperform IR (by using a given answerdocument corpus such as the TREC-10collection or web search otherwise).4.
Segment the documents obtained intosentences and smooth out white spacevariations and html and other tags, asbefore.5.
Replace the question term in each sentenceby the question tag (?<NAME>?, in thecase of BIRTHYEAR).6.
Using the pattern table developed for thatparticular question type, search for thepresence of each pattern.
Select wordsmatching the tag ?<ANSWER>?
as theanswer.7.
Sort these answers by their pattern?sprecision scores.
Discard duplicates (byelementary string comparisons).
Returnthe top 5 answers.4 ExperimentsFrom our Webclopedia QA Typology(Hovy et al, 2002a) we selected 6 differentquestion types: BIRTHDATE, LOCATION,INVENTOR, DISCOVERER, DEFINITION,WHY-FAMOUS.
The pattern table for eachof these question types was constructed usingAlgorithm 1.Some of the patterns obtained long withtheir precision are as followsBIRTHYEAR1.0 <NAME> ( <ANSWER> - )0.85 <NAME> was born on <ANSWER> ,0.6 <NAME> was born in <ANSWER>0.59 <NAME> was born <ANSWER>0.53 <ANSWER> <NAME> was born0.5 - <NAME> ( <ANSWER>0.36 <NAME> ( <ANSWER> -0.32 <NAME> ( <ANSWER> ) ,0.28 born in <ANSWER> , <NAME>0.2 of <NAME> ( <ANSWER>INVENTOR1.0 <ANSWER> invents <NAME>1.0 the <NAME> was invented by<ANSWER>1.0 <ANSWER> invented the <NAME> in1.0 <ANSWER> ' s invention of the<NAME>1.0 <ANSWER> invents the <NAME> .1.0 <ANSWER> ' s <NAME> was1.0 <NAME> , invented by <ANSWER>1.0 <ANSWER> ' s <NAME> and1.0 that <ANSWER> ' s <NAME>1.0 <NAME> was invented by <ANSWER> ,DISCOVERER1.0 when <ANSWER> discovered<NAME>1.0 <ANSWER> ' s discovery of <NAME>1.0 <ANSWER> , the discoverer of<NAME>1.0 <ANSWER> discovers <NAME> .1.0 <ANSWER> discover <NAME>1.0 <ANSWER> discovered <NAME> , the1.0 discovery of <NAME> by <ANSWER>.0.95 <NAME> was discovered by<ANSWER>0.91 of <ANSWER> ' s <NAME>0.9 <NAME> was discovered by<ANSWER> inDEFINITION1.0 <NAME> and related <ANSWER>s1.0 <ANSWER> ( <NAME> ,1.0 <ANSWER> , <NAME> .1.0 , a <NAME> <ANSWER> ,1.0 ( <NAME> <ANSWER> ) ,1.0 form of <ANSWER> , <NAME>1.0 for <NAME> , <ANSWER> and1.0 cell <ANSWER> , <NAME>1.0 and <ANSWER> > <ANSWER> ><NAME>0.94 as <NAME> , <ANSWER> andWHY-FAMOUS1.0 <ANSWER> <NAME> called1.0 laureate <ANSWER> <NAME>1.0 by the <ANSWER> , <NAME> ,1.0 <NAME> - the <ANSWER> of1.0 <NAME> was the <ANSWER> of0.84 by the <ANSWER> <NAME> ,0.8 the famous <ANSWER> <NAME> ,0.73 the famous <ANSWER> <NAME>0.72 <ANSWER> > <NAME>0.71 <NAME> is the <ANSWER> ofLOCATION1.0 <ANSWER> ' s <NAME> .1.0 regional : <ANSWER> : <NAME>1.0 to <ANSWER> ' s <NAME> ,1.0 <ANSWER> ' s <NAME> in1.0 in <ANSWER> ' s <NAME> ,1.0 of <ANSWER> ' s <NAME> ,1.0 at the <NAME> in <ANSWER>0.96 the <NAME> in <ANSWER> ,0.92 from <ANSWER> ' s <NAME>0.92 near <NAME> in <ANSWER>For each question type, we extracted thecorresponding questions from the TREC-10set.
These questions were run through thetesting phase of the algorithm.
Two sets ofexperiments were performed.
In the firstcase, the TREC corpus was used as the inputsource and IR was performed by the IRcomponent of our QA system (Lin, 2002).
Inthe second case, the web was the input sourceand the IR was performed by the AltaVistasearch engine.Results of the experiments, measured byMean Reciprocal Rank (MRR) score(Voorhees, 01), are:TREC CorpusQuestion type Number ofquestionsMRR onTREC docsBIRTHYEAR 8 0.48INVENTOR 6 0.17DISCOVERER 4 0.13DEFINITION 102 0.34WHY-FAMOUS 3 0.33LOCATION 16 0.75WebQuestion type Number ofquestionsMRR on theWebBIRTHYEAR 8 0.69INVENTOR 6 0.58DISCOVERER 4 0.88DEFINITION 102 0.39WHY-FAMOUS 3 0.00LOCATION 16 0.86The results indicate that the systemperforms better on the Web data than on theTREC corpus.
The abundance of data on theweb makes it easier for the system to locateanswers with high precision scores (thesystem finds many examples of correctanswers among the top 20 when using theWeb as the input source).
A similar result forQA was obtained by Brill et al (2001).
TheTREC corpus does not have enough candidateanswers with high precision score and has tosettle for answers extracted from sentencesmatched by low precision patterns.
TheWHY-FAMOUS question type is anexception and may be due to the fact that thesystem was tested on a small number ofquestions.5 Shortcoming and ExtensionsNo external knowledge has been added tothese patterns.
We frequently observe theneed for matching part of speech and/orsemantic types, however.
For example, thequestion: ?Where are the Rocky Mountainslocated??
is answered by ?Denver?s newairport, topped with white fiberglass cones inimitation of the Rocky Mountains in thebackground, continues to lie empty?, becausethe system picked the answer ?thebackground?
using the pattern ?the <NAME>in <ANSWER>,?.
Using a named entitytagger and/or an ontology would enable thesystem to use the knowledge that?background?
is not a location.DEFINITION questions pose a relatedproblem.
Frequently the system?s patternsmatch a term that is too general, thoughcorrect technically.
For ?what is nepotism?
?the pattern ?<ANSWER>, <NAME>?matches ?
?in the form of widespreadbureaucratic abuses: graft, nepotism??
; for?what is sonar??
the pattern ?<NAME> andrelated <ANSWER>s?
matches ?
?while itssonar and related underseas systems arebuilt?
?.The patterns cannot handle long-distancedependencies.
For example, for ?Where isLondon??
the system cannot locate the answerin ?London, which has one of the most busiestairports in the world, lies on the banks of theriver Thames?
due to the explosive danger ofunrestricted wildcard matching, as would berequired in the pattern ?<QUESTION>,(<any_word>)*, lies on <ANSWER>?.
Thisis one of the reasons why the system performsvery well on certain types of questions fromthe web but performs poorly with documentsobtained from the TREC corpus.
Theabundance and variation of data on theInternet alows the system to find an instanceof its patterns without losing answers to long-term dependencies.
The TREC corpus, on theother hand, typically contains fewer candidateanswers for a given question and many of theanswers present may match only long-termdependency patterns.More information needs to be added to thetext patterns regarding the length of theanswer phrase to be expected.
The systemsearches in the range of 50 bytes of theanswer phrase to capture the pattern.
It fails toperform under certain conditions asexemplified by the question ?When wasLyndon B. Johnson born??.
The systemselects the sentence ?Tower gained nationalattention in 1960 when he lost to democraticSen.
Lyndon B. Johnson, who ran for both re-election and the vice presidency?
using thepattern ?<NAME> <ANSWER> ?
?.
Thesystem lacks the information that the<ANSWER> tag should be replaced exactlyby one word.
Simple extensions could bemade to the system so that instead ofsearching in the range of 50 bytes for theanswer phrase it could search for the answerin the range of 1?
2 chunks (basic phrases inEnglish such as simple NP, VP, PP, etc.
).A more serious limitat on is that thepresent framework can handle only oneanchor point (the question term) in thecandidate answer sentence.
It cannot work fortypes of question that require multiple wordsfrom the question to be in the answersentence, possibly apart from each other.
Forexample, in ?Which county does the city ofLong Beach lie?
?, the answer ?Long Beach issituated in Los Angeles County?
requires thepattern.
?<QUESTION_TERM_1> situated in<ANSWER> <QUESTION_TERM_2>?,where <QUESTION_TERM_1> and<QUESTION_TERM_2> represent the terms?Long Beach?
and ?county?
respectively.The performance of the system dependssignificantly on there being only one anchorword, which allows a single word matchbetween the question and the candidateanswer sentence.
The presence of multipleanchor words would help to eliminate manyof the candidate answers by simply using thecondition that all the anchor words from thequestion must be present in the candidateanswer sentence.The system does not classify or make anydistinction between upper and lower caselet ers.
For example, ?What is micron??
isanswered by ?In Boise, Idaho, a spokesmanfor Micron, a maker of semiconductors, saidSimms are ?
a very high volume product forus ??
?.
The answer returned by the systemwould have been perfect if the word ?micron?had been capitalized in the question.Canonicalization of words is also an issue.While giving examples in the bootstrappingprocedure, say, for BIRTHDATE questions,the answer term could be written in manyways (for example, Gandhi?s birth date can bewr tten as ?1869?, ?Oct.
2, 1869?, ?2ndOctober 1869?,  ?October 2 1869?, and soon).
Instead of enlisting all the possibilities adate tagger could be used to cluster all thevariations and tag them with the same term.The same idea could also be extended forsmoothing out the variations in the questionterm for names of persons (Gandhi could bewritten as ?Mahatma Gandhi?, ?MohandasKaramchand Gandhi?, etc.
).6 ConclusionThe web results easily outperform theTREC results.
This suggests that there is aneed to integrate the outputs of the Web andthe TREC corpus.
Since the output from theWeb contains many correct answers amongthe top ones, a simple word count could helpin eliminating many unlikely answers.
Thiswould work well for question types likeBIRTHDATE or LOCATION but is not clearfor question types like DEFINITION.The simplicity of this method makes itperfect for multilingual QA.
Many toolsrequired by sophisticated QA systems (namedentity taggers, parsers, ontologies, etc.)
arelanguage specific and require significanteffort to adapt to a new language.
Since theanswer patterns used in this method arelearned using only a small number of manualtraining terms, one can rapidly learn patternsfor new languages, assuming the web searchengine is appropriately switched.AcknowledgementsThis work was supported by the AdvancedResearch and Development Activity(ARDA)'s Advanced Question Answering forIntelligence (AQUAINT) Program undercontract number MDA908-02-C-0007.ReferencesBrill, E., J. Lin, M. Banko, S. Dumais, and A. Ng.2001.
Data-Intensive Question Answering.Proceedings of the TREC-10 Conference.NIST, Gaithersburg, MD, 183?
9.Gusfield, D. 1997.
Algorithms on Strings, Treesand Sequences: Computer Science andComputational Biology.
Chapter 6: LinearTime construction of Suffix trees, 94?
121.Harabagiu, S., D. Moldovan, M. Pasca, R.Mihalcea, M. Surdeanu, R. Buneascu, R. G?rju,V.
Rus and P. Morarescu.
2001.
FALCON:Boosting Knowledge for Answer Engines.Proceedings of the 9th Text RetrievalConference (TREC-9), NIST, 479?
488.Hovy, E.H., U. Hermjakob, and C.-Y.
Lin.
2001.The Use of External Knowledge in FactoidQA.
Proceedings of the TREC-10Conference.
NIST, Gaithersburg, MD, 166?174.Hovy, E.H., U. Hermjakob, and D. Ravichandran.2002a.
A Question/Answer Typology withSurface Text Patterns.
Proceedings of theHuman Language Technology (HLT)conference.
San Diego, CA.Hovy, E.H., U. Hermjakob, C.-Y.
Lin, and D.Ravichandran.
2002b.
Using Knowledge toFacilitate Pinpointing of Factoid Answers.Proceedings of the COLING-2002 conference.Taipei, Taiwan.Lee, G.G., J. Seo, S. Lee, H. Jung, B-H. Cho, C.Lee, B-K. Kwak, J, Cha, D. Kim, J-H. An, H.Kim, and K. Kim.
2001.
SiteQ: EngineeringHigh Performance QA System Using Lexico-Semantic Pattern Matching and Shallow NLP.Proceedings of the TREC-10 Conference.NIST, Gaithersburg, MD, 437?
46.Lin, C-Y.
2002.
The Effectiveness of Dictionaryand Web-Based Answer Reranking.Proceedings of the COLING-2002 conference.Taipei, Taiwan.Prager, J. and J. Chu-Carroll.
2001.
Use ofWordNet Hypernyms for Answering What-IsQuestions.
Proceedings of the TREC-10Conference.
NIST, Gaithersburg, MD, 309?316.Riloff, E. 1996.
Automatically GeneratingExtraction Patterns from Untagged Text.Proceedings of the Thirteenth NationalConference on Artificial Intelligence (AAAI-96), 1044?
1049.Soubbotin, M.M.
and S.M.
Soubbotin.
2001.Patterns of Potential Answer Expressions asClues to the Right Answer.
Proceedings of theTREC-10 Conference.
NIST, Gaithersburg,MD, 175?
182.Srihari, R. and W. Li.
2000.
A QuestionAnswering System Supported by InformationExtraction.
Proceedings of the 1st Meeting ofthe North American Chapter of the Associationfor Computational Linguistics (ANLP-NAACL-00), Seattle, WA, 166?
172.Voorhees, E. 2001.
Overview of the QuestionAnswering Track.
Proceedings of the TREC-10Conference.
NIST, Gaithersburg, MD, 157?165.Wang, B., H. Xu, Z. Yang, Y. Liu, X. Cheng, D.Bu, and S. Bai.
2001.
TREC-10 Experiments atCAS-ICT: Filtering, Web, and QA.Proceedings of the TREC-10 Conference.NIST, Gaithersburg, MD, 229?
41.
