Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 54?59,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsPattern Learning for Relation Extraction with a Hierarchical Topic ModelEnrique Alfonseca Katja Filippova Jean-Yves DelortGoogle ResearchBrandschenkestrasse 1108002 Zurich, Switzerland{ealfonseca,katjaf,jydelort}@google.comGuillermo Garrido?NLP & IR Group, UNEDJuan del Rosal, 16.28040 Madrid, Spainggarrido@lsi.uned.esAbstractWe describe the use of a hierarchical topicmodel for automatically identifying syntacticand lexical patterns that explicitly state on-tological relations.
We leverage distant su-pervision using relations from the knowledgebase FreeBase, but do not require any man-ual heuristic nor manual seed list selections.Results show that the learned patterns can beused to extract new relations with good preci-sion.1 IntroductionThe detection of relations between entities for theautomatic population of knowledge bases is veryuseful for solving tasks such as Entity Disambigua-tion, Information Retrieval and Question Answer-ing.
The availability of high-coverage, general-purpose knowledge bases enable the automatic iden-tification and disambiguation of entities in textand its applications (Bunescu and Pasca, 2006;Cucerzan, 2007; McNamee and Dang, 2009; Kwoket al, 2001; Pasca et al, 2006; Weld et al, 2008;Pereira et al, 2009; Kasneci et al, 2009).Most early works in this area were designedfor supervised Information Extraction competitionssuch as MUC (Sundheim and Chinchor, 1993) andACE (ACE, 2004; Doddington et al, 2004; Li etal., 2011), which rely on the availability of anno-tated data.
Open Information Extraction (Sekine,2006; Banko et al, 2007; Bollegala et al, 2010)started as an effort to approach relation extraction in?Work done during an internship at Google Zurich.a completely unsupervised way, by learning regular-ities and patterns from the web.
Two example sys-tems implementing this paradigm are TEXTRUN-NER (Yates et al, 2007) and REVERB (Fader et al,2011).
These systems do not need any manual dataor rules, but the relational facts they extract are notimmediately disambiguated to entities and relationsfrom a knowledge base.A different family of unsupervised methods forrelation extraction is unsupervised semantic pars-ing, which aims at clustering entity mentions andrelation surface forms, thus generating a semanticrepresentation of the texts on which inference maybe used.
Some techniques that have been used areMarkov Random Fields (Poon and Domingos, 2009)and Bayesian generative models (Titov and Klemen-tiev, 2011).
These are quite powerful approachesbut have very high computational requirements (cf.
(Yao et al, 2011)).A good trade-off between fully supervised andfully unsupervised approaches is distant supervi-sion, a semi-supervised procedure consisting of find-ing sentences that contain two entities whose rela-tion we know, and using those sentences as train-ing examples for a supervised classifier (Hoffmannet al, 2010; Wu and Weld, 2010; Hoffmann et al,2011; Wang et al, 2011; Yao et al, 2011).
A usualproblem is that two related entities may co-occur inone sentence for many unrelated reasons.
For ex-ample, Barack Obama is the president of the UnitedStates, but not every sentence including the two en-tities supports and states this relation.
Much of theprevious work uses heuristics, e.g.
extracting sen-tences only from encyclopedic entries (Mintz et al,542009; Hoffmann et al, 2011; Wang et al, 2011), orsyntactic restrictions on the sentences and the entitymentions (Wu and Weld, 2010).
These are usuallydefined manually and may need to be adapted to dif-ferent languages and domains.
Manually selectedseeds can also be used (Ravichandran and Hovy,2002; Kozareva and Hovy, 2010).The main contribution of this work is presentinga variant of distance supervision for relation extrac-tion where we do not use heuristics in the selectionof the training data.
Instead, we use topic models todiscriminate between the patterns that are expressingthe relation and those that are ambiguous and can beapplied across relations.
In this way, high-precisionextraction patterns can be learned without the needof any manual intervention.2 Unsupervised relational pattern learningSimilar to other distant supervision methods, our ap-proach takes as input an existing knowledge basecontaining entities and relations, and a textual cor-pus.
In this work it is not necessary for the corpusto be related to the knowledge base.
In what followswe assume that all the relations studied are binaryand hold between exactly two entities in the knowl-edge base.
We also assume a dependency parser isavailable, and that the entities have been automat-ically disambiguated using the knowledge base assense inventory.One of the most important problems to solve indistant supervision approaches is to be able to dis-tinguish which of the textual examples that includetwo related entities, ei and ej , are supporting the re-lation.
This section describes a fully unsupervisedsolution to this problem, computing the probabilitythat a pattern supports a given relation, which willallow us to determine the most likely relation ex-pressed in any sentence.
Specifically, if a sentencecontains two entities, ei and ej , connected through apattern w, our model computes the probability thatthe pattern is expressing any relation ?P (r|w)?
forany relation r defined in the knowledge base.
Notethat we refer to patterns with the symbol w, as theyare the words in our topic models.Preprocessing As a first step, the textual corpusis processed and the data is transformed in the fol-lowing way: (a) the input corpus is parsed and en-Author-book(Mark Twain, Adventures of HuckleberryFinn)ARG1poss,,ARG2ARG1nn,,novelsnn,,ARG2ARG1nsubj--released ARG2dobjqqARG2 ARG1conjrrARG1nsubj,,wrote ARG2dobjrrARG1poss,,ARG2...(Jhumpa Lahiri, The Namesake)ARG1nn--ARG2ARG2 byprepqqARG1nnuuARG1nn,,novelappos--ARG2ARG2 byprepqqARG1nnuuARG2 byprepqqARG1nnuuARG1poss--ARG1...(...)Person-parent(Liza Minneli, Judy Garland)...(Achilles, Peleus)...(...)Person-death place(Napoleon Bonaparte, SaintHelena)...(Johann Christian Bach, Lon-don)...(...)Person-birth place(Charles Darwin, Shrewsbury)...(Anthony Daniels, Salisbury)...(...)Figure 1: Example of a generated set of document collec-tions from a news corpus for relation extraction.
Largerboxes are document collections (relations), and innerboxes are documents (entity pairs).
Document containdependency patterns, which are words in the topic model.tities are disambiguated; (b) for each relation r inthe knowledge base, a new (initially empty) docu-ment collection Cr is created; (c) for each entity pair(ei, ej) which are related in the knowledge base, anew (initially empty) document Dij is created; (d)for each sentence in the input corpus containing onemention of ei and one mention of ej , a new term isadded to Dij consisting of the context in which thetwo entities were seen in the document.
This contextmay be a complex structure, such as the dependencypath joining the two entities, but it is considered forour purposes as a single term; (e) for each relation rrelating ei with ej , document Dij is added to collec-tion Cr.
Note that if the two entities are related indifferent ways at the same time, an identical copy ofthe document Dij will be added to the collection forall those relations.Figure 1 shows a set of document collections gen-55Figure 2: Plate diagram of the generative model used.erated for three relations using this procedure.
Eachrelation r has associated a different document col-lection, which contains one document associated toeach entity pair from the knowledge base which isin relation r. The words in each document can be,for example, all the dependency paths that have beenobserved in the input textual corpus between the tworelated entities.
Each document will contain somevery generic paths (e.g.
the two entities consecutivein the text) and some more specific paths.Generative model Once these collections arebuilt, we use the generative model from Figure 2to learn the probability that a dependency path isconveying some relation between the entities it con-nects.
This model is very similar to the one usedby Haghighi and Vanderwende (2009) in the con-text of text summarization.
w (the observed vari-able) represents a pattern between two entities.
Thetopic model ?G captures general patterns that appearfor all relations.
?D captures patterns that are spe-cific about a certain entity pair, but which are notgeneralizable across all pairs with the same relation.Finally ?A contains the patterns that are observedacross most pairs related with the same relation.
?Ais the topic model of interest for us.We use Gibbs sampling to estimate the differentmodels from the source data.
The topic assignments(for each pattern) that are the output of this processare used to estimate P (r|w): when we observe pat-tern w, the probability that it conveys relation r.3 Experiments and resultsSettings We use Freebase as our knowledge base.It can be freely downloaded1.
text corpus used con-tains 33 million English news articles that we down-loaded between January 2004 and December 2011.A random sample of 3M of them is used for buildingthe document collections on which to train the topicmodels, and the remaining 30M is used for testing.The corpus is preprocessed by identifying Freebaseentity mentions, using an approach similar to (Milneand Witten, 2008), and parsing it with an inductivedependency parser (Nivre, 2006).From the three million training documents, a setof document collections (one per relation) has beengenerated, by considering the sentences that containtwo entities which are related in FreeBase throughany binary relation and restricting to high-frequency200 relations.
Two ways of extracting patterns havebeen used: (a) Syntactic, taking the dependencypath between the two entities, and (b) Intertext,taking the text between the two.
In both cases, atopic model has been trained to learn the probabil-ity of a relation given a pattern w: p(r|w).
For ?we use symmetric Dirichlet priors ?G = 0.1 and?D = ?A = 0.001, following the intuition that forthe background the probability mass across patternsshould be more evenly distributed.
?
is set as (15,15, 1), indicating in the prior that we expect morepatterns to belong to the background and entity-pair-specific distributions due to the very noisy nature ofthe input data.
These values have not been tuned.As a baseline, using the same training corpus, wehave calculated p(r|w) using the maximum likeli-hood estimate: the number of times that a pattern whas been seen connecting two entities for which rholds divided by the total frequency of the pattern.Extractions evaluation The patterns have beenapplied to the 30 million documents left for testing.For each pair of entities disambiguated as FreeBaseentities, if they are connected through a known pat-tern, they are assigned argmaxr p(r|w).
We haverandomly sampled 4,000 such extractions and sentthem to raters.
An extraction is to be judged cor-rect if both it is correct in real life and the sentencefrom which it was extracted really supports it.
We1http://wiki.freebase.com/wiki/Data dumps56Figure 3: Evaluation of the extractions.
X-axis has the threshold for p(r|w), and Y-axis has the precision of the extractions as a percentage.have collected three ratings per example and takenthe majority decision.
There was disagreement for9.4% of the items on whether the sentence supportsthe relation, and for 20% of the items on whether therelation holds in the real world.The results for different thresholds of p(r|w) areshown in Figure 3.
As can be seen, the MLE base-lines (in red with syntactic patterns and green withintertext) perform consistently worse than the mod-els learned using the topic models (in pink and blue).The difference in precision, aggregated across all re-lations, is statistically significant at 95% confidencefor most of the thresholds.Extractions aggregation We can take advantageof redundancy on the web to calculate a support met-ric for the extractions.
In this experiment, for everyextracted relation (r, e1, e2), for every occurrenceof a pattern wi connecting e1 and e2, we add upp(r|wi).
Extractions that are obtained many timesand from high-precision patterns will rank higher.Table 1 describes the results of this aggregation.We have considered the top four highest-frequencyrelations for people.
After aggregating all the ex-tracted relations and ranking them by support, wehave divided the evaluation set into two parts: (a)for relations that were not already in FreeBase, weevaluate the precision; (b) for extractions that werealready in FreeBase, we take the top-confidence sen-tence identified and evaluate whether the sentenceis providing support to the relation.
For each ofthese, both syntactic patterns and intermediate-textpatterns have been evaluated.The results are very interesting: using syntax,Death place appears easy to extract new relationsand to find support.
The patterns obtained are quiteunambiguous, e.g.ARG1subj**died atprepvvhomepobjwwinprepuuARG2pobjwwRelation Unknown relations Known relationsCorrect relation P@50 Sentence support P@50Syntax Intertext Syntax IntertextParent 0.58 0.38 1.00 1.00Death place 0.90 0.68 0.98 0.94Birth place 0.38 0.56 0.54 0.98Nationality 0.86 0.78 0.34 0.40Table 1: Evaluation on aggregated extractions.On the other hand, birth place and nationality havevery different results for new relation acquisitionvs.
finding sentence support for new relations.
Thereason is that these relations are very correlated toother relations that we did not have in our trainingset.
In the case of birth place, many relations re-fer to having an official position in the city, such asmayor; and for nationality, many of the patterns ex-tract presidents or ministers.
Not having mayor orpresident in our initial collection (see Figure 1), thesupport for these patterns is incorrectly learned.
Inthe case of nationality, however, even though the ex-tracted sentences do not support the relation (P@50= 0.34 for intertext), the new relations extracted aremostly correct (P@50 = 0.86) as most presidents andministers in the real world have the nationality of thecountry where they govern.4 ConclusionsWe have described a new distant supervision modelwith which to learn patterns for relation extractionwith no manual intervention.
Results are promising,we could obtain new relations that are not in Free-Base with a high precision for some relation types.
Itis also useful to extract support sentences for knownrelations.
More work is needed in understandingwhich relations are compatible or overlapping andwhich ones can partially imply each other (such aspresident-country or born in-mayor).57ReferencesACE.
2004.
The automatic content extraction projects.http://projects.ldc.upenn.edu/ace.Michele Banko, Michael J. Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open infor-mation extraction from the web.
In IJCAI?07.D.T.
Bollegala, Y. Matsuo, and M. Ishizuka.
2010.
Rela-tional duality: Unsupervised extraction of semantic re-lations between entities on the web.
In Proceedings ofthe 19th international conference on World wide web,pages 151?160.
ACM.R.
Bunescu and M. Pasca.
2006.
Using encyclopedicknowledge for named entity disambiguation.
In Pro-ceedings of EACL, volume 6, pages 9?16.S.
Cucerzan.
2007.
Large-scale named entity disam-biguation based on wikipedia data.
In Proceedings ofEMNLP-CoNLL, volume 2007, pages 708?716.G.
Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,S.
Strassel, and R. Weischedel.
2004.
The automaticcontent extraction (ace) program?tasks, data, and eval-uation.
In Proceedings of LREC, volume 4, pages837?840.
Citeseer.A.
Fader, S. Soderland, and O. Etzioni.
2011.
Identify-ing relations for open information extraction.
In Pro-ceedings of Empirical Methods in Natural LanguageProcessing.A.
Haghighi and L. Vanderwende.
2009.
Exploring con-tent models for multi-document summarization.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 362?370.
Association for Computational Lin-guistics.R.
Hoffmann, C. Zhang, and D.S.
Weld.
2010.
Learning5000 relational extractors.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 286?295.
Association for Computa-tional Linguistics.R.
Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, andD.S.
Weld.
2011.
Knowledge-based weak supervisionfor information extraction of overlapping relations.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies-Volume 1, pages 541?550.
Asso-ciation for Computational Linguistics.G.
Kasneci, M. Ramanath, F. Suchanek, and G. Weikum.2009.
The yago-naga approach to knowledge discov-ery.
ACM SIGMOD Record, 37(4):41?47.Z.
Kozareva and E. Hovy.
2010.
Learning argumentsand supertypes of semantic relations using recursivepatterns.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 1482?1491.
Association for Computational Lin-guistics.C.
Kwok, O. Etzioni, and D.S.
Weld.
2001.
Scalingquestion answering to the web.
ACM Transactions onInformation Systems (TOIS), 19(3):242?262.D.
Li, S. Somasundaran, and A. Chakraborty.
2011.
Acombination of topic models with max-margin learn-ing for relation detection.P.
McNamee and H.T.
Dang.
2009.
Overview of the tac2009 knowledge base population track.
In Text Analy-sis Conference (TAC).D.
Milne and I.H.
Witten.
2008.
Learning to link withwikipedia.
In Proceeding of the 17th ACM conferenceon Information and knowledge management, pages509?518.
ACM.M.
Mintz, S. Bills, R. Snow, and D. Jurafsky.
2009.
Dis-tant supervision for relation extraction without labeleddata.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP: Volume 2-Volume 2, pages 1003?1011.
Association for Computational Linguistics.J.
Nivre.
2006.
Inductive dependency parsing.
InText, Speech and Language Technology, volume 34.Springer Verlag.M.
Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.2006.
Organizing and searching the world wide webof facts-step one: the one-million fact extraction chal-lenge.
In Proceedings of the National Conference onArtificial Intelligence, page 1400.
Menlo Park, CA;Cambridge, MA; London; AAAI Press; MIT Press;1999.F.
Pereira, A. Rajaraman, S. Sarawagi, W. Tunstall-Pedoe, G. Weikum, and A. Halevy.
2009.
An-swering web questions using structured data: dreamor reality?
Proceedings of the VLDB Endowment,2(2):1646?1646.H.
Poon and P. Domingos.
2009.
Unsupervised seman-tic parsing.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Process-ing: Volume 1-Volume 1, pages 1?10.
Association forComputational Linguistics.D.
Ravichandran and E. Hovy.
2002.
Learning surfacetext patterns for a question answering system.
In Pro-ceedings of the 40th Annual Meeting on Associationfor Computational Linguistics, pages 41?47.
Associa-tion for Computational Linguistics.S.
Sekine.
2006.
On-demand information extraction.
InProceedings of the COLING/ACL on Main conferenceposter sessions, pages 731?738.
Association for Com-putational Linguistics.Beth M. Sundheim and Nancy A. Chinchor.
1993.
Sur-vey of the message understanding conferences.
InHLT?93.58I.
Titov and A. Klementiev.
2011.
A bayesian model forunsupervised semantic parsing.
In The 49th AnnualMeeting of the Association for Computational Linguis-tics.C.
Wang, J.
Fan, A. Kalyanpur, and D. Gondek.
2011.Relation extraction with relation topics.
In Proceed-ings of Empirical Methods in Natural Language Pro-cessing.Daniel S. Weld, Fei Wu, Eytan Adar, Saleema Amershi,James Fogarty, Raphael Hoffmann, Kayur Patel, andMichael Skinner.
2008.
Intelligence in wikipedia.
InProceedings of the 23rd national conference on Artifi-cial intelligence, pages 1609?1614.
AAAI Press.F.
Wu and D.S.
Weld.
2010.
Open information extractionusing wikipedia.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 118?127.
Association for ComputationalLinguistics.L.
Yao, A. Haghighi, S. Riedel, and A. McCallum.
2011.Structured relation discovery using generative models.In Empirical Methods in Natural Language Process-ing (EMNLP).A.
Yates, M. Cafarella, M. Banko, O. Etzioni, M. Broad-head, and S. Soderland.
2007.
Textrunner: Open in-formation extraction on the web.
In Proceedings ofHuman Language Technologies: The Annual Confer-ence of the North American Chapter of the Associationfor Computational Linguistics: Demonstrations, pages25?26.
Association for Computational Linguistics.59
