Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 271?277,Dublin, Ireland, August 23-24, 2014.ECNU: One Stone Two Birds: Ensemble of Heterogenous Measures forSemantic Relatedness and Textual EntailmentJiang Zhao, Tian Tian Zhu, Man Lan?Department of Computer Science and TechnologyEast China Normal University51121201042,51111201046@ecnu.cn; mlan@cs.ecnu.edu.cn?AbstractThis paper presents our approach to se-mantic relatedness and textual entailmentsubtasks organized as task 1 in SemEval2014.
Specifically, we address two ques-tions: (1) Can we solve these two sub-tasks together?
(2) Are features proposedfor textual entailment task still effectivefor semantic relatedness task?
To addressthem, we extracted seven types of featuresincluding text difference measures pro-posed in entailment judgement subtask, aswell as common text similarity measuresused in both subtasks.
Then we exploitedthe same feature set to solve the both sub-tasks by considering them as a regressionand a classification task respectively andperformed a study of influence of differ-ent features.
We achieved the first and thesecond rank for relatedness and entailmenttask respectively.1 IntroductionDistributional Semantic Models (DSMs)(surveyedin (Turney et al., 2010)) exploit the co-occurrencesof other words with the word being modeled tocompute the semantic meaning of the word un-der the distributional hypothesis: ?similar wordsshare similar contexts?
(Harris, 1954).
Despitetheir success, DSMs are severely limited to modelthe semantic of long phrases or sentences sincethey ignore grammatical structures and logicalwords.
Compositional Distributional SemanticModels (CDSMs)(Zanzotto et al., 2010; Socher etThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/al., 2012) extend DSMs to sentence level to cap-ture the compositionality in the semantic vectorspace, which has seen a rapidly growing interestin recent years.
Although several CDSMs havebeen proposed, benchmarks are lagging behind.Previous work (Grefenstette and Sadrzadeh, 2011;Socher et al., 2012) performed experiments ontheir own datasets or on the same datasets whichare limited to a few hundred instances of very shortsentences with a fixed structure.To provide a benchmark so as to compare dif-ferent CDSMs, the sentences involving composi-tional knowledge task in SemEval 2014 (Marelli etal., 2014) develops a large dataset which is full oflexical, syntactic and semantic phenomena.
It con-sists of two subtasks: semantic relatedness task,which measures the degree of semantic relatednessof a sentence pair by assigning a relatedness scoreranging from 1 (completely unrelated) to 5 (veryrelated); and textual entailment (TE) task, whichdetermines whether one of the following three re-lationships holds between two given sentences Aand B: (1) entailment: the meaning of B can beinferred from A; (2) contradiction: A contradictsB; (3) neutral: the truth of B cannot be inferred onthe basis of A.Semantic textual similarity (STS) (Lintean andRus, 2012) and semantic relatedness are closelyrelated and interchangeably used in many liter-atures except that the concept of semantic simi-larity is more specific than semantic relatednessand the latter includes concepts as antonymy andmeronymy.
In this paper we regard the semanticrelatedness task as a STS task.
Besides, regardlessof the original intention of this task, we adoptedthe mainstream machine learning methods insteadof CDSMs to solve these two tasks by extractingheterogenous features.271Like semantic relatedness, TE task (surveyedin (Androutsopoulos and Malakasiotis, 2009)) isalso closely related to STS task since in TE tasklots of similarity measures at different levels areexploited to boost classification.
For example,(Malakasiotis and Androutsopoulos, 2007) usedten string similarity measures such as cosine sim-ilarity at the word and the character level.
There-fore, the first fundamental question arises, i.e.,?Can we solve both of these two tasks together?
?At the same time, since high similarity does notmean entailment holds, the TE task also utilizesother features besides similarity measures.
For ex-ample, in our previous work (Zhao et al., 2014)text difference features were proposed and provedto be effective.
Therefore, the second question sur-faces here, i.e., ?Are features proposed for TE taskstill effective for STS task??
To answer the firstquestion, we extracted seven types of features in-cluding text similarity and text difference and thenfed them to classifiers and regressors to solve TEand STS task respectively.
Regarding the secondquestion, we conducted a series of experimentsto study the performance of different features forthese two tasks.The rest of the paper is organized as follows.Section 2 briefly describes the related work onSTS and TE tasks.
Section 3 presents our systemsincluding features, learning methods, etc.
Section4 shows the experimental results on training dataand Section 5 reports the results of our submittedsystems on test data and gives a detailed analysis.Finally, Section 6 concludes this paper with futurework.2 Related WorkExisting work on STS can be divided into 4categories according to the similarity measuresused (Gomaa and Fahmy, 2013): (1) string-basedmethod (B?ar et al., 2012; Malakasiotis and An-droutsopoulos, 2007) which calculates similaritiesusing surface strings at either character level orword level; (2) corpus-based method (Li et al.,2006) which measures word or sentence similar-ities using the information gained from large cor-pora, including Latent Semantic Analysis (LSA),pointwise mutual information (PMI), etc.
(3)knowledge-based method (Mihalcea et al., 2006)which estimates similarities with the aid of ex-ternal resources, such as WordNet1; (4) hybrid1http://wordnet.princeton.edu/method (Zhu and Lan, 2013; Croce et al., 2013)which integrates multiple similarity measures andadopts supervised machine learning algorithms tolearn the different contributions of different fea-tures.The approaches to the task of TE can be roughlydivided into two groups: (1) logic inferencemethod (Bos and Markert, 2005) where automaticreasoning tools are used to check the logical repre-sentations derived from sentences and (2) machinelearning method (Zhao et al., 2013; Gomaa andFahmy, 2013) where a supervised model is builtusing a variety of similarity scores.Unlike previous work which separately ad-dressed these two closely related tasks by usingsimple feature types, in this paper we endeavor tosimultaneously solve these two tasks by using het-erogenous features.3 Our SystemsWe consider the two tasks as one by exploiting thesame set of features but using different learningmethods, i.e., classification and regression.
Seventypes of features are extracted and most of themare based on our previous work on TE (Zhao etal., 2014) and STS (Zhu and Lan, 2013).
Manylearning algorithms and parameters are examinedand the final submitted systems are configured ac-cording to the preliminary results on training data.3.1 PreprocessingThree text preprocessing operations were per-formed before we extracted features, which in-cluded: (1) we converted the contractions to theirformal writings, for example, doesn?t is rewrit-ten as does not.
(2) the WordNet-based Lemma-tizer implemented in Natural Language Toolkit2was used to lemmatize all words to their nearestbase forms in WordNet, for example, was is lem-matized to be.
(3) we replaced a word from onesentence with another word from the other sen-tence if the two words share the same meaning,where WordNet was used to look up synonyms.No word sense disambiguation was performed andall synsets for a particular lemma were considered.3.2 Feature Representations3.2.1 Length Features (len)Given two sentences A and B, this feature typerecords the length information using the follow-2http://nltk.org/272ing eight measure functions:|A|, |B|, |A?B|, |B ?A|, |A ?B|, |A ?B|,(|A|?|B|)|B|,(|B|?|A|)|A|where |A| stands for the number of non-repeatedwords in sentence A , |A?B| means the number ofunmatched words found in A but not in B , |A ?B|stands for the set size of non-repeated words foundin either A or B and |A ?
B| means the set size ofshared words found in both A and B .Moreover, in consideration of different types ofwords make different contributions to text similar-ity, we also recorded the number of words in setA?B and B ?A whose POS tags are noun, verb,adjective and adverb respectively.
We used Stan-ford POS Tagger3for POS tagging.
Finally, wecollected a total of sixteen features.3.2.2 Surface Text Similarity (st)As shown in Table 1, we adopted six commonlyused functions to calculate the similarity betweensentence A and B based on their surface forms,where?
?x and?
?y are vectorial representations ofsentences A and B in tf ?
idf schema.Measure DefinitionJaccard Sjacc= |A ?
B|/|A ?
B|Dice Sdice= 2 ?
|A ?
B|/(|A|+ |B|)Overlap Sover= |A ?
B|/|A| and |A ?
B|/|B|Cosine Scos=?
?x ??
?y /(??
?x ?
?
??
?y ?
)Manhattan M(?
?x ,?
?y ) =n?i=1|xi?
yi|Euclidean E(?
?x ,?
?y ) =?n?i=1(xi?
yi)2Table 1: Surface text similarity measures and theirdefinitions used in our experiments.We also used three statistical correlation coef-ficients (i.e., Pearson, Spearmanr, Kendalltau) tomeasure similarity by regarding the vectorial rep-resentations as different variables.
Thus we got tenfeatures at last.3.2.3 Semantic Similarity (ss)The above surface text similarity features onlyconsider the surface words rather than their ac-tual meanings in sentences.
In order to build thesemantic representations of sentences, we used alatent model to capture the contextual meaningsof words.
Specifically, we adopted the weightedtextual matrix factorization (WTMF) (Guo andDiab, 2012) to model the semantics of sentencesdue to its reported good ability to model shorttexts.
This model first factorizes the original term-sentence matrix X into two matrices such that3http://nlp.stanford.edu/software/tagger.shtmlXi,j?
PT?,i.Q?,j, where P?,iis a latent seman-tic vector profile for word wiand Q?,jis a vectorprofile that represents the sentence sj.
Then weemployed the new representations of sentences,i.e., Q, to calculate the semantic similarity be-tween sentences using Cosine, Manhattan, Eu-clidean, Pearson, Spearmanr, Kendalltau measuresrespectively, which results in six features.3.2.4 Grammatical Relationship (gr)The grammatical relationship feature measuresthe semantic similarity between two sentencesat the grammar level and this feature type wasalso explored in our previous work (Zhao et al.,2013; Zhu and Lan, 2013).
We used StanfordParser4to acquire the dependency informationfrom sentences and the grammatical informationare represented in the form of relation unit, e.g.nsubj(example, this), where nsubj stands for a de-pendency relationship between example and this.We obtained a sequence of relation units for eachsentence and then used them to estimate similarityby adopting eight measure functions described inSection 3.2.1, resulting in eight features.3.2.5 Text Difference Measures (td)There are two types of text difference measures.The first feature type is specially designed forthe contradiction entailment relationship, whichis based on the following observation: there ex-ist antonyms between two sentences or the nega-tion status is not consistent (i.e., one sentence hasa negation word while the other does not have) ifcontradiction holds.
Therefore we examined eachsentence pair and set this feature as 1 if at least oneof these conditions is met, otherwise -1.
WordNetwas used to look up antonyms and a negation listwith 28 words was used.The second feature type is extracted from twoword sets A?B and B?A as follows: we first cal-culated the similarities between every word fromA ?
B and every word from B ?
A , then took themaximum, minimum and average value of them asfeatures.
In our experiments, four WordNet-basedsimilarity measures (i.e., path, lch, wup, jcn (Go-maa and Fahmy, 2013)) were used to calculate thesimilarity between two words.Totally, we got 13 text difference features.4http://nlp.stanford.edu/software/lex-parser.shtml2733.2.6 String Features (str)This set of features is taken from our previouswork (Zhu and Lan, 2013) due to its superior per-formance.Longest common sequence (LCS) We computedthe LCS similarity on the original and lemmatizedsentences.
It was calculated by finding the maxi-mum length of a common contiguous subsequenceof two strings and then dividing it by the smallerlength of two strings to eliminate the impacts oflength imbalance.Jaccard similarity using n-grams We obtainedn-grams at three different levels, i.e., the origi-nal word level, the lemmatized word level and thecharacter level.
Then these n-grams were used forcalculating Jaccard similarity defined in Table 1.In our experiments, n = {1, 2, 3} were used forthe word level and n = {2, 3, 4} were used for thecharacter level.Weighted word overlap (WWO) Since not allwords are equally important, the traditional Over-lap similarity may not be always reasonable.
Thuswe used the information content of word w to es-timate the importance of word w as follows:ic(w) = ln?w??Cfreq(w?
)freq(w)where C is the set of words in the corpus andfreq(w) is the frequency of the word w in thecorpus.
To compute ic(w), we used the Web 1T5-gram Corpus5.
Then the WWO similarity oftwo sentence s1and s2was calculated as follows:Simwwo(s1, s2) =?w?s1?s2ic(w)?w??s2ic(w?
)Due to its asymmetry, we used the harmonic meanof Simwwo(s1, s2) and Simwwo(s2, s1) as the fi-nal WWO similarity.
The WWO similarity is cal-culated on the original and lemmatized strings re-spectively.Finally, we got two LCS features, nine Jaccardn-gram features and two WWO features.3.2.7 Corpus-based Features (cps)Two types of corpus-based feature are also bor-rowed from our previous work (Zhu and Lan,2013), i.e., vector space sentence similarity andco-occurrence retrieval model (CRM), which re-sults in six features.5https://catalog.ldc.upenn.edu/LDC2006T13Co-occurrence retrieval model (CRM) TheCRM word similarity is calculated as follows:SimCRM(w1, w2) =2 ?
|c(w1) ?
c(w2)||c(w1)|+ |c(w2)|where c(w) is the set of words that co-occur withword w. We used the 5-gram part of the Web 1T5-gram Corpus to obtain c(w).
We only consid-ered the word w with |c(w)| > T and then tookthe top 200 co-occurring words ranked by the co-occurrence frequency as its c(w).
In our experi-ment, we set T = {50, 200}.
To propagate thesimilarity from words to sentences, we adoptedthe best alignment strategy used in (Banea et al.,2012) to align two sentences.Vector space sentence similarity This feature setis taken from (?Sari?c et al., 2012), which is basedon distributional vectors of words.
First we per-formed latent semantic analysis (LSA) over twocorpora, i.e., the New York Times Annotated Cor-pus (NYT) (Sandhaus, 2008) andWikipedia, to es-timate the distributions of words.
Then we usedtwo strategies to convert the distributional mean-ings of words to sentence level: (i) simply sum-ming up the distributional vector of each word win the sentence, (ii) using the information contentic(w) to weigh the LSA vector of each wordw andsumming them up.
Then we used cosine similarityto measure the similarity of two sentences.3.3 Learning AlgorithmsWe explored several classification algorithms toclassify entailment relationships and regressionalgorithms to predict similarity scores using theabove 72 features after performing max-min stan-dardization procedure by scaling them to [-1,1].Five supervised learning methods were explored:Support Vector Machine (SVM) which makes thedecisions according to the hyperplanes, RandomForest (RF) which constructs a multitude of de-cision trees at training time and selects the modeof the classes output by individual trees, GradientBoosting (GB) that produces a prediction modelin the form of an ensemble of weak predictionmodels, k-nearest neighbors (kNN) that decidesthe class labels with the aid of the classes of knearest neighbors, and Stochastic Gradient De-scent (SGD) which uses SGD technique to min-imize loss functions.
These supervised learningmethods are implemented in scikit-learn toolkit(Pedregosa et al., 2011).
Besides, we also useda semi-supervised learning strategy for both tasks274in order to make full use of unlabeled test data.Specifically, the co-training algorithm was used toaddress TE task according to (Zhao et al., 2014).Its strategy is to train two classifiers with two dataviews and to add the top confident predicted in-stances by one classifier to expand the training setof another classifier and then to re-train the twoclassifiers on the expanded training sets.
For STStask, we utilized CoReg algorithm (Zhou and Li,2005) which uses two kNN regressors to performco-training paradigm.3.4 Evaluation MeasuresIn order to evaluate the performance of differ-ent algorithms, we adopted the official evaluationmeasures, i.e., Pearson correlation coefficient forSTS task and accuracy for TE task.4 Experiments on Training DataTo make a reasonable comparison between differ-ent algorithms, we performed 5-fold cross valida-tion on training data with 5000 sentence pairs.
Theparameters tuned in different algorithms are listedbelow: the trade-off parameter c in SVM, the num-ber of trees n in RF, the number of boosting stagesn in GB, the number of nearest neighbors k in kNNand the number of passes over the training data nin SGD.
The rest parameters are set to be default.AlgorithmSTS task TE taskPearson para.
Accuracy para.SVM .807?.058 c=10 83.46?2.09 c=100RF .805?.052 n=40 83.16?2.64 n=30GB .806?.055 n=210 83.22?2.48 n=140kNN .797?.062 k=25 82.54?2.45 k=17SGD .765?.064 n=29 78.88?1.99 n=15Table 2: The 5-fold cross validation results ontraining data with mean and standard deviation foreach algorithm.Table 2 reports the experimental results of 5-fold cross validation with mean and standard devi-ation and the optimal parameters on training data.The results of semi-supervised learning methodsare not listed because only a few parameters aretried due to the limit of time.
From this table wesee that SVM, RF and GB perform comparable re-sults to each other.5 Results on Test Data5.1 Submitted System ConfigurationsAccording to the above preliminary experimentalresults, we configured five final systems for eachtask.
Table 3 presents the classification and regres-sion algorithms with their parameters used in thefive systems for each task.System STS task TE task1 SVR, c=10 SVC, c=1002 GB, n=210 GB, n=1403 RF, n=40 RF, n=304 CoReg, k=13 co-training, k=405 majority voting majority votingTable 3: Five system configurations for test datafor two tasks.Among them, System 1 acts as our primaryand baseline system that employs SVM algorithmand as comparison System 2 and System 3 exploitGB and RF algorithm respectively.
Unlike super-vised settings in the aforementioned systems, Sys-tem 4 employs a semi-supervised learning strategyto make use of unlabeled test data.
For CoReg,the number of iteration and the number of near-est neighbors are set as 100 and 13 respectively,and for each iteration in co-training, the numberof confident predictions is set as 40.
To furtherimprove performance, System 5 combines the re-sults of 5 different algorithms (i.e.
MaxEnt, SVM,kNN, GB, RF) through majority voting.
We usedthe averaged values of the outputs from differentregressors as final similarity scores for semanticsimilarity measurement task and chose the majorclass label for entailment judgement task.5.2 Results and DiscussionTable 4 lists the final results officially released bythe organizers in terms of Pearson and accuracy.The best performance among these five systems isshown in bold font.
All participants can submit amaximum of five runs for each task and only oneprimary system is involved in official ranking.
Thelower part of Table 4 presents the top 3 results andthe results with ?
are achieved by our systems.System STS task TE task(%)1 0.8279 83.6412 0.8389 84.1283 0.8414 83.9454 0.8210 81.1655 0.8349 83.986rank 1st 0.8279* 84.575rank 2nd 0.8272 83.641*rank 3rd 0.8268 83.053Table 4: The results of our five systems for twotasks and the officially top-ranked systems.From this table, we found that (1) System 3 (us-275ing GB algorithm) and System 2 (using RF algo-rithm) achieve the best performance among threesupervised systems in STS and TE task respec-tively.
However, there is no significant differenceamong these systems.
(2) Surprisingly, the semi-supervised system (i.e., System 4) that employsthe co-training strategy to make use of test dataperforms the worst, which is beyond our expecta-tion.
Based on our further observation in TE task,the possible reason is that a lot of misclassified ex-amples are added into the training pool in the ini-tial iteration, which results in worse models builtin the subsequent iterations.
And we speculate thatthe weak learner kNN employed in CoReg maylead to poor performance as well.
(3) The major-ity voting strategy fails to boost the performancesince GB and RF algorithm obtain the best perfor-mance among these algorithms.
(4) Our systemsobtain very good results on both STS and TE task,i.e., we rank 1st out of 17 participants in STS taskand rank 2nd out of 18 participants in TE task ac-cording to the results of primary systems and asshown in Table 4 our primary system (i.e., System1) do not achieve the best performance.In a nutshell, our systems rank first and secondin STS and TE task respectively.
Therefore theanswer to the first question raised in Section 1 isyes.
For two tasks, i.e., STS and TE, which arevery closely related but slightly different, we canuse the same features to solve them together.5.3 Feature Combination ExperimentsTo answer the second question and explore the in-fluences of different feature types, we performeda series of experiments under the best system set-ting.
Table 5 shows the results of different featurecombinations where for each time we selected andadded one best feature type.
From this table, wefind that for STS the most effective feature is cpsand for TE task is td.
Almost all feature types havepositive effects on performance.
Specifically, tdalone achieves 81.063% in TE task which is quiteclose to the best performance (84.128%) and cpsalone achieves 0.7544 in STS task.
Moreover, thetd feature proposed for TE task is quite effectivein STS task as well, which suggests that text se-mantic difference measures are also crucial whenmeasuring sentence similarity.Therefore the answer to the second question isyes.
It is clear that the features proposed for TE arealso effective for STS and heterogenous featuresyield better performance than a single feature type.len st ss gr td str cps result+ 0.7544 (STS)+ + 0.8057(+5.13)+ + + 0.8280(+2.23)+ + + + 0.8365(+0.85)+ + + + + 0.8426(+0.61)+ + + + + + 0.8432(+0.06)+ + + + + + + 0.8429(-0.03)+ 81.063 (TE)+ + 82.484(+1.421)+ + + 82.992(+0.508)+ + + + 83.844(+0.852)+ + + + + 83.925(+0.081)+ + + + + + 84.067(+0.142)+ + + + + + + 84.128(+0.061)Table 5: Results of feature combinations, the num-bers in the brackets are the performance incre-ments compared with the previous results.6 ConclusionWe set up five state-of-the-art systems and eachsystem employs different classifiers or regressorsusing the same feature set.
Our submitted systemsrank the 1st out of 17 teams in STS task with thebest performance of 0.8414 in terms of Pearsoncoefficient and rank the 2nd out of 18 teams inTE task with 84.128% in terms of accuracy.
Thisresult indicates that (1) we can use the same fea-ture set to solve these two tasks together, (2) thefeatures proposed for TE task are also effectivefor STS task and (3) heterogenous features out-perform a single feature.
For future work, we mayexplore the underlying relationships between thesetwo tasks to boost their performance by each other.AcknowledgmentsThis research is supported by grants from Na-tional Natural Science Foundation of China(No.60903093) and Shanghai Knowledge ServicePlatform Project (No.
ZF1213).ReferencesIon Androutsopoulos and Prodromos Malakasiotis.2009.
A survey of paraphrasing and textual entail-ment methods.
arXiv preprint arXiv:0912.3747.Carmen Banea, Samer Hassan, Michael Mohler, andRada Mihalcea.
2012.
Unt:a supervised synergisticapproach to semantictext similarity.
In First JointConference on Lexical and Computational Seman-tics (*SEM.276Daniel B?ar, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
Ukp: Computing seman-tic textual similarity by combining multiple contentsimilarity measures.
In Proceedings of the FirstJoint Conference on Lexical and Computational Se-mantics, pages 435?440.
Association for Computa-tional Linguistics.Johan Bos and Katja Markert.
2005.
Recognising tex-tual entailment with logical inference.
In Proceed-ings of the conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing, pages 628?635.
Association for Compu-tational Linguistics.Danilo Croce, Valerio Storch, and Roberto Basili.2013.
Unitor-core typed: Combining text similarityand semantic filters through sv regression.
In Pro-ceedings of the 2nd Joint Conference on Lexical andComputational Semantics, page 59.Wael H Gomaa and Aly A Fahmy.
2013.
A survey oftext similarity approaches.
International Journal ofComputer Applications, 68(13):13?18.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical composi-tional distributional model of meaning.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing, pages 1394?1404.
Asso-ciation for Computational Linguistics.Weiwei Guo and Mona Diab.
2012.
Modeling sen-tences in the latent space.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics.Zellig S Harris.
1954.
Distributional structure.
ThePhilosophy of Linguistics,.Yuhua Li, David McLean, Zuhair A Bandar, James DO?shea, and Keeley Crockett.
2006.
Sentence sim-ilarity based on semantic nets and corpus statistics.Knowledge and Data Engineering, IEEE Transac-tions on, 18(8):1138?1150.Mihai C. Lintean and Vasile Rus.
2012.
Measuring se-mantic similarity in short texts through greedy pair-ing and word semantics.
In FLAIRS Conference.AAAI Press.Prodromos Malakasiotis and Ion Androutsopoulos.2007.
Learning textual entailment using svms andstring similarity measures.
In Proceedings of theACL-PASCAL Workshop on Textual Entailment andParaphrasing, pages 42?47.
Association for Com-putational Linguistics.M.
Marelli, L. Bentivogli, M. Baroni, R. Bernardi,S.
Menini, and R. Zamparelli.
2014.
Semeval-2014task 1: Evaluation of compositional distributionalsemantic models on full sentences through seman-tic relatedness and textual entailment.
In Proceed-ings of SemEval 2014: International Workshop onSemantic Evaluation.Rada Mihalcea, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In AAAI, vol-ume 6, pages 775?780.Fabian Pedregosa, Ga?el.
Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and?Edouard Duchesnay.
2011.Scikit-learn: Machine learning in Python.
Journalof Machine Learning Research, 12:2825?2830.Evan Sandhaus.
2008.
The new york times annotatedcorpus ldc2008t19.
Philadelphia: Linguistic DataConsortium.Socher, Richard, Huval Brody, Manning Christopher,and Ng Andrew.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of EMNLP, Jeju Island, Korea.Peter D Turney, Patrick Pantel, et al.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of artificial intelligence research,37(1):141?188.Frane?Sari?c, Goran Glava?s, Mladen Karan, Jan?Snajder,and Bojana Dalbelo Ba?si?c.
2012.
Takelab: Systemsfor measuring semantic text similarity.
In Proceed-ings of the First Joint Conference on Lexical andComputational Semantics, pages 441?448, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Fabio Massimo Zanzotto, Ioannis Korkontzelos,Francesca Fallucchi, and Suresh Manandhar.
2010.Estimating linear models for compositional distri-butional semantics.
In Proceedings of the 23rd In-ternational Conference on Computational Linguis-tics, pages 1263?1271.
Association for Computa-tional Linguistics.Jiang Zhao, Man Lan, and Zheng-Yu Niu.
2013.
Ec-nucs: Recognizing cross-lingual textual entailmentusing multiple text similarity and text differencemeasures.
In Proceedings of the Seventh Interna-tional Workshop on Semantic Evaluation (SemEval2013), pages 118?123, Atlanta, Georgia, USA, June.Association for Computational Linguistics.Jiang Zhao, Man Lan, Zheng-Yu Niu, and DonghongJi.
2014.
Recognizing cross-lingual textual entail-ment with co-training using similarity and differenceviews.
In The 2014 International Joint Conferenceon Neural Networks (IJCNN2014).
IEEE.Zhi-Hua Zhou and Ming Li.
2005.
Semi-supervisedregression with co-training.
In IJCAI, pages 908?916.Tian Tian Zhu and Man Lan.
2013.
Ecnucs: Measur-ing short text semantic equivalence using multiplesimilarity measurements.
In Proceedings of the 2ndJoint Conference on Lexical and Computational Se-mantics, page 124.277
