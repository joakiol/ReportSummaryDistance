Proceedings of the ACL Student Research Workshop, pages 103?108,Ann Arbor, Michigan, June 2005. c?2005 Association for Computational LinguisticsCentrality Measures in Text Mining:Prediction of Noun Phrases that Appear in AbstractsZhuli XieDepartment of Computer ScienceUniversity of Illinois at ChicagoChicago, IL 60607, U. S. Azxie@cs.uic.eduAbstractIn this paper, we study different centralitymeasures being used in predicting nounphrases appearing in the abstracts of sci-entific articles.
Our experimental resultsshow that centrality measures improve theaccuracy of the prediction in terms ofboth precision and recall.
We also foundthat the method of constructing NounPhrase Network significantly influencesthe accuracy when using the centralityheuristics itself, but is negligible when itis used together with other text features indecision trees.1 IntroductionResearch on text summarization, information re-trieval, and information extraction often faces thequestion of how to determine which words aremore significant than others in text.
Normally weonly consider content words, i.e., the open classwords.
Non-content words or stop words, whichare called function words in natural language proc-essing, do not convey semantics so that they areexcluded although they sometimes appear morefrequently than content words.
A content word isusually defined as a term, although a term can alsobe a phrase.
Its significance is often indicated byTerm Frequency (TF) and Inverse Document Fre-quency (IDF).
The usage of TF comes from ?thesimple notion that terms which occur frequently ina document may reflect its meaning more stronglythan terms that occur less frequently?
(Jurafskyand Martin, 2000).
On the contrary, IDF assignssmaller weights to terms which are contained inmore documents.
That is simply because ?the moredocuments having the term, the less useful the termis in discriminating those documents having itfrom those not having it?
(Yu and Meng, 1998).TF and IDF also find their usage in automatictext summarization.
In this circumstance, TF isused individually more often than together withIDF, since the term is not used to distinguish adocument from another.
Automatic text summari-zation seeks a way of producing a text which ismuch shorter than the document(s) to be summa-rized, and can serve as a surrogate for full-text.Thus, for extractive summaries, i.e., summariescomposed of original sentences from the text to besummarized, we try to find those terms which aremore likely to be included in the summary.The overall goal of our research is to build amachine learning framework for automatic textsummarization.
This framework will learn the rela-tionship between text documents and their corre-sponding abstracts written by human.
At thecurrent stage the framework tries to generate a sen-tence ranking function and use it to produce extrac-tive summaries.
It is important to find a set offeatures which represent most information in a sen-tence and hence the machine learning mechanismcan work on it to produce a ranking function.
Thenext stage in our research will be to use the frame-work to generate abstractive summaries, i.e.
sum-maries which do not use sentences from the inputtext verbatim.
Therefore, it is important to knowwhat terms should be included in the summary.In this paper we present the approach of usingsocial network analysis technique to find terms,specifically noun phrases (NPs) in our experi-ments, which occur in the human-written abstracts.We show that centrality measures increase the pre-diction accuracy.
Two ways of constructing noun103phrase network are compared.
Conclusions andfuture work are discussed at the end.2 Centrality MeasuresSocial network analysis studies linkages amongsocial entities and the implications of these link-ages.
The social entities are called actors.
A socialnetwork is composed of a set of actors and the rela-tion or relations defined on them (Wasserman andFaust, 1994).
Graph theory has been used in socialnetwork analysis to identify those actors who im-pose more influence upon a social network.
A so-cial network can be represented by a graph withthe actors denoted by the nodes and the relationsby the edges or links.
To determine which actorsare prominent, a measure called centrality is intro-duced.
In practice, four types of centrality are oftenused.Degree centrality measures how many directconnections a node has to other nodes in a net-work.
Since this measure depends on the size ofthe network, a standardized version is used when itis necessary to compare the centrality across net-works of different sizes.DegreeCentrality(ni) = d(ni)/(u-1),where d(ni) is the degree of node i in a networkand u is the number of nodes in that network.Closeness centrality focuses on the distances anactor is from all other nodes in the network.
?ui i jj=1ClosenessCentrality(n ) = (u- 1) d(n ,n ) ,where d(ni, nj) is the shortest distance betweennode i and j.Betweenness centrality emphasizes that for anactor to be central, it must reside on many ge-odesics of other nodes so that it can control theinteractions between them.
?
jk i jkj<kig (n ) /gBetweennessCentrality(n ) =(u- 1)(u- 2) / 2,where gjk is the number of geodesics linking node jand k, gjk(ni) is the number of geodesics linking thetwo nodes that contain node i.Betweenness centrality is widely used becauseof its generality.
This measure assumes that infor-mation flow between two nodes will be on the ge-odesics between them.
Nevertheless, ?It is quitepossible that information will take a more circui-tous route either by random communication or [bybeing] channeled through many intermediaries inorder to 'hide' or 'shield' information?.
(Stephensonand Zelen, 1989).Stephenson and Zelen (1989) developed infor-mation centrality which generalizes betweennesscentrality.
It focuses on the information containedin all paths originating with a specific actor.
Thecalculation for information centrality of a node isin the Appendix.Recently centrality measures have started togain attention from researchers in text processing.Corman et al (2002) use vectors, which consist ofNPs, to represent texts and hence analyze mutualrelevance of two texts.
The values of the elementsin a vector are determined by the betweenness cen-trality of the NPs in a text being analyzed.
Erkanand Radev (2004) use the PageRank method,which is the application of centrality concept to theWeb, to determine central sentences in a cluster forsummarization.
Vanderwende et al (2004) also usethe PageRank method to pick prominent triples, i.e.
(node i, relation, node j), and then use the triples togenerate event-centric summaries.3 NP NetworksTo construct a network for NPs in a text, we trytwo ways of modeling the relation between them.One is at the sentence level: if two noun phrasescan be sequentially parsed out from a sentence, alink is added between them.
The other way is at thedocument level: we simply add a link to every pairof noun phrases which are parsed out in succes-sion.
The difference between the two ways is thatthe network constructed at the sentence level ig-nores the existence of certain connections betweensentences.We process a text document in four steps.First, the text is tokenized and stored into an in-ternal representation with structural information.Second, the tokenized text is tagged by the Brilltagging algorithm POS tagger.1Third, the NPs in a text document are parsed ac-cording to 35 parsing rules as shown in Figure 1.
Ifa new noun phrase is found, a new node is formedand added to the network.
If the noun phrase al-ready exists in the network, the node containing itwill be identified.
A link will be added betweentwo nodes if they are parsed out sequentially for1 The POS tagger we used can be obtained fromhttp://web.media.mit.edu/~hugo/montytagger/104the network formed at the document level, or se-quentially in the same sentence for the networkformed at the sentence level.Finally, after the text document has been proc-essed, the centrality of each node in the network isupdated.4 Predicting NPs Occurring in AbstractsIn this paper, we refer the NPs occur both in a textdocument and its corresponding abstract as Co-occurring NPs (CNPs).4.1 CMP-LG CorpusIn our experiment, a corpus of 183 documents wasused.
The documents are from the Computationand Language collection and have been marked inXML with tags providing basic information aboutthe document such as title, author, abstract, body,sections, etc.
This corpus is a part of the TIPSTERText Summarization Evaluation Conference(SUMMAC) effort acting as a general resource tothe information retrieval, extraction and summari-zation communities.
We excluded five documentsfrom this corpus which do not have abstracts.4.2 Using Noun Phrase Centrality HeuristicsWe assume that a noun phrase with high centralityis more likely to be a central topic being addressedin a document than one with low centrality.
Giventhis assumption, we performed an experiment, inwhich the NPs with highest centralities are re-trieved and compared with the actual NPs in theabstracts.
To evaluate this method, we use Preci-sion, which measures the fraction of true CNPs inall predicted CNPs, and Recall, which measuresthe fraction of correctly predicted CNPs in allCNPs.After establishing the NP network for a docu-ment and ranking the nodes according to their cen-tralities, we must decide how many NPs should beretrieved.
This number should not be too big; oth-erwise the Precision value will be very low, al-though the Recall will be higher.
If this number isvery small, the Recall will decrease correspond-ingly.
We adopted a compound metric ?
F-measure, to balance the selection:Based on our study of 178 documents in theCMP-LG corpus, we find that the number of CNPsis roughly proportional to the number of NPs in theabstract.
We obtain a linear regression model forthe data shown in Figure 2 and use this model tocalculate the number of nodes we should retrievefrom the NP network, given the number of NPs inthe abstract known a priori:One could argue that the number of abstract NPs isunknown a priori and thus the proposed method isof limited use.
However, the user can provide anestimate based on the desired number of words inthe summary.
Here we can adopt the same way ofasking the user to provide a limit for the NPs in thesummary.
We used the actual number of NPs theauthor used in his/her abstract in our experiment.Figure 2.
Scatter Plot of CNPs05101520253035400 10 20 30 40 50 60 70Number of NPs in AbstractNumber of CNPsOur experiment results are shown in Figure 3(a)and 3(b).
In 3(a) the NP network is formed at sen-NX --> CDNX --> CD NNSNX --> NNNX --> NN NNNX --> NN NNSNX --> NN NNS NNNX --> NNPNX --> NNP CDNX --> NNP NNPNX --> NNP NNPSNX --> NNP NNNX --> NNP NNP NNPNX --> JJ NNNX --> JJ NNSNX --> JJ NN NNSNX --> PRP$ NNSNX --> PRP$ NNNX --> PRP$ NN NNNX --> NNSNX --> PRPNX --> WP$ NNSNX --> WDTNX --> EXNX --> WPNX --> DT JJ NNNX --> DT CD NNSNX --> DT VBG NNNX --> DT NNSNX --> DT NNNX --> DT NN NNNX --> DT NNPNX --> DT NNP NNNX --> DT NNP NNPNX --> DT NNP NNP NNPNX -->DT NNP NNP NN NNFigure 1.
NP Parsing RulesF-measure=2*Precision*Recall/(Precision+Recall)Number of Common NPs =0.555 * Number of NPs in Abstract + 2.435105tence level.
In this way, it is possible the graph willbe composed of disconnected subgraphs.
In suchcase, we calculate the closeness centrality (cc),betweenness centrality (bc), and the informationcentrality (ic) within the subgraphs while the de-gree centrality (dc) is still computed for the overallgraph.
In 3(b), the network is constructed at thedocument level.
Therefore, it is guaranteed thatevery node is reachable from all other node.Figure 3(a) shows the simplest centrality meas-ure dc performs best, with Precision, Recall, and F-measure all greater than 0.2, which are twice of bcand almost ten times of cc and ic.In Figure 3(b), however, all four measures arearound 0.25 in all three evaluation metrics.
Thisresult suggests to us that when we choose a cen-trality to represent the prominence of a NP in thetext, not only does the kind of the centrality matter,but also the way of forming the NP network.Overall, the heuristic of using centrality itselfdoes not achieve impressive scores.
We will see inthe next section that using decision trees is a muchbetter way to perform the predictions, when usingcentrality together with other text features.4.3 Using Decision TreesWe obtain the following features for all NPs in adocument from the CMP-LG corpus:Position: the order of a NP appearing in the text,normalized by the total number of NPs.Article: three classes are defined for this attribute:INDEfinite (contains a or an), DEFInite (containsthe), and NONE (all others).Degree centrality: obtained from the NP networkCloseness centrality: obtained from the NP net-workBetweenness centrality: obtained from the NPnetworkInformation centrality: obtained from the NPnetworkHead noun POS tag: a head noun is the last wordin the NP.
Its POS tag is used here.Proper name: whether the NP is a proper name,by looking at the POS tags of all words in the NP.Number: whether the NP is just one number.Frequency: how many times a NP occurs in a text,normalized by its maximum.In abstract: whether the NP appears in the author-provided abstract.
This attribute is the target for thedecision trees to classify.Figure 3(a).
Centrality Heuristics(Network at Sentence Level)00.050.10.150.20.250.3Precision Recall F-measuredcccbcicFigure 3(b).
Centrality Heuristics(Network at Document Level)00.050.10.150.20.250.3Precision Recall F-measuredcccbcicIn order to learn which type of centrality meas-ures helps to improve the accuracy of the predic-tions, and to see whether centrality measures arebetter than term frequency, we experiment with sixgroups of feature sets and compare their perform-ances.
The six groups are:All: including all features above.DC: including only the degree centrality measure,and other non-centrality measures except for Fre-quency.CC: same as DC except for using closeness cen-trality instead of degree centrality.BC: same as DC except for using betweennesscentrality instead of degree centrality.IC: same as DC except for using information cen-trality instead of degree centrality.FQ: including Frequency and all other non-centrality features.The 178 documents have generated more than100,000 training records.
Among them only a verysmall portion (2.6%) belongs to the positive class.When using decision tree algorithm on such imbal-anced attribute, it is very common that the classwith absolute advantages will be favored (Japko-wicz, 2000; Kubat and Matwin, 1997).
To reduce106Precision 0.817 0.816 0.795 0.809 0.767 0.787 0.732 0.762 0.774 0.795 0.769 0.779Recall 0.971 0.984 0.96 0.972 0.791 0.866 0.8 0.819 0.651 0.696 0.639 0.662F-measure 0.887 0.892 0.869 0.883 0.779 0.825 0.764 0.789 0.706 0.742 0.696 0.715Precision 0.795 0.82 0.795 0.803 0.772 0.806 0.768 0.782 0.767 0.806 0.766 0.78Recall 0.944 0.976 0.946 0.955 0.79 0.892 0.755 0.812 0.72 0.892 0.644 0.752F-measure 0.863 0.891 0.864 0.873 0.781 0.846 0.761 0.796 0.743 0.846 0.698 0.763Set 1 Set 2 Set 3 Mean Set 1 Set 2 Set 3 Mean Set 1 Set 2 Set 3 MeanPrecision 0.738 0.799 0.745 0.761 0.722 0.759 0.743 0.742 0.774 0.79 0.712 0.759Recall 0.698 0.874 0.733 0.768 0.666 0.799 0.667 0.711 0.763 0.878 0.78 0.807F-measure 0.716 0.835 0.737 0.763 0.693 0.779 0.702 0.724 0.768 0.831 0.744 0.781Precision 0.767 0.799 0.75 0.772 0.756 0.798 0.759 0.771 0.734 0.794 0.74 0.756Recall 0.672 0.814 0.666 0.717 0.769 0.916 0.72 0.802 0.728 0.886 0.707 0.774F-measure 0.716 0.806 0.705 0.742 0.762 0.853 0.738 0.784 0.73 0.837 0.722 0.763Set 1 Set 2 Set 3 Mean Set 1 Set 2 Set 3 Mean Set 1 Set 2 Set 3 MeanCCBCSentenceLevelDocumentLevelAll DCSentenceLevelDocumentLevelIC FQTable 1.
Results for Using 6 Feature Sets with YaDTthe unfair preference, one way is to boost the weakclass, e.g., by replicating instances in the minorityclass (Kubat and Matwin, 1997; Chawla et al,2000).
In our experiments, the 178 documentswere arbitrarily divided into three roughly equalgroups, generating 36,157, 37,600, and 34,691 re-cords, respectively.
After class balancing, the re-cords are increased to 40,109, 42,210, and 38,499.The three data sets were then run through the deci-sion tree algorithm YaDT (Yet another DecisionTree builder), which is much more efficient thanC4.5 (Ruggieri, 2004),2 with 10-fold cross valida-tion.The experiment results of using YaDT withthree data sets and six feature groups to predict theCNPs are shown in Table 1.
The mean values ofthree metrics are also shown in Figure 4(a) and4(b).
Decision trees achieve much higher scorescompared with the scores obtained by using cen-trality heuristics.
Together with other text features,DC, CC, BC, and IC obtain scores over 0.7 in allthree metric, which are comparable to the scoresobtained by using FQ.
Moreover, when using allthe features, decision trees achieve over 0.8 in pre-cision and over 0.95 in recall.
F-measure is as highas 0.88.
To see whether F-measure of All is statis-tically better than that of other settings, we run t-tests to compare them using values of F-measureobtained in the 10-fold cross-validation from thethree data sets.
The results show the mean value ofF-measure of All is significantly higher (p-value=0.000) than that of other settings.Differently from the experiments that use centralityheuristics by itself, almost no obvious distinctions2 The YaDT software can be obtained fromhttp://www.di.unipi.it/~ruggieri/software.htmlcan be observed when comparing the performancesof YaDT with NP network formed in two ways.5 Conclusions and Future workWe have studied four kinds of centrality measuresin order to identify prominent noun phrases in textdocuments.
Overall, the centrality heuristic itselfdoes not demonstrate its superiority.
Among fourcentrality measures, degree centrality performs thebest in the heuristic when the NP network is con-structed at the sentence level, which indicates othercentrality measures obtained from the subgraphscan not represent very well the prominence of theNPs in the global NP network.
When the NP net-work is constructed at the document level, the dif-ferences between the centrality measures becomenegligible.
However, networks formed at thedocument level overlook the connections betweensentences as there is only one kind of link; on theother hand, NP networks formed at the sentencelevel ignore connections between sentences.
Weplan to extend our study to construct NP networkswith weighted links.
The key problem will be howto determine the weights for links between twoNPs in the same sentence, in the same paragraphbut different sentences, and in different paragraphs.We consider introducing the concept of entropyfrom Information Theory to solve this problem.In our experiments with YaDT, it seems the waysof forming NP network are not critical.
We learnthat, at least in this circumstance, the decision treesalgorithm is more robust than the centrality heuris-tic.
When using all features in YaDT, recallreaches 0.95, which means the decision trees findout 95% of CNPs in the abstracts from the textdocuments, without increasing mistakes as the107Figure 4(a).
Results with NP NetworkFormed in Sentence Level0.60.70.80.91Precision Recall F-measureAllDCCCBCICFQFigure 4(b).
Results with NP NetworkFormed in Document Level0.60.70.80.91Precision Recall F-measureAllDCCCBCICFQprecision is improved at the same time.
Using allfeatures in YaDT achieves better results than usingcentrality feature or frequency individually withother features implies centrality features may cap-ture somewhat different information from the text.To make this research more robust, we will in-clude reference resolution into our study.
We willalso include centrality measures as sentencefeatures in producing extractive summaries.ReferencesN.
Chawla, K. Bowyer, L. Hall, and W. P. Kegelmeyer.2000.
SMOTE: synthetic minority over-samplingtechnique.
In Proc.
of the International Conferenceon Knowledge Based Computer Systems, India.S.
Corman, T. Kuhn, R. McPhee, and K. Dooley.
2002.Studying complex discursive systems: Centeringresonance analysis of organizational communication.Human Communication Research, 28(2):157-206.G.
Erkan and D. R. Radev.
2004.
The University ofMichigan at DUC 2004.
In Document UnderstandingConference 2004, Boston, MA.N.
Japkowicz.
2000.
The class imbalance problem: sig-nificance and strategies.
In Proc.
of the 2000 Interna-tional Conference on Artificial Intelligence.D.
Jurafsky and J. H. Martin.
2000.
Speech and Lan-guage Processing: An Introduction to Natural Lan-guage Processing, Computational Linguistics, andSpeech Recognition.
Prentice Hall, Upper SaddleRiver, NJ.M.
Kubat and S. Matwin.
1997.
Addressing the curse ofimbalanced data sets: one-sided sampling.
In Proc.
ofthe Fourteenth International Conference on MachineLearning, Morgan Kauffman, 179?186.S.
Ruggieri.
2004.
YaDT: Yet another Decision Treebuilder.
In Proc.
of the 16th International Conferenceon Tools with Artificial Intelligence (ICTAI 2004),260-265.
Boca Raton, FLK.
Stephenson and M. Zelen.
1989.
Rethinking central-ity: Methods and applications.
Social Networks.
11:1-37.L.
Vanderwende, M. Banko and A. Menezes.
2004.Event-Centric Summary Generation.
In DocumentUnderstanding Conference 2004.
Boston, MA.S.
Wasserman and K. Faust.
1994.
Social NetworkAnalysis: Methods and applications.
CambridgeUniversity Press.C.
T. Yu and W. Meng.
1998.
Principles of DatabaseQuery Processing for Advanced Applications.
Mor-gan Kaufmann Publishers, San Francisco, CA.Appendix: Calculation of Information Cen-tralityConsider a network with n points where every pairof points is reachable.
Define the n n?
matrix( )ijB b=  by:0    if points  and  are incident1    otherwise;1 + degree of pointijiii jbb i?= ?
?=Define the matrix 1( )ijC c B ?= = .
The value of Iij(the information in the combined path Pij) is givenexplicitly by1( 2 )ij ii jj ijI c c c?= + ?
.We can write1 11 ( 2 ) 2n nij ii jj ij iij jI c c c nc T R= == + ?
= + ??
?
,where1 1andn njj ijj jT c R c= == =?
?
.Therefore the centrality for point i can be explicitlywritten as12 ( 2 ) /i ii iinInc T R c T R n= =+ ?
+ ?.
(Stephenson and Zelen 1989).108
