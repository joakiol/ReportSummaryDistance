Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 187?192,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsAutomatically Mining Question Reformulation Patterns fromSearch Log DataXiaobing Xue?Univ.
of Massachusetts, Amherstxuexb@cs.umass.eduYu Tao?Univ.
of Science and Technology of Chinav-yutao@microsoft.comDaxin Jiang Hang LiMicrosoft Research Asia{djiang,hangli}@microsoft.comAbstractNatural language questions have become pop-ular in web search.
However, various ques-tions can be formulated to convey the sameinformation need, which poses a great chal-lenge to search systems.
In this paper, we au-tomatically mined 5w1h question reformula-tion patterns from large scale search log data.The question reformulations generated fromthese patterns are further incorporated into theretrieval model.
Experiments show that us-ing question reformulation patterns can sig-nificantly improve the search performance ofnatural language questions.1 IntroductionMore and more web users tend to use natural lan-guage questions as queries for web search.
Somecommercial natural language search engines such asInQuira and Ask have also been developed to answerthis type of queries.
One major challenge is that var-ious questions can be formulated for the same infor-mation need.
Table 1 shows some alternative expres-sions for the question ?how far is it from Boston toSeattle?.
It is difficult for search systems to achievesatisfactory retrieval performance without consider-ing these alternative expressions.In this paper, we propose a method of automat-ically mining 5w1h question1 reformulation pat-terns to improve the search relevance of 5w1h ques-tions.
Question reformulations represent the alter-native expressions for 5w1h questions.
A question?Contribution during internship at Microsoft Research Asia15w1h questions start with ?Who?, ?What?, ?Where?,?When?, ?Why?
and ?How?.Table 1: Alternative expressions for the original questionOriginal Question:how far is it from Boston to SeattleAlternative Expressions:how many miles is it from Boston to Seattledistance from Boston to SeattleBoston to Seattlehow long does it take to drive from Boston to Seattlereformulation pattern generalizes a set of similarquestion reformulations that share the same struc-ture.
For example, users may ask similar questions?how far is it from X1 to X2?
where X1 and X2represent some other cities besides Boston and Seat-tle.
Then, similar question reformulations as in Ta-ble 1 will be generated with the city names changed.These patterns increase the coverage of the systemby handling the queries that did not appear beforebut share similar structures as previous queries.Using reformulation patterns as the key concept,we propose a question reformulation framework.First, we mine the question reformulation patternsfrom search logs that record users?
reformulationbehavior.
Second, given a new question, we usethe most relevant reformulation patterns to generatequestion reformulations and each of the reformula-tions is associated with its probability.
Third, theoriginal question and these question reformulationsare then combined together for retrieval.The contributions of this paper are summarized astwo folds.
First, we propose a simple yet effectiveapproach to automatically mine 5w1h question re-formulation patterns.
Second, we conduct compre-hensive studies in improving the search performanceof 5w1h questions using the mined patterns.187GeneratingReformulationPatternsSearchLogSet = { (q,qr)}PatternBaseP = { (p,pr)}O ffline PhaseqnewGeneratingQuestionReformulations{ qrnew}  Retrieval Model  { D }New QuestionQuestionReformulationRetrievedDocumentsO nline PhaseFigure 1: The framework of reformulating questions.2 Related WorkIn the Natural Language Processing (NLP) area, dif-ferent expressions that convey the same meaningare referred as paraphrases (Lin and Pantel, 2001;Barzilay and McKeown, 2001; Pang et al, 2003;Pas?ca and Dienes, 2005; Bannard and Callison-Burch, 2005; Bhagat and Ravichandran, 2008;Callison-Burch, 2008; Zhao et al, 2008).
Para-phrases have been studied in a variety of NLP appli-cations such as machine translation (Kauchak andBarzilay, 2006; Callison-Burch et al, 2006), ques-tion answering (Ravichandran and Hovy, 2002) anddocument summarization (McKeown et al, 2002).Yet, little research has considered improving websearch performance using paraphrases.Query logs have become an important resourcefor many NLP applications such as class and at-tribute extraction (Pas?ca and Van Durme, 2008),paraphrasing (Zhao et al, 2010) and language mod-eling (Huang et al, 2010).
Little research has beenconducted to automatically mine 5w1h question re-formulation patterns from query logs.Recently, query reformulation (Boldi et al, 2009;Jansen et al, 2009) has been studied in web search.Different techniques have been developed for querysegmentation (Bergsma and Wang, 2007; Tan andPeng, 2008) and query substitution (Jones et al,2006; Wang and Zhai, 2008).
Yet, most previousresearch focused on keyword queries without con-sidering 5w1h questions.3 Mining Question ReformulationPatterns for Web SearchOur framework consists of three major components,which is illustrated in Fig.
1.Table 2: Question reformulation patterns generated forthe query pair (?how far is it from Boston to Seattle?,?distance from Boston to Seattle?
).S1 = {Boston}:(?how far is it from X1 to Seattle?,?distance from X1 to Seattle?
)S2 = {Seattle}:(?how far is it from Boston to X1?,?distance from Boston to X1?
)S3 = {Boston, Seattle}:(?how far is it fromX1 toX2?,?distance from X1 to X2?
)3.1 Generating Reformulation PatternsFrom the search log, we extract all successive querypairs issued by the same user within a certain timeperiod where the first query is a 5w1h question.
Insuch query pair, the second query is considered asa question reformulation.
Our method takes thesequery pairs, i.e.
Set = {(q, qr)}, as the input andoutputs a pattern base consisting of 5w1h questionreformulation patterns, i.e.
P = {(p, pr)}).
Specif-ically, for each query pair (q, qr), we first collect allcommon words between q and qr except for stop-words ST 2, where CW = {w|w ?
q, w ?
q?, w /?ST}.
For any non-empty subset Si of CW , thewords in Si are replaced as slots in q and qr to con-struct a reformulation pattern.
Table 2 shows exam-ples of question reformulation patterns.
Finally, thepatterns observed in many different query pairs arekept.
In other words, we rely on the frequency of apattern to filter noisy patterns.
Generating patternsusing more NLP features such as the parsing infor-mation will be studied in the future work.3.2 Generating Question ReformulationsWe describe how to generate a set of question refor-mulations {qnewr } for an unseen question qnew.First, we search P = {(p, pr)} to find all ques-tion reformulation patterns where p matches qnew.Then, we pick the best question pattern p?
accord-ing to the number of prefix words and the total num-ber of words in a pattern.
We select the pattern thathas the most prefix words, since this pattern is morelikely to have the same information as qnew.
If sev-eral patterns have the same number of prefix words,we use the total number of words to break the tie.After picking the best question pattern p?, we fur-ther rank all question reformulation patterns con-taining p?, i.e.
(p?, pr), according to Eq.
1.2Stopwords refer to the function words that have little mean-ing by themselves, such as ?the?, ?a?, ?an?, ?that?
and ?those?.188Table 3: Examples of the question reformulations and their corresponding reformulation patternsqnew: how good is the eden pure air system qnew : how to market a restaurantp?
: how good is the X p?
: how to market a Xqnewr pr qnewr preden pure air system X marketing a restaurant marketing a Xeden pure air system review X review how to promote a restaurant how to promote a Xeden pure air system reviews X reviews how to sell a restaurant how to sell a Xrate the eden pure air system rate the X how to advertise a restaurant how to advertise a Xreviews on the eden pure air system reviews on the X restaurant marketing X marketingP (pr|p?)
=f(p?, pr)?p?r f(p?, p?r)(1)Finally, we generate k question reformulationsqnewr by applying the top k question reformulationpatterns containing p?.
The probability P (pr|p?)
as-sociated with the pattern (p?, pr) is assigned to thecorresponding question reformulation qnewr .3.3 Retrieval ModelGiven the original question qnew and k question re-formulations {qnewr }, the query distribution model(Xue and Croft, 2010) (denoted as QDist) is adoptedto combine qnew and {qnewr } using their associatedprobabilities.
The retrieval score of the documentD,i.e.
score(qnew,D), is calculated as follows:score(qnew,D) = ?
log P (qnew|D)+(1 ?
?
)k?i=1P (pri |p?)
log P (qnewri |D) (2)In Eq.
2, ?
is a parameter that indicates the prob-ability assigned to the original query.
P (pri |p?)
isthe probability assigned to qnewri .
P (qnew|D) andP (q?|D) are calculated using the language model(Ponte and Croft, 1998; Zhai and Lafferty, 2001).4 ExperimentsA large scale search log from a commercial searchengine (2011.1-2011.6) is used in experiments.From the search log, we extract all successive querypairs issued by the same user within 30 minutes(Boldi et al, 2008)3 where the first query is a 5w1hquestion.
Finally, we extracted 6,680,278 questionreformulation patterns.For the retrieval experiments, we randomly sam-ple 10,000 natural language questions as queries3In web search, queries issued within 30 minutes are usuallyconsidered having the same information need.Table 4: Retrieval Performance of using question refor-mulations.
?
denotes significantly different with Orig.NDCG@1 NDCG@3 NDCG@5Orig 0.2946 0.2923 0.2991QDist 0.3032?
0.2991?
0.3067?from the search log before 2011.
For each question,we generate the top ten questions reformulations.The Indri toolkit4 is used to implement the languagemodel.
A web collection from a commercial searchengine is used for retrieval experiments.
For eachquestion, the relevance judgments are provided byhuman annotators.
The standard NDCG@k is usedto measure performance.4.1 Examples and PerformanceTable 3 shows examples of the generated questionsreformulations.
Several interesting expressions aregenerated to reformulate the original question.We compare the retrieval performance of usingthe question reformulations (QDist) with the perfor-mance of using the original question (Orig) in Table4.
The parameter ?
of QDist is decided using ten-fold cross validation.
Two sided t-test are conductedto measure significance.Table 4 shows that using the question reformula-tions can significantly improve the retrieval perfor-mance of natural language questions.
Note that, con-sidering the scale of experiments (10,000 queries),around 3% improvement with respect to NDCG is avery interesting result for web search.4.2 AnalysisIn this subsection, we analyze the results to betterunderstand the effect of question reformulations.First, we report the performance of always pick-ing the best question reformulation for each query(denoted as Upper) in Table 5, which provides an4www.lemurproject.org/189Table 5: Performance of the upper bound.NDCG@1 NDCG@3 NDCG@5Orig 0.2946 0.2923 0.2991QDist 0.3032 0.2991 0.3067Upper 0.3826 0.3588 0.3584Table 6: Best reformulation within different positions.top 1 within top 2 within top 349.2% 64.7% 75.4%upper bound for the performance of the question re-formulation.
Table 5 shows that if we were ableto always picking the best question reformulation,the performance of Orig could be improved byaround 30% (from 0.2926 to 0.3826 with respect toNDCG@1).
It indicates that we do generate somehigh quality question reformulations.Table 6 further reports the percent of those 10,000queries where the best question reformulation can beobserved in the top 1 position, within the top 2 posi-tions and within the top 3 positions, respectively.Table 6 shows that for most queries, our methodsuccessfully ranks the best reformulation within thetop 3 positions.Second, we study the effect of different typesof question reformulations.
We roughly divide thequestion reformulations generated by our methodinto five categories as shown in Table 7.
For eachcategory, we report the percent of reformulationswhich performance is bigger/smaller/equal with re-spect to the original question.Table 7 shows that the ?more specific?
reformula-tions and the ?equivalent?
reformulations are morelikely to improve the original question.
Reformu-lations that make ?morphological change?
do nothave much effect on improving the original ques-tion.
?More general?
and ?not relevant?
reformu-lations usually decrease the performance.Third, we conduct the error analysis on the ques-tion reformulations that decrease the performanceof the original question.
Three typical types of er-rors are observed.
First, some important words areremoved from the original question.
For example,?what is the role of corporate executives?
is reformu-lated as ?corporate executives?.
Second, the refor-mulation is too specific.
For example, ?how to effec-tively organize your classroom?
is reformulated as?how to effectively organize your elementary class-room?.
Third, some reformulations entirely changeTable 7: Analysis of different types of reformulations.Type increase decrease sameMorphological change 11% 10% 79%Equivalent meaning 32% 30% 38%More specific/Add words 45% 39% 16%More general/Remove words 38% 48% 14%Not relevant 14% 72% 14%Table 8: Retrieval Performance of other query processingtechniques.NDCG@1 NDCG@3 NDCG@5ORIG 0.2720 0.2937 0.3151NoStop 0.2697 0.2893 0.3112DropOne 0.2630 0.2888 0.3102QDist 0.2978 0.3052 0.3250the meaning of the original question.
For example,?what is the adjective of anxiously?
is reformulatedas ?what is the noun of anxiously?.Fourth, we compare our question reformulationmethod with two long query processing techniques,i.e.
NoStop (Huston and Croft, 2010) and DropOne(Balasubramanian et al, 2010).
NoStop removes allstopwords in the query and DropOne learns to dropa single word from the query.
The same query set asBalasubramanian et al (2010) is used.
Table 8 re-ports the retrieval performance of different methods.Table 8 shows that both NoStop and DropOne per-form worse than using the original question, whichindicates that the general techniques developed forlong queries are not appropriate for natural languagequestions.
On the other hand, our proposed methodoutperforms all the baselines.5 ConclusionImproving the search relevance of natural languagequestions poses a great challenge for search systems.We propose to automatically mine 5w1h question re-formulation patterns from search log data.
The ef-fectiveness of the extracted patterns has been shownon web search.
These patterns are potentially usefulfor many other applications, which will be studied inthe future work.
How to automatically classify theextracted patterns is also an interesting future issue.AcknowledgmentsWe would like to thank W. Bruce Croft for his sug-gestions and discussions.190ReferencesN.
Balasubramanian, G. Kumaran, and V.R.
Carvalho.2010.
Exploring reductions for long web queries.
InProceeding of the 33rd international ACM SIGIR con-ference on Research and development in informationretrieval, pages 571?578.
ACM.C.
Bannard and C. Callison-Burch.
2005.
Paraphras-ing with bilingual parallel corpora.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics, pages 597?604.
Association forComputational Linguistics.R.
Barzilay and K.R.
McKeown.
2001.
Extracting para-phrases from a parallel corpus.
In Proceedings ofthe 39th Annual Meeting on Association for Computa-tional Linguistics, pages 50?57.
Association for Com-putational Linguistics.S.
Bergsma and Q. I. Wang.
2007.
Learning nounphrase query segmentation.
In EMNLP-CoNLL07,pages 819?826, Prague.R.
Bhagat and D. Ravichandran.
2008.
Large scale ac-quisition of paraphrases for learning surface patterns.Proceedings of ACL-08: HLT, pages 674?682.Paolo Boldi, Francesco Bonchi, Carlos Castillo, Deb-ora Donato, Aristides Gionis, and Sebastiano Vigna.2008.
The query-flow graph: model and applications.In CIKM08, pages 609?618.P.
Boldi, F. Bonchi, C. Castillo, and S. Vigna.
2009.From ?Dango?
to ?Japanese Cakes?
: Query refor-mulation models and patterns.
In Web Intelligenceand Intelligent Agent Technologies, 2009.
WI-IAT?09.IEEE/WIC/ACM International Joint Conferences on,volume 1, pages 183?190.
IEEE.C.
Callison-Burch, P. Koehn, and M. Osborne.
2006.Improved statistical machine translation using para-phrases.
In Proceedings of the main conference onHuman Language Technology Conference of the NorthAmerican Chapter of the Association of Computa-tional Linguistics, pages 17?24.
Association for Com-putational Linguistics.C.
Callison-Burch.
2008.
Syntactic constraints on para-phrases extracted from parallel corpora.
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing, pages 196?205.
Associationfor Computational Linguistics.Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li,Kuansan Wang, Fritz Behr, and C. Lee Giles.
2010.Exploring web scale language models for search queryprocessing.
In WWW10, pages 451?460, New York,NY, USA.
ACM.S.
Huston and W.B.
Croft.
2010.
Evaluating ver-bose query processing techniques.
In Proceedingof the 33rd international ACM SIGIR conference onResearch and development in information retrieval,pages 291?298.
ACM.B.J.
Jansen, D.L.
Booth, and A. Spink.
2009.
Patternsof query reformulation during web searching.
Journalof the American Society for Information Science andTechnology, 60(7):1358?1371.R.
Jones, B. Rey, O. Madani, andW.
Greiner.
2006.
Gen-erating query substitutions.
In WWW06, pages 387?396, Ediburgh, Scotland.D.
Kauchak and R. Barzilay.
2006.
Paraphrasing forautomatic evaluation.D.-K. Lin and P. Pantel.
2001.
Discovery of inferencerules for question answering.
Natural Language Pro-cessing, 7(4):343?360.K.R.
McKeown, R. Barzilay, D. Evans, V. Hatzi-vassiloglou, J.L.
Klavans, A. Nenkova, C. Sable,B.
Schiffman, and S. Sigelman.
2002.
Tracking andsummarizing news on a daily basis with columbia?snewsblaster.
In Proceedings of the second interna-tional conference on Human Language TechnologyResearch, pages 280?285.MorganKaufmann Publish-ers Inc.B.
Pang, K. Knight, and D. Marcu.
2003.
Syntax-basedalignment of multiple translations: Extracting para-phrases and generating new sentences.
In Proceedingsof the 2003 Conference of the North American Chap-ter of the Association for Computational Linguistics onHuman Language Technology-Volume 1, pages 102?109.
Association for Computational Linguistics.M.
Pas?ca and P. Dienes.
2005.
Aligning needles in ahaystack: Paraphrase acquisition across the web.
Nat-ural Language Processing?IJCNLP 2005, pages 119?130.M.
Pas?ca and B.
Van Durme.
2008.
Weakly-supervisedacquisition of open-domain classes and class attributesfrom web documents and query logs.
In Proceed-ings of the 46th Annual Meeting of the Association forComputational Linguistics (ACL-08), pages 19?27.J.
M. Ponte and W. B. Croft.
1998.
A language modelingapproach to information retrieval.
In SIGIR98, pages275?281, Melbourne, Australia.D.
Ravichandran and E. Hovy.
2002.
Learning sur-face text patterns for a question answering system.
InACL02, pages 41?47.B.
Tan and F. Peng.
2008.
Unsupervised querysegmentation using generative language models andWikipedia.
In WWW08, pages 347?356, Bei-jing,China.X.
Wang and C. Zhai.
2008.
Mining term associationpatterns from search logs for effective query reformu-lation.
In CIKM08, pages 479?488, Napa Valley, CA.X.
Xue and W. B. Croft.
2010.
Representing queriesas distributions.
In SIGIR10 Workshop on Query Rep-191resentation and Understanding, pages 9?12, Geneva,Switzerland.C.
Zhai and J. Lafferty.
2001.
A study of smoothingmethods for language models applied to ad hoc infor-mation retrieval.
In SIGIR01, pages 334?342, NewOrleans, LA.S.
Zhao, H. Wang, T. Liu, and S. Li.
2008.
Pivot ap-proach for extracting paraphrase patterns from bilin-gual corpora.
Proceedings of ACL-08: HLT, pages780?788.S.
Zhao, H. Wang, and T. Liu.
2010.
Paraphrasing withsearch engine query logs.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics, pages 1317?1325.
Association for ComputationalLinguistics.192
