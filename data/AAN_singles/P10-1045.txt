Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 435?444,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsLatent variable models of selectional preferenceDiarmuid O?
Se?aghdhaUniversity of CambridgeComputer LaboratoryUnited Kingdomdo242@cl.cam.ac.ukAbstractThis paper describes the application ofso-called topic models to selectional pref-erence induction.
Three models relatedto Latent Dirichlet Allocation, a provenmethod for modelling document-word co-occurrences, are presented and evaluatedon datasets of human plausibility judge-ments.
Compared to previously proposedtechniques, these models perform verycompetitively, especially for infrequentpredicate-argument combinations wherethey exceed the quality of Web-scale pre-dictions while using relatively little data.1 IntroductionLanguage researchers have long been aware thatmany words place semantic restrictions on thewords with which they can co-occur in a syntacticrelationship.
Violations of these restrictions makethe sense of a sentence odd or implausible:(1) Colourless green ideas sleep furiously.
(2) The deer shot the hunter.Recognising whether or not a selectional restrictionis satisfied can be an important trigger for metaphor-ical interpretations (Wilks, 1978) and also plays arole in the time course of human sentence process-ing (Rayner et al, 2004).
A more relaxed notion ofselectional preference captures the idea that certainclasses of entities are more likely than others tofill a given argument slot of a predicate.
In Natu-ral Language Processing, knowledge about proba-ble, less probable and wholly infelicitous predicate-argument pairs is of value for numerous applica-tions, for example semantic role labelling (Gildeaand Jurafsky, 2002; Zapirain et al, 2009).
Thenotion of selectional preference is not restrictedto surface-level predicates such as verbs and mod-ifiers, but also extends to semantic frames (Erk,2007) and inference rules (Pantel et al, 2007).The fundamental problem that selectional prefer-ence models must address is data sparsity: in manycases insufficient corpus data is available to reliablymeasure the plausibility of a predicate-argumentpair by counting its observed frequency.
A rarelyseen pair may be fundamentally implausible (acarrot laughed) or plausible but rarely expressed(a manservant laughed).1 In general, it is benefi-cial to smooth plausibility estimates by integratingknowledge about the frequency of other, similarpredicate-argument pairs.
The task thus share someof the nature of language modelling; however, it isa task less amenable to approaches that require verylarge training corpora and one where the semanticquality of a model is of greater importance.This paper takes up tools (?topic models?
)that have been proven successful in modellingdocument-word co-occurrences and adapts themto the task of selectional preference learning.
Ad-vantages of these models include a well-definedgenerative model that handles sparse data well,the ability to jointly induce semantic classes andpredicate-specific distributions over those classes,and the enhanced statistical strength achieved bysharing knowledge across predicates.
Section 2surveys prior work on selectional preference mod-elling and on semantic applications of topic models.Section 3 describes the models used in our exper-iments.
Section 4 provides details of the experi-mental design.
Section 5 presents results for ourmodels on the task of predicting human plausibilityjudgements for predicate-argument combinations;we show that performance is generally competi-1At time of writing, Google estimates 855 hitsfor ?a|the carrot|carrots laugh|laughs|laughed?
and 0hits for ?a|the manservant|manservants|menservantslaugh|laughs|laughed?
; many of the carrot hits are falsepositives but a significant number are true subject-verbobservations.435tive with or superior to a number of other models,including models using Web-scale resources, espe-cially for low-frequency examples.
In Section 6 wewrap up by summarising the paper?s conclusionsand sketching directions for future research.2 Related work2.1 Selectional preference learningThe representation (and latterly, learning) of selec-tional preferences for verbs and other predicateshas long been considered a fundamental problemin computational semantics (Resnik, 1993).
Manyapproaches to the problem use lexical taxonomiessuch as WordNet to identify the semantic classesthat typically fill a particular argument slot for apredicate (Resnik, 1993; Clark and Weir, 2002;Schulte im Walde et al, 2008).
In this paper, how-ever, we focus on methods that do not assumethe availability of a comprehensive taxonomy butrather induce semantic classes automatically froma corpus of text.
Such methods are more generallyapplicable, for example in domains or languageswhere handbuilt semantic lexicons have insufficientcoverage or are non-existent.Rooth et al (1999) introduced a model of se-lectional preference induction that casts the prob-lem in a probabilistic latent-variable framework.In Rooth et al?s model each observed predicate-argument pair is probabilistically generated from alatent variable, which is itself generated from an un-derlying distribution on variables.
The use of latentvariables, which correspond to coherent clustersof predicate-argument interactions, allow proba-bilities to be assigned to predicate-argument pairswhich have not previously been observed by themodel.
The discovery of these predicate-argumentclusters and the estimation of distributions on latentand observed variables are performed simultane-ously via an Expectation Maximisation procedure.The work presented in this paper is inspired byRooth et al?s latent variable approach, most di-rectly in the model described in Section 3.3.
Erk(2007) and Pado?
et al (2007) describe a corpus-driven smoothing model which is not probabilisticin nature but relies on similarity estimates froma ?semantic space?
model that identifies semanticsimilarity with closeness in a vector space of co-occurrences.
Bergsma et al (2008) suggest learn-ing selectional preferences in a discriminative way,by training a collection of SVM classifiers to recog-nise likely and unlikely arguments for predicatesof interest.Keller and Lapata (2003) suggest a simple al-ternative to smoothing-based approaches.
Theydemonstrate that noisy counts from a Web searchengine can yield estimates of plausibility forpredicate-argument pairs that are superior to mod-els learned from a smaller parsed corpus.
The as-sumption inherent in this approach is that given suf-ficient text, all plausible predicate-argument pairswill be observed with frequency roughly correlatedwith their degree of plausibility.
While the model isundeniably straightforward and powerful, it has anumber of drawbacks: it presupposes an extremelylarge corpus, the like of which will only be avail-able for a small number of domains and languages,and it is only suitable for relations that are iden-tifiable by searching raw text for specific lexicalpatterns.2.2 Topic modellingThe task of inducing coherent semantic clusters iscommon to many research areas.
In the field ofdocument modelling, a class of methods knownas ?topic models?
have become a de facto stan-dard for identifying semantic structure in docu-ments.
These include the Latent Dirichlet Al-location (LDA) model of Blei et al (2003) andthe Hierarchical Dirichlet Process model of Tehet al (2006).
Formally seen, these are hierarchi-cal Bayesian models which induce a set of latentvariables or topics that are shared across docu-ments.
The combination of a well-defined prob-abilistic model and Gibbs sampling procedure forestimation guarantee (eventual) convergence andthe avoidance of degenerate solutions.
As a resultof intensive research in recent years, the behaviourof topic models is well-understood and computa-tionally efficient implementations have been de-veloped.
The tools provided by this research areused in this paper as the building blocks of ourselectional preference models.Hierarchical Bayesian modelling has recentlygained notable popularity in many core areas ofnatural language processing, from morphologicalsegmentation (Goldwater et al, 2009) to opinionmodelling (Lin et al, 2006).
Yet so far there havebeen relatively few applications to traditional lex-ical semantic tasks.
Boyd-Graber et al (2007) in-tegrate a model of random walks on the WordNetgraph into an LDA topic model to build an unsuper-vised word sense disambiguation system.
Brody436and Lapata (2009) adapt the basic LDA model forapplication to unsupervised word sense induction;in this context, the topics learned by the model areassumed to correspond to distinct senses of a partic-ular lemma.
Zhang et al (2009) are also concernedwith inducing multiple senses for a particular term;here the goal is to identify distinct entity types inthe output of a pattern-based entity set discoverysystem.
Reisinger and Pas?ca (2009) use LDA-likemodels to map automatically acquired attributesets onto the WordNet hierarchy.
Griffiths et al(2007) demonstrate that topic models learned fromdocument-word co-occurrences are good predictorsof semantic association judgements by humans.Simultaneously to this work, Ritter et al (2010)have also investigated the use of topic modelsfor selectional preference learning.
Their goal isslightly different to ours in that they wish to modelthe probability of a binary predicate taking twospecified arguments, i.e., P (n1, n2|v), whereas wemodel the joint and conditional probabilities of apredicate taking a single specified argument.
Themodel architecture they propose, LinkLDA, fallssomewhere between our LDA and DUAL-LDAmodels.
Hence LinkLDA could be adapted to esti-mate P (n, v|r) as DUAL-LDA does, but a prelimi-nary investigation indicates that it does not performwell in this context.
The most likely explanationis that LinkLDA generates its two arguments in-dependently, which may be suitable for distinctargument positions of a given predicate but is un-suitable when one of those ?arguments?
is in factthe predicate.The models developed in this paper, though in-tended for semantic modelling, also bear some sim-ilarity to the internals of generative syntax modelssuch as the ?infinite tree?
(Finkel et al, 2007).
Insome ways, our models are less ambitious thancomparable syntactic models as they focus on spe-cific fragments of grammatical structure rather thanlearning a more general representation of sentencesyntax.
It would be interesting to evaluate whetherthis restricted focus improves the quality of thelearned model or whether general syntax modelscan also capture fine-grained knowledge about com-binatorial semantics.3 Three selectional preference models3.1 NotationIn the model descriptions below we assume a predi-cate vocabulary of V types, an argument vocab-ulary of N types and a relation vocabulary ofR types.
Each predicate type is associated witha singe relation; for example the predicate typeeat:V:dobj (the direct object of the verb eat) istreated as distinct from eat:V:subj (the subject ofthe verb eat).
The training corpus consists of Wobservations of argument-predicate pairs.
Eachmodel has at least one vocabulary of Z arbitrar-ily labelled latent variables.
fzn is the number ofobservations where the latent variable z has beenassociated with the argument type n, fzv is thenumber of observations where z has been associ-ated with the predicate type v and fzr is the numberof observations where z has been associated withthe relation r. Finally, fz?
is the total number ofobservations associated with z and f?v is the totalnumber of observations containing the predicate v.3.2 Latent Dirichlet AllocationAs noted above, LDA was originally introduced tomodel sets of documents in terms of topics, or clus-ters of terms, that they share in varying proportions.For example, a research paper on bioinformaticsmay use some vocabulary that is shared with gen-eral computer science papers and some vocabularythat is shared with biomedical papers.
The analogi-cal move from modelling document-term cooccur-rences to modelling predicate-argument cooccur-rences is intuitive: we assume that each predicate isassociated with a distribution over semantic classes(?topics?)
and that these classes are shared acrosspredicates.
The high-level ?generative story?
forthe LDA selectional preference model is as follows:(1) For each predicate v, draw a multinomial dis-tribution ?v over argument classes from aDirichlet distribution with parameters ?.
(2) For each argument class z, draw a multinomialdistribution ?z over argument types from aDirichlet with parameters ?.
(3) To generate an argument for v, draw an ar-gument class z from ?v and then draw anargument type n from ?zThe resulting model can be written as:P (n|v, r) =?zP (n|z)P (z|v, r) (1)?
?zfzn + ?fz?
+N?fzv + ?zf?v +?z?
?z?
(2)437Due to multinomial-Dirichlet conjugacy, the dis-tributions ?v and ?z can be integrated out and donot appear explicitly in the above formula.
Thefirst term in (2) can be seen as a smoothed esti-mate of the probability that class z produces theargument n; the second is a smoothed estimate ofthe probability that predicate v takes an argumentbelonging to class z.
One important point is thatthe smoothing effects of the Dirichlet priors on ?vand ?z are greatest for predicates and argumentsthat are rarely seen, reflecting an intuitive lack ofcertainty.
We assume an asymmetric Dirichlet prioron ?v (the ?
parameters can differ for each class)and a symmetric prior on ?z (all ?
parameters areequal); this follows the recommendations of Wal-lach et al (2009) for LDA.
This model estimatespredicate-argument probabilities conditional on agiven predicate v; it cannot by itself provide jointprobabilities P (n, v|r), which are needed for ourplausibility evaluation.Given a dataset of predicate-argument combina-tions and values for the hyperparameters ?
and ?,the probability model is determined by the classassignment counts fzn and fzv.
Following Grif-fiths and Steyvers (2004), we estimate the modelby Gibbs sampling.
This involves resampling thetopic assignment for each observation in turn usingprobabilities estimated from all other observations.One efficiency bottleneck in the basic sampler de-scribed by Griffiths and Steyvers is that the entireset of topics must be iterated over for each observa-tion.
Yao et al (2009) propose a reformulation thatremoves this bottleneck by separating the probabil-ity mass p(z|n, v) into a number of buckets, someof which only require iterating over the topics cur-rently assigned to instances of type n, typically farfewer than the total number of topics.
It is possibleto apply similar reformulations to the models pre-sented in Sections 3.3 and 3.4 below; depending onthe model and parameterisation this can reduce therunning time dramatically.Unlike some topic models such as HDP (Teh etal., 2006), LDA is parametric: the number of top-ics Z must be set by the user in advance.
However,Wallach et al (2009) demonstrate that LDA is rela-tively insensitive to larger-than-necessary choicesofZ when the Dirichlet parameters ?
are optimisedas part of model estimation.
In our implementationwe use the optimisation routines provided as partof the Mallet library, which use an iterative proce-dure to compute a maximum likelihood estimate ofthese hyperparameters.23.3 A Rooth et al-inspired modelIn Rooth et al?s (1999) selectional preferencemodel, a latent variable is responsible for generat-ing both the predicate and argument types of an ob-servation.
The basic LDAmodel can be extended tocapture this kind of predicate-argument interaction;the generative story for the resulting ROOTH-LDAmodel is as follows:(1) For each relation r, draw a multinomial dis-tribution ?r over interaction classes from aDirichlet distribution with parameters ?.
(2) For each class z, draw a multinomial ?z overargument types from a Dirichlet distributionwith parameters ?
and a multinomial ?z overpredicate types from a Dirichlet distributionwith parameters ?.
(3) To generate an observation for r, draw a classz from ?r, then draw an argument type nfrom ?z and a predicate type v from ?z .The resulting model can be written as:P (n, v|r) =?zP (n|z)P (v|z)P (z|r) (3)?
?zfzn + ?fz?
+N?fzv + ?fz?
+ V ?fzr + ?zf?r +?z?
?z?
(4)As suggested by the similarity between (4) and (2),the ROOTH-LDA model can be estimated by anLDA-like Gibbs sampling procedure.Unlike LDA, ROOTH-LDA does model the jointprobability P (n, v|r) of a predicate and argumentco-occurring.
Further differences are that infor-mation about predicate-argument co-occurrence isonly shared within a given interaction class ratherthan across the whole dataset and that the distribu-tion ?z is not specific to the predicate v but ratherto the relation r. This could potentially lead to aloss of model quality, but in practice the ability toinduce ?tighter?
clusters seems to counteract anydeterioration this causes.3.4 A ?dual-topic?
modelIn our third model, we attempt to combine the ad-vantages of LDA and ROOTH-LDA by cluster-ing arguments and predicates according to separate2http://mallet.cs.umass.edu/438class vocabularies.
Each observation is generatedby two latent variables rather than one, which po-tentially allows the model to learn more flexibleinteractions between arguments and predicates.
:(1) For each relation r, draw a multinomial distri-bution ?r over predicate classes from a Dirich-let with parameters ?.
(2) For each predicate class c, draw a multinomial?c over predicate types and a multinomial ?cover argument classes from Dirichlets withparameters ?
and ?
respectively.
(3) For each argument class z, draw a multinomialdistribution ?z over argument types from aDirichlet with parameters ?.
(4) To generate an observation for r, draw a predi-cate class c from ?r, a predicate type from?c,an argument class z from ?c and an argumenttype from ?z .The resulting model can be written as:P (n, v|r) =?c?zP (n|z)P (z|c)P (v|c)P (c|r)(5)?
?c?zfzn + ?fz?
+N?fzc + ?zf?c +?z?
?z?
?fcv + ?fc?
+ V ?fcr + ?cf?r +?c?
?c?
(6)To estimate this model, we first resample the classassignments for all arguments in the data andthen resample class assignments for all predicates.Other approaches are possible ?
resampling argu-ment and then predicate class assignments for eachobservation in turn, or sampling argument and pred-icate assignments together by blocked sampling ?though from our experiments it does not seem thatthe choice of scheme makes a significant differ-ence.4 Experimental setupIn the document modelling literature, probabilistictopic models are often evaluated on the likelihoodthey assign to unseen documents; however, it hasbeen shown that higher log likelihood scores donot necessarily correlate with more semanticallycoherent induced topics (Chang et al, 2009).
Onepopular method for evaluating selectional prefer-ence models is by testing the correlation betweentheir predictions and human judgements of plausi-bility on a dataset of predicate-argument pairs.
Thiscan be viewed as a more semantically relevant mea-surement of model quality than likelihood-basedmethods, and also permits comparison with non-probabilistic models.
In Section 5, we use twoplausibility datasets to evaluate our models andcompare to other previously published results.We trained our models on the 90-million wordwritten component of the British National Corpus(Burnard, 1995), parsed with the RASP toolkit(Briscoe et al, 2006).
Predicates occurring withjust one argument type were removed, as were alltokens containing non-alphabetic characters; noother filtering was done.
The resulting datasets con-sisted of 3,587,172 verb-object observations with7,954 predicate types and 80,107 argument types,3,732,470 noun-noun observations with 68,303predicate types and 105,425 argument types, and3,843,346 adjective-noun observations with 29,975predicate types and 62,595 argument types.During development we used the verb-noun plau-sibility dataset from Pado?
et al (2007) to directthe design of the system.
Unless stated other-wise, all results are based on runs of 1,000 iter-ations with 100 classes, with a 200-iteration burninperiod after which hyperparameters were reesti-mated every 50 iterations.3 The probabilities es-timated by the models (P (n|v, r) for LDA andP (n, v|r) for ROOTH- and DUAL-LDA) weresampled every 50 iterations post-burnin and av-eraged over three runs to smooth out variance.To compare plausibility scores for different pred-icates, we require the joint probability P (n, v|r);as LDA does not provide this, we approximatePLDA(n, v|r) = PBNC(v|r)PLDA(n|v, r), wherePBNC(v|r) is proportional to the frequency withwhich predicate v is observed as an instance ofrelation r in the BNC.For comparison, we reimplemented the methodsof Rooth et al (1999) and Pado?
et al (2007).
Asmentioned above, Rooth et al use a latent-variablemodel similar to (4) but without priors, trainedvia EM.
Our implementation (henceforth ROOTH-EM) chooses the number of classes from the range(20, 25, .
.
.
, 50) through 5-fold cross-validation ona held-out log-likelihood measure.
Settings outsidethis range did not give good results.
Again, we runfor 1,000 iterations and average predictions over3These settings were based on the MALLET defaults; wehave not yet investigated whether modifying the simulationlength or burnin period is beneficial.439LDA 0 Nouns: agreement, contract, permission, treaty, deal, .
.
.1 Nouns information, datum, detail, evidence, material, .
.
.2 Nouns skill, knowledge, country, technique, understanding, .
.
.ROOTH-LDA 0 Nouns force, team, army, group, troops, .
.
.0 Verbs join, arm, lead, beat, send, .
.
.1 Nouns door, eye, mouth, window, gate, .
.
.1 Verbs open, close, shut, lock, slam, .
.
.DUAL-LDA 0N Nouns house, building, site, home, station, .
.
.1N Nouns stone, foot, bit, breath, line, .
.
.0V Verbs involve, join, lead, represent, concern, .
.
.1V Verbs see, break, have, turn, round, .
.
.ROOTH-EM 0 Nouns system, method, technique, skill, model, .
.
.0 Verbs use, develop, apply, design, introduce, .
.
.1 Nouns eye, door, page, face, chapter,.
.
.1 Verbs see, open, close, watch, keep,.
.
.Table 1: Most probable words for sample semantic classes induced from verb-object observationsthree runs.
Pado?
et al (2007), a refinement of Erk(2007), is a non-probabilistic method that smoothspredicate-argument counts with counts for other ob-served arguments of the same predicate, weightedby the similarity between arguments.
Followingtheir description, we use a 2,000-dimensional spaceof syntactic co-occurrence features appropriate tothe relation being predicted, weight features withthe G2 transformation and compute similarity withthe cosine measure.5 Results5.1 Induced semantic classesTable 1 shows sample semantic classes induced bymodels trained on the corpus of BNC verb-objectco-occurrences.
LDA clusters nouns only, whileROOTH-LDA and ROOTH-EM learn classes thatgenerate both nouns and verbs and DUAL-LDAclusters nouns and verbs separately.
The LDA clus-ters are generally sensible: class 0 is exemplifiedby agreement and contract and class 1 by informa-tion and datum.
There are some unintuitive blips,for example country appears between knowledgeand understanding in class 2.
The ROOTH-LDAclasses also feel right: class 0 deals with nounssuch as force, team and army which one might join,arm or lead and class 1 corresponds to ?things thatcan be opened or closed?
such as a door, an eye or amouth (though the model also makes the question-able prediction that all these items can plausiblybe locked or slammed).
The DUAL-LDA classesare notably less coherent, especially when it comesto clustering verbs: DUAL-LDA?s class 0V, likeROOTH-LDA?s class 0, has verbs that take groupsas objects but its class 1V mixes sensible confla-tions (turn, round) with very common verbs such assee and have and the unrelated break.
The generalimpression given by inspection of the DUAL-LDAmodel is that it has problems with mixing and doesnot manage to learn a good model; we have trieda number of solutions (e.g., blocked sampling ofargument and predicate classes), without overcom-ing this brittleness.
Unsurprisingly, ROOTH-EM?sclasses have a similar feel to ROOTH-LDA; ourgeneral impression is that some of ROOTH-EM?sclasses look even more coherent than the LDA-based models, presumably because it does not usepriors to smooth its per-class distributions.5.2 Comparison with Keller and Lapata(2003)Keller and Lapata (2003) collected a dataset ofhuman plausibility judgements for three classesof grammatical relation: verb-object, noun-nounmodification and adjective-noun modification.
Theitems in this dataset were not chosen to balanceplausibility and implausibility (as in prior psy-cholinguistic experiments) but according to theircorpus frequency, leading to a more realistic task.30 predicates were selected for each relation;each predicate was matched with three argumentsfrom different co-occurrence bands in the BNC,e.g., naughty-girl (high frequency), naughty-dog(medium) and naughty-lunch (low).
Each predicatewas also matched with three random arguments440Verb-object Noun-noun Adjective-nounSeen Unseen Seen Unseen Seen Unseenr ?
r ?
r ?
r ?
r ?
r ?AltaVista (KL) .641 ?
.551 ?
.700 ?
.578 ?
.650 ?
.480 ?Google (KL) .624 ?
.520 ?
.692 ?
.595 ?
.641 ?
.473 ?BNC (RASP) .620 .614 .196 .222 .544 .604 .114 .125 .543 .622 .135 .102ROOTH-EM .455 .487 .479 .520 .503 .491 .586 .625 .514 .463 .395 .355Pado?
et al .484 .490 .398 .430 .431 .503 .558 .533 .479 .570 .120 .138LDA .504 .541 .558 .603 .615 .641 .636 .666 .594 .558 .468 .459ROOTH-LDA .520 .548 .564 .605 .607 .622 .691 .722 .575 .599 .501 .469DUAL-LDA .453 .494 .446 .516 .496 .494 .553 .573 .460 .400 .334 .278Table 2: Results (Pearson r and Spearman ?
correlations) on Keller and Lapata?s (2003) plausibility datawith which it does not co-occur in the BNC (e.g.,naughty-regime, naughty-rival, naughty-protocol).In this way two datasets (Seen and Unseen) of 90items each were assembled for each predicate.Table 2 presents results for a variety of predictivemodels ?
the Web frequencies reported by Kellerand Lapata (2003) for two search engines, frequen-cies from the RASP-parsed BNC,4 the reimple-mented methods of Rooth et al (1999) and Pado?
etal.
(2007), and the LDA, ROOTH-LDA and DUAL-LDA topic models.
Following Keller and Lapata,we report Pearson correlation coefficients betweenlog-transformed predicted frequencies and the gold-standard plausibility scores (which are already log-transformed).
We also report Spearman rank cor-relations except where we do not have the origi-nal predictions (the Web count models), for com-pleteness and because the predictions of preferencemodels are may not be log-normally distributed ascorpus counts are.
Zero values (found only in theBNC frequency predictions) were smoothed by 0.1to facilitate the log transformation; it seems naturalto take a zero prediction as a non-specific predic-tion of very low plausibility rather than a ?missingvalue?
as is done in other work (e.g., Pado?
et al,2007).Despite their structural differences, LDA andROOTH-LDA perform similarly - indeed, theirpredictions are highly correlated.
ROOTH-LDAscores best overall, outperforming Pado?
et al?s(2007) method and ROOTH-EM on every datasetand evaluation measure, and outperforming Kellerand Lapata?s (2003) Web predictions on every Un-4The correlations presented here for BNC counts are no-tably better than those reported by Keller and Lapata (2003),presumably reflecting our use of full parsing rather than shal-low parsing.seen dataset.
LDA also performs consistently well,surpassing ROOTH-EM and Pado?
et al on all butone occasion.
For frequent predicate-argumentpairs (Seen datasets), Web counts are clearly better;however, the BNC counts are unambiguously supe-rior to LDA and ROOTH-LDA (whose predictionsare based entirely on the generative model even forobserved items) for the Seen verb-object data only.As might be suspected from the mixing problemsobserved with DUAL-LDA, this model does notperform as well as LDA and ROOTH-LDA, thoughit does hold its own against the other selectionalpreference methods.To identify significant differences between mod-els, we use the statistical test for correlated corre-lation coefficients proposed by Meng et al (1992),which is appropriate for correlations that sharethe same gold standard.5 For the seen data thereare few significant differences: ROOTH-LDA andLDA are significantly better (p < 0.01) than Pado?et al?s model for Pearson?s r on seen noun-noundata, and ROOTH-LDA is also significantly better(p < 0.01) using Spearman?s ?.
For the unseendatasets, the BNC frequency predictions are unsur-prisingly significantly worse at the p < 0.01 levelthan all smoothing models.
LDA and ROOTH-LDA are significantly better (p < 0.01) than Pado?et al on every unseen dataset; ROOTH-EM is sig-nificantly better (p < 0.01) than Pado?
et al onUnseen adjectives for both correlations.
Meng etal.
?s test does not find significant differences be-tween ROOTH-EM and the LDA models despitethe latter?s clear advantages (a number of condi-tions do come close).
This is because their pre-dictions are highly correlated, which is perhaps5We cannot compare our data to Keller and Lapata?s Webcounts as we do not possess their per-item scores.44150 100 150 20000.10.20.30.40.50.60.70.80.91No.
of classes?
(a) Verb-object50 100 150 20000.10.20.30.40.50.60.70.80.91No.
of classes?
(b) Noun-noun50 100 150 20000.10.20.30.40.50.60.70.80.91No.
of classes?
(c) Adjective-nounFigure 1: Effect of number of argument classes on Spearman rank correlation with LDA: the solid anddotted lines show the Seen and Unseen datasets respectively; bars show locations of individual samplesunsurprising given that they are structurally similarmodels trained on the same data.
We hypothesisethat the main reason for the superior numerical per-formance of the LDA models over EM is the prin-cipled smoothing provided by the use of Dirichletpriors, which has a small but discriminative effecton model predictions.
Collating the significancescores, we find that ROOTH-LDA achieves themost positive outcomes, followed by LDA and thenby ROOTH-EM.
DUAL-LDA is found significantlybetter than Pado?
et al?s model on unseen adjective-noun combinations, and significantly worse thanthe same model on seen adjective-noun data.Latent variable models that use EM for infer-ence can be very sensitive to the number of latentvariables chosen.
For example, the performanceof ROOTH-EM worsens quickly if the number ofclusters is overestimated; for the Keller and Lap-ata datasets, settings above 50 classes lead to clearoverfitting and a precipitous drop in Pearson cor-relation scores.
On the other hand, Wallach et al(2009) demonstrate that LDA is relatively insensi-tive to the choice of topic vocabulary size Z whenthe ?
and ?
hyperparameters are optimised appro-priately during estimation.
Figure 1 plots the effectof Z on Spearman correlation for the LDA model.In general, Wallach et al?s finding for documentmodelling transfers to selectional preference mod-els; within the range Z = 50?200 performanceremains at a roughly similar level.
In fact, we donot find that performance becomes significantlyless robust when hyperparameter reestimation isdeactiviated; correlation scores simply drop by asmall amount (1?2 points), irrespective of the Zchosen.
ROOTH-LDA (not graphed) seems slightlymore sensitive to Z; this may be because the ?
pa-rameters in this model operate on the relation levelrather than the document level and thus fewer ?ob-servations?
of class distributions are available whenreestimating them.5.3 Comparison with Bergsma et al (2008)As mentioned in Section 2.1, Bergsma et al (2008)propose a discriminative approach to preferencelearning.
As part of their evaluation, they comparetheir approach to a number of others, includingthat of Erk (2007), on a plausibility dataset col-lected by Holmes et al (1989).
This dataset con-sists of 16 verbs, each paired with one plausibleobject (e.g., write-letter) and one implausible ob-ject (write-market).
Bergsma et al?s model, trainedon the 3GB AQUAINT corpus, is the only modelreported to achieve perfect accuracy on distinguish-ing plausible from implausible arguments.
It wouldbe interesting to do a full comparison that controlsfor size and type of corpus data; in the meantime,we can report that the LDA and ROOTH-LDAmodels trained on verb-object observations in theBNC (about 4 times smaller than AQUAINT) alsoachieve a perfect score on the Holmes et al data.66 Conclusions and future workThis paper has demonstrated how Bayesian tech-niques originally developed for modelling the top-ical structure of documents can be adapted tolearn probabilistic models of selectional preference.These models are especially effective for estimat-ing plausibility of low-frequency items, thus distin-guishing rarity from clear implausibility.The models presented here derive their predic-tions by modelling predicate-argument plausibilitythrough the intermediary of latent variables.
Asobserved in Section 5.2 this may be a suboptimal6Bergsma et al report that all plausible pairs were seen intheir corpus; three were unseen in ours, as well as 12 of theimplausible pairs.442strategy for frequent combinations, where corpuscounts are probably reliable and plausibility judge-ments may be affected by lexical collocation ef-fects.
One principled method for folding corpuscounts into LDA-like models would be to use hi-erarchical priors, as in the n-gram topic model ofWallach (2006).
Another potential direction forsystem improvement would be an integration ofour generative model with Bergsma et al?s (2008)discriminative model ?
this could be done in a num-ber of ways, including using the induced classesof a topic model as features for a discriminativeclassifier or using the discriminative classifier toproduce additional high-quality training data fromnoisy unparsed text.Comparison to plausibility judgements gives anintrinsic measure of model quality.
As mentionedin the Introduction, selectional preferences havemany uses in NLP applications, and it will be inter-esting to evaluate the utility of Bayesian preferencemodels in contexts such as semantic role labellingor human sentence processing modelling.
The prob-abilistic nature of topic models, coupled with anappropriate probabilistic task model, may facilitatethe integration of class induction and task learningin a tight and principled way.
We also anticipatethat latent variable models will prove effective forlearning selectional preferences of semantic predi-cates (e.g., FrameNet roles) where direct estimationfrom a large corpus is not a viable option.AcknowledgementsThis work was supported by EPSRC grantEP/G051070/1.
I am grateful to Frank Keller andMirella Lapata for sharing their plausibility data,and to Andreas Vlachos and the anonymous ACLand CoNLL reviewers for their helpful comments.ReferencesShane Bergsma, Dekang Lin, and Randy Goebel.
2008.Discriminative learning of selectional preferencesfrom unlabeled text.
In Proceedings of EMNLP-08,Honolulu, HI.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Jordan Boyd-Graber, David Blei, and Xiaojin Zhu.2007.
A topic model for word sense disambigua-tion.
In Proceedings of EMNLP-CoNLL-07, Prague,Czech Republic.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Pro-ceedings of the ACL-06 Interactive Presentation Ses-sions, Sydney, Australia.Samuel Brody and Mirella Lapata.
2009.
Bayesianword sense induction.
In Proceedings of EACL-09,Athens, Greece.Lou Burnard, 1995.
Users?
Guide for the British Na-tional Corpus.
British National Corpus Consortium,Oxford University Computing Service, Oxford, UK.Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,Chong Wang, and David M. Blei.
2009.
Readingtea leaves: How humans interpret topic models.
InProceedings of NIPS-09, Vancouver, BC.Stephen Clark and David Weir.
2002.
Class-basedprobability estimation using a semantic hierarchy.Computational Linguistics, 28(2):187?206.Katrin Erk.
2007.
A simple, similarity-based modelfor selectional preferences.
In Proceedings of ACL-07, Prague, Czech Republic.Jenny Rose Finkel, Trond Grenager, and Christopher D.Manning.
2007.
The infinite tree.
In Proceedings ofACL-07, Prague, Czech Republic.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2009.
A Bayesian framework for wordsegmentation: Exploring the effects of context.
Cog-nition, 112(1):21?54.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101(suppl.
1):5228?5235.Thomas L. Griffiths, Mark Steyvers, and Joshua B.Tenenbaum.
2007.
Topics in semantic representa-tion.
Psychological Review, 114(2):211?244.Virginia M. Holmes, Laurie Stowe, and Linda Cupples.1989.
Lexical expectations in parsing complement-verb sentences.
Journal of Memory and Language,28(6):668?689.Frank Keller and Mirella Lapata.
2003.
Using the Webto obtain frequencies for unseen bigrams.
Computa-tional Linguistics, 29(3):459?484.Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, andAlexander Hauptmann.
2006.
Which side are youon?
Identifying perspectives at the document andsentence levels.
In Proceedings of CoNLL-06, NewYork, NY.Xiao-LiMeng, Robert Rosenthal, and Donald B. Rubin.1992.
Comparing correlated correlation coefficients.Psychological Bulletin, 111(1):172?175.443Sebastian Pado?, Ulrike Pado?, and Katrin Erk.
2007.Flexible, corpus-based modelling of human plau-sibility judgements.
In Proceedings of EMNLP-CoNLL-07, Prague, Czech Republic.Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,Timothy Chklovski, and Eduard Hovy.
2007.
ISP:Learning inferential selectional preferences.
In Pro-ceedings of NAACL-HLT-07, Rochester, NY.Keith Rayner, Tessa Warren, Barbara J. Juhasz, and Si-mon P. Liversedge.
2004.
The effect of plausibilityon eye movements in reading.
Journal of Experi-mental Psychology: Learning Memory and Cogni-tion, 30(6):1290?1301.Joseph Reisinger and Marius Pas?ca.
2009.
Latent vari-able models of concept-attribute attachment.
In Pro-ceedings of ACL-IJCNLP-09, Singapore.Philip S. Resnik.
1993.
Selection and Information:A Class-Based Approach to Lexical Relationships.Ph.D.
thesis, University of Pennsylvania.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A La-tent Dirichlet Allocation method for selectional pref-erences.
In Proceedings of ACL-10, Uppsala, Swe-den.Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-roll, and Franz Beil.
1999.
Inducing a semanticallyannotated lexicon via EM-based clustering.
In Pro-ceedings of ACL-99, College Park, MD.Sabine Schulte im Walde, Christian Hying, ChristianScheible, and Helmut Schmid.
2008.
CombiningEM training and the MDL principle for an automaticverb classification incorporating selectional prefer-ences.
In Proceedings of ACL-08:HLT, Columbus,OH.Yee W. Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.HannaWallach, DavidMimno, and AndrewMcCallum.2009.
Rethinking LDA: Why priors matter.
In Pro-ceedings of NIPS-09, Vancouver, BC.Hanna Wallach.
2006.
Topic modeling: Beyond bag-of-words.
In Proceedings of ICML-06, Pittsburgh,PA.Yorick Wilks.
1978.
Making preferences more active.Artificial Intelligence, 11:197?225.Limin Yao, David Mimno, and Andrew McCallum.2009.
Efficient methods for topic model inferenceon streaming document collections.
In Proceedingsof KDD-09, Paris, France.Ben?at Zapirain, Eneko Agirre, and Llu?
?s Ma`rquez.2009.
Generalizing over lexical features: Selec-tional preferences for semantic role classification.
InProceedings of ACL-IJCNLP-09, Singapore.Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-RongWen.
2009.
Employing topic models for pattern-based semantic class discovery.
In Proceedings ofACL-IJCNLP-09, Singapore.444
