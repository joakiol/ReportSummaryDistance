Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),pages 70?79, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational LinguisticsHedge Detection as a Lens on Framing in the GMO Debates:A Position PaperEunsol Choi?, Chenhao Tan?, Lillian Lee?, Cristian Danescu-Niculescu-Mizil?
and Jennifer Spindel?
?Department of Computer Science, ?Department of Plant Breeding and GeneticsCornell Universityec472@cornell.edu, chenhao|llee|cristian@cs.cornell.edu, jes462@cornell.eduAbstractUnderstanding the ways in which participantsin public discussions frame their arguments isimportant in understanding how public opin-ion is formed.
In this paper, we adopt the po-sition that it is time for more computationally-oriented research on problems involving fram-ing.
In the interests of furthering that goal,we propose the following specific, interestingand, we believe, relatively accessible ques-tion: In the controversy regarding the useof genetically-modified organisms (GMOs) inagriculture, do pro- and anti-GMO articles dif-fer in whether they choose to adopt a more?scientific?
tone?Prior work on the rhetoric and sociology ofscience suggests that hedging may distin-guish popular-science text from text writtenby professional scientists for their colleagues.We propose a detailed approach to studyingwhether hedge detection can be used to un-derstanding scientific framing in the GMO de-bates, and provide corpora to facilitate thisstudy.
Some of our preliminary analyses sug-gest that hedges occur less frequently in scien-tific discourse than in popular text, a findingthat contradicts prior assertions in the litera-ture.
We hope that our initial work and datawill encourage others to pursue this promisingline of inquiry.1 Introduction1.1 Framing, ?scientific discourse?, and GMOsin the mediaThe issue of framing (Goffman, 1974; Scheufele,1999; Benford and Snow, 2000) is of great im-portance in understanding how public opinion isformed.
In their Annual Review of Political Sciencesurvey, Chong and Druckman (2007) describe fram-ing effects as occurring ?when (often small) changesin the presentation of an issue or an event produce(sometimes large) changes of opinion?
(p. 104);as an example, they cite a study wherein respon-dents answered differently, when asked whether ahate group should be allowed to hold a rally, depend-ing on whether the question was phrased as one of?free speech?
or one of ?risk of violence?.The genesis of our work is in a framing questionmotivated by a relatively current political issue.
Inmedia coverage of transgenic crops and the use ofgenetically modified organisms (GMOs) in food, dopro-GMO vs. anti-GMO articles differ not just withrespect to word choice, but in adopting a more ?sci-entific?
discourse, meaning the inclusion of moreuncertainty and fewer emotionally-laden words?
Weview this as an interesting question from a text anal-ysis perspective (with potential applications and im-plications that lie outside the scope of this article).1.2 Hedging as a sign of scientific discourseTo obtain a computationally manageable character-ization of ?scientific discourse?, we turned to stud-ies of the culture and language of science, a bodyof work spanning fields ranging from sociology toapplied linguistics to rhetoric and communication(Gilbert and Mulkay, 1984; Latour, 1987; Latourand Woolgar, 1979; Halliday and Martin, 1993; Baz-erman, 1988; Fahnestock, 2004; Gross, 1990).One characteristic that has drawn quite a bit ofattention in such studies is hedging (Myers, 1989;70Hyland, 1998; Lewin, 1998; Salager-Meyer, 2011).1Hyland (1998, pg.
1) defines hedging as the ex-pression of ?tentativeness and possibility?
in com-munication, or, to put it another way, language cor-responding to ?the writer withholding full commit-ment to statements?
(pg.
3).
He supplies manyreal-life examples from scientific research articles,including the following:1.
?It seems that this group plays a critical role inorienting the carboxyl function?
(emphasis Hy-land?s)2.
?...implies that phytochrome A is also not nec-essary for normal photomorphogenesis, at leastunder these irradiation conditions?
(emphasisHyland?s)3.
?We wish to suggest a structure for the salt ofdeoxyribose nucleic acid (D.N.A.)?
(emphasisadded)2Several scholars have asserted the centrality of hedg-ing in scientific and academic discourse, which cor-responds nicely to the notion of ?more uncertainty?mentioned above.
Hyland (1998, p. 6) writes, ?De-spite a widely held belief that professional scientificwriting is a series of impersonal statements of factwhich add up to the truth, hedges are abundant inscience and play a critical role in academic writing?.Indeed, Myers (1989, p. 13) claims that in scien-tific research articles, ?The hedging of claims is socommon that a sentence that looks like a claim buthas no hedging is probably not a statement of newknowledge?.3Not only is understanding hedges important to un-derstanding the rhetoric and sociology of science,but hedge detection and analysis ?
in the sense ofidentifying uncertain or uncertainly-sourced infor-mation (Farkas et al, 2010) ?
has important appli-cations to information extraction, broadly construed,and has thus become an active sub-area of natural-language processing.
For example, the CoNLL 20101In linguistics, hedging has been studied since the 1970s(Lakoff, 1973).2This example originates from Watson and Crick?s land-mark 1953 paper.
Although the sentence is overtly tentative,did Watson and Crick truly intend to be polite and modest intheir claims?
See Varttala (2001) for a review of arguments re-garding this question.3Note the inclusion of the hedge ?probably?.Shared Task was devoted to this problem (Farkaset al, 2010).Putting these two lines of research together, wesee before us what appears to be an interesting in-terdisciplinary and, at least in principle, straightfor-ward research program: relying on the aforemen-tioned rhetoric analyses to presume that hedging isa key characteristic of scientific discourse, build ahedge-detection system to computationally ascertainwhich proponents in the GMO debate tend to usemore hedges and thus, by presumption, tend to adopta more ?scientific?
frame.41.3 ContributionsOur overarching goal in this paper is to convincemore researchers in NLP and computational linguis-tics to work on problems involving framing.
Wetry to do so by proposing a specific problem thatmay be relatively accessible.
Despite the apparentdifficulty in addressing such questions, we believethat progress can be made by drawing on observa-tions drawn from previous literature across manyfields, and integrating such work with movementsin the computational community toward considera-tion of extra-propositional and pragmatic concerns.We have thus intentionally tried to ?cover a lot ofground?, as one referee put it, in the introductorymaterial just discussed.Since framing problems are indeed difficult, weelected to narrow our scope in the hope of makingsome partial progress.
Our technical goal here, atthis workshop, where hedge detection is one of themost relevant topics to the broad questions we haveraised, is not to learn to classify texts as being pro-vs. anti-GMO, or as being scientific or not, per se.5Our focus is on whether hedging specifically, con-sidered as a single feature, is correlated with thesedifferent document classes, because of the previousresearch attention that has been devoted to hedgingin particular and because of hedging being one of thetopics of this workshop.
The point of this paper is4However, this presumption that more hedges characterize amore scientific discourse has been contested.
See section 2 fordiscussion and section 4.2 for our empirical investigation.5Several other groups have addressed the problem of try-ing to identify different sides or perspectives (Lin et al, 2006;Hardisty et al, 2010; Beigman Klebanov et al, 2010; Ahmedand Xing, 2010).71thus not to compare the efficacy of hedging featureswith other types, such as bag-of-words features.
Ofcourse, to do so is an important and interesting di-rection for future work.In the end, we were not able to achieve satisfac-tory results even with respect to our narrowed goal.However, we believe that other researchers may beable to follow the plan of attack we outline below,and perhaps use the data we are releasing, in orderto achieve our goal.
We would welcome hearing theresults of other people?s efforts.2 How should we test whether hedgingdistinguishes scientific text?One very important point that we have not yet ad-dressed is: While the literature agrees on the impor-tance of hedging in scientific text, the relative de-gree of hedging in scientific vs. non-scientific text isa matter of debate.On the one side, we have assertions like those ofFahnestock (1986), who shows in a clever, albeitsmall-scale, study involving parallel texts that whenscientific observations pass into popular accounts,changes include ?removing hedges ... thus con-ferring greater certainty on the reported facts?
(pg.275).
Similarly, Juanillo, Jr. (2001) refers to a shiftfrom a forensic style to a ?celebratory?
style whenscientific research becomes publicized, and creditsBrown (1998) with noting that ?celebratory scien-tific discourses tend to pay less attention to caveats,contradictory evidence, and qualifications that arehighlighted in forensic or empiricist discourses.
Bydownplaying scientific uncertainty, it [sic] alludes togreater certainty of scientific results for public con-sumption?
(Juanillo, Jr., 2001, p. 42).However, others have contested claims that thepopularization process involves simplification, dis-tortion, hype, and dumbing down, as Myers (2003)colorfully puts it; he provides a critique of the rel-evant literature.
Varttala (1999) ran a corpus anal-ysis in which hedging was found not just in pro-fessional medical articles, but was also ?typical ofpopular scientific articles dealing with similar top-ics?
(p. 195).
Moreover, significant variation in useof hedging has been found across disciplines and au-thors?
native language; see Salager-Meyer (2011) orVarttala (2001) for a review.To the best of our knowledge, there have been nolarge-scale empirical studies validating the hypoth-esis that hedges appear more or less frequently inscientific discourse.Proposed procedure Given the above, our firststep must be to determine whether hedges are moreor less prominent in ?professional scientific?
(hence-forth ?prof-science??)
vs. ?public science?
(hence-forth ?pop-science?)
discussions of GMOs.
Ofcourse, for a large-scale study, finding hedges re-quires developing and training an effective hedge de-tection algorithm.If the first step shows that hedges can indeed beused to effectively distinguish prof-science vs. pop-science discourse on GMOs, then the second step isto examine whether the use of hedging in pro-GMOarticles follows our inferred ?scientific?
occurrencepatterns to a greater extent than the hedging in anti-GMO articles.However, as our hedge classifier trained on theCoNLL dataset did not perform reliably on the dif-ferent domain of prof-science vs. pop-science dis-cussions of GMOs, we focus the main content of thispaper on the first step.
We describe data collectionfor the second step in the appendix.3 DataTo accomplish the first step of our proposed pro-cedure outlined above, we first constructed a prof-science/pop-science corpus by pulling text fromWeb of Science for prof-science examples and fromLexisNexis for pop-science examples, as describedin Section 3.1.
Our corpus will be posted onlineat https://confluence.cornell.edu/display/llresearch/HedgingFramingGMOs.As noted above, computing the degree of hedg-ing in the aforementioned corpus requires access toa hedge-detection algorithm.
We took a supervisedapproach, taking advantage of the availability of theCoNLL 2010 hedge-detection training and evalua-tion corpora, described in Section 3.23.1 Prof-science/pop-science data: LEXIS andWOSAs mentioned previously, a corpus of prof-scienceand pop-science articles is required to ascertainwhether hedges are more prevalent in one or the72Dataset Doc type # docs # sentences Avg sentence length Flesch reading easeProf-science/pop-science corpusWOS abstracts 648 5596 22.35 23.39LEXIS (short) articles 928 36795 24.92 45.78Hedge-detection corporaBio (train) abstracts, articles 1273, 9 14541 (18% uncertain) 29.97 20.77Bio (eval) articles 15 5003 (16% uncertain) 31.30 30.49Wiki (train) paragraphs 2186 11111 (22% uncertain) 23.07 35.23Wiki (eval) paragraphs 2346 9634 (23% uncertain) 20.82 31.71Table 1: Basic descriptive statistics for the main corpora we worked with.
We created the first two.
Higher Fleschscores indicate text that is easier to read.other of these two writing styles.
Since our ultimategoal is to look at discourse related to GMOs, we re-strict our attention to documents on this topic.Thomson Reuter?s Web of Science (WOS), adatabase of scientific journal and conference arti-cles, was used as a source of prof-science samples.We chose to collect abstracts, rather than full scien-tific articles, because intuition suggests that the lan-guage in abstracts is more high-level than that in thebodies of papers, and thus more similar to the lan-guage one would see in a public debate on GMOs.To select for on-topic abstracts, we used the phrase?transgenic foods?
as a search keyword and dis-carded results containing any of a hand-selected listof off-topic filtering terms (e.g., ?mice?
or ?rats?
).We then made use of domain expertise to manuallyremove off-topic texts.
The process yielded 648 doc-uments for a total of 5596 sentences.Our source of pop-science articles was Lexis-Nexis (LEXIS).
On-topic documents were collectedfrom US newspapers using the search keywords ?ge-netically modified foods?
or ?transgenic crops?
andthen imposing the additional requirement that atleast two terms on a hand-selected list7 be presentin each document.
After the removal of duplicatesand texts containing more than 2000 words to deleteexcessively long articles, our final pop-science sub-corpus was composed of 928 documents.7The term list: GMO, GM, GE, genetically modified, ge-netic modification, modified, modification, genetic engineer-ing, engineered, bioengineered, franken, transgenic, spliced,G.M.O., tweaked, manipulated, engineering, pharming, aqua-culture.3.2 CoNLL hedge-detection training data 8As described in Farkas et al (2010), the motivationbehind the CoNLL 2010 shared task is that ?distin-guishing factual and uncertain information in texts isof essential importance in information extraction?.As ?uncertainty detection is extremely important forbiomedical information extraction?, one componentof the dataset is biological abstracts and full arti-cles from the BioScope corpus (Bio).
Meanwhile,the chief editors of Wikipedia have drawn the at-tention of the public to specific markers of uncer-tainty known as weasel words9: they are words orphrases ?aimed at creating an impression that some-thing specific and meaningful has been said?, when,in fact, ?only a vague or ambiguous claim, or evena refutation, has been communicated?.
An exampleis ?It has been claimed that ...?
: the claimant has notbeen identified, so the source of the claim cannot beverified.
Thus, another part of the dataset is a setof Wikipedia articles (Wiki) annotated with weasel-word information.
We view the combined Bio+Wikicorpus (henceforth the CoNLL dataset) as valuablefor developing hedge detectors, and we attempt tostudy whether classifiers trained on this data can begeneralized to other datasets.3.3 ComparisonTable 1 gives the basic statistics on the main datasetswe worked with.
Though WOS and LEXIS differ inthe total number of sentences, the average sentencelength is similar.
The average sentence length in Biois longer than that in Wiki.
The articles in WOSare markedly more difficult to read than the articles8http://www.inf.u-szeged.hu/rgai/conll2010st/9http://en.wikipedia.org/wiki/Weasel word73in LEXIS according to Flesch reading ease (Kincaidet al, 1975).4 Hedging to distinguish scientific text:Initial annotationAs noted in Section 1, it is not a priori clear whetherhedging distinguishes scientific text or that morehedges correspond to a more ?scientific?
discourse.To get an initial feeling for how frequently hedgesoccur in WOS and LEXIS, we hand-annotated asample of sentences from each.
In Section 4.1, weexplain the annotation policy of the CoNLL 2010Shared Task and our own annotation method forWOS and LEXIS.
After that, we move forward inSection 4.2 to compare the percentage of uncertainsentences in prof-science vs. pop-science text onthis small hand-labeled sample, and gain some ev-idence that there is indeed a difference in hedge oc-currence rates, although, perhaps surprisingly, thereseem to be more hedges in the pop-science texts.As a side benefit, we subsequently use thehand-labeled sample we produce to investigate theaccuracy of an automatic hedge detector in theWOS+LEXIS domain; more on this in Section 5.4.1 Uncertainty annotationCoNLL 2010 Shared Task annotation policy Asdescribed in Farkas et al (2010, pg.
4), the data an-notation polices for the CoNLL 2010 Shared Taskwere that ?sentences containing at least one cuewere considered as uncertain, while sentences withno cues were considered as factual?, where a cueis a linguistic marker that in context indicates un-certainty.
A straightforward example of a sentencemarked ?uncertain?
in the Shared Task is ?Mild blad-der wall thickening raises the question of cystitis.
?The annotated cues are not necessarily general, par-ticularly in Wiki, where some of the marked cuesare as specific as ?some of schumann?s best choralwriting?, ?people of the jewish tradition?, or ?certainleisure or cultural activities?.Note that ?uncertainty?
in the Shared Task def-inition also encompassed phrasing that ?creates animpression that something important has been said,but what is really communicated is vague, mislead-ing, evasive or ambiguous ... [offering] an opinionwithout any backup or source?.
An example of suchDataset % of uncertain sentencesWOS (estimated from 75-sentence sample) 20LEXIS (estimated from 78-sentence sample) 28Bio 17Wiki 23Table 2: Percentages of uncertain sentences.a sentence, drawn from Wikipedia and marked ?un-certain?
in the Shared Task, is ?Some people claimthat this results in a better taste than that of other dietcolas (most of which are sweetened with aspartamealone).?
; Farkas et al (2010) write, ?The ... sentencedoes not specify the source of the information, it isjust the vague term ?some people?
that refers to theholder of this opinion?.Our annotation policy We hand-annotated 200randomly-sampled sentences, half from WOS andhalf from LEXIS10, to gauge the frequency withwhich hedges occur in each corpus.
Two annota-tors each followed the rules of the CoNLL 2010Shared Task to label sentences as certain, uncertain,or not a proper sentence.11 The annotators agreed on153 proper sentences of the 200 sentences (75 fromWOS and 78 from LEXIS).
Cohen?s Kappa (Fleiss,1981) was 0.67 on the annotation, which means thatthe consistency between the two annotators was fairor good.
However, there were some interesting caseswhere the two annotators could not agree.
For ex-ample, in the sentence ?Cassava is the staple food oftropical Africa and its production, averaged over 24countries, has increased more than threefold from1980 to 2005 ...
?, one of the annotators believedthat ?more than?
made the sentence uncertain.
Theseborderline cases indicate that the definition of hedg-ing should be carefully delineated in future studies.4.2 Percentages of uncertain sentencesTo validate the hypothesis that prof-science articlescontain more hedges, we computed the percentage10We took steps to attempt to hide from the annotators anyexplicit clues as to the source of individual sentences: the sub-set of authors who did the annotation were not those that col-lected the data, and the annotators were presented the sentencesin random order.11The last label was added because of a few errors in scrapingthe data.74of uncertain sentences in our labeled data.
As shownin Table 2, we observed a trend contradicting ear-lier studies.
Uncertain sentences were more frequentin LEXIS than in WOS, though the difference wasnot statistically significant12 (perhaps not surprisinggiven the small sample size).
The same trend wasseen in the CoNLL dataset: there, too, the percent-age of uncertain sentences was significantly smallerin Bio (prof-science articles) than in Wiki.
In orderto make a stronger argument about prof-science vspop-science, however, more annotation on the WOSand LEXIS datasets is needed.5 ExperimentsAs stated in Section 1, our proposal requires devel-oping an effective hedge detection algorithm.
Ourapproach for the preliminary work described in thispaper is to re-implement Georgescul?s (2010) algo-rithm; the experimental results on the Bio+Wiki do-main, given in Section 5.1, are encouraging.
Thenwe use this method to attempt to validate (at a largerscale than in our manual pilot annotation) whetherhedges can be used to distinguish between prof-science and pop-science discourse on GMOs.
Un-fortunately, our results, given in Section 5.2, areinconclusive, since our trained model could notachieve satisfactory automatic hedge-detection ac-curacy on the WOS+LEXIS domain.5.1 MethodWe adopted the method of Georgescul (2010): Sup-port Vector Machine classification based on a Gaus-sian Radial Basis kernel function (Vapnik, 1998; Fanet al, 2005), employing n-grams from annotated cuephrases as features, as described in more detail be-low.
This method achieved the top performance inthe CoNLL 2010 Wikipedia hedge-detection task(Farkas et al, 2010), and SVMs have been proveneffective for many different applications.
We usedthe LIBSVM toolkit in our experiments13.As described in Section 3.2, there are two separatedatasets in the CoNLL dataset.
We experimented onthem separately (Bio, Wiki).
Also, to make our clas-sifier more generalizable to different datasets, we12Throughout, ?statistical significance?
refers to the studentt-test with p < .05.13http://www.csie.ntu.edu.tw/?cjlin/libsvm/also trained models based on the two datasets com-bined (Bio+Wiki).
As for features, we took advan-tage of the observation in Georgescul (2010) that thebag-of-words model does not work well for this task.We used different sets of features based on hedgecue words that have been annotated as part of theCoNLL dataset distribution14.
The basic feature setwas the frequency of each hedge cue word from thetraining corpus after removing stop words and punc-tuation and transforming words to lowercase.
Then,we extracted unigrams, bigrams and trigrams fromeach hedge cue phrase.
Table 3 shows the numberof features in different settings.
Notice that there aremany more features in Wiki.
As mentioned above,in Wiki, some cues are as specific as ?some of schu-mann?s best choral writing?, ?people of the jewishtradition?, or ?
certain leisure or cultural activities?.Taking n-grams from such specific cues can causesome sentences to be classified incorrectly.Feature source #featuresBio 220Bio (cues + bigram + trigram) 340Wiki 3740Wiki (cues + bigram + trigram) 10603Table 3: Number of features.Best cross-validation performanceDataset (C, ?)
P R FBio (40, 2?3) 84.0 92.0 87.8Wiki (30, 2?6) 64.0 76.3 69.6Bio+Wiki (10, 2?4) 66.7 78.3 72.0Table 4: Best 5-fold cross-validation performance for Bioand/or Wiki after parameter tuning.
As a reminder, werepeat that our intended final test set is the WOS+LEXIScorpus, which is disjoint from Bio+Wiki.We adopted several techniques from Georgescul(2010) to optimize performance through cross vali-dation.
Specifically, we tried different combinationsof feature sets (the cue phrases themselves, cues +14For the Bio model, we used cues extracted from Bio.
Like-wise, the Wiki model used cues from Wiki, and the Bio+Wikimodel used cues from Bio+Wiki.75Evaluation set Model P R FWOS+LEXIS Bio 54 68 60WOS+LEXIS Wiki 38 54 45WOS+LEXIS Bio+Wiki 21 93 34Sub-corpus performance of the model based on BioWOS Bio 58 73 65LEXIS Bio 52 64 57Table 5: The upper part shows the performance on WOSand LEXIS based on models trained on the CoNLLdataset.
The lower part gives the sub-corpus results forBio, which provided the best performance on the fullWOS+LEXIS corpus.unigram, cues + bigram, cues + trigram, cues + uni-gram + bigram + trigram, cues + bigram + trigram).We tuned the width of the RBF kernel (?)
and theregularization parameter (C) via grid search over thefollowing range of values: {2?9, 2?8, 2?7, .
.
.
, 24}for ?, {1, 10, 20, 30, .
.
.
, 150} for C. We also trieddifferent weighting strategies for negative and pos-itive classes (i.e., either proportional to the numberof positive instances, or uniform).
We performed 5-fold cross validation for each possible combinationof experimental settings on the three datasets (Bio,Wiki, Bio+Wiki).Table 4 shows the best performance on all threedatasets and the corresponding parameters.
In thethree datasets, cue+bigram+trigram provided thebest performance, and the weighted model con-sistently produced superior results to the uniformmodel.
The F1 measure for Bio was 87.8, whichwas satisfactory, while the F1 results for Wiki were69.6, which were the worst of all the datasets.This resonates with our observation that the task onWikipedia is more subtly defined and thus requiresa more sophisticated approach than counting the oc-currences of bigrams and trigrams.5.2 Results on WOS+LEXISNext, we evaluated whether our best classifiertrained on the CoNLL dataset can be generalized toother datasets, in particular, the WOS and LEXIScorpus.
Performance was measured on the 153 sen-tences on which our annotators agreed, a datasetthat was introduced in Section 4.1.
Table 5 showshow the best models trained on Bio, Wiki, andEvaluation set (C, ?)
P R FWOS + LEXIS (50, 2?9) 68 62 65WOS (50, 2?9) 85 73 79LEXIS (50, 2?9) 57 54 56Table 6: Best performance after parameter tuningbased on the 153 labeled WOS+LEXIS sentences; thisgives some idea of the upper-bound potential of ourGeorgescul-based method.
The training set is Bio, whichgave the best performance in Table 5.Bio+Wiki, respectively, performed on the 153 la-beled sentences.
First, we can see that the perfor-mance degraded significantly compared to the per-formance for in-domain cross validation.
Second, ofthe three different models, Bio showed the best per-formance.
Bio+Wiki gave the worst performance,which hints that combining two datasets and cuewords may not be a promising strategy: althoughBio+Wiki shows very good recall, this can be at-tributed to its larger feature set, which contains allavailable cues and perhaps as a result has a very highfalse-positive rate.
We further investigated and com-pared performance on LEXIS and WOS for the bestmodel (Bio).
Not surprisingly, our classifier worksbetter in WOS than in LEXIS.It is clear that there exist domain differences be-tween the CoNLL dataset and WOS+LEXIS.
To bet-ter understand the poor cross-domain performanceof the classifier, we tuned another model based onthe performance on the 153 labeled sentences us-ing Bio as training data.
As we can see in Table6, the performance on WOS improved significantly,while the performance on LEXIS decreased.
Thisis probably caused by the fact that WOS is a col-lection of scientific paper abstracts, which is moresimilar to the training corpus than LEXIS, which isa collection of news media articles15.
Also, LEXISarticles are hard to classify even with the tunedmodel, which challenges the effectiveness of a cue-words frequency approach beyond professional sci-entific texts.
Indeed, the simplicity of our reim-plementation of Georgescul?s algorithm seems tocause longer sentences to be classified as uncer-tain, because cue phrases (or n-grams extracted from15The Wiki model performed better on LEXIS than on WOS.Though the performance was not good, this result further rein-forces the possibility of a domain-dependence problem.76cue phrases) are more likely to appear in lengthiersentences.
Analysis of the best performing modelshows that the false-positive sentences are signifi-cantly longer than the false-negative ones.16Dataset Model % classified uncertainWOS Bio 16LEXIS Bio 19WOS Tuned 15LEXIS Tuned 14Table 7: For completeness, we report here the percentageof uncertain sentences in WOS and LEXIS according toour trained classifiers, although we regard these results asunreliable since those classifiers have low accuracy.
Biorefers to the best model trained on Bio only in Section 5.1,while Tuned refers to the model in Table 6 that is tunedbased on the 153 labeled sentences in WOS+LEXIS.While the cross-domain results were not reliable,we produced preliminary results on whether thereexist fewer hedges in scientific text.
We can see thatthe relative difference in certain/uncertain ratios pre-dicted by the two different models (Bio, Tuned) aredifferent in Table 7.
In the tuned model, the differ-ence between LEXIS and WOS in terms of the per-centage of uncertain sentences was not statisticallysignificant, while in the Bio model, their differencewas statistically significant.
Since the performanceof our hedge classifier on the 153 hand-annotatedWOS+LEXIS sentences was not reliable, though,we must abstain from making conclusive statementshere.6 Conclusion and future workIn this position paper, we advocated that researchersapply hedge detection not only to the classic moti-vation of information-extraction problems, but alsoto questions of how public opinion forms.
We pro-posed a particular problem in how participants in de-bates frame their arguments.
Specifically, we askedwhether pro-GMO and anti-GMO articles differ inadopting a more ?scientific?
discourse.
Inspired byearlier studies in social sciences relating hedging totexts aimed at professional scientists, we proposed16Average length of true positive sentences : 28.6, false pos-itive sentences 35.09, false negative sentences: 22.0.addressing the question with automatic hedge de-tection as a first step.
To develop a hedge clas-sifier, we took advantage of the CoNLL datasetand a small annotated WOS and LEXIS dataset.Our preliminary results show there may exist a gapwhich indicates that hedging may, in fact, distin-guish prof-science and pop-science documents.
Infact, this computational analysis suggests the possi-bility that hedges occur less frequently in scientificprose, which contradicts several prior assertions inthe literature.To confirm the argument that pop-science tendsto use more hedging than prof-science, we needa hedge classifier that performs more reliably inthe WOS and LEXIS dataset than ours does.
Aninteresting research direction would be to developtransfer-learning techniques to generalize hedgeclassifiers for different datasets, or to develop a gen-eral hedge classifier relatively robust to domain dif-ferences.
In either case, more annotated data onWOS and LEXIS is needed for better evaluation ortraining.Another strategy would be to bypass the first step,in which we determine whether hedges are moreor less prominent in scientific discourse, and pro-ceed directly to labeling and hedge-detection in pro-GMO and anti-GMO texts.
However, this will notanswer the question of whether advocates in debatesother than on GMO-related topics employ a morescientific discourse.
Nonetheless, to aid those whowish to pursue this alternate strategy, we have col-lected two sets of opinionated articles on GMO (pro-and anti-); see appendix for more details.Acknowledgments We thank Daniel Hopkins andBonnie Webber for reference suggestions, and theanonymous reviewers for helpful and thoughtfulcomments.
This paper is based upon work sup-ported in part by US NSF grants IIS-0910664 andIIS-1016099, a US NSF graduate fellowship to JS,Google, and Yahoo!ReferencesAmr Ahmed and Eric P Xing.
Staying informed: su-pervised and semi-supervised multi-view topicalanalysis of ideological perspective.
In EMNLP,pages 1140?1150, 2010.Charles Bazerman.
Shaping Written Knowledge:77The Genre and Activity of the Experimental Ar-ticle in Science.
University of Wisconsin Press,Madison, Wis., 1988.Beata Beigman Klebanov, Eyal Beigman, andDaniel Diermeier.
Vocabulary choice as an indi-cator of perspective.
In ACL Short Papers, pages253?257, Stroudsburg, PA, USA, 2010.
Associa-tion for Computational Linguistics.Robert D. Benford and David A.
Snow.
Framingprocesses and social movements: An overviewand assessment.
Annual Review of Sociology, 26:611?639, 2000.Richard Harvey Brown.
Toward a democratic sci-ence: Scientific narration and civic communica-tion.
Yale University Press, New Haven, 1998.Dennis Chong and James N. Druckman.
Framingtheory.
Annual Review of Political Science, 10:103?126, 2007.Jeanne Fahnestock.
Accommodating Science.
Writ-ten Communication, 3(3):275?296, 1986.Jeanne Fahnestock.
Preserving the figure: Consis-tency in the presentation of scientific arguments.Written Communication, 21(1):6?31, 2004.Rong-En Fan, Pai-Hsuen Chen, and Chih-Jen Lin.Working set selection using second order in-formation for training support vector machines.JMLR, 6:1889?1918, December 2005.
ISSN1532-4435.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra,Ja?nos Csirik, and Gyo?rgy Szarvas.
The CoNLL-2010 shared task: Learning to detect hedges andtheir scope in natural language text.
In CoNLL?Shared Task, pages 1?12, 2010.Joseph L. Fleiss.
Statistical Methods for Ratesand Proportions.
Wiley series in probability andmathematical statistics.
John Wiley & Sons, NewYork, second edition, 1981.Maria Georgescul.
A hedgehop over a max-marginframework using hedge cues.
In CONLL?Shared-Task, pages 26?31, 2010.G.
Nigel Gilbert and Michael Joseph Mulkay.
Open-ing Pandora?s box: A sociological analysis of sci-entists?
discourse.
CUP Archive, 1984.Erving Goffman.
Frame analysis: An essay on theorganization of experience.
Harvard UniversityPress, 1974.Alan G. Gross.
The rhetoric of science.
HarvardUniversity Press, Cambridge, Mass., 1990.Michael Alexander Kirkwood Halliday andJames R. Martin.
Writing science: Literacy anddiscursive power.
Psychology Press, London[u.a.
], 1993.Eric A Hardisty, Jordan Boyd-Graber, and PhilipResnik.
Modeling perspective using adaptorgrammars.
In EMNLP, pages 284?292, 2010.Ken Hyland.
Hedging in scientific research articles.John Benjamins Pub.
Co., Amsterdam; Philadel-phia, 1998.Napoleon K. Juanillo, Jr. Frames for Public Dis-course on Biotechnology.
In Genetically ModifiedFood and the Consumer: Proceedings of the 13thmeeting of the National Agricultural Biotechnol-ogy Council, pages 39?50, 2001.J.
Peter Kincaid, Robert P. Fishburne, Richard L.Rogers, and Brad S. Chissom.
Derivation of newreadability formulas for navy enlisted personnel.Technical report, National Technical InformationService, Springfield, Virginia, February 1975.George Lakoff.
Hedges: A study in meaning cri-teria and the logic of fuzzy concepts.
Journal ofPhilosophical Logic, 2(4):458?508, 1973.Bruno Latour.
Science in action: How to follow sci-entists and engineers through society.
HarvardUniversity Press, Cambridge, Mass., 1987.Bruno Latour and Steve Woolgar.
Laboratory life:The social construction of scientific facts.
SagePublications, Beverly Hills, 1979.Beverly A. Lewin.
Hedging: Form and functionin scientific research texts.
In Genre Studies inEnglish for Academic Purposes, volume 9, pages89?108.
Universitat Jaume I, 1998.Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, andAlexander Hauptmann.
Which side are you on?identifying perspectives at the document and sen-tence levels.
In CoNLL, 2006.Greg Myers.
The pragmatics of politeness in sci-entific articles.
Applied Linguistics, 10(1):1?35,1989.78Greg Myers.
Discourse studies of scientific popular-ization: Questioning the boundaries.
DiscourseStudies, 5(2):265?279, 2003.Franc?oise Salager-Meyer.
Scientific discourse andcontrastive linguistics: hedging.
European Sci-ence Editing, 37(2):35?37, 2011.Dietram A. Scheufele.
Framing as a theory of mediaeffects.
Journal of Communication, 49(1):103?122, 1999.Vladimir N. Vapnik.
Statistical Learning Theory.Wiley-Interscience, 1998.Teppo Varttala.
Remarks on the communicativefunctions of hedging in popular scientific and spe-cialist research articles on medicine.
English forSpecific Purposes, 18(2):177?200, 1999.Teppo Varttala.
Hedging in scientifically orienteddiscourse: Exploring variation according to dis-cipline and intended audience.
PhD thesis, Uni-versity of Tampere, 2001.7 Appendix: pro- vs. anti-GMO datasetHere, we describe the pro- vs. anti-GMO dataset wecollected, in the hopes that this dataset may provehelpful in future research regarding the GMO de-bates, even though we did not use the corpus in theproject described in this paper.The second step of our overall procedure out-lined in the introduction ?
that step being to ex-amine whether the use of hedging in pro-GMO arti-cles corresponds with our inferred ?scientific?
oc-currence patterns more than that in anti-GMO ar-ticles ?
requires a collection of opinionated arti-cles on GMOs.
Our first attempt to use news me-dia articles (LEXIS) was unsatisfying, as we foundmany articles attempt to maintain a neutral position.This led us to collect documents from more stronglyopinionated organizational websites such as Green-peace (anti-GMO), Non GMO Project (anti-GMO),or Why Biotechnology (pro-GMO).
Articles werecollected from 20 pro-GMO and 20 anti-GMO or-ganizational web sites.After the initial collection of data, near-duplicatesand irrelevant articles were filtered through cluster-ing, keyword searches and distance between wordvectors at the document level.
We have collected762 ?anti?
documents and 671 ?pro?
documents.We reduced this to a 404 ?pro?
and 404 ?con?set as follows.
Each retained ?document?
con-sists of only the first 200 words after excluding thefirst 50 words of documents containing over 280words.
This was done to avoid irrelevant sectionssuch as Educators have permission to reprint arti-cles for classroom use; other users, please contacteditor@actionbioscience.org for reprint permission.See reprint policy.The data will be posted online athttps://confluence.cornell.edu/display/llresearch/HedgingFramingGMOs.79
