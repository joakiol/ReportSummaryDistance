Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1614?1623,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsClustering Aspect-related Phrases by Leveraging Sentiment DistributionConsistencyLi Zhao, Minlie Huang, Haiqiang Chen*, Junjun Cheng*, Xiaoyan ZhuState Key Laboratory of Intelligent Technology and SystemsNational Laboratory for Information Science and TechnologyDept.
of Computer Science and Technology, Tsinghua University, Beijing, PR China*China Information Technology Security Evaluation Centerzhaoli19881113@126.com aihuang@tsinghua.edu.cnAbstractClustering aspect-related phrases in termsof product?s property is a precursor pro-cess to aspect-level sentiment analysiswhich is a central task in sentiment analy-sis.
Most of existing methods for address-ing this problem are context-based modelswhich assume that domain synonymousphrases share similar co-occurrence con-texts.
In this paper, we explore a novelidea, sentiment distribution consistency,which states that different phrases (e.g.
?price?, ?money?, ?worth?, and ?cost?)
ofthe same aspect tend to have consistentsentiment distribution.
Through formal-izing sentiment distribution consistency assoft constraint, we propose a novel unsu-pervised model in the framework of Poste-rior Regularization (PR) to cluster aspect-related phrases.
Experiments demonstratethat our approach outperforms baselinesremarkably.1 IntroductionAspect-level sentiment analysis has become a cen-tral task in sentiment analysis because it can ag-gregate various opinions according to a product?sproperties, and provide much detailed, complete,and in-depth summaries of a large number of re-views.
Aspect finding and clustering, a precursorprocess of aspect-level sentiment analysis, has at-tracted more and more attentions (Mukherjee andLiu, 2012; Chen et al., 2013; Zhai et al., 2011a;Zhai et al., 2010).Aspect finding and clustering has never been atrivial task.
People often use different words orphrases to refer to the same product property (alsocalled product aspect or feature in the literature).Some terms are lexically dissimilar while seman-tically close, which makes the task more challeng-ing.
For example, ?price?, ?money?
, ?worth?
and?cost?
all refer to the aspect ?price?
in reviews.In order to present aspect-specific summaries ofopinions, we first of all, have to cluster differentaspect-related phrases.
It is expensive and time-consuming to manually group hundreds of aspect-related phrases.
In this paper, we assume that theaspect phrases have been extracted in advance andwe keep focused on clustering domain synony-mous aspect-related phrases.Existing studies addressing this problem aremainly based on the assumption that differentphrases of the same aspect should have similar co-occurrence contexts.
In addition to the traditionalassumption, we develop a new angle to address theproblem, which is based on sentiment distributionconsistency assumption that different phrases ofthe same aspect should have consistent sentimentdistribution, which will be detailed soon later.Figure 1: A semi-structured Review.This new angle is inspired by this simple obser-vation (as illustrated in Fig.
1): two phrases withinthe same cluster are not likely to be simultaneouslyplaced in Pros and Cons of the same review.
Astraightforward way to use this information is toformulate cannot-link knowledge in clustering al-gorithms (Chen et al., 2013; Zhai et al., 2011b).However, we have a particularly different mannerto leverage the knowledge.Due to the availability of large-scale semi-structured customer reviews (as exemplified inFig.
1) that are supported by many web sites,we can easily get the estimation of sentiment dis-tribution for each aspect phrase by simply count-ing how many times a phrase appears in Pros and1614Cons respectively.
As illustrated in Fig.
2, wecan see that the estimated sentiment distributionof a phrase is close to that of its aspect.
Theabove observation suggests the sentiment distri-bution consistency assumption: different phrasesof the same aspect tend to have the same senti-ment distribution, or to have statistically closedistributions.
This assumption is also verified byour data: for most (above 91.3%) phrase with rela-tively reliable estimation (whose occurrence?50),the KL-divergence between the sentiment distri-bution of a phrase and that of its correspondingaspect is less than 0.05.Figure 2: The sentiment distribution of aspect?battery?
and its related-phrases on nokia 5130with a large amount of reviews.It is worth noting that, the sentiment distributionof a phrase can be estimated accurately only whenwe obtain a sufficient number of reviews.
Whenthe number of reviews is limited, however, the es-timated sentiment distribution for each phrase isunreliable (as shown in Fig.
3).
A key issue,arisen here, is how to formulate this assumption ina statistically robust manner.
The proposed modelshould be robust when only a limited number ofreviews are available.Figure 3: The sentiment distribution of aspect?battery?
and its related-phrases on nokia 3110cwith a small mumber of reviews.To deal with this issue, we model sentiment dis-tribution consistency as soft constraint, integratedinto a probabilistic model that maximizes the datalikelihood.
We design the constraint to work inthe following way: when we have sufficient ob-servations, the constraint becomes tighter, whichplays a more important role in the learning pro-cess; when we have limited observations, the con-straint becomes very loose so that it will have lesseffect on the model.In this paper, we propose a novel unsupervisedmodel, Sentiment Distribution Consistency Reg-ularized Multinomial Naive Bayes (SDC-MNB).The context part is modeled by Multinomial NaiveBayes in which aspect is treated as latent variable,and Sentiment distribution consistency is encodedas soft constraint within the framework of Poste-rior Regularization (PR) (Graca et al., 2008).
Themain contributions of this paper are summarizedas follows:?
We study the problem of clustering phrasesby integrating both context informationand sentiment distribution of aspect-relatedphrases.?
We explore a novel concept, sentiment distri-bution consistency(SDC), and model it as softconstraint to guide the clustering process.?
Experiments show that our model outper-forms the state-of-art approaches for aspectclustering.The rest of this paper is organized as follows.We introduce the SDC-MNB model in Section 2.We present experiment results in Section 3.
InSection 4, we survey related work.
We summarizethe work in Section 5.2 Sentiment Distribution ConsistencyRegularized Multinomial Naive BayesIn this section, we firstly introduce our assumptionsentiment distribution consistency formally andshow how to model the above assumption as softconstraint , which we term SDC-constraint.
Sec-ondly, we show how to combine SDC-constraintwith the probabilistic context model.
Finally, wepresent the details for context and sentiment ex-traction.2.1 Sentiment Distribution ConsistencyWe define aspect as a set of phrases that refer tothe same property of a product and each phrase istermed aspect-related phrase (or aspect phrase inshort).
For example, the aspect ?battery?
containsaspect phrases such as ?battery?, ?battery life?,?power?, and so on.1615F the aspect phrase setfjthe jthaspect phraseyjthe aspect for aspect phrase fjA the aspect setaithe ithaspectD the set of context documentsdjthe context document of fjV the word vocabularywtthe tthword in vocabulary Vwdj,kthe kthword in djNtjthe number of times word wtoccurs in djP the product setpkthe kthproductuikthe sentiment distribution parameterof aspect aion pks?jkthe estimated sentiment distribution parameterof phrase fjon pknjkthe occurrence times of aspect phrase fjon pk?
?jkthe sample standard deviation?
the model parametersp?
(ai|dj) the posterior distribution of aigiven djq(yj= ai)the projected posterior distributionof aigiven djTable 1: NotationsLet us consider the sentiment distribution on acertain aspect ai.
In a large review dataset, as-pect aicould receive many comments from differ-ent reviewers.
For each comment, we assume thatpeople either praise or complain about the aspect.So each comment on the aspect can be seen as aBernoulli trial, where the aspect receives positivecomments with probability pai1.
We introduce arandom variable Xaito denote the sentiment onaspect ai, where Xai= 1 means that aspect aireceives positive comments, Xai= 0 means thataspect aireceives negative comments.
Obviously,the sentiment on aspect aifollows the Bernoullidistribution,Pr(Xai) = pXaiai?
(1 ?
pai)1?Xai, Xai?
{0, 1}.
(1)Or in short,Xai?
Bernoulli(pai)Let us see the case for aspect phrase fj, wherefj?
aspect ai.
Similarly, each comment on an as-pect phrase fjcan also be seen as a Bernoulli trial.We introduce a random variable Xfjto denote thesentiment on aspect phrase fj, where Xfj= 1means that aspect fjreceives positive comments,Xfj= 0 means that aspect fjreceives negativecomments.
As just discussed, we assume that eachaspect phrase follows the same distribution with1positive comment means that an aspect term is observedin Pros of a review.the corresponding aspect.
This leads to the fol-lowing formal description:?
Sentiment Distribution Consistency : Thesentiment distribution of aspect phrase is thesame as that of the corresponding aspect.Formally, for all aspect phrase fj?
aspectai, Xfj?
Bernoulli(pai).2.2 Sentiment Distribution ConsistencyConstraintAssuming the sentiment distribution of aspect aiisgiven in advance, we need to judge whether an as-pect phrase fjbelongs to the aspect aiwith limitedobservations for fj.
Let?s consider the example inFig.
4.
For aspect phrase 3, we have no definiteanswer due to the limited number of observations.For aspect phrase 1, it seems that the sentimentdistribution is consistent with that of the left as-pect.
However, we can not say that the phrase be-longs to the aspect because the distribution maybe the same for two different aspects.
For aspectphrase 2, we are confident that its sentiment dis-tribution is different from that of the left aspect,given sufficient observations.Figure 4: Sentiment distribution of an aspect, andobservations on aspect phrases.To be concise, we judge an aspect phrasedoesn?t belong to certain aspect only when we areconfident that they follow different sentiment dis-tributions.Inspired by the intuition, we conduct intervalparameter estimation for parameter pfj(sentimentdistribution for phrase fj) with limited observa-tions, and thus get a confidence interval for pfj.If pai(sentiment distribution for aspect ai) is notin the confidence interval of pfj, we then are con-fident that they follow different distributions.
Inother words, if aspect phrase fj?
aspect ai, weare confident that paiis in the confidence intervalof pfj.More formally, we use uikto denote the senti-ment distribution parameter of aspect aion prod-uct pk, and assume that uikis given in advance.1616We want to know whether the sentiment distribu-tion on aspect phrase fjis the same as that of as-pect aion product pkgiven a limited number ofobservations (samples).
It?s straightforward to cal-culate the confidence interval for parameter sjkinthe Bernoulli distribution function.
Let the sam-ple mean of njksamples be s?jk, and the samplestandard deviation be ??jk.
Since the sample sizeis small here, we use the Student-t distribution tocalculate the confidence interval.
According to ourassumption, we are confident that uikis in the con-fidence interval if fj?
ai.s?jk?C??jk?njk?
uik?
s?jk+C?
?jk?njk, ?fj?
ai,?k.
(2)where we look for t-table to find C correspondingto a certain confidence level(such as 95%) with thefreedom of njk?
1.
For simplicity, we representthe above confidence interval by [s?jk?
djk, s?jk+djk], where djk= C?
?jk?njk.We introduce an indicator variable zijto repre-sent whether the aspect phrase fjbelongs to aspectai, as follows:zji={1 ; if fj?
ai0 ; otherwise(3)This leads to our SDC-constraint function.?
= zji|uik?
s?jk| ?
djk,?i, j, k (4)SDC-constraint are flexible for modeling Senti-ment Distribution Consistency.
The more obser-vations we have, the smaller djkis.
For frequentaspect phrase, the constraint can be very informa-tive because it can filter unrelated aspects for as-pect phrase fj.
The less observations we have,the larger djkis.
For rare aspect phrases, the con-straint can be very loose, and will not have mucheffect on the clustering process for aspect phrasefj.
In this way, the model can work very robustly.SDC-constraints are data-driven constraints.Usually we have many reviews about hundreds ofproducts in our dataset.
For each aspect phrase,there are |A| ?
|P | constraints (the number of as-pects times the number of product).
With thou-sands of constraints about which aspect it is notlikely to belong to, the model learns to which as-pect a phrase fjshould be assigned.
Althoughmost constraints may be loose because of the lim-ited observations, SDC-constraint can still play animportant role in the learning process.2.3 Sentiment Distribution ConsistencyRegularized Multinomial Naive Bayes(SDC-MNB)In this section, we present our probabilistic modelwhich employs both context information and sen-timent distribution.First of all, we extract a context document dfor each aspect phrase, which will be described inSection 2.5.
In other word, a phrase is representedby its context document.
Assuming that the doc-uments in D are independent and identically dis-tributed, the probability of generating D is thengiven by:p?
(D) =|D|?j=1p?
(dj) =|D|?j=1?yj?Ap?
(dj, yj) (5)where yjis a latent variable indicating the aspectlabel for aspect phrase fj, and ?
is the model pa-rameter.In our problem, we are actually more inter-ested in the posterior distribution over aspect,i.e., p?(yj|dj).
Once the learned parameter ?
isobtained, we can get our clustering result fromp?
(yj|dj), by assigning aspect aiwith the largestposterior to phrase fj.
We can also enforce SDC-constraint in expectation(on posterior p?).
We useq(Y ) to denote the valid posterior distribution thatsatisfy our SDC-constraint, and Q to denote thevalid posterior distribution space, as follows:Q = {q(Y ) : Eq[zji|uik?
s?jk|] ?
djk, ?i, j, k}.
(6)Since posterior plays such an important role injoining the context model and SDC-constraint, weformulate our problem in the framework of Poste-rior Regularization (PR).
PR is an efficient frame-work to inject constraints on the posteriors of la-tent variables.
Instead of restricting p?directly,which might not be feasible, PR penalizes the dis-tance of p?to the constraint set Q.
The posterior-regularized objective is termed as follows:max?
{log p?
(D) ?
minq?QKL(q(Y )||p?
(Y |D))} (7)By trading off the data likelihood of the ob-served context documents (as defined in the firstterm), and the KL divergence of the posteriorsto the valid posterior subspace defined by SDC-constraint (as defined in the second term), the ob-jective encourages models with both desired pos-terior distribution and data likelihood.
In essence,the model attempts to maximize data likelihood ofcontext subject (softly) to SDC-constraint.16172.3.1 Multinomial Naive BayesIn spirit to (Zhai et al., 2011a), we use Multino-mial Naive Bayes (MNB) to model the contextdocument.
Let wdj,kdenotes the kthword in doc-ument dj, where each word is from the vocabularyV = {w1, w2, ..., w|V |}.
For each aspect phrasefj, the probability of its latent aspect being aiandgenerating context document diisp?
(dj, yj= ai) = p(ai)|dj|?k=1p(wdj,k|ai) (8)where p(ai) and p(wdj,k|ai) are parameters of thismodel.
Each word wdj,kis conditionally indepen-dent of all other words given the aspect ai.Although MNB has been used in existing workfor aspect clustering, all of the studies used it ina semi-supervised manner, with labeled data orpseudo-labeled data.
In contrast, MNB proposedhere is used in an unsupervised manner for aspect-related phrases clustering.2.3.2 SDC-constraintAs mentioned above, the constraint posterior setQis defined byQ = {q(Y ) : q(yj= ai)|uik?
s?jk| ?
djk,?i, j, k}.
(9)We can see that Q denotes a set of linear con-straints on the projected posterior distribution q.Note that we do not directly observe uik, the sen-timent distribution of aspect aion product pk.
Foraspect phrase fjthat belongs to aspect ai, we es-timate uikby counting all sentiment samples.
Weuse the posterior p?
(ai|dj) to approximately rep-resent how likely phrase fjbelongs to aspect ai.uik=1?|D|j=1njkp?(ai|dj)|D|?j=1njkp?
(ai|dj)s?jk(10)where p?
(ai|dj) is short for p?
(yj= ai|dj), theprobability that aspect phrase fjbelongs to aigiven the context document dj.
We estimate uikinthis way because observations for aspect are rela-tively sufficient for a reliable estimation since ob-servations for an aspect are aggregated from thosefor all phrases belonging to that aspect.2.4 The Optimization AlgorithmThe optimization algorithm for the objective (seeEq.
7) is an EM-like two-stage iterative algorithm.In E-step, we first calculate the posterior distri-bution p?
(ai|dj), then project it onto the valid pos-terior distribution space Q.
Given the parameters?, the posterior distribution can be calculated byEq.
11.p?
(ai|dj) =p(ai)?|dj|k=1p(wdj,k|ai)?|A|r=1p(ar)?|dj|k=1p(wdj,k|ar)(11)We use the above posterior distribution to updatethe sentiment parameter for each aspect by Eq.
10.The projected posterior distribution q is calculatedbyq = argminq?QKL(q(Y )||p?
(Y |D)) (12)For each instance, there are |A| ?
|P | constraints.However, we can prune a large number of uselessconstraints derived from limited observations.
Allconstraints with djk> 1 can be pruned, due tothe fact that the parameter uik, s?jkis within [0,1],and the difference can not be larger than 1.
Thisoptimization problem in Eq.
12 is easily solved viathe dual form by the projected gradient algorithm(Boyd and Vandenberghe, 2004):max?
?0(?|A|?i=1|P |?k=1?ikdjk?log|A|?i=1p?
(ai|dj)exp{?|P |?k=1?ik|uik?
s?jk|} ?
????
)(13)where ?
controls the slack size for constraint.
Aftersolving the above optimization problem and ob-taining the optimal ?, we can calculate the pro-jected posterior distribution q byq(yj= ai) =1Zp?
(ai|dj)exp{?|P |?k=1?ik|uik?s?jk|} (14)where Z is the normalization factor.
Note that sen-timent distribution consistency is actually modeledas instance-level constraint here, which makes itvery efficient to solve.In M-step, the projected posteriors q(Y ) arethen used to compute sufficient statistics and up-date the models parameters ?.
Given the projectedposteriors q(Y ), the parameters can be updated byEq.
15,16.p(ai) =1 +?|D|j=1q(yj= ai)|A| + |D|(15)p(wt|ai) =1 +?|D|j=1Ntiq(yj= ai)|V | +?|V |m=1?|D|j=1Nmjq(yj= ai)(16)where Ntjis the number of times that the word wtoccurs in document dj.The parameters are initialized randomly, and werepeat E-step and M-step until convergence.16182.5 Data Extraction2.5.1 Context ExtractionIn order to extract the context document d for eachaspect phrase, we follow the approach in Zhai etal.
(2011a).
For each aspect phrase, we generateits context document by aggregating the surround-ing texts of the phrase in all reviews.
The preced-ing and following t words of a phrase are taken asthe context where we set t = 3 in this paper.
Stop-words and other aspect phrases are removed.
Forexample, the following review contains two aspectphrases, ?screen?
and ?picture?,The LCD screen gives clear picture.For ?screen?, the surrounding texts are {the,LCD, gives, clear, picture}.
We remove stop-words ?the?, and the aspect term ?picture?, andthe resultant context of ?screen?
in this review iscontext(screen) ={LCD, screen, gives, clear}.Similarly, the context of ?picture?
in this review iscontext(picture) ={gives, clear}.By aggregating the contexts of all the reviewsthat contain aspect phrase fj, we obtain the cor-responding context document dj.2.5.2 Sentiment ExtractionSince we use semi-structured reviews, we ob-tain the estimated sentiment distribution by sim-ply counting how many times each aspect phraseappears in Pros and Cons reviews for each prod-uct respectively.
So for each aspect phrase fj, letn+jkdenotes the times that fjappears in Pros ofall reviews for product pk, and let n?jkdenotes thetimes that fjappears in Cons of all reviews forproduct pk.
So the total number of occurrence of aphrase is njk= n+jk+ n?jk.
We have samples like(1,1,1,0,0) where 1 means a phrase occurs in Prosof a review, and 0 in Cons.
Given a sequence ofsuch observations, the sample mean is easily com-puted as s?jk=n+jkn+jk+n?jk.
And the sample standarddeviation is ??jk=?
(1?s?jk)2?n+jk+(s?jk)2?n?jknjk?1.3 Experiments3.1 Data PreparationThe details of our review corpus are givenin Table 2.
This corpus contains semi-structured customer reviews from four do-mains: Camera, Cellphone, Laptop, and MP3.These reviews were crawled from the followingweb sites: www.amazon.cn, www.360buy.com,www.newegg.com.cn, and www.zol.com.
The as-pect label of each aspect phrases is annotated byhuman curators.Camera Cellphone Laptop MP3#Products 449 694 702 329#Reviews 101,235 579,402 102,439 129,471#Aspect Phrases 236 230 238 166#Aspect 12 10 14 8Table 2: Statistics of the review corpus.
# denotesthe size.3.2 Evaluation MeasuresWe adapt three measures Purity, Entropy, andRand Index for performance evaluation.
Thesemeasures have been commonly used to evaluateclustering algorithms.Given a data set DS, suppose its gold-standardpartition is G = {g1, ..., gj, ..., gk}, where kis the number of clusters.
A clustering algo-rithm partitions DS into k disjoint subsets, sayDS1, DS2, ..., DSk.Entropy: For each resulting cluster, we can mea-sure its entropy using Eq.
17, where Pi(gj) is theproportion of data points of class gjin DSi.
Theentropy of the entire clustering result is calculatedby Eq.
18.entropy(DSi) = ?k?j=1Pi(gj)log2Pi(gj) (17)entropy(DS) =k?i=1|DSi||DS|entropy(DSi) (18)Purity: Purity measures the extent that a clustercontains only data from one gold-standard parti-tion.
The cluster purity is computed with Eq.
19.The total purity of the whole clustering result (allclusters) is computed with Eq.
20.purity(DSi) = maxjPi(gj) (19)purity(DS) =k?i=1|DSi||DS|purity(DSi) (20)RI: The Rand Index(RI) penalizes both false posi-tive and false negative decisions during clustering.Let TP (True Positive) denotes the number of pairsof elements that are in the same set in DS and inthe same set in G. TN (True Negative) denotesnumber of pairs of elements that are in differentsets in DS and in different sets in G. FP (False1619Camera Cellphone Laptop MP3P RI E P RI E P RI E P RI EKmeans 43.48% 83.52% 2.098 48.91% 84.80% 1.792 43.46% 87.11% 2.211 40.00% 70.98% 2.047L-EM 54.89% 87.07% 1.690 51.96% 86.64% 1.456 48.94% 84.53% 2.039 44.24% 75.37% 1.990LDA 36.84% 83.28% 2.426 48.65% 85.33% 1.833 35.02% 83.53% 2.660 36.12% 76.08% 2.296Constraint-LDA 43.30% 86.01% 2.216 47.89% 86.04% 1.974 32.35% 84.86% 2.676 50.70% 81.42% 1.924SDC-MNB 56.42% 88.16% 1.725 67.95% 90.62% 1.266 55.52% 90.72% 1.780 58.06% 83.57% 1.578Table 3: Comparison to unsupervised baselines.
(P is short for purity, E for entropy, and RI for randomindex.
)Positive) denotes number of pairs of elements inS that are in the same set in DS and in differentsets in G. FN (False Negative) denotes number ofpairs of elements that are in different sets in DSand in the same set in G. The Rand Index(RI) iscomputed with Eq.
21.RI(DS) =TP + TNTP + TN + FP + FN(21)3.3 Evaluation Results3.3.1 Comparison to unsupervised baselinesWe compared our approach with several existingunsupervised methods.
Some of the methods aug-mented unsupervised models by incorporating lex-ical similarity and other domain knowledge.
Allof them are context-based models.2We list thesemodels as follows.?
Kmeans: Kmeans is the most popular cluster-ing algorithm.
Here we use the context distri-butional similarity (cosine similarity) as thesimilarity measure.?
L-EM: This is a state-of-the-art unsupervisedmethod for clustering aspect phrases (Zhai etal., 2011a).
L-EM employed lexical knowl-edge to provide a better initialization for EM.?
LDA: LDA is a popular topic model(Blei etal., 2003).
Given a set of documents, it out-puts groups of terms of different topics.
Inour case, each aspect phrase is processed as aterm.3Each sentence in a review is consid-ered as a document.
Each aspect is consid-ered as a topic.
In LDA, a term may belongto more than one topic/group, but we take thetopic/group with the maximum probability.2In our method, we collect context document for eachaspect phrase.
This process is conducted for L-EM and K-means.
But for LDA and Constraint-LDA, we take each sen-tence of reviews as a document.
This setting for the LDAbaselines is adapted from previous work.3Each aspect phrase is pre-processed as a single word(e.g., ?battery life?
is treated as battery-life).
Other wordsare normally used in LDA.?
Constraint-LDA: Constraint-LDA (Zhai etal., 2011b) is a state-of-the-art LDA-basedmethod that incorporates must-link andcannot-link constraints for this task.
We setthe damping factor ?
= 0.3 and relaxationfactor ?
= 0.9, as suggested in the originalreference.For all methods that depend on the random ini-tiation, we use the average results of 10 runs as thefinal result.
For all LDA-based models, we choose?
= 50/T , ?
= 0.1, and run 1000 iterations.Experiment results are shown in Table 3.
Wecan see that our approach almost outperforms allunsupervised baseline methods by a large marginon all domains.
In addition, we have the followingobservations:?
LDA and Kmeans perform poorly due to thefact that the two methods do not use any priorknowledge.
It is also shown that only usingthe context distributional information is notsufficient for clustering aspect phrases.?
Constraint-LDA and L-EM that utilize priorknowledge perform better.
We can see thatConstraint-LDA outperforms LDA in termsof RI (Rand Index) on all domains.
L-EMachieves the best results against the baselines.This demonstrates the effectiveness to incor-porate prior knowledge.?
SDC-MNB produces the optimal resultsamong all models for clustering.
Methodsthat use must-links and cannot-links may suf-fer from noisy links.
For L-EM, we findthat it is sensitive to noisy must-links.
AsL-EM assumes that must-link is transitive,several noisy must-links may totally misla-bel the softly annotated data.
For Constraint-LDA, it is more robust than L-EM, becauseit doesn?t assume the transitivity of must-link.
However, it only promotes the RI (RandIndex) consistently by leveraging pair-wiseprior knowledge, but sometimes it hurts the1620performance with respect to purity or en-tropy.
Our method is consistently better onalmost all domains, which shows the advan-tages of the proposed model.?
SDC-MNB is remarkably better than base-lines, particularly for the cellphone domain.We argue that this is because we have thelargest number of reviews for each productin the cellphone domain.
The larger datasetgives us more observations on each phrase,so that we obtain more reliable estimation ofmodel parameters.3.3.2 Comparison to supervised baselinesWe further compare our methods with two super-vised models.
For each supervised model, weprovide a proportion of manually labeled data fortraining, which is randomly selected from gold-standard annotations.
However, we didn?t use anylabeled data for our approach.?
MNB: The labeled seeds are used to train aMNB classifier to classify all unlabeled as-pect phrases into different classes.?
L-Kmeans: In L-Kmeans, the clusters of thelabeled seeds are fixed at the initiation andremain unchanged during iteration.Purity RI EntropyMNB-5% 53.21% 85.77% 1.854MNB-10% 59.55% 86.70% 1.656MNB-15% 66.06% 88.39% 1.449L-Kmeans-10% 53.54% 86.15% 1.745L-Kmeans-15% 57.00% 86.89% 1.643L-Kmeans-20% 60.97% 87.63% 1.528SDC-MNB 59.49% 88.26% 1.580Table 4: Comparison to supervised baselines.MNB-5% means MNB with 5% labeled data.We experiment with several settings: taking5%, 10% and 15% of the manually labeled aspectphrases for training, and the remainder as unla-beled data.
Experiment results is shown in Table4 (the results are averaged over 4 domains).
Wecan see that our unsupervised approach is roughlyas good as the supervised MNB with 10% labeleddata.
Our unsupervised approach is also slightlybetter than L-Kmeans with 15% labeled data.
Thisresult further demonstrates the effectiveness of ourmodel.3.3.3 Influence of parametersWe vary the confidence level from 90% to 99.9%to see how it impacts on the performance of SDC-MNB.
The results are presented in Fig.
5 (the re-sults are averaged over 4 domains).
We can seethat the performance of clustering is fairly stablewhen changing the confidence level, which im-plies the robustness of our model.Figure 5: Influence of the confidence level onSDC-MNB.3.3.4 Analysis of SDC-constraintAs mentioned in Section 2.2, SDC-constraint isdependent on the number of observations.
Moreobservations we get, more informative the con-straint is, which means the constraint is tighter anddjk(see Eq.4) is smaller.
For all k, we count howmany djkis less than 0.2 (and 1) on average foreach aspect phrase fj.
djkis calculated with aconfidence level of 99%.
The statistics of con-straints is given in Table 5.
We can see that thecellphone domain has the most informative andlargest constraint set, that may explain why SDC-MNB achieves the largest purity gain(over L-EM)in cellphone domain.#(djk< 0.2) #(0.2 < djk< 1) purity gainCamera 3.02 8.78 1.53%Cellphone 17.29 30.5 15.99%Laptop 4.6 13.22 6.58%MP3MP4 6.1 10.7 13.82%Table 5: Constraint statistics on different domains.4 Related WorkOur work is related to two important researchtopics: aspect-level sentiment analysis, andconstraint-driven learning.
For aspect-level senti-ment analysis, aspect extraction and clustering arekey tasks.
For constraint-driven learning, a varietyof frameworks and models for sentiment analysishave been studied extensively.There have been many studies on clusteringaspect-related phrases.
Most existing studies are1621based on context information.
Some works alsoencoded lexical similarity and synonyms as priorknowledge.
Carenini et al.
(2005) proposed amethod that was based on several similarity met-rics involving string similarity, synonyms, and lex-ical distances defined with WordNet.
Guo et al.
(2009) proposed a multi-level latent semantic as-sociation model to capture expression-level andcontext-level topic structure.
Zhai et al.
(2010)proposed an EM-based semi-supervised learningmethod to group aspect expressions into user-specified aspects.
They employed lexical knowl-edge to provide a better initialization for EM.
InZhai et al.
(2011a), an EM-based unsupervisedversion was proposed.
The so-called L-EM modelfirst generated softly labeled data by grouping fea-ture expressions that share words in common, andthen merged the groups by lexical similarity.
Zhaiet al.
(2011b) proposed a LDA-based methodthat incorporates must-link and cannot-link con-straints.Another line of work aimed to extract and clus-ter aspect words simultaneously using topic mod-eling.
Titov and McDonald (2008) proposed themulti-grain topic models to discover global andlocal aspects.
Branavan et al.
(2008) proposeda method which first clustered the key-phrasesin Pros and Cons into some aspect categoriesbased on distributional similarity, then built a topicmodel modeling the topics or aspects.
Zhao et al.
(2010) proposed the MaxEnt-LDA (a MaximumEntropy and LDA combination) hybrid model tojointly discover both aspect words and aspect-specific opinion words, which can leverage syn-tactic features to separate aspects and sentimentwords.
Mukherjee and Liu (2012) proposed asemi-supervised topic model which used user-provided seeds to discover aspects.
Chen et al.
(2013) proposed a knowledge-based topic modelto incorporate must-link and cannot-link informa-tion.
Their model can adjust topic numbers auto-matically by leveraging cannot-link.Our work is also related to general constraint-driven(or knowledge-driven) learning models.Several general frameworks have been proposed tofully utilize various prior knowledge in learning.Constraint-driven learning (Chang et al., 2008)(CODL) is an EM-like algorithm that incorpo-rates per-instance constraints into semi-supervisedlearning.
Posterior regularization (Graca et al.,2007) (PR) is a modified EM algorithm in whichthe E-step is replaced by the projection of themodel posterior distribution onto the set of dis-tributions that satisfy auxiliary expectation con-straints.
Generalized expectation criteria (Drucket al., 2008) (GE) is a framework for incorporatingpreferences about model expectations into param-eter estimation objective functions.
Liang et al.
(2009) developed a Bayesian decision-theoreticframework to learn an exponential family modelusing general measurements on the unlabeled data.In this paper, we model our problem in the frame-work of posterior regularization.Many works promoted the performance of sen-timent analysis by incorporating prior knowledgeas weak supervision.
Li and Zhang (2009) in-jected lexical prior knowledge to non-negative ma-trix tri-factorization.
Shen and Li (2011) furtherextended the matrix factorization framework tomodel dual supervision from both document andword labels.
Vikas Sindhwani (2008) proposed ageneral framework for incorporating lexical infor-mation as well as unlabeled data within standardregularized least squares for sentiment predictiontasks.
Fang (2013)proposed a structural learningmodel with a handful set of aspect signature termsthat are encoded as weak supervision to extract la-tent sentiment explanations.5 ConclusionsAspect finding and clustering is an important taskfor aspect-level sentiment analysis.
In order tocluster aspect-related phrases, this paper has ex-plored a novel concept, sentiment distribution con-sistency.
We formalize the concept as soft con-straint, integrate the constraint with a context-based probabilistic model, and solve the problemin the posterior regularization framework.
Theproposed model is also designed to be robust withboth sufficient and insufficient observations.
Ex-periments show that our approach outperformsstate-of-the-art baselines consistently.AcknowledgmentsThis work was partly supported by the followinggrants from: the National Basic Research Program(973 Program) under grant No.2012CB316301and 2013CB329403, the National Science Foun-dation of China project under grant No.61332007and No.
61272227, and the Beijing Higher Educa-tion Young Elite Teacher Project.1622ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet allocation.
J. Mach.
Learn.Res., 3:993?1022, March.Stephen Boyd and Lieven Vandenberghe.
2004.
Con-vex Optimization.
Cambridge University Press, NewYork, NY, USA.S.
R. K. Branavan, Harr Chen, Jacob Eisenstein, andRegina Barzilay.
2008.
Learning document-levelsemantic properties from free-text annotations.
InProceedings of the Association for ComputationalLinguistics (ACL).Giuseppe Carenini, Raymond T. Ng, and Ed Zwart.2005.
Extracting knowledge from evaluative text.In Proceedings of the 3rd International Conferenceon Knowledge Capture, K-CAP ?05, pages 11?18,New York, NY, USA.
ACM.Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, andDan Roth.
2008.
Learning and inference withconstraints.
In Proceedings of the 23rd NationalConference on Artificial Intelligence - Volume 3,AAAI?08, pages 1513?1518.
AAAI Press.Zhiyuan Chen, Arjun Mukherjee, Bing Liu, MeichunHsu, Mal Castellanos, and Riddhiman Ghosh.
2013.Exploiting domain knowledge in aspect extraction.In EMNLP, pages 1655?1667.
ACL.Gregory Druck, Gideon Mann, and Andrew McCal-lum.
2008.
Learning from labeled features usinggeneralized expectation criteria.
In Proceedings ofthe 31st Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, SIGIR ?08, pages 595?602, New York,NY, USA.
ACM.Lei Fang, Minlie Huang, and Xiaoyan Zhu.
2013.
Ex-ploring weakly supervised latent sentiment expla-nations for aspect-level review analysis.
In Qi He,Arun Iyengar, Wolfgang Nejdl, Jian Pei, and RajeevRastogi, editors, CIKM, pages 1057?1066.
ACM.Joao V. Graca, Lf Inesc-id, Kuzman Ganchev, BenTaskar, Joo V. Graa, L F Inesc-id, Kuzman Ganchev,and Ben Taskar.
2007.
Expectation maximizationand posterior constraints.
In In Advances in NIPS,pages 569?576.Honglei Guo, Huijia Zhu, Zhili Guo, XiaoXun Zhang,and Zhong Su.
2009.
Product feature categorizationwith multilevel latent semantic association.
In Pro-ceedings of the 18th ACM Conference on Informa-tion and Knowledge Management, CIKM ?09, pages1087?1096, New York, NY, USA.
ACM.Tao Li, Yi Zhang, and Vikas Sindhwani.
2009.
A non-negative matrix tri-factorization approach to senti-ment classification with lexical prior knowledge.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP: Volume 1 - Volume 1, ACL ?09, pages244?252, Stroudsburg, PA, USA.
Association forComputational Linguistics.Percy Liang, Michael I. Jordan, and Dan Klein.
2009.Learning from measurements in exponential fami-lies.
In Proceedings of the 26th Annual Interna-tional Conference on Machine Learning, ICML ?09,pages 641?648, New York, NY, USA.
ACM.Arjun Mukherjee and Bing Liu.
2012.
Aspect extrac-tion through semi-supervised modeling.
In Proceed-ings of the 50th Annual Meeting of the Associationfor Computational Linguistics: Long Papers - Vol-ume 1, ACL ?12, pages 339?348, Stroudsburg, PA,USA.
Association for Computational Linguistics.Chao Shen and Tao Li.
2011.
A non-negative matrixfactorization based approach for active dual super-vision from document and word labels.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing, EMNLP ?11, pages 949?958, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Vikas Sindhwani and Prem Melville.
2008.Document-word co-regularization for semi-supervised sentiment analysis.
In ICDM, pages1025?1030.
IEEE Computer Society.Ivan Titov and Ryan McDonald.
2008.
Modeling on-line reviews with multi-grain topic models.
In Pro-ceedings of the 17th International Conference onWorld Wide Web, WWW ?08, pages 111?120, NewYork, NY, USA.
ACM.Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.
2010.Grouping product features using semi-supervisedlearning with soft-constraints.
In Proceedings ofthe 23rd International Conference on Computa-tional Linguistics, COLING ?10, pages 1272?1280,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.2011a.
Clustering product features for opinion min-ing.
In Proceedings of the Fourth ACM Interna-tional Conference on Web Search and Data Mining,WSDM ?11, pages 347?354, New York, NY, USA.ACM.Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia.2011b.
Constrained lda for grouping product fea-tures in opinion mining.
In Proceedings of the 15thPacific-Asia Conference on Advances in Knowl-edge Discovery and Data Mining - Volume PartI, PAKDD?11, pages 448?459, Berlin, Heidelberg.Springer-Verlag.Wayne X. Zhao, Jing Jiang, Hongfei Yan, and Xiaom-ing Li.
2010.
Jointly modeling aspects and opin-ions with a MaxEnt-LDA hybrid.
In Proceedings ofthe 2010 Conference on Empirical Methods in Nat-ural Language Processing, EMNLP ?10, pages 56?65, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.1623
