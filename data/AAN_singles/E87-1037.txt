A Comparison of Rule-Invocation Strategiesin Context-Free Chart ParsingMats  Wi rdnDepar tment  of Computer  and  Informat ion Sc ienceL inkSp ing  Un ivers i tyS-581 83 L inkSp ing ,  SwedenAbstractCurrently several grammatical formalisms convergetowards being declarative and towards utilizingcontext-free phrase-structure grammar as a back-bone, e.g.
LFG and PATR-II.
Typically the pro-cessing of these formalisms is organized within achart-parsing framework.
The declarative charac-ter of the formalisms makes it important to decideupon an overall optimal control strategy on the partof the processor.
In particular, this brings the rule-invocation strategy into critical focus: to gain max-imal processing efficiency, one has to determine thebest way of putting the rules to use.
The aim of thispaper is to provide a survey and a practical compari-son of fundamental rule-invocation strategies withincontext-free chart parsing.1 Backgroundand IntroductionAn apparent tendency in computational linguisticsduring the last few years has been towards declara-tive grammar formalisms.
This tendency has mani-fested itself with respect to linguistic tools, perhapsseen most clearly in the evolution from ATNs  withtheir strongly procedural grammars to PATR-II inits various incarnations (Shieber et al 1983, Kart-tunen 1986), and to logic-based formalisms such asDCG (Pereira and Warren 1980).
It has also man-ifested itself in linguistic theor/es, where there hasbeen a development from systems employing sequen-tial derivations in the analysis of sentence struc-tures to systems like LFG and GPSG which estab-lish relations among the elements of a sentence in anorder-independent and also direction-independentway.
For example, phenomena such as rule order-ing simply do not arise in these theories.This research has been supported by the National SwedishBoard for Technical Development.In addition, declarative formalisms are, in princi-ple, processor-independent.
Procedural formalisms,although possibly highly standardized (like Woods'ATN formalism), typically make references to an(abstract) machine.By virtue of this, it is possible for grammar writ-ers to concentrate on linguistic issues, leaving asidequestions of how to express their descriptions in away which provides for efficient execution by the pro-cessor at hand.Processing efficiency instead becomes an issue forthe designer of the processor, who has to find anoverall aoptimal~ control strategy for the processingof the grammar.
In particular (and also because ofthe potentially very large number of rules in realis-tic natural-language systems), this brings the rule-invocation strategy I into critical focus: to gain max-imal processing efficiency, one has to determine thebest way of putting the rules to use.
2This paper focuses on rule-invocation strategiesfrom the perspective of (context-free) chart parsing(Kay 1973, 1982; Kaplan 1973).Context-free phrase-structure grammar is of in-terest here in particular because it is utilized asthe backbone of many declarative formalisms.
Thechart-parsing framework is of interest in this connec-tion because, being a C'higher-order algorithm" (Kay1982:329), it lends itself easily to the processing ofdifferent grammatical formalisms.
At the same timeit is of course a natural test bed for experiments withvarious control strategies.Previously a number of comparisons of rule-invocation strategies in this or in similar settingshave been reported:ZThis term seems to have been coined by Thompson(1981).
Basically, it refers to the spectrum between top-downand bottom-up rocessing of the grammar rules.2The other principal control-strategy dimension, the search~g;/(depth-first vs. breadth-first), is irrelevant for the effi-ciency in chart parsing since it only affects the order in whichsuccessive (partial) analyses are developed.226Kay (1982) is the principal source, providing avery general exposition of the control strategies anddata structures involved in chart parsing.
In con-sidering the efficiency question, Kay favours a ~di-rected ~ bottom-up strategy (cf.
section 2.2.3}.Thompson (1981) is another fundamental source,though he discusses the effects of various rule-invocation strategies mainly from the perspective ofGPSG parsing which is not the main point here.Kilbury (1985) presents a left-corner strategy, ar-guing that with respect to natural-language gram-mars it will generally outperform the top-down(Earley-style) strategy.Wang (1985) discusses Kilbury's and Earley's al-gorithms, favouring the latter because of the ineffi-cient way in which bottom-up algorithms deal withrules with right common factors.
Neither Wang norKilbury considers .the natural approach to overcom-ing this problem, viz.
top-down filtering (of.
section2.2.3).As for empirical studies, Slocum (1981) is a richsource.
Among many other things, he provides omeperformance data regarding top-down filtering.Pratt (1975) reports on a successful augmentationof a bottom-up chart-like parser with a top-downfilter.Tomita (1985, 1986) introduces a very efficient,extended LR-parsing algorithm that can deal withfull context-free languages.
Based on empirical com-parisons, Tomita shows his algorithm to be superiorto Earley's algorithm and also to a modified ver-sion thereof (corresponding here to %elective top-downS; cf.
section 2.1.2).
Thus, with respect toraw efficiency, it seems clear that Tomita's algorithmis superior to comparable chart-parsing algorithms.However, a chart-parsing framework does have itsadvantages, particularly in its flexibility and open-endedness.The contribution this paper makes is:to survey fundamental strategies for rule-invocation within a context-free chart-parsingframework; in particularto specify ~directed ~ versions of Kilbury's strat-egy; and?
to provide a practical comparison of the strate-gies based on empirical results.2 A Survey ofRule- Invocat ion StrategiesThis section surveys the fundamental rule-invocationstrategies in context-flee chart parsing.
3 In a chart-parsing framework, different rule-invocation strate-gies correspond to different conditions for and waysof predicting new edges 4.
This section will thereforein effect constitute a survey of different methods forpredicting new edges.2.1 Top-Down StrategiesThe principle of top-down parsing is to use the rulesof the grammar to generate a sentence that matchesthe one being analyzed.2.1.1 Top-DownA strategy for top-down chart parsing 5 is given be-low.
Assume a context-free grammar G. Also, wemake the usual assumption that G is cycle-free, i.e.,it does not contain derivations of the form A1 --* A~,A2 "-+ Aa, .
.
.
,  Ai --* A1.St ra tegy  16 (TD)Whenever an active edge is added to the chart,if its first required constituent is C, then add anempty active C edge for every rule in G whichexpands C. 7This principle will apply to itself recursively, en-suring that all subsidiary active edges also get pro-duced.2.1.2 Select ive Top-DownRealistic natural-language rammars are likely to behighly branching.
A weak point of the ~normal =top-down strategy above will then be the excessivenumber of predictions typically made: in the begin-ning of a phrase new edges will be introduced forall constituents, and constituents within those con-stituents, that the phrase can possibly start with.One way of limiting the number of predictionsis by making the strategy %elective = (GriffithsaI assume a basic familiarity with chart parsing.
For anexcellent introduction, see Thompson and Ritchie (1984).4Edges correspond to "states ~ in Earley (1970) and toUitemsn in Aho and Ullman (1972:320).5Top-down (context-free) chart parsing is sometimes calledUEarley-style" chart parsing because it corresponds tothe wayin which Earley's algorithm (Earley 1970) works.
It shouldbe pointed out that the paree-forest representation employedhere does not suffer from the kind of defect claimed by Tomita(1985:762, 1986:74) to result from Earley's algorithm.6This formulation is equivalent to the one in Thompson(1981:4).7Note that in order to handle left-recursive rules withoutgoing into an infinite loop, this strategy needs a redundancycheck which prevents more than one identical active edge frombeing added to the chart.227and Petrick 1965:291): by looking at the cate-gory/categories of the next word, it is possible to ruleout some proposed edges that are known not to com-bine with the corresponding inactive edge(s).
Giventhat top-down chart parsing starts with a scanningphase, the adoption of this filter is straightforward.The strategy makes use of a reachability relationwhere A\]~B holds if there exists some derivationfrom A to B such that B is the first element in astring dominated by A.
Given preterminal look-ahead symbol(s) py corresponding to the next word,the processor can then ask if the first required con-stituent of a predicted active edge (say, C) can some-how start with (some) p~..
In practice, the relation isimplemented as a precompiled table.
Determining ifholds can then be made very fast and in constanttime.
(Cf.
Pratt 1975:424.
)The strategy presented here corresponds to Kay'sadirected top-down" strategy (Kay 1982:338) andcan be specified in the following manner.Strategy 2 {TD0)Let r(X} be the first required constituent of the(active) edge X.
Let u be the vertex to whichthe active edge about to be proposed extends.Let Pl,..., Pn be the preterminal categories ofthe edges extending from v that correspond tothe next word.
-- Whenever an active edge.
is added to the chart, if its first required con-stituent is C, then for every rule in G whichexpands C add an empty active C edge if forsome \] r(C) = pj or r(O)~pj.2 .2  Bot tom-Up St ra teg iesThe principle of bottom-up parsing is to reduce asequence of phrases whose types match the right-hand side of a grammar ule to a phrase of the typeof the left-hand side of the rule.
To make a reductionpossible, all the right-hand-side phrases have to bepresent.
This can be ensured by matching from rightto left in the right-hand side of the grammar ule;this is for example the case with the Cocke--Kasami-Younger algorithm (Aho and Ullman 1972).A problem with this approach is that the analy-sis of the first part of a phrase has no influence onthe analysis of the latter parts until the results fromthem are combined.
This problem can be met byadopting left-corner parsing.2.2.1 Left  CornerLeft-corner parsing is a bottom-up technique wherethe right-hand-side symbols of the rules are matchedfrom left to right, s Once the left-corner symbol hasbeen found, the grammar rule can be used to predictwhat may come next.A basic strategy for left-corner chart parsing isgiven below.S t ra tegy  3 g (LC)Whenever an inactive edge is added to thechart, if its category is T, then for every rule inG with T as left-corner symbol add an emptyactive edge.
1?Note that this strategy will make aminimal" pre-dictions, i.e., it will only predict he nezt higher-levelphrases which a given constituent can begin.2.2.2 Left  Corner  b la K i lburyKilbury (1985) presents a modified left-corner strat-egy.
Basically it amounts to this: instead of predict-hag empty active edges, edges which subsume theinactive edge that provoked the new edge are pre-dicted.
A predicted new edge may then be eitheractive or inactive depending on the contents of theinactive edge and on what is required by the newedge.This strategy has two clear advantages: First, itsaves many edges compared to the anormal" left cor-ner because it never produces empty active edges.Secondly (and not pointed out by Kilbury), the usualredundancy check is not needed here since the strat-egy itself avoids the risk of predicting more than oneidentical edge.
The reason for this is that a predictededge always subsumes the triggering (inactive) edge.Since the triggering edge is guaranteed to be unique,the subsuming edge will also be unique.
By virtueof this, Kilbury's prediction strategy is actually thesimplest of all the strategies considered here.The price one has to pay for this is that ruleswith empty-string productions (or e-productions, i.e.rules of the form A -* e), cannot be handled.
Thismight look like a serious limitation since most cur-rent linguistic theories (e.g., LFG, GPSG) make ex-plicit use of e-productions, typically for the handlingof gaps.
On the other hand, context-free gram-mars can be converted into grammars without e-productions (Aho and Ullman 1972:150).In practice however, e-productions can be han-dled in various ways which circumvent he prob-lem.
For example, Karttunen's D-PATR systemSThe left corner of a rule is the leftmost symbol of its right-hand side.
?This formulation isagain equivalent to the one in Thomp-son (1981:4).
Thompson however refers to it a8 "bottom-up".
*?In this case, left-recursive rules will not lead to infiniteloops.
The redundancy check is still needed to prevent super-fluotm analyses from being generated, though.228does not allow empty productions.
Instead, it takescare of fillers and gaps through a ~threading" tech-nique (Karttunen 1986:77).
Indeed, the system hasbeen successfully used for writing LFG-style gram-mars (e.g., Dyvik 1986).Kilbury's left-corner strategy can be specified inthe following manner.Strategy 4 (LCK)Whenever an inactive edge is added to thechart, if its category is T, then for every rulein G with T as left-corner symbol add an edgethat subsumes the T edge.2.2.3 Top-Down F i l ter ingAs often pointed out, bottom-up and left-cornerstrategies encounter problems with sets of rules likeA ~ BC and A --* C (right common factors).
Forexample, assuming standard grammar rules, whenparsing the phrase athe birds fly" an unwanted sen-tence ~birds fly" will be discovered.This problem can be met by adopting top-dowNj~tering, a technique which can be seen as thedual of the selective top-down strategy.
Descrip-tions of top-down filtering are given for example inKay (1982) (~directed bottom-up parsing") and inSlocum (1981:2).
Also, the aoracle" used by Pratt(1975:424) is a top-down filter.Essentially top-down filtering is like running a top-down parser in parallel with a bottom-up parser.The (simulated} top-down parser rejects some of theedges that the bottom-up parser proposes, vis.
thosethat the former would not discover.
The additionalquestion that the top-down filter asks is then: isthere any place in a higher-level structure for thephrase about to be built by the bottom-up parser?On the chart, this corresponds to asking if any (ac-tive) edge ending in the starting vertex of the pro-posed edge needs this this kind of edge, directly orindirectly.
The procedure for computing the answerto this again makes use of the reachability relation(cf.
section 2.1.2).
11Adding top-down filtering to the LC strategyabove produces the following strategy.Strategy 5 (Let)Let v be the vertex from which the triggeringedge T extends.
Let At, ..., Am be the ac-tive edges incident to v, and let r(A~) be theirl*Kilbury (1985:10) actually makes use of a similar rela-tion encoding the left-branchings of the grammar (the "first-relation"), but he uses it only for speeding up grammar-ruleaccess (by indexing rules from left corners) and not for thepurpose of filtering out unwanted edges.respective first required constituents.
- - When-ever an inactive edge is added to the chart, if itscategory is T, then for every rule C in G withT as left-corner symbol add an empty active Cedge if for some i r(A,) = C or r(A,)~C.Analogously, adding top-down filtering to Kil-bury's strategy LCK results in the following.S t ra tegy  6 (LCKt)(Same preconditions as above.)
- -  Wheneveran inactive edge is added to the chart, if itscategory is T, then for every rule C in G withT as left-corner symbol add a C edge subsumingthe T edge if for some i r(A,) = C or r(A~)~C.One of the advantages with chart parsing is direc-tion independence: the words of a sentence do nothave to be parsed strictly from left to right but canbe parsed in any order.
Although this is still possibleusing top-down filtering, processing becomes ome-what less straightforward (cf.
Kay 1982:352).
Thesimplest way of meeting this problem, and also thesolution adopted here, is to presuppose l ft-to-rightparsing.2.2.4 Select iv i tyBy again adopting a kind of lookahead and by uti-lizing the reachability relation )~, it is possible tolimit the number of edges built even further.
Thislookahead can be realized by performing a dictionarylookup of the words before actually building the cor-responding inactive edges, storing the results in atable.
Being analogous to the filter used in the di-rected top-down strategy, this filter makes ure thata predicted edge can somehow be extended given thecategory/categories of the next word.
Note that thisfilter only affects active predicted edges.Adding selectivity to Kilbury's strategy LCK re-sults in the following.S t ra tegy  7 (LCK,)Let p l , .
.
.
,  p,, be the categories of the word cor-responding to the preterminal edges extendingfrom the vertex to which the T edge is incident.Let r(C) be defined as above.
- -  Whenever aninactive edge is added to the chart, if its cate-gory is T, then for every rule C in G with T asleft-corner symbol add a C edge subsuming theT edge if for some \] r(C) = py or r(C)~py.2.2.5 Top-Down F i l ter ing and  Select ivityThe final step is to combine the two previous trate-gies to arrive at a maximally directed version of Kil-229bury's strategy.
Again, left-to-right processing ispresupposed.Strategy 8 (LCK,t)Let r(A,), r(C), and pj be defined analogouslyto the previous.
- -  Whenever an inactive dge isadded to the chart, if its category is T, then forevery rule C in G with T as left-corner symboladd a C edge subsuming the T edge if for some ir(A,) = C or r(A,)~C and for some i r(C) = pyor r(C)\]~pj.3 Empir ical  ResultsIn order to assess the practical behaviour of thestrategies discussed above, a test bench was devel-oped where it was made possible in effect to switchbetween eight different parsers corresponding to theeight strategies above, and also between differentgrammars, dictionaries, and sentence sets.Several experiments were conducted along theway.
The test grammars used were first partly basedon a Swedish D-PATR grammar by Merkel (1986).Later on, I decided to use (some of) the data com-piled by Tomita (1986) for the testings of his ex-tended LR parser.This section presents the results of the latter ex-periments.3 .1  Grammars  and  Sentence  SetsThe three grammars and two sentence sets used inthese experiments have been obtained from MasaruTomita and can be found in his book (Tomita 1986).Grammars I and II are toy grammars consistingof 8 and 43 rules, respectively.
Grammar III with224 rules is constructed to fit sentence set I which isa collection of 40 sentences collected from authentictexts.
(Grammar IV with 394 rules was not usedhere.
)Because grammar I l l  contains one empty produc-tion, not all sentences of sentence set I will be cor-rectly parsed by Kilbury's algorithm.
For the pur-pose of these experiments, I collected 21 sentencesout of the sentence set.
This reduced set will hence-forth be referred to as sentence set I.
12 The sen-tences in this set vary in length between 1 and 27words.Sentence set II was made systematically from theschemanoun verb det noun (prep det noun) "-z.12The sentences in the set are 1-3, 9, 13-15, 19-25, 29, and35-40 (cf.
Tomita 1986:152).An example of a sentence with this structure is ~Isaw the man in the park with a telescope.. . '
.
Inthese experiments n = 1, .
.
.
,  7 was used.The dictionary was constructed from the categorysequences given by Tomita together with the sen-tences (Tomita 1986 pp.
185-189).3 .2  E f f i c iency  MeasuresA reasonable fficiency measure in chart parsing isthe number of edges produced.
The motivation forthis is that the working of a chart parser is tightlycentered around the production and manipulationof edges, and that much of its work can somehowbe reduced to this.
For example, a measure of theamount of work done at each vertex by the procedurewhich implements ~the fundamental rule" (Thomp-son 1981:2) can be expressed as the product of thenumber of incoming active edges and the number ofoutgoing inactive edges.
In addition, the number ofchart edges produced is a measure which is indepen-dent of implementation and machine.On the other hand, the number of edges does notgive any indication of the overhead costs involved invarious strategies.
Hence I also provide figures ofthe parsing times, albeit with a warning for takingthem too seriously, zsThe experiments were run on Xerox 1186 Lisp ma-chines.
The time measures were obtained using theInterlisp-D function TIMEALL.
The time figures be-low give the CPU time in seconds (garbage-collectiontime and swapping time not included; the latter washowever almost non-existent).3 .3  Exper imentsThis section presents the results of the experiments.In the tables, the fourth column gives the accumu-lated number of edges over the sentence set.
The sec-ond and third columns give the corresponding num-bers of active and inactive edges, respectively.
Thefifth column gives the accumulated CPU time in sec-onds.
The last column gives the rank of the strate-gies with respect o the number of edges producedand, in parentheses, with respect o time consumed(ff differing from the former).Table 1 shows the results of the first experiment:running grammar I (8 rules) with sentence set II (7sentences).
There were 625 parses for every strategy(1, 2, 5, 14, 42, 132, and 429).iSThe parsers are experimental in character and were notcoded for maximal efficiency.
For example, edges at a givenvertex are being searched linearly.
On the other hand, gram-mar rules (llke reachability relations) are indexed through pre-compiled hashtables.230Experiment 1:Strategy ActiveTD 1628TD, 1579LC 3104LCt 1579LCK 2873LCK, 697LCKt 1460LCK.
527Table 1Grammar I, sentence set IIInactive Total Time Rank3496 5124 62 63496 5075 58 4 (5)3967 7071 79 83496 5075 57 43967 6840 64 73967 4664 47 2 (3)3496 4956 45 3 (2)3496 4023 40 1Table 2Experiment 2: Grammar II, sentence set IIStrategy Active Inactive Total Time RankTD 5015 2675 7690 121 6TDo 3258 2675 5933 78 4LC 7232 5547 12779 192 8LC?
3237 2675 5912 132 3 (7)LCK 6154 5547 11701 i17 7 (5)LCK.
1283 5547 6830 70 5 (2)LCKt 2719 2675 5394 74 2 (3)LCK,t 915 2675 3590 41 1Experiment 3:Strategy ActiveTD 13676TDo 9301LC 19522LCe 9301LCK 18227LCK, 1359LCK, 8748LCKe, 718Table $Grammar III, sentenceInactive52785278798052787980798052785278set IITotal Time Rank18954 910 6 (5)14579 765 427502 913 8 (6)14579 2604 4 (8}26207 731 7 (3)9339 482 214026 1587 3 (7)5996 352 1Table 4Experiment 4: Grammar III, sentence set IStrategy Active Inactive Total Time RankTD 30403 8376 38779 1524 6 (4)TD, 14389 8376 23215 1172 4 (2)LC 42959 19451 62410 2759 8 (6)LCt 14714 8376 23'090 5843 3 (8)LCK 38040 19451 57491 1961 7(5)LCKo 3845 19451 23296 1410 5 (3)LCKt 12856 8376 21232 3898 2 (7)LCKst 1265 8376 9641 1019 1Table 2 shows the results of the second experi-ment: grammar II with sentence set II.
This gram-mar handles PP attachment in a way different fromgrammars I and III which leads to fewer parses: 322for every strategy.Table 3 shows the results of the third experiment:grammar III (224 rules) with sentence set II.
Again,there were 625 parses for every strategy.Table 4 shows the results of the fourth experiment:running grammar III with sentence set I (21 sen-tences}.
There were 885 parses for every strategy.4 Discuss ionThis section summarizes and discusses the results ofthe experiments.As for the three undirected methods, and withrespect o the number of edges produced, the top-down (Earley-style) strategy performs best while thestandard left-corner strategy is the worst alternative.Kilbury's strategy, by saving active looping edges,produces omewhat fewer edges than the standardleft-corner strategy.
More apparent is its time ad-vantage, due to the basic simplicity of the strategy.For example, it outperforms the top-down strategyin experiments 2 and 3.Results like those above are of course stronglygrammar dependent.
If, for example, the branchingfactor of the grammar increases, top-down overpre-dictions will soon dominate superfluous bottom-upsubstring eneration.
This was clearly seen in someof the early experiments not showed here.
In caseslike this, bottom-up arsing becomes advantageousand, in particular, Kilbury's strategy will outper-form the two others.Thus, although Wang (1985:7) seems to be right inclaiming that ~... Earley's algorithm is better thanKilbury's in general.
", in practice this can often bedifferent (as Wang himself recognizes).
Incidentally,Wang's own example (:4), aimed at showing that Kil-bury's algorithm handles right recursion worse thanEarley's algorithm, illustrates this:Assume a grammar with rules S --* Ae, A --* aA,A -* b and a sentence aa a a a b c" to be parsed.Here a bottom-up arser such as Kilbury's will ob-viously do some useless work in predicting severalunwanted S edges.
But even so the top-down over-predictions will actually dominate: the Earley-stylestrategy gives 16 active and 12 inactive edges, to-tailing 28 edges, whereas Kilbury's strategy gives 9and 16, respectively, totalling 25 edges.The directed methods - -  those based on selectiv-ity or top-down filtering - -  reduce the number ofedges very significantly.
The selectivity filter here231turned out to be much more time efficient, though.Selectivity testing is also basically a simple opera-tion, seldom involving more than a few lookups (de-pending on the degree of lexical ambiguity).Paradoxically, the effect of top-down filtering wasto degrade time performance as the grammars grewlarger.
To a large extent this is likely to havebeen caused by implementation idiosyncrasies: ac-tive edges incident to a vertex were searched linearly;when the number of edges increases, this gets verycostly.
After all, top-down filtering is generally con-sidered beneficial (e.g.
Slocum 1981:4).The maximally directed strategy m Kilbury's al-gorithm with selectivity and top-down filteringremained the most efficient one throughout all theexperiments, both with respect to edges producedand time consumed (but more so with respect to theformer).
Top-down filtering did not degrade timeperformance quite as much in this case, presumablybecause of the great number of active edges cut offby the selectivity filter.Finally, it should be mentioned that bottom-upparsing enjoys a special advantage not shown here,namely in being able to detect ungrammatical sen-tences much more effectively than top-down meth-ods (cf.
Kay 1982:342).5 ConclusionThis paper has surveyed the fundamental rule-invocation strategies in context-free chart parsing.In order to arrive at some quantitative measureof their performance characteristics, the strategieshave been implemented and tested empirically.
Theexperiments clearly indicate that it is possible tosignificantly increase efficiency in chart parsing byfine-tuning the rule-invocation strategy.
Fine-tuninghowever also requires that the characteristics of thegrammars to be used are borne in mind.
Never-theless, the experiments indicate that in general di-rected methods are to be preferred to undirectedmethods; that top-down is the best undirected strat-egy; that Kilbury's original algorithm is not in itselfa very good candidate, but that its directed versions-- in particular the one with both selectivity andtop-down filtering -- are very promising.Future work along these lines is planned to involveapplication of (some of) the strategies above withina unification-based parsing system.AcknowledgementsI would like to thank Lars Ahrenberg, Nils Dahlb~k,Arne Jbnsson, Magnus Merkel, Ivan Rankin, and ananonymous referee for the very helpful commentsthey have made on various drafts of this paper.
Inaddition I am indebted to Masaru Tomita for pro-viding me with his test grammars and sentences, andto Martin Kay for comments in connection with mypresentation.ReferencesAho, Alfred V. and Jeffrey D. Ullman (1972).
TheTheory of Parsing, Translation, and Compiling.Volume I: Parsing.
Prentice-Hall, Englewood Cliffs,New Jersey.Dyvik, Helge (1986).
Aspects of Unification-BasedChart Parsing.
Ms. Department of Linguistics andPhonetics, University of Bergen, Bergen, Norway.Earley, Jay (1970).
An Efficient Context-FreeParsing Algorithm.
Communications of the ACM13(2):94--102.Griffiths, T. V. and Stanley R. Petrick (1965).On the Relative Efficiences of Context-Free Gram-mar Recognizers.
Communications of the ACM8(5):289-300.Kaplan, Ronald M. (1973).
A General SyntacticProcessor.
In: Randall Rustin, ed., Natural Lan-guage Processing.
Algorithmics Press, New York,New York: 193-241.Karttunen, Lauri (1986).
D-PATR: A Develop-ment Environment for Unification-Based Grammars.Proe.
11th COLING, Bonn, Federal Republic of Ger-many: 74-80.Kay, Martin (1973).
The MIND System.
In: Ran-dal\] Rustin, ed., Natural Language Processing.
AI-gorithmics Press, New York, New York: 155-188.Kay, Martin (1982).
Algorithm Schemata nd DataStructures in Syntactic Processing.
In: Sture All~n,ed., Tezt Processinf.
Proceedinqs of Nobel Sympo-sium 51.
Almqvist & Wiksell International, Stock-holm, Sweden: 327-358.
Also: CSL-80-12, XeroxPARC, Palo Alto, California.Kilbury, James (1985).
Chart Parsing and theEarley Algorithm.
KIT-Report 24, ProjektgruppeKfiustliche Intelligenz und Textverstehen, Techni-sche Universit~t Berlin, West Berlin.
Also in:U. Klenk, ed.
(1985), Konteztfreie Syntazen undverwandte Systeme.
Vortr~ge ine8 Kolloquiumsin Grand Ventron im Oktober, 1984.
Niemeyer,Tfibingen, Federal Republic of Germany.232Merkel, Magnus (1986).
A Swedish Grammar inD-PATR.
Experiences of Working with D-PATR.Research report LiTH-IDA-R-86-31, Department ofComputer and Information Science, LinkSping Uni-versity, LinkSping, Sweden.Pereira, Fernando C. N. and David H. D. Warren(1980).
Definite Clause Grammars for LanguageAnalysis--A Survey of the Formalism and a Com-parison with Augmented Transition Networks.
Ar-tificial Intelligence 13(3):231-278.Pratt, Vaughan R. (1975).
LINGOL - -  A ProgressReport.
Proc.
Sth IJCAI, Tbilisi, Georgia, USSR:422-428.Shieber, Stuart M., Hans Uszkoreit, Fernando C. N.Pereira, Jane J. Robinson, and Mabry Tyson (1983).The Formalism and Implementation f PATR-II.
In:Barbara Grosz and Mark Stickel, eds., Research onInteractive Acquisition and Use of Knowledge.
SRIFinal Report 1894, SRI International, Menlo Park,California.Slocum, Jonathan (1981).
A Practical Comparisonof Parsing Strategies.
Proc.
19th ACL, Stanford,California: 1-6.Thompson, Henry (1981).
Chart Parsing and RuleSchemata in GPSG.
Research Paper No.
165, De-partment of Artificial Intelligence, University of Ed-inburgh, Edinburgh, Scotland.
Also in: Proc.
19thACL, Stanford, California: 167-172.Thompson, Henry and Graeme Ritchie (1984).
Im-plementing Natural Language Parsers.
In: TimO'Shea and Marc Eisenstadt, Arh'ficial Intelligence:Tools, Techniques, and Applications.
Harper & Row,New York, New York: 245-300.Tomita, Masaru (1985).
An Efficient Context-freeParsing Algorithm For Natural Languages.
Proc.9th IJCAI, Los Angeles, California: 756=764.Tomita, Masaru (1986).
E~cient Parsing for Nat-ural Language.
A Fast Algorithm for Practical Sys-tems.
Kluwer Academic Publishers, NorweU, Mas-sachusetts.Wang, Weiguo (1985}.
Computational LinguisticsTechnical Notes No.
2.
Technical Report 85/013,Computer Science Department, Boston University,Boston, Massachusetts.233
