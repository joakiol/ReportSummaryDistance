Using LTAG Based Features in Parse Reranking?Libin ShenDept.
of Computer & Info.
Sci.University of Pennsylvanialibin@cis.upenn.eduAnoop SarkarSchool of Computing ScienceSimon Fraser Universityanoop@cs.sfu.caAravind K. JoshiDept.
of Computer & Info.
Sci.University of Pennsylvaniajoshi@cis.upenn.eduAbstractWe propose the use of Lexicalized TreeAdjoining Grammar (LTAG) as a sourceof features that are useful for rerankingthe output of a statistical parser.
In thispaper, we extend the notion of a tree ker-nel over arbitrary sub-trees of the parse tothe derivation trees and derived trees pro-vided by the LTAG formalism, and in ad-dition, we extend the original definitionof the tree kernel, making it more lexi-calized and more compact.
We use LTAGbased features for the parse reranking taskand obtain labeled recall and precision of89.7%/90.0% on WSJ section 23 of PennTreebank for sentences of length ?
100words.
Our results show that the useof LTAG based tree kernel gives rise toa 17% relative difference in f -score im-provement over the use of a linear kernelwithout LTAG based features.1 IntroductionRecent work in statistical parsing has explored al-ternatives to the use of (smoothed) maximum likeli-hood estimation for parameters of the model.
Thesealternatives are distribution-free (Collins, 2001),providing a discriminative method for resolvingparse ambiguity.
Discriminative methods provide aranking between multiple choices for the most plau-sible parse tree for a sentence, without assuming thata particular distribution or stochastic process gener-ated the alternative parses.
?We would like to thank Michael Collins for providing theoriginal n-best parsed data on which we ran our experimentsand the anonymous reviewers for their comments.
The sec-ond author is partially supported by NSERC, Canada (RGPIN:264905).Discriminative methods permit the use of featurefunctions that can be used to condition on arbitraryaspects of the input.
This flexibility makes it possi-ble to incorporate features of various of kinds.
Fea-tures can be defined on characters, words, part ofspeech (POS) tags and context-free grammar (CFG)rules, depending on the application to which themodel is applied.Features defined on n-grams from the input arethe most commonly used for NLP applications.Such n-grams can either be defined explicitly us-ing some linguistic insight into the problem, or themodel can be used to search the entire space of n-gram features using a kernel representation.
Oneexample is the use of a polynomial kernel over se-quences.
However, to use all possible n-gram fea-tures typically introduces too many noisy features,which can result in lower accuracy.
One way tosolve this problem is to use a kernel function that istailored for particular NLP applications, such as thetree kernel (Collins and Duffy, 2001) for statisticalparsing.In addition to n-gram features, more complexhigh-level features are often exploited to obtainhigher accuracy, especially when discriminativemodels are used for statistical parsing.
For ex-ample, all possible sub-trees can be used as fea-tures (Collins and Duffy, 2002; Bod, 2003).
How-ever, most of the sub-trees are linguistically mean-ingless, and are a source of noisy features thus limit-ing efficiency and accuracy.
An alternative to the useof arbitrary sets of sub-trees is to use the set of ele-mentary trees as defined in Lexicalized Tree Adjoin-ing Grammar (LTAG) (Joshi and Schabes, 1997).LTAG based features not only allow a more limitedand a linguistically more valid set of features oversub-trees, they also provide the use of features thatuse discontinuous sub-trees which are outside thescope of previous tree kernel definitions using arbi-trary sub-trees.
In this paper, we use the LTAG basedfeatures in the parse reranking problem (Collins,2000; Collins and Duffy, 2002).
We use the Sup-port Vector Machine (SVM) (Vapnik, 1999) basedalgorithm proposed in (Shen and Joshi, 2003) as thereranker in this paper.
We apply the tree kernel toderivation trees of LTAG, and extract features fromderivation trees.
Both the tree kernel and the linearkernel on the richer feature set are used.
Our exper-iments show that the use of tree kernel on derivationtrees makes the notion of a tree kernel more power-ful and more applicable.2 Lexicalized Tree Adjoining GrammarIn this section, we give a brief introduction to theLexicalized Tree Adjoining Grammar (more detailscan be found in (Joshi and Schabes, 1997)).
InLTAG, each word is associated with a set of elemen-tary trees.
Each elementary tree represents a possi-ble tree structure for the word.
There are two kindsof elementary trees, initial trees and auxiliary trees.Elementary trees can be combined through two op-erations, substitution and adjunction.
Substitution isused to attach an initial tree, and adjunction is usedto attach an auxiliary tree.
In addition to adjunction,we also use sister adjunction as defined in the LTAGstatistical parser described in (Chiang, 2000).1 Thetree resulting from the combination of elementarytrees is is called a derived tree.
The tree that recordsthe history of how a derived tree is built from theelementary trees is called a derivation tree.2We illustrate the LTAG formalism using an exam-ple.Example 1: Pierre Vinken will join the board as anon-executive director.The derived tree for Example 1 is shown in Fig.
1(we omit the POS tags associated with each word tosave space), and Fig.
2 shows the elementary treesfor each word in the sentence.
Fig.
3 is the deriva-tion tree (the history of tree combinations).
One of1Adjunction is used in the case where both the root node andthe foot node appear in the Treebank tree.
Sister adjunction isused in generating modifier sub-trees as sisters to the head, e.gin basal NPs.2Each node ??n?
in the derivation tree is an elementary treename ?
along with the location n in the parent elementary treewhere ?
is inserted.
The location n is the Gorn tree address (seeFig.
4).SNPPierre VinkenVPwill VPVPjoin NPthe boardPPas NPa non-executive directorFigure 1: Derived tree (parse tree) for Example 1.NPPierreNPVinkenVPwill VP?SNP?
VPjoin NP?NPtheNPboardVPVP?
PPas NP?NPaNPnon-executiveNPdirector?1: ?2: ?2: ?1:?3: ?3: ?4: ?6: ?5: ?4:Figure 2: Elementary trees for Example 1.the properties of LTAG is that it factors recursion inclause structure from the statement of linguistic con-straints, thus making these constraints strictly local.For example, in the derivation tree of Examples 1,?1(join) and ?2(V inken) are directly connectedwhether there is an auxiliary tree ?2(will) or not.We will show how this property affects our redefinedtree kernel later in this paper.
In our experiments inthis paper, we only use LTAG grammars where eachelementary tree is lexicalized by exactly one word(terminal symbol) on the frontier.3 Parse RerankingIn recent years, reranking techniques have been suc-cessfully used in statistical parsers to rerank the out-put of history-based models (Black et al, 1993).
Inthis paper, we will use the LTAG based features toimprove the performance of reranking.
Our motiva-tions for using LTAG based features for rerankingare the following:?
Unlike the generative model, it is trivial to in-corporate features of various kinds in a rerank-ing setting.
Furthermore the nature of rerank-ing makes it possible to use global features,?1(join)???2(Vinken)?00??1(Pierre)?0??2(will)?01?
?3(board)?011??3(the)?0??4(as)?01??4(director)?011??5(non-executive)?0?
?6(a)?0?Figure 3: Derivation tree: shows how the elementarytrees shown in Fig.
2 can be combined to provide ananalysis for the sentence in Example 1.which allow us to combine features that are de-fined on arbitrary sub-trees in the parse tree andfeatures defined on a derivation tree.?
Several hand-crafted and arbitrary featureshave been exploited in the statistical parsingtask, especially when parsing the WSJ PennTreebank dataset where performance has beenfinely tuned over the years.
Showing a positivecontribution in this task will be a convincingtest for the use of LTAG based features.?
The parse reranking dataset is well established.We use the dataset defined in (Collins, 2000).In (Collins, 2000), two reranking algorithms wereproposed.
One was based on Markov RandomFields, and the other was based on the Boosting al-gorithm.
In both these models, the loss functionswere computed directly on the feature space.
Fur-thermore, a rich feature set was introduced that wasspecifically selected by hand to target the limitationsof generative models in statistical parsing.In (Collins and Duffy, 2002), the Voted Percep-tron algorithm was used for parse reranking.
TheS0NP00?
VP01join010 NP011?Figure 4: Example of how each node in an elemen-tary tree has a unique node address using the Gornnotation.
0 is the root with daughters 00, 01, and soon recursively, e.g.
first daughter 01 is 010.VPwill VP... PP... NPaFigure 5: A sub-tree which is linguistically mean-ingless.tree kernel was used to compute the number of com-mon sub-trees of two parse trees.
The features usedby this tree kernel contains all the hand selected fea-tures of (Collins, 2000).
It is worth mentioning thatthe f -scores reported in (Collins and Duffy, 2002)are about 1% less than the results in (Collins, 2000).In (Shen and Joshi, 2003), a SVM based rerank-ing algorithm was proposed.
In that paper, the no-tion of preference kernels was introduced to solvethe reranking problem.
Two distinct kernels, the treekernel and the linear kernel were used with prefer-ence kernels.4 Using LTAG Based Features4.1 MotivationWhile the tree kernel is an easy way to compute sim-ilarity between two parse trees, it takes too many lin-guistically meaningless sub-trees into consideration.Let us consider the example sentence in Example1.
The parse tree, or derived tree, for this sentenceis shown in Fig.
1.
Fig.
5 shows one of the lin-guistically meaningless sub-trees.
The number ofmeaningless sub-trees is a misleading measure fordiscriminating good parse trees from bad.
Further-more, the number of meaningless sub-trees is fargreater than the number of useful sub-trees.
Thislimits both efficiency and accuracy on the test data.The use of unwanted sub-trees greatly increases thehypothesis space of a learning machine, and thus de-creases the expected accuracy on test data.
In thiswork, we consider the hypothesis that linguisticallymeaningful sub-trees reveal correlations of interestand therefore are useful in stochastic models.We notice that each sub-tree of a derivation treeis linguistically meaningful because it represents avalid sub-derivation.
We claim that derivation treesprovide a more accurate measure of similarity be-tween two parses.
This is one of the motivationsfor applying tree kernels to derivation trees.
Notethat the use of features on derivation trees is differ-ent from the use of features on dependency graphs,derivation trees include many complex patterns oftree names and attachment sites and can representword to word dependencies that are not possible intraditional dependency graphs.For example, the derivation tree for Example 1with and without optional modifiers such as ?4(as)are minimally different.
In contrast, in derived(parse) trees, there is an extra VP node whichchanges quite drastically the set of sub-trees withand without the PP modifier.
In addition, using onlysub-trees from the derived tree, we cannot repre-sent a common sub-tree that contains only the wordsVinken and join since this would lead to a discontin-uous sub-tree.
However, LTAG based features canrepresent such cases trivially.The comparison between (Collins, 2000) and(Collins and Duffy, 2002) in ?3 shows that it is hardto add new features to improve performance.
Ourhypothesis is that the LTAG based features providea novel set of abstract features that complement thehand selected features from (Collins, 2000) and theLTAG based features will help improve performancein parse reranking.4.2 Extracting Derivation TreesBefore we can use LTAG based features we needto obtain an LTAG derivation tree for each parsetree under consideration by the reranker.
Our solu-tion is to extract elementary trees and the derivationtree simultaneously from the parse trees producedby an n-best statistical parser.
Our training andtest data consists of n-best output from the Collinsparser (see (Collins, 2000) for details on the dataset).Since the Collins parser uses a lexicalized context-free grammar as a basis for its statistical model, weobtain parse trees that are of the type shown in Fig.6.
From this tree we extract elementary trees andderivation trees by recursively traversing the spineof the parse tree.
The spine is the path from a non-terminal lexicalized by a word to the terminal sym-bol on the frontier equal to that word.
Every sub-treerooted at a non-terminal lexicalized by a differentword is excised from the parse tree and recorded intoS(join)NP-A(Vinken)Pierre VinkenVP(join)will VP(join)VP(join)join NP-A(board)the boardPP(as)as NP-A(director)a non-executive directorFigure 6: Sample output parse from the Collinsparser.
Each non-terminal is lexicalized by the pars-ing model.
-A marks arguments recovered by theparser.the derivation tree as a substitution.
Repeated non-terminals on the spine (e.g.
VP(join) .
.
.
VP(join) inFig.
6) are excised along with the sub-trees hang-ing off of it and recorded into the derivation tree asan adjunction.
The only other case is those sub-trees rooted at non-terminals that are attached tothe spine.
These sub-trees are excised and recordedinto the derivation tree as cases of sister adjunction.Each sub-tree excised is recursively analyzed withthis method, split up into elementary trees and thenrecorded into the derivation tree.
The output of ouralgorithm for the input parse tree in Fig.
6 is shownin Fig.
2 and Fig.
3.
Our algorithm is similar tothe derivation tree extraction explained in (Chiang,2000), except we extract our LTAG from n-best setsof parse trees, while in (Chiang, 2000) the LTAG isextracted from the Penn Treebank.3 For other tech-niques for LTAG grammar extraction see (Xia, 2001;Chen and Vijay-Shanker, 2000).4.3 Using Derivation TreesIn this paper, we have described two models to em-ploy derivation trees.
Model 1 uses tree kernels onderivation trees.
In order to make the tree kernelmore lexicalized, we extend the original definitionof the tree kernel, which we will describe below.Model 2 abstracts features from derivation trees anduses them with a linear kernel.In Model 1, we combine the SVM results of thetree kernel on derivation trees with the SVM resultsgiven by a linear kernel based on features on the de-rived trees.3Also note that the path from the root node to the foot nodein auxiliary trees can be greater than one (for trees with S roots).In Model 2, the vector space of the linear kernelconsists of both LTAG based features defined on thederived trees and features defined on the derivationtree.
The following LTAG features have been usedin Model 2.?
Elementary tree.
Each node in the derivationtree is used as a feature.?
Bigram of parent and its child.
Each pairof parent elementary tree and child elementarytree, as well as the type of operation (substi-tution, adjunction or sister adjunction) and theGorn address on parent (see Fig.
4) is used as afeature.?
Lexicalized elementary tree.
Each elemen-tary tree associated with its lexical item is usedas a feature.?
Lexicalized bigram.
In Bigram of parent andits child, each elementary tree is lexicalized(we use closed class words, e.g.
adj, adv, prep,etc.
but not noun or verb).4.4 Lexicalized Tree KernelIn (Collins and Duffy, 2001), the notion of a tree ker-nel is introduced to compute the number of commonsub-trees of two parse trees.
For two parse trees, p1and p2, the tree kernel Tree(p1, p2) is defined as:Tree(p1, p2) =?n1 in p1n2 in p2T (n1, n2) (1)The recursive function T is defined as follows: If n1and n2 have the same bracketing tag (e.g.
S, NP, VP,.
.
.)
and the same number of children,T (n1, n2) = ?
?i(1 + T (n1i, n2i)), (2)where, nki is the ith child of the node nk, ?
is aweight coefficient used to control the importance oflarge sub-trees and 0 < ?
?
1.If n1 and n2 have the same bracketing tag but dif-ferent number of children, T (n1, n2) = ?.
If theydon?t have the same bracketing tag, T (n1, n2) = 0.In (Collins and Duffy, 2002), lexical items are alllocated at the leaf nodes of parse trees.
ThereforeVP(join)VP(join)V(join) NP(board)PP(as)P(as) NP(director)tree with root node n:VPVPV NPPPP NPptn(n):lex(n): (join, join, as)Figure 7: A lexicalized sub-tree rooted at n andits decomposition into a pattern, ptn(n) and corre-sponding vector of lexical information, lex(n).sub-trees that do not contain any leaf node are notlexicalized.
Furthermore, due to the introduction ofparameter ?, lexical information is almost ignoredfor sub-trees whose root node is not close to the leafnodes, i.e.
sub-trees rooted at S node.In order to make the tree kernel more lexicalized,we associate each node with a lexical item.
For ex-ample, Fig.
7 shows a lexicalized sub-tree and itsdecomposition into features.
As shown in Fig.
7 thelexical information lex(t) extracted from the lexical-ized tree consists of words from the root and its im-mediate children.
This is because we wish to ig-nore irrelevant lexicalizations such as NP(board) inFig.
7.A lexicalized sub-tree rooted on node n is splitinto two parts.
One is the pattern tree of n, ptn(n).The other is the vector of lexical information of n,lex(n), which contains the lexical items of the rootnode and the children of the root.For two tree nodes n1 and n2, the recursive func-tion LT (n1, n2) used to compute the lexicalized treekernel is defined as follows.LT (n1, n2) = (1 + Cnt(lex(n1), lex(n2)))?
T ?
(ptn(n1), ptn(n2)), (3)where T ?
is the same as the original recursive func-tion T defined in (2), except that T is defined onparse tree nodes, while T ?
is defined on patterns ofparse tree nodes.
Cnt(x, y) counts the number ofcommon elements in vector x and y.
For example,Cnt((join, join, as), (join, join, in)) = 2, since2 elements of the two vectors are the same.It can be shown that the lexicalized tree kernelcounts the number of common sub-trees that meetthe following constraints.?
None or one node in the sub-tree is lexicalized?
The lexicalized node is the root node or a childof the root, if applicable.Therefore our new tree kernel is more lexicalized.On the other hand, it immediately follows that thelexicalized tree kernel is well-defined.
It means thatwe can embed the lexicalized tree kernel into a highdimensional space.
The proof is similar to the prooffor the tree kernel in (Collins and Duffy, 2001).Another important advantage of the lexicalizedtree kernel is that it is more compressible.
It is notedin (Collins and Duffy, 2001) that training trees canbe combined by sharing sub-trees to speed up thetest.
As far as the lexicalized tree kernel is con-cerned, the pattern trees are more compressible be-cause there is no lexical item at the leaf nodes ofpattern trees.
Lexical information can be attachedto the nodes of the result pattern forest.
In our ex-periment, we select five parses from each sentencein Collins?
training data and represent these parseswith shared structure.
The number of the nodes inthe pattern forest is only 1/7 of the total number ofthe nodes the selected parse trees.4.5 Tree Kernel for Derivation TreesIn order to apply the (lexicalized) tree kernel toderivation trees, we need to make some modifica-tions to the original recursive definition of the treekernel.For derivation trees, the recursive function is trig-gered if the two root nodes have the same non-lexicalized elementary tree (sometimes called su-pertag).
Note that these two nodes will have thesame number of children which are initial trees (aux-iliary trees are not counted).
In comparison, the re-cursive function in (2), T (n1, n2) is computed if andonly if n1 and n2 have the same bracketing tag andthey have the same number of children.For each node, its children are attached with oneof the two distinct operations, substitution or adjunc-tion.
For substituted children, the computation of thetree kernel is almost the same as that for CFG parsetree.
However, there is a problem with the adjoinedchildren.
Let us first have a look at a sentence inPenn Treebank.Example 2: COMMERCIAL PAPER placed di-rectly by General Motors Acceptance Corp.: 8.55%30 to 44 days; 8.25% 45 to 59 days; 8.45% 60 to 89days; 8% 90 to 119 days; 7.90% 120 to 149 days;7.80% 150 to 179 days; 7.55% 180 to 270 days.In this example, seven sub-trees of the same typeare sister adjoined to the same place of an initial tree.So the number of common sub-trees increases dra-matically if the tree kernel is applied on two similarparses of this sentence.
Experimental evidence indi-cates that this is harmful to accuracy.
Therefore, forderivation trees, we are only interested in sub-treesthat contain at most 2 adjunction branches for eachnode.
The number of constrained common sub-treesfor the derivation tree kernel can be computed bythe recursive function DT over derivation tree nodesn1, n2:DT (n1, n2) = (1 + A1(n1, n2) + A2(n1, n2))?
T?
(sub(n1), sub(n2)) (4)where sub(nk) is the sub-tree of nk in which chil-dren adjoined to the root of nk are pruned.
T?
issimilar to the original recursive function T definedin (2), but it is defined on derivation tree nodes re-cursively.
A1 and A2 are used to count the numberof common sub-trees whose root nodes only containone or two adjunction children respectively.A1(n1, n2) =?i,jDT (a1i, a2j),where, a1i is the ith adjunct of n1, and a2j is the jthadjunct of n2.
Similarly, we have:A2(n1, n2) =?i<k,j<lDT (a1i, a2j) ?
DT (a1k, a2l)The tree kernel for derivation trees is a well-definedkernel function because we can easily define an em-bedding space according to the definition of the newtree kernel.
By substituting DT for T ?
in (3), we ob-tain the lexicalized tree kernel for LTAG derivationtrees (using LT in (1)).5 ExperimentsAs described above, we use the SVM based votingalgorithm (Shen and Joshi, 2003) in our rerankingexperiments.
We use preference kernels and pair-wise parse trees in our reranking models.We use the same data set as described in (Collins,2000).
Section 2-21 of the Penn WSJ Treebank areused as training data, and section 23 is used for fi-nal test.
The training data contains around 40,000sentences, each of which has 27 distinct parses onaverage.
Of the 40,000 training sentences, the first36,000 are used to train SVMs.
The remaining 4,000sentences are used as development data.Due to the computational complexity of SVM, wehave to divide training data into slices to speed uptraining.
Each slice contain two pairs of parses fromevery sentence.
Specifically, slice i contains pos-itive samples ((p?k, pki),+1) and negative samples((pki, p?k),?1), where p?k is the best parse for sen-tence k, pki is the parse with the ith highest log-likelihood in all the parses for sentence k and it isnot the best parse (Shen and Joshi, 2003).
There areabout 60000 samples in each slice in average.For the tree kernel SVMs of Model 1, we take3 slices as a chunk, and train an SVM for eachchunk.
Due to the limitation of computing resource,we have only trained on 3 chunks.
The results oftree kernel SVMs are combined with simple com-bination.
Then the outcome is combined with theresult of the linear kernel SVMs trained on featuresextracted from the derived trees which are reportedin (Shen and Joshi, 2003).
For each parse, the num-ber of the brackets in it and the log-likelihood givenby Collins?
parser Model 2 are also used in the com-putation of the score of a parse.
For each parse p, itsscore Sco(p) is defined as follows:Sco(p) = MT (p) + ?
?
ML(p) + ?
?
l(p) + ?
?
b(p),where MT (p) is the output of the tree kernel SVMs,ML(p) is the output of linear kernel SVMs, l(p) isthe log-likelihood of parse p, and b(p) is the num-ber of brackets in parse p. We noticed that the SVMsystems prefers to give higher scores to the parseswith less brackets.
As a result, the system has a highprecision but a low recall.
Therefore, we take thenumber of brackets, b(p), as a feature to make therecall and precision balanced.
The three weight pa-rameters are tuned on the development data.The results are shown in Table 1.
With Model1, we achieve LR/LP of 89.7%/90.0% on sentences?40 Words (2245 sentences)Model LR LP CBs 0 CBs 2 CBsCO99 88.5% 88.7% 0.92 66.7% 87.1%CO00 90.1% 90.4% 0.73 70.7% 89.6%CD02 89.1% 89.4% 0.85 69.3% 88.2%SJ 03 89.9% 90.3% 0.75 71.7% 89.4%M1 90.2% 90.5% 0.72 72.3% 90.0%M2 89.8% 90.3% 0.76 71.6% 89.6%?100 Words (2416 sentences)Model LR LP CBs 0 CBs 2 CBsCO99 88.1% 88.3% 1.06 64.0% 85.1%CO00 89.6% 89.9% 0.87 68.3% 87.7%CD02 88.6% 88.9% 0.99 66.5% 86.3%SJ 03 89.4% 89.8% 0.89 69.2% 87.6%M1 89.7% 90.0% 0.86 70.0% 88.2%M2 89.3% 89.8% 0.89 69.1% 87.7%Table 1: Results on section 23 of the WSJ Tree-bank.
LR/LP = labeled recall/precision.
CBs = av-erage number of crossing brackets per sentence.
0CBs, 2 CBs are the percentage of sentences with0 or ?
2 crossing brackets respectively.
CO99 =(Collins, 1999) Model 2.
CO00 = (Collins, 2000).CD02 = (Collins and Duffy, 2002).
SJ03 = linearkernel of (Shen and Joshi, 2003).
M1=Model 1.M2=Model 2.with ?
100 words.
Our results show a 17% rel-ative difference in f -score improvement over theuse of a linear kernel without LTAG based features(Shen and Joshi, 2003).
In addition, we also getnon-trivial improvement on the number of crossingbrackets.
These results verify the benefit of usingLTAG based features and confirm the hypothesis thatLTAG based features provide a novel set of abstractfeatures that complement the hand selected featuresfrom (Collins, 2000).
Our results on Model 1 showa 1% error reduction on the previous best rerankingresult using the dataset reported in (Collins, 2000).Also, Model 1 provides a 10% reduction in errorover (Collins and Duffy, 2002) where the featuresfrom tree kernel were over arbitrary sub-trees.For Model 2, we first train 22 SVMs on 22 dis-tinct slices.
Then we combine the results of individ-ual SVMs with simple combination.
However, theoverall performance does not improve.
But we no-tice that the use of LTAG based features gives rise to0.8740.8750.8760.8770.8780.8790.880.8810 5 10 15 20ID of sliceswithout LTAGwith LTAGFigure 8: Comparison of performance of individualSVMs in Model 2: with and without LTAG basedfeatures.
X-axis stands for the ID of the slices onwhich the SVMs are trained.Y-axis stands for the f -score.improvement on most of the single SVMs, as shownin Fig.
8.We think there are several reasons to account forwhy our Model 2 doesn?t work as well for the fulltask when compared with Model 1.
Firstly, the train-ing slice is not large enough.
Local optimization oneach slice does not result in global optimization (asseen in Fig.
8).
Secondly, the LTAG based featuresthat we have used in the linear kernel in Model 2 arenot as useful as the tree kernel in Model 1.4 The lastreason is that we do not set the importance of LTAGbased features.
One shortcoming of kernel methodsis that the coefficient of each feature must be set be-fore the training (Herbrich, 2002).
In our case, wedo not tune the coefficients for the LTAG based fea-tures in Model 2.6 Conclusions and Future WorkIn this paper, we have proposed methods for usingLTAG based features in the parse reranking task.The experimental results show that the use of LTAGbased features gives rise to improvement over al-ready finely tuned results.
We used LTAG based fea-tures for the parse reranking task and obtain labeledrecall and precision of 89.7%/90.0% on WSJ sec-tion 23 of Penn Treebank for sentences of length ?100 words.
Our results show that the use of LTAG4In Model 1, we implicitly take every sub-tree of the deriva-tion trees as a feature, but in Model 2, we only consider a smallset of sub-trees in a linear kernel.based tree kernel gives rise to a 17% relative differ-ence in f -score improvement over the use of a linearkernel without LTAG based features.
In future work,we will use some light-weight machine learning al-gorithms for which training is faster, such as vari-ants of the Perceptron algorithm.
This will allow usto use larger training data chunks and take advan-tage of global optimization in the search for relevantfeatures.ReferencesE.
Black, F. Jelinek, J. Lafferty, Magerman D. M., R. Mercer,and S. Roukos.
1993.
Towards history-based grammars:Using richer models for probabilistic parsing.
In Proc.
of theACL 1993.R.
Bod.
2003.
An Efficient Implementation of a New DOPModel.
In Proc.
of EACL 2003, Budapest.J.
Chen and K. Vijay-Shanker.
2000.
Automated Extraction ofTAGs from the Penn Treebank.
In Proc.
of the 6th IWPT.D.
Chiang.
2000.
Statistical Parsing with an Automatically-Extracted Tree Adjoining Grammar.
In Proc.
of ACL-2000.M.
Collins and N. Duffy.
2001.
Convolution kernels for naturallanguage.
In Proc.
of the 14th NIPS.M.
Collins and N. Duffy.
2002.
New ranking algorithms forparsing and tagging: Kernels over discrete structures, andthe voted perceptron.
In Proc.
of ACL 2002.M.
Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, University of Pennsylvania.M.
Collins.
2000.
Discriminative reranking for natural lan-guage parsing.
In Proc.
of 7th ICML.M.
Collins.
2001.
Parameter estimation for statistical parsingmodels: Theory and practice of distribution-free methods.In Proc.
of IWPT 2001.
Invited Talk at IWPT 2001.R.
Herbrich.
2002.
Learning Kernel Classifiers: Theory andAlgorithms.
MIT Press.A.
K. Joshi and Y. Schabes.
1997.
Tree-adjoining grammars.In G. Rozenberg and A. Salomaa, editors, Handbook of For-mal Languages, volume 3, pages 69 ?
124.
Springer.L.
Shen and A. K. Joshi.
2003.
An SVM based voting algo-rithm with application to parse reranking.
In Proc.
of CoNLL2003.V.
N. Vapnik.
1999.
The Nature of Statistical Learning Theory.Springer, 2nd edition.F.
Xia.
2001.
Investigating the Relationship between Gram-mars and Treebanks for Natural Languages.
Ph.D. thesis,University of Pennsylvania, Philadelphia, PA.
