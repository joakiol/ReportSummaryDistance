Multi-level Dialogue Act TagsAlexander Clark and Andrei Popescu-BelisISSCO / TIM / ETIUniversity of GenevaUNI-MAIL, Boulevard du Pont-d?Arve 40CH-1211 Geneva 4Switzerlandasc@aclark.demon.co.uk andrei.popescu-belis@issco.unige.chAbstractIn this paper we discuss the use of multi-layered tagsets for dialogue acts, in the con-text of dialogue understanding for multi-party meeting recording and retrieval ap-plications.
We discuss some desiderata forsuch tagsets and critically examine someprevious proposals.
We then define MAL-TUS, a new tagset based on the ICSI-MRand Switchboard tagsets, which satisfiesthese requirements.
We present some ex-periments using MALTUS which attemptto compare the merits of integrated versusmulti-level classifiers for the detection of di-alogue acts.1 IntroductionThe processing of dialogues by computers servestwo main applicative goals: understanding of hu-man dialogues, for information extraction or sum-marization, and human-computer dialogue manage-ment, for language-based or multimodal interfaces.Whether the computer takes part in a dialogue oronly attempts to monitor a recorded one, it is im-portant to detect the functions of each of the humanutterances that constitute the dialogue.
In addition,when the computer must generate an utterance asa reply, this must also bear some of the functionsexpected by the hearer in return.In this article, we focus on dialogue understand-ing for a dialogue storage and retrieval application,developed in the (IM)2 project1.
The goal of theapplication is the multimodal recording of meetings(such as staff or business meetings), the processingand storage of the recordings into a database, and the1(IM)2 stands for Interactive Multimodal InformationManagement, a project sponsored by the Swiss Govern-ment (see http://www.im2.ch).possibility of querying the dialogue database (Arm-strong et al, 2003).
The query interface and theprocessing of the dialogue must therefore meet theneeds of the potential users of the system, who willattempt to retrieve various types of information fromthe meeting recordings.
While the result of the queryis in general a chunk of recorded dialogue (prefer-ably with multimedia rendering), the criteria used toquery the database can vary from trivial (?who at-tended the meeting??)
to very abstract (?what werethe main decisions??).
Some form of understandingof the dialogue structure is thus required for a sig-nificant proportion of potential queries (more aboutrequirements in subsection 2.3).The utterance functions with which we deal in thispaper are dialogue acts.
Although dialogue acts (DA)tags are commonly used as a simple representationof the function of an utterance in dialogue, there islittle consensus amongst researchers about what setof DA tags is appropriate in a particular situation.Our own application domain, meeting recording, iscomparatively open-ended and we do not yet havea clear understanding of precisely what features willbe most useful.
In section 2, we will try to under-stand the multiplicity of DA tagsets, then we will an-alyze (section 3) the dialogue data and annotationson which we work.
These considerations promptedus to abstract a new DA tagset, of which we explainthe merits in section 4.
Experiments on the auto-matic annotation of DAs using the MALTUS tagsetare described in section 5; the results (subsection 5.2)are followed by a brief discussion.2 Understanding Dialogue Structure:Dialogue Acts2.1 The Concepts behind Dialogue ActsDialogues are series of speaker turns.
Utterances canbe defined as the atomic subparts of a turn that ac-complish one or more ?functions?
with respect tospeaker interaction.
Utterances are in general sig-nalled by syntactic and/or prosodic means, but thespecificity of their ?function?
belongs to pragmat-ics (Levinson, 1983, ch.
4).
Linguists have identifiedseveral dimensions for the role of sentences utteredin a dialogue.
These dimensions are not mutually ex-clusive, and there are certainly correlations betweensome of them (e.g.
?question?
as a speech act andas a member of an adjacency pair).?
Speech acts (Searle, 1969; Vanderveken, 1990):(1) representatives, such as assertions or con-clusions; (2) directives, such as requests, ques-tions, suggestions; (3) commissives, such aspromises, threatenings, offers; (4) expressivessuch as thanks, apologies, congratulations; (5)declarations, such as excommunications, decla-rations of war, christening, firing from employ-ment, etc.?
Turn management: backchannel, floor holder,floor grabber, hold;?
Adjacency pairs: utterances can be the first partor the second part of exchange pairs such as re-quest / accept (or refuse); offer / accept; assess/ (dis)agree; question / answer; etc.?
Overall organization and topics: openings, clos-ings, topic-changers, topic-continuers, etc.?
Politeness management: face-threatening, face-saving, neutral;?
Rhetorical role: elaboration, purpose, restate-ment, etc.2.2 Dialogue Acts in ComputationalLinguisticsThere is not much agreement, within the CL/NLPcommunity, on the definition of a dialogue act.
Theterm denotes some function of an utterance in a dia-logue, not reducible to its syntactic or semantic con-tent.
The function is selected, in general, amonga set of possible dialogue acts (a DA tagset) thatdepends on the goals of its creator (Traum, 2000).One of the main inspiration sources for DA tagsetsare speech acts, but the original repertoire (Searle,1969; Vanderveken, 1990) has been gradually en-riched with other possible functions.
From the nu-merous DA tagsets (Klein and Soria, 1998), the fol-lowing are particularly relevant to a general-domainmeeting recording application.The DA tags in DAMSL (Allen and Core, 1997)are nearly all independent: the DAMSL guidelinesstate that all tags (i.e.
all ?functions?)
that charac-terize an utterance should be associated with it.
TheDAMSL tags are grouped in four dimensions: com-municative status, information level, forward-lookingfunction and backward-looking function.
In fact,several theories are conflated in DAMSL, which wasinitially designed as a shared resource with a focusprimarily on task-oriented dialogs (Core and Allen,1997).
There are about 4 million possible combi-nations of DAMSL tags, which make a huge searchspace for automatic annotation.The application of DAMSL to the Switchboarddata (two-party telephone conversations) lead toSWBD-DAMSL (Jurafsky et al, 1997), a smallertagset than DAMSL.
About 200,000 SWBD utter-ances were first annotated with DAMSL tags: it wasobserved that only 220 combinations of tags occurred(Jurafsky et al, 1998).
These 220 labels were thenclustered into 42 tags, such as: statement (36%),opinion (13%), agree/accept (5%), yes-no-question(2%).
The resulting search space (42 mutually ex-clusive tags) was well adapted to the initial goals,viz., the automatic annotation of dialogue acts andthe use of dialogue act specific language models inspeech recognition (Stolcke et al, 2000).2.3 Requirements for the Definition of aDA TagsetIn this paper, our goal is to design a new DA tagsetfor our application, with the following constraints inmind (see also the analysis by D. Traum (2000)):?
Relation to one or more existing theories (de-scriptive, explanatory, etc.).?
Compatibility with the observed functions of ac-tual utterances in context, in a given domain.?
Empirical validation: reliability of human appli-cation of the tagset to typical data (high inter-annotator agreement, at least potentially).?
Possibility of automatic annotation (this re-quirement is specific to NLP).?
Relevance to the targeted NLP application:there are numerous possible functions of utter-ances, but only some of them are really use-ful to the application.
Within our IM2.MDMproject, a study has been conducted on the rel-evance of dialogue acts (in particular) to typicaluser queries on meeting recordings (Lisowska,2003)2.?
Mapping (at least partially) to existing tagsets,so that useful insights are preserved, and datacan be reused.2Many other potential uses of dialogue act informa-tion have been hypothesized, such as their use to increaseASR accuracy (Stolcke et al, 2000), or to locate ?hotspots?
in meetings (Wrede and Shriberg, 2003).3 Available Data and Annotations:ICSI Meeting RecorderThe volume of available annotated data suffers fromthe diversity of DA tagsets (Klein and Soria, 1998).One of the most significant resources is the Switch-board corpus mentioned above, but telephone con-versations have many differences with multi-partymeetings.
Apart from the data recently available inthe IM2 project, results reported in this paper makeuse of the ICSI Meeting Recording (MR) corpus oftranscribed and annotated dialogues (Morgan et al,2003; Shriberg et al, 2004)3.3.1 Overview of ICSI MR CorpusThe ICSI-MR corpus consists of 75 one-hour record-ings of staff meetings, each involving up to eightspeakers on separate mike channels.
Each channelwas manually transcribed and timed, then annotatedwith dialogue act and adjacency pair information(Shriberg et al, 2004).
Following a preliminary re-lease in November 2003 (sound files, transcriptions,and annotations), the full corpus was released inFebruary 2004 to IM2 partners.The dialogue act annotation makes use of the pre-existing segmentation of each channel into (prosodic)utterances, sometimes segmented further into func-tional utterances, each of them bearing a separate di-alogue act.
There are about 112,000 prosodic utter-ances, and about 7,200 are segmented into two func-tional utterances (only one is segmented in three).3.2 Discussion of the ICSI-MR DA TagsetEach functional utterance from the ICSI-MR corpusis marked with a dialogue label, composed of oneor more tags from the ICSI-MR tagset (Dhillon etal., 2004).
The tagset, which is well documented,is based on SWBD-DAMSL, but unlike SWBD-DAMSL, it allows one utterance to be marked withmultiple tags.
Also, the SWBD-DAMSL tagset wasextended, for instance with disruption tags such as?interrupted?, ?abandoned?, etc.
Utterances can alsobe marked as ?unintelligible?
or ?non-speech?.
AnICSI-MR label is made of a general tag, followedby zero or more specific tags, followed or not by adisruption tag:gen_tag [^spec_tag_1 ... ^spec_tag_n] [.d]Our formalization of the guidelines using rewritingrules (Popescu-Belis, 2003) shows that few tags aremutually exclusive.
The number of possible combi-nations (DA labels) reaches several millions.
For in-stance, even when not considering disruption marks,3See http://www.icsi.berkeley.edu/Speech/mr/the labels are a combination of one general tag outof 11, and one or more specific tags out of 39.
If upto five specific tags are allowed (as observed empir-ically in the annotated data), there are more than7,000,000 possible labels; if specific tags are limitedto four, there are about 1,000,000 possible labels.Some studies acknowledge the difficulties of an-notating precisely with ICSI-MR, but also thefine-grained distinctions it allows for, e.g.
be-tween the possible functions of four related dis-course particles (?yeah?, ?right?, ?okay?, and ?uhhuh?
):agreement/acceptance, acknowledgment, backchan-nel, floor grabber (Bhagat et al, 2003).
Conversely,inter-annotator agreement on such fine-grained dis-tinctions (specific tags) is lower than agreement onmajor classes, though the kappa-statistic normallyused to measure agreement adjusts to a certain ex-tent for this.
In fact, ICSI-MR also provided a setof five ?classmaps?
that indicate how to group tagsinto categories which reduce the number of possiblelabels.
For instance, the simplest one reduces allDA labels to only five classes: statement, question,backchannel, floor holder/grabber, disruption.
OurMALTUS proposal (see 4.1 below) could be viewedas a classmap too: it preserves however more ICSI-MR tags than the existing classmaps, and assigns inaddition conditions of mutual exclusiveness.We also note that, while SWBD-DAMSL was anattempt to reduce the dimensionality of the DAMSLtagset (which had a clear theoretical base), the ICSI-MR tagset alows SWBD tags to be combined againinstead of going back to DAMSL tags.
Althoughour proposal that we proceed to describe (MALTUS)remains close to ICSI-MR for reusability reasons, weare also working on a more principled DA tagset thatdeparts from ICSI-MR (Popescu-Belis, 2003).3.3 Some Figures for the ICSI-MR DataIn the process of conversion to MALTUS (see 4.2below), we validated the ICSI-MR data and madeseveral observations.
Detected incoherent combina-tions of tags (e.g., two general tags in a label) andother remarks have also been sent back to ICSI.We first separate prosodic utterances into func-tional utterances, so that each utterance has one DAlabel (and not two, separated by ?|?
), thus obtaining120,205 utterances.
Also at this stage, we split ut-terances that correspond to reported speech (markedwith ?:?).
We then discard the disruption marks to fo-cus on the DA labels only ?
about 12,000 labels out ofca.
120,000 are disruption marks, or contain one.
Weare left with 113,560 utterances with DA labels, with776 observed types of labels.
An important param-eter is the number of occurring vs. possible labels,Nb.
of Nb.
of Nb.
of Nb.
oftags in theoretical occurring tokenslabel comb.
comb.1 11 11 68,2132 429 129 37,8893 8,151 402 5,0544 100,529 176 2,0645 904,761 49 3266 6,333,327 9 147 .
.
.
0 0Total: 7,347,208 776 113,560Table 1: Number of possible labels (combinations oftags): theoretical vs. actual.Maximal nb.
Maximal theoreticalof tags accuracy on ICSI-MR1 0.6012 0.9343 0.9794 0.9975 0.9996 1Table 2: Maximal accuracy of DA tagging of theICSI-MR data that could be reached using a limitednumber of tags per label.which depends a lot on the number of specific tagsin a label, as summarized in table 1.
The maximumobserved in the available data is five specific tags ina label (hence six tags in all).There is no guarantee that meaningful labels can-not have more than six tags.
However, such labelsare probably very infrequent, and a reasonable op-tion for automatic tagging is to limit the numberof tag combinations, which is the main goal of theMALTUS tagset.
The maximal accuracies that couldbe obtained on the available ICSI-MR data if thenumber of tags in a label was limited to 1, 2, etc.are shown in Table 2.
In computing the accuracy weconsider here only perfect matches, but scores couldbe higher if partial matches count too.
Two or threetags per label already allow very high accuracy, whileconsiderable reducing the search space.4 The MALTUS DA Tagset4.1 DefinitionWe defined MALTUS (Multidimensional AbstractLayered Tagset for Utterances) in order to reducethe number of possible combinations by assigningexclusiveness constraints among tags, while remain-ing compatible with ICSI-MR (Popescu-Belis, 2003).MALTUS is more abstract than ICSI-MR, but canbe refined if needed.
An utterance is either markedU (undecipherable) or it has a general tag and zeroor more specific tags.
It can also bear a disruptionmark.
More formally (?
means optional):DA -> (U | (gen_tag (spec_tags)?))
(.D)?gen_tag -> S | Q | B | Hspec_tags -> (RP | RN | RU)?
AT?
DO?
PO?The glosses of the tags, generally inspired fromICSI-MR, are:?
U = undecipherable (unclear, noisy)?
S = statement?
Q = question?
B = backchannel?
H = hold (floor holder, floor grabber, hold)?
RP = positive answer (or positive response)?
RN = negative answer (or negative response)?
RU = other answer (or undecided answer or re-sponse)?
RI = restated information?
DO = command or other performative (can berefined into: command, commitment, sugges-tion, open-option, explicit performative)?
AT = the utterance is related to attention man-agement (can be refined into: acknowledgement,rhetorical question backchannel, understandingcheck, follow me, tag question)?
PO = the utterance is related to politeness (canbe refined into sympathy, apology, downplayer,?thanks?, ?you?re welcome?)?
D = the utterance has been interrupted or aban-doned4.2 Conversion of ICSI-MR to MALTUSThere are only about 500 possible MALTUS labels,but observations of the converted ICSI-MR datashow again that the probability distribution is veryskewed.
An explicit correspondence table and con-version procedure were designed to convert ICSI-MRto MALTUS, so that the considerable ICSI-MR re-source can be reused.Correspondences between MALTUS and othertagsets (Klein and Soria, 1998) were also provided(Popescu-Belis, 2003).
Such ?mappings?
are imper-fect for two reasons: first, they work only in onedirection, from the more specific tagset (ICSI-MR /SWBD / DAMSL) to the more abstract one (MAL-TUS).
Second, a mapping is incomplete if one doesnot state which tags must be mutually exclusive.For MALTUS too, the idea to use at most threetags per label in an automatic annotation programmight reduce the search space without decreasing theaccuracy too much.
Another idea is to use only thelabels that appear in the data that is, only 50 labels.An even smaller search space is provided by the 26MALTUS labels that occur more than 10 times each.If only these are used for tagging, then only 70 occur-rences (only 0.061% of the total) would be incorrectlytagged, on the ICSI-MR reference data.
Occurringlabels ordered alphabetically and their frequencies(when greater than 10) are listed below.B (15180)H (12288)Q (5320)Q^AT (3137)Q^AT^RI (69)Q^DO (239)Q^RI (60)Q^RN (19)S (51304)S^AT (8280)S^AT^RI (273)S^DO (3935)S^DO^RI (32)S^DO^RN (38)S^DO^RP (41)S^DO^RU (16)S^PO (791)S^PO^RI (13)S^PO^RU (61)S^RI (765)S^RI^RN (46)S^RI^RP (436)S^RI^RU (18)S^RN (2219)S^RP (7612)S^RU (1298)Further analysis will tell whether this list shouldbe enriched with useful labels that are absent fromit.
Also, a comparison of MALTUS to the SWBDset (26 labels vs. 42) should determine whether theloss in informativeness in MALTUS is compensatedby the gain in search space size and in theoreticalgrounding.5 Automatic ClassificationAs discussed above, one of the desiderata for a tagsetin this application domain is that the tags can be ap-plied automatically.
A requirement for annotationsthat can only be applied manually is clearly unre-alistic except for meetings of very high importance.The ICSI-MR corpus on the other hand is concernedwith producing a body of annotated data that canbe used by researchers for a wide range of differentpurposes: linguists who are interested in particularforms of interaction, researchers in acoustics and soon.
It is by no means a criticism of their work thatsome of the distinctions that they annotate or at-tempt to annotate cannot be reliably automated.Here we report some preliminary experiments onthe automatic annotation of meeting transcripts withthese tagsets.
Our focus here is not so much on eval-uating a classifier for this task but rather evaluatingthe tagsets: we are interest in the extent to whichthey can be predicted reliably from easily extractedfeatures of the utterance and its context.
Addition-ally we are interested in the multi-level nature of thetagsets and exploring the extent to which the internalstructure of the tags allows other options for classi-fiers.
Therefore, our goal in these experiments is notto build a high performance classifier; rather, it is toexplore the extent to which multi level tagsets canbe predicted by classifying each level separately ?i.e.
by having a set of ?orthogonal?
classifiers ?
asopposed to classifying the entire structured objectin a single step using a single multi-class classifieron a flattened representation.
Accordingly there area number of areas in which our experimental setupdiffers from that which would be appropriate whenperforming experiments to evaluate a classifier.Since in this paper we are not using prosodic oracoustic information, but just the manual transcrip-tions, there are two sources of information that canbe used to classify utterances.
First, the sequence ofwords that constitutes the utterance, and secondlythe surrounding utterances and their classification.generally in prior research in this field, some formof sequential inference algorithm has been used tocombine the local decisions about the DA of each ut-terance into a classification of the whole utterance.The common way of doing this has been to use ahidden Markov model to model the sequence and touse a standard decoding algorithm to find either thesequence with maximum a posteriori (MAP) likeli-hood or to select for each utterance the DA withMAP likelihood.
In the work here, we will ignorethis complexity and allow our classifier access to thegold standard classification of the surrounding utter-ances.
This will make the task substantially easier,since in a real application, there will be some noisein the labels.5.1 Feature selectionThere are two sorts of features that we shall use here?
internal lexical features derived from the words inthe utterance, and contextual features derived fromthe surrounding utterances.
At our current state ofknowledge we have a very good idea about what thelexically derived features should be, and how theyshould be computed ?
namely n-grams or gappy n-grams including positional information.
Addition-ally, there are ways of computing these efficiently.However, with regard to the contextually derived fea-tures, our knowledge is much less complete.
(Stolckeet al, 2000) showed that in the Switchboard corpusthere was little dependence beyond the immediatelyadjacent utterance, but whether this also applies inthis multi-party domain is unknown.
Thus we findourselves in a rather asymmetric position with re-gard to these two information sources.
As we arenot here primarily interested in constructing a highperformance classifier, but rather identifying the pre-dictable elements of the tag, we have resolved thisproblem by deliberately selecting a rather limited setof lexical features, together with a limited set of con-textual features.
Otherwise, we feel that our experi-ments would be overly biased towards those elementsof the tag that are predictable from the internal lex-ical evidence.We used as lexical features the 1000 most frequentwords, together with additional features for thesewords occurring at the beggining or end of the ut-terance.
This gives an upper bound of 3000 lexicalfeatures.
We experimented with a variety of simplecontextual features.Preceding same label (SL) the immediately pre-ceding utterance on the same channel has a par-ticular DA tag.Preceding label (PL) a preceding utterance on adifferent channel has a particular DA tag.
Weconsider an utterance to be preceding if it startsbefore the start of the current utterance.Overlapping label (OL) an utterance on anotherchannel with a particular DA tag overlaps thecurrent utterance.
We anticipate this being use-ful for identifying backchannels.Containing label (CL) an utterance on anotherchannel with a particular DA tag contains thecurrent channel ?
i.e.
the start is before the startof the current utterance and the end is after theend of the current utterance.Figure 1 shows an artificial example in a multi-party dialog with four channels.
This illustrates thefeatures that will be defined for the classification ofthe utterance that is shaded.
In this example wewill have the following features SL:C1, PL:B1, PL:D1,CL:D1, OL:A1, OL:B1, OL:B2, OL:D1.
We have foundABCDA1B1 B2C1D1Figure 1: Artificial example illustrating contextualfeatures defined for a particular utterance (shaded).There are four channels labelled A to D; each boxrepresents an utterance, and the DA tag is repre-sented by the characters inside each box.that the overlapping label feature set does not helpthe classifiers here, so we have used the remainingthree contextual feature sets.
Note the absence ofcontextual features corresponding to labels of utter-ances that strictly follow the target utterance.
Wefelt that given the fact that we use the gold standardtags this would be too powerful.The data made available to us was preprocessedin a number of ways.
The most significant changewas to split utterances that had been labelled with asequence of DA labels (joined with pipes).
We sep-arated the utterances and the labels at the appro-priate points and realigned.
The data was providedwith individual time stamps for each word using aspeech recognizer in forced recognition mode: wherethere were errors or mismatches we discarded thewords.5.2 ResultsWe use a Maximum Entropy (ME) classifier (Man-ning and Klein, 2003) which allows an efficient com-bination of many overlapping features.
We selected5 meetings (6771 utterances after splitting) to use asour test set and 40 as our training set leaving a fur-ther five for possible later experiments.
As a simplebaseline we use the classifier which just guesses themost likely class.
We first performed some experi-ments on the original tag sets to see how predictablethey are.We started by defining a simple six-way classifica-tion task which classifies disruption forms, and unde-cipherable forms as well as the four general tags de-fined above.
This is an empirically very well-foundeddistinction: the ICSI-MR group have provided someinter-annotator agreement figures(Carletta et al,1997) for a very similar task and report a kappaof 0.79.
Our ME classifier scored 77.9% (baseline54.0%).We also tested a few simple binary classificationsto see how predictable they are.
Utterances are anno-tated for example with a tag J if they are a joke.
Aswould be expected, the Joke/Non-Joke classificationproduced results not distinguishable from chance.The performance of the classifiers on separating dis-rupted utterances from non disrupted forms scoredslightly above chance at 89.9% (against baseline of87.0%).
We suspect that more sophisticated contex-tual features could allow better performance here.A more relevant performance criterion for our appli-cation is the accuracy of classification into the fourgeneral tags.
In this case we removed disrupted andundecipherable utterances, slightly reducing the sizeof the test set, and achieved a score of 84.9% (base-line 64.1%).With regard to the larger sets of tags, since theyhave some internal structure it should accordinglybe possible to identify the different parts separately,and then combine the results.
We have therefore per-formed some preliminary experiments with classifiersthat classify each level separately.
We again removedthe disruption tags since with out current frameworkwe are unable to predict them accurately.
The base-line for this task is again a classifier that chooses themost likely tag (S) which gives 41.9% accuracy.
Us-ing a single classifier on this complex task gave anaccuracy of 73.2%.We then constructed six classifiers as followsPrimary classifier S, H, Q or BPoliteness classifier PO or not POAttention classifier AT or not ATOrder classifier DO or not DORestatement classifier RI or not RIResponse classifier RP, RN, RU or no responseThese were trained separately in the obvious way andthe results combined.
This complex classifier gave anaccuracy 70.5%.
This mild decrease in performanceis rather surprising ?
one would expect the perfor-mance to increase as the data sets for each distinctionget larger.
This can be explained by dependences be-tween the classifications.
There are a number of waysthis could be treated ?
for example, one could use asequence of classifiers, where each classifier can usethe output of the previous classifier as a feature inthe next.
It is also possible that these dependenciesreflect idiosyncracies of the tagging process: tenden-cies of the annotators for whatever reasons to favouror avoid certain combinations of tags.6 ConclusionWe have discussed some issues concerning the designand use of dialogue act tagsets.
It is too early todraw firm conclusions from this preliminary study.We can note the obvious point that simplified smallertagsets are easier to predict accurately than largerones.
There appear to be non-trivial dependenciesbetween the tags for reasons that are not yet clear.We expect the performance of a final, fully automaticclassifier to be substantially higher than the resultspresented here, owing to the use of more powerfulclassifiers and, more importantly, larger and richerfeature sets.
Finally we note that an important pointof tagset design has not been addressed empiricallyhere: the question of whether particular distinctionsin the tagset are actually useful in our application.Future studies will address this point by studyingthe queries formulated by potential users of meetingprocessing and retrieval systems.AcknowledgmentsWe are grateful to the ICSI MR group for shar-ing with us the data as part of the IM2/ICSIagreement ?
in particular to Barbara Peskinand Liz Shriberg.
This research is part of theMultimodal Dialogue Management module (seehttp://www.issco.unige.ch/projects/im2/mdm)of the IM2 project.ReferencesJames F. Allen and Mark G. Core.
1997.
DAMSL:Dialog act markup in several layers (draft 2.1).Technical report, Multiparty Discourse Group,Discourse Research Initiative, September/October1997.Susan Armstrong, Alexander Clark, Giovanni Coray,Maria Georgescul, Vincenzo Pallotta, AndreiPopescu-Belis, David Portabella, Martin Rajman,and Marianne Starlander.
2003.
Natural languagequeries on natural language data: a database ofmeeting dialogues.
In NLDB?2003 (8th Inter-national Conference on Applications of NaturalLanguage to Information Systems), Burg/Cottbus,Germany.Sonali Bhagat, Hannah Carvey, and ElizabethShriberg.
2003.
Automatically generated prosodiccues to lexically ambiguous dialog acts in multi-party meetings.
In ICPhS 2003, Barcelona.Jean Carletta, Amy Isard, Stephen Isard, Jacque-line C. Kowtko, Gwyneth Doherty-Sneddon, andAnne H. Anderson.
1997.
The reliability of a di-alogue structure coding scheme.
ComputationalLinguistics, 23:13?31.Mark G. Core and James F. Allen.
1997.
Codingdialogues with the DAMSL annotation scheme.
InDavid Traum, editor, Working Notes: AAAI FallSymposium on Communicative Action in Humansand Machines, pages 28?35, Menlo Park, CA.American Association for Artificial Intelligence.Rajdip Dhillon, Sonali Bhagat, Hannah Carvey,and Elizabeth Shriberg.
2004.
Meeting recorderproject: Dialog act labeling guide.
TechnicalReport TR-04-002, ICSI (International ComputerScience Institute), Berkeley, CA.Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-asca.
1997.
Switchboard SWBD-DAMSL shallow-discourse-function annotation (coders manual,draft 13).
Technical Report 97-02, University ofColorado, Institute of Cognitive Science.Daniel Jurafsky, Elizabeth Shriberg, Barbara Fox,and Traci Curl.
1998.
Lexical, prosodic, and syn-tactic cues for dialog acts.
In ACL/COLING-98Workshop on Discourse Relations and DiscourseMarkers, pages 114?120.Marion Klein and Claudia Soria.
1998.
Dialogueacts.
In Marion Klein, Niels Ole Bernsen, SarahDavies, Laila Dybkjaer, Juanma Garrido, Hen-rik Kasch, Andreas Mengel, Vito Pirrelli, Mas-simo Poesio, Silvia Quazza, and Claudia Soria,editors, MATE Deliverable 1.1: Supported CodingSchemes, MATE (Multilevel Annotation, ToolsEngineering) European Project LE4-8370.Stephen C. Levinson.
1983.
Pragmatics.
CambridgeUniversity Press, Cambridge, UK.Agnes Lisowska.
2003.
Multimodal interface designfor the multimodal meeting domain: Preliminaryindications from a query analysis study.
Technicalreport, IM2.MDM, 11/2003.Christopher Manning and Dan Klein.
2003.
Opti-mization, maxent models, and conditional estima-tion without magic.
In Tutorial at HLT-NAACL2003 and ACL 2003.
ACL, Edmonton, Canada.Nelson Morgan, Don Baron, Sonali Bhagat, HannahCarvey, Rajdip Dhillon, Jane A. Edwards, DavidGelbart, Adam Janin, Ashley Krupski, BarbaraPeskin, Thilo Pfau, Elizabeth Shriberg, AndreasStolcke, and Chuck Wooters.
2003.
Meetingsabout meetings: research at ICSI on speech inmultiparty conversations.
In ICASSP 2003 (In-ternational Conference on Acoustics, Speech, andSignal Processing), Hong Kong, China.Andrei Popescu-Belis.
2003.
Dialogue act tagsets formeeting understanding: an abstraction based onthe DAMSL, Switchboard and ICSI-MR tagsets.Technical report, IM2.MDM, v1.1, 09/2003.John R. Searle.
1969.
Speech Acts.
Cambridge Uni-versity Press, Cambridge, UK.Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat,Jeremy Ang, and Hannah Carvey.
2004.
The ICSImeeting recorder dialog act (MRDA) corpus.
InProceedings of SIGDIAL ?04 (5th SIGdial Work-shop on Discourse and Dialog), Cambridge, MA.Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliz-abeth Shriberg, Rebecca Bates, Daniel Jurafsky,Paul Taylor, Rachel Martin, Marie Meteer, andCarol Van Ess-Dykema.
2000.
Dialogue act mod-eling for automatic tagging and recognition ofconversational speech.
Computational Linguistics,26(3):339?371.David R. Traum.
2000.
20 questions for dialogue acttaxonomies.
Journal of Semantics, 17(1):7?30.Daniel Vanderveken.
1990.
Meaning and speech acts.Cambridge University Press, Cambridge, UK.Britta Wrede and Elizabeth Shriberg.
2003.
Therelationship between dialogue acts and hot spotsin meetings.
In IEEE Speech Recognition and Un-derstanding Workshop, St. Thomas, U.S. VirginIslands.
