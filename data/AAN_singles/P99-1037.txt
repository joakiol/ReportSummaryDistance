Memory-Based Morphological AnalysisAntal van den  Bosch  and Wal ter  Dae lemansILK / Computational LinguisticsTilburg University{antalb,walter}@kub.nl}Abst rac tWe present a general architecture for efficientand deterministic morphological nalysis basedon memory-based learning, and apply it tomorphological nalysis of Dutch.
The systemmakes direct mappings from letters in contextto rich categories that encode morphologicalboundaries, syntactic lass labels, and spellingchanges.
Both precision and recall of labeledmorphemes are over 84% on held-out dictionarytest words and estimated to be over 93% in freetext.1 In t roduct ionMorphological analysis is an essential compo-nent in language ngineering applications rang-ing from spelling error correction to machinetranslation.
Performing a full morphologicalanalysis of a wordform is usually regarded as asegmentation f the word into morphemes, com-bined with an analysis of the interaction of thesemorphemes that determine the syntactic lassof the wordform as a whole.
The complexity ofwordform morphology varies widely among theworld's languages, but is regarded quite higheven in the relatively simple cases, such as En-glish.
Many wordforms in English and otherwestern languages contain ambiguities in theirmorphological composition that can be quite in-tricate.
General classes of linguistic knowledgethat are usually assumed to play a role in thisdisambiguation process are knowledge of (i) themorphemes of a language, (ii) the morphotac-tics, i.e., constraints on how morphemes are al-lowed to attach, and (iii) spelling changes thatcan occur due to morpheme attachment.State-of-the art systems for morphologicalanalysis of wordforms are usually based ontwo-level finite-state transducers (FSTS, Kosken-niemi (1983)).
Even with the availability ofsophisticated evelopment tools, the cost andcomplexity of hand-crafting two-level rules ishigh, and the representation of concatenativecompound morphology with continuation lexi-cons is difficult.
As in parsing, there is a trade-off between coverage and spurious ambiguity inthese systems: the more sophisticated the rulesbecome, the more needless ambiguity they in-troduce.In this paper we present a learning approachwhich models morphological analysis (includ-ing compounding) of complex wordforms as se-quences of classification tasks.
Our model,MBMA (Memory-Based Morphological Analy-sis), is a memory-based learning system (Stan-fill and Waltz, 1986; Daelemans et al, 1997).Memory-based learning is a class of induc-tive, supervised machine learning algorithmsthat learn by storing examples of a task inmemory.
Computational effort is invested ona "call-by-need" basis for solving new exam-ples (henceforth called instances) of the sametask.
When new instances are presented to amemory-based learner, it searches for the best-matching instances in memory, according to atask-dependent similarity metric.
When it hasfound the best matches (the nearest neighbors),it transfers their solution (classification, label)to the new instance.
Memory-based learn-ing has been shown to be quite adequate forvarious natural-language processing tasks suchas stress assignment (Daelemans et al, 1994),grapheme-phoneme conversion (Daelemans andVan den Bosch, 1996; Van den Bosch, 1997),and part-of-speech tagging (Daelemans et al,1996b).The paper is structured as follows.
First, wegive a brief overview of Dutch morphology inSection 2.
We then turn to a description ofMBMA in Section 3.
In Section 4 we present285the experimental outcomes of our study withMBMA.
Section 5 summarizes our findings, re-ports briefly on a partial study of English show-ing that the approach is applicable to other lan-guages, and lists our conclusions.2 Dutch  Morpho logyThe processes of Dutch morphology includeinflection, derivation, and compounding.
In-flection of verbs, adjectives, and nouns ismostly achieved by suffixation, but a circum-fix also occurs in the Dutch past participle (e.g.ge+werk+t as the past participle of verb werken,to work).
Irregular inflectional morphology isdue to relics of ablaut (vowel change) and tosuppletion (mixing of different roots in inflec-tional paradigms).
Processes of derivation inDutch morphology occur by means of prefixa-tion and suffixation.
Derivation can change thesyntactic lass of wordforms.
Compounding inDutch is concatenative (as in German and Scan-dinavian languages)' words can be strung to-gether almost unlimitedly, with only a few mor-photactic constraints, e.g., rechtsinformatica-toepassingen (applications of computer sciencein Law).
In general, a complex wordform inher-its its syntactic properties from its right-mostpart (the head).
Several spelling changes occur:apart from the closed set of spelling changes dueto irregular morphology, a number of spellingchanges is predictably due to morphologicalcontext.
The spelling of long vowels varies be-tween double and single (e.g.
ik loop, I run,versus wii Iop+en, we run); the spelling of root-final consonants can be doubled (e.g.
ik stop,I stop, versus wij stopp+en, we stop); there isvariation between s and z and f and v (e.g.
huis,house, versus huizen, houses).
Finally, betweenthe parts of a compound, a linking morphememay appear (e.g.
staat+s+loterij, state lottery).For a detailed iscussion of morphological phe-nomena in Dutch, see De Haas and Trommelen(1993).
Previous approaches to Dutch morpho-logical analysis have been based on finite-statetransducers (e.g., XEROX'es morphological n-alyzer), or on parsing with context-free wordgrammars interleaved with exploration of pos-sible spelling changes (e.g.
Heemskerk and vanHeuven (1993); or see Heemskerk (1993) for aprobabilistic variant).3 Applying memory-based learningto morphological ana lys i sMost linguistic problems can be seen as,context-sensitive mappings from one representation toanother (e.g., from text to speech; from a se-quence of spelling words to a parse tree; froma parse tree to logical form, from source lan-guage to target language, etc.)
(Daelemans,1995).
This is also the case for morphologi-cal analysis.
Memory-based learning algorithmscan learn mappings (classifications) if a suffi-cient number of instances of these mappings ispresented to them.We drew our instances from the CELEX lex-ical data base (Baayen et al, 1993).
CELEXcontains a large lexical data base of Dutch word-forms, and features a full morphological naly-sis for 247,415 of them.
We took each wordformand its associated analysis, and created task in-stances using a windowing method (Sejnowskiand Rosenberg, 1987).
Windowing transformseach wordform into as many instances as it hasletters.
Each example focuses on one letter,and includes a fixed number of left and rightneighbor letters, chosen here to be five.
Con-sequently, each instance spans eleven letters,which is also the average word length in theCELEX data base.
Moreover, we estimatedfrom exploratory data analysis that this con-text would contain enough information to allowfor adequate disambiguation.To illustrate the construction of instances,Table 1 displays the 15 instances derived fromthe Dutch example word abnormaliteiten (ab-normalities) and their associated classes.
Theclass of the first instance is "A+Da", whichsays that (i) the morpheme starting in a is anadjective ("A") 1, and (ii) an a was deleted atthe end ("+Da").
The coding thus tells thatthe first morpheme is the adjective abnorrnaal.The second morpheme, iteit, has class "N_A,".This complex tag indicates that when iteit at-taches right to an adjective (encoded by "A,"),the new combination becomes a noun ("N_").Finally, the third morpheme is en, which is aplural inflection (labeled "m" in CELEX).
Thisway we generated an instance base of 2,727,4621CELEX features ten syntactic tags: noun (N), adjec-tive (A), quantifier/numeral (Q), verb (V), article (D),pronoun (O), adverb (B), preposition (P), conjunction(C), interjection (J), and abbreviation (X).286instances.
Within these instances, 2422 differ-ent class labels occur.
The most frequently oc-curring class label is "0", occurring in 72.5% ofall instances.
The three most frequent non-nulllabels are "N" (6.9%), "V" (3.6%), and "m"(1.6%).
Most class labels combine a syntacticor inflectional tag with a spelling change, andgenerally have a low frequency.When a wordform is listed in CELEX as hav-ing more than one possible morphological la-beling (e.g., a morpheme may be N or V, theinflection -en may be plural for nouns or infini-tive for verbs), these labels are joined into am-biguous classes ("N/V") and the first generatedexample is labeled with this ambiguous class.Ambiguity in syntactic and inflectional tags oc-curs in 3.6% of all morphemes in our CELEXdata.The  memory-based  learning algorithm usedwithin MBMA is ml -m (Daelemans and Vanden Bosch, 1992; Dae lemans et al, 1997), anextension of IBI (Aha et al, 1991).
IBI-IG con-structs a data base of instances in memory  dur-ing learning.
New instances are classified byIBI-IG by matching them to all instances inthe instance base, and calculating with eachmatch  the distance between the new instanceX and the memory  instance Y, A (X~Y)  ----~-\]n W(fi)~(xi,yi),  where W(f i )  is the weight i----1of the ith feature, and 5(x~, Yi) is the distancebetween the values of the ith feature in in-stances X and Y.
When the values of the in-stance features are symbolic, as with our linguis-tic tasks, the simple overlap distance function5 is used: 5(xi,yi) = 0 i f  xi = Yi, else 1.
The(most frequently occurring) classification of thememory instance Y with the smallest A(X,  Y)is then taken as the classification of X.The weighting function W(f i )  computes foreach feature, over the full instance base, itsinformation gain, a function from informationtheory; cf.
Quinlan (1986).
In short, the infor-mation gain of a feature expresses its relativeimportance compared to the other features inperforming the mapping from input to classi-fication.
When information gain is used in thesimilarity function, instances that match on im-portant features are regarded as more alike thaninstances that match on unimportant features.In our experiments, we are primarily inter-ested in the generalization accuracy of trainedmodels, i.e., the ability of these models to usetheir accumulated knowledge to classify newinstances that were not in the training mate-rial.
A method that gives a good estimateof the generalization performance of an algo-rithm on a given instance base, is 10-fold cross-validation (Weiss and Kulikowski, 1991).
Thismethod generates on the basis of an instancebase 10 subsequent partitionings into a trainingset (90%) and a test set (10%), resulting in 10experiments.4 Exper iments :  MBMA of  Dutchword formsAs described, we performed 10-fold cross vali-dation experiments in an experimental matrixin which MBMA is applied to the full instancebase, using a context width of five left and rightcontext letters.
We structure the presentationof the experimental outcomes as follows.
First,we give the generalization accuracies on test in-stances and test words obtained in the exper-iments, including measurements of generaliza-tion accuracy when class labels are interpretedat lower levels of granularity.
While the lattermeasures give a rough idea of system accuracy,more insight is provided by two additional anal-yses.
First, precision and recall rates of mor-phemes are given.
We then provide predictionaccuracies of syntactic word classes.
Finally, weprovide estimations on free-text accuracies.4.1 Genera l i za t ion  accurac iesThe percentages of correctly classified test in-stances are displayed in the top line of Table 2,showing an error in test instances of about 4.1%(which is markedly better than the baseline r-ror of 27.5% when guessing the most frequentclass "0"), which translates in an error at theword level of about 35%.
The output of MBMAcan also be viewed at lower levels of granularity.We have analyzed MBMA's output at the threefollowing lower granularity levels:1.
Only decide, per letter, whether a seg-mentation occurs at that letter, and if so,whether it marks the start of a derivationalstem or an inflection.
This can be derivedstraightforwardly from the full-task classlabeling.2.
Only decide, per letter, whether a segmen-tation occurs at that letter.
Again, this can287instancenumber1234leftcontext- -  - a_ _ a b5 _ a b n6 a b n o7 b n o r8 n o r mo r m a10 r m a I11 rn a I i12131415a I i tI i t ei t e it e i tI fOCUSletter Iaa bb nn oo rr mm aa II ii tt ee ii tt ee nrightcontext TASKb n o r m A+Dan o r m a 0o r m a I 0r m a I i 0m a I i t 0a I i t e 0I i t e i 0i t e i t 0t e i t e N_A,e i t e n 0i t e n _ 0_ 0_ 0_ m_ 0t e n _e n nTable 1: Instances with morphological nalysis classifications derived from abnormaliteiten, ana-lyzed as \[abnormaal\]A\[iteit\]N_A,\[en\]m.be derived straightforwardly.
This task im-plements egmentation of a complex wordform into morphemes.3.
Only check whether the desired spellingchange is predicted correctly.
Because ofthe irregularity of many spelling changesthis is a hard task.The results from these analyses are displayedin Table 2 under the top line.
First, Ta-ble 2 shows that performance on the lower-granularity tasks that exclude detailed syntac-tic labeling and spelling-change prediction isabout 1.1% on test instances, and roughly 10%on test words.
Second, making the distinctionbetween inflections and other morphemes i  al-most as easy as just determining whether thereis a boundary at all.
Third, the relatively lowscore on correctly predicted spelling changes,80.95%, indicates that it is particularly hardto generalize from stored instances of spellingchanges to new ones.
This is in accordance withthe common linguistic view on spelling-changeexceptions.
When, for instance, a past-tenseform of a verb involves a real exception (e.g.,the past tense of Dutch brengen,  to bring, isbracht ) ,  it is often the case that this exception isconfined to generalize to only a few other exam-ples of the same verb (b rachten ,  gebracht )  andnot to any other word that is not derived fromthe same stem, while the memory-based learn-ing approach is not aware of such constraints.A post-processing step that checks whether theproposed morphemes are also listed in a mor-pheme lexicon would correct many of these er-rors, but has not been included here.4.2  P rec i s ion  and  recal l  of  morphemesPrecision is the percentage of morphemes pre-dicted by MBMA that is actually a morphemein the target analysis; recall is the percentageof morphemes in the target analysis that arealso predicted by MBMA.
Precision and recallof morphemes can again be computed at differ-ent levels of granularity.
Table 3 displays thesecomputed values.
The results show that bothprecision and recall of fully-labeled morphemeswithin test words are relatively low.
It comesas no surprise that the level of 84% recalledfully labeled morphemes, including spelling in-formation, is not much higher than the level of80% correctly recalled spelling changes (see Ta-ble 2).
When word-class information, type ofinflection, and spelling changes are discarded,precision and recall of basic segment types be-comes quite accurate: over 94%.288instances wordsclass labeling granularity labeling example % :t: % +full morphological nalysis \[abnormaai\]A\[iteit\]N_A,\[en\]m 95.88 0.04 64.63 0.24derivation/inflection \[abnormal\]deriv\[iteit\]deriv\[en\]in/l 98.83 0.02 89.62 0.17segmentation \[abnormal\]\[iteit\]\[en\] 98.97 0.02 90.69 0.02spelling changes +Da 80.95 0.40Table 2: Generalization accuracies in terms of the percentage of correctly classified test instancesand words, with standard eviations (+) of MBMA applied to full Dutch morphological nalysis andthree lower-granularity tasks derived from MBMA's full output.
The example word abnormaliteitenis shown according to the different labeling granularities, and only its single spelling change at thebottom line).precision recalltask variation (%) (%)full morphological nalysis 84.33 83.76derivation/inflection 94.72 94.07segmentation 94.83 94.18Table 3: Precision and recall of morphemes, de-rived from the classification output of MBMAapplied to the full task and two lower-granularity variations of Dutch morphologicalanalysis, using a context width of five left andright letters.4.3 Pred ic t ing  the syntact ic  class ofword formsSince MBMA predicts the syntactic label ofmorphemes, and since complex Dutch word-forms generally inherit their syntactic proper-ties from their right-most morpheme, MBMA'ssyntactic labeling can be used to predict thesyntactic lass of the full wordform.
When ac-curate, this functionality can be an asset in han-dling unknown words in part-of-speech taggingsystems.
The results, displayed in Table 4, showthat about 91.2% of all test words are assignedthe exact tag they also have in CELEX (includ-ing ambiguous tags such as "N/V" - 1.3% word-forms in the CELEX dataset have an ambiguoussyntactic tag).
When MBMA's output is alsoconsidered correct if it predicts at least one outof the possible tags listed in CELEX, the accu-racy on test words is 91.6%.
These accuraciescompare favorably with a related (yet strictlyincomparable) approach that predicts the wordclass from the (ambiguous) part-of-speech tagsof the two surrounding words, the first letter,and the final three letters of Dutch words, viz.71.6% on unknown words in texts (Daelemanset al, 1996a).
!syntactic lass correct test wordsprediction words (%) -4-!exact 91.24 0.21exact or among alternatives 91.60 0.21Table 4: Average prediction accuracies (withstandard deviations) of MBMA on syntacticclasses of test words.
The top line displays exactmatches with CELEX tags; the bottom line alsoincludes predictions that are among CELEX al-ternatives.4.4 Free text  es t imat ionAlthough some of the above-mentioned accu-racy results, especially the precision and recallof fully-labeled morphemes, eem not very high,they should be seen in the context of the testthey are derived from: they stem from held-outportions of dictionary words.
In texts sampledfrom real-life usage, words are typically smallerand morphologically ess complex, and a rela-tively small set of words re-occurs very often.It is therefore relevant for our study to havean estimate of the performance of MBMA onreal texts.
We generate such an estimate fol-lowing these considerations: New, unseen textis bound to contain a lot of words that are in the245,000 CELEX data base, but also some numberof unknown words.
The morphological naly-ses of known words are simply retrieved by thememory-based learner from memory.
Due tosome ambiguity in the class labeling in the database itself, retrieval accuracy will be somewhat289below 100%.
The morphological nalyses of un-known words are assumed to be as accurate aswas tested in the above-mentioned experiments:they can be said to be of the type of dictionarywords in the 10% held-out test sets of 10-foldcross validation experiments.
CELEX bases itswordform frequency information on word countsmade on the 42,380,000-words Dutch INL cor-pus.
5.06% of these wordforms are wordformtokens that occur only once.
We assume thatthis can be extrapolated to the estimate thatin real texts, 5% of the words do not occurin the 245,000 words of the CELEX data base.Therefore, a sensible estimate of the accura-cies of memory-based learners on real text is aweighted sum of accuracies comprised of 95% ofthe reproduction accuracy (i.e, the error on thetraining set itself), and 5% of the generalizationaccuracy as reported earlier.Table 5 summarizes the estimated generaliza-tion accuracy results computed on the resultsof MBMA.
First, the percentages of correct in-stances and words are estimated to be above98% for the full task; in terms of words, it is es-t imated that 84% of all words are fully correctlyanalyzed.
When lower-granularity classificationtasks are discerned, accuracies on words are es-timated to exceed 96% (on instances, less than1% errors are estimated).
Moreover, precisionand recall of morphemes on the full task areestimated to be above 93%.
A considerable sur-plus is obtained by memory retrieval in the es-timated percentage of correct spelling changes:93%.
Finally, the prediction of the syntactictags of wordforms would be about 97% accord-ing to this estimate.We briefly note that Heemskerk (1993) re-ports a correct word score of 92% on free texttest material yielded by the probabilistic mor-phological analyzer MORPA.
MORPA segmentswordforms, decides whether a morpheme is astem, an affix or an inflection, detects pellingchanges, and assigns a syntactic tag to the word-form.
We have not made a conversion of ouroutput to Heemskerk's (1993).
Moreover, aproper comparison would demand the same testdata, but we believe that the 92% correspondsroughly to our MBMA estimates of 97.2% correctsyntactic tags, 93.1% correct spelling changes,and 96.7% correctly segmented words.Estimatecorrect instances, full taskcorrect words, full task98.4%84.2%correct instances, derivation/inflection 99.6%correct words, derivation/inflection 96.7%correct instances, segmentationcorrect words, segmentation99.6%96.7%precision of fully-labeled morphemes 93.6%recall of fully-labeled morphemes 93.2%precision of deriv./intl, morphemes 98.5%recall of deriv./inft, morphemes 98.0%precision of segments 98.5%recall of segments 97.9%correct spelling changescorrect syntactic wordform t a ~Table 5: Estimations of accuracies on real text,derived from the generalization accuracies ofMBMA on full Dutch morphological analysis.5 Conc lus ionsWe have demonstrated the applicability ofmemory-based learning to morphological nal-ysis, by reformulating the problem as a classi-fication task in which letter sequences are clas-sifted as marking different ypes of morphemeboundaries.
The generalization performance ofmemory-based learning algorithms to the taskis encouraging, given that the tests are doneon held-out (dictionary) words.
Estimates offree-text performance give indications of highaccuracies: 84.6% correct fully-analyzed words(64.6% on unseen words), and 96.7% correctlysegmented and coarsely-labeled words (about90% for unseen words).
Precision and recallof fully-labeled morphemes i  estimated in realtexts to be over 93% (about 84% for unseenwords).
Finally, the prediction of (possibly am-biguous) syntactic classes of unknown word-forms in the test material was shown to be91.2% correct; the corresponding free-text es-timate is 97.2% correctly-tagged wordforms.In comparison with the traditional approach,which is not immune to costly hand-crafting andspurious ambiguity, the memory-based learningapproach applied to a reformulation ofthe prob-lem as a classification task of the segmentationtype, has a number of advantages:290?
it presupposes no more linguistic knowl-edge than explicitly present in the cor-pus used for training, i.e., it avoids aknowledge-acquisition bottleneck;?
it is language-independent, as it functionson any morphologically analyzed corpus inany language;?
learning is automatic and fast;?
processing is deterministic, non-recurrent(i.e., it does not retry analysis generation)and fast, and is only linearly related to thelength of the wordform being processed.The language-independence of the approachcan be illustrated by means of the following par-tial results on MBMA of English.
We performedexperiments on 75,745 English wordforms fromCELEX and predicted the lower-granularitytasks of predicting morpheme boundaries (Vanden Bosch et al, 1996).
Experiments yielded88.0% correctly segmented test words when de-ciding only on the location of morpheme bound-aries, and 85.6% correctly segmented test wordsdiscerning between derivational nd inflectionalmorphemes.
Both results are roughly compa-rable to the 90% reported here (but note thedifference in training set size).A possible limitation of the approach maybe the fact that it cannot return more thanone possible segmentation for a wordform.
E.g.the compound word kwartslagen can be inter-preted as either kwart+slagen (quarter turns)or kwarts+lagen (quartz layers).
The memory-based approach would select one segmentation.However, true segmentation ambiguity of thistype is very rare in Dutch.
Labeling ambigu-ity occurs more often (3.6% of all morphemes),and the current approach simply produces am-biguous tags.
However, it is possible for ourapproach to return distributions of possibleclasses, if desired, as well as it is possible to "un-pack" ambiguous labeling into lists of possiblemorphological nalyses of a wordform.
If, forexample, MBMA's output for the word bakken(bake, an infinitive or plural verb form, or bins,a plural noun) would be \[bak\]v/N\[en\]tm/i/m,then this output could be expanded unambigu-ously into the noun analysis \[bak\]N\[en\]m (plu-ral) and the two verb readings \[bak\]y\[en\]i (in-finitive) and \[bak\]y\[en\]tm (present ense plu-ral).Points of future research are comparisonswith other morphological analyzers and lem-matizers; applications of MBMA to other lan-guages (particularly those with radically differ-ent morphologies); and qualitative analyses ofMBMA's output in relation with linguistic pre-dictions of errors and markedness ofexceptions.AcknowledgementsThis research was done in the context ofthe "Induction of Linguistic Knowledge" (ILK)research programme, supported partially bythe Netherlands Organization for Scientific Re-search (NWO).
The authors wish to thank TonWeijters and the members of the Tilburg ILKgroup for stimulating discussions.
A demonstra-tion version of the morphological nalysis sys-tem for Dutch is available via ILK's homepagehttp : / / i l k .
kub.
nl.ReferencesD.
W. Aha, D. Kibler, and M. Albert.
1991.Instance-based learning algorithms.
MachineLearning, 6:37-66.R.
H. Baayen, R. Piepenbrock, and H. van Rijn.1993.
The CELEX lexical data base on CD-ROM.
Linguistic Data Consortium, Philadel-phia, PA.W.
Daelemans and A.
Van den Bosch.
1992.Generalisation performance of backpropaga-tion learning on a syllabification task.
InM.
F. J. Drossaers and A. Nijholt, editors,Proc.
of TWLT3: Connectionism and Nat-ural Language Processing, pages 27-37, En-schede.
Twente University.W.
Daelemans and A.
Van den Bosch.1996.
Language-independent da a-orientedgrapheme-to-phoneme conversion.
In J. P. H.Van Santen, R. W. Sproat, J. P. Olive, andJ.
Hirschberg, editors, Progress in SpeechProcessing, pages 77-89.
Springer-Verlag,Berlin.W.
Daelemans, S. Gillis, and G. Durieux.1994.
The acquisition of stress: a data-oriented approach.
Computational Linguis-tics, 20(3):421-451.W.
Daelemans, J. Zavrel, and P. Berck.1996a.
Part-of-speech tagging for Dutch withMBT, a memory-based tagger generator.
InK.
van der Meer, editor, Informatieweten-schap 1996, Wetenschappelijke bijdrage aan291de Vierde Interdisciplinaire Onderzoekscon-ferentie In,formatiewetenchap, ages 33-40,The Netherlands.
TU Delft.W.
Daelemans, J. Zavrel, P. Berck, and S. Gillis.1996b.
MBT: A memory-based part of speechtagger generator.
In E. Ejerhed and I. Dagan,editors, Proc.
of Fourth Workshop on VeryLarge Corpora, pages 14-27.
ACL SIGDAT.W.
Daelemans, A.
Van den Bosch, and A. Weij-ters.
1997.
IGwree: using trees for com-pression and classification in lazy learningalgorithms.
Artificial Intelligence Review,11:407-423,W.
Daelemans.
1995.
Memory-based lexical ac-quisition and processing.
In P. Steffens, ed-itor, Machine Translation and the Lexicon,Lecture Notes in Artificial Intelligence, pages85-98.
Springer-Verlag, Berlin.W.
De Haas and M. Trommelen.
1993.
Mor-,fologisch andboek van her Nederlands: Eenoverzicht van de woordvorming.
SDU, 'sGravenhage, The Netherlands.J.
Heemskerk and V. van Heuven.
1993.MORPA: A morpheme lexicon-based mor-phological parser.
In V. van Heuven andL.
Pols, editors, Analysis and synthesis o,fspeech; Strategic research towards high-qualityspeech generation, pages 67-85.
Mouton deGruyter, Berlin.J.
Heemskerk.
1993.
A probabilistic ontext-free grammar for disambiguation i  morpho-logical parsing.
In Proceedings of the 6th Con-ference of the EACL, pages 183-192.K.
Koskenniemi.
1983.
Two-level morphol-ogy: a general computational model -for word--form recognition and production.
Ph.D. the-sis, University of Helsinki.J.R.
Quinlan.
1986.
Induction of DecisionTrees.
Machine Learning, 1:81-206.T.
J. Sejnowski and C. S. Rosenberg.
1987.
Par-allel networks that learn to pronounce Englishtext.
Complex Systems, 1:145-168.C.
Stanfill and D. Waltz.
1986.
Towardmemory-based reasoning.
Communicationso,f the ACM, 29(12):1213-1228, December.A.
Van den Bosch, W. Daelemans, and A. Weij-ters.
1996.
Morphological analysis as classi-fication: an inductive-learning approach.
InK.
Ofiazer and H. Somers, editors, Proceed-ings of the Second International Con,ferenceon New Methods in Natural Language Pro-cessing, NeMLaP-P, Ankara, Turkey, pages79-89.A.
Van den Bosch.
1997.
Learning to pro-nounce written words: A study in inductivelanguage learning.
Ph.D. thesis, UniversiteitMaastricht.S.
Weiss and C. Kulikowski.
1991.
Computersystems that learn.
San Mateo, CA: MorganKaufmann.292
