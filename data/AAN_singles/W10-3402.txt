Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 2?11,Beijing, August 2010SemanticNet-Perception of Human PragmaticsAmitava Das1 and Sivaji Bandyopadhyay2Department of Computer Science and EngineeringJadavpur Universityamitava.santu@gmail.com1 sivaji_cse_ju@yahoo.com2AbstractSemanticNet is a semantic network oflexicons to hold human pragmaticknowledge.
So far Natural LanguageProcessing (NLP) research patronizedmuch of manually augmented lexiconresources such as WordNet.
But thesmall set of semantic relations likeHypernym, Holonym, Meronym andSynonym etc are very narrow to cap-ture the wide variations human cogni-tive knowledge.
But no such informa-tion could be retrieved from availablelexicon resources.
SemanticNet is theattempt to capture wide range of con-text dependent semantic inferenceamong various themes which humanbeings perceive in their pragmaticknowledge, learned by day to day cog-nitive interactions with the surroundingphysical world.
SemanticNet holdshuman pragmatics with twenty well es-tablished semantic relations for everypair of lexemes.
As every pair of rela-tions cannot be defined by fixed num-ber of certain semantic relation labelsthus additionally contextual semanticaffinity inference in SemanticNet couldbe calculated by network distance andrepresented as a probabilistic score.SemanticNet is being presently devel-oped for Bengali language.1 Historical MotivationSemantics (from Greek "??????????"
- seman-tikos) is the study of meaning, usually in lan-guage.
The word "semantics" itself denotes arange of ideas, from the popular to the highlytechnical.
It is often used in ordinary languageto denote a problem of understanding thatcomes down to word selection or connotation.We studied with various Psycholinguistics ex-periments to understand how human naturalintelligence helps to understand general se-mantic from nature.
Our study was to under-stand the human psychology about semanticsbeyond language.
We were haunting for theintellectual structure of the psychological andneurobiological factors that enable humans toacquire, use, comprehend and produce naturallanguages.
Let?s come with an example ofsimple conversation about movie between twopersons.Person A: Have you seen themovie ?No Man's Land??
Howis it?Person B: Although it isgood but you should see?The Hurt Locker?
?May be the conversation looks very casual,but our intension was to find out the directionof the decision logic on the Person B?s brain.We start digging to find out the nature of hu-man intelligent thinking.
A prolonged discus-sion with Person B reveals that the decisionlogic path to recommend a good movie was asthe Figure 1.
The highlighted red paths are theshortest semantic affinity distances of the hu-man brain.We call it semantic thinking.
Although thederivational path of semantic thinking is notsuch easy as we portrait in Figure 1 but wekeep it easier for understandability.
Actually ahuman try to figure out the closest semanticaffinity node into his pragmatics knowledge bynatural intelligence.
In the previous examplePerson B find out with his intelligence that NoMan's Land is a war movie and got Oscar2award.
Oscar award generally cracked by Hol-lywood movies and thus Person B start search-ing his pragmatics network to find out a moviefall into war genre, from Hollywood and maybe got Oscar award.
Person B finds out thename of a movie The Hurt Locker at nearerdistance into his pragmatics knowledge net-work which is an optimized recommendationthat satisfy all the criteria.
Noticeably Person Bdidn?t choice the other paths like Bollywood,Foreign movie etc.Figure 1: Semantic ThinkingAnd thus our aim was to develop a computa-tional lexicon structure for semantics as humanpragmatics knowledge.
We spare long time tofind out the most robust structure to representpragmatics knowledge properly and it shouldbe easy understandable for next level of searchand usability.We look into literature that probably directto the direction of our ideological thinking.
Wefound that in the year of 1996 Push Singh andMarvin Minsky proposed the field has shat-tered into subfields populated by researcherswith different goals and who speak very differ-ent technical languages.
Much has beenlearned, and it is time to start integrating whatwe've learned, but few researchers are widelyversed enough to do so.
They had a proposalfor how to do so in their ConceptNet work.They developed lexicon resources like Con-ceptNet (Liu and Singh, 2004).
ConceptNet-ConceptNet is a large-scale semantic network(over 1.6 million links) relating a wide varietyof ordinary objects, events, places, actions, andgoals by only 20 different link types, minedfrom corpus.The present task of developing SemanticNetis to capture semantic affinity knowledge ofhuman pragmatics as a lexicon database.
Weextend our vision from the human commonsense (as in ConceptNet) to human pragmaticsand have proposed semantic relations for everypair of lexemes that cannot be defined by fixednumber of certain semantic relation labels.Contextual semantic affinity inference in Se-manticNet could be calculated by network dis-tance and represented as a probabilistic score.SemanticNet is being presently developed forBengali language.2 Semantic RolesThe ideological study of semantic roles startedage old ago since Panini?s karaka theory thatassigns generic semantic roles to words in anatural language sentence.
Semantic roles aregenerally domain specific in nature such asFROM_DESTINATION,TO_DESTINATION,DEPARTURE_TIME etc.
Verb-specific se-mantic roles have also been defined such asEATER and EATEN for the verb eat.
Thestandard datasets that are used in various Eng-lish SRL systems are: PropBank (Palmer et al,2005), FrameNet (Fillmore et al, 2003) andVerbNet (Kipper et al, 2006).
These collec-tions contain manually developed well-trustedgold reference annotations of both syntacticand predicate-argument structures.PropBank defines semantic roles for eachverb.
The various semantic roles identified(Dowty, 1991) are Agent, patient or theme etc.In addition to verb-specific roles, PropBankdefines several more general roles that can ap-ply to any verb (Palmer et al, 2005).FrameNet is annotated with verb frame se-mantics and supported by corpus evidence.The frame-to-frame relations defined in Fra-meNet are Inheritance, Perspective_on, Sub-frame, Precedes, Inchoative_of, Causative_ofand Using.
Frame development focuses on pa-raphrasability (or near paraphrasability) ofwords and multi-words.VerbNet annotated with thematic roles referto the underlying semantic relationship be-tween a predicate and its arguments.
The se-mantic tagset of VerbNet consists of tags asagent, patient, theme, experiencer, stimulus,instrument, location, source, goal, recipient,benefactive etc.It is evident from the above discussions thatno adequate semantic role set exists that can bedefines across various domains.
Hence pro-3posed SemanticNet does not only rely on fixedtype of semantics roles as ConceptNet.
Forsemantic relations we followed the 20 relationsdefined in ConceptNet.
Additionally we pro-posed semantic relations for every pair of lex-icons cannot be defined by exact semantic roleand thus we formulated a probabilistic scorebased technique.
Semantic affinity in Seman-ticNet could be calculated by network distance.Details could be found in relevant Section 8.3 CorpusPresent SemanticNet has been developed forBengali language.
Resource acquisition is oneof the most challenging obstacles to work withelectronically resource constrained languageslike Bengali.
Although Bengali is the sixth1popular language in the World, second in Indiaand the national language in Bangladesh.There was another issue drive us long wayto find out the proper corpus for the develop-ment of SemanticNet.
As the notion is to cap-ture and store human pragmatic knowledge sothe hypothesis was chosen corpus should notbe biased towards any specific domain know-ledge as human pragmatic knowledge is notconstricted to any domain rather it has a widespread range over anything related to universeand life on earth.
Additionally it must be largerin size to cover mostly available general con-cepts related to any topic.
After a detail analy-sis we decided it is better to choose NEWScorpus as various domains knowledge like Pol-itics, Sports, Entertainment, Social Issues,Science, Arts and Culture, Tourism, Adver-tisement, TV schedule, Tender, Comics andWeather etc are could be found only in NEWScorpus.Statistics NEWSTotal no.
of news documents in thecorpus 108,305Total no.
of sentences in the corpus 2,822,737Avg no.
of sentences in a document 27Total no.
of wordforms in the corpus 33,836,736Avg.
no.
of wordforms in a document 313Total no.
of distinct wordforms in thecorpus 467,858Table 1:  Bengali Corpus Statistics1http://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakersFortunately such corpus development couldbe found in (Ekbal and Bandyopadhyay, 2008)for Bengali.
We obtained the corpus from theauthors.
The Bengali NEWS corpus consistedof consecutive 4 years of NEWS stories withvarious sub domains as reported above.
For thepresent task we have used the Bengali NEWScorpus, developed from the archive of a lead-ing Bengali NEWS paper 2  available on theWeb.
The NEWS corpus is quite larger in sizeas reported in Table 1.4 AnnotationFrom the collected document set 200 docu-ments have been chosen randomly for the an-notation task.
Three annotators (Mr. X, Mr. Yand Mr. Z) participated in the present task.Annotators were asked to annotate the themewords (topical expressions) which best de-scribe the topical snapshot of the document.The agreement of annotations among threeannotators has been evaluated.
The agreementsof tag values at theme words level is reportedin Table 2.Annotators X vs. Y X Vs. Z Y Vs. Z AvgPercentage 82.64% 71.78% 80.47% 78.3%All Agree 75.45%Table 2: Agreement of annotators at themewords level5 Theme IdentificationTerm Frequency (TF) plays a crucial role toidentify document relevance in Topic-BasedInformation Retrieval.
The motivation behinddeveloping Theme detection technique is thatin many documents relevant words may notoccur frequently or irrelevant words may occurfrequently.
Moreover for the lexicon affinityinference, topic or theme words are the onlystrong clue to start with.
The Theme detectiontechnique has been proposed to resolve theseissues to identify discourse level most relevantthematic nodes in terms of word or lexiconusing a standard machine learning technique.The machine learning technique used here isConditional Random Field (CRF)3.
The themeword detection has been defined as a sequence2http://www.anandabazar.com/3http://crfpp.sourceforge.net4labeling problem using various useful depend-ing features.
Depending upon the series of in-put features, each word is tagged as eitherTheme Word (TW) or Other (O).5.1 Feature OrganizationThe set of features used in the present taskhave been categorized as Lexico-Syntactic,Syntactic and Discourse level features.
Theseare listed in the Table 3 below and have beendescribed in the subsequent subsections.Types FeaturesLexico-SyntacticPOSFrequencyStemmingSyntactic Chunk Label Dependency Parsing DepthDiscourse LevelTitle of the DocumentFirst ParagraphTerm DistributionCollocationTable 3: Features5.2 Lexico-Syntactic Features5.2.1 Part of Speech (POS)It has been shown by Das and Bandyopadhyay,(2009), that theme bearing words in sentencesare mainly adjective, adverb, noun and verbsas other POS categories like pronoun, preposi-tion, conjunct, article etc.
have no relevancetowards thematic semantic of any document.The detail of the POS tagging system chosenfor the present task could be found in (Das andBandyopadhyay 2009).5.3 FrequencyFrequency always plays a crucial role in identi-fying the importance of a word in the docu-ment or corpus.
The system generates fourseparate high frequent word lists after functionwords are removed for four POS categories:adjective, adverb, verb and noun.
Word fre-quency values are then effectively used as acrucial feature in the Theme Detection tech-nique.5.4 StemmingSeveral words in a sentence that carry thematicinformation may be present in inflected forms.Stemming is necessary for such inflectedwords before they can be searched in appropri-ate lists.
Due to non availability of good stem-mers in Indian languages especially in Bengali,a stemmer based on stemming cluster tech-nique has been used as described in (Das andBandyopadhyay, 2010).
This stemmer analyz-es prefixes and suffixes of all the word formspresent in a particular document.
Words thatare identified to have the same root form aregrouped in a finite number of clusters with theidentified root word as cluster center.5.5 Syntactic Features5.5.1 Chunk LabelWe found that Chunk level information is verymuch effective to identify lexicon inferenceaffinity.
As an example:( 	)/NP ()/NP ()/NP()/JJP (?
)/SYMThe movies released by Sa-tyajit Roy are excellent.In the above example two lexicons?/release?
and ?/movie?
are collo-cated in a chunk and they are very much se-mantically neighboring in human pragmaticknowledge.
Chunk feature effectively used insupervised classifier.
Chunk labels are definedas B-X (Beginning), I-X (Intermediate) and E-X (End), where X is the chunk label.
In thetask of identification of Theme expressions,chunk label markers play a crucial role.
Fur-ther details of development of chunking sys-tem could be found in (Das and Bandyopad-hyay 2009).5.5.2 Dependency ParserDependency depth feature is very useful toidentify Theme expressions.
A particularTheme word generally occurs within a particu-lar range of depth in a dependency tree.
Themeexpressions may be a Named Entity (NE: per-son, organization or location names), a com-mon noun (Ex: accident, bomb blast, strike etc)or words of other POS categories.
It has beenobserved that depending upon the nature ofTheme expressions it can occur within a cer-tain depth in the dependency tree in the sen-tences.
A statistical dependency parser has5been used for Bengali as described in (Ghoshet al, 2009).5.6 Discourse Level Features5.6.1 Positional AspectDepending upon the position of the thematicclue, every document is divided into a numberof zones.
The features considered for eachdocument are Title words of the document, thefirst paragraph words and the words from thelast two sentences.
A detailed study was doneon the Bengali news corpus to identify theroles of the positional aspect features of a doc-ument (first paragraph, last two sentences) inthe detection of theme words.
The importanceof these positional features has been describedin the following section.5.6.2 Title WordsIt has been observed that the Title words of adocument always carry some meaningful the-matic information.
The title word feature hasbeen used as a binary feature during CRFbased machine learning.5.6.3 First Paragraph WordsPeople usually give a brief idea of their beliefsand speculations about any related topic ortheme in the first paragraph of the documentand subsequently elaborate or support theirideas with relevant reasoning or factual infor-mation.
Hence first paragraph words are in-formative in the detection of Thematic Expres-sions.5.6.4 Words From Last Two SentencesIt is a general practice of writing style thatevery document concludes with a summary ofthe overall story expressed in the document.We found that it is very obvious that everydocument ended with dense theme/topic wordsin the last two sentences.5.6.5 Term Distribution ModelAn alternative to the classical TF-IDF weight-ing mechanism of standard IR has been pro-posed as a model for the distribution of a word.The model characterizes and captures the in-formativeness of a word by measuring howregularly the word is distributed in a document.Thus the objective is to estimate  that measuresthe distribution pattern of the k occurrences ofthe word wi in a document d. Zipf's law de-scribes distribution patterns of words in an en-tire corpus.
In contrast, term distribution mod-els capture regularities of word occurrence insubunits of a corpus (e.g., documents, para-graphs or chapters of a book).
A good under-standing of the distribution patterns is useful toassess the likelihood of occurrences of a themeword in some specific positions (e.g., first pa-ragraph or last two sentences) of a unit of text.Most term distribution models try to character-ize the informativeness of a word identified byinverse document frequency (IDF).
In thepresent work, the distribution pattern of a wordwithin a document formalizes the notion oftheme inference informativeness.
This is basedon the Poisson distribution.
Significant Themewords are identified using TF, Positional andDistribution factor.
The distribution functionfor each theme word in a document is eva-luated as follows:( )1 11 1( ) / ( ) /n nd i i i i ii if w S S n TW TW n?
?= == ?
+ ??
?where n=number of sentences in a documentwith a particular theme word Si=sentence id ofthe current sentence containing the theme wordand Si-1=sentence id of the previous sentencecontaining the query term, iTW is the positionalid of current Theme word and 1iTW ?
is the posi-tional id of the previous Theme word.5.6.6 CollocationCollocation with other thematicwords/expressions is undoubtedly an importantclue for identification of theme sequence pat-terns in a document.
As we used chunk levelcollocation to capture thematic words (as de-scribed in 5.5.1) and in this section we are in-troducing collocation feature as inter-chunkcollocation or discourse level collocation withvarious granularity as sentence level, para-graph level or discourse level.6 Theme ClusteringTheme clustering algorithms partition a set ofdocuments into finite number of topic basedgroups or clusters in terms of themewords/expressions.
The task of document clus-tering is to create a reasonable set of clusters6for a given set of documents.
A reasonablecluster is defined as the one that maximizes thewithin-cluster document similarity and mini-mizes between-cluster similarities.
There aretwo principal motivations for the use of thistechnique in the theme clustering setting: effi-ciency, and the cluster hypothesis.The cluster hypothesis (Jardine and vanRijsbergen, 1971) takes this argument a stepfurther by asserting that retrieval from a clus-tered collection will not only be more efficient,but will in fact improve retrieval performancein terms of recall and precision.
The basic no-tion behind this hypothesis is that by separat-ing documents according to topic, relevantdocuments will be found together in the samecluster, and non-relevant documents will beavoided since they will reside in clusters thatare not used for retrieval.
Despite the plausibil-ity of this hypothesis, there is only mixed ex-perimental support for it.
Results vary consi-derably based on the clustering algorithm anddocument collection in use (Willett, 1988).
Weemploy the clustering hypothesis only tomeasure inter-document level thematic affinityinference on semantics.Application of the clustering technique tothe three sample documents results in the fol-lowing theme-by-document matrix, A, wherethe rows represent various documents and thecolumns represent the themes politics, sport,and travel.election cricket hotelA parliament sachin vacationgovernor soccer tourist?
??
?= ?
??
??
?The similarity between vectors is calculatedby assigning numerical weights to these wordsand then using the cosine similarity measure asspecified in the following equation., ,1, .Nk j k j i k i jis q d q d w w?
?
?
?=?
?= = ??
??
?
?
---- (1)This equation specifies what is known as thedot product between vectors.
Now, in general,the dot product between two vectors is not par-ticularly useful as a similarity metric, since it istoo sensitive to the absolute magnitudes of thevarious dimensions.
However, the dot productbetween vectors that have been length norma-lized has a useful and intuitive interpretation: itcomputes the cosine of the angle between thetwo vectors.
When two documents are identic-al they will receive a cosine of one; when theyare orthogonal (share no common terms) theywill receive a cosine of zero.
Note that if forsome reason the vectors are not stored in anormalized form, then the normalization canbe incorporated directly into the similaritymeasure as follows.Of course, in situations where the documentcollection is relatively static, it makes sense tonormalize the document vectors once and storethem, rather than include the normalization inthe similarity metric., ,12 2, ,1 1,Ni k i jik j N Ni k i ki iw ws q dw w?
?== =??
?=?
??
?
???
?
----(2)Calculating the similarity measure and usinga predefined threshold value, documents areclassified using standard bottom-up soft clus-tering k-means technique.
The predefined thre-shold value is experimentally set as 0.5 asshown in Table 4.ID Theme 1 2 31 (administration) 0.63 0.12 0.041  (good-government) 0.58 0.11 0.061  (society) 0.58 0.12 0.031  (law) 0.55 0.14 0.082  (research) 0.11 0.59 0.022  (college) 0.15 0.55 0.012	 (higher study) 0.12 0.66 0.013  (jehadi) 0.13 0.05 0.583  (mosque) 0.05 0.01 0.863 	  (New Delhi) 0.12 0.04 0.653 	 (Kashmir) 0.03 0.01 0.93Table 4: Five cluster centroids (mean j??
).A set of initial cluster centers is necessary inthe beginning.
Each document is assigned tothe cluster whose center is closest to the doc-ument.
After all documents have been as-signed, the center of each cluster is recom-puted as the centroid or mean ??
(where ?
?is7the clustering coefficient) of its members thatis ( )1/jj x cc x??
?
?= ?
.
The distance functionis the cosine vector similarity function.Table 4 gives an example of theme centroidsby the K-means clustering.
Bold words inTheme column are cluster centers.
Cluster cen-ters are assigned by maximum clustering coef-ficient.
For each theme word, the cluster fromTable 4 is still the dominating cluster.
For ex-ample, ??
has a higher membershipprobability in cluster1 than in other clusters.But each theme word also has some non-zeromembership in all other clusters.
This is usefulfor assessing the strength of association be-tween a theme word and a topic.
Comparingtwo members of the cluster2, ?	?
and?
	?, it is seen that ?
	?
is stronglyassociated with cluster2 (p=0.65) but it hassome affinity with other clusters as well (e.g.,p =0.12 with the cluster1).
This is a good ex-ample of the utility of soft clustering.
Thesenon-zero values are still useful for calculatingvertex weight during Semantic RelationalGraph generation.7 Semantic Relational GraphRepresentation of input text document(s) in theform of graph is the key to our design prin-ciple.
The idea is to build a document graphG=<V,E> from a given source documentd D?
.
At this preprocessing stage, text istokenized, stop words are eliminated, andwords are stemmed.
Thus, the text in eachdocument is split into fragments and eachfragment is represented with a vector of consti-tuent theme words.
These text fragments be-come the nodes V in the document graph.The similarity between two nodes is ex-pressed as the weight of each edge E of thedocument graph.
A weighted edge is added tothe document graph between two nodes if theyeither correspond to adjacent text fragments inthe text or are semantically related by themewords.
The weight of an edge denotes the de-gree of the semantic inference relationship.The weighted edges not only denote documentlevel similarity between nodes but also interdocument level similarity between nodes.
Thusto build a document graph G, only the edgeswith edge weight greater than some predefinedthreshold value are added to G, which basical-ly constitute edges E of the graph G.The Cosine similarity measure has beenused here.
In cosine similarity, each documentd is denoted by the vector ( )V d?
derived fromd, with each component in the vector for eachTheme words.
The cosine similarity betweentwo documents (nodes) d1 and d2 is computedusing their vector representations ( 1)V d?and( 2)V d?as equation (1) and (2) (Described inSection 6).
Only a slight change has been donei.e.
the dot product of two vec-tors ( 1) ( 2)V d V d?
??
is defined as1( 1) ( 2)MiV d V d=?
.The Euclidean length of d is defined tobe21( )MiidV=??
where M is the total number ofdocuments in the corpus.
Theme nodes withina cluster are connected by vertex, weight iscalculated by clustering co-efficient of thosetheme nodes.
Additionally inter cluster vertex-es are there.
Cluster centers are interconnectedwith weighted vertex.
The weight is calculatedby cluster distance as measured by cosine simi-larity measure as discussed earlier.To better aid our understanding of the auto-matically determined category relationships wevisualized this network using the Fruchterman-Reingold force directed graph layout algorithm(Fruchterman and Reingold, 1991) and theNodeXL network analysis tool (Smith et al,2009)4.
A theme relational model graph drawnby NoddeXL is shown in Figure 1.8 Semantic Distance MeasurementFinally generated semantic relational graph isthe desired SemanticNet that we proposed.Generated Bengali SemanticNet consist of al-most 90K high frequent Bengali lexicons.
Onlyfour categories of POS (noun, adjective, ad-verb and verb) considered for present genera-tion as reported in Section 5.2.1.
In the gener-ated Bengali SemanticNet al the lexicons areconnected with weighted vertex either directly4Available fromhttp://www.codeplex.com/NodeXL8Figure 1: Semantic Relational Graph by NodeXLor indirectly.
Semantic lexicon inference couldbe identified by network distance of any twonodes by calculating the distance in terms ofweighted vertex.
We computed the relevanceof semantic lexicon nodes by summing up theedge scores of those edges connecting the nodewith other nodes in the same cluster.
As clustercenters are also interconnected with weightedvertex so inter-cluster relations could be alsocalculated in terms of weighted network dis-tance between two nodes within two separateclusters.
As an example:Figure 2: Semantic Affinity GraphThe lexicon semantic affinity inference fromFigure 2 could be calculated as follows:0000( , )              ----(1) or=  ---(2)nkkd i jnmm kkcccvS w wkvlk=====????
?where ( , )d i jS w w =  semantic affinity dis-tance between two lexicons wi and wj.
Equa-tion (1) and (2) are for intra-cluster and inter-cluster semantic distance measure respectively.k=number of weighted vertex between twolexicons wi and wj.
vk is the weighted vertexbetween two lexicons.
m=number of clustercenters between two lexicons.
lc is the distancebetween cluster centers between two lexicons.For illustration of present technique let takean example:(Argentina, goal)= 0.5 0.3 0.42+=(Gun, goal)= 0.22 0.5 0.01 1?
?+ ??
??
?
=0It is evident from the previous example thatthe score based semantic distance can betterillustrate lexicon affinity between Argentinaand goal but is no lexicon affinity relation be-tween gun and goal.Instead of giving only certain semantic rela-tions like WordNet or ConceptNet the presentrelative probabilistic score based lexicon affin-ity distance based technique can represent bestacceptable solution for representing the humanpragmatic knowledge.
Not only ideologicallyrather the SemanticNet provide a good solutionto any type of NLP problem.
A detail analysisof Information retrieval system using Seman-ticNet is detailed in evaluation section.Although every lexicon pair cannot be la-beled by exact semantic role but we try to keepa few semantic roles to establish a crossroadfrom previous computational lexicon tech-niques to this new one.
These semantic rela-tions may be treated as a bridge to traverseSemanticNet by gathering knowledge fromother resources WordNet and ConceptNet.Approximately 22k (24% of overall Seman-ticNet) lexicons are tagged with appropriatesemantic roles by two processes as describedbelow.99 Semantic Role AssignmentTwo types of methods have been taken to as-sign pair wise lexicon semantic affinity rela-tions.
First one is derived from ConceptNet.
Inthe second technique sub-graph is identifiedconsisting of a nearest verb and roles are as-signed accordingly.9.1 Semantic Roles from ConceptNetA ConceptNet API5 written in Java has beenused to extract pair wise relation from Con-ceptNet.
A Bengali-English dictionary (ap-proximately 102119 entries) has been devel-oped using the Samsad Bengali-English dictio-nary6 used here for equivalent lookup of Eng-lish meaning of each Bengali lexicon.
Ob-tained semantic relations from ConceptNet forany lexicon English pair are assigned to sourceBengali pair lexicons.
As an example:(?Tree?,?Gree?)
(?!?,??
) OftenNear PartOf PropertyOf IsA9.2 Verb Sub-Graph IdentificationIt is an automatic process using manuallyaugmented list of only 220 Bengali verbs.
Thisprocess starts from any arbitrary node of anycluster and start finding any nearest verb with-in the cluster.
The system uses the manuallyaugmented list of verbs as partly reported inTable 5.Verb English Gloss Probable RelationsBe IsA!
Have CapableOf" Have CapableOfbn_aikaar Made MadeOf Live LocationOfTable 5: Semantic RelationsThe semantic relation labels attached withevery verb in the manually augmented list (asreported in Table 5) is then automatically as-signed between each pair of lexicons.5http://web.media.mit.edu/~hugo/conceptnet/6http://dsal.uchicago.edu/dictionaries/biswas_bengali/10 EvaluationIt is bit difficult to evaluate this type of lexiconresources automatically.
Manual validationmay be suggested as a better alternative but weprefer for a practical implementation basedevaluation strategy.For evaluation of Bengali SemanticNet it isused in Information Retrieval task using cor-pus from Forum for Information RetrievalEvaluation (FIRE) 7  ad-hoc mono-lingual in-formation retrieval task for Bengali language.Two different strategies have been taken.
Firsta standard IR technique with TF-IDF, zonalindexing and ranking based technique (Ban-dyopadhyay et al, 2008) has been taken.Second technique uses more or less same strat-egy along with query expansion technique us-ing SemanticNet (Although the term Seman-ticNet was not mentioned there) as a resource(Bhaskar et al, 2010).Only the following evaluation metrics havebeen listed for each run: mean average preci-sion (MAP), Geometric Mean Average Preci-sion (GM-AP), (document retrieved relevantfor the topic) R-Precision (R-Prec), Binary pre-ferences (Bpref) and Reciprical rank of toprelevant document (Recip_Rank).
The evalua-tion strategy follows the global standard asText Retrieval Conference (TREC)8 metrics.
Itis clearly evident from the system results asreported in Table 6 that SemanticNet is a betterway to solve lexicon semantic affinity.ScoresBengali IR usingIR SemanticNetMAP 0.0200 0.4002GM_AP 0.0004 0.3185R-Prec 0.0415 0.3894Bpref 0.0583 0.3424Recip_Rank 0.4432 0.6912Table 6: Information Retrieval using Seman-ticNetEvaluation result shows effectiveness of de-veloped SemanticNet in IR.
Further analysis7http://www.isical.ac.in/~clia/index.html8http://trec.nist.gov/10revealed that general query expansion tech-nique generally used WordNet synonyms as aresource.
But in reality ?$	?
and ?
??could not be clustered in one cluster thoughthey represent same semantic of ?heart?.
Firstone used in general context whereas the secondone used only in literature.
If there is anyproblem to understand Bengali let come withan example of English.
Conceptually "you"and "thy" could be mapped in same cluster asthey both represent the semantic of 2nd personbut in reality "thy" simply refers to theliterature of the great English poet Shakes-peare.
Standard lexicons cannot discriminatethis type of fine-grained semantic differences.11 Conclusion and Future TaskExperimental result of Information Retrievalusing SemanticNet proves it is a better solutionrather than any existing lexicon resources.
Thedevelopment strategy employs less human in-terruption rather a general architecture ofTheme identification or Theme Clusteringtechnique using easily extractable linguisticsknowledge.
The proposed technique could bereplicated for any new language.SemanticNet could be useful any kind of In-formation Retrieval technique, InformationExtraction technique, and topic based Summa-rization and we hope for newly identified NLPsub disciplines such as Stylometry or Author-ship detection and plagiarism detection etc.Our future task will be in the direction ofdifferent experiments of NLP as mentionedabove to profoundly establish the efficiency ofSemanticNet.
Furthermore we will try to de-velop SemanticNet for many other languages.ReferencesBandhyopadhyay S., Das A., Bhaskar P.. EnglishBengali Ad-hoc Monolingual Information Re-trieval Task Result at FIRE 2008.
In WorkingNote of Forum for FIRE-2008.Bhaskar P., Das A.,Pakray P.and BandyopadhyayS.(2010).
Theme Based English and Bengali Ad-hoc Monolingual Information Retrieval in FIRE2010, In FIRE-2010.Das A. and Bandyopadhyay S. (2009).
Theme De-tection an Exploration of Opinion Subjectivity.In Proceeding of Affective Computing & Intelli-gent Interaction (ACII).Das A. and Bandyopadhyay S. (2010).
Morpholog-ical Stemming Cluster Identification for Bangla,In Knowledge Sharing Event-1: Task 3: Mor-phological Analyzers and Generators, January,2010, Mysore.Ekbal A., Bandyopadhyay S (2008).
A Web-basedBengali News Corpus for Named Entity Recog-nition.
Language Resources and EvaluationJournal.
pages 173-182, 2008Fillmore Charles J., Johnson Christopher R., andPetruck Miriam R. L.. 2003.
Background toFrameNet.
International Journal of Lexicogra-phy, 16:235?250.Fruchterman Thomas M. J. and  Reingold EdwardM.(1991).
Graph drawing by force-directedplacement.
Software: Practice and Experience,21(11):1129?1164.Ghosh A., Das A., Bhaskar P., BandyopadhyayS.(2009).
Dependency Parser for Bengali: the JUSystem at ICON 2009.
In NLP Tool ContestICON 2009, December 14th-17th, Hyderabad.Jardine, N. and van Rijsbergen, C. J.
(1971).
Theuse of hierarchic clustering in information re-trieval.
Information Storage and Retrieval, 7,217-240.Kipper Karin, Korhonen Anna, Ryant Neville, andPalmer Martha.
Extending VerbNet with NovelVerb Classes.
LREC 2006.Liu Hugo and Singh Push (2004).
ConceptNet: apractical commonsense reasoning toolkit.
BTTechnology Journal, 22(4):211-226.Palmer Martha, Gildea Dan, Kingsbury Paul, TheProposition Bank: A Corpus Annotated withSemantic Roles, Computational LinguisticsJournal, 31:1, 2005.Singh Push and Williams William (2003).
LifeNet:a propositional model of ordinary human activi-ty.
In the Proc.
Of DC-KCAP 2003.Singh Push, Barry Barbara, and Liu Hugo (2004).Teaching machines about everyday life.
BTTechnology Journal, 22(4):227-240.Smith Marc, Ben Shneiderman, Natasa Milic-Frayling, Eduarda Mendes Rodrigues, VladimirBarash, Cody Dunne, Tony Capone, Adam Per-er, and Eric Gleave.
2009.
Analyzing (socialmedia) networks with NodeXL.
In C&T ?09:Proc.
Fourth International Conference on Com-munities and Technologies, LNCS.
Springer.Willerr, P. (1988).
Recent trends in hierarchic doc-ument clustering: A critical review.
InformationProcessing and Management, 24(5), 577-597.11
