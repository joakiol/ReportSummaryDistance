Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 593?600Manchester, August 2008Robust Similarity Measures for Named Entities MatchingErwan Moreau1Institut Te?le?com ParisTech& LTCI CNRSerwan.moreau@enst.frFranc?ois YvonUniv.
Paris Sud& LIMSI CNRSyvon@limsi.frOlivier Cappe?Institut Te?le?com ParisTech& LTCI CNRScappe@enst.frAbstractMatching coreferent named entities with-out prior knowledge requires good similar-ity measures.
Soft-TFIDF is a fine-grainedmeasure which performs well in this task.We propose to enhance this kind of met-rics, through a generic model in whichmeasures may be mixed, and show experi-mentally the relevance of this approach.1 IntroductionIn this paper, we study the problem of matchingcoreferent named entities (NE in short) in text col-lections, focusing primarily on orthographic vari-ations in nominal groups (we do not handle thecase of pronominal references).
Identifying textualvariations in entities is useful in many text min-ing and/or information retrieval tasks (see for ex-ample (Pouliquen et al, 2006)).
As described inthe literature (e.g.
(Christen, 2006)), textual dif-ferences between entities are due to various rea-sons: typographical errors, names written in dif-ferent ways (with/without first name/title, etc.
),abbreviations, lack of precision in organizationnames, transliterations, etc.
For example, onewants ?Mr.
Rumyantsev?
to match with ?Alexan-der Rumyanstev?
but not with ?Mr.
Ryabev?.Here we do not address the related problem of dis-ambiguation2 (e.g.
knowing whether a given oc-currence of ?George Bush?
refers to the 41st or43rd president of the USA), because it is techni-cally very different from the matching problem.c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1Now at LIPN - Univ.
Paris 13 & UMR CNRS 7030.2Which is essential in the Web People Search task.There are different ways to tackle the problemof NE matching: the first and certainly most reli-able one consists in studying the specific featuresof the data, and then use any available tool to de-sign a specialized method for the matching task.This approach will generally take advantage oflanguage-specific (e.g.
in (Freeman et al, 2006))and domain-specific knowledge, of any externalresources (e.g.
database, names dictionaries, etc.
),and of any information about the entities to pro-cess, e.g.
their type (person name, organization,etc.
), or internal structure (e.g.
in (Prager et al,2007)).
In such an in-depth approach, supervisedlearning is helpful: it has been used for examplein a database context3 in (Bilenko et al, 2003), butthis approach requires labeled data which is usu-ally costly.
All those data specific appproacheswould necessitate some sort of human expertise.The second approach is the robust one: wepropose here to try to match any kind of NE,extracted from ?real world?
(potentially noisy)sources, without any kind of prior knowledge4.One looks for coreferent NE, whatever their type,source, language5 or quality6.
Such robust simi-larity methods may be useful for a lot of generictasks, in which maximum accuracy is not the maincriterion, or simply where the required resourcesare not available.The literature on string comparison metrics isabundant, containing both general techniques and3The matching task is quite different in this framework,because one observes records (structured information).4In this kind of knowledge are included the need for hand-tuning parameters or defining specific thresholds.5Actually we have only studied English and French (ourapproach is neither ?multilingual?, in the sense that it is notspecific to multilingual documents).6In particular, this task clearly depends on the NE recog-nition step, which may introduce errors.593more linguistically motivated measures, see e.g.
(Cohen et al, 2003) for a review.
From a bird?s eyeview, these measures can be sorted in two classes:?Sequential character-based methods?
and ?Bag-of-words methods?7.
Both classes show relevantresults, but do not capture the same kind of simi-larity.
In a robust approach for NE matching, oneneeds a more fine-grained method, which performsat least as well as bag-of-words methods, withoutignoring coreferent pairs that such methods miss.A first attempt in this direction was introducedin (Cohen et al, 2003), in the form of a measurecalled Soft-TFIDF.
We will show that this measurehas theoretical pitfalls and a few practical draw-backs.
Nevertheless, Soft-TFIDF outperforms thebetter standard string similarity measures in theNE matching task.
That is why we propose to gen-eralize and improve its principle, and show exper-imentally that this approach is relevant.In section 2 we introduce standard similar-ity measures and enhance the definition of Soft-TFIDF.
Then we define a generic model in whichsimilarity measures may be combined (section 3).Finally, section 4 shows that experiments with twodifferent corpora validate our approach.2 Approximate matching methodsWe present below some of the main string similar-ity measures used to match named entities (Chris-ten, 2006; Cohen et al, 2003; Bilenko et al, 2003).2.1 Classical metrics2.1.1 Sequential character based methodsLevenshtein edit distance.
This well-known dis-tance metric d represents the minimum numberof insertions, deletions or substitutions needed totransform a string x into another string y.
For ex-ample, d(kitten, sitting) = 3 (k 7?
s, e 7?
i,?
7?
g).
The corresponding normalized similaritymeasure is defined as s = 1 ?
d/max(|x|, |y|).
Alot of variants and/or improvements exist (Navarro,2001), among which:?
Damerau.
One basic edit operation is added:a transposition consists in swapping twocharacters;?
Needleman-Wunch.
Basic edit operationcosts are parameterized: G is the cost of a gap7We omit measures based on phonetic similarity suchas Soundex, because they are language-specific and/or type-specific (person names).
(insertion or deletion), and there is a functioncost(c, c?)
which gives the cost of substitutingc with c?
for any pair of characters (c, c?
).Jaro metric (Winkler, 1999).
This measure isbased on the number and the order of commoncharacters.
Given two strings x = a1.
.
.
anandy = b1.
.
.
bm, let H = min(n,m)/2: aiis in com-mon with y if there exists bjin y such that ai= bjand i ?
H ?
j ?
i + H .
Let x?
= a?1.
.
.
a?n?(resp.
y?
= b?1.
.
.
b?m?)
be the sequence of charac-ters from x (resp.
y) in common with y (resp.
x),in the order they appear in x (resp.
y).
Any posi-tion i such that a?i6= b?iis called a transposition.Let T be the number of transpositions between x?and y?
divided by 2:Jaro(x, y) =13?
(|x?||x|+|y?||y|+|y?|?T|y?|)2.1.2 Bag-of-words methodsWith these methods, each NE is represented asa set of features (generally words or characters n-grams8).
Let X = {xi}1?i?nand Y = {yi}1?i?mbe the sets representing the entities x, y. Simplestmeasures only count the number of elements incommon9, e.g:Overlap(x, y) =|X ?
Y |min(|X|, |Y |)Some more subtle techniques are based on avector representation of entities x and y, whichmay take into account parameters that are arenot included in the sets themselves.
Let A =(a1, .
.
.
, a|?|) and B = (b1, .
.
.
, b|?|) be such vec-tors10, the widely used cosine similarity is:cos(A,B) =?|?|i=1aibi??|?|i=1a2i?
?|?|i=1b2iTraditionally, TF-IDF weights are used invectors (Term Frequency-Inverse Document Fre-quency).
In the NE case, this value represents theimportance each feature w (e.g.
word) has for anentity x belonging to the set E of entities:tf(w, x) =nw,x?w??
?nw?,x, idf(w) = log|E||{x ?
E|w ?
x}|,tfidf(w, x) = tf(w, x) ?
idf(w).with nw,xthe number of times w appears in x.Thus the similarity score is CosTFIDF(x, y) =Cos(A,B), where each ai(resp.
bi) in A (resp.
inB) is tfidf(wi, x) (resp.
tfidf(wi, y)).8In the remaining the term n-grams is always used forcharacters n-grams.9|E| denotes the number of elements in E.10?
is the vocabulary, containing all possible features.5942.2 Special measures for NE matchingExperiments show that sequential character-basedmeasures catch mainly coreferent pairs of long NEthat differ only by a few characters.
Bag-of-wordsmethods suit better to the NE matching problem,since they are more flexible about word order andposition.
But a lot of coreferent pairs can not beidentified by such measures, because of small dif-ferences between words: for example, ?DirectorElBaradei?
and ?Director-General ElBareidi?
isout of reach for such methods.
That is why ?sec-ond level?
measures are relevant: their principle isto apply a sub-measure sim?
to all pairs of wordsbetween the two NE and to compute a final scorebased on these values.
This approach is possiblebecause NE generally contain only a few words.Monge-Elkan measure belongs to this category:it simply computes the average of the better pairsof words according to the sub-measure:sim(x, y) =1nn?i=1mmaxj=1(sim?
(xi, yj)).But experiments show that Monge-Elkan doesnot perform well.
Actually, its very simple behav-ior favors too much short entities, because averag-ing penalizes a lot every non-matching word.A more elaborated measure is proposed in (Co-hen et al, 2003): Soft-TFIDF is intended preciselyto take advantage of the good results obtained withCosine/TFIDF, without automatically discardingwords which are not strictly identical.
The originaldefinition is the following: let CLOSE(?,X, Y )be the set of words w ?
X such that there ex-ists a word v ?
Y such that sim?
(w, v) > ?.
LetN(w, Y ) = max({sim?
(w, v)|v ?
Y }).
For anyw ?
CLOSE(?,X, Y ), letSw,X,Y= weight(w,X) ?
weight(w, Y ) ?N(w, Y ),where weight(w,Z) = tfidf(w,Z)?
?w?Ztfidf(w,Z)2.Finally,SoftTFIDF(X,Y ) =?w?CLOSE(?,X,Y )Sw,X,Y.This definition is not entirely correct, be-cause weight(w, Y ) = 0 if w /?
Y (in otherwords, w must appear in both X and Y , thusSoftTFIDF(X,Y ) would always be equal toCosTFIDF(X,Y )).
We propose instead the fol-lowing corrected definition, which corresponds tothe implementation the authors provided in thepackage SecondString11:11http://secondstring.sourceforge.netLet CLOSEST(?,w,Z) = {v ?
Z | ?v?
?
Z :sim?
(w, v) ?
sim?
(w, v?)
?
sim?
(w, v) > ?
}.SoftTFIDF(X,Y ) =?w?Xweight(w,X) ?
?w,Y,where ?w,Z= 0 if CLOSEST(?,w,Z) = ?, and?w,Z= weight(w?, Z) ?
sim?(w,w?)
otherwise,with12 w?
?
CLOSEST(?,w,Z).As one may see, SoftTFIDF relies on the sameprinciple than Monge-Elkan: for each word xiin the first entity, find a word yjin the secondone that maximizes sim?
(xi, yj).
Therefore, thesemeasures have both the drawback not to be sym-metric.
Furthermore, there is another theoreticalpitfall with SoftTFIDF: in Monge-Elkan, the fi-nal score is simply normalized in [0, 1] using theaverage among words of the first entity.
Accord-ing to the principle of the Cosine angle of TF-IDF-weighted vectors, SoftTFIDF uses both vec-tors norms.
However the way words are ?approx-imately matched?
does not forbid the matching ofa given word in the second entity twice: in thiscase, normalization is wrong because this word iscounted only once in the norm of the second vec-tor.
Consequently there is a potential overflow: ac-tually it is not hard to find simple examples wherethe final score is greater than 1, even if this case isunlikely with real NE and a high threshold ?.3 Generalizing Soft-TFIDF3.1 A unifying framework for similaritymeasuresWe propose to formalize similarity measures in thegeneric model below.
This model is intended todefine, compare and possibly mix different kindsof measures.
The underlying idea is simply thatmost measures may be viewed as a process follow-ing different steps: representation as a sequence offeatures13 (e.g.
tokenization), alignment and a wayto compute the final score.
We propose to define asimilarity measure sim through these three steps,each of them is modeled as a function14:Representation.
Given a set F of features, letfeatures(e) = ?a1, .
.
.
, an?
be a function that as-12If |CLOSEST(?, w, Z)| > 1, pick any such w?
in theset.
In the case of matching words between NE, this shouldalmost never happen.13We use the word feature for the sake of generality.14Of course, alternative definitions may be relevant.
In par-ticular one may wish to allow the alignment function to returna set of graphs instead of only one.
In the same way, one maywish to add a special vertex ?
to the graph, in order to repre-sent the fact that a feature is not matched by adding an edgebetween this feature and ?.595signs an (ordered) sequence of features to any en-tity e (ai?
F for any i).
Features may be of anykind (e.g.
characters, words, n-grams, or even con-textual elements of the entity) ;Alignment.
Given a function simF : F 2 7?
Rwhich defines similarity between any pair of fea-tures, let algn(?a1, .
.
.
, an?, ?a?1, .
.
.
, a?n??)
= Gbe a function which assigns a graph G to any pairof features sequences.
G = (V,E) is a bipartiteweighted graph where:?
The set of vertices is V = A ?
A?, whereA and A?
are the partitions defined as A ={v1, .
.
.
, vn} and A?
= {v?1, .
.
.
, v?n?}.
Eachvi(resp.
v?i) represents (the position of) thecorresponding feature ai(resp.
a?i) ;?
The set of weighted edges is E ={(vij, v?i?j, sj)}1?j?|E|, where vij?
A,v?i?j?
A?.
Weights sjgenerally depend onsimF(aij, a?i?j).Scoring.
Finally sim = score(G), where scoreassigns a real value (possibly normalized in [0, 1])to the alignment G.The representation step is not particularly origi-nal, since different kinds of representation have al-ready been used both with sequential methods and?bag-of-features?
methods.
However our modelalso entails an alignment step, which does not existwith bag-of-features methods.
Actually, the align-ment is implicit with such methods, and we willshow that making it visible is essential in the caseof NE matching.In the remaining of this paper we will only con-sider normalized metrics (scores belong to [0, 1]).3.2 Revisiting classical similarity measuresMeasures presented in section 2 may be definedwithin the model presented above.
This mod-elization is only intended to provide a theoreticalviewpoint on the measures: for all practical pur-poses, standard implementations are clearly moreefficient.
Below we do not detail the represen-tation step, because there is no difficulty with it,and also because it is interesting to consider thatany measure may be used with different kindsof features, as we will show in the next section.Let S = ?a1, .
.
.
, an?
= features(e) and S?
=?a?1, .
.
.
, a?n??
= features(e?)
for any pair of enti-ties (e, e?
).3.2.1 Levenshtein-like similarityThe function alignlev(S, S?)
is defined in thefollowing way: let Glevbe the set of all graphsG = (V,E) such that any pair of edges(vij, v?i?j, sj), (vik, v?i?k, sk) ?
E satisfies (ij<ik?
i?j< i?k) ?
(ij> ik?
i?j> i?k).
Thisconstraint ensures that the sequential order of fea-tures is respected15 , and that no feature may bematched twice.
In the simplest form of Leven-shtein16, simF (a, b) = 1 if a = b and 0 otherwise:for any (vij, v?i?j, sj) ?
E, sj= simF(aij, a?i?j).Letsim(G) = M ?ng?costg?|E|+?
(vij,v?i?j,sj)?Esj,where M = max(n, n?)
and ngis the number ofvertices that are not connected (i.e.
the number ofinserted or deleted words).
costg= 1 in the simpleLevenshtein form, but may be a parameter in theNeedleman-Wunch variant (gap cost).
In brief, theprinciple in this definition is to count the positionswhere no edit operation is needed: thus maximiz-ing sim(G) is equivalent to minimizing the cost ofan alignment:alignlev(S, S?)
= G, where G is any graph suchthat sim(G) = max({sim(G?)|G?
?
Glev}).Finally, the function scorelevis simply definedas scorelev(G) = sim(G)/max(n, n?).
It is nothard to see that this definition is equivalent to theusual one (see section 2): basically, the graph rep-resents the concept called trace in (Wagner andFischer, 1974), except that the cost function is ?re-versed?
to become a similarity function.Figure 1: Example of Levenshtein alignmentkittsittingen011101Suppose costg= 1:sim(G) = M ?ng?|E|+?ej?Esjsim(G) = 7 ?
1 ?
6 + 4sim(G) = 4scorelev(G) = 4/7.3.2.2 Bag of featuresFor all simple measures using only sets of fea-tures, the function alignbag(S, S?)
is defined inthe following way: let G be the set of all graphs15Constraints are a bit more complex for Damerau.16In the Needleman-Wunch variant, simF should dependon the cost function, e.g.
: simF (a, b) = 1?
cost(a, b).596G = (V,E) such that if (vij, v?i?j, sj) ?
E thenaij= a?i?j(equivalently simF (aij, a?i?j) = 1).
Nowlet once(G) be the set of all G ?
G such thatany pair of edges (vij, v?i?j, sj), (vik, v?i?k, sk) ?
Esatisfies ij6= ik?
i?j6= i?k(at most one matchfor each feature), and aij6= aik(a feature oc-curring several times is matched only once).
Letsim(G) =?
(vij,v?i?j,sj)?Esjfor any G = (V,E).alignbag(S, S?)
= G, where G is any graph suchthat sim(G) = max({sim(G?)
|G?
?
once(G)}).Since all weights are equal to 1, one may showthat sim(G) = |S ?
S?| for any G ?
once(G).Thus the score function is simply used for nor-malization, depending on the given measure: forexample, scoreoverlap(G) =sim(G)min(n, n?
).3.2.3 Soft-TFIDFThe case of Cosine measure with TFIDFweighted vectors is a bit different.
Here we definethe SoftTFIDF version: let algnsoft(S, S?)
be thegraph G = (V,E) defined as17 (vij, v?i?j, sj) ?
E ifand only if a?i?j= select(CLOSEST(?, aij, S?
)),where CLOSEST is the function defined in sec-tion 2 and select(E) is a function returning thefirst element in E if |E| > 0, and is undefinedotherwise18.
For any such edge, the weight sjissj= simF(aij, a?i?j) ?idf(aij)n?idf(a?i?j)n?.Once again, let sim(G) =?
(vij,v?i?j,sj)?Esj.scoresoft(G) = sim(G)/(?S|| ?
?S??
), where?
?a1, .
.
.
, an?|| =???
?n?i=1(idf(ai)n)2.Although it is not explicitly used in this defini-tion, term frequency is taken into account throughthe number of edges: suppose a given term t ap-pears m times in S and m?
times in S?, all m ver-tices corresponding to t in A (the partition repre-senting S) will be connected to all m?
vertices cor-responding to t in A?.
Thus there will be m ?
m?edges, which is exactly the unnormalized product17In the simple case of CosTFIDF, the condition would be:(vij, v?i?j, sj) ?
E if and only if aij= a?i?j.
In other words,all identical features (and only they) are connected.18?the first element?
means that select(E) may return anye ?
E, provided the same element is always returned for thesame set.of term frequencies tf(t, S) ?
tf(t, S?)
?n ?n?.
Thussumming m ?
m?
times idf(t)/n ?
idf(t)/n?
insim(G) is equal to tfidf(t, S) ?
tfidf(t, S?)
(nor-malization is computed in the same way).3.3 Meta-Levenshtein: Soft-TFIDF withLevenshtein alignmentWe have shown in part 2.2 that there are somepitfalls in Soft-TFIDF, especially in the way thealignment is computed: no symmetry, possiblescore overflow.
But experiments show that tak-ing words IDF into account increases performance,and that Soft-TFIDF, i.e.
the possible matchingof words that are not strictly identical, increasesperformance (see section 4).
That is why improv-ing this kind of measure is interesting.
Follow-ing the model we proposed above, we propose tomix the cosine-like similarity used in Soft-TFIDFwith a Levenshtein-like alignment.
The followingmeasure, called Meta-Levenshtein (ML for short),takes IDFs into account but is not a bag-of-featuresmetrics.Let us define alignMLin the following way: letGMLbe defined exactly as the set of graphs Glev(see part 3.2.1), except that weights are defined asin the case of Soft-TFIDF: for any G = (V,E) ?Glevand for any edge (vij, v?i?j, sj) ?
E, letsj= simF(aij, a?i?j) ?idf(aij)n?idf(a?i?j)n?.Let sim(G) =?
(vij,v?i?j,sj)?Esj, andalignML(S, S?)
= G, where G is such thatsim(G) = max({sim(G?)
|G??
GML}).
Finally,scoreML(G) = sim(G)/(?S|| ?
?S??
).Compared to Soft-TFIDF, ML solves the prob-lem of symmetry (ML(S, S?)
= ML(S?, S)), andalso the potential overflow, because no feature maybe matched twice (see fig.
2).
Of course, the align-ment is less flexible in ML, since it must satisfy thesequential order of features.
Practically, this mea-sure may be efficiently implemented in the sameway as Levenshtein similarity, including option-ally the Damerau extension for transpositions.
Wehave also tested a simple variant with possible ex-tended transpositions, i.e.
cases like ABC com-pared to CA, where both C and A are matched.3.4 Recursive combinations for NE matchingOne of the points we want to emphasize throughthe generic framework presented above is the mod-597Figure 2: Soft-TFIDF vs. ML alignmentWith sim(A,D) ?
?, and sim(C,E) ?
sim(B,E) ?
?
:ACBADEFSoft-TFIDFACBADEFMLularity of similarity measures.
Our viewpoint isthat traditional measures may be seen not only intheir original context, but also as modular param-eterized functions.
The first application of such adefinition is already in use in the form of measureslike Monge-Elkan or Soft-TFIDF, which rely onsome sub-measure to compare words inside NEs.But we will show that modularity is also usefulat a lower level: measures concerning words mayrely on similarity between (for example) n-grams,and even at this restricted level numerous possiblekinds of similarity may be used.Moreover, from the viewpoint of applications itis not very costly to compute similarities betweenn-grams and even between words.
The numberof n-grams is clearly bounded, and the number ofwords is not so high because there are only about 2words by entity in average, and overall some wordsappear very often in entities19.4 Experiments4.1 DataTwo corpora were used.
Both contain mainly newsand press articles, collected from various interna-tional sources.
The first one, called ?Iran Nu-clear Threat?
(INT in short), is in English andwas extracted from the NTI (Nuclear Threat Ini-tiative) web site20.
It is 236,000 words long.
Oursecond corpus, called ?French Speaking Medias?
(FSM in short), is 856,000 words long.
It was ex-tracted from a regular crawling of a set of French-speaking international newspapers web sites dur-ing a short time-frame (in July 2007).
GATE21was used as the named entities recognizer for INT,whereas Arisem22 performed the tagging of NEs19In the corpora we studied, 1172 NE (resp.
2533) contain1107 distinct words (resp.
2785).20http://www.nti.org21http://gate.ac.uk22http://www.arisem.comfor FSM.
Recognition errors23 appear in both cor-pora, but significantly less in FSM.
We restrictedthe sets of NEs to those recognized as locations,organizations and persons, and decided to workonly on entities appearing at least twice.
Finallyfor INT (resp.
FSM) we obtain 1,588 distinctNE (resp.
3,278) accounting altogether for 33,147(resp.
23,725) occurrences.Of course, it would be too costly to manuallylabel as match (positive) or non-match (negative)the whole set containing n ?
(n ?
1)/2 pairs, forthe observed values of n. The approach consist-ing in labeling only a randomly chosen subset ofpairs is ineffective, because of the disproportionbetween the number of negative and positive pairs(less than 0.1%).
Therefore we tried to find all pos-itive pairs, assuming the remaining lot are nega-tive.
Practically, the labeling step was based onlyon the best pairs as identified by a large set ofmeasures24.
The guidelines we used for labelingare the following: any incomplete, over-tagged orsimply wrongly recognized NE is discarded.
Thenremaining pairs are classified as positive (corefer-ent), negative (non-coreferent), or ?don?t know?25.Corpus Discarded Pos.
Neg.
Don?t knowINT 416 / 1,588 764 2,821 302FSM 745 / 3,278 741 32,348 419According to our initial hypotheses, all non-tagged pairs are considered as negative in the ex-periments below.
?Don?t know?
pairs are ignored.As a further note, about 20% of the pairs are notorthographically similar (e.g.
acronyms and theirexpansion): these pairs are out of reach of our tech-niques, and would require additional knowledge.4.2 Observations4.2.1 Taking IDF into accountTo evaluate the contribution of IDF26 in scor-ing the coreference degree between NE, let us ob-23Mainly truncated entities, over-tagged entities, and com-mon nouns beginning with a capital letter.24This is a potential methodological bias, but we hope tohave kept its effect as low as possible: the measures we usedare quite diverse and do not assign good scores to the samepairs; therefore, for each measure, we expect that the poten-tial misses (false negatives) will be matched by some othermeasure, thus allowing a fair evaluation of its performance.A few positive pairs are manually added (mainly acronyms).25All ambiguous cases, mainly due to some missing preci-sion (e.g.
?Ministry of Foreign Affairs?
and ?Russian Min-istry of Foreign Affairs?
), and more rarely homonymy (e.g.?Lebedev?
and ?
[Valery|Oleg] Lebedev?
)26It may be noticed that the Term Frequency in TFIDF israrely important, since a given word appear almost alwaysonly once in a NE.598serve the differences among best scored pairs formeasures Bag-of-words Cosine and Cosine overTFIDF weighted vectors.
For example, the for-mer will assign 0.5 to pair ?Prime Minister TonyBlair?/?Blair?
(from corpus INT), whereas thelatter gives 0.61.
As expected, IDF weights lightenthe effect of non-informative words and strengthenimportant words.
In both corpora, The F1-measurefor TFIDF Cosine is about 10 points (in average)better than for Bag-of-words Cosine (see fig.
3).4.2.2 Soft-TFIDF problems: normalization,threshold and sub-measureAs we have explained in section 2.2, the Soft-TFIDF measure (Cohen et al, 2003) may sufferfrom normalization problems.
This is probablythe reason why the authors seem to use it parsi-moniously, i.e.
only in the case words are veryclose (which is verified using a high threshold?).
Indeed, problems occur when the sub-measureand/or the threshold are not carefully chosen, caus-ing performances drop: using Jaro measure witha very low threshold (0.2 here), performancesare even worst than Bag-of-words cosine (see fig.3).
This is due to the double matching problem:for example, pair ?Tehran Times (Tehran)?/?InterPress Service?
(from INT) is scored more than 1.0because ?Tehran?
matches ?Inter?
twice: evenwith a low score as a coefficient, ?Inter?
has ahigh IDF compared to ?Press?
and ?Service?, socounting it twice makes normalization wrong.However, this problem may be solved by choos-ing a more adequate sub-measure: experimentsshow that using the CosTFIDF measure with bi-grams or trigrams outperforms standard CosT-FIDF.
Of course, there are some positive pairsthat are found ?later?
by Soft-TFIDF, since it mayonly increase score.
But the ?soft?
comparisonbrings back to the top ranked pairs a lot of positiveones.
In both corpora, the best sub-measure foundis CosTFIDF with trigrams.
?Mohamed ElBa-radei?/?Director Mohammad ElBaradei?
(INT)or ?Chine?/?China?
(FSM) are typical positivepairs found by this measure but not by standardCosTFIDF.
Here no threshold is needed anymorebecause the sub-measure has been chosen withcare, depending on the data, in order to avoid thenormalization problem.
This is clearly a drawbackfor Soft-TFIDF: it may perform well, but only withhand-tuning sub-measure and/or threshold.4.2.3 Beyond Soft-TFIDF: (recursive) MLIn the FSM corpus, replacing Soft-TFIDF with(simple) Meta-Levenshtein at the word level doesnot decrease performance, even though the align-ment is more constrained in the latter case.
Us-ing the same sub-measure to compare words (tri-grams CosTFIDF), it does neither increase perfor-mance.
A few positive pairs are missed in the INTcorpus, due to the more flexible word order in En-glish: ?U.S.
State Department?/?US Departmentof State?
is such an example (12 among 764 areconcerned).
This problem is easily solved with theML variant with extended transposition (see part3.3): in both corpora, there are no positive pairsrequiring more than a gap of one word in the align-ment.
Thus this measure is not only performant butalso robust, since it does not need any hand-tuning.As a second step, we want to improve resultsby selecting a more fine-grained sub-measure.
Wehave tried several ideas, such as using differentkinds of n-grams similarity inside the words sim-ilarity measure.
Firstly, trigrams performed bet-ter than bigrams or simple characters.
Secondly,the best trigrams similarity method found is actu-ally very simple: it consists in using CosTFIDFcomputed on the trigrams contexts, i.e.
the set ofclosest27 trigrams of all occurrences of the giventrigram.
Unsurprisingly, good scores are generallyobtained for pairs of trigrams that have commoncharacters.
But it seems that this approach alsoenhances robustness, because it finds similaritiesbetween ?close characters?
: in the French corpus,one observes quite good scores between trigramscontaining an accentuated version and the non ac-centuated version of the same character.
Further-more, some character encoding errors are some-how corrected this way28.
This is possibly the rea-son why the improvement of results is better inFSM than in INT (see table 1).Finally, using also ML to compute similaritybetween words29 yields the best results.
Thismeans that compared to the simple CosTFIDF sub-measure, one does not compare bags of trigramsbut ordered sequences of trigrams30.27We have tried different window sizes for such contexts,from 2 to 10 trigrams long: performances were approximatelythe same.
We only consider trigrams found in the entities.28For example, the ??
in the name ?Lugovo???
appears also inFSM as i, as y, as a`, and is sometimes deleted.29i.e.
not only between sequences of words: in this caseML is run between trigrams at the word level, and then an-other time between words at the NE level.30It is hard to tell whether it is the sequential alignment or599Figure 3: F1-Measures for FSM (percentages)0204060801000  500  1000  1500  2000  2500F1-Measuren best scored pairs (considered as positive)Bag of words CosineCosine TFIDF (words)Soft-TFIDF (Jaro)Soft-TFIDF (TFIDF 3g)ML (ML/contexts 3g)Example: for Cosine TFIDF with words, if the threshold isset in such a way that (only) the 1000 top ranked pairs areclassified as positive, then the F1-measure is around 60%.Table 1: Best F1-measures (percentages)INT FSMMeasure F1 P R F1 P RCosine 51.6 63.2 43.6 59.5 76.2 48.7CosTFIDF 62.6 71.7 55.6 69.9 84.2 59.8Soft TFIDF/3g 68.6 74.2 63.9 73.1 79.8 67.6ML/ML-context 70.6 72.6 68.7 77.0 82.5 72.2P/R: Corresponding Precision/Recall.4.3 Global resultsResults are synthesized in table 1, which is basedon the maximum F1-measure for each measure.One observes that F1-measure is 3 to 6 points bet-ter for Soft-TFIDF than for standard TF-IDF, andthat our measure still increases F1-measure by 2(INT) to 4 points (FSM).
Results show that itscontribution consists mainly in improving the re-call, which means that our measure is able to catchmore positive pairs than Soft-TFIDF: for exam-ple, the pair ?Fatah Al Islam?/ ?Fateh el-Islam?
(FSM) is scored 0.54 by SoftTFIDF and 0.70 byML.
Our measure remains the best for all values ofn in fig.
3, and results are similar for F0.5-measureand F2-measure: thus, irrespective of specific ap-plication needs which may favor precision or re-call, ML seems preferable.5 ConclusionIn conclusion, we have proposed a generic modelto show that similarity measures may be combinedin numerous ways.
We have tested such a combi-nation, based on Soft-TFIDF, which performs bet-the ?right?
use of the trigrams sub-measure which is responsi-ble for the improvement, since the only possible comparisonat this level is Soft-TFIDF.ter than all existing similarity metrics on two cor-pora.
Our measure is robust, since it does not relyon any kind of prior knowledge.
Thus it may beeasily used, in particular in applications where NEmatching is useful but is not the essential task.AcknowledgementsThis work has been funded by the National ProjectCap Digital - Infom@gic.
We thank Lo?
?s Rigouste(Pertimm) and Nicolas Dessaigne and Aure?lie Mi-geotte (Arisem) for providing us with the anno-tated French corpus.ReferencesBilenko, Mikhail, Raymond J. Mooney, William W.Cohen, Pradeep Ravikumar, and Stephen E. Fien-berg.
2003.
Adaptive name matching in informationintegration.
IEEE Intelligent Systems, 18(5):16?23.Christen, Peter.
2006.
A comparison of personal namematching: Techniques and practical issues.
Techni-cal Report TR-CS-06-02, Department of ComputerScience, The Australian National University, Can-berra 0200 ACT, Australia, September.Cohen, William W., Pradeep Ravikumar, andStephen E. Fienberg.
2003.
A comparison ofstring distance metrics for name-matching tasks.
InKambhampati, Subbarao and Craig A. Knoblock,editors, Proceedings of IJCAI-03 Workshop onInformation Integration on the Web (IIWeb-03),August 9-10, 2003, Acapulco, Mexico, pages 73?78.Freeman, Andrew, Sherri L. Condon, and ChristopherAckerman.
2006.
Cross linguistic name matchingin English and Arabic.
In Moore, Robert C., Jeff A.Bilmes, Jennifer Chu-Carroll, and Mark Sanderson,editors, Proc.
HLT-NAACL.Navarro, Gonzalo.
2001.
A guided tour to approximatestring matching.
ACM Comput.
Surv., 33(1):31?88.Pouliquen, Bruno, Ralf Steinberger, Camelia Ignat,Irina Temnikova, Anna Widiger, Wajdi Zaghouani,and Jan Zizka.
2006.
Multilingual person namerecognition and transliteration.
CORELA - Cogni-tion, Representation, Langage.Prager, John, Sarah Luger, and Jennifer Chu-Carroll.2007.
Type nanotheories: a framework for termcomparison.
In Proceedings of CIKM ?07, pages701?710, New York, NY, USA.
ACM.Wagner, R. and M. Fischer.
1974.
The string-to-stringcorrection problem.
JACM, 21(1):168?173.Winkler, W. E. 1999.
The state of record linkageand current research problems.
Technical ReportRR99/04, US Bureau of the Census.600
