A Simple Hybrid Aligner for Generating Lexical Correspondences inParallel TextsLars AHRENBERG, Mikael ANDERSSON & Magnus MERKELNLPLAB, Department of Computer and Information ScienceLinktping University, S-581 83 Linktping, Swedenlah@ida.liu.se, mikander@hotmail.com, magme@ida.liu.seAbstractWe present an algorithm for bilingual wordalignment that extends previous work bytreating multi-word candidates on a par withsingle words, and combining some simpleassumptions about the translation process tocapture alignments for low frequency words.As most other alignment algorithms it uses co-occurrence statistics as a basis, but differs inthe assumptions it makes about the translationprocess.
The algorithm has been implementedin a modular system that allows the user toexperiment with different combinations andvariants of these assumptions.
We giveperformance results from two evaluations,which compare well with results reported inthe literature.Introduct ionIn recent years much progress have been made inthe area of bilingual alignment for the support oftasks such as machine translation, machine-aidedtranslation, bilingual lexicography andterminology.
For instance, Melamed (1997a)reports that his word-to-word model fortranslational equivalence produced lexicon entrieswith 99% precision and 46% recall when trainedon 13 million words of the Hansard corpus, whererecall was measured as the fraction of words fromthe bitext that were assigned some translation.Using the same model but less data, aFrench/English software manual of 400,000words, Resnik and Melamed (1997) reported 94%precision with 30% recall.While these figures are indeed impressive, moretelling figures can only be obtained by measuringthe effect of the alignment system on somespecific task.
Dagan and Church (1994) reportsthat their Termight system helped double the speedat which terminology lists could be compiled atthe AT&T Business Translation Services.It is also clear that the usability of bilingualconcordances would be greatly improved if thesystem could indicate both items of a translationpair and if phrases could be looked up with thesame ease and precision as single words(Macklovitch and Hannan 1996).For the language pairs that are of particularinterest to us, English vs. other Germaniclanguages, the ability to handle multi-word unitsadequately is crucial (cf.
Jones and Alexa 1997).In English a large number of technical terms aremulti-word compounds, while the correspondingterms in other Germanic languages are oftensingle-word compounds.
We illustrate with a fewexamples from an English/Swedish computermanual:Table 1.
Equivalent compounds in anEnglish/Swedish bitext.Englishfrle managernetwork serveroperating systemsetup directorySwedishftlhanteraren~itverksserveroperativsysteminstallationskatalogAlso, many common adverbials and prepositionsare multi-word units, which may or may not betranslated as such.Table 2.
Equivalent adverbials and prepositionsEnglish Swedishafter all n~ir allt kommer,omkringtrots in spite ofin general i allm~inhet1.
The ProblemThe problem we consider is how to find word andphrase alignments for a bitext that is alreadyaligned at the sentence level.
Results should bedelivered in a form that could easily be checkedand corrected by a human user.Although we primarily use the system for29bitexts with an English and a Scandinavian half,the system should preferably be useful for manydifferent language pairs.
Thus we don~ rely on theexistence of POS-taggers or lemmatizers for thelanguages involved, but wish to providemechanisms that a user can easily adapt to newlanguages.The organisation of the paper is as follows: Insection 2 we relate this approach to previous work,in section 3 we motivate and spell out ourassumptions about the behaviour of lexical units intranslation, in section 4 we present the basicfeatures of the algorithm, and in section 5 wepresent results from an evaluation and try tocompare these to the results of others.2.
P rev ious  workMost algorithms for bilingual word alignment odate have been based on the probabilistictranslation models first proposed by Brown et al(1988, 1990), especially Model I and Model 2.These models explicitly exclude multi-word unitsfrom consideration 1.
Melamed (1997b), however,proposes a method for the recognition of multi-word compounds in bitexts that is based on thepredictive value of a translation model.
A trialtranslation model that treat certain multi-wordsequences as units is compared with a basetranslation model that treats the same sequences asmultiple single-word units.A drawback with Melamed's method is thatcompounds are defined relative to a giventranslation and not with respect to language-internal criteria.
Thus, if the method is used toconstruct a bilingual concordance, there is a riskthat compounds and idioms that translatecompositionally will not be found.
Moreover, it iscomputationally expensive and, since it constructscompounds incrementally, adding one word at atime, requires many iterations and muchprocessing to find linguistic units of the propersize.Kitamura and Matsumoto (1996) present resultsfrom aligning multi-word and single wordexpressions with a recall of 80 per cent if partiallycorrect ranslations were included.
Their method isiterative and is based on the use of the Dicecoefficient.
Smadja et.
al (1996) also use the DiceModel 3-5 includes multi-word units in one direction.coefficient as their basis for aligning collocationsbetween English and French.
Their evaluationshow results of 73 per cent accuracy (precision) onaverage.3.
Underlying assumptionsAs Fung and Church (1994) we wish to estimatethe bilingual lexicon directly.
Unlike Fung andChurch our texts are already aligned at sentencelevel and the lexicon is viewed, not merely asword associations, but as associations betweenlexical units of the two languages.We assume that texts have structure at manydifferent levels.
At the most concrete level a text issimply a sequence of characters.
At the next levela text is a sequence of word tokens, where wordtokens are defined as sequences of alphanumericcharacter strings that are separated from oneanother by a finite set of delimiters uch as spacesand punctuation marks.
While many characters canbe used either as word delimiters or as non-delimiters, we prefer to uphold a consistentdifference between delimiters and non-delimiters,for the ease of implementation that it allows.
Atthe same time, however, the tokenizer ecognizescommon abbreviations with internal punctuationmarks and regularizes clitics to words (e.g.
can't isregularized to can not).At the next level up a text can be viewed as apartially ordered bag of lexical units.
It is a bagbecause the same unit can occur several times in asingle sentence.
It is partially ordered because alexical unit may extend across other lexical units,as inHe turned the offer down.Tabs were kept on him.We say that words express lexical units, and thatunits are expressed by words.
A unit may beexpressed by a multi-word sequence, while a givenword can express at most one lexical unit.
2It is often hard to tell the difference between alexical unit and a lexical complex.
We assume that2 This latter assumption is actually too strict forGermanic languages where morphologicalcompounding is a productive process, but we make itnevertheless, as we have no means too identifycompounds reliably.
Moreover, the borderline betweena lexicalized compound and a compositional compoundis hard to draw consistently, anyway.30recurrent collocations that pass certain structuraland contextual tests are candidate xpressions forlexical units.
If such collocations are found tocorrespond to something in the other half of thebitext on the basis of co-occurrence measures, theyare regarded as expressions of lexical units.
Thiswill include compound names uch as New York"~enry Kissinger' and ~World War II" andcompound terms such as 'network serverdirectory'.
Thus, as with the compositionalcompounds just discussed, we prefer high recall tohigh precision in identifying multi-word units.The expressions of a lexical unit form anequivalence class.
An equivalence class for asingle-word unit includes its morphologicalvariants.
An equivalence class for a multi-wordunit should include syntactic variants as well.
Forinstance, the lexical unit turn down should includep~urned own' ~urning down' as well as expressionswhere the particle is separated from the verb bysome appropriate phrase, as in the example above.The current system, though, does not provide forsyntactic variants.Our aim is to establish relations not onlybetween corresponding words and word sequencesin the bitext, but also between correspondinglexical units.
A problem is then that the algorithmcannot recognize lexical units directly, but onlytheir expressions.
It helps to include lexical unitsin the underlying model, however, as they haveexplanatory value.
Moreover, the algorithm can bemade to deliver its output in the form ofcorrespondences between equivalence classes ofexpressions belonging to the same lexical unit.For the purpose of generating the alignment andthe dictionary we divide the lexical units into threeclasses:1. irrelevant units,2.
closed class units,3.
open class unitsThe same categories apply to expressions.Irrelevant units are simply those that we don~twant to include.
They have to be listed explicitly.The reason for not including some items may varywith the purpose of alignment.
Even if we wish thealignment to be as complete as possible, it mightbe useful to exclude certain units that we suspectmay confuse the algorithm.
For instance, the do-support found in English usually has nocounterpart in other languages.
Thus, the differentforms of 'do' may be excluded from considerationfrom the start.As for the translation relation we make thefollowing assumptions:1.
A lexical unit in one half of the bitextcorresponds to at most one lexical unit in the otherhalf.
This can be seen as a generalization of theone-to-one assumption for word-to-wordtranslation used by Melamed (1997a) and isexploited for the same purpose, i.e.
to excludelarge numbers of candidate alignments, when goodinitial alignments have been found.2.
Open class and closed class lexical units areusually translated and there are a limited numberof lexical units in the other language that arecommonly used to translate them.
Whiledeliberately vague this assumption is whatmotivates our search for frequent pairs <sourceexpression, target expression> with high mutualinformation.
It also motivates our choice ofregarding additions and deletions of lexical unitsin translation as haphazard apart from the case of arestricted set of irrelevant units that we assume canbe known in advance.3.
Open class units can only be aligned withopen class units, and closed class units can only bealigned with closed class units.
This assumptionseems generally correct and has the effect ofreducing the number of candidate alignmentssignificantly.
Closed class units have to be listedexplicitly.
The assumption is that we know the twolanguages ufficiently well to be able to come upwith an appropriate list of closed class units andexpressions.
Multi-word closed class units arelisted separately.
Closed class units can be furtherclassified for the purposes of alignment (seebelow).4.
If some expression for the lexical unit Us isfound corresponding to some expression for thelexical unit UT, then assume that any expression ofUs may correspond to any expression of UT.
Thisassumption is in accordance with the often madeobservation that morphological properties are notinvariants in translation.
It is used to make thealgorithm more greedy by accepting infrequentalignments that are morphological variants ofhigh-rating ones.5.
If one half of an aligned sentence pair is theexpression of a single lexical unit, then assumethat the other half is also.
This is definitely aheuristic, but it has been shown to be very useful31for technical texts involving English andScandinavian, where terms are often found in listsor table cells (Tiedemann 1997).
This heuristic isuseful for finding alignments regardless offrequencies.Similarly, if there is only one non-aligned(relevant open class) word left in a partiallyaligned sentence, assume that it corresponds to theremaining (relevant open class) words of thecorresponding sentence.6.
Position matters, i.e.
while word order is notan invariant of translation it is not random either.We implement the contribution of position as adistribution of weights over the candidate pairs ofexpressions drawn from a given pair of sentences.Expressions that are close in relative positionreceive higher weights, while expressions that arefar from each other eceive lower weights.4.
The Approach4.1 InputA bitext aligned at the sentence l vel.4.2 OutputThere are two types of output data: (i) a table oflink types in the form of a bilingual dictionarywhere each entry has the form <<sf  .... t">, s beingthe source expression type and t I .... t n the targetexpression types that were found to correspond tos; and (ii) a table of link instances <<s,t><i,j>>sorted by sentence pairs, where s is someexpression from the source text, t is an expressionfrom the translated text, and i and j are the (within-sentence) positions of the first word of s and t,respectively.4.3 Preprocess ingBoth halves of the bitext are regularized.When open class multi-word units are to beincluded, they are generated in a preprocessingstage for both the source and target texts andassembled in a table.
For this purpose, we use thephrase extracting program described in Merkel etal.
(1994).4.4 Basic operat ionThe basic algorithm combines the K-vec approach,described by Fung and Church (1993), with thegreedy word-to-word algorithm of Melamed(1997a).
In addition, open class expressions arehandled separately from closed class expressions,and sentences consisting of a single expression arehandled in the manner of Tiedemann (1997).The algorithm is iterative, repeating the sameprocess of generating translation pairs from thebitext, and then reducing the bitext by removingthe pairs that have been found before the nextiteration starts.
The algorithm will stop when nomore pairs can be generated, or when a givennumber of iterations have been completed.In each iteration, the following operations areperformed:(i) For each open class expression in the sourcehalf of the bitext (with frequency higher than 3),the open class expressions in correspondingsentences of the other half are ranked according totheir likelihood as translations of the given sourceexpression.We estimate the probability that a candidatetarget expression is a translation by counting co-occurrences of the expressions within sentencepairs and overall occurrences in the bitext as awhole.
Then the t-score, used by Fung and Church,is calculated, and the candidates are ranked on thebasis of this value:In our case K is the number of sentence pairs inprob(V~,Vt) - prob(V~) prob(V,)  t--the bitext.
The target expression giving the highestt-score is selected as a translation provided thefollowing two conditions are met: (a) this t-scoreis higher than a given threshold, and (b) the overallfrequency of the pair is sufficiently high.
(Theseare the same conditions that are used by Fung andChurch.
)This operation yields a list of translation pairsinvolving open class expressions.
(ii) The same as in (i) but this time with theclosed class expressions.
A difference from theprevious tage is that only target candidates of theproper sub-category or sub-categories for thesource expression are considered.
Conjunctionsand personal pronouns are for example specifiedfor both the target and the source languages.
Thisstrategy helps to limit the search space whenclosed-class expressions are linked.32(iii) Open class expressions that constitute asentence on their own (not counting irrelevantword tokens) generate translation pairs with theopen class expressions of the correspondingsentence.
(iv) When all (relevant) source expressions havebeen tried in this manner, a number of translationpairs have been obtained that are entered in theoutput table and then removed from the bitext.This will affect t-scores by reducing mariginalfrequencies and will also cause fewer candidatepairs to be considered in the sequel.
The reducedbitext is input for the next iteration.4.5 Var iantsThe basic algorithm is enhanced by a number ofmodules that can be combined freely by the user.These modules are?
a morphological module that groupsexpressions that are identical modulo specifiedsets of suffices;?
a weight module that affects the likelihood of acandidate translation according to its positionin the sentence;?
a phrase module that includes multi-wordexpressions generated in the pre-processingstage as candidate xpressions for alignment.4.5.1 The morphological moduleThe morphological module collects open classtranslation pairs that are similar to the ones that arefound by the basic algorithm.
More precisely, ifthe pair (X, Y) has been generated as a translationpair in some iteration, other candidate pairs with Xas the first element are searched.
A pair (X, Z) isconsidered to be a translation pair iff there existstrings W, F and G such thatY --- i~rF,Z = IY/Gand F and G have been defined as differentsuffices of the same paradigm.The data needed for this module consists ofsimple suffix lists for regular paradigms of thelanguages involved.
For example, \[0, s, ed, ing\] isa suffix list for regular English verbs.
They have tobe defined by the user in advance.When the morphological module is used, it ispossible to reverse the direction of the linkingprocess at a certain stage.
After each iteration oflinking expressions from source to target, thedifferent inflectional variants of the target wordare used as input data and these candidates arethen linked from target to source.
This strategymakes it possible to link low-frequency sourceexpressions belonging to the same suffixparadigm.4.5.2 The weight moduleThe weight module distribute weights over thetarget expressions depending on their positionrelative to the given source expression.
Theweights must be provided by the user in the formof lists of numbers (greater than or equal to 0).The weight for a pair is caclulated as the sum ofthe weights for the instances of that pair.
Thisweight is then used to adjust the co-occurrenceprobabilities by using the weight instead of the co-occurrence frequency as input to the the t-scoreformula.
The threshold used is adjustedaccordingly.
In the current configuration ofweights, the threshold is increased by 1.
In theweight module it is possible to specify themaximal distance between a source and targetexpression measured as their relative position inthe sentences.4.5.3 The phrase moduleWhen the phrase module is invoked, multi-wordexpressions are also considered as potentialelements of translation pairs.
The multi-wordexpressions to be considered are generated in aspecial pre-processing phase and stored in a phrasetable.T-scores for candidate translation pairsinvolving multi-word expressions are calculated inthe same way as for single words.
When weightsare used the weight of a multi-word expression isconsidered equal to that of its first word.It can happen that the t-scores for two pairs<s,tl> and <s,t;>, where t I is a multi-wordexpression and P is a word that is part of t 1, will beidentical or almost identical.
In this case we preferthe almost identical target multi-word expressionover a single word candidate if it has a t-valueover the threshold and is one of the top six targetcandidates.
When a multi-word expression isfound to be an element of a translation pair, theexpressions that overlap with it, whether multi-word or single-word expressions, are removedfrom the current agenda and not considered untilthe next iteration.335.
Evaluat ionThe algorithm was tested on two different texts;one novel (66,693 source words) and onecomputer program manual (169.779 source words)which both were translated from English intoSwedish.
The tests were run on a Sun UltraSparclWorkstation with 320 MB RAM and took 55minutes for the novel and 4 and a half hour for theprogram manual.The tests were run with three differentconfigurations on each text: (i) the baseline (B)configuration which is the t-score measure, (ii) allmodules except he weights module (AM-W), but alinkdistance constraint was used and set to 10; and(iii) all modules (AM) including morphology,weights and phrases.
The t-score threshold usedwas 1.65 for B and AM-W, and 2.7 for AM, theminimum frequency of source expression was setto 3.
Closed-class expressions were linked in allconfigurations.
In the baseline configuration nodistinction was made between closed-class andopen-class expressions.
In the AM-W and AMtests the closed-class expressions were dividedinto different subcategories and at the end of eachiteration the linking direction was reversed at theend of each of the six iterations which improvesthe chances of linking low frequency sourceexpressions.
The characteristics of the source textsused are shown in Table 3.Table 4.
Results from two bitexts, using T-scoreW), and all modules (AMTable 3.
Characteristics for the two source textsSize in running wordsNo of word typesWord tTpes frequency 3orhigherWord types frequency 2or 1Multi-word expression types(found in pre-processins)The novel containsNovel Prog.
Man.66,693 169,7799,917 3,8282,870 2,2747,047 1,554243 981a high number of lowfrequency words whereas the program manualcontains a higher proportion of words that thealgorithm acturally tested as the frequencythreshold was set to 3.The results from the tests are shown in Table 4.The evaluation was done on an extract from theautomatically produced ictionary.
All expressionsstarting with the letters N, O and P were evaluatedfor all three configurations of each text.The results from the novel show that recall isalmost tripled in the sample, from 234 in the Bconfiguration to 709 linked source expressionswith the AM configuration.
Precision values forthe novel lie in the range from 90.13 to 92.50 percent when partial links are judged as errors andslightly higher if they are not.
The use of weightsseems to make precision somewhat lower for thenovel which perhaps could be explained by thefact that the novel is a much more varied text type.For the program manual the recall results are asgood as for the novel (three times as many linkedsource types for the AM configuration comparedto baseline).
Precision is increased, but perhaps notonly (B), all modules except the weights (AM-Linked source expressionesLinked multi-word expr.Link tTpes in totalLinks in evaluated sampleCorrect links in sampleErrors in samplePartial inks in samplePrecisionPrecision (only errors)Token recallType recall freq 3 or higherType recall freq 2 or INovel Program Manual1,575 2,467 2,895 2,8780 177 ~ 187 7342,059 4,833 5,754 7,487234 573 709 1,005207 530 639 75321 19 30 1226 24 40 13088.46% 92.50% 90.13% 74.93%91.03% 96.68% 95.77% 87.86%50.9%54.88%54.6%72.06%3.15%56.70%82.65%4.87%B AM-W1,631 2,7480 6832,740 7,241318 953199 65551 13768 16162.58% 68.73%83.96% 85.62%60.2% 67.1%73.88% 82.10%0 12.74%67.3%85.53%12.74%34to the level we anticipated at first.
Multi-wordexpressions are linked with a relatively high recall(above 70%), but the precision of these links arenot as high as for single words.
Our evaluations ofthe links show that one major problem lies in thequality of the multi-word expressions that are fedinto the alignment program.
As the program worksiteratively and in the current version starts with themulti-word expressions, any errors at this stagewill have consequences in later iterations.We have run each module separately andobserved that the addition of each moduleimproves the baseline configuration by itself.
Tocompare our results to those from otherapproaches is difficult.
Not only are we dealingwith different language pairs but also withdifferent texts and text types.
There is also theissue of different evaluation criteria.
A pure word-to-word alignment cannot be compared to anapproach where lexical units (both single wordexpressions and multi-word expressions) arelinked.
Neither can the combined approach becompared to a pure phrase alignment programbecause the aims of the alignment are different.However, as far as we can judge given thesedifficulties, the results presented in this paper areon par with previous work for precision andpossibly an improvement on recall because of howwe handle low-frequency variants in themorphology module and by using the single-word-line strategy.
The handling of closed-classexpressions have also been improved due to thedivision of these expressions into subcategorieswhich limits the search space considerably.AcknowledgementsThis work is part of the project "Parallell corporain Link6ping, Uppsala and G6teborg" (PLUG),jointly funded by Nutek and HSFR under theSwedish National research programme inLanguage Technology.ReferencesBrown, P.F., J. Cocke, S. Della Pietra, V. Della Pietra, F.Jelinek, IL Mercer, & P. Roossin.
(1988) "A StatisticalApproach to Language Translation."
Proceedings of the12th International Conference on ComputationalLinguistics.
Budapest.Brown, P F, J. Cocke, S. Della Pietra, V. Della Pielra, F.Jelinek, R. Mercer, & P. Roossin.
(1990) "A StatisticalApproach to Machine Translation."
ComputationalLinguistics 16(2).Dagan, I, & K. W. Church.
(1994) "Termight: Identifyingand Translating Technical Terminology."
Proceedingsfrom the Conference on Applied Natural LanguageProcessing; StuttgarLFung, P, & K. W. Church.
(1994) "K-vec: A NewApproach for Aligning Parallel Texts."
Proceedings fromthe 15th International Conference on ComputationalLinguistics, Kyoto.Jones, D: & M. Alexa (1997) '~rowards automaticallyaligning German compounds with English word groups.
"In New Methods in Language Processing (eds.
Jones D.& H. Somers).
UCL Press, London.Kitamura, M. & Y. Matsumoto (1996) "AutomaticExtraction of Word Sequence Correspondences inParallel Corpora".
In Proceedings of the Fourth AnnualWorkshop on Very Large Corpora (WVLC-4),Copenhagen.Macklovitch, E., & Marie-Loiuse Hannan.
(1996) "LineUp: Advances in Alignment Technology and TheirImpact on Translation Support Tools."
In Proceedings ofthe Second Conference of the Association for MachineTranslation i  the Americas, Montreal.Melamed, I. D. (1997a) "A Word-to-Word Model ofTranslational Equivalence."
Proceedings of the 35thConference of the Association for ComputationalLinguistics, Madrid.Melamed, I. Dan.
(1997b) "Automatic Discovery of Non-Compositional Compounds in Parallel Data."
Paperpresented atthe 2nd Conference on Empirical Methods inNatural Language Processing, Providence.Merkel, M. B. Nilsson, & L. Ahrenberg, (1994) "A Phrase-Retrieval System Based on Recurrence."
In Proceedingsof the Second Annual Workshop on Very Large Corpora(WVLC-2).
Kyoto.Resnik, P. & I. D. Melamed.
(1997) "Semi-AutomaticAcquisition of Domain-Specific Translation Lexicons.
"In Proceedings of the 7th ACL Conference on AppliedNatural Language Processing.
Washington DC.Smadja F., K. McKeown, & V. Hatzivassiloglou, (1996)"Franslating Collocations for Bilingual Lexicons: AStatistical Approach."
In Computational Linguistics, Vol.22No.
1.Tiedemann, J6rg.
(1997) "Automatic Lexicon ExWactionfi'om Aligned Bilingual Corpora."
Diploma Thesis, Otto-von-Guericke-Universit~t Magdeburg.35
