Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317?1327,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsSequence-Level Knowledge DistillationYoon Kimyoonkim@seas.harvard.eduAlexander M. Rushsrush@seas.harvard.eduSchool of Engineering and Applied SciencesHarvard UniversityCambridge, MA, USAAbstractNeural machine translation (NMT) offers anovel alternative formulation of translationthat is potentially simpler than statistical ap-proaches.
However to reach competitive per-formance, NMT models need to be exceed-ingly large.
In this paper we consider applyingknowledge distillation approaches (Bucila etal., 2006; Hinton et al, 2015) that have provensuccessful for reducing the size of neural mod-els in other domains to the problem of NMT.We demonstrate that standard knowledge dis-tillation applied to word-level prediction canbe effective for NMT, and also introduce twonovel sequence-level versions of knowledgedistillation that further improve performance,and somewhat surprisingly, seem to elimi-nate the need for beam search (even when ap-plied on the original teacher model).
Our beststudent model runs 10 times faster than itsstate-of-the-art teacher with little loss in per-formance.
It is also significantly better thana baseline model trained without knowledgedistillation: by 4.2/1.7 BLEU with greedy de-coding/beam search.
Applying weight prun-ing on top of knowledge distillation results ina student model that has 13?
fewer param-eters than the original teacher model, with adecrease of 0.4 BLEU.1 IntroductionNeural machine translation (NMT) (Kalchbrennerand Blunsom, 2013; Cho et al, 2014; Sutskever etal., 2014; Bahdanau et al, 2015) is a deep learning-based method for translation that has recently shownpromising results as an alternative to statistical ap-proaches.
NMT systems directly model the proba-bility of the next word in the target sentence sim-ply by conditioning a recurrent neural network onthe source sentence and previously generated targetwords.While both simple and surprisingly accurate,NMT systems typically need to have very high ca-pacity in order to perform well: Sutskever et al(2014) used a 4-layer LSTM with 1000 hidden unitsper layer (herein 4?1000) and Zhou et al (2016) ob-tained state-of-the-art results on English ?
Frenchwith a 16-layer LSTM with 512 units per layer.
Thesheer size of the models requires cutting-edge hard-ware for training and makes using the models onstandard setups very challenging.This issue of excessively large networks has beenobserved in several other domains, with much fo-cus on fully-connected and convolutional networksfor multi-class classification.
Researchers have par-ticularly noted that large networks seem to be nec-essary for training, but learn redundant representa-tions in the process (Denil et al, 2013).
Thereforecompressing deep models into smaller networks hasbeen an active area of research.
As deep learningsystems obtain better results on NLP tasks, compres-sion also becomes an important practical issue withapplications such as running deep learning modelsfor speech and translation locally on cell phones.Existing compression methods generally fall intotwo categories: (1) pruning and (2) knowledge dis-tillation.
Pruning methods (LeCun et al, 1990; Heet al, 2014; Han et al, 2016), zero-out weights orentire neurons based on an importance criterion: Le-Cun et al (1990) use (a diagonal approximation to)1317the Hessian to identify weights whose removal min-imally impacts the objective function, while Hanet al (2016) remove weights based on threshold-ing their absolute values.
Knowledge distillation ap-proaches (Bucila et al, 2006; Ba and Caruana, 2014;Hinton et al, 2015) learn a smaller student networkto mimic the original teacher network by minimiz-ing the loss (typically L2 or cross-entropy) betweenthe student and teacher output.In this work, we investigate knowledge distilla-tion in the context of neural machine translation.
Wenote that NMT differs from previous work which hasmainly explored non-recurrent models in the multi-class prediction setting.
For NMT, while the modelis trained on multi-class prediction at the word-level,it is tasked with predicting complete sequence out-puts conditioned on previous decisions.
With thisdifference in mind, we experiment with standardknowledge distillation for NMT and also proposetwo new versions of the approach that attempt to ap-proximately match the sequence-level (as opposedto word-level) distribution of the teacher network.This sequence-level approximation leads to a sim-ple training procedure wherein the student networkis trained on a newly generated dataset that is theresult of running beam search with the teacher net-work.We run experiments to compress a large state-of-the-art 4 ?
1000 LSTM model, and find that withsequence-level knowledge distillation we are able tolearn a 2?
500 LSTM that roughly matches the per-formance of the full system.
We see similar resultscompressing a 2 ?
500 model down to 2 ?
100 ona smaller data set.
Furthermore, we observe thatour proposed approach has other benefits, such asnot requiring any beam search at test-time.
As a re-sult we are able to perform greedy decoding on the2 ?
500 model 10 times faster than beam search onthe 4 ?
1000 model with comparable performance.Our student models can even be run efficiently ona standard smartphone.1 Finally, we apply weightpruning on top of the student network to obtain amodel that has 13?
fewer parameters than the origi-nal teacher model.
We have released all the code forthe models described in this paper.21https://github.com/harvardnlp/nmt-android2https://github.com/harvardnlp/seq2seq-attn2 Background2.1 Sequence-to-Sequence with AttentionLet s = [s1, .
.
.
, sI ] and t = [t1, .
.
.
, tJ ] be (randomvariable sequences representing) the source/targetsentence, with I and J respectively being thesource/target lengths.
Machine translation involvesfinding the most probable target sentence given thesource:argmaxt?Tp(t | s)where T is the set of all possible sequences.
NMTmodels parameterize p(t | s) with an encoder neuralnetwork which reads the source sentence and a de-coder neural network which produces a distributionover the target sentence (one word at a time) giventhe source.
We employ the attentional architecturefrom Luong et al (2015), which achieved state-of-the-art results on English?
German translation.32.2 Knowledge DistillationKnowledge distillation describes a class of methodsfor training a smaller student network to performbetter by learning from a larger teacher network(in addition to learning from the training data set).We generally assume that the teacher has previouslybeen trained, and that we are estimating parame-ters for the student.
Knowledge distillation suggeststraining by matching the student?s predictions to theteacher?s predictions.
For classification this usuallymeans matching the probabilities either via L2 onthe log scale (Ba and Caruana, 2014) or by cross-entropy (Li et al, 2014; Hinton et al, 2015).Concretely, assume we are learning a multi-classclassifier over a data set of examples of the form(x, y) with possible classes V .
The usual trainingcriteria is to minimize NLL for each example fromthe training data,LNLL(?)
= ?|V|?k=11{y = k} log p(y = k |x; ?
)where 1{?}
is the indicator function and p thedistribution from our model (parameterized by ?
).3Specifically, we use the global-general attention modelwith the input-feeding approach.
We refer the reader to the orig-inal paper for further details.1318Figure 1: Overview of the different knowledge distillation approaches.
In word-level knowledge distillation (left) cross-entropyis minimized between the student/teacher distributions (yellow) for each word in the actual target sequence (ECD), as well asbetween the student distribution and the degenerate data distribution, which has all of its probabilitiy mass on one word (black).
Insequence-level knowledge distillation (center) the student network is trained on the output from beam search of the teacher networkthat had the highest score (ACF).
In sequence-level interpolation (right) the student is trained on the output from beam search ofthe teacher network that had the highest sim with the target sequence (ECE).This objective can be seen as minimizing the cross-entropy between the degenerate data distribution(which has all of its probability mass on one class)and the model distribution p(y |x; ?
).In knowledge distillation, we assume access toa learned teacher distribution q(y |x; ?T ), possiblytrained over the same data set.
Instead of minimiz-ing cross-entropy with the observed data, we insteadminimize the cross-entropy with the teacher?s prob-ability distribution,LKD(?
; ?T ) =?|V|?k=1q(y = k |x; ?T )?log p(y = k |x; ?
)where ?T parameterizes the teacher distribution andremains fixed.
Note the cross-entropy setup is iden-tical, but the target distribution is no longer a sparsedistribution.4 Training on q(y |x; ?T ) is attractivesince it gives more information about other classesfor a given data point (e.g.
similarity betweenclasses) and has less variance in gradients (Hintonet al, 2015).4 In some cases the entropy of the teacher/student distribu-tion is increased by annealing it with a temperature term ?
> 1p?
(y |x) ?
p(y |x) 1?After testing ?
?
{1, 1.5, 2} we found that ?
= 1 worked best.Since this new objective has no direct term for thetraining data, it is common practice to interpolatebetween the two losses,L(?
; ?T ) = (1?
?)LNLL(?)
+ ?LKD(?
; ?T )where ?
is mixture parameter combining the one-hotdistribution and the teacher distribution.3 Knowledge Distillation for NMTThe large sizes of neural machine translation sys-tems make them an ideal candidate for knowledgedistillation approaches.
In this section we explorethree different ways this technique can be applied toNMT.3.1 Word-Level Knowledge DistillationNMT systems are trained directly to minimize wordNLL, LWORD-NLL, at each position.
Therefore ifwe have a teacher model, standard knowledge distil-lation for multi-class cross-entropy can be applied.We define this distillation for a sentence as,LWORD-KD = ?J?j=1|V|?k=1q(tj = k | s, t<j)?log p(tj = k | s, t<j)where V is the target vocabulary set.
The studentcan further be trained to optimize the mixture of1319LWORD-KD and LWORD-NLL.
In the context of NMT,we refer to this approach as word-level knowledgedistillation and illustrate this in Figure 1 (left).3.2 Sequence-Level Knowledge DistillationWord-level knowledge distillation allows transfer ofthese local word distributions.
Ideally however, wewould like the student model to mimic the teacher?sactions at the sequence-level.
The sequence distri-bution is particularly important for NMT, becausewrong predictions can propagate forward at test-time.First, consider the sequence-level distributionspecified by the model over all possible sequencest ?
T ,p(t | s) =J?j=1p(tj | s, t<j)for any length J .
The sequence-level negative log-likelihood for NMT then involves matching the one-hot distribution over all complete sequences,LSEQ-NLL = ?
?t?T1{t = y} log p(t | s)= ?J?j=1|V|?k=11{yj = k} log p(tj = k | s, t<j)= LWORD-NLLwhere y = [y1, .
.
.
, yJ ] is the observed sequence.Of course, this just shows that from a negativelog likelihood perspective, minimizing word-levelNLL and sequence-level NLL are equivalent in thismodel.But now consider the case of sequence-levelknowledge distillation.
As before, we can simplyreplace the distribution from the data with a prob-ability distribution derived from our teacher model.However, instead of using a single word prediction,we use q(t | s) to represent the teacher?s sequencedistribution over the sample space of all possible se-quences,LSEQ-KD = ?
?t?Tq(t | s) log p(t | s)Note that LSEQ-KD is inherently different fromLWORD-KD, as the sum is over an exponential num-ber of terms.
Despite its intractability, we positthat this sequence-level objective is worthwhile.
Itgives the teacher the chance to assign probabilities tocomplete sequences and therefore transfer a broaderrange of knowledge.
We thus consider an approxi-mation of this objective.Our simplest approximation is to replace theteacher distribution q with its mode,q(t | s) ?
1{t = argmaxt?Tq(t | s)}Observing that finding the mode is itself intractable,we use beam search to find an approximation.
Theloss is thenLSEQ-KD ?
?
?t?T1{t = y?}
log p(t | s)= ?
log p(t = y?
| s)where y?
is now the output from running beam searchwith the teacher model.Using the mode seems like a poor approximationfor the teacher distribution q(t | s), as we are ap-proximating an exponentially-sized distribution witha single sample.
However, previous results showingthe effectiveness of beam search decoding for NMTlead us to belief that a large portion of q?s mass liesin a single output sequence.
In fact, in experimentswe find that with beam of size 1, q(y?
| s) (on aver-age) accounts for 1.3% of the distribution for Ger-man?
English, and 2.3% for Thai?
English (Ta-ble 1: p(t = y?
)).5To summarize, sequence-level knowledge distil-lation suggests to: (1) train a teacher model, (2) runbeam search over the training set with this model, (3)train the student network with cross-entropy on thisnew dataset.
Step (3) is identical to the word-levelNLL process except now on the newly-generateddata set.
This is shown in Figure 1 (center).5Additionally there are simple ways to better approximateq(t | s).
One way would be to consider a K-best list from beamsearch and renormalizing the probabilities,q(t | s) ?
q(t | s)?t?TK q(t | s)where TK is the K-best list from beam search.
This wouldincrease the training set by a factor of K. A beam of size5 captures 2.8% of the distribution for German ?
English,and 3.8% for Thai ?
English.
Another alternative is to use aMonte Carlo estimate and sample from the teacher model (sinceLSEQ-KD = Et?q(t | s)[?
log p(t | s) ]).
However in practice wefound the (approximate) mode to work well.13203.3 Sequence-Level InterpolationNext we consider integrating the training data backinto the process, such that we train the studentmodel as a mixture of our sequence-level teacher-generated data (LSEQ-KD) with the original trainingdata (LSEQ-NLL),L = (1?
?
)LSEQ-NLL + ?LSEQ-KD= ?(1?
?)
log p(y | s)?
?
?t?Tq(t | s) log p(t | s)where y is the gold target sequence.Since the second term is intractable, we couldagain apply the mode approximation from the pre-vious section,L = ?(1?
?)
log p(y | s)?
?
log p(y?
| s)and train on both observed (y) and teacher-generated (y?)
data.
However, this process is non-ideal for two reasons: (1) unlike for standard knowl-edge distribution, it doubles the size of the trainingdata, and (2) it requires training on both the teacher-generated sequence and the true sequence, condi-tioned on the same source input.
The latter concernis particularly problematic since we observe that yand y?
are often quite different.As an alternative, we propose a single-sequenceapproximation that is more attractive in this setting.This approach is inspired by local updating (Lianget al, 2006), a method for discriminative train-ing in statistical machine translation (although toour knowledge not for knowledge distillation).
Lo-cal updating suggests selecting a training sequencewhich is close to y and has high probability underthe teacher model,y?
= argmaxt?Tsim(t,y)q(t | s)where sim is a function measuring closeness (e.g.Jaccard similarity or BLEU (Papineni et al, 2002)).Following local updating, we can approximate thissequence by running beam search and choosingy?
?
argmaxt?TKsim(t,y)where TK is the K-best list from beam search.We take sim to be smoothed sentence-level BLEU(Chen and Cherry, 2014).We justify training on y?
from a knowledge distil-lation perspective with the following generative pro-cess: suppose that there is a true target sequence(which we do not observe) that is first generatedfrom the underlying data distributionD.
And furthersuppose that the target sequence that we observe (y)is a noisy version of the unobserved true sequence:i.e.
(i) t ?
D, (ii) y ?
(t), where (t) is, for ex-ample, a noise function that independently replaceseach element in t with a random element in V withsome small probability.6 In such a case, ideally thestudent?s distribution should match the mixture dis-tribution,DSEQ-Inter ?
(1?
?
)D + ?q(t | s)In this setting, due to the noise assumption,D nowhas significant probability mass around a neighbor-hood of y (not just at y), and therefore the argmaxof the mixture distribution is likely something otherthan y (the observed sequence) or y?
(the output frombeam search).
We can see that y?
is a natural approx-imation to the argmax of this mixture distributionbetween D and q(t | s) for some ?.
We illustratethis framework in Figure 1 (right) and visualize thedistribution over a real example in Figure 2.4 Experimental SetupTo test out these approaches, we conduct two sets ofNMT experiments: high resource (English ?
Ger-man) and low resource (Thai?
English).The English-German data comes from WMT2014.7 The training set has 4m sentences and wetake newstest2012/newstest2013 as the dev set andnewstest2014 as the test set.
We keep the top 50kmost frequent words, and replace the rest with UNK.The teacher model is a 4 ?
1000 LSTM (as in Lu-ong et al (2015)) and we train two student models:2?
300 and 2?
500.
The Thai-English data comesfrom IWSLT 2015.8 There are 90k sentences in the6While we employ a simple (unrealistic) noise function forillustrative purposes, the generative story is quite plausible if weconsider a more elaborate noise function which includes addi-tional sources of noise such as phrase reordering, replacementof words with synonyms, etc.
One could view translation hav-ing two sources of variance that should be modeled separately:variance due to the source sentence (t ?
D), and variance dueto the individual translator (y ?
(t)).7http://statmt.org/wmt148https://sites.google.com/site/iwsltevaluation2015/mt-track1321Figure 2: Visualization of sequence-level interpolation on anexample German ?
English sentence: Bis 15 Tage vor An-reise sind Zimmer-Annullationen kostenlos.
We run beamsearch, plot the final hidden state of the hypotheses using t-SNEand show the corresponding (smoothed) probabilities with con-tours.
In the above example, the sentence that is at the top ofthe beam after beam search (green) is quite far away from gold(red), so we train the model on a sentence that is on the beambut had the highest sim (e.g.
BLEU) to gold (purple).training set and we take 2010/2011/2012 data as thedev set and 2012/2013 as the test set, with a vocabu-lary size is 25k.
Size of the teacher model is 2?500(which performed better than 4?1000, 2?750 mod-els), and the student model is 2?100.
Other trainingdetails mirror Luong et al (2015).We evaluate on tokenized BLEU withmulti-bleu.perl, and experiment withthe following variations:Word-Level Knowledge Distillation (Word-KD)Student is trained on the original data and addition-ally trained to minimize the cross-entropy of theteacher distribution at the word-level.
We tested?
?
{0.5, 0.9} and found ?
= 0.5 to work better.Sequence-Level Knowledge Distillation (Seq-KD)Student is trained on the teacher-generated data,which is the result of running beam search and tak-ing the highest-scoring sequence with the teachermodel.
We use beam size K = 5 (we did not seeimprovements with a larger beam).Sequence-Level Interpolation (Seq-Inter) Stu-dent is trained on the sequence on the teacher?s beamthat had the highest BLEU (beam size K = 35).
Weadopt a fine-tuning approach where we begin train-ing from a pretrained model (either on original dataor Seq-KD data) and train with a smaller learningrate (0.1).
For English-German we generate Seq-Inter data on a smaller portion of the training set(?
50%) for efficiency.The above methods are complementary and canbe combined with each other.
For example, wecan train on teacher-generated data but still in-clude a word-level cross-entropy term between theteacher/student (Seq-KD + Word-KD in Table 1),or fine-tune towards Seq-Inter data starting from thebaseline model trained on original data (Baseline +Seq-Inter in Table 1).95 Results and DiscussionResults of our experiments are shown in Table1.
We find that while word-level knowledge dis-tillation (Word-KD) does improve upon the base-line, sequence-level knowledge distillation (Seq-KD) does better on English ?
German and per-forms similarly on Thai ?
English.
Combiningthem (Seq-KD + Word-KD) results in further gainsfor the 2 ?
300 and 2 ?
100 models (although notfor the 2 ?
500 model), indicating that these meth-ods provide orthogonal means of transferring knowl-edge from the teacher to the student: Word-KD istransferring knowledge at the the local (i.e.
word)level while Seq-KD is transferring knowledge at theglobal (i.e.
sequence) level.Sequence-level interpolation (Seq-Inter), in addi-tion to improving models trained via Word-KD andSeq-KD, also improves upon the original teachermodel that was trained on the actual data but fine-tuned towards Seq-Inter data (Baseline + Seq-Inter).In fact, greedy decoding with this fine-tuned modelhas similar performance (19.6) as beam search withthe original model (19.5), allowing for faster decod-ing even with an identically-sized model.We hypothesize that sequence-level knowledgedistillation is effective because it allows the studentnetwork to only model relevant parts of the teacherdistribution (i.e.
around the teacher?s mode) insteadof ?wasting?
parameters on trying to model the entire9For instance, ?Seq-KD + Seq-Inter + Word-KD?
in Table1 means that the model was trained on Seq-KD data and fine-tuned towards Seq-Inter data with the mixture cross-entropyloss at the word-level.1322Model BLEUK=1 ?K=1 BLEUK=5 ?K=5 PPL p(t = y?)English?
German WMT 2014Teacher Baseline 4?
1000 (Params: 221m) 17.7 ?
19.5 ?
6.7 1.3%Baseline + Seq-Inter 19.6 +1.9 19.8 +0.3 10.4 8.2%Student Baseline 2?
500 (Params: 84m) 14.7 ?
17.6 ?
8.2 0.9%Word-KD 15.4 +0.7 17.7 +0.1 8.0 1.0%Seq-KD 18.9 +4.2 19.0 +1.4 22.7 16.9%Baseline + Seq-Inter 18.5 +3.6 18.7 +1.1 11.3 5.7%Word-KD + Seq-Inter 18.3 +3.6 18.5 +0.9 11.8 6.3%Seq-KD + Seq-Inter 18.9 +4.2 19.3 +1.7 15.8 7.6%Seq-KD + Word-KD 18.7 +4.0 18.9 +1.3 10.9 4.1%Seq-KD + Seq-Inter + Word-KD 18.8 +4.1 19.2 +1.6 14.8 7.1%Student Baseline 2?
300 (Params: 49m) 14.1 ?
16.9 ?
10.3 0.6%Word-KD 14.9 +0.8 17.6 +0.7 10.9 0.7%Seq-KD 18.1 +4.0 18.1 +1.2 64.4 14.8%Baseline + Seq-Inter 17.6 +3.5 17.9 +1.0 13.0 10.0%Word-KD + Seq-Inter 17.8 +3.7 18.0 +1.1 14.5 4.3%Seq-KD + Seq-Inter 18.2 +4.1 18.5 +1.6 40.8 5.6%Seq-KD + Word-KD 17.9 +3.8 18.8 +1.9 44.1 3.1%Seq-KD + Seq-Inter + Word-KD 18.5 +4.4 18.9 +2.0 97.1 5.9%Thai?
English IWSLT 2015Teacher Baseline 2?
500 (Params: 47m) 14.3 ?
15.7 ?
22.9 2.3%Baseline + Seq-Inter 15.6 +1.3 16.0 +0.3 55.1 6.8%Student Baseline 2?
100 (Params: 8m) 10.6 ?
12.7 ?
37.0 1.4%Word-KD 11.8 +1.2 13.6 +0.9 35.3 1.4%Seq-KD 12.8 +2.2 13.4 +0.7 125.4 6.9%Baseline + Seq-Inter 12.9 +2.3 13.1 +0.4 52.8 2.5%Word-KD + Seq-Inter 13.0 +2.4 13.7 +1.0 58.7 3.2%Seq-KD + Seq-Inter 13.6 +3.0 14.0 +1.3 106.4 3.9%Seq-KD + Word-KD 13.7 +3.1 14.2 +1.5 67.4 3.1%Seq-KD + Seq-Inter + Word-KD 14.2 +3.6 14.4 +1.7 117.4 3.2%Table 1: Results on English-German (newstest2014) and Thai-English (2012/2013) test sets.
BLEUK=1: BLEU score with beamsize K = 1 (i.e.
greedy decoding); ?K=1: BLEU gain over the baseline model without any knowledge distillation with greedydecoding; BLEUK=5: BLEU score with beam size K = 5; ?K=5: BLEU gain over the baseline model without any knowledgedistillation with beam size K = 5; PPL: perplexity on the test set; p(t = y?
): Probability of output sequence from greedy decoding(averaged over the test set).
Params: number of parameters in the model.
Best results (as measured by improvement over thebaseline) within each category are highlighted in bold.space of translations.
Our results suggest that thisis indeed the case: the probability mass that Seq-KD models assign to the approximate mode is muchhigher than is the case for baseline models trainedon original data (Table 1: p(t = y?)).
For example,on English ?
German the (approximate) argmaxfor the 2 ?
500 Seq-KD model (on average) ac-counts for 16.9% of the total probability mass, whilethe corresponding number is 0.9% for the baseline.This also explains the success of greedy decodingfor Seq-KD models?since we are only modelingaround the teacher?s mode, the student?s distributionis more peaked and therefore the argmax is mucheasier to find.
Seq-Inter offers a compromise be-tween the two, with the greedily-decoded sequenceaccounting for 7.6% of the distribution.Finally, although past work has shown that mod-els with lower perplexity generally tend to have1323Model Size GPU CPU AndroidBeam = 1 (Greedy)4?
1000 425.5 15.0 ?2?
500 1051.3 63.6 8.82?
300 1267.8 104.3 15.8Beam = 54?
1000 101.9 7.9 ?2?
500 181.9 22.1 1.92?
300 189.1 38.4 3.4Table 2: Number of source words translated per second acrossGPU (GeForce GTX Titan X), CPU, and smartphone (SamsungGalaxy 6) for the various English?
German models.
We wereunable to open the 4?
1000 model on the smartphone.higher BLEU, our results indicate that this is notnecessarily the case.
The perplexity of the baseline2 ?
500 English?
German model is 8.2 while theperplexity of the corresponding Seq-KD model is22.7, despite the fact that Seq-KD model does sig-nificantly better for both greedy (+4.2 BLEU) andbeam search (+1.4 BLEU) decoding.5.1 Decoding SpeedRun-time complexity for beam search grows linearlywith beam size.
Therefore, the fact that sequence-level knowledge distillation allows for greedy de-coding is significant, with practical implications forrunning NMT systems across various devices.
Totest the speed gains, we run the teacher/student mod-els on GPU, CPU, and smartphone, and check theaverage number of source words translated per sec-ond (Table 2).
We use a GeForce GTX Titan X forGPU and a Samsung Galaxy 6 smartphone.
We findthat we can run the student model 10 times fasterwith greedy decoding than the teacher model withbeam search on GPU (1051.3 vs 101.9 words/sec),with similar performance.5.2 Weight PruningAlthough knowledge distillation enables trainingfaster models, the number of parameters for thestudent models is still somewhat large (Table 1:Params), due to the word embeddings which dom-inate most of the parameters.10 For example, on the10Word embeddings scale linearly while RNN parametersscale quadratically with the dimension size.Model Prune % Params BLEU Ratio4?
1000 0% 221 m 19.5 1?2?
500 0% 84 m 19.3 3?2?
500 50% 42 m 19.3 5?2?
500 80% 17 m 19.1 13?2?
500 85% 13 m 18.8 18?2?
500 90% 8 m 18.5 26?Table 3: Performance of student models with varying % of theweights pruned.
Top two rows are models without any pruning.Params: number of parameters in the model; Prune %: Percent-age of weights pruned based on their absolute values; BLEU:BLEU score with beam search decoding (K = 5) after retrain-ing the pruned model; Ratio: Ratio of the number of parametersversus the original teacher model (which has 221m parameters).2 ?
500 English ?
German model the word em-beddings account for approximately 63% (50m outof 84m) of the parameters.
The size of word em-beddings have little impact on run-time as the wordembedding layer is a simple lookup table that onlyaffects the first layer of the model.We therefore focus next on reducing the mem-ory footprint of the student models further throughweight pruning.
Weight pruning for NMT was re-cently investigated by See et al (2016), who foundthat up to 80 ?
90% of the parameters in a largeNMT model can be pruned with little loss in perfor-mance.
We take our best English?
German studentmodel (2?
500 Seq-KD + Seq-Inter) and prune x%of the parameters by removing the weights with thelowest absolute values.
We then retrain the prunedmodel on Seq-KD data with a learning rate of 0.2and fine-tune towards Seq-Inter data with a learningrate of 0.1.
As observed by See et al (2016), re-training proved to be crucial.
The results are shownin Table 3.Our findings suggest that compression benefitsachieved through weight pruning and knowledgedistillation are orthogonal.11 Pruning 80% of theweight in the 2 ?
500 student model results in amodel with 13?
fewer parameters than the originalteacher model with only a decrease of 0.4 BLEU.While pruning 90% of the weights results in a moreappreciable decrease of 1.0 BLEU, the model is11To our knowledge combining pruning and knowledge dis-tillation has not been investigated before.1324drastically smaller with 8m parameters, which is26?
fewer than the original teacher model.5.3 Further Observations?
For models trained with word-level knowledgedistillation, we also tried regressing the studentnetwork?s top-most hidden layer at each timestep to the teacher network?s top-most hiddenlayer as a pretraining step, noting that Romeroet al (2015) obtained improvements with asimilar technique on feed-forward models.
Wefound this to give comparable results to stan-dard knowledge distillation and hence did notpursue this further.?
There have been promising recent results oneliminating word embeddings completely andobtaining word representations directly fromcharacters with character composition models,which have many fewer parameters than wordembedding lookup tables (Ling et al, 2015a;Kim et al, 2016; Ling et al, 2015b; Jozefowiczet al, 2016; Costa-Jussa and Fonollosa, 2016).Combining such methods with knowledge dis-tillation/pruning to further reduce the memoryfootprint of NMT systems remains an avenuefor future work.6 Related WorkCompressing deep learning models is an active areaof current research.
Pruning methods involve prun-ing weights or entire neurons/nodes based on somecriterion.
LeCun et al (1990) prune weights basedon an approximation of the Hessian, while Han et al(2016) show that a simple magnitude-based pruningworks well.
Prior work on removing neurons/nodesinclude Srinivas and Babu (2015) and Mariet andSra (2016).
See et al (2016) were the first to ap-ply pruning to Neural Machine Translation, observ-ing that that different parts of the architecture (in-put word embeddings, LSTM matrices, etc.)
admitdifferent levels of pruning.
Knowledge distillationapproaches train a smaller student model to mimica larger teacher model, by minimizing the loss be-tween the teacher/student predictions (Bucila et al,2006; Ba and Caruana, 2014; Li et al, 2014; Hin-ton et al, 2015).
Romero et al (2015) addition-ally regress on the intermediate hidden layers of thestudent/teacher network as a pretraining step, whileMou et al (2015) obtain smaller word embeddingsfrom a teacher model via regression.
There has alsobeen work on transferring knowledge across differ-ent network architectures: Chan et al (2015b) showthat a deep non-recurrent neural network can learnfrom an RNN; Geras et al (2016) train a CNN tomimic an LSTM for speech recognition.
Kuncoroet al (2016) recently investigated knowledge distil-lation for structured prediction by having a singleparser learn from an ensemble of parsers.Other approaches for compression involve lowrank factorizations of weight matrices (Denton et al,2014; Jaderberg et al, 2014; Lu et al, 2016; Prab-havalkar et al, 2016), sparsity-inducing regularizers(Murray and Chiang, 2015), binarization of weights(Courbariaux et al, 2016; Lin et al, 2016), andweight sharing (Chen et al, 2015; Han et al, 2016).Finally, although we have motivated sequence-levelknowledge distillation in the context of training asmaller model, there are other techniques that trainon a mixture of the model?s predictions and the data,such as local updating (Liang et al, 2006), hope/feartraining (Chiang, 2012), SEARN (Daume?
III et al,2009), DAgger (Ross et al, 2011), and minimumrisk training (Och, 2003; Shen et al, 2016).7 ConclusionIn this work we have investigated existing knowl-edge distillation methods for NMT (which work atthe word-level) and introduced two sequence-levelvariants of knowledge distillation, which provideimprovements over standard word-level knowledgedistillation.We have chosen to focus on translation as thisdomain has generally required the largest capacitydeep learning models, but the sequence-to-sequenceframework has been successfully applied to a widerange of tasks including parsing (Vinyals et al,2015a), summarization (Rush et al, 2015), dialogue(Vinyals and Le, 2015; Serban et al, 2016; Li etal., 2016), NER/POS-tagging (Gillick et al, 2016),image captioning (Vinyals et al, 2015b; Xu et al,2015), video generation (Srivastava et al, 2015), andspeech recognition (Chan et al, 2015a).
We antici-pate that methods described in this paper can be usedto similarly train smaller models in other domains.1325References[Ba and Caruana2014] Lei Jimmy Ba and Rich Caruana.2014.
Do Deep Nets Really Need to be Deep?
InProceedings of NIPS.
[Bahdanau et al2015] Dzmitry Bahdanau, KyunghyunCho, and Yoshua Bengio.
2015.
Neural MachineTranslation by Jointly Learning to Align and Translate.In Proceedings of ICLR.
[Bucila et al2006] Cristian Bucila, Rich Caruana, andAlexandru Niculescu-Mizil.
2006.
Model Compres-sion.
In Proceedings of KDD.
[Chan et al2015a] William Chan, Navdeep Jaitly, QuocLe, and Oriol Vinyals.
2015a.
Listen, Attend andSpell.
arXiv:1508.01211.
[Chan et al2015b] William Chan, Nan Rosemary Ke, andIan Laner.
2015b.
Transfering Knowledge from aRNN to a DNN.
arXiv:1504.01483.
[Chen and Cherry2014] Boxing Chen and Colin Cherry.2014.
A Systematic Comparison of Smoothing Tech-niques for Sentence-Level BLEU.
In Proceedings ofthe Ninth Workshop on Statistical Machine Transla-tion.
[Chen et al2015] Wenlin Chen, James T. Wilson, StephenTyree, Kilian Q. Weinberger, and Yixin Chen.
2015.Compressing Neural Networks with the HashingTrick.
In Proceedings of ICML.
[Chiang2012] David Chiang.
2012.
Hope and Fearfor Discriminative Training of Statistical TranslationModels.
In JMLR.
[Cho et al2014] Kyunghyun Cho, Bart van Merrienboer,Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,Holger Schwenk, and Yoshua Bengio.
2014.
LearningPhrase Representations using RNN Encoder-Decoderfor Statistical Machine Translation.
In Proceedings ofEMNLP.
[Costa-Jussa and Fonollosa2016] Marta R. Costa-Jussaand Jose A.R.
Fonollosa.
2016.
Character-based Neu-ral Machine Translation.
arXiv:1603.00810.
[Courbariaux et al2016] Matthieu Courbariaux, ItayHubara, Daniel Soudry, Ran El-Yaniv, and YoshuaBengio.
2016.
Binarized Neural Networks: TrainingNeural Networks with Weights and ActivationsConstrained to +1 or ?1.
arXiv:1602.02830.[Daume?
III et al2009] Hal Daume?
III, John Langford,and Daniel Marcu.
2009.
Search-based StructuredPrediction.
Machine Learning.
[Denil et al2013] Misha Denil, Babak Shakibi, LaurentDinh, Marc?Aurelio Ranzato, and Nando de Freitas.2013.
Predicting Parameters in Deep Learning.
InProceedings of NIPS.
[Denton et al2014] Emily L. Denton, Wojciech Zaremba,Joan Bruna, Yann LeCun, and Rob Fergus.
2014.
Ex-ploiting Linear Structure within Convolutional NeuralNetworks for Efficient Evaluation.
In Proceedings ofNIPS.
[Geras et al2016] Krzysztof J. Geras, Abdel rahman Mo-hamed, Rich Caruana, Gregor Urban, Shengjie Wang,Ozlem Aslan, Matthai Philipose, Matthew Richard-son, and Charles Sutton.
2016.
Blending LSTMs intoCNNs.
In Proceedings of ICLR Workshop.
[Gillick et al2016] Dan Gillick, Cliff Brunk, OriolVinyals, and Amarnag Subramanya.
2016.
Multilin-gual Language Processing from Bytes.
In Proceedingsof NAACL.
[Han et al2016] Song Han, Huizi Mao, and William J.Dally.
2016.
Deep Compression: Compressing DeepNeural Networks with Pruning, Trained Quantizationand Huffman Coding.
In Proceedings of ICLR.
[He et al2014] Tianxing He, Yuchen Fan, Yanmin Qian,Tian Tan, and Kai Yu.
2014.
Reshaping Deep Neu-ral Network for Fast Decoding by Node-Pruning.
InProceedings of ICASSP.
[Hinton et al2015] Geoffrey Hinton, Oriol Vinyals, andJeff Dean.
2015.
Distilling the Knowledge in a NeuralNetwork.
arXiv:1503.0253.
[Jaderberg et al2014] Max Jaderberg, Andrea Vedaldi,and Andrew Zisserman.
2014.
Speeding up Convo-lutional Neural Networks with Low Rank Expansions.In BMCV.
[Jozefowicz et al2016] Rafal Jozefowicz, Oriol Vinyals,Mike Schuster, Noam Shazeer, and Yonghui Wu.2016.
Exploring the Limits of Language Modeling.arXiv:1602.02410.
[Kalchbrenner and Blunsom2013] Nal Kalchbrenner andPhil Blunsom.
2013.
Recurrent Continuous Transla-tion Models.
In Proceedings of EMNLP.
[Kim et al2016] Yoon Kim, Yacine Jernite, David Son-tag, and Alexander M. Rush.
2016.
Character-AwareNeural Language Models.
In Proceedings of AAAI.
[Kuncoro et al2016] Adhiguna Kuncoro, Miguel Balles-teros, Lingpeng Kong, Chris Dyer, and Noah A. Smith.2016.
Distilling an Ensemble of Greedy DependencyParsers into One MST Parser.
In Proceedings ofEMNLP.
[LeCun et al1990] Yann LeCun, John S. Denker, andSara A. Solla.
1990.
Optimal Brain Damage.
In Pro-ceedings of NIPS.
[Li et al2014] Jinyu Li, Rui Zhao, Jui-Ting Huang, andYifan Gong.
2014.
Learning Small-Size DNN withOutput-Distribution-Based Criteria.
In Proceedings ofINTERSPEECH.
[Li et al2016] Jiwei Li, Michael Galley, Chris Brockett,Jianfeg Gao, and Bill Dolan.
2016.
A Diversity-Promoting Objective Function for Neural Conversa-tional Models.
In Proceedings of NAACL 2016.1326[Liang et al2006] Percy Liang, Alexandre Bouchard-Cote, Dan Klein, and Ben Taskar.
2006.
An End-to-End Discriminative Approach to Machine Translation.In Proceedings of COLING-ACL.
[Lin et al2016] Zhouhan Lin, Matthieu Coubariaux,Roland Memisevic, and Yoshua Bengio.
2016.
NeuralNetworks with Few Multiplications.
In Proceedings ofICLR.
[Ling et al2015a] Wang Ling, Tiago Lui, Luis Marujo,Ramon Fernandez Astudillo, Silvio Amir, Chris Dyer,Alan W Black, and Isabel Trancoso.
2015a.
FindingFunction in Form: Composition Character Models forOpen Vocabulary Word Representation.
In Proceed-ings of EMNLP.
[Ling et al2015b] Wang Ling, Isabel Trancoso, ChrisDyer, and Alan W Black.
2015b.
Character-basedNeural Machine Translation.
arXiv:1511.04586.
[Lu et al2016] Zhiyun Lu, Vikas Sindhwani, and Tara N.Sainath.
2016.
Learning Compact Recurrent NeuralNetworks.
In Proceedings of ICASSP.
[Luong et al2015] Minh-Thang Luong, Hieu Pham, andChristopher D. Manning.
2015.
Effective Approachesto Attention-based Neural Machine Translation.
InProceedings of EMNLP.
[Mariet and Sra2016] Zelda Mariet and Suvrit Sra.
2016.Diversity Networks.
In Proceedings of ICLR.
[Mou et al2015] Lili Mou, Ge Li, Yan Xu, Lu Zhang, andZhi Jin.
2015.
Distilling Word Embeddings: An En-coding Approach.
arXiv:1506.04488.
[Murray and Chiang2015] Kenton Murray and DavidChiang.
2015.
Auto-sizing Neural Networks: WithApplications to N-Gram Language Models.
In Pro-ceedings of EMNLP.
[Och2003] Franz J. Och.
2003.
Minimum Error RateTraining in Statistical Machine Translation.
In Pro-ceedings of ACL.
[Papineni et al2002] Kishore Papineni, Slim Roukos,Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: AMethod for Automatic Evaluation of Machine Trans-lation.
In Proceedings of ICML.
[Prabhavalkar et al2016] Rohit Prabhavalkar, Ouais Al-sharif, Antoine Bruguier, and Ian McGraw.
2016.On the Compression of Recurrent Neural Networkswith an Application to LVCSR Acoustic Modeling forEmbedded Speech Recognition.
In Proceedings ofICASSP.
[Romero et al2015] Adriana Romero, Nicolas Ballas,Samira Ebrahimi Kahou, Antoine Chassang, CarloGatta, and Yoshua Bengio.
2015.
FitNets: Hints forThin Deep Nets.
In Proceedings of ICLR.
[Ross et al2011] Stephane Ross, Geoffrey J. Gordon, andDrew Bagnell.
2011.
A Reduction of Imitation Learn-ing and Structured Prediction to No-Regret OnlineLearning.
In Proceedings of AISTATS.
[Rush et al2015] Alexander M. Rush, Sumit Chopra, andJason Weston.
2015.
A Neural Attention Model forAbstractive Sentence Summarization.
In Proceedingsof EMNLP.
[See et al2016] Abigail See, Minh-Thang Luong, andChristopher D. Manning.
2016.
Compression of Neu-ral Machine Translation via Pruning.
In Proceedingsof CoNLL.
[Serban et al2016] Iulian V. Serban, Allesandro Sordoni,Yoshua Bengio, Aaron Courville, and Joelle Pineau.2016.
Building End-to-End Dialogue Systems UsingGenerative Hierarchical Neural Network Models.
InProceedings of AAAI.
[Shen et al2016] Shiqi Shen, Yong Cheng, Zhongjun He,Wei He, Hua Wu, Masong Sun, and Yang Liu.
2016.Minimum Risk Training for Neural Machine Transla-tion.
In Proceedings of ACL.
[Srinivas and Babu2015] Suraj Srinivas and R. VenkateshBabu.
2015.
Data-free Parameter Pruning for DeepNeural Networks.
BMVC.
[Srivastava et al2015] Nitish Srivastava, Elman Mansi-mov, and Ruslan Salakhutdinov.
2015.
UnsupervisedLearning of Video Representations using LSTMs.Proceedings of ICML.
[Sutskever et al2014] Ilya Sutskever, Oriol Vinyals, andQuoc Le.
2014.
Sequence to Sequence Learning withNeural Networks.
In Proceedings of NIPS.
[Vinyals and Le2015] Oriol Vinyals and Quoc Le.
2015.A Neural Conversational Model.
In Proceedings ofICML Deep Learning Workshop.
[Vinyals et al2015a] Oriol Vinyals, Lukasz Kaiser, TerryKoo, Slave Petrov, Ilya Sutskever, and Geoffrey Hin-ton.
2015a.
Grammar as a Foreign Language.
In Pro-ceedings of NIPS.
[Vinyals et al2015b] Oriol Vinyals, Alexander Toshev,Samy Bengio, and Dumitru Erhan.
2015b.
Show andTell: A Neural Image Caption Generator.
In Proceed-ings of CVPR.
[Xu et al2015] Kelvin Xu, Jimma Ba, Ryan Kiros,Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdi-nov, Richard Zemel, and Yoshua Bengio.
2015.
Show,Attend and Tell: Neural Image Caption Generationwith Visual Attention.
In Proceedings of ICML.
[Zhou et al2016] Jie Zhou, Ying Cao, Xuguang Wang,Peng Li, and Wei Xu.
2016.
Deep Recurrent Modelswith Fast-Forward Connections for Neural MachineTranslation.
In Proceedings of TACL.1327
