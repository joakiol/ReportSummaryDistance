Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 68?77,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsAutomated Verb Sense Labelling Based on Linked Lexical ResourcesKostadin Cholakov1Judith Eckle-Kohler2,3Iryna Gurevych2,31Humboldt-Universit?at zu Berlin, kostadin.cholakov@anglistik.hu-berlin.de2Ubiquitous Knowledge Processing Lab (UKP-TUDA)Dept.
of Computer Science, Technische Universit?at Darmstadt3Ubiquitous Knowledge Processing Lab (UKP-DIPF)German Institute for Educational Research and Educational Informationhttp://www.ukp.tu-darmstadt.deAbstractWe present a novel approach for creat-ing sense annotated corpora automatically.Our approach employs shallow syntactico-semantic patterns derived from linked lex-ical resources to automatically identify in-stances of word senses in text corpora.
Weevaluate our labelling method intrinsicallyon SemCor and extrinsically by using au-tomatically labelled corpus text to train aclassifier for verb sense disambiguation.Testing this classifier on verbs from theEnglish MASC corpus and on verbs fromthe Senseval-3 all-words disambiguationtask shows that it matches the performanceof a classifier which has been trained onmanually annotated data.1 IntroductionSense annotated corpora are important resourcesin NLP as they can be used as training data (e.g.,for word sense disambiguation (WSD) or semanticrole labelling) or as sources for the acquisition oflexical information (e.g., selectional preference in-formation).
Typically, a particular sense inventoryfrom a lexical resource is used to annotate some orall words with word senses from this sense inven-tory.
For instance, various sense-annotated cor-pora based on WordNet (WN; (Fellbaum, 1998))exist, such as the data from the Senseval competi-tions,1or the SemCor corpus.2Such corpora areusually created manually which is expensive andtime consuming.
Furthermore, the corpora are of-ten domain specific (e.g.
newspaper texts) whichmakes statistical systems trained on them stronglybiased.We present a novel approach for creating senseannotated corpora automatically.
Our approach1http://www.senseval.org2http://www.cse.unt.edu/?rada/downloads.html#semcoremploys shallow syntactico-semantic patterns de-rived from linked lexical resources (LLRs) to auto-matically identify instances of word senses in textcorpora.
We significantly extend previous work onthis task by making two important contributions:(i) we employ a large-scale LLR for automaticallycreating sense annotated data and (ii) we performmeaningful intrinsic and application-based eval-uations of our method on large sense annotateddatasets.LLRs are the result of integrating severallexical-semantic resources by linking them at theword sense level.
Examples of large LLRs arethe multilingual BabelNet (Navigli and Ponzetto,2012), an integration of wordnets and Wikipedia3,or UBY, (Gurevych et al., 2012), the resource weemploy in our work here.
UBY is an integration ofmultiple resources, such as wordnets, Wikipedia,Wiktionary (WKT)4, FrameNet (FN; (Baker et al.,1998)) and VerbNet (VN; (Kipper et al., 2008)) forEnglish and German.A distinguishing feature of LLRs is the enrichedsense representation for word senses that are in-terlinked since different resources provide differ-ent, often complementary information.
Annotat-ing corpora with such enriched sense representa-tions turns them into versatile training data for sta-tistical systems.Our first contribution (i) also addresses a con-siderable gap in recent research regarding auto-mated sense labelling of verbs.
Most previouswork is done on nouns.
However, verbs pose abigger challenge due to their high polysemy andthe fact that, unlike nouns, syntax is of crucial im-portance because it often reflects particular aspectsof verb meaning.
That is why, here we focus onverbs and present results and evaluations for thispreviously neglected part-of-speech (POS).
Ourmethod, however, can be applied to other parts-of3http://www.wikipedia.org4http://www.wiktionary.org68speech as well.Regarding (ii), we are the first to perform mean-ingful intrinsic and extrinsic evaluations of auto-matically labelled data on a larger scale.
The in-trinsic evaluation measures the performance of ourmethod on the manually annotated SemCor cor-pus.
The extrinsic evaluation compares the perfor-mance of a classifier for verb sense disambigua-tion (VSD) which has been trained (a) on auto-matically sense labelled data and (b) on manuallyannotated data.
Both settings achieve very simi-lar results which means that competitive VSD canbe performed without the need of costly manuallycreated training data.
This could be beneficial inlanguages (e.g., German, Spanish) for which elab-orate lexical-semantic resources exist but large,high-quality sense annotated corpora are unavail-able.
Moreover, we experiment with various link-ings between lexical resources in order to inves-tigate how different resource combinations affectthe performance of automated sense labelling.
Weshow that combining all available resources mightnot be the best option.The remainder of the paper is organised as fol-lows.
Section 2 presents our method.
Section 3 de-scribes the data used in the experiments.
Section4 presents the results of the evaluations.
Section5 analyses in detail the differences between ourmethod and previous work.
Section 6 concludesthe paper.2 Automated Labelling of Verb SensesThis section describes our novel approach for au-tomated sense labelling of verbs in a corpus, whichexploits the added value of LLRs.2.1 ApproachOur approach to automatically label corpus in-stances of verb senses with sense identifiers froman LLR is based on a pattern-based representationof verb senses.
Such patterns constitute a commonformat for the representation of verb senses avail-able in LLRs and verb instances found in corpora.The common format we developed resembles asyntactico-semantic clause pattern which we calla sense pattern (SP).
Based on a comparison of thederived SPs by means of a similarity metric, verbinstances in a corpus can automatically be labelledwith sense identifiers from an LLR.SPs can be derived from corpus instances andfrom information given in LLRs, in particular,sense examples and more abstract predicate argu-ment structure information.2.2 Step 1: Creation of SPs from LLRsFor the creation of SPs, we employ the large-scaleLLR UBY which combines 10 lexical resourcesfor English and German to make use of the en-riched verb sense representations provided by thesense links between various resources available inUBY.
Although our method can work with anyLLR, we choose UBY because the various re-sources are represented in a standardised format(Eckle-Kohler et al., 2012) and sense links be-tween them can uniformly and conveniently be ac-cessed via the freely available UBY-API.5Since we evaluate our method on data annotatedwith WN senses, we create SPs for enriched WNsenses (see example given in Table 1).
We enrichWN senses by aggregating lexical information thatcan be accessed through links given in UBY tocorresponding verb senses in other resources.In this setting, enrichment means that we makeuse of sense examples from WN, from FN viathe WN?FN linking, and from WKT via theWN?WKT linking.
In addition, we use ab-stract predicate-argument structure informationfrom VN via the WN?VN linking (see Table 1).6For phrasal verb senses (e.g., write up) andother verbal multiword expressions (e.g., knowwhat?s going on) listed in WN, UBY rarely pro-vides links to other resources.
Therefore, we in-duced sense links by following the one sense percollocation assumption.7Based on this assump-tion, we linked each sense of a verbal multiwordverb lemma in WN with each sense of the samemultiword lemma in FN and WKT.From sense examples, we derive two differentkinds of SPs.
Based on a fragment of a sense ex-ample given by a window w around the target verblemma we create: (i) lemma SPs (LSPs) consistingonly of lemmas (including the target verb) and (ii)abstract SPs (ASPs) consisting of the target verblemma and items from a fixed, linguistically mo-tivated vocabulary.
This is based on the intuitionthat LSPs are important to identify relatively fixed5http://code.google.com/p/uby/6Although VN is linked to sense examples given in thePropBank corpus, the rationale behind using just abstractpredicate-argument structure information was to explore,which effect this type of information has on the performanceof an automated labelling algorithm.7It assumes that nearby words provide strong and consis-tent clues to the sense of a target word, see Yarowsky (1995).69WN sense tell%2:32:00:: (let something be known) Corresponding sense patterns (SPs)WN Tell them that you will be late LSP ?
tell them that you will beASP ?
tell PP that PP be JJWN?FN But an insider told TODAY : ?
There was no animosity.?
LSP ?
but an insider tell Today : ?
there beASP ?
person tell location be feelingWN?WKT Please tell me the time.
LSP ?
Please tell me the timeASP ?
tell PP eventWN?VN Agent[+animate| + organization] V ASP ?
PP tell group about communicationRecipient[+animate| + organization]about Topic[+communication]Table 1: Examples of SPs derived from an enriched WN sense in UBY.
PP, JJ, and VV are POS tagsfrom the Penn Treebank tagset, standing for personal pronoun, adjective and full verb.verbal multiword expressions in a corpus, whereasASPs are necessary to identify productively usedverb senses that are constrained in their use onlyby their syntactic behaviour and particular seman-tic properties, such as selectional preferences ontheir arguments.The fixed vocabulary used for the creation ofASPs consists of (i) the target verb lemma, (ii) se-lected POS tags from the Penn Treebank Tagset(Marcus et al., 1993), (iii) a list of particular func-tion words that play an important role in fine-grained subcategorisation frames of verbs (Eckle-Kohler and Gurevych, 2012) and (iv) semantic cat-egories of nouns given by WN semantic fields.
Weselected POS tags that play an important role insyntactic realisations of verbs, e.g.
POS tags forpersonal pronouns which are potential verb argu-ments.
In our experiments, we tried different setsof function words and POS tags.
For instance,we found that some function words (e.g., reflex-ive pronouns) and some POS tags (e.g., those forpast participles and comparative adjectives) intro-duced too much noise in the data and therefore wedid not select them for the final vocabulary.8In order to create SPs from sense examples,we apply POS tagging and lemmatisation usingthe TreeTagger (Schmid, 1994) and named entitytagging using the Stanford Named Entity Recog-niser (Klein et al., 2003).
The named entitytags attached by the Named Entity Recogniser aremapped to WN semantic fields.For the generation of ASPs from sense exam-ples, we used a window size of w = 7, whilethe generation of LSPs has been performed withw = 5 in order to put a focus on the closely neigh-bouring lexemes in multiword verb lemmas.
The8The vocabulary used for the creation of ASPs is availableat http://www.ukp.tu-darmstadt.de/data/.window size was set empirically using the EnglishLexical Sample task of the Senseval-2 dataset asa development set.
The same set was also usedfor the development of the linguistically motivatedvocabulary for ASPs.9From the abstract predicate-argument struc-ture information given in VN, we derived onlyASPs.
For this, we employed the subcategori-sation frames, as well as the semantic role andselectional preference information from VN, andcreated ASPs based on manually created map-pings between these information types and thecontrolled vocabulary used for ASPs.2.3 Step 2: Automated LabellingFor the automated labelling of verbs in a corpus,we first derive SPs from each corpus sentence con-taining a target verb.
SPs are derived from corpussentences by applying the same procedure as de-scribed in Step 1 for the creation of SPs from senseexamples, the window size used is w = 7.To compare two SPs, we propose a similaritymetric based on Dice?s coefficient which calcu-lates the sum of the weighted number of their com-mon bi-grams, tri-grams, and four-grams.
For-mally, the similarity score simw?
[0..1] of twoSPs p1, p2is defined as:(1) simw(p1, p2) =4?n=2|Gn(p1)?Gn(p2)|?nnormwwhere w >= 1 is the size of the window aroundthe target verb, Gn(pi), i ?
{1, 2} is the set of n-9However, the Senseval-2 data are annotated with sensekeys of the WN pre-release version 1.7 and therefore, we hadto employ an automated mapping of WN 1.7 pre-release toWN 3.0 sense keys provided by Rada Mihalcea.
Since thismapping turned out to be rather noisy, we did not use theSenseval-2 data in our evaluations.70Automated labelling of corpus instancesfor each sentence siwith verb vderive LSPiand ASPiforall j = sizeOf(UBY-LSP(v))compare LSPiwith LSPjin UBY-LSP(v):maxSim(LSPi) = argmaxjscore(LSPi, LSPj)add sense(argmaxj) to MostSimilarSenses(LSPi)forall k = sizeOf(UBY-ASP(v))compare ASPiwith ASPkin UBY-ASP(v):maxSim(ASPi) = argmaxkscore(ASPi, ASPk)add sense(argmaxk) to MostSimilarSenses(ASPi)if maxSimi,j>= threshold t andmaxSimi,j>= maxSimi,klabel(si) = random(MostSimilarSenses(LSPi))else if maxSimi,k>= threshold tlabel(si) = random(MostSimilarSenses(ASPi))end ifend forTable 2: Algorithm for labelling corpus instanceswith WordNet senses.grams occurring in SP pi, and normwis the nor-malisation factor defined by the sum of the max-imum number of common bigrams, trigrams andfourgrams in the window w. Similarity metricsbased on Dice?s coefficient have often been usedin Lesk-based WSD (Lesk, 1986) to calculate theoverlap of two sets (e.g., Baldwin et al.
(2010)).
Inour case, however, the elements of the two sets arebigrams, trigrams and fourgrams, while in Lesk-based algorithms typically sets of unigrams arecompared, thus not accounting for word order.Table 2 shows the algorithm used for automatedlabelling of corpus instances in pseudo-code.
Thealgorithm assumes that for each verb v, the corre-sponding set of SPs derived from UBY sense ex-amples (UBY-LSP(v) and UBY-ASP(v) in Table2) has already been computed.For each corpus sentence containing a targetverb v, the corresponding SPs for verb v derivedfrom UBY are scored by the similarity metric in(1).
The SPs with the maximum score that is abovea threshold t form the set of most similar senses.From this set, the algorithm picks one sense ran-domly as a label.
How often this happens, dependson the value of t: the percentage of randomly se-lected senses ranges from about 33% for t = 0.14to about 50% for t = 0.04.3 DataWeb corpora.
For the automated labelling of cor-pus data with WN senses, we use two very largeweb corpora: the English ukWaC corpus (Ba-roni et al., 2009) and the article pages extractedfrom the English Wikipedia using the Java-basedWikipedia API JWPL (Zesch et al., 2008).
Fur-ther, for the evaluation of our method, we use threemanually sense annotated data sets.SemCor.
We use the SemCor 3.0 corpus whichis annotated with WN 3.0 senses.MASC.
MASC is a balanced subset of 500Kwords of written texts and transcribed speechdrawn primarily from the Open American Na-tional Corpus (OANC).10The texts come from 19different genres which allows us to test our methodon real-life data from multiple sources.
The cor-pus is annotated with various types of linguisticinformation, including WN 3.0 sense annotationsfor instances of selected words.
Therefore, MASCis a lexical sample corpus.We extracted instances of 16 MASC verbs(11,997 instances) which have been sense anno-tated.
Most instances are annotated by multipleannotators and, to create a gold standard, we tookthe sense preferred by the majority of annotatorsand ignored instances where there were ties.Senseval-3.
In the test corpus of the Senseval-3 all-words disambiguation task sense annotationsare provided for each content word in a chunkof the WSJ corpus (5,000 words of running text).The third annotated data set for our experiment isformed by extracting all verb instances from thistest corpus.
Note that the gold standard annota-tions in Senseval-3 were made using WN 1.7.1.In our experiments, we use Rada Mihalcea?s con-version of the corpus to WN 3.0.11However, wefound out that some verb instances were convertedto sense labels that do not exist in WN 3.0.
Af-ter removing those instances, there were 305 verbswith 592 instances left.4 Experiments and EvaluationNext, we present the intrinsic and the application-based evaluations of our method.4.1 Intrinsic EvaluationWe intrinsically evaluate the performance of theautomated labelling algorithm for the Senseval-3verbs which occur in the SemCor corpus.
Occur-rences of these 152 verbs in SemCor are processed10http://www.americannationalcorpus.org/11http://www.cse.unt.edu/?rada/downloads.html#sensevalsemcor71WN?FN?WKT WN?FN?WKT?VNt Cov Cov Acc Cov Cov Acc(Inst.)
(Sense) (Inst.)
(Sense)0.04 0.55 0.27 0.32 0.48 0.25 0.350.07 0.15 0.17 0.36 0.13 0.15 0.420.1 0.11 0.14 0.35 0.10 0.13 0.420.14 0.02 0.07 0.41 0.02 0.05 0.47Table 3: Performance of the automated labellingalgorithm evaluated for occurrences of Senseval-3verbs in SemCor.by the labelling algorithm with a window sizew = 7 and the automatically annotated WN 3.0senses are compared with the gold senses availablein SemCor 3.0.Quantitative Evaluation.
We calculated theaccuracy as the percentage of correctly labelled in-stances and the instance coverage as the percent-age of labelled instances.
The sense coverage iscalculated as the percentage of all predicted (notannotated) senses relative to all gold verb sensesgiven in SemCor.A random sense baseline yields 15% accuracy.Note that a MFS baseline based on WN wouldnot be meaningful, because the WordNet MFS isbased on the frequency distribution of annotatedsenses in SemCor.Table 3 shows accuracy and coverage resultsof the automated labelling algorithm for differentvalues of the threshold t and two combinations ofsense links from UBY.
Depending on the thresholdt, 2% to 55% of the verb instances in SemCor canautomatically be labelled, and the instance cov-erage goes largely in parallel to the coverage ofpredicted WN senses.
Accuracy ranges between32% and 47% and exceeds the random sense base-line by a large margin.
Lowering the threshold in-creases the coverage of the labelling method, butit also leads to a decrease in accuracy of 9 percent-age points (12 for the configuration with VN).Adding more patterns from VN via the WN?VN alignment, leads to a decrease in both instanceand sense coverage combined with an increase inaccuracy.
Since SemCor is a rather small corpus,the increase in instance coverage is not as clearas for large Web corpora such as the ukWaC cor-pus.
Labelling a 1GB subset of the ukWaC cor-pus based on patterns derived from the WN?FN?WKT alignments resulted in 15MB of labelleddata, whereas 25MB labelled data could be createdfrom the same subset with the additional patternsfrom the WN?VN alignment.Qualitative Analysis.
In Table 4, we show ex-amples of the highest ranking patterns and the cor-responding labelled SemCor instances for sensesthat were correctly and falsely annotated.
The ex-amples in Table 4 show that the similarity metricassigns the highest values to instances where func-tion words (e.g., in, to, who) or POS tags (e.g., PP,VV) from the ASP vocabulary occur in the im-mediate neighbourhood of the target verb.
Sincesuch functions words play an important role in theASPs derived from VN, the VN ASPs possiblytend to dominate over the SPs derived from senseexamples, which explains the observed decrease incoverage (see Table 3).The falsely labelled instances turn out to be ex-amples of WN senses where the gold sense is verysimilar to the automatically attached sense as evi-dent from the synset definition given in the right-most column.4.2 Extrinsic EvaluationWe extrinsically evaluate our method for auto-mated verb sense labelling by using it for learninga classifier for VSD in a train-test setting.
We usefeatures which have been widely used in super-vised WSD systems, in particular features basedon dependency parsing.
While this might seemto be in contrast to our labelling algorithm whichis based on shallow linguistic preprocessing, it isfully justified by the purpose of our extrinsic eval-uation: The main purpose of the extrinsic evalua-tion is not to outperform state-of-the-art VSD sys-tems, but to show that, when operating with rea-sonable features, a classifier trained on the dataautomatically labelled with our method performsequally well as when this classifier is trained onmanually annotated data.4.2.1 FeaturesThe training and test data are parsed with the Stan-ford parser (Klein and Manning, 2003) which pro-vides Stanford Dependencies output (De Marneffeet al., 2006) as well as phrase structure trees.
Weemploy the Stanford Named Entity Recogniser toidentify named entities.
We then extract lexical,syntactic, and semantic features from the parse re-sults for classification.Lexical features include the lemmas and POStags of the two words before and after the tar-get verb.
To extract syntactic features we selectall dependency relations from the parser output in72SemCor instance SP derived from SemCor score WN sense ID (gold sense in brackets)Some of the New York Philharmonicmusicians who live in the suburbs spentyesterday morning digging themselvesfree from snow.of group person who livein location VVD time timeVVG0.29 live%2:42:08:: (live%2:42:08::)These societies can expect to face diffi-cult times.group expect to VV JJevent0.22 expect%2:31:01:: (expect%2:31:01::)As autumn starts its annual sweep , fewAmericans and Canadians realize howfortunate they are in having the world ?sfinest fall coloring.JJ attribute JJ person real-ize how JJ PP be in0.22 realize%2:31:00:: ?
perceive (an idea orsituation) mentally (realize%2:31:01::?
be fully aware or cognizant of)Dan Morgan told himself he would for-get Ann Turner.person person VVD PP PPforget person location0.16 forget%2:31:00:: ?
be unable to re-member (forget%2:31:01:: ?
dismissfrom the mind; stop remembering)Table 4: Examples of SemCor instances with high similarity scores (upper half shows correctly labelledinstances, lower half incorrectly labelled instances.which the target verb is related to a noun, a pro-noun, or a named entity.
For each selected word,the lemma of the word (or the named entity tag incase of proper nouns) is combined with the typeof the dependency relation which exists betweenit and the verb to form a separate feature.
In asimilar feature, the lemma of the selected word isreplaced by its POS tag.
The semantic featuresinclude all synsets found in WN for nominal argu-ments of the verb.
Personal pronouns are mappedto ?person?
and the three synsets found in WN 3.0for this word are taken as features.4.2.2 Train and Test DataUsing exactly the same method as intrinsicallyevaluated in section 4.1, we automatically labelledoccurrences of the 16 MASC verbs and the 305Senseval-3 verbs in both web corpora with WNsenses.
Only occurrences with similarity scoreabove 0.1 are labelled ?
all other occurrences arediscarded.
We refer to the resulting data as au-tomatically labelled corpus (ALC) and use it astraining data for statistical VSD.Instances of the test verbs found in SemCor arealso used as training data in order to compare theperformance of the classifier in a fully supervisedsetting.MASC.
There are 22 senses with instances inMASC which are not found in SemCor.
For theALC this number is 34.
However, in the latterthere are 27 senses, instances of which are un-seen in MASC.
20 of those represent phrasal verbswhich we attribute to the special treatment of suchverbs in our method.The classifier cannot correctly classify senseswhich are not seen in the training data.
The cov-erage of the ALC is 88.05% and that of SemCor?
94.8%.
The SemCor data can mainly covermore test instances of 3 verbs ?
launch, rule, andtransfer ?
the WN senses of which lack senseexamples or links to other senses in UBY.
Un-like the hand-labelled SemCor data, our automatedsense labelling method is limited to the informa-tion found in the LLR used.
However, there arealso 330 MASC instances covered by the ALConly.
Those are mostly instances of phrasal verbs,such as rip off and show up.
Note that the defini-tion of coverage we use here makes its values theupper bounds for the performance of the classifier.Senseval-3.
We also generated training data au-tomatically for the 305 Senseval verbs.
However,only 152 of those verbs (442 instances) are foundin SemCor.
This means we cannot train the classi-fier for the remaining Senseval verbs.
The cover-age of the SemCor training data for the 152 verbswhich can be classified is 96.15% and that of theALC ?
95.25%.
For all 592 Senseval test in-stances, the coverage of the ALC is 90.38%.4.2.3 Results and AnalysisWe trained a separate logistic regression classi-fier for each test verb in the two datasets us-ing the WEKA data mining software (Hall et al.,2009) with default parameters.
The classifierswere trained with features extracted from (i) theSemCor hand-labelled data and (ii) the ALC.MASC.
The classifier achieves 50.23% accu-racy when SemCor is used and 49% when theALC is employed.
The difference in the results isnot statistically significant at p < 0.05.
The MFS73baseline scores at 41.72%.Senseval-3.
The classifier achieves 43.24%with the ALC.
We assigned the MFS to each ofthe 143 test verbs not found in SemCor since wecannot train the classifier for those.
The achievedaccuracy is 45.2%.
We also measured accuracyin a setup where no MFS back-off strategy wasemployed for SemCor (152 test verbs with 442instances).
When trained on SemCor data, theclassifier achieves 48.64% accuracy compared to47.51% for the ALC.
All differences in the resultsare not statistically significant at p < 0.05.
Fi-nally, the MFS baseline accuracy is significantlylower at 25.34% for all 305 test verbs.For both test datasets, the overall performanceof the classifier when trained on automatically la-belled data is very close to the setting in whichmanually created training data is employed.
Wethus conclude that the quality of the data producedby our sense labelling method is sufficient andthese data can be directly used for training a statis-tical VSD classifier.
As a reference, the state-of-the-art supervised VSD system described in Chenand Palmer (2009) achieves 64.8% accuracy on theSenseval-2 fine-grained data.
However, we cannotcompare to this result due to the different sense in-ventory which the Senseval-2 data were annotatedwith.4.2.4 Sense LinksIn order to investigate the effect of LLRs, weperformed experiments in which sense examplesfound in WN only were used.
We also experi-mented with various combinations of the resourcesavailable in UBY to determine the contribution ofeach of those to our method.
Table 5 shows the re-sults.
The setting which includes only WN has theworst performance, thus clearly showing the ben-efits of using LLRs.
Next, the inclusion of WKTimproves both coverage and accuracy.
We con-clude that WKT plays an important role in discov-ering additional verb senses.
Finally, similarly tothe results of the intrinsic evaluation, adding VNto the mix increases slightly the coverage but de-creases accuracy.5 Related Work and DiscussionOur work is related to previous research on(i) using a combination of lexical resources forknowledge-based WSD, (ii) using lexical re-sources for distant supervision, and (iii) the auto-mated acquisition of sense-annotated data.MASC SensevalCov Acc Cov AccWN 0.6573 0.3498 0.6372 0.3209WN?FN 0.8562 0.4810 0.8812 0.4172WN?FN?WKT 0.8805 0.4900 0.9038 0.4324WN?FN?WKT?VN 0.8822 0.4688 0.9139 0.4054Table 5: Performance of the various combinationsof lexical resources.Knowledge-based WSD.
While the combina-tion of sense-annotated data and wordnets hasbeen described for knowledge-based WSD before(e.g., Navigli and Velardi (2005; Agirre and Soroa(2009) who use graph algorithms), only recentlyPonzetto and Navigli (2010) have investigated theimpact of the combination of different lexical re-sources on the performance of WSD.
They alignedWN senses with Wikipedia articles and employedtwo simple knowledge-based algorithms, i.e., aLesk-based algorithm and a graph-based algo-rithm, to evaluate the resulting LLR for WSD.While their evaluation demonstrates that the useof an LLR boosts the performance of knowledege-based WSD, it is restricted to nouns only sinceWikipedia provides very few verb senses.
More-over, lexical resources that are rich in lexical-syntactic information such as VN have not beeninvolved.Miller et al.
(2012) employ a Lesk-based algo-rithm which makes use of a combination of WNand an automatically acquired distributional the-saurus.
Lesk-based algorithms play a central rolein knowledge-based WSD.
Based on the overlapof the context of the target word and sense defi-nitions in a given sense inventory, they assign thesense with the highest overlap as disambiguationresult.
We were kindly provided with the systemdescribed in Miller et al.
(2012) and we were ableto test its performance on our test sets.
The sys-tem achieved only 33.86% and 30.16% accuracyfor the MASC and the Senseval-3 verbs, respec-tively, which is far below the results we presented.This low performance is due to the fact that Lesk-based algorithms do not account for word order.Such information is important especially for verbsenses, as the syntactic behaviour of a verb reflectsaspects of its meaning.Distant supervision.
Distant supervision isa learning paradigm similar to semi-supervisedlearning.
Unlike semi-supervised methods whichtypically employ a supervised classifier and a74small number of seed instances to do bootstraplearning (Yarowsky, 1995; Mihalcea, 2004; Fujitaand Fujino, 2011), in distant supervision trainingdata are created in a single run from scratch byaligning corpus instances with entries in a knowl-edge base.
Distant supervision methods that haveused LLRs as knowledge bases have been previ-ously applied in relation extraction, e.g.
Freebase(Mintz et al., 2009; Surdeanu et al., 2012) and Ba-belNet (Krause et al., 2012; Moro et al., 2013).However, as far as we are aware, we are the first toapply distant supervision to the task of verb sensedisambiguation.Acquisition of sense-annotated data.
Mostprevious work on using lexical resources for au-tomatically acquiring sense-annotated data eitherwas mostly restricted to noun senses or, unlikeus, did not present a meaningful evaluation.
Lea-cock et al.
(1998) describe the automated creationof training data for supervised WSD on the ba-sis of WN as a lexical resource combined withcorpus statistics, but they evaluate their approachjust on one noun, verb, and adjective, and thusit is unclear whether their results can be gener-alized.
Cuadros and Rigau (2008) used the ap-proach of Leacock et al.
(1998) to automaticallybuild a large KnowNet from the Web, but theyevaluated this resource only for WSD of nouns.However, the system based on KnowNet yields re-sults below the SemCor-MFS baseline.
Mihalceaand Moldovan (1999) use WordNet glosses to ex-tract sense examples from the Web via a search en-gine and use this approach in a subsequent paper(Mihalcea, 2002) to generate a sense tagged cor-pus.
For five randomly selected nouns, they per-formed a comparative evaluation of a WSD classi-fier trained on an automatically tagged corpus onthe one hand, and on the manually annotated datafrom the Senseval-2 English lexical sample taskon the other hand.
The results obtained for thesefive nouns seem to be similar but the dataset usedis too small to draw meaningful conclusions andmoreover, it does not cover verbs.
Mostow andDuan (2011) presented a system that extracts ex-ample contexts for nouns and apply these contextsin (Duan and Yates, 2010) for WSD by using themto label text and train a statistical classifier.
Anevaluation of this classifier yielded results similarto those obtained by a supervised WSD system.K?ubler and Zhekova (2009) extract examplesentences from several English dictionaries andvarious types of corpora, including web corpora.They employ a Lesk-based algorithm to automati-cally annotate the target word instances in the ex-tracted example sentences with WN senses anduse them in one of their experiments as train-ing data for a WSD classifier.
However, the per-formance of the system decreased significantlyachieving the lowest accuracy among all systemconfigurations.
The authors provide only the over-all accuracy score, so we do not know how disam-biguation of verbs was affected.Summary.
We consider the ability to estab-lish a link between the rich knowledge available inLLRs and corpora of any kind to be the main ad-vantage of our automated labelling method.
How-ever, to automatically label a suffcient amountof data for supervised learning, very large cor-pora are required.
Our method can be extendedto other POS (using sense examples and possiblyother types of lexical information), as well as toother languages where (linked) lexical resourcesare available.6 ConclusionIn this paper, we presented a novel method for cre-ating sense labelled corpora automatically.
We ex-ploit LLRs and perform large-scale intrinsic andapplication-based evaluations.
The results of thoseevaluations show that the quality of the sense la-belled corpora created with our method matchesthat of manually annotated corpora.In future research, we plan to use PropBank(Palmer et al., 2005) in order to extract senseexamples for VN as well.
This might improvethe performance of lexical resource combinationswhich include VN.
We will also apply our methodto languages (e.g., German) for which lexical re-sources are available but no or little sense anno-tated corpora exist.AcknowledgmentsThis work has been supported by the VolkswagenFoundation as part of the Lichtenberg- Professor-ship Program under grant No.
I/82806 and by theGerman Research Foundation under grant No.
GU798/9-1.
We would like to thank the anonymousreviewers for their valuable feedback.75ReferencesEneko Agirre and Aitor Soroa.
2009.
PersonalizingPageRank for Word Sense Disambiguation.
In Pro-ceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL 2009), pages 33?41, Athens, Greece.C.F.
Baker, C.J.
Fillmore, and J.B. Lowe.
1998.
TheBerkeley FrameNet project.
In Proceedings of the36th Annual Meeting of the Association for Compu-tational Linguistics and 17th International Confer-ence on Computational Linguistics-Volume 1, pages86?90, Montreal, Canada.Timothy Baldwin, Sunam Kim, Francis Bond, SanaeFujita, David Martinez, and Takaaki Tanaka.
2010.A Reexamination of MRD-Based Word Sense Dis-ambiguation.
ACM Transactions on Asian Lan-guage Information Processing (TALIP), 9(1):4:1?4:21.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky wideweb: a collection of very large linguistically pro-cessed web-crawled corpora.
Language Resourcesand Evaluation, 43(3):209?226.Jinying Chen and Martha Palmer.
2009.
Improv-ing English verb sense disambiguation performancewith linguistically motivated features and clear sensedistinction boundaries.
Language Resources andEvaluation, 43:181?208.Montse Cuadros and German Rigau.
2008.
Knownet:Building a large net of knowledge from the web.In 22nd International Conference on ComputationalLinguistics (COLING), pages 161?168, Manchester,UK.M.C.
De Marneffe, B. MacCartney, and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In Proceedings of the 5th In-ternational Conference on Language Resources andEvaluation (LREC 2006), pages 449?454, Genoa,Italy.Weisi Duan and Alexander Yates.
2010.
Extractingglosses to disambiguate word senses.
In HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, HLT ?10, pages627?635, Los Angeles, USA.Judith Eckle-Kohler and Iryna Gurevych.
2012.Subcat-LMF: Fleshing out a standardized format forsubcategorization frame interoperability.
In Pro-ceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL 2012), pages 550?560, Avignon,France.Judith Eckle-Kohler, Iryna Gurevych, Silvana Hart-mann, Michael Matuschek, and Christian M. Meyer.2012.
UBY-LMF ?
A uniform format for standard-izing heterogeneous lexical-semantic resources inISO-LMF.
In Proceedings of the 8th InternationalConference on Language Resources and Evaluation(LREC 2012), pages 275?282, Istanbul, Turkey.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, MA,USA.Sanae Fujita and Akinori Fujino.
2011.
Word sensedisambiguation by combining labeled data expan-sion and semi-supervised learning method.
In Pro-ceedings of the 5th International Joint Conferenceon Natural Language Processing, pages 676?685,Chiang Mai, Thailand.Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-mann, Michael Matuschek, Christian M. Meyer, andChristian Wirth.
2012.
UBY - a large-scale uni-fied lexical-semantic resource based on LMF.
InProceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Lin-guistics (EACL 2012), pages 580?590.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.ACM SIGKDD Explorations Newsletter, 11(1):10?18.Karin Kipper, Anna Korhonen, Neville Ryant, andMartha Palmer.
2008.
A large-scale classificationof English verbs.
Language Resources and Evalua-tion, 42:21?40.D.
Klein and C.D.
Manning.
2003.
Accurate un-lexicalized parsing.
In Proceedings of the 41stAnnual Meeting on Association for ComputationalLinguistics-Volume 1, pages 423?430, Sapporo,Japan.
Association for Computational Linguistics.Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-pher D. Manning.
2003.
Named entity recognitionwith character-level models.
In Proceedings of theSeventh Conference on Natural Language Learningat HLT-NAACL 2003, pages 180?183, Edmonton,Canada.Sebastian Krause, Hong Li, Hans Uszkoreit, and FeiyuXu.
2012.
Large-scale learning of relation-extraction rules with distant supervision from theweb.
In Proceedings of the 11th International Se-mantic Web Conference, pages 263?278, Boston,Masachusetts, USA, 11.
Springer.Sandra K?ubler and Desislava Zhekova.
2009.
Semi-Supervised Learning for Word Sense Disambigua-tion: Quality vs.
Quantity.
In Proceedings of theInternational Conference RANLP-2009, pages 197?202, Borovets, Bulgaria.Claudia Leacock, George A. Miller, and MartinChodorow.
1998.
Using corpus statistics and word-net relations for sense identification.
ComputationalLinguistics, 24(1):147?165.76Michael Lesk.
1986.
Automatic sense disambigua-tion using machine readable dictionaries: how to tella pine cone from an ice cream cone.
In Proceed-ings of the 5th Annual International Conference onSystems Documentation, pages 24?26, Toronto, On-tario, Canada.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: the penn treebank.
Compu-tational Linguistics, 19(2):313?330.Rada Mihalcea and Dan Moldovan.
1999.
An auto-matic method for generating sense tagged corpora.In Proceedings of the American Association for Ar-tificial Intelligence (AAAI 1999), Orlando, Florida,USA.Rada Mihalcea.
2002.
Bootstrapping large sensetagged corpora.
In Proceedings of the Third Interna-tional Conference of Language Resources and Eval-uation (LREC 2002), pages 1407?1411, Las Palmas,Canary Islands, Spain.Rada Mihalcea.
2004.
Co-training and self-trainingfor word sense disambiguation.
In Proceedingsof the Conference on Computational Natural Lan-guage Learning (CoNLL-2004), Boston, MA, USA.Tristan Miller, Chris Biemann, Torsten Zesch, andIryna Gurevych.
2012.
Using distributional similar-ity for lexical expansion in knowledge-based wordsense disambiguation.
In Proceedings of the 24thInternational Conference on Computational Lin-guistics (COLING 2012), pages 1781?1796, Mum-bai, India.Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages1003?1011, Suntec, Singapore.Andrea Moro, Hong Li, Sebastian Krause, Feiyu Xu,Roberto Navigli, and Hans Uszkoreit.
2013.
Se-mantic rule filtering for web-scale relation extrac-tion.
In Proceedings of the 12th InternationalSemantic Web Conference, Sydney, Australia, 10.Springer.Jack Mostow and Weisi Duan.
2011.
Generating ex-ample contexts to illustrate a target word sense.
InProceedings of the Sixth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 105?110, Portland, Oregon, June.
Associationfor Computational Linguistics.Roberto Navigli and Simone Paolo Ponzetto.
2012.BabelNet: The automatic construction, evaluationand application of a wide-coverage multilingual se-mantic network.
Artificial Intelligence, 193:217?250.Roberto Navigli and Paola Velardi.
2005.
Structuralsemantic interconnections: A knowledge-based ap-proach to word sense disambiguation.
IEEE Trans-actions on Pattern Analysis and Machine Intelli-gence, 27(7):1075?1086.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The Proposition Bank: A corpus annotated with se-mantic roles.
Computational Linguistics, 31(1):71?105.Simone Paolo Ponzetto and Roberto Navigli.
2010.Knowledge-rich word sense disambiguation rivalingsupervised systems.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics (ACL 2010), pages 1522?1531, Uppsala,Sweden.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, pages 44?49, Manchester, UK.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,and Christopher D. Manning.
2012.
Multi-instancemulti-label learning for relation extraction.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 455?465, Jeju Island, Korea.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Pro-ceedings of the 33rd annual meeting on Associa-tion for Computational Linguistics, pages 189?196,Cambridge, Massachusetts, USA.Torsten Zesch, Christof M?uller, and Iryna Gurevych.2008.
Extracting lexical semantic knowledge fromwikipedia and wiktionary.
In Proceedings of the 6thInternational Conference on Language Resourcesand Evaluation (LREC 2008), volume 8, pages1646?1652, Marrakech, Morocco.77
