Term Recognition Using Technical Dictionary HierarchyJong-Hoon Oh, KyungSoon Lee, and Key-Sun ChoiComputer Science Dept., Advanced Information TechnologyResearch Center (AITrc), andKorea Terminology Research Center for Language and Knowledge Engineering (KORTERM)Korea Advanced Institute of Science & Technology (KAIST)Kusong-Dong, Yusong-Gu Taejon, 305-701 Republic of Korea{rovellia,kslee,kschoi}@world.kaist.ac.krAbstractIn recent years, statistical approaches onATR (Automatic Term Recognition) haveachieved good results.
However, there arescopes to improve the performance inextracting terms still further.
For example,domain dictionaries can improve theperformance in ATR.
This paper focuses ona method for extracting terms using adictionary hierarchy.
Our method producesrelatively good results for this task.IntroductionIn recent years, statistical approaches on ATR(Automatic Term Recognition) (Bourigault,1992; Dagan et al 1994; Justeson and Katz,1995; Frantzi, 1999) have achieved good results.However, there are scopes to improve theperformance in extracting terms still further.
Forexample, the additional technical dictionariescan be used for improving the accuracy inextracting terms.
Although, the hardship onconstructing an electronic dictionary was majorobstacles for using an electronic technicaldictionary in term recognition, the increasingdevelopment of tools for building electroniclexical resources makes a new chance to usethem in the field of terminology.
From theseendeavour, a number of electronic technicaldictionaries (domain dictionaries) have beenacquired.Since newly produced terms are usually madeout of existing terms, dictionaries can be used asa source of them.
For example, ?distributeddatabase?
is composed of ?distributed?
and?database?
that are terms in a computer sciencedomain.
Further, concepts and terms of a domainare frequently imported from related domains.For example, the term ?GeographicalInformation System (GIS)?
is used not only in acomputer science domain, but also in anelectronic domain.
To use these properties, it isnecessary to build relationships betweendomains.
The hierarchical clustering methodused in the information retrieval offers a goodmeans for this purpose.
A dictionary hierarchycan be constructed by the hierarchical clusteringmethod.
The hierarchy helps to estimate therelationships between domains.
Moreover theestimated relationships between domains can beused for weighting terms in the corpus.
Forexample, a domain of electronics may have adeep relationship to that of computer science.
Asa result, terms in the dictionary of electronicsdomain have a higher probability to be terms ofcomputer science domain than terms in thedictionary of others do (Felber, 1984).The recent works on ATR identify thecandidate terms using shallow syntacticinformation and score the terms using statisticalmeasure such as frequency.
The candidate termsare ranked by the score and are truncated by thethresholds.
However, the statistical methodsolely may not give accurate performance incase of small sized corpora or very specializeddomains, where the terms may not appearrepeatedly in the corpora.In our approach, a dictionary hierarchy isused to avoid these limitations.
In the nextsection, we describe the overall methoddescription.
In section 2, section 3, and section 4,we describe primary methods and its details.
Insection 5, we describe experiments and results1 Method DescriptionThe description of the proposed method isshown in figure 1.
There are three main steps inour method.
In the first stage, candidate termsthat are complex nominal are extracted by alinguistic filter and a dictionary hierarchy isconstructed.
In the second stage, candidate termsare scored by each weighting scheme.
Indictionary weighing scheme, candidate terms arescored based on the kind of domain dictionarywhere terms appear.
In statistical weightingscheme, terms are scored by their frequency inthe given corpus.
In transliterated wordweighting scheme, terms are scored by thenumber of transliterated foreign words in theterms.
In the third stage, each weight isnormalized and combined to Term weight(Wterm), and terms are extracted by Term weight.Figure 1.
The method description2 Dictionary Hierarchy2.1 ResourceFieldAgrochemical, Aerology, Physics, Biology,Mathematics, Nutrition, Casting, Welding,Dentistry, Medical, Electronical engineering,Computer science, Electronics, Chemicalengineering, Chemistry.... and so on.Table 1.
The fragment of a list: dictionaries ofdomains used for constructing the hierarchy.A dictionary hierarchy is constructed usingbi-lingual dictionaries (English to Korean) of thefifty-seven domains.
Table 1 lists the domainsthat are used for constructing the dictionaryhierarchy.
The dictionaries belong to domains ofscience and technology.
Moreover, terms that donot appear in any dictionary (henceforth we callthem unregistered terms) are complemented by adomain tagged corpus.
We use a corpus, calledETRI-KEMONG test collection, with thedocuments of seventy-six domains tocomplement unregistered terms and to eliminatecommon term.2.2 Constructing Dictionary HierarchyThe clustering method is used for constructinga dictionary hierarchy.
The clustering is astatistical technique to generate a categorystructure using the similarity betweendocuments (Anderberg, 1973).
Among theclustering methods, a reciprocal nearestneighbor (RNN) algorithm (Murtaugh, 1983)based on a hierarchical clustering model is used,since it joins the cluster minimizing the increasein the total within-group error sum of squares ateach stage and tends to make a symmetrichierarchy (Lorr, 1983).
The algorithm to form acluster can be described as follows:1.
Determine all inter-object (orinter-dictionary) dissimilarity.2.
Form cluster from two closest objects(dictionaries) or clusters.3.
Recalculate dissimilarities between newcluster created in the step2 and otherobject (dictionary) or cluster alreadymade.
(all other inter-point dissimilaritiesare unchanged).4.
Return to Step2, until all objects(including cluster) are in the one cluster.In the algorithm, all objects are treated as avector such as Di = (xi1, xi2, ... , xiL ).
In the step1, inter-object dissimilarity is calculated basedon the Euclidian distance.
In the step2, theclosest object is determined by a RNN.
Forgiven object i and object j, we can define thatthere is a RNN relationship between i and jwhen the closest object of i is object j and theclosest object of j is object i.
This is the reasonwhy the algorithm is called a RNN algorithm.
Adictionary hierarchy is constructed by thealgorithm, as shown in figure 2.
There are tendomains in the hierarchy ?
this is a fragment ofwhole hierarchy.TechnicalDictionariesDomaintaggedDocuments?.A CB D ?.ConstructinghierarchyPOS-taggedCorpus Linguistic filterAbbreviation andTranslation pairsextractionCandidate termFrequency basedWeighingTransliteratedWord detectionTransliterated wordBased WeightingComplementUnregistered TermScoring by hierarchyEliminateCommon WordDictionary basedWeightingStatisticalWeightTransliteratedWord WeightDictionaryWeightTerm RecognitionFigure 2.
The fragment of whole dictionaryhierarchy : The hierarchy shows that domainsclustered in the terminal node such as chemicalengineering and chemistry are highly related.2.3 Scoring Terms Using DictionaryHierarchyThe main idea for scoring terms using thehierarchy is based on the premise that terms inthe dictionaries of the target domain and termsin the dictionary of the domain related to thetarget domain act as a positive indicator forrecognizing terms.
Terms in the dictionaries ofthe domains that are not related to the targetdomain act as a negative indicator forrecognizing terms.
We apply the premise forscoring terms using the hierarchy.
There arethree steps to calculate the score.1.
Calculating the similarity between thedomains using the formula (2.1) (Maynardand Ananiadou, 1998)whereDepthi: the depth of the domaini node in thehierarchyCommonij: the depth of the deepest nodesharing between the domaini and thedomainj in the path from the root.In the formula (2.1), the depth of the nodeis defined as a distance from the root ?
thedepth of a root is 1.
For example, let theparent node of C1 and C8 be the root ofhierarchy in figure 2.
The similarity between?Chemistry?
and ?Chemical engineering?
iscalculated as shown below in table 2:Domain Chemistry ChemicalEngineeringPath fromthe rootRoot->C8->C9->ChemistryRoot->C8->C9->ChemicalEngineeringDepthi 4 4Common ij 3 3Similarityij2*3/(4+4) =0.75 2*3/(4+4) =0.75Table 2.
Similarityij  calculation: The table showsan example in caculating similarity using formula(2.1).
In the example, Chemical engineeringdomain and Chemistry domain are used.
Path,Depth, and Common are calculated according tofigure 1.
Then similarity between domains aredetermined to 0.75.2.Term scoring by distance between a targetdomain and domains where terms appear:whereN: the number of dictionaries where aterm appearSimilarityti: the similarity between thetarget domain and the domain dictionarywhere a term appearsFor example, in figure 2, let the targetdomain be physics and a term ?radioactive?appear in physics, chemistry and astronomydomain dictionaries.
Then similarity betweenphysics and the domains where the term?radioactive?
appears can be estimated byformula (2.1) as shown below.
Finally,Score(radioactive) is calculated by formula(2.2) ?
score is (0.4+1+0.7)/3.
:N 3similarity physics-chemistry 0.4similarity physics-physics 1similarity physics-astronomy 0.7Score(radioactive) 2.1*1/3 = 0.7Table 3.
Scoring terms based on similaritybetween domains3.
Complementing unregistered terms andcommon terms by domain tagged corpora.
)1.2(2jiijij depthdepthCommonsimilarity+?=)2.2(1)(1?==NitisimilarityNtermScorewhereW: the number of words in the term ??
?dofi: the number of domain that words inthe term appear in the domain taggedcorpus.Consider two exceptional possible cases.
First,there are unregistered terms that are notcontained in any dictionaries.
Second, somecommonly used terms can be used to describe aspecial concept in a specific domain dictionary.Since an unregistered term may be a newlycreated term of domains, it should be consideredas a candidate term.
In contrast with anunregistered term, common terms should beeliminated from candidate terms.
Therefore, thescore calculated in the step 2 should becomplemented for these purposes.
In our method,the domain tagged corpus (ETRI 1997) is used.Each word in the candidate terms ?
they arecomposed of more than one word ?
can appearin the domain tagged corpus.
We can count thenumber of domains where the word appears.
Ifthe number is large, we can determine that theword have a tendency to be a common word.
Ifthe number is small, we can determine that theword have a high probability to be a valid term.In this paper, the score calculated by thedictionary hierarchy is called Dictionary Weight(WDic).3.
Statistical MethodThe statistical method is divided into twoelements.
The first element, the StatisticalWeight, is based on the frequencies of terms.The second element, the Transliterated wordWeight, which is based on the number oftransliterated foreign word in the candidate term.This section describes the above two elements.3.1.
Statistical Weight: Frequency BasedWeightIn the Statistical Weight, not onlyabbreviation pairs and translation pairs in aparenthetical expression but also frequencies ofterms are considered.
Abbreviation pairs andtranslation pairs are detected using the followingsimple heuristics:For a given parenthetical expression A(B),1.
Check on a fact that A and B areabbreviation pairs.
The capital letter of A iscompared with that of B.
If the half of thecapital letter are matched for each othersequentially, A and B are determined toabbreviation pairs (Hisamitsu et.
al, 1998).For example, ?ISO?
and ?InternationalStandardization Organization?
is detected asan abbreviation in a parenthetical expression?ISO (International StandardizationOrganization)?.2.
Check on a fact that A and B are translationpairs.
Using the bi-lingual dictionary, it isdetermined.After detecting abbreviation pairs andtranslation pairs, the Statistical Weight (WStat) ofthe terms is calculated by the formula (3.1).where?
: a candidate term|?|: the length of a term??
?S (?
): abbreviation and translation pairs of???T(?
): The set of candidate terms that nest???f(?
): the frequency of ??
?C(T(?
)): The number of elements in T(?
)In the formula (3.1), the nested relation isdefined as follows: let A and B be a candidateterm.
If A contains B, we define that A nests B.The formula implies that abbreviation pairsand translation pairs related to ???
is counted aswell as ???
itself and productivity of words inthe nested expression containing ???
gives moreweight, when the generated expression contains???.
Moreover, formula (1) deals with a single-word term, since an abbreviation such as GUI(Graphical User Interface) is single word termand English multi-word term usually translatedto Korean single-word term ?
(e.g.
distributeddatabase => bunsan deitabeisu))3.2(*)1)(()( 1WdofScoreWWiiDic?=+= ??
( )?????????????????????????????+??=????????}{)()(}{)()1.3())(()()()()(???????????????
?STSStatotherwiseTCffnestedisiffW3.2 Transliterated word Weight: ByAutomatic Extraction of TransliteratedwordsTechnical terms and concepts are created inthe world that must be translated or transliterated.Transliterated terms are one of important cluesto identify the terms in the given domain.
Weobserve dictionaries of computer science andchemistry domains to investigate thetransliterated foreign words.
In the result ofobservation, about 53% of whole entries in adictionary of a computer science domain aretransliterated foreign words and about 48% ofwhole entries in a dictionary of a chemistrydomain are transliterated foreign words.
Becausethere are many possible transliterated forms andthey are usually unregistered terms, it is difficultto detect them automatically.In our method, we use HMM (Hidden MarkovModel) for this task (Oh, et al, 1999).
The mainidea for extracting a foreign word is that thecomposition of foreign words would be differentfrom that of pure Korean words, since thephonetic system for the Korean language isdifferent from that of the foreign language.Especially, several English consonants thatoccur frequently in English words, such as?p?, ?t?, ?c?, and ?f?, are transliterated into Koreanconsonants ?p?, ?t?, ?k?, and ?p?
respectively.Since these consonants of Korean are not used inpure Korean words frequently, this property canbe used as an important clue for extracting aforeign word from Korean.
For example, in aword, ?si-seu-tem?
(system), the syllable ?tem?have a high probability to be a syllable oftransliterated foreign word, since the consonantof ?t?
in the syllable ?tem?
is usually not used ina pure Korean word.
Therefore, the consonantinformation which is acquired from a corpus canbe used to determine whether a syllable in thegiven term is likely to be the part of a foreignword or not.Using HMM, a syllable is tagged with ?K?
or?F?.
A syllable tagged with ?K?
means that it ispart of a pure Korean word.
A syllable taggedwith ?F?
means that it is part of a transliteratedword.
For example, ?si-seu-tem-eun (system is)?is tagged with  ?si/F + seu/F + tem/F + eun/K?.We use consonant information to detect atransliterated word like lexical information inpart-of-speech-tagging.
The formula (3.2) isused for extracting a transliterated word and theformula (3.3) is used for calculating theTransliterated Word Weight (WTrl).
The formula(3.3) implies that terms have more transliteratedforeign words than common words do.wheresi: i-th consonant in the given word.ti: i-th tag (?F?
or ?K?)
of the syllable in thegiven word.where|?| is the number of words in the term ?trans(?)
is the number of transliteratedwords in the term ?4.Term WeightingThe three individual weights described aboveare combined according to the followingformula (4.1) called Term Weight (WTerm) foridentifying the relevant terms.Where?
: a candidate term ??
?f,g,h : normalization function?+?+?
= 1In the formula (4.1), the three individualweights are normalized by the function f, g, andh respectively and weighted parameter ?,?, and?.
The parameter ?,?, and ?
are determined byexperiment with the condition ?+?+?
= 1.
Eachvalue which is used in this paper is ?=0.6, ?=0.1, and ?=0.3 respectively.)3.3()()(??
?transWTrl =)2.3()|(),|()|()()()|(1321121????????????=??==?
?niiiniiii tsptttpttptpSPSTP)1.4())(())(())(()(???????StatTrlDictermWhWgWfW?+?+?=5.
ExperimentThe proposed method is tested on a corpus ofcomputer science domains, called the KT testcollection.
The collection contains 4,434documents and 67,253 words and containsdocuments about the abstract of the paper (Park.et al, 1996).
It was tagged with a part-of-speechtagger for evaluation.
We examined theperformance of the Dictionary Weight (WDic) toshow its usefulness.
Moreover, we examinedboth the performance of the C-value that isbased on the statistical method (Frantzi.
et al,1999) and the performance of the proposedmethod.5.1 Evaluation CriteriaTwo domain experts manually carry out theassessment of the list of terms extracted by theproposed method.
The results are accepted as thevalid term when both of the two experts agree onthem.
This prevents the evaluation from beingcarried out subjectively, when one expertassesses the results.
The results are evaluated bya precision rate.
A precision rate means that theproportion of correct answers to the extractedresults by the system.5.2 Evaluation by Dictionary Weight(WDic)In this section, the evaluation is performedusing only WDic to show the usefulness of adictionary hierarchy to recognize the relevantterms The Dictionary Weight is based on thepremise that the information of the targetdomain is a good indicator for identifying terms.The term in the dictionaries of the target domainand the domain related to the target domain actsas a positive indicator for recognizing terms.The term in the dictionaries of the domains,which are not related to the target domain acts asa negative indicator for recognizing terms.
Thedictionary hierarchy is constructed to estimatethe similarity between one domain and another.Top 10% Bottom 10%The Valid Term 94% 54.8%Non-Term 6% 45.2%Table 4.  terms and non-terms by DictionaryWeightThe result, depicted in table 4, can beinterpreted as follows: In the top 10% of theextracted terms, 94% of them are the valid termsand 6% of them are non-terms.
In the bottom10% of the extracted terms, 54.8% of them arethe valid terms and 45.2% of them are non-terms.This means that the relevant terms are muchmore than non-terms in the top 10% of the result,while non-terms are much more than therelevant terms in the bottom 10% of the result.The results are summarized as follow:!
"According as a term has a highDictionary Weight (WDic), it is aptto be valid.!
"More valid terms have a highDictionary Weight (WDic) thannon-terms do5.3 Overall PerformanceTable 5 and figure 3 show the performance ofthe proposed method and of the C-value method.By dividing the ranked lists into 10 equalsections, the results are compared.
Each sectioncontains the 1291 terms and is evaluatedindependently.C-value The proposedmethodSection # oftermPrecision # oftermPrecision1 1181 91.48% 1241 96.13%2 1159 89.78% 1237 95.82%3 1207 93.49% 1213 93.96%4 1192 92.33% 1174 90.94%5 1206 93.42% 1154 89.39%6 981 75.99% 1114 86.29%7 934 72.35% 1044 80.87%8 895 69.33% 896 69.40%9 896 69.40% 780 60.42%10 578 44.77% 379 29.36%Table 5.
Precision rates of C-value and theproposed method : Section contain 1291 terms andprecision is evaluated independently.
For example,in section 1, since there are 1291 candidate termsand 1241 relevant terms by the proposed method,the precision rate in section 1 is 96.13% .The result can be interpreted as follows.
In thetop sections, the proposed method shows thehigher precision rate than the C-value does.
Thedistribution of valid terms is also better for theproposed method, since there is a downwardtendency from section 1 to section 10.
Thisimplies that the terms with higher weight scoredby our method have a higher probability to bevalid terms.
Moreover, the precision rate of ourmethod shows the rapid decrease from section 6to section 10.
This indicates that most of validterms are located in the top sections.20%30%40%50%60%70%80%90%100%1 2 3 4 5 6 7 8 9 10SectionPrecisionThe Proposed method C-valueFigure 2.
The performance of C-value and theproposed method in each sectionThe results can be summarized as follow :!
"The proposed method extracts a validterm more accurate than C-value does.!
"Most of the valid terms are in the topsection extracted by the proposedmethod.ConclusionIn this paper, we have described a method forterm extraction using a dictionary hierarchy.
It isconstructed by clustering method and is used forestimating the relationships between domains.Evaluation shows improvement over the C-value.Especially, our approach can distinguish thevalid terms efficiently ?
there are more validterms in the top sections and less valid terms inthe bottom sections.
Although the methodtargets Korean, it can be applicable to Englishby slight change on the Tweight (WTrl).However, there are many scopes for furtherextensions of this research.
The problems ofnon-nominal terms (Klavans and Kan, 1998),term variation (Jacquemin et al, 1997), andrelevant contexts (Maynard and Ananiadou,1998), can be considered for improving theperformance.
Moreover, it is necessary to applyour method to practical NLP systems, such as aninformation retrieval system and amorphological analyser.AcknowledgementsKORTERM is sponsored by the Ministry of Cultureand Tourism under the program of King SejongProject.
Many fundamental researches are supportedby the fund of Ministry of Science and Technologyunder a project of plan STEP2000.
And this workwas partially supported by the KOSEF through the?Multilingual Information Retrieval?
project at theAITrc.ReferencesAnderberg, M.R.
(1973) Cluster Analysis forApplications.
New York: AcademicBourigault, D. (1992) Surface grammatical analysisfor the extraction of terminological noun phrases.In Proceedings of the 14th International Conferenceon Computational Linguistics, COLING?92 pp.977-981.Dagan, I. and K. Church.
(1994) Termight:Identifying and terminology In Proceedings of the4th Conference on Applied Natural LanguageProcessing, Stuttgart/Germany, 1994.
Associationfor Computational Linguistics.ETRI (1997) Etri-Kemong setFelber Helmut (1984) Terminology Manual,International Information Centre for Terminology(Infoterm)Frantzi, K.T.
and S.Ananiadou (1999) TheC-value/NC-value domain independent method formulti-word term extraction.
Journal of NaturalLanguage Processing, 6(3) pp.
145-180Hisamitsu, Toru and Yoshiki Niwa (1998) Extractionof useful terms from parenthetical expressions byusing simple rules and statistical measures.
In FirstWorkshop on Computational TerminologyComputerm?98, pp 36-42Jacquemin, C., Judith L.K.
and Evelyne, T. (1997)Expansion of Muti-word Terms for indexing andRetrieval Using Morphology and Syntax, 35thAnnual Meeting of the Association forComputational Linguistics, pp 24-30Justeson, J.S.
and S.M.
Katz (1995) Technicalterminology : some linguistic properties and analgorithm for identification in text.
NaturalLanguage Engineering, 1(1) pp.
9-27Klavans, J. and Kan M.Y (1998) Role of Verbs inDocument Analysis, In Proceedings of the 17thInternational Conference on ComputationalLinguistics, COLING?98 pp.
680-686.Lauriston, A.
(1996) Automatic Term Recognition :performance of Linguistic and StatisticalTechniques.
Ph.D. thesis, University of ManchesterInstitute of Science and Technology.Lorr, M. (1983) Cluster Analysis and Its Application,Advances in Information System Science,8 ,pp.169-192Murtagh, F. (1983) A Survey of Recent Advances inHierarchical Clustering Algorithms, ComputerJournal, 26, 354-359Maynard, D. and Ananiadou, S. (1998) AcquiringContext Information for Term Disambiguation InFirst Workshop on Computational TerminologyComputerm?98, pp 86-90Oh, J.H.
and K.S.
Choi (1999) Automatic extractionof a transliterated foreign word using hiddenmarkov model , In Proceedings of the 11th Koreanand Processing of Korean Conference pp.
137-141(In Korean).Park, Y.C., K.S.
Choi, J.K.Kim and Y.H.
Kim (1996).Development of the KT test collection forresearchers in information retrieval.
In the 23thKISS Spring Conference (in Korean)
