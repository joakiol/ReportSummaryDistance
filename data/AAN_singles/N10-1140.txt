Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966?974,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAccurate Non-Hierarchical Phrase-Based TranslationMichel Galley and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305{mgalley,manning}@cs.stanford.eduAbstractA principal weakness of conventional (i.e.,non-hierarchical) phrase-based statistical machinetranslation is that it can only exploit continuousphrases.
In this paper, we extend phrase-baseddecoding to allow both source and target phrasaldiscontinuities, which provide better generalizationon unseen data and yield significant improvementsto a standard phrase-based system (Moses).More interestingly, our discontinuous phrase-based system also outperforms a state-of-the-arthierarchical system (Joshua) by a very significantmargin (+1.03 BLEU on average on five Chinese-English NIST test sets), even though both Joshuaand our system support discontinuous phrases.Since the key difference between these two systemsis that ours is not hierarchical?i.e., our systemuses a string-based decoder instead of CKY, and itimposes no hard hierarchical reordering constraintsduring training and decoding?this paper setsout to challenge the commonly held belief thatthe tree-based parameterization of systems suchas Hiero and Joshua is crucial to their goodperformance against Moses.1 IntroductionPhrase-based machine translation models (Och andNey, 2004) advanced the state of the art by extend-ing the basic translation unit from words to phrases.By conditioning translations on more than a sin-gle word, a statistical machine translation (SMT)system benefits from the larger context of a phrasepair to properly handle multi-word units and lo-cal reorderings.
Experimentally, it was found thatlonger phrases yield better MT output (Koehn et al,2003).
However, while it is computationally feasi-ble at training time to extract phrase pairs of nearlyunbounded size (Zhang and Vogel, 2005; Callison-Burch et al, 2005), phrase pairs applicable at testtime tend to be fairly short.
Indeed, data sparsityoften forces conventional phrase-based systems tosegment test sentences into small phrases, and there-fore to translate dependent words (e.g., the Frenchne .
.
.
pas) separately instead of jointly.We present a solution to this sparsity problem bygoing beyond using only continuous phrases, andinstead define our translation unit as any subset ofwords of a sentence, i.e., a discontinuous phrase.We generalize conventional multi-beam string-baseddecoding (Koehn, 2004) to allow variable-size dis-continuities in both source and target phrases.
Sinceeach sentence pair can be more flexibly decomposedinto translation units, it is possible to exploit the richcontext of longer (possibly discontinuous) phrasesto improve translation quality.
Our decoder providestwo extensions to Moses (Koehn et al, 2007): (a) tocope with source gaps, we follow (Lopez, 2007) toefficiently find all discontinuous phrases in the train-ing data that also appear in the input sentence; (b) toenable target discontinuities, we augment transla-tion hypotheses to not only record the current par-tial translation, but also a set of subphrases that maybe appended to the partial translation at some laterstages of decoding.
With these enhancements, ourbest discontinuous system outperforms Moses withlexicalized reordering by 0.77 BLEU and 1.53 TERpoints on average.We also show that our approach compares favor-ably to binary synchronous context-free grammar(2-SCFG) systems such as Hiero (Chiang, 2007),even though 2-SCFG systems also allow phrasal dis-continuities.
Part of this difference may be due to adifference of expressiveness, since 2-SCFG modelsimpose hard hierarchical constraints that our mod-els do not impose.
Recent work (Wellington etal., 2006; S?gaard and Kuhn, 2009; S?gaard and966aiak ambjbl bnai akbjbl bnamsource: ai ckbj dlbm apdn cttarget:(iii)(ii)(i)Figure 1: 2-SCFG systems such as Hiero are unable to in-dependently generate translation units a, b, c, and d withthe following types of alignments: (i) inside-out (Wu,1997); (ii) cross-serial DTU (S?gaard and Kuhn, 2009);(iii) ?bonbon?
(Simard et al, 2005).
Standard phrase-based decoders cope with (i), but not (ii) and (iii).
Ourphrase-based decoder handles all three cases.Wu, 2009) has questioned the empirical adequacy of2-SCFG systems, which are unable to perform anyof the transformations shown in Fig.
1.
For instance,using manually-aligned bitexts for 12 European lan-guages pairs, S?gaard and Kuhn found that inside-out and cross-serial discontinuous translation units(DTU) account for 1.6% (Danish-English) to 18.6%(French-English) of all translation units.
The em-pirical adequacy of 2-SCFG models would presum-ably be lower with automatically-aligned texts and ifthe study also included non-European languages.
Incontrast, phrase-based systems can properly handleinside-out alignments when used with a reasonablylarge distortion limit, and all configurations in Fig.
1are accounted for in our system.
In our experiments,we show that our discontinuous phrase-based sys-tem outperforms Joshua (Li et al, 2009), a reimple-mentation of Hiero, by 1.03 BLEU points and 1.19TER points on average.
A final compelling advan-tage of our decoder is that it preserves the compu-tational efficiency of Moses (i.e., time complexity islinear when a distortion limit is used), while SCFGdecoders have a running time that is at least cubic(Huang et al, 2005).2 Discontinuous Phrase ExtractionIn this section, we introduce the extraction of dis-continuous phrases for phrase-based MT.
We willdescribe a decoder that can handle such phrasesin the next section.
Formally, we define the dis-continuous phrase-based translation problem as fol-lows.
We are given a source sentence f = fJ1 =f1, .
.
.
, fj, .
.
.
, fJ , which is to be translated into atarget sentence e = eI1 = e1, .
.
.
, ei, .
.
.
, fI .
Un-like (Och and Ney, 2004), in this work, a sentencepair may be segmented into phrases that are not con-Hiero:This work:ne veux plus Xje ne veux plus Xdo not want X anymoreI do not want X anymoreveuxne ... plusje ne ... plusne veux plusje ne veux plusveux ... jouerdo ... wantnot ... anymoreI ... not ... anymoredo not want ... anymoreI do not want ... anymoredo ... want to playjeneveuxplusjouerIdonotwanttoplayanymoreFigure 2: Due to hierarchical constraints, Hiero only ex-tracts two discontinuous phrases from the alignment onthe left, but our system extracts 11 (only 6 are shown).tinuous, so each phrase is characterized by a cover-age set, i.e., a set of word indices.
Assuming thatthe sentence pair (f , e) is decomposed into K dis-continuous phrases, we use s = (s1, .
.
.
, sK) andt = (t1, .
.
.
, tK) to respectively represent the de-composition of the source and target sentence intoK word subsets that are complementary and non-overlapping.
A pair of coverage sets (sk, tk) is saidto be consistent with the word alignment A if thefollowing condition holds:?
(i, j) ?
A : i ?
sk ??
j ?
tk (1)For continuous phrases, finding all phrase pairsthat satisfy this condition can be done in O(nm3)time (Och and Ney, 2004), where n is the length ofthe sentence and m is the maximum phrase length.The set of discontinuous phrases is exponential inthe maximum span length, so phrase extraction mustbe tailored to a specific text (e.g., a given test sen-tence) for relatively large m values.
Lopez (2007)presents an efficient solution using suffix arrays forfinding all discontinuous phrases of the training datathat are relevant to a given test sentence or test set.A complete overview of this technique is beyondthe scope of this paper, though we will mentionthat it solves a phrase collocation problem by effi-ciently identifying collocated continuous phrases ofthe training data that also happen to be collocated inthe test sentence.
While this technique was primar-ily designed for extracting hierarchical phrases forHiero (Chiang, 2007), it can readily be applied tothe problem of finding all discontinuous phrases forour phrase-based system.
Indeed, the suffix-arraytechnique gives us for each input sentence a list ofrelevant source coverage sets.
For each such sk, wecan easily enumerate each tk satisfying Eq.
1.
The967!!"!
!
!# $% & ' ( )* +, -.!
!he said are to thisone accessmake arrangementshe said are ... for thisvisitarrangements ... madehe saidoo-------score = -1.3arefor this | madeooooo--ooscore = -4.8arrangementsmadeoo-----ooscore = -3.2madefor thisooooo--ooscore = -6.1for thisooooo--ooscore = -7.2visitoooooooooscore = -8.5source:translationoptions(subset):stateexpansions:* *Figure 3: A particular decoder search path for the input shown at the top.
Note that this example contains a cross-serialDTU (which interleaves arrangements ... made with are ... for this), a structure Hiero can?t handle.key difference between Hiero-style extraction andour work is that Eq.
1 is the only constraint.1 Sinceour decoder doesn?t impose hierarchical constraints,we exploit all discontinuous phrase pairs consis-tent with the word alignment, which often includessound translations not captured by Hiero (e.g., ne .
.
.plus translating to not .
.
.
anymore in Fig.
2).3 DecoderThe core engine of our phrase-based system, Phrasal(Cer et al, 2010), is a multi-stack decoder similar toMoses (Koehn, 2004), which we extended to sup-port variable-size gaps in the source and the target.InMoses, partial translation hypotheses are arrangedinto different stacks according to the total number ofinput words they cover.
At every translation step,stacks are pruned using partial translation cost and alower bound on the estimated future cost.
Pruningis implemented using both threshold and histogrampruning, and Moses allows for hypothesis recombi-nation between hypotheses that are indistinguishableaccording to the underlying models.The key difference between Moses and our sys-tem is that, in order to account for target disconti-nuities, phrases that contains gaps in the target areappended to a partial translation hypothesis in mul-tiple steps.
Specifically, each translation hypothesisin our decoder is not only represented as a transla-tion prefix and a coverage set as in Moses, but it alsocontains a set of isolated phrases (shown in italic inFig.
3) that must be added to the translation at somelater time.
For instance, the figure shows how the1In order to keep the number of phrases manageable, weadditionally require that each (maximal) contiguous substringof sk and tk be connected with at least one word alignment.Beam search algorithm.1 create initial hypothesis H?
; add it to Sg02 for j = 0 to J3 if j > 0 then4 for n = 1 to N5 for each Hnew in consolidate(Hcjn)6 add Hnew to Sgj7 if j < J then8 for n = 1 to N9 Hold := Hgjn10 u := first uncovered source word of Hold11 for m = u to u + distortionLimit12 for each (sk, tk) in translation options(m)13 if source sk does not overlap Hold then14 Hnew :=combine(Hold, sk, tk)15 add Hnew to Scj+l, where l = |sk|16 return argmax(SgJ )Table 1: Discontinuous phrase-based MT.phrase pair (\??
?, arrangements ... made) is be-ing added to a partial translation.
The prefix (ar-rangements) is immediately appended to form thehypothesis (he said arrangements), and the isolatedphrase (made) is stored for later use.A beam search algorithm for discontinuousphrase-based MT is shown in Table 1.
Pruning isdone implicitly in the table to avoid cluttering thepseudo-code.
The algorithm handles 2J + 1 stacksSg0 , Sg1 , .
.
.
, SgJ and Sc1, .
.
.
, ScJ , where each stackmay contain up to N hypotheses Hj1, .
.
.
,HjN .The main loop of the algorithm alternates twostages: grow (lines 7?15) and consolidate (lines 3?6).2 The grow stage is similar to standard phrase-2The distinction between Sgi and Sci stacks ensures that theconsolidate operation does not read and write hypotheses on thesame stack.
While it may seem effective to store hypotheses in968based MT: we take a hypothesis Hgjn from Sgj andcombine it with a translation option (sk, tk), whichyields a new hypothesis that is added to stack Scj+l(where l = |sk|).
The second stage, consolidate, letsthe decoder select any number of isolated phrases(not necessarily all, and possibly zero) and appendthem in any order at the end of the current trans-lation.3 Consolidation operations are marked withstars in the figure (for simplicity, the figure doesnot display consolidations that keep hypotheses un-changed).
We limit the number of isolated phrasesto 4, which is generally enough to account for mosttransformations seen in the data.
Any hypothesis inthe last beam SgJ is automatically discarded if it con-tains any isolated phrase.One last difference with standard decoders isthat we also handle source discontinuities.
Thisproblem is a known instance of MT by patternmatching (Lopez, 2007), which we already men-tioned in the previous section.
The function transla-tion options(m) of Table 1 returns the set of optionsapplicable at position m using this pattern match-ing algorithm.
Since this function is invoked a largenumber of times, it is important to precompute itsreturn values for each m prior to decoding.4 FeaturesOur system incorporates the same eight baseline fea-tures of Moses: two relative-frequency phrase trans-lation probabilities p(e|f) and p(f |e), two lexically-weighted phrase translation probabilities (Koehn etal., 2003) lex(e|f) and lex(f |e), a language modelprobability, word penalty, phrase penalty, and lineardistortion, and we optionally add 6 lexicalized re-ordering features as computed in Moses.Our computation of linear distortion is differentfrom the one in Moses, since we need to accountfor discontinuous phrases.
We found that it iscrucial to penalize discontinuous phrases that haverelatively long gaps.
Hence, in our computation ofdifferent stacks depending on the number of isolated phrases,we have not found various implementations of this idea to workbetter than the algorithm described here.3We let isolated phrases be reordered freely, with only threeconstraints: (1) the internal word order must be preserved, i.e., aphrase may not be split or reordered.
(2) isolated phrases drawnfrom the same discontinuous phrase must appear in the specifiedorder (i.e., the phrase A ... B ... C may not yield the translationA ... C ... B).
(3) Empty gaps are forbidden.Figure 4: Linear distortion computed using both continu-ous and discontinuous phrase.linear distortion, we treat continuous subphrasesof each discontinuous phrase as if they werecontinuous phrases on their own.
Specifically,let s?
= (s?1, .
.
.
, s?L) be the list of L (maximal)continuous subphrases of the K source phrases(L ?
K) selected for a given translation hypothesis.Subphrases in s?
are enumerated according to theirorder in the target language, which may be differentfrom the source-side ordering.
We then computethe linear distortion between pair of successiveelements (s?i, s?i+1) as follows:d(?s) = s?first1 +L?i=2??
?s?lasti?1 + 1?
s?firsti??
?where the superscripts first and last respectivelyrefer to source position of the first and last wordof a given subphrase.
Fig.
4 shows an example ofhow distortion is computed for phrases (s1, s2, s3),including the discontinuous phrase s2 split into threecontinuous subphrases.
In practice, we computeintra-phrase (shown with thin arrows in the figure)and inter-phrase linear distortion separately in orderto produce two distinct features, since translationtends to improves when the intra-phrase cost has alower feature weight.Finally, we add two features that are not presentin Moses.
First, we penalize target discontinuitiesby including a feature that is the sum of the lengthsof all target gaps.
The second feature is the countof discontinuous phrases that are in configurations(cross-serial DTU (S?gaard and Kuhn, 2009) and?bonbon?
(Simard et al, 2005)) that can?t be han-dled by 2-SCFG systems.
The advantage of suchfeatures is two-fold.
First, similarly to hierarchi-cal systems, they prevent many distorted reorderingsthat are unlikely to correspond to quality transla-tions.
Second, it imposes soft rather than hard con-straints, which means that the decoder is entirelyfree to violate hierarchical constraints when theseviolations are supported by other features.9695 Experimental SetupThree systems are evaluated in this paper: Moses(Koehn et al, 2007), Joshua (Li et al, 2009) ?
areimplementation of Hiero, and our phrase-basedsystem.
We made our best attempts to make our sys-tem comparable to Moses.
That is, when no discon-tinuous phrases are provided to our system, it gener-ates an output that is almost identical to Moses (onlyabout 1% of translations differ on average).
In bothsystems, we use the default settings of Moses, i.e.,we set the beam size to 200, the distortion limit to 6,we limit to 20 the number of target phrases that areloaded for each source phrase, and we use the samedefault eight features of Moses.
We use version 1.3of Joshua with its default settings.
Both Moses andour system are evaluated with and without lexical-ized reordering (Tillmann, 2004).4 We believe itto be fair to compare Joshua against phrase-basedsystems that exploit lexicalized reordering, since Hi-ero?s hierarchical rules are also lexically sensitive.5The language pair for our experiments is Chinese-to-English.
The training data consists of about 28million English words and 23.3 million Chinesewords drawn from various news parallel corpora dis-tributed by the Linguistic Data Consortium (LDC).In order to provide experiments comparable to previ-ous work, we used the same corpora as (Wang et al,2007).
We performed word alignment using a cross-EM word aligner (Liang et al, 2006).
For this, weran two iterations of IBM Model 1 and two HMMiterations.
Finally, we generated a symmetric wordalignment from cross-EM Viterbi alignment usingthe Moses grow-diag heuristic in the case Moses andour system.
In the case of Joshua, we used the grow-diag-final heuristic since this gave better results.In order to train a competitive baseline given ourcomputational resources, we built a large 5-gramlanguage model using the Xinhua and AFP sections4We use Moses?
default orientations: monotone, swap, anddiscontinuous.
As far as this reordering model is concerned,we treat discontinuous phrases as continuous, i.e., we simplyignore what lies within gaps to determine phrase orientation.5(Tillmann, 2004) learns for each phrase a tendency to ei-ther remain monotone or to swap with other phrases.
As notedin (Lopez, 2008), Hiero can represent the same informationwith hierarchical rules of the form uX, Xu, and XuX.
Hi-ero actually models lexicalized reordering patterns that (Till-mann, 2004) does not account for, e.g., a transformation fromX1uX2v to X2u?v?X1.of the Gigaword corpus (LDC2007T40) in additionto the target side of the parallel data.
This data rep-resents a total of about 700 million words.
We man-ually removed documents of Gigaword that were re-leased during periods that overlap with those of ourdevelopment and test sets.
The language model wassmoothed with the modified Kneser-Ney algorithmas implemented in SRILM (Stolcke, 2002), and weonly kept 4-grams and 5-grams that occurred at leastthree times in the training data.For tuning and testing, we use the official NISTMT evaluation data for Chinese from 2003 to 2008(MT03 to MT08), which all have four English ref-erences for each input sentence.
We used the 1664sentences of MT06 for tuning and development andall other sets for testing.
Parameter tuning wasdone with minimum error rate training (Och, 2003),which was used to maximize IBM BLEU-4 (Pap-ineni et al, 2001).
Since MERT is prone to searcherrors, especially with large numbers of parameters,we ran each tuning experiment four times with dif-ferent initial conditions.
We used n-best lists of size200.
In the final evaluations, we report results usingboth TER version 0.7.25 (Snover et al, 2006) andBLEU-4 (both uncased).6 ResultsWe start by comparing some translations generatedby the best configurations of Joshua, Moses, and ourphrase-based decoder, systems we will empiricallyevaluate later in this section.
Fig.
5 shows trans-lations of our development set MT06, which wereselected because our system makes a crucial use ofdiscontinuous phrases.
In the first example, the Chi-nese input contains S ...
?, which typically trans-lates as when.
Lacking an entry for the input phraseS}C?e:?
in its phrase table, Moses isunable to translate this segment appropriately, andmust instead split this phrase to generate the trans-lation when the right was deprived of, where ?
istranslated into of.
This is evidently a poor transla-tion.
Conversely, our system uses a discontinuousphrase to translate S ...
?, and translates the inter-vening words separately.The remaining three translations all contain cross-serial DTUs (S?gaard and Kuhn, 2009) and thuswould be difficult to generate using 2-SCFG sys-tems.
The second example motivates the idea970MT06 ?
segment 1589Reference: Under such cir-cumstances, when the rightof existence was deprived,the only way remaining wasto overthrow the existingdynasty by force and try toreplace it.Joshua: Under such cir-cumstances, when life bedeprived, can only resort toviolence to overthrow thecurrent dynasty, trying to re-place,Moses: Under such circum-stances, when the right wasdeprived of, can only adoptthe means of violence, inan attempt to overthrow thepresent dynasty replaced,This work: Under such cir-cumstances, when he was de-prived of the right to life, itcan only resort to violence inan attempt to overthrow thecurrent dynasty replaced,in this kind case when life right was deprive when only can use violence of means( ?
?
??
, S } C ?
e: ?
, ?
?
??
??
?
K?
...under such circumstances , when he was deprived of the right to life , it can only resort to violence ...MT06 ?
segment 1044Reference: CCP organi-zation ministry demandsto further enlarge strengthof supervision of leadingcadres and cadre selectionand appointmentJoshua: Department de-mands further intensify su-pervision over the workof selecting and appointingleading cadres, and inten-sifyMoses: The central organi-zation department, called onleading cadres, further in-crease the intensity of super-vision over work of selectingand appointing cadres.This work: The central orga-nization department has calledfor further increase the inten-sity of supervision of leadingcadres and the work of select-ing and appointing cadres.CCP request further increase to leading cadres and cadre selection appointment work of supervision intensity-??
?B ? e ?'
?
??
r?
?
r?
?
?
( ?\ ?
?
?the central organization department has called for further increase the intensity of supervision of leading cadres and ... ...MT06 ?
segment 559Reference: The governmentwill take all possible mea-sures to prevent similar inci-dents from happening in thefuture.Joshua: Government willtake all measures to preventthe re-occurrence of similarincidents in the future.Moses: The government willtake all measures to preventthe occurrence of similar inci-dents in the future.This work: The governmentwill take all measures to pre-vent similar incidents fromhappening again in the future.government will take all measure to prevent future again happen similar of incidents??
  ??
  ??
e 2b ??
?
?
{< ?
??
the government will take all measures to prevent similar incidents from happening again in the future .MT06 ?
segment 769Reference: He also said thatthe arrangements are beingmade now for the visits.Joshua: He also said thatnow is making arrange-ments for this visit.Moses: He also said that thecurrent visit is to make ar-rangements.This work: He also said thatthe current arrangements aremade for the visit.he also said now are for this one visit make arrangements?
?
?
(( : ?
  ??
\?
??
he also said that the current arrangements are made for the visit .Figure 5: Actual translations produced by Joshua, Moses, and our system.
For our system, we also display phrasealignments, including discontinuous phrase alignments.
Results for these three systems here are displayed in rows 2,4, and 8 of Table 2.
The thick blue arrows represent alignments between discontinuous phrases, while red segmentedarrows align continuous phrases.971MT06 (tune) MT03 MT04 MT05 MT08 ALLSystem Gaps LexR BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER BLEU TER1 hierarchical(Joshua)src yes 33.55 58.04 33.25 59.73 36.03 58.92 32.03 61.11 26.30 61.30 31.70 58.212 src+tgt yes 33.84 58.11 33.47 59.85 36.10 58.82 32.17 61.20 26.61 61.21 31.90 58.223 phrase-based(Moses)no no 33.17 59.24 32.60 60.80 35.38 59.55 31.15 62.43 25.56 61.98 31.08 59.144 no yes 34.25 58.23 33.72 60.42 36.37 59.18 32.49 61.80 26.70 61.48 32.16 58.565 discontinuousphrase-based(this work)src no 33.77 58.56 33.20 60.42 36.17 59.13 31.75 61.62 25.99 61.47 31.68 58.606 tgt no 33.27 58.98 32.95 60.42 35.41 59.35 31.08 62.45 25.69 61.71 31.17 58.937 src+tgt no 33.86 58.26 33.32 60.02 36.36 58.56 31.87 61.35 26.13 61.29 31.81 58.258 src+tgt yes 35.00 56.85 34.96 57.97 37.44 57.61 33.39 59.92 26.74 60.51 32.93 57.03Improvement over hierarchical +1.16 ?1.26 +1.49 ?1.88 +1.34 ?1.21 +1.22 ?1.28 +0.13 ?0.70 +1.03 ?1.19Improvement over phrase-based +0.75 ?1.38 +1.24 ?2.45 +1.07 ?1.57 +0.90 ?1.88 +0.04 ?0.97 +0.77 ?1.53Number of sentences 1664 919 1788 1082 1357 6810Table 2: Our system compared again conventional and hierarchical phrase-based MT (Moses and Joshua).
usinguncased BLEUr4n4[%] and TER[%].
LexR indicates whether lexicalized reordering is enabled or not.
We use ran-domization tests (Riezler and Maxwell, 2005) to determine significance of our best results (row 8) against Joshua (row2) and Moses (row 4): differences marked in bold are significant at the p ?
.01 level.that larger translation units, including discontinuousphrases, lead to better translations.
The reference in-cludes the translation enlarge strength of supervisionof leading cadres, and our system is able to producea translation that is almost identical (increase the in-tensity of supervision of leading cadres) using onlytwo phrases, pulling together input words that arefairly far apart in the sentence.
The third Chinesesentence has a word order quite different from En-glish, but our decoder flexibly reorders it in a mannerthat can?t be handled with SCFG decoders to givea word order (prevent similar events from happen-ing) that matches the one in the reference.
The lastChinese sentence includes the topicalization word: (for), which indicates the input sentence has nosubject.
One way to properly handle this translationis to turn the sentence into a passive in English (asin the reference), a transformation our system does,thanks to its support for complex reorderings.Our main results are displayed in Table 2.
First,Joshua systematically outperforms the Moses base-line (+0.82 BLEU point and ?0.92 TER point onaverage), but performance of the two is about thesame when Moses incorporates lexicalized reorder-ing.
This finding is consistent with previous work(Lopez, 2008).
The results of our system displayedin rows 5?8 demonstrate that our system consis-tently outperforms Moses, whether they both uselexicalized reordering or not.
The performance ofour best system?i.e., with lexicalized reorderingand both source and target gaps?is significantlybetter than the best Moses system (+0.77 BLEUand ?1.53 TER).
While the performance of our sys-tem without lexicalized reordering is close to that ofJoshua, our system with lexicalized reordering sig-nificantly outperforms Joshua (p ?
.01) in 9 out of10 evaluations.
The single experiment where our im-provement over Hiero is insignificant (i.e., BLEU onMT08) is mainly affected by a discrepancy of length(our brevity penalty on MT08 is 0.92).It is interesting to notice that our system allowingphrasal discontinuities only on the source (row 5)performs almost as well as the system that allowsthem on both sides (row 7).
For instance, whilesource discontinuities improve performance by 0.7BLEU point on MT06, further enabling target dis-continuities only raises performance by a mere 0.09BLEU point.
This naturally raises the question ofwhether our support for target gaps is ineffective,or whether target-discontinuous phrases are some-what superfluous to the MT task.
While it is cer-tainly difficult to either confirm or deny the latterhypothesis, we can at least compare our handling oftarget-discontinuous phrases with hierarchical sys-tems.
In one additional set of experiments, we re-moved target-discontinuous phrases in Joshua priorto MERT and test time.
Specifically, we removedall hierarchical phrases whose target side has theform uXv, uXvX, XuXv, and uXvXw, and onlyallowed rules whose target side has the form uX,Xu, XuX, XXu, or uXX.
After this filtering,we found that target-discontinuous phrases in Joshuaare also not crucial to its performance, since their re-moval only caused a drop of 0.2 BLEU point (row 1)and almost no change in terms of TER.We speculatethat using target discontinuous phrases is more diffi-9721 2 3 4 5 6 7050001000015000# of English words per phrasewordmassMosesthis workFigure 6: Phrase length histogram for MT06.cult, since it represents a generation rather than justa matching problem.In this paper, we have also argued that a mainbenefit of discontinuous phrases?and particularlysource-discontinuous phrases?is that the decoder isallowed to use larger translation units than when re-stricted to continuous phrases.
This claim is con-firmed in Fig.
6.
We find that our decoder makeseffective use of the extended set of translation op-tions at its disposal: While the Moses baseline trans-lates MT06 with an average 1.73 words per phrase,adding support for discontinuities increases this av-erage to 2.16, and reduces by 43% the use of sin-gle word phrases.
On MT06, 53% of the translatedsentences produced by our best system use at leastone source-discontinuous phrase, and 9% of themexploit one or more target-discontinuous phrases.7 Related WorkThe main goal of this paper is to show that discontin-uous phrases can greatly improve the performanceof phrase-based systems.
While some of the mostrecent phrase-based systems (Chiang, 2007; Watan-abe et al, 2006) exploit context-free decoding algo-rithms (CKY, Earley, etc.)
to cope with discontinu-ities, our system preserves the simplicity and speedof conventional phrase-based decoders, and in par-ticular does not build any intermediate tree structure,does not impose any hard reordering constraintsother than the distortion limit, and still achievestranslation performance that is superior to that of astate-of-the-art hierarchical system.A few previous non-hierarchical systems havealso exploited phrasal discontinuities.
The most no-table previous attempt to incorporate gaps is de-scribed in (Simard et al, 2005).
Simard et alpresents an extension to Moses that allows gaps inboth source and target phrases, though each of theirgap symbols must span exactly one word.
This factmakes decoding simpler, since the position of all tar-get words in a translation hypothesis is known assoon as the hypothesis is laid down, but fixed-sizediscontinuous phrases are less general and increasesparsity.
By comparison, our gaps may span anynumber of words, so we have an increased ability toflexibly match the input sentence effectively.
(Cregoand Yvon, 2009) also handles gaps, though this workis applicable to an n-gram-based SMT framework(Mario`o et al, 2006), which is fairly different fromthe phrase-based framework.8 ConclusionsIn this paper, we presented a generalization of con-ventional phrase-based decoding to handle discon-tinuities in both source and target phrases.
Oursystem significantly outperforms Moses and Joshua,two standard implementations of conventional andhierarchical phrase-based decoding.
We found thatallowing discontinuities in the source is more use-ful than target discontinuities in our system, thoughwe found that this turns out to also be the case withthe hierarchical phrases of Joshua.
In future work,we plan to extend the parameterization of phrase-based lexicalized reordering models to be sensitiveto these discontinuities, and we will also consideradding syntactic features to our models to penal-ize discontinuities that are not syntactically moti-vated (Marton and Resnik, 2008; Chiang et al,2009).
The discontinuous phrase-based MT systemdescribed in this work is part of Phrasal, an open-source phrase-based system available for downloadat http://nlp.stanford.edu/software/phrasal.AcknowledgementsThe authors thank three anonymous reviewers, DanJurafsky, Spence Green, Steven Bethard, Daniel Cer,Chris Callison-Burch, and Pi-Chuan Chang for theirhelpful comments.
This paper is based on workfunded by the Defense Advanced Research ProjectsAgency through IBM.
The content does not neces-sarily reflect the views of the U.S. Government, andno official endorsement should be inferred.973ReferencesChris Callison-Burch, Colin Bannard, and JoshSchroeder.
2005.
Scaling phrase-based statisticalmachine translation to larger corpora and longerphrases.
In Proc.
of ACL, pages 255?262.Daniel Cer, Michel Galley, Dan Jurafsky, and Christo-pher Manning.
2010.
Phrasal: A statistical machinetranslation toolkit for exploring new model features.In Proc.
of NAACL-HLT, Demonstration Session.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine translation.In Proc.
of NAACL, pages 218?226.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Josep Crego and Fran?ois Yvon.
2009.
Gappy transla-tion units under left-to-right SMT decoding.
In Proc.of EAMT.Liang Huang, Hao Zhang, and Daniel Gildea.
2005.
Ma-chine translation as lexicalized parsing with hooks.
InProc.
of the Ninth International Workshop on ParsingTechnology, pages 65?73.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of NAACL, pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen,Christine Moran, Richard Zens, Chris Dyer, OndrejBojar, Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proc.
of ACL, Demonstration Session.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In Proc.
of AMTA, pages 115?124.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenN.
G. Thornton, Jonathan Weese, and Omar F. Zaidan.2009.
Joshua: an open source toolkit for parsing-based MT.
In Proc.
of WMT.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proc.
of HLT-NAACL, pages104?111.Adam Lopez.
2007.
Hierarchical phrase-based transla-tion with suffix arrays.
In Proc.
of EMNLP-CoNLL,pages 976?985.Adam Lopez.
2008.
Tera-scale translation models viapattern matching.
In Proc.
of COLING.Jose?
B. Mario`o, Rafael E. Banchs, Josep M. Crego, Adria`de Gispert, Patrik Lambert, Jose?
A. R. Fonollosa, andMarta R. Costa-jussa`.
2006.
N-gram-based machinetranslation.
Computational Linguistics, 32(4):527?549.Yuval Marton and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrased-based translation.In Proc.
of ACL, pages 1003?1011.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proc.
of ACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automatic eval-uation of machine translation.
In Proc.
of ACL.Stefan Riezler and John T. Maxwell.
2005.
On some pit-falls in automatic evaluation and significance testingfor MT.
In Proc.
of Workshop on Evaluation Mea-sures, pages 57?64.Michel Simard, Nicola Cancedda, Bruno Cavestro, MarcDymetman, Eric Gaussier, Cyril Goutte, Kenji Ya-mada, Philippe Langlais, and Arne Mauser.
2005.Translating with non-contiguous phrases.
In Proc.
ofHLT-EMNLP, pages 755?762.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proc.
of AMTA, pages 223?231.Anders S?gaard and Jonas Kuhn.
2009.
Empirical lowerbounds on alignment error rates in syntax-based ma-chine translation.
In Proc.
of the Third Workshop onSyntax and Structure in Statistical Translation (SSST-3) at NAACL HLT 2009, pages 19?27.Anders S?gaard and Dekai Wu.
2009.
Empirical lowerbounds on translation unit error rate for the full classof inversion transduction grammars.
In Proc.
of IWPT,pages 33?36.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proc.
of ICSLP, pages 901?904.Christoph Tillmann.
2004.
A unigram orientation modelfor statistical machine translation.
In Proc.
of HLT-NAACL, pages 101?104.Chao Wang, Michael Collins, and Philipp Koehn.
2007.Chinese syntactic reordering for statistical machinetranslation.
In Proc.
of EMNLP-CoNLL.Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.2006.
Left-to-right target generation for hierarchicalphrase-based translation.
In Proc.
of ACL.Benjamin Wellington, Sonjia Waxmonsky, and I. DanMelamed.
2006.
Empirical lower bounds on thecomplexity of translational equivalence.
In Proc.
ofCOLING-ACL, pages 977?984.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404.Ying Zhang and Stephan Vogel.
2005.
An efficientphrase-to-phrase alignment model for arbitrarily longphrase and large corpora.
In Proc.
of EAMT.974
