Proceedings of NAACL-HLT 2013, pages 918?927,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsClassifying Temporal Relations with Rich Linguistic KnowledgeJennifer D?Souza and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{jld082000,vince}@hlt.utdallas.eduAbstractWe examine the task of temporal relation clas-sification.
Unlike existing approaches to thistask, we (1) classify an event-event or event-time pair as one of the 14 temporal relationsdefined in the TimeBank corpus, rather thanas one of the six relations collapsed from theoriginal 14; (2) employ sophisticated linguis-tic knowledge derived from a variety of se-mantic and discourse relations, rather than fo-cusing on morpho-syntactic knowledge; and(3) leverage a novel combination of rule-basedand learning-based approaches, rather than re-lying solely on one or the other.
Experimentswith the TimeBank corpus demonstrate thatour knowledge-rich, hybrid approach yieldsa 15?16% relative reduction in error over astate-of-the-art learning-based baseline sys-tem.1 IntroductionRecent years have seen a surge of interest in tem-poral information extraction (IE).
Temporal relationclassification, one of the most important temporalIE tasks, involves classifying a given event-eventpair or event-time pair as one of a set of predefinedtemporal relations.
The creation of the TimeBankcorpus (Pustejovsky et al 2003) and the organiza-tion of the TempEval-1 (Verhagen et al 2007) andTempEval-2 (Verhagen et al 2010) evaluation ex-ercises have facilitated the development and evalua-tion of temporal relation classification systems.Our goal in this paper is to advance the state ofthe art in temporal relation classification.
Our workdiffers from existing work with respect to both thecomplexity of the task we are addressing and the ap-proach we adopt.
Regarding task complexity, ratherthan focus on six temporal relations as is typicallydone in previous work (see Section 2 for more infor-mation), we address an arguably more challengingversion of the task where we consider all the 14 re-lations originally defined in the TimeBank corpus.Our approach to temporal relation classificationcan be distinguished from existing approaches intwo respects.
The first involves a large-scale ex-pansion of the linguistic features made availableto the classification system.
Recall that exist-ing approaches have relied primarily on morpho-syntactic features as well as a few semantic fea-tures extracted from WordNet synsets and VerbO-cean?s (Chklovski and Pantel, 2004) semantic rela-tions.
On the other hand, we propose not only novellexical and grammatical features, but also sophis-ticated features involving semantics and discourse.Most notably, we propose (1) semantic features en-coding a variety of semantic relations, includingPropBank-style predicate-argument relations as wellas those extracted from the Merriam-Webster dictio-nary, and (2) discourse features encoding automat-ically computed Penn Discourse TreeBank (PDTB)style (Prasad et al 2008) discourse relations.Second, while the vast majority of existing ap-proaches to temporal relation classification arelearning-based, we propose a system architecture inwhich we combine a learning-based approach and arule-based approach.
Our motivation behind adopt-ing a hybrid approach stems from two hypotheses.First, a rule-based method could better handle theskewed class distribution underlying the dataset for918our 14-class classification problem.
Second, betterdecision rules could be formed by leveraging hu-man insights to combine the available linguistic fea-tures than by using fully automatic machine learn-ing methods.
Note that while rule-based approacheshave been shown to underperform learning-basedapproaches on this task (Mani et al 2006), to ourknowledge they have not been used in combinationwith learning-based approaches.
Moreover, whilethe rules employed in previous work are createdbased on intuition (e.g., Mani et al(2006), Pus?cas?u(2007)), our rules are created in a data-driven man-ner via a manual inspection of the annotated tempo-ral relations in the TimeBank corpus.Experiments on the TimeBank corpus demon-strate the effectiveness of our knowledge-rich, hy-brid approach to temporal relation classification: ityields a 15?16% relative reduction in error over astate-of-the-art learning-based baseline system.To our knowledge, we are the first to (1) report re-sults for the 14-class temporal relation classificationtask on the TimeBank (v1.2) corpus; (2) success-fully employ automatically computed PDTB-stylediscourse relations to improve performance on thistask; and (3) show that a hybrid approach to thistask can yield better results than either a rule-basedor learning-based approach.
Note that hybrid ap-proaches in this spirit were popular in the naturallanguage processing community back in the mid-90s(Klavans and Resnik, 1994).
We believe that theyare among the most competitive approaches to lan-guage processing tasks that require complex reason-ing and should be given more attention in the com-munity.
We release the complete set of rules that wemined from the TimeBank corpus and used in ourrule-based approach in hopes that our insights intohow features can be combined as decision rules canbenefit researchers interested in this task.The rest of the paper is organized as follows.
Sec-tion 2 provides an overview of the TimeBank cor-pus.
Sections 3 and 4 describe the baseline systemand our approach, respectively.
We present evalua-tion results in Section 5 and conclude in Section 6.2 CorpusFor evaluation, we use the TimeBank (v1.2) cor-pus, which consists of 183 newswire articles.
Ineach article, the events, times, and their temporal re-lations are marked up.
An event, which can be atensed verb, adjective, or nominal, contains variousattributes, including the class of event, tense, aspect,polarity, and modality.
A time expression has a classattribute, which specifies whether it is a date, time,duration, or set, and its value is normalized based onTIMEX3.
A temporal relation can be an order rela-tion, which orders two events (as in sentence (1)), oran anchor relation, which anchors an event to a timeexpression (as in sentence (2)).
(1) A steep rise in world oil prices fol-lowed the Kuwait invasion.
(2) We are there to stay for a long period.Each temporal relation has a type.
For example,the relation defined on rise and invasion in (1) hastype After, whereas the relation defined on stay andperiod in (2) has type During.
Note that a temporalrelation is defined on an ordered pair.
For exam-ple, in (1), the pair (rise, invasion) has type After,whereas the pair (invasion, rise) has type Before).14 relation types are defined and used to annotatethe temporal relations in the TimeBank corpus.
Ta-ble 1 provides a brief description of these relationtypes and the relevant statistics.In our experiments, we assume that our tempo-ral relation classification system is given an event-event or event-time pair that is known to belong toone of the 14 relation types defined in TimeBank andaims to determine its relation type.
Following pre-vious evaluations of the temporal relation classifica-tion task on the TimeBank corpus (e.g., Mani et al(2006), Chambers et al(2007)) and in TempEval-1/2, we assume as input gold events and time ex-pressions.Unlike Mani et al(2006) and Chambers et al(2007), who focus on six relation types (Simul-taneous, Before, IBefore, Begins, Ends, and In-cludes), we report results on 14 relation types.
Notethat the aforementioned six relation types are cho-sen by (1) discarding During, During Inv, andIdentity, and (2) combining the two relation typesin each of the five pairs, namely (Before, After),(IBefore, IAfter), (Includes, Is Included), (Be-gins, Begun By), and (Ends, Ended By), into a sin-gle type because they are inverses of each other.
Inother words, if a relation instance (e1, e2) is anno-919Id Relation Description Total % E-E E-T1 Simultaneous e1 and e2 happen at the same time or are temporally distinguishable 660 (13.3) 599 612 Identity e1 and e2 are coreferent 702 (14.1) 696 63 Before e1 happens before e2 in time 689 (13.9) 639 504 After e1 happens after e2 in time 744 (15) 681 635 IBefore e1 happens immediately before e2 in time 39 (0.8) 38 16 IAfter e1 happens immediately after e2 in time 28 (0.6) 25 37 Includes As in Ed arrived in Seoul last Sunday (e1=last Sunday; e2=arrived) 758 (15.3) 318 4408 Is Included As in Ed arrived in Seoul last Sunday (e1=arrived; e2=last Sunday) 762 (15.3) 201 5619 During e1 persists throughout duration e2 102 (2.1) 19 8310 During Inv e2 persists throughout duration e1 124 (2.5) 44 8011 Begins e1 marks the beginning of e2 66 (1.3) 44 2212 Begun By e2 marks the beginning of e1 61 (1.2) 32 2913 Ends e1 marks the end of e2 66 (1.3) 21 4514 Ended By e2 marks the end of e1 170 (3.42) 93 77Table 1: The 14 temporal relations and their frequency of occurrences in TimeBank (v1.2).
Each relation is definedon an ordered event-event or event-time pair (e1,e2).
The ?Total?
and ?%?
columns show the number and percentageof instances annotated with the corresponding relation in the corpus, respectively, and the ?E-E?
and ?E-T?
columnsshow the breakdown by the number of event-event pairs and event-time pairs.tated as After, it is replaced with the instance (e2,e1) with class Before, and subsequently a relationclassifier is presented with (e2, e1) but not (e1, e2).On the other hand, our 14-class task is arguablymore challenging since our system has to further dis-tinguish a relation type from its inverse given an in-stance in which the two elements are in arbitrary or-der.3 Baseline Temporal Relation ClassifierSince the currently best-performing systems fortemporal relation classification are learning-based,we will employ a learning-based system as our base-line.
Below we describe how we train this baseline.Without loss of generality, assume that (e1,e2) isan event-event/event-time pair such that (1) e1 pre-cedes e2 in the associated text and (2) (e1,e2) be-longs to one of the 14 TimeBank temporal rela-tion types.
We create one training instance for eachevent-event/event-time pair in a training documentthat satisfies the two conditions above, labeling itwith the relation type that exists between e1 and e2.To build a strong baseline, we represent eachinstance using 68 linguistic features modeled af-ter the top-performing temporal relation classifica-tion systems on TimeBank (e.g., Mani et al(2006),Chambers et al(2007)) and in the TempEval sharedtasks (e.g., Min et al(2007), Pus?cas?u (2007), Ha etal.
(2010), Llorens et al(2010), Mirroshandel andGhassem-Sani (2011)).1 These features can be di-vided into six categories, as described below.Lexical (5).
The strings of e1 and e2, the headwords of e1 and e2, and a binary feature indicatingwhether e1 and e2 have the same string.Grammatical (33).
The POS tags of the headwords of e1 and e2, the POS tags of the five to-kens preceding and following e1 and e2, the POSbigram formed from the head word of e1 and its pre-ceding token, the POS bigram formed from the headword of e2 and its preceding token, the POS tag pairformed from the head words of e1 and e2, the prepo-sitional lexeme of the prepositional phrase (PP) if e1is headed by a PP (Chambers et al 2007), the prepo-sitional lexeme of the PP if e2 is headed by a PP, theprepositional lexeme of the PP if e1 is governed bya PP (Mirroshandel and Ghassem-Sani, 2011), theprepositional lexeme of the PP if e2 is governed bya PP, the POS of the head of the verb phrase (VP) ife1 is governed by a VP, the POS of the head of theVP if e2 is governed by a VP, whether e1 syntacti-cally dominates e2 (Chambers et al 2007), and theshortest path from e1 to e2 in the associated syntac-tic parse tree.
We obtain parse trees and POS tagsusing the Stanford CoreNLP tool.21Note, however, that these features were designed for thearguably simpler 6-class temporal relation classification tasks.2http://nlp.stanford.edu/software/corenlp.shtml920Entity attributes (13).
The tense, aspect, modal-ity, polarity, and event type of e1 and e2 if they areevents (if one of them is a time expression, then theclass attribute will be set to its class and the rest ofthem will have the value NULL), pairwise featuresformed by pairing up the tense values, the aspectvalues, and the class values of e1 and e2.Semantic (7).
The subordinating temporal role to-ken of e1 if it appears within a temporal semanticrole argument (Llorens et al 2010), the subordinat-ing temporal role token of e2 if it appears within atemporal semantic role argument, the first WordNetsynset to which e1 belongs, the first WordNet synsetto which e2 belongs, and whether e1 and e2 are in thehappens-before, happens-after, and similar relationaccording to VerbOcean.3Distance (1).
Are e1 and e2 in the same sentence?DCT related (3).
The temporal relation type be-tween e1 and the document creation time (DCT) [itsvalue can be one of the 14 relation types, or NULLif no relation exists], the temporal relation type be-tween e2 and the DCT, and whether e1 and e2 havedifferent relation types with the DCT.After creating the training instances, we traina 14-class classifier on them using SVMmulticlass(Tsochantaridis et al 2004).4 We then use it tomake predictions on the test instances, which aregenerated in the same way as the training instances.4 Our Hybrid ApproachIn this section, we describe our hybrid learning-based and rule-based approach to temporal relationclassification.
Section 4.1 describes our novel fea-tures, which will be used to augment the baselinefeature set (see Section 3) to train a temporal rela-tion classifier.
Section 4.2 outlines our manual rulecreation process.
Section 4.3 discusses how we com-bine our hand-crafted rules and the learned classifierto make predictions in our hybrid approach.3happens-after is not a relation in VerbOcean: we create thisrelation simply by inverting the happens-before relation.4For all the experiments involving SVMmulticlass, we set C,the regularization parameter, to 10,000, since preliminary ex-periments indicate that preferring generalization to overfitting(by setting C to a small value) tends to yield poorer classifica-tion performance.
The remaining learning parameters are set totheir default values.4.1 Six Types of New Features4.1.1 Pairwise FeaturesRecall that some of the features in the baseline fea-ture set are computed based on either e1 or e2 butnot both.
Since our task is to predict the relation be-tween them, we hypothesize that pairwise features,which are computed based on both elements, couldbetter capture the relationship between them.Specifically, we introduce pairwise versions of thehead word feature and the two prepositional lexeme-based features in the baseline.
In addition, we createtwo quadruple-wise features, one by pairing up thetense and class attribute values of e1 with those ofe2, and the other by pairing up their tense and as-pect values.
Next, we create two trace features, onebased on prepositions and the other on verbs, sinceprepositions and verb tenses have been shown toplay an important role in temporal relation classifi-cation The preposition trace feature is computed by(1) collecting the list of prepositions along the pathfrom e1/e2 to the root of its syntactic parse trees, and(2) concatenating the resulting lists computed frome1 and e2.
The verb trace feature is computed in asimilar manner, except that we collect the POS tagsof the verbs appearing in the corresponding paths.4.1.2 Dependency RelationsWe introduce features computed based on de-pendency parse trees obtained via the StanfordCoreNLP tool, motivated by our observation thatsome dependency relation types are more closelyassociated with certain temporal relation types thanwith others.
Let us illustrate with an example:(3) Ed changed his plans as the mood tookhim.In (3), there is a adverbial clause modifier depen-dency between changed and took, because took ap-pears in an adverbial clause (headed by as) modify-ing changed.
Intuitively, if the two events partici-pate in this type of dependency relation and the ad-verbial clause is headed by as and there is a tempo-ral relation between them, then it is likely that thistemporal relation is Simultaneous.
While the tem-poral relation type is dependent on the connectiveheading the adverbial clause, in general an adverbialclause modifier dependency between two events im-plies that their temporal relation is likely to be Si-921multaneous, Before, or After.Given the potential usefulness of dependency re-lations for temporal relation classification, we cre-ate dependency-based features as follows.
For eachof the 25 dependency relation types produced bythe Stanford parser, we create four binary features:whether e1/e2 is the governing entity in the relation,and whether e1/e2 is the dependent in the relation.4.1.3 Webster RelationsSome events are not connected by a dependency re-lation but by a lexical relation.
We hypothesize thatsome of these lexical relations could be useful fortemporal relation classification.
Consider the fol-lowing example.
(4) The phony war has finished and the realreferendum campaign has begun.In this sentence, the two events, finished and be-gun, are connected by an antonym relation.
Statisti-cally speaking, if (1) two events are in two clausesconnected by a coordinating conjunction (e.g., and),(2) one is an antonym of the other, and (3) there isa temporal relation between them, then the temporalrelation is likely to be Simultaneous.Given the potential usefulness of lexical rela-tions for temporal relation classification, we cre-ate features based on four types of lexical re-lations present in Webster?s online thesaurus5,namely synonyms, related-words, near-antonyms,and antonyms.
Specifically, for each event e appear-ing in TimeBank, we first use the head word of e toretrieve four lists, which are the lists correspondingto the synonyms, related words, near-antonyms, andantonyms of e. Then, given a training/test instanceinvolving e1 and e2, we create eight binary features:whether e1 appears in e2?s list of synonyms/relatedwords/near-antonyms/antonyms, and whether e2 ap-pears in e1?s list of synonyms/related words/near-antonyms/antonyms.4.1.4 WordNet RelationsPrevious uses of WordNet for temporal relation clas-sification are limited to synsets (e.g., Llorens et al(2010)).
We hypothesize that other WordNet lexicalrelations could also be useful for the task.
Specif-ically, we employ four types of WordNet relations,5http://www.merriam-webster.com/namely hypernyms, hyponyms, troponyms, and sim-ilar, to create eight binary features for temporal rela-tion classification.
These eight features are createdfrom the four WordNet relations in the same way asthe eight features were created from the four Web-ster relations in the previous subsection.4.1.5 Predicate-Argument RelationsSo far we have exploited lexical and dependencyrelations for temporal relation classification.
Wehypothesize that semantic relations, in particularpredicate-argument relations, could be useful for thetask.
Consider the following example.
(5) ?What sector is stepping forward topick up the slack??
he asked.Using SENNA (Collobert et al 2011), a PropBank-style semantic role labeler, we know that forward isin the directional argument of the predicate stepping.This enables us to infer that an Includes relation ex-ists between stepping and forward since intuitivelyan action includes a direction.As another example, consider another PropBank-style predicate-argument relation, cause.
Assumingthat e2 is in e1?s cause argument, we can infer thate2 occurs Before e1 since intuitively the cause of anaction precedes the action.Consequently, we create features for tempo-ral relation classification based on four typesof PropBank-style predicate-argument relations,namely directional, manner, temporal, and cause.Specifically, using SENNA?s output, we create fourbinary features that encode whether argument e2 isrelated to predicate e1 through the four types of rela-tions, and we create another four binary features thatencode whether argument e1 is related to predicatee2 through the four types of relations.4.1.6 Discourse RelationsRhetorical relations such as causation, elaborationand enablement could aid in tracking the temporalprogression of the discourse (Hitzeman et al 1995).Hence, unlike syntactic dependencies and predicate-argument relations through which we can identifyintra-sentential temporal relations, discourse rela-tions can potentially be exploited to discover bothinter-sentential and intra-sentential temporal rela-tions.
However, no recent work has attempted touse discourse relations for temporal relation clas-922(6) { Arg1 Hewlett-Packard Co. said it raised its stake in Octel Communications Corp. to 8.5% of thecommon shares outstanding.
Arg1} { Arg2 RESTATEMENT In a Securities and Exchange Commis-sion filing, Hewlett-Packard said it now holds 1,384,119 Octel common shares Arg2}.
(7) { Arg1 Reports said that Saudi Arabia told U.S. oil companies of a 15?20 percent cutback in its oilsupply in September.
Arg1} { Conn SYNCHRONY Meanwhile Conn} { Arg2 Egypt?s Middle EastAgency said Thursday that Saddam was the target of an assassination attempt.
Arg2}Table 2: Examples illustrating the usefulness of discourse relations for temporal relation classification.sification.
In this subsection, we examine whetherwe can improve a temporal relation identifier viaexplicit and implicit PDTB-style discourse relationsautomatically extracted by Lin et als (2013) end-to-end discourse parser.Let us first review PDTB-style discourse rela-tions.
Each relation is represented by a triple (Arg1,sense, Arg2), where Arg1 and Arg2 are the two ar-guments of the relation and sense is the sense/typeof the relation.
A discourse relation can be explicitor implicit.
An explicit relation is triggered by a dis-course connective.
On the other hand, an implicitrelation is not triggered by a discourse connective,and may exist only between two consecutive sen-tences.
Generally, implicit relations are much harderto identify than their explicit counterparts.Next, to motivate why discourse relations can beuseful for temporal relation classification, we usetwo examples (see Table 2), one involving an im-plicit relation (Example (6)) and the other an explicitrelation (Example (7)).
For convenience, both sen-tences are also annotated using Lin et als (2013)discourse parser, which marks up the two argumentswith the Arg1 and Arg2 tags and outputs the rela-tion sense next to the beginning of Arg2.In (6), we aim to determine the order relation be-tween the reporting event said and the occurrenceevent filing.
The parser determines that a RESTATE-MENT implicit relation exists between the two sen-tences.
Intuitively, if no asynchronous relations canbe found among the events in two discourse unitsconnected by the RESTATEMENT relation, then thetemporal relation between two temporally linkedevents within these units is likely to be either Iden-tity or Simultaneous.
In this case, we can rule outIdentity: since said and filing belong to differentevent classes, they are not coreferent.In (7), we aim to determine the anchor relationbetween the reporting event said and the date Thurs-day.
The parser determines that a SYNCHRONYexplicit relation triggered by Meanwhile exists be-tween the two sentences.
Intuitively, if a temporallyrelated reporting event and date occur within differ-ent discourse units connected by the SYNCHRONYrelation, then it is likely that the event Is Includedin the date.
Note that without this discourse relation,it could be difficult for a machine to confidently as-sociate a reporting event with a date occurring in adifferent discourse segment.Given the potential usefulness of discourse rela-tions for temporal relation classification, we createfour features based on discourse relations.
In thefirst feature, if e1 is in Arg1, e2 is in Arg2, and Arg1and Arg2 possess an explicit relation with sense s,then its feature value is s; otherwise its value isNULL.
In the second feature, if e2 is in Arg1, e1 is inArg2, and Arg1 and Arg2 possess a explicit relationwith sense s, then its feature value is s; otherwiseits value is NULL.
The third and fourth features arecomputed in the same way as the first two features,except that they are computed over implicit ratherthan explicit relations.4.2 Manual Rule CreationAs noted before, we adopt a hybrid learning-basedand rule-based approach to temporal relation clas-sification.
Hence, in addition to training a tempo-ral relation classifier, we also manually design a setof rules in which each rule returns a temporal rela-tion type for a given test instance.
We hypothesizethat a rule-based approach can complement a purelylearning-based approach, since a human could com-bine the available linguistic features into rules usingcommonsense knowledge that may not be accessibleto a learning algorithm.The design of the rules is partly based on intu-923ition and partly data-driven: we first use our intu-ition to come up with a rule and then manually re-fine it based on the observations we made on theTimeBank data.
For this purpose, we partition theTimeBank documents into five folds of roughly thesame size, reserving three folds for developing ourrules and using the remaining two folds for evaluat-ing final system performance.
We order these rulesin decreasing order of accuracy, where the accuracyof a rule is defined as the number of times the ruleyields the correct temporal relation type divided bythe number of times it is applied, as measured on thethree development folds.
A new instance is classi-fied using the first applicable rule in the ruleset.Some of these rules were shown in the previ-ous subsection when we motivated each feature typewith examples.
The complete set of rules can be ac-cessed via our website.64.3 Combining Rules and Machine LearningWe investigate three ways to combine the hand-crafted rules and the machine-learned classifier.In the first method, we employ all of the rules asadditional features for training the classifier.
Thevalue of each such feature is the temporal relationtype predicted by the corresponding rule.The second method can be viewed as an extensionof the first one.
Given a test instance, we first applyto it the ruleset composed only of rules that are atleast 80% accurate.
If none of the rules is applicable,we classify it using the classifier employed in thefirst method.7The third method is essentially the same as thesecond, except we do not employ the rules as fea-tures when training the classifier.5 Evaluation5.1 Experimental SetupDataset.
As mentioned before, we partition the183 documents in the TimeBank (v1.2) corpus intofive folds of roughly the same size, reserving threefolds (say Folds 1?3) for manual rule development6http://www.hlt.utdallas.edu/?jld082000/temporal-relations/7Although this classifier is applied to only those test in-stances that the rules cannot handle, we did not retrain it ononly those training instances that the rules cannot handle.and using the remaining two folds (say Folds 4?5)for testing.
We perform two-fold cross-validationexperiments using the two test folds.
In the first foldexperiment, we train a temporal relation classifier onFolds 1?4 and test on Fold 5; and in the second foldexperiment, we train the classifier on all but Fold 4and test on Fold 4.
The results reported in the rest ofthe paper are averaged over the two test folds.Evaluation metrics.
We employ accuracy (Acc)and macro F-score (Fma).
Accuracy is the per-centage of correctly classified test instances, and isthe standard evaluation metric for temporal relationclassification.
Since each test instance belongs toone of the 14 temporal relation types, accuracy is thesame as micro F-score.
On the other hand, macro F-score is rarely used to evaluate this task.
We chose itbecause it could provide insights into how well ourapproach performs on the minority classes.5.2 Results and DiscussionTable 3 shows the two-fold cross-validation resultsfor our 14-class temporal relation classification task.The six columns of the table correspond to six dif-ferent system architectures.
The ?Feature?
columncorresponds to a purely learning-based architecturewhere the results are obtained simply by training atemporal relation classifier using the available fea-tures.
The next two columns correspond to twopurely rule-based architectures, differing by whetherall rules are used regardless of their accuracy orwhether only high-accuracy rules (i.e., those that areat least 80% accurate) are used.
The rightmost threecolumns correspond to the three ways of combiningrules and machine learning described in Section 4.3.On the other hand, the rows of the table differ interms of what features are available to a system.
Inrow 1, only the baseline features are available.
In thesubsequent rows, the six types of features discussedin Section 4 are added incrementally to the baselinefeature set.
This means that the last row correspondsto the case where all feature types are used.A point merits clarification.
It may not be imme-diately clear how to interpret the results under, forinstance, the ?All Rules?
column.
In other words,it may not be clear what it means to add the sixtypes of features incrementally to a rule-based sys-tem.
Recall that one of our goals is to comparea purely learning-based system with a purely rule-924Features All Rules All Rules with Features + Rules + Rules + Features +accuracy ?
0.8 Rules as Features Features Rules as FeaturesFeature Type Acc Fma Acc Fma Acc Fma Acc Fma Acc Fma Acc Fma1 Baseline 45.3 24.9 ?
?
?
?
?
?
?
?
?
?2 + Pairwise 46.5 25.8 37.6 26.5 5.1 13.9 46.7 26.5 48.0 31.9 48.2 32.13 + Dependencies 47.0 25.9 39.0 27.8 6.9 15.7 47.2 26.7 49.2 32.3 49.2 32.64 + WordNet 46.9 26.0 43.5 30.4 6.9 15.7 47.5 26.8 49.2 32.3 49.5 32.85 + Webster 46.9 25.8 43.3 29.9 6.9 15.7 48.1 26.8 49.2 32.0 50.1 33.16 + PropBank 47.2 26.0 44.3 30.5 8.1 16.6 48.0 26.8 49.5 32.2 50.0 33.07 + Discourse 48.1 26.6 47.5 35.1 12.8 23.3 48.9 27.5 53.0 36.0 53.4 36.6Table 3: Two-fold cross-validation accuracies and macro F-scores as features are added incrementally to the baseline.based system, since we hypothesized that humansmay be better at combining the available featuresto form rules than a learning algorithm would be.To facilitate this comparison, all and only those fea-tures that are available to a learning-based system ina given row can be used in hand-crafting the rulesof the rule-based system in the same row.
The othercolumns involving the use of rules can be interpretedin a similar manner.The highest accuracy and macro F-score areachieved when all types of features are used incombination with the ?Rules + Features + Rulesas Features?
architecture.
Specifically, this systemachieves an accuracy of 53.4% and a macro F-scoreof 36.6% on the 2000-instance test set.
This trans-lates to a relative error reduction of 15?16% in com-parison to the baseline result shown in row 1.
Acloser examination of these results reveals that thehand-crafted rules used by the system correctly clas-sify 239 of the 305 test instances to which they areapplicable.
In other words, the rules achieve a preci-sion of 78.3% and a recall of 15.3% on the test data.Our results suggest that the rules are effective atimproving performance when they are used to makeclassification decisions prior to the application ofthe classifier, as the performance of the ?Rules +Features + Rules as Features?
architecture is sig-nificantly better than that of the ?Features + Rulesas Features?
architecture.8 On the other hand, the?Rules + Features + Rules as Features?
architecturedoes not benefit from the use of rules as features,as its performance is statistically indistinguishablefrom that of the ?Rules + Features?
architecture.Nevertheless, both ?Rules + Features + Rules asFeatures?
and ?Rules + Features?
are significantly8Unless otherwise stated, all statistical significance tests arepaired t-tests, with p < 0.05.better than the remaining four architectures.
Thissuggests that the best-performing approach for our14-class temporal relation classification task is thehybrid approach where high-accuracy rules are firstapplied and then the learned classifier is used to clas-sify those cases that cannot be handled by the rules.Among the remaining four architectures, ?AllRules with accuracy ?
0.8?, the version of the rule-based architecture where only the high-accuracyrules are used, performs significantly worse than theothers, presumably because the coverage of the rule-set is low.
The results of the two feature-based archi-tectures, ?Features?
and ?Features + Rules as Fea-tures?, are statistically indistinguishable from eachother at the p < 0.01 level.
At the p < 0.05level, however, their results are mixed: ?Features +Rules as Features?
is better than ?Features?
accord-ing to accuracy, whereas the reverse is true accord-ing to macro F-score.
Combining these results withthose we discussed above concerning the ?Rules +Features?
and ?Rules + Features + Rules as Fea-tures?
architectures, we can conclude that the fea-tures encoding the hand-crafted rules are (mildly)useful only when used in combination with a weak-performing system.
Finally, comparing the ?Fea-tures?
architecture and the ?All Rules?
architecture,we also see mixed results: ?Features?
is better than?All Rules?
according to accuracy, whereas the re-verse is true according to macro F-score.
Theseresults confirm our earlier hypothesis that the rule-based system is indeed better at identifying instancesof minority relation types.Next, to determine whether the addition of a par-ticular type of features to the feature set is use-ful, we apply the paired t-test to each pair of ad-jacent rows in Table 3.
We found that addingpairwise features, dependency relations, and most925Event-Event Event-TimeFeature Type Acc Fma Acc Fma1 Baseline 36.7 15.6 63.3 19.22 + Pairwise 40.4 25.4 64.7 24.23 + Dependencies 42.4 28.4 64.9 25.44 + WordNet 42.6 28.1 64.7 25.35 + Webster 43.0 29.7 64.6 25.36 + PropBank 43.2 28.6 64.3 25.17 + Discourse 46.8 36.3 65.4 26.4Table 4: Event-event and event-time classification resultsof our best system (Rules + Features+ Rules as features).importantly, discourse relations significantly im-proves both accuracy and macro F-score (p < 0.05).Adding the Webster relations improves accuracy at aslightly lower significance level (p < 0.07) but doesnot significantly improve macro F-score.
Some-what counter-intuitively, the WordNet and predicate-argument relations are not useful.
We speculate thattheir failure to improve performance could be at-tributed to the fact that these relations are extractedby imperfect analyzers.
Additional experiments in-volving the use of gold-standard quality features areneeded to precisely determine the reason.Recall that the results shown in Table 3 were com-puted over both the order (i.e., event-event) and an-chor (i.e., event-time) temporal relations.
To gainadditional insights into our best-performing system,we show in Table 4 its performance on classify-ing event-event and event-time relations separately.In comparison to the baseline, both accuracy andmacro F-score increase significantly when our sys-tem is used in combination with all feature types.In particular, our system yields a relative error re-duction of 16?25% for event-event classification and6?9% for event-time classification over the base-line.
The pairwise features, as well as dependencyrelations and discourse relations, contribute signif-icantly to the classification of both event-event andevent-time relations.Finally, we show in Table 5 the per-class resultsof the baseline system and our best-performing sys-tem.
As we can see, our system performs signifi-cantly better than the baseline on all relation types,owing to a simultaneous rise in recall and precision.6 ConclusionsWe have investigated a knowledge-rich, hybrid ap-proach to the 14-class temporal relation classifica-Baseline Our SystemRelation R P F R P FSimultaneous 22.5 30.5 25.9 29.5 39.5 33.8Identity 56.5 51.5 53.9 59.0 57.5 58.2Before 39.5 38.5 39.0 50.5 50.5 50.5After 50.5 35.0 41.4 59.5 44.5 50.9IBefore 0.0 0.0 0.0 32.5 85.5 47.1IAfter 0.0 0.0 0.0 5.5 50.0 9.9Includes 54.5 50.5 52.4 61.0 55.5 58.1Is Included 71.5 64.5 67.8 74.5 65.0 69.4During 11.0 31.0 16.2 19.0 34.5 24.5During Inv 14.0 20.0 16.5 19.5 40.5 26.3Begins 4.5 10.0 6.2 37.0 43.5 40.0Begun By 6.5 14.5 9.0 35.0 44.0 39.0Ends 6.5 10.0 7.9 23.5 70.0 35.2Ended By 9.0 10.0 9.5 29.0 26.5 27.7Table 5: Per-class results of the baseline system and ourbest system (Rules + Features+ Rules as features).tion task.
Results on the TimeBank corpus showthat our approach achieves a relative error reductionof 15?16% over a learning-based baseline that em-ploys a state-of-the-art feature set.
Our results sug-gest that (1) the pairwise features, dependency rela-tions, and discourse relations are useful for temporalrelation classification; and (2) hand-crafted rules canbetter handle the skewed class distribution underly-ing our dataset via improving minority class predic-tion.
To our knowledge, we are the first to (1) re-port results for the 14-class temporal relation clas-sification task on TimeBank; (2) successfully em-ploy PDTB-style discourse relations to improve thistask; and (3) show that a hybrid approach to this taskcan yield better results than either a rule-based orlearning-based approach.
To stimulate research onthis task, we make our complete set of hand-craftedrules available to other researchers.
We believe thathybrid rule-based and learning-based approaches arepromising approaches to language processing tasksthat require complex reasoning and hope that theywill be given more attention in the community.AcknowledgmentsWe thank the three anonymous reviewers for theirdetailed and insightful comments on an earlier draftof the paper.
This work was supported in part byNSF Grants IIS-1147644 and IIS-1219142.
Anyopinions, findings, or conclusions expressed in thispaper are those of the authors and do not necessarilyreflect the views or official policies of NSF.926ReferencesNathanael Chambers, Shan Wang, and Dan Jurafsky.2007.
Classifying temporal relations between events.In Proceedings of the 45th Annual Meeting of the Asso-ciation for Computational Linguistics Companion Vol-ume: Proceedings of the Demo and Poster Sessions,pages 173?176.Timothy Chklovski and Patrick Pantel.
2004.
Verbo-cean: Mining the web for fine-grained semantic verbrelations.
In Proceedings of the 2004 Conference onEmpirical Methods in Natural Language Processing,pages 33?40.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel P. Kuksa.2011.
Natural language processing (almost) fromscratch.
Journal of Machine Learning Research,12:2493?2537.Eun Young Ha, Alok Baikadi, Carlyle Licata, and JamesLester.
2010.
NCSU: Modeling temporal relationswith markov logic and lexical ontology.
In Proceed-ings of the 5th International Workshop on SemanticEvaluation, pages 341?344.Janet Hitzeman, Marc Moens, and Claire Grover.
1995.Algorithms for analysing the temporal structure of dis-course.
In Proceedings of the 7th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, pages 253?260.Judith Klavans and Philip Resnik, editors.
1994.
TheBalancing Act: Combining Symbolic and StatisticalApproaches to Language.
Association for Computa-tional Linguistics.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2013.A PDTB-styled end-to-end discourse parser.
NaturalLanguage Engineering (to appear).Hector Llorens, Estela Saquete, and Borja Navarro.2010.
TIPSem (English and Spanish): EvaluatingCRFs and semantic roles in TempEval-2.
In Proceed-ings of the 5th International Workshop on SemanticEvaluation, pages 284?291.Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong MinLee, and James Pustejovsky.
2006.
Machine learningof temporal relations.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 753?760.Congmin Min, Munirathnam Srikanth, and AbrahamFowler.
2007.
LCC-TE: A hybrid approach to tem-poral relation identification in news text.
In Proceed-ings of the Fourth International Workshop on SemanticEvaluations (SemEval-2007), pages 219?222.Seyed Abolghasem Mirroshandel and GholamrezaGhassem-Sani.
2011.
Temporal relation extractionusing expectation maximization.
In Proceedings of theInternational Conference Recent Advances in NaturalLanguage Processing 2011, pages 218?225.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The penn discourse treebank 2.0.
InProceedings of the 6th International Conference onLanguage Resources and Evaluation.Georgiana Pus?cas?u.
2007.
WVALI: Temporal relationidentification by syntactico-semantic analysis.
In Pro-ceedings of the Fourth International Workshop on Se-mantic Evaluations (SemEval-2007), pages 484?487.James Pustejovsky, Patrick Hanks, Roser Sauri, AndrewSee, David Day, Lisa Ferro, Robert Gaizauskas, Mar-cia Lazo, Andrea Setzer, and Beth Sundheim.
2003.The TimeBank corpus.
In Corpus Linguistics, pages647?656.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2004.
Support vec-tor machine learning for interdependent and structuredoutput spaces.
In Proceedings of the 21st InternationalConference on Machine Learning, pages 104?112.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
SemEval-2007 Task 15: TempEval tempo-ral relation identification.
In Proceedings of theFourth International Workshop on Semantic Evalua-tions (SemEval-2007), pages 75?80.Marc Verhagen, Roser Sauri, Tommaso Caselli, andJames Pustejovsky.
2010.
SemEval-2010 Task 13:TempEval-2.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation, pages 57?62.927
