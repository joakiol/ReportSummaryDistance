Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 248?255,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsFormalism-Independent Parser Evaluation with CCG and DepBankStephen ClarkOxford University Computing LaboratoryWolfson Building, Parks RoadOxford, OX1 3QD, UKstephen.clark@comlab.ox.ac.ukJames R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australiajames@it.usyd.edu.auAbstractA key question facing the parsing commu-nity is how to compare parsers which usedifferent grammar formalisms and producedifferent output.
Evaluating a parser on thesame resource used to create it can leadto non-comparable accuracy scores and anover-optimistic view of parser performance.In this paper we evaluate a CCG parser onDepBank, and demonstrate the difficultiesin converting the parser output into Dep-Bank grammatical relations.
In addition wepresent a method for measuring the effec-tiveness of the conversion, which providesan upper bound on parsing accuracy.
TheCCG parser obtains an F-score of 81.9%on labelled dependencies, against an upperbound of 84.8%.
We compare the CCGparser against the RASP parser, outperform-ing RASP by over 5% overall and on the ma-jority of dependency types.1 IntroductionParsers have been developed for a variety of gram-mar formalisms, for example HPSG (Toutanova etal., 2002; Malouf and van Noord, 2004), LFG (Ka-plan et al, 2004; Cahill et al, 2004), TAG (Sarkarand Joshi, 2003), CCG (Hockenmaier and Steed-man, 2002; Clark and Curran, 2004b), and variantsof phrase-structure grammar (Briscoe et al, 2006),including the phrase-structure grammar implicit inthe Penn Treebank (Collins, 2003; Charniak, 2000).Different parsers produce different output, for ex-ample phrase structure trees (Collins, 2003), depen-dency trees (Nivre and Scholz, 2004), grammati-cal relations (Briscoe et al, 2006), and formalism-specific dependencies (Clark and Curran, 2004b).This variety of formalisms and output creates a chal-lenge for parser evaluation.The majority of parser evaluations have used testsets drawn from the same resource used to developthe parser.
This allows the many parsers based onthe Penn Treebank, for example, to be meaningfullycompared.
However, there are two drawbacks to thisapproach.
First, parser evaluations using differentresources cannot be compared; for example, the Par-seval scores obtained by Penn Treebank parsers can-not be compared with the dependency F-scores ob-tained by evaluating on the Parc Dependency Bank.Second, using the same resource for developmentand testing can lead to an over-optimistic view ofparser performance.In this paper we evaluate a CCG parser (Clarkand Curran, 2004b) on the Briscoe and Carroll ver-sion of DepBank (Briscoe and Carroll, 2006).
TheCCG parser produces head-dependency relations, soevaluating the parser should simply be a matter ofconverting the CCG dependencies into those in Dep-Bank.
Such conversions have been performed forother parsers, including parsers producing phrasestructure output (Kaplan et al, 2004; Preiss, 2003).However, we found that performing such a conver-sion is a time-consuming and non-trivial task.The contributions of this paper are as follows.First, we demonstrate the considerable difficultiesassociated with formalism-independent parser eval-uation, highlighting the problems in converting the248output of a parser from one representation to an-other.
Second, we develop a method for measur-ing how effective the conversion process is, whichalso provides an upper bound for the performance ofthe parser, given the conversion process being used;this method can be adapted by other researchersto strengthen their own parser comparisons.
Andthird, we provide the first evaluation of a wide-coverage CCG parser outside of CCGbank, obtainingimpressive results on DepBank and outperformingthe RASP parser (Briscoe et al, 2006) by over 5%overall and on the majority of dependency types.2 Previous WorkThe most common form of parser evaluation is to ap-ply the Parseval metrics to phrase-structure parsersbased on the Penn Treebank, and the highest re-ported scores are now over 90% (Bod, 2003; Char-niak and Johnson, 2005).
However, it is unclearwhether these high scores accurately reflect the per-formance of parsers in applications.
It has been ar-gued that the Parseval metrics are too forgiving andthat phrase structure is not the ideal representationfor a gold standard (Carroll et al, 1998).
Also, us-ing the same resource for training and testing mayresult in the parser learning systematic errors whichare present in both the training and testing mate-rial.
An example of this is from CCGbank (Hock-enmaier, 2003), where all modifiers in noun-nouncompound constructions modify the final noun (be-cause the Penn Treebank, from which CCGbank isderived, does not contain the necessary informationto obtain the correct bracketing).
Thus there are non-negligible, systematic errors in both the training andtesting material, and the CCG parsers are being re-warded for following particular mistakes.There are parser evaluation suites which havebeen designed to be formalism-independent andwhich have been carefully and manually corrected.Carroll et al (1998) describe such a suite, consistingof sentences taken from the Susanne corpus, anno-tated with Grammatical Relations (GRs) which spec-ify the syntactic relation between a head and depen-dent.
Thus all that is required to use such a scheme,in theory, is that the parser being evaluated is ableto identify heads.
A similar resource ?
the ParcDependency Bank (DepBank) (King et al, 2003)?
has been created using sentences from the PennTreebank.
Briscoe and Carroll (2006) reannotatedthis resource using their GRs scheme, and used it toevaluate the RASP parser.Kaplan et al (2004) compare the Collins (2003)parser with the Parc LFG parser by mapping LFG F-structures and Penn Treebank parses into DepBankdependencies, claiming that the LFG parser is con-siderably more accurate with only a slight reduc-tion in speed.
Preiss (2003) compares the parsers ofCollins (2003) and Charniak (2000), the GR finderof Buchholz et al (1999), and the RASP parser, us-ing the Carroll et al (1998) gold-standard.
The PennTreebank trees of the Collins and Charniak parsers,and the GRs of the Buchholz parser, are mapped intothe required GRs, with the result that the GR finderof Buchholz is the most accurate.The major weakness of these evaluations is thatthere is no measure of the difficultly of the conver-sion process for each of the parsers.
Kaplan et al(2004) clearly invested considerable time and ex-pertise in mapping the output of the Collins parserinto the DepBank dependencies, but they also notethat ?This conversion was relatively straightforwardfor LFG structures .
.
.
However, a certain amount ofskill and intuition was required to provide a fair con-version of the Collins trees?.
Without some measureof the difficulty ?
and effectiveness ?
of the con-version, there remains a suspicion that the Collinsparser is being unfairly penalised.One way of providing such a measure is to con-vert the original gold standard on which the parseris based and evaluate that against the new gold stan-dard (assuming the two resources are based on thesame corpus).
In the case of Kaplan et al (2004), thetesting procedure would include running their con-version process on Section 23 of the Penn Treebankand evaluating the output against DepBank.
As wellas providing some measure of the effectiveness ofthe conversion, this method would also provide anupper bound for the Collins parser, giving the scorethat a perfect Penn Treebank parser would obtain onDepBank (given the conversion process).We perform such an evaluation for the CCG parser,with the surprising result that the upper bound onDepBank is only 84.8%, despite the considerable ef-fort invested in developing the conversion process.2493 The CCG ParserClark and Curran (2004b) describes the CCG parserused for the evaluation.
The grammar used by theparser is extracted from CCGbank, a CCG version ofthe Penn Treebank (Hockenmaier, 2003).
The gram-mar consists of 425 lexical categories ?
expressingsubcategorisation information ?
plus a small num-ber of combinatory rules which combine the cate-gories (Steedman, 2000).
A supertagger first assignslexical categories to the words in a sentence, whichare then combined by the parser using the combi-natory rules and the CKY algorithm.
A log-linearmodel scores the alternative parses.
We use thenormal-form model, which assigns probabilities tosingle derivations based on the normal-form deriva-tions in CCGbank.
The features in the model aredefined over local parts of the derivation and includeword-word dependencies.
A packed chart represen-tation allows efficient decoding, with the Viterbi al-gorithm finding the most probable derivation.The parser outputs predicate-argument dependen-cies defined in terms of CCG lexical categories.More formally, a CCG predicate-argument depen-dency is a 5-tuple: ?hf , f, s, ha, l?, where hf is thelexical item of the lexical category expressing thedependency relation; f is the lexical category; s isthe argument slot; ha is the head word of the ar-gument; and l encodes whether the dependency islong-range.
For example, the dependency encodingcompany as the object of bought (as in IBM boughtthe company) is represented as follows:?bought, (S\NP1 )/NP2 , 2, company, ??
(1)The lexical category (S\NP1 )/NP2 is the cate-gory of a transitive verb, with the first argument slotcorresponding to the subject, and the second argu-ment slot corresponding to the direct object.
Thefinal field indicates the nature of any long-range de-pendency; in (1) the dependency is local.The predicate-argument dependencies ?
includ-ing long-range dependencies ?
are encoded in thelexicon by adding head and dependency annota-tion to the lexical categories.
For example, theexpanded category for the control verb persuadeis (((S [dcl]persuade\NP 1)/(S [to]2\NP X))/NP X,3).
Nu-merical subscripts on the argument categories rep-resent dependency relations; the head of the finaldeclarative sentence is persuade; and the head of theinfinitival complement?s subject is identified withthe head of the object, using the variable X, as instandard unification-based accounts of control.Previous evaluations of CCG parsers have used thepredicate-argument dependencies from CCGbank asa test set (Hockenmaier and Steedman, 2002; Clarkand Curran, 2004b), with impressive results of over84% F-score on labelled dependencies.
In this paperwe reinforce the earlier results with the first evalua-tion of a CCG parser outside of CCGbank.4 Dependency Conversion to DepBankFor the gold standard we chose the version of Dep-Bank reannotated by Briscoe and Carroll (2006),consisting of 700 sentences from Section 23 of thePenn Treebank.
The B&C scheme is similar to theoriginal DepBank scheme (King et al, 2003), butoverall contains less grammatical detail; Briscoe andCarroll (2006) describes the differences.
We chosethis resource for the following reasons: it is pub-licly available, allowing other researchers to com-pare against our results; the GRs making up the an-notation share some similarities with the predicate-argument dependencies output by the CCG parser;and we can directly compare our parser against anon-CCG parser, namely the RASP parser.
We chosenot to use the corpus based on the Susanne corpus(Carroll et al, 1998) because the GRs are less likethe CCG dependencies; the corpus is not based onthe Penn Treebank, making comparison more diffi-cult because of tokenisation differences, for exam-ple; and the latest results for RASP are on DepBank.The GRs are described in Briscoe and Carroll(2006) and Briscoe et al (2006).
Table 1 lists theGRs used in the evaluation.
As an example, the sen-tence The parent sold Imperial produces three GRs:(det parent The), (ncsubj sold parent ) and(dobj sold Imperial).
Note that some GRs ?
inthis example ncsubj ?
have a subtype slot, givingextra information.
The subtype slot for ncsubj isused to indicate passive subjects, with the null value?
?
for active subjects and obj for passive subjects.Other subtype slots are discussed in Section 4.2.The CCG dependencies were transformed intoGRs in two stages.
The first stage was to createa mapping between the CCG dependencies and the250GR descriptionconj coordinatoraux auxiliarydet determinerncmod non-clausal modifierxmod unsaturated predicative modifiercmod saturated clausal modifierpmod PP modifier with a PP complementncsubj non-clausal subjectxsubj unsaturated predicative subjectcsubj saturated clausal subjectdobj direct objectobj2 second objectiobj indirect objectpcomp PP which is a PP complementxcomp unsaturated VP complementccomp saturated clausal complementta textual adjunct delimited by punctuationTable 1: GRs in B&C?s annotation of DepBankGRs.
This involved mapping each argument slot inthe 425 lexical categories in the CCG lexicon ontoa GR.
In the second stage, the GRs created from theparser output were post-processed to correct some ofthe obvious remaining differences between the CCGand GR representations.In the process of performing the transformationwe encountered a methodological problem: with-out looking at examples it was difficult to createthe mapping and impossible to know whether thetwo representations were converging.
Briscoe et al(2006) split the 700 sentences in DepBank into a testand development set, but the latter only consists of140 sentences which was not enough to reliably cre-ate the transformation.
There are some developmentfiles in the RASP release which provide examples ofthe GRs, which were used when possible, but theseonly cover a subset of the CCG lexical categories.Our solution to this problem was to convert thegold standard dependencies from CCGbank intoGRs and use these to develop the transformation.
Sowe did inspect the annotation in DepBank, and com-pared it to the transformed CCG dependencies, butonly the gold-standard CCG dependencies.
Thus theparser output was never used during this process.We also ensured that the dependency mapping andthe post processing are general to the GRs schemeand not specific to the test set or parser.4.1 Mapping the CCG dependencies to GRsTable 2 gives some examples of the mapping; %l in-dicates the word associated with the lexical categoryCCG lexical category slot GR(S [dcl ]\NP1 )/NP2 1 (ncsubj %l %f )(S [dcl ]\NP1 )/NP2 2 (dobj %l %f)(S\NP)/(S\NP)1 1 (ncmod %f %l)(NP\NP1 )/NP2 1 (ncmod %f %l)(NP\NP1 )/NP2 2 (dobj %l %f)NP [nb]/N1 1 (det %f %l)(NP\NP1 )/(S [pss]\NP)2 1 (xmod %f %l)(NP\NP1 )/(S [pss]\NP)2 2 (xcomp %l %f)((S\NP)\(S\NP)1 )/S [dcl ]2 1 (cmod %f %l)((S\NP)\(S\NP)1 )/S [dcl ]2 2 (ccomp %l %f)((S [dcl ]\NP1 )/NP2 )/NP3 2 (obj2 %l %f)(S [dcl ]\NP1 )/(S [b]\NP)2 2 (aux %f %l)Table 2: Examples of the dependency mappingand %f is the head of the constituent filling the argu-ment slot.
Note that the order of %l and %f varies ac-cording to whether the GR represents a complementor modifier, in line with the Briscoe and Carroll an-notation.
For many of the CCG dependencies, themapping into GRs is straightforward.
For example,the first two rows of Table 2 show the mapping forthe transitive verb category (S [dcl ]\NP1 )/NP2 : ar-gument slot 1 is a non-clausal subject and argumentslot 2 is a direct object.Creating the dependency transformation is moredifficult than these examples suggest.
The first prob-lem is that the mapping from CCG dependencies toGRs is many-to-many.
For example, the transitiveverb category (S [dcl ]\NP)/NP applies to the cop-ula in sentences like Imperial Corp. is the parentof Imperial Savings & Loan.
With the default anno-tation, the relation between is and parent would bedobj, whereas in DepBank the argument of the cop-ula is analysed as an xcomp.
Table 3 gives some ex-amples of how we attempt to deal with this problem.The constraint in the first example means that, when-ever the word associated with the transitive verb cat-egory is a form of be, the second argument is xcomp,otherwise the default case applies (in this case dobj).There are a number of categories with similar con-straints, checking whether the word associated withthe category is a form of be.The second type of constraint, shown in the thirdline of the table, checks the lexical category of theword filling the argument slot.
In this example, if thelexical category of the preposition is PP/NP , thenthe second argument of (S [dcl ]\NP)/PP maps toiobj; thus in The loss stems from several fac-tors the relation between the verb and prepositionis (iobj stems from).
If the lexical category of251CCG lexical category slot GR constraint example(S [dcl ]\NP1 )/NP2 2 (xcomp %l %f) word=be The parent is Imperial(dobj %l %f) The parent sold Imperial(S [dcl ]\NP1 )/PP2 2 (iobj %l %f) cat=PP/NP The loss stems from several factors(xcomp %l %f) cat=PP/(S [ng ]\NP) The future depends on building ties(S [dcl ]\NP1 )/(S [to]\NP)2 2 (xcomp %f %l %k) cat=(S [to]\NP)/(S [b]\NP) wants to wean itself away fromTable 3: Examples of the many-to-many nature of the CCG dependency to GRs mapping, and a ternary GRthe preposition is PP/(S [ng ]\NP), then the GRis xcomp; thus in The future depends on buildingties the relation between the verb and prepositionis (xcomp depends on).
There are a number ofCCG dependencies with similar constraints, many ofthem covering the iobj/xcomp distinction.The second difficulty is that not all the GRs are bi-nary relations, whereas the CCG dependencies are allbinary.
The primary example of this is to-infinitivalconstructions.
For example, in the sentence Thecompany wants to wean itself away from expensivegimmicks, the CCG parser produces two dependen-cies relating wants, to and wean, whereas there isonly one GR: (xcomp to wants wean).
The fi-nal row of Table 3 gives an example.
We im-plement this constraint by introducing a %k vari-able into the GR template which denotes the ar-gument of the category in the constraint column(which, as before, is the lexical category of theword filling the argument slot).
In the example, thecurrent category is (S [dcl ]\NP1 )/(S [to]\NP)2 ,which is associated with wants; this combines with(S [to]\NP)/(S [b]\NP), associated with to; andthe argument of (S [to]\NP)/(S [b]\NP) is wean.The %k variable allows us to look beyond the argu-ments of the current category when creating the GRs.A further difficulty is that the head passing con-ventions differ between DepBank and CCGbank.
Byhead passing we mean the mechanism which de-termines the heads of constituents and the mecha-nism by which words become arguments of long-range dependencies.
For example, in the sentenceThe group said it would consider withholding roy-alty payments, the DepBank and CCGbank annota-tions create a dependency between said and the fol-lowing clause.
However, in DepBank the relationis between said and consider, whereas in CCGbankthe relation is between said and would.
We fixed thisproblem by defining the head of would consider tobe consider rather than would, by changing the an-notation of all the relevant lexical categories in theCCG lexicon (mainly those creating aux relations).There are more subject relations in CCGbank thanDepBank.
In the previous example, CCGbank has asubject relation between it and consider, and also itand would, whereas DepBank only has the relationbetween it and consider.
In practice this means ig-noring a number of the subject dependencies outputby the CCG parser.Another example where the dependencies differis the treatment of relative pronouns.
For example,in Sen. Mitchell, who had proposed the streamlin-ing, the subject of proposed is Mitchell in CCGbankbut who in DepBank.
Again, we implemented thischange by fixing the head annotation in the lexicalcategories which apply to relative pronouns.4.2 Post processing of the GR outputTo obtain some idea of whether the schemes wereconverging, we performed the following oracle ex-periment.
We took the CCG derivations fromCCGbank corresponding to the sentences in Dep-Bank, and forced the parser to produce gold-standard derivations, outputting the newly createdGRs.
Treating the DepBank GRs as a gold-standard,and comparing these with the CCGbank GRs, gaveprecision and recall scores of only 76.23% and79.56% respectively (using the RASP evaluationtool).
Thus given the current mapping, the perfectCCGbank parser would achieve an F-score of only77.86% when evaluated against DepBank.On inspecting the output, it was clear that anumber of general rules could be applied to bringthe schemes closer together, which was imple-mented as a post-processing script.
The first setof changes deals with coordination.
One sig-nificant difference between DepBank and CCG-bank is the treatment of coordinations as argu-ments.
Consider the example The president andchief executive officer said the loss stems from sev-eral factors.
For both DepBank and the trans-formed CCGbank there are two conj GRs arising252from the coordination: (conj and president) and(conj and officer).
The difference arises in thesubject of said: in DepBank the subject is and:(ncsubj said and ), whereas in CCGbank thereare two subjects: (ncsubj said president ) and(ncsubj said officer ).
We deal with this dif-ference by replacing any pairs of GRs which differonly in their arguments, and where the argumentsare coordinated items, with a single GR containingthe coordination term as the argument.Ampersands are a frequently occurring problemin WSJ text.
For example, the CCGbank analysisof Standard & Poor?s index assigns the lexical cat-egory N /N to both Standard and &, treating themas modifiers of Poor, whereas DepBank treats & asa coordinating term.
We fixed this by creating conjGRs between any & and the two words either side;removing the modifier GR between the two words;and replacing any GRs in which the words either sideof the & are arguments with a single GR in which &is the argument.The ta relation, which identifies text adjuncts de-limited by punctuation, is difficult to assign cor-rectly to the parser output.
The simple punctuationrules used by the parser do not contain enough in-formation to distinguish between the various casesof ta.
Thus the only rule we have implemented,which is somewhat specific to the newspaper genre,is to replace GRs of the form (cmod say arg)with (ta quote arg say), where say can be anyof say, said or says.
This rule applies to only a smallsubset of the ta cases but has high enough precisionto be worthy of inclusion.A common source of error is the distinction be-tween iobj and ncmod, which is not surprising giventhe difficulty that human annotators have in distin-guishing arguments and adjuncts.
There are manycases where an argument in DepBank is an adjunctin CCGbank, and vice versa.
The only change wehave made is to turn all ncmod GRs with of as themodifier into iobj GRs (unless the ncmod is a par-titive predeterminer).
This was found to have highprecision and applies to a large number of cases.There are some dependencies in CCGbank whichdo not appear in DepBank.
Examples include anydependencies in which a punctuation mark is one ofthe arguments; these were removed from the output.We attempt to fill the subtype slot for some GRs.The subtype slot specifies additional informationabout the GR; examples include the value obj in apassive ncsubj, indicating that the subject is an un-derlying object; the value num in ncmod, indicating anumerical quantity; and prt in ncmod to indicate averb particle.
The passive case is identified as fol-lows: any lexical category which starts S [pss]\NPindicates a passive verb, and we also mark any verbsPOS tagged VBN and assigned the lexical categoryN /N as passive.
Both these rules have high preci-sion, but still leave many of the cases in DepBankunidentified.
The numerical case is identified usingtwo rules: the num subtype is added if any argumentin a GR is assigned the lexical category N /N [num],and if any of the arguments in an ncmod is POStagged CD.
prt is added to an ncmod if the modi-fiee has any of the verb POS tags and if the modifierhas POS tag RP.The final columns of Table 4 show the accuracyof the transformed gold-standard CCGbank depen-dencies when compared with DepBank; the sim-ple post-processing rules have increased the F-scorefrom 77.86% to 84.76%.
This F-score is an upperbound on the performance of the CCG parser.5 ResultsThe results in Table 4 were obtained by parsing thesentences from CCGbank corresponding to thosein the 560-sentence test set used by Briscoe et al(2006).
We used the CCGbank sentences becausethese differ in some ways to the original Penn Tree-bank sentences (there are no quotation marks inCCGbank, for example) and the parser has beentrained on CCGbank.
Even here we experiencedsome unexpected difficulties, since some of the to-kenisation is different between DepBank and CCG-bank and there are some sentences in DepBankwhich have been significantly shortened comparedto the original Penn Treebank sentences.
We mod-ified the CCGbank sentences ?
and the CCGbankanalyses since these were used for the oracle ex-periments ?
to be as close to the DepBank sen-tences as possible.
All the results were obtained us-ing the RASP evaluation scripts, with the results forthe RASP parser taken from Briscoe et al (2006).The results for CCGbank were obtained using theoracle method described above.253RASP CCG parser CCGbankRelation Prec Rec F Prec Rec F Prec Rec F # GRsaux 93.33 91.00 92.15 94.20 89.25 91.66 96.47 90.33 93.30 400conj 72.39 72.27 72.33 79.73 77.98 78.84 83.07 80.27 81.65 595ta 42.61 51.37 46.58 52.31 11.64 19.05 62.07 12.59 20.93 292det 87.73 90.48 89.09 95.25 95.42 95.34 97.27 94.09 95.66 1 114ncmod 75.72 69.94 72.72 75.75 79.27 77.47 78.88 80.64 79.75 3 550xmod 53.21 46.63 49.70 43.46 52.25 47.45 56.54 60.67 58.54 178cmod 45.95 30.36 36.56 51.50 61.31 55.98 64.77 69.09 66.86 168pmod 30.77 33.33 32.00 0.00 0.00 0.00 0.00 0.00 0.00 12ncsubj 79.16 67.06 72.61 83.92 75.92 79.72 88.86 78.51 83.37 1 354xsubj 33.33 28.57 30.77 0.00 0.00 0.00 50.00 28.57 36.36 7csubj 12.50 50.00 20.00 0.00 0.00 0.00 0.00 0.00 0.00 2dobj 83.63 79.08 81.29 87.03 89.40 88.20 92.11 90.32 91.21 1 764obj2 23.08 30.00 26.09 65.00 65.00 65.00 66.67 60.00 63.16 20iobj 70.77 76.10 73.34 77.60 70.04 73.62 83.59 69.81 76.08 544xcomp 76.88 77.69 77.28 76.68 77.69 77.18 80.00 78.49 79.24 381ccomp 46.44 69.42 55.55 79.55 72.16 75.68 80.81 76.31 78.49 291pcomp 72.73 66.67 69.57 0.00 0.00 0.00 0.00 0.00 0.00 24macroaverage 62.12 63.77 62.94 65.61 63.28 64.43 71.73 65.85 68.67microaverage 77.66 74.98 76.29 82.44 81.28 81.86 86.86 82.75 84.76Table 4: Accuracy on DepBank.
F-score is the balanced harmonic mean of precision (P ) and recall (R):2PR/(P + R).
# GRs is the number of GRs in DepBank.The CCG parser results are based on automati-cally assigned POS tags, using the Curran and Clark(2003) tagger.
The coverage of the parser on Dep-Bank is 100%.
For a GR in the parser output to becorrect, it has to match the gold-standard GR exactly,including any subtype slots; however, it is possiblefor a GR to be incorrect at one level but correct ata subsuming level.1 For example, if an ncmod GR isincorrectly labelled with xmod, but is otherwise cor-rect, it will be correct for all levels which subsumeboth ncmod and xmod, for example mod.
The micro-averaged scores are calculated by aggregating thecounts for all the relations in the hierarchy, includingthe subsuming relations; the macro-averaged scoresare the mean of the individual scores for each rela-tion (Briscoe et al, 2006).The results show that the performance of the CCGparser is higher than RASP overall, and also higheron the majority of GR types (especially the morefrequent types).
RASP uses an unlexicalised pars-ing model and has not been tuned to newspaper text.On the other hand it has had many years of develop-ment; thus it provides a strong baseline for this testset.
The overall F-score for the CCG parser, 81.86%,is only 3 points below that for CCGbank, which pro-1The GRs are arranged in a hierarchy, with those in Table 1 atthe leaves; a small number of more general GRs subsume these(Briscoe and Carroll, 2006).vides an upper bound for the CCG parser (given theconversion process being used).6 ConclusionA contribution of this paper has been to high-light the difficulties associated with cross-formalismparser comparison.
Note that the difficulties are notunique to CCG, and many would apply to any cross-formalism comparison, especially with parsers usingautomatically extracted grammars.
Parser evalua-tion has improved on the original Parseval measures(Carroll et al, 1998), but the challenge remains todevelop a representation and evaluation suite whichcan be easily applied to a wide variety of parsersand formalisms.
Despite the difficulties, we havegiven the first evaluation of a CCG parser outside ofCCGbank, outperforming the RASP parser by over5% overall and on the majority of dependency types.Can the CCG parser be compared with parsersother than RASP?
Briscoe and Carroll (2006) give arough comparison of RASP with the Parc LFG parseron the different versions of DepBank, obtaining sim-ilar results overall, but they acknowledge that the re-sults are not strictly comparable because of the dif-ferent annotation schemes used.
Comparison withPenn Treebank parsers would be difficult because,for many constructions, the Penn Treebank trees and254CCG derivations are different shapes, and reversingthe mapping Hockenmaier used to create CCGbankwould be very difficult.
Hence we challenge otherparser developers to map their own parse output intothe version of DepBank used here.One aspect of parser evaluation not covered in thispaper is efficiency.
The CCG parser took only 22.6seconds to parse the 560 sentences in DepBank, withthe accuracy given earlier.
Using a cluster of 18 ma-chines we have also parsed the entire Gigaword cor-pus in less than five days.
Hence, we conclude thataccurate, large-scale, linguistically-motivated NLP isnow practical with CCG.AcknowledgementsWe would like to thanks the anonymous review-ers for their helpful comments.
James Curran wasfunded under ARC Discovery grants DP0453131and DP0665973.ReferencesRens Bod.
2003.
An efficient implementation of a new DOPmodel.
In Proceedings of the 10th Meeting of the EACL,pages 19?26, Budapest, Hungary.Ted Briscoe and John Carroll.
2006.
Evaluating the accuracyof an unlexicalized statistical parser on the PARC DepBank.In Proceedings of the Poster Session of COLING/ACL-06,Sydney, Australia.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.
Thesecond release of the RASP system.
In Proceedings ofthe Interactive Demo Session of COLING/ACL-06, Sydney,Australia.Sabine Buchholz, Jorn Veenstra, and Walter Daelemans.
1999.Cascaded grammatical relation assignment.
In Proceedingsof EMNLP/VLC-99, pages 239?246, University of Mary-land, June 21-22.A.
Cahill, M. Burke, R. O?Donovan, J. van Genabith, andA.
Way.
2004.
Long-distance dependency resolution in au-tomatically acquired wide-coverage PCFG-based LFG ap-proximations.
In Proceedings of the 42nd Meeting of theACL, pages 320?327, Barcelona, Spain.John Carroll, Ted Briscoe, and Antonio Sanfilippo.
1998.Parser evaluation: a survey and a new proposal.
In Proceed-ings of the 1st LREC Conference, pages 447?454, Granada,Spain.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
In Pro-ceedings of the 43rd Annual Meeting of the ACL, Universityof Michigan, Ann Arbor.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.In Proceedings of the 1st Meeting of the NAACL, pages 132?139, Seattle, WA.Stephen Clark and James R. Curran.
2004a.
The importance ofsupertagging for wide-coverage CCG parsing.
In Proceed-ings of COLING-04, pages 282?288, Geneva, Switzerland.Stephen Clark and James R. Curran.
2004b.
Parsing the WSJusing CCG and log-linear models.
In Proceedings of the42nd Meeting of the ACL, pages 104?111, Barcelona, Spain.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguistics,29(4):589?637.James R. Curran and Stephen Clark.
2003.
Investigating GISand smoothing for maximum entropy taggers.
In Proceed-ings of the 10th Meeting of the EACL, pages 91?98, Bu-dapest, Hungary.Julia Hockenmaier and Mark Steedman.
2002.
Generativemodels for statistical parsing with Combinatory CategorialGrammar.
In Proceedings of the 40th Meeting of the ACL,pages 335?342, Philadelphia, PA.Julia Hockenmaier.
2003.
Data and Models for StatisticalParsing with Combinatory Categorial Grammar.
Ph.D. the-sis, University of Edinburgh.Ron Kaplan, Stefan Riezler, Tracy H. King, John T. MaxwellIII, Alexander Vasserman, and Richard Crouch.
2004.Speed and accuracy in shallow and deep stochastic parsing.In Proceedings of the HLT Conference and the 4th NAACLMeeting (HLT-NAACL?04), Boston, MA.Tracy H. King, Richard Crouch, Stefan Riezler, Mary Dalrym-ple, and Ronald M. Kaplan.
2003.
The PARC 700 Depen-dency Bank.
In Proceedings of the LINC-03 Workshop, Bu-dapest, Hungary.Robert Malouf and Gertjan van Noord.
2004.
Wide coverageparsing with stochastic attribute value grammars.
In Pro-ceedings of the IJCNLP-04 Workshop: Beyond shallow anal-yses - Formalisms and statistical modeling for deep analyses,Hainan Island, China.Joakim Nivre and Mario Scholz.
2004.
Deterministic depen-dency parsing of English text.
In Proceedings of COLING-2004, pages 64?70, Geneva, Switzerland.Judita Preiss.
2003.
Using grammatical relations to compareparsers.
In Proceedings of the 10th Meeting of the EACL,pages 291?298, Budapest, Hungary.Anoop Sarkar and Aravind Joshi.
2003.
Tree-adjoining gram-mars and its application to statistical parsing.
In Rens Bod,Remko Scha, and Khalil Sima?an, editors, Data-orientedparsing.
CSLI.Mark Steedman.
2000.
The Syntactic Process.
The MIT Press,Cambridge, MA.Kristina Toutanova, Christopher Manning, Stuart Shieber, DanFlickinger, and Stephan Oepen.
2002.
Parse disambiguationfor a rich HPSG grammar.
In Proceedings of the First Work-shop on Treebanks and Linguistic Theories, pages 253?263,Sozopol, Bulgaria.255
