Automatic Text Categorizationin Terms of Genre and AuthorEfs ta th ios  S tamatatos*University of PatrasGeorge  Kokk inak is  *University of PatrasN ikos  Fakotak is  tUniversity of PatrasThe two main factors that characterize a text are its content and its style, and both can be usedas a means of categorization.
In this paper we present an approach to text categorization iterms of genre and author for Modern Greek.
In contrast o previous tylometric approaches,we attempt to take full advantage of existing natural language processing (NLP) tools.
To thisend, we propose aset of style markers including analysis-level measures that represent the way inwhich the input text has been analyzed and capture useful stylistic information without additionalcost.
We present aset of small-scale but reasonable experiments in text genre detection, authoridentification, and author verification tasks and show that the proposed method performs betterthan the most popular distributional lexical measures, i.e.,functions of vocabulary richness andfrequencies of occurrence of the most frequent words.
All the presented experiments are based onunrestricted text downloaded from the World Wide Web without any manual text preprocessingor text sampling.
Various performance issues regarding the training set size and the significance ofthe proposed style markers are discussed.
Our system can be used in any application that requiresfast and easily adaptable text categorization i terms of stylistically homogeneous categories.Moreover, the procedure of defining analysis-level markers can be followed in order to extractuseful stylistic information using existing text processing tools.1.
IntroductionThe rapid expansion of the World Wide Web (WWW) in recent years has resultedin the creation of large volumes of text in electronic form.
NLP applications uch asinformation retrieval and information extraction have been developed to treat thisinformation automatically.
Since the Internet is a very heterogeneous domain, theseapplications usually involve text categorization tasks with the following desiderata:?
minimal computational cost,?
ability to handle real-world (or unrestricted) text, and?
either ease of adaptation to a certain domain or application or generalityin order to cover a wide range of domains or applications.
* University of Patras, Department ofElectrical & Computer Engineering, 26500 Patras, Greece.
E-mail:stamatatos@wcl.ee.upatras.gr.t University of Patras, Department ofElectrical & Computer Engineering, 26500 Patras, Greece.
E-mail:fakotaki@wcl.ee.upatras.gr.University of Patras, Department ofElectrical & Computer Engineering, 26500 Patras, Greece.
E-mail:gkokkin@wcl.ee.upatras.gr.?
2001 Association for Computational LinguisticsComputational Linguistics Volume 26, Number 4The two main factors that characterize a text are its content and its style, both ofwhich can be used for categorization purposes.
Nevertheless, the literature on compu-tational stylistics is very limited in comparison to the work dealing with the proposi-tional content of the text.
This is due to the lack of a formal definition of style as wellas to the inability of current NLP systems to incorporate stylistic theories that requirecomplicated information.
In contrast o traditional stylistics based on formal linguistictheories, the use of statistical methods in style processing has proved to be a reliableapproach (Biber 1995).
According to the stylostatisticians, a given style is defined as aset of measurable patterns, called style markers.
We adopt this definition in this study.Typical classificatory tasks in computational stylistics are the following:?
Text genre detection concerns the identification of the kind (or functionalstyle) of the text (Karlgren and Cutting 1994; Michos et al 1996; Kessler,Nunberg, and Schi.itze 1997).?
Authorship attribution concerns the identification of the author of the text(Holmes and Forsyth 1995; Baayen, Van Halteren, and Tweedie 1996;Tweedie, Singh, and Holmes 1996).These tasks have so far been considered completely separate problems.
A typical textcategorization system utilizing stylistic analysis (i.e., either text genre or authorshipidentification) is usually based on the following modules:.2.Extraction of style markers: A set of quantifiable measures are definedand a text-processing tool is usually developed, to automatically countthem.Classification procedure: A disambiguation method (e.g., statistical,connectionist, etc.)
is applied to classify the text in question into apredefined category (i.e., a text genre or an author).The most important computational pproaches to text genre detection have fo-cused on the use of simple measures that can be easily detected and reliably countedby a computational tool (Kessler, Nunberg, and Sch~itze 1997).
To this end, various setsof style markers have been proposed (Karlgren and Cutting 1994), all of which are, inessence, subsets of the set used by Biber (1995), who ranked registers along seven di-mensions by applying factor analysis to a set of lexical and syntactic style markers thathad been manually counted.
In general, the current text genre detection approachestry to avoid using existing text processing tools rather than taking advantage of them.Authorship attribution studies have focused on the establishment of the authorshipof anonymous or doubtful iterary texts, such as the Federalist Papers, 12 of whichare of disputed authorship (Mosteller and Wallace 1984; Holmes and Forsyth 1995).Typical methodologies deal with a limited number of candidate authors using longtext samples of several thousand words.
Almost all the approaches to this task arebased mainly on distributional lexical style markers.
In a review paper of authorshipattribution studies, Holmes (1994) claims: "yet, to date, no stylometrist has managedto establish a methodology which is better able to capture the style of a text than thatbased on lexical items" (p. 87).To the best of our knowledge, there is still no computational system that candistinguish the texts of a randomly chosen group of authors without requiring humanassistance in the selection of both the most appropriate set of style markers and themost accurate disambiguation procedure.472Stamatatos, Fakotakis, and Kokkinakis Text CategorizationIn this paper we describe an approach to text categorization i terms of genre andauthor based on a new stylometric method that utilizes already existing NLP tools.
Inaddition to the style markers relevant o the actual output of the NLP tool (i.e., theanalyzed text), we introduce analysis-level style markers, which represent the way inwhich the text has been analyzed by that tool.
Such measures contain useful stylisticinformation and are easily available without additional computational cost.To illustrate, we apply the proposed technique to text categorization tasks for Mod-ern Greek corpora using an already existing sentence and chunk boundaries detector(SCBD) in unrestricted Modern Greek text (Stamatatos, Fakotakis, and Kokkinakis2000).
We present a set of small-scale but reasonable xperiments in text genre detec-tion, author identification, and author verification tasks and show that the performanceof the proposed method is better in comparison with the most popular distributionallexical measures, i.e., functions of vocabulary richness and frequencies of occurrenceof the most frequent words.
Our approach is trainable and can be easily adapted toany set of stylistically homogeneous categories.We begin by discussing work relevant o text genre detection and authorship attri-bution focusing on the various types of style markers employed (Section 2).
Next, wedescribe the proposed solution for extracting style markers using already existing NLPtools (Section 3) and apply our method to Modern Greek (Section 4), briefly describingthe SCBD and proposing our set of style markers.
The techniques used for automaticcategorization of the stylistic vectors are discussed in Section 5.
Section 6 deals withthe application of our approach to text genre detection, and Section 7, with authorshipattribution, for both author identification and author verification.
In Sections 8 and9, we discuss important performance issues of the proposed methodology and theconclusions that can be drawn from this study.2.
Current Trends in StylometryThe main feature that characterizes both text genre detection and authorship attri-bution studies is the selection of the most appropriate measures, namely, those thatreflect the style of the writing.
Various sets have been proposed in the literature.
Inthis section, we classify the most popular of the proposed style markers, taking intoaccount he information required for their calculation rather than the task they havebeen applied to.2.1 Token-Level MeasuresThe simplest approach considers the sample text as a set of tokens grouped in sen-tences.
Typical measures of this category are word count, sentence count, character perword count, and punctuation marks count.
Such features have been widely used inboth text genre detection and authorship attribution research since they can be easilydetected and computed.
It is worth noting that the first pioneering works in author-ship attribution, when no powerful computational systems were available, were basedexclusively on these measures.
For example, Morton (1965) used sentence length fortesting the authorship of Greek prose, Brinegar (1963) adopted word length measures,and Brainerd's (1974) approach was based on distribution of syllables per word.
Al-though such measures seemed to work in specific cases, they became subject o heavycriticism for their lack of generality (Smith 1983, 1985).2.2 Syntactic AnnotationThe use of measures related to syntactic annotation of the text is very common in textgenre detection.
Such measures provide very useful information for the exploration473Computational Linguistics Volume 26, Number 4of the characteristics of style (Biber 1995).
Typical paradigms are passive count, nom-inalization count, and counts of the frequency of various syntactic ategories (e.g.,part-of-speech tags).
Recently, syntactic information has also been applied to author-ship attribution.
Specifically, Baayen, Van Halteren, and Tweedie (1996) used frequen-cies of occurrence of rewrite rules as they appear in a syntactically annotated corpusand proved that they perform better than word frequencies.
Their calculation requirestagged or parsed text, however.
Current NLP tools are not able to provide accuratecalculation results for many of the previously proposed style markers.
In the study ofregister variation conducted by Biber (1995), a subset of the measures (i.e., the simplestones) was calculated by computational tools and the remaining were counted manu-ally.
Additionally, the automatically acquired measures were counterchecked manually.Many researchers, therefore, try to avoid the use of features related to syntactic an-notation in order to avoid such problems (Kessler, Nunberg, and Sch~itze 1997).
Asa result, the recent advances in computational linguistics have not notably affectedresearch in computational stylistics.2.3 Vocabulary RichnessVarious measures have been proposed for capturing the richness or the diversity ofthe vocabulary of a text and they have been applied mainly to authorship attributionstudies.
The most typical measure of this category is the type-token ratio V/N, whereV is the size of the vocabulary of the sample text, and N is the number of tokens ofthe sample text.
Similar features are the hapax legomena (i.e., words occurring oncein the sample text) and the dislegomena (i.e., words occurring twice in the sampletext).
Since text length dramatically affects these features, many researchers have pro-posed functions of these features that they claim are text length independent (Honor61979; Yule 1944; Sichel 1975).
Additionally, instead of using a single measure, someresearchers have used a set of such vocabulary richness functions in combination withmultivariate statistical techniques to achieve better results in authorship attribution(Holmes 1992).
In general, these measures are not computationally expensive.
How-ever, according to results of recent studies, the majority of the vocabulary richnessfunctions are highly text length dependent and quite unstable for texts shorter than1,000 words (Tweedie and Baayen 1998).2.4 Common Word FrequenciesInstead of using vocabulary distribution measures, ome researchers have counted thefrequency of occurrence of individual words in the sample text.
Such counts are a reli-able discriminating factor (Karlgreen and Cutting 1994; Kessler, Nunberg, and Schi~tze1997) and have been applied to many works in text genre detection.
Their calculation issimple, but nontrivial effort is required for the selection of the most appropriate wordsfor a given problem.
Morever, the words that best distinguish a given group of authorscannot be applied to a different group of authors with the same success (Holmes andForsyth 1995).
Oakman (1980) notes: "The lesson seems clear not only for functionwords but for authorship word studies in general: particular words may work forspecific cases uch as 'The Federalist Papers' but cannot be counted on for other anal-yses" (p. 28).
Furthermore, the results of such studies are highly language dependent.Michos et al (1996) introduce the idea of grouping certain words in categories, uch asidiomatic expressions, cientific terminology, formal words, and so on.
Although thissolution is language independent, i  requires the construction of a complicated com-putational mechanism for the automated detection of the categories in the sample text.Alternatively, the use of sets of common high-frequency words (typically 30 or50 words) has been applied mainly to authorship attribution studies (Burrows 1987).474Stamatatos, Fakotakis, and Kokkinakis Text CategorizationNCPtoo, A ,ys,sI i measures JL.
measures jFigure 1The proposed method.The application of a principal components analysis on the frequencies of occurrenceof the most frequent words achieved remarkable results in plotting the texts in thespace of the first two principal components, for a wide variety of authors (Burrows1992).
This approach is language independent and computationally inexpensive.
Vari-ous additional restrictions to this basic method have been proposed (e.g., separation ofcommon homographic forms, removal of proper names from the most frequent wordlist, etc.
), aimed at improving its performance.
For a fully automated system, suchrestrictions require robust and accurate NLP tools.3.
The Proposed MethodOur method attempts to exploit already existing NLP tools for the extraction of stylisticinformation.
To this end, we use two types of measures, as can be seen in Figure 1:?
measures relevant o the actual output of the NLP tool (i.e., usuallytagged or parsed text), and?
measures relevant o the particular methodology by which the NLP toolanalyzes the text (analysis-level measures).Thus, the set of style markers is adapted to a specific, already existing NLP tool,taking into account its particular properties.
Analysis-level measures capture usefulstylistic information without additional cost.
The NLP tool is not considered a blackbox.
Therefore, full access to its source code is required in order to define and measureanalysis-level style markers.
Moreover, tool-specific knowledge, rather than language-specific knowledge, is required for the definition of such measures.
In other words,researchers using this approach can define analysis-level measures based on their deepunderstanding of a particular NLP tool even if they are not familiar with the naturallanguage to which the methodology is to be applied.To illustrate the proposed method, we apply it to Modern Greek using the SCBD,an existing NLP tool able to detect sentence and chunk boundaries in unrestricted text,as described in the next section.
In addition to a set of easily computable f atures (i.e.,token-level and syntax-level measures) provided by the actual output of the SCBD,475Computational Linguistics Volume 26, Number 4we use a set of analysis-level features, i.e., measures that represent the way in whichthe input text has been analyzed by the SCBD.The particular analysis-level style markers can be calculated only when this specificcomputational tool is utilized.
However, the SCBD is a general-purpose tool and wasnot designed for providing stylistic information exclusively.
Thus, any NLP tool (e.g.,part-of-speech taggers, parsers, etc.)
can provide similar measures.
The appropriateanalysis-level style markers have to be defined according to the methodology used bythe tool in order to analyze the text.
For example, some similar measures have beenused in stylistic experiments in information retrieval on the basis of a robust parserbuilt for information retrieval purposes (Strzalkowski 1994).
This parser produces treesto represent the structure of the sentences that compose the text.
However, it is setto "skip" or surrender attempts to parse clauses after reaching a time-out hreshold.When the parser skips, it notes that in the parse tree.
The measures proposed byKarlgren (1999) as indicators of clausal complexity are the average parse tree depthand the number of parser skips per sentence, which in essence are analysis-level stylemarkers.4.
Style Markers for Modem GreekAs mentioned above, the subset of style markers used for Modern Greek depends onthe text analysis by the specific NLP tool, the SCBD.
Thus, before describing the setof style markers we used, we briefly present he main features of the SCBD.4.1 Description of the SCBDThe SCBD is a text-processing tool able to deal with unrestricted Modern Greek text.No manual preprocessing is required.
It performs the following procedures:Sentence boundary detection: The following punctuation marks areconsidered potential sentence boundaries: period, exclamation point,question mark, and ellipsis.
A set of automatically acquireddisambiguation rules (Stamatatos, Fakotakis, and Kokkinakis 1999) isapplied to every potential sentence boundary in order to locate theactual sentence boundaries.
These rules utilize neither lexicons withspecialized information or abbreviation lists.Chunk boundary detection: Intrasentential phrase detection is achievedthrough multiple-pass parsing making use of an approximately450-keyword lexicon (i.e., closed-class words such as articles andprepositions) and a 300-suffix lexicon containing the most commonsuffixes of Modern Greek words.
Initially, using the suffix lexicon, a setof morphological descriptions i assigned to any word of the sentencenot included in the keyword lexicon.
If the suffix of a word does notmatch any of the entries of the suffix lexicon, then no morphologicaldescription is assigned to this word.
It is marked as a special word andis not ignored in subsequent analysis.
Then, each parsing pass (fivepasses are performed) analyzes a part of the sentence, based on theresults of the previous passes, and the remaining part is kept for thesubsequent passes.
In general, the first passes try to detect simple casesthat are easily recognizable, while the last passes deal with morecomplicated ones.
Cases that are not covered by the disambiguationrules remain unanalyzed.
The detected chunks are noun phrases (NPs),476Stamatatos, Fakotakis, and Kokkinakis Text CategorizationUnrestricted Texti,Sentence Boundary I:!Detection II.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Assignment of ~ t_.._._.__._._.lMorphological Descriptions H~_ ~=~ ..... ~ .
.
.
.
.
.
.
.
.
.
.
.
~,X~K~yv~r~ts II Multiple-Pass Parsing ~ \[-_.._.______.JParsed TextFigure 2The SCBD structure.prepositional phrases (PPs), verb phrases (VPs), and adverbial phrases(APs).
In addition, two chunks are usually connected by a sequence ofconjunctions (CONs).The SCBD is able to cope rapidly with any piece of text, even ill-formed text, andits performance is comparable to more sophisticated systems that require more com-plicated resources.
Figure 2 gives an overview of the SCBD.
An example of its outputfor a sample text, together with a rough English translation (included in parentheses),is given below (note that special words, those that do not match with any of the storedsuffixes, are marked with an asterisk):VP\[&eu 0gAco uoz pg{oo (I don't want to pour)\] NP\[A&& (oil)\] PP\[crrr/9~wr~& (in thefire)\] CON\[of&kale (but)\] VP\[rr~?re4a; (I believe)\] CON\[drL (that)\] NP\[r/err~fldpvu~rr/(the encumbrance)\] PP\[o-rou rrpo~rcoko7Lcr#6 (of the budget)\] PP\[ozrr6rov?
flov&evrg?
(by the deputies)\] VP\[&u #rcopeg uce rrpoe#erpeirc~L (can not bemeasured)\] #6uo (merely) PP\[#e rc~ 5*&?.*6px.
rcou c~uc,Spo#~n&u (with the 5bil.
Dr. of the retroactive salaries)\] troy (that) NP\[rc~po~u re&evrcdc~ (they tooklately)\] VP\[rrponc~&cburc~?
(causing)\] NP\[rr/(Sva~op&~ r~\]g ~oLu~?
7v,&#r/~ (thediscontent of the public opinion)\].It is worth noting that we did not modify the structure of the SCBD in order tocalculate style markers, aside from adding simple functions for their measurement.4.2 Stylometric LevelsOur aim during the definition of the set of style markers was to take full advantageof the analysis of the text by the SCBD.
To this end, we included measures relevant tothe actual output of this tool as well as measures relevant to the methodology used bythe SCBD to analyze the text.
Specifically, the proposed set of style markers comprisesthree levels:?
Token Level: The sample text is considered as a set of tokens grouped insentences.
This level is based on the output of the sentence boundary477Computational Linguistics Volume 26, Number 4detector:CodeM01M02M03Descriptiondetected sentences/words 1punctuation marks/wordsdetected sentences/potential sentence boundariesPhrase Level: The sample text is considered as a set of phrases (i.e.,chunks).
This level is based on the output of the chunk boundarydetector:CodeM04M05M06M07M08M09M10Ml lM12M13Descriptiondetected NPs/total detected chunksdetected VPs/total detected chunksdetected APs/total detected chunksdetected PPs/total detected chunksdetected CONs/total detected chunkswords included in NPs/detected NPswords included in VPs/detected VPswords included in APs/detected APswords included in PPs/detected PPswords included in CONs/detected CONsAnalysis Level: Measures that represent the way in which the sampletext has been analyzed by the particular methodology of the SCBD areincluded here:CodeM14M15M16M17M18M19M20M21M22Descriptiondetected keywords/wordsspecial words/wordsassigned morphological descriptions/wordschunks' morphological descriptions/total detected chunkswords remaining unanalyzed after pass 1/wordswords remaining unanalyzed after pass 2/wordswords remaining unanalyzed after pass 3/wordswords remaining unanalyzed after pass 4/wordswords remaining unanalyzed after pass 5/wordsIt is clear that the analysis level contains extremely useful stylistic information.
Forexample, M14 and M15 are valuable markers that indicate of the percentage of high-frequency words and the percentage of unusual words included in the sample text,respectively.
M16 is a useful indicator of the morphological ambiguity of the wordsand M17 indicates the degree to which this ambiguity has been resolved.
Moreover,markers M18 to M22 indicate the syntactic omplexity of the text.
Since the first parsingpasses analyze the most common cases, it is easy to understand that a large part ofa syntactically complicated text would not be analyzed by them (e.g., high values forM18, M19, and M20 in conjunction with low values for M21 and M22).
Similarly, asyntactically simple text would be characterized by low values for M18, M19, and M20.1 We consider words as word tokens.478Stamatatos, Fakotakis, and Kokkinakis Text CategorizationNote that all the proposed style markers are produced as ratios of two relativemeasures in order for them to be stable over the text length.
However, they are notstandardized.5.
Text CategorizationThe methodology described in the previous ection provides a vector of 22 variablesfor each text.
For automatically classifying this vector into one group (either genre orauthor) various techniques are available, which stem from multivariate statistics (e.g.,discriminant analysis), neural networks, and machine learning (e.g., decision trees).Recently, Yang (1999) studied the performance of several classifiers on text categoriza-tion tasks and concluded that all the tested methods perform comparably when thetraining set comprises over 300 instances per category.
On the other hand, when thenumber of positive training instances per category is small (less than 10) a regression-like method called linear least-squares fit and k-nearest neighbors outperform neuralnetworks and naive Bayes classifiers (Yang and Liu 1999).In the present paper we used two well-known techniques of multivariate statistics:multiple regression and discriminant analysis.
The response of these techniques i veryfast since they are based on the calculation of simple linear functions.
Moreover, theirtraining procedures do not require excessive time or computational cost.
Thus, theycan be easily incorporated into a real-time application.5.1 Multiple RegressionMultiple regression predicts values of a group of response (dependent) variables froma collection of predictor (independent) variable values (Edwards 1979).
The responseis expressed as a linear combination of the predictor variables, namely:yi = bo + z~bli + z2b2i  + ? "
q- zrbri q- eiwhere yi is the response for the ith category (i.e., text genre), Zl, z2,... , Zr are the pre-dictor variables (i.e., in our case r = 22), b0, bli, b2i .
.
.
.
.
bri, are the unknown coefficientscalculated uring the training procedure, and ei is the random error.
An indication ofthe goodness of fit of the model is provided by the coefficient of determination, R 2,defined as follows:nR2 _ j= ly/E (yj - 9)j= lwhere n is the total amount of the training data (texts), 9 is the mean response, andfinally, ~j and yj are the estimated response and the training response value, respec-tively.
R 2 equals 1 if the fitted equation passes through all the data points, and, at theother extreme, equals 0.Moreover, multiple regression can also be used for the estimation of the signifi-cance of the independent variables.
In particular, the amount by which R 2 is reduced ifa certain independent variable is deleted from the regression equation (in other words,the contribution of the independent variable to R 2) is represented by the squared semi-partial correlation sri 2 (Tabachnick and Fidell 1996):sr~ = t~ (1 - a 2)df ~?s479Computational Linguistics Volume 26, Number 4where ti is the value of the t statistic for the ith variable and dffr?~ are the residualdegrees of freedom.
Thus, the contribution of an independent variable to R 2 can beexpressed as a function of the absolute value of t. The absolute t value of the jthestimated regression coefficient bj is calculated as follows:bjtbj = S--~where Sb i is the standard error.
The greater the t value, the more important the con-tribution of the independent variable (i.e., style marker) to the response value.5.2 Discriminant AnalysisThe mathematical objective of discriminant analysis is to weight and linearly com-bine the discriminating variables in some way so that the groups are forced to be asstatistically distinct as possible (Eisenbeis and Avery 1972).
The optimal discriminantfunction, therefore, is assumed to be a linear function of the variables and is deter-mined by maximizing the between-group variance while minimizing the within-groupvariance using a training sample.Discriminant analysis can be used for predicting the group membership of pre-viously unseen cases (i.e., test data) based on Mahalonobis distance (i.e., a measureof distance between two points in the space defined by multiple correlated variables).Initially, for each group, the location of the centroids, i.e., the points that representthe means for all variables in the multivariate space defined by the independent vari-ables, is determined.
Then, for each case the Mahalanobis distances from each of thegroup centroids are computed and the case is classified into the closest group.
TheMahalanobis distance d of a vector x from a mean vector mx is given by the formula:d 2 = (x  - mx)'C~-l(x - mx)where Cx is the covariance matrix of x.
Using this classification method we can alsoderive the probability that a case belongs to a particular group (i.e., posterior proba-bilities), which is roughly proportional to the Mahalanobis distance from that groupcentroid.
Discriminant analysis has been employed by researchers in automatic textgenre detection (Biber 1993b; Karlgren and Cutting 1994) since it offers a simple androbust solution despite the fact that it presupposes normal distributions of the dis-criminating variables.6.
Text Genre Detection6.1 Genre-based CorpusSince no Modern Greek corpus covering a wide range of text genres was available,we decided to compose one from scratch.
The corpus used in experiments in Michoset al (1996) includes a limited number of carefully selected and manually edited textsdivided into generic categories (e.g., journalistic, scientific, etc.).
In general, the useof already existing corpora not built for text genre detection (e.g., the Brown corpus)raises several problems ince such categories may not be stylistically homogeneous(Kessler, Nunberg, and Schiitze 1997).
The corpus used in our study contains textsthat meet the following criteria:?
Real-world text: The texts have to be already in electronic form and thusmay be ill-formed.480Stamatatos, Fakotakis, and Kokkinakis Text CategorizationTable 1The genre-based corpus.Code Text Genre Texts Words Source(Average)G01 Press editorial 25 729G02 Press reportage 25 902G03 Academic prose 25 2,120G04 Official documents 25 1,059G05 Literature 25 1,508G06 Recipes 25 109G07 Curricula vitae 25 333G08 Interviews 25 2,625G09 Planned speeches 25 2,569G10 Broadcast news, scripted 25 137Newspaper TO BHMANewspaper TO BHMAJournal of ARCHIVES OFHELLENIC PATHOLOGYHigh Court decisions,Ministerial decisionsVarious pagesMagazine NETLIFEVarious pagesNewspaper TO BHMAMinistry of defenseRadio station FLASH 9.61?
Raw text: Neither manually inserted tags nor other manualtext-preprocessing restrictions are set.?
Whole text: Neither text length limitations nor other manualtext-sampling restrictions are set.
In other words, a text has to beavailable as it appears in its source.We constructed a corpus by downloading texts from various WWW sites editedin Modern Greek, trying to cover as many genres as possible.
This corpus is shownin Table 1.
Although the complete set of text genres may differ significantly amongtwo languages (Biber 1995), they usually overlap to a great extent, especially for Indo-European languages.
The set we propose, therefore, can be compared to the ones usedin similar studies of English (Karlgren and Cutting 1994; Biber 1995).
Additionally,no manual preprocessing was performed aside from removing unnecessary headingsirrelevant to the text itself.It must also be pointed out that the last three text genres (i.e., G08, G09, andG10) refer to spoken language that has been transcribed either before (i.e., plannedspeeches, broadcast news) or after (i.e., interviews) it has been uttered.
On the otherhand, G01 to G07 refer to written language.The genre-based corpus was divided into a training part and a test part of equalsize.
Ten texts per genre were included in the training corpus and ten texts per genrewere included in the test corpus.
The remaining five texts per genre were used onlyin the experiments described in Section 7.6.2 Setting the BaselineTo evaluate the proposed approach, we decided to apply two previous tylometric ap-proaches that are based on distributional lexical measures to the same testing round:(i) a multivariate model of functions of vocabulary richness (Holmes 1992) and (ii) thefrequencies ofoccurrence of the most frequent words (Burrows 1992).
These two meth-ods were selected since they are language independent and computationally inexpen-sive.To measure the richness of the vocabulary, we used a set of five functions, namely,K proposed by Yule (1944), R proposed by Honor6 (1979), W proposed by Brunet(1978), S proposed by Sichel (1975), and D proposed by Simpson (1949), which are481Computational Linguistics Volume 26, Number 4defined as follows:10 4 (Ei~__i i 2Wi -  N)K =N 2(1001ogN) R -(1 - (~-))W = N v-~V2S -VV i ( i -  1)D = Z.-, 'NTlqL-i'~i=1 " "where Vi is the number of words used exactly i times (see Section 2.3 for the defini-tion of V and N) and o~ is a parameter usually fixed at 0.17.
The same set of func-tions has been used by Baayen and his colleagues for similar purposes (Baayen, VanHalteren, and Tweedie 1996).
For every text, these functions are calculated and a vec-tor of five parameters i produced.
These vectors can then be classified to the mostlikely genre by applying one of the classification techniques discussed in the previ-ous section.
Hereafter, this approach will be called VR (which stands for vocabularyrichness).The second method, which is lexically based, uses the frequencies of occurrence ofthe most frequent words of the training corpus as style markers.
Typically, sets of 30or 50 most frequent words are used (Baayen, Van Halteren, and Tweedie 1996; Holmesand Forsyth 1995).
For comparison purposes, we employed two sets of common wordsbased on 30 and 50 most frequent words of the training corpus, respectively.
Thus,for each text a vector of 30 (or 50) parameters indicating the frequencies of the mostfrequent words of the training corpus (normalized by the text length) are calculated.As above, these vectors can then be classified to the most likely genre.
These twoapproaches will be called CWF-30 and CWF-50 for common word frequencies and thenumber of the high-frequency words.6.3 ResultsThe entire corpus described in the previous ection was analyzed by the SCBD, whichautomatically provided avector of 22 parameters for each text.
The vectors of the train-ing corpus were used in order to extract he classification model using both multipleregression and discriminant analysis.
These classification models were then applied tothe vectors of the test corpus for cross-validating their performance on unseen cases.The same training and test procedure was performed for the VR approach and for theCWF-30 and CWF-50 methods.Comparative results in terms of identification error (i.e., erroneously classifiedtexts/total texts) are given in Figure 3.
In general, discriminant analysis seems to bebetter able to distinguish the texts of the test corpus.
The performance of the VRapproach is quite poor.
This is due to the limited text length of the majority of thetexts of the genre-based corpus (Tweedie and Baayen 1998).
Moreover, our approachis more accurate than the CWF-30 and the CWF-50.
The identification error rate ofour approach using both multiple regression and discriminant analysis is given inTable 2.
Although the average rror rate is equal for the two methodologies, there aresignificant differences in the disambiguation accuracy of certain text genres (see G01and G05).
In general, the error rate is more normally distributed using discriminantanalysis.
Moreover, approximately 60% of the identification errors using multiple re-482Stamatatos, Fakotakis, and Kokkinakis Text CategorizationOur approachVRCWF-50CWF-30\[\] Discriminant analysis ?
Multiple regression0.44?
.
.
.
.
.
,,,, i i  i , i  l 0 .410.22. .
I 0.2\]0.221 0.220 0.1 0.2 0.3 0.4Ident i f icat ion er rorFigure 3Comparative r sults for text genre detection.0.5Table 2The text genre detection results.Identification ErrorCode Multiple Regression Discriminant AnalysisG01 0.7 0.4G02 0.2 0.1G03 0.0 0.0G04 0.1 0.2G05 0.1 0.4G06 0.0 0.0G07 0.4 0.4G08 0.1 0.0G09 0.2 0.2G10 0.0 0.1Average 0.18 0.18gression were caused by G01 and G07, while 65% of the identification errors usingdiscriminant analysis were caused by G01, G05, and G07.
On the other hand, G04,G06, G08, and G10 are stylistically homogeneous to a great extent in both cases.The complete identification results of our method using discriminant analysis arepresented in a confusion matrix in Table 3.
Each row represents a text genre beingtested and the columns represent he classification results of the test texts of thatparticular genre.
The main misclassifications are as follows:?
press editorial ~ press reportage.
Notice that the texts were taken from thesame newspaper, which is published on a weekly basis.
In many cases,therefore, the reportage documents review a whole week and presentsome comments by the author.?
curricula vitae --~ official documents.
Both are usually characterized by anabstract style.?
literature ~ interviews and planned speeches.
These two text genres of thespoken language usually involve narration.483Computational Linguistics Volume 26, Number 4Table 3Confusion matrix for text genre detection using discriminant analysis.Actual ClassificationG01 G02 G03 G04 G05 G06 G07 G08 G09 G10Total TextsG01 6 3 0 0 0 0 0 0 1G02 0 9 0 0 0 0 0 0 0G03 0 0 10 0 0 0 0 0 0G04 0 1 0 8 0 0 0 0 0G05 0 0 0 0 6 0 0 2 2G06 0 0 0 0 0 10 0 0 0G07 0 0 0 3 0 0 6 0 0G08 0 0 0 0 0 0 0 10 0G09 1 0 0 0 0 0 0 1 8G10 0 0 0 1 0 0 0 0 00 101 100 101 100 100 101 100 100 109 104035302520151050t.
?3I\[\] Correct ?
ErrorI I AText  length  ( in words)Figure 4Text length related to accuracy for the text genre detection experiment.Note that spoken language text genres (i.e., G08-G10) have a lower identification errorrate, on average (0.10), than written language text genres (0.21) as calculated by eithermultiple regression or discriminant analysis.The classification accuracy of our method related to the text length for the textgenre experiment using multiple regression is presented in Figure 4.
Due to the stylistichomogeneity of recipes and broadcast news, the accuracy of texts shorter than 500 words(see Table 1) is relatively high.
In addition, texts over 1,500 words seem to be classifiedmore reliably.
Note that according to Biber (1990, 1993a) a text length of 1,000 wordsis adequate for representing the distributions of many core linguistic features of astylistic category.484Stamatatos, Fakotakis, and Kokkinakis Text CategorizationTable 4The structure of the Modern Greek weekly newspaper TO BHMA.Section Title (Translation) DescriptionCodeA TO BHMA (the tribune)BCDEISZTNEEX' E I IOXEE (new ages)TOAAAOBHMA (the other tribune)ANA FiTY~H (development)H &PAXMH ZAZ (your money)EI&IKH EKZ~O ZH (special issue)BIBAIA (books)TEXNE22KAIKAAAITEXNEZ~ (arts and artists)TA~I,~IA (travels)Editorials, diaries,reportage, politics,international ffairs,sport reviewsCultural supplementReview magazineBusiness, financePersonal financeIssue of the weekBook review supplementArt review supplementTravels upplement7.
Authorship Attribution7.1 Author-based CorpusIn authorship attribution experiments we chose to deal with texts taken from news-papers, since a wide variety of authors frequently publish their writings in the press,making the collection of a considerable number of texts for several authors easier.
Inparticular, the corpus used for this study comprises texts downloaded from the WWWsite of the Modern Greek weekly newspaper TO BHMA,  (the tribune).2 The structure ofthis newspaper is shown in Table 4.
We performed experiments based on two groupsof authors, namely:.2.Group A: Ten randomly selected authors whose writings are frequentlyfound in section A.
This section comprises texts written mainly byjournalists on a variety of current affairs.
Moreover, a certain author maysign texts from different text genres (e.g., editorial, reportage, tc.).
Notethat in many cases such writings are highly edited to conform to apredefined style, thus washing out specific characteristics of the authors,which complicates the task of attributing authorship.Group B: Ten randomly selected authors whose writings are frequentlyfound in section B.
This supplement comprises essays on science,culture, history, and so on, in other words, writings in which theidiosyncratic style of the author is not overshadowed by functionalobjectives.
In general, the texts included in the B section are written byscholars, rather than journalists.Analytical information on the author-based corpus is in Table 5.
All the downloadedtexts were taken from issues published uring 1998 in order to minimize the potentialchange of the personal style of an author over time.
The last column of this table refersto the thematic area of the majority of the writings of each author.
This informationwas not taken into account during the construction of the corpus.
The author-based2 The Web address i : http://tovima.dolnet.gr485Computational Linguistics Volume 26, Number 4Table 5The author-based corpus.Group Code Author Name Texts Words Thematic Area(Average)AA01 N. Nikolaou 20 797 EconomyA02 N. Marakis 20 871 International ffairsA03 D. Psychogios 20 535 PoliticsA04 G. Bitros 20 689 Politics, societyA05 D. Nikolakopoulos 20 1,162 Politics, societyA06 T. Lianos 20 696 SocietyA07 K. Chalbatzakis 20 1,061 TechnologyA08 G. Lakopoulos 20 1,248 PoliticsA09 R. Someritis 20 721 Politics, societyA10 D. Mitropoulos 20 888 International ffairsB01 D. Maronitis 20 589 Culture, societyB02 M. Ploritis 20 1,147 Culture, historyB03 K. Tsoukalas 20 1,516 International ffairsB04 C. Kiosse 20 1,741 ArchaeologyB05 S. Alachiotis 20 958 BiologyB06 G. Babiniotis 20 1,273 LinguisticsB07 T. Tasios 20 1,049 Technology, societyB08 G. Dertilis 20 916 History, societyB09 A. Liakos 20 1,291 History, societyB10 G. Vokos 20 1,002 Philosophycorpus was divided into a training part and a test part of equal size (i.e., 10 texts perauthor for training and 10 texts per author for test).7.2 Author IdentificationAs for the text genre detection experiment, the entire corpus was first analyzed auto-matically by the SCBD.
We then used the stylistic vectors of the training corpus to trainthe classification model for each group separately, based on multiple regression anddiscriminant analysis.
We cross-validated the acquired models by applying them tothe test corpus of the corresponding group.
The same procedure was followed basedon the VR, CWF-30, and CWF-50 approaches.
Comparative results in terms of theidentification error rate for groups A and B are given in Figures 5 and 6, respectively.As in the case of text genre detection, the VR method achieved far lower accuracyresults than the others.
The performance of the CWF-30 and CWF-50 is significantlybetter in group B than in group A.
In both groups, our approach achieved the bestperformance.The identification error rates of our approach using both multiple regression anddiscriminant analysis are presented in Table 6.
For group A, there are significant dif-ferences in the accuracy of the two techniques.
However, three authors (A01, A03,and A06) are responsible for approximately 50% of the average rror rate, probablybecause the average text length of these authors is relatively short, i.e., shorter than800 words (see Table 5).On the other hand, the two techniques give similar disambiguation results forgroup B.
A considerable percentage of the average rror rate is caused by the authorsB01, B05, and B08 (i.e., 65% for multiple regression, 55% for discriminant analysis).These authors also have a relatively short average text length, i.e., shorter than 1,000words.486Stamatatos, Fakotakis, and Kokkinakis Text Categorization[] Discriminant analysis ?
Multiple regressionOur approachVRCWF-50CWF-300.34: i l i , ,  I I ) ,28. .
.
.
.
.
.
.
L .
.
.
.
= .... .
.
.
.
.
.
.
l l J l l~$I i I I i i0 0.1 0.2 0.3 0.4 0.5 0.6 0.7Ident i f icat ion errorFigure 5Comparative r sults for authorship identification in group A.Our approachVRCWF-50CWF-30[] Discriminant analysis ?
Multiple regression0.310 .30 .3510.35i i i J0 0.1 0.2 0.3 0.4 0.5Ident i f icat ion errorFigure 6Comparative r sults for authorship identification i  group B.0.6Table 6The author identification results for both group A and group B.Identification ErrorCode Multiple Discriminant CodeRegression AnalysisIdentification ErrorMultiple DiscriminantRegression AnalysisA01 0.5 0.4 B01A02 0.3 0.2 B02A03 0.6 0.5 B03A04 0.2 0.1 B04A05 0.3 0.3 B05A06 0.7 0.5 B06A07 0.3 0.3 B07A08 0.1 0.1 B08A09 0.2 0.3 B09A10 0.2 0.1 B10Average 0.34 0.28 Average0.7 0.60.0 0.00.2 0.40.1 0.10.7 0.40.3 0.30.0 0.10.6 0.60.1 0.10.4 0.40.31 0.30487Computational Linguistics Volume 26, Number 4\[\] Correct ?
Error1201008060 ~D4020Figure 7o I ~ At?3 O hr~Text length (in words)Text length related to accuracy for the author identification experiments.It seems, therefore, that text length is a crucial factor in identifying the stylisticfeatures that characterize a certain author.
Classification accuracy for both groups us-ing multiple regression related to text length is presented in more detail in Figure 7.Approximately 80% (i.e., 53 out of 65) of the total erroneously classified texts areshorter than 1,000 words.
Moreover, the accuracy results for the two groups are com-parable.
In fact, the best results have been achieved under discriminant analysis forgroup A.
This fact verifies that the proposed set of style markers is capable of captur-ing the underlying stylistic features that characterize the author of a text even whendealing with texts taken from various text genres.
Note that CWF-30 and CWF-50failed to achieve comparable performance for groups A and B.7.3 Author VerificationInstead of trying to select he most likely author of a given text from among a givengroup of authors (i.e., the author identification problem), many applications requirethe confirmation (or rejection) of the hypothesis that a given person is the author of thetext (i.e., the author verification problem).
In such cases, the classification procedureis less complicated since there are only two possible answers: yes, i.e., the author inquestion is indeed the person who wrote the text, or no, i.e., the text was not writtenby this person.Implementing an automatic author verification system requires:The development of a response function for a given author.
For a giventext, this function must provide a response value based on the vector ofthe style markers of the text.The definition of a threshold value.
Any text whose response value isgreater than that of the threshold is accepted as written by the author inquestion.
Otherwise, it is rejected.488Stamatatos, Fakotakis, and Kokkhlakis Text Categorization- -  FR .
.
.
.
.
.
.
FA .
.
.
.
Mean10.90.80.70.60.50.40.30.20.10 - -  " .
.
.
.
.
.
.
.
.
.
.
~ " "  " i  .
.
.
.
.
?"
.
.
.
.
J P I I I i \[0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1xRFigure 8FR, FA, and mean error for group A related to threshold values expressed as subdivisions of R.Additionally, for measuring the accuracy of the author verification method fora given author, False Rejection (FR) and False Acceptance (FA) can be used.
Thesemeasures are commonly used in the area of speaker verification in speech processing(Fakotakis, Tsopanoglou, and Kokkinakis 1993) and are defined as follows:FR = rejected texts of the author~total texts of the authorFA = accepted texts of other authors~total texts of other authorsIn our study, we used the response functions taken from the application of multipleregression to group A and group B, as described in the previous ection.
The selectionof a threshold value, on the other hand, is highly dependent on the application.
Someapplications require either minimal FR or minimal FA, while others require minimalmean error, i.e., (FR + FA)/2.We chose to express the threshold value as a function of the multiple correlationcoefficient R = +v~ of the regression functions (see Section 5.1) since it measuresthe degree to which the regression function fits the training data.
It equals 1 if thefitted equation passes through all the data points and at the other extreme, equals 0,as already mentioned for R 2.
Figures 8 and 9 depict he variation of the average FR,FA, and the mean error values for the test corpus of group A and group B, respectively,using various subdivisions of R as threshold.
Notice that the evaluation shown usedtexts within the same group of authors for testing (i.e., closed-set evaluation).
Lowthreshold values correspond to minimal FR, while high threshold values correspondto minimal FA.
The minimal mean error corresponds tothreshold values between 0.4Rand 0.5R for both groups.
The FR and FA values for group A and group B using 0.5Ras threshold are given in Table 7.
The greatest part of the total FR in both groupsaccounts for the authors characterized by short text length (i.e., group A: A01, A03,and A06, group B: B01, B05, and B08) as in the case of author identification.
On theother hand, FA seems to be highly relevant o the threshold value.
The smaller thethreshold value, the greater the false acceptance.489Computational Linguistics Volume 26, Number 4- -  FR .
.
.
.
.
.
.
FA .
.
.
.
Mean10.90.80.70.60.50.40.30.20.10 ~ t ~ ~ , k .
.
.
.
?
.
.
.
.
.
?
.
.
.
.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1xRFigure 9FR, FA, and mean error for group B related to threshold values expressed as subdivisions of R.Table 7The author verification results for both groups (threshold=R/2).Code R/2 FR FA Code RI2 FR FAA01 0.33 0.5 0.033 B01 0.32 0.3 0.022A02 0.33 0.3 0.011 B02 0.42 0.0 0.044A03 0.36 0.6 0.044 B03 0.33 0.0 0.155A04 0.36 0.2 0.111 B04 0.33 0.1 0.089A05 0.35 0.3 0.067 B05 0.28 0.6 0.144A06 0.35 0.7 0.044 B06 0.36 0.2 0.011A07 0.34 0.2 0.044 B07 0.38 0.0 0.022A08 0.31 0.1 0.111 B08 0.30 0.6 0.100A09 0.35 0.2 0.055 B09 0.36 0.0 0.055A10 0.35 0.1 0.089 B10 0.40 0.4 0.033Average 0.35 0.32 0.061 Average 0.35 0.22 0.0688.
Performance Issues8.1 Training Set SizeOur study makes use of 10 training texts from each category (i.e., either a text genre oran author) in order to extract he appropriate coefficients.
This assessment meets thecriteria of a system that requires easy adaptation of the text categorization methodol-ogy to a certain domain.
Biber (1990, 1993a) claims that it is possible to represent thedistributions of many core linguistic features of a stylistic category based on relativelyfew texts from each category (as few as 10 texts), but we were interested in exploringthe way in which the identification error rate is affected by increasing the trainingdata.
To this end, we performed experiments on text genre detection using multipleregression based on variable training data.
Specifically, we varied the training corpus,including 7 to 15 texts for each genre, but used the same test corpus of 10 texts forall of the experiments.
The same procedure was followed for the lexically based ap-proaches VR, CWF-30, and CWF-50.
Comparat ive results of the average identificationerror rate related to the training set size are shown in Figure 10.490Stamatatos, Fakotakis, and Kokkinakis Text Categorization.
.
.
.
VR - - - CWF-30.
.
.
.
.
CWF-50 Our approacht _t _.20.60.50.40.30.20.106Q7 8 9 10 11 12 13 14 15 16Training set (in texts per genre)Figure 10The identification error rate of the text genre detection experiment related to the training setsize.The performance of VR is not significantly affected by increasing the training setsize.
On the other hand, the identification error rate of CWF-30, CWF-50, and that ofour approach is generally reduced by increasing the number of texts used for training.The performance of CWF-30 is more stable as compared to CWF-50 but is lower thanthat of our set of style markers.The best results are achieved by our approach using 14 training texts per genre(i.e., only 15 out of 100 texts misclassified).
However, the identification error rate doesnot continuously decrease from 11 to 15 training texts; the identification error rateusing 12 as well as 15 training texts for each category is greater than the rate attainedby using 10 texts.
Thus, it is clear that satisfactory accuracy can be achieved with only10 training texts.8.2 Significance of Style MarkersThe proposed set of style markers is divided into three levels--token level, syntax level,and analysis level.
It would be useful to calculate the contribution of each marker, andconsequently of each level, to the classification procedure.
To this end, we used theabsolute t values of the linear regression functions that indicate the contribution ofeach independent variable to the response value (see Section 5.1).The average absolute t values of the 22 style markers, taking into account heregression functions for both text genre and author identification experiments, arepresented in Table 8.
In both cases, the most important stylometric level is the tokenlevel, while the syntax level contributes the least to the final response.
On the otherhand, M02, M12, and M15 are the most important style markers for text genre detection(i.e., average t > 1.50) while the token-level measures, M01, M02, and M03, are themost valuable measures for authorship attribution (for the specific groups of authors).8.3 Defective Computational AnalysisThe set of style markers is provided by the SCBD, an existing computational tool.
Toexplore the degree to which the accuracy results are dependent on the accuracy of491Computational Linguistics Volume 26, Number 4Table 8Absolute t values (average) for the regression functions of both text genre detection andauthorship attribution.Absolute t Values (Average)Stylometric Style Marker Text Genre AuthorshipLevel Detection AttributionM01 1.06 1.80Token level M02 2.52 1.85M03 1.43 1.98Level average 1.67 1.88M04 0.57 0.76M05 0.58 0.77M06 0.56 0.77M07 0.57 0.75M08 0.57 0.76Syntax level M09 0.77 0.98M10 0.93 0.85Mll 0.59 0.90M12 1.72 1.07M13 0.67 0.97Level average 0.75 0.86M14 1.03 1.30M15 2.11 1.05M16 1.45 0.79M17 1.08 1.42Analysis level M18 0.72 1.06M19 1.14 0.84M20 1.00 0.86M21 0.81 0.90M22 0.65 0.84Level average 1.11 1.01the SCBD, we created an artificial defect in the output of the SCBD by corrupting thesentence and chunk boundary detection procedures.
In particular:in the sentence boundary detection procedure, only periods wereconsidered to denote a potential sentence boundary, andthe fifth parsing pass was excluded from the chunk boundary detectionprocedure.These changes significantly decreased the accuracy of the output of the SCBD.We performed the text genre experiment again using multiple regression based onthe defective data.
The average identification error rate was increased approximately25% (i.e., new identification error = 0.23).
As expected, the accuracy of the text cate-gorization methodology strongly depends on the accuracy of the SCBD.
Note that thecontribution of the stylometric levels to the final response has also changed.
Table 9shows average absolute t values for both the regular and the defective computationalanalysis.
Although the token-level measures are still the most important contributorsto the response, the disproportion between them and both the analysis-level and thesyntax-level measures has considerably decreased.492Stamatatos, Fakotakis, and Kokkinakis Text CategorizationTable 9Absolute values of t (average) of the stylometric levels for both regular and defective analysis.Absolute t (average) StylometricLevel Regular Analysis Defective AnalysisToken level 1.67 1.55Syntax level 0.75 0.97Analysis level 1.11 1.299.
ConclusionsIn this paper we presented an approach to text categorization i terms of stylisti-cally homogeneous categories, either text genres or authors.
The results of apply-ing this methodology to text genre detection and author identification and verifica-tion experiments are strongly encouraging; this methodology outperforms existinglexically based methods.
Since the stylistic differences are clearer among text gen-res, the results achieved in text genre detection are considerably better than thoseof the authorship attribution tasks.
However, in both cases, a limited number oftext genres or authors are responsible for the greatest part of the identification er-ror rate.As seen in Figures 4 and 7, text length plays an important role, especially inthe case of author identification.
A lower boundary of 1,000 words for each textseems reasonable for assuring improved performance.
Nevertheless, when dealingwith real-world text, it is not always possible to reach this lower bound.
The cor-pora used in all the experiments presented here consist of real-world texts down-loaded from the Internet without any manual text preprocessing or text samplinglimitations.
The majority of these texts have an average text length shorter than 1,000words.Our experiments have shown that our method can be applied to a randomlyselected group of stylistically homogeneous categories without any manual adapta-tion restrictions.
A training corpus consisting of 10 texts per category is adequate forachieving relatively high classification accuracy.
We attempted to take advantage ofexisting NLP tools by using analysis-level style markers that provide useful stylisticinformation without any additional cost.
In essence, such measures represent the wayin which the text has been analyzed by the computational tool.
We proved that thesemeasures are more important to the final response than measures related to the actualoutput of the tool on the syntactic level (see Table 8).Much work remains to be done on the stylistic interpretation f the acquired resultsand the automatic extraction of stylistic onclusions related to both the text itself and itsauthor.
Such stylistic onclusions could explain the differences and similarities amongvarious genres or authors on a formal basis.
Moreover, the definition of a basic textlength unit would open the way to the exploration of the variation of style within asingle text.
This procedure could assist in the detection of certain sections of the inputtext where the useful information is more likely to be found.
We believe that suchtasks can be performed using a set of style markers imilar to the one we proposed.Finally, the combination of our approach with lexically based methods, such as CWF-30, can result in a very reliable text categorization system in terms of stylisticallyhomogeneous categories.493Computational Linguistics Volume 26, Number 4AcknowledgmentWe would like to thank the anonymous CLreviewers for their valuable and insightfulcomments.
Their suggestions have greatlyimproved an earlier draft of this paper.ReferencesBaayen, Harald, Hans Van Halteren, andFiona Tweedie.
1996.
Outside the cave ofshadows: Using syntactic annotation toenhance authorship attribution.
Literaryand Linguistic Computing, 11(3):121-131.Biber, Douglas.
1990.
Methodological issuesregarding corpus-based analyses oflinguistic variations.
Literary and LinguisticComputing, 5:257-269.Biber, Douglas.
1993a.
Representativeness incorpus design.
Literary and LinguisticComputing, 8:1-15.Biber, Douglas.
1993b.
Usingregister-diversified corpora for generallanguage studies.
ComputationalLinguistics, 19(2):219-242.Biber, Douglas.
1995.
Dimensions of RegisterVariation: A Cross-Linguistic Comparison.Cambridge University Press.Brainerd, Barron.
1974.
Weighting Evidence inLanguage and Literature: A StatisticalApproach.
University of Toronto Press.Brinegar, Claude S. 1963.
Mark Twain andthe Quintus Curtius Snodgrass letters: Astatistical test of authorship.
Journal of theAmerican Statistical Association, 58:85-96.Brunet, Ettienne.
1978.
Vocabulaire de JeanGiraudoux: Structure t Evolution.
Slatkine.Burrows, John F. 1987.
Word-patterns andstory-shapes: The statistical analysis ofnarrative style.
Literary and LinguisticComputing, 2(2):61-70.Burrows, John F. 1992.
Not unless you asknicely: The interpretative n xus betweenanalysis and information.
Literary andLinguistic Computing, 7(2):91-109.Edwards, Allen F. 1979.
Multiple Regressionand the Analysis of Variance and Covariance.W.
H. Freeman, San Francisco, CA.Eisenbeis, Robert A., and Robert B. Avery.1972.
Discriminant Analysis andClassification Procedures: Theory andApplications.
D.C. Health and Co.,Lexington, MA.Fakotakis, Nikos, Anastasios Tsopanoglou,and George Kokkinakis.
1993.
Atext-independent speaker ecognitionsystem based on vowel spotting.
SpeechCommunication, 12:57-68.Holmes, David I.
1992.
A stylometricanalysis of Mormon scripture and relatedtexts.
Journal of the Royal Statistical Society,Series A, 155(1):91-120.Holmes, David I.
1994.
Authorshipattribution.
Computers and the Humanities,28:87-106.Holmes, David I., and Richard S. Forsyth.1995.
The Federalist revisited: Newdirections in authorship attribution.Literary and Linguistic Computing,10(2):111-127.HonorG Antony.
1979.
Some simplemeasures of richness of vocabulary.Association for Literary and LinguisticComputing Bulletin, 7(2):172-177.Karlgren, Jussi.
1999.
Stylistic experimentsin information retrieval.
In T.Strzalkowski, editor, Natural LanguageInformation Retrieval.
Kluwer AcademicPublishers, pages 147-166.Karlgren, Jussi and Douglass Cutting.
1994.Recognizing text genres with simplemetrics using discriminant analysis.
InProceedings ofthe 15th InternationalConference on Computational Linguistics(COLING '94), pages 1,071-1,075.Kessler, Brett, Geoffrey Nunberg, andHinrich Schiitze.
1997.
Automaticdetection of text genre.
In Proceedings of35th Annual Meeting, pages 32-38.Association for ComputationalLinguistics.Michos, Stefanos, Efstathios Stamatatos,Nikos Fakotakis, and George Kokkinakis.1996.
An empirical text categorizingcomputational model based on stylisticaspects.
In Proceedings ofthe 8th Conferenceon Tools with Artificial Intelligence(ICTAI'96), pages 71-77.Morton, Andrew Q.
1965.
The authorship ofGreek prose.
Journal of the Royal StatisticalSociety, Series A, 128:169-233.Mosteller, Fredrick and David Wallace.
1984.Applied Bayesian and Classical Inference: TheCase of the Federalist Papers.Addison-Wesley, Reading, MA.Oakman, Robert L. 1980.
Computer Methodsfor Literary Research.
University of SouthCarolina Press, Columbia.Sichel, Herbert S. 1975.
On a distributionlaw for word frequencies.
Journal of theAmerican Statistical Association, 70:542-547.Simpson, Edward H. 1949.
Measurement ofdiversity.
Nature, 163:688.Smith, M. W. A.
1983.
Recent experienceand new developments of methods forthe determination of authorship.Association for Literary and LinguisticComputing Bulletin, 11:73-82.Smith, M. W. A.
1985.
An investigation of494Stamatatos, Fakotakis, and Kokkinakis Text CategorizationMorton's method to distinguishElizabethan playwrights.
Computers andthe Humanities, 19(1):3-21.Stamatatos, Efstathios, Nikos Fakotakis, andGeorge Kokkinakis.
1999.
Automaticextraction of rules for sentence boundarydisambiguation.
In Proceedings oftheWorkshop in Machine Learning in HumanLanguage Technology, Advance Course onArti~'cial Intelligence (ACAI'99),pages 88-92.Stamatatos, Efstathios, Nikos Fakotakis, andGeorge Kokkinakis.
2000.
A practicalchunker for unrestricted text.
InProceedings ofthe 2nd InternationalConference on Natural Language Processing,pages 139-150.Strzalkowski, Tomek.
1994.
Robust textprocessing in automated informationretrieval.
In Proceedings ofthe 4th ConferenceOn Applied Natural Language Processing(ANLP'94), pages 168-173.Tabachnick, Barbara G. and Linda S. Fidell.1996.
Using Multivariate Statistics.
Thirdedition.
HarperCollins College Publishers.Tweedie, Fiona and Harald Baayen.
1998.How variable may a constant be?Measures of lexical richness inperspective.
Computers and the Humanities,32(5):323-352.Tweedie, Fiona, Sameer Singh, and David I.Holmes.
1996.
Neural networkapplications in stylometry: The FederalistPapers.
Computers and the Humanities,30(1):1-10.Yang, Yiming.
1999.
An evaluation ofstatistical approaches to textcategorization.
I formation RetrievalJournal, 1(1):69-90.Yang, Yiming and Xin Liu.
1999.
Are-examination f text categorizationmethods.
In Proceedings ofthe ACM SIGIRConference on Research and Development inInformation Retrieval (SIGIR'99),pages 42--49.Yule, George U.
1944.
The Statistical Study ofLiterary Vocabulary.
Cambridge UniversityPress.495
