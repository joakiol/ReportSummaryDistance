Dimensionality Reduction with Multilingual ResourceYingJu Xia                      Hao Yu                        Gang ZouFujitsu Research & Development Center Co.,LTD.13F Tower A, Ocean International Center, No.56 Dong Si Huan Zhong Rd, Chaoyang District,Beijing, China, 100025{yjxia,yu,zougang}@cn.fujitsu.comAbstractQuery and document representation is akey problem for information retrieval andfiltering.
The vector space model (VSM)has been widely used in this domain.
Butthe VSM suffers from high dimensionality.The vectors built from documents alwayshave high dimensionality and contain toomuch noise.
In this paper, we present anovel method that reduces the dimensional-ity using multilingual resource.
We intro-duce a new metric called TC to measure theterm consistency constraints.
We deduce aTC matrix from the multilingual corpus andthen use this matrix together with the term-by-document matrix to do the Latent Se-mantic Indexing (LSI).
By adopting differ-ent TC threshold, we can truncate the TCmatrix into small size and thus lower thecomputational cost of LSI.
The experimen-tal results show that this dimensionality re-duction method improves the retrieval per-formance significantly.1 Introduction1.1 Basic conceptsThe vast amount of electronic information that isavailable today requires effective techniques foraccessing relevant information from it.
The meth-odologies developed in information retrieval aim atdevising effective means to extract relevant docu-ments in a collection when a user query is given.
Ininformation retrieval and filtering, Query anddocument representation is a key problem andmany techniques have been developed.
Amongthese techniques, the vector space model (VSM)proposed by Salton (1971; 1983) has been widelyused.
In the VSM, a document is represented by avector of terms.
The cosine of the angle betweentwo document vectors indicates the similarity be-tween the corresponding documents.
A smallerangle corresponds to a larger cosine value and in-dicates higher document similarity.
A query, whichdescribes the information need, is encoded as avector as well.
Retrieval of documents that satisfythe information need is achieved by finding thedocuments most similar to the query, or equiva-lently, the document vectors closest to the queryvector.
There are several advantages to this ap-proach beyond its mathematical simplicity.
Aboveall, it is efficient to compute and store the wordcounts.
This is one reason that why VSM is widelyused for query and document representation.
Butthis method has problem that the vectors built fromdocuments always have high dimensionality andcontain too much noise.
The high dimensionalitycauses high computational and memory require-ments while noise in the vectors degrades the sys-tem performance.1.2 Related worksTo address these problems, many dimensionalityreduction techniques have been applied to queryand document representation.
Among these tech-niques, Latent Semantic Indexing (LSI) (Deer-wester et al, 1990; Hofmann, 1999; Ding, 2000;Jiang and Littman, 2000; Ando, 2001; Kokiopou-lou and Saad, 2004; Lee et al, 2006) is a well-known approach.
LSI constructs a smaller docu-ment matrix that retains only the most importantinformation from the original by using the SingularValue Decomposition (SVD).
Many modificationshave been made to this approach (Hofmann, 1999;Ding, 2000; Jiang and Littman, 2000; Kokiopoulou613and Saad, 2004; Sun et al, 2004; Husbands et al,2005).
Among them, IRR (Ando and Lee, 2001) isa subspace-projection method that counteracts ten-dency to ignore minority-class documents.
This isdone by repeatedly rescaling vectors to amplify thepresence of documents poorly represented in pre-vious iterations.In concept indexing (CI) (Karypis and Han,2000) method, the original set of documents is firstclustered into k similar groups, and then for eachgroup, the centroid vector (i.e., the vector obtainedby averaging the documents in the group) is usedas one of the k axes of the lower dimensional space.The key motivation behind this dimensionality re-duction approach is the view that each centroidvector represents a concept present in the collec-tion, and the lower dimensional representation ex-presses each document as a function of these con-cepts.
George and Han (2000) extend concept in-dexing in the context of supervised dimensionalityreduction.
To capture the concept, phrase also hasbeen used as indexing entries (Mao and Chu, 2002).The LPI method (Isbell and Viola, 1999) tries todiscover the local structure and obtains a compactdocument representation subspace that best detectsthe essential semantic structure.
The LPI uses Lo-cality Preserving Projections (LPP) (Xiaofei Heand Partha, 2003) to learn a semantic space fordocument representation.
Xiaofei He et al, (2004)try to get sets of highly-related words, queries anddocuments are represented by their distance tothese sets.
These algorithms have successfully re-duced the dimensionality and improve the retrievalperformance but at the mean time they led to ahigh computational complexity.1.3 Our methodIn this study, we propose a novel method that re-duces the dimensionality using multilingual re-source.
We first introduce a new metric called TCto measure the term consistency constraints.
Weuse this metric to deduce a TC matrix from themultilingual corpus.
Then we combine this matrixto the term-by-document matrix and do the LatentSemantic Indexing.
By adopting different TCthreshold, we can truncate the TC matrix into smallsize and thus lower the computational cost of LSI.The remainder of this paper is organized as fol-lows.
Section 2 describes the dimensionality reduc-tion method using multilingual resource.
Section 3shows the experimental results to evaluate the di-mensionality reduction method.
Finally, we pro-vide conclusions and remarks of future work inSection 4.2 Dimensionality reduction using multi-lingual resource2.1 MotivationAs mentioned above, the queries and documentsare represented by vectors of terms.
The weight ofeach term indicates its contribution to the vectors.Many weighting schemes have been proposed.
Thesimplest form is to use the term-frequency (TF) asthe term weight.
In this condition, a document canbe represented as a vector ),...,,( 21 ntftftfd =r, whereis the frequency of the ith term in the document.A widely used refinement to this model is toweight each term based on its inverse documentfrequency (IDF) in the documents collection.
Thisis commonly done by multiplying the frequency ofeach term i by , where N is the totalnumber of documents in the collection, and isthe number of documents that contain the ith term.This leads to the TF-IDF representation of thedocuments.
Although the TF-IDF weightingscheme has many variants (Buckley, 1985; Berryet al, 1999; Robertson et al, 1999), the idea is thesame one that uses the statistical information suchas TF and IDF to calculate the term weight ofvectors.itf)/log( idfNidfThis kind of statistical information is independ-ence with languages.
For example, in one language,say La, we have a vocabulary Va = {w1a, w2a, ?,wna} and a documents collection Da = {d1a, d2a,?,dma }.
If this documents collection has a parallelcorpus in language Lb, say, Db = {d1b, d2b,?, dmb }and a vocabulary Vb = {w1b, w2b, ?, wnb}.
Whenwe put a query Qka = {qk1a, qk2a ,?, qkla } (qkia ?Va)into an information retrieval system.
The informa-tion retrieval system will converts the query Qkaand the documents in the collection Da into vectors.By calculating the similarity between query Qkaand each document dia, the system selects thedocuments whose similarity is higher than athreshold as the results Rka.
If we translate thequery Qka into language Lb and get query Qkb, whenputting the Qkb into the same information retrievalsystem, we get the retrieval results Rkb.
Since theQka and Qkb contain the same content and only ex-pressed in different languages.
We expect that Rka614and Rkb will contain the same content.
If this as-sumption holds, the vocabulary which is used tobuild queries and documents vectors should havehigh representative ability.
Since the weight ofeach term in the vector is calculated by the statisti-cal information such as TF and IDF.
If the vocabu-lary Va and Vb have high representative ability,their statistical information will be consistent aswell.
This is the main motivation of our dimen-sionality reduction method.2.2 Dimensionality reduction methodThe most straightforward way to measure theword?s representability in multilingual resource isto calculate the TF and IDF of each word in differ-ent languages.
But this method has one problemthat the TF-IDF scheme is dedicated for each sin-gle document, the same word will have differentweight in different documents.
It is impractical toimpose the consistency constraint to every docu-ment.
Even we can do that, this method still has thedrawback that it is very difficult to port to anotherdocuments collection.
To address this problem, weconsider the whole documents collection as onesingle document.
In this condition, the IDF will bea fixed number.We introduce a new metric to measure the termconsistency called TC.
Figure 1 and Figure 2 illus-trate the basic idea.
In these figures, the curve Lashows the word logarithmic frequency in thedocuments collection of language La, the curve Lbshows the corresponding translation?s logarithmicfrequency in the documents collection of languageLb.
TCi and TCj are the term consistency of wi andwj respectively.Figure 1 shows the TC in normal condition thatthe average word frequency in language a is proxi-mate to that of language b.
In this case, the TC isdefined as below:))log(/)log(),log(/)min(log()( aibibiaibi ffffwTC =       (1)Here fia  is the frequency of wia in language a. fibis the frequency of the wia?s translation in languageb.
In multilingual case, the TC(wi) will be definedas below:))(...),(min()( nibii wTCwTCwTC =   (In the case that2)the average word frequency inlanto calculate the TC of wi as below:(3)Here H is distance between the moving averageguage a is different with that of language b, wewill first calculate the moving average as shown inthe Figure 2.
After that, we use the moving average)))/(log()log(),log(/))min((log()( HfffHfwTC aibibiaibi ++=and the original one.wordsFrequencyLanguage aLanguage bw i w jTC iTC jFigure 1.
TC in normal conditionwordsFrequencyLanguage aLanguage bMoving averagew i w jTC iTC jFigure 2.
TC in shift conditionOnce we get the guage a,we present i diag(TC ,?do =B):TC of every word in lant in a diagonal matrix T =tt?
1TC2, ?
, TCt), TC1 ?
TC2 ?
?
?
TCt.When applying the TC matrix tT in informa-tion retrieval, we combine T  into the tttt?
erm-by-cument matrix dtA ?
.
Where dtA ?
[aij] and theaij is the weight of term i in ument j.
We get anew matrix dtttdt ATB ???
= .
Then following theclassical LSI dt?
by a low-rank ap-proximation derived from its truncated SingularValue Decomposition (SVDTndnnntdt VUB ????
?=Here IUUT = , IVV T = ,doc, we replace),...,,( 21 ndiag ??
?=?== 0...... 121?
=???
+ nr?
r??
?
?
.ain pro m of LSI is that it usually led toa high computational complexity sinc he matrixmatriThe m blee tdt?
usually in 10B 3-105 dimensional space.
Tolower the computational cost, we truncate the TCx ttT ?
according to different TC thresholdand get a new matrix ),...,,(?
21 ttt TCTCTCdiagT =?
,0......21 1 ===????
r TCTCTCTCTC .
Then?
?+ trwe get AT ?
?= .
Since r is small than t, the drrrdrB ?615computational cost on the matrix wil.
Note thatb).
To  this one-to-many phe-nostem to evaluateeduction method presented inSection 2.
The term weight in the term-by-docomes from Chinese Linguisticeseldc.org/drB ??
l lowerthan tB ?
the matrix drB ??
is deducedfrom the TC matrix ttT ?
which is sorted by wordrepresentative ability.
It will contain less noise andoutperform the original matrix dtA ?
.
The experi-mental results have shown the effective of thismethod.For one word wdia in languag , there are al-ways several translations in language Le Lab, say (wi1b,wi2b,?, wik  handlemenon, we calculate the co-occurrence of wiaand each translation and select the highest one asthe translation of wia.3 ExperimentsWe adopt a VSM based IR sythe dimensionality rcument matrix is calculated by the TF-IDFweighting scheme.3.1 Training and test corporaThe training corpusData Consortium (http://www.chin , ab-?2004-863-(?2003-863-006?).
It isa Cbreviate as CLDC).
Its code number is009?.
This parallel corpus contains parallel texts inChinese, English and Japanese.
It is aligned to sen-tence level.
The sentence alignment is manuallyverified and the sampling examination shows theaccuracy reaches 99.9%.The experiments are conducted on two test cor-pora.
The first one is the information retrieval testcorpus gotten from CLDChinese IR corpus and contains 20 topics for test.Each topic has key words and description and nar-rative.
The second one is the Reuters 2001 data(http://about.reuters.com/researchandstandards/corpus/ ).
This corpus is a collection of about 810,000Reuters English news stories from August 20, 1996to August 19, 1997.
It was used by the TREC-10Filtering Tracks (Robertson and Soboroff, 2002).In TREC-10, 84 Reuters categories were used tosimulate user profiles.The evaluate measure is a version of vanRijsbergen(1979)?s F measure with ?=1(we de-note it as F1).3.2 Experimental resultsThe table1 and table2 show the experimental re-sults conducted on Chinese and English test Cor-pus respectively.
In these tables, we compare ourmethod with basic LSI and LPI (Xiaofei et.al,2004).
In the table1, the ?C-E?
means the TC ma-trix gotten from Chinese-English training collec-tion (deduced from the trilingual training corpus).The ?C-J?
means that the TC matrix gotten fromChinese-Japanese training collection, and so forcethe ?C-E-J?.
All the TC matrices have been normal-ized to range from 0 to 1.
The threshold ?
is usedto truncate the TC matrix into small size.
Bigger ?corresponds to smaller truncated TC matrix.
Notethat here ?
is discrete since for some ?, the size oftruncated matrix is very similar.
For example,when ?
= 0.85 and ?
= 0.9, the size of truncated TCmatrices are the same one.LSI: 0.3785,  LPI: 0.405?
C-E C-J C-E-J0.3 0.404 0.4014 0.41240.4 0.4098 0.406 0.41850.45 0.4159 0.4185 0.42260.5 0.4204 0.4124 0.41050.55 0.4061 0.4027 0.39970.6 0.3913 0.3992 0.3960.8 0.3856 0.3867 0.38420.85 0.3744 0.3754 0.3768Table1.F1 measure of Chinese test corpusLSI: 0.3416,  LPI: 0.3556?
E-C E-J E-C-J0.3 0.356 0.3478 0.35780.4 0.3578 0.3596 0.37020.45 0.3698 0.3651 0.37340.5 0.3636 0.3575 0.3630.55 0.3523 0.3564 0.34770.6 0.3422 0.3448 0.34580.8 0.3406 0.3397 0.33780.85 0.3304 0.3261 0.3278Table2.
F1 measure of English test corpusFrom the experimental results, we can see thatour method make great enhancement to the basicLSI method.
And our method also outperforms theLPI method in both test corpora.
Comparing theperformance on different training collection, wecan find that the difference is subtle.
In Chinesetest corpus, the TC matrix gotten from C-E-J train-ing collection get the best performance (F1=0.4226)at ?=0.45 while the C-E test collection get 0.4204616at ?=0.5 and the C-J test collection get 0.4185 at?=0.45.
For the English test corpus, the trilingualtraining collection also gets the best performance.But the difference between bilingual and trilingualtraining collection is also subtle (E-C-J: F1=0.3734,E-C: F1=0.3698, E-J: F1=0.3651).
In the Englishtest corpus, all the training collection get the bestperformance at ?=0.45.As mentioned before, the bigger ?
means thesmaller size of the truncated TC matrix.
Whilesmall size of the truncated TC matrix means lowcomputational cost and high system speed.
This isone of the advantages of our method over the tradi-tional LSI method.
We conducted some experi-ments to test the system speed on different thresh-old ?.
We use the number of documents per sec-ond (docs/s) to denote this kind of system speed.The experiment is conducted on the personal com-puter with a Pentium (R) 4 processor @2.8GHz,256 KB cache and 512 MB memory.
Table 3shows the experimental results that the ?
vs. sys-tem speed and Figure 3 illustrates the F1 measurevs.
the system speed.Baseline(LSI): 566.5 docs/s?
C-E C-J C-E-J0.3 1039.3 1034.4 1355.00.4 1148.4 1188.9 1372.50.45 1290.5 1246.9 1391.30.5 1323.9 1323.3 1469.60.55 1393.3 1392.6 1563.80.6 1413.3 1508.8 1590.10.8 1513.1 1555.6 1660.50.85 1641.1 1778.2 1773.5Table 3. ?
vs. system speed0.370.380.390.40.410.420.431000 1200 1400 1600 1800docs/sFMeasureC-EC-JC-E-JFigure 3.
F1 measure vs. system speed4 ConclusionsIn this paper, we present a novel method that re-duces the dimensionality using multilingual re-source.
We deduce a TC matrix from the multilin-gual corpus and then truncate it to small size ac-cording to different TC threshold.
Then we use thetruncated matrix together with the term-by-document matrix to do the LSI analysis.
Since thetruncated TC matrix is sorted by word representa-tive ability.
It will contain less noise than the origi-nal term-by-document matrix.
The experimentalresults have shown the effectiveness of this method.In the future, we will try to find the optimaltruncate threshold ?
automatically.
And since itis more difficult to get the parallel corpora thancomparable corpora, we will explore using com-parable corpora to do the dimensionality reduc-tion.AcknowledgementThis research was carried out through financialsupport provided under the NEDO InternationalJoint Research Grant Program (NEDO Grant).ReferencesAndo R. K., ?Latent Semantic Space: Iterative Scalingimproves precision of inter-document similaritymeasurement?, in Proc.
of the 23th InternationalACM SIGIR, Athens, Greece, 2000.Ando R. K., and Lee L., ?Iterative Residual Rescaling:An Analysis and Generalization of LSI?, in Proc.
ofthe 24th International ACM SIGIR, New Orleans,LA, 2001.Arampatzis A., Beney J., Koster C.H.A., and T.P.
vander Weide.
KUN on the TREC9 Filtering Track: In-crementality, decay, and theshold optimization foradaptive filtering systems.
The ninth Text RetrievalConference, November 9-12, 2000 Gaithersburg, MD,Avi Arampatzis and Andre van Hameren The Score-Distributional Threshold Optimization for AdaptiveBinary Classification Tasks , SIGIR?01, September9-12,2001, New Orleans, Louisiana,USA.
285-293Berry M., Drmac Z., and Jessup E.. Matrices, vectorspaces, and information retrieval.
SIAM Review,41(2):pp335-362, 1999.Bingham E. and Mannila H., ?Random Projection indimensionality reduction: applications to image andtext data?, Proc.
Of the seventh ACM SIGKDD In-ternational Conference on Knowledge Discovery andData Mining, p. 245-250,2001.Buckley C..
Implementation of the SMART informationretrieval system.
Technical Report TR85-686, De-partment of Computer Science, Cornell University,617Ithaca, NY 14853, May 1985.
Source code availableat ftp://ftp.cs.cornell.edu/pub/smart.C.H.
Lee, H.C. Yang, and S.M.
Ma, ?A Novel Multi-Language Text Categorization System Using LatentSemantic Indexing?, The First International Confer-ence on Innovative Computing, Information andControl (ICICIC-06), Beijing, China, 2006.C.J.
van Rijsbergen.
Information Retrieval, chapter 7.Butterworths, 2 edition, 1979.Deerwester S. C., Dumais S. T., Landauer T. K., FurnasG.
W., and harshman R. A., ?Indexing by Latent Se-mantic Analysis?, Journal of the American Society ofInformation Science, 41(6):391-407, 1990.Ding C. H.. A probabilistic model for dimensionalityreduction in information retrieval and filtering.
InProc.
of 1st SIAM Computational Information Re-trieval Workshop, October 2000.George Karypis, Eui-Hong (Sam) Han, Fast supervise ddimensionality reduction algorithm with applicationsto document categorization & retrieval ,Proceedingsof the ninth international conference on Informationand knowledge management, November 2000Hofmann T., ?Probabilistic Latent Semantic Indexing?,in Proc.
of the 22th International ACM SIGIR,Berkeley, California, 1999.Husbands, P., Simon, H., and Ding, C. Term norm dis-tribution and its effects on latent semantic indexing,Information Processing and Management: an Interna-tional Journal, v.41 n.4, p.777-787, July 2005Isbell C. L. and Viola P., ?Restructuring Sparse HighDimensional Data for Effective Retrieval?, Advancesin Neural Information Systems, 1999.Jiang F. and Littman M.L., Approximate dimensionequalization in vector-based information retrieval.Proc.
17th Int'l Conf.
Machine Learning, 2000.Karypis G. and Han E.H.. Concept indexing: A fast di-mensionality reduction algorithm with applications todocument retrieval & categorization.
Technical Re-port TR-00-016, Department of Computer Science,University of USAKokiopoulou E., Saad Y., Polynomial filtering in latentsemantic indexing for information re-trieval ,Proceedings of the 27th annual internationalACM SIGIR conference on Research and develop-ment in information retrieval SIGIR '04,July 2004Mao W. and Chu W.W.. Free-text medical documentretrieval via phrase-based vector space model.
InProceedings of AMIA Annual Symp 2002.Minnesota, Minneapolis, 2000.
Available on the WWWat URL http://www.cs.umn.edu/~karypis.Robertson SE, Walker S, Beaulieu M,Okapi at TREC-7:automatic ad hoc, filtering, VLC and interactivetrack- Proceedings of the seventh Text RetrievalConference, TREC-7, pp.
253-264 ,1999Robertson, S., & Soboroff, I., The TREC-10 Filteringtrack final report.
Proceeding of the Tenth Text RE-trieval Conference (TREC-10) pp.
26-37.
NationalInstitute of Standards and Technology, special publi-cation 500-250., 2002Salton, G, the SMART Retrieval System ?
Experimentsin Automatic Document Processing.
Prentice-Hall,Englewood.
Cliffs, New Jersey,1971.Salton, G., Dynamic Information and Library process-ing.
Prentice-Hall, Englewood Cliffs, New Jer-sey,1983.Salton, G and McGill.
M.J., Introduction to ModernInformation retrieval.
McGraw Hill, New York,1983.Sun, J.T.
, Chen , Z. , Zeng , H.J.
, Lu, Y.C.
, Shi, C.Y.and Ma, W.Y.
,?Supervised Latent Semantic Index-ing for Document Categorization?
, In Proceedings ofthe Fourth IEEE International Conference on DataMining 2004Xiaofei He and Partha Niyogi, ?Locality PreservingProjections?,in Advances in Neural InformationProcessing Systems 16, Vancouver, Canada, 2003.Xiaofei He, Deng Cai, Haifeng Liu, Wei-Ying Ma, Lo-cality preserving indexing for document representa-tion, Proceedings of the 27th annual internationalACM SIGIR conference on Research and develop-ment in information retrieval SIGIR '04, July 2004Zhai C., Jansen P., Roma N., Stoica E., and Evans D.A..Optimization in CLARIT adaptive filtering.
In pro-ceeding of the Eight Text Retrieval Conference 1999,253-258.Zhang Y., and Callan J..
Maximum likelihood Estima-tion for Filtering Thresholds.
SIGIR?01, September9-12,2001, New Orleans, Louisiana,USA.
294-302618
