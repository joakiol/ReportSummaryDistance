Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 528?538,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsGlossBoot: Bootstrapping Multilingual Domain Glossaries from the WebFlavio De Benedictis, Stefano Faralli and Roberto NavigliDipartimento di InformaticaSapienza Universita` di Romaflavio.debene@gmail.com,{faralli,navigli}@di.uniroma1.itAbstractWe present GlossBoot, an effectiveminimally-supervised approach to ac-quiring wide-coverage domain glossariesfor many languages.
For each languageof interest, given a small number ofhypernymy relation seeds concerning atarget domain, we bootstrap a glossaryfrom the Web for that domain by means ofiteratively acquired term/gloss extractionpatterns.
Our experiments show highperformance in the acquisition of domainterminologies and glossaries for threedifferent languages.1 IntroductionMuch textual content, such as that available onthe Web, contains a great deal of information fo-cused on specific areas of knowledge.
However,it is not infrequent that, when reading a domain-specific text, we humans do not know the mean-ing of one or more terms.
To help the humanunderstanding of specialized texts, repositories oftextual definitions for technical terms, called glos-saries, are compiled as reference resources withineach domain of interest.
Interestingly, electronicglossaries have been shown to be key resourcesnot only for humans, but also in Natural LanguageProcessing (NLP) tasks such as Question Answer-ing (Cui et al, 2007), Word Sense Disambiguation(Duan and Yates, 2010; Faralli and Navigli, 2012)and ontology learning (Navigli et al, 2011; Ve-lardi et al, 2013).Today large numbers of glossaries are availableon the Web.
However most such glossaries aresmall-scale, being made up of just some hundredsof definitions.
Consequently, individual glossariestypically provide a partial view of a given domain.Moreover, there is no easy way of retrieving thesubset of Web glossaries which appertains to a do-main of interest.
Although online services suchas Google Define allow the user to retrieve defi-nitions for an input term, such definitions are ex-tracted from Web glossaries and put together forthe given term regardless of their domain.
As a re-sult, gathering a large-scale, full-fledged domainglossary is not a speedy operation.Collaborative efforts are currently producinglarge-scale encyclopedias, such as Wikipedia,which are proving very useful in NLP (Hovy et al,2013).
Interestingly, wikipedias also include man-ually compiled glossaries.
However, such glos-saries still suffer from the same above-mentionedproblems, i.e., being incomplete or over-specific,1and hard to customize according to a user?s needs.To automatically obtain large domain glos-saries, over recent years computational ap-proaches have been developed which extract tex-tual definitions from corpora (Navigli and Velardi,2010; Reiplinger et al, 2012) or the Web (Fujiiand Ishikawa, 2000).
The former methods startfrom a given set of terms (possibly automaticallyextracted from a domain corpus) and then har-vest textual definitions for these terms from theinput corpus using a supervised system.
Web-based methods, instead, extract text snippets fromWeb pages which match pre-defined lexical pat-terns, such as ?X is a Y?, along the lines of Hearst(1992).
These approaches typically perform withhigh precision and low recall, because they fallshort of detecting the high variability of the syn-tactic structure of textual definitions.
To addressthe low-recall issue, recurring cue terms occurringwithin dictionary and encyclopedic resources canbe automatically extracted and incorporated intolexical patterns (Saggion, 2004).
However, thisapproach is term-specific and does not scale to ar-bitrary terminologies and domains.In this paper we propose GlossBoot, a novelapproach which reduces human intervention to abare minimum and exploits the Web to learn a1http://en.wikipedia.org/wiki/Portal:Contents/Glossaries528Pattern and glossary extractionInitial seedselection  Gloss ranking and filteringSeedqueries Seedselectioninitialseedsnewseedssearch results domainglossary Gkfinalglossary 1 2 3 4 5Figure 1: The GlossBoot bootstrapping process for glossary learning.full-fledged domain glossary.
Given a domain anda language of interest, we bootstrap the glossarylearning process with just a few hypernymy rela-tions (such as computer is-a device), with the onlycondition that the (term, hypernym) pairs must bespecific enough to implicitly identify the domainin the target language.
Hence we drop the require-ment of a large domain corpus, and also avoid theuse of training data or a manually defined set oflexical patterns.
To the best of our knowledge, thisis the first approach which jointly acquires largeamounts of terms and glosses from the Web withminimal supervision for any target domain andlanguage.2 GlossBootOur objective is to harvest a domain glossary Gcontaining pairs of terms/glosses in a given lan-guage.
To this end, we automatically populate aset of HTML patterns P which we use to extractdefinitions from Web glossaries.
Initially, bothP := ?
and G := ?.
We incrementally populatethe two sets by means of an initial seed selectionstep and four iterative steps (cf.
Figure 1):Step 1.
Initial seed selection: first, we manu-ally select a set of K hypernymy relation seedsS = {(t1, h1), .
.
.
, (tK , hK)}, where the pair (ti,hi) contains a term ti and its generalization hi(e.g., (firewall, security system)).
This is the onlyhuman input to the entire glossary learning pro-cess.
The selection of the input seeds plays a keyrole in the bootstrapping process, in that the pat-tern and gloss extraction process will be driven bythese seeds.
The chosen hypernymy relations thushave to be as topical and representative as pos-sible for the domain of interest (e.g., (compiler,computer program) is an appropriate pair for com-puter science, while (byte, unit of measurement)is not, as it might cause the extraction of severalglossaries of units and measures).We now set the iteration counter k to 1 and startthe first iteration of the glossary bootstrapping pro-cess (steps 2-5).
After each iteration k, we keeptrack of the set of glosses Gk, acquired during it-eration k.Step 2.
Seed queries: for each seed pair (ti, hi),we submit the following query to a Web searchengine: ?ti?
?hi?
glossaryKeyword2 (whereglossaryKeyword is the term in the target lan-guage referring to glossary (i.e., glossary for En-glish, glossaire for French etc.))
and collect thetop-ranking results for each query.3 Each result-ing page is a candidate glossary for the domainimplicitly identified by our relation seeds S.Step 3.
Pattern and glossary extraction: weinitialize the glossary for iteration k as follows:Gk := ?.
Next, from each resulting page, we har-vest all the text snippets s starting with ti and end-ing with hi (e.g., ?firewall</b> ?
a <i>securitysystem?
where ti = firewall and hi = security sys-tem), i.e., s = ti .
.
.
hi.
For each such text snippets, we perform the following substeps:(a) extraction of the term/gloss separator: westart from ti and move right until we extractthe longest sequence pM of HTML tags andnon-alphanumeric characters, which we call theterm/gloss separator, between ti and the glossarydefinition (e.g., ?</b> -?
between ?firewall?
and?a?
in the above example).
(b) gloss extraction: we expand the snippet sto the right of hi in search of the entire glossof ti, i.e., until we reach a block element (e.g.,<span>, <p>, <div>), while ignoring format-ting elements such as <b>, <i> and <a> whichare typically included within a definition sen-tence.
As a result, we obtain the sequenceti pM glosss(ti) pR, where glosss(ti) is our glossfor seed term ti in snippet s (which includes hi byconstruction) and pR is the HTML block element2In what follows we use the typewriter font forkeywords and term/gloss separators.3We use the Google Ajax APIs, which return the 64 top-ranking search results.529Generalized pattern HTML text snippet<strong> * </strong> - * </span> <strong>Interrupt</strong> - The suspension of normalprogram execution to perform a higher priority service rou-tine as requested by a peripheral device.
</span><dt> * </dt><dd> * </dd> <dt>Netiquette</dt><dd>The established conventionsof online politeness are called netiquette.</dd><h3> * </h3><p> * </p> <h3>Compiler</h3><p>A program that translatessource code, such as C++ or Pascal, into directly executablemachine code.</p><span> * </span> - * </p> <span>Signature</span> - A function?s name and param-eter list.
</p><span> * </span>: * <span> <span>Blog</span>: Short for ?web log?, a blog is anonline journal.
<span>Table 1: Examples of generalized patterns together with matching HTML text snippets.Figure 2: An example of decomposition during pattern extraction for a snippet matching the seed pair(firewall, security system).to the right of the extracted gloss.
In Figure 2 weshow the decomposition of our example snippetmatching the seed (firewall, security system).
(c) pattern instance extraction: we extract thefollowing pattern instance:pL ti pM glosss(ti) pR,where pL is the longest sequence of HTML tagsand non-alphanumeric characters obtained whenmoving to the left of ti (see Figure 2).
(d) pattern extraction: we generalize the abovepattern instance to the following pattern:pL ?
pM ?
pR,i.e., we replace ti and glosss(ti) with *.
For theabove example, we obtain the following pattern:<p><b> * </b> - * </p>.Finally, we add the generalized pattern to the setof patterns P , i.e., P := P ?
{pL ?
pM ?
pR}.We also add the first sentence of the retrieved glossglosss(ti) to our glossary Gk, i.e., Gk := Gk ?
{(ti, first(glosss(ti)))}, where first(g) returnsthe first sentence of gloss g.(e) pattern matching: finally, we look for addi-tional pairs of terms/glosses in the Web page con-taining the snippet s by matching the page againstthe generalized pattern pL ?
pM ?
pR.
We thenadd toGk the new (term, gloss) pairs matching thegeneralized pattern.
In Table 1 we show some non-trivial generalized patterns together with matchingHTML text snippets.As a result of step 3, we obtain a glossary Gkfor the terms discovered at iteration k.Step 4.
Gloss ranking and filtering: impor-tantly, not all the extracted definitions pertain tothe domain of interest.
In order to rank the glossesobtained at iteration k by domain pertinence, weassume that the terms acquired at previous itera-tions belong to the target domain, i.e., they are do-main terms at iteration k. Formally, we define theterminology T k?11 of the domain terms accumu-lated up until iteration k ?
1 as follows: T k?11 :=?k?1i=1 T i, where T i := {t : ?
(t, g) ?
Gi}.
For thebase step k = 1, we define T 01 := {t : ?
(t, g) ?G1}, i.e., we use the first-iteration terminology it-self.To rank the glosses, we first transform each ac-quired gloss g to its bag-of-word representationBag(g), which contains all the single- and multi-word expressions in g. We use the lexicon of thetarget language?s Wikipedia together with T k?11 inorder to obtain the bag of content words.4 Then we4In fact Wikipedia is only utilized in the multi-word iden-tification phase.
We do not use Wikipedia for discoveringnew terms.530Term Gloss Hypernym # Seeds Scoredynamic packet filter A firewall facility that can monitor the state of ac-tive connections and use this information to determinewhich network packets to allow through the firewallfirewall 2 0.75die An integrated circuit chip cut from a finished wafer.
integrated circuit 1 0.75constructor a method used to help create a new object and ini-tialise its datamethod 0 1.00Table 2: Examples of extracted terms, glosses and hypernyms (seeds are in bold, domain terms, i.e., inT k?11 , are underlined, non-domain terms in italics).calculate the domain score of a gloss g as follows:score(g) = |Bag(g) ?
Tk?11 ||Bag(g)| .
(1)Finally, we use a threshold ?
(whose tuning isdescribed in the experimental section) to removefrom Gk those glosses g whose score(g) < ?.In Table 2 we show some glosses in the com-puter science domain (second column, domainterms are underlined) together with their scores(last column).Step 5.
Seed selection for next iteration: wenow aim at selecting the new set of hypernymyrelation seeds to be used to start the next iteration.We perform three substeps:(a) Hypernym extraction: for each newly-acquired term/gloss pair (t, g) ?
Gk, we automati-cally extract a candidate hypernym h from the tex-tual gloss g. To do this we use a simple unsuper-vised heuristic which just selects the first term inthe gloss.5 We show an example of hypernym ex-traction for some terms in Table 2 (we report theterm in column 1, the gloss in column 2 and thehypernyms extracted by the first term hypernymextraction heuristic in column 3).
(b) (Term, Hypernym)-ranking: we sort all theglosses in Gk by the number of seed terms foundin each gloss.
In the case of ties (i.e., glosses withthe same number of seed terms), we further sortthe glosses by the score given in Formula 1.
Weshow an example of rank for some glosses in Table2, where seed terms are in bold, domain terms (i.e.,in T k?11 ) are underlined, and non-domain termsare shown in italics.5While more complex strategies could be used, such assupervised classifiers (Navigli and Velardi, 2010), we foundthat this heuristic works well because, even when it is not ahypernym, the first term plays the role of a cue word for thedefined term.
(c) New seed selection: we select the (term, hy-pernym) pairs corresponding to the K top-rankingglosses.Finally, if k equals the maximum number of it-erations, we stop.
Else, we increment the iterationcounter (i.e., k := k + 1) and jump to step (2) ofour glossary bootstrapping algorithm after replac-ing S with the new set of seeds.The output of glossary bootstrapping is a do-main glossary G := ?i=1,...,maxGi, whichincludes a domain terminology T := {t :?
(t, g) ?
G} (i.e., T := Tmax1 ) and a set ofglosses glosses(t) for each term t ?
T (i.e.,glosses(t) := {g : ?
(t, g) ?
G}).3 Experimental Setup3.1 Domains and Gold StandardsFor our experiments we focused on four differ-ent domains, namely, Computing, Botany, Envi-ronment, and Finance, and on three languages,namely, English, French and Italian.
Note that notall the four domains are clear-cut.
For instance, theEnvironment domain is quite interdisciplinary, in-cluding terms from fields such as Chemistry, Biol-ogy, Law, Politics, etc.For each domain and language we selectedas gold standards well-reputed glossaries onthe Web, such as: the Utah computing glos-sary,6 the Wikipedia glossary of botanical terms,7a set of Wikipedia glossaries about environ-ment,8 and the Reuters glossary for Finance9(full list at http://lcl.uniroma1.it/glossboot/).
We report the size of the fourgold-standard datasets in Table 4.6http://www.math.utah.edu/?wisnia/glossary.html7http://en.wikipedia.org/wiki/Glossary of botanical terms8http://en.wikipedia.org/wiki/List of environmental issues,http://en.wikipedia.org/wiki/Glossary of environmental science,http://en.wikipedia.org/wiki/Glossary of climate change9http://glossary.reuters.com/index.php/Main Page531Computing Botany Environment Financechip circuit leaf organ sewage waste eurobond bonddestructor method grass plant acid rain rain asset play stockcompiler program cultivar variety ecosystem system income stock securityscanner device gymnosperm plant air monitoring sampling financial intermediary institutionfirewall security system flower reproductive organ global warming temperature derivative financial productTable 3: Hypernymy relation seeds used to bootstrap glossary learning in the four domains for the Englishlanguage.3.2 Seed SelectionFor each domain and language we manually se-lected five seed hypernymy relations, shown forthe English language in Table 3.
The seedswere selected by the authors on the basis ofjust two conditions: i) the seeds should coverdifferent aspects of the domain and should, in-deed, identify the domain implicitly, ii) at least10,000 results should be returned by the searchengine when querying it with the seeds plus theglossaryKeyword (see step (2) of GlossBoot).The seed selection was not fine-tuned (i.e., it wasnot adjusted to improve performance), so it mightwell be that better seeds would provide betterresults (see, e.g., (Kozareva and Hovy, 2010b)).However, this type of consideration is beyond thescope of this paper.3.2.1 Evaluation measuresWe performed experiments to evaluate the qualityof both terms and glosses, as jointly extracted byGlossBoot.Terms.
For each domain and language we cal-culated coverage, extra-coverage and precision ofthe acquired terms T .
Coverage is the ratio of ex-tracted terms in T also contained in the gold stan-dard T?
to the size of T?
.
Extra-coverage is calcu-lated as the ratio of the additional extracted termsin T \ T?
over the number of gold standard termsT?
.
Finally, precision is the ratio of extracted termsin T deemed to be within the domain.
To calcu-late precision we randomly sampled 5% of the re-trieved terms and asked two human annotators tomanually tag their domain pertinence (with adju-dication in case of disagreement; ?
= .62, indicat-ing substantial agreement).
Note that by samplingon the entire set T , we calculate the precision ofboth terms in T ?
T?
, i.e., in the gold standard, andterms in T \ T?
, i.e., not in the gold standard, whichare not necessarily outside the domain.Glosses.
We calculated the precision of the ex-tracted glosses as the ratio of glosses which wereboth well-formed textual definitions and specificBotany Comput.
Environ.
FinanceENGold std.
terms 772 421 713 1777GlossBoot terms 5598 3738 4120 5294glosses 11663 4245 5127 6703FRGold std.
terms 662 278 117 109GlossBoot terms 3450 3462 1941 1486glosses 5649 3812 2095 1692ITGold std.
terms 205 244 450 441GlossBoot terms 1965 3356 1630 3601glosses 2678 5891 1759 5276Table 4: Size of the gold-standard andautomatically-acquired glossaries for the fourdomains in the three languages of interest.to the target domain.
Precision was determined ona random sample of 5% of the acquired glosses foreach domain and language.
The annotation wasmade by two annotators, with ?
= .675, indicat-ing substantial agreement.3.3 Parameter tuningWe tuned the minimum and maximum length ofboth pL and pR (see step (3) of GlossBoot) andthe threshold ?
that we use to filter out non-domainglosses (see step (4) of GlossBoot) using an extradomain, i.e., the Arts domain.
To do this, we cre-ated a development dataset made up of the full setof 394 terms from the Tate Gallery glossary,10 andbootstrapped our glossary extraction method withjust one seed, i.e., (fresco, painting).
We chose anoptimal value of ?
= 0.1 on the basis of a har-monic mean of coverage and precision.
Note that,since precision also concerns terms not in the goldstandard, we had to manually validate a sample ofthe extracted terms for each of the 21 tested valuesof ?
?
{0, 0.05, 0.1, .
.
.
, 1.0}.4 Results and Discussion4.1 TermsThe size of the extracted terminologies for the fourdomains after five iterations are reported in Table4.
In Table 5 we show examples of the possi-ble scenarios for terms: in-domain extracted terms10http://www.tate.org.uk/collections/glossary/532In-domain In-domain Out-of-domain In-domain(in gold std, ?
T?
?
T ) (not in gold std, ?
T \ T? )
(not in gold std, ?
T \ T? )
(missed, ?
T?
\ T )Computing software, inheritance, mi-croprocessorclipboard, even parity, su-doergs1-128 label, grayscale,quantum dotsopenwindows, sun mi-crosystems, hardwiredBotany pollinium, stigma, spore vegetation, dichogamous,fertilisationion, free radicals, mana-mananomenclature, endemism,insectivorousEnvironment carcinogen, footprint, solarpowerfrigid soil, biosafety, firesimulatorepidermis, science park,alumg8, best practice,polystyreneFinance cash, bond, portfolio trustor, naked option, mar-ket priceprecedent, immigration,heavy industryco-location, petrodollars,euronextTable 5: Examples of extracted (and missed) terms.Botany Comput.
Environ.
FinanceENPrecision 95% 98% 94% 98%Coverage 85% 40% 35% 32%Extra-coverage 640% 848% 542% 266%FRPrecision 80% 97% 83% 98%Coverage 97% 27% 14% 26%Extra-coverage 425% 1219% 1646% 1350%ITPrecision 89% 98% 76% 99%Coverage 42% 27% 11% 73%Extra-coverage 511% 1349% 356% 746%Table 6: Precision, coverage and extra-coverage ofthe term extraction phase after 5 iterations.which are also found in the gold standard (col-umn 2), in-domain extracted terms but not in thegold standard (column 3), out-of-domain extractedterms (column 4), and domain terms in the goldstandard but not extracted by our approach (col-umn 5).A quantitative evaluation is provided in Table6, which shows the percentage results in terms ofprecision, coverage, and extra-coverage after 5 it-erations of GlossBoot.
For the English languagewe observe good coverage (between 32% and 40%on three domains, with a high peak of 85% cover-age on Botany) and generally very high precisionvalues.
Moreover for the French and the Italianlanguages we observe a peak in the Botany and Fi-nance domains respectively, while the lowest per-formances in terms of precision and coverage areobserved for Environment, i.e., the most interdis-ciplinary domain.In all three languages GlossBoot provides veryhigh extra coverage of domain terms, i.e., addi-tional terms which are not in the gold standard butare returned by our system.
The figures, shown inTable 6, range between 266% (4726/1777) for theEnglish Finance domain and 1646% (1926/117)for the French Environment domain.
These re-sults, together with the generally high precisionvalues, indicate the larger extent of our boot-strapped glossaries compared to our gold stan-dards.Botany Computing Environm.
FinanceMin Max Min Max Min Max Min Max26% 68% 8% 39% 5% 33% 14% 30%Table 7: Coverage ranges for single-seed term ex-traction for the English language.Number of seeds.
Although the choice of se-lecting five hypernymy relation seeds is quite arbi-trary, it shows that we can acquire a reliable termi-nology with minimal human intervention.
Now, anobvious question arises: what if we bootstrappedGlossBoot with fewer hypernym seeds, e.g., justone seed?
To answer this question we replicatedour English experiments on each single (term, hy-pernym) pair in our seed set.
In Table 7 we showthe coverage ranges ?
i.e., the minimum and max-imum coverage values ?
for the five seeds on eachdomain.
We observe that the maximum coveragecan attain values very close to those obtained withfive seeds.
However, the minimum coverage val-ues are much lower.
So, if we adopt a 1-seed boot-strapping policy there is a high risk of acquiringa poorer terminology unless we select the singleseed very carefully, whereas we have shown thatjust a few seeds can cope with domain variabil-ity.
Similar considerations can be made regardingdifferent seed set sizes (we also tried 2, 3 and 4).So five is not a magic number, just one which canguarantee an adequate coverage of the domain.Number of iterations.
In order to study the cov-erage trend over iterations we selected 5 seeds forour tuning domain (i.e., Arts, see Section 3.3).Figure 3 shows the size (left graph), coverage,extra-coverage and precision (middle graph) of theacquired glossary after each iteration, from 1 to20.
As expected, (extra-)coverage grows over iter-ations, while precision drops.
Stopping at iteration5, as we do, is optimal in terms of the harmonicmean of precision and coverage (right graph inFigure 3).53310002000300040005000600070002  4  6  8  10  12  14  16  18  20iterationNumber of terms and glosses extracted over iterationstermsglosses 10%100%1000%2  4  6  8  10  12  14  16  18  20iterationCoverage, extra-coverage and precision over iterationsprecisioncoverageextra-coverage 30%32%34%36%38%40%2  4  6  8  10  12  14  16  18  20iterationHarmonic mean of precision and coverage over iterationsharmonic mean of precision and coverageFigure 3: Size, coverage and precision trends for Arts (tuning domain) over 20 iterations for English.Botany Comput.
Environm.
FinanceEN 96% 94% 97% 97%FR 88% 89% 88% 95%IT 94% 98% 83% 99%Table 8: Precision of the glosses for the four do-mains and for the three languages.4.2 GlossesWe show the results of gloss evaluation in Ta-ble 8.
Precision ranges between 83% and 99%,with three domains performing above 92% on av-erage across languages, and the Environment do-main performing relatively worse because of itshighly interdisciplinary nature (89% on average).We observe that these results are strongly corre-lated with the precision of the extracted terms (cf.Table 6), because the retrieved glosses of domainterms are usually in-domain too, and follow a def-initional style because they come from glossaries.Note, however, that the gloss precision can alsobe higher than term precision, because many perti-nent glosses might be extracted for the same term,cf.
Table 4.5 Comparative Evaluation5.1 Comparison with Google DefineWe performed a comparison with Google De-fine,11 a state-of-the-art definition search service.This service inputs a term query and outputs a listof glosses.
First, we randomly sampled 100 termsfrom our gold standard for each domain and eachof the three languages.
Next, for each domain andlanguage, we manually calculated the fraction ofterms for which an in-domain definition was pro-vided by Google Define and GlossBoot.
Table 9shows the coverage results.Google Define outperforms our system on allfour domains (with a few exceptions).
However11Accessible from Google search by means of thedefine: keyword.Botany Comput.
Environm.
FinanceEN Google Define 90% 87% 84% 82%GlossBoot 77% 47% 44% 51%FR Google Define 40% 48% 36% 82%GlossBoot 88% 42% 22% 32%IT Google Define 52% 74% 78% 80%GlossBoot 64% 38% 44% 92%Table 9: Number of domain glosses (from a ran-dom sample of 100 gold standard terms per do-main) retrieved using Google Define and Gloss-Boot.we note that Google Define: i) requires knowingthe domain term to be defined in advance, whereaswe jointly acquire thousands of terms and glossesstarting from just a few seeds; ii) does not discrim-inate between glosses pertaining to the target do-main and glosses concerning other fields or senses,whereas we extract domain-specific glosses.5.2 Comparison with TaxoLearnWe also compared GlossBoot with a recent ap-proach to glossary learning embedded into aframework for graph-based taxonomy learningfrom scratch, called TaxoLearn (Navigli et al,2011).
Since this approach requires the manualselection of a domain corpus to automatically ex-tract terms and glosses, we decided to keep a levelplaying field and experimented with the same do-main used by the authors, i.e., Artificial Intelli-gence (AI).
TaxoLearn was applied to the entireset of IJCAI 2009 proceedings, resulting in the ex-traction of 427 terms and 834 glosses.12 As re-gards GlossBoot, we selected 10 seeds to cover allthe fields of AI, obtaining 5827 terms and 6716glosses after 5 iterations, one order of magnitudegreater than TaxoLearn.As for the precision of the extracted terms, werandomly sampled 50% of them for each system.We show in Table 10 (first row) the estimated term12Available at: http://lcl.uniroma1.it/taxolearn534GlossBoot TaxoLearnTerm Precision 82.3% (2398/2913) 77.0% (164/213)Gloss Precision 82.8% (2780/3358) 78.9% (329/417)Table 10: Estimated term and gloss precision ofGlossBoot and TaxoLearn for the Artificial Intel-ligence domain.precision for GlossBoot and TaxoLearn.
The pre-cision value for GlossBoot is lower than the preci-sion values of the four domains in Table 6, dueto the AI domain being highly interdisciplinary.TaxoLearn obtained a lower precision because itacquires a full-fledged taxonomy for the domain,thus also including higher-level concepts which donot necessarily pertain to the domain.We performed a similar evaluation for the pre-cision of the acquired glosses, by randomly sam-pling 50% of them for each system.
We show inTable 10 (second row) the estimated gloss preci-sion of GlossBoot and TaxoLearn.
Again, Gloss-Boot outperforms TaxoLearn, retrieving a largeramount of glosses (6716 vs. 834) with higher pre-cision.
We remark, however, that in TaxoLearnglossary extraction is a by-product of the taxon-omy learning process.Finally, we note that we cannot compare withapproaches based on lexical patterns (such as(Kozareva and Hovy, 2010a)), because they arenot aimed at learning glossaries, but just at re-trieving sentence snippets which contain pairs ofterms/hypernyms (e.g., ?supervised systems suchas decision trees?
).6 Related WorkThere are several techniques in the literature forthe automated acquisition of definitional knowl-edge.
Fujii and Ishikawa (2000) use an n-grammodel to determine the definitional nature of textfragments, whereas Klavans and Muresan (2001)apply pattern matching techniques at the lexicallevel guided by cue phrases such as ?is called?and ?is defined as?.
Cafarella et al (2005) de-veloped a Web search engine which handles moregeneral and complex patterns like ?cities such asProperNoun(Head(NP ))?
in which it is possi-ble to constrain the results with syntactic proper-ties.
More recently, a domain-independent super-vised approach was presented which learns Word-Class Lattices (WCLs), i.e.
lattice-based definitionclassifiers that are applied to candidate sentencescontaining the input terms (Navigli and Velardi,2010).
WCLs have been shown to perform withhigh precision in several domains (Velardi et al,2013).To avoid the burden of manually creating atraining dataset, definitional patterns can be ex-tracted automatically.
Reiplinger et al (2012) ex-perimented with two different approaches for theacquisition of lexical-syntactic patterns.
The firstapproach involves bootstrapping patterns from adomain corpus, and then manually refining the ac-quired patterns.
The second approach, instead,involves automatically acquiring definitional sen-tences by using a more sophisticated syntactic andsemantic processing.
The results shows high pre-cision in both cases.However, these approaches to glossary learningextract unrestricted textual definitions from opentext.
In order to filter out non-domain definitions,Velardi et al (2008) automatically extract a do-main terminology from an input corpus which theylater use for assigning a domain score to each har-vested definition and filtering out non-domain can-didates.
The extraction of domain terms from cor-pora can be performed either by means of statis-tical measures such as specificity and cohesion(Park et al, 2002), or just TF*IDF (Kim et al,2009).To avoid the use of a large domain corpus, ter-minologies can be obtained from the Web by usingDoubly-Anchored Patterns (DAPs) which, given a(term, hypernym) pair, harvest sentences match-ing manually-defined patterns like ?<hypernym>such as <term>, and *?
(Kozareva et al, 2008).Kozareva and Hovy (2010a) further extend thisterm extraction process by harvesting new hy-pernyms using the corresponding inverse patterns(called DAP?1) like ?
* such as <term1>, and<term2>?.
Similarly to our approach, they dropthe requirement of a domain corpus and startfrom a small number of (term, hypernym) seeds.However, while Doubly-Anchored Patterns haveproven useful in the induction of domain tax-onomies (Kozareva and Hovy, 2010a), they cannotbe applied to the glossary learning task, becausethe extracted sentences are not formal definitions.In contrast, GlossBoot performs the novel taskof multilingual glossary learning from the Web bybootstrapping the extraction process with a few(term, hypernym) seeds.
Bootstrapping techniques(Brin, 1998; Agichtein and Gravano, 2000; Pas?caet al, 2006) have been successfully applied toseveral tasks, including high-precision semanticlexicon extraction from large corpora (Riloff andJones, 1999; Thelen and Riloff, 2002; McIntosh535Domain Term GlossENBotany deciduous losing foliage at the end of the growing season.Computing information space The abstract concept of everything accessible using networks: the Web.Finance discount The difference between the lower price paid for a security and the security?sface amount at issue.FRBotany insectivore Qui capture des insectes et en absorbe les matie`res nutritives.Computing notebook C?est l?appellation d?un petit portable d?une taille proche d?une feuille A4.Environment e?cosyste`me Ensemble des e?tres vivants et des e?le?ments non vivants d?un milieu qui sontlie?s vitalement entre eux.ITComputing link Collegamento tra diverse pagine web, puo` essere costituito da immagini otesto.Environment effetto serra Riscaldamento dell?atmosfera terrestre dovuto alla presenza di gasnell?atmosfera (anidride carbonica, metano e vapore acqueo) che osta-colano l?uscita delle radiazioni infrarosse emesse dal suolo terreste versol?alto.Finance spread Indica la differenza tra la quotazione di acquisto e quella di vendita.Table 11: An excerpt of the domain glossaries acquired for the three languages.and Curran, 2008; McIntosh and Curran, 2009),learning semantic relations (Pantel and Pennac-chiotti, 2006), extracting surface text patterns foropen-domain question answering (Ravichandranand Hovy, 2002), semantic tagging (Huang andRiloff, 2010) and unsupervised Word Sense Dis-ambiguation (Yarowsky, 1995).
By exploiting the(term, hypernym) seeds to bootstrap the itera-tive acquisition of extraction patterns from Webglossary pages, we can cover the high variabil-ity of textual definitions, including both sentencesmatching the above-mentioned lexico-syntacticpatterns (e.g., ?a corpus is a collection of docu-ments?)
and glossary-style definitions (e.g., ?cor-pus: a collection of document?)
independently ofthe target domain and language.7 ConclusionsIn this paper we have presented GlossBoot, anew, minimally-supervised approach to multilin-gual glossary learning.
Starting from a few hyper-nymy relation seeds which implicitly identify thedomain of interest, we apply a bootstrapping ap-proach which iteratively obtains HTML patternsfrom Web glossaries and then applies them to theextraction of term/gloss pairs.
To our knowledge,GlossBoot is the first approach to large-scale glos-sary learning which jointly acquires thousands ofterms and glosses for a target domain and languagewith minimal supervision.The gist of GlossBoot is our glossary bootstrap-ping approach, thanks to which we can drop therequirements of existing techniques such as theavailability of domain text corpora, which oftendo not contain enough definitions, and the man-ual specification of lexical patterns, which typi-cally extract sentence snippets, instead of formalglosses.GlossBoot will be made available to the re-search community as open-source software.
Be-yond the immediate usability of its output andits effective use for domain Word Sense Disam-biguation (Faralli and Navigli, 2012), we wishto show the benefit of GlossBoot in gloss-drivenapproaches to ontology learning (Navigli et al,2011; Velardi et al, 2013) and semantic networkenrichment (Navigli and Ponzetto, 2012).
In Ta-ble 11 we show an excerpt of the acquired glos-saries.
All the glossaries and gold standards cre-ated for our experiments are available from the au-thors?
Web site http://lcl.uniroma1.it/glossboot/.We remark that the terminologies covered withGlossBoot are not only precise, but also one or-der of magnitude greater than those covered inindividual online glossaries.
As future work weplan to study the ability of GlossBoot to acquiredomain glossaries at different levels of specificity(i.e., domains vs. subdomains).
We also plan toexploit the acquired HTML patterns for imple-menting an open-source glossary crawler, alongthe lines of Google Define.AcknowledgmentsThe authors gratefully acknowledgethe support of the ERC StartingGrant MultiJEDI No.
259234.536ReferencesEugene Agichtein and Luis Gravano.
2000.
Snow-ball: extracting relations from large plain-text col-lections.
In Proceedings of the 5th ACM confer-ence on Digital Libraries, pages 85?94, San Anto-nio, Texas, USA.Sergey Brin.
1998.
Extracting patterns and relationsfrom the World Wide Web.
In Proceedings of theInternational Workshop on The World Wide Web andDatabases, pages 172?183, London, UK.Michael J. Cafarella, Doug Downey, Stephen Soder-land, and Oren Etzioni.
2005.
KnowItNow: Fast,scalable information extraction from the web.
InProceedings of Human Language Technology Con-ference and Conference on Empirical Methods inNatural Language Processing, pages 563?570, Van-couver, British Columbia, Canada.Hang Cui, Min-Yen Kan, and Tat-Seng Chua.
2007.Soft pattern matching models for definitional ques-tion answering.
ACM Transactions on InformationSystems, 25(2):1?30.Weisi Duan and Alexander Yates.
2010.
Extractingglosses to disambiguate word senses.
In Proceed-ings of Human Language Technologies: The 11thAnnual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 627?635, Los Angeles, CA, USA.Stefano Faralli and Roberto Navigli.
2012.
ANew Minimally-supervised Framework for DomainWord Sense Disambiguation.
In Proceedings ofthe 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and Compu-tational Natural Language Learning, pages 1411?1422, Jeju, Korea.Atsushi Fujii and Tetsuya Ishikawa.
2000.
Utilizingthe World Wide Web as an encyclopedia: extract-ing term descriptions from semi-structured texts.
InProceedings of the 38th Annual Meeting on Associa-tion for Computational Linguistics, pages 488?495,Hong Kong.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 15th International Conference on ComputationalLinguistics, pages 539?545, Nantes, France.Eduard H. Hovy, Roberto Navigli, and Simone PaoloPonzetto.
2013.
Collaboratively built semi-structured content and Artificial Intelligence: Thestory so far.
Artificial Intelligence, 194:2?27.Ruihong Huang and Ellen Riloff.
2010.
Induc-ing domain-specific semantic class taggers from (al-most) nothing.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 275?285, Uppsala, Sweden.Su Nam Kim, Timothy Baldwin, and Min-Yen Kan.2009.
An unsupervised approach to domain-specificterm extraction.
In Proceedings of the AustralasianLanguage Technology Workshop, pages 94?98, Syd-ney, Australia.Judith Klavans and Smaranda Muresan.
2001.
Evalu-ation of the DEFINDER system for fully automaticglossary construction.
In Proceedings of the Amer-ican Medical Informatics Association (AMIA) Sym-posium, pages 324?328, Washington, D.C., USA.Zornitsa Kozareva and Eduard Hovy.
2010a.
Asemi-supervised method to learn and construct tax-onomies using the Web.
In Proceedings of Empiri-cal Methods in Natural Language Processing, pages1110?1118, Cambridge, MA, USA.Zornitsa Kozareva and Eduard H. Hovy.
2010b.
Notall seeds are equal: Measuring the quality of textmining seeds.
In Proceedings of Human Lan-guage Technologies: The 11th Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 618?626, LosAngeles, California, USA.Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.2008.
Semantic class learning from the Web withhyponym pattern linkage graphs.
In Proceedings ofthe 46th Annual Meeting of the Association for Com-putational Linguistics, pages 1048?1056, Colum-bus, Ohio, USA.Tara McIntosh and James R. Curran.
2008.
Weightedmutual exclusion bootstrapping for domain indepen-dent lexicon and template acquisition.
In Proceed-ings of the Australasian Language Technology Asso-ciation Workshop, pages 97?105, CSIRO ICT Cen-tre, Tasmania.Tara McIntosh and James R. Curran.
2009.
Reducingsemantic drift with bagging and distributional sim-ilarity.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP, pages 396?404, Suntec,Singapore.Roberto Navigli and Simone Paolo Ponzetto.
2012.BabelNet: The automatic construction, evaluationand application of a wide-coverage multilingual se-mantic network.
Artificial Intelligence, 193:217?250.Roberto Navigli and Paola Velardi.
2010.
LearningWord-Class Lattices for definition and hypernym ex-traction.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, pages 1318?1327, Uppsala, Sweden.Roberto Navigli, Paola Velardi, and Stefano Faralli.2011.
A graph-based algorithm for inducing lexi-cal taxonomies from scratch.
In Proceedings of the22th International Joint Conference on Artificial In-telligence, pages 1872?1877, Barcelona, Spain.537Marius Pas?ca, Dekang Lin, Jeffrey Bigham, AndreiLifchits, and Alpa Jain.
2006.
Names and similari-ties on the web: Fact extraction in the fast lane.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 809?816, Sydney, Australia.Patrick Pantel and Marco Pennacchiotti.
2006.Espresso: Leveraging Generic Patterns for Auto-matically Harvesting Semantic Relations.
In Pro-ceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics(COLING-ACL), Sydney, Australia, pages 113?120,Sydney, Australia.Youngja Park, Roy J. Byrd, and Branimir K. Boguraev.2002.
Automatic glossary extraction: beyond termi-nology identification.
In Proceedings of the 19th In-ternational Conference on Computational Linguis-tics, pages 1?7, Taipei, Taiwan.Deepak Ravichandran and Eduard Hovy.
2002.
Learn-ing surface text patterns for a question answeringsystem.
In Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, pages41?47, Philadelphia, Pennsylvania.Melanie Reiplinger, Ulrich Scha?fer, and MagdalenaWolska.
2012.
Extracting glossary sentences fromscholarly articles: A comparative evaluation of pat-tern bootstrapping and deep analysis.
In Proceed-ings of the ACL-2012 Special Workshop on Redis-covering 50 Years of Discoveries, pages 55?65, JejuIsland, Korea.Ellen Riloff and Rosie Jones.
1999.
Learning dic-tionaries for information extraction by multi-levelbootstrapping.
In Proceedings of the sixteenth na-tional conference on Artificial intelligence and theeleventh Innovative applications of artificial intelli-gence conference, pages 474?479, Menlo Park, CA,USA.Horacio Saggion.
2004.
Identifying definitions in textcollections for question answering.
In Proceedingsof the Fourth International Conference on LanguageResources and Evaluation, pages 1927?1930, Lis-bon, Portugal.Michael Thelen and Ellen Riloff.
2002.
A bootstrap-ping method for learning semantic lexicons usingextraction pattern contexts.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 214?221, Salt Lake City,UT, USA.Paola Velardi, Roberto Navigli, and PierluigiD?Amadio.
2008.
Mining the Web to createspecialized glossaries.
IEEE Intelligent Systems,23(5):18?25.Paola Velardi, Stefano Faralli, and Roberto Navigli.2013.
OntoLearn Reloaded: A graph-based algo-rithm for taxonomy induction.
Computational Lin-guistics, 39(3).David Yarowsky.
1995.
Unsupervised Word SenseDisambiguation rivaling supervised methods.
InProceedings of the 33rd Annual Meeting of the As-sociation for Computational Linguistics, pages 189?196, Cambridge, MA, USA.538
