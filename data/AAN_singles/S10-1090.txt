Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 402?406,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsGPLSI-IXA: Using Semantic Classes to Acquire Monosemous TrainingExamples from Domain TextsRub?en Izquierdo & Armando Su?arezGPLSI GroupUniversity of Alicante.
Spain{ruben,armando}@dlsi.ua.esGerman RigauIXA NLP Group.EHU.
Donostia, Spaingerman.rigau@ehu.esAbstractThis paper summarizes our participationin task #17 of SemEval?2 (All?wordsWSD on a specific domain) using a su-pervised class-based Word Sense Disam-biguation system.
Basically, we use Sup-port Vector Machines (SVM) as learningalgorithm and a set of simple features tobuild three different models.
Each modelconsiders a different training corpus: Sem-Cor (SC), examples from monosemouswords extracted automatically from back-ground data (BG), and both SC andBG (SCBG).
Our system explodes themonosemous words appearing as mem-bers of a particular WordNet semanticclass to automatically acquire class-basedannotated examples from the domain text.We use the class-based examples gatheredfrom the domain corpus to adapt our tra-ditional system trained on SemCor.
Theevaluation reveal that the best results areachieved training with SemCor and thebackground examples from monosemouswords, obtaining results above the firstsense baseline and the fifth best positionin the competition rank.1 IntroductionAs empirically demonstrated by the last SensEvaland SemEval exercises, assigning the appropriatemeaning to words in context has resisted all at-tempts to be successfully addressed.
In fact, super-vised word-based WSD systems are very depen-dent of the corpora used for training and testingthe system (Escudero et al, 2000).
One possiblereason could be the use of inappropriate level ofabstraction.Most supervised systems simply model eachpolysemous word as a classification problemwhere each class corresponds to a particular synsetof the word.
But, WordNet (WN) has been widelycriticized for being a sense repository that oftenprovides too fine?grained sense distinctions forhigher level applications like Machine Translationor Question & Answering.
In fact, WSD at thislevel of granularity has resisted all attempts of in-ferring robust broad-coverage models.
It seemsthat many word?sense distinctions are too subtleto be captured by automatic systems with the cur-rent small volumes of word?sense annotated ex-amples.Thus, some research has been focused on deriv-ing different word-sense groupings to overcomethe fine?grained distinctions of WN (Hearst andSch?utze, 1993), (Peters et al, 1998), (Mihalceaand Moldovan, 2001), (Agirre and LopezDeLa-Calle, 2003), (Navigli, 2006) and (Snow et al,2007).
That is, they provide methods for groupingsenses of the same word, thus producing coarserword sense groupings for better disambiguation.In contrast, some research have been focused onusing predefined sets of sense-groupings for learn-ing class-based classifiers for WSD (Segond et al,1997), (Ciaramita and Johnson, 2003), (Villarejoet al, 2005), (Curran, 2005), (Kohomban and Lee,2005) and (Ciaramita and Altun, 2006).
That is,grouping senses of different words into the sameexplicit and comprehensive semantic class.
Mostof the later approaches used the original Lexico-graphical Files of WN (more recently called Su-perSenses) as very coarse?grained sense distinc-tions.We suspect that selecting the appropriate levelof abstraction could be on between both levels.Thus, we use the semantic classes modeled by theBasic Level Concepts1(BLC) (Izquierdo et al,2007).
Our previous research using BLC empiri-cally demonstrated that this automatically derived1http://adimen.si.ehu.es/web/BLC402set of meanings groups senses into an adequatelevel of abstraction in order to perform class-basedWord Sense Disambiguation (WSD) (Izquierdo etal., 2009).
Now, we also show that class-basedWSD allows to successfully incorporate monose-mous examples from the domain text.
In fact,the robustness of our class-based WSD approachis shown by our system that just uses the Sem-Cor examples (SC).
It performs without any kindof domain adaptation as the Most Frequent Sense(MFS) baseline.This paper describes our participation inSemEval-2010 Task 17 (Agirre et al, 2010).
Insection 2 semantic classes used and selection al-gorithm used to obtain them automatically fromWordNet are described.
In section 3 the techniqueemployed to extract monosemous examples frombackground data is described.
Section 4 explainsthe general approach of our system, and the ex-periments designed, and finally, in section 5, theresults and some analysis are shown.2 Semantic ClassesThe set of semantic classes used in this work arethe Basic Level Concepts2(BLC) (Izquierdo etal., 2007).
These concepts are small sets of mean-ings representing the whole nominal and verbalpart of WN.
BLC can be obtained by a very simplemethod that uses basic structural WordNet proper-ties.
In fact, the algorithm only considers the rel-ative number of relations of each synset alng thehypernymy chain.
The process follows a bottom-up approach using the chain of hypernymy rela-tions.
For each synset in WN, the process selectsas its BLC the first local maximum according tothe relative number of relations.
The local maxi-mum is the synset in the hypernymy chain havingmore relations than its immediate hyponym andimmediate hypernym.
For synsets having multi-ple hypernyms, the path having the local maxi-mum with higher number of relations is selected.Usually, this process finishes having a number ofpreliminary BLC.
Figure 1 shows an example ofselection of a BLC.
The figure represents the hy-pernymy hierarchy of WordNet, with circles rep-resenting synsets, and links between them repre-senting hypernym relations.
The algorithm selectsthe D synset as BLC for J, due to D is the firstmaximum in the hypernymy chain, according tothe number of relations (F has 2 hyponyms, D has2http://adimen.si.ehu.es/web/BLC3, and A has 2, so D is the first maximum).IDG HCAJF232BE2B L CS y n s e tFigure 1: Example of BLC selectionObviously, while ascending through this chain,more synsets are subsumed by each concept.
Theprocess finishes checking if the number of con-cepts subsumed by the preliminary list of BLC ishigher than a certain threshold.
For those BLCnot representing enough concepts according to thethreshold, the process selects the next local max-imum following the hypernymy hierarchy.
Thus,depending on the type of relations considered tobe counted and the threshold established, differentsets of BLC can be easily obtained for each WNversion.We have selected the set which considers WNversion 3.0, the total number of relations persynset, and a minimum threshold of 20 concepts tofilter out not representative BLC (BLC?20).
Thisset has shown to reach good performance on previ-ous SensEval and SemEval exercices (Izquierdo etal., 2009).
There are 649 different BLC for nounson WordNet 3.0, and 616 for verbs.
Table 2 showsthe three most frequent BLC per POS, with thenumber of synsets subsumed by each concept, andits WordNet gloss.3 Using Monosemous Examples from theDomainWe did not applied any kind of specific domainadaptation technique to our class-based supervisedsystem.
In order to adapt our supervised system tothe environmental domain we only increased thetraining data with new examples of the domain.
Toacquire these examples, we used the environmen-tal domain background documents provided by theorganizers.
Specifically, we used the 122 back-403PoS Num.
BLC GlossNouns4.792 person.n.01 a human being1.935 activity.n.01 any specific behavior1.846 act.n.02 something that people do or cause to happenVerbs1.541 change.v.01 cause to change; make different; cause a transformation1.085 change.v.02 undergo a change; become different in essence; losing one?s or its original na-ture519 move.v.02 cause to move or shift into a new position or place, both in a concrete and in anabstract senseTable 1: Most frequent BLC?20 semantic classes on WordNet 3.0ground documents3.
TreeTagger has been usedto preprocess the documents, performing PoS tag-ging and lemmatization.
Since the backgrounddocuments are not semantically annotated, and oursupervised system needs labeled data, we have se-lected only the monosemous words occurring inthe documents.
In this way, we have obtained au-tomatically a large set of examples annotated withBLC.
Table 3 presents the total number of trainingexamples extracted from SemCor (SC) and fromthe background documents (BG).
As expected, bythis method a large number of monosemous ex-amples can be obtained for nouns and verbs.
Alsoas expected, verbs are much less productive thannouns.
However, all these background examplescorrespond to a reduced set of 7,646 monosemouswords.Nouns Verbs N+VSC 87.978 48.267 136.245BG 193.536 10.821 204.357Total 281.514 59.088 340.602Table 2: Number of training examplesTable 3 lists the ten most frequent monosemousnouns and verbs occurring in the background doc-uments.
Note that all these examples are monose-mous according to BLC?20 semantic classes.Nouns VerbsLemma # ex.
Lemma # ex.1 biodiversity 7.476 monitor 7882 habitat 7.206 achieve 7843 specie 7.067 target 4844 climate 3.539 select 3455 european 2.818 enable 3346 ecosystem 2.669 seem 2877 river 2.420 pine 2818 grassland 2.303 evaluate 2469 datum 2.276 explore 20010 directive 2.197 believe 172Table 3: Most frequent monosemic words in BG3We used the documents contained on the trial data andthe background.4 System OverviewOur system applies a supervised machine learn-ing approach.
We apply a feature extractor torepresent the training examples of the examplesacquired from SemCor and the background doc-uments.
Then, a machine learning engine usesthe annotated examples to train a set of classi-fiers.
Support Vector Machines (SVM) have beenproven to be robust and very competitive in manyNLP tasks, and in WSD in particular (M`arquez etal., 2006).
We used the SVM-Light implementa-tion4(Joachims, 1998).We create a classifier for each semantic class.This approach has several advantages compared toword?based approach.
The training data per clas-sifier is increased (we can use examples of dif-ferent target words for a single classifier, when-ever all examples belong to the same semanticclass), the polysemy is reduced (some differentword senses can be collapsed into the same se-mantic class), and, finally, semantic classes pro-vide higher levels of abstraction.For each polysemous word occurring in the testcorpus, we obtain its potential BLC?20 classes.Then, we only apply the classifiers correspondingto the BLC-20 classes of the polysemous word.
Fi-nally, our system simply selects the BLC?20 classwith the greater prediction.In order to obtain the correct WordNet 3.0synset required by the task, we apply a simpleheuristic that has shown to be robust and accurate(Kohomban and Lee, 2005).
Our classifiers ob-tain first the semantic class, and then, the synset ofthe first WordNet sense that fits with the semanticclass is assigned to the word.We selected a simple feature set widely used inmany WSD systems.
In particular, we use a win-dow of five tokens around the target word to ex-tract word forms, lemmas; bigrams and trigramsof word forms and lemmas; trigrams of PoS tags,4http://svmlight.joachims.org404and also the most frequent BLC?20 semantic classof the target word in the training corpus.Our system is fully described in (Izquierdo etal., 2009).
The novelty introduced here is the useof semantic classes to obtain monosemous exam-ples from the domain corpus.Following the same framework (BLC?20 se-mantic architecture and basic set of features) wedesigned three runs, each one using a differenttraining corpus.?
SC: only training examples extracted fromSemCor?
BG: only monosemous examples extractedfrom the background data?
SCBG: training examples extracted fromSemCor and monosemous background dataThe first run shows the behavior of a supervisedsystem trained on a general corpus, and tested in aspecific domain.
The second one analyzes the con-tribution of the monosemous examples extractedfrom the background data.
Finally, the third runstudies the robustness of the approach when com-bining the training examples from SemCor andfrom the background.5 Results and DiscussionA total of 29 runs has been submitted for the En-glish All?words WSD on a Specific Domain.
Ta-ble 5 shows the ranking results of our three runswith respect to the other participants.
The figuresfor the first sense (1sense) and random sense (Ran-dom) baselines are included.In general, the results obtained are not veryhigh.
The best system only achieves a precision of0.570, and the first sense baseline reaches a preci-sion of 0.505.
This shows that the task is hard tosolve, and the domain adaptation of WSD systemsis not an easy task.Interestingly, our worst result is obtained by thesystem using only the monosemous backgroundexamples (BG).
This system ranks 23th with a Pre-cision and Recall of 0.380 (0.385 for nouns and0.366 for verbs).
The system using only SemCor(SC) ranks 6th with Precision and Recall of 0.505(0.527 for nouns and 0.443 for verbs).
This is alsothe performance of the first sense baseline.
As ex-pected, the best result of our three runs is obtainedwhen combining the examples from SemCor andthe background (SCBG).
This supervised systemobtains the 5th position with a Precision and Re-call of 0.513 (0.534 for nouns, 0.454 for verbs)which is slightly above the baseline.Rank Precision Recall1 0.570 0.5552 0.554 0.5403 0.534 0.5284 0.522 0.516(SCBG) 5 0.513 0.5131sense 0.505 0.505(SC) 6 0.505 0.5057 0.512 0.4958 0.506 0.4939 0.504 0.49110 0.481 0.48111 0.492 0.47912 0.461 0.46013 0.447 0.44114 0.436 0.43515 0.440 0.43416 0.496 0.43317 0.498 0.43218 0.433 0.43119 0.426 0.42520 0.424 0.42221 0.437 0.39222 0.384 0.384(BG) 23 0.380 0.38024 0.381 0.35625 0.351 0.35026 0.370 0.34527 0.328 0.32228 0.321 0.31529 0.312 0.303Random 0.230 0.230Table 4: Results of task#17Possibly, the reason of low performance of theBG system is the high correlation between the fea-tures of the target word and its semantic class.
Inthis case, these features correspond to the monose-mous word while when testing corresponds to thetarget word.
However, it also seems that class-based systems are robust enough to incorporatelarge sets of monosemous examples from the do-main text.
In fact, to our knowledge, this is the firsttime that a supervised WSD algorithm have beensuccessfully adapted to an specific domain.
Fur-thermore, our system trained only on SemCor alsoachieves a good performance, reaching the firstsense baseline, showing that class-based WSD ap-proaches seem to be robust to domain variations.AcknowledgmentsThis paper has been supported by the Euro-pean Union under the project KYOTO (FP7 ICT-211423), the Valencian Region Government un-der PROMETEO project for excellence groupsand the Spanish Government under the projects405KNOW2 (TIN2009-14715-C04-04) and TEXT-MESS-2 (TIN2009-13391-C04-04).ReferencesE.
Agirre and O. LopezDeLaCalle.
2003.
Clusteringwordnet word senses.
In Proceedings of RANLP?03,Borovets, Bulgaria.Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-baum, Shu kai Hsieh, Maurizio Tesconi, Mon-ica Monachini, Piek Vossen, and Roxanne Segers.2010.
Semeval-2010 task 17: All-words word sensedisambiguation on a specific domain.
In Proceed-ings of the 5th International Workshop on SemanticEvaluations (SemEval-2010), Association for Com-putational Linguistics.M.
Ciaramita and Y. Altun.
2006.
Broad-coveragesense disambiguation and information extractionwith a supersense sequence tagger.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP?06), pages 594?602,Sydney, Australia.
ACL.M.
Ciaramita and M. Johnson.
2003.
Supersense tag-ging of unknown nouns in wordnet.
In Proceedingsof the Conference on Empirical methods in naturallanguage processing (EMNLP?03), pages 168?175.ACL.J.
Curran.
2005.
Supersense tagging of unknownnouns using semantic similarity.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics (ACL?05), pages 26?33.
ACL.G.
Escudero, L. M`arquez, and G. Rigau.
2000.
AnEmpirical Study of the Domain Dependence of Su-pervised Word Sense Disambiguation Systems.
InProceedings of the joint SIGDAT Conference on Em-pirical Methods in Natural Language Processingand Very Large Corpora, EMNLP/VLC, Hong Kong,China.M.
Hearst and H. Sch?utze.
1993.
Customizing a lexi-con to better suit a computational task.
In Proceed-ingns of the ACL SIGLEX Workshop on Lexical Ac-quisition, Stuttgart, Germany.R.
Izquierdo, A. Suarez, and G. Rigau.
2007.
Explor-ing the automatic selection of basic level concepts.In Galia Angelova et al, editor, International Con-ference Recent Advances in Natural Language Pro-cessing, pages 298?302, Borovets, Bulgaria.Rub?en Izquierdo, Armando Su?arez, and German Rigau.2009.
An empirical study on class-based word sensedisambiguation.
In Proceedings of the 12th Con-ference of the European Chapter of the ACL (EACL2009), pages 389?397, Athens, Greece, March.
As-sociation for Computational Linguistics.T.
Joachims.
1998.
Text categorization with sup-port vector machines: learning with many relevantfeatures.
In Claire N?edellec and C?eline Rouveirol,editors, Proceedings of ECML-98, 10th EuropeanConference on Machine Learning, pages 137?142,Chemnitz, DE.
Springer Verlag, Heidelberg, DE.Upali S. Kohomban and Wee Sun Lee.
2005.
Learningsemantic classes for word sense disambiguation.
InACL ?05: Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics, pages34?41, Morristown, NJ, USA.
Association for Com-putational Linguistics.Ll.
M`arquez, G. Escudero, D.
Mart?
?nez, and G. Rigau.2006.
Supervised corpus-based methods for wsd.
InE.
Agirre and P. Edmonds (Eds.)
Word Sense Disam-biguation: Algorithms and applications., volume 33of Text, Speech and Language Technology.
Springer.R.
Mihalcea and D. Moldovan.
2001.
Automatic gen-eration of coarse grained wordnet.
In Proceding ofthe NAACL workshop on WordNet and Other Lex-ical Resources: Applications, Extensions and Cus-tomizations, Pittsburg, USA.R.
Navigli.
2006.
Meaningful clustering of senseshelps boost word sense disambiguation perfor-mance.
In ACL-44: Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th annual meeting of the Association forComputational Linguistics, pages 105?112, Morris-town, NJ, USA.
Association for Computational Lin-guistics.W.
Peters, I. Peters, and P. Vossen.
1998.
Automaticsense clustering in eurowordnet.
In First Interna-tional Conference on Language Resources and Eval-uation (LREC?98), Granada, Spain.F.
Segond, A. Schiller, G. Greffenstette, and J. Chanod.1997.
An experiment in semantic tagging using hid-den markov model tagging.
In ACL Workshop onAutomatic Information Extraction and Building ofLexical Semantic Resources for NLP Applications,pages 78?81.
ACL, New Brunswick, New Jersey.R.
Snow, Prakash S., Jurafsky D., and Ng A.
2007.Learning to merge word senses.
In Proceedings ofJoint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 1005?1014.L.
Villarejo, L. M`arquez, and G. Rigau.
2005.
Ex-ploring the construction of semantic class classi-fiers for wsd.
In Proceedings of the 21th AnnualMeeting of Sociedad Espaola para el Procesamientodel Lenguaje Natural SEPLN?05, pages 195?202,Granada, Spain, September.
ISSN 1136-5948.406
