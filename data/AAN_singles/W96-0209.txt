APPORTIONING DEVELOPMENT EFFORTIN A PROBABIL IST IC  LR PARSING SYSTEMTHROUGH EVALUATIONJ ohn  Carrol l  Ted Br i scoeCogn i t ive  and Comput ing  Sciences Computer  LaboratoryUn ivers i ty  of Sussex Un ivers i ty  of Cambr idgeBr ighton  BN1 9QH,  UK Pembroke  St reet ,  Cambr idge  CB2 3QG,  UKjohn.
carroll@cogs.susx, ac.
uk ejb @cl.
cam.
ac.
ukAbst ractWe describe an implemented system for robustdomain-independent syntactic parsing of English,using a unification-based grammar of part-of-speech and punctuation labels coupled with aprobabilistic LR parser.
We present evaluationsof the system's performance along several differ-ent dimensions; these enable us to assess the con-tribution that each individual part is making tothe success of the system as a whole, and thusprioritise the effort to be devoted to its furtherenhancement.
Currently, the system is able toparse around 80% of sentences in a substantialcorpus of general text containing a number ofdistinct genres.
On a random sample of 250such sentences the system has a mean crossingbracket rate of 0.71 and recall and precision of83% and 84~0 respectively when evaluated againstmanually-disambiguated analyses I .1.
INTRODUCTIONThis work is part of an effort to develop a ro-bust, domain-independent syntactic parser capa-ble of yielding the unique correct analysis for un-restricted naturally-occurring input.
Our goal isto develop a system with performance compara-ble to extant part-of-speech taggers, returning asyntactic analysis from which predicate-argumentstructure can be recovered, and which can sup-port semantic interpretation.
The requirement fora domain-independent a alyser favours statistical1Some of this work was carried out while thesecond author was visiting Rank Xerox, Grenoble.The work was also supported by UK DTI/SALTproject 41/5808 'Integrated Language Database', andby SERC/EPSRC Advanced Fellowships to both au-thors.
Geoff Nunberg provided encouragement andmuch advice on the analysis of punctuation, and GregGrefenstette undertook the original corpus tokenisa-tion and segmentation for the punctuation experi-ments.
Bernie .\]ones and Kiku Ribas made helpfulcomments on an earlier draft.
We are responsible forany mistakes.92techniques to resolve ambiguities, whilst the lat-ter goal favours a more sophisticated grammaticalformalism than is typical in statistical approachesto robust analysis of corpus material.Briscoe ~ Carroll (1993) describe a proba-blistic parser using a wide-coverage unification-based grammar of English written in the AlveyNatural Language Tools (ANLT) metagrammat-ical formalism (Briscoe et al, 1987), generatingaround 800 rules in a syntactic variant of the Def-inite Clause Grammar formalism (DCG, PereiraWarren, 1980) extended with iterative (Kleene)operators.
The ANLT grammar is linked to a lex-icon containing about 64K entries for 40K lex-emes, including detailed subcategorisation infor-mation appropriate for the grammar, built semi-automatically from a learners' dictionary (Car-roll L= Grover, 1989).
The resulting parser isefficient, constructing a parse forest in roughlyquadratic time (empirically), and efficiently re-turning the ranked n-most likely analyses (Car-roll, 1993, 1994).
The probabilistic model is arefinement of probabilistic ontext-free grammar(PCFG) conditioning CF 'backbone' rule applica-tion on LR state and lookahead item.
Unificationof the 'residue' of features not incorporated intothe backbone is performed at parse time in con-junction with reduce operations.
Unification fail-ure results in the associated erivation being as-signed a probability of zero.
Probabilities are as-signed to transitions in the LALR(1) action tablevia a process of supervised training based on com-puting the frequency with which transitions aretraversed in a corpus of parse histories.
The resultis a probabilistic parser which, unlike a PCFG, iscapable of probabilistically discriminating deriva-tions which differ only in terms of order of appli-cation of the same set of CF backbone rules, dueto the parse context defined by the LR table.Experiments with this system revealed threemajor problems which our current research is ad-dressing.
Firstly, improvements in probabilisticparse selection will require a 'lexicalised' gram-mar/parser in which (minimally) probabilities areassociated with alternative subcategorisation pos-sibilities of individual lexical items.
Currently, therelative frequency of subcategorisation possibili-ties for individual exical items is not recorded inwide-coverage l xicons, such as ANLT or COM-LEX (Grishman e?
al., 1994).
Secondly, removalof punctuation from the input (after segmen-tation into text sentences) worsens performanceas punctuation both reduces syntactic ambigu-ity (Jones, 1994) and signals non-syntactic (dis-course) relations between text units (Nunberg,1990).
Thirdly, the largest source of error on un-seen input is the omission of appropriate subcate-gorisation values for lexical items (mostly verbs),preventing the system from finding the correctanalysis.
The current coverage--the proportionof sentences for which at least one analysis wasfoundS--of this system on a general corpus (e.g.Brown or LOB) is estimated to be around 20%by Briscoe (1994).
Therefore, we have developeda variant probabilistic LR parser which does notrely on subcategorisation a d uses punctuation toreduce ambiguity, The analyses produced by thisparser can be utilised for phrase-finding applica-tions, recovery of subcategorisation frames, andother 'intermediate' l vel parsing problems.2.
PART:OF-SPEECH TAGSEQUENCE GRAMMARWe utilised the ANLT metagrammatical formal-ism to develop a feature-based, eclarative de-scription of part-of-speech (PoS) label sequences(see e.g.
Church, 1988) for English.
This gram-mar compiles into a DCG-like grammar of ap-proximately 400 rules.
It has been designedto enumerate possible valencies for predicates(verbs, adjectives and nouns) by including sep-arate rules for each pattern of possible comple-mentation in English.
The distinction between ar-guments and adjuncts is expressed, following X-bar theory (e.g.
Jackendoff, 1977), by Chomsky-adjunction of adjuncts to maximal projections(XP ~ XP Adjunct) as opposed to government ofarguments (i.e.
arguments are sisters within X1projections; X1 --~ X0 Argl .
.
.
ArgN).
Althoughthe grammar enumerates complementation pos-sibilities and checks for global sentential well-formedness, it is best described as 'intermediate'as it does not attempt o associate 'displaced' con-stituents with their canonical position / grammat-ical role.The other difference between this grammar2Briscoe & Carroll (1995) note that "coverage" isa weak measure since discovery of one or more globalanalyses does not entail that the correct analysis isrecovered.and a more conventional one is that it incorporatessome rules specifically designed to overcome lim-itations or idiosyncrasies of the tagging process.For example, past participles functioning adjec-tivally, as in (la), are fl'equently tagged as pastparticiples (VVN) as in (lb), so the grammar in-corporates a rule (violating X-bar theory) whichparses past participles as adjectival premodifiersin this context.
(1) a The disembodied headb The_AT disembodied_VVN head_NN1Similar idiosyncratic rules are incorporated fordealing with gerunds, adjective-noun conversions,idiom sequences, and so forth.
Further details ofthe PoS grammar are given in Briscoe & Carroll(1994, 1995).The grammar currently covers around 80% ofthe Susanne corpus (Sampson, 1995), a 138K wordtreebanked and balanced subset of the Brown cor-pus.
Many of the 'failures' are due to the rootS(entence) requirement enforced by the parserwhen dealing with fragments from dialogue andso forth.
We have not relaxed this requirementsince it increases ambiguity, our primary interestat this point being the extraction of subcategorisa-tion information from full clauses in corpus data.3.
TEXT GRAMMAR ANDPUNCTUATIONNunberg (1990) develops a partial 'text' grammarfor English which incorporates mnany constraintsthat (ultimately) restrict syntactic and seman-tic interpretation.
For example, textual adjunctclauses introduced by colons scope over followingpunctuation, as (2a) illustrates; whilst textual ad-juncts introduced by dashes cannot intervene be-tween a bracketed adjunct and the textual unit towhich it attaches, as in (2b).
(2) a *He told them his reason: he would notrenegotiate his contract, but he did notexplain to the team owners.
(vs. butwould stay)b *She left - who could blame her - (dur-ing the chainsaw scene) and went home.We have developed a declarative grammar inthe ANLT metagrammatical formalism, based onNunberg's procedural description.
This grammarcaptures the bulk of the text-sentential constraintsdescribed by Nunberg with a grammar which com-piles into 26 DCG-tike rules.
Text grammar anal-yses are useful because they demarcate some ofthe syntactic boundaries in the text sentence andthus reduce ambiguity, and because they identifythe units for which a syntactic analysis hould, in93principle, be found; for example, in (3), the ab-sence of dashes would mislead a parser into seek-ing a syntactic relationship between three and thefollowing names, whilst in fact there is only a dis-course relation of elaboration between this textadjunct and pronominal three.
(3) The three - Miles J. Cooperman, SheldonTeller, and Richard Austin - and eightother defendants were charged in six in-dictments with conspiracy to violate fed-eral narcotic law.Further details of the text grammar are givenin Briscoe ~ Carroll (1994, 1995).
The textgrammar has been tested on the Susanne corpusand covers 99.8% of sentences.
(The failures aremostly text segmentation problems).
The numberof analyses varies from one (71%) to the thousands(0.1%).
Just over 50% of Susanne sentences con-tain some punctuation, so around 20% of the sin-gleton parses are punctuated.
The major source ofambiguity in the analysis of punctuation concernsthe function of commas and their relative scope asa result of a decision to distinguish delimiters andseparators (Nunberg 1990:36).
Therefore, a textsentence containing eight commas (and no otherpunctuation) will have 3170 analyses.
The mul-tiple uses of commas cannot be resolved withoutaccess to (at least) the syntactic ontext of occur-rence.4.
THE INTEGRATEDGRAMMARDespite Nunberg's observation that text grammaris distinct from syntax, text grammatical mbigu-ity favours interleaved application of text gram-matical and syntactic onstraints.
Integrating thetext and the PoS sequence grammars is straight-forward and the result remains modular, in thatthe text grammar is 'folded into' the PoS sequencegrammar, by treating text and syntactic ategoriesas overlapping and dealing with the properties ofeach using disjoint sets of features, principles offeature propagation, and so forth.
In addition tothe core text-grammatical rules which carry overunchanged from the stand-alone text grammar, 44syntactic rules (of pre- and post- posing, and co-ordination) now include (often optional) commamarkers corresponding to the purely 'syntactic'uses of punctuation.The approach to text grammar taken here is inmany ways similar to that of Jones (1994).
How-ever, he opts to treat punctuation marks as cliticson words which introduce additional featural in-formation into standard syntactic rules.
Thus, hisgrammar is thoroughly integrated and it would beharder to extract an independent text grammaror build a modular semantics.
Our less-tightly in-tegrated grammar is described in more detail inBriscoe & Carroll (1994).5.
PARS ING THE SUSANNE ANDSEC CORPORAWe have used the integrated grammar to parsethe Susanne corpus and the quite distinct SpokenEnglish Corpus (SEC; Taylor ~ Knowles, 1988), a50K word treebanked corpus of transcribed Britishradio programmes punctuated by the corpus com-pilers.
Both corpora were retagged using the Ac-quilex HMM tagger (Elworthy, 1993, 1994) trainedon text tagged with a slightly modified version ofCLAWS-II labels (Garside et al, 1987).
In con-trast to previous systems taking as input fully-determinate sequences of PoS labels, such as Fid-ditch (Hindle, 1989) and MITFP (de Marcken,1990), for each word the tagger returns multiplelabel hypotheses, and each is thresholded beforebeing passed on to the parser: a given label is re-tained if it is the highest-ranked, or, if the highest-ranked label is assigned a likelihood of less than0.9, if its likelihood is within a factor of 50 of this.We thus attempt o minimise the effect of incor-rect tagging on the parsing component by allow-ing label ambiguities, but control the increase inindeterminacy and concomitant decrease in subse-quent processing efficiency by applying the thresh-olding technique.
On Susanne, retagging allowingonly a single label per word results in a 97.90%label/word assignment accuracy, whereas multi-label tagging with this thresholding scheme resultsin 99.51% accuracy.In an earlier paper (Briscoe & Carroll, 1995)we gave results for a previous version of the gram-mar and parsing system.
We have made a num-ber of significant improvements to the system sincethen, the most fundamental being the use of multi-ple labels for each word.
System accuracy evalua-tion results are also improved since we now outputtrees that conform more closely to the annotationconventions employed in the test treebank.COVERAGE AND AMBIGUITYTo examine the efficiency and coverage of thegrammar we applied it to our retagged versions ofSusanne and SEC.
We used the ANLT chart parser(Carroll, 1993), but modified just to count thenumber of possible parses in the parse forests (Bil-lot ~ Lang, 1989) rather than actually unpackingthem.
We also imposed a per-sentence time-outof 30 seconds CPU time, running in Franz Alle-gro Common Lisp 4.2 on an HP PA-RISC 715/100workstation with 128 Mbytes of physical memory.For both corpora, the majority of sentences94Parse fails1-9 parses10-99 parses100-999 parses1K-9.9K parses10K-99K parses100K+ parsesTime-outsNumber of sentencesMean sentence length (MSL)MSL - failsMSL - time-outsAverage Parse BaseSusanne1476 21.0%1436 20.5%1218 17.4%953 13.6%694 9.9%474 6.8%750 10.7%13 0.2%701420.120.973.61.313SEC809 31.3%477 18.4%378 14.6%276 10.7%225 8.7%154 6.0%264 10.2%4 0.2%271722.629.565.81.300Table 1: Grammar coverage on Susanne and SECanalysed successfully received under 100 parses,although there is a long tail in the distribu-tion.
Monitoring this distribution is helpful duringgrammar development to ensure that coverage isincreasing but the ambiguity rate is not.
A moresuccinct though less intuitive measure of ambigu-ity rate for a given corpus is Briscoe & Carroll's(1995) average parse base (APB), defined as thegeometric mean over all sentences in the corpusof ?/~, where n is the number of words in a sen-tence, and p, the number of parses for that sen-tence.
Thus, given a sentence n words long, theAPB raised to the nth power gives the number ofanalyses that the grammar can be expected to as-sign to a sentence of that length in the corpus.
Ta-ble 1 gives these measures for all of the sentencesin Susanne and in SEC.As the grammar was developed solely with ref-erence to Susanne, coverage of SEC is quite robust.The two corpora differ considerably since the for-mer is drawn from American written text whilstthe latter represents British transcribed spokenmaterial.
The corpora overall contain materialdrawn from widely disparate genres / registers,and are more complex than those used in DARPAATIS tests, and more diverse than those usedin MUCs and probably also the Penn Treebank.Black et al (1993) report a coverage of around95% on computer manuals, as opposed to our cov-erage rate of 70-80% on much more heterogeneousdata and longer sentences.
The APBs for Susanneand SEC of 1.313 and 1.300 respectively indicatethat sentences of average length in each corpuscould be expected to be assigned of the order of238 and 376 analyses (i.e.
1.3132?n and 1.300226).The parser throughput on these tests, for sen-tences successfully analysed, is around 25 wordsper CPU second on an HP PA-RISC 715/100.Sentences of up to 30 tokens (words plus sentence-internal punctuation) are parsed in an average ofunder 1 second each, whilst those around 60 tokenstake on average around 7 seconds.
Nevertheless,the relationship between sentence length and pro-cessing time is fitted well by a quadratic function,supporting the findings of Carroll (1994) that inpractice NL grammars do not evince worst-caseparsing complexity.Grammar  Deve lopment  & Ref inementThe results we report above relate to the latestversion of the tag sequence grammar.
To date, wehave spent about one person-year on grammar de-velopment, with the effort spread fairly evenly overa two-and-a-half-year period.
The various phasesin the development and refinement of the grammarcan be observed in an analysis of the coverage andAPB for Susanne and SEC over this period--seetable 2.
The phases, with dates, were:6 /92-11 /93  Initial development of the grammar.11 /93-7 /94  Substantial increase in coverage onthe development corpus (Susanne), correspond-ing to a drive to increase the general coverageof the grammar by analysing parse failures onactual corpus material.
From a lower initial fig-ure, coverage of SEC (unseen corpus), increasedby a larger factor.7 /94-12 /94  Incremental improvements in cover-age, but at the cost of increasing the ambiguityof the grammar.12 /94-10/95  Improving the accuracy of the sys-tem by trying to ensure that the correct analysiswas in the set returned.Since the coverage on SEC is increasing at thesame time as on Susanne, we can conclude thatthe grammar has not been specifically tuned tothe particular sublanguages or genres representedin the development corpus.
Also, although thealmost-50% initial coverage on the heterogeneous95Susannedate coverage APB 2?111/93 47.8% 6671/94 56.7% 1607/94 75.3% 19212/94 79.0% 21710/95 79.0% 238SECcoverage34.3%45.7%67.1%68.9%68.7%Table 2: Grammar coverage and ambiguity duringdevelopmenttext of Susanne compares well with the state-of-the-art in grammar-based approaches to NL anal-ysis (e.g.
see Taylor el al., 1989; Alshawi el al.,1992), it is clear that the subsequent grammar re-finement phases have led to major improvementsin coverage and reductions in spurious ambiguity.We have experimented with increasing therichness of the lexical feature set by incorporatingsubcategorisation i formation for verbs into thegrammar and lexicon.
We constructed randomlyfrom Susanne a test corpus of 250 in-coverage sen-tences, and in this, for each word tagged as pos-sibly being an open-class verb (i.e.
not a modalor auxiliary) we extracted from the ANLT lexi-con (Carroll & Grover, 1989) all verbal entries forthat word.
We then mapped these entries intoour PoS grammar experimental subcategorisationscheme, in which we distinguished each possiblepattern of complementation allowed by the gram-mar (but not control relationships, specificationof prepositional heads of PP complements etc.
asin the full ANLT representation scheme).
Wethen attempted to parse the test sentences, us-ing the derived verbal entries instead of the orig-inal generic entries which generalised over all thesubcategorisation possibilities.
31 sentences nowfailed to receive a parse, a decrease in coverage of12%.
This is due to the fact that the ANLT lexi-con, although large and comprehensive by currentstandards (Briscoe & Carroll, 1996), neverthelesscontains many errors of omission.PARSE SELECT IONA probabilistic LR parser was trained with the in-tegrated grammar by exploiting the Susanne tree-bank bracketing.
An LR parser (Briscoe & Car-roll, 1993) was applied to unlabelled bracketedsentences from the Susanne treebank, and a newtreebank of 1758 correct and complete analyseswith respect to the integrated grammar was con-structed semi-automatically by manually resolvingthe remaining ambiguities.
250 sentences from thenew treebank, selected randomly, were kept back96for testing 3.
The remainder, together with a fur-ther set of analyses from 2285 treebank sentencesthat were not checked manually, were used totrain a probabilistic version of the LR parser, us-ing Good-Turing smoothing to estimate the prob-ability of unseen transitions in the LALR(1) ta-ble (Briscoe & Carroll, 1993; Carroll, 1993).
Theprobabilistic parser can then return a ranking ofall possible analyses for a sentence, or efficientlyreturn just the n-most probable (Carroll, 1993).The probabilistic parser was tested on the250 sentences held out from the manually-disambiguated treebank (of lengths 3-56 tokens,mean 18.2).
The parser was set up to returnonly the highest-ranked analysis for each sentence.Table 3 shows the results of this test- -with re-spect to the original Susanne bracketings--usingthe Grammar Evaluation Interest Group scheme(GEIG, see e.g.
Harrison et al, 1991) 4.
This com-pares unlabelled bracketings derived from corpustreebanks with those derived from parses for thesame sentences by computing recall, the ratio ofmatched brackets over all brackets in the treebank;precision, the ratio of matched brackets over allbrackets found by the parser; mean crossings, thenumber of times a bracketed sequence output bythe parser overlaps with one from the treebankbut neither is properly contained in the other, av-eraged over all sentences; and zero crossings, thepercentage of sentences for which the analysis re-turned has zero crossings.The table also gives an indication of the bestand worst possible performance of the disambigua-tion component of the system, showing the resultsobtained when parse selection is replaced by a sim-ple random choice, and the results of evaluatingthe analyses in the manually-disambiguated ree-bank against the corresponding original Susannebracketings.
In this latter figure, the mean numberof crossings (0.41) is greater than zero mainly be-cause of incompatibilities between the structuralrepresentations chosen by the grammarian and thecorresponding ones in the treebank.
Precision isless than 100% due to crossings, minor mismatchesand inconsistencies (due to the manual nature ofthe markup process) in tree annotations, and thefact that Susanne often favours a "flat" treatmentof VP constituents, whereas our grammar alwaysmakes an explicit choice between argument- andadjunct-hood.
Thus, perhaps a more informa-tive test of the accuracy of our probabilistic sys-tem would be evaluation against the manually-disambiguated corpus of analyses assigned by thegrammar.
In this, the mean crossing figure drops3The appendix contains a random sample of sen-tences from the test corpus.4We would like to thank Phil Harrison for supplyingthe evaluation software.Zero Mean Recall Precisioncrossings crossingsProbabilistic parser analysesTop-ranked analysis 59.6% 1.03 74.0% 73.0%Random analysis 40.4% 1.84 58.6% 60.0%Manually-disambiguated analyses' Ideal' analysis 80.1% 0.41 85.4% 82.9%Table 3: GEIG evaluation metrics for test set of 250 held-back sentences against Susanne bracketingsto 0.71 and the recall and precision rise to 83-84%,as shown in table 4.Black el al.
(1993:7) use the crossing bracketsmeasure to define a notion of structural consis-tency, where the structural consistency rate for thegrammar is defined as the proportion of sentencesfor which at least one analysis--from the manytypically returned by the grammar--contains ocrossing brackets, and report a rate of around95% for the IBM grammar tested on the com-puter manual corpus.
However, a problem withthe GEIG scheme and with structural consistencyis that both are still weak measures (designedto avoid problems of parser/treebank represen-tational compatibility) which lead to unintuitivenumbers whose significance still depends heavilyon details of the relationship between the repre-sentations compared (e.g.
between structure as-signed by a grammar and that in a treebank).
Oneparticular problem with the crossing bracket mea-sure is that a single attachment mistake mbeddedn levels deep (and perhaps completely innocuous,such as an "aside" delimited by dashes) can leadto n crossings being assigned, whereas incorrectidentification of arguments and adjuncts can gounpunished in some cases.Schabes et al (1993) and Magerman (1995)report results using the GEIG evaluation schemewhich are numerically similar in terms of parse se-lection to those reported here, but achieve 100%coverage.
However, their experiments are notstrictly comparable because they both utilise morehomogeneous and probably simpler corpora.
(Theappendix gives an indication of the diversity ofthe sentences in our corpus).
In addition, Sch-abes et al do not recover tree labelling, whilstMagerman has developed a parser designed to pro-duce identical analyses to those used in the PennTreebank, removing the problem of spurious er-rors due to grammatical incompatibility.
Boththese approaches achieve better coverage by con-structing the grammar fully automatically, but asan inevitable side-effect the range of text phenom-ena that can be parsed becomes limited to thosepresent in the training material, and being able todeal with new ones would entail further substan-tial treebanking efforts.To date, no robust parser has been shownto be practical and useful for some NLP task.However, it seems likely that, say, rule-to-rule se-mantic interpretation will be easier with hand-constructed grammars with an explicit, determi-nate rule-set.
A more meaningful parser compar-ison would require application of different parsersto an identical and extended test suite and utilisa-tion of a more stringent standard evaluation pro-cedure sensitive to node labellings.Training Data Size and AccuracyStatistical HMM-based part-of-speech taggers re-quire of the order of 100K words and upwards oftraining data (Weischedel et al, 1993:363); tag-gers inducing non-probabilistic rules (e.g.
Brill,1994) require similar amounts (Gaizauskas, pc).Our probabilistic disambiguation system currentlymakes no use of lexical frequency information,training only on structural configurations.
Nev-ertheless, the number of parameters in the prob-abilistic model is large: it is the total number ofpossible transitions in an LALR(1) table contain-ing over 150000 actions.
It is therefore interestingto investigate whether the system requires moreor less training data than a tagger.We therefore ran the same experiment asabove, using GEIG to measure the accuracy ofthe system on the 250 held-back sentences, butvarying the amount of training data with whichthe system was provided.
We started at the fullamount (3793 trees), and then successively halvedit by selecting the appropriate number of trees atrandom.
The results obtained are given in figure 1.The results show convincingly that the systemis extremely robust when confronted with limitedamounts of training data: when using a mere onesixty-fourth of the full amount (59 trees), accuracywas degraded by only 10-20%.
However, thereis a large decrease in accuracy with no trainingdata (i.e.
random choice).
Conversely, accuracy isstill improving at 3800 trees, with no sign of over-training, although it appears to be approaching anupper asymptote.
To determine what this might97Zero Mean Recall Precisioncrossings crossingsProbabilistic parser analysesTop-ranked analysis 67.2% 0.71 82.9% 83.9%Table 4: GEIG evaluation metrics for test set of 250 held-back sentences against he manually-disambigatedanalyses2-1.5-100% 1.0-50% 0.5-0% o.o[ \ [ \ ]  Mean crossings \[~\] Recall \[~\] Precision\ [~ Zero crossings\[\] ................................... \[\] ...................................... El .................................. \[\] .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
\[\] ......................................... \[\] ......................................... \[\] ............................... .
.~I I I i I I fAll 1/2 1/4 1/8 1/16 1/32 1/64 NoneFraction of 3793 training sentences usedFigure 1: GEIG metrics for held-back sentences, training on varying amounts of databe, we ran the system on a set of 250 sentences ran-domly extracted from the training corpus.
On thisset, the system achieves a zero crossings rate of60.0%, mean crossings 0.88, and recall and preci-sion of 77.0% and 75.2% respectively, with respectto the original Susanne bracketings.
Although thisis a different set of sentences, it is likely that theupper asymptote for accuracy for the test corpuslies in this region.
Given that accuracy is increas-ing only slowly and is relatively close to the asymp-tote it is therefore unlikely that it would be worthinvesting effort in increasing the size of the train-ing corpus at this stage in the development of thesystem.6.
CONCLUSIONSIn this paper we have outlined an approach to ro-bust domain-independent parsing, in which sub-categorisation constraints play no part, resultingin coverage that greatly improves upon more con-ventional grammar-based approaches to NL textanalysis.
We described an implemented system,and evaluated its performance along several dif-ferent dimensions.
We assessed its coverage andthat of previous versions on a development cor-pus and an unseen corpus, and demonstrated thatthe grammar efinement we have carried out hasled to substantial improvements in coverage andreductions in spurious ambiguity.
We also evalu-ated the accuracy of parse selection with respectto treebank analyses, and, by varying the amountof training material, we showed that it requirescomparatively ittle data to achieve a good levelof accuracy.We have made good progress in increasinggrammar coverage, though we have now reacheda point of diminishing returns.
Further significantimprovements in this area would require corpus-specific additions and tuning whose benefit wouldnot necessarily carry over to other corpora.
In theapplication we are currently using the system for- -automatic extraction of subcategorisation frames,and more generally argument structure, from largeamounts of text (Briscoe ~ Carroll, 1996)--we donot need full coverage; 70-80% appears to be suf-ficient.
However, further improvements in cover-age will require some automated approach to ruleinduction driven by parse failure.
Since our eval-uations indicate that our system achieves a good98level of accuracy with little treebank data, andthat 67-75% coverage was achieved for Englishquite early in the grammar refinement effort, port-ing the current system to other languages shouldbe possible with small-to-medium-sized treebanks(around 20K words) and feasible manual effort(of the order of 12 person-months for grammar-writing and treebanking).
This may yield a sys-tem accurate nough for some types of application,given that the system is not restricted to return-ing the single highest ranked analysis but can re-turn the n-highest ranked for further application-specific selection.Although we report promising results, parseselection that is sufficiently accurate for manypractical applications will require a more lexi-calised system.
Magerman's (1995) parser is anextension of the history-based parsing approachdeveloped at IBM (Black et al, 1993) in whichrules are conditioned on lexical and other (es-sentially arbitrary) information available in theparse history.
In future work, we intend to ex-plore a more restricted and semantically-drivenversion of this approach in which, firstly, probabili-ties are associated with different subcategorisationpossibilities, and secondly, alternative predicate-argument structures derived from the grammarare ranked probabilistically.
However, the mas-sively increased coverage obtained here by relaxingsubcategorisation constraints underlines the needto acquire accurate and complete subcategorisa-tion frames in a corpus-driven fashion, before suchconstraints can be exploited robustly and effec-tively with free text.REFERENCESAlshawi, H., Carter, D., Crouch, R., Pulman, S.,Rayner, M., ~ Smith, A.
1992.
CLARE: a contex-tual reasoning and cooperative response frameworkfor the Core Language Engine.
SRI International,Cambridge, UK.Billot, S. & Lang, B.
1989.
The structure of sharedforests in ambiguous parsing.
In Proceedings ofthe 27lh Meeting of Association for ComputationalLinguistics, Vancouver, Canada.
143-151.Black, E., Garside, R. & Leech, G.
(eds.)
1993.Statistically-driven computer grammars of En-glish: the IBM~ Lancaster approach.
Amsterdam,The Netherlands: Rodopi.Brill, E. 1994.
Some advances in transformation-based part of speech tagging.
In Proceedings of the12th National Conference on Artificial Intelligence(AAAI-94), Seattle, WA.Briscoe, E. 1994.
Prospects for practical parsing ofunrestricted text: robust statistical parsing tech-niques.
In Oostdijk, N L: de Haan, P. eds.
Corpus-based Research into Language.
Rodopi, Amster-dam: 97-120.Briscoe, E. & Carroll, J.
1993.
Generalised prob-abilistic LR parsing for unification-based gram-mars.
Computational Linguistics 19.1: 25-60.Briscoe, E. ,~ Carroll, J.
1994.
Parsing ('with)punctuation etc.
Rank Xerox Research Centre,Grenoble, MLTT-TR-007.Briscoe, E. ,~ Carroll, J.
1995.
Developing andevaluating a probabilistic LR parser of part-of-speech and punctuation labels.
In Proceedings ofthe 4th ACL/SIGPARSE International Workshopon Parsing Technologies, Prague, Czech Republic.48-58.Briscoe, E.  Carroll, J.
1996.
Automatic extrac-tion of subcalegorization from corpora.
Under re-view.Briscoe, E., Grovel', C., Boguraev, B.
& Carroll, J.1987.
A formalism and environment for the devel-opment of a large grammar of English.
In Proceed-ings of the lOth International Joint Conference onArtificial Intelligence, Milan, Italy.
703-708.Carroll, J.
1993.
Practical unification-based pars-ing of natural language.
Cambridge University,Computer Laboratory, TR-314.Carroll, J.
1994.
Relating complexity to prac-tical performance in parsing with wide-coverageunification grammars.
In Proceedings of the 32ndMeeting of Association for Computational Lin-guistics, Las Cruces, NM.
287-294.Carroll, J.
~: Grover, C. 1989.
The derivationof a large computational lexicon for English fromLDOCE.
In Boguraev, B.
&: Briscoe, E. eds.
Com-putational Lexicography for Natural Language Pro-cessing.
Longman, London: 117-134.Church, K. 1988.
A stochastic parts program andnoun phrase parser for unrestricted text.
In Pro-ceedings of the 2nd Conference on Applied NaturalLanguage Processing, Austin, Texas.
136-143.Elworthy, D. 1993.
Part-of-speech lagging andphrasal tagging.
Acquilex-II Working Paper 10,Cambridge University Computer Laboratory (canbe obtained from cide@cup.cam.ac.uk).Elworthy, D. 1994.
Does Baum:Welch re-estimation help taggers?.
In Proceedings of the 4thConference on Applied NLP, Stuttgart, Germany.Garside, R., Leech, G. & Sampson, G. 1987.
Com-putational analysis of English.
Harlow, UK: Long-mai l .Grishman, R., Macleod, C. & Meyers, A.
1994.Comlex syntax: building a computational lexicon.In Proceedings of the International Conference onComputational Linguistics, COLING-94, Kyoto,Japan.
268-272.99Harrison, P., Abney, S., Black, E., Flickenger,D., Gdaniec, C., Grishman, R., Hindle, D., In-gria, B., Marcus, M., Santorini, B.
& Strza-lkowski, T. 1991.
Evaluating syntax performanceof parser/grammars of English.
In Proceedingsof the Workshop on Evaluating Natural LanguageProcessing Systems, ACL.Hindle, D. 1989.
Acquiring disambiguation rulesfrom text.
In Proceedings of the 27th Annual Meet-ing of the Association for Computational Linguis-tics, Vancouver, Canada.
118-25.Jackendoff, R. 1977.
X-bar syntax.
Cambridge,MA: MIT Press.Jones, B.
1994.
Can punctuation help parsing?.In Proceedings of the International Conference onComputational Linguistics, COLING-94, Kyoto,Japan.Magerman, D. 1995.
Statistical decision-tree mod-els for parsing.
In Proceedings of the 33rd AnnualMeeting of the Association for Computational Lin-guistics, Boston, MA.de Marcken, C. 1990.
Parsing the LOB corpus.In Proceedings of the 28th Annual Meeting of theAssociation for Computational Linguistics, NewYork.
243-251.Nunberg, G. 1990.
The linguistics of punctuation.CSLI Lecture Notes 18, Stanford, CA.Pereira, F. & Warren, D. 1980.
Definite clausegrammars for language analysis - a survey of theformalism and a comparison with augmented tran-sition networks.
Artificial Intelligence 13.3: 231-278.Sampson, G. 1995.
English for the computer.
Ox-ford, UK: Oxford University Press.Schabes, Y., Roth, M. & Osborne, R. 1993.
Pars-ing of the Wall Street Journal with the inside-outside algorithm.
In Proceedings of the Meetingof European Association for Computational Lin-guistics, Utrecht, The Netherlands.Taylor, L., Grover, C. & Briscoe, E. 1989.
Thesyntactic regularity of English noun phrases.
InProceedings of the 4th European Meeting of the As-sociation for Computational Linguistics, Manch-ester, UK.
256-263.Taylor, L. &: Knowles, G. 1988.
Manual of in-formation to accompany the SEC corpus: themachine-readable corpus of spoken English.
Uni-versity of Lancaster, UK, Ms.Weischedel, R., Meteer, M., Schwartz, R.,Ramshaw, L. & Palmucci J.
1993.
Coping withambiguity and unknown words through probabilis-tic models.
Computational Linguistics 19(2): 359-382.100APPENDIXBelow is a random sample of the 250-sentence t stset.
The test set comprises the Brown genre cat-egories: "press reportage"; "belles lettres, biog-raphy, memoirs"; and "learned (mainly scientificand technical) writing".
"Yes, your honour", replied Bellows.This is another of the modifications of policyon Laos that the Kennedy administration hasfelt compelled to make.On Monday, the Hughes concern was formallydeclared bankrupt after its directors indicatedthey could not draw up a plan for reorganiza-tion.Ierulli will replace Desmond D. Connall whohas been called to active military service butis expected back on the job by March 31.Place kicking is largely a matter of timing,Moritz declared.Ritchie walked up to him at the magazinestand.Hector Lopez, subbing for Berra, smashed a3-run homer off Bill Henry during another 5-run explosion in the fourth.That's how he first won the Masters in 1958.Cooperman and Teller are accused of selling$4,700 worth of heroin to a convicted nar-cotics peddler, Otis Sears, 45, of 6934 Indi-ana av.However, the system is designed, ingeniouslyand hopefully, so that no one man could ini-tiate a thermonuclear war.He bent down, a black cranelike figure, andput his mouth to the ground.Those who actually get there find that it isn'tspooky at all but as brilliant as a tile in sun-light.Others look to more objective devices of or-der.What additional roles has the scientific un-derstanding of the 19th and 20th centuriesplayed?If we look at recent art we find it preoccupiedwith form.Hence the beatniks sustain themselves onmarijuana, jazz, free swinging poetry, ex-hausting themselves in orgies of sex; some ofthem are driven over the borderline of sanityand lose contact with reality.Heidenstam could never be satisfied by sur-face.Individual human strength is needed to pitagainst an inhuman condition.
