Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358?363,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsCitation Resolution: A method for evaluating context-based citationrecommendation systemsDaniel DumaUniversity of EdinburghD.C.Duma@sms.ed.ac.ukEwan KleinUniversity of Edinburghewan@staffmail.ed.ac.ukAbstractWouldn?t it be helpful if your text edi-tor automatically suggested papers that arerelevant to your research?
Wouldn?t itbe even better if those suggestions werecontextually relevant?
In this paper wename a system that would accomplish thisa context-based citation recommendation(CBCR) system.
We specifically presentCitation Resolution, a method for the eval-uation of CBCR systems which exclusivelyuses readily-available scientific articles.Exploiting the human judgements that arealready implicit in available resources, weavoid purpose-specific annotation.
We ap-ply this evaluation to three sets of methodsfor representing a document, based on a)the contents of the document, b) the sur-rounding contexts of citations to the doc-ument found in other documents, and c) amixture of the two.1 IntroductionImagine that you were working on a draft paperwhich contained a sentence like the following:1A variety of coherence theories havebeen developed over the years ... andtheir principles have found applicationin many symbolic text generation sys-tems (e.g.
[CITATION HERE])Wouldn?t it be helpful if your editor automat-ically suggested some references that you couldcite here?
This is what a citation recommenda-tion system ought to do.
If the system is able totake into account the context in which the citationoccurs ?
for example, that papers relevant to ourexample above are not only about text generation1Adapted from the introduction to Barzilay and Lapata(2008)systems, but specifically mention applying coher-ence theories ?
then this would be much moreinformative.
So we define a context-based citationrecommendation (CBCR) system as one that assiststhe author of a draft document by suggesting otherdocuments with content that is relevant to a partic-ular context in the draft.Our longer term research goal is to provide sug-gestions that satisfy the requirements of specificexpository or rhetorical tasks, e.g.
provide supportfor a particular argument, acknowledge previouswork that uses the same methodology, or exem-plify work that would benefit from the outcomesof the author?s work.
However, our current pa-per has more modest aims: we present initial re-sults using existing IR-based approaches and weintroduce an evaluation method and metric.
CBCRsystems are not yet widely available, but a num-ber of experiments have been carried out that maypave the way for their popularisation, e.g.
He et al(2010), Sch?afer and Kasterka (2010) and He et al(2012).
It is within this early wave of experimentsthat our work is framed.A main problem we face is that evaluating theperformance of these systems ultimately requireshuman judgement.
This can be captured as a set ofrelevance judgements for candidate citations overa corpus of documents, which is an arduous ef-fort that requires considerable manual input andvery careful preparation.
In designing a context-based citation recommendation system, we wouldideally like to minimise these costs.Fortunately there is already an abundance ofdata that meets our requirements: every scientificpaper contains human ?judgements?
in the formof citations to other papers which are contextuallyappropriate: that is, relevant to specific passagesof the document and aligned with its argumenta-tive structure.
Citation Resolution is a method forevaluating CBCR systems that is exclusively basedon this source of human judgements.358Let?s define some terminology.
In the follow-ing passage, the strings ?Scott and de Souza, 1990?and ?Kibble and Power, 2004?
are both citation to-kens:A variety of coherence theories havebeen developed over the years ... andtheir principles have found applicationin many symbolic text generation sys-tems (e.g.
Scott and de Souza, 1990;Kibble and Power, 2004)Note that a citation token can use any standard for-mat.
Furthermore?
a citation context is the context in which a ci-tation token occurs, with no limit as to repre-sentation of this context, length or processinginvolved;?
a collection-internal reference is a referencein the bibliography of the source documentthat matches a document in a given corpus;?
a resolvable citation is an in-text citation to-ken which resolves to a collection-internalreference.2 Related workWhile the existing work in this specific area isfar from extensive, previous experiments in evalu-ating context-based citation recommendation sys-tems have used one of three approaches.
First,evaluation can be carried out through user studies,which is costly because it cannot be reused (e.g.Chandrasekaran et al (2008)).Second, a set of relevance judgements can becreated for repeated testing.
Ritchie (2009) detailsthe building of a large set of relevance judgementsin order to evaluate an experimental document re-trieval system.
The judgements were mainly pro-vided by the authors of papers submitted to a lo-cally organised conference, for over 140 queries,each of them being the main research questionof one paper.
This is a standard approach in IR,known as building a test collection (Sanderson,2010), which the author herself notes was an ar-duous and time-consuming task.Third, as we outlined above, existing citationsbetween papers can be exploited as a source ofhuman judgements.
The most relevant previouswork on this is He et al (2010), who built an ex-perimental CBCR system using the whole index ofCiteSeerX as a test collection (over 450,000 docu-ments).
They avoided direct human evaluation andinstead used three relevance metrics:?
Recall, the presence of the original referencein the list of suggestions generated by the sys-tem;?
Co-cited probability, a ratio between, on theone hand, the number of papers citing boththe original reference and a recommendedone, and on the other hand, the number of pa-pers citing either of them; and?
Normalized Discounted Cumulative Gain, ameasure based on the rank of the original ref-erence in the list of suggested references, itsscore decreasing logarithmically.However, these metrics fail to adequately recog-nise that the particular reference used by an authore.g.
in support of an argument or as exemplifica-tion of an approach, may not be the most appro-priate that could be found in the whole collection.This does not just amount to a difference of opin-ion between different authors; it is possible thatwithin a large enough collection there exists a pa-per which the original author herself would con-sider to be more appropriate by any criteria (per-suasive power, discoverability or the publication,etc.)
than the one actually cited in the paper.
Also,given that recommending the original citation usedby the author in first position is our key criterion, ametric with smooth discounting like NDCG is toolenient for our purposes.We have then chosen top-1 accuracy as our met-ric, where every time the original citation is first onthe list of suggestions, it receives a score of 1, and0 otherwise, and these scores are averaged overall resolved citations in the document collection.This metric is intuitive in measuring the efficiencyof the system at this task, as it is immediately in-terpretable as a percentage of success.While previous experiments in CBCR, like theones we have just presented, have treated the taskas an Information Retrieval problem, our ultimatepurpose is different and travels beyond IR intoQuestion Answering.
We want to ultimately beable to assess the reason a document was cited inthe context of the argumentation structure of thedocument, following previous work on the auto-matic classification of citation function by Teufelet al (2006), Liakata et al (2012) and Sch?afer andKasterka (2010).
We expect this will allow us toidentify claims made in a draft paper and matchthem with related claims made in other papers forsupport or contrast, and so offer answers in theform of relevant passages extracted from the sug-359gested documents.It is frequently observed that the reasons for cit-ing a paper go beyond its contribution to the fieldand its relevance to the research being reported(Hyland, 2009).
There is a large body of researchon the motivations behind citing documents (Mac-Roberts and MacRoberts, 1996), and it is likelythat this will come to play a part in our research inthe future.In this paper, however, we present our initialresults which compare three different sets of IR-based approaches to generating the document rep-resentation for a CBCR system.
One is based onthe contents of the document itself, one is basedon the existing contexts of citations of this paperin other documents, and the third is a mixture ofthe two.3 The task: Citation ResolutionIn this section we present the evaluation method inmore abstract terms; for the implementation usedin this paper, please see Sections 4 and 5.
Thecore criterion of this task is to use only the humanjudgements that we have clearest evidence for.
Letd be a document and R the collection of all doc-uments that are referenced in d. We believe it isreasonable to assume that the author of documentd knows enough about the contents of each doc-ument Rito choose the most appropriate citationfrom the collection R for every citation context inthe document.This captures a very strong relevance judge-ment about the relation between a particular cita-tion context in the document and a particular citedreference document.
We use these judgements forevaluation: our task is to match every citation con-text in the document (i.e.
the surrounding contextof a citation token) with the right reference fromthe list of references cited by that paper.This task differs somewhat from standard Infor-mation Retrieval, in that we are not trying to re-trieve a document from a larger collection outsidethe source document, but trying to resolve the cor-rect reference for a given citation context from anexisting list of documents, that is, from the bibli-ography that has been manually curated by the au-thors.
Our document collection used for retrievalis further composed of only the references of thatdocument that we can access.The algorithm for the task is presented in Figure1.
For any given test document (2), we first extractall the citation tokens found in the text that cor-respond to a collection-internal reference (a).
Wethen create a document representation of the refer-enced document (currently a Vector Space Model,but liable to change).
This representation can bebased on any information found in the documentcollection, excluding the document d itself: e.g.the text of the referenced document and the text ofdocuments that cite it.For each citation token we then extract its con-text (b.i), which becomes the query in IR terms.One way of doing this that we present here is toselect a list of word tokens around the citation.
Wethen attempt to resolve the citation by computinga score for the match between each reference rep-resentation and the citation context (b.ii).
We rankall collection-internal references by this score indecreasing order, aiming for the original referenceto be in the first position (b.iii).In the case where multiple citations share thesame context, that is, they are made in di-rect succession (e.g.
?...compared with previousapproaches (Author (2005), Author and Author(2007))?
), the first n elements of the list of sug-gested documents all count as the first element.That is, if any of the references in a multiple ci-tation of n elements appears in the first n posi-tions of the list of suggestions, it counts as a suc-cessful resolution and receives a score of 1.
Thefinal score is averaged over all citation contextsprocessed.The set of experiments we present here applythis evaluation to test a number of IR techniqueswhich we detail in the next section.1.
Given document collection D2.
For every test document d(a) For every reference r in its bibliography Ri.
If r is in document collection Dii.
Add all inline citations Crin d to list C(b) For each citation c in Ci.
Extract context ctxcof cii.
Choose which document r in R best matchesctxciii.
Measure accuracyFigure 1: Algorithm for citation resolution.4 ExperimentsOur test corpus consists of approx.
9000 papersfrom the ACL Anthology2converted from PDF to2http://http://aclweb.org/anthology/360XML format.
This corpus, the rationale behind itsselection and the process used to convert the filesis described in depth in Ritchie et al (2006).
Thisis an ideal corpus for these tests for a large numberof reasons, but these are key for us: all the papersare freely available, the ratio of collection-internalreferences for each paper is high (the authors mea-sure it at 0.33) and it is a familiar domain for us.For our tests, we selected the documents ofthis corpus with at least 8 collection-internal refer-ences.
This yielded a total of 278 test documentsand a total of 5446 resolvable citations.We substitute all citations in the text with ci-tation token placeholders and extract the citationcontext for each using a simple window of up tow words left and w words right around the place-holder.
This produces a list of word tokens that isequivalent to a query in IR.This is a frequently employed technique (He etal., 2010), although it is often observed that thismay be too simplistic a method (Ritchie, 2009).Other methods have been tried, e.g.
full sentenceextraction (He et al, 2012) and comparing thesemethods is something we plan to incorporate infuture work.We then make the document?s collection-internal references our test collection D and use anumber of methods for generating the documentrepresentation.
We use the well-known VectorSpace Model and a standard implementation of tf-idf and cosine similarity as implemented by thescikit-learn Python framework3.
At present, weare applying no cut-off and just rank all of the doc-ument?s collection-internal references for each ci-tation context, aiming to rank the correct one inthe first positions in the list.We tested three different approaches to gener-ating a document?s VSM representation: internalrepresentations, which are based on the contentsof the document, external representations, whichare built using a document?s incoming link cita-tion contexts (following Ritchie (2009) and He etal.
(2010)) and mixed representations, which arean attempt to combine the two.?
The internal representations of the documentswere generated using three different methods:title plus abstract, full text and passage.
Pas-sage consists in splitting the document intohalf-overlapping passages of a fixed length ofk words and choosing for each document the3http://scikit-learn.orgpassage with the maximum cosine similarityscore with the query.
We present the resultsof using 250, 300 and 350 as values for k.?
The external representations (inlink context)are based on extracting the context around ci-tation tokens to the document from other doc-uments in the collection, excluding the set oftest papers.
This is the same as using the an-chor text of a hyperlink to improve results inweb-based IR (see Davison (2000) for exten-sive analyis).
This context is extracted in thesame way as the query: as a window, or listof w tokens surrounding the citation left andright.
We present our best results, using sym-metrical and asymmetrical windows of w =[(5, 5), (10, 10), (10, 5), (20, 20), (30, 30)].?
We build the mixed representations by simplyconcatenating the internal and external bags-of-words that represent the documents, fromwhich we then build the VSM representa-tion.
For this, we combine different windowsizes for the inlink context with: full text, ti-tle abstract and passage350.5 Results and discussionTable 1 presents a selection of the most relevantresults, where the best result and document rep-resentation method of each type is highlighted.We present results for the most relevant parametervalues, producing the highest scores of all thosetested.From a close look at internal methods, we cansee that the passage method with k = 400 beatsboth full text and title abstract, suggesting that amore elaborate way of building a document repre-sentation should improve results.
This is consis-tent with previous findings: Gay et al (2005) hadalready reported that using selected sections pluscaptions of figures and title and abstract to buildthe internal document representation improves theresults of their indexing task by 7.4% over justusing title and abstract.
Similarly, Jimeno-Yepeset al (2013) showed that automatically generatedsummaries lead to similar recall and better index-ing precision than full-text articles for a keyword-based indexing task.However, it is immediately clear that purely ex-ternal methods obtain higher scores than internalones.
The best score of 0.413 is obtained by theinlink context method with a window of 10 tokensleft, 5 right, combined with the similarly-sized ex-361Method window5 5 window10 10 window10 5 window20 20 window30 30Internal methodsfull text 0.318 0.340 0.337 0.369 0.370title abstract 0.296 0.312 0.312 0.322 0.311passage250 0.343 0.367 0.359 0.388 0.382passage350 0.346 0.371 0.364 0.388 0.381passage400 0.348 0.371 0.362 0.391 0.380External methodsinlink context10 0.391 0.406 0.405 0.395 0.387inlink context20 0.386 0.406 0.413 0.412 0.402inlink context30 0.380 0.403 0.400 0.411 0.404Mixed methodsinlink context 20 full text 0.367 0.407 0.399 0.431 0.425inlink context 20 title abstract 0.419 0.447 0.441 0.453 0.437inlink context 20 passage250 0.420 0.458 0.451 0.469 0.451inlink context 10 passage350 0.435 0.465 0.459 0.464 0.450inlink context 20 passage350 0.426 0.464 0.456 0.469 0.456Table 1: Accuracy for each document representation method (rows) and context window size (columns).traction method for the query (window10 10).
Wefind it remarkable that inlink context is superior tointernal methods, beating the best (passage400) by0.02 absolute accuracy points.
Whether this is be-cause the descriptions of these papers in the con-texts of incoming link citations capture the essenceor key relevance of the paper, or whether this ef-fect is due to authors reusing their work or to thesedescriptions originating in a seed paper and be-ing then propagated through the literature, remaininteresting research questions that we intend totackle in future work.The key finding from our experiments is how-ever that a mixture of internal and externalmethods beats both individually.
The highestscore is 0.469, achieved by a combination of in-link context 20 and the passage method, for a win-dow of w = 20, with a tie between using 250 and350 as values for k (passage size).
The small dif-ference in score between parameter values is per-haps not as relevant as the finding that, taken to-gether, mixed methods consistently beat both ex-ternal and internal methods.These results also show that the task is far fromsolved, with the highest accuracy achieved beingjust under 47%.
There is clear room for improve-ment, which we believe could firstly come from amore targeted extraction of text, both for generat-ing the document representations and for extract-ing the citation contexts.Our ultimate goal is matching claims and com-paring methods, which would likely benefit froman analysis of the full contents of the documentand not just previous citations of it, so in futurework we also intend to use the context from thesuccessful external results as training data for asummarisation stage.6 Conclusion and future workIn this paper we have presented Citation Reso-lution: an evaluation method for context-basedcitation recommendation (CBCR) systems.
Ourmethod exploits the implicit human relevancejudgements found in existing scientific articles andso does not require purpose-specific human anno-tation.We have employed Citation Resolution to testthree approaches to building a document repre-sentation for a CBCR system: internal (based onthe contents of the document), external (based onthe surrounding contexts to citations to that doc-ument) and mixed (a mixture of the two).
Ourevaluation shows that: 1) using chunks of a doc-ument (passages) as its representation yields bet-ter results that using its full text, 2) external meth-ods obtain higher scores than internal ones, and 3)mixed methods yield better results than either inisolation.We intend to investigate more sophisticatedways of document representation and of extract-ing a citation?s context.
Our ultimate goal is notjust to suggest to the author documents that are?relevant?
to a specific chunk of the paper (sen-tence, paragraph, etc.
), but to do so with attentionto rhetorical structure and thus to citation function.We also aim to apply our evaluation to other docu-ment collections in different scientific domains inorder to test to what degree these results can begeneralized.362ReferencesRegina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Compu-tational Linguistics, 34(1):1?34.Kannan Chandrasekaran, Susan Gauch, PraveenLakkaraju, and Hiep Phuc Luong.
2008.
Concept-based document recommendations for citeseer au-thors.
In Adaptive Hypermedia and Adaptive Web-Based Systems, pages 83?92.
Springer.Brian D Davison.
2000.
Topical locality in the web.
InProceedings of the 23rd annual international ACMSIGIR conference on Research and development ininformation retrieval, pages 272?279.
ACM.Clifford W Gay, Mehmet Kayaalp, and Alan R Aron-son.
2005.
Semi-automatic indexing of full textbiomedical articles.
In AMIA Annual SymposiumProceedings, volume 2005, page 271.
AmericanMedical Informatics Association.Qi He, Jian Pei, Daniel Kifer, Prasenjit Mitra, and LeeGiles.
2010.
Context-aware citation recommenda-tion.
In Proceedings of the 19th international con-ference on World wide web, pages 421?430.
ACM.Jing He, Jian-Yun Nie, Yang Lu, and Wayne Xin Zhao.2012.
Position-aligned translation model for cita-tion recommendation.
In String Processing and In-formation Retrieval, pages 251?263.
Springer.Ken Hyland.
2009.
Academic discourse: English in aglobal context.
Bloomsbury Publishing.Antonio J Jimeno-Yepes, Laura Plaza, James G Mork,Alan R Aronson, and Alberto D??az.
2013.
Meshindexing based on automatically generated sum-maries.
BMC bioinformatics, 14(1):208.Maria Liakata, Shyamasree Saha, Simon Dobnik,Colin Batchelor, and Dietrich Rebholz-Schuhmann.2012.
Automatic recognition of conceptualizationzones in scientific articles and two life science ap-plications.
Bioinformatics, 28(7):991?1000.Michael H MacRoberts and Barbara R MacRoberts.1996.
Problems of citation analysis.
Scientometrics,36(3):435?444.Anna Ritchie, Simone Teufel, and Stephen Robertson.2006.
Creating a test collection for citation-based irexperiments.
In Proceedings of the main conferenceon Human Language Technology Conference of theNorth American Chapter of the Association of Com-putational Linguistics, pages 391?398.
Associationfor Computational Linguistics.Anna Ritchie.
2009.
Citation context analysis for in-formation retrieval.
Technical report, University ofCambridge Computer Laboratory.Mark Sanderson.
2010.
Test collection based evalua-tion of information retrieval systems.
Now Publish-ers Inc.Ulrich Sch?afer and Uwe Kasterka.
2010.
Scientific au-thoring support: A tool to navigate in typed citationgraphs.
In Proceedings of the NAACL HLT 2010workshop on computational linguistics and writing:Writing processes and authoring aids, pages 7?14.Association for Computational Linguistics.Simone Teufel, Advaith Siddharthan, and Dan Tidhar.2006.
Automatic classification of citation function.In Proceedings of the 2006 Conference on Empiri-cal Methods in Natural Language Processing, pages103?110.
Association for Computational Linguis-tics.363
