Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 649?652,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsEnsemble Models for Dependency Parsing:Cheap and Good?Mihai Surdeanu and Christopher D. ManningComputer Science DepartmentStanford University, Stanford, CA 94305{mihais,manning}@stanford.eduAbstractPrevious work on dependency parsing usedvarious kinds of combination models but asystematic analysis and comparison of theseapproaches is lacking.
In this paper we imple-mented such a study for English dependencyparsing and find several non-obvious facts: (a)the diversity of base parsers is more importantthan complex models for learning (e.g., stack-ing, supervised meta-classification), (b) ap-proximate, linear-time re-parsing algorithmsguarantee well-formed dependency trees with-out significant performance loss, and (c) thesimplest scoring model for re-parsing (un-weighted voting) performs essentially as wellas other more complex models.
This studyproves that fast and accurate ensemble parserscan be built with minimal effort.1 IntroductionSeveral ensemble models have been proposed forthe parsing of syntactic dependencies.
These ap-proaches can generally be classified in two cate-gories: models that integrate base parsers at learn-ing time, e.g., using stacking (Nivre and McDon-ald, 2008; Attardi and Dell?Orletta, 2009), and ap-proaches that combine independently-trained mod-els only at parsing time (Sagae and Lavie, 2006; Hallet al, 2007; Attardi and Dell?Orletta, 2009).
In thelatter case, the correctness of the final dependencytree is ensured by: (a) selecting entire trees proposedby the base parsers (Henderson and Brill, 1999); or(b) re-parsing the pool of dependencies proposed bythe base models (Sagae and Lavie, 2006).
The lat-ter approach was shown to perform better for con-stituent parsing (Henderson and Brill, 1999).While all these models achieved good perfor-mance, the previous work has left several questionsDevel In domain Out of domainLAS LAS UAS LAS UASMST 85.36 87.07 89.95 80.48 86.08Malt?AE 84.24 85.96 88.64 78.74 84.18Malt?CN 83.75 85.61 88.14 78.55 83.68Malt?AS 83.74 85.36 88.06 77.23 82.39Malt?AS 82.43 83.90 86.70 76.69 82.57Malt?CN 81.75 83.53 86.17 77.29 83.02Malt?AE 80.76 82.51 85.35 76.18 82.02Table 1: Labeled attachment scores (LAS) and unlabeled at-tachment scores (UAS) for the base models.
The parsers arelisted in descending order of LAS in the development partition.unanswered.
Here we answer the following ques-tions, in the context of English dependency parsing:1.
When combining models at parsing time, whatis the best scoring model for candidate depen-dencies during re-parsing?
Can a meta classi-fier improve over unsupervised voting?2.
Are (potentially-expensive) re-parsing strate-gies justified for English?
What percentage oftrees are not well-formed if one switches to alight word-by-word voting scheme?3.
How important is the integration of base parsersat learning time?4.
How do ensemble models compare againststate-of-the-art supervised parsers?2 SetupIn our experiments we used the syntactic dependen-cies from the CoNLL 2008 shared task corpus (Sur-deanu et al, 2008).We used seven base parsing models in this paper:six are variants of the Malt parser1 and the seventhis the projective version of MSTParser that uses onlyfirst-order features2 (or MST for short).
The six Malt1http://maltparser.org/2http://sourceforge.net/projects/mstparser/649Unweighted Weighted by Weighted by Weighted by Weighted byPOS of modifier label of dependency dependency length sentence length# of parsers LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS3 86.03 89.44 86.02 89.43 85.53 88.97 85.85 89.23 86.03 89.454 86.79 90.14 86.68 90.07 86.38 89.78 86.46 89.79 86.84 90.185 86.98 90.33 86.95 90.30 86.60 90.06 86.87 90.22 86.86 90.226 87.14 90.51 87.17 90.50 86.74 90.22 86.91 90.23 87.04 90.377 86.81 90.21 86.82 90.21 86.50 90.01 86.71 90.08 86.80 90.19Table 2: Scores of unsupervised combination models using different voting strategies.
The combined trees are assembled using aword-by-word voting scheme.parser variants are built by varying the parsing algo-rithm (we used three parsing models: Nivre?s arc-eager (AE), Nivre?s arc-standard (AS), and Coving-ton?s non-projective model (CN)), and the parsingdirection (left to right (?)
or right to left (?
)), sim-ilar to (Hall et al, 2007).
The parameters of the Maltmodels were set to the values reported in (Hall etal., 2007).
The MST parser was used with the de-fault configuration.
Table 1 shows the performanceof these models in the development and test parti-tions.3 Experiments3.1 On scoring models for parser combinationThe most common approach for combiningindependently-trained models at parsing time is toassign each candidate dependency a score basedon the number of votes it received from the baseparsers.
Considering that parsers specialize indifferent phenomena, these votes can be weightedby different criteria.
To understand the importanceof such weighting strategies we compare severalvoting approaches in Table 2: in the ?unweighted?strategy all votes have the same weight; in all otherstrategies each vote is assigned a value equal tothe accuracy of the given parser in the particularinstance of the context considered, e.g., in the?weighted by POS of modifier?
model we use theaccuracies of the base models for each possiblepart-of-speech (POS) tag of a modifier token.
Inthe table we show results as more base parsers areadded to the ensemble (we add parsers in the ordergiven by Table 1).
The results in Table 2 indicatethat weighting strategies do not have an importantcontribution to overall performance.
The onlyapproach that outperforms the LAS score of theunweighted voting model is the model that weighsparsers by their accuracy for a given modifier POStag, but the improvement is marginal.
On the otherPOS(m) POS(m) ?
POS(h) length(s)MST 38 56 26Malt?AE 0 6 6Malt?CN 0 14 7Malt?AS 0 61 0Malt?AS 0 0 3Malt?CN 0 9 0Malt?AE 0 0 0Table 3: Total number of minority dependencies with precisionlarger than 50%, for different base parsers and most represen-tative features (m - modifier, h - head, s - sentence).
Theseare counts of tokens, computed in the development corpus of33,368 dependencies.hand, the number of base parsers in the ensemblepool is crucial: performance generally continues toimprove as more base parsers are considered.
Thebest ensemble uses 6 out of the 7 base parsers.3It is often argued that the best way to re-scorecandidate dependencies is not through voting butrather through a meta-classifier that selects candi-date dependencies based on their likelihood of be-longing to the correct tree.
Unlike voting, a meta-classifier can combine evidence from multiple con-texts (such as the ones listed in Table 2).
However,in our experiments such a meta-classifier4 did notoffer any gains over the much simpler unweightedvoting strategy.
We explain these results as follows:the meta-classifier can potentially help only when itproposes dependencies that disagree with the major-ity vote.
We call such dependencies minority depen-dencies.5 For a given parser and context instance(e.g., a modifier POS), we define precision of mi-nority dependencies as the ratio of minority depen-dencies in this group that are correct.
Obviously, a3We drew similar conclusions when we replaced voting withthe re-parsing algorithms from the next sub-section.4We implemented a L2-regularized logistic regression clas-sifier using as features: identifiers of the base models, POS tagsof head and modifier, labels of dependencies, length of depen-dencies, length of sentence, and combinations of the above.5(Henderson and Brill, 1999) used a similar framework inthe context of constituent parsing and only three base parsers.650group of minority dependencies provides beneficialsignal only if its precision is larger than 50%.
Ta-ble 3 lists the total number of minority dependenciesin groups with precision larger than 50% for all ourbase parsers and the most representative features.The table shows that the number of minority depen-dencies with useful signal is extremely low.
All inall, it accounts for less than 0.7% of all dependen-cies in the development corpus.3.2 On re-parsing algorithmsTo guarantee that the resulting dependency tree iswell-formed, most previous work used the dynamicprogramming algorithm of Eisner (1996) for re-parsing (Sagae and Lavie, 2006; Hall et al, 2007).6However, it is not clear that this step is necessary.In other words, how many sentences are not well-formed if one uses a simple word-by-word votingscheme?
To answer this, we analyzed the outputof our best word-by-word voting scheme (six baseparsers weighted by the POS of the modifier).
Theresults for both in-domain and out-of-domain test-ing corpora are listed in Table 4.
The table showsthat the percentage of badly-formed trees is rela-tively large: almost 10% out of domain.
This in-dicates that the focus on algorithms that guaranteewell-formed trees is justified.However, it is not clear how the Eisner algo-rithm, which has runtime complexity of O(n3) (n?
number of tokens per sentence), compares againstapproximate re-parsing algorithms that have lowerruntime complexity.
One such algorithm was pro-posed by Attardi and Dell?Orletta (2009).
The al-gorithm, which has a runtime complexity of O(n),builds dependency trees using a greedy top-downstrategy, i.e., it starts by selecting the highest-scoringroot node, then the highest-scoring children, etc.
Wecompare these algorithms against the word-by-wordvoting scheme in Table 5.7 The results show thatboth algorithms pay a small penalty for guaranteeingwell-formed trees.
This performance drop is statis-tically significant out of domain.
On the other hand,the difference between the Eisner and Attardi algo-rithms is not statistically significant out of domain.6We focus on projective parsing algorithms because 99.6%of dependencies in our data are projective (Surdeanu et al,2008).7Statistical significance was performed using Dan Bikel ran-domized parsing evaluation comparator at 95% confidence.In domain Out of domainZero roots 0.83% 0.70%Multiple roots 3.37% 6.11%Cycles 4.29% 4.23%Total 7.46% 9.64%Table 4: Percentage of badly-formed dependency trees whenbase parsers are combined using a word-by-word votingscheme.
The different error classes do not sum up to the listedtotal because the errors are not mutually exclusive.In domain Out of domainLAS UAS LAS UASWord by word 88.89 91.52 82.13?
87.51?Eisner 88.83?
91.47?
81.99 87.32Attardi 88.70 91.34 81.82 87.29Table 5: Scores of different combination schemes.
?
indicatesthat a model is significantly different than the next lower rankedmodel.This experiment proves that approximate re-parsingalgorithms are a better choice for practical purposes,i.e., ensemble parsing in domains different from thetraining material of the base models.3.3 On parser integration at learning timeRecent work has shown that the combination ofbase parsers at learning time, e.g., through stacking,yields considerable benefits (Nivre and McDonald,2008; Attardi and Dell?Orletta, 2009).
However, itis unclear how these approaches compare against thesimpler ensemble models, which combine parsersonly at runtime.
To enable such a comparison, wereimplemented the best stacking model from (Nivreand McDonald, 2008) ?
MSTMalt ?
which trains avariant of the MSTParser that uses additional fea-tures extracted from the output of a Malt parser.In Table 6, we compare this stacking approachagainst four variants of our ensemble models.
Thesuperscript in the ensemble name indicates the run-time complexity of the model (O(n3) or O(n)).
Thecubic-time models use all base parsers from Table 1and the Eisner algorithm for re-parsing.
The linear-time models use only Malt-based parsers and theAttardi algorithm for re-parsing.
The subscript inthe model names indicates the percentage of avail-able base parsers used, e.g., ensemble350% uses onlythe first three parsers from Table 1.
These re-sults show that MSTMalt is statistically equivalentto an ensemble that uses MST and two Malt vari-ants, and both our ensemble100% models are signifi-cantly better than MSTMalt.
While this comparisonis somewhat unfair (MSTMalt uses two base models,whereas our ensemble models use at least three) it651In domain Out of domainLAS UAS LAS UASensemble3100% 88.83?
91.47?
81.99?
87.32?ensemble1100% 88.01?
90.76?
80.78 86.55ensemble350% 87.45 90.17 81.12 86.62MSTMalt 87.45?
90.22?
80.25?
85.90?ensemble150% 86.74 89.62 79.44 85.54Table 6: Comparison of different combination strategies.In domain Out of domainLAS UAS LAS UASCoNLL 2008, #1 90.13?
92.45?
82.81?
88.19?ensemble3100% 88.83?
91.47?
81.99?
87.32?CoNLL 2008, #2 88.14 90.78 80.80 86.12ensemble1100% 88.01 90.76 80.78 86.55Table 7: Comparison with state of the art parsers.does illustrate that the advantages gained from com-bining parsers at learning time can be easily sur-passed by runtime combination models that have ac-cess to more base parsers.
Considering that variantsof shift-reduce parsers can be generated with min-imal effort (e.g., by varying the parsing direction,learning algorithms, etc.)
and combining models atruntime is simpler than combining them at learningtime, we argue that runtime parser combination is amore attractive approach.3.4 Comparison with the state of the artIn Table 7 we compare our best ensemble modelsagainst the top two systems of the CoNLL-2008shared task evaluation.
The table indicates that ourbest ensemble model ranks second, outperformingsignificantly 19 other systems.
The only model per-forming better than our ensemble is a parser thatuses higher-order features and has a higher runtimecomplexity (O(n4)) (Johansson and Nugues, 2008).While this is certainly proof of the importance ofhigher-order features, it also highlights a pragmaticconclusion: in out-of-domain corpora, an ensembleof models that use only first-order features achievesperformance that is within 1% LAS of much morecomplex models.4 ConclusionsThis study unearthed several non-intuitive yet im-portant observations about ensemble models for de-pendency parsing.
First, we showed that the diver-sity of base parsers is more important than complexlearning models for parser combination, i.e., (a) en-semble models that combine several base parsers atruntime performs significantly better than a state-of-the-art model that combines two parsers at learningtime, and (b) meta-classification does not outper-form unsupervised voting schemes for the re-parsingof candidate dependencies when six base models areavailable.
Second, we showed that well-formed de-pendency trees can be guaranteed without signifi-cant performance loss by linear-time approximatere-parsing algorithms.
And lastly, our analysis in-dicates that unweighted voting performs as well asweighted voting for the re-parsing of candidate de-pendencies.
Considering that different base modelsare easy to generate, this work proves that ensembleparsers that are both accurate and fast can be rapidlydeveloped with minimal effort.AcknowledgmentsThis material is based upon work supported by the AirForce Research Laboratory (AFRL) under prime contractno.
FA8750-09-C-0181.
Any opinions, findings, andconclusion or recommendations expressed in this mate-rial are those of the authors and do not necessarily reflectthe view of the Air Force Research Laboratory (AFRL).We thank Johan Hall, Joakim Nivre, Ryan McDonald,and Giuseppe Attardi for their help in understanding de-tails of their models.ReferencesG.
Attardi and F. Dell?Orletta.
2009.
Reverse revisionand linear tree combination for dependency parsing.In Proc.
of NAACL-HLT.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proc.
of COL-ING.J.
Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi,M.
Nilsson, and M. Saers.
2007.
Single malt orblended?
A study in multilingual parser optimization.In Proc.
of CoNLL Shared Task.J.
C. Henderson and E. Brill.
1999.
Exploiting diversityin natural language processing: Combining parsers.
InProc.
of EMNLP.R.
Johansson and P. Nugues.
2008.
Dependency-basedsyntactic semantic analysis with PropBank and Nom-Bank.
In Proc.
of CoNLL Shared Task.J.
Nivre and R. McDonald.
2008.
Integrating graph-based and transition-based dependency parsers.
InProc.
of ACL.K.
Sagae and A. Lavie.
2006.
Parser combination byreparsing.
In Proc.
of NAACL-HLT.M.
Surdeanu, R. Johansson, A. Meyers, L. Marquez, andJ.
Nivre.
2008.
The CoNLL-2008 shared task on jointparsing of syntactic and semantic dependencies.
InProc.
of CoNLL.652
