Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 333?336,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsSimultaneous dialogue act segmentation and labelling using lexical andsyntactic featuresRamon Granell, Stephen PulmanOxford University Computing Laboratory,Wolfson Building, Parks Road,Oxford, OX1 3QD, Englandramg@comlab.ox.ac.uksgp@clg.ox.ac.ukCarlos-D.
Mart?
?nez-HinarejosInstituto Tecnolo?gico de Informa?tica,Universidad Polite?cnica de Valencia,Camino de Vera, s/n, 46022, Valencia, Spaincmartine@dsic.upv.esAbstractSegmentation of utterances and annotationas dialogue acts can be helpful for sev-eral modules of dialogue systems.
In thiswork, we study a statistical machine learn-ing model to perform these tasks simulta-neously using lexical features and incorpo-rating deterministic syntactic restrictions.There is a slight improvement in both seg-mentation and labelling due to these re-strictions.1 IntroductionDialogue acts (DA) are linguistic abstractions thatare commonly accepted and employed by the thedialogue community.
In the framework of dia-logue systems, they can be helpful to identify andmodel user intentions and system answers by thedialogue manager.
Furthermore, in other dialoguemodules such as the automatic speech recognizeror speech synthesiser, DA information may be alsoused to increase their performance.Many researchers have studied automatic DAlabelling using different techniques.
However, inmost of this work it is common to assume that thedialogue turns are already segmented into separateutterances, where each utterance corresponds tojust one DA label, as in (Stolcke et al(2000); Jiand Bilmes (2005); Webb et al(2005)).
This isnot a realistic situation because the segmentationof turns into utterances is not a trivial problem.There have been many previous approaches tosegmentation of turns prior to DA labelling, be-ginning with (Stolcke and Shriberg (1996)).
Typ-ically some combination of words and part ofspeech (POS) tags is used to predict segmentationboundaries.
In this work we make use of a sta-tistical model to solve both the DA labelling taskand the segmentation task simultaneously, follow-ing (Ang et al(2005); Mart?
?nez-Hinarejos et al(2006)).
Our aim is to see whether going beyondthe word n-gram models can improve accuracy,using syntactic information (constituent structure)obtained from the dialogue transcriptions.
We ex-amine whether this information can improve thesegmentation of the dialogue turns into DA seg-ments.
Intuitively, it seems logical to believe thatmost of these segments must coincide with partic-ular syntactic structures, and that segment bound-aries would respect constituent boundaries.2 Dialogue dataThe dialogue corpus used to perform the exper-iments is the Switchboard database (SWBD).
Itconsists of human-human conversations by tele-phone about generic topics.
There are 1155 5-minute conversations, comprising approximately205000 utterances and 1.4 million words.
The sizeof the vocabulary is approximately 22000 words.All this corpus has been manually annotated atthe dialogue act level using the SWBD-DAMSLscheme, (Jurafsky et al(1997)), consisting of 42different labels.
Every dialogue turn was manu-ally segmented into utterances.
The average num-ber of segments (utterances) per dialogue turn is1.78 with a standard deviation of 1.41.
Each utter-ance was assigned one SWBD-DAMSL label (seeFigure 1).3 Syntactic analysis of DA segmentsAn initial analysis of the syntactic structures of thedialogue data was performed to study their possi-ble relevance for DA segmentation.333- $LAUGH he waits until it gets about seventeen below up here $SEG and then he calls us , $SEGsd sd- he waits until it gets about seventeen below up here and then he calls us .Figure 1: The first row is an original segmented dialogue turn, where the $SEG label indicates the endof a DA segment.
The second row contains the corresponding DA label for each segment, where ?sd?corresponds to the SWBD-DAMSL label of Statement non-opinion.
The third row is the input for theparser.3.1 Parsing of spontaneous dialoguesOne of the main problems we face when we tryto syntactically analyse a corpus transcribed fromspontaneous speech by different people such asSWBD corpus, is the inconsistency of annotationconventions for spontaneous speech phenomenaand punctuation marks.
This can be problematicfor parsers, as they work at the sentence level.Some of the dialogue turns of the SWBD corpusare not transcribed using consistent punctuationconventions.
We therefore carried out some pre-processing so that all turns end with proper punc-tuation marks.
Additionally, the non-verbal labels(e.g.
$LAUGH, $OVERLAP, $SEG, ...) are re-moved.
In Figure 1 there is an example of thisprocess.The Stanford Parser, (Klein and Manning(2003)) was used for the syntactic analysis of thetranscriptions of SWBD dialogues.
The Englishgrammar used to train the parser is based on thestandard LDC Penn Treebank WSJ training sec-tions 2-21.
Is is important to remark that the natureof the training corpus (journalistic style reports)is different from the transcriptions of spontaneousspeech conversations.
We would therefore expecta decrease in accuracy.
As output of the parsingprocess, a tree that contains syntactic structureswas provided (e.g.
see Figure 2).3.2 Syntactic features and segmentationAs we are interested in studying the coincidenceof syntactic structures with DA segments, we willselect two general features for each word (see Fig-ure 3):?
Most general syntactic category that startswith a word, (MGSS), i.e., the root of the cur-rent subtree of the syntactic analysis, (e.g.
inFigure 2, ?CC?
is the MGSS of the first wordof the second segment, ?and?).?
Most general syntactic category that endswith a word, (MGSE), i.e., the root of the(ROOT(S (: -)(S(NP (PRP he))(VP (VBZ waits)(SBAR (IN until)(S(NP (PRP it))(VP (VBZ gets)(PP (IN about)(NP (NN seventeen)))(PP (IN below)(ADVP (RB up) (RB here))))))))(CC and)(S(ADVP (RB then))(NP (PRP he))(VP (VBZ calls)(NP (PRP us))))(.
.
)))Figure 2: Example of the syntactic analysis of thedialogue turn that appears in Figure 1.subtree of the syntactic analysis that endswith that word, (e.g.
in Figure 2, ?S?
isthe MGSE of last word of the first segment,?here?
).Using these features, we have analysed the syn-tactic categories of boundary words of segments.Particularly, it seems interesting to studyMGSE oflast word of the segment and MGSS of first wordof the segment, because it indicates which syntac-tic structure ends before the segment boundary andwhich one starts after it.
As there is always the be-ginning of a segment with the first word of the turnand the end of a segment with the last word of theturn, we are ignoring these for the analysis, be-cause we are looking for intra-turn segments.
Re-sults of this analysis can be seen in Table 1.4 The modelThe statistical model used to DA label andsegment the dialogues is extensively explainedin (Mart?
?nez-Hinarejos (2008)).
Basically, it is334ROOT+-+: $LAUGH S+he+NP VP+waits+VBZ SBAR+until+IN S+it+NP VP+gets+VBZPP+about+IN NP+seventeen+PP PP+below+IN ADVP+up+RB RB+here+S $SEGCC+and+CC S+then+ADVP NP+he+NP VP+calls+VBZ NP+us+S .+.+ROOT $SEGFigure 3: For each word of the example turn of Figure 1, MGSS (item before the word) and MGSE (itemafter the word) are obtained from the tree of Figure 2.
Non-verbal labels were reincorporated.MGSE MGSSOcc % Cat Occ % Cat33516 37.1 , 30318 33.5 ROOT30640 33.9 ROOT 19988 22.1 CC7801 8.6 : 13275 14.7 NP7134 7.9 S 10187 11.3 S2687 3.0 NP 3508 3.9 SBAR2319 2.6 PRN 3421 3.8 ADVP750 0.8 VP 2034 2.2 VP531 0.6 ADVP 1957 2.2 INTJ478 0.5 PP 1300 1.4 UH465 0.5 RB 972 1.1 PP4078 4.5 Other 3481 3.8 OtherTable 1: Occurrences and percentage of the syn-tactic categories that correspond with the most fre-quent MGSE of the last segment word (except lastsegment) andMGSS of the first segment word (ex-cept first segment).based on a combination of a Hidden MarkovModel at lexical level and a Language Model (n-gram) at DA level.
The Viterbi algorithm is usedto find the most likely sequence of DA labels ac-cording to the trained models.
The segmentationis obtained from the jumps between DAs of thissequence.The previous section has shown that the MGSEand MGSS for the segments boundary words areconcentrated in a small set of categories (see Ta-ble 1).
Therefore, one quick and easy way to in-corporate this information to the existing model isto add some restrictions during the decoding pro-cess, giving the model:U?
= argmaxUmaxr,sr1r?k=1Pr(uk|uk?1k?n?1) ?
?Pr(W sksk?1+1|uk)?
(xsk)where U?
is the sequence of DAs that we will getfrom the annotation/segmentation process.
Thesearch process produces a segmentation s =(s0, s1, .
.
.
, sr), that divides the word sequenceW into the segments W s1s0+1Ws2s1+1 .
.
.Wsrsr?1+1.Each segment is assigned to a DA ui that formsthe DA sequence U = u1 .
.
.
ur.
xi correspondsto the syntactic features of the i word that can beMGSE, MGSS or both of them, and?
(xi) =??
?1 if xi ?
X0 otherwisewhere X can be a subset of all the possible syn-tactic categories that correspond to:1. the most frequent MGSE of last segmentword, if x is MGSE.2.
the most frequent MGSS of first segmentword, if x is MGSS3.
the most frequent combinations of both pre-vious sets.It means that we will only allow a segment end-ing when the MGSE of a word is in this set, ora start of a segment when the MGSS of the fol-lowing word is in the corresponding set or bothconditions at the same time.5 Experiments and resultsTen cross-validation experiments were performedfor each model using, in each experiment a train-ing partition composed of 1136 dialogues anda test set of 19 dialogues, as in (Stolcke et al(2000); Webb et al(2005); Mart?
?nez-Hinarejoset al(2006)).
The N-grams were obtained usingthe SLM toolkit (Rosenfeld (1998)) with Good-Turing discounting and the HMMs were trainedusing the Baum-Welch algorithm.
We use the fol-lowing evaluation measures:?
To evaluate the labelling, we use the DA Er-ror Rate (equivalent to Word Error Rate) andthe percentage of error labelling of wholeturns.?
For the segment evaluation, we only checkwhere the segments bounds are produced(word position in the segment), making useof F-score obtained from precision and recall.335The results from using different sizes for the setX are shown for labelling performance in Tables 2and 3, and F-score of the segmentation in Table 4.Model/SizeX 5 10 20 AllMGSE 53.31 54.76 54.60 54.76MGSS 53.35 52.76 54.92 54.76Both 53.58 52.84 54.76 54.76Table 2: DAER for models using MGSE, MGSSand both features.
SizeX indicates the size of theset of most frequent categories accepted.
Withoutsyntactic categories (baseline) we obtain a DAERof 54.41.Model/SizeX 5 10 20 AllMGSE 53.61 55.41 55.34 55.77MGSS 53.61 53.32 55.63 55.77Both 53.46 53.10 55.19 55.77Table 3: Percentage of error of labelling of com-plete turns for all the possible models.
The base-line value is 55.41.Model / SizeX 5 10 20 AllMGSE 73.08 71.18 71.44 71.17MGSS 73.60 73.72 71.44 71.17Both 74.36 74.08 71.75 71.16Table 4: F-score of segmentation.
The baselinevalue is 71.17.6 Discussion and future workIn this work, we have used lexical and syntacticfeatures for labelling and segmenting DAs simul-taneously.
Syntactic features obtained automati-cally were deterministically applied during the sta-tistical decoding process.
There is a slight im-provement using syntactic information, obtainingbetter results than reported in other work suchas (Mart?
?nez-Hinarejos et al(2006)).
The F-scoreof the segmentation improves 3% using the syn-tactic features, however values are slightly worse(2%) than results in (Stolcke and Shriberg (1996)).As future work, we think that incorporating thesyntactic information in a non-deterministic waymight further improve the annotation and segmen-tation scores.
Furthermore, it is possible to makeuse of additional information from the syntacticstructure, rather than just the boundary informa-tion we are currently using.
Finally, an evalua-tion over different corpora must be done to checkboth the performance of the proposed model andthe reusability of the syntactic sets.AcknowledgmentsThis work was partially funded by the Compan-ions project (http://www.companions-project.org)sponsored by the European Commission as part ofthe Information Society Technologies (IST) pro-gramme under EC grant number IST-FP6-034434.ReferencesAng J., Liu Y., Shriberg E. 2005.
Automatic Dialog ActSegmentation and Classification in Multiparty Meet-ings.
Proc.
ICASSP, Philadelphia, USA, pp.
1061-1064Ji, G and Bilmes, J.
2005.
Dialog act tagging usinggraphical models.
Proc.
ICASSP, Philadelphia, USAJurafsky, D. Shriberg, E., Biasca, D. 1997.
Switchboardswbd-damsl shallow- discourse-function annotationcoders manual.
Tech.
Rep. 97-01, University of Col-orado Institute of Cognitive ScienceKlein D. and Manning, C. D. 2003.
Accurate Unlex-icalized Parsing.
Proc.
ACL, Sapporo, Japan, pp.423-430Mart?
?nez-Hinarejos, C. D., Granell, R., Bened?
?, J. M.2006.
Segmented and unsegmented dialogue-actannotation with statistical dialogue models.
Proc.COLING/ACL Sydney, Australia, pp.
563-570Mart?
?nez-Hinarejos, C. D., Bened?
?, J. M., Granell, R.2008.
Statistical framework for a spanish spokendialogue corpus.
Speech Communication, vol.
50,number 11-12, pp.
992-1008Rosenfeld, R. 1998.
The cmu-cambridge statisticallanguage modelling toolkit v2.
Technical report,Carnegie Mellon UniversityStolcke, A. and Shriberg, E. 1996.
Automatic linguis-tic segmentation of conversational speech.
Proc.
ofICSLP, Philadelphia, USAStolcke, A., Coccaro, N., Bates, R., Taylor, P., vanEss-Dykema, C., Ries, K., Shriberg, E., Jurafsky,D., Martin, R., Meteer, M. 2000.
Dialogue actmodelling for automatic tagging and recognitionof conversational speech.
Computational Linguistics26 (3), 1-34Webb, N., Hepple, M., Wilks, Y.
2005.
Dialogue actclassification using intra-utterance features.
Proc.
ofthe AAAI Workshop on Spoken Language Under-standing.
Pittsburgh, USA336
