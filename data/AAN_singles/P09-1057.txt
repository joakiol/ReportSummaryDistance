Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 504?512,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPMinimized Models for Unsupervised Part-of-Speech TaggingSujith Ravi and Kevin KnightUniversity of Southern CaliforniaInformation Sciences InstituteMarina del Rey, California 90292{sravi,knight}@isi.eduAbstractWe describe a novel method for the taskof unsupervised POS tagging with a dic-tionary, one that uses integer programmingto explicitly search for the smallest modelthat explains the data, and then uses EMto set parameter values.
We evaluate ourmethod on a standard test corpus usingdifferent standard tagsets (a 45-tagset aswell as a smaller 17-tagset), and show thatour approach performs better than existingstate-of-the-art systems in both settings.1 IntroductionIn recent years, we have seen increased interest inusing unsupervised methods for attacking differ-ent NLP tasks like part-of-speech (POS) tagging.The classic Expectation Maximization (EM) algo-rithm has been shown to perform poorly on POStagging, when compared to other techniques, suchas Bayesian methods.In this paper, we develop new methods for un-supervised part-of-speech tagging.
We adopt theproblem formulation of Merialdo (1994), in whichwe are given a raw word sequence and a dictio-nary of legal tags for each word type.
The goal isto tag each word token so as to maximize accuracyagainst a gold tag sequence.
Whether this is a real-istic problem set-up is arguable, but an interestingcollection of methods and results has accumulatedaround it, and these can be clearly compared withone another.We use the standard test set for this task, a24,115-word subset of the Penn Treebank, forwhich a gold tag sequence is available.
Thereare 5,878 word types in this test set.
We usethe standard tag dictionary, consisting of 57,388word/tag pairs derived from the entire Penn Tree-bank.1 8,910 dictionary entries are relevant to the5,878 word types in the test set.
Per-token ambigu-ity is about 1.5 tags/token, yielding approximately106425 possible ways to tag the data.
There are 45distinct grammatical tags.
In this set-up, there areno unknown words.Figure 1 shows prior results for this prob-lem.
While the methods are quite different,they all make use of two common model ele-ments.
One is a probabilistic n-gram tag modelP(ti|ti?n+1...ti?1), which we call the grammar.The other is a probabilistic word-given-tag modelP(wi|ti), which we call the dictionary.The classic approach (Merialdo, 1994) isexpectation-maximization (EM), where we esti-mate grammar and dictionary probabilities in or-der to maximize the probability of the observedword sequence:P (w1...wn) =?t1...tnP (t1...tn) ?
P (w1...wn|t1...tn)?
?t1...tnn?i=1P (ti|ti?2 ti?1) ?
P (wi|ti)Goldwater and Griffiths (2007) report 74.5%accuracy for EM with a 3-gram tag model, whichwe confirm by replication.
They improve this to83.9% by employing a fully Bayesian approachwhich integrates over all possible parameter val-ues, rather than estimating a single distribution.They further improve this to 86.8% by using pri-ors that favor sparse distributions.
Smith and Eis-ner (2005) employ a contrastive estimation tech-1As (Banko and Moore, 2004) point out, unsupervisedtagging accuracy varies wildly depending on the dictionaryemployed.
We follow others in using a fat dictionary (with49,206 distinct word types), rather than a thin one derivedonly from the test set.504System Tagging accuracy (%)on 24,115-word corpus1.
Random baseline (for each word, pick a random tag from the alternatives given bythe word/tag dictionary)64.62.
EM with 2-gram tag model 81.73.
EM with 3-gram tag model 74.54a.
Bayesian method (Goldwater and Griffiths, 2007) 83.94b.
Bayesian method with sparse priors (Goldwater and Griffiths, 2007) 86.85.
CRF model trained using contrastive estimation (Smith and Eisner, 2005) 88.66.
EM-HMM tagger provided with good initial conditions (Goldberg et al, 2008) 91.4*(*uses linguistic constraints and manual adjustments to the dictionary)Figure 1: Previous results on unsupervised POS tagging using a dictionary (Merialdo, 1994) on the full45-tag set.
All other results reported in this paper (unless specified otherwise) are on the 45-tag set aswell.nique, in which they automatically generate nega-tive examples and use CRF training.In more recent work, Toutanova and John-son (2008) propose a Bayesian LDA-based gener-ative model that in addition to using sparse priors,explicitly groups words into ambiguity classes.They show considerable improvements in taggingaccuracy when using a coarser-grained version(with 17-tags) of the tag set from the Penn Tree-bank.Goldberg et al (2008) depart from the Bayesianframework and show how EM can be used to learngood POS taggers for Hebrew and English, whenprovided with good initial conditions.
They uselanguage specific information (like word contexts,syntax and morphology) for learning initial P(t|w)distributions and also use linguistic knowledge toapply constraints on the tag sequences allowed bytheir models (e.g., the tag sequence ?V V?
is dis-allowed).
Also, they make other manual adjust-ments to reduce noise from the word/tag dictio-nary (e.g., reducing the number of tags for ?the?from six to just one).
In contrast, we keep all theoriginal dictionary entries derived from the PennTreebank data for our experiments.The literature omits one other baseline, whichis EM with a 2-gram tag model.
Here we obtain81.7% accuracy, which is better than the 3-grammodel.
It seems that EM with a 3-gram tag modelruns amok with its freedom.
For the rest of this pa-per, we will limit ourselves to a 2-gram tag model.2 What goes wrong with EM?We analyze the tag sequence output produced byEM and try to see where EM goes wrong.
Theoverall POS tag distribution learnt by EM is rel-atively uniform, as noted by Johnson (2007), andit tends to assign equal number of tokens to eachtag label whereas the real tag distribution is highlyskewed.
The Bayesian methods overcome this ef-fect by using priors which favor sparser distribu-tions.
But it is not easy to model such priors intoEM learning.
As a result, EM exploits a lot of raretags (like FW = foreign word, or SYM = symbol)and assigns them to common word types (in, of,etc.
).We can compare the tag assignments from thegold tagging and the EM tagging (Viterbi tag se-quence).
The table below shows tag assignments(and their counts in parentheses) for a few wordtypes which occur frequently in the test corpus.word/tag dictionary Gold tagging EM taggingin?
{IN, RP, RB, NN, FW, RBR} IN (355) IN (0)RP (3) RP (0)FW (0) FW (358)of?
{IN, RP, RB} IN (567) IN (0)RP (0) RP (567)on?
{IN,RP, RB} RP (5) RP (127)IN (129) IN (0)RB (0) RB (7)a?
{DT, JJ, IN, LS, FW, SYM, NNP} DT (517) DT (0)SYM (0) SYM (517)We see how the rare tag labels (like FW, SYM,etc.)
are abused by EM.
As a result, many word to-kens which occur very frequently in the corpus areincorrectly tagged with rare tags in the EM taggingoutput.We also look at things more globally.
We inves-tigate the Viterbi tag sequence generated by EMtraining and count how many distinct tag bigramsthere are in that sequence.
We call this the ob-served grammar size, and it is 915.
That is, intagging the 24,115 test tokens, EM uses 915 of theavailable 45 ?
45 = 2025 tag bigrams.2 The ad-vantage of the observed grammar size is that we2We contrast observed size with the model size for thegrammar, which we define as the number of P(t2|t1) entriesin EM?s trained tag model that exceed 0.0001 probability.505L8L0they    can          fish       .
I        fishL1L2 L3L4L6L5 L7L9L10L11STARTPROAUXVNPUNCd1 PRO-theyd2 AUX-cand3 V-cand4 N-fishd5 V-fishd6 PUNC-.d7 PRO-Ig1 PRO-AUXg2 PRO-Vg3 AUX-Ng4  AUX-Vg5 V-Ng6 V-Vg7 N-PUNCg8 V-PUNCg9 PUNC-PROg10 PRO-NdictionaryvariablesgrammarvariablesInteger ProgramMinimize: ?i=1?10 giConstraints:1.
Single left-to-right path (at each node, flow in = flow out)e.g.,  L0 = 1L1 = L3 + L42.
Path consistency constraints (chosen path respects chosendictionary & grammar)e.g., L0 ?
d1L1 ?
g1IP formulationtraining textlinkvariablesFigure 2: Integer Programming formulation for finding the smallest grammar that explains a given wordsequence.
Here, we show a sample word sequence and the corresponding IP network generated for thatsequence.can compare it with the gold tagging?s observedgrammar size, which is 760.
So we can safely saythat EM is learning a grammar that is too big, stillabusing its freedom.3 Small ModelsBayesian sparse priors aim to create small mod-els.
We take a different tack in the paper anddirectly ask: What is the smallest model that ex-plains the text?
Our approach is related to mini-mum description length (MDL).
We formulate ourquestion precisely by asking which tag sequence(of the 106425 available) has the smallest observedgrammar size.
The answer is 459.
That is, thereexists a tag sequence that contains 459 distinct tagbigrams, and no other tag sequence contains fewer.We obtain this answer by formulating the prob-lem in an integer programming (IP) framework.Figure 2 illustrates this with a small sample wordsequence.
We create a network of possible tag-gings, and we assign a binary variable to each linkin the network.
We create constraints to ensurethat those link variables receiving a value of 1form a left-to-right path through the tagging net-work, and that all other link variables receive avalue of 0.
We accomplish this by requiring thesum of the links entering each node to equal tothe sum of the links leaving each node.
We alsocreate variables for every possible tag bigram andword/tag dictionary entry.
We constrain link vari-able assignments to respect those grammar anddictionary variables.
For example, we do not allowa link variable to ?activate?
unless the correspond-ing grammar variable is also ?activated?.
Finally,we add an objective function that minimizes thenumber of grammar variables that are assigned avalue of 1.Figure 3 shows the IP solution for the exampleword sequence from Figure 2.
Of course, a smallgrammar size does not necessarily correlate withhigher tagging accuracy.
For the small toy exam-ple shown in Figure 3, the correct tagging is ?PROAUX V .
PRO V?
(with 5 tag pairs), whereas theIP tries to minimize the grammar size and picksanother solution instead.For solving the integer program, we use CPLEXsoftware (a commercial IP solver package).
Alter-natively, there are other programs such as lp solve,which are free and publicly available for use.
Oncewe create an integer program for the full test cor-pus, and pass it to CPLEX, the solver returns an506word sequence: they can fish .
I fishTagging Grammar SizePRO AUX N .
PRO N 5PRO AUX V .
PRO N 5PRO AUX N .
PRO V 5PRO AUX V .
PRO V 5PRO V N .
PRO N 5PRO V V .
PRO N 5PRO V N .
PRO V 4PRO V V .
PRO V 4Figure 3: Possible tagging solutions and corre-sponding grammar sizes for the sample word se-quence from Figure 2 using the given dictionaryand grammar.
The IP solver finds the smallestgrammar set that can explain the given word se-quence.
In this example, there exist two solutionsthat each contain only 4 tag pair entries, and IPreturns one of them.objective function value of 459.3CPLEX also returns a tag sequence via assign-ments to the link variables.
However, there areactually 104378 tag sequences compatible with the459-sized grammar, and our IP solver just selectsone at random.
We find that of all those tag se-quences, the worst gives an accuracy of 50.8%,and the best gives an accuracy of 90.3%.
Wealso note that CPLEX takes 320 seconds to returnthe optimal solution for the integer program corre-sponding to this particular test data (24,115 tokenswith the 45-tag set).
It might be interesting to seehow the performance of the IP method (in termsof time complexity) is affected when scaling up tolarger data and bigger tagsets.
We leave this aspart of future work.
But we do note that it is pos-sible to obtain less than optimal solutions faster byinterrupting the CPLEX solver.4 Fitting the ModelOur IP formulation can find us a small model, butit does not attempt to fit the model to the data.
For-tunately, we can use EM for that.
We still giveEM the full word/tag dictionary, but now we con-strain its initial grammar model to the 459 tag bi-grams identified by IP.
Starting with uniform prob-abilities, EM finds a tagging that is 84.5% accu-rate, substantially better than the 81.7% originallyobtained with the fully-connected grammar.
Sowe see a benefit to our explicit small-model ap-proach.
While EM does not find the most accurate3Note that the grammar identified by IP is not uniquelyminimal.
For the same word sequence, there exist other min-imal grammars having the same size (459 entries).
In our ex-periments, we choose the first solution returned by CPLEX.in onIN INRP RPword/tag dictionary RB RBNNFWRBRobserved EM dictionary FW (358) RP (127)RB (7)observed IP+EM dictionary IN (349) IN (126)RB (9) RB (8)observed gold dictionary IN (355) IN (129)RB (3) RP (5)Figure 4: Examples of tagging obtained from dif-ferent systems for prepositions in and on.sequence consistent with the IP grammar (90.3%),it finds a relatively good one.The IP+EM tagging (with 84.5% accuracy) hassome interesting properties.
First, the dictionarywe observe from the tagging is of higher qual-ity (with fewer spurious tagging assignments) thanthe one we observe from the original EM tagging.Figure 4 shows some examples.We also measure the quality of the two observedgrammars/dictionaries by computing their preci-sion and recall against the grammar/dictionary weobserve in the gold tagging.4 We find that preci-sion of the observed grammar increases from 0.73(EM) to 0.94 (IP+EM).
In addition to removingmany bad tag bigrams from the grammar, IP min-imization also removes some of the good ones,leading to lower recall (EM = 0.87, IP+EM =0.57).
In the case of the observed dictionary, usinga smaller grammar model does not affect the pre-cision (EM = 0.91, IP+EM = 0.89) or recall (EM= 0.89, IP+EM = 0.89).During EM training, the smaller grammar withfewer bad tag bigrams helps to restrict the dictio-nary model from making too many bad choicesthat EM made earlier.
Here are a few examplesof bad dictionary entries that get removed whenwe use the minimized grammar for EM training:in ?
FWa ?
SYMof ?
RPIn ?
RBRDuring EM training, the minimized grammar4For any observed grammar or dictionary X,Precision (X) =|{X}?
{observedgold}||{X}|Recall (X) =|{X}?
{observedgold}||{observedgold}|507Model Tagging accuracy Observed size Model sizeon 24,115-wordcorpusgrammar(G), dictionary(D) grammar(G), dictionary(D)1.
EM baseline with full grammar + full dictio-nary81.7 G=915, D=6295 G=935, D=64302.
EM constrained with minimized IP-grammar+ full dictionary84.5 G=459, D=6318 G=459, D=64143.
EM constrained with full grammar + dictio-nary from (2)91.3 G=606, D=6245 G=612, D=62984.
EM constrained with grammar from (3) + fulldictionary91.5 G=593, D=6285 G=600, D=63735.
EM constrained with full grammar + dictio-nary from (4)91.6 G=603, D=6280 G=618, D=6337Figure 5: Percentage of word tokens tagged correctly by different models.
The observed sizes and modelsizes of grammar (G) and dictionary (D) produced by these models are shown in the last two columns.helps to eliminate many incorrect entries (i.e.,zero out model parameters) from the dictionary,thereby yielding an improved dictionary model.So using the minimized grammar (which hashigher precision) helps to improve the quality ofthe chosen dictionary (examples shown in Fig-ure 4).
This in turn helps improve the tagging ac-curacy from 81.7% to 84.5%.
It is clear that theIP-constrained grammar is a better choice to runEM on than the full grammar.Note that we used a very small IP-grammar(containing only 459 tag bigrams) during EMtraining.
In the process of minimizing the gram-mar size, IP ends up removing many good tag bi-grams from our grammar set (as seen from the lowmeasured recall of 0.57 for the observed gram-mar).
Next, we proceed to recover some good tagbigrams and expand the grammar in a restrictedfashion by making use of the higher-quality dic-tionary produced by the IP+EM method.
We nowrun EM again on the full grammar (all possibletag bigrams) in combination with this good dictio-nary (containing fewer entries than the full dictio-nary).
Unlike the original training with full gram-mar, where EM could choose any tag bigram, nowthe choice of grammar entries is constrained bythe good dictionary model that we provide EMwith.
This allows EM to recover some of thegood tag pairs, and results in a good grammar-dictionary combination that yields better taggingperformance.With these improvements in mind, we embarkon an alternating scheme to find better models andtaggings.
We run EM for multiple passes, and ineach pass we alternately constrain either the gram-mar model or the dictionary model.
The procedureis simple and proceeds as follows:1.
Run EM constrained to the last trained dictio-nary, but provided with a full grammar.52.
Run EM constrained to the last trained gram-mar, but provided with a full dictionary.3.
Repeat steps 1 and 2.We notice significant gains in tagging perfor-mance when applying this technique.
The taggingaccuracy increases at each step and finally settlesat a high of 91.6%, which outperforms the exist-ing state-of-the-art systems for the 45-tag set.
Thesystem achieves a better accuracy than the 88.6%from Smith and Eisner (2005), and even surpassesthe 91.4% achieved by Goldberg et al (2008)without using any additional linguistic constraintsor manual cleaning of the dictionary.
Figure 5shows the tagging performance achieved at eachstep.
We found that it is the elimination of incor-rect entries from the dictionary (and grammar) andnot necessarily the initialization weights from pre-vious EM training, that results in the tagging im-provements.
Initializing the last trained dictionaryor grammar at each step with uniform weights alsoyields the same tagging improvements as shown inFigure 5.We find that the observed grammar also im-proves, growing from 459 entries to 603 entries,with precision increasing from 0.94 to 0.96, andrecall increasing from 0.57 to 0.76.
The figurealso shows the model?s internal grammar and dic-tionary sizes.Figure 6 and 7 show how the precision/recallof the observed grammar and dictionary varies fordifferent models from Figure 5.
In the case of theobserved grammar (Figure 6), precision increases5For all experiments, EM training is allowed to run for40 iterations or until the likelihood ratios between two subse-quent iterations reaches a value of 0.99999, whichever occursearlier.50800.10.20.30.40.50.60.70.80.91Precision/ Recall of observed grammarTagging ModelModel 1 Model 2 Model 3 Model 4 Model 5PrecisionRecallFigure 6: Comparison of observed grammars fromthe model tagging vs. gold tagging in terms of pre-cision and recall measures.00.10.20.30.40.50.60.70.80.91Precision/ Recall of observed dictionaryTagging ModelModel 1 Model 2 Model 3 Model 4 Model 5PrecisionRecallFigure 7: Comparison of observed dictionaries fromthe model tagging vs. gold tagging in terms of pre-cision and recall measures.Model Tagging accuracy on24,115-word corpusno-restarts with 100 restarts1.
Model 1 (EM baseline) 81.7 83.82.
Model 2 84.5 84.53.
Model 3 91.3 91.84.
Model 4 91.5 91.85.
Model 5 91.6 91.8Figure 8: Effect of random restarts (during EMtraining) on tagging accuracy.at each step, whereas recall drops initially (owingto the grammar minimization) but then picks upagain.
The precision/recall of the observed dictio-nary on the other hand, is not affected by much.5 Restarts and More DataMultiple random restarts for EM, while not oftenemphasized in the literature, are key in this do-main.
Recall that our original EM tagging with afully-connected 2-gram tag model was 81.7% ac-curate.
When we execute 100 random restarts andselect the model with the highest data likelihood,we get 83.8% accuracy.
Likewise, when we ex-tend our alternating EM scheme to 100 randomrestarts at each step, we improve our tagging ac-curacy from 91.6% to 91.8% (Figure 8).As noted by Toutanova and Johnson (2008),there is no reason to limit the amount of unlabeleddata used for training the models.
Their modelsare trained on the entire Penn Treebank data (in-stead of using only the 24,115-token test data),and so are the tagging models used by Goldberget al (2008).
But previous results from Smith andEisner (2005) and Goldwater and Griffiths (2007)show that their models do not benefit from usingmore unlabeled training data.
Because EM is ef-ficient, we can extend our word-sequence train-ing data from the 24,115-token set to the entirePenn Treebank (973k tokens).
We run EM trainingagain for Model 5 (the best model from Figure 5)but this time using 973k word tokens, and furtherincrease our accuracy to 92.3%.
This is our finalresult on the 45-tagset, and we note that it is higherthan previously reported results.6 Smaller Tagset and IncompleteDictionariesPreviously, researchers working on this task havealso reported results for unsupervised tagging witha smaller tagset (Smith and Eisner, 2005; Gold-water and Griffiths, 2007; Toutanova and John-son, 2008; Goldberg et al, 2008).
Their systemswere shown to obtain considerable improvementsin accuracy when using a 17-tagset (a coarser-grained version of the tag labels from the PennTreebank) instead of the 45-tagset.
When tag-ging the same standard test corpus with the smaller17-tagset, our method is able to achieve a sub-stantially high accuracy of 96.8%, which is thebest result reported so far on this task.
The ta-ble in Figure 9 shows a comparison of differentsystems for which tagging accuracies have beenreported previously for the 17-tagset case (Gold-berg et al, 2008).
The first row in the tablecompares tagging results when using a full dictio-nary (i.e., a lexicon containing entries for 49,206word types).
The InitEM-HMM system fromGoldberg et al (2008) reports an accuracy of93.8%, followed by the LDA+AC model (LatentDirichlet Allocation model with a strong Ambigu-ity Class component) from Toutanova and John-son (2008).
In comparison, the Bayesian HMM(BHMM) model from Goldwater et al (2007) and509Dict IP+EM (24k) InitEM-HMM LDA+AC CE+spl BHMMFull (49206 words) 96.8 (96.8) 93.8 93.4 88.7 87.3?
2 (2141 words) 90.6 (90.0) 89.4 91.2 79.5 79.6?
3 (1249 words) 88.0 (86.1) 87.4 89.7 78.4 71Figure 9: Comparison of different systems for English unsupervised POS tagging with 17 tags.the CE+spl model (Contrastive Estimation with aspelling model) from Smith and Eisner (2005) re-port lower accuracies (87.3% and 88.7%, respec-tively).
Our system (IP+EM) which uses inte-ger programming and EM, gets the highest accu-racy (96.8%).
The accuracy numbers reported forInit-HMM and LDA+AC are for models that aretrained on all the available unlabeled data fromthe Penn Treebank.
The IP+EM models used inthe 17-tagset experiments reported here were nottrained on the entire Penn Treebank, but insteadused a smaller section containing 77,963 tokensfor estimating model parameters.
We also includethe accuracies for our IP+EM model when usingonly the 24,115 token test corpus for EM estima-tion (shown within parenthesis in second columnof the table in Figure 9).
We find that our perfor-mance does not degrade when the parameter esti-mation is done using less data, and our model stillachieves a high accuracy of 96.8%.6.1 Incomplete Dictionaries and UnknownWordsThe literature also includes results reported in adifferent setting for the tagging problem.
In somescenarios, a complete dictionary with entries forall word types may not be readily available to usand instead, we might be provided with an incom-plete dictionary that contains entries for only fre-quent word types.
In such cases, any word notappearing in the dictionary will be treated as anunknown word, and can be labeled with any ofthe tags from given tagset (i.e., for every unknownword, there are 17 tag possibilities).
Some pre-vious approaches (Toutanova and Johnson, 2008;Goldberg et al, 2008) handle unknown words ex-plicitly using ambiguity class components condi-tioned on various morphological features, and thishas shown to produce good tagging results, espe-cially when dealing with incomplete dictionaries.We follow a simple approach using just oneof the features used in (Toutanova and Johnson,2008) for assigning tag possibilities to every un-known word.
We first identify the top-100 suffixes(up to 3 characters) for words in the dictionary.Using the word/tag pairs from the dictionary, wetrain a simple probabilistic model that predicts thetag given a particular suffix (e.g., P(VBG | ing) =0.97, P(N | ing) = 0.0001, ...).
Next, for every un-known word ?w?, the trained P(tag | suffix) modelis used to predict the top 3 tag possibilities for?w?
(using only its suffix information), and subse-quently this word along with its 3 tags are added asa new entry to the lexicon.
We do this for every un-known word, and eventually we have a dictionarycontaining entries for all the words.
Once the com-pleted lexicon (containing both correct entries forwords in the lexicon and the predicted entries forunknown words) is available, we follow the samemethodology from Sections 3 and 4 using integerprogramming to minimize the size of the grammarand then applying EM to estimate parameter val-ues.Figure 9 shows comparative results for the 17-tagset case when the dictionary is incomplete.
Thesecond and third rows in the table shows taggingaccuracies for different systems when a cutoff of2 (i.e., all word types that occur with frequencycounts < 2 in the test corpus are removed) anda cutoff of 3 (i.e., all word types occurring withfrequency counts < 3 in the test corpus are re-moved) is applied to the dictionary.
This yieldslexicons containing 2,141 and 1,249 words respec-tively, which are much smaller compared to theoriginal 49,206 word dictionary.
As the resultsin Figure 9 illustrate, the IP+EM method clearlydoes better than all the other systems except forthe LDA+AC model.
The LDA+AC model fromToutanova and Johnson (2008) has a strong ambi-guity class component and uses more features tohandle the unknown words better, and this con-tributes to the slightly higher performance in theincomplete dictionary cases, when compared tothe IP+EM model.7 DiscussionThe method proposed in this paper is simple?once an integer program is produced, there aresolvers available which directly give us the so-lution.
In addition, we do not require any com-plex parameter estimation techniques; we train ourmodels using simple EM, which proves to be effi-cient for this task.
While some previous methods510word type Gold tag Automatic tag # of tokens tagged incorrectly?s POS VBZ 173be VB VBP 67that IN WDT 54New NNP NNPS 33U.S.
NNP JJ 31up RP RB 28more RBR JJR 27and CC IN 23have VB VBP 20first JJ JJS 20to TO IN 19out RP RB 17there EX RB 15stock NN JJ 15what WP WDT 14one CD NN 14?
POS : 14as RB IN 14all DT RB 14that IN RB 13Figure 10: Most frequent mistakes observed in the model tagging (using the best model, which gives92.3% accuracy) when compared to the gold tagging.introduced for the same task have achieved bigtagging improvements using additional linguisticknowledge or manual supervision, our models arenot provided with any additional information.Figure 10 illustrates for the 45-tag set some ofthe common mistakes that our best tagging model(92.3%) makes.
In some cases, the model actuallygets a reasonable tagging but is penalized perhapsunfairly.
For example, ?to?
is tagged as IN by ourmodel sometimes when it occurs in the context ofa preposition, whereas in the gold tagging it is al-ways tagged as TO.
The model also gets penalizedfor tagging the word ?U.S.?
as an adjective (JJ),which might be considered valid in some casessuch as ?the U.S. State Department?.
In othercases, the model clearly produces incorrect tags(e.g., ?New?
gets tagged incorrectly as NNPS).Our method resembles the classic MinimumDescription Length (MDL) approach for modelselection (Barron et al, 1998).
In MDL, thereis a single objective function to (1) maximize thelikelihood of observing the data, and at the sametime (2) minimize the length of the model descrip-tion (which depends on the model size).
How-ever, the search procedure for MDL is usuallynon-trivial, and for our task of unsupervised tag-ging, we have not found a direct objective functionwhich we can optimize and produce good taggingresults.
In the past, only a few approaches uti-lizing MDL have been shown to work for naturallanguage applications.
These approaches employheuristic search methods with MDL for the taskof unsupervised learning of morphology of natu-ral languages (Goldsmith, 2001; Creutz and La-gus, 2002; Creutz and Lagus, 2005).
The methodproposed in this paper is the first application ofthe MDL idea to POS tagging, and the first touse an integer programming formulation ratherthan heuristic search techniques.
We also notethat it might be possible to replicate our modelsin a Bayesian framework similar to that proposedin (Goldwater and Griffiths, 2007).8 ConclusionWe presented a novel method for attackingdictionary-based unsupervised part-of-speech tag-ging.
Our method achieves a very high accuracy(92.3%) on the 45-tagset and a higher (96.8%) ac-curacy on a smaller 17-tagset.
The method worksby explicitly minimizing the grammar size usinginteger programming, and then using EM to esti-mate parameter values.
The entire process is fullyautomated and yields better performance than anyexisting state-of-the-art system, even though ourmodels were not provided with any additional lin-guistic knowledge (for example, explicit syntacticconstraints to avoid certain tag combinations suchas ?V V?, etc.).
However, it is easy to model someof these linguistic constraints (both at the local andglobal levels) directly using integer programming,and this may result in further improvements andlead to new possibilities for future research.
Fordirect comparison to previous works, we also pre-sented results for the case when the dictionariesare incomplete and find the performance of oursystem to be comparable with current best resultsreported for the same task.9 AcknowledgementsThis research was supported by the DefenseAdvanced Research Projects Agency underSRI International?s prime Contract NumberNBCHD040058.511ReferencesM.
Banko and R. C. Moore.
2004.
Part of speechtagging in context.
In Proceedings of the Inter-national Conference on Computational Linguistics(COLING).A.
Barron, J. Rissanen, and B. Yu.
1998.
The min-imum description length principle in coding andmodeling.
IEEE Transactions on Information The-ory, 44(6):2743?2760.M.
Creutz and K. Lagus.
2002.
Unsupervised discov-ery of morphemes.
In Proceedings of the ACL Work-shop on Morphological and Phonological Learningof.M.
Creutz and K. Lagus.
2005.
Unsupervisedmorpheme segmentation and morphology inductionfrom text corpora using Morfessor 1.0.
Publicationsin Computer and Information Science, Report A81,Helsinki University of Technology, March.Y.
Goldberg, M. Adler, and M. Elhadad.
2008.
EM canfind pretty good HMM POS-taggers (when given agood start).
In Proceedings of the ACL.J.
Goldsmith.
2001.
Unsupervised learning of the mor-phology of a natural language.
Computational Lin-guistics, 27(2):153?198.S.
Goldwater and T. L. Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speechtagging.
In Proceedings of the ACL.M.
Johnson.
2007.
Why doesnt EM find good HMMPOS-taggers?
In Proceedings of the Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL).B.
Merialdo.
1994.
Tagging English text with aprobabilistic model.
Computational Linguistics,20(2):155?171.N.
Smith and J. Eisner.
2005.
Contrastive estimation:Training log-linear models on unlabeled data.
InProceedings of the ACL.K.
Toutanova and M. Johnson.
2008.
A BayesianLDA-based model for semi-supervised part-of-speech tagging.
In Proceedings of the Advances inNeural Information Processing Systems (NIPS).512
