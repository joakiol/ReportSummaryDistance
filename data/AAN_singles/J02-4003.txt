c?
2002 Association for Computational LinguisticsAutomatic Summarization ofOpen-Domain Multiparty Dialogues inDiverse GenresKlaus Zechner?Educational Testing ServiceAutomatic summarization of open-domain spoken dialogues is a relatively new research area.
Thisarticle introduces the task and the challenges involved and motivates and presents an approachfor obtaining automatic-extract summaries for human transcripts of multiparty dialogues of fourdifferent genres, without any restriction on domain.We address the following issues, which are intrinsic to spoken-dialogue summarization andtypically can be ignored when summarizing written text such as news wire data: (1) detection andremoval of speech disfluencies; (2) detection and insertion of sentence boundaries; and (3) detectionand linking of cross-speaker information units (question-answer pairs).A system evaluation is performed using a corpus of 23 dialogue excerpts with an averageduration of about 10 minutes, comprising 80 topical segments and about 47,000 words total.
Thecorpus was manually annotated for relevant text spans by six human annotators.
The global eval-uation shows that for the two more informal genres, our summarization system using dialogue-specific components significantly outperforms two baselines: (1) a maximum-marginal-relevanceranking algorithm using TF*IDF term weighting, and (2) a LEAD baseline that extracts the firstn words from a text.1.
IntroductionAlthough the field of summarizing written texts has been explored for many decades,gaining significantly increased attention in the last five to ten years, summarizationof spoken language is a comparatively recent research area.
As the number of spokenaudio databases is growing rapidly, however, we predict that the need for high-qualitysummarization of information contained in this medium will increase substantially.Summarization of spoken dialogues, in particular, may aid in the archiving, indexing,and retrieval of various records of oral communication, such as corporate meetings,sales interactions, or customer support.The purpose of this article is to explore the issues of spoken-dialogue summa-rization and to describe and evaluate an implementation addressing some of the corechallenges intrinsic to the task.
We will use an implementation of a state-of-the-arttext summarization method (maximum marginal relevance, or MMR) as the mainbaseline for comparative evaluations, and then add a set of components addressingissues specific to spoken dialogues to this MMR module to create our spoken dialoguesummarization system, which we call DIASUMM.We consider the following dimensions to be relevant for our research; the combi-nation of these dimensions distinguishes our work from most other work in the field?
Educational Testing Service, Rosedale Road MS 11-R, Princeton, NJ 08541.
E-mail: kzechner@ets.org448Computational Linguistics Volume 28, Number 4of summarization:?
spoken versus written language?
multiparty dialogues versus texts written by one author?
unrestricted versus restricted domains?
diverse genres versus a single genreThe main challenges this work has to address, in addition to the challenges of written-text summarization, are as follows:?
coping with speech disfluencies?
identifying the units for extraction?
maintaining cross-speaker coherence?
coping with speech recognition errorsWe will discuss these challenges in more detail in the following section.
Althoughwe have addressed the issue of speech recognition errors in previous related work(Zechner and Waibel 2000b), for the purpose of this article, we exclusively use humantranscripts of spoken dialogues.Intrinsic evaluations of text summaries usually use sentences as their basic units.For our data, however, sentence boundaries are typically not available in the first place.Thus we devise a word-based evaluation metric derived from an average relevancescore from human relevance annotations (section 6.2).The organization of this article is as follows: Section 2 provides the motivationfor our research, introducing and discussing the main challenges of spoken-dialoguesummarization, followed by a section on related work (section 3).
Section 4 describesthe corpus we use to develop and evaluate our system, along with the proceduresemployed for corpus annotation.
The system architecture and its components are de-scribed in detail in section 5, along with evaluations thereof.
Section 6 presents theglobal evaluation of our approach, before we conclude the article with a discussionof our results, contributions, and directions for future research in this field (sections 7and 8).2.
MotivationConsider the following example from a phone conversation drawn from the EnglishCALLHOME database (LDC 1996).
It is a transcript of a conversation between twonative speakers of American English; one person is in the New York area (speakera), the other one (speaker b) in Israel.
It was recorded about a month after YitzhakRabin?s assassination (1995).
This dialogue segment is about one minute of real time.The audio is segmented into speaker turns using silence heuristics,1 and each turnis marked with a turn number and with the speaker label.
Noises are removed toincrease readability.21 Therefore, in some cases, we can find several turns of one speaker following each other.2 Hence there can be ?missing?
turns (e.g., turn 37), in case they contain only noises and no actual words.449Zechner Automatic Summarization of Dialogues28 a: oh29 b: they didn?t know he was going to get shot but itwas at a peace rally so i mean it just worked out30 b: i mean it was a good place for the poor guy to diei mean because it was you know right after the rallyand everything was on film and everything31 a: yeah32 b: oh the whole country we just finished the thirty daysmourning for him now you know it?s uh oh everybody?sstill in shock it?s33 a: oh34 a: i know35 b: terrible what?s going on over here36 b: and this guy that killed him they show him on t vsmiling he?s all happy he did it and everything heisn?t even sorry or anything38 a: there are i39 b: him him he and his brother you know the two ofthem were in it together and there?s a whole groupnow it?s like a a conspiracy oh it?s eh40 a: mm41 a: with the kahane chai42 b: unbelievable43 b: yeah yeah it?s all those people yeah you probably seethem running around new york don?t you they?re all44 a: yeah45 a: oh yeah they?re here46 b: new york based yeah47 a: oh there?s48 a: all those fanatics49 a: like the extreme50 b: oh but51 b: but wh- what?s the reaction in america really i meani mean do people care you know i mean you know do they52 a: yeah mo- most pe- i mean uh53 a: i don?t know what commu- i mean like the jewish community54 a: a lot e- all of us were55 a: very upset and there were lots all the56 b: yeah57 a: like two days after did it happen like on a sunday58 b: yeah it hap- it happened on it happened on a saturday nightBy looking at this transcript we can readily identify some of the phenomena thatwould cause difficulties for conventional summarizers of written texts:?
Some turns (e.g., turn 51) contain many disfluencies that (1) make themhard to read and (2) reduce the relevance of the information containedtherein.?
Some (important) pieces of information are distributed over a sequenceof turns (e.g., turns 53?54?55, 45?47?48?49); this is due to a silence-based450Computational Linguistics Volume 28, Number 4segmentation algorithm that causes breaks in logically connected clauses.A traditional summarizer might render these sequences incompletely.?
Some turns are quite long (e.g., 36, 39) and contain several sentences; awithin-turn segmentation seems necessary to avoid the extraction of toomuch extraneous information when only parts of a turn contain relevantinformation.?
Some of the information is constructed interactively by both speakers;the prototypical cases are question-answer pairs (e.g., turns 51?52ff.,turns 57?58).
A traditional text summarizer might miss either question oranswer and hence produce a less meaningful summary.We shall discuss these arising issues along with an indication of our computationalremedies in the following subsections.
We want to stress beforehand, though, thatthe originality of our system should not be seen in the particular implementation ofits individual components, but rather in their selection and specific composition toaddress the issues at hand in an effective and also efficient way.2.1 Disfluency DetectionThe two main negative effects speech disfluencies have on summarization are that they(1) decrease the readability of the summary and (2) increase its noncontent noise.
Inparticular for informal conversations, the percentage of disfluent words is quite high,typically around 20% of the total words spoken.3 This means that this issue should,in our opinion, be addressed to improve the quality (readability and conciseness) ofthe generated summaries.In section 5.3 we shall present three components for identifying most of the majorclasses of speech disfluencies in the input of the summarization system, such as filledpauses, repetitions, and false starts.
All detected disfluencies are marked in this processand can be selectively excluded during summary generation.2.2 Sentence Boundary DetectionUnlike written texts, in which punctuation markers clearly indicate clause and sen-tence boundaries, spoken language is generated as a sequence of streams of words, inwhich pauses (silences between words) do not always match linguistically meaningfulsegments: A speaker can pause in the middle of a sentence or even a phrase, or, onthe other hand, might not pause at all after the end of a sentence or clause.This mismatch between acoustic and linguistic segmentation is reflected in theoutput of a speech recognizer, which typically generates a sequence of speaker turnswhose boundaries are marked by periods of silence (or nonspeech).
As a result, onespeaker?s turn may contain multiple sentences, or, on the other hand, a speaker?ssentence might span more than one turn.
In a test corpus of five English CALLHOMEdialogues with an average length of 320 turns, we found on average of about 30 suchcontinuations of logical clauses over automatically determined acoustic segments perdialogue.The main problems for a summarizer would thus be (1) the lack of coherence andreadability of the output because of incomplete sentences and (2) extraneous infor-mation due to extracted units consisting of more than one sentence.
In section 5.4 we3 Although other studies have found percentages lower than this figure, we included content-lesscategories such as discourse markers or rhetorical connectives, which are often not regarded asdisfluencies per se.451Zechner Automatic Summarization of Dialoguesdescribe a component for sentence segmentation that addresses both of these prob-lems.2.3 Distributed InformationSince we have multiparty conversations as opposed to monologues, sometimes thecrucial information is found in a sequence of turns from several speakers, the proto-typical case of this being a question-answer pair.
If the summarizer were to extractonly the question or only the answer, the lack of the corresponding answer or questionwould often cause a severe reduction of coherence in the summary.In some cases, either the question or the answer is very short and does not containany words with high relevance that would yield a substantial weight in the summa-rizer.
In order not to lose these short sentences at a later stage, when only the mostrelevant sentences are extracted, we need to identify matching question-answer pairsahead of time, so that the summarizer can output the matching sentences during sum-mary generation as one unit.
We describe our approach to cross-speaker informationlinking in section 5.5.2.4 Other IssuesWe see the work reported in this article as the first in-depth analysis and evaluationin the area of open-domain spoken-dialogue summarization.
Given the large scope ofthis undertaking, we had to restrict ourselves to those issues that are, in our opinion,the most salient for the task at hand.A number of other important issues for summarization in general and for speechsummarization in particular are either simplified or not addressed in this article andleft for future work in this field.
In the following, we briefly mention some of theseissues, indicating their potential relevance and promise.2.4.1 Topic Segmentation.
In many cases, spoken dialogues are multitopical.
For theEnglish CALLHOME corpus, we determined an average topic length of about one to twominutes?
speaking time (or about 200?400 words).
Summarization can be accomplishedfaster and more concisely if it operates on smaller topical segments rather than on longpieces of input consisting of diverse topics.Although we have implemented a topic segmentation component as part of oursystem for these reasons, all of the evaluations are based on the topical segmentsdetermined by human annotators.
Therefore, this component will not be discussedin this article.
Furthermore, topical segmentation is not an issue intrinsic to spokendialogues, which in our opinion justifies this simplification.2.4.2 Anaphora Resolution.
An analogous reasoning holds for the issue of anaphoraresolution: Although it would certainly be desirable, for the sake of increased coher-ence and readability, to employ a well-working anaphora resolution component, thisissue is not specific to the task at hand, either.
One could argue that particularly forsummarization of more informal conversations, in which personal pronouns are ratherfrequent, anaphora resolution might be more helpful than for, say, summarization ofwritten texts.
But we conjecture that this task might also prove more challenging thanwritten-text anaphora resolution.
In our system, we did not implement a module foranaphora resolution.2.4.3 Discourse Structure.
Previous work indicates that information about discoursestructure from written texts can help in identifying the more salient and relevantsentences or clauses for summary generation (Marcu 1999; Miike et al 1994).
Much452Computational Linguistics Volume 28, Number 4less exploration has been done, however, in the area of automatic analysis of dis-course structure for non-task-oriented spoken dialogues in unrestricted domains, suchas CALLHOME (LDC 1996).
Research for those kinds of corpora reported in Jurafsky etal.
(1998), Stolcke et al (2000), Levin et al (1999), and Ries et al (2000) focuses more ondetecting localized phenomena such as speech acts, dialogue games, or functional ac-tivities.
We conjecture that there are two reasons for this: (1) free-flowing spontaneousconversations have much less structure than task-oriented dialogues, and (2) the au-tomatic detection of hierarchical structure would be much harder than it is for writtentexts or dialogues based on a premeditated plan.Although we believe that in the long run attempts to automatically identify thediscourse structure of spoken dialogues may benefit summarization, in this article, wegreatly simplify this matter and exclusively look at local contexts in which speakersinteractively construct shared information (the question-answer pairs).2.4.4 Speech Recognition Errors.
Throughout this article, our simplifying assumptionis that our input comes from a perfect speech recognizer; that is, we use humantextual transcripts of the dialogues in our corpus.
Although there are cases in whichthis assumption is justifiable, such as transcripts provided by news services in parallelto the recorded audio data, we believe that in general a spoken dialogue summarizerhas to be able to accept corrupted input from an automatic speech recognizer (ASR),as well.
Our system is indeed able to work with ASR output; it is integrated in a largersystem (Meeting Browser) that creates, summarizes, and archives meeting records andis connected to a speech recognition engine (Bett et al 2000).
Further, we have shownin previous work how we can use ASR confidence scores (1) to reduce the word errorrate within the summary and (2) to increase the summary accuracy (Zechner andWaibel 2000b).2.4.5 Prosodic Information.
A further simplifying assumption of this work is thatprosodic information is not available, with the exception of start and end times ofspeaker turns.
Considering the results reported by Shriberg et al (1998) and Shriberget al (2000), we conjecture that future work in this field will demonstrate the addi-tional benefit of incorporating prosodic information, such as stress, pitch, and intra-turn pauses, into the summarization system.
In particular, we would expect improvedsystem performance when speech recognition hypotheses are used as input: In thatcase, the prosodic information could compensate to some extent for incorrect wordinformation.3.
Related WorkThe vast majority of summarization research in the past clearly has focused exclu-sively on written text.
A good selection of both early seminal papers and more recentwork can be found in Mani and Maybury (1999).
In general, most summarizationapproaches can be classified as either corpus-based, statistical summarization (suchas Kupiec, Pedersen, and Chen [1995]), or knowledge-based summarization (such asReimer and Hahn [1988]) in which the text domain is restricted.
(The MMR method[Carbonell, Geng, and Goldstein 1997], which we are using as the summarizationengine for our DIASUMM system, belongs to the first category.)
More recently, Marcu(1999) presented work on using automatically detected discourse structure for summa-rization.
Knight and Marcu (2000) and Berger and Mittal (2000) presented approachesin which summarization can be reformulated as a problem of machine translation:453Zechner Automatic Summarization of Dialoguestranslating a long sentence into a shorter sentence, or translating a Web page into abrief gist, respectively.Two main areas are exceptions to the focus on text summarization in past work:(1) summarization of task-oriented dialogues in restricted domains and (2) summa-rization of spoken news in unrestricted domains.
We shall discuss both of these areasin the following subsections, followed by a discussion of prosody-based emphasis de-tection in spoken language, and finally by a summary of research most closely relatedto the topic of this work.3.1 Summarization of Dialogues in Restricted DomainsDuring the past decade, there has been significant progress in the area of closed-domain spoken-dialogue translation and understanding, even with automatic speechrecognition input.
Two examples of systems developed in that time frame are JANUS(Lavie et al 1997) and VERBMOBIL (Wahlster 1993).In that context, several spoken-dialogue summarization systems have been de-veloped whose goal it is to capture the essence of the task-based dialogues at hand.The MIMI system (Kameyama and Arima 1994; Kameyama, Kawai, and Arima 1996)deals with the travel reservation domain and uses a cascade of finite-state pattern rec-ognizers to find the desired information.
Within VERBMOBIL, a more knowledge-richapproach is used (Alexandersson and Poller 1998; Reithinger et al 2000).
The domainhere is travel planning and negotiation of a trip.
In addition to finite-state transducersfor content extraction and statistical dialogue act recognition, VERBMOBIL also uses adialogue processor and a summary generator that have access to a world knowledgedatabase, a domain model, and a semantic database.
The abstract representations builtby this summarizer allow for summary generation in multiple languages.3.2 Summarization of Spoken NewsWithin the context of the Text Retrieval Conference (TREC) spoken document retrieval(SDR) conferences (Garofolo et al 1997; Garofolo et al 1999) as well as the recentDefense Advanced Research Project Agency (DARPA) broadcast news workshops, anumber of research groups have been developing multimedia browsing tools for text,audio, and video data, which should facilitate the access to news data, combiningdifferent modalities.Hirschberg et al (1999) and Whittaker et al (1999) present a system that supportslocal navigation for browsing and information extraction from acoustic databases,using speech recognizer transcripts in tandem with the original audio recording.
Al-though their interface helps users in the tasks of relevance ranking and fact finding,it is less helpful in the creating of summaries, partly because of imperfect speechrecognition.Valenza et al (1999) present an audio summarization system that combines acous-tic confidence scores with relevance scores to obtain more accurate and reliable sum-maries.
An evaluation showed that human judges preferred summaries with a com-pression rate of about 15% (30 words per minute at a speaking rate of about 200 wordsper minute) and that the summary word error rate was significantly smaller than theword error rate for the full transcript.Hori and Furui (2000) use salience features in combination with a language modelto reduce Japanese broadcast news captions by about 30?40% while keeping the mean-ing of about 72% of all sentences in the test set.
Another speech-related reduction ap-proach was presented recently by Koumpis and Renals (2000), who summarize voicemail in the Small Message format.454Computational Linguistics Volume 28, Number 43.3 Prosody-Based Emphasis Detection in Spoken AudioWhereas most approaches to summarizing acoustic data rely on the word informa-tion (provided by a human or ASR transcript), there have been attempts to generatesummaries based on emphasized regions in a discourse, using only prosodic features.Chen and Withgott (1992) train a hidden Markov model on transcripts of spontaneousspeech, labeled for different degrees of emphasis by a panel of listeners.
Their ?au-dio summaries?
on an unseen (but rather small) test set achieve a remarkably goodagreement with human annotators (?
> 0.5).
Stifelman (1995) uses a pitch-based em-phasis detection algorithm developed by Arons (1994) to find emphasized passagesin a 13-minute discourse.
In her analysis, she finds good agreement between theseemphasized regions and the beginnings of manually marked discourse segments (inthe framework of Grosz and Sidner [1986]).
Although these are promising results, be-ing suggestive of the role of prosody for determining emphasis, relevance, or saliencein spoken discourse, in this work we restrict the use of prosody to the turn lengthand interturn pause features.
We conjecture, however, that the integration of prosodicand word level information would be a fruitful research area that would have to beexplored in future work.3.4 Spoken Dialogue Summarization in Unrestricted DomainsWaibel, Bett, and Finke (1998) report results of their summarizer on automaticallytranscribed SWITCHBOARD (SWBD) data (Godfrey, Holliman, and McDaniel 1992), theword error rate being about 30%.
Their implementation used an algorithm inspiredby MMR, but they did not address any dialogue- or speech-related issues in theirsummarizer.
In a question-answer test with summaries of five dialogues, participantscould identify most of the key concepts using a summary size of only five turns.These results varied widely (between 20% and 90% accuracy) across the five differentdialogues tested in this experiment.Our own previous work (Zechner and Waibel 2000a) addressed for the first timethe combination of challenges of dialogue summarization with summarization of spo-ken language in unrestricted domains.
We presented a first prototype of DIASUMMthat addressed the issues of disfluency detection and removal and sentence boundarydetection, as well as cross-speaker information linking.This work extends and expands these initial attempts substantially, in that we arenow focusing on (1) a systematic training of the major components of the DIASUMMsystem, enabled by the recent availability of a large corpus of disfluency-annotatedconversations (LDC 1999b), and (2) the exploration of three more genres of spokendialogues in addition to the English CALLHOME corpus (NEWSHOUR, CROSSFIRE, GROUPMEETINGS).
Further, the relevance annotations are now performed by a set of six humanannotators, which makes the global system evaluation more meaningful, consideringthe typical divergence among different annotators?
relevance judgments.4.
Data Annotation4.1 Corpus CharacteristicsTable 1 provides the statistics on the corpus used for the development and evaluationof our system.
We use data from four different genres, two being more informal, twomore formal:?
English CALLHOME and CALLFRIEND: from the Linguistic DataConsortium (LDC) collections, eight dialogues for the devtest set455Zechner Automatic Summarization of DialoguesTable 1Data characteristics for the corpus (average over dialogues).
8E-CH, 4E-CH: EnglishCallHome; NHOUR: NewsHour; XFIRE: CrossFire; G-MTG: Group Meetings.Data Set 8E-CH 4E-CH NHOUR XFIRE G-MTGFormal/informal informal informal formal formal informalTopics predetermined no no yes yes yesDialogue excerpts (total) 8 4 3 4 4Topical segments (total) 28 23 8 14 7Different speakers 2.1 2 2 6 7.5Turns 242 276 25 96 140Sentences 280 366 101 281 304Sentences per turn 1.2 1.3 4.1 2.9 2.2Questions (in %) 3.7 6.4 6.3 9.8 4.0False starts (in %) 12.1 11.0 2.0 7.2 13.9Words 1685 1905 1224 3165 2355Words per sentence 6.0 5.2 12.1 11.3 7.7Disfluent (in %) 16.0 16.3 5.1 4.2 13.2Disfluencies 222 259 48 95 266Disfluencies per sentence 0.79 0.71 0.48 0.34 0.87Empty coordinating conjunctions (in %) 30.3 30.4 64.8 50.7 24.3Lexicalized filled pauses (in %) 18.8 21.0 17.2 23.5 13.9Editing terms (in %) 3.6 1.6 3.4 5.7 3.3Nonlexicalized filled pauses (in %) 20.8 29.9 0.7 2.3 29.5Repairs (in %) 26.6 17.1 13.8 17.8 29.0(8E-CH) and four dialogues for the eval set (4E-CH).4 These arerecordings of phone conversations between two family members orfriends, typically about 30 minutes in length; the excerpts we used werematched with the transcripts, which typically represent 5?10 minutes ofspeaking time.?
NEWSHOUR (NHOUR): Excerpts from PBS?s NewsHour television showwith Jim Lehrer (recorded in 1998).?
CROSSFIRE (XFIRE): Excerpts from CNN?s CrossFire television show withBill Press and Robert Novak (recorded in 1998).?
GROUP MEETINGS (G-MTG): Excerpts from recordings of project groupmeetings in the Interactive Systems Labs at Carnegie Mellon University.Furthermore, we used the Penn Treebank distribution of the SWITCHBOARD corpus,annotated with disfluencies, to train the major components of the system (LDC 1999b).From Table 1 we can see that the two more formal corpora, NEWSHOUR andCROSSFIRE, have longer sentences, more sentences per turn, and fewer disfluencies(particularly nonlexicalized filled pauses and false starts) than English CALLHOMEand the GROUP MEETINGS.
This means that their flavor is more like that of written textand not so close to the conversational speech typically found in the SWITCHBOARD orCALLHOME corpora.4 We used the devtest set corpus for system development and tuning and set aside the eval set for thefinal global system evaluation.
For the other three genres, two dialogue excerpts each were used for thedevtest set, the remainder for the eval set.456Computational Linguistics Volume 28, Number 44.2 Corpus Annotation4.2.1 First Annotation Phase.
All the annotations were performed on human-gener-ated transcripts of the dialogues.
The CALLHOME and GROUP MEETINGS dialogueswere automatically partitioned into speaker turns (by means of a silence heuristic);the other corpora were segmented manually (based on the contents and flow of theconversation).5There were six naive human annotators performing the task;6 only four, however,completed the entire set of dialogues.
Thus, the number of annotations available foreach dialogue varies from four to six.
Prior to the relevance annotations, the annotatorshad to mark topical boundaries, because we want to be able to define and then createsummaries for each topical segment separately (as opposed to a whole conversationconsisting of multiple topics).
The notion of a topic was informally defined as a regionin the text that ends, according to the annotation manual, ?when the speakers shifttheir topic of discussion.
?Once the topical segments were marked, for each such segment, each annota-tor had to identify the most relevant information units (IUs), called nucleus IUs, andsomewhat relevant IUs, called satellite IUs.
IUs are often equivalent to sentences butcan span longer or shorter contiguous segments of text, dependent on the annotator?schoice.
The overall goal of this relevance markup was to create a concise and readablesummary containing the main information present in the topical segment.
Annotatorswere also asked to mark the most salient words within their annotated IUs with a +,which would render a summary with a somewhat more telegraphic style (+-markedwords).We also asked that the human annotators stay within a preset target length fortheir summaries: The +-marked words in all IUs within a topical segment shouldbe 10?20% of all the words in the segment.
The guideline was enforced by a checkerprogram that was run during and after annotation of a transcript and that also ensuredthat no markup errors and no accidental word deletions occurred.
We provide a briefexample here (n[, n] mark the beginning and end of a nucleus IU, the phrase they flyto Boston was +-marked as the core content within this IU):B: heck it might turn out that you know n[ if+they +fly in +to +boston i can n]4.2.2 Creation of Gold-Standard Summaries.
After the first annotation phase, inwhich each coder worked independently according to the guidelines described above,we devised a second phase, in which two coders from the initial group were askedto create a common-ground annotation, based on the majority opinion of the wholegroup.
To construct such a majority opinion guideline automatically, we assignedweights to all words in nucleus IUs and satellite IUs and added all weights for allmarked words of all coders for every turn.7 The total turn weights were then sorted bydecreasing value to provide a guide for the two coders in the second phase as to whichturns they should focus their annotations on for the common-ground or gold-standardsummaries.5 This fact may partially account for NEWSHOUR and CROSSFIRE turns being longer than CALLHOME andGROUP MEETING turns.6 Naive in this context means that they were nonexperts in linguistics or discourse analysis.7 The weights were set as follows: nucleus IUs: 3.0 if +-marked, 2.0 otherwise; satellite IUs: 1.0 if+-marked, 0.5 otherwise.457Zechner Automatic Summarization of DialoguesTable 2Nuclei and satellites: Length in tokens and relative frequency (in % of all tokens).Annotator/ Avg.
Nuc.
Avg.
Sat.
Nuc-Tokens Nuc-+-Marked Sat-Tokens Sat-+-MarkedData Set Length Length (in %) (in %) (in %) (in %)LB 12.993 13.732 11.646 8.558 5.363 3.818BR 16.507 14.551 11.978 8.339 10.558 7.645SC 20.720 14.093 29.412 18.045 6.517 4.796RW 22.899 19.576 19.352 11.332 2.757 1.718RC 23.741 18.553 43.573 15.434 12.749 0.333JK 39.203 9.794 26.355 11.204 0.711 0.465Gold 21.763 6.462 13.934 6.573 0.179 0.000CALLHOME 17.108 13.099 21.962 11.003 5.126 1.932NEWSHOUR 25.828 16.733 29.536 13.530 4.300 2.947CROSSFIRE 33.923 22.132 21.705 10.615 1.853 0.976MEETINGS 37.674 23.413 23.034 9.222 7.456 1.123All Dialogues 23.152 16.173 22.796 10.807 4.665 1.636Other than this guideline, the requirements were almost exactly identical to thosein phase 1, except that (1) the pair of annotators was required to work together on thistask to be able to reach a consensus opinion, and (2) the preset relative word lengthof the gold summary (10?20%) applied only to the nucleus IUs.As for the topical boundaries, which obviously vary among coders, a list of bound-ary positions chosen by the majority (at least half) of the coders in the first phase wasprovided.
In this gold-standard phase, the two coders mostly stayed with these sug-gestions and changed less than 15% of the suggested topic boundaries, the majorityof which were minor (less than two turns?
difference in boundary position).4.2.3 General Annotation Analysis.
Table 2 provides the statistics on the frequenciesof the annotated nucleus and satellite IUs.
We make the following observations:?
On average, about 23% of all tokens were assigned to a nucleus IU and5% to a satellite IU; counting only the +-marked tokens, this reduces toabout 11% and 2% of all tokens, respectively.?
The average total lengths of nuclei and satellites vary widely acrosscorpora: between 17.1 (13.1) tokens for CALLHOME and 37.7 (23.4) tokensfor GROUP MEETINGS data.
This is most likely a reflection on the typicallength of turns in the different subcorpora.?
A similar variation is also observed across annotators: between 12 and 40tokens for nucleus-IUs and between 9 and 20 tokens for satellites.
Thegranularity of IUs is quite different across annotators.?
Since some annotators mark a larger number of IUs than others, there isan even larger discrepancy in the relative number of words assigned tonucleus IUs and satellite IUs among the different annotators: 11?44%(nucleus IUs) and 0?13% (satellite IUs).?
The ratio of nucleus versus satellite tokens also varies greatly among theannotators: from about 1:1 to 40:1.458Computational Linguistics Volume 28, Number 4?
The ratio of nucleus and satellite tokens that are +-marked varies greatly:between 36 and 77% for nucleus IUs and between 2 and 80% forsatellite IUs.From these observations, we conclude that merging the nucleus and satellite IUsinto one class would yield a more consistent picture than keeping them separate.
Asimilar argument can be made for the +-marked passages, in which we also find aquite high intercoder variation in relative +-marking.
This led us to the decision ofgiving equal weight to any word in an IU, irrespective of IU type or marking, for thepurpose of global system evaluation.Finally, we conjecture that the average length of our extraction units should bein the 10?40 words range, which roughly corresponds to about 3?12 seconds of realtime, assuming an average word length of 300 milliseconds.
As a comparison, wenote that Valenza et al (1999) found summaries with 30-grams8 working well in theirexperiments, a finding that is in line with our observations here on typical human IUlengths.4.2.4 Intercoder Agreement.
Agreement between coders (and between automatic meth-ods and coders) has been measured in the summarization literature with quite a widerange of methods: Rath, Resnick, and Savage (1961) use Kendall?s ?
; Kupiec, Ped-ersen, and Chen (1995) (among many others) use percentage agreement; and Aone,Okurowski, and Gorlinsky (1997) (among others) use the notions of precision, recall,and F1-score, which are commonly employed in the information retrieval community.Similarly, in the literature on discourse segmentation and labeling, a variety of differ-ent agreement measures have been used, including precision and recall (Hearst 1997;Passonneau and Litman 1997), Krippendorff?s (1980) ?
(Passonneau and Litman 1997)and Cohen?s (1960) ?
(Carletta et al 1997).In this work, we use the two following metrics: (1) the ?-statistic in its extension formore than two coders (Davies and Fleiss 1982); and (2) precision, recall, and F1-score.9We will discuss the ?-statistic first.For intercoder agreement with respect to topical boundaries, agreement is foundif boundaries fall within the same 50-word bin of a dialogue.
Relevance agreementsare computed at the word level.
For relevance markings, we compute ?
both for thethree-way case (nucleus IUs, satellite IUs, unmarked) and the two-way case (any IUs,unmarked).10 Topical-boundary agreement was not evaluated for two of the GROUPMEETINGS dialogues, in which only one of four annotators marked any text-internaltopic boundary.
We compute agreements for each dialogue separately and report thearithmetic means for the five subcorpora in Table 3.
We observe that agreement for top-ical boundaries is much higher than for relevance markings.
Furthermore, agreementis generally higher for CALLHOME and comparatively low for the GROUP MEETINGScorpus.As a second evaluation metric, we compute precision, recall, and F1-scores for thesame four annotators and the same sets of subcorpora as before.
For topical boundaries,a match means that the boundaries fall within ?3 turns of each other, and for relevant8 A 30-gram is a passage of text containing 30 adjacent words.9 Precision is the ratio of correctly matched items over all items (boundaries, marked words); recall is theratio of correctly matched items over all items that need to be matched; and the F1-score combinesprecision (P) and recall (R) in the following way: F1 = 2PRP+R .10 These computations were performed for those four (out of six) annotators who completed the entirecorpus markup.459Zechner Automatic Summarization of DialoguesTable 3Intercoder annotation ?
agreement for topical boundaries and relevance markings.8E-CH 4E-CH NHOUR XFIRE G-MTG OverallTopical boundaries 0.503 0.402 0.256 0.331 0.174 0.384Relevance markings (3 way) 0.147 0.161 0.123 0.089 0.040 0.117Relevance markings (2 way) 0.157 0.169 0.124 0.100 0.046 0.126Table 4Intercoder annotation F1-agreement for topical boundaries and relevance markings.8E-CH 4E-CH NHOUR XFIRE G-MTG OverallTopical boundaries .54 .44 .53 .38 .18 .45Relevance markings (2 way) .38 .39 .38 .32 .32 .36words a match means that the two words to be compared are both in a nucleus orsatellite IU.
The results can be seen in Table 4.4.2.5 Disfluency and Sentence Boundary Annotation.
In addition to the annotationfor topic boundaries and relevant text spans, the corpus was also annotated for speechdisfluencies in the same style as the Penn Treebank SWITCHBOARD corpus (LDC 1999b).One coder (different from the six annotators mentioned before) manually tagged thecorpus for disfluencies and sentence boundaries following the SWITCHBOARD disflu-ency annotation style book (Meteer et al 1995).4.2.6 Question-Answer Annotation.
A final type of annotation was performed on theentire corpus to mark all questions and their answers, for the purpose of training andevaluation of the question-answer linking system component.
Questions and answerswere annotated in the following way: Every sentence that is a question was marked aseither a Yes-No-question or a Wh-question.
Exceptions were back-channel questions,such as ?Is that right??
; rhetorical questions, such as ?Who would lie in public??
; andother questions that do not refer to a propositional content.
These were not supposedto be marked (even if they have an apparent answer), since we see the latter classof questions as irrelevant for the purpose of increasing the local coherence withinsummaries.
For each Yes-No-question and Wh-question that has an answer, the answerwas marked with its relative offset to the question to which it belongs.
Some answersare continued over several sentences, but only the core answer (which usually consistsof a single sentence) was marked.
This decision was made to bias the answer detectionmodule toward brief answers and to avoid the question-answer regions?
getting toolengthy, at the expense of summary conciseness.5.
Dialogue Summarization System5.1 System ArchitectureThe global system architecture of the spoken-dialogue summarization system pre-sented in this article (DIASUMM) is depicted in Figure 1.
The input data are a time-ordered sequence of speaker turns with the following quadruple of information: starttime, end time, speaker label, and word sequence.
The seven major components areexecuted sequentially, yielding a pipeline architecture.460Computational Linguistics Volume 28, Number 4False Start Detection(+ Chunk Parser)Sentence Boundary DetectionPOS TaggerRepetition FilterQuestion & Answer DetectionSentence Ranking & SelectionTopic SegmentationDisfluencyDetectionExtraction UnitIdentificationdialogue transcriptdialogue summaryFigure 1Global system architecture.The following subsections describe the components of the system in more detail.As argued earlier, the topic detection component is not relevant for the way we con-duct the global system evaluation and hence is not discussed here.
(We implemented avariant of Hearst?s [1997] TextTiling algorithm.)
The three components involved in dis-fluency detection are the part-of-speech (POS) tagger, the false-start detection module,and the repetition filter.
They are discussed in subsection 5.3, followed by a subsectionon sentence boundary detection (5.4).
The question-answer pair detection is describedin subsection 5.5, and the sentence selection module, performing relevance ranking,in subsection 5.6.5.2 Input TokenizationWe eliminate all human and nonhuman noises and incomplete words from the in-put transcript.
Further, we eliminate all information on case and punctuation, since461Zechner Automatic Summarization of Dialogueswe emulate the ASR output in that regard, which does not provide this informa-tion.Contractions such as don?t or I?ll are divided and treated as separate words?inthese examples we would obtain do n?t and I ?ll.5.3 Disfluency Detection5.3.1 Motivation.
Conversational, informal spoken language is quite different fromwritten language in that a speaker?s utterances are typically much less well-formedthan a writer?s sentences.
We can observe a set of disfluencies such as false starts, hes-itations, repetitions, filled pauses, and interruptions.
Additionally, in speech there isno good match between linguistically motivated sentence boundaries and turn bound-aries or recognition hypotheses from automatic speech recognition.5.3.2 Types of Disfluencies.
The classification of disfluencies in this work followsShriberg (1994), Meteer et al (1995), and Rose (1998).
It is worth noting, however,that any disfluency classification will be only an approximation of the assumed realphenomena and that often boundaries between different classes are fuzzy and hardto decide for human annotators (cf.
Meteer et al [1995] on annotators?
problems withthe classification of the word so).?
Filled pauses: We follow Rose?s (1998) classification of nonlexicalizedfilled pauses (typically uh, um) and lexicalized filled pauses (e.g., like, youknow).
Whereas the former are usually nonambiguous and hence easy todetect, the latter are ambiguous and much harder to detect accurately.?
Restarts or repairs: These are fragments that are resumed, but withoutcompletely abandoning the first attempt.
We follow the notation inMeteer et al (1995) and Shriberg (1994), which has these parts:(1) reparandum, (2) interruption point (+), (3) interregnum (editingphase, {.
.
.
}), and (4) repair.?
Repetition: A restart with a verbatim repetition of a word or asequence of words: [ she is + she is ] happy.?
Insertion: A repetition of the reparandum, with some word(s)inserted: [ she liked + {um} she really liked ] it.?
Substitution: The reparandum is not repeated: [ she + {uh} mywife ] liked it.?
False starts: These are abandoned, incomplete clauses.
In some cases,they may occur at the end of an utterance, and they can be due tointerruption by another speaker.
Example: so we didn?t?they have notaccepted our proposal.5.3.3 Related Work.
The past decade has produced a substantial amount of research inthe area of detecting intonational and linguistic boundaries in conversational speech,as well as in the area of detecting and correcting speech disfluencies.
Whereas earlierwork tended to look at these phenomena in isolation (Nakatani and Hirschberg 1994;Stolcke and Shriberg 1996), more recent work has attempted to solve several taskswithin one framework (Heeman and Allen 1999; Stolcke et al 1998).Most approaches use some kind of prosodic information, such as duration ofpauses, stress, and pitch contours, and most of them combine this prosodic informa-tion with information about word identity and sequence (n-grams, hidden Markov462Computational Linguistics Volume 28, Number 4models).
In the study of Stolcke et al (1998), the goal was to detect sentence bound-aries and a variety of speech disfluencies on a large portion of the SWITCHBOARDcorpus.
An explicit comparison was made between prosodic and word-based models,and the results showed that an n-gram model, enhanced with segmental informa-tion about turn boundaries, significantly outperformed the prosodic model.
Modelcombination improved the overall results, but only to a small extent.
In more recentresearch, Shriberg et al (2000) reported that for sentence boundary detection in twodifferent corpora (BROADCAST NEWS and SWITCHBOARD), prosodic models outperformword-based language models and a model combination yields additional performancegains.5.3.4 Overview.
In the following, we will discuss the three components of the DIASUMMsystem that perform disfluency detection:?
a POS tagger that tags, in addition to the standard SWITCHBOARDTreebank-3 tag set (LDC 1999b), the following disfluent regions or words:1. coordinating conjunctions that don?t serve their usual connectiverole, but act more as links between subsequent speech acts of aspeaker (e.g., and then; we call these empty coordinatingconjunctions in this work)2. lexicalized filled pauses (labeled as discourse markers in theTreebank-3 corpus; e.g., you know, like)3. editing terms within speech repairs (e.g., I mean)4. nonlexicalized filled pauses (e.g., um, uh)?
a decision tree (supported by a shallow chunk parser) that decideswhether to label a particular sentence as a false start?
a repetition detection script (for repeated sequences of up to four words)5.3.5 Training Corpus.
For training, we used a part of the SWITCHBOARD transcriptsthat was manually annotated for sentence boundaries, POS, and the following typesof disfluent regions (LDC 1999b):?
{A. .
.
}: asides (very rare; we ignore them in our experiments)?
{C. .
.
}: empty coordinating conjunctions (e.g., and then)?
{D. .
.
}: discourse markers (i.e., lexicalized filled pauses in our terminology,e.g., you know)?
{E. .
.
}: editing terms (within repairs; e.g., I mean)?
{F. .
.
}: filled pauses (nonlexicalized; e.g., uh)?
[.
.
.
+ .
.
.
]: repairs: the part before the + is called reparandum (to beremoved), the part after the + repair (proper)Sentence boundaries can be at the end of completed sentences (E S) or of noncompletedsentences, such as false starts or abandoned clauses (N S).463Zechner Automatic Summarization of DialoguesTable 5Precision, recall and F1-scores of the four disfluency tag categories for the SWITCHBOARD testset.Description Count Tag Precision Recall F1Empty coordinating conjunctions 5,990 CO 0.84 0.93 0.88Lexicalized filled pauses 5,787 DM 0.95 0.90 0.93Editing terms 1,004 ET 0.98 0.94 0.96Nonlexicalized filled pauses 12,926 UH 0.98 0.98 0.98Table 6POS tagging accuracy on five subcorpora (evaluated on 500-word samples).8E-CH 4E-CH NHOUR XFIRE G-MTGKnown words 92.8 90.6 92.7 90.6 93.2Unknown words (total) 48.0 (25) 44.4 (9) 69.6 (23) 86.4 (22) 92.6 (27)Overall 90.6 89.8 91.6 90.4 93.25.3.6 POS Tagger.
We are using Brill?s rule-based POS tagger (Brill 1994).
Its basicalgorithm at run time (after training) can be described as follows:1.
Tag every word with its most likely tag, predicting tags of unknownwords based on rules.2.
Change every tag according to its right and left context (both words andtags are considered), following a list of rules.For preprocessing, we replaced the tags in the regions of {C. .
.
}, {D. .
.
}, and {E. .
.
}with the tags CO (coordinating), DM (discourse marker), and ET (editing term), re-spectively.
(The filler regions {F. .
. }
are already tagged with UH in the corpus.)
Linesthat contain typographical errors were excluded from the training corpus.
We furthereliminated all incomplete words (XX tag) and combined multiwords, marked by a GWtag, into a single word (hence eliminating the GW tag).11 The entire resulting new tagset had 42 tags.12Training of the POS tagger proceeded in three stages, using about 250,000 taggedwords for each stage.
The trained POS tagger?s performance on an unseen test set ofabout 185,000 words is 94.1% tag accuracy (untrained baseline: 84.8% accuracy).Table 5 shows precision, recall, and F1-scores for the four categories of disfluencytags, measured on the test set after the last training phase.
We see that the nonlexical-ized filler words are almost perfectly tagged (F1 = 0.98), whereas the hardest task forthe tagger is the empty coordinating conjunctions (F1 = 0.88): There are a few highlyambiguous words in that set, such as and, so, and or.Table 6 shows the POS tagging accuracy on the five subcorpora of our dialoguecorpus, evaluated on a sample of 500 words per subcorpus.
We see that the POS-tagging accuracy is slightly lower than for the SWITCHBOARD set that was used for11 The sole function of the GW tag is to label words that are considered to be parts of other words butwere transcribed separately, such as: drug/GW testing/NN.12 For a description of the POS tags used in that database see Santorini (1990) and LDC (1999a).464Computational Linguistics Volume 28, Number 4Table 7Disfluency tag detection (F1) for five subcorpora (results in parentheses: less than 10 tags to bedetected).8E-CH 4E-CH NHOUR XFIRE G-MTGCO .89 .89 .38 .77 .54DM .93 .73 .90 .82 .30ET .95 .95 (.94) .85 .88UH .56 .62 (.14) (.28) .45training (approximately 90?93%; global average: 91.1%).
Further we observe that withthe exception of the CALLHOME corpora, the majority of unknown words were actuallytagged correctly.
The most frequent errors were (1) conjunctions tagged as emptycoordinated conjunctions, (2) proper names tagged as regular nouns, and (3) adverbstagged as adjectives.Finally, we look at the POS tagger?s performance for the four disfluency tags CO,DM, ET, and UH in our five subcorpora; the results of this evaluation are presented inTable 7.
We can see that the detection accuracy is generally lower than for the corpuson which we trained the tagger (SWITCHBOARD), but still quite good in general.
Themajor exceptions are the UH tags, on which the F1-scores are comparatively low for allsubcorpora.
The reason for this can be found mostly in words like yes, no, uh-huh, right,okay, and yeah, which are often tagged with UH in SWITCHBOARD but frequently are notconsidered to be irrelevant words in our corpus and hence not marked as disfluent(e.g., if they are considered to be the answer to a question or a summary-relevantacknowledgment).
We circumvent potential exclusion from the summary output ofthese and other words that might be erroneously tagged as nonlexicalized filled pauses(UH) by marking a small set of words as exempt from removal (see section 5.5.6).5.3.7 False Start Detection.
False starts are quite frequent in spontaneous speech,occurring at a rate of about 10?15% of all sentences (SWITCHBOARD, CALLHOME).
Theyinvolve less than 10% of the total words of a dialogue; about 34% of the words inthese incomplete sentences are part of some other disfluencies, such as filled pausesor repairs.
(In complete sentences, only about 15% of the words are part of thesedisfluencies.)
For CALLHOME, the average length of complete sentences is about 6words, of incomplete sentences about 4.1 words (including disfluencies).We trained a C4.5 decision tree (Quinlan 1992) on 8,000 sentences of SWITCHBOARD.As features we use the first and last four trigger words (words that have a highincidence around sentence boundaries) and POS of every sentence, as well as the firstand last four chunks from a POS-based chunk parser.
This chunk parser is basedon a simple context-free POS grammar for English.
It outputs a phrasal bracketingof the input string (e.g., noun phrases or prepositional phrases).
Further, we encodethe length of the sentence in words and the number of the words not parsed by thechunk parser.
We observed that whereas the chunk information itself does not improveperformance over the baseline of using trigger words and POS information only, thederived feature of ?number of not parsed words?
actually does improve the results.We ran the decision tree on data with perfect POS tags (for SWITCHBOARD only),disfluency tags (except for repairs), and sentence boundaries.
The evaluations wereperformed on independent test sets of about 3,000 sentences for SWITCHBOARD and ofour complete dialogue corpus.
Table 8 shows the results of these experiments.
Typicalerrors, where complete sentences were classified as incomplete, are inverted forms or465Zechner Automatic Summarization of DialoguesTable 8False start classification results for different corpora (F1).SWBD 8E-CH 4E-CH NHOUR XFIRE G-MTGFalse start frequency (in %) 12.3 12.1 11.0 2.0 7.2 13.9False start detection (F1) .611 .545 .640 .286 .352 .557Table 9Detection accuracy for repairs on the basis of individual word tokens using the repetition filter.8E-CH 4E-CH NHOUR XFIRE G-MTGRepair tokens (%) 4.7 3.8 2.2 1.3 7.9Precision .88 .78 .25 .35 .91Recall .41 .32 .01 .04 .27F1-score .56 .45 .02 .08 .41ellipsis at the end of a sentence (e.g., neither do I, it seems to).
The performance forthe informal corpora (CALLHOME, GROUP MEETINGS) is better than that for the formalcorpora (NEWSHOUR, CROSSFIRE); this is related to the fact that the relative frequencyof false starts is markedly lower in these latter data sets and that these corpora aremore dissimilar to the training corpus (SWITCHBOARD).5.3.8 Repetition Detection.
The repetition detection component is concerned with(verbatim) repetitions within a speaker?s turn, the most frequently occurring case of allspeech repairs for informal dialogues (insertions and substitutions are comparativelyless frequent).
Repeated phrases can potentially be interrupted by other disfluencies,such as filled pauses or editing terms.
Repetition detection is performed with a scriptthat can identify repetitions of word/POS sequences of length one to four (longerrepetitions are extremely rare: on average, less than 1% of all repetitions).
Words thathave been marked as disfluent by the POS tagger are ignored when the repeatedsequences are considered, so we can correctly detect repetitions such as [ he said uh to+ he said to ] him.
.
.
.We are evaluating the precision, recall, and F1-scores for this component at thelevel of individual words when the POS tagger and the sentence boundary detectioncomponent are used.
Table 9 shows the results.
We see that for the informal subcor-pora, we get very good precision (only a few repetitions detected are incorrect), andrecall is in the 25?45% range (since we cannot detect substitution or insertion type ofrepairs).
The results for the formal subcorpora are considerably worse, so this filtershould probably not be used for corpora with as few repetitions as NEWSHOUR orCROSSFIRE.
We checked all of the 95 false positives of this evaluation and observed thatin the majority of cases (41%), the repetition was correctly detected but was not markedby the human annotator, since it might be considered a case of emphasis.
We believethat although some nuances of the sentence(s) might be lost, for the purpose of sum-marization it makes perfect sense to reduce this information.
Sometimes, individualwords are repeated for emphasis, sometimes whole sentences (e.g., ?Good./ Good./?
).In the following example from English CALLHOME, the emphasis is rather extreme:203 B: [...] How is the new person doing?
q/204 A: Very very very very very well.
/ [...]466Computational Linguistics Volume 28, Number 4Further, about 19% of false positives were correct but not annotated because they spanmultiple turns, and about 14% were erroneously missed by the human annotator.
Onlythe remaining cases (26%) were actual false positives, caused by incorrect POS tags(5%, typically an incorrectly tagged ?that/WDT that/DT?
sequence at the beginningof a relative clause) or incorrect sentence boundaries (21%).There have been attempts to get a more complete coverage of detection and cor-rection of all types of speech repairs (Heeman and Allen 1999).
We decided, however,to use a simple method here that works well for a large subset of cases and is veryefficient at the same time.5.3.9 Disfluency Correction in DIASUMM.
After detection, the correction of disfluen-cies is straightforward.
When DIASUMM generates its output from the ranked list ofsentences, it skips the false starts, the repetitions, and the words that were tagged withCO, DM, ET, or UH by the POS tagger.5.4 Sentence Boundary Detection5.4.1 Introduction.
The purpose of the sentence boundary detection component is toinsert linguistically meaningful sentence boundaries in the text, given a POS-taggedinput.
We consider all intraturn and interturn boundary positions for every speakerin a conversation.
We use the abbreviations EOS for end of complete sentence (E S in theSWITCHBOARD corpus) and NEOS for end of noncomplete sentence (N S in the SWITCHBOARDcorpus).
The frequency of sentence boundaries (with respect to the total number ofwords) is about 13.3%, most of the boundaries (almost 90%) being end markers ofcompleted sentences (SWITCHBOARD).5.4.2 Training and Testing.
We trained a C4.5 decision tree and computed its inputfeatures from a context of four words before and after a potential sentence boundary,motivated by the results of Gavalda`, Zechner, and Aist (1997).
Also following Gavalda`,Zechner, and Aist (1997), we used 60 trigger words with high predictive potential,employing the score computation method described in this article.The decision tree input features for every word position are as follows:?
POS tag (42 different tags)?
trigger word (60 different trigger words)?
turn boundary before this word??
if turn boundary: length of pause after last turn of same speakerSince NEOS boundaries occur very infrequently (only about 10% of all boundaries,which is only about 1% of all potential boundaries), we decided to merge this classwith the EOS class and report results for this combined class only (CEOS).
(We relied onthe false-start detection module described above to identify the NEOS sentences withinthis merged class of sentences after the sentence boundary classification.
)For training, we used 25,000 words from the Treebank-3 corpus; the test set sizewas 1,000 words.
Table 10 shows the results in detail for the various parameter com-binations.
We see that for good performance we need to know about one of these twofeatures: ?is there a turn boundary before this word??
or ?pause duration after lastturn from same speaker.
?467Zechner Automatic Summarization of DialoguesTable 10Sentence boundary detection accuracy (F1-score).With Interturn Pause Duration?
Yes NoWith Turn Boundary Info?
Yes No Yes NoTraining set .904 .903 .900 .884Test set .887 .884 .884 .825Table 11Inter- and intraturn boundary detection (BD) results on 1,000-word test set.Occurrence (%) Detection Accuracy (F1)Interturn non-BD 12 (1.2) .56Interturn BD 112 (11.3) .95Intraturn non-BD 809 (81.4) .99Intraturn BD 61 (6.1) .775.4.3 Effect of Imperfect POS Tagging.
To see how much influence an imperfect POStagging might have on these results, we POS-tagged the test set data using the POStagger described above.
For this and the following experiments, we increased thetraining corpus for the decision tree to 40,000 words.
The POS tagger accuracy for thistest set was about 95.3%, and the F1-score for CEOS was .882, which is 98.9% of .892 onperfect POS-tagged input.
This is encouraging, since it shows that the decision tree isnot very sensitive to the majority of POS errors.5.4.4 Interturn and Intraturn Boundaries.
In this analysis, we are interested in com-paring the detection of sentence boundaries between turns (interturn) to the detectionof boundaries within a turn (intraturn).
Table 11 shows the results of this analysis (sametest set as above).
As might be expected, the performance is very good for the twofrequent classes: sentence boundaries at the end of turns and nonboundaries withinturns (F1 > .95), but considerably worse for the two more infrequent cases.
The veryrare cases (around 1% only) of non?sentence boundaries at the end of turns (i.e.
turn?continuations) show the lowest performance (F1 = .56).5.4.5 Sentence Boundary Detection on Dialogue Corpus.
To get a picture of the realis-tic performance of the sentence boundary detection component, using the (imperfect)POS tagger and a faster, but slightly less accurate, decision tree,13 we evaluate thesentence boundary detection accuracy for all five subcorpora of our dialogue corpus.Table 12 provides the results of these experiments.
The results reflect a trend verysimilar to that for the SWITCHBOARD corpus, in that the two more frequent classes (in-terturn boundaries and intraturn nonboundaries) have high detection scores, whereasthe two more infrequent classes are less well detected.
Furthermore, we observe thatin cases in which the relative frequency of rare classes is further reduced, the classi-fication accuracy declines overproportionally (particularly for the rarest class of theinterturn nonboundaries).
Also, overall boundary detection is better for the two moreinformal corpora, CALLHOME and GROUP MEETINGS (F1 > .72).13 This decision tree uses a different type of encoding, but the same input features.468Computational Linguistics Volume 28, Number 4Table 12Boundary detection (BD) accuracy (F1) for five subcorpora (in parentheses: relative frequencyof class in percent).8E-CH 4E-CH NHOUR XFIRE G-MTGInterturn non-BD .51 (2.9) .31 (1.4) [0] (0.0) .10 (0.1) .06 (0.1)Interturn BD .84 (9.9) .89 (12.3) .93 (2.0) .89 (2.9) .93 (5.4)Intraturn non-BD .97 (80.7) .97 (79.5) .97 (91.8) .97 (91.2) .97 (87.6)Intraturn BD .60 (6.5) .65 (6.8) .56 (6.2) .42 (5.8) .56 (6.9)Overall BD .75 (16.4) .80 (19.1) .66 (8.2) .58 (8.7) .72 (12.4)Overall non-BD .95 (83.6) .96 (80.9) .97 (91.8) .97 (91.3) .96 (87.6)5.5 Cross-Speaker Information Linking5.5.1 Introduction.
One of the properties of multiparty dialogues is that shared infor-mation is created between dialogue participants.
The most obvious interactions of thiskind are question-answer (Q-A) pairs.
The purpose of this component is to create au-tomatically such coherent pieces of relevant information, which can then be extractedtogether while generating the summary.
The effects of such linkings on actual sum-maries can be seen in two dimensions: (1) increased local coherence in the summaryand (2) a potentially higher informativeness of the summary.
Since Q-A linking has aside effect in that other information will be lost with respect to a summary of the samelength without Q-A linking, the second claim is much less certain to hold than thefirst.
We investigated these questions in related work (Zechner and Lavie 2001) andfound that although Q-A linking does not significantly change the informativenessof summaries on average, it does increase summary coherence (fluency) significantly.In this section, we will be concerned with the following two intuitive subtasks ofQ-A linking: (1) identifying questions (Qs) and (2) finding their corresponding an-swers.5.5.2 Related Work.
Detecting a question and its corresponding answer can be seenas a subtask of the speech act detection and classification task.
Recently, Stolcke et al(2000) presented a comprehensive approach to dialogue act modeling with statisticaltechniques.
A good overview and comparison of recent related work can also be foundin Stolcke et al?s article.
Results from their evaluations on SWITCHBOARD data showthat word-based speech act classifiers usually perform better than prosody-based clas-sifiers, but that a model combination of the two approaches can yield an improvementin classification accuracy.5.5.3 Corpus Statistics.
For training of the question detection module, we used themanually annotated set of about 200,000 SWITCHBOARD speech acts14 (SAs);15 for train-ing of the answer detection component, we used the eight English CALLHOME dia-logues (8E-CH), which were manually annotated for Q-A pairs.
Although we wereaiming to detect all questions in the question detection module, the answer detectionmodule focuses on Q-A pairs only: We exclude all questions from consideration thatare not Yes-No- (YN) or Wh-questions (such as rhetorical or back-channel questions),14 In this work, the notions of speech acts and sentences can be considered equivalent.15 From the Johns Hopkins University Large Vocabulary Continuous Speech Recognition (LVCSR)Summer Workshop 1997.
Thanks to Klaus Ries for providing the data, which are also available fromhttp://www.colorado.edu/ling/jurafsky/ws97/.469Zechner Automatic Summarization of DialoguesTable 13Frequency of different types of questions in the 8E-CH data set.Sentences 2,211Wh-questions total 20. .
.
With immediate answers 15 (75%)YN-questions total 48. .
.
With immediate answers 38 (79%)Qs excluded for Q-A detection 15Questions total 83 (3.75%)as well as those that do not have an answer in the dialogue.
Thus we employ only68 pf the 83 questions marked in the 8E-CH data set for these evaluations.
Table 13provides the statistics concerning questions and answers for the 8E-CH subcorpusand shows that for a small but significant number of questions, the answer does notimmediately follow the question speech act (delayed answers).5.5.4 Automatic Question Detection.
We used two different methods, both trained onSWITCHBOARD data: (1) a speech act tagger16 and (2) a decision tree based on triggerword and part-of-speech information.Speech act tagger.
The speech act tagger tags one speech act at a time and hence canmake use only of speech act unigram information.
Within a speech act, it uses a lan-guage model based on POS and the 500 most frequent word/POS pairs.
It was trainedon the aforementioned SWITCHBOARD speech act training set.
It was not optimized forthe task of question detection.
Its typical run time for speech act classification is about10 speech acts per second.Decision tree question classifier.
The decision tree classifier (C4.5) uses the followingset of features: (1) POS and trigger word information for the first and last five tokensof each speech act;17 (2) speech act length, and (3) occurrence of POS bigrams.
The setof trigger words is the same as for the sentence boundary detection module.
The POSbigrams were designed to be most discriminative between question speech acts (q-SAs)and non?question speech acts (non-q-SAs).
The bigrams were obtained as follows:1.
For a balanced set of q-SAs and non-q-SAs (about 9,000 SAs each):Count all the POS bigrams in positions 1 .
.
.
5 and (n ?
4) .
.
.n (usingSTART and END for the first and last bigrams, respectively) and memorizeposition (beginning or end of SA) and type (q-SA vs. non-q-SA).2.
For all bigrams:(a) Add one to the count (to prevent division by zero).
(b) Divide the q-SA count by the non-q-SA count.
(c) If the ratio is smaller than one, invert it (ratio := 1/ratio).
(d) Multiply the result of (c) by the sum of q-SA count andnon-q-SA count.183.
Extract the 100 bigrams with the highest value.16 Thanks to Klaus Ries for providing us with the software.17 Shorter speech acts are padded with dummies.18 Leaving out this step favors low-frequency, high-discriminative bigrams too much and causes a slightreduction in overall Q-detection performance.470Computational Linguistics Volume 28, Number 4Table 14Question detection on the 8E-CH corpus using two different classifiers.SA Tagger Decision TreeOverall error 3.2% 4.7%Precision .57 .63Recall .61 .51F1 .59 .56Typical classification time (SAs/sec) 10 1,000Experiments and results.
The question detection decision tree was trained on a set ofabout 20,000 speech acts from the SWITCHBOARD corpus.
We first evaluated the speechact tagger and the decision tree classifier on the 8E-CH data set.
Whereas in the laterstage of answer detection, questions without answers and nonpropositional questionsare ignored, at this point we are interested in the detection of all annotated questionsin the corpus.
This also reflects the fact that the training set contains all possible typesof questions.Table 14 reports the results of the question detection experiments with the twoclassifiers used on the 8E-CH subcorpus.
We note that whereas the decision tree isperforming only slightly worse than the speech act tagger, its typical classificationtime is two orders of magnitude faster.
Based on these observations, we decided touse the question detection decision tree in the Q-A linking component of the DIASUMMsystem.5.5.5 Detecting the Answers.
After identifying which sentences are questions, thenext step is to identify the answers to them.
From the 8E-CH statistics of Table 13we observe that for more than 75% of the YN- and Wh-questions, the answer is tobe found in the first sentence of the speaker talking after the speaker uttering thequestion.
In the remainder of cases, the majority of answers are in the second (insteadof the first) sentence of the responding speaker.
Further, the speaker who has posed aquestion usually utters no (or only very few) sentences after the question is asked andbefore the next speaker starts talking.In addition to detecting sequential Q-A pairs, we also want to be able to detectsimple embedded questions, as shown in this example of a brief clarification dialogue:Q 1 A: When are we meeting then?Q 2 B: You mean tomorrow?3 A: Yes.4 B: At 4pm.We devise the following heuristics to detect answers to question speech acts whichhave been previously identified:?
If the first speaker change after the question occurs more than maxChgsentences after the question, the search is stopped and no Q-A pair isreturned.?
Answer hypotheses are sought for maximally maxSeek sentences after thefirst speaker change following the question, but not over interruptionsby any other speaker; that is, we check within a single speaker region471Zechner Automatic Summarization of Dialogues(this is the stopping criterion for the following two heuristics).
Anexception occurs if there is an embedded question in the first singlespeaker region: In that case, we look at the next region where a speakerdifferent from the initial Q-speaker is active.19?
Answers have to be minimally minAns words long; if they are shorter,we add the next sentence to the current answer hypothesis.?
Even if the minimum answer length is reached, the answer can beoptionally extended if at least one word in the answer matches a wordfrom the question (one of two different stop lists (StopShort, StopLong) orno stop list is used to remove function words from consideration).20We have these further restrictions for the case of embedded questions:1.
If we detect a potential embedded Q-A pair, the answer to thesurrounding question must immediately follow the answer to theembedded question (i.e., the region following the potential answerregion of the embedded question?sentence 4 in our aboveexample?must (1) not contain a question itself and (2) be from adifferent speaker than the surrounding question).2.
A crossover is prohibited; that is, we eliminate all pairs ?Qj, Al?
when apair ?Qi, Ak?
was already detected, with i < j < k < l (k, l being startindices of answer spans).The output of the algorithm is a list of triples ?Q, Astart , Aend?, where Q is thesentence ID of the question and Astart the first and Aend the last sentence of theanswer.
As mentioned above, we use only 68 of the 83 questions marked in the 8E-CH data set for these evaluations, since only these are YN- or Wh-questions thatactually have answers in the dialogue.
There are four possible outcomes for each triple:(1) irrelevant: a Q-A pair with an incorrectly hypothesized question (this is the faultof the question detection module, not of this heuristic); (2) missed: the answer wasmissed entirely; (3) completely correct: Aend coincides with the correct answer sentenceID; and (4) correct range: the answer is contained in the interval [Astart , Aend ] butdoes not coincide with Aend .
For the calculation of precision, recall, and F1-score, wecount classes (3) and (4) as correct and use the sum of all classes for the denominatorof precision and the total number of Q-A pairs (68 in this development set) as thedenominator of recall.To determine the best parameters, we varied them across a reasonable set of valuesand ran the answer detection script for all combinations of parameters.
The best results(with respect to F1-score) using questions detected by the speech act tagger and thedecision tree are reported in Table 15.
In the DIASUMM system, we use the followingoptimal parameter settings for the answer detection heuristics: maxChg = 2, maxSeek =4, minAns = 10, sim = on , stop = no.Finally, we evaluated the performance of both the Q-detection module and thecombined Q-A detection on all five subcorpora, using the decision tree for questiondetection; the results are reported in Table 16.
Except for the rather small NEWSHOUR19 This would be sentence 4 in the example above.20 StopLong contains 571 words, StopShort only 89 words, most of which are auxiliary verbs and fillerwords.472Computational Linguistics Volume 28, Number 4Table 15Q-A detection results using two different classifiers for question detection (68 Q-A pairs to bedetected).SA Tagger Decision TreeAll hypothesized Q-A pairs 80 54Correct [(3) and (4)] 42 31maxChg (1?5) 4 2maxSeek (2?4) 3?4 2?4minAns (1?10) 5?10 2?10Similarity extension (on/off) on onStop list (no/short/long) no/short no/shortPrecision .53 .57Recall .62 .46F1-score .57 .51Table 16Performance comparison for Q- and Q-A detection (Q-detection with the decision treequestion classifier).8E-CH 4E-CH NHOUR XFIRE G-MTGQ to detect 83 94 19 110 49Q-hypotheses 67 60 16 71 52Q-detection (F1) .56 .58 .80 .60 .59Q-A pairs to detect 68 69 18 79 32Q-A pair hypotheses 54 54 14 54 33Q-A detection (F1) .51 .60 .81 .51 .51corpus (with fewer than 20 questions or Q-A pairs to identify), the typical Q-detectionF1-score is around .6 and the Q-A detection F1-score around .5.
In two cases, the Q-Adetection performance is slightly better than the Q-detection performance.
This can beexplained by the fact that the answer detection algorithm prunes away a number ofQ-hypotheses, reducing the space for potential Q-A hypotheses.5.5.6 Q-A Detection within DIASUMM.
When we use the Q-A detection module aspart of the DIASUMM system, we want to ensure that (1) there are no Q-A pairs con-taining Q-sentences that are false starts and that (2) the initial part of an answer isnot lost in case the disfluency detection component marks some indicative words asdisfluencies.
To satisfy the first constraint, we block Q-detection of sentences that havebeen previously classified as false starts; as for the second constraint, we create a listof indicative words (relevant for YN-questions) that are not to be removed by thesummary generator if they appear in the beginning (leading five words) of answers.215.6 Sentence Ranking and Selection5.6.1 Introduction.
The sentence ranking and selection component is an implementa-tion of the MMR algorithm (Carbonell, Geng, and Goldstein 1997), applied to extract-ing the most relevant sentences from a topical segment of a dialogue.
The component?soutput in isolation serves as the MMR baseline for the global system evaluation.
Its21 The current list comprises the following words: no, yes, yeah, yep, sure, uh-huh, mhm, nope.473Zechner Automatic Summarization of Dialoguespurpose is to determine weights for terms and sentences, to rank the sentences ac-cording to their relevance within each topical segment of the dialogue, and finallyto select the sentences for the summary output according to their rank, as well as toother criteria, such as question-answer linkages, established by previous components.The selected sentences are presented to the user in text order.5.6.2 Tokenization.
In addition to the tokenization rules for the global system (sec-tion 5.2), we apply a simple six-character truncation for stemming and use a stop wordlist to eliminate frequent noncontent words.
In the experiments, we used the followingfive different stop word lists:?
the original SMART list (Salton 1971) (SMART-O)?
a manually edited stop list based on SMART (SMART-M)?
a stop list with all closed-class words from the POS tagger?s lexicon(POS-O)?
a manually edited stop list based on the POS tagger?s lexicon andfrequent closed-class words in the CALLHOME training corpus (POS-M)?
an empty stop list (EMPTY)5.6.3 Term and Sentence Weighting.
The basic idea for determining the most relevantsentences within a topical segment is as follows: First, we compute a vector of wordweights for the segment tfq (including all stemmed non?stop words) and do the samefor each sentence (tft), then we compute the similarity between sentence and segmentvectors for each sentence.
That way, sentences that have many words in common withthe segment vector are rewarded and receive a higher relevance weight.Whereas we compose the sentence vectors tft using direct term frequency counts,the weights for segment terms are determined according to one of the three formulaein equation (1) (freq, smax, and log), inspired by Cornell University?s SMART system(Salton 1971):tfi,s = fi,s or 0.5 + 0.5fi,sfsmaxor 1 + log fi,s, (1)where fi,s are the in-segment frequencies of a stem and fsmax are maximal segmentfrequencies of any stem in the segment.
Finally, we multiply an inverse document fre-quency (IDF) weight to tfs to obtain the segment vectors tfq, as shown in equations (2)and (3):tfi,q = tfi,sIDFi,s (2)IDFi,s = 1 + logNsegisegorNsegiseg.
(3)IDF values are computed with respect to a collection of topical segments, either thecurrent dialogue (DIALOGUE) or a set of dialogues (CORPUS).
Nseg is the total numberof topical segments in the IDF corpus, and iseg is the number of segments in which thetoken i appears at least once.
The effect of using IDF values is to boost those wordsthat are (relatively) unique to any given segment over those that are more evenlydistributed across the corpus.As stated above, the main algorithm is a version of the MMR algorithm (Carbonell,Geng, and Goldstein 1997; Carbonell and Goldstein 1998), which emphasizes sentences474Computational Linguistics Volume 28, Number 4that contain many highly weighted terms for the current segment (salience) and aresufficiently dissimilar to previously ranked sentences (diversity or antiredundancy).The MMR formula is given in equation (4):nextsentence = arg maxtnr,j(?sim1(query , tnr,j) ?
(1 ?
?)
maxtr,ksim2(tnr ,j , tr,k)).
(4)The MMR formula describes an iterative algorithm and states that the next sentence tobe put in the ranked list will be taken from the sentences that have not yet been ranked(tnr ).
This sentence is (1) maximally similar to a query and (2) maximally dissimilarto the sentences that have already been ranked (tr).
We use the topical segment wordvector tfq as query vector.
The ?
parameter (0.0 ?
?
?
1.0) is used to trade off theinfluence of salience against that of redundancy.Both similarity metrics (sim1, sim2) are inner vector products of stemmed-termfrequencies (equations (5) and (6)).
sim1 can be normalized in different ways: (1) toyield a cosine vector product (division by product of vector lengths), (2) division bynumber of content words,22 and (3) no normalization:sim1 =tfq tft| tfq||tft|ortfq tft1 +?i tfi,tor tfq tft (5)sim2 =tft1 tft2| tft1|| tft2|(6)Emphasis factors.
Every sentence?s similarity weight (sim1) can be (de)emphasized,based on a number of its properties.
We implemented optional emphasis factors for:?
Lead emphasis: for the leading n% of a segment?s sentences: sim ?1 = sim1l,with l being the lead factor.?
Q-A emphasis: for all sentences that belong to a question-answer pair:sim ?1 = sim1q, with q being the Q-A emphasis factor.?
False-start deemphasis: for all sentences that are false starts: sim ?1 = sim1f ,with f being the false-start factor.?
Speaker emphasis: for each individual speaker s, an emphasis factor se canbe defined: sim ?1 = sim1se for all sentences of speaker s.23These parameters can serve to fine-tune the system for particular applications or userpreferences.
For example, if the false starts are deemphasized, they are less likelyto trigger a question?s being linked to them in the linking process.
If questions andanswers are emphasized, more of them will show up in the summary, increasing itscoherence and readability.
In a situation in which a particular speaker?s statements areof higher interest than those of other speakers, his sentences can be emphasized, aswell.Since sim2 is a cosine vector product and hence in [0,1], we have to normalizesim1 to [0,1] as well to enable a proper application of the MMR formula.
For thisnormalization of sim1, we divide each sim1 score by the maximum of all sim1 scoresin a segment after initial computation and application of the various emphasis factorsdescribed here.22 To avoid division by zero, we add one to every sentence length.23 Speaker emphasis is not used in our evaluations.475Zechner Automatic Summarization of Dialogues5.6.4 Q-A Linking.
While generating the output summary from the MMR-rankedlist of sentences, whenever a question or an answer is encountered (detected previ-ously by the Q-A detection module), the corresponding answer/question is linked toit and moved up the relevance ranking list to immediately follow the current ques-tion/answer.
If the question-answer pair consists of more than two sentences, the link-ages are repeated until no further questions or answers can be added to the currentlinkage cluster.5.6.5 Summary Types.
DIASUMM can generate several different types of summaries,the two main versions being (1) the CLEAN summary, which is based on the outputof all DIASUMM components (disfluency detection, sentence boundary detection, Q-Alinking), and (2) the TRANS summary, in which all dialogue specific components areignored (essentially, this is an MMR summary of the original dialogue transcript).For the purpose of the global system evaluation, we use only these two versions ofsummaries, as well as LEAD baseline summaries, where the summary is formed byextracting the first n words from a topical segment.24Furthermore, the system can generate phrasal summaries, which render the sen-tences in the same ranking order as the CLEAN summary but reduce the output tonoun phrases and potentially other phrases, depending on the setting of parameters.25In Figure 2 we show an example of a set of LEAD, TRANS, CLEAN, and PHRASALsummaries.
The set was generated from the CALLHOME transcript presented in sec-tion 2.5.6.6 System Tuning.
This section describes how we arrive at an optimal parametersetting for each subcorpus (CALLHOME, NEWSHOUR, CROSSFIRE, GROUP MEETINGS).
Wewant to establish an MMR baseline for the global system evaluations with which wecan then compare the results of the entire DIASUMM system.
Note that for all the tuningexperiments described in this subsection, we did not make use of any other DIASUMMcomponents, namely, disfluency detection, sentence boundary detection, and question-answer linking.
All experiments were based on the human gold standard with respectto topical segments.
We used only the devtest set for the four subcorpora here (8E-CH = CALLHOME, DT-NH = NEWSHOUR, DT-XF = CROSSFIRE, and DT-MTG = GROUPMEETINGS).Since the length of turns varies widely, one could argue that an easy way to in-crease performance for the MMR baseline (which does not use automatic sentenceboundary detection) might be to split overly long turns evenly into shorter chunks.This was done by Valenza et al (1999), who experimented with lengths of 10?30 wordsper extract fragment.
We add this option as an additional parameter to the MMR base-line.
If the parameter is set to n words, turns with a length l ?
1.5n get cut into piecesof lengths n iteratively until the last remaining piece is l < 1.5n.Evaluation metric.
To evaluate the performance of this component, we use theword-based evaluation metric described in section 6.2, which gives the highest scoresto summaries containing words with the highest average relevance scores, as markedby human annotators.
We then average these scores over all topical segment sum-maries of a particular subcorpus.24 Note that LEAD summaries are to be distinguished from summaries in which lead emphasis is used, asdescribed above.
In the latter case, the segment-initial sentence weights are increased, whereas in theformer case, we strictly extract the leading n words from a given segment.25 To determine these constituents, we use the output of the chunk parser employed by the false startdetection component.476Computational Linguistics Volume 28, Number 4LEAD:1 a: Oh2 b: They didn?t know he was going to get shot butit was at a peace rally so I mean it just worked out3 b: I mean it was a good place for the poor guy to die I meanbecause it was you know right after the rally andeverything was on film and everything [...]TRANS:2 b: They didn?t know he was going to get shot butit was at a peace rally so I mean it just worked out3 b: I mean it was a good place for the poor guy to dieI mean because it was you know right after therally and everything was on film and everything11 b: Him [...]CLEAN:7 b: We just finished the thirty days mourning for him nowit?s everybody?s still in shock it?s terrible what?sgoing on over here31 b: What?s the reaction in america really do people care [...]34 a: Most I don?t know what I mean like the jewish communitya lot all of us were very upsetPHRASAL:4 b: it just worked ... it was a goodplace for the poor guy to die ... it was [...]7 b: we just finished the thirty days mourningfor him ... it?s ... everybody?s ...in shock it?s ... going ...31 b: ?s the reaction in america ... do people care ...34 a: i don?t know ... mean like the jewishcommunity a lot ... of us wereNote: The turn IDs are just indicating the relative position of the turns within the original text and do notalways correspond to the turn numbers of the original or to the turn numbers of the other summaries.
The.
.
.
marks the position in those sentences where the length threshold for a summary was reached.Figure 2Example summaries of 20% length: LEAD, TRANS, CLEAN and PHRASAL.Parameter tuning.
The system tuning proceeded in three phases, in which we heldthe summary size constant to 15% and optimized the following set of parameters:1.
Term weight type: freq, smax, log2.
Normalization: cos, length, none3.
IDF type: corpus, dialogue, none4.
IDF method: log, mult5.
Extract span: 10?30 or original turn (orig)477Zechner Automatic Summarization of DialoguesTable 17Optimally tuned parameters for MMR baseline system (tuning on devtest set subcorpora).8E-CH DT-NH DT-XF DT-MtgTerm weight type smax smax smax smaxNormalization cos none cos noneIDF type corpus corpus corpus corpusIDF method log log mult logExtract span 20 orig 25 origMMR-?
0.85 0.8 1.0 0.8Stop list SMART-M POS-M POS-M POS-MLead factor 1.0 1.0 1.0 2.06.
MMR-?
: 0.8?1.07.
Stop lists: SMART-O, SMART-M, POS-O, POS-M, EMPTY8.
Lead factor: 1.0?5.0 (applied to first 20% of sentences)Table 17 shows the parameter settings that were determined to be optimal for theMMR baseline system (TRANS summaries).5.7 System PerformanceThe majority of the system components are implemented in Perl5, except for theC4.5 decision tree (Quinlan 1992), the chunk parser (Ward 1991), and the POS tagger(Brill 1994), which were implemented in C by the respective authors.
We measured thesystem runtime on a 300 MHz Sun Ultra60 dual-processor workstation with 1 GB mainmemory, summarizing all 23 dialogue excerpts from our corpus.
The average runtimefor the whole system, including all of its components except for the topic segmentationmodule, was 17.8 seconds, and for the sentence selection component alone 7.0 seconds(per-dialogue average).
The average ratio of system runtime to dialogue duration was0.029 (2.9% of real speaking time).6.
Evaluation6.1 IntroductionTraditionally, summarization systems have been evaluated in two major ways: (1) in-trinsically, measuring the amount of the core information preserved from the originaltext (Kupiec, Pedersen, and Chen 1995; Teufel and Moens 1997), and (2) extrinsically,measuring how much the summary can benefit in accomplishing another task (e.g.,finding a document relevant to a query or classifying a document into a topical cate-gory) (Mani et al 1998).In this work, we focus on intrinsic evaluation exclusively.
That is, we want toassess how well the summaries preserve the essential information contained in theoriginal texts.
As other studies have shown (Rath, Resnick, and Savage 1961; Marcu1999), the level of agreement between human annotators about which passages tochoose to form a good summary is usually quite low.
Our own findings, reported insection 4.2.4, support this in that the intercoder agreement, here measured on a wordlevel, is rather low.We decided to minimize the bias that would result from selecting either a par-ticular human annotator, or even the manually created gold standard, as a reference478Computational Linguistics Volume 28, Number 4for automatic evaluation; instead, we weigh all annotations from all human codersequally.
Intuitively, we want to reward summaries that contain a high number ofwords considered to be relevant by most annotators.
We formalize this notion in thefollowing subsection.6.2 Evaluation MetricAll evaluations are based on topically coherent segments from the dialogue excerptsof our corpus.
As mentioned before, the segment boundaries were chosen from thehuman gold standard for the purpose of the global system evaluation.For each segment s, for each annotator a, and for each word position wi, we definea boolean word vector of annotations ws,a, each component ws,a,i being 1 if the wordwi is part of a nucleus IU or a satellite IU for that annotator and segment, and 0otherwise.
We then sum over all annotators?
annotation vectors and normalize themby the number of annotators per segment (A) to obtain the average relevance vectorfor segment s, rs:rs,i =?Aa=1 ws,a,iA.
(7)To obtain the summary accuracy score sas,n for any segment summary with length n,we multiply the boolean summary vector summs26 by the average relevance vector rs,and then divide this product by the sum of the n highest scores within rs (maximumachievable score), rsorts being the vector rs sorted by relevance weight in descendingorder:sas,n =summsrs?ni=1 rsorts,i(8)It is easy to see that the summary accuracy score always is in the interval [0.0, 1.0].6.3 Global System EvaluationWhereas section 5 was concerned with the design and evaluation of the individualsystem components, the goal here is to describe and analyze the quality of the globalsystem, with all its components combined.
In this section, we compare our DIASUMMsystem with the MMR baseline system, which operates without any dialogue-specificcomponents, and with the LEAD baseline.
We described the optimization and fine-tuning of the MMR system in subsection 5.6.6.
The second column of Table 18 presentsthe average relevance scores for this MMR baseline, averaged over the five summarysizes of 5%, 10%, 15%, 20%, and 25% length, for the four devtest set and the four evalset subcorpora; the first column of this table shows the results for the LEAD baseline.We used the optimized baseline MMR parameters and varied only the emphasisparameters for (1) false starts, (2) lead factor, and (3) Q-A sentences, to optimize theCLEAN summaries further.
(Again, for this step, we used only the devtest subcorpora.
)For each corpus in the devtest set, we determined the optimal parameter settings andreport the corresponding results also for the eval set subcorpora.
Column 3 in Table 18provides the results for this optimized DIASUMM system.
Further, in column 4, we pro-vide the summary accuracy averages for the human gold standard (nucleus IUs only,fixed-length summaries).
Table 19 shows the best emphasis parameter combinationsfor the DIASUMM summaries used in these evaluations.We determined the statistical differences between the DIASUMM system and thetwo baselines for the eval set, using the Wilcoxon rank sum test for each of the four26 Definition: 1 if summs,i is contained in the summary, 0 otherwise.479Zechner Automatic Summarization of DialoguesTable 18Average summary accuracy scores: devtest set and eval set subcorpora on optimizedparameters, comparing LEAD, MMR baseline, DIASUMM, and the human gold standard.Subcorpus LEAD MMR DIASUMM Gold [Nucleus IUs] (Size in %)8E-CH 0.463 0.545 0.597 0.709 (13.1)DT-NH 0.386 0.637 0.554 0.791 (20.9)DT-XF 0.516 0.595 0.541 0.764 (11.4)DT-MTG 0.488 0.594 0.606 0.705 (14.9)4E-CH 0.438 0.526 0.614 0.793 (12.9)EVAL-NH 0.692 0.526 0.506 0.850 (14.4)EVAL-XF 0.378 0.564 0.566 0.790 (13.9)EVAL-MTG 0.324 0.449 0.583 0.704 (16.0)Table 19Best emphasis parameters for the DIASUMM system, trained on the devtest set.Corpus False Start Q-A Lead FactorCALLHOME 0.5 1.0 2.0NEWSHOUR 0.5 2.0 1.0CROSSFIRE 0.5 1.0 1.0GROUP MEETINGS 0.5 1.0 3.0Table 20Average summary accuracy scores for different system configurations for the four differentsubcorpora.Corpus LEAD MMR DFF-ONLY SB-ONLY NO-QA DIASUMM4E-CH .438 .526 .599 .547 .603 .614EVAL-NH .692 .526 .551 .608 .619 .506EVAL-XF .378 .564 .528 .525 .537 .566EVAL-GMTG .324 .449 .488 .513 .584 .583subcorpora.
Comparisons were made for each of the five summary sizes within eachtopical segment.
For the CALLHOME and GROUP MEETINGS subcorpora, our DIASUMMsystem is significantly better than the MMR baseline (p < 0.01); for the two more formalsubcorpora, NEWSHOUR and CROSSFIRE, the differences between the performance ofthe two systems are not significant.
Except for on the NEWSHOUR subcorpus, both theMMR baseline and the DIASUMM system perform significantly better than the LEADbaseline.6.4 DiscussionTable 20 shows the average performance of the following six system configurations,averaged over all topical segments and all summary sizes (5?25% length summaries;in configurations 3?5 below, components used are in addition to the core MMR sum-marizer):1.
LEAD: using the first n% of the words in a segment2.
MMR: the MMR baseline (tuned; see above)480Computational Linguistics Volume 28, Number 43.
DFF-ONLY: using the disfluency detection components (POS tagger,false-start detection, repetition detection), but no sentence boundarydetection or question-answer linking4.
SB-ONLY: using the sentence boundary detection module, but no otherdialogue-specific modules5.
NO-QA: a combination of DFF-ONLY and SB-ONLY (all preprocessingcomponents used except for question-answer linking)6.
DIASUMM: complete system with all components (all disfluency detectioncomponents, sentence boundary detection, and Q-A linking)We observe that in all subcorpora, except for CROSSFIRE, the addition of eitherthe disfluency components or the sentence boundary component improves the sum-mary accuracy over that of the MMR baseline.
As we would expect, given the muchhigher frequency of disfluencies in the two informal subcorpora (CALLHOME, GROUPMEETINGS), the relative performance increase of DFF-ONLY over the MMR baseline ismuch higher here (about 10?15%) than for the two more formal subcorpora (5% andbelow).
Looking at the performance increase of SB-ONLY, we find marked improve-ments over the MMR baseline for those two subcorpora that use the true original turnboundaries in the MMR baseline: GROUP MEETINGS and NEWSHOUR (>10%); for thetwo other subcorpora, the improvement is below 5%.
Furthermore, the combinationof the disfluency detection and sentence boundary detection components (NO-QA)improves the results over the configurations DFF-ONLY and SB-ONLY.The situation is much less uniform when we add the question-answer detectioncomponent (this then corresponds to the full DIASUMM system): In the CROSSFIREcorpus, we have the largest performance increase (we also have the highest relativefrequency of question speech acts here).
For the two informal corpora, the changeis only minor; for NEWSHOUR, the performance decreases substantially.
We showedin Zechner and Lavie (2001), however, that in general, for dialogues with relativelyfrequent Q-A exchanges, the accuracy of a summary (informativeness) does not changesignificantly when the Q-A detection component is applied.
On the other hand, the(local) coherence of the summary does increase significantly, but we cannot measurethis increase with the evaluation criterion of summary accuracy used here.To conclude, we have shown that using dialogue-specific components, with thepossible exception of the Q-A detection module, can help in creating more accuratesummaries for more informal, casual, spontaneous dialogues.
When more formal con-versations (which may even be partially scripted), containing relatively few disfluen-cies, are involved, either a simple LEAD method or a standard MMR summarizer willbe much harder to improve upon.7.
Discussion and Directions for Future WorkThe problem of how to generate readable and concise summaries automatically forspoken dialogues of unrestricted domains involves many challenges that need to beaddressed.
Some of the research issues are similar or identical to those faced in summa-rizing written texts (such as topic segmentation, determining the most salient/relevantinformation, anaphora resolution, summary evaluation), but other additional dimen-sions are added on top of this list, including speech disfluency detection, sentenceboundary detection, cross-speaker information linking, and coping with imperfectspeech recognition.
The line of argument of this article has been that whereas using a481Zechner Automatic Summarization of Dialoguestraditional approach for written text summarization (such as the MMR-based sentenceselection component within DIASUMM) may be a good starting point, addressing thedialogue-specific issues is key for obtaining better summaries for informal genres.We decided to focus on the three problems of (1) speech disfluency detection,(2) sentence boundary detection, and (3) cross-speaker information linking and im-plemented trainable system components to address each of these issues.
Both theevaluations of the individual components of our spoken-dialogue summarization sys-tem and the global evaluations as well have shown that we can successfully make useof the SWITCHBOARD corpus (LDC 1999b) to train a system that works well on twoother genres of informal dialogues, CALLHOME and GROUP MEETINGS.
We conjecturethat the reasons why the DIASUMM system was not able to improve over the MMRbaseline for the two other corpora, which are more formal, lies in their very nature ofbeing of a quite different genre: the NEWSHOUR and CROSSFIRE corpora have longerturns and sentences, as well as fewer disfluencies.
We would also conjecture that theirsentence structures are more complex than what we typically find in the other corporaof more colloquial, spontaneous conversations.Future work will have to address the issue of whether the availability of train-ing data for more formal dialogues (in size and annotation style comparable to theSWITCHBOARD corpus, though) could lead to an improvement in performance on thosedata sets, as well, or if even then a standard written-text-based summarizer would behard to improve upon.Given the complexity of the task, we had to make a number of simplifying assump-tions, most notably about the input data for our system: We use perfect transcripts byhumans instead of ASR transcripts, which, for these genres, typically show word errorrates (WERs) ranging from 15% to 35%.
Previous related work (Valenza et al 1999;Zechner and Waibel 2000b) demonstrated that the actual WERs in summaries gen-erated from ASR output are usually substantially lower than the full-ASR-transcriptWER and can further be reduced by taking acoustically derived confidence scores intoaccount.We further did not explore the potential improvements of components as well asof the system overall when prosodic information such as stress and pitch is added asan input feature.
Past work in related fields (Shriberg et al 1998; Shriberg et al 2000)suggests that particularly for ASR input, noticeable improvements might be achievablewhen such input is provided.Although presegmentation of the input into topically coherent segments certainlyis a useful step in summarization for any kind of texts (written or spoken), we havenot addressed and discussed this issue in this article.Finally, we think that there is more work needed in the area of automaticallyderiving discourse structures for spoken dialogues in unrestricted domains, even ifthe text spans covered might be only local (because of a lack of global discourseplans).
We believe that a summarizer, in addition to knowing about the interactivelyconstructed and coherent pieces of information (such as in question-answer pairs),could make good use of such structured information and be better guided in makingits selections for summary generation.
In addition, this discourse structure might aidmodules that perform automatic anaphora detection and resolution.8.
ConclusionsWe have motivated, implemented, and evaluated an approach for automatically cre-ating extract summaries for open-domain spoken dialogues in informal and formalgenres of multiparty conversations.
Our dialogue summarization system DIASUMM482Computational Linguistics Volume 28, Number 4uses trainable components to detect and remove speech disfluencies (making the out-put more readable and less noisy), to determine sentence boundaries (creating suitabletext spans for summary generation), and to link cross-speaker information units (al-lowing for increased summary coherence).We used a corpus of 23 dialogue excerpts from four different genres (80 topical seg-ments, about 47,000 words) for system development and evaluation and the disfluency-annotated SWITCHBOARD corpus (LDC 1999b) for training of the dialogue-specific com-ponents.
Our corpus was annotated by six human coders for topical boundaries andrelevant text spans for summaries.
Additionally, we had annotations made for disflu-encies, sentence boundaries, question speech acts, and the corresponding answers tothose question speech acts.In a global system evaluation we compared the MMR-based sentence selectioncomponent with the DIASUMM system using all of its components discussed in this arti-cle.
The results showed that (1) both a baseline MMR system as well as DIASUMM createbetter summaries than a LEAD baseline (except for NEWSHOUR) and that (2) DIASUMMperforms significantly better than the baseline MMR system for the informal dialoguecorpora (CALLHOME and GROUP MEETINGS).AcknowledgmentsWe are grateful to Alex Waibel, Alon Lavie,Jaime Carbonell, Vibhu Mittal, JadeGoldstein, Klaus Ries, Lori Levin, andMarsal Gavalda` for many discussions,suggestions, and comments regarding thiswork.
We also want to commend the corpusannotators for their efforts.
Finally, we wantto thank the four anonymous reviewers fortheir detailed feedback on a preliminarydraft, which greatly helped improve thisarticle.
This work was performed while theauthor was affiliated with the LanguageTechnologies Institute at Carnegie MellonUniversity and was supported in part bygrants from the U.S. Department of Defense.ReferencesAlexandersson, Jan and Peter Poller.
1998.Towards multilingual protocol generationfor spontaneous speech dialogues.
InProceedings of INLG-98,Niagara-on-the-Lake, Canada, August.Aone, Chinatsu, Mary Ellen Okurowski,and James Gorlinsky.
1997.
Trainable,scalable summarization using robust NLPand machine learning.
In ACL/EACL-97Workshop on Intelligent and Scalable TextSummarization, Madrid.Arons, Barry.
1994.
Pitch-based emphasisdetection for segmenting speech.
InProceedings of ICSLP-94, pages 1931?1934.Berger, Adam L. and Vibhu O. Mittal.
2000.OCELOT: A system for summarizing Webpages.
In Proceedings of the 23rdACM-SIGIR Conference.Bett, Michael, Ralph Gross, Hua Yu, XiaojinZhu, Yue Pan, Jie Yang, and Alex Waibel.2000.
Multimodal meeting tracker.
InProceedings of the Conference onContent-Based Multimedia Information Access(RIAO-2000), Paris, April.Brill, Eric.
1994.
Some advances intransformation-based part of speechtagging.
In Proceedings of AAAI-94.Carbonell, Jaime, Yibing Geng, and JadeGoldstein.
1997.
Automatedquery-relevant summarization anddiversity-based reranking.
In Proceedingsof the IJCAI-97 Workshop on AI and DigitalLibraries, Nagoya, Japan.Carbonell, Jaime and Jade Goldstein.
1998.The use of MMR, diversity-basedreranking for reordering documents andproducing summaries.
In Proceedings of the21st ACM-SIGIR International Conference onResearch and Development in InformationRetrieval, Melbourne, Australia.Carletta, Jean, Amy Isard, Stephen Isard,Jacqueline C. Kowtko, GwynethDoherty-Sneddon, and Anne H.Anderson.
1997.
The reliability of adialogue structure coding scheme.Computational Linguistics, 23(1):13?31.Chen, Francine R. and Margaret Withgott.1992.
The use of emphasis toautomatically summarize a spokendiscourse.
In Proceedings of ICASSP-92,pages 229?332.Cohen, Jacob.
1960.
A coefficient ofagreement for nominal scales.
Educationaland Psychological Measurement, 20(1):37?46.Davies, Mark and Joseph L. Fleiss.
1982.Measuring agreement for multinomialdata.
Biometrics, 38:1047?1051, December.Garofolo, John S., Ellen M. Voorhees, CedricG.
P. Auzanne, and Vincent M. Stanford.483Zechner Automatic Summarization of Dialogues1999.
Spoken document retrieval: 1998evaluation and investigation of newmetrics.
In Proceedings of the ESCAWorkshop: Accessing Information in SpokenAudio, pages 1?7, Cambridge, UK, April.Garofolo, John S., Ellen M. Voorhees,Vincent M. Stanford, and Karen SparckJones.
1997.
TREC-6 1997 spokendocument retrieval track overview andresults.
In Proceedings of the 1997 TREC-6Conference, pages 83?91, Gaithersburg,MD, November.Gavalda`, Marsal, Klaus Zechner, andGregory Aist.
1997.
High performancesegmentation of spontaneous speechusing part of speech and trigger wordinformation.
In Proceedings of the fifth ANLPConference, Washington, DC, pages 12?15.Godfrey, J. J., E. C. Holliman, andJ.
McDaniel.
1992.
SWITCHBOARD:Telephone speech corpus for research anddevelopment.
In Proceedings of ICASSP-92,volume 1, pages 517?520.Grosz, Barbara J. and Candace L. Sidner.1986.
Attention, intentions, and thestructure of discourse.
ComputationalLinguistics, 12(3):175?204.Hearst, Marti A.
1997.
TextTiling:Segmenting text into multi-paragraphsubtopic passages.
ComputationalLinguistics, 23(1):33?64.Heeman, Peter A. and James F. Allen.
1999.Speech repairs, intonational phrases, anddiscourse markers: Modeling speakers?utterances in spoken dialogue.Computational Linguistics, 25(4):527?571.Hirschberg, Julia, Steve Whittaker, DonHindle, Fernando Pereira, and AmitSinghal.
1999.
Finding information inaudio: A new paradigm for audiobrowsing/retrieval.
In Proceedings of theESCA Workshop: Accessing Information inSpoken Audio, pages 117?122, Cambridge,UK, April.Hori, Chiori and Sadaoki Furui.
2000.Automatic speech summarization basedon word significance and linguisticlikelihood.
In Proceedings of ICASSP-00,pages 1579?1582, Istanbul, Turkey, June.Jurafsky, Daniel, Rebecca Bates, NoahCoccaro, Rachel Martin, Marie Meteer,Klaus Ries, Elizabeth Shriberg, AndreasStolcke, Paul Taylor, and Carol VanEss-Dykema.
1998.
SwitchBoard discourselanguage modeling project: Final report.Research Note 30, Center for Languageand Speech Processing, Johns HopkinsUniversity, Baltimore, MD.Kameyama, Megumi, and I. Arima.
1994.Coping with aboutness complexity ininformation extraction from spokendialogues.
In Proceedings of ICSLP 94,pages 87?90, Yokohama, Japan.Kameyama, Megumi, Goh Kawai, and IsaoArima.
1996.
A real-time system forsummarizing human-human spontaneousspoken dialogues.
In Proceedings ofICSLP-96, pages 681?684.Knight, Kevin and Daniel Marcu.
2000.Statistics-based summarization?Step one:Sentence compression.
In Proceedings of the17th National Conference of the AAAI.Koumpis, Konstantinos and Steve Renals.2000.
Transcription and summarization ofvoicemail speech.
In Proceedings ofICSLP-00, pages 688?691, Beijing, China,October.Krippendorff, Klaus.
1980.
Content Analysis.Sage, Beverly Hills, CA.Kupiec, J., J. Pedersen, and F. Chen.
1995.
Atrainable document summarizer.
InProceedings of the 18th ACM-SIGIRConference, pages 68?73.Lavie, Alon, Alex Waibel, Lori Levin,Michael Finke, Donna Gates, MarsalGavalda`, Torsten Zeppenfeld, andPuming Zhan.
1997.
Janus III:Speech-to-speech translation in multiplelanguages.
In IEEE International Conferenceon Acoustics, Speech and Signal Processing,Munich.Levin, Lori, Klaus Ries, Ann Thyme?-Gobbel,and Alon Lavie.
1999.
Tagging of speechacts and dialogue games in Spanish callhome.
In Proceedings of the ACL-99Workshop on Discourse Tagging, CollegePark, MD.Linguistic Data Consortium (LDC).
1996.CallHome and CallFriend LVCSRdatabases.Linguistic Data Consortium (LDC).
1999a.Addendum to the part-of-speech taggingguidelines for the Penn Treebank project(Modifications for the SwitchBoardcorpus).
LDC CD-ROM LDC99T42.Linguistic Data Consortium (LDC).
1999b.Treebank-3: Databases of disfluencyannotated Switchboard transcripts.
LDCCD-ROM LDC99T42.Mani, Inderjeet, David House, Gary Klein,Lynette Hirschman, Leo Obrst, ThereseFirmin, Michael Chrzanowski, and BethSundheim.
1998.
The TIPSTER SUMMACtext summarization evaluation.
TechnicalReport MTR 98W0000138, MitreCorporation, October 1998.Mani, Inderjeet and Mark T. Maybury,editors.
1999.
Advances in Automatic TextSummarization.
MIT Press, Cambridge.Marcu, Daniel.
1999.
Discourse trees aregood indicators of importance in text.
InI.
Mani and M. T. Maybury, editors,484Computational Linguistics Volume 28, Number 4Advances in Automatic Text Summarization.MIT Press, Cambridge, pages 123?136.Meteer, Marie, Ann Taylor, RobertMacIntyre, and Rukmini Iyer.
1995.Dysfluency annotation stylebook for theSwitchboard corpus.
Linguistic DataConsortium (LDC) CD-ROM LDC99T42.Miike, Seiji, Etuso Itoh, Kenji Onon, andKazuo Sumita.
1994.
A full-text retrievalsystem with a dynamic abstractgeneration function.
In Proceedings of the17th ACM-SIGIR Conference, pages 318?327.Nakatani, Christine H. and Julia Hirschberg.1994.
A corpus-based study of repair cuesin spontaneous speech.
Journal of theAcoustic Society of America, 95(3):1603?1616.Passonneau, Rebecca J. and Diane J. Litman.1997.
Discourse segmentation by humanand automated means.
ComputationalLinguistics, 23(1):103?139.Quinlan, J. Ross.
1992.
C4.5: Programs forMachine Learning.
Morgan Kaufmann, SanMateo, CA.Rath, G. J., A. Resnick, and T. R. Savage.1961.
The formation of abstracts by theselection of sentences.
AmericanDocumentation, 12(2):139?143.Reimer, U. and U. Hahn.
1988.
Textcondensation as knowledge baseabstraction.
In Proceedings of the fourthConference on Artificial IntelligenceApplications, pages 338?344, San Diego.Reithinger, Norbert, Michael Kipp, RalfEngel, and Jan Alexandersson.
2000.Summarizing multilingual spokennegotiation dialogues.
In Proceedings of the38th Conference of the Association forComputational Linguistics, pages 310?317,Hong Kong, China, October.Ries, Klaus, Lori Levin, Liza Valle, AlonLavie, and Alex Waibel.
2000.
Shallowdiscourse genre annotation inCALLHOME Spanish.
In Proceedings of theSecond Conference on Language Resources andEvaluation (LREC-2000), Athens,May/June.Rose, Ralph Leon.
1998.
The CommunicativeValue of Filled Pauses in Spontaneous Speech.Ph.D.
thesis, University of Birmingham,Birmingham, UK.Salton, Gerard, editor.
1971.
The SMARTRetrieval System?Experiments in AutomaticText Processing.
Prentice Hall, EnglewoodCliffs, NJ.Santorini, Beatrice.
1990.
Part-of-SpeechTagging guidelines for the Penn Treebankproject.
Linguistic Data Consortium(LDC) CD-ROM LDC99T42.Shriberg, Elizabeth E. 1994.
Preliminaries to aTheory of Speech Disfluencies.
Ph.D. thesis,University of Berkeley, Berkeley.Shriberg, Elizabeth, Rebecca Bates, AndreasStolcke, Paul Taylor, Daniel Jurafsky,Klaus Ries, Noah Coccaro, Rachel Martin,Marie Meteer, and Carol VanEss-Dykema.
1998.
Can prosody aid theautomatic classification of dialog acts inconversational speech?
Language andSpeech, 41(3?4):439?487.Shriberg, Elizabeth, Andreas Stolcke, DilekHakkani-Tu?r, and Go?khan Tu?r.
2000.Prosody-based automatic segmentation ofspeech into sentences and topics.
SpeechCommunication, 32(1?2):127?154.Stifelman, Lisa J.
1995.
A discourse analysisapproach to structured speech.
InAAAI-95 Spring Symposium on EmpiricalMethods in Discourse Interpretation andGeneration, Stanford, March.Stolcke, Andreas, Klaus Ries, Noah Coccaro,Elizabeth Shriberg, Rebecca Bates, DanielJurafsky, Paul Taylor, Rachel Martin,Carol Van Ess-Dykema, and MarieMeteer.
2000.
Dialogue act modeling forautomatic tagging and recognition ofconversational speech.
ComputationalLinguistics, 26(3):339?373.Stolcke, Andreas and Elizabeth Shriberg.1996.
Automatic linguistic segmentationof conversational speech.
In Proceedings ofICSLP-96, pages 1005?1008.Stolcke, Andreas, Elizabeth Shriberg,Rebecca Bates, Mari Ostendorf, DilekHakkani, Madeleine Plauche, Go?khanTu?r, and Yu Lu.
1998.
Automaticdetection of sentence boundaries anddisfluencies based on recognized words.In Proceedings of ICSLP-98, volume 5,pages 2247?2250, Sydney, December.Teufel, Simone and Marc Moens.
1997.Sentence extraction as a classification task.In ACL/EACL-97 Workshop on Intelligentand Scalable Text Summarization, Madrid.Valenza, Robin, Tony Robinson, MarianneHickey, and Roger Tucker.
1999.Summarisation of spoken audio throughinformation extraction.
In Proceedings ofthe ESCA Workshop: Accessing Information inSpoken Audio, pages 111?116, Cambridge,UK, April.Wahlster, Wolfgang.
1993.Verbmobil?Translation of face-to-facedialogs.
In Proceedings of MT Summit IV,Kobe, Japan.Waibel, Alex, Michael Bett, and MichaelFinke.
1998.
Meeting browser: Trackingand summarizing meetings.
In Proceedingsof the DARPA Broadcast News Workshop.Ward, Wayne.
1991.
Understandingspontaneous speech: The PHOENIXsystem.
In Proceedings of ICASSP-91,485Zechner Automatic Summarization of Dialoguespages 365?367.Whittaker, Steve, Julia Hirschberg, JohnChoi, Don Hindle, Fernando Pereira, andAmit Singhal.
1999.
SCAN: Designing andevaluating user interfaces to supportretrieval from speech archives.
InProceedings of the 22nd ACM-SIGIRInternational Conference on Research andDevelopment in Information Retrieval,pages 26?33, Berkeley, August.Zechner, Klaus and Alon Lavie.
2001.Increasing the coherence of spokendialogue summaries by cross-speakerinformation linking.
In Proceedings of theNAACL-01 Workshop on AutomaticSummarization, pages 22?31, Pittsburgh,June.Zechner, Klaus and Alex Waibel.
2000a.DIASUMM: Flexible summarization ofspontaneous dialogues in unrestricteddomains.
In Proceedings of COLING-2000,pages 968?974, Saarbru?cken, Germany,July/August.Zechner, Klaus and Alex Waibel.
2000b.Minimizing word error rate in textualsummaries of spoken language.
InProceedings of the First Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics (NAACL-2000),pages 186?193, Seattle, April/May.
