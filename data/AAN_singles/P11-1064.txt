Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 632?641,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAn Unsupervised Model for Joint Phrase Alignment and ExtractionGraham Neubig1,2 Taro Watanabe2, Eiichiro Sumita2, Shinsuke Mori1, Tatsuya Kawahara11Graduate School of Informatics, Kyoto UniversityYoshida Honmachi, Sakyo-ku, Kyoto, Japan2National Institute of Information and Communication Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, JapanAbstractWe present an unsupervised model for jointphrase alignment and extraction using non-parametric Bayesian methods and inversiontransduction grammars (ITGs).
The key con-tribution is that phrases of many granulari-ties are included directly in the model throughthe use of a novel formulation that memorizesphrases generated not only by terminal, butalso non-terminal symbols.
This allows fora completely probabilistic model that is ableto create a phrase table that achieves com-petitive accuracy on phrase-based machinetranslation tasks directly from unaligned sen-tence pairs.
Experiments on several languagepairs demonstrate that the proposed modelmatches the accuracy of traditional two-stepword alignment/phrase extraction approachwhile reducing the phrase table to a fractionof the original size.1 IntroductionThe training of translation models for phrase-based statistical machine translation (SMT) systems(Koehn et al, 2003) takes unaligned bilingual train-ing data as input, and outputs a scored table ofphrase pairs.
This phrase table is traditionally gen-erated by going through a pipeline of two steps, firstgenerating word (or minimal phrase) alignments,then extracting a phrase table that is consistent withthese alignments.However, as DeNero and Klein (2010) note, thistwo step approach results in word alignments thatare not optimal for the final task of generatingphrase tables that are used in translation.
As a so-lution to this, they proposed a supervised discrimi-native model that performs joint word alignment andphrase extraction, and found that joint estimation ofword alignments and extraction sets improves bothword alignment accuracy and translation results.In this paper, we propose the first unsuper-vised approach to joint alignment and extraction ofphrases at multiple granularities.
This is achievedby constructing a generative model that includesphrases at many levels of granularity, from minimalphrases all the way up to full sentences.
The modelis similar to previously proposed phrase alignmentmodels based on inversion transduction grammars(ITGs) (Cherry and Lin, 2007; Zhang et al, 2008;Blunsom et al, 2009), with one important change:ITG symbols and phrase pairs are generated inthe opposite order.
In traditional ITG models, thebranches of a biparse tree are generated from a non-terminal distribution, and each leaf is generated bya word or phrase pair distribution.
As a result, onlyminimal phrases are directly included in the model,while larger phrases must be generated by heuris-tic extraction methods.
In the proposed model, ateach branch in the tree, we first attempt to gener-ate a phrase pair from the phrase pair distribution,falling back to ITG-based divide and conquer strat-egy to generate phrase pairs that do not exist (or aregiven low probability) in the phrase distribution.We combine this model with the Bayesian non-parametric Pitman-Yor process (Pitman and Yor,1997; Teh, 2006), realizing ITG-based divide andconquer through a novel formulation where thePitman-Yor process uses two copies of itself as a632base measure.
As a result of this modeling strategy,phrases of multiple granularities are generated, andthus memorized, by the Pitman-Yor process.
Thismakes it possible to directly use probabilities of thephrase model as a replacement for the phrase tablegenerated by heuristic extraction techniques.Using this model, we perform machine transla-tion experiments over four language pairs.
We ob-serve that the proposed joint phrase alignment andextraction approach is able to meet or exceed resultsattained by a combination of GIZA++ and heuristicphrase extraction with significantly smaller phrasetable size.
We also find that it achieves superiorBLEU scores over previously proposed ITG-basedphrase alignment approaches.2 A Probabilistic Model for Phrase TableExtractionThe problem of SMT can be defined as finding themost probable target sentence e for the source sen-tence f given a parallel training corpus ?E ,F?e?
= argmaxeP (e|f , ?E ,F?
).We assume that there is a hidden set of parameters?
learned from the training data, and that e is condi-tionally independent from the training corpus given?.
We take a Bayesian approach, integrating over allpossible values of the hidden parameters:P (e|f , ?E ,F?)
=?
?P (e|f , ?
)P (?|?E ,F?).
(1)If ?
takes the form of a scored phrase table, wecan use traditional methods for phrase-based SMT tofind P (e|f , ?)
and concentrate on creating a modelfor P (?|?E ,F?).
We decompose this posterior prob-ability using Bayes law into the corpus likelihoodand parameter prior probabilitiesP (?|?E ,F?)
?
P (?E ,F?|?
)P (?
).In Section 3 we describe an existing method, andin Section 4 we describe our proposed method formodeling these two probabilities.3 Flat ITG ModelThere has been a significant amount of work inmany-to-many alignment techniques (Marcu andWong (2002), DeNero et al (2008), inter alia), andin particular a number of recent works (Cherry andLin, 2007; Zhang et al, 2008; Blunsom et al, 2009)have used the formalism of inversion transductiongrammars (ITGs) (Wu, 1997) to learn phrase align-ments.
By slightly limit reordering of words, ITGsmake it possible to exactly calculate probabilitiesof phrasal alignments in polynomial time, which isa computationally hard problem when arbitrary re-ordering is allowed (DeNero and Klein, 2008).The traditional flat ITG generative probabil-ity for a particular phrase (or sentence) pairPflat(?e, f?
; ?x, ?t) is parameterized by a phrase ta-ble ?t and a symbol distribution ?x.
We use the fol-lowing generative story as a representative of the flatITG model.1.
Generate symbol x from the multinomial distri-bution Px(x; ?x).
x can take the values TERM,REG, or INV.2.
According to the x take the following actions.
(a) If x = TERM, generate a phrase pair fromthe phrase table Pt(?e, f?
; ?t).
(b) If x = REG, a regular ITG rule, gener-ate phrase pairs ?e1, f1?
and ?e2, f2?
fromPflat, and concatenate them into a singlephrase pair ?e1e2, f1f2?.
(c) If x = INV, an inverted ITG rule, followsthe same process as (b), but concatenatef1 and f2 in reverse order ?e1e2, f2f1?.By taking the product of Pflat over every sentencein the corpus, we are able to calculate the likelihoodP (?E ,F?|?)
=??e,f??
?E,F?Pflat(?e, f?
; ?
).We will refer to this model as FLAT.3.1 Bayesian ModelingWhile the previous formulation can be used as-is inmaximum likelihood training, this leads to a degen-erate solution where every sentence is memorized asa single phrase pair.
Zhang et al (2008) and otherspropose dealing with this problem by putting a priorprobability P (?x, ?t) on the parameters.633We assign ?x a Dirichlet prior1, and assign thephrase table parameters ?t a prior using the Pitman-Yor process (Pitman and Yor, 1997; Teh, 2006),which is a generalization of the Dirichlet processprior used in previous research.
It is expressed as?t ?PY (d, s, Pbase) (2)where d is the discount parameter, s is the strengthparameter, and Pbase is the base measure.
The dis-count d is subtracted from observed counts, andwhen it is given a large value (close to one), lessfrequent phrase pairs will be given lower relativeprobability than more common phrase pairs.
Thestrength s controls the overall sparseness of the dis-tribution, and when it is given a small value the dis-tribution will be sparse.
Pbase is the prior probabilityof generating a particular phrase pair, which we de-scribe in more detail in the following section.Non-parametric priors are well suited for mod-eling the phrase distribution because every time aphrase is generated by the model, it is ?memorized?and given higher probability.
Because of this, com-mon phrase pairs are more likely to be re-used (therich-get-richer effect), which results in the induc-tion of phrase tables with fewer, but more helpfulphrases.
It is important to note that only phrasesgenerated by Pt are actually memorized and givenhigher probability by the model.
In FLAT, only min-imal phrases generated after Px outputs the terminalsymbol TERM are generated from Pt, and thus onlyminimal phrases are memorized by the model.While the Dirichlet process is simply the Pitman-Yor process with d = 0, it has been shown that thediscount parameter allows for more effective mod-eling of the long-tailed distributions that are oftenfound in natural language (Teh, 2006).
We con-firmed in preliminary experiments (using the datadescribed in Section 7) that the Pitman-Yor processwith automatically adjusted parameters results in su-perior alignment results, outperforming the sparseDirichlet process priors used in previous research2.The average gain across all data sets was approxi-mately 0.8 BLEU points.1The value of ?
had little effect on the results, so we arbi-trarily set ?
= 1.2We put weak priors on s (Gamma(?
= 2, ?
= 1)) andd (Beta(?
= 2, ?
= 2)) for the Pitman-Yor process, and set?
= 1?10 for the Dirichlet process.3.2 Base MeasurePbase in Equation (2) indicates the prior probabilityof phrase pairs according to the model.
By choosingthis probability appropriately, we can incorporateprior knowledge of what phrases tend to be alignedto each other.
We calculate Pbase by first choosingwhether to generate an unaligned phrase pair (where|e| = 0 or |f | = 0) according to a fixed probabil-ity pu3, then generating from Pba for aligned phrasepairs, or Pbu for unaligned phrase pairs.For Pba, we adopt a base measure similar to thatused by DeNero et al (2008):Pba(?e, f?)
=M0(?e, f?)Ppois(|e|;?
)Ppois(|f |;?
)M0(?e, f?)
=(Pm1(f |e)Puni(e)Pm1(e|f)Puni(f))12 .Ppois is the Poisson distribution with the averagelength parameter ?.
As long phrases lead to spar-sity, we set ?
to a relatively small value to allowus to bias against overly long phrases4.
Pm1 is theword-based Model 1 (Brown et al, 1993) probabil-ity of one phrase given the other, which incorporatesword-based alignment information as prior knowl-edge in the phrase translation probability.
We takethe geometric mean5of the Model 1 probabilities inboth directions to encourage alignments that are sup-ported by both models (Liang et al, 2006).
It shouldbe noted that while Model 1 probabilities are used,they are only soft constraints, compared with thehard constraint of choosing a single word alignmentused in most previous phrase extraction approaches.For Pbu, if g is the non-null phrase in e and f , wecalculate the probability as follows:Pbu(?e, f?)
= Puni(g)Ppois(|g|;?
)/2.Note that Pbu is divided by 2 as the probability isconsidering null alignments in both directions.4 Hierarchical ITG ModelWhile in FLAT only minimal phrases were memo-rized by the model, as DeNero et al (2008) note3We choose 10?2, 10?3, or 10?10 based on which valuegave the best accuracy on the development set.4We tune ?
to 1, 0.1, or 0.01 based on which value gives thebest performance on the development set.5The probabilities of the geometric mean do not add to one,but we found empirically that even when left unnormalized, thisprovided much better results than the using the arithmetic mean,which is more theoretically correct.634and we confirm in the experiments in Section 7, us-ing only minimal phrases leads to inferior transla-tion results for phrase-based SMT.
Because of this,previous research has combined FLAT with heuris-tic phrase extraction, which exhaustively combinesall adjacent phrases permitted by the word align-ments (Och et al, 1999).
We propose an alterna-tive, fully statistical approach that directly modelsphrases at multiple granularities, which we will referto as HIER.
By doing so, we are able to do away withheuristic phrase extraction, creating a fully proba-bilistic model for phrase probabilities that still yieldscompetitive results.Similarly to FLAT, HIER assigns a probabilityPhier(?e, f?
; ?x, ?t) to phrase pairs, and is parame-terized by a phrase table ?t and a symbol distribu-tion ?x.
The main difference from the generativestory of the traditional ITG model is that symbolsand phrase pairs are generated in the opposite order.While FLAT first generates branches of the derivationtree using Px, then generates leaves using the phrasedistribution Pt, HIER first attempts to generate thefull sentence as a single phrase from Pt, then fallsback to ITG-style derivations to cope with sparsity.We allow for this within the Bayesian ITG contextby defining a new base measure Pdac (?divide-and-conquer?)
to replace Pbase in Equation (2), resultingin the following distribution for ?t.
?t ?
PY (d, s, Pdac) (3)Pdac essentially breaks the generation of a sin-gle longer phrase into two generations of shorterphrases, allowing even phrase pairs for whichc(?e, f?)
= 0 to be given some probability.
Thegenerative process of Pdac, similar to that of Pflatfrom the previous section, is as follows:1.
Generate symbol x from Px(x; ?x).
x can takethe values BASE, REG, or INV.2.
According to x take the following actions.
(a) If x = BASE, generate a new phrase pairdirectly from Pbase of Section 3.2.
(b) If x = REG, generate ?e1, f1?
and ?e2, f2?from Phier, and concatenate them into asingle phrase pair ?e1e2, f1f2?.Figure 1: A word alignment (a), and its derivations ac-cording to FLAT (b), and HIER (c).
Solid and dotted linesindicate minimal and non-minimal pairs respectively, andphrases are written under their corresponding instance ofPt.
The pair hate/cou?te is generated from Pbase.
(c) If x = INV, follow the same process as(b), but concatenate f1 and f2 in reverseorder ?e1e2, f2f1?.A comparison of derivation trees for FLAT andHIER is shown in Figure 1.
As previously de-scribed, FLAT first generates from the symbol dis-tribution Px, then from the phrase distribution Pt,while HIER generates directly from Pt, which fallsback to divide-and-conquer based on Px when nec-essary.
It can be seen that while Pt in FLAT only gen-erates minimal phrases, Pt in HIER generates (andthus memorizes) phrases at all levels of granularity.4.1 Length-based Parameter TuningThere are still two problems with HIER, one theo-retical, and one practical.
Theoretically, HIER con-tains itself as its base measure, and stochastic pro-cess models that include themselves as base mea-sures are deficient, as noted in Cohen et al (2010).Practically, while the Pitman-Yor process in HIERshares the parameters s and d over all phrase pairs inthe model, long phrase pairs are much more sparse635Figure 2: Learned discount values by phrase pair length.than short phrase pairs, and thus it is desirable toappropriately adjust the parameters of Equation (2)according to phrase pair length.In order to solve these problems, we reformulatethe model so that each phrase length l = |f |+|e| hasits own phrase parameters ?t,l and symbol parame-ters ?x,l, which are given separate priors:?t,l ?
PY (s, d, Pdac,l)?x,l ?
Dirichlet(?
)We will call this model HLEN.The generative story is largely similar to HIERwith a few minor changes.
When we generate a sen-tence, we first choose its length l according to a uni-form distribution over all possible sentence lengthsl ?
Uniform(1, L),where L is the size |e| + |f | of the longest sentencein the corpus.
We then generate a phrase pair fromthe probability Pt,l(?e, f?)
for length l. The basemeasure for HLEN is identical to that of HIER, withone minor change: when we fall back to two shorterphrases, we choose the length of the left phrase fromll ?
Uniform(1, l ?
1), set the length of the rightphrase to lr = l?ll, and generate the smaller phrasesfrom Pt,ll and Pt,lr respectively.It can be seen that phrases at each length are gen-erated from different distributions, and thus the pa-rameters for the Pitman-Yor process will be differ-ent for each distribution.
Further, as ll and lr mustbe smaller than l, Pt,l no longer contains itself as abase measure, and is thus not deficient.An example of the actual discount values learnedin one of the experiments described in Section 7is shown in Figure 2.
It can be seen that, as ex-pected, the discounts for short phrases are lower thanthose of long phrases.
In particular, phrase pairs oflength up to six (for example, |e| = 3, |f | = 3) aregiven discounts of nearly zero while larger phrasesare more heavily discounted.
We conjecture that thisis related to the observation by Koehn et al (2003)that using phrases where max(|e|, |f |) ?
3 causesignificant improvements in BLEU score, while us-ing larger phrases results in diminishing returns.4.2 ImplementationPrevious research has used a variety of samplingmethods to learn Bayesian phrase based alignmentmodels (DeNero et al, 2008; Blunsom et al, 2009;Blunsom and Cohn, 2010).
All of these techniquesare applicable to the proposed model, but we chooseto apply the sentence-based blocked sampling ofBlunsom and Cohn (2010), which has desirable con-vergence properties compared to sampling singlealignments.
As exhaustive sampling is too slow forpractical purpose, we adopt the beam search algo-rithm of Saers et al (2009), and use a probabilitybeam, trimming spans where the probability is atleast 1010 times smaller than that of the best hypoth-esis in the bucket.One important implementation detail that is dif-ferent from previous models is the management ofphrase counts.
As a phrase pair ta may have beengenerated from two smaller component phrases tband tc, when a sample containing ta is removed fromthe distribution, it may also be necessary to decre-ment the counts of tb and tc as well.
The ChineseRestaurant Process representation of Pt (Teh, 2006)lends itself to a natural and easily implementable so-lution to this problem.
For each table representing aphrase pair ta, we maintain not only the number ofcustomers sitting at the table, but also the identitiesof phrases tb and tc that were originally used whengenerating the table.
When the count of the tableta is reduced to zero and the table is removed, thecounts of tb and tc are also decremented.5 Phrase ExtractionIn this section, we describe both traditional heuris-tic phrase extraction, and the proposed model-basedextraction method.636Figure 3: The phrase, block, and word alignments usedin heuristic phrase extraction.5.1 Heuristic Phrase ExtractionThe traditional method for heuristic phrase extrac-tion from word alignments exhaustively enumeratesall phrases up to a certain length consistent with thealignment (Och et al, 1999).
Five features are usedin the phrase table: the conditional phrase proba-bilities in both directions estimated using maximumlikelihood Pml(f |e) and Pml(e|f), lexical weight-ing probabilities (Koehn et al, 2003), and a fixedpenalty for each phrase.
We will call this heuristicextraction from word alignments HEUR-W. Theseword alignments can be acquired through the stan-dard GIZA++ training regimen.We use the combination of our ITG-based align-ment with traditional heuristic phrase extraction asa second baseline.
An example of these alignmentsis shown in Figure 3.
In model HEUR-P, minimalphrases generated from Pt are treated as aligned, andwe perform phrase extraction on these alignments.However, as the proposed models tend to align rel-atively large phrases, we also use two other tech-niques to create smaller alignment chunks that pre-vent sparsity.
We perform regular sampling of thetrees, but if we reach a minimal phrase generatedfrom Pt, we continue traveling down the tree un-til we reach either a one-to-many alignment, whichwe will call HEUR-B as it creates alignments simi-lar to the block ITG, or an at-most-one alignment,which we will call HEUR-W as it generates wordalignments.
It should be noted that forcing align-ments smaller than the model suggests is only usedfor generating alignments for use in heuristic extrac-tion, and does not affect the training process.5.2 Model-Based Phrase ExtractionWe also propose a method for phrase table ex-traction that directly utilizes the phrase probabil-ities Pt(?e, f?).
Similarly to the heuristic phrasetables, we use conditional probabilities Pt(f |e)and Pt(e|f), lexical weighting probabilities, and aphrase penalty.
Here, instead of using maximumlikelihood, we calculate conditional probabilities di-rectly from Pt probabilities:Pt(f |e) = Pt(?e, f?)/?{f?
:c(?e,f??
)?1}Pt(?e, f??
)Pt(e|f) = Pt(?e, f?)/?{e?:c(?e?,f?
)?1}Pt(?e?, f?
).To limit phrase table size, we include only phrasepairs that are aligned at least once in the sample.We also include two more features: the phrasepair joint probability Pt(?e, f?
), and the averageposterior probability of each span that generated?e, f?
as computed by the inside-outside algorithmduring training.
We use the span probability as itgives a hint about the reliability of the phrase pair.
Itwill be high for common phrase pairs that are gen-erated directly from the model, and also for phrasesthat, while not directly included in the model, arecomposed of two high probability child phrases.It should be noted that while for FLAT and HIER Ptcan be used directly, as HLEN learns separate modelsfor each length, we must combine these probabilitiesinto a single value.
We do this by settingPt(?e, f?)
= Pt,l(?e, f?)c(l)/L?l?=1c(l?
)for every phrase pair, where l = |e|+ |f | and c(l) isthe number of phrases of length l in the sample.We call this model-based extraction method MOD.5.3 Sample CombinationAs has been noted in previous works, (Koehn et al,2003; DeNero et al, 2006) exhaustive phrase extrac-tion tends to out-perform approaches that use syn-tax or generative models to limit phrase boundaries.DeNero et al (2006) state that this is because gen-erative models choose only a single phrase segmen-tation, and thus throw away many good phrase pairsthat are in conflict with this segmentation.Luckily, in the Bayesian framework it is simple toovercome this problem by combining phrase tables637from multiple samples.
This is equivalent to approx-imating the integral over various parameter configu-rations in Equation (1).
In MOD, we do this by takingthe average of the joint probability and span prob-ability features, and re-calculating the conditionalprobabilities from the averaged joint probabilities.6 Related WorkIn addition to the previously mentioned phrasealignment techniques, there has also been a signif-icant body of work on phrase extraction (Moore andQuirk (2007), Johnson et al (2007a), inter alia).DeNero and Klein (2010) presented the first workon joint phrase alignment and extraction at multiplelevels.
While they take a supervised approach basedon discriminative methods, we present a fully unsu-pervised generative model.A generative probabilistic model where longerunits are built through the binary combination ofshorter units was proposed by deMarcken (1996) formonolingual word segmentation using the minimumdescription length (MDL) framework.
Our work dif-fers in that it uses Bayesian techniques instead ofMDL, and works on two languages, not one.Adaptor grammars, models in which non-terminals memorize subtrees that lie below them,have been used for word segmentation or othermonolingual tasks (Johnson et al, 2007b).
The pro-posed method could be thought of as synchronousadaptor grammars over two languages.
However,adaptor grammars have generally been used to spec-ify only two or a few levels as in the FLAT model inthis paper, as opposed to recursive models such asHIER or many-leveled models such as HLEN.
Oneexception is the variational inference method foradaptor grammars presented by Cohen et al (2010)that is applicable to recursive grammars such asHIER.
We plan to examine variational inference forthe proposed models in future work.7 Experimental EvaluationWe evaluate the proposed method on translationtasks from four languages, French, German, Span-ish, and Japanese, into English.de-en es-en fr-en ja-enTM (en) 1.80M 1.62M 1.35M 2.38MTM (other) 1.85M 1.82M 1.56M 2.78MLM (en) 52.7M 52.7M 52.7M 44.7MTune (en ) 49.8k 49.8k 49.8k 68.9kTune (other) 47.2k 52.6k 55.4k 80.4kTest (en) 65.6k 65.6k 65.6k 40.4kTest (other) 62.7k 68.1k 72.6k 48.7kTable 1: The number of words in each corpus for TM andLM training, tuning, and testing.7.1 Experimental SetupThe data for French, German, and Spanish are fromthe 2010 Workshop on Statistical Machine Transla-tion (Callison-Burch et al, 2010).
We use the newscommentary corpus for training the TM, and thenews commentary and Europarl corpora for trainingthe LM.
For Japanese, we use data from the NTCIRpatent translation task (Fujii et al, 2008).
We usethe first 100k sentences of the parallel corpus for theTM, and the whole parallel corpus for the LM.
De-tails of both corpora can be found in Table 1.
Cor-pora are tokenized, lower-cased, and sentences ofover 40 words on either side are removed for TMtraining.
For both tasks, we perform weight tuningand testing on specified development and test sets.We compare the accuracy of our proposed methodof joint phrase alignment and extraction using theFLAT, HIER and HLEN models, with a baseline ofusing word alignments from GIZA++ and heuris-tic phrase extraction.
Decoding is performed usingMoses (Koehn and others, 2007) using the phrasetables learned by each method under consideration,as well as standard bidirectional lexical reorderingprobabilities (Koehn et al, 2005).
Maximum phraselength is limited to 7 in all models, and for the LMwe use an interpolated Kneser-Ney 5-gram model.For GIZA++, we use the standard training reg-imen up to Model 4, and combine alignmentswith grow-diag-final-and.
For the proposedmodels, we train for 100 iterations, and use the finalsample acquired at the end of the training process forour experiments using a single sample6.
In addition,6For most models, while likelihood continued to increasegradually for all 100 iterations, BLEU score gains plateaued af-ter 5-10 iterations, likely due to the strong prior information638de-en es-en fr-en ja-enAlign Extract # Samp.
BLEU Size BLEU Size BLEU Size BLEU SizeGIZA++ HEUR-W 1 16.62 4.91M 22.00 4.30M 21.35 4.01M 23.20 4.22MFLAT MOD 1 13.48 136k 19.15 125k 17.97 117k 16.10 89.7kHIER MOD 1 16.58 1.02M 21.79 859k 21.50 751k 23.23 723kHLEN MOD 1 16.49 1.17M 21.57 930k 21.31 860k 23.19 820kHIER MOD 10 16.53 3.44M 21.84 2.56M 21.57 2.63M 23.12 2.21MHLEN MOD 10 16.51 3.74M 21.69 3.00M 21.53 3.09M 23.20 2.70MTable 2: BLEU score and phrase table size by alignment method, extraction method, and samples combined.
Boldnumbers are not significantly different from the best result according to the sign test (p < 0.05) (Collins et al, 2005).we also try averaging the phrase tables from the lastten samples as described in Section 5.3.7.2 Experimental ResultsThe results for these experiments can be found in Ta-ble 2.
From these results we can see that when usinga single sample, the combination of using HIER andmodel probabilities achieves results approximatelyequal to GIZA++ and heuristic phrase extraction.This is the first reported result in which an unsu-pervised phrase alignment model has built a phrasetable directly from model probabilities and achievedresults that compare to heuristic phrase extraction.
Itcan also be seen that the phrase table created by theproposed method is approximately 5 times smallerthan that obtained by the traditional pipeline.In addition, HIER significantly outperforms FLATwhen using the model probabilities.
This confirmsthat phrase tables containing only minimal phrasesare not able to achieve results that compete withphrase tables that use multiple granularities.Somewhat surprisingly, HLEN consistentlyslightly underperforms HIER.
This indicatespotential gains to be provided by length-basedparameter tuning were outweighed by losses dueto the increased complexity of the model.
Inparticular, we believe the necessity to combineprobabilities from multiple Pt,l models into a singlephrase table may have resulted in a distortion of thephrase probabilities.
In addition, the assumptionthat phrase lengths are generated from a uniformdistribution is likely too strong, and further gainsprovided by Pbase.
As iterations took 1.3 hours on a singleprocessor, good translation results can be achieved in approxi-mately 13 hours, which could further reduced using distributedsampling (Newman et al, 2009; Blunsom et al, 2009).FLAT HIERMOD 17.97 117k 21.50 751kHEUR-W 21.52 5.65M 21.68 5.39MHEUR-B 21.45 4.93M 21.41 2.61MHEUR-P 21.56 4.88M 21.47 1.62MTable 3: Translation results and phrase table size for var-ious phrase extraction techniques (French-English).could likely be achieved by more accurate modelingof phrase lengths.
We leave further adjustments tothe HLEN model to future work.It can also be seen that combining phrase tablesfrom multiple samples improved the BLEU scorefor HLEN, but not for HIER.
This suggests that forHIER, most of the useful phrase pairs discovered bythe model are included in every iteration, and the in-creased recall obtained by combining multiple sam-ples does not consistently outweigh the increasedconfusion caused by the larger phrase table.We also evaluated the effectiveness of model-based phrase extraction compared to heuristic phraseextraction.
Using the alignments from HIER, we cre-ated phrase tables using model probabilities (MOD),and heuristic extraction on words (HEUR-W), blocks(HEUR-B), and minimal phrases (HEUR-P) as de-scribed in Section 5.
The results of these ex-periments are shown in Table 3.
It can be seenthat model-based phrase extraction using HIER out-performs or insignificantly underperforms heuris-tic phrase extraction over all experimental settings,while keeping the phrase table to a fraction of thesize of most heuristic extraction methods.Finally, we varied the size of the parallel corpusfor the Japanese-English task from 50k to 400k sen-639Figure 4: The effect of corpus size on the accuracy (a) andphrase table size (b) for each method (Japanese-English).tences and measured the effect of corpus size ontranslation accuracy.
From the results in Figure 4(a), it can be seen that at all corpus sizes, the re-sults from all three methods are comparable, withinsignificant differences between GIZA++ and HIERat all levels, and HLEN lagging slightly behind HIER.Figure 4 (b) shows the size of the phrase table in-duced by each method over the various corpus sizes.It can be seen that the tables created by GIZA++ aresignificantly larger at all corpus sizes, with the dif-ference being particularly pronounced at larger cor-pus sizes.8 ConclusionIn this paper, we presented a novel approach to jointphrase alignment and extraction through a hierar-chical model using non-parametric Bayesian meth-ods and inversion transduction grammars.
Machinetranslation systems using phrase tables learned di-rectly by the proposed model were able to achieveaccuracy competitive with the traditional pipeline ofword alignment and heuristic phrase extraction, thefirst such result for an unsupervised model.For future work, we plan to refine HLEN to usea more appropriate model of phrase length thanthe uniform distribution, particularly by attemptingto bias against phrase pairs where one of the twophrases is much longer than the other.
In addition,we will test probabilities learned using the proposedmodel with an ITG-based decoder.
We will also ex-amine the applicability of the proposed model in thecontext of hierarchical phrases (Chiang, 2007), orin alignment using syntactic structure (Galley et al,2006).
It is also worth examining the plausibilityof variational inference as proposed by Cohen et al(2010) in the alignment context.AcknowledgmentsThis work was performed while the first authorwas supported by the JSPS Research Fellowship forYoung Scientists.ReferencesPhil Blunsom and Trevor Cohn.
2010.
Inducing syn-chronous grammars with slice sampling.
In Proceed-ings of the Human Language Technology: The 11thAnnual Conference of the North American Chapter ofthe Association for Computational Linguistics.Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A Gibbs sampler for phrasal syn-chronous grammar induction.
In Proceedings of the47th Annual Meeting of the Association for Computa-tional Linguistics, pages 782?790.Peter F. Brown, Vincent J.Della Pietra, Stephen A. DellaPietra, and Robert.
L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19:263?311.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar F. Zaidan.2010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint 5th Workshopon Statistical Machine Translation and MetricsMATR,pages 17?53.Colin Cherry and Dekang Lin.
2007.
Inversion transduc-tion grammar for joint phrasal translation modeling.In Proceedings of the NAACL Workshop on Syntax andStructure in Machine Translation.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Shay B. Cohen, David M. Blei, and Noah A. Smith.2010.
Variational inference for adaptor grammars.
InProceedings of the Human Language Technology: The64011th Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 564?572.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics,pages 531?540.Carl de Marcken.
1996.
Unsupervised Language Acqui-sition.
Ph.D. thesis, Massachusetts Institute of Tech-nology.John DeNero and Dan Klein.
2008.
The complexity ofphrase alignment problems.
In Proceedings of the 46thAnnual Meeting of the Association for ComputationalLinguistics, pages 25?28.John DeNero and Dan Klein.
2010.
Discriminative mod-eling of extraction sets for machine translation.
In Pro-ceedings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 1453?1463.John DeNero, Dan Gillick, James Zhang, and Dan Klein.2006.
Why generative phrase models underperformsurface heuristics.
In Proceedings of the 1st Workshopon Statistical Machine Translation, pages 31?38.John DeNero, Alex Bouchard-Co?te?, and Dan Klein.2008.
Sampling alignment structure under a Bayesiantranslation model.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, pages 314?323.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, andTakehito Utsuro.
2008.
Overview of the patent trans-lation task at the NTCIR-7 workshop.
In Proceedingsof the 7th NTCIR Workshop Meeting on Evaluation ofInformation Access Technologies, pages 389?400.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of the 44th Annual Meeting of the Association forComputational Linguistics, pages 961?968.J.
Howard Johnson, Joel Martin, George Foster, andRoland Kuhn.
2007a.
Improving translation qualityby discarding most of the phrasetable.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007b.
Adaptor grammars: A framework for spec-ifying compositional nonparametric Bayesian models.Advances in Neural Information Processing Systems,19:641.Philipp Koehn et al 2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedings ofthe 45th Annual Meeting of the Association for Com-putational Linguistics.Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofthe Human Language Technology Conference (HLT-NAACL), pages 48?54.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descriptionfor the 2005 IWSLT speech translation evaluation.
InProceedings of the International Workshop on SpokenLanguage Translation.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the HumanLanguage Technology Conference - North AmericanChapter of the Association for Computational Linguis-tics Annual Meeting (HLT-NAACL), pages 104?111.Daniel Marcu andWilliamWong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
pages 133?139.Robert C. Moore and Chris Quirk.
2007.
An iteratively-trained segmentation-free phrase translation model forstatistical machine translation.
In Proceedings ofthe 2nd Workshop on Statistical Machine Translation,pages 112?119.David Newman, Arthur Asuncion, Padhraic Smyth, andMax Welling.
2009.
Distributed algorithms fortopic models.
Journal of Machine Learning Research,10:1801?1828.Franz Josef Och, Christoph Tillmann, and Hermann Ney.1999.
Improved alignment models for statistical ma-chine translation.
In Proceedings of the 4th Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 20?28.Jim Pitman and Marc Yor.
1997.
The two-parameterPoisson-Dirichlet distribution derived from a stablesubordinator.
The Annals of Probability, 25(2):855?900.Markus Saers, Joakim Nivre, and Dekai Wu.
2009.Learning stochastic bracketing inversion transductiongrammars with a cubic time biparsing algorithm.
InProceedings of the The 11th International Workshopon Parsing Technologies.Yee Whye Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proceed-ings of the 44th Annual Meeting of the Association forComputational Linguistics.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008.
Bayesian learning of non-compositional phrases with synchronous parsing.
Pro-ceedings of the 46th Annual Meeting of the Associationfor Computational Linguistics, pages 97?105.641
