Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 63?68,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsPARMA: A Predicate Argument AlignerTravis Wolfe, Benjamin Van Durme, Mark Dredze, Nicholas Andrews,Charley Beller, Chris Callison-Burch, Jay DeYoung, Justin Snyder,Jonathan Weese, Tan Xu?, and Xuchen YaoHuman Language Technology Center of ExcellenceJohns Hopkins University, Baltimore, Maryland USA?University of Maryland, College Park, Maryland USAAbstractWe introduce PARMA, a system for cross-document, semantic predicate and argu-ment alignment.
Our system combines anumber of linguistic resources familiar toresearchers in areas such as recognizingtextual entailment and question answering,integrating them into a simple discrimina-tive model.
PARMA achieves state of theart results on an existing and a new dataset.We suggest that previous efforts have fo-cussed on data that is biased and too easy,and we provide a more difficult datasetbased on translation data with a low base-line which we beat by 17% F1.1 IntroductionA key step of the information extraction pipelineis entity disambiguation, in which discovered en-tities across many sentences and documents mustbe organized to represent real world entities.
TheNLP community has a long history of entity dis-ambiguation both within and across documents.While most information extraction work focuseson entities and noun phrases, there have been afew attempts at predicate, or event, disambigua-tion.
Commonly a situational predicate is taken tocorrespond to either an event or a state, lexicallyrealized in verbs such as ?elect?
or nominaliza-tions such as ?election?.
Similar to entity coref-erence resolution, almost all of this work assumesunanchored mentions: predicate argument tuplesare grouped together based on coreferent events.The first work on event coreference dates back toBagga and Baldwin (1999).
More recently, thistask has been considered by Bejan and Harabagiu(2010) and Lee et al (2012).
As with unanchoredentity disambiguation, these methods rely on clus-tering methods and evaluation metrics.Another view of predicate disambiguation seeksto link or align predicate argument tuples to an ex-isting anchored resource containing references toevents or actions, similar to anchored entity dis-ambiguation (entity linking) (Dredze et al, 2010;Han and Sun, 2011).
The most relevant, and per-haps only, work in this area is that of Roth andFrank (2012) who linked predicates across docu-ment pairs, measuring the F1 of aligned pairs.Here we present PARMA, a new system for pred-icate argument alignment.
As opposed to Roth andFrank, PARMA is designed as a a trainable plat-form for the incorporation of the sort of lexical se-mantic resources used in the related areas of Rec-ognizing Textual Entailment (RTE) and QuestionAnswering (QA).
We demonstrate the effective-ness of this approach by achieving state of the artperformance on the data of Roth and Frank despitehaving little relevant training data.
We then showthat while the ?lemma match?
heuristic provides astrong baseline on this data, this appears to be anartifact of their data creation process (which washeavily reliant on word overlap).
In response, weevaluate on a new and more challenging dataset forpredicate argument alignment derived from multi-ple translation data.
We release PARMA as a newframework for the incorporation and evaluation ofnew resources for predicate argument alignment.12 PARMAPARMA (Predicate ARguMent Aligner) is apipelined system with a wide variety of featuresused to align predicates and arguments in two doc-uments.
Predicates are represented as mentionspans and arguments are represented as corefer-ence chains (sets of mention spans) provided byin-document coreference resolution systems suchas included in the Stanford NLP toolkit.
Resultsindicated that the chains are of sufficient qualityso as not to limit performance, though future work1https://github.com/hltcoe/parma63RF?
Australian [police]1 have [arrested]2 a man in the western city of Perth over an alleged [plot]3 to [bomb]4 Israeli diplomatic[buildings]5 in the country , police and the suspect s [lawyer]6 [said]7?
Federal [police]1 have [arrested]2 a man over an [alleged]5 [plan]3 to [bomb]4 Israeli diplomatic [posts]8 in Australia , thesuspect s [attorney]6 [said]7 TuesdayLDC MTC?
As I [walked]1 to the [veranda]2 side , I [saw]2 that a [tent]3 is being decorated for [Mahfil-e-Naat]4 -LRB- A [get-together]5in which the poetic lines in praise of Prophet Mohammad are recited -RRB-?
I [came]1 towards the [balcony]2 , and while walking over there I [saw]2 that a [camp]3 was set up outside for the [Naatia]4[meeting]5 .Figure 1: Example of gold-standard alignment pairs from Roth and Frank?s data set and our data setcreated from the LDC?s Multiple Translation Corpora.
The RF data set exhibits high lexical overlap,where most of the alignments are between identical words like police-police and said-said.
The LDCMTC was constructed to increase lexical diversity, leading to more challenging alignments like veranda-balcony and tent-campmay relax this assumption.We refer to a predicate or an argument as an?item?
with type predicate or argument.
An align-ment between two documents is a subset of allpairs of items in either documents with the sametype.2 We call the two documents being alignedthe source document S and the target documentT .
Items are referred to by their index, and ai,j is abinary variable representing an alignment betweenitem i in S and item j in T .
A full alignment is anassignment ~a = {aij : i ?
NS , j ?
NT }, whereNS and NT are the set of item indices for S and Trespectively.We train a logistic regression model on exam-ple alignmentsand maximize the likelihood of adocument alignment under the assumption that theitem alignments are independent.
Our objectiveis to maximize the log-likelihood of all p(S, T )with an L1 regularizer (with parameter ?).
Afterlearning model parameters w by regularized max-imum likelihood on training data, we introducinga threshold ?
on alignment probabilities to get aclassifier.
We perform line search on ?
and choosethe value that maximizes F1 on dev data.
Train-ing was done using the Mallet toolkit (McCallum,2002).2.1 FeaturesThe focus of PARMA is the integration of a diverserange of features based on existing lexical seman-tic resources.
We built PARMA on a supervisedframework to take advantage of this wide varietyof features since they can describe many differentcorrelated aspects of generation.
The followingfeatures cover the spectrum from high-precision2Note that type is not the same thing as part of speech: weallow nominal predicates like ?death?.to high-recall.
Each feature has access to the pro-posed argument or predicate spans to be linked andthe containing sentences as context.
While we usesupervised learning, some of the existing datasetsfor this task are very small.
For extra training data,we pool material from different datasets and usethe multi-domain split feature space approach tolearn dataset specific behaviors (Daume?, 2007).Features in general are defined over mentionspans or head tokens, but we split these featuresto create separate feature-spaces for predicates andarguments.3For argument coref chains we heuristicallychoose a canonical mention to represent eachchain, and some features only look at this canon-ical mention.
The canonical mention is cho-sen based on length,4 information about the headword,5 and position in the document.6 In mostcases, coref chains that are longer than one areproper nouns and the canonical mention is the firstand longest mention (outranking pronominal ref-erences and other name shortenings).PPDB We use lexical features from the Para-phrase Database (PPDB) (Ganitkevitch et al,2013).
PPDB is a large set of paraphrases ex-tracted from bilingual corpora using pivoting tech-niques.
We make use of the English lexical portionwhich contains over 7 million rules for rewritingterms like ?planet?
and ?earth?.
PPDB offers avariety of conditional probabilities for each (syn-chronous context free grammar) rule, which we3While conceptually cleaner, In practice we found thissplitting to have no impact on performance.4in tokens, not counting some words like determiners andauxiliary verbs5like its part of speech tag and whether the it was taggedas a named entity6mentions that appear earlier in the document and earlierin a given sentence are given preference64treat as independent experts.
For each of these ruleprobabilities (experts), we find all rules that matchthe head tokens of a given alignment and have afeature for the max and harmonic mean of the logprobabilities of the resulting rule set.FrameNet FrameNet is a lexical database basedon Charles Fillmore?s Frame Semantics (Fill-more, 1976; Baker et al, 1998).
The database(and the theory) is organized around seman-tic frames that can be thought of as descrip-tions of events.
Frames crucially include spec-ification of the participants, or Frame Elements,in the event.
The Destroying frame, for in-stance, includes frame elements Destroyer orCause Undergoer.
Frames are related to otherframes through inheritance and perspectivization.For instance the frames Commerce buy andCommerce sell (with respective lexical real-izations ?buy?
and ?sell?)
are both perspectives ofCommerce goods-transfer (no lexical re-alizations) which inherits from Transfer (withlexical realization ?transfer?
).We compute a shortest path between headwordsgiven edges (hypernym, hyponym, perspectivizedparent and child) in FrameNet and bucket by dis-tance to get features.
We also have a binary featurefor whether two tokens evoke the same frame.TED Alignments Given two predicates or argu-ments in two sentences, we attempt to align thetwo sentences they appear in using a Tree EditDistance (TED) model that aligns two dependencytrees, based on the work described by (Yao et al,2013).
We represent a node in a dependency treewith three fields: lemma, POS tag and the typeof dependency relation to the node?s parent.
TheTED model aligns one tree with the other usingthe dynamic programming algorithm of Zhang andShasha (1989) with three predefined edits: dele-tion, insertion and substitution, seeking a solutionyielding the minimum edit cost.
Once we havebuilt a tree alignment, we extract features for 1)whether the heads of the two phrases are alignedand 2) the count of how many tokens are alignedin both trees.WordNet WordNet (Miller, 1995) is a databaseof information (synonyms, hypernyms, etc.)
per-taining to words and short phrases.
For each entry,WordNet provides a set of synonyms, hypernyms,etc.
Given two spans, we use WordNet to deter-mine semantic similarity by measuring how manysynonym (or other) edges are needed to link twoterms.
Similar words will have a short distance.For features, we find the shortest path linking thehead words of two mentions using synonym, hy-pernym, hyponym, meronym, and holonym edgesand bucket the length.String Transducer To represent similarity be-tween arguments that are names, we use a stochas-tic edit distance model.
This stochastic string-to-string transducer has latent ?edit?
and ?no edit?regions where the latent regions allow the modelto assign high probability to contiguous regions ofedits (or no edits), which are typical between vari-ations of person names.
In an edit region, param-eters govern the relative probability of insertion,deletion, substitution, and copy operations.
Weuse the transducer model of Andrews et al (2012).Since in-domain name pairs were not available, wepicked 10,000 entities at random from Wikipediato estimate the transducer parameters.
The entitylabels were used as weak supervision during EM,as in Andrews et al (2012).For a pair of mention spans, we compute theconditional log-likelihood of the two mentions go-ing both ways, take the max, and then bucket to getbinary features.
We duplicate these features withcopies that only fire if both mentions are tagged asPER, ORG or LOC.3 EvaluationWe consider three datasets for evaluating PARMA.For richer annotations that include lemmatiza-tions, part of speech, NER, and in-doc corefer-ence, we pre-processed each of the datasets usingtools7 similar to those used to create the AnnotatedGigaword corpus (Napoles et al, 2012).Extended Event Coreference Bank Based onthe dataset of Bejan and Harabagiu (2010), Lee etal.
(2012) introduced the Extended Event Coref-erence Bank (EECB) to evaluate cross-documentevent coreference.
EECB provides document clus-ters, within which entities and events may corefer.Our task is different from Lee et al but we canmodify the corpus setup to support our task.
Toproduce source and target document pairs, we se-lect the first document within every cluster as thesource and each of the remaining documents astarget documents (i.e.
N ?
1 pairs for a clusterof size N ).
This yielded 437 document pairs.Roth and Frank The only existing dataset forour task is from Roth and Frank (2012) (RF), who7https://github.com/cnap/anno-pipeline65annotated documents from the English GigawordFifth Edition corpus (Parker et al, 2011).
The datawas generated by clustering similar news storiesfrom Gigaword using TF-IDF cosine similarity oftheir headlines.
This corpus is small, containingonly 10 document pairs in the development set and60 in the test set.
To increase the training size,we train PARMA with 150 randomly selected doc-ument pairs from both EECB and MTC, and theentire dev set from Roth and Frank using multi-domain feature splitting.
We tuned the threshold?
on the Roth and Frank dev set, but choose theregularizer ?
based on a grid search on a 5-foldversion of the EECB dataset.Multiple Translation Corpora We constructeda new predicate argument alignment datasetbased on the LDC Multiple Translation Corpora(MTC),8 which consist of multiple English trans-lations for foreign news articles.
Since these mul-tiple translations are semantically equivalent, theyprovide a good resource for aligned predicate ar-gument pairs.
However, finding good pairs is achallenge: we want pairs with significant overlapso that they have predicates and arguments thatalign, but not documents that are trivial rewritesof each other.
Roth and Frank selected documentpairs based on clustering, meaning that the pairshad high lexical overlap, often resulting in mini-mal rewrites of each other.
As a result, despite ig-noring all context, their baseline method (lemma-alignment) worked quite well.To create a more challenging dataset, we se-lected document pairs from the multiple transla-tions that minimize the lexical overlap (in En-glish).
Because these are translations, we knowthat there are equivalent predicates and argumentsin each pair, and that any lexical variation pre-serves meaning.
Therefore, we can select pairswith minimal lexical overlap in order to createa system that truly stresses lexically-based align-ment systems.Each document pair has a correspondence be-tween sentences, and we run GIZA++ on thesesentences to produce token-level alignments.
Wetake all aligned nouns as arguments and all alignedverbs (excluding be-verbs, light verbs, and report-ing verbs) as predicates.
We then add negative ex-amples by randomly substituting half of the sen-tences in one document with sentences from an-8LDC2010T10, LDC2010T11, LDC2010T12,LDC2010T14, LDC2010T17, LDC2010T23, LDC2002T01,LDC2003T18, and LDC2005T050.3 0.4 0.5 0.6 0.7 0.80.00.20.40.60.81.0Performance vs Lexical OverlapDoc-pair Cosine SimilarityF1Figure 2: We plotted the PARMA?s performance oneach of the document pairs.
Red squares show theF1 for individual document pairs drawn from Rothand Frank?s data set, and black circles show F1 forour Multiple Translation Corpora test set.
The x-axis represents the cosine similarity between thedocument pairs.
On the RF data set, performanceis correlated with lexical similarity.
On our morelexically diverse set, this is not the case.
Thiscould be due to the fact that some of the docu-ments in the RF sets are minor re-writes of thesame newswire story, making them easy to align.other corpus, guaranteed to be unrelated.
Theamount of substitutions we perform can vary the?relatedness?
of the two documents in terms ofthe predicates and arguments that they talk about.This reflects our expectation of real world data,where we do not expect perfect overlap in predi-cates and arguments between a source and targetdocument, as you would in translation data.Lastly, we prune any document pairs that havemore than 80 predicates or arguments or have aJaccard index on bags of lemmas greater than 0.5,to give us a dataset of 328 document pairs.Metric We use precision, recall, and F1.
For theRF dataset, we follow Roth and Frank (2012) andCohn et al (2008) and evaluate on a version of F1that considers SURE and POSSIBLE links, whichare available in the RF data.
Given an alignmentto be scored A and a reference alignment B whichcontains SURE and POSSIBLE links, Bs andBp re-spectively, precision and recall are:P = |A ?Bp||A| R =|A ?Bs||Bs|(1)66F1 P REECB lemma 63.5 84.8 50.8PARMA 74.3 80.5 69.0RF lemma 48.3 40.3 60.3Roth and Frank 54.8 59.7 50.7PARMA 57.6 52.4 64.0MTC lemma 42.1 51.3 35.7PARMA 59.2 73.4 49.6Table 1: PARMA outperforms the baseline lemmamatching system on the three test sets, drawn fromthe Extended Event Coreference Bank, Roth andFrank?s data, and our set created from the MultipleTranslation Corpora.
PARMA achieves a higher F1and recall score than Roth and Frank?s reportedresult.and F1 as the harmonic mean of the two.
Resultsfor EECB and MTC reflect 5-fold cross validation,and RF uses the given dev/test split.Lemma baseline Following Roth and Frank weinclude a lemma baseline, in which two predicatesor arguments align if they have the same lemma.94 ResultsOn every dataset PARMA significantly improvesover the lemma baselines (Table 1).
On RF,compared to Roth and Frank, the best publishedmethod for this task, we also improve, makingPARMA the state of the art system for this task.Furthermore, we expect that the smallest improve-ments over Roth and Frank would be on RF, sincethere is little training data.
We also note that com-pared to Roth and Frank we obtain much higherrecall but lower precision.We also observe that MTC was more challeng-ing than the other datasets, with a lower lemmabaseline.
Figure 2 shows the correlation betweendocument similarity and document F1 score forRF and MTC.
While for RF these two measuresare correlated, they are uncorrelated for MTC.
Ad-ditionally, there is more data in the MTC datasetwhich has low cosine similarity than in RF.5 ConclusionPARMA achieves state of the art performance onthree datasets for predicate argument alignment.It builds on the development of lexical semanticresources and provides a platform for learning toutilize these resources.
Additionally, we show that9We could not reproduce lemma from Roth and Frank(shown in Table 1) due to a difference in lemmatizers.
We ob-tained 55.4; better than their system but worse than PARMA.task difficulty can be strongly tied to lexical simi-larity if the evaluation dataset is not chosen care-fully, and this provides an artificially high baselinein previous work.
PARMA is robust to drops in lex-ical similarity and shows large improvements inthose cases.
PARMA will serve as a useful bench-mark in determining the value of more sophis-ticated models of predicate-argument alignment,which we aim to address in future work.While our system is fully supervised, and thusdependent on manually annotated examples, weobserved here that this requirement may be rela-tively modest, especially for in-domain data.AcknowledgementsWe thank JHU HLTCOE for hosting the winterMiniSCALE workshop that led to this collabora-tive work.
This material is based on research spon-sored by the NSF under grant IIS-1249516 andDARPA under agreement number FA8750-13-2-0017 (the DEFT program).
The U.S. Governmentis authorized to reproduce and distribute reprintsfor Governmental purposes.
The views and con-clusions contained in this publication are those ofthe authors and should not be interpreted as repre-senting official policies or endorsements of NSF,DARPA, or the U.S. Government.ReferencesNicholas Andrews, Jason Eisner, and Mark Dredze.2012.
Name phylogeny: A generative model ofstring variation.
In Empirical Methods in NaturalLanguage Processing (EMNLP).Amit Bagga and Breck Baldwin.
1999.
Cross-document event coreference: Annotations, exper-iments, and observations.
In Proceedings of theWorkshop on Coreference and its Applications,pages 1?8.
Association for Computational Linguis-tics.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceed-ings of the 36th Annual Meeting of the Associa-tion for Computational Linguistics and 17th Inter-national Conference on Computational Linguistics -Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA,USA.
Association for Computational Linguistics.Cosmin Adrian Bejan and Sanda Harabagiu.
2010.Unsupervised event coreference resolution with richlinguistic features.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 1412?1422, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.67Trevor Cohn, Chris Callison-Burch, and Mirella Lap-ata.
2008.
Constructing corpora for the develop-ment and evaluation of paraphrase systems.
Com-put.
Linguist., 34(4):597?614, December.Hal Daume?.
2007.
Frustratingly easy domain adap-tation.
In Annual meeting-association for computa-tional linguistics, volume 45, page 256.Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-ber, and Tim Finin.
2010.
Entity disambiguationfor knowledge base population.
In Conference onComputational Linguistics (Coling).Charles J. Fillmore.
1976.
Frame semantics andthe nature of language.
Annals of the New YorkAcademy of Sciences: Conference on the Origin andDevelopment of Language and Speech, 280(1):20?32.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
Ppdb: The paraphrasedatabase.
In North American Chapter of the Asso-ciation for Computational Linguistics (NAACL).Xianpei Han and Le Sun.
2011.
A generative entity-mention model for linking entities with knowledgebase.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies-Volume 1, pages 945?954.
Association for Computational Linguistics.Heeyoung Lee, Marta Recasens, Angel Chang, MihaiSurdeanu, and Dan Jurafsky.
2012.
Joint entity andevent coreference resolution across documents.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL).Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://www.cs.umass.edu/ mccallum/mallet.George A Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Courtney Napoles, Matthew Gormley, and BenjaminVan Durme.
2012.
Annotated gigaword.
In AKBC-WEKEX Workshop at NAACL 2012, June.Robert Parker, David Graff, Jumbo Kong, Ke Chen,and Kazuaki Maeda.
2011.
English gigaword fifthedition.Michael Roth and Anette Frank.
2012.
Aligning predi-cate argument structures in monolingual comparabletexts: A new corpus for a new task.
In *SEM 2012:The First Joint Conference on Lexical and Compu-tational Semantics ?
Volume 1: Proceedings of themain conference and the shared task, and Volume 2:Proceedings of the Sixth International Workshop onSemantic Evaluation (SemEval 2012), pages 218?227, Montre?al, Canada, 7-8 June.
Association forComputational Linguistics.Xuchen Yao, Benjamin Van Durme, Peter Clark, andChris Callison-Burch.
2013.
Answer extraction assequence tagging with tree edit distance.
In NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL).K.
Zhang and D. Shasha.
1989.
Simple fast algorithmsfor the editing distance between trees and relatedproblems.
SIAM J.
Comput., 18(6):1245?1262, De-cember.68
