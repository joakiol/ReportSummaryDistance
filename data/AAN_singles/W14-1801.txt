Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 1?11,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsAutomated Measures of Specific Vocabulary Knowledge from ConstructedResponses (?Use These Words to Write a Sentence Based on this Picture?
)Swapna SomasundaranEducational Testing Services660 Rosedale Road,Princeton, NJ 08541, USAssomasundaran@ets.orgMartin ChodorowHunter College and the Graduate CenterCity University of New York,New York, NY 10065, USAmartin.chodorow@hunter.cuny.eduAbstractWe describe a system for automaticallyscoring a vocabulary item type that askstest-takers to use two specific words inwriting a sentence based on a picture.
Thesystem consists of a rule-based componentand a machine learned statistical modelwhich uses a variety of construct-relevantfeatures.
Specifically, in constructing thestatistical model, we investigate if gram-mar, usage, and mechanics features devel-oped for scoring essays can be applied toshort answers, as in our task.
We also ex-plore new features reflecting the quality ofthe collocations in the response, as well asfeatures measuring the consistency of theresponse to the picture.
System accuracyin scoring is 15 percentage points greaterthan the majority class baseline and 10percentage points less than human perfor-mance.1 IntroductionIt is often said that the best way to see if a per-son knows the meaning of a word is to have thatperson use the word in a sentence.
Despite thiswidespread view, most vocabulary testing contin-ues to rely on multiple choice items (e.g.
(Law-less et al., 2012; Lawrence et al., 2012)).
Infact, few assessments use constructed sentence re-sponses to measure vocabulary knowledge, in partbecause of the considerable time and cost requiredto score such responses manually.
While muchprogress has been made in automatically scor-ing writing quality in essays (Attali and Burstein,2006; Leacock et al., 2014; Dale et al., 2012),the essay scoring engines do not measure profi-ciency in the use of specific words, except perhapsfor some frequently confused homophones (e.g.,its/it?s, there/their/they?re, affect/effect).In this paper we present a system for automatedscoring of targeted vocabulary knowledge basedon short constructed responses in a picture de-scription task.
Specifically, we develop a systemfor scoring a vocabulary item type that is in op-erational use in English proficiency tests for non-native speakers.
Each task prompt in this item typeconsists of two target key words, for which the vo-cabulary proficiency is tested, and a picture thatprovides the context for the sentence construction.The task is to generate a single sentence, incorpo-rating both key words, consistent with the picture.Presumably, a test-taker with competent knowl-edge of the key words will be able to use them in awell-formed grammatical sentence in the contextof the picture.Picture description tasks have been employed ina number of areas of study ranging from secondlanguage acquisition to Alzheimer?s disease (El-lis, 2000; Forbes-McKay and Venneri, 2005).
Pic-tures and picture-based story narration have alsobeen used to study referring expressions (Lee etal., 2012) and to analyze child narratives in orderto predict language impairment (Hassanali et al.,2013).
Evanini et al.
(2014) employ a series ofpictures and elicit (oral) story narration to test En-glish language proficiency.
In our task, the pictureis used as a constraining factor to limit the typeand content of sentences that can be generated us-ing the given key words.In the course of developing our system, we ex-amined existing features that have been developedfor essay scoring, such as detectors of errors ingrammar, usage and mechanics, as well as col-location features, to see if they can be re-usedfor scoring short responses.
We also developednew features for assessing the quality of sentenceconstruction using Pointwise Mutual Information(PMI).
As our task requires responses to describethe prompt pictures, we manually constructed de-tailed textual descriptions of the pictures, and de-1veloped features that measure the overlap betweenthe content of the responses and the textual de-scription.
Our automated scoring system is partlybased on deterministic scoring criteria and partlystatistical.
Overall, it achieves an accuracy of76%, which is a 15 percentage point improvementover a simple majority class baseline.The organization of this paper is as follows:Section 2 describes the picture description taskand the scoring guide that is used to manuallyscore the picture description responses opera-tionally.
It also considers which aspects of scor-ing may be handled best by deterministic proce-dures and which are more amenable to statisticalmodeling.
Section 3 details the construction of areference corpus of text describing each picture,and Section 4 presents the features used in scor-ing.
Section 5 describes our system architectureand presents our experiments and results.
Detailedanalysis is presented in Section 6, followed by re-lated work in Section 7 and a summary with direc-tions for future research in Section 8.2 Task Description and DataThe picture description task is an item type that isin actual operational use as part of a test of En-glish.
It consists of a picture, along with two keywords, one or both of which may be in an inflectedform.
Test-takers are required to use the two wordsin one sentence to describe the picture.
They maychange the inflections of the words as appropriateto the context of their sentence, but they must usesome form of both words in one sentence.
Requir-ing them to produce a response based on the pic-ture constrains the variety of sentences and wordsthat they are likely to generate.Trained human scorers evaluate the responsesbased on appropriate use of grammar and the rel-evance of the sentence to the picture.
The opera-tional scoring guide is as follows:score = 3 The response consists of ONE sen-tence that: (a) has no grammatical errors, (b)contains forms of both key words used appro-priately, AND (c) is consistent with the pic-ture.score = 2 The response consists of one ormore sentences that: (a) have one or moregrammatical errors that do not obscure themeaning, (b) contain BOTH key words, (butthey may not be in the same sentence andthe form of the word(s) may not be accurate),AND (c) are consistent with the picture.score = 1 The response: (a) has errors that in-terfere with meaning, (b) omits one or bothkey words, OR (c) is not consistent with thepicture.score = 0 The response is blank, written ina foreign language, or consists of keystrokecharacters.Our decisions about scoring system design arebased on the scoring guide and its criteria.
Someaspects of the scoring can be handled by simplepattern matching or lookup, while others requiremachine learning.
For example, score 0 is as-signed to responses that are blank or are not inEnglish.
This can be detected and scored in astraightforward way.
On the other hand, the de-termination of grammaticality for the score points3, 2 and 1 depends on the presence and severityof grammatical errors.
A wide variety of such er-rors appear in responses, including errors of punc-tuation, subject-verb agreement, preposition usageand article usage.
The severity of an error dependson how problematic the error is, and the systemwill have to learn this from the behavior of thetrained human scorer(s), making this aspect of thescoring more amenable to statistical modeling.Similarly, statistical modeling is more suitablefor determining the consistency of the responsewith respect to the picture.
According to the scor-ing guide, a response gets a score of 0 or 1 if it isnot consistent with the picture, and gets a score of2 or 3 if it is consistent.
Thus, this aspect cannotsolely determine the score of a response ?
it influ-ences the score in conjunction with other languageproficiency factors.
Further, measures of how rel-evant a response is to a picture are likely to fall ona continuous scale, making a statistical modelingapproach appropriate.Finally, although there are some aspects of thescoring guide, such as the number of sentencesand the presence of the key words, that can bemeasured trivially, they do not act as sole deter-minants of the score.
For example, having morethan one sentence can result in the response re-ceiving a score of 2 or 1.
The number of sentencesworks in conjunction with other factors such asseverity of grammar errors and relevance to thepicture.
Hence its contribution to the final scoreis best modeled statistically.2As a result of the heterogeneous nature of theproblem, our system is made up of a statisticallearning component as well as a non-statisticalcomponent.2.1 DataThe data set consists of about 58K responses to434 picture prompts.
The mean response lengthwas 11.26 words with a standard deviation of 5.10.The data was split into 2 development sets (con-sisting of a total of about 2K responses) and a fi-nal train-test set (consisting of the remaining 56Kresponses) used for evaluation.
All 58K responseswere human scored using the scoring rubric dis-cussed in Section 2.
About 17K responses weredouble annotated.
The inter-annotator agreement,using quadratic weighted kappa (QWK), was 0.83.Score point 3, the most frequent class, was as-signed to 61% of the responses, followed by scorepoint 2 (31%), score point 1 (7.6%) and scorepoint 0 (0.4%).3 Reference Picture DescriptionsThe pictures in our task vary in their complexity.A typical prompt picture might be a photographof an outdoor marketplace, the inside of an airportterminal, a grocery store, a restaurant or a storeroom.
Because consistency with respect to the pic-ture is a crucial component in our task, we neededa reliable and exhaustive textual representation ofeach picture.
Therefore, we manually constructeda reference text corpus for each of our 434 pic-ture prompts.
We chose to use manual creation ofthe reference corpus instead of trying automatedimage recognition because automated methods ofimage recognition are error prone and would resultin a noisy reference corpus.
Additionally, auto-mated approaches would, at best, give us a (noisy)list of items that are present in the picture, but notthe overall scene or event depicted.Two annotators employed by a company thatspecializes in annotation created the reference cor-pora of picture descriptions.
The protocol used forcreating the reference corpus is shown below:Part-1: List the items, setting, and eventsin the picture.List, one by one, all the items and events yousee in the picture.
These may be animate ob-jects (e.g.
man), inanimate objects (e.g.
table)or events (e.g.
dinner).
Try to capture both theoverall setting (restaurant), as well as the ob-jects that make up the picture (e.g.
man, table,food).
These are generally (but not necessar-ily) nouns and noun phrases.
Some picturescan have many items, while some have only afew.
The goal is to list 10-15 items and to cap-ture as many items as possible, *starting withthe most obvious ones*.If the picture is too sparse, and you are notable to list at least 10 items, please indicatethis as a comment.Part:2 Describe the pictureDescribe the scene unfolding in the picture.The scene in the picture may be greater thanthe sum of its parts (many of which you willlist in part-1).
For example, the objects in apicture could be ?shoe?
?man?
?chair?, but thescene in the picture could be that of a shoepurchase.
The description tries to recreate thescene (or parts of the scene) depicted in thepicture.Generate a paragraph of 5-7 sentences de-scribing the picture.
Some of these sentenceswill address what is going on, while some mayaddress relations between items.
The propor-tions of these will differ, based on the picture.Make sure that you generate at least one sen-tence containing the two key words.If the picture is too simple, and you are notable to generate at least 5 sentences, pleaseindicate this as a comment.The human annotator was given the picture andthe two key words.
The protocol for creating eachreference corpus asked the annotator to first ex-haustively list all the items (animate and inani-mate) in the picture.
Then, the annotator wasasked to describe the scene in the picture.
We usedthis two step process in order to capture, as muchas possible, all objects, relationships between ob-jects, settings and events depicted in the pictures.The size of the reference corpus for each promptis much larger than the single sentence test-takerresponse.
This is intentional as the goal is to makethe reference corpus as exhaustive as possible.
Weused a single annotator for each prompt.
Doubleannotation using a secondary annotator was donein cases where we felt that the coverage of the cor-pus created by the primary annotator was insuffi-3cient1.In order to test coverage, we used a small devel-opment set of essays from each prompt and com-pared the coverage of the generated reference cor-pus over the development essays.
If the cover-age (proportion of content words in the responsesthat were found in the reference corpus) was lessthan 50% (this was the case for about 20% ofthe prompts), we asked the secondary annotator tocreate a new reference corpus for the prompt.
Thetwo reference corpora for the prompt were thensimply combined to form a single reference cor-pus.4 Features for automated scoringBecause the score points in the scoring guide con-flate, to some degree, syntactic, semantic, andother weaknesses in the response, we carried outa scoring study on a second small developmentset (comprising of a total of 80 responses from 4prompts, picked randomly) to gather insight intothe general problems in English language profi-ciency exhibited in the responses.
For the study,it was necessary to have test-taker responses re-scored by an annotator using an analytic schemewhich makes the types and locations of prob-lems explicit.
This exercise revealed that, in ad-dition to the factors stated explicitly in the scor-ing guide, there is another factor that results inlow comprehension (readability) of the sentenceand that reflects lower English proficiency.
Specif-ically, the annotator tagged many sentences as be-ing ?awkward?.
This awkwardness was due topoor choice of words or to poor construction of thesentence.
For example, in the sentence ?The manis putting some food in bags while he is record-ing for the payment?, ?recording for the payment?was marked as an awkward phrase.
Based on ourannotation of the scores and on the descriptions inthe scoring guide, we selected features designed tocapture grammar, picture relevance and awkwardusage.
We discuss each of our feature sets in thefollowing subsections.4.1 Features for Grammatical ErrorDetectionEssay scoring engines such as e-raterR?
(Attaliand Burstein, 2006) typically use a number of1We do not conduct inter-annotator agreement studies asthe goal of the double annotation was to create a diverse de-scription.grammar, usage and mechanics features that de-tect and quantify different types of English usageerrors in essays.
Examples of some of these errortypes are: Run-on Sentences, Subject Verb Agree-ment Errors, Pronoun Errors, Missing Posses-sive Errors, Wrong Article Errors, Missing Arti-cle Errors, Preposition Errors, Non-standard Verbor Word Form Errors, Double Negative Errors,Fragment or Missing Comma Errors, Ill-formedVerb Errors, Wrong Form of Word Errors, SpellingErrors, Wrong Part of Speech Errors, and MissingPunctuation Errors .In addition to these, essay scoring engines of-ten also use as features the Number of Sentencesthat are Short, the Number of Sentences that areLong, the Number of Passive Sentences, and otherfeatures that are relevant only for longer texts suchas essays.
Accordingly, we selected, from e-rater113 grammar, word usage, mechanics and lexicalcomplexity features that could be applied to ourshort response task.
This forms our grammar fea-ture set.4.2 Features for Measuring ContentRelevanceWe generated a set of features that measure thecontent overlap between a given response and thecorresponding reference corpus for the prompt.For this, first the keywords and the stop wordswere removed from the response and the referencecorpus, and then the proportion of overlap was cal-culated between the lemmatized content words ofthe response and the lemmatized version of thecorresponding reference corpus, as follows:|Response ?
Corpus||Response|It is not always necessary for the test-taker touse exactly the same words found in the referencecorpus.
For example, the annotator might havereferred to a person in the picture as a ?lady?,while a response may refer to the same personas a ?woman?
or ?girl?
or even just ?person?.Thus, we needed to go beyond simple lexicalmatch.
In order to account for synonyms, we ex-panded the content words in the reference corpusby adding their synonyms, as provided in Lin?sthesaurus (Lin, 1998) and then compared the ex-panded reference to each response.
Along thesame lines, we also used expansions from Word-Net synonyms, WordNet hypernyms and WordNethyponyms.
The following is the list of our content4relevance features.
Each measures the proportionof overlap as described by the equation above be-tween the lemmatized response and1.
lemmas: the lemmatized reference corpus.2.
cov-lin: the reference corpus expanded usingLin?s thesaurus.3.
cov-wn-syns: the reference corpus expandedusing WordNet Synonyms.4.
cov-wn-hyper: the reference corpus ex-panded using WordNet Hypernyms.5.
cov-wn-hypo: the reference corpus ex-panded using WordNet Hyponyms.6.
cov-all: the reference corpus expanded usingall of the above methods.Mean proportions of overlap ranged from 0.65for lemmas to 0.97 for cov-all.The 6 features listed above, along with theprompt id give a total of 7 features that form ourrelevance feature set.
We use prompt id as a fea-ture because the extent of overlap can depend onthe prompt.
Some pictures are very sparse, so,the description of the picture in the response willbe short, and will not vary much from the refer-ence corpus.
For these, a good amount of overlapbetween the response and reference corpus is ex-pected.
Other pictures are very dense with a largenumber of objects and items shown.
In this case,any single response may describe just a small sub-set of the items and satisfy the consistency criteria,and consequently, even a small overlap betweenthe response and the reference corpus may be suf-ficient.4.3 Features for Awkward Word UsageIn order to measure awkward word usage, we ex-plored PMI-based features, and also investigatedwhether some features developed for essay scor-ing can be used effectively for this purpose.4.3.1 PMI-based ngram featuresNon-native writing is often characterized by in-appropriate combinations of words, indicating thewriter?s lack of knowledge of collocations.
For ex-ample, ?recording for the payment?
might be bet-ter expressed as ?entering the price in the cash reg-ister?.
As ?recording for the payment?
is an inap-propriate construction, it is not likely to be com-mon, for example, in a large web corpus.
We usethis intuition in constructing our PMI-based fea-tures.We find the PMI of all adjacent word pairs(bigrams), as well as all adjacent word triples(trigrams) in the Google 1T web corpus (Brantsand Franz, 2006) using the TrendStream database(Flor, 2013).PMI between word pairs (bigram AB) is definedas:log2p(AB)p(A).p(B)and between word triples (trigram ABC) aslog2p(ABC)p(A).p(B).p(C)The higher the value of the PMI, the more com-mon is the collocation for the word pair/triple inwell formed texts.
On the other hand, negativevalues of PMI indicate that the given word pair (ortriple) is less likely than chance to occur together.We hypothesized that this would be a good indica-tor of awkward usage, as suggested in (Chodorowand Leacock, 2000).The PMI values for adjacent words obtainedover the entire response are then assigned to bins,with 8 bins for word pairs and another 8 for wordtriples.
Each bin represents a range for PMI p tak-ing real values R as follows:bin1= {p ?
R | p > 20}bin2= {p ?
R | 10 < p ?
20}bin3= {p ?
R | 1 < p ?
10}bin4= {p ?
R | 0 < p ?
1}bin5= {p ?
R | ?
1 < p ?
0}bin6= {p ?
R | ?
10 < p ?
?1}bin7= {p ?
R | ?
20 < p ?
?10}bin8= {p ?
R | p ?
?20}Once the PMI values for the adjacent word pairsin the response are generated, we generate two setsof features.
The first set is based on the countsof word pairs falling into each bin (for example,Number of pairs falling into bin1, Number of pairsfalling into bin2and so on).
The second set of fea-tures are based on percentages (for example Per-centage of pairs falling into bin1, Percentage ofpairs falling into bin2etc.).
These two sets resultin a total of 16 features.
We similarly generate16 more features for adjacent word triples.
We5use percentages in addition to raw counts to ac-count for the length of the response.
For example,it is possible for a long sentence to have phrasesthat are awkward as well as well formed, givingthe same counts of phrases in the high-PMI valuebins as that of a short sentence that is entirely wellformed.In addition to binning, we also encode as fea-tures the maximum, minimum and median PMIvalue obtained over all word pairs.
The first twofeatures capture the best and the worst word col-locations in a response.
The median PMI valuecaptures the overall general quality of the responsein a single number.
For example, if this is a lownumber, then the response generally has many badphrasal collocations.
Finally a null-PMI feature isused to count the number of pairs that had zeroentries in the database.
This feature is an indica-tor that the given words or word collocations werenot found even once in the database.
Given thesize of the underlying database, this usually hap-pens in cases when words are misspelled, or whenthe words never occur together.All features created for bigrams are also createdfor trigrams.
We thus have a total of 40 features,called the pmi feature set.4.3.2 Features from essay scoringA number of measures of collocation quality havebeen proposed and implemented (e.g.
(Futagi etal., 2008; Dahlmeier and Ng, 2011)).
We use e-rater?s measure of the density of ?good?
colloca-tions found in the response.
Another source ofdifficulty for non-native writers is the selection ofappropriate prepositions.
We use the mean proba-bility assigned by e-rater to the prepositions in theresponse.
These two measures, one for the qual-ity of collocations and the other for the quality ofprepositions, are combined in our colprep featureset.4.4 Scoring Rubric-based FeaturesAs seen in Section 2, some of the criteria for scor-ing are quite straightforward (e.g.
?omits one orboth key words?).
While these are not sole deter-minants of a score, they are certainly strong influ-ences.
Thus, we encode four criteria from the scor-ing guide.
These form our final feature set, rubric,and are binary values, answering the questions: Isthe first key word from the prompt present in theresponse?
Is the second key word from the promptpresent in the response?
Are both key words fromthe prompt present in the response?
Is there morethan one sentence in the response?Table 1 provides a list of feature types and thecorresponding number of features of each type.Feature set type Number of Featuresgrammar 113relevance 7pmi 40colprep 2rubric 4Table 1: Feature sets and the counts of features ineach set5 System and EvaluationFigure 1: System ArchitectureAs noted earlier, the system is partly rule-basedand partly statistical.
Figure 1 illustrates the sys-tem architecture.
The rule-based part capturesthe straightforward deterministic scoring criteriawhile the machine learning component encodesfeatures described in Section 4 and learns how toweight the features for scoring based on human-scored responses.As described in Section 2, detection of condi-tions that result in a score of zero are straight-forward.
Our rule-based scorer (shown as ?For-eign Language Detector?
in Figure 1) assigns azero score to a response if it is blank or non-English.
The system determines if the response isnon-English based on the average of PMI bigramscores over the response.
If the average score isless than a threshold value, the system tags it as6a non-English sentence.
The threshold was deter-mined by manually inspecting the PMI values ob-tained for sentences belonging to English and non-English news texts.
Responses given zero scoresby this module are filtered out and do not go to thenext stage.Responses that pass the rule-based scorer arethen sent to the statistical scorer.
Here, we encodethe features discussed in Section 4.
Spell checkingand correction are carried out before features forcontent relevance and PMI-based awkward wordusage are computed.
This is done in order to pre-vent misspellings from affecting the reference cor-pus match or database search.
The original textis sent to the Grammar feature generator as it cre-ates features based on misspellings and other wordform errors.
Finally, we use all the features to traina Logistic Regression model using sklearn.
Notethat the statistical system predicts all 4 scores (0through 3).
This is because the rule-based systemis not perfect; that is, it might miss some responsesthat should receive zero scores, and pass them overto the next stage.5.1 MetricsWe report our results using overall accuracy,quadratic weighted kappa (QWK) and score-levelprecision, recall and f-measure.
The precision Pof the system is calculated for each score point iasPi=|Si?Hi||Si|where |Si| is the number of responses given ascore of i by the system, and |Si?Hi| is the num-ber of responses given a score of i by the systemas well as the human rater.Similarly, recall, R is calculated for each scorepoint i asRi=|Si?Hi||Hi|F-measure Fiis calculated as the harmonicmean of the precision Piand recall Riat eachscore point i.
Accuracy is the ratio of the num-ber of responses correctly classified over the totalnumber of responses.5.2 ResultsAll of the responses in the train-test set werepassed through the rule-based zero-scorer.
A totalof 210 responses had been scored as zero by thehuman scorer.
The rule-based system scored 222responses as zeros, of which 184 were correct.The precision Pruleof the rule-based system iscalculated asPrule0=184222= 82.9%Similarly, Recall is calculated asRrule0=184210= 87.6%The corresponding F-measure is 85.2%The remaining responses pass to the next stagewhere machine learning is employed.
We per-formed 10 fold cross-validation experiments us-ing Logistic Regression as well as Random Forestlearners.
As the results are comparable, we onlyreport those from logistic regression.Accuracy in % Agreement (QWK)Baseline 61.00 -System 76.23 0.63Human 86.00 0.83Table 2: Overall system and human accuracy(in percentage) and agreement (using QuadraticWeighted Kappa)Table 2 reports the results.
The system achievesan accuracy of 76.23%, which is more than a 15percentage point improvement over the majorityclass baseline of 61%.
The majority class base-line always predicts a score of 3.
Compared to hu-man performance, system performance is 10 per-centage points lower (human-human agreementis 86%).
Quadratic weighted kappa for system-human agreement is also lower (0.63) than forhuman-human agreement (0.83).Table 3 reports the precision, recall and F-measure of the system for each of the score points.Score point Precision Recall F-measure0 84.2 68.3 72.91 78.4 67.5 72.62 70.6 50.4 58.83 77.8 90.5 83.6Table 3: Overall system performance at each scorepoint using all features6 AnalysisIn order to understand the usefulness of each fea-ture set in scoring the responses, we constructed7systems using first the individual features alone,and then using feature combinations.
Table 4 re-ports the accuracy of the learner using individualfeatures alone.
We see that, individually, each fea-ture set performs much below the performance ofthe full system (that has an accuracy of 76.23%),which is expected, as each feature set representsa particular aspect of the construct.
However, ingeneral, each of the feature-sets (except colprep)shows improvement over baseline, indicating thatthey contribute towards performance improvementin the automated system.Grammar features are the best of the individ-ual feature sets at 70% accuracy, indicating thatgrammatical error features developed for longertexts can be applied to single sentences.
The PMI-based feature set is the second best performer, in-dicating its effectiveness in capturing word usageissues.
While colprep and pmi both capture awk-ward usage, pmi alone shows better performance(67.44%) than colprep alone (61.26%).
Also,when rubric is used alone, the resulting systemproduces a four percentage point improvementover the baseline, with 65% accuracy, indicatingthe presence of responses where the test-takers arenot able to incorporate one or both words in a sin-gle sentence.
The relevance feature set by itselfdoes not show substantial improvement over thebaseline.
This is not surprising, as according tothe scoring guide, a response gets a score of 0 or 1if it does not describe the picture, and gets a scoreof 2 or 3 if it is relevant to the picture.
Hence, thisfeature cannot solely and accurately determine thescore.Feature Set Accuracy in %grammar 70.30pmi 67.44rubric 65.00relevance 62.50colprep 61.26Table 4: System performance for individual fea-turesTable 5 reports accuracies of systems built us-ing feature set combinations.
The first feature setcombination, grammar + colprep, is a set of allfeatures obtained from essay scoring.
Here we seethat addition of colprep does not improve the per-formance over that obtained by grammar featuresalone.
Further, when colprep is combined withpmi (colprep+pmi, row 2), there is a slight dropin performance as compared to using pmi-basedfeatures alone.
These results indicate that colprep,while being useful for larger texts, does not trans-fer well to the simple single sentence responses inour task.Further, in Table 5 we see that the system usinga combination of the pmi feature set and the rele-vance feature set (pmi+relevance) achieves an ac-curacy of 69%.
Thus, this feature combination isable to improve performance over that using eitherfeature set alone, indicating that while content rel-evance features by themselves do not create an im-pact, they can improve performance when addedto other features.
Finally, the feature combinationof all new features developed for this task (pmi +relevance+ rubric) yields 73% accuracy, which isagain better than each individual feature set?s per-formance, indicating that they can be synergisti-cally combined to improve system performance.Feature Set Accuracy in %(i) grammar + colprep 70.31(ii) colprep + pmi 67.42(iii) pmi + relevance 69.05(iv) pmi + relevance + rubric 73.21Table 5: System performance for feature combi-nations (i) typically used in essay scoring, (ii) thatmeasure awkwardness, (iii) newly proposed here,(iv) newly proposed plus rubric-specific criteria7 Related WorkMost work in automated scoring and learner lan-guage analysis has focused on detecting grammarand usage errors (Leacock et al., 2014; Dale et al.,2012; Dale and Narroway, 2012; Gamon, 2010;Chodorow et al., 2007; Lu, 2010).
This is doneeither by means of handcrafted rules or with sta-tistical classifiers using a variety of information.In the case of the latter, the emphasis has been onrepresenting the contexts of function words, suchas articles and prepositions.
This work is rele-vant inasmuch as errors in using content words,such as nouns and verbs, are often reflected in thefunctional elements which accompany them, forexample, articles that indicate the definiteness orcountability of nouns, and prepositions that markthe cases of the arguments of verbs.Previous work (Bergsma et al., 2009; Bergsmaet al., 2010; Xu et al., 2011) has shown that mod-8els which rely on large web-scale n-gram countscan be effective for the task of context-sensitivespelling correction.
Measures of ngram associa-tion such as PMI, log likelihood, chi-square, andt have a long history of use for detecting colloca-tions and measuring their quality (see (Manningand Sch?utze, 1999) and (Leacock et al., 2014)for reviews).
Our application of a large n-gramdatabase and PMI is to detect inappropriate wordusage.Our task also differs from work focusing onevaluating content (e.g.
(Meurers et al., 2011;Sukkarieh and Blackmore, 2009; Leacock andChodorow, 2003)) in that, although we are look-ing for usage of certain content words, we focusprimarily on measuring knowledge of vocabulary.Recent work on assessment measures of depthof vocabulary knowledge (Lawless et al., 2012;Lawrence et al., 2012), has argued that knowl-edge of specific words can range from superficial(idiomatic associations built up through word co-occurrence) to topical (meaning-related associa-tions between words) to deep (definitional knowl-edge).
Some of our features (e.g.
awkward wordusage) capture some of this information (e.g., id-iomatic associations between words), but assign-ing the depth of knowledge of the key words is notthe focus of our task.Work that is closely related to ours is that ofKing and Dickinson (2013).
They parse picturedescriptions from interactive learner sentences,classify sentences into syntactic types and extractthe logical subject, verb and object in order to re-cover simple semantic representations of the de-scriptions.
We do not explicitly model the seman-tic representations of the pictures, but rather ourgoal in this work is to ascertain if a response isrelevant to the picture and to measure other fac-tors that reflect vocabulary proficiency.We employ human annotators and use wordsimilarity measures to obtain alternative forms ofdescription because the proprietary nature of ourdata prevents us from releasing our pictures tothe public.
However, crowd sourcing has beenused by other researchers to collect human labelsfor images and videos.
For example, Rashtchianet al.
(2010) use Amazon Mechanical Turk andVon Ahn and Dabbish (2004) create games to en-tice players to correctly label images.
Chen andDolan (2011) use crowd sourcing to collect multi-ple paraphrased descriptions of videos to create aparaphrasing corpus.In a vast body of related work, automatedmethods have been explored for the generationof descriptions of images (Kulkarni et al., 2013;Kuznetsova et al., 2012; Li et al., 2011; Yao etal., 2010; Feng and Lapata, 2010a; Feng and La-pata, 2010b; Leong et al., 2010; Mitchell et al.,2012).
There is also work in the opposite di-rection, of finding or generating pictures for agiven narration.
Joshi et al.
(2006) found thebest set of images from an image database tomatch the keywords in a story.
Coyne and Sproat(2001) developed a natural language understand-ing system which converts English text into three-dimensional scenes that represent the text.
For ahigh-stakes assessment, it would be highly unde-sirable to have any noise in the gold-standard ref-erence picture descriptions.
Hence we chose to usemanual description for creating our reference cor-pus.8 Summary and Future DirectionsWe investigated different types of features for au-tomatically scoring a vocabulary item type whichrequires the test-taker to use two words in writ-ing a sentence based on a picture.
We generated acorpus of picture descriptions for measuring therelevance of responses, and as a foundation forfeature development, we performed preliminaryfine-grained annotations of responses.
The fea-tures used in the resulting automated scoring sys-tem include newly developed statistical measuresof word usage and response relevance, as well asfeatures that are currently found in essay scoringengines.
System performance shows an overallaccuracy in scoring that is 15 percentage pointsabove the majority class baseline and 10 percent-age points below human performance.There are a number of avenues open for futureexploration.
The automated scoring system mightbe improved by extending the relevance featureto include overlap with previously collected high-scoring responses.
The reference corpus couldalso be expanded and diversified by using a largenumber of annotators, at least some of whom arespeakers of the languages that are most promi-nently represented in the population of test-takers.Finally, one particular avenue we would like to ex-plore is the use of our features to provide feedbackin low stakes practice environments.9ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-rater v. 2.0.
Journal of Technology,Learning, and Assessment, 4:3.Shane Bergsma, Dekang Lin, and Randy Goebel.2009.
Web-scale n-gram models for lexical disam-biguation.
In IJCAI.Shane Bergsma, Emily Pitler, and Dekang Lin.
2010.Creating robust supervised classifiers via web-scalen-gram data.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 865?874.
Association for Computa-tional Linguistics.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram Version 1.
In Linguistic Data Consortium,Philadelphia.David L Chen and William B Dolan.
2011.
Collect-ing highly parallel data for paraphrase evaluation.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies-Volume 1, pages 190?200.Association for Computational Linguistics.Martin Chodorow and Claudia Leacock.
2000.
An un-supervised method for detecting grammatical errors.In Proceedings of the Conference of the North Amer-ican Chapter of the Association of ComputationalLinguistics (NAACL), pages 140?147.Martin Chodorow, Joel R Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involvingprepositions.
In Proceedings of the fourth ACL-SIGSEM workshop on prepositions, pages 25?30.Association for Computational Linguistics.Bob Coyne and Richard Sproat.
2001.
Wordseye: anautomatic text-to-scene conversion system.
In Pro-ceedings of the 28th annual conference on Computergraphics and interactive techniques, pages 487?496.ACM.Daniel Dahlmeier and Hwee Tou Ng.
2011.
Cor-recting semantic collocation errors with L1 inducedparaphrases.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?11, pages 107?117, Stroudsburg, PA,USA.
Association for Computational Linguistics.Robert Dale and George Narroway.
2012.
A frame-work for evaluating text correction.
In LREC, pages3015?3018.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A report on the preposition anddeterminer error correction shared task.
In Proceed-ings of the Seventh Workshop on Building Educa-tional Applications Using NLP, pages 54?62.
Asso-ciation for Computational Linguistics.Rod Ellis.
2000.
Task-based research and languagepedagogy.
Language teaching research, 4(3):193?220.Keelan Evanini, Michael Heilman, Xinhao Wang, andDaniel Blanchard.
2014.
Automated scoring forTOEFL Junior comprehensive writing and speaking.Technical report, ETS, Princeton, NJ.Yansong Feng and Mirella Lapata.
2010a.
How manywords is a picture worth?
Automatic caption genera-tion for news images.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 1239?1249, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Yansong Feng and Mirella Lapata.
2010b.
Topicmodels for image annotation and text illustration.In Human Language Technologies: The 2010 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics, HLT?10, pages 831?839, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Michael Flor.
2013.
A fast and flexible architecture forvery large word n-gram datasets.
Natural LanguageEngineering, 19(1):61?93.KE Forbes-McKay and Annalena Venneri.
2005.
De-tecting subtle spontaneous language decline in earlyAlzheimers disease with a picture description task.Neurological sciences, 26(4):243?254.Yoko Futagi, Paul Deane, Martin Chodorow, and JoelTetreault.
2008.
A computational approach to de-tecting collocation errors in the writing of non-nativespeakers of English.
Computer Assisted LanguageLearning, 21(4):353?367.Michael Gamon.
2010.
Using mostly native data tocorrect errors in learners?
writing: A meta-classifierapproach.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 163?171.
Association for Computa-tional Linguistics.Khairun-nisa Hassanali, Yang Liu, and ThamarSolorio.
2013.
Using Latent Dirichlet Allocationfor child narrative analysis.
ACL 2013, page 111.Dhiraj Joshi, James Z. Wang, and Jia Li.
2006.
Thestory picturing engine?a system for automatic textillustration.
ACM Trans.
Multimedia Comput.
Com-mun.
Appl., 2(1):68?89, February.Levi King and Markus Dickinson.
2013.
Shallow se-mantic analysis of interactive learner sentences.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 11?21, Atlanta, Georgia, June.
Associationfor Computational Linguistics.Girish Kulkarni, Visruth Premraj, Vicente Ordonez,Sagnik Dhar, Siming Li, Yejin Choi, Alexander C.Berg, and Tamara L. Berg.
2013.
Babytalk: Under-standing and generating simple image descriptions.IEEE Transactions on Pattern Analysis and MachineIntelligence, 99(PrePrints):1.10Polina Kuznetsova, Vicente Ordonez, Alexander CBerg, Tamara L Berg, and Yejin Choi.
2012.
Col-lective generation of natural image descriptions.
InProceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics: LongPapers-Volume 1, pages 359?368.
Association forComputational Linguistics.Ren?e Lawless, John Sabatini, and Paul Deane.
2012.Approaches to assessing partial vocabulary knowl-edge and supporting word learning: Assessing vo-cabulary depth.
In Annual Meeting of the Ameri-can Educational Research Association, April 13-17,2012, Vancouver, CA.Joshua Lawrence, Elizabeth Pare-Blagoev, Ren?e Law-less, and Chen Deane, Paul andLi.
2012.
Gen-eral vocabulary, academic vocabulary, and vocabu-lary depth: Examiningpredictors of adolescent read-ing comprehension.
In Annual Meeting of the Amer-ican Educational Research Association.Claudia Leacock and Martin Chodorow.
2003.
C-rater:Automated scoring of short-answer questions.
Com-puters and the Humanities, 37(4):389?405.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2014.
Automated GrammaticalError Detection for Language Learners.
SynthesisLectures on Human Language Technologies.
Mor-gan & Claypool.Choonkyu Lee, Smaranda Muresan, and KarinStromswold.
2012.
Computational analysis of re-ferring expressions in narratives of picture books.NAACL-HLT 2012, page 1.Chee Wee Leong, Rada Mihalcea, and Samer Hassan.2010.
Text mining for automatic image tagging.
InProceedings of the 23rd International Conference onComputational Linguistics: Posters, pages 647?655.Association for Computational Linguistics.Siming Li, Girish Kulkarni, Tamara L Berg, Alexan-der C Berg, and Yejin Choi.
2011.
Composingsimple image descriptions using web-scale n-grams.In Proceedings of the Fifteenth Conference on Com-putational Natural Language Learning, pages 220?228.
Association for Computational Linguistics.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 17th inter-national conference on Computational linguistics-Volume 2, pages 768?774.
Association for Compu-tational Linguistics.Xiaofei Lu.
2010.
Automatic analysis of syntacticcomplexity in second language writing.
Interna-tional Journal of Corpus Linguistics, 15(4).Christopher D. Manning and Hinrich Sch?utze.
1999.Foundations of Statistical Natural Language Pro-cessing.
MIT Press.Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey MBailey.
2011.
Integrating parallel analysis mod-ules to evaluate the meaning of answers to readingcomprehension questions.
International Journal ofContinuing Engineering Education and Life LongLearning, 21(4):355?369.Margaret Mitchell, Xufeng Han, Jesse Dodge, AlyssaMensch, Amit Goyal, Alex Berg, Kota Yamaguchi,Tamara Berg, Karl Stratos, and Hal Daum?e III.2012.
Midge: Generating image descriptions fromcomputer vision detections.
In Proceedings of the13th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 747?756.
Association for Computational Linguistics.Cyrus Rashtchian, Peter Young, Micah Hodosh, andJulia Hockenmaier.
2010.
Collecting image annota-tions using Amazon?s Mechanical Turk.
In Proceed-ings of the NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Mechan-ical Turk, pages 139?147.
Association for Computa-tional Linguistics.Jana Zuheir Sukkarieh and John Blackmore.
2009.C-rater: Automatic content scoring for short con-structed responses.
In FLAIRS Conference.Luis Von Ahn and Laura Dabbish.
2004.
Labelingimages with a computer game.
In Proceedings of theSIGCHI conference on Human factors in computingsystems, pages 319?326.
ACM.Wei Xu, Joel Tetreault, Martin Chodorow, Ralph Gr-ishman, and Le Zhao.
2011.
Exploiting syntacticand distributional information for spelling correc-tion with web-scale n-gram models.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing, pages 1291?1300.
Asso-ciation for Computational Linguistics.Benjamin Z Yao, Xiong Yang, Liang Lin, Mun WaiLee, and Song-Chun Zhu.
2010.
I2t: Image pars-ing to text description.
Proceedings of the IEEE,98(8):1485?1508.11
