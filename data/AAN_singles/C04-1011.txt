Kullback-Leibler Distancebetween Probabilistic Context-Free Grammarsand Probabilistic Finite AutomataMark-Jan NederhofFaculty of ArtsUniversity of GroningenP.O.
Box 716NL-9700 AS Groningen, The Netherlandsmarkjan@let.rug.nlGiorgio SattaDepartment of Information EngineeringUniversity of Paduavia Gradenigo, 6/AI-35131 Padova, Italysatta@dei.unipd.itAbstractWe consider the problem of computing theKullback-Leibler distance, also called therelative entropy, between a probabilisticcontext-free grammar and a probabilistic fi-nite automaton.
We show that there isa closed-form (analytical) solution for onepart of the Kullback-Leibler distance, viz.the cross-entropy.
We discuss several ap-plications of the result to the problem ofdistributional approximation of probabilis-tic context-free grammars by means of prob-abilistic finite automata.1 IntroductionAmong the many formalisms used for descrip-tion and analysis of syntactic structure of natu-ral language, the class of context-free grammars(CFGs) is by far the best understood and mostwidely used.
Many formalisms with greater gen-erative power, in particular the different typesof unification grammars, are ultimately basedon CFGs.Regular expressions, with their proceduralcounter-part of finite automata (FAs), are notable to describe hierarchical, tree-shaped struc-ture, and thereby seem less suitable than CFGsfor full analysis of syntactic structure.
How-ever, there are many applications where onlypartial or approximated analysis of structure isneeded, and where full context-free processingcould be prohibitively expensive.
Such appli-cations can for example be found in real-timespeech recognition systems: of the many hy-potheses returned by a speech recognizer, shal-low syntactic analysis may be used to select asmall subset of those that seem most promis-ing for full syntactic processing in a next phase,thereby avoiding further computational costsfor the less promising hypotheses.As FAs cannot describe structure as such, itis impractical to write the automata requiredfor such applications by hand, and even diffi-cult to derive them automatically by training.For this reason, the used FAs are often derivedfrom CFGs, by means of some form of approx-imation.
An overview of different methods ofapproximating CFGs by FAs, along with an ex-perimental comparison, was given by (Nederhof,2000).The next step is to assign probabilities to thetransitions of the approximating FA, as the ap-plication outlined above requires a qualitativedistinction between hypotheses rather than thepurely boolean distinction of language member-ship.
Under certain circumstances, this may bedone by carrying over the probabilities from aninput probabilistic CFG (PCFG), as shown forthe special case of n-grams by (Rimon and Herz,1991; Stolcke and Segal, 1994), or by trainingof the FA on a corpus generated by the PCFG(Jurafsky et al, 1994).
See also (Mohri andNederhof, 2001) for discussion of related ideas.An obvious question to ask is then howwell the resulting PFA approximates the inputPCFG, possibly for different methods of deter-mining an FA and different ways of attachingprobabilities to the transitions.
Until now, anydirect way of measuring the distance betweena PCFG and a PFA has been lacking.
As wewill argue in this paper, the natural distancemeasure between probability distributions, theKullback-Leibler (KL) distance, is difficult tocompute.
(The KL distance is also called rela-tive entropy.)
We can however derive a closed-form (analytical) solution for the cross entropyof a PCFG and a PFA, provided the FA under-lying the PFA is deterministic.
The differencebetween the cross-entropy and the KL distanceis the entropy of the PCFG, which does not relyon the PFA.
This means that if we are interestedin the relative quality of different approximat-ing PFAs with respect to a single input PCFG,the cross-entropy may be used instead of theKL distance.
The constraint of determinism isnot a problem in practice, as any FA can bedeterminized, and FAs derived by approxima-tion algorithms are normally determinized (andminimized).As a second possible application, we now lookmore closely into the matter of determinizationof finite-state models.
Not all PFAs can be de-terminized, as discussed by (Mohri, 1997).
Thisis unfortunate, as deterministic (P)FAs processinput with time and space costs independentof the size of the automaton, whereas thesecosts are linear in the size of the automatonin the nondeterministic case, which may be toohigh for some real-time applications.
Insteadof distribution-preserving determinization, wemay therefore approximate a nondeterministicPFA by a deterministic PFA whose probabilitydistribution is close to, but not necessarily iden-tical to, that of the first PFA.
Again, an impor-tant question is how close the two models are toeach other.
It was argued before by (Juang andRabiner, 1985; Falkhausen et al, 1995; Viholaet al, 2002) that the KL distance between finite-state models is difficult to compute in general.The theory developed in this paper shows how-ever that the cross-entropy between the inputPFA and the approximating deterministic PFAcan be expressed in closed form, relying on thefact that a PFA can be seen as a special case ofa PCFG.
Thereby, different approximating de-terministic PFAs can be compared for closenessto the input PFA.
We can even compute theKL distance between two unambiguous PFAs,in closed form.
(It is not difficult to see thatambiguity is a decidable property for FAs.
)The structure of this paper is as follows.We provide some preliminary definitions in Sec-tion 2.
Section 3 discusses the expected fre-quency of a rule in derivations allowed by aPCFG, and explains how such values can be ef-fectively computed.
The KL distance betweena PCFG and a PFA is closely related to theentropy of the PCFG, which we discuss in Sec-tion 4.
Essential to our approach is the inter-section of PCFGs and PFAs, to be discussed inSection 5.
As we show in Section 6, the partof the KL distance expressing the cross-entropycan be computed in closed form, based on thisintersection.
Section 7 concludes this paper.2 PreliminariesThroughout the paper we use mostly stan-dard formal language notation, as for instancein (Hopcroft and Ullman, 1979; Booth andThompson, 1973), which we summarize below.A context-free grammar (CFG) is a 4-tupleG = (?,N, S,R) where ?
and N are finite dis-joint sets of terminals and nonterminals, respec-tively, S ?
N is the start symbol and R is a fi-nite set of rules.
Each rule has the form A?
?,where A ?
N and ?
?
(?
?N)?.The ?derives?
relation ?
associated with Gis defined on triples consisting of two strings?, ?
?
(?
?
N)?
and a rule pi ?
R. We write?
pi?
?
if and only if ?
is of the form uA?and ?
is of the form u?
?, for some u ?
??,?
?
(?
?
N)?, and pi = (A ?
?).
A left-mostderivation (for G) is a string d = pi1 ?
?
?pim,m ?
0, such that ?0pi1?
?1pi2?
?
?
?pim?
?m, forsome ?0, .
.
.
, ?m ?
(?
?
N)?
; d =  (where denotes the empty string) is also a left-mostderivation.
In the remainder of this paper,we will let the term ?derivation?
refer to ?left-most derivation?, unless specified otherwise.
If?0pi1?
?
?
?pim?
?m for some ?0, .
.
.
, ?m ?
(?
?N)?,then we say that d = pi1 ?
?
?pim derives ?m from?0 and we write ?0d?
?m; d =  derives any?0 ?
(?
?N)?
from itself.A (left-most) derivation d such that S d?
w,w ?
?
?, is called a complete derivation.
If d isa complete derivation, we write y(d) to denotethe (unique) string w ?
??
such that S d?
w.The language generated by G is the set of allstrings y(d) derived by complete derivations,i.e., L(G) = {w |S d?
w, d ?
R?, w ?
??
}.It is well-known that there is a one-to-one cor-respondence between complete derivations andparse trees for strings in L(G).A probabilistic CFG (PCFG) is a pair Gp =(G, pG), where G is a CFG and pG is a functionfrom R to real numbers in the interval [0, 1].A PCFG is proper if?pi=(A??)
pG(pi) = 1 forall A ?
N .
Function pG can be used to as-sociate probabilities to derivations of the un-derlying CFG G, in the following way.
Ford = pi1 ?
?
?pim ?
R?, m ?
0, we define pG(d) =?mi=1 pG(pii) if Sd?
w for some w ?
?
?, andpG(d) = 0 otherwise.
The probability of a stringw ?
??
is defined as pG(w) =?d:y(d)=w pG(d).A PCFG is consistent if?w pG(w) = 1.
Con-sistency implies that the PCFG defines a proba-bility distribution on the set of terminal stringsas well as on the set of grammar derivations.
Ifa PCFG is proper, then consistency means thatno probability mass is lost in ?infinite?
deriva-tions.A finite automaton (FA) is a 5-tuple M = (?,Q, q0, Qf , T ), where ?
and Q are two finite setsof terminals and states, respectively, q0 is theinitial state, Qf ?
Q is the set of final states,and T is a finite set of transitions, each of theform s a7?
t, where s, t ?
Q and a ?
?.
Aprobabilistic finite automaton (PFA) is a pairMp = (M,pM ), where M is an FA and pM is afunction from T to real numbers in the interval[0, 1].1For a fixed (P)FA M , we define a configu-ration to be an element of Q ?
?
?, and wedefine the relation ` on triples consisting oftwo configurations and a transition ?
?
T by:(s, w)?` (t, w?)
if and only if w is of the form aw?,for some a ?
?, and ?
= (s a7?
t).
A completecomputation is a string c = ?1 ?
?
?
?m, m ?
0,such that (s0, w0)?1` (s1, w1)?2` ?
?
?
?m` (sm, wm),for some (s0, w0), .
.
.
, (sm, wm) ?
Q??
?, withs0 = q0, sm ?
Qf and wm = , and we write(s0, w0)c` (sm, wm).
The language accepted byM is L(M) = {w ?
??
| (q?, w)c` (s, ), c ?T ?, s ?
Qf}.For a PFA Mp = (M,pM ), and c = ?1 ?
?
?
?m ?T ?, m ?
0, we define pM (c) =?mi=1 pM (?i) ifc is a complete computation, and pM (c) = 0otherwise.
A PFA is consistent if?c pM (c) = 1.We say M is unambiguous if for each w ?
?
?,?s?Qf [(q0, w)c` (s, )] for at most one c ?
T ?.We say M is deterministic if for each s and a,there is at most one transition s a7?
t. Deter-minism implies unambiguity.
It can be morereadily checked whether an FA is determinis-tic than whether it is unambiguous.
Further-more, any FA can be effectively turned into adeterministic FA accepting the same language.Therefore, this paper will assume that FAs aredeterministic, although technically, unambigu-ity is sufficient for our constructions to apply.3 Expectation of rule frequencyHere we discuss how we can compute the ex-pectation of the frequency of a rule or a non-terminal over all derivations of a probabilisticcontext-free grammar.
These quantities will beused later by our algorithms.1Our definition of PFAs amounts to a slight loss ofgenerality with respect to standard definitions, in thatthere are no epsilon transitions and no probability func-tion on states being final.
We want to avoid these con-cepts as they would cause some technical complicationslater in this article.
There is no loss of generality how-ever if we may assume an end-of-sentence marker, whichis often the case in practice.Let (A ?
?)
?
R be a rule of PCFG Gp,and let d ?
R?
be a complete derivation in Gp.We define f(A?
?
; d) as the number of occur-rences, or frequency , of A ?
?
in d. Similarly,the frequency of nonterminal A in d is definedas f(A; d) =??
f(A?
?
; d).
We consider thefollowing related quantitiesEpG f(A?
?
; d) =?dpG(d) ?
f(A?
?
; d),EpG f(A; d) =?dpG(d) ?
f(A; d)=?
?EpG f(A?
?
; d).A method for the computation of these quan-tities is reported in (Hutchins, 1972), based onthe so-called momentum matrix.
We proposean alternative method here, based on an idearelated to the inside-outside algorithm (Baker,1979; Lari and Young, 1990; Lari and Young,1991).
We observe that we can factorize aderivation d at each occurrence of rule A ?
?into an ?innermost?
part d2 and two ?outermost?parts d1 and d3.
We can then writeEpG f(A?
?
; d) =?d=pi1??
?pim,m1,m2,w,?,v,x:Sd1?wA?, with d1=pi1???pim1?1,(A??
)=pim1 ,?d2?v, with d2=pim1+1??
?pim2 ,?d3?x, with d3=pim2+1??
?pimm?i=1pG(pii).Next we group together all of the innermost andall of the outermost derivations and writeEpG f(A?
?
; d) =outGp(A) ?
pG(A?
?)
?
inGp(?
)whereoutGp(A) =?d=pi1???pim,d?=pi?1??
?pi?m?,w,?,x:Sd?wA?, ?d?
?xm?i=1pG(pii) ?m??i=1pG(pi?i)andinGp(?)
=?d=pi1??
?pim,v:?d?vm?i=1pG(pii).Both outGp(A) and inGp(?)
can be described interms of recursive equations, of which the leastfixed-points are the required values.
If Gp isproper and consistent, then inGp(?)
= 1 foreach ?
?
(?
?
N)?.
Quantities outGp(A) forevery A can all be (exactly) calculated by solv-ing a linear system, requiring an amount of timeproportional to the cube of the size of Gp; seefor instance (Corazza et al, 1991).On the basis of all the above quantities, anumber of useful statistical properties of Gp canbe easily computed, such as the expected lengthof derivations, denoted EDL(Gp) and the ex-pected length of sentences, denoted EWL(Gp),discussed before by (Wetherell, 1980).
Thesequantities satisfy the relationsEDL(Gp) = EpG |d| =?A?
?outGp(A) ?
pG(A?
?)
?
inGp(?
),EWL(Gp) = EpG |y(d)| =?A?
?outGp(A) ?
pG(A?
?)
?
inGp(?)
?
|?|?
,where for a string ?
?
(N ?
?)?
we write |?|?to denote the number of occurrences of terminalsymbols in ?.4 Entropy of PCFGsIn this section we introduce the notion of deriva-tional entropy of a PCFG, and discuss an algo-rithm for its computation.Let Gp = (G, pG) be a PCFG.
For a nonter-minal A of G, let us define the entropy of A asthe entropy of the distribution pG on all rulesof the form A?
?, i.e.,H(A) = EpG log1pG(A?
?)=??pG(A?
?)
?
log1pG(A?
?
).The derivational entropy of Gp is defined asthe expectation of the information of the com-plete derivations generated by Gp, i.e.,Hd(Gp) = EpG log1pG(d)=?dpG(d) ?
log1pG(d).
(1)We now characterize derivational entropy usingexpected rule frequencies asHd(Gp) =?dpG(d) ?
log1pG(d)=?dpG(d) ?
log?A??(1pG(A?
?))f(A??
;d)=?dpG(d) ??A??f(A?
?
; d) ?
log1pG(A?
?)=?A??log1pG(A?
?)?
?dpG(d) ?
f(A?
?
; d) =?A??log1pG(A?
?)?
EpG f(A?
?
; d) =?A??log1pG(A?
?)?
outGp(A) ?
pG(A?
?)?inGp(?)
=?AoutGp(A) ???pG(A?
?)
?
log1pG(A?
?)?inGp(?
).As already discussed, under the assumptionthat Gp is proper and consistent we haveinGp(?)
= 1 for every ?.
Thus we can writeHd(Gp) =?AoutGp(A) ?H(A).
(2)The computation of outGp(A) was discussedin Section 3, and also H(A) can easily be calcu-lated.Under the restrictive assumption that aPCFG is proper and consistent, the characteri-zation in (2) was already known from (Grenan-der, 1976, Theorem 10.7, pp.
90?92).
The proofreported in that work is different from ours anduses a momentum matrix (Section 3).
Our char-acterization above is more general and uses sim-pler notation than the one in (Grenander, 1976).The sentential entropy , or entropy for short,of Gp is defined as the expectation of the infor-mation of the strings generated by Gp, i.e.,H(Gp) = EpG log1pG(w)=?wpG(w) ?
log1pG(w), (3)assuming 0 ?
log 10 = 0, for strings w not gen-erated by Gp.
It is not difficult to see thatH(Gp) ?
Hd(Gp) and equality holds if and onlyif G is unambiguous (Soule, 1974, Theorem 2.2).As ambiguity of CFGs is undecidable, it followsthat we cannot hope to obtain a closed-formsolution for H(Gp) for which equality to (2) isdecidable.
We will return to this issue in Sec-tion 6.5 Weighted intersectionIn order to compute the cross-entropy definedin the next section, we need to derive a sin-gle probabilistic model that simultaneously ac-counts for both the computations of an under-lying FA and the derivations of an underlyingPCFG.
We start from a construction originallypresented in (Bar-Hillel et al, 1964), that com-putes the intersection of a context-free languageand a regular language.
The input consists of aCFG G = (?, N, S, R) and an FA M = (?, Q,q0, Qf , T ); note that we assume, without lossof generality, that G and M share the same setof terminals ?.The output of the construction is CFG G?
=(?, N?, S?, R?
), where N?
= Q ?
(?
?
N) ?Q ?
{S?
}, and R?
consists of the set of rulesthat is obtained as follows.?
For each s ?
Qf , let S?
?
(q0, S, s) be arule of G?.?
For each rule A ?
X1 ?
?
?Xm of Gand each sequence of states s0, .
.
.
, smof M , with m ?
0, let (s0, A, sm) ?
(s0, X1, s1) ?
?
?
(sm?1, Xm, sm) be a rule ofG?
; form = 0, G?
has a rule (s0, A, s0)?
for each state s0.?
For each transition s a7?
t of M , let(s, a, t)?
a be a rule of G?.Note that for each rule (s0, A, sm) ?
(s0, X1, s1) ?
?
?
(sm?1, Xm, sm) there is a uniquerule A ?
X1 ?
?
?Xm from which it has beenconstructed by the above.
Similarly, each rule(s, a, t) ?
a uniquely identifies a transitions a7?
t. This means that if we take a completederivation d?
in G?, we can extract a sequenceh1(d?)
of rules from G and a sequence h2(d?)
oftransitions from M , where h1 and h2 are stringhomomorphisms that we define point-wise as?
h1(pi?)
= , if pi?
is S?
?
(q0, S, s);h1(pi?)
= pi, if pi?
is (s0, A, sm) ?
(s0, X1, s1) ?
?
?
(sm?1, Xm, sm) and pi is(A?
X1 ?
?
?Xm);h1(pi?)
= , if pi?
is (s, a, t)?
a;?
h2(pi?)
= , if pi?
is S?
?
(q0, S, s);h2(pi?)
= ?
, if pi?
is (s, a, t) ?
a and ?
iss a7?
t;h2(pi?)
= , if pi?
is (s0, A, sm) ?
(s0, X1, s1) ?
?
?
(sm?1, Xm, sm).We define h(d?)
= (h1(d?
), h2(d?)).
It can beeasily shown that if S?d??
w and h(d?)
= (d, c),then for the same w we have S d?
w and(q0, w)c` (s, ), some s ?
Qf .
Conversely,if for some w, d and c we have S d?
w and(q0, w)c` (s, ), some s ?
Qf , then there is pre-cisely one derivation d?
such that h(d?)
= (d, c)and S?d??
w.As noted before by (Nederhof and Satta,2003), this construction can be extended to ap-ply to a PCFG Gp = (G, pG) and an FA M .
Theoutput is a PCFG G?,p = (G?, pG?
), where G?is defined as above and pG?
is defined by:?
pG?(S?
?
(q0, S, s)) = 1;?
pG?
((s0, A, sm) ?
(s0, X1, s1) ?
?
?
(sm?1, Xm, sm)) = pG(A?
X1 ?
?
?Xm);?
pG?
((s, a, t)?
a) = 1.Note that G?,p is non-proper.
More specifically,probabilities of rules with left-hand side S?
or(s0, A, sm) might not sum to one.
This is nota problem for the algorithms presented in thispaper, as we have never assumed properness forour PCFGs.
What is most important here is thefollowing property of G?,p.
If d?, d and c aresuch that h(d?)
= (d, c), then pG?(d?)
= pG(d).Let us now assume that M is deterministic.
(In fact, the weaker condition of M being unam-biguous is sufficient for our purposes, but unam-biguity is not a very practical condition.)
Givena string w and a transition s a7?
t of M we definef(s a7?
t;w) as the frequency (number of occur-rences) of s a7?
t in the unique computation ofM , if it exists, that accepts w; this frequency is0 if w is not accepted by M .
On the basis of theabove construction of G?,p and of Section 3, wefindEpG f(sa7?
t; y(d)) =?dpG(d) ?
f(sa7?
t; y(d)) =outG?,p((s, a, t)) ?
pG?
((s, a, t)?a) ?
inG?,p(a) =outG?,p((s, a, t)) (4)6 Kullback-Leibler distanceIn this section we consider the Kullback-Leiblerdistance between a PCFGs and a PFA, andpresent a method for its optimization under cer-tain assumptions.
Let Gp = (G, pG) be a consis-tent PCFG and let Mp = (M,pM ) be a consis-tent PFA.
We demand that M be deterministic(or more generally, unambiguous).
Let us firstassume that L(G) ?
L(M); we will later dropthis constraint.The cross-entropy of Gp and Mp is defined asusual for probabilistic models, viz.
as the expec-tation under distribution pG of the informationof the strings generated by M , i.e.,H(Gp ||Mp) = EpG log1pM (w)=?wpG(w) ?
log1pM (w).The Kullback-Leibler distance of Gp and Mp isdefined asD(Gp ||Mp) = EpG logpG(w)pM (w)=?wpG(w) ?
logpG(w)pM (w).Quantity D(Gp ||Mp) can also be expressed asthe difference between the cross-entropy of Gpand Mp and the entropy of Gp, i.e.,D(Gp ||Mp) = H(Gp ||Mp)?H(Gp).
(5)Let G?,p be the PCFG obtained by intersectingGp with the non-probabilistic FA M underlyingMp, as in Section 5.
Using (4) the cross-entropyof Gp and Mp can be expressed asH(Gp ||Mp) =?wpG(w) ?
log1pM (w)=?dpG(d) ?
log1pM (y(d))=?dpG(d) ?
log?sa7?t(1pM (sa7?
t))f(sa7?t;y(d))=?dpG(d) ?
?sa7?tf(s a7?
t; y(d)) ?
log1pM (sa7?
t)=?sa7?tlog1pM (sa7?
t)?
?dpG(d) ?
f(sa7?
t; y(d)) =?sa7?tlog1pM (sa7?
t)?
EpG f(sa7?
t; y(d)) =?sa7?tlog1pM (sa7?
t)?
outG?,p((s, a, t)).We can combine the above with (5) to obtainD(Gp ||Mp) =?sa7?toutG?,p((s, a, t)) ?
log1pM (sa7?
t)?H(Gp).The values of outG?,p can be calculated eas-ily, as discussed in Section 3.
Computation ofH(Gp) in closed-form is problematic, as alreadypointed out in Section 4.
However, for manypurposes computation of H(Gp) is not needed.For example, assume that the non-probabilistic FA M underlying Mp is given, andour goal is to measure the distance between Gpand Mp, for different choices of pM .
Then thechoice that minimizes H(Gp ||Mp) determinesthe choice that minimizes D(Gp ||Mp), irre-spective of H(Gp).
Formally, we can use theabove characterization to computep?M = argmaxpMD(Gp ||Mp)= argmaxpMH(Gp ||Mp).When L(G) ?
L(M) is non-empty, bothD(Gp ||Mp) and H(Gp ||Mp) are undefined, astheir definitions imply a division by pM (w) = 0for w ?
L(G)?
L(M).
In cases where the non-probabilistic FA M is given, and our goal is tocompare the relative distances between Gp andMp for different choices of pM , it makes senseto ignore strings in L(G) ?
L(M), and defineD(Gp ||Mp), H(Gp ||Mp) and H(Gp) on the do-main L(G) ?
L(M).
Our equations above thenstill hold.
Note that strings in L(M)?L(G) canbe ignored since they do not contribute non-zerovalues to D(Gp ||Mp) and H(Gp ||Mp).7 ConclusionsWe have discussed the computation of theKL distance between PCFGs and deterministicPFAs.
We have argued that exact computationis difficult in general, but for determining therelative qualities of different PFAs, with respectto their closeness to an input PCFG, it sufficesto compute the cross-entropy.
We have shownthat the cross-entropy between a PCFG and adeterministic PFA can be computed exactly.These results can also be used for comparinga pair of PFAs, one of which is deterministic.Generalization of PCFGs to probabilistic tree-adjoining grammars (PTAGs) is also possible,by means of the intersection of a PTAG and aPFA, along the lines of (Lang, 1994).AcknowledgementsHelpful comments from Zhiyi Chi are gratefullyacknowledged.
The first author is supported bythe PIONIER Project Algorithms for Linguis-tic Processing , funded by NWO (Dutch Orga-nization for Scientific Research).
The secondauthor is partially supported by MIUR (ItalianMinistry of Education) under project PRIN No.2003091149 005.ReferencesJ.K.
Baker.
1979.
Trainable grammars forspeech recognition.
In J.J. Wolf and D.H.Klatt, editors, Speech Communication PapersPresented at the 97th Meeting of the Acousti-cal Society of America, pages 547?550.Y.
Bar-Hillel, M. Perles, and E. Shamir.
1964.On formal properties of simple phrase struc-ture grammars.
In Y. Bar-Hillel, editor,Language and Information: Selected Essayson their Theory and Application, chapter 9,pages 116?150.
Addison-Wesley.T.L.
Booth and R.A. Thompson.
1973.
Ap-plying probabilistic measures to abstract lan-guages.
IEEE Transactions on Computers,C-22(5):442?450, May.A.
Corazza, R. De Mori, R. Gretter, andG.
Satta.
1991.
Computation of probabilitiesfor an island-driven parser.
IEEE Transac-tions on Pattern Analysis and Machine In-telligence, 13(9):936?950.M.
Falkhausen, H. Reininger, and D. Wolf.1995.
Calculation of distance measures be-tween Hidden Markov Models.
In Proceedingsof Eurospeech ?95, pages 1487?1490, Madrid.U.
Grenander.
1976.
Lectures in Pattern The-ory, Vol.
I: Pattern Synthesis.
Springer-Verlag.J.E.
Hopcroft and J.D.
Ullman.
1979.
Intro-duction to Automata Theory, Languages, andComputation.
Addison-Wesley.S.E.
Hutchins.
1972.
Moments of strings andderivation lengths of stochastic context-freeggrammars.
Information Sciences, 4:179?191.B.-H. Juang and L.R.
Rabiner.
1985.
A prob-abilistic distance measure for hidden Markovmodels.
AT&T Technical Journal, 64(2):391?408.D.
Jurafsky, C. Wooters, G. Tajchman, J. Se-gal, A. Stolcke, E. Fosler, and N. Morgan.1994.
The Berkeley Restaurant Project.
InProceedings of the International Conferenceon Spoken Language Processing (ICSLP-94),pages 2139?2142, Yokohama, Japan.B.
Lang.
1994.
Recognition can be harderthan parsing.
Computational Intelligence,10(4):486?494.K.
Lari and S.J.
Young.
1990.
The estimationof stochastic context-free grammars using theInside-Outside algorithm.
Computer Speechand Language, 4:35?56.K.
Lari and S.J.
Young.
1991.
Applications ofstochastic context-free grammars using theInside-Outside algorithm.
Computer Speechand Language, 5:237?257.M.
Mohri and M.-J.
Nederhof.
2001.
Regu-lar approximation of context-free grammarsthrough transformation.
In J.-C. Junqua andG.
van Noord, editors, Robustness in Lan-guage and Speech Technology, pages 153?163.Kluwer Academic Publishers.M.
Mohri.
1997.
Finite-state transducers inlanguage and speech processing.
Computa-tional Linguistics, 23(2):269?311.M.-J.
Nederhof and G. Satta.
2003.
Proba-bilistic parsing as intersection.
In 8th Inter-national Workshop on Parsing Technologies,pages 137?148, LORIA, Nancy, France, April.M.-J.
Nederhof.
2000.
Practical experi-ments with regular approximation of context-free languages.
Computational Linguistics,26(1):17?44.M.
Rimon and J. Herz.
1991.
The recogni-tion capacity of local syntactic constraints.In Fifth Conference of the European Chap-ter of the Association for Computational Lin-guistics, Proceedings of the Conference, pages155?160, Berlin, Germany, April.S.
Soule.
1974.
Entropies of probabilistic gram-mars.
Information and Control, 25:57?74.A.
Stolcke and J. Segal.
1994.
Precise N -gram probabilities from stochastic context-free grammars.
In 32nd Annual Meeting ofthe Association for Computational Linguis-tics, Proceedings of the Conference, pages 74?79, Las Cruces, New Mexico, USA, June.M.
Vihola, M. Harju, P. Salmela, J. Suon-tausta, and J. Savela.
2002.
Two dissimilar-ity measures for HMMs and their applicationin phoneme model clustering.
In ICASSP2002, volume I, pages 933?936.C.S.
Wetherell.
1980.
Probabilistic languages:A review and some open questions.
Comput-ing Surveys, 12(4):361?379, December.
