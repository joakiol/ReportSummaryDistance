Active Learning Based Corpus AnnotationHongyan Song1 and Tianfang Yao2Shanghai Jiao Tong UniversityDepartment of Computer Science and EngineeringShanghai, China 2002401songhongyan@sjtu.org2yao-tf@cs.sjtu.edu.cnAbstractOpinion Mining aims to automatically acquireuseful opinioned information and knowledgein subjective texts.
Research of Chinese Opin-ioned Mining requires the support of annotatedcorpus for Chinese opinioned-subjective texts.To facilitate the work of corpus annotators,this paper implements an active learning basedannotation tool for Chinese opinioned ele-ments which can identify topic, sentiment, andopinion holder in a sentence automatically.1 IntroductionOpinion Mining is a novel and important re-search topic, aiming to automatically acquireuseful opinioned information and knowledge insubjective texts (Liu et al 2008).
This techniquehas wide and many real world applications, suchas e-commerce, business intelligence, informa-tion monitoring, public opinion poll, e-learning,newspaper and publication compilation, andbusiness management.
For instance, a typicalopinion mining system produces statistical re-sults from online product reviews, which can beused by potential customers when decidingwhich model to choose, by manufacturers to findout the possible areas of improvement, and bydealers for sales plan evaluation (Yao et al2008).According to Kim and Hovy (2004), an opin-ion is composed of four parts, namely, topic,holder, sentiment, and claim, in which the holderexpresses the claim including positive or nega-tive sentiment towards the topic.
For example, inthe sentence I like this car, I is the holder, like isthe positive sentiment, car is the topic, and thewhole sentence is the claim.Research on Chinese opinion mining technol-ogy requires the support of annotated corpus forChinese opinioned-subjective text.
Since the cor-pus includes deep level information related toword segmentation, part-of-speech, syntax, se-mantics, opinioned elements, and some otherinformation, the finished annotation is very com-plicated.
Hence, it is necessary to develop anautomatic tool to facilitate the work of annotatorsso that the efficiency and accuracy of annotationcan be improved.When developing the automatic annotation tool,we find it is most difficult for the tool to annotateopinioned elements automatically.
Becauseunlike other elements such as part-of-speech, anddependency relationship that needed to be anno-tated in the corpus, there is no available tool thatcan identify opinioned elements automatically.Special classifiers should be constructed to solvethis problem.In traditional supervised learning tasks, train-ing process consumes all the available annotatedtraining instances, so a classifier with high classi-fication accuracy might be constructed.
Whentraining a classifier for opinioned elements, it isvery expensive and time-consuming to get anno-tated instances.
On the other hand, unannotatedinstances are abundant in this case, because allthe texts in the corpus can be regarded as unan-notated instances before being annotated.
Thisscenario is very appropriate for active learningapplication.
An active learning algorithm picksup the instances which will improve the per-formance of the classifier to the largest extentinto the training set, and often produce classifierwith higher accuracy using less training instances.Active learning algorithm is featured withsmaller training set size, less influence from un-balanced training data and better classificationperformance comparing to classical learning al-gorithm.
This paper experimentally demonstratesthe validity of active learning algorithm whenused for opinioned elements identification andproposes a computational method for overall sys-tem performance evaluation which consists of F-measure, training time, and number of traininginstances.2 Related WorkCommon active learning algorithms can be di-vided into two classes, membership query andselective sampling (Dagan and Engelson, 1995).For membership query, algorithm constructslearning instances by itself according to theknowledge learnt, and submits the instances forhuman processing (Angluin, 1988) (Sammut andBanerji, 1986) (Shapiro, 1982).
Although thismethod has proved high learning efficiency (Da-gan and Engelson, 1995), it can be applied infewer scenarios.
Since constructing meaningfultraining instance without the knowledge of targetconcept is rather difficult.
As to selective sam-pling, algorithm picks up training instanceswhich can improve the performance of the classi-fier to the largest extent from a large variety ofavailable instances.
Algorithm in this class canbe further divided into stream-based algorithmand pool-based algorithm according to how in-stances are saved (Long et al 2008).
For stream-based algorithm (Engelson and Dagon, 1999)(Freund et al 1997), unannotated instances aresubmitted to the system successively.
All theinstances not selected by the algorithms will bediscarded.
As to pool-based algorithm (Muslea etal, 2006) (McCallum and Nigam, 1998) (Lewisand Gail, 1994), the algorithm choose the mostappropriate training instances from all the avail-able instances.
Instance not selected might havechance to be picked up in the next round.
Thoughits computational complexity is higher, selectivesampling is widely used as an active learningmethod for no prior knowledge of the target con-cept is required.Although much research has been made inthe field, we found no case which deals withmulti-classification problem in active learning.Besides, there is no available method to evaluatethe performance of active learning in informationextraction.3 Active Learning Based Corpus Anno-tation3.1 System StructureThe pool-based active learning algorithm iscomposed of two main parts: a learning engineand a selecting engine (Figure 1).
The learningengine uses instances in the training set to im-prove the performance of the classifier.
The se-lecting engine picks up unannotated instancesaccording to preset rules, submits these instancesfor human annotation, and incorporates theseinstances into the training set after the annotationis completed.
The learning engine and the select-ing engine work in turns.
The performance of theclassifier tends to improve with the increasing ofthe training set size.
When the preset condition ismet, the training process will finish.Figure 1 System WorkflowFor our active learning based annotation tool,the workflow is as follows.1.
Convert raw texts into the format whichthe algorithm can deal with.2.
Selecting engine picks up instances whichare expected to improve the performance of theclassifier to the largest extent.3.
Annotate these instances manually.4.
Learning engine incorporate these anno-tated instances into the training set, and use thenew training set to train the classifier.5.
Find out whether the performance of theclassifier satisfies the preset standard.
If not, goto step 2.6.
Use the classifier to identify the opinionedelement in the unannotated dataset.7.
Convert the result into the required format.3.2 Learning EngineThe learning engine maintains the classifier byiteratively training classifiers with new trainingsets.
The classifier adopted determines the uplimit of the system performance.
We use SupportVector Machine (SVM) (Vapnik, 1995) (Boser etal, 1992) (Chang and Lin, 1992) as the classifierfor our system for its high generalization per-formance even with feature vectors of high di-mension and its ability to manage kernel func-tions that map input data to higher dimensionalspace without increasing computational com-plexity.3.3 Selecting EngineIn our system, selecting engine picks up in-stances for human annotation, and puts the anno-tated instance into the training set.
The strategyadopted when selecting training instance is criti-cal to the overall performance of the active learn-ing algorithm.
A good strategy will more likelyto produce a classifier with high accuracy fromless training instances.The strategy we adopted here is to choose theinstances which the classifier is most unsureabout which class they belong to.
For a linear bi-classification SVM, these instances are the onesclosest to the separating hyper plane.
That means,the selecting engine will choose training in-stances according to their geometric distances tothe hyper plane.
The instance with least distancewill be selected as the next instance to be addedinto the training set while the other instances willbe saved for future reference.The computational complexity of getting thedistance between an instance and the hyper planeis low.
However, this method can not be appliedto SVM with non-linear kernel for geometricdistances are meaningless in these cases.
We useradial basis function, which is non-linear, as thekernel function in our system for it outperformslinear kernel in the experiment.
Hence, we mustfind another method to pick up training instances.Non-linear SVM decides the class an in-stance belongs to according to its decision func-tion value.S( ) ( )ss s sxy x y K x x bD??
?&& & &               (1)The instance will be classified into one cer-tain class if , or the other classif .
However, it will be difficult to clas-sify the instance according to SVM theoryif( ) 0y x !&( ) 0y x &( ) 0y x  & .
Hence, we may deduce that SVM ismost unsure when classifying an instance withleast absolute decision function value.We define the Predict Value (PV) as thevalue based on which selecting engine picks uptraining instances.For bi-classification SVM, we have PVequals to the absolute decision function value,namely,PV( ) ( )x&y x&(2)Instances with the minimum PV will be selectedinto the training set before other instances.For example, if we want to identify all thetopics in the sentence,I like this car very much, but the price is a littlebit too high.???????????????
?The PV of each instance in the sentences arelisted in Table 1.
They are calculated from thedecision function of the SVM gained from thelast round of iteration.Instances PV?
I 0.260306643320642?
very 0.553855024703612??
like 0.427269428974918?
this 0.031682276068012?
type 0.366598504697780?
car 0.095961213527654?
0.178633448748979??
but 0.092571306234562??
price 0.052164989563922?
high 0.539913276317129?
(auxiliary word) 0.458036102580422?
a little bit 0.439936293288062?
0.375263535139242Table 1 Example of 2-Classification SVMPredict ValueSuppose all the instances in this sentencehave not been added into the training set.
This(0.0316), price (0.0521), and but (0.0925) will beselected into the training set successively forthey have the minimal PVs.For multi-classification SVM, it will be morecomplicated to find the training instances.
Be-cause common multi-classification SVM is im-plemented by voting process (Hsu and Lin, 2002),there are1( 1)2t t?
 decision function values in t-classification SVM.In our system, we need to classify instancesinto 4 classes, namely, topic, holder, sentimentand other.
So a 4-classification SVM is adopted.Suppose for an instance, we get 6 Decision Func-tion Values from 6 bi-classification SVMs as inTable 2.No.
Classification Decision Function Value Result1 Class 0 Vs Class 1 1.00032792289507 02 Class 0 Vs Class 2 0.999999993721249 03 Class 0 Vs Class 3 1.00032792289507 04 Class 1 Vs Class 2 0.106393804825973 15 Class 1 Vs Class 3 -5.20417042793042E-18 36 Class 2 Vs Class 3 -0.106393804825973 3Table 2 Example of 4-Classification SVM DecisionProcessFor each bi-classification SVM, the class in-stance belongs to is determined by whether thedecision function value is greater than or lessthan zero.
The instance in Table 2 belongs toClass 0 since there 3 votes out of 6 votes forClass 0.
When deciding which class an instancebelongs to, only the decision function valuesfrom bi-classification SVMs with correct voteswill work on the certainty of the final result.Hence, we define Predict Value for multi-classification SVMs as the arithmetic mean valueof the absolute decision function value of everybi-classification SVM with correct vote,^t1, bi classification SVMs with correct votes1( ) y ( )kt t `x xk  ?
?&39&ir(3)For the instance in Table2, the value is calculatedfrom the decision function values from bi-classification SVMs numbered 1, 2, and 3.3.4 ExperimentsTo prove the validity of active learning algorithmand find out the relations between the perform-ance of the classifiers and the way the classifiersare trained, we carried out batches of experi-ments.In most information extraction tasks, a wordand its context are considered a learning sample,and encoded as feature vectors.
In our experi-ments, context data includes the part-of-speechtag, dependency relation, word semantic mean-ing, and word disambiguation information of theword being classified, its neighboring words andits parent word in dependency grammar.
Part-of-speech tag and dependency relation are commonfeatures for Chinese Natural Language Process-ing (NLP) tasks1.
We get word semantic mean-ing from HowNet, which is an online common-sense knowledge base unveiling inter-conceptualrelations and inter-attribute relations of conceptsas connoting in lexicons of the Chinese and theEnglish equivalents (Zhendong Dong and QiangDong, 1999).
Given an occurrence of a word innatural language text, word sense disambiguationis the process of identifying which sense of theword is intended if the word has a number of dis-tinct senses.
According to Song and Yao (2009),this information may help in Chinese NLP taskssuch as topic identification.Lack of explicit boundary between traininginstances and testing instances is a great differ-ence between common machine learning algo-rithm and learning algorithm designed for corpusannotation.
For common machine learning algo-rithm such as human face recognition, the quan-tity of training instances is limited while the test-ing instances could be infinite.
It is unnecessaryand impossible to annotate all the testing in-stances.
However, when annotating a corpus, allthe texts need to be annotated are decided be-forehand.
Although tools automated part of theannotation process, the results still need to bereviewed for several times to ensure the qualityof annotation.
That means in an annotation sce-nario, all the data to be processed are availableduring the training stage.The raw texts used in our experiments aretaken from forums of chinacars.com.
These textsinclude explicit subjective opinion and informalnetwork language, which are necessary for opin-ion mining research.
Most of them are commentscomposed of one or more sentences on certaintype of vehicle.
The detailed opinion elementsdistributions are showed in table 3.We use all the texts as testing data set and asubset of it as a training data set.
First of all, wepick up 10 instances for each class, and train asimple classification model with them.
Then, thebaseline system picks up k instances in sequenceand adds them into the training data set to train anew classification model iteratively until thetraining data set is as large as the testing data set,1 We use Language Technology Platform (LTP), developedby Center for Information Retrieval, Harbin Institute ofTechnology, for part-of-speech tagging, dependency rela-tionship analysis and word sense disambiguation in ourexperiment.while the active learning system picks up in-stances according to the strategy in Chapter 3.3.Type No.
of InstancesTopic 638Sentiment 769Holder 46Other 1500Total 2953Table 3 Detailed Information of the Data SetWe use three bi-classification model to testthe performance of the active learning system ontopic, sentiment, and holder identification sepa-rately and a four-classification model to identifythe three opinion elements simultaneously.
Theresults of the experiments are illustrated in Fig-ure 2, 3, 4, and 5 respectively.
Table 4, 5, and 6provide the detailed F-measure trends while dif-ferent numbers of instances are added into thetraining data set in each rounds.
For each ex-periment, we try to compare the performanceswhen we add different number of instances intothe training data set in each round of iteration.Figure 2 Topic IdentificationFigure 3 Sentiment IdentificationFigure 4 Holder IdentificationFigure 5 All Opinion Elements IdentificationAs are illustrated in the figures, the activelearning system can always achieve better or atleast no worse performance than baseline system.For example, when adding 200 instances in eachround for topic identification task (Figure2 andTable 4), the active learning system reaches itspeak value in F-measure (0.8644) with only 600training instances.
This F-measure value is evenhigher than the value the baseline system get(0.8604) after taking all the 2953 training in-stances.The active learning system outperforms thebaseline system greatly especially when dealingwith unbalanced data set (Figure 4 and Table 4).In opinion holder identification task, the baselinesystem can not find any holder until 1600 train-ing instances are taken while the active learningsystem reaches its peak F-measure value (0.8810)with only 600 training instances.
That meanswhen using active learning algorithm, it is possi-ble for us to save some time for optimizing theparameters when dealing with unbalanced data.The number of instances added to the trainingdata set in each round (k) influences the perform-ance of the active learning algorithm in a largeextent.
When a smaller value is assigned to k, theactive learning system will tend to achieve betterF-measure (Table 4) with less training instancescomparing to the baseline system.
Advantages ofthe active learning system will be diminished bythe increase in k (Table 6).4 Evaluation of Active Learning Algo-rithmFor active learning algorithm based on member-ship query, its training process will probably takelonger time by the time the optimum classier isfound, since the training process consists of sev-eral rounds of iteration.
At the beginning of theiteration, the classification speed of the model ismuch faster due to less training instances areused and the model is simple.
With more andmore training instances are added into the train-ing data set, the model will become more com-plex and more time will be needed for classifica-?
Topic Sentiment Holder All Three Elements No.
ofInstances BaselineActiveLearningBaselineActiveLearningBaselineActiveLearningBaselineActiveLearning200 0.7118 0.6221  0.6481 0.0103 0.0000 0.0000 0.6968  0.3874400 0.8072 0.8287  0.7344 0.6239 0.0000 0.0000 0.7691  0.7336600 0.8237 0.8644  0.7845 0.7860 0.0000 0.8810 0.7907  0.7979800 0.8250 0.8625  0.7876 0.8133 0.0000 0.8810 0.8020  0.82401000 0.8386 0.8613  0.7878 0.8189 0.0000 0.8810 0.8101  0.83781200 0.8389 0.8588  0.7992 0.8153 0.0000 0.8810 0.8128  0.83771400 0.8489 0.8588  0.8011 0.8141 0.0000 0.8810 0.8178  0.84711600 0.8450 0.8581  0.8033 0.8150 0.0426 0.8810 0.8211  0.84681800 0.8521 0.8581  0.8059 0.8183 0.1224 0.8810 0.8271  0.84792000 0.8528 0.8585  0.8169 0.8197 0.6857 0.8810 0.8348  0.84812200 0.8560 0.8583  0.8109 0.8200 0.8101 0.8810 0.8372  0.84682400 0.8592 0.8592  0.8186 0.8195 0.8395 0.8810 0.8404  0.84742600 0.8620 0.8610  0.8165 0.8205 0.8675 0.8810 0.8440  0.84632800 0.8578 0.8610  0.8138 0.8177 0.8810 0.8810 0.8464  0.84432953 0.8604 0.8604  0.8183 0.8183 0.8810 0.8810 0.8446  0.8446Table 4 F-measure Trends when k=200Topic Sentiment Holder All Three Elements No.
ofInstances BaselineActiveLearningBaselineActiveLearningBaselineActiveLearningBaselineActiveLearning500 0.8198 0.7730  0.7616 0.1369 0.0000 0.0000 0.7831  0.51731000 0.8386 0.8508  0.7878 0.7566 0.0000 0.8837 0.8101  0.77761500 0.8468 0.8592  0.8039 0.8175 0.0833 0.8810 0.8194  0.83982000 0.8528 0.8610  0.8169 0.8183 0.6857 0.8810 0.8348  0.84842500 0.8626 0.8583  0.8168 0.8205 0.8395 0.8810 0.8427  0.84632953 0.8604 0.8604  0.8183 0.8183 0.8810 0.8810 0.8446  0.8446Table 5  F-measure Trends when k=500Topic Sentiment Holder All Three Elements No.
ofInstances BaselineActiveLearningBaselineActiveLearningBaselineActiveLearningBaselineActiveLearning1000 0.8386 0.8335  0.7878 0.3514 0.0000 0.0000 0.8101  0.75342000 0.8528 0.8581  0.8169 0.8170 0.6857 0.8810 0.8348  0.83762953 0.8604 0.8604  0.8183 0.8183 0.8810 0.8810 0.8446  0.8446Table 6  F-measure Trends when k=1000tion.
On account of the features of active learn-ing algorithm, we believe it is necessary to find away to balance the performance of the classifierand the time it take in training process for a thor-ough evaluation of the algorithm.We define the measurement for time as:kTC(4)where C is the number of all the possible traininginstances available, k is the number of traininginstances added into the training data set in eachround of iteration.
T is the approximate value ofthe inverse ratio of the time it takes for trainingprocess.
T will have a greater value if the trainingprocess takes less time.
Its range is (0, 1] justsimilar to F-measure.We define the measurement for the traininginstances used as:(1 )nKC (5)where n is the number of the training instancesactually used.
K will have a greater value if lesstraining instances are used in the training process.The range of K is [0, 1).To judge the overall performance of an activelearning algorithm, we consider the F-measure(F) of the classifier, the time it takes during thetraining process, and the training instances used.We define the Active Learning Performance(ALP) as the harmonic mean of the three aspects:1( )(6)( ) ( )ALPK F TF k C nF C k k C n F C C nD E JD E J ?
?
?
?
?
 ?
  ?
?
where + + =1D E J , and > @, , 0,1D E J ?
.
Theyare the weights for the three measurements.
Thegreater the value of a certain weight is, the moreimportant the measurement is in the overall per-formance.
The greater the value of the ALP is,the better the performance of the active learningalgorithm.
For instance, when training a classi-fier for sentiment identification using activelearning algorithm, we get a classifier with F-measure of 0.8189 using 1000 training instancesand a classifier with F-measure of 0.8200 using2200 training instances (Table 4).
Sup-pose1= = =3D E J , we calculate the value of ALPfor the two cases according to equation (6) andget 0.1714 and 0.1507 as results respectively.That means a people with no preference amongF-measure, the number of training instancesadopted and the time used during training proc-ess will choose to get a classifier with less train-ing instances, less training time and less F-measure value.5 ConclusionThis paper experimentally demonstrates the va-lidity of active learning algorithm when used foropinioned elements identification and proposed acomputational method for overall system per-formance evaluation which consists of F-measure, training time, and number of traininginstances.
According to our tests, active learningalgorithm outperforms the base line system inmost of the cases especially when fewer in-stances are added into the training data set ineach round of iteration.
However, the methodcould extent the training time in a large scale.
Tobalance the pros and cons of active learning algo-rithm, it might be helpful to adjust the number oftraining instances added in each round dynami-cally in the training process.
For instance, addless training instances at the beginning of thetraining process to ensure a high peak value of F-measure could be achieved and add more train-ing instances later so that time spent on trainingprocess could be reduced.AcknowledgmentsThe author of this paper would like to thank In-formation Retrieval Lab, Harbin Institute ofTechnology for providing the tool (LTP) used inexperiments.
This research was supported byNational Natural Science Foundation of ChinaGrant No.60773087.ReferencesAndrew K. McCallum, Kamal Nigam.
1998.
Employ-ing EM in Pool-based Active Learning for TextClassification.
In Proceedings of the 15th Interna-tional Conference on Machine Learning.Bernhard E. Boser, Isabelle M. Guyon, and VladimirN.
Vapnik.
1992.
A Training Algorithm for Opti-mal Margin Classifiers.
In Proceedings of the FifthAnnual Workshop on Computational LearningTheory.Chih-Chung Chang and Chih-Jen Lin.
2001.
LIBSVM:a library for support vector machines.
Softwareavailable at http://www.csie.ntu.edu.tw/~cjlin/libsvmChih-Wei Hsu and Chih-Jen Lin.
2002.
A Compari-son of Methods for Multi-class Support VectorMachines.
IEEE Transactions on Neural Networks.Claude Sammut and Ranan B. Banerji.
1986.
Learn-ing Concepts by Asking Questions.
MachineLearning: An Artificial Intelligence Approach,1986, 2: 167-191Dana Angluin.
1988.
Queries and Concept Learning.Machine Learning, 1988, 2(4): 319-342David D. Lewis, William A. Gail.
1994.
A SequentialAlgorithm for Training Text Classifiers.
In Pro-ceedings of the 17th Annual International ACMSIGIR Conference on Research and Developmentin Information Retrieval.Ehud Y. Shapiro.
1982.
Algorithmic Program Debug-ging.
M.I.T.
Press.Ido Dagan, Sean P. Engelson.
1995.
Committee-Based Sampling for Training Probabilistic Classi-fiers.
In Proceedings of the International Confer-ence on Machine Learning.Ion Muslea, Steven Minton, Craig A. Knoblock.
2006.Active Learning with Multiple Views.
Journal ofArtificial Intelligence Research, 2006, 27(1): 203-233.Quansheng Liu, Tianfang Yao, Gaohui Huang, JunLiu, Hongyan Song.
2008.
A Survey of OpinionMining for Texts.
Journal of Chinese InformationProcessing.
2008, 22(6):63-68.
Jun Long, Jianping Yin, En Zhu, and Wentao Zhao.
ASurvey of Active Learning.
2008.
Journal of Com-puter Research and Development, 2008, 45(z1):300-304.Shlomo A. Engelson, Ido Dagon.
1999.
Committee-based Sample Selection for Probabilistic Classifi-ers.
Journal of Artificial Intelligence Research,1999, 11: 335-360.Hongyan Song, Jun Liu, Tianfang Yao, QuanshengLiu, Gaohui Huang.
2009.
Construction of an An-notated Corpus for Chinese Opinioned-SubjectiveTexts.
Journal of Chinese Information Processing,2009, 23(2): 123-128.Hongyan Song and Tianfang Yao.
2009.
ImprovingChinese Topic Extraction Using Word Sense Dis-ambiguation Information.
In Proceedings of the 4thInternational Conference on Innovative Computing,Information and Control.Soo-Min Kim and Eduard Hovy.
2004.
Determiningthe Sentiment of Opinions.
In Proceedings of theConference on Computational Linguistics: 1367-1373.Tianfang Yao, Xiwen Cheng, Feiyu Xu, HansUszkoreit, and Rui Wang.
2008.
A Survey of Opin-ion Mining for Texts.
Journal of Chinese Informa-tion Processing, 2008, 22(3): 71-80.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer.Yoav Freund, H.Sebastian Seung, Eli Shamir, NaftaliTishby.
1997.
Selective Sampling Using the Queryby Committee Algorithm.
Machine Learning,28(2-3): 133-168Zhendong Dong and Qiang Dong.
1999.
HowNet.http://www.keenage.com
