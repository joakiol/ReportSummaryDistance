A Bootstrapping Approach to Named Entity Classification UsingSuccessive LearnersCheng Niu, Wei Li, Jihong Ding, Rohini K. SrihariCymfony Inc.600 Essjay Road, Williamsville, NY 14221.
USA.
{cniu, wei, jding, rohini}@cymfony.comAbstractThis paper presents a new bootstrappingapproach to named entity (NE)classification.
This approach only requiresa few common noun/pronoun seeds thatcorrespond to the concept for the targetNE type, e.g.
he/she/man/woman forPERSON NE.
The entire bootstrappingprocedure is implemented as training twosuccessive learners: (i) a decision list isused to learn the parsing-based highprecision NE rules; (ii) a Hidden MarkovModel is then trained to learn stringsequence-based NE patterns.
The secondlearner uses the training corpusautomatically tagged by the first learner.The resulting NE system approachessupervised NE performance for some NEtypes.
The system also demonstratesintuitive support for tagging user-definedNE types.
The differences of thisapproach from the co-training-based NEbootstrapping are also discussed.1 IntroductionNamed Entity (NE) tagging is a fundamental taskfor natural language processing and informationextraction.
An NE tagger recognizes and classifiestext chunks that represent various proper names,time, or numerical expressions.
Seven types ofnamed entities are defined in the MessageUnderstanding Conference (MUC) standards,namely, PERSON (PER), ORGANIZATION(ORG), LOCATION (LOC), TIME, DATE,MONEY, and PERCENT1 (MUC-7 1998).1 This paper only focuses on classifying proper names.
Time andnumerical NEs are not yet explored using this method.There is considerable research on NE taggingusing different techniques.
These include systemsbased on handcrafted rules (Krupka 1998), as wellas systems using supervised machine learning,such as the Hidden Markov Model (HMM) (Bikel1997) and the Maximum Entropy Model(Borthwick 1998).The state-of-the-art rule-based systems andsupervised learning systems can reach near-humanperformance for NE tagging in a targeted domain.However, both approaches face a seriousknowledge bottleneck, making rapid domainporting difficult.
Such systems cannot effectivelysupport user-defined named entities.
That is themotivation for using unsupervised or weakly-supervised machine learning that only requires araw corpus from a given domain for this NEresearch.
(Cucchiarelli & Velardi 2001) discussedboosting the performance of an existing NE taggerby unsupervised learning based on parsingstructures.
(Cucerzan & Yarowsky 1999), (Collins& Singer 1999) and (Kim 2002) presented varioustechniques using co-training schemes for NEextraction seeded by a small list of proper namesor handcrafted NE rules.
NE tagging has two tasks:(i) NE chunking; (ii) NE classification.
Parsing-supported NE bootstrapping systems includingours only focus on NE classification, assuming NEchunks have been constructed by the parser.The key idea of co-training is the separation offeatures into several orthogonal views.
In case ofNE classification, usually one view uses thecontext evidence and the other relies on the lexiconevidence.
Learners corresponding to differentviews learn from each other iteratively.One issue of co-training is the error propagationproblem in the process of the iterative learning.The rule precision drops iteration-by-iteration.
Inthe early stages, only few instances are availablefor learning.
This makes some powerful statisticalmodels such as HMM difficult to use due to theextremely sparse data.This paper presents a new bootstrappingapproach using successive learning and concept-based seeds.
The successive learning is as follows.First, some parsing-based NE rules are learnedwith high precision but limited recall.
Then, theserules are applied to a large raw corpus toautomatically generate a tagged corpus.
Finally, anHMM-based NE tagger is trained using thiscorpus.
There is no iterative learning between thetwo learners, hence the process is free of the errorpropagation problem.
The resulting NE systemapproaches supervised NE performance for someNE types.To derive the parsing-based learner, instead ofseeding the bootstrapping process with NEinstances from a proper name list or handcraftedNE rules as (Cucerzan & Yarowsky 1999),(Collins & Singer 1999) and (Kim 2002), thesystem only requires a few common noun orpronoun seeds that correspond to the concept forthe targeted NE, e.g.
he/she/man/woman forPERSON NE.
Such concept-based seeds sharegrammatical structures with the correspondingNEs, hence a parser is utilized to supportbootstrapping.
Since pronouns and common nounsoccur more often than NE instances, richercontextual evidence is available for effectivelearning.
Using concept-based seeds, the parsing-based NE rules can be learned in one iteration sothat the error propagation problem in the iterativelearning can be avoided.This method is also shown to be effective forsupporting NE domain porting and is intuitive forconfiguring an NE system to tag user-defined NEtypes.The remaining part of the paper is organized asfollows.
The overall system design is presented inSection 2.
Section 3 describes the parsing-basedNE learning.
Section 4 presents the automaticconstruction of annotated NE corpus by parsing-based NE classification.
Section 5 presents thestring level HMM NE learning.
Benchmarks areshown in Section 6.
Section 7 is the Conclusion.2 System DesignFigure 1 shows the overall system architecture.Before the bootstrapping is started, a large rawtraining corpus is parsed by the English parserfrom our InfoXtract system (Srihari et al 2003).The bootstrapping experiment reported in thispaper is based on a corpus containing ~100,000news articles and a total of ~88,000,000 words.The parsed corpus is saved into a repository, whichsupports fast retrieval by a keyword-basedindexing scheme.Although the parsing-based NE learner is foundto suffer from the recall problem, we can apply thelearned rules to a huge parsed corpus.
In otherwords, the availability of an almost unlimited rawcorpus compensates for the modest recall.
As aresult, large quantities of NE instances areautomatically acquired.
An automaticallyannotated NE corpus can then be constructed byextracting the tagged instances plus theirneighboring words from the repository.Repository(parsed corpus)Decision ListNE LearningHMMNE LearningConcept-based Seedsparsing-based NE rulestraining corpusbased on tagged NEsNE tagging using   parsing-based rulesNETaggerBootstrapping ProcedureBootstrapping ProcedureFigure 1.
Bootstrapping System ArchitectureThe bootstrapping is performed as follows:1.
Concept-based seeds are provided by theuser.2.
Parsing structures involving concept-basedseeds are retrieved from the repository totrain a decision list for NE classification.3.
The learned rules are applied to the NEcandidates stored in the repository.4.
The proper names tagged in Step 3 andtheir neighboring words are put together asan NE annotated corpus.5.
An HMM is trained based on the annotatedcorpus.3 Parsing-based NE Rule LearningThe training of the first NE learner has three majorproperties: (i) the use of concept-based seeds, (ii)support from the parser, and (iii) representation asa decision list.This new bootstrapping approach is based onthe observation that there is an underlying conceptfor any proper name type and this concept can beeasily expressed by a set of common nouns orpronouns, similar to how concepts are defined bysynsets in WordNet (Beckwith 1991).Concept-based seeds are conceptuallyequivalent to the proper name types that theyrepresent.
These seeds can be provided by a userintuitively.
For example, a user can use pill, drug,medicine, etc.
as concept-based seeds to guide thesystem in learning rules to tag MEDICINE names.This process is fairly intuitive, creating a favorableenvironment for configuring the NE system to thetypes of names sought by the user.An important characteristic of concept-basedseeds is that they occur much more often thanproper name seeds, hence they are effective inguiding the non-iterative NE bootstrapping.A parser is necessary for concept-based NEbootstrapping.
This is due to the fact that concept-based seeds only share pattern similarity with thecorresponding NEs at structural level, not at stringsequence level.
For example, at string sequencelevel, PERSON names are often preceded by a setof prefixing title words Mr./Mrs./Miss/Dr.
etc., butthe corresponding common noun seedsman/woman etc.
cannot appear in such patterns.However, at structural level, the concept-basedseeds share the same or similar linguistic patterns(e.g.
Subject-Verb-Object patterns) with thecorresponding types of proper names.The rationale behind using concept-based seedsin NE bootstrapping is similar to that for parsing-based word clustering (Lin 1998): conceptuallysimilar words occur in structurally similar context.In fact, the anaphoric function of pronouns andcommon nouns to represent antecedent NEsindicates the substitutability of proper names bythe corresponding common nouns or pronouns.
Forexample, this man can be substituted for the propername John Smith in almost all structural patterns.Following the same rationale, a bootstrappingapproach is applied to the semantic lexiconacquisition task [Thelen & Riloff.
2002].The InfoXtract parser supports dependencyparsing based on the linguistic units constructed byour shallow parser (Srihari et al 2003).
Five typesof the decoded dependency relationships are usedfor parsing-based NE rule learning.
These are alldirectional, binary dependency links betweenlinguistic units:(1) Has_Predicate: from logical subject to verbe.g.
He said she would want him to join.
he: Has_Predicate(say)she: Has_Predicate(want)him: Has_Predicate(join)(2) Object_Of : from logical object to verbe.g.
This company was founded to providenew telecommunication services.
company: Object_Of(found)service: Object_Of(provide)(3) Has_Amod: from noun to its adjective modifiere.g.
He is a smart, handsome young man.
man: Has_AMod(smart)man: Has_AMod(handsome)man: Has_AMod(young)(4) Possess: from the possessive noun-modifier tohead noune.g.
His son was elected as mayor of the city.
his: Possess(son)city: Possess(mayor)(5) IsA:  equivalence relation from one NP toanother NPe.g.
Microsoft spokesman John Smith is apopular man.
spokesman: IsA(John Smith)John Smith: IsA(man)The concept-based seeds used in theexperiments are:1.
PER: he, she, his, her, him, man, woman2.
LOC: city, province, town, village3.
ORG: company, firm, organization, bank,airline, army, committee, government,school, university4.
PRO: car, truck, vehicle, product, plane,aircraft, computer, software, operatingsystem, data-base, book, platform, networkNote that the last target tag PRO (PRODUCT)is beyond the MUC NE standards: we added thisNE type for the purpose of testing the system?scapability in supporting user-defined NE types.From the parsed corpus in the repository, allinstances of the concept-based seeds associatedwith one or more of the five dependency relationsare retrieved:  821,267 instances in total in ourexperiment.
Each seed instance was assigned aconcept tag corresponding to NE.
For example,each instance of he is marked as PER.
The markedinstances plus their associated parsing relationshipsform an annotated NE corpus, as shown below:he/PER:   Has_Predicate(say)she/PER:   Has_Predicate(get)company/ORG:  Object_Of(compel)city/LOC:   Possess(mayor)car/PRO:  Object_Of(manufacture)HasAmod(high-quality)???
?This training corpus supports the Decision ListLearning which learns homogeneous rules (Segal& Etzioni 1994).
The accuracy of each rule wasevaluated using Laplace smoothing:No.category  NEnegativepositive1positive+++=accuracyIt is noteworthy that the PER tag dominates thecorpus due to the fact that the pronouns he and sheoccur much more often than the seeded commonnouns.
So the proportion of NE types in theinstances of concept-based seeds is not the same asthe proportion of NE types in the proper nameinstances.
For example, considering a running textcontaining one instance of John Smith and oneinstance of a city name Rochester, it is more likelythat John Smith will be referred to by he/him thanRochester by (the) city.
Learning based on such acorpus is biased towards PER as the answer.To correct this bias, we employ the followingmodification scheme for instance count.
Supposethere are a total of PERN  PER instances, LOCNLOC instances, ORGN  ORG instances, PRON  PROinstances, then in the process of rule accuracyevaluation, the involved instance count for any NEtype will be adjusted by the coefficientNEPRO,ORGLOCPERminN) N, N, N(N .
For example, ifthe number of the training instances of PER is tentimes that of PRO, then when evaluating a ruleaccuracy, any positive/negative count associatedwith PER will be discounted by 0.1 to correct thebias.A total of 1,290 parsing-based NE rules arelearned, with accuracy higher than 0.9.
Thefollowing are sample rules of the learned decisionlist:Possess(wife)  PERPossess(husband)  PERPossess(daughter)  PERPossess(bravery)  PERPossess(father)  PERHas_Predicate(divorce)  PERHas_Predicate(remarry)  PERPossess(brother)  PERPossess(son)  PERPossess(mother)  PERObject_Of(deport)  PERPossess(sister)  PERPossess(colleague)  PERPossess(career)  PERPossess(forehead)  PERHas_Predicate(smile)  PERPossess(respiratory system)  PER{Has_Predicate(threaten),Has_Predicate(kill)} PER???
?Possess(concert hall)  LOCHas_AMod(coastal)  LOCHas_AMod(northern)  LOCHas_AMod(eastern)  LOCHas_AMod(northeastern)  LOCPossess(undersecretary)  LOCPossess(mayor)  LOCHas_AMod(southern)  LOCHas_AMod(northwestern)  LOCHas_AMod(populous)  LOCHas_AMod(rogue)  LOCHas_AMod(southwestern)  LOCPossess(medical examiner)  LOCHas_AMod(edgy)  LOC???
?Has_AMod(broad-base)  ORGHas_AMod(advisory)  ORGHas_AMod(non-profit)  ORGPossess(ceo)  ORGPossess(operate loss)  ORGHas_AMod(multinational)  ORGHas_AMod(non-governmental)  ORGPossess(filings)  ORGHas_AMod(interim)  ORGHas_AMod(for-profit)  ORGHas_AMod(not-for-profit)  ORGHas_AMod(nongovernmental)  ORGObject_Of(undervalue)  ORG???
?Has_AMod(handheld)  PROHas_AMod(unman)  PROHas_AMod(well-sell)  PROHas_AMod(value-add)  PROObject_Of(refuel)  PROHas_AMod(fuel-efficient)  PROObject_Of(vend)  PROHas_Predicate(accelerate)  PROHas_Predicate(collide)  PROObject_Of(crash)  PROHas_AMod(scalable)  PROPossess(patch)  PROObject_Of(commercialize)PROHas_AMod(custom-design)  PROPossess(rollout)  PROObject_Of(redesign)  PRO???
?Due to the unique equivalence nature of the IsArelation, the above bootstrapping procedure canhardly learn IsA-based rules.
Therefore, we add thefollowing IsA-based rules to the top of the decisionlist: IsA(seed) tag of the seed, for example:IsA(man)  PERIsA(city)  LOCIsA(company)  ORGIsA(software)  PRO4 Automatic Construction of AnnotatedNE CorpusIn this step, we use the parsing-based first learnerto tag a raw corpus in order to train the second NElearner.One issue with the parsing-based NE rules ismodest recall.
For incoming documents,approximately 35%-40% of the proper names areassociated with at least one of the five parsingrelations.
Among these proper names associatedwith parsing relations, only ~5% are recognized bythe parsing-based NE rules.So we adopted the strategy of applying theparsing-based rules to a large corpus (88 millionwords), and let the quantity compensate for thesparseness of tagged instances.
A repository levelconsolidation scheme is also used to improve therecall.The NE classification procedure is as follows.From the repository, all the named entitycandidates associated with at least one of the fiveparsing relationships are retrieved.
An NEcandidate is defined as any chunk in the parsedcorpus that is marked with a proper name Part-Of-Speech (POS) tag (i.e.
NNP or NNPS).
A total of1,607,709 NE candidates were retrieved in ourexperiment.
A small sample of the retrieved NEcandidates with the associated parsingrelationships are shown below:Deep South : Possess(project)Ramada : Possess(president)Argentina : Possess(first lady)???
?After applying the decision list to the above theNE candidates, 33,104 PER names, 16,426 LOCnames, 11,908 ORG names and 6,280 PRO nameswere extracted.It is a common practice in the bootstrappingresearch to make use of heuristics that suggestconditions under which instances should share thesame answer.
For example, the one sense perdiscourse principle is often used for word sensedisambiguation (Gale et al 1992).
In this research,we used the heuristic one tag per domain for multi-word NE in addition to the one sense per discourseprinciple.
These heuristics were found to be veryhelpful in improving the performance of thebootstrapping algorithm for the purpose of bothincreasing positive instances (i.e.
tag propagation)and decreasing the spurious instances (i.e.
tagelimination).
The following are two examples toshow how the tag propagation and eliminationscheme works.Tyco Toys occurs 67 times in the corpus, and 11instances are recognized as ORG, only oneinstance is recognized as PER.
Based on theheuristic one tag per domain for multi-word NE,the minority tag of PER is removed, and all the 67instances of Tyco Toys are tagged as ORG.Three instances of Postal Service arerecognized as ORG, and two instances arerecognized as PER.
These tags are regarded asnoise, hence are removed by the tag eliminationscheme.The tag propagation/elimination scheme isadopted from (Yarowsky 1995).
After this step, atotal of 386,614 proper names were recognized,including 134,722 PER names, 186,488 LOCnames, 46,231 ORG names and 19,173 PROnames.
The overall precision was ~90%.
Thebenchmark details will be shown in Section 6.The extracted proper name instances then led tothe construction of a fairly large training corpussufficient for training the second NE learner.Unlike manually annotated running text corpus,this corpus consists of only sample stringsequences containing the automatically tagged NEinstances and their left and right neighboringwords within the same sentence.
The twoneighboring words are always regarded as commonwords while constructing the corpus.
This is basedon the observation that the proper names usuallydo not occur continuously without any punctuationin between.A small sample of the automaticallyconstructed corpus is shown below:in <LOC> Argentina </LOC> .<LOC> Argentina </LOC> 'sand <PER> Troy Glaus </PER> walkcall <ORG> Prudential Associates </ORG> ., <PRO> Photoshop </PRO> hasnot <PER> David Bonderman </PER> ,???
?This corpus is used for training the second NElearner based on evidence from string sequences,to be described in Section 5 below.5 String Sequence-based NE LearningString sequence-based HMM learning is set as ourfinal goal for NE bootstrapping because of thedemonstrated high performance of this type of NEtaggers.In this research, a bi-gram HMM is trainedbased on the sample strings in the annotated corpusconstructed in section 4.
During the training, eachsample string sequence is regarded as anindependent sentence.
The training process issimilar to (Bikel 1997).The HMM is defined as follows: Given a wordsequence nn00 fwfwsequenceW =  (wherejf denotes a single token feature which will bedefined below), the goal for the NE tagging task isto find the optimal NE tag sequencen210 ttttsequence T = , which maximizes theconditional probability sequence)W |sequence Pr(T(Bikel 1997).
By Bayesian equality, this isequivalent to maximizing the joint probabilitysequence) Tsequence,Pr(W .
This joint probabilitycan be computed by bi-gram HMM as follows:?
?=i)t,f,w|t,f,wPr(sequence) T sequence,Pr(W1i1-i1-iiiiThe back-off model is as follows,)t,w|)Pr(tt,t|f,wPr()-(1)t,f,w|t,f,w(P)t,f,w|t,f,wPr(1i1ii1iiii11i1-i1-iiii011i1-i1-iiii?????+=?
?where V denotes the size of the vocabulary, theback-off coefficients ?
?s are determined using theWitten-Bell smoothing algorithm.
The quantities)t,,w|t,f,w(P 1i11iiii0 ???
if ,)t,t|f,w(P 1iiii0 ?
, )t,w|(tP 1i1-ii0 ?
,)t|f,w(P iii0 , )t|(fP ii0 , )w|(tP 1-ii0 , )(tP i0 , and)t|(wP ii0  are computed by the maximumlikelihood estimation.We use the following single token feature setfor HMM training.
The definitions of thesefeatures are the same as in (Bikel 1997).
)t | f,w Pr( ) - (1 )t,t | f, w (P)t,t |f,w Pr(iii 2 1iiii021iiii?
?
+ =??
)w | Pr(t ) -(1 )t ,w|(tP)t ,w | Pr(t1 - i i 3 1i1-ii031i1-ii?
?
+ =??
)t | (f)Pt | (w Pr ) -(1)t |f,w (P)t|f,w Pr(ii0i i 4 iii04iii?
?
+ =) t ( P ) - (1 ) w | (t P ) w | Pr(t i0 5 1 - ii051-ii ?
?
+ =V1 ) - (1 )t |(wP)t| Pr(w 6 ii06ii ?
?
+ =twoDigitNum, fourDigitNum,containsDigitAndAlpha,containsDigitAndDash,containsDigitAndSlash,containsDigitAndComma,containsDigitAndPeriod, otherNum, allCaps,capPeriod, initCap, lowerCase, other.6 Benchmarking and DiscussionTwo types of benchmarks were measured: (i) thequality of the automatically constructed NEcorpus, and (ii) the performance of the HMM NEtagger.
The HMM NE tagger is considered to bethe resulting system for application.
Thebenchmarking shows that this system approachesthe performance of supervised NE tagger for twoof the three proper name NE types in MUC,namely, PER NE and LOC NE.We used the same blind testing corpus of300,000 words containing 20,000 PER, LOC andORG instances that were truthed in-houseoriginally for benchmarking the existingsupervised NE tagger (Srihari, Niu & Li 2000).This has the benefit of precisely measuringperformance degradation from the supervisedlearning to unsupervised learning.
Theperformance of our supervised NE tagger using theMUC scorer is shown in Table 1.Table 1.
Performance of Supervised NE TaggerType Precision Recall F-MeasurePERSON 92.3% 93.1% 92.7%LOCATION 89.0% 87.7% 88.3%ORGANIZATION 85.7% 87.8% 86.7%To benchmark the quality of the automaticallyconstructed corpus (Table 2), the testing corpus isfirst processed by our parser and then saved intothe repository.
The repository level NEclassification scheme, as discussed in section 4, isapplied.
From the recognized NE instances, theinstances occurring in the testing corpus arecompared with the answer key.Table 2.
Quality of the Constructed CorpusType PrecisionPERSON 94.3%LOCATION 91.7%ORGANIZATION 88.5%To benchmark the performance of the HMMtagger, the testing corpus is parsed.
The nounchunks with proper name POS tags (NNP andNNPS) are extracted as NE candidates.
Thepreceding word and the succeeding word of the NEcandidates are also extracted.
Then we apply theHMM to the NE candidates with their neighboringcontext.
The NE classification results are shown inTable 3.Table 3.
Performance of the second HMM NEType Precision Recall F-MeasurePERSON 86.6% 88.9% 87.7%LOCATION 82.9% 81.7% 82.3%ORGANIZATION 57.1% 48.9% 52.7%Compared with our existing supervised NEtagger, the degradation using the presentedbootstrapping method for PER NE, LOC NE, andORG NE are 5%, 6%, and 34% respectively.The performance for PER and LOC are above80%, approaching the performance of supervisedlearning.
The reason for the low recall of ORG(~50%) is not difficult to understand.
For PERSONand LOCATION, a few concept-based seeds seemto be sufficient in covering their sub-types (e.g.
thesub-types COUNTRY, CITY, etc forLOCATION).
But there are hundreds of sub-typesof ORG that cannot be covered by less than adozen concept-based seeds, which we used.
As aresult, the recall of ORG is significantly affected.Due to the same fact that ORG contains manymore sub-types, the results are also noisier, leadingto lower precision than that of the other two NEtypes.
Some threshold can be introduced, e.g.perplexity per word, to remove spurious ORG tagsin improving the precision.
As for the recall issue,fortunately, in a real-life application, theorganization type that a user is interested in usuallyis in a fairly narrow spectrum.
We believe that theperformance will be better if only company namesor military organization names are targeted.In addition to the key NE types in MUC, oursystem is able to recognize another NE type,namely, PRODUCT (PRO) NE.
We instructed ourtruthing team to add this NE type into the testingcorpus which contains ~2,000 PRO instances.Table 4 shows the performance of the HMM on thePRO tag.Table 4.
Performance of PRODUCT NETYPE PRECISION RECALL F-MEASUREPRODUCT 67.3% 72.5% 69.8%Similar to the case of ORG NEs, the number ofconcept-based seeds is found to be insufficient tocover the variations of PRO subtypes.
So theperformance is not as good as PER and LOC NEs.Nevertheless, the benchmark shows the systemworks fairly effectively in extracting the user-specified NEs.
It is noteworthy that domainknowledge such as knowing the major sub-types ofthe user-specified NE type is valuable in assistingthe selection of appropriate concept-based seedsfor performance enhancement.The performance of our HMM tagger iscomparable with the reported performance in(Collins & Singer 1999).
But our benchmarking ismore extensive as we used a much larger data set(20,000 NE instances in the testing corpus) thantheirs (1,000 NE instances).7 ConclusionA novel bootstrapping approach to NEclassification is presented.
This approach does notrequire iterative learning which may suffer fromerror propagation.
With minimal humansupervision in providing a handful of concept-based seeds, the resulting NE tagger approachessupervised NE performance in NE types forPERSON and LOCATION.
The system alsodemonstrates effective support for user-defined NEclassification.AcknowledgementThis work was partly supported by a grant from theAir Force Research Laboratory?s InformationDirectorate (AFRL/IF), Rome, NY, under contractF30602-01-C-0035.
The authors wish to thankCarrie Pine and Sharon Walter of AFRL forsupporting and reviewing this work.ReferencesBikel, D. M. 1997.
Nymble: a high-performancelearning name-finder.
Proceedings of ANLP 1997,194-201, Morgan Kaufmann Publishers.Beckwith, R. et al 1991.
WordNet: A Lexical DatabaseOrganized on Psycholinguistic Principles.
Lexicons:Using On-line Resources to build a Lexicon, UriZernik, editor, Lawrence Erlbaum, Hillsdale, NJ.Borthwick, A. et al 1998.
Description of the MENEnamed Entity System.
Proceedings of MUC-7.Collins, M. and Y.
Singer.
1999.
Unsupervised Modelsfor Named Entity Classification.
Proceedings of the1999 Joint SIGDAT Conference on EMNLP and VLC.Cucchiarelli, A. and P. Velardi.
2001.
UnsupervisedNamed Entity Recognition Using Syntactic and Se-mantic Contextual Evidence.
ComputationalLinguistics, Volume 27, Number 1, 123-131.Cucerzan, S. and D. Yarowsky.
1999.
LanguageIndependent Named Entity Recognition CombiningMorphological and Contextual Evidence.Proceedings of the 1999 Joint SIGDAT Conference onEMNLP  and VLC, 90-99.Gale, W., K. Church, and D. Yarowsky.
1992.
OneSense Per Discourse.
Proceedings of the 4th DARPASpeech and Natural Language Workshop.
233-237.Kim, J., I. Kang, and K. Choi.
2002.
UnsupervisedNamed Entity Classification Models and theirEnsembles.
COLING 2002.Krupka, G. R. and K. Hausman.
1998.
IsoQuest Inc:Description of the NetOwl Text Extraction System asused for MUC-7.
Proceedings of MUC-7.Lin, D.K.
1998.
Automatic Retrieval and Clustering ofSimilar Words.
COLING-ACL 1998.MUC-7, 1998.
Proceedings of the Seventh MessageUnderstanding Conference (MUC-7).Thelen, M. and E. Riloff.
2002.
A BootstrappingMethod for Learning Semantic Lexicons usingExtraction Pattern Contexts.
Proceedings of EMNLP2002.Segal, R. and O. Etzioni.
1994.
Learning decision listsusing homogeneous rules.
Proceedings of the 12thNational Conference on Artificial Intelligence.Srihari, R., W. Li, C. Niu and T. Cornell.
2003.InfoXtract: An Information Discovery EngineSupported by New Levels of Information Extraction.Proceeding of HLT-NAACL 2003 Workshop onSoftware Engineering and Architecture of LanguageTechnology Systems, Edmonton, Canada.Srihari, R., C. Niu, & W. Li.
2000.
A Hybrid Approachfor Named Entity and Sub-Type Tagging.Proceedings of ANLP 2000, Seattle.Yarowsky, David.
1995.
Unsupervised Word SenseDisambiguation Rivaling Supervised Method.
ACL1995.
