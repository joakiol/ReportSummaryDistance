Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 195?200,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsApplying morphological decomposition to statistical machine translationSami Virpioja and Jaakko Va?yrynen and Andre?
Mansikkaniemi and Mikko KurimoAalto University School of Science and TechnologyDepartment of Information and Computer SciencePO BOX 15400, 00076 Aalto, Finland{svirpioj,jjvayryn,ammansik,mikkok}@cis.hut.fiAbstractThis paper describes the Aalto submissionfor the German-to-English and the Czech-to-English translation tasks of the ACL2010 Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR.Statistical machine translation has focusedon using words, and longer phrases con-structed from words, as tokens in the sys-tem.
In contrast, we apply different mor-phological decompositions of words usingthe unsupervised Morfessor algorithms.While translation models trained using themorphological decompositions did not im-prove the BLEU scores, we show that theMinimum Bayes Risk combination witha word-based translation model producessignificant improvements for the German-to-English translation.
However, we didnot see improvements for the Czech-to-English translations.1 IntroductionThe effect of morphological variation in languagescan be alleviated by using word analysis schemes,which may include morpheme discovery, part-of-speech tagging, or other linguistic information.Words are very convenient and even efficient rep-resentation in statistical natural language process-ing, especially with English, but morphologicallyrich languages can benefit from more fine-grainedinformation.
For instance, statistical morphs dis-covered with unsupervised methods result in bet-ter performance in automatic speech recognitionfor highly-inflecting and agglutinative languages(Hirsima?ki et al, 2006; Kurimo et al, 2006).Virpioja et al (2007) applied morph-basedmodels in statistical machine translation (SMT)between several language pairs without gainingimprovement in BLEU score, but obtaining re-ductions in out-of-vocabulary rates.
They uti-lized morphs both in the source and in the tar-get language.
Later, de Gispert et al (2009)showed that Minimum Bayes Risk (MBR) com-bination of word-based and morph-based trans-lation models improves translation with Arabic-to-English and Finnish-to-English language pairs,where only the source language utilized morph-based models.
Similar results have been shown forFinnish-to-English and Finnish-to-German in per-formance evaluation of various unsupervised mor-pheme analysis algorithms in Morpho Challenge2009 competition (Kurimo et al, 2009).We continue the research described above andexamine how the level of decomposition affectsboth the individual morph-based systems andMBR combinations with the baseline word-basedmodel.
Experiments are conducted with theWMT10 shared task data for German-to-Englishand Czech-to-English language pairs.2 MethodsIn this work, morphological analyses are con-ducted on the source language data, and each dif-ferent analysis is applied to create a unique seg-mentation of words into morphemes.
Translationsystems are trained with the Moses toolkit (Koehnet al, 2007) from each differently segmented ver-sion of the same source language to the target lan-guage.
Evaluation with BLEU is performed onboth the individual systems and system combina-tions, using different levels of decomposition.2.1 Morphological models for wordsMorfessor (Creutz and Lagus, 2002; Creutz andLagus, 2007, etc.)
is a family of methods forunsupervised morphological segmentation.
Mor-fessor does not limit the number of morphemesfor each word, making it suitable for agglutina-tive and compounding languages.
An analysis of asingle word is a list of non-overlapping segments,195morphs, stored in the model lexicon.
We use boththe Morfessor Baseline (Creutz and Lagus, 2005b)and the Morfessor Categories-MAP (Creutz andLagus, 2005a) algorithms.1 Both are formulatedin a maximum a posteriori (MAP) framework, i.e.,the learning algorithm tries to optimize the prod-uct of the model prior and the data likelihood.The generative model applied by MorfessorBaseline assumes that the morphs are independent.The resulting segmentation can be influenced byusing explicit priors for the morph lengths andfrequencies, but their effect is usually minimal.The training data has a larger effect on the re-sults: A larger data set alows a larger lexicon,and thus longer morphs and less morphs per word(Creutz and Lagus, 2007).
Moreover, the modelcan be trained with or without taking into accountthe word frequencies.
If the frequencies are in-cluded, the more frequent words are usually un-dersegmented compared to a linguistic analysis,whereas the rare words are oversegmented (Creutzand Lagus, 2005b).
An easy way to control theamount of segmentation is to weight the trainingdata likelihood by a positive factor ?.
If ?
> 1,the increased likelihood results in longer morphs.If ?
< 1, the morphs will be shorter and the wordsmore segmented.Words that are not present in the training datacan be segmented using an algorithm similar toViterbi.
The algorithm can be modified to allownew morphs types to be used by using an approx-imative cost of adding them into the lexicon (Vir-pioja and Kohonen, 2009).
The modification pre-vents oversegmentation of unseen word forms.
Inmachine translation, this is important especiallyfor proper nouns, for which there is usually noneed for translation.The Morfessor Categories-MAP algorithm ex-tends the model by imposing morph categories ofstems, prefixes and suffixes, as well as transitionprobabilities between them.
In addition, it appliesa hierarchical segmentation model that allows it toconstruct new stems from smaller pieces of ?non-morphemes?
(Creutz and Lagus, 2007).
Due tothese features, it can provide reasonable segmen-tations also for those words that contain new mor-phemes.
The drawback of the more sophisticatedmodel is the slower and more complex training al-gorithm.
In addition, the amount of the segmenta-1The respective software is available at http://www.cis.hut.fi/projects/morpho/tion is harder to control.Morfessor Categories-MAP was applied to sta-tistical machine translation by Virpioja et al(2007) and de Gispert et al (2009).
However,Kurimo et al (2009) report that Morfessor Base-line outperformed Categories-MAP in Finnish-to-English and German-to-English tasks both withand without MBR combination, although the dif-ferences were not statistically significant.
In allthe previous cases, the models were trained onword types, i.e., without using their frequencies.Here, we also test models trained on word tokens.2.2 Statistical machine translationWe utilize the Moses toolkit (Koehn et al, 2007)for statistical machine translation.
The default pa-rameter values are used except with the segmentedsource language, where the maximum sentencelength is increased from 80 to 100 tokens to com-pensate for the larger number of tokens in text.2.3 Morphological model combinationFor combining individual models, we apply Min-imum Bayes Risk (MBR) system combination(Sim et al, 2007).
N-best lists from multipleSMT systems trained with different morpholog-ical analysis methods are merged; the posteriordistributions over the individual lists are interpo-lated to form a new distribution over the mergedlist.
MBR hypotheses selection is then performedusing sentence-level BLEU score (Kumar andByrne, 2004).In this work, the focus of the system combina-tion is not to combine different translation systems(e.g., Moses and Systran), but to combine systemstrained with the same translation algorithm usingthe same source language data with with differentmorphological decompositions.3 ExperimentsThe German-to-English and Czech-to-Englishparts of the ACL WMT10 shared task data wereinvestigated.
Vanilla SMT models were trainedwith Moses using word tokens for MBR combi-nation and comparison purposes.
Several differentmorphological segmentation models for Germanand Czech were trained with Morfessor.
Each seg-mentation model corresponds to a morph-basedSMT model trained with Moses.
The word-basedvanilla Moses model is compared to each morph-based model as well as to several MBR com-196binations between word-based translation modelsand morph-based translation models.
Quantitativeevaluation is carried out using the BLEU scorewith re-cased and re-tokenized translations.4 DataThe data used in the experiments consistedof Czech-to-English (CZ-EN) and German-to-English (DE-EN) parallel language data fromACL WMT10.
The data was divided into distincttraining, development, and evaluation sets.
Statis-tics and details are shown in Table 1.Aligned data from Europarl v5 and NewsCommentary corpora were included in trainingGerman-to-English SMT models.
The Englishpart from the same data sets was used for train-ing a 5-gram language model, which was used inall translation tasks.
The Czech-to-English trans-lation model was trained with CzEng v0.9 (train-ing section 0) and News Commentary data.
Themonolingual German and Czech parts of the train-ing data sets were used for training the morph seg-mentation models with Morfessor.The data sets news-test2009, news-syscomb2009 and news-syscombtune2010from the ACL WMT 2009 and WMT 2010,were used for development.
The news-test2008,news-test2010, and news-syscombtest2010 datasets were used for evaluation.4.1 PreprocessingAll data sets were preprocessed before use.
XML-tags were removed, text was tokenized and char-acters were lowercased for every training, devel-opment and evaluation set.Morphological models for German and Czechwere trained using a corpus that was a combina-tion of the respective training sets.
Then the mod-els were used for segmenting all the data sets, in-cluding development and evaluation sets, with theViterbi algorithm discussed in Section 2.1.
Themodification of allowing new morph types for out-of-vocabulary words was not applied.The Moses cleaning script performed additionalfiltering on the parallel language training data.Specifically, sentences with over 80 words wereremoved from the vanilla Moses word-based mod-els.
For morph-based models the limit was setto 100 morphs, which is the maximum limit ofthe Giza++ alignment tool.
After filtering with athreshold of 100 tokens, the different morph seg-mentations for DE-EN training data from com-bined Europarl and News Commentary data setsranged from 1 613 556 to 1 624 070 sentences.Similarly, segmented CZ-EN training data rangedfrom 896 163 to 897 744 sentences.
The vanillawords-based model was trained with 1 609 998sentences for DE-EN and 897 497 sentences forCZ-EN.5 ResultsThe details of the ACL WMT10 submissions areshown in Table 2.
The results of experiments withdifferent morphological decompositions and MBRsystem combinations are shown in Table 3.
Thesignificances of the differences in BLEU scoresbetween the word-based model (Words) and mod-els with different morphological decompositionswas measured by dividing each evaluation data setinto 49 subsets of 41?51 sentences, and using theone-sided Wilcoxon signed rank test (p < 0.05).5.1 SegmentationWe created several word segmentations with Mor-fessor baseline and Morfessor Categories-MAP(CatMAP).
Statistics for the different segmenta-tions are given in Table 3.
The amount of seg-mentation was measured as the average number ofmorphs per word (m/w) and as the percentage ofsegmented words (s-%) in the training data.
In-creasing the data likelihood weight ?
in Morfes-sor Baseline increases the amount of segmentationfor both languages.
However, it had little effecton the proportion of segmented words in the threeevaluation data sets: The proportion of segmentedword tokens was 10?11 % for German and 8?9 %for Czech, whereas the out-of-vocabulary rate was7.5?7.8 % for German and 4.8?5.6 % for Czech.Disregarding the word frequency informationin Morfessor Baseline (nofreq) produced moremorphs per word type and segmented nearlyall words in the training data.
The MorfessorCatMAP algorithm created segmentations with thelargest number of morphs per word, but did notsegment as many words as the Morfessor Baselinewithout the frequencies.5.2 Morph-based translation systemsThe models with segmented source language per-formed worse individually than the word-basedmodels.
The change in the BLEU score was statis-tically significant in almost all segmentations and197Data set Statistics Training Development EvaluationSentences Words per sentence SM LM TMDE CZ EN DE CZ EN DE-EN CZ-EN {DE,CZ}-EN {DE,CZ}-ENEuroparl v5 1 540 549 23.2 25.2 x x xNews Commentary 100 269 21.9 18.9 21.5 x x x x xCzEng v0.9 (training section 0) 803 286 8.3 9.9 x xnews-test2009 2 525 21.7 18.8 23.2 xnews-syscomb2009 502 19.7 17.2 21.1 xnews-syscombtune2010 455 20.2 17.3 21.0 xnews-test2008 2 051 20.3 17.8 21.7 xnews-test2010 2 489 21.7 18.4 22.3 xnews-syscombtest2010 2 034 22.0 18.6 22.6 xTable 1: Data sets for the Czech-to-English and German-to-English SMT experiments, including thenumber of aligned sentences and the average number of words per sentence in each language.
The datasets used for model training, development and evaluation are marked.
Training is divided into German(DE) and Czech (CZ) segmentation model (SM) training, English (EN) language model (LM) trainingand German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation model (TM) training.Submission Segmentation model for source language BLEU-cased(news-test2010)aalto DE-EN WMT10 Morfessor Baseline (?
= 0.5) 17.0aalto DE-EN WMT10 CatMAP Morfessor Categories-MAP 16.5aalto CZ-EN WMT10 Morfessor Baseline (?
= 0.5) 16.2aalto CZ-EN WMT10 CatMAP Morfessor Categories-MAP 15.9Table 2: Our submissions for the ACL WMT10 shared task in translation.
The translation models aretrained from the segmented source language into unsegmented target language with Moses.all evaluation sets.
Morfessor Baseline (?
= 0.5)was the best individual segmented model for bothGerman and Czech in the sense that it had thelowest number of significant decreases the BLEUscore compared to the word-based model.
Remov-ing word frequency information with MorfessorBaseline and using Morfessor CatMAP gave thelowest BLEU scores with both source languages.5.3 Translation system combinationFor the DE-EN language pair, all MBR systemcombinations between each segmented model andthe word-based model had slightly higher BLUEscores than the individual word-based model.Nearly all improvements were statistically signifi-cant.The BLEU scores for the MBR combinationsin the CZ-EN language pair were mostly not sig-nificantly different from the individual word-basedmodel.
Two scores were significantly lower.6 DiscussionWe have applied concatenative morphologicalanalysis, in which each original word token is seg-mented into one or more non-overlapping morphtokens.
Our results with different levels of seg-mentation with Morfessor suggest that the optimallevel of segmentation is language pair dependentin machine translation.Our approach for handling rich morphology hasnot been able to directly improve the translationquality.
We assume that improvements might stillbe possible by carefully tuning the amount of seg-mentation.
The experiments in this paper withdifferent values of the ?
parameter for Morfes-sor Baseline were conducted with the word fre-quencies.
The parameter had little effect on theproportion of segmented words in the evaluationdata sets, as frequent words were not segmentedat all, and out-of-vocabulary words were likely tobe oversegmented by the Viterbi algorithm.
Fu-ture work includes testing a larger range of val-ues for ?, also for models trained without theword frequencies, and using the modification ofthe Viterbi algorithm proposed in Virpioja and Ko-honen (2009).It might also be helpful to only segment selectedwords, where the selection would be based on thepotential benefit in the translation process.
In gen-eral, the direct segmentation of words into morphsis problematic because it increases the numberof tokens in the text and directly increases bothmodel training and decoding complexity.
How-ever, an efficient segmentation decreases the num-ber of types and the out-of-vocabulary rate (Virpi-oja et al, 2007).We have replicated here the result that an MBRcombination of a morph-based MT system with198Segmentation (DE) Statistics (DE) BLEU-cased (DE-EN)news-test2008 news-test2010 news-syscombtest2010m/w s-% No MBR MBR with No MBR No MBR MBR withWords WordsWords 1.00 0.0% 16.37 - 17.28 13.22 -Morfessor Baseline (?
= 0.5) 1.82 72.4% 15.19?
16.47+ 17.04?
13.28?
13.70+Morfessor Baseline (?
= 1.0) 1.65 61.0% 15.14?
16.54+ 16.87?
11.95?
13.66+Morfessor Baseline (?
= 5.0) 1.24 23.7% 15.04?
16.44?
16.63?
11.78?
13.43+Morfessor CatMAP 2.25 67.5% 14.21?
16.42?
16.53?
11.15?
13.61+Morfessor Baseline nofreq 2.24 91.6% 13.98?
16.47+ 16.36?
10.66?
13.58+Segmentation (CZ) Statistics (CZ) BLEU-cased (CZ-EN)news-test2008 news-test2010 news-syscombtest2010m/w s-% No MBR MBR with No MBR No MBR MBR withWords WordsWords 1.00 0.0% 14.91 - 16.73 12.75 -Morfessor Baseline (?
= 0.5) 1.19 17.7% 13.22?
14.87?
16.01?
12.60?
12.53?Morfessor Baseline (?
= 1.0) 1.09 8.1% 13.33?
14.88?
16.10?
11.29?
12.84?Morfessor Baseline (?
= 5.0) 1.03 2.9% 13.53?
14.83?
15.92?
11.17?
12.85?Morfessor CatMAP 2.29 71.9% 11.93?
14.86?
15.79?
10.12?
10.79?Morfessor Baseline nofreq 2.18 90.3% 12.43?
14.96?
15.82?
10.13?
12.89?Table 3: Results for German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation models.The source language is segmented with the shown algorithms.
The amount of segmentation in the train-ing data is measured with the average number of morphs per word (m/w) and as proportion of segmentedwords (s-%) against the word-based model (Words).
The trained translation systems are evaluated in-dependently (No MBR) and in Minimum Bayes Risk system combination of word-based translationsystems (MBR).
Unchanged (?
), significantly higher (+) and lower (?)
BLEU scores compared to theword-based translation model (Words) are marked.
The best morph-based model for each column isemphasized.a word-based MT system can produce a BLEUscore that is higher than from either of the indi-vidual systems (de Gispert et al, 2009; Kurimoet al, 2009).
With the DE-EN language pair, theimprovement was statistically significant with alltested segmentation models.
However, the im-provements were not as large as those obtainedbefore and the results for the CZ-EN languagepair were not significantly different in most cases.Whether this is due to the different languages,training data sets, the domain of the evaluationdata sets, or some problems in the model training,is currently uncertain.One very different approach for applying dif-ferent levels of linguistic analysis is factor mod-els for SMT (Koehn and Hoang, 2007), wherepre-determined factors (e.g., surface form, lemmaand part-of-speech) are stored as vectors for eachword.
This provides better integration of mor-phosyntactic information and more control of theprocess, but the translation models are more com-plex and the number and factor types in each wordmust be fixed.Our submissions to the ACL WMT10 sharedtask utilize unsupervised morphological decompo-sition models in a straightforward manner.
Theindividual morph-based models trained with thesource language words segmented into morphsdid not improve the vanilla word-based modelstrained with the unsegmented source language.We have replicated the result for the German-to-English language pair that an MBR combina-tion of a word-based and a segmented morph-based model gives significant improvements to theBLEU score.
However, we did not see improve-ments for the Czech-to-English translations.AcknowledgmentsThis work was supported by the Academy ofFinland in the project Adaptive Informatics, theFinnish graduate school in Language Technology,and the IST Programme of the European Commu-nity, under the FP7 project EMIME (213845).ReferencesMathias Creutz and Krista Lagus.
2002.
Unsuper-vised discovery of morphemes.
In Proceedings ofthe Workshop on Morphological and PhonologicalLearning of ACL?02, pages 21?30, Philadelphia,Pennsylvania, USA.Mathias Creutz and Krista Lagus.
2005a.
Inducing themorphological lexicon of a natural language fromunannotated text.
In Proceedings of the AKRR?05,Espoo, Finland.199Mathias Creutz and Krista Lagus.
2005b.
Unsu-pervised morpheme segmentation and morphologyinduction from text corpora using Morfessor 1.0.Technical Report A81, Publications in Computerand Information Science, Helsinki University ofTechnology.Mathias Creutz and Krista Lagus.
2007.
Unsuper-vised models for morpheme segmentation and mor-phology learning.
ACM Transactions on Speech andLanguage Processing, 4(1), January.Adria` de Gispert, Sami Virpioja, Mikko Kurimo, andWilliam Byrne.
2009.
Minimum Bayes risk com-bination of translation hypotheses from alternativemorphological decompositions.
In Proceedings ofHuman Language Technologies: The 2009 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, Com-panion Volume: Short Papers, pages 73?76, Boul-der, USA, June.
Association for Computational Lin-guistics.Teemu Hirsima?ki, Mathias Creutz, Vesa Siivola, MikkoKurimo, Sami Virpioja, and Janne Pylkko?nen.2006.
Unlimited vocabulary speech recognitionwith morph language models applied to Finnish.Computer Speech and Language, 20(4):515?541.Philipp Koehn and Hieu Hoang.
2007.
Factored trans-lation models.
In Proceedings of the EMNLP 2007,pages 868?876, Prague, Czech Republic, June.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Annual Meeting of ACL, demonstration ses-sion, pages 177?180, Czech Republic, June.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine transla-tion.
In Proceedings of the HLT-NAACL 2004, pages169?176.Mikko Kurimo, Antti Puurula, Ebru Arisoy, Vesa Si-ivola, Teemu Hirsima?ki, Janne Pylkko?nen, TanelAluma?e, and Murat Saraclar.
2006.
Unlimited vo-cabulary speech recognition for agglutinative lan-guages.
In Proceedings of the HLT-NAACL 2006,pages 487?494, New York, USA.Mikko Kurimo, Sami Virpioja, Ville T. Turunen,Graeme W. Blackwood, and William Byrne.
2009.Overview and results of Morpho Challenge 2009.
InWorking Notes for the CLEF 2009 Workshop, Corfu,Greece, September.K.
C. Sim, W. J. Byrne, M. J. F. Gales, H. Sahbi, andP.
C. Woodl.
2007.
Consensus network decodingfor statistical machine translation system combina-tion.
In IEEE Int.
Conf.
on Acoustics, Speech, andSignal Processing.Sami Virpioja and Oskar Kohonen.
2009.
Unsuper-vised morpheme analysis with Allomorfessor.
InWorking notes for the CLEF 2009 Workshop, Corfu,Greece.Sami Virpioja, Jaakko J. Va?yrynen, Mathias Creutz,and Markus Sadeniemi.
2007.
Morphology-awarestatistical machine translation based on morphs in-duced in an unsupervised manner.
In Proceedingsof the Machine Translation Summit XI, pages 491?498, Copenhagen, Denmark, September.200
