In: Proceedings of CoNLL-2000 and LLL-2000, pages 67-72, Lisbon, Portugal, 2000.Knowledge-Free Induction of MorphologyUsing Latent Semantic AnalysisPat r i ck  Schone and Dan ie l  Ju ra fskyUniversity of ColoradoBoulder, Colorado 80309{schone, jurafsky}@cs.colorado.eduAbst ractMorphology induction is a subproblem ofimportant tasks like automatic learning ofmachine-readable dictionaries and grammar in-duction.
Previous morphology induction ap-proaches have relied solely on statistics of hy-pothesized stems and affixes to choose whichaffixes to consider legitimate.
Relying on stem-and-affix statistics rather than semantic knowl-edge leads to a number of problems, such as theinappropriate use of valid affixes ("ally" stem-ming to "all").
We introduce a semantic-basedalgorithm for learning morphology which onlyproposes affixes when the stem and stem-plus-affix are sufficiently similar semantically.
Weimplement our approach using Latent Seman-tic Analysis and show that our semantics-onlyapproach provides morphology induction resultsthat rival a current state-of-the-art system.1 In t roduct ionComputational morphological analyzers haveexisted in various languages for years and it hasbeen said that "the quest for an efficient methodfor the analysis and generation of word-forms isno longer an academic research topic" (Karlssonand Karttunen, 1997).
However, developmentof these analyzers typically begins with humanintervention requiring time spans from days toweeks.
If it were possible to build such ana-lyzers automatically without human knowledge,significant development time could be saved.On a larger scale, consider the taskof inducing machine-readable dictionaries(MRDs) using no human-provided information("knowledge-free").
In building an MRD,"simply expanding the dictionary to encompassevery word one is ever likely to encounter...failsto take advantage of regularities" (Sproat,1992, p. xiii).
Hence, automatic morphologicalanalysis is also critical for selecting appropriateand non-redundant MRD headwords.For the reasons expressed above, we are in-terested in knowledge-free morphology induc-tion.
Thus, in this paper, we show how to au-tomatically induce morphological relationshipsbetween words.Previous morphology induction approaches(Goldsmith, 1997, 2000; D4Jean, 1998; Gauss-ier, 1999) have focused on inflectional languagesand have used statistics of hypothesized stemsand affixes to choose which affixes to considerlegitimate.
Several problems can arise usingonly stem-and-affix statistics: (1) valid affixesmay be applied inappropriately ("ally" stem-ming to "all"), (2) morphological ambiguitymay arise ("rating" conflating with "rat" in-stead of "rate"), and (3) non-productive affixesmay get accidentally pruned (the relationshipbetween "dirty" and "dirt" may be lost)3Some of these problems could be resolvedif one could incorporate word semantics.
Forinstance, "all" is not semantically similar to"ally," so with knowledge of semantics, an algo-rithm could avoid conflating these two words.To maintain the "knowledge-free" paradigm,such semantics would need to be automati-cally induced.
Latent Semantic Analysis (LSA)(Deerwester, et al, 1990); Landauer, et al,1998) is a technique which automatically iden-tifies semantic information from a corpus.
Wehere show that incorporating LSA-based seman-tics alone into the morphology-induction pro-cess can provide results that rival a state-ohthe-art system based on stem-and-affix statis-tics (Goldsmith's Linguistica).1Error examples are from Goldsmith's Linguistica67Our algorithm automatically extracts poten-tial affixes from an untagged corpus, identifiesword pairs sharing the same proposed stem buthaving different affixes, and uses LSA to judgesemantic relatedness between word pairs.
Thisprocess erves to identify valid morphological re-lations.
Though our algorithm could be appliedto any inflectional language, we here restrictit to English in order to perform evaluationsagainst the human-labeled CELEX database(Baayen, et al, 1993).2 P rev ious  workExisting induction algorithms all focus on iden-tifying prefixes, suffixes, and word stems in in-flectional languages (avoiding infixes and otherlanguage types like concatenative or aggluti-native languages (Sproat, 1992)).
They alsoobserve high frequency occurrences of someword endings or beginnings, perform statisticsthereon, and propose that some of these ap-pendages are valid morphemes.However, these algorithms differ in specifics.D~Jean (1998) uses an approach derived fromHarris (1951) where word-splitting occurs if thenumber of distinct letters that follows a givensequence of characters urpasses a threshoid.He uses these hypothesized affixes to resegmentwords and thereby identify additional affixesthat were initially overlooked.
His overall goal isdifferent from ours: he primarily seeks an affixinventory.Goldsmith (1997) tries cutting each wordin exactly one place based on probability andlengths of hypothesized stems and affixes.
Heapplies the EM algorithm to eliminate inappro-priate parses.
He collects the possible suffixesfor each stem calling these a signature whichaid in determining word classes.
Goldsmith(2000) later incorporates minimum descriptionlength to identify stemming characteristics thatmost compress the data, but his algorithm oth-erwise remains similar in nature.
Goldsmith'salgorithm is practically knowledge-free, thoughhe incorporates capitalization removal and someword segmentation.Gaussier (1999) begins with an inflectionallexicon and seeks to find derivational morphol-ogy.
The words and parts of speech from hisinflectional lexicon serve for building relationalfamilies of words and identifying sets of wordpairs and suffixes therefrom.
Gaussier splitswords based on p-similarity - words that agreein exactly the first p characters.
He also buildsa probabilistic model which indicates that theprobability of two words being morphologicalvariants is based upon the probability of theirrespective changes in orthography and morpho-syntactics.3 Cur rent  approachOur algorithm also focuses on inflectional lan-guages.
However, with the exception of wordsegmentation, we provide it no human informa-tion and we consider only the impact of seman-tics.
Our approach (see Figure 1) can be de-composed into four components: (1) initiallyselecting candidate affixes, (2) identifying af-fixes which are potential morphological vari-ants of each other, (3) computing semantic vec-tors for words possessing these candidate affixes,and (4) selecting as valid morphological variantsthose words with similar semantic vectors.Figure 1: Processing ArchitectureStage 1 Stage 2 Stage 3 Stage 4Identify I\[ paa~~ l~ I\[ semantic II variantspotential \[lare pos lmell vectors II that haveaffixes I I morplm- I I for I I slmuar........ ) ( logical \]( words \] ( semantic3.1 Hypothes iz ing  affixesTo select candidate affixes, we, like Gaussier,identify p-similar words.
We insert words into atrie (Figure 2) and extract potential affixes byobserving those places in the trie where branch-ing occurs.
Figure 2's hypothesized suffixes areNULL, "s," "ed," "es," "ing," "e," and "eful.
"We retain only the K most-frequent candidateaffixes for subsequent processing.
The value forK needs to be large enough to account for thenumber of expected regular affixes in any givenlanguage as well as some of the more frequentirregular affixes.
We arbitrarily chose K to be200 in our system.
(It should also be mentionedthat we can identify potential prefixes by insert-ing words into the trie in reversed order.
Thisprefix mode can additionally serve for identify-ing capitalization.
)68F igure  2: Trie structure(0 0()( )3.2 Morpho log ica l  variantsWe next identify pairs of candidate affixes thatdescend from a common ancestor node in thetrie.
For example, ("s", NULL) constitutes sucha pair from Figure 2.
We call these pairs rules.Two words sharing the same root and thesame affix rule, such as "cars" and "car," formwhat we call a pair of potential morphologicalvariants (PPMVs).
We define the ruleset of agiven rule to be the set of all PPMVs that havethat rule in common.
For instance, from Figure2, the ruleset for ("s", NULL) would be the pairs"cars/car" and "cares/care."
Our algorithm es-tablishes a list which identifies the rulesets forevery hypothesized rule extracted from the dataand then it must proceed to determine whichrulesets or PPMVs describe true morphologicalrelationships.3.3 Computing Semantic VectorsDeerwester, et al (1990) showed that it ispossible to find significant semantic relation-ships between words and documents in a corpuswith virtually no human intervention (with thepossible exception of a human-built stop wordlist).
This is typically done by applying singu-lar value decomposition (SVD) to a matrix, M,where each entry M(i,j) contains the frequencyof word i as seen in document j of the corpus.This methodology is referred to as Latent Se-mantic Analysis (LSA) and is well-described inthe literature (Landauer, et al, 1998; Manningand Schfitze, 1999).SVDs seek to decompose a matrix A into theproduct of three matrices U, D, and V T whereU and V T are  orthogonal matrices and D isa diagonal matrix containing the singular val-ues (squared eigenvalues) of A.
Since SVD'scan be performed which identify singular val-ues by descending order of size (Berry, et al,1993), LSA truncates after finding the k largestsingular values.
This corresponds to projectingthe vector representation of each word into ak-dimensional subspace whose axes form k (la-tent) semantic directions.
These projections areprecisely the rows of the matrix product UkDk.A typical k is 300, which is the value we used.However, we have altered the algorithm some-what to fit our needs.
First, to stay as close tothe knowledge-free scenario as possible, we nei-ther apply a stopword list nor remove capitaliza-tion.
Secondly, since SVDs are more designedto work on normally-distributed data (Manningand Schiitze, 1999, p. 565), we operate on Z-scores rather than counts.
Lastly, instead ofgenerating a term-document matrix, we build aterm-term atrix.Schiitze (1993) achieved excellent perfor-mance at classifying words into quasi-part-of-speech classes by building and perform-ing an SVD on an Nx4N term-term matrix,M(i,Np+j).
The indices i and j represent thetop N highest frequency words.
The p valuesrange from 0 to 3 representing whether the wordindexed by j is positionally offset from the wordindexed by i by -2, -1, +1, or +2, respectively.For example, if "the" and "people" were re-spectively the 1st and 100th highest frequencywords, then upon seeing the phrase "the peo-ple," Schfitze's approach would increment thecounts of M(1,2N+100) and M(100,N+i).We used Schfitze's general framework but tai-lored it to identify local semantic information.We built an Nx2N matrix and our p values cor-respond to those words whose offsets from wordi are in the intervals \[-50,-1\] and \[1,501, respec-tively.
We also reserve the Nth position as acatch-all position to account for all words thatare not in the top (N-l).
An important issue toresolve is how large should N be.
We would like69to be able to incorporate semantics for an arbi-trarily large number of words and LSA quicklybecomes impractical on large sets.
Fortunately,it is possible to build a matrix with a smallervalue of N (say, 2500), perform an SVD thereon,and then fold in remaining terms (Manning andSchfitze, 1999, p. 563).
Since the U and V ma-trices of an SVD are orthogonal matrices, thenuuT:vvT : I .
This implies that AV=UD.This means that for a new word, w, one canbuild a vector ~T which identifies how w relatesto the top N words according to the p differentconditions described above.
For example, if wwere one of the top N words, then ~w T wouldsimply represent w's particular ow from the Amatrix.
The product f~w = ~wTVk is the projec-tion of ~T into the k-dimensional latent seman-tic space.
By storing an index to the words ofthe corpus as well as a sorted list of these words,one can efficiently build a set of semantic vec-tors which includes each word of interest.3.4 Stat i s t i ca l  Computat ionsMorphologically-related words frequently sharesimilar semantics, so we want to see how well se-mantic vectors of PPMVs correlate.
If we knowhow PPMVs correlate in comparison to otherword pairs from their same rulesets, we can ac-tually determine the semantic-based probabilitythat the variants are legitimate.
In this section,we identify a measure for correlating PPMVsand illustrate how ruleset-based statistics helpidentify legitimate PPMVs.3.4.1 Semant ic  Cor re la t ion  of  WordsThe cosine of the angle between two vectors v land v2 is given by,cos(v l ,v2) -  v l -v2II v l  llll v2 H"We want to determine the correlation betweeneach of the words of every PPMV.
We use whatwe call a normalized cosine score (NCS) as a cor-relation.
To obtain a NCS, we first calculate thecosine between each semantic vector, nw, andthe semantic vectors from 200 randomly chosenwords.
By this means we obtain w's correlationmean (#w) and standard deviation (aw).
If vis one of w's variants, then we define the NCSbetween ~w and nv  to becos(nw, nv)  - #y ).
min (ye{w,v} ayTable 1 provides normalized cosine scores forseveral PPMVs from Figure 2 and from amongwords listed originally as errors in other sys-tems.
(NCSs are effectively Z-scores.
)Table  1: Normalized Cosines for various PPMVsPPMVs I NCSs PPMVs NCSs Icar/cars 5.6 ally/allies 6.5car/caring -0.71 ally/all -1.3car/cares -0.14 dirty/dirt  2.4car/cared i -0.96 rat ing/rate 0.973.4.2 Ru leset - leve l  S ta t i s t i csBy considering NCSs for all word pairs cou-pled under a particular rule, we can deter-mine semantic-based probabilities that indicatewhich PPMVs are legitimate.
We expect ran-dom NCSs to be normally-distributed accord-ing to Af(0,1).
Given that a particular ulesetcontains nR PPMVs, we can therefore approx-imate the number (nT), mean (#T) and stan-dard deviation (aT) of true correlations.
If we_C .~___~ 2 .
define ~z(#,a)  to be fee  " - J ax, then wecan compute the probability that the particularcorrelation is legitimate:Pr( true) = nT ~ Z(~T ,aT)(nR--nT ~z(O, 1) +nT~Z(~T, aT)"3.4.3 Subru lesIt is possible that a rule can be hypothesizedat the trie stage that is true under only certainconditions.
A prime example of such a rule is("es", NULL).
Observe from Table 1 that theword "cares" poorly correlates with "car."
Yet,it is true that "-es" is a valid suffix for the words"flashes," "catches," "kisses," and many otherwords where the "-es" is preceded by a voicelesssibilant.Hence, there is merit to considering subrulesthat arise while performing analysis on a par-ticular rule.
For instance, while evaluating the("es", NULL) rule, it is desirable to also con-sider potential subrules such as ("ches", "ch")and ("tes", "t").
One might expect hat the av-erage NCS for the ("ches", "ch") subrule mightbe higher than the overall rule ("es", NULL)whereas the opposite will likely be true for("tes', "t").
Table 2 confirms this.70Table 2: Analysis of subrulesRule/Subrule I Average StDev t#instances("es", NULL) 1.62("ches", "ch" ) 2.20("shes", "sh") 2.39("res", "r") -0.69("tes","t") -0.582.43 1731.66 321.52 150.47 60.93 114 Resu l t sWe compare our algorithm to Goldsmith's Lin-guistica (2000) by using CELEX's (Baayen,et al, 1993) suffixes as a gold standard.CELEX is a hand-tagged, morphologically-analyzed atabase of English words.
CELEXhas limited coverage of the words from our dataset (where our data consists of over eight mil-lion words from random subcollections of TRECdata (Voorhees, et a1,1997/8)), so we only con-sidered words with frequencies of 10 or more.F igure 3: Morphological directed graphs(b) (f)concerned concerted(a) / (c) (g) kconcerns concerts , ~eoncer~ conceri~ (e) .\ conc(edr)ning con~eh!ting /(i) (j)concerto ~-- concertosMorphological relationships can be representedgraphically as directed graphs (see Figure 3,where three separate graphs are depicted).
De-veloping a scoring algorithm to compare di-rected graphs is likely to be prone to disagree-ments.
Therefore, we score only the vertex setsof directed graphs.
We will refer to these ver-tex sets as conflation sets.
For example, con-cern's conflation set contains itself as well as"concerned," "concerns," and "concerning" (or,in shorthand notation, the set is {a,b,c,d}).To evaluate an algorithm, we sum the num-ber of correct (C), inserted (Z), and deleted (D)words it predicts for each hypothesized confla-tion set.
If Xw represents word w's conflationset according to the algorithm, and if Yw repre-sents its CELEX-based conflation set, thenC = Ew( Ixw AYwl/lYwl),= Evw(lYw - (Xw NYw)I/IYwl), andz = Ew( lx~ - (xw AY~)I/IYwl).However, in making these computations, we dis-regard any CELEX words that are not in thealgorithm's data set and vice versa.For example, suppose two algorithms were be-ing compared on a data set where all the wordsfrom Figure 3 were available except "concert-ing" and "concertos."
Suppose further that onealgorithm proposed that {a,b,c,d,e,f,g,i} formeda single conflation set whereas the other algo-rithm proposed the three sets {a,b,c,d},{e,g,i},and {f}.
Then Table 3 illustrates how the twoalgorithms would be scored.Table 3: Example of scoringI II a I b I c I d I e I f I g I i IITotaltC1 4/4 4/4 4/4 4/4 3/3 3/3 3/3 1/1 8D1 0/4 0/4 0/4 0/4 0/3 0/3 0/3 0/1 0Zl 4/4 4/4 4/4 4/4 5/3 5/3 5/3 7/1 16C2 4/4 4/4 4/4 4/4 2/3 2/3 1/3 1/1 20/3D2 0/4 0/4 0/4 0/4 1/3 1/3 2/3 0/1 4/3Z2 0/4i0/4 0/4 0/4 1/3 1/3 0/3 2/1 8/3To explain Table 3, consider algorithm one'sentries for 'a.'
Algorithm one had pro-posed that Xa={a,b,c,d,e,f,g,i} when in reality,Ya={a,b,c,d}.
Since IXa NYal = 4 and IYal=4,then CA=4/4.
The remaining values of the tablecan be computed accordingly.Using the values from Table 3, we canalso compute precision, recall, and F-Score.Precision is defined to be C/(C+Z), recall isC/(C+D), and F-Score is the product of pre-cision and recall divided by the average of thetwo.
For the first algorithm, the precision, re-call, and F-Score would have respectively been1/3, 1, and 1/2.
In the second algorithm, thesenumbers would have been 5/7, 5/6, and 10/13.Table 4 uses the above scoring mechanism tocompare between Linguistica nd our system (atvarious probability thresholds).
Note that sinceLinguistica removes capitalization, it will havea different otal word count than our system.71Table 4: Performance on English CELEXAlgorithm LinguisticaLSA- LSA- LSA-based based basedpr_> 0.5 pr_> 0.7 pr> 0.85#Correct 10515 10529 10203 9863#Inserts 2157 1852 1138 783#Deletes 2571 2341 i 2667 3007Precision 83.0% 85.0% 90.0% 92.6%Recall 80.4% 81.8% 79.3% 76.6%F-Score 81.6% 83.4% 84.3% 83.9%5 Conc lus ionsThese results suggest hat semantics and LSAcan play a key part in knowledge-free mor-phology induction.
Semantics alone worked atleast as well as Goldsmith's frequency-based ap-proach.
Yet we believe that semantics-basedand frequency-based approaches play comple-mentary roles.
In current work, we are examin-ing how to combine these two approaches.Re ferencesAlbright, A. and B. P. Hayes.
1999.
An au-tomated learner for phonology and mor-phology.
Dept.
of Linguistics, UCLA.
Athttp: //www.humnet.ucla.edu/humnet/linguis-tics/people/hayes/learning/learner.pdf.Baayen, R. H., R. Piepenbrock, and H. van Rijn.1993.
The CELEX lexical database (CD-ROM),Linguistic Data Consortium, University of Penn-sylvania, Philadelphia, PA.Berry, M., T. Do, G. O'Brien, V. Krishna, andS.
Varadhan.
1993.
SVDPACKC user's guide.
CS-93-194, University of Tennessee.D~jean, H. 1998.
Morphemes as necessary con-cepts for structures: Discovery from untaggedcorpora.
University of Caen-Basse Normandie.http://www.info.unicaen.fr/~ DeJean/travail/ar -ticles/pgl 1.htm.Deerwester, S., S. T. Dumais, G. W. Furnas, T. K.Landauer, and R. Harshman.
1990.
Indexing byLatent Semantic Analysis.
Journal of the Ameri-can Society for Information Science.Gaussier, l~.
1999.
Unsupervised learning of deriva-tional morphology from inflectional lexicons.
A CL'99 Workshop Proceedings: Unsupervised Learn-ing in Natural Language Processing, University ofMaryland.Goldsmith, J.
1997.
Unsupervised learning of themorphology of a natural anguage.
University ofChicago.Goldsmith, J.
2000.
Unsupervised learning ofthe morphology of a natural language.
Uni-versity of Chicago.
http://humanities.uchi-cago.edu/faculty/goldsmith.Harris, Z.
1951.
Structural Linguistics.
University ofChicago Press.Hull, D. A. and G. Grefenstette.
1996.
A de-tailed analysis of English stemming algorithms.XEROX Technical Report, http://www.xrce.xe-rox.com/publis/mltt/mltt-023.ps.Krovetz, R. 1993.
Viewing morphology as an infer-ence process.
Proceedings of the 16thA CM/SIGIRConference, pp.
191-202.Jurafsky, D. S. and J. H. Martin.
2000.
Speech andLanguage Processing.
Prentice Hall, Inc., Engle-wood, N.J.Karlsson, F. and L. Karttunen,.
1997.
"Sub-sentencial Processing."
In Survey of the State ofthe Art in Human Language Technology, R.
Cole,Ed., Giardini Editori e Stampatori, Italy.Koskenniemi, K. 1983.
Two-level Morphology: aGeneral Computational Model for Word-FormRecognition and Production.
Ph.D. thesis, Univer-sity of Helsinki.Landauer,T.
K., P. W. Foltz, and D. Laham.
1998.Introduction to Latent Semantic Analysis.
Dis-course Processes.
Vol.
25, pp.
259-284.Lovins, J.
1968.
Development of a stemming al-gorithm.
Mechanical Translation and Computa-tional Linguistics, Vol.
11, pp.22-31Manning, C. D. and H. Schfitze.
1999.
Foundationsof Statistical Natural Language Processing, MITPress, Cambridge, MA.Porter, M. 1980.
An algorithm for suffix stripping.Program, Vol.
14(3), pp.130-137.Ritchie, G. and G. J. Russell.
1992.
Computationalmorphology: Practical Mechanisms for the En-glish Lexicon.
MIT.Schfitze, H. 1993.
Distributed syntactic representa-tions with an application to part-of-speech tag-ging.
Proceedings of the IEEE International Con-ference on Neural Networks, pp.
1504-1509.Scott, D. 1992.
Multivariate Density Estimation:Theory, Practice, and Visualization.
John Wiley& Sons, New York.Sproat, R. 1992.
Morphology and Computation.
MITPress, Cambridge, MA.Van den Bosch, A. and W. Daelemans.
1999.Memory-based morphological nalysis.
Proc.
ofthe 37th Annual Meeting of the ACL, Universityof Maryland, pp.
285-292.Voorhees, E., D. Hoffman, and C. Barnes.
1996-7.TREC Information Retrieval: Text Research Col-lection, Vols.
4-5 (CD-ROM), National Instituteof Standards and Technology.Woods, W. 2000.
Aggressive morphology for robustlexical coverage.
Proceedings of the 6th ANLP/lstNAACL, Seattle, WA.72
