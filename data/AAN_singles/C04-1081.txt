Chinese Segmentation and New Word Detectionusing Conditional Random FieldsFuchun Peng, Fangfang Feng, Andrew McCallumComputer Science Department, University of Massachusetts Amherst140 Governors Drive, Amherst, MA, U.S.A. 01003{fuchun, feng, mccallum}@cs.umass.eduAbstractChinese word segmentation is a difficult, im-portant and widely-studied sequence modelingproblem.
This paper demonstrates the abil-ity of linear-chain conditional random fields(CRFs) to perform robust and accurate Chi-nese word segmentation by providing a prin-cipled framework that easily supports the in-tegration of domain knowledge in the form ofmultiple lexicons of characters and words.
Wealso present a probabilistic new word detectionmethod, which further improves performance.Our system is evaluated on four datasets usedin a recent comprehensive Chinese word seg-mentation competition.
State-of-the-art perfor-mance is obtained.1 IntroductionUnlike English and other western languages, manyAsian languages such as Chinese, Japanese, andThai, do not delimit words by white-space.
Wordsegmentation is therefore a key precursor for lan-guage processing tasks in these languages.
For Chi-nese, there has been significant research on find-ing word boundaries in unsegmented sequences(see (Sproat and Shih, 2002) for a review).
Un-fortunately, building a Chinese word segmentationsystem is complicated by the fact that there is nostandard definition of word boundaries in Chinese.Approaches to Chinese segmentation fall roughlyinto two categories: heuristic dictionary-basedmethods and statistical machine learning methods.In dictionary-based methods, a predefined dictio-nary is used along with hand-generated rules forsegmenting input sequence (Wu, 1999).
Howeverthese approaches have been limited by the impos-sibility of creating a lexicon that includes all pos-sible Chinese words and by the lack of robust sta-tistical inference in the rules.
Machine learning ap-proaches are more desirable and have been success-ful in both unsupervised learning (Peng and Schuur-mans, 2001) and supervised learning (Teahan et al,2000).Many current approaches suffer from either lackof exact inference over sequences or difficulty in in-corporating domain knowledge effectively into seg-mentation.
Domain knowledge is either not used,used in a limited way, or used in a complicated wayspread across different components.
For example,the N-gram generative language modeling based ap-proach of Teahan et al(2000) does not use domainknowledge.
Gao et al(2003) uses class-based lan-guage for word segmentation where some word cat-egory information can be incorporated.
Zhang etal (2003) use a hierarchical hidden Markov Modelto incorporate lexical knowledge.
A recent advancein this area is Xue (2003), in which the author usesa sliding-window maximum entropy classifier to tagChinese characters into one of four position tags,and then covert these tags into a segmentation usingrules.
Maximum entropy models give tremendousflexibility to incorporate arbitrary features.
How-ever, a traditional maximum entropy tagger, as usedin Xue (2003), labels characters without consideringdependencies among the predicted segmentation la-bels that is inherent in the state transitions of finite-state sequence models.Linear-chain conditional random fields (CRFs)(Lafferty et al, 2001) are models that addressboth issues above.
Unlike heuristic methods, theyare principled probabilistic finite state models onwhich exact inference over sequences can be ef-ficiently performed.
Unlike generative N-gram orhidden Markov models, they have the ability tostraightforwardly combine rich domain knowledge,for example in this paper, in the form of multiplereadily-available lexicons.
Furthermore, they arediscriminatively-trained, and are often more accu-rate than generative models, even with the same fea-tures.
In their most general form, CRFs are arbitraryundirected graphical models trained to maximizethe conditional probability of the desired outputsgiven the corresponding inputs.
In the linear-chainspecial case we use here, they can be roughly un-derstood as discriminatively-trained hidden Markovmodels with next-state transition functions repre-sented by exponential models (as in maximum en-tropy classifiers), and with great flexibility to viewthe observation sequence in terms of arbitrary, over-lapping features, with long-range dependencies, andat multiple levels of granularity.
These beneficialproperties suggests that CRFs are a promising ap-proach for Chinese word segmentation.New word detection is one of the most impor-tant problems in Chinese information processing.Many machine learning approaches have been pro-posed (Chen and Bai, 1998; Wu and Jiang, 2000;Nie et al, 1995).
New word detection is normallyconsidered as a separate process from segmentation.However, integrating them would benefit both seg-mentation and new word detection.
CRFs provide aconvenient framework for doing this.
They can pro-duce not only a segmentation, but also confidencein local segmentation decisions, which can be usedto find new, unfamiliar character sequences sur-rounded by high-confidence segmentations.
Thus,our new word detection is not a stand-alone process,but an integral part of segmentation.
Newly detectedwords are re-incorporated into our word lexicon,and used to improve segmentation.
Improved seg-mentation can then be further used to improve newword detection.Comparing Chinese word segmentation accuracyacross systems can be difficult because many re-search papers use different data sets and differentground-rules.
Some published results claim 98% or99% segmentation precision and recall, but these ei-ther count only the words that occur in the lexicon,or use unrealistically simple data, lexicons that haveextremely small (or artificially non-existant) out-of-vocabulary rates, short sentences or many num-bers.
A recent Chinese word segmentation competi-tion (Sproat and Emerson, 2003) has made compar-isons easier.
The competition provided four datasetswith significantly different segmentation guidelines,and consistent train-test splits.
The performance ofparticipating system varies significantly across dif-ferent datasets.
Our system achieves top perfor-mance in two of the runs, and a state-of-the-art per-formance on average.
This indicates that CRFs are aviable model for robust Chinese word segmentation.2 Conditional Random FieldsConditional random fields (CRFs) are undirectedgraphical models trained to maximize a conditionalprobability (Lafferty et al, 2001).
A commonspecial-case graph structure is a linear chain, whichcorresponds to a finite state machine, and is suitablefor sequence labeling.
A linear-chain CRF with pa-rameters ?
= {?1, ...} defines a conditional proba-bility for a state (label) sequence y = y1...yT (forexample, labels indicating where words start or havetheir interior) given an input sequence x = x1...xT(for example, the characters of a Chinese sentence)to beP?
(y|x) = 1Zx exp( T?t=1?k?kfk(yt?1, yt,x, t)),(1)where Zx is the per-input normalization that makesthe probability of all state sequences sum to one;fk(yt?1, yt,x, t) is a feature function which is of-ten binary-valued, but can be real-valued, and ?k isa learned weight associated with feature fk.
Thefeature functions can measure any aspect of a statetransition, yt?1 ?
yt, and the entire observation se-quence, x, centered at the current time step, t. Forexample, one feature function might have value 1when yt?1 is the state START, yt is the state NOT-START, and xt is a word appearing in a lexicon ofpeople?s first names.
Large positive values for ?kindicate a preference for such an event; large nega-tive values make the event unlikely.The most probable label sequence for an input x,y?
= argmaxy P?
(y|x),can be efficiently determined using the Viterbi al-gorithm (Rabiner, 1990).
An N -best list of label-ing sequences can also be obtained using modi-fied Viterbi algorithm and A* search (Schwartz andChow, 1990).The parameters can be estimated by maximumlikelihood?maximizing the conditional probabilityof a set of label sequences, each given their cor-responding input sequences.
The log-likelihood oftraining set {(xi, yi) : i = 1, ...M} is writtenL?
=?ilogP?
(yi|xi)=?i( T?t=1?k?kfk(yt?1, yt,x, t)?
logZxi).Traditional maximum entropy learning algorithms,such as GIS and IIS (della Pietra et al, 1995), canbe used to train CRFs.
However, our implemen-tation uses a quasi-Newton gradient-climber BFGSfor optimization, which has been shown to convergemuch faster (Malouf, 2002; Sha and Pereira, 2003).The gradient of the likelihood is ?P?(y|x)/?
?k =?i,tfk(yt?1, y(i)t ,x(i), t)??i,y,tP?
(y|x(i))fk(yt?1, yt,x(i), t)CRFs share many of the advantageous propertiesof standard maximum entropy classifiers, includingtheir convex likelihood function, which guaranteesthat the learning procedure converges to the globalmaximum.2.1 Regularization in CRFsTo avoid over-fitting, log-likelihood is usually pe-nalized by some prior distribution over the parame-ters.
A commonly used prior is a zero-mean Gaus-sian.
With a Gaussian prior, log-likelihood is penal-ized as follows.L?
=?ilogP?(yi|xi)?
?k?2k2?2k(2)where ?2k is the variance for feature dimension k.The variance can be feature dependent.
Howeverfor simplicity, constant variance is often used forall features.
We experiment an alternate version ofGaussian prior in which the variance is feature de-pendent.
We bin features by frequency in the train-ing set, and let the features in the same bin sharethe same variance.
The discounted value is set to be?kdck/Me?
?2 where ck is the count of features, M isthe bin size set by held out validation, and dae is theceiling function.
See Peng and McCallum (2004)for more details and further experiments.2.2 State transition featuresVarying state-transition structures with differentMarkov order can be specified by different CRFfeature functions, as determined by the number ofoutput labels y examined together in a feature func-tion.
We define four different state transition featurefunctions corresponding to different Markov orders.Higher-order features capture more long-range de-pendencies, but also cause more data sparsenessproblems and require more memory for training.The best Markov order for a particular applicationcan be selected by held-out cross-validation.1.
First-order: Here the inputs are examined inthe context of the current state only.
Thefeature functions are represented as f(yt,x).There are no separate parameters for state tran-sitions.2.
First-order+transitions: Here we add parame-ters corresponding to state transitions.
The fea-ture functions used are f(yt,x), f(yt?1, yt).3.
Second-order: Here inputs are examined in thecontext of the current and previous states.
Fea-ture function are represented as f(yt?1, yt,x).4.
Third-order: Here inputs are examined inthe context of the current, and two previousstates.
Feature function are represented asf(yt?2, yt?1, yt,x).3 CRFs for Word SegmentationWe cast the segmentation problem as one of se-quence tagging: Chinese characters that begin a newword are given the START tag, and characters inthe middle and at the end of words are given theNONSTART tag.
The task of segmenting new, un-segmented test data becomes a matter of assigninga sequence of tags (labels) to the input sequence ofChinese characters.Conditional random fields are configured as alinear-chain (finite state machine) for this purpose,and tagging is performed using the Viterbi algo-rithm to efficiently find the most likely label se-quence for a given character sequence.3.1 Lexicon features as domain knowledgeOne advantage of CRFs (as well as traditional max-imum entropy models) is its flexibility in using ar-bitrary features of the input.
To explore this advan-tage, as well as the importance of domain knowl-edge, we use many open features from external re-sources.
To specifically evaluate the importance ofdomain knowledge beyond the training data, we di-vide our features into two categories: closed fea-tures and open features, (i.e., features allowed in thecompetition?s ?closed test?
and ?open test?
respec-tively).
The open features include a large word list(containing single and multiple-character words), acharacter list, and additional topic or part-of-speechcharacter lexicons obtained from various sources.The closed features are obtained from training dataalone, by intersecting the character list obtainedfrom training data with corresponding open lexi-cons.Many lexicons of Chinese words and charactersare available from the Internet and other sources.Besides the word list and character list, our lexiconsinclude 24 lists of Chinese words and characters ob-tained from several Internet sites1 cleaned and aug-mented by a local native Chinese speaker indepen-dently of the competition data.
The list of lexiconsused in our experiments is shown in Figure 1.3.2 Feature conjunctionsSince CRFs are log-linear models, feature conjunc-tions are required to form complex, non-linear de-cision boundaries in the original feature space.
We1http://www.mandarintools.com,ftp://xcin.linux.org.tw/pub/xcin/libtabe,http://www.geocities.com/hao510/wordlistnoun (e.g.,?,?)
verb (e.g.,?
)adjective (e.g.,?,?)
adverb (e.g.,!,?
)auxiliary (e.g.,,?)
preposition (e.g.,?
)number (e.g.,,) negative (e.g.,X,:)determiner (e.g.,?,?,Y) function (e.g.
?,?
)letter (English character) punctuation (e.g., # $)last name (e.g.,K) foreign name (e.g.,?
)maybe last-name (e.g.,?,[) plural character (e.g.,?,?
)pronoun (e.g.,fi,?,?)
unit character (e.g.,G,?
)country name (e.g.,?,?)
Chinese place name (e.g.,?
)organization name title suffix (e.g.,?,?
)title prefix (e.g.,,?)
date (e.g.,#,?,?
)Figure 1: Lexicons used in our experimentsC?2: second previous character in lexiconC?1: previous character in lexiconC1: next character in lexiconC2: second next character in lexiconC0C1: current and next character in lexiconC?1C0: current and previous character in lexiconC?2C?1: previous two characters in lexiconC?1C0C1: previous, current, and next character in the lexiconFigure 2: Feature conjunctions used in experimentsuse feature conjunctions in both the open and closedtests, as listed Figure 2.4 Probabilistic New Word IdentificationSince no vocabulary list could ever be complete,new word (unknown word) identification is an im-portant issue in Chinese segmentation.
Unknownwords cause segmentation errors in that these out-of-vocabulary words in input text are often in-correctly segmented into single-character or otheroverly-short words (Chen and Bai, 1998).
Tradi-tionally, new word detection has been considered asa standalone process.
We consider here new worddetection as an integral part of segmentation, aimingto improve both segmentation and new word detec-tion: detected new words are added to the word listlexicon in order to improve segmentation; improvedsegmentation can potentially further improve newword detection.
We measure the performance ofnew word detection by its improvements on seg-mentation.Given a word segmentation proposed by the CRF,we can compute a confidence in each segment.
Wedetect as new words those that are not in the existingword list, yet are either highly confident segments,or low confident segments that are surrounded byhigh confident words.
A confidence threshold of 0.9is determined by cross-validation.Segment confidence is estimated using con-strained forward-backward (Culotta and McCal-lum, 2004).
The standard forward-backward algo-rithm (Rabiner, 1990) calculates Zx, the total like-lihood of all label sequences y given a sequence x.Constrained forward-backward algorithm calculatesZ ?x, total likelihood of all paths passing througha constrained segment (in our case, a sequence ofcharacters starting with a START tag followed by afew NONSTART tags before the next START tag).The confidence in this segment is then Z?xZx , a realnumber between 0 and 1.In order to increase recall of new words, we con-sider not only the most likely (Viterbi) segmen-tation, but the segmentations in the top N mostlikely segmentations (an N -best list), and detectnew words according to the above criteria in all Nsegmentations.Many errors can be corrected by new word de-tection.
For example, person name ?????
hap-pens four times.
In the first pass of segmentation,two of them are segmented correctly and the othertwo are mistakenly segmented as ??
?
??
(theyare segmented differently because Viterbi algorithmdecodes based on context.).
However, ????
?is identified as a new word and added to the wordlist lexicon.
In the second pass of segmentation, theother two mistakes are corrected.5 Experiments and AnalysisTo make a comprehensive evaluation, we use allfour of the datasets from a recent Chinese word seg-mentation bake-off competition (Sproat and Emer-son, 2003).
These datasets represent four differentsegmentation standards.
A summary of the datasetsis shown in Table 1.
The standard bake-off scoringprogram is used to calculate precision, recall, F1,and OOV word recall.5.1 Experimental designSince CTB and PK are provided in the GB encod-ing while AS and HK use the Big5 encoding, weconvert AS and HK datasets to GB in order to makecross-training-and-testing possible.
Note that thisconversion could potentially worsen performanceslightly due to a few conversion errors.We use cross-validation to choose Markov-orderand perform feature selection.
Thus, each trainingset is randomly split?80% used for training and theremaining 20% for validation?and based on vali-dation set performance, choices are made for modelstructure, prior, and which word lexicons to include.The choices of prior and model structure shown inTable 2 are used for our final testing.We conduct closed and open tests on all fourdatasets.
The closed tests use only material from thetraining data for the particular corpus being tested.Open tests allows using other material, such as lex-icons from Internet.
In open tests, we use lexi-cons obtained from various resources as describedCorpus Abbrev.
Encoding #Train words #Test Words OOV rate (%)UPenn Chinese Treebank CTB GB 250K 40K 18.1Beijing University PK GB 1.1M 17K 6.9Hong Kong City U HK Big 5 240K 35K 7.1Academia Sinica AS Big 5 5.8M 12K 2.2Table 1: Datasets statisticsbin-Size M Markov orderCTB 10 first-order + transitionsPK 15 first-order + transitionsHK 1 first-orderAS 15 first-order + transitionsTable 2: Optimal prior and Markov order settingin Section 3.1.
In addition, we conduct cross-datasettests, in which we train on one dataset and test onother datasets.5.2 Overall resultsFinal results of CRF based segmentation with newword detection are summarized in Table 3.
The up-per part of the table contains the closed test results,and the lower part contains the open test results.Each entry is the performance of the given metric(precision, recall, F1, and Roov) on the test set.ClosedPrecision Recall F1 RoovCTB 0.828 0.870 0.849 0.550PK 0.935 0.947 0.941 0.660HK 0.917 0.940 0.928 0.531AS 0.950 0.962 0.956 0.292OpenPrecision Recall F1 RoovCTB 0.889 0.898 0.894 0.619PK 0.941 0.952 0.946 0.676HK 0.944 0.948 0.946 0.629AS 0.953 0.961 0.957 0.403Table 3: Overall results of CRF segmentation onclosed and open testsTo compare our results against other systems,we summarize the competition results reportedin (Sproat and Emerson, 2003) in Table 4.
XXc andXXo indicate the closed and open runs on datasetXX respectively.
Entries contain the F1 perfor-mance of each participating site on different runs,with the best performance in bold.
Our results arein the last row.
Column SITE-AVG is the averageF1 performance over the datasets on which a site re-ported results.
Column OUR-AVG is the average F1performance of our system over the same datasets.Comparing performance across systems is diffi-cult since none of those systems reported resultson all eight datasets (open and closed runs on 4datasets).
Nevertheless, several observations couldbe made from Table 4.
First, no single systemachieved best results in all tests.
Only one site (S01)achieved two best runs (CTBc and PKc) with an av-erage of 91.8% over 6 runs.
S01 is one of the bestsegmentation systems in mainland China (Zhang etal., 2003).
We also achieve two best runs (ASo andHKc), with a comparable average of 91.9% over thesame 6 runs, and a 92.7% average over all the 8 runs.Second, performance varies significantly across dif-ferent datasets, indicating that the four datasets havedifferent characteristics and use very different seg-mentation guidelines.
We also notice that the worstresults were obtained on CTB dataset for all sys-tems.
This is due to significant inconsistent segmen-tation in training and testing (Sproat and Emerson,2003).
We verify this by another test.
We randomlysplit the training data into 80% training and 20%testing, and run the experiments for 3 times, result-ing in a testing F1 of 97.13%.
Third, consider acomparison of our results with site S12, who usea sliding-window maximum entropy model (Xue,2003).
They participated in two datasets, with anaverage of 93.8%.
Our average over the same tworuns is 94.2%.
This gives some empirical evidenceof the advantages of linear-chain CRFs over sliding-window maximum entropy models, however, thiscomparison still requires further investigation sincethere are many factors that could affect the perfor-mance such as different features used in both sys-tems.To further study the robustness of our approachto segmentation, we perform cross-testing?that is,training on one dataset and testing on other datasets.Table 5 summarizes these results, in which the rowsare the training datasets and the columns are thetesting datasets.
Not surprisingly, cross testing re-sults are worse than the results using the sameASc ASo CTBc CTBo HKc HKo PKc PKo SITE-AVG OUR-AVGS01 93.8 88.1 88.1 90.1 95.1 95.3 91.8 91.9S02 87.4 91.2 89.3 87.2S03 87.2 82.9 88.6 92.5 87.8 93.6S04 93.9 93.7 93.8 94.4S05 94.2 73.2 89.4 85.6 91.5S06 94.5 82.9 92.4 92.4 90.6 91.9S07 94.0 94.0 94.6S08 90.4 95.6 93.6 93.8 93.4 94.0S09 96.1 94.6 95.4 94.9S10 83.1 90.1 94.7 95.9 91.0 90.8S11 90.4 88.4 87.9 88.6 88.8 93.6S12 95.9 91.6 93.8 94.295.6 95.7 84.9 89.4 92.8 94.6 94.1 94.6 92.7Table 4: Comparisons against other systems: the first column contains the 12 sites participating in bake-offcompetition; the second to the ninth columns contain their results on the 8 runs, where a bold entry is thewinner of that run; column SITE-AVG contains the average performance of the site over the runs in which itparticipated, where a bold entry indicates that this site performs better than our system; column OUR-AVGis the average of our system over the same runs, where a bolded entry indicates our system performs betterthan the other site; the last row is the performance of our system over all the runs and the overall average.source as training due to different segmentationpolicies, with an exception on CTB where mod-els trained on other datasets perform better than themodel trained on CTB itself.
This is due to the dataproblem mentioned above.
Overall, CRFs performrobustly well across all datasets.From both Table 3 and 5, we see, as expected,improvement from closed tests to open tests, indi-cating the significant contribution of domain knowl-edge lexicons.ClosedCTB PK HK ASCTB 0.822 0.810 0.815PK 0.816 0.824 0.830HK 0.790 0.807 0.825AS 0.890 0.844 0.864OpenCTB PK HK ASCTB 0.863 0.870 0.894PK 0.852 0.862 0.871HK 0.861 0.871 0.889AS 0.898 0.867 0.871Table 5: Crossing test of CRF segmentation5.3 Effects of new word detectionTable 6 shows the effect of new word detectionon the closed tests.
An interesting observation isCTB PK HK ASw/o NWD 0.792 0.934 0.916 0.956NWD 0.849 0.941 0.928 0.946Table 6: New word detection effects: w/o NWD isthe results without new word detection and NWD isthe results with new word detection.that the improvement is monotonically related to theOOV rate (OOV rates are listed in Table 1).
Thisis desirable because new word detection is mostneeded in situations that have high OOV rate.
Atlow OOV rate, noisy new word detection can resultin worse performance, as seen in the AS dataset.5.4 Error analysis and discussionSeveral typical errors are observed in error anal-ysis.
One typical error is caused by inconsistentsegmentation labeling in the test set.
This is mostnotorious in CTB dataset.
The second most typi-cal error is in new, out-of-vocabulary words, espe-cially proper names.
Although our new word detec-tion fixes many of these problems, it is not effectiveenough to recognize proper names well.
One solu-tion to this problem could use a named entity ex-tractor to recognize proper names; this was found tobe very helpful in Wu (2003).One of the most attractive advantages of CRFs(and maximum entropy models in general) is its theflexibility to easily incorporate arbitrary features,here in the form domain-knowledge-providing lex-icons.
However, obtaining these lexicons is not atrivial matter.
The quality of lexicons can affectthe performance of CRFs significantly.
In addition,compared to simple models like n-gram languagemodels (Teahan et al, 2000), another shortcomingof CRF-based segmenters is that it requires signifi-cantly longer training time.
However, training is aone-time process, and testing time is still linear inthe length of the input.6 ConclusionsThe contribution of this paper is three-fold.
First,we apply CRFs to Chinese word segmentation andfind that they achieve state-of-the art performance.Second, we propose a probabilistic new word de-tection method that is integrated in segmentation,and show it to improve segmentation performance.Third, as far as we are aware, this is the first workto comprehensively evaluate on the four benchmarkdatasets, making a solid baseline for future researchon Chinese word segmentation.AcknowledgmentsThis work was supported in part by the Center for In-telligent Information Retrieval, in part by The Cen-tral Intelligence Agency, the National Security Agencyand National Science Foundation under NSF grant #IIS-0326249, and in part by SPAWARSYSCEN-SD grantnumber N66001-02-1-8903.ReferencesK.J.
Chen and M.H.
Bai.
1998.
Unknown Word Detec-tion for Chinese by a Corpus-based Learning Method.Computational Linguistics and Chinese LanguageProcessing, 3(1):27?44, Feburary.A.
Culotta and A. McCallum.
2004.
Confidence Esti-mation for Information Extraction.
In Proceedings ofHuman Language Technology Conference and NorthAmerican Chapter of the Association for Computa-tional Linguistics(HLT-NAACL).S.
della Pietra, V. della Pietra, and J. Lafferty.
1995.
In-ducing Features Of Random Fields.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,19(4).J.
Gao, M. Li, and C. Huang.
2003.
Improved Source-Channel Models for Chinese Word Segmentation.
InProceedings of the 41th Annual Meeting of Associa-tion of Computaional Linguistics (ACL), Japan.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proceedingsof the 18th International Conf.
on Machine Learning,pages 282?289.R.
Malouf.
2002.
A Comparison of Algorithms for Max-imum Entropy Parameter Estimation.
In Sixth Work-shop on Computational Language Learning (CoNLL).J.
Nie, M. Hannan, and W. Jin.
1995.
Unknown WordDetection and Segmentation of Chinese using Statis-tical and Heuristic Knowledge.
Communications ofthe Chinese and Oriental Languages Information Pro-cessing Society, 5:47?57.F.
Peng and A. McCallum.
2004.
Accurate Informa-tion Extraction from Research Papers using Condi-tional Random Fields.
In Proceedings of HumanLanguage Technology Conference and North Amer-ican Chapter of the Association for ComputationalLinguistics(HLT-NAACL), pages 329?336.F.
Peng and D. Schuurmans.
2001.
Self-Supervised Chi-nese Word Segmentation.
In F. Hoffmann et al, ed-itor, Proceedings of the 4th International Symposiumof Intelligent Data Analysis, pages 238?247.
Springer-Verlag Berlin Heidelberg.L.
Rabiner.
1990.
A Tutorial on Hidden Markov Mod-els and Selected Applications in Speech Recognition.In Alex Weibel and Kay-Fu Lee, editors, Readings inSpeech Recognition, pages 267?296.R.
Schwartz and Y. Chow.
1990.
The N-best Algorithm:An Efficient and Exact Procedure for Finding the Nmost Likely Sentence Hypotheses.
In Proceedings ofIEEE International Conference on Acoustics, Speech,and Signal Processing (ICASSP).F.
Sha and F. Pereira.
2003.
Shallow Parsing with Con-ditional Random Fields.
In Proceedings of HumanLanguage Technology Conference and North Amer-ican Chapter of the Association for ComputationalLinguistics(HLT-NAACL).R.
Sproat and T. Emerson.
2003.
First International Chi-nese Word Segmentation Bakeoff.
In Proceedings ofthe Second SIGHAN Workshop on Chinese LanguageProcessing.R.
Sproat and C. Shih.
2002.
Corpus-based Methodsin Chinese Morphology and Phonology.
In Proceed-ings of the 19th International Conference on Compu-tational Linguistics (COLING).W.
J. Teahan, Y. Wen, R. McNab, and I. H. Wit-ten.
2000.
A Compression-based Algorithm for Chi-nese Word Segmentation.
Computational Linguistics,26(3):375?393.A.
Wu and Z. Jiang.
2000.
Statistically-Enhanced NewWord Identification in a Rule-Based Chinese System.In Proceedings of the Second Chinese Language Pro-cessing Workshop, pages 46?51, Hong Kong, China.Z.
Wu.
1999.
LDC Chinese Segmenter.http://www.ldc.upenn.edu/ Projects/ Chinese/ seg-menter/ mansegment.perl.A.
Wu.
2003.
Chinese Word Segmentation in MSR-NLP.
In Proceedings of the Second SIGHAN Work-shop on Chinese Language Processing, Japan.N.
Xue.
2003.
Chinese Word Segmentation as Charac-ter Tagging.
International Journal of ComputationalLinguistics and Chinese Language Processing, 8(1).H.
Zhang, Q. Liu, X. Cheng, H. Zhang, and H. Yu.2003.
Chinese Lexical Analysis Using HierarchicalHidden Markov Model.
In Proceedings of the SecondSIGHAN Workshop, pages 63?70, Japan.
