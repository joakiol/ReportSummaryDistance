Proceedings of NAACL-HLT 2013, pages 733?738,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsMore than meets the eye: Study of Human Cognition in Sense AnnotationSalil JoshiIBM Research IndiaBangalore, Indiasaljoshi@in.ibm.comDiptesh KanojiaGautam Buddha Technical UniversityLucknow, Indiadipteshkanojia@gmail.comPushpak BhattacharyyaComputer Science and Engineering DepartmentIndian Institute of Technology, BombayMumbai, Indiapb@cse.iitb.ac.inAbstractWord Sense Disambiguation (WSD) ap-proaches have reported good accuracies inrecent years.
However, these approaches canbe classified as weak AI systems.
Accordingto the classical definition, a strong AI basedWSD system should perform the task of sensedisambiguation in the same manner and withsimilar accuracy as human beings.
In orderto accomplish this, a detailed understandingof the human techniques employed for sensedisambiguation is necessary.
Instead ofbuilding yet another WSD system that usescontextual evidence for sense disambiguation,as has been done before, we have taken a stepback - we have endeavored to discover thecognitive faculties that lie at the very core ofthe human sense disambiguation technique.In this paper, we present a hypothesis regard-ing the cognitive sub-processes involved in thetask ofWSD.We support our hypothesis usingthe experiments conducted through the meansof an eye-tracking device.
We also strive tofind the levels of difficulties in annotating vari-ous classes of words, with senses.
We believe,once such an in-depth analysis is performed,numerous insights can be gained to develop arobust WSD system that conforms to the prin-ciple of strong AI.1 IntroductionWord Sense Disambiguation (WSD) is formallydefined as the task of computationally identifyingsenses of a word in a context.
The phrase ?in acontext?
is not defined explicitly in the literature.NLP researchers define it according to their conve-nience.
In our current work, we strive to unravelthe appropriate meaning of contextual evidenceused for the human annotation process.
Chatterjeeet al(2012) showed that the contextual evidenceis the predominant parameter for the human senseannotation process.
They also state that WSD issuccessful as a weak AI system, and further analysisinto human cognitive activities lying at the heart ofsense annotation can aid in development of a WSDsystem built upon the principles of strong AI.Knowledge based approaches, which can be con-sidered to be closest form of WSD conforming tothe principles of strong AI, typically achieve lowaccuracy.
Recent developments in domain-specificknowledge based approaches have reported higheraccuracies.
A domain-specific approach due toAgirre et al(2009) beats supervised WSD donein generic domains.
Ponzetto and Navigli (2010)present a knowledge based approach which rivalsthe supervised approaches by using the semanticrelations automatically extracted from Wikipedia.They reported approximately 7% gain over thecloset supervised approach.In this paper, we delve deep into the cognitive rolesassociated with sense disambiguation through themeans of an eye-tracking device capturing the gazepatterns of lexicographers, during the annotationprocess.
In-depth discussions with trained lexicog-raphers indicate that there are multiple cognitivesub-processes driving the sense disambiguationtask.
The eye movement paths available from thescreen recordings done during sense annotationconform to this theory.Khapra et al(2011) points out that the accuracyof various WSD algorithms is poor on certain733Part-of-speech (POS) categories, particularly, verbs.It is also a general observation for lexicographersinvolved in sense annotation that there are differentlevels of difficulties associated with various classesof words.
This fact is also reflected in our analysison sense annotation.
The data available after theeye-tracking experiments gave us the fixation timesand saccades pertaining to different classes ofwords.
From the analysis of this data we drawconclusive remarks regarding the reasons behindthis phenomenon.
In our case, we classified wordsbased on their POS categories.In this paper, we establish that contextual evidence isthe prime parameter for the human annotation.
Fur-ther, we probe into the implication of context usedas a clue for sense disambiguation, and the mannerof its usage.
In this work, we address the followingquestions:?
What are the cognitive sub-processes associ-ated with the human sense annotation task??
Which classes of words are more difficult to dis-ambiguate and why?By providing relevant answers to these questions weintend to present a comprehensive understanding ofsense annotation as a complex cognitive process andthe factors involved in it.
The remainder of this pa-per is organized as follows.
Section 2 contains re-lated work.
In section 3 we present the experimentalsetup.
Section 4 displays the results.
We summarizeour findings in section 5.
Finally, we conclude thepaper in section 6 presenting the future work.2 Related WorkAs mentioned earlier, we used the eye-trackingdevice to ascertain the fact that contextual evidenceis the prime parameter for human sense annotationas quoted by Chatterjee et al(2012) who used dif-ferent annotation scenarios to compare human andmachine annotation processes.
An eye movementexperiment was conducted by Vainio et al(2009)to examine effects of local lexical predictabilityon fixation durations and fixation locations duringsentence reading.
Their study indicates that locallexical predictability influences in decisions but notwhere the initial fixation lands in a word.
In anotherwork based on word grouping hypothesis and eyemovements during reading by Drieghe et al(2008),the distribution of landing positions and durations offirst fixations in a region containing a noun precededby either an article or a high-frequency three-letterword were compared.Recently, some work is done on the study of senseannotation.
A study of sense annotations done on 10polysemous words was conducted by Passonneauet al(2010).
They opined that the word meanings,contexts of use, and individual differences amongannotators gives rise to inter-annotation variations.De Melo et al(2012) present a study with afocus on MASC (Manually-Annotated SubCorpus)project, involving annotations done using WordNetsense identifiers as well as FrameNet lexical units.In our current work we use eye-tracking as a toolto make findings regarding the cognitive processesconnected to the human sense disambiguationprocedure, and to gain a better understandingof ?contextual evidence?
which is of paramountimportance for human annotation.
Unfortunately,our work seems to be a first of its kind, and to thebest of our knowledge we do not know of any suchwork done before in the literature.3 Experimental SetupWe used a generic domain (viz., News) corpus inHindi language for experimental purposes.
To iden-tify the levels of difficulties associated with humanannotation, across various POS categories, we con-ducted experiments on around 2000 words (includ-ing function words and stop words).
The analysiswas done only for open class words.
The statisticspertaining to the our experiment are illustrated in ta-ble 1.
For statistical significance of our experiments,we collected the data with the help of 3 skilled lexi-cographers and 3 unskilled lexicographers.POS Noun Verb Adjective Adverb#(senses) 2.423 3.814 2.602 3.723#(tokens) 452 206 96 177Table 1: Number of words (tokens) and average degreeof corpus polysemy (senses) of words per POS category(taken from Hindi News domain) used for experimentsFor our experiments we used a Sense Annotation734Figure 1: Sense marker tool showing an example Hindi sentence in the Context Window and the wordnet synsets ofthe highlighted word in the Synset Window with the black dots and lines indicating the scan pathTool, designed at IIT Bombay and an eye-trackingdevice.
The details of the tools and their purposesare explained below:3.1 The Sense Marker ToolA word may have a number of senses, and the taskof identifying and marking which particular sensehas been used in the given context, is known assense marking.The Sense Marker tool1 is a Graphical User Inter-face based tool developed using Java, which facil-itates the task of manual sense marking.
This tooldisplays the senses of the word as available in theMarathi, Hindi and Princeton (English) WordNetsand allows the user to select the correct sense of theword from the candidate senses.3.2 Eye-Tracking deviceAn eye tracker is a device for measuring eye posi-tions and eye movement.
A saccade denotes move-1http://www.cse.iitb.ac.in/s?alilj/resources/SenseMarker/SenseMarkerTool.zipment to another position.
The resulting series of fix-ations and saccades is called a scan path.
Figure 1shows a sample scan path.
In our experiments, wehave used an eye tracking device manufactured bySensoMotoric Instruments2.
We recorded saccades,fixations, length of each fixation and scan paths onthe stimulus monitor during the annotation process.A remote eye-tracking device (RED) measures gazehotspots on a stimulus monitor.4 ResultsIn our experiments, each lexicographer performedsense annotation on the stimulus monitor of theeye tracking device.
Fixation times, saccadesand scan paths were recorded during the senseannotation process.
We analyzed this data and thecorresponding observations are enumerated below.Figure 2 shows the annotation time taken by differ-ent lexicographers across POS categories.
It can beobserved that the time taken for disambiguating theverbs is significantly higher than the remaining POS2http://www.smivision.com/735Unskilled Lexicographer (Seconds) Skilled Lexicographer (Seconds)Word Degree ofpolysemyThypo Tclue Tgloss Ttotal Thypo Tclue Tgloss TtotallAnA (laanaa - to bring) 4 0.63 0.80 5.20 6.63 0.31 1.20 1.82 3.30krnA (karanaa - to do) 22 0.90 1.42 2.20 4.53 0.50 0.64 1.14 2.24jtAnA (jataanaa - to express) 4 0.70 2.45 5.93 9.09 0.25 0.39 0.62 1.19Table 2: Comparison of time taken across different cognitive stages of sense annotation by lexicographers for verbsFigure 2: Histogram showing time taken (in seconds) byeach lexicographer across POS categories for sense anno-tationcategories.
This behavior can be consistently seenin the timings recorded for all the six lexicographers.Table 2 presents the comparison of time takenacross different cognitive stages of sense annotationby lexicographers for some of the most frequentlyoccurring verbs.To know if the results gathered from all the lexicog-raphers are consistent, we present the correlation be-tween each pair of lexicographers in table 3.
Thetable also shows the value of the t-test statistic gen-erated for each pair of lexicographers.5 DiscussionThe data obtained from the eye-tracking device andcorresponding analysis of the fixation times, sac-cades and scan paths of the lexicographers?
eyes re-veal that sense annotation is a complex cognitiveprocess.
From the videos of the scan paths obtainedfrom the eye-tracking device and from detailed dis-cussion with lexicographers it can be inferred thatthis cognitive process can be broken down into 3stages:1.
When a lexicographer sees a word, he/shemakes a hypothesis about the domain and con-sequently about the correct sense of the word,mentally.
In cases of highly polysemous words,the hypothesis may narrow down to multiplesenses.
We denote the time required for thisphase as Thypo.2.
Next the lexicographer searches for clues tosupport this hypothesis and in some cases toeliminate false hypotheses, when the word ispolysemous.
These clues are available in theform of neighboring words around the targetword.
We denote the time required for this ac-tivity as Tclue.3.
The clue words aid the lexicographer to decidewhich one of the initial hypotheses was true.To narrow down the candidate synsets, the lex-icographers use synonyms of the words in asynset to check if the sentence retains its mean-ing.From the scan paths and fixation times obtainedfrom the eye-tracking experiment, it is evident thatstages 1, 2 and 3 are chronological stages in the hu-man cognitive process associated with sense disam-biguation.
In cases of highly polysemous words andinstances where senses are fine-grained, stages 2 and3 get interleaved.
It is also clear that each stage takesup separate proportions of the sense disambiguationtime for humans.
Hence time taken to disambiguatea word using the Sense Marker Tool (as explained inSection 3.1) can be factored as follows:Ttotal = Thypo + Tclue + TglossWhere:Ttotal = Total time for sense disambiguation736Correlation value T-test statisticLexicographer B C D E F B C D E FA 0.933 0.976 0.996 0.996 0.769 0.007 0.123 0.185 0.036 0.006B 0.987 0.960 0.915 0.945 0.009 0.028 0.084 0.026C 0.989 0.968 0.879 0.483 0.088 0.067D 0.988 0.820 0.367 0.709E 0.734 0.418Table 3: Pairwise correlation between annotation time taken by lexicographersThypo = Time for hypothesis buildingTclue = Clue word searching timeTgloss = Gloss Matching time and winner senseselection time.The results in table 2 reveal the different ratios oftime invested during each of the above stages.
Thypotakes the minimum amount of time among the dif-ferent sub-processes.
Tgloss > Tclue in all cases.?
For unskilled lexicographers: Tgloss >> Tcluebecause of errors in the initial hypothesis.?
For skilled lexicographers: Tgloss ?
Tclue, asthey can identify the POS category of the wordand their hypothesis thus formed is pruned.Hence during selection of the winner sense,they do not browse through other POS cate-gories, which unskilled lexicographers do.The results shown in figure 2 reveal that verbs takethe maximum disambiguation time.
In fact theaverage time taken by verbs is around 75% morethan the time taken by other POS categories.
Thissupports the fact that verbs are the most difficult todisambiguate.The analysis of the scan paths and fixation timesavailable from the eye-tracking experiments in caseof verbs show that the Tgloss covers around 66%of Ttotal, as shown in table 2.
This means that thelexicographer takes more time in selecting a winnersense from the list of wordnet senses.
This happenschiefly because of following reasons:1.
Higher degree of polysemy of verbs comparedto other POS categories (as shown in tables 1and 2).2.
In several cases the senses are fine-grained.3.
Sometimes the hypothesis of the lexicogra-phers may not match any of the wordnet senses.The lexicographer then selects the wordnetsense closest to their hypothesis.Adverbs and adjectives show higher degree of pol-ysemy than nouns (as shown in table 1), but takesimilar disambiguation time as nouns (as shown infigure 2).
In case of adverbs and adjectives, the lex-icographer is helped by their position around a verbor noun respectively.
So, Tclue only involves search-ing for the nearby verbs or nouns, as the case maybe, hence reducing total disambiguation time Ttotal.6 Conclusion and Future WorkIn this paper we examined the cognitive process thatenables the human sense disambiguation task.
Wehave also laid down our findings regarding the vary-ing levels of difficulty in sense annotation acrossdifferent POS categories.
These experiments arejust a stepping stone for going deeper into findingthe meaning and manner of usage of contextualevidence which is fundamental to the human senseannotation process.In the future we aim to perform an in-depth analy-sis of clue words that aid humans in sense disam-biguation.
The distance of clue words from the tar-get word and their and pattern of occurrence couldgive us significant insights into building a ?Discrim-ination Net?.ReferencesE.
Agirre, O.L.
De Lacalle, A. Soroa, and I. Fakultatea.2009.
Knowledge-based wsd on specific domains:performing better than generic supervised wsd.
Pro-ceedigns of IJCAI, pages 1501?1506.Arindam Chatterjee, Salil Joshi, Pushpak Bhattacharyya,Diptesh Kanojia, and Akhlesh Meena.
2012.
A737study of the sense annotation process: Man v/s ma-chine.
In Proceedings of 6th International Conferenceon Global Wordnets, January.G.
De Melo, C.F.
Baker, N. Ide, R.J. Passonneau, andC.
Fellbaum.
2012.
Empirical comparisons of mascword sense annotations.
In Proceedings of the 8thinternational conference on language resources andevaluation (LREC12).
Istanbul: European LanguageResources Association (ELRA).D.
Drieghe, A. Pollatsek, A. Staub, and K. Rayner.
2008.The word grouping hypothesis and eye movementsduring reading.
Journal of Experimental Psychology:Learning, Memory, and Cognition, 34(6):1552.Mitesh M. Khapra, Salil Joshi, and Pushpak Bhat-tacharyya.
2011.
It takes two to tango: A bilingualunsupervised approach for estimating sense distribu-tions using expectation maximization.
In Proceedingsof 5th International Joint Conference on Natural Lan-guage Processing, pages 695?704, Chiang Mai, Thai-land, November.
Asian Federation of Natural Lan-guage Processing.R.J.
Passonneau, A. Salleb-Aouissi, V. Bhardwaj, andN.
Ide.
2010.
Word sense annotation of polysemouswords by multiple annotators.
Proceedings of LREC-7, Valleta, Malta.S.P.
Ponzetto and R. Navigli.
2010.
Knowledge-richword sense disambiguation rivaling supervised sys-tems.
In Proceedings of the 48th annual meeting of theassociation for computational linguistics, pages 1522?1531.
Association for Computational Linguistics.S.
Vainio, J.
Hyo?na?, and A. Pajunen.
2009.
Lexical pre-dictability exerts robust effects on fixation duration,but not on initial landing position during reading.
Ex-perimental psychology, 56(1):66.738
