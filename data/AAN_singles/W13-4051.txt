Proceedings of the SIGDIAL 2013 Conference, pages 334?338,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsOpen-domain Utterance Generation for Conversational Dialogue Systemsusing Web-scale Dependency StructuresHiroaki Sugiyama?, Toyomi Meguro?, Ryuichiro Higashinaka?
?, Yasuhiro Minami?
?NTT Communication Science Laboratories2-4, Hikari-dai, Seika-cho, Souraku-gun, Kyoto, Japan?
?NTT Media Intelligence Laboratories1-1, Hikari-no-Oka, Yokosuka-shi, Kanagawa, Japan{sugiyama.hiroaki,meguro.toyomi,higashinaka.ryuichiro,minami.yasuhiro}@lab.ntt.co.jpAbstractEven though open-domain conversationaldialogue systems are required in manyfields, their development is complicatedbecause of the flexibility and variety ofuser utterances.
To address this flexibil-ity, previous research on conversational di-alogue systems has selected system utter-ances from web articles based on surfacecohesion and shallow semantic coherence;however, the generated utterances some-times contain irrelevant sentences with re-spect to the input user utterance.
We pro-pose a template-based approach that fillstemplates with the most salient words ina user utterance and with related wordsthat are extracted using web-scale depen-dency structures gathered from Twitter.Our open-domain conversational dialoguesystem outperforms retrieval-based con-ventional systems in chat experiments.1 IntroductionThe need for open-domain conversational dia-logue systems continues to grow.
Such systemsare beginning to be actively investigated from theirsocial and entertainment aspects (Shibata et al2009; Ritter et al 2011; Wong et al 2012);conversational dialogues also have potential fortherapy purposes and for evoking a user?s uncon-scious requests in task-oriented dialogues (Bick-more and Cassell, 2001).
However, developingopen-domain conversational dialogue systems isdifficult, since the huge variety of user utterancesmakes it harder to build knowledge resources forgenerating appropriate system responses.
To ad-dress this issue, previous research has selected sys-tem utterances from web articles or microblogs onthe basis of surface cohesion and shallow seman-tic coherence (Shibata et al 2009; Jafarpour andBurges, 2010; Wong et al 2012); however, the se-lected utterances sometimes contain sentences ir-relevant to the user utterance since they originallyappeared in a different context.To satisfy both web-scale topic coverage andsuppression of irrelevant sentences, we propose atemplate-based approach that fills templates withwords related to the topic of the user utterance andwith words related to the topic-words.
This ap-proach enables us to generate a wide range of sys-tem responses when we properly extract relatedwords.
To obtain words related to topic-words,we analyzed the dependency structures of a hugenumber of sentences posted to such microblogs asTwitter, where a large number and variety of sen-tences are posted daily.
This way, we can generatea variety of appropriate system responses despitewide variation in user utterances.We develop a conversational dialogue systemthat generates system utterances with our pro-posed utterance generation approach and exam-ine its effectiveness by chat experiments with realusers.2 Related WorkTo generate system utterances for conversationaldialogue systems, Ritter et al(2011) proposed astatistical machine translation-based approach thatconsiders source-reply tweet pairs as a bilingualcorpus.
They compared the following three ap-proaches: IR-status, which retrieves reply tweetswhose associated source tweets most resemblethe user utterance (Jafarpour and Burges, 2010);IR-response, which retrieves reply tweets thatare the most similar to the user utterance; andtheir proposed SMT-based approach, named MT-chat.
They reported that MT-chat outperformedthe other approaches and that IR-response was su-perior to IR-status.
However, these approachesused only the words, and not the structures, of userutterances to generate system utterances.Yoshino et al(2011) proposed a QA systemthat answers questions about current events by re-trieving, from news articles, descriptions contain-ing similar dependency structures as those of theuser?s questions.
Although this retrieval-based ap-proach is effective for answering the user?s fac-tual questions, it is insufficient to generate sub-jective utterances for conversational dialogue sys-tems since such systems are required to introduce334new topics or to respond with opinions related touser utterances.3 Open-domain Utterance GenerationOpen-domain conversational dialogue systemsshould be able to respond to any user utterance onany topic.
To achieve this, we adopt a template-based approach that estimates the topic of theuser utterance, extracts words related to the topic-words, and fills templates with these words.
Thetemplate-based approach resembles previous rule-based approaches, but these dialogue systems haddifficulty achieving coverage for template fillers.In contrast, our approach utilizes the dependencystructures of sentences gathered from microblogsthat have a wide range of topics, in order to ex-tract the related words used in template-filling.The dependency parser we use is a state-of-the-artJapanese dependency parser that uses ConditionalRandom Fields trained on text and blog posts, andperforms cascaded chunking until all dependen-cies are found.
This parser achieved 84.59% de-pendency accuracy on a corpus of Japanese blogposts (Imamura et al 2007).Microblog posts do not typically contain formu-laic utterances such as greetings or back-channels.Therefore, in addition to the template-filling ap-proach, we adopt dialogue act based utterancegeneration for the formulaic uttenances.
Figure 1illustrates the whole architecture of our system.3.1 Topic-word-driven Template-basedUtterance GenerationOur topic-word-driven template-based approachconsists of the following three steps: topic estima-tion, related word extraction, and template-fillingutterance generation.3.1.1 Topic EstimationWe identify three types of potential topic in an in-put user utterance: proper nouns, common nouns,and predicates (verbs, adjectives, adjectival verbs,and verbal nouns).Proper Nouns We take the last proper nounthat appears in the user?s utterance as a poten-tial topic.
Since general Japanese morphologi-cal analyzers cannot capture recent proper nouns,we complement the proper noun dictionary entrieswith Wikipedia entries1.Common Nouns To identify potential topicsfrom common nouns, we calculate the inverse doc-ument frequency (IDF) of each common noun (allnouns except for proper, time-related, and verbalones) in the user?s utterance.
We use a corpus ofmicroblog posts and treat each post as a document.We adopt the word with the highest IDF as a po-tential topic.1https://github.com/nabokov/mecab-dic-overdriveRelated word ExtraconTopicEsmaonTemplate-filling based U"erance GeneraonUser U"eranceSystem U"eranceTopic-word-driven  Template-based U"eranceTopic-wordTopic-related wordsDialogueControlDialogue act EsmaonU"erance Generaon with Predicted Dialogue actUser?s Dialogue actSystem?s Dialogue actDialogue act based U"erance (only for greengs and  back-channel feedback)Topic-word-driven  Template-based  ApproachDialogue actbased Approach (Secondarily)Figure 1: System ArchitecturePredicates We take the predicate that composesa dependency in the highest layer of the depen-dency structure as a potential topic.
For example,we adopt ?ask?, but not ?walk?
from the utterance?I asked the man walking on the street?.3.1.2 Related word ExtractionTo obtain topic-related words, a thesaurus or topicmodel such as Latent Dirichlet Allocation are themost popular approaches (Blei et al 2003).
How-ever, these approaches return semantically simi-lar words to input query words, which do not ef-fectively introduce new information into the sys-tem utterances.
Therefore, we count the depen-dencies between words in a huge number of sen-tences gathered from microblogs, and utilize themost frequently dependent words.
This approachenables us to extract adjectives related to propernoun topics; for example, the adjectives beautiful,good, clear, white, and huge are extracted for Mt.Fuji.
Since microblogs contain a huge number ofsubjective posts, we expect the extracted words tobe subjective and suitable for conversational dia-logue systems.
In this work, we extract adjectivesfor proper and common nouns, and nouns and theircase frames for predicates.
Examples of extractedwords are shown in Table 2.3.1.3 Template-filling Utterance GenerationWe generate two types of system utterances usingmanually defined templates: subjective sentenceswith proper nouns and common nouns; and ques-tions with predicates and their case frames.Noun-driven Subjective Sentence GenerationWe generate system utterances using the properand common nouns and their related adjectives.Here, we adopt different templates for each wordtype; proper nouns have explicit meanings, so ad-jectives related to them are easily suited for any di-alogue context.
By contrast, since common nounsare used in various contexts in microblogs, ad-jectives related to common nouns may not fit thedialogue context.
Thus, we use ?suki?
(?like?in English), or ?nigate?
(?don?t like?
in English)in the templates based on the proportion of posi-tive/negative adjectives in the set of related wordsfor a common noun topic.
Table 3 shows represen-tative examples for each type.
If the system gener-335ates subjective utterances as the system?s own im-pression of the dialogue topic, the user will expectthe system to justify or explain its opinion; how-ever, our system cannot answer that kind of ques-tion.
Thus, we define the templates using hedgessuch as ?I hear that...?
to avoid such questions.The number of templates for proper nouns is eight,and for common nouns is four for each polarity.Predicate-driven Question Sentence Genera-tion We generate question sentences using pred-icates and their related nouns and case frames.
Toelicit user utterances on a particular topic, we gen-erate How/What/Where/When types of questionsas shown in Table 3.
To select a question word,we use the predicate types and the classes of therelated nouns.
If the predicate type is adjective oradjectival noun, we select ?how?
for the questionword.
If the predicate type is verbal noun or verband location class words appear in the related nounphrase, we select ?where?
for the question word;the time class induces the question word ?when?.When no proper noun is found in the topic-word,we select ?what?.
The number of templates forproper nouns is three for each interrogative type.3.2 Dialogue act based Utterance GenerationOur approach has difficulty generating appropriateresponses to formulaic utterances such as greet-ings and back-channels.
To address this weakness,we adopt dialogue act based utterance generationfor these types of utterance.
A dialogue act is anabstract expression of a speaker?s intention (Stol-cke et al 2000); we used the 33 dialogue acts de-fined in Meguro et al(2010).Our dialogue act based approach estimates thenext dialogue act that the system should outputbased on the user?s utterance, and generates a sys-tem utterance based on the system?s predicted di-alogue act if the dialogue act is greetings, sympa-thy, non-sympathy, filler, or confirmation.3.2.1 User?s Dialogue act EstimationWe collected 1,259 conversational dialogues from47 human subjects and labeled each sentence ofthe collected data using the 33 dialogue acts.67,801 dialogue acts are contained in the corpus.We estimated the 33 dialogue acts from userutterances using a logistic regression model andadopted 1- and 2-gram words and 3- and 4-gramcharacters as model features.
We trained ourmodel using 1,000 dialogues and evaluated it us-ing 259 dialogues.
The estimation accuracy wasabout 61%, whereas the human annotation agree-ment rate was about 59%.3.2.2 Dialogue control Model and UtteranceGeneration with Predicted Dialogue actWe developed a dialogue control model that esti-mates the system?s next dialogue act based on theuser?s dialogue act.
The model features are theuser?s current dialogue act vector, the system?s lastdialogue act vector, and the user?s last dialogue actvector.
Each dialogue act vector consists of a 33-dimensional binary vector space.
We used the dia-logue corpus described above to train and evaluateour model, which we trained with 1,000 dialoguesand evaluated using 259 dialogues.
The estimationaccuracy was 31%, whereas the dialogue act an-notation agreement rate between humans is 60%.We exploited the fact that formulaic utterances canpre-define corresponding utterances regardless ofthe context.
Table 4 shows example generated sen-tences for each dialogue act.4 Experiment4.1 Experiment SettingWe recruited ten native Japanese-speaking partic-ipants in their 20?s and 30?s (two males and eightfemales) from outside of the authors?
organiza-tion, who have experience using chat systems (notbots).
Each participant chatted with the followingsystems, provided subjective evaluation scores foreach system for each of the eight criteria shown inTable 1 (2)-(10) using 7-point Likert scales, and atthe end ranked all the systems.
We examined theeffectiveness of our proposed approach by com-parison with the following six systems.We built the following proposed systems withabout 150 M posts gathered from Twitter (ex-cluding posts that contain ?
@?, ?RT?, ?http?
andbrackets, and posts that don?t contain any depen-dency pairs).
At the beginning of a dialogue orthe end of a conversation topic when the topic-based approach didn?t generate system utterances,the proposed approaches generated questions suchas ?What is your favorite movie??
to introducethe next conversation topic.
These questions weregathered from utterances in the self-introductionphase (about the five initial utterances) of each di-alogue in our dialogue corpus.
We manually se-lected 109 questions that have no context from179 questions gathered from our corpus, and chosea question at random to generate each topic-inductive question.Proposed-All This approach used all found top-ics: proper and common nouns, and predicates.This approach is expected to be well-balancedsince it generates both content-focused utterancesand general WH-type questions.Proposed-Nouns This approach used onlyproper and common nouns, not predicates.Proposed-Predicates This approach used onlypredicates, not proper nor common nouns.Retrieval-Self This approach resembles the IR-response method in Ritter et al(2011).
This ap-proach chose the most similar posts to the user ut-336Prop.-All Prop.-Noun Prop.-Pred.
Ret.-self Ret.-reply Human(1) Number of superior prefs.
vs. Prop.-All - 4 3 0??
2?
9??
(2) Naturalness of dialogue flow 4.0 3.1??
3.5 2.2??
3.5 6.5??
(3) Grammatical correctness 4.0 3.7 4.4 4.1 3.9 6.4??
(4) Dialogue usefulness 3.7 2.9??
3.9 2.7??
3.5 6.1??
(5) Ease of considering next utterance 3.5 3.4 4.4??
2.4??
3.3 5.7??
(6) Variety of system utterances 4.3 4.0 4.2 2.9??
4.0 5.5(7) User motivation 4.5 4.0?
4.7 3.7?
4.6 5.6??
(8) System motivation that the user feels 4.7 4.1?
4.3 3.5??
4.5 5.7?
(9) Desire to chat again 3.7 2.8??
3.3 2.0??
3.1 5.7??
(10) Averaged score of all evaluation items 4.05 3.50??
4.08 2.93??
3.8?
5.9?
?Table 1: System preferences and evaluation scores on 7-point Likert scale (?
: p<.1, ??
: p<.05)terance from source posts using the Lucene2 infor-mation retrieval library, which is an IDF-weightedvector-space similarity.
We built about 55 Msource-reply post pairs from Twitter.Retrieval-Reply This approach is the same asthe IR-status method in Ritter et al(2011).
Itchooses a reply post whose associated source postsmost resemble the user?s utterance.Human As an upper-bound of these systems,the user chats with a human using the same chatinterface used by the other systems.Each dialogue took place over four minutes andwas conducted through a text chat interface, andthe orders of presentation of systems to partici-pants was randomized.
Since the humans haveto type their utterances and the systems can gen-erate utterances much faster than typing, we setthe transition of the system utterances to about tenseconds to avoid different response intervals be-tween the systems and the humans.
Table 5 showsa dialogue example.4.2 Results and DiscussionTable 1 shows that Proposed-All is ranked thehighest of all the automatic systems (1), andachieves the best average evaluation scores (2)-(10).
Statistical analyses were performed usingthe Binomial test for (1) and Welch?s t test for (2)to (10).
Proposed-All was ranked higher than theretrieval-based approaches (10 of 10 participantsranked Proposed-All higher than Retrieval-Self,and 8 participants ranked Proposed-All higherthan Retrieval-Reply), but none of our three pro-posed approaches was ranked significantly higherthan the others.The evaluation scores also demonstrate thecharacteristics of each approach.
Proposed-Nounsshows significantly low scores in dialogue flow(2), dialogue usefulness (4), and system motiva-tion (9).
Since this approach is overly affected bythe nouns in the user utterances, users didn?t feelthat the system was actually thinking.
Proposed-Predicates shows a high score in ease of thinkingabout the next utterance (5) since it generates WH-type questions for which users can easily produceanswer utterances.2http://lucene.apache.orgFor conventional retrieval-based approaches,contrary to Ritter et al(2011), Retrieval-Selfshows significantly lower scores in almost allthe evaluation items, and Retrieval-Reply showsscores close to Proposed-All.
These results re-flect the retrieved corpus size, which is 40 timeslarger than that of Ritter et al(2011).
Whenthe retrieval performance improves, Retrieval-Selfreturns posts that are too similar to user utter-ances, while Retrieval-Reply can find appropri-ate source posts.
Retrieval-Reply shows almostthe same scores as Proposed-All for each singleevaluation metric, but Retrieval-Reply is inferiorto Proposed-All in the averaged evaluation items(10).
This is a reason why Retrieval-Reply is alsoinferior in (1).None of the systems approached human per-formance.
The users thought that the systemswere not able to respond to user utterances thatreferred to the system itself, like personal ques-tions; and that the systems didn?t understand userutterances since the systems sometimes generatea question that contains different but semanticallysimilar words to those used by the user, due to thelack of thesaurus knowledge.5 ConclusionsWe proposed a novel open-domain utterance gen-eration approach for a conversational dialoguesystem that generates system utterances usingtemplates populated with topics and related wordsextracted from a huge number of dependencystructures.
Our chat experiments demonstratedthat our template-based approach generated sys-tem utterances preferred over those producedwith retrieval-based approaches, and that WH-type questions make it easy for users to producetheir next utterance.
Our work also indicatedthat template-based utterance generation, which isconsidered a legacy approach, has potential whenthe template-filling resource is huge.
Future workincludes improving the data-driven topic selec-tion in the proposed approach, the aggregation ofwords with web-scale class structures like Tama-gawa et al(2012), response generation for utter-ances that describe the systems themselves, andexploitation of information about the user to gen-erate system utterances.337ReferencesTimothy Bickmore and Justine Cassell.
2001.
Re-lational Agents: A Model and Implementation ofBuilding User Trust.
In Proceedings of the SIGCHIConference on Human Factors in Computing Sys-tems, pages 396?403.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Kenji Imamura, Genichiro Kikui, and Norihito Yasuda.2007.
Japanese Dependency Parsing Using Sequen-tial Labeling for Semi-Spoken Language.
In Pro-ceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 225?228.Sina Jafarpour and Christopher J.C. Burges.
2010.
Fil-ter, Rank, and Transfer the Knowledge: Learningto Chat.
Technical Report MSR-TR-2010-93, Mi-crosoft.Toyomi Meguro, Ryuichiro Higashinaka, Yasuhiro Mi-nami, and Kohji Dohsaka.
2010.
ControllingListening-oriented Dialogue using Partially Observ-able Markov Decision Processes.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics, pages 761?769.Alan Ritter, Colin Cherry, and William.B.
Dolan.2011.
Data-Driven Response Generation in SocialMedia.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 583?593.Masahiro Shibata, Tomomi Nishiguchi, and YoichiTomiura.
2009.
Dialog System for Open-EndedConversation Using Web Documents.
Informatica,33:277?284.Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliz-abeth Shriberg, Rebecca Bates, Daniel Jurafsky,Paul Taylor, Rachel Martin, Carol Van Ess-Dykema,and Marie Meteer.
2000.
Dialogue Act Mod-eling for Automatic Tagging and Recognition ofConversational Speech.
Computational Linguistics,26(3):339?373.Susumu Tamagawa, Takeshi Morita, and Takahira Ya-maguchi.
2012.
Extracting Property Semanticsfrom Japanese Wikipedia.
In Proceedings of the 8thinternational conference on Active Media Technol-ogy, pages 357?368.Wilson Wong, Lawrence Cavedon, John Thangara-jah, and Lin Padgham.
2012.
Strategies forMixed-Initiative Conversation Management usingQuestion-Answer Pairs.
In Proceedings of the 24thInternational Conference on Computational Lin-guistics, pages 2821?2834.Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawa-hara.
2011.
Spoken Dialogue System based on In-formation Extraction using Similarity of PredicateArgument Structures.
In Proceedings of the 12thAnnual Meeting of the Special Interest Group onDiscourse and Dialogue, pages 59?66.AppendixTopic-word Extracted topic-related wordsMt.Fuji beautiful, good, clear, white, huge (adj)Bread delicious, good, like, cute (adj)Eat I (?ga?
: subject-case), rice (?wo?
: target-case)together (?ni?
: ?with?,?
to?
)Table 2: Extracted topic-related words (translatedby authors)Word type Template exampleProper nouns I hear that [Noun] is [adj], isn?t it?
[Noun] ha [adj] deshitakke?
(JP)General nouns Actually, I am weak in [Noun]...[Noun] ha chotto nigate desu... (JP)Adjectives and How does it [Noun]?adjectival nouns douiu huuni [Noun] nndesuka?
(JP)Verbs and Where do you [Verb]?verbal nouns doko de [Verb] nndesuka?
(JP)Table 3: Templates for topic types (translated byauthors)Dialogue act ExampleGreetings Hello.Sympathy Good./ Nice.Non-sympathy Not so good..Filler Uh.Confirmation Really?/Oh, [Noun].Table 4: Generated utterances for dialogue acts(translated by authors)Speaker UtteranceUser Hello.System Hello.
Do you like movies?User I like movies.System Good.
I think movie is fun, isn?t it?User I like reading more than going to the movies.System Reading is also good.User I borrow books from my local library.System Nice.
I hear that libraries are quiet.User Yes, they usually are.I used to study, write reports, and read books.Table 5: Dialogues between user and Proposed-All (translated by authors)338
