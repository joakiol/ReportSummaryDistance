Proceedings of the ACL Student Research Workshop, pages 74?80,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAnnotating named entities in clinical textby combining pre-annotation and active learningMaria SkeppstedtDept.
of Computer and Systems Sciences (DSV)Stockholm University, Forum 100, 164 40 Kista, Swedenmariask@dsv.su.seAbstractFor expanding a corpus of clinical text, an-notated for named entities, a method thatcombines pre-tagging with a version of ac-tive learning is proposed.
In order to fa-cilitate annotation and to avoid bias, twoalternative automatic pre-taggings are pre-sented to the annotator, without reveal-ing which of them is given a higher con-fidence by the pre-tagging system.
Thetask of the annotator is to select the cor-rect version among these two alternatives.To minimise the instances in which noneof the presented pre-taggings is correct,the texts presented to the annotator are ac-tively selected from a pool of unlabelledtext, with the selection criterion that oneof the presented pre-taggings should havea high probability of being correct, whilestill being useful for improving the resultof an automatic classifier.1 IntroductionOne of the key challenges for many NLP appli-cations is to create the annotated corpus neededfor development and evaluation of the application.Such a corpus is typically created through man-ual annotation, which is a time-consuming task.Therefore, there is a need to explore methods forsimplifying the annotation task and for reducingthe amount of data that must be annotated.Annotation can be simplified by automatic pre-annotation, in which the task of the annotator isto improve or correct annotations provided by anexisting system.
The amount of data needed to beannotated can be reduced by active learning, i.e.by actively selecting data to annotate that is usefulto a machine learning system.
When using pre-tagged data, the annotator might, however, be bi-ased to choose the annotation provided by the pre-tagger.
Also, if the produced pre-taggings are notgood enough, it is still a time-consuming task tocorrect them or select the correct tagging amongmany suggestions.Consequently, there is a need to further explorehow an annotated corpus can be expanded withless effort and using methods that will not bias theannotators.2 BackgroundThe background discusses basic ideas of pre-annotation and active learning, as well as the parti-cular challenges associated with annotating clini-cal text.2.1 Annotating clinical textA number of text annotation projects have beencarried out in the clinical domain, some of themincluding annotations of clinical named entities,such as mentions of symptoms, diseases and med-ication.
Such studies have for example beendescribed by Ogren et al(2008), Chapman etal.
(2008), Roberts et al(2009), Wang (2009),Uzuner et al(2010), Koeling et al(2011) and Al-bright et al(2013).As in many specialised domains, expert annota-tors are typically required to create a reliable an-notated clinical corpus.
These expert annotatorsare often more expensive than annotators withoutthe required specialised knowledge.
It is also diffi-cult to use crowdsourcing approaches, such as us-ing e.g.
Amazon?s Mechanical Turk to hire on-line annotators with the required knowledge (Xiaand Yetisgen-Yildiz, 2012).
A further challengeis posed by the content of the clinical data, whichis often sensitive and should therefore only be ac-cessed by a limited number of people.
Researchcommunity annotation is consequently another op-tion that is not always open to annotation projectsin the clinical domain, even if there are examplesof such community annotations also for clinicaltext, e.g.
described by Uzuner et al(2010).74To simplify the annotation process, and to min-imise the amount of annotated data is thereforeeven more important for annotations in the clini-cal domain than for annotation in general.2.2 Pre-annotationA way to simplify annotation is automatic pre-annotation (or pre-tagging), in which a text is auto-matically annotated by an existing system, beforeit is given to the annotator.
Instead of annotatingunlabelled data, the annotator either corrects mis-takes made by this existing system (Chou et al2006), or chooses between different taggings pro-vided by the system (Brants and Plaehn, 2000).The system providing the pre-annotations couldbe rule- or terminology based, not requiring an-notated data (Mykowiecka and Marciniak, 2011),as well as a machine learning/hybrid system thatuses the annotations provided by the annotator toconstantly improve the pre-annotation (Tomaneket al 2012).
There exist several annotation toolsthat facilitate the use of pre-annotation by allow-ing the user to import pre-annotations or by pro-viding pre-annotation included in the tools (Nevesand Leser, 2012).A condition for pre-annotation to be useful isthat the produced annotations are good enough, orthe effect can be the opposite, slowing the annota-tors down (Ogren et al 2008).
Another potentialproblem with pre-annotation is that it might biastowards the annotations given by the pre-tagging,for instance if a good pre-tagger reduces the atten-tion of the annotators (Fort and Sagot, 2010).2.3 Active learningActive learning can be used to reduce the amountof annotated data needed to successfully train amachine learning model.
Instead of randomly se-lecting annotation data, instances in the data thatare highly informative, and thereby also highlyuseful for the machine learning system, are thenactively selected.
(Olsson, 2008, p. 27).There are several methods for selecting themost informative instances among the unlabelledones in the available pool of data.
A frequentlyused method is uncertainty sampling, in which in-stances that the machine learner is least certainhow to classify are selected for annotation.
Fora model learning to classify into two classes, in-stances, for which the classifier has no clear pref-erence for one of the two alternatives, are chosenfor annotation.
If there are more than two classes,the confidence for the most probable class can beused as the measure of uncertainty.
Only using thecertainty level for the most probable classificationmeans that not all available information is used,i.e.
the information of the certainty levels for theless probable classes.
(Settles, 2009)An alternative for a multi-class classifier istherefore to instead use the difference of the cer-tainty levels for the two most probable classes.
Ifcp1 is the most probable class and cp2 is the sec-ond most probable class for the observation xn,the margin used for measuring uncertainty for thatinstance is:Mn = P (cp1|xn)?
P (cp2|xn) (1)An instance with a large margin is easy to clas-sify because the classifier is much more certain ofthe most probable classification than on the secondmost probable.
Instances with a small margin, onthe other hand, are difficult to classify, and there-fore instances with a small margin are selected forannotation (Schein and Ungar, 2007).
A commonalternative is to use entropy as an uncertainty mea-sure, which takes the certainty levels of all possi-ble classes into account (Settles, 2009).There are also a number of other possible meth-ods for selecting informative instances for anno-tation, for instance to use a committee of learnersand select the instances for which the committeedisagrees the most, or to search for annotation in-stances that would result in the largest expectedchange to the current model (Settles, 2009).There are also methods to ensure that the se-lected data correctly reflects the distribution in thepool of unlabelled data, avoiding a selection ofoutliers that would not lead to a correct model ofthe available data.
Such methods for structuredprediction have been described by Symons et al(2006) and Settles and Craven (2008).Many different machine learning methods havebeen used together with active learning for solvingvarious NLP tasks.
Support vector machines havebeen used for text classification (Tong and Koller,2002), using properties of the support vector ma-chine algorithm for determining what unlabelleddata to select for classification.
For structured out-put tasks, such as named entity recognition, hid-den markov models have been used by Schefferet al(2001) and conditional random fields (CRF)by Settles and Craven (2008) and Symons et al(2006).75Olsson (2008) suggests combining active learn-ing and pre-annotation for a named entity recogni-tion task, that is providing the annotator with pre-tagged data from an actively learned named entityrecogniser.
It is proposed not to indiscriminatelypre-tagg the data, but to only provide those pre-annotated labels to the human annotator, for whichthe pre-tagger is relatively certain.3 MethodPrevious research on pre-annotation shows twoseemingly incompatible desirable properties in apre-annotation system.
A pre-annotation that isnot good enough might slow the human annota-tor down, whereas a good pre-annotation mightmake the annotator lose concentration, trusting thepre-annotation too much, resulting in a biased an-notation.
One possibility suggested in previousresearch, is to only provide pre-annotations forwhich the pre-annotation system is certain of itsclassification.
For annotations of named entities intext, this would mean to only provide pre-taggedentities for which the pre-annotations system iscertain.
Such a high precision pre-tagger might,however, also bias the human annotator towardsnot correcting the pre-annotation.Even more incompatible seems a combinationbetween pre-annotation and active learning, thatis to provide the human annotator with pre-taggeddata that has been selected for active learning.The data selected for annotation when using activelearning, is the data for which the pre-annotator ismost uncertain and therefore the data which wouldbe least suitable for pre-annotation.The method proposed here aims at finding away of combining pre-annotation and active learn-ing while reducing the risk of annotation bias.Thereby decreasing the amount of data that needsto be annotated as well as facilitating the annota-tion, without introducing bias.
A previous versionof this idea has been outlined by Skeppstedt andDalianis (2012).The method is focused on the annotation ofnamed entities in clinical text, that is marking ofspans of text as well as classification of the spansinto an entity class.3.1 Pre-annotationAs in standard pre-annotation, the annotator willbe presented with pre-tagged data, and does nothave to annotate the data from scratch.To reduce the bias problem that might be asso-ciated with pre-tagging, the mode of presentationwill, however, be slightly different in the methodproposed here.
Instead of presenting the best tag-ging for the human annotator to correct, or topresent the n best taggings, the two best taggingsproduced by a pre-tagger will be presented, with-out informing the annotator which of them that thepre-tagger considers most likely.When being presented with two possible anno-tations of the same text without knowing which ofthem that the pre-annotation system considers asmost likely, the annotator always has to make anactive choice of which annotation to choose.
Thisreduces the bias to one particular pre-annotation,thereby eliminating a drawback associated withstandard pre-annotation.
Having to consider twoalternatives might add cognitive load to the anno-tator compared to correcting one alternative, butought to be easier than annotating a text that is notpre-tagged.The reason for presenting two annotations, asopposed to three or more, is that it is relativelyeasy to compare two texts, letting your eyes wan-der from one text to the other, when you have onecomparison to make.
Having three optional an-notations would result in three comparisons, andhaving four would result in six comparisons, andso on.
Therefore, having two optional annotationsto choose from, reduces the bias problem while atthe same time still offering a method for speedingup the annotation.A simple Java program for choosing betweentwo alternative pre-annotated sentences has beencreated (Figure 1).
The program randomlychooses in which of the two text boxes to placewhich pre-annotation.
The user can either choosethe left or the right annotation, or that none of themis correct.The data will be split into sentences, and onesentence at time will be presented to the annotatorfor annotation.3.2 Active learningTo choose from two presented annotations mightalso potentially be faster than making correctionsto one presented annotation.
For this to be thecase, however, one of the presented annotationshas to be a correct annotation.
In order to achievethat, the proposed method is to use a version ofactive learning.76Figure 1: A simple program for choosing between two alternative annotations, showing a constructedexample in English.The standard use of active learning is to activelyselect instances to annotate that are useful to a ma-chine learner.
Instances for which the machinelearning model can make a confident classifica-tion are not presented to the annotator, as theseinstances will be of little benefit for improving themachine learning system.The version of active learning proposed here isretaining this general idea of active learning, butis also adding an additional constraint to what in-stances that are actively selected for annotation.This constraint is to only select text passages forwhich it is probable that one of the two bestpre-taggings is correct, i.e.
the pre-tagger has tobe confident that one of the two presented pre-annotations is correct, but it should be uncertainas to which one of them is correct.For ensuring that the sentences selected for an-notation are informative enough, the previouslydescribed difference of the certainty level of thetwo most probable classes will be used.
The samestandard for expressing margin as used in (1), canbe used here, except that in (1), cp1 and cp2 standfor classification of one instance, whereas in thiscase the output is a sequence of labels, labellingeach token in a sentence.
Therefore, cp1 and cp2stand for the classification of a sequence of labels.Let cp1 be the most probable labelling sequence,cp2 the second most probable labelling sequenceand cp3 the third most probable labelling sequence.Moreover, let xn be the observations in sentencen, then the following margins can be defined forthat sentence:MtoSecond n = P (cp1|xn)?
P (cp2|xn) (2)MtoThird n = P (cp1|xn)?
P (cp3|xn) (3)To make the probability high that one of thetwo presented pre-annotations is correct, the samemethod that is used for determining that an an-notation instance is informative enough could beused.
However, instead of minimising the marginbetween two classification instances, it is ensuredthat the margin in high enough.
That is, the differ-ence in certainty level between the two most prob-able annotations and the third most probable mustbe high enough to make it probable that one of thetwo best classification candidates is correct.
Thiscan be achieved by forcing MtoThird to be above athreshold, t.The criteria for selecting the next candidate sen-tence to annotate can then be described as:x?
= argminxP (cp1|x)?
P (cp2|x) (4)whereP (cp1|x)?
P (cp3|x) > tAs instances with the highest possible P (cp2|x)in relation to P (cp1|x) are favoured, no thresholdfor the margin between P (cp2|x) and P (cp3|x) isneeded.It might be difficult to automatically determinean appropriate value of the threshold t. Therefore,the proposed method for finding a good threshold,is to adapt it to the behaviour of the annotator.
Ifthe annotator often rejects the two presented pre-taggings, text passages for which the pre-tagger ismore certain ought to be selected, that is the valueof t ought to be increased.
On the other hand,if one of the presented pre-taggings often is se-lected by the annotator as the correct annotation,the value of t can be decreased, possibly allowingfor annotation instances with a smaller MtoSecond.3.3 Machine learning systemAs machine learning system, the conditional ran-dom fields system CRF++ (Kudo, 2013) will be77used.
This system uses a combination of forwardViterbi and backward A* search for finding thebest classification sequence for an input sentence,given the trained model.
It can also produce then-best classification sequences for each sentence,which is necessary for the proposed pre-tagger thatpresents the two best pre-taggings to the humanannotator.CRF++ can also give the conditional probablyfor the output, that is for the entire classificationsequence of a sentence, which is needed in the pro-posed active learning algorithm.3.4 MaterialsThere is a corpus of Swedish clinical text, i.e.the text in the narrative part of the health record,that contains clinical text from the Stockholm area,from the years 2006-2008 (Dalianis et al 2009).A subset of this corpus, containing texts from anemergency unit of internal medicine, has been an-notated for four types of named entities: disorder,finding, pharmaceutical drug and body structure(Skeppstedt et al 2012).
For approximately onethird of this annotated corpus, double annotationhas been performed, and the instances, for whichthere were a disagreement, have been resolved byone of the annotators.The annotated corpus will form the main sourceof materials for the study proposed here, and addi-tional data to annotate will be selected from a poolof unlabelled data from internal medicine emer-gency notes.The larger subset of the annotated data, onlyannotated by one annotator, will be referred toas Single (containing 45 482 tokens), and thesmaller subset, annotated by two annotators, willbe referred to as Double (containing 25 370 to-kens).
The Single subset will be the main sourcefor developing the pre-annotation/active learningmethod, whereas the Double subset will be usedfor a final evaluation.3.5 Step-by-step explanationThe proposed method can be divided into 8 steps:1.
Train a CRF model with a randomly selectedsubset of the Single part of the annotated cor-pus, the seed set.
The size of this seed set, aswell as suitable features for the CRF modelwill be evaluated using cross validation onthe seed set.
The size should be as small aspossible, limiting the amount of initial anno-tation needed, but large enough to have re-sults in line with a baseline system using ter-minology matching for named entity recog-nition (Skeppstedt et al 2012).2.
Apply the constructed CRF model on unla-belled data from the pool of data from in-ternal medicine emergency notes.
Let themodel, which operates on a sentence level,provide the three most probable label se-quences for each sentence, together with itslevel of certainty.3.
Calculate the difference in certainty be-tween the most probable and the third mostprobable suggestion sequence for each sen-tence, that is MtoThird.
Start with a lowthreshold t and place all sentences withMtoThird above the threshold t in a list ofcandidates for presenting to the annotator(that is the sentences fulfilling the criterionP (cp1|x)?
P (cp3|x) > t).4.
Order the sentences in the list of se-lected candidates in increasing order ofMtoSecond.
Present the sentence with thelowest MtoSecond to the annotator.
This is thesentence, for which the pre-tagger is most un-certain of which one of the two most probablepre-taggings is correct.Present the most probable pre-annotationas well as the second most probable pre-annotation, as shown in Figure 1.5.
If the annotator chooses that none of the pre-sented pre-annotations is correct, discard theprevious candidate selection and make a newone from the pool with a higher thresholdvalue t. Again, order the sentences in increas-ing order of MtoSecond, and present the sen-tence with the lowest MtoSecond to the anno-tator.Repeat step 3., 4. and 5., gradually increasingthe threshold until the annotator accepts oneof the presented pre-annotations.6.
Continue presenting the annotator with thetwo most probable pre-annotations for thesentences in the list of selected candidatesentences, and allow the human annotator tochoose one of the pre-annotations.78The threshold t could be further adjusted ac-cording to how often the option ?None?
ischosen.7.
Each selected annotation is added to a setof annotated data.
When a sufficiently largeamount of new sentences have been added tothis set, the model needs to be retrained withthe new data.
The retraining of the model canbe carried out as a background process whilethe human annotator is annotating.
In or-der to use the annotator time efficiently, thereshould not be any waiting time while retrain-ing.8.
When the model has been retrained, the pro-cess starts over from step 2.3.6 EvaluationThe text passages chosen in the selection processwill, as explained above, be used to re-train themachine learning model, and used when select-ing new text passages for annotation.
The effectof adding additional annotations will also be con-stantly measured, using cross validation on theseed set.
The additional data added by the activelearning experiments will, however, not be usedin the validation part of the cross validation, butonly be used as additional training data, in order tomake sure that the results are not improved due toeasily classified examples being added to the cor-pus.When an actively selected corpus of the samesize as the entire Single subset of the corpus hasbeen created, this actively selected corpus will beused for training a machine learning model.
Theperformance of this model will then be comparedto a model trained on the single subset.
Both mod-els will be evaluated on the Double subset of thecorpus.
The hypothesis is that the machine learn-ing model trained on the corpus partly created bypre-tagging and active learning will perform bet-ter than the model created on the original Singlesubset.4 ConclusionA method that combines pre-annotation and activelearning, while reducing annotation bias, is pro-posed.
A program for presenting pre-annotateddata to the human annotator for selection has beenconstructed, and a corpus of annotated data suit-able as a seed set and as evaluation data hasbeen constructed.
The active learning part of theproposed method remains, however, to be imple-mented.Applying the proposed methods aims at creat-ing a corpus suitable for training a machine learn-ing system to recognise the four entities Disorder,Finding, Pharmaceutical drug and Body struc-ture.
Moreover, methods for facilitating annotatedcorpus construction will be explored, potentiallyadding new knowledge to the science of annota-tion.AcknowledgementsI am very grateful to the reviewers and the pre-submission mentor for their many valuable com-ments.
I would also like to thank Hercules Dalia-nis and Magnus Ahltorp as well as the participantsof the ?Southern California Workshop on MedicalText Analysis and Visualization?
for fruitful dis-cussions on the proposed method.ReferencesDaniel Albright, Arrick Lanfranchi, Anwen Fredrik-sen, William F 4th Styler, Colin Warner, Jena DHwang, Jinho D Choi, Dmitriy Dligach, Rod-ney D Nielsen, James Martin, Wayne Ward, MarthaPalmer, and Guergana K Savova.
2013.
Towardscomprehensive syntactic and semantic annotationsof the clinical narrative.
J Am Med Inform Assoc,Jan.Thorsten Brants and Oliver Plaehn.
2000.
Interactivecorpus annotation.
In LREC.
European LanguageResources Association.Wendy W Chapman, John N Dowling, and GeorgeHripcsak.
2008.
Evaluation of training with an an-notation schema for manual annotation of clinicalconditions from emergency department reports.
IntJ Med Inform, Epub 2007 Feb 20, 77(2):107?113,February.Wen-chi Chou, Richard Tzong-han Tsai, and Ying-shan Su.
2006.
A semi-automatic method for anno-tating a biomedical proposition bank.
In FLAC?06.ACL.Hercules Dalianis, Martin Hassel, and SumithraVelupillai.
2009.
The Stockholm EPR Corpus -Characteristics and Some Initial Findings.
In Pro-ceedings of ISHIMR 2009, Evaluation and imple-mentation of e-health and health information initia-tives: international perspectives.
14th InternationalSymposium for Health Information Management Re-search, Kalmar, Sweden, pages 243?249.Kare?n Fort and Beno?
?t Sagot.
2010.
Influence ofpre-annotation on pos-tagged corpus development.79In Proceedings of the Fourth Linguistic AnnotationWorkshop, LAW IV ?10, pages 56?63, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Rob Koeling, John Carroll, Rosemary Tate, andAmanda Nicholson.
2011.
Annotating a corpus ofclinical text records for learning to recognize symp-toms automatically.
In Proceedings of the LOUHI2011, Third International Workshop on Health Doc-ument Text Mining and Information Analysis.Taku Kudo.
2013.
CRF++: Yet Another CRF toolkit.http://crfpp.sourceforge.net/.
Accessed 2013-05-21.Agnieszka Mykowiecka and Ma?gorzata Marciniak.2011.
Some remarks on automatic semantic an-notation of a medical corpus.
In Proceedings ofthe LOUHI 2011, Third International Workshopon Health Document Text Mining and InformationAnalysis.Mariana Neves and Ulf Leser.
2012.
A survey on an-notation tools for the biomedical literature.
Brief-ings in Bioinformatics.Philip Ogren, Guergana Savova, and ChristopherChute.
2008.
Constructing evaluation corpora forautomated clinical named entity recognition.
In Pro-ceedings of the Sixth International Language Re-sources and Evaluation (LREC?08), pages 3143?3149, Marrakech, Morocco, May.
European Lan-guage Resources Association (ELRA).Fredrik Olsson.
2008.
Bootstrapping Named EntityAnnotation by Means of Active Machine Learning.Ph.D.
thesis, University of Gothenburg.
Faculty ofArts.Angus Roberts, Robert Gaizauskas, Mark Hepple,George Demetriou, Yikun Guo, Ian Roberts, andAndrea Setzer.
2009.
Building a semantically an-notated corpus of clinical texts.
J. of Biomedical In-formatics, 42:950?966, October.Tobias Scheffer, Christian Decomain, and Stefan Wro-bel.
2001.
Active hidden markov models for in-formation extraction.
In Proceedings of the 4th In-ternational Conference on Advances in IntelligentData Analysis, IDA ?01, pages 309?318, London,UK, UK.
Springer-Verlag.Andrew I. Schein and Lyle H. Ungar.
2007.
Ac-tive learning for logistic regression: an evaluation.Mach.
Learn., 68(3):235?265, October.Burr Settles and Mark Craven.
2008.
An analysisof active learning strategies for sequence labelingtasks.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?08, pages 1070?1079, Stroudsburg, PA,USA.
Association for Computational Linguistics.Burr Settles.
2009.
Active learning literature survey.Computer Sciences Technical Report 1648, Univer-sity of Wisconsin?Madison.Maria Skeppstedt and Hercules Dalianis.
2012.
Usingactive learning and pre-tagging for annotating clin-ical findings in health record text.
In Proceedingsof SMBM 2012 - The 5th International Symposiumon Semantic Mining in Biomedicine, pages 98?99,Zurich, Switzerland, September 3-4.Maria Skeppstedt, Maria Kvist, and Hercules Dalianis.2012.
Rule-based entity recognition and coverage ofSNOMED CT in Swedish clinical text.
In Proceed-ings of the Eight International Conference on Lan-guage Resources and Evaluation (LREC?12), pages1250?1257, Istanbul, Turkey, may.
European Lan-guage Resources Association (ELRA).Christopher T. Symons, Nagiza F. Samatova, RamyaKrishnamurthy, Byung H. Park, Tarik Umar, DavidButtler, Terence Critchlow, and David Hysom.2006.
Multi-criterion active learning in conditionalrandom fields.
In Proceedings of the 18th IEEE In-ternational Conference on Tools with Artificial In-telligence, ICTAI ?06, pages 323?331, Washington,DC, USA.
IEEE Computer Society.Katrin Tomanek, Philipp Daumke, Frank Enders, JensHuber, Katharina Theres, and Marcel Mu?ller.
2012.An interactive de-identification-system.
In Proceed-ings of SMBM 2012 - The 5th International Sympo-sium on Semantic Mining in Biomedicine, pages 82?86, Zurich, Switzerland, September 3-4.Simon Tong and Daphne Koller.
2002.
Supportvector machine active learning with applications totext classification.
J. Mach.
Learn.
Res., 2:45?66,March.O?zlem Uzuner, Imre Solti, Fei Xia, and EithonCadag.
2010.
Community annotation experimentfor ground truth generation for the i2b2 medicationchallenge.
J Am Med Inform Assoc, 17(5):519?523.Yefeng Wang.
2009.
Annotating and recognisingnamed entities in clinical notes.
In Proceedings ofthe ACL-IJCNLP Student Research Workshop, pages18?26, Singapore.Fei Xia and Meliha Yetisgen-Yildiz.
2012.
Clinicalcorpus annotation: Challenges and strategies.
InThe Third Workshop on Building and Evaluating Re-sources for Biomedical Text Mining (BioTxtM), anLREC Workshop.
Turkey.80
