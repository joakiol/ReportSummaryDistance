Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 11?20,Columbus, June 2008. c?2008 Association for Computational LinguisticsResponse-Based Confidence Annotation for Spoken Dialogue SystemsAlexander GruensteinSpoken Language Systems GroupM.I.T.
Computer Science and Artificial Intelligence Laboratory32 Vassar St, Cambridge, MA 02139 USAalexgru@csail.mit.eduAbstractSpoken and multimodal dialogue systems typ-ically make use of confidence scores to chooseamong (or reject) a speech recognizer?s N-best hypotheses for a particular utterance.
Weargue that it is beneficial to instead chooseamong a list of candidate system responses.We propose a novel method in which a con-fidence score for each response is derivedfrom a classifier trained on acoustic and lex-ical features emitted by the recognizer, aswell as features culled from the generation ofthe candidate response itself.
Our response-based method yields statistically significantimprovements in F-measure over a baseline inwhich hypotheses are chosen based on recog-nition confidence scores only.1 IntroductionThe fundamental task for any spoken dialogue sys-tem is to determine how to respond at any given timeto a user?s utterance.
The challenge of understand-ing and correctly responding to a user?s natural lan-guage utterance is formidable even when the wordshave been perfectly transcribed.
However, dialoguesystem designers face a greater challenge becausethe speech recognition hypotheses which serve asinput to the natural language understanding compo-nents of a system are often quite errorful; indeed, itis not uncommon to find word error rates of 20-30%for many dialogue systems under development in re-search labs.
Such high error rates often arise due tothe use of out-of-vocabulary words, noise, and theincreasingly large vocabularies of more capable sys-tems which try to allow for greater naturalness andvariation in user input.Traditionally, dialogue systems have relied onconfidence scores assigned by the speech recognizerto detect speech recognition errors.
In a typicalsetup, the dialogue system will choose to either ac-cept (that is, attempt to understand and respond to)or reject (that is, respond to the user with an indica-tion of non-understanding) an utterance by thresh-olding this confidence score.Stating the problem in terms of choosing whetheror not to accept a particular utterance for process-ing, however, misses the larger picture.
From theuser?s perspective, what is truly important is whetheror not the system?s response to the utterance is cor-rect.
Sometimes, an errorful recognition hypothe-sis may result in a correct response if, for example,proper names are correctly recognized; conversely,a near-perfect hypothesis may evoke an incorrect re-sponse.
In light of this, the problem at hand is betterformulated as one of assigning a confidence scoreto a system?s candidate response which reflects theprobability that the response is an acceptable one.If the system can?t formulate a response in which ithas high confidence, then it should clarify, indicatenon-understanding, and/or provide appropriate help.In this paper, we present a method for assign-ing confidence scores to candidate system responsesby making use not only of features obtained fromthe speech recognizer, but also of features culledfrom the process of generating a candidate systemresponse, and derived from the distribution of can-didate responses themselves.
We first compile a listof unique candidate system responses by processing11each hypothesis on the recognizer?s N-best list.
Wethen train a Support Vector Machine (SVM) to iden-tify acceptable responses.
When given a novel ut-terance, candidate responses are ranked with scoresoutput from the SVM.
Based on the scores, the sys-tem can then either respond with the highest-scoringcandidate, or reject all of the candidate responsesand respond by indicating non-understanding.Part of the motivation for focusing our efforts onselecting a system response, rather than a recogni-tion hypothesis, can be demonstrated by countingthe number of unique responses which can be de-rived from an N-best list.
Figure 1 plots the meannumber of unique system responses, parses, andrecognition hypotheses given a particular maximumN-best list length; it was generated using the datadescribed in section 3.
Generally, we observe thatabout half as many unique parses are generated asrecognition hypotheses, and then half again as manyunique responses.
Since many hypotheses evoke thesame response, there is no value in discriminatingamong these hypotheses.
Instead, we should aimto gain information about the quality of a responseby pooling knowledge gleaned from each hypothesisevoking that response.We expect a similar trend of multiple hypothe-ses mapping to a single parse in any dialogue sys-tem where parses contain a mixture of key syntac-tic and semantic structure?as is the case here?orwhere they contain only semantic information (e.g.,slot/value pairs).
Parsers which retain more syn-tactic structure would likely generate more uniqueparses, however many of these parses would prob-ably map to the same system response since a re-sponse doesn?t typically hinge on every syntactic de-tail of an input utterance.The remainder of our discussion proceeds as fol-lows.
In section 2 we place the method presentedhere in context in relation to other research.
In sec-tion 3, we describe the City Browser multimodal di-alogue system, and the process used to collect datafrom users?
interactions with the system.
We thenturn to our techniques for annotating the data insection 4 and describe the features which are ex-tracted from the labeled data in section 5.
Finally,we demonstrate how to build a classifier to rank can-didate system responses in section 6, which we eval-uate in section 7.0 10 20 30 40 5001020304050Maximum N?best lengthMean N?best LengthMean Unique ParsesMean Unique ResponsesFigure 1: The mean N-best recognition hypothesis listlength, mean number of unique parses derived from theN-best list of recognition hypotheses, and mean numberof unique system responses derived from those parses,given a maximum recognition N-best list length.2 Related WorkThere has been much research into derivingutterance-level confidence scores based on featuresderived from the process of speech recognition.
Thebaseline utterance-level confidence module we makeuse of in this paper was introduced in (Hazen et al,2002); we use a subset of the recognizer-derived fea-tures used by this module.
In it, confidence scoresare derived by training a linear projection model todifferentiate utterances with high word error rates.The utterance-level confidence scores are used to de-cide whether or not the entire utterance should beaccepted or rejected, while the decision as to howto respond is left out of the classification process.Of course, most other recognizers make use of utter-ance or hypothesis level confidence scores as well;see, for example (San-Segundo et al, 2000; Chase,1997).
(Litman et al, 2000) demonstrate the additionaluse of prosodic features in deriving confidencescores, and transition the problem from one of worderror rate to one involving concept error rate, whichis more appropriate in the context of spoken dia-logue systems.
However, they consider only the toprecognition hypothesis.Our work has been heavily influenced by (Gabs-dil and Lemon, 2004), (Bohus and Rudnicky, 2002),(Walker et al, 2000), and (Chotimongkol and Rud-12nicky, 2001) all of which demonstrate the utility oftraining a classifier with features derived from thenatural language and dialogue management compo-nents of a spoken dialogue system to better predictthe quality of speech recognition results.
The workdescribed in (Gabsdil and Lemon, 2004) is espe-cially relevant, because, as in our experiments, thedialogue system of interest provides for map-basedmultimodal dialogue.
Indeed, we view the exper-iments presented here as extending and validatingthe techniques developed by Gabsdil and Lemon.Our work is novel, however, in that we reframethe problem as choosing among system responses,rather than among recognizer hypotheses.
By re-casting the problem in these terms, we are able tointegrate information from all recognition hypothe-ses which contribute to a single response, and to ex-tract distributional features from the set of candi-date responses.
Another key difference is that ourmethod produces confidence scores for the candi-date responses themselves, while the cited methodsproduce a decision as to whether an utterance, ora particular recognition hypothesis, should be ac-cepted, rejected, or (in some cases), ignored by thedialogue system.In addition, because of the small size of thedataset used in (Gabsdil and Lemon, 2004), the au-thors were limited to testing their approach withleave-one-out cross validation, which means that,when testing a particular user?s utterance, other ut-terances from the same user also contributed tothe training set.
Their method also does not pro-vide for optimizing a particular metric?such as F-measure?although, it does solve a more difficult3-class decision problem.
Finally, another key dif-ference is that we make use of an n-gram languagemodel with a large vocabulary of proper names,whereas theirs is a context-free grammar with asmaller vocabulary.
(Niemann et al, 2005) create a dialogue sys-tem architecture in which uncertainty is propagatedacross each layer of processing through the use ofprobabilities, eventually leading to posterior proba-bilities being assigned to candidate utterance inter-pretations.
Unlike our system, in which we train asingle classifier using arbitrary features derived fromeach stage of processing, each component (recog-nizer, parser, etc) is trained separately and must becapable of assigning conditional probabilities to itsoutput given its input.
The method hinges on proba-bilistic inference, yet it is often problematic to mapa speech recognizer?s score to a probability as theirapproach requires.
In addition, the method is evalu-ated only in a toy domain, using a few sample utter-ances.3 Experimental DataThe data used for the experiments which followwere collected from user interactions with CityBrowser, a web-based, multimodal dialogue system.A thorough description of the architecture and ca-pabilities can be found in (Gruenstein et al, 2006;Gruenstein and Seneff, 2007).
Briefly, the versionof City Browser used for the experiments in this pa-per allows users to access information about restau-rants, museums, and subway stations by navigatingto a web page on their own computers.
They canalso locate addresses on the map, and obtain drivingdirections.
Users can interact with City Browser?smap-based graphical user interface by clicking anddrawing; and they can speak with it by talking intotheir computer microphone and listening to a re-sponse from their speakers.
Speech recognition isperformed via the SUMMIT recognizer, using a tri-gram language model with dynamically updatableclasses for proper nouns such as city, street, andrestaurant names?see (Chung et al, 2004) for a de-scription of this capability.
Speech recognition re-sults were parsed by the TINA parser (Seneff, 1992)using a hand-crafted grammar.
A discourse mod-ule (Filisko and Seneff, 2003) then integrates con-textual knowledge.
The fully formed request is sentto the dialogue manager, which attempts to craftan appropriate system response?both in terms ofa verbal and graphical response.
The GENESISsystem (Seneff, 2002) uses hand-crafted generationrules to produce a natural language string, which issent to an off-the-shelf text-to-speech synthesizer.Finally, the user hears the response, and the graphi-cal user interface is updated to show, for example, aset of search results on the map.3.1 Data CollectionThe set of data used in this paper was collectedas part of a controlled experiment in which users13worked through a set of scenarios by accessing theCity Browser web page from their own computers,whenever and from wherever they liked.
Interestedreaders may refer to (Gruenstein and Seneff, 2007)for more information on the experimental setup, aswell as for an initial analysis of a subset of the dataused here.
Users completed a warmup scenario inwhich they were simply told to utter ?Hello CityBrowser?
to ensure that their audio setup and webbrowser were working properly.
They then workedthrough ten scenarios presented sequentially, fol-lowed by time for ?free play?
in which they coulduse the system however they pleased.As users interact with City Browser, logs aremade recording their interactions.
In addition torecording each utterance, every time a user clicksor draws with the mouse, these actions are recordedand time-stamped.
The outputs of the various stagesof natural language processing are also logged, sothat the ?dialogue state?
of the system is tracked.This means that, associated with each utterance inthe dataset is, among other things, the following in-formation:?
a recording of the utterance;?
the current dialogue state, which includes in-formation such as recently referred to entitiesfor anaphora resolution;?
the state of the GUI, including: the current po-sition and bounds of the map, any points of in-terest (POIs) displayed on the map, etc.;?
the contents of any dynamically updatable lan-guage model classes; and?
time-stamped clicks, gestures, and other userinterface interaction performed by the user be-fore and during speech.The utterances of 38 users who attempted mostor all of the scenarios were transcribed, providing1,912 utterances used in this study.
The utteranceswere drawn only from the 10 ?real?
scenarios; ut-terances from the initial warmup and final free playtasks were discarded.
In addition, a small number ofutterances were eliminated because logging glitchesmade it impossible to accurately recover the dia-logue system?s state at the time of the utterance.The class n-gram language model used for datacollection has a vocabulary of approximately 1,200words, plus about 25,000 proper nouns.4 Data AnnotationGiven the information associated with each utter-ance in the dataset, it is possible to ?replay?
an ut-terance to the dialogue system and obtain the sameresponse?both the spoken response and any up-dates made to the GUI?which was originally pro-vided to the user in response to the utterance.
Inparticular, we can replicate the reply frame whichis passed to GENESIS in order to produce a nat-ural language response; and we can replicate thegui reply frame which is sent to the GUI so that itcan be properly updated (e.g., to show the results ofa search on the map).The ability to replicate the system?s response toeach utterance also gives us the flexibility to try outalternative inputs to the dialogue system, given thedialogue state at the time of the utterance.
So, in ad-dition to transcribing each utterance, we also passedeach transcript through the dialogue system, yield-ing a system response.
In the experiments that fol-low, we considered the system?s response to the tran-scribed utterance to be the correct response for thatutterance.
It should be noted that in some cases,even given the transcript, the dialogue system mayreject and respond by signally non-understanding?if, for example, the utterance can?t be parsed.
Inthese cases, we take the response reject to be thecorrect response.We note that labeling the data in this fashionhas limitations.
Most importantly, the system mayrespond inappropriately even to a perfectly tran-scribed utterance.
Such responses, given our label-ing methodology, would incorrectly be labeled ascorrect.
In addition, sometimes it may be the casethat there are actually several acceptable responsesto a particular utterances.5 Feature ExtractionFor each utterance, our goal is to produce a set ofcandidate system responses, where each response isalso associated with a vector of feature values to beused to classify it as acceptable or unacceptable.Responses are labeled as acceptable if they matchthe system response produced from the transcrip-tion, and as unacceptable otherwise.We start with the N-best list output by the speechrecognizer.
For each hypothesis, we extract a set14Recognition Distributional Response(a) Best across hyps: (b) Drop: (c) Other: percent top 3 response typetotal score per word total drop mean words percent top 5 num foundacoustic score per bound acoustic drop top rank percent top 10 POI typelexical score per word lexical drop n-best length percent nbest is subsettop response type parse statusresponse rank geographical filternum distinctTable 1: Features used to train the acceptability classifier.
Nine features are derived from the recognizer; seven haveto do with the distribution of responses; and six come from the process of generating the candidate response.of acoustic, lexical, and total scores from the recog-nizer.
These scores are easily obtained, as they com-prise a subset of the features used to train the rec-ognizer?s existing confidence module; see (Hazen etal., 2002).
The features used are shown in Table 1a.We then map each hypothesis to a candidate sys-tem response, by running it through the dialoguesystem given the original dialogue state.
From theseoutputs, we collect a list of unique responses, whichis typically shorter than the recognizer?s N-best list,as multiple hypotheses typically map to the same re-sponse.We now derive a set of features for each uniqueresponse.
First, each response inherits the best valuefor each recognizer score associated with a hypoth-esis which evoked that response (see Table 1a).
Inaddition, the drop in score between the response?sscore for each recognition feature and the top valueoccurring in the N-best list is used as a feature (seeTable 1b).
Finally, the rank of the highest hypothe-sis on the N-best list which evoked the response, themean number of words per hypothesis evoking theresponses, and the length of the recognizer?s N-bestlist are used as features (see Table 1c).Distributional features are also generated basedon the distribution of hypotheses on the N-best listwhich evoked the same response.
The percent oftimes a particular response is evoked by the top 3,top 5, top 10, and by all hypotheses on the N-bestlist are used as features.
Features are generated, aswell, based on the distribution of responses on thelist of unique responses.
These features are: the ini-tial ranking of this response on the list, the numberof distinct responses on the list, and the type of re-sponse that was evoked by the top hypothesis on therecognizer N-best list.Finally, features derived from the response itself,and natural language processing performed to de-rive that response, are also calculated.
The high-level type of the response, as well as the type andnumber of any POIs returned by a database queryare used as features if they exist, as is a booleanindicator as to whether or not these results are asubset of the results currently shown on the dis-play.
If any sort of ?geographical filter?, such asan address or circled region, is used to constrain thesearch, then the type of this filter is also used as afeature.
Finally, the ?best?
parse status of any hy-potheses leading to this response is also used, wherefull parse  robust parse  no parse.Table 1 lists all of the features used to train theclassifier, while Table 3 (in the appendix) lists thepossible values for the non-numerical features.
Fig-ure 3 (in the appendix) gives an overview of the fea-ture extraction process, as well as the classificationmethod described in the next section.6 Classifier Training and ScoringFor a given utterance, we now have a candidate listof responses derived from the speech recognizer?sN-best list, a feature vector associated with each re-sponse, and a label telling us the ?correct?
response,as derived from the transcript.
In order to build aclassifier, we first label each response as either ac-ceptable or unacceptable by comparing it to the sys-tem?s response to the transcribed utterance.
If thetwo responses are identical, then the response is la-beled as acceptable; otherwise, it is labeled as un-acceptable.
This yields a binary decision problemfor each response, given a set of features.
We traina Support Vector Machine (SVM) to make this deci-15sion, using the Weka toolkit, version 3.4.12 (Wittenand Frank, 2005).Given a trained SVM model, the procedure forprocessing a novel utterance is as follows.
First,classify each response (and its associated featurevector) on the response list for that utterance usingthe SVM.
By using a logistic regression model fit onthe training data, an SVM score between ?1 and 1for each response is yielded, where responses withpositive scores are more likely to be acceptable, andthose with negative scores are more likely to be un-acceptable.Next, the SVM scores are used to rank the list ofresponses.
Given a ranked list of such responses, thedialogue system has two options: it can choose thetop scoring response, or it can abstain from choos-ing any response.
The most straightforward methodfor making such a decision is via a threshold: if thescore of the top response is above a certain thresh-old, this response is accepted; otherwise, the systemabstains from choosing a response, and instead re-sponds by indicating non-understanding.
Figure 3(in the appendix) provides a graphical overview ofthe response confidence scoring process.At first blush, a natural threshold to choose is 0,as this marks the boundary between acceptable andunacceptable.
However, it may be desirable to opti-mize this threshold based on the desired characteris-tics of the dialogue system?in a mission-critical ap-plication, for example, it may be preferable to acceptonly high-confidence responses, and to clarify other-wise.
We can optimize the threshold as we like usingeither the same training data, or a held-out develop-ment set, so long as we have an objective functionwith which to optimize.
In the evaluation that fol-lows, we optimize the threshold using the F-measureon the training data as the objective function.
Itwould also be interesting to optimize the thresholdin a more sophisticated manner, such as that devel-oped in (Bohus and Rudnicky, 2005) where task suc-cess is used to derive the cost of misunderstandingsand false rejections, which in turn are used to set arejection threshold.While a thresholding approach makes sense, otherapproaches are feasible as well.
For instance, a sec-ond classifier could be used to decide whether or notto accept the top ranking response.
The classifiercould take into account such features as the spreadin scores among the responses, the number classi-fied as acceptable, the drop between the top scoreand the second-ranked score, etc.7 EvaluationWe evaluated the response-based method using thedata described in section 3, N-best lists with a maxi-mum length of 10, and an SVM with a linear kernel.We note that, in the live system, two-pass recogni-tion is performed for some utterances, in which akey concept recognized in the first pass (e.g., a cityname) causes a dynamic update to the contents ofa class in the n-gram language model (e.g., a setof street names) for the second pass?as in the ut-terance Show me thirty two Vassar Street in Cam-bridge where the city name (Cambridge) triggersa second pass in which the streets in that city aregiven a higher weight.
This two-pass approach hasbeen shown previously to decrease word and con-cept error rates (Gruenstein and Seneff, 2006), eventhough it can be susceptible to errors in understand-ing.
However, since all street names, for example,are active in the vocabulary at all times, the two-pass approach is not strictly necessary to arrive atthe correct hypotheses.
Hence, for simplicity, in theexperiments reported here, we do not integrate thetwo-pass approach?as this would require us to po-tentially do a second recognition pass for every can-didate response.
In a live system, a good strategymight be to consider a second recognition pass basedon the top few candidate responses alone, whichwould produce a new set of candidates to be scored.We performed 38-fold cross validation, where ineach case the held-out test set was comprised of allthe utterances of a single user.
This ensured that weobtained an accurate prediction of a novel user?s ex-perience, although it meant that the test sets were notof equal size.
We calculated F-measure for each testset, using the methodology described in figure 4 (inthe appendix).7.1 BaselineAs a baseline, we made use of the existing confi-dence module in the SUMMIT recognizer (Hazenet al, 2002).
The module uses a linear projectionmodel to produce an utterance level confidence scorebased on 15 features derived from recognizer scores,16Method FRecognition Confidence (Baseline) .62Recog Features Only .62Recog + Distributional .67Recog + Response .71*Recog + Response + Distributional .72**Table 2: Average F-measures obtained via per-usercross-validation of the response-based confidence scor-ing method using the feature sets described in Section 5,as compared to a baseline system which chooses the tophypothesis if the recognizer confidence score exceeds anoptimized rejection threshold.
The starred scores are astatistically significant (* indicates p < .05, ** indicatesp < .01) improvement over the baseline, as determinedby a paired t-test.and from comparing hypotheses on the N-best list.In our evaluation, the module was trained and testedon the same data as the SVM model using cross-validation.An optimal rejection threshold was determined,as for the SVM method, using the training data withF-measure as the objective function.
For each utter-ance, if the confidence score exceeded the threshold,then the response evoked from the top hypothesis onthe N-best list was chosen.7.2 ResultsTable 2 compares the baseline recognizer confidencemodule to our response-based confidence annotator.The method was evaluated using several subsets ofthe features listed in Table 1.
Using features derivedfrom the recognizer only, we obtain results compa-rable to the baseline.
Adding the response and dis-tributional features yields a 16% improvement overthe baseline system, which is statistically significantwith p < .01 according to a paired t-test.
While thedistributional features appear to be helpful, the fea-ture values derived from the response itself are themost beneficial, as they allow for a statistically sig-nificant improvement over the baseline when pairedon their own with the recognizer-derived features.Figure 2 plots ROC curves comparing the perfor-mance of the baseline model to the best response-based model.
The curves were obtained by varyingthe value of the rejection threshold.
We observe thatthe response-based model outperforms the baseline0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 100.10.20.30.40.50.60.70.80.91False Positive RateTruePositiveRateRecognition + Response + DistributionalRecognition Confidence (Baseline)Figure 2: Receiver Operator Characteristic (ROC) curves(averaged across each cross-validation fold) comparingthe baseline to the best response-based model.no matter what we set our tolerance for false posi-tives to be.The above results were obtained by using an SVMwith a linear kernel, where feature values were nor-malized to be on the unit interval.
We also triedusing a quadratic kernel, retaining the raw featurevalues, and reducing the number of binary featuresby manually binning the non-numeric feature val-ues.
Each change resulted in a slight decrease inF-measure.8 Conclusion and Future WorkWe recast the problem of choosing among an N-bestlist of recognition hypotheses as one of choosing thebest candidate system response which can be gen-erated from the recognition hypotheses on that list.We then demonstrated a framework for assigningconfidence scores to those responses, by using thescores output by an SVM trained to discriminate be-tween acceptable and unacceptable responses.
Theclassifier was trained using a set of features derivedfrom the speech recognizer, culled from the genera-tion of each response, and calculated based on eachresponse?s distribution.
We tested our methods us-ing data collected by users interacting with the CityBrowser multimodal dialogue system, and showedthat they lead to a significant improvement over abaseline which makes an acceptance decision basedon an utterance-level recognizer confidence score.The technique developed herein could be refinedin several ways.
First and foremost, it may well be17possible to find additional features with discrimina-tory power.
Also, the decision as to whether or notto choose the top-scoring response could potentiallybe improved by choosing a more appropriate metricthan F-measure as the objective function, or perhapsby using a second classifier at this stage.Finally, our experiments were performed off-line.In order to better test the approach, we plan to de-ploy the classifier as a component in the running di-alogue system.
This presents some processing timeconstraints (as multiple candidate responses must begenerated); and it introduces the confounding factorof working with a recognizer that can make multi-ple recognition passes after language model recon-figuration.
These challenges should be tractable forN-best lists of modest length.AcknowledgmentsThank you to Stephanie Seneff for her guidanceand advice.
Thanks to Timothy J. Hazen for hisassistance with the confidence module.
Thanks toAli Mohammad for discussions about the machinelearning aspects of this paper and his comments ondrafts.
And thanks to four anonymous reviewers forconstructive criticism.
This research is sponsoredby the T-Party Project, a joint research program be-tween MIT and Quanta Computer Inc., Taiwan.ReferencesDan Bohus and Alex Rudnicky.
2002.
Integrating mul-tiple knowledge sources for utterance-level confidenceannotation in the CMU Communicator spoken dialogsystem.
Technical Report CS-190, Carnegie MellonUniversity.Dan Bohus and Alexander I. Rudnicky.
2005.
A princi-pled approach for rejection threshold optimization inspoken dialog systems.
In Proc.
of INTERSPEECH.Lin Chase.
1997.
Word and acoustic confidence annota-tion for large vocabulary speech recognition.
In Proc.of 5th European Conference on Speech Communica-tion and Technology, pages 815?818.Ananlada Chotimongkol and Alexander I. Rudnicky.2001.
N-best speech hypotheses reordering using lin-ear regression.
In Proc.
of 7th European Conferenceon Speech Communication and Technology.Grace Chung, Stephanie Seneff, Chao Wang, and LeeHetherington.
2004.
A dynamic vocabulary spokendialogue interface.
In Proc.
of INTERSPEECH, pages327?330.Ed Filisko and Stephanie Seneff.
2003.
A context res-olution server for the Galaxy conversational systems.In Proc.
of EUROSPEECH.Malte Gabsdil and Oliver Lemon.
2004.
Combiningacoustic and pragmatic features to predict recognitionperformance in spoken dialogue systems.
In Proc.
ofAssociation for Computational Linguistics.Alexander Gruenstein and Stephanie Seneff.
2006.Context-sensitive language modeling for large sets ofproper nouns in multimodal dialogue systems.
InProc.
of IEEE/ACL 2006 Workshop on Spoken Lan-guage Technology.Alexander Gruenstein and Stephanie Seneff.
2007.
Re-leasing a multimodal dialogue system into the wild:User support mechanisms.
In Proc.
of the 8th SIGdialWorkshop on Discourse and Dialogue, pages 111?119.Alexander Gruenstein, Stephanie Seneff, and ChaoWang.
2006.
Scalable and portable web-basedmultimodal dialogue interaction with geographicaldatabases.
In Proc.
of INTERSPEECH.Timothy J. Hazen, Stephanie Seneff, and Joseph Po-lifroni.
2002.
Recognition confidence scoring andits use in speech understanding systems.
ComputerSpeech and Language, 16:49?67.Diane J. Litman, Julia Hirschberg, and Marc Swerts.2000.
Predicting automatic speech recognition perfor-mance using prosodic cues.
In Proc.
of NAACL, pages218 ?
225.Michael Niemann, Sarah George, and Ingrid Zukerman.2005.
Towards a probabilistic, multi-layered spokenlanguage interpretation system.
In Proc.
of 4th IJCAIWorkshop on Knowledge and Reasoning in PracticalDialogue Systems, pages 8?15.Rube?n San-Segundo, Bryan Pellom, Wayne Ward, andJose?
M. Pardo.
2000.
Confidence measures for dia-logue management in the CU Communicator System.In Proc.
of ICASSP.Stephanie Seneff.
1992.
TINA: A natural language sys-tem for spoken language applications.
ComputationalLinguistics, 18(1):61?86.Stephanie Seneff.
2002.
Response planning and gen-eration in the MERCURY flight reservation system.Computer Speech and Language, 16:283?312.Marilyn Walker, Jerry Wright, and Irene Langkilde.2000.
Using natural language processing and dis-course features to identify understanding errors in aspoken dialogue system.
In Proc.
17th InternationalConf.
on Machine Learning, pages 1111?1118.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical machine learning tools and techniques.
MorganKaufmann, San Francisco, 2nd edition.18RecognitionN-bestHypothesisRankStSaSl?DSResponseParse?thirtytwovassalstreetincambridge045.328.526.5?R0FULLthirtytwovassarstreetincambridge145.027.130.5?R1FULLthirtytwovassarstreetinincambridge244.226.030.4?R1ROBUSTatthirtytwovassarstreet<noise>340.126.529.4?R1FULLatthirtytwovassalstreet incambridge439.526.329.0?R1FULLthirtytwovassarstreetcambridge<noise>538.425.828.4?R1FULLthirtytwovassarstreetincanton638.025.828.3?R2FULLthirtytwovassalstreet inincanton733.522.527.5?R3ROBUSTtwentyvassarinstreet inzoom832.422.326.3?R4NONEthirtytwovassarstreetincambridge<noise>932.019.526.7?R1FULLResponseListResponseRankStSaSl%Top3%Top5Dist.Parse?SVMScoreR0045.328.526.5.33.85FULL?.42R1145.027.130.5.66.25FULL?.73?R1R2638.025.828.30.00.05FULL?-.32R3733.522.527.50.00.05ROBUST?-.55R4832.422.336.30.00.05NONE?-.92Figure 3: The feature extraction and classification process.
The top half of the digram shows how an N-best listof recognizer hypotheses, with associated scores from the recognizer, are processed by the dialogue system (DS) toproduce a list of responses.
Associated with each response is a set of feature values derived from the response itself,as well as the process of evoking the response (e.g.
the parse status).
The bottom half of the figure shows how theunique responses are collapsed into a list.
Each response in the list inherits the best recognition scores available fromhypotheses evoking that response; each also has feature values associated with it derived from the distribution of thatresponse on the recognizer N-best list.
Each set of feature values is classified by a Support Vector Machine, and theresulting score is used to rank the responses.
If the highest scoring response exceeds the rejection threshold, then it ischosen as the system?s response.19Feature Possible Valuesresponse typetop response typegeography, give directions, goodbye, greetings, help directions did not understand from place,help directions did not understand to place, help directions no to or from place,help directions subway, hide subway map, history cleared, list cuisine, list name, list street,no circled data, no data, no match near, non unique near, ok, panning down, panning east,panning south, panning up, panning west, presupp failure, provide city for address, refined result,reject or give help, show address, show subway map, speak properties, speak property,speak verify false, speak verify true, welcome gui, zooming, zooming in, zooming outPOI type none, city, museum, neighborhood, restaurant, subway stationparse status no parse, robust parse, full parsegeographical filter none, address, circle, line, list item, map bounds, museum, neighborhood, point, polygon, restaurant,subway station, cityTable 3: The set of possible values for non-numerical features, which are converted to sets of binary features.ResponseScoreTypeLabelR0S0speak_propertyacceptableR1S1list_cuisineunacceptableR2S2speak_propertyunacceptableCaseI:ExampleRankedResponseListCaseIR0isacceptableandisnotrejectS 0?T?T.P.S 0<T?F.N.ResponseScoreTypeLabelR0S0speak_propertyunacceptableR1S1list_cuisineunacceptableR2S2speak_propertyunacceptableR3S3rejectunacceptableR4S4zooming_outunacceptableCaseII:ExampleRankedResponseListCaseIINocandidateresponsesacceptable,oracceptableresponseisreject(a)R0isnotrejectS 0?T?F.P.S 0<T?T.N.
(b)R0isrejectS 0?T?T.N.S 0<T?T.N.ResponseScoreTypeLabelR0S0speak_propertyunacceptableR1S1list_cuisineacceptableR2S2speak_propertyunacceptableR3S3rejectunacceptableR4S4zooming_outunacceptableCaseIII:ExampleRankedResponseListCaseIIIRn(withn>0)isacceptableandisnotreject(a)R0isnotrejectS 0?T?F.P.S 0<T?F.N.
(b)R0isrejectS 0?T?F.N.S 0<T?F.N.Figure 4: Algorithm for calculating the F-measure confusion matrix of True Positives (T.P.
), False Positives (F.P.
),True Negatives (T.N.
), and False Negatives (F.N.).
The ranking technique described in this paper creates a list ofcandidate system responses ranked by their scores.
The top scoring response is then accepted if its score exceeds athreshold T, otherwise all candidate responses are rejected.
As such, the problem is not a standard binary decision.We show all possible outcomes from the ranking process, and note whether each case is counted as a T.P., F.P., T.N.,or F.N.
We note that given this algorithm for calculating the confusion matrix, no matter how we set the threshold T,F-measure will always be penalized if Case III occurs.20
