Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 60?68,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsCorrecting errors in speech recognition with articulatory dynamicsFrank RudziczUniversity of Toronto, Department of Computer ScienceToronto, Ontario, Canadafrank@cs.toronto.eduAbstractWe introduce a novel mechanism forincorporating articulatory dynamics intospeech recognition with the theory of taskdynamics.
This system reranks sentence-level hypotheses by the likelihoods oftheir hypothetical articulatory realizationswhich are derived from relationshipslearned with aligned acoustic/articulatorydata.
Experiments compare this with twobaseline systems, namely an acoustic hid-den Markov model and a dynamic Bayesnetwork augmented with discretized rep-resentations of the vocal tract.
Our sys-tem based on task dynamics reduces word-error rates significantly by 10.2% relativeto the best baseline models.1 IntroductionAlthough modern automatic speech recognition(ASR) takes several cues from the biological per-ception of speech, it rarely models its biologicalproduction.
The result is that speech is treatedas a surface acoustic phenomenon with lexical orphonetic hidden dynamics but without any phys-ical constraints in between.
This omission leadsto some untenable assumptions.
For example,speech is often treated out of convenience as a se-quence of discrete, non-overlapping packets, suchas phonemes, despite the fact that some major dif-ficulties in ASR, such as co-articulation, are bydefinition the result of concurrent physiologicalphenomena (Hardcastle and Hewlett, 1999).Many acoustic ambiguities can be resolvedwith knowledge of the vocal tract?s configuration(O?Shaughnessy, 2000).
For example, the threenasal sonorants, /m/, /n/, and /ng/, are acousti-cally similar (i.e., they have large concentrationsof energy at the same frequencies) but uniquelyand reliably involve bilabial closure, tongue-tipelevation, and tongue-dorsum elevation, respec-tively.
Having access to the articulatory goals ofthe speaker would, in theory, make the identifica-tion of linguistic intent almost trivial.
Althoughwe don?t typically have access to the vocal tractduring speech recognition, its configuration canbe estimated reasonably well from acoustics alonewithin adequate models or measurements of thevocal tract (Richmond et al, 2003; Toda et al,2008).
Evidence that such inversion takes placenaturally in humans during speech perception sug-gests that the discriminability of speech sounds de-pends powerfully on their production (Libermanand Mattingly, 1985; D?Ausilio et al, 2009).This paper describes the use of explicit modelsof physical speech production within recognitionsystems.
Initially, we augment traditional modelsof ASR with probabilistic relationships betweenacoustics and articulation learned from appropri-ate data.
This leads to the incorporation of a high-level, goal-oriented, and control-based theory ofspeech production within a novel ASR system.2 Background and related workThe use of theoretical (phonological) features ofthe vocal tract has provided some improvementover traditional acoustic ASR systems in phonemerecognition with neural networks (Kirchhoff,1999; Roweis, 1999), but there has been verylittle work in ASR informed by direct measure-ments of the vocal tract.
Recently, Markov etal.
(2006) have augmented hidden Markov modelswith Bayes networks trained to describe articula-tory constraints from a small amount of Japanesevocal tract data, resulting in a small phoneme-error reduction.
This work has since been ex-panded upon to inform ASR systems sensitive tophysiological speech disorders (Rudzicz, 2009).Common among previous efforts is an interpre-tation of speech as a sequence of short, instanta-neous observations devoid of long-term dynamics.602.1 Articulatory phonologyArticulatory phonology bridges the divide be-tween the physical manifestation of speech and itsunderlying lexical intentions.
Within this disci-pline, the theory of task dynamics is a combinedmodel of physical articulator motion and the plan-ning of abstract vocal tract configurations (Saltz-man, 1986).
This theory introduces the notion thatall observed patterns of speech are the result ofoverlapping gestures, which are abstracted goal-oriented reconfigurations of the vocal tract, suchas bilabial closure or velar opening (Saltzman andMunhall, 1989).
Each gesture occurs within oneof the following tract variables (TVs): velar open-ing (VEL), lip aperture (LA) and protrusion (LP),tongue tip constriction location (TTCL) and de-gree (TTCD) 1, tongue body constriction location(TBCL) and degree (TBCD), lower tooth height(LTH), and glottal vibration (GLO).
For example,the syllable pub consists of an onset (/p/), a nu-cleus (/ah/), and a coda (/b/).
Four gestural goalsare associated with the onset, namely the shuttingof GLO and of VEL, and the closure and release ofLA.
Similarly, the nucleus of the syllable consistsof three goals, namely the relocation of TBCD andTBCL, and the opening of GLO.
The presence andextent of these gestural goals are represented byfilled rectangles in figure 1.
Inter-gestural timingsbetween these goals are specified relative to oneanother according to human data as described byNam and Saltzman (2003).TBCD closedopenGLO openclosedLA openclosed100 200 300 400Time (ms)Figure 1: Canonical example pub from Saltzmanand Munhall (1989).The presence of these discrete goals influencesthe vocal tract dynamically and continuouslyas modelled by the following non-homogeneoussecond-order linear differential equation:Mz??+Bz?+K(z?
z?)
= 0.
(1)1Constriction locations generally refer to the front-backdimension of the vocal tract and constriction degrees gener-ally refer to the top-down dimension.Here, z is a continuous vector representing the in-stantaneous positions of the nine tract variables,z?
is the target (equilibrium) positions of thosevariables, and vectors z?
and z??
represent the firstand second derivatives of z with respect to time(i.e., velocity and acceleration), respectively.
Thematrices M, B, and K are syllable-specific coef-ficients describing the inertia, damping, and stiff-ness, respectively, of the virtual gestures.
Gener-ally, this theory assumes that the tract variables aremutually independent, and that the system is criti-cally damped (i.e., the tract variables do not oscil-late around their equilibrium positions) (Nam andSaltzman, 2003).
The continuous state, z, of equa-tion (1) is exemplified by black curves in figure 1.2.2 Articulatory dataTract variables provide the dimensions of an ab-stract gestural space independent of the physicalcharacteristics of the speaker.
In order to com-plete our articulatory model, however, we requirephysical data from which to infer these high-levelarticulatory goals.Electromagnetic articulography (EMA) is amethod to measure the motion of the vocal tractduring speech.
In EMA, the speaker is placedwithin a low-amplitude electromagnetic field pro-duced within a cube of a known geometry.
Tinysensors within this field induce small electric cur-rents whose energy allows the inference of artic-ulator positions and velocities to within 1 mm oferror (Yunusova et al, 2009).
We derive data forthe following study from two EMA sources:?
The University of Edinburgh?s MOCHAdatabase, which provides phonetically-balanced sentences repeated from TIMIT(Zue et al, 1989) uttered by a male and afemale speaker (Wrench, 1999), and?
The University of Toronto?s TORGOdatabase, from which we select sentencesrepeated from TIMIT from two femalesand three males (Rudzicz et al, 2008).
(Cerebrally palsied speech, which is thefocus of this database, is not included here).For the following study we use the eight 2D po-sitions common to both databases, namely the up-per lip (UL), lower lip (LL), upper incisor (UI),lower incisor (LI), tongue tip (TT), tongue blade(TB), and tongue dorsum (TD).
Since these po-sitions are recorded in 3D in TORGO, we project61these onto the midsagittal plane.
(Additionally, theMOCHA database provides velum (V) data on thisplane, and TORGO provides the left and right lipcorners (LL and RL) but these are excluded fromstudy except where noted).All articulatory data is aligned with its associ-ated acoustic data, which is transformed to Mel-frequency cepstral coefficients (MFCCs).
Sincethe 2D EMA system in MOCHA and the 3D EMAsystem in TORGO differ in their recording rates,the length of each MFCC frame in each databasemust differ in order to properly align acousticswith articulation in time.
Therefore, each MFCCframe covers 16 ms in the TORGO database, and32 ms in MOCHA.
Phoneme boundaries are de-termined automatically in the MOCHA databaseby forced alignment, and by a speech-languagepathologist in the TORGO database.We approximate the tract variable space fromthe physical space of the articulators, in general,through principal component analysis (PCA) onthe latter, and subsequent sigmoid normalizationon [0,1].
For example, the LTH tract variable is in-ferred by calculating the first principal componentof the two-dimensional lower incisor (LI) motionin the midsagittal plane, and by normalizing theresulting univariate data through a scaled sigmoid.The VEL variable is inferred similarly from velum(V) EMA data.
Tongue tip constriction locationand degree (TTCL and TTCD, respectively) areinferred from the 1st and 2nd principal componentsof tongue tip (TT) EMA data, with TBCL andTBCD inferred similarly from tongue body (TB)data.
Finally, the glottis (GLO) is inferred by voic-ing detection on acoustic energy below 150 Hz(O?Shaughnessy, 2000), lip aperture (LA) is thenormalized Euclidean distance between the lips,and lip protrusion (LP) is the normalized 2nd prin-cipal component of the midpoint between the lips.All PCA is performed without segmentation of thedata.
The result is a low-dimensional set of contin-uous curves describing goal-relevant articulatoryvariables.
Figure 2, for example, shows the degreeof the lip aperture (LA) over time for all instancesof the /b/ phoneme in the MOCHA database.
Therelevant articulatory goal of lip closure is evident.3 Baseline systemsWe now turn to the task of speech recognition.Traditional Bayesian learning is restricted to uni-versal or immutable relationships, and is agnos-0 50 100 150 20000.20.40.60.81Time (ms)normalizedLAFigure 2: Lip aperture (LA) over time during allMOCHA instances of /b/.tic towards dynamic systems or time-varying rela-tionships.
Dynamic Bayes networks (DBNs) aredirected acyclic graphs that generalize the power-ful stochastic mechanisms of Bayesian represen-tation to temporal sequences.
We are free to ex-plicitly provide topological (i.e., dependency) re-lationships between relevant variables in our mod-els, which can include measurements of tract data.We examine two baseline systems.
Thefirst is the standard acoustic hidden Markovmodel (HMM) augmented with a bigram languagemodel, as shown in figure 3(a).
Here, Wt ?Wt+1represents word transition probabilities, learnedby maximum likelihood estimation, and Pht ?Pht+1 represents phoneme transition probabilitieswhose order is explicitly specified by the relation-ship Wt ?
Pht .
Likewise, each phoneme Ph con-ditions the sub-phoneme state, Qt , whose transi-tion probabilities Qt ?
Qt+1 describe the dynam-ics within phonemes.
The variable Mt refers tohidden Gaussian indices so that the likelihoodsof acoustic observations, Ot , are represented by amixture of 4, 8, 16, or 32 Gaussians for each stateand each phoneme.
See Murphy (2002) for a fur-ther description of this representation.The second baseline model is the articulatorydynamic Bayes network (DBN-A).
This augmentsthe standard acoustic HMM by replacing hiddenindices, Mt , with discrete observations of the vo-cal tract, Kt , as shown in figure 3(b).
The patternof acoustics within each phoneme is dependent ona relatively restricted set of possible articulatoryconfigurations (Roweis, 1999).
To find these dis-crete positions, we obtain k vectors that best de-62scribe the articulatory data according to k-meansclustering with the sum-of-squares error function.During training, the DBN variable Kt is set ex-plicitly to the index of the mean vector nearest tothe current frame of EMA data at time t. In thisway, the relationship Kt ?
Ot allows us to learnhow discretized articulatory configurations affectacoustics.
The training of DBNs involves a spe-cialized version of expectation-maximization, asdescribed in the literature (Murphy, 2002; Ghahra-mani, 1998).
During inference, variables Wt , Pht ,and Kt become hidden and we marginalize overtheir possible values when computing their likeli-hoods.
Bigrams are computed by maximum like-lihood on lexical annotations in the training data.T BCBT BDcCBDcl Bos Bl BDcos BDce B e BDc(a) HMMTBCBTBDcCBDcl Bos Bl BDcos BDce B e BDc(b) DBN-AFigure 3: Baseline systems: (a) acoustic hiddenMarkov model and (b) articulatory dynamic Bayesnetwork.
NodeWt represents the current word, Phtis the current phoneme, Qt is that phoneme?s dy-namic state, Ot is the acoustic observation, Mt isthe Gaussian mixture component, and Kt is the dis-cretized articulatory configuration.
Filled nodesrepresent observed variables during training, al-though only Ot is observed during recognition.Square nodes are discrete variables while circularnodes are continuous variables.4 Switching Kalman filterOur first experimental system attempts speechrecognition given only articulatory data.
The truestate of the tract variables at time t?1 constitutesa 9-dimensional vector, xt?1, of continuous val-ues.
Under the task dynamics model of section2.1, the motions of these tract variables obey crit-ically damped second-order oscillatory relation-ships.
We start with the simplifying assumption oflinear dynamics here with allowances for randomGaussian process noise, vt , since articulatory be-haviour is non-deterministic.
Moreover, we knowthat EMA recordings are subject to some error(usually less than 1 mm (Yunusova et al, 2009)),so the actual observation at time t, yt , will not ingeneral be the true position of the articulators.
As-suming that the relationship between yt and xt isalso linear, and that the measurement noise, wt ,is also Gaussian, then the dynamical articulatorysystem can be described byxt = Dtxt?1 +vtyt =Ctxt +wt .(2)Eqs.
2 form the basis of the Kalman filterwhich allows us to use EMA measurements di-rectly, rather than quantized abstractions thereofas in the DBN-A model.
Obviously, since artic-ulatory dynamics vary significantly for differentgoals, we replicate eq.
(2) for each phoneme andconnect these continuous Kalman filters togetherwith discrete conditioning variables for phonemeand word, resulting in the switching Kalman fil-ter (SKF) model.
Here, parameters Dt and vt areimplicit in the relationship xt ?
xt+1, and param-eters Ct and wt are implicit in xt ?
yt .
In thismodel, observation yt is the instantaneous mea-surements derived from EMA, and xt is their truehidden states.
These parameters are trained usingexpectation-maximization, as described in the lit-erature (Murphy, 1998; Deng et al, 2005).5 Recognition with task dynamicsOur goal is to integrate task dynamics within anASR system for continuous sentences called TD-ASR.
Our approach is to re-rank an N-best list ofsentence hypotheses according to a weighted like-lihood of their articulatory realizations.
For ex-ample, if a word sequence Wi : wi,1 wi,2 ... wi,mhas likelihoods LX(Wi) and L?
(Wi) according topurely acoustic and articulatory interpretations ofan utterance, respectively, then its overall scorewould beL(Wi) = ?LX(Wi)+(1??)L?
(Wi) (3)given a weighting parameter ?
set manually, as insection 6.2.
Acoustic likelihoods LX(Wi) are ob-tained from Viterbi paths through relevant HMMsin the standard fashion.5.1 The TADA componentIn order to obtain articulatory likelihoods, L?
(Wi),for each word sequence, we first generate artic-ulatory realizations of those sequences according63to task dynamics.
To this end, we use compo-nents from the open-source TADA system (Namand Goldstein, 2006), which is a complete imple-mentation of task dynamics.
From this toolbox,we use the following components:?
A syllabic dictionary supplemented withthe International Speech Lexicon Dictionary(Hasegawa-Johnson and Fleck, 2007).
Thisbreaks word sequences Wi into syllable se-quences Si consisting of onsets, nuclei, andcoda and covers all of MOCHA and TORGO.?
A syllable-to-gesture lookup table.
Givena syllabic sequence, Si, this table providesthe gestural goals necessary to produce thosesyllables.
For example, given the syllablepub in figure 1, this table provides the tar-gets for the GLO, VEL, TBCL, and TBCDtract variables, and the parameters for thesecond-order differential equation, eq.
1,that achieves those goals.
These parametershave been empirically tuned by the authorsof TADA according to a generic, speaker-independent representation of the vocal tract(Saltzman and Munhall, 1989).?
A component that produces the continuoustract variable paths that produce an utter-ance.
This component takes into account var-ious physiological aspects of human speechproduction, including intergestural and in-terarticulator co-ordination and timing (Namand Saltzman, 2003; Goldstein and Fowler,2003), and the neutral (?schwa?)
forces of thevocal tract (Saltzman and Munhall, 1989).This component takes a sequence of gestu-ral goals predicted by the segment-to-gesturelookup table, and produces appropriate pathsfor each tract variable.The result of the TADA component is a set ofN 9-dimensional articulatory paths, TVi, neces-sary to produce the associated word sequences, Wifor i = 1..N. Since task dynamics is a prescrip-tive model and fully deterministic, TVi sequencesare the canonical or default articulatory realiza-tions of the associated sentences.
These canonicalrealizations are independent of our training data,so we transform them in order to more closely re-semble the observed articulatory behaviour in ourEMA data.
Towards this end, we train a switch-ing Kalman filter identical to that in section 4, ex-cept the hidden state variable xt is replaced by theobserved instantaneous canonical TVs predictedby TADA.
In this way we are explicitly learninga relationship between TADA?s task dynamics andhuman data.
Since the lengths of these sequencesare generally unequal, we align the articulatory be-haviour predicted by TADA with training data fromMOCHA and TORGO using standard dynamictime warping (Sakoe and Chiba, 1978).
Duringrun-time, the articulatory sequence yt most likelyto have been produced by the human data given thecanonical sequence TVi is inferred by the Viterbialgorithm through the SKF model with all othervariables hidden.
The result is a set of articulatorysequences, TV?i , for i = 1..N, that represent thepredictions of task dynamics that better resembleour data.5.2 Acoustic-articulatory inversionIn order to estimate the articulatory likelihoodof an utterance, we need to evaluate each trans-formed articulatory sequence, TV?i , within proba-bility distributions ranging over all tract variables.These distributions can be inferred using acoustic-articulatory inversion.
There are a number of ap-proaches to this task, including vector quantiza-tion, and expectation-maximization with Gaussianmixtures (Hogden and Valdez, 2001; Toda et al,2008).
These approaches accurately inferred thexy position of articulators to within 0.41 mm and2.73 mm.
Here, we modify the approach takenby Richmond et al (2003), who estimate proba-bility functions over the 2D midsagittal positionsof 7 articulators, given acoustics, with a mixture-density network (MDN).
An MDN is essentially atypical discriminative multi-layer neural networkwhose output consists of the parameters to Gaus-sian mixtures.
Here, each Gaussian mixture de-scribes a probability function over TV positionsgiven the acoustic frame at time t. For exam-ple, figure 4 shows an intensity map of the likelyvalues for tongue-tip constriction degree (TTCD)for each frame of acoustics, superimposed withthe ?true?
trajectory of that TV.
Our networks aretrained with acoustic and EMA-derived data as de-scribed in section 2.2.5.3 Recognition by rerankingDuring recognition of a test utterance, a standardacoustic HMM produces word sequence hypothe-ses, Wi, and associated likelihoods, L(Wi), for i =1..N. The expected canonical motion of the tractvariables, TVi is then produced by task dynamics64Figure 4: Example probability density of tonguetip constriction degree over time, inferred fromacoustics.
The true trajectory is superimposed as ablack curve.for each of these word sequences and transformedby an SKF to better match speaker data, givingTV?i .
The likelihoods of these paths are then eval-uated within probability distributions produced byan MDN.
The mechanism for producing the artic-ulatory likelihood is shown in figure 5.
The overalllikelihood, L(Wi) = ?LX(Wi)+ (1??)L?
(Wi), isthen used to produce a final hypothesis list for thegiven acoustic input.6 ExperimentsExperimental data is obtained from two sources,as described in section 2.2.
We procure 1200sentences from Toronto?s TORGO database, and896 from Edinburgh?s MOCHA.
In total, there are460 total unique sentence forms, 1092 total uniqueword forms, and 11065 total words uttered.
Ex-cept where noted, all experiments randomly splitthe data into 90% training and 10% testing sets for5-cross validation.
MOCHA and TORGO data arenever combined in a single training set due to dif-fering EMA recording rates.
In all cases, modelsare database-dependent (i.e., all TORGO data isconflated, as is all of MOCHA).For each of our baseline systems, we calcu-late the phoneme-error-rate (PER) and word-error-rate (WER) after training.
The phoneme-error-rate is calculated according to the proportion offrames of speech incorrectly assigned to the properphoneme.
The word-error-rate is calculated asthe sum of insertion, deletion, and substitution er-rors in the highest-ranked hypothesis divided bythe total number of words in the correct orthogra-phy.
The traditional HMM is compared by vary-ing the number of Gaussians used in the modellingSystem Parameters PER (%) WER (%)HMM|M|= 4 29.3 14.5|M|= 8 27.0 13.9|M|= 16 26.1 10.2|M|= 32 25.6 9.7DBN-A|K|= 4 26.1 13.0|K|= 8 25.2 11.3|K|= 16 24.9 9.8|K|= 32 24.8 9.4Table 1: Phoneme- and Word-Error-Rate (PERand WER) for different parameterizations of thebaseline systems.No.
of Gaussians1 2 3 4LTH ?0.28 ?0.18 ?0.15 ?0.11LA ?0.36 ?0.32 ?0.30 ?0.29LP ?0.46 ?0.44 ?0.43 ?0.43GLO ?1.48 ?1.30 ?1.29 ?1.25TTCD ?1.79 ?1.60 ?1.51 ?1.47TTCL ?1.81 ?1.62 ?1.53 ?1.49TBCD ?0.88 ?0.79 ?0.75 ?0.72TDCL ?0.22 ?0.20 ?0.18 ?0.17Table 2: Average log likelihood of true tract vari-able positions in test data, under distributions pro-duced by mixture density networks with varyingnumbers of Gaussians.of acoustic observations.
Similarly, the DBN-Amodel is compared by varying the number of dis-crete quantizations of articulatory configurations,as described in section 3.
Results are obtained bydirect decoding.
The average results across bothdatabases, between which there are no significantdifferences, are shown in table 1.
In all casesthe DBN-A model outperforms the HMM, whichhighlights the benefit of explicitly conditioningacoustic observations on articulatory causes.6.1 Efficacy of TD-ASR componentsIn order to evaluate the whole system, we start byevaluating its parts.
First, we test how accuratelythe mixture-density network (MDN) estimates theposition of the articulators given only informationfrom the acoustics available during recognition.Table 2 shows the average log likelihood over eachtract variable across both databases.
These re-sults are consistent with the state-of-the-art (Todaet al, 2008).
In the following experiments, we useMDNs that produce 4 Gaussians.65T TBCDclosClBedBedpnGpnGL 1L 2OOOL NGA10lo234Do20l0l?BnB?BnB ??
1??
2OOO??
N??
??sC?????CoT??
?s?1?l?dBGe?dBGe ???
1??
?
2OOO???
NpD??s0????CoT??
?s?1?l ????
i?
L?
1L?
2OOOL?
Nd0???0?
?loFigure 5: The TD-ASR mechanism for deriving articulatory likelihoods, L?
(Wi), for each word sequenceWi produced by standard acoustic techniques.Manner Canonical Transformedapproximant 0.19 0.16fricative 0.37 0.29nasal* 0.24 0.18retroflex 0.23 0.19plosive 0.10 0.08vowel 0.27 0.25Table 3: Average difference between predictedtract variables and observed data, on [0,1] scale.
(*) Nasals are evaluated only with MOCHA data,since TORGO data lacks velum measurements.We evaluate how closely transformations to thecanonical tract variables predicted by TADA matchthe data.
Namely, we input the known orthographyfor each test utterance into TADA, obtain the pre-dicted canonical tract variables TV, and transformthese according to our trained SKF.
The resultingpredicted and transformed sequences are alignedwith our measurements derived from EMA withdynamic time warping.
Finally, we measure theaverage difference between the observed data andthe predicted (canonical and transformed) tractvariables.
Table 3 shows these differences accord-ing to the phonological manner of articulation.
Inall cases the transformed tract variable motion ismore accurate, and significantly so at the 95% con-fidence level for nasal and retroflex phonemes, andat 99% for fricatives.
The practical utility of thetransformation component is evaluated in its effecton recognition rates, as described below.6.2 Recognition with TD-ASRWith the performance of the components of TD-ASR better understood, we combine these andstudy the resulting composite TD-ASR system.0 0.2 0.4 0.6 0.8 188.599.510?WER (%)TORGOMOCHAFigure 6: Word-error-rate according to varying ?,for both TORGO and MOCHA data.Figure 6 shows the WER as a function of ?
withTD-ASR and N = 4 hypotheses per utterance.
Theeffect of ?
is clearly non-monotonic, with articula-tory information clearly proving useful.
Althoughsystems whose rankings are weighted solely by thearticulatory component perform better than the ex-clusively acoustic systems, the lists available to theformer are procured from standard acoustic ASR.Interestingly, the gap between systems trained tothe two databases increases as ?
approaches 1.0.Although this gap is not significant, it may be theresult of increased inter-speaker articulatory varia-tion in the TORGO database, which includes morethan twice as many speakers as MOCHA.Figure 7 shows the WER obtained with TD-ASR given varying-length N-best lists and ?
=0.7.
TD-ASR accuracy at N = 4 is significantlybetter than both TD-ASR at N = 2 and the base-line approaches of table 1 at the 95% confidencelevel.
However, for N > 4 there is a noticeableand systematic worsening of performance.662 3 4 5 6 7 88.28.48.68.899.29.49.69.8Length of N?best listWER (%)TORGOMOCHAFigure 7: Word-error-rate according to vary-ing lengths of N-best hypotheses used, for bothTORGO and MOCHA data.The optimal parameterization of the TD-ASRmodel results in an average word-error-rate of8.43%, which represents a 10.3% relative error re-duction over the best parameterization of our base-line models.
The SKF model of section 4 differsfrom the HMM and DBN-A baseline models onlyin its use of continuous (rather than discrete) hid-den dynamics and in its articulatory observations.However, its performance is far more variable, andless conclusive.
On the MOCHA database theSKF model had an average of 9.54% WER witha standard deviation of 0.73 over 5 trials, and anaverage of 9.04% WER with a standard deviationof 0.64 over 5 trials on the TORGO database.
De-spite the presupposed utility of direct articulatoryobservations, the SKF system does not performsignificantly better than the best DBN-A model.Finally, the experiments of tables 6 and 7 arerepeated with the canonical tract variables passeduntransformed to the probability maps generatedby the MDNs.
Predictably, resulting articulatorylikelihoods L?
are less representative and increas-ing their contribution ?
to the hypothesis rerank-ing does not improve TD-ASR performance sig-nificantly, and in some instances worsens it.
Al-though TADA is a useful prescriptive model ofgeneric articulation, its use must be tempered withknowledge of inter-speaker variability.7 Discussion and conclusionsThe articulatory medium of speech rarely informsmodern speech recognition.
We have demon-strated that the use of direct articulatory knowl-edge can substantially reduce phoneme and worderrors in speech recognition, especially if thatknowledge is motivated by high-level abstrac-tions of vocal tract behaviour.
Task dynamic the-ory provides a coherent and biologically plausiblemodel of speech production with consequences forphonology (Browman and Goldstein, 1986), neu-rolinguistics (Guenther and Perkell, 2004), and theevolution of speech and language (Goldstein et al,2006).
We have shown that it is also useful withinspeech recognition.We have overcome a conceptual impediment inintegrating task dynamics and ASR, which is theformer?s deterministic nature.
This integration isaccomplished by stochastically transforming pre-dicted articulatory dynamics and by calculatingthe likelihoods of these dynamics according tospeaker data.
However, there are several new av-enues for exploration.
For example, task dynamicslends itself to more general applications of con-trol theory, including automated self-correction,rhythm, co-ordination, and segmentation (Fried-land, 2005).
Other high-level questions also re-main, such as whether discrete gestures are thecorrect biological and practical paradigm, whethera purely continuous representation would be moreappropriate, and whether this approach general-izes to other languages.In general, our experiments have revealed verylittle difference between the use of MOCHA andTORGO EMA data.
An ad hoc analysis of someof the errors produced by the TD-ASR systemfound no particular difference between how sys-tems trained to each of these databases recognizednasal phonemes, although only those trained withMOCHA considered velum motion.
Other errorscommon to both sources of data include phonemeinsertion errors, normally vowels, which appear toco-occur with some spurious motion of the tonguebetween segments, especially for longer N-bestlists.
Despite the relative slow motion of the ar-ticulators relative to acoustics, there remains someintermittent noise.As more articulatory data becomes availableand as theories of speech production become morerefined, we expect that their combined value tospeech recognition will become indispensable.AcknowledgmentsThis research is funded by the Natural Sciencesand Engineering Research Council and the Uni-versity of Toronto.67ReferencesCatherine P. Browman and Louis M. Goldstein.
1986.
To-wards an articulatory phonology.
Phonology Yearbook,3:219?252.Alessandro D?Ausilio, Friedemann Pulvermuller, PaolaSalmas, Ilaria Bufalari, Chiara Begliomini, and LucianoFadiga.
2009.
The motor somatotopy of speech percep-tion.
Current Biology, 19(5):381?385, February.Jianping Deng, M. Bouchard, and Tet Yeap.
2005.
SpeechEnhancement Using a Switching Kalman Filter with a Per-ceptual Post-Filter.
In Acoustics, Speech, and Signal Pro-cessing, 2005.
Proceedings.
(ICASSP ?05).
IEEE Interna-tional Conference on, volume 1, pages 1121?1124, 18-23,.Bernard Friedland.
2005.
Control System Design: An Intro-duction to State-Space Methods.
Dover.Zoubin Ghahramani.
1998.
Learning dynamic Bayesian net-works.
In Adaptive Processing of Sequences and DataStructures, pages 168?197.
Springer-Verlag.Louis M. Goldstein and Carol Fowler.
2003.
Articulatoryphonology: a phonology for public language use.
Phonet-ics and Phonology in Language Comprehension and Pro-duction: Differences and Similarities.Louis Goldstein, Dani Byrd, and Elliot Saltzman.
2006.
Therole of vocal tract gestural action units in understandingthe evolution of phonology.
In M.A.
Arib, editor, Actionto Language via the Mirror Neuron System, pages 215?249.
Cambridge University Press, Cambridge, UK.Frank H. Guenther and Joseph S. Perkell.
2004.
A neu-ral model of speech production and its application tostudies of the role of auditory feedback in speech.
InBen Maassen, Raymond Kent, Herman Peters, Pascal VanLieshout, and Wouter Hulstijn, editors, Speech MotorControl in Normal and Disordered Speech, chapter 4,pages 29?49.
Oxford University Press, Oxford.William J. Hardcastle and Nigel Hewlett, editors.
1999.Coarticulation ?
Theory, Data, and Techniques.
Cam-bridge University Press.Mark Hasegawa-Johnson and Margaret Fleck.
2007.
Inter-national Speech Lexicon Project.John Hogden and Patrick Valdez.
2001.
A stochasticarticulatory-to-acoustic mapping as a basis for speechrecognition.
In Proceedings of the 18th IEEE Instrumen-tation and Measurement Technology Conference, 2001.IMTC 2001, volume 2, pages 1105?1110 vol.2.Katrin Kirchhoff.
1999.
Robust Speech Recognition Us-ing Articulatory Information.
Ph.D. thesis, University ofBielefeld, Germany, July.Alvin M. Liberman and Ignatius G. Mattingly.
1985.
Themotor theory of speech perception revised.
Cognition,21:1?36.Konstantin Markov, Jianwu Dang, and Satoshi Nakamura.2006.
Integration of articulatory and spectrum featuresbased on the hybrid HMM/BN modeling framework.Speech Communication, 48(2):161?175, February.Kevin Patrick Murphy.
1998.
Switching Kalman Filters.Technical report.Kevin Patrick Murphy.
2002.
Dynamic Bayesian Networks:Representation, Inference and Learning.
Ph.D. thesis,University of California at Berkeley.Hosung Nam and Louis Goldstein.
2006.
TADA (TAsk Dy-namics Application) manual.Hosung Nam and Elliot Saltzman.
2003.
A competitive, cou-pled oscillator model of syllable structure.
In Proceedingsof the 15th International Congress of Phonetic Sciences(ICPhS 2003), pages 2253?2256, Barcelona, Spain.Douglas O?Shaughnessy.
2000.
Speech Communications ?Human and Machine.
IEEE Press, New York, NY, USA.Korin Richmond, Simon King, and Paul Taylor.
2003.Modelling the uncertainty in recovering articulation fromacoustics.
Computer Speech and Language, 17:153?172.Sam T. Roweis.
1999.
Data Driven Production Models forSpeech Processing.
Ph.D. thesis, California Institute ofTechnology, Pasadena, California.Frank Rudzicz, Pascal van Lieshout, Graeme Hirst, GeraldPenn, Fraser Shein, and Talya Wolff.
2008.
Towards acomparative database of dysarthric articulation.
In Pro-ceedings of the eighth International Seminar on SpeechProduction (ISSP?08), Strasbourg France, December.Frank Rudzicz.
2009.
Applying discretized articulatoryknowledge to dysarthric speech.
In Proceedings ofthe 2009 IEEE International Conference on Acoustics,Speech, and Signal Processing (ICASSP09), Taipei, Tai-wan, April.Hiroaki Sakoe and Seibi Chiba.
1978.
Dynamic program-ming algorithm optimization for spoken word recognition.IEEE Transactions on Acoustics, Speech, and Signal Pro-cessing, ASSP-26, February.Elliot L. Saltzman and Kevin G. Munhall.
1989.
A dynam-ical approach to gestural patterning in speech production.Ecological Psychology, 1(4):333?382.Elliot M. Saltzman, 1986.
Task dynamic co-ordination of thespeech articulators: a preliminary model, pages 129?144.Springer-Verlag.Tomoki Toda, Alan W. Black, and Keiichi Tokuda.
2008.Statistical mapping between articulatory movements andacoustic spectrum using a Gaussian mixture model.Speech Communication, 50(3):215?227, March.Alan Wrench.
1999.
The MOCHA-TIMIT articulatorydatabase, November.Yana Yunusova, Jordan R. Green, and Antje Mefferd.
2009.Accuracy Assessment for AG500, Electromagnetic Artic-ulograph.
Journal of Speech, Language, and Hearing Re-search, 52:547?555, April.Victor Zue, Stephanie Seneff, and James Glass.
1989.Speech Database Development: TIMIT and Beyond.
InProceedings of ESCA Tutorial and Research Workshop onSpeech Input/Output Assessment and Speech Databases(SIOA-1989), volume 2, pages 35?40, Noordwijkerhout,The Netherlands.68
