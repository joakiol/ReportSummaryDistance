Proceedings of EACL '99Pars ing  w i th  an Extended Domain  o f  Loca l i tyJohn Carroll, Nicolas Nicolov, Olga Shaumyan, Martine Smets g?
David WeirSchool of Cognitive and Computing SciencesUniversity of SussexBrighton, BN1 9QH, UKAbstractOne of the claimed benefits of Tree Ad-joining Grammars is that they have anextended omain of locality (EDOL).
Weconsider how this can be exploited tolimit the need for feature structure uni-fication during parsing.
We comparetwo wide-coverage l xicalized grammarsof English, LEXSYS and XTAG, findingthat the two grammars exploit EDOL indifferent ways.1 IntroductionOne of the most basic properties of Tree AdjoiningGrammars (TAGS) is that they have an extendeddomain  of  local i ty (EDOL) (Joshi, 1994).
Thisrefers to the fact that the elementary trees thatmake up the grammar are larger than the cor-responding units (the productions) that are usedin phrase-structure ule-based frameworks.
Theclaim is that in Lexicalized TAGS (LTAGs) the el-ementary trees provide a domain of locality largeenough to state co-occurrence relationships be-tween a lexical item (the anchor  of the elemen-tary tree) and the nodes it imposes constraintson.
We will call this the extended domain  oflocal i ty hypothes is .For example, wh-movement can be expressedlocally in a tree that will be anchored by a verbof which an argument is extracted.
Consequently,features which are shared by the extraction siteand the wh-word, such as case, do not need to bepercolated, but are directly identified in the tree.Figure 1 shows a tree in which the case featureat the extraction site and the wh-word share thesame value)1The anchor, substitution and foot nodes of treesare marked with the symbols o, $ and *, respectively.Words in parenthesis are included in trees to provideexamples of strings this tree can derive.Much of the research on TAGS can be seen asillustrating how its EDOL can be exploited in vari-ous ways.
However, to date, only indirect evidencehas been given regarding the beneficial effects ofthe EDOL on parsing efficiency.
The argument,due to Schabes (1990), is that benefits to parsingarise from lexicalization, and that lexicalization isonly possible because of the EDOL.
A parser deal-ing with a lexicalized grammar needs to consideronly those elementary structures that can be as-sociated with the lexical items appearing in theinput.
This can substantially reduce the effectivegrammar size at parse time.
The argument hatan EDOL is required for lexicalization is based onthe observation that not every set of trees thatcan be generated by a CFG can be generated bya lexicalized CFG.
But does the EDOL have anyother more direct effects on parsing efficiency?On the one hand, it is a consequence of theEDOL that wide-coverage LTAGs are larger thantheir rule-based counterparts.
With larger ele-mentary structures, generalizations are lost re-garding the internal structure of the elementarytrees.
Since parse time depends on grammar size,this could have an adverse effect on parsing effi-ciency.
However, the problem of grammar size inTAG has to some extent been addressed both withrespect o grammar encoding (Evans et al, 1995;Candito, 1996) and parsing (Joshi and Srinivas,1994; Evans and Weir, 1998).On the other hand, if the EDOL hypothesis holdsfor those dependencies that are being checked bythe parser, then the burden of passing feature val-ues around during parsing will be less than in arule-based framework.
If all dependencies thatthe parser is checking can be stated directly withinthe elementary structures of the grammar, theydo not need to be computed ynamically duringthe parsing process by means of feature percola-tion.
For example, there is no need to use a slashfeature to establish filler-gap dependencies overunbounded istances across the tree if the EDOL217Proceedings of EACL '99S$ NP\[?~ :,?
?\] S(whom)SNP VPeFigure 1: Localizing a filler-gap dependencymakes it possible for the gap and its filler to belocated within the same elementary structure.This paper presents an investigation i to the ex-tent to which the EDOL reduces the need for fea-ture passing in two existing wide-coverage ram-mars: the XTAG grammar (XTAG-Group, 1995),and the LEXSYS grammar (Carroll et al, 1998).It can be seen as an evaluation of how well thesetwo grammars make use of the EDOL hypothesiswith respect to those dependencies that are beingchecked by the parser.2 Parsing Unification-BasedGrammarsIn phrase-structure rule-based parsing, each rulecorresponds to a local tree.
A rule is applied toa sequence of existing contiguous constituents, ifthey are compatible with the daughters.
In thecase of context-free grammar (CFG), the compat-ibility check is just equality of atomic symbols,and an instantiated daughter is merely the corre-sponding sub-constituent.However, unification-based extensions ofphrase-structures grammars are used becausethey are able to encode local and non-localsyntactic dependencies (for example, subject-verb agreement in English) with re-entrantfeatures and feature percolation, respectively:Constituents are represented by DAGS (directedacyclic graphs), the compatibility check is unifi-cation, and it is the result of each unification thatis used to instantiate the daughters.
Graph uni-fication based on the UNION-FIND algorithm hastime complexity that is near-linear in the numberof feature structure nodes in the inputs (Huet,1975; Ait-Kaci, 1984); however, feature structuresin wide-coverage grammars can contain hundredsof nodes (see e.g., HPSG (Pollard and Sag, 1994)),and since unification is a primitive operation theoverall number of unification attempts duringparsing can be very large.
Unification thereforehas a substantial practical cost.Efficient graph unification must also ensure thatit does not destructively modify the input struc-tures, since the same rule may be used severaltimes within a single derivation, and also thesame constituent may be used within differentpartial analyses with features instantiated in dif-ferent ways.
Copying input feature structuresin their entirety before each unification wouldsolve this problem, but the space usage rendersthis approach impractical.
Therefore, various'quasi-destructive' algorithms (Karttunen, 1986;Kogure, 1990; Tomabechi, 1991) and algorithmsusing 'skeletal' DACS with updates (Pereira, 1985;Emele, 1991) have been proposed, which at-tempt to minimize copying.
But even with goodimplementations of the best of these improvedalgorithms, parsers designed for wide-coverageunification-based phrase-structure grammars us-ing large HPSG-style feature graphs pend around85-90% of their time unifying and copying featurestructures (Tomabechi, 1991), and may allocate inthe region of 1-2 Mbytes memory while parsingsentences of only eight words or so (Flickinger,p.c.).
Although comfortably within main mem-ory capacities of current workstations, uch largeamounts of short-term storage allocation overflowCPU caches, and storage management overheadsbecome significant.In the case of unification-based LTAG the situa-tion is even more problematic.
Elementary struc-tures are larger than productions, and the poten-tial is that the parser will have to make copiesof entire trees and associated feature structures.Furthermore, the number of trees that an LTAG218Proceedings of EACL '99S S$ NP\[.g~ : Eli VP SNP\[ w:3~g l VP Lcase : nomjoV\[~g,:m\] SNP oV\[.g,: 3~g\] SNPIlovesFigure 2: Unanchored and anchored trees localizing subject/verb agreementparser must consider tends to be far larger thanthe number of rules in a corresponding phrase-structure grammar.
On the other hand, the EDOLhas the potential to eliminate some or all of featurepercolation, and in the remainder of this section,we explain how.An LTAG consists of a set of unanchored treessuch as the one shown on the left of Figure 2.This shows a tree for transitive verbs where sub-ject/verb agreement is captured directly with re-entrancy between the value of agr feature struc-tures at the anchor (verb) node and the subjectnode.
Notice the re-entrancy between the anchornode and the substitution ode for the subject.However these are not the trees that the parserworks with; the parser is given trees that havebeen anchored by the morphologically analysedwords in the input sentence.
For example, the treeshown on the right is the result of anchoring thetree shown on the left with the word loves.
An-choring instantiates the agr feature of the anchornode as 3sg which has the effect (due to the re-entrancy in the unanchored tree) of instantiatingthe agr feature at the subject node as 3sg.Anchored elementary trees are translated by theparser into a sequence of what we will refer toas parser  act ions.
For example, once the treeshown on the right of Figure 2 has been asso-ciated with the word loves in the input, it canbe recognized with a sequence of parser actionsthat involve finding a NP constituent on the right(corresponding to the object), possibly perform-ing adjunctions at the VP node, and then findinganother NP constituent on the left (correspondingto the subject).
We say that the two NP substitu-tion nodes and the VP node are the sites of parseractions in this tree.
Problems arise, and the EDOLhypothesis i violated, when there is a dependencebetween different parser actions.The EDOL hypothesis tates that elementarytrees provide a domain of locality large enough tostate co-occurrence relationship between the an-chor of the tree and the nodes it imposes con-straints on.
If all dependencies relevant to theparser can be captured in this way then, once anelementary tree has been anchored by a particu-lar lexical item, the settings of feature values atall of the dependent nodes will have been fixed,and no feature percolation can occur.
Each uni-fication is a purely local operation with no reper-cussions on the rest of the parsing process.
Nocopying of feature structures is required, so mem-ory usage is greatly reduced, and complex quasi-destructive algorithms with their associated com-putational overheads can be dispensed with.Note that, although feature percolation is elim-inated when the EDOL hypothesis holds, the fea-ture structure at a node can still change.
For ex-ample, substituting a tree for a proper noun atthe subject position of the tree in Figure 2 wouldcause the .feature structure at the node for thesubject to'include pn:+.
This, however, does notviolate the EDOL hypothesis ince this feature isnot coreferenced with any other feature in the tree.3 Analysis of two wide-coveragegrammarsAs we have seen, the EDOL of LTAGs makes it pos-sible, at least in principle, to locally express de-pendencies which cannot be localized in a CFG-based formalism.
In this section we consider twoexisting grammars: the XTAG grammar, a wide-coverage LTAG, and the LEXSYS grammar, a wide-coverage D-Tree Substitution Grammar (Rambowet al, 1995).
For each grammar we investigate theextend to which they do not take full advantageof the EDOL and require percolation of features atparse time.There are a number of instances in which depen-dencies are not localized in the XTAG grammar,most of which involve auxiliary trees.
There are219Proceedings of EACL '99three types of auxiliary trees: predicative, modi-fier and coordination auxiliary trees.
In predica-tive auxiliary trees the anchor is also the head ofthe tree and becomes the head of the tree resultingfrom the adjunction.
In modifier auxiliary trees,the anchor is not the head of the tree, and the sub-tree headed by the anchor usually plays a role ofadjunct in the resulting tree.
Coordination auxil-iary trees are similar to modifier auxiliary trees inthat they are anchored by the conjunction whichis not the head of the phrase.
One of the conjoinednodes is a foot node, the other one a substitutionnode.3.1 Modifier Auxiliary TreesIn modifier auxiliary trees - -  an example of whichis shown in Figure 32 - -  the feature values at theroot and foot nodes are set by the node at whichthe auxiliary tree is adjoined, and have to be per-colated between the foot node and the root node.The LEXSYS grammar adopts a similar account ofmodification.From a parsing point of view, this does not re-sult in the need for feature percolation: only thefoot node of the modifier tree is the site of a parseraction, and the root node is ignored by the processthat interprets the tree for the parser.3.2 Coordination Auxiliary TreesAn example of an XTAG coordination auxiliarytree is shown on the left of Figure 4.
This case isdifferent from the modification case since featuresof the substitution ode have to be identical to fea-tures of the foot node (which wiIl match those atthe adjunction site).
From a parsing point of viewthese nodes are both the sites of actions, resultingin the need for feature percolation.
For example,for the NP coordination tree shown in Figure 4,if one of the conjuncts is a wh-phrase, the otherconjunct must be a wh-phrase too, as in who orwhat did this?
but *John and who did this?
Thewh-feature has to be percolated between the twonodes on each side of the conjunction.In the LEXSYS grammar, a coordination tree isanchored by a head of the tree, not by the con-junction.
To illustrate (see the tree on the rightof Figure 4), N P-coordination trees are anchoredby a noun, and features uch as wh and case areground during anchoring.
As a result, there is noneed for passing of these features in the coordina-tion trees of the LEXSYS grammar.2All examples relating to the XTAG grammar comefrom the XTAG report (XTAG-Group, 1995).
Theyhave been simplified to the extent that only detailsrelevant o the discussion are included.As for agreement features, there are two casesto consider: if the conjunction is and, the numberfeature of the whole phrase is plural; if the con-junction is or, the number feature is the same asthe last conjunct's (XTAG-Group, 1999).
In boththe XTAG and LEXSYS grammars, this is achievedby having separate trees for each type of conjunc-tion.3.3 Predicative Auxil iary TreesIn the XTAC grammar, subject raising and aux-iliary verbs anchor auxiliary trees rooted in VP,without a subject3; they can be adjoined at theVP node of any compatible verb tree.
With thisarrangement, subject-verb agreement must be es-tablished dynamically.
The agr feature of the NPsubject must match the agr feature of whicheverVP ends up being highest at the end of the deriva-tion.
In Figure 5, the bought tree has been an-chored in such a way that adjunction at the VPnode is obligatory, since a matrix clause cannothave mode:ppart.
4 When the tree for has is ad-joined at the VP node the agr features of the sub-ject will agree with those of bought.
The featurestructure at the root of the tree for has is unifiedwith the upper feature structure at the VP nodeof the tree for bought, and the feature structureat the foot of the tree for has is unified with thelower feature structure at the VP node of the treefor bought.
The foot node of the has tree is the VPnode on the frontier of the tree.
Note that evenafter the tree has been anchored, re-entrancy offeatures occurs in the tree.Thus, there are two sites in the tree for bought(the subject NP node and the VP node) at whichparser actions will take place (substitution andadjunction, respectively) such that a dependencybetween the values of the features at these twonodes must be established by the parser.The situation is similar for case assignment(also shown in the Figure 5): the value of a fea-ture ass-case (the assign case feature) on the high-est VP is coreferred with the value of the featurecase on the subject NP.
For finite verbs, the valueof the feature ass-case is determined by the modeof the verb.
For infinitive verbs, case is assignedin various ways, the details of which are not rele-vant to the discussion here.
The subject is in thenominative case if the verb is finite, and in theaccusative otherwise.
As with the agr feature, thevalue of the case feature cannot be instantiated3To allow for long distance dependency, subjectraising verbs must anchor an auxiliary tree, with iden-tical root and foot nodes, a VP.4Unifying the two feature structures at the VP nodewould cause a matrix clause to have mode:ppart.220Proceedings of EACL '99Nmo Adj ?
NI~IredFigure 3: XTAG example of modifying auxiliary treef* NP\[=~ m~\] oConj .
NP\[ Wh:IandNp\[ ~h:- 1s N p,t.
Conj NPoNF h: Lcase .'
--nora/ace\]applesFigure 4: Coordination in XTAG (left) and LEXSYS (right)in the anchored elementary tree of the main verbbecause auxiliary verb trees can be adjoined.The same observations apply to the XTAG treat-ment of copula with predicative categories such asan adjective.
As shown in Figure 6, these pred-icative AP trees have a subject but no verb; treesfor raising verbs or the copula can be adjoinedinto them.
As in the previous example, the agrfeatures of the verb and subject cannot be instan-tiated in the elementary tree because the verb andits subject are not present in the same tree.From the examples we have seen, it appears thatthe XTAG grammar does not take full advantageof the EDOL with respect o a number of syntacticfeatures, for example those relating to agreementand case.
The LEXSYS grammar takes a rather dif-ferent approach to phenomena that XTAG handleswith predicative auxiliary trees.The LEXSYS grammar has been designed to lo-calize syntactic dependencies in elementary trees.As in the XTAC grammar, unbounded ependen-cies between gap and filler are localized in elemen-tary trees; but unlike the XTAG grammar, othertypes of syntactic dependencies, uch as agree-ment, are also localized.
All finite verbs, includingauxiliary and raising verbs, anchor a tree rootedin S, and thus are in the same tree as the subjectwith which they agree.
An example involving fi-nite verbs is shown in Figure 7.
Since verb treescannot be substituted between the subject and theverb, the agr feature can be grounded when ele-mentary trees are anchored, rather than duringthe derivation.
The case feature of the subjectcan be specified even in the unanchored elemen-tary tree: in trees for finite verbs the subject hasnominative case; in trees for for  .
.
.
to clauses ithas accusative case.As can be seen from the tree on the right ofFigure 7, subject raising and auxiliary verbs arerooted in S and take a VP complement.
So thesentence He seems to like apples is produced bysubstituting a VP-rooted tree for to like into a treefor seems.Thus, for all three trees shown in Figure 7,once anchoring has taken place, all of the syn-tactic features being checked by the parser aregrounded.
Hence, the parser does not have tocheck for dependencies between the parser actionstaking place at different sites in the tree.3.4 Semantic DependenciesThere are many examples where the ?TAG gram-mar, but not the LEXSYS grammar, localizes se-mantic dependencies: for example, dependencies221Proceedings of EACL '99S\[modo: \]\]$ NP \[:g:o::\[~\] vP L:o%: T?
mJ~.~~rnode : ppart\]Iboughtfagr : 3sg "1.ore /L : o:Th :omj., oo dhasFigure 5: XTAG example with a raising verbS \[mode : I~1\], I .NP - VP  \ [m?~oV\[mooo:,o \] ,vP I II e o Advis IupsetFigure 6: XTAG example of a predicative adjectivebetween an adjective and its subject.
As shown inFigure 6, in XTAG the predicative adjective and itssubject are localized in the same elementary tree,and selectional restrictions can be locally imposedby the adjective on the subject without the needfor feature percolation.
On the other hand, in theLEXSYS grammar, the dependency between upsetand he in he looks upset could not be checked ur-ing parsing without he use of feature passing be-tween the subject and AP node of the tree in themiddle of Figure 7.3.5 Perco la t ion  o f  Features  in LEXSYSThis section considers a limited number of caseswhere it appears that it is not possible to setall syntactic features by anchoring an elementarytree.When two nodes other than the anchor of thetree are syntactically dependent, feature valuesmay have to be percolated between these nodes(the anchor does not determine the value of thesefeatures).
For example, in English adjectives thatcan have S subjects determine the verb form of thesubject.
Hence, in Figure 8, the verb form featureof the subject is not determined by the anchorof the tree (the verb) but by the complement ofthe anchor (the adjective).
The verb form featuremust therefore be percolated from the adjectivephrase to the subject.The XTAG grammar localizes this dependency(see Figure 6).
However, as we have seen, agree-ment features are not localized in this analysis.The problem then is that it does not seem to bepossible to localize all syntactic features in thiscase .Feature percolation is also required in theLEXSYS grammar for prepositional phrases whichcontain a wh-word, because the value of the whfeature is not set by the anchor of the phrase (thepreposition) but by the complement (as in thesereports, the wording on the covers of which hascaused so much controversy, are to be destroyed5 ).The value of the feature wh is set by the N P-complement, and percolated to the root of the PP.4 Conc lus ionsIn XTAG both syntactic and semantic features areconsidered uring parsing, whereas in the LEXSYS5Example borrowed from Gazdar et al 1985.222Proceedings of EACL '99S S$ NP \[~": 3,, \] VP  $ NP  \[~e: 3'~ml VP Lcase : nomjVi  ''g' : 3'g \] SNP o V L,.,o,~ : i,,,~j o \['g': 3,g l ,L AP  Lmocle : indJ IIlikes looksSSNP\[:~<>V\[~g, : 3,g \] Lmode : indj .,~ VP  \[mode : inf\]seemsFigure 7: LEXSYS example for case and agr featuresSSS\[mooV ragr : 3sg \] $ J~P \[subj : mode : \[~\]\] j Lmode : indJlooksFigure 8: LEXSYS example for subject/adjective syntactic dependencysystem only syntactic dependencies are consideredduring parsing; semantic dependencies are leftfor a later processing phase.
The LEXSYS parserreturns a complete set of all syntactically well-formed derivations.
Semantic information canbe recovered from derivation trees and then pro-cessed as desired.From a processing point of view, the XTAG andLEXSYS grammars are examples that show thatthe checking of dependencies involves a trade-off.
6On the one hand, a greater number of parses maybe returned if the only dependencies checked aresyntactic, since possible violations of semantic de-pendencies are ignored.
On the other hand, aswe have seen in this paper, there are potentiallysubstantial benefits to parsing efficiency if all de-pendencies that the parser is checking can be lo-calized with the EDOL.
It is tOO early to say howbest to make the trade-off, but by comparing theway that the XTAG and LEXSYS grammars exploitthe EDOL, we hope to have shed some light onthe role that the EDOL can play with respect oparsing efficiency.6These are both grammars for English.
Hence,whether the conclusions we draw apply to other lan-guages is outside the scope of the present work.5 AcknowledgementsThis work is supported by UK EPSR.C projectGR/K97400 and by an EPSRC Advanced Fellow-ship to the first author.
We would like to thankRoger Evans, Gerald Gazdar & K. Vijay-Shankerfor helpful discussions.ReferencesHassan Ait-Kaci.
1984.
A Lattice Theoretic Ap-proach to Computation Based on a Calculus ofPartially Ordered Type Structures.
Ph.D. the-sis, Department of Computer and InformationScience, University of Pennsylvania, Philadel-phia, PA.Marie-H~l~ne Candito.
1996.
A principle-based hierarchical representation of LTAGs.In Proceedings of the 16th International Con-ference on Computational Linguistics, Copen-hagen, Denmark, August.John Carroll, Nicolas Nicolov, Olga Shaumyan,Martine Smets, and David Weir.
1998.
TheLEXSYS Project.
In Proceedings of theFourth International Workshop on Tree Adjoin-ing Grammars and Related Frameworks, pages29-33.223Proceedings of EACL '99Martin Emele.
1991.
Unification with lazy non-redundant copying.
In Proceedings of the 29thMeeting of the Association for ComputationalLinguistics, pages 323-330, Berkeley, CA.Roger Evans and David Weir.
1998.
A structure-sharing parser for lexicalized grammars.
In Pro-ceedings of the 36th Meeting of the Associationfor Computational Linguistics and the 17th In-ternational Conference on Computational Lin-guistics, pages 372-378.Roger Evans, Gerald Gazdar, and David Weir.1995.
Encoding lexicalized Tree AdjoiningGrammars with a nonmonotonic nheritance hi-erarchy.
In Proceedings of the 33rd Meeting ofthe Association for Computational Linguistics,pages 77-84.G.
P. Huet.
1975.typed A-calculus.ence, 1:27-57.A unification algorithm forTheoretical Computer Sci-Aravind Joshi and B. Srinivas.
1994.
Disambigua-tion of super parts of speech (or supertags): Al-most parsing.
In Proceedings of the 15th Inter-national Conference on Computational Linguis-tics, pages 154-160.Aravind Joshi.
1994.
Preface to special issue onTree-Adjoining Grammars.
Computational In-telligence, 10(4):vii-xv.Lauri Karttunen.
1986.
D-PATR: A developmentenvironment for unification-based grammars.In Proceedings of the 11th International Confer-ence on Computational Linguistics, pages 74-80, Bonn, Germany.Kiyoshi Kogure.
1990.
Strategic lazy incremen-tal copy graph unification.
In Proceedings ofthe 13th International Conference on Compu-tational Linguistics, pages 223-228, Helsinki.Fernando Pereira.
1985.
A structure-sharing rep-resentation for unification-based grammar for-malisms.
In Proceedings of the 23rd Meeting ofthe Association for Computational Linguistics,pages 137-144.Carl Pollard and Ivan Sag.
1994.
Head-DrivenPhrase Structure Grammar.
University ofChicago Press, Chicago.Owen Rambow, K. Vijay-Shanker, and DavidWeir.
1995.
D-Tree Grammars.
In Proceed-ings of the 33rd Meeting of the Association forComputational Linguistics, pages 151-158.Yves Schabes.
1990.
Mathematical nd Computa-tional Aspects of Lexicalized Grammars.
Ph.D.thesis, Department of Computer and Informa-tion Science, University of Pennsylvania.Hideto Tomabechi.
1991.
Quasi-destructivegraph unification.
In Proceedings of the 29thMeeting of the Association for ComputationalLinguistics, pages 315-322, Berkeley, CA.The XTAG-Group.
1995.
A lexicalized Tree Ad-joining Grammar for English.
Technical ReportIRCS Report 95-03, The Institute for Researchin Cognitive Science, University of Pennsylva-nia.The XTAG-Group.
1999.
A lexicalized TreeAdjoining Grammar for English.
TechnicalReport http://www, c is .
upenn, edu/-xtag/tech- repor t / tech- repor t ,  html, The In-stitute for Research in Cognitive Science,University of Pennsylvania.224
