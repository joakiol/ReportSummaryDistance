Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 436?446,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsJoint Bootstrapping of Corpus Annotations and Entity TypesHrushikesh Mohapatra Siddhanth JainIIT BombaySoumen Chakrabarti?AbstractWeb search can be enhanced in powerful ways if to-ken spans in Web text are annotated with disambiguatedentities from large catalogs like Freebase.
Entity anno-tators need to be trained on sample mention snippets.Wikipedia entities and annotated pages offer high-qualitylabeled data for training and evaluation.
Unfortunately,Wikipedia features only one-ninth the number of enti-ties as Freebase, and these are a highly biased sampleof well-connected, frequently mentioned ?head?
entities.To bring hope to ?tail?
entities, we broaden our goal to asecond task: assigning types to entities in Freebase butnot Wikipedia.
The two tasks are synergistic: know-ing the types of unfamiliar entities helps disambiguatementions, and words in mention contexts help assigntypes to entities.
We present TMI, a bipartite graphicalmodel for joint type-mention inference.
TMI attemptsno schema integration or entity resolution, but exploitsthe above-mentioned synergy.
In experiments involving780,000 people in Wikipedia, 2.3 million people in Free-base, 700 million Web pages, and over 20 professionaleditors, TMI shows considerable annotation accuracy im-provement (e.g., 70%) compared to baselines (e.g., 46%),especially for ?tail?
and emerging entities.
We also com-pare with Google?s recent annotations of the same corpuswith Freebase entities, and report considerable improve-ments within the people domain.1 IntroductionThanks to automatic information extraction and se-mantic Web efforts, keyword search over unstruc-tured Web text is rapidly evolving toward entity-and type-oriented queries (Guo et al 2009; Pan-tel et al 2012) over semi-structured databases suchas Wikipedia, Freebase, and other forms of LinkedData.A key enabling component for such enhancedsearch capability is a type and entity catalog.
Thisincludes a directed acyclic graph of types under thesubTypeOf relation between types, and entities at-tached to one or more types via instanceOf edges.
?soumen@cse.iitb.ac.inYAGO (Suchanek et al 2007) provides such a cat-alog by unifying Wikipedia and WordNet, followedby some cleanup.Another enabling component is an annotated cor-pus in which token spans (e.g., the word ?Albert?
)are identified as a mention of an entity (e.g., thePhysicist Einstein).
Equipped with suitable indices,a catalog and an annotated corpus let us find ?sci-entists who played some musical instrument?, andanswer many other powerful classes of queries (Liet al 2010; Sawant and Chakrabarti, 2013).Consequently, accurate corpus annotation hasbeen intensely investigated (Mihalcea and Csomai,2007; Cucerzan, 2007; Milne and Witten, 2008;Kulkarni et al 2009; Han et al 2011; Ratinov etal., 2011; Hoffart et al 2011).
With two exceptions(Zheng et al 2012; Gabrilovich et al 2013) that wediscuss later, public-domain corpus annotation workhas almost exclusively used Wikipedia and deriva-tives, partly because Wikipedia provides not only astandardized space of entities, but also reliably la-beled mention text within its own documents, whichcan be used to train machine learning algorithms forentity disambiguation.However, the high quality of Wikipedia comesat the cost of low entity coverage (4.2 million)and bias toward often-mentioned, richly-connected?head?
entities.
Hereafter, Wikipedia entities arecalled W .
Freebase has fewer editorial controls, buthas at least nine times as many entities.
This is par-ticularly perceptible for people entities: one needsto be relatively famous to be featured on Wikipedia,but Freebase is less selective.
Hereafter, Freebaseentities are called F .As in any heavy-tailed distribution, even rela-tively obsecure entities from F \W are collectivelymentioned a great many times on the Web, and in-cluding them in Web annotation is critical, if entity-oriented search is to impact the vast number of tail436queries submitted to Web search engines.Primary goal ?
corpus annotation: We havethus established a pressing need to bootstrap from asmall entity catalog W (such as Wikipedia entities),and a small reference corpus CW (e.g., Wikipediatext) reliably annotated with entities from W , to amuch larger catalog F (e.g., Freebase), and an open-domain large payload corpus C (e.g., the Web).We can and will use entities in F ?
W 1 in thebootstrapping process, but the real challenge is toannotate C with mentions m of entities in F \W .Unlike for F ?W , we have no training mentions forF \W .
Therefore, the main disambiguation signalis from the immediate entity neighborhood N(e) ofthe candidate entity e in the Freebase graph.
I.e., ifm also reliably mentions some entity in N(e), thene becomes a stronger candidate.
Unfortunately, formany ?tail?
entities e ?
F \W , N(e) is sparse.
Isthere hope for annotating the Web with tail entities?Here, we achieve enhanced accuracy for the primaryannotation goal by extending it with a related sec-ondary goal.Secondary goal ?
entity typing: If we had avail-able a suitable type catalog T with associated enti-ties in W , which in turn have known textual men-tions, we can build models of contexts referring totypes like chemists, sports people and politicans.When faced with people called John Williams inF \W , we may first try to associate them with types.This can then help disambiguate mentions to specificinstances of John Williams in F \W .
In principle,useful information may also flow in the reverse di-rection: words in mention contexts may help assigntypes to entities in F \W .
For reasons to be madeclear, we choose YAGO (Suchanek et al 2007) asthe type catalog T accompanying entities in W .Our contributions: We present TMI, a bootstrap-ping system for solving the two tasks jointly.
Apartfrom matches between the context of m and entitynames in N(e), TMI combines and balances evi-dence from two other sources to decide if e is men-tioned at token span m, and has type t:?
a language model for the context in which enti-ties of type t are usually mentioned1With F=Freebase and W=Wikipedia, F ?W ?W but notquite; W \ F is small but non-empty.?
correlations between t and certain path featuresgenerated from N(e).TMI uses a novel probabilistic graphical model for-mulation to integrate these signals.
We give a de-tailed account of our design of node and edge po-tentials, and a natural reject option (recall/precisiontradeoff).We report on extensive experiments using YAGOtypes, Wikipedia entities and text, Freebase en-tities, and text from ClueWeb122, a 700-million-page Web corpus.
We focus on all people enti-ties in Wikipedia and Freebase, and provide threekinds of evaluation.
First, we evaluate TMI on over1100 entities in F ?
W and 5500 snippets fromWikipedia text, where it visibly improves upon base-lines and a recently proposed alternative method(Zheng et al 2012).
Second, we resort to exten-sive manual evaluation of annotation on ClueWeb12Web text with Freebase entities, by professional ed-itors at a commercial search company.
TMI againclearly outperforms strong baselines, doing partic-ularly well for nascent or tail entities.
TMI im-proves per-snippet accuracy, for some classes ofentities, from 46% to 70%, and pooled F1 scorefrom 66% to 73%.
Third, we compare TMI an-notations with Google?s FACC1 (Gabrilovich et al2013) annotations restricted to people; TMI is sig-nificantly better.
Our annotations and related datacan be downloaded from http://www.cse.iitb.ac.in/?soumen/doc/CSAW/.
To our knowl-edge, this is among the first reports on extensive hu-man evaluation of machine annotation for F \W ona large Web corpus.2 Related workThe vast majority of entity annotation work (Mi-halcea and Csomai, 2007; Cucerzan, 2007; Milneand Witten, 2008; Kulkarni et al 2009; Han et al2011; Ratinov et al 2011; Hoffart et al 2011) useWikipedia or derivative knowledge bases.
(Ritter etal., 2011) and (Zheng et al 2012) are notable ex-ceptions.
(Ritter et al 2011) use entity names fordistant supervision in POS tagging, chunking andbroad named entity typing in short tweets, which aredifferent from our goals.Recently, others have investigated inferring types2http://lemurproject.org/clueweb12/437Antony JohnWilliams0ggbn2k JohnWilliamsJohnWilliams03nmyfz0bhbqmmUKSt AspahchemistacademicChemSpidertypetypefoundernationalityplace of birthMuskegonWisconsinMadisonAthleteAmericanfootballplayerplace of birthtypetypeeducation education???
typeEuniceKanenstenhawiWilliamschildren?
Antony John Williams, VP forStrategic Development and Head ofthe Cheminformatics group for theRoyal Society of Chemistry hasbeen honoured by MicrosoftResearch for his ??
to a 20-0 lead by the secondquarter with running back JohnWilliams?s 1 yard touchdownquarterback Neil O Donnell?s ??
Massachusetts, on 17 September1696, the daughter of Puritanminister Rev.
John Williams and hiswife Eunice Mather Williams ?ChemSpider, ChemistrySteelers, Packers, touchdown,quarterback, Colts, quarter, yard,goal, Indianapolis, field, receiverMassachusetts, Eunice Williams,minister, Puritan, kanenstenhawi,Rev.Mention contexts:Salient words from page containing contexts:(a) (b) (c)Harvard UniversityFig.
1: Signal synergies.
Three of the many people mentioned as ?John Williams?
on the Web are shown, withFreebase MIDs.
(a) Easy case where Freebase neighbors of 0ggbn2k match snippet and salient text, and type links arealso available.
(b) No match exists between Freebase neighborhood and snippet, but type links help attach the snippetto 03nmvfz.
(c) Freebase provides no types, but we can provide types from YAGO based on snippet and salient text,which also match neighbors of 0bhbqmm.of emerging entities (related to our secondary goal).In concurrent work, (Nakashole et al 2013) pro-pose integer linear program formulations for infer-ring types of emerging entities from the way theirmentions are embedded in curated relation-revealingphrases.
(Lin et al 2012) earlier approached theproblem using weight propagation in a bipartitegraph connecting unknown to known entities viatextual relation patterns.
Both note that this canboost mention disambiguation accuracy.The closest work to ours is by (Zheng et al2012): they use semisupervised learning to anno-tate a corpus with Freebase entities.
Like (Milneand Witten, 2008), they depend on unambiguousentity-mention pairs to bootstrap a classifier, thenapply it to unlabeled, ambiguous mentions, creatingmore training data.
They use a per-type languagemodel like us (?3.3), but this is used as a secondarycause for (word) feature generation, supplementingand smoothing entity-specific language models.
Incontrast, we use a rigorous graphical model to com-bine new signals, not depending on naturally unam-biguous mentions.
Finally, in the interest of fullyautomated evaluation, they limit their experimentsto F ?W and Wikipedia corpus, thus differing crit-ically from our human evaluation on F and a Webcorpus.
(Gabrilovich et al 2013) have recently releasedFACC1: annotations of ClueWeb09 and ClueWeb12with Freebase entities.
Their algorithm is not yetpublic.
They report: ?Due to the sheer size of thedata, it was not possible to verify all the automaticannotations manually.
Based on a small-scale hu-man evaluation, the precision .
.
.
is believed to bearound 80?85%.
Estimating the recall is of coursedifficult; however, it is believed to be around 70-85%.?
In ?5, we will see that, for people entities,TMI greatly increases recall beyond FACC1, keep-ing precision unimpaired.3 The three signalsFig.
1 shows three Freebase entities mentioned as?John Williams?
in Web text, represented as nodeswith Freebase ?MID?s e = 0ggbn2k, 03nmvfz,and 0bhbqmm, embedded in their Freebase graphneighborhoods.
Owing to larger size and higherflux, Freebase shows less editorial uniformity thanWikipedia.
This shows up in missing or non-standard relation edges.
Unlike YAGO, whereeach entity is attached to one or more types, e =0bhbqmm does not have a type link.
Many peoplehave a link labeled profession, which is a second438kind of type link.
Entities like e = 0bhbqmm alsohave small, uninformative graph neighborhoods.Also shown are three mention contexts, each rep-resented by the snippet immediately surrounding themention, and salient words from the documents con-taining each snippet.
(Salient words may be ex-tracted as words that contribute the largest compo-nents to the document represented as a TFIDF vec-tor.)
(a) shows a favorable but relatively rare casewhere e = 0ggbn2k has reliable type links.
We can-not assume there will be a 1-to-1 correspondence be-tween Freebase and YAGO types, but in ?3.2 we willdescribe how to learn associations between Freebasepaths around entities and their YAGO types.
Thesnippet and salient words show reasonable overlapwith N(e).
In ?3.4 we will describe features thatcharacterize such overlap.
In (b), e = 03nmvfzis reliably typed, but there is no direct match be-tween N(e) and the snippet.
Nevertheless, the snip-pet can be reliably annotated with 03nmvfz if wecan learn associations between types American foot-ball player and Athlete (or their approximate YAGOtarget types) and several context/salient words (see?3.3).
In (c), e = 0bhbqmm is not reliably typed.However, there are matches between N(e) and con-text/salient words.
Once the snippet-to-entity asso-ciation is established, it is easier to assign 0bhbqmmto suitable types in YAGO.In this section we will first describe our design ofthe target type space, and then the tree signals thatwill be used in our joint inference.3.1 Designing the target type spaceBy typing two people called John Williams as ac-tor and footballer, we may also disambiguate theirmentions accurately.
Therefore, we need a well-organized type space where the types?
collectively cover most entities of interest,?
offer reasonable type prediction accuracy, and?
can be selected algorithmically, for any do-main.Wikipedia and Freebase have many obscure typeslike ?people born in 1937?
or ?artists from On-tario?
which satisfy none of the above requirements.YAGO, on the other hand, has a clean hierarchy ofover 200,000 types with broad coverage and fine dif-ferentiation.
Most entities in F \ W can be accu-if t has < Nlow = 5000 member entities thenreject t from our type spacereturnif t has > Nhigh = 25000 member entities thenfor each immediate child t?
?
t docall ChooseTypes(t?
)elseaccept t into our type space (but do not recurse)Fig.
2: Procedure ChooseTypes(t).rately attached to one or more YAGO types.YAGO lists around 37,000 subtypes of person.To satisfy the three requirements above, we calledChooseTypes(person) (Fig.
2); this resulted in130 suitable types being selected.
These directlycovered 80% of Freebase people; the rest couldmostly be attached to slightly over-generic typeswithin our selection.3.2 Predicting types from entity neighborhoodThere will generally not be a simple mapping be-tween Freebase and target types.
E.g., entity emay be known as a Mayor in Freebase, but theclosest YAGO type may be Politician.
Edge andnode labels from the Freebase graph neighborhoodN(e) can embed many clues for assigning e a targettype.
E.g., e = 03nmvfz may have an edge labeledplayedFor to a node representing the Wikipedia en-tity Pittsburgh Steelers, which has a typelink to NFL team.
This two-hop link label sequencewould repeat for a large number of players, and canbe used as a feature in a classifier.00.20.40.60.811 2 3 4HopsReachableWholePrune1Prune2Prune3Fig.
3: Freebase has small diameter despite graph thin-ning.
Prune1 removes paths from the whole Free-base graph that pass through nodes /user/root and /com-mon/topic, Prune2 also removes node /people/person,and Prune3 removes several other high degree hubs.Two further refinements are needed to make thiswork.
First, we have to collect path labels aroundnegative instances we well, and submit positive andnegative path labels to a binary classifier to can-439cel the effect of frequent but non-informative pathtypes.
Second, indiscriminate expansion around e isinfeasible because the Freebase graph has very smalldiameter.
Even after substantial pruning, paths oflength 3 and 4 reach over 40% and 96% of all nodes(Fig.
3).
This increases computational burden andfloods us with noisy and spurious paths.
We rem-edy the problem using an idea from PathRank (Laoand Cohen, 2010).
Instead of trying to explore allpaths originating (or terminating) at e, where e mayor may not belong to a target type, we focus on pathsbetween e and other known members of the targettype.3.3 Type ?language model?To exploit the second signal, shown in Fig.
1(b), weneed to model the association between target YAGOtypes and the mention contexts of Wikipedia entitiesknown to belong to those types.
This model compo-nent is in the same spirit as (Zheng et al 2012).For each target YAGO type t, we sample positiveentities e ?
F ?W , and for each e, we collect, fromWikipedia annotated text, a corpus of snippets men-tioning e. We remove the mention words and retainthe rest.
We also collect salient words from the en-tire Wikipedia document containing the snippet, asshown in Fig.
1.At this point each target type is associated witha ?corpus?
of contexts, each represented by snippetwords.
We compute the IDF of all words in thiscorpus3, and then represent each type as a TFIDFvector (Salton and McGill, 1983).
A test context isturned into a similar vector, and its score with re-spect to t is the cosine between these two vectors.This simple approach was found superior to buildinga more traditional smoothed multinomial unigrammodel (Zhai, 2008) for each type.
Given the outputof this component feeds into an outer discriminativeinference mechanism, a strict probabilistic model isnot necessary.3.4 Entity neighborhood match with snippetThe third signal is a staple of any disambiguationwork: match the occurrence context against theneighborhood in the structured representation.
Inword sense disambiguation (WSD), support for as-signing a word in context to a synset comes from3Generic IDF from Wiki text does not work.matches between, say, other words in the context andthe WordNet neighborhood of the proposed synset.As in WSD, many approaches to Wikification mea-sure some local consistency between a mention mand the neighborhood N(e) of a candidate entity e.N(e) is again limited by a maximum path length `.From snippetmwe extract all phrases P (m) exclud-ing the mention words.
For each phrase p ?
P (m),if p occurs at least once4 in any node of N(e), thenwe accumulate a credit of |p|?w?p IDF(w), wherew ranges over words in p, IDF(w) is its inversedocument frequency (Salton and McGill, 1983) inWikipedia, and |p| is the length of the phrase.
Thisrewards exact phrase matches.et t?m2m1e?Mentions in contextCandidateentitiesCandidate typesFig.
4: Tripartite assignment problem.4 Unified modelFigure 4 abstracts out the signals shown in Figure 1into a tripartite assignment problem.
Each mentionlikem1 has to choose at most one entity from amongcandidate aliasing entities like e and e?.
Each entitye ?
F \W has to choose one type (for simplicity weignore zero or more than one types as possibilities)from candidates like t and t?.The configuration of thick (green) edges shouldbe preferred to alternative dotted (red) edges underthese considerations:?
There is high local affinity or compatibility be-tween e and t, based on associations between tand N(e) as discussed in ?3.2.?
There are better textual matches between N(e)and m1, as compared to N(e?)
and m1.?
In aggregate, the non-mention tokens in thecontext of m1,m2 (shown as gray horizontallines) match well the language model associ-ated with mentions of entities of type t (ratherthan t?
).4Incorporating term frequency often polluted the score.4400?0t10t0?t2?
(t1)t1?
(t0)t0??
(e1)e1?
(e0)e0??
(e1)e1?
(e5)e5Snippet nodepotentialSnippet nodepotentialEntity nodepotentialUniform node potentialfor dummy entity?????
?e1e0t00t00t0Potential of edgeconnecting snippetto dummy entitynode???
?e1e00...0t0t0Potential of edgeconnecting snippetnode to entity nodeFig.
5: Illustration of the proposed bipartite graphical model, with tables for node and edge potentials and synthetic?-entity nodes to implement the reject option.We now present a unified model that combines thesignals and solves the two proposed tasks jointly.We model two kinds of decision variables, whichwill be represented by nodes in a graphical model.Associated with each entity e ?
F \ W there is ahidden type variable (node) Te, which can take on avalue from (some subset of) the type catalog T .
As-sociated with each mention m (along with its snip-pet context and all its observable features) there isa hidden entity variable Em, which can take on val-ues from some subset of entities.
(For simplicity, weassume that entities in F ?W have already been an-notated in the corpus, and no m of interest mentionssuch entities.
)We will model the probability of a joint assign-ment of values to Te, Em aslog Pr(~t,~e) = ?
?e?e(te) + ?
?m?m(em)+?e,m?e,m(te, em)?
const., (1)where node log potentials are called ?e, ?m, edgelog potentials are called ?e,m, and ?, ?
are tunedconstants.
The log partition function, written?const.?
above, will not be of interest in infer-ence, where we will seek to choose ~t,~e to maximizePr(~t,~e).
In this section, we will design the node andedge log potentials.4.1 Node log potentialsEach node Te is associated with a node potential ta-ble, mapping from possible types in Te to nonneg-ative potentials.
The potential values are suppliedfrom ?3.2 as the classifier output scores.Each node Em is associated with a node poten-tial table, mapping from possible entities in Em tononnegative potentials.
The potential values are sup-plied from ?3.4.4.2 Edge log potentialsSuppose we assign Em = e, and Te = t. Thenwe would like the non-mention context words ofm to be highly compatible with the type ?languagemodel?
developed in ?3.3.If e is among the set of values Em that Em cantake, then nodes Te and Em are joined by an edge.This edge is associated with an edge potential table?e,m : Te?Em ?
R+.
?e,m(?, e?)
will be set to zero(cells shaded gray in Fig.
5) when e 6= e?.
?e,m(t, e)is set to the cosine match score described in ?3.3.4.3 The reject (a.k.a.
null, nil, NA, ?)
optionAn algorithm may reject many snippets, i.e., refrainfrom annotating them.
This could be because thesnippet mentions an entity outside F (and outsideW ), or the system wishes to ensure high precision atsome cost to recall.Rejection is modeled by adding, for each snippetm, a pseudo or ?null?
entity?m (also called ?no an-441notation?
NA, null or nil in the TAC-KBP5 commu-nity).
For simplicity, we assume that ?m and ?m?are incomparable or distinct for m 6= m?.
I.e., wedo not offer to cluster mentions of unknown enti-ties.
These remain separate, unconnected nodes inthe (augmented) Freebase graph.If we choose Em = ?m, we get zero credit formatching non-mention text inm toN(?m), becauseN(?m) = ?
and ?3.4 has no information to con-tribute.
I.e., we set ?m(?m) = 0.
In general,?m(?)
?
0, so ?m gets the lowest possible credit(but this will be modified shortly).There is also a type variable T?m .
To what typeshould we assign ?entity?
?m?
Because ?m has noconnections in the Freebase graph, no hint can comefrom ?3.2.
Put differently, ?
?m(t) will be constant(say, zero) for all snippets m and types t.Even if we do not know the entity mentioned inm, the non-mention text in m will have differentialaffinity to different types, obtained from ?3.3.
Thismeans that, if Em = ?m is chosen, T?m will beargmaxt ?
?m,m(t,?m), which explains the non-mention words in m using the best available lan-guage model associated with some type.
For a dif-ferent entity Em = e0 and type Te0 = t assign-ment to win, ??m(e0)+?
?e0(t)+?e0,m(t, e0) mustexceed the null score above.
This provides a us-able recall/precision handle: we modify ?m(?m) toa tuned number; making it smaller generally giveshigher recall and lower precision.4.4 Inference and trainingThe goal of collective inference will be to assign atype value to each Te and an entity value to eachEm.
We seek the maximum a-posteriori (MAP) la-bel, for which we use tree-reweighted message pass-ing (TRW-S) (Kolmogorov, 2006).
Our graph hasplenty of bipartite cycles, so inference is approxi-mate.
Given the sparsity of data, we preferred todelexicalize our objective (1), i.e., avoid word-levelfeatures and pre-aggregate their signals via time-tested aggregators (such as TFIDF cosine).
As a re-sult we have only two free parameters ?, ?
in (1),which we tune via grid search.
A more principledtraining regimen is left for future work.5http://www.nist.gov/tac/2013/KBP/5 ExperimentsWe focus our experiments on one broad type of en-tities, people, that is more challenging for disam-biguators than typical showcase examples of distin-guishing (Steve) Jobs from employment and AppleInc.
from fruit.We report on three sets of experiments.
In ?5.1,we restrict to entities from F ?
W and Wikipediatext, for which ground truth annotation is avail-able.
In ?5.2, we evaluate TMI and baselines onClueWeb12 and entities from Freebase, not lim-ited to Wikipedia.
In ?5.3, we compare TMIwith Google?s recently published FACC1 annota-tions (Gabrilovich et al 2013).5.1 Reference corpus CW with F ?W entitiesLimited to people, |F | = 2323792, |W | = 807381,|F \ W | = 1544942, and |F ?
W | = 778850.
Itis easiest to evaluate TMI and others on Wikipediaentities.
They have known YAGO types.
Wikipediatext has explicit (disambiguated) entity annotations.For these reasons, the few known systems forFreebase-based annotation (Zheng et al 2012) areevaluated exclusively on F ?W .5.1.1 Seeding and setupPeople in F ?W are known by one or more men-tion words/phrases.
From these, we collect mentionphrases along with the candidate entity set for eachphrase.
The number of candidates is the phrase?s de-gree of ambiguity.
We sort phrases by ambiguity anddraw a sample over the ambiguity range.
This givesus seed phrases with representative ambiguity.
Thenwe collect all entities mentioned by these phrases.Overall we collect about 1100 entities and 5500 dis-tinct mentions.Contrast this with (Zheng et al 2012), who sam-ple entities from much fewer than 130, and largelywell-separated types: professional athletes, aca-demics, actors, films, books, hotels, and tourist at-tractions.
If there were only two namesakes, an ac-tor and a politician, the politician disappears, leav-ing a naturally unambiguous alias.
I.e., (Zheng etal., 2012) did not ?complete?
their entity sets withaliased entities.
For all these reasons, Z0 numbershere are not directly comparable to those in their pa-per.4425.1.2 Tasks and baselinesThe structure of TMI suggests two natural base-lines to compare against it.
TMI solves two tasks si-multaneously: assign types to entities and entities tosnippets.
So the first baseline, T0, is one that solvesthe typing task separately, and the second, A0, doessnippet annotation separately.
A third baseline, Z0,from (Zheng et al 2012) does only snippet annota-tion; they do not consider typing entities.While evaluating types output by TMI and T0against ground truth, we may wish to assign partialcredit for overlapping types, e.g., athlete vs. soccerplayer, because our types form an incomplete hierar-chy.
We use the standard ?M&W?
score of semanticsimilarity between types (Milne and Witten, 2008)for this.As regards snippet annotation, Z0 (Zheng et al2012) does not specify any mechanism for han-dling ?.
Therefore we run two sets of experiments.In one we eliminate all snippets with ground truth?.
A0, and TMI are also debarred from returning ?for any snippet.
In the other, snippets marked ?
inground truth are included.
A0 and TMI are enabledto return ?, but Z0 cannot.TMI A0 Z00/1 snippet accuracy 0.827 0.699 0.627Fig.
6: Snippet annotation onCW corpus, F ?W entities,?
not allowed.TMI A0 Z00/1 snippet accuracy 0.7307 0.651 0.622Snippet precision 0.858 0.843 0.622Snippet recall 0.777 0.692 0.639Snippet F1 0.815 0.760 0.630Fig.
7: Snippet annotation onCW corpus, F ?W entities,?
allowed.5.1.3 Snippet annotation resultsFig.
6 shows snippet annotation accuracy (frac-tion of snippets labeled with the correct entity) when?
is not allowed as an entity.
As two uninformedrefernces, uniform random choice gives an accuracyof 0.423 and choosing the entity with the largestprior gives an accuracy of 0.767.
TMI is consid-erably better than A0, which is better than Z0 andthe uninformed references.
This is despite trainingZ0?s per-type topic models not only on unambiguous0.730.930.850.520.74 0.810.420.75 0.8500.20.40.60.810?19 20?39 >=40Degree-->Accuracy-->TMI A0 Z00.730.980.380.750.240.340.180.980.1300.50.40.70.260?69 50?89 >=40Degree-->AccueayT6-->MIZ ?
?Fig.
8: Bucketed comparison between TMI and baselines,F ?W , ?
allowed.TMI T00/1 type accuracy 0.80 0.81M&W type accuracy 0.82 0.83Fig.
9: Type inference, CW corpus, F ?W entities.snippets, but also on a disjoint fraction ofF?W , as asurrogate for Wikipedia?s containment in Freebase.Fig.
7 repeats the experiment while allowing ?.Here, in 0/1 accuracy, ?
is regarded as just anotherentity.
Again, we see that TMI has a clear advantage.Z0?s performance here is worse than in (Zheng etal., 2012).
This is explained by our much larger anddifficult-to-separate type system.We disaggregate the summary results into buck-ets, shown in Fig.
8.
Each bucket covers a range ofdegrees of entity nodes in Freebase, while roughlybalacing the number of snippets in each bucket.
TMIgenerally shows larger gains for low-degree buckets.5.1.4 Type prediction resultsWe also compared the type inference accuracy ofTMI and T0; (Zheng et al 2012) do not infer types.The summary is in Fig.
9.
Two uninformed baselinesare worth mentioning.
Uniform random choice over130 types gave only 2% accuracy.
Chossing the typewith largest prior probability gave 28.2% accuracy.TMI is much better, but offers no significant ben-efit (or degradation) compared to T0.
We verified,partly by way of debugging, that there do exist enti-ties e with small degree but a modest number of as-signed snippets, for which snippet-to-N(e) matches443angus mcdonald, chris robinson, christopher henry, elizabethcameron, george woods , henry barnes, jack scott, jeremyrobert, john sherman, leonard thomas, marc anthony, mitchelldonald, morrison mark, parker edward, richard andrew , simonscott, stephen ross, stuart baron, tom clark, whitney john, austinscott, barbara johnson, brian peterson, carlos rivero, davidberman, david johns, donald fraser, george davies, george fisher,graham smith, john pepper, jonathan edwards, kevin brown,kevin hughes, matt johnson, michael davidson, nancy johnson,paul holmes, pedro martins, peter frank, peter mitchell, petermullen, robert stern, roger edwards, stuart walker, terry evans,tony angelo, tony ward, william jarvis, william sampsonFig.
10: Seed mentions for confusion clusters for Webcorpus C and entities in F .provide a boost to type prediction accuracy (about50%), as compared to T0 (about 20% for these in-stances).
Therefore, the flow of information betweentype and entity assignments is, in principle, bidirec-tional, in the regime of such entities.5.2 Payload corpus C with entities in FRecall our main goal is to annotate payload cor-pus C with entities in all of F .
Experience withF ?W and CW may not be representative of F andC.
Entities in F \W may not come with referencementions, and their type and entity neighborhoodsmay be sparse.
Furthermore, compared to the closedworld of F ?W , evaluating TMI and baselines overC and F \ W is challenging.
Entities in F \ Wdo not have ground truth types in the type catalogT (here, YAGO), nor snippets labeled by humans asmentioning them.
Therefore, we need human edi-torial judgment, which is scarce.
Even though TMIcan be applied at Web scale, the scale if evaluationis limited by editorial input.5.2.1 Seeding and data setupThere are about 2.3 million Freebase entities con-nected to /people/person via type links.
Similar to?5.1.1, we chose phrases (Fig.
10) with diverse de-gree of ambiguity (Fig.
11), to seed confusion clus-ters.
Then we completed the clusters by includingaliased entities, as before, so as not to artifically re-duce the degree of ambiguity.
Note that entities inW can and do contend with entities in F \W .
Thecluster size distribution is shown in Fig.
12.
Limitedby editorial budget, we finished with 634 entities,238 distinct aliases and 4,500 snippets.We used the 700-million-page ClueWeb12 Webcorpus.
All phrases in the expanded clusters are0.070309085248516?>=54D5e?g1r42-gA2c1uay4Duay4D1A2c-=?Ag5T1M2=gI5A541c?Au=5aaA=2cy>1a52gy4Zu=4?>142r?g>2g5Z-y4Z1Iy4D54>u2AA1y?>A?g-y4Z1A2gTc?u=y5a1Zye?Z>2g5eyg>1A544T4285415Z-y4Z>uy4a2>14?e542A=2cy>1u1a52gy4Z5Z-y4Z1=y4I541Iy4D54y?>A?g1-yD5cyg1>u2AAe?e?yg1a52gy4Z1A=2cy>>A?y4A1=1-yaD54>u45yc?g1>u2AA1>?c2g42r54A1y4A=?41c24A2gI5A541Z1c?Au=5aagyguT1M2=g>2g1>45r42cy4?gZy1gyguT1M2=g>2gcy4u1ygA=2gT1Zyg?yD5e?g1M1r42-gM2=g1-?aa?yc1>=54cygM2=g1>=54cyg1M4M2=g1=5g4T1ry4g5>M2=g151I5II54M545cT142r54A1M2=g>2gMyuD1Z5gA2g1>u2AA=5g4T1u=4?>A2I=5484y=yc151>c?A=8524851-5>u2AA16?>=548524851a5c?5a1-22Z>8524851Z1-22Z>8524851ryrT1-22Z>64ygD1I5A541a5=cygg5Z-y4Z1M2=g1ua?6A2gZ2gyaZ1c164y>54Z2gyaZ1Z1c?Au=5aaZy4?5g184y=yc1>c?A=u=y4a5>1-?aa?yc1My4e?>ry4ry4y1a1M2=g>2gy4A=?41-?aa?yc1>ycI>2g?gA?2g>?cr?8??AT???
0.0703090??g?II5A>1?.000??????g?II5A>?r?8?ATFig.
11: Ambiguity distribution for Web corpus C. Un-ambiguous names are usually fully expanded and veryrare, if at all present, in evaluated snippets.0.0703090.
8 .. .8 7.
78 3.
38 9.
9852416?>6=Degr-Accuay4rTrT-gccuFig.
12: Confusion cluster size distribution for Web cor-pus C.loaded into a trie used by a map-reduce job to ex-tract documents, then snippets, from the corpus.Some phrases in Figs.
10 and 11 have overwhelm-ing numbers of pages with matches.
In produc-tion, we naturally want all of them to be annotated.But human editorial judgment being the bottleneck,we sampled 50% or 50,000 snippets, whichever wassmaller.
Starting with about 752,450 pages, we ranthe Stanford NER (Finkel et al 2005) to mark per-son spans.
Pages with fewer than five non-person to-kens per person were discarded; this effectively dis-carded long list pages without any informative textto disambiguate anyone, and left us with 574,135pages.
From these we collected 304,309 snippetswhere the mention phrase is marked by the NER asa person.
Each seed phrase leads to one cluster onwhich TMI and A0 are run.
Note that ?
must beallowed on the open Web.5.2.2 Editorial judgmentFinally, for each algorithm, about 634 entity-typeand about 4500 snippet-entity assignments are ran-domly sampled and sent to 20 editors in a commer-cial search engine company, who judged each as-signment as correct or incorrect, without knowingwhich algorithm produced the annotation, to avoid444TMI A0entity0/1 accuracy .714 .562Pooled recall .764 .869Precision .714 .562F1 .738 .683e?W(0/1acc.
)e?F\W(0/1)TMI .75 .62A0 .65 .42Fig.
13: Snippet summary for F and payload corpus C.bias.
Because the editors are trained professionals(unlike Mechanical Turks), we increased our evalu-ation coverage by having each type or entity assign-ment reviewed by one editor.Pooling: Ideally, editors can be asked to find thebest type or entity for each entity or snippet, but,given the size and diversity of Freebase, the cogni-tive burden would be unacceptable.
In the Wikipediacorpus CW , a snippet marked?
(no entity) by an al-gorithm can be judged a loss of recall if Wikipediaground truth annotates it with an entity.
Unfortu-nately, this is no longer practical for Web corpus C,because 8,217 snippets marked ?
would have to bemanually inspected and compared with a large num-ber of candidate entities in Freebase.
Therefore, weadopt pooling as in TREC.
(Although the pool issmall, A0 has very high recall.)
Recall is evaluatedwith respect to the union of snippets annoted with anon-?
entity by at least one competing algorithm,with agreement in case of more than one.0/1 Type accuracy: Editors judged each pro-posed type as correct or not.
Unlike in ?5.1, wherethe true and proposed types could be compared viaM&W (Milne and Witten, 2008), they could not beasked to objectively estimate relatedness betweentypes.
Therefore we present only their reported post-hoc 0/1 accuracy for types: T0 and TMI have 0/1type accuracy of 0.828 and 0.818.5.2.3 Snippet annotation resultsGiven the large gap between TMI and Z0 in theeasier setup in ?5.1, we no longer consider Z0, andinstead focus on TMI vs. A0.
The summary com-parison of A0 vs. TMI is shown in Fig.
13.
HereTMI?s absolute gains in 0/1 accuracy and F1 areeven larger than in ?5.1.
To understand TMI?s per-formance across a diversity of Freebase entity nodese, as a function of 1. the size and richness of N(e),and 2. the number of snippets claimed to mention e,we disaggregate the data of Fig.
13 into buckets of0.730.98 0.90 0.930.52 0.84 0.570.750.40.50.80.70.90.1063 467 962 ?>=0Degree--?Accuracy--?
TMIA00.730.980.750.920.420.770.710.940.40.70.90.5063 867 962 ?>10=DegDDrr?-AAcDuay1rr?
TMIZ0Fig.
14: 0/1 accuracy and F1 for snippets, payload corpusC and entities in F .Snippet label judgements %TMI ok, FACC1 ok, neither ?
22TMI ok, FACC1 wrong, neither ?
6TMI6= ?
ok, FACC1=?
wrong 40TMI6= ?
wrong, FACC1=?
23TMI wrong, FACC1 wrong, neither ?
2TMI= ?
wrong, FACC16= ?
correct 4TMI= ?, FACC16= ?, wrong 3(TMI= ?, FACC1= ?, not judged) -Fig.
15: TMI vs. FACC1 comparison.consecutive degrees, roughly balancing the numberof snippets per bucket, as shown in Fig.
14.
At thevery low end of almost disconnected entity nodes,no algorithm does very well, because these entitiesare also hardly ever mentioned.
When the entity ispopular and well-connected, TMI?s benefits are rela-tively modest.
TMI?s gains are best in the mid-rangeof degrees.
The gap narrows for large-degree nodes,which is expected.5.3 Comparison with FACC1After collecting our pool of snippets as in ?5.2.2,we consulted FACC1 (Gabrilovich et al 2013), andpassed on FACC1 annotations to our editors.
As be-fore, the identity of the algorithm was concealed.Results are shown in Fig.
15.
In a large 40% ofcases, TMI labels correctly while FACC1 backs off.The converse, where FACC1 backs off and TMImakes a mistake, is about half as frequent.
Thesepreliminary numbers suggest that TMI is able topush recall beyond FACC1 while also giving betterprecision.4456 ConclusionWe presented a formal model for bootstrapping fromYAGO types and entities annotated in Wikipedia totwo tasks, 1. annotating Web snippets with Freebaseentities, and 2. associating Freebase entities withYAGO types.
We presented TMI, a system to solvethe two tasks jointly.
Experiments show that TMI?ssnippet annotation accuracy, especially for relativelyweakly-connected Freebase entities, is superior tobaselines.
We aim to extend from people to all majorFreebase categories, and larger Web crawls.Acknowledgment: We are grateful to ShrikantNaidu, Muthusamy Chelliah, and the editors fromYahoo!
for their generous support.
Shashank Guptahelped process FACC1 data.ReferencesS.
Cucerzan.
2007.
Large-scale named entity disam-biguation based on Wikipedia data.
In EMNLP Con-ference, pages 708?716.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbs sam-pling.
In ACL Conference, pages 363?370.Evgeniy Gabrilovich, Michael Ringgaard, and AmarnagSubramanya.
2013.
FACC1: Freebase annotationof ClueWeb corpora.
http://lemurproject.org/clueweb12/, June.
Version 1 (Release date 2013-06-26, Format version 1, Correction level 0).Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li.
2009.Named entity recognition in query.
In SIGIR Confer-ence, pages 267?274.
ACM.Xianpei Han, Le Sun, and Jun Zhao.
2011.
Collectiveentity linking in Web text: A graph-based method.
InSIGIR Conference, pages 765?774.Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,Hagen Fu?rstenau, Manfred Pinkal, Marc Spaniol,Bilyana Taneva, Stefan Thater, and Gerhard Weikum.2011.
Robust disambiguation of named entities in text.In EMNLP Conference, pages 782?792, Edinburgh,Scotland, UK, July.
SIGDAT.Vladimir Kolmogorov.
2006.
Convergent tree-reweighted message passing for energy minimization.IEEE PAMI, 28(10):1568?1583, October.Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, andSoumen Chakrabarti.
2009.
Collective annotation ofWikipedia entities in Web text.
In SIGKDD Confer-ence, pages 457?466.Ni Lao and William W. Cohen.
2010.
Relational re-trieval using a combination of path-constrained ran-dom walks.
Machine Learning, 81(1):53?67, October.Xiaonan Li, Chengkai Li, and Cong Yu.
2010.
Enti-tyEngine: Answering entity-relationship queries usingshallow semantics.
In CIKM, October.
(demo).Thomas Lin, Mausam, and Oren Etzioni.
2012.
No nounphrase left behind: detecting and typing unlinkable en-tities.
In EMNLP Conference, pages 893?903.R Mihalcea and A Csomai.
2007.
Wikify!
: linking doc-uments to encyclopedic knowledge.
In CIKM, pages233?242.David Milne and Ian H Witten.
2008.
Learning to linkwith Wikipedia.
In CIKM, pages 509?518.Ndapandula Nakashole, Tomasz Tylenda, and GerhardWeikum.
2013.
Fine-grained semantic typing ofemerging entities.
In ACL Conference.Patrick Pantel, Thomas Lin, and Michael Gamon.
2012.Mining entity types from query logs via user intentmodeling.
In ACL Conference, pages 563?571, JejuIsland, Korea, July.Lev Ratinov, Dan Roth, Doug Downey, and Mike An-derson.
2011.
Local and global algorithms fordisambiguation to Wikipedia.
In ACL Conference,ACL/HLT, pages 1375?1384, Portland, Oregon.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: an exper-imental study.
In EMNLP Conference, pages 1524?1534, Edinburgh, UK.
ACL.G Salton and M J McGill.
1983.
Introduction to ModernInformation Retrieval.
McGraw-Hill.Uma Sawant and Soumen Chakrabarti.
2013.
Learn-ing joint query interpretation and response ranking.
InWWW Conference, Brazil.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
YAGO: A core of semantic knowl-edge unifying WordNet and Wikipedia.
In WWWCon-ference, pages 697?706.
ACM Press.ChengXiang Zhai.
2008.
Statistical language modelsfor information retrieval: A critical review.
Founda-tions and Trends in Information Retrieval, 2(3):137?213, March.Zhicheng Zheng, Xiance Si, Fangtao Li, Edward Y.Chang, and Xiaoyan Zhu.
2012.
Entity disambigua-tion with Freebase.
In Web Intelligence Conference,WI-IAT ?12, pages 82?89.446
