Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 87?92,October 25, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsSemantic Query Expansion for Arabic Information RetrievalAshraf Y. Mahgoub  Mohsen A. RashwanComputer Engineering Department, CairoUniversity, EgyptElectronics and CommunicationsEngineering Department, Cairo University,Egyptashraf.thunderstorme@gmail.com mrashwan@rdi-eg.comHazem Raafat Mohamed A. ZahranComputer Science Department, KuwaitUniversity, Kuwait City, KuwaitComputer Engineering Department, CairoUniversity, Egypthazem@cs.ku.edu.kw moh.a.zahran@eng.cu.edu.egMagda B. FayekComputer Engineering Department, CairoUniversity, Egyptmagdafayek@ieee.orgAbstractTraditional keyword based search is found tohave some limitations.
Such as word senseambiguity, and the query intent ambiguitywhich can hurt the precision.
Semantic searchuses the contextual meaning of terms inaddition to the semantic matching techniquesin order to overcome these limitations.
Thispaper introduces a query expansion approachusing an ontology built from Wikipedia pagesin addition to other thesaurus to improvesearch accuracy for Arabic language.
Ourapproach outperformed the traditional keywordbased approach in terms of both F-score andNDCG measures.1 IntroductionAs traditional keyword based searchtechniques are known to have some limitations,many researchers are concerned with overcomingthese limitations by developing semanticinformation retrieval techniques.
These techniquesare concerned with the meaning the user seeksrather than the exact words of the user?s query.We consider four main features that make usersprefer semantic based search systems overkeyword-based: Handling Generalizations,Handling Morphological Variants, HandlingConcept matches, and Handling synonyms withthe correct sense (Word Sense Disambiguation).2 Semantic-based Search FeaturesIn this section we will discuss the main featuresof semantic search that makes it more temptingchoice over the traditional keyword basedtechniques.2.1 Handling GeneralizationHandling generalizations allows the systemto provide the user with pages that containsmaterial relevant to sub-concepts of the user?squery.
Consider the following example in Table 1where a query contains a general term or concept??
?(Violence).87User?s Query In Arabic Equivalent Query InEnglish???????
??
???
????
??Violence in Africa?Table1: Example Query 1Semantic-based search engines should be able torecognise pages with sub-concepts like:????
?(extermination), ??
?  (suppression),????
?(torture) as relevant to user?s query.2.2 Handling Morphological VariationsHandling morphological variations allows thesystem to provide the user with pages that containwords derived from the same root as those inuser?s query.
Consider the following example inTable 2.User?s Query In Arabic Equivalent Query InEnglish??????
??????????
??
?Development in theMiddle East?Table2: Example Query 2Pages that contain morphological variants ofthe word ????????
(Development) such as ????????,???????
?, and ??????????
should also be consideredrelevant to user?s query.2.3 Handling Concept MatchesThe system should also be aware of conceptsor named entities that may be addressed withdifferent words.
Consider the following examplein Table 3.User?s Query In Arabic Equivalent Query InEnglish??
??Egypt?Table3: Example Query 3The term ?????
has other equivalentexpressions like [????????
???
???????
?, ?
??????????
?, ???????
???].
So documents that contain any ofthese expressions should be considered relevant.2.4 Handling Synonyms With Correct SenseAlthough the meaning of many Arabic wordsdepends on the word?s diacritics, most Arabic textis un-vowelized.
For example, Table 4 shows theword ?????
has more than a single meaningdepending on its diacritization.
System should beaware which meaning to consider for expansion.Arabic vowelizedwordEnglishequivalentArabicsynonyms?????
People, nation ???,????????????
Branches ???
?Table4: Different senses for word ??
?3 Related WorkQuery expansion techniques have beenconsidered by many researchers.
The mostsuccessful query expansion techniques depend onautomatic relevance feedback with noconsideration of semantic relations.
(Jinxi Xu and Ralph, 2001) used the highestTF-IDF 50 terms extracted from the top 10retrieved documents from AFP (i.e.
theTREC2001 corpus).
These 50 terms whereweighted due to their TF-IDF scores and added tothe original query -with addition to terms fromother thesaurus-with the following formula:( )           ( )      ?
(   )Where D is the top retrieved documents and tis the original term.
Larkey and Connell (2001)used a similar technique, but with a differentscoring method.Wikipedia has been considered as anontology source by many researchers.
This is dueto its large coverage, up-to-date, and domainindependency.
As in (Alkhalifa and Rodrguez,2008), they proposed an automatic technique forextending Named Entities of Arabic WordNetusing Wikipedia.
They depended mainly onWikipedia?s ?redirect?
pages and Cross-Linguallinks.
Also a large scale taxonomy fromWikipedia deriving technique was proposed by(Pozetto and Strube, 2007).88(Abouenour et al., 2010) proposed a systemthat uses Arabic WordNet to enhance Arabicquestion/answering.
Synonyms from WordNet areused to expand the question in order to extract themost semantically relevant passages to thequestion.
(Milne et al., 2007) proposed a system called?KORU?
for query expansion using Wikipedia?smost relevant articles to user?s query.
The systemallows the user to refine the set of Wikipediapages to be used for expansion.
KORU used?Redirect?
pages for expansion; ?Hyper Links?and ?Disambiguation Pages?
to disambiguateunrestricted text.Our proposed system differs from KORU inseveral points:(1) Adding ?Subcategories?
to handlegeneralization.
(2) Adding Wikipedia ?Gloss?
?
First phraseof the article ?
when there is no?Redirect?
pages available.
(3) Allowing the user to either expand allterms in a single query, or expand eachterm separately producing multiplequeries.
The result lists of these multiplequeries are then combined into a singleresult list.
(4) Adding terms from another twosupportive thesaurus, namely ?Al Raed?dictionary and our constructed?Google_WordNet?
dictionary.4 Proposed System4.1 Arabic ResourcesWe depend in our query expansionmechanism on three Arabic resources: (1) ArabicWikipedia Dump, (2) ?Al Raed?
Dictionary.
(2)?Google_WordNet?
Dictionary.4.1.1 Arabic WikipediaOur system depends mainly on ArabicWikipedia as the main semantic informationsource.
According to Wikipedia, the ArabicWikipedia is currently the 23rd largest edition ofWikipedia by article count, and is the first Semiticlanguage to exceed 100,000 articles.We were able to extract 397,552 ArabicSemantic set, with 690,236 collocations.
The term?Semantic Set?
stands for a set of expressions thatrefer to the same Meaning or Entity.
For example,the following set of concepts forms a semantic setfor ??????????
(Britain): [??????????
???????
????????,?????????
??????
?????????
???????
???????
?, ????????
?, ?
??????????????
].To extract the semantic sets, we depend onthe ?redirect?
pages in addition to the article glossthat may contain a semantic match.
This matchappears in the first paragraph of the article in abold font.
The categorization system of Wikipediais very useful in the task of expanding genericqueries in a more specified form.
This is done byadding ?subcategories?
of the original term to theexpanded terms.4.1.2 The Al Raed Monolingual Dictionary:The ?Al Raed?
Dictionary is a monolingualdictionary for modern words 1 .
The dictionarycontains 204303 modern Arabic expressions.4.1.3 The Google_WordNet DictionaryWe collected all the words in WordNet, andtranslated them to Arabic using Google Translate.For each English word, Google Translate providesdifferent Arabic translations for the English wordeach corresponds to a different sense, each sensehas a list of different possible English synonyms.Using this useful information we were able toextend WordNet Synset entries into a bilingualArabic-English dictionary that maps a set ofArabic synonyms to its equivalent set of Englishsynonyms.
The basic idea is that, two sets ofEnglish synonyms (each allegedly belongs to adifferent sense) can be fused together into onesense if the number of overlapping words betweenthe two sets is two or more.
Fusing two Englishsets together will fuse also their Arabictranslations into one set, thus forming a list ofArabic synonyms matched to a list of Englishsynonyms.
Table 5 shows a sample of GoogleTranslate for the word ?tough?.
We can fuse thefirst and the fourth sense together because theyhave two words in common namely ?strong?
and?robust?.
The same applies to the second and thethird senses with ?strict?
and ?tough?
in common.1 Available athttp://www.almaany.com/appendix.php?language=arabic&category=??????&lang_name=???
?89Thus forming two new mappings as shown inTable 6.????
solid, strong, robust, firm,durable????
strict, rigorous, tough, rigid,firm, stringent????
tough, harsh, rough, severe,strict, stern???
strong, powerful, sturdy,robust, vigorousTable 5: A sample of Google Translate result for theword ?tough????
?, ???
solid, strong, robust, firm,durable, powerful, sturdy,vigorous???
?, ????
strict, rigorous, tough, rigid,firm, stringent, harsh, rough,severe, sternTable 6: Mapping between a set of Arabic synonyms toa set of English synonyms.Finally, we use words of the same Arabic set asan expansion to each other in queries.4.2 Indexing and RetrievalOur system depends on ?LUCENE?, which isfree open source information retrieval libraryreleased under the Apache Software License.LUCENE was originally written in Java, but it hasported to other programming languages as well.We use the ?.Net?
version of LUCENE.LUCENE depends on the Vector Space Model(VSM) of information retrieval, and the Booleanmodel to determine how relevant a givenDocument is to a User's query.
LUCENE has veryuseful set of features, as the ?OR?
and ?AND?operators that we depend on for our expandedqueries.
Documents are analyzed before adding tothe index on two steps: diacritics and stop-wordsRemoval, and text Normalization.
A list of 75words (Contains: Pronouns, Prepositions?etc.
)has been used as stop-words.4.2.1 NormalizationThree normalization rules were used:?
Replace ???
with ???.?
Replace ??
?, ??
?, ???
with ????
Replace ???
with ??
?4.2.2 StemmingWe implemented Light-10 stemmer developedby Larkey (2007), as it showed superiorperformance over other stemming approaches.Instead of stemming the whole corpus beforeindexing, we grouped set of words with the samestem and found in the same document into adictionary, and then use this dictionary inexpansion.
This reduces the probability ofmatching between two words sharing the samestem but with different senses, as they must befound in the same document in corpus to be usedin expansion.Consider the following example in table 7:Arabic Word Stem EnglishEquivalent??????
???
??
Obedience???????
??
??
PlagueTable : Example of two words sharing the same stembut have different senses.We see that both words share the same stem????
?, yet we don?t expand the word ??????
withthe word ?????????
as there is no document in thecorpus that contains both words.4.3 Query ExpansionTo expand a query, we first locate namedentities or concepts that appear in the query inWikipedia.
If a named entity or a concept has beenlocated, we add title of ?redirect?
pages that leadsto the same concept in addition to itssubcategories from Wikipedia?s categorizationsystem.
If not, we depend on the other twodictionaries ?Al Raed and Google_WordNet- forexpansion.We investigated two methodologies for queryexpansion; the first is the most common queryexpansion methodology which is to produce asingle expanded query that contains all expandedterms.
The second methodology we introduced isto expand each term one at a time producingmultiple queries, and then combine the results ofthese queries into a single result list.
The secondmethodology was found less sensitive to noise90because for each expanded query, there is onlyone source of noise which is the term beingexpanded, while other terms are left withoutexpansion.
It also allows the system to boostdocuments from one expanded query over otherdocuments according to the relevancy score of theexpanded term.The following example explains this intuition:For the query ???????
?????
?Single Expanded Query:)????
?OR   ????
?OR  ??????)
(??
?OR   ??????OR????
?OR  ????
?OR  ?????
?????
???
?OR  ???OR???
????
???
(Multiple Expanded Queries:1-)????
?OR  ????
?OR  ??????
(??
?2- ??????)
????
?OR  ?????
?OR  ????
?OR  ????
?OR ?????
?????
???
?OR  ??
?OR  ???
????
???
(We see that the term ???????
gets fewerexpansions than the term ????????
; this isbecause the term ????????
is less frequent in thecorpus thus it needs more expansions.
We thencombine the results of the two queries by thefollowing algorithm:1- Foreach expanded querya.
Foreach retrieved documentforb.
If the final list containsincrement the score of    by(      )c. Else add     to final listWhere    is a list of relevancy factorscalculated for each term in the original query.
Thisfactor depends on the term frequency in corpus.is calculated according to the followingformula:(                     (                   )Where   is the term we need to calculate itsrelevancy score,              is the numbers oftimes the term   appeared in the corpus, andis the number of timeswords that share the same stem of the termappeared in the corpus.
Then we sort the final listin ascending order according to their scores.Note that the multiple expanded queriesmethodology consumes more time over the singleexpanded query.
This is because each expandedquery is sent to LUCENE separately.
Then wecombine the returned documents lists of thequeries into a final documents list.We also limit the maximum number of addedterms for each term in order to reduce the noiseeffect of query expansion step; this maximumnumber also depends on the term?s relevancyfactor.
We set the maximum number of addedterms to a single query to 50.
Each term getsexpanded with number of terms proportional to itsrelevancy score.
This also increases the recall asless frequent terms gets expanded more times thanmost frequent terms, allowing LUCENE to findmore relevant pages for infrequent terms.5 ExperimentsFor testing our system, we used a data setconstructed from ?Zad Al Ma?ad?
book written bythe Islamic scholar ?Ibn Al-Qyyim?.
The data setcontains 25 queries and 2730 documents.
Titles ofthe book chapters are used as ?Queries?
andsections of each chapter are used as set of relevantdocuments for that query.
Each query is testedagainst the whole sections.The following tables show the values ofprecision, recall, f-score, and NDCG (NormalizeDiscounted Cumulative Gain) of three runs.R1: No expansion is used (base line).R2: Single expanded query.R3: Multiple expanded queries methodology.R1 R2 R3Precision @1 0.68 0.6 0.72Precision @5 0.504 0.576 0.568Precision @10 0.38 0.436 0.444Precision @20 0.268 0.3 0.326Precision @30 0.2038 0.232 0.2546Table : Levels of PrecisionR1 R2 R3Recall @1 0.1346 0.1067 0.1361Recall @5 0.3258 0.35721 0.3465Recall @10 0.3908 0.4292 0.4390Recall @20 0.4804 0.5487 0.5393Recall @30 0.5089 0.5806 0.594491Table : Levels of RecallR1 R2 R3F-score @1 0.1919 0.1535 0.1948F-score @5 0.3249 0.3635 0.3528F-score @10 0.3067 0.3466 0.3516F-score @20 0.2701 0.3122 0.3243F-score @30 0.2334 0.2697 0.2868Table : Levels of F-ScoreR1 R2 R3NDCG @1 0.68 0.6 0.72NDCG @5 0.8053 0.8496 0.8349NDCG @10 0.7659 0.8304 0.8316NDCG @20 0.7392 0.7993 0.8186NDCG @30 0.7323 0.7944 0.8001Table : Levels of NDCG6 ConclusionIn this paper we introduced a new techniquefor semantic query expansion using a domainindependent semantic ontology constructed fromArabic Wikipedia.
We focused on four featuresfor semantic search: (1) Handling Generalizations.
(2) Handling Morphological Variants.
(3)Handling Concept Matches.
(4) HandlingSynonyms with correct senses.
We compared bothsingle expanded query and multiple expandedqueries approaches against the traditional keywordbased search.
Both techniques showed betterresults than the base line.
While the MultipleExpanded Queries approach performed better thanSingle Expanded Query in most levels.7 ACKNOWLEDGMENTThe authors wish to thank the anonymousreviewers for their constructive comments andsuggestions.
This work was supported byRDI?
(http://www.rdi-eg.com/)8 ReferencesDavid Milne Ian H. Witten David M. Nichols.
2007.
Aknowledge-based search engine powered byWikipedia.
Conference on Information andKnowledge Management (CIKM).Jinxi Xu, Alexander Fraser and Ralph Weischedel.2001.
Cross-lingual Retrieval at BBN.
TREC10Proceedings.Lahsen Abouenour, Karim Bouzouba, and PaoloRosso.
2010.
An evaluated semantic queryexpansion and structure-based approach forenhancing Arabic question/answering.
InternationalJournal on Information and CommunicationTechnologies.Leah S. Larkey and Margaret E. Connell.
2001.
ArabicInformation Retrieval at UMass.
TREC10Proceedings.Leah S. Larkey and Lisa Ballesteros and Margaret E.Connell.
2007.
Arabic ComputationalMorphology Text, Speech and LanguageTechnology.Musa Alkhalifa and Horacio Rodrguez.
2008.Automatically Extending Named Entities coverageof Arabic WordNet using Wikipedia.
InternationalJournal on Information and CommunicationTechnologies.Simone Paolo Ponzetto and Michael Strube.
2007.Deriving a large scale taxonomy from Wikipedia.AAAI'07 Proceedings of the 22nd nationalconference on Artificial intelligence.92
