A METHOD FOR IMPROVING AUTOMATIC  WORDCATEGORIZAT IONEmin Erkan Korkmaz GSktfirk U~olukDepartment of Computer EngineeringMiddle East Technical UniversityEmails: korkmaz@ceng.metu.edu.trucoluk@ceng.metu.edu.trAbstractThis paper presents a new approach toautomatic word categorization which im-proves both the efficiency of the algorithmand the quality of the formed clusters.
Theunigram and the bigram statistics of a cor-pus of about two million words are usedwith an efficient distance function to mea-sure the similarities of words, and a greedyalgorithm to put the words into clusters.The notions of fuzzy clustering like clusterprototypes, degree of membership are usedto form up the clusters.
The algorithm is ofunsupervised type and the number of clus-ters are determined at run-time.1 IntroductionStatistical natural anguage processing is a challeng-ing area in the field of computational natural an-guage learning.
Researchers of this field have anapproach to language acquisition in which learningis visualised as developing a generative, stochasticmodel of language and putting this model into prac-tice (de Marcken, 1996).
It has been shown practi-cally that the usage of such an approach can yieldbetter performances for acquiring and representingthe structure of language.Automatic word categorization is an importantfield of application in statistical natural languageprocessing where the process is unsupervised and iscarried out by working on n-gram statistics to findout the categories of words.
Research in this areapoints out that it is possible to determine the struc-ture of a natural anguage by examining the regular-ities of the statistics of the language (Finch, 1993).The organization of this paper is as follows.
Afterthe related work in the area of word categorizationis presented in section 2, a general background ofthe categorization process is described in 3 section,which is followed by presentation ofnewly proposedmethod.
In section 4 the results of the experimentsare given.
We discuss the relevance of the resultsand conclude in the last section.2 Related WorkThere exists previous work in which the unigram andthe bigram statistics are used for automatic wordclustering.
Here it is concluded that the frequencyof single words and the frequencies of occurance ofword pairs of a large corpus can give the necessaryinformation to build up the word clusters.
Finch(Finch and Chater, 1992), makes use of these bigramstatistics for the determination ofthe weight matrixof a neural network.
Brown, (Brown et al, 1992)uses the same bigrams and by means of a greedyalgorithm forms the hierarchical clusters of words.Genetic algorithms have also been successfulyused for the categorization process.
Lanchorst(Lankhorst, 1994) uses genetic algorithms to deter-mine the members of predetermined classes.
Thedrawback of his work is that the number of classesis determined previous to run-time and the geneticalgorithm only searches for the membership ofthoseclasses.McMahon and Smith (McMahon and Smith,1996) also use the mutual information of a corpusto find the hierarchical clusters.
However instead ofusing a greedy algorithm they use a top-down ap-proach to form the clusters.
Firstly by using themutual information the system divides the initialset containing all the words to be clustered into twoparts and then the process continues on these newclusters iteratively.Statistical NLP methods have been used also to-gether with other methods of NLP.
Wilms (Wilms,1995) uses corpus based techniques together withknowledge-based t chniques in order to induce a lex-ical sublanguage grammar.
Machine Translation isan other area where knowledge bases and statisticsKorkmaz ~ U~oluk 43 Automatic Word CategorizationEmin Erkan Korkmaz and GSktiirk U~oluk (1997) A Method for Improving Automatic Word Categorization.In T.M.
EUison (ed.)
CoNLL97: Computational Natural Language Learning, ACL pp 43-49.
(~) 1997 Association for Computational Linguisticsare integrated.
Knight, (Knight et al, 1994) aims toscale-up grammar-based, knowledge-based MT tech-niques by means of statistical methods.3 Word CategorizationThe words in a natural language can be visualisedas consisting of two different sets.
The closed classwords and the open class ones.
New open class wordscan be added to the language as the language pro-gresses, however the closed class is a fixed one andno new words are added to the set.
For instance theprepositions are in the closed class.
However nounsare in the open class, since new nouns can be addedto the language as the social and economical life pro-gresses.
However the most frequently used words ina natural language usually form a closed class.Zipf, (Zipf, 1935), who is a linguist, was one ofthe early researchers on statistical language models.His work states that only 2% of the words of a largeEnglish corpus form 66% of the total corpus.
There-fore, it can be claimed that by working on a smallset consisting of frequent words it is possible to builda framework for the whole natural language.N-gram models of language are commonly used tobui ld up such framework.
An n-gram model canbe formed by collecting the probabilities of wordstreams (will : 1..n).
The probabilities will beused to form the model where we can predict thebehaviour of the language up to n words.
There ex-ists current research that use bigram statistics forword categorization i  which probabilities of wordpairs in the text are collected and processed.3.1 Mutua l  In fo rmat ionAs stated in the related work part these n-gram mod-els can be used together with the concept of mutualinformation to form the clusters.
Mutual Informa-tion is based on the concept of entropy which can bedefined informally as the uncertainty of a stochas-tic experiment.
Let X be a stochastic variable de-fined over the set X = {Xl,X2,...,x,~} where theprobabilities Px(xi) are defined for 1 _< i _~ n asPx(xi) = P(X = xi) then the entropy of X, H(X)is defined as:H{X}=-  E Px(x,)logPx(x,) (1)And if Y is another stochastic variable than themutual information between these two stochasticvariables is defined as:I{X:  V} : H{X} + H{Y} - H{X,Y} (2)Korkmaz ~ O~olukHere H{X, Y} is the joint entropy of the stochas-tic variables X and Y.
The joint entropy is definedas :44H{X,Y}=-  E E P~u(x"YJ)l?gP~u(x"YJ)(3)And in this formulation Pxu(xi, yj) is the jointprobability defined as P~u(xi, yj) : P(X : x~, Y :Given a lexicon space W = {wl, w2, ..., w,} con-sisting of n words to be clustered, we can use theformulation of mutual information for the bigramstatistics of a natural language corpus.
In this for-mulation X and Y are defined over the sets of thewords appearing in the first and second positions re-spectively.
So the mutual information that is theamount of knowledge that a word in a corpus cangive about the proceeding word can be reformulatedusing the bigram statistics as follows:N~j , Nij.
N** i tx:Y}= (4)l _< i _< n l _< j _< nIn this formulation N** is the total number ofword pairs in the corpus and N~j is the number ofoccurances of word pair (wordi, wordj), Ni.
is thenumber of occurences of wordi and N.j  is the num-ber of occurences of wordj respectively.
This formu-lation denotes the amount of linguistic knowledgepreserved in bigram of words in a natural anguage.3.2 C lus ter ing  ApproachWhen the mutual information is used for clustering,the process is carried out somewhat at a macro-level.Usually search techniques and tools are used to-gether with the mutual information in order to formsome combinations of different sets each of which isthen subject to some validity test.
The idea usedfor the validity testing process is as follows.
Sincethe mutual information denotes the amount of prob-abilistic knowledge that a word provides on the pro-ceeding word in a corpus, if similar behaving wordsare collected together to the same clusters than theloss of mutual information would be minimal.
Sothe search is among possible alternatives for sets orclusters with the aim to obtain a minimal loss inmutual information.Although this top-to-bottom ethod seems theo-retically possible, in the presented work a differentapproach, which is bottom-up is used.
In this incre-mental approach, set prototypes are first built andthen combined with other sets or single words toAutomatic Word Categorizationform larger ones.
The method is based on the sim-ilarities or differences between single words ratherthan the mutual information of a whole corpus.
Incombining words into sets a fuzzy set approach isused.
The authors believe that this serves to deter-mine the behavior of the whole set more properly.Using this constructive approach, it is possible tovisualize the word clustering problem as the prob-lem of clustering points in an n-dimensional spaceif the lexicon space to be clustered consists of nwords.
The points that are the words in a corpusare positioned on this n-dimensional space accord-ing to their behaviour elated to other words in thelexicon space.
Each wordi is placed on the i th di-mension according to its bigram statistic with theword representing the dimension.
So the degree ofsimilarity between two words can be defined as hav-ing close bigram statistics in the corpus.
Wordsare distributed in the plane according to those bi-gram statistics.
The idea is quite simple: Let wland w~ be two words from the corpus.
Let Z bethe stochastic variable ranging over the words to beclustered.
Then if Px (Wl, Z) is close to Px (w2, Z)and if Px(Z, wl) is close to Px(Z, w2) for Z rang-ing over all the words to be clustered in the corpus,than we can state a closeness between the words Wland w2.
Here Px is the probability of occurences ofword pairs as stated in section 3.1.
Px(wl, Z) is theprobability where Wl appears as the first element ina word pair and Px(Z, wl) is the reverse probabil-ity where wl is the second element of the word pair.This is the same for w2 respectively.In order to start the clustering process, a distancefunction has to be defined between the elements inour plane.
Using the idea presented above we definea simple distance function between words using thebigram statistics.
The distance function D betweentwo words wl and w2 is defined as follows:D(wl,  w2) = Dl(Wl, w2) + D2(wl,w2)where(5)Dl(Wl,W2) : ~ \] Px(wl,Wl)- Px(w2,wl) ll<i<n(6)andD~(wl, w2) = ~ I PX(W,, Wl) - -  Px(wi, w2) \]l<i<n(7)Korkmaz ~ O~oluk 45Here n is the total number of words to be clus-tered.
Since Px(wi wj) is defined as ~ the pro-' Nee  'portion of the number of occurences of word pairwi and wj to the total number of word pairs in thecorpus, the distance function for wl and w2 reducesdown to:D(wl, w2) = ~ \[ N~li- Nw2i \ [+  IN/w1 -Nivo2 \[l<#.<n(8)Having such a distance function, it is possible tostart the clustering process.
The first idea that canbe used is to form a greedy algorithm to start form-ing the hierarchy of word clusters.
If the lexiconspace to be clustered consists of {Wl, w2, ..., wn},then the first element from the lexicon space wl istaken and a cluster with this word and its near-est neighbour or neighbors is formed.
Then thelexicon space is {(Wl, W,,, ..., Wsk), Wi, ..., Wn) where(wl, ws,, ..., wsk) is the first cluster formed.
The pro-cess is repeated with the first element in the listthat is outside the formed sets, wi for our case andthe process iterates until no word is left not beinga member of any set.
The formed sets will be theclusters at the bottom of the cluster hierarchy.
Thento determine the behaviour of a set, the frequenciesof its elements are added and the previous processis carried on the sets this t ime rather than on sin-gle words until the cluster hierarchy is formed, sothe algorithm stops when a single set is formed thatcontains all the words in the lexicon space.In the early stages of this research such a greedymethod was used to form the clusters, however,though some clusters at the low levels of the treeseemed to be correctly formed, as the number ofelements in a cluster increased towards the higherlevels, the clustering results became unsatisfactory.Two main factors were observed as the reasons forthe unsatisfactory results.These were:?
Shortcomings of the greedy type algorithm.?
Inadequency of the method used to obtain theset behaviour from its element's properties.The greedy method results in a nonoptimal clus-tering in the initial level.
To make this point clearconsider the following example: Let us assume thatfour words wl,w2, w3 and w4 are forming the lexiconspace.
And let the distances between these wordsbe defined as d~,~j.
Then consider the distribu-tion in Figure 1.
If the greedy method first tries tocluster wl.
Then it will be clustered with w2, sinceAutomatic Word CategorizationSet ILEXICON SPACE' ~  ~ 3 (?=pected)Figure 1: Example  for the c luster ing prob lem of greedy al-gor i thm in a lexicon space with four  different words.
Note  thatd~,~ 3 is the smal lest  d is tance in the d istr ibut ion.
However  sincewl  is taken into cons iderat ion,  it forms set l  wi th its nearest  neigh-bour  w~ and ws combines wi th  w4 and form set2, a l though w~ isnearer .
And  the expected  th i rd  set is not  formed.the smallest d~l,w , for the first word is d~l,w~.
Sothe second word will be captured in the set and thealgorithm will skip w2 and continue the clusteringprocess with w3.
At this point, though w3 is clos-est to w2, since it is captured in a set and since w3is more closer to w4 than the center of this set is,a new cluster will be formed with members w3 andw4.
However, as it can be obviously seen visuallyfrom Figure 1 the first optimal cluster to be formedbetween these four words is the set {w2, w3}.The second problem causing unsatisfactory clus-tering occurs after the initial sets are formed.
Ac-cording to the algorithm after each cluster is formed,the clusters behave exactly like other single wordsand get into clustering with other clusters or singlewords.
However to continue the process, the bigramstatistics of the clusters or in other words the com-mon behaviour of the elements in a cluster should bedetermined so that the distance between the clusterand other elements in the search space could be cal-culated.
One easy way to determine this behaviouris to find the average of the statistics of all the ele-ments in a cluster.
This method has its drawbacks.The points in the search space for the natural lan-guage application are very close to each other.
Fur-thermore, if the corpus used for the process is notlarge, the proximity problem becomes more severe.On the other hand the linguistic role of a word mayvary in contexts in different sentences.
Many wordsare used as noun, adjective or falling into some otherlinguistic category depending on the context.
It canbe claimed that each word initially shall be placed ina cluster according to its dominant role.
However todetermine the behaviour of a set the dominant rolesof its elements hould also be used.
Somehow thecommon properties (bigrams) of the elements houldbe always used and the deviations of each elementshould be eliminated in the process.3.2.1 Improving the Greedy  MethodThe clustering process is improved to overcomethe above mentioned rawbacks.The idea used to find the optimal cluster for eachword at the initial step is to form up such initial clus-ters in the algorithm used in which words are allowedto be members of more than one class.
So after thefirst pass over the lexicon space, intersecting clus-ters are formed.
For the lexicon space presented inFigure 1 with four words, the expected third set isalso formed.
As the second step these intersectingsets are combined into a single set.
Then the clos-est two words (according to the distance function)are found in each combined set and these two closestwords are taken into consideration as the prototypefor that set.
After finding the centroids of all sets,the distances between a member and all the cen-troids are calculated for all the words in the lexiconspace.
Following this, each word is moved to the setwhere the distance between this member and the setcenter is minimal.
This procedure is necessary sincethe initial sets are formed with combining the inter-secting sets.
When these intersecting sets are com-bined the set center of the resulting set might be faraway from some elements and there may be othercloser set centers formed with other combinations,so a reorganization of membership is appropriate.3.2.2 Fuzzy  Membersh ipAs presented in the previous ection the clusteringprocess builds up a cluster hierarchy.
In the firststep, words are combined to form the initial clusters,then those clusters become members of the processthemselves.
To combine clusters into new ones thestatistical behaviour of them should be determined,since bigram statistics are used for the process.
Thestatistical behaviour ~of a cluster is related to thebigrams of its members.
In order to find out thedominant statistical role of each cluster the notionof fuzzy membership is used.The problem that each word can belong to morethan one linguistic category brings up the idea thatthe sets of word clusters cannot have crisp border-lines and even i fa word is in a set due to its dominantlinguistic role in the corpus, it can have a degree ofmembership to the other clusters in the search space.Therefore fuzzy membership can be used for deter-mining the bigram statistics of a cluster.Researchers working on fuzzy clustering presenta framework for defining fuzzy membership of ele-ments.
Gath and Geva (Gath and Geva, 1989) de-scribe such an unsupervised optimal fuzzy cluster-ing.
They present he K-means algorithm based onminimization of an objective function.
For the pur-Korkmaz 8t Ufoluk 46 Automatic Word Categorizationthe 5.002056%and 3.281249%to 2.836796%of 2.561952%a 2.107116%m 1.591189%he 1.533916%was 1.419838%that  1.306431%his 1.124362%it 1.061797%Table 1: Frequencies of the most frequent en wordspose of this research only the membership functionof the presented algorithm is used.
The membershipfunction uij that is the degree of membership of thei th element o the jth cluster is defined as:~x77~y= -K 1 ir - (9) Ek=l  I  X77 5Here Xi denotes an element in the search space,l,~ is the centroid of the jth cluster.
K denotes thenumber of clusters.
And d2(Xi, ~)  is the distanceof Xith element o the centroid I,~ of the jth cluster.The parameter q is the weighting exponent for ztijand controls the fuzziness of the resulting cluster.After the degrees of membership for all the ele-ments of all classes in the search space are calculated,the bigram statistics of the classes are added up.
Tofind those statistics the following method is used:The bigram statistics of each element is multipliedwith the degree of the membership of the element inthe working set and this forms the amount of statis-tical knowledge passed from the element o that set.So the elements chosen as set centroids will be theones that affect a set's statistical behaviour mostly.Hence an element away from a centroid will have alesser statistical contribution.4 Resu l tsThe algorithm is tested on a corpus formed with on-line novels collected from the www page of the "BookStacks Unlimited, Inc." The corpus consists of twelvefree on-line novels adding up to about 1.700.000words.
The corpus is passed through a filtering pro-cess where the special words, useless characters andwords are filtered and the frequencies of words arecollected.
Then the most frequent housand wordsare chosen and they are sent to the clustering processdescribed in the previous sections.
These most fre-quent thousand words form the 70.4% of the wholecorpus.
The percentage goes up to about 77% if thenext most frequent thousand is added to the lexi-con space.
The first ten most frequent words in theKorkmaz ~ Ufolukcorpora and their frequencies are presented in Table1.The clustering process builds up a tree of wordshaving words on the leaves and clusters on the in-ner nodes.
The starting node denotes the largestclass containing all the lexicon space.
The numberof leaves that is the number of clusters formed atthe initial step is 60.
The depth of the tree is 8.Leaves appear starting from the 5th level and theyare mainly located at the 5th and 6th level.
Thenumber of nodes connecting the initial clusters is 18.So on the average about three clusters are combinedtogether in the second step.
Table 2 displays two re-sults from the clustering tree.
The first one collectsa set of nouns from the lexicon space.
However thesecond one is somewhat ill-structured namely twoprepositions, two adjectives and a verb cluster arecombined into one.Some linguistic categories inferred by the algo-r ithm are listed below:.
prepos i t ions( l ) :  by wi th  in to and  ofs preposi t ions(2) :  f rom on at  fors preposi t ions(3) :  must  might  will should could would mays determiners ( l )  : your  its our  these some this my her  all anyno.
preposi t ions(4) :  between among aga inst  th rough underupon over about.
adject ives( l )  : large young smal l  good  long?
nouns( l )  : spir it  body  son head power  age character  deathsense par t  case s ta te?
verbs( l )  : exc la imed answered cr ied says  knew felt said o ris was  saw did asked gave took  made thought  e i ther  to ldwhether  repl ied because  though how repeated  openremained lived died lay does  why?
verbs(2) : shouted  wrote  showed spoke makes  droppedstruck laid kept held ra ised led car r ied  sent brought  rosedrove threw drew shook talked yourse l f  l istened wishedmeant  ought  seem seems seemed tr ied wanted  began usedcont inued re turned  appeared  comes  knows  l iked loved?
adjectives(2) : sad wonderfu l  special  f resh ser ious par t i cu la rpainful  terr ible p leasant  happy  easy hard  sweet?
nouns(2)  : boys  girls gent lemen ladiess adverbs( l )  : scarcely hard ly  ne i ther  probab ly?
verbs(3) : consider  emember  forget  suppose believe say doth ink know feel unders tand?
verbs(4) : keeping car ry ing  put t ing  turn ing  shut  ho ld ingget t ing  hear ing  knowing  f inding drawing  leaving giv ing tak-ing mak ing  hav ing  be ing seeing do ings nouns(3)  : streets vi l lage window evening morn ing  n ightmiddle rest end road  sun garden table  room ground doorchurch world name people city year  day  t ime house count ryway place fact  r iver next  ear th$ nouns(4)  : beauty  conf idence p leasure interest  fo r tune  hap-piness tears47 Automatic Word Categorizationbccccbccccbr,,rirs ~o@n~i~i!inpassion questions books mannermarriage ideas storyspeech faces soundfeelings coursedistancepointdaughterfriendfamilychildrenmenbcccccty/ /oa~t confidencepleasuresociety sun interestaction garden fortunetable happines room tears grounddoorchurchworldnamepeoplecitybetweenamongagainstthroughunderuponoveraboutintobeforeafterthanlikebbbccdown small twenty five giventen taken out good threeup long twoTable 2: Examples from the cluster hierarchyKorkmaz ~ 09oluk 48 Automatic Word CategorizationThe ill-placed members in the clusters are shownabove using bold font.
The clusters represent the lin-guistic categories with a high success rate (,~ 91%).Some semantic relations in the clusters can also beobserved.
Group nouns(2) is a good example forsuch a semantic relation between the words in a clus-ter.5 Discussion And ConclusionIt can be claimed that the results obtained in thisresearch are encouraging.
Although the corpus usedfor the clustering is quite small compared to otherresearches, the clusters formed seem to represent thelinguistic ategories.
It is believed that the incorrectones are due to the poorness of the knowledge con-veyed through the corpus.
With a larger trainingdata, an increase in the convergence of frequencies,thus an increase in the quality of clusters is expected.Since the distance function depends on only the dif-ference of the bigram statistics, the running time ofthe algorithm is quite low compared to algorithmsusing mutual information.
Though the complexity ofthe two algorithms are the same there is an increasein the efficiency due to the lack of time consumingmathematical operations like division and multipli-cation needed to calculate the mutual informationof the whole corpus.This research as focussed on adding fuzziness tothe categorization process.
Therefore different simi-larity metrics have not been tested for the algorithm.For further research the algorithm could be testedwith different distance metrics.
The metrics fromthe statistical theory given in (de Marcken, 1996)could be used to improve the algorithm.
Also thealgorithm could be used to infer the phrase struc-ture of a natural anguage.
Finch (Finch, 1993)again uses the mutual information to find out suchstructures.
Using fuzzy membership degrees couldbe another way to repeat he same process.
To findout the phrases, most frequent sentence segments ofsome length could be collected from a corpus.
In ad-dition to the frequencies and bigrams of words, thestatistics for these frequent segments could be gath-ered and then they could also be passed to the clus-tering inference mechanism and the resulting clus-ters would then be expected to hold such phrasestogether with the words.To conclude, it can claimed that automatic wordcategorization is the initial step for the acquisitionof the structure in a natural language and the samemethod could be used with modifications and im-provements o find out more abstract structures inthe language and moving this abstraction up to thesentence l vel succesfuly might make it possible forKorkmaz ~ (/~oluka computer to acquire the whole grammar of anynatural language automatically.Re ferencesBrown, P.F., V.J.
Della Pietra, P.V.
de Souza,J.C.
Lai, and R.L.
Mercer.
Class-based n-grammodels of natural language.
ComputationalLinguistics, 18(4):467-477, 1992Finch, Steven.
Finding Structure in language.
PhDThesis.
Centre for Cognitive Science, Univer-sity of Edinburgh, 1993.Finch, S. and N. Chater, Automatic methods forfinding linguistic categories.
In Igor Alexan-der and John Taylor, editors, Artificial NeuralNetworks, volume 2.
Elsevier Science Publish-ers, 1992Gath, I and A.B.
Geva.
Unsupervised OptimalFuzzy Clustering.
IEEE Transactions on pat-tern analysis and machine intelligence, Vol.
11,No.
7, July 1989.Knight, Kevin, Ishwar Chander, Matthew Haines,Vasieios Hatzivassiloglou, Eduard Hovy, IidaMasayo, Steve Luk, Okumura Akitoshi,Richard Whitney, Yamada Kenji.
Integrat-ing Knowledge Bases and Statistics in MT.Proceedings of the 1st AMTA Conference.Columbia, MD.
1994.Lankhorst, M.M.
A Genetic Algorithm for Auto-matic Word Categorization.
In: E.
Backer(ed.
), Proceedings ofComputing Science in theNetherlands CSN'94, SION, 1994, pp.
171-182.de Marcken, Carl G. Unsupervised Language Acqui-sition PhD Thesis, 1996.McMahon, John G. and Francis J. Smith.
Improv-ing Statistical Language Model Performancewith Automatically Generated Word Hierar-chies.
Computational Linguistics, 1996.Wilms, Geert Jan.
Automated Induction of a Lex-ical Sublanguage Grammar Using a HybridSystem of Corpus and Knowledge-Based Tech-niques.
Mississippi State University.
PhD The-sis, 1995.Zipf, G. K. The psycho-biology of Language.
Boston:Houghton Mifflin.
1935.49 Automatic Word Categorization
