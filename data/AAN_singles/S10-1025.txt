Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 123?128,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSWAT: Cross-Lingual Lexical Substitution using Local Context Matching,Bilingual Dictionaries and Machine TranslationRichard Wicentowski, Maria Kelly, Rachel LeeDepartment of Computer ScienceSwarthmore CollegeSwarthmore, PA 19081 USArichardw@cs.swarthmore.edu, {mkelly1,rlee1}@sccs.swarthmore.eduAbstractWe present two systems that select themost appropriate Spanish substitutes fora marked word in an English test sen-tence.
These systems were official en-tries to the SemEval-2010 Cross-LingualLexical Substitution task.
The first sys-tem, SWAT-E, finds Spanish substitutionsby first finding English substitutions inthe English sentence and then translatingthese substitutions into Spanish using anEnglish-Spanish dictionary.
The secondsystem, SWAT-S, translates each Englishsentence into Spanish and then finds theSpanish substitutions in the Spanish sen-tence.
Both systems exceeded the base-line and all other participating systems bya wide margin using one of the two officialscoring metrics.1 IntroductionWe present two systems submitted as official en-tries to the SemEval-2010 Cross-Lingual LexicalSubstitution task (Mihalcea et al, 2010).
In thistask, participants were asked to substitute a singlemarked word in an English sentence with the mostappropriate Spanish translation(s) given the con-text.
On the surface, our two systems are very sim-ilar, performing monolingual lexical substitutionand using translation tools and bilingual dictionar-ies to make the transition from English to Spanish.2 ScoringThe task organizers used two scoring metricsadapted from the SemEval-2007 English LexicalSubstitution task (McCarthy and Navigli, 2007).For each test item i, human annotators provided amultiset of substitutions, Ti, that formed the goldstandard.
Given a system-provided multiset an-swer Sifor test item i, the best score for a sin-gle test item is computed using (1).
Systems wereallowed to provide an unlimited number of re-sponses in Si, but each item?s best score was di-vided by the number of answers provided in Si.best score =?s?Sifrequency(s ?
Ti)|Si| ?
|Ti|(1)The out-of-ten score, henceforth oot, limited sys-tems to a maximum of 10 responses for each testitem.
Unlike the best scoring method, the finalscore for each test item in the oot method is not di-vided by the actual number of responses providedby the system; therefore, systems could maximizetheir score by always providing exactly 10 re-sponses.
In addition, since Siis a multiset, the10 responses in Sineed not be unique.oot score =?s?Sifrequency(s ?
Ti)|Ti|(2)Further details on the oot scoring method and itsimpact on our systems can be found in Section 3.4.The final best and oot score for the system iscomputed by summing the individual scores foreach item and, for recall, dividing by the numberof tests items, and for precision, dividing by thenumber of test items answered.
Our systems pro-vided a response to every test item, so precisionand recall are the same by this definition.For both best and oot, the Mode recall (simi-larly, Mode precision) measures the system?s abil-ity to identify the substitute that was the annota-tors?
most frequently chosen substitute, when sucha most frequent substitute existed (McCarthy andNavigli, 2007).3 SystemsOur two entries were SWAT-E and SWAT-S. Bothsystems used a two-step process to obtain a rankedlist of substitutes.
The SWAT-E system first used amonolingual lexical substitution algorithm to pro-vide a ranked list of English substitutes and then123these substitutes were translated into Spanish toobtain the cross-lingual result.
The SWAT-S sys-tem performed these two steps in the reverse or-der: first, the English sentences were translatedinto Spanish and then the monolingual lexical sub-stitution algorithm was run on the translated out-put to provide a ranked list of Spanish substitutes.3.1 Syntagmatic coherenceThe monolingual lexical substitution algorithmused by both systems is an implementation of thesyntagmatic coherence criterion used by the IRST2system (Giuliano et al, 2007) in the SemEval-2007 Lexical Substitution task.For a sentence Hwcontaining the target wordw, the IRST2 algorithm first compiles a set, E, ofcandidate substitutes for w from a dictionary, the-saurus, or other lexical resource.
For each e ?
E,Heis formed by substituting e for w in Hw.
Eachn-gram (2 ?
n ?
5) of Hecontaining the sub-stitute e is assigned a score, f , equal to how fre-quently the n-gram appeared in a large corpus.For all triples (e, n, f) where f > 0, we add(e, n, f) to E?.
E?is then sorted by n, with tiesbroken by f .
The highest ranked item in E?, there-fore, is the triple containing the synonym e thatappeared in the longest, most frequently occurringn-gram.
Note that each candidate substitute e canappear multiple times in E?
: once for each valueof n.The list E?becomes the final output of the syn-tagmatic coherence criterion, providing a rankingfor all candidate substitutes in E.3.2 The SWAT-E system3.2.1 ResourcesThe SWAT-E system used the English Web1T 5-gram corpus (Brants and Franz, 2006), the Span-ish section of the Web1T European 5-gram cor-pus (Brants and Franz, 2009), Roget?s online the-saurus1, NLTK?s implementation of the LancasterStemmer (Loper and Bird, 2002), Google?s onlineEnglish-Spanish dictionary2, and SpanishDict?sonline dictionary3.
We formed a single Spanish-English dictionary by combining the translationsfound in both dictionaries.1http://thesaurus.com2http://www.google.com/dictionary3http://www.spanishdict.com3.2.2 Ranking substitutesThe first step in the SWAT-E algorithm is to createa ranked list of English substitutes.
For each En-glish test sentence Hwcontaining the target wordw, we use the syntagmatic coherence criterion de-scribed above to create E?, a ranking of the syn-onyms of w taken from Roget?s thesaurus.
We usethe Lancaster stemmer to ensure that we count allmorphologically similar lexical substitutes.Next, we use a bilingual dictionary to convertour candidate English substitutes into candidateSpanish substitutes, forming a new ranked list S?.For each item (e, n, f) in E?, and for each Spanishtranslation s of e, we add the triple (s, n, f) to S?.Since different English words can have the sameSpanish translation s, we can end up with multipletriples in S?that have the same values for s and n.For example, if s1is a translation of both e1ande2, and the triples (e1, 4, 87) and (e2, 4, 61) appearin E?, then S?will contain the triples (s1, 4, 87)and (s1, 4, 61).
We merge all such ?duplicates?
bysumming their frequencies.
In this example, wewould replace the two triples containing s1with anew triple, (s1, 4, 148).
After merging all dupli-cates, we re-sort S?by n, breaking ties by f .
No-tice that since triples are merged only when both sand n are the same, Spanish substitutes can appearmultiple times in S?
: once for each value of n.At this point, we have a ranked list of candi-date Spanish substitutes, S?.
From this list S?,we keep only those Spanish substitutes that aredirect translations of our original word w. Thereason for doing this is that some of the transla-tions of the synonyms of w have no overlappingmeaning with w. For example, the polysemousEnglish noun ?bug?
can mean a flaw in a com-puter program (cf.
test item 572).
Our thesauruslists ?hitch?
as a synonym for this sense of ?bug?.Of course, ?hitch?
is also polysemous, and not ev-ery translation of ?hitch?
into Spanish will havea meaning that overlaps with the original ?bug?sense.
Translations such as ?enganche?, havingthe ?trailer hitch?
sense, are certainly not appro-priate substitutes for this, or any, sense of the word?bug?.
By keeping only those substitutes that arealso translations of the original word w, we main-tain a cleaner list of candidate substitutes.
We callthis filtered list of candidates S.3.2.3 Selecting substitutesFor each English sentence in the test set, we nowhave a ranked list of cross-lingual lexical substi-1241: best = {(s1, n1, f1)}2: j ?
23: while (nj== n1) and (fj?
0.75?f1) do4: best?
best ?
{(sj, nj, fj)}5: j ?
j + 16: end whileFigure 1: The method for selecting multiple an-swers in the best method used by SWAT-Etutes, S. In the oot scoring method, we selectedthe top 10 substitutes in the ranked list S. If therewere less than 10 items (but at least one item) inS, we duplicated answers from our ranked list un-til we had made 10 guesses.
(See Section 3.4 forfurther details on this process.)
If there were noitems in our ranked list, we returned the most fre-quent translations of w as determined by the un-igram counts of these translations in the SpanishWeb1T corpus.For our best answer, we returned multiple re-sponses when the highest ranked substitutes hadsimilar frequencies.
Since S was formed by trans-ferring the frequency of each English substitute eonto all of its Spanish translations, a single Englishsubstitute that had appeared with high frequencywould lead to many Spanish substitutes, each withhigh frequencies.
(The frequencies need not be ex-actly the same due to the merging step describedabove.)
In these cases, we hedged our bet by re-turning each of these translations.Representing the i-th item in S as (si, ni, fi),our procedure for creating the best answer can befound in Figure 1.
We allow all items from S thathave the same value of n as the top ranked itemand have a frequency at least 75% that of the mostfrequent item to be included in the best answer.Of the 1000 test instances, we provided a single?best?
candidate 630 times, two candidates 253times, three candidates 70 times, four candidates30 times, and six candidates 17 times.
(We neverreturned five candidates).3.3 SWAT-S3.3.1 ResourcesThe SWAT-S system used both Google?s4and Ya-hoo?s5online translation tools, the Spanish sectionof the Web1T European 5-gram corpus, Roget?sonline thesaurus, TreeTagger (Schmid, 1994) for4http://translate.google.com/5http://babelfish.yahoo.com/morphological analysis and both Google?s and Ya-hoo?s6English-Spanish dictionaries.
We formeda single Spanish-English dictionary by combiningthe translations found in both dictionaries.3.3.2 Ranking substitutesTo find the cross-lingual lexical substitutes for atarget word in an English sentence, we first trans-late the sentence into Spanish and then use thesyntagmatic coherence criterion on the translatedSpanish sentence.In order to perform this monolingual Spanishlexical substitution, we need to be able to iden-tify the target word we are attempting to substitutein the translated sentence.
We experimented withusing Moses (Koehn et al, 2007) to perform themachine translation and produce a word alignmentbut we found that Google?s online translation toolproduced better translations than Moses did whentrained on the Europarl data we had available.In the original English sentence, the target wordis marked with an XML tag.
We had hoped thatGoogle?s translation tool would preserve the XMLtag around the translated target word, but that wasnot the case.
We also experimented with usingquotation marks around the target word instead ofthe XML tag.
The translation tool often preservedquotation marks around the target word, but alsoyielded a different, and anecdotally worse, transla-tion than the same sentence without the quotationmarks.
(We will, however, return to this strategyas a backoff method.)
Although we did not exper-iment with using a stand-alone word alignment al-gorithm to find the target word in the Spanish sen-tence, Section 4.3 provides insights into the possi-ble performance gains possible by doing so.Without a word alignment, we were left withthe following problem: Given a translated Span-ish sentence H , how could we identify the word wthat is the translation of the original English targetword, v?
Our search strategy proceeded as fol-lows.1.
We looked up v in our English-Spanish dictio-nary and searched H for one of these trans-lations (or a morphological variant), choosingthe matching translation as the Spanish targetword.
If the search yielded multiple matches,we chose the match that was in the most similarposition in the sentence to the position of v in6http://education.yahoo.com/reference/dict en es/125the English sentence.
This method identified amatch in 801 of the 1000 test sentences.2.
If we had not found a match, we translated eachword in H back into English, one word at atime.
If one of the re-translated words was asynonym of v, we chose that word as the targetword.
If there were multiple matches, we againused position to choose the target.3.
If we still had no match, we used Yahoo?s trans-lation tool instead of Google?s, and repeatedsteps 1. and 2. above.4.
If we still had no match, we reverted to usingGoogle?s translation tool, this time explicitlyoffsetting the English target word with quota-tion marks.In 992 of the 1000 test sentences, this four-stepprocedure produced a Spanish sentence Hwwitha target w. For each of these sentences, we pro-duced E?, the list of ranked Spanish substitutesusing the syntagmatic selection coherence crite-rion described in Section 3.1.
We used the Span-ish Web1T corpus as a source of n-gram counts,and we used the Spanish translations of v as thecandidate substitution set E. For the remaining8 test sentences where we could not identify thetarget word, we set E?equal to the top 10 mostfrequently occurring Spanish translations of v asdetermined by the unigram counts of these trans-lations in the Spanish Web1T corpus.3.3.3 Selecting substitutesFor each English sentence in the test set, we se-lected the single best item in E?as our answer forthe best scoring method.For the oot scoring method, we wanted to en-sure that the translated target word w, identified inSection 3.3.2, was represented in our output, evenif this substitute was poorly ranked in E?.
If w ap-peared in E?, then our oot answer was simply thefirst 10 entries in E?.
If w was not in E?, then ouranswer was the top 9 entries in E?followed by w.As we had done with our SWAT-E system, ifthe oot answer contained less than 10 items, werepeated answers until we had made 10 guesses.See the following section for more information.3.4 oot selection detailsThe metric used to calculate oot precision in thistask (Mihalcea et al, 2010) favors systems that al-ways propose 10 candidate substitutes over thosethat propose fewer than 10 substitutes.
For eachtest item the oot score is calculated as follows:oot score =?s?Sifrequency(s ?
Ti)|Ti|The final oot recall is just the average of thesescores over all test items.
For test item i, Siisthe multiset of candidates provided by the system,Tiis the multiset of responses provided by the an-notators, and frequency(s ?
Ti) is the number oftimes each item s appeared in Ti.Assume that Ti= {feliz, feliz, contento, ale-gre}.
A system that produces Si= {feliz, con-tento}would receive a score of2+14= 0.75.
How-ever a system that produces Siwith feliz and con-tento each appearing 5 times would receive a scoreof5?2+5?14= 3.75.
Importantly, a system that pro-duced Si= {feliz, contento} plus 8 other responsesthat were not in the gold standard would receivethe same score as the system that produced onlySi= {feliz, contento}, so there is never a penaltyfor providing all 10 answers.For this reason, in both of our systems, we en-sure that our oot response always contains exactly10 answers.
To do this, we repeatedly append ourlist of candidates to itself until the length of the listis equal to or exceeds 10, then we truncate the listto exactly 10 answers.
For example, if our orig-inal candidate list was [a, b, c, d], our final ootresponse would be [a, b, c, d, a, b, c, d, a, b].Notice that this is not the only way to producea response with 10 answers.
An alternative wouldbe to produce a response containing [a, b, c, d]followed by 6 other unique translations from theEnglish-Spanish dictionary.
However, we foundthat padding the response with unique answerswas far less effective than repeating the answersreturned by the syntagmatic coherence algorithm.4 Analysis of ResultsTable 1 shows the results of our two systems com-pared to two baselines, DICT and DICTCORP, andthe upper bound for the task.7Since all of thesesystems provide an answer for every test instance,precision and recall are always the same.
Theupper bound for the best metric results from re-turning a single answer equal to the annotators?most frequent substitute.
The upper bound for theoot metric is obtained by returning the annotator?smost frequent substitute repeated 10 times.7Details on the baselines and the upper bound can befound in (Mihalcea et al, 2010).126best ootSystem R Mode R R Mode RSWAT-E 21.5 43.2 174.6 66.9SWAT-S 18.9 36.6 98.0 79.0DICT 24.3 50.3 44.0 73.5DICTCORP 15.1 29.2 29.2 29.2upper bound 40.6 100.0 405.9 100.0Table 1: System performance using the two scor-ing metrics, best and oot.
All test instances wereanswered, so precision equals recall.
DICT andDICTCORP are the two baselines.Like the IRST2 system (Giuliano et al, 2007)submitted in the 2007 Lexical Substitution task,our system performed extremely well on the ootscoring method while performing no better thanaverage on the best method.
Further analysisshould be done to determine if this is due to aflaw in the approach, or if there are other factorsat work.4.1 Analysis of the oot methodOur oot performance was certainly helped by thefact that we chose to provide 10 answers for eachtest item.
One way to measure this is to scoreboth of our systems with all duplicate candidatesremoved.
We can see that the recall of both sys-tems drops off sharply: SWAT-E drops from 174.6to 36.3, and SWAT-S drops from 98.0 to 46.7.
Aswas shown in Section 3.4, the oot system shouldalways provide 10 answers; however, 12.8% ofthe SWAT-S test responses, and only 3.2% of theSWAT-E test responses contained no duplicates.
Infact, 38.4% of the SWAT-E responses containedonly a single unique answer.
Providing duplicateanswers allowed us to express confidence in thesubstitutes found.
If duplicates were forbidden,simply filling any remaining answers with othertranslations taken from the English-Spanish dic-tionary could only serve to increase performance.Another way to measure the effect of alwaysproviding 10 answers is to modify the responsesprovided by the other systems so that they, too,always provide 10 answers.
Of the 14 submittedsystems, only 5 (including our systems) provided10 answers for each test item.
Neither of the twobaseline systems, DICT and DICTCORP, provided10 answers for each test item.
Using the algorithmdescribed in Section 3.4, we re-scored each of thesystems with answers duplicated so that each re-sponse contained exactly 10 substitutes.
As shownfilled oot ootSystem R P R PSWAT-E 174.6 174.6 174.6 174.6IRST-1 126.0 132.6 31.5 33.1SWAT-S 98.0 98.0 98.0 98.0WLVUSP 86.1 86.1 48.5 48.5DICT 71.1 71.1 44.0 44.0DICTCORP 66.7 66.7 15.1 15.1Table 2: System performance using oot for thetop 4 systems when providing exactly 10 substi-tutes for all answered test items (?filled oot?
), aswell as the score as submitted (?oot?
).in Table 2, both systems still far exceed the base-line, SWAT-E remains the top scoring system, andSWAT-S drops to 3rd place behind IRST-1, whichhad finished 12th with its original submission.4.2 Analysis of oot Mode RAlthough the SWAT-E system outperformed theSWAT-S system in best recall, best Mode recall(?Mode R?
), and oot recall, the SWAT-S systemoutperformed the SWAT-E system by a large mar-gin in oot Mode R (see Table 1).
This result iseasily explained by first referring to the methodused to compute Mode recall: a score of 1 wasgiven to each test instance where the oot responsecontained the annotators?
most frequently chosensubstitute; otherwise 0 was given.
The average ofthese scores yields Mode R. A system can max-imize its Mode R score by always providing 10unique answers.
SWAT-E provided an average of3.3 unique answers per test item and SWAT-S pro-vided 6.9 unique answers per test item.
By provid-ing more than twice the number of unique answersper test item, it is not at all surprising that SWAT-Soutperformed SWAT-E in the Mode R measure.4.3 Analysis of SWAT-SIn the SWAT-S system, 801 (of 1000) test sen-tences had a direct translation of the target wordpresent in Google?s Spanish translation (identi-fied by step 1 in Section 3.3.2).
In these cases,the resulting output was better than those caseswhere a more indirect approach (steps 2-4) wasnecessary.
The oot precision on the test sentenceswhere the target was found directly was 101.3,whereas the precision of the test sentences wherea target was found more indirectly was only 84.6.The 8 sentences where the unigram backoff was127best ootSWAT-E P Mode P P Mode Padjective 25.94 50.67 192.78 85.78noun 22.34 40.44 197.87 59.11verb 18.62 41.46 155.16 55.12adverb 15.68 33.78 119.51 66.22SWAT-S P Mode P P Mode Padjective 21.70 40.00 126.41 86.67noun 24.77 45.78 107.85 82.22verb 13.58 27.80 69.04 71.71adverb 10.46 22.97 80.26 66.22Table 3: Precision of best and oot for both sys-tems, analyzed by part of speech.used had a precision of 77.4.
This analysis in-dicates that using a word alignment tool on thetranslated sentence pairs would improve the per-formance of the method.
However, since the pre-cision in those cases where the target word couldbe identified was only 101.3, using a word align-ment tool would almost certainly leave SWAT-S asa distant second to the 174.6 precision achieved bySWAT-E.4.4 Analysis by part-of-speechTable 3 shows the performance of both systemsbroken down by part-of-speech.
In the IRST2 sys-tem submitted to the 2007 Lexical Substitutiontask, adverbs were the best performing word class,followed distantly by adjectives, then nouns, andfinally verbs.
However, in this task, we found thatadverbs were the hardest word class to correctlysubstitute.
Further analysis should be done to de-termine if this is due to the difficulty of the partic-ular words and sentences chosen in this task, theadded complexity of performing the lexical substi-tution across two languages, or some independentfactor such as the choice of thesaurus used to formthe candidate set of substitutes.5 ConclusionsWe presented two systems that participated inthe SemEval-2010 Cross-Lingual Lexical Substi-tution task.
Both systems use a two-step processto obtain the lexical substitutes.
SWAT-E first findsEnglish lexical substitutes in the English sentenceand then translates these substitutes into Spanish.SWAT-S first translates the English sentences intoSpanish and then finds Spanish lexical substitutesusing these translations.The official competition results showed that ourtwo systems performed much better than the othersystems on the oot scoring method, but that weperformed only about average on the best scoringmethod.The analysis provided here indicates that the ootscore for SWAT-E would hold even if every sys-tem had its answers duplicated in order to ensure10 answers were provided for each test item.
Wealso we showed that a word alignment tool wouldlikely improve the performance of SWAT-S, butthat this improvement would not be enough to sur-pass SWAT-E.ReferencesT.
Brants and A. Franz.
2006.
Web 1T 5-gram,ver.
1.
LDC2006T13, Linguistic Data Consortium,Philadelphia.T.
Brants and A. Franz.
2009.
Web 1T 5-gram, 10 Eu-ropean Languages, ver.
1.
LDC2009T25, LinguisticData Consortium, Philadelphia.Claudio Giuliano, Alfio Gliozzo, and Carlo Strappa-rava.
2007.
FBK-irst: Lexical Substitution TaskExploiting Domain and Syntagmatic Coherence.
InProceedings of the Fourth International Workshopon Semantic Evaluations (SemEval-2007).Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions.E.
Loper and S. Bird.
2002.
NLTK: The NaturalLanguage Toolkit.
In Proceedings of the ACL-02Workshop on Effective Tools and Methodologies forTeaching Natural Language Processing and Compu-tational Linguistics.D.
McCarthy and R. Navigli.
2007.
SemEval-2007Task 10: English lexical substitution task.
In Pro-ceedings of SemEval-2007.Rada Mihalcea, Ravi Sinha, and Diana McCarthy.2010.
Semeval-2010 Task 2: Cross-lingual lex-ical substitution.
In Proceedings of the 5thInternational Workshop on Semantic Evaluations(SemEval-2010).Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In International Con-ference on New Methods in Language Processing.128
