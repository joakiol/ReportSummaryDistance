Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
238?247, Prague, June 2007. c?2007 Association for Computational LinguisticsA Sequence Alignment Model Based on the Averaged PerceptronDayne FreitagFair Isaac Corporation3661 Valley Centre DriveSan Diego, CA 92130, USADayneFreitag@fairisaac.comShahram KhadiviLehrstuhl fu?r Informatik 6Computer Science DepartmentRWTH Aachen UniversityD-52056 Aachen, Germanykhadivi@cs.rwth-aachen.deAbstractWe describe a discriminatively trained se-quence alignment model based on the av-eraged perceptron.
In common with otherapproaches to sequence modeling using per-ceptrons, and in contrast with comparablegenerative models, this model permits andtransparently exploits arbitrary features ofinput strings.
The simplicity of perceptrontraining lends more versatility than compa-rable approaches, allowing the model to beapplied to a variety of problem types forwhich a learned edit model might be useful.We enumerate some of these problem types,describe a training procedure for each, andevaluate the model?s performance on sev-eral problems.
We show that the proposedmodel performs at least as well as an ap-proach based on statistical machine transla-tion on two problems of name translitera-tion, and provide evidence that the combina-tion of the two approaches promises furtherimprovement.1 IntroductionSequence alignment is a problem that crops up inmany forms, both in computational linguistics (CL)and in other endeavors.
The ability to find an op-timal alignment between two sequences has foundapplication in a number of areas of CL, includ-ing phonetic modeling (Ristad and Yianilos, 1998),name transcription (Huang et al, 2004), and dupli-cate detection or information integration (Bilenkoand Mooney, 2003; McCallum et al, 2005).
Se-quence alignment is a member of a broader class ofproblems which we might call sequence transduc-tion, to which one of the core CL challenges, ma-chine translation, belongs.Under the assumption that one string (the target)is produced through a series of local edits to anotherstring (the source), and given an edit cost matrix,the optimal sequence of edits can be efficiently com-puted through dynamic programming (Needlemanand Wunsch, 1970).
While the cost matrix tradition-ally has been set by hand, several recent papers haveproposed determining edit costs empirically.
Theseproposals arise from a variety of learning paradigms,including generative models (Ristad and Yianilos,1998; Bilenko and Mooney, 2003), conditional ran-dom fields (McCallum et al, 2005), maximum-margin methods (Joachims, 2003), and gradientboosting (Parker et al, 2006).
While approachesbased on generative models support only limited fea-ture engineering, discriminative approaches sharethe advantage of allowing arbitrary features of theinput sequences.We describe a new sequence alignment modelbased on the averaged perceptron (Collins, 2002),which shares with the above approaches the abilityto exploit arbitrary features of the input sequences,but is distinguished from them by its relative sim-plicity and the incremental character of its trainingprocedure.
The fact that it is an online algorithmmakes it straightforward to adapt to a range of prob-lems.
To show this, we evaluate the approach onseveral different tasks, some of them merely illus-trative, but some with clear practical significance,238particularly the problem of named entity transcrip-tion.2 The Algorithm2.1 The FormalismSuppose we are given two sequences, sm1 ?
?
?sand tn1 ?
?
?t .
We desire a real-valued functionA(s, t) which assigns high scores to pairs s, t withhigh affinity, where affinity is an application-specificnotion (e.g., t is a likely phoneme sequence repre-sented by the letter sequence s).
If we stipulate thatthis score is the sum of the individual scores of aseries of edits, we can find the highest-scoring suchseries through a generalization of the standard editdistance:A(si1, tj1) =max????
?a?,tj (s, i, t, j) + A(si1, tj?11 )asi,?
(s, i, t, j) + A(si?11 , tj1)asi,tj (s, i, t, j) + A(si?11 , tj?11 )(1)with A(?, ?)
= 0.
The function asi,tj (s, i, t, j) rep-resents the score of substituting tj for si; a?,tj andasi,?
represent insertion and deletion, respectively.
Ifwe assume constant-time computation of primitiveedit costs, this recursive definition of A allows us tofind the highest scoring series of edits for a givensequence pair in time proportional to the product oftheir lengths.
Note that a is indexed by the charac-ters involved in an edit (i.e., inserting ?e?
generallyhas a different cost than inserting ?s?).
Note furtherthat the score associated with a particular operationmay depend on any features computable from therespective positions in the two sequences.In the experiments reported in this paper, we as-sume that each local function a is defined in termsof p + q features, {f1, ?
?
?
, fp, fp+1, ?
?
?
, fp+q},and that these features have the functional form??
?N 7?
R. In other words, each feature takesa sequence and an index and returns a real value.The first p features are defined over sequences fromthe source alphabet, while the remaining q are de-fined over the target alhabet.1 In this paper we usecharacter n-gram indicator features.1Of course, features that depend jointly on both sequencesmay also be of interest.1: Given a set S of source sequences2: V ?
[], an empty list3: ??
0, a weight vector4: for some number of iterations do5: for s in S do6: Pick t, t?, t having higher affinity with s7: ?e, v?
?
A?
(s, t)8: ?e?, v??
?
A?
(s, t?
)9: if v?
?
v then10: ??
?
+ ?
(s, t, e)?
?
(s, t?, e?
)11: end if12: Append ?
to V13: end for14: end for15: Return the mean ?
from VTable 1: The training algorithm.
A?
is the affinityfunction under model parameters ?, returning editsequence e and score v.The score of a particular edit is a linear combina-tion of the corresponding feature values:a(s, i, t, j) =p?k=1?k ?
fk(s, i) +p+q?k=p+1?k ?
fk(t, j)(2)The weights ?k are what we seek to optimize in or-der to tune the model for our particular application.2.2 A Perceptron-Based Edit ModelIn this section we present a general-purpose exten-sion of perceptron training for sequence labeling,due to Collins (2002), to the problem of sequencealignment.
Take ?
to be a model parameterization,and let A?
(s, t) return an optimal edit sequence e,with its score v, given input sequences s and t un-der ?.
Elements of sequence e are character pairs?cs, ct?, with cs ?
?s ?
{?}
and ct ?
?t ?
{?
},where ?
represents the empty string.
Let ?
(s, t, e)be a feature vector, having the same dimensional-ity as ?, for a source, target, and corresponding editsequence.
This feature vector is the sum of featurevectors at each point in e as it is played out alonginput sequences s and t.Table 1 shows the basic algorithm.
Starting witha zero parameter vector, we iterate through the col-lection of source sequences.
For each sequence, wepick two target sequences having unequal affinity239with the source sequence (Line 6).
If the scores re-turned by our current model (Lines 7 and 8) agreewith our ordering, we do nothing.
Otherwise, weupdate the model using the perceptron training rule(Line 10).
Ultimately, we return ?
averaged over alldatapoint presentations.2.3 Training ModesThe algorithm presented in Table 1 does not specifyhow the two target sequences t and t?
are to be cho-sen in Line 6.
The answer to this question dependson the application.
There are fundamentally two set-tings, depending on whether or not target strings aredrawn from the same set as source strings; we willcall the setting in which source and target strings in-habit the same set the affinity setting, and refer to thethe case where they form different sets as the trans-duction setting.
Here, we sketch four problem sce-narios, two from each setting, and specify a targetselection procedure appropriate for each.Affinity, ranking.
The task poses a latent affinitybetween strings, but we can measure it only indi-rectly.
In particular, we can order some of the targetsequences according to their affinity with a sourcesequence s. In this case, we train as follows: Ordera sample of the target sequences according to thispartial order.
Let t and t?
be two sequences fromthis order, such that t is ordered higher than t?.Affinity, classification.
The sequences in ??
canbe grouped into classes, and we wish the model toassign high affinity to co-members of a class andlow affinity to members of different classes.
Trainas follows: For each s, sample t from among its co-members and t?
from among the members of otherclasses.Transduction, ranking.
The data is presented assource-target pairs, where each t is a transductionof the corresponding s. We wish to learn a modelwhich, given a novel s, will enable us to rank can-didate transductions.
Train as follows: Given s, lett be the target sequence provided to us.
Sample t?from among the other target sequences.Transduction, generation.
We are again givensource-target pairs.
We wish to learn to generate aprobable target string, given a novel source string.Train as follows: Generate a t?
that is approximatelyoptimal according to the current model.
Note thatsince edit decisions are based in part on (arbitrary)edit?ing ?
STR?INGSfs,it ft,TRfs,t ft,Rfs, in f?fs, iTable 2: Features with non-zero value for an exam-ple string pair and a model of order 2.features of the target sequence, and since generationinvolves construction of the target sequence, it is notuncommon for a greedy generator to make edit de-cisions which are locally optimal, but which resultseveral edits later in a partially constructed sequencein which no good edits are available.
Thus, the prob-lem of generation does not correspond to a simplerecurrence relation like Equation 1.
Consequently,we experimented with several heuristic approachesto generation and found that a beam search workswell.3 EvaluationTo establish the effectiveness of the model, wetrained it on a range of problems, including instancesof each of the four settings enumerated above.
Prob-lems ranged from the merely illustrative to a non-trivial application of computational linguistics.3.1 Feature ConstructionWithout exception, the features we provide to thealgorithm are the same in all experiments.
Givena user-specified order k, we define a Boolean fea-ture for every distinct character gram observed in thedata of length k or smaller.
Recall that there are twodisjoint sets of features, those defined over stringsdrawn from the source and target alhabets, respec-tively.
Given a source string and index, those fea-tures have value 1 whose corresponding grams (ofsize k or smaller) are observed preceding or follow-ing the index (preceding features are distinct fromfollowing ones); given a target string and index, weobserve only preceding grams.
Although it is pos-sible to observe following grams in the target stringin some settings, it is not possible in general (i.e.,not when generating strings).
We therefore adhereto this restriction for convenience and uniformity.An example will make this clear.
In Table 2 we240are midway through the conversion of the sourcestring ?editing?
into the target string ?STRINGS?.Below the two strings are those gram features whichhave non-zero value at the indicated cursors.
Theunderbar character encodes on which side of the cur-sor a gram is observed.
Note that an empty-gramfeature, which always tests true, is also included, al-lowing us to experiment with 0-order models.3.2 Illustrative ProblemsTo test the ability of the model to recover knownedit affinities, we experimented with a simple artifi-cial problem.
Using a large list of English words,we define an edit affinity that is sensitive only toconsonants.
Specifically, the affinity between twowords is the maximum number of consonant self-substitutions, with any substitutions involving thefirst five consonants counting for five normal substi-tutions.
Thus, substituting ?b?
for ?b?
contributes 5 tothe score, substituting ?z?
for ?z?
contributes 1, whileoperations other than self-subsitutions, and any op-erations involving vowels, contribute 0.One epoch of training is conducted as follows.For each word s in the training set, we choose 10other words from the set at random and sort thesewords according to both the true and estimated affin-ity.
Let t be the string with highest true affinity; lett?
(the decoy) be the string with highest estimatedaffinity.
We performed 3-fold cross-validation on acollection of 33,432 words, in each fold training themodel for 5 epochs.2Our performance metric is ranking accuracy, thefraction of target string pairs to which the estimatedranking assigns the same order as the true one.
Dur-ing testing, for each source string, we sample at ran-dom 1000 other strings from the hold-out data, andcount the fraction of all pairs ordered correctly ac-cording to this criterion.A 0-order model successfully learns to rankstrings according to this affinity with 99.3% ac-curacy, while ranking according to the unmodifiedLevenshtein distance yields 76.4%.
Table 3 showsthe 6 averaged weights with the highest magnitude2Here and in other experiments involving the edit model, thenumber of epochs was set arbitrarily, and not based on perfor-mance on a development set.
Beyond the number of epochs re-quired for convergence, we have not observed much sensitivityin test accuracy to the number of epochs.
?d, d?
: f?
61.1?c, c?
: f?
60.6?g, g?
: f?
60.3?b, b?
: f?
59.1?f, f?
: f?
57.0?t, t?
: f?
18.6Table 3: Largest weights in a consonant-preservingedit affinity in which the first five consonants aregiven 5 times as much weight as others.from a model trained on one of the folds.
In pre-senting weights, we follow the formatting conven-tion edit:feature.
Since the model in question is oforder 0, all features in Table 3 are the ?empty fea-ture.?
Note how idempotent substitutions involvingthe 5 highly weighted consonants are weighted sig-nificantly higher than the remaining operations.3.3 RhymingWhile the above problem illustrates the ability ofthe proposed algorithm to learn latent alignmentaffinities, it is expressible as a order-0 model.
Asomewhat more interesting problem is that of mod-eling groups of rhyming words.
This problem isan instance of what we called the ?classification?scenario in Section 2.3.
Because English lettershave long since lost their direct correspondence tophonemes, the problem of distinguishing rhymingEnglish words is difficult for a knowledge-lean editmodel.
What?s more, the importance of a letter isdependent on context; letters near the end of a wordare more likely to be significant.We derived groups of rhyming words from theCMU pronouncing dictionary (CMU, 1995), dis-carding any singleton groups.
This yielded 21,396words partitioned into 3,799 groups, ranging in sizefrom 464 words (nation, location, etc.)
down to 2.We then divided the words in this data set at randominto three groups for cross-validation.Training was conducted as follows.
For eachword in the training set, we selected at random up to5 rhyming words and 5 non-rhyming words.
Thesewords were ranked according to affinity with thesource word under the current model.
Let t bethe lowest scoring rhyming word, and let t?
be thehighest-scoring non-rhyming word.241Model PrecisionLevenshtein 0.126Longest common suffix 0.130PTEM, Order 0 0.505PTEM, Order 3 0.790Table 4: Micro-averaged break-even precision onthe task of grouping rhyming English words.For each word in the hold-out set, we scored andranked all rhyming words in the same set, as wellas enough non-rhyming words to total 1000.
Wethen recorded the precision at the point in this rank-ing where recall and precision are most nearly equal.Our summary statistic is the micro-averaged break-even precision.Table 4 presents the performance of the proposedmodel and compares it with two simple baselines.Not surprisingly, performance increases with in-creasing order.
The simple heuristic approaches farequite poorly by comparison, reflecting the subtletyof the problem.3.4 TranscriptionOur work was motivated by the problem of namedentity transcription.
Out-of-vocabulary (OOV)terms are a persistent problem in statistical machinetranslation.
Often, such terms are the names of en-tities, which typically have low corpus frequencies.In translation, the appropriate handling of names isoften to transcribe them, to render them idiomati-cally in the target language in a way that preserves,as much as possible, their phonetic structure.
Evenwhen an OOV term is not a name, transcribing itpreserves information that would otherwise be dis-carded, leaving open the possibility that downstreamapplications will be able to make use of it.The state of the art in name transcription involvessome form of generative model, sometimes in com-bination with additional heuristics.
The generativecomponent may involve explicitly modeling pho-netics.
For example, Knight and Graehl (1998)employ cascaded probabilistic finite-state transduc-ers, one of the stages modeling the orthographic-to-phonetic mapping.
Subsequently, Al-Onaizanand Knight (2002) find they can boost perfor-mance by combining a phonetically-informed modelTask Train Dev Eval ELen FLenA-E 8084 1000 1000 6.5 4.9M-E 2000 430 1557 16.3 23.0Table 5: Characteristics of the two transcription datasets, Arabic-English (A-E) and Mandarin-English(M-E), including number of training, development,and evaluation pairs (Train, Dev, and Eval), andmean length in characters of English and foreignstrings (ELen and FLen).with one trained only on orthographic correspon-dences.
Huang et al (2004), construct a probabilis-tic Chinese-English edit model as part of a largeralignment solution, setting edit weights in a heuris-tic bootstrapped procedure.In rendering unfamiliar written Arabic words orphrases in English, it is generally impossible toachieve perfect performance, because many sounds,such as short vowels, diphthong markers, and dou-bled consonants, are conventionally not written inArabic.
We calculate from our experimental datasetsthat approximately 25% of the characters in the En-glish output must be inferred.
Thus, a character errorrate of 25% can be achieved through simple translit-eration.3.4.1 Transcribing namesWe experimented with a list of 10,084 personalnames distributed by the Linguistic Data Consor-tium (LDC).
Each entry in the database includesan arabic name in transliterated ASCII (SATTSmethod) and its English rendering.
The Arabicnames appear as they would in conventional writ-ten Arabic, i.e., lacking short vowels and other di-acritics.
We randomly segregated 1000 entries forevaluation and used the rest for training.
The A-Erow in Table 5 summarizes some of this data set?scharacteristics.We trained the edit model as follows.
For eachtraining pair the indicated English rendering con-stitutes our true target (t), and we use the currentmodel to generate an alternate string (t?
), updatingthe model in the event t?
yields a higher score than t.This was repeated for 10 epochs.
We experimentedwith a model of order 3.Under this methodology, we observed a 1-best ac-242?p, h?
: ft,a 38.5?p, t?
: ft,a 30.8?p, h?
: ft,ya 11.8?p, t?
: fs, p<e> -8.6?p, h?
: ft,rya -12.1?p, t?
: ft,uba -14.4Table 6: Some of the weights governing the han-dling of the tah marbouta (   ) in an order-3 Arabic-English location name transcription model.
Buck-walter encoding of Arabic characters is used here forpurposes of display.
The symbol ?<e>?
representsend of string.curacy of 0.552.
It is difficult to characterize thestrength of this result relative to those reported in theliterature.
Al-Onaizan and Knight (2002) report a 1-best accuracy of 0.199 on a corpus of Arabic personnames (but an accuracy of 0.634 on English names),using a ?spelling-based?
model, i.e., a model whichhas no access to phonetic information.
However,the details of their experiment and model differ fromours in a number of respects.It is interesting to see how a learned edit modelhandles ambiguous letters.
Table 6 shows theweights of some of the features governing the han-dling of the character  (tah marbouta) from exper-iments with Arabic place names.
This character,which represents the ?t?
sound, typically appears atthe end of words.
It is generally silent, but is spokenin certain grammatical constructions.
In its silentform, it is typically transcribed ?ah?
(or ?a?
); in itsspoken form, it is transcribed ?at?.
The weights inthe table reflect this ambiguity and illustrate someof the criteria by which the model chooses the ap-propriate transcription.
For example, the negativeweight on the feature fs, p<e> inhibits the produc-tion of ?t?
at the end of a phrase, where ?h?
is almostalways more appropriate.
Similarly, ?h?
is morecommon following ?ya?
in the target string (often aspart of the larger suffix ?iyah?).
However, the pre-ceding context ?rya?
is usually observed in the word?qaryat?, meaning ?village?
as in ?the village of ...?In this grammatical usage, the tah marbouta is spo-ken and therefore rendered with a ?t?.
Consequently,the corresponding weight in the ?h?
interpretation isinhibitory.The Al-Onaizan and Knight spelling model canbe regarded as a statistical machine translation(SMT) system which translates source languagecharacters to target language characters in the ab-sence of phonetic information.
For comparisonwith state of the art, we used the RWTH phrase-based SMT system (Zens et al, 2005) to build anArabic-to-English transliteration system.
This sys-tem frames the transcription problem as follows.
Weare given a sequence of source language charac-ters sm1 representing a name, which is to be trans-lated into a sequence of target language characterstn1 .
Among all possible target language character se-quences, we will choose the character sequence withthe highest probability:t?n?1 = argmaxn,tn1{Pr(tn1 |sm1 )} (3)The posterior probability Pr(tn1 |sm1 ) is modeled di-rectly using a log-linear combination of severalmodels (Och and Ney, 2002), including a character-based phrase translation model, a character-basedlexicon model, a 4-gram character sequence model,a character penalty and a phrase penalty.
The firsttwo models are used for both directions: Arabicto English and English to Arabic.
We do not useany reordering model because the target charactersequence is always monotone with respect to thesource character sequence.
More details about thebaseline system can be found in (Zens et al, 2005).We remark in passing that while the perceptron-based edit model is a general algorithm for learn-ing sequence alignments using simple features, theabove SMT approach combines several models,some of which have been the subject of research inthe fields of speech recognition and machine trans-lation for several years.
Furthermore, we made aneffort to optimize the performance of the SMT ap-proach on the tasks presented here.Table 7 compares this system with the edit model.The difference between the 1-best accuracies of thetwo systems is significant at the 95% level, usingthe bootstrap for testing.
However, we can improveon both systems by combining them.
We segregated1000 training documents to form a development set,and used it to learn linear combination coefficientsover our two systems, resulting in a combined sys-tem that scored 0.588 on the evaluation set?a sta-243Model 1best 5bestSMT 0.528 0.824PTEM, Order 3 0.552 0.803Linear combination 0.588 0.850Table 7: 1-best and 5-best transcription accuracies.The successive improvements in 1-best accuracy aresignificant at the 95% confidence level.tistically significant improvement over both systemsat the 95% confidence level.3.4.2 Ranking transcriptionsIn some applications, instead of transcribing aname in one language into another, it is enough justto rank candidate transcriptions.
For example, wemay be in possession of comparable corpora in twolanguages and the means to identify named entitiesin each.
If we can rank the likely transcriptions ofa name, we may be able to align a large portion ofthe transliterated named entities, potentially extend-ing the coverage of our machine translation system,which will typically have been developed using asmaller parallel corpus.
This idea is at the heart ofseveral recent attempts to improve the handling ofnamed entities in machine translation (Huang et al,2004; Lee and Chang, 2003).
A core componentof all such approaches is a generative model simi-lar in structure to the ?spelling?
model proposed byAl-Onaizan and Knight.When ranking is the objective, we can adopt atraining procedure that is much less expensive thanthe one used for generation.
Let t be the correct tran-scription for a source string (s).
Sample some num-ber of strings at random (200 in the following exper-iments) from among the transcriptions in the trainingset of strings other than s. Let t?
be the string havinghighest affinity with s, updating the model, as usual,if t?
scores higher than t.In addition to the Arabic-English corpus, we alsoexperiment with a corpus distributed by the LDCof full English names paired with their Mandarinspelling.
The M-E row of Table 5 summarizes char-acteristics of this data set.
Because we are inter-ested in an approximate comparison with similar ex-periments in the literature, we selected at random2430 for training and 1557 for evaluation, whichare the data sizes used by Lee and Chang (2003)for their experiments.
In these experiments, theChinese names are represented as space-separatedpinyin without tonal markers.Note that this problem is probably harder than theArabic one, for several reasons.
For one thing, theletters in a Mandarin transcription of a foreign namerepresent syllables, leading to a somewhat lossierrendering of foreign names in Mandarin than in Ara-bic.
On a more practical level, this data set is noisier,occasionally containing character sequences in onestring for which corresponding characters are lack-ing from its paired string.
On the other hand, theMandarin problem contains full names, rather thanname components, which provides more context forranking.We trained the edit model on both data sets us-ing both the sampling procedure outlined above andthe self-generation training regime, in each case for20 epochs, producing models of orders from 1 to 3.However, we found that the efficiency of the phrase-based SMT system described in the previous sectionwould be limited for this task, mainly due to tworeasons: the character-based phrase models due topossible unseen phrases in an evaluation corpus, andthe character sequence model as all candidate tran-scriptions confidently belong to the target language.Therefore, to make the phrase-based SMT systemrobust against data sparseness for the ranking task,we also make use of the IBM Model 4 (Brown etal., 1993) in both directions.
The experiments showthat IBM Model 4 is a reliable model for the rankingtask.
For each evaluation pair, we then ranked allavailable evaluation transcriptions, recording wherein this list the true transcription fell.Table 8 compares the various models, showingthe fraction of cases for which the true transcriptionwas ranked highest, and its mean reciprocal rank(MRR).
Both the phrase-based SMT model and theedit model perform well on this task.
While the bestconfiguration of PTEM out-performs the best SMTmodel, the differences are not significant at the 95%confidence level.
However, compare these perfor-mance scores to those returned by the system of Leeand Chang (2003), who reported a peak MRR of0.82 in similar experiments involving data differentfrom ours.The PTEM rows in the table are separated into244Model C-E Task A-E TaskACC MRR ACC MRRSMT 0.795 0.797 0.982 0.985SMT w/o LM 0.797 0.798 0.983 0.985IBM 4 0.961 0.971 0.978 0.987SMT + IBM 4 0.971 0.977 0.991 0.994PTEMG, Ord.
1 0.843 0.877 0.959 0.975PTEMG, Ord.
2 0.970 0.978 0.968 0.980PTEMG, Ord.
3 0.975 0.982 0.971 0.983PTEMR, Ord.
1 0.961 0.973 0.992 0.995PTEMR, Ord.
2 0.960 0.972 0.989 0.993PTEMR, Ord.
3 0.960 0.972 0.989 0.994Table 8: Performance on two transcription rankingtasks, showing fraction of cases in which the correcttranscription was ranked highest, accuracy (ACC)and mean reciprocal rank of the correct transcription(MRR).those in which the model was trained using thesame procedure as for generation (PTEMG), andthose in which the quicker ranking-specific train-ing regime was used (PTEMR).
The comparison isinteresting, inasmuch it does not support the con-clusion that one regime is uniformly superior to theother.
While generation regime yields the best per-formance on Arabic (using a high-order model), theranking regime scores best on Mandarin (with a low-order model).
When training a model to generate, itseems clear that more context in the form of largern-grams is beneficial.
This is particularly true forMandarin, where an order-1 model probably doesnot have the capacity to generate plausible decoys.4 DiscussionThis paper is not the first to show that perceptrontraining can be used in the solution of problemsinvolving transduction.
Both Liang, et al(2006),and Tillmann and Zhang (2006) report on effectivemachine translation (MT) models involving largenumbers of features with discriminatively trainedweights.
The training of these models is an in-stance of the ?Generation?
scenario outlined in Sec-tion 2.3.
However, because machine translation isa more challenging problem than name transcrip-tion (larger vocabularies, higher levels of ambigu-ity, non-monotonic transduction, etc.
), our general-purpose approach to generation training may be in-tractable for MT.
Instead, much of the focus of thesepapers are the heuristics that are required in order totrain such a model in this fashion, including featureselection using external resources (phrase tables),staged training, and generating to BLEU-maximalsequences, rather than the reference target.Klementiev and Roth (2006) explore the use of aperceptron-based ranking model for the purpose offinding name transliterations across comparable cor-pora.
They do not calculate an explicit alignment be-tween strings.
Instead, they decompose a string pairinto a collection of features derived from charac-ter n-grams heuristically paired based on their loca-tions in the respective strings.
Thus, Klementiev andRoth, in common with the two MT approaches de-scribed above, carefully control the features used bythe perceptron.
In contrast to these approaches, ouralgorithm discovers latent alignments, essentiallyselecting those features necessary for good perfor-mance on the task at hand.As noted in the introduction, several previous pa-pers have proposed general, discriminatively trainedsequence alignment models, as alternatives to thegenerative model proposed by Ristad and Yianilos.McCallum, et al (2005), propose a conditional ran-dom field for sequence alignment, designed for theimportant problem of duplicate detection and infor-mation integration.
Comprising two sub-models,one for matching strings and one for non-matching,the model is trained on sequence pairs explicitlylabeled ?match?
or ?non-match,?
and some careis apparently needed in selecting appropriate non-matching strings.
It is therefore unclear how thismodel would be extended to problems involvingranking or generation.Joachims (2003) proposes SVM-align, a sequencealignment model similar in structure to that de-scribed here, but which sets weights through di-rect numerical optimization.
Training involves ex-posing the model to sequence pairs, along with thecorrect alignment and some number of ?decoy?
se-quences.
The reliance on an explicit alignment andhand-chosen decoys yields a somewhat less flexi-ble solution than that presented here.
It is not clearwhether these features of the training regime are in-dispensable, or whether they might be generalized to245increase the approach?s scope.
Note that where di-rectly maximizing the margin is feasible, it has beenshown empirically to be superior to perceptron train-ing (Altun et al, 2003).Parker et al (2006), propose to align sequences bygradient tree boosting.
This approach has the attrac-tive characteristic that it supports a factored repre-sentation of edits (a characteristic it shares with Mc-Callum et al).
Although this paper does not evaluatethe method on any problems from computational lin-guistics (the central problem is musical informationretrieval), gradient tree boosting has been shown tobe an effective technique for other sorts of sequencemodeling drawn from computational linguistics (Di-etterich et al, 2004).5 ConclusionMotivated by the problem of Arabic-English tran-scription of names, we adapted recent work in per-ceptron learning for sequence labeling to the prob-lem of sequence alignment.
The resulting algorithmshows clear promise not only for transcription, butalso for ranking of transcriptions and structural clas-sification.
We believe this versatility will lead toother successful applications of the idea, both withincomputational linguistics and in other fields involv-ing sequential learning.Acknowledgment of supportThis material is based upon work supported bythe Defense Advanced Research Projects Agency(DARPA/IPTO) under Contract HR0011-06-C-0023.
Any opinions, findings and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflect theviews of the Defense Advanced Research ProjectsAgency (DARPA).ReferencesY.
Al-Onaizan and K. Knight.
2002.
Machine translit-eration of names in Arabic text.
In Proceedings ofthe ACL-02 workshop on computational approaches tosemitic languages.Y.
Altun, I. Tsochantaridis, and T. Hofmann.
2003.
Hid-den Markov support vector machines.
In Proceedingsof ICML-2003.M.
Bilenko and R. Mooney.
2003.
Adaptive duplicatedetection using learnable string similarity measures.In Proceedings of KDD-2003.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics, 19(2), June.CMU.
1995.
The CMU pronouncing dictionary.http://www.speech.cs.cmu.edu/cgi-bin/cmudict.
Ver-sion 0.6.M.
Collins.
2002.
Discriminative training methods forhidden Markov models: theory and experiments withperceptron algorithms.
In Proceedings of EMNLP-2002.T.
Dietterich, A. Ashenfelter, and Y. Bulatov.
2004.Training conditional random fields via gradient treeboosting.
In Proceedings of ICML-2004.F.
Huang, S. Vogel, and A. Waibel.
2004.
Improvingnamed entity translation combining phonetic and se-mantic similarities.
In Proceedings of HLT-NAACL2004.T.
Joachims.
2003.
Learning to align sequences: amaximum-margin approach.
Technical report, CornellUniversity.A.
Klementiev and D. Roth.
2006.
Weakly supervisednamed entity transliteration and discovery from mul-tilingual comparable corpora.
In Proceedings of Col-ing/ACL 2006.K.
Knight and J. Graehl.
1998.
Machine transliteration.Computational Linguistics, 24(4).C.-J.
Lee and J.S.
Chang.
2003.
Acquisition of English-Chinese transliterated word pairs from parallel-alignedtexts using a statistical machine transliteration model.In Proceedings of the HLT-NAACL 2003 Workshop onBuilding and Using Parallel Texts.P.
Liang, A.
Bouchard-Co?te?, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach tomachine translation.
In Proceedings of COLING2006/ACL 2006.A.
McCallum, K. Bellare, and F. Pereira.
2005.
A condi-tional random field for discriminatively-trained finite-state string edit distance.
In Proceedings of UAI-2005.S.B.
Needleman and C.D.
Wunsch.
1970.
A generalmethod applicable to the search for similarities in theamino acid sequence of two proteins.
Journal ofMolecular Biology, 48.F.J.
Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In ACL02, pages 295?302, Philadelphia,PA, July.246C.
Parker, A. Fern, and P Tadepalli.
2006.
Gradientboosting for sequence alignment.
In Proceedings ofAAAI-2006.E.S.
Ristad and P.N.
Yianilos.
1998.
Learning string-editdistance.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 20.C.
Tillmann and T. Zhang.
2006.
A discriminative globaltraining algorithm for statistical MT.
In Proceedingsof Coling/ACL 2006.R.
Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,J.
Xu, Y. Zhang, and H. Ney.
2005.
The RWTHphrase-based statistical machine translation system.
InProceedings of the International Workshop on SpokenLanguage Translation (IWSLT), pages 155?162, Pitts-burgh, PA, October.247
