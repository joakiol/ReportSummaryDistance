Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 949?958,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA Non-negative Matrix Factorization Based Approach for Active DualSupervision from Document and Word LabelsChao Shen and Tao LiSchool of Computing and Information SciencesFlorida International UniversityMiami, FL 33199 USA{cshen001,taoli}@cs.fiu.eduAbstractIn active dual supervision, not only informa-tive examples but also features are selected forlabeling to build a high quality classifier withlow cost.
However, how to measure the infor-mativeness for both examples and feature onthe same scale has not been well solved.
Inthis paper, we propose a non-negative matrixfactorization based approach to address this is-sue.
We first extend the matrix factorizationframework to explicitly model the correspond-ing relationships between feature classes andexamples classes.
Then by making use ofthe reconstruction error, we propose a unifiedscheme to determine which feature or exam-ple a classifier is most likely to benefit fromhaving labeled.
Empirical results demonstratethe effectiveness of our proposed methods.1 IntroductionActive learning, as an effective paradigm to optimizethe learning benefit from domain experts?
feedbackand to reduce the cost of acquiring labeled examplesfor supervised learning, has been intensively stud-ied in recent years (McCallum and Nigam, 1998;Tong and Koller, 2002; Settles, 2009).
Traditionalapproaches for active learning query the human ex-perts to obtain the labels for intelligently chosendata samples.
However, in text classification wherethe input data is generally represented as document-word matrices, human supervision can be obtainedon both documents and words.
For example, in sen-timent analysis of product reviews, human labelerscan label reviews as positive or negative, they canalso label the words that elicit positive sentiment(such as ?sensational?
and ?electrifying?)
as posi-tive and words that evoke negative sentiment (suchas ?depressed?
and ?unfulfilling?)
as negative.
Re-cent work has demonstrated that labeled words (orfeature supervision) can greatly reduce the numberof labeled samples for building high-quality classi-fiers (Druck et al, 2008; Zaidan and Eisner, 2008).In fact, different kinds of supervision generally havedifferent acquisition costs, different degrees of util-ity and are not mutually redundant (Sindhwani etal., 2009).
Ideally, effective active learning schemesshould be able to utilize different forms of supervi-sion.To incorporate the supervision on words and doc-uments at same time into the active learning scheme,recently an active dual supervision (or dual activelearning) has been proposed (Melville and Sind-hwani, 2009; Sindhwani et al, 2009).
Comparingwith traditional active learning which aims to selectthe most ?informative?
examples (e.g., documents)for domain experts to label, active dual supervi-sion selects both the ?informative?
examples (e.g.,documents) and features (e.g., words) for labeling.For active dual supervision to be effective, thereare three important components: a) an underlyinglearning mechanism that is able to learn from boththe labeled examples and features (i.e., incorporat-ing supervision on both examples and features); b)methods for estimating the value of information forexample and feature labels; and c) a scheme thatshould be able to trade-off the costs and benefits ofthe different forms of supervision since they havedifferent labeling costs and different benefits.949In Sindhwani et al?s initial work on active dualsupervision (Sindhwani et al, 2009), a transductivebipartite graph regularization approach is used forlearning from both labeled examples and features.In addition, uncertainty sampling and experimentaldesign are used for selecting informative examplesand features for labeling.
To trade-off between dif-ferent types of supervision, a simple probabilisticinterleaving scheme where the active learner prob-abilistically queries the example oracle and the fea-ture oracle is used.
One problem in their method isthat the values of acquiring the feature labels andthe example labels are not on the same scale.Recently, Li et al (Li et al, 2009) proposed adual supervision method based on constrained non-negative tri-factorization of the document-term ma-trix where the labeled features and examples arenaturally incorporated as sets of constraints.
Hav-ing a framework for incorporating dual-supervisionbased on matrix factorization, gives rise to the nat-ural question of how to perform active dual super-vision in this setting.
Since rows and columns aretreated equally in estimating the errors of matrix fac-torization, another question is can we address thescaling issue in comparing the value of feature la-bels and example labels.In this paper, we study the problem of ac-tive dual supervision using non-negative matrix tri-factorization.
Our work is based on the dual supervi-sion framework using constrained non-negative tri-factorization proposed in (Li et al, 2009).
We firstextend the framework to explicitly model the corre-sponding relationships between feature classes andexample classes.
Then by making use of the recon-struction error criterion in matrix factorization, wepropose a unified scheme to evaluate the value offeature and example labels.
Instead of comparingthe estimated performance increase of new featurelabels or example labels, our proposed scheme as-sumes that a better supervision (a feature label or aexample label) should lead to a more accurate re-construction of the original data matrix.
In our pro-posed scheme, the value of feature labels and ex-ample labels is computed on the same scale.
Theexperiments show that our proposed unified schemeto query selection (i.e., feature/example selection forlabeling) outperforms the interleaving schemes andthe scheme based on expected log gain.The rest of this paper is organized as follows: therelated work is discussed in Section 2, and the dualsupervision framework based on non-negative ma-trix tri-factorization is introduced in Section 3.
Weextend non-negative matrix tri-factorization to activelearning settings in Section 4, and propose a unifiedscheme for query selection in Section 5.
Experi-ments are presented in Section 6, and finally Section7 concludes the paper.2 Related WorkWe point the reader to a recent report (Settles, 2009)for an in-depth survey on active learning.
In thissection, we briefly cover related work to position ourcontributions appropriately.Active Learning/Active Dual Supervision Mostprior work in active learning has focused on pooled-based techniques, where examples from an unla-beled pool are selected for labeling (Cohn et al,1994).
With the study of learning from labeled fea-tures, many research efforts on active learning withfeature supervision are also reported (Melville et al,2005; Raghavan et al, 2006).
(Godbole et al, 2004)proposed the notion of feature uncertainty and in-corporated the acquired feature labels into learningby creating one-term mini-documents.
(Druck et al,2009) performed active learning via feature labelingusing several uncertainty reduction heuristics usingthe learning model developed in (Druck et al, 2008).
(Sindhwani et al, 2009) studied the problem of ac-tive dual supervision from examples and featuresusing a graph-based dual supervision method witha simple probabilistic method for interleaving fea-ture labels and example labels.
In our work, we de-velop our active dual supervision framework usingconstrained non-negative tri-factorization and alsopropose a unified scheme to evaluate the value offeature and example labels.
We note the very re-cent work of (Attenberg et al, 2010), which pro-poses a unified approach for the dual active learn-ing problem using expected utility where the utilityis defined as the log gain of the classification modelwith a new labeled document or word.
Conceptu-ally, our proposed unified scheme is a special caseof the expected utility framework where the utilityis computed using the matrix reconstruction error.The utility based on the log gain of the classification950model may not be reliable as small model changesresulted from a single additional example label orfeature label may not be reflected in the classifica-tion performance (Attenberg et al, 2010).
The em-pirical comparisons show that our proposed unifiedscheme based on reconstruction error outperformsthe expected log gain.Dual Supervision Note that a learning methodthat is capable of performing dual supervision (i.e.,learning from both labeled examples and features)is the basis for active dual supervision.
Dual su-pervision is a relatively new area of research andfew methods have been developed for dual super-vision.
In (Sindhwani and Melville, 2008; Sind-hwani et al, 2008), a bipartite graph regularizationmodel (GRADS) is used to diffuse label informa-tion along both sides of the document-term matrixand to perform dual supervision for semi-supervisedsentiment analysis.
Conceptually, their model im-plements a co-clustering assumption closely relatedto Singular Value Decomposition (see also (Dhillon,2001; Zha et al, 2001) for more on this perspec-tive).
In (Sandler et al, 2008), standard regulariza-tion models are constrained using graphs of word co-occurrences.
In (Melville et al, 2009), Naive Bayesclassifier is extended, where the parameters, the con-ditional word distributions given the classes, are es-timated by combining multiple sources, e.g.
docu-ment labels and word labels.
Our work is based onthe dual supervision framework using constrainednon-negative tri-factorization.3 Learning with Dual Supervision viaTri-NMFOur dual supervision model is based on non-negative matrix tri-factorization (Tri-NMF), wherethe non-negative input document-word matrix is ap-proximated by 3 factor matrices as X ?
GSF T , inwhich,X is an n?m document-term matrix,G is ann ?
k non-negative orthogonal matrix representingthe probability of generating a document from a doc-ument cluster, F is an m?
k non-negative orthogo-nal matrix representing the probability of generatinga word from a word cluster, and S is a k ?
k non-negative matrix providing the relationship betweendocument cluster space and word cluster space.While Tri-NMF is first applied in co-clustering, Liet al (Li et al, 2009) extended it to incorporate la-beled words and documents as dual supervision viatwo loss terms in the objective function of Tri-NMFas following:minF,G,S ?X ?GSF T ?2+?
trace[(F ?
F0)TC1(F ?
F0)]+?
trace[(G?G0)TC2(G?G0)].
(1)Here, ?
> 0 is a parameter which determines theextent to which we enforce F ?
F0 to its labeledrows.
C1 is a m ?
m diagonal matrix whose en-try (C1)ii = 1 if the row of F0 is labeled, that is,the class of the i-th word is known and (C1)ii = 0otherwise.
?
> 0 is a parameter which determinesthe extent to which we enforce G ?
G0 to its la-beled rows.
C2 is a n ?
n diagonal matrix whoseentry (C2)ii = 1 if the row of G0 is labeled, thatis, the category of the i-th document is known and(C2)ii = 0 otherwise.
The squared loss terms ensurethat the solution for G,F in the otherwise unsuper-vised learning problem be close to the prior knowl-edge G0, F0.
So the partial labels on documents andwords can be described using G0 and F0, respec-tively.4 Dual Supervision with Explicit ClassAlignment4.1 Modeling the Relationships between WordClasses and Document ClassesIn the solution to Equation 1, we have S = GTXF ,orSlk = gTl Xfk =1|Rl|1/2|Ck|1/2?i?Rl?j?CkXij ,(2)where |Rl| is the size of the l-th document class, and|Ck| is the size of the k-th word class (Ding et al,2006).
Note that Slk represents properly normalizedwithin-class sum of weights (l = k) and between-class sum of weights (l 6= k).
So, S represents therelationship between the classes over documents andthe classes over words.
Under the assumption thatthe i-th document class should correspond to the i-th word class, S should be an approximate diago-nal matrix, since the documents of i-th class is morelikely to contain the words of the i-th class.
Note951that S is not an exact diagonal matrix, since a doc-ument of one class apparently can use words fromother classes (especially G and F are required to beapproximately orthogonal, which means the classi-fication is rigorous).
However, in Equation 1, thereare no explicit constraints on the relationship be-tween word classes and document classes.
Instead,the relationship is established and enforced implic-itly using existing labeled documents and words.In active learning, the set of starting labeled doc-uments or words is small, and this may generate anill-formed S, leading to an incorrect alignment ofword classes and document classes.
To explicitlymodel the relationships between word classes anddocument classes, we constrain the shape of S viaan extra loss term in the objective function as fol-lows:minF,G,S ?X ?GSF T ?2+?
trace[(F ?
F0)TC1(F ?
F0)]+?
trace[(G?G0)TC2(G?G0)]+?
trace[(S ?
S0)T (S ?
S0)](3)where S0 is a diagonal matrix.How to Choose S0 If there is no orthogonal con-straint on F,G and I-divergence is used as the ob-jective function, it can been shown that the factorsof Tri-NMF have probabilistic interpretation (Dinget al, 2008; Shen et al, 2011):Fil = P (w = wi|zw = l),Gjk = P (d = dj |zd = k),Skl = P (zd = k, zw = l),(4)where w is word variable, d is document variable,and zw, zd are random variables indicating wordclass and document class respectively.
F and Grepresent posterior distributions for words and docu-ments, and S represents the joint distribution of doc-ument class and word class.
With such an interpre-tation, S0 can be easily decided in balanced classifi-cation problems with each diagonal entry equals toone over the number of classes.However, in our setting of Tri-NMF, orthogonalconstraints are enforced on F,G and Euclidean dis-tance is used as the objective function.
To pre-compute S0, one way is to first solve the optimiza-tion problem Equation 1 with another constraint thatS should be diagonal.
Alternatively, to keep it sim-ple, we ignore the known label information and justassume there exists a diagonal matrix S0 and twoorthogonal matrices G,F , thatGS0F T ?
X.Thentrace[XXT ] ?
trace[GS0F TFST0 GT ],= trace[S0ST0 F TFGTG],= trace[S0ST0 ],= ?k(S0)2kk.
(5)So if a classification problem is balanced with Kclasses, S0 can be estimated as following:(S0)kl ={ ?trace[XXT ]K l = k,0 otherwise.
(6)4.2 Computing AlgorithmThis optimization problem can be solved using thefollowing update rulesGjk ?
Gjk XFS+?C2G0(GGTXFS+?GGTC2G)jk ,Sjk ?
Sjk FTXTG+?S0(FTFSGTG+?S)jk ,Fjk ?
Fjk XTGST+?C1F0(FFTXTGST+?C1F )jk .
(7)The algorithm consists of an iterative procedure us-ing the above three rules until convergence.Theorem 4.1 The above iterative algorithm con-verges.Theorem 4.2 At convergence, the solution satisfiesthe Karuch-Kuhn-Tucker (KKT) optimality condi-tion, i.e., the algorithm converges correctly to a lo-cal optima.Theorem 4.1 can be proved using the standard aux-iliary function approach (Lee and Seung, 2001).Proof of Theorem 4.2: Proof for the update rulesof G,F is the same as in (Li et al, 2009).
Here wefocus on the update rule of S. We want to minimizeL(S) = ?X ?GSF T ?2+?
trace[(F ?
F0)TC1(F ?
F0)]+?
trace[(G?G0)TC2(G?G0)]+?
trace[(S ?
S0)T (S ?
S0)].
(8)952The gradient of L is?L?S = 2FTFSGTG?
2F TXTG+ 2?
(S ?
S0)The KKT complementarity condition for the non-negativity of Sjk gives[2F TFSGTG?2F TXTG+2?
(S?S0)]jkSjk = 0.This is the fixed point relation that local minima forS must satisfy, which is equivalent with the updaterule of S in Equation 7.5 A Unified Scheme for Query SelectionUsing the Reconstruction Error5.1 IntroductionAn ideal active dual supervision scheme should beable to evaluate the value of acquiring labels for doc-uments and words on the same scale.
In the initialstudy of dual active supervision, different scores areused for documents and words (e.g.
uncertainty fordocuments and certainty for words), and thus theyare not on the same scale (Sindhwani et al, 2009).Recently, the framework of Expected Utility (Esti-mated Risk Minimization) is proposed in (Attenberget al, 2010).
At each step of the framework, the nextword or document selected for labeling is the onethat will result in the highest estimated improvementin classifier performance as defined as:EU(qj) =K?k=1P (qj = ck)U(qj = ck), (9)where K is the class number, P (qj = ck) indicatesthe probability that qj , j-th query (a word or docu-ment), belongs to the k-th class, and the U(qj = ck)indicates the utility that qj belongs to the k-th class.However, the choice of the utility measure is still achallenge.5.2 Reconstruction ErrorIn our matrix factorization framework, rows andcolumns are treated equally in estimating the errorsof matrix factorization, and the reconstruction erroris thus a natural measure of utility.
Let the currentsupervision knowledge be G0, F0.
To select a newunlabeled document/word for labeling, we assumethat a good supervision should lead to a good con-strained factorization for the document-term matrix,X ?
GSF T .
If the new query qj is a word and itslabel is k, then the new factorization isG?j=k, S?j=k, F ?j=k= argminG,S,F ?X ?GSF T ?2+ ?
trace[(G?G0)TC2(G?G0)]+ ?
trace[(F ?
F0,j=k)TC1(F ?
F0,j=k)]+ ?
trace[(S ?
S0)T (S ?
S0)],(10)where F0,j=k is same as F0 except thatF0,j=k(j, k) = 1.
In other words, we obtaineda new factorization using the labeled words.
Sim-ilarly, if the new query qj is a document, then thenew factorization isG?j=k, S?j=k, F ?j=k= argminG,S,F ?X ?GSF T ?2+ ?
trace[(G?G0,j=k)TC2(G?G0,j=k)]+ ?
trace[(F ?
F0)TC1(F ?
F0)]+ ?
trace[(S ?
S0)T (S ?
S0)],(11)where G0,j=k is same as G0 except thatG0,j=k(j, k) = 1.
In other words, we obtaineda new factorization using the labeled documents.Then the new reconstruction error isRE(qj = k) = ?X ?G?j=kS?j=kF ?j=k?2.
(12)So the expected utility of a document or word labelquery, qj , can be computed asEU(qj) =K?k=1P (qj = k)?
(?RE(qj = k)).
(13)To calculate the P (qj = k), which is the posteriordistribution for words or documents, probabilisticinterpretation of Tri-NMF is abused.
When a queryqj is a word, P (qj = k) isP (zw = k|w = wi)?
P (w = wi|zw = k)?Kj=1 P (zw = k, zd = j)= Fik ?
?Kj=1 Skj , (14)otherwise,P (zd = k|d = di)?
P (d = di|zd = k)?Kj=1 P (zw = j, zd = k)= Gik ?
?Kj=1 Sjk.
(15)9535.3 Algorithm DescriptionComputational Improvement: It can be computa-tionally intensive if the reconstruction error is com-puted for all unknown documents and words.
In-spired by (Attenberg et al, 2010), we first select thetop 100 unknown words that the current model ismost certain about, and the top 100 unknown docu-ments that the current model is most uncertain about.Then we identify the words or documents in thispool with the highest expected utility (reconstruc-tion error).
Equations 14 and 15 are used to performthe initial selection of top 100 unknown words andtop 100 unknown documents.Algorithm 1 Active Dual Supervision AlgorithmBased on Matrix FactorizationINPUT: X , document-word matrix; F0, current la-beled words; G0, current labeled documents; O, theoracleOUTPUT: G, classification result for all documentsin X1.
Get base factorization of X: G,S, F .2.
Active dual supervisionrepeatD is the set of top 100 unlabeled documentswith most uncertainty;W is the set of top 100 unlabeled words withmost certainty;Q = D ?W ;for all q ?
Q dofor k = 1 to K doGet G?q=k, F ?q=k, S?q=k by Equation 10 orEquation 11 according to whether thequery q is a document or a word;Calculate EU(q) by Equation 13;q?
= argmaxq EU(q);Acquire new label of q?, l from O;G,F, S = G?q?=l, F ?q?=l, S?q?=l;until stop criterion is met.The overall algorithm procedure is described inAlgorithm 1.
First we iteratively use the updat-ing rules of Equation 7 to obtain the factoriza-tion G,F, S based on initial labeled documents andwords.
Then to select a new query, for each unla-beled document or word in the pool and for eachpossible class, we compute the reconstruction errorwith new supervision (using the current factoriza-tion results as initialization values).
It is efficient tocompute a new factorization due to the sparsity ofthe matrices.
The document-term matrix is typicallyvery sparse with z  nm non-zero entries while k istypically also much smaller than document numbern, and word numberm.
By using sparse matrix mul-tiplications and avoiding dense intermediate matri-ces, updating F, S,G each takesO(k2(m+n)+kz)time per iteration which scales linearly with the di-mensions and density of the data matrix (Li et al,2009).
Empirically, the number of iterations that isneeded to compute the new factorization is usuallyvery small (less than 10).6 Experiments6.1 Experiments SettingsThree popular binary text classification datasets areused in the experiments: ibm-mac (1937 examples),baseball-hockey (1988 examples) and med-space(1972 examples) datasets.
All of them are drawnfrom the 20-newsgroups text collection1 where thetask is to assign messages into the newsgroup inwhich they appeared.
Top 1500 frequent words ineach dataset are used as features in the binary vec-tor representation.
These datasets have labels for allthe documents.
For a document query, the oracle re-turns its label.
We construct the word oracle in thesame manner as in (Sindhwani et al, 2009): firstcompute the information gain of words with respectto the known true class labels in the training splits ofa dataset, and then the top 100 words as ranked byinformation gain are assigned the label which is theclass in which the word appears more frequently.
Tothose words with labels, the word oracle returns itslabel; otherwise, the oracle returns a ?don?t know?response (no word label is obtained for learning, butthe word is excluded from the following query se-lection).Results are averaged over 10 random training-test splits.
For each split, 30% examples are usedfor testing.
All methods are initialized by a ran-dom choice of 10 document labels and 10 word la-bels.
For simplicity, we follow the widely used costmodel (Raghavan and Allan, 2007; Druck et al,1http://www.ai.mit.edu/people/jrennie/20_newsgroups/9542008; Sindhwani et al, 2009) where features areroughly 5 times cheaper to label than examples, sowe assume the cost is 1 for a word query and is 5 fora document query.
We set ?
= ?
= 5, ?
= 1 for allthe following experiments2.0.450.50.550.60.650.70.750.810-1020-1530-2040-2550-30400-50500-60600-70700-80800-90Accuracy#labeled documents-#labeled wordsw/o.
constraint on Sw/.
constraint on S(a) baseball-hockey0.50.550.60.650.70.750.80.8510-1020-1530-2040-2550-30400-50500-60600-70700-80800-90Accuracy#labeled documents-#labeled wordsw/o.
constraint on Sw/.
constraint on S(b) ibm-mac0.50.550.60.650.70.7510-1020-1530-2040-2550-30400-50500-60600-70700-80800-90Accuracy#labeled documents-#labeled wordsw/o.
constraint on Sw/.
constraint on S(c) med-spaceFigure 1: Comparing the performance of dual supervisionvia Tri-NMF w/ and w/o the constraint on S.2We do not perform fine tuning on the parameters since themain objective of the paper is to demonstrate the effectivenessof matrix factorization based methods for dual active supervi-sion.
A vigorous investigation on the parameter choices is ourfurther work.6.2 Experimental ResultsEffect of Constraints on S in Constrained Tri-NMF Figure 1 demonstrates the effectiveness ofdual supervision with explicit class alignment viaTri-NMF as described in Section 4.
When thereare enough labeled documents and words, the con-straints on S have a relative small impact on the per-formance of dual supervision.
However, in the be-ginning phase of active learning, the labeled datasetcan be small (such as 10 labeled documents and 10labeled words).
In this case, without the constraintof S, the matrix factorization may generate incorrectclass alignment, thus lead to almost random classi-fication results (around 50% accuracy), as shown inFigure 1, and further make unreasonable the follow-ing evaluation of queries.Comparing Query Selection Approaches Figure2 compares our proposed unified scheme (denoted asExpected-reconstruction-error) with the followingbaselines using Tri-NMF as the classifier for dualsupervision: (1).
Interleaved-uncertainty whichfirst selects feature query by certainty and samplequery by uncertainty and then combines the twotypes of queries using an interleaving scheme.
Theinterleaving probability (probability to select thequery as a document) is set as 0.2, 0.4, 0.6 and0.8.
(2).
Expected-log-gain which selects featureand sample query by maximizing the expected loggain.
Expected-reconstruction-error outperformsinterleaving schemes with all the different interleav-ing probability values with which we experimented.It also has a better performance than Expected-log-gain.
Although log gain is a finer-grained utilitymeasure of classifier performance than accuracy andhas a good performance in the setting with a large setof starting labeled documents (e.g., 100 documents),it is not reliable especially in the setting with a smallset of labeled data.
Different from the Expected-log-gain, Expected-reconstruction-error estimates theutility using the matrix reconstruction error, makinguse of information of all documents and words, in-cluding those unlabeled.Comparing Interleaving Scheme vs. the Uni-fied Scheme To further demonstrate the benefitof the proposed unified scheme , we compare itwith its interleaved version: Interleaved-expected-9550.70.720.740.760.780.80.820.840  100  200  300  400  500  600  700  800AccuracyLabeling CostExpected-log-gainInterleaved-uncertainty-0.2Interleaved-uncertainty-0.4Interleaved-uncertainty-0.6Interleaved-uncertainty-0.8Expected-reconstruction-error(a) baseball-hockey0.70.720.740.760.780.80.820  100  200  300  400  500  600  700  800AccuracyLabeling CostExpected-log-gainInterleaved-uncertainty-0.2Interleaved-uncertainty-0.4Interleaved-uncertainty-0.6Interleaved-uncertainty-0.8Expected-reconstruction-error(b) ibm-mac0.560.580.60.620.640.660.680.70  100  200  300  400  500  600  700  800AccuracyLabeling CostExpected-log-gainInterleaved-uncertainty-0.2Interleaved-uncertainty-0.4Interleaved-uncertainty-0.6Interleaved-uncertainty-0.8Expected-reconstruction-error(c) med-spaceFigure 2: Comparing the different query selection approaches in active learning via Tri-NMF with dual supervision.0.70.720.740.760.780.80.820.840  100  200  300  400  500  600  700  800AccuracyLabeling CostInterleaved-expected-reconstruction-error-0.2Interleaved-expected-reconstruction-error-0.4Interleaved-expected-reconstruction-error-0.6Interleaved-expected-reconstruction-error-0.8Expected-reconstruction-error(a) baseball-hockey0.680.70.720.740.760.780.80.820  100  200  300  400  500  600  700  800AccuracyLabeling CostInterleaved-expected-reconstruction-error-0.2Interleaved-expected-reconstruction-error-0.4Interleaved-expected-reconstruction-error-0.6Interleaved-expected-reconstruction-error-0.8Expected-reconstruction-error(b) ibm-mac0.560.580.60.620.640.660.680.70  100  200  300  400  500  600  700  800AccuracyLabeling CostInterleaved-expected-reconstruction-error-0.2Interleaved-expected-reconstruction-error-0.4Interleaved-expected-reconstruction-error-0.6Interleaved-expected-reconstruction-error-0.8Expected-reconstruction-error(c) med-spaceFigure 3: Comparing the unified and interleaving scheme based on reconstruction error.construction-error which computes the utility of aquery using the reconstruction error, but uses inter-leaving scheme to decide which type of query toselect.
We experiment with different interleavingprobability values ranging from 0.2 to 0.8, whichlead to quite different performance results.
FromFigure 3, the optimal interleaving probability valuevaries on different datasets.
For example, the proba-bility value of 0.8 is among the optimal interleavingprobability values on baseball-hockey dataset butperforms poorly on ibm-mac dataset.
This obser-vation also illustrates the need for a unified scheme,because of the difficulty in choosing the optimal in-terleaving probability value.
Although the proposedunified scheme is not significantly better than its in-terleaving counterparts for all interleaving probabil-ity values on all datasets, it avoids the bad choices.Figure 5 presents the sequence of different querytypes selected by our unified scheme and it clearlydemonstrates the distribution patterns of differentquery types.
At the beginning phase of active learn-ing, word queries have much higher probabilities tobe selected, which is consistent with the result ofprevious work: feature labels can be more effec-tive than examples in text classification (Druck et50  100  150  200  250  300QueryTypeQuery SequenceWordDocument(a) baseball-hockey50  100  150  200  250  300QueryTypeQuery SequenceWordDocument(b) ibm-macFigure 5: Example of query sequence.al., 2008).
And in the later learning phase, docu-ments are more likely to be selected, since the num-ber of words that can benefit the classification ismuch smaller than the effective documents.Reconstruction Error vs. Interleaving uncer-tainty using GRADS It should be pointed out thatour unified scheme for query selection based on re-construction error does not rely on the estimationof model performance on training data and can beeasily integrated with other dual supervision mod-9560.860.870.880.890.90.910.920.930.940  100  200  300  400  500  600  700  800AccuracyLabeling CostGRADS-Interleaving-0.5GRADS-Reconstruction-Error(a) baseball-hockey0.620.640.660.680.70.720.740.760.780.80.820.840  100  200  300  400  500  600  700  800AccuracyLabeling CostGRADS-Interleaving-0.5GRADS-Reconstruction-Error(b) ibm-mac0.860.870.880.890.90.910.920.930  100  200  300  400  500  600  700  800AccuracyLabeling CostGRADS-Interleaving-0.5GRADS-Reconstruction-Error(c) med-spaceFigure 4: GRADS with reconstruction error and interleaving uncertainty.els such as GRADS (Sindhwani et al, 2008).
Fig-ure 4 shows the comparison of GRADS using theinterleaved scheme with an interleaving probabilityof 0.5, and using our unified scheme based on recon-struction error.
Among the 3 datasets we used, thereconstruction error based approach outperforms theinterleaving scheme on baseball-hockey and ibm-mac, and has similar performance with the interleav-ing scheme on med-space.0.50.550.60.650.70.750  100  200  300  400  500  600  700  800AccuracyLabeling CostGRADS-Interleaving-0.2GRADS-Interleaving-0.4GRADS-Interleaving-0.6GRADS-Interleaving-0.8Tri-NMF-Reconstruction-ErrorFigure 6: Comparing active dual supervision using ma-trix factorization with GRADS on sentiment analysis.Comparing Active Dual Supervision Using Ma-trix Factorization with GRADS on SentimentAnalysis The sentiment analysis experiment isconducted on the movies review dataset (Pang et al,2002), containing 1000 positive and 1000 negativemovie reviews.
The results are shown in Figure 6.The experimental results clearly demonstrate the ef-fectiveness of our approach, denoted as Tri-NMF-Reconstruction-Error.7 ConclusionsIn this paper, we study the problem of active dualsupervision, and propose a matrix tri-factorizationbased approach to address the issue, how to evaluatelabeling benifit of different types of queries (exam-ples or features) in the same scale.
Following ex-tending the nonnegative matrix tri-factorization tothe active dual supervision setting, we use the recon-struction error to evaluate the value of feature andexample labels.
Experimental results show that ourproposed approach outperforms existing methods.AcknowledgementThe work is partially supported by NSF grantsDMS-0915110, CCF-0830659, and HRD-0833093.We would like to thank Dr. Vikas Sindhwani forhis insightful discussions and for sharing us with hisGRADS code.ReferencesJ.
Attenberg, P. Melville, and F. Provost.
2010.
A Uni-fied Approach to Active Dual Supervision for Label-ing Features and Examples.
Machine Learning andKnowledge Discovery in Databases, pages 40?55.D.
Cohn, L. Atlas, and R. Ladner.
1994.
Improving gen-eralization with active learning.
Machine Learning,15(2):201?221.I.S.
Dhillon.
2001.
Co-clustering documents and wordsusing bipartite spectral graph partitioning.
In Pro-ceedings of the seventh ACM SIGKDD internationalconference on Knowledge discovery and data mining,pages 269?274.
ACM.C.
Ding, T. Li, W. Peng, and H. Park.
2006.
Orthogonalnonnegative matrix t-factorizations for clustering.
InProceedings of the 12th ACM SIGKDD international957conference on Knowledge discovery and data mining,pages 126?135.
ACM.C.
Ding, T. Li, and W. Peng.
2008.
On the equiva-lence between non-negative matrix factorization andprobabilistic latent semantic indexing.
ComputationalStatistics & Data Analysis, 52(8):3913?3927.G.
Druck, G. Mann, and A. McCallum.
2008.
Learn-ing from labeled features using generalized expecta-tion criteria.
In Proceedings of the 31st annual in-ternational ACM SIGIR conference on Research anddevelopment in information retrieval, pages 595?602.ACM.G.
Druck, B.
Settles, and A. McCallum.
2009.
Activelearning by labeling features.
In Proceedings of the2009 conference on Empirical methods in natural lan-guage processing, pages 81?90.
Association for Com-putational Linguistics.S.
Godbole, A. Harpale, S. Sarawagi, and S. Chakrabarti.2004.
Document classification through interactive su-pervision of document and term labels.
KnowledgeDiscovery in Databases: PKDD 2004, pages 185?196.D.D.
Lee and H.S.
Seung.
2001.
Algorithms for non-negative matrix factorization.
Advances in neural in-formation processing systems, 13.T.
Li, Y. Zhang, and V. Sindhwani.
2009.
A non-negativematrix tri-factorization approach to sentiment classifi-cation with lexical prior knowledge.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting of theACL, pages 244?252.
Association for ComputationalLinguistics.A.K.
McCallum and K. Nigam.
1998.
Employing EMand pool-based active learning for text classification.In Proceedings of the Fifteenth International Confer-ence on Machine Learning.
Citeseer.P.
Melville and V. Sindhwani.
2009.
Active dual su-pervision: Reducing the cost of annotating examplesand features.
In Proceedings of the NAACL HLT 2009Workshop on Active Learning for Natural LanguageProcessing, pages 49?57.
Association for Computa-tional Linguistics.P.
Melville, M. Saar-Tsechansky, F. Provost, andR.
Mooney.
2005.
An expected utility approach toactive feature-value acquisition.
In Proceedings ofFifth IEEE International Conference on Data Mining.IEEE.P.
Melville, W. Gryc, and R.D.
Lawrence.
2009.
Senti-ment analysis of blogs by combining lexical knowl-edge with text classification.
In Proceedings ofthe 15th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 1275?1284.
ACM.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
: sentiment classification using machine learningtechniques.
In Proceedings of the 2002 conferenceon Empirical methods in natural language processing,pages 79?86.
Association for Computational Linguis-tics.H.
Raghavan and J. Allan.
2007.
An interactive algo-rithm for asking and incorporating feature feedbackinto support vector machines.
In Proceedings of the30th annual international ACM SIGIR conference onResearch and development in information retrieval,pages 79?86.
ACM.H.
Raghavan, O. Madani, and R. Jones.
2006.
Activelearning with feedback on features and instances.
TheJournal of Machine Learning Research, 7:1655?1686.T.
Sandler, P.P.
Talukdar, L.H.
Ungar, and J. Blitzer.2008.
Regularized learning with networks of features.Advances in Neural Information Processing Systems,pages 1401?1408.B.
Settles.
2009.
Active Learning Literature Survey.Technical Report 1648.C.
Shen, T. Li, and C. Ding.
2011.
Integrating Clusteringand Multi-Document Summarization by Bi-mixtureProbabilistic Latent Semantic Analysis (PLSA) withSentence Bases.
In Proceedings of the national con-ference on Artificial intelligence.
AAAI Press.V.
Sindhwani and P. Melville.
2008.
Document-wordco-regularization for semi-supervised sentiment anal-ysis.
In Data Mining, Eighth IEEE International Con-ference on, pages 1025?1030.
IEEE.V.
Sindhwani, J. Hu, and A. Mojsilovic.
2008.
Regular-ized co-clustering with dual supervision.
Advances inNeural Information Processing Systems, 21.V.
Sindhwani, P. Melville, and R.D.
Lawrence.
2009.Uncertainty sampling and transductive experimentaldesign for active dual supervision.
In Proceedings ofthe 26th Annual International Conference on MachineLearning, pages 953?960.
ACM.S.
Tong and D. Koller.
2002.
Support vector machineactive learning with applications to text classification.The Journal of Machine Learning Research, 2:45?66.Omar F. Zaidan and Jason Eisner.
2008.
Modeling anno-tators: A generative approach to learning from annota-tor rationales.
In Proceedings of the 2008 conferenceon Empirical methods in natural language processing,pages 31?40.
Association for Computational Linguis-tics, October.H.
Zha, X.
He, C. Ding, H. Simon, and M. Gu.
2001.
Bi-partite graph partitioning and data clustering.
In Pro-ceedings of the tenth international conference on In-formation and knowledge management, pages 25?32.ACM.958
