A HYBRID REASONING MODEL FOR INDIRECT ANSWERSNancy  GreenDepartment  of Computer  ScienceUniversity of DelawareNewark, DE 19716, USAInternet: green@udel.eduSandra CarberryDepartment of Computer  ScienceUniversity of DelawareVisitor: Inst.
for Research in Cognitive ScienceUniversity of PennsylvaniaInternet: carberry@udel.eduAbst rac tThis paper presents our implemented computa-tional model for interpreting and generating in-direct answers to Yes-No questions.
Its main fea-tures are 1) a discourse-plan-based approach toimplicature, 2) a reversible architecture for gen-eration and interpretation, 3) a hybrid reasoningmodel that employs both plan inference and log-ical inference, and 4) use of stimulus conditionsto model a speaker's motivation for providing ap-propriate, unrequested information.
The modelhandles a wider range of types of indirect answersthan previous computational models and has sev-eral significant advantages.1.
INTRODUCTIONImagine a discourse context for (1) in which R'suse of just (ld) is intended to convey a No, i.e.,that R is not going shopping tonight.
(By con-vention, square brackets indicate that the enclosedtext was not explicitly stated.)
The part of R's re-sponse consisting of (ld) - (le) is what we call anindirect answer to a Yes-No question, and if (lc)had been uttered, (lc) would have been called adirect answer.l.a.
Q: I need a ride to the mall.b.
Are you going shopping tonight?c.
R: \[no\]d. My car's not running.e.
The rear axle is broken.According to one study of spoken English\[Stenstrhm, 1984\], 13 percent of responses to Yes-No questions were indirect answers.
Thus, theability to interpret indirect answers is required forrobust dialogue systems.
Furthermore, there aregood reasons for generating indirect answers in-stead of just yes, no, or I don't know.
First, theymay provide information which is needed to avoidmisleading the questioner \[Hirschberg, 1985\].
Sec-ond, they contribute to an efficient dialogue byanticipating follow-up questions.
Third, they maybe used for social reasons, as in (1).This paper provides a computational modelfor the interpretation and generation of indirectanswers to Yes-No questions in English.
More pre-cisely, by a Yes-No question we mean one or moreutterances used as a request by Q (the questioner)that R (the responder) convey R's evaluation ofthe truth of a proposition p. An indirect answerimplicitly conveys via one or more utterances R'sevaluation of the truth of the questioned proposi-tion p, i.e.
that p is true, that p is false, that thereis some truth to p, that p may be true, or thatp may be false.
Our model presupposes that Q'squestion has been understood by R as intended byQ, that Q's request was appropriate, and that Qand R are engaged in a cooperative goal-directeddialogue.
The interpretation and generation com-ponents of the model have been implemented inCommon Lisp on a Sun SPARCstation.The model employs an agent's pragmaticknowledge of how language typically is used toanswer Yes-No questions in English to constrainthe process of generating and interpreting indirectanswers.
This knowledge is encoded as a set ofdomain-independent discourse plan operators anda set of coherence rules, described in section 2.1Figure 1 shows the architecture of our system.
Itis reversible in that the same pragmatic knowl-edge is used by the interpretation and generationmodules.
The interpretation algorithm, describedin section 3, is a hybrid approach employing bothplan inference and logical inference to infer R's dis-course plan.
The generation algorithm, describedin section 4, constructs R's discourse plan in twophases.
During the first phase, stimulus condi-tions are used to trigger goals to include appro-priate, extra information in the response plan.
Inthe second phase, the response plan is pruned toeliminate parts which can be inferred by Q.hOur main sources of data were previous studies\[Hirschberg, 1985, Stenstrhm, 1984\], transcripts ofnaturally occurring two-person dialogue \[AmericanExpress transcripts, 1992\], and constructed examples.58discourse plan operatorsdiscourse xpectationresponse --I INTERPRETATION I I G:NERATION Icoherence rulesdiscourse xpectationR's beliefsFigure 1: Architecture of system2.
PRAGMATIC  KNOWLEDGELinguists (e.g.
see discussion in \[Levinson, 1983\])have claimed that use of an utterance in a dia-logue may create shared expectations about sub-sequent utterances.
In particular, a Yes-No ques-tion creates the discourse xpectation that R willprovide R's evaluation of the truth of the ques-tioned proposition p. Furthermore, Q's assump-tion that R's response is relevant riggers Q's at-tempt to interpret R's response as providing therequested information.
We have observed thatcoherence relations imilar to the subject-matterrelations of Rhetorical Structure Theory (RST)\[Mann and Thompson, 1987\] can be used in defin-ing constraints on the relevance of.an indirect an-swer.
For example, the relation between the (im-plicit) direct answer in (2b) and each of the indi-rect answers in (2c) - (2e) is similar to RST's rela-tions of Condition, Elaboration, and (Volitional)Cause, respectively.2.a.
Q: Are you going shopping tonight?b.
R: \[yes\]c. if I finish my homeworkd.
I'm going to Macy'se.
Winter clothes are on saleFurthermore, for Q to interpret any of (2c) - (2e)as conveying an affirmative answer, Q must be-lieve that R intended Q to recognize the relationalproposition holding between the indirect answerand (2b), e.g.
that (2d) is an elaboration of (25).Also, coherence relations hold between parts of anindirect answer consisting of multiple utterances.For example, (le) describes the cause of the fail-ure reported in (ld).
Finally, we have observedthat different relations are usually associated withdifferent ypes of answers.
Thus, a speaker whohas inferred a plausible coherence relation holdingbetween an indirect answer and a possible (im-plicit) direct answer may be able to infer the di-rect answer.
(If more than one coherence relation( (Plausible (cr-obstacle((not ( in-state ?stateq ?tq))(not (occur ?eventp ?tp)))))<-(state ?stateq)(event ?eventp)(timeperiod ?tq)(timeperiod ?tp)(before ?tq ?tp)(app-cond ?stateq ?eventp)(unless (in-state ?stateq ?tq))(un less  (occur ?eventp ?
tp ) ) )Figure 2: A coherence rule for cr-obstacleis plausible, or if the same coherence relation isused with more than one type of answer, then theindirect answer may be ambiguous.
)In our model we formally represent he co-herence relations which constrain indirect answersby means of coherence rules.
Each rule consistsof a consequent of the form (Plausible (CR qp)) and an antecedent which is a conjunction ofconditions, where CR is the name of a coherencerelation and q and p are formulae, symbols pre-fixed with "?"
are variables, and all variables areimplicitly universally quantified.
Each antecedentcondition represents a condition which is true iffit is believed by R to be mutually believed withQ.2 Each rule represents ufficient conditions forthe plausibility of (CR q p) for some CR, q, p. Anexample of one of the rules describing the Obsta-2Our model of R's beliefs (and similarly for Q's),represented asa set of Horn clauses, includes 1) generalworld knowledge presumably shared with Q, 2) knowl-edge about the preceding discourse, and 3) R's beliefs(including "weak beliefs"} about Q's beliefs.
Much ofthe shared world knowledge needed to evaluate the co-herence rules consists of knowledge from domain planoperators.59(Answer-yes s h ?p):Applicability conditions:(discourse-expectation(informif s h ?p))(believe s ?p)Nucleus:(inform s h ?p)Satellites:(Use-condition s h ?p)(Use-cause s h ?p)(Use-elaboration s h ?p)Primary goals:(BMB h s ?p)Figure 3: Discourse plan(Answer-no s h ?p):Applicability conditions:(discourse-expectation(informif s h ?p))(believe s (not ?p))Nucleus:(inform s h (not ?p))Satellites:(Use-otherwise s h (not ?p))(Use-obstacle s h (not ?p))(Use-cont ras t  s h (not ?p))Primary goals:(BMB h s (not ?p))operators for Yes and No answerscle relation 3 is shown in Figure 2.
The predicatesused in the rule are defined as follows: (in-state p/) denotes that p holds during t, (occur p t) de-notes that p happens during t, (state z) denotesthat the type of x is state, (event x) denotes thatthe type of x is event, (timeperiod t) denotes thatt is a t ime interval, (before tl t2) denotes that t lbegins before or at the same time as t2, (app-condq p} denotes that q is a plausible enabling con-dition for doing p, and (unless p) denotes that pis not provable from the beliefs of the reasoner.For example, this rule describes the relation be-tween (ld) and (lc), where (ld) is interpreted as(not (in-state (running R-car) Present)) and (lc)as (not (occur (go-shopping R) Future)).
That is,this relation would be plausible if Q and R sharethe belief that a plausible enabling condition of asubaction of a plan for R to go shopping at themall is that R's car be in running condition.In her study of responses to questions, Sten-strSm \[Stenstrfm, 1984\] found that direct an-swers are often accompanied by extra, relevantinformation, 4 and noted that often this extra in-formation is similar in content to an indirect an-swer.
Thus, the above constraints on the relevanceof an indirect answer can serve also as constraintson information accompanying a direct answer.
Formaximum generality, therefore, we went beyondour original goal of handling indirect answers tothe goal of handling what we call full answers.
Afull answer consists of an implicit or explicit directanswer (which we call the nucleus) and, possibly,extra, relevant information (satellites).
s In ourawhile Obstacle is not one of the original relationsof RST, it is similar to the causal relations of RST.461 percent of direct No answers and 24 percent ofdirect Yes answers5The terms nucleus and satellite have been bor-rowed from RST to reflect the informational con-straints within a full answer.
Note that according toRST, a property of the nucleus is that its removal re-model, we represent each type of full answer as a(top-level) discourse plan operator.
By represent-ing answer types as plan operators, generation canbe modeled as plan construction, and interpreta-tion as plan recognition.Examples of (top-level) operators describing afull Yes answer and a full No answer are shownin Figure 3.
6 To explain our notation, s andh are constants denoting speaker (R) and hearer(Q), respectively.
Symbols prefixed with "?"
de-note propositional variables.
The variables in theheader of each top-level operator will be instan-tiated with the questioned proposition.
In inter-preting example (1), ?p would be instantiated withthe proposition that R is going shopping tonight.Thus, instantiating the Answer-No operator inFigure 3 with this proposition would produce aplan for answering that P~ is not going shoppingtonight.
Applicability conditions are necessaryconditions for appropriate use of a plan operator.For example, it is inappropriate for R to give anaffirmative answer that p if R believes p is false.Also, an answer to a Yes-No question is not ap-propriate unless s and h share the discourse ex-pectation that s will provide s's evaluation of thetruth of the questioned proposition p, which wedenote as (discourse-ezpectation (informif s h p)).Primary goals describe the intended effects of theplan operator.
We use (BMB h s p) to denotethat h believes it mutually believed with s that p\[Clark and Marshall, 1981\].In general, the nucleus and satellites of a dis-course plan operator describe primitive or non-primitive communicative acts.
Our formalism el-suits in incoherence.
However, in our model, a di-rect answer may be removed without causing incoher-ence, provided that it is inferable from the rest of theresponse.6The other top-level operators in our model,Answer-hedged, Answer-maybe, and Answer-maybe-not, represent the other answer types handled.60(Use-obstacle s h ?p):;; s tells h of an obstacle explaining; ;  the failure ?pExistential variable: ?qApplicability conditions:(believe s (cr-obstacle ?q ?p))(Plausible (cr-obstacle ?q ?p))Stimulus conditions:(explanation-indicated s h ?p ?q)(excuse-indicated s h ?p ?q)Nucleus:(inform s h ?q)Satellites:(Use-elaboration s h ?q)(Use-obstacle s h ?q)(Use-cause s h ?q)Primary goals:(BMB h s (cr-obstacle ?q ?p))Figure 4: Discourse plan operator for Obstaclelows zero, one, or more occurrences of a satellitein a full answer, and the expected (but not re-quired) order of nucleus and satellites is the orderthey are listed in the operator.
(inform s h p) de-notes the primitive act of s informing h that p.The satellites in Figure 3 refer to non-primitiveacts, described by discourse plan operators whichwe have defined (one for each coherence relationused in a full answer).
For example, Use-obstacle,a satellite of Answer-no in Figure 3, is defined inFigure 4.To explain the additional notation in Figure 4,(cr-obstacle q p) denotes that the coherence rela-tion named obstacle holds between q and p. Thus,the first applicability condition can be glossed asrequiring that s believe that the coherence rela-tion holds.
In the second applicability condition,(Plausible (cr-obstacle q p)) denotes that, givenwhat s believes to be mutually believed with h,the coherence relation (cr-obstacle q p) is plausi-ble.
This sort of applicability condition is evalu-ated using the coherence rules described above.Stimulus conditions describe conditions moti-vating a speaker to include a satellite during planconstruction.
They can be thought of as trig-gers which give rise to new speaker goals.
Inorder for a satellite to be selected during gen-eration, all of its applicability conditions and atleast one of its stimulus conditions must hold.While stimulus conditions may be derivative ofprinciples of cooperativity \[Grice, 1975\] or po-liteness \[Brown and Levinson, 1978\], they providea level of precompiled knowledge which reducesthe amount of reasoning required for content-planning.
For example, Figure 5 depicts the dis-course plan which would be constructed by R (andAnswer-no/\\[Ic\] Use-obstacle/\Id Use-obstacleJleFigure 5: Discourse plan underlying (ld) - (le)must be inferred by Q) for (1).
The first stimu-lus condition of Use-obstacle, which is defined asholding whenever s suspects that h would be sur-prised that p holds, describes R's reason for includ-ing (le).
The second stimulus condition, which isdefined as holding whenever s suspects that theYes-No question is a prerequest \[Levinson, 1983\],describes R's reason for including (ld).
73.
INTERPRETAT IONWe assume that interpretation of dialogue iscontrolled by a Discourse Model Processor(DMP), which maintains a Discourse Model\[Carberry, 1990\] representing what Q believes Rhas inferred so far concerning Q's plans.
The dis-course xpectation generated by a Yes-No questionleads the DMP to invoke the answer recognitionprocess to be described in this section.
If answerrecognition is unsuccessful, the DMP would invokeother types of recognizers for handling less pre-ferred types of responses, such as I don't know ora clarification subdialogue.
To give an example ofwhere our recognition algorithm fits into the aboveframework, consider (4).4a.
Q: Is Dr. Smith teaching CSI next fall?b.
R: Do you mean Dr. Smithson?c.
Q: Yes.d.
R: \[no\]e. He will be on sabbatical next fall.f.
Why do you ask?Note that a request for clarification and its answerare given in (4b) - (4c).
Our recognition algorithm,when invoked with (4e) - (4f) as input, would inferan Answer-no plan accounting for (4e) and satis-fying the discourse xpectation generated by (4a).When invoked by the DMP, our interpretationmodule plays the role of the questioner Q. Theinputs to interpretation i our model consist of7Stimulus conditions are formally defined by rulesencoded in the same formalism as used for our co-herence rules.
A full description of the stimu-lus conditions used in our model can be found in\[Green, in preparation\].611) the set of discourse plan operators and the setof coherence rules described in section 2, 2) Q'sbeliefs, 3) the discourse expectation (discourse-expectation (informif s h p)), and 4) the semanticrepresentation of the sequence of utterances per-formed by R during R's turn.
The output is apartially ordered set (possibly empty) of answerdiscourse plans which it is plausible to ascribe to Ras underlying It's response.
The set is ordered byplausibility using preference criteria.
Note that weassume that the final choice of a discourse plan toascribe to R is made by the DMP, since the DMPmust select an interpretation consistent with theinterpretation of any remaining parts of R's turnnot accounted fo~ by the answer discourse plan,e.g.
(4f).To give a high-level description of our answerinterpretation algorithm, first, each (top-level) an-swer discourse plan operator is instantiated withthe questioned proposition from the discourse x-pectation.
For example (1), each answer operatorwould be instantiated with the proposition thatR is going shopping tonight.
Next, the answerinterpreter must verify that the applicability con-ditions and primary goals which would be held byR if R were pursuing the plan are consistent withQ's beliefs about It's beliefs and goals.
Consis-tency checking is implemented using a Horn clausetheorem-prover.
For all candidate answer planswhich have not been eliminated uring consistencychecking, recognition continues by attempting tomatch the utterances in R's turn to the actionsspecified in the candidates.
However, no candi-date plan may be constructed which violates thefollowing structural constraint.
Viewing a candi-date plan's structure as a tree whose leaves areprimitive acts from which the plan was inferred,no subtree Ti may contain an act whose sequentialposition in the response is included in the rangeof sequential positions in the response of acts in asubtree Tj having the same parent node as 7~.
Forexample, (5e) cannot be interpreted as related to(5c) by cr-obstaele, due to the occurrence of (5d)between (5c) and (5e).
Note that a more coherentresponse would consist of the sequence, (5c), (5e),(Sd).5.a .
O: Are you going shopping ton ight?b.
R: \[no\]c. My car's not running.d, Bes ides ,  I'm too t i red .e.
The t iming be l t  i s  broken.To recognize a subplan for a non-primitive ac-tion, e.g.
Use-obstacle in Figure 4, a similar proce-dure is used.
Note that any applicability conditionof the form (Plausible (CR q p)) is defined to beconsistent with Q's beliefs if it is provable, i.e.,if the antecedents of a coherence rule for CR aretrue with respect o what Q believes to be mutu-ally believed with R. The recognition process fornon-primitive actions differs in that these opera-tors contain existential variables which must beinstantiated.
In our model, the answer interpreterfirst attempts to instantiate an existential variablewith a proposition from R's response.
For exam-ple (1), the existential variable ?q of Use-obstaclewould be instantiated with the proposition thatR's car is not running.
However, if ( ld) was notexplicitly stated by R, i.e., if R's response had justconsisted of (le), it would be necessary for ?q tobe instantiated with a hypothesized proposition,corresponding to (ld), to understand how (le) re-lates to R's answer.
The answer interpreter findsthe hypothesized proposition by a subprocedurewe refer to as hypothesis generation.Hypothesis generation is constrained by theassumption that R's response is coherent, i.e., that(le) may play the role of a satellite in a subplan ofsome Answer plan.
Thus, the coherence rules areused as a source of knowledge for generating hy-potheses.
Hypothesis generation begins with ini-tializing the root of a tree of hypotheses with aproposition p0 to be related to a plan, e.g.
theproposition conveyed by (le).
A tree of hypothe-ses is constructed by expanding each of its nodesin breadth-first order until all goal nodes (as de-fined below) have been reached, subject to a limiton the depth of the breadth-first search, s A nodecontaining a proposition Pi is expanded by search-ing for all propositions Pi+l such that for somecoherence relation CR which may be used in thetype of answer being recognized, (Plausible (CR pipi+l)) holds from Q's point of view.
(The search isimplemented using a Horn clause theorem prover.
)The plan operator invoking hypothesis gener-ation has a partially instantiated applicability con-dition of the form, (Plausible (CR ?q p)), whereCR is a coherence relation, p is the propositionthat was used to instantiate the header variable ofthe operator, and ?q is the operator's existentialvariable.
Since the purpose of the search is to finda proposition qwith which to instantiate ?q, a goalnode is defined as a node containing a propositionq satisfying the above condition.
(E.g.
in Figure 6P0 is the proposition conveyed by (le), Px is theproposition conveyed by (ld), P0 and Pl are plau-sibly related by er-obstaele, P2 is the propositionconveyed by a No answer to (la), Pl and P2 areplausibly related by cr-obstacle, P2 is a goal node,and therefore, Pl will be used to instantiate theexistential variable ?q in Use-obstacle.
)After the existential variable is instantiated,plan recognition proceeds as described above atSPlacing a limit on the maximum depth of the treeis reasonable, given human processing constraints.62~ goal (conveyed if lc were uttered)hypothesized (conveyed if ld were uttered)proposition from utterance (conveyed in le)Figure 6: Hypothesis generation tree relating (le)to (lc)the point where the remaining conditions arechecked for consistency.
9 For example, as recog-nition of the Use-obstacle subplan proceeds, (le)would be recognized as the realization of a Use-obstacle satellite of this Use-obstacle subplan.
Ul-timately, the inferred plan would be the same asthat shown in Figure 5, except hat (ld) would bemarked as hypothesized.The set of candidate plans inferred from a re-sponse are ranked using two preference criteria.
1?First, as the number of hypothesized propositionsin a candidate increases, its plausibility decreases.Second, as the number of non-hypothesized propo-sitions accounted for by the plan increases, itsplausibility increases.To summarize the interpretation algorithm, itis primarily expectation-driven in the sense thatthe answer interpreter attempts to interpret R'sresponse as an answer generated by some answerdiscourse plan operator.
Whenever the answer in-terpreter is unable to relate an utterance to theplan which it is currently attempting to recognize,the answer interpreter attempts to find a connec-tion by hypothesis generation.
Logical inferenceplays a supplementary ole, namely, in consistencychecking (including inferring the plausibility of co-herence relations) and in hypothesis generation.4.
GENERATIONThe inputs to generation consist of 1) the samesets of discourse plan operators and coherencerules used in interpretation, 2) R's beliefs, and 3)the same discourse xpectation.
The output is a9Note that, in general, any nodes on the path be-tween p0 and Ph, where Ph is the hypothesis returned,will be used as additional hypotheses (later) to connectwhat was said to ph.1?Another possible criterion is whether the actualordering reflects the default ordering specified in thediscourse plan operators.
We plan to test the useful-ness of this criterion.discourse plan for an answer (indirect, if possible).Generation of an indirect reply has two phases: 1)content planning, in which the generator creates adiscourse plan for a full answer, and 2) plan prun-ing, in which the generator determines which partsof the planned full answer do not need to be ex-plicitly stated.
For example, given an appropriateset of R's beliefs, our system generates a plan forasserting only the proposition conveyed in (le) asan answer to (lb).
11Content-planning is performed by top-downexpansion of an answer discourse plan operator.Note that applicability conditions prevent inap-propriate use of an operator, but they do notmodel a speaker's motivation for providing extrainformation.
Further, a full answer might providetoo much information if every satellite whose oper-ator's applicability conditions held were includedin a full answer.
On the other hand, at the time Ris asked the question, R may not yet have the pri-mary goals of a potential satellite.
To overcomethese limitations, we have incorporated stimulusconditions into the discourse plan operators in ourmodel.
As mentioned in section 2, stimulus condi-tions can be thought of as triggers or motivatingconditions which give rise to new speaker goals.By analyzing the speaker's possible motivation forproviding extra information in the examples in ourcorpus, we have identified a small set of stimu-lus conditions which reflect general concerns ofaccuracy, efficiency, and politeness.
In order fora satellite to be included in a full answer, all ofits applicability conditions and at least one of itsstimulus conditions must hold.
(A theorem proveris used to search for an instantiation of the exis-tential variable satisfying the above conditions.
)The output of the content-planning phase, adiscourse plan representing a full answer, is theinput to the plan-pruning phase.
The goal of thisphase is to make the response more concise, i.e.
todetermine which of the planned acts can be omit-ted while still allowing Q to infer the full plan.
Todo this, the generator considers each of the actsin the frontier of the full plan tree from right toleft (thus ensuring that a satellite is considered be-fore its nucleus).
The generator creates trial plansconsisting of the original plan minus the nodespruned so far and minus the current node.
Then,the generator simulates Q's interpretation of thetrial plan.
If Q could infer the full plan (as themost preferred plan), then the current node canbe pruned.
Note that, even when it is not possi-ble to prune the direct answer, a benefit of thisapproach is that it generates appropriate xtra in-formation with direct answers.11The tactical component must choose an appropri-ate expression to refer to R's car's timing belt, de-pending on whether (ld) is omitted.635.
RELATED RESEARCHIt has been noted \[Diller, 1989, Hirsehberg, 1985,Lakoff, 1973\] that indirect answers conversa-tionally implicale \[Grice, 1975\] direct answers.Recently, philosophers \[Thomason, 1990, MeCaf-ferty, 1987\] have argued for a plan-based ap-proach to conversational implicature.
Plan-basedcomputational models have been proposed forsimilar discourse interpretation problems, e.g.indirect speech acts \[Perrault and Allen, 1980,Hinkelman, 1989\], but none of these models ad-dress the interpretation f indirect answers.
Also,our use of coherence relations, both 1) as con-straints on the relevance of indirect answers, and2) in our hypothesis generation algorithm, isunique in plan-based interpretation models.In addition to RST, a number of theories oftext coherence have been proposed \[Grimes, 1975,Halliday, 1976, Hobbs, 1979, Polanyi, 1986,Reiehman, 1984\].
Coherence relations havebeen used in interpretation \[Dahlgren, 1989,Wu and Lytinen, 1990\].
However, inference of co-herence relations alone is insufficient for inter-preting indirect answers, since additional prag-matic knowledge (what we represent as discourseplan operators) and discourse expectations arenecessary also.
Coherence relations have beenused in generation \[MeKeown, 1985, Hovy, 1988,Moore and Paris, 1988, Horacek, 1992\] but noneof these models generate indirect answers.
Also,our use of stimulus conditions i  unique in gener-ation models.Most previous formal and computationalmodels of conversational implicature \[Gazdar,1979, Green, 1990, Hirschberg, 1985, Lasearidesand Asher, 1991\] derive implieatures by classi-cal or nonclassical logical inference with one ormore licensing rules defining a class of implica-tures.
Our coherence rules are similar conceptu-ally to the licensing rules in Lascarides et al'smodel of temporal implicature.
(However, dif-ferent coherence relations play a role in indirectanswers.)
While Lascarides et al model tem-poral implicatures as defeasible inferences, suchan approach to indirect answers would fail todistinguish what R intends to convey by his re-sponse from other default inferences.
We claimthat R's response in (1), for example, does notwarrant he attribution to R of the intention toconvey that the rear axle of R's car is made ofmetal.
Hirsehberg's model for deriving scalar im-plicatures addresses only a few of the types ofindirect answers that our model does.
Further-more, our discourse-plan-based approach avoidsproblems faced by licensing-rule-based approachesin handling backward cancellation and multiple-utterance responses \[Green and Carberry, 1992\].Also, a potential problem faced by those ap-proaches is scalability, i.e., as licensing rules forhandling more types of implieature are added, ruleconflicts may arise and tractability may decrease.In contrast, our approach avoids uch problems byrestricting the use of logical inference.6.
CONCLUSIONWe have described our implemented computa-tional model for interpreting and generating in-direct answers to Yes-No questions.
Its main fea-tures are 1) a discourse-plan-based approach toimplicature, 2) a reversible architecture, 3) a hy-brid reasoning model, and 4) use of stimulus condi-tions for modeling a speaker's motivation for pro-viding appropriate extra information.
The modelhandles a wider range of types of indirect answersthan previous computational models.
Further-more, since Yes-No questions and their answershave features in common with other types of adja-cency pairs \[Levinson, 1983\], we expect hat thisapproach can be extended to them as well.
Fi-nally, a discourse-plan-based approach to implica-ture has significant advantages over a licensing-rule-based approach.
In the future, we wouldlike to integrate our interpretation a d generationcomponents with a dialogue system and investi-gate other factors in generating indirect answers(e.g.
multiple goals, stylistic concerns).Re ferences\[Allen, 1979\] James F. Allen.
A Plan-Based Ap-proach 1o Speech Act Recognition.
PhD the-sis, University of Toronto, Toronto, Ontario,Canada, 1979.\[American Express transcripts, 1992\]American Express tapes.
Transcripts of audio-tape conversations made at SRI International,Menlo Park, California.
Prepared by JaequelineKowto under the direction of Patti Price.\[Brown and Levinson, 1978\] Penelope Brown andStephen Levinson.
Universals in languageusage: Politeness phenomena.
In Es-ther N. Goody, editor, Questions and politeness:Strategies in social inleraction, pages 56-289.Cambridge University Press, Cambridge, 1978.\[Carberry, 1990\] Sandra Carberry.
Plan Recogni-tion in Natural Language Dialogue.
MIT Press,Cambridge, Massachusetts, 1990.\[Clark and Marshall, 1981\] H. Clark and C. Mar-shall.
Definite reference and mutual knowl-edge.
In A. K. Joshi, B. Webber, and I. Sag,editors, Elements of discourse understanding.Cambridge University Press, Cambridge, 1981.64\[Dahlgren, 1989\] Kathleen Dahlgren.
Coherencerelation assignment.
In Proceedings of the An-nual Meeting of the Cognitive Science Society,pages 588-596, 1989.\[Diller, 1989\] Anne-Marie Diller.
La pragmatiquedes questions et des rdponses.
In TfibingerBeitr~ige zur Linguistik 243.
Gunter Narr Ver-lag, Tiibingen, 1989.\[Gazdar, 1979\] G. Gazdar.
Pragmatics: lmplica-ture, Presupposition, and Logical Form.
Aca-demic Press, New York, 1979.\[Green, 1990\] Nancy L. Green.
Normal state im-plicature.
In Proceedings of the 28th AnnualMeeting of the Association for ComputationalLinguistics, pages 89-96, 1990.\[Green, in preparation\] Nancy L. Green.
A Com-putational Model for Interpreting and Generat-ing Indirect Answers.
PhD thesis, University ofDelaware, in preparation.\[Green and Carberry, 1992\] Nancy L. Green andSandra Carberry.
Conversational implicaturesin indirect replies.
In Proceedings of the 30thAnnual Meeting of the Association for Compu-tational Linguistics, pages 64-71, 1992.\[Grice, 1975\] H. Paul Grice.
Logic and conver-sation.
In P. Cole and J. L. Morgan, editors,Syntax and Semantics III: Speech Acts, pages41-58, New York, 1975.
Academic Press.\[Grimes, 1975\] J. E. Grimes.
The Thread of Dis-course.
Mouton, The Hague, 1975.\[Halliday, 1976\] M. Halliday.
Cohesion in English.Longman, London, 1976.\[Hinkelman, 1989\] Elizabeth Ann Hinkelman.Linguistic and Pragmatic Constraints on Utter-ance Interpretation.
PhD thesis, University ofRochester, 1989.\[Hirschberg, 1985\] Julia Bell Hirschberg.
A The-ory of Scalar Implicalure.
PhD thesis, Univer-sity of Pennsylvania, 1985.\[Hobbs, 1979\] Jerry R. Hobbs.
Coherence andcoreference.
Cognitive Science, 3:67-90, 1979.\[Horacek, 1992\] Helmut Horacek.
An IntegratedView of Text Planning.
In R. Dale, E. Hovy, D.RSsner, and O.
Stock, editors, Aspects of Auto-mated Natural Language Generation, pages 29-44, Berlin, 1992.
Springer-Verlag.\[Hovy, 1988\] Eduard H. Hovy.
Planning coherentmultisentential text.
In Proceedings of the 26thAnnual Meeting of the Association for Compu-tational Linguistics, pages 163-169, 1988.\[Lakoff, 1973\] Robin Lakoff.
Questionable an-swers and answerable questions.
In Braj B.Kachru, Robert B. Lees, Yakov Malkiel, An-gelina Pietrangeli, and Sol Saporta, editors, Pa-pers in Honor of Henry and Rende Kahane,pages 453-467, Urbana, 1973.
University of Illi-nois Press.\[Lascarides and Asher, 1991\] Alex Lascarides andNicholas Asher.
Discourse relations and defea-sible knowledge.
In Proceedings of the 29th An-nual Meeting of the Association for Computa-tional Linguistics, pages 55-62, 1991.\[Levinson, 1983\] S. Levinson.
Pragmatics.
Cam-bridge University Press, Cambridge, 1983.\[McCafferty, 1987\] Andrew Schaub McCafferty.Reasoning about lmplicature: a Plan-Based Ap-proach.
PhD thesis, University of Pittsburgh,1987.\[McKeown, 1985\] Kathleen R. McKeown.
TextGeneration.
Cambridge University Press, 1985.\[Mann and Thompson, 1987\] W. C. Mann andS.
A. Thompson.
Rhetorical structure theory:Toward a functional theory of text organization.Text, 8(3):167-182, 1987.\[Moore and Paris, 1988\] Johanna D. Moore andCecile L. Paris.
Constructing coherent text us-ing rhetorical relations.
In Proceedings of thelOth Annual Conference of the Cognitive Sci-ence Society, August 1988.\[Perrault and Allen, 1980\] R. Per-rault and J. Allen.
A plan-based analysis ofindirect speech acts.
American Journal of Com-putational Linguistics, 6(3-4):167-182, 1980.\[Polanyi, 1986\] Livia Polanyi.
The linguistics dis-course model: Towards a formal theory of dis-course structure.
Technical Report 6409, BoltBeranek and Newman Laboratories Inc., Cam-bridge, Massachusetts, 1987.\[Reichman, 1984\] Rachel Reichman.
Extendedperson-machine interface.
Artificial Intelli-gence, 22:157-218, 1984.\[StenstrSm, 1984\] Anna-Brita StenstrSm.
Ques-tions and responses in english conversation.
InClaes Schaar and Jan Svartvik, editors, LundStudies in English 68.
CWK Gleerup, MalmS,Sweden, 1984.\[Thomason, 1990\] Richmond H. Thomason.
Ac-commodation, meaning, and implicature: In-terdisciplinary foundations for pragmatics.
InP.
Cohen, J. Morgan, and M. Pollack, edi-tors, Intentions in Communication.
MIT Press,Cambridge, Massachusetts, 1990.\[Wu and Lytinen, 1990\] Horng Jyh Wu andSteven Lytinen.
Coherence relation reasoningin persuasive discourse.
In Proceedings of theAnnual Meeting of the Cognitive Science Soci-ety, pages 503-510, 1990.65
