Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 138?141,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsOWNS: Cross-lingual Word Sense Disambiguation Using WeightedOverlap Counts and Wordnet Based Similarity MeasuresLipta Mahapatra Meera MohanDharmsinh Desai UniversityNadiad, Indialipta.mahapatra89@gmail.commu.mohan@gmail.comMitesh M. Khapra Pushpak BhattacharyyaIndian Institute of Technology BombayPowai, Mumbai 400076,Indiamiteshk@cse.iitb.ac.inpb@cse.iitb.ac.inAbstractWe report here our work on EnglishFrench Cross-lingual Word Sense Disam-biguation where the task is to find thebest French translation for a target Englishword depending on the context in which itis used.
Our approach relies on identifyingthe nearest neighbors of the test sentencefrom the training data using a pairwisesimilarity measure.
The proposed mea-sure finds the affinity between two sen-tences by calculating a weighted sum ofthe word overlap and the semantic over-lap between them.
The semantic overlapis calculated using standard Wordnet Sim-ilarity measures.
Once the nearest neigh-bors have been identified, the best trans-lation is found by taking a majority voteover the French translations of the nearestneighbors.1 IntroductionCross Language Word Sense Disambiguation(CL-WSD) is the problem of finding the correcttarget language translation of a word given thecontext in which it appears in the source language.In many cases a full disambiguation may not benecessary as it is common for different meaningsof a word to have the same translation.
This is es-pecially true in cases where the sense distinctionis very fine and two or more senses of a word areclosely related.
For example, the two senses ofthe word letter, namely, ?formal document?
and?written/printed message?
have the same Frenchtranslation ?lettre?.
The problem is thus reducedto distinguishing between the coarser senses ofa word and ignoring the finer sense distinctionswhich is known to be a common cause of errorsin conventional WSD.
CL-WSD can thus be seenas a slightly relaxed version of the conventionalWSD problem.
However, CL-WSD has its ownset of challenges as described below.The translations learnt from a parallel corpusmay contain a lot of errors.
Such errors are hardto avoid due to the inherent noise associated withstatistical alignment models.
This problem can beovercome if good bilingual dictionaries are avail-able between the source and target language.
Eu-roWordNet1 can be used to construct such a bilin-gual dictionary between English and French but itis not freely available.
Instead, in this work, weuse a noisy statistical dictionary learnt from theEuroparl parallel corpus (Koehn, 2005) which isfreely downloadable.Another challenge arises in the form of match-ing the lexical choice of a native speaker.
For ex-ample, the word coach (as in, vehicle) may gettranslated differently as autocar, autobus or buseven when it appears in very similar contexts.Such decisions depend on the native speaker?s in-tuition and are very difficult for a machine to repli-cate due to their inconsistent usage in a paralleltraining corpus.The above challenges are indeed hard to over-come, especially in an unsupervised setting, as ev-idenced by the lower accuracies reported by allsystems participating in the SEMEVAL SharedTask on Cross-lingual Word Sense Disambigua-tion (Lefever and Hoste, 2010).
Our systemranked second in the English French task (in theout-of-five evaluation).
Even though its averageperformance was lower than the baseline by 3%it performed better than the baseline for 12 out ofthe 20 target nouns.Our approach identifies the top-five translationsof a word by taking a majority vote over the trans-lations appearing in the nearest neighbors of thetest sentence as found in the training data.
Weuse a pairwise similarity measure which finds theaffinity between two sentences by calculating a1http://www.illc.uva.nl/EuroWordNet138weighted sum of the word overlap and the seman-tic overlap between them.
The semantic overlap iscalculated using standard Wordnet Similarity mea-sures.The remainder of this paper is organized as fol-lows.
In section 2 we describe related work onWSD.
In section 3 we describe our approach.
InSection 4 we present the results followed by con-clusion in section 5.2 Related WorkKnowledge based approaches to WSD such asLesk?s algorithm (Lesk, 1986), Walker?s algorithm(Walker and Amsler, 1986), Conceptual Density(Agirre and Rigau, 1996) and Random Walk Algo-rithm (Mihalcea, 2005) are fundamentally overlapbased algorithms which suffer from data sparsity.While these approaches do well in cases wherethere is a surface match (i.e., exact word match)between two occurrences of the target word (say,training and test sentence) they fail in cases wheretheir is a semantic match between two occurrencesof the target word even though there is no surfacematch between them.
The main reason for thisfailure is that these approaches do not take intoaccount semantic generalizations (e.g., train is-a vehicle).On the other hand, WSD approaches which useWordnet based semantic similarity measures (Pat-wardhan et al, 2003) account for such seman-tic generalizations and can be used in conjunc-tion with overlap based approaches.
We there-fore propose a scoring function which combinesthe strength of overlap based approaches ?
fre-quently co-occurring words indeed provide strongclues ?
with semantic generalizations using Word-net based similarity measures.
The disambigua-tion is then done using k-NN (Ng and Lee, 1996)where the k nearest neighbors of the test sentenceare identified using this scoring function.
Oncethe nearest neighbors have been identified, the besttranslation is found by taking a majority vote overthe translations of these nearest neighbors.3 Our approachIn this section we explain our approach for CrossLanguage Word Sense Disambiguation.
The mainemphasis is on disambiguation i.e.
finding Englishsentences from the training data which are closelyrelated to the test sentence.3.1 Motivating ExamplesTo explain our approach we start with two moti-vating examples.
First, consider the following oc-currences of the word coach:?
S1:...carriage of passengers by coach andbus...?
S2:...occasional services by coach and busand the transit operations...?
S3:...the Gloucester coach saw the game...In the first two cases, the word coach appearsin the sense of a vehicle and in both the cases theword bus appears in the context.
Hence, the sur-face similarity (i.e., word-overlap count) of S1andS2would be higher than that of S1and S3andS2and S3.
This highlights the strength of overlapbased approaches ?
frequently co-occurring wordscan provide strong clues for identifying similar us-age patterns of a word.Next, consider the following two occurrences ofthe word coach:?
S1:...I boarded the last coach of the train...?
S2:...I alighted from the first coach of thebus...Here, the surface similarity (i.e., word-overlapcount) of S1and S2is zero even though in boththe cases the word coach appears in the sense ofvehicle.
This problem can be overcome by us-ing a suitable Wordnet based similarity measurewhich can uncover the hidden semantic similaritybetween these two sentences by identifying that{bus, train} and {boarded, alighted} are closelyrelated words.3.2 Scoring functionBased on the above motivating examples, we pro-pose a scoring function for calculating the simi-larity between two sentences containing the targetword.
Let S1be the test sentence containing mwords and let S2be a training sentence containingn words.
Further, let w1ibe the i-th word of S1and let w2jbe the j-th word of S2.
The similaritybetween S1and S2is then given by,Sim(S1, S2) = ?
?Overlap(S1, S2)+ (1?
?)
?
Semantic Sim(S1, S2)(1)where,139Overlap(S1, S2) =1m + nm?i=1n?j=1freq(w1i) ?
1{w1i=w2j}and,Semantic Sim(S1, S2) =1mm?i=1Best Sim(w1i, S2)where,Best Sim(w1i, S2) = maxw2j?S2lch(w1i, w2j)We used the lch measure (Leacock and Chodorow,1998) for calculating semantic similarity of twowords.
The semantic similarity between S1andS2is then calculated by simply summing over themaximum semantic similarity of each constituentword of S1over all words of S2.
Also note thatthe overlap count is weighted according to the fre-quency of the overlapping words.
This frequencyis calculated from all the sentences in the train-ing data containing the target word.
The ratio-nal behind using a frequency-weighted sum is thatmore frequently appearing co-occurring words arebetter indicators of the sense of the target word(of course, stop words and function words are notconsidered).
For example, the word bus appearedvery frequently with coach in the training dataand was a strong indicator of the vehicle senseof coach.
The values of Overlap(S1, S2) andSemantic Sim(S1, S2) are appropriately nor-malized before summing them in Equation (1).
Toprevent the semantic similarity measure from in-troducing noise by over-generalizing we chose avery high value of ?.
This effectively ensuredthat the Semantic Sim(S1, S2) term in Equation(1) became active only when the Overlap(S1, S2)measure suffered data sparsity.
In other words, weplaced a higher bet on Overlap(S1, S2) than onSemantic Sim(S1, S2) as we found the formerto be more reliable.3.3 Finding translations of the target wordWe used GIZA++2 (Och and Ney, 2003), a freelyavailable implementation of the IBM alignmentmodels (Brown et al, 1993) to get word levelalignments for the sentences in the English-French2http://sourceforge.net/projects/giza/portion of the Europarl corpus.
Under this align-ment, each word in the source sentence is alignedto zero or more words in the corresponding tar-get sentence.
Once the nearest neighbors for a testsentence are identified using the similarity scoredescribed earlier, we use the word alignment mod-els to find the French translation of the target wordin the top-k nearest training sentences.
Thesetranslations are then ranked according to the num-ber of times they appear in these top-k nearestneighbors.
The top-5 most frequent translationsare then returned as the output.4 ResultsWe report results on the English-French Cross-Lingual Word Sense Disambiguation task.
Thetest data contained 50 instances for 20 polysemousnouns, namely, coach, education, execution, fig-ure, job, letter, match, mission, mood, paper, post,pot, range, rest, ring, scene, side, soil, strain andtest.
We first extracted the sentences containingthese words from the English-French portion ofthe Europarl corpus.
These sentences served as thetraining data to be compared with each test sen-tence for identifying the nearest neighbors.
Theappropriate translations for the target word in thetest sentence were then identified using the ap-proach outlined in section 3.2 and 3.3.
For thebest evaluation we submitted two runs: one con-taining only the top-1 translation and another con-taining top-2 translations.
For the oof evaluationwe submitted one run containing the top-5 trans-lations.
The system was evaluated using Precisionand Recall measures as described in the task pa-per (Lefever and Hoste, 2010).
In the oof evalua-tion our system gave the second best performanceamong all the participants.
However, the averageprecision was 3% lower than the baseline calcu-lated by simply identifying the five most frequenttranslations of a word according to GIZA++ wordalignments.
A detailed analysis showed that in theoof evaluation we did better than the baseline for12 out of the 20 nouns and in the best evaluationwe did better than the baseline for 5 out of the 20nouns.
Table 1 summarizes the performance of oursystem in the best evaluation and Table 2 gives thedetailed performance of our system in the oof eval-uation.
In both the evaluations our system pro-vided a translation for every word in the test dataand hence the precision was same as recall in allcases.
We refer to our system as OWNS (Overlap140and WordNet Similarity).System Precision RecallOWNS 16.05 16.05Baseline 20.71 20.71Table 1: Performance of our system in best evalu-ationWord OWNS Baseline(Precision) (Precision)coach 45.11 39.04education 82.15 80.4execution 59.22 39.63figure 30.56 35.67job 43.93 40.98letter 46.01 42.34match 31.01 15.73mission 55.33 97.19mood 35.22 64.81paper 48.93 40.95post 36.65 41.76pot 26.8 65.23range 16.28 17.02rest 39.89 38.72ring 39.74 33.74scene 33.89 38.7side 37.85 36.58soil 67.79 59.9strain 21.13 30.02test 64.65 61.31Average 43.11 45.99Table 2: Performance of our system in oof evalua-tion5 ConclusionWe described our system for English FrenchCross-Lingual Word Sense Disambiguation whichcalculates the affinity between two sentences bycombining the weighted word overlap counts withsemantic similarity measures.
This similarityscore is used to find the nearest neighbors of thetest sentence from the training data.
Once thenearest neighbors have been identified, the besttranslation is found by taking a majority vote overthe translations of these nearest neighbors.
Oursystem gave the second best performance in theoof evaluation among all the systems that partic-ipated in the English French Cross-Lingual WordSense Disambiguation task.
Even though the av-erage performance of our system was less than thebaseline by around 3%, it outperformed the base-line system for 12 out of the 20 nouns.ReferencesEneko Agirre and German Rigau.
1996.
Word sensedisambiguation using conceptual density.
In In Pro-ceedings of the 16th International Conference onComputational Linguistics (COLING).Peter E Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:parameter estimation.
Computational Linguistics,19:263?311.P.
Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In In Proceedings of theMT Summit.C.
Leacock and M. Chodorow, 1998.
Combining lo-cal context and WordNet similarity for word senseidentification, pages 305?332.
In C. Fellbaum (Ed.
),MIT Press.Els Lefever and Veronique Hoste.
2010.
Semeval-2010 task 3: Cross-lingual word sense disambigua-tion.
In Proceedings of the 5th International Work-shop on Semantic Evaluations (SemEval-2010), As-sociation for Computational Linguistics.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: how to tell apine cone from an ice cream cone.
In In Proceed-ings of the 5th annual international conference onSystems documentation.Rada Mihalcea.
2005.
Large vocabulary unsupervisedword sense disambiguation with graph-based algo-rithms for sequence data labeling.
In In Proceed-ings of the Joint Human Language Technology andEmpirical Methods in Natural Language ProcessingConference (HLT/EMNLP), pages 411?418.Hwee Tou Ng and Hian Beng Lee.
1996.
Integratingmultiple knowledge sources to disambiguate wordsenses: An exemplar-based approach.
In In Pro-ceedings of the 34th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages40?47.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Siddharth Patwardhan, Satanjeev Banerjee, and TedPedersen.
2003.
Using measures of semantic re-latedness for word sense disambiguation.
In In pro-ceedings of the Fourth International Conference onIntelligent Text Processing and Computation Lin-guistics (CICLing.D.
Walker and R. Amsler.
1986.
The use of machinereadable dictionaries in sublanguage analysis.
In InAnalyzing Language in Restricted Domains, Grish-man and Kittredge (eds), LEA Press, pages 69?83.141
