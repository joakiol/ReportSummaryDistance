Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 307?310,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsCMU System Combination via Hypothesis Selection for WMT?10Almut Silja HildebrandCarnegie Mellon UniversityPittsburgh, USAsilja@cs.cmu.eduStephan VogelCarnegie Mellon UniversityPittsburgh, USAvogel@cs.cmu.eduAbstractThis paper describes the CMU entry forthe system combination shared task atWMT?10.
Our combination method is hy-pothesis selection, which uses informationfrom n-best lists from the input MT sys-tems, where available.
The sentence levelfeatures used are independent from theMT systems involved.
Compared to thebaseline we added source-to-target wordalignment based features and trained sys-tem weights to our feature set.
We com-bined MT systems for French - Englishand German - English using provided dataonly.1 IntroductionFor the combination of machine translation sys-tems there have been several approaches describedin recent publications.
One uses confusion net-works formed along a skeleton sentence to com-bine translation systems as described in (Rosti etal., 2008) and (Karakos et al, 2008).
A differentapproach described in (Heafield et al, 2009) is notkeeping the skeleton fixed when aligning the sys-tems.
Another approach selects whole hypothesesfrom a combined n-best list (Hildebrand and Vo-gel, 2008).Our setup follows the latter approach.
We com-bine the output from the submitted translation sys-tems, including n-best lists where available, intoone joint n-best list, then calculate a set of fea-tures consistently for all hypotheses.
We use MERtraining on the provided development data to de-termine feature weights and re-rank the joint n-best list.
We train to maximize BLEU.2 FeaturesFor our entries to the WMT?09 we used the follow-ing feature groups (in parenthesis are the numberof separate feature values per group):?
Language model scores (3)?
Word lexicon scores (6)?
Sentence length features (3)?
Rank feature (1)?
Normalized n-gram agreement (6)?
Source-target word alignment features (6)?
Trained system weights (no.
of systems)The details on language model and word lexi-con scores can be found in (Hildebrand and Vogel,2008) and details on the rank feature and the nor-malized n-gram agreement can be found in (Hilde-brand and Vogel, 2009).
We use three sentencelength features, which are the ratio of the hypoth-esis length to the length of the source sentence,the diversion of this ratio from the overall lengthratio of the bilingual training data and the differ-ence between the hypothesis length and the av-erage length of the hypotheses in the n-best listfor the respective source sentence.
The systemweights are trained together with the other featureweights during MERT using a binary feature persystem.
To the feature vector for each hypothe-sis one feature per input system is added; for eachhypothesis one of the features is one, indicatingwhich system it came from, all others are zero.2.1 Source-Target Word Alignment FeaturesWe trained the IBM word alignment models upto model 4 using the GIZA++ toolkit (Och andNey, 2003) on the bilingual training corpus.
Thena forced alignment algorithm utilizes the trainedmodels to align each source sentence to each trans-lation hypothesis in its respective n-best list.We use the alignment score given by the wordalignment models, the number of unaligned words307and the number of NULL aligned words, all nor-malized by the sentence length, as three separatefeatures.
We calculate these alignability featuresfor both language directions.3 ExperimentsIn the WMT shared translation task only a verysmall number of participants submitted n-bestlists, e.g.
in the German-English track there wereonly four n-best lists among the 16 submissions.Our combination method is proven to work signif-icantly better when n-best lists are available.For all our experiments on the data fromWMT?09, which was available for system combi-nation development as well as the WMT?10 sharedtask data we used the same setup and the same sta-tistical models.To train our language models and word lexicawe only used provided data.
We trained the sta-tistical word lexica on the parallel data providedfor each language pair1.
For each combination weused three language models: a 4-gram languagemodel trained on the English part of the paralleltraining data, a 1.2 giga-word 3-gram languagemodel trained on the provided monolingual En-glish data, and an interpolated 5-gram languagemodel trained on the English GigaWord corpus.We used the SRILM toolkit (Stolcke, 2002) fortraining.
We chose to train three separate LMsfor the three corpora, so the feature weight train-ing can automatically determine the importance ofeach corpus for this task.
The reason for trainingonly a 3-gram LM from the wmt10 monolingualdata was simply that there were not sufficient timeand resources available to train a bigger model.For each of the two language pairs we compareda combination that used the word alignment fea-tures, or trained system weights or both of thesefeature groups in addition to the features describedin (Hildebrand and Vogel, 2009) which serves abaseline for this set of experiments.For combination we tokenized and lowercasedall data, because the n-best lists were submittedin various formats.
Therefore we report the caseinsensitive scores here.
The combination was op-timized toward the BLEU metric, therefore TERresults might not be very meaningful here and areonly reported for completeness.1http://www.statmt.org/wmt10/translation-task.html#training3.1 French-English data from WMT?09We used 14 systems from the restricted data trackof the WMT?09 including five n-best lists.
Thescores of the individual systems for the combina-tion tuning set range from BLEU 27.93 for the bestto 15.09 for the lowest ranked individual system(case insensitive evaluation).system tune testbest single 27.93 / 56.53 27.21 / 56.99baseline 30.17 / 54.76 28.89 / 55.74+ wrd al 30.67 / 54.34 28.69 / 55.67+ sys weights 29.71 / 55.45 28.07 / 56.18all features 30.30 / 54.53 28.37 / 55.77Table 1: French-English Results: BLEU / TERThe combination outperforms the best singlesystem by 1.7 BLEU points.
Here adding the 14binary features for training system weights withMERT hurts the combinations performance on theunseen data.
The reason for this might be therather small tuning set of 502 sentences with onereference.
Adding the word alignment featuresdoes not improve the result either, the differenceto the baseline is at the noise level.3.2 German-English data from WMT?09For our experiments on the development data forGerman-English we used the top 12 systems, scor-ing between BLEU 23.01 and BLEU 16.06, ex-cluding systems known to use data beyond the pro-vided data.
Within those 12 system outputs werefour n-best lists, three of which were 100-best andone was 10-best.system tune testbest single 23.01 / 60.52 21.44 / 62.33baseline 26.28 / 58.69 23.62 / 60.49+ wrd al 26.25 / 59.13 23.42 / 61.11+ sys weights 26.78 / 58.48 23.28 / 60.80all features 26.81 / 58.12 23.51 / 60.25Table 2: German-English Results: BLEU / TEROur system combination via hypothesis selec-tion could improve translation quality by +2.2BLEU over the best single system on the unseentest set.
Again, the differences between the fourdifferent feature sets are not significant on the un-seen test set.3083.3 French-English WMT?10 systemcombination shared taskOut of 14 systems submitted to the French-Englishtranslation task, we combined the top 11 systems,the best of which scored 28.58 BLEU and the last24.16 BLEU on the tuning set.
There were onlythree n-best lists among the submissions.
We in-cluded up to 100 hypotheses per system in ourjoint n-best list.system tune testbest sys.
28.58 / 54.17 29.98 / 52.62 / 53.88baseline 30.67 / 52.62 29.94 / 52.53 / -+ w. al 30.69 / 52.76 29.97 / 52.76 / 53.76+ sys w. 30.90 / 52.44 29.79 / 52.84 / 54.05all feat.
31.10 / 52.06 29.80 / 52.86 / 53.67Table 3: French-English Results: BLEU / TER /MaxSimOur system combination via hypothesis selec-tion could not improve the translation quality com-pared to the best single system on the unseen data.Adding any of the new feature groups to the base-line does not change the result of the combinationsignificantly.
This result could be explained by thefact, that due to computational problems and timeconstraints we were not able to train our models onthe whole provided French-English training data.This should only affect the lexicon and word align-ment feature groups though.3.4 German-English WMT?10 systemcombination shared taskFor the German-English combination we used 13out of the 16 submitted systems, which scored be-tween BLEU 25.01 to BLEU 19.76 on the tuningset.
Our combination could improve translationquality by +1.64 BLEU compared to the best sys-tem.system tune testbest sys.
25.01 / 58.34 23.89 / 59.14 / 51.10baseline 26.47 / 56.89 25.44 / 57.96 / -+ w. al 26.37 / 57.02 25.25 / 58.34 / 50.72+ sys w. 27.67 / 56.05 25.53 / 57.70 / 51.06all feat.
27.66 / 56.35 25.25 / 57.86 / 50.83Table 4: German-English Results: BLEU / TER /MaxSimThe word alignment features seem to hurt per-formance slightly, which might be due to the more	  	    !"	#		$	%&'	Figure 1: German-English ?10: Contributions ofthe individual systems to the final translation, per-centages and absolute number of hyps chosen.difficult word alignment between German and En-glish compared to other language pairs.
But thisis not really a strong conclusion, because all dif-ferences of the results on the unseen data are notsignificant.Figure 1 shows, how many hypotheses werecontributed by the individual systems to the finaltranslation (unseen data) in the baseline combina-tion compared with the one with trained systemweights.
The systems A to M are ordered by theirBLEU score on the development set.
The barsshow percentages of the test set, the numbers listednext to the systems A to M give the absolute num-ber of hypotheses chosen from the system for thetwo depicted combinations.
The systems whichprovided n-best lists, marked with a star in the di-agram, clearly dominate the selection in the base-line, but this effect is gone when system weightsare used.
The dominance of system A in the lat-ter is to be expected, because it is a whole BLEUpoint ahead of the next ranking system on the sys-tem combination tuning set.
In the baseline com-bination identical hypotheses contributed by dif-ferent systems have an identical total score.
In309that case the hypothesis is attributed to all systemswhich contributed it.
This accounts for the highertotal number of hypotheses shown in the graphicfor the baseline as well as for part of the contri-butions of the low ranking systems.
For example35 hypotheses were provided identically from twosystems and still four hypotheses were producedby all 13 systems, for example the sentence: ?aberes geht auch um wirtschaftliche beziehungen .?
-?but it is also about economic relations .
?.4 ConclusionsIn this paper we explored new features in our sys-tem combination system, which performs hypoth-esis selection.
We used hypothesis to source sen-tence alignment scores as well system weight fea-tures.Most systems available for combination did notsubmit n-best lists, which decreases the effective-ness of our combination method significantly.The reason for not getting an improvement fromword alignment features might be that the top sys-tems might be using more clever word alignmentstrategies than running the GIZA++ toolkit out ofthe box.
Therefore the alignability according tothese weaker models does not give useful rankinginformation for rescoring.Experiments on different language pairs anddata sets have shown improvements for trainingsystem weights in the past for certain setups.Combining up to 14 individual translation sys-tems adds that many features to the feature set forwhich weights have to optimized via MERT.
Theprovided tuning set of 455 sentences with onlyone reference is extremely small.
It is possible,that MERT could not reliably determine featureweights here.
In the setup where this feature setwas used successfully, a tuning set of close to 2000lines with four references was available.
It is notpossible to improve the tuning data situation by us-ing the provided data from last years workshop asadditional tuning data, because the set of systemssubmitted is not the same and even the systemssubmitted by the same sites might have changedsignificantly.Interesting to note is that looking at the num-bers, the German-English combination with animprovement of +1.64 BLEU over the best sin-gle system seems to have worked much better thanthe French-English one with no improvement.
Butlooking at the preliminary human evaluation resultthe picture is opposite: For German-English ourcombination is ranked below several of the singlesystems and most of the combinations, while forFrench-English it tops the list of all systems andcombinations in the workshop.AcknowledgmentsWe would like to thank the participants in theWMT?10 shared translation task for providingtheir data, especially n-best lists.
This work waspartly funded by DARPA under the project GALE(Grant number #HR0011-06-2-0001).ReferencesKenneth Heafield, Greg Hanneman, and Alon Lavie.2009.
Machine translation system combination withflexible word ordering.
In StatMT ?09: Proceed-ings of the Fourth Workshop on Statistical MachineTranslation, pages 56?60, Morristown, NJ, USA.Association for Computational Linguistics.Almut Silja Hildebrand and Stephan Vogel.
2008.Combination of machine translation systems via hy-pothesis selection from combined n-best lists.
InMT at work: Proceedings of the Eighth Confer-ence of the Association for Machine Translation inthe Americas, pages 254?261, Waikiki, Hawaii, Oc-tober.
Association for Machine Translation in theAmericas.Almut Silja Hildebrand and Stephan Vogel.
2009.CMU system combination for WMT?09.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, pages 47?50, Athens, Greece,March.
Association for Computational Linguistics.Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,and Markus Dreyer.
2008.
Machine translationsystem combination using itg-based alignments.
InProceedings of ACL-08: HLT, Short Papers, pages81?84, Columbus, Ohio, June.
Association for Com-putational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2008.
Incremental hy-pothesis alignment for building confusion networkswith application to machine translation system com-bination.
In Proceedings of the Third Workshopon Statistical Machine Translation, pages 183?186,Columbus, Ohio, June.
Association for Computa-tional Linguistics.Andreas Stolcke.
2002.
Srilm - an extensible lan-guage modeling toolkit.
In Proceedings Interna-tional Conference for Spoken Language Processing,Denver, Colorado, September.310
