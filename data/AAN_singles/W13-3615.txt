Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 109?114,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsA Noisy Channel Model Framework for Grammatical CorrectionL.
Amber Wilcox-O?HearnDepartment of Computer ScienceUniversity of Torontoamber@cs.toronto.eduAbstractWe report on the TOR system that partic-ipated in the 2013 CoNLL shared task ongrammatical correction.
The system wasa provisional implementation of a beamsearch correction over a noisy channelmodel.
Although the results on the sharedtask test set were poor, the approach maystill be promising, as there are many as-pects of the current implementation thatcould be optimised.
Grammatical correc-tion is inherently difficult both to performand to evaluate.
As such, possible im-provements to the evaluation are also dis-cussed.1 IntroductionGrammatical correction covers many subproblemsincluding spelling correction, lexical choice, andeven paraphrasing.
There is a sense in which syn-tax is separable from semantics and discourse.
Asentence may be parsable in a language, even if itis nonsensical.
On the other hand, many errors thatwe consider a matter of grammar, such as someinstances of determiner choice, are only incorrectbecause of the semantic or discourse properties ofthe sentence in its context.Another complexity is that there are degrees ofgrammatical correctness.
Some sentences are notparsable, but others are just awkward sounding, orunconventional.So a grammatical error may manifest in a mes-sage that doesn?t code a meaning in the languageat all, and the task becomes inferring a plausiblemeaning and coding it correctly.
This is analogousto non-word spelling errors.
Alternatively, it mayresult in a meaning that is not exactly what wasintended.
This is more like a real-word spellingerror.In either case, the implication is that in order todetect and correct a grammatical error, we must beable to infer the intended meaning.
This points tothe depth of the problem.1.1 Confusion SetsA common and useful way to construe error cor-rection, including grammatical correction, is tofirst classify sets of alternatives that are mutu-ally confusable.
This is typically done at thelexical level, though the idea is generalizable tomultiword expressions, constructions, or phrases.Then the text under examination is searched forinstances of members of these confusion sets.
Fi-nally a heuristic is used to decide whether one ofits alternatives would have been a more appropri-ate choice in its context.
Within this framework,there are different approaches to these steps.In choosing our confusion sets, we wanted tobe flexible and extensible.
Therefore, we did notwant to depend on corpora of errors with annotatedcorrections to infer alternatives.
So we collectedgeneral statistics from corpora that were assumedto be correct, and used those to evaluate proposedcorrections to observed sentences.
This approachis not unique to this model.
It is seen, for exam-ple, in (De Felice and Pulman, 2007), (Tetreaultand Chodorow, 2008), and (Gamon et al 2009),among others.However, the main difference between our sys-tem and previous ones is that we do not select ourconfusion sets in advance of statistical modelling.That is, although the confusion sets we used werebased on POS tagsets, there was no classifying orlearning to discriminate among members of a con-fusion set before the task.
The aim of this choicewas to make our system more general and flex-ible.
We can now modify our confusion sets atruntime without retraining any models.
The provi-sional confusion sets we used are somewhat arbi-trary, but this can be changed independently of therest of the system.Although our system was not competitive at this109stage, it provides a preliminary basis for furtherexperiments.The remainder of this paper describes theframework and the initial implementation of thatframework that was used in the shared task, aswell as future improvements to the model.
Wealso discuss the difficulty in evaluating such sys-tems.
All of the code used to generate our submis-sion is freely available for examination and use onGitHub (Wilcox-O?Hearn and Wilcox-O?Hearn,2013).2 Overview of the systemWe approach grammatical error correction using anoisy channel model.
Such a model is also used by(Park and Levy, 2011) and (West, Park, and Levy,2011).
One appealing aspect of this model is that itmakes explicit the cost of error, such that a correc-tion must not only be more likely than the observa-tion to be proposed, but it must be more likely evengiven that errors are less likely than non-errors toa degree specified by the properties of the channel.In practice this can mitigate false positives that re-sult from overconfidence in a language model.A grammatical error is treated as a transforma-tion of some original, correct sentence, S, gener-ated by a language model M .
We attempt to re-cover the original sentence by hypothesizing pos-sible transformations that could have resulted inthe observed sentence S?.
If we estimate that it ismore likely that S was generated by M and trans-formed into S?
than that S?
was generated by Mand left unchanged, we propose S as a correction.In this preliminary implementation of theframework, we use a combination of word andPOS n-gram models as the language generationmodel, while POS tags form the basis of our chan-nel model.To generate sentence hypotheses that can in-clude multiple interacting errors interleaved withnon-errors while putting a bound on the size of thesearch space, we use a left-to-right beam search.This differs from the beam search used by Dal-heimer and Ng (2012a).
In their work, the searchspace is constructed by generating variations ofthe entire sentence.
Just as here, at each iteration,they make every variation appropriate at a singleposition, but they evaluate the whole sentence con-taining that correction.
Although sentences thatrequire multiple interacting corrections will ini-tially have a low score under this method, a largeenough beam width will allow the corrections tobe made one at a time without being lost fromconsideration.
In our model, by evaluating par-tial sentences from left-to-right, we hope to lessenthe need for a large beam width, by holding off in-tegration of the continuation of the sentence, andletting it unfold in a way that more closely mimicshuman sentence comprehension.2.1 The language modelTo model language generation, we used an inter-polation of two n-gram models, a trigram modelbased on regular word types, and a 5-gram modelof POS tags.
The data for these models wasderived by combining the corrected version ofthe NUCLE corpus (Dalheimer, Ng, and Wu,2013) with a randomly chosen selection of ar-ticles from Wikipedia as provided by the West-bury Lab Wikipedia corpus (Shaoul and Westbury,2010), which we tokenised using NLTK (Bird,Loper, and Klein, 2009) to match the format ofthe shared task.
The precise set of articles usedis included in our GutHub repository (Wilcox-O?Hearn and Wilcox-O?Hearn, 2013).
We usedSRILM 1.7.0 (Stolcke, 2002) to generate a mod-est trigram model of 5K words.
We then passedthe same data through the Stanford POS taggerv3.1.4 (Toutanova, Klein, Manning, and Singer,2003) and again through SRILM to produce a POS5-gram model.2.2 The channel modelThe channel model provides a definition of trans-formations that could have been applied to a sen-tence before we observed it.
Our system consid-ers only transformations of single words, specif-ically, only single word insertions, deletions, andsubstitutions.
This cannot represent every gram-mar error we might encounter, but makes a goodfirst approximation, and it represents all errors inthis iteration of the shared task.
To simplify thedescription and implementation, we equivalentlyconsider the empty string to be a valid word in-cluded in some substitution (confusion) sets, anddefine the channel as one that sometimes replacesa word with one of the alternatives in its confu-sion set.
The probability of such replacement is aparameter ?
to be inferred.As explained in the introduction, one goal of oursystem is to allow flexible confusion sets that donot need to be fully specified in advance of learn-ing statistics about them.
Therefore, we define our110confusion sets in terms of the standard POS tagsetsas given by the Stanford tagger, using a notion ofclosed vs. open word classes.2.2.1 Closed ClassesFor our purposes, a closed word class is a set ofwords that has a relatively small, finite numberof members.
We composed the following closedclasses out of POS tagsets for the purposes of thistask:?
DT ?
{?},?
MD ?
{?},?
IN ?
TO ?
{?},?
a hand-built class called AUX, consisting of?be?, ?do?, ?have?, and ?get?
verbs, ?
TO ?{?
}.We then restricted each class to the k most fre-quently occurring words within it.
Our provisionalsystem used k = 5.In the standard tagset, the set TO contains onlythe word ?to?.
We have put ?to?
into two dif-ferent classes, because the same word form rep-resents both the preposition and infinitive verbmarker.
Although the second such class is labelled?AUX?, it does not correspond directly to thestandard definition of auxiliary as given by gram-mars of English.
First, ?to?
does not meet alofthe properties of auxiliaries.
For example, becauseit does not occur with a subject, it cannot partici-pate in subject-auxiliary inversion.
On the otherhand, although modals are traditionally a subclassof auxiliaries, we have left them separate as de-fined in the tagset.The intuition guiding those decisions was basedon grammatical function and patterns of alterna-tives.
Verb forms in English often consist of aclosed class word w, followed by a main verb, theform of which combines with the particular w toindicate the tense and aspect.
In other words, wfunctions as a verb form marker, and doesn?t carryother information.
Modals, in contrast, have uni-form grammatical co-occurrence patterns, essen-tially all being followed by bare infinitives.
Theyhave the semantic function of expressing modality,and are alternatives to one another.Ultimately, which words are best classed as al-ternatives should be determined empirically.2.2.2 Open ClassesWe used two open classes specific to this task,verbs and nouns.The verb errors of this year?s task included verbform and subject-verb agreement.
Ideally, to findcandidates for the confusion set of a verb v, wewould want to produce morphological variationsof v whose POS tag is different from that of v.This was approximated with the following heuris-tic.
We defined the prefix of v to be the initialcharacters of v, including least the first character,and not any of the final four, except when the firstcharacter was one of those.
We collected all wordsin the vocabulary starting with that prefix, whosestem given by the NLTK Porter stemmer matchedthe corresponding stem of v and that had appearedat least once with a POS tag indicating a verb of adifferent form from that of v.Similarly, the only noun errors under consider-ation were noun number errors, meaning a changefrom singular to plural or vice versa.
We used thesame prefix and stem-matching heuristic as in theverb case to find opposite-numbered nouns for thistask.3 The correction processIn order to detect multiple interacting errors, wewould like to consider every possible variation ofevery word in the sentence.
To mitigate the combi-natorial expense, we use a beam search as follows.Proceeding word-by-word through the sen-tence, we keep a list of the n most likely sentencebeginning fragments.
Our provisional system usedn = 5.
When we reach the observed word w, thenfor each sentence fragment si in the list, we com-pute the estimated probability that the correct sen-tence next contained w?
instead of w, using ourn-gram probability estimate P (w?|si), and that thechannel model transformed it to w, by dividing theprobability of error ?
by the number of variationsin the confusion set of w?, C(w?).
We also esti-mate the probability that w was the original word.Because our closed classes each include the emptystring, every empty string in the observed sentencecould have been produced by the deletion of amember of any of the closed classes.
Therefore,we also consider the possibility of inserting eachword x, from each closed class.
In total, the fol-lowing probabilities are estimated:(no error)p = P (w|si)?
(1?
?
)111and for each word x in each closed class, otherthan the empty string:(a deletion, no substitution)p = P (xw|si)?
?/|C(x)| ?
(1?
?
)and for each variation of w, w?
:(a substitution)p = P (w?|si)?
?/|C(w?
)|and for each variation of w, w?, and each word xin each closed class, other than the empty string:(a deletion and a substitution)p = P (xw?|si)?
?/|C(x)| ?
?/|C(w?
)|The n most likely such extended fragments arethen kept for the next iteration.
Finally, at the endof the sentence, the sentence with the highest prob-ability is returned as the correction.
Probabilitiesare treated as per-word perplexity in order not topenalise longer sentences.4 EvaluationThe shared task was evaluated using a section ofthe NUCLE corpus (see (Dalheimer, Ng, and Wu,2013)), and the corresponding corrections as anno-tated by English instructors.
The types of correc-tions ranged from simple and well-defined, such asthe addition, removal, or exchange of an article ordeterminer, to the entire rephrasing of a sentence.Sometimes the corrections were strictly grammat-ical, in that the original was not well-formed En-glish.
Some were more stylistic; what the studenthad written was awkward, or sounded disfluent,even if it could have been parsed acceptably.
Thisis appropriate and consistent with the nature of theproblem.
However, it does make evaluation almostas challenging as the task itself.Often if a sentence has grammatical errors,there are many different ways to repair the error.Teams were encouraged to submit alternative cor-rections when it was believed that their systems?output ought to be considered valid, even if it didnot match the particular annotation given by thegrader.Another problem with the evaluation, however,actually stemmed from the simplification of thetask.
Because grammatical correction is inher-ently difficult, and because some of the difficultyincreases gradually by type as just described, thetask for this year was made more moderate by se-lecting only 5 error types from the 27 types definedin the corpus.
However, this resulted in two diffi-culties.The first was that some error types were closelyrelated.
Errors of verb form, verb tense, verbmodal, and subject-verb agreement may haveoverlapping interpretation.
Those error types arenot necessarily distinguishable by our method.For example, there is a sentence in the test set:Firstly , security systems are improved in many ar-eas such as school campus or at the workplace .which is corrected to:Firstly , security systems have improved in manyareas such as school campus or at the workplace .with the annotation of verb tense error type, andthus not part of this task.On the other hand, there is also a sentence:... the electric systems were short circuited...which is corrected to:... the electric systems short circuited...with the annotation of verb form error type, andthus part of this task.Second, sometimes an annotation not evaluatedin this task that resulted in a change of word formwas necessarily accompanied by changes to wordsthat were included in the task.
This meant that inorder for the system to match the gold annotations,it would have to propose a sentence that was gram-matically incorrect.
This is suboptimal.
Althoughit could sometimes be mitigated by the alternativecorrection appeal process, that may not have beenadequate to address all such occurrences.
Moreaccurate scoring might be obtained if only the sen-tences that do not contain other correction typesare included in the test set.An example of this is the sentence:Take Singapore for example , these are installed...The annotation corrects this sentence to:Take Singapore for example , surveillance is in-stalled...However, the replacement of these with surveil-lance is not in the task, so to get it correct, a sys-tem would have to hypothesize:Take Singapore for example , these is installed...112Evaluation Prec.
Rec.
F-meas.Original task 0.1767 0.0481 0.0756Strict 5 types 0.2079 0.0568 0.0892With alternatives 0.3067 0.0877 0.1364Table 1: Results5 ResultsThe results of our system were not competitive.Table 1 lists our scores on the original annotation(line 1), and after alternative answers were consid-ered (line 3).
It also shows what our system wouldhave scored if only the sentences in the test setwhich contained no errors types other than thosespecified for the task were included (line 2).6 Future WorkThere are several simple steps that we expect willimprove our system.First, the language models could be improved.They could use corpora better matched to the dataset, and they could have larger vocabulary sizes.We also observe that the POS models, because oftheir inherently small vocabulary, seem to be im-paired by the backoff paradigm.
In this case, if asequence is unattested, it is unlikely that the proba-bility is better estimated by ignoring the beginningof it.
Rather, it is likely to indicate an error.
Sinceerror detection and correction is precisely what weare attempting, it may be that backoff smoothingis detrimental to the POS models.
This hypothesisshould be tested empirically.Second, there are several parameters that couldbe tuned for better performance, including for ex-ample, ?, the probability that the channel insertsan error, the beam width n, and the thresholds forthe number of alternatives considered in a closedclass.The stemmer we used was not a very sophis-ticated proxy for morphological analysis, and itmade errors in both directions that affected our re-sults.Finally, there are more classes of error thatcould be easily included in the sets we have de-fined.
Because they interact, our system may per-form better when the allowable transformationsare more comprehensive and can complement oneanother.AcknowledgmentsThis work was supported by the assistance ofZooko Wilcox-O?Hearn, who contributed code,analysis, and review, and by Graeme Hirst whoprovided encouragement and advice.ReferencesBird, Steven, Edward Loper and Ewan Klein 2009.Natural Language Processing with Python.
O?ReillyMedia Inc.Dahlmeier, Daniel, and Hwee Tou Ng.
A beam-searchdecoder for grammatical error correction.
Proceed-ings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning.
Associationfor Computational Linguistics, 2012.Daniel Dahlmeier, Hwee Tou Ng 2012.
Better Eval-uation for Grammatical Error Correction.
Proceed-ings of the 2012 Conference of the North AmericanChapter of the Association for Computational Lin-guistics (NAACL 2012).
(pp.
568 572).
Montreal,Canada.Daniel Dahlmeier, Hwee Tou Ng, Siew Mei Wu 2013.Building a Large Annotated Corpus of Learner En-glish: The NUS Corpus of Learner English.
To ap-pear in Proceedings of the 8th Workshop on Innova-tive Use of NLP for Building Educational Applica-tions (BEA 2013).
Atlanta, Georgia, USA.De Felice, Rachele, and Stephen G. Pulman.
2007Automatically acquiring models of preposition use.Proceedings of the Fourth ACL-SIGSEM Workshopon Prepositions.
Association for Computational Lin-guistics, 2007.Gamon, Michael, et alUsing contextual speller tech-niques and language modeling for ESL error correc-tion.
Urbana 51 (2009): 61801.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, ChristianHadiwinoto, Joel Tetreault 2013.
The CoNLL-2013Shared Task on Grammatical Error Correction.
Toappear in Proceedings of the Seventeenth Confer-ence on Computational Natural Language Learning.Albert Park and Roger Levy 2011.
Automated wholesentence grammar correction using a noisy channelmodel.
Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies.
Volume 1, pp 934-944.Shaoul, C. and Westbury C. 2010 TheWestbury Lab Wikipedia Corpus Edmon-ton, AB: University of Alberta (downloadedfrom http://www.psych.ualberta.ca/ westbury-lab/downloads/westburylab.wikicorp.download.html)A. Stolcke 2002 SRILM ?
An Extensible LanguageModeling Toolkit.
Proc.
Intl.
Conf.
on Spoken Lan-guage Processing, vol.
2, pp.
901-904, Denver.113Tetreault, Joel R., and Martin Chodorow.
The ups anddowns of preposition error detection in ESL writ-ing.
Proceedings of the 22nd International Confer-ence on Computational Linguistics-Volume 1.
Asso-ciation for Computational Linguistics, 2008.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
Feature-Rich Part-of-SpeechTagging with a Cyclic Dependency Network.
Pro-ceedings of HLT-NAACL 2003, pp.
252-259.West, Randy, Y. Albert Park, and Roger Levy.
Bilin-gual random walk models for automated grammarcorrection of esl author-produced text.
Proceedingsof the 6th Workshop on Innovative Use of NLP forBuilding Educational Applications.
Association forComputational Linguistics, 2011.L.
Amber Wilcox-O?Hearn and Zooko Wilcox-O?Hearn 2013. gc https://github.com/lamber/gc/tree/d4bc96f03263b8ed00b9629f22dfe0950b37129b114
