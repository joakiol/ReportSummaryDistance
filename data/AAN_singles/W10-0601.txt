Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 1?9,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsLearning semantic features for fMRI data from definitional textFrancisco Pereira, Matthew Botvinick and Greg DetrePsychology Department and Princeton Neuroscience InstitutePrinceton UniversityPrinceton, NJ 08540{fpereira,matthewb,gdetre}@princeton.eduAbstract(Mitchell et al, 2008) showed that it was pos-sible to use a text corpus to learn the value ofhypothesized semantic features characterizingthe meaning of a concrete noun.
The authorsalso demonstrated that those features couldbe used to decompose the spatial pattern offMRI-measured brain activation in response toa stimulus containing that noun and a pictureof it.
In this paper we introduce a method forlearning such semantic features automaticallyfrom a text corpus, without needing to hypoth-esize them or provide any proxies for theirpresence on the text.
We show that those fea-tures are effective in a more demanding classi-fication task than that in (Mitchell et al, 2008)and describe their qualitative relationship tothe features proposed in that paper.1 IntroductionIn the last few years there has been a gradual in-crease in the number of papers that resort to machinelearning classifiers to decode information from thepattern of activation of activation of voxels acrossthe brain (see (Norman et al, 2006) and (Haynes andRees, 2006) for pointers to much of this work).
Re-cently, however, interest has shifted to discoveringhow the information present is encoded, rather thanjust whether it is present, and also testing theoriesabout that encoding.
One especially compelling ex-ample of the latter is (Kay et al, 2008), where theauthors postulate a mathematical model for how vi-sual information gets transformed into the fMRI sig-nal one can record from visual cortex and, after fit-ting the model, validate it by using it to predict fMRI	 		 		 		      Figure 1: top: A complex pattern of activation is ex-pressed as a combination of three basic patterns.
bottom:The pattern can be written as a row vector, and the com-bination as a linear combination of three row vectors.activation for novel stimuli.
A second example is,of course, (Mitchell et al, 2008), which aims at de-composing the pattern of activation in response to apicture+noun stimulus into a combination of basicpatterns corresponding to the key semantic featuresof the stimulus.
A schematic view of this is givenin Figure 1, where the complex pattern on the left issplit into three simpler ones.
This is done by deter-mining the value of several hypothesized semanticfeatures and using them as the combination weightsfor basic patterns, which can then be extracted fromfMRI data.Ideally, semantic features should reflect what isin a subject?s mind when she thinks about a con-crete concept, e.g.
whether it is animate or inani-mate, or an object versus something natural.
It alsoseems reasonable to expect that the main seman-tic features would likely be shared by most peoplethinking about the same concept; talking to some-one about a chair or table requires a common un-derstanding of the characteristics of that concept.
(Mitchell et al, 2008) proposed a method for captur-ing such common understanding, by considering 251verbs 1 reflecting, in their words, ?basic sensory andmotor activities, actions performed on objects, andactions involving changes to spatial relationships?.For each of the 60 nouns corresponding to the stim-ului shown, they counted the co-occurrence of thenoun with each of the 25 verbs in a large text corpus,converting those 25 counts into normalized featurevalues (the 25-vector has length 1).
The hypothe-sis subjacent to this procedure is that the 25 verbsare a good proxy for the main characteristics of aconcept, and that their frequent co-occurrence withthe corresponding noun in text means that many dif-ferent sources (and people) have that association inmind when using the noun; in a nutshell, the associa-tion reflects common understanding of the meaningof the noun.
The results in (Mitchell et al, 2008)are an extremely compelling demonstration that textcorpora contain information useful for parsing brainactivation into component patterns that reflect se-mantic features.We would like to go beyond the analysis in(Mitchell et al, 2008) by considering that stipulat-ing the semantic features to consider ?
via the verbproxy ?
may limit the information that can be ex-tracted.
The verbs were selected to capture a rangeof characteristics described above, but this does notguarantee that those will be all the ones that are rele-vant, even for concrete concepts.
But how to identifycharacteristics beyond those that one could hypoth-esize in advance?This paper describes an approach to identifyingsemantic features from a text corpus in an unsuper-vised manner, without the need to specify verbs orany other proxy for those features.
The first aspectof the approach is the use of a text corpus that goesbeyond merely containing occurrences of the words.We use a subset of Wikipedia 2, which we chose be-cause articles are definitional in style and also editedby many people, ensuring that they will contain theessential shared knowledge pertaining to the subjectof the article.
The articles in the subset were cho-sen because they pertained to concrete or imageableconcepts, and the methodology for deciding on thisis described in Section 2.2.2.
One property in par-1see, hear, listen, taste, smell, eat, touch, rub, lift, manipu-late, run, push, fill, move, ride, say, fear, open, approach, near,enter, drive, wear, break and clean2http://en.wikipedia.orgticular of text defining a concept will be especiallyhelpful here: in order to make its meaning precise,it has to touch on most related concepts.
This meansthat we will still be resorting to co-ocurrence withour target nouns in order to identify semantic fea-tures, but not of a fixed set of verbs; rather, we areconsidering all possible related words.The tool we will use to do so is latent Dirichletallocation (LDA, (Blei et al, 2003)).
This tech-nique produces a generative probabilistic model oftext corpora where each document (article) is viewedas a bag-of-words (i.e.
only which words appear,and how often, matters) with each word being drawnfrom a finite mixture of an underlying set of topics,each of which is in turn a probability distributionover vocabulary words.
We will use topics as oursemantic features, with the proportions of each topicin the article for a given noun being the values of thefeatures for that noun.
(Murphy et al, 2009) does something similar inflavour to this, by decomposing the patterns of co-occurrences in a text corpus between the 20000 mostfrequent nouns and 5000 most frequent verbs usingSVD.
This is used to identify 25 singular vectorswhich yield feature values across nouns.2 Methods and Data2.1 DataWe use the dataset from (Mitchell et al, 2008),which contains data from 9 subjects.
For each sub-ject there is a dataset of 360 examples - averagefMRI volume around the peak of an experiment trial- comprising 6 replications (epochs) of each of 60nouns as stimuli.
The 60 nouns also belong to oneof 12 semantic categories, hence there are two la-bels for classification tasks.
We refer the reader tothe original paper for more details about the specificcategories and nouns chosen.All of our classification experiments are done over360 examples, rather than 60 average noun images,as we want to leverage having multiple instances ofthe same noun and use cross-validation.
We alsoreplicated the main experiment in (Mitchell et al,2008), and for that we used the 60 average noun im-ages, with their mean image subtracted from each ofthem.22.2 Semantic FeaturesThe experiments described on the paper rely on us-ing two different kinds of semantic features (low-dimensional representations of data) to decomposeeach example in constituent basis images; these twokinds are described blow.2.2.1 Science Semantic Features (SSF)These are the semantic features used in (Mitchellet al, 2008) to represent a given stimulus.
Theywere obtained by considering co-occurrence countsof the noun naming each stimulus with each of 25verbs in a text corpus, yielding a vector of 25 countswhich was normalized to have unit length.
The low-dimensional representation of the brain image for agiven noun is thus a 25-dimensional vector.
The leftof Figure 2 shows the value of these features for the60 nouns considered.2.2.2 Wikipedia Semantic Features (WSF)To obtain the Wikipedia semantic features weconsidered concepts rather than nouns, though wewill use the latter terminology in the rest of the pa-per for consistency with (Mitchell et al, 2008).
Westarted with the classical lists of words in (Paivio etal., 1968) and (Battig and Montague, 1969), as wellas modern revisions/extensions (Clark and Paivio,2004) and (Van Overschelde, 2004), and looked forwords corresponding to concepts that were deemedconcrete or imageable (be it because of their scorein one of the norms or through editorial decision),identified the corresponding Wikipedia article ti-tles (e.g.
?airplane?
is ?Fixed-wing aircraft?)
andalso compiled related articles which were linkedto from these (e.g.
?Aircraft cabin?).
If therewere words in the original lists with multiple mean-ings we included the articles for at least severalof those meanings.
Given the time available, westopped the process with a list of 3500 concepts andtheir corresponding articles (a corpus we call the?Weekipedia?).
We used Wikipedia Extractor 3 toremove any HTML or wiki formatting and annota-tions and processed the resulting text through themorphological analysis tool Morpha (Minnen et al,3http://medialab.di.unipi.it/wiki/Wikipedia_extractor2001) 4 to lemmatize all the words to their basicstems (e.g.
?taste?,?tasted?,?taster?
and ?tastes?
allbecome the same word).The resulting text corpus was processed withtopic modelling software to build several LDAmod-els.
The articles were converted to the required for-mat, keeping only words that appeared in at least twoarticles, and words were also excluded resorting toa custom stopword list.
We run the software vary-ing the number of topics allowed from 10 to 60, inincrements of 5, and allowing the software to esti-mate the ?
parameter.
The ?
parameter influencesthe number of topics used for each example.
For agiven number of topics K, this yielded distributionsover the vocabulary for each topic and one vector oftopic probabilities per article/concept; this vector isthe low-dimensional representation of the concept.Note also that, since the probabilities add up to 1,the presence of one semantic feature trades off withthe presence of the others.The middle and right of Figure 2 shows the valueof these features for the 60 nouns considered in 25and 50 topic models, respectively.2.2.3 Relating semantic features to brainimagesnotation Each example corresponds to the averagefMRI volume around the peak of a trial, account-ing for haemodynamic delay.
This 3D volume canbe unfolded into a vector x with as many entries asvoxels.
A dataset is a n ?
m matrix X where rowi is the example vector xi.
Similarly to (Mitchellet al, 2008), each example x will be expressed asa linear combination of basis images b1, .
.
.
,bKof the same dimensionality, with the weights givenby the semantic feature vector z = [z1, .
.
.
, zK ](see Figure 1 for an illustration of this).
The low-dimensional representation of X is a n ?
K matrixZ where row i is a semantic feature vector zi and thecorresponding basis images are a K ?m matrix B,where row k corresponds to basis image bk.learning and prediction Learning the basis im-ages givenX and Z (top part of Figure 4) can be de-composed into a set of independent regression prob-4http://www.informatics.susx.ac.uk/research/groups/nlp/carroll/morph.html3Figure 2: The value of semantic features for the 60 nouns considered, using SSF with 25 verbs (left) and WSF with25 and 50 topics (middle and right).
The 60 nouns belong to one of 12 categories, and those are arranged in sequence.Although a few of the SSF features might correspond to WSF features, the majority of them do not.lems, one per voxel j, i.e.
the values of voxel jacross all examples, X(:, j), are predicted from Zusing regression coefficients B(:, j), which are thevalues of voxel j across basis images.Predicting the semantic feature vector z for an ex-ample x (bottom part of Figure 4) is a regressionproblem where x?
is predicted from B?
using regres-sion coefficients z?.
For WSF, the prediction of thesemantic feature vector is done under the additionalconstraint that the values need to add up to 1.
Anysituation where linear regression was unfeasible be-cause the square matrix in the normal equations wasnot invertible was addressed by replacing the designmatrix by its singular value decomposition, leavingonly non-zero singular values.3 Experiments and Discussion3.1 Classification/Reconstruction on semanticfeature space3.1.1 Experiment detailsSeveral classification experiments are describedin (Mitchell et al, 2008).
The main one aims atgauging the accuracy of matching unseen stimuli totheir unseen fMRI images and is schematized in Fig-ure 3.
To do this, the authors consider the 60 averageexamples of each stimulus and, in turn, leave outeach of 1770 possible pairs of examples.
For eachleft out pair, they learn a set of basis images usingthe remaining 58 examples and their respective SSFrepresentations.
They then use the SSF representa-					Figure 3: The classification task in (Mitchell et al, 2008)is such that semantic feature representations of the 2test nouns are used, in conjunction with the image ba-sis learned on the training set, to predict their respectivetest examples and use that prediction in a 2-way classifi-cation.tion of the two left-out examples and the basis togenerate a predicted example for each one of them.These can then be used in a two-way matching taskwith the actual examples that were left out, wherethe outcome is correct or incorrect.
Note that this isnot done over the entire brain but over a selection of500 stable voxels, as determined by computing theirreproducibility over the 58 examples in each leave-one-out fold.
This criterion identifies voxels whoseactivation levels across the 58 nouns bear the samerelationship to each other over epochs (mathemat-ically, the vector of activation levels across the 60sorted nouns is highly correlated between epochs).We reproduced this experiment for the sake of com-parison and describe the results in Section 3.4.Whereas (Mitchell et al, 2008) aimed at predict-ing the activation of a set of voxels, and judging how4						Figure 4: Our classification task requires learning an im-age basis from a set of training examples and their re-spective semantic feature representations.
This is used topredict semantic feature values for test set examples andfrom those one can classify against the known semanticfeature values for all 60 nouns.good that prediction is by its 2-way accuracy, thispaper focuses on a different sort of experiment: pre-diction of semantic feature values for a test exam-ple, as schematized in FIgure 4.
In this experiment,the semantic features get used to learn basis imagesfrom training examples, with the goal of reconstruct-ing those training examples as well as possible.
Thislearning does not contemplate the labels ?
categoryor noun ?
of the training examples.
The basis imagesare used, in turn, to predict semantic feature valuesfor test examples and determining, in essence, whichsemantic features are active during a test example.The criterion for judging whether this is a good pre-diction will be how well can we classify the category(1-of-12) and noun (1-of-60) noun of a test example.Good classification performance implies that the se-mantic features capture activation that is relevant tothe task in the corresponding basis images and that,in combination, the features contain enough infor-mation to distinguish the various nouns.We will use either a leave-one-epoch-out (6 fold)or a leave-one-noun-out (60 fold) cross-validationand we perform the following steps in each fold:1. from each training set Xtrain and correspond-ing semantic features Ztrain, select the top1000 most reproducible voxels and learn an im-age basis B using those2.
use the test set Xtest and basis B to predict asemantic feature representation Zpred for thoseexamples3.
use nearest-neighbour classification to predictthe labels of examples in Xtest, by comparingZpred for each example with known semanticfeatures Z4.
use the semantic features Zpred together withbasis B to reconstruct test examples asXpred = ZbredB and compute squared errorbetweenXpred andXtest (over selected voxels)This allows us to do both kinds of cross-validation, as there is always one semantic featurevector for each different noun in Z regardless.
Thisprocedure is unbiased, and we tested this empiricallyusing a permutation test (examples permuted withinepoch) to verify the accuracy results for either taskwere at chance level.3.1.2 Experiment resultsFigure 5 shows the results using leave-one-epoch-out cross-validation.
For each subject (row), thereis one plot of reconstruction error (column 1) andone for error in category classification (column 2)and noun classification (column 3).
Each plot con-trasts the error obtained using SSF with that ob-tained using WSF with 10-60 topics, in incrementsof 5; WSF is as good or better than SSF in both cat-egory and noun classification.
Given the the resultsare over 360 test examples we are not displaying er-ror bars; each number of topics for which WSF isbetter as deemed by a paired t-test (0.01 significancelevel, uncorrected) is highlighted by a square on theplot.
The same is true for the category task whenusing leave-one-noun-out cross-validation, but nei-ther WSF nor SSF appear to do well in the nountask except for subject P1, where WSF again dom-inates.
Results overall are somewhat lower than forthe leave-one-epoch-out cross-validation.
Given thatthe comparison results are qualitatively similar andspace is limited we did not include the correspond-ing figure.
In both cross-validations the reconstruc-tion error of WSF starts higher than that of SSFand decreases monotonically until they are roughlymatched.
Our conjecture is that WSF semantic fea-tures are sparser and thus there are fewer basis im-ages being added to predict any given test example.As the number of topics increases, this ceases to bethe case.One salient aspect of Figure 5 is that accuracy ismuch higher than chance for subjects P1-P4 than forP5-P9, and this corresponds to the subjects where510 15 20 25 30 35 40 45 50 55 600500reconstruction errorP110 15 20 25 30 35 40 45 50 55 600.60.81category error10 15 20 25 30 35 40 45 50 55 600.80.91noun error10 15 20 25 30 35 40 45 50 55 600500P210 15 20 25 30 35 40 45 50 55 600.60.8110 15 20 25 30 35 40 45 50 55 600.80.9110 15 20 25 30 35 40 45 50 55 600500P310 15 20 25 30 35 40 45 50 55 600.60.8110 15 20 25 30 35 40 45 50 55 600.80.9110 15 20 25 30 35 40 45 50 55 600500P410 15 20 25 30 35 40 45 50 55 600.60.8110 15 20 25 30 35 40 45 50 55 600.80.9110 15 20 25 30 35 40 45 50 55 600500P510 15 20 25 30 35 40 45 50 55 600.60.8110 15 20 25 30 35 40 45 50 55 600.80.9110 15 20 25 30 35 40 45 50 55 600500P610 15 20 25 30 35 40 45 50 55 600.60.8110 15 20 25 30 35 40 45 50 55 600.80.9110 15 20 25 30 35 40 45 50 55 600500P710 15 20 25 30 35 40 45 50 55 600.60.8110 15 20 25 30 35 40 45 50 55 600.80.9110 15 20 25 30 35 40 45 50 55 600500P810 15 20 25 30 35 40 45 50 55 600.60.8110 15 20 25 30 35 40 45 50 55 600.80.9110 15 20 25 30 35 40 45 50 55 600500# topicsP910 15 20 25 30 35 40 45 50 55 600.60.81# topics10 15 20 25 30 35 40 45 50 55 600.80.91# topicsFigure 5: For each of the 9 subjects (rows) a comparison between SSF and WSF (using 10-60 topics) in reconstructionerror (column 1) and classification error in the category (column 2) and noun (column 3) tasks.
In each plot WSF isred (full line), SSF is blue (constant dashed line) and chance level is black (constant dotted line).
The reconstructionerror is measured on left out examples, over the 1000 voxels selected on the training set.
These results were obtainedusing leave-one-epoch-out cross-validation (one epoch containing one instance of all nouns is left out in each of 6folds).
Error bars are not shown, given their small size (there are 360 examples), but each number of topics for whichWSF error is significantly lower than SSF error is highlighted with a square.P1 P2 P3 P4 P5 P6 P7 P8 P9same 0.57 0.39 0.36 0.32 0.26 0.16 0.26 0.24 0.18category 0.50 0.32 0.30 0.28 0.24 0.14 0.23 0.21 0.16other 0.45 0.30 0.27 0.22 0.22 0.13 0.21 0.20 0.14same minus other 0.12 0.09 0.09 0.10 0.04 0.03 0.05 0.04 0.04same minus category 0.07 0.07 0.06 0.04 0.02 0.02 0.03 0.03 0.02Table 1: For each subject (column), the average correlation between one test example of a noun and all training setexamples of the same noun (same), those which are not the same but belong to the same category (category) and thosewhich are not in the same category (other).
The correlation is computed over the 1000 voxels selected in the trainingset which are used to learn the image basis.
Note the difference between same and other for subjects P1-P4, in contrastwith that for subjects P5-P9.
This was computed using leave-one-epoch-out cross-validation, and thus should be usedin conjunction with Figure 5.6WSF is significantly better than SSF.
In an effortto find out why this was the case, we computed ameasure of consistency of the data from each of thesubjects; intuitively, this is the degree to which thebrain activation pattern was similar between trialswith the same noun stimulus (and dissimilar for tri-als where the stimulus was different).
This was com-puted in leave-one-epoch-out cross-validation, andconsisted of examining the correlation ?
computedacross selected voxels ?
of a test example with train-ing examples of the same noun (same), the samecategory but a different noun (same category) anddifferent category and noun (other); the measureswere averaged across examples.
In leave-one-group-out cross-validation subjects P1-P4 have higher dif-ferences between correlation within examples of anoun and examples in the same category or othercategories than subjects P5-P9, which suggests thatthe former are more consistent in how they elicit pat-terns in response to the same stimulus.3.2 Classification on voxel spaceIn order to have an idea of how much of the infor-mation present either SSF or WSF can extract andconvey via their respective low-dimensional repre-sentations, we also trained a simple Gaussian NaiveBayes (GNB) classifier on voxels selected using thesame reproducibility criterion described earlier.
Weused leave-one-epoch-out cross-validation and bothcategory and noun tasks, respectively top and bot-tom of Table 2.
Contrasting this with Figure 5,it?s clear that the accuracies in the category taskare comparable, whereas those in the noun task aresomewhat lower; this suggests that either informa-tion about individual nouns is lost when convertingfrom voxels to semantic features, or that nearest-neighbour is not the best classifier to use.3.3 Similarity between SSF and WSFrepresentationsIn order to gauge the quality of the semantic featurerepresentations we can consider both how much theydiffer between different nouns (and different cate-gories) and also how consistent they are for the 6 ex-amples of the same noun.
This is shown for subjectP1 in Figure 6, where the semantic feature vectorslearned for 360 examples are correlated, for WSF 50(left) and SSF 25 (right).
Examples are sorted so thatWSF 50SSF25correlation between 25 SSF and 50 WSF across 360 nouns5 10 15 20 25 30 35 40 45 50510152025?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 1Figure 7: Correlation between each pair of SSF andWSFvectors of predicted feature values across 360 examples.the 6 examples of the same noun are together, andadjacent to the other 24 belonging to the same cat-egory (and the category changes are labelled.
Notethat these are the values obtained when each exam-ple was in the test set, rather than the values derivedfrom text for each noun; this is why the semanticfeature vectors for the 6 examples of the same nounare different.
WSF 50 is such that nouns belongingto the same category share many feature values, andhence show up as large blocks along the diagonal ofthe correlation matrix.
Less of the noun specific in-formation is being captured, but it is sometimes vis-ible as the smaller blocks along the diagonal, insidethe large blocks.We can also consider the question of whether SSFand WSF representations are similar, i.e.
whether agiven SSF feature has values across examples sim-ilar to a given WSF feature.
This can be doneby considering the correlation between each pair ofpredicted SSF/WSF vectors across 360 examples,which is shown in Figure 7.
This suggests very fewof the semantic features are similar when predictedfor examples in the test set, and as was already evi-dence in Figure 2.3.4 Leave-2-out 2-way classificationWe have also attempted to replicate the results inthe main experiment in (Mitchell et al, 2008),schematized in Figure 3 and described earlier in Sec-tion 3.1.1.
The results of this are shown in Ta-ble 3, which compares the mean accuracy across7category accuracy#voxels 100 250 500 1000 1500 2000 5000 all voxelsP1 0.43 0.53 0.54 0.56 0.53 0.52 0.42 0.08P2 0.30 0.34 0.32 0.30 0.28 0.26 0.22 0.08P3 0.25 0.27 0.29 0.27 0.26 0.26 0.21 0.08P4 0.42 0.40 0.41 0.38 0.38 0.39 0.31 0.08P5 0.20 0.21 0.21 0.17 0.16 0.14 0.11 0.08P6 0.27 0.23 0.19 0.16 0.14 0.13 0.10 0.08P7 0.21 0.19 0.19 0.19 0.18 0.16 0.13 0.08P8 0.14 0.13 0.12 0.14 0.13 0.13 0.12 0.08P9 0.18 0.21 0.21 0.21 0.22 0.21 0.19 0.08noun accuracy#voxels 100 250 500 1000 1500 2000 5000 all voxelsP1 0.34 0.41 0.41 0.41 0.35 0.33 0.23 0.02P2 0.26 0.32 0.29 0.22 0.18 0.17 0.08 0.02P3 0.17 0.20 0.21 0.17 0.14 0.12 0.07 0.02P4 0.21 0.23 0.22 0.20 0.18 0.16 0.14 0.02P5 0.11 0.09 0.08 0.06 0.05 0.05 0.03 0.02P6 0.13 0.08 0.06 0.04 0.04 0.04 0.02 0.02P7 0.08 0.07 0.08 0.07 0.07 0.07 0.05 0.02P8 0.07 0.08 0.06 0.05 0.05 0.04 0.03 0.02P9 0.06 0.08 0.06 0.06 0.05 0.05 0.04 0.02Table 2: top: Accuracy of a Gaussian Naive Bayes classifier trained on various numbers of voxels selected by thereproducibility criterion, on the category prediction task, using leave-one-epoch-out cross-validation.
bottom: Same,for the noun prediction task.animal(1)bodypart(31)building(61)buildingpart (91)clothing(121)furniture(151)insect(181)kitchen(211)manmade(241)tool(271)vegetable(301)vehicle(331)correlation between predicted WSF 50 dimensional vectors for 360 nounsanimal (1)bodypart (31)building (61)buildingpart (91)clothing (121)furniture (151)insect (181)kitchen (211)manmade (241)tool (271)vegetable (301)vehicle (331)00.20.40.60.81animal(1)bodypart(31)building(61)buildingpart (91)clothing(121)furniture(151)insect(181)kitchen(211)manmade(241)tool(271)vegetable(301)vehicle(331)correlation between predicted SSF 25 dimensional vectors for 360 nounsanimal (1)bodypart (31)building (61)buildingpart (91)clothing (121)furniture (151)insect (181)kitchen (211)manmade (241)tool (271)vegetable (301)vehicle (331)00.20.40.60.81Figure 6: left: correlation between the WSF 50 predicted feature vectors for the 360 examples right: same for theSSF 25 predicted feature vectors8SSF Org 20 25 30 35 40 45 50P1 0.84 0.83 0.88 0.91 0.87 0.89 0.85 0.85 0.86P2 0.80 0.76 0.75 0.77 0.74 0.76 0.72 0.72 0.73P3 0.78 0.78 0.76 0.78 0.73 0.76 0.72 0.70 0.78P4 0.82 0.72 0.88 0.88 0.85 0.86 0.86 0.85 0.87P5 0.85 0.78 0.79 0.84 0.78 0.71 0.78 0.73 0.78P6 0.77 0.85 0.82 0.84 0.78 0.79 0.76 0.81 0.75P7 0.78 0.73 0.83 0.84 0.80 0.81 0.79 0.75 0.74P8 0.77 0.68 0.66 0.68 0.64 0.62 0.67 0.64 0.69P9 0.75 0.82 0.77 0.81 0.77 0.79 0.81 0.78 0.78Table 3: Results of a replication of the leave-2-noun-out2-way classification experiment in (Mitchell et al, 2008).For subjects P1-P9, SSF represents the mean accuracyobtained using SSF (across 1770 leave-2-out pairs), Orgthe mean accuracy reported in (Mitchell et al, 2008) andthe remaining columns the mean accuracy obtained usingWSF with 20-50 topics.1770 leave-2-out pairs using SSF, the mean accuracyreported in (Mitchell et al, 2008) and the mean ac-curacy using WSF with 20-50 topics.
We were notable to exactly reproduce the numbers in (Mitchellet al, 2008), despite the same data preprocessing(making each example mean 0 and standard devia-tion 1, prior to averaging all the repetitions of eachnoun, and then subtracting the mean of all averageexamples from each one), the same voxel selectionprocedure (using 500 voxels) and the same ridge re-gression function (although (Mitchell et al, 2008)does not mention the value of the ridge parameter ?,which we assumed to be 1).
We will endeavour toidentify the source of the discrepancies, but it wasnot possible to do so in time for this paper.4 ConclusionsWe have shown that it is feasible to learn seman-tic features from a text corpus, without the need topostulate what they might represent in the brain, ei-ther directly or via proxy indicators like the verbs in(Mitchell et al, 2008).
Furthermore, we have shownthat those semantic features are superior to the fea-tures proposed in (Mitchell et al, 2008) in two de-manding classification tasks that require using thefeatures to decompose brain activation into basis im-ages related to them.
Further analysis of those andother results obtained classifying directly from vox-els suggest that the semantic features capture a largeamount of category-level information, and at least afraction of the noun-level information present in thepattern of brain activation.
(Mitchell et al, 2008).AcknowledgmentsWe would like to thank David Blei for discussions about topic mod-elling in general and of the Wikipedia corpus in particular and KenNorman for valuable feedback at various stages of the work.ReferencesWilliam F Battig and William E Montague.
1969.
Cate-goryNorms for Verbal Items in 56 Categories.
Journalof Experimental Psychology, 80(3).D M Blei, A Y Ng, and M I Jordan.
2003.
Latent Dirich-let alocation.
Journal of Machine Learning Research,3:993?1022.James M Clark and Allan Paivio.
2004.
Extensions ofthe Paivio, Yuille, and Madigan (1968) norms.
Be-havior research methods, instruments, & computers :a journal of the Psychonomic Society, Inc, 36(3):371?83, August.John-Dylan Haynes and Geraint Rees.
2006.
Decodingmental states from brain activity in humans.
Naturereviews.
Neuroscience, 7(7):523?34.Kendrick N Kay, Thomas Naselaris, Ryan J Prenger, andJack L Gallant.
2008.
Identifying natural images fromhuman brain activity.
Nature, 452(7185):352?5.G.
Minnen, J. Carroll, and D. Pearce.
2001.
Appliedmorphological processing of English.
Natural Lan-guage Engineering, 7(03):207223.Tom M Mitchell, Svetlana V Shinkareva, Andrew Carl-son, Kai-Min Chang, Vicente L Malave, Robert a Ma-son, and Marcel Adam Just.
2008.
Predicting humanbrain activity associated with the meanings of nouns.Science (New York, N.Y.), 320(5880):1191?5.B.
Murphy, M. Baroni, and M. Poesio.
2009.
EEG Re-sponds to Conceptual Stimuli and Corpus Semantics.Proceedings of ACL/EMNLP.Kenneth A Norman, Sean M Polyn, Greg J Detre, andJames V Haxby.
2006.
Beyond mind-reading: multi-voxel pattern analysis of fMRI data.
Trends in cogni-tive sciences, 10(9):424?30.Allan Paivio, John C Yuille, and Stephen A Madigan.1968.
Concreteness, Imagery, and MeaningfulnessValues for 925 Nouns.
Journal of Experimental Psy-chology, 76(1).J Van Overschelde.
2004.
Category norms: An up-dated and expanded version of the Battig and Mon-tague (1969) norms.
Journal of Memory and Lan-guage, 50(3):289?335.9
