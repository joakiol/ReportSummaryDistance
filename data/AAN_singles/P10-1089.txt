Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 865?874,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsCreating Robust Supervised Classifiers via Web-Scale N-gram DataShane BergsmaUniversity of Albertasbergsma@ualberta.caEmily PitlerUniversity of Pennsylvaniaepitler@seas.upenn.eduDekang LinGoogle, Inc.lindek@google.comAbstractIn this paper, we systematically assess thevalue of using web-scale N-gram data instate-of-the-art supervised NLP classifiers.We compare classifiers that include or ex-clude features for the counts of variousN-grams, where the counts are obtainedfrom a web-scale auxiliary corpus.
Weshow that including N-gram count featurescan advance the state-of-the-art accuracyon standard data sets for adjective order-ing, spelling correction, noun compoundbracketing, and verb part-of-speech dis-ambiguation.
More importantly, when op-erating on new domains, or when labeledtraining data is not plentiful, we show thatusing web-scale N-gram features is essen-tial for achieving robust performance.1 IntroductionMany NLP systems use web-scale N-gram counts(Keller and Lapata, 2003; Nakov and Hearst,2005; Brants et al, 2007).
Lapata and Keller(2005) demonstrate good performance on eighttasks using unsupervised web-based models.
Theyshow web counts are superior to counts from alarge corpus.
Bergsma et al (2009) propose un-supervised and supervised systems that use countsfrom Google?s N-gram corpus (Brants and Franz,2006).
Web-based models perform particularlywell on generation tasks, where systems choosebetween competing sequences of output text (suchas different spellings), as opposed to analysistasks, where systems choose between abstract la-bels (such as part-of-speech tags or parse trees).In this work, we address two natural and relatedquestions which these previous studies leave open:1.
Is there a benefit in combining web-scalecounts with the features used in state-of-the-art supervised approaches?2.
How well do web-based models perform onnew domains or when labeled data is scarce?We address these questions on two generationand two analysis tasks, using both existing N-gramdata and a novel web-scale N-gram corpus thatincludes part-of-speech information (Section 2).While previous work has combined web-scale fea-tures with other features in specific classificationproblems (Modjeska et al, 2003; Yang et al,2005; Vadas and Curran, 2007b), we provide amulti-task, multi-domain comparison.Some may question why supervised approachesare needed at all for generation problems.
Whynot solely rely on direct evidence from a giant cor-pus?
For example, for the task of prenominal ad-jective ordering (Section 3), a system that needsto describe a ball that is both big and red can sim-ply check that big red is more common on the webthan red big, and order the adjectives accordingly.It is, however, suboptimal to only use N-gramdata.
For example, ordering adjectives by directweb evidence performs 7% worse than our bestsupervised system (Section 3.2).
No matter howlarge the web becomes, there will always be plau-sible constructions that never occur.
For example,there are currently no pages indexed by Googlewith the preferred adjective ordering for bedrag-gled 56-year-old [professor].
Also, in a particu-lar domain, words may have a non-standard usage.Systems trained on labeled data can learn the do-main usage and leverage other regularities, such assuffixes and transitivity for adjective ordering.With these benefits, systems trained on labeleddata have become the dominant technology in aca-demic NLP.
There is a growing recognition, how-ever, that these systems are highly domain de-pendent.
For example, parsers trained on anno-tated newspaper text perform poorly on other gen-res (Gildea, 2001).
While many approaches haveadapted NLP systems to specific domains (Tsu-ruoka et al, 2005; McClosky et al, 2006; Blitzer865et al, 2007; Daume?
III, 2007; Rimell and Clark,2008), these techniques assume the system knowson which domain it is being used, and that it hasaccess to representative data in that domain.
Theseassumptions are unrealistic in many real-world sit-uations; for example, when automatically process-ing a heterogeneous collection of web pages.
Howwell do supervised and unsupervised NLP systemsperform when used uncustomized, out-of-the-boxon new domains, and how can we best design oursystems for robust open-domain performance?Our results show that using web-scale N-gramdata in supervised systems advances the state-of-the-art performance on standard analysis and gen-eration tasks.
More importantly, when operatingout-of-domain, or when labeled data is not plen-tiful, using web-scale N-gram data not only helpsachieve good performance ?
it is essential.2 Experiments and Data2.1 Experimental DesignWe evaluate the benefit of N-gram data on multi-class classification problems.
For each task, wehave some labeled data indicating the correct out-put for each example.
We evaluate with accuracy:the percentage of examples correctly classified intest data.
We use one in-domain and two out-of-domain test sets for each task.
Statistical signifi-cance is assessed with McNemar?s test, p<0.01.We provide results for unsupervised approachesand the majority-class baseline for each task.For our supervised approaches, we represent theexamples as feature vectors, and learn a classi-fier on the training vectors.
There are two fea-ture classes: features that use N-grams (N-GM)and those that do not (LEX).
N-GM features arereal-valued features giving the log-count of a par-ticular N-gram in the auxiliary web corpus.
LEXfeatures are binary features that indicate the pres-ence or absence of a particular string at a given po-sition in the input.
The name LEX emphasizes thatthey identify specific lexical items.
The instantia-tions of both types of features depend on the taskand are described in the corresponding sections.Each classifier is a linear Support Vector Ma-chine (SVM), trained using LIBLINEAR (Fan et al,2008) on the standard domain.
We use the one-vs-all strategy when there are more than two classes(in Section 4).
We plot learning curves to mea-sure the accuracy of the classifier when the num-ber of labeled training examples varies.
The sizeof the N-gram data and its counts remain constant.We always optimize the SVM?s (L2) regulariza-tion parameter on the in-domain development set.We present results with L2-SVM, but achieve sim-ilar results with L1-SVM and logistic regression.2.2 Tasks and Labeled DataWe study two generation tasks: prenominal ad-jective ordering (Section 3) and context-sensitivespelling correction (Section 4), followed by twoanalysis tasks: noun compound bracketing (Sec-tion 5) and verb part-of-speech disambiguation(Section 6).
In each section, we provide refer-ences to the origin of the labeled data.
For theout-of-domain Gutenberg and Medline data usedin Sections 3 and 4, we generate examples our-selves.1 We chose Gutenberg and Medline in orderto provide challenging, distinct domains from ourtraining corpora.
Our Gutenberg corpus consistsof out-of-copyright books, automatically down-loaded from the Project Gutenberg website.2 TheMedline data consists of a large collection of on-line biomedical abstracts.
We describe how la-beled adjective and spelling examples are createdfrom these corpora in the corresponding sections.2.3 Web-Scale Auxiliary DataThe most widely-used N-gram corpus is theGoogle 5-gram Corpus (Brants and Franz, 2006).For our tasks, we also use Google V2: a newN-gram corpus (also with N-grams of length one-to-five) that we created from the same one-trillion-word snapshot of the web as the Google 5-gramCorpus, but with several enhancements.
These in-clude: 1) Reducing noise by removing duplicatesentences and sentences with a high proportionof non-alphanumeric characters (together filteringabout 80% of the source data), 2) pre-convertingall digits to the 0 character to reduce sparsity fornumeric expressions, and 3) including the part-of-speech (POS) tag distribution for each N-gram.The source data was automatically tagged withTnT (Brants, 2000), using the Penn Treebank tagset.
Lin et al (2010) provide more details on the1http://webdocs.cs.ualberta.ca/?bergsma/Robust/provides our Gutenberg corpus, a link to Medline, and alsothe generated examples for both Gutenberg and Medline.2www.gutenberg.org.
All books just released in 2009 andthus unlikely to occur in the source data for our N-gram cor-pus (from 2006).
Of course, with removal of sentence dupli-cates and also N-gram thresholding, the possible presence ofa test sentence in the massive source data is unlikely to affectresults.
Carlson et al (2008) reach a similar conclusion.866N-gram data and N-gram search tools.The third enhancement is especially relevanthere, as we can use the POS distribution to collectcounts for N-grams of mixed words and tags.
Forexample, we have developed an N-gram search en-gine that can count how often the adjective un-precedented precedes another adjective in our webcorpus (113K times) and how often it follows one(11K times).
Thus, even if we haven?t seen a par-ticular adjective pair directly, we can use the posi-tional preferences of each adjective to order them.Early web-based models used search engines tocollect N-gram counts, and thus could not use cap-italization, punctuation, and annotations such aspart-of-speech (Kilgarriff and Grefenstette, 2003).Using a POS-tagged web corpus goes a long wayto addressing earlier criticisms of web-based NLP.3 Prenominal Adjective OrderingPrenominal adjective ordering strongly affects textreadability.
For example, while the unprecedentedstatistical revolution is fluent, the statistical un-precedented revolution is not.
Many NLP systemsneed to handle adjective ordering robustly.
In ma-chine translation, if a noun has two adjective mod-ifiers, they must be ordered correctly in the tar-get language.
Adjective ordering is also neededin Natural Language Generation systems that pro-duce information from databases; for example, toconvey information (in sentences) about medicalpatients (Shaw and Hatzivassiloglou, 1999).We focus on the task of ordering a pair of adjec-tives independently of the noun they modify andachieve good performance in this setting.
Follow-ing the set-up of Malouf (2000), we experimenton the 263K adjective pairs Malouf extracted fromthe British National Corpus (BNC).
We use 90%of pairs for training, 5% for testing, and 5% fordevelopment.
This forms our in-domain data.3We create out-of-domain examples by tokeniz-ing Medline and Gutenberg (Section 2.2), thenPOS-tagging them with CRFTagger (Phan, 2006).We create examples from all sequences of two ad-jectives followed by a noun.
Like Malouf (2000),we assume that edited text has adjectives orderedfluently.
We extract 13K and 9.1K out-of-domainpairs from Gutenberg and Medline, respectively.43BNC is not a domain per se (rather a balanced corpus),but has a style and vocabulary distinct from our OOD data.4Like Malouf (2000), we convert our pairs to lower-case.Since the N-gram data includes case, we merge counts fromthe upper and lower case combinations.The input to the system is a pair of adjectives,(a1, a2), ordered alphabetically.
The task is toclassify this order as correct (the positive class) orincorrect (the negative class).
Since both classesare equally likely, the majority-class baseline isaround 50% on each of the three test sets.3.1 Supervised Adjective Ordering3.1.1 LEX featuresOur adjective ordering model with LEX features isa novel contribution of this paper.We begin with two features for each pair: an in-dicator feature for a1, which gets a feature value of+1, and an indicator feature for a2, which gets afeature value of ?1.
The parameters of the modelare therefore weights on specific adjectives.
Thehigher the weight on an adjective, the more it ispreferred in the first position of a pair.
If the alpha-betic ordering is correct, the weight on a1 shouldbe higher than the weight on a2, so that the clas-sifier returns a positive score.
If the reverse order-ing is preferred, a2 should receive a higher weight.Training the model in this setting is a matter of as-signing weights to all the observed adjectives suchthat the training pairs are maximally ordered cor-rectly.
The feature weights thus implicitly producea linear ordering of all observed adjectives.
Theexamples can also be regarded as rank constraintsin a discriminative ranker (Joachims, 2002).
Tran-sitivity is achieved naturally in that if we correctlyorder pairs a ?
b and b ?
c in the training set,then a ?
c by virtue of the weights on a and c.While exploiting transitivity has been shownto improve adjective ordering, there are manyconflicting pairs that make a strict linear order-ing of adjectives impossible (Malouf, 2000).
Wetherefore provide an indicator feature for the paira1a2, so the classifier can memorize exceptionsto the linear ordering, breaking strict order tran-sitivity.
Our classifier thus operates along the linesof rankers in the preference-based setting as de-scribed in Ailon and Mohri (2008).Finally, we also have features for all suffixes oflength 1-to-4 letters, as these encode useful infor-mation about adjective class (Malouf, 2000).
Likethe adjective features, the suffix features receive avalue of +1 for adjectives in the first position and?1 for those in the second.3.1.2 N-GM featuresLapata and Keller (2005) propose a web-basedapproach to adjective ordering: take the most-867System IN O1 O2Malouf (2000) 91.5 65.6 71.6web c(a1, a2) vs. c(a2, a1) 87.1 83.7 86.0SVM with N-GM features 90.0 85.8 88.5SVM with LEX features 93.0 70.0 73.9SVM with N-GM + LEX 93.7 83.6 85.4Table 1: Adjective ordering accuracy (%).
SVMand Malouf (2000) trained on BNC, tested onBNC (IN), Gutenberg (O1), and Medline (O2).frequent order of the words on the web, c(a1, a2)vs. c(a2, a1).
We adopt this as our unsupervisedapproach.
We merge the counts for the adjectivesoccurring contiguously and separated by a comma.These are indubitably the most important N-GMfeatures; we include them but also other, tag-basedcounts from Google V2.
Raw counts include caseswhere one of the adjectives is not used as a mod-ifier: ?the special present was?
vs. ?the presentspecial issue.?
We include log-counts for thefollowing, more-targeted patterns:5 c(a1 a2 N.*),c(a2 a1 N.*), c(DT a1 a2 N.*), c(DT a2 a1 N.*).We also include features for the log-counts ofeach adjective preceded or followed by a wordmatching an adjective-tag: c(a1 J.
*), c(J.
* a1),c(a2 J.
*), c(J.
* a2).
These assess the positionalpreferences of each adjective.
Finally, we includethe log-frequency of each adjective.
The more fre-quent adjective occurs first 57% of the time.As in all tasks, the counts are features in a clas-sifier, so the importance of the different patterns isweighted discriminatively during training.3.2 Adjective Ordering ResultsIn-domain, with both feature classes, we set astrong new standard on this data: 93.7% accuracyfor the N-GM+LEX system (Table 1).
We trainedand tested Malouf (2000)?s program on our data;our LEX classifier, which also uses no auxiliarycorpus, makes 18% fewer errors than Malouf?ssystem.
Our web-based N-GM model is also su-perior to the direct evidence web-based approachof Lapata and Keller (2005), scoring 90.0% vs.87.1% accuracy.
These results show the benefitof our new lexicalized and web-based features.Figure 1 gives the in-domain learning curve.With fewer training examples, the systems withN-GM features strongly outperform the LEX-onlysystem.
Note that with tens of thousands of test5In this notation, capital letters (and regular expressions)are matched against tags while a1 and a2 match words.60657075808590951001e51e41e3100Accuracy(%)Number of training examplesN-GM+LEXN-GMLEXFigure 1: In-domain learning curve of adjectiveordering classifiers on BNC.60657075808590951001e51e41e3100Accuracy(%)Number of training examplesN-GM+LEXN-GMLEXFigure 2: Out-of-domain learning curve of adjec-tive ordering classifiers on Gutenberg.examples, all differences are highly significant.Out-of-domain, LEX?s accuracy drops a shock-ing 23% on Gutenberg and 19% on Medline (Ta-ble 1).
Malouf (2000)?s system fares even worse.The overlap between training and test pairs helpsexplain.
While 59% of the BNC test pairs wereseen in the training corpus, only 25% of Gutenbergand 18% of Medline pairs were seen in training.While other ordering models have also achieved?very poor results?
out-of-domain (Mitchell,2009), we expected our expanded set of LEX fea-tures to provide good generalization on new data.Instead, LEX is very unreliable on new domains.N-GM features do not rely on specific pairs intraining data, and thus remain fairly robust cross-domain.
Across the three test sets, 84-89% ofexamples had the correct ordering appear at leastonce on the web.
On new domains, the learnedN-GM system maintains an advantage over the un-supervised c(a1, a2) vs. c(a2, a1), but the differ-ence is reduced.
Note that training with 10-fold868cross validation, the N-GM system can achieve upto 87.5% on Gutenberg (90.0% for N-GM + LEX).The learning curve showing performance onGutenberg (but still training on BNC) is particu-larly instructive (Figure 2, performance on Med-line is very similar).
The LEX system performsmuch worse than the web-based models acrossall training sizes.
For our top in-domain sys-tem, N-GM + LEX, as you add more labeled ex-amples, performance begins decreasing out-of-domain.
The system disregards the robust N-gramcounts as it is more and more confident in the LEXfeatures, and it suffers the consequences.4 Context-Sensitive Spelling CorrectionWe now turn to the generation problem of context-sensitive spelling correction.
For every occurrenceof a word in a pre-defined set of confusable words(like peace and piece), the system must select themost likely word from the set, flagging possibleusage errors when the predicted word disagreeswith the original.
Contextual spell checkers areone of the most widely used NLP technologies,reaching millions of users via compressed N-grammodels in Microsoft Office (Church et al, 2007).Our in-domain examples are from the New YorkTimes (NYT) portion of Gigaword, from Bergsmaet al (2009).
They include the 5 confusion setswhere accuracy was below 90% in Golding andRoth (1999).
There are 100K training, 10K devel-opment, and 10K test examples for each confusionset.
Our results are averages across confusion sets.Out-of-domain examples are again drawn fromGutenberg and Medline.
We extract all instancesof words that are in one of our confusion sets,along with surrounding context.
By assuming theextracted instances represent correct usage, we la-bel 7.8K and 56K out-of-domain test examples forGutenberg and Medline, respectively.We test three unsupervised systems: 1) Lapataand Keller (2005) use one token of context on theleft and one on the right, and output the candidatefrom the confusion set that occurs most frequentlyin this pattern.
2) Bergsma et al (2009) measurethe frequency of the candidates in all the 3-to-5-gram patterns that span the confusable word.
Foreach candidate, they sum the log-counts of all pat-terns filled with the candidate, and output the can-didate with the highest total.
3) The baseline pre-dicts the most frequent member of each confusionset, based on frequencies in the NYT training data.System IN O1 O2Baseline 66.9 44.6 60.6Lapata and Keller (2005) 88.4 78.0 87.4Bergsma et al (2009) 94.8 87.7 94.2SVM with N-GM features 95.7 92.1 93.9SVM with LEX features 95.2 85.8 91.0SVM with N-GM + LEX 96.5 91.9 94.8Table 2: Spelling correction accuracy (%).
SVMtrained on NYT, tested on NYT (IN) and out-of-domain Gutenberg (O1) and Medline (O2).7075808590951001e51e41e3100Accuracy(%)Number of training examplesN-GM+LEXN-GMLEXFigure 3: In-domain learning curve of spellingcorrection classifiers on NYT.4.1 Supervised Spelling CorrectionOur LEX features are typical disambiguation fea-tures that flag specific aspects of the context.
Wehave features for the words at all positions ina 9-word window (called collocation features byGolding and Roth (1999)), plus indicators for aparticular word preceding or following the con-fusable word.
We also include indicators for allN-grams, and their position, in a 9-word window.For N-GM count features, we follow Bergsmaet al (2009).
We include the log-counts of allN-grams that span the confusable word, with eachword in the confusion set filling the N-gram pat-tern.
These features do not use part-of-speech.Following Bergsma et al (2009), we get N-gramcounts using the original Google N-gram Corpus.While neither our LEX nor N-GM features arenovel on their own, they have, perhaps surpris-ingly, not yet been evaluated in a single model.4.2 Spelling Correction ResultsThe N-GM features outperform the LEX features,95.7% vs. 95.2% (Table 2).
Together, theyachieve a very strong 96.5% in-domain accuracy.869This is 2% higher than the best unsupervised ap-proach (Bergsma et al, 2009).
Web-based modelsagain perform well across a range of training datasizes (Figure 3).The error rate of LEX nearly triples on Guten-berg and almost doubles on Medline (Table 2).
Re-moving N-GM features from the N-GM + LEX sys-tem, errors increase around 75% on both Guten-berg and Medline.
The LEX features provide nohelp to the combined system on Gutenberg, whilethey do help significantly on Medline.
Note thelearning curves for N-GM+LEX on Gutenberg andMedline (not shown) do not display the decreasethat we observed in adjective ordering (Figure 2).Both the baseline and LEX perform poorly onGutenberg.
The baseline predicts the majorityclass from NYT, but it?s not always the majorityclass in Gutenberg.
For example, while in NYTsite occurs 87% of the time for the (cite, sight,site) confusion set, sight occurs 90% of the time inGutenberg.
The LEX classifier exploits this bias asit is regularized toward a more economical model,but the bias does not transfer to the new domain.5 Noun Compound BracketingAbout 70% of web queries are noun phrases (Barret al, 2008) and methods that can reliably parsethese phrases are of great interest in NLP.
Forexample, a web query for zebra hair straightenershould be bracketed as (zebra (hair straightener)),a stylish hair straightener with zebra print, ratherthan ((zebra hair) straightener), a useless productsince the fur of zebras is already quite straight.The noun compound (NC) bracketing task isusually cast as a decision whether a 3-word NChas a left or right bracketing.
Most approaches areunsupervised, using a large corpus to compare thestatistical association between word pairs in theNC.
The adjacency model (Marcus, 1980) pro-poses a left bracketing if the association betweenwords one and two is higher than between twoand three.
The dependency model (Lauer, 1995a)compares one-two vs. one-three.
We include de-pendency model results using PMI as the associ-ation measure; results were lower with the adja-cency model.As in-domain data, we use Vadas and Curran(2007a)?s Wall-Street Journal (WSJ) data, an ex-tension of the Treebank (which originally left NPsflat).
We extract all sequences of three consec-utive common nouns, generating 1983 examplesSystem IN O1 O2Baseline 70.5 66.8 84.1Dependency model 74.7 82.8 84.4SVM with N-GM features 89.5 81.6 86.2SVM with LEX features 81.1 70.9 79.0SVM with N-GM + LEX 91.6 81.6 87.4Table 3: NC-bracketing accuracy (%).
SVMtrained on WSJ, tested on WSJ (IN) and out-of-domain Grolier (O1) and Medline (O2).60657075808590951001e310010Accuracy(%)Number of labeled examplesN-GM+LEXN-GMLEXFigure 4: In-domain NC-bracketer learning curvefrom sections 0-22 of the Treebank as training, 72from section 24 for development and 95 from sec-tion 23 as a test set.
As out-of-domain data, weuse 244 NCs from Grolier Encyclopedia (Lauer,1995a) and 429 NCs from Medline (Nakov, 2007).The majority class baseline is left-bracketing.5.1 Supervised Noun BracketingOur LEX features indicate the specific noun ateach position in the compound, plus the three pairsof nouns and the full noun triple.
We also add fea-tures for the capitalization pattern of the sequence.N-GM features give the log-count of all subsetsof the compound.
Counts are from Google V2.Following Nakov and Hearst (2005), we also in-clude counts of noun pairs collapsed into a singletoken; if a pair occurs often on the web as a singleunit, it strongly indicates the pair is a constituent.Vadas and Curran (2007a) use simpler features,e.g.
they do not use collapsed pair counts.
Theyachieve 89.9% in-domain on WSJ and 80.7% onGrolier.
Vadas and Curran (2007b) use compara-ble features to ours, but do not test out-of-domain.5.2 Noun Compound Bracketing ResultsN-GM systems perform much better on this task(Table 3).
N-GM+LEX is statistically significantly870better than LEX on all sets.
In-domain, errorsmore than double without N-GM features.
LEXperforms poorly here because there are far fewertraining examples.
The learning curve (Figure 4)looks much like earlier in-domain curves (Fig-ures 1 and 3), but truncated before LEX becomescompetitive.
The absence of a sufficient amount oflabeled data explains why NC-bracketing is gen-erally regarded as a task where corpus counts arecrucial.All web-based models (including the depen-dency model) exceed 81.5% on Grolier, whichis the level of human agreement (Lauer, 1995b).N-GM + LEX is highest on Medline, and closeto the 88% human agreement (Nakov and Hearst,2005).
Out-of-domain, the LEX approach per-forms very poorly, close to or below the base-line accuracy.
With little training data and cross-domain usage, N-gram features are essential.6 Verb Part-of-Speech DisambiguationOur final task is POS-tagging.
We focus on onefrequent and difficult tagging decision: the distinc-tion between a past-tense verb (VBD) and a pastparticiple (VBN).
For example, in the troops sta-tioned in Iraq, the verb stationed is a VBN; troopsis the head of the phrase.
On the other hand, forthe troops vacationed in Iraq, the verb vacationedis a VBD and also the head.
Some verbs make thedistinction explicit (eat has VBD ate, VBN eaten),but most require context for resolution.Conflating VBN/VBD is damaging because it af-fects downstream parsers and semantic role la-belers.
The task is difficult because nearby POStags can be identical in both cases.
When theverb follows a noun, tag assignment can hinge onworld-knowledge, i.e., the global lexical relationbetween the noun and verb (E.g., troops tends tobe the object of stationed but the subject of vaca-tioned).6 Web-scale N-gram data might help im-prove the VBN/VBD distinction by providing rela-tional evidence, even if the verb, noun, or verb-noun pair were not observed in training data.We extract nouns followed by a VBN/VBD in theWSJ portion of the Treebank (Marcus et al, 1993),getting 23K training, 1091 development and 1130test examples from sections 2-22, 24, and 23, re-spectively.
For out-of-domain data, we get 21K6HMM-style taggers, like the fast TnT tagger used on ourweb corpus, do not use bilexical features, and so perform es-pecially poorly on these cases.
One motivation for our workwas to develop a fast post-processor to fix VBN/VBD errors.examples from the Brown portion of the Treebankand 6296 examples from tagged Medline abstractsin the PennBioIE corpus (Kulick et al, 2004).The majority class baseline is to choose VBD.6.1 Supervised Verb DisambiguationThere are two orthogonal sources of informationfor predicting VBN/VBD: 1) the noun-verb pair,and 2) the context around the pair.
Both N-GMand LEX features encode both these sources.6.1.1 LEX featuresFor 1), we use indicators for the noun and verb,the noun-verb pair, whether the verb is on an in-house list of said-verb (like warned, announced,etc.
), whether the noun is capitalized and whetherit?s upper-case.
Note that in training data, 97.3%of capitalized nouns are followed by a VBD and98.5% of said-verbs are VBDs.
For 2), we provideindicator features for the words before the nounand after the verb.6.1.2 N-GM featuresFor 1), we characterize a noun-verb relation viafeatures for the pair?s distribution in Google V2.Characterizing a word by its distribution has along history in NLP; we apply similar techniquesto relations, like Turney (2006), but with a largercorpus and richer annotations.
We extract the 20most-frequent N-grams that contain both the nounand the verb in the pair.
For each of these, we con-vert the tokens to POS-tags, except for tokens thatare among the most frequent 100 unigrams in ourcorpus, which we include in word form.
We maskthe noun of interest as N and the verb of interestas V .
This converted N-gram is the feature label.The value is the pattern?s log-count.
A high countfor patterns like (N that V), (N have V) suggeststhe relation is a VBD, while patterns (N that wereV), (N V by), (V some N) indicate a VBN.
As al-ways, the classifier learns the association betweenpatterns and classes.For 2), we use counts for the verb?s context co-occurring with a VBD or VBN tag.
E.g., we seewhether VBD cases like troops ate or VBN caseslike troops eaten are more frequent.
Although ourcorpus contains many VBN/VBD errors, we hopethe errors are random enough for aggregate countsto be useful.
The context is an N-gram spanningthe VBN/VBD.
We have log-count features for allfive such N-grams in the (previous-word, noun,verb, next-word) quadruple.
The log-count is in-871System IN O1 O2Baseline 89.2 85.2 79.6ContextSum 92.5 91.1 90.4SVM with N-GM features 96.1 93.4 93.8SVM with LEX features 95.8 93.4 93.0SVM with N-GM + LEX 96.4 93.5 94.0Table 4: Verb-POS-disambiguation accuracy (%)trained on WSJ, tested on WSJ (IN) and out-of-domain Brown (O1) and Medline (O2).808590951001e41e3100Accuracy(%)Number of training examplesN-GM (N,V+context)LEX (N,V+context)N-GM (N,V)LEX (N,V)Figure 5: Out-of-domain learning curve of verbdisambiguation classifiers on Medline.dexed by the position and length of the N-gram.We include separate count features for contextsmatching the specific noun and for when the nountoken can match any word tagged as a noun.ContextSum: We use these context counts in anunsupervised system, ContextSum.
Analogouslyto Bergsma et al (2009), we separately sum thelog-counts for all contexts filled with VBD andthen VBN, outputting the tag with the higher total.6.2 Verb POS Disambiguation ResultsAs in all tasks, N-GM+LEX has the best in-domainaccuracy (96.4%, Table 4).
Out-of-domain, whenN-grams are excluded, errors only increase around14% on Medline and 2% on Brown (the differ-ences are not statistically significant).
Why?
Fig-ure 5, the learning curve for performance on Med-line, suggests some reasons.
We omit N-GM+LEXfrom Figure 5 as it closely follows N-GM.Recall that we grouped the features into twoviews: 1) noun-verb (N,V) and 2) context.
If weuse just (N,V) features, we do see a large drop out-of-domain: LEX (N,V) lags N-GM (N,V) even us-ing all the training examples.
The same is true us-ing only context features (not shown).
Using bothviews, the results are closer: 93.8% for N-GM and93.0% for LEX.
With two views of an example,LEX is more likely to have domain-neutral fea-tures to draw on.
Data sparsity is reduced.Also, the Treebank provides an atypical num-ber of labeled examples for analysis tasks.
In amore typical situation with less labeled examples,N-GM strongly dominates LEX, even when twoviews are used.
E.g., with 2285 training exam-ples, N-GM+LEX is statistically significantly bet-ter than LEX on both out-of-domain sets.All systems, however, perform log-linearly withtraining size.
In other tasks we only had a handfulof N-GM features; here there are 21K features forthe distributional patterns of N,V pairs.
Reducingthis feature space by pruning or performing trans-formations may improve accuracy in and out-of-domain.7 Discussion and Future WorkOf all classifiers, LEX performs worst on all cross-domain tasks.
Clearly, many of the regularitiesthat a typical classifier exploits in one domain donot transfer to new genres.
N-GM features, how-ever, do not depend directly on training examples,and thus work better cross-domain.
Of course, us-ing web-scale N-grams is not the only way to cre-ate robust classifiers.
Counts from any large auxil-iary corpus may also help, but web counts shouldhelp more (Lapata and Keller, 2005).
Section 6.2suggests that another way to mitigate domain-dependence is having multiple feature views.Banko and Brill (2001) argue ?a logical nextstep for the research community would be to di-rect efforts towards increasing the size of anno-tated training collections.?
Assuming we really dowant systems that operate beyond the specific do-mains on which they are trained, the communityalso needs to identify which systems behave as inFigure 2, where the accuracy of the best in-domainsystem actually decreases with more training ex-amples.
Our results suggest better features, suchas web pattern counts, may help more than ex-panding training data.
Also, systems using web-scale unlabeled data will improve automatically asthe web expands, without annotation effort.In some sense, using web counts as featuresis a form of domain adaptation: adapting a webmodel to the training domain.
How do we ensurethese features are adapted well and not used indomain-specific ways (especially with many fea-tures to adapt, as in Section 6)?
One option may872be to regularize the classifier specifically for out-of-domain accuracy.
We found that adjusting theSVM misclassification penalty (for more regular-ization) can help or hurt out-of-domain.
Otherregularizations are possible.
In each task, thereare domain-neutral unsupervised approaches.
Wecould encode these systems as linear classifierswith corresponding weights.
Rather than a typicalSVM that minimizes the weight-norm ||w|| (plusthe slacks), we could regularize toward domain-neutral weights.
This regularization could be opti-mized on creative splits of the training data.8 ConclusionWe presented results on tasks spanning a range ofNLP research: generation, disambiguation, pars-ing and tagging.
Using web-scale N-gram dataimproves accuracy on each task.
When less train-ing data is used, or when the system is used on adifferent domain, N-gram features greatly improveperformance.
Since most supervised NLP systemsdo not use web-scale counts, further cross-domainevaluation may reveal some very brittle systems.Continued effort in new domains should be a pri-ority for the community going forward.AcknowledgmentsWe gratefully acknowledge the Center for Lan-guage and Speech Processing at Johns HopkinsUniversity for hosting the workshop at which partof this research was conducted.ReferencesNir Ailon and Mehryar Mohri.
2008.
An efficient re-duction of ranking to classification.
In COLT.Michele Banko and Eric Brill.
2001.
Scaling to veryvery large corpora for natural language disambigua-tion.
In ACL.Cory Barr, Rosie Jones, and Moira Regelson.
2008.The linguistic structure of English web-searchqueries.
In EMNLP.Shane Bergsma, Dekang Lin, and Randy Goebel.2009.
Web-scale N-gram models for lexical disam-biguation.
In IJCAI.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In ACL.Thorsten Brants and Alex Franz.
2006.
The GoogleWeb 1T 5-gram Corpus Version 1.1.
LDC2006T13.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large language mod-els in machine translation.
In EMNLP.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speech tagger.
In ANLP.Andrew Carlson, Tom M. Mitchell, and Ian Fette.2008.
Data analysis project: Leveraging massivetextual corpora using n-gram statistics.
Technial Re-port CMU-ML-08-107.Kenneth Church, Ted Hart, and Jianfeng Gao.
2007.Compressing trigram language models with Golombcoding.
In EMNLP-CoNLL.Hal Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In ACL.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9.Dan Gildea.
2001.
Corpus variation and parser perfor-mance.
In EMNLP.Andrew R. Golding and Dan Roth.
1999.
A Winnow-based approach to context-sensitive spelling correc-tion.
Machine Learning, 34(1-3):107?130.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In KDD.Frank Keller and Mirella Lapata.
2003.
Using the webto obtain frequencies for unseen bigrams.
Computa-tional Linguistics, 29(3):459?484.Adam Kilgarriff and Gregory Grefenstette.
2003.
In-troduction to the special issue on the Web as corpus.Computational Linguistics, 29(3):333?347.Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel,Ryan McDonald, Martha Palmer, Andrew Schein,Lyle Ungar, Scott Winters, and Pete White.
2004.Integrated annotation for biomedical information ex-traction.
In BioLINK 2004: Linking Biological Lit-erature, Ontologies and Databases.Mirella Lapata and Frank Keller.
2005.
Web-basedmodels for natural language processing.
ACMTransactions on Speech and Language Processing,2(1):1?31.Mark Lauer.
1995a.
Corpus statistics meet the nouncompound: Some empirical results.
In ACL.Mark Lauer.
1995b.
Designing Statistical LanguageLearners: Experiments on Compound Nouns.
Ph.D.thesis, Macquarie University.Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,David Yarowsky, Shane Bergsma, Kailash Patil,Emily Pitler, Rachel Lathbury, Vikram Rao, KapilDalwani, and Sushant Narsale.
2010.
New tools forweb-scale N-grams.
In LREC.873Robert Malouf.
2000.
The order of prenominal adjec-tives in natural language generation.
In ACL.Mitchell P. Marcus, Beatrice Santorini, and MaryMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Mitchell P. Marcus.
1980.
Theory of Syntactic Recog-nition for Natural Languages.
MIT Press, Cam-bridge, MA, USA.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Reranking and self-training for parseradaptation.
In COLING-ACL.Margaret Mitchell.
2009.
Class-based ordering ofprenominal modifiers.
In 12th European Workshopon Natural Language Generation.Natalia N. Modjeska, Katja Markert, and Malvina Nis-sim.
2003.
Using the Web in machine learning forother-anaphora resolution.
In EMNLP.Preslav Nakov and Marti Hearst.
2005.
Search enginestatistics beyond the n-gram: Application to nouncompound bracketing.
In CoNLL.Preslav Ivanov Nakov.
2007.
Using the Web as an Im-plicit Training Set: Application to Noun CompoundSyntax and Semantics.
Ph.D. thesis, University ofCalifornia, Berkeley.Xuan-Hieu Phan.
2006.
CRFTagger: CRF EnglishPOS Tagger.
crftagger.sourceforge.net.Laura Rimell and Stephen Clark.
2008.
Adapting alexicalized-grammar parser to contrasting domains.In EMNLP.James Shaw and Vasileios Hatzivassiloglou.
1999.
Or-dering among premodifiers.
In ACL.Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,and Jun?ichi Tsujii.
2005.
Developing a robust part-of-speech tagger for biomedical text.
In Advances inInformatics.Peter D. Turney.
2006.
Similarity of semantic rela-tions.
Computational Linguistics, 32(3):379?416.David Vadas and James R. Curran.
2007a.
Addingnoun phrase structure to the Penn Treebank.
In ACL.David Vadas and James R. Curran.
2007b.
Large-scalesupervised models for noun phrase bracketing.
InPACLING.Xiaofeng Yang, Jian Su, and Chew Lim Tan.
2005.Improving pronoun resolution using statistics-basedsemantic compatibility information.
In ACL.874
