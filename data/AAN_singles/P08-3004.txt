Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 19?24,Columbus, June 2008. c?2008 Association for Computational LinguisticsCombining Source and Target Language Information forName Tagging of Machine Translation OutputShasha LiaoNew York University715 Broadway, 7th floorNew York, NY 10003 USAliaoss@cs.nyu.eduAbstractA Named Entity Recognizer (NER) generallyhas worse performance on machine translatedtext, because of the poor syntax of the MToutput and other errors in the translation.
Assome tagging distinctions are clearer in thesource, and some in the target, we tried tointegrate the tag information from both sourceand target to improve target language taggingperformance, especially recall.In our experiments with Chinese-to-EnglishMT output, we first used a simple merge of theoutputs from an ET (Entity Translation) systemand an English NER system, getting an absolutegain of 7.15% in F-measure, from 73.53% to80.68%.
We then trained an MEMM module tointegrate them more discriminatively, and got afurther average gain of 2.74% in F-measure,from 80.68% to 83.42%.1 IntroductionBecause of the growing multilingual environmentfor NLP, there is an increasing need to be able toannotate and analyze the output of machinetranslation (MT) systems.
But treating this task asone of processing ?ordinary text?
can lead to poorresults.
We examine this problem with respect tothe name tagging of English text.A Named Entity Recognizer (NER) trained onan English corpus does not have the sameperformance when applied to machine-translatedtext.
From our experiments on NIST 05 Chinese-to-English MT evaluation data, when we used thesame English NER to tag the reference translationand the MT output, the F-measure was 81.38% forthe reference but only 73.53% for the MT output.There are two primary reasons for this.
First, theperformance of current translation systems is notvery good, and so the output is quite different fromStandard English text.
The fluency of the translatedtext will be poor, and the context of a named entitymay be weird.
Second, the translated text has someforeign names which are hard for the English NERto recognize, even if they are well translated by theMT system, because such names appear veryinfrequently in the English training corpus.Training an NER on MT output does not seemto be an attractive solution.
It may take a lot oftime to manually annotate a large amount oftraining data, and this labor may have to berepeated for a new MT system or even a newversion of an existing MT system.
Furthermore,the resulting system may still not work well, in sofar as the translation is not good and information issomehow distorted.
In fact, sometimes themeanings of the translated sentences are hard todecipher unless we check the source language orget a human translated document as reference.
As aresult, we need source language information to aidthe English NER.However, it is also not enough to rely entirelyon the source language NE results and map themonto the translated English text.
First, the wordalignment from source language to Englishgenerated by the MT system may not be accurate,leading to problems in mapping the Chinese nametags.
Second, the translated text is not exactly sameas the source language because there may beinformation missed or added.
For example, theChinese phrase ?????
?, which is not a namein Chinese, and should be literally translated as19?the subway in Hong Kong?, may end up beingtranslated to ?mtrc?, the abbreviation of ?The MassTransit Railway Corporation?, which is anorganization in Hong Kong (and so should get aname tag in English).If we can use the information from both thesource language and the translated text, we cannotonly find the named entities missed by the EnglishNER, but also modify incorrect boundaries in theEnglish results which are caused by the badcontent.
However, using word alignment to mapthe source language information into the Englishtext is problematic, for two reasons: First, the wordalignment produced by machine translation istypically not very good, with a Chinese-EnglishAER (alignment error rate) of about 40% (Dengand William 2005).
So just using word alignmentto map the information would introduce a lot ofnoise.
Second, in the case of function words inEnglish which have no corresponding realization inChinese, traditional word alignment would alignthe function word with another Chineseconstituent, such as a name, which could lead toboundary errors in tagging English names.
Wehave therefore used an alternative method to fetchthe source language information for informationextraction, which is called Entity Translation and isdescribed in Section 3.2 MotivationWhen we use the English NER to annotate thetranslated text, we find that the performance is notas good as English texts.
This is due to severaltypes of problems.2.1 Bad name contextsProducing correct word order is very hard for aphrase-based MT system, particularly whentranslating between two such disparate languages,and there are still a lot of Chinese syntax structuresleft in translated text, which are usually not regularEnglish expressions.
As a result, it is hard for theEnglish NER to detect names in these contexts.1Ex.
1. annan said, "kumaratunga presidentpersonally against him to areas under guerrillacontrol field visit because it feared the rebelswill use his visit as a political chip"1The MT system we used generates monocase translations, sowe show all the translations in lower case.It is hard to recognize from this example thatkumaratunga is a person name unless we arealready familiar with this name or realize this is anormal Chinese expression structure, although notan English one.Ex.
2.
A reporter from shantou <ORG2>university school of medicine</ORG>, facultyof medicine, university of <GPE>hongkong</GPE>, <ORG>influenza researchcenter</ORG> was informed that ?...Here source language information can help fixincorrect name boundaries assigned by the EnglishNER, especially from a messy context.
In Example3, the source language tagger can tell us that?shantou university?
and ?university of hongkong?
are two named entities, allowing us to fixthe wrong name boundaries of the English NER.2.2 Bad translationsThere are cases where the MT system does notrecognize there is a name and translates it assomething else, and if we do not refer to the sourcelanguage, we sometimes cannot understand thesentence, or annotate it.Ex.
3. xinhua shanghai , january 1(<ORG>feng yizhen su lofty</ORG>) snow ,frozen , and the shanghai airport staff in snowand inalienable .The translation system does not output the namescorrectly, and only when we look at the Chinesesentence can we know that there are two personnames here, one is ?feng yizhen?, and the other is?su lofty?, where the second one is translatedincorrectly.
English NER treats the whole as anORGANIZATION as there is no punctuation toseparate the two names.2.3 Unknown foreign namesThere are many Chinese GPE and PERSON nameswhich are missed because they appear rarely inEnglish text, especially city, county or evenprovince names, and so are hard for English NERto detect or classify.
However, on the Chinese side,they may be common names and so easily tagged.2We use the entity types of ACE (the Automatic ContentExtraction evaluation) for name types.
Here ORG =?ORGANIZATION?
is the tag for an organization; GPE =?Geo-Political Entity?
is the tag for a location with agovernment; other locations (e.g., ?Sahara Desert?)
are taggedas LOCATION.20Ex.
4.
At present, shishi city in the province toachieve a village public transportation, villagewater ; village of cable television .The city names in examples 4 are famous inChinese but do not appear much in English text,and so are missed by the English NER; however, aChinese NER would be able to tag them as namedentities.3 Entity Translation SystemThe MT pipeline we employ begins with an EntityTranslation (ET) system which identifies andtranslates the names in the text (Heng Ji et al,2007).
This system runs a source-language NER(based on an HMM) and then uses a variety ofstrategies to translate the names it identifies.
Onestrategy, for example, uses a corpus-trained nametransliteration component coupled with a targetlanguage model to select the best transliteration.The source text, annotated with name translations,is then passed to a statistical, phrase-based MTsystem (Zens and Ney, 2004).
Depending on itsphrase table and language model, this name-awareMT system would decide whether to accept thetranslation provided by ET.
Experiments show thatthe MT system with ET pre-processing canproduce better translations than the MT systemalone, with 17% relative error reduction on overallname translation.The strategy combining multiple transliterationsand selection based on a language model isparticularly effective for foreign (non-Chinese)person names rendered in Chinese.
If these namesdid not appear in the bilingual training material,they would be mistranslated by an MT systemwithout ET.
These names are often also difficultfor the English tagger, so ET can benefit bothtranslation and name recognition.For each name tagged by ET, we see if thetranslation string proposed by ET appears in thetranslation produced by the MT system.
If so, weuse the ET output to assign an ?ET name type?
tothat string in the translation.
This approach avoidsthe problems of using word alignments from theMT system; in particular, the alignment of functionwords in English with names in Chinese.4 Integrating source and targetinformationWe first try a very simple merge method to seehow much gain can be gotten by simply combiningthe two sources.
After that, we describe a corpus-trained model which addresses some of the tagconflict situations and gets additional gains.4.1 Results from English NER and ETFirst, we analyzed the English NER and ET outputto see the named entity distribution of the twosources.
We focus on the differences between thembecause when they agree, we can expect littleimprovement from using source languageinformation.
In the nist05 data, we find 1893named entities in the English NER output (targetlanguage part) and 1968 named entities in the EToutput (source language part); 1171 of them are thesame.
This means that 38.14% of the names taggedin the target language and 40.5% of those in thesource language do not have a corresponding tag inthe other language, which suggests that the sourceand target NER may have different strengths onname tagging.We checked the names which are taggeddifferently, and there are 347 correct names fromET missed by English NER and 418 from EnglishNER missed by ET.4.2 Simple MergeFirst, in order to see if the ET system can reallyhelp the English NER, we do a simple mergeexperiment, which just adds the named entitiesextracted from the ET system into the EnglishNER results, so long as there is no conflictbetween them (i.e., so long as the ET-tagged namedoes not overlap an English NE-tagged name).Our experiments show that this simple methodcan improve the English NER result substantially(Table 5-1), especially for recall, confirming ourintuition.We checked the errors produced by this simplemerge method, and divided them into four types.1.
Missed by both sources.2.
Missed by one source and erroneously taggedby the other3.
Erroneously tagged by both sources4.
Conflict situations where the English NE-tagged name is wrong but the ET-tagged nameis correct.21Although there is not much we can do for the firstthree error types, we can address the last error typeby some intelligent learning method.
In NIST05data, there are 261 names which have conflicts,and we can get more gains here.There are two kinds of conflicts: A type conflictwhich occurs when the ET and English NER tagthe same named entity but give it different types;and a boundary conflict which occurs when there isa tag overlap between English NER and ET.
Wetreat these two kinds of conflict differently byusing different features to indicate them.4.3 Maximum Entropy Markov ModelWe use a MEMM (Maximum Entropy MarkovModel) as our tagging model.
An MEMM is avariation on traditional Hidden Markov Models(HMM).
Like an HMM, it attempts to characterizea string of tokens as a most likely set of transitionsthrough a Markov model.
The MEMM allowsobservations to be represented as arbitraryoverlapping features (such as word, capitalization,formatting, part-of-speech), and defines theconditional probability of state sequences givenobservation sequences.
It does this by using themaximum entropy framework to fit a set ofexponential models that represent the probabilityof a state given an observation and the previousstate (McCallum et al 2000).In our experiment, we train the maximumentropy framework at the token level, and use theBIO types as the states to be predicted.
There arefour entity types: PERSON, ORGANIZATION,GPE and LOCATION, and so a total of 9 states.4.4 Feature Sets for MEMMIn our experiment, we are interested not only intraining a module, but also in measuring thedifferent performance for different scales oftraining corpora.
If a small annotated corpus canget reasonable gain, this method for combiningtaggers will be much more practical.As a result, we first build a small feature set andenlarge it by adding more features, expecting thatthe small feature set may get better performancewith a small training corpus.Set 1: Features Focusing on Current Tag andPrevious State InformationWe first try to use few features to see how muchgain we can get if we only consider the taginformation from ET and English NER, and theprevious state.
These features are:F1: current token?s type in ETF2: current token?s type in English NERF3: Feature1+Feature2F4: if there is a type conflict + ET type +English NER typeF5: if there is a type conflict +ET typeconfidence + English NER confidenceF6: if there is a boundary conflict + ET type +English NER typeF7: if there is a boundary conflict + ET tokenconfidence + English NER confidenceF8: state for the previous tokenF4 and F5 are used to help resolve the typeconflicts, and F6 and F7 to resolve boundaryconflicts.
When there is a conflict, we need theconfidence information from both ET and EnglishNER to indicate which side to choose.The English NER reports a margin, which canbe used to gauge tag confidence.
The margin is thedifference in log probability between the toptagging hypothesis and a hypothesis which assignsthe name a different NE tag, or no NE tag.
We usethis as the confidence of English NER output.For ET output, the situation is morecomplicated.
We use different confidence methodsfor type and boundary conflicts.
For type conflicts,we use the source of the ET translation as the ?typeconfidence?, for example, if the ET result comesfrom a person name list, the output is probablycorrect.
For boundary conflicts, as the ET systemuses some pruning strategy to fix the boundaryerrors in word alignment, and the translationprocedure contains several disparate componentswhich produce different kind of confidencemeasure, it is not reasonable to use Chinese NERconfidence as the confidence estimate.
As a result,we check if the token is capitalized in ETtranslation, and treat it as the ?token confidence?.Set 2: Set 1 + Current Token InformationF9: current token + ET type+ English NERtypeToken information can be used to predict the resultwhen there is a conflict, as the conflict reasonvaries and in some cases without knowing thetoken itself, it is hard to know the right choice.
Asa result, we add the current token feature but this isthe only place we use token information.22Set 3: Set2 + Sequence InformationOur experiments showed some performance gainwith only the current token features and theprevious state, but we still wanted to see ifadditional features ?
such as information on theprevious and following tokens ?
would help.
Tothis end, we added such features, while stillretaining our focus on the ET and English NERinformation:F10: English NER result of the current token +that of the previous tokenF11: ET result of the current token + ET resultof the previous token.F12: English NER result of the current token +that of the next token.F13: ET result of the current token + that ofthe next token.5 ExperimentThe experiment was carried out on the Chinesepart of the NIST 05 machine translation evaluation(NIST05) and NIST 04 machine translationevaluation (NIST04) data, where NISTT05contains 100 documents and NIST04 contains 200documents.
We annotated all the data in NIST05and 120 documents for NIST04 for ourexperiment.The ET system used a Chinese HMM-basedNER trained on 1,460,648 words; the Englishname tagger was also HMM-based and trained on450,000 words.First, we want to see the result with very smalltraining data, and so divided the NIST05 data into5 subsets, each containing 20 documents.
We ran across validation experiment on this small corpus,with 4 subsets as training data and 1 as testingdata.
We refer to this configuration as Corpus13.Second, to see whether increasing the trainingdata would appreciably influence the result, weadded the annotated NIST04 data into the trainingcorpus, and we call this configuration Corpus2.3We conducted some experiments with a small corpus inwhich we relied on the alignment information from the MTsystem, but the results were much worse than using the EToutput.
Simple merge using alignment yielded a name taggerF score of 73.34% (1.42% worse than the baseline, 75.76%),while ET F score of 81.23%; MEMM with minimal featuresusing alignment yielded an improvement of 1.7% (vs. 7.9%using ET).Figure 1.
Flow chart of our system5.1 Simple Merge ResultThe simple merge method gets a significant F-measure gain of 7.15% from the English NERbaseline, which confirms our intuition that somenamed entities are easy to tag in source languageand others in target language.
This representsprimarily a significant recall improvement, 14.37%.NER baseline Simple MergeP85.68 82.70R64.39 78.76F73.53 80.68Table 1.
Simple merge method on Corpus1 (100 documents)5.2 Integrating Results on Corpus1On this small training corpus, we test each subsetwith other subsets as training data, and calculatethe total performance on the whole corpus.
Thebest result comes from Set2 instead of Set3,presumably because the training data is too smallto handle the richer model of Set3.
Our experimentshows that we can get 1.9% gain over simplemerge method with Set 2 using 80 documents astraining data.Simple Merge Set1 Set2 Set3P82.7084.73 84.72 84.48R78.7678.01 80.55 80.15F80.6881.23 82.58 82.26English NEIntegrationProcedureETChinese NEEnglish TextFinal Tagged TextET-Tagged TextNE-Tagged TextChinese TextMT23Table 2.
Results on Corpus1, which contains 100 documents,with 80 documents used for training at each fold.5.3 Integrating Results on Corpus2On this corpus, every training data set contains 200documents, and we can get a gain of 2.74% overthe simple merge method.
With the larger trainingset, the richer model (Set 3) now outperforms theothers.Simple Merge Set1 Set2 Set3P82.7085.04 85.15 85.78R78.7678.09 80.59 81.18F80.6881.42 82.81 83.42Table 3.
Result on Corpus2 (220 documents), with 200documents used for training at each fold of cross-validation.On corpus2, Using a Wilcoxon Matched-Pairs test,with a 10-fold division, all the sets performsignificantly better (in F-measure) than the simplemerge at a 95% confidence level.6 Prior WorkHuang and Vogel (2002) describe an approach toextract a named entity translation dictionary from abilingual corpus while concurrently improving thenamed entity annotation quality.
They use astatistical alignment model to align the entities anditeratively extract the name pairs with higheralignment probability and treat them as globalinformation to improve the monolingual namedentity annotation quality for both languages.
Usingthis iterative method, they get a smaller but cleanernamed entity translation dictionary and improvethe annotation F-measure from 70.03 to 78.15 forChinese and 73.38 to 81.46 in English.
This workis similar in using information from the sourcelanguage (in this case mediated by the wordalignment) to improve the target language tagging.However, they used bi-texts (with hand-translated,relatively high-quality English) and so did notencounter the problems, mentioned above, whicharise with MT output.7 ConclusionWe present an integrated approach to extract thenamed entities from machine translated text, usingname entity information from both source andtarget language.
Our experiments show that with acombination of ET and English NER, we can get aconsiderably better NER result than would bepossible with either alone, and in particular, a largeimprovement in name identification recall.MT output poses a challenge for any type oflanguage analysis, such as relation or eventrecognition or predicate-argument analysis.
Eventhough MT is improving, this problem is likely tobe with us for some time.
The work reported hereindicates how source language information can bebrought to bear on such tasks.The best F-measure in our experiments exceedsthe score of the English NER on reference text,which reflects the intuition that even for welltranslated text, we can still benefit from sourcelanguage information.AcknowledgmentsThis material is based upon work supported by theDefense Advanced Research Projects Agencyunder Contract No.
HR0011-06-C-0023, and theNational Science Foundation under Grant NO.
IIS-0534700.
Any opinions, findings and conclusionsexpressed in this material are those of the authorand do not necessarily reflect the views of the U. S.Government.ReferencesYonggang Deng, Byrne and William J.
2005.
HMMWord and Phrase Alignment for Statistical MachineTranslation.
Proc.
Human Language TechnologyConference and Empirical Methods in NaturalLanguage Processing.Fei Huang and Vogel, S. 2002.
Improved named entitytranslation and bilingual named entityextraction.
Proc.
Fourth IEEE Int'l.
Conf.
onMultimodal Interfaces.A.
McCallum, D. Freitag and F. Pereira.
2000.Maximum entropy Markov models for informationextraction and segmentation.
Proc.
17thInternational Conf.
on Machine Learning.Heng Ji, Matthias Blume, Dayne Freitag,RalphGrishman, Shahram Khadivi and Richard Zens.2007.
NYU-Fair Isaac-RWTH Chinese to EnglishEntity Translation 07 System.
Proceedings of ACEET 2007 PI/Evaluation Workshop.
Washington.Richard Zens and Hermann Ney.
2004.
Improvements inphrase-based statistical Machine Translation.
InProc.
HLT/NAACL,Boston24
