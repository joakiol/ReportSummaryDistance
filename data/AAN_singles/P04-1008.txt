Statistical Modeling for Unit Selection in Speech SynthesisCyril Allauzen and Mehryar Mohri and Michael Riley?AT&T Labs ?
Research180 Park Avenue, Florham Park, NJ 07932, USA{allauzen, mohri, riley}@research.att.comAbstractTraditional concatenative speech synthesis systemsuse a number of heuristics to define the target andconcatenation costs, essential for the design of theunit selection component.
In contrast to these ap-proaches, we introduce a general statistical model-ing framework for unit selection inspired by auto-matic speech recognition.
Given appropriate data,techniques based on that framework can result in amore accurate unit selection, thereby improving thegeneral quality of a speech synthesizer.
They canalso lead to a more modular and a substantially moreefficient system.We present a new unit selection system based onstatistical modeling.
To overcome the original ab-sence of data, we use an existing high-quality unitselection system to generate a corpus of unit se-quences.
We show that the concatenation cost canbe accurately estimated from this corpus using a sta-tistical n-gram language model over units.
We usedweighted automata and transducers for the repre-sentation of the components of the system and de-signed a new and more efficient composition algo-rithm making use of string potentials for their com-bination.
The resulting statistical unit selection isshown to be about 2.6 times faster than the last re-lease of the AT&T Natural Voices Product whilepreserving the same quality, and offers much flex-ibility for the use and integration of new and morecomplex components.1 MotivationA concatenative speech synthesis system (Hunt andBlack, 1996; Beutnagel et al, 1999a) consists ofthree components.
The first component, the text-analysis frontend, takes text as input and outputsa sequence of feature vectors that characterize theacoustic signal to synthesize.
The first element ofeach of these vectors is the predicted phone or half-phone; other elements are features such as the pho-netic context, acoustic features (e.g., pitch, dura-tion), or prosodic features.?
This author?s new address is: Google, Inc, 1440 Broadway,New York, NY 10018, riley@google.com.The second component, unit selection, deter-mines in a set of recorded acoustic units corre-sponding to phones (Hunt and Black, 1996) or half-phones (Beutnagel et al, 1999a) the sequence ofunits that is the closest to the sequence of fea-ture vectors predicted by the text analysis frontend.The final component produces an acoustic signalfrom the unit sequence chosen by unit selectionusing simple concatenation or other methods suchas PSOLA (Moulines and Charpentier, 1990) andHNM (Stylianou et al, 1997).Unit selection is performed by defining two costfunctions: the target cost that estimates how thefeatures of a recorded unit match the specified fea-ture vector and the concatenation cost that estimateshow well two units will be perceived to match whenappended.
Unit selection then consists of finding,given a specified sequence of feature vectors, theunit sequence that minimizes the sum of these twocosts.The target and concatenation cost functions havetraditionally been formed from a variety of heuris-tic or ad hoc quality measures based on features ofthe audio and text.
In this paper, we follow a differ-ent approach: our goal is a system based purely onstatistical modeling.
The starting point is to assumethat we have a training corpus of utterances labeledwith the appropriate unit sequences.
Specifically,for each training utterance, we assume available asequence of feature vectors f = f1 .
.
.
fn and thecorresponding units u = u1 .
.
.
un that should beused to synthesize this utterance.
We wish to esti-mate from this corpus two probability distributions,P (f |u) and P (u).
Given these estimates, we canperform unit selection on a novel utterance using:u = argmaxuP (u|f) (1)= argminu(?
logP (f |u) ?
logP (u)) (2)Equation 1 states that the most likely unit se-quence is selected given the probabilistic modelused.
Equation 2 follows from the definition ofconditional probability and that P (f) is fixed for agiven utterance.
The two terms appearing in Equa-tion 2 can be viewed as the statistical counterpartsof the target and concatenation costs in traditionalunit selection.The statistical framework just outlined is simi-lar to the one used in speech recognition (Jelinek,1976).
We also use several techniques that havebeen very successfully applied to speech recogni-tion.
For instance, in this paper, we show how?
logP (u) (the concatenation cost) can be accu-rately estimated using a statistical n-gram languagemodel over units.
Two questions naturally arise.
(a) How can we collect a training corpus for build-ing a statistical model?
Ideally, the training cor-pus could be human-labeled, as in speech recog-nition and other natural language processing tasks.But this seemed impractical given the size of theunit inventory, the number of utterances needed forgood statistical estimates, and our limited resources.Instead, we chose to use a training corpus gener-ated by an existing high-quality unit selection sys-tem, that of the AT&T Natural Voices Product.
Ofcourse, building a statistical model on that outputcan, at best, only match the quality of the origi-nal.
But, it can serve as an exploratory trial to mea-sure the quality of our statistical modeling.
As wewill see, it can also result in a synthesis system thatis significantly faster and modular than the originalsince there are well-established algorithms for rep-resenting and optimizing statistical models of thetype we will employ.
To further simplify the prob-lem, we will use the existing traditional target costs,providing statistical estimates only of the concate-nation costs (?
logP (u)).
(b) What are the benefits of a statistical modelingapproach?
(1) High-quality cost functions.
One issuewith traditional unit selection systems is thattheir cost functions are the result of the followingcompromise: they need to be complex enoughto have a perceptual meaning but simple enoughto be computed efficiently.
With our statisticalmodeling approach, the labeling phase could beperformed offline by a highly accurate unit selec-tion system, potentially slow and complex, whilethe run-time statistical system could still be fast.Moreover, if we had audio available for our trainingcorpus, we could exploit that in the initial label-ing phase for the design of the unit selection system.
(2) Weighted finite-state transducer representa-tion.
In addition to the already mentioned synthesisspeed and the opportunity of high-quality measuresin the initial offline labeling phase, another benefitof this approach is that it leads to a natural represen-tation by weighted transducers, and hence enablesus to build a unit selection system using generaland flexible representations and methods already inuse for speech recognition, e.g., those found in theFSM (Mohri et al, 2000), GRM (Allauzen et al,2004) and DCD (Allauzen et al, 2003) libraries.Other unit selection systems based on weightedtransducers were also proposed in (Yi et al, 2000;Bulyko and Ostendorf, 2001).
(3) Unit selection algorithms and speed-up.
Wepresent a new unit selection system based on sta-tistical modeling.
We used weighted automata andtransducers for the representation of the compo-nents of the system and designed a new and efficientcomposition algorithm making use of string poten-tials for their combination.
The resulting statisticalunit selection is shown to be about 2.6 times fasterthan the last release of the AT&T Natural VoicesProduct while preserving the same quality, and of-fers much flexibility for the use and integration ofnew and more complex components.2 Unit Selection Methods2.1 Overview of a Traditional Unit SelectionSystemThis section describes in detail the cost functionsused in the AT&T Natural Voices Product that wewill use as the baseline in our experimental results,see (Beutnagel et al, 1999a) for more details aboutthis system.
In this system, unit selection is basedon (Hunt and Black, 1996) but using units corre-sponding to halfphones instead of phones.
Let Ube the set of recorded units.
Two cost functionsare defined: the target cost Ct(fi, ui) is used toestimate the mismatch between the features of thefeature vector fi and the unit ui; the concatena-tion cost Cc(ui, uj) is used to estimate the smooth-ness of the acoustic signal when concatenating theunits ui and uj .
Given a sequence f = f1 .
.
.
fnof feature vectors, unit selection can then be formu-lated as the problem of finding the sequence of unitsu = u1 .
.
.
un that minimizes these two costs:u = argminu?Un(n?i=1Ct(fi, ui) +n?i=2Cc(ui?1, ui))In practice, not all unit sequences of a given lengthare considered.
A preselection method such as theone proposed by (Conkie et al, 2000) is used.
Thecomputation of the target cost can be split in twoparts: the context cost Cp that is the component ofthe target cost corresponding to the phonetic con-text, and the feature cost Cf that corresponds theother components of the target cost:Ct(fi, ui) = Cp(fi, ui) + Cf (fi, ui) (3)For each phonetic context ?
of length 5, a list L(?
)of the units that are the most frequently used in thephonetic context ?
is computed.
For each featurevector fi in f , the candidate units for fi are com-puted in the following way.
Let ?i be the 5-phonecontext of fi in f .
The context costs between fi andall the units in the preselection list of the phoneticcontext ?i are computed and the M units with thebest context cost are selected:Ui = M-bestui?L(?i)(Cp(fi, ui))The feature costs between fi and the units in Ui arethen computed and the N units with the best targetcost are selected:U ?i = N-bestui?Ui(Cp(fi, ui) + Cf (fi, ui))The unit sequence u verifying:u = argminu?U ?1??
?U ?n(n?i=1Ct(fi, ui) +n?i=2Cc(ui?1, ui))is determined using a classical Viterbi search.
Thus,for each position i, the N2 concatenation costs be-tween the units in U ?i and U ?i+1 need to be com-puted.
The caching method for concatenation costsproposed in (Beutnagel et al, 1999b) can be used toimprove the efficiency of the system.2.2 Statistical Modeling ApproachOur statistical modeling approach was describedin Section 1.
As already mentioned, our generalapproach would consists of deriving both the tar-get cost ?
logP (f |u) and the concatenation cost?
logP (u) from appropriate training data usinggeneral statistical methods.
To simplify the prob-lem, we will use the existing target cost provided bythe traditional unit selection system and concentrateon the problem of estimating the concatenation cost.We used the unit selection system presented in theprevious section to generate a large corpus of morethan 8M unit sequences, each unit corresponding toa unique recorded halfphone.
This corpus was usedto build an n-gram statistical language model us-ing Katz backoff smoothing technique (Katz, 1987).This model provides us with a new cost function, thegrammar cost Cg, defined by:Cg(uk|u1...uk?1) = ?
log(P (uk|u1...uk?1))where P is the probability distribution estimated byour model.
We used this new cost function to re-place both the concatenation and context costs usedin the traditional approach.
Unit selection then con-sists of finding the unit sequence u such that:u = argminu?Unn?i=1(Cf (fi, ui)+Cg(ui|ui?k .
.
.
ui?1))In this approach, rather than using a preselectionmethod such as that of (Conkie et al, 2000), we areusing the statistical language model to restrict thecandidate space (see Section 4.2).3 Representation by Weighted Finite-StateTransducersAn important advantage of the statistical frame-work we introduced for unit selection is that the re-sulting components can be naturally represented byweighted finite-state transducers.
This casts unit se-lection into a familiar schema, that of a Viterbi de-coder applied to a weighted transducer.3.1 Weighted Finite-State TransducersWe give a brief introduction to weighted finite-statetransducers.
We refer the reader to (Mohri, 2004;Mohri et al, 2000) for an extensive presentation ofthese devices and will use the definitions and nota-tion introduced by these authors.A weighted finite-state transducer T is an 8-tupleT = (?,?, Q, I, F,E, ?, ?)
where ?
is the finiteinput alphabet of the transducer, ?
is the finite out-put alphabet, Q is a finite set of states, I ?
Q theset of initial states, F ?
Q the set of final states,E ?
Q ?
(?
?
{}) ?
(?
?
{}) ?
R ?
Q a fi-nite set of transitions, ?
: I ?
R the initial weightfunction, and ?
: F ?
R the final weight functionmapping F to R. In our statistical framework, theweights can be interpreted as log-likelihoods, thusthere are added along a path.
Since we use the stan-dard Viterbi approximation, the weight associatedby T to a pair of strings (x, y) ?
??
?
??
is givenby:[[T ]](x, y) = minpi?R(I,x,y,F )?
[p[pi]] + w[pi] + ?
[n[pi]]where R(I, x, y, F ) denotes the set of paths from aninitial state p ?
I to a final state q ?
F with inputlabel x and output label y, w[pi] the weight of thepath pi, ?
[p[pi]] the initial weight of the origin stateof pi, and ?
[n[pi]] the final weight of its destination.A Weighted automaton A = (?, Q, I, F,E, ?, ?
)is defined in a similar way by simply omitting theoutput (or input) labels.
We denote by ?2(T ) the0 1a 2b 3c 4d(a)01a:x5a:u2b:y6b:v3c:z 4d:t7c:w 8a:s(b)01a:x2a:u3b:y4b:v5c:z6c:w7d:t(c)Figure 1: (a) Weighted automaton T1.
(b) Weightedtransducer T2.
(c) T1 ?
T2, the result of the compo-sition of T1 and T2.weighted automaton obtained from T by removingits input labels.A general composition operation similar tothe composition of relations can be defined forweighted finite-state transducers (Eilenberg, 1974;Berstel, 1979; Salomaa and Soittola, 1978; Kuichand Salomaa, 1986).
The composition of two trans-ducers T1 and T2 is a weighted transducer denotedby T1 ?
T2 and defined by:[[T1 ?
T2]](x, y) = minz???
{[[T1]](x, z) + [[T2]](z, y)}There exists a simple algorithm for constructingT = T1 ?
T2 from T1 and T2 (Pereira and Riley,1997; Mohri et al, 1996).
The states of T are iden-tified as pairs of a state of T1 and a state of T2.
Astate (q1, q2) in T1?T2 is an initial (final) state if andonly if q1 is an initial (resp.
final) state of T1 and q2is an initial (resp.
final) state of T2.
The transitionsof T are the result of matching a transition of T1and a transition of T2 as follows: (q1, a, b, w1, q?1)and (q2, b, c, w2, q?2) produce the transition((q1, q2), a, c, w1 + w2, (q?1, q?2)) (4)in T .
The efficiency of this algorithm was critical tothat of our unit selection system.
Thus, we designedan improved composition that we will describe later.Figure 1(c) gives the resulting of the composition ofthe weighted transducers given figure 2(a) and (b).3.2 Language Model Weighted TransducerThe n-gram statistical language model we constructfor unit sequences can be represented by a weightedautomaton G which assigns to each sequence u itslog-likelihood:[[G]](u) = ?
log(P (u)).
(5)according to our probability estimate P .
Sincea unit sequence u uniquely determines the corre-sponding halfphone sequence x, the n-gram statis-tical model equivalently defines a model of the jointdistribution of P (x, u).
G can be augmented todefine a weighted transducer G?
assigning to pairs(x, u) their log-likelihoods.
For any halfphone se-quence x and unit sequence u, we define G?
by:[[G?
]](x, u) = ?
logP (u) (6)The weighted transducer G?
can be used to generateall the unit sequences corresponding to a specifichalfphone sequence given by a finite automaton p,using composition: p ?
G?.
In our case, we also wishto use the language model transducer G?
to limit thenumber of candidate unit sequences considered.
Wewill do that by giving a strong precedence to n-grams of units that occurred in the training corpus(see Section 4.2).Example Figure 2(a) shows the bigram model Gestimated from the following corpus:<s> u1 u2 u1 u2 </s><s> u1 u3 </s><s> u1 u3 u1 u2 </s>where ?s?
and ?/s?
are the symbols marking thestart and the end of an utterance.
When the unit u1is associated to the halfphone p1 and both units u1and u2 are associated to the halfphone p2, the corre-sponding weighted halfphone-to-unit transducer G?is the one shown in Figure 2(b).3.3 Unit Selection with Weighted Finite-StateTransducersFrom each sequence f = f1 .
.
.
fn of feature vec-tors specified by the text analysis frontend, we canstraightforwardly derive the halfphone sequence tobe synthesized and represent it by a finite automa-ton p, since the first component of each feature vec-tor fi is the corresponding halfphone.
Let W be theweighted automaton obtained by composition of pwith G?
and projection on the output:W = ?2(p ?
G?)
(7)W represents the set of candidate unit sequenceswith their respective grammar costs.
We can thenuse a speech recognition decoder to search for thebest sequence u since W can be thought of as the</s>u3</s>/0.703.
?/3.647 u1u1/0.703</s>/1.466u3/1.871u1/0.955u2u2/1.466u3/0.921?/5.034u2/0.514</s>/0.410?/4.053u1/1.108<s>?/5.216u1/0.003</s>u3?:</s>/0.703.?
:?/3.647 u1p1:u1/0.703?:</s>/1.466p2:u3/1.871p1:u1/0.955u2p2:u2/1.466p2:u3/0.921?:?/5.034p2:u2/0.514?:</s>/0.410?:?/4.053p1:u1/1.108<s>?
:?/5.216p1:u1/0.003(a) (b)Figure 2: (a) n-gram language model G for unit sequences.
(b) Corresponding halfphone-to-unit weightedtransducer G?.counterpart of a speech recognition transducer, fthe equivalent of the acoustic features and Cf theanalogue of the acoustic cost.
Our decoder uses astandard beam search of W to determine the bestpath by computing on-the-fly the feature cost be-tween each unit and its corresponding feature vec-tor.Composition constitutes the most costly opera-tion in this framework.
Section 4 presents severalof the techniques that we used to speed up that al-gorithm in the context of unit selection.4 Algorithms4.1 Composition with String PotentialsIn general, composition may create non-coaccessible states, i.e., states that do not admit apath to a final state.
These states can be removedafter composition using a standard connection (ortrimming) algorithm that removes unnecessarystates.
However, our purpose here is to avoid thecreation of such states to save computational time.To that end, we introduce the notion of stringpotential at each state.Let i[pi] (o[pi]) be the input (resp.
output) label ofa path pi, and denote by x ?
y the longest commonprefix of two strings x and y.
Let q be a state in aweighted transducer.
The input (output) string po-tential of q is defined as the longest common prefixof the input (resp.
output) labels of all the paths inT from q to a final state:pi(q) =?pi??
(q,F )i[pi]po(q) =?pi??
(q,F )o[pi]The string potentials of the states of T can be com-puted using the generic shortest-distance algorithmof (Mohri, 2002) over the string semiring.
They canbe used in composition in the following way.
Wewill say that two strings x and y are comparable ifx is a prefix of y or y is a prefix of x.Let (q1, q2) be a state in T = T1 ?
T2.
Notethat (q1, q2) is a coaccessible state only if the out-put string potential of q1 in T1 and the input stringpotential of q2 in T2 are comparable, i.e., po(q1) isa prefix of pi(q2) or pi(q2) is a prefix of po(q1).Hence, composition can be modified to create onlythose states for which the string potentials are com-patible.As an example, state 2 = (1, 5) of the transducerT = T1 ?
T2 in Figure 1 needs not be created sincepo(1) = bcd and pi(5) = bca are not comparablestrings.The notion of string potentials can be extendedto further reduce the number of non-coaccessiblestates created by composition.
The extended inputstring potential of q in T , is denoted by p?i(q) and isthe set of strings defined by:p?i(q) = pi(q) ?
?i(q) (8)where ?i(q) ?
?
and is such that for every ?
?
?i(q), there exist a path pi from q to a final state suchthat pi(q)?
is a prefix of the input label of pi.
The ex-tended output string potential of q, p?o(q), is definedsimilarly.
A state (q1, q2) in T1 ?
T2 is coaccessibleonly if(p?o(q1) ?
??)
?
(p?i(q2) ?
??)
6= ?
(9)Using string potentials helped us substantially im-prove the efficiency of composition in unit selection.4.2 Language Model Transducer ?
BackoffAs mentioned before, the transducer G?
representsan n-gram backoff model for the joint probabilitydistribution P (x, u).
Thus, backoff transitions areused in a standard fashion when G?
is viewed as anautomaton over paired sequences (x, u).
Since weuse G?
as a transducer mapping halfphone sequencesto unit sequences to determine the most likely unitsequence u given a halfphone sequence x 1we needto clarify the use of the backoff transitions in thecomposition p ?
G?.Denote by O(V ) the set of output labels of a setof transitions V .
Then, the correct use derived fromthe definition of the backoff transitions in the jointmodel is as follows.
At a given state s of G?
and fora given input halfphone a, the outgoing transitionswith input a are the transitions V of s with inputlabel a, and for each b 6?
O(V ), the transition of thefirst backoff state of s with input label a and outputb.For the purpose of our unit selection system, wehad to resort to an approximation.
This is because ingeneral, the backoff use just outlined leads to exam-ining, for a given halfphone, the set of all units pos-sible at each state, which is typically quite large.2Instead, we restricted the inspection of the backoffstates in the following way within the compositionp ?
G?.
A state s1 in p corresponds in the composedtransducer p ?
G?
to a set of states (s1, s2), s2 ?
S2,where S2 is a subset of the states of G?.
Whencomputing the outgoing transitions of the states in(s1, s2) with input label a, the backoff transitions ofa state s2 are inspected if and only if none of thestates in S2 has an outgoing transition with input la-bel a.1This corresponds to the conditional probability P (u|x) =P (x, u)/P (x).2Note that more generally the vocabulary size of our statis-tical language models, about 400,000, is quite large comparedto the usual word-based models.4.3 Language Model Transducer ?
ShrinkingA classical algorithm for reducing the size of ann-gram language model is shrinking using theentropy-based method of (Stolcke, 1998) or theweighted difference method (Seymore and Rosen-feld, 1996), both quite similar in practice.
In ourexperiments, we used a modified version of theweighted difference method.
Let w be a unit andlet h be its conditioning history within the n-grammodel.
For a given shrink factor ?, the transitioncorresponding to the n-gram hw is removed fromthe weighted automaton if:log(P?
(w|h)) ?
log(?hP?
(w|h?))
?
?c(hw) (10)where h?
is the backoff sequence associated with h.Thus, a higher-order n-gram hw is pruned whenit does not provide a probability estimate signifi-cantly different from the corresponding lower-ordern-gram sequence h?w.This standard shrinking method needs to be mod-ified to be used in the case of our halfphone-to-unitweighted transducer model with the restriction onthe traversal of the backoff transitions described inthe previous section.
The shrinking methods musttake into account all the transitions sharing the sameinput label at the state identified with h and its back-off state h?.
Thus, at each state identified with h inG?, a transition with input label x is pruned when thefollowing condition holds:?w?Xxhlog(P?
(w|h)) ??w?Xxh?log(?hP?
(w|h?))
?
?c(hw)where h?
is the backoff sequence associate with hand Xxk is the set of output labels of all the outgoingtransitions with input label x of the state identifiedwith k.5 Experimental resultsWe used the AT&T Natural Voices Product speechsynthesis system to synthesize 107,987 AP news ar-ticles, generating a large corpus of 8,731,662 unitsequences representing a total of 415,227,388 units.We used this corpus to build several n-gram Katzbackoff language models with n = 2 or 3.
Ta-ble 1 gives the size of the resulting language modelweighted automata.
These language models werebuilt using the GRM Library (Allauzen et al, 2004).We evaluated these models by using them to syn-thesize an AP news article of 1,000 words, corre-sponding to 8250 units or 6 minutes of synthesizedspeech.
Table 2 gives the unit selection time (in sec-onds) taken by our new system to synthesize this APModel No.
of states No.
of transitions2-gram, unshrunken 293,935 5,003,3363-gram, unshrunken 4,709,404 19,027,2443-gram, ?
= ?4 2,967,472 14,223,2843-gram, ?
= ?1 2,060,031 12,133,9653-gram, ?
= 0 1,681,233 10,217,1643-gram, ?
= 1 1,370,220 9,146,7973-gram, ?
= 4 934,914 7,844,250Table 1: Size of the stochastic language models fordifferent n-gram order and shrinking factor.Model composition search total timebaseline system - - 4.5s2-gram, unshrunken 2.9s 1.0s 3.9s3-gram, unshrunken 1.2s 0.5s 1.7s3-gram, ?
= ?4 1.3s 0.5s 1.8s3-gram, ?
= ?1 1.5s 0.5s 2.0s3-gram, ?
= 0 1.7s 0.5s 2.2s3-gram, ?
= 1 2.1s 0.6s 2.7s3-gram, ?
= 4 2.7s 0.9s 3.6sTable 2: Computation time for each unit selectionsystem when used to synthesize the same AP newsarticle.news article.
Experiments were run on a 1GHz Pen-tium III processor with 256KB of cache and 2GB ofmemory.
The baseline system mentioned in this ta-ble is the AT&T Natural Voices Product which wasalso used to generate our training corpus using theconcatenation cost caching method from (Beutnagelet al, 1999b).
For the new system, both the compu-tation times due to composition and to the searchare displayed.
Note that the AT&T Natural VoicesProduct system was highly optimized for speed.
Inour new systems, the standard research software li-braries already mentioned were used.
The searchwas performed using the standard speech recog-nition Viterbi decoder from the DCD library (Al-lauzen et al, 2003).
With a trigram language model,our new statistical unit selection system was about2.6 times faster than the baseline system.A formal test using the standard mean of opinionscore (MOS) was used to compare the quality of thehigh-quality AT&T Natural Voices Product synthe-sizer and that of the synthesizers based on our newunit selection system with shrunken and unshrunkentrigram language models.
In such tests, several lis-teners are asked to rank the quality of each utterancefrom 1 (worst score) to 5 (best).
The MOS results ofthe three systems with 60 utterances tested by 21 lis-teners are reported in Table 3 with their correspond-Model raw score normalized scorebaseline system 3.54?
.20 3.09?
.223-gram, unshrunken 3.45?
.20 2.98?
.213-gram, ?
= ?1 3.40?
.20 2.93?
.22Table 3: Quality testing results: we report for eachsystem, the mean and standard error of the raw andthe listener-normalized scores.ing standard error.
The difference of scores betweenthe three systems is not statistically significant (firstcolumn), in particular, the absolute difference be-tween the two best systems is less than .1.Different listeners may rank utterances in dif-ferent ways.
Some may choose the full range ofscores (1?5) to rank each utterance, others may se-lect a smaller range near 5, near 3, or some otherrange.
To factor out such possible discrepancies inranking, we also computed the listener-normalizedscores (second column of the table).
This was donefor each listener by removing the average score overthe full set of utterances, dividing it by the stan-dard deviation, and by centering it around 3.
Theresults show that the difference between the normal-ized scores of the three systems is not significantlydifferent.
Thus, the MOS results show that the threesystems have the same quality.We also measured the similarity of the two bestsystems by comparing the number of common unitsthey produce for each utterance.
On the AP news ar-ticle already mentioned, more than 75% of the unitswere common.6 ConclusionWe introduced a statistical modeling approach tounit selection in speech synthesis.
This approach islikely to lead to more accurate unit selection sys-tems based on principled learning algorithms andtechniques that radically depart from the heuristicmethods used in the traditional systems.
Our pre-liminary experiments using a training corpus gener-ated by the AT&T Natural Voices Product demon-strates that statistical modeling techniques can beused to build a high-quality unit selection system.It also shows other important benefits of this ap-proach: a substantial increase of efficiency and agreater modularity and flexibility.AcknowledgmentsWe thank Mark Beutnagel for helping us clarifysome of the details of the unit selection system inthe AT&T Natural Voices Product speech synthe-sizer.
Mark also generated the training corpora andset up the listening test used in our experiments.We also acknowledge discussions with Brian Roarkabout various statistical language modeling topicsin the context of unit selection.ReferencesCyril Allauzen, Mehryar Mohri, and MichaelRiley.
2003.
DCD Library - Decoder Li-brary, software collection for decoding and re-lated functions.
In AT&T Labs - Research.http://www.research.att.com/sw/tools/dcd.Cyril Allauzen, Mehryar Mohri, and BrianRoark.
2004.
A General Weighted Gram-mar Library.
In Proceedings of the NinthInternational Conference on Automata (CIAA2004), Kingston, Ontario, Canada, July.http://www.research.att.com/sw/tools/grm.Jean Berstel.
1979.
Transductions and Context-Free Languages.
Teubner Studienbucher:Stuttgart.Mark Beutnagel, Alistair Conkie, JuergenSchroeter, and Yannis Stylianou.
1999a.The AT&T Next-Gen system.
In Proceedings ofthe Joint Meeting of ASA, EAA and DAGA, pages18?24, Berlin, Germany.Mark Beutnagel, Mehryar Mohri, and Michael Ri-ley.
1999b.
Rapid unit selection from a largespeech corpus for concatenative speech synthesis.In Proceedings of Eurospeech, volume 2, pages607?610.Ivan Bulyko and Mari Ostendorf.
2001.
Unit selec-tion for speech synthesis using splicing costs withweighted finite-state trasnducers.
In Proceedingsof Eurospeech, volume 2, pages 987?990.Alistair Conkie, Mark Beutnagel, Ann Syrdal, andPhilip Brown.
2000.
Preselection of candidateunits in a unit selection-based text-to-speech syn-thesis system.
In Proceedings of ICSLP, vol-ume 3, pages 314?317.Samuel Eilenberg.
1974.
Automata, Languagesand Machines, volume A.
Academic Press.Andrew Hunt and Alan Black.
1996.
Unit selec-tion in a concatenative speech synthesis system.In Proceedings of ICASSP?96, volume 1, pages373?376, Atlanta, GA.Frederick Jelinek.
1976.
Continuous speech recog-nition by statistical methods.
IEEE Proceedings,64(4):532?556.Slava M. Katz.
1987.
Estimation of probabilitiesfrom sparse data for the language model com-ponent of a speech recogniser.
IEEE Transac-tions on Acoustic, Speech, and Signal Processing,35(3):400?401.Werner Kuich and Arto Salomaa.
1986.
Semir-ings, Automata, Languages.
Number 5 in EATCSMonographs on Theoretical Computer Science.Springer-Verlag, Berlin, Germany.Mehryar Mohri, Fernando C. N. Pereira, andMichael Riley.
1996.
Weighted automata in textand speech processing.
In Proceedings of the12th European Conference on Artificial Intelli-gence (ECAI 1996), Workshop on Extended fi-nite state models of language, Budapest, Hun-gary.
John Wiley and Sons, Chichester.Mehryar Mohri, Fernando C. N. Pereira, andMichael Riley.
2000.
The Design Principlesof a Weighted Finite-State Transducer Library.Theoretical Computer Science, 231(1):17?32.http://www.research.att.com/sw/tools/fsm.Mehryar Mohri.
2002.
Semiring Frameworksand Algorithms for Shortest-Distance Problems.Journal of Automata, Languages and Combina-torics, 7(3):321?350.Mehryar Mohri.
2004.
Weighted Finite-StateTransducer Algorithms: An Overview.
In Car-los Mart?
?n-Vide, Victor Mitrana, and GheorghePaun, editors, Formal Languages and Applica-tions, volume 148, VIII, 620 p. Springer, Berlin.Eric Moulines and Francis Charpentier.
1990.Pitch-synchronous waveform processing tech-niques for text-to-speech synthesis using di-phones.
Speech Communication, 9(5-6):453?467.Fernando C. N. Pereira and Michael D. Riley.
1997.Speech Recognition by Composition of WeightedFinite Automata.
In Finite-State Language Pro-cessing, pages 431?453.
MIT Press.Arto Salomaa and Matti Soittola.
1978.
Automata-Theoretic Aspects of Formal Power Series.Springer-Verlag: New York.Kristie Seymore and Ronald Rosenfeld.
1996.Scalable backoff language models.
In Pro-ceedings of ICSLP, volume 1, pages 232?235,Philadelphia, Pennsylvania.Andreas Stolcke.
1998.
Entropy-based pruningof backoff language models.
In Proc.
DARPABroadcast News Transcription and Understand-ing Workshop, pages 270?274.Yannis Stylianou, Thierry Dutoit, and JuergenSchroeter.
1997.
Diphone conactenation usinga harmonic plus noise model of speech.
In Pro-ceedings of Eurospeech.Jon Yi, James Glass, and Lee Hetherington.
2000.A flexible scalable finite-state transducer archi-tecture for corpus-based concatenative speechsynthesis.
In Proceedings of ICSLP, volume 3,pages 322?325.
