Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 465?473,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsTermWeighting Schemes for Latent Dirichlet AllocationAndrew T. WilsonSandia National LaboratoriesPO Box 5800, MS 1323Albuquerque, NM 87185-1323, USAatwilso@sandia.govPeter A. ChewMoss Adams LLP6100 Uptown Blvd.
NE, Suite 400Albuquerque, NM 87110-4489, USAPeter.Chew@MossAdams.comAbstractMany implementations of Latent Dirichlet Al-location (LDA), including those described inBlei et al (2003), rely at some point on theremoval of stopwords, words which are as-sumed to contribute little to the meaning ofthe text.
This step is considered necessary be-cause otherwise high-frequency words tend toend up scattered across many of the latent top-ics without much rhyme or reason.
We show,however, that the ?problem?
of high-frequencywords can be dealt with more elegantly, andin a way that to our knowledge has not beenconsidered in LDA, through the use of appro-priate weighting schemes comparable to thosesometimes used in Latent Semantic Indexing(LSI).
Our proposed weighting methods notonly make theoretical sense, but can also beshown to improve precision significantly on anon-trivial cross-language retrieval task.1 IntroductionLatent Dirichlet Allocation (LDA) (Blei et al, 2003),like its more established competitors Latent Seman-tic Indexing (LSI) (Deerwester et al, 1990) andProbabilistic Latent Semantic Indexing (PLSI) (Hof-mann, 1999), is a model which is applicable to theanalysis of text corpora.
It is claimed to differ fromLSI in that LDA is a generative Bayesianmodel (Bleiet al, 2003), although this may depend upon themanner in which one approaches LSI (see for exam-ple Chew et al (2010)).
In LDA as applied to textanalysis, each document in the corpus is modeled asa mixture over an underlying set of topics, and eachtopic is modeled as a probability distribution over theterms in the vocabulary.As the newest among the above-mentioned tech-niques, LDA is still in a relatively early stage of de-velopment.
It is also sufficiently different from LSI,probably themost popular andwell-known compres-sion technique for information retrieval (IR), thatmany practitioners of LSI may perceive a ?barrier toentry?
to LDA.
This in turn perhaps explains why no-tions such as term weighting, which have been com-monplace in LSI for some time (Dumais, 1991), havenot yet found a place in LDA.
In fact, it is often as-sumed that weighting is unnecessary in LDA.
Forexample, Blei et al (2003) contrast the use of tf-idf weighting in both non-reduced space (Salton andMcGill, 1983) and LSI on the one hand with PLSIand LDA on the other, where no mention is made ofweighting.
Ramage et al (2008) propose a simpleterm-frequency weighting scheme for tagged docu-ments within the framework of LDA, although termweighting is not their focus and their scheme is in-tended to incorporate document tags into the samemodel that represents the documents themselves.In this paper, we produce evidence that termweighting should be given consideration withinLDA.
First and foremost, this is shown empiri-cally through a non-trivial multilingual retrieval taskwhich has previously been used as the basis fortests of variants of LSI.
We also show that termweighting allows one to avoid maintenance of stop-lists, which can be awkward especially for multilin-gual data.
With appropriate term weighting, high-frequency words (which might otherwise be elimi-nated as stopwords) are assigned naturally to topics465by LDA, rather than dominating and being scatteredacross many topics as happens with the standard uni-form weighting.
Our approach belies the usuallyunstated, but widespread, assumption in papers onLDA that the removal of stopwords is a necessarypre-processing step (see e.g.
Blei et al (2003); Grif-fiths and Steyvers (2004)).It might seem that to demonstrate this it would benecessary to perform a test that directly compares theresults when stoplists are used to those when weight-ing are used.
However, we believe that stopwordsare highly ad-hoc to begin with.
Assuming a vocab-ulary of n words and a stoplist of x items, there are(at least in theory)(nx)possible stoplists.
To be surethat no stoplist improves on a particular termweight-ing scheme we would have to test every one of these.In addition, our tests are with a multilingual dataset,which raises the issue that a domain-appropriate sto-plist for a particular corpus and language may not beavailable.
This is even more true if we pre-processthe dataset morphologically (for example, with stem-ming).
Therefore, rather than attempting a directcomparison of this type, we take the position that itis possible to sidestep the need for stoplists and to doso in a non-ad-hoc way.The paper is organized as follows.
Section 2 de-scribes the general framework of LDA, which hasonly very recently been applied to cross-languageIR.
In Section 3, we look at alternatives to the?standard?
uniform weighting scheme (i.e., lack ofweighting scheme) commonly used in LDA.
Sec-tion 4 discusses the framework we use for empiri-cal testing of our hypothesis that a weighting schemewould be beneficial.
We present the results of thiscomparison in Section 5 along with an impressionis-tic comparison of the output of the different alterna-tives.
We conclude in Section 6.2 Latent Dirichlet AllocationOur IR framework is multilingual Latent Dirich-let Allocation (LDA), first proposed by Blei et al(2003) as a general Bayesian framework with initialapplication to topicmodeling.
It is only very recentlythat variants of LDA have been applied to cross-language IR: examples are Cimiano et al (2009) andNi et al (2009).As an approach to topic modeling, LDA relies onthe idea that the tokens in a document are drawn in-dependently from a set of topics where each topic isa distribution over types (words) in the vocabulary.The mixing coefficients for topics within each docu-ment and weights for types in each topic can be spec-ified a priori or learned from a training corpus.
Bleiet al initially proposed a variational model (2003)for learning topics from data.
Griffiths and Steyvers(2004) later developed a Markov chain Monte Carloapproach based on collapsed Gibbs sampling.In this model, the mixing weights for topics withineach document and the multinomial coefficients forterms within each topic are hidden (latent) and mustbe learned from a training corpus.
Blei et al (2003)proposed LDA as a general Bayesian framework andgave a variational model for learning topics fromdata.
Griffiths and Steyvers (2004) subsequently de-veloped a stochastic learning algorithm based on col-lapsed Gibbs sampling.
In this paper we will focuson the Gibbs sampling approach.2.1 Generative Document ModelThe LDA algorithm models the D documents in acorpus as mixtures of K topics where each topic isin turn a distribution over W terms.
Given ?, thematrix of mixing weights for topics within each doc-ument, and?, the matrix of multinomial coefficientsfor each topic, we can use this formulation to de-scribe a generative model for documents (Alg.
1).Restating the LDA model in linear-algebraicterms, we can say that the product of ?
(theK ?Wcolumn-stochastic topic-by-type matrix) and ?
(theD ?
K column-stochastic topic-by-document ma-trix) is the originalD?W term-by-documentmatrix.In this sense, LDA computes a matrix factorizationof the term-by-document matrix in the sameway thatLSI or non-negative matrix factorization (NMF) do.In fact, LDA is a special case of NMF, but unlike inNMF, there is a unique factorization in LDA.
We seethis as a feature recommending LDA above NMF.Our objective is to reverse the generative model tolearn the contents of ?
and ?
given a training corpusD, a number of topics K, and symmetric Dirichletprior distributions over both ?
and ?
with hyperpa-rameters ?
and ?, respectively.466for k = 1 toK doDraw ?k ?
Dirichlet(?
)end forfor d = 1 to D doDraw ?
?
Dirichlet(?
)Draw N ?
Poisson(?
)for i = 1 to N doDraw z ?
Multinomial(?
)Draw w ?
Multinomial(?
(z))end forend forAlgorithm 1: Generative algorithm for LDA.
This willgenerate D documents with N tokens each.
Each tokenis drawn from one of K topics.
The distributions overtopics and terms have Dirichlet hyperparameters ?
and?
respectively.
The Poisson distribution over the tokencount may be replaced with any other convenient distri-bution.2.2 Learning Topics via Collapsed GibbsSamplingRather than learn ?
and ?
directly, we use collapsedGibbs sampling (Geman et al (1993), Chatterji andPachter (2004)) to learn the latent assignment of to-kens to topics z given the observed tokens x.The algorithm operates by repeatedly samplingeach zij from a distribution conditioned on the val-ues of all other elements of z.
This requires main-taining counts of tokens assigned to topics globallyand within each document.
We use the following no-tation for these sums:Nijk: Number of tokens of type wi in document djassigned to topic kN?stijk : The sum Nijk with the contribution of tokenxst excludedWe indicate summation over all values of an indexwith (?
).Given the current state of z the conditional proba-bility of zij is:p(zij = k|z?ij , x, d, ?, ?)
=p(xij |?k) p(k|dj) ?N?iji(?
)k + ?N?ij(?)(?
)k + W?N?ij(?
)jk + ?N(?)j(?)
+ T?
(1)As Griffiths and Steyvers (2004) point out, this isan intuitive result.
The first term, p(xij |?k), indi-cates the importance of term xij in topic k. The sec-ond term, p(k|dj), indicates the importance of topick in document j.
The sum of the terms is normalizedimplicitly to 1 when we draw each new zij .We sample a new value for zij for every token xijduring each iteration of Gibbs sampling.
We run thesampler for a burn-in period of a few hundred itera-tions to allow it to reach its converged state and thenestimate ?
and ?
from z as follows:?jk =N(?
)jk + ?N(?)j(?)
+ T?
(2)?ki =Ni(?
)k + ?N(?)(?
)k + W?
(3)2.3 Classifying New DocumentsIn LSI, new documents not in the original trainingset can be ?projected?
into the semantic space of thetraining set.
The equivalent process in LDA is oneof classification: given a corpus D?
of one or morenew documents we use the existing topics ?
to com-pute a maximum a posteriori estimate of the mixingcoefficients ??.
This follows the same Monte Carloprocess of repeatedly resampling a set of token-to-topic assignments z?
for the tokens x?
in the new doc-uments.
These new tokens are used to compute thefirst term p(k|dj) in Eq.
1.
We re-use the topic as-signments z from the training corpus to compute thesecond term p(xij |?k).
Tokens with new types thatwere not present in the vocabulary of the trainingcorpus do not participate in classification.The resulting distribution ??
essentially encodeshow likely each new document is to relate to each ofthe K topics.
We can use this matrix to computepairwise similarities between any two documentsfrom either corpus (training or newly-classified).Whereas in LSI it may make sense to compute sim-ilarity between documents using the cosine met-ric (since the ?dimensions?
defining the space areorthogonal), we compute similarities in LDA us-ing either the symmetrized Kullback-Leibler (KL)or Jensen-Shannon (JS) divergences (Kullback andLeibler (1951), Lin (2002)) since these are methodsof measuring the similarity between probability dis-tributions.4673 Term Weighting Schemes and LDAThe standard approach presented above assumes, ef-fectively, that each token is equally important in cal-culating the conditional probabilities.
From both aninformation-theoretic and a linguistic point of view,however, it is clear that this is not the case.
In En-glish, a term such as ?the?
which occurs with highfrequency in many documents does not contribute asmuch to the meaning of each document as a lower-frequency term such as ?corpus?.
It is an axiom ofinformation theory that an event a?s information con-tent (in bits) is equal to log2 1p(a) = ?
log2 p(a).Treating tokens as events, we can say that the in-formation content of a particular token of type t is?
log2 p(t).
Furthermore, as is well-known, we canestimate p(t) from observed frequencies in a corpus:it is simply the number of tokens of type t in the cor-pus, divided by the total number of tokens in the cor-pus.
For high-probability terms such as ?the?, there-fore, ?
log2 p(t) is low.
Our basic hypothesis is thatrecalculating p(zij |z, x, ?, ?)
to take the informationcontent of each token into account will improve theresults of LDA.
Specifically, we have incorporateda weighting term into Eq.
1 by replacing the countsdenoted N with weights denotedM .p(zij = k|z?ij , x, d, ?, ?)
?M?iji(?
)k + ?M?ij(?)(?
)k + W?M?ij(?
)jk + ?M(?)j(?)
+ T?
(4)Here Mijk is the total weight of tokens of type iin document j assigned to topic k instead of the totalnumber of tokens.
All of the machinery for Gibbssampling and the estimation of ?
and ?
from z re-mains unchanged.We appeal to an urn model to explain the intuitionbehind this approach.
In the original LDA formula-tion, each topic ?
can be modeled as an urn contain-ing a large number of balls of uniform size.
Eachball assumes one ofW different colors (one color foreach term in the vocabulary).
The frequency of oc-currence of each color in the urn is proportional to thecorresponding term?s weight in topic ?.
We incor-porate a term weighting scheme by making the sizeof each ball proportional to the weight of its corre-sponding term.
This makes the probability of draw-ing the ball for a termw proportional to both the termweightm(w) and its multinomial weight ?w:p(w|?, ?,m) = ?w m(w)?w?W m(w)(5)We can now expand Eq.
4 to obtain a new samplingequation for use with the Gibbs sampler.p(zij = k|z?ij , x,d,m, ?, ?)
=m(xi)N?iji(?
)k + ?
?w m(w)N?ijw(?
)k + W?
?w m(w)N?ijwjk + ?
?w m(w)Nwj(?)
+ T?
(6)If all weightsm(w) = 1 this reduces immediatelyto the standard LDA formulation in Eq.
1.The information measure we describe above isconstant for a particular term across the entire cor-pus, but it is possible to conceive of other, more so-phisticated weighting schemes as well, for examplethose where term weights vary by document.
Point-wise mutual information (PMI) is one such weight-ing scheme which has a solid basis in informationtheory and has been shown to work well in the con-text of LSI (Chew et al, 2010).
According to PMI,the weight of a given term w in a given documentd is the pointwise mutual information of the termand document, or?
log2p(w|d)p(w) .
Extending the LDAmodel to accommodate PMI is straightforward.
Wereplace m(xi) and m(w) in Eq.
4 with m(xi, d) asfollows.m(xi, d) = ?
log2p(xi|d)p(xi)= ?
log2#[tokens of type xi in d]#[tokens of type xi](7)It is possible for PMI of a term within a documentto be negative.
When this happens, we clamp theweight of the offending term to zero in that docu-ment.
In practice, we observe this only with com-mon words (e.g.
?and?, ?in?, ?of?, ?that?, ?the?
and?to?
in English) that are assigned very lowweight ev-erywhere else in the corpus.
This clamping does notnoticeably affect the results.In the next sections, we describe tests which haveenabled us to evaluate empirically which of theseformulations works best in practice.4684 Testing FrameworkIn this paper, we chose to test our hypotheses withthe same cross-language retrieval task used in a num-ber of previous studies of LSI (e.g.
Chew and Abde-lali (2007)).
Briefly, the task is to train an IR modelon one particular multilingual corpus, then deployit on a separate multilingual corpus, using a docu-ment in one language to retrieve related documentsin other languages.
This task is difficult because ofthe size of the datasets involved.
Its usefulness be-comes apparent when we consider the following twouse cases: a humanwishing (1) to use a search engineto retrieve relevant documents in many languages re-gardless of the language in which the query is posed;or (2) to produce a clustering or visualization of doc-uments according to their topics even when the doc-uments are in different languages.The training corpus consists of the text of the Biblein 31,226 parallel chunks, corresponding generallyto verses, in Arabic, English, French, Russian andSpanish.
These data were obtained from the Un-bound Bible project (Biola University (2006)).
Thetest data, obtained from http://www.kuran.gen.tr/, is the text of the Quran in the same 5 languages,in 114 parallel chunks corresponding to suras (chap-ters).
The task, in short, is to use the training datato inform whatever linguistic, semantic, or statisticalmodel is being tested, and then to infer characteris-tics of the test data in such a way that the test docu-ments can automatically be matched with their trans-lations in other languages.
Though the documentscome from a specific domain (scriptural texts), whatis of interest is comparative results using differentweighting schemes, holding the datasets and othersettings constant.
The training and test datasets arelarge enough to allow statistically significant obser-vations to be made, and if a significant difference isobserved between experiments using two settings, itis to be expected that similar basic differences wouldbe observed with any other set of training and testdata.
In any case, it should be noted that the Bibleand Quran were written centuries apart, and in differ-ent original languages; we believe this contributesto a clean separation of training and test data, andmakes for a non-trivial retrieval task.In our framework, a term-by-document matrix isformed from the Bible as a parallel verse-alignedcorpus.
We employed two different approachesto tokenization, one (word-based tokenization) inwhich text was tokenized at every non-word char-acter, and the other (unsupervised morpheme-basedtokenization) in which after word-based tokeniza-tion, a further pre-processing step (based on Gold-smith (2001)) was performed to add extra breaks ateverymorpheme.
It is shown elsewhere (Chew et al,2010) that this step leads to improved performancewith LSI.
In each verse, all languages are concate-nated together, allowing terms (either morphemes orwords) from all languages to be represented in everyverse.
Cross-language homographs such as ?mien?in English and French are treated as distinct termsin our framework.
Thus, if there are L languages,D documents (each of which is translated into eachof the L languages), andW distinct linguistic termsacross all languages, then the term-by-document ma-trix is of dimensionsW byD (notW byD?L); withthe Bible as a training corpus, the actual numbers inour case are 160,345?
31,226.
As described in Sec.2.2, we use this matrix as the input to a collapsedGibbs sampling algorithm to learn the latent assign-ment of tokens in all five languages to language-independent topics, as well as the latent assignmentof language-independent topics to the multilingual(parallel) documents.
In general, we specified, arbi-trarily but consistently across all tests, that the num-ber of topics to be learned should be 200.
Other pa-rameters for the Gibbs sampler held constant werethe number of iterations for burn-in (200) and thenumber of iterations for sampling (1).To evaluate our different approaches to weighting,we use classification as described in Sec.
2.3 to ob-tain, for each document from the Quran test corpus,a probability distribution across the topics learnedfrom the Bible.
While in training we have D multi-lingual documents, in testing we haveD?
?L docu-ments, each in a specific language, for which a distri-bution is computed.
For theQuran data, this amountsto 114 ?
5 = 570 documents.
This is because ourgoal is to match documents with their translationsin other languages using just the probability distri-butions.
For each source-language/target-languagepair L1 and L2, we obtain the similarity of each ofthe 114 documents in L1 to each of the 114 doc-uments in L2.
We found that similarity here isbest computed using the Jensen-Shannon divergence469TokenizationWeighting Scheme Word MorphemeUnweighted 0.505 0.544log p(w|L) 0.616 0.641PMI 0.612 0.686Table 1: Summary of comparison results.
This tableshows the average precision at one document (P1) foreach of the tokenization and weighting schemes we eval-uated.
Detailed results are presented in Table 2.
(Lin, 2002) and so this measure was used in alltests.
Ultimately, the measure of how well a partic-ular method performs is average precision at 1 doc-ument (P1).
Among the various measurements forevaluating the performance of IR systems (Saltonand McGill (1983), van Rijsbergen (1979)), this isa fairly standard measure.
For a particular source-target pair, this is the percentage (out of 114 cases)where a document in L1 is most similar to its matein L2.
With 5 languages, there are 25 source-targetpairs, and we can also calculate average P1 acrossall language pairs.
Here, we average across 114 ?25 (or 2,850) cases.
This is why even small differ-ences in P1 can be statistically significant.5 ResultsFirst, we present a summary of our results in Table 1which clearly demonstrates that it is better in LDA touse some kind of weighting scheme rather than theuniform weights in the standard LDA formulationfrom Eq.
1.
This is true whether tokenization is byword or by morpheme.
All increases from the base-line precision at 1 document (0.505 and 0.544 re-spectively), whether under log or PMIweighting, arehighly significant (p < 10?11).
Furthermore, all in-creases in precision when moving from word-basedto morphology-based tokenization are also highlysignificant (p < 5 ?
10?5 without weighting, p <5?10?3 with log-weighting, and p< 2?10?15 withPMI weighting).
The best result overall, where P1 is0.686, is obtained with morphological tokenizationand PMI weighting (parallel to the results in (Chewet al, 2010) with LSI), and again the difference be-tween this result and its nearest competitor of 0.641is highly significant (p < 3 ?
10?6).
We return tocomment below on lack of an increase in P1 whenmoving from log-weighting to PMI-weighting underword-based tokenization.These results can also be broken out by languagepair, as shown in Table 2.
Here, it is apparent thatArabic, and to a lesser extent Russian, are harder lan-guages in the IR problem at hand.
Our intuition isthat this is connected with the fact that these two lan-guages have a more complex morphological struc-ture: words are formed by a process of agglutination.A consequence of this is that single Arabic and Rus-sian tokens can less frequently be mapped to singletokens in other languages, which appears to ?con-fuse?
LDA (and also, as we have found, LSI).
Thecomplex morphology of Russian and Arabic is alsoreflected in the type-token ratios for each language:in our English Bible, there are 12,335 types (uniquewords) and 789,744 tokens, a type-token ratio of0.0156.
The ratios for French, Spanish, Russian andArabic are 0.0251, 0.0404, 0.0843 and 0.1256 re-spectively.
Though the differences may not be ex-plicable in purely statistical terms (there may be lin-guistic factors at play which cannot be reduced tostatistics), it seems plausible that choosing a subop-timal term-weighting scheme could exacerbate anyintrinsic problems of statistical imbalance.
Consid-ering this, it is interesting to note that the greatestgains, when moving from unweighted LDA to ei-ther form of weighted LDA, are often to be foundwhere Russian and/or Arabic are involved.
This, tous, shows the value of using a multilingual datasetas a testbed for our different formulations of LDA:it allows problems which may not be apparent whenworking with a monolingual dataset to come moreeasily to light.We have mentioned that the best results are withPMI and morphological tokenization, and also thatthere is an increase in precision for many language ofthe pairs when morphological (as opposed to word-based) tokenization is employed.
To us, the resultsleave little doubt that both weighting and morpho-logical tokenization are independently beneficial.
Itappears, though, that morphology and weighting arealso complementary and synergistic strategies forimproving the results of LDA: for example, a subop-timal approach in tokenization may at best place anupper bound on the overall precision achievable, andperhaps at worst undo the benefits of a good weight-ing scheme.
This may explain the one apparentlyanomalous result, which is the lack of an increase in470Original Words Morphological TokenizationEN ES RU AR FR EN ES RU AR FRLDAEN 1.000 0.500 0.447 0.132 0.816 1.000 0.500 0.658 0.211 0.640 ENES 0.649 1.000 0.307 0.175 0.781 0.605 1.000 0.482 0.175 0.737 ESRU 0.430 0.316 1.000 0.149 0.430 0.553 0.421 1.000 0.272 0.553 RUAR 0.070 0.149 0.114 1.000 0.096 0.123 0.105 0.228 1.000 0.114 ARFR 0.781 0.693 0.421 0.175 1.000 0.693 0.640 0.667 0.211 1.000 FRLog-WLDAEN 1.000 0.518 0.518 0.228 0.658 1.000 0.675 0.561 0.219 0.754 ENES 0.558 1.000 0.605 0.254 0.763 0.711 1.000 0.570 0.289 0.860 ESRU 0.605 0.615 1.000 0.298 0.702 0.684 0.667 1.000 0.289 0.728 RUAR 0.404 0.430 0.526 1.000 0.439 0.430 0.439 0.535 1.000 0.404 ARFR 0.667 0.667 0.658 0.281 1.000 0.711 0.667 0.561 0.289 1.000 FRPMI-WLDAEN 1.000 0.579 0.658 0.272 0.702 1.000 0.719 0.658 0.342 0.851 ENES 0.596 1.000 0.623 0.246 0.693 0.816 1.000 0.675 0.272 0.798 ESRU 0.649 0.579 1.000 0.307 0.693 0.702 0.693 1.000 0.360 0.772 RUAR 0.351 0.368 0.421 1.000 0.351 0.456 0.474 0.509 1.000 0.377 ARFR 0.693 0.667 0.605 0.254 1.000 0.825 0.772 0.719 0.333 1.000 FRTable 2: Full results for precision at one document for all combinations of LDA, Log-WLDA, PMI-WLDA, wordtokenization and morphological tokenization.precision moving from log-WLDA to PMI-WLDAunder word-based tokenization: if word-based tok-enization is suboptimal, PMI weighting cannot com-pensate for that.
Effectively, for best results, theright strategies have to be pursued with respect bothto morphology and to weighting.Finally, we can illustrate the differences betweenweighted and unweighted LDA in another way.
Asdiscussed earlier, each topic in LDA is a probabil-ity distribution over terms.
For each topic, we canlist the most probable terms in decreasing order ofprobability; this gives a sense of what each topicis ?about?
and whether the groupings of terms ap-pear reasonable.
Since we use 200 topics, an ex-haustive listing is impractical here, but in Table 3we present some representative examples from un-weighted LDA and PMI-WLDA that we judged tobe of interest.
It appears to us that the groupings arenot perfect under either LDA or PMI-WLDA; underboth methods, we find examples of rather heteroge-neous topics, whereas we would like each topic to besemantically focused.
Still, a comparison of the out-put with LDA and PMI-WLDA sheds some light onwhy PMI-WLDA makes it less necessary to removestopwords.
Note that all words listed for the top twotopics under LDA would commonly be consideredstopwords.
This might also be true of the words intopic 1 for PMI-WLDA, but in the latter case, thetopic is actually one of themost semantically focusedin that the top words have a clear semantic connec-tion to one another.
This cannot be said of topics 1and 2 in LDA.
For one thing, many of the same termsthat appear in topic 1 reappear in topic 2, making thetwo topics hard to distinguish from one another.
Sec-ondly, the terms have only a loose semantic connec-tion to one another: ?the?, ?and?, and ?of?
are all high-frequency and likely to co-occur, but they are differ-ent parts of speech and have very different functionsin English.
One might say that topics 1 and 2 in LDAare a rag-bag of high-frequency words, and it is un-surprising that these topics do little to help charac-terize documents in our cross-language IR task.
Thesame cannot be said of any of the top 5 topics in PMI-WLDA.We believe this illustrates well, and at a fun-damental level, why weighted forms of LDA workbetter in practice than unweighted LDA.6 ConclusionWe have conducted a series of experiments to evalu-ate the effect of different weighting schemes on La-tent Dirichlet Allocation.
Our results demonstrate,perhaps contrary to the conventional wisdom thatweighting is unnecessary in LDA, that weightingschemes (and other pre-processing strategies) simi-471Weighting SchemeLDA (no weighting) PMI-WLDATopic 1 2 3 4 5 1 2 3 4 5Termsthe the vanit?
as c?rcel under city coeur sat col?reet de vanidad comme prison sous ville heart assis iraand et vanity como ?????
???
ciudad coraz?n vent wrathlos of ????
???
prison ???
??????
??????
wind anger?
and ?????
un ???????
debajo ?????
??????
viento furory y aflicci?n a prisonniers ombre twelve ????
sentado ???
?les de poursuite one ???????
bases douze ???
?????
fureur?
?
??????
???
bound basas doce ????
?????
??
?de la pr?dicateur une prisi?n sombra ????
????
sitting ????
?of la ????
????
prisoners dessous ??????
???????
???
contreTable 3: Top 10 terms within top 5 topics for each of LDA and PMI-WLDA.
Terms that appear twice within the sametopic (e.g.
?la?
in LDA topic 2) are words from different languages with the same spelling (here Spanish and French).lar to those commonly employed in other approachesto IR (such as LSI) can significantly improve theperformance of a system.
Our approach also runscounter to the standard position in LDA that it isnecessary or desirable to remove stopwords as a pre-processing step, and we have presented an alterna-tive approach of applying an appropriate weightingscheme within LDA.
This approach is preferable be-cause it is considerably less ad-hoc than the construc-tion of stoplists.
We have shown mathematicallyhow alternative weighting schemes can be incorpo-rated into the Gibbs sampling model.
We have alsodemonstrated that, far from being arbitrary, the in-troduction of weighting into the LDA model has asolid and rational basis in information and probabil-ity theory, just as the basic LDA model itself has.In future work, we would like to explore furtherenhancements to weighting in LDA.
There are manyvariants which can be considered: one example isthe incorporation of word order and context throughan n-gram model based on conditional probabilities.We also aim to evaluate LDA against LSIwith a viewto establishingwhether one can be said to outperformthe other consistently in terms of precision, with ap-propriate settings held constant.
Finally, we wouldlike to determine whether other techniques whichhave been shown to benefit LSI can also be usefullybrought to bear in LDA, just as we have shown herein the case of term weighting.ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of MachineLearning Research 3, pages 993?1022.Sourav Chatterji and Lior Pachter.
2004.
Multiple Or-ganism Gene Finding by Collapsed Gibbs Sampling.In RECOMB ?04: Proceedings of the eighth annual in-ternational conference on Research in computationalmolecular biology, pages 187?193, New York, NY,USA.
ACM.Peter A. Chew and Ahmed Abdelali.
2007.
Bene-fits of the ?Massively Parallel Rosetta Stone?
: Cross-Language Information Retrieval with Over 30 Lan-guages.
In Association for Computational Linguistics,editor, Proceedings of the 45th meeting of the Associ-ation of Computational Linguistics, pages 872?879.Peter A. Chew, Brett W. Bader, Stephen Helmreich,Ahmed Abdelali, and Stephen J. Verzi.
2010.An Information-Theoretic, Vector-Space-Model Ap-proach to Cross-Language Information Retrieval.Journal of Natural Language Engineering.
Forthcom-ing.Philipp Cimiano, Antje Schultz, Sergej Sizov, PhilippSorg, and Steffen Staab.
2009.
Explicit VersusLatent Concept Models for Cross-Language Informa-tion Retrieval.
In Proceedings of the 21st Inter-national Joint Conference on Artificial Intelligence,pages 1513?1518.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by Latent Semantic Analysis.
Jour-nal of the American Society of Information Science,41(6):391?407.Susan T. Dumais.
1991.
Improving the Retrieval of In-formation from External Sources.
Behavior ResearchMethods, Instruments and Computers, 23(2):229?236.472Stuart Geman, Donald Geman, K. Abend, T. J. Harley,and L. N. Kanal.
1993.
Stochastic Relaxation, GibbsDistributions and the Bayesian Restoration of Images*.Journal of Applied Statistics, 20(5):25?62.J.
Goldsmith.
2001.
Unsupervised Learning of the Mor-phology of a Natural Language.
Computational Lin-guistics, 27(2):153?198.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing Scientific Topics.
In Proceedings of the Na-tional Academy of Sciences USA, volume 101, pages5228?5235.Thomas Hofmann.
1999.
Probablistic Latent SemanticIndexing.
In Proceedings of the 22nd Annual Interna-tional SIGIR Conference, pages 53?57.Solomon Kullback and Richard A. Leibler.
1951.
OnInformation and Sufficiency.
Annals of MathematicalStatistics, 22:49?86.J.
Lin.
2002.
DivergenceMeasures based on the ShannonEntropy.
IEEE Transactions on Information Theory,37(1):145?151, August.Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.2009.
Mining Multilingual Topics from Wikipedia.
In18th International World Wide Web Conference, pages1155?1155, April.Daniel Ramage, Paul Heymann, Christopher D. Man-ning, and Hector Garcia-Molina.
2008.
Clustering theTagged Web.
In Second ACM International Confer-ence on Web Search and Data Mining (WSDM 2009),November.G.
Salton and M. McGill, editors.
1983.
Introduction toModern Information Retrieval.
McGraw-Hill.Biola University.
2006.
The Unbound Bible.http://www.unboundbible.com.C.J.
van Rijsbergen.
1979.
Information Retrieval.Butterworth-Heinemann.473
