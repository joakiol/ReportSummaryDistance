Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 387?394, Vancouver, October 2005. c?2005 Association for Computational LinguisticsRobust Textual Inference via Graph MatchingAria D. HaghighiDept.
of Computer ScienceStanford UniversityStanford, CAaria42@stanford.eduAndrew Y. NgDept.
of Computer ScienceStanford UniversityStanford, CAang@cs.stanford.eduChristopher D. ManningDept.
of Computer ScienceStanford UniversityStanford, CAmanning@cs.stanford.eduAbstractWe present a system for deciding whethera given sentence can be inferred fromtext.
Each sentence is represented as adirected graph (extracted from a depen-dency parser) in which the nodes repre-sent words or phrases, and the links repre-sent syntactic and semantic relationships.We develop a learned graph matching ap-proach to approximate entailment usingthe amount of the sentence?s semanticcontent which is contained in the text.
Wepresent results on the Recognizing TextualEntailment dataset (Dagan et al, 2005),and show that our approach outperformsBag-Of-Words and TF-IDF models.
In ad-dition, we explore common sources of er-rors in our approach and how to remedythem.1 IntroductionA fundamental stumbling block for several NLP ap-plications is the lack of robust and accurate seman-tic inference.
For instance, question answering sys-tems must be able to recognize, or infer, an answerwhich may be expressed differently from the query.Information extraction systems must also be ablerecognize the variability of equivalent linguistic ex-pressions.
Document summarization systems mustgenerate succinct sentences which express the samecontent as the original document.
In Machine Trans-lation evaluation, we must be able to recognize legit-imate translations which structurally differ from ourreference translation.One sub-task underlying these applications is theability to recognize semantic entailment; whetherone piece of text follows from another.
In contrastto recent work which has successfully utilized logic-based abductive approaches to inference (Moldovanet al, 2003; Raina et al, 2005b), we adopt a graph-based representation of sentences, and use graphmatching approach to measure the semantic over-lap of text.
Graph matching techniques have provento be a useful approach for tractable approximatematching in other domains including computer vi-sion.
In the domain of language, graphs providea natural way to express the dependencies betweenwords and phrases in a sentence.
Furthermore,graph matching also has the advantage of providinga framework for structural matching of phrases thatwould be difficult to resolve at the level of individualwords.2 Task Definition and DataWe describe our approach in the context of the 2005Recognizing Textual Entailment (RTE) Challenge(Dagan et al, 2005), but note that our approach eas-ily extends to other related inference tasks.
The sys-tem presented here was one component of our re-search group?s 2005 RTE submission (Raina et al,2005a) which was the top-ranking system accordingto one of the two evaluation metrics.In the 2005 RTE domain, we are given a set ofpairs, each consisting of two parts: 1) the text, a387SNP-BezosNNPBezosVP-establishedVBDestablishedNP-companyDTaNNcompanyBezos(person)company(organization)establish(VBD)Subj (Agent) Obj (Patient)Figure 1: An example parse tree and the correspond-ing dependency graph.
Each phrase of the parse treeis annotated with its head word, and the parentheti-cal edge labels in the dependency graph correspondto semantic roles.small passage,1 and the hypothesis, a single sen-tence.
Our task is to decide if the hypothesis is ?en-tailed?
by the text.
Here, ?entails?
does not meanstrict logical implication, but roughly means thata competent speaker with basic world-knowledgewould be happy to conclude the hypothesis given thetext.
This criterion has an aspect of relevance logicas opposed to material implication: while variousadditional background information may be neededfor the hypothesis to follow, the text must substan-tially support the hypothesis.Despite the informality of the criterion and thefact that the available world knowledge is leftunspecified, human judges show extremely goodagreement on this task ?
3 human judges indepen-dent of the organizers calculated agreement rateswith the released data set ranging from 91?96% (Da-gan et al, 2005).
We believe that this in part reflectsthat the task is fairly natural to human beings.
Fora flavor of the nature (and difficulty) of the task, seeTable 1.We give results on the data provided for the RTEtask which consists of 567 development pairs and800 test pairs.
In both sets the pairs are divided into7 tasks ?
each containing roughly the same numberof entailed and not-entailed instances ?
which wereused as both motivation and means for obtaining andconstructing the data items.
We will use the follow-ing toy example to illustrate our representation andmatching technique:Text: In 1994, Amazon.com was founded by Jeff Bezos.Hypothesis: Bezos established a company.1Usually a single sentence, but occasionally longer.3 Semantic Representation3.1 The Need for DependenciesPerhaps the most common representation of text forassessing content is ?Bag-Of-Words?
or ?Bag-of-N-Grams?
(Papineni et al, 2002).
However, such rep-resentations lose syntactic information which canbe essential to determining entailment.
Consider aQuestion Answer system searching for an answerto When was Israel established?
A representationwhich did not utilize syntax would probably enthusi-astically return an answer from (the 2005 RTE text):The National Institute for Psychobiology in Israelwas established in 1979.In this example, it?s important to try to match rela-tionships as well as words.
In particular, any answerto the question should preserve the dependency be-tween Israel and established.
However, in the pro-posed answer, the expected dependency is missingalthough all the words are present.Our approach is to view sentences as graphs be-tween words and phrases, where dependency rela-tionships, as in (Lin and Pantel, 2001), are charac-terized by the path between vertices.Given this representation, we judge entailment bymeasuring not only how many of the hypothesis ver-tices are matched to the text but also how well therelationships between vertices in the hypothesis arepreserved in their textual counterparts.
For the re-mainder of the section we outline how we producegraphs from text, and in the next section we intro-duce our graph matching model.3.2 From Text To GraphsStarting with raw English text, we use a version ofthe parser described in (Klein and Manning, 2003),to obtain a parse tree.
Then, we derive a dependencytree representation of the sentence using a slightlymodified version of Collins?
head propagation rules(Collins, 1999), which make main verbs not auxil-iaries the head of sentences.
Edges in the depen-dency graph are labeled by a set of hand-createdtgrep expressions.
These labels represent ?sur-face?
syntax relationships such as subj for subjectand amod for adjective modifier, similar to the rela-tions in Minipar (Lin and Pantel, 2001).
The depen-dency graph is the basis for our graphical represen-tation, but it is enhanced in the following ways:388Task Text Hypothesis EntailedQuestion An-swer (QA)Prince Charles was previously married to PrincessDiana, who died in a car crash in Paris in August1997.Prince Charles and Princess Diana gotmarried in August 1997.FalseMachineTranslation(MT)Sultan Al-Shawi, a.k.a the Attorney, said during afuneral held for the victims, ?They were all chil-dren of Iraq killed during the savage bombing.
?.The Attorney, said at the funeral, ?Theywere all Iraqis killed during the brutalshelling.
?.TrueComparableDocuments(CD)Napster, which started as an unauthorized song-swapping Web site, has transformed into a legalservice offering music downloads for a monthlyfee.Napster illegally offers music down-loads.FalseParaphraseRecognition(PP)Kerry hit Bush hard on his conduct on the war inIraq.Kerry shot Bush.
FalseInformationRetrieval (IR)The country?s largest private employer, Wal-MartStores Inc., is being sued by a number of its femaleemployees who claim they were kept out of jobs inmanagement because they are women.Wal-Mart sued for sexual discrimina-tion.TrueTable 1: Some Textual Entailment examples.
The last three demonstrate some of the harder instances.1.
Collapse Collocations and Named-Entities: We?collapse?
dependency nodes which representnamed entities (e.g., Jeff Bezos in Figure fig-example) and also collocations listed in Word-Net, including verbs and their adjacent particles(e.g., blow off in He blew off his work) .2.
Dependency Folding: As in (Lin and Pan-tel, 2001), we found it useful to fold cer-tain dependencies (such as modifying preposi-tions) so that modifiers became labels connect-ing the modifier?s governor and dependent di-rectly.
For instance, in the text graph in Figure2, we have changed in from a word into a rela-tion between its head verb and the head of itsNP complement.3.
Semantic Role Labeling: We also augmentthe graph representation with Probank-stylesemantic roles via the system described in(Toutanova et al, 2005).
Each predicate addsan arc labeled with the appropriate seman-tic role to the head of the argument phrase.This helps to create links between words whichshare a deep semantic relation not evident inthe surface syntax.
Additionally, modifyingphrases are labeled with their semantic types(e.g., in 1991 is linked by a Temporal edge inthe text graph of Figure 2), which should beuseful in Question Answering tasks.4.
Coreference Links: Using a co-rereference res-olution tagger, coref links are added through-out the graph.
These links allowed connectingthe referent entity to the vertices of the referringvertex.
In the case of multiple sentence texts, itis our only ?link?
in the graph between entitiesin the two sentences.For the remainder of the paper, we will refer tothe text as T and hypothesis as H , and will speakof them in graph terminology.
In addition we willuse HV and HE to denote the vertices and edges,respectively, of H .4 Entailment by Graph MatchingWe take the view that a hypothesis is entailed fromthe text when the cost of matching the hypothesisgraph to the text graph is low.
For the remainder ofthis section, we outline a general model for assign-ing a match cost to graphs.For hypothesis graph H , and text graph T , amatching M is a mapping from the vertices of H tothose of T .
For vertex v in H , we will use M(v) todenote its ?match?
in T .
As is common in statisticalmachine translation, we allow nodes in H to map tofictitious NULL vertices in T if necessary.
Supposethe cost of matching M is Cost(M).
If M is the setof such matchings, we define the cost of matchingH to T to beMatchCost(H,T ) = minM?MCost(M) (1)Suppose we have a model, VertexSub(v,M(v)),which gives us a cost in [0, 1], for substituting ver-tex v in H for M(v) in T .
One natural cost model389is to use the normalized cost for each of the vertexsubstitutions in M :VertexCost(M) = 1Z?v?HVw(v)VertexSub(v,M(v))(2)Here, w(v) represents the weight or relative im-portance for vertex v, and Z = ?v?HV w(v) isa normalization constant.
In our implementation,the weight of each vertex was based on the part-of-speech tag of the word or the type of named entity,if applicable.
However, there are several other pos-sibilities including using TF-IDF weights for wordsand phrases.Notice that when Cost(M) takes the form of(2), computing MatchCost(H,T ) is equivalent tofinding the minimal cost bipartite graph-matching,which can be efficiently computed using linear pro-gramming.We would like our cost-model to incorporatesome measure of how relationships in H are pre-served in T under M .
Ideally, a matching shouldpreserve all local relationships; i.e, if v ?
v?
?
HE ,then M(v) ?
M(v?)
?
TE .
When this conditionholds for all edges in H , H is isomorphic to a sub-graph of T .What we would like is an approximate notion ofisomorphism, where we penalize the distortion ofeach edge relation in H .
Consider an edge e =(v, v?)
?
HE , and let ?M (e) be the path from M(v)to M(v?)
in T .Again, suppose we have a model,PathSub(e, ?M (e)) for assessing the ?cost?
ofsubstituting a direct relation e ?
HE for its coun-terpart, ?M (e), under the matching.
This leads toa formulation similar to (2), where we consider thenormalized cost of substituting each edge relationin H with a path in T :RelationCost(M) = 1Z?e?HEw(e)PathSub(e, ?M (e))(3)where Z = ?e?HE w(e) is a normalization con-stant.
As in the vertex case, we have weightsfor each hypothesis edge, w(e), based upon theedge?s label; typically subject and object relationsare more important to match than others.
Our fi-nal matching cost is given by a convex mixture ofSubj (Agent)establish(VBD)Bezos(person)Company(organization)Obj (Patient)Subj (Agent)found(VBD)Jeff Bezos(person)Amazon.com(organization)Obj (Patient)In (Temporal)1991(date)SynonymMatchCost: 0.4HyponymMatchCost: 0.0ExactMatchCost: 0.0Vertex Cost: (0.0 + 0.2 + 0.4)/3 = 0.2Relation Cost: 0  (Graphs Isomorphic)Match Cost: 0.55 (0.2) + (.45) 0.0 = 0.11Figure 2: Example graph matching (?
= 0.55) forexample pair.
Dashed lines represent optimal match-ing.the vertex and relational match costs: Cost(M) =?VertexCost(M) + (1 ?
?
)RelationCost(M).Notice that minimizing Cost(M) is computa-tionally hard since if our PathSub model as-signs zero cost only for preserving edges, thenRelationCost(M) = 0 if and only if H is isomorphicto a subgraph of T .
Since subgraph isomophism isan NP-complete problem, we cannot hope to have anefficient exact procedure for minimizing the graphmatching cost.
As an approximation, we can ef-ficiently find the matching M?
which minimizesVertexCost(?
); we then perform local greedy hill-climbing search, beginning from M?, to approxi-mate the minimal matching.
The allowed operationsare changing the assignment of any hypothesis ver-tex to a text one, and, to avoid ridges, swapping twohypothesis assignments5 Node and Edge Substitution ModelsIn the previous section we described our graphmatching model in terms of our VertexSub model,which gives a cost for substituting one graph vertexfor another, and PathSub, which gives a cost for sub-stituting the path relationship between two paths inone graph for that in another.
We now outline thesemodels.5.1 Vertex substitution cost modelOur VertexSub(v,M(v)) model is based upon asliding scale, where progressively higher costs are390given based upon the following conditions:?
Exact Match: v and M(v) are identical words/phrases.?
Stem Match: v and M(v)?s stems match or oneis a derivational form of the other; e.g., matchingcoaches to coach.?
Synonym Match: v and M(v) are synonyms ac-cording to WordNet (Fellbaum, 1998).
In particu-lar we use the top 3 senses of both words to deter-mine synsets.?
Hypernym Match: v is a ?kind of?
M(v), asdetermined by WordNet.
Note that this feature isasymmetric.?
WordNet Similarity: v and M(v) are similar ac-cording to WordNet::Similarity (Peder-sen et al, 2004).
In particular, we use the measuredescribed in (Resnik, 1995).
We found it usefulto only use similarities above a fixed threshold toensure precision.?
LSA Match: v and M(v) are distributionallysimilar according to a freely available Latent Se-mantic Indexing package,2 or for verbs similaraccording to VerbOcean (Chklovski and Pantel,2004).?
POS Match: v and M(v) have the same part ofspeech.?
No Match: M(v) is NULL.Although the above conditions often produce rea-sonable matchings between text and hypothesis, wefound the recall of these lexical resources to be farfrom adequate.
More robust lexical resources wouldalmost certainly boost performance.5.2 Path substitution cost modelOur PathSub(v ?
v?,M(v) ?
M(v?))
model isalso based upon a sliding scale cost based upon thefollowing conditions:?
Exact Match: M(v) ?
M(v?)
is an en edge inT with the same label.?
Partial Match: M(v) ?
M(v?)
is an en edge inT , not necessarily with the same label.?
Ancestor Match: M(v) is an ancestor of M(v?
).We use an exponentially increasing cost for longerdistance relationships.2Available at http://infomap.stanford.edu?
Kinked Match: M(v) and M(v?)
share a com-mon parent or ancestor in T .
We use an exponen-tially increasing cost based on the maximum ofthe node?s distances to their least common ances-tor in T .These conditions capture many of the commonways in which relationships between entities are dis-torted in semantically related sentences.
For in-stance, in our system, a partial match will occurwhenever an edge type differs in detail, for instanceuse of the preposition towards in one case and to inthe other.
An ancestor match will occur whenever anindirect relation leads to the insertion of an interven-ing node in the dependency graph, such as matchingJohn is studying French farming vs. John is studyingFrench farming practices.5.3 Learning WeightsIs it possible to learn weights for the relative impor-tance of the conditions in the VertexSub and PathSubmodels?
Consider the case where match costs aregiven only by equation (2) and vertices are weighteduniformly (w(v) = 1).
Suppose that ?
(v,M(v))is a vector of features3 indicating the cost accord-ing to each of the conditions listed for matching vto M(v).
Also let w be weights for each elementof ?(v,M(v)).
First we can model the substitutioncost for a given matching as:VertexSub(v,M(v)) = exp (wT ?
(v,M(v)))1 + exp (wT ?
(v,M(v)))Letting s(?)
be the 1-sigmoid function used in theright hand side of the equation above, our finalmatching cost as a function of w is given byc(H,T ;w) = minM?M1|HV |?v?Hs(wT ?
(v,M(v)))(4)Suppose we have a set of text/hypothesis pairs,{(T (1),H(1)), .
.
.
, (T (n),H(n))}, with labels y(i)which are 1 if H(i) is entailed by T (i) and 0otherwise.
Then we would like to choose w tominimize costs for entailed examples and maximizeit for non-entailed pairs:3In the case of our ?match?
conditions, these features willbe binary.391?
(w) =?i:y(i)=1log c(H(i), T (i);w) +?i:y(i)=0log(1 ?
c(H(i), T (i);w))Unfortunately, ?
(w) is not a convex function.
No-tice that the cost of each matching, M , implicitlydepends on the current setting of the weights w. Itcan be shown that since each c(H,T ;w) involvesminimizing M ?
M, which depends on w, it is notconvex.
Therefore, we can?t hope to globally opti-mize our cost functions over w and must settle foran approximation.One approach is to use coordinate ascent over Mand w. Suppose that we begin with arbitrary weightsand given these weights choose M (i) to minimizeeach c(H(i), T (i);w).
Then we use a relaxed form ofthe cost function where we use the matchings foundin the last step:c?
(H(i), T (i);w) = 1|HV |?v?Hs(wT?
(v,M (i)(v)))Then we maximize w with respect to ?
(w) witheach c(?)
replaced with the cost-function c?(?).
Thisstep involves only logistic regression.
We repeat thisprocedure until our weights converge.To test the effectiveness of the above procedurewe compared performance against baseline settingsusing a random split on the development set.
Pickingeach weight uniformly at random resulted in 53%accuracy.
Setting all weights identically to an arbi-trary value gave 54%.
The procedure above, wherethe weights are initialized to the same value, resultedin an accuracy of 57%.
However, we believe thereis still room for improvement since carefully-handchosen weights results in comparable performanceto the learned weights on the final test set.
We be-lieve this setting of learning under matchings is arather general one and could be beneficial to otherdomains such as Machine Translation.
In the future,we hope to find better approximation techniques forthis problem.6 ChecksOne systematic source of error coming from our ba-sic approach is the implicit assumption of upwardsmonotonicity of entailment; i.e., if T entails H thenadding more words to T should also give us a sen-tence which entails H .
This assumption, also madeby other recent abductive approaches (Moldovan etal., 2003), does not hold for several classes of exam-ples.
Our formalism does not at present provide ageneral solution to this issue, but we include specialcase handling of the most common types of cases,which we outline below.4 These checks are done af-ter graph matching and assume we have stored theminimal cost matching.Negation CheckText: Clinton?s book is not a bestsellerHypothesis: Clinton?s book is a bestsellerTo catch such examples, we check that each hy-pothesis verb is not matched to a text word whichis negated (unless the verb pairs are antonyms) andvice versa.
In this instance, the is in H , denoted byisH , is matched to isT which has a negation modifier,notT , absent for isH .
So the negation check fails.Factive CheckText: Clonaid claims to have cloned 13 babies worldwide.Hypothesis: Clonaid has cloned 13 babies.Non-factive verbs (claim, think, charged, etc.)
incontrast to factive verbs (know, regret, etc.)
havesentential complements which do not represent truepropositions.
We detect such cases, by checking thateach verb in H that is matched in T does not have anon-factive verb for a parent.Superlative CheckText: The Osaka World Trade Center is the tallest building inWestern Japan.Hypothesis: The Osaka World Trade Center is the tallest build-ing in Japan.In general, superlative modifiers (most, biggest,etc.)
invert the typical monotonicity of entailmentand must be handled as special cases.
For anynoun n with a superlative modifier (part-of-speechJJS) in H , we must ensure that all modifier relationsof M(n) are preserved in H .
In this example, build-ingH has a superlative modifier tallestH , so we mustensure that each modifier relation of JapanT , a noun4All the examples are actual, or slightly altered, RTE exam-ples.392Method Accuracy CWSRandom 50.0% 0.500Bag-Of-Words 49.5% 0.548TF-IDF 51.8% 0.560GM-General 56.8% 0.614GM-ByTask 56.7% 0.620Table 2: Accuracy and confidence weighted score(CWS) for test set using various techniques.dependent of buildingT , has a WesternT modifier notin H .
So its fails the superlative check.Additionally, during error analysis on the devel-opment set, we spotted the following cases whereour VertexSub function erroneously labeled verticesas similar, and required special case consideration:?
Antonym Check: We consistently found that theWordNet::Similarity modules gave high-similarity to antonyms.5 We explicitly checkwhether a matching involved antonyms and rejectunless one of the vertices had a negation modifier.?
Numeric Mismatch: Since numeric expressionstypically have the same part-of-speech tag (CD),they were typically matched when exact matchescould not be found.
However, mismatching nu-merical tokens usually indicated that H was notentailed, and so pairs with a numerical mismatchwere rejected.7 Experiments and ResultsFor our experiments we used the devolpement andtest sets from the Recognizing Textual Entailmentchallenge (Dagan et al, 2005).
We give results forour system as well as for the following systems:?
Bag-Of-Words: We tokenize the text and hypoth-esis and strip the function words, and stem the re-sulting words.
The cost is given by the fraction ofthe hypothesis not matched in the text.?
TF-IDF: Similar to Bag-Of-Words except thatthere is a tf.idf weight associated with each hy-pothesis word so that more ?important?
words arehigher weight for matching.5This isn?t necessarily incorrect, but is simply not suitablefor textual inference.Task GM-General GM-ByTaskAccuracy CWS Accuracy CWSCD 72.0% 0.742 76.0% 0.771IE 55.9% 0.583 55.8% 0.595IR 52.2% 0.564 51.1% 0.572MT 50.0% 0.497 43.3% 0.489PP 58.0% 0.741 58.0% 0.746QA 53.8% 0.537 55.4% 0.556RC 52.1% 0.539 52.9% 0.523Table 3: Accuracy and confidence weighted score(CWS) split by task on the RTE test set.We also present results for two graph matching(GM) systems.
The GM-General system fits a sin-gle global threshold from the development set.
TheGM-ByTask system fits a different threshold foreach of the tasks.Our results are summarized in Table 2.
As the re-sult indicates, the task is particularly hard; all RTEparticipants scored between 50% and 60% in termsof overall accuracy (Dagan et al, 2005).
Nevever-theless, both GM systems perform better than eitherBag-Of-Words or TF-IDF.
CWS refers to Confi-dence Weighted Score (also known as average pre-cision).
This measure is perhaps a more insightfulmeasure, since it allows the inclusion of a rankingof answers by confidence and assesses whether youare correct on the pairs that you are most confidentthat you know the answer to.
To assess CWS, ourn answers are sorted in decreasing order by the con-fidence we return, and then for each i, we calculateai, our accuracy on our i most confident predictions.Then CWS = 1n?ni=1 ai.We also present results on a per-task basis in Ta-ble 3.
Interestingly, there is a large variation in per-formance depending on the task.8 ConclusionWe have presented a learned graph matching ap-proach to approximating textual entailment whichoutperforms models which only match at the wordlevel, and is competitive with recent weighed ab-duction models (Moldovan et al, 2003).
In addition,we explore problematic cases of nonmonotonicity inentailment, which are not naturally handled by ei-ther subgraph matching or the so-called ?logic form?393Text Hypothesis True Ans.
Our Ans.
Conf CommentsA Filipino hostage in Iraq was re-leased.A Filipino hostagewas freed in Iraq.True True 0.84 Verb rewrite is handled.Phrasal ordering does notaffect cost.The government announced lastweek that it plans to raise oilprices.Oil prices drop.
False False 0.95 High cost given for substitutingword for its antonym.Shrek 2 rang up $92 million.
Shrek 2 earned $92million.True False 0.59 Collocation ?rang up?
isnot known to be similar to?earned?.Sonia Gandhi can be defeated inthe next elections in India by BJP.Sonia Gandhi is de-feated by BJP.False True 0.77 ?can be?
does not indicate thecomplement event occurs.Fighters loyal to Moqtada al-Sadrshot down a U.S. helicopter Thurs-day in the holy city of Najaf.Fighters loyal toMoqtada al-Sadrshot down Najaf.False True 0.67 Should recognize non-Locationcannot be substituted for Loca-tion.C and D Technologies announcedthat it has closed the acquisition ofDatel, Inc.Datel Acquired Cand D technologies.False True 0.64 Failed to penalize switch in se-mantic role structure enoughTable 4: Analysis of results on some RTE examples along with out guesses and confidence probabilitiesinference of (Moldovan et al, 2003) and have pro-posed a way to capture common cases of this phe-nomenon.
We believe that the methods employedin this work show much potential for improving thestate-of-the-art in computational semantic inference.9 AcknowledgmentsMany thanks to Rajat Raina, Christopher Cox,Kristina Toutanova, Jenny Finkel, Marie-Catherinede Marneffe, and Bill MacCartney for providing uswith linguistic modules and useful discussions.
Thiswork was supported by the Advanced Research andDevelopment Activity (ARDA)?s Advanced Ques-tion Answering for Intelligence (AQUAINT) pro-gram.ReferencesTimothy Chklovski and Patrick Pantel.
2004.
VerbO-cean: Mining the web for fine-grained semantic verbrelations.
In EMNLP.Michael Collins.
1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Universityof Pennsylvania.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The PASCAL recognizing textual entailmentchallenge.
In Proceedings of the PASCAL ChallengesWorkshop Recognizing Textual Entailment.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In ACL, pages 423?430.Dekang Lin and Patrick Pantel.
2001.
DIRT - discoveryof inference rules from text.
In Knowledge Discoveryand Data Mining, pages 323?328.Dan I. Moldovan, Christine Clark, Sanda M. Harabagiu,and Steven J. Maiorano.
2003.
Cogex: A logic proverfor question answering.
In HLT-NAACL.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In ACL.Ted Pedersen, Siddharth Parwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity ?
measuring the relat-edness of concepts.
In AAAI.Rajat Raina, Aria Haghighi, Christopher Cox, JennyFinkel, Jeff Michels, Kristina Toutanova, Bill Mac-Cartney, Marie-Catherine de Marneffe, Christopher D.Manning, and Andrew Y. Ng.
2005a.
Robust textualinference using diverse knowledge sources.
In Pro-ceedings of the First PASCAL Challenges Workshop.Southampton, UK.Rajat Raina, Andrew Y. Ng, and Christopher D. Man-ning.
2005b.
Robust textual inference via learning andabductive reasoning.
In Proceedings of AAAI 2005.AAAI Press.Philip Resnik.
1995.
Using information content to evalu-ate semantic similarity in a taxonomy.
In IJCAI, pages448?453.Kristina Toutanova, Aria Haghighi, and Cristiopher Man-ning.
2005.
Joint learning improves semantic role la-beling.
In Association of Computational Linguistics(ACL).394
