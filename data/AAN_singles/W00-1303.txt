Japanese Dependency Structure AnalysisBased on Support Vector MachinesTaku Kudo  and Yuj i  MatsumotoGraduate School of Information Science,Nara Institute of Science and Technology{taku-ku, matsu}@is, aist-nara, ac.
jp.
.
.
.
.
.
.
- "  rAbst ractThis paper presents a method of Japanesedependency structure analysis based on Sup--port Vector Machines (SVMs).
Conventionalparsing techniques based on Machine Learn-ing framework, such as Decision Trees andMaximum Entropy Models, have difficultyin selecting useful features as well as find-ing appropriate combination of selected fea-tures.
On the other hand, it is well-knownthat SVMs achieve high generalization per-formance ven with input data of very highdimensional feature space.
Furthermore, byintroducing the Kernel principle, SVMs cancarry out the training in high-dimensional?
spaces with a smaller computational cost in-dependent of their dimensionality.
We applySVMs to Japanese dependency structure iden-tification problem.
Experimental results onKyoto University corpus show that our sys-tem achieves the accuracy of 89.09% even withsmall training data (7958 sentences).1 In t roduct ionDependency structure analysis has been rec-ognized as a basic technique in Japanesesentence analysis, and a number of stud-ies have been proposed for years.
Japanesedependency structure is usually defined interms of the relationship between phrasalunits called 'bunsetsu' segments (hereafter"chunks~).
Generally, dependency structureanalysis consists of two steps.
In the firststep, dependency matrix is constructed, inwhich each element corresponds to a pair ofchunks and represents he probability of a de-pendency relation between them.
The secondstep is to find the optimal combination of de-pendencies to form the entire sentence.In previous approaches, these probabilitesof dependencies axe given by manually con-structed rules.
However, rule-based ap-proaches have problems in coverage and con-sistency, since there are a number of featuresthat affect the accuracy of the final results,and these features usually relate to one an-other.On the other hand, as large-scale taggedcorpora have become available these days,a number of statistical parsing techniqueswhich estimate the dependency probabilitiesusing such tagged corpora have been devel-oped(Collins, 1996; Fujio and Matsumoto,1998).
These approaches have overcome thesystems based on the rule-based approaches.Decision Trees(Haruno et al, 1998) and Max-imum Entropy models(Ratnaparkhi, 1997;Uchimoto et al, 1999; Charniak, 2000) havebeen applied to dependency orsyntactic struc-ture analysis.
However, these models requirean appropriate feature selection in order toachieve a high performance.
In addition, ac-quisition of an efficient combination of fea-tures is difficult in these models.In recent years, new statistical earningtechniques such as Support Vector Machines(SVMs) (Cortes and Vapnik, 1995; Vap-nik, 1998) and Boosting(Freund and Schapire,1996) are proposed.
These techniques take astrategy that maximize the margin betweencritical examples and the separating hyper-plane.
In particular, compared with otherconventional statistical earning algorithms,SVMs achieve high generalization even withtraining data of a very high dimension.
Fur-thermore, by optimizing the Kernel function,SVMs can handle non-linear feature spaces,and carry out the training with consideringcombinations of more than one feature.Thanks to such predominant ature, SVMsdeliver state-of-the-art performance in real-world applications such as recognition ofhand-written letters, or of three dimensionalimages.
In the field of natural anguage pro-cessing, SVMs are also applied to text cate-gorization, and are reported to have achieved18high accuracy without falling into over-fittingeven with a large number of words taken as thefeatures (Joachims, 1998; Taira and Haruno,1999).In this paper, we propose an applicationof SVMs to Japanese dependency structureanalysis.
We use the features that have beenstudied in conventional statistical dependencyanalysis with a little modification on them.2 Suppor t  Vector  Mach ines2.1 Opt ima l  Hyperp laneLet us define the training data which belongeither to positive or negative class as follows.
(xl, v l ) , .
.
.
,  (xi, v~),...,  (x~, v~)x iEa  n , y /E{+i , -1}xi is a feature vector of i-th sample, which isrepresented by an n dimensional vector (xi =( f l , .
.
.
, f f , )  E Rn).
Yi is a scalar value thatspecifies the class (positive(+l) or negative(-l) class) of i-th data.
Formally, we can definethe pattern recognition problem as a learningand building process of the decision functionf: lq.
n ~ {=i::l}.In basic SVMs framework, we try to sepa-rate the positive and negative xamples in thetraining data by a linear hyperplane writtenas:(w-x )+b=0 w e Rn, beR.
(1)It is supposed that the farther the positiveand negative xamples are separated by thediscrimination function, the more accuratelywe could separate unseen test examples withhigh generalization performance.
Let us con-sider two hyperplanes called separating hyper-planes:(w-x i )+b_> 1 i f (y i= l )  (2)(w-x i )  + b _< -1  if (Yi =--1) .
(3)(2) (3) can be written in one formula as:w\[(w- x~) + b\] >_ 1 (i = 1, .
.
.
,0 .
(4)Distance from the separating hyperplane tothe point xi can be written as:d(w, b; xi) = Iw" ~ + blIlwllThus, the margin between two separating hy-perplanes can be written as:rain d(w,b ;x i )+ rain d(w,b ;x i )x i  ;Yi = l x i ;y i=- - i= rain Iw 'x i+b I+  rainx,;y,=l Ilwll x,;~,=-i2Ilwll"Iw" xi + blIIwllTo maximize this margin, we should minimizeHwll.
In other words, this problem becomesequivalent to solving the following optimiza-tion problem:Minimize: L(w) = ?l\[wl\[ 2Subject to:  y i \ [ (w-x i )+b\ ]> 1 ( i= l , .
.
.
, l ) .Furthermore, this optimization problem canbe rewritten into the dual form problem: Findthe Lagrange multipliers c~i >_ O(i = 1, .
.
.
,  l)so that:Maximize:l 1 l- ~ a~c~y~y~(xi, xj) (5) L(~) = Z~ _~i=  l ",j:'= 1Subject to:l~ _> 0, ~ ~y~ = 0 (i = 1 , .
.
.
,  l)i=1In this dual form problem, xi with non-zero aiis called a Support Vector.
For the SupportVectors, w and b can thus be expressed asfollowsw = E o~iYi x i  b = w ?
x i  - Yi.i ; x i6SVsThe elements of the set SVs  are the SupportVectors that lie on the separating hyperplanes.Finally, the decision function ff : R n ---r {::El}can be written as:f(x) = sgn(i;x, ~esvs c~iYi (xi - x) + b)(6)= sgn (w-x  + b).2.2 Soft Marg inIn the case where we cannot separate train-ing examples linearly, "Soft Margin" methodforgives ome classification errors that may becaused by some noise in the training examples.First, we introduce non-negative slack vari-ables, and (2),(3) are rewritten as:(w-x i )+b_> 1 -~ i  i f (y i= l )(w.  xi) + b >_ -1  + ~i if (Yi = -1).19In this case, we minimize the following valueinstead of 1 2  llwlll-Ilwll + c (7)i----1The first term in (7) specifies the size of mar-gin and the second term evaluates how far thetraining data are away from the optimal sep-arating hyperplane.
C is the parameter thatdefines the balance of two quantities.
If wemake C larger, the more classification errorsare neglected.Though we omit the details here, minimiza-tion of (7) is reduced to the problem to mini-mize the objective function (5) under the fol-lowing constraints.0 < ai _< c,  a y/= 0 (i = 1 , .
.
.
,  z)Usually, the value of C is estimated experi-mentally.2.3 Kerne l  Funct ionIn general classification problems, there arecases in which it is unable to separate thetraining data linearly.
In such cases, the train-ing data could be separated linearly by ex-panding all combinations of features as newones, and projecting them onto a higher-dimensional space.
However, such a naive ap-proach requires enormous computational over-head.Let us consider the case where we projectthe training data x onto a higher-dimensionalspace by using projection function ?
1 Aswe pay attention to the objective function (5)and the decision function (6), these functionsdepend only on the dot products of the in-put training vectors.
If we could calculate thedot products from xz and x2 directly withoutconsidering the vectors ~(xz) and ?
(x2) pro-jected onto the higher-dimensional space, wecan reduce the computational complexity con-siderably.
Namely, we can reduce the compu-tational overhead if we could find the functionK that satisfies:~(xl)  " ?
(x2) ---- K(Xl~ x2).
(8)On the other hand, since we do not needitself for actual learning and classification,1In general, It(x) is a mapping into Hi lbert space.all we have to do is to prove the existence ofthat satisfies (8) provided the function K isselected properly.
It is known that (8) holds ifand only if the function K satisfies the Mercercondition (Vapnik, 1998).In this way, instead of projecting the train-ing data onto the high-dimensional space, wecan decrease the computational overhead byreplacing the dot products, which is calculatedin optimization and classification steps, withthe function K.Such a function K is called a Kerne l  func-t ion.
Among the many kinds of Kernel func-tions available, we will focus on the d-th poly-nomial kernel:K(x l ,x2)  = (Xl'X2--t-1) a.
(9)Use of d-th polynomial kernel function allowsus to build an optimal separating hyperplanewhich takes into account all combination offeatures up to d.Using a Kernel function, we can rewrite thedecision function as:y = sgn (i;x~ esVs oLiyiK(xi,x)+b).
(10)3 Dependency  Ana lys i s  us ingSVMs3.1 The  Probab i l i ty  Mode lThis section describes a general formulation ofthe probability model and parsing techniquesfor Japanese statistical dependency analysis.First of all, we let a sequence ofchunks be {bz,b2...,bm} by B, andthe sequence dependency pattern be{Dep(1),Dep(2),...,Dep(m - 1)} by D,where Dep(i) = j means that the chunk b~depends on (modifies) the chunk bj.In this framework, we suppose that the de-pendency sequence D satisfies the followingconstraints.1.
Except for the rightmost one, each chunkdepends on (modifies) exactly one of thechunks appearing to the right.2.
Dependencies do not cross each other.Statistical dependency structure analysisis defined as a searching problem for thedependency pattern D that maximizes theconditional probability P(DIB ) of the in-20put sequence under the above-mentioned con-straints.Dbest = argmax P(D\[B)DIf we assume that the dependency probabil-ities are mutually independent, P(DIB ) couldbe rewritten as:rn -1P(DIB) = ~I P (Dep( i )= j  Ifit)i=1fit = { f l , .
.
.
, fn}  e R n.P(Dep(i) = J If0) represents the probabilitythat bi depends on (modifies) bt.
fit is an n di-mensional feature vector that represents var-ious kinds of linguistic features related withthe chunks bi and b t.We obtain Dbest aking into all the combina-tion of these probabilities.
Generally, the op-timal solution Dbest Can be identified by usingbottom-up algorithm such as CYK algorithm.Sekine suggests an efficient parsing techniquefor Japanese sentences that parses from theend of a sentence(Sekine et al, 2000).
We ap-ply Sekine's technique in our experiments...?3.2 Training with SVMsIn order to use SVMs for dependency analysis,we need to prepare positive and negative x-amples ince SVMs is a binary classifier.
Weadopt a simple and effective method for ourpurpose: Out of all combination oftwo chunksin the training data, we take a pair of chunksthat axe in a dependency relation as a positiveexample, and two chunks that appear in a sen-tence but are not in a dependency relation asa negative xample.LJ (f J,y t) = {(f12,y12), (f23,v23),~l_<j<m---, (fro-1 m, Ym-1 m)}fij = {f l , - - - ,  fn )  e R nYij E (Depend(q-l), Not-Depend(-1)}Then, we define the dependency probabilityP ( Dep( i) = j l f'ij ):P(Dep(i) =j I f'ij) =(11)(11) shows that the distance between test datafr O and the separating hyperplane is put intothe sigmoid function, assuming it representsthe probability value of the dependency rela-tion.We adopt this method in our experimentto transform the distance measure obtainedin SVMs into a probability function and an-alyze dependency structure with a fframeworkof conventional probability model 23.3 Stat ic  and Dynamic  FeaturesFeatures that are supposed to be effectivein Japanese dependency analysis are: headwords and their parts-of-speech, particles andinflection forms of the words that appearat the end of chunks, distance between twochunks, existence of punctuation marks.
Asthose are solely defined by the pair of chunks,we refer to them as static features.Japanese dependency relations are heavilyconstrained by such static features ince theinflection forms and postpositional particlesconstrain the dependency relation.
However,when a sentence is long and there are morethan one possible dependents, static features,by themselves cannot determine the correctdependency.
Let us look at the following ex-ample.watashi-ha kono-hon-wo motteim josei-wo sagasiteiruI-top, this book-acc, have, lady-acc, be looking forIn this example, "kono-hon-wo(this book-acc)" may modify either of "motteiru(have)"or "sagasiteiru(be looking for)" and cannotbe determined only with the static features.However, "josei-wo (lady-acc)" can modifythe only the verb "sagasiteiru,".
Knowingsuch information is quite useful for resolv-ing syntactic ambiguity, since two accusativenoun phrses hardly modify the same verb.
Itis possible to use such information if we addnew features related to other modifiers.
Inthe above case, the chunk "sagasiteiru" canreceive a new feature of accusative modifica-tion (by "josei-wo") during the parsing pro-cess, which precludes the chunk "kono-hon-wo" from modifying "sagasiteiru" since thereis a strict constraint about double-accusative2Experimentally, it is shown that tlie sigmoid func-tion gives a good approximation of probability func-tion from the decision function of SVMs(Platt, 1999).21 ?modification that will be learned from train-ing examples.
We decided to take into consid-eration all such modification information byusing functional words or inflection forms ofmodifiers.Using such information about modifiers inthe training phase has no difficulty since theyare clearly available in a tree-bank.
On theother hand, they are not known in the parsingphase of the test data.
This problem can beeasily solved if we adopt a bottom-up arsingalgorithm and attach the modification infor-mation dynamically to the newly constructedphrases (the chlmks that become the head ofthe phrases).
As we describe later we apply abeam search for parsing, and it is possible tokeep several intermediate solutions while sup-pressing the combinatorial explosion.We refer to the features that are added in-crementally during the parsing process as dy-namic  features.4 Exper iments  and  D iscuss ion4.1 Exper iments  Sett ingWe use Kyoto University text corpus (Ver-sion 2.0) consisting of articles of Mainichinewspaper annotated with dependency struc-ture(Kurohashi and Nagao, 1997).
7,958 sen-tences from the articles on January 1st to Jan-uary 7th are used for the training data, and1,246 sentences from the articles on January9th are used for the test data.
For the kernelfunction, we used the polynomial function (9).We set the soft margin parameter C to be 1.The feature set used in the experiments areshown in Table 1.
The static features are ba-sically taken from Uchimoto's list(Uchimotoet al, 1999) with little modification.
In Table1, 'Head' means the rightmost content wordin a chunk whose part-of-speech is not a func-tional category.
'Type' means the rightmostfunctional word or the inflectional form of therightmost predicate if there is no functionalword in the chunk.
The static features in-clude the information on existence of brack-ets, question marks and punctuation marksetc.
Besides, there are features that showthe relative relation of two chunks, such asdistance, and existence of brackets, quotationmarks and punctuation marks between them.For dynamic features, we selected func-tional words or inflection forms of the right-most predicates in the chunks that appear be-tween two chunks and depend on the modi-flee.
Considering data sparseness problem, weStaticFeaturesDynamicFeaturesHead(surface-form, POS,POS-subcategory,inflection-type,inflection-form),TypeLeft/ (surface-form, POS,Right POS-subcategory,Chunks inflection-type,inflection-form),brackets,quotation-marks,punctuation-marks,position in sentence(beginning, end)distance(I,2-5,6-),Between case-particles,Chunks brackets,quotation-marks,punctuation-marksForm of functional words or in-flection that modifies the rightchunkTable 1: Features used in experimentsapply a simple filtering based on the part-of-speech of functional words: We use the lexicalform if the word's POS is particle, adverb, ad-nominal or conjunction.
We use the inflectionform if the word has inflection.
We use thePOS tags for others.4.2 Results of  Exper imentsTable 2 shows the result of parsing accuracyunder the condition k = 5 (beam width), andd = 3 (dimension of the polynomial functionsused for the kernel function).This table shows two types of dependencyaccuracy, A and B.
The training data size ismeasured by the number of sentences.
The ac-curacy A means the accuracy of the entire de-pendency relations.
Since Japanese is a head-final language, the second chunk from the endof a sentence always modifies the last chunk.The accuracy B is calculated by excluding thisdependency relation.
Hereafter, we use the ac-curacy A, if it is not explicitly specified, sincethis measure is usually used in other litera-ture.4.3 Effects of  Dynamic  FeaturesTable3 shows the accuracy when only staticfeatures are used.
Generally, the results with22Trainingdata1172191730324318554067567958Dependency Accuracy86.52%87.21%87.67%88.35%88.66%88.77%89.09%B84.86%85.62%86.14%86.91%87.26%87.38%87.74%SentenceAccuracy39.31%40.06%42.94%44.15%45.20%45.36%46.17%Table 2: Result (d = 3, k = 5)Trainingdata1172191730324318554067567958Dependency AccuracyA86.12%86.81%87.62%88.33%88.40%88.55%88.77%B84.41%85.18%86.10%86.89%86.96%87.13%87.38%SentenceAccuracy38.50%39.80%42.45%44.47%43.66%45.04%45.04%Table 3: Result without dynamic features(d = 3, k = 5)dynamic feature set is better than the resultswithout hem.
The results with dynamic fea-tures constantly outperform that with staticfeatures only.
In most of cases, the improve-ments is significant.
In the experiments, werestrict he features only from the chunks thatappear between two chunks being in consider-ation, however, dynamic features could be alsotaken from the chunks that appear not be-tween the two chunks.
For example, we couldalso take into consideration the chunk that ismodified by the right chunk, or the chunksWm.58,r/~0  ~o0 4c00 $000 ~00 7Oo0Nt~Iber of TriL~img 0,11~a (l~'sl~c~)Figure 1: Training Data vs AccuracyDimension Dependency Sentenceof Kernel Accuracy Accuracy1234N/A86.87%87.67%8'7.72%N/A40.60%42.94%42.78%Table 4: Dimension vs.
Accuracy (3032 sen-tences, k = 5)that modify the left chunk.
We leave experi-ment in such a setting for the future work.4.4 Training data  vs. AccuracyFigure 1 shows the relationship between thesize of the training data and the parsing accu-racy.
This figure shows the accuracy of withand without the dynamic features.The parser achieves 86.52% accuracy fortest data even with small training data (1172sentences).
This is due to a good character-istic of SVMs to cope with the data sparse-ness problem.
Furthermore, it achieves almost100% accuracy for the training data, showingthat the training data are completely sepa-rated by appropriate combination of features.Generally, selecting those specific features ofthe training data tends to cause overfitting,and accuracy for test data may fall.
However,the SVMs method achieve a high accuracy notonly on the training data but also on the testdata.
We claim that this is due to the highgeneralization ability of SVMs.
In addition,observing at the learning curve, further im-provement will be possible if we increase thesize of the training data.4.5 Kernel  Funct ion vs. AccuracyTable 4 shows the relationship between the di-mension of the kernel function and the parsingaccuracy under the condition k -- 5.As a result, the case of d ---- 4 gives the bestaccuracy.
We could not carry out the trainingin realistic time for the case of d = 1.This result supports our intuition that weneed a combination of at least two features.In other words, it will be hard to confirm adependency relation with only the features ofthe modifier or the modfiee.
It is natural thata dependency relation is decided by at leastthe information from both of two chunks.
Inaddition, further improvement has been pos-sible by considering combinations of three ormore features.23 r~ .I,- LBeam Dependency SentenceWidth Accuracy Accuracy1357101588.66%88.74%88.77%88.76%88.67%88.65%45.76%45.20%45.36%45.36%45.28%45.28%Table 5: Beam width vs.
Accuracy (6756 sen-tences, d = 3)4.6 Beam width  vs. AccuracySekine (Sekine et al, 2000) gives an interest-ing report about the relationship between thebeam width and the parsing accuracy.
Gener-ally, high parsing accuracy is expected whena large beam width is employed in the depen-dency structure analysis.
However, the resultis against our intuition.
They report that abeam width between 3 and 10 gives the bestparsing accuracy, and parsing accuracy fallsdown with a width larger than 10.
This resultsuggests that Japanese dependency structuresmay consist of a series of local optimizationprocesses.We evaluate the relationship between thebeam width and the parsing accuracy.
Table 5shows their relationships under the conditiond = 3, along with the changes of the beamwidth from k = 1 to 15.
The best parsingaccuracy is achieved at k ---- 5 and the bestsentence accuracy is achieved at k = 5 andk=7.We have to consider how we should set thebeam width that gives the best parsing accu-racy.
We believe that the beam width thatgives the best parsing accuracy is related notonly with the length of the sentence, but alsowith the lexical entries and parts-of-speechthat comprise the chunks.4.7 Commit tee  based approachInstead of learning a single c lassier  using alltraining data, we can make n classifiers di-viding all training data by n, and the finalresult is decided by their voting.
This ap-proach would reduce computational overhead.The use of multi-processing computer wouldhelp to reduce their training time considerablysince all individual training can be carried outin parallel.To investigate the effectiveness of thismethod, we perform a simple experiment: Di-viding all training data (7958 sentences) by4, the final dependency score is given by aweighted average of each scores.
This simplevoting approach is shown to achieve the ac-curacy of 88.66%, which is nearly the sameaccuracy achieved 5540 training sentences.In this experiment, we simply give an equalweight to each classifier.
However, if we op-timized the voting weight more carefully, thefurther improvements would be achieved (Inuiand Inni, 2000).4.8 Comparison with Related WorkUchimoto (Uchimoto et al, 1999) and Sekine(Sekine et al, 2000) report that using KyotoUniversity Corpus for their training and test-ing, they achieve around 87.2% accuracy bybuilding statistical model based on MaximumEntropy framework.
For the training data, weused exactly the same data that they used inorder to make a fair comparison.
In our ex-periments, the accuracy of 89.09% is achievedusing same training data.
Our model outper-forms Uchimoto's model as far as the accura-cies are compared.Although Uchimoto suggests that the im-portance of considering combination of fea-tures, in ME framework we must expandthese combination by introducing new fea-ture set.
Uchimoto heuristically selects "effec-tive" combination of features.
However, sucha manual selection does not always cover allrelevant combinations that are important inthe determination of dependency relation.We believe that our model is better thanothers from the viewpoints of coverage andconsistency, since our model earns the combi-nation of features without increasing the com-putational complexity.
If we want to recon-sider them, all we have to do is just to changethe Kernel function.
The computational com-plexity depends on the number of support vec-tors not on the dimension of the Kernel func-tion.4.9 Future  WorkThe simplest and most effective way to achievebetter accuracy is to increase the trainingdata.
However, the proposed method thatuses all candidates that form dependency re-lation requires a great amount of time to com-pute the separating hyperplaneas the size ofthe training data increases.
The experimentsgiven in this paper have actually taken long24training time 3To handle large size of training data, wehave to select only the related portion of ex-amples that are effective for the analysis.
Thiswill reduce the training overhead as well asthe analysis time.
The committee-based ap-proach discussed section 4.7 is one method ofcoping with this problem.
For future research,to reduce the computational overhead, we willwork on methods for sample selection as fol-lows:?
Introduction of constraints on non-dependencySome pairs of chunks need not considersince there is no possibility of depen-dency between them from grammaticalconstraints.
Such pairs of chunks are notnecessary to use as negative xamples inthe training phase.
For example, a chunkwithin quotation marks may not modifya chunk that locates outside of the quo-tation marks.
Of course, we have to becareful in introducing such constraints,and they should be learned from existingcorpus.?
Integration with other simple modelsSuppose that a computationally ight andmoderately accuracy learning model isobtainable (there are actually such sys-tems based on probabilistic parsing mod-els).
We can use the system to outputsome redundant parsing results and useonly those results for the positive andnegative xamples.
This is another wayto reduce the size of training data.?
Error-driven data selectionWe can start with a small size of train-ing data with a small size of featureset.
Then, by analyzing held-out rainingdata and selecting the features that affectthe parsing accuracy.
This kind of grad-ual increase of training data and featureset will be another method for reducingthe computational overhead.5 SummaryThis paper proposes Japanese dependencyanalysis based on Support Vector Machines.Through the experiments with Japanesebracketed corpus, the proposed methodachieves a high accuracy even with a small3With AlphaServer 8400 (617Mhz), it took 15 daysto train with 7958 sentences.training data and outperforms existing meth-ods based on Maximum Entropy Models.
Theresult shows that Japanese dependency anal-ysis can be effectively performed by use ofSVMs due to its good generalization a d non-overfitting characteristics.Re ferencesEugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Processing of the NAACL2000, pages 132-139.Michael Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In Pro-ceedings of the ACL '96, pages 184-191.C.
Cortes and Vladimir N. Vapnik.
1995.
SupportVector Networks.
Machine Learning, 20:273-297.Y.
Freund and Schapire.
1996.
Experiments witha new Boosting algoritm.
In 13th InternationalConference on Machine Learning.Masakazu Fujio and Yuji Matsumoto.
1998.Japanese Dependency Structure Analysis basedon Lexicalized Statistics.
In Proceedings ofEMNLP '98, pages 87-96.Msahiko Haruno, Satoshi Shirai, and YoshifumiOoyama.
1998.
Using Decision Trees to Con-struct a Partial Parser.
In Proceedings of theCOLING '98, pages 505-511.Takashi Inui and Kentaro Inui.
2000.
Committe-based Decision Making in Probabilistic PartialParsing.
In Proceedings of the COLING 2000,pages 348-354.Thorsten Joachims.
1998.
Text Categorizationwith Support Vector Machines: Learning withMany Relevant Features.
In European Confer-ence on Machine Learning (ECML).Sadao Kurohashi and Makoto Nagao.
1997.
KyotoUniversity text corpus project.
In Proceedingsof the ANLP, Japan, pages 115-118.John C. Platt.
1999.
Probabilistic Outputs forSupport Vector Machines and Comparisons toRegularized Likelihood Methods.
In Advancesin Large Margin Classifiers.
MIT Press.Adwait Ratnaparkhi.
1997.
A Liner ObservedTime Statistical Parser Based on Maximum En-tropy Models.
In Proceedings of EMNLP 'gZSatoshi Sekine, Kiyotaka Uchimoto, and HitoshiIsahara.
2000.
Backward Beam Search Algo-rithm for Dependency Analysis of Japanese.
InProceedings of the COLING 2000, pages 754-760.Hirotoshi Taira and Masahiko Haruno.
1999.
Fea-ture Selection in SVM Text Categorization.
InAAAI-99.Kiyotaka Uchimoto, Satoshi Sekine, and HitoshiIsahara.
1999.
Japanese Dependency StructureAnalysis Based on Maximum Entropy Models.In Proceedings of the EA CL, pages 196-203.Vladimir N. Vapnik.
1998.
Statistical LearningTheory.
Wiley-Interscience.25
