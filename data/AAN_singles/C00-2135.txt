Acquisition of Phrase-level Bilingual Correspondenceusing Dependency StructureKaoru Yamamoto  and Yuji MatsumotoGraduate School of Information Science,Nara Inst i tute of Science and Technology,8916-5, Takaymna-cho, Ikoma-shi, Nara, JapanAbst rac tThis paper describes a method to find phrase-level translation patterns from parallel corporaby applying dependency structure analysis.
Weuse statistical dependency parsers to determinedependency relations between base phrases in aseN;ence.
Our method is tested with a businessexpression corpus containing 10000 EnglishJapanese sentence pairs and achieved approx-imately 90 % accuracy in extracting bilingualcorrespondences.
The result shows that the useof dependency relation helps to acquire interest-ing translation patterns.1 I n t roduct ionSince the advent of statistical methods in Ma-chine qh'anslation, the bilingual sentence align-merit (Brown et al, 1991) or word alignment(Dagan et al, 1992) have been explored andachieved numerous success over the last decade.In coN;rasl,, fewer resull;s are reported in phrase-level correspondence.
As word sequences arenot translated literally a word for a word,acquiring phraseqevel correspondence still re-mains an important problem to be exploited.This paper proposes a method to extractphrase-level correspondence fi'om sentence-aligned parallel corpora using statistically prob-able dependency relations, i.e.
head-modifier re-lations in a sentence.The distinct characteristics of our approachis two-fold.
First, our approach uses depen-dency relations rather than alignment, cognateand/or position heuristics previously applied(Melamed, 1995).
Our approach is based onthe assumption that the word ordering and po-sitions may not necessarily coincide between thetwo languages, but the dependency structurebetween words will be preserved.
We believethat dependency relations offer richer linguisticclues (syntactic information) and are effcctivefor language pairs with different word orderingconstraints.Secondly, statistical dependency parsers areused to obtain candidate patterns.
Previousmethods mostly use rule-based parsers for pre-processing(Matsumoto e  al., 1993), (Kitamuraand Matsumoto, 1995).
The progress in parsingtechnology are noteworthy, and in particular,various tatistical dependency models have beenproposed(Collins, 1997),, (Ratnaparkhi, 1997),(Charniak, 2000).
It has an advantage over therule-based counterpart in that it achieves widercoverage, does not need to care for consistencyin rule writing, and is robust o domain changes.We conjecture that our approach improves cov-erage a.nd robustness by use of sl;atistical depen-dency parsers.In this paper, we aim to bwestigate tim effi-cacy of statistically probable dependency struc-ture in finding phrase level bilingual correspon-dence.
Though our discussion will proceed forEnglish Japanese phrasal correspondence, theproposed approach is applicable to any pair oflanguages.This paper is organised as follows: In thenext section, we present he overview of our ap-proach.
In Sections 3 and 4, components areelaborated in detail.
In Section 5~ experimentand results are given.
In Section 6, we compareour approach with related works, and finally ourfindings are concluded in Section 7.2 Overv iew of Our ApproachOur approach presupposes a sentence-alignedparallel corpora.
The task is divided into twosteps: a monolingual step in which candidatepatterns are generated by use of dependency re-lations, and a bilingual step in which these can-didate patterns fi'om each language are paired933J Sentences  --aligned-- E Sentencesi ii ; i iCandidate GeneratorJ Cand idatesI Cand idate  GeneratorIE Cand idates/?Phrase Matching \]JE T rans la t ion  Pat ternsFigure 1: flow of our approachwith their translations.
Figure1 shows the flowof our method.Our primary aim is to investigate the effec-tiveness of dependency structures in the mono-lingual candidate generation step.
For this rea-son, the bilingual step borrows the weightedDice coefficient and greedy determination from(Kitanmra and Matsumoto, 1996).In the following sections, we explain each stepin detail.3 Dependency-Preserv ing  Cand idatePat ternsDependency grammar or related paradigm(Hudson, 1984) focuses on individual words andtheir relationships.
In this framework, everyphrase is regarded as consisting of a gover-nor and dependants, where dependants maybe optionally classified further.
The syntacti-cally dominating word is selected as the gov-ernor, with modifiers and complements actingas dependants.
Dependency structures are suit-ably depicted as a directed acyclic graph(DAG),where arrows direct from dependants to gover-nors.We use a maximum likelihood model pro-posed in (Fujio and Matsumoto, 1998) wherethe dependency probability between segmentsare determined based on its co-occurrence anddistance.
It has constraints that (a) dependen-cies do not cross, (b) ee;ch segment has at leastone governor I .
Furthermore, the model has an1except for the 'root' segment.
For Japanese, the'root' segment is the rightmost segment.
For English,option to allow multiple dependencies whoseprobabilities are above certain confidence.
Itis useflfi for cases where phrasal dependenciescannot be determined correctly using only syn-tactic information.
It has an effect of improvingrecall by sacrificing precision and may containmore partially correct results useful for our can-didate pattern generation.We apply the following notions as unitsof segments: For English, (a) a prepositionor conjunction is grouped into the succeedingbaseNPs 2, (b) auxiliary verbs are grouped intothe succeeding main verb.
For Japanese, one(or a sequence of) content word(s) optionallyfollowed by function words 3.Having chunked into suitable segments, sen-tcnccs are parsed to obtain dependency rela-tions.
We have setup the following three mod-els:1. best -one  mode l  : uses only the mostlikely (statistically best) dependency rela-tio~m.
At most one dependency is allowedfor each segment.2.
ambiguous  mode l  : uses dependency re-lations above the certain confidence score0.54 .
Multiple dependencies may be con-sidered for each segment.3.
ad jacent  mode l  : uses only adjacency re-lations between segments.
A segment is ad-jacent to the previous segment.In tile ambiguous model, we expect thatnlore likely dependency relations will appearfrequently given in a large corpus, thereby in-creasing the correlation score.
Hence, ambiguityat parsing phase will hopefully resolved in thefollowing bilingual pairing phase.
As for the ad-jacent model, only chunking and its adjacencyare used.Finally, dependency relations between seg-ments is used to generate candidate patterns.the segment that contains tim mahl verb is regarded asthe 'root' segment.2a ba~seNP or 'minimal' NP is non-reeursive NP, i.e.none of its child constituents are NPs.
'~often referred ,~s a bunsetsu.4statistically-not-the-best dependencies m'e also in-eluded ifprob(k th  -- ranked  dependency)p~ob((g + 1)th ~ged dependency) -> O.S O)934\[1\]s i ze  i)s i ze  2)s i ze  3)\[saw\] \[agirl\] \[inl he park\]{1, saw, gM, park}{l_saw, girl_saw, in-park_saw}{lgirlsaw(T), l_in-parksaw(T)}\[I\] \[saw\] \[agMl \[inl the park\]s ize  i)s i ze  2)s i ze  3){I, saw, girl, park}{ saw_l, girlsaw, in-park_girl }{girl saw_l(L), in-parkgirlsaw(L) }Illsize l)size 2)size 3)Figure 2: best-one model\[saw\] \[a gM\] \[inl theparkl{I, saw, gM, park }{ I__saw, gM saw, inq~ark saw, in-park_girl }{ I_ gil'l saw(T), l j n-parksaw(T), in-park, girl saw(L) }Figure 3: anfl.figuous modelIn this paper, dependency size of a candidatepattern designates the nulnber of segments con-netted through dependency relations.
Figures2, 3, and 4 illustrate xamples of English can(li-(late patterns of dependency size 71, 2 and 3 forthe proposed ependency models.In a del)endency-connected candidate pat-tern, function words of the governor segment isdropped.
This is to cope with data sparsenessin generated candidate patterns.
Moreover, twotypes of DACs can be generated from patternsof size 3, and we use DAO-type tags ('I2 and'T') to distinguish their types.
W(' also notethat candidate patterns do not necessarily forlow the word ordering of original sentences.The algorithn~ is as follows:Input :  a corpus, the inininmm occurrencethreshold in a corpus fmin and the dependencysize dw.For each sentence ill a corpus, process tlm fol-lowing:1.
Part-of-Speech Tagging2.
Chunking: Rules are written as regular ex-pressions defined over POS word sequences.3.
Dependency Analysis4.
Candidate Pattern Generation: Candidatepatterns are generated and stored withtheir sentence ID.
Dependency-connectedpatterns of less than or equal to the sizedw are extracted.Figure 4: adjacent modelOutput :  a hash-table that maps from candi-(late patterns appearing at least the minimumoccurrence f'min to their sentence IDs found inthe corpus.4 Phrase- leve l  Cor respondenceAcqu is i t ionPairing of candidate patterns is a confl)inatorialproblem and we take tile following tactics toreduce the seard~ space.
First, our algorithmworks in a greedy manlmr.
This nmans that atranslation pair deternfined in the early stage ofthe algorithm will imver be consktered again.Secondly, filtering process is incorporated.Figure 5 illustrates filtering for a sentence pair"l saw a girl in the park/*\]~ :~ ./L ~ , DJ cO (J/" ~- ~ }~\]-".
A set of candidate patterns derived fi'olnEnglish is depicted on tile left, while thai; fromJapanese is depicted on the right.
Once apair "I_girl_saw(T)/&..~'~  _~ k (T)" is de-termied as a translation pair, then the algo-r ithm assumes that "gI~_ ~'~ ~ }~\]:-- (T)" willnot be paired with candidate patterns relatedto "Lgirl~qaw(T)" (cancelled by diagonal inesin Figure 5) for tile sentence pair.
The oper-ation effectively discards the found pairs andcauses recalculation of correlation scores in theproceeding iterations.As mentioned in Section 2, our correlationscore is calculated by the weighted Dice Coeffi-cient defined as:2f .ipj) = + .5where .\[j and .re are the number of occurrencesin Japanese and English corpora respectivelyand fej is the number of co-occurrences.
'.\['.he algorithm is as follows:I nput :  hash-tables of candidate patterns foreach language, the initial threshold of frequency.fc~,rr and the final threshold of fi'equency fmin.935/ \ [  I\[ .
\[ "i ,  i IL .
'\[iFigure 5: Filtering: word correspondence =Repeat the following until fcurr reaches fmin.1.
For each pair of English candidate pe andJapanese candidate pj appearing at leastf~,.r times, identify the most likely cor-respondences according to the correlationscores.?
For an English pattern Pc, obtain thecorrespondence andidate set pa = {Pjl, Pj2, ..., Pin } su& that sim(pe,pjk)> log2 fmir~ for all k. Similarly, obtainthe correspondence andidate set PEfor an Japanese pattern pj?
Register (Pe,Pj) as a translation pair ifpj = arglnax Pjk E PJ  sim( Pc, Pjk )and Pc = argmax Pek C PE sin1( pj,p& ).
The correlation score of (Pe,Pj)is the highest among PJ  for Pe and PEfor pj.2.
Filter out the co-occurrence positions forPc, Pj, and related candidate patterns.3.
Lower the threshold of frequency if no morepairs are found with fcurr.5 Experiment and Result5.1 Exper imenta l  Set t ingWe use a business expression corpus (Takuboand Hashimoto, 1995) containing 10000 sen-tences pairs which are pre-aligned.NLP tools are summarised in Table 1.Parameter setting are as follows: dependencysize d~ is set to 3.
Initially, fc~,.~ and fmin areset to 100 and 2 respectively.
As tile algorithmproceeds, f ,u~ is adjusted to half of its previousvalue if it is greater than 10.
Otherwise f,~r," is(I, gI~)(saw, g \]5_ )(girl, ~" ~k)(park, g~, N )preprocessing toolPOS(E) ChaSen2.0 96% precisionPOS(J) ChaSen2.0 97% precisionchunking(E) SNPlexl.0 rule-basedcNmking(J) Unit rule-baseddependency(E) edep trial systemdependency(J) jdep 85 87 % precisionTable 1: NLP toolsdecremented by i.
If the number  of registeredtranslation pairs is less than 10, then fcurr islowered in the next iteration.
All parametersare empirically chosen.5.2 Resu l tOur approach is evaluated by the metrics de-fined below:count(pl,)pr ccisicm - count (px )Ep, (h;ngth(pt) * cofreq(pt) )coverage = ~pl occur (Pl )Precision measures the correctness of ex-tracted translation pairs, while coverage mea-sures tile proportion of correct ranslation pairsin the parallel corpora.
Let X be a pattern.count(X) gives tile mmlber of X returned,occur(X) gives the mlmber of occurrences of Xin each corpus, length(X) gives the dependencysize of X and cofrcq(X) gives the number of co-occurrences in the parallel corpora.. Px nmansextracted patterns, and of which correct pat-terns are designated as pt- p~ means the candi-date patterns generated from each side of paral-lel corpora.
Coverage is calculated for English936th2512 710 69 48 137 106 195 294 673 1502 414( *2 264total 725( *totM 989correct extracted c / e6 6 100.007 100.007 85.714 100.0013 100.0013 76.9220 95.0029 t.00.0072 93.05164 91.46461 89.80474 55.69796 - -1269 - -precision1.00.0095.0095.8392.3097.2992.0092.8594.9494.1592.8391.0877.93)91.0877.93)Tal)le 2: Precision: best one modelth correct25 612 710 69 48 137 116 185 294 683 1182 432( *2 256total 712( *total 968extracted67741.3131929731264687597651.524C / e100.00100.0085.71100.00100.0084.6194.73100.0093.1593.6591..5033.72precision100.00i00.0095.0095.8397.2994.0094.2095.9194.7394.2793.0763.51)93.0763.51)Table 3: Precision: aml)iguous modeland Japanese separately and then thier nman istaken.Precision for each model is summarised in Ta-bles 2, 3, and 4, while coverage is shown in Table5.
To examine the characteristics of each model,we expand correspondence andidate sets PEand Pa so that patterns '5 with tile correlationscore > l og2  2 (> 1) are also considered.
Theseare marked by asterisks "*" in Tables.Random samples of correct and near-correcttranslation pairs are shown in Table 6, Table7 respectively.
Extracted translation pairs arematched against he original corpora to restoretheir word ordering.
This restoration is donenmnually this time, but can be automated withlittle modification i  our algorithm.5 1.
(L patterns where f~j  = f(, = f j  = fi,~i,, = 2(*2total( *totMth25 612 710 69 48 137 106 185 294 683 1142 419280694correct extracted6774:1313192973126484496781974 1277c / o precision100.00 100.00100.00 100.0085.71 95.00100.00 95.83100.00 97.2984.61 92.0094.73 92.75100.00 94.8993.15 94.1593.65 92.5986.57 88.8656.45 76.27)Table 4: Precision: ad.jacent model88.8676.27)model English Japanese coverageI)est-one 18.16 % 18.43 % 18.29 %best one* 19.12 % 19.59 % 19.13 %~mt)iguous 18.63 % 18.82 % 18.72 %ambiguous* 19.57 % 19.95 % 19.76 %~d.ia('ent 17.74 % 18.03 % 17.88 %adjacent* 18.69 % 19.20 % 18.94 %Table 5: Coverage5.3  D iscuss ionAs we see from Tal)le 2 and 3, the t)est onemodel adfieves 1)etter precision than the adja-cent model.
Upon inspecting the results, nearlythe same translation patterns are extracted forhigher thresholds.
This is because our depen-dency parsers use the distance feature in deter-mining dependency.
Consequently, nearer seg-ments are likely to 1)e dependency-related.
Ex-periment data shows that tile exact overlapsare found in 9348 out of 14705 (63.55%) candi-(late patterns for English and 6625 out of 11566(57.27%) for Japanese.However, the difference appears when thethreshold reaches 3 and patterns uch as " nothesitate to contact/~)~,  < ~*~,~"  which isnot found in the adjacent model are extracted.Moreover, the l)est-~one model is l)ettm" in termsof coverage.
These results support that the de-pendency relations appear useful clues than justbeing linearly ordered.Comparing the 1)est one model with the am-t)iguous model, the aml)iguous model achievesa higher precision except for *2.
This indicates937Englishthank+youconsultations+includeapply+for_t he_positionthank+you+in_advancenot +hesitate+to_contactb e+enclosed+a_copybe_writ ing+toJet+knowapplications+includeupcoming_borard+of_director_s "_meetingwillJ~ave+to_cancelhave+high_hopebusiness+is_expandedwe+have_learned+t~om_your_faxleaving+in+about _ten_daysget +you+in_close_business_relationshipwe+are_inquiring+regardingpay+special_at tcntionJapanese~b U D~'n~- 9~_t~ :b o ~ +~ 6-~_~ ~ ~-~?
~-_a',5 _~ +i~_t~ < +t~:k ~ ~ lc_ +Jt/l.f,~_t- ,5,~_1_0_ H _f~ +,N ~score4.70372.32192.21571.60001.60001.05661.05661.00001.00001.00001.00001.00001.00001.00001.00001.00001.0000Table 6: random smnples of correct ranslation patterns in best-one model.
"+" indicates a segment-sepm'ator ~md "2' indicates a morpheme-sepm'ator.~Engl ish(have_been_pleased) +to_serve+as_thier_main_banker\[be_held\] -t-aLhotel_new_oht aniassets _position+ (in_good_shape)(have_been_placed) +into_our_file(put) +one_monthJimit\[passed\] +on_past _ uesdayJapanese~.l, k+ ~ _a) + 7 7 4 :bl_O H _cO +iN liJ~Table 7: randmn samples of near-correct translation patterns where score is 1.000.
Segments o bc deletedto become correct patterns are embraced by "0".
Segments o be added are embraced by "~"that the accuracy of dependency parsers cur-rently achieves are insufficient, and therefore,better to expand the possibilities of candidatepatterns by allowing redundant dependency re-lations.
As the dependency parsers improve, tlmbest~one model will outperform the ambiguousmodel.
However, as the result of *2 shows, can-didates from redundant dependency relationsare mostly exl;racted at the low threshohl.
Theoverall trend reveals that redundant relationsact as noise at low thresholds, but help to scaleup the the correlation score at higher thresh-olds.As shown in Table 6, a domain-specific dis-ambiguation sample ("Thank youFb U ;b~ ~ 9 "vs. "Thank you in advance/~:b -z "E ~3N~b 3= W ?
9-") is found.
As for long-distancedependency-related ranslation patterns, "~i"-case (nominative) and verb patterns (consulta-t;ions include/~,~ t:-- I,:1;.
~ ~ ~ ) are extracted 6.6A typical Japanese sentence follows S-O-V s~ructure:Other types of long-distance translation pat-terns such as "~d "-case (accusative) and verbpatterns (be held at X/X -d ~g@.9- ;5 ) are not ex-tracted even candidate patterns fi'om each cor-pus are generated.Generally speaking, acquiring long-distancetranslation patterns is a hard problem.
We stillrequire fllrther investigation examining underwhat circumstance the dependency relations arereally effective.
So far, we use relatively "clean"business expression corpora which is a collec-tion of standard usage.
However, in the realworld setting, more repetitions and variationswill be observed.
Adjuncts can be placed in lessconstrained way and the adjacent model cannotdeal with if they are apart.
In such cases, awdl-ablilty of robust dependency parsers become s-sential, dependency relations plays a key role infinding the long-distance translation patterns.while tile English counterpart follows S-V-O s~rucfiure.9386 Re la ted  WorksSmadja et a1.
(1996) finds rigid and flexible col-locations.
They first identify candidate collo-eLtions in English~ and subsequently, find thecorresponding lq'ench collocations by graduallyexpanding the candidate word sequences.
Ki-tamura et a1.
(1996) enmnerates word sequencesof arbitrary length (n-grmn of content words)that appear more than the mininmln thresholdfrom English and Japanese and attempts to tlndthe correspomlence based on the prepared can-didate lists.Difference from Smadja et a1.
(t996) is thatour method is hi-directional nd difference fromKiLamura et al (1996) is that we use de-pendency relations whidl leads to "structured"phrasal correspondence as opposed to "flat" ad-jacent correspondence.On the other hand, Matsumoto et a1.
(1993),KiLamura et a1.
(1995) and Meyers et a1.(1996)use.
dependency structure for structm'al match-ing, of sentences to acquire translation rules.Their methods empl W grammar-based parsersand only work for declarative sentences.
Theirobjectives are complete inatching of dependencytrees of two languages.
:\[nstea(t, our method uses statistical depen-dency parsers and are not restricted to sim-ple sentences for input.
Fnrthermore, we areconcerned with partial matdfing of dependencytrees o that the overall robustness and coveragewill be improved.7 Conc lus ionIn this paper, we propose a method to findphrase-level bilingual correspondence using de-pendency structure from parallel corpora.
Wehave conducted a preliminary experiment with10000 business sentence pairs of English andJapanese and achieved approximately 90% pre-cision.Though a fuller investigation still requires,our finding shows that the dependency rela-tions serve as useful inguistic lues in the taskof phrase-level Mlingual correspondence acqui-sition.Re ferencesP.\]?.
Brown, J.C. Lai, and R,.L.
Mercer.
1991..Aligning sentences in parallel corpora.
InACL-29: 29th Annual Mceting of the Asso-ciation .for Computational Linguistics, pages169-176.E.
Charniak.
2000.
A maximum-entropy-inspired parser.
In NAA CL-2000: 1st Meet-ing of the North American Cltapter of theAssociation for Computational Lingv, islics,pages 132 139.M.J.
Collins.
1997.
Three generative, lex-icalised models for statistical parsing.
InACL-97: 35th Annual Meeting of the Asso-ciation for Computational Linguistic% pages16-23.I.
Dagan, K. Church, and W. Gale.
1992.
R.c)-bust bilingual word alignment for machineaided translation.
In PTve.
of life Workshopon Very Large Co77)ora, pages 1-8.M.
Fujio and Y. Matsumoto.
1998.
Japanesedependency structure analysis based on Icyicalized statistics.
In Pros.
of 3rd Conf.
onEmpcricat Mcthods in Natural Language Pro-cessing, pages 88 96.R.. Hudson.
1984.
Word Grammar.
Blackwell.M.
Kitamura and Y. Matsumoto.
1995.
A ma-chine translation system based on transla-tion rules acquired from parallel corpora.
InPros.
of Recent Advances in Natural Lan-nguage P~vccssing, pages 27-44.M.
Kitamura and Y. Matsumoto.
1996.
Aul, o-matic extraction of word sequence correspon-dences in parallel corpora.
In Pros./~ttt Work-shop on Very Large Corpora, pages 79 87.Y.
Matsumoto, H. Ishimoto, and T. Utsuro.1993.
Structural matching of parallel texts.In ACL-93: 31st Annual Mcetin 9of the Asso-ciation for Computational Linguistics, pages23-30.I.D.
Melamed.
1995.
Automatic evaluation anduniform filter cascades for inducing n-besttranslation lexicons.
In Pros.
of 3rd Work-shop on Very Large Cmpora, pages 184-198.A.
I/atnaparkhi.
1997.
A linear observed timestatistical parser based on maximum entropymodels.
In Proc of 2nd Conf.
on Empsri-eal Methods in Natural Language P~vcessing,pages 1-10.K.
Takubo and M. Hashimoto.
1995.
A Dictio-nary of English Bussiness Letter" Expressions.Nihon Keizai Shimbun, Inc.939
