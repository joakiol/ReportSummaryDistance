Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 38?45,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsDetecting Structural Events for Assessing Non-Native SpeechLei ChenEducational Testing ServicePrinceton NJ USALChen@ets.orgSu-Youn YoonEducational Testing ServicePrinceton NJ USASYoon@ets.orgAbstractStructural events, (i.e., the structure of clausesand disfluencies) in spontaneous speech, areimportant components of human speaking andhave been used to measure language devel-opment.
However, they have not been ac-tively used in automated speech assessmentresearch.
Given the recent substantial progresson automated structural event detection onspontaneous speech, we investigated the de-tection of clause boundaries and interruptionpoints of edit disfluencies on transcriptionsof non-native speech data and extracted fea-tures from the detected events for speechassessment.
Compared to features com-puted on human-annotated events, the featurescomputed on machine-generated events showpromising correlations to holistic scores thatreflect speaking proficiency levels.1 IntroductionSpontaneous speech utterances are organized in astructured way and generated dynamically with op-tional disfluencies.
In second language acquisition(SLA) research, information related to the structureof utterances and profile of disfluencies has beenwidely used to monitor speakers?
language develop-ment processes (Iwashita, 2006).
However, struc-tural events in human conversations have not beenactively used in the automated speech assessment re-search.
For example, most research that used Auto-matic Speech Recognition (ASR) technology to au-tomatically score speaking proficiency (Neumeyeret al, 2000; Zechner et al, 2007) focused on word-level cues for fluency and accuracy.In the last decade, a large amount of research (Go-toh and Renals, 2000; Shriberg et al, 2000; Liu,2004; Ostendorf et al, 2008) has been conductedon structural event detection (i.e., sentence and dis-fluency structure).
This research has resulted inbetter models for structural event detection.
Thedetected structural events have been found to helpmany of the following natural language processing(NLP) tasks: speech parsing, information retrieval,machine translation, and extractive speech summa-rization (Ostendorf et al, 2008).Because structural event information: (1) is im-portant for understanding/processing speech, (2)has been successfully used in monitoring languagedevelopment, which will be summarized in Sec-tion 2, (3) has received limited attention in auto-mated speech assessment, and (4) has been activelyinvestigated in the speech research domain in thepast decade, it is worthwhile investigating the util-ity of using structural event detection on automatedspeech assessment.
Because of the fairly low wordaccuracy currently achieved when recognizing spon-taneous non-native speech of mixed proficiency lev-els and native language backgrounds, this study willfocus on the transcribed words rather than speechrecognition outputs.This paper is organized as follows: Section 2 re-views previous research; Section 3 reports on thedata used in the paper, including the collection, scor-ing, transcription, and annotation processes; Sec-tion 4 discusses the methods we utilized for struc-tural event detection; Section 5 describes the exper-iments of structural event detection; Section 6 de-scribed the features derived from the event sequence38for assessing speech and evaluation results on thesefeatures; Section 7 discusses the findings of our re-search and plans for future directions.2 Previous ResearchIn the SLA and child language development researchfields, language development is measured accord-ing to fluency, accuracy, and complexity (Iwashita,2006).
Structural events are used to derive the fea-tures measuring syntactic complexity.
For example,typical metrics for measuring syntactic complexityinclude: length of production units (e.g., T-units1,clauses, verb phrases, and sentences), amount ofembedding, subordination and coordination, rangeof structural types, and structural sophistication.Iwashita (2006) investigated several measures ofsyntactic complexity on data generated by learnersof Japanese.
The author reported that some mea-surements (e.g., T-unit length, the number of clausesper T-unit, and the number of independent clausesper T-unit) were good at predicting learners?
profi-ciency levels.In addition, speech disfluencies are used to mea-sure language development.
For example, Lennon(1990) used a dozen features related to speed,pauses, and several disfluency markers, such asfilled pauses per T-unit, to measure the improvementof English proficiency for four German-speakingwomen during a six-month study in England.
Hefound a significant change in filled pauses per T-unitduring the study process.These two types of features derived from struc-tural events were combined in other previous stud-ies.
For example, Mizera (2006) used fluency fac-tors related to speed, voiced smoothness (frequencyof repetitions or self-corrections), pauses, syntacticcomplexity (mean length of T-units), and accuracy,to measure speaking proficiency on 20 non-nativeEnglish speakers.
In this experiment, disfluency-related factors, such as the total number of voiceddisfluencies, correlated strongly with the fluencyscore (r = ?0.45); however, the syntactic com-plexity factor only showed a moderate correlation(r = 0.310).There have been previous efforts in using NLP1A T-unit is defined as essentially a main clause plus anyother clauses which are dependent upon it (Hunt, 1970).technology to automatically calculate syntactic com-plexity metrics on learners?
writing data.
For exam-ple, Lu (2009) and Sagae et al (2005) used parsingto get structural information on written texts; how-ever, such efforts have not been undertaken in as-sessing speech data.Chen et al (2010) annotated structural events(such as clause structure and disfluencies) on En-glish language learners?
speech transcriptions andextracted features based on the structural event pro-file.
They found that the features derived from struc-tural event profile show promising correlation to hu-man holistic scores.
Berstein et al (2010) also com-puted the features related to sentence lengths andthe counts of syntactic entities.
They found the ex-tracted features were highly correlated to holisticscores measuring test-takers?
language proficiencyin both English and Spanish.In the speech research domain, a large amountof research has been conducted to detect struc-tural events in speech transcriptions and recognizedwords using lexical and prosodic cues.
Using a lan-guage model (LM) trained on words combined withthe events of interest is a popular technique for us-ing textual information for structural event detec-tion.
For example, Heeman and Allen (1999) devel-oped a LM including part of speech (POS) tags, dis-course markers (e.g., right, anyway), speech repairs,and intonational phrases.
In this way, structural in-formation (e.g., speech repairs), could be predictedusing a traditional speech recognition approach.Prosodic information has been widely used to fur-ther improve textual models.
For example, a sim-ple prosodic feature, pause duration between words,was used in Gotoh and Renals (2000) to detect sen-tence boundaries.
It was found that the pause dura-tion model alone was better than using an LM alone,and the combination of the two models further im-proved the performance.More advanced prosody models were used inother research on sentence boundary and speech re-pair detections (Shriberg et al, 2000; Shriberg andStolcke, 2004).
A general framework was built com-bining textual and prosodic cues to detect variouskinds of structural events in speech, including sen-tence boundaries, disfluencies, topic boundaries, di-alog acts, emotion, etc.
Shriberg and Stolcke (2004)extracted prosodic features such as pause, phone du-39ration, rhyme duration, and F0 features.
Using allof these features, a decision tree was built to de-tect possible structural events.
An LM augmentedwith structural event tokens was also used to de-tect structural events based on textual cues.
Fi-nally, a Hidden Markov Model (HMM) was usedto combine estimations from the textual model (anaugmented LM with structural events) and prosodicmodel (decision-tree based on prosodic features).Research on structural event detection has beenstrongly affected by the DARPA EARS pro-gram (EARS, 2002).
As in Shriberg et al (2000), thestructural event detection (e.g., sentence units (SUs)and speech repairs) investigated in EARS was a clas-sification task utilizing both prosodic and textualknowledge sources.
New approaches for combin-ing the two knowledge sources, including maximumentropy (MaxEnt) and conditional random fields(CRFs), were studied to address the weaknesses ofthe generative HMM approach (Liu et al, 2004).
Liuet al (2005) concluded that ?adding textual infor-mation, building a more robust prosodic model, us-ing conditional modeling approaches (Maxent andCRF), and system combination all yield perfor-mance gains.
?3 Non-native Structural Event CorpusNon-native speech data were collected from theTOEFL Practice Test Online (TPO) (ETS, 2006).In each TPO test, test-takers were required to re-spond to six speaking test items, in which they wererequired to provide information or opinions on fa-miliar topics, based on their personal experience orbackground knowledge.
For example, the test-takerswere asked to describe their opinions about living onor off campus.A total of 1066 responses were collected from ex-aminees.
Then, a group of experienced human ratersscored these items based on the scoring rubrics de-signed for scoring the TPO test.
For each item, twohuman raters independently assigned 4-point holis-tic scores for test-takers?
English proficiency levels.The speaking content was transcribed by a pro-fessional transcribing agency.
On the transcrip-tions, structural event annotations were added, in-cluding (1) locations of clause boundaries, (2) typesof clauses (e.g., noun clauses, adjective clauses, ad-verb clauses, etc.
), and (3) disfluencies.Disfluencies can further be sub-classified into sev-eral groups: silent pauses, filled pauses (e.g., uh andum), false starts, repetitions, and repairs.
The repeti-tions and repairs were denoted as ?edit disfluency?,which were comprised of a reparandum, an optionalediting term, and a correction.
The reparandum isthe part of an utterance that a speaker wants to re-peat or change, while the correction contains thespeaker?s correction.
The editing term can be afilled pause (e.g., um) or an explicit expression (e.g.,sorry).
The interruption point (IP), occurring at theend of the reparandum, is where the fluent speech isinterrupted to prepare for the correction.For the research reported in this paper, we focuson two structural events: the locations of clause-ending boundaries (CBs) and interruption points(IPs) of edit disfluencies.
Note that if several clauses(in different layers of a clause hierarchy) end at thesame word boundary, these clause boundaries werecollapsed into one CB event.Two persons annotated the corpus separately andtheir annotation quality was monitored by using sev-eral Kappa computations.
For CBs, ?
ranges from0.85 to 0.90; for IPs, ?
ranges from 0.63 to 0.83.Generally, a ?
greater than 0.8 indicates a goodbetween-rater agreement and ?
in the range of 0.6to 0.8 indicates acceptable agreement (Landis andKoch, 1977).
Therefore, we believe that our humanannotations are sufficiently reliable to be used in thefollowing experiments.4 Methods of Structural Event Detection4.1 Features for structural event detectionIn previous research (Gotoh and Renals, 2000;Shriberg et al, 2000; Liu, 2004), prosodic cueswere found to be helpful, however, such findingson native speech data may not work well with non-native speech data.
Anderson-Hsieh and Venkata-giri (1994) compared the pause frequencies of threegroups of speakers (native, high-scoring, and low-scoring non-native speakers).
They found that pausefrequency was higher for groups of speakers withlower speaking skills.
For native speakers, a longpause after a word-ending boundary is an impor-tant cue for signaling the existence of a sentence orclause boundary.
However, the fact that there are40more frequent pauses in non-native speech obscuresthis relationship.On our non-native speech corpus, we conducteda pilot study on a widely-used prosodic feature, thepause duration2 after a word, for its predictive abil-ity to detect clause boundaries.
If the duration of thepause after a word boundary is longer than 0.15 sec-ond, we call it a long pause.
We measured the likeli-hood of being a CB event on the words followed by along pause.
For each score level, the likelihoods are:15% for a score of 1, 22% for a score of 2, 28% fora score of 3, and 35% for a score of 4.
Clearly, forlow-proficiency speakers (i.e., speakers with a scoreof 1), long pauses in their utterances are not tightlylinked to CBs.
Therefore, more research is neededto utilize prosodic cues on non-native speech; in thispaper, we focus on lexical features.4.2 Statistical modelsBased on lexical features, the structural event detec-tion task can be generalized as follows:E?
= argmaxEP (E|W )Given that E denotes the between-word event se-quence and W denotes the corresponding lexicalcues, the goal is to find the event sequence that hasthe greatest probability, given the observed features.Recently, conditional modeling approaches weresuccessfully used in sentence units (SUs) and speechrepairs detection (Liu, 2004).
Hence, we use theMaximum Entropy (MaxEnt) (Berger et al, 1996)and Conditional Random Fields (CRFs) (Lafferty etal., 2001) approaches to build statistical models forstructural event detection.5 Structural Event Detection Experiment5.1 SetupIn our experiment, the whole corpus described inSection 3 was split into a training set (train), a devel-opment test set (dev), and testing set (test), withoutspeaker overlap between any pair of sets.
Table 1summarizes the numbers of items and words, as wellas structural events of each dataset.2Pause durations were obtained by running forced alignmentusing speech and transcriptions on a tri-phone HMM speechrecognizertrain dev test# item 664 101 301# word 71523 10509 33754# CB 6121 918 2852# IP 1767 267 1112Table 1: The number of items, words, and structuralevents of the three sets in the TPO corpusOn average, each item contains about 108.6words, 9.3 CBs, and 3.0 IPs.
9% of the word bound-aries are associated with a CB event and 3% of theword boundaries are associated with an IP event.Clearly, these CB and IP events are sparse and sucha skewed distribution of structural events increasesthe difficulty of structural event detection.5.2 ModelsThe following two conditional models were built todetect CB and IP events:?
MaxEnt: Given wi as the word token at po-sition i, the word n-gram features include:?wi?, ?wi?1, wi?, ?wi, wi+1?, ?wi?2, wi?1, wi?,?wi, wi+1, wi+2?, and ?wi?1, wi, wi+1?.
Giventi as the POS tag3 at position i, the POSn-gram features include: ?ti?, ?ti?1, ti?,?ti, ti+1?, ?ti?2, ti?1, ti?, ?ti, ti+1, ti+2?, and?ti?1, ti, ti+1?.For IP detection, in addition to the n-gram fea-tures described above, another four featuresthat capture syntactic pattern of disfluencies areutilized:?
filled pause adjacency: This feature hasa binary value showing whether a filledpause such as uh or um was adjacent tothe current word (wi).?
word repetition: This feature has a binaryvalue showing whether the current word(wi) was repeated in the following 5 wordsor not.3POS tags were obtained by tagging words using a MaxEntPOS tagger, which was implemented in the OpenNLP toolkitand trained on the Switchboard (SWBD) corpus.
This POS tag-ger was trained on about 528K word/tag pairs and achieved antagging accuracy of 96.3% on a test set of 379K words.41?
similarity: This feature has a continuousvalue which measures the similarity be-tween the reparandum and correction.
As-suming that wi was the end of the reparan-dum, the start point and the end point ofthe reparandum and correction were es-timated, and the string edit distance be-tween the reparandum and correction wascalculated.
The start point and the endpoint of the reparandum and correctionwere estimated as follows; if wi appearedin the following 5 words, the second oc-currence was defined as the end of the cor-rection.
Otherwise, wi+5 was defined asthe end of correction.
Secondly, N , thelength of the correction was calculated,and wi?N+1 was defined as the start pointof the reparandum.
During the calculationof the string edit distance, a word frag-ment was considered to be the same asa word whose initial character sequencesmatched it.?
length of correction: This feature countsthe number of words in the correction.The first two features are similar to the featuresused in (Liu, 2004) while the last two featuresprovide important keys in distinguishing editdisfluencies from fluent speech.
Since the cor-rection is composed of word sequences that aresimilar to the reparandum, these two featuresare higher than zero when the target word is apart of the edit disfluency.
In addition, thesetwo numeric features were discretized by usingan equal-distance binning approach.Using n-gram features for CB detection and allthese lexical features for IP detection, we usedthe Maxent toolkit designed by Zhang (2005) tobuild MaxEnt models.
The L-BFGS parameterestimation method is used, with the Gaussian-prior smoothing technique to avoid over-fitting.The Gaussian prior is estimated on the dev set.?
CRF: All features which were described inbuilding MaxEnt models were used in the CRFmodel.
We used the Java-based NLP packageMallet (McCallum, 2005) to build CRF mod-els.
Similar to MaxEnt models, Gaussian-priorsmoothing was used with the priors estimatedon the dev set.These models were trained using the train set.
Be-sides Gaussian priors, other parameters in the modeltraining (i.e., the training iteration number as well asthe cutting-point for event decisions) were estimatedusing the dev set.
Finally, the trained models wereevaluated on the test set.5.3 Evaluation of event detectionSince structural event detection was treated as a clas-sification task in this paper, four standard evaluationmetrics were used:accuracy =TP + TNTP + FP + TN + FNprecision =TPTP + FPrecall =TPTP + FNF1 = 2?recall ?
precisionrecall + precisionwhere, TP and FP denote the number of true pos-itives and false positives, and TN and FN denotethe number of true negatives and false negatives.
Astructural event (a CB or IP boundary) is treated as apositive class.
In our experiment, since we treatedprecision and recall as equally important, the F1measurement was used.For each model, if the estimated probability,P (Ei|W ), is larger than a threshold, the correspond-ing word boundary will be estimated to be a positiveclass.
The threshold was chosen when a maximalF1 score was achieved on the dev set.A model that always predicts the majority class(a no-event in this study) was treated as a baselinemodel.
For CB detection, this type of baseline modelresulted in an accuracy of 91.6%; for IP detection,this type of baseline model resulted in an accuracyof 96.7%.5.4 Results of structural event detectionTable 2 summarizes the performance of the twomodels on the CB and IP detection tasks.For CB detection, two conditional models are su-perior to the baseline CB detection (with an accuracyof 91.6%); they achieved relatively high F1 scores42Acc.
Pre.
Rec.
F1CBMaxEnt 94.5 66.1 71.8 0.689CRF 96.1 82.3 68.6 0.749IPMaxEnt 98.1 61.8 55.2 0.583CRF 98.4 76.9 48.0 0.591Table 2: Experimental results of the CB and IP detectionmeasurement using accuracy (Acc.
), precision (Pre.
), re-call (Rec.)
and F1 measurement (F1) on the TPO dataranging from 0.689 to 0.749.
Between the two mod-els, the CRF model achieved the higher F1 scoreat 0.749, The lower F-score of the MaxEnt modelmay be caused by the fact that the MaxEnt modeldoes not use event history information in its decod-ing process.However, these two models achieved lower per-formance on the task of detecting IPs for editing dis-fluencies.
F-scores became about 0.58 to 0.59 forIP detections.
The degraded performance may becaused by the extremely low IP distribution (only3%) in our data.
Between the two modeling ap-proaches, consistent with the result shown for CBdetection, the CRF model achieved a higher F1score (0.591).6 Using Detected Structural Events forSpeech Assessment6.1 Features assessing proficiencyMany previous SLA studies used the length of pro-duction units and frequency of disfluencies as met-rics to measure language development (Iwashita,2006; Lennon, 1990; Mizera, 2006).
Our automatedstructural event detection provides the locations ofCBs and IPs, which can be used to compute thesefeatures for use in speech assessment.Using Nw to represent the total number of wordsin the spoken response (without pruning the reparan-dums and edit terms in the edit disfluencies), NCas the total number of CBs, and NIP as the totalnumber of IPs detected on transcriptions of speechstreams, the following features (i.e, mean length ofclause (MLC), interruption points per clause (IPC),and interruption points per word (IPW)) were de-rived:MLC = Nw/NCIPC = NIP /NCIPW = NIP /NwThe IPW can be treated as the IPC normalizedby the MLC.
The reason for this normalization isthat disfluency behavior is influenced by various fac-tors, such as speakers?
proficiency levels as well asthe difficulty of utterances?
structure.
For example,Roll et al (2007) found that the complexity of ex-pression, computed based on the language?s parsing-tree structure, influenced the frequency of disflu-encies in their experiment on Swedish responses.Therefore, the fact that IPW is the IPC normalizedby MLC (a feature related to complexity of utter-ances?
structure) helps to reduce the impact of utter-ances?
structure and to highlight contributions fromthe speaker?s proficiency.6.2 Results of measuring the derived featuresOn the test set, we produced CB and IP event se-quences estimated by the MaxEnt and CRF models,respectively.
These machine-generated events wereevaluated by comparison with human annotations,which were denoted as REF.The proposed features described in Section 6.1were computed on the word/event sequence of eachitem.
In addition, given the fact that each item onlycovers approximately one-minute of speech and thecontent is quite limited, we also extracted featureson the test-taker level by combining the detectedevents of all of the items spoken by each test-taker.Then, according to the score handling protocol usedin TPO, the human-holistic scores from the first hu-man rater were used as item scores to compute Pear-son correlation coefficients (rs) with the features.For the test-taker level evaluation, we used the aver-age score for each test-taker from all of his/her itemscores.Table 3 reports on the evaluation results of thefeatures derived from the structural event estima-tions.
Compared to rs computed on the speakerlevel using multiple (as many as 6) items, rs com-puted on the item level are generally lower.
Thisis because words and events are limited in this one-minute long response.
Among the three features, the43Model rMLC rIPC rIPWPer itemREF 0.003 ?0.369 ?0.402MaxEnt ?0.012 ?0.329 ?0.343CRF ?0.042 ?0.328 ?0.335Per speakerREF 0.066 ?0.453 ?0.516MaxEnt 0.055 ?0.396 ?0.417CRF 0.043 ?0.355 ?0.366Table 3: Correlation coefficients (rs) between the fea-tures derived from structural events with human scoreson the item and speaker levelsMLC shows the lowest r to human holistic scores.
Incontrast, the two features derived from interruptionpoints show promising rs to human holistic scores.Between them, the IPW always shows a higher rthan the IPC.
Compared to the features extracted onhuman annotations, the features derived from struc-tural events automatically estimated by the two NLPmodels show a lower but sufficiently high r. Thefeatures derived from the MaxEnt model?s estima-tions on the test-taker level show a greater r than thefeatures derived from the CRF model estimations.7 DiscussionThree features measuring syntactic complexity anddisfluency profile of speaking, MLC, IPC, and IPW,were extracted on the structural event sequences es-timated by the developed models.
Compared to thefeatures extracted from the human-annotated struc-tural events, the features derived from machine-generated event sequences show promisingly closecorrelations.Applying automated structural event detection tospontaneous speech brings many benefits for auto-matic speech assessment.
First, obtaining informa-tion beyond the word level, such as the structure ofclauses and disfluencies, can expand and improvethe construct4 coverage of speech features.
Second,knowing the structure of utterances helps to facili-tate the application of more NLP processing meth-ods (e.g., collocation detection that requires infor-mation about sentence boundaries), to speech con-4A construct is the set of knowledge, skills, and abilitiesmeasured by a test.tent.
In this study, using only simple word andPOS based n-gram features, CBs can be detectedrelatively well (with an F1 score of approximately0.70).
More lexical features reflecting repair proper-ties were found to help improve IP detection perfor-mance.
In addition, IP-based features derived frommachine-generated event sequences show promis-ing correlation with human holistic scores.
Resultsin detection of clause boundaries and interruptionpoints support the approach of utilizing automatedstructural event detection on speech assessment.We plan to continue our research in the followingthree directions.
First, we will investigate integrat-ing prosodic cues to further improve the structuralevent detection performance on non-native speech.Second, we will investigate estimating structuralevents directly on speech recognition results.
Third,other aspects of syntactic complexity, such as theembedding of clauses, will be studied to provide abroader set of features for speech assessment.ReferencesJ.
Anderson-Hsieh and H. Venkatagiri.
1994.
Syllableduration and pausing in the speech of chinese ESLspeakers.
TESOL Quarterly, pages 807?812.A.
Berger, S. Pietra, and V. Pietra.
1996.
A maximum en-tropy approach to natural language processing.
Com-putational Linguistics, 22:39?72.J.
Berstein, J. Cheng, and M. Suzuki.
2010.
Fluency andStructural Complexity as Predictors of L2 Oral Profi-ciency.
In Proc.
of InterSpeech.L.
Chen, J. Tetreault, and X. Xi.
2010.
Towards usingstructural events to assess non-native speech.
In FifthWorkshop on Innovative Use of NLP for Building Ed-ucational Applications, page 74.EARS.
2002.
DARPA EARS Program.
http://projects.ldc.upenn.edu/EARS/.ETS.
2006.
TOEFL Practice Online Test (TPO).Y.
Gotoh and S. Renals.
2000.
Sentence boundary de-tection in broadcast speech transcript.
In Proceed-ings of the International Speech Communication As-sociation (ISCA) Workshop: Automatic Speech Recog-nition: Challenges for the new Millennium ASR-2000.P.
Heeman and J. Allen.
1999.
Speech repairs, in-tonational phrased and discourse markers: Modelingspeakers?
utterances in spoken dialogue.
Computa-tional Linguistics.K.
W. Hunt.
1970.
Syntactic maturity in school chil-dren and adults.
In Monographs of the Society for Re-44search in Child Development.
University of ChicagoPress, Chicago, IL.N.
Iwashita.
2006.
Syntactic complexity measures andtheir relation to oral proficiency in Japanese as a for-eign language.
Language Assessment Quarterly: AnInternational Journal, 3(2):151?169.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random field: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof the International Conference on Machine Learning(ICML).J.
R Landis and G. G Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,pages 159?174.P.
Lennon.
1990.
Investigating fluency in EFL: A quanti-tative approach.
Language Learning, 40(3):387?417.Y.
Liu, A. Stolcke, E. Shriberg, and M. Harper.
2004.Comparing and combining generative and poste-rior probability models: Some advances in sentenceboundary detection in speech.
In Proceedings of theEmpirical Methods in Natural Language Processing(EMNLP).Y.
Liu, E. Shriberg, A. Stolcke, B. Peskin, J. Ang, HillardD., M. Ostendorf, M. Tomalin, P. Woodland, andM.
Harper.
2005.
Structural Metadata Research in theEARS Program.
In Proceedings of the InternationalConference of Acoustics, Speech, and Signal Process-ing (ICASSP).Y.
Liu.
2004.
Structural Event Detection for Rich Tran-scription of Speech.
Ph.D. thesis, Purdue University.X.
Lu.
2009.
Automatic measurement of syntactic com-plexity in child language acquisition.
InternationalJournal of Corpus Linguistics, 14(1):3?28.A.
McCallum.
2005.
Mallet: A machine learning toolkitfor language.
http://mallet.cs.umass.edu.G.
J. Mizera.
2006.
Working memory and L2 oral flu-ency.
Ph.D. thesis, University of Pittsburgh.L.
Neumeyer, H. Franco, V. Digalakis, and M. Weintraub.2000.
Automatic Scoring of Pronunciation Quality.Speech Communication, 30:83?93.M.
Ostendorf, B. Favre, R. Grishman, D. Hakkani-Tur, M. Harper, D. Hillard, J. Hirschberg, HengJi, J.G.
Kahn, Yang Liu, S. Maskey, E. Matusov,H.
Ney, A. Rosenberg, E. Shriberg, Wen Wang, andC.
Woofers.
2008.
Speech segmentation and spokendocument processing.
Signal Processing Magazine,IEEE, 25(3):59?69, May.M.
Roll, J. Frid, and M. Horne.
2007.
Measuring syntac-tic complexity in spontaneous spoken Swedish.
Lan-guage and Speech, 50(2):227.K.
Sagae, A. Lavie, and B. MacWhinney.
2005.
Auto-matic measurement of syntactic development in childlanguage.
In Proc.
of ACL, volume 100.E.
Shriberg and A. Stolcke.
2004.
Direct modeling ofprosody: An overview of applications in automaticspeech processing.
In Proceedings of the InternationalConference on Speech Prosody.E.
Shriberg, A. Stolcke, D. Hakkani-Tur, and G. Tur.2000.
Prosody-based automatic segmentation ofspeech into sentences and topics.
Speech Communi-cation, 32(1-2):127?154.K.
Zechner, D. Higgins, and Xiaoming Xi.
2007.SpeechRater: A Construct-Driven Approach to Scor-ing Spontaneous Non-Native Speech.
In Proc.
SLaTE.L.
Zhang.
2005.
Maximum Entropy Model-ing Toolkit for Python and C++.
http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html.45
