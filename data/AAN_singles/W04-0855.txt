UBB system at Senseval3Gabriela SerbanDepartment of Computer ScienceUniversity ?Babes-Bolyai?Romaniagabis@cs.ubbcluj.roDoina TatarDepartment of Computer ScienceUniversity ?Babes-Bolyai?Romaniadtatar@cs.ubbcluj.roAbstractIt is known that whenever a system?s actionsdepend on the meaning of the text being pro-cessed, disambiguation is beneficial or even nec-essary.
The contest Senseval is an internationalframe where the research in this important fieldis validated in an hierarchical manner.
In thispaper we present our system participating forthe first time at Senseval 3 contest on WSD,contest developed in March-April 2004.
Wepresent also our intentions on improving oursystem, intentions occurred from the study ofresults.1 IntroductionWord Sense Disambiguation (WSD) is the pro-cess of identifying the correct meanings of wordsin particular contexts (Manning and Schutze,1999).
It is only an intermediate task in NLP,like POS tagging or parsing.
Examples of finaltasks are Machine Translation, Information Ex-traction or Dialogue systems.
WSD has been aresearch area in NLP for almost the beginningof this field due to the phenomenon of polysemythat means multiple related meanings with asingle word (Widdows, 2003).
The most im-portant robust methods in WSD are: machinelearning methods and dictionary based meth-ods.
While for English exist some machine read-able dictionaries, the most known being Word-Net (Christiane Fellbaum, 1998), for Romanianuntil now does not exist any.
Therefore for ourapplication we used the machine learning ap-proach.2 Machine learning approach inWSDOur system falls in the supervised learning ap-proach category.
It was trained to learn a clas-sifier that can be used to assign a yet unseen ex-ample to one or two of a fixed number of senses.We had a trained corpus (a number of annotatedcontexts), from where the system learned theclassifier, and a test corpus which the systemwill annotate.In our system we used the Vector SpaceModel: a context c was represented as a vec-tor ~c of some features which we will present bel-low.
By a context we mean the same definitionas in Senseval denotation: the content between?context?
and ?/context?.The notations used to explain our method are(Manning and Schutze, 1999):?
w - the word to be disambiguate;?
s1, ?
?
?
, sNs the senses for w;?
c1, ?
?
?
, cNc the contexts for w;?
v1, ?
?
?
, vNf the features selected.As we treated each word w to be disam-biguated separately, let us explain the methodfor a single word.
The features selected wasthe set of ALL words used in the trained corpus(nouns, verbs, prepositions, etc) , so we used thecooccurrence paradigm (Dagan, Lee and Pereira, 1994).The vector of a context c of the target wordw is defined as:?
~c = (w1, ?
?
?
, w|W |) where wi is the numberof occurences of the word vi in the contextc and vi is a word from the entire trainedcorpus of | W | words.The similarity between two contexts ca, cb isthe normalised cosine between the vectors ~caand ~cb (Jurafsky and Martin, 2000):cos(~ca, ~cb) =?mj=1 wa,j ?
wb,j?
?mj=1 w2a,j ?
?mj=1 w2b,jand sim(~ca, ~cb) = cos(~ca, ~cb).Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of SystemsThe number wi is the weight of the featurevi.
This can be the frequency of the feature vi(term frequency or tf), or ?inverse documentfrequency ?, denoted by idf .
In our system weconsidered all the words from the entire corpus,so both these aspects are satisfied.3 k-NN or memory based learningAt training time, our k-NN model memorizes allthe contexts in the training set by their associ-ated features.
Later, when proceeds a new con-text cnew, the classifier first selects k contextsin the training set that are closest to cnew, thenpick the best sense (senses) for cnew (Jacksonand Moulinier, 2002).?
TRAINING: Calculate ~c for each context c.?
TEST: CalculateStep1.A = {~c | sim( ~cnew,~c) ismaxim, | A |= k}that means A is the set of the k nearestneighbors contexts of ~cnew.Step2.Score(cnew, sj) =?~ci?A(sim( ~cnew, ~ci)?
aij)where aij is 1 if ~ci has the sense sj and aijis 0 otherwise.Step3.
Finally,s?
= argmaxjScore(cnew, sj).We used the value of k set to 3 after someexperimental verifications.A major problem with supervised approachesis the need for a large sense tagged training set.The bootstrapping methods use a small numberof contexts labeled with senses having a highdegree of confidence.These labeled contexts are used as seeds totrain an initial classifier.
This is then used toextract a larger training set from the remain-ing untagged contexts.
Repeating this process,the number of training contexts grows and thenumber of untagged contexts reduces.
We willstop when the remaining unannotated corpus isempty or any new context can?t be annotated.In (Tatar and Serban, 2001), (Serban and Tatar,2003) we presented an algorithm which falls inthis category.
The algorithm is based on the twoprinciples of Yarowsky (Resnik and Yarowsky,1999):?
One sense per discourse: the sense of a tar-get word is highly consistent within a givendiscourse (document);?
One sense per collocation: the contextualfeatures ( nearby words) provide strongclues to the sense of a target word.Also, for each iteration, the algorithm usesa NBC classifier.
We intend to present a sec-ond system based on this algorithm at the nextSenseval contest.4 Implementation detailsOur disambiguation system is written in JDK1.4.In order to improve the performance of thedisambiguation algorithm, we made the follow-ing refinements in the above k-NN algorithm.First one is to substitute the lack of an efficienttool for stemming words in Romanian.1.
We defined a relation between words as ?
:W ?
W , where W is the set of words.
Ifw1 ?
W and w2 ?
W are two words, wesay that (w1, w2) ?
?
if w1 and w2 havethe same grammatical root.
Therefore, ifw is a word and C is a context, we say thatw occurs in C iff exists a word w2 ?
Cso that (w,w2) ?
?.
In other words, wereplaced the stemming step with collectingall the words with the same root in a singleclass.
This collection is made consideringthe rules for romanian morphology;2.
The step 3 of the algorithm for choosingthe appropriate sense (senses) of a poly-semic word w in a given context C (infact the sense that maximizes the set S ={Score(C, sj) | j = 1, ?
?
?Ns} of scores forC) is divided in three sub-steps:?
If there is a single sense s that maxi-mizes S, then s is reported as the ap-propriate sense for C;?
If there are two senses s1 and s2 thatmaximize S, then s1 and s2 are re-ported as the appropriate senses for C;?
Consider that Max1 and Max2 arethe first two maximum values from Swhere (Max1 > Max2).
If Max1 isobtained for a sense s1 and if Max2 isobtained for a sense s2 and ifMax1?Max2 ?
Pwhere P = Max1?Min(Ns?1) and Min is theminimum score from S, then s1 and s2are reported as the appropriate sensesfor C.Experimentally, we proved that the above im-provements grow the precision of the disam-biguation process.5 Conclusions after the evaluationCoarse-grained score for our system UBB usingkey ?EVAL/RomanianLS.test.key?
was:precision: 0.722 (2555.00 correct of 3541.00attempted)recall: 0.722 (2555.00 correct of 3541.00 intotal)attempted: 100.00Fine-grained score was:precision: 0.671 (2376.50 correct of 3541.00attempted)recall: 0.671 (2376.50 correct of 3541.00 intotal)attempted: 100.00Considering as baseline procedure the major-ity sense (all contexts are solved with the mostfrequent sense in the training corpus), for theword nucleu (noun) is obtained a precision of0,78 while our procedure obtained 0,81.
Also,for the word desena (verb) the baseline proce-dure of the majority sense obtains precision 0,81while our procedure obtained 0,85.At this stage our system has not as a goal tolabel with U (unknown) a context, every timechoosing one or two from the best scored senses.Annotating with the label U is one of our com-ing improving.
This can be done simply byadding as a new sense for each word the senseU .
A simple experiment reported a number ofright annotated contexts.Another direction to improve our system isto exploit better the senses as they are done intraining corpus: our system simply consider thefirst sense.ReferencesI.
Dagan, L. Lee and F. C. N. Pereira.
1994.Similarity-based Estimation of Word Cooc-curences Probabilities.
Meeting of the Asso-ciation for Computational Linguistics, 272?278.Christiane Fellbaum.
1998.
WordNet: An elec-tronic lexical database.
The MIT Press.P.
Jackson and I. Moulinier.
2002.
NaturalLanguage Processing for Online Applications.John Benjamin Publ.
Company.D.
Jurafsky and J. Martin.
2000.
Speech andlanguage processing.
Prentice-Hall, NJ.C.
Manning and H. Schutze.
1999.
Foundationof statistical natural language processing.
TheMIT Press.Ruslan Mitkov,editor.
2002 The Oxford Hand-book of Computational Linguistics.
OxfordUniversity Press.P.
Resnik and D. Yarowsky.
1999.
Distinguish-ing Systems and Distinguishing sense: newevaluation methods for WSD.
Natural Lan-guage Engineering, 5(2):113-134.G.
Serban and D. Tatar.
2003.
Word Sense Dis-ambiguation for Untagged Corpus: Applica-tion to Romanian Language.
CICLing-2003,LNCS 2588, 270?275.D.
Tatar and G. Serban.
2001.
A new algorithmfor WSD.
Studia Univ.
?Babes-Bolyai?, In-formatica, 2 99?108.D.
Widdows.
2003.
A mathematical model forcontext and word meaning.
Fourth Interna-tional Conference on Modeling and using con-text, Stanford, California, June 23-25.
