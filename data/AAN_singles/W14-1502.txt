Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 11?20,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsDistributional Composition using Higher-Order Dependency VectorsJulie Weeds, David Weir and Jeremy ReffinDepartment of InformaticsUniversity of SussexBrighton, BN1 9QH, UK{J.E.Weeds, D.J.Weir, J.P.Reffin}@sussex.ac.ukAbstractThis paper concerns how to apply compo-sitional methods to vectors based on gram-matical dependency relation vectors.
Wedemonstrate the potential of a novel ap-proach which uses higher-order grammat-ical dependency relations as features.
Weapply the approach to adjective-noun com-pounds with promising results in the pre-diction of the vectors for (held-out) ob-served phrases.1 IntroductionVector space models of semantics characterise themeaning of a word in terms of distributional fea-tures derived from word co-occurrences.
The mostwidely adopted basis for word co-occurrence isproximity, i.e.
that two words (or more generallylexemes) are taken to co-occur when they occurtogether within a certain sized window, or withinthe same sentence, paragraph, or document.
Lin(1998), in contrast, took the syntactic relationshipbetween co-occurring words into account: the dis-tributional features of a word are based on theword?s grammatical dependents as found in a de-pendency parsed corpus.
For example, observingthat the word glass appears as the indirect objectof the verb fill, provides evidence that the wordglass has the distributional feature iobj:fill, whereiobj denotes the inverse indirect object grammati-cal relation.
The use of grammatical dependents asword features has been exploited in the discoveryof tight semantic relations, such as synonymy andhypernymy, where an evaluation against a goldstandard such as WordNet (Fellbaum, 1998) canbe made (Lin, 1998; Weeds and Weir, 2003; Cur-ran, 2004).Pado and Lapata (2007) took this further byconsidering not just direct grammatical depen-dents, but also including indirect dependents.Thus, observing the sentence She filled her glassslowly would provide evidence that the word glasshas the distributional feature iobj:advmod:slowlywhere iobj:advmod captures the indirect depen-dency relationship between glass and slowly in thesentence.Note that Pado and Lapata (2007) includeda basis mapping function that gave their frame-work flexibility as to how to map paths such asiobj:advmod:slowly onto the basis of the vectorspace.
Indeed, the instantiation of their frameworkthat they adopt in their experiments uses a ba-sis mapping function that removes the dependencypath to leave just the word, so iobj:advmod:slowlywould be mapped to slowly.In this paper, we are concerned with the prob-lem of distributional semantic composition.
Weshow that the idea that the distributional seman-tics of a word can be captured with higher-orderdependency relationships, provides the basis fora simple approach to compositional distributionalsemantics.
While our approach is quite gen-eral, dealing with arbitrarily high-order depen-dency relationships, and the composition of ar-bitrary phrases, in this paper we consider onlyfirst and second order dependency relations, andadjective-noun composition.In Section 2, we illustrate our proposal byshowing how second order dependency relationscan play a role in computing the semantics ofadjective-noun composition.
In Section 3 we de-scribe a number of experiments that are intendedto evaluate the approach, with the results presentedin Section 4.The basis for our evaluation follows Baroni and11Zamparelli (2010) and Guevara (2010).
Typically,compositional distributional semantic models canbe used to generate an (inferred) distributionalvector for a phrase from the (observed) distribu-tional vectors of the phrase?s constituents.
Oneof the motivations for doing this is that the ob-served distributional vectors for most phrases tendto be very sparse, a consequence of the frequencywith which typical phrases occur in even large cor-pora.
However, there are phrases that occur suffi-ciently frequently that a reasonable characterisa-tion of their meaning can be captured with theirobserved distributional vector.
Such phrases canbe exploited in order to assess the quality of amodel of composition.
This is achieved by mea-suring the distributional similarity of the observedand inferred distributional vectors for these highfrequency phrases.The contributions of this paper are as follows.We propose a novel approach to phrasal composi-tion which uses higher order grammatical depen-dency relations as features.
We demonstrate itspotential in the context of adjective-noun compo-sition by comparing (held-out) observed and in-ferred phrasal vectors.
Further, we compare dif-ferent vector operations, different feature associa-tion scores and investigate the effect of weightingfeatures before or after composition.2 Composition with Higher-orderDependenciesConsider the problem of adjective-noun compo-sition.
For example, what is the meaning of thephrase small child?
How does it relate to themeanings of the lexemes small and child?
Figure 1shows a dependency analysis for the sentence Thevery small wet child cried loudly.
Tables 1 and2 show the grammatical dependencies (with otheropen-class words) for the lexemes small and childwhich would be extracted from it.the/D very/R small/J wet/J child/N cry/V loudly/Ramodamodadvmoddetnsubj advmodFigure 1: Example Dependency TreeFrom Table 1 we see what kinds of (higher-order) dependency paths appear in the distribu-tional features of adjectives such as small.
Simi-larly, Table 2 indicates this for nouns such as child.1st-order advmod:very/Ramod:child2nd-order amod:amod:wet/Jamod:nsubj:cry/V3rd-order amod:nsubj:advmod:loudly/RTable 1: Grammatical Dependencies of small1st-order amod:wet/Jamod:small/Jnsubj:cry/V2nd-order amod:advmod:very/Rnsubj:advmod:loudly/RTable 2: Grammatical Dependencies of childIt is clear that with a conventional grammaticaldependency-based approach where only first or-der dependencies for small and child would beconsidered, there will be very little overlap be-tween the features of nouns and adjectives becausequite different grammatical relations are used inthe two types of vectors, and correspondingly lex-emes with different parts of speech appear at theend of these paths.However, as our example illustrates, it is possi-ble to align the 2nd-order feature space of adjec-tives with the 1st-order feature space of nouns.
Inthis example, we have evidence that children cryand that small things cry.
Consequently, in orderto compose an adjective with a noun, we wouldwant to align 2nd-order features of the adjectivewith 1st-order features of the noun; this gives us aprediction of the first order features of the noun inthe context of the adjective1.This idea extends in a straightforward way be-yond adjective-noun composition.
For example, itis possible to align the 3rd order features of ad-jectives with 2nd order features of nouns, which issomething that would be useful if one wanted tocompose verbs with their arguments.
These argu-ments will include adjective-noun compounds andtherefore adjective-noun compounds require 2nd-order features which can be aligned with the firstorder features of the verbs.
This is, however, not1Note that it would also be possible to align 2nd-orderfeatures of the noun with 1st-order features of the adjective,resulting in a prediction of the first order features of the ad-jective in the context of the noun.12something that we will pursue further in this paper.We now clarify how features vectors are alignedand then composed.
Suppose that the lexemes w1and w2which we wish to compose are connectedby relation r. Let w1be the head of the relationand w2be the dependent.
In our example, w1ischild, w2is small and r is amod.
We first pro-duce a reduced vector for w2which is designedto lie in a comparable feature space as the vectorfor w1.
To do this we take the set of 2nd orderfeatures of w2which start with the relation r?
andreduce them to first order features (by removingthe r?
at the start of the path).
So in our example,we create a reduced vector for small where fea-tures amod:nsubj:x for some token x are reducedto nsubj:x, features amod:amod:x for some tokenx are reduced to the feature amod:x, and featuresamod:nsubj:advmod:x for some token x are re-duced to nsubj:advmod:x.
Once the vector for w2has been reduced, it can be composed with the vec-tor for w1using standard vector operations.In Section 3 we describe experiments that ex-plore the effectiveness of this approach to distri-butional composition by measuring the similarityof composed vectors with observed vectors for aset of frequently occurring adjective-noun pairs(details given below).
We evaluate a number ofinstantiations of our approach, and in particular,there are three aspects of the model where alter-native solutions are available: the choice of whichvector composition operation to use; the choice ofhow to weight dependency features; and the ques-tion as to whether feature weighting should takeplace before or after composition.Vector composition operation.
We considereach of the following seven alternatives: pointwiseaddition (add), pointwise multiplication (mult),pointwise geometric mean2(gm), pointwise max-imum (max), pointwise minimum (min), first ar-gument (hd), second argument (dp).
The lattertwo operations simply return the first (respectivelysecond) of the input vectors.Feature weighting.
We consider three options.Much work in this area has used positive pointwisemutual information (PPMI) (Church and Hanks,1989) to weight the features.
However, PPMI isknown to over-emphasise low frequency events,and as a result there has been a recent shift to-wards using positive localised mutual information2The geometric mean of x and y is?
(x ?
y).PPMI(x, y) ={I(x, y) if I(x, y) > 00 otherwisewhere I(x, y) = logP (x,y)P (x).P (y)PLMI(x, y) ={L(x, y) if L(x, y) > 00 otherwisewhere L(x, y) = P (x, y).log(P (x,y)P (x).P (y)PNPMI(x, y) ={N(x, y) if N(x, y) > 00 otherwisewhere N(x, y) =1?log(P (y).logP (x,y)P (x).P (y)Table 3: Feature Association Scores(PLMI) (Scheible et al., 2013) and positive nor-malised point wise mutual information (PNPMI)(Bouma, 2009).
For definitions, see Table 3.Timing of feature weighting.
We consider twoalternatives: we can weight features before com-position so that the composition operation is ap-plied to weighted vectors, or we can compose vec-tors prior to feature weighting, in which case thecomposition operation is applied to unweightedvectors, and feature weighting is applied in thecontext of making a similarity calculation.
In otherwork, the former order is often implied.
For exam-ple, Boleda et al.
(2013) state that they use ?PMIto weight the co-occurrence matrix?.
However, ifwe allow the second order, features which mighthave a zero association score in the context of thethe individual lexemes, could be considered sig-nificant in the context of the phrase.3 EvaluationOur experimental evaluation of the approach isbased on the assumption, which is commonlymade elsewhere, that where there is a reasonableamount of corpus data available for a phrase, thiswill generate a good estimate of the vector of thephrase.
It has been shown (Turney, 2012; Baroniand Zamparelli, 2010) that such ?observed?
vec-tors are indeed reasonable for adjective-noun andnoun-noun compounds.
Hence, in order to evalu-ate the compositional models under considerationhere, we compare observed phrasal vectors withinferred phrasal vectors, where the comparison ismade using the cosine measure.
We note that it is13not possible to draw conclusions from the absolutevalue of the cosine score since this would favourmodels which always assign higher cosine scores.Hence, we draw conclusions from the change incosine score with respect to a baseline within thesame model.MethodologyFor each noun and adjective which occur morethan a threshold number of times in a corpus, wefirst extract conventional first order dependencyvectors.
The features of these lexemes define thesemantic space, and feature probabilities (for usein association scores) are calculated from this data.Given a list of adjective-noun phrases, we ex-tract first order vectors for the nouns and secondorder vectors for the adjectives, which we refer toas observed constituent vectors.
We also extractfirst order vectors for the nouns in the context ofthe adjective, which we refer to as the observedphrasal vector.For each adjective-noun pair, we build bespokeconstituent vectors for the adjective and noun, inwhich we remove all counts which arise from co-occurrences with that specific adjective-noun pair.It is these constituent vectors that are used as thebasis for inferring the vector for that particularadjective-noun phrase.Our rationale for this is as follows.
Without thismodification, the observed constituent vectors willcontain co-occurrences which are due to the ob-served adjective-noun vector co-occurrences.
Tosee why this is undesirable, suppose that one of theadjective-noun phrases was small child.
We takethe observed vector for small child to be what weare calling the observed phrasal vector for child (inthe context of small).
Suppose that when buildingthe observed phrasal vector, we observe the phrasethe small child cried.
This will lead to a count forthe feature nsubj:cry in the observed phrasal vec-tor for child.But if we are not careful, this same phrase willcontribute to counts in the constituent vectors forsmall and child, producing counts for the featuresamod:nsubj:cry and nsubj:cry, in their respectivevectors.
To see why these counts should not be in-cluded when building the constituent vectors thatwe compose to produce inferred vectors for theadjective-noun phrase small child, consider thecase where all of the evidence for small things be-ing things that can cry and children being thingsthat can crying comes from having observed thephrase small children crying.
Despite not havinglearnt anything about the composition of small andchild in general, we would be able to infer the cryfeature for the phrase.
An adequate model of com-position should be able to infer this on the basisthat other small things have been seen to cry, andthat non-small children have been seen to cry.Here, we compare the proposed approach,based on higher order dependencies, with thestandard method of composing conventional first-order dependency vectors.
The vector operation,hd provides a baseline for comparison which isthe same in both approaches.
This baseline corre-sponds to a composition model where the first or-der dependencies of the phrase (i.e.
the noun in thecontext of the adjective) are taken to be the sameas the first order dependencies of the uncontextu-alized noun.
For example, if we have never seenthe phrase small child before, we would assumethat it means the same as the head word child.We hypothesise that it is not possible to im-prove on this baseline using traditional first-orderdependency relation vectors, since the vector forthe modifier does not contain features of the righttype, but that with the proposed approach, the in-ferred vector for a phrase such as small child willbe closer than observed vector for child to the ob-served vector for small child.
We also ask the re-lated question of whether our inferred vector forsmall child is closer than the constituent vector forsmall to the observed vector for small child.
Thiscomparison is achieved through use of the vectoroperation dp that ignores the vector for the head,simply returning a first-order vector derived fromthe dependent.Experimental SettingsOur corpus is a mid-2011 dump of WikiPedia.This has been part-of-speech tagged, lemmatisedand dependency parsed using the Malt Parser(Nivre, 2004).
All major grammatical dependencyrelations involving open class parts of speech(nsubj, dobj, iobj, conj, amod, advmod, nnmod)have been extracted for all POS-tagged and lem-matised nouns and adjectives occurring 100 ormore times.
In past work with conventional de-pendency relation vectors we found that using afeature threshold of 100, weighting features withPPMI and a cosine similarity score work well.For experimental purposes, we have taken14spanish british african japanesemodern classical female naturaldigital military medical musicalscientific free black whiteheavy common small largestrong short long goodsimilar previous future originalformer subsequent next possibleTable 4: Adjectives considered32 of the most frequently occurring adjectives(see Table 4).
These adjectives include oneswhich would generally be considered intersective(e.g., female), subsective (e.g,, long) and non-subsective/intensional (e.g., former) (Pustejovsky,2013) .
For all of these adjectives there are at least100 adjective-noun phrases which occur at least100 times in the corpus.
We randomly selected50 of the phrases for each adjective.
Note thatour proposed method does not require any hyperparameters to be set during training, nor does itrequire a certain number of phrases per adjective.For the purpose of these experiments we have a listof 1600 adjective-noun phrases, all of which occurat least 100 times in WikiPedia.4 Results and DiscussionTables 5 and 6 summarise the average cosines forthe proposed higher-order dependency approachand the conventional first-order dependency ap-proach, respectively.
In each case, we considereach combination of vector operation, feature as-sociation score, and composition timing (i.e.
be-fore, or after, vector weighting).Table 7 shows the average improvement overthe baseline (hd), for each combination of exper-imental variables, when considering the proposedhigher-order dependency approach.
Note that thisis an average of paired differences (and not the dif-ference of the averages in Table 6).
For brevity, weomit the results for PNPMI here, since there do notappear to be substantial differences between usingPPMI and PNPMI.
To indicate statistical signifi-cance, we show estimated standard errors in themeans.
All differences are statistically significant(under a paired t-test) except those marked ?.From Table 5, we see that none of the com-positional operations on conventional dependencyvectors are able to beat the baseline of selectingthe head vector (hd).
This is independent of thechoice of association measure and the order inwhich weighting and composition are carried out.For the higher order dependency vectors (Tables6 and 7), we note, in contrast, that some com-positional operations produce large increases incosine score compared to the head vector alone(hd).
Table 7 examines the statistical significanceof these differences.
We find that for the inter-sective composition operations (mult, min, andgm), performance is statistically superior to usingthe head alone in all experimental conditions stud-ied.
By contrast, additive measures (add, max)typically have no impact, or decrease performancemarginally relative to the head alone.
An explana-tion for these significant differences is that inter-sective vector operations are able to encapsulatethe way that an adjective disambiguates and spe-cialises the sense of the noun that it is modifying.We also note that the alternative baseline, dp,which estimates the features of a phrase to be theaggregation of all things which are modified bythe adjective, performs significantly worse thanthe standard baseline, hd, which estimates the fea-tures of a phrase to be the features of the headnoun.
This is consistent with the intuition that thedistributional vector for small child should moresimilar to the vector for child than it is to the vec-tor for the things that can be small.Considering the different intersective opera-tions, mult appears to be the best choice whenthe feature association score is PPMI or PNPMIand gm appears to be the best choice when the fea-ture association score is PLMI.Further, PLMI consistently gives all of the vec-tor pairings higher cosine scores than PPMI.
SincePLMI assigns less weight to low frequency eventand more weight to high frequency events, thissuggests that all of the composition methods, in-cluding the baseline (hd), do better at predictingthe high frequency co-occurrences.
This is not sur-prising as these will more likely have been seenwith the phrasal constituents in other contexts.Our final observation, based on Table 6, is thatthe best order in which to carry out weighting andcomposition appears to depend on the choice offeature association score.
In general, it appearsbetter to weight the features and then composevectors.
This is always true when using PNPMIor PLMI.
However, using PPMI, the highest per-formance is achieved by composing the raw vec-tors using multiplication and then weighing the15weight:compose compose:weightPPMI PNPMI PLMI PPMI PNPMI PLMIx?
s x?
s x?
s x?
s x?
s x?
sadd 0.12 (0.06) 0.13 (0.05) 0.15 (0.16) 0.11 (0.05) 0.12 (0.06) 0.22 (0.20)max 0.12 (0.06) 0.13 (0.05) 0.15 (0.16) 0.11 (0.05) 0.12 (0.06) 0.22 (0.20)mult 0.06 (0.05) 0.06 (0.06) 0.06 (0.11) 0.07 (0.05) 0.07 (0.12) 0.07 (0.05)min 0.05 (0.05) 0.06 (0.05) 0.04 (0.09) 0.05 (0.04) 0.05 (0.04) 0.04 (0.08)gm 0.06 (0.05) 0.06 (0.05) 0.07 (0.11) 0.05 (0.04) 0.06 (0.04) 0.08 (0.11)hd 0.13 (0.07) 0.15 (0.07) 0.28 (0.22) 0.13 (0.07) 0.15 (0.07) 0.28 (0.22)Table 5: Means and Standard Deviations for Cosines Between Observed and Predicted Vectors for Con-ventional First-Order Dependency Based Approach.weight:compose compose:weightPPMI PNPMI PLMI PPMI PNPMI PLMIx?
s x?
s x?
s x?
s x?
s x?
sadd 0.14 (0.06) 0.16 (0.06) 0.29 (0.21) 0.10 (0.04) 0.12 (0.05) 0.29 (0.22)max 0.10 (0.04) 0.11 (0.04) 0.27 (0.21) 0.10 (0.04) 0.11 (0.04) 0.26 (0.21)mult 0.30 (0.12) 0.33 (0.12) 0.40 (0.29) 0.34 (0.10) 0.32 (0.10) 0.32 (0.27)min 0.26 (0.11) 0.27 (0.11) 0.40 (0.24) 0.24 (0.10) 0.25 (0.10) 0.37 (0.23)gm 0.27 (0.11) 0.29 (0.11) 0.46 (0.20) 0.26 (0.10) 0.27 (0.10) 0.44 (0.22)dp 0.10 (0.05) 0.10 (0.05) 0.20 (0.20) 0.10 (0.05) 0.10 (0.05) 0.20 (0.20)hd 0.13 (0.07) 0.15 (0.07) 0.28 (0.22) 0.13 (0.07) 0.15 (0.07) 0.28 (0.22)Table 6: Means and Standard Deviations for Cosines Between Observed and Predicted Vectors for Pro-posed Higher-Order Dependency Based Approachremaining features.
This can be explained byconsidering the recall and precision of the com-posed vector?s prediction of the observed vec-tor.
If we compose using gm before weightingvectors, we increase the recall of the prediction,but decrease precision.
Whether we use PPMI,PNPMI or PLMI, recall of features increases from88.8% to 99.5% and precision drops from 5.5% to4.8%.
If we compose using mult before weight-ing vectors, contrary to expectation, recall de-creases and precision increases.
Whether we usePPMI, PNPMI or PLMI, recall of features de-creases from 88.8% to 59.4% but precision in-creases from 5.5% to 18.9%.
Hence, multiplica-tion of the raw vectors is causing a lot of potentialshared features to be ?lost?
when the weightingis subsequently carried out (since multiplicationstretches out the value space).
This leads to anincrease in cosines when PPMI is used for weight-ing, and a decrease in cosines when PLMI is used.Hence, it appears that the features being removedby multiplying the raw vectors before weightingmust be low frequency co-occurrences, which arenot observed with the phrase.5 Related WorkIn this work, we bring together ideas from sev-eral different strands of distributional semantics:incorporating syntactic information into the distri-butional representation of a lexeme; representingphrasal meaning by creating distributional repre-sentations through composition; and representingword meaning in context by modifying the distri-butional representation of a word.The use of syntactic structure in distributionalrepresentations is not new.
Two of the earliestproponents of distributional semantics, Lin (1998)and Lee (1999) used features based on first orderdependency relations between words in their dis-tributional representations.
More recently, Padoand Lapata (2007) propose a semantic space basedon dependency paths.
This model outperformedtraditional word-based models which do not takesyntax into account in a synonymy relation detec-tion task and a prevalent sense acquisition task.The problem of representing phrasal meaninghas traditionally been tackled by taking vector rep-resentations for words (Turney and Pantel, 2010)and combining them using some function to pro-16weight:compose compose:weightPPMI PLMI PPMI PLMIx?
sx?x?
sx?x?
sx?x?
sx?add 0.01 (0.001) ?0.004 (0.003) -0.03 (0.001) ?0.006 (0.004)max -0.03 (0.001) -0.01 (0.003) -0.04 (0.001) -0.02 (0.003)mult 0.16 (0.002) 0.11 (0.006) 0.21 (0.002) 0.03 (0.006)min 0.13 (0.001) 0.11 (0.007) 0.10 (0.001) 0.09 (0.007)gm 0.14 (0.001) 0.18 (0.005) 0.12 (0.001) 0.16 (0.005)dp -0.03 (0.002) -0.09 (0.007) -0.04 (0.002) -0.09 (0.007)Table 7: Means and Standard Errors for Increases in Cosine with respect to the hd Baseline for ProposedHigher-Order Dependency Based Approach.
All differences statistically significant (under a paired t-test) except those marked ?.duce a data structure that represents the phraseor sentence.
Mitchell and Lapata (2008, 2010)found that simple additive and multiplicative func-tions applied to proximity-based vector represen-tations were no less effective than more com-plex functions when performance was assessedagainst human similarity judgements of simplepaired phrases.The simple functions evaluated by Mitchell andLapata (2008) are generally acknowledged to haveserious theoretical limitations in their treatmentof composition.
How can a commutative func-tion such as multiplication or addition provide dif-ferent interpretations for different word orderingssuch as window glass and glass window?
Themajority of attempts to rectify this have offereda more complex, non-commutative function ?such as weighted addition ?
or taken the viewthat some or all words are no longer simple vec-tors.
For example, in the work of Baroni andZamparelli (2010) and Guevara (2010), an adjec-tive is viewed as a modifying function and rep-resented by a matrix.
Coecke et al.
(2011) andGrefenstette et al.
(2013) also incorporate the no-tion of function application from formal seman-tics.
They derived function application from syn-tactic structure, representing functions as tensorsand arguments as vectors.
The MV-RNN modelof Socher et al.
(2012) broadened the Baroni andZamparelli (2010) approach; all words, regardlessof part-of-speech, were modelled with both a vec-tor and a matrix.
This approach also shared fea-tures with Coecke et al.
(2011) in using syntaxto guide the order of phrasal composition.
Thesehigher order structures are typically learnt or in-duced using a supervised machine learning tech-nique.
For example, Baroni and Zamparelli (2010)learnt their adjectival matrixes by performing re-gression analysis over pairs of observed nouns andadjective-noun phrases.
As a consequence of thecomputational expense of the machine learningtechniques involved, implementations of these ap-proaches typically require a considerable amountof dimensionality reduction.A long-standing topic in distributional seman-tics has been the modification of a canonical repre-sentation of a lexeme?s meaning to reflect the con-text in which it is found.
Typically, a canonicalvector for a lexeme is estimated from all corpusoccurrences and the vector then modified to reflectthe instance context (Lund and Burgess, 1996;Erk and Pad?o, 2008; Mitchell and Lapata, 2008;Thater et al., 2009; Thater et al., 2010; Thater etal., 2011; Van de Cruys et al., 2011; Erk, 2012).As described in Mitchell and Lapata (2008, 2010),lexeme vectors have typically been modified usingsimple additive and multiplicative compositionalfunctions.
Other approaches, however, share withour proposal the use of syntax to drive modifica-tion of the distributional representation (Erk andPad?o, 2008; Thater et al., 2009; Thater et al., 2010;Thater et al., 2011).
For example, in the SVS rep-resentation of Erk and Pad?o (2008), a word wasrepresented by a set of vectors: one which en-codes its lexical meaning in terms of distribution-ally similar words3, and one which encodes theselectional preferences of each grammatical rela-tion it supports.
A word?s meaning vector was up-dated in the context of another word by combiningit with the appropriate selectional preferences vec-3These are referred to as second-order vectors usingthe terminology of Grefenstette (1994) and Sch?utze (1998).However, this refers to a second-order affinity between thewords and is not related to the use of grammatical depen-dency relations.17tor of the contextualising word.Turney (2012) offered a model of phrasal levelsimilarity which combines assessments of word-level semantic relations.
This work used twodifferent word-level distributional representationsto encapsulate two types of similarity.
Distribu-tional similarity calculated from proximity-basedfeatures was used to estimate domain similarityand distributional similarity calculated from syn-tactic pattern based features is used to estimatefunctional similarity.
The similarity of a pair ofcompound noun phrases was computed as a func-tion of the similarities of the components.
Cru-cially different from other models of phrasal levelsimilarity, it does not attempt to derive modifiedvectors for phrases or words in context.6 Conclusions and Further WorkVectors based on grammatical dependency rela-tions are known to be useful in the discovery oftight semantic relations, such as synonymy andhypernymy, between lexemes (Lin, 1998; Weedsand Weir, 2003; Curran, 2004).
It would be use-ful to be able to extend these methods to deter-mine similarity between phrases (of potentiallydifferent lengths).
However, conventional ap-proaches to composition, which have been ap-plied to proximity-based vectors, cannot sensiblybe used on vectors that are based on grammaticaldependency relations.In our approach, we consider the vector for aphrase to be the vector for the head lexeme inthe context of the other phrasal constituents.
LikePado and Lapata (2007), we extend the conceptof a grammatical dependency relation feature toinclude dependency relation paths which incor-porate higher-order dependencies between words.We have shown how it is possible to align the de-pendency path features for words of different syn-tactic types, and thus produce composed vectorswhich predict the features of one constituent in thecontext of the other constituent.In our experiments with AN compounds, wehave shown that these predicted vectors are closerthan the head constituent?s vector to the observedphrasal vector.
We have shown this is true evenwhen the observed phrase is in fact unobserved,i.e.
when its co-occurrences do not contribute tothe constituents?
vectors.
Consistent with work us-ing proximity-based vectors, we have found thatintersective operations perform substantially bet-ter than additive operations.
This can be under-stood by viewing the intersective operations as en-capsulating the way that adjectives can specialisethe meaning of the nouns that they modify.We have investigated the interaction betweenthe vector operation used for composition, the fea-ture association score and the timing of applyingfeature weights.
We have found that multiplicationworks best if using PPMI to weight features, butthat geometric mean is better if using the increas-ingly popular PLMI weighting measure.
Whilstapplying an intersective composition operation be-fore applying feature weighting does allow morefeatures to be retained in the predicted vector (itis possible to achieve 99.5% recall), in general,this does not correspond with an increase in co-sine scores.
In general, the corresponding drop inprecision (i.e., the over-prediction of unobservedfeatures) causes the cosine to decrease.
The oneexception to this is using multiplication with thePPMI feature weighting score.
Here we actuallysee a drop in recall, and an increase in precisiondue to the nature of multiplication and PPMI.One assumption that has been made throughoutthe work, is that the observed phrasal vector pro-vides a good estimate of the distributional repre-sentation of the phrase and, consequently, the bestcomposition method is the one which returns themost similar prediction.
However, in general, wenotice that while the recall of the compositionalmethods is good, the precision is very low.
Lack ofprecision may be due to the prevalence of plausi-ble, but unobserved, co-occurrences of the phrase.Consequently, this introduces uncertainty into theconclusions which can be drawn from a study suchas this.
Further work is required to develop effec-tive intrinsic and extrinsic evaluations of modelsof composition.A further interesting area of study is whetherdistributional models that include higher-ordergrammatical dependencies can tell us more aboutthe lexical semantics of a word than the conven-tional first-order models, for example by distin-guishing semantic relations such as synonymy,antonymy, hypernymy and co-hyponymy.AcknowledgementsThis work was funded by UK EPSRC projectEP/IO37458/1 ?A Unified Model of Composi-tional and Distributional Compositional Seman-tics: Theory and Applications?.18ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing.Gemma Boleda, Marco Baroni, The Nghia Pham, andLouise McNally.
2013.
Intensionality was only al-leged: On adjective-noun composition in distribu-tional semantics.
In Proceedings of the 10th Inter-national Conference on Computational Semantics(IWCS 2013) ?
Long Papers, pages 35?46, Pots-dam, Germany, March.
Association for Computa-tional Linguistics.Gerlof Bouma.
2009.
Normalised (point wise) mu-tual information in collocation extraction, from formto meaning: Processing texts automatically.
In Pro-ceedings of the Biennial International Conference ofthe German Society for Computational Linguisticsand Language Technology.Kenneth Ward Church and Patrick Hanks.
1989.
Wordassociation norms, mutual information, and lexicog-raphy.
In Proceedings of the 27th Annual Meetingon Association for Computational Linguistics, ACL?89, pages 76?83, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2011.
Mathematical foundations for a com-positional distributed model of meaning.
LinguisticAnalysis, 36(1-4):345?384.James Curran.
2004.
From Distributional to SemanticSimilarity.
Ph.D. thesis, University of Edinburgh.Katrin Erk and Sebastian Pad?o.
2008.
A structuredvector space model for word meaning in context.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages897?906, Honolulu, Hawaii, October.
Associationfor Computational Linguistics.Katrin Erk.
2012.
Vector space models of word mean-ing and phrase meaning: a survey.
Language andLinguistics Compass, 6(10):635?653.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Edward Grefenstette, Georgiana Dinu, Yao-ZhongZhang, Mehrnoosh Sadrzadeh, and Marco Baroni.2013.
Multi-step regression learning for compo-sitional distributional semantics.
Proceedings ofthe 10th International Conference on ComputationalSemantics (IWCS 2013).Gregory Grefenstette.
1994.
Corpus-derived first, sec-ond and third-order word affinities.
In Proceedingsof Euralex 1994.Emiliano Guevara.
2010.
A Regression Model ofAdjective-Noun Compositionality in DistributionalSemantics.
In Proceedings of the ACL GEMS Work-shop, pages 33?37.Lillian Lee.
1999.
Measures of distributional simi-larity.
In Proceedings of the 37th Annual Meetingof the Association for Computational Linguistics,pages 25?32, College Park, Maryland, USA, June.Association for Computational Linguistics.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 17th Inter-national Conference on Computational Linguistings(COLING 1998).K.
Lund and C. Burgess.
1996.
Producinghigh-dimensional semantic spaces from lexical co-occurrence.
Behavior Research Methods, Instru-mentation, and Computers, 28:203?208.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedingsof ACL-08: HLT, pages 236?244, Columbus, Ohio,June.
Association for Computational Linguistics.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Joakim Nivre.
2004.
Incrementality in determinis-tic dependency parsing.
In Proceedings of the ACLWorkshop on Incremental Parsing, pages 50?57.Sebastian Pado and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.James Pustejovsky.
2013.
Inference patterns with in-tensional adjectives.
In Proceedings of the IWCSWorkshop on Interoperable Semantic Annotation,Potsdam,Germany, March.
Association for Compu-tational Linguistics.Silke Scheible, Sabine Schulte im Walde, and SylviaSpringorum.
2013.
Uncovering distributional dif-ferences between synonyms and antonyms in a wordspace model.
In Proceedings of the InternationalJoint Conference on Natural Language Processing,pages 489?497, Nagoya, Japan.Heinrich Sch?utze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?123.Richard Socher, Brody Huval, Christopher D Manning,and Andrew Y Ng.
2012.
Semantic compositional-ity through recursive matrix-vector spaces.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211.
Association for Computational Linguis-tics.Stefan Thater, Georgiana Dinu, and Manfred Pinkal.2009.
Ranking paraphrases in context.
In Proceed-ings of the 2009 Workshop on Applied Textual Infer-ence, pages 44?47, Suntec, Singapore, August.
As-sociation for Computational Linguistics.19Stefan Thater, Hagen F?urstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations us-ing syntactically enriched vector models.
In Pro-ceedings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, pages 948?957,Uppsala, Sweden, July.
Association for Computa-tional Linguistics.Stefan Thater, Hagen Frstenau, and Manfred Pinkal.2011.
Word meaning in context: A simple and ef-fective vector model.
In Proceedings of 5th Interna-tional Joint Conference on Natural Language Pro-cessing (IJCNLP 2011).P.
D. Turney and P. Pantel.
2010.
From frequency tomeaning: Vector space models of semantics.
Jour-nal of Artificial Intelligence Research, 37:141?188.Peter D. Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.Journal of Artificial Intelligence Research, 44.Tim Van de Cruys, Thierry Poibeau, and Anna Korho-nen.
2011.
Latent vector weighting for word mean-ing in context.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1012?1022, Edinburgh, Scotland,UK., July.
Association for Computational Linguis-tics.Julie Weeds and David Weir.
2003.
A general frame-work for distributional similarity.
In Proceedings ofthe 2003 Conference on Empirical Methods in Nat-ural Language Processing, pages 81?88, Sapporo,Japan.20
