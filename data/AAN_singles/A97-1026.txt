An Automatic Scoring System ForAdvanced Placement Biology EssaysJill Burstein, Susanne Wolff, Chi LuEducational Testing Service MS-11RPrinceton, NJ 08541e-mail: fourstein@ets, orgRandy M. KaplanAdvanced TechnologyPECO EnergyPhiladelphia, PAAbstractThis paper describes a prceXype for automaticallyscoring College Board Advanced Placement (AP)Biology essays.
I.
The scoring technique used inthis study was based on a previous method used toscore sentence-length responses (Burstein, et al1996).
One hundred training essays were used tobuild an example-based lexicon and conceptgranunars.
The prototype accesses informationfrom the lexicon and concept grammars to scoreessays by assigning a classification ofExcellent orPoor based on the number of points assignedduring scoring.
Final computer-based ssay scoresare based on the system's recognition of conceptualinformation i  the essays.
Conceptual analysis inessays is essential to provide a classification basedon the essay content.
In addition, computer-generated information about essay content can beused to produce diagnostic feedback.
The set ofessays used in this study had been scored by humanraters.
The results reported in the paper show 94%agreement on exact or adjacent scores betweenhuman rater scores and computer-hased scores for105 test essays.
The methods underlying thisapplication could be used in a number ofapplications involving rapid semantic analysis oftextual materials, especially with regard toscientific or other technical text.INTRODUCTIONTo replace the conventional multiplequestions on standardized xaminations,choice~Test i ems in this paper are copyrighted byEducational Testing Service (ETS).
No furtherreproduction is permitted without writtenpermission of ETS.174Educational Testing Service (ETS) is currentlydeveloping computer-based scoring tools forautomatic scoring of natural language constructed-responses - responses that are written, such as ashort-answer o  an essay.
The purpose of this workis to develop computer-based methods for scoringso that computer-administered natural languageconstructed-response it ms can be used onstandardized tests and scored efficiently withregard to time and cost.Until recently, ETS's automated scoring effortswere primarily devoted to the development ofcomputer programs used to score short-answerconstructed-responses of up to 15 words (Bursteinand Kaplan, 1995 and Burstein et al, 1996).
Inthis study a classification of Excellent or Poor wasautomatically assigned to an AP Biology essay.Our initial goal in this study was to develop aprototype scoring system that could reliably assigna classification ofExcellent to a set of AP Biologyessays.
For the evaluation of the scoring method,a small sample of Poor essays were also scored tocompare the results.
2Human rater scoring of AP Biology essays is basedon a highly constrained scoring key, called arubric, that specifies the criteria human raters useto assign scores to essays.
Accordingly, for the testquestion studied here, the criteria for point2 The Poor classification isnot an official APclassification.
It was used in this study todistinguish t e Excellent essays with scores of9 and l0 from essays with lower end scores in the0 - 3 range.assignment are highly constrained.
Essentially, theessay can be treated as a sequence of short-answerresponses.
Given our preliminary successes withtest questions that elicit multiple responses fromexaminees, imilar scoring methods were appliedfor scoring AP Biology essay.
The results show87% agreement for exact scores between humanrater and computer scores, and 94% agreement forexact or adjacent scores between human rater andcomputer scores.This work is also applicable for other types ofassessment as well, such as for employee trainingcourses in corporate and government settings.Since the methods discussed in this paper describetechniques for analysis of semantic information itext, presumably this application could be extendedto public informational settings, in which peoplemight key in "requests for information" in anumber of domains.
In particular, these methodscould be successfully applied to the analysis ofnatural language responses for highly constraineddomains, such as exist in scientific or technicalfields.SYSTEM TRAININGOne hundred Excellent essays from the original200 essays were selected to train the scoringsystem.
The original 200 essays were divided into atraining set and test set, selected arbitrarily fromthe lowest examinee identification number.
Only85 of the original 100 in the test set were includedin the study due to illegibility, or use of diagramsinstead of text to respond to the question.
Forconvenience during training, and later, for scoring,essays were divided up by section, as specified inthe scoring guide (see Figure 1), and stored indirectories by essay section.
Specifically, the PartA's of the essays were stored in a separatedirectory, as were Part B's, and Part C's.Examinees typically partitioned the essay intosections that corresponded tothe scoring uide.System training involved the following steps thatare discussed in subsequent sections: a) manuallexicon development, b) automatic generation ofconcept-structure representation (CSR), c) manualcreation of a computer-based rubric, d) manualCSR "fine-tuning", e) automatic rule generation,and f) evaluation of training process.Lexicon DevelopmentExample-based approaches to lexicon developmenthave been shown to effectively exemplify wordmeaning within a domain (Richardson, et al,1993, and Tsutsumi 1992).
It has been furtherpointed out by Wilks, et al 1992, that word sensescan be effectively captured on the basis of textualmaterial, The lexic, on dw?lopcd for this study usedan example-based approach to compile a list oflexical items that characterized the contentvocabulary used in the domain of the test question(i.e., gel electrophoresis).
The lexicon is composedof words and terms from the relevant vocabulary ofthe essays used for training.To build the lexicon, all words and termsconsidered to contribute to the core meaning ofeach relevant sentence in an essay, were includedin the lexicon.
The decision with regard towhether or not a sentence was relevant was basedon information provided in the scoring guide (inFigure 1).
For instance, in the sentence, "SmallerDNA fragments mave faster than larger ones.
", theterms Smaller, DNA, fragments, move, faster,larger are considered to be the most meaningfulterms in the sentence.
This is based on the criteriafor a correct response for the Rate/Size category inthe scoring guide.Each lexical entry contained a superordinateconcept and an associated list of metonyms.Metonyms are words or terms which are acceptablesubstitutions for a given word or term (Gerstl,1991).
Metonyms for concepts in the domain ofthis test question were selected from the exampleresponses in the training data This paradigm wasused to identify word similarity in the domain ofthe essays.
For instance, the scoring programneeded to recognize that sentences, uch as SmallerDNA fragments move faster than larger ones andThe smaller segments of DNA will travel morequickly than the bi~.~er ones, contain alternatewords with similar meanings in the test questiondomain.
To determine alternate words withsimilar meanings, metonyms for words, such asfragments and move were established in the175lexicon so that the system could identify whichwords had similar meanings in the test itemdomain.
The example lexical entries in (1)illustrate that the words fragment and segment aremetonyms in this domain, as well as the wordsmove and travel.
In (1), FRAGMENT and MOVEare the higher level lexical concepts.
Theassociated metonymsfor FRAGMENT and MOVEare in adjacent lists illustrated in (1).(1).
Sample Lexical EntrieswouM be digested only once, leaving 2 pieces.
",and "The DNA fragment wouM only have 2segments," the phrases DATA segment and DNAfragment are paraphrases of each other, and 2pieces and 2 segments are paraphrases of eachother.
These sentences are represented bythe CSRin (2a) and in (2b).(2)a.
NP: \[DNA,FRAGMENT\]NP: \[TWO,FRAGMENT\]FRAGMENT \[fragment particle segment...\]MOVE \[ move travel pass pull repel attract ...\]In the final version of the CSR, phrasalconstituents are reduced to a general XP node, as isillustrated inConcept-Structure Representations (CSR)Obviously, no two essays will be identical, and it isunlikely that two sentences in two different essayswill be worded exactly alike.
Therefore, scoringsystems must be able to recognize paraphrasedinformation in sentences across essay responses..To identify paraphrased information in sentences,the scoring system must be able to identify similarwords in consistent syntactic patterns.
As,Montemagni and Vanderwende (1993) have alsopointed out, structural patterns are more desirablethan string patterns for capturing semanticinformation from text.
We have implemented aconcept-extraction program for preprocessing ofessay data that outputs conceptual information as itexists in the structure of a sentence.
The programreads in a parse tree generated by MicrosoR'sNatural Language Processing Tools (MSNLP) foreach sentence in an essay) The programsubstitutes words in the parse tree withsuperordinate concepts from the lexicon, andextracts the phrasal nodes containing theseconcepts.
(Words in the phrasal node which do notmatch a lexical concept are not included in the setof extracted phrasal nodes.)
The resultingstructures are CSRs.
Each CSR represents asentence according to conceptual content andphra~l constituent structure.
CSRs characterizeparaphrased information in sentences.
Forexample, in the sentences "The DNA segment(2)b..XP: \[DNA,FRAGMENT\]XP: ITWO,FRAGMENTISince phrasal category does not have to bespecified, the use of a generalized XP nodeminimizes the number of required lexical entries,as well as the number of concept grammar ulesneeded for the scoring process.The Computer RubricRecall that a rubric is a scoring key.
Rubriccategories are the criteria that determine a correctresponse.
A computer-based rubric was manuallycreated for the purpose of classifying sentences inessays by rubric category during the automatedscoring process.
Computer ubric categories arecreated for the bulleted categories listed in thehuman rater scoring guide illustrated in Figure 1.3 See http://research.microsoR.com/research/nlp forinformation on MS-NLP.176Part A.
Explain how the principles of gel electrophoresis allow forthe separation of DNA fragments (4 point maximum).?
Electricity ......... Elechical potential?
Charge ..............
Negatively charged fragments?
Rate/Size ..........
Smaller fragments move faster?
Calibration.
...... DNA's ...used as markers/standards?
Resolution ........
Concentration ofgel?
Apparatus ........ Use of wells, gel material...Past B.
Describe the results you would expect from electrophoreticseparation of fragments from the following treatments ofthe DNAsegment shown in the question.
(4 point maximum).?
Treatment I .......
Describe 4 bands/fragments?
Treatment II......Describe 2 bands/l~agments?
Treatment lll.....Describe 5 bands/fragments?
Treatment IV.....Describe 1band/fragmentPart CI.
The mechanism of action o f r~' iet ion  enz3anes.
(4 pointmaximum)?
Recognition .......
Binding of enzyme to target sequence?
CuRing.
............
Enzyme cuts at every location?
Alternate ........... Point about enzyme cutting at specificlocation?
Detail Point.
..... May generate sticky endsPart C2: The different results...if a mutation occurred at therecognition site for enzyme Y.?
Change in I ....... 1 band/fragment?
Change in III....4 bands/fragments?
Alternate ........... Y no longer ecognized and cut?
Detail Point ....... Y site might become an X siteFigure 1: Scoring Guide ExcerptAccordingly, the computer-rubric categories werethe following.
For Part A, the categories wereElectricity, Charge, Rate~size, Calibration,Resolution, and Apparatus.
For Part B thecategories were, Treatment I, Treatment 2,Treatment 3, and Treatment IV.
For Part C1, thecategories were: Recognition, Cutting, Alternate,and Detail Point.
For Part C2, the categories wereChange in l, Change in II, Alternate, and DetailPoint.
Each computer-rubric category exists as anelectronic file and contains the related conceptgrammar ules used during the scoring process.The concept grammar rules are described later inthe paper.Fine-Tuning CSRsCSRs were generated for all sentences in an essay.During training, the CSRs of relevant sentencesfrom the training set were placed into computer-rubric category files.
Relevant sentences in essayswere sentences identified in the scoring guide ascontaining information relevant to a rubriccategory.
For example, the representation for thesentence, "The DNA fragment would only have 2segments," was placed in the computer ubriccategory file for Treatment II.Typically, CSRs are generated with extraneousconcepts that do not contribute to the core meaningof the response.
For the purpose of conceptgrammar rule generation, each CSR from thetraining data must contain only concepts whichdenote the core meaning of the sentence.Extraneous concepts had to be removed before therule generation process, so that the concept-structure information i the concept grammar ruleswould be precise.The process of removing extraneous concepts fromthe CSRs is currently done manually.
For thisstudy, all concepts in the CSR that were consideredto be extraneous to the core meaning of thesentence were removed by hand.
For example, inthe sentence, The DNA segment would be digestedonly once, leaving 2 pieces, the CSR in (3) wasgenerated.
For Treatment \]I, the scoring guideindicates that if the sentence makes a reference to 2fragments that it should receive one point.
(Theword, piece, is a metonym for the concept,fragment, so these two words may be usedinterchangably.)
The CSR in (3) was generated bythe concept-extraction program.
The CSR in (4)(in which XP:\[DNA,FRAGMENT\] was removed)illustrates the fine-tuned version of the CSR in (3).The CSR in (4) was then used for the rulegeneration process, described in the next section.177(3) XP:\[DNA,FRAGMENT\]XP:\[TWO,FRAGMENT\](4) XP:\[TWO,FRAGMENT\]Concept Grammar Rule GenerationAt this point in the process, each computer rubriccategow is an electronic file which contains fine-tuned, CSRs.
The CSRs in the computer rubriccategories exemplify the information required toreceive credit for a sentence in a response.
Wehave developed a program that automaticallygenerates rules from CSP.s by generatingpermutations of each CSR The example rules in(5) were generated from the CSR in (4).
The rulesin (5) were used during automated scoring(described in the following section).which looks for matches between CSRs and/orsubsets of CSRs, and concept grammar ules inrubric categories associated with each essay part.Recall that CSRs often have extraneous conceptsthat do not contribute to the core meaning of thesentence.
Therefore, the scoring program looks formatches between concept grammar rules andsubsets of CSRs, if no direct match can be foundfor the complete set of concepts in a CSR.
Thescoring program assigns points to an essay as rulematches are found, according to the scoring guide(see Figure 1).
A total number of points isassigned to the essay after the program has lookedat all sentences in an essay.
Essays receiving atotal of at least 9 points are classified as Excellent,essays with 3 points or less are classified as Poor,and essays with 4 - 8 points are classified as "NotExcellent."
The example output in Appendix 1illustrates matches found between sentences in theessay and the rubric rules from an Excellent essay.(5)a.
XP:\[TWO, FRAGMENT\]b. XP:\[FRAGMENT,TWO\]The trade-off or generating rules automatically inthis manner is rule overgeneration, but this doesnot appear to be problematic for the automatedscoring process.
Automated rule generation issignificantly faster and more accurate than writingthe rules by hand.
We estimate that it would havetaken two people about two weeks of full-timework to manually create the rules.
Inevitably,there would have been typographical errors andother kinds of "human error".
It takesapproximately 3 minutes to automatically generatethe rules.AUTOMATED SCORINGThe 85 remaining Excellent test essays and a set of20 Poor essays used in this study were scored.First, all sentences in Parts A, B and C of eachessay were parsed using MSNLP.
Next,inflectional suffixes were automatically removedfrom the words in the parsed sentences, sinceinflectional suffixed forms are not included in thelexicon.
CSRs were automatically generated for allsentences in each essay.
For each part of the essay,the scoring program uses a searching algorithmRESULTSTable 1 shows the results of using the automaticscoring prototype to score 85 Excellent test essays,and 20 Poor test essays.
Coverage (Cov) illustrateshow many essays were assigned a score.
Accuracy(Acc) indicates percentage of agreement betweenthe computer-based score and the human raterscore.
Accuracy within 1 (w/i 1) or 2 points (w/i 2)shows the amount of agreement between thecomputer scores and human raters cores, within 1or 2 points of human rater scores, respectively.
ForExcellent essays computer-based scores would be 1or 2 points below the 9 point minimum, and forPoor essays, they would be 1 or 2 points above the3 point maximum.Data SetExcellentPoorTotalCov Ace100% 89%100% 75%100% 87%Acc w/i 1 Acc w/i 295% 100%90% 95%94% 96%Table 1: Results of Automatic Scoring Prototype178ERROR ANALYSISAn error analysis of the data indicated thefollowing two error categories that reflected amethodological problem: a) Lexicon Deficiencyand b) Concept Grammar Rule Deficiency.
Theseerror categories are discussed briefly below.
Botherror types could be resolved in future research.Scoring errors can be linked to data entry errors,morphological stripping errors, parser errors, anderroneous rules generated ue to misinterpretationsof the scoring guide.
These errors, however, areperipheral to the underlying methods applied inthis study.Lexical DeficiencyRecall that the lexicon in this study was built fromrelevant vocabulary in the set of 100 trainingessays.
Therefore, vocabulary which occurs in thetest data, but not in the training data was ignoredduring the process of concept-extraction.
Thisyielded incomplete CSRs, and degraded scoringresulted.
For instance, while the core concept ofthe commonly occurring phrase one band is moreoften than not expressed as one band, or onefragment, other equivalent expressions existed inthe test data some of which did not occur in thetraining data.
From our 185 essays we extractedpossible substitutions of the term one fragment.These are: one spot, one band, one inclusive line,one probe, one group, one bond, one segment, onelength o f  nucleotides, one marking, one strand,one solid clump, in one piece, one bar, one mass,one stripe, one bar, and one blot.
An even largersample of essays could contain more alternate wordor phrase substitutions than those are listed here.Perhaps, increased coverage for the test data can beachieved ff additional standard ictionary sourcesare used to create a lexicon, in conjunction withthe example based method used in this study(Richardson et al, 1993).
Corpus-based techniquesusing domain-specific texts (e.g., Biologytextbooks) might also be helpful (Church andHanks, 1990).Concept Grammar Rule DeficiencyIn our error analysis, we found cases in whichinformation in a test essay was expressed in anovel way that is not represented in the set ofconcept grammar ules.
In these cases, essayscores were degraded.
For example, the sentence,"The action o f  this mutation would nullify theeffect o f  the site, so the enzyme Y would not affectthe site o f  the mutation. "
is expressed uniquely, ascompared to its paraphrases in the training set.This response says in a somewhat roundabout waythat due to the mutation, the enzyme will notrecognize the site and will not cut the DNA at thispoint.
No rule was found to match the CSRgenerated for this test response.SUMMARY AND CONCLUSIONSThis prototype scoring system for AP Biologyessays uccessfully scored the Excellent and Pooressays with 87% exact agreement with humangrader scores.
For the same set of essays, therewas 94% agreement between the computer scoresand human rater scores for exact or adjacentscores.
The preprocessing steps required forautomated scoring are mostly automated.
Manualprocesses, such as lexicon development could beautomated in the future using standard context-based, word distribution methods (Smadja, 1993),or other corpus-based techniques.
The erroranalysis from this study suggests that dictionary-based methods, combined with our currentexample-based approach, might effectively help toexpand the lexicon).
Such methods could broadenthe lexicon and reduce the dependencies ontraining data vocabulary.
The automation of thefine-tuned CSRs will require more research.
Afully automated process would be optimal withregard to time and cost savings.
Work at thediscourse level will have to be done to deal withmore sophisticated responses which are currentlytreated as falling outside of the norm.Perhaps the most attractive feature of this systemin a testing environment is that it is defensible.The representation used in the system denotes thecontent of essay responses based on lexicalmeanings and their relationship to syntacticstructure.
The computer-based scores reflect thecomputer-based analysis of the response content,and how it compares to the scoring guidedeveloped by human experts.
Informationgenerated by the system which denotes response179content can be used to generate useful diagnosticfeedback to examinees.Since our methods explicitly analyze the content oftext, these or similar methods could be applied in avariety of testing, training or information retrievaltasks.
For instance, these natural languageprocessing techniques could be used for WorldWide Web-based queries, especially with regard toscientific subject matter or other materialproducing constrained natural language t xt.Richardson, Stephen D., Lucy Vandervende, andWilliam Dolan.
(1993).
CombiningDictionary-Based and Example-BasedMethods for Natural Language Analysis.(MSR-TR-93-08).
Redmond, WA:MicrosoflCorporation.Smadja,Frank.(1993).
Retrieving CollocationsfromText:Xtract.
Computational Linguistics.19(1), 143-177.ACKNOWLEDGMENTSWe are grateful to the College Board for support ofthis project.
We are thankful to AltameseJackenthal for her contributions tothis project.
Weare also grateful to Mary Dee Harris and twoanonymous reviewers for helpful comments andsuggestions on earlier versions of this paper.ReferencesBurstein, Jill C., Randy M. Kaplan, Susanne Wolffand Chi Lu.
(1996).
Using Lexical SemanticTechniques to Classify Free Responses.Proceedings from the SIGLEX96 Workshop,ACL, University of California, Santa Cruz.Tsutsumi,T.
(1992) Word Sense Disambiguation byExamples.
In K. Jensen, G. Heidorn and S.Richardson (Eds), Natural LanguageProcessing: the PLNLP Approach, KluwerAcademic Publishers, Boston, MA.Wilks, Y., D. Fass, C. Guo, J. McDonald, T.Plate, and B. Slator.
(1992).
ProvidingMachine Tractable Dictionary Tools.
In J.Pustejovsky (Ed), Semantics and the Lexicon,Kluwer Academic Publishers, Boston, MA.Gerstl, P. (1991).
A Model for the Interaction ofLexical and Non-Lexical Knowledge in theDetermination of Word Meaning.
In J.Pustejovsky and S. Bergler (Eds), LexicalSemantics and Knowledge Representation,Springer-Verlag, New York, NY.Church, K and P. Hanks.
Word AssociationNorms, Mutual Information andLexicography.
Computational Linguistics,16(1), 22-29.Montemagni, Simonetta nd Lucy Vanderwende(1993).
"Structural Patterns versus StringPatterns for Extracting Semantic Informationfrom Dictionaries," In K. Jensen, G. Heidornand S. Richardson (Eds), Natural LanguageProcessing: the PLNLP Approach, KluwerAcademic Publishers, Boston, MA..180Appendix 1: Sample Rule Matchesfor a Scored EssayPart A:"The cleaved DNA is then placed in a gel electrophoresis boxthat has a positlve and a negative end to it.
"Rubric category : CHARGERubric Rule:XP: \[DNA\],XP: \[NEGATIVE\]"The lonEer, heavier bands would move the least and thesmaller lighter bands would move the most and farther from thestarting point.
"Rubric category: RATE/SIZERubric Rule:XP: \[LARGE SIZE\],XP:\[MOVE,LESS\]Part B:"If  the DNA was digested with only enzyme X then there wouldbe 4_ separate bands that would develop.
"Rubric category :Treatment IRubric Rule:XP:\[FOUR\]" I f  the DNA was digested only with enzyme Y then two\[raRments or RFLP's would be visible.
"Rubric Category: Treatment IIRubric Rule:XP:\[TWO, FRAGMENT\]"If the DNA was digested with both the X and the Y enzyme thenthere would be 5 RFLP's o f  400 base pairs, 500 base pairs,1,200 base pairs, 1,300 b.p and 1,500 b.p.
"Rubric category : Treatment III~ubric Rule: XP:\[FIVE,FRAGMENT\]"If the DNA was undigested then we would find no t~'LP's and,as a result, there would be no bandin?
that would occur.
"Rubric category: Treatment IVRubric Rule:XP:\[NOT,FRAGMENT\]Parts CI m~d C2"Restriction enzymes are types o f  proteins which recognizecertain recognition sites along the DNA sequence and cleave theDNA at that end.
"Rubric category RECOGNITIONRule:XP:\[CUT, DNA\]"Therefore, there would be no cut at that location and no RbT_~produced at the Y recognition site.
"Rubric CategoryRule:XP:\[NOT\],XP:\[CUT\],XP:\[SITE\]181
