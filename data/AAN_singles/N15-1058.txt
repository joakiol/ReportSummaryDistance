Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 556?566,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsMultiview LSA: Representation Learning via Generalized CCAPushpendre Rastogi1and Benjamin Van Durme1,2and Raman Arora11Center for Language and Speech Processing2Human Language Technology Center of ExcellenceJohns Hopkins UniversityAbstractMultiview LSA (MVLSA) is a generalizationof Latent Semantic Analysis (LSA) that sup-ports the fusion of arbitrary views of data andrelies on Generalized Canonical CorrelationAnalysis (GCCA).
We present an algorithmfor fast approximate computation of GCCA,which when coupled with methods for han-dling missing values, is general enough to ap-proximate some recent algorithms for induc-ing vector representations of words.
Exper-iments across a comprehensive collection oftest-sets show our approach to be competitivewith the state of the art.1 IntroductionWinograd (1972) wrote that: ?Two sentences areparaphrases if they produce the same representationin the internal formalism for meaning?.
This intu-ition is made soft in vector-space models (Turneyand Pantel, 2010), where we say that expressions inlanguage are paraphrases if their representations areclose under some distance measure.One of the earliest linguistic vector space mod-els was Latent Semantic Analysis (LSA).
LSA hasbeen successfully used for Information Retrieval butit is limited in its reliance on a single matrix, orview, of term co-occurrences.
Here we address thesingle-view limitation of LSA by demonstrating thatthe framework of Generalized Canonical Correla-tion Analysis (GCCA) can be used to perform Mul-tiview LSA (MVLSA).
This approach allows for theuse of an arbitrary number of views in the induc-tion process, including embeddings induced usingother algorithms.
We also present a fast approx-imate method for performing GCCA and approxi-mately recover the objective of (Pennington et al,2014) while accounting for missing values.Our experiments show that MVLSA is com-petitive with state of the art approached forinducing vector representations of words andphrases.
As a methodological aside, we discuss the(in-)significance of conclusions being drawn fromcomparisons done on small sized datasets.2 MotivationLSA is an application of Principal Component Anal-ysis (PCA) to a term-document cooccurrence ma-trix.
The principal directions found by PCA formthe basis of the vector-space in which to representthe input terms (Landauer and Dumais, 1997).
Adrawback of PCA is that it can leverage only a sin-gle source of data and it is sensitive to scaling.An arguably better approach to representationlearning is Canonical Correlation Analysis (CCA)that induces representations that are maximally cor-related across two views, allowing the utilization oftwo distinct sources of data.
While an improvementover PCA, being limited to only two views is un-fortunate in light of the fact that many sources ofdata (perspectives) are frequently available in prac-tice.
In such cases it is natural to extend CCA?s orig-inal objective of maximizing correlation betweentwo views by maximizing some measure of the ma-trix ?
that contains all the pairwise correlations be-tween linear projections of the covariates.
Thisis how Generalized Canonical Correlation Analy-sis (GCCA) was first derived by Horst (1961).
Re-cently these intuitive ideas about benefits of lever-aging multiple sources of data have received strongtheoretical backing due to the work by Sridharan and556Kakade (2008) who showed that learning with mul-tiple views is beneficial since it reduces the com-plexity of the learning problem by restricting thesearch space.
Recent work by Anandkumar et al(2014) showed that at least three views are neces-sary for recovering hidden variable models.Note that there exist different variants of GCCAdepending on the measure of ?
that we choose tomaximize.
Kettenring (1971) enumerated a varietyof possible measures, such as the spectral-norm of?.
Kettenring noted that maximizing this spectral-norm is equivalent to finding linear projections ofthe covariates that are most amenable to rank-onePCA, or that can be best explained by a single termfactor model.
This variant was named MAX-VARGCCA and was shown to be equivalent to a proposalby Carroll (1968), which searched for an auxiliaryorthogonal representation G that was maximallycorrelated to the linear projections of the covariates.Carroll?s objective targets the intuition that represen-tations leveraging multiple views should correlatewith all provided views as much as possible.3 Proposed Method: MVLSALet Xj?
RN?dj?j ?
[1, .
.
.
, J ] be the mean cen-tered matrix containing data from view j such thatrow i of Xjcontains the information for word wi.Let the number of words in the vocabulary beN andnumber of contexts (columns in Xj) be dj.
Follow-ing standard notation (Hastie et al, 2009) we callX>jXjthe scatter matrix and Xj(X>jXj)?1X>jtheprojection matrix.The objective of MAX-VAR GCCA can be writtenas the following optimization problem: Find G ?RN?rand Uj?
Rdj?rthat solve:arg minG,UjJ?j=1??G?XjUj?
?2Fsubject to G>G = I.
(1)The matrix G that solves problem (1) is our vectorrepresentation of the vocabulary.
Finding G reducesto spectral decomposition of sum of projection ma-trices of different views: DefinePj=Xj(X>jXj)?1X>j, (2)M =J?j=1Pj.
(3)Then, for some positive diagonal matrix ?, G andUjsatisfy:MG =G?, (4)Uj=(X>jXj)?1X>jG.
(5)Computationally storing Pj?
RN?Nis prob-lematic owing to memory constraints.
Further, thescatter matrices may be non-singular leading to anill-posed procedure.
We now describe a novel scal-able GCCA with `2-regularization to address theseissues.Approximate Regularized GCCA: GCCA can beregularized by adding rjI to scatter matrix X>jXjbefore doing the inversion where rjis a small con-stant e.g.
10?8.
Projection matrices in (2) and (3)can then be written as?Pj=Xj(X>jXj+ rjI)?1X>j, (6)M =J?j=1?Pj.
(7)Next, to scale up GCCA to large datasets, wefirst form a rank-m approximation of projection ma-trices (Arora and Livescu, 2012) and then extendit to an eigendecomposition for M following ideasby Savostyanov (2014).
Consider the rank-m SVDof Xj:Xj= AjSjB>j,where Sj?
Rm?mis the diagonal matrix with m-largest singular values of Xjand Aj?
RN?mandBj?
Rm?djare the corresponding left and rightsingular vectors.
Given this SVD, write the jthpro-jection matrix as?Pj= AjS>j(rjI + SjS>J)?1SjA>j,= AjTjT>jA>j,where Tj?
Rm?mis a diagonal matrix such thatTjT>j= S>j(rjI + SjS>J)?1Sj.
Finally, we note557that the sum of projection matrices can be expressedas M =?M?M>where?M = [A1T1.
.
.
AJTJ] ?
RN?mJ.Therefore, eigenvectors of matrix M , i.e.
the ma-trix G that we are interested in finding, are the leftsingular vectors of?M , i.e.
?M = GSV>.
Theseleft singular vectors can be computed by using In-cremental PCA (Brand, 2002) since?M may be toolarge to fit in memory.3.1 Computing SVD of mean centered XjRecall that we assumed Xjto be mean centered ma-trices.
Let Zj?
RN?djbe sparse matrices con-taining mean-uncentered cooccurrence counts.
Letfj= nj?
tjbe the preprocessing function that weapply to Zj:Yj=fj(Zj), (8)Xj=Yj?
1(1>Yj).
(9)In order to compute the SVD of mean centered ma-trices Xjwe first compute the partial SVD of un-centered matrix Yjand then update it (Brand (2006)provides details).
We experimented with represen-tations created from the uncentered matrices Yjandfound that they performed as well as the mean cen-tered versions but we would not mention them fur-ther since it is computationally efficient to follow theprincipled approach.
We note, however, that eventhe method of mean-centering the SVD produces anapproximation.3.2 Handling missing rows across viewsWith real data it may happen that a term was notobserved in a view at all.
A large number ofmissing rows can corrupt the learnt representationssince the rows in the left singular matrix becomezero.
To counter this problem we adopt a variantof the ?missing-data passive?
algorithm from VanDe Velden and Bijmolt (2006) who modified theGCCA objective to counter the problem of missingrows.1The objective now becomes:arg minG,UjJ?j=1??Kj(G?XjUj)?
?2Fsubject to G>G = I,(10)where [Kj]ii= 1 if row i of view j is observed andzero otherwise.
Essentially Kjis a diagonal row-selection matrix which ensures that we optimize ourrepresentations only on the observed rows.
Note thatXj= KjXjsince the rows that Kjremoved werealready zero.
Let, K =?jKjthen the optima ofthe objective can be computed by modifying equa-tion (7) as:M =K?12(J?j=1Pj)K?12.
(11)Again, if we regularize and approximate the GCCAsolution we get G as the left singular vectors ofK?12?M .
We mean center the matrices using onlythe observed rows.Also note that other heuristic weighting schemescould be used here.
For example if we modify ourobjective as follows then we would approximatelyrecover the objective of Pennington et al (2014):minimize:G,UjJ?j=1??WjKj(G?XjUj)?
?2Fsubject to: G>G = I(12)where[Wj]ii=(wiwmax)34if wi< wmaxelse 1,and wi=?k[Xj]ik.4 DataTraining Data We used the English portion of thePolyglot Wikipedia dataset released by Al-Rfou et1A more recent effort, by van de Velden and Takane(2012), describes newer iterative and non-iterative (Test-Equating Method) approaches for handling missing values.
Itis possible that using one of those methods could improve per-formance.558al.
(2013) to create 15 irredundant views of cooc-currence statistics where element [z]ijof view Zkrepresents that number of times word wjoccurred kwords behind wi.
We selected the top 500K wordsby occurrence to create our vocabulary for the restof the paper.We extracted cooccurrence statistics from a largebitext corpus that was made by combining a num-ber of parallel bilingual corpora as part of the Para-Phrase DataBase (PPDB) project: Table 1 gives asummary, Ganitkevitch et al (2013) provides furtherdetails.
Element [z]ijof the bitext matrix representsthe number of times English word wiwas automati-cally aligned to the foreign word wj.We also used the dependency relations in the An-notated Gigaword Corpus (Napoles et al, 2012) tocreate 21 views2where element [z]ijof view Zdrep-resents the number of times word wjoccurred as thegovernor of word wiunder dependency relation d.We combined the knowledge of paraphrasespresent in FrameNet and PPDB by using the datasetcreated by Rastogi and Van Durme (2014) to con-struct a FrameNet view.
Element [z]ijof theFrameNet view represents whether word wiwaspresent in frame fj.
Similarly we combined theknowledge of morphology present in the CatVardatabase released by Habash and Dorr (2003) andmorpha released by Minnen et al (2001) along withmorphy that is a part of WordNet.
The morphologi-cal views and the frame semantic views were espe-cially sparse with densities of 0.0003% and 0.03%.While the approach allows for an arbitrary numberof distinct sources of semantic information, such asgoing further to include cooccurrence in WordNetsynsets, we considered the described views to berepresentative, with further improvements possibleas future work.Test Data We evaluated the representations on theword similarity datasets listed in Table 2.
The first10 datasets in Table 2 were annotated with differentrubrics and rated on different scales.
But broadlythey all contain human judgements about how simi-lar two words are.
The ?AN-SYN?
and ?AN-SEM?datasets contain 4-tuples of analogous words and the2Dependency relations employed: nsubj, amod, advmod,rcmod, dobj, prep of, prep in, prep to, prep on, prep for,prep with, prep from, prep at, prep by, prep as, prep between,xsubj, agent, conj and, conj but, pobj.Embeddings?(Incremental,?Missing?value?aware,?Max-??Var?GCCA)?Monolingual?Text?From?Wikipedia?Word?Aligned?Bitext?(Fr,?Zh,?Es,?De,??)?Dependency?Rela?ons?(nsubj,?amod,?advmod,??)?
Morphology?(CatVar,?Morphy/a)?Frame?Rela?ons?
(FrameNet)?Figure 1: An illustration of datasets used.Language Sentences English TokensBitext-Arabic 8.8M 190MBitext-Czech 7.3M 17MBitext-German 1.8M 44MBitext-Spanish 11.1M 241MBitext-French 30.9M 671MBitext-Chinese 10.3M 215MMonotext-En-Wiki 75M 1700MTable 1: Portion of data used to create GCCA representa-tions (in millions).task is to predict the missing word given the firstthree.
Both of these are open vocabulary tasks whileTOEFL is a closed vocabulary task.4.1 Significance of comparisonWhile surveying the literature we found that perfor-mance on word similarity datasets is typically re-ported in terms of the Spearman correlation betweenthe gold ratings and the cosine distance between nor-malized embeddings.
However researchers do notreport measures of significance of the difference be-tween the Spearman Correlations even for compar-isons on small evaluation sets.3This motivated ourdefining a method for calculating the Minimum Re-quired Difference for Significance (MRDS).Minimum Required Difference for Significance(MRDS): Imagine two lists of ratings over the same3For example, the comparative difference by competing al-gorithms reported by Faruqui et al (2014) could not be signif-icant for the Word Similarity test set released by Finkelstein etal.
(2001), even if we assumed a correlation between competingmethods as high as 0.9, with a p value threshold of 0.05.
Similarsuch comparisons on small datasets are performed by Hill et al(2014a).559Acronym Size ?0.50.01?0.70.01?0.90.01?0.50.05?0.70.05?0.90.05ReferenceMEN 3000 4.2 3.2 1.8 3.0 2.3 1.3 (Bruni et al, 2012)RW 2034 5.1 3.9 2.3 3.6 2.8 1.6 (Luong et al, 2013)SCWS 2003 5.1 4.0 2.3 3.6 2.8 1.6 (Huang et al, 2012)SIMLEX 999 7.3 5.7 3.2 5.2 4.0 2.3 (Hill et al, 2014b)WS 353 12.3 9.5 5.5 8.7 6.7 3.9 (Finkelstein et al, 2001)MTURK 287 13.7 10.6 6.1 9.7 7.5 4.3 (Radinsky et al, 2011)WS-REL 252 14.6 11.3 6.5 10.3 8.0 4.6 (Agirre et al, 2009)WS-SEM 203 16.2 12.6 7.3 11.5 8.9 5.1 -Same-As-Above-RG 65 28.6 22.3 12.9 20.6 16.0 9.2 (Rubenstein and Goodenough, 1965)MC 30 41.7 32.7 19.0 30.6 23.9 13.8 (Miller and Charles, 1991)AN-SYN 10675 - - 0.95 - - 0.68 (Mikolov et al, 2013a)AN-SEM 8869 - - 1.03 - - 0.74 -Same-As-Above-TOEFL 80 - - 8.13 - - 6.63 (Landauer and Dumais, 1997)Table 2: List of test datasets used.
The columns headed ?rp0contain MRDS values.
The rows for accuracy based testsets contain ?p0which does not depend on r. See ?
4.1 for details.items, produced respectively by algorithms A andB, and then a list of gold ratings T .
Let rAT,rBTand rABdenote the Spearman correlations be-tween A : T , B : T and A : B respectively.
Letr?AT, r?BT, r?ABbe their empirical estimates and as-sume that r?BT> r?ATwithout loss of generality.For word similarity datasets we define ?rp0as theMRDS, such that it satisfies the following proposi-tion:(rAB< r) ?
(|r?BT?
r?AT|<?rp0) =?
pval > p0.
Here pval is the probability of the test statisticunder the null hypothesis that rAT= rBTfoundusing the Steiger?s test (Steiger, 1980).
The aboveconstraint ensures that as long as the correlation be-tween the competing methods is less than r and thedifference between the correlations of the scores ofthe competing methods to the gold ratings is lessthan ?rp0, then the pvalue of the null hypothesis willbe greater than p0.
We can then ask what we con-sider a reasonable upper bound on the agreement ofratings produced by competing algorithms: for in-stance two algorithms correlating above 0.9 mightnot be considered meaningfully different.
Thatleaves us with the second part of the predicate whichensures that as long as the difference between thecorrelations of the competing algorithms to the goldscores is less than ?rp0then the null hypothesis ismore likely than p0.We can find ?rp0as follows: Let stest denoteSteiger?s test predicate which satisfies the following:stest-p(r?AT, r?BT, rAB, p0, n) =?
pval < p0Once we define this predicate then we can use it toset up an optimistic problem where our aim is to find?rp0by solving the following:?rp0= min{?|?
0<r?<1 stest-p(r?,min(r?+?, 1), r, p0, n)}Note that MRDS is a liberal threshold and it onlyguarantees that differences in correlations below thatthreshold can never be statistically significant (un-der the given parameter settings).
MRDS might op-timistically consider some differences as significantwhen they are not, but it is at least useful in reducingsome of the noise in the evaluations.
The values of?rp0are shown in Table 2.For the accuracy based test-sets we foundMRDS= ?p0that satisfied the following:0 < (??B??
?A) < ?p0=?
p(?B?
?A) > p0Specifically, we calculated the posterior probabil-ity p(?B?
?A) with a flat prior of ?
(1, 1) to solvethe following:4?p0= min{?|?
0<?<min(1?
?, 0.9)p(?B??A|??A=?,??B=?
+ ?, n) < p0} Here ?Aand ?B4This instead of using McNemar?s test (McNemar, 1947)since the Bayesian approach is tractable and more direct.
A cal-culation with ?
(0.5, 0.5) as the prior changed ?0.5from 6.63to 6.38 for the TOEFL dataset but did not affect MRDS for theAN-SEM and AN-SYN datasets.560are probability of correctness of algorithms A, Band??A,?
?Bare observed empirical accuracies.Unfortunately there are no widely reported train-test splits of the above datasets, leading to potentialconcerns of soft supervision (hyper-parameter tun-ing) on these evaluations, both in our own work andthroughout the existing literature.
We report on theresulting impact of various parameterizations, andour final results are based on a single set of parame-ters used across all evaluation sets.5 Experiments and ResultsWe wanted to answer the following questionsthrough our experiments: (1) How do hyper-parameters affect performance?
(2) What is the con-tribution of the multiple sources of data to perfor-mance?
(3) How does the performance of MVLSAcompare with other methods?
For brevity we showtuning runs only on the larger datasets.
We alsohighlight the top performing configurations in boldusing the small threshold values in column ?0.090.05ofTable 2.Effect of Hyper-parameters fj: We modeled thepreprocessing function fjas the composition of twofunctions, fj= nj?
tj.
njrepresents nonlinearpreprocessing that is usually employed with LSA.We experimented by setting njto be: identity; loga-rithm of count plus one; and the fourth root of thecount.
tjrepresents the truncation of columnsand can be interpreted as a type of regularization ofthe raw counts themselves through which we pruneaway the noisy contexts.
Decrease in tjalso reducesthe influence of views that have a large number ofcontext columns and emphasizes the sparser views.Table 3 and Table 4 show the results.Test Set Log Count Count14MEN 67.5 59.7 70.7RW 31.1 25.3 37.8SCWS 64.2 58.2 66.6AN-SYN 45.7 21.1 53.6AN-SEM 25.4 15.9 38.7Table 3: Performance versus nj, the non linear process-ing of cooccurrence counts.
t = 200K, m = 500, v =16, k = 300.
All the top configurations determined by?0.090.05are in bold font.Test Set 6.25K 12.5K 25K 50K 100K 200KMEN 70.2 71.2 71.5 71.6 71.2 70.7RW 41.8 41.7 41.5 40.9 39.6 37.8SCWS 67.1 67.3 67.1 67.0 66.9 66.6AN-SYN 59.2 60.0 59.5 58.4 56.1 53.6AN-SEM 37.7 38.6 39.4 39.2 38.4 38.7Table 4: Performance versus the truncation threshold, t,of raw cooccurrence counts.
We used nj= Count14andother settings were the same as Table 3.m: The number of left singular vectors extractedafter SVD of the preprocessed cooccurrence matri-ces can again be interpreted as a type of regular-ization, since the result of this truncation is that wefind cooccurrence patterns only between the top leftsingular vectors.
We set mj= max(dj,m) withm = [100, 300, 500].
See table 5.Test Set 100 200 300 500MEN 65.6 68.5 70.1 71.1RW 34.6 36.0 37.2 37.1SCWS 64.2 65.4 66.4 66.5AN-SYN 50.5 56.2 56.4 56.4AN-SEM 24.3 31.4 34.3 40.6Table 5: Performance versusm, the number of left singu-lar vectors extracted from raw cooccurrence counts.
Weset nj= Count14, t = 100K, v = 25, k = 300.k: Table 6 demonstrates the variation in perfor-mance versus the dimensionality of the learnt vec-tor representations of the words.
Since the dimen-sions of the MVLSA representations are orthogonalto each other therefore creating lower dimensionalrepresentations is a trivial matrix slicing operationand does not require retraining.Test Set 10 50 100 200 300 500MEN 49.0 67.0 69.7 70.2 70.1 69.8RW 28.8 33.3 35.0 35.2 37.2 38.3SCWS 57.8 64.4 65.2 66.1 66.4 65.1AN-SYN 9.0 41.2 52.2 55.4 56.4 54.4AN-SEM 2.5 21.8 34.8 35.8 34.3 33.8Table 6: Performance versus k, the final dimensionalityof the embeddings.
We set m = 300 and other settingswere same as Table 5.v: Expression 12 describes a method to set Wj.We experimented with a different, more global,561heuristic to set [Wj]ii= (Kww?
v), essentiallyremoving all words that did not appear in v viewsbefore doing GCCA.
Table 7 shows that changes inv are largely inconsequential for performance.Test Set 16 17 21 25 29MEN 70.4 70.4 70.2 70.1 70.0RW 39.9 38.8 39.7 37.2 33.5SCWS 67.0 66.8 66.5 66.4 65.7AN-SYN 56.0 55.8 55.9 56.4 56.0AN-SEM 34.6 34.3 34.0 34.3 34.3Table 7: Performance versus minimum view supportthreshold v, The other hyperparameters were nj=Count14, m = 300, t = 100K.
Though a clear bestsetting did not emerge, we chose v = 25 as the middleground.rj: The regularization parameter ensures that allthe inverses exist at all points in our method.
Wefound that the performance of our procedure was in-variant to r over a large range from 1 to 1e-10.
Thiswas because even the 1000th singular value of ourdata was much higher than 1.Contribution of different sources of data Table 8shows an ablative analysis of performance wherewe remove individual views or some combinationof them and measure the performance.
It is clear bycomparing the last column to the second column thatadding in more views improves performance.
Alsowe can see that the Dependency based views and theBitext based views give a larger boost than the mor-phology and FrameNet based views, probably be-cause the latter are so sparse.Comparison to other word representation cre-ation methods There are a large number of meth-ods of creating representations both multilingual andmonolingual.
There are many new methods such asby Yu and Dredze (2014), Faruqui et al (2014), Hilland Korhonen (2014), and Weston et al (2014) thatare performing multiview learning and could be con-sidered here as baselines: however it is not straight-forward to use those systems to handle the varietyof data that we are using.
Therefore, we directlycompare our method to the Glove and the SkipGrammodel of Word2Vec as the performance of those sys-tems is considered state of the art.
We trained thesetwo systems on the English portion of the PolyglotWikipedia dataset.5We also combined their outputsusing MVLSA to create MV-G-WSG) embeddings.We trained our best MVLSA system with datafrom all views and by using the individual bestsettings of the hyper-parameters.
Specifically theconfiguration we used was as follows: nj=Count14, t = 12.5K,m = 500, k = 300, v = 16.To make a fair comparison we also provide resultswhere we used only the views derived from the Poly-glot Wikipedia corpus.
See column MVLSA (AllViews) and MVLSA (Wiki) respectively.
It is clearlyvisible that MVLSA on the monolingual data itselfis competitive with Glove but worse than Word2Vecon the word similarity datasets and it is substan-tially worse than both the systems on the AN-SYNand AN-SEM datasets.
However with the additionof multiple views MVLSA makes substantial gains,shown in column MV Gain, and after consuming theGlove and WSG embeddings it again improves per-formance by some margins, as shown in column G-WSG Gain, and outperforms the original systems.Using GCCA itself for system combination providesclosure for the MVLSA algorithm since multipledistinct approaches can now be simply fused usingthis method.
Finally we contrast the Spearman cor-relations rswith Glove and Word2Vec before andafter including them in the GCCA procedure.
Thevalues demonstrate that including Glove and WSGduring GCCA actually increased the correlation be-tween them and the learnt embeddings, which sup-ports our motivation for performing GCCA in thefirst place.6 Previous WorkVector space representations of words have been cre-ated using diverse frameworks including Spectralmethods (Dhillon et al, 2011; Dhillon et al, 2012),6Neural Networks (Mikolov et al, 2013b; Col-lobert and Lebret, 2013), and Random Projections(Ravichandran et al, 2005; Bhagat and Ravichan-5We explicitly provided the vocabulary file to Glove andWord2Vec and set the truncation threshold for Word2Vec to10.
Glove was trained for 25 iterations.
Glove was provideda window of 15 previous words and Word2Vec used a symmet-ric window of 10 words.6cis.upenn.edu/?ungar/eigenwords562Test SetAllViews!Framenet !Morphology !Bitext !Wikipedia !Dependency!Morphology!Framenet!Morphology!Framenet!BitextMEN 70.1 69.8 70.1 69.9 46.4 68.4 69.5 68.4RW 37.2 36.4 36.1 32.2 11.6 34.9 34.1 27.1SCWS 66.4 65.8 66.3 64.2 54.5 65.5 65.2 60.8AN-SYN 56.4 56.3 56.2 51.2 37.6 50.5 54.4 46.0AN-SEM 34.3 34.3 34.3 36.2 4.1 35.3 34.5 30.6Table 8: Performance versus views removed from the multiview GCCA procedure.
!Framenet means that the viewcontaining counts derived from Frame semantic dataset was removed.
Other columns are named similarly.
The otherhyperparameters were nj= Count14, m = 300, t = 100K, v = 25, k = 300.Test Set Glove WSG MV MVLSA MVLSA MVLSA MV G-WSG rsMVLSA rsMV-G-WSGG-WSG Wiki All Views Combined Gain Gain Glove WSG Glove WSGMEN 70.4 73.9 76.0 71.4 71.2 75.8 ?0.2 4.6?71.9 89.1 85.8 92.3RW 28.1 32.9 37.2 29.0 41.7 40.5 12.7?
?1.2 72.3 74.2 80.2 75.6SCWS 54.1 65.6 60.7 61.8 67.3 66.4 5.5?
?0.9 87.1 94.5 91.3 96.3SIMLEX 33.7 36.7 41.1 34.5 42.4 43.9 7.9?1.5 62.4 78.2 79.3 86.0WS 58.6 70.8 67.4 68.0 70.8 70.1 2.8?
?0.7 72.3 88.1 81.8 91.8MTURK 61.7 65.1 59.8 59.1 59.7 62.9 0.6 3.2 80.0 87.7 87.3 92.5WS-REL 53.4 63.6 59.6 60.1 65.1 63.5 5.0?
?1.6 58.2 81.0 69.6 85.3WS-SEM 69.0 78.4 76.1 76.8 78.8 79.2 2.0 0.4 74.4 90.6 83.9 94.0RG 73.8 78.2 80.4 71.2 74.4 80.8 3.2 6.4?80.3 90.6 91.8 92.9MC 70.5 78.5 82.7 76.6 75.9 77.7 ?0.7 2.8 80.1 94.1 91.4 95.8AN-SYN 61.8 59.8 51.0 42.7 60.0 64.3 17.3?4.3?AN-SEM 80.9 73.7 73.5 36.2 38.6 77.2 2.4?38.6?TOEFL 83.8 81.2 86.2 78.8 87.5 88.8 8.7?1.3Table 9: Comparison of Multiview LSA against Glove and WSG(Word2Vec Skip Gram).
Using ?0.90.05as the thresholdwe highlighted the top performing systems in bold font.
?marks significant increments in performance due to use ofmultiple views in the Gain columns.
The rscolumns demonstrate that GCCA increased pearson correlation.dran, 2008; Chan et al, 2011).7They have beentrained using either one (Pennington et al, 2014)8or two sources of cooccurrence statistics (Zou et al,2013; Faruqui and Dyer, 2014; Bansal et al, 2014;Levy and Goldberg, 2014)9or using multi-modaldata (Hill and Korhonen, 2014; Bruni et al, 2012).Dhillon et al (2011) and Dhillon et al (2012)were the first to use CCA as the primary methodto learn vector representations and Faruqui andDyer (2014) further demonstrated that incorporat-7code.google.com/p/word2vec,metaoptimize.com/projects/wordreprs8nlp.stanford.edu/projects/glove9ttic.uchicago.edu/?mbansal/data/syntacticEmbeddings.zip,cs.cmu.edu/?mfaruqui/soft.htmling bilingual data through CCA improved perfor-mance.
More recently this same phenomenon wasreported by Hill et al (2014a) through their exper-iments over neural representations learnt from MTsystems.
Various other researchers have tried to im-prove the performance of their paraphrase systemsor vector space models by using diverse sources ofinformation such as bilingual corpora (Bannard andCallison-Burch, 2005; Huang et al, 2012; Zou et al,2013),10structured datasets (Yu and Dredze, 2014;Faruqui et al, 2014) or even tagged images (Bruni10An example of complementary views: Chan et al (2011)observed that monolingual distributional statistics are suscep-tible to conflating antonyms, where bilingual data is not; onthe other hand bilingual statistics are susceptible to noisy align-ments, where monolingual data is not.563et al, 2012).
However, most previous work11didnot adopt the general, simplifying view that all ofthese sources of data are just cooccurrence statisticscoming from different sources with underlying la-tent factors.12Bach and Jordan (2005) presented a probabilisticinterpretation for CCA.
Though they did not gener-alize it to include GCCA we believe that one couldgive a probabilistic interpretation of MAX-VARGCCA.
Such a probabilistic interpretation would al-low for an online-generative model of lexical repre-sentations, which unlike methods like Glove or LSAwould allows us to naturally perplexity or generatesequences.
We also note that V?
?a et al (2007) pre-sented a neural network model of GCCA and adap-tive/incremental GCCA.
To the best of our knowl-edge both of these approaches have not been usedfor word representation learning.CCA is also an algorithm for multi-view learning(Kakade and Foster, 2007; Ganchev et al, 2008) andwhen we view our work as an application of multi-view learning to NLP, this follows a long chain of ef-fort started by Yarowsky (1995) and continued withCo-Training (Blum and Mitchell, 1998), CoBoost-ing (Collins and Singer, 1999) and 2 view percep-trons (Brefeld et al, 2006).7 Conclusion and Future WorkWhile previous efforts demonstrated that incorporat-ing two views is beneficial in word-representationlearning, we extended that thread of work to alogical extreme and created MVLSA to learn dis-tributed representations using data from 46 views!13Through evaluation of our induced representations,shown in Table 9, we demonstrated that the MVLSAalgorithm is able to leverage the information presentin multiple data sources to improve performance ona battery of tests against state of the art baselines.In order to perform MVLSA on large vocabularies11Ganitkevitch et al (2013) did employ a rich set of di-verse cooccurrence statistics in constructing the initial PPDB,but without a notion of ?training?
a joint representation beyondrandom projection to a binary vector subspace (bit-signatures).12Note that while Faruqui et al (2014) performed belief prop-agation over a graph representation of their data, such an undi-rected weighted graph can be viewed as an adjacency matrix,which is then also a cooccurrence matrix.13Code and data available at www.cs.jhu.edu/?prastog3/mvlsawith up to 500K words we presented a fast scalablealgorithm.
We also showed that a close variant ofthe Glove objective proposed by Pennington et al(2014) could be derived as a heuristic for handlingmissing data under the MVLSA framework.
In or-der to better understand the benefit of using mul-tiple sources of data we performed MVLSA usingviews derived only from the monolingual Wikipediadataset thereby providing a more principled alterna-tive of LSA that removes the need for heuristicallycombining word-word cooccurrence matrices into asingle matrix.
Finally, while surveying the litera-ture we noticed that not enough emphasis was beinggiven towards establishing the significance of com-parative results and proposed a method, (MRDS),to filter out insignificant comparative gains betweencompeting algorithms.Future Work Column MVLSA Wiki of Table 9shows us that MVLSA applied to monolingual datahas mediocre performance compared to the base-lines of Glove and Word2Vec on word similaritytasks and performs surprisingly worse on the AN-SEM dataset.
We believe that the results could beimproved by (1) either using recent methods forhandling missing values mentioned in footnote 1 orby using the heuristic count dependent non-linearweighting mentioned by Pennington et al (2014)and that sits well within our framework as exempli-fied in Expression 12 (2) by using even more views,which look at the future words as well as views thatcontain PMI values.
Finally, we note that Table 8shows that certain datasets can actually degrade per-formance over certain metrics.
Therefore we are ex-ploring methods for performing discriminative opti-mization of weights assigned to views, for purposesof task-based customization of learned representa-tions.AcknowledgmentsThis material is based on research sponsored bythe Defense Advanced Research Projects Agency(DARPA) under the Deep Exploration and Fil-tering of Text (DEFT) Program, agreement num-ber FA8750-13-2-001, as well as the NationalScience Foundation (NSF), agreement numberBCS-1344269.
We also thank Juri Ganitkevitch forproviding the word aligned bitext corpus.564ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and wordnet-based approaches.
In Proceedingsof NAACL-HLT.
ACL.Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013.Polyglot: Distributed word representations for multi-lingual nlp.
In Proceedings of CoNLL.
ACL.Animashree Anandkumar, Rong Ge, Daniel Hsu,Sham M. Kakade, and Matus Telgarsky.
2014.
Ten-sor decompositions for learning latent variable models.JMLR, 15.Raman Arora and Karen Livescu.
2012.
Kernel CCA formulti-view learning of acoustic features using articu-latory measurements.
MLSLP.Francis R Bach and Michael I Jordan.
2005.
A prob-abilistic interpretation of canonical correlation analy-sis.
Technical Report 688, Department of Statistics,University of California, Berkeley.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of ACL.
ACL.Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014.Tailoring continuous word representations for depen-dency parsing.
In Proceedings of ACL.
ACL.Rahul Bhagat and Deepak Ravichandran.
2008.
Largescale acquisition of paraphrases for learning surfacepatterns.
In Proceedings of ACL-HLT.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of COLT.
ACM.Matthew Brand.
2002.
Incremental singular value de-composition of uncertain data with missing values.In Computer Vision?ECCV 2002, pages 707?720.Springer.Matthew Brand.
2006.
Fast low-rank modifications ofthe thin singular value decomposition.
Linear algebraand its applications, 415(1).Ulf Brefeld, Thomas G?artner, Tobias Scheffer, and StefanWrobel.
2006.
Efficient co-regularised least squaresregression.
In Proceedings of ICML.
ACM.Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran.
2012.
Distributional semantics in tech-nicolor.
In Proceedings of ACL.
ACL.J Douglas Carroll.
1968.
Generalization of canonicalcorrelation analysis to three or more sets of variables.In Proceedings of APA, volume 3.Tsz Ping Chan, Chris Callison-Burch, and Benjamin VanDurme.
2011.
Reranking bilingually extracted para-phrases using monolingual distributional similarity.
InProceedings of EMNLP Workshop: GEMS.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In Proceedingsof EMNLP.
ACL.Ronan Collobert and R?emi Lebret.
2013.
Word embed-dings through hellinger pca.
Technical report, Idiap.Paramveer Dhillon, Dean Foster, and Lyle Ungar.
2011.Multi-view learning of word embeddings via CCA.
InProcesdings of NIPS.Paramveer Dhillon, Jordan Rodu, Dean P Foster, andLyle H Ungar.
2012.
Two step CCA: A new spec-tral method for estimating vector models of words.
InProceedings of ICML.
ACM.Manaal Faruqui and Chris Dyer.
2014.
Improving vectorspace word representations using multilingual correla-tion.
In Proceedings of EACL.Manaal Faruqui, Jesse Dodge, Sujay Jauhar, Chris Dyer,Eduard Hovy, and Noah Smith.
2014.
Retrofittingword vectors to semantic lexicons.
In Proceedings ofthe deep learning and representation learning work-shop, NIPS.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context: The conceptrevisited.
In Proceedings of WWW.
ACM.Kuzman Ganchev, Joao Graca, John Blitzer, and BenTaskar.
2008.
Multi-view learning over structured andnon-identical outputs.
In Proceedings of UAI.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
Ppdb: The paraphrasedatabase.
In Proceedings of NAACL-HLT.Nizar Habash and Bonnie Dorr.
2003.
Catvar: Adatabase of categorial variations for english.
In Pro-ceedings of MT Summit.Trevor Hastie, Robert Tibshirani, and Jerome Friedman.2009.
The Elements Of Statistical Learning, volume 2.Springer.Felix Hill and Anna Korhonen.
2014.
Learning abstractconcept embeddings from multi-modal data: Sinceyou probably can?t see what i mean.
In Proceedingsof EMNLP.
ACL.Felix Hill, KyungHyun Cho, Sebastien Jean, ColineDevin, and Yoshua Bengio.
2014a.
Not all neu-ral embeddings are born equal.
arXiv preprintarXiv:1410.0718.Felix Hill, Roi Reichart, and Anna Korhonen.
2014b.Simlex-999: Evaluating semantic models with(genuine) similarity estimation.
arXiv preprintarXiv:1408.3456.Paul Horst.
1961.
Generalized canonical correlationsand their applications to experimental data.
Journalof Clinical Psychology, 17(4).565Eric H. Huang, Richard Socher, Christopher D. Manning,and Andrew Y. Ng.
2012.
Improving word representa-tions via global context and multiple word prototypes.In Proceedings of ACL.
ACL.Sham M Kakade and Dean P Foster.
2007.
Multi-view regression via canonical correlation analysis.
InLearning Theory.
Springer.Jon R Kettenring.
1971.
Canonical analysis of severalsets of variables.
Biometrika, 58(3):433?451.Thomas K Landauer and Susan T Dumais.
1997.
A so-lution to plato?s problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychological review, 104(2):211.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proceedings of ACL.ACL.Minh-Thang Luong, Richard Socher, and Christopher D.Manning.
2013.
Better word representations with re-cursive neural networks for morphology.
In Proceed-ings of CoNLL.
ACL.Quinn McNemar.
1947.
Note on the sampling error ofthe difference between correlated proportions or per-centages.
Psychometrika, 12(2).Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado,and Jeff Dean.
2013a.
Distributed representations ofwords and phrases and their compositionality.
In Pro-ceedings of NIPS.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Proceedings of NAACL-HLT,pages 746?751.George A. Miller and Walter G. Charles.
1991.
Contex-tual correlates of semantic similarity.
Language andCognitive Processes, 6(1).Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of english.
NaturalLanguage Engineering, 7(03).Courtney Napoles, Matthew Gormley, and Benjamin VanDurme.
2012.
Annotated gigaword.
In Proceedingsof NAACL Workshop: AKBC-WEKEX.Jeffrey Pennington, Richard Socher, and Christopher D.Manning.
2014.
Glove: global vectors for word rep-resentation.
In Proceedings of EMNLP.
ACL.Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich,and Shaul Markovitch.
2011.
A word at a time: Com-puting word relatedness using temporal semantic anal-ysis.
In Proceedings of WWW.
ACM.Pushpendre Rastogi and Benjamin Van Durme.
2014.Augmenting framenet via PPDB.
In Proceedings ofthe Second Workshop on EVENTS: Definition, Detec-tion, Coreference, and Representation.
ACL.Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.2005.
Randomized algorithms and nlp: Using localitysensitive hash functions for high speed noun cluster-ing.
In Proceedings of ACL.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Communicationsof the ACM, 8(10).Dmitry Savostyanov.
2014.
Efficient way to findsvd of sum of projection matrices?
MathOver-flow.
URL:http://mathoverflow.net/q/178573 (version:2014-08-14).Karthik Sridharan and Sham M Kakade.
2008.
An infor-mation theoretic framework for multi-view learning.In Proceedings of COLT.James H Steiger.
1980.
Tests for comparing elements ofa correlation matrix.
Psychological Bulletin, 87(2).Peter D Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.Journal of AI Research, 37(1).Michel Van De Velden and Tammo HA Bijmolt.
2006.Generalized canonical correlation analysis of matriceswith missing rows: a simulation study.
Psychome-trika, 71(2).Michel van de Velden and Yoshio Takane.
2012.
Gener-alized canonical correlation analysis with missing val-ues.
Computational Statistics, 27(3).Javier V?
?a, Ignacio Santamar?
?a, and Jes?us P?erez.
2007.
Alearning algorithm for adaptive canonical correlationanalysis of several data sets.
Neural Networks, 20(1).Jason Weston, Sumit Chopra, and Keith Adams.
2014.#tagspace: Semantic embeddings from hashtags.
InProceedings of EMNLP, Doha, Qatar.
ACL.Terry Winograd.
1972.
Understanding natural language.Cognitive psychology, 3(1):1?191.David Yarowsky.
1995.
Unsupervised WSD rivaling su-pervised methods.
In Proceedings of ACL.
ACL.Mo Yu and Mark Dredze.
2014.
Improving lexical em-beddings with semantic knowledge.
In Proceedings ofACL.
ACL.Will Zou, Richard Socher, Daniel Cer, and ChristopherManning.
2013.
Bilingual word embeddings forphrase-based machine translation.
In Proceedings ofEMNLP.
ACL.566
