Proceedings of the 6th Workshop on Statistical Machine Translation, pages 452?456,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsThe BM-I2R Haitian-Cre?ole-to-English translation system descriptionfor the WMT 2011 evaluation campaignMarta R. Costa-jussa`Barcelona Media Innovation CenterAv Diagonal, 177, 9th floor08018 Barcelonamarta.ruiz@barcelonamedia.orgRafael E. BanchsInstitute for Infocomm Research1 Fusionopolis Way 21-01Singapore 138632rembanchs@i2r.a-star.edu.sgAbstractThis work describes the Haitian-Cre?ole to En-glish statistical machine translation systembuilt by Barcelona Media Innovation Center(BM) and Institute for Infocomm Research(I2R) for the 6th Workshop on Statistical Ma-chine Translation (WMT 2011).
Our sys-tem carefully processes the available data anduses it in a standard phrase-based system en-hanced with a source context semantic featurethat helps conducting a better lexical selectionand a feature orthogonalization procedure thathelps making MERT optimization more reli-able and stable.
Our system was ranked first(among a total of 9 participant systems) by theconducted human evaluation.1 IntroductionDuring years there has been a big effort to producenatural language processing tools that try to under-stand well written sentences, but the question is howwell do these tools work to analyze the contentsof SMS.
For example, not even syntactic tools likestemming can bring to common stems words thathave been shortened (like Xmas or Christmas).This paper describes our participation on the 6thWorkshop on Statistical Machine Translation (WMT2011).
The featured task from the workshop wasto translate Haitian-Cre?ole SMS messages into En-glish.
According to the WMT 2011 organizers, thesetext messages (SMS) were sent by people in Haiti inthe aftermath of the January 2010 earthquake.
Ourobjective in this featured task is to translate fromHaitian-Cre?ole into English either using raw or cleandata.We propose to build an SMT system which couldbe used for both raw and clean data.
Our base-line system is an standard phrase-based SMT sys-tem built with Moses (Koehn et al, 2007).
Startingfrom this system we propose to introduce a semanticfeature function based on latent semantic indexing(Landauer et al, 1998).
Additionally, as a total dif-ferent approximation, we propose to orthogonalizethe standard feature functions of the phrase-basedtable using the Gram-Schmidt methodology (Greub,1975).
Then, we experimentally combine both en-hancements.The only difference among the raw and cleanSMT system were the training sentences.
In orderto translate the clean data, we propose to normalizethe corpus of short messages given very scarce re-sources.
We only count with a small set of parallelcorpus at the level of sentence of chat and standardlanguage.
A nice normalization methodology canallow to make the task of communication easier.
Wepropose a statistical normalization technique usingthe scarce resources we have based on a combina-tion of statistical machine translation techniques.The rest of this paper is organized as follows.
Sec-tion 2 briefly describes the phrase-based SMT sys-tem which is used as a reference system.
Next, sec-tion 3 describes our approximation to introduce se-mantics in the baseline system.
Section 4 reports ouridea of orthogonalizing the feature functions in thetranslation table.
Section 5 details the data process-ing and the data conversion from raw to clean.
Asfollows, section 6 shows the translation results.
Fi-nally, section 7 reports most relevant conclusions ofthis work.4522 Phrase-based SMT baseline systemThe phrase-based approach to SMT performs thetranslation splitting the source sentence in segmentsand assigning to each segment a bilingual phrasefrom a phrase-table.
Bilingual phrases are trans-lation units that contain source words and targetwords, e.g.
unite?
de traduction ?
translation unit,and have different scores associated to them.
Thesebilingual phrases are then selected in order to maxi-mize a linear combination of feature functions.
Suchstrategy is known as the log-linear model (Och,2003) and it is formally defined as:e?
= argmaxe[M?m=1?mhm (e, f)](1)where hm are different feature functions withweights ?m.
The two main feature functions arethe translation model (TM) and the target lan-guage model (LM).
Additional models include lexi-cal weights, phrase and word penalty and reordering.3 Semantic feature functionSource context information is generally disregardedin phrase-based systems given that all training sen-tences contribute equally to the final translation.The main objective in this section is to motivatethe use of a semantic feature function we have re-cently proposed (Banchs and Costa-jussa`, 2011) forincorporating source context information into thephrase-based statistical machine translation frame-work.
Such a feature is based on the use of a sim-ilarity metric for assessing the degree of similaritybetween the sentences to be translated and the sen-tences in the original training dataset.The measured similarity is used to favour thosetranslation units that have been extracted from train-ing sentences that are similar to the current sen-tence to be translated and to penalize those trans-lation units than have been extracted from unrelatedor dissimilar training sentences.
In the proposed fea-ture, sentence similarity is measured by means of thecosine distance in a reduced dimension vector-spacemodel, which is constructed by using Latent Seman-tic Indexing (Landauer et al, 1998), a well knowdimensionality reduction technique that is based onthe singular value decomposition of a matrix (Goluband Kahan, 1965).The main motivation of this semantic feature isthe fact that source context information is actuallyhelpful for disambiguating the sense of a given wordduring the translation process.
Consider for instancethe Spanish word banco which can be translated intoEnglish as either bank or bench depending on thespecific context it occurs.
By comparing a giveninput sentence containing the Spanish word bancowith all training sentences from which phrases in-cluding this word where extracted, we can figureout which is the most appropriated sense for thisword in the given sentence.
This is because for thesense bank the Spanish word banco will be morelike to co-occur with words such as dinero (money),cuenta (account), intereses (interest), etc., while forthe sense bench it would be more likely to co-occurwith words such as plaza (square), parque (park),mesa (table), etc; and the chances are high for suchdisambiguating words to appear in one or more ofthe training sentences from which bilingual phrasescontaining banco has been extracted.In the particular case of translation tasks wheremulti-domain corpora is used for training machinetranslation systems, such as the Haitian-Creole-to-English task considered here, the proposed seman-tic feature has proven to contribute to a better lexi-cal selection during the decoding process.
However,in tasks considering mono-domain corpora the se-mantic feature does not improves translation qualityas the most frequent translation pairs learned by thesystem are actually the correct ones.Another important issue related to the semanticfeature discussed here is that it is a dynamic featurein the sense that it is computed for each potentialtranslation unit according to the current input sen-tence being translated.
This makes the implementa-tion of this semantic feature very expensive from acomputational point of view.
At this moment, we donot have an efficient implementation, which makes itunfeasible in the practice to apply this methodologyto large training corpora.As the training corpus available for the Haitian-Creole-to-English is both small in size and multi-domain in nature, it constitutes the perfect scenariofor experimenting with the recently proposed sourcecontext semantic feature.
For more details about im-453plementation and performance of this methodologyin a different translation task, the reader should referto (Banchs and Costa-jussa`, 2011).4 Heuristic enhacementThe phrase-based SMT baseline system contains,by default, 5 feature functions which are the con-ditional and posterior probabilities, the direct andindirect lexical scores and the phrase penalty.
Usu-ally, these feature functions are not statistical inde-pendent from each other.
Based on the analogy be-tween the statistical and geometrical concepts of in-dependence and orthogonality, and given that, dur-ing MERT, the optimization of feature combinationis conducted on log-probability space; we decidedto explore the effect of using a set of orthogonal fea-tures during MERT optimization.It is well know in both spectral analysis and vec-tor space decomposition that orthogonal bases allowfor optimal representations of signals and variables,as they allow for each individual natural componentto be represented independently of the others.
Inlinear lattice predictors, for instance, each filter co-efficient can be optimized independently from theothers while convergence to the optimal solution isguarantied (Haykin, 1996).
In the case of statisti-cal machine translation, the linear nature of featurecombination in log-probability space suggested usthat transforming the features into a set of orthog-onal features could make MERT optimization morerobust and efficient.According to this, we used Gram-Schmidt(Greub, 1975) to transform all available featurefunctions into an orthogonal set of feature func-tions.
This orthogonalization process was con-ducted directly over the log-probability space, i.e,given the five vectors representing the featurefunctions h1, h2, h3, h4, h5, we used the Gram-Schmidt algorithm to construct an orthogonal basisv1, v2, v3, v4, v5.
The resulting set of features con-sisted of 5 vectors that form an orthogonal basis.This new orthogonal set of features was used forMERT optimization and decoding.5 Experimental frameworkIn this section we report the details of the used datapreprocessing and raw to clean data conversion.5.1 Data preprocessingThe WMT evaluation provided a high variety ofdata.
Our preprocessing consisted of the following:?
Lowercase and tokenize all files using thescripts from Moses.?
In the case of the haitian-Creole side of thedata, replace all stressed vowels by their plainforms.?
Filter out those sentences which had no wordsor more than 120.Table 1 shows the data statistics of the differentsources before and after this preprocessing.
The dif-ferent sources of the table include: in-domain SMSdata (SMS); medical domain (medical); newswiredomain (newswire); united nations (un); state de-partment (state depart.
); guidelines for approapriateinternational disaster donations (guidelines); kren-gle senetences (krengle) and a glossary includeswikipedia name entities and haitisurf dictionary.The sources of this material are specified in the webpage of the workshop.All data from table 1 was concatenated and usedas training corpus.
The English part of this data wasused to build the language model.
As developmentand test corpus we used the data provided by theorganization.
Both development and test contained900 sentences.Finally, in the evaluation, we included develop-ment and tests as part of the training corpus, andthen, we translated the evaluation set.5.2 Raw to clean data conversionThis featured task contained two subtasks.
One wasto translate raw data and the other was to translateclean data.
Therefore, we have to build two sys-tems.
Our raw data system was built using the train-ing data from table 1.
The clean data system wasbuilt using all training data from table 1 except in-domain SMS data.
Particularly, a modified versionof the in-domain SMS data was included in the cleandata system.
The modification consisted in cleaningthe original in-domain SMS data using an standardMoses SMT system.
We built an SMT system totranslate from raw data to clean data.
This SMT sys-tem was built with the development, test and evalu-ation data which in total were 2700 sentences.
We454Statisticsbefore afterSMSsentences 17,192 16,594words 386.0k 383.0kmedicalsentences 1,619 1,619words 10.4k 10.4knewswiresentences 13,517 13,508words 326.9k 326.7kwikipediasentences 8,476 8,476words 113.9k 113.9kunsentences 91 91words 1,906 1,906state depart.sentences 56 14words 450 355guidelinessentences 60 9words 795 206krenglesentences 658 655words 4.2k 4.2kbiblesentences 30,715 30,677words 946k 944kglossarysentences 49,990 49,980words 126.4k 126.3kTable 1: Data Statistics before and after training prepro-cessing.
Number of words are from the English side.used 2500 sentences as training data and 200 sen-tences for development to adjust weights.
The rawand clean systems were tuned with their respectivedevelopments and tested on their respective tests.6 Experimental resultsIn this section we report the results of the approachesproposed in previous sections.
Table 2 and 3 reportthe results on the development and test sets on theraw and clean subtask, respectively.First row on both tables report the results of thebaseline system briefly described in section 2.
Sec-ond row and third row on both tables report the per-formance of the semantic feature function and on theheuristic approach of orthogonalization (orthofea-tures) respectively.
Finally, the last row on bothtables report the performance of both semantic andheuristic features when combined.Results shown in tables 2 and 3 do not showcoherent improvements when introducing the newSystem Dev Testbaseline 32.00 31.01+semanticfeature 32.34 30.68+orthofeatures 31.63 29.90+semanticfeature+orthofeatures 32.21 30.34Table 2: BLEU results for the raw data.
Best results inbold.System Dev Testbaseline 35.86 33.78+semanticfeature 35.98 33.90+orthofeatures 35.57 34.10+semanticfeature+orthofeatures 36.28 33.53Table 3: BLEU results for the clean data.
Best results inbold.methodologies proposed.
The clean data seems tobenefit from the semantic features and the orthofea-tures separately.
However, the raw data seems not tobenefit from the orthofeatures and keep the similarperformance to the baseline system when using thesemantic feature.
Although, this trend is clear, theresults are not conclusive.
Therefore, we decided toparticipate in the evaluation with the full system (in-cluding the semantic features and orthofeatures) inthe clean track and with the system including the se-mantic feature in the raw track.
Actually, we usedthose systems that performed best in the develop-ment set.
Additionally, results with the semanticfeature may not be significantly better than the base-line system, but we have seen it actually heps to im-prove lexical selection in practice in previous works(Banchs and Costa-jussa`, 2011).7 ConclusionsThis paper reports the BM-I2R system description inthe Haitian-Cre?ole to English translation task.
Thissystem was ranked first in the WMT 2011 by theconducted human evaluation.
The translation sys-tem uses a PBSMT system enhanced with two dif-ferent methodologies.
First, we experiment with theintroduction of a semantic feature which is capa-ble of introducing source context information.
Sec-ond, we propose to transform the five standard fea-ture functions used in the translation model of thePBSMT system into five orthogonal feature func-455tions using the Gram-Schmidt methodology.
Resultsshow that the first methodology can be used for bothraw and clean data.
Whereas the second seems toonly benefit clean data.AcknowledgmentsThe research leading to these results has receivedfunding from the Spanish Ministry of Science andInnovation through the Juan de la Cierva fellowshipprogram.
The authors would like to thank BarcelonaMedia Innovation Center and Institute for InfocommResearch for their support and permission to publishthis research.ReferencesR.
Banchs and M.R.
Costa-jussa`.
2011.
A semantic fea-ture for statistical machine translation.
In 5th Work-shop on Syntax, Semantics and Structure in StatisticalTranslation (at ACL HLT 2011), Portland.G.
H. Golub and W. Kahan.
1965.
Calculating the sin-gular values and pseudo-inverse of a matrix.
journal ofthe society for industrial and applied mathematics.
InNumerical Analysis 2(2), pages 205?224.W.
Greub.
1975.
Linear Algebra.
Springer.S.
Haykin.
1996.
Adaptive Filter Theory.
Prentice Hall.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proc.
of the 45thAnnual Meeting of the Association for ComputationalLinguistics, pages 177?180, Prague, Czech Republic,June.T.
K. Landauer, D. Laham, and P. Foltz.
1998.
Learninghuman-like knowledge by singular value decomposi-tion: A progress report.
In Conference on Advances inNeural Information Processing Systems, pages 45?51,Denver.F.J.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
of the 41th Annual Meet-ing of the Association for Computational Linguistics,pages 160?167, Sapporo, July.456
