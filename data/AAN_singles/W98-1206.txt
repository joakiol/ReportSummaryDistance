//II/IIIl/The effect of a l ternat ive  tree representat ions  on tree bankgrammarsMark Johnson*Cognitive and Linguistic Sciences, Box 1978Brown UniversityProvidence, RI 02912, USAMark_Johnson@Brown.
eduAbstractThe performance of PCFGs estimated fromtree banks is shown to be sensitive to the par-ticular way in which linguistic constructionsare represented as trees in the tree bank.
Thispaper presents a theoretical nalysis of theeffect of different ree representations for PPattachment on PCFG models, and introducesa new methodology for empirically examin-ing such effects using tree transformations.
Itshows that one transformation, which copiesthe label of a parent node onto the labels ofits children, can improve the performance ofa PCFG model in terms of labelled precisionand recall on held out data from 73% (pre-cision) and 69% (recall) to 80% and 79% re-spectively.
It also points out that if only max-imum likelihood parses are of interest henmany productions can be ignored, since theyare subsumed by combinations of other pro-ductions in the grammar.
In the Penn II treebank grammar, almost 9% of productions aresubsumed in this way.1 In t roduct ionParsers which are capable of analysing unre-stricted text are of considerable scientific inter-est, and have technological applications in ar-eas such as machine translation and informa-tion retrieval as well.
One way to produce sucha parser is to extract a grammar from one of thelarger tree bank corpora currently available.The relative frequency estimator describedbelow provides a simple way to estimate froma tree bank corpus a Probabalistic ContextFree Grammar (PCFG) that generates Part OfI would like to thank Chris Manning, whose observa-tion that PCFG parsers do not accurately reproduce PPattachment preferences in their training data stimulatedthis work, as well as Eugene Charniak, Stuart Gemanand our students at Brown.Speech (POS) tags.
Such a PCFG induced froma sufficiently large corpus typically generates allpossible POS tag strings.
A parsing system canbe obtained by using a parser to find the max-imum likelihood parse tree for an input string.Such parsing systems often perform as well asother broad coverage parsing systems for pre-dicting tree structure from POS tags (Char-niak, 1996).
In addition, many more sophis-ticated parsing models are elaborations of suchPCFG models, so understanding the propertiesof PCFGs is likely to be useful (Charniak, 1997;Collins, 1997).It is well-known that natural language ex-hibits dependencies that Context Free Gram-mars (CFGs), and hence PCFGs, cannot de-scribe (Shieber, 1985).
But as explained be-low, the independence assumptions implicit inPCFGs introduce biases in the statistical modelinduced from a tree bank even in constructionswhich are adequately described by a CFG.
Thedirection and size of these biases depend on fac-tors such as the following:?
the precise tree structures used in the treebank, and?
whether the set of well-formed trees accord-ing to the linguistic model used to assigntrees to strings can be described with aCFG.This paper explains how such biases can arise,and presents a series of experiments in which thetrees of a tree bank corpus are systematicallytransformed to other tree structures to obtain agrammar used for parsing, and the inverse treeJohnson 39 The effect of alternative tree representationsMark Johnson (1998) The effect of alternative tree representations on tree bank grammars.
In D.M.W.
Powers (ed.
)NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 39-48.transform is applied to the structures producedusing this grammar before evaluation.
One ofthe transformations described here improves theaverage labelled precision and recall on held outdata from 73% (precision) and 69% (recall) to80% and 79% respectively.2 P robaba l i s t i c  Context  F reeGrammarsA PCFG is a CFG in which each productionA ~ a in the grammar's et of productions R isassociated with an emission probability P(Aa) that satisfies a normalization constrainty~ P(A~a)  = 1~:A--+c~ERand a consistency or tightness constraint notdiscussed here.A PCFG defines a probability distributionover the (finite) parse trees generated by thegrammar, where the probability of a tree T isgiven byP(T) = H P(A ~ o0 Cr(A'~a)A.-+o~ERwhere C~(A --+ a) is the 'count' of the localtree consisting of a parent node labelled A witha sequence of immediate children nodes labelleda in r, or equivalently, the number of times theproduction A ~ a is used in the derivation ~-.The PCFG which assigns maximum likeli-hood to a tree bank corpus ~ is given by therelative frequency estimator.Pe(A ~ a) = C~(A --+ a)~a':A~a'eR C~(A -~ a')Here C?
(A -+ a) refers to the 'count' of thelocal tree in the tree bank, or equivalently, thenumber of times the production A -+ a wouldbe used in derivations of exactly the trees in ~.It is practical to induce PCFGs from treebank corpora and find maximum likelihoodparses for such PCFGs using relatively modestcomputing equipment.
All the experiments re-ported here used the Penn II Wall Street Jour-nal (WSJ) corpus, modified as described byCharniak (Charniak, 1996), i.e., empty nodeswere deleted, and all other components ofnodeslabels except syntactic ategory were removed.Grammar induction or training used the39,832 trees in the F2-21 sections of the Penn IIWSJ corpus, and testing was performed on the1,576 sentences of length 40 or less of the F22section of the corpus.
Parsing was performedusing an exhaustive CKY parser that returneda maximum likelihood parse.
Ties betweenequally likely parses were broken randomly; onthe tree bank grammar this leads to fluctuationsin labelled precision and recall with a standarddeviation of approximately 0.07%.3 D i f fe rent  t ree  s t ruc turerepresentat ions  of  ad junct ionThere is considerable variation in the tree struc-tures used in the linguistic literature to repre-sent various linguistic constructions.
In this pa-per we focus on variations in the representationof adjunction constructions, particularly PP ad-junction, but similiar variation occurs in otherconstructions as well.Early analyses in transformational grammartypically adopted a 'flat' representation of ad-junction structures in which adjuncts are repre-sented as siblings of the phrasal head, as shownin Figure 1.
This representation does not sys-tematically distinguish between adjuncts andarguments, as both are attached as children ofa single maximal projection.The Penn II tree bank represents PP adjunc-tion to VP in this manner, presumably becauseit permits the annotators to avoid having to de-termine whether the PP in question is an ad-junct or an argument.Because this representation attaches all of theadjuncts modifying the same phrase to the samenode, distinct CFG productions are required foreach possible number of adjuncts.
Thus the setof all possible trees following this representa-tion scheme can only be generated by a CFGif one imposes an upper bound on the numberof PPs that can be adjoined to any one singlephrase, but according to standard linguistic wis-dom there is no natural bound on the numberof PPs that may be adjoined to a single phrase.Johnson 40 The effect of alternative tree representationsmB|mmmmmmmDmmmImIIII|IIIIil||II|ilIII!IIVP VPI I l I I J tate her dinner with a fork ate her dinner with a fork at the tableFigure 1: 'Flat' attachment representations of adjunction, where adjuncts are attached as siblingsof a lexical head (in this case, the verb ate).
The Penn II tree bank represents VP adjunction inthis manner.Later transformational nalyses adopted themore complex 'Chomsky adjunction' repre-sentation of adjunction structures for theory-internal reasons (e.g., it was a corollary ofEmmonds' "Structure Preserving Hypothesis").This representation provides an additional levelof recursive phrasal structure for each adjunct,as depicted in Figure 2.Modern transformational grammar, followingChomsky's X I theory of phrase structure, rep-resents adjunction with similiar recursive struc-tures; the major difference being that the non-maximal phrasal nodes are given a new, distinctcategory label.Because the Chomsky adjunction structureand the X I theory based on it use a single ruleto recursively adjoin an arbitrary number of ad-juncts, the set of all tree structures required bythis representation scheme can be generated bya CFG.The Penn II tree bank uses a mixed kind ofrepresentation for NP adjunction, involving twolevels of phrasal structure irrespective of thenumber of adjuncts, as shown in Figure 3.
Thisrepresentation permits adjuncts to be system-atically distinguished from arguments, althoughthis does not seem to have been done systemat-ically in the Penn II corpus.
1 Just as with the'fiat' representation, the set of all possible treesrequired by this mixed representation cannot be1The tree annotation conventions u ed in the Penn IIcorpus are described indetail in (Bies et al, 1995).
Themixed representation arises from the fact that "postmod-ifiers are Chomsky-adjoined to the phrase they modify"with the proviso that "consecutive unrelated adjunctsare non-recursively attached to the NP the modify".However, because constructions such as appositives, em-phatic reflexives and phrasal titles are associated withtheir own level of NP structure, it is possible for NPswith more than two levels of structure to appear.NPNP PPN~/"~P about Ithe rulesI tN to the agencylettersFigure 4: A tree structure generated by anyPCFG that generates the trees in Figure 3, yet itdoes not fit the general representational schemefor adjunction structures used in the Penn IItree bank.generated by a CFG unless the number of PPsadjoined to a single phrase is bounded.Perhaps more seriously for PCFG modellingof such tree structures, a PCFG which can gen-erate a nontrivial subset of such 'two level' NPtree structures will also generate tree struc-tures which are not instances of this representa-tional scheme.
For example, the NP productionneeded to produce the leftmost ree in Figure 3can apply recursively, generating an alternativetree structure for the yield of the rightmost reeof Figure 3, as shown in Figure 4.
It is not clearwhat interpretation to give tree structures uchas these, as they do not fit the chosen represen-tational scheme for adjunction structures.4 PCFG mode ls  o f  PP  ad junct ionThis section presents a theoretical investigationinto the effect of different ree representationson the performance of PCFG models of PP  ad-junction.
The analysis of four different modelsis presented here.Clearly actual tree bank data is far more corn-Johnson 41 The effect of alternative tree representationsVPVP V f f~~'~PPPP at the table VP PP V ~ ~  I~ P  with \[a fork ~/~"~P with \[a forkI I I Iate her dinner ate her dinnerFigure 2: 'Chomsky adjunction' representations of adjunction, where each adjunct is attached asthe unique sibling of a phrasal node (in this case, VP).
Chomsky's X I theory, used by moderntransformational grammar, analyses adjunction in a structurally similiar way, except hat the non-maximal (in these examples, non-root) phrasal nodes are given a new category label (in this caseVI).NP NPNP PPN \[ to thelagency N \[ to thel agency about ~he rulesI Iletters lettersFigure 3: The representation f NP adjunction used in Penn II tree bank, where adjuncts areattached as siblings of a single NP node.plicated than the simple models investigated inthis section, and the next section investigatesthe effects of different ree representations em-pirically by applying tree transformations to thePenn II tree bank representations.
However, thetheoretical models discussed in this section showclearly that the choice of tree representation canin principle affect the generalizations made bya PCFG model.
(At) VP (B1) VPDet NDet N P NP Det N4.1 The  Penn II  tree bankrepresentat ionsSuppose we train a PCFG on a corpus ~1 con-sisting only of two different ree structures: theNP attachment s ructure labelled (A1) and theVP attachment tree labelled (B1).In the Penn II tree bank, structure (A1) oc-curs 7,033 times in the F2-21 subcorpora nd279 times in the F22 subcorpus, and struc-ture (B1) occurs 7,717 times in the F2-21 sub-corpora and 299 times in the F22 subcorpus.Thus f ~ 0.48 in both the F2-21 subcorporaand the F22 corpus.Returning to the theoretical nalysis, the rel-ative frequency counts 6'i and the non-unit pro-duction probability estimates P1 for the PCFGinduced from this two-tree corpus are as follows:Johnson 42 The effect of ahernative tree representationslIIIIIiII|IIIIIIIIIIII!1IIIIIIIII!IIRVP --+ V NPVP ~ V NP PPNP --> Det NNP --* NP PPOf course, in a realC~(R) ~'I(R).f f1 - /  1 - /2 2/(2 + f)/ / / (2 + f)ree bank the counts ofall these productions would also include theiroccurences in other constructions, o the theo-retical analysis presented here is a crude ideal-ization.Thus the estimated likelihoods using P1 of thetree structures (A1) and (B1) are:4f 2~1(A1) - (2 + f)34 (i - f )PI(B1) -(2 + f)2"Clearly PI(A1) < f and PI(B1) < (1 - f)except at f = 0 and f = 1,so in general theestimated frequencies using P1 differ from thefrequencies ofA1 and B1 in the training corpus.This is not too surprising, as the PCFG P1 as-signs non-zero probability to trees not in thetraining corpus.
For example, P1 assigns non-zero probability to the tree in Figure 4.
Wediscuss the ramifications of this in section 6.In any case, in the parsing applications men-tioned earlier the absolute magnitude of theprobability of a tree is not of direct interest;rather we are concerned with its probability rel-ative to the probabilities of other, alternativetree structures.
Thus it is arguably more rea-sonable to ignore the "spurious" tree structuresgenerated by P1 but not present in the train-ing corpus, and compare the estimated relativefrequencies of (A1) and (B1) under P1 to theirfrequencies in the training data.Ideally the estimated relative frequency \]1 of(A1)/1 = PI(T =A l :~ 'e  {A1,B1})PI(A1)/22- fwill be close to its actual frequency f in thetraining corpus.
The relationship between f0.80.60.40.2I I I ?o ' " ' "  I//d ~ ?
!p /" /?
/?
/// //B?
. "
.m .
./ /0 "--"::';:'~"" I I I0 0.2 0.4 0.6 0.8 1fFigure 5: The estimated normalized frequency\] of NP attachment using the PCFG modelsdiscussed in the text as a function of the relativefrequency f of NP attachment in the trainingdata.and ./1 is plotted in Figure 5.
The value of \]1can diverge substantially from f. For example,at f = 0.48 (the estimate obtained from thePenn II corpus presented above) \]x = 0.15.4.2 'Chomsky adjunetion'representat ionsNow suppose that the corpus contains the fol-lowing two trees (A2) and (B2), which are the'Chomsky adjunction' representations of NPand VP attached PP's respectively, with rela-tive frequencies f and (1 - f) as before.
(A2) vP  (B2) VPV~P V ~ PN~/ '~P  V/~NP P/~NPThe counts C2 and the non-unit productionprobability estimates P2 for the PCFG inducedfrom this two-tree corpus are as follows:Johnson 43 The effect of alternative tree representationsR \] C2(a) #2(a)VP-+VNP \[ 1 1 / (2 - f )VP~VPPP  1 - f  (1 - f ) / (2 - f )NP ~ Det N 2 2/(2 + f)NP ~ NP PP f f /(2 + f )The estimated likelihoods using P2 of the treestructures (A2) and (B2) are:41 =(4 - f2)(2 + f)24 (1 - f)P2(B2) - (4 -  f2)2As in the previous ubsection P2(A2) < f andP2(B2) < (1 - f )  because the PCFG assignsnon-zero probability to trees not in the train-ing corpus.
Again, we calculate the estimatedrelative frequencies of (A2) and (B2) under P2.72 = ff'2(T = As: T e {A2,B2})f2 _ 2 f= 2f2_ f_2The relationship between f and f2 is plottedin Figure 5.
The value of f2 can diverge fromf, although not as widely as fl.
For example,at f = 0148 f2 = 0.36.
Thus the precise treestructure representations used to train a PCFGcan have a marked effect on its performance.4.3 Penn II representations withparent annotationOne of the weaknesses of a PCFG is that itis insensitive to non-local relationships betweennodes.
If these relationships are significant thena PCFG will be a poor language model.
Indeed,the sense in which the set of trees generated bya CFG is "context free" is precisely that the la-bel on a node completely characterizes the rela-tionships between the subtree dominated by thenode and the set of nodes that properly domi-nate this subtree.Thus one way of relaxing the independenceassumptions implicit in a PCFG model is tosystematically encode more information i nodelabels about their context.
This subsection ex-plores a particularly simple kind of contextualencoding: the label of the parent of each non-root nonpreterminal node is appended to thatnode's label.
The labels of the root node andthe terminal and preterminal nodes are left un-changed.For example, assuming that the Penn II for-mat trees (A1) and (B1) of subsection 4.1 areimmediately dominated by a node labelled S,this relabelling applied to those trees producesthe trees (A3) and (B3) below.
(A3) VpAsV~NAVpNp~'p '^Np(Ba) VP^S~ A V pDet N P NP PPWe can perform the same theoretical analy-sis on this two tree corpus that we applied tothe previous corpora to investigate the effect ofthis relabelling on the PCFG modelling of PPattachment s ructures.The counts C3 and the non-unit productionprobability estimates P3 for the PCFG inducedfrom this two-tree corpus are as follows:VpAs  -+ V NPAVP fVpAS- - rVNpAVpPPAVP 1 - f  1 fNpAVp --+ Det N 1 - f 1 - fNpAvp -+ NpANp ppANp f fThe estimated likelihoods using P3 of the treestructures (A3) and (B3) are:= 12#3(B3) = (1 - f)2As in the previous ubsection P3(A3) < f andP3(B3) < (1 - f).
Again, we calculate the es-timated relative frequencies of (A2) and (B2)under P2.\]3 = P3( "r = A3:'r e {A3, B3})Johnson 44 The effect of alternative tree representationsIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIf2f2 + (I- f)2,_The relationship between f and \]3 is plottedin Figure 5.
The value of f3 can diverge fromf, just like the other estimates.
For example,at f --- 0.48 \]3 = 0.46.
Thus as expected, in-creased context information in the form of anenriched node labelling scheme can markedlychange PCFG modelling performance.5 T ree  t rans format ionsThe last section presented simplified theoreticalanalyses of the effect of variation in tree rep-resentation and node labelling on PCFG mod-elling of PP attachment preferences.
This sec-tion reports the results of an empirical investi-gation into the effect of changes in tree repre-sentation.
These experiments were conductedby:1...systematically transforming the trees in thetraining corpus F2-21 by applying a treetransform X,inducing a PCFG Gx from the transformedF2-21 trees,finding the maximum likelihood parsesY(~-)x of the yield of each sentence in theF22 corpus with respect o the PCFG Gx,.
applying the inverse transform X -1 tothese maximum likelihood parse treesY(~)x to yield a sequence of 'detrans-formed' trees X-I(Y(~')x) using (approx-imately) the same representational systemas the tree bank itself, and.
evaluating the detransformed treesX - I (Y (?
)x )  with the standard labelledprecision and recall measures.Statistics were also collected on the propertiesof the grammar Gx and its detransformed max-imum likelihood parses X-I(Y(~)x);  the fullresults are presented in Table 1.The columns of that table correspond to dif-ferent sequences of trees as follows.F22: the trees from the F22 subcorpus of thePenn II tree bank,F22 Id: the maximum likelihood parses ofthe yields of the F22 subcorpus using thePCFG estimated from the F22 subcorpusitself,Id: the maximum likelihood parses of theyields of the F22 subcorpus using thePCFG estimated from the F2-21 subcor-pus (i.e., this corresponds to applying anidentity transform),Parent: as above, except that the parentannotation transform described in subsec-tion 4.3 was used in training and evalua-tion,VP: as in Id, except hat the flat VP structuresused in the Penn II tree bank were trans-formed into recursive Chomsky adjunctionstructures as described below,NP: as above, except that the one-level NPstructures used in the Penn II tree bankwere transformed into recursive Chomskyadjunction structures, andVP-NP: as above, except hat both NP and VPstructures were transformed into recursiveChomsky adjunction structures.The F22 tree sequence column provides infor-mation on the distribution of subtrees in the testtree sequence itself.
The F22 Id PCFG givesdata on the case where the PCFG is trained onthe same data that it is evaluated on, namelythe F22 subcorpus.
This column is included be-cause it is often assumed that the performanceof such a model is a reasonable upper boundon what can be expected from models inducedfrom training data distinct from the test data.The remaining columns describe PCFGs in-duced from versions of the F2-21 subcorpora ob-tained by applying tree transformations in themanner described above.The VP transform is the result of exhaus-tively applying the tree transforms below.
Thefirst transform transforms VP expansions withJohnson 45 The effect of ahernative tree representationsNo.
of rulesPrecisionRecallNP attachmentsVP attachmentsNP* attachmentsVP* attachmentsF2211279299339412F22 Id2,2690.7720.72804243668Id14,9620.7350.6966738467663Parent22,7730.8010.793217350234461VP14,3930.7220.67702403493NP  I VP -NP14,866 I 14,2970.738 f 0.7300.698 I 0.70551 3294271 061 I 401I 151 650 ,Table 1: The results of an empirical study of the effect of tree structure on PCFG models.
Eachcolumn corresponds to the sequence of trees, either consisting of the F22 subcorpus or transforms ofthe maximum likelihood parses of the yields of the F22 subcorpus with respect o different PCFGs,as explained in the text.
The first row reports the number of productions in these PCFGs, andthe next two rows give the labelled precision and recall of these sequences of trees.
The last fourrows report the number of times particular kinds of subtrees appear in these sequences of trees, asexplained in the text.final PPs into Chomsky adjunction structures,and the second transform adjoins final PPs witha following comma punctuation into Chomskyadjunction structures.
In both cases it is re-quired that the 'lowered' sequence of subtreesa be of length 2 or greater.
This ensures thatthe transforms will only apply a finite numberof times.
These two rules have the effect of con-verting VP final PPs into Chomsky adjunctionstructures.VPVPVP PPa PP a /~VPVPVP PPa PP AAThe NP transform is similiar to the VP trans-form.
It too is the result of exhaustively apply-ing two tree transformation rules.
These havethe effect of converting NP final PPs into Chore-sky adjunction structures.
In this case, we re-quire that a be of length 1 or greater.NPNPNP PPNP a PP ~ aa /'-~ A NP aANPNPNP PPNP eeA /--~ /~ ~ NP a/x /xaThe NP-VP transform is the result of apply-ing all four of the above tree transforms.The rows of Table 1 provide descriptions ofthese tree sequences (after 'untransformation',as described above) and, if appropriate, thePCFGs that generated them.The labelled precision and recall figures areobtained by regarding a sequence of trees ?
asa multiset or bag E(~) of edges, i.e., triples(N, l, r> where N is a nonterminal label and land r are left and right string positions in yieldof the entire corpus.
(Root nodes and preter-minal nodes are ignored in these edge sets, asthey are given as input to the parser).
Relativeto a 'test sequence' of trees ?'
(here the F22subcorpus) the labelled precision and recall ofa sequence of trees ~ with the same yield as ?
'Johnson 46 The effect of ahernative tree representationsII!iIIIIIIIIIIIIIIiIIIIIIIIIIIiIIIIIIIIIIIIIIIIIIIare calculated as follows:nPrecision( ) = I E(?
)InRecall( ) = IE( ')I(The 'fY operation above refers to multiset in-tersection).
Precision is the fraction of edgesin the tree sequence to be evaluated which alsoappear in the test sequence, and recall is thefraction of edges in the test sequence which alsoappear in sequence to be evaluated.The rows labelled NP attachments and VPattachments provide the number of times thefollowing tree schema, which represent a singlePP attachment, match the tree sequence.
2 Inthese schema, V can be instantiated by any ofthe verbal preterminal tags used in the Penn IIcorpus.VP VPV NP V NP PPA N'P PPA AThe rows labelled NP* attachments and VP*attachments provide the number of times thatthe following more relaxed schema match thetree sequence.
Here a can be instantiated byany sequence of trees, and V can be instantiatedby the same range of preterminal tags as above.VP VPV NP V NP PP aA N'P PP aA A/--~As expected, the PCFG based on the Parenttransformation, which copies the label of eachparent node onto those of its children, outper-forms all other PCFGs in terms of labelled pre-cision and recall.2The Penn II markup scheme permits a 'pseudo-attachment' notation for indicating ambiguous at-tachment.
However, this is only used relativelyinfrequently--the pseudo-attachment markup, only ap-pears 27 times in the entire Penn II tree bank--and wasignored here.
Pseudo-attachment structures count as VPattachment s ructures here.The various adjunction transformations onlyhad minimal effect on labelled precision and re-call.
Perhaps this is because PP attachmentambiguities, despite their important role in lin-guistic and parsing theory, are just one sourceof ambiguity among many in real language, andthe effect of the alternative representations hasonly minor effect.Indeed, in some cases moving to the purport-edly linguistically more realistic tree Chomskyadjunction representations actually decreasedperformance on these measures.
On reflec-tion, perhaps this should not be surprising.The Chomsky adjunction representations aremotivated within the theoretical framework ofTransformational Grammar, which explicitlyargues for nonlocal, indeed, non context free,dependencies.
Thus its poor performance whenused as input to a statistical model which is in-sensitive to such dependencies is to be expected.Indeed, it might be the case that the additionaladjunction odes inserted in the tree transfor-mations above have the effect of converting alocal dependency (which can be described by aPCFG) into a nonlocal dependency (which can-not).Another initially surprising property of thetree sequences produced by the PCFGs is thatthey do not reflect at all well the frequency ofthe different kinds of PP attachment found inthe Penn II corpus.
This is in fact to be ex-pected, since the sequences consist of maximumlikelihood parses.
To see this, consider any of theexamples analysed in section 4.
In all of thesecases, the corpora contained two tree structures,and the induced PCFG associates each with anestimated likelihood.
If these likelihoods differ,then a maximum likelihood parser will alwaysreturn the same maximum likelihood tree struc-ture each time it is presented with its yield, andwill never return the tree structure with lowerlikelihood, even though the PCFG assigns it anonzero likelihood.Thus the surprising fact is that these PCFGparsers ever produce a nonzero number of NPattachments and VP attachments in the sametree sequence.
This is possible because the nodelabel V in the attachment schema bove abbre-Johnson 47 The effect of alternative tree representationsviates several different preterminal labels (i.e.,the set of all verbal tags).
Further investiga-tion shows that once the V label in NP andVP attachment schemas is instantiated with aparticular verbal tag, only either the relevantNP attachment schema or the VP attachmentschema appears in the tree sequence.
For in-stance, in the Id tree sequence (i.e., producedby the standard tree bank grammar) the 67 NPattachments all occured with the V label instan-tiated to the verbal tag AUX.
36 Subsumed ru les  in t ree  bankgrammarsIt was mentioned in subsection 4.1 that it is pos-sible for the PCFG induced from a tree bank togenerate trees that are not meaningful represen-tations with respect to the original tree bankrepresentational scheme.
The PCFG inducedfrom the F2-21 subcorpus contains the follow-ing two productions:P(NP -+ NP PP) = 0.112P(NP -~ NP PP PP) = 0.006These productions generate the Penn II rep-resentations of one and two PP adjunctions toNP, as explained above.
However, the second ofthese productions will never be used in a max-imum likelihood parse, as the parse of the se-quence NP PP PP involving two applications ofthe first rule has a higher estimated likelihood.In fact, all of the productions of the formNP --+ NP ppn where n > 1 in the PCFG in-duced from the F2-21 subcorpus are subsumedby the NP --+ NP PP production in this way.Thus PP  adjunction to  NP in the maximumlikelihood parses using this PCFG always ap-pear as Chomsky adjunctions, even though theoriginal tree bank did not use this representa-tional scheme for adjunction!In fact, a large number of productions in thePCFG induced from the F2-21 subcorpus aresubsumed in this way.
Of the 14,962 produc-tions in the PCFG, 1,327, or just under 9%,are subsumed by combinations of two or more3This tag was introduced by (Charniak, 1996) to dis-tinguish auxiliary verbs from main verbs.productions.
Since these productions are neverused to construct a maximum likelihood parse,':they can be ignored if only maximum likelihoodparses are required.7 Conc lus ionThere may be several ways of representing aparticular linguistic construction as a tree.
Be-cause of the independence assumptions implicitin a PCFG, the kind of tree representationemployed can have a dramatic impact on thequality of the PCFG model induced.
Thispaper introduces a new methodology for ex-amining these effects utilitizing tree transfor-mations, and showed that one transformation,which copies the label of a parent node onto thelabels of its children, can dramatically improvethe performance of a PCFG model in terms oflabelled precision and recall.
It also pointedout that if only maximum likelihood parses areof-interest hen many productions can be ig-nored, since they are subsumed by combinationsof other productions in the grammar.Re ferencesBies, Ann, Mark Ferguson, Karen Katz, and RobertMacIntyre, 1995.
Bracketting Guideliness forTreebank H style Penn Treebank Project.
Linguis-tic Data Consortium.Charniak, Eugene.
1996.
Tree-bank grammars.
InProceedings of the Thirteenth National Confer-ence on Artificial Intelligence, pages 1031-1036,Menlo Park.
AAAI Press/MIT Press.Charniak, Eugene.
1997.
Statistical parsing witha context-free grammar and word statistics.
InProceedings of the Fourteenth National Confer-ence on Artificial Intelligence, Menlo Park.
AAAIPress/MIT Press.Collins, Michael.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In The Proceedingsof the 35th Annual Meeting of the Association forComputational Linguistics, San Francisco.
Mor-gan Kaufmann.Shieber, Stuart M. 1985.
Evidence against theContext-Freeness of natural anguage.
Linguis-tics and Philosophy, 8(3):333-344.Johnson 48 The effect of alternative tree representationsIIIIIIIIIIIIIIIIIIIIII
