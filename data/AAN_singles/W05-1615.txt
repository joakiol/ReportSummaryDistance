AbstractPost-editing is commonly performed on computer-generated texts, whether from Machine Translation(MT) or NLG systems, to make the texts accept-able to end users.
MT systems are often evaluatedusing post-edit data.
In this paper we describe ourexperience of using post-edit data to evaluateSUMTIME-MOUSAM, an NLG system that pro-duces marine weather forecasts.1 IntroductionNatural Language Generation (NLG) systems must ofcourse be evaluated, like all NLP systems.
Previous work onNLG evaluation has focused on either experiments con-ducted with users who read the generated texts, or on com-parisons of generated texts to corpora of human-writtentexts.
In this paper we describe an evaluation technique,which looks at how much humans need to post-edit gener-ated texts before they are released to users.
Post-editevaluations are common in machine translation, but we be-lieve that ours is the first large-scale post-edit evaluation ofan NLG system.The system being evaluated is SUMTIME-MOUSAM [Sri-pada et al 2003], an NLG system, which generates marineweather forecasts from Numerical Weather Prediction(NWP) data.
SUMTIME-MOUSAM is operational and is usedby Weathernews (UK) Ltd to generate 150 draft forecastsper day, which are post-edited by Weathernews forecastersbefore being released to clients.2 Background2.1 Evaluating NLG SystemsCommon evaluation techniques for NLG systems [Mellishand Dale, 1998] include:?
Showing generated texts to users, and measuring howeffective they are at achieving their goal, compared tosome control text (for example, [Young, 1999])?
Asking experts to rate computer-generated texts invarious ways, and comparing this to their rating ofmanually authored texts (for example, [Lester andPorter, 1997])?
Automatically comparing generated texts to a corpus ofhuman authored texts (for example, [Bangalore et al2000]).Each of these techniques is effective under different ap-plication contexts in which NLG systems operate.
For in-stance, a corpus based technique is effective when a highquality corpus is available.
The appeal of post-edit evalua-tion as done with SUMTIME-MOUSAM is that (A) the editsshould indicate actual mistakes instead of just differences inhow things can be said and (B) the amount of post-editingrequired is a very important practical measure of how usefulthe system is to real users (forecasters in our case).Post-edit evaluations are a standard technique in MachineTranslation [Hutchins and Somers, 1992].
The only previ-ous use of post-edit evaluation in NLG that we are aware ofis Mitkov and An Ha [2003], but their evaluation is rela-tively small, and they give little information about it.2.2 SUMTIME-MOUSAMSUMTIME-MOUSAM [Sripada et al 2003] is an NLG systemthat generates textual weather forecasts from numericalweather prediction (NWP) data.
The forecasts are marineforecasts for offshore oilrigs.
Table 1 shows a small extractfrom the NWP data for 12-06-2002, and Table 2 shows partof the textual forecast that SUMTIME-MOUSAM generatesfrom the NWP data.
The Wind statements in Table 2 aremostly based on the NWP data in Table 1.Time WindDirWind Spd10mWind Spd50mGust10mGust50m06:00 W 10.0 12.0 12.0 16.009:00 W 11.0 14.0 14.0 17.012:00 WSW 10.0 12.0 12.0 16.015:00 SW 7.0 9.0 9.0 11.018:00 SSW 8.0 10.0 10.0 12.021:00 S 9.0 11.0 11.0 14.000:00 S 12.0 15.0 15.0 19.0Table 1.
Weather Data produced by an NWP model for 12-Jun 2002Evaluation of an NLG System using Post-Edit Data: Lessons LearntSomayajulu G. Sripada and Ehud Reiter and Lezan HawizyDepartment of Computing ScienceUniversity of AberdeenAberdeen, AB24 3UE, UK{ssripada,ereiter,lhawizy}@csd.abdn.ac.ukSUMTIME-MOUSAM generates texts in three stages[Reiter and Dale, 2000].Document Planning: Text structure is specified byWeathernews, via a control file.
The key content-determination task is selecting ?important?
or ?significant?data points from the underlying weather data to be includedin the forecast text.
SUMTIME-MOUSAM uses a bottom-upsegmentation algorithm for this task [Sripada et al 2002].Micro-planning: The key decisions here are lexical selec-tion, aggregation, and ellipsis.
SUMTIME-MOUSAM usesrules for this that are derived from corpus analysis and otherknowledge acquisition activities [Reiter et al 2003; Sripadaet al 2003].Realization: SUMTIME-MOUSAM uses a simple realiserthat is tuned to the Weathernews weather sublanguage.SUMTIME-MOUSAM is partially controlled by a controldata file that Weathernews can edit.
For example, this filespecifies error function data that controls the segmentationprocess for content determination.
The error function datadecides the level of abstraction achieved by the segmenta-tion process ?
the larger the error function value the higherthe level of abstraction achieved by segmentation.2.3 SUMTIME-MOUSAM at WeathernewsWeathernews (UK) Ltd, a private sector weather servicescompany, uses SUMTIME-MOUSAM to generate draft fore-casts.
The process is illustrated in Figure 1.
Forecastersload the NWP data for the forecast into Marfors, which isWeathernews?
forecasting tool.
Using Marfors, forecastersedit the NWP data, using their meteorological expertise andadditional information such as satellite weather maps.
Theythen invoke SUMTIME-MOUSAM to generate an initial draftof the forecast.
This initial draft helps the forecaster under-stand the NWP data, and often suggests further edits to theNWP data.
The generate-and-edit-data process may be re-peated.
When the forecaster is satisfied with the NWP data,he invokes SUMTIME-MOUSAM again to generate a finaldraft textual forecast, marked ?Pre-edited Text?
in Figure 1.The forecaster then uses Marfors to post-edit the textualforecast.
When the forecaster is done, Marfors assemblesthe complete forecast from the individual fields, and sends itto the customer.Section 2.
FORECAST 6 - 24 GMT, Wed 12-Jun 2002Field TextWIND(KTS) 10M W 8-13 backing SW by mid after-noon and S 10-15 by midnight.WIND(KTS) 50M W 10-15 backing SW by mid after-noon and S 13-18 by midnight.WAVES(M)SIG HT0.5-1.0 mainly SW swell.WAVES(M)MAX HT1.0-1.5 mainly SW swell falling 1.0or less mainly SSW swell by after-noon, then rising 1.0-1.5 by mid-night.WAVE PERIOD(SEC)Wind wave 2-4 mainly 6 secondSW swell.WINDWAVEPERIOD (SEC)2-4.SWELL PERIOD(SEC)5-7.WEATHER Mainly cloudy with light rainshowers becoming overcast aroundmidnight.VISIBILITY(NM)Greater than 10.AIR TEMP(C) 8-10 rising 9-11 around midnight.CLOUD(OKTAS/FT)4-6 ST/SC 400-600 lifting 6-8ST/SC 700-900 around midnight.Table 2.
Extract from SUMTIME-MOUSAM Forecast Pro-duced for 12-Jun 2002 (AM).Figure 1.
Schematic Showing SUMTIME-MOUSAM Used at WeathernewsPost-editedTextMarforsData EditorMarfors Text EditorSUMTIME-MOUSAMMarforsData EditorPre-editedText SUMTIME-MOUSAMText 1 Data 1Edited DataNWP DataNote that SUMTIME-MOUSAM is used for two purposesby Weathernews; to help forecasters understand and there-fore edit the NWP data, and to help generate texts for cus-tomers.
In this paper we focus on evaluating the secondusage of the system (generating texts for customers).When a forecast is complete, Marfors saves the final ed-ited NWP data, marked ?Edited data?
in Figure 1 and thefinal edited forecast marked ?Post-edited Text?
into a data-base.
This data is forwarded to us for 150 sites per day; thisis the basis of our post-edit evaluation.
Marfors does notdirectly save the SUMTIME-MOUSAM text that forecastersedit (?Pre-edited Text?
in Figure 1), but we can reconstructthis text by running the system on the final edited NWPdata.3 Post-Edit Evaluation3.1 DataThe evaluation was carried out on 2728 forecasts, collectedduring period June to August 2003.
Each forecast wasroughly of 400 words, so there are about 1 million words inall in the corpus.For each forecast, we have the following data?
Data: The final edited NWP data?
Pre-edit text: The final draft forecast produced bySUMTIME-MOUSAM, which we reconstruct as de-scribed in Section 2.3.?
Post-edit text: The manually post-edited forecast,which was sent to the client.?
Background information: includes date, location, andforecasterWe do not currently use the NWP data (other than forreconstructing SUMTIME-MOUSAM texts), although wehope in the future to include it in our analyses, in a mannerroughly analogous to Reiter and Sripada [2003].
This dataset continues to grow, we receive approximately 150 newforecasts per day.3.2 Analysis ProcedureThe following procedure is performed automatically by asoftware tool.
First, we perform some data transformationand cleaning.
This includes breaking sentences up intophrases, where each phrase describes the weather at onepoint in time.For example, the pre-edit text in Figure 2 would be bro-ken up into three phrases:A1 SW 20-25A2 backing SSW 28-33 by middayA3 then gradually increasing 34-39 by midnightFigure 2.
Example pre-edit and post-edit texts from the post-edit corpusThe Figure 2 post-edit text is divided into two phrases:B1 SW 22-27B2 gradually increasing SSW 34-39The second step is to align phrases from these two tablesas a preparation for comparison in the next step.
Alignmentis a complex activity and is described in detail next.
To startwith we generate an exhaustive list of all the possible com-binations of phase alignments.For example, consider the texts in Figure 2.
Here we gen-erate the following list of possible alignments:{(A1, B1), (A1, B2), (A2, B1), (A2, B2), (A3, B1), (A3,B2)}Next, we compute match scores for each of these possi-ble alignments and use them for selecting the right align-ments.
For each unedited phrase Ai, the alignment with thehighest matching score is selected.
For the purpose of com-puting the match scores, phrases are parsed using ?parts ofspeech?
designed for weather sublanguage such as direction,speed and time.
The total match score of a pair of phrases iscomputed as the sum of the match scores for their constitu-ents.
Match score (MS) for a pair of constituents dependsupon their part of speech and also their degree of match.
MSis defined as a product of two terms as explained below:?
Match score due to degree of match: we assign a matchscore of 2 for exact matches, 1 for partial matches and0 for mismatches.?
Weight factor denoting importance of constituents foralignment: Constituents belonging to certain parts ofspeech (POS) are more significant for alignment thanothers.
For example, times are more significant foralignment than verbs.
Also weights are varied for thesame POS based on its context in the phrase.
For ex-ample, direction receives higher weight if it occurs ina phrase without a time or speed.
This is because insuch phrases direction is the only means for align-ment.Continuing with our example sentences in Figure 2, weshow below how we find an alignment for A3.
As describedearlier, A3 can be aligned to either B1 or B2.
The MS for(A3, B1) is zero as shown in Table 3.A.
Pre-edit Text: SW 20-25 backing SSW 28-33 bymidday, then gradually increasing 34-39 by midnight.B.
Post-edit Text: SW 22-27 gradually increasingSSW 34-39.POS A3 B1 MSconjunction Then <none> 0Adverb Gradually <none> 0Verb Increasing <none> 0Direction <none> SW 0Speed range 34-39 22-27 0Time By midnight <none> 0Table 3 Match Score for A3 and B1The MS for (A3, B2) is 2*(2*w1+w2) where w1 is theweight for Adverb/verb and w2 (>w1) for speed as shown inTable 4.
Based on the match scores computed above A3 isaligned with B2.
Similarly A1 is aligned with B1.
A2 isunaligned, and treated as a deleted phrase.POS A3 B2 MSconjunction Then <none> 0Adverb Gradually Gradually w1*2Verb Increasing Increasing w1*2Direction <none> SSW 0Speed range 34-39 34-39 w2*2Time By midnight <none> 0Table 4.
Match Score for A3 and B2The third step is to compare aligned phrases, such as A1and B1.
One evaluation metric is based on comparingaligned phrases as a whole.
Here we simply record ?match?or ?mismatch?.
For example, both (A1, B1) and (A3, B2)are mismatches.
We then compare constituents in thephrases to determine more details about the mismatches.For this detailed comparison we use the domain-specificpart-of-speech tags described earlier.
Each part-of-speechshould occur at most once in a phrase (in our weather sub-language), so we simply align on the basis of the tag.
Afterconstituents are aligned, we label each pre-edit/post-editpair as match, replace, add, or delete.
For example, A and Bare analysed as in Table 5.POS A B labelDirection SW SW matchSpeed 20-25 22-27 replaceConjunction then <none> deleteAdverb gradually gradually matchVerb increasing increasing matchDirection <none> SSW addSpeed 34-39 34-39 matchTime by midnight <none> deleteTable 5.
Detailed Edit Analysis3.3 Analysis of ResultsWe processed 2728 forecast pairs (pre-edited and post-edited).
These were divided into 73041 phrases.
Out ofthese, the alignment procedure failed to align 7608 (10%)phrases.
For instance, in the example of Section 3.2, phraseA2 was not aligned with any B phrase.
Alignment failuregenerally indicates that the forecaster is unhappy withSUMTIME-MOUSAM?s segmentation that is with the sys-tem?s content determination.
We have manually analysedsome of these cases, and in general it seems the forecastersare performing more sophisticated data analysis thanSUMTIME-MOUSAM, and are also more sensitive to whichchanges are significant enough to be reported to the user.We have manually inspected alignment quality of 100random phrase pairs to determine cases where our alignmentprocedure erroneously aligned phrases.
We found one caseof improper alignment.
The pre-edited phrase ?soon becom-ing variable?
has not been aligned to its corresponding iden-tical post-edited phrase.
Inspection of the rest of the corpusshowed that this error repeated 54 times in the whole cor-pus.
These cases have been classified as alignment failuresand therefore do not affect the post-edit analysis.Time (Hours) Direction Speed00 ESE 1203 ESE 1206 ESE 1109 ESE 1112 ESE 1015 ESE 818 ESE 921 ESE 1124 ESE 13Table 6.
Wind 10m data for 14 Jul 2003For example, consider the Wind 10m data shown in Table6.
Our content determination algorithm first segments thedata in table 6 (see Sripada et al[2002] for more details).Segmentation is the process of fitting straight lines to a dataset in such a way that a minimum error is introduced by thelines.
Since the direction data is constant at ESE, there isonly one segment for this data.Figure 3.
Segmentation of Wind speed data shown in Ta-ble 6.Wind speed data however is segmented by two lines asshown in Figure 3, one line joining the point (0,12) with(15,8) and the second joining the point (15,8) with (24,13).Our content selection algorithm therefore selects data pointsSegmentation of Wind 10m data024681012140 3 6 9 12 15 18 21 24TimeWindSpeed(0,12), (15,8) and (24,13) to be included in the forecast.
Inthis case our system produced:?ESE 10-15 gradually easing 10 or less by mid afternoonthen increasing 11-16 by midnight?However, forecasters view this data as a special case anddon?t segment it the way we do.
Here the wind speed is al-ways in the range of ?10-15?
except at 1500 and 1800 hours.Therefore they mention the change as an additional informa-tion to an otherwise constant wind speed.
In this case, theforecaster edited text is:?ESE 10-15 decreasing 10 or less for a time later?.Talking about the segmentation differences, one of theforecasters at Weathernews told us that another factor af-fecting segmentation is related to the end user.
End users ofthe marine forecasts are oil company staff who scheduleactivities on the oilrigs in the North Sea.
Over the yearsforecasters at Weathernews have acquired a good under-standing of the informational needs of the oil company staff.So they use the forecast statements as messages to the endusers about the weather and know what kind of messageswill be useful to the end users.
In the example texts shownin Figure 2 the forecaster could have thought that the impor-tant message to communicate about wind is that it is in-creasing monotonically and is likely to be in the range be-tween 22 (the actual initial wind speed) and 39.
Everythingelse distracts this primary message and therefore needs to beavoided.
Once again there is post segmentation reasoningused by the forecasters.
We are investigating better patternmatching techniques and better user models to improve ourcontent selection.S.
No.
Mismatch Type Freq.
%1.
Ellipses (word additionsand deletions)35874 652.
Data Related Replacements(range and direction re-placements)10781 203.
Lexical Replacements 8264 15Total 54919Table 7.
Results of the Evaluation showing summary cate-gories and their frequenciesGoing back to the successfully aligned phrases, 43914(60%) are perfect matches, and the remaining 21519 (30%)are mismatches.
Table 7 summarises the mismatches.Here, each mismatch is classified as?
Ellipses: additions and deletions.
For example, delet-ing the time phrase by midnight in the (A3, B2) pair.These generally indicate problems with SUMTIME-MOUSAM?s aggregation and ellipsis.?
Data replacements: changes (replaces) to constituentsthat directly convey NWP data, such as wind speedand direction.
For example, changing 20-25 to 22-27in the (A1, B1) pair.
These can indicate content prob-lems.
They also occur when forecasters believe theNWP data is incorrect but decide to just correct theforecast text and not the data (eg, skip generate-and-edit step described in section 2.3).?
Lexical replacements: All other changes (replaces).
Forexample, if the conjunction ?then?
was replaced by?and?.
This generally indicates a problem inSUMTIME-MOUSAM?s lexicalisation strategy.For each pair of phrases compared in the evaluation, wehave counted the number of times each edit operation suchas add, delete and replace is performed by forecasters.
Forexample consider the two phrase pairs shown in Table 5.For the first phrase pair of ?SW 20-25?
and ?SW 22-27?
fore-casters performed zero add, zero delete and one replaceoperation (?20-25?
is replaced by ?22-27?).
For the secondphrase pair of ?then gradually increasing 34-39 by mid-night?
and ?gradually increasing SSW 34-39?
forecastersperformed one add (added ?SSW?
), two delete (deleted?then?
and ?by midnight?)
and zero replace operations.
Wehypothesized that forecasters were making significantlymore add and delete operations than replace operations.
Forverifying this, we have performed a pairwise t-test.
Vari-able1 for the t-test represents the sum of the counts of addand delete operations for each pair of phrases.
Variable2represents the count of replace operations.
For example, forthe two phrase pairs shown in Table 5, variable1 has valuesof zero and three where as variable2 has values of one andzero.
This test showed (with a p value less than 10-20) thatforecasters were performing more additions and deletionsthan replacements.
In other words, ellipsis is the main prob-lem in our system.
Most (25235 out of 35874, 70%) of theseerrors are deletions, where the forecaster deletes words fromSUMTIME-MOUSAM?s texts.A manual analysis of some ellipsis cases has highlightedsome general phenomena.
First of all, many ellipsis casesare ?downstream?
consequences of earlier changes.
For ex-ample, if we look at the (A3, B2) pair above, this containsthree ellipsis changes: then was deleted, SSW was added,and by midnight was deleted.
The first two of these changesare a direct consequence of the deletion of phrase A2.
IfSUMTIME-MOUSAM?s content determination system waschanged so that it did not generate A2, then the micro plan-ner would have expressed A3 as gradually increasing SSW34-39 by midnight, which is identical to B2 except for bymidnight.The deletion of by midnight is an example of anothercommon phenomenon, which is disagreement among indi-viduals as to how text should be written.
As described in[Reiter et al 2003], some forecasters elide the last timephrase in simple sentences such as this one, and some donot.
An earlier version of SUMTIME-MOUSAM in factwould have elided this time phrase, but we changed the be-havior of the system in this regard after consultation.
Ellip-sis errors are inevitable in cases where the different fore-casters disagree about when to elide.
However, since post-editors can delete words more quickly than they can addwords, it probably makes sense from a practical perspectiveto be conservative about elision, and only elide in unambi-guous cases.
We will not further discuss data replacementerrors, since they reflect either content problems or caseswhere NWP data was not corrected at the input time butedited directly in the final text.We have discussed lexical replacement errors in detailelsewhere [Reiter and Sripada, 2002].
In general terms,some errors reflect problems with SUMTIME-MOUSAM; forexample, the system overuses then as a connective, so fore-casters often replaced then by alternative connectives suchas and.
However, many lexical replacement errors simplyreflected the lexical preferences of individual forecasters[Reiter and Sripada, 2002].
For example, SUMTIME-MOUSAM always uses the verb easing to indicate a reduc-tion in wind speed.
Most forecasters were happy with this,but 3 individuals usually changed this to decreasing.A general observation is that some forecasters post-editedtexts much more than others.
For example, while overall28% of phrases were edited, edit rates by individual fore-casters varied from 4% to 93%.
We do not know why editrates vary so much, although it may be significant the indi-vidual with the highest (93%) edit rate is one of the mostexperienced forecasters, who takes well-justified pride inproducing well-crafted forecasts.Summarizing the results of our evaluation:1.
SUMTIME-MOUSAM?s content determination can defi-nitely be improved, by using more sophisticated segmenta-tion techniques.2.
SUMTIME-MOUSAM?s micro-planner can certainly beimproved in places, for example by varying connectives.However, many post-edits are due to individual differences,which we cannot do anything about.We are currently carrying out another evaluation of SUM-TIME-MOUSAM by the end users, oilrig staff and other ma-rine staff who regularly read weather forecasts.
In this studywe compare user?s comprehension of weather informationfrom human written and computer generated forecast texts.We also measure user ratings (preference) of human writtenand computer generated texts.
Preliminary results from ourstudy indicate that users make fewer mistakes on compre-hension questions when they are shown texts that use com-puter generated words with human selected content.
Gener-ally users seem to prefer computer generated texts to humanwritten texts given the same underlying weather data.4 Lessons from our Post-Edit EvaluationAs stated in Section 2.1, we were attracted to post-editevaluation because we believed that (A) people would onlyedit things that were clearly wrong; and (B) post-editing wasan important usefulness metric from the perspective of ourusers (forecasters).Looking back, (B) was certainly true.
The amount ofpost-editing that generated texts require is a crucial compo-nent of the cost of using SUMTIME-MOUSAM, and hence ofthe attractiveness of the system to users (forecasters).
Al-though we have not measured the time required for perform-ing post-edits, we have used edit-distance measures used inMT evaluations as an approximate cost metric.
We havecomputed our cost metric by setting different cost (weight)values to different edit operations.
Cost of add and replaceoperations is set to 5 and cost of delete is set to 1 as used inSu et al[1992].
The ratio of the cost of edits and the cost ofwriting the entire forecast manually (adding all the words) iscomputed to be 0.15.
(A) however was perhaps less truethan we had hoped.
Wagner [1998] also described post-edited texts in MT as at times noisy.
Our analysis of manu-ally written forecasts [Reiter and Sripada, 2002] had high-lighted a number of ?noise?
elements that made it more dif-ficult to extract information from such corpora.
Basicallythere are many ways of communicating information in text,and the fact that a generated text doesn?t match a corpus textdoes not mean that the generated text is wrong.
We as-sumed that people would only post-edit mistakes, where thegenerated text was wrong or sub-optimal, and hence post-edit data would be better for evaluation purposes than cor-pus comparisons.In fact, however, there were many justifications for post-edits:1.
Fixing problems in the generated texts (such asoveruse of then);2.
Refining/optimizing the texts (such as using fora time);3.
Individual preferences (such as easing vs de-creasing); and4.
Downstream consequences of earlier changes(such as introducing SSW in B2, in the exampleof Section 3.2).We wanted to use our post-edit data to improve the sys-tem, not just to quantify its performance, and we discoveredthat we could not do this without attempting to analyze whypost-edits were made.
Probably the best way of doing thiswas to discuss post-edits with the forecasters.
Alternatively,we could have asked forecasters to fill in problem sheets tocapture their explanation of post-edits.
Such feedback fromthe forecasters would have allowed us to reason with post-edit data to improve our system.
In [Reiter et al 2003] weexplained that we found that analysis of human-written cor-pora was more useful if it was combined with directly work-ing with domain experts; and essentially this (perhaps notsurprisingly) is our conclusion about post-edit data as well.One of the lessons we learnt from this exercise has beenthat post-edit evaluations are useful to compute a cost metricto quantify the usefulness of a system.
For example, as de-scribed earlier, we have computed a cost metric, 0.15 signi-fying the post-editing effort.
Post-edit evaluations are alsouseful in revealing general problem areas in a system.
Forexample, as described in section 3.3, our evaluation showedthat ellipsis related problems are more serious in our systemthan others.
However, post-edit evaluations are not affectivein discovering specific problems in a system.
The main rea-son for this is that many post-edits, as stated earlier, do notactually fix problems in the generated text at all.
The realpost-edits that fixed problems in the generated text wereburied among the other noisy post-edits.This lesson of course is the result of our method of post-edit evaluation.
Post-editing was not supported bySUMTIME-MOUSAM and forecasters used Marfors (see sec-tion 2.3) to perform post-editing.
Therefore, we had to ac-cept the post-edit data with all the noise.
In MT, post-editorsoften work under predefined guidelines on post-editing andalso use post-editing tools.
For example, post-editing toolsautomatically revise texts to fix ?down-stream?
conse-quences of human edits.
If post-edit tools are similarly inte-grated into NLG systems, there is going to be a significantreduction in the number of noisy post-edits allowing us tofocus on real post-edits.Because post-editing is subjective varying from individ-ual to individual, we need to understand the post-editingbehaviour of individuals to analyze the noisy post-edit data.Although we have data on forecaster variations in our post-edit corpus, these variations have not been observed fromdifferent forecasters post-editing the same text.
This wecould have achieved by performing a pilot before the actualevaluation.
For the pilot all the forecasters post-edit thesame set of forecasts, thus revealing their individual prefer-ences.
Post-edit data from the pilot would have enabled usto factor out the effects of forecaster variation from the realevaluation data.
As described above noise in the post-editdata can be reduced by using post-edit tools and by perform-ing a pilot before the real evaluation.
This means that post-edit evaluations need preparation in the form of developingpost-edit tools and carrying out pilot studies.
This is anotherlesson we learnt from our current evaluation.Although analyzing the post-edit data was a major en-deavour for us, the overall cost of post-edit evaluation wasnot much compared to the effort that would have been re-quired to conduct end user experiments on 2728 texts.
Ofcourse, this was only true because SUMTIME-MOUSAMtexts were being post-edited in any case by Weathernews.The cost-effectiveness of post-edit evaluation is less clear ifthe evaluators must organize and pay for the post-editing, asMitkov and An Ha [2003] did.
In this context we shouldspeculate that when more and more NLG systems are de-ployed in the real world, post-editing will be accepted as acomponent in the process of automatic text generation muchin the same way post-editing is now a part of MT.5 ConclusionEvaluation is a key aspect of NLG; we need to know howwell theories and systems work.
We have used analysis ofpost-edits, a popular evaluation technique in machine trans-lation, to evaluate SUMTIME-MOUSAM, an NLG systemthat generates marine weather forecasts.
We encounteredsome problems, such as the need to identify why post-editswere made which make post-edit data hard to discover spe-cific clues for system improvement.
However, post-editevaluation can reveal problem areas in the system and alsoquantify system utility for real users.References[Bangalore et al, 2000] Srinivas Bangalore, Owen Ram-bow, and Steve Whittaker.
2000.
Evaluation metrics forgeneration.
In Proc.
of the First International NaturalLanguage Generation Conference (INLG2000), Israel.
[Hutchins and Somers, 1992] John Hutchins and Harold L.Somers, 1992.
An Introduction to Machine Translation,Academic Press.
[Lester and Porter, 1997] James Lester and Bruce Porter.1997.
Developing and empirically evaluating robustexplanation generators: The KNIGHT experiments.Computational Linguistics, 23-1:65-103.
[Mellish and Dale, 1998] Chris Mellish and Robert Dale,1998.
Evaluation in the context of natural language gen-eration, Computer Speech and Language 12:349-373.
[Mitkov and An Ha, 2003] Ruslan Mitkov and Le An Ha,2003.
Computer-Aided Generation of Multiple-ChoiceTests, In Proc.
of the HLT-NAACL03 Workshop onBuilding Educational Applications Using NLP, Edmon-ton, Canada, pp.
17-22.
[Reiter and Dale, 2000] Ehud Reiter and Robert Dale, 2000.Building Natural Language Generation Systems.
Cam-bridge University Press.
[Reiter and Sripada, 2002] Ehud Reiter and Somayajulu G.Sripada, 2002.
Human Variation and Lexical Choice.Computational Linguistics 28:545-553.
[Reiter et al, 2003] Ehud Reiter, Somayajulu G. Sripada,and Roma Robertson, 2003.
Acquiring Correct Knowl-edge for Natural Language Generation.
Journal of Artifi-cial Intelligence Research, 18: 491-516, 2003.
[Reiter and Sripada, 2003] Ehud Reiter and Somayajulu G.Sripada, 2003.
Learning the Meaning and Usage of TimePhrases from a Parallel Text-Data Corpus.
In Proc.
of theHLT-NAACL03 Workshop on Learning Word Meaningfrom Non-Linguistic Data, pp 78-85.
[Sripada et al, 2002] Somayajulu, G. Sripada, Ehud Reiter,Jim Hunter and Jin Yu.
2002 Segmenting Time Seriesfor Weather Forecasting.
In: Macintosh, A., Ellis, R. andCoenen, F. (ed) Proc.
of ES2002, pp.
193-206.
[Sripada et al, 2003] Somayajulu G. Sripada, Ehud Reiter,and Ian Davy, 2003.
SUMTIME-MOUSAM: Configur-able Marine Weather Forecast Generator.
Expert Update,6(3):4-10.
[Su et al, 1992] Keh-Yih Su, Ming-Wen Wu and Jing-ShinChang, 1992, A new quantitative quality measure formachine translation systems.
In Proceedings ofCOLING-92, Nantes, pp 433-439.
[Wagner, 1998] Simone Wagner, 1998.
Small Scale Evalua-tion Methods In: Rita N?bel; Uta Seewald-Heeg (eds.
):Evaluation of the Linguistic Performance of MachineTranslation Systems.
Proc.
of the Workshop at theKONVENS-98.
Bonn, pp 93-105.
[Young, 1999] Michael Young, 1999.
Using Grice?s maximof quantity to select the content of plan description, Arti-ficial Intelligence 115:215-256.
