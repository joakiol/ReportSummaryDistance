Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 84?93,Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational LinguisticsOn Named Entity Recognition in Targeted Twitter Streams in PolishJakub PiskorskiLinguistic Engineering GroupPolish Academy of SciencesJakub.Piskorski@ipipan.waw.plMaud EhrmannDepartment of Computer ScienceSapienza University of Romeehrmann@di.uniroma1.itAbstractThis paper reports on some experimentsaiming at tuning a rule-based NER sys-tem designed for detecting names in Pol-ish online news to the processing of tar-geted Twitter streams.
In particular, oneexplores whether the performance of thebaseline NER system can be improvedthrough the incremental application ofknowledge-poor methods for name match-ing and guessing.
We study various set-tings and combinations of the methods andpresent evaluation results on five corporagathered from Twitter, centred around ma-jor events and known individuals.1 IntroductionRecently, Twitter emerged as an important so-cial medium providing most up-to-date informa-tion and comments on current events of any kind.This results in an ever-growing interest of vari-ous organizations in tools for real-time monitor-ing of Twitter streams to collect their business-specific information therefrom for analysis pur-poses.
Since monitoring the entire Twitter streamappears to be unfeasible due to the high volumeof published tweets, one usually monitors targetedTwitter streams, i.e., streams of tweets potentiallysatisfying specific information needs.Applications for monitoring Twitter streamsusually require named entity recognition (NER)capacity.
However, due to the nature of Twittermessages, i.e., being short, noisy, written in an in-formal style, lacking punctuation and capitaliza-tion, containing misspellings, non-standard abbre-viations, and non grammatically correct sentences,the application of even basic NLP tools (trained onformal texts) on tweets usually results in poor per-formances.
In the case of well-formed texts suchas online news, exploitation of contextual clues iscrucial to named entity identification and classifi-cation (e.g., ?Mayor of ?
in the left context of a cap-italized token is a reliable pattern to classify it ascity name).
Such external evidence is often miss-ing in tweets, and entity names are frequently in-complete, abbreviated or glued with other words.Furthermore, deployment of supervised ML-basedtechniques for NER from tweets is challengingdue to the dynamic nature of Twitter.In this paper, we report on experiments aimingat tuning a rule-based NER system, initially de-signed for detecting names in Polish online news,to the processing of targeted Twitter streams.
Inparticular, we explore whether the performance ofthe baseline NER system can be improved throughthe utilization of knowledge-poor methods (basedon string distance metrics) for name matchingand name guessing.
In comparison to English,Polish is a free-word order and highly inflectivelanguage, with particularly complex declensionparadigm of proper names, which makes NER forPolish a more difficult task.The remaining part of the paper is structuredas follows.
First, Section 2 provides informationon related work.
Next, Section 3 describes thebaseline NER system and the knowledge-poor en-hancements.
Subsequently, Section 4 presents theevaluation results.
Finally, Section 5 gives a sum-mary and an outlook as regards future research.2 Related WorkThe problem of NER has gained lot of attention inthe last two decades and a vast bulk of researchon development of NER from formal texts ex-ists (Nadeau and Sekine, 2007).
Although most ofthe reported work focused on NER for major lan-guages, efforts on NER for Polish have also beenreported.
(Piskorski, 2005) describes a rule-basedNER system for Polish that covers the classicalnamed-entity types, i.e., persons, locations, orga-nizations, as well as numeral and temporal expres-84sions.
(Marcin?czuk and Piasecki, 2007) and (Mar-cin?czuk and Piasecki, 2010) report on a memory-based learning and Hidden Markov Model ap-proach resp.
to automatic extraction of informa-tion on events in the reports of Polish Stockhold-ers, which involves NER.
Also in (Lubaszewski,2007) and (Lubaszewski, 2009) some general-purpose information extraction tools for Polishare addressed.
Efforts related to creation of adictionary of Warsaw urban proper names ori-ented towards NER is reported in (Savary et al2009; Marciniak et al 2009).
(Gralin?ski et al2009) present NERT, another rule-based NER sys-tem for Polish which covers similar types of NEsas (Piskorski, 2005).
Finally, some efforts onCRF-based NER methods for Polish are reportedin (Waszczuk et al 2010) and (Marcin?czuk andJanicki, 2012).While NER from formal texts has been wellstudied, relatively little work on NER for Twit-ter was reported.
(Locke and Martin, 2009) pre-sented a SVM-based classifier for classifying per-sons, locations and organizations in Twitter.
(Rit-ter et al 2011) described an approach to segmen-tation and classification of a wider range of namesin tweets based on CRFs (using POS and shallowparsing features) and Labeled LDA resp.
(Liu etal., 2011) proposed NER (segmentation and clas-sification) approach for tweets, which combinesKNN and CRFs paradigms.
The reported preci-sion/recall figures are significantly lower than thestate-of-the-art results for NER from well-formedtexts and oscillate around 50-80%.
Better resultswere reported in case of extracting names fromtargeted tweets (person names from tweets onlive sport events) (Choudhury and Breslin, 2011).
(Nebhi, 2012) presented a rule-based NER systemfor detecting persons, organizations and locationswhich exploits an external global knowledge baseon entities to disambiguate NE type.
(Liu et al2012) proposed a factor graph-based approach tojointly conducting NER and NEN (Named EntityNormalization), which improves F-measure per-formance of NER and accuracy of NEN whenrun sequentially.
An Expectation-Maximizationapproach to NE disambiguation problem was re-ported by (Davis et al 2012).
Finally, (Li et al2012) presented an unsupervised system for ex-tracting (no classification) NEs in targeted Twitterstreams, which exploits knowledge gathered fromthe web and exhibits comparable performance tothe supervised approaches mentioned earlier.Most of the above mentioned work on NER intweets focused on English.
To our best knowledgeno prior work on NER in tweets in Polish has beenreported, which makes our effort a pioneering con-tribution in this specific field.
Our work also con-tributes to NER from targeted Twitter streams.3 Named Entity Extraction fromTargeted Tweets in PolishThe objective of this work is to explore vari-ous linguistically lightweight strategies to adaptan existing news-oriented rule-based NER systemfor Polish to the processing of tweets in targetedTwitter streams.
Starting from the adaptation ofa NER rule-based system to the processing oftweets (Section 3.1), we incrementally refine theapproach with, first, the introduction of a stringsimilarity-based name matching step (Section 3.2)and, second, the exploitation of corpus statisticsand knowledge-poor method for name guessing(Section 3.3).3.1 NER Grammar for PolishThe starting point of our explorations is an exist-ing NER system for Polish, modeled as a cascadeof finite-state grammars using the EXPRESS for-malism (Piskorski, 2007).
Similarly to rule-basedapproaches to NER for many other Indo-Europeanlanguages, the grammars consist of a set of extrac-tion patterns for person, organization and locationnames.
The patterns exploit both internal (e.g.,company designators) and external clues (e.g., ti-tles and functions of a person, etc.)
for name de-tection and classification; a simple extraction pat-tern for person names can be illustrated as follows:PER :> ( ( gazetteer & [TYPE: "firstname",SURFACE: #F] )( gazetteer & [TYPE: "initial",SURFACE: #I] ) ?
( surname-candidate & [SURFACE: #L] )):name-> name: person & [NAME: #FULL-NAME]& #full_name := ConcWithBlanks(#F,#I,#L).This rule first matches a sequence consisting of: afirst name (through a gazetteer look-up), an op-tional initial (gazetteer look-up as well) and, fi-nally, a sequence of characters considered as sur-name candidate (e.g., capitalized tokens), whichwas detected by a lower-level grammar1 andis represented as a structure of type surname-candidate.
The right-hand side of the extraction1Lower-level grammar extract small-scale structureswhich might constitute parts of named entities.85pattern specifies the output structure of type per-son with one attribute called NAME, whose valueis simply a concatenation of the values of the vari-ables #F, #I and #L assigned to the surface formsof the matched first name, initial and surname can-didate respectively.Overall the grammar contains 15 extraction pat-terns for person names, 10 for location names,and 10 for organization names.
It relies on ahuge gazetteer of circa 294K entries, which isan extended version of the gazetteer describedin (Savary and Piskorski, 2011) and includes, i.a.,39K inflected forms of both Polish and foreignfirst names, 86K inflected forms of surnames, 5Kof organisation names (only partially inflected),10K of inflected location names (e.g., city names,country names, rivers, etc.).
No morphological an-alyzer for Polish is used and only a tiny fraction ofthe extraction patterns relies on morphological in-formation (encoded in the gazetteer).
In this orig-inal grammar, the patterns are divided into sure-fire patterns and less reliable patterns (whose pre-cision is expected to be lower).
The latter onesare patterns that rely solely on gazetteer informa-tion (simple look-up), which might have ambigu-ous interpretation, e.g., patterns that only matchfirst names in text.
When applied on conven-tional online news, the performance of this orig-inal NER grammar oscillates around 85% in termsof F-measure.In order to process tweets, we slightly modi-fied this grammar, mostly by simplifying it.
Sincementions of entities in tweets frequently occur assingle tokens (e.g., external evidence as in clas-sical news is often missing), we did not keep thedistinction between sure-fire and less-reliable pat-terns.
Furthermore, the original NER grammar?included?
a mechanism (encoded directly in pat-tern specification) to lemmatize the recognizednames as well as to extract various attributes suchas titles (e.g., ?Pan?
(Mr.)) and position (e.g.,?Prezydent?
(president)) for persons.
As we aremainly interested in the detection and classifica-tion of NEs while processing tweets, these func-tionalities were not needed and the grammar sim-ply extracts names and their type.
This ?reduced?NER grammar constitutes the baseline approach,and will be referred to as BASE in the remain-ing part of the paper.
It is worth mentioning thatwe tested as well a version of the grammar withlower-cased lexical resources, but due to poor re-sults (mainly due to high ambiguity of lower-caselexical entries) we did not conduct further explo-rations in this direction.3.2 String distance-based Name MatchingIn tweets, names are often abbreviated (e.g., ?Parl.Europ.?
and ?PE?
are abbreviations of ?Parla-ment Europejski?
), glued to other words (e.g.,?prezydent Komorowski?
is sometimes written as?prezydentKomorowski?)
and misspelled variantsare frequent (e.g., ?Donlad Tusk?
is a frequentmisspelling of ?Donald Tusk?).
The NER gram-mar ?as is?
would fail to recognize the particularnames in the aforementioned examples.
There-fore, in order to improve the recall of the ?tweetgrammar?, we perform a second run deployingstring distance metrics (in the entire targeted Twit-ter stream) for matching new mentions of namespreviously recognized by the NER grammar (seeSection 3.1).
Furthermore, due to the highly in-flective character of Polish, we also expect to cap-ture with string distance metrics non-nominativementions of names (e.g., ?Rzeczpospolitej - geni-tive/dative/locative form of ?Rzeczpospolita?
- thename of a Polish daily newspaper), which the NERgrammar might have failed to recognize.Inspired by the work reported in (Piskorski etal., 2009) we explored the performance of sev-eral string distance metrics.
First, we tested thebaseline Levenshtein edit distance metric givenby the minimum number of character-level oper-ations (insertion, deletion, or substitution) neededto transform one string into another (Levenshtein,1965).
Next, we used an extension thereof, namelySmith-Waterman (SW) metric (Smith and Water-man, 1981), which additionally allows for vari-able cost adjustment to the cost of a gap and vari-able cost of substitutions (mapping each pair ofsymbols from alphabet to some cost).
We used avariant of this metric, where the Smith-Watermanscore is normalized using the Dice coefficient (theaverage length of strings compared).Subsequently, we explored variants of the Jarometric (Jaro, 1989; Winkler, 1999).
It considersthe number and the order of the common char-acters between the two strings being compared.More precisely, given two strings s = a1 .
.
.
aKand t = b1 .
.
.
bL, we say that ai in s is commonwith t if there is a bj = ai in t such that i ?
R ?j ?
i+R, where R = bmax(|s|, |t|)/2c?
1.
Fur-thermore, let s?
= a?1 .
.
.
a?K?
be the characters in86s which are common with t (with preserved orderof appearance in s) and let t?
= b?1 .
.
.
b?L?
be de-fined analogously.
A transposition for s?
and t?
isdefined as any position i such that a?i 6= b?i.
Let usdenote the number of transpositions for s?
and t?as Ts?,t?
.
The Jaro similarity is then calculated as:J(s, t) =13?
(|s?||s|+|t?||t|+|s?| ?
bTs?,t?/2c|s?|)A Winkler variant of Jaro metric boosts thissimilarity for strings with agreeing initial charac-ters and is calculated as:JW (s, t) = J(s, t) + ?
?
boostp(s, t) ?
(1?
J(s, t))where ?
denotes the common prefix adjustmentfactor (default value is 0.1) and boostp(s, t) =min(|lcp(s, t)|, p).
Here lcp(s, t) denotes thelongest common prefix between s and t. Further, pstands for the upper bound of |lcp(s, t)|2 , i.e., upfrom a certain length of lcp(s, t) the ?boost value?remains the same.The q-gram metric (Ukkonen, 1992) is basedon the intuition that two strings are similar ifthey share a large number of character-level q-grams.
We used a variant thereof, namely skip-gram metric (Keskustalo et al 2003), which ex-hibited better performance than any other variantof character-level q-grams based metrics.
It isbased on the idea that in addition to forming bi-grams of adjacent characters, bigrams that skipcharacters are considered.
Gram classes are de-fined that specify what kind of skip-grams are cre-ated, e.g.
{0, 1} class means that normal bigramsare formed, and bigrams that skip one character.In particular, we tested {0, 1} and {0, 2} classes.Due to the nature of Twitter we expected skip-grams to be particularly useful in our experiments.Considering the declension paradigm of Polishwe also considered the basic CommonPrefix met-ric introduced in (Piskorski et al 2009), which isbased on the longest common prefix.
It is calcu-lated as:CP (s, t) = (|lcp(s, t)|)2/|s| ?
|t|Finally, we evaluated the performance oflongest common sub-strings distance metric,which recursively finds and removes the longest2Here p is set to 6.common sub-string in the two strings compared.Let lcs(s, t) denote the first longest common sub-string for s and t and let s?p denote a string ob-tained by removing from s the first occurrence ofp in s. The LCS metric is calculated as:LCS(s, t) =??????
?0 if |lcs(s, t)| ?
2|lcs(s, t)|+ LCS(s?lcs(s,t), t?lcs(s,t))otherwiseThe string distance-based name matching de-scribed in this section will be referred to asMATCH-X, with X standing for the name of thestring distance metric being used.3.3 Name ClusteringSince contextual clues for recognizing names informal texts are often missing in tweets, we ad-ditionally developed a rudimentary name guesserto boost the recall.
Let us also observe that usingstring distance metrics described in Section 3.2 tomatch all not yet captured mentions of previouslyrecognized names might not be easy due the factthat the process of creating abbreviations in Twit-ter is very productive, e.g., ?Rzeczpospolita?
ap-pears abbreviated as ?
Rzepa?, Rzp.
or ?RP, whichare substantially different from the original name.The main idea beyond the name guesser is basedon the following assumption: given a targetedTwitter stream, if a capitalized word n-gram hasa couple of ?similar?
word n-grams in the samestream, most of which are not recognized as validword forms, then such a group of n-grams wordare most likely named mentions of the same entity(e.g., person, organization or location, etc.).
To bemore precise, the name guesser works as follows.1.
Compute S = {s1, s2, ....sk} - a set of worduni- and bigrams (cluster seeds) in the Twit-ter stream3, where frequency(si) ?
?4 andcharacter ?
length(si) ?
3 for all si ?
S.2.
Create an initial set of singleton ?name?
clus-ters: C = {C1, C2, .
.
.
, Ck} with Ci = {si}.3.
Build clusters of simmilar n-gramsaround the selected uni- and bigrams3The vast majority of names annotated in our test corpusare either word unigrams or bigrams (see Section 4.1.)4?
We explored various values of this parameter, which isdescribed in Section 4.287using the string distance metric m: As-sign each word n-gram w in the Twitterstream to at most one cluster Cj withj ?
arg minx?
{1,2,...,k} distm(sx, w)5, anddistm(sj , w) ?
maxDist, where maxDistis a predefined constant.4.
Iteratively merge most-simmilar clusters inC: If ?Cx, Cy ?
C with DIST (Cx, Cy) ?DIST (Ci, Cj) for i, j ?
{1, .
.
.
, |C|}6 andDIST (Cx, Cy) ?
maxDist then C = C \{Cx, Cy} ?
(Cx ?
Cy).5.
Discard ?small?
clusters:C = {Cx ?
C : |Cx| ?
3}6.
Discard clusters containing high number ofn-grams, whose parts are valid word forms,but not proper names: C = {Cx ?C : ?w?CxWordForm?(w)|Cx|?
0.3}, whereWordForm?
(w) = 1 if all the wordsconstituting the word n-gram w are validword forms, but not proper names, andWordForm?
(w) = 0 otherwise, e.g.,WordForm?
(Jan Grzyb) = 0 since Grzyb(eng.
mushroom) can be interpreted as avalid word form, which is not a proper name,whereas Jan has only proper name interpre-tation.7.
Use the n-grams in the remaining clustersin C (each of them is considered to containnamed mentions of the same entity) to matchnames in the Twitter stream through simplelexicon look-up.For computing similarity of n-grams and merg-ing clusters we used the longest common sub-strings (LCS) metric which performed on averagebest (in terms of F-measure) in the context of namematching (see Section 3.2 and 4).
For checkingwhether tokens constitute valid word forms we ex-ploited PoliMorf (Wolin?ski et al 2012), a freelyavailable morphological dictionary of Polish, con-sisting of circa 6.7 million word forms, includ-ing proper names.
Proper names are distinguishedfrom other entries in the aforementioned resource.The name guesser sketched above will be re-ferred to as CLUSTERING.
Instead of building the5We denote the distance between two strings x and y mea-sured with the string distance metric m as distm(x, y)6DIST (Cx, Cy) = ?s?Cx?t?Cydistm(s,t)|Cx|?|Cy|(averagedistance between strings in the two clusters)name clusters around n-grams, whose frequencyexceeds certain threshold, we also tested buildingclusters around least frequent n-grams (i.e., whosefrequency is ?
3), which will be referred to asCLUSTERING-INFRQ.
The name guesser runs ei-ther independently or on top of the NER grammardescribed in Section 3.1 in order to detect ?new?names in the unconsumed part of the tweet collec-tion, i.e., names recognized by the grammar arepreserved.
It is important to emphasize that theclustering-based name guesser only detects nameswithout classifying them.4 Experiments4.1 DatasetWe have gathered tweet collections using Twit-ter search API7 focusing on some major events in2012/2013 and on famous individuals, namely: (a)Boston marathon bombings, (b) general commentson Donald Tusk, the prime minister of Poland,(c) discussion on the public comments of AntoniMacierewicz (a politician of the Law and Justiceopposition party in Poland) on the Polish presi-dent crash in Smolen?sk (Russia) in 2010, (d) de-bate on the controversial firing of the journalistCezary Gmyz from one of the major Polish news-papers Rzeczpospolita and, (e) a collection of ran-dom tweets in Polish.
Each tweet collection wasextracted using simple queries, e.g., "zamach AND(Boston OR Bostonie)" ("attack" AND "?Boston"?either in nominative of locative form) for collect-ing tweets on the Boston bombings.
From eachcollection a subset of randomly chosen tweets wasselected for evaluation purposes.
We will referto the latter as the test corpus, whereas the entiretweet collections will be referred to as the streamcorpus.In the stream corpus, we computed for eachtweet: (a) the text-like fraction of its body, i.e., thefraction of the body which contains text, and (b)the lexical validity, i.e., the percentage of tokens inthe text-like part of the body of the tweet which arevalid word forms in Polish8.
Figure 1 and 2 showthe histograms for text-like fraction and lexical va-lidity of the tweets in each collection in the streamcorpus.
We can observe that large portion of thetweets contains significant text-like part, which is7https://dev.twitter.com8For computing lexical validity we usedPoliMorf (Wolin?ski et al 2012), already mentioned inthe previous section.88also lexically valid.
Interestingly, the random col-lection exhibits lower lexical validity, which is dueto more colloquial language used in the tweets inthis collection.10 20 30 40 50 60 70 80 90 100051015202530 Boston Tusk Macierewicz Gmyz RandomText-like fractionPercentage of TweetsFigure 1: Text-like fraction of the tweets in eachcollection.10 20 30 40 50 60 70 80 90 10005101520253035 Boston Tusk Macierewicz Gmyz RandomText-lik litftraceo-enrlPekgwks?eer?Figure 2: Lexical validity of the tweets in eachcollection.We built the test corpus by randomly select-ing tweets whose text-like fraction of the bodywas ?
80%, additionally checking the languageand removing duplicates.
These tweets were af-terwards manually annotated with person, loca-tion and organization names, according to the fol-lowing guidelines: consideration of unigram en-tities, non-inclusion of titles, functions and alike,non-inclusion of spurious punctuation marks andexclusion of names starting with ?
@?, since theirrecognition as names is trivial.The test corpus statistics are provided in Ta-ble 1.
We provide in brackets the number of tweetsin the corresponding tweet collections in the en-tire stream corpus.
In this test corpus, 86,7% ofthe annotated names are word unigrams, whereasbigrams constitute 12,7% of the annotated namesand 3- and 4-grams account only for a tiny frac-tion (0,6%); this is in line with the characteristicsof the Twitter language, which favours quick andsimple expressions.
For each collection, we com-puted the name diversity as the ratio between en-tity occurrences and unique entities, as well as theaverage number of entities per tweet9.
Targetedstream corpora show a medium name diversity(except for Boston and Gmyz collections, centredon a very specific location and person name resp.
)and a high rate of entity per tweet (around 2.2), incontrast with random corpus which shows a highname diversity (0.79) for a low average number ofentity per tweets.
Reported to the limited numberof characters in tweets (140), the important signifi-cant number of entity per tweet in targeted streamsaccounts, on the one hand, for the usefulness ofworking on targeted streams and, on the other, forthe importance of NER in tweets.Corpus #tweets name #names #PER #LOC #ORGdiversity pertweetBoston 198 0.24 2.16 34 298 96(2953)Tusk 232 0.36 2.42 393 88 80(1186)Macierewicz 303 0.32 2.17 494 60 104(931)Gmyz 310 0.24 2.09 471 18 159(672)Random 286 0.79 0.36 59 19 27(7806)Table 1: Test corpus statistics.4.2 EvaluationIn our experiments we evaluated the performanceof (i) the NER grammar (BASE), a combina-tion thereof with (ii) different name matchingstrategies (MATCH) and (iii) different variants ofthe name guesser (CLUSTERING, CLUSTERING-INFRQ) and, finally, (iv) the combinations of alltechniques.
Within the MATCH configuration, weexperimented all string distance metrics presentedin 3.2 but since Jaro, Jaro-Winkler and Smith-Waterman metrics performed on average worsethan the others, we did not consider them infurther experiments.
We selected the best per-forming metric, LCS 10, as the one used by thename guesser (CLUSTERING) in subsequent exper-iments.
As a complement, we measured the per-formance of the name guesser alone to compareit with BASE.
Furthermore, name matching and9In the limit of our reference corpora, i.e.
entities of typeperson, location and organization.10Skip-grams was the other metric which exhibited similarperformance89name guessing algorithms were using the tweetcollections in the stream corpus (as quasi ?Twitterstream window?)
in order to gather knowledge formatching/guessing ?new?
names in the test corpus.We measured the performance of the differentconfigurations in terms of Precision (P), Recall(R) and F-measure (F), according to two differ-ent schemes: exact match, where entity types andboth boundaries should match perfectly, and fuzzymatch, which allows for one name boundary re-turned by the system to be different from the ref-erence, i.e., either too short or too long on the leftor on the right, but not on both.
Furthermore, sincethe clustering-based name guesser described in 3.3does not classify names, for any settings with thistechnique we only evaluated name detection per-formance, i.e., no distinction between name typeswas made.
The overall summary of the results forthe entire pool of tweet collections, is presented inTable 3.In the context of the CLUSTERING algorithm weexplored various settings as regards the minimumfrequency of an n-gram to be considered as clus-ter seed (?
parameter - see Section 3.3).
Moreprecisely, we tested values in the range of 1 to30 for all corpora and system settings which in-cluded CLUSTERING, and compared the resultingP/R and F figures.
An example of a curve with P/Rvalues (exact match) of BASE-CLUSTERING algo-rithm applied on the ?Boston?
corpus with vary-ing values of ?
is given in Figure 3.
One can ob-serve and hypothesize that the frequency thresholddoes not impact much the performance.
Suchlikecurves for other settings were of a similar nature.Therefore we decided to set the ?
to 1 in all set-tings reported in Table 3.4.3 Results analysisThe performance of the NER grammars is surpris-ingly good, both in case of exact and fuzzy matchevaluation.
Except for random corpus (whichshows rather low performance with 55% precisionand 39% recall), precision figures oscillate around85-95%, whereas recall is somewhat worse (60-75%), as was to be expected.
The low recall for?Gmyz?
corpus is due to the non-matching of a fre-quently occurring person name.
Precision and re-call figures for each entity type for BASE are givenin Table 2.
In general, recognition of organizationnames appears to be more difficult (lower recall),especially in the random corpus.1 0 2 3 4 5 6 7 8 19 11 10 12 13 14 15 16 17 18 09 01 00 02 03 04 05 06 07 08 2999B19B09B29B39B49B59B69B79B81 ostnTuTkM atnciiTexet-tlik frtlafco-cxnPllgwlsclnwx?e?cfc?lrln?-?gcfl?cc?Figure 3: Precision and Recall figures for BASE-CLUSTERING applied on ?Boston?
corpus, withdifferent frequency thresholds of n-grams to beconsidered cluster seeds.Corpus PER ORG LOCP R P R P RBoston 31.6 35.3 87.9 30.2 94.3 71.8Tusk 87.6 71.2 82.4 35.0 89.9 70.5Gmyz 85.5 32.5 82.8 15.1 88.9 44.4Macierewicz 93.6 80.2 71.2 35.6 83.7 60.0Random 56.7 55.9 0 0 53.3 42.1Table 2: Precision/recall figures for person, or-ganization and location name recognition (exactmatch) with BASE.Extending BASE with MATCH yields some im-provements in terms of recall (including randomcorpus), whereas precision either oscillates aroundthe figures achieved by BASE, or deteriorates.
Incase of ?Gmyz?
corpus, we can observe significantgain in both recall and precision through using thename matching step.
With regard to the other cor-pora, the reason for not obtaining a significant gaincould be due to two reasons: (a) the n-grams iden-tified as similar to the names recognized by BASEare already covered by BASE with some patterns(e.g., inflected forms of many entities are stored inthe gazetteer), or (b) using string distance metricsin the MATCH step might not be the best method tocapture mentions of a recognized entity, as exem-plified in Table 4, where the mentions of a news-paper Rzeczpospolita (captured by BASE) may besignificantly different, e.g., in terms of the charac-ter length.Regarding the results for CLUSTERING-INFRQ,running it alone, yielded poor results for all cor-pora, only in case of the?Gmyz?
corpus a gaincould be observed.
CLUSTERING performed betterthan CLUSTERING-INFRQ for all corpora.Deploying BASE with CLUSTERING on top ofit results in up to 1.5-6% (exact match) and 4-90EXACT MATCHMethod Boston Tusk Gmyz Macierewicz AVERAGEP R F P R F P R F P R F P R FBASE 85.6 59.6 70.2 87.7 65.9 75.3 85.3 28.5 42.8 90.5 71.3 79.8 87.3 56.3 67.0BASE-MATCH-LEV 80.8 62.9 70.7 87.4 66.5 75.5 90.9 63.6 74.8 90.2 72.3 80.3 87.3 66.3 75.3BASE-MATCH-SW 70.9 62.1 66.3 76.6 67.5 71.8 78.0 59.1 68.0 89.4 73.1 80.4 78.7 65.5 71.6BASE-MATCH-J 67.7 62.1 64.8 79.3 68.1 73.3 60.9 48.3 53.9 60.0 73.3 65.9 67.0 63.0 64.5BASE-MATCH-JW 63.2 62.1 62.7 75.5 68.3 71.7 48.2 48.9 48.6 58.0 74.0 65.0 61.2 63.3 62.0BASE-MATCH-SKIP(0,1) 80.9 62.1 70.3 87.6 66.5 75.6 91.3 63.0 74.5 90.3 72.2 80.2 87.5 66.0 75.2BASE-MATCH-SKIP(0,2) 80.9 62.1 70.3 87.7 66.3 75.5 91.5 63.0 74.6 90.6 72.2 80.4 87.7 65.9 75.2BASE-MATCH-CP 80.2 59.6 68.4 87.7 66.0 75.3 83.5 58.6 68.9 90.2 71.4 79.7 85.4 63.9 73.1BASE-MATCH-LCS 80.7 63.6 71.1 86.8 67.0 75.7 82.3 59.0 68.7 90.2 72.9 80.7 85 65.6 74.1CLUSTERING 66.2 10.0 17.4 60.6 33.2 42.9 61.3 36.0 45.3 52.9 33.4 41.0 60.3 28.2 36.7CLUSTERING-INFRQ 37.5 1.4 2.7 27.3 1.1 2.1 60.7 31.5 41.5 54.8 28.6 37.6 45.1 15.7 21.0BASE-CUSTERING 86.8 67.8 76.1 91.1 72.7 80.9 80.6 61.0 69.4 86.3 74.6 80.0 86.2 69.0 76.6BASE-CLUSTERING-INFRQ 89.7 65.0 75.3 89.4 69.3 78.1 81.2 58.5 68.0 89.9 74.2 81.3 87.6 66.8 75.7BASE-MATCH-CLUSTERING 87.6 75.9 81.4 90.2 73.8 81.2 74.1 62.8 68.0 86.1 76.3 80.9 84.5 72.2 77.9BASE-MATCH-CLUSTERING-INFRQ 90.0 73.4 80.8 88.6 70.4 78.5 74.3 60.3 66.6 89.6 75.8 82.1 85.6 70.0 77.0FUZZY MATCHMethod Boston Tusk Gmyz Macierewicz AVERAGEP R F P R F P R F P R F P R FBASE 86.6 60.3 71.1 92.2 69.3 79.1 88.0 29.5 44.2 95.0 74.8 83.7 90.5 58.5 69.5BASE-MATCH-LEV 81.7 63.6 71.5 92.3 70.2 79.8 93.6 65.4 77.0 94.9 76.1 84.5 90.6 68.8 78.2BASE-MATCH-SW 73.3 64.3 68.5 80.8 71.3 75.8 91.4 67.6 77.7 94.2 77.1 84.8 84.9 70.1 76.7BASE-MATCH-J 70.5 64.7 67.5 85.5 73.4 79.0 86.2 68.4 76.2 63.4 77.5 69.8 76.4 71.0 73.1BASE-MATCH-JW 65.8 64.7 65.3 81.9 74.0 77.7 68.2 69.1 68.7 61.4 78.4 68.9 69.3 71.6 70.2BASE-MATCH-SKIP(0,1) 81.8 62.9 71.1 92.3 70.1 79.6 94.0 64.8 76.7 95.1 76.0 84.5 90.8 68.5 78.0BASE-MATCH-SKIP(0,2) 81.8 62.9 71.1 92.2 69.7 79.4 94.2 64.8 76.8 95.0 75.7 84.3 90.8 68.3 77.9BASE-MATCH-CP 81.1 60.3 69.2 92.2 69.3 79.1 93.8 65.9 77.4 95.0 75.2 84.0 90.5 67.7 77.4BASE-MATCH-LCS 81.6 64.3 71.9 92.4 71.3 80.5 93.1 66.7 77.7 94.9 76.7 84.9 90.5 69.8 78.8CLUSTERING 83.1 12.6 21.9 96.4 52.8 68.2 89.2 52.3 66.0 87.7 55.5 68.0 89.1 43.3 56.0CLUSTERING-INFRQ 87.5 3.3 6.3 68.2 2.7 5.1 91.1 47.2 62.2 94.2 49.1 64.5 85.3 25.6 34.5BASE-CLUSTERING 93.1 72.7 81.6 96.9 77.4 86.0 94.5 71.4 81.4 91.7 79.3 85.1 94.1 75.2 83.5BASE-CLUSTERING-INFRQ 95.5 69.2 80.2 95.9 74.3 83.7 96.4 69.4 80.7 96.9 79.9 87.6 96.2 73.2 83.1BASE-MATCH-CLUSTERING 93.3 80.8 86.6 96.5 79.0 86.9 92.9 78.7 85.2 91.8 81.3 86.2 93.6 80.0 86.2BASE-MATCH-CLUSTERING-INFRQ 95.1 77.6 85.5 96.0 76.3 85.0 94.5 76.7 84.7 96.6 81.8 88.6 95.6 78.1 86.0Table 3: Precision, Recall and F-measure figures for exact (top) and fuzzy match (bottom).
The bestresults are highlighted in bold.CEZARY GMYZ zwolniony z "Rzeczpospolitej".
To efekt spotkania zZarza?dem i Rada?
Nadzorcza?
wydawcy dziennika http://t.co/QspE3edh@agawaa ...usi?ujesz czepic?
sie szczeg?
?u, gdy istota sprawy jest taka:Rzepa/Gmyz pitolili bez sensu.Konflikt w Rzepie?
Ta ca?a sytuacja na to wskazuje.
Gmyz sie?
nie wycofuje,a Rzepa jak najbardziej.
@volanowski Nowa linia: Gmyz wyrzucony z Rzepy czyli PO we wszystkichsprawach smolen?skich jest cacy i super.
Ludzie na to nie p?jda.
@TomaszSkory Byc?
moz?e "Rz" i Gmyz p?aca?
teraz w?as?nie za "skr?tymys?lowe" swoich informator?w.
Dlaczego RMF nie p?aci za "skr?ty" swoich?Gmyz wylecia?
z RP, a Ziemkiewicz straci?
Subotnik?
Nie lepiej by?o niecozejs?c?
z 3.50 z?, czy chodzi o cos?
zupe?nie innego?Gmyz wyrzucony z "Rzeczpospolitej".
"Dzisiaj zwolniono mnie dyscyp-linarnie": Cezary Gmyz straci?
prace?
w "Rzeczp... http://t.co/ObZIxXMLTable 4: Examples of various ways of referring toa newspaper Rzeczpospolita in tweets.10% (fuzzy match) gain in F-measure comparedto BASE (mainly thanks to gain in recall), ex-cept ?Gmyz?
corpus, where the gain is higher.The average gain over the four targeted corporaagainst the best combination of BASE-MATCH inF-measure is 1.3%.
We observed comparable im-provement for the random corpus.
It turned outCLUSTERING often contributes to the recognitionof names glued to other words and/or character se-quences.Combining BASE with MATCH-LCS and CLUS-TERING/CLUSTERING-INFRQ yields further im-provements against the other settings.
In par-ticular, the gain in F-measure of BASE-MATCH-CLUSTERING against BASE, measured over thefour targeted corpora, is 10.9% and 16.7% for ex-act and fuzzy match respectively (mainly due togain in recall).Considering the nature of Twitter messages theaverage F-measure score over the four targetedcorpora for BASE-MATCH-CLUSTERING, amount-ing to 77.9% (exact match) and 86.2% (fuzzymatch) can be seen as a fairly good result.
Al-though the difference in some of the correspond-ing scores for exact and fuzzy match appear sub-stantial, it is worth mentioning that CLUSTERINGalgorithm often guesses name candidates that areeither preceded or followed by some charactersnot belonging to the name itself, which is pe-nalized in exact-match evaluation.
This problemcould be alleviated through deployment of heuris-tics to trim such ?unwanted?
characters.
Anothersource of false positives extracted by CLUSTER-ING is the fact that this method might, beyondperson, organization and location types, recognizeany kind of NEs, which, even not very frequent, ispenalized since they are not present in our refer-ence corpus.In general, considering the shortness of namesin Twitter, the major type of errors in all settingsare either added or missed entities, but more rarelyoverlapping problems.
One of the main source oferrors is due to the fact that single-token names,which are frequent in tweets, often exhibit type91ambiguity.
Once badly recognized, these errorsare propagated over the next processing steps.5 Conclusions and OutlookIn this paper we have reported on experiments ontuning an existing finite-state based NER gram-mar for processing formal texts to NER fromtargeted Twitter streams in Polish through com-bining it with knowledge-poor techniques forstring distance-based name matching and corpusstatistics-based name guessing.
Surprisingly, theNER grammar alone applied on the four test cor-pora (including circa 2300 proper names) yieldedP, R, and F figures for exact (fuzzy) matchingproper names (including: person, organization andlocations) of 87.3% (90.5%), 56.3% (58.5) and67% (69.5%) resp., which can be considered fairlyreasonable result, though some variations acrosstweet collections could be observed (dependingon the topic and how people ?tweet?
about).The integration of the presented knowledge-poortechniques for name matching/guessing resultedin P, R and F figures for exact (fuzzy) match-ing names of 84.5% (93.6%), 72.2% (80.0) and77.9% (86.2%) resp.
(setting with best F-measurescores), which constitutes a substantial improve-ment against the grammar-based approach.
Wecan observe that satisfactory-performing NERfrom targeted Twitter streams in Polish can beachieved in a relatively straightforward manner.As future work to enhance our experiments, weenvisage to: (a) enlarge the pool of test corpora,(b) carry out a more thorough error analysis, (c)test a wider range of string distance metrics (Co-hen et al 2003), (d) study the applicability of theparticular NER grammar rules w.r.t.
their useful-ness in NER in targeted Twitter streams and (e),compare our approach with an unsupervised ML-approach, e.g.
as in (Li et al 2012).AcknowledgmentsThe authors gratefully acknowledge the support ofthe ERC Starting Grant MultiJEDI No.
259234and the Polish National Science Centre grant NN516 481940 ?Diversum?.ReferencesSmitashree Choudhury and John Breslin.
2011.
Ex-tracting Semantic Entities and Events from SportsTweets.
In Proceedings of the 1st Workshop onMaking Sense of Microposts (#MSM2011), pages22?32.William W. Cohen, Pradeep Ravikumar, andStephen E. Fienberg.
2003.
A Comparison ofString Distance Metrics for Name-matching Tasks.In Proceedings of the IJCAI-2003 Workshop onInformation Integration on the Web (IIWeb-03),pages 73?78.Alexandre Davis, Adriano Veloso, Altigran S. da Silva,Wagner Meira, Jr., and Alberto H. F. Laender.2012.
Named Entity Disambiguation in StreamingData.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics:Long Papers - Volume 1, ACL ?12, pages 815?824,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Filip Gralin?ski, Krzysztof Jassem, Micha?
Marcin?czuk,and Pawe?
Wawrzyniak.
2009.
Named entity recog-nition in machine anonymization.
In Recent Ad-vances in Intelligent Information Systems, pages247?260, Warsaw.
Exit.Mathew Jaro.
1989.
Advances in record linkingmethodology as applied to the 1985 census of TampaFlorida.
Journal of the American Statistical Society,84(406):414?420.Heikki Keskustalo, Ari Pirkola, Kari Visala, ErkkaLepp?nen, and Kalervo J?rvelin.
2003.
Non-adjacent Digrams Improve Matching of Cross-lingual Spelling Variants.
In Proceedings of SPIRE,LNCS 22857, Manaus, Brazil, pages 252?265.Vladimir Levenshtein.
1965.
Binary Codes for Cor-recting Deletions, Insertions, and Reversals.
Dok-lady Akademii Nauk SSSR, 163(4):845?848.Chenliang Li, Jianshu Weng, Qi He, Yuxia Yao, An-witaman Datta, Aixin Sun, and Bu-Sung Lee.
2012.TwiNER: Named Entity Recognition in TargetedTwitter Stream.
In Proceedings of the 35th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR ?12,pages 721?730, New York, NY, USA.
ACM.Xiaohua Liu, Shaodian Zhang, Furu Wei, and MingZhou.
2011.
Recognizing Named Entities inTweets.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1,pages 359?367, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Xiaohua Liu, Ming Zhou, Furu Wei, Zhongyang Fu,and Xiangyang Zhou.
2012.
Joint Inference ofNamed Entity Recognition and Normalization forTweets.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers - Volume 1, ACL ?12, pages 526?535, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.92Brian Locke and James Martin.
2009.
Named EntityRecognition: Adapting to Microblogging.
SeniorThesis, University of Colorado.Wies?aw Lubaszewski.
2007.
Information extractiontools for polish text.
In Proc.
of LTC?07, Poznan?,Poland, Poznan?.
Wydawnictwo Poznanskie.Wies?aw Lubaszewski.
2009.
S?owniki komputerowe iautomatyczna ekstrakcja informacji z tekstu.
AGHUczelniane Wydawnictwa Naukowo-Dydaktyczne,Krak?w.Micha?
Marcin?czuk and Maciej Janicki.
2012.
Opti-mizing CRF-Based Model for Proper Name Recog-nition in Polish Texts.
In A. Gelbukh, editor,CICLing 2012, Part I, volume 7181 of LectureNotes in Computer Science (LNCS), pages 258??269.
Springer, Heidelberg.Micha?
Marcin?czuk and Maciej Piasecki.
2007.
Pat-tern extraction for event recognition in the reportsof polish stockholders.
In Proceedings of IMCSIT?AAIA?07, Wis?a, Poland, pages 275?284.Micha?
Marcin?czuk and Maciej Piasecki.
2010.Named Entity Recognition in the Domain of Pol-ish Stock Exchange Reports.
In Proceedings of In-telligent Information Systems 2010, Siedlce, Poland,pages 127?140.Ma?gorzata Marciniak, Joanna Rabiega-Wis?niewska,Agata Savary, Marcin Wolin?ski, and Celina Heliasz.2009.
Constructing an Electronic Dictionary of Pol-ish Urban Proper Names.
In Recent Advances in In-telligent Information Systems.
Exit.David Nadeau and Satoshi Sekine.
2007.
A Sur-vey of Named Entity Recognition and Classification.Lingvisticae Investigationes, 30(1):3?26.Kamel Nebhi.
2012.
Ontology-Based Information Ex-traction from Twitter.
In Proceedings of the COL-ING 2012 IEEASM Workshop, Mumbai, India.Jakub Piskorski, Karol Wieloch, and Marcin Sydow.2009.
On Knowledge-poor Methods for PersonName Matching and Lemmatization for HighlyInflectional Languages.
Information Retrieval,12(3):275?299.Jakub Piskorski.
2005.
Named-Entity Recognition forPolish with SProUT.
In LNCS Vol 3490: Proceed-ings of IMTCI 2004, Warsaw, Poland.Jakub Piskorski.
2007.
ExPRESS ?
Extraction Pat-tern Recognition Engine and Specification Suite.
InProceedings of FSMNLP 2007.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named Entity Recognition in Tweets: An Ex-perimental Study.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2011), pages 1524?1534, Ed-inburgh, Scotland, UK.
Association for Computa-tional Linguistics.Agata Savary and Jakub Piskorski.
2011.
LanguageResources for Named Entity Annotation in the Na-tional Corpus of Polish.
Control and Cybernetics,40(2):361?391.Agata Savary, Joanna Rabiega-Wis?niewska, andMarcin Wolin?ski.
2009.
Inflection of Polish Multi-Word Proper Names with Morfeusz and Multiflex.LNAI, 5070.T.
Smith and M. Waterman.
1981.
Identificationof Common Molecular Subsequences.
Journal ofMolecular Biology, 147:195?197.Esko Ukkonen.
1992.
Approximate String Matchingwith q-grams and Maximal Matches.
TheoreticalComputer Science, 92(1):191?211.Jakub Waszczuk, Katarzyna G?owin?ska, Agata Savary,and Adam Przepi?rkowski.
2010.
Tools andMethodologies for Annotating Syntax and NamedEntities in the National Corpus of Polish.
In Pro-ceedings of the International Multiconference onComputer Science and Information Technology (IM-CSIT 2010): Computational Linguistics ?
Applica-tions (CLA?10), pages 531?539.William Winkler.
1999.
The State of Record Link-age and Current Research Problems.
Technical re-port, Statistical Research Division, U.S. Bureau ofthe Census, Washington, DC.Marcin Wolin?ski, Marcin Mi?kowski, Maciej Ogrod-niczuk, Adam Przepi?rkowski, and ?ukasz Sza-lkiewicz.
2012.
PoliMorf: A (not so) new openmorphological dictionary for Polish.
In Proceedingsof the Eighth International Conference on LanguageResources and Evaluation, LREC 2012, pages 860?864.93
