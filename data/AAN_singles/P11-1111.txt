Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1109?1116,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsOrdering Prenominal Modifiers with a Reranking ApproachJenny LiuMIT CSAILjyliu@csail.mit.eduAria HaghighiMIT CSAILme@aria42.comAbstractIn this work, we present a novel approachto the generation task of ordering prenomi-nal modifiers.
We take a maximum entropyreranking approach to the problem which ad-mits arbitrary features on a permutation ofmodifiers, exploiting hundreds of thousands offeatures in total.
We compare our error rates tothe state-of-the-art and to a strong Google n-gram count baseline.
We attain a maximumerror reduction of 69.8% and average error re-duction across all test sets of 59.1% comparedto the state-of-the-art and a maximum error re-duction of 68.4% and average error reductionacross all test sets of 41.8% compared to ourGoogle n-gram count baseline.1 IntroductionSpeakers rarely have difficulty correctly orderingmodifiers such as adjectives, adverbs, or gerundswhen describing some noun.
The phrase ?beau-tiful blue Macedonian vase?
sounds very natural,whereas changing the modifier ordering to ?blueMacedonian beautiful vase?
is awkward (see Table1 for more examples).
In this work, we considerthe task of ordering an unordered set of prenomi-nal modifiers so that they sound fluent to native lan-guage speakers.
This is an important task for naturallanguage generation systems.Much linguistic research has investigated the se-mantic constraints behind prenominal modifier or-derings.
One common line of research suggeststhat modifiers can be organized by the underlyingsemantic property they describe and that there isa.
the vegetarian French lawyerb.
the French vegetarian lawyera.
the beautiful small black purseb.
the beautiful black small pursec.
the small beautiful black pursed.
the small black beautiful purseTable 1: Examples of restrictions on modifier orderingsfrom Teodorescu (2006).
The most natural sounding or-dering is in bold, followed by other possibilities that mayonly be appropriate in certain situations.an ordering on semantic properties which in turnrestricts modifier orderings.
For instance, Sproatand Shih (1991) contend that the size property pre-cedes the color property and thus ?small black cat?sounds more fluent than ?black small cat?.
Using> to denote precedence of semantic groups, somecommonly proposed orderings are: quality > size> shape > color > provenance (Sproat and Shih,1991), age > color > participle > provenance >noun > denominal (Quirk et al, 1974), and value> dimension > physical property > speed > humanpropensity > age > color (Dixon, 1977).
However,correctly classifying modifiers into these groups canbe difficult and may be domain dependent or con-strained by the context in which the modifier is beingused.
In addition, these methods do not specify howto order modifiers within the same class or modifiersthat do not fit into any of the specified groups.There have also been a variety of corpus-based,computational approaches.
Mitchell (2009) uses1109a class-based approach in which modifiers aregrouped into classes based on which positions theyprefer in the training corpus, with a predefined or-dering imposed on these classes.
Shaw and Hatzi-vassiloglou (1999) developed three different ap-proaches to the problem that use counting methodsand clustering algorithms, and Malouf (2000) ex-pands upon Shaw and Hatzivassiloglou?s work.This paper describes a computational solution tothe problem that uses relevant features to model themodifier ordering process.
By mapping a set offeatures across the training data and using a maxi-mum entropy reranking model, we can learn optimalweights for these features and then order each set ofmodifiers in the test data according to our featuresand the learned weights.
This approach has not beenused before to solve the prenominal modifier order-ing problem, and as we demonstrate, vastly outper-forms the state-of-the-art, especially for sequencesof longer lengths.Section 2 of this paper describes previous compu-tational approaches.
In Section 3 we present the de-tails of our maximum entropy reranking approach.Section 4 covers the evaluation methods we used,and Section 5 presents our results.
In Section 6 wecompare our approach to previous methods, and inSection 7 we discuss future work and improvementsthat could be made to our system.2 Related WorkMitchell (2009) orders sequences of at most 4 mod-ifiers and defines nine classes that express the broadpositional preferences of modifiers, where position1 is closest to the noun phrase (NP) head and posi-tion 4 is farthest from it.
Classes 1 through 4 com-prise those modifiers that prefer only to be in posi-tions 1 through 4, respectively.
Class 5 through 7modifiers prefer positions 1-2, 2-3, and 3-4, respec-tively, while class 8 modifiers prefer positions 1-3,and finally, class 9 modifiers prefer positions 2-4.Mitchell counts how often each word type appears ineach of these positions in the training corpus.
If anymodifier?s probability of taking a certain position isgreater than a uniform distribution would allow, thenit is said to prefer that position.
Each word type isthen assigned a class, with a global ordering definedover the nine classes.Given a set of modifiers to order, if the entireset has been seen at training time, Mitchell?s sys-tem looks up the class of each modifier and then or-ders the sequence based on the predefined orderingfor the classes.
When two modifiers have the sameclass, the system picks between the possibilities ran-domly.
If a modifier was not seen at training timeand thus cannot be said to belong to a specific class,the system favors orderings where modifiers whoseclasses are known are as close to their classes?
pre-ferred positions as possible.Shaw and Hatzivassiloglou (1999) use corpus-based counting methods as well.
For a corpus withw word types, they define a w ?
w matrix whereCount[A,B] indicates how often modifier A pre-cedes modifier B.
Given two modifiers a and b toorder, they compare Count[a, b] and Count[b, a] intheir training data.
Assuming a null hypothesis thatthe probability of either ordering is 0.5, they use abinomial distribution to compute the probability ofseeing the ordering < a, b > for Count[a, b] num-ber of times.
If this probability is above a certainthreshold then they say that a precedes b. Shaw andHatzivassiloglou also use a transitivity method to fillout parts of the Count table where bigrams are notactually seen in the training data but their counts canbe inferred from other entries in the table, and theyuse a clustering method to group together modifierswith similar positional preferences.These methods have proven to work well, but theyalso suffer from sparsity issues in the training data.Mitchell reports a prediction accuracy of 78.59%for NPs of all lengths, but the accuracy of her ap-proach is greatly reduced when two modifiers fallinto the same class, since the system cannot makean informed decision in those cases.
In addition, if amodifier is not seen in the training data, the systemis unable to assign it a class, which also limits accu-racy.
Shaw and Hatzivassiloglou report a highest ac-curacy of 94.93% and a lowest accuracy of 65.93%,but since their methods depend heavily on bigramcounts in the training corpus, they are also limited inhow informed their decisions can be if modifiers inthe test data are not present at training time.In this next section, we describe our maximumentropy reranking approach that tries to develop amore comprehensive model of the modifier orderingprocess to avoid the sparsity issues that previous ap-1110proaches have faced.3 ModelWe treat the problem of prenominal modifier or-dering as a reranking problem.
Given a set B ofprenominal modifiers and a noun phrase head HwhichB modifies, we define ?
(B) to be the set of allpossible permutations, or orderings, of B.
We sup-pose that for a set B there is some x?
?
?
(B) whichrepresents a ?correct?
natural-sounding ordering ofthe modifiers in B.At test time, we choose an ordering x ?
?
(B) us-ing a maximum entropy reranking approach (Collinsand Koo, 2005).
Our distribution over orderingsx ?
?
(B) is given by:P (x|H,B,W ) =exp{W T?
(B,H, x)}?x???
(B) exp{W T?
(B,H, x?
)}where ?
(B,H, x) is a feature vector over a particu-lar ordering of B and W is a learned weight vectorover features.
We describe the set of features in sec-tion 3.1, but note that we are free under this formu-lation to use arbitrary features on the full ordering xof B as well as the head noun H , which we implic-itly condition on throughout.
Since the size of theset of prenominal modifiers B is typically less thansix, enumerating ?
(B) is not expensive.At training time, our data consists of sequences ofprenominal orderings and their corresponding nom-inal heads.
We treat each sequence as a training ex-ample where the labeled ordering x?
?
?
(B) is theone we observe.
This allows us to extract any num-ber of ?labeled?
examples from part-of-speech text.Concretely, at training time, we select W to maxi-mize:L(W ) =???(B,H,x?
)P (x?|H,B,W )???
?W?22?2where the first term represents our observed datalikelihood and the second the ?2 regularization,where ?2 is a fixed hyperparameter; we fix the valueof ?2 to 0.5 throughout.
We optimize this objectiveusing standard L-BFGS optimization techniques.The key to the success of our approach is us-ing the flexibility afforded by having arbitrary fea-tures ?
(B,H, x) to capture all the salient elementsof the prenominal ordering data.
These features canbe used to create a richer model of the modifier or-dering process than previous corpus-based countingapproaches.
In addition, we can encapsulate previ-ous approaches in terms of features in our model.Mitchell?s class-based approach can be expressed asa binary feature that tells us whether a given permu-ation satisfies the class ordering constraints in hermodel.
Previous counting approaches can be ex-pressed as a real-valued feature that, given all n-grams generated by a permutation of modifiers, re-turns the count of all these n-grams in the originaltraining data.3.1 Feature SelectionOur features are of the form ?
(B,H, x) as expressedin the model above, and we include both indica-tor features and real-valued numeric features in ourmodel.
We attempt to capture aspects of the modifierpermutations that may be significant in the orderingprocess.
For instance, perhaps the majority of wordsthat end with -ly are adverbs and should usually bepositioned farthest from the head noun, so we candefine an indicator function that captures this featureas follows:?
(B,H, x) =??
?1 if the modifier in position iof ordering x ends in -ly0 otherwiseWe create a feature of this form for every possiblemodifier position i from 1 to 4.Wemight also expect permutations that contain n-grams previously seen in the training data to be morenatural sounding than other permutations that gener-ate n-grams that have not been seen before.
We canexpress this as a real-valued feature:?
(B,H, x) =?count in training data of alln-grams present in xSee Table 2 for a summary of our features.
Manyof the features we use are similar to those in Dunlopet al (2010), which uses a feature-based multiple se-quence alignment approach to order modifiers.1111Numeric Featuresn-gram Count If N is the set of all n-grams present in the permutation, returnsthe sum of the counts of each element of N in the training data.A separate feature is created for 2-gms through 5-gms.Count of Head Noun and Closest Modifier Returns the count of < M,H > in the training data where H isthe head noun and M is the modifier closest to H .Length of Modifier?
Returns the length of modifier in position iIndicator FeaturesHyphenated?
Modifier in position i contains a hyphen.Is Word w?
Modifier in position i is word w ?
W , where W is the set of allword types in the training data.Ends In e?
Modifier in position i ends in suffix e ?
E, where E = {-al -ble-ed -er -est -ic -ing -ive -ly -ian}Is A Color?
Modifier in position i is a color, where we use a list of commoncolorsStarts With a Number?
Modifier in position i starts with a numberIs a Number?
Modifier in position i is a numberSatisfies Mitchell Class Ordering The permutation?s class ordering satisfies the Mitchell class or-dering constraintsTable 2: Features Used In Our Model.
Features with an asterisk (*) are created for all possible modifier positions ifrom 1 to 4.4 Experiments4.1 Data Preprocessing and SelectionWe extracted all noun phrases from four corpora: theBrown, Switchboard, and Wall Street Journal cor-pora from the Penn Treebank, and the North Amer-ican Newswire corpus (NANC).
Since there werevery few NPs with more than 5 modifiers, we keptthose with 2-5 modifiers and with tags NN or NNSfor the head noun.
We also kept NPs with only 1modifier to be used for generating <modifier, headnoun> bigram counts at training time.
We then fil-tered all these NPs as follows: If the NP containeda PRP, IN, CD, or DT tag and the correspondingmodifier was farthest away from the head noun, weremoved this modifier and kept the rest of the NP.
Ifthe modifier was not the farthest away from the headnoun, we discarded the NP.
If the NP contained aPOS tag we only kept the part of the phrase up to thistag.
Our final set of NPs had tags from the followinglist: JJ, NN, NNP, NNS, JJS, JJR, VBG, VBN, RB,NNPS, RBS.
See Table 3 for a summary of the num-ber of NPs of lengths 1-5 extracted from the fourcorpora.Our system makes several passes over the dataduring the training process.
In the first pass,we collect statistics about the data, to be usedlater on when calculating our numeric features.To collect the statistics, we take each NP inthe training data and consider all possible 2-gms through 5-gms that are present in the NP?smodifier sequence, allowing for non-consecutiven-grams.
For example, the NP ?the beautifulblue Macedonian vase?
generates the following bi-grams: <beautiful blue>, <blue Macedonian>,and <beautiful Macedonian>, along with the 3-gram <beautiful blue Macedonian>.
We keep atable mapping each unique n-gram to the numberof times it has been seen in the training data.
Inaddition, we also store a table that keeps track ofbigram counts for < M,H >, where H is thehead noun of an NP and M is the modifier clos-est to it.
In the example ?the beautiful blue Mace-donian vase,?
we would increment the count of <Macedonian, vase > in the table.
The n-gram and< M,H > counts are used to compute numeric fea-1112Number of Sequences (Token)1 2 3 4 5 TotalBrown 11,265 1,398 92 8 2 12,765WSJ 36,313 9,073 1,399 229 156 47,170Switchboard 10,325 1,170 114 4 1 11,614NANC 15,456,670 3,399,882 543,894 80,447 14,840 19,495,733Number of Sequences (Type)1 2 3 4 5 TotalBrown 4,071 1,336 91 8 2 5,508WSJ 7,177 6,687 1,205 182 42 15,293Switchboard 2,122 950 113 4 1 3,190NANC 241,965 876,144 264,503 48,060 8,451 1,439,123Table 3: Number of NPs extracted from our data for NP sequences with 1 to 5 modifiers.ture values.4.2 Google n-gram BaselineThe Google n-gram corpus is a collection of n-gramcounts drawn from public webpages with a total ofone trillion tokens ?
around 1 billion each of unique3-grams, 4-grams, and 5-grams, and around 300,000unique bigrams.
We created a Google n-gram base-line that takes a set of modifiers B, determines theGoogle n-gram count for each possible permutationin ?
(B), and selects the permutation with the high-est n-gram count as the winning ordering x?.
Wewill refer to this baseline as GOOGLE N-GRAM.4.3 Mitchell?s Class-Based Ordering ofPrenominal Modifiers (2009)Mitchell?s original system was evaluated using onlythree corpora for both training and testing data:Brown, Switchboard, and WSJ.
In addition, theevaluation presented by Mitchell?s work considers aprediction to be correct if the ordering of classes inthat prediction is the same as the ordering of classesin the original test data sequence, where a classrefers to the positional preference groupings definedin the model.
We use a more stringent evaluation asdescribed in the next section.We implemented our own version of Mitchell?ssystem that duplicates the model and methods butallows us to scale up to a larger training set and toapply our own evaluation techniques.
We will referto this baseline as CLASS BASED.4.4 EvaluationTo evaluate our system (MAXENT) and our base-lines, we partitioned the corpora into training andtesting data.
For each NP in the test data, we gener-ated a set of modifiers and looked at the predictedorderings of the MAXENT, CLASS BASED, andGOOGLE N-GRAM methods.
We considered a pre-dicted sequence ordering to be correct if it matchesthe original ordering of the modifiers in the corpus.We ran four trials, the first holding out the Browncorpus and using it as the test set, the second hold-ing out the WSJ corpus, the third holding out theSwitchboard corpus, and the fourth holding out arandomly selected tenth of the NANC.
For each trialwe used the rest of the data as our training set.5 ResultsThe MAXENT model consistently outperformsCLASS BASED across all test corpora and sequencelengths for both tokens and types, except when test-ing on the Brown and Switchboard corpora for mod-ifier sequences of length 5, for which neither ap-proach is able to make any correct predictions.
How-ever, there are only 3 sequences total of length 5in the Brown and Swichboard corpora combined.1113Test Corpus Token Accuracy (%) Type Accuracy (%)2 3 4 5 Total 2 3 4 5 TotalBrown GOOGLE N-GRAM 82.4 35.9 12.5 0 79.1 81.8 36.3 12.5 0 78.4CLASS BASED 79.3 54.3 25.0 0 77.3 78.9 54.9 25.0 0 77.0MAXENT 89.4 70.7 87.5 0 88.1 89.1 70.3 87.5 0 87.8WSJ GOOGLE N-GRAM 84.8 53.5 31.4 71.8 79.4 82.6 49.7 23.1 16.7 76.0CLASS BASED 85.5 51.6 16.6 0.6 78.5 85.1 50.1 19.2 0 78.0MAXENT 95.9 84.1 71.2 80.1 93.5 94.7 81.9 70.3 45.2 92.0Switchboard GOOGLE N-GRAM 92.8 68.4 0 0 90.3 91.7 68.1 0 0 88.8CLASS BASED 80.1 52.6 0 0 77.3 79.1 53.1 0 0 75.9MAXENT 91.4 74.6 25.0 0 89.6 90.3 75.2 25.0 0 88.4One Tenth of GOOGLE N-GRAM 86.8 55.8 27.7 43.0 81.1 79.2 44.6 20.5 12.3 70.4NANC CLASS BASED 86.1 54.7 20.1 1.9 80.0 80.3 51.0 18.4 3.3 74.5MAXENT 95.2 83.8 71.6 62.2 93.0 91.6 78.8 63.8 44.4 88.0Test Corpus Number of Features Used In MaxEnt ModelBrown 655,536WSJ 654,473Switchboard 655,791NANC 565,905Table 4: Token and type prediction accuracies for the GOOGLE N-GRAM, MAXENT, and CLASS BASED approachesfor modifier sequences of lengths 2-5.
Our data consisted of four corpuses: Brown, Switchboard, WSJ, and NANC.The test data was held out and each approach was trained on the rest of the data.
Winning scores are in bold.
Thenumber of features used during training for the MAXENT approach for each test corpus is also listed.MAXENT also outperforms the GOOGLE N-GRAMbaseline for almost all test corpora and sequencelengths.
For the Switchboard test corpus tokenand type accuracies, the GOOGLE N-GRAM base-line is more accurate than MAXENT for sequencesof length 2 and overall, but the accuracy of MAX-ENT is competitive with that of GOOGLE N-GRAM.If we examine the error reduction between MAX-ENT and CLASS BASED, we attain a maximum errorreduction of 69.8% for the WSJ test corpus acrossmodifier sequence tokens, and an average error re-duction of 59.1% across all test corpora for tokens.MAXENT also attains a maximum error reduction of68.4% for the WSJ test corpus and an average errorreduction of 41.8% when compared to GOOGLE N-GRAM.It should also be noted that on average the MAX-ENT model takes three hours to train with severalhundred thousand features mapped across the train-ing data (the exact number used during each test runis listed in Table 4) ?
this tradeoff is well worth theincrease we attain in system performance.6 AnalysisMAXENT seems to outperform the CLASS BASEDbaseline because it learns more from the trainingdata.
The CLASS BASED model classifies eachmodifier in the training data into one of nine broadcategories, with each category representing a differ-ent set of positional preferences.
However, many ofthe modifiers in the training data get classified to thesame category, and CLASS BASED makes a randomchoice when faced with orderings of modifiers all inthe same category.
When applying CLASS BASED11140 20 40 60 80 1000102030405060708090100 Sequences of 2 ModifiersPortion of NANC Used in Training (%)Correct Predictions(%)MaxEntClassBased(a)0 20 40 60 80 1000102030405060708090100 Sequences of 3 ModifiersPortion of NANC Used in Training (%)Correct Predictions(%)MaxEntClassBased(b)0 20 40 60 80 1000102030405060708090100 Sequences of 4 ModifiersPortion of NANC Used in Training (%)Correct Predictions(%)MaxEntClassBased(c)0 20 40 60 80 1000102030405060708090100 Sequences of 5 ModifiersPortion of NANC Used in Training (%)Correct Predictions(%)MaxEntClassBased(d)0 20 40 60 80 1000102030405060708090100 All Modifier SequencesPortion of NANC Used in Training (%)Correct Predictions(%)MaxEntClassBased(e)0 20 40 60 80 10001234567 x 105 Features Used by MaxEnt ModelPortion of NANC Used in Training (%)Number of FeaturesUsed(f)Figure 1: Learning curves for the MAXENT and CLASS BASED approaches.
We start by training each approach onjust the Brown and Switchboard corpora while testing on WSJ.
We incrementally add portions of the NANC corpus.Graphs (a) through (d) break down the total correct predictions by the number of modifiers in a sequence, while graph(e) gives accuracies over modifier sequences of all lengths.
Prediction percentages are for sequence tokens.
Graph (f)shows the number of features active in the MaxEnt model as the training data scales up.1115to WSJ as the test data and training on the other cor-pora, 74.7% of the incorrect predictions containedat least 2 modifiers that were of the same positionalpreferences class.
In contrast, MAXENT allows usto learn much more from the training data.
As a re-sult, we see much higher numbers when trained andtested on the same data as CLASS BASED.The GOOGLE N-GRAM method does better thanthe CLASS BASED approach because it contains n-gram counts for more data than the WSJ, Brown,Switchboard, and NANC corpora combined.
How-ever, GOOGLE N-GRAM suffers from sparsity issuesas well when testing on less common modifier com-binations.
For example, our data contains rarelyheard sequences such as ?Italian, state-owned, hold-ing company?
or ?armed Namibian nationalist guer-rillas.?
While MAXENT determines the correct or-dering for both of these examples, none of the per-mutations of either example show up in the Googlen-gram corpus, so the GOOGLE N-GRAM method isforced to randomly select from the six possibilities.In addition, the Google n-gram corpus is composedof sentence fragments that may not necessarily beNPs, so we may be overcounting certain modifierpermutations that can function as different parts of asentence.We also compared the effect that increasing theamount of training data has when using the CLASSBASED and MAXENT methods by initially train-ing each system with just the Brown and Switch-board corpora and testing on WSJ.
Then we incre-mentally added portions of NANC, one tenth at atime, until the training set included all of it.
The re-sults (see Figure 1) show that we are able to benefitfrom the additional data much more than the CLASSBASED approach can, since we do not have a fixedset of classes limiting the amount of information themodel can learn.
In addition, adding the first tenthof NANC made the biggest difference in increasingaccuracy for both approaches.7 ConclusionThe straightforward maximum entropy rerankingapproach is able to significantly outperform previouscomputational approaches by allowing for a richermodel of the prenominal modifier ordering process.Future work could include adding more features tothe model and conducting ablation testing.
In addi-tion, while many sets of modifiers have stringent or-dering requirements, some variations on orderings,such as ?former famous actor?
vs. ?famous formeractor,?
are acceptable in both forms and have dif-ferent meanings.
It may be beneficial to extend themodel to discover these ambiguities.AcknowledgementsMany thanks to Margaret Mitchell, Regina Barzilay, Xiao Chen,and members of the CSAIL NLP group for their help and sug-gestions.ReferencesM.
Collins and T. Koo.
2005.
Discriminative rerankingfor natural language parsing.
Computational Linguis-tics, 31(1):25?70.R.
M. W. Dixon.
1977.
Where Have all the AdjectivesGone?
Studies in Language, 1(1):19?80.A.
Dunlop, M. Mitchell, and B. Roark.
2010.
Prenomi-nal modifier ordering via multiple sequence alignment.In Human Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 600?608.
Association for Computational Linguistics.R.
Malouf.
2000.
The order of prenominal adjectivesin natural language generation.
In Proceedings ofthe 38th Annual Meeting on Association for Computa-tional Linguistics, pages 85?92.
Association for Com-putational Linguistics.M.
Mitchell.
2009.
Class-based ordering of prenominalmodifiers.
In Proceedings of the 12th European Work-shop on Natural Language Generation, pages 50?57.Association for Computational Linguistics.R.
Quirk, S. Greenbaum, R.A. Close, and R. Quirk.
1974.A university grammar of English, volume 1985.
Long-man London.J.
Shaw and V. Hatzivassiloglou.
1999.
Ordering amongpremodifiers.
In Proceedings of the 37th annual meet-ing of the Association for Computational Linguisticson Computational Linguistics, pages 135?143.
Asso-ciation for Computational Linguistics.R.
Sproat and C. Shih.
1991.
The cross-linguistic dis-tribution of adjective ordering restrictions.
Interdisci-plinary approaches to language, pages 565?593.A.
Teodorescu.
2006.
Adjective Ordering RestrictionsRevisited.
In Proceedings of the 25th West Coast Con-ference on Formal Linguistics, pages 399?407.
WestCoast Conference on Formal Linguistics.1116
