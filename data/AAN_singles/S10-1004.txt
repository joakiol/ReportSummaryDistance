Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 21?26,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSemEval-2010 Task 5: Automatic Keyphrase Extraction from ScientificArticlesSu Nam Kim,?
Olena Medelyan,?
Min-Yen Kan?
and Timothy Baldwin??
Dept of Computer Science and Software Engineering, University of Melbourne, Australia?
Pingar LP, Auckland, New Zealand?
School of Computing, National University of Singapore, Singaporesunamkim@gmail.com, medelyan@gmail.com,kanmy@comp.nus.edu.sg, tb@ldwin.netAbstractThis paper describes Task 5 of theWorkshop on Semantic Evaluation 2010(SemEval-2010).
Systems are to automat-ically assign keyphrases or keywords togiven scientific articles.
The participatingsystems were evaluated by matching theirextracted keyphrases against manually as-signed ones.
We present the overall rank-ing of the submitted systems and discussour findings to suggest future directionsfor this task.1 Task DescriptionKeyphrases1are words that capture the main top-ics of a document.
As they represent these keyideas, extracting high-quality keyphrases can ben-efit various natural language processing (NLP) ap-plications such as summarization, information re-trieval and question-answering.
In summariza-tion, keyphrases can be used as a form of se-mantic metadata (Barzilay and Elhadad, 1997;Lawrie et al, 2001; D?Avanzo and Magnini,2005).
In search engines, keyphrases can supple-ment full-text indexing and assist users in formu-lating queries.Recently, a resurgence of interest in keyphraseextraction has led to the development of severalnew systems and techniques for the task (Franket al, 1999; Witten et al, 1999; Turney, 1999;Hulth, 2003; Turney, 2003; Park et al, 2004;Barker and Corrnacchia, 2000; Hulth, 2004; Mat-suo and Ishizuka, 2004; Mihalcea and Tarau,2004; Medelyan and Witten, 2006; Nguyen andKan, 2007; Wan and Xiao, 2008; Liu et al, 2009;Medelyan, 2009; Nguyen and Phan, 2009).
These1We use ?keyphrase?
and ?keywords?
interchangeably torefer to both single words and phrases.
?Min-Yen Kan?s work was funded by National ResearchFoundation grant ?Interactive Media Search?
(grant # R-252-000-325-279).have showcased the potential benefits of keyphraseextraction to downstream NLP applications.In light of these developments, we felt that thiswas an appropriate time to conduct a shared taskfor keyphrase extraction, to provide a standard as-sessment to benchmark current approaches.
A sec-ond goal of the task was to contribute an additionalpublic dataset to spur future research in the area.Currently, there are several publicly availabledata sets.2For example, Hulth (2003) contributed2,000 abstracts of journal articles present in In-spec between the years 1998 and 2002.
The dataset contains keyphrases (i.e.
controlled and un-controlled terms) assigned by professional index-ers ?
1,000 for training, 500 for validation and500 for testing.
Nguyen and Kan (2007) col-lected a dataset containing 120 computer sciencearticles, ranging in length from 4 to 12 pages.The articles contain author-assigned keyphrasesas well as reader-assigned keyphrases contributedby undergraduate CS students.
In the generalnewswire domain, Wan and Xiao (2008) devel-oped a dataset of 308 documents taken from DUC2001 which contain up to 10 manually-assignedkeyphrases per document.
Several databases, in-cluding the ACM Digital Library, IEEE Xplore,Inspec and PubMed provide articles with author-assigned keyphrases and, occasionally, reader-assigned ones.
Medelyan (2009) automaticallygenerated a dataset using tags assigned by theusers of the collaborative citation platform CiteU-Like.
This dataset additionally records how manypeople have assigned the same keyword to thesame publication.
In total, 180 full-text publi-cations were annotated by over 300 users.3De-spite the availability of these datasets, a standard-ized benchmark dataset with a well-defined train-2All data sets listed below are available fordownload from http://github.com/snkim/AutomaticKeyphraseExtraction3http://bit.ly/maui-datasets21ing and test split is needed to maximize compara-bility of results.For the SemEval-2010 Task 5, we havecompiled a set of 284 scientific articles withkeyphrases carefully chosen by both their authorsand readers.
The participants?
task was to developsystems which automatically produce keyphrasesfor each paper.
Each team was allowed to sub-mit up to three system runs, to benchmark thecontributions of different parameter settings andapproaches.
Each run consisted of extracting aranked list of 15 keyphrases from each docu-ment, ranked by their probability of being reader-assigned keyphrases.In the remainder of the paper, we describethe competition setup, including how data collec-tion was managed and the evaluation methodol-ogy (Section 2).
We present the results of theshared task, and discuss the immediate findings ofthe competition in Section 3.
In Section 4 we as-sess the human performance by comparing reader-assigned keyphrases to those assigned by the au-thors.
This gives an approximation of an upper-bound performance for this task.2 Competition Setup2.1 DataWe collected trial, training and test data fromthe ACM Digital Library (conference and work-shop papers).
The input papers ranged from 6to 8 pages, including tables and pictures.
To en-sure a variety of different topics was representedin the corpus, we purposefully selected papersfrom four different research areas for the dataset.In particular, the selected articles belong to thefollowing four 1998 ACM classifications: C2.4(Distributed Systems), H3.3 (Information Searchand Retrieval), I2.11 (Distributed Artificial In-telligence ?
Multiagent Systems) and J4 (Socialand Behavioral Sciences ?
Economics).
All threedatasets (trial, training and test) had an equal dis-tribution of documents from among the categories(see Table 1).
This domain specific informationwas provided with the papers (e.g.
I2.4-1 or H3.3-2), in case participant systems wanted to utilizethis information.
We specifically decided to strad-dle different areas to see whether participant ap-proaches would work better within specific areas.Participants were provided with 40, 144, and100 articles, respectively, in the trial, training andtest data, distributed evenly across the four re-search areas in each case.
Note that the trial data isa subset of the training data.
Since the original for-mat for the articles was PDF, we converted theminto (UTF-8) plain text using pdftotext, and sys-tematically restored full words that were originallyhyphenated and broken across two lines.
This pol-icy potentially resulted in valid hyphenated formshaving their hyphen (-) removed.All collected papers contain author-assignedkeyphrases, part of the original PDF file.
We addi-tionally collected reader-assigned keyphrases foreach paper.
We first performed a pilot annotationtask with a group of students to check the stabil-ity of the annotations, finalize the guidelines, anddiscover and resolve potential issues that may oc-cur during the actual annotation.
To collect the ac-tual reader-assigned keyphrases, we then hired 50student annotators from the Computer Science de-partment of the National University of Singapore.We assigned 5 papers to each annotator, esti-mating that assigning keyphrases to each papershould take about 10-15 minutes.
Annotators wereexplicitly told to extract keyphrases that actuallyappear in the text of each paper, rather than to cre-ate semantically-equivalent phrases, but could ex-tract phrases from any part of the document (in-cluding headers and captions).
In reality, on av-erage 15% of the reader-assigned keyphrases didnot appear in the text of the paper, but this is stillless than the 19% of author-assigned keyphrasesthat did not appear in the papers.
These valueswere computed using the test documents only.
Inother words, the maximum recall that the partici-pating systems can achieve on these documents is85% and 81% for the reader- and author-assignedkeyphrases, respectively.As some keyphrases may occur in multipleforms, in our evaluation we accepted two differ-ent versions of genitive keyphrases: A of B ?
BA (e.g.
policy of school = school policy) and A?sB ?
A B (e.g.
school?s policy = school pol-icy).
In certain cases, such alternations change thesemantics of the candidate phrase (e.g., matter offact vs. ?fact matter).
We judged borderline casesby committee and do not include alternations thatwere judged to be semantically distinct.Table 1 shows the distribution of the trial, train-ing and test documents over the four different re-search areas, while Table 2 shows the distributionof author- and reader-assigned keyphrases.Interestingly, among the 387 author-assigned22Dataset Total Document TopicC H I JTrial 40 10 10 10 10Training 144 34 39 35 36Test 100 25 25 25 25Table 1: Number of documents per topic in thetrial, training and test datasets, across the fourACM document classificationsDataset Author Reader CombinedTrial 149 526 621Training 559 1824 2223Test 387 1217 1482Table 2: Number of author- and reader-assignedkeyphrases in the different datasetskeywords, 125 keywords match exactly withreader-assigned keywords, while many more near-misses (i.e.
partial matches) occur.2.2 Evaluation Method and BaselineTraditionally, automatic keyphrase extraction sys-tems have been assessed using the proportion oftop-N candidates that exactly match the gold-standard keyphrases (Frank et al, 1999; Witten etal., 1999; Turney, 1999).
In some cases, inexactmatches, or near-misses, have also been consid-ered.
Some have suggested treating semantically-similar keyphrases as correct based on simi-larities computed over a large corpus (Jarmaszand Barriere, 2004; Mihalcea and Tarau, 2004),or using semantic relations defined in a the-saurus (Medelyan and Witten, 2006).
Zesch andGurevych (2009) compute near-misses using an n-gram based approach relative to the gold standard.For our shared task, we follow the traditional ex-act match evaluation metric.
That is, we match thekeyphrases in the answer set with those the sys-tems provide, and calculate micro-averaged preci-sion, recall and F-score (?
= 1).
In the evaluation,we check the performance over the top 5, 10 and15 candidates returned by each system.
We rankthe participating systems by F-score over the top15 candidates.Participants were required to extract ex-isting phrases from the documents.
Sinceit is theoretically possible to retrieve author-assigned keyphrases from the original PDF arti-cles, we evaluate the participating systems overthe independently-generated and held-out reader-assigned keyphrases, as well as the combined setof keyphrases (author- and reader-assigned).All keyphrases in the answer set are stemmedusing the English Porter stemmer for both thetraining and test dataset.4We computed a TF?IDF n-gram based baselineusing both supervised and unsupervised learningsystems.
We use 1, 2, 3-grams as keyphrase can-didates, used Na?
?ve Bayes (NB) and MaximumEntropy (ME) classifiers to learn two supervisedbaseline systems based on the keyphrase candi-dates and gold-standard annotations for the train-ing documents.
In total, there are three baselines:two supervised and one unsupervised.
The per-formance of the baselines is presented in Table 3,where R indicates reader-assigned keyphrases andC indicates combined (both author- and reader-assigned) keyphrases.3 Competition ResultsThe trial data was downloaded by 73 differentteams, of which 36 teams subsequently down-loaded the training and test data.
21 teams partici-pated in the final competition, of which two teamswithdrew their systems.Table 4 shows the performance of the final 19submitted systems.
5 teams submitted one run,6 teams submitted two runs and 8 teams sub-mitted the maximum number of three runs.
Werank the best-performing system from each teamby micro-averaged F-score over the top 15 can-didates.
We also show system performance overreader-assigned keywords in Table 5, and overauthor-assigned keywords in Table 6.
In all thesetables, P, R and F denote precision, recall and F-score, respectively.The best results over the reader-assigned andcombined keyphrase sets are 23.5% and 27.5%,respectively, achieved by the HUMB team.
Mostsystems outperformed the baselines.
Systems alsogenerally did better over the combined set, as thepresence of a larger gold-standard answer set im-proved recall.In Tables 7 and 8, we ranked the teams by F-score, computed over the top 15 candidates foreach of the four ACM document classifications.The numbers in brackets are the actual F-scores4Using the Perl implementation available at http://tartarus.org/?martin/PorterStemmer/; we in-formed participants that this was the stemmer we would beusing for the task, to avoid possible stemming variations be-tween implementations.23Method Top 5 candidates Top 10 candidates Top 15 candidatesby P R F P R F P R FTF?IDF R 17.8% 7.4% 10.4% 13.9% 11.5% 12.6% 11.6% 14.5% 12.9%C 22.0% 7.5% 11.2% 17.7% 12.1% 14.4% 14.9% 15.3% 15.1%NB R 16.8% 7.0% 9.9% 13.3% 11.1% 12.1% 11.4% 14.2% 12.7%C 21.4% 7.3% 10.9% 17.3% 11.8% 14.0% 14.5% 14.9% 14.7%ME R 16.8% 7.0% 9.9% 13.3% 11.1% 12.1% 11.4% 14.2% 12.7%C 21.4% 7.3% 10.9% 17.3% 11.8% 14.0% 14.5% 14.9% 14.7%Table 3: Baseline keyphrase extraction performance for one unsupervised (TF?IDF) and two supervised(NB and ME) systemsSystem Rank Top 5 candidates Top 10 candidates Top 15 candidatesP R F P R F P R FHUMB 1 39.0% 13.3% 19.8% 32.0% 21.8% 26.0% 27.2% 27.8% 27.5%WINGNUS 2 40.2% 13.7% 20.5% 30.5% 20.8% 24.7% 24.9% 25.5% 25.2%KP-Miner 3 36.0% 12.3% 18.3% 28.6% 19.5% 23.2% 24.9% 25.5% 25.2%SZTERGAK 4 34.2% 11.7% 17.4% 28.5% 19.4% 23.1% 24.8% 25.4% 25.1%ICL 5 34.4% 11.7% 17.5% 29.2% 19.9% 23.7% 24.6% 25.2% 24.9%SEERLAB 6 39.0% 13.3% 19.8% 29.7% 20.3% 24.1% 24.1% 24.6% 24.3%KX FBK 7 34.2% 11.7% 17.4% 27.0% 18.4% 21.9% 23.6% 24.2% 23.9%DERIUNLP 8 27.4% 9.4% 13.9% 23.0% 15.7% 18.7% 22.0% 22.5% 22.3%Maui 9 35.0% 11.9% 17.8% 25.2% 17.2% 20.4% 20.3% 20.8% 20.6%DFKI 10 29.2% 10.0% 14.9% 23.3% 15.9% 18.9% 20.3% 20.7% 20.5%BUAP 11 13.6% 4.6% 6.9% 17.6% 12.0% 14.3% 19.0% 19.4% 19.2%SJTULTLAB 12 30.2% 10.3% 15.4% 22.7% 15.5% 18.4% 18.4% 18.8% 18.6%UNICE 13 27.4% 9.4% 13.9% 22.4% 15.3% 18.2% 18.3% 18.8% 18.5%UNPMC 14 18.0% 6.1% 9.2% 19.0% 13.0% 15.4% 18.1% 18.6% 18.3%JU CSE 15 28.4% 9.7% 14.5% 21.5% 14.7% 17.4% 17.8% 18.2% 18.0%LIKEY 16 29.2% 10.0% 14.9% 21.1% 14.4% 17.1% 16.3% 16.7% 16.5%UvT 17 24.8% 8.5% 12.6% 18.6% 12.7% 15.1% 14.6% 14.9% 14.8%POLYU 18 15.6% 5.3% 7.9% 14.6% 10.0% 11.8% 13.9% 14.2% 14.0%UKP 19 9.4% 3.2% 4.8% 5.9% 4.0% 4.8% 5.3% 5.4% 5.3%Table 4: Performance of the submitted systems over the combined author- and reader-assigned keywords,ranked by F-scorefor each team.
Note that in the case of a tie inF-score, we ordered teams by descending F-scoreover all the data.4 Discussion of the Upper-BoundPerformanceThe current evaluation is a testament to the gainsmade by keyphrase extraction systems.
The sys-tem performance over the different keyword cat-egories (reader-assigned and author-assigned) andnumbers of keyword candidates (top 5, 10 and 15candidates) attest to this fact.The top-performing systems return F-scores inthe upper twenties.
Superficially, this number islow, and it is instructive to examine how muchroom there is for improvement.
Keyphrase extrac-tion is a subjective task, and an F-score of 100% isinfeasible.
On the author-assigned keyphrases inour test collection, the highest a system could the-oretically achieve was 81% recall5and 100% pre-cision, which gives a maximum F-score of 89%.However, such a high value would only be possi-ble if the number of keyphrases extracted per doc-ument could vary; in our task, we fixed the thresh-olds at 5, 10 and 15 keyphrases.5The remaining 19% of keyphrases do not actually appearin the documents and thus cannot be extracted.Another way of computing the upper-boundperformance would be to look into how well peo-ple perform the same task.
We analyzed theperformance of our readers, taking the author-assigned keyphrases as the gold standard.
The au-thors assigned an average of 4 keyphrases to eachpaper, whereas the readers assigned 12 on average.These 12 keyphrases cover 77.8% of the authors?keyphrases, which corresponds to a precision of21.5%.
The F-score achieved by the readers on theauthor-assigned keyphrases is 33.6%, whereas theF-score of the best-performing system on the samedata is 19.3% (for top 15, not top 12 keyphrases,see Table 6).We conclude that there is definitely still roomfor improvement, and for any future shared tasks,we recommend against fixing any threshold on thenumber of keyphrases to be extracted per docu-ment.
Finally, as we use a strict exact matchingmetric for evaluation, the presented evaluation fig-ures are a lower bound for performance, as se-mantically equivalent keyphrases are not countedas correct.
For future runs of this challenge, webelieve a more semantically-motivated evaluationshould be employed to give a more accurate im-pression of keyphrase acceptability.24System Rank Top 5 candidates Top 10 candidates Top 15 candidatesP R F P R F P R FHUMB 1 30.4% 12.6% 17.8% 24.8% 20.6% 22.5% 21.2% 26.4% 23.5%KX FBK 2 29.2% 12.1% 17.1% 23.2% 19.3% 21.1% 20.3% 25.3% 22.6%SZTERGAK 3 28.2% 11.7% 16.6% 23.2% 19.3% 21.1% 19.9% 24.8% 22.1%WINGNUS 4 30.6% 12.7% 18.0% 23.6% 19.6% 21.4% 19.8% 24.7 22.0%ICL 5 27.2% 11.3% 16.0% 22.4% 18.6% 20.3% 19.5% 24.3% 21.6%SEERLAB 6 31.0% 12.9% 18.2% 24.1% 20.0% 21.9% 19.3% 24.1% 21.5%KP-Miner 7 28.2% 11.7% 16.5% 22.0% 18.3% 20.0% 19.3% 24.1% 21.5%DERIUNLP 8 22.2% 9.2% 13.0% 18.9% 15.7% 17.2% 17.5% 21.8% 19.5%DFKI 9 24.4% 10.1% 14.3% 19.8% 16.5% 18.0% 17.4% 21.7% 19.3%UNICE 10 25.0% 10.4% 14.7% 20.1% 16.7% 18.2% 16.0% 19.9% 17.8%SJTULTLAB 11 26.6% 11.1% 15.6% 19.4% 16.1% 17.6% 15.6% 19.4% 17.3%BUAP 12 10.4% 4.3% 6.1% 13.9% 11.5% 12.6% 14.9% 18.6% 16.6%Maui 13 25.0% 10.4% 14.7% 18.1% 15.0% 16.4% 14.9% 18.5% 16.1%UNPMC 14 13.8% 5.7% 8.1% 15.1% 12.5% 13.7% 14.5% 18.0% 16.1%JU CSE 15 23.4% 9.7% 13.7% 18.1% 15.0% 16.4% 14.4% 17.9% 16.0%LIKEY 16 24.6% 10.2% 14.4% 17.9% 14.9% 16.2% 13.8% 17.2% 15.3%POLYU 17 13.6% 5.7% 8.0% 12.6% 10.5% 11.4% 12.0% 14.9% 13.3%UvT 18 20.4% 8.5% 12.0% 15.6% 13.0% 14.2% 11.9% 14.9% 13.2%UKP 19 8.2% 3.4% 4.8% 5.3% 4.4% 4.8% 4.7% 5.8% 5.2%Table 5: Performance of the submitted systems over the reader-assigned keywords, ranked by F-scoreSystem Rank Top 5 candidates Top 10 candidates Top 15 candidatesP R F P R F P R FHUMB 1 21.2% 27.4% 23.9% 15.4% 39.8% 22.2% 12.1% 47.0% 19.3%KP-Miner 2 19.0% 24.6% 21.4% 13.4% 34.6% 19.3% 10.7% 41.6% 17.1%ICL 3 17.0% 22.0% 19.2% 13.5% 34.9% 19.5% 10.5% 40.6% 16.6%Maui 4 20.4% 26.4% 23.0% 13.7% 35.4% 19.8% 10.2% 39.5% 16.2%SEERLAB 5 18.8% 24.3% 21.2% 13.1% 33.9% 18.9% 10.1% 39.0% 16.0%SZTERGAK 6 14.6% 18.9% 16.5% 12.2% 31.5% 17.6% 9.9% 38.5% 15.8%WINGNUS 7 18.6% 24.0% 21.0% 12.6% 32.6% 18.2% 9.3% 36.2% 14.8%DERIUNLP 8 12.6% 16.3% 14.2% 9.7% 25.1% 14.0% 9.3% 35.9% 14.7%KX FBK 9 13.6% 17.6% 15.3% 10.0% 25.8% 14.4% 8.5% 32.8% 13.5%BUAP 10 5.6% 7.2% 6.3% 8.1% 20.9% 11.7% 8.3% 32.0% 13.2%JU CSE 11 12.0% 15.5% 13.5% 8.5% 22.0% 12.3% 7.5% 29.0% 11.9%UNPMC 12 7.0% 9.0% 7.9% 7.7% 19.9% 11.1% 7.1% 27.4% 11.2%DFKI 13 12.8% 16.5% 14.4% 8.5% 22.0% 12.3% 6.6% 25.6% 10.5%SJTULTLAB 14 9.6% 12.4% 10.8% 7.8% 20.2% 11.3% 6.2% 24.0% 9.9%Likey 15 11.6% 15.0% 13.1% 7.9% 20.4% 11.4% 5.9% 22.7% 9.3%UvT 16 11.4% 14.7% 12.9% 7.6% 19.6% 11.0% 5.8% 22.5% 9.2%UNICE 17 8.8% 11.4% 9.9% 6.4% 16.5% 9.2% 5.5% 21.5% 8.8%POLYU 18 3.8% 4.9% 4.3% 4.1% 10.6% 5.9% 4.1% 16.0% 6.6%UKP 19 1.6% 2.1% 1.8% 0.9% 2.3% 1.3% 0.8% 3.1% 1.3%Table 6: Performance of the submitted systems over the author-assigned keywords, ranked by F-score5 ConclusionThis paper has described Task 5 of the Workshopon Semantic Evaluation 2010 (SemEval-2010), fo-cusing on keyphrase extraction.
We outlined thedesign of the datasets used in the shared task andthe evaluation metrics, before presenting the offi-cial results for the task and summarising the im-mediate findings.
We also analyzed the upper-bound performance for this task, and demon-strated that there is still room for improvementover the task.
We look forward to future advancesin automatic keyphrase extraction based on thisand other datasets.ReferencesKen Barker and Nadia Corrnacchia.
Using nounphrase heads to extract document keyphrases.
InProceedings of BCCSCSI : Advances in Artificial In-telligence.
2000, pp.96?103.Regina Barzilay and Michael Elhadad.
Using lexi-cal chains for text summarization.
In Proceedingsof ACL/EACL Workshop on Intelligent Scalable TextSummarization.
1997, pp.
10?17.Ernesto D?Avanzo and Bernado Magnini.
AKeyphrase-Based Approach to Summarization: theLAKE System at DUC-2005.
In Proceedings ofDUC.
2005.Eibe Frank and Gordon W. Paynter and Ian H. Wittenand Carl Gutwin and Craig G. Nevill-Manning.
Do-main Specific Keyphrase Extraction.
In Proceedingsof IJCAI.
1999, pp.668?673.Annette Hulth.
Improved automatic keyword extrac-tion given more linguistic knowledge.
In Proceed-ings of EMNLP.
2003, 216?223.Annette Hulth.
Enhancing Linguistically OrientedAutomatic Keyword Extraction.
In Proceedings ofHLT/NAACL.
2004, pp.
17?20.Mario Jarmasz and Caroline Barriere.
Using semanticsimilarity over tera-byte corpus, compute the perfor-mance of keyphrase extraction.
In Proceedings ofCLINE.
2004.Dawn Lawrie and W. Bruce Croft and Arnold Rosen-berg.
Finding Topic Words for Hierarchical Summa-rization.
In Proceedings of SIGIR.
2001, pp.
349?357.25Rank Group C Group H Group I Group J1 HUMB(28.3%) HUMB(30.2%) HUMB(24.2%) HUMB(27.4%)2 ICL(27.2%) WINGNUS(28.9%) SEERLAB(24.2%) WINGNUS(25.4%)3 KP-Miner(25.5%) SEERLAB(27.8%) KP-Miner(22.8%) ICL(25.4%)4 SZTERGAK(25.3%) KP-Miner(27.6%) KX FBK(22.8%) SZTERGAK(25.17%)5 WINGNUS(24.2%) SZTERGAK(27.6%) WINGNUS(22.3%) KP-Miner(24.9%)6 KX FBK(24.2%) ICL(25.5%) SZTERGAK(22.25%) KX FBK(24.6%)7 DERIUNLP(23.6%) KX FBK(23.9%) ICL(21.4%) UNICE(23.5%)8 SEERLAB(22.0%) Maui(23.9%) DERIUNLP(20.1%) SEERLAB(23.3%)9 DFKI(21.7%) DERIUNLP(23.6%) DFKI(19.3%) DFKI(22.2%)10 Maui(19.3%) UNPMC(22.6%) BUAP(18.5%) Maui(21.3%)11 BUAP(18.5%) SJTULTLAB(22.1%) SJTULTLAB(17.9%) DERIUNLP(20.3%)12 JU CSE(18.2%) UNICE(21.8%) JU CSE(17.9%) BUAP(19.7%)13 Likey(18.2%) DFKI(20.5%) Maui(17.6%) JU CSE(18.6%)14 SJTULTLAB(17.7%) BUAP(20.2%) UNPMC(17.6%) UNPMC(17.8%)15 UvT(15.8%) UvT(20.2%) UNICE(14.7%) Likey(17.2%)16 UNPMC(15.2%) Likey(19.4%) Likey(11.3%) SJTULTLAB(16.7%)17 UNIC(14.3%) JU CSE(17.3%) POLYU(13.6%) POLYU(14.3%)18 POLYU(12.5%) POLYU(15.8%) UvT(10.3%) UvT(12.6%)19 UKP(4.4%) UKP(5.0%) UKP(5.4%) UKP(6.8%)Table 7: System ranking (and F-score) for each ACM classification: combined keywordsRank Group C Group H Group I Group J1 ICL(23.3%) HUMB(25.0%) HUMB(21.7%) HUMB(24.7%)2 KX FBK(23.3%) WINGNUS(23.5%) KX FBK(21.4%) WINGNUS(24.4%)3 HUMB(22.7%) SEERLAB(23.2%) SEERLAB(21.1%) SZTERGAK(24.4%)4 SZTERGAK(22.7%) KP-Miner(22.4%) WINGNUS(19.9%) KX FBK(24.4%)5 DERIUNLP(21.5%) SZTERGAK(21.8%) KP-Miner(19.6%) UNICE(23.8%)6 KP-Miner(21.2%) KX FBK(21.2%) SZTERGAK(19.6%) ICL(23.5%)7 WINGNUS(20.0%) ICL(20.1%) ICL(19.6%) KP-Miner(22.6%)8 SEERLAB(19.4%) DERIUNLP(20.1%) DFKI(18.5%) SEERLAB(22.0%)9 DFKI(19.4%) DFKI(19.5%) SJTULTLAB(17.6%) DFKI(21.7%)10 JU CSE(17.0%) SJTULTLAB(19.5%) DERIUNLP(17.3%) BUAP(19.6%)11 Likey(16.4%) UNICE(19.2%) JU CSE(16.7%) DERIUNLP(19.0%)12 SJTULTLAB(15.8%) Maui(18.1%) BUAP(16.4%) Maui(17.8%)13 BUAP(15.5%) UNPMC(18.1%) UNPMC(16.1%) JU CSE(17.9%)14 Maui(15.2%) Likey(16.9%) Maui(14.9%) Likey(17.5%)15 UNICE(14.0%) UvT(16.4%) UNICE(14.0%) UNPMC(16.6%)16 UvT(14.0%) POLYU(15.5%) POLYU(11.9%) SJTULTLAB(16.3%)17 UNPMC(13.4%) BUAP(14.9%) Likey(10.4%) POLYU(13.3%)18 POLYU(12.5%) JU CSE(12.6%) UvT(9.5%) UvT(13.0%)19 UKP(4.5%) UKP(4.3%) UKP(5.4%) UKP(6.9%)Table 8: System ranking (and F-score) for each ACM classification: reader-assigned keywordsZhiyuan Liu and Peng Li and Yabin Zheng and SunMaosong.
Clustering to Find Exemplar Terms forKeyphrase Extraction.
In Proceedings of EMNLP.2009, pp.
257?266.Yutaka Matsuo and Mitsuru Ishizuka.
Keyword Ex-traction from a Single Document using Word Co-occurrence Statistical Information.
InternationalJournal on Artificial Intelligence Tools.
2004, 13(1),pp.
157?169.Olena Medelyan and Ian H. Witten.
Thesaurus basedautomatic keyphrase indexing.
In Proceedings ofACM/IEED-CS JCDL.
2006, pp.
296?297.Olena Medelyan.
Human-competitive automatic topicindexing.
PhD Thesis.
University of Waikato.
2009.Rada Mihalcea and Paul Tarau.
TextRank: BringingOrder into Texts.
In Proceedings of EMNLP.
2004,pp.
404?411.Thuy Dung Nguyen and Min-Yen Kan. Key phraseExtraction in Scientific Publications.
In Proceed-ings of ICADL.
2007, pp.
317?326.Chau Q. Nguyen and Tuoi T. Phan.
An ontology-basedapproach for key phrase extraction.
In Proceedingsof the ACL-IJCNLP.
2009, pp.
181?184.Youngja Park and Roy J. Byrd and Branimir Bogu-raev.
Automatic Glossary Extraction Beyond Termi-nology Identification.
In Proceedings of COLING.2004, pp.
48?55.Peter Turney.
Learning to Extract Keyphrases fromText.
In National Research Council, Institute for In-formation Technology, Technical Report ERB-1057.1999.Peter Turney.
Coherent keyphrase extraction via Webmining.
In Proceedings of IJCAI.
2003, pp.
434?439.Xiaojun Wan and Jianguo Xiao.
CollabRank: to-wards a collaborative approach to single-documentkeyphrase extraction.
In Proceedings of COLING.2008, pp.
969?976.Ian H. Witten and Gordon Paynter and EibeFrank and Car Gutwin and Graig Nevill-Manning.KEA:Practical Automatic Key phrase Extraction.In Proceedings of ACM conference on Digital li-braries.
1999, pp.
254?256.Torsten Zesch and Iryna Gurevych.
ApproximateMatching for Evaluating Keyphrase Extraction.
InProceedings of RANLP.
2009.26
