RAIS INS ,  SULTANAS~ AND CURRANTS:  LEX ICALCLASS IF ICAT ION AND ABSTRACTION V IA  CONTEXT PR IMINGDavid J. HutchesDepartment of Computer Science and Engineering, Mail Code 0114University of California, San DiegoLa Jolla, CA 92093-0114dhutches@ucsd.eduAbst rac tIn this paper we discuss the results of experimentswhich use a context, essentially an ordered set oflexical items, as the seed from which to build anetwork representing statistically important rela-tionships among lexical items in some corpus.
Ametric is then applied to the nodes in the networkin order to discover those pairs of items related byhigh indices of similarity.
The goal of this researchis to instantiate a class of items corresponding toeach item in the priming context.
We believe thatthis instantiation process is ultimately a specialcase of abstraction over the entire network; in thisabstraction, similar nodes are collapsed into meta-nodes which may then function as if they were sin-gle lexical items.I.
Mot ivat ion  and  BackgroundWith respect to the processing of language,one of the tasks at which human beings seem rel-atively adept is the ability to determine when it isappropriate to make generalizations and when it isappropriate to preserve distinctions.
The processof abstraction and knowing when it might reason-ably be used is a necessary tool in reducing thecomplexity of the task of processing natural lan-guage.
Part of our current research is an investi-gation into how the process of abstraction mightbe realized using relatively low-level statistical in-formation extracted from large textual corpora.Our experiments are an attempt to discovera method by which class information about themembers of some sequence of lexical items maybe obtained using strictly statistical methods.
Forour purposes, the class to which a lexical item be-longs is defined by its instantiation.
Given somecontext such as he walked across  the room, wewould like to be able to instantiate classes of itemscorresponding to each item in the context (e.g., theclass associated with walked might include itemssuch as paced, s tepped,  or sauntered) .The corpora used in our experiments are theLancaster-Oslo-Bergen (LOB) corpus and a sub-set of the ACL/DCI  Wall Street Journal (WSJ)corpus.
The LOB corpus consists of a totalof 1,008,035 words, composed of 49,174 uniquewords.
The subset of the WSJ  corpus that weuse has been pre-processed such that all lettersare folded to lower case, and numbers have beencollapsed to a single token; the subset consists of18,188,548 total words and 159,713 unique words.I I .
Context  P r imingIt is not an uncommon notion that a wordmay be defined not rigourously as by the as-signment of static syntactic and semantic lasses,but dynamically as a function of its usage (Firth1957, 11).
Such usage may be derived from co-occurrence information over the course of a largebody of text.
For each unique lexical item in a cor-pus, there exists an "association eighbourhood"in which that item lives; such a neighbourhoodis the probability distribution of the words withwhich the item has co-occurred.
If one posits thatsimilar lexical items will have similar neighbour-hoods, one possible method of instantiating a classof lexical items would be to examine all uniqueitems in a corpus and find those whose neighbour-hoods are most similar to the neighbourhood ofthe item whose class is being instantiated.
How-ever, the potential computational problems of suchan approach are clear.
In the context of our ap-proach to this problem, most lexical items in thesearch space are not even remotely similar to theitem for which a class is being instantiated.
Fur-thermore, a substantial part of a lexical item's as-sociation neighbourhood provides only superficialinformation about that item.
What is requiredis a process whereby the search space is reduceddramatically.
One method of accomplishing thispruning is via context priming.In context priming, we view a context as theseed upon which to build a network describing thatpart of the corpus which is, in some sense, closeto the context.
Thus, just as an individual lexicalitem has associated with it a unique neighbour-hood, so too does a context have such a neigh-bourhood.
The basic process of building a net-work is straightforward.
Each item in the primingcontext has associated with it a unique neighbour-hood defined in terms of those lexical items withwhich it has co-occurred.
Similarly, each of these292latter items also has a unique association eigh-bourhood.
Generating a network based on somecontext consists in simply expanding nodes (lexi-cM items) further and further away from the con-text until some threshold, called the depth of thenetwork, is reached.Just as we prune the total set of unique lexicalitems by context priming, we also prune the neigh-bourhood of each node in the network by using astatistical metric which provides some indicationof how important he relationship is between eachlexical item and the items in its neighbourhood.In the results we describe here, we use mutual in-formation (Fano 1961, 27-28; Church and Hanks1990) as the metric for neighbourhood pruning,pruning which occurs as the network is being gen-erated.
Yet, another parameter controlling thetopology of the network is the extent of the "win-dow" which defines the neighbourhood of a lexi-cal item (e.g., does the neighbourhood of a lexicalitem consist of only those items which have co-occurred at a distance of up to 3, 5, 10, or 1000words from the item).I I I .
Operat ions  on the  NetworkThe network primed by a context consistsmerely of those lexical items which are closelyreachable via co-occurrence from the priming con-text.
Nodes in the network are lexical items; arcsrepresent co-occurrence relations and carry thevalue of the statistical metric mentioned aboveand the distance of co-occurrence.
With such anetwork we attempt to approximate the statisti-cally relevant neighbourhood in which a particularcontext might be found.In the tests performed on the network thusfar we use the similarity metricS(x, y) - IA n BI 2IA u BIwhere x and y are two nodes representing lexicalitems, the neighbourhoods of which are expressedas the sets of arcs A and B respectively.
The met-ric S is thus defined in terms of the cardinalities ofsets of arcs.
Two arcs are said to be equal if theyreference (point to) the same lexical item at thesame offset distance.
Our metric is a modificationof the Tanimoto coefficient (Bensch and Savitch1992); the numerator is squared in order to assigna higher index of similarity to those nodes whichhave a higher percentage of arcs in common.Our first set of tests concentrated directly onitems in the seed context.
Using the metric above,we attempted to instantiate classes of lexical itemsfor each item in the context.
In those cases wherethere were matches, the results were often encour-aging.
For example, in the LOB corpus, using theseed context John walked across the room, a net-work depth of 6, a mutual information thresholdof 6.0 for neighbourhood pruning, and a windowof 5, for the item John, we instantiated the class{Edward, David, Char les,  Thomas}.
A similar teston the WSJ corpus yielded the following class forjohnr ichard,paul , thomas,edward,david,dona ld ,dan ie l , f  rank,michael ,dennis ,j oseph, j  im,alan,dan,rogerRecall that the subset of the WSJ corpus we usehas had all items folded to lower case as part ofthe pre-processing phase, thus all items in an in-stantiated class will also be folded to lower case.In other tests, the instantiated classes wereless satisfying, such as the following class gener-ated for wi fe using the parameters above, theLOB, and the context his wife walked acrossthe roommouth,father,uncle,lordship, }finger s,mother,husband,f ather ' s,shoulder,mother ' s,brotherIn still other cases, a class could not be instan-tiated at all, typically for items whose neigh-bourhoods were too small to provide meaningfulmatching information.IV .
Abst rac t ionIt is clear that even the most perfectly derivedlexical classes will have members in common.
Thedifferent senses of bank are often given as the clas-sic example of a lexically ambiguous word.
Fromour own data, we observed this problem because ofour preprocessing of the WSJ corpus; the instan-tiation of the class associated with mark includedsome proper names, but also included items suchas marks, cur renc ies ,  yen, and do l la r ,  a con-founding of class information that would not haveoccurred had not case folding taken place.
Ide-ally, it would be useful if a context could be madeto exert a more constraining influence during thecourse of instantiating classes.
For example, if itis reasonably clear from a context, such as markloves  mary, that the "mark" in question is thehuman rather than the financial variety, how maywe ensure that the context provides the properconstraining information if loves  has never co-occurred with mark in the original corpus?293In the case of the ambiguous mark above,while this item does not appear in the neighbour-hood of loves,  other lexical items do (e.g., every-one, who, him, mr), items which may be membersof a class associated with mark.
What is proposed,then, is to construct incrementally classes of itemsover the network, such that these classes may thenfunction as a single item for the purpose of deriv-ing indices of similarity.
In this way, we wouldnot be looking for a specific match between markand loves,  but rather a match among items inthe same class as mark; items in the same class asloves,  and items in the same class as mary.
Withthis in mind, our second set of experiments con-centrated not specifically on items in the primingcontext, but on the entire network, searching forcandidate items to be collapsed into meta-nodesrepresenting classes of items.Our initial experiments in the generation ofpairs of items which could be collapsed into meta-nodes were more successful than the tests basedon items in the priming context.
Using the LOBcorpus, the same parameters as before, and thepriming context John walked across  the room,the following set of pairs represents some of thegood matches over the generated network.
(minut es ,days) , ( three, f  ive) , ( f  ew, f ive) ,(2,3),(f ig , t  able),(days,years),(40,50),(me,him),(three,f w),(4,5),(50,100),(currants,sultanas),(sultanas,raisins),(currants,raisins),...Using the WSJ  corpus, again the same parameters,and the context john walked across the room,part of the set of good matches generated was(months,weeks),(rose,f ell),(days,weeks),(s ingle-a-plus,t riple-b-plus),(single-a-minus,t riple-b-plus),(lawsuit ,complaint),(analyst ,economist)(j ohn,robert), (next ,past ), ( s ix,f ive),(lower,higher),(goodyear,f irest one),(prof it,loss),(billion,million),(j llne ,march),(concedes ,acknowledges),(days ,weeks ), (months ,years ),...It should be noted that the sets given above repre-sent the best good matches.
Empirically, we foundthat a value of S > 1.0 tends to produce the mostmeaningful pairings.
At S < 1.0, the amount of"noisy" pairings increases dramatically.
This isnot an absolute threshold, however, as apparentlyunacceptable pairings do occur at S > 12, suchas, for example, the pairs (catching, teamed),(accumulating, rebuffed), and (father, mind).V.
Future ResearchThe results of our initial experiments in gen-erating classes of lexical items are encouraging,though not conclusive.
We believe that by in-crementally collapsing pairs of very similar itemsinto meta-nodes, we may accomplish a kind of ab-straction over the network which will ultimatelyallow the more accurate instantiation of classesfor the priming context.
The notion of incremen-tally merging classes of lexical items is intuitivelysatisfying and is explored in detail in (Brown,et al 1992).
The approach taken in the citedwork is somewhat different than ours and whileour method is no less computationally complexthan that of Brown, et al, we believe that it issomewhat more manageable because of the prun-ing effect provided by context priming.
On theother hand, unlike the work described by Brown,et al, we as yet have no clear criterion for stoppingthe merging process, save an arbitrary threshold.Finally, it should be noted that our goal is not,strictly speaking, to generate classes over an entirevocabulary, but only that portion of the vocabu-lary relevant for a particular context.
It is hopedthat, by priming with a context, we may be able toeffect some manner of word sense disambiguationin those cases where the meaning of a potentiallyambiguous item ,nay be resolved by hints in thecontext.VI .
Re ferencesBensch, Peter A. and Walter J. Savitch.
1992.
"An Occurrence-Based Model of Word Cat-egorization".
Third Meeting on Mathemat-ics of Language.
Austin, Texas: Associationfor Computational Linguistics, Special Inter-est Group on the Mathematics of Language.Brown, Peter F., et al 1992.
"Class-Based n-gram Models of Natural Language".
Compu-tational Linguistics 18.4: 467-479.Church, Kenneth Ward, and Patrick Hanks.
1990.
"Word Association Norms, Mutual Informa-tion, and Lexicography".
Computational Lin-guistics 16.1: 22-29.Fano, Robert M. 1961.
Transmission o/ In\]or-marion: A Statistical Theory o\] Communica-tions.
New York: MIT Press.Firth, J\[ohn\] R\[upert\].
1957.
"A Synopsis of Lin-guistic Theory, 1930-55."
Studies in Linguis-tic Analysis.
Philological Society, London.Oxford, England: Basil Blackwelh 1-32.294
