Distribution-Based Pruning of Backoff Language ModelsJianfeng GaoMicrosoft Research ChinaNo.
49 Zhichun Road Haidian District100080, China,jfgao@microsoft.comKai-Fu LeeMicrosoft Research ChinaNo.
49 Zhichun Road Haidian District100080, China,kfl@microsoft.comAbstractWe propose a distribution-based pruning ofn-gram backoff language models.
Insteadof the conventional approach of pruningn-grams that are infrequent in training data,we prune n-grams that are likely to beinfrequent in a new document.
Our methodis based on the n-gram distribution i.e.
theprobability that an n-gram occurs in a newdocument.
Experimental results show thatour method performed 7-9% (wordperplexity reduction) better thanconventional cutoff methods.1 IntroductionStatistical language modelling (SLM) has beensuccessfully applied to many domains such asspeech recognition (Jelinek, 1990), informationretrieval (Miller et al, 1999), and spokenlanguage understanding (Zue, 1995).
Inparticular, n-gram language model (LM) hasbeen demonstrated to be highly effective forthese domains.
N-gram LM estimates theprobability of a word given previous words,P(wn|w1,?,wn-1).In applying an SLM, it is usually the casethat more training data will improve a languagemodel.
However, as training data sizeincreases, LM size increases, which can lead tomodels that are too large for practical use.To deal with the problem, count cutoff(Jelinek, 1990) is widely used to prunelanguage models.
The cutoff method deletesfrom the LM those n-grams that occurinfrequently in the training data.
The cutoffmethod assumes that if an n-gram is infrequentin training data, it is also infrequent in testingdata.
But in the real world, training data rarelymatches testing data perfectly.
Therefore, thecount cutoff method is not perfect.In this paper, we propose adistribution-based cutoff method.
Thisapproach estimates if an n-gram is ?likely to beinfrequent in testing data?.
To determine thislikelihood, we divide the training data intopartitions, and use a cross-validation-likeapproach.
Experiments show that this methodperformed 7-9% (word perplexity reduction)better than conventional cutoff methods.In section 2, we discuss prior SLM research,including backoff bigram LM, perplexity, andrelated works on LM pruning methods.
Insection 3, we propose a new criterion for LMpruning based on n-gram distribution, anddiscuss in detail how to estimate thedistribution.
In section 4, we compare ourmethod with count cutoff, and presentexperimental results in perplexity.
Finally, wepresent our conclusions in section 5.2 Backoff Bigram and CutoffOne of the most successful forms of SLM is then-gram LM.
N-gram LM estimates theprobability of a word given the n-1 previouswords, P(wn|w1,?,wn-1).
In practice, n is usuallyset to 2 (bigram), or 3 (trigram).
For simplicity,we restrict our discussion to bigram, P(wn|wn-1),which assumes that the probability of a worddepends only on the identity of the immediatelypreceding word.
But our approach extends toany n-gram.Perplexity is the most common metric forevaluating a bigram LM.
It is defined as,?==?
?Niii wwPNPP 11 )|(log12 (1)where N is the length of the testing data.
Theperplexity can be roughly interpreted as thegeometric mean of the branching factor of thedocument when presented to the languagemodel.
Clearly, lower perplexities are better.One of the key issues in language modellingis the problem of data sparseness.
To deal withthe problem, (Katz, 1987) proposed a backoffscheme, which is widely used in bigramlanguage modelling.
Backoff scheme estimatesthe probability of an unseen bigram by utilizingunigram estimates.
It is of the form:???
>=???
?otherwisewPwwwcwwPwwPiiiiiidii )()(0),()|()|(1111 ?
(2)where c(wi-1,wi) is the frequency of word pair(wi-1,wi) in training data, Pd represents theGood-Turing discounted estimate for seenword pairs, and ?
(wi-1) is a normalizationfactor.Due to the memory limitation in realisticapplications, only a finite set of word pairs haveconditional probabilities P(wn|wn-1) explicitlyrepresented in the model, especially when themodel is trained on a large corpus.
Theremaining word pairs are assigned a probabilityby back-off (i.e.
unigram estimates).
The goalof bigram pruning is to remove uncommonexplicit bigram estimates P(wn|wn-1) from themodel to reduce the number of parameters,while minimizing the performance loss.The most common way to eliminate unusedcount is by means of count cutoffs (Jelinek,1990).
A cutoff is chosen, say 2, and allprobabilities stored in the model with 2 orfewer counts are removed.
This methodassumes that there is not much differencebetween a bigram occurring once, twice, or notat all.
Just by excluding those bigrams with asmall count from a model, a significant savingin memory can be achieved.
In a typicaltraining corpus, roughly 65% of unique bigramsequences occur only once.Recently, several improvements over countcutoffs have been proposed.
(Seymore andRosenfeld, 1996) proposed a different pruningscheme for backoff models, where bigrams areranked by a weighted difference of the logprobability estimate before and after pruning.Bigrams with difference less than a thresholdare pruned.
(Stolcke, 1998) proposed a criterion forpruning based on the relative entropy betweenthe original and the pruned model.
The relativeentropy measure can be expressed as a relativechange in training data perplexity.
All bigramsthat change perplexity by less than a thresholdare removed from the model.
Stolcke alsoconcluded that, for practical purpose, themethod in (Seymore and Rosenfeld, 1996) is avery good approximation to this method.All previous cutoff methods describedabove use a similar criterion for pruning, that is,the difference (or information loss) between theoriginal estimate and the backoff estimate.After ranking, all bigrams with difference smallenough will be pruned, since they contain nomore information.3 Distribution-Based CutoffAs described in the previous section, previouscutoff methods assume that training data coverstesting data.
Bigrams that are infrequent intraining data are also assumed to be infrequentin testing data, and will be cutoff.
But in thereal world, no matter how large the trainingdata, it is still always very sparse compared toall data in the world.
Furthermore, training datawill be biased by its mixture of domain, time, orstyle, etc.
For example, if we use newspaper intraining, a name like ?Lewinsky?
may havehigh frequency in certain years but not others; ifwe use Gone with the Wind in training,?Scarlett O?Hara?
will have disproportionatelyhigh probability and will not be cutoff.We propose another approach to pruning.We aim to keep bigrams that are more likely tooccur in a new document.
We therefore proposea new criterion for pruning parameters frombigram models, based on the bigramdistribution i.e.
the probability that a bigramwill occur in a new document.
All bigrams withthe probability less than a threshold areremoved.We estimate the probability that a bigramoccurs in a new document by dividing trainingdata into partitions, called subunits, and use across-validation-like approach.
In theremaining part of this section, we firstlyinvestigate several methods for termdistribution modelling, and extend them tobigram distribution modelling.
Then weinvestigate the effects of the definition of thesubunit, and experiment with various ways todivide a training set into subunits.
Experimentsshow that this not only allows a much moreefficient computation for bigram distributionmodelling, but also results in a more generalbigram model, in spite of the domain, style, ortemporal bias of training data.3.1 Measure of GeneralityProbabilityIn this section, we will discuss in detail how toestimate the probability that a bigram occurs ina new document.
For simplicity, we define adocument as the subunit of the training corpus.In the next section, we will loosen thisconstraint.Term distribution models estimate theprobability Pi(k), the proportion of times that ofa word wi appears k times in a document.
Inbigram distribution models, we wish to modelthe probability that a word pair (wi-1 ,wi) occursin a new document.
The probability can beexpressed as the measure of the generality of abigram.
Thus, in what follows, it is denoted byPgen(wi-1,wi).
The higher the Pgen(wi-1,wi) is, forone particular document, the less informativethe bigram is, but for all documents, the moregeneral the bigram is.We now consider several methods for termdistribution modelling, which are widely usedin Information Retrieval, and extend them tobigram distribution modelling.
These methodsinclude models based on the Poissondistribution (Mood et al, 1974), inversedocument frequency (Salton and Michael,1983), and Katz?s K mixture (Katz, 1996).3.1.1 The Poisson DistributionThe standard probabilistic model for thedistribution of a certain type of event over unitsof a fixed size (such as periods of time orvolumes of liquid) is the Poisson distribution,which is defined as follows:!);()(kekPkPkiiii??
?
?== (3)In the most common model of the Poissondistribution in IR, the parameter ?i>0 is theaverage number of occurrences of wi perdocument, that isNcfii =?
, where cfi is thenumber of documents containing wi, and N isthe total number of documents in the collection.In our case, the event we are interested in is theoccurrence of a particular word pair (wi-1,wi)and the fixed unit is the document.
We can usethe Poisson distribution to estimate an answerto the question: what is the probability that aword pair occurs in a document.
Therefore, wegetieiPwwP iigen??
??
?=?= 1);0(1),( 1 (4)It turns out that using Poisson distribution, wehave Pgen(wi-1,wi) ?
c(wi-1,wi).
This means thatthis criterion is equivalent to count cutoff.3.1.2 Inverse Document Frequency (IDF)IDF is a widely used measure of specificity(Salton and Michael, 1983).
It is the reverse ofgenerality.
Therefore we can also derivegenerality from IDF.
IDF is defined as follows:)log(ii dfNIDF = (5)where, in the case of bigram distribution, N isthe total number of documents, and dfi is thenumber of documents that the contain wordpair (wi-1,wi).
The formulaidfN=log gives fullweight to a word pair (wi-1,wi) that occurred inone document.
Therefore, let?s assume,iiiiigen IDFwwCwwP)(),( ,11 ??
?
(6)It turns out that based on IDF, our criterion isequivalent to the count cutoff weighted by thereverse of IDF.
Unfortunately, experimentsshow that using (6) directly does not get anyimprovement.
In fact, it is even worse thancount cutoff methods.
Therefore, we use thefollowing form instead,?iiiiigen IDFwwCwwP)(),( ,11 ??
?
(7)where ?
is a weighting factor tuned tomaximize the performance.3.1.3 K MixtureAs stated in (Manning and Sch?tze, 1999), thePoisson estimates are good for non-contentwords, but not for content words.
Severalimprovements over Poisson have beenproposed.
These include two-Poisson Model(Harter, 1975) and Katz?s K mixture model(Katz, 1996).
The K mixture is the better.
It isalso a simpler distribution that fits empiricaldistributions of content words as well asnon-content words.
Therefore, we try to use Kmixture for bigram distribution modelling.According to (Katz, 1996), K mixture modelestimates the probability that word wi appears ktimes in a document as follows:kki kP )1(1)1()( 0, +++?= ??????
(8)where ?k,0=1 iff k=0 and ?k,0=0 otherwise.
?and ?
are parameters that can be fit using theobserved mean ?
and the observed inversedocument frequency IDF as follow:Ncf=?
(9)dfNIDF log= (10)dfdfcfIDF ?=?
?= 12??
(11)???
= (12)where again, cf is the total number ofoccurrence of word wi in the collection, df is thenumber of documents in the collection that wioccurs in, and N is the total number ofdocuments.The bigram distribution model is a variationof the above K mixture model, where weestimate the probability that a word pair(wi-1,wi) , occurs in a document by:?=?
?=Kkiiigen kPwwP11 )(1),( (13)where K is dependent on the size of the subunit,the larger the subunit, the larger the value (inour experiments, we set K from 1 to 3), andPi(k) is the probability of word pair (wi-1,wi)occurs k times in a document.
Pi(k) is estimatedby equation (8), where ?
, and ?
are estimatedby equations (9) to (12).
Accordingly, cf is thetotal number of occurrence of a word pair(wi-1,wi) in the collection, df is the number ofdocuments that contain (wi-1,wi), and N is thetotal number of documents.3.1.4 ComparisonOur experiments show that K mixture is thebest among the three in most cases.
Somepartial experimental results are shown in table1.
Therefore, in section 4, all experimentalresults are based on K mixture method.Word PerplexitySize of Bigram(Number ofBigrams)Poisson IDF KMixture2000000 693.29 682.13 633.235000000 631.64 628.84 603.7010000000 598.42 598.45 589.34Table 1: Word perplexity comparison ofdifferent bigram distribution models.3.2 AlgorithmThe bigram distribution model suggests asimple thresholding algorithm for bigrambackoff model pruning:1.
Select a threshold ?.2.
Compute the probability that each bigramoccurs in a document individually byequation (13).3.
Remove all bigrams whose probability tooccur in a document is less than ?, andrecomputed backoff weights.4 ExperimentsIn this section, we report the experimentalresults on bigram pruning based on distributionversus count cutoff pruning method.In conventional approaches, a document isdefined as the subunit of training data for termdistribution estimating.
But for a very largetraining corpus that consists of millions ofdocuments, the estimation for the bigramdistribution is very time-consuming.
To copewith this problem, we use a cluster ofdocuments as the subunit.
As the number ofclusters can be controlled, we can define anefficient computation method, and optimise theclustering algorithm.In what follows, we will report theexperimental results with document and clusterbeing defined as the subunit, respectively.
Inour experiments, documents are clustered inthree ways: by similar domain, style, or time.In all experiments described below, we use anopen testing data consisting of 15 millioncharacters that have been proofread andbalanced among domain, style and time.Training data are obtained from newspaper(People?s Daily) and novels.4.1 Using Documents as SubunitsFigure 1 shows the results when we define adocument as the subunit.
We usedapproximately 450 million characters ofPeople?s Daily training data (1996), whichconsists of 39708 documents.012345678910550 600 650 700 750 800MillionsWord PerplexitySize(NumberofBigrams)Count CutoffDistribution CutoffFigure 1: Word perplexity comparison of cutoffpruning and distribution based bigram pruningusing a document as the subunit.4.2 Using Clusters by Domain asSubunitsFigure 2 shows the results when we define adomain cluster as the subunit.
We also usedapproximately 450 million characters ofPeople?s Daily training data (1996).
To clusterthe documents, we used an SVM classifierdeveloped by Platt (Platt, 1998) to clusterdocuments of similar domains togetherautomatically, and obtain a domain hierarchyincrementally.
We also added a constraint tobalance the size of each cluster, and finally weobtained 105 clusters.
It turns out that usingdomain clusters as subunits performs almost aswell as the case of documents as subunits.Furthermore, we found that by using thepruning criterion based on bigram distribution,a lot of domain-specific bigrams are pruned.
Itthen results in a relatively domain-independentlanguage model.
Therefore, we call thispruning method domain subtraction basedpruning.012345678910550 600 650 700 750 800MillionsWord PerplexitySize(NumberofBigrams)Count CutoffDistribution CutoffFigure 2: Word perplexity comparison of cutoffpruning and distribution based bigram pruningusing a domain cluster as the subunit.4.3 Using Clusters by Style asSubunitsFigure 3 shows the results when we define astyle cluster as the subunit.
For this experiment,we used 220 novels written by different writers,each approximately 500 kilonbytes in size, anddefined each novel as a style cluster.
Just like indomain clustering, we found that by using thepruning criterion based on bigram distribution,a lot of style-specific bigrams are pruned.
Itthen results in a relatively style-independentlanguage model.
Therefore, we call thispruning method style subtraction basedpruning.012345678910500 520 540 560 580 600 620 640 660 680 700MillionsWord PerplexitySize(NumberofBigrams)Count CutoffDistribution CutoffFigure 3: Word perplexity comparison of cutoffpruning and distribution based bigram pruningusing a style cluster as the subunit.4.4 Using Clusters by Time asSubunitsIn practice, it is relatively easier to collect largetraining text from newspaper.
For example,many Chinese SLMs are trained fromnewspaper, which has high quality andconsistent in style.
But the disadvantage is thetemporal term phenomenon.
In other words,some bigrams are used frequently during onetime period, and then never used again.Figure 4 shows the results when we define atemporal cluster as the subunit.
In thisexperiment, we used approximately 9,200million characters of People?s Daily trainingdata (1978--1997).
We simply clustered thedocument published in the same month of thesame year as a cluster.
Therefore, we obtained240 clusters in total.
Similarly, we found thatby using the pruning criterion based on bigramdistribution, a lot of time-specific bigrams arepruned.
It then results in a relativelytime-independent language model.
Therefore,we call this pruning method temporalsubtraction based pruning.00.20.40.60.811.21.41200 1300 1400 1500 1600 1700 1800 1900 2000MillionsWord PerplexitySize(NumberofBigrams)Count CutoffDistribution CutoffFigure 4: Word perplexity comparison of cutoffpruning and distribution based bigram pruningusing a temporal cluster as the subunit.4.3 SummaryIn our research lab, we are particularlyinterested in the problem of pinyin to Chinesecharacter conversion, which has a memorylimitation of 2MB for programs.
At 2MBmemory, our method leads to 7-9% wordperplexity reduction, as displayed in table 2.Subunit Word PerplexityReductionDocument 9.3%Document Cluster byDomain7.8%Document Cluster byStyle7.1%Document Cluster byTime7.3%Table 2: Word perplexity reduction for bigramof size 2M.As shown in figure 1-4, although as the size oflanguage model is decreased, the perplexityrises sharply, the models created with thebigram distribution based pruning haveconsistently lower perplexity values than forthe count cutoff method.
Furthermore, whenmodelling bigram distribution on documentclusters, our pruning method results in a moregeneral n-gram backoff model, which resists todomain, style or temporal bias of training data.5 ConclusionsIn this paper, we proposed a novel approach forn-gram backoff models pruning: keep n-gramsthat are more likely to occur in a newdocument.
We then developed a criterion forpruning parameters from n-gram models, basedon the n-gram distribution i.e.
the probabilitythat an n-gram occurs in a document.
Alln-grams with the probability less than athreshold are removed.
Experimental resultsshow that the distribution-based pruningmethod performed 7-9% (word perplexityreduction) better than conventional cutoffmethods.
Furthermore, when modelling n-gramdistribution on document clusters createdaccording to domain, style, or time, the pruningmethod results in a more general n-grambackoff model, in spite of the domain, style ortemporal bias of training data.AcknowledgementsWe would like to thank Mingjjing Li, ZhengChen, Ming Zhou, Chang-Ning Huang andother colleagues from Microsoft Research,Jian-Yun Nie from the University of Montreal,Canada, Charles Ling from the University ofWestern Ontario, Canada, and Lee-Feng Chienfrom Academia Sinica, Taiwan, for their helpin developing the ideas and implementation inthis paper.
We would also like to thank JianZhu for her help in our experiments.ReferencesF.
Jelinek, ?Self-organized language modeling forspeech recognition?, in Readings in SpeechRecognition, A. Waibel and K.F.
Lee, eds.,Morgan-Kaufmann, San Mateo, CA, 1990, pp.450-506.D.
Miller, T. Leek, R. M. Schwartz, ?A hiddenMarkov model information retrieval system?, inProc.
22nd International Conference on Researchand Development in Information Retrieval,Berkeley, CA, 1999, pp.
214-221.V.W.
Zue, ?Navigating the informationsuperhighway using spoken language interfaces?,IEEE Expert, S. M. Katz, ?Estimation ofprobabilities from sparse data for the languagemodel component of a speech recognizer?, IEEETransactions on Acoustic, Speech and SignalProcessing, ASSP-35(3): 400-401, March, 1987.K.
Seymore, R. Rosenfeld, ?Scalable backofflanguage models?, in Porc.
InternationalConference on Speech and Language Processing,Vol1.
Philadelphia,PA,1996, pp.232-235A.
Stolcke, ?Entropy-based Pruning of BackoffLanguage Models?
in Proc.
DRAPA NewsTranscriptionand Understanding Workshop,Lansdowne, VA. 1998. pp.270-274M.
Mood, A. G. Franklin, and C. B. Duane,?Introduction to the theory of statistics?, NewYork: McGraw-Hill, 3rd edition, 1974.G.
Salton, and J. M. Michael, ?Introduction toModern Information Retrieval?, New York:McGraw-Hill, 1983.S.
M. Katz, ?Distribution of content words andphrases in text and language modeling?, NaturalLanguage Engineering, 1996(2): 15-59C.
D. Manning, and H. Sch?tze, ?Foundations ofStatistical Natural Language Processing?, TheMIT Press, 1999.S.
Harter, ?A probabilistic approach to automatickeyword indexing: Part II.
An algorithm forprobabilistic indexing?, Journal of the AmericanSociety for Information Science, 1975(26):280-289J.
Platt, ?How to Implement SVMs?, IEEEIntelligent System Magazine, Trends andControversies, Marti Hearst, ed., vol 13, no 4,1998.
