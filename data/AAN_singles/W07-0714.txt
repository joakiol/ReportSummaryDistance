Proceedings of the Second Workshop on Statistical Machine Translation, pages 104?111,Prague, June 2007. c?2007 Association for Computational LinguisticsLabelled Dependencies in Machine Translation EvaluationKarolina Owczarzak Josef van Genabith Andy WayNational Centre for Language TechnologySchool of Computing, Dublin City UniversityDublin 9, Ireland{owczarzak,josef,away}@computing.dcu.ieAbstractWe present a method for evaluating thequality of Machine Translation (MT)output, using labelled dependenciesproduced by a Lexical-FunctionalGrammar (LFG) parser.
Our dependency-based method, in contrast to most popularstring-based evaluation metrics, does notunfairly penalize perfectly valid syntacticvariations in the translation, and theaddition of WordNet provides a way toaccommodate lexical variation.
Incomparison with other metrics on 16,800sentences of Chinese-English newswiretext, our method reaches high correlationwith human scores.1 IntroductionSince the creation of BLEU (Papineni et al, 2002)and NIST (Doddington, 2002), the subject ofautomatic evaluation metrics for MT has beengiven quite a lot of attention.
Although widelypopular thanks to their speed and efficiency, bothBLEU and NIST have been criticized forinadequate accuracy of evaluation at the segmentlevel (Callison-Burch et al, 2006).
As stringbased-metrics, they are limited to superficialcomparison of word sequences between atranslated sentence and one or more referencesentences, and are unable to accommodate anylegitimate grammatical variation when it comes tolexical choices or syntactic structure of thetranslation, beyond what can be found in themultiple references.
A natural next step in the fieldof evaluation was to introduce metrics that wouldbetter reflect our human judgement by acceptingsynonyms in the translated sentence or evaluatingthe translation on the basis of what syntacticfeatures it shares with the reference.Our method follows and substantially extendsthe earlier work of Liu and Gildea (2005), who usesyntactic features and unlabelled dependencies toevaluate MT quality, outperforming BLEU onsegment-level correlation with human judgement.Dependencies abstract away from the particulars ofthe surface string (and syntactic tree) realizationand provide a ?normalized?
representation of(some) syntactic variants of a given sentence.While Liu and Gildea (2005) calculate n-grammatches on non-labelled head-modifier sequencesderived by head-extraction rules from syntactictrees, we automatically evaluate the quality oftranslation by calculating an f-score on labelleddependency structures produced by a Lexical-Functional Grammar (LFG) parser.
Thesedependencies differ from those used by Liu andGildea (2005), in that they are extracted accordingto the rules of the LFG grammar and they arelabelled with a type of grammatical relation thatconnects the head and the modifier, such assubject, determiner, etc.
The presence ofgrammatical relation labels adds another layer ofimportant linguistic information into thecomparison and allows us to account for partialmatches, for example when a lexical item findsitself in a correct relation but with an incorrectpartner.
Moreover, we use a number of best parsesfor the translation and the reference, which servesto decrease the amount of noise that can beintroduced by the process of parsing and extractingdependency information.The translation and reference files areanalyzed by a treebank-based, probabilistic LFGparser (Cahill et al, 2004), which produces a set ofdependency triples for each input.
The translationset is compared to the reference set, and thenumber of matches is calculated, giving the104precision, recall, and f-score for each particulartranslation.In addition, to allow for the possibility of validlexical differences between the translation and thereferences, we follow Kauchak and Barzilay(2006) in adding a number of synonyms in theprocess of evaluation to raise the number ofmatches between the translation and the reference,leading to a higher score.In an experiment on 16,800 sentences ofChinese-English newswire text with segment-levelhuman evaluation from the Linguistic DataConsortium?s (LDC) Multiple Translation project,we compare the LFG-based evaluation methodwith other popular metrics like BLEU, NIST,General Text Matcher (GTM) (Turian et al, 2003),Translation Error Rate (TER) (Snover et al,2006)1, and METEOR (Banerjee and Lavie, 2005),and we show that combining dependencyrepresentations with synonyms leads to a moreaccurate evaluation that correlates better withhuman judgment.
Although evaluated on adifferent test set, our method also outperforms thecorrelation with human scores reported in Liu andGildea (2005).The remainder of this paper is organized asfollows: Section 2 gives a basic introduction toLFG; Section 3 describes related work; Section 4describes our method and gives results of theexperiment on the Multiple Translation data;Section 5 discusses ongoing work; Section 6concludes.2 Lexical-Functional GrammarIn Lexical-Functional Grammar (Kaplan andBresnan, 1982; Bresnan, 2001) sentence structureis represented in terms of c(onstituent)-structureand f(unctional)-structure.
C-structure representsthe word order of the surface string and thehierarchical organisation of phrases in terms ofCFG trees.
F-structures are recursive feature (orattribute-value) structures, representing abstractgrammatical relations, such as subj(ect), obj(ect),obl(ique), adj(unct), etc., approximating topredicate-argument structure or simple logicalforms.
C-structure and f-structure are related in1 We omit HTER (Human-Targeted Translation ErrorRate), as it is not fully automatic and requires humaninput.terms of functional annotations (attribute-valuestructure equations) in c-structure trees, describingf-structures.While c-structure is sensitive to surfacerearrangement of constituents, f-structure abstractsaway from the particulars of the surfacerealization.
The sentences John resigned yesterdayand Yesterday, John resigned will receive differenttree representations, but identical f-structures,shown in (1).
(1) C-structure:                         F-structure:SNP                      VP|JohnV               NP-TMP|                      |resigned       yesterdaySUBJ        PRED   johnNUM    sgPERS   3PRED       resignTENSE     pastADJ      {[PRED   yesterday]}SNP       NP       VP|                 |            |Yesterday  John        V|resignedSUBJ        PRED   johnNUM    sgPERS   3PRED       resignTENSE     pastADJ      {[PRED   yesterday]}Note that if these sentences were a translation-reference pair, they would receive a less-than-perfect score from string-based metrics.
Forexample, BLEU with add-one smoothing2 givesthis pair a score of barely 0.3781.
This is because,although all three unigrams from the ?translation?
(John; resigned; yesterday) are present in thereference, which contains four items including thecomma (Yesterday; ,; John; resigned), the?translation?
contains only one bigram (Johnresigned) that matches the ?reference?
(Yesterday,; , John; John resigned), and no matchingtrigrams.The f-structure can also be described in termsof a flat set of triples.
In triples format, the f-structure in (1) is represented as follows:{subj(resign, john), pers(john, 3), num(john, sg),tense(resign, past), adj(resign, yesterday),pers(yesterday, 3), num(yesterday, sg)}.2 We use smoothing because the original BLEU metricgives zero points to sentences with fewer than one four-gram.105Cahill et al (2004) presents a set of Penn-IITreebank-based LFG parsing resources.
Theirapproach distinguishes 32 types of dependencies,including grammatical functions andmorphological information.
This set can be dividedinto two major groups: a group of predicate-onlydependencies and non-predicate dependencies.Predicate-only dependencies are those whose pathends in a predicate-value pair, describinggrammatical relations.
For example, for the f-structure in (1), predicate-only dependencies wouldinclude: {subj(resign, john), adj(resign,yesterday)}.Other predicate-only dependencies include:apposition, complement, open complement,coordination, determiner, object, second object,oblique, second oblique, oblique agent, possessive,quantifier, relative clause, topic, and relativeclause pronoun.
The remaining non-predicatedependencies are: adjectival degree, coordinationsurface form, focus, complementizer forms: if,whether, and that, modal, number, verbal particle,participle, passive, person, pronoun surface form,tense, and infinitival clause.In parser evaluation, the quality of the f-structures produced automatically can be checkedagainst a set of gold standard sentences annotatedwith f-structures by a linguist.
The evaluation isconducted by calculating the precision and recallbetween the set of dependencies produced by theparser, and the set of dependencies derived fromthe human-created f-structure.
Usually, twoversions of f-score are calculated: one for all thedependencies for a given input, and a separate onefor the subset of predicate-only dependencies.In this paper, we use the parser developed byCahill et al (2004), which automatically annotatesinput text with c-structure trees and f-structuredependencies, obtaining high precision and recallrates.
33 Related work3.1 String-based metricsThe insensitivity of BLEU and NIST to perfectlylegitimate syntactic and lexical variation has beenraised, among others, in Callison-Burch et al(2006), but the criticism is widespread.
Even the3 A demo of the parser can be found at http://lfg-demo.computing.dcu.ie/lfgparser.htmlcreators of BLEU point out that it may notcorrelate particularly well with human judgment atthe sentence level (Papineni et al, 2002).Recently a number of attempts to remedy theseshortcomings have led to the development of otherautomatic MT evaluation metrics.
Some of themconcentrate mainly on word order, like GeneralText Matcher (Turian et al, 2003), whichcalculates precision and recall for translation-reference pairs, weighting contiguous matchesmore than non-sequential matches, or TranslationError Rate (Snover et al, 2006), which computesthe number of substitutions, insertions, deletions,and shifts necessary to transform the translationtext to match the reference.
Others try toaccommodate both syntactic and lexicaldifferences between the candidate translation andthe reference, like CDER (Leusch et al, 2006),which employs a version of edit distance for wordsubstitution and reordering; or METEOR(Banerjee and Lavie, 2005), which uses stemmingand WordNet synonymy.
Kauchak and Barzilay(2006) and Owczarzak et al (2006) useparaphrases during BLEU and NIST evaluation toincrease the number of matches between thetranslation and the reference; the paraphrases areeither taken from WordNet4 in Kauchak andBarzilay (2006) or derived from the test set itselfthrough automatic word and phrase alignment inOwczarzak et al (2006).
Another metric makinguse of synonyms is the linear regression modeldeveloped by Russo-Lassner et al (2005), whichmakes use of stemming, WordNet synonymy, verbclass synonymy, matching noun phrase heads, andproper name matching.
Kulesza and Shieber(2004), on the other hand, train a Support VectorMachine using features such as proportion of n-gram matches and word error rate to judge a giventranslation?s distance from human-level quality.3.2 Dependency-based metricThe metrics described above use only string-basedcomparisons, even while taking into considerationreordering.
By contrast, Liu and Gildea (2005)present three metrics that use syntactic andunlabelled dependency information.
Two of thesemetrics are based on matching syntactic subtreesbetween the translation and the reference, and one4 http://wordnet.princeton.edu/106is based on matching headword chains, i.e.sequences of words that correspond to a path in theunlabelled dependency tree of the sentence.Dependency trees are created by extracting aheadword for each node of the syntactic tree,according to the rules used by the parser of Collins(1999), where every subtree represents themodifier information for its root headword.
Thedependency trees for the translation and thereference are converted into flat headword chains,and the number of overlapping n-grams betweenthe translation and the reference chains iscalculated.
Our method, extending this line ofresearch with the use of labelled LFGdependencies, partial matching, and n-best parses,allows us to considerably outperform Liu andGildea?s (2005) highest correlations with humanjudgement (they report 0.144 for the correlationwith human fluency judgement, 0.202 for thecorrelation with human overall judgement),although it has to be kept in mind that suchcomparison is only tentative, as their correlation iscalculated on a different test set.4 LFG f-structure in MT evaluationLFG-based automatic MT evaluation reflects thesame process that underlies the evaluation ofparser-produced f-structure quality against a goldstandard: we parse the translation and thereference, and then, for each sentence, we checkthe set of labelled translation dependencies againstthe set of labelled reference dependencies,counting the number of matches.
As a result, weobtain the precision and recall scores for thetranslation, and we calculate the f-score for thegiven pair.4.1 Determining parser noiseBecause we are comparing two outputs that wereproduced automatically, there is a possibility thatthe result will not be noise-free, even if the parserfails to provide a parse only in 0.1% of cases.To assess the amount of noise that the parserintroduces, Owczarzak et al (2006) conducted anexperiment where 100 English sentences werehand-modified so that the position of adjuncts waschanged, but the sentence remained grammaticaland the meaning was not influenced.
This way, anideal parser should give both the source and themodified sentence the same f-structure, similarly tothe example presented in (1).
The modifiedsentences were treated like a translation file, andthe original sentences played the part of thereference.
Each set was run through the parser, andthe dependency triples obtained from the?translation?
were compared against thedependency triples for the ?reference?, calculatingthe f-score.
Additionally, the same ?translation-reference?
set was scored with other metrics (TER,METEOR, BLEU, NIST, and GTM).
The results,including the distinction between f-scores for alldependencies and predicate-only dependencies,appear in Table 1.baseline modifiedTER 0.0 6.417METEOR   1.0 0.9970BLEU 1.0000 0.8725NIST 11.5232 11.1704 (96.94%)GTM 100 99.18dep f-score  100 96.56dep_preds f-score 100 94.13Table 1.
Scores for sentences with reordered adjunctsThe baseline column shows the upper bound for agiven metric: the score which a perfect translation,word-for-word identical to the reference, wouldobtain.5 The other column lists the scores that themetrics gave to the ?translation?
containingreordered adjunct.
As can be seen, the dependencyand predicate-only dependency scores are lowerthan the perfect 100, reflecting the noiseintroduced by the parser.We propose that the problem of parsernoise can be alleviated by introducing a number ofbest parses into the comparison between thetranslation and the reference.
Table 2 shows howincreasing the number of parses available forcomparison brings our method closer to an idealnoise-free parser.5 Two things have to be noted here: (1) in the case ofNIST the perfect score differs from text to text, which iswhy the percentage points are provided along thenumerical score, and (2) in the case of TER the lowerthe score, the better the translation, so the perfecttranslation will receive 0, and there is no upper boundon the score, which makes this particular metricextremely difficult to directly compare with others.107dependency f-score1 best 96.562 best 97.315 best 97.9010 best 98.3120 best 98.5930 best 98.7450 best 98.79baseline 100Table 2.
Dependency f-scores for sentences with reorderedadjuncts with n-best parses availableIt has to be noted, however, that increasing thenumber of parses beyond a certain threshold doeslittle to further improve results, and at the sametime it considerably decreases the efficiency of themethod, so it is important to find the right balancebetween these two factors.
In our opinion, theoptimal value would be 10-best parses.4.2 Correlation with human judgement ?MultiTrans4.2.1 Experimental designTo evaluate the correlation with humanassessment, we used the data from the LinguisticData Consortium Multiple Translation Chinese(MTC) Parts 2 and 4, which consists of multipletranslations of Chinese newswire text, four human-produced references, and segment-level humanscores for a subset of the translation-referencepairs.
Although a single translated segment wasalways evaluated by more than one judge, thejudges used a different reference every time, whichis why we treated each translation-reference-human score triple as a separate segment.
In effect,the test set created from this data contained 16,800segments.
As in the previous experiment, thetranslation was scored using BLEU, NIST, GTM,TER, METEOR, and our labelled dependency-based method.4.2.2 Labelled dependency-based methodWe examined a number of modifications of thedependency-based method in order to find outwhich one gives the highest correlation withhuman scores.
The correlation differences betweenimmediate neighbours in the ranking were oftentoo small to be statistically significant; however,there is a clear overall trend towards improvement.Besides the plain version of the dependency f-score, we also looked at the f-score calculated onpredicate dependencies only (ignoring ?atomic?features such as person, number, tense, etc.
), whichturned out not to correlate well with humanjudgements.Another addition was the use of 2-, 10-, or 50-best parses of the translation and referencesentences, which partially neutralized parser noiseand resulted in increased correlations.We also created a version where predicatedependencies of the type subj(resign,John) are splitinto two parts, each time replacing one of theelements participating in the relation with avariable, giving in effect subj(resign,x) andsubj(y,John).
This lets us score partial matches,where one correct lexical object happens to finditself in the correct relation, but with an incorrect?partner?.Lastly, we added WordNet synonyms into thematching process to accommodate lexicalvariation, and to compare our WordNet-enhancedmethod with the WordNet-enhanced version ofMETEOR.4.2.3 ResultsWe calculated Pearson?s correlation coefficient forsegment-level scores that were given by eachmetric and by human judges.
The results of thecorrelation are shown in Table 3.
Note that thecorrelation for TER is negative, because in TERzero is the perfect score, in contrast to othermetrics where zero is the worst possible score;however, this time the absolute values can beeasily compared to each other.
Rows are orderedby the highest value of the (absolute) correlationwith the human score.First, it seems like none of the metrics is verygood at reflecting human fluency judgments; thecorrelation values in the first column aresignificantly lower than the correlation withaccuracy.
This finding has been previouslyreported, among others, in Liu and Gildea (2005).However, the dependency-based method in almostall its versions has decidedly the highestcorrelation in this area.
This can be explained bythe method?s sensitivity to the grammaticalstructure of the sentence: a more grammaticaltranslation is also a translation that is more fluent.As to the correlation with human evaluation oftranslation accuracy, our method currently falls108short of METEOR.
This is caused by the fact thatMETEOR assign relatively little importance to theposition of a specific word in a sentence, thereforerewarding the translation for content rather thanlinguistic form.
Interestingly, while METEOR,with or without WordNet, considerablyoutperforms all other metrics when it comes to thecorrelation with human judgements of translationaccuracy, it falls well behind most versions of ourdependency-based method in correlation withhuman scores of translation fluency.Surprisingly, adding partial matching to thedependency-based method resulted in the greatestincrease in correlation levels, to the extent that thepartial-match versions consistently outperformedversions with a larger number of parses availablebut without the partial match.
The most interestingeffect was that the partial-match versions (eventhose with just a single parse) offered resultscomparable to or higher than the addition ofWordNet to the matching process when it comes toaccuracy and overall judgement.5 Current and future workFluency and accuracy are two very differentaspects of translation quality, each with its own setof conditions along which the input is evaluated.Therefore, it seems unfair to expect a singleautomatic metric to correlate highly with humanjudgements of both at the same time.
This patternis very noticeable in Table 3: if a metric is(relatively) good at correlating with fluency, itsaccuracy correlation suffers (GTM might serve asan example here), and the opposite holds as well(see METEOR?s scores).
It does not mean that anyimprovement that increases the method?scorrelation with one aspect will result in a decreasein the correlation with the other aspect; but it doessuggest that a possible way of development wouldbe to target these correlations separately, if wewant our automated metrics to reflect humanscores better.
At the same time, string-basedmetrics might have already exhausted theirpotential when it comes to increasing theircorrelation with human evaluation; as has beenpointed out before, these metrics can only tell usthat two strings differ, but they cannot distinguishlegitimate grammatical variance fromungrammatical variance.
As the quality of MTTable 3.
Pearson?s correlation between human scores andevaluation metrics.
Legend: d = dependency f-score, _pr =predicate-only f-score, 2, 10, 50 = n-best parses; var =partial-match version; M = METEOR, WN = WordNet6improves, the community will need metrics that aremore sensitive in this respect.
After all, the truequality of MT depends on producing grammaticaloutput which describes the same concept as thesource utterance, and the string identity with areference is only a very selective approximation ofthis goal.6 In general terms, an increase of 0.022 or more betweenany two scores in the same column is significant with a95% confidence interval.
The statistical significance ofcorrelation differences was calculated using Fisher?s z?transformation and the general formula for confidenceinterval.fluency  accuracy  averaged_50+WN 0.177 M+WN 0.294 M+WN 0.255d+WN 0.175 M   0.278 d_50_var 0.252d_50_var 0.174 d_50_var 0.273 d_50+WN 0.250GTM 0.172 NIST 0.273 d_10_var 0.250d_10_var 0.172 d_10_var 0.273 d_2_var 0.247d_50 0.171 d_2_var 0.270 d+WN 0.244d_2_var 0.168 d_50+WN 0.269 d_50 0.243d_10 0.168 d_var 0.266 d_var 0.243d_var 0.165 d_50 0.262 M   0.242d_2 0.164 d_10 0.262 d_10 0.242d   0.161 d+WN 0.260 NIST 0.238BLEU 0.155 d_2 0.257 d_2 0.237M+WN 0.153 d  0.256 d   0.235M   0.149 d_pr 0.240 d_pr 0.216NIST 0.146 GTM 0.203 GTM 0.208d_pr 0.143 BLEU 0.199 BLEU 0.197TER -0.133 TER -0.192 TER -0.182109In order to maximize the correlation withhuman scores of fluency, we plan to look moreclosely at the parser output, and implement somebasic transformations which would allow an evendeeper logical analysis of input (e.g.
passive toactive voice transformation).Additionally, we want to take advantage ofthe fact that the score produced by the dependency-based method is the proportional average ofmatches for a group of up to 32 (but usually farfewer) different dependency types.
We plan toimplement a set of weights, one for eachdependency type, trained in such a way as tomaximize the correlation of the final dependency f-score with human evaluation.
In a preliminaryexperiment, for example, assigning a low weight tothe topic dependency increases our correlationsslightly (this particular case can also be seen as atransformation into a more basic logical form byremoving non-elementary dependency types).In a similar direction, we want toexperiment more with the f-score calculations.Initial check shows that assigning a higher weightto recall than to precision improves results.To improve the correlation with accuracyjudgements, we would like to experiment using aparaphrase set derived from a large parallel corpus,as described in Owczarzak et al (2006).
Whileretaining the advantage of having a similar size toa corresponding set of WordNet synonyms, this setwill also capture low-level syntactic variations,which can increase the number of matches.6 ConclusionsIn this paper we present a linguistically-motivated method for automatically evaluating theoutput of Machine Translation.
Most currentlyused popular metrics rely on comparing translationand reference on a string level.
Even givenreordering, stemming, and synonyms for individualwords, current methods are still far from reachinghuman ability to assess the quality of translation,and there exists a need in the community todevelop more dependable metrics.
Our methodexplores one such direction of development,comparing the sentences on the level of theirgrammatical structure, as exemplified by their f-structure labelled dependency triples produced byan LFG parser.
In our experiments we showed thatthe dependency-based method correlates higherthan any other metric with human evaluation oftranslation fluency, and shows high correlationwith the average human score.
The use ofdependencies in MT evaluation has not beenextensively researched before (one exception herewould be Liu and Gildea (2005)), and requiresmore research to improve it, but the method showspotential to become an accurate evaluation metric.AcknowledgementsThis work was partly funded by Microsoft IrelandPhD studentship  2006-8  for the first author of thepaper.
We would also like to thank our reviewersand Dan Melamed for their insightful comments.All remaining errors are our own.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation withImproved Correlation with Human Judgments.Proceedings of the Workshop on Intrinsic andExtrinsic Evaluation Measures for MT and/orSummarization at the Association for ComputationalLinguistics Conference 2005: 65-73.
Ann Arbor,Michigan.Joan Bresnan.
2001.
Lexical-Functional Syntax,Blackwell, Oxford.Aoife Cahill, Michael Burke, Ruth O?Donovan, Josefvan Genabith, and Andy Way.
2004.
Long-DistanceDependency Resolution in Automatically AcquiredWide-Coverage PCFG-Based LFG Approximations,In Proceedings of Association for ComputationalLinguistics 2004: 320-327.
Barcelona, Spain.Chris Callison-Burch, Miles Osborne and PhilippKoehn.
2006.
Re-evaluating the role of BLEU inMachine Translation Research.
Proceedings of theEuropean Chapter of the Association forComputational Linguistics 2006: 249-256.
Oslo,Norway.Michael J. Collins.
1999.
Head-driven StatisticalModels for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia.George Doddington.
2002.
Automatic Evaluation of MTQuality using N-gram Co-occurrence Statistics.Proceedings of Human Language TechnologyConference 2002: 138-145.
San Diego, California.Kaplan, R. M., and J. Bresnan.
1982.
Lexical-functionalGrammar: A Formal System for Grammatical110Representation.
In J. Bresnan (ed.
), The MentalRepresentation of Grammatical Relations.
MITPress, Cambridge.David Kauchak and Regina Barzilay.
2006.Paraphrasing for Automatic Evaluation.
Proceedingsof Human Language Technology ?
North AmericanChapter of the Association for ComputationalLinguistics Conference 2006: 45-462.
New York,New York.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translationmodels.
Proceedings of the Workshop on MachineTranslation: From real users to research at theAssociation for Machine Translation in the AmericasConference 2004: 115-124.
Washington, DC.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
Proceedings of MTSummit 2005: 79-86.
Phuket, Thailand.Alex Kulesza and Stuart M. Shieber.
2004.
A learningapproach to improving sentence-level MT evaluation.In Proceedings of the Conference on Theoretical andMethodological Issues in Machine Translation 2004:75-84.
Baltimore, Maryland.Gregor Leusch, Nicola Ueffing and Hermann Ney.2006.
CDER: Efficient MT Evaluation Using BlockMovements.
Proceedings of European Chapter of theAssociation for Computational LinguisticsConference 2006: 241-248.
Trento, Italy.Ding Liu and Daniel Gildea.
2005.
Syntactic Featuresfor Evaluation of Machine Translation.
InProceedings of the Workshop on Intrinsic andExtrinsic Evaluation Measures for MachineTranslation and/or Summarization at the Associationfor Computational Linguistics Conference 2005.
AnnArbor, Michigan.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Modes.Computational Linguistics, 29:19-51.Karolina Owczarzak, Declan Groves, Josef vanGenabith, and Andy Way.
2006.
Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation.Proceedings of the Workshop on Statistical MachineTranslation at the Human Language Technology ?North American Chapter of the Association forComputational Linguistics Conference 2006: 86-93.New York, New York.Kishore Papineni, Salim Roukos, Todd Ward, andWeiJing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofAssociation for Computational LinguisticsConference 2002: 311-318.
Philadelphia,Pennsylvania.Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.2005.
A Paraphrase-based Approach to MachineTranslation Evaluation.
Technical Report LAMP-TR-125/CS-TR-4754/UMIACS-TR-2005-57, Universityof Maryland, College Park, Maryland.Mathew Snover, Bonnie Dorr, Richard Schwartz, JohnMakhoul, Linnea Micciula.
2006.
A Study ofTranslation Error Rate with Targeted HumanAnnotation.
Proceedings of the Association forMachine Translation in the Americas Conference2006: 223-231.
Boston, Massachusetts.Joseph P. Turian, Luke Shen, and I. Dan Melamed.2003.
Evaluation of Machine Translation and ItsEvaluation.
Proceedings of MT Summit 2003: 386-393.
New Orleans, Luisiana.Ying Zhang and Stephan Vogel.
2004.
Measuringconfidence intervals for the machine translationevaluation metrics.
Proceedings of Conference onTheoretical and Methodological Issues in MachineTranslation 2004: 85-94.
Baltimore, Maryland.111
