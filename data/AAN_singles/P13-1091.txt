Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 924?932,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsParsing Graphs with Hyperedge Replacement GrammarsDavid ChiangInformation Sciences InstituteUniversity of Southern CaliforniaJacob AndreasColumbia UniversityUniversity of CambridgeDaniel BauerDepartment of Computer ScienceColumbia UniversityKarl Moritz HermannDepartment of Computer ScienceUniversity of OxfordBevan JonesUniversity of EdinburghMacquarie UniversityKevin KnightInformation Sciences InstituteUniversity of Southern CaliforniaAbstractHyperedge replacement grammar (HRG)is a formalism for generating and trans-forming graphs that has potential appli-cations in natural language understand-ing and generation.
A recognition al-gorithm due to Lautemann is known tobe polynomial-time for graphs that areconnected and of bounded degree.
Wepresent a more precise characterization ofthe algorithm?s complexity, an optimiza-tion analogous to binarization of context-free grammars, and some important im-plementation details, resulting in an algo-rithm that is practical for natural-languageapplications.
The algorithm is part of Boli-nas, a new software toolkit for HRG pro-cessing.1 IntroductionHyperedge replacement grammar (HRG) is acontext-free rewriting formalism for generatinggraphs (Drewes et al, 1997), and its synchronouscounterpart can be used for transforming graphsto/from other graphs or trees.
As such, it has greatpotential for applications in natural language un-derstanding and generation, and semantics-basedmachine translation (Jones et al, 2012).
Fig-ure 1 shows some examples of graphs for natural-language semantics.A polynomial-time recognition algorithm forHRGs was described by Lautemann (1990), build-ing on the work of Rozenberg and Welzl (1986)on boundary node label controlled grammars, andothers have presented polynomial-time algorithmsas well (Mazanek and Minas, 2008; Moot, 2008).Although Lautemann?s algorithm is correct andtractable, its presentation is prefaced with the re-mark: ?As we are only interested in distinguish-ing polynomial time from non-polynomial time,the analysis will be rather crude, and implemen-tation details will be explicated as little as possi-ble.?
Indeed, the key step of the algorithm, whichmatches a rule against the input graph, is describedat a very high level, so that it is not obvious (for anon-expert in graph algorithms) how to implementit.
More importantly, this step as described leadsto a time complexity that is polynomial, but poten-tially of very high degree.In this paper, we describe in detail a more effi-cient version of this algorithm and its implementa-tion.
We give a more precise complexity analysisin terms of the grammar and the size and maxi-mum degree of the input graph, and we show howto optimize it by a process analogous to binariza-tion of CFGs, following Gildea (2011).
The re-sulting algorithm is practical and is implementedas part of the open-source Bolinas toolkit for hy-peredge replacement grammars.2 Hyperedge replacement grammarsWe give a short example of how HRG works, fol-lowed by formal definitions.2.1 ExampleConsider a weighted graph language involving justtwo types of semantic frames (want and believe),two types of entities (boy and girl), and two roles(arg0 and arg1).
Figure 1 shows a few graphs fromthis language.Figure 2 shows how to derive one of thesegraphs using an HRG.
The derivation starts witha single edge labeled with the nonterminal sym-bol S .
The first rewriting step replaces this edgewith a subgraph, which we might read as ?The924boy?girl?want?
arg0arg1boy?believe?
arg1want?believe?
arg1want?
arg1girl?arg0boy?arg0arg1arg0Figure 1: Sample members of a graph language,representing the meanings of (clockwise from up-per left): ?The girl wants the boy,?
?The boy isbelieved,?
and ?The boy wants the girl to believethat he wants her.
?boy wants something (X) involving himself.?
Thesecond rewriting step replaces the X edge with an-other subgraph, which we might read as ?The boywants the girl to believe something (Y) involvingboth of them.?
The derivation continues with athird rewriting step, after which there are no morenonterminal-labeled edges.2.2 DefinitionsThe graphs we use in this paper have edge labels,but no node labels; while node labels are intu-itive for many graphs in NLP, using both node andedge labels complicates the definition of hyper-edge grammar and algorithms.
All of our graphsare directed (ordered), as the purpose of mostgraph structures in NLP is to model dependenciesbetween entities.Definition 1.
An edge-labeled, ordered hyper-graph is a tuple H = ?V, E, ?
?, where?
V is a finite set of nodes?
E ?
V+ is a finite set of hyperedges, each ofwhich connects one or more distinct nodes?
?
: E ?
C assigns a label (drawn from thefinite set C) to each edge.For brevity we use the terms graph and hyper-graph interchangeably, and similarly for edge andhyperedge.
In the definition of HRGs, we will usethe notion of hypergraph fragments, which are theelementary structures that the grammar assemblesinto hypergraphs.Definition 2.
A hypergraph fragment is a tuple?V, E, ?, X?, where ?V, E, ??
is a hypergraph andX ?
V+ is a list of distinct nodes called the ex-ternal nodes.The function of graph fragments in HRG isanalogous to the right-hand sides of CFG rulesand to elementary trees in tree adjoining gram-mars (Joshi and Schabes, 1997).
The externalnodes indicate how to integrate a graph into an-other graph during a derivation, and are analogousto foot nodes.
In diagrams, we draw them with ablack circle ( ).Definition 3.
A hyperedge replacement grammar(HRG) is a tuple G = ?N,T, P, S ?
where?
N and T are finite disjoint sets of nonterminaland terminal symbols?
S ?
N is the start symbol?
P is a finite set of productions of the formA ?
R, where A ?
N and R is a graph frag-ment over N ?
T .We now describe the HRG rewriting mecha-nism.Definition 4.
Given a HRG G, we define the re-lation H ?G H?
(or, H?
is derived from H in onestep) as follows.
Let e = (v1 ?
?
?
vk) be an edge inH with label A.
Let (A?
R) be a production ofG,where R has external nodes XR = (u1 ?
?
?
uk).
Thenwe write H ?G H?
if H?
is the graph formed byremoving e from H, making an isomorphic copyof R, and identifying vi with (the copy of) ui fori = 1, .
.
.
, k.Let H ?
?G H?
(or, H?
is derived from H) be thereflexive, transitive closure of?G.
The graph lan-guage of a grammar G is the (possibly infinite) setof graphs H that have no edges with nonterminallabels such thatS ?
?G H.When a HRG rule (A ?
R) is applied to anedge e, the mapping of external nodes in R to the9251X ?believe?
arg1girl?arg01Y1 2Y?12want?arg0arg1S1boy?Xwant?
arg1arg02 believe?
arg1want?
arg1girl?arg0boy?arg0Y3want?believe?
arg1want?
arg1girl?arg0boy?arg0arg1arg0Figure 2: Derivation of a hyperedge replacement grammar for a graph representing the meaning of ?Theboy wants the girl to believe that he wants her.
?nodes of e is implied by the ordering of nodesin e and XR.
When writing grammar rules, wemake this ordering explicit by writing the left handside of a rule as an edge and indexing the externalnodes of R on both sides, as shown in Figure 2.HRG derivations are context-free in the sensethat the applicability of each production dependson the nonterminal label of the replaced edge only.This allows us to represent a derivation as a deriva-tion tree, and sets of derivations of a graph as aderivation forest (which can in turn represented ashypergraphs).
Thus we can apply many of themethods developed for other context free gram-mars.
For example, it is easy to define weightedand synchronous versions of HRGs.Definition 5.
If K is a semiring, a K-weightedHRG is a tuple G = ?N,T, P, S , ?
?, where?N, T, P, S ?
is a HRG and ?
: P ?
K assigns aweight in K to each production.
The weight of aderivation ofG is the product of the weights of theproductions used in the derivation.We defer a definition of synchronous HRGs un-til Section 4, where they are discussed in detail.3 ParsingLautemann?s recognition algorithm for HRGs is ageneralization of the CKY algorithm for CFGs.Its key step is the matching of a rule against theinput graph, analogous to the concatenation oftwo spans in CKY.
The original description leavesopen how this matching is done, and because ittries to match the whole rule at once, it has asymp-totic complexity exponential in the number of non-terminal edges.
In this section, we present a re-finement that makes the rule-matching procedureexplicit, and because it matches rules little by lit-tle, similarly to binarization of CFG rules, it doesso more efficiently than the original.Let H be the input graph.
Let n be the number ofnodes in H, and d be the maximum degree of anynode.
Let G be a HRG.
For simplicity, we assumethat the right-hand sides of rules are connected.This restriction entails that each graph generatedby G is connected; therefore, we assume that H isconnected as well.
Finally, let m be an arbitrarynode of H called the marker node, whose usagewill become clear below.13.1 Representing subgraphsJust as CKY deals with substrings (i, j] of the in-put, the HRG parsing algorithm deals with edge-induced subgraphs I of the input.
An edge-induced subgraph of H = ?V, E, ??
is, for some1To handle the more general case where H is not con-nected, we would need a marker for each component.926subset E?
?
E, the smallest subgraph containingall edges in E?.
From now on, we will assume thatall subgraphs are edge-induced subgraphs.In CKY, the two endpoints i and j com-pletely specify the recognized part of the input,wi+1 ?
?
?w j.
Likewise, we do not need to store allof I explicitly.Definition 6.
Let I be a subgraph of H. A bound-ary node of I is a node in I which is either a nodewith an edge in H\I or an external node.
A bound-ary edge of I is an edge in I which has a boundarynode as an endpoint.
The boundary representationof I is the tuple ?bn(I), be(I, v),m ?
I?, where?
bn(I) is the set of boundary nodes of I?
be(I, v) be the set of boundary edges of v in I?
(m ?
I) is a flag indicating whether themarker node is in I.The boundary representation of I suffices tospecify I compactly.Proposition 1.
If I and I?
are two subgraphs of Hwith the same boundary representation, then I =I?.Proof.
Case 1: bn(I) is empty.
If m ?
I and m ?
I?,then all edges of H must belong to both I and I?,that is, I = I?
= H. Otherwise, if m < I and m < I?,then no edges can belong to either I or I?, that is,I = I?
= ?.Case 2: bn(I) is nonempty.
Suppose I , I?
;without loss of generality, suppose that there is anedge e that is in I \ I?.
Let ?
be the shortest path(ignoring edge direction) that begins with e andends with a boundary node.
All the edges along ?must be in I \ I?, or else there would be a boundarynode in the middle of ?, and ?
would not be theshortest path from e to a boundary node.
Then, inparticular, the last edge of ?must be in I \ I?.
Sinceit has a boundary node as an endpoint, it must be aboundary edge of I, but cannot be a boundary edgeof I?, which is a contradiction.
If two subgraphs are disjoint, we can use theirboundary representations to compute the boundaryrepresentation of their union.Proposition 2.
Let I and J be two subgraphswhose edges are disjoint.
A node v is a boundarynode of I ?
J iff one of the following holds:(i) v is a boundary node of one subgraph but notthe other(ii) v is a boundary node of both subgraphs, andhas an edge which is not a boundary edge ofeither.An edge is a boundary edge of I ?
J iff it has aboundary node of I ?
J as an endpoint and is aboundary edge of I or J.Proof.
(?)
v has an edge in either I or J and anedge e outside both I and J.
Therefore it must be aboundary node of either I or J.
Moreover, e is nota boundary edge of either, satisfying condition (ii).(?)
Case (i): without loss of generality, assumev is a boundary node of I.
It has an edge e in I, andtherefore in I ?
J, and an edge e?
outside I, whichmust also be outside J.
For e < J (because I andJ are disjoint), and if e?
?
J, then v would be aboundary node of J.
Therefore, e?
< I ?
J, so v isa boundary node of I ?
J.
Case (ii): v has an edgein I and therefore I ?
J, and an edge not in eitherI or J.
This result leads to Algorithm 1, which runs intime linear in the number of boundary nodes.Algorithm 1 Compute the union of two disjointsubgraphs I and J.for all v ?
bn(I) doE ?
be(I, v) ?
be(J, v)if v < bn(J) or v has an edge not in E thenadd v to bn(I ?
J)be(I ?
J, v)?
Efor all v ?
bn(J) doif v < bn(I) thenadd v to bn(I ?
J)be(I ?
J, v)?
be(I, v) ?
be(J, v)(m ?
I ?
J)?
(m ?
I) ?
(m ?
J)In practice, for small subgraphs, it may be moreefficient simply to use an explicit set of edges in-stead of the boundary representation.
For the Geo-Query corpus (Tang and Mooney, 2001), whosegraphs are only 7.4 nodes on average, we gener-ally find this to be the case.3.2 TreewidthLautemann?s algorithm tries to match a ruleagainst the input graph all at once.
But we can op-timize the algorithm by matching a rule incremen-tally.
This is analogous to the rank-minimizationproblem for linear context-free rewriting systems.Gildea has shown that this problem is related to927the notion of treewidth (Gildea, 2011), which wereview briefly here.Definition 7.
A tree decomposition of a graphH = ?V, E?
is a tree T , each of whose nodes ?is associated with sets V?
?
V and E?
?
E, withthe following properties:1.
Vertex cover: For each v ?
V , there is a node?
?
T such that v ?
V?.2.
Edge cover: For each e = (v1 ?
?
?
vk) ?
E,there is exactly one node ?
?
T such that e ?E?.
We say that ?
introduces e. Moreover,v1, .
.
.
, vk ?
V?.3.
Running intersection: For each v ?
V , the set{?
?
T | v ?
V?}
is connected.The width of T is max |V?| ?
1.
The treewidth of His the minimal width of any tree decompositionof H.A tree decomposition of a graph fragment?V, E, X?
is a tree decomposition of ?V, E?
that hasthe additional property that all the external nodesbelong to V?
for some ?.
(Without loss of general-ity, we assume that ?
is the root.
)For example, Figure 3b shows a graph, and Fig-ure 3c shows a tree decomposition.
This decom-position has width three, because its largest nodehas 4 elements.
In general, a tree has width one,and it can be shown that a graph has treewidth atmost two iff it does not have the following graphas a minor (Bodlaender, 1997):K4 =Finding a tree decomposition with minimalwidth is in general NP-hard (Arnborg et al, 1987).However, we find that for the graphs we are inter-ested in in NLP applications, even a na?
?ve algo-rithm gives tree decompositions of low width inpractice: simply perform a depth-first traversal ofthe edges of the graph, forming a tree T .
Then,augment the V?
as necessary to satisfy the runningintersection property.As a test, we extracted rules from the Geo-Query corpus (Tang and Mooney, 2001) using theSynSem algorithm (Jones et al, 2012), and com-puted tree decompositions exactly using a branch-and-bound method (Gogate and Dechter, 2004)and this approximate method.
Table 1 shows that,in practice, treewidths are not very high even whencomputed only approximately.method mean maxexact 1.491 2approximate 1.494 3Table 1: Mean and maximum treewidths of rulesextracted from the GeoQuery corpus, using exactand approximate methods.
(a) 0abelieve?
arg1bgirl?arg01Y(b) 010b 10ab 1arg1ab 1Y?0barg0bgirl??0believe?
?Figure 3: (a) A rule right-hand side, and (b) a nicetree decomposition.Any tree decomposition can be converted intoone which is nice in the following sense (simpli-fied from Cygan et al (2011)).
Each tree node ?must be one of:?
A leaf node, such that V?
= ?.?
A unary node, which introduces exactly oneedge e.?
A binary node, which introduces no edges.The example decomposition in Figure 3c is nice.This canonical form simplifies the operation of theparser described in the following section.Let G be a HRG.
For each production (A ?R) ?
G, find a nice tree decomposition of R andcall it TR.
The treewidth of G is the maximum928treewidth of any right-hand side in G.The basic idea of the recognition algorithm isto recognize the right-hand side of each rule incre-mentally by working bottom-up on its tree decom-position.
The properties of tree decomposition al-low us to limit the number of boundary nodes ofthe partially-recognized rule.More formally, let RD?
be the subgraph of R in-duced by the union of E??
for all ??
equal to ordominated by ?.
Then we can show the following.Proposition 3.
Let R be a graph fragment, and as-sume a tree decomposition of R. All the boundarynodes of RD?
belong to V?
?
Vparent(?).Proof.
Let v be a boundary node of RD?.
Node vmust have an edge in RD?
and therefore in R??
forsome ??
dominated by or equal to ?.Case 1: v is an external node.
Since the rootnode contains all the external nodes, by the run-ning intersection property, both V?
and Vparent(?
)must contain v as well.Case 2: v has an edge not in RD?.
Thereforethere must be a tree node not dominated by orequal to ?
that contains this edge, and thereforev.
So by the running intersection property, ?
andits parent must contain v as well.
This result, in turn, will allow us to bound thecomplexity of the parsing algorithm in terms of thetreewidth of G.3.3 Inference rulesWe present the parsing algorithm as a deductivesystem (Shieber et al, 1995).
The items haveone of two forms.
A passive item has the form[A, I, X], where X ?
V+ is an explicit orderingof the boundary nodes of I.
This means that wehave recognized that A ?
?G I.
Thus, the goalitem is [S ,H, ?].
An active item has the form[A?
R, ?, I, ?
], where?
(A?
R) is a production of G?
?
is a node of TR?
I is a subgraph of H?
?
is a bijection between the boundary nodesof RD?
and those of I.The parser must ensure that ?
is a bijection whenit creates a new item.
Below, we use the notation{e 7?
e?}
or {e 7?
X} for the mapping that sendseach node of e to the corresponding node of e?or X.Passive items are generated by the followingrule:?
Root [B?
Q, ?, J, ?
][B, J, X]where ?
is the root of TQ, and X j = ?
(XQ, j).If we assume that the TR are nice, then the in-ference rules that generate active items follow thedifferent types of nodes in a nice tree decomposi-tion:?
Leaf[A?
R, ?, ?, ?
]where ?
is a leaf node of TR.?
(Unary) Nonterminal[A?
R, ?1, I, ?]
[B, J, X][A?
R, ?, I ?
J, ?
?
{e 7?
X}]where ?1 is the only child of ?, and e is intro-duced by ?
and is labeled with nonterminal B.?
(Unary) Terminal[A?
R, ?1, I, ?][A?
R, ?, I ?
{e?
}, ?
?
{e 7?
e?
}]where ?1 is the only child of ?, e is introducedby ?, and e and e?
are both labeled with ter-minal a.?
Binary[A?
R, ?1, I, ?1] [A?
R, ?2, J, ?2][A?
R, ?, I ?
J, ?1 ?
?2]where ?1 and ?2 are the two children of ?.In the Nonterminal, Terminal, and Binary rules,we form unions of subgraphs and unions of map-pings.
When forming the union of two subgraphs,we require that the subgraphs be disjoint (however,see Section 3.4 below for a relaxation of this con-dition).
When forming the union of two mappings,we require that the result be a bijection.
If eitherof these conditions is not met, the inference rulecannot apply.For efficiency, it is important to index the itemsfor fast access.
For the Nonterminal inferencerule, passive items [B, J, X] should be indexed bykey ?B, |bn(J)|?, so that when the next item on theagenda is an active item [A ?
R, ?1, I, ?
], weknow that all possible matching passive items are929S ?XXXX ?aa aaa(a) (b)aa aa aa(c)Figure 4: Illustration of unsoundness in the recog-nition algorithm without the disjointness check.Using grammar (a), the recognition algorithmwould incorrectly accept the graph (b) by assem-bling together the three overlapping fragments (c).under key ??
(e), |e|?.
Similarly, active items shouldbe indexed by key ??
(e), |e|?
so that they can befound when the next item on the agenda is a pas-sive item.
For the Binary inference rule, activeitems should be indexed by their tree node (?1or ?2).This procedure can easily be extended to pro-duce a packed forest of all possible derivationsof the input graph, representable as a hypergraphjust as for other context-free rewriting formalisms.The Viterbi algorithm can then be applied tothis representation to find the highest-probabilityderivation, or the Inside/Outside algorithm to setweights by Expectation-Maximization.3.4 The disjointness checkA successful proof using the inference rules abovebuilds an HRG derivation (comprising all therewrites used by the Nonterminal rule) which de-rives a graph H?, as well as a graph isomorphism?
: H?
?
H (the union of the mappings from allthe items).During inference, whenever we form the unionof two subgraphs, we require that the subgraphsbe disjoint.
This is a rather expensive operation:it can be done using only their boundary represen-tations, but the best algorithm we are aware of isstill quadratic in the number of boundary nodes.Is it possible to drop the disjointness check?
Ifwe did so, it would become possible for the algo-rithm to recognize the same part of H twice.
Forexample, Figure 4 shows an example of a grammarand an input that would be incorrectly recognized.However, we can replace the disjointness checkwith a weaker and faster check such that anyderivation that merges two non-disjoint subgraphswill ultimately fail, and therefore the derivedgraph H?
is isomorphic to the input graph H?
asdesired.
This weaker check is to require, whenmerging two subgraphs I and J, that:1.
I and J have no boundary edges in common,and2.
If m belongs to both I and J, it must be aboundary node of both.Condition (1) is enough to guarantee that ?
is lo-cally one-to-one in the sense that for all v ?
H?, ?restricted to v and its neighbors is one-to-one.
Thisis easy to show by induction: if ?I : I?
?
H and?J : J?
?
H are locally one-to-one, then ?I ?
?Jmust also be, provided condition (1) is met.
Intu-itively, the consequence of this is that we can de-tect any place where ?
changes (say) from beingone-to-one to two-to-one.
So if ?
is two-to-one,then it must be two-to-one everywhere (as in theexample of Figure 4).But condition (2) guarantees that ?
maps onlyone node to the marker m. We can show this againby induction: if ?I and ?J each map only one nodeto m, then ?I?
?J must map only one node to m, bya combination of condition (2) and the fact that theinference rules guarantee that ?I , ?J , and ?I ?
?Jare one-to-one on boundary nodes.Then we can show that, since m is recognizedexactly once, the whole graph is also recognizedexactly once.Proposition 4.
If H and H?
are connected graphs,?
: H?
?
H is locally one-to-one, and ?
?1 is de-fined for some node of H, then ?
is a bijection.Proof.
Suppose that ?
is not a bijection.
Thenthere must be two nodes v?1, v?2 ?
H?
such that?
(v?1) = ?
(v?2) = v ?
H. We also know that thereis a node, namely, m, such that m?
= ?
?1(m) is de-fined.2 Choose a path ?
(ignoring edge direction)from v to m. Because ?
is a local isomorphism,we can construct a path from v?1 to m?
that mapsto ?.
Similarly, we can construct a path from v?2to m?
that maps to ?.
Let u?
be the first node thatthese two paths have in common.
But u?
must havetwo edges that map to the same edge, which is acontradiction.
2If H were not connected, we would choose the marker inthe same connected component as v.9303.5 ComplexityThe key to the efficiency of the algorithm is thatthe treewidth of G leads to a bound on the numberof boundary nodes we must keep track of at anytime.Let k be the treewidth of G. The time complex-ity of the algorithm is the number of ways of in-stantiating the inference rules.
Each inference rulementions only boundary nodes of RD?
or RD?i , allof which belong to V?
(by Proposition 3), so thereare at most |V?| ?
k + 1 of them.
In the Nonter-minal and Binary inference rules, each boundaryedge could belong to I or J or neither.
Therefore,the number of possible instantiations of any infer-ence rule is in O((3dn)k+1).The space complexity of the algorithm is thenumber of possible items.
For each active item[A?
R, ?, I, ?
], every boundary node of RD?
mustbelong to V??Vparent(?)
(by Proposition 3).
There-fore the number of boundary nodes is at most k+1(but typically less), and the number of possibleitems is in O((2dn)k+1).4 Synchronous ParsingAs mentioned in Section 2.2, because HRGs havecontext-free derivation trees, it is easy to definesynchronous HRGs, which define mappings be-tween languages of graphs.Definition 8.
A synchronous hyperedge re-placement grammar (SHRG) is a tuple G =?N, T, T ?, P, S ?, where?
N is a finite set of nonterminal symbols?
T and T ?
are finite sets of terminal symbols?
S ?
N is the start symbol?
P is a finite set of productions of the form(A?
?R,R?,??
), where R is a graph fragmentover N ?
T and R?
is a graph fragment overN ?
T ?.
The relation ?
is a bijection linkingnonterminal mentions in R and R?, such thatif e ?
e?, then they have the same label.
Wecall R the source side and R?
the target side.Some NLP applications (for example, wordalignment) require synchronous parsing: given apair of graphs, finding the derivation or forest ofderivations that simultaneously generate both thesource and target.
The algorithm to do this is astraightforward generalization of the HRG parsingalgorithm.
For each rule (A?
?R,R?,??
), we con-struct a nice tree decomposition of R?R?
such that:?
All the external nodes of both R and R?
be-long to V?
for some ?.
(Without loss of gen-erality, assume that ?
is the root.)?
If e ?
e?, then e and e?
are introduced by thesame tree node.In the synchronous parsing algorithm, passiveitems have the form [A, I, X, I?, X?]
and activeitems have the form [A?
R : R?, ?, I, ?, I?, ??
].For brevity we omit a re-presentation of all the in-ference rules, as they are very similar to their non-synchronous counterparts.
The main difference isthat in the Nonterminal rule, two linked edges arerewritten simultaneously:[A?
R : R?, ?1, I, ?, I?, ??]
[B, J, X, J?, X?][A?
R : R?, ?, I ?
J, ?
?
{e j 7?
X j},I?
?
J?, ??
?
{e?j 7?
X?j}]where ?1 is the only child of ?, e and e?
are bothintroduced by ?
and e ?
e?, and both are labeledwith nonterminal B.The complexity of the parsing algorithm isagain in O((3dn)k+1), where k is now the max-imum treewidth of the dependency graph as de-fined in this section.
In general, this treewidth willbe greater than the treewidth of either the source ortarget side on its own, so that synchronous parsingis generally slower than standard parsing.5 ConclusionAlthough Lautemann?s polynomial-time extensionof CKY to HRGs has been known for some time,the desire to use graph grammars for large-scaleNLP applications introduces some practical con-siderations not accounted for in Lautemann?s orig-inal presentation.
We have provided a detailed de-scription of our refinement of his algorithm and itsimplementation.
It runs in O((3dn)k+1) time andrequires O((2dn)k+1) space, where n is the num-ber of nodes in the input graph, d is its maximumdegree, and k is the maximum treewidth of therule right-hand sides in the grammar.
We havealso described how to extend this algorithm tosynchronous parsing.
The parsing algorithms de-scribed in this paper are implemented in the Boli-nas toolkit.33The Bolinas toolkit can be downloaded from?http://www.isi.edu/licensed-sw/bolinas/?.931AcknowledgementsWe would like to thank the anonymous reviewersfor their helpful comments.
This research was sup-ported in part by ARO grant W911NF-10-1-0533.ReferencesStefan Arnborg, Derek G. Corneil, and AndrzejProskurowski.
1987.
Complexity of finding embed-dings in a k-tree.
SIAM Journal on Algebraic andDiscrete Methods, 8(2).Hans L. Bodlaender.
1997.
Treewidth: Algorithmictechniques and results.
In Proc.
22nd InternationalSymposium on Mathematical Foundations of Com-puter Science (MFCS ?97), pages 29?36, Berlin.Springer-Verlag.Marek Cygan, Jesper Nederlof, Marcin Pilipczuk,Micha?
Pilipczuk, Johan M. M. van Rooij, andJakub Onufry Wojtaszczyk.
2011.
Solving connec-tivity problems parameterized by treewidth in singleexponential time.
Computing Research Repository,abs/1103.0534.Frank Drewes, Hans-Jo?rg Kreowski, and Annegret Ha-bel.
1997.
Hyperedge replacement graph gram-mars.
In Grzegorz Rozenberg, editor, Handbook ofGraph Grammars and Computing by Graph Trans-formation, pages 95?162.
World Scientific.Daniel Gildea.
2011.
Grammar factorization bytree decomposition.
Computational Linguistics,37(1):231?248.Vibhav Gogate and Rina Dechter.
2004.
A completeanytime algorithm for treewidth.
In Proceedings ofthe Conference on Uncertainty in Artificial Intelli-gence.Bevan Jones, Jacob Andreas, Daniel Bauer,Karl Moritz Hermann, and Kevin Knight.
2012.Semantics-based machine translation with hyper-edge replacement grammars.
In Proc.
COLING.Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In Grzegorz Rozenberg andArto Salomaa, editors, Handbook of Formal Lan-guages and Automata, volume 3, pages 69?124.Springer.Clemens Lautemann.
1990.
The complexity ofgraph languages generated by hyperedge replace-ment.
Acta Informatica, 27:399?421.Steffen Mazanek and Mark Minas.
2008.
Parsing ofhyperedge replacement grammars with graph parsercombinators.
In Proc.
7th International Work-shop on Graph Transformation and Visual ModelingTechniques.Richard Moot.
2008.
Lambek grammars, tree ad-joining grammars and hyperedge replacement gram-mars.
In Proc.
TAG+9, pages 65?72.Grzegorz Rozenberg and Emo Welzl.
1986.
Bound-ary NLC graph grammars?basic definitions, nor-mal forms, and complexity.
Information and Con-trol, 69:136?167.Stuart M. Shieber, Yves Schabes, and Fernando C. N.Pereira.
1995.
Principles and implementation ofdeductive parsing.
Journal of Logic Programming,24:3?36.Lappoon Tang and Raymond Mooney.
2001.
Usingmultiple clause constructors in inductive logic pro-gramming for semantic parsing.
In Proc.
EuropeanConference on Machine Learning.932
