Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 101?104,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPLeveraging Structural Relations for Fluent Compressionsat Multiple Compression RatesSourish Chaudhuri, Naman K. Gupta, Noah A. Smith, Carolyn P. Ros?Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA-15213, USA.
{sourishc, nkgupta, nasmith, cprose}@cs.cmu.eduAbstractPrior approaches to sentence compressionhave taken low level syntactic constraints intoaccount in order to maintain grammaticality.We propose and successfully evaluate a morecomprehensive, generalizable feature set thattakes syntactic and structural relationships intoaccount in order to sustain variable compres-sion rates while making compressed sentencesmore coherent, grammatical and readable.1 IntroductionWe present an evaluation of the effect of syntac-tic and structural constraints at multiple levels ofgranularity on the robustness of sentence com-pression at varying compression rates.
Our eval-uation demonstrates that the new feature set pro-duces significantly improved compressionsacross a range of compression rates compared toexisting state-of-the-art approaches.
Thus, wename our system for generating compressions theAdjustable Rate Compressor (ARC).Knight and Marcu (2000) (K&M, henceforth)presented two approaches to the sentence com-pression problem: one using a noisy channelmodel, the other using a decision-based model.The performances of the two models were com-parable though their experiments suggested thatthe noisy channel model degraded more smooth-ly than the decision-based model when tested onout-of-domain data.
Riezler et al (2003) appliedlinguistically rich LFG grammars to a sentencecompression system.
Turner and Charniak (2005)achieved similar performance to K&M using anunsupervised approach that induced rules fromthe Penn Treebank.A variety of feature encodings have previous-ly been explored for the problem of sentencecompression.
Clarke and Lapata (2007) includeddiscourse level features in their framework toleverage context for enhancing coherence.McDonald?s (2006) model (M06, henceforth) issimilar to K&M except that it uses discriminativeonline learning to train feature weights.
A keyaspect of the M06 approach is a decoding algo-rithm that searches the entire space of compres-sions using dynamic programming to choose thebest compression (details in Section 2).
We useM06 as a foundation for this work because itssoft constraint approach allows for natural inte-gration of additional classes of features.
Similarto most previous approaches, our approach com-presses sentences by deleting words only.The remainder of the paper is organized asfollows.
Section 2 discusses the architecturalframework.
Section 3 describes the innovationsin the proposed model.
We conclude after pre-senting the results of our evaluation in Section 4.2 Experimental ParadigmSupervised approaches to sentence compressiontypically use parallel corpora consisting of origi-nal and compressed sentences (paired corpus,henceforth).
In this paper, we will refer to thesepairs as a 2-tuple <x, y>, where x is the originalsentence and y is the compressed sentence.We implemented the M06 system as an expe-rimental framework in which to conduct our in-vestigation.
The system uses as input the pairedcorpus, the corresponding POS tagged corpus,the paired corpus parsed using the Charniakparser (Charniak, 2000), and dependency parsesfrom the MST parser (McDonald et al, 2005).Features are extracted over adjacent pairs ofwords in the compressed sentence and weightsare learnt at training time using the MIRA algo-rithm (Crammer and Singer, 2003).
We decodeas follows to find the best compression:Let the score of a compression y for a sen-tence x be s(x, y).
This score is factored using afirst-order Markov assumption over the words inthe compressed sentence, and is defined by thedot product between a high dimensional featurerepresentation and a corresponding weight vector(for details, refer to McDonald, 2006).
The equa-tions for decoding are as follows:1),,,(][max][0.0]1[iijxsjCiCCij101where C is the dynamic programming table andC[i] represents the highest score for compres-sions ending at word i for the sentence x.The M06 system takes the best scoring com-pression from the set of all possible compres-sions.
In the ARC system, the model determinesthe compression rate and enforces a target com-pression length by altering the dynamic pro-gramming algorithm as suggested by M06:1,]][1[0.0]1][1[rrCC,1i),,(]1][[max]][[ ijxsrjCriC ijwhere C is the dynamic programming table asbefore and C[i][r] is the score for the best com-pression of length r that ends at position i in thesentence x.
This algorithm runs in O (n2r) time.We define the rate of human generated com-pressions in the training corpus as the gold stan-dard compression rate (GSCR).
We train a linearregression model over the training data to predictthe GSCR for a sentence based on the ratio be-tween the lengths of each compressed-originalsentence pair in the training set.
The predictedcompression rate is used to force the system tocompress sentences in the test set to a specifictarget length.
Based on the computed regression,the formula for computing the Predicted Com-pression Rate (PCR) from the Original SentenceLength (OSL) is as follows:OSLPCR 004.086.0In our work, enforcing specific compressionrates serves two purposes.
First, it allows us tomake a more controlled comparison across ap-proaches, since variation in compression rateacross approaches confounds comparison of oth-er aspects of performance.
Second, it allows usto investigate how alternative models work athigher compression rates.
Here our primary con-tribution is of robustness of the approach withrespect to alternative feature spaces and com-pression rates.3 Extended Feature SetA major focus of our work is the inclusion ofnew types of features derived from syntactic ana-lyses in order to make the resulting compressionsmore grammatical and thus increase the versatili-ty of the resulting compression models.The M06 system uses features extracted fromthe POS tagged paired corpus: POS bigrams,POS context of the words added to or droppedfrom the compression, and other informationabout the dropped words.
For a more detaileddescription, please refer to McDonald, 2006.From the phrase structure trees, M06 extractscontext information about nodes that subsumedropped words.
These features attempt to ap-proximately encode changes in the grammarrules between source and target sentences.
De-pendency features include information about thedropped words?
parents as well as conjunctionfeatures of the word and the parent.Our extensions to the M06 feature set are in-spired by an analysis of the compressions gener-ated by it, and allow for a richer encoding ofdropped words and phrases using properties ofthe words and their syntactic relations to the restof the sentence.
Consider this example (droppedwords are marked as such):* 68000 Sweden AB of Uppsala , Sweden , intro-duced the TeleServe , an integrated answeringmachine and voice-message handler that links aMacintosh to Touch-Tone phones .Note in the above example that the syntactichead of the sentence introduced has beendropped.
Using the dependency parse, we add aclass of features to be learned during training thatlets the system decide when to drop the syntactichead of the sentence.
Also note that answeringmachine in the original sentence was precededby an while the word the was used with Tele-serve (dropped in the compression).
While POSinformation helps the system to learn that theanswering machine is a good POS sequence, wedo not have information that links the correctarticle to the noun.
Information from the depen-dency parse allows us to learn when we can dropwords whose heads are retained and when wecan drop a head and still retain the dependent.Now, consider the following example:Examples for editors are applicable to awk pat-terns , grep and egrep .Here, Examples has been dropped, while foreditors which has Examples as a head is retained.Besides, in the sequence, editors are applica-ble?, the word editors behaves as the subject ofare although the correct compression would haveexamples as its subject.
A change in the argu-ments of the verbs will distort the meaning of thesentence.
We augmented the feature set to in-clude a class of features about structural informa-tion that tells us when the subject (or object) of averb can be dropped while the verb itself is re-tained.
Thus, now if the system does retain the102are, it is more likely to retain the correct argu-ments of the word from the original sentence.The new classes of features use only the de-pendency labels generated by the parser and arenot lexicalized.
Intuitively, these features helpcreate units within the sentences that are tightlybound together, e.g., a subject and an object withits parent verb.
We notice, as one would expect,that some dependency bindings are less strongthan others.
For instance, when faced with achoice, our system drops a relative pronoun thusbreaking the dependency between the retainednoun and the relative pronoun, rather than dropthe noun, which was the retained subject.Below is a summary of the information thatthe new features in our system encode:[Parent-Child]- When a word is dropped, is itsparent retained in the compression?
[Dependent]- When a word is dropped, areother words dependent on it (its children)also dropped or are they retained?
[Verb-Arg]- Information from the dependencyparse about the subjects and objects ofverbs can be used to encode more specificfeatures (similar to the above) that saywhether or not the subject (or object) wasretained when the verb was dropped.
[Sent-Head-Dep]- Is the syntactic head of asentence dropped?4 EvaluationWe evaluate our model in comparison with M06.At training time, compression rates were not en-forced on the ARC or M06 model.
Our evalua-tion demonstrates that the proposed feature setproduces more grammatical sentences acrossvarying compression rates.
In this section,GSCR denotes gold standard compression rate(i.e., the compression rate found in training data),CR denotes compression rate.4.1 CorporaSentence compression systems have been testedon product review data from the Ziff-Davis (ZD,henceforth) Corpus by Knight and Marcu (2000),general news articles by Clarke and Lapata (CL,henceforth) corpus (2007) and biomedical ar-ticles (Lin and Wilbur, 2007).
To evaluate oursystem, we used 2 test sets: Set 1 contained 50sentences; all 32 sentences from the ZD test setand 18 additional sentences chosen randomlyfrom the CL test set; Set 2 contained 40 sen-tences selected from the CL corpus, 20 of whichwere compressed at 75% of GSCR and 20 at50% of GSCR (the percentages denote the en-forced compression rates).Three examples comparing compressed sen-tences are given below:Original: Like FaceLift, much of ATM 's screenperformance depends on the underlying applica-tion.Human: Much of ATM 's performance dependson the underlying application .M06: 's screen performance depends on applica-tionARC: ATM 's screen performance depends onthe underlying application .Original: The discounted package for the Sparc-server 470 is priced at $89,900 , down from theregular $107,795 .Human: The Sparcserver 470 is priced at$89,900 , down from the regular $107,795 .M06: Sparcserver 470 is $89,900 regular$107,795ARC: The discounted package is priced at$89,900 , regular $107,795 .The example below has compressions at 50%compression rate for M06 and ARC systems:Original: Cutbacks in local defence establish-ments is also a factor in some constituencies .M06: establishments is a factor in some consti-tuencies .ARC: Cutbacks is a factor in some constituen-cies .Note that the subject of is is correctly retainedin the ARC system.4.2 User StudyIn order to evaluate the effect of the features thatwe added to create the ARC model, we con-ducted a user study, adopting an experimentalmethodology similar to that used by K&M andM06.
Each of four human judges, who were na-tive speakers of English and not involved in theresearch we report in this paper, were instructedto rate two different sets of compressions alongtwo dimensions, namely Grammaticality andCompleteness, on a scale of 1 to 5.
We chose toreplace Importance (used by K&M), which is atask specific and possibly user specific notion,with the more general notion of Completeness,defined as the extent to which the compressedsentence is a complete sentence and communi-cates the main idea of the original sentence.For Set 1, raters were given the original sen-tence and 4 compressed versions (presented in103random order as in the M06 evaluation): the hu-man compression, the compression produced bythe original M06 system, the compression fromthe M06 system with GSCR, and the ARC sys-tem with GSCR.
For Set 2, raters were given theoriginal sentence, this time with two compressedversions, one from the M06 system and one fromthe ARC system, which were presented in a ran-dom order.
Table 1 presents all the results interms of human ratings of Grammaticality andCompleteness as well as automatically computedROUGE F1 scores (Lin and Hovy, 2003).
Thescores in parentheses denote standard deviations.Grammati-cality(HumanScores)Com-pleteness(HumanScores)ROUGEF1GoldStandard4.60 (0.69) 3.80(.99) 1.00 (0)ARC(GSCR)3.70 (1.10) 3.50(1.10) .72 (.18)M06 3.50 (1.30) 3.10(1.30) .70 (.20)M06(GSCR)3.10 (1.10) 3.10(1.10) .71 (.18)ARC(75%CR)2.60 (1.10) 2.60(1.10) .72 (.14)M06(75%CR)2.20 (1.20) 2.00(1.00) .67 (.20)ARC(50%CR)2.30 (1.30) 1.90(1.00) .54 (.22)M06(50%CR)1.90 (1.10) 1.80(1.00) .58 (.22)Table 1: Results of human judgments and ROUGE F1ROUGE scores were determined to have asignificant positive correlation both with Gram-maticality (R = .46, p < .0001) and Completeness(R = .39, p < .0001) when averaging across the 4judges?
ratings.
On Set 1, a 2-tailed paired t-testreveals similar patterns for Grammaticality andCompleteness: the human compressions are sig-nificantly better than any of the systems.
ARC issignificantly better than M06, both with enforcedGSCR and without.
M06 without GSCR is sig-nificantly better than M06 with GSCR.
In Set 2(with 75% and 50% GSCR enforced), the qualityof compressions degrade as compression rate ismade more severe; however, the ARC modelconsistently outperforms the M06 model with astatistically significant margin across compres-sion rates on both evaluation criteria.5 Conclusions and Future WorkIn this paper, we designed a set of new classes offeatures to generate better compressions, andthey were found to produce statistically signifi-cant improvements over the state-of-the-art.However, although the user study demonstratesthe expected positive impact of grammatical fea-tures, an error analysis (Gupta et al, 2009) re-veals some limitations to improvements that canbe obtained using grammatical features that referonly to the source sentence structure, since thesyntax of the source sentence is frequently notpreserved in the gold standard compression.
Inour future work, we hope to explore alternativeapproaches that allow reordering or paraphrasingalong with deleting words to make compressedsentences more grammatical and coherent.AcknowledgmentsThe authors thank Kevin Knight and DanielMarcu for sharing the Ziff-Davis corpus as wellas the output of their systems, and the anonym-ous reviewers for their comments.
This work wassupported by the Cognitive and Neural SciencesDivision, grant number N00014-00-1-0600.ReferencesEugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proc.
of  NAACL.James Clarke and Mirella Lapata, 2007.
ModellingCompression With Discourse Constraints.
In Proc.of EMNLP-CoNLL.Koby Crammer and Y.
Singer.
2003.
Ultraconserva-tive online algorithms for multi-class problems.JMLR.Naman K. Gupta, Sourish Chaudhuri and Carolyn P.Ros?, 2009.
Evaluating the Syntactic Transforma-tions in Gold Standard Corpora for Statistical Sen-tence Compression .
In Proc.
of HLT-NAACL.Kevin Knight and Daniel Marcu.
2000.
Statistics-Based Summarization ?
Step One: Sentence Com-pression.
In Proc.
of AAAI.Jimmy Lin and W. John Wilbur.
2007.
Syntactic sen-tence compression in the biomedical domain: faci-litating access to related articles.
Information Re-trieval, 10(4):393-414.Chin-Yew Lin and Eduard H. Hovy 2003.
AutomaticEvaluation of Summaries Using N-gram Co-occurrence Statistics.
In Proc.
of HLT-NAACL.Ryan McDonald, 2006.
Discriminative sentence com-pression with soft syntactic constraints.
In Proc.
ofEACL.Ryan McDonald, Koby Crammer, and Fernando Pe-reira.
2005.
Online large-margin training of depen-dency parsers.
In Proc.of ACL.S.
Riezler, T. H. King, R. Crouch, and A. Zaenen.2003.
Statistical sentence condensation using am-biguity packing and stochastic disambiguation me-thods for lexical-functional grammar.
In Proc.
ofHLT-NAACL.104
