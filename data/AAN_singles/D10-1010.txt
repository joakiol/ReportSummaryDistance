Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 97?107,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsLearning the Relative Usefulness of Questions in Community QARazvan BunescuSchool of EECSOhio UniversityAthens, OH 43201, USAbunescu@ohio.eduYunfeng HuangSchool of EECSOhio UniversityAthens, OH 43201, USAyh324906@ohio.eduAbstractWe present a machine learning approach forthe task of ranking previously answered ques-tions in a question repository with respect totheir relevance to a new, unanswered refer-ence question.
The ranking model is trainedon a collection of question groups manuallyannotated with a partial order relation reflect-ing the relative utility of questions inside eachgroup.
Based on a set of meaning and struc-ture aware features, the new ranking model isable to substantially outperformmore straight-forward, unsupervised similarity measures.1 IntroductionOpen domain Question Answering (QA) is one ofthe most complex and challenging tasks in naturallanguage processing.
In general, a question answer-ing system may need to integrate knowledge comingfrom a wide variety of linguistic processing taskssuch as syntactic parsing, semantic role labeling,named entity recognition, and anaphora resolution(Prager, 2006).
State of the art implementations ofthese linguistic analysis tasks are still limited in theirperformance, with errors that compound and prop-agate into the final performance of the QA system(Moldovan et al, 2002).
Consequently, the perfor-mance of open domain QA systems has yet to ar-rive at a level at which it would become a feasiblealternative to the current paradigms for informationaccess based on keyword searches.Recently, community-driven QA sites such as Ya-hoo!
Answers and WikiAnswers 1 have established1answers.yahoo.com, wiki.answers.coma new approach to question answering that shiftsthe inherent complexity of open domain QA fromthe computer system to volunteer contributors.
Thecomputer is no longer required to perform a deeplinguistic analysis of questions and generate corre-sponding answers, and instead acts as a mediatorbetween users submitting questions and volunteersproviding the answers.An important objective in community QA is tominimize the time elapsed between the submissionof questions by users and the subsequent postingof answers by volunteer contributors.
One usefulstrategy for minimizing the response latency is tosearch the QA repository for similar questions thathave already been answered, and provide the cor-responding ranked list of answers, if such a ques-tion is found.
The success of this approach de-pends on the definition and implementation of thequestion-to-question similarity function.
In the sim-plest solution, the system searches for previouslyanswered questions based on exact string match-ing with the reference question.
Alternatively, sitessuch as WikiAnswers allow the users to mark ques-tions they think are rephrasings (?alternate word-ings?, or paraphrases) of existing questions.
Thesequestion clusters are then taken into account whenperforming exact string matching, therefore increas-ing the likelihood of finding previously answeredquestions that are semantically equivalent to the ref-erence question.In order to lessen the amount of work requiredfrom the contributors, an alternative approach is tobuild a system that automatically finds rephrasingsof questions, especially since question rephrasing97seems to be computationally less demanding thanquestion answering.
According to previous work inthis domain, a question is considered a rephrasing ofa reference question Q0 if it uses an alternate word-ing to express an identical information need.
Forexample, Q0 and Q1 below are rephrasings of eachother, and consequently they are expected to havethe same answer.Q0 What should I feed my turtle?Q1 What do I feed my pet turtle?Paraphrasings of a new question cannot always befound in the community QA repository.
We believethat computing a ranked list of existing questionsthat at least partially address the original informa-tion need could also be useful to the user, at leastuntil other users volunteer to give an exact answerto the original, unanswered reference question.
Forexample, in the absence of any additional informa-tion about the reference question Q0, the expectedanswers to questions Q2 and Q3 below may be seenas partially overlapping in information content withthe expected answer for the reference question Q0.An answer to question Q4, on the other hand, is lesslikely to benefit the user, even though it has a signif-icant lexical overlap with the reference question.Q2 What kind of fish should I feed my turtle?Q3 What do you feed a turtle that is the size of aquarter?Q4 What kind of food should I feed a turtle dove?In this paper, we propose a supervised learningapproach to the question ranking problem, a gen-eralization of the question paraphrasing problem inwhich questions are ranked in a partial order basedon the relative information overlap between their ex-pected answers and the expected answer of the refer-ence question.
Underlying the question ranking taskis the expectation that the user who submits a ref-erence question will find the answers of the highlyranked questions to be more useful than the answersassociated with the lower ranked questions.
For thereference question Q0 above, the learned rankingmodel is expected to produce a partial order in whichQ1 is ranked higher than Q2, Q3 and Q4, whereasQ2 and Q3 are ranked higher than Q4.2 Partially Ordered Datasets for QuestionRankingIn order to enable the evaluation of question rank-ing approaches, we have previously created a datasetof 60 groups of questions (Bunescu and Huang,2010b).
Each group consists of a reference question(e.g.
Q0 above) that is associated with a partially or-dered set of questions (e.g.
Q1 to Q4 above).
Foreach reference questions, its corresponding partiallyordered set is created from questions in Yahoo!
An-swers and other online repositories that have a highcosine similarity with the reference question.
Outof the 26 top categories in Yahoo!
Answers, the 60reference questions span a diverse set of categories.Figure 1 lists the 20 categories covered, where eachcategory is shown with the number of correspondingreference questions between parentheses.Travel (10), Computers & Internet (6),Beauty & Style (5), Entertainment &Music (5), Food & Drink (5), Health (5),Arts & Humanities (3), Cars &Transportation (3), Consumer Electronics(3), Pets (3), Family & Relationships(2), Science & Mathematics (2),Education & Reference (1), Environment(1), Local Businesses (1), Pregnancy &Parenting (1), Society & Culture (1),Sports (1), Yahoo!
Products (1)Figure 1: The 20 categories represented in the dataset.Inside each group, the questions are manually an-notated with a partial order relation, according totheir utility with respect to the reference question.We use the notation ?Qi ?
Qj |Qr?
to encode thefact that questionQi is more useful than questionQjwith respect to the reference question Qr.
Similarly,?Qi = Qj?will be used to express the fact that ques-tions Qi and Qj are reformulations of each other(the reformulation relation is independent of the ref-erence question).
The partial ordering among thequestionsQ0 toQ4 above can therefore be expressedconcisely as follows: ?Q0 = Q1?, ?Q1 ?
Q2|Q0?,?Q1 ?
Q3|Q0?, ?Q2 ?
Q4|Q0?, ?Q3 ?
Q4|Q0?.Note that we do not explicitly annotate the rela-tion ?Q1 ?
Q4|Q0?, since it can be inferred basedon the transitivity of the more useful than relation:?Q1 ?
Q2|Q0??
?Q2 ?
Q4|Q0?
?
?Q1 ?
Q4|Q0?.98REFERENCE QUESTION (Qr)Q5 What?s a nice summer camp to go to in Florida?PARAPHRASING QUESTIONS (P )Q6 What camps are good for a vacation during the summer in FL?Q7 What summer camps in FL do you recommend?USEFUL QUESTIONS (U )Q8 Does anyone know a good art summer camp to go to in FL?Q9 Are there any good artsy camps for girls in FL?Q10 What are some summer camps for like singing in Florida?Q11 What is a good cooking summer camp in FL?Q12 Do you know of any summer camps in Tampa, FL?Q13 What is a good summer camp in Sarasota FL for a 12 year old?Q14 Can you please help me find a surfing summer camp for beginners in Treasure Coast, FL?Q15 Are there any acting summer camps and/or workshops in the Orlando, FL area?Q16 Does anyone know any volleyball camps in Miramar, FL?Q17 Does anyone know about any cool science camps in Miami?Q18 What?s a good summer camp you?ve ever been to?NEUTRAL QUESTIONS (N )Q19 What?s a good summer camp in Canada?Q20 What?s the summer like in Florida?Table 1: A question group.Also note that no relation is specified between Q2and Q3, and similarly no relation can be inferred be-tween these two questions.
This reflects our beliefthat, in the absence of any additional information re-garding the user or the ?turtle?
referenced in Q0, wecannot compare questions Q2 and Q3 in terms oftheir usefulness with respect to Q0.Table 1 shows another reference questionQ5 fromour dataset, together with its annotated group ofquestions Q6 to Q20.
In order to make the anno-tation process easier and reproducible, we have di-vided it into two levels of annotation.
During thefirst annotation stage, each question group is parti-tioned manually into 3 subgroups of questions:?
P is the set of paraphrasing questions.?
U is the set of useful questions.?
N is the set of neutral questions.A question is deemed useful if its expected answermay overlap in information content with the ex-pected answer of the reference question.
The ex-pected answer of a neutral question, on the otherhand, should be irrelevant with respect to the ref-erence question.
Let Qr be the reference question,Qp ?
P a paraphrasing question, Qu ?
U a usefulquestion, and Qn ?
N a neutral question.
Then thefollowing relations are assumed to hold among thesequestions:1.
?Qp ?
Qu|Qr?
: a paraphrasing question ismore useful than a useful question.2.
?Qu ?
Qn|Qr?
: a useful question is more use-ful than a neutral question.Note that as long as these relations hold betweenthe 3 types of questions, the names of the sub-groups and their definitions are irrelevant with re-spect to the implied set of more useful than rela-tions, since only the implied ternary relations willbe used for training and evaluating question rank-ing approaches.
We also assume that, by tran-sitivity, the following ternary relations also hold:?Qp ?
Qn|Qr?, i.e.
a paraphrasing question ismore useful than a neutral question.
Furthermore, ifQp1 , Qp2 ?
P are two paraphrasing questions, thisimplies ?Qp1 = Qp2 |Qr?.99For the vast majority of questions, the first annota-tion stage is straightforward and non-controversial.In the second annotation stage, we perform a finerannotation of relations between questions in themiddle group U .
Table 1 shows two such relations(using indentation): ?Q8 ?
Q9|Q5?
and ?Q8 ?Q10|Q5?.
Question Q8 would have been a rephras-ing of the reference question, were it not for thenoun ?art?
modifying the focus noun phrase ?sum-mer camp?.
Therefore, the information content ofthe answer to Q8 is strictly subsumed in the infor-mation content associated with the answer to Q5.Similarly, inQ9 the focus noun phrase is further spe-cialized through the prepositional phrase ?for girls?.Therefore, (an answer to) Q9 is less useful to Q5than (an answer to) Q8, i.e.
?Q8 ?
Q9|Q5?.
Fur-thermore, the focus ?art summer camp?
in Q8 con-ceptually subsumes the focus ?summer camps forsinging?
in Q10, therefore ?Q8 ?
Q10|Q5?.We call this dataset simple since most of the ref-erence questions are shorter than the other questionsin their group.
We have also created a complex ver-sion of the same dataset, by selecting as the refer-ence question in each group a longer question fromthe same group.
For example, ifQ0 were a referencequestion, it would be replaced with a more complexquestion, such as Q2, or Q3.
The annotation is re-done to reflect the relative usefulness relations withrespect to the new reference questions.
We believethat the new complex dataset is closer to the actualdistribution of questions in community QA reposi-tories: unanswered questions tend to be more spe-cific (longer), whereas general questions (shorter)are more likely to have been answered already.
Eachdataset is annotated by two annotators, leading toa total of 4 datasets: Simple1, Simple2, Complex1,and Complex2.Table 2 presents the following statistics on the twotypes of datasets (Simple, Complex) for each anno-tator (1, 2): the total number of paraphrasings (P),the total number of useful questions (U), the totalnumber of neutral questions (N ), the total numberof more useful than ordered pairs encoded in thedataset, either explicitly or through transitivity, andthe Inter-Annotator Agreement (ITA).
We computethe ITA as the precision (P) and recall (R) with re-spect to the more useful than ordered pairs encodedin one annotation (Pairs1) relative to the orderedDataset P U N Pairs ITASimple1 164 775 594 11015 P: 76.6Simple2 134 778 621 10436 R: 81.6Complex1 103 766 664 10654 P: 71.3Complex2 89 730 714 9979 R: 81.3Table 2: Dataset statistics.pairs encoded in the other annotation (Pairs2).P = |Pairs1 ?
Pairs2|Pairs1R = |Pairs1 ?
Pairs2|Pairs2The statistics in Table 2 indicate that the secondannotator was in general more conservative in tag-ging questions as paraphrases or useful questions.3 Unsupervised Methods for QuestionRankingAn ideal question ranking method would take an ar-bitrary triplet of questions Qr, Qi and Qj as input,and output an ordering between Qi and Qj with re-spect to the reference question Qr, i.e.
one of ?Qi ?Qj |Qr?, ?Qi = Qj |Qr?, or ?Qj ?
Qi|Qr?.
One ap-proach is to design a usefulness function u(Qi, Qr)that measures how useful question Qi is for the ref-erence question Qr, and define the more useful than(?)
relation as follows:?Qi ?
Qj |Qr?
?
u(Qi, Qr) > u(Qj , Qr)If we define I(Q) to be the information need asso-ciated with question Q, then u(Qi, Qr) could be de-fined as a measure of the relative overlap betweenI(Qi) and I(Qr).
Unfortunately, the informationneed is a concept that, in general, is defined onlyintensionally and therefore it is difficult to measure.For lack of an operational definition of the informa-tion need, we will approximate u(Qi, Qr) directlyas a measure of the similarity between Qi and Qr.The similarity between two questions can be seen asa special case of text-to-text similarity, consequentlyone possibility is to use a general text-to-text simi-larity function such as cosine similarity in the vectorspace model (Baeza-Yates and Ribeiro-Neto, 1999):cos(Qi, Qr) =QTi Qr?Qi?
?Qr?100Here, Qi and Qr denote the corresponding tf?idfvectors.As a measure of question similarity, one majordrawback of cosine similarity is that it is obliviousof the meanings of words in each question.
This par-ticular problem is illustrated by the three questionsbelow.
Q22 and Q23 have the same cosine similar-ity with Q21, they are therefore indistinguishable interms of their usefulness to the reference questionQ21, even though we expect Q22 to be more use-ful than Q23 (a place that sells hydrangea often sellsother types of plants too, possibly including cacti).Q21 Where can I buy a hydrangea?Q22 Where can I buy a cactus?Q23 Where can I buy an iPad?To alleviate the lexical chasm, we can redefineu(Qi, Qr) to be the similarity measure proposed by(Mihalcea et al, 2006) as follows:mcs(Qi, Qr) =?w?
{Qi}maxSim(w,Qr) ?
idf(w)?w?{Qi}idf(w)+?w?
{Qr}maxSim(w,Qi) ?
idf(w)?w?
{Qr}idf(w)Since scaling factors are immaterial for ranking, wehave ignored the normalization constant containedin the original measure.
For each word w ?
Qi,maxSim(w,Qr) computes the maximum semanticsimilarity between w and any word wr ?
Qr.
Thesimilarity scores are weighted by the correspond-ing idf?s, and normalized.
A similar score is com-puted for each word w ?
Qr.
The score computedby maxSim depends on the actual function usedto compute the word-to-word semantic similarity.In this paper, we evaluated four of the knowledge-based measures explored in (Mihalcea et al, 2006):wup (Wu and Palmer, 1994), res (Resnik, 1995), lin(Lin, 1998), and jcn (Jiang and Conrath, 1997).4 Supervised Learning for QuestionRankingCosine similarity, henceforth referred as cos, treatsquestions as bags-of-words.
The meta-measure pro-posed in (Mihalcea et al, 2006), henceforth calledmcs, treats questions as bags-of-concepts.
Both cosand mcs ignore the syntactic relations between thewords in a question, and therefore may miss impor-tant structural information.
In the next three sec-tions we describe a set of structural features that webelieve are relevant for judging question similarity.These and other types of features will be integratedin an SVM model for ranking, as described later inSection 4.4.4.1 Matching the Focus WordsIf we consider the question Q24 below as reference,question Q26 will be deemed more useful than Q25when using cos or mcs because of the higher rela-tive lexical and conceptual overlap with Q24.
How-ever, this is contrary to the actual ordering ?Q25 ?Q26|Q24?, which reflects the fact that Q25, whichexpects the same answer type as Q24, should bedeemed more useful than Q26, which has a differ-ent answer type.Q24 What are some good thriller movies?Q25 What are some thriller movies with happy end-ing?Q26 What are some good songs from a thrillermovie?The analysis above shows the importance of us-ing the answer type when computing the similar-ity between two questions.
However, instead of re-lying exclusively on a predefined hierarchy of an-swer types, we identify the question focus of a ques-tion, defined as the set of maximal noun phrases inthe question that corefer with the expected answer(Bunescu and Huang, 2010a).
Focus nouns such asmovies and songs provide more discriminative in-formation than general answer types such as prod-ucts.
We use answer types only for questions suchas Q27 or Q28 below that lack an explicit questionfocus.
In such cases, an artificial question focus iscreated from the answer type (e.g.
location for Q27,or method for Q28).101Q27 Where can I buy a good coffee maker?Q28 How do I make a pizza?Let fi and fr be the focus words corresponding toquestions Qi and Qr.
We introduce a focus feature?f , and set its value to be equal with the similaritybetween the focus words:?f (Qi, Qr) = wsim(fi, fr) (1)We use wsim to denote a generic word meaning sim-ilarity measure (e.g.
wup, res, lin or jcn).
Whencomputing the focus feature, the non-focus word?movie?
in Q26 will not be compared with the fo-cus word ?movies?
in Q24, and therefore Q26 willhave a lower value for this feature than Q25, i.e.
?f (Q26, Q24) < ?f (Q25, Q24).4.2 Matching the Main VerbsIn addition to the question focus, the main verb ofa question can also provide key information in es-timating question-to-question similarity.
We definethe main verb to be the content verb that is highestin the dependency tree of the question, e.g.
buy forQ27, or make for Q28.
If the question does not con-tain a content verb, the main verb is defined to be thehighest verb in the dependency tree, as for exampleare in Q24 to Q26.
The utility of a question?s mainverb in judging its similarity to other questions canbe seen more clearly in the questions below, whereQ29 is the reference:Q29 How can I transfer music from iTunes to myiPod?Q30 How can I upload music to my iPod?Q31 How can I play music in iTunes?The fact that upload, as the main verb of Q30, ismore semantically related to transfer is essential indeciding that ?Q30 ?
Q31|Q29?, i.e.
Q30 is moreuseful than Q31 to Q29.Let vi and vr be the main verbs corresponding toquestions Qi and Qr.
We introduce a main verb fea-ture ?v as follows:?v(Qi, Qr) = wsim(vi, vr) (2)If Q29 is considered as reference question, it is ex-pected that the main verb feature for question Q30will have a higher value than the main verb featurefor Q31, i.e.
?f (Q31, Q29) < ?f (Q30, Q29).Figure 2: Matched dependency trees.4.3 Matching the Dependency TreesThe question focus and the main verb are only twoof the nodes in the syntactic dependency tree of aquestion.
In general, all the words in a question areimportant when judging its semantic similarity withanother question.
We therefore propose a more gen-eral feature that exploits the dependency structureof the question and, in doing so, it also considersall the words in the question, like cos and mcs.
Forany given question we initially ignore the directionof the dependency arcs and change the question de-pendency tree to be rooted at the focus word, as il-lustrated in Figure 2 for questions Q5 and Q9.
In-terrogative patterns such as ?What is?
or ?Are thereany?
are automatically eliminated from the depen-dency trees.
We define the dependency tree similar-ity between two questions Qi and Qr to be a func-tion of similarities wsim(vi, vr) computed betweenaligned nodes vi ?
Qi and vr ?
Qr.
The nodesof two dependency trees are aligned through a func-tion MaxMatch(ui.C, ur.C) that takes two sets ofchildren nodes as arguments, one from Qi and onefrom Qr, and finds the maximum weighted bipartitematching between ui.C and ur.C.
Given two chil-dren nodes vi ?
ui.C and vr ?
ur.C, the weight ofa potential matching between vi and vr is defined102simply as wsim(vi, vr).
MaxMatch(ui.C, ur.C) isfurthermore constrained to match only nodes thathave compatible part-of-speech tags (e.g.
nounsare matched to nouns, verbs are matched to verbs),and children nodes that have the same head-modifierrelationship with their parents (i.e.
they are bothheads, or they are both dependents of their par-ents).
Table 3 shows the recursive algorithm usedTreeMatch(ui, ur)[In]: Two dependency tree nodes ui, ur.
[Out]: A set of node pairsM.1.
setM?
{(ui, ur)}2. for each (vi, vr) ?
MaxMatch(ui.C, ur.C):3. setM?M?
TreeMatch(vi, vr)4. returnMTable 3: Dependency Tree Matching.for finding a matching between two question depen-dency trees rooted at the focus words.
The initialarguments of the algorithm are the two focus wordsui = fi and ur = fr.
Thus, the pair (fi, fr) isthe first pair of nodes to be added to the matchingM in step 1.
In the next step, we compute the maxi-mumweighted matching between the children nodesui.C and ur.C, and recursively call the matching al-gorithm on pairs of matched nodes (vi, vr) fromM.The algorithm stops when MaxMatch returns anempty matching, which may happen when reach-ing leaf nodes, or when no pair of children nodeshas compatible POS tags, or child-parent dependen-cies.
Figure 2 shows the results of applying thetree matching algorithm on questions Q5 and Q9.Matched nodes share the same index and are shownin circles, whereas unmatched nodes are shown initalics.We introduce a new feature ?t(Qi, Qr) whosevalue is defined as the dependency tree similaritybetween questions Qi and Qr.
Once the optimummatchingM(Qi, Qr) between dependency trees hasbeen found, ?t(Qi, Qr) is computed as the nor-malized sum of the similarities between pairs ofmatched nodes vi and vr, as shown in Equations 3and 4 below.
When computing the similarity be-tween two matched nodes, we factor in the similar-ities between corresponding pairs of words on thepaths fi ; vi, fr ; vr between the focus words fi,fr and the nodes vi, vr, as shown in Equation 5.
Thishas the effect of reducing the importance of wordsthat are farther away from the focus word in the de-pendency tree.
?t(Qi, Qr) =sim(Qi, Qr)?sim(Qi, Qi)sim(Qr, Qr)(3)sim(Qi, Qr) =?
(vi,vr)?M(Qi,Qr)sim(fi ; vi, fr ; vr) (4)sim(u1 ; un, v1 ; vn) =n?i=1wsim(ui, vi) (5)If the word similarity function is normalized anddefined to return 1 for identical words, the nor-malizer in Equation 3 becomes equivalent with?|Qi||Qr|.
Thus, words that are left unmatched im-plicitly decrease the dependency tree similarity.4.4 An SVM Model for Ranking QuestionsWe consider learning a usefulness functionu(Qi, Qr) of the following general, linear form:u(Qi, Qr) = wT?
(Qi, Qr) (6)The vector ?
(Qi, Qr) is defined to contain the fol-lowing generic features:1.
?f (Qi, Qr) = the semantic similarity betweenfocus words, as described in Section 4.1.2.
?v(Qi, Qr) = the semantic similarity betweenmain verbs, as described in Section 4.2.3.
?t(Qi, Qr) = the semantic similarity betweenthe dependency trees, as described in Sec-tion 4.3.4. cos(Qi, Qr) = the cosine similarity between thetwo questions, as described in Section 3.5. mcs(Qi, Qr) = the bag-of-concepts similaritybetween the two questions, as described in Sec-tion 3.Each of the generic features ?f , ?v, ?t, andmcs cor-responds to four actual features, one for each possi-ble choice of the word similarity functionwsim (i.e.wup, res, lin or jcn).
An additional pair of featuresis targeted at questions containing locations:1036.
?l(Qi, Qr) = 1 if both questions contain loca-tions, 0 otherwise.7.
?d(Qi, Qr) = the normalized geographical dis-tance between the locations in Qi and Qr, 0 if?l(Qi, Qr) = 0.Given two location names, we first find their latitudeand longitude using Google Maps, and then com-pute the spherical distance between them using thehaversine formula.The corresponding parameters w will be trainedon pairs from one of the partially ordered datasetsdescribed in Section 2.
We use the kernel version ofthe large-margin ranking approach from (Joachims,2002) which solves the optimization problem in Fig-ure 3 below.
The aim of this formulation is to find aminimize:J(w, ?)
= 12?w?2 + C?
?rijsubject to:wT?
(Qi, Qr)?wT?
(Qj , Qr) ?
1?
?rij?rij ?
0?Qr, Qi, Qj ?
D, ?Qi ?
Qj |Qr?Figure 3: SVM ranking optimization problem.weight vector w such that 1) the number of rankingconstraints u(Qi, Qr) ?
u(Qj , Qr) from the train-ing data D that are violated is minimized, and 2) theranking function u(Qi, Qr) generalizes well beyondthe training data.
The learned w is a linear combina-tion of the feature vectors ?
(Qi, Qr), which makesit possible to use kernels.5 Experimental EvaluationWe use the four question ranking datasets describedin Section 2 to evaluate the three similarity mea-sures cos, mcs, and ?t, as well as the SVM rank-ing model.
We report one set of results for each ofthe four word similarity measures wup, res, lin orjcn.
Each question similarity measure is evaluatedin terms of its accuracy on the set of ordered pairs,and the performance is averaged between the twoannotators for the Simple and Complex datasets.
If?Qi ?
Qj |Qr?
is a relation specified in the anno-tation, we consider the tuple ?Qi, Qj , Qr?
correctlyclassified if and only if u(Qi, Qr) > u(Qj , Qr),where u is the question similarity measure.
We usedthe SVMlight 2 implementation of ranking SVMs,with a cubic kernel and the standard parameters.
TheSVM ranking model was trained and tested using10-fold cross-validation, and the overall accuracywas computed by averaging over the 10 folds.We used the NLTK 3 implementation of the foursimilarity measures wup, res, lin or jcn.
The idf val-ues for each word were computed from frequencycounts over the entire Wikipedia.
For each ques-tion, the focus is identified automatically by an SVMtagger trained on a separate corpus of 2,000 ques-tions manually annotated with focus information(Bunescu and Huang, 2010a).
The SVM taggeruses a combination of lexico-syntactic features anda quadratic kernel to achieve a 93.5% accuracy ina 10-fold cross validation evaluation on the 2,000questions.
The head-modifier dependencies werederived automatically from the syntactic parse treeusing the head finding rules from (Collins, 1999).The syntactic tree is obtained using Spear 4, a syn-tactic parser which comes pre-trained on an addi-tional treebank of questions.
The main verb ofa question is identified deterministically using abreadth first traversal of the dependency tree.The overall accuracy results presented in Table 4show that the SVM ranking model obtains by far thebest performance on both datasets, a substantial 10%higher than cos, which is the best performing unsu-pervised method.
The random baseline ?
assigninga random similarity value to each pair of questions ?results in 50% accuracy.
Even though its use of wordsenses was expected to lead to superior results, mcsdoes not perform better than cos on this dataset.
Ourimplementation of mcs did however perform betterthan cos on the Microsoft paraphrase corpus (Dolanet al, 2004).
One possible reason for this behav-ior is that mcs seems to be less resilient than costo differences in question length.
Whereas the Mi-crosoft paraphrase corpus was specifically designedsuch that ?the length of the shorter of the two sen-tences, in words, is at least 66% that of the longer?
(Dolan and Brockett, 2005), the question rankingdatasets place no constraints on the lengths of the2svmlight.joachims.org3www.nltk.org4www.surdeanu.name/mihai/spear104Question wup res lin jcnDataset cos mcs ?t mcs ?t mcs ?t mcs ?t SVMSimple 73.7 69.1 69.4 71.3 71.8 70.8 69.8 71.9 71.7 82.1Complex 72.6 64.1 69.6 66.0 71.5 66.9 69.1 69.4 71.0 82.5Table 4: Pairwise accuracy results.Dataset al ?
?f ?
?v ?
?t ?
?l,d ?cos ?mcs ?
?f,tSimple 82.1 79.3 82.0 80.2 81.5 80.3 81.4 78.5Complex 82.5 81.3 81.3 78.7 81.8 79.2 81.8 77.4Table 5: Ablation results.questions.
However, even though by themselves themeaning aware mcs and the structure-and-meaningaware ?t do not outperform the bag-of-words cos,they do help in increasing the performance of theSVM ranking model, as can be inferred from the cor-responding columns in Table 5.
The table shows theresults of ablation experiments in which all but onetype of features are used.
The results indicate thatall types of features are useful, with significant con-tributions being brought especially by cos and thefocus related features ?f,t.The measures investigated in this paper are allcompositional and reduce the similarity computa-tions to word level.
The following question patternsillustrate the need to design more complex similaritymeasures that take into account the context of everyword in the question:P1 Where can I find a job around ?City ?
?P2 What are some famous people from ?City ?
?P3 What is the population of ?City ?
?Below are three instantiations of the first questionpattern:Q32 Where can I find a job around Anaheim, CA?Q33 Where can I find a job around Los Angeles?Q34 Where can I find a job around Vista, CA?If we take Q32 as reference question, the fact thatthe distance between Los Angeles and Anaheim issmaller than the distance between Vista and Ana-heim leads the ranking system to rank Q33 as moreuseful than Q34 with respect to Q32, which is theexpected result.
The preposition ?around?
from thecity context in the first pattern is a good indica-tor that proximity relations are relevant in this case.When the same three cities are used for instantiatingthe other two patterns, it can be seen that the prox-imity relations are no longer as relevant for judgingthe relative usefulness of questions.6 Future WorkWe plan to integrate context dependent word sim-ilarity measures into a more robust question util-ity function.
We also plan to make the dependencytree matching more flexible in order to account forparaphrase patterns that may differ in their syntacticstructure.
The questions that are posted on commu-nity QA sites often contain spelling or grammaticalerrors.
Consequently, we will work on interfacingthe question ranking system with a separate moduleaimed at fixing orthographic and grammatical errors.7 Related WorkThe question rephrasing subtask has spawned a di-verse set of approaches.
(Hermjakob et al, 2002)derive a set of phrasal patterns for question reformu-lation by generalizing surface patterns acquired au-tomatically from a large corpus of web documents.The focus of the work in (Tomuro, 2003) is on deriv-ing reformulation patterns for the interrogative partof a question.
In (Jeon et al, 2005), word trans-lation probabilities are trained on pairs of seman-tically similar questions that are automatically ex-tracted from an FAQ archive, and then used in alanguage model that retrieves question reformula-tions.
(Jijkoun and de Rijke, 2005) describe an FAQ105question retrieval system in which weighted com-binations of similarity functions corresponding toquestions, existing answers, FAQ titles and pagesare computed using a vector space model.
(Zhao etal., 2007) exploit the Encarta logs to automaticallyextract clusters containing question paraphrases andfurther train a perceptron to recognize question para-phrases inside each cluster based on a combinationof lexical, syntactic and semantic similarity features.More recently, (Bernhard and Gurevych, 2008) eval-uated various string similarity measures and vec-tor space based similarity measures on the task ofretrieving question paraphrases from the WikiAn-swers repository.
The aim of the question searchtask presented in (Duan et al, 2008) is to returnquestions that are semantically equivalent or closeto the queried question, and is therefore similar toour question ranking task.
Their approach is eval-uated on a dataset in which questions are catego-rized either as relevant or irrelevant.
Our formula-tion of question ranking is more general, and in par-ticular subsumes the annotation of binary questioncategories such as relevant vs. irrelevant, or para-phrases vs. non-paraphrases.
Moreover, we are ableto exploit the annotated utility relations as super-vision in a learning for ranking approach, whereas(Duan et al, 2008) use the annotated dataset to tunethe 3 parameters of a mostly unsupervised approach.The question ranking task was first formulated in(Bunescu and Huang, 2010b), where an initial ver-sion of the dataset was also described.
In this pa-per, we introduce 4 versions of the dataset, a moregeneral meaning and structure aware similarity mea-sure, and a supervised model for ranking that sub-stantially outperforms the previously proposed util-ity measures.8 ConclusionWe presented a supervised learning approach to thequestion ranking task in which previously knownquestions are ordered based on their relative util-ity with respect to a new, reference question.
Wecreated four versions of a dataset of 60 groups ofquestions 5, each annotated with a partial order rela-tion reflecting the relative utility of questions insideeach group.
An SVM ranking model was trained5The dataset will be made publicly available.on the dataset and evaluated together with a set ofsimpler, unsupervised question-to-question similar-ity models.
Experimental results demonstrate theimportance of using structure and meaning awarefeatures when computing the relative usefulness ofquestions.AcknowledgmentsWe would like to thank the anonymous reviewers fortheir insightful comments.ReferencesRicardo Baeza-Yates and Berthier Ribeiro-Neto.
1999.Modern Information Retrieval.
ACM Press, NewYork.Delphine Bernhard and Iryna Gurevych.
2008.
Answer-ing learners?
questions by retrieving question para-phrases from social Q&A sites.
In EANL ?08: Pro-ceedings of the Third Workshop on Innovative Use ofNLP for Building Educational Applications, pages 44?52, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Razvan Bunescu and Yunfeng Huang.
2010a.
Towards ageneral model of answer typing: Question focus iden-tification.
In Proceedings of The 11th InternationalConference on Intelligent Text Processing and Com-putational Linguistics (CICLing 2010), RCS Volume,pages 231?242.Razvan Bunescu and Yunfeng Huang.
2010b.
A utility-driven approach to question ranking in social QA.In Proceedings of The 23rd International Conferenceon Computational Linguistics (COLING 2010), pages125?133.Michael Collins.
1999.
Head-driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.William B. Dolan and Chris Brockett.
2005.
Automat-ically constructing a corpus of sentential paraphrases.In Proceedings of the Third International Workshop onParaphrasing (IWP2005), pages 9?16.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised construction of large paraphrase corpora:Exploiting assively parallel news sources.
In Proceed-ings of The 20th International Conference on Compu-tational Linguistics (COLING?04), page 350.Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and YongYu.
2008.
Searching questions by identifying questiontopic and question focus.
In Proceedings of ACL-08:HLT, pages 156?164, Columbus, Ohio, June.Ulf Hermjakob, Abdessamad Echihabi, and DanielMarcu.
2002.
Natural language based reformulation106resource and web exploitation for question answering.In Proceedings of TREC-2002.Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and an-swer archives.
In Proceedings of the 14th ACM in-ternational conference on Information and knowledgemanagement (CIKM?05), pages 84?90, NewYork, NY,USA.
ACM.J.J.
Jiang and D.W. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InProceedings of the International Conference on Re-search in Computational Linguistics, pages 19?33.Valentin Jijkoun and Maarten de Rijke.
2005.
Retrievinganswers from frequently asked questions pages on theWeb.
In Proceedings of the 14th ACM internationalconference on Information and knowledge manage-ment (CIKM?05), pages 76?83, New York, NY, USA.ACM.Thorsten Joachims.
2002.
Optimizing search engines us-ing clickthrough data.
In Proceedings of the EighthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining (KDD-2002), Ed-monton, Canada.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the Fifteenth In-ternational Conference on Machine Learning (ICML?98), pages 296?304, San Francisco, CA, USA.
Mor-gan Kaufmann Publishers Inc.Rada Mihalcea, Courtney Corley, and Carlo Strappar-ava.
2006.
Corpus-based and knowledge-based mea-sures of text semantic similarity.
In Proceedings ofthe 21st national conference on Artificial intelligence(AAAI?06), pages 775?780.
AAAI Press.Dan I. Moldovan, Marius Pasca, Sanda M. Harabagiu,and Mihai Surdeanu.
2002.
Performance issues anderror analysis in an open-domain question answeringsystem.
In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics, pages33?40, Philadelphia, PA, July.John M. Prager.
2006.
Open-domain question-answering.
Foundations and Trends in InformationRetrieval, 1(2):91?231.Philip Resnik.
1995.
Using information content to eval-uate semantic similarity in a taxonomy.
In IJCAI?95:Proceedings of the 14th international joint conferenceon Artificial intelligence, pages 448?453, San Fran-cisco, CA, USA.
Morgan Kaufmann Publishers Inc.Noriko Tomuro.
2003.
Interrogative reformulation pat-terns and acquisition of question paraphrases.
In Pro-ceedings of the Second International Workshop onParaphrasing, pages 33?40, Morristown, NJ, USA.Association for Computational Linguistics.Zhibiao Wu and Martha Palmer.
1994.
Verbs seman-tics and lexical selection.
In Proceedings of the 32ndannual meeting on Association for Computational Lin-guistics, pages 133?138, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Shiqi Zhao, Ming Zhou, and Ting Liu.
2007.
Learn-ing question paraphrases for QA from Encarta logs.
InProceedings of the 20th international joint conferenceon Artifical intelligence (IJCAI?07), pages 1795?1800,San Francisco, CA, USA.
Morgan Kaufmann Publish-ers Inc.107
