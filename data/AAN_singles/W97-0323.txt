Exemplar-Based Word Sense Disambiguation:Some Recent ImprovementsHwee Tou NgDSO National  Laborator ies20 Science Park  DriveSingapore 118230nhweetou@dso, org .
sgAbst rac tIn this paper, we report recent improve-ments to the exemplar-based learning ap-proach for word sense disambiguation thathave achieved higher disambiguation accu-racy.
By using a larger value of k, thenumber of nearest neighbors to use for de-termining the class of a test example, andthrough 10-fold cross validation to auto-matically determine the best k, we have ob-tained improved disambiguation accuracyon a large sense-tagged corpus first used in(Ng and Lee, 1996).
The accuracy achievedby our improved exemplar-based classifieris comparable to the accuracy on the samedata set obtained by the Naive-Bayes al-gorithm, which was reported in (Mooney,1996) to have the highest disambiguationaccuracy among seven state-of-the-art ma-chine learning algorithms.1 IntroductionMuch recent research on word sense disambigua-tion (WSD) has adopted a corpus-based, learningapproach.
Many different learning approaches havebeen used, including neural networks (Leacock et al,1993), probabilistic algorithms (Bruce and Wiebe,1994; Gale et al, 1992a; Gale et al, 1995; Leacock etal., 1993; Yarowsky, 1992), decision lists (Yarowsky,1994), exemplar-based learning algorithms (Cardie,1993; Ng and Lee, 1996), etc.In particular, Mooney (1996) evaluated sevenstate-of-the-art machine learning algorithms on acommon data set for disambiguating six senses ofthe word "line".
The seven algorithms that he eval-uated are: a Naive-Bayes classifier (Duda and Hart,1973), a perceptron (Rosenblatt, 1958), a decision-tree learner (Quinlan, 1993), a k nearest-neighborclassifier (exemplar-based l arner) (Cover and Hart,1967), logic-based DNF and CNF learners (Mooney,1995), and a decision-list learner (Rivest, 1987).His results indicate that the simple Naive-Bayes al-gorithm gives the highest accuracy on the "line"corpus tested.
Past research in machine learninghas also reported that the Naive-Bayes algorithmachieved good performance on other machine learn-ing tasks (Clark and Niblett, 1989; Kohavi, 1996).This is in spite of the conditional independence as-sumption made by the Naive-Bayes algorithm, whichmay be unjustified in the domains tested.
Gale,Church and Yarowsky (Gale et al, 1992a; Gale et al,1995; Yarowsky, 1992) have also successfully usedthe Naive-Bayes algorithm (and several extensionsand variations) for word sense disambiguation.On the other hand, our past work on WSD (Ngand Lee, 1996) used an exemplar-based (or near-est neighbor) learning approach.
Our WSD pro-gram, LEXAS, extracts a set of features, includingpart of speech and morphological form, surroundingwords, local collocations, and verb-object syntacticrelation from a sentence containing the word to bedisambiguated.
These features from a sentence forman example.
LEXAS then uses the exemplar-basedlearning algorithm PEBLS (Cost and Salzberg, 1993)to find the sense (class) of the word to be disam-biguated.In this paper, we report recent improvements othe exemplar-based l arning approach for WSD thathave achieved higher disambiguation accuracy.
Theexemplar-based learning algorithm PEBLS containsa number of parameters that must be set beforerunning the algorithm.
These parameters includethe number of nearest neighbors to use for deter-mining the class of a test example (i.e., k in a knearest-neighbor classifier), exemplar weights, fea-ture weights, etc.
We found that the number k ofnearest neighbors used has a considerable impact onthe accuracy of the induced exemplar-based classi-fier.
By using 10-fold cross validation (Kohavi and208John, 1995) on the training set to automatically de-termine the best k to use, we have obtained im-proved disambiguation accuracy on a large sense-tagged corpus first used in (Ng and Lee, 1996).
Theaccuracy achieved by our improved exemplar-basedclassifier is comparable to the accuracy on the samedata set obtained by the Naive-Bayes algorithm,which was reported in (Mooney, 1996) to have thehighest disambiguation accuracy among seven state-of-the-art machine learning algorithms.The rest of this paper is organized as follows.
Sec-tion 2 gives a brief description of the exemplar-basedalgorithm PEBLS and the Naive-Bayes algorithm.Section 3 describes the 10-fold cross validation train-ing procedure to determine the best k number ofnearest neighbors to use.
Section 4 presents the dis-ambiguation accuracy of PEBLS and Naive-Bayes onthe large corpus of (Ng and Lee, 1996).
Section 5discusses the implications of the results.
Section 6gives the conclusion.2 Learning Algorithms2.1 PEBLSThe heart of exemplar-based learning is a measureof the similarity, or distance, between two examples.If the distance between two examples i small, thenthe two examples are similar.
In PEBLS (Cost andSalzberg, 1993), the distance between two symbolicvalues vl and v2 of a feature f is defined as:nd(vl, v2) = E IP(CdVl) - P(c~I~)Ii= lwhere n is the total number of classes.
P(Ci\]vl)h N 1 is estimated by N1 ' W ere ~,~ "s the number oftraining examples with value vl for feature f thatis classified as class i in the training corpus, andN1 is the number of training examples with valueVl for feature f in any class.
P(Ci\]v2) is estimatedsimilarly.
This distance metric of PEBLS is adaptedfrom the value difference metric of the earlier workof (Stanfill and Waltz, 1986).
The distance betweentwo examples i  the sum of the distances betweenthe values of all the features of the two examples.Let k be the number of nearest neighbors to usefor determining the class of a test example, k >_ 1.During testing, a test example is compared againstall the training examples.
PEBLS then determinesthe k training examples with the shortest distance tothe test example.
Among these k closest matchingtraining examples, the class which the majority ofthese k examples belong to will be assigned as theclass of the test example, with tie among multiplemajority classes broken randomly.Note that the nearest neighbor algorithm testedin (Mooney, 1996) uses Hamming distance as thedistance metric between two symbolic feature values.This is different from the above distance metric usedin PEBLS.2.2 Naive-BayesOur presentation of the Naive-Bayes algorithm(Duda and Hart, 1973) follows that of (Clark andNiblett, 1989).
This algorithm is based on Bayes'theorem:P(nv~ IC~)P(C~)P(Ci\] A vj) = P(Avj) i = 1...  uwhere P(Ci\[ A vj) is the probability that a test ex-ample is of class Ci given feature values vj.
(Avjdenotes the conjunction of all feature values in thetest example.)
The goal of a Naive-Bayes classifieris to determine the class Ci with the highest condi-tional probability P(Ci\] A vj).
Since the denomina-tor P(Avj) of the above xpression is constant for allclasses Ci, the problem reduces to finding the classCi with the maximum value for the numerator.The Naive-Bayes classifier assumes independenceof example features, o thatP(AvilC,) = H P(v  ledJDuring training, Naive-Bayes constructs the ma-trix P(vjICi), and P(Ci) is estimated from the dis-tribution of training examples among the classes.
Toavoid one zero count of P(vj \[Ci) nullifying the effectof the other non-zero conditional probabilities in themultiplication, we replace zero counts of P (vj\]Ci) byP(Ci)/N, where N is the total number of trainingexamples.
Other more complex smoothing proce-dures (such as those used in (Gale et al, 1992a)) arealso possible, although we have not experimentedwith these other variations.For the experimental results reported in this pa-per, we used the implementation f Naive-Bayesalgorithm in the PEBLS program (Rachlin andSalzberg, 1993), which has an option for trainingand testing using the Naive-Bayes algorithm.
Weonly changed the handling of zero probability countsto the method just described.3 Improvements to Exemplar-BasedWSDPEBLS contains a number of parameters that mustbe set before running the algorithm.
These param-eters include k (the number of nearest neighbors to209use for determining the class of a test example), ex-emplar weights, feature weights, etc.
Each of theseparameters has a default value in PEBLS, eg., k = 1,no exemplar weighting, no feature weighting, etc.We have used the default values for all parame-ter settings in our previous work on exemplar-basedWSD reported in (Ng and Lee, 1996).
However, ourpreliminary investigation i dicates that, among thevarious learning parameters of PEBLS, the numberk of nearest neighbors used has a considerable im-pact on the accuracy of the induced exemplar-basedclassifier.Cross validation is a well-known technique thatcan be used for estimating the expected error rateof a classifier which has been trained on a particulardata set.
For instance, the C4.5 program (Quinlan,1993) contains an option for running cross valida-tion to estimate the expected error rate of an in-duced rule set.
Cross validation has been proposedas a general technique to automatically determinethe parameter settings of a given learning algorithmusing a particular data set as training data (Kohaviand John, 1995).In m-fold cross validation, a training data setis partitioned into m (approximately) equal-sizedblocks, and the learning algorithm is run m times.In each run, one of the m blocks of training data isset aside as test data (the holdout set) and the re-maining m-  1 blocks are used as training data.
Theaverage rror rate of the m runs is a good estimateof the error rate of the induced classifier.For a particular parameter setting, we can runm-fold cross validation to determine the expectederror rate of that particular parameter setting.
Wecan then choose an optimal parameter setting thatminimizes the expected error rate.
Kohavi and John(1995) reported the effectiveness of such a techniquein obtaining optimal sets of parameter settings overa large number of machine learning problems.In our present study, we used 10-fold cross vali-dation to automatically determine the best k (num-ber of nearest neighbors) to use from the trainingdata.
To determine the best k for disambiguatinga word on a particular training set, we run 10-foldcross validation using PEBLS 21 times, each timewith k = 1,5, 10, 15, .
.
.
,  85, 90, 95,100.
We computethe error rate for each k, and choose the value of kwith the minimum error rate.
Note that the auto-matic determination of the best k through 10-foldcross validation makes use of only the training set,without looking at the test set at all.4 Exper imenta l  Resu l tsMooney (1996) has reported that the Naive-Bayesalgorithm gives the best performance on disam-biguating six senses of the word "line", among sevenstate-of-the-art learning algorithms tested.
How-ever, his comparative study is done on only one wordusing a data set of 2,094 examples.
In our presentstudy, we evaluated PEBLS and Naive-Bayes on amuch larger corpus containing sense-tagged occur-rences of 121 nouns and 70 verbs.
This corpus wasfirst reported in (Ng and Lee, 1996), and it containsabout 192,800 sense-tagged word occurrences of 191most frequently occurring and ambiguous words ofEnglish.
1 These 191 words have been tagged withsenses from WOI:tDNET (Miller, 1990), an on-line,electronic dictionary available publicly.
For this setof 191 words, the average number of senses per nounis 7.8, while the average number of senses per verb is12.0.
The sentences in this corpus were drawn fromthe combined corpus of the i million word Browncorpus and the 2.5 million word Wall Street Journal(WSJ) corpus.We tested both algorithms on two test sets fromthis corpus.
The first test set, named BC50, consistsof 7,119 occurrences of the 191 words appearing in50 text files of the Brown corpus.
The second testset, named WSJ6, consists of 14,139 occurrences ofthe 191 words appearing in 6 text files of the WSJcorpus.
Both test sets are identical to the ones re-ported in (Ng and Lee, 1996).Since the primary aim of our present study is thecomparative valuation of learning algorithms, notfeature representation, we have chosen, for simplic-ity, to use local collocations as the only features inthe example representation.
Local collocations havebeen found to be the single most informative setof features for WSD (Ng and Lee, 1996).
That lo-cal collocation knowledge provides important cluesto WSD has also been pointed out previously byYarowsky (1993).Let w be the word to be disambiguated, and let12 ll w rl r2 be the sentence fragment containingw.
In the present study, we used seven features inthe representation f an example, which are the localcollocations of the surrounding 4 words.
These sevenfeatures are: 12-11, ll-rl, rl-r2, l l, rl, 12, and r2.
Thefirst three features are concatenation f two words.
2The experimental results obtained are tabulatedin Table 1.
The first three rows of accuracy fig-1This corpus is available from the LinguisticData Consortium (LDC).
Contact the LDC atldc@unagi.cis.upenn.edu for details.2The first five of these seven features were also usedin (Ng and Lee, 1996).210Algorithm BC50 WSJ6Sense 1 40.5% 44.8%Most Frequent 47.1% 63.7%Ng & Lee (1996) 54.0% 68.6%PEBLS (k = 1) 55.0% 70.2%PEBLS (k = 20) 58.5% 74.5%PEBLS (10-fold c.v.) 58.7% 75.2%Naive-Bayes 58.2% 74.5%Table 1: Experimental Resultsures are those of (Ng and Lee, 1996).
The defaultstrategy of picking the most frequent sense has beenadvocgted as the baseline performance for evaluat-ing WSD programs (Gale et al, 1992b; Miller et al,1994).
There are two instantiations of this strat-egy in our current evaluation.
Since WORDNET or-ders its senses uch that sense 1 is the most frequentsense, one possibility is to always pick sense 1 asthe best sense assignment.
This assignment methoddoes not even need to look at the training exam-ples.
We call this method "Sense 1" in Table 1.
An-other assignment method is to determine the mostfrequently occurring sense in the training examples,and to assign this sense to all test examples.
We callthis method "Most Frequent" in Table 1.The accuracy figures of LEXAS as reported in (Ngand Lee, 1996) are reproduced in the third rowof Table 1.
These figures were obtained using allfeatures including part of speech and morphologi-cal form, surrounding words, local collocations, andverb-object syntactic relation.
However, the featurevalue pruning method of (Ng and Lee, 1996) only se-lects surrounding words and local collocations as fea-ture values if they are indicative of some sense classas measured by conditional probability (See (Ng andLee, 1996) for details).The next three rows show the accuracy figures ofPEBLS using the parameter setting of k = 1, k = 20,and 10-fold cross validation for finding the best k,respectively.
The last row shows the accuracy fig-ures of the Naive-Bayes algorithm.
Accuracy figuresof the last four rows are all based on only sevencollocation features as described earlier in this sec-tion.
However, all possible feature values (collocatedwords) are used, without employing the feature valuepruning method used in (Ng and Lee, 1996).Note that the accuracy figures of PEBLS withk = 1 are 1.0% and 1.6% higher than the accuracyfigures of (Ng and Lee, 1996) in the third row, alsowith k = 1.
The feature value pruning method of(Ng and Lee, 1996) is intended to keep only featurevalues deemed important for classification.
It seemsthat the pruning method has filtered out some usefiflcollocation values that improve classification accu-racy, such that this unfavorable effect outweighs theadditional set of features (part of speech and mor-phological form, surrounding words, and verb-objectsyntactic relation) used.Our results indicate that although Naive-Bayesperforms better than PEBLS with k = 1, PEBLSwith k = 20 achieves comparable performance.
Fur-thermore, PEBLS with 10-fold cross validation to se-lect the best k yields results lightly better than theNaive-Bayes algorithm.5 DiscussionTo understand why larger values of k are needed,we examined the performance ofPEBLS when testedon the WSJ6 test set.
During 10-fold cross valida-tion runs on the training set, for each of the 191words, we compared two error rates: the minimumexpected error rate of PEBLS using the best k, andthe expected error rate of the most frequent clas-sifter.
We found that for 13 words out of the 191words, the minimum expected error rate of PEBLSusing the best k is still higher than the expectederror rate of the most frequent classifier.
That is,for these 13 words, PEBLS will produce, on average,lower accuracy than the most frequent classifier.Importantly, for 11 of these 13 words, the best kfound by PEBLS are at least 85 and above.
This in-dicates that for a training data set when PEBLS hastrouble even outperforming the most frequent clas-sifter, it will tend to use a large value for k. This isexplainable since for a large value of k, PEBLS willtend towards the performance of the most frequentclassifier, as it will find the k closest matching train-ing examples and select the majority class amongthis large number of k examples.
Note that in theextreme case when k equals the size of the trainingset, PEBLS will behave xactly like the most frequentclassifier.Our results indicate that although PEBLS withk = 1 gives lower accuracy compared with Naive-Bayes, PEBLS with k = 20 performs as well as Naive-Bayes.
Furthermore, PEBLS with automatically se-lected k using 10-fold cross validation gives slightlyhigher performance ompared with Naive-Bayes.
Webelieve that this result is significant, in light of thefact that Naive-Bayes has been found to give thebest performance for WSD among seven state-of-the-art machine learning algorithms (Mooney, 1996).It demonstrates that an exemplar-based l arning ap-proach is suitable for the WSD task, achieving highdisambiguation accuracy.One potential drawback of an exemplar-based211learning approach is the testing time required, sinceeach test example must be compared with everytraining example, and hence the required testingtime grows linearly with the size of the training set.However, more sophisticated indexing methods uchas that reported in (Friedman et al, 1977) can re-duce this to logarithmic expected time, which willsignificantly reduce testing time.In the present study, we have focused on the com-parison of learning algorithms, but not on featurerepresentation f examples.
Our past work (Ng andLee, 1996) suggests that multiple sources of knowl-edge are indeed useful for WSD.
Future work willexplore the addition of these other features to fur-ther improve disambiguation accuracy.Besides the parameter k, PEBLS also containsother learning parameters such as exemplar weightsand feature weights.
Exemplar weighting has beenfound to improve classification performance (Costand Saizberg, 1993).
Also, given the relative impor-tance of the various knowledge sources as reportedin (Ng and Lee, 1996), it may be possible to improvedisambignation performance by introducing featureweighting.
Future work can explore the effect of ex-emplar weighting and feature weighting on disam-biguation accuracy.6 Conc lus ionIn summary, we have presented improvements to theexemplar-based learning approach for WSD.
By us-ing a larger value of k, the number of nearest neigh-bors to use for determining the class of a test ex-ample, and through 10-fold cross validation to au-tomatically determine the best k, we have obtainedimproved isambignation accuracy on a large sense-tagged corpus.
The accuracy achieved by our im-proved exemplar-based classifier is comparable tothe accuracy on the same data set obtained by theNaive-Bayes algorithm, which was recently reportedto have the highest disambignation accuracy amongseven state-of-the-art machine learning algorithms.7 AcknowledgementsThanks to Ray Mooney for helpful discussions, andthe anonymous reviewers for their comments.Re ferencesRebecca Bruce and Janyce Wiebe.
1994.
Word-sense disambiguation using decomposable mod-els.
In Proceedings of the 32nd Annual Meetingof the Association for Computational Linguistics,Las Cruces, New Mexico.Claire Cardie.
1993.
A case-based approach toknowledge acquisition for domain-specific sen-tence analysis.
In Proceedings of the Eleventh Na-tional Conference on Artificial Intelligence, pages798-803, Washington, DC.Peter Clark and Tim Niblett.
1989.
The CN2 induc-tion algorithm.
Machine Learning, 3(4):261-283.Scott Cost and Steven Salzberg.
1993.
A weightednearest neighbor algorithm for learning with sym-bolic features.
Machine Learning, 10(1):57-78.T.
M. Cover and P. Hart.
1967.
Nearest neighborpattern classification.
IEEE Transactions on In-formation Theory, 13(1):21-27.Richard Duda and Peter Hart.
1973.
Pattern Clas-sification and Scene Analysis.
Wiley, New York.J.
Friedman, J. Bentley, and R. Finkel.
1977.
Analgorithm for finding best matches in logarithmicexpected time.
A CM Transactions on Mathemat-ical Software, 3(3):209-226.William Gale, Kenneth Ward Church, and DavidYarowsky.
1992a.
A Method for DisambiguatingWord Senses in a Large Corpus.
Computers andthe Humanities, 26:415-439.William Gale, Kenneth Ward Church, and DavidYarowsky.
1992b.
Estimating upper and lowerbounds on the performance of word-sense disam-biguation programs.
In Proceedings of the 30thAnnual Meeting of the Association for Computa-tional Linguistics, Newark, Delaware.William Gale, Kenneth Ward Church, and DavidYarowsky.
1995.
Discrimination Decisions for100,000 Dimensional Spaces.
Annals of Opera-tions Research, 55:323-344.Ron Kohavi and George H. John.
1995.
Automaticparameter selection by minimizing estimated er-ror.
In Machine Learning: Proceedings of theTwelfth International Conference.Ron Kohavi.
1996.
Scaling up the accuracy ofNaive-Bayes classifiers: A decision-tree hybrid.
InProceedings of the Second International Confer-ence on Knowledge Discovery and Data Mining.Claudia Leacock, Geoffrey Towell, and EllenVoorhees.
1993.
Corpus-based statistical senseresolution.
In Proceedings of the ARPA HumanLanguage Technology Workshop.George A. Miller, Ed.
1990.
WordNet: An on-linelexical database.
International Journal of Lexi-cography, 3(4):235-312.212George A. Miller, Martin Chodorow, Shari Landes,Claudia Leacock, and Robert G. Thomas.
1994.Using a semantic oncordance for sense identifi-cation.
In Proceedings of the ARPA Human Lan-guage Technology Workshop.Raymond J. Mooney.
1995.
Encouraging experi-mental results on learning CNF.
Machine Learn-ing, 19(1):79-92.Raymond J. Mooney.
1996.
Comparative xperi-ments on disambiguating word senses: An illus-tration of the role of bias in machine learning.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP).Hwee Tou Ng and Hian Beng Lee.
1996.
Integratingmultiple knowledge sources to disambiguate wordsense: An exemplar-based approach.
In Proceed-ings of the 34th Annual Meeting of the Associa-tion for Computational Linguistics (ACL), pages40-47.J.
Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo, CA.John Rachlin and Steven Salzberg.
1993.
PEBLS3.0 User's Guide.R.
L. Rivest.
1987.
Learning decision lists.
MachineLearning, 2(3):229-246.F.
Rosenblatt.
1958.
The pereeptron: A probabilis-tic model for information storage and organizationin the brain.
Psychological Review, 65:386--408.C Stanfill and David Waltz.
1986.
Toward memory-based reasoning.
Communications of the A CM,29(12):1213-1228.David Yarowsky.
1992.
Word-sense disambigua-tion using statistical models of Roget's categoriestrained on large corpora.
In Proceedings of theFifteenth International Conference on Computa-tional Linguistics, pages 454-460, Nantes, France.David Yarowsky.
1993.
One sense per colloca-tion.
In Proceedings of the ARPA Human Lan-guage Technology Workshop.David Yarowsky.
1994.
Decision lists for lexical am-biguity resolution: Application to accent restora-tion in Spanish and French.
In Proceedings of the32nd Annual Meeting of the Association for Com-putational Linguistics, Las Cruces, New Mexico.213
