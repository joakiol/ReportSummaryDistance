Toward a Task-based Gold Standard for Evaluationof NP Chunks and Technical TermsNina WacholderRutgers Universitynina@scils.rutgers.eduPeng SongRutgers Universitypsong@paul.rutgers.eduAbstractWe propose a gold standard for evaluating twotypes of information extraction output -- nounphrase (NP) chunks (Abney 1991; Ramshaw andMarcus 1995) and technical terms (Justeson andKatz 1995; Daille 2000; Jacquemin 2002).
Thegold standard is built around the notion that sincedifferent semantic and syntactic variants of termsare arguably correct, a fully satisfactory assess-ment of the quality of the output must includetask-based evaluation.
We conducted an experi-ment that assessed subjects?
choice of index termsin an information access task.
Subjects showedsignificant preference for index terms that arelonger, as measured by number of words, andmore complex, as measured by number of prepo-sitions.
These terms, which were identified by ahuman indexer, serve as the gold standard.
Theexperimental protocol is a reliable and rigorousmethod for evaluating the quality of a set of terms.An important advantage of this task-based evalua-tion is that a set of index terms which is differentthan the gold standard can ?win?
by providingbetter information access than the gold standarditself does.
And although the individual humansubject experiments are time consuming, the ex-perimental interface, test materials and dataanalysis programs are completely re-usable.1 IntroductionThe standard metrics for evaluation of the output ofNLP systems are precision and recall.
Given an ar-guably correct list of the units that a system wouldidentify if it performed perfectly, there should inprinciple be no discrepancy between the units identi-fied by a system and the units that are either useful ina particular application or are preferred by humanbeings for use in a particular task.
But when the satis-factory output can take many different forms, as insummarization and generation, evaluation by preci-sion and recall is not sufficient.
In these cases, thechallenge for system designers and users is to effec-tively distinguish between systems that provide gen-erally satisfactory output and systems that do not.NP chunks (Abney 1991; Ramshaw and Marcus1995; Evans and Zhai 1996; Frantzi and Ananiadou1996) and technical terms (Dagan and Church 1994;Justeson and Katz 1995; Daille 1996; Jacquemin2001; Bourigault et al 2002) fall into this difficult-to-assess category.
NPs are recursive structures.
For themaximal NP large number of recent newspaper articleson biomedical science and clinical practice, a full-fledged parser would legitimately identify (at least)seven NPs in addition to the maximal one: largenumber; recent newspaper articles; large number ofrecent newspaper articles; biomedical science; clini-cal practice; biomedical science and clinical prac-tice; and recent newspaper articles on biomedicalscience and clinical practice.
To evaluate the per-formance of a parser, NP chunks can usefully beevaluated by a gold standard; many systems (e.g.,Ramshaw and Marcus 1995 and Cardie and Pierce1988) use the Penn Treebank for this type of evalua-tion.
But for most applications, output that lists amaximal NP and each of its component NPs is bulkyand redundant.
Even a system that achieves 100%precision and recall in identifying all of the NPs in adocument needs criteria for determining which unitsto use in different contexts or applications.Technical terms are a subset of NP chunks.
Jac-quemin (2001:3) defines terms as multi-word ?vehi-cles of scientific and technical information?.
1  Theoperational difficulty, of course, is to decide whethera specific term is a vehicle of scientific and technicalinformation (e.g., birth date or light truck).
Evalua-tion of mechanisms that filter out some terms whileretaining others is subject to this difficulty.
This isexactly the kind of case where context plays a sig-nificant role in deciding whether a term conforms to adefinition and where experts disagree.In this paper, we turn to an information accesstask in order to assess terms identified by differenttechniques.
There are two basic types of informationaccess mechanisms, searching and browsing.
Insearching, the user generates the search terms; in1 Jacquemin does not use the modifier technical.Edmonton, May-June 2003Main Papers , pp.
189-196Proceedings of HLT-NAACL 2003browsing, the user recognizes potentially useful termsfrom a list of terms presented by the system.
When aninformation seeker can readily think up a suitableterm or linguistic expression to represent the informa-tion need, direct searching of text by user-generatedterms is faster and more effective than browsing.However, when users do not know (or can?t remem-ber) the exact expression used in relevant documents,they necessarily struggle to find relevant informationin full-text search systems.
Experimental studies haverepeatedly shown that information seekers use manydifferent terms to describe the same concept and fewof these terms are used frequently (Furnas et al 1987;Saracevic et al 1988; Bates et al 1998).
When in-formation seekers are unable to figure out the termused to describe a concept in a relevant document,electronic indexes are required for successful infor-mation access.NP chunks and technical terms have been pro-posed for use in this task (Boguraev and Kennedy1997; Wacholder 1998).
NP chunks and technicalterms have also been used in phrase browsing andphrase hierarchies (Jones and Staveley  1999;  Nevill-Manning et al 1999; Witten et al 1999; Lawrie andCroft 2000) and summarization (e.g., McKeown et al1999; Oakes and Paice 2001).
In fact, the distinctionbetween task-based evaluation of a system and preci-sion/recall evaluation of the quality of system outputis similar to the extrinsic/intrinsic evaluation ofsummarization (Gallier and Jones 1993).In order to focus on the subjects?
choice of indexterms rather than on other aspects of the informationaccess process, we asked subject to find answers toquestions in a college level text book.
Subjects usedthe Experimental Searching and Browsing Interface(ESBI) to browse a list of terms that were identifiedby different techniques and then merged.
Subjectsselect an index term by clicking on it in order to hy-perlink to the text itself.
By design, ESBI forces thesubjects to access the text indirectly, by searchingand browsing the list of index terms, rather than bydirect searching of the text.Three sets of terms were used in the experiment:one set (HS) was identified using the head-sortingmethod of Wacholder (1998); the second set (TT)was identified by an implementation of the technicalterm algorithm of Justeson and Katz (1995); a thirdset (HUM) was created by a human indexer.
Themethods for identifying these terms will be discussedin greater detail below.Somewhat to our surprise, subjects displayed avery strong preference for the index terms that wereidentified by the human indexer.
Table 1 shows thatwhen measured by percentage terms selected, sub-jects chose over 13% of the available human terms,but only 1.73% and 1.43% of the automatically se-lected terms; by this measure the subjects?
preferencefor the human terms was more than 7 times greaterthan the preference for either of the automatic tech-niques.
(In Table 1 and in the rest of this paper, allindex term counts are by type rather than by token,unless otherwise indicated.
)HUM HS TTTotal number ofterms 673 7980 1788Number of termsselected  89 114 31Percentage ofterms selected 13.22% 1.43% 1.73%Table 1: Percentage of terms selected by humansubjects relative to number of terms in the entireindex.This initial experiment strongly indicates that 1) peo-ple have a demonstrable preference for differenttypes of index terms; 2) these human terms are a verygood gold standard.
If subjects use a greater propor-tion of the terms identified by a particular technique,the terms can be judged better than the terms identi-fied by another technique, even if the terms are dif-ferent.
Any automatic technique capable ofidentifying terms that are preferred over these humanterms would be a very strong system indeed.
Fur-thermore, the properties of the terms preferred by theexperimental subjects can be used to guide design ofsystems for identifying and selecting NP chunks andtechnical terms.In the next section, we describe the design of theexperiment and in Section 3, we report on what theexperimental data shows about human preferencesfor different kinds of index terms.2 Experimental designOur experiment assesses the index terms vis a vistheir usefulness in a strictly controlled informationaccess task.
Subjects responded to a set of questionswhose answers were contained in a 350 page college-level text (Rice, Ronald E., McCreadie, Maureen andChang, Shan-ju L. (2001) Accessing and BrowsingInformation and Communication.
Cambridge, MA:MIT Press.)
Subjects used the Experimental Search-ing and Browsing Interface (ESBI) which forcesthem to access text via the index terms; direct textsearching was prohibited.
25 subjects participated inthe experiment; they were undergraduate and gradu-ate students at Rutgers University.
The experimentswere conducted by graduate students at the RutgersUniversity School of Communication, Informationand Library Studies (SCILS).2.1 ESBI (Experimental Searching and Brows-ing Interface)Subjects used the Experimental Searching andBrowsing Interface (ESBI) to find the answers to thequestions.
After an initial training session, ESBI pre-sents the user with a Search/Browse screen (notshown); the question appears at the top of the screen.The subject may enter a string to search for in theindex, or click on the "Browse" button for access tothe whole index.
At this point, "search" and "browse"apply only to the list of index terms, not to the text.The user may either browse the entire list of indexterms or may enter a search term and specify criteriato select the subset of terms that will be returned.Most people begin with the latter option because thecomplete list of index terms is too long to be easilybrowsed.
The user may select (click on) an indexterm to view a list of the contexts in which the termappears.
If the context appears useful, the user maychoose to view the term in its full context; if not, theuser may either do additional browsing or start theprocess over again.Figure 1 shows a screen shot of ESBI after thesearcher has entered the string democracy in thesearch box.
This view shows the demo question andthe workspace for entering answers.
The string was(previously) entered in the search box and all indexterms that include the word democracy are displayed.Although it is not illustrated here, ESBI also permitssubstring searching and the option to specify casesensitivity.Regardless of the technique by which the termwas identified, terms are organized by grammaticalhead of the phrase.
Preliminary analysis of our resultshas shown that most subjects like this analysis, whichresembles standard organization of back-of-the-bookindexes.Readers may notice that the word participationappears at the left-most margin, where it representsthe set of terms whose head is participation.
The in-dented occurrence represents the individual term.Selecting the left-most occurrence brings up contextsfor all phrases for which participation is a head.
Se-lecting on the indented occurrence brings up contextsfor the noun participation only when it is not part ofa larger phrase.
This is explained to subjects duringthe pre-experimental training and an experimenter ispresent to remind subjects of this distinction if aquestion arises during the experiment.Readers may also notice that in Figure 1, one ofthe terms, participation require, is ungrammatical.This particular error was caused by a faulty part-of-speech tag.
But since automatically identified indexterms typically include some nonsensical terms, wehave left these terms in ?
these terms are one of theproblems that information seekers have to cope within a realistic task-based evaluation.Figure 1: ESBI Screen shot2.2 QuestionsAfter conducting initial testing to find out what typesof questions subjects founder hard or easy, we spentconsiderable effort to design a set of 26 questions ofvarying degrees of difficulty.
To obtain an initialassessment of difficulty, one of the experimentersused ESBI to answer all of the questions and rateeach question with regard to how difficult it was toanswer using the ESBI system.
For example, thequestion What are the characteristics ofMarchionini's model of browsing?
was rated veryeasy because searching on the string marchioninireveals an index term Marchionini's which is linkedto the text sentence: Marchionini's model of browsingconsiders five interactions among the information-seeking factors of "task, domain, setting, user charac-teristics and experience, and system content and in-terface" (p.107).
The question What factorsdetermine when users decide to stop browsing?
wasrated very difficult because searching on stop (orsynonyms such as halt, cease, end, terminate, finish,etc.)
reveals no helpful index terms, while searchingon factors or browsing yields an avalanche of over500 terms, none with any obvious relevance.After subjects finished answering each question,they were asked to rate the question in terms of itsdifficulty.
A positive correlation between judgmentsof the experimenters and the experimental subjects(Sharp et al, under submission) confirmed that wehad successfully devised questions with a range ofdifficulty.
In general, questions that included termsactually used in the index were judged easier; ques-tions where the user had to devise the index termswere judged harder.To avoid effects of user learning, questions werepresented to subjects in random order; in the one hourexperiment, subjects answered an average of about 9questions.2.3 TermsAlthough the primary goal of this research is to pointthe way to improved techniques for automatic crea-tion of index terms, we used human created terms tocreate a baseline.
For the human index terms, weused the pre-existing back-of-the-book index, whichwe believe to be of high quality.2The two techniques for automatic identificationwere the technical terms algorithm of Justeson andKatz (1995) and the head sorting method (Dagan andChurch (1994); Wacholder (1998).
In the implemen-tation of the Justeson and Katz?
algorithm, technicalterms are multi-word NPs repeated above somethreshold in a corpus; in the head sorting method,technical terms are identified by grouping nounphrases with a common head (e.g., health-care work-ers  and asbestos workers), and selecting as termsthose NPs whose heads appear in two or morephrases.
Definitionally, technical terms are a propersubset of terms identified by Head Sorting.
Differ-ences in the implementations, especially the pre-processing module, result in there being some termsidentified by Termer that were not identified by HeadSorting.Table 2 shows the number of terms identified byeach method.
(*Because some terms are identified bymore than one technique, the percentage adds up tomore than 100%.)
The fewest terms (673) were iden-tified by the human method; in part this reflects thejudgment of the indexer and in part it is a result ofrestrictions on index length in a printed text.
Thelargest number of terms (7980) was identified by thehead sorting method.
This is because it applieslooser criteria for determining a term than does theJusteson and Katz algorithm which imposes a verystrict standard--no single word can be considered aterm, and an NP must be repeated in full to be con-sidered a term.2 Jim Snow  prepared the index under the supervision ofSCILS Professor James D. Anderson.HUM HS TT TotalTotalnumberof terms673 7980 1788 9992Per-centageof totalnumberof terms6.73% 79.86% 17.89% *Table 2: Number of terms in index by method ofidentificationWacholder et al (2000) showed that when experi-mental subjects were asked to assess the usefulnessof terms for an information access task without actu-ally using the terms for information access showedthat the terms identified by the technical term algo-rithm, which are considerably fewer than the termsidentified by head sorting, were overall of higherquality than the terms identified by the head sortingmethod.
However, the fact that subjects assigned ahigh rank to many of the terms identified by HeadSorting suggested that the technical term algorithmwas failing to pick up many potentially useful indexterms.In preparation for the experiment, all index termswere merged into a single list and duplicates wereremoved, resulting in a list of nearly 10,000 indexterms.2.4 Tracking resultsIn the experiment, we logged the terms that sub-jects searched for (i.e., entered in a search box) andselected.
In this paper, we report only on the termsthat the subjects selected (i.e., clicked on).
This isbecause if a subject entered a single word, or a sub-part of a word in the search box, ESBI returned tothem a list of index terms; the subject then selected aterm to view the context in which it appears in thetext.
This term might have been the same term origi-nally searched for or it might have been a super-string.
The terms that subjects selected for searchingare interesting in their own right, but are not analyzedhere.3 ResultsAt the outset of this experiment, we did not knowwhether it would be possible to discover differencesin human preferences for terms in the informationaccess task reported on in this paper.
We thereforestarted our research with the null hypothesis that allindex terms are created equal.
If users selected indexterms in roughly the same proportion as the termsoccur in the text, the null hypothesis would beproven.The results strongly discredit the null hypothesis.Table 3 shows that when measured by percentage ofterms selected, subjects selected on over 13.2% of theavailable human terms, but only 1.73% and 1.43%respectively of the automatically selected terms.
Ta-ble 3 also shows that although the human index termsformed only 6% of the total number of index terms,40% of the terms which were selected by subjects inorder to view the context were identified by humanindexing.
Although 80% of the index terms wereidentified by head sorting, only 51% of the termssubjects chose to select had been identified by thismethod.
(*Because of overlap of terms selected bydifferent techniques, total is greater than 100%)HM HS  TT TotalAll terms  673 7980 1788 9992Percentageof  allterms6.73% 79.9% 17.9% *Totalnumber ofterms se-lected89 114 31 223Percentageof termsselected39.9% 51.1% 13.9 *Percentageof avail-able termsselected13.2% 1.43% 1.73%Table 3: Subject selection of index terms, bymethod.To determine whether the numbers represent statisti-cally significant evidence that the null hypothesis iswrong, we represent the null hypothesis (HT)) as (1)and the falsification of the null hypothesis (HA) as(2).HT: P1/?1 = P2/?2                          (1)HA:  P1/?1 ?
P2/?2                          (2)Pi is the expected percentage of the selected termsthat are type i in all the selected terms; ?i is the ex-pected percentage if there is no user preference, i.e.the proportion of this term type i in all the terms.
Werewrite the above as (3).HT: X = 0    HA: X ?
0    X = P1/?1 ?
P2/?2    (3)Assuming that X is normally distributed, we can usea one-sample t test on X to decide whether to acceptthe hypothesis (1).
The two-tailed t test (df =222)produces a p-value of less than .01% for the compari-son of the expected and selected proportions of a)human terms and head sorted terms and b) humanterms and technical terms.
In contrast, the p-value forthe comparison of head-sorted and technical termswas 33.7%, so we draw no conclusions about relativepreferences for head sorted and technical terms.We also considered the possibility that our formu-lation of questions biased the terms that the subjectsselected, perhaps because the words of the questionsoverlapped more with the terms selected by one ofthe methods.
3 We took the following steps:1) For each search word, calculate the number ofterms overlapping with it from each source.2) Based on these numbers, determine the proportionof terms provided by each method.3) Sum the proportions of all the search words.As measured by the terms the subjects saw duringbrowsing, 22% were human terms, 62% were headsorted terms and 16% were technical terms.
Using thesame reasoning about the null hypothesis as above,the p-value for the comparison of the ratios of humanand head sorted terms was less than 0.01%, as wasthe comparison of the ratios of the human and techni-cal terms.
This supports the validity of the results ofthe initial test.
In contrast, the p-value for the com-parison of the two automatic techniques was 77.3%.Why did the subjects demonstrate such a strongpreference for the human terms?
Table 4 illustratessome important differences between the human termsand the automatically identified terms.
The termsselected on are longer, as measured in number ofwords, and more complex, as measured by number ofprepositions per index terms and by number of con-tent-bearing words.
As shown in Table 5, the differ-ence of these complexity measures between humanterms and automatically identified terms are statisti-cally significant.Since longer terms are more specific than shorterterms (for example, participation in a democracy islonger and more specific than democracy), the resultssuggest that subjects prefer the more specific terms.If this result is upheld in future research, it has practi-cal implications for the design of automatic termidentification systems.Num-ber oftermsselectedAveragelength ofterm inwordsPreposi-tions perindextermContent-bearingwordsper in-dex termHM 89 6.22 1.4 4.54HS 114 2.59 0.026 2.23TT 31 2.26 0 2.26Table 4: Measures of index term complexityAveragelength ofterm innumber ofwordsNumberof prepo-sitionsper indextermNumber ofcontent-bearingwords perindex termHM vs HS  <0.01% <0.01% <0.01%HM vs TT <0.01% <0.01% <0.01%HS vs TT 0.57% 8.33% 77.8%Table 5: Result of two-independent-sample two-tailed t-test on index term complexity.
The num-bers in the cells are p-value of the test.4.3    Relationship between Term Source andSearch EffectivenessIn this paper, our primary focus is on the question ofwhat makes index terms 'better', as measured by userpreferences in a question-answering task.
Also ofinterest, of course, is what makes index terms 'better'in terms of how accurate the resulting users' answersare.
The problem is that any facile judgment of free-text answer accuracy is bound to be arbitrary andpotentially unreliable; we discuss this in detail in[26].
Nevertheless, we address the issue in a prelimi-nary way in the current paper.
We used an ad hoc setof canonical answers to score subjects' answers on ascale of 1 to 3, where 1 stands for 'very accurate', 2stands for 'partly accurate' and 3 represents 'not at allaccurate'.
Using general loglinear regression (Poissonmodel) under the hypothesis that these two variablesare independent of each other, our analysis showedthat there is a systematic relationship (significanceprobability is 0.0504) between source of selectedterms and answer accuracy.
Specifically, in caseswhere subjects used more index terms identified bythe human indexer, the answers were more accurate.On the basis of our initial accuracy judgments, wecan therefore draw the preliminary conclusion thatterms that were better in that they were preferred bythe experimental subjects were also better in that theywere associated with better answers.
We plan to con-duct a more in-depth analysis of answer accuracy andwill report on it in future work.But the primary question addressed in this paperis how to reliably assess NP chunks and technicalterms.
These results constitute experimental evidencethat the index terms identified by the human indexerconstitute a gold standard, at least for the text used inthe experiment.
Any set of index terms, regardless ofthe technique by which they were created or the crite-ria by they were selected, can be compared vis a vistheir usefulness in the information access task.4 DiscussionThe contribution of this paper is the description of atask-based gold-standard method for evaluating theusefulness and therefore the quality of NP chunksand technical terms.
In this section, we address anumber of questions about this method.1) What properties of terms can this techniquebe used to study??
One word or many.
There are two parts tothe process of identifying NP terms: NPchunks that are candidate terms must beidentified and candidate terms must be fil-tered in order to select a subset appropriatefor use in the intended application.
Justesonand Katz (1995) is an example of an algo-rithm where the process used for identifyingNP chunks is also the filtering process.
Abyproduct of this technique is that single-word terms are excluded.
In part, this is be-cause it is much harder to determine in con-text which single words actually qualify asterms.
But dictionaries of technical termi-nology have many one-word terms.?
Simplex or complex NPs (e.g., Church1988; Hindle and Rooth 1991; Wacholder1998) identify simplex or base NPs ?
NPswhich do not have any component NPs -- atleast in part because this bypasses the needto solve the quite difficult attachment prob-lem, i.e., to determine which simpler NPsshould be combined to output a more com-plex NP.
But if people find complex NPsmore useful than simpler ones, it is impor-tant to focus on improvement of techniquesto reliably identify more complex terms.?
Semantic and syntactic terms variants.Daille et al (1996), Jacquemin (2001) andothers address the question of how to iden-tify semantic (synonymous) and syntacticvariants.
But independent of the question ofhow to recognize variants is the question ofwhich variants are to be preferred for differ-ent kinds of uses.?
Impact of errors.
Real-world NLP systemshave a measurable error rate.
By conductingexperiments in which terms with errors areinclude in the set of test terms, the impact ofthese errors can be measured.
The useful-ness of a set of terms presumably is at leastin part a function of the impact of the errors,whether the errors are a by-product of thealgorithm or the implementation of the algo-rithm.2) Could the set of human index terms be usedas a gold standard without conducting thehuman subject experiments?
This of coursecould be done, but then the terms are beingevaluated by a fixed standard ?
by definition, noset of terms can do better than the gold standard.This experimental method leaves open the possi-bility that there is a set of terms that is betterthan the gold standard.
In this case, of course, thegold standard would no longer be a gold standard-- perhaps we would have to call it a platinumstandard.3) How reproducible is the experiment?
The ex-periment can be re-run with any set of termsdeemed to be representative of the content of theRice text.
The preparation of the materials foradditional texts is admittedly time-consuming.But over time a sizable corpus of experimentalmaterials in different domains could be built up.These materials could be used for training aswell as for testing.4) How extensible is the gold standard?
The ex-perimental protocol will be validated only ifequally useful index terms can be created forother texts.
We anticipate that they can.5) How can this research help in the design ofreal world NLP systems?
This technique canhelp in assessing the relative usefulness of exist-ing techniques for identifying terms.
It is possi-ble, for example, there already exist techniquesfor identifying terms that are superior to the twotested here.
If we can find such systems, their al-gorithms should be preferred.
If not, there re-mains a need for development of algorithms toidentify single word terms and complex phrases.6) Do the benefits of this evaluation techniqueoutweigh the costs?
Given the fundamental dif-ficulty of evaluating NP chunks and technicalterms, task-based evaluation is a promising sup-plement to evaluation by precision and recall.These relatively time-consuming human subjectexperiments surely will not be undertaken bymost system developers; ideally, they should beperformed by neutral parties who do not have astake in the outcome.7) Should automated indexes try to imitate hu-man indexers?
Automated indexes should  con-tain terms that are most easily processed byusers.
If the properties of such terms can be re-liably discovered, developers of systems thatidentify terms intended to be processed by peo-ple surely should pay attention.5 ConclusionIn this paper we have reported on a rigorous experi-mental technique for black-box evaluation of the use-fulness of NP chunks and technical terms in aninformation access task.
Our experiment shows that itis possible to reliably identify human preferences forsets of terms.The set of human terms created for use in a back-of-the-book index serves as a gold standard.
An ad-vantage of the task-based evaluation is that a set ofterms could outperform the gold standard; any systemthat could do this would be a good system indeed.The two automatic methods that we evaluatedperformed much less well than the terms created bythe human indexer; we plan to evaluate additionaltechniques for term identification in the hope of iden-tifying automatic methods that identify index termsthat people prefer over the human terms.
We alsoplan to prepare test materials in different domains,and assess in greater depth the properties of the termsthat our experimental subjects preferred; our goal isto develop practical guidelines for the identificationand selection of technical terms that are optimal forhuman users.
We will also study the impact of se-mantic differences between terms on user preferencesand investigate whether terms which are preferred forinformation access are equally suitable for other NLPtasks.6 AcknowledgementsWe are grateful to the other members of the RutgersNLP-I research group, Lu Liu, Mark Sharp, andXiaojun Yuan, for their valuable contribution to thisproject.
We also thank Paul Kantor, Judith L. Kla-vans, Evelyne Tzoukermann , Min Yen Kan, andthree anonymous reviewers for their helpful sugges-tions.
Funding for this research has been provided bythe Rutgers University Information Science andTechnology Council.ReferencesAbney, Steven (1991) Parsing by chunks.
Principle-Based Parsing, edited by Steven Abney, RobertBerwick and Carol Tenny.
Kluwer: Dordrecht.Bates, Marcia J.
(1998) Indexing and access for digitallibraries and the Internet: human, database and do-main factors.
Journal of the American Society forInformation Science, 49(13), 1185-1205.Boguraev, Branimir and Kennedy, Christopher (1997)Salience-based content characterization of text.
ACLEACL Workshop on Intelligent Scalable TextSummarization, 2-9.Bourigault, Didier, Jacquemin, Christian and L?Homme,Marie Claude (2001) Recent Advances in Computa-tional Terminology.
John Benjamins: Philadelphia,PA.Church, Kenneth Ward (1988) A Stochastic Parts Pro-gram and Noun Phrase Parser for Unrestricted Text.Proceedings of Second Applied Natural LanguageProcessing Conference, pp.136-143.Dagan, Ido and Church, Kenneth (1994) TERMIGHT:Identifying and translating technical terminology.Proceedings of the Fourth ACL Conference on Ap-plied Natural Language Processing, pp.34-40.Daille Beatrice (1996) Study and implementation ofcombined techniques for automatic extraction of ter-minology.
The Balancing Act, pp.49-66.
Edited byJudith L. Klavans and Philip Resnik.
MIT Press,Cambridge, MA.Daille, Beatrice, Habert, Benoit., Jacquemin, Christian,& Royaute, Jean (2000) Empirical observation ofterm variations and principles for their description.Terminology, 3(2):197-258.Furnas, George, Landauer, Thomas, Gomez, Louis &Dumais, Susan T. (1987) The vocabulary problemin human-system communication.
Communicationsof the ACM, 30(11), 964-971.Galliers, Julia Rose & Jones, Karen Sparck (1995)Evaluating natural language processing systems.
Lec-ture Notes in Artificial Intelligence.
Springer, NewYork, 1995.Jacquemin, Christian (2001).
Spotting and DiscoveringTerms through Natural Language Processing.
Cam-bridge, MA: MIT Press.Jones, Steve and Staveley, Mark S. (1999) Phrasier: asystem for interactive document retrieval using key-phrases.
Proceedings of the 22nd annual interna-tional ACM SIGIR conference, pp.160-167.Justeson, John S. & Slava M. Katz (1995) ?Technicalterminology: some linguistic properties and an algo-rithm for identification in text?, Natural LanguageEngineering 1(1):9-27.Hindle, Donald and Rooth, Matt (1993) Structural am-biguity and lexical relations.
Computational Linguis-tics 19(1):103-120.Lawrie, Dawn and Croft, W. Bruce (2000) Discoveringand comparing topic hierarchies.
Proceedings  ofRIAO 2000 Conference, 314-330.McKeown, Kathy, Klavans, Judith, Hatzivassiloglou,Vasileios, Barzilay, Regina and Eskin, Eleazar(1999) Towards multidocument summarization byreformulation: Progress and prospects.
Proceedingsof AAAI-99, pp.453-460.Nevill-Manning, Craig, Witten, Ian and Paynter,Gordon W. (1999).
Lexically-generated subject hi-erarchies for browsing large collections.
Int?l Jour-nal on Digital Libraries, 2(2-3):111-123.Oakes, Michael P. and Paice, Chris D. (2001) Term ex-traction for automatic abstracting.
In Bourigault etal., eds.Ramshaw, Lance A., and Marcus, Mitchell P. (1995)Text chunking using transformation-based learning.Proceedings of the Third ACL Workshop on VeryLarge Corpora, pp.
82-94.Rice, Ronald E., Maureen McCreadie & Shan-ju L.Chang (2001).
Accessing and Browsing Informationand Communication.
Cambridge, MA: MIT Press.Saracevic, Tefko, Paul Kantor, Alice Y. Chamis &Donna Trivison (1988) A study of informationseeking and retrieving: I.
Background and method-ology.
Journal of the American Society for Infor-mation Science, 39(3), 161-176.Sharp, M., Liu, L., Yuan, X., Song, P., & Wacholder, N.(2003).
Question difficulty effects on question an-swering involving mandatory use of a term index.Under submission.Wacholder, N., Sharp, M., Liu, L., Yuan, X., & Song, P.(2003).
Experimental study of index terms and in-formation access.
Under submission.Wacholder, Nina (1998) "Simplex noun phrases clus-tered by head: a method for identifying significanttopics in a document", Proceedings of Workshop onthe Computational Treatment of Nominals, pp.70-79.
COLING-ACL, October 16, 1998.Wacholder, Nina, Judith L. Klavans and David KirkEvans (2000) "Evaluation of automatically identifiedindex terms for browsing electronic documents",Proceedings of the NAACL/ANLP2000, Seattle,Washington.Witten, Ian H., Paynter, Gordon W., Eibe, Frank, Gut-win, and Nevill-Manning Craig G. KEA: practicalautomatic keyphrase extraction.
Proceedings of thefourth ACM Conference on Digital Libraries,pp.254-255.
