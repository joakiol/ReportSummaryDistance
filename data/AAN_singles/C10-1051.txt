Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 447?455,Beijing, August 2010A Novel Reordering Model Based on Multi-layer Phrase for Sta-tistical Machine TranslationYanqing He1,     Yu Zhou2,     Chengqing Zong2,     Huilin Wang11Institute of Scientific and TechnicalInformation of China{heyq,wanghl}@istic.ac.cn2Institute of Automation, ChineseAcademy of Sciences{yzhou,cqzong}@nlpr.ia.ac.cnAbstractPhrase reordering is of great importancefor statistical machine translation.
Ac-cording to the movement of phrase trans-lation, the pattern of phrase reorderingcan be divided into three classes: mono-tone, BTG (Bracket TransductionGrammar) and hierarchy.
It is a goodway to use different styles of reorderingmodels to reorder different phrases ac-cording to the characteristics of both thereordering models and phrases itself.
Inthis paper a novel reordering modelbased on multi-layer phrase (PRML) isproposed, where the source sentence issegmented into different layers of phras-es on which different reordering modelsare applied to get the final translation.This model has some advantages: differ-ent styles of phrase reordering modelsare easily incorporated together; when acomplicated reordering model is em-ployed, it can be limited in a smallerscope and replaced with an easier reor-dering model in larger scope.
So thismodel better trade-offs the translationspeed and performance simultaneously.1 IntroductionIn statistical machine translation (SMT), phrasereordering is a complicated problem.
Accordingto the type of phrases, the existing phrase reor-dering models are divided into two categories:contiguous phrase-based reordering models andnon-contiguous phrase-based reordering models.Contiguous phrase-based reordering modelsare designed to reorder contiguous phrases.
Insuch type of reordering models, a contiguousphrase is reordered as a unit and the movementsof phrase don?t involve insertions inside the oth-er phrases.
Some of these models are content-independent, such as distortion models (Och andNey, 2004; Koehn et al, 2003) which penalizetranslation according to jump distance of phrases,and flat reordering model (Wu, 1995; Zens et al,2004)which assigns constant probabilities formonotone order and non-monotone order.
Thesereordering models are simple and the contents ofphrases have not been considered.
So it?s hard toobtain a satisfactory translation performance.Some lexicalized reordering models (Och et al,2004; Tillmann 2004, Kumar and Byrne, 2005,Koehn et al, 2005) learn local orientations (mo-notone or non-monotone) with probabilities foreach bilingual phrase from training data.
Thesemodels are phrase-dependent, so improvementsover content-independent reordering models areobtained.
However, many parameters need to beestimated.Non-contiguous phrase-based reorderingmodels are proposed to process non-contiguousphrases and the movements of phrase involveinsertion operations.
This type of reorderingmodels mainly includes all kinds of syntax-based models where more structural informationis employed to obtain a more flexible phrasemovement.
Linguistically syntactic approaches(Yamada and Knight, 2001; Galley et al, 2004,2006; Marcu et al, 2006; Liu et al, 2006; Shie-ber et al, 1990; Eisner, 2003; Quirk et al, 2005;Ding and Palmer, 2005) employ linguisticallysyntactic information to enhance their reorderingcapability and use non-contiguous phrases to447obtain some generalization.
The formally syn-tax-based models use synchronous context-freegrammar (SCFG) but induce a grammar from aparallel text without relying on any linguisticannotations or assumptions (Chiang, 2005;Xiong et al, 2006).
A hierarchical phrase-basedtranslation model (HPTM) reorganizes phrasesinto hierarchical ones by reducing sub-phrases tovariables (Chiang 2005).
Xiong et al (2006) isan enhanced bracket transduction grammar witha maximum entropy-based reordering model(MEBTG).
Compared with contiguous phrase-based reordering model, Syntax-based modelsneed to shoulder a great deal of rules and havehigh computational cost of time and space.
Thetype of reordering models has a weaker ability ofprocessing long sentences and large-scale data,which heavily restrict their application.The above methods have provided variousphrases reordering strategies.
According to themovement of phrase translation, the pattern ofphrase reordering can be divided into threeclasses: monotone, BTG (Bracket TransductionGrammar) (Wu, 1995) and hierarchy.
In fact formost sentences, there may be some phraseswhich have simple reordering patterns, such asmonotone or BTG style.
It is not necessary toreorder them with a complicated mechanism, e.g.hierarchy.
It is a good idea that different reorder-ing models are employed to reorder differentphrases according to the characteristics of boththe reordering models and the phrases itself.This paper thus gives a novel reordering modelbased on multi-layer phrase (PRML), where thesource sentence is segmented into different lay-ers of phrases on which different reorderingmodels are applied to get the final translation.Our model has the advantages as follow: (1)PRML segments source sentence into multiple-layer phrases by using punctuation and syntacticinformation and the design of segmentation al-gorithm corresponds to each reordering model.Different reordering models are chosen for eachlayer of phrases.
(2) In our model different reor-dering models can be easily integrated togetherto obtain a combination of multiple phrase reor-dering models.
(3) Our model can incorporatesome complicated reordering models.
We limitthem in relatively smaller scopes and replacethem with easier reordering models in largerscopes.
In such way our model better trade-offsthe translation speed and performance simulta-neously.
(4) Our segmentation strategy doesn?timpair translation quality by controlling phrasetranslation tables to determine the scope of eachreordering model in each source sentence.
Thepoor phrase translations generated by the formerreordering model, still have chances of beingrevised by the latter reordering model.Our work is similar to the phrase-level systemcombination (Mellebeek et al, 2006).
We shareone important characteristic: we decompose in-put sentence into chunks and recompose thetranslated chunks in output.
The differences arethat, we segment the input sentence into multi-layer phrases and we reorder their translationswith a multi-layer decoder.The remainder of the paper is organized asfollows: Section 2 gives our reordering modelPRML.
Section 3 presents the details of the sen-tence segmentation algorithm and the decodingalgorithm.
Section 4 shows the experimental re-sults.
Finally, the concluding remarks are givenin Section 5.2 The ModelWe use an example to demonstrate our motiva-tion.
Figure 1 shows a Chinese and English sen-tence pair with word alignment.
Each solid linedenotes the corresponding relation between aChinese word and an English word.
Figure 2shows our reordering mechanism.
For the sourcesentence, the phrases in rectangle with roundcorner in row 2 obviously have a monotonetranslation order.
For such kinds of phrase a mo-notone reordering model is enough to arrangetheir translations.
Any two neighbor consecutivephrases in the ellipses in row 3 have a straightorders or inverted order.
So BTG reorderingmodel is appropriate to predict the order of thistype of phrases.
Inside the phrases in the ellipsesin row 3 there are possibly more complicatedhierarchical structures.
For the phrase ???
???
?
?, a rule ?
1 1X towards the road to X&o ??
???
?has the ascendancy over the monotone and BTGstyle of reordering model.
Hierarchy style ofreordering models, such as HPTM reorderingmodel, can translate non-contiguous phrases andhas the advantage of capturing the translation ofsuch kind of phrases.The whole frame of our model PRML isshown in Figure 3.
PRML is composed of a448segmentation sentence module and a decoderwhich consists of three different styles of phrasereordering models.
The source sentence is seg-mented into 3 layers of phrases: the originalwhole sentence, sub-sentences and chunks.
Theoriginal whole sentence is considered as thefirst-layer phrase and is segmented into sub-sentences to get the second-layer phrase.
By fur-ther segmenting these sub-sentences, the chunksare obtained as the third-layer phrase.
The wholetranslation process includes three steps: 1) Inorder to capture the most complicated structureof phrases inside chunks, HPTM reorderingmodel are chosen to translate the chunks.
So thetranslations of chunks are obtained.
2) Combinethe bilingual chunks generated by step 1 withthose bilingual phases generated by the MEBTGtraining model as the final phrase table andtranslate the sub-sentences with MEBTG reor-dering model, the translations of sub-sentencesare obtained.
3) Combine the bilingual sub-sentences generated by step 2 with those bilin-gual phases generated by the Monotone trainingmodel as the final phrase table and translate theoriginal whole sentences with monotone reorder-Figure 1.
An example of Chinese-English sentence pair with their word alignmentFigure 2.
Diagram of Translation Using PRML.Figure 3.
Frame of PRML449Figure 4.
General frame of our modeling model, the translations of  the original wholesentences are obtained.We also give a general frame of our model inFigure 4.
In the segmentation module, an inputsource sentence is segmented into G layers ofcontiguous source strings, Layer 1, Layer 2, ?,Layer G. The phrases of lower-order layer arere-segmented into the phrases of higher-orderlayer.
The phrases of the same layer can becombined into the whole source sentence.
Thedecoding process starts from the phrases of thehighest-order layer.
For each layer of phrases areordering model is chosen to generate the trans-lations of phrases according to their characteris-tics.
The generated translations of phrases in thehigher-order layer are fed as a new added trans-lation source into the next lower-order reorder-ing model.
After the translations of the phrase inLayer 2 are obtained, they are fed into the Reor-dering model 1 as well as the source sentence(the phrase in Layer 1) to get the target transla-tion.Due to the complexity of the language, theremay be some sentences whose structures don?tconform to the pattern of the reordering modelswe choose.
So in our segmentation module, ifthe sentence doesn?t satisfy the segmentationconditions of current layer, it will be fed into thesegmentation algorithm of the next layer.
Evenin the worst condition when the sentence isn?tsegmented into any phrase by segmentationmodule, it will be translated as the whole sen-tence to get the final translation by the highest-order reordering model.Our model tries to grasp firstly the simplereordering modes in source sentence by the low-er layer of phrase segmentations and controlsmore complicated reordering modes inside thehigher layers of phrases.
Then we choose somecomplicated reordering models to translate thosephrases.
Thus search space and computationalcomplexity are both reduced.
After obtaining thetranslation of higher layer?s phrases, it is enoughfor simple reordering models to reorder them.Due to phrase segmentation some phrases maybe translated poorly by the higher layer of reor-dering models, but they still have chances of be-ing revised by the lower layer of reorderingmodel because in lower layer of reordering mod-el the input phrases have not these hard segmen-tation boundary and our model uses phrase trans-lation tables to determine the scope of each reor-dering model.There are two key issues in our model.
Thefirst one is how to segment the source sentenceinto different layers of phrases.
The second oneis how to choose a reordering model for differentlayer of phrases.
In any case the design of seg-menting sentence module should consider thecharacteristic of the reordering model of phrases.3 ImplementationThe segmentation module consists of the sub-sentence segmentation and chunk segmentation.The decoder combines three reordering models,HPTM, MEBTG, and a monotone reorderingmodel.3.1 Segmentation moduleWe define the sub-sentence as the word se-quence which can be translated in monotone or-der.
The following six punctua-tions: ?
?
?
 ?
 ?
 ?
in Chinese,and .
!
?
, : ; in English are chosen as the seg-mentation anchor candidates.
Except Chinesecomma, all the other five punctuations can ex-450press one semantic end and another semanticbeginning.
In most of the time, it has high errorrisk to segment the source sentence by commas.So we get help from syntactic information ofChinese dependency tree to guarantee the mono-tone order of Chinese sub-sentences.The whole process of sub-sentencesegmentation includes training and segmenting.Training: 1) The word alignment of trainingparallel corpus is obtained; 2) The parallelsentence pairs in training corpus are segmentedinto sub-sentences candidates.
For a Chinese-English sentence pair with their word alignmentin training data, all bilingual punctuations arefound firstly, six punctuations respectively????????
in Chinese and ??
!
.
, : ;?
inEnglish.
The punctuation identification number(id) sets in Chinese and English are respectivelyextracted.
For a correct punctuation id pair (id_c,id_e), the phrase before id_e in English sentenceshould be the translation of the phrase beforeid_c in Chinese sentence, namely the number ofthe links 1 between the two phrases should beequal.
In order to guarantees the property wecalculate a bilingual alignment ratio for eachChinese-English punctuation id pair according tothe following equation.
For the punctuation idpair (id_c, id_e), bilingual alignment ratioconsists of two value, Chinese-Englishalignment ratio (CER) and English-Chinesealignment ratio (ECR).1 _11 _1( )( )iji id cj Jijj id ei IACERAGGd dd dd dd d?
?1 _11 _1( )( )ijj id ei Iiji id cj JAECRAGGd dd dd dd d?
?where ( )ijAG is an indicator function whose valueis 1 when the word id pair ( , )i j is in the wordalignment and is 0 otherwise.
I and J are thelength of the Chinese English sentence pair.CER of a correct punctuation id pair will beequal to 1.0.
So does ECR.
In view of the errorrate of word alignment, the punctuation id pairswill be looked as the segmentation anchor ifboth CER and ECR are falling into the thresholdrange (minvalue, maxvalue).
Then all thepunctuation id pairs are judged according to thesame method and those punctuation id pairs1 Here a link between a Chinese word and an English wordmeans the word alignment between them.satisfying the requirement segment the sentencepair into sub-sentence pairs.
3) The first word ofChinese sub-sentence in each bilingual sub-sentence pair is collected.
We filter these wordswhose frequency is larger than predefinedthreshold to get segmentation anchor word set(SAWS).Segmenting: 1) The test sentence in Chinese issegmented into segments by the six Chinesepunctuation ????????
in the sentence.
2)If the first word of a segment is in SAWS thepunctuation at the end of the segment is chosenas the segmentation punctuation.
3) If a segmentsatisfies the property of ?dependency integrity?the punctuation at the end of the segment is alsochosen as the segmentation punctuation.
Here?dependency integrity?
is defined in adependency tree.
Figure 5 gives the part outputFigure 5.
The part dependency parser outputof a Chinese sentence.of ?lexical dependency parser?2  for a Chinesesentence.
There are five columns of data for eachword which are respectively the word id, theword itself, its speech of part, the id of its headword and their dependency type.
In the sentencethe Chinese word sequence ???
??
??
??
(US congressional representatives say that)?has such a property: Each word in the sequencehas a dependency relation with the word whichis still in the sequence except one word whichhas a dependency relation with the root, e.g.
id 4.We define the property as ?dependency integri-ty?.
Our reason is: a sub-sentence with the prop-erty of ?dependency integrity?
has relatively in-dependent semantic meaning and a large possi-bility of monotone translation order.
4) The un-ion of the segmentation punctuations in step 2)and 3) are the final sub-sentence segmentationtags.2 http://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.htmlID              word          POS        head id  dependency type1 ??
NR 3 NMOD2 ??
NN 3 NMOD3 ??
NN 4 SUB4 ??
VV 0 ROOT5 ?
PU 4 P6 ???
NN 7 VMOD7 ??
VV 9 VMOD8 ?
PU 9 P?
?
?
?
?
?
?
?
?
?451After sub-sentence segmentation, chunkssegmentation is carried out in each sub-sentence.We define the chunks as the word sequencewhich can be translated in monotone order orinverted order.
Here the knowledge of the?phrase structure parser?
3  and the ?lexicalizeddependency parser?
are integrated to segmentthe sub-sentence into chunks.
In a Chinesephrase structure parser tree the nouns phrase (NP)and preposition phrase (PP) are relatively inde-pendent in semantic expressing and relativelyflexible in translation.
So in the chunk segmenta-tion, only the NP structure and PP structure inthe Chinese structure parsing tree are found asphrase structure chunk.
The process of chunksegmentation is described as follows: 1) the testsub-sentence is parsed to get the phrase structuretree and dependency parsing tree; 2) We traversethe phrase structure tree to extract sub-tree of?NP?
and ?PP?
to obtain the phrase structurechunks.
3) We mark off the word sequences with?dependency integrity?
in the dependency tree.
4)Both the two kinds of chunks are recombined toobtain the final result of chunk segmentation.3.2 DecodingOur decoder is composed of three styles of reor-dering models: HPTM, MEBTG and a monotonereordering model.According to Chiang (2005), given thechunk chunkc , a CKY parser finds ch u n ke, the Eng-lish yield of the best derivation hptmDthat hasChinese yield chunkc :( )( )( argmax Pr( ))hptm chunkchunk chunk hptmchunk hptmC D Ce e De DHere the chunks not the whole source sentenceare fed into HPTM decoder to get the L-besttranslations and feature scores of the chunks.
Wecombine all the chunks, their L-best translationsand the feature scores into a phrase table, namelychunk phrase table.
We only choose 4 translationscores (two translation probability based on fre-quency and two lexical weights based on wordalignment) because the language model score,phrase penalty score and word penalty score willbe re-calculated in the lower layer of reordering3 http://nlp.stanford.edu/software/lex-parser.shtmlmodel and need not be kept here.
Meantime wechange the log values of the scores into probabil-ity value.
In the chunk phrase table each phrasepair has a Chinese phrase, an English phrase andfour translations feature scores.
In each phrasepair the Chinese phrase is one of our chunks, theEnglish phrase is one translation of L-best of thechunk.In MEBTG (Xiong et al, 2006), three rulesare used to derive the translation of each sub-sentence: lexical rule, straight rule and invertedrule.
Given a source sub-sentence sub sentC  , itfinds the final sub-sentence translation sub sentE from the best derivation m eb tgD:( )( )( arg max Pr( ))mebtg sub sentsub sent sub sent mebtgmebtgC D CE E DE D  Generally chunk segmentation will make someHPTM rules useless and reduce the translationperformance.
So in MEBTG we also use basephrase pair table which contains the contiguousphrase translation pairs consistent with wordalignment.
We merge the chunk phrase tableand base phrase table together and feed theminto MEBTG to translate each sub-sentence.Thus the K-Best translation and feature scores ofeach sub-sentence are obtained and then are re-combined into a new phrase table, namely sub-sentence phrase table, by using the same methodwith chunk phrase table.Having obtained the translation of each sub-sentence we generate the final translation of thewhole source sentence by a monotone reorderingmodel.
Our monotone reordering model employsa log-linear direct translation model.
Threephrase tables: chunk phrase table, sub-sentencephrase table and base phrase table are mergedtogether and fed into the monotone decoder.Thus the decoder will automatically choosethose phrases it need.
In each phrase table eachsource phrase only has four translation probabili-ties for its candidate translation.
So it?s easy tomerge them together.
In such way all kinds ofphrase pairs will automatically compete accord-ing to their translation probabilities.
So ourPRML model can automatically decide whichreordering model is employed in each phrasescope of the whole source sentence.
It?s worthnoting that the inputs of the three reordering452model have no segmentation tag.
Because anysegmentation for the input before decoding willinfluence the use of some rules or phrase pairsand may cause some rules or phrase pairs losses.It would be better to employ different phrasetable to limit reordering models and let each de-coder automatically decide reordering model foreach segments of the input.
Thus by controllingthe phrase tables we apply different reorderingmodels on different phrases.
For each reorderingmodel we perform the maximum BLEU training(Venugopal et al 2005) on a development set.For HPTM the training is same as Chiang 2007.For MEBTG we use chunk phrase table and basetable to obtain translation parameters.
For mono-tone reordering model all the three phrase tablesare merged to get translation weights.4  ExperimentsThis section gives the experiments with Chinese-to-English translation task in news domain.
Ourevaluation metric is case-insensitive BLEU-4(Papineni et al 2002).
We use NIST MT 2005,NIST MT 2006 and NIST MT 2008 as our testdata.
Our training data is filtered from the LDCcorpus4.
Table 1 gives the statistics of our data.4.1 Evaluating translation PerformanceWe compare our PRML against two baselines:MEBTG system developed in house accordingto Xiong (2006, 2008) and HPTM system5 inPYTHON based on HPTM reordering model(Chiang 2007).
In MEBTG phrases of up to 10words in length on the Chinese side are extractedand reordering examples are obtained withoutlimiting the length of each example.
Only thelast word of each reordering example is used aslexical feature in training the reordering modelby the maximum entropy based classifier6.
Wealso set a swapping window size as 8 and thebeam threshold as 10.
It is worth noting that ourMEBTG system uses cube-pruning algorithm(Chiang 2005) from bottom to up to generate the4 LDC corpus lists: LDC2000T46,  LDC2000T50,LDC2002E18, LDC2002E27, LDC2002L27, LDC2002T01,LDC2003E07, LDC2003E14, LDC2003T17, LDC2004E12,LDC2004T07, LDC2004T08, LDC2005T01, LDC2005T06,LDC2005T10, LDC2005T34, LDC2006T04, LDC2007T095 We are extremely thankful to David Chiang who original-ly implement the PYTHON decoder and share with us.6 http://maxent.sourceforge.net/Set Language Sentence Vocabulary A. S. LTraindataChinese 297,069 6,263 11.9English 297,069 8,069 13.6NIST05Chinese 1,082 5669 28.2English 4,328 7575 32.7NIST06Chinese 1,664 6686 23.5English 6,656 9388 28.9NIST08Chinese 1,357 6,628 24.5English 5,428 9,594 30.8Table 1.
The statistics of training data and testdata, A. S. L is average sentence length.N-best list not the lazy algorithm of (Huang andChiang, 2005).
We also limit the length of theHPTM initial rules no more than 10 words andthe number of non-terminals within two.
In thedecoding for the rules the beam pruning parame-ter is 30 and threshold pruning parameter is 1.0.For hypotheses the two pruning parameters arerespectively 30 and 10.
In our PRML minva-lue=0.8, maxvalue=1.25, which are obtained byminimum error rate training on the developmentset.
The predefined value for filtering SAWS isset as 100.The translation performance of the three reor-dering model is shown in Table 2.
We can findthat PRML has a better performance thanMEBTG with a relatively 2.09% BLEU score inNIST05, 5.60% BLEU score in NIST06 and5.0% BLEU score in NIST08.
This indicates thatthe chunk phrase table increases the reorderingability of MEBTG.
Compared with HPTM,PRML has a comparable translation performancein NIST08.
In NIST05 and NIST06 our modelhas a slightly better performance than HPTM.Because PRML limit hierarchical structure reor-dering model in chunks while HPTM use themin the whole sentence scope (or in a lengthscope), HPTM has a more complicated reorder-ing mechanism than PRML.
The experiment re-sult shows even though we use easier reorderingmoels in larger scope, e.g.
MEBTG and monoto-Model Nist05 Nist06 Nist08HPTM 0.3183 0.1956 0.1525MEBTG 0.3049 0.1890 0.1419PRML 0.3205 0.1996 0.1495Table 2.
The translation performance453ne reordering model, we have a comparativelytranslation performance as HPTM.4.2 Evaluating translation speedTable 3 shows the average decoding time on testdata for the three phrase reordering models on adouble processor of a dual 2.0 Xeon machine.Time denotes mean time of per-sentence, inseconds.
It is seen that PRML is the slower thanMEBTG but reduce decoding time with a rela-tively 54.85% seconds in NIST05, 75.67%seconds in NIST06 and 65.28% seconds inNIST08.
For PRML, 93.65% average decodingtime in NIST05 is spent in HPTM, 4.89% timein MEBTG and 1.46% time in monotone reor-dering decoder.Model Nist05 Nist06 Nist08HPTM 932.96 1235.21 675MEBTG 43.46 27.16 10.24PRML 421.20 300.52 234.33Table 3.
The average decoding time4.3 Evaluating the performance of eachlayer of phrase tableIn order to evaluate the performance of eachreordering model, we run the monotone decoderwith different phrase table in NIST05.
Table 4list the size of each phrase table.
From the re-sults in Table 5 it is seen that the performance ofusing three phrase tables is the best.
Comparedwith the base phrase table, the   translation per-formances are improved with relatively 10.86%BLEU score by adding chunk phrase table and11% BLEU score by adding sub-sentence table.The result of row 4 has a comparable to the onein row 5.
It indicates the sub-sentence phrasetable has contained the information of HPTMreordering model.
The case of row 4 to row 2 isthe same.Phrase table Phrase pairBase 732732Chunk 86401Sub-sentence 24710Table 4.
The size of each phrase table.Phrase table Reordering model BLEUBase Monotone 0.2871Base +chunk monotone+HPTM 0.3180Base +sub-sentence tablemonotone+HPTM+MEBTG0.3187Base +chunk+subsentencemonotone+HPTM+MEBTG0.3205Table 5.
The performance of phrase table5  ConclusionsIn this paper, we propose a novel reorderingmodel based on multi-layer phrases (PRML),where the source sentence is segmented into dif-ferent layers of phrases and different reorderingmodels are applied to get the final translation.Our model easily incorporates different styles ofphrase reordering models together, includingmonotone, BTG, and hierarchy or other morecomplicated reordering models.
When a compli-cated reordering model is used, our model canlimit it in a smaller scope and replace it with aneasier reordering model in larger scope.
In suchway our model better trade-offs the translationspeed and performance simultaneously.In the next step, we will use more features tosegment the sentences such as syntactical fea-tures or adding a dictionary to supervise thesegmentation.
And also we will try to incorpo-rate other systems into our model to improve thetranslation performance.6 AcknowledgementsThe research work has been partially funded bythe Natural Science Foundation of China underGrant No.
6097 5053, and 60736014, the Na-tional Key Technology R&D Program underGrant No.
2006BAH03B02, the Hi-Tech Re-search and Development Program (?863?
Pro-gram) of China under Grant No.2006AA010108-4, and also supported by theChina-Singapore Institute of Digital Media(CSIDM) project under grant No.
CSIDM-200804, and Research Project ?Language andKnowledge Technology?
of Institute of Scientif-ic and Technical Information of China(2009DP01-6).454ReferencesDavid Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of ACL 2005, pages 263?270.David Chiang, 2007.
Hierarchical Phrase-basedTranslation.
Computational Linguistics,33(2):201-228.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependencyinsertion grammars.
In proceeding of 43th Meet-ing of the Association for Computational Linguis-tics, 541-548Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In proceedingsof the 41th Meeting of the Association for Compu-tational Linguistics (companion volume).Michel Galley, Mark Hopkins, Kevin Knight andDaniel Marcu.
2004.
What?s in a translation rule?In proceedings of HLTNAACL- 2004.Michel Galley, Jonathan Graehl, Kevin Knight, Da-niel Marcu, Steve DeNeefe, Wei Wang, IgnacioThayer.
2006.
Scalable Inference and Training ofContext-Rich Syntactic Translation Models.
InProceedings of the joint conference of the Interna-tional Committee on Computational Linguisticsand the Association for Computational Linguistics.Sydney, Australia.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technology, Vancouver,October, pages 53?64.Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu, 2002.
BLEU: a method for auto-matic evaluation of machine translation.
In Pro-ceedings of the 40th Annual Meeting of the ACL.page 311-318, Philadelphia, PA.Philipp Koehn, Franz J. Och and Daniel Marcu.
2003.Statistical phrase-based translation.
In proceed-ings of HLT-NAACL-03, 127-133Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne andDavid Talbot.
2005.
Edinburgh System Descrip-tion for the 2005 IWSLT Speech Translation Eval-uation.
In International Workshop on Spoken Lan-guage Translation.Shankar Kumar and William Byrne.
2005.
Localphrase reordering models for statistical machinetranslation.
In Proceedings of HLT-EMNLP.Yang Liu, Qun Liu and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical MachineTranslation.
In proceedings of ACL-06, 609-616.Daniel Marcu and William Wong.
2002.
A phrase-based, joint probability model for statistical ma-chine translation.
In proceedings of EMNLP-02,133-139.Daniel Marcu, Wei Wang, Abdessamad Echihabi,and Kevin Knight.
2006.
SPMT: Statistical Ma-chine Translation with Syntactified Target Lan-guage Phrases.
In Proceedings of EMNLP-2006,44-52, Sydney, AustraliaBart Mellebeek, Karolina Owczarzak, Josef Van Ge-nabith, Andy Way.
2006.
Multi-Engine MachineTranslation By Recursive Sentence Decomposition.In Proceedings of AMTA 2006Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417-449Franz Josef Och, Ignacio Thayer, Daniel Marcu, Ke-vin Knight, Dragos Stefan Munteanu, Quamrul Ti-pu, Michel Galley, andMark Hopkins.
2004.
Arab-ic and Chinese MT at USC/ISI.
Presentation givenat NIST Machine Translation Evaluation Work-shop.Chris Quirk, Arul Menezes and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal SMT.
In proceedings of the 43thMeeting of the Association for ComputationalLinguistics, 271-279S.
Shieber and Y. Schabes.
1990.
Synchronous treeadjoining grammars.
In proceedings of COLING-90.Christoph Tillmann.
2004.
A block orientation modelfor statistical machine translation.
In HLT-NAACL, Boston, MA, USA.Ashish Venugopal, Stephan Vogel and Alex Waibel.2003.
Effective Phrase Translation Extractionfrom Alignment Models, in Proceedings of the 41stACL,  319-326.Dekai Wu.
1995.
Stochastic inversion transductiongrammars, with application to segmentation,bracketing, and alignment of parallel corpora.
Inproceeding of IJCAL 1995, 1328-1334,Montreal,August.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Max-imum Entropy Based phrase reordering model forstatistical machine translation.
In proceedings ofCOLING-ACL, Sydney, Australia.Deyi Xiong, Min Zhang, Ai Ti Aw, Haitao Mi, QunLiu and Shouxun Lin.
Refinements in BTG-basedStatistical Machine Translation.
In Proceedings ofIJCNLP 2008.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In proceedingsof the 39th Meeting of the ACL, 523-530.R.
Zens, H. Ney, T. Watanabe, and E. Sumita.
2004.Reordering Constraints for Phrase-Based Statis-tical MachineTranslation.
In Proceedings of CoL-ing 2004, Geneva, Switzerland, pp.
205-211.455
