Ranking Text Units According to Textual Saliency, Connect iv i tyand Topic AptnessAnton io  Sanfilippo*LINGLINKAnite Systems13 rue Robert StumperL-2557 LuxembourgAbst ractAn efficient use of lexical cohesion is describedfor ranking text units according to their contri-bution in defining the meaning of a text (textualsaliency), their ability to form a cohesive sub-text (textual connectivity) and the extent andeffectiveness to which they address the differenttopics which characterize the subject matter ofthe text (topic aptness).
A specific applicationis also discussed where the method described isemployed to build the indexing component of asummarization system to provide both genericand query-based indicative summaries.1 In t roduct ionAs information systems become a more inte-gral part of personal computing, it appearsclear that summarization technology must beable to address users' needs effectively if it isto meet the demands of a growing market inthe area of document management.
Minimally,the abridgement of a text according to a user'sneeds involves selecting the most salient por-tions of the text which are topically best suitedto represent the user's interests.
This selec-tion must also take into consideration the de-gree of connectivity among the chosen text por-tions so as to minimize the danger of produc-ing summaries which contain poorly linked sen-tences.
In addition, the assessment of textualsaliency, connectivity and topic aptness mustbe computed efficiently enough so that summa-?
This work was carried out within the InformationTechnology Group at SHARP Laboratories of Europe,Oxford, UK.
I am indebted to Julian Asquith, Jan I J-dens, Ian Johnson and Victor Poznarlski for helpful com-ments on previous versions of this document.. Manythanks also to Stephen Burns for internet programmingsupport., Ralf Steinberger for assistance in dictionaryconversion, and Charlotte Boynton for editorial help.rization can be conveniently performed on-line.The goal of this paper is to show how these ob-jectives can be achieved through a conceptualindexing technique based on an efficient use oflexical cohesion.2 BackgroundLexical cohesion has been widely used in textanalysis for the comparative assessment ofsaliency and connectivity of text fragments.Following Hoey (1991), a simple way of com-puting lexical cohesion in a text is to segmentthe text into units (e.g sentences) and to countnon-stop words 1 which co-occur in each pair ofdistinct text units, as shown in Table 2 for thetext in Table 1.
Text units which contain agreater number of shared non-stop words aremore likely to provide a better abridgement ofthe original text for two reasons:?
the more often a word with high informa-tional content occurs in a text, the moretopical and germane to the text the wordis likely to be, and?
the greater the number of times two textunits share a word, the more connectedthey are likely to be.Text saliency and connectivity for each text unitis therefore stablished by summing the num-ber of shared words associated with the textunit.
According to Hoey, the number of links(e.g.
shared words) across two text units mustbe above a certain threshold for the two textunits to achieve a lexical cohesion rank.
For ex-ample, if only individual scores greater than 21Non-stop words can be intuitively thought of aswords which have high informational content.
They usu-ally exclude words with a very high fequency of occur-rence, especially closed class words such as determiners,prepositions and conjunctions (Fox, 1992).1157#1# Apple Looking for a Partner#2# NEW YORK (Reuter) - Apple is activelylooking for a friendly merger partner,according to several executives closeto the company, the New York Timessaid on Thursday.#3# One executive who does business withApple said Apple employees told himthe company was again in talks withSun Microsystems, the paper said.#4# On Wednesday, Saudi Arabia's PrinceAlwaleed Bin Talal Bin Abdulaziz A1Saud said he owned more than fivepercent of the computer maker's stock,recently buying shares on the openmarket for a total of $115 million.#5# Oracle Corp Chairman Larry Ellisonconfirmed on March 27 he had formed anindependent investor group to gaugeinterest in taking over Apple.#6# The company was not immediatelyavailable to comment.Table h Sample text with numbered text unitsText units#1# #2##1# #3##1# #4##1# #5##1# #6##2# #3##2# #4##2# #5##2# #6##3# #4##3# #5##3# #6##4# #5##4# #6##5# #6#Words shared ScoreApple, look, partner 3Apple, Apple 20Apple 10Apple, Apple,executive, company 40Apple 1company 10Apple, Apple 2company 1000Table 2: Measuring lexical cohesion in text unitpairs.are taken into account, the final scores and con-sequent ranking order computable from Table 2are:.
first: text unit #2# (final score: 7);?
second: text unit #3# (final score: 4), and?
third: text unit #1# (final score: 3).A text abridgement can be obtained by select-ing text units in ranking order according to thetext percentage specified by the user.
For ex-ample, a 35% abridgement of the text in Ta-ble 2 would result in the selection of text units#2# and #3#.As Hoey points out, additional techniquescan be used to refine the assessment of lexi-cal cohesion.
A typical example is the use ofthesaurus functions uch as synonymy and hy-ponymy to extend the notion of word sharingacross text units, as exemplified in Hirst and St-Onge (1997) and Barzilay and Elhadad (1997)with reference to WordNet (Miller et al, 1990).Such an extension may improve on the assess-ment of textual saliency and connectivity thusproviding better generic summaries, as arguedin Barzilay and Elhadad (1997).There are basically two problems with theuses of lexical cohesion for summarization re-viewed above.
First, the basic algorithm re-quires that (i) all unique pairwise permutationsof distinct text units be processed, and (ii) allcross-sentence word combinations be evaluatedfor each such text unit pair.
The complexity ofthis algorithm will therefore be O(n 2 ?
m 2) forn text units in a text and m words in a textunit of average length in the text at hand.
Thisestimate may get worse as conditions uch assynonymy and hyponymy are checked for eachword pair to extend the notion of lexical cohe-sion, e.g.
using WordNet as in Barzilay and E1-hadad (1997).
Consequently, the approach maynot be suitable for on-line use with longer inputtexts.
Secondly, the use of thesauri envisagedin both Hirst and St-Onge (1997) and Barzi-lay and Elhadad (1997) does not address thequestion of topical aptness.
Thesaural relationssuch as synonymy and hyponymy are meant ocapture word similarity in order to assess lexicalcohesion among text units, and not to provide athematic haracterization f text units.
2 Con-sequently, it will not be possible to index andretrieve text units in term of topic aptness ac-cording to users' needs.
In the remaining partof the paper, we will show how these concernsof efficiency and thematic haracterization canbe addressed with specific reference to a systemperforming eneric and query-based indicative2Notice incidentally that such thematic haracteriza-tion could not be achieved using thesauri such as Word-Net since since WordNet does not provide an arrange-ment of synonym sets into classes of discourse topics (e.g.finance, sport, health).1158summaries.3 An  E f f i c ient  Method fo rComput ing  Lex ica l  Cohes ionThe method we are about to describe comprisesthree phases:?
a p reparatory  phase  where the inputtext undergoes a number of normalizationsso as to facilitate the process of assessinglexical cohesion;?
an index ing  phase  where the sharing ofelements indicative of lexical cohesion is as-sessed for each text unit, and?
a rank ing  phase  where the assessment oflexical cohesion carried out in the indexingphase is used to rank text units.3.1 P reparatory  PhaseDuring the preparatory phase, the text under-goes a number of normalizations which have thepurpose of facilitating the process of computinglexical cohesion, including:?
removal of formatting commands?
text segmentation, i.e.
breaking the inputtext into text units?
part-of-speech tagging?
recognition of proper names?
recognition of multi-word expressions?
removal of stop words?
word tokenization, e.g.
lemmatization.3.2 Index ing  PhaseIn providing a solution for the efficiency prob-lem, our aim is to compute lexical cohesion forall text units in a text without having to pro-cess all cross-sentence word combinations for allunique and distinct pair-wise text unit permu-tations.
To achieve this objective, we indexeach text unit with reference to each word oc-curring in it and reverse-index each such wordwith reference to all other text units in whichthe word occurs, as shown in Table 3 for textunit #2#.
The sharing of words can then bemeasured by counting all occurrences of iden-tical text units linked to the words associatedwith the "head" text unit (#2# in Table 3), asshown in Table 4.
By repeating the two opera-I < company {#3#,#6#} >#2# < executive {#3#} >< look {#1#} >< partner {#i#} >Table 3: Text unit #2# and its words with point-ers to the other text units in which they occur.Table 4: Total number of lexical cohesion linkswhich text unit #2# has with all other text unitstions described above for each text unit in thetext shown in Table 1, we will obtain a table oflexical cohesion links equivalent to that shownon Table 2.According to this method, we are still pro-cessing pair-wise permutations of text units tocollect lexical cohesion links as shown in Ta-ble 4.
However, there are two important differ-ences with the original algorithm.
First, non-cohesive text units are not taken into account(e.g.
the pair #2#-#4# in the example un-der analysis); therefore, on average the numberof text unit permutations will be significantlysmaller than that processed in the original al-gorithm.
With reference to the text in Table 1,for example, we would be processing 7 text unitpermutations less which is over 41% of the num-ber of text unit permutations which need com-puting according to the original algorithm, asshown in Table 2.
Secondly, although pair-wisetext unit combinations are still processed, weavoid doing so for all cross-sentence word per-mutations.
Consequently, the complexity of thealgorithm is O(n 2 ?
m) for n text units in a textand m words in a text unit of average lengthin the text as compared to O(n 2 , m 2) for theoriginal algorithm.
3ZA further improvement yet would be to avoid count-ing lexical cohesion links per text unit as in Table 4,and just sum all text unit occurrences associated withreversed-indexed words in structures uch as those inTable 3, e.g.
the lexical cohesion score for text unit#2# would simply be 9.
This would remove the needof processing pair-wise text unit permutations for theassessment of lexical cohesion links, thus bringing thecomplexity clown to O(n * m).
Such further step, how-ever, would preempt he possibility of excluding lexicalcohesion scores for text unit pairs which are below agiven threshold.1159?
LetTRSH be the lexical cohesion thresholdTU be the current text unitLC Tu be the current lexical cohesion scoreof TU (i.e.
LC Tv is the count of tokenizedwords TU shares with some other text unit)- CLevel.
be the level of the current lexical co-hesion score calculated as the difference be-tween LC Tv and TRSH- Score be the lexical cohesion score previouslyassigned TU (if any)- Level  be the level for the lexical cohesionscore previously assigned to TU (if any)- i f  LC TU -~ 0, then  do noth ing- else~ if the scoring structure(Level,  TU, Score) exists,  then* if  Level  > CLevel ,  then  do noth ing.
else, i f  Level  = CLeve l ,  then  the newscoring structure is(Level,  TU, Score + LC  Tu )* else, i f  CLeve l  > 0, then?
i f  Level  > 0, then  the new scoringstructure is (1, TU, Score + LC  TU)?
else, i f  Level  < O, then  the new scor-ing structure is (1, TU, LC  TU).
else the new scoring structure is(CLevel ,  TU, LC  ~'u)- else* if  CLeve l  > 0, then  create the scoringstructure (1, TU, LC  Tu)* else create the scoring structure( C Level,  TU, LC  T~\] )Table 5: Method for ranking text units accord-ing to lexical cohesion scores.3.3 Ranking PhaseEach text unit is ranked with reference to thetotal number of lexical cohesion scores collected,such as those shown in Table 4.
The objectiveof such a ranking process is to assess the im-port of each score and combine all scores intoa rank for each text unit.
In performing thisassessment, provisions are made for a thresh-old which specifies the minimal number of linksrequired for text units to  be lexically cohesive,following Hoey's approach (see ?1).
The proce-dure outlined in Table 5 describes the scoringmethodology adopted.
Ranking a text unit ac-cording to this procedure involves adding thelexical cohesion scores associated with the textunit which are either?
Costant values- TRSH = 2- T U  = $2#?
Scoring text unit  #2$- Lexical cohesion with text unit  #6#* LC  TU = 1.
CLeve l  -- -1  (i.e.
LC Tu-  TRSH)* no previous scoring structure.
current scoring structure: ( -1 ,#2#,  1)- Lexical cohesion with text unit  #S#* LC  TU ~.
1* CLeve l  = -1. previous scoring structure: i - l ,  #2#, 1).
current scoring structure: ( -1 ,  #2#, 2)- Lexical cohesion with text unit  #3#* LC  Tu = 4* CLeve l  = 2. previous scoring structure: i - I ,  #2#, 2).
current scoring structure: (0, #25, 4)- Lexical cohesion with text unit  #1#* LC  TU = 3.
CLeve l  = 1. previous scoring structure: (1, #2#, 4)* final scoring structure: (1, #2#, 7)Table 6: Ranking text unit #2# for lexical cohe-sion.?
above the threshold, or?
below the threshold and of the same mag-nitude.If the threshold is 0, then there is a single leveland the final score is the sum of all scores.
Sup-pose for example, we are ranking text units #2#with reference to the scores in Table 4 with alexical cohesion threshold of 2.
In this case weapply the ranking procedure in Table 5 to eachscore in Table 4, as shown in Table 6.
Followingthis procedure for all text units in Table 1, wewill obtain the ranking in Table 7.4 Assessing Topic AptnessWhen used with a dictionary database provid-ing information about the thematic domain ofwords (e.g.
business, politics, sport), the samemethod can be slightly modified to compute lex-ical cohesion with reference to discourse topicsrather than words.
Such an application makes1160Rank Text unit Level Score1st #2# 1 72nd #3# 1 43rd #1# 1 34th #5# 0 25th #6# - I  26th #4# - - -  - 0Table 7: Ranking for all text units in the textshown on Table 1.\[\[ WORD_POS CODE EXPLANATIONcompany_npartnerdaF Finance & BusinessMI Military (the armed forces)SCG Scouting & Girl GuidesTH TheatreDA Dance & ChoreographyF Finance & BusinessMGE Marriage, Divorce,Relationships & InfidelityTG Team GamesTable 8: Fragment of dictionary database pro-viding subject domain information.it possible to detect he major topics of a docu-ment automatically and to assess how well eachtext unit represents hese topics.In our implementation, we used the "subjectdomain codes" provided in the machine read-able version of CIDE (Cambridge InternationalDictionary of English (Procter, 1995)).
Table 8provides an illustrative example of the infor-mation used.
Both the indexing and rankingphases are carried out with reference to subjectdomain codes rather than words.As shown in Table 9 for text unit #1#, the in-dexing procedure provides a record of the sub-ject domain codes occurring in each text unit;each such subject code is reverse-indexed withreference to all other text units in which thesubject code occurs.
In addition, a record ofwhich word originates which cohesion link iskept for each text unit index.
The main func-tion of keeping track of this information is toavoid counting lexical cohesion links generatedby overlapping domain codes which relate to thesame word - -  for words associated with morethan one code.
Such provision is required in or-der to avoid, or at least reduce the chances of,counting codes which are out of context, that iscodes which relate to senses of the word otherthan the intended sense.
For example, the wordpartner occurring in the first two text units ofthe text in Table 1 is associated with four dif-< F {#2#-partner,I #3#-company,#1#-partner #6#-company} >< NGE {#2#-partner} >< TG {#2#-partner} >Table 9: Text unit #1# and its subject domaincodes with pointers to the other text units inwhich they occur.#3# #6## l#-partner 1 1F Fcompany companyTable 10: Total number of lexical cohesion linksinduced by subject domain codes for text unit#I#.ferent subject codes pertaining to the domainsof Dance (DA), Finance (F), Marriage (M) andTeam Games (TG), as shown in Table 8.
How-ever, only the Finance reading is appropriate inthe given context.
If we count the cohesion linksgenerated by partner we would therefore countthree incorrect cohesion links.
By excluding allfour cohesion links, the inclusion of contextuallyinappropriate cohesion links is avoided.
Need-less to say, we will also throw away the correctcohesion link (F in this case).
However, this losscan be redressed if we also compute lexical co-hesion links generated from shared words acrosstext units as discussed in ?2, and combine theresults with the lexical cohesion ranks obtainedwith subject domain codes.The lexical cohesion links for text unit #1#will therefore be scored as shown in Table 10,where associations between link scores and rele-vant codes as well as the words generating themare maintained.
As can be observed, only theappropriate code expansion F (Finance) for thewords partner and company is taken into ac-count.
This is simply because F is the only codeshared by the two words (see Table 8).As mentioned earlier, lexical cohesion linksinduced by subject domain scores can be usedto rank text units using the procedure shown inTable 5.
Other uses include providing a topicprofile of the text and an indication of how welleach text unit represents a given topic.
For ex-ample, the code BZ (Business & Commerce) isassociated with the words:1161#2#-executive#3#-executive#3#-business#4#-market#5#-interest#2 #3#1BZbusiness1BZexecut.I 2BZ  BZexecut, execut.business1 2BZ BZexecut, execut.business#4# #5#1 1BZ BZmarket  interest1 1BZ BZmarket  interest1 1BZ BZmarket  interest1BZinterest1BZmarketTable 11: Lexical cohesion links relating to codeBZ.CODES TEXT UNIT PAIRSBZ 2 -32-42-5~43-53-23-4~54 -24-34-34-55-25-35-35-4F 1 -21-31-62-12-32-63-13-26-1~2FA 2-55-2IV 4-55-4CN 944-3Table 12: Subject domain codes and the textunits pairs they relate.?
execut ive  occurring once in text units #2#and #3#;?
bus iness  occurring once in text unit #3#;?
market  occurring once in text unit #4#, and?
i n te res t  occurring once in text unit #5#.After calculating the lexical cohesion links forall text units following the method illustratedin Tables 9-10 for text unit #1#, the links scoredfor the code BZ will be as shown in Table 11.
Byrepeating this operation for all codes for whichthere are lexical cohesion scores - -  F, FA, IVand CN for the text under analysis - -  we couldthen count all text unit pairs which each coderelates, as shown in Table 12.
The relations be-tween subject domain codes and text unit pairsin Table 12 can subsequently be turned into per-centage ratios to provide a topic/theme profileof the text as shown in Table 13.By keeping track of the links among textunits, relevant codes and their originatingwords, it is also possible to retrieve text unitson the basis of specific subject domain codesor specific words.
When retrieving on specific50%31.25%6.25%6.25%6.25%Table 13:BZ Business & CommerceF Finance & BusinessIV Investment & Stock MarketsFA Overseas Politics &International RelationsCN CommunicationsTopic profile of document in Table 1,according to the distribution of subject domaincodes across text units shown in Table 12.words, there is also the option of expanding theword into subject domain codes and using theseto retrieve text units.
The retrieved text unitscan then be ordered according to the rankingorder previously computed.5 App l i ca t ions ,  Extens ions  andEva luat ionAn implementation of this approach to lexicalcohesion has been used as the driving engine ofa summarization system developed at SHARPLaboratories of Europe.
The system is designedto handle requests for both generic and query-based indicative summaries.
The level-baseddifferentiation of text units obtained throughthe ranking procedure discussed in ?3.3, is usedto select the most salient and better connectedportion of text units in a text corresponding tothe summary ratio requested by the user.
Inaddition, the user can display a topic profile ofthe input text, as shown in Table 13 and choosewhichever code(s) s/he is interested in, specify asummary ratio and retrieve the wanted portionof the text which best represents the topic(s)selected.
Query-based summaries can also beissued by entering keywords; in this case thereis the option of expanding key-words into codesand use these to issue a summary query.The method described can also be used to de-velop a conceptal indexing component for infor-mation retrieval, following Dobrov et al (1997).Because an attempt is made to prune contex-tually inappropriate sense expansions of words,the present method may help reducing the am-biguity problem.Possible improvements of this approach canbe implemented taking into account additionalways of assessing lexical cohesion such as:?
the presence of synonyms or hyponymsacross text units (Hoey, 1991; Hirst and St-Onge, 1997; Barzilay and Elhadad 1997);1162?
the presence of lexical cohesion establishedwith reference to lexical databases offer-ing a semantic lassification of words otherthan synonyms, hyponyms and subject do-main codes;?
the presence of near-synonymous wordsacross text units established by using amethod for estimating the degree of seman-tic similarity between word pairs such asthe one proposed by Resnik (1995);?
the presence of anaphoric links across textunits (Hoey, 1991; Boguraev & Kennedy,1997), and?
the presence of formatting commands as in-dicators of the relevance of particular typesof text fragments.To evaluate the utility of the approach tolexical cohesion developed for summarization,a testsuite was created using 41 Reuter's newsstories and related summaries (available athttp ://www.
yahoo, com/headlines/news/),by annotating each story with best summarylines.
In one evaluation experiment, summaryratio was set at 20% and generic summarieswere obtained for the 41 texts.
On average,60~0 of each summary contained best summarylines.
The ranking method used in this evalu-ation was based on combined lexical cohesionscores based on lemmas and their associatedsubject domain codes given in CIDE.
Summaryresults obtained with the Autosummarizefacility in Microsoft Word 97 were used asbaseline for comparison.
On average, only30% of each summary in Word 97 containedbest summary lines.
In future work, we hopeto corroborate these results and to extendtheir validity with reference to query-basedindicative summaries using the evaluationframework set within the context of SUMMAC(Automatic Text Summarization Conference,see ht tp  ://www.
t ips ter ,  org/).Re ferencesBarzilay, R. and M. Elhadad (1997) UsingLexical Chains for Text Summarization.In I. Mani and M. Maybury (eds) Intel-ligent Scalable Text Summarization, Pro-ceedings of a Workshop Sponsored by theAssociation for Computational Linguistics,Madrid, Spain.Boguraev, B.
&: C. Kennedy (1997) Salience-based Content Characterization of TextDocuments.
In I. Mani and M. Maybury(eds) Intelligent Scalable Text Summariza-tion, Prooceedings of a Workshop Spon-sored by the Association for ComputationalLinguistics, Madrid, Spain.Dobrov, B., N. Loukachevitch and T. Yud-ina (1997) Conceptual Indexing Using The-matic Representation f Texts.
In The 6thText Retrieval Conference (TREC-6).Fox, C. (1992) Lexical Analysis and Stoplists.In Frakes W and Baeza-Yates R (eds) Infor-mation Retrieval: Data Structures &: Algo-rithms.
Prentice Hall, Upper Saddle River,N J, USA, pp.
102-130.Hirst, G. and D. St-Onge (1997) LexicalChains as Representation f context for thedetection and correction of malapropism.In C. Fellbaum (ed) WordNet: An elec-tronic lexical database and some of its ap-plications.
MIT Press, Cambridge, Mass.Hoey, M. (1991) Patterns of Lexis in Text.OUP, Oford, UK.Miller, G., Beckwith, R., C. Fellbaum, D.Gross and K. Miller (1990) Introduc-tion to WordNet: An on-line lexicaldatabase.
International Journal of Lexi-cography, 3(4):235-312.Procter, P. (1995) Cambridge InternationalDictionary of English, CUP, London.Philip Resnik (1995) Using information con-tent to evaluate semantic similarity in ataxonomy.
In IJCAI-95.1163
