Overview of the University of Pennsylvania'sTIPSTER ProjectUniversity of PennsylvaniaBreck  Ba ldwin  Thomas  S. Mor ton  Ami t  BaggaIns t i tu te  fo r  Research  Depar tment  o f  Computer  Depar tment  o f  Computerin  Cogn i t ive  Sc ience  and  In format ion  Sc ience  Sc ienceUn ivers i ty  o f  Pennsy lvan ia  Un ivers i ty  o f  Pennsy lvan ia  Duke  Un ivers i ty{breck, tsmort on, bagga}@unagi, cis.
upenn, eduI n t roduct ionCAMP software has been used in a variety of areas,but at the end of T IPSTER it finishes as it started-as a coreference annotation system.
The corefer-ence output has been used to participate in MUC-6 and MUC-7, served as the foundation for threetypes of summarization engines and been input toa cross-document coreference system for names andevents.
This document focuses on the most success-ful of these application, a query sensitive summa-rization system and a cross-document coreferencesystem.Dynamic  Core ference-BasedSummar izat ionWe have developed a query-sensitive t xt summa-rization technology well suited for the task of deter-mining whether a document is relevant o a query.Enough of the document is displayed for the userto determine whether the document should be readin its entirety.
Evaluations indicate that summariesare classified for relevance nearly as well as full doc-uments.
This approach is based on the concept thata good summary will represent each of the topicsin the query and is realized by selecting sentencesfrom the document until all the phrases in the querywhich are represented in the summary are 'covered.
'A phrase in the document is considered to cover aphrase in the query if it is coreferent with it.
Thisapproach maximizes the space of entities retainedin the summary with minimal redundancy.
Thesoftware is built upon the CAMP NLP system \[3\].P rob lem StatementGiven the relative immaturity of summarizationtechnologies and their evaluation, it is worthwhileto describe our approach in detail and the prob-lems it is intended to solve.
An important aspectof our technique is that we produce sentence xtrac-tion summaries which are constructed by selectingsentences from the source document.
In addition,our summaries are focused on providing relevantinformation about a query.
We feel that the cur-rent state-of-the-art techniques are better equippedto produce high quality query-sensitive summariesthan generic summaries.
Our goal is to produce'indicative' summaries \[5\] which allow a user to de-termine whether the document is relevant o his orher query.
The summary is not intended to replacethe document or provide answers to questions di-rectly but may have this effect.Casting our technology in terms of a product,we see the application as an intermediate step be-tween viewing entire documents and the output ofan information retrieval engine.
Instead of lookingat either headlines or an entire document, the userwould look at the summaries of the documents andthen decide whether the document merited furtherreading.ApproachWe conducted a simple experiment with summariesproduced in the T IPSTER summarization dry run\[8\].
For 5 queries with 200 documents each, wetook the set of summaries produced by the 6 dry-run participants and retained only those summariesthat were true-positives, i.e., the summary wasjudged 'relevant' and the full document was judged'relevant'.
Over all the queries, at least one ofthe six systems produced a true-positive summaryfor 96.6% of the documents, although no individ-ual system performed nearly at that level.
Thismeant that some existing technology produced acorrect summary for almost every relevant docu-ment.
Hence we viewed the problem as one of bal-ancing the capabilities of our system to behave like151the amalgamated system implicit in joined output.Based on this result we are confident hat this classof summarization is tractable with current tech-nologies and this has strongly motivated our designdecisions.Upon encountering a query like "Reportingon possibility of and search for extra-terrestriallife/intelligence.
", we assume that the user has de-fined a class of actions, ideas, and/or entities thathe or she is interested in.
The job of an informa-tion retrieval engine is to find instantiations ofthoseclasses in text documents in some database.
Weview summarization as an additional step in thisprocess where we attempt o present he user withthe smallest collection of sentences in the documentthat instantiate the user specified classes and donot mislead the user about the overall content ofthe document.
By doing so, we can greatly shortenthe amount of the document hat the user mustread in order to determine whether the documentis relevant for the user's needs.Just as information retrieval algorithms approx-imate document relatedness by examining variousstring matchings between the query and the text,we approximate certain classes of coreference be-tween the query and the text by examining lin-guistic information.
These coreference r lations in-clude identity of reference and part-whole relationsfor nominal and verbal phrases3 This moves us astep closer to reasoning at a more appropriate l velof generalization, for summarization, which is stilltechnologically feasible.
Below are examples indi-cating the classes of relatedness that we are tryingto capture.The  ident i ty  re la t ion  between the  queryand the documentNoun phrase coreference is the best understoodclass of relations that we compute.
For example,there is coreference between 'Federal EmergencyManagement Agency' in the query and the acronym'FEMA'  in the document below:Query: What is the main function of the Fed-era l  Emergency  Management  Agencyand the funding level provided to meet emer-gencies?Document: .
.
.
FEMA agrees that "fine-tuning" is needed to the 1974 act establishinga coordinated federal program to prepare forlit is not clear whether more sophisticated anno-tations are appropriate for information retrieval, andperhaps more to the point, it is not clear that there axesufficient resources to process 2 GB collections of data.152and respond to hurricanes, tornadoes, stormsand floods .
.
.
.Since these noun phrases refer to the same entity inthe world, sentences that mention the organizationwould be particularly valuable in a summary.
Thisclass of coreference can include people, companiesand objects such as automobiles or aluminum sid-ing.
It need not be restricted to proper nouns asit is possible to refer to an entity using commonnouns, i.e.
'the agency' and pronouns.Identity also holds between events mentioned inthe query and document.
Sometimes the eventthat a query describes is the best indicator of whatdocument should be retrieved, and correspondinglywhat sentences are appropriate for a summary.Consider the following:Query: A relevant document will provide newtheories about the  1960's assass inat ion  ofPresident Kennedy.Document: .
.
.
The  House AssassinationsCommittee concluded in 1978 that Kennedywas "probably" assass inated  as the result of aconspiracy involving a second gunman, a find-ing that broke from the Warren Commission'sbelief that Lee Harvey Oswald acted alone inDallas on Nov. 22, 1963 .
.
.
.The noun phrase 'the 1960's assassination' refersto an event, which is the same as the one referredto in the document with the verb 'assassinated'.Note also that there is coreference between 'Presi-dent Kennedy' and 'Kennedy' in the document.The part-whole re la t ion  between thequery  and the documentIn addition to the identity relation, phrases in atext which refer to parts of an entity or conceptmentioned in the query will likely provide usefulinformation, and therefore should be included in asummary.
Finding these relations in in general isbeyond the scope of this paper, however, our ap-proximation of a subclass of these relations provedhelpful for a number of queries.A strong example of the part-whole relation oc-curs when a country is mentioned in the query anda province or city within that country is mentionedin the document.
For example:Query: Document will discuss efforts by theblack majority in South  Af r ica  to over-th row domination by the white minority gov-ernment.Document: About 90 soldiers have beenarrested and face possible death sentencesstemming from a coup attempt in Bo-phuthatswana,  .
.
.
Rebel soldiers s taged thetakeover bid Wednesday, detaining homelandPresident Lucas Mangope .
.
.
.Bophuthatswana is inside South Africa, and sen-tences that mention it are clearly good candidatesfor inclusion in a summary.We also consider part-whole relations betweenevents as in the relation between 'overthrow' and'staged' and 'detained'.
Those events are sub-partsof overthrow events, and as such, sentences thatcontain sub-parts of the events are reasonable can-didates for inclusion in summaries.ImplementationThe summarization technique was developed withinthe CAMP NLP framework.
This system providesan integrated environment in which to access manylevels of linguistic information as well as worldknowledge.
Its main components include: namedentity recognition, tokenization, sentence detec-tion, part-of-speech tagging, morphological analy-sis, parsing, argument detection, and coreferenceresolution.
Many of the techniques used for thesetasks perform at or near the state of the art and aredescribed in more depth in \[16, 12, 11, 9, 6, 2, 3\].The system produces coreference annotated ocu-ments which serve as the input to the summariza-tion algorithm.Re la t ing  the  query  to the  documentThe relationships discussed previously are approx-imated via a series of associations between tokensin the query, headline, and the body of the docu-ment.
Event references are captured by associatingverbs or nominalizations in the query with verbsand nominalizations in the document.Given three verbal forms vl in the query, v2 inthe document, and v3 in the set of all verbal forms,where a verbal form is the morphological root of averb or the verb root corresponding to a nominal-ization, vl is associated with v2 if at least one ofthe following criteria are met:1.
(Vl ?v2)  Ap(vl,v2)/(p(vl)p(v2)) -->52.
(vl =v2)  A (3v3 7~Vl I p(vl,v3)/p(vl)p(v3) -> 5)3.
(Vl = v2) A ((subject(vl) = subject(v2)) V(object(v1) =object(v2)))153Here p(vi) is the probability that vi occurs in a doc-ument and p(vi, vj) is the probability that vi andvj occur in the same document.
These probabili-ties are based on frequencies gathered from approx-imately 45,000 Wall Street Journal articles.
Crite-rion 1 is a measure of mutual information betweentwo verbs.
Criterion 2 is used to rule out frequentlyoccurring verbs such as "be" and "make".
Crite-rion 3 allows for verbs which are ruled out by cri-terion 2 to be associated when additional contextis available.
This is important since some queriesonly contain verbal forms which are ruled out bycriterion 2.Relationships between proper nouns are made onthe basis of string matches, acronym matching, anddictionary lookup.
Acronyms are determined eitherthrough a table lookup or an appositive construc-tion occurring in the document which designatesthe acronym for a specific proper noun.
A propernoun in the query is considered associated witha proper noun in the document if it matches thestring or acronym of the proper noun in the docu-ment or it appears in the definition of the propernoun in the document.
A reverse dictionary lookupoften allows cities to be associated with the countrythey are in.A token in the query which is a lowercase noun oradjective is associated with any token in the doc-ument which matches its morphological root andpart of speech.Tokens which occur in the headline are associ-ated with tokens in the document body using thesame criteria as the query, with the exclusion ofthe dictionary lookup.
The dictionary lookup wasexcluded because the headline will likely use thesame lexicalization of a proper noun as that usedin a document.
This is less likely to be the casewith the query.Select ing  a sentenceThe associations discussed in the previous sectionare used to rank and select sentences from the doc-ument.
Every token in the document which is asso-ciated with the same token in the query or headlineis considered to be in the same coreference chain.
Asentence which contains any token in a given coref-erence chain is said to cover that chain.The following scores are computed for each sen-tence in the document:1.
The number of coreference chains from the querywhich are covered by the sentence and haven'tbeen covered by a previously selected sentence.2.
The number of noun coreference chains from thequery which are covered by the sentence and thenumber of verbal terms in the sentence which arechained to the query.3.
The number of coreference chains from the head-line which are covered by the sentence andhaven't been covered by a previously selectedsentence.4.
The number of noun coreference chains from theheadline which are covered by the sentence andthe number of verbal terms in the sentence whichare chained to the headline.5.
The number of coreference chains which are cov-ered by the sentence and haven't been covered bya previously selected sentence.6.
The number of noun coreference chains which arecovered by the sentence.7.
The index of the sentence in the document; sen-tences are sequentially numbered.The sentences are sorted based on the abovescores, where the ith scoring criteria is only consid-ered in case of a tie for all criteria less than i. Scores1-6 are ranked in descending order while score 7 isranked in ascending order.
The top-ranked sen-tence is selected, and scores 1, 3, and 5 are recom-puted in order to select he next sentence.
Selectionhalts when all coreference chains in the query havebeen covered and the summary contains at least 4sentences.Scores 1 and 2 are used to select sentences whichare related to the query.
Scores 3 and 4 are mo-tivated by documents which have 1 or 2 sentenceswhich appear elated to the query but if presentedalone would give a false impression of the true con-tent of the document.
Thus sentences related to theheadline are presented to provide additional back-ground.
Consider the following example:Query: What evidence is there of paramilitaryactivity in the U.S.?Summary: .
.
.
Last month the extremists usedrocket-propelled grenades for the first time inthree attacks on police and paramilitary units.This sentence was selected because it contains to-kens which are in coreference chains with tokensin the query; however, alone it is potentially mis-leading because the place of the attack is not men-tioned.
This ambiguity is resolved when the follow-ing sentence is selected because it is well associatedwith the headline.154Summary: .
.
.S ikh militants may have ac-quired one or two U.S.-made Stinger anti-aircraft missiles and hidden them inside theGolden Temple, the Sikh faith's holiest shrine,Punjab police officials said Saturday ....This provides enough background information forthe reader to realize that the para-mil itary activityis not taking place in the U.S. and thus that thedocument is irrelevant o the query.Likewise, scores 5 and 6 act similarly to 3 and4 for documents which do not contain a headline.We found this particularly important for advertise-ments which often don't state a product or com-pany name in the beginning of the document, butwill repeat hese names numerous times throughoutthe document.Generat ing  the  summaryOnce sentences have been selected, they are pre-sented in the order they occurred in the document.Pronouns which do not have a referent in the pre-vious sentence of the summary are filled with amore descriptive string whenever a referent can bedetermined.
If space is of concern, prepositionalphrases attached to nouns (which are not nominal-izations), appositives, conjoined noun phrases andrelative clauses are removed, provided they containno tokens associated with the query or the head-line.
Since determining pronoun referents and theselection of clauses for removal are subject to er-rors, filled pronouns are placed in square bracketsand removed clauses are replaced with an ellipsisto indicate to the reader that the original text hasbeen modified.Example  summaryAn example summary which demonstrates many ofthe features of our system appears below.
It hasbeen constrained to be approximately 10% of theoriginal document length, so it is not representa-tive of the summaries used in the evaluation, butit contains examples of the of both pronoun fillingand clause deletion.The last sentence in the summary was selectedfirst because the tokens "death", "sentence", "kill",and "term" were associated with the nominaliza-tion "punishment".
The stranded pronoun "it" hasalso been filled.
Sentence 2 was selected next be-cause of the match-up between the verb "is" andthe object "deterrent" in the document and thequery.
Finally, the first sentence was chosen be-cause there is another mention of the prison name"Marion" in the document.
This summary differsfrom the one generated when the 10% length con-straint is not imposed, because some higher rankedsentences were passed over since their inclusionwould have exceeded the length restriction.Query: Is there data available to suggest hatcapital punishment is a deterrent to crime?Summary: "Marion is basically the end of theline," Bogdan said....
There is no deterrent ... to keep them fromdoing this again.Additionally, \[the pending Senate bill\] wouldcreate five new death penalty offenses: mur-der by a federal inmate serving a life sentence;drug kingpins in a continuing criminal enter-prise even if no murders occur; drug kingpinswho try to kill to obstruct justice; drug felonswho unintentionally kill with aggravated reck-lessness; and people who kill with a firearmduring a violent ... crime.EvaluationIn order to evaluate our summarization algorithm,we selected 10 unseen queries from the Text RE-trieval Conference (TREC) document collection.Summaries were generated for 200 documents, 20per query, and assessors 2 were asked to make rele-vance judgments based on the summaries.
A doc-ument was considered relevant if it contained theinformation requested in the query or if the as-sessor believed that the full document would likelycontain this information.
The relevance judgmentswere then compared to those made by the TRECassessors using the full document.
This comparisonplaces a summary in one of the following categories:?
a = judged relevant, full document is relevant?
b = judged relevant, full document is irrelevant?
c = judged irrelevant, full document is relevant?
d = judged irrelevant, full document is irrelevantPrecision, recall, and accuracy are then computedas follows:precision = a/ (a+b)recall = a/ (a+c)accuracy = (a+d) / (a+b+c+d)Compression is computed over the number ofnon-whitespace haracters in the summary and theoriginal document.
Here compression is defined as2Each author served as an assessor making judg-ments for 100 documents across 10 queries.155the percentage of the document hat was not in-cluded in the summary:compression = ( length,~ .
.
.
.
.
.
t - length  .
.
.
.
.
.
~ )lengthdoeurnentThe results from our experiment are shown in thefollowing table:Precision 82.8% 101/(101+21)Recall 77.7% 101/(101+29)Compression 82.8% (704686-121272)/704686Accuracy 75.0% (101+49)/200A second evaluation on 910 documents was per-formed for \[5\].
These results superficially appearsignificantly worse than those from the initial eval-uation however a more careful analysis (provided inthe discussion section) shows that they are in factsimilar to the results of the previous evaluation.Precision 80.3% 322/(322+79)Recall 57.6% 322/(322+237)Compression 83.0%Accuracy 65.3% (322+272)/910DiscussionWe view the results of the first evaluation aspromising in that they compare favorably withinter-assessor consistency using the entire docu-ment.
\[15\] reports unanimous relevance judgmentsby three assessors for 71.7% of the documents.
In-terpolating this figure to two assessors yields an80.1% agreement figure.
Using summaries whichon average are only 17.2% of the original docu-ment, our assessors matched the TREC assessorsfor 75.0% of the documents.The second evaluation yielded a much lower re-call figure while precision remained comparable.This, however, is also the case when the same asses-sors judgments on the full documents are comparedto those of the TREC assessors.
These results areas follows:Precision 83.5% 167/(167+33)Recall 63.5% 167/(167+96)Compression 100.0%Accuracy 69.3% (167+124)/420We view these results as favorable as well since ouraccuracy is 65.3% using 17.0% of the document onaverage compared to 69.3% accuracy using the en-tire document.
The discrepancy between the twoevaluations appears to be based on the assessors inthe second evaluation using a stricter criteria forrelevance than that used by the previous evalua-tion's assessors or the TREC assessors.It was noted after the first evaluation that dif-ferent criteria for relevance accounted for some ofthe disagreement between our assessors and theTREC assessors.
Many documents considered rele-vant were marked as irrelevant due to different no-tions of relevance and not because the summaryfailed to provide material on which to base a correctdecision.
These difficulties only hinder the evalua-tion of a summary system and not its use in an ap-plication, since a user will have a clear idea of hisor her intentions when determining a document'srelevance.As we mentioned previously, our approach hasbeen to balance methods of relating the query tosentences in the document.
The nearly 100% recallof the dry-run summaries encouraged us, and weeven used the output of those summaries to pro-vide a test-bed for evaluating our summaries.
Al-though we never actively sought o emulate aspectsof other systems directly, our final algorithm doesshare some basic ideas and approaches from thosesystems.
Some of the similarities are listed below:In \[4\], they eliminate redundant information fromsummaries by classifying sentences according toMaximal Marginal Relevance (MMR).
MMR rankstext chunks according to their dissimilarity to oneanother.
Summaries can then be produced withsentences that are maximally dissimilar, therebyincreasing the likelihood that distinguishing infor-mation will be in the summary.
One can view ourcoverage requirement for terms in the query as anattempt o pick dissimilar sentences from the doc-ument.
Instead of MMR, we use the fact that asentence which does not contain redundantly re-ferring phrases to the query is more highly rankedthan a sentence that does.Our individual sentence scoring algorithm sharessome properties with \[14\].
Their approach includesscores for anaphoric density, string equivalence withthe title or headline of a document, and positionof the sentence in the document.
However, we donot take advantage of overt cues for summary sen-tences, such as 'in summary'  or 'in conclusion', nordo we use temporal information in generating asummary.Like many systems, we do a form of word ex-pansion in attempting to relate the query to thedocument.
However, the fact that we restrict ex-pansion to proper nouns and verbs and their nom-inalizations is notable.
We found this limited setof expansions restricts the relations between thetext and the query well and also fits within theframework of part-whole relations in coreference.We did not consider part-whole relations for com-mon nouns, because in practice we have not had156very good results limiting over-generation i  thatdomain.In the next section we discuss a novel technologyfor cross document coreference.
Like the summa-rization system just discussed, it takes within doc-ument coreference annotated text, produces sum-maries in a very similar form to the above, andindividuates entities based on the similarity of thesummaries produced.Cross -document  Core ferenceCross-document coreference occurs when the sameperson, place, event, or concept is discussed in morethan one text source.
Computer ecognition of thisphenomenon is important because it helps break"the document boundary" by allowing a user toexamine information about a particular entity frommultiple text sources at the same time.
In partic-ular, resolving cross-document coreferences allowsa user to identify trends and dependencies acrossdocuments.
Cross-document coreference can alsobe used as the central tool for producing summariesfrom multiple documents, and for information fu-sion, both of which have been identified as advancedareas of research by the T IPSTER Phase I I I  pro-gram.
Cross-document coreference was also iden-tified as one of the potential tasks for the SixthMessage Understanding Conference (MUC-6) butwas not included as a formal task because it wasconsidered too ambitious \[10\].In this paper we describe a highly success-ful cross-document coreference r solution algorithmwhich uses the Vector Space Model to resolve am-biguities between people having the same name.
Inaddition, we also describe a scoring algorithm forevaluating the cross-document coreference chainsproduced by our system and we compare our algo-rithm to the scoring algorithm used in the MUC-6(within document) coreference task.Cross -Document  Core ference :  TheProb lemCross-document coreference is a distinct technol-ogy from Named Entity recognizers like IsoQuest'sNetOwl and IBM's Textract because it attemptsto determine whether name matches are actuallythe same individual (not all John Smiths are thesame).
Neither NetOwl or Textract have mecha-nisms which try to keep same-named individualsdistinct if they are different people.Cross-document coreference also differs in sub-stantial ways from within-document coreference.Within a document here is a certain amount ofconsistency which cannot be expected across docu-ments.
In addition, the problems encountered dur-ing within document coreference are compoundedwhen looking for coreferences across documents be-cause the underlying principles of linguistics anddiscourse context no longer apply across docu-ments.
Because the underlying assumptions incross-document coreference are so distinct, they re-quire novel approaches.Arch i tec ture  and  the  Methodo logyFigure 1 shows the architecture of the cross-document system developed.
The system is builtupon the University of Pennsylvania's within doc-ument coreference system, CAMP, which partici-pated in the Seventh Message Understanding Con-ference (MUC-7) within document coreference task.Our system takes as input the coreference pro-cessed ocuments output by CAMP.
It then passesthese documents through the SentenceExtractormodule which extracts, for each document, all thesentences relevant o a particular entity of inter-est.
The VSM-Disambiguate module then uses avector space model algorithm to compute similari-ties between the sentences extracted for each pairof documents.Details about each of the main steps of the cross-document coreference algorithm are given below.?
First, for each article, CAMP is run on the ar-ticle.
It produces coreference chains for all theentities mentioned in the article.
For example,consider the two extracts in Figures 2 and 4.
Thecoreference hains output by CAMP for the twoextracts are shown in Figures 3 and 5.?
Next, for the coreference hain of interest withineach article (for example, the coreference chainthat contains "John Perry"), the Sentence Ex-tractor module extracts all the sentences thatcontain the noun phrases which form the coref-erence chain.
In other words, the SentenceEx-tractor module produces a "summary" of the ar-ticle with respect to the entity of interest.
Thesesummaries are a special case of the query sen-sitive techniques being developed at Penn usingCAMP.
Therefore, for doc.36 (Figure 2), sinceat least one of the three noun phrases ("JohnPerry," "he," and "Perry") in the coreferencechain of interest appears in each of the three sen-tences in the extract, the summary produced bySentenceExtractor is the extract itself.
On theother hand, the summary produced by Sentence-Extractor for the coreference hain of interest inJohn Perry, of Weston Golf Club, an-nounced his resignation yesterday.
He was thePresident of the Massachusetts Golf Associa-tion.
During his two years in o/rice, Perryguided the MGA into a closer relationshipwith the Women's Golf Association of Mas-sachusetts.Figure 2: Extract from doc.36) I I I?
,IFigure 3: Coreference Chains for doc.36doc.38 is only the first sentence of the extract be-cause the only element of the coreference chainappears in this sentence.For each article, the VSM-Disambiguate mod-ule uses the summary extracted by the Sentence-Extractor and computes its similarity with thesummaries extracted from each of the other ar-ticles.
Summaries having similarity above a cer-tain threshold are considered to be regarding thesame entity.Un ivers i ty  o f  Pennsy lvan ia ' s  CAMPSystemThe University of Pennsylvania's CAMP systemresolves within document coreferences for severaldifferent classes including pronouns, and propernames \[7\].
It ranked among the top systems in thecoreference task during the MUC-6 and the MUC-7evaluations.The coreference hains output by CAMP enableus to gather all the information about the entity ofinterest in an article.
This information about theentity is gathered by the SentenceExtractor moduleand is used by the VSM-Disambiguate module fordisambiguation purposes.
Consider the extract fordoc.36 shown in Figure 2.
We are able to includethe fact that the John Perry mentioned in this ar-ticle was the president of the Massachusetts GolfAssociation only because CAMP recognized thatthe "he" in the second sentence is coreferent with"John Perry" in the first.
And it is this fact whichactually helps VSM-Disambiguate decide that the157Coreference Chains for doc.01I ~11~~ pUe~ ~.rg: httYc~fr eP~el~: ~Yc sa ~:~ :Sm Core f~nc~h~for doc.02 --'~'~'Cross-Document Coreference ChainsIiVSM- \] Disambiguate'01 {summary.nn I ~SentenceExtractorFigure 1: Architecture of the Cross-Document Coreference SystemOliver "Biff" Kelly of Weymouth succeedsJohn Perry as president of the MassachusettsGolf Association.
"We will have continuedgrowth in the future," said Kelly, who willserve for two years.
"There's been a lot ofchanges and there will be continued changesas we head into the year 2000.
"Figure 4: Extract from doc.38I I ?'
,IIIFigure 5: Coreference Chains for doc.38two John Perrys in doc.36 and doc.38 are the sameperson.The Vector Space ModelThe vector space model used for disambiguatingentities across documents is the standard vectorspace model used widely in information retrieval\[13\].
In this model, each summary extracted by158the SentenceExtractor module is stored as a vectorof terms.
The terms in the vector are in their mor-phological root form and are filtered for stop-words(words that have no information content like a, the,of, an, .
.
.
).
If $1 and $2 are the vectors for the twosummaries extracted from documents D1 and D2,then their similarity is computed as:Sire(S1, S2) = ~ w~ x w~jcommon terms tjwhere tj is a term present in both $1 and $2, Wljis the weight of the term tj in S~ and w2j is theweight of tj in $2.The weight of a term tj in the vector Si for asummary is given by:t f ?
log2 q_ 2 wij = x/s~ l + si2 .
.
.+s i rwhere tff is the frequency of the term tj in the sum-mary, N is the total number of documents in thecollection being examined, and df is the number ofdocuments in the collection that the term tj occurs2 2 is the cosine normaliza- in.
x/s~x + Sis "q-... + Sintion factor and is equal to the Euclidean length ofthe vector Si.The VSM-Disambiguate module, for each sum-mary Si, computes the similarity of that summarywith each of the other summaries.
If the similar-ity computed is above a pre-defined threshold, thenthe entity of interest in the two summaries are con-sidered to be coreferent.Exper imentsThe cross-document coreference system was testedon a highly ambiguous test set which consisted of197 articles from 1996 and 1997 editions of theNew York Times.
The sole criteria for includingan article in the test set was the presence or theabsence of a string in the article which matchedthe "/ John.
*?Smith/" regular expression.
In otherwords, all of the articles either contained the nameJohn Smith or contained some variation with a mid-dle initial/name.
The system did not use any NewYork Times data for training purposes.
The an-swer keys regarding the cross-document chains weremanually created, but the scoring was completelyautomated.Analysis of the DataThere were 35 different John Smiths mentioned inthe articles.
Of these, 24 of them only had one ar-ticle which mentioned them.
The other 173 articleswere regarding the 11 remaining John Smiths.
Thebackground of these John Smiths , and the num-ber of articles pertaining to each, varied greatly.Descriptions of a few of the John Smiths are:Chairman and CEO of General Motors, assistanttrack coach at UCLA, the legendary explorer, andthe main character in Disney's Pocahontas, formerpresident of the Labor Party of Britain.Resu l tsFigure 6 shows the precision, recall, and F-Measure(with equal weights for both precision and recall)using the B-CUBED scoring algorithm.
The Vec-tor Space Model in this case constructed the spaceof terms only from the summaries extracted bySentenceExtractor.
In comparison, Figure 7 showsthe results (using the B-CUBED scoring algorithm)when the vector space model constructed the spaceof terms from the articles input to the system (itstill used the summaries when computing the simi-larity).
The importance of using CAMP to extractsummaries i  verified by comparing the highest F-Measures achieved by the system for the two cases.The highest F-Measure for the former case is 84.6%while the highest F-Measure for the latter case is78.0%.
In comparison, for this task, named-entity159100908O70g so~ 403020100Precision/Recall vs Threshold.
"'~,.
O ru AIg: Precision\~  OurAIg: Recall -~-.
~, O rAu Ig.
'F-Measure -m--.m \ ' , ,.
/ ~,~t ,: i~1%.
~ '~"13"  B"D"ET"  B"  "E}" G"  "E~ ""B ' "  B " '~"E~""  t3 'ElI I I I I I I I I0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9ThresholdFigure 6: Precision, Recall, and F-Measure Usingthe B-CUBED Algorithm With Training On theSummariesPrecision/Recall vs Threshold100 .
.90 L "',.. # Our Alg: Precision - -~F ~'.
/ Ou(AIg: Recall -~--80 t ~ u  Ig."
F-Measure - B - - , :  ".50 I-/'~/ '~'~ - ""EL "B"" '{:~.
B .
.
.
.
.
.
.
.30 ~7 / --"~'"+'-+--+--+--+---i---~--~....- -+-  - .k-  -+_  _+,..~.1.
~ +lO00 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0,9ThresholdFigure 7: Precision, Recall, and F-Measure Usingthe B-CUBED Algorithm With Training On EntireArticlestools like NetOwl and Textract would mark all theJohn Smiths the same.
Their performance usingour scoring algorithm is 23% precision, and 100%recall.Figures 8 and 9 show the precision, recall, andF-Measure calculated using the MUC scoring al-gorithm.
Also, the baseline case when all theJohn Smiths are considered to be the same personachieves 83% precision and 100% recall.
The highinitial precision is mainly due to the fact that theMUC algorithm assumes that all errors are equal.We have also tested our system on other classesof cross-document coreference like names of compa-nies, and events.
Details about these experimentscan be found in \[1\].&==2~DI1.10090 {80 '7O60504030201000Precision/Recall vs Threshold,,j.
~gK, ,  o ; : ; : ; : ; : ; : ; : ; :~ , .~- -~.
MUC AIg: Precision --e--, .
MUC AIg: Recall --e--.'~/'.~.
MUC AIg: F-Measure-B- -', "B ' .B .
.
B .
.B"  ~W.
"O- -EF -  O - -O .
B - - ID - .B .
.
B .~K "13I I I I I I I I I0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9ThresholdFigure 8: Precision, Recall, and F-Measure Usingthe MUC Algorithm With Training On the Sum-mariesm EEP.Precision/Recall vs Threshold100 ~"'.0- ' o 9 o 9 : ; : ; : ; : ; : ; :90 - & ' "~ '~ MUC AIg: Precision .
-e -'r-/e- -+,~-~ MUC AIg: Recall -+---80'  ', ',, MUC AIg: F-Measure -m--.
~ \[\]'-B70 \ "13.60 -.+..~.
~'~'B- -B .~50 "'~'" "~3"'B"B'G"O40 "'+-'+"+--+--~_ _~__+..+3020100 I I I I I I I F i0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9ThresholdFigure 9: Precision, Recall, and F-Measure Usingthe MUC Algorithm With Training On Entire Ar-ticlesConc lus ionsThe T IPSTER phase III program has allowed us toexplore some of the potential application areas ofcoreference annotation.
We have reported on ourstrongest results, a summarization system and across-document coreference system for names.The query-sensitive t xt summarization systemis nearly as effective as full text documents fordetermining whether a document is relevant tothe query.
The system uses a limited class ofcoreference-based r lations between the query andthe document o select sentences which representinstantiations of entities, events, or concepts artic-ulated in the query.As a novel research problem, cross document160coreference provides an different perspective fromrelated phenomenon like named entity recognitionand within document coreference.
Our systemtakes summaries about an entity of interest anduses various information retrieval metrics to rankthe similarity of the summaries.
We found it quitechallenging to arrive at a scoring metric that sat-isfied our intuitions about what was good systemoutput v.s.
bad, but we have developed a scoringalgorithm that is an improvement for this class ofdata over other within document coreference scor-ing algorithms.
Our results are quite encouragingwith potential performance being as good as 84.6%(F-Measure).Future  Goa lsCentral to the future of this research program isthe CAMP software system.
We are continually re-fining and extending the software to better capturethe coreference relations that we need and to re-duce genre dependent aspects of the system.
Weare currently exploring visualization interfaces toboth within and cross-document coreference whichwe believe will provide strong motivation for im-portance of corefence annotation of free text data-bases.
In addition, we are interested in generatingcross-document summaries based on similar tech-niques to our within document summarization sys-tem.References\[1\] Bagga Amit and Breck Baldwin.
How muchprocessing is required for cross-documentcoreference?
In The First International Con-ference on Language Resources and Evaluationon Linguistics Coreference, Granada, Spain,1998.\[2\] Breck Baldwin.
CogNIAC: High precisioncoreference with limited knowledge and lin-guistic resources.
In Proceedings of the A CLWorkshop on Operational Factors in Practical,Robust Anaphora resolution /or UnrestrictedTexts, pages 38-45, Madrid, Spain, June 1997.\[3\] Breck Baldwin, Christine Doran, Jeffrey C.Reynar, Michael Niv, B. Srinivas, and MarkWasson.
EAGLE: An extensible architecturefor general inguistic engineering.
In Proceed-ings of RIAO-97, Montreal, 1997.\[4\] Michael Bett and Jade Go lds te in .
Auto-mated query-relevant document summariza-tion.
In Proceedings of Tipster Text Phase III12-Month Workshop, 1997.\[5\] Michael Chrzanowski, Therese Firmin,Lynette Hirschman, David House, In-derjeet Mani, Leo Obrst, Sara Shel-ton, Beth Sundheim, and Sandra Wag-ner.
(SUMMAC) call for participation.http://www.tipster.org/summcall.htm, Jan-uary 1998.\[6\] Michael John Collins.
A New Statistical ParserBased on Bigram Lexical Dependencies.
InProceedings of the 3~th Annual Meeting of theACL, 1996.\[7\] Baldwin Breck et al University of pennsylva-nia: Description of the university of pennsyl-vania system used for muc-6.
In Proceedings ofthe Sixth Message Understanding Conference(MUC-6), pages 177-191, 1995.\[8\] Therese Hand.
Tipster summarization evalu-ation task:dry-run evaluation results.
In Pro-ceedings of Tipster Text Phase III 12-MonthWorkshop, 1997.\[9\] Daniel Karp, Yves Schabes, Martin Zaidel,and Dania Egedi.
A freely available wide cov-erage morphological nalyzer for english.
InProceedings of the 15th International Confer-ence on Computational Linguistics, 1994.\[10\] Ralph Grishman.
Whither Written LanguageEvaluation?
In Proceedings of the Human Lan-guage Technology Workshop, 1994.\[11\] Adwait Ratnaparkhi.
A Maximum EntropyPart of Speech Tagger.
In Eric BriU and Ken-neth Church, editors, Conference on EmpiricalMethods in Natural Language Processing, Uni-versity of Pennsylvania, May 17-18 1996.\[12\] Jeffrey C. Reynar and Adwait Ratnaparkhi.
Amaximum entropy approach to identifying sen-tence boundaries.
In Proceedings of the FifthConference on Applied Natural Language Pro-cessing, pages 16-19, Washington, D.C., April1997.\[13\] Gerard Salton.
Automatic Text Processing:The Transformation, Analysis, and Retrievalof Information by Computer.
Addison-Wesley,1989.\[14\] Tomek Strzalkowski, Fang Lin, Jin Wang,Langdon White, and Bowden Wise.
Naturallanguage information retrieval and summariza-tion.
In Proceedings of Tipster Text Phase III12-Month Workshop, 1997.161\[15\] Ellen M. Voorhees and Donna Harman.Overview of the fifth Text REtrieval Confer-ence (TREC-5).
In Proceedings of the FifthText REtrieval Conference (TREC-5), pages1-28.
NIST 500-238, 1997.\[16\] Nina Wacholder, Yael Ravin, and MisookChoi.
Disambiguation f proper names in text.In Proceedings of the Fifth Conference on Ap-plied Natural Language Processing, May 1997.
