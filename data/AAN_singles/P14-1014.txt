Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 144?154,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsTagging The Web: Building A Robust Web Tagger with Neural NetworkJi Ma?, Yue Zhang?and Jingbo Zhu?
?Northeastern University, China?Singapore University of Technology and Designmajineu@gmail.comyue zhang@sutd.edu.sgzhujingbo@mail.neu.edu.cnAbstractIn this paper, we address the problem ofweb-domain POS tagging using a two-phase approach.
The first phase learns rep-resentations that capture regularities un-derlying web text.
The representation isintegrated as features into a neural networkthat serves as a scorer for an easy-first POStagger.
Parameters of the neural networkare trained using guided learning in thesecond phase.
Experiment on the SANCL2012 shared task show that our approachachieves 93.15% average tagging accu-racy, which is the best accuracy reportedso far on this data set, higher than thosegiven by ensembled syntactic parsers.1 IntroductionAnalysing and extracting useful information fromthe web has become an increasingly important re-search direction for the NLP community, wheremany tasks require part-of-speech (POS) tag-ging as a fundamental preprocessing step.
How-ever, state-of-the-art POS taggers in the literature(Collins, 2002; Shen et al, 2007) are mainly opti-mized on the the Penn Treebank (PTB), and whenshifted to web data, tagging accuracies drop sig-nificantly (Petrov and McDonald, 2012).The problem we face here can be consideredas a special case of domain adaptation, where wehave access to labelled data on the source domain(PTB) and unlabelled data on the target domain(web data).
Exploiting useful information fromthe web data can be the key to improving webdomain tagging.
Towards this end, we adopt theidea of learning representations which has beendemonstrated useful in capturing hidden regular-ities underlying the raw input data (web text, inour case).Our approach consists of two phrases.
In thepre-training phase, we learn an encoder that con-verts the web text into an intermediate represen-tation, which acts as useful features for predictiontasks.
We integrate the learned encoder with a setof well-established features for POS tagging (Rat-naparkhi, 1996; Collins, 2002) in a single neuralnetwork, which is applied as a scorer to an easy-first POS tagger.
We choose the easy-first taggingapproach since it has been demonstrated to givehigher accuracies than the standard left-to-rightPOS tagger (Shen et al, 2007; Ma et al, 2013).In the fine-tuning phase, the parameters of thenetwork are optimized on a set of labelled train-ing data using guided learning.
The learned modelpreserves the property of preferring to tag easywords first.
To our knowledge, we are the first toinvestigate guided learning for neural networks.The idea of learning representations from un-labelled data and then fine-tuning a model withsuch representations according to some supervisedcriterion has been studied before (Turian et al,2010; Collobert et al, 2011; Glorot et al, 2011).While most previous work focus on in-domain se-quential labelling or cross-domain classificationtasks, we are the first to learn representations forweb-domain structured prediction.
Previous worktreats the learned representations either as modelparameters that are further optimized in super-vised fine-tuning (Collobert et al, 2011) or asfixed features that are kept unchanged (Turian etal., 2010; Glorot et al, 2011).
In this work,we investigate both strategies and give empiricalcomparisons in the cross-domain setting.
Our re-sults suggest that while both strategies improvein-domain tagging accuracies, keeping the learnedrepresentation unchanged consistently results inbetter cross-domain accuracies.We conduct experiments on the official data setprovided by the SANCL 2012 shared task (Petrovand McDonald, 2012).
Our method achieves a93.15% average accuracy across the web-domain,which is the best result reported so far on this data144set, higher than those given by ensembled syntac-tic parsers.
Our code will be publicly available athttps://github.com/majineu/TWeb.2 Learning from Web TextUnsupervised learning is often used for trainingencoders that convert the input data to abstract rep-resentations (i.e.
encoding vectors).
Such repre-sentations capture hidden properties of the input,and can be used as features for supervised tasks(Bengio, 2009; Ranzato et al, 2007).
Among themany proposed encoders, we choose the restrictedBoltzmann machine (RBM), which has been suc-cessfully used in many tasks (Lee et al, 2009b;Hinton et al, 2006).
In this section, we give somebackground on RBMs and then show how they canbe used to learn representations of the web text.2.1 Restricted Boltzmann MachineThe RBM is a type of graphical model that con-tains two layers of binary stochastic units v ?
{0, 1}Vand h ?
{0, 1}H, corresponding to a setof visible and hidden variables, respectively.
TheRBM defines the joint probability distribution overv and h by an energy functionE(v,h) = ?c?h?
b?v ?
h?Wv, (1)which is factorized by a visible bias b ?
RV, ahidden bias c ?
RHand a weight matrix W ?RH?V.
The joint distribution P (v,h) is given byP (v,h) =1Zexp(E(v,h)), (2)where Z is the partition function.The affine form of E with respect to v and himplies that the visible variables are conditionallyindependent with each other given the hidden layerunits, and vice versa.
This yields the conditionaldistribution:P (v|h) =V?j=1P (vj|h) P (h|v) =H?i=1P (hi|v)P (vj= 1|h) = ?
(bj+W?jh) (3)P (hi= 1|v) = ?
(cj+Wi?v) (4)Here ?
denotes the sigmoid function.
Parametersof RBMs ?
= {b, c,W} can be trained efficientlyusing contrastive divergence learning (CD), see(Hinton, 2002) for detailed descriptions of CD.2.2 Encoding Web Text with RBMMost of the indicative features for POS disam-biguation can be found from the words and wordcombinations within a local context (Ratnaparkhi,1996; Collins, 2002).
Inspired by this observa-tion, we apply the RBM to learn feature repre-sentations from word n-grams.
More specifically,given the ithword wiof a sentence, we applyRBMs to model the joint distribution of the n-gram(wi?l, ?
?
?
, wi+r), where l and r denote the leftand right window, respectively.
Note that the vis-ible units of RBMs are binary.
While in our case,each visible variable corresponds to a word, whichmay take on tens-of-thousands of different values.Therefore, the RBM need to be re-factorized tomake inference tractable.We utilize the Word Representation RBM (WR-RBM) factorization proposed by Dahl et al(2012).
The basic idea is to share word representa-tions across different positions in the input n-gramwhile using position-dependent weights to distin-guish between different word orders.Let wkbe the k-th entry of lexicon L, and wkbe its one-hot representation (i.e., only the k-thcomponent of wkis 1, and all the others are 0).Let v(j)represents the j-th visible variable of theWRRBM, which is a vector of length |L|.
Thenv(j)= wkmeans that the j-th word in the n-gramis wk.
Let D ?
RD?|L|be a projection matrix,then Dwkprojects wkinto a D-dimensional realvalue vector (embedding).
For each position j,there is a weight matrix W(j)?
RH?D, whichis used to model the interaction between the hid-den layer and the word projection in position j.The visible biases are also shared across differentpositions (b(j)= b ?j) and the energy function is:E(v,h) = ?c?h?n?j=1(b?v(j)+ h?W(j)Dv(j)),(5)which yields the conditional distributions:P (v|h) =n?j=1P (v(j)|h) P (h|v) =?i=1P (hi|v)P (hi= 1|v) = ?
(ci+n?j=1W(j)i?Dv(j)) (6)P (v(j)= wk|h) =1Zexp(b?wk+ h?W(j)Dwk)(7)145Again Z is the partition function.The parameters {b, c,D,W(1), .
.
.
,W(n)}can be trained using a Metropolis-Hastings-basedCD variant and the learned word representationsalso capture certain syntactic information; seeDahl et al (2012) for more details.Note that one can stack standard RBMs on topof a WRRBM to construct a Deep Belief Network(DBN).
By adopting greedy layer-wise training(Hinton et al, 2006; Bengio et al, 2007), DBNsare capable of modelling higher order non-linearrelations between the input, and has been demon-strated to improve performance for many com-puter vision tasks (Hinton et al, 2006; Bengio etal., 2007; Lee et al, 2009a).
However, in this workwe do not observe further improvement by em-ploying DBNs.
This may partly be due to the factthat unlike computer vision tasks, the input struc-ture of POS tagging or other sequential labellingtasks is relatively simple, and a single non-linearlayer is enough to model the interactions withinthe input (Wang and Manning, 2013).3 Neural Network for POSDisambiguationWe integrate the learned WRRBM into a neuralnetwork, which serves as a scorer for POS dis-ambiguation.
The main challenge to designingthe neural network structure is: on the one hand,we hope that the model can take the advantageof information provided by the learned WRRBM,which reflects general properties of web texts, sothat the model generalizes well in the web domain;on the other hand, we also hope to improve themodel?s discriminative power by utilizing well-established POS tagging features, such as those ofRatnaparkhi (1996).Our approach is to leverage the two sources ofinformation in one neural network by combiningthem though a shared output layer, as shown inFigure 1.
Under the output layer, the networkconsists of two modules: the web-feature mod-ule, which incorporates knowledge from the pre-trained WRRBM, and the sparse-feature module,which makes use of other POS tagging features.3.1 The Web-Feature ModuleThe web-feature module, shown in the lower leftpart of Figure 1, consists of a input layer and twohidden layers.
The input for the this module is theword n-gram (wi?l, .
.
.
, wi+r), the form of whichFigure 1: The proposed neural network.
The web-feature module (lower left) and sparse-featuremodule (lower right) are combined by a sharedoutput layer (upper).is identical to the training data of the pre-trainedWRRBM.The first layer is a linear projection layer, whereeach word in the input is projected into a D-dimensional real value vector using the projectionoperation described in Section 2.2.
The output ofthis layer o1wis the concatenation of the projec-tions of wi?l, .
.
.
, wi+r:o1w=???M1wwi?l...M1wwi+r???
(8)Here M1wdenotes the parameters of the first layerof the web-feature module, which is a D ?
|L|projection matrix.The second layer is a sigmoid layer to modelnon-linear relations between the word projections:o2w= ?
(M2wo1w+ b2w) (9)Parameters of this layer include: a bias vectorb2w?
RHand a weight matrix M2w?
RH?nD.The web-feature module enables us to explorethe learned WRRBM in various ways.
First, it al-lows us to investigate knowledge from the WR-RBM incrementally.
We can choose to use onlythe word representations of the learned WRRBM.This can be achieved by initializing only the firstlayer of the web module with the projection matrixD of the learned WRRBM:M1w?
D. (10)Alternatively, we can choose to use the hiddenstates of the WRRBM, which can be treated as the146representations of the input n-gram.
This can beachieved by also initializing the parameters of thesecond layer of the web-feature module using theposition-dependent weight matrix and hidden biasof the learned WRRBM:b2w?
c (11)M2w?
(W(1), .
.
.
,W(n)) (12)Second, the web-feature module also allows usto make a comparison between whether or not tofurther adjust the pre-trained representation in thesupervised fine-tuning phase, which correspondsto the supervised learning strategies of Turian et al(2010) and Collobert et al (2011), respectively.
Toour knowledge, no investigations have been pre-sented in the literature on this issue.3.2 The Sparse-Feature ModuleThe sparse-feature module, as shown in the lowerright part of Figure 1, is designed to incorporatecommonly-used tagging features.
The input forthis module is a vector of boolean values ?
(x) =(f1(x), .
.
.
, fk(x)), where x denotes the partiallytagged input sentence and fi(x) denotes a fea-ture function, which returns 1 if the correspond-ing feature fires and 0 otherwise.
The first layer ofthis module is a linear transformation layer, whichconverts the high dimensional sparse vector into afixed-dimensional real value vector:os= Ms?
(x) + bs(13)Depending on the specific task being considered,the output of this layer can be further fed to othernon-linear layers, such as a sigmoid or hyperbolictangent layer, to model more complex relations.For POS tagging, we found that a simple linearlayer yields satisfactory accuracies.The web-feature and sparse-feature modules arecombined by a linear output layer, as shown in theupper part of Figure 1.
The value of each unit inthis layer denotes the score of the correspondingPOS tag.oo= Mo(owos)+ bo(14)In some circumstances, probability distributionover POS tags might be a more preferable formof output.
Such distribution can be easily obtainedby adding a soft-max layer on top of the outputlayer to perform a local normalization, as done byCollobert et al (2011).Algorithm 1 Easy-first POS taggingInput: x a sentence of m words w1, .
.
.
, wmOutput: tag sequence of x1: U?
[w1, .
.
.
, wm] // untagged words2: while U 6= [] do3: (w?,?t)?
arg max(w,t)?U?TS(w, t)4: w?.t?
?t5: U?
U/[w?]
// remove w?
from U6: end while7: return [w1.t, .
.
.
, wm.t]4 Easy-first POS tagging with NeuralNetworkThe neural network proposed in Section 3 is usedfor POS disambiguation by the easy-first POS tag-ger.
Parameters of the network are trained usingguided learning, where learning and search inter-act with each other.4.1 Easy-first POS taggingPseudo-code of easy-first tagging is shown in Al-gorithm 1.
Rather than tagging a sentence fromleft to right, easy-first tagging is based on a deter-ministic process, repeatedly selecting the easiestword to tag.
Here ?easiness?
is evaluated basedon a statistical model.
At each step, the algorithmadopts a scorer, the neural network in our case,to assign a score to each possible word-tag pair(w, t), and then selects the highest score one (w?,?t)to tag (i.e., tag w?
with?t).
The algorithm repeatsuntil all words are tagged.4.2 TrainingThe training algorithm repeats for several itera-tions over the training data, which is a set of sen-tences labelled with gold standard POS tags.
Ineach iteration, the procedure shown in Algorithm2 is applied to each sentence in the training set.At each step during the processing of a trainingexample, the algorithm calculates a margin lossbased on two word-tag pairs (w, t) and (w?,?t) (line4 ?
line 6).
(w, t) denotes the word-tag pair thathas the highest model score among those that areinconsistent with the gold standard, while (w?,?t)denotes the one that has the highest model scoreamong those that are consistent with the gold stan-dard.
If the loss is zero, the algorithm continues toprocess the next untagged word.
Otherwise, pa-rameters are updated using back-propagation.The standard back-propagation algorithm147(Rumelhart et al, 1988) cannot be applieddirectly.
This is because the standard loss iscalculated based on a unique input vector.
Thiscondition does not hold in our case, because w?and w may refer to different words, which meansthat the margin loss in line 6 of Algorithm 2 iscalculated based on two different input vectors,denoted by ?w??
and ?w?, respectively.We solve this problem by decomposing the mar-gin loss in line 6 into two parts:?
1 + nn(w, t), which is associated with ?w?;?
?nn(w?,?t), which is associated with ?w?
?.In this way, two separate back-propagation up-dates can be used to update the model?s parameters(line 8 ?
line 11).
For the special case where w?and w do refer to the same word w, it can be easilyverified that the two separate back-propagation up-dates equal to the standard back-propagation witha loss 1 + nn(w, t)?
nn(w,?t) on the input ?w?.The algorithm proposed here belongs to a gen-eral framework named guided learning, wheresearch and learning interact with each other.
Thealgorithm learns not only a local classifier, but alsothe inference order.
While previous work (Shen etal., 2007; Zhang and Clark, 2011; Goldberg andElhadad, 2010) apply guided learning to train alinear classifier by using variants of the percep-tron algorithm, we are the first to combine guidedlearning with a neural network, by using a marginloss and a modified back-propagation algorithm.5 Experiments5.1 SetupOur experiments are conducted on the data setprovided by the SANCL 2012 shared task, whichaims at building a single robust syntactic anal-ysis system across the web-domain.
The dataset consists of labelled data for both the source(Wall Street Journal portion of the Penn Treebank)and target (web) domains.
The web domain datacan be further classified into five sub-domains, in-cluding emails, weblogs, business reviews, newsgroups and Yahoo!Answers.
While emails andweblogs are used as the development sets, reviews,news groups and Yahoo!Answers are used as thefinal test sets.
Participants are not allowed to useweb-domain labelled data for training.
In addi-tion to labelled data, a large amount of unlabelleddata on the web domain is also provided.
StatisticsAlgorithm 2 Training over one sentenceInput: (x, t) a tagged sentence, neural net nnOutput: updated neural net nn?1: U?
[w1, .
.
.
, wm] // untagged words2: R?
[(w1, t1), .
.
.
, (wm, tm)] // reference3: while U 6= [] do4: (w, t)?
arg max(w,t)?
(U?T/R)nn(w, t)5: (w?,?t)?
arg max(w,t)?Rnn(w, t)6: loss?
max(0, 1 + nn(w, t)?
nn(w?,?t))7: if loss > 0 then8: e??
nn.BackPropErr(?w?
?,?nn(w?,?t))9: e?
nn.BackPropErr(?w?, 1+nn(w, t))10: nn.Update(?w?
?, e?
)11: nn.Update(?w?, e)12: else13: U?
U/{w?
}, R?
R/(w?,?t)14: end if15: end while16: return nnabout labelled and unlabelled data are summarizedin Table 1 and Table 2, respectively.The raw web domain data contains much noise,including spelling error, emotions and inconsis-tent capitalization.
Following some participants(Le Roux et al, 2012), we conduct simple prepro-cessing steps to the input of the development andthe test sets1?
Neutral quotes are transformed to opening orclosing quotes.?
Tokens starting with ?www.
?, ?http.?
or end-ing with ?.org?, ?.com?
are converted to a?#URL?
symbol?
Repeated punctuations such as ?!!!!?
are col-lapsed into one.?
Left brackets such as ?<?,?{?
and ?[?
areconverted to ?-LRB-?.
Similarly, right brack-ets are converted to ?-RRB-??
Upper cased words that contain more than 4letters are lowercased.?
Consecutive occurrences of one or more dig-its within a word are replaced with ?#DIG?We apply the same preprocessing steps to all theunlabelled data.
In addition, following Dahl et1The preprocessing steps make use of no POS knowledge,and does not bring any unfair advantages to the participants.148Training set Dev set Test setWSJ-Train Emails Weblogs WSJ-dev Answers Newsgroups Reviews WSJ-test#Sen 30060 2,450 1,016 1,336 1,744 1,195 1,906 1,640#Words 731,678 29,131 24,025 32,092 28,823 20,651 28,086 35,590#Types 35,933 5,478 4,747 5,889 4,370 4,924 4,797 6,685Table 1: Statistics of the labelled data.
#Sen denotes number of sentences.
#Words and #Types denotenumber of words and unique word types, respectively.Emails Weblogs Answers Newsgroups Reviews#Sen 1,194,173 524,834 27,274 1,000,000 1,965,350#Words 17,047,731 10,365,284 424,299 18,424,657 29,289,169#Types 221,576 166,515 33,325 357,090 287,575Table 2: Statistics of the raw unlabelled data.features templatesunigram H(wi), C(wi), L(wi), L(wi?1), L(wi+1), ti?2, ti?1, ti+1, ti+2bigram L(wi) L(wi?1), L(wi) L(wi+1), ti?2ti?1, ti?1ti+1, ti+1ti+2,L(wi) ti?2, L(wi) ti?1, L(wi) ti+1, L(wi) ti+2trigram L(wi) ti?2ti?1, L(wi) ti?1ti+1, L(wi) ti+1ti+2Table 3: Feature templates, where widenotes the current word.
H(w) and C(w) indicates whether wcontains hyphen and upper case letters, respectively.
L(w) denotes a lowercased w.al.
(2012) and Turian et al (2010), we also low-ercased all the unlabelled data and removed thosesentences that contain less than 90% a-z letters.The tagging performance is evaluated accord-ing to the official evaluation metrics of SANCL2012.
The tagging accuracy is defined as the per-centage of words (punctuations included) that arecorrectly tagged.
The averaged accuracies are cal-culated across the web domain data.We trained the WRRBM on web-domain dataof different sizes (number of sentences).
The datasets are generated by first concatenating all thecleaned unlabelled data, then selecting sentencesevenly across the concatenated file.For each data set, we investigate an extensive setof combinations of hyper-parameters: the n-gramwindow (l, r) in {(1, 1), (2, 1), (1, 2), (2, 2)}; thehidden layer size in {200, 300, 400}; the learningrate in {0.1, 0.01, 0.001}.
All these parameters areselected according to the averaged accuracy on thedevelopment set.5.2 BaselineWe reimplemented the greedy easy-first POS tag-ger of Ma et al (2013), which is used for all theexperiments.
While the tagger of Ma et al (2013)utilizes a linear scorer, our tagger adopts the neuralnetwork as its scorer.
The neural network of ourbaseline tagger only contains the sparse-featuremodule.
We use this baseline to examine the per-formance of a tagger trained purely on the sourcedomain.
Feature templates are shown in Table 3,which are based on those of Ratnaparkhi (1996)and Shen et al (2007).Accuracies of the baseline tagger are shown inthe upper part of Table 6.
Compared with theperformance of the official baseline (row 4 of Ta-ble 6), which is evaluated based on the output ofBerkeleyParser (Petrov et al, 2006; Petrov andKlein, 2007), our baseline tagger achieves com-parable accuracies on both the source and targetdomain data.
With data preprocessing, the aver-age accuracy boosts to about 92.02 on the test setof the target domain.
This is consistent with pre-vious work (Le Roux et al, 2011), which foundthat for noisy data such as web domain text, datacleaning is a effective and necessary step.5.3 Exploring the Learned KnowledgeAs mentioned in Section 3.1, the knowledgelearned from the WRRBM can be investigatedincrementally, using word representation, whichcorresponds to initializing only the projectionlayer of web-feature module with the projectionmatrix of the learned WRRBM, or ngram-levelrepresentation, which corresponds to initializingboth the projection and sigmoid layers of the web-feature module by the learned WRRBM.
In eachcase, there can be two different training strate-gies depending on whether the learned representa-tions are further adjusted or kept unchanged dur-ing the fine-turning phrase.
Experimental resultsunder the 4 combined settings on the developmentsets are illustrated in Figure 2, 3 and 4, where the14996.596.696.796.896.9200 400 600 800 1000AccuracyNumber of unlabelled sentences (k)WSJword-fixedword-adjustngram-fixedngram-adjustFigure 2: Tagging accuracies on the source-domain data.
?word?
and ?ngram?
denote usingword representations and n-gram representations,respectively.
?fixed?
and ?adjust?
denote that thelearned representation are kept unchanged or fur-ther adjusted in supervised learning, respectively.89.89090.290.490.690.891200 400 600 800 1000AccuracyNumber of unlabelled sentences (k)Emailword-fixedword-adjustngram-fixedngram-adjustFigure 3: Accuracies on the email domain.94.89595.295.495.695.8200 400 600 800 1000AccuracyNumber of unlabelled sentences (k)Weblogword-fixedword-adjustngram-fixedngram-adjustFigure 4: Accuracies on the weblog domain.x-axis denotes the size of the training data and y-axis denotes tagging accuracy.5.3.1 Effect of the Training StrategyFrom Figure 2 we can see that when knowl-edge from the pre-trained WRRBM is incorpo-method all non-oov oovbaseline 89.81 92.42 65.64word-adjust +0.09 ?0.05 +1.38word-fix +0.11 +0.13 +1.73ngram-adjust +0.53 +0.52 +0.53ngram-fix +0.69 +0.60 +2.30Table 4: Performance on the email domain.rated, both the training strategies (?word-fixed?vs ?word-adjusted?, ?ngram-fixed?
vs ?ngram-adjusted?)
improve accuracies on the source do-main, which is consistent with previous findings(Turian et al, 2010; Collobert et al, 2011).
Inaddition, adjusting the learned representation orkeeping them fixed does not result in too much dif-ference in tagging accuracies.On the web-domain data, shown in Figure 3 and4, we found that leaving the learned representationunchanged (?word-fixed?, ?ngram-fixed?)
yieldsconsistently higher performance gains.
This re-sult is to some degree expected.
Intuitively, unsu-pervised pre-training moves the parameters of theWRRBM towards the region where properties ofthe web domain data are properly modelled.
How-ever, since fine-tuning is conducted with respectto the source domain, adjusting the parametersof the pre-trained representation towards optimiz-ing source domain tagging accuracies would dis-rupt its ability in modelling the web domain data.Therefore, a better idea is to keep the representa-tion unchanged so that we can learn a function thatmaps the general web-text properties to its syntac-tic categories.5.3.2 Word and N-gram RepresentationFrom Figures 2, 3 and 4, we can see thatadopting the ngram-level representation consis-tently achieves better performance compared withusing word representations only (?word-fixed?vs ?ngram-fixed?, ?word-adjusted?
vs ?ngram-adjusted?).
This result illustrates that the ngram-level knowledge captures more complex interac-tions of the web text, which cannot be recoveredby using only word embeddings.
Similar resultwas reported by Dahl et al (2012), who foundthat using both the word embeddings and the hid-den units of a tri-gram WRRBM as additional fea-tures for a CRF chunker yields larger improve-ments than using word embeddings only.Finally, more detailed accuracies under the 4settings on the email domain are shown in Table4.
We can see that the improvement of using word150RBM-E RBM-W RBM-M+acc%Emails +0.73 +0.37 +0.69Weblog +0.31 +0.52 +0.54cov%Emails 95.24 92.79 93.88Weblog 90.21 97.74 94.77Table 5: Effect of unlabelled data.
?+acc?
denotesimprovement in tagging accuracy and ?cov?
de-notes the lexicon coverages.representations mainly comes from better accu-racy of out-of-vocabulary (oov) words.
By con-trast, using n-gram representations improves theperformance on both oov and non-oov.5.4 Effect of Unlabelled Domain DataIn some circumstances, we may know beforehandthat the target domain data belongs to a certainsub-domain, such as the email domain.
In suchcases, it might be desirable to train WRRBM usingdata only on that domain.
We conduct experimentsto test whether using the target domain data totrain the WRRBM yields better performance com-pared with using mixed data from all sub-domains.We trained 3 WRRBMs using the email do-main data (RBM-E), weblog domain data (RBM-W) and mixed domain data (RBM-M), respec-tively, with each data set consisting of 300k sen-tences.
Tagging performance and lexicon cover-ages of each data set on the development sets areshown in Table 5.
We can see that using the targetdomain data achieves similar improvements com-pared with using the mixed data.
However, for theemail domain, RBM-W yields much smaller im-provement compared with RBM-E, and vice versa.From the lexicon coverages, we can see that thesub-domains varies significantly.
The results sug-gest that using mixed data can achieve almost asgood performance as using the target sub-domaindata, while using mixed data yields a much morerobust tagger across all sub-domains.5.5 Final ResultsThe best result achieved by using a 4-gram WR-RBM, (wi?2, .
.
.
, wi+1), with 300 hidden unitslearned on 1,000k web domain sentences areshown in row 3 of Table 6.
Performance of thetop 2 systems of the SANCL 2012 task are alsoshown in Table 6.
Our greedy tagger achieves 93%tagging accuracy, which is significantly better thanthe baseline?s 92.02% accuracy (p < 0.05 by Mc-Nemar?s test).
Moreover, we achieve the high-est tagging accuracy reported so far on this dataset, surpassing those achieved using parser combi-nations based on self-training (Tang et al, 2012;Le Roux et al, 2012).
In addition, different fromLe Roux et al (2012), we do not use any externalresources in data cleaning.6 Related WorkLearning representations has been intensivelystudied in computer vision tasks (Bengio et al,2007; Lee et al, 2009a).
In NLP, there is alsomuch work along this line.
In particular, Col-lobert et al (2011) and Turian et al (2010) learnword embeddings to improve the performance ofin-domain POS tagging, named entity recogni-tion, chunking and semantic role labelling.
Yanget al (2013) induce bi-lingual word embeddingsfor word alignment.
Zheng et al (2013) investi-gate Chinese character embeddings for joint wordsegmentation and POS tagging.
While those ap-proaches mainly explore token-level representa-tions (word or character embeddings), using WR-RBM is able to utilize both word and n-gram rep-resentations.Titov (2011) and Glorot et al (2011) proposeto learn representations from the mixture of bothsource and target domain unlabelled data to im-prove cross-domain sentiment classification.
Titov(2011) also propose a regularizer to constrain theinter-domain variability.
In particular, their reg-ularizer aims to minimize the Kullback-Leibler(KL) distance between the marginal distributionsof the learned representations on the source andtarget domains.Their work differs from ours in that their ap-proaches learn representations from the featurevectors for sentiment classification, which mightbe of thousands of dimensions.
Such high di-mensional input gives rise to high computationalcost and it is not clear whether those approachescan be applied to large scale unlabelled data, withhundreds of millions of training examples.
Ourmethod learns representations from only word n-grams with n ranging from 3 to 5, which canbe easily applied to large scale-data.
In addition,while Titov (2011) and Glorot et al (2011) use thelearned representation to improve cross-domainclassification tasks, we are the first to apply it tocross-domain structured prediction.Blitzer et al (2006) propose to induce sharedrepresentations for domain adaptation, which isbased on the alternating structure optimization151System Answer Newsgroup Review WSJ-t Avgbaseline-raw 89.79 91.36 89.96 97.09 90.31baseline-clean 91.35 92.06 92.92 97.09 92.02best-clean 92.37 93.59 93.62 97.44 93.15baseline-offical 90.20 91.24 89.33 97.08 90.26Le Roux et al(2011) 91.79 93.81 93.11 97.29 92.90Tang et al (2012) 91.76 92.91 91.94 97.49 92.20Table 6: Main results.
?baseline-raw?
and ?baseline-clean?
denote performance of our baseline taggeron the raw and cleaned data, respectively.
?best-clean?
is best performance achieved using a 4-gramWRRBM.
The lower part shows accuracies of the official baseline and that of the top 2 participants.
(ASO) method of Ando and Zhang (2005).
Theidea is to project the original feature representa-tions into low dimensional representations, whichyields a high-accuracy classifier on the target do-main.
The new representations are induced basedon the auxiliary tasks defined on unlabelled datatogether with a dimensionality reduction tech-nique.
Such auxiliary tasks can be specific to thesupervised task.
As pointed out by Plank (2009),for many NLP tasks, defining the auxiliary tasks isa non-trivial engineering problem.
Compared withBlitzer et al (2006), the advantage of using RBMsis that it learns representations in a pure unsuper-vised manner, which is much simpler.Besides learning representations, another lineof research addresses domain-adaptation by in-stance re-weighting (Bickel et al, 2007; Jiangand Zhai, 2007) or feature re-weighting (Satpaland Sarawagi, 2007).
Those methods assume thateach example x that has a non-zero probability onthe source domain must have a non-zero proba-bility on the target domain, and vice-versa.
Aspointed out by Titov (2011), such an assumptionis likely to be too restrictive since most NLP tasksadopt word-based or lexicon-based features thatvary significantly across different domains.Regarding using neural networks for sequentiallabelling, our approach shares similarity with thatof Collobert et al (2011).
In particular, we bothuse a non-linear layer to model complex relationsunderling word embeddings.
However, our net-work differs from theirs in the following aspects.Collobert et al (2011) model the dependency be-tween neighbouring tags in a generative manner,by employing a transition score Aij.
Training thescore involves a forward process of complexityO(nT2), where T denotes the number of tags.
Ourmodel captures such a dependency in a discrimina-tive manner, by just adding tag-related features tothe sparse-feature module.
In addition, Collobertet al (2011) train their network by maximizing thetraining set likelihood, while our approach is tominimize the margin loss using guided learning.7 ConclusionWe built a web-domain POS tagger using atwo-phase approach.
We used a WRRBM tolearn the representation of the web text andincorporate the representation in a neural net-work, which is trained using guided learningfor easy-first POS tagging.
Experiment showedthat our approach achieved significant improve-ment in tagging the web domain text.
In ad-dition, we found that keeping the learned repre-sentations unchanged yields better performancecompared with further optimizing them on thesource domain data.
We release our tools athttps://github.com/majineu/TWeb.For future work, we would like to investigatethe two-phase approach to more challenging tasks,such as web domain syntactic parsing.
We be-lieve that high-accuracy web domain taggers andparsers would benefit a wide range of downstreamtasks such as machine translation2.8 AcknowledgementsWe would like to thank Hugo Larochelle for hisadvices on re-implementing WRRBM.
We alsothank Nan Yang, Shujie Liu and Tong Xiao forthe fruitful discussions, and three anonymous re-viewers for their insightful suggestions.
This re-search was supported by the National ScienceFoundation of China (61272376; 61300097), theresearch grant T2MOE1301 from Singapore Min-istry of Education (MOE) and the start-up grantSRG ISTD2012038 from SUTD.ReferencesRie Ando and Tong Zhang.
2005.
A high-performancesemi-supervised learning method for text chunk-2This work is done while the first author is visiting SUTD.152ing.
In Proceedings of the 43rd Annual Meetingof the Association for Computational Linguistics(ACL?05), pages 1?9, Ann Arbor, Michigan, June.Association for Computational Linguistics.Yoshua Bengio, Pascal Lamblin, Dan Popovici, andHugo Larochelle.
2007.
Greedy layer-wise train-ing of deep networks.
In B. Sch?olkopf, J. Platt, andT.
Hoffman, editors, Advances in Neural Informa-tion Processing Systems 19, pages 153?160.
MITPress, Cambridge, MA.Yoshua Bengio.
2009.
Learning deep architectures forAI.
Foundations and Trends in Machine Learning,2(1):1?127.
Also published as a book.
Now Pub-lishers, 2009.Steffen Bickel, Michael Brckner, and Tobias Scheffer.2007.
Discriminative learning for differing trainingand test distributions.
In Proc of ICML 2007, pages81?88.
ACM Press.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 120?128, Sydney, Australia, July.Association for Computational Linguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing - Volume 10, EMNLP?02, pages 1?8, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2493?2537.George E. Dahl, Ryan P. Adams, and Hugo Larochelle.2012.
Training restricted boltzmann machines onword observations.
In John Langford and JoellePineau, editors, Proceedings of the 29th Interna-tional Conference on Machine Learning (ICML-12),ICML ?12, pages 679?686, New York, NY, USA,July.
Omnipress.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In Proc ofICML 2011, pages 513?520.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, HLT ?10, pages 742?750, Stroudsburg, PA,USA.
Association for Computational Linguistics.Geoffrey E. Hinton, Simon Osindero, and Yee-WhyeTeh.
2006.
A fast learning algorithm for deep beliefnets.
Neural Comput., 18(7):1527?1554, July.Geoffrey E. Hinton.
2002.
Training products of ex-perts by minimizing contrastive divergence.
NeuralComput., 14(8):1771?1800, August.Jing Jiang and ChengXiang Zhai.
2007.
Instanceweighting for domain adaptation in nlp.
In Pro-ceedings of the 45th Annual Meeting of the Associ-ation of Computational Linguistics, pages 264?271,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-sul Samad Zadeh Kaljahi, and Anton Bryl.
2012.DCU-Paris13 Systems for the SANCL 2012 SharedTask.
In Proceedings of the NAACL 2012 FirstWorkshop on Syntactic Analysis of Non-CanonicalLanguage (SANCL), pages 1?4, Montr?eal, Canada,June.Honglak Lee, Roger Grosse, Rajesh Ranganath, andAndrew Y. Ng.
2009a.
Convolutional deep beliefnetworks for scalable unsupervised learning of hi-erarchical representations.
In Proc of ICML 2009,pages 609?616.Honglak Lee, Peter Pham, Yan Largman, and AndrewNg.
2009b.
Unsupervised feature learning for audioclassification using convolutional deep belief net-works.
In Y. Bengio, D. Schuurmans, J. Lafferty,C.
K. I. Williams, and A. Culotta, editors, Advancesin Neural Information Processing Systems 22, pages1096?1104.Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang.
2013.Easy-first pos tagging and dependency parsing withbeam search.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 110?114,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 404?411, Rochester, New York, April.Association for Computational Linguistics.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 shared task on parsing the web.
Notes ofthe First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 433?440,Sydney, Australia, July.
Association for Computa-tional Linguistics.Barbara Plank.
2009.
Structural correspondence learn-ing for parse disambiguation.
In Alex Lascarides,153Claire Gardent, and Joakim Nivre, editors, EACL(Student Research Workshop), pages 37?45.
The As-sociation for Computer Linguistics.Marc?Aurelio Ranzato, Christopher Poultney, SumitChopra, and Yann LeCun.
2007.
Efficient learn-ing of sparse representations with an energy-basedmodel.
In B. Sch?olkopf, J. Platt, and T. Hoffman,editors, Advances in Neural Information Process-ing Systems 19, pages 1137?1144.
MIT Press, Cam-bridge, MA.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.Williams.
1988.
Neurocomputing: Foundationsof research.
chapter Learning Representationsby Back-propagating Errors, pages 696?699.
MITPress, Cambridge, MA, USA.Sandeepkumar Satpal and Sunita Sarawagi.
2007.
Do-main adaptation of conditional probability modelsvia feature subsetting.
In PKDD, volume 4702 ofLecture Notes in Computer Science, pages 224?235.Springer.Libin Shen, Giorgio Satta, and Aravind Joshi.
2007.Guided learning for bidirectional sequence classi-fication.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 760?767, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Buzhou Tang, Min Jiang, and Hua Xu.
2012.Varderlibt?s systems for sancl2012 shared task.
InProceedings of the NAACL 2012 First Workshopon Syntactic Analysis of Non-Canonical Language(SANCL), Montr?eal, Canada, June.Ivan Titov.
2011.
Domain adaptation by constraininginter-domain variability of latent feature representa-tion.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 62?71, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 384?394, Up-psala, Sweden, July.
Association for ComputationalLinguistics.Mengqiu Wang and Christopher D. Manning.
2013.Effect of non-linear deep architecture in sequence la-beling.
In Proceedings of the 6th International JointConference on Natural Language Processing (IJC-NLP).Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-hai Yu.
2013.
Word alignment modeling with con-text dependent deep neural network.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 166?175, Sofia, Bulgaria, August.
As-sociation for Computational Linguistics.Yue Zhang and Stephen Clark.
2011.
Syntax-basedgrammaticality improvement using ccg and guidedsearch.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1147?1157, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.2013.
Deep learning for Chinese word segmenta-tion and POS tagging.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, pages 647?657, Seattle, Wash-ington, USA, October.
Association for Computa-tional Linguistics.154
