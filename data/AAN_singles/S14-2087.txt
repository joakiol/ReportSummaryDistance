Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 503?507,Dublin, Ireland, August 23-24, 2014.SA-UZH: Verb-based Sentiment AnalysisNora Hollenstein, Michi Amsler, Martina Bachmann, Manfred KlennerInstitute of Computational Linguistics, University of ZurichBinzmuehlestrasse 14, CH-8050 Zurich, Switzerland{hollenstein,mamsler,bachmann,klenner}@ifi.uzh.chAbstractThis paper describes the details of oursystem submitted to the SemEval-2014shared task about aspect-based sentimentanalysis on review texts.
We participatedin subtask 2 (prediction of the polarityof aspect terms) and 4 (prediction of thepolarity of aspect categories).
Our ap-proach to determine the sentiment of as-pect terms and categories is based on lin-guistic preprocessing, including a com-positional analysis and a verb resource,task-specific feature engineering and su-pervised machine learning techniques.
Weused a Logistic Regression classifier tomake predictions, which were rankedabove-average in the shared task.1 IntroductionAspect-based sentiment analysis refers to theproblem of predicting the polarity of an explicitor implicit mention of a target in a sentence ortext.
The SemEval-2014 shared task required sen-timent analysis of laptop and restaurant reviewson sentence level and comprised four subtasks(Pontiki et al., 2014).
The organizers created andshared manually labelled domain-specific trainingand test data sets.
Two of the four subtasks dealtwith determining the sentiment of a given aspectterm (explicitly mentioned) or aspect category (ex-plicitly or implicitly mentioned) in a sentence.The subtasks we participated in do not include therecognition of aspects.
Given the sentence ?Thesushi rolls were perfect, but overall it was too ex-pensive.
?, ?sushi rolls?
is an aspect term, and thecorresponding aspect categories are ?food?
andThis work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/?price?.
The correct predictions would be the fol-lowing:?
Subtask 2 (aspect terms): {sushi rolls?
pos-itive}?
Subtask 4 (aspect categories): {food ?
posi-tive, price ?
negative}To solve these tasks, we introduce a Logis-tic Regression Model for target-specific sentimentanalysis.
Features are derived from a fine-grainedpolarity lexicon, a verb resource specifying expec-tations and effects of the verbs functional roles,and a compositional analysis.
In our experimentson the restaurant and laptop reviews data for theSemEval-2014 shared task, we found that im-provements over the baseline are possible for allclasses except ?conflict?.2 Related WorkWe focus on the question whether fine-grained lin-guistic sentiment analysis improves target-specificpolarity classification.
Existing approaches toaspect-based sentiment detection have focused ondifferent aspects of this task, e.g.
the identifi-cation of targets and their components (Popescuand Etzioni, 2005) and sentence-level composition(Moilanen and Pulman, 2007).
Ding et al.
(2008)and Hu and Liu (2004) produced lexicon-basedapproaches, which perform quite well in a largenumber of domains, and Blair-Goldensohn et al.
(2008) combined lexicon-based methods and su-pervised learning.
Jiang et al.
(2011) used a depen-dency parser to generate a set of aspect dependentfeatures for classification.
For our system we builta sentiment composition resembling the one ofL?aubli et al.
(2012), which was developed for Ger-man.
Moreover, our verb resource has some simi-larity with the one of Neviarouskaya et al.
(2009):both rely on verb classes and utilize verb-specific503behavior.
However, only we specify the individ-ual verb?s (default) perspective on each role (andare, thus, able to count polar propagations).
Seealso Reschke and Anand (2011), who describe indetail how polar (verb) complements combine toverb frame polarity (again without recording andusing role perspectives as we do).3 System DescriptionIn this section we present the details of our senti-ment analysis system.
We used the same prepro-cessing and learning algorithm for both subtasks(2 & 4).
Only the feature extraction was expandedin subtask 4 for determining the polarities of as-pect categories (see section 3.3).
The data setsconsisted of restaurant and laptop reviews, whichcomprise about 3?000 manually classified target-specific sentences for each domain.3.1 Sentiment CompositionThe fundamental steps of our sentiment analysissystem are parsing the sentences, rule-based sen-timent analysis using a polarity lexicon and a verbresource, feature extraction and training a machinelearning algorithm.
In this section we will de-scribe the composition of the lexicon as well as thestructure of the sentiment composition pipeline.Category ExamplePOS strong ?awesome?POS weak ?adequate?NEG strong ?catastrophe?NEG weak ?demotivated?POS active ?generous?POS passive ?noteworthy?NEG active ?rebellion?NEG passive ?orphaned?Table 1: Additional categories in our fine-grainedpolarity lexiconThe same polarity lexicon was used for bothdomains.
After mapping the polarities from thelexicon to the words and multi-word expressions,we calculated the polarity of nominal (NPs) andprepositional phrases (PPs) by means of lexicalmarking and the syntactic analysis of a depen-dency parser (Choi and Palmer, 2011).
We did notimplement any rules for neutral phrases, all wordsand phrases not marked as positive or negative areconsidered as neutral.
In general, the polarities arepropagated bottom-up to their respective heads ofthe NPs/PPs in composition with the subordinates.Shifters and negation words are also taken into ac-count.
The parser output is converted into a con-straint grammar (CG) format for the subsequentanalysis of words and phrases.
To conduct thiscomposition of polarity for the phrases we imple-mented a CG with the vislcg3 tools (VISL-group,2013).
The next stage of our sentiment detectionis the verb resource, which was also implementedwith the vislcg3 tools and will be explained in thenext section.3.2 Verb-based Sentiment AnalysisIn order to combine the composition of the po-lar phrases with verb information, we encoded theimpact of the verbs on polarity using three di-mensions: effects, expectations and verb polarity.While effects should be understood as the outcomeinstantiated through the verb, expectations can beunderstood as anticipated polarities induced by theverb.
Effects and expectations are assigned to sub-jects or objects, not to the verb itself.
A positiveor negative verb effect propagates from the verb toa subject or object if the latter receives the polar-ity of the verb.
For a verb expectation, the subjector object is expected to be polar and thus receivesa polarity even if the sentiment composition re-sulted neutral (see examples below).
The verb po-larity as such is the evaluation of the whole verbalphrase.
Moreover, we process predicative and pas-sive verbs, adapting the effects and expectations tothe syntactic structure.Since these effects and expectations match di-rectly to the subject and objects of a sentence,they are of great use detecting the polarity of as-pect terms (which are predominantly subjects orobjects).
We present the following examples ex-tracted from the training data to illustrate three di-mensions annotated by the verb analysis:?
Example of a positive effect on the direct ob-ject of a sentence induced by the verb: ?Ilove (verb POS) the operating system and thepreloaded software (POS EFF).??
Example for a negative expectation on aprepositional object induced by the verb:?[...]
the guy, who constantly com-plains (verb NEG) about the noise level(NEG EXP).??
Example of positive predicative effectswith an auxiliary, non-polar verb: ?Ser-504vice (POS predicative) is (verb PRED) great,takeout (POS predicative) is (verb PRED)good too.
?Furthermore, we make a distinction between thedifferent prepositions a verb can invoke and thesucceeding semantic changes.
For example, theverb ?to die?
can be annotated in three differentmanners, depending on the prepositional object:1.
?My phone died (verb NEG).?2.
?Their pizza (POS EFF) is to die (verb POS)for.?3.
?He died (verb NEG) of cancer(NEG EXP).
?To summarize, in addition to verb polarity, weintroduce effects and expectations to verb frames,which are determined through the syntactic patternfound, the bottom-up calculated phrase polaritiesand the meaning of the verb itself.
We manuallycategorized approx.
300 of the most frequent pos-itive and negative English verbs and their respec-tive verb frames.Laptop reviewsFeature Occurrences in %Verbs effects 367 12.05Verb expectations 6 0.02Predicatives 298 9.78Polar verbs 530 17.39Restaurant reviewsFeature Occurrences in %Verbs effects 246 8.09Verb expectations 12 0.04Predicatives 378 12.43Polar verbs 521 17.13Table 2: Occurrences and percentage of sentencesof annotated polar verb features in the training dataof the shared taskIn table 2, we illustrate the relevance of the lin-guistic features of this verb resource by showing inhow many sentences of the training set these anno-tations appear.
Since we merely annotated the verbframes of the most frequent English verbs, it isconceivable that this resource may have a consid-erably greater effect if more domain-specific verbsare modelled.After this final sentiment composition step, allderived polarity chunks are converted into a set offeatures for machine learning algorithms.3.3 Feature ExtractionIn a first step of our system, the sentences areparsed, phrase polarities are calculated and verbeffects and expectations are assigned.
Subse-quently, a feature extractor, which extracts and ag-gregates polar information, operates on the out-put.
The Simple Logistic Regression classifierfrom weka Hall et al.
(2009) is then trained onthese features.We developed a feature extraction pipeline thatretrieves information about various polarity levelsin words, syntactic functions and phrases of thesentences in the data set.
In order to use our senti-ment composition approach for machine learning,we extract three different sets of features, result-ing in a total of 32 features for subtask 2 and 39features for subtask 4.In short, the feature sets are constructed as fol-lows:?
Lexicon-based features: These features com-prise simple frequency counts of positive andnegative words in the sentences and binaryfeatures showing whether any positive ornegative, strong or active tokens are presentat all.
Furthermore, these features not onlyinclude absolute counts but also token ratios.?
Composition-based features: This feature setdescribes the information found in nomi-nal, prepositional and verbal phrases, suchas the number of positive/negative phraseheads or predicative verb effects found.
Itis also possible to distinguish between fea-tures which represent frequency counts andfeatures which represent polarity ratios.?
Target-specific features: This set includesfeatures from the previous two sets in con-nection with the aspect terms, e.g.
whetherthe aspect term has a verb expectation orwhether the aspect term is the head of a neg-ative/positive phrase, the subject or direct ob-ject, etc.
In this set we also include accu-mulative features that represent the completeamount of polar information in connectionwith an aspect term.?
(only for subtask 4) Category-specific fea-tures: These features are based on a co-occurrence analysis of the most frequentwords used in each category.
That is to505say, we calculated the frequencies of all po-lar nouns, verbs and adjectives that appear insentences of the same category in order tofind category-specific words which have aninfluence on the polarity.
This set includesfeatures such as the number of category-specific words occurring in the sentence, etc.For the classification of the aspect terms andcategories of the sentences into the four classes(positive, negative, neutral and conflict), wetrained a Simple Logistic Regression classifier onthe features described above.
We also exploredother machine learning algorithms such as SVMsand artificial neural networks, however, the Logis-tic Regression proved to yield the best results.4 Results & DiscussionIn this section we present and discuss the resultsof our system in the SemEval 2014 shared task.The results of our submission for subtasks 2 and 4,compared to the majority baselines, can be foundin table 3.
Our system performs significantly bet-ter on restaurant reviews than on laptop reviews,probably due to the fact that our polarity lexi-con comprises more restaurant-specific vocabu-lary than computer-specific vocabulary.Subtask Data Baseline Acc.
(2) Laptops 47.06 58.30(2) Restaurants 57.8 70.98(4) Restaurants 59.84 73.10Table 3: Shared-Task results for subtask 2 (aspectterm polarity) and subtask 4 (aspect category po-larity)In both subtasks, calculating the polarity of theaspect terms and the aspect categories, the classpositive scores better than the three other classes.In all data sets and all subtasks positive was themajority class of the four-partite classification:42% in the aspect terms of the laptop reviews, 59%in the aspect terms and aspect categories of thelaptop reviews equally (measured in the trainingdata).
Thus, it is not surprising that the most fre-quent error of our system is to categorize neutralaspect terms and categories as positive.We do not achieve any improvements for theclass conflict.
The latter is very hard to detect, notonly because this class is difficult to define but alsobecause of the lack of training data given for thisclass.
This could not be improved even thoughwe included lexical features to address this par-ticular class, for example, Boolean features show-ing whether an adversative conjunction is presentin the sentence or whether the count of positivechunks equals the count of negative chunks in thesame sentence.
These features are in line withthe theory that aspects are considered controver-sial if positive and negative occurrences are bal-anced and no polarity clearly prevails.
Further-more, the conflictive facet of a sentence is fre-quently not represented in the words (e.g.
?It hasno camera, but I can always buy and install oneeasy.?
; camera = conflict).
Thus, it becomes chal-lenging to generate features for this class conflictwith a lexicon-based approach.Furthermore, since our verb resource was newlyimplemented, there are still many verbs (espe-cially domain-specific verbs) which will have tobe modelled in addition to the most frequent En-glish verbs included in the analysis by now.
An-other limitation of our current system is the factthat verb negation is not yet implemented: Weprocess negation occurring in noun phrases (e.g.
?a not so tasty chicken curry?
), but not when thenegation word relates to the verb (e.g.
?we didn?tcomplain?
).In summary, our aspect-based sentiment anal-ysis pipeline takes into consideration many lin-guistic characteristics relevant for detecting opin-ion, and still provides the possibility to expand ourcompositional resources.5 ConclusionGiven the above-average results obtained in theshared task system ranking, we conclude that themethod for aspect-based sentiment analysis in re-view texts presented in this paper yields competi-tive results.
We showed that the performance forthis task can be improved by using linguisticallymotivated features for all classes except conflict.We presented a supervised aspect-based senti-ment analysis system to detect target-specific po-larity with features derived from a fine-grained po-larity lexicon, a verb resource and compositionalanalysis based on a dependency parser.
Our resultshave shown that deeper linguistic analysis can pos-itively influence the detection of target-specificpolarities on sentence level in review texts.506AcknowledgementsWe would like to thank the organizers of theshared task for their effort, as well as the reviewersfor their helpful comments on the paper.ReferencesSasha Blair-Goldensohn, Kerry Hannan, RyanMcDonald, Tyler Neylon, George A. Reis, , andJeff Reynar.
Building a sentiment summarizerfor local service reviews.
In WWW Workshop onNLP in the Information Explosion Era, 2008.Jinho D. Choi and Martha Palmer.
Getting themost out of transition-based dependency pars-ing.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguis-tics: Human Language Technologies, HLT ?11,pages 687?692, Stroudsburg, PA, USA, 2011.ACL.Xiaowen Ding, Bing Liu, and Philip S. Yu.
Aholistic lexicon-based approach to opinion min-ing.
In Proceedings of the 2008 InternationalConference on Web Search and Data Mining,WSDM ?08, pages 231?240, New York, NY,USA, 2008.
ACM.Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-hard Pfahringer, Peter Reutemann, and Ian H.Witten.
The weka data mining software: Anupdate.
SIGKDD Explor.
Newsl., 11(1):10?18,November 2009.Minqing Hu and Bing Liu.
Mining and sum-marizing customer reviews.
In Proceedings ofthe Tenth ACM SIGKDD International Confer-ence on Knowledge Discovery and Data Min-ing, KDD ?04, pages 168?177, New York, NY,USA, 2004.
ACM.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu,and Tiejun Zhao.
Target-dependent twitter sen-timent classification.
In Proceedings of the 49thAnnual Meeting of the Association for Compu-tational Linguistics, pages 151?160.
Associa-tion for Computational Linguistics, 2011.Samuel L?aubli, Mario Schranz, Urs Christen, andManfred Klenner.
Sentiment Analysis for Me-dia Reputation Research.
In Proceedings ofKONVENS 2012 (PATHOS 2012 workshop),pages 274?281, Vienna, Austria, 2012.Karo Moilanen and Stephen Pulman.
Sentimentcomposition.
In Proceedings of RANLP-2007,pages 378?382, Borovets, Bulgaria, 2007.Alena Neviarouskaya, Helmut Prendinger, andMitsuru Ishizuka.
Semantically distinct verbclasses involved in sentiment analysis.
IADISAC (1), 2009.Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Harris Papageorgiou, Ion Androutsopoulos,and Suresh Manandhar.
SemEval-2014 Task 4:Aspect Based Sentiment Analysis.
In Proceed-ings of the 8th International Workshop on Se-mantic Evaluation (SemEval 2014), Dublin, Ire-land, 2014.Ana-Maria Popescu and Oren Etzioni.
Extractionof product features and opinions from reviews.In Proceedings of HLT-EMNLP-05, pages 339?349, Vancouver, Canada, 2005.Kevin Reschke and Pranav Anand.
Extractingcontextual evaluativity.
In Proceedings of theNinth International Conference on Computa-tional Semantics, pages 370?374, 2011.VISL-group.
http://beta.visl.sdu.dk/cg3.html.
In-stitute of Language and Communication (ISK),University of Southern Denmark, 2013.507
