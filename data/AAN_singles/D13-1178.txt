Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1721?1731,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsGenerating Coherent Event Schemas at ScaleNiranjan Balasubramanian, Stephen Soderland, Mausam, Oren EtzioniComputer Science & EngineeringUniversity of WashingtonSeattle, WA 98195, USA{niranjan,ssoderlan, mausam, etzioni}@cs.washington.eduAbstractChambers and Jurafsky (2009) demonstratedthat event schemas can be automatically in-duced from text corpora.
However, our analy-sis of their schemas identifies several weak-nesses, e.g., some schemas lack a commontopic and distinct roles are incorrectly mixedinto a single actor.
It is due in part to theirpair-wise representation that treats subject-verb independently from verb-object.
This of-ten leads to subject-verb-object triples that arenot meaningful in the real-world.We present a novel approach to inducingopen-domain event schemas that overcomesthese limitations.
Our approach uses co-occurrence statistics of semantically typed re-lational triples, which we call Rel-grams (re-lational n-grams).
In a human evaluation, ourschemas outperform Chambers?s schemas bywide margins on several evaluation criteria.Both Rel-grams and event schemas are freelyavailable to the research community.1 IntroductionEvent schemas (also known as templates or frames)have been widely used in information extraction.An event schema is a set of actors (also known asslots) that play different roles in an event, such asthe perpetrator, victim, and instrument in a bomb-ing event.
They provide essential guidance in ex-tracting information related to events from free text(Patwardhan and Riloff, 2009), and can also aid inother NLP tasks, such as coreference (Irwin et al2011), summarization (Owczarzak and Dang, 2010),and inference about temporal ordering and causality.Actor Rel ActorA1:<person> failed A2:testA1:<person> was suspended for A3:<time period>A1:<person> used A4:<substance, drug>A1:<person> was suspended for A5:<game, activity>A1:<person> was in A6:<location>A1:<person> was suspended by A7:<org, person>Actor Instances:A1: {Murray, Morgan, Governor Bush, Martin, Nelson}A2: {test}A3: {season, year, week, month, night}A4: {cocaine, drug, gasoline, vodka, sedative}A5: {violation, game, abuse, misfeasance, riding}A6: {desert, Simsbury, Albany, Damascus, Akron}A7: {Fitch, NBA, Bud Selig, NFL, Gov Jeb Bush}Table 1: An event schema produced by our system, rep-resented as a set of (Actor,Rel, Actor) triples, and a setof instances for each actor A1, A2, etc.
For clarity weshow unstemmed verbs.Until recently, all event schemas in use in NLPwere hand-engineered, e.g., the MUC templates andACE event relations (ARPA, 1991; ARPA, 1998;Doddington et al 2004).
This led to technology thatcould only focus on specific domains of interest andhas not been applicable more broadly.The seminal work of Chambers and Jurafsky(2009) showed that event schemas can also be in-duced automatically from text corpora.
Instead oflabeled roles these schemas have a set of relationsand actors that serve as arguments.1 Their systemis fully automatic, domain-independent, and scalesto large text corpora.However, we identify several limitations in theschemas produced by their system.2 Their schemas1In the rest of this paper we use event schemas to refer tothese automatically induced schemas with actors and relations.2Available at http://www.usna.edu/Users/cs/nchamber/data/schemas/acl091721Actor Rel ActorA1 caused A2A2 spread A1A2 burned A1- extinguished A1A1 broke out -- put out A1Actor Instances:A1: {fire, aids, infection, disease}A2: {virus, bacteria, disease, urushiol, drug}Table 2: An event schema from Chambers?
system thatmixes the events of fire spreading and disease spreading.often lack coherence: mixing unrelated events andhaving actors whose entities do not play the samerole in the schema.
Table 2 shows an event schemafrom Chambers that mixes the events of fire spread-ing and disease spreading.Much of the incoherence of Chambers?
schemascan be traced to their representation that uses pairsof elements from an assertion, thus, treating subject-verb and verb-object separately.
This often leads tosubject-verb-object triples that do not make sense inthe real world.
For example, the assertions ?firecaused virus?
and ?bacteria burned AIDS?
are im-plicit in Table 2.Another limitation in schemas Chambers releasedis that they restrict schemas to two actors, which canresult in combining different actors.
Table 4 showsan example of combining perpeterators and victimsinto a single actor.1.1 ContributionsWe present an event schema induction algorithm thatovercomes these weaknesses.
Our basic represen-tation is triples of the form (Arg1, Relation, Arg2),extracted from a text corpus using Open InformationExtraction (Mausam et al 2012).
The use of triplesaids in agreement between subject and object of arelation.
The use of Open IE leads to more expres-sive relation phrases (e.g., with prepositions).
Wealso assign semantic types to arguments, both to al-leviate data sparsity and to produce coherent actorsfor our schemas.Table 1 shows an event schema generated by oursystem.
It has six relations and seven actors.
Theschema makes several related assertions about a per-son using a drug, failing a test, and getting sus-pended.
The main actors in the schema include theperson who failed the test, the drug used, and theagent that suspended the person.Our first step in creating event schemas is to tab-ulate co-occurrence of tuples in a database that wecall Rel-grams (relational n-grams) (Sections 3, 5.1).We then perform analysis on a graph induced fromthe Rel-grams database and use this to create eventschemas (Section 4).We compared our event schemas with those ofChambers on several metrics including whether theschema pertains to a coherent topic or event andwhether the actors play a coherent role in that event(Section 5.2).
Amazon Mechanical Turk workersjudged that our schemas have significantly better co-herence ?
92% versus 82% have coherent topic and81% versus 59% have coherent actors.We release our open domain event schemas andthe Rel-grams database3 for further use by the NLPcommunity.2 System OverviewOur approach to schema generation is based on theidea that frequently co-occurring relations in textcapture relatedness of assertions about real-worldevents.
We begin by extracting a set of relational tu-ples from a large text corpus and tabulate occurrenceof pairs of tuples in a database.We then construct a graph from this database andidentify high-connectivity nodes (relational tuples)in this graph as a starting point for constructing eventschemas.
We use graph analysis to rank the tu-ples and merge arguments to form the actors in theschema.3 Modeling Relational Co-occurrenceIn order to tabulate pairwise occurences of relationaltuples we need a suitable relation-based represen-tation.
We now describe the extraction and rep-resentation of relations, a database for storing co-occurrence information, and our probabilistic modelfor the co-occurrence.
We call this model Rel-grams, as it can be seen as a relational analog to then-grams language model.3.1 Relations Extraction and RepresentationWe extract relational triples from each sentence ina large corpus using the OLLIE Open IE system3Available at http://relgrams.cs.washington.edu1722Tuples TableId Arg1 Rel Arg2 Count... ... ... ... ...13 bomb explode in <loc> 54714 bomb explode in Baghdad 2215 bomb explode in market 7... ... ... ... ...87 bomb kill <per> 173... ... ... ... ...92 <loc> be suburb of <loc> 1023... ... ... ... ...BigramCounts TableT1 T2 Dist.
Count E11 E12 E21 E22... ... ... ... ... ... ... ...13 87 1 27 25 0 0 013 87 2 35 33 0 0 0... ... ... ... ... ... ... ...13 87 10 62 59 0 0 087 13 1 6 0 0 0 0... ... ... ... ... ... ... ...92 13 1 12 0 0 12 0... ... ... ... ... ... ...
...Figure 1: Tables in the Rel-grams Database: Tuples maps tuples to unique identifiers, BigramCounts provides theco-occurrence counts (Count) within various distances (Dist.
), and four types of argument equality counts (E11-E22).E11 is the number of times when T1.Arg1 = T2.Arg1, E12 is when T1.Arg1 = T2.Arg2 and so on.
(Mausam et al 2012).4 This provides relational tu-ples in the format (Arg1, Relation, Arg2) where eachtuple element is a phrase from the sentence.
Thesentence ?He cited a new study that was released byUCLA in 2008.?
produces three tuples:1.
(He, cited, a new study)2.
(a new study, was released by, UCLA)3.
(a new study, was released in, 2008)Relational triples provide a more specific repre-sentation which is less ambiguous when comparedto (subj, verb) or (verb, obj) pairs.
However, usingrelational triples also increases sparsity.
To reducesparsity and to improve generalization, we representthe relation phrase by its stemmed head verb plusany prepositions.
The relation phrase may includeembedded nouns, in which case these are stemmedas well.
Moreover, tuple arguments are representedas stemmed head nouns, and we also record seman-tic types of the arguments.We selected 29 semantic types from WordNet, ex-amining the set of instances on a small developmentset to ensure that the types are useful, but not overlyspecific.
The set of types are: person, organization,location, time unit, number, amount, group, busi-ness, executive, leader, effect, activity, game, sport,device, equipment, structure, building, substance,nutrient, drug, illness, organ, animal, bird, fish, art,book, and publication.To assign types to arguments, we apply StanfordNamed Entity Recognizer (Finkel et al 2005)5, andalso look up the argument in WordNet 2.1 and record4Available at: http://knowitall.github.io/ollie/5We used the system downloaded from: http://nlp.stanford.edu/software/CRF-NER.shtml and usedthe seven class CRF model distributed with it.the first three senses if they map to our target se-mantic types.
We use regular expressions to recog-nize dates and numeric expressions, and map per-sonal pronouns to <person>.
We associate all typesfound by this mechanism with each argument.
Thetuples in the example above are normalized to thefollowing:1.
(He, cite, study)2.
(He, cite, <activity>)3.
(<person>, cite, study)4.
(<person>, cite, <activity>)5.
(study, be release by, UCLA)6.
(study, be release by, <organization>)7.
(study, be release in, 2008)8.
(study, be release in, <time unit>)9.
(<activity>, be release by, UCLA)...In our preliminary experiments, we found that us-ing normalized relation strings and semantic classesfor arguments results in a ten-fold increase in thenumber of Rel-grams with a minimum support.3.2 Co-occurrence TabulationWe construct a database to hold co-occurrencestatistics for pairs of tuples found in each document.Figure 1 shows examples for the types of statisticscontained in the database.
The database consistsof two tables: 1) Tuples ?
Maps each tuple to aunique identifier and tabulates tuple counts.
2) Bi-gramCounts ?
Stores the directional co-occurrencefrequency, a count for tuple T followed by T ?
at adistance of k, and tabulates the number of times thesame argument was present in the pair of tuples.Equality Constraints: Along with the co-occurrence counts, we record the equality of argu-ments in Rel-grams pairs.
We assert an argument1723Table 3: Given a source tuple, the Rel-grams languagemodel estimates the probability of encountering other re-lational tuples in a document.
For clarity, we show theunstemmed version.Top tuples related to(<person>, convicted of, murder)1.
(<person>, convicted in, <time unit>)2.
(<person>, sentenced to, death)3.
(<person>, sentenced to, year)4.
(<person>, convicted in, <location>)5.
(<person>, sentenced to, life)6.
(<person>, convicted in, <person>)7.
(<person>, convicted after, trial)8.
(<person>, sent to, prison)pair is equal if they are from the same token se-quence in the source sentence or one argument is aco-referent mention of the other.
We use the Stan-ford Co-reference system (Lee et al 2013)6 to de-tect co-referring mentions.
There are four possibleequalities depending on the specific pair of argu-ments in the tuples are the same, shown as E11, E12,E21 and E22 in Figure 1.
For example, the E21 col-umn has counts for the number of times the Arg2 ofT1 was determined to be the same as the Arg1 of T2.Implementation and Query Language: We pop-ulated the Rel-grams database using OLLIE extrac-tions from a set of 1.8 Million New York Times arti-cles drawn from the Gigaword corpus.
The databaseconsisted of approximately 320K tuples that havefrequency ?
3 and 1.1M entries in the bigram table.The Rel-grams database allows for powerfulquerying using SQL.
For example, Table 3 shows themost frequent rel-grams associated with the querytuple (<person>, convicted of, murder).3.3 Rel-grams Language ModelFrom the tabulated co-occurrence statistics, we esti-mate bi-gram conditional probabilities of tuples thatoccur within a window of k tuples from each other.Formally, we use Pk(T ?|T ) to denote the conditionalprobability that T ?
follows T within a window of ktuples.
To discount estimates from low-frequencytuples, we use a ?-smoothed estimate:6Available for download at: http://nlp.stanford.edu/software/dcoref.shtmlPk(T?|T ) =#(T, T ?, k) + ?
?T ??
?V#(T, T ?
?, k) + ?
?
|V |(1)where, #(T, T ?, k) is the number of times T ?
fol-lows T within a window of k tuples.
k = 1 in-dicates adjacent tuples in the document.
|V | is thenumber of unique tuples in the corpus.
For experi-ments in this paper, we set ?
to 0.05.Co-occurrence within a small window is usu-ally more reliable but is also sparse, whereas co-occurrence within larger windows addresses sparsitybut may lead to topic drift.
To leverage the bene-fits of different window sizes, we also define a met-ric with a weighted average of window sizes from 1to 10, where the weight decays as window size in-creases.
For example, with ?
set to 0.5 in equation2, a window of k+1 has half the weight of a windowof k.P (T ?|T ) =?10k=1 ?kPk(T ?|T )?10k=1 ?k(2)We believe that Rel-grams is a valuable sourceof common-sense knowledge and may be useful forseveral downstream tasks such as improving infor-mation extractors, inference of implicit information,etc.
We assess its usefulness in the context of gener-ating event schemas.4 Schema GenerationWe now use Rel-grams to identify relations and ac-tors pertaining to a particular event.
Our schemageneration consists of three steps.
First, we build arelation graph of tuples (G) using connections iden-tified by Rel-grams.
Second, we identify a set ofseed tuples as starting points for schemas.
We usegraph analysis to find the tuples most related to eachseed.
Finally, we merge the arguments in these tu-ples to create actors and output the final schema.Next we describe each of these steps in detail.4.1 Rel-graph constructionWe define a Rel-graph as an undirected weightedgraph G = (V,E), whose vertices (V ) are relationtuples with edges (E), where an edge between ver-tices T and T ?
is weighted by the symmetric condi-tional probability SCP (T, T ?)
defined as1724SCP (T, T ?)
= P (T |T ?)?
P (T ?|T ) (3)Both conditional probabilities are computed inEquation 2.
Figure 2 shows a portion of a Rel-graphwhere the thickness of the edge indicates symmetricconditional probability.
(bomb, explode at, <location>)(bomb, explode on,          <time_unit>) (bomb, kill,      <person>)(bomb, wound,       <person>) (<person>, plant,          bomb)(<organization>, claim, responsibility)Figure 2: Part of a Rel-graph showing tuples stronglyassociated with (bomb, explode at, <location>).
Undi-rected edges are weighted by symmetric conditionalprobability with line thickness indicating weight.4.2 Finding Related TuplesOur goal is to find closely related tuples that per-tain to an event or topic.
First, we locate high-connectivity nodes in the Rel-graph to use as seeds.We sort nodes by the sum of their top 25 edgeweights7 and take the top portion of this list afterfiltering out redundant views of the same relation.For each seed (Q), we find related tuples by ex-tracting the sub-graph (GQ) from Q?s neighbors(within two hops from Q) in the Rel-graph.
Graphanalysis can detect the strongly connected nodeswithin this sub-graph, representing tuples that fre-quently co-occur in the context of the seed tuple.Page rank is a well-known graph analysis algo-rithm that uses graph connectivity to identify impor-tant nodes within a graph (Brin and Page, 1998).
Weare interested in connectivity within a subgraph withrespect to a designated query node (the seed).
Con-nection to a query node can help minimize conceptdrift and ensure that the selected tuples are closelyrelated to the main topic of the sub-graph.7Limiting to the top 25 edges avoids overly general tuplesthat occur in many topics, which tend to have a large number ofweak edges.In this work, we adapt the Personalized PageR-ank algorithm (Haveliwala, 2002).
The personalizedversion of PageRank returns ranks of various nodeswith respect to a given query node and hence is moreappropriate for our task than the basic PageRank al-gorithm.
Within the subgraph GQ for a given seedQ, we compute a solution to the following set ofPageRank Equations:PRQ(T )= (1?
d) + dXT ?SCP (T, T ?)PRQ(T?)
if T = Q= dXT ?SCP (T, T ?)PRQ(T?)
otherwiseHere PRQ(T ) denotes the page rank of a tuple Tpersonalized for the query tuple Q.
It is a sum ofall its neighbors?
page ranks, weighted by the edgeweights; d is the damping probability, which we setto be 0.85 in our implementation.The solution is computed iteratively by initializ-ing the page rank of Q to 1 and all others to 0, thenrecomputing page rank values until they converge towithin a small .
This computation remains scalable,since we restrict it to subgraphs a small number ofhops away from the query node.
This is a standardpractice to handle large graphs (Agirre and Soroa,2009; Mausam et al 2010).4.3 Creating Actors and RelationsWe take the top n tuples from GQ according totheir Page rank scores.
From each tuple T :(Arg1, Rel, Arg2) in GQ, we record two actors(A1, A2) corresponding to Arg1 and Arg2, and addRel to the list of relations that they participate in.Then, we merge actors in two steps.
First, we col-lect the equality constraints for the tuples in GQ.
Ifthe arguments corresponding to any pair of actorshave a non-zero equality constraint then we mergethem.
Second, we merge actors that perform simi-lar actions.
A1 and A2 are merged if they are con-nected to the same actor A3 through the same rela-tion.
For example, A1 and A2 in (A1:lawsuit, file by,A3:company) and (A2:suit, file by, A3:company),will be merged into a single actor.
To avoid mergingdistinct actors, we use a small list of rules that spec-ify the semantic type pairs that cannot be merged(e.g., location-date).
Also, we do not merge two ac-tors, if it can result in a relation where the same actor1725System A1 Rel A2Relgrams {bomb, missile, grenade, device} explode in {city, Bubqua, neighborhood}{bomb, missile, grenade, device} explode kill {people, civilian, lawmaker, owner, soldier}{bomb, missile, grenade, device} explode on {Feb., Fri., Tues., Sun., Sept.}{bomb, missile, grenade, device} explode wound {civilian, person, people, soldier, officer}{bomb, missile, grenade, device} explode in {Feb., Beirut Monday, Sept., Aug.}{bomb, missile, grenade, device} explode injure {woman, people, immigrant, policeman}Chambers {bomb, explosion, blast, bomber, mine} explode {soldier, child, civilian, bomber, palestinian}{soldier, child, civilian, bomber, palestinian} set off {bomb, explosion, blast, bomber, mine, bombing}{bomb, explosion, blast, bomber, mine} kill {soldier, child, civilian, bomber, palestinian}{soldier, child, civilian, bomber, palestinian} detonate {bomb, explosion, blast, bomber, mine, bombing}{bomb, explosion, blast, bomber, mine} injure {soldier, child, civilian, bomber, palestinian}{soldier, child, civilian, bomber, palestinian} plant {bomb, explosion, blast, bomber, mine, bombing}Relgrams {Carey, John Anthony Volpe, Chavez, She } veto {legislation, bill, law, measure, version}{legislation, bill, law, measure, version} be sign by {Carey, John Anthony Volpe, Chavez, She }{legislation, bill, law, measure, version} be pass by {State Senate, State Assembly, House, Senate, Parliament}{Carey, John Anthony Volpe, Chavez, She } sign into {law}{Carey, John Anthony Volpe, Chavez, She } to sign {bill}{Carey, John Anthony Volpe, Chavez, She } be governor of {Massachusetts, state, South Carolina, Texas, California}Chambers {clinton, bush, bill, president, house} oppose {bill, measure, legislation, plan, law}{clinton, bush, bill, president, house} sign {bill, measure, legislation, plan, law}{clinton, bush, bill, president, house} approve {bill, measure, legislation, plan, law}{clinton, bush, bill, president, house veto {bill, measure, legislation, plan, law}{clinton, bush, bill, president, house} support {bill, measure, legislation, plan, law}{clinton, bush, bill, president, house} pass {bill, measure, legislation, plan, law}Table 4: ?Bombing?
and ?legislation?
schema examples from Rel-grams and Chambers represented as a set of(A1, Rel, A2) tuples, where the schema provides a set of instances for each actor A1 and A2.
Relations and argu-ments are in the stemmed form, e.g., ?explode kill?
refers to ?exploded killing?.
Instances in bold produce tuples thatare not valid in the real world.is both the Arg1 and Arg2.Finally, we generate an ordered list of tuples usingthe final set of actors and their relations.
The out-put tuples are sorted by the average page rank of theoriginal tuples, thereby reflecting their importancewithin the sub-graph GQ.5 EvaluationWe present experiments to explore two main ques-tions: How well do Rel-grams capture real worldknowledge, and what is the quality of event schemasbuilt using Rel-grams.5.1 Evaluating Rel-gramsWhat sort of common-sense knowledge is encap-sulated in Rel-grams?
How often does it indicatean implication between a pair of statements, andhow often does it indicate a common real-worldevent or topic?
To answer these questions, we con-ducted an experiment to identify a subset of our Rel-grams database with high precision for two forms ofcommon-sense knowledge:?
Implication: The Rel-grams express an im-plication from T to T?
or from T?
to T, abi-directional form of the Recognizing Tex-tual Entailment (RTE) guidelines (Dagan et al2005).?
Common Topic: Is there an underlying com-mon topic or event to which both T and T?
arerelevant?We also evaluated whether both T and T?
are validtuples that are well-formed and make sense in thereal world, a necessary pre-condition for either im-plication or common topic.We are particularly interested in the highest pre-cision portion of our Rel-grams database.
Thedatabase has 1.1M entries with support of at leastthree instances for each tuple.
To find the highestprecision subset of these, we identified tuples thathave at least 25 Rel-grams, giving us 12,600 seedtuples with a total of over 280K Rel-grams.
Finally,we sorted this subset by the total symmetrical con-ditional probability of the top 25 Rel-grams for eachseed tuple.We tagged a sample of this 280K set of Rel-gramsfor valid tuples, implication between T and T?, andcommon topic.
We found that in the top 10% of thisset, 87% of the seed tuples were valid and 74% of theRel-grams had both tuples valid.
Of the Rel-grams1726System Id A1 Rel A2Relgrams R1 bomb explode in citybomb explode kill peoplebomb explode on Fri.... ...
...Chambers C1 blast explode childchild detonate blastchild plant bomb... ... ...Table 5: A grounded instantiation of the schemas fromTable 4, where each actor is represented as a randomlyselected instance.with both tuples valid, 83% expressed an implicationbetween the tuples, and 90% had a common topic.There were several reasons for invalid tuples ?parsing errors; binary projections of inherently n-aryrelations, for example (<person>, put, <person>);head-noun only representation omitting essential in-formation; and incorrect semantic types, primarilydue to NER tagging errors.While the Rel-grams suffer from noise in the tu-ple validity, there is clearly strong signal in the dataabout common topic and implication between tuplesin the Rel-grams.
As we demonstrate in the follow-ing section, an end task can use graph analysis tech-niques to amplify this strong signal, producing high-quality relational schemas.5.2 Schemas EvaluationIn our schema evaluation, we are interested inassessing how well the schemas correspond tocommon-sense knowledge about real world events.To this end, we focus on three measures, topical co-herence, tuple validity, and actor coherence.A good schema must be topically coherent, i.e.,the relations and actors should relate to some realworld topic or event.
The tuples that comprise aschema should be valid assertions that make sensein the real world.
Finally, each actor in the schemashould belong to a cohesive set that plays a consis-tent role in the relations.
Since there are no goodautomated ways to make such judgments, we per-form a human evaluation using workers from Ama-zon?s Mechanical Turk (AMT).We compare Rel-grams schemas against the state-of-the-art narrative schemas released by Cham-bers (Chambers and Jurafsky, 2009).8 Chambers?8Available at http://www.usna.edu/Users/cs/System Id A1 Rel A2Relgrams R11 bomb explode in citymissile explode in citygrenade explode in city... ... ...Relgrams R21 missile explode in citymissile explode in neighborhoodmissile explode in front... ... ...Table 6: A schema instantiation used to test for actor co-herence.
Each of the top instances for A1 or A2 is pre-sented, holding the relation and the other actor fixed.schemas are less expressive than ours ?
they do notassociate types with actors and each schema has aconstant pre-specified number of relations.
For afair comparison we use a similarly expressive ver-sion of our schemas that strips off argument typesand has the same number of relations per schema(six) as their highest quality output set.5.2.1 Evaluation DesignWe created two tasks for AMT annotators.
Thefirst task tests the coherence and validity of rela-tions in a schema and the second does the samefor the schema actors.
In order to make the tasksunderstandable to unskilled AMT workers, we fol-lowed the accepted practice of presenting them withgrounded instances of the schemas (Wang et al2013), e.g., instantiating a schema with a specific ar-gument instead of showing the various possibilitiesfor an actor.First, we collect the information in schemas as aset of tuples: S = {T1, T2, ?
?
?
, Tn}, where each tu-ple is of the form T : (X,Rel, Y ), which conveysa relationship Rel between actors X and Y .
Eachactor is represented by its highest frequency exam-ples (instances).
Table 4 shows examples of schemasfrom Chambers and Rel-grams represented in thisformat.
Then, we create grounded tuples by ran-domly sampling from top instances for each actor.Task I: Topical Coherence To test whether the re-lations in a schema form a coherent topic or event,we presented the AMT annotators with a schema asa set of grounded tuples, showing each relation inthe schema, but randomly selecting one of the top 5instances from each actor.
We generated five suchnchamber/data/schemas/acl091727Figure 3: (a) Has Topic: Percentage of schema instanti-ations with a coherent topic.
(b) Valid Tuples: Percent-age of grounded statements that assert valid real-worldrelations.
(c) Valid + On Topic: Percentage of groundedstatements where 1) the instantiation has a coherent topic,2) the tuple is valid and 3) the relation belongs to thecommon topic.
All differences are statistically significantwith a p-value < 0.01.instantiations for each schema.
An example instan-tiation is shown in Table 5.We ask three kinds of questions on each groundedschema: (1) is each of the grounded tuples valid (i.e.meaningful in the real world); (2) do the majority ofrelations form a coherent topic; and (3) does eachtuple belong to the common topic.
Similar to pre-vious AMT studies we get judgments from multiple(five) annotators on each task and use the majoritylabels (Snow et al 2008).Our instructions specified that the annotatorsshould ignore grammar and focus on whether a tuplemay be interpreted as a real world statement.
For ex-ample, the first tuple in R1 in Table 5 is a valid state-ment ?
?a bomb exploded in a city?, but the tuplesin C1 ?a blast exploded a child?, ?a child detonateda blast?, and ?a child planted a blast?
don?t makesense.Task II: Actor Coherence To test whether the in-stances of an actor form a coherent set, we held therelation and one actor fixed and presented the AMTannotators with the top 5 instances for the other ac-tor.
The first example R11 in Table 6 holds therelation ?explode in?
fixed, and A2 is grounded tothe randomly selected instance ?city?.
We presentgrounded tuples by varying A1 and ask annotators tojudge whether these instances form a coherent topicand whether each instance belongs to that commontopic.
As with Task I, we create five random instan-tiations for each schema.Figure 4: Actor Coherence: Has Role bars compare thepercentage of tuples where the tested actors have a co-herent role.
Fits Role compares the percentage of topinstances that fit the specified role for the tested actors.All differences are statistically significant with a p-value< 0.01.5.2.2 ResultsWe obtained a test set of 100 schemas per systemby randomly sampling from the top 500 schemasfrom each system.
We evaluate this test set usingTask I and II as described above.
For both tasks weobtained ratings from five turkers and use the major-ity labels as the final annotation.Does the schema belong to a single topic?
TheHas Topic bars in Figure 3 show results for schemacoherence.
Rel-grams has a higher proportion ofschemas with a coherent topic, 91% compared to82% for Chambers?.
This is a 53% reduction in in-coherent schemas.Do tuples assert valid real-world relations?
TheValid Tuples bars in Figure 3 compare the percent-age of valid grounded tuples in the schema instan-tiations.
A tuple was labeled valid if a majority ofthe annotators labeled it to be meaningful in the realworld.
Here we see a dramatic difference ?
Rel-grams have 92% valid tuples, compared with Cham-bers?
61%.What proportion of tuples belong?
The Valid +On Topic bars in Figure 3 compare the percentageof tuples that are both valid and on topic, i.e., fitsthe main topic of the schema.
Tuples from schemainstantiations that did not have a coherent topic werelabeled incorrect.Rel-grams have a higher proportion of valid tu-ples belonging to a common topic, 82% compared to172858% for Chambers?
schemas, a 56% error reduction.This is the strictest of the experiments described thusfar ?
1) the schema must have a topic, 2) the tuplemust be valid, and 3) the tuple must belong to thetopic.Do actors represent a coherent set of argu-ments?
We evaluated schema actors from the top25 schemas in Chambers?
and Rel-grams schemas,using grounded instances such as those in Table 6.Figure 4 compares the percentage of tuples wherethe actors play a coherent role (Has Role), and thepercentage of instances that fit that role for the actor(Fits Role).
Rel-grams has much higher actor co-herence than Chambers?, with 97% judged to have atopic compared to 81%, and 81% of instances fittingthe common role compared with Chambers?
59%.5.2.3 Error AnalysisThe errors in both our schemas and those ofChambers are primarily due to mismatched actorsand from extraction errors, although Chambers?schemas have a larger number of actor mismatch er-rors and the cause of the errors is different for eachsystem.Examining the data published by Chambers, themain source of invalid tuples are mismatch of sub-ject and object for a given relation, which accountsfor 80% of the invalid tuples.
We hypothesize thatthis is due to the pair-wise representation that treatssubject-verb and verb-object separately, causing in-consistent s-v-o tuples.
An example is (boiler, light,candle) where (boiler, light) and (light, candle) arewell-formed, yet the entire tuple is not.
In addition,43% of the invalid tuples seem to be from errors bythe dependency parser.Our schemas also suffer from mismatched actors,despite the semantic typing of the actors ?
we founda mismatch in 56% of the invalid tuples (5% ofall tuples).
A general type such as <person> or<organization> may still have an instance that doesnot play the same role as other instances.
For exam-ple a relation (A1, graduated from, A2) has A2 thatis mostly school names, but also includes ?church?which leads to an invalid tuple.Extraction errors account for 47% of the invalidtuples in our schemas, primarily errors that truncatean n-ary relation as a binary tuple.
For example, thesentence ?Mr.
Diehl spends more time ... than thecommissioner?
is misanalysed by the Open IE ex-tractor as (Mr. Diehl, spend than, commissioner).6 Related WorkPrior work by Chambers and Jurafsky (2008;2009; 2010) showed that event sequences (narrativechains) mined from text can be used to induce eventschemas in a domain-independent fashion.
How-ever, our manual evaluation of their output showedkey limitations which may limit applicability.As pointed out earlier, a major weakness inChambers?
approach is the pair-wise representationof subject-verb and verb-object.
Also, their releaseda set of schemas are limited to two actors, althoughthis number can be increased by setting a chain split-ting parameter.Chambers and Jurafsky (2011) extended schemageneration to learn domain-specific event templatesand associated extractors.
In work parallel to ours,Cheung et al(2013), developed a probabilistic so-lution for template generation.
However, their ap-proach requires performing joint probability estima-tion using EM, which can limit scaling to large cor-pora.In this work we developed an Open IE basedsolution to generate schemas.
Following priorwork (Balasubramanian et al 2012), we use OpenIE triples for modeling relation co-occurrence.
Weextend the triple representation with semantic typesfor arguments to alleviate sparisty and to improvecoherence.
We developed a page rank based schemainduction algorithm which results in more coherentschemas with several actors.
Unlike Chambers?
ap-proach this method does not require explicit param-eter tuning for controlling the number of actors.While our event schemas are close to being tem-plates (because of associated types, and actor clus-tering), they do not have associated extractors.
Ourfuture work will focus on building extractors forthese.
It will also be interesting to compare withCheung?s system on smaller focused corpora.Defining representations for events is a topic ofactive interest (Fokkens et al 2013).
In this work,we use a simpler representation, defining eventschemas as a set of actors with associated types anda set of roles they play.17297 ConclusionsWe present a system for inducing event schemasfrom text corpora based on Rel-grams, a languagemodel derived from co-occurrence statistics of re-lational triples (Arg1, Relation, Arg2) extracted bya state-of-the-art Open IE system.
By using triplesrather than a pair-wise representation of subject-verband verb-object, we achieve more coherent schemasthan Chambers and Jurafsky (2009).
In particular,our schemas have higher topic coherence (92% com-pared to Chambers?
82%; make a higher percentageof valid assertions (94% compared with 61%); andhave greater actor coherence (81% compared with59%).Our schemas are also more expressive than thosepublished by Chambers ?
we have semantic typingfor the actors, we are not limited to two actors perschema, and our relation phrases include preposi-tions and are thus more precise and have higher cov-erage of actors involved in the event.Our future plans are to build upon our eventschemas to create an open-domain event extractor.This will extend each induced schema to have asso-ciated extractors.
These extractors will operate on adocument and instantiate an instance of the schema.We have created a Rel-grams database with 1.1Mentries and a set of over 2K event schemas from acorpus of 1.8M New York Times articles.
Both arefreely available to the research community9 and mayprove useful for a wide range of NLP applications.AcknowledgmentsWe thank the anonymous reviewers, Tony Fader, andJanara Christensen for their valuable feedback.
Thispaper was supported by Office of Naval Research(ONR) grant number N00014-11-1-0294, Army Re-search Office (ARO) grant number W911NF-13-1-0246, Intelligence Advanced Research ProjectsActivity (IARPA) via Air Force Research Lab-oratory (AFRL) contract number FA8650-10-C-7058, and Defense Advanced Research ProjectsAgency (DARPA) via AFRL contract number AFRLFA8750-13-2-0019.
The U.S. Government is autho-rized to reproduce and distribute reprints for Gov-ernmental purposes notwithstanding any copyrightannotation thereon.
The views and conclusions con-tained herein are those of the authors and should9available at http://relgrams.cs.washington.edunot be interpreted as necessarily representing the of-ficial policies or endorsements, either expressed orimplied, of ONR, ARO, IARPA, AFRL, or the U.S.Government.ReferencesEneko Agirre and Aitor Soroa.
2009.
Personalizingpagerank for word sense disambiguation.
In 12th Con-ference of the European Chapter of the Association forComputational Linguistics (EACL), pages 33?41.ARPA.
1991.
Proc.
3rd Message Understanding Conf.Morgan Kaufmann.ARPA.
1998.
Proc.
7th Message Understanding Conf.Morgan Kaufmann.Niranjan Balasubramanian, Stephen Soderland, Oren Et-zioni, et al2012.
Rel-grams: a probabilistic modelof relations in text.
In Proceedings of the Joint Work-shop on Automatic Knowledge Base Construction andWeb-scale Knowledge Extraction, pages 101?105.
As-sociation for Computational Linguistics.Sergey Brin and Lawrence Page.
1998.
The anatomy of alarge-scale hypertextual web search engine.
ComputerNetworks, 30(1-7):107?117.N.
Chambers and D. Jurafsky.
2008.
Unsupervisedlearning of narrative event chains.
In Proceedings ofACL-08: HLT.N.
Chambers and D. Jurafsky.
2009.
Unsupervisedlearning of narrative schemas and their participants.
InProceedings of ACL.N.
Chambers and D. Jurafsky.
2010.
A database of nar-rative schemas.
In Proceedings of LREC.N.
Chambers and D. Jurafsky.
2011.
Template-basedinformation extraction without the templates.
In Pro-ceedings of ACL.J.
Cheung, H. Poon, and L. Vandervende.
2013.
Prob-abilistic frame induction.
In Proceedings of NAACLHLT.I.
Dagan, O. Glickman, and B. Magnini.
2005.
ThePASCAL Recognising Textual Entailment Challenge.Proceedings of the PASCAL Challenges Workshop onRecognising Textual Entailment, pages 1?8.G.
Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,S.
Strassel, , and R. Weischedel.
2004.
The auto-matic content extraction (ACE) program-tasks, data,and evaluation.
In Procs.
of LREC.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informationinto information extraction systems by gibbs sampling.In Proceedings of the 43rd Annual Meeting on Associ-ation for Computational Linguistics, pages 363?370.Association for Computational Linguistics.1730Antske Fokkens, Marieke van Erp, Piek Vossen, SaraTonelli, Willem Robert van Hage, BV SynerScope,Luciano Serafini, Rachele Sprugnoli, and Jesper Hoek-sema.
2013.
Gaf: A grounded annotation frameworkfor events.
NAACL HLT 2013, page 11.Taher H. Haveliwala.
2002.
Topic-sensitive pagerank.
InWWW, pages 517?526.Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.2011.
Narrative schema as world knowledge forcoreference resolution.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 86?92.
Associ-ation for Computational Linguistics.Heeyoung Lee, Angel Chang, Yves Peirsman, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2013.Deterministic coreference resolution based on entity-centric, precision-ranked rules.
Computational Lin-guistics, (Just Accepted):1?54.Mausam, Stephen Soderland, Oren Etzioni, Daniel Weld,Kobi Reiter, Michael Skinner, Marcus Sammer, andJeff Bilmes.
2010.
Panlingual lexical translation viaprobabilistic inference.
Artificial Intelligence Journal(AIJ).Mausam, Michael Schmitz, Robert Bart, Stephen Soder-land, and Oren Etzioni.
2012.
Open language learningfor information extraction.
In Proceedings of EMNLP.K.
Owczarzak and H.T.
Dang.
2010.
Overview of the tac2010 summarization track.S.
Patwardhan and E. Riloff.
2009.
A unified model ofphrasal and sentential evidence for information extrac-tion.
In Proceedings of EMNLP 2009.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In Proceedings of the conference on empiricalmethods in natural language processing, pages 254?263.
Association for Computational Linguistics.A.
Wang, C.D.V.
Hoang, and M-Y.
Kan. 2013.
Perspec-tives on crowdsourcing annotations for Natural Lan-guage Processing.
Language Resources and Evalua-tion, 47:9?31.1731
