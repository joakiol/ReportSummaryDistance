Proceedings of ACL-08: HLT, pages 121?129,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsGrounded Language Modeling forAutomatic Speech Recognition of Sports VideoMichael FleischmanMassachusetts Institute of TechnologyMedia Laboratorymbf@mit.eduDeb RoyMassachusetts Institute of TechnologyMedia Laboratorydkroy@media.mit.eduAbstractGrounded language models represent the rela-tionship between words and the non-linguisticcontext in which they are said.
This paper de-scribes how they are learned from large cor-pora of unlabeled video, and are applied to thetask of automatic speech recognition of sportsvideo.
Results show that grounded languagemodels improve perplexity and word errorrate over text based language models, and fur-ther, support video information retrieval betterthan human generated speech transcriptions.1 IntroductionRecognizing speech in broadcast video is a neces-sary precursor to many multimodal applicationssuch as video search and summarization (Snoekand Worring, 2005;).
Although performance isoften reasonable in controlled environments (suchas studio news rooms), automatic speech recogni-tion (ASR) systems have significant difficulty innoisier settings (such as those found in live sportsbroadcasts) (Wactlar et al, 1996).
While manyresearches have examined how to compensate forsuch noise using acoustic techniques, few haveattempted to leverage information in the visualstream to improve speech recognition performance(for an exception see Murkherjee and Roy, 2003).In many types of video, however, visual contextcan provide valuable clues as to what has beensaid.
For example, in video of Major LeagueBaseball games, the likelihood of the phrase ?homerun?
increases dramatically when a home run hasactually been hit.
This paper describes a methodfor incorporating such visual information in anASR system for sports video.
The method is basedon the use of grounded language models to repre-sent the relationship between words and the non-linguistic context to which they refer (Fleischmanand Roy, 2007).Grounded language models are based on re-search from cognitive science on grounded modelsof meaning.
(for a review see Roy, 2005, and Royand Reiter, 2005).
In such models, the meaning ofa word is defined by its relationship to representa-tions of the language users?
environment.
Thus,for a robot operating in a laboratory setting, wordsfor colors and shapes may be grounded in the out-puts of its computer vision system (Roy & Pent-land, 2002); while for a simulated agent operatingin a virtual world, words for actions and eventsmay be mapped to representations of the agent?splans or goals (Fleischman & Roy, 2005).This paper extends previous work on groundedmodels of meaning by learning a grounded lan-guage model from naturalistic data collected frombroadcast video of Major League Baseball games.A large corpus of unlabeled sports videos is col-lected and paired with closed captioning transcrip-tions of the announcers?
speech.
1  This corpus isused to train the grounded language model, whichlike traditional language models encode the priorprobability of words for an ASR system.
Unliketraditional language models, however, groundedlanguage models represent the probability of aword conditioned not only on the previous word(s),but also on features of the non-linguistic context inwhich the word was uttered.Our approach to learning grounded languagemodels operates in two phases.
In the first phase,events that occur in the video are represented usinghierarchical temporal pattern automatically mined1Closed captioning refers to human transcriptions of speechembedded in the video stream primarily for the hearing im-paired.
Closed captioning is reasonably accurate (although notperfect) and available on some, but not all, video broadcasts.121Figure 1.
Representing events in video.
a) Events are represented by first abstracting the raw video into visual con-text, camera motion, and audio context features.
b) Temporal data mining is then used to discover hierarchical tem-poral patterns in the parallel streams of features.
c) Temporal patterns found significant in each iteration are storedin a codebook that is used to represent high level events in video.from low level features.
In the second phase, aconditional probability distribution is estimatedthat describes the probability that a word was ut-tered given such event representations.
In the fol-lowing sections we describe these two aspects ofour approach and evaluate the performance of ourgrounded language model on a speech recognitiontask using video highlights from Major LeagueBaseball games.
Results indicate improved per-formance using three metrics: perplexity, worderror rate, and precision on an information retrievaltask.2 Representing Events in Sports VideoRecent work in video surveillance has demon-strated the benefit of representing complex eventsas temporal relations between lower level sub-events (Hongen et al, 2004).
Thus, to representevents in the sports domain, we would ideally firstrepresent the basic sub events that occur in sportsvideo (e.g., hitting, throwing, catching, running,etc.)
and then build up complex events (such ashome run) as a set of temporal relations betweenthese basic events.
Unfortunately, due to the limi-tations of computer vision techniques, reliablyidentifying such basic events in video is not feasi-ble.
However, sports video does have characteris-tics that can be exploited to effectively representcomplex events.Like much broadcast video, sports video ishighly produced, exploiting many different cameraangles and a human director who selects whichcamera is most appropriate given what is happen-ing on the field.
The styles that different directorsemploy are extremely consistent within a sport andmake up a ?language of film?
which the machinecan take advantage of in order to represent theevents taking place in the video.Thus, even though it is not easy to automati-cally identify a player hitting a ball in video, it iseasy to detect features that correlate with hitting,e.g., when a scene focusing on the pitching moundimmediately jumps to one zooming in on the field(see Figure 1).
Although these correlations are notperfect, experiments have shown that baseballevents can be classified using such features(Fleischman et al, 2007).We exploit the language of film to representevents in sports video in two phases.
First, lowlevel features that correlate with basic events insports are extracted from the video stream.
Then,temporal data mining is used to find patternswithin this low level event stream.2.1 Feature ExtractionWe extract three types of features: visual con-text features, camera motion features, and audiocontext features.122Visual Context FeaturesVisual context features encode general proper-ties of the visual scene in a video segment.
Super-vised classifiers are trained to identify thesefeatures, which are relatively simple to classify incomparison to high level events (like home runs)that require more training data and achieve loweraccuracy.
The first step in classifying visual con-text features is to segment the video into shots (orscenes) based on changes in the visual scene due toediting (e.g.
jumping from a close up to a wideshot of the field).
Shot detection and segmentationis a well studied problem; in this work we use themethod of Tardini et al (2005).After the video is segmented into shots, indi-vidual frames (called key frames) are selected andrepresented as a vector of low level features thatdescribe the key frame?s color distribution, en-tropy, etc.
(see Fleischman and Roy, 2007 for thefull list of low level features used).
The WEKAmachine learning package is used to train a boosteddecision tree to classify these frames into one ofthree categories: pitching-scene, field-scene, other(Witten and Frank, 2005).
Those shots whose keyframes are classified as field-scenes are then sub-categorized (using boosted decision trees) into oneof the following categories: infield, outfield, wall,base, running, and misc.
Performance of theseclassification tasks is approximately 96% and 90%accuracy respectively.Camera Motion FeaturesIn addition to visual context features, we alsoexamine the camera motion that occurs within avideo.
Unlike visual context features, which pro-vide information about the global situation that isbeing observed, camera motion features representmore precise information about the actions occur-ring in a video.
The intuition here is that the cam-era is a stand in for a viewer?s focus of attention.As actions occur in a video, the camera moves tofollow it; this camera motion thus mirrors the ac-tions themselves, providing informative featuresfor event representation.Like shot boundary detection, detecting the mo-tion of the camera in a video (i.e., the amount itpans left to right, tilts up and down, and zooms inand out) is a well-studied problem.
We use thesystem of Bouthemy et al (1999) which computesthe camera motion using the parameters of a two-dimensional affine model to fit every pair of se-quential frames in a video.
A 15 state 1st orderHidden Markov Model, implemented with theGraphical Modeling Toolkit,2 then converts theoutput of the Bouthemy system into a stream ofclustered characteristic camera motions (e.g.
state12 clusters together motions of zooming in fastwhile panning slightly left).Audio ContextThe audio stream of a video can also provide use-ful information for representing non-linguistic con-text.
We use boosted decision trees to classifyaudio into segments of speech, excited_speech,cheering, and music.
Classification operates on asequence of overlapping 30 ms frames extractedfrom the audio stream.
For each frame, a featurevector is computed using, MFCCs (often used inspeaker identification and speech detection tasks),as well as energy, the number of zero crossings,spectral entropy, and relative power between dif-ferent frequency bands.
The classifier is applied toeach frame, producing a sequence of class labels.These labels are then smoothed using a dynamicprogramming cost minimization algorithm (similarto those used in Hidden Markov Models).
Per-formance of this system achieves between 78%and 94% accuracy.2.2 Temporal Pattern MiningGiven a set of low level features that correlate withthe basic events in sports, we can now focus onbuilding up representations of complex events.Unlike previous work (Hongen et al, 2005) inwhich representations of the temporal relationsbetween low level events are built up by hand, weemploy temporal data mining techniques to auto-matically discover such relations from a large cor-pus of unannotated video.As described above, ideal basic events (such ashitting and catching) cannot be identified easily insports video.
By finding temporal patterns betweenaudio, visual and camera motion features, how-ever, we can produce representations that arehighly correlated with sports events.
Importantly,such temporal patterns are not strictly sequential,but rather, are composed of features that can occur2http://ssli.ee.washington.edu/~bilmes/gmtk/123in complex and varied temporal relations to eachother.To find such patterns automatically, we followprevious work in video content classification inwhich temporal data mining techniques are used todiscover event patterns within streams of lowerlevel features.
The algorithm we use is fully unsu-pervised and proceeds by examining the relationsthat occur between features in multiple streamswithin a moving time window.
Any two featuresthat occur within this window must be in one ofseven temporal relations with each other (e.g.
be-fore, during, etc.)
(Allen, 1984).
The algorithmkeeps track of how often each of these relations isobserved, and after the entire video corpus is ana-lyzed, uses chi-square analyses to determine whichrelations are significant.
The algorithm iteratesthrough the data, and relations between individualfeatures that are found significant in one iteration(e.g.
[OVERLAP, field-scene, cheer]), are them-selves treated as individual features in the next.This allows the system to build up higher-ordernested relations in each iteration (e.g.
[BEFORE,[OVERLAP, field-scene, cheer], field scene]]).The temporal patterns found significant in thisway make up a codebook which can then be usedas a basis for representing a video.
The term code-book is often used in image analysis to describe aset of features (stored in the codebook) that areused to encode raw data (images or video).
Suchcodebooks are used to represent raw video usingfeatures that are more easily processed by thecomputer.Our framework follows a similar approach inwhich raw video is encoded (using a codebook oftemporal patterns) as follows.
First, the raw videois abstracted into the visual context, camera mo-tion, and audio context feature streams (as de-scribed in Section 2.1).
These feature streams arethen scanned, looking for any temporal patterns(and nested sub-patterns) that match those found inthe codebook.
For each pattern, the duration forwhich it occurs in the feature streams is treated asthe value of an element in the vector representationfor that video.Thus, a video is represented as an n length vec-tor, where n is the total number of temporal pat-terns in the codebook.
The value of each elementof this vector is the duration for which the patternassociated with that element was observed in thevideo.
So, if a pattern was not observed in a videoat all, it would have a value of 0, while if it wasobserved for the entire length of the video, it wouldhave a value equal to the number of frames presentin that video.Given this method for representing the non-linguistic context of a video, we can now examinehow to model the relationship between such con-text and the words used to describe it.3 Linguistic MappingModeling the relationship between words and non-linguistic context assumes that the speech utteredin a video refers consistently (although not exclu-sively) to the events being represented by the tem-poral pattern features.
We model this relationship,much like traditional language models, using con-ditional probability distributions.
Unlike tradi-tional language models, however, our groundedlanguage models condition the probability of aword not only on the word(s) uttered before it, butalso on the temporal pattern features that describethe non-linguistic context in which it was uttered.We estimate these conditional distributions using aframework similar that used for training acousticmodels in ASR and translation models in MachineTranslation (MT).We generate a training corpus of utterancespaired with representations of the non-linguisticcontext in which they were uttered.
The first stepin generating this corpus is to generate the lowlevel features described in Section 2.1 for eachvideo in our training set.
We then segment eachvideo into a set of independent events based on thevisual context features we have extracted.
We fol-low previous work in sports video processing(Gong et al, 2004) and define an event in a base-ball video as any sequence of shots starting with apitching-scene and continuing for four subsequentshots.
This definition follows from the fact that thevast majority of events in baseball start with apitch and do not last longer than four shots.
Foreach of these events in our corpus, a temporal pat-tern feature vector is generated as described in sec-tion 2.2.
These events are then paired with all thewords from the closed captioning transcription thatoccur during each event (plus or minus 10 sec-onds).
Because these transcriptions are not neces-sarily time synched with the audio, we use themethod described in Hauptmann and Witbrock124(1998) to align the closed captioning to the an-nouncers?
speech.Previous work has examined applying modelsoften used in MT to the paired corpus describedabove (Fleischman and Roy, 2006).
Recent workin automatic image annotation (Barnard et al,2003; Blei and Jordan, 2003) and natural languageprocessing (Steyvers et al, 2004), however, havedemonstrated the advantages of using hierarchicalBayesian models for related tasks.
In this work wefollow closely the Author-Topic (AT) model (Stey-vers et al, 2004) which is a generalization of La-tent Dirichlet Allocation (LDA) (Blei et al, 2005).3LDA is a technique that was developed tomodel the distribution of topics discussed in a largecorpus of documents.
The model assumes thatevery document is made up of a mixture of topics,and that each word in a document is generatedfrom a probability distribution associated with oneof those topics.
The AT model generalizes LDA,saying that the mixture of topics is not dependenton the document itself, but rather on the authorswho wrote it.
According to this model, for eachword (or phrase) in a document, an author is cho-sen uniformly from the set of the authors of thedocument.
Then, a topic is chosen from a distribu-tion of topics associated with that particular author.Finally, the word is generated from the distributionassociated with that chosen topic.
We can expressthe probability of the words in a document (W)given its authors (A) as:?
???
?
?=Wm Ax TzdxzpzmpAAWp )|()|(1)|(  (1)where T is the set of latent topics that are inducedgiven a large set of training data.We use the AT model to estimate our groundedlanguage model by making an analogy betweendocuments and events in video.
In our framework,the words in a document correspond to the wordsin the closed captioning transcript associated withan event.
The authors of a document correspond tothe temporal patterns representing the non-linguistic context of that event.
We modify the ATmodel slightly, such that, instead of selecting from3In the discussion that follows, we describe a method for es-timating unigram grounded language models.
Estimatingbigram and trigram models can be done by processing onword pairs or triples, and performing normalization on theresulting conditional distributions.a uniform distribution (as is done with authors ofdocuments), we select patterns from a multinomialdistribution based upon the duration of the pattern.The intuition here is that patterns that occur for alonger duration are more salient and thus, shouldbe given greater weight in the generative process.We can now rewrite (1) to give the probability ofwords during an event (W) given the vector of ob-served temporal patterns (P) as:????
?
?=Wm Px TzxpxzpzmpPWp )()|()|()|(  (2)In the experiments described below we followSteyver et al, (2004) and train our AT model usingGibbs sampling, a Markov Chain Monte Carlotechnique for obtaining parameter estimates.
Werun the sampler on a single chain for 200 iterations.We set the number of topics to 15, and normalizethe pattern durations first by individual patternacross all events, and then for all patterns within anevent.
The resulting parameter estimates aresmoothed using a simple add N smoothing tech-nique, where N=1 for the word by topic counts andN=.01 for the pattern by topic counts.4 EvaluationIn order to evaluate our grounded language model-ing approach, a parallel data set of 99 MajorLeague Baseball games with corresponding closedcaptioning transcripts was recorded from live tele-vision.
These games represent data totaling ap-proximately 275 hours and 20,000 distinct eventsfrom 25 teams in 23 stadiums, broadcast on fivedifferent television stations.
From this set, sixgames were held out for testing (15 hours, 1200events, nine teams, four stations).
From this testset, baseball highlights (i.e., events which termi-nate with the player either out or safe) were handannotated for use in evaluation, and manually tran-scribed in order to get clean text transcriptions forgold standard comparisons.
Of the 1200 events inthe test set, 237 were highlights with a total wordcount of 12,626 (vocabulary of 1800 words).The remaining 93 unlabeled games are used totrain unigram, bigram, and trigram grounded lan-guage models.
Only unigrams, bigrams, and tri-grams that are not proper names, appear greaterthan three times, and are not composed only ofstop words were used.
These grounded languagemodels are then combined in a backoff strategy125with traditional unigram, bigram, and trigram lan-guage models generated from a combination of theclosed captioning transcripts of all training gamesand data from the switchboard corpus (see below).This backoff is necessary to account for the wordsnot included in the grounded language model itself(i.e.
stop words, proper names, low frequencywords).
The traditional text-only language models(which are also used below as baseline compari-sons) are generated with the SRI language model-ing toolkit (Stolcke, 2002) using Chen andGoodman's modified Kneser-Ney discounting andinterpolation (Chen and Goodman, 1998).
Thebackoff strategy we employ here is very simple: ifthe ngram appears in the GLM then it is used, oth-erwise the traditional LM is used.
In future workwe will examine more complex backoff strategies(Hsu, in review).We evaluate our grounded language modelingapproach using 3 metrics: perplexity, word errorrate, and precision on an information retrieval task.4.1 PerplexityPerplexity is an information theoretic measure ofhow well a model predicts a held out test set.
Weuse perplexity to compare our grounded languagemodel to two baseline language models: a lan-guage model generated from the switchboard cor-pus, a commonly used corpus of spontaneousspeech in the telephony domain (3.65M words; 27kvocab); and a language model that interpolates(with equal weight given to both) between theswitchboard model and a language model trainedonly on the baseball-domain closed captioning(1.65M words; 17k vocab).
The results of calculat-ing perplexity on the test set highlights for thesethree models is presented in Table 1 (lower is bet-ter).Not surprisingly, the switchboard languagemodel performs far worse than both the interpo-lated text baseline and the grounded languagemodel.
This is due to the large discrepancy be-tween both the style and vocabulary of languageabout sports compared to the domain of telephonysampled by the switchboard corpus.
Of more in-terest is the decrease in perplexity seen when usingthe grounded language model compared to the in-terpolated model.
Note that these two languagemodels are generated using the same speech tran-scriptions, i.e.
the closed captioning from the train-ing games and the switchboard corpus.
However,whereas the baseline model remains the same foreach of the 237 test highlights, the grounded lan-guage model generates different word distributionsfor each highlight depending on the event featuresextracted from the highlight video.Switchboard Interpolated(Switch+CC)Groundedppl 1404 145.27 83.88Table 1.
Perplexity measures for three different lan-guage models on a held out test set of baseball high-lights (12,626 words).
We compare the groundedlanguage model to two text based language models: onetrained on the switchboard corpus alone; and interpo-lated with one trained on closed captioning transcrip-tions of baseball video.4.2 Word Accuracy and Error RateWord error rate (WER) is a normalized measure ofthe number of word insertions, substitutions, anddeletions required to transform the output tran-scription of an ASR system to a human generatedgold standard transcription of the same utterance.Word accuracy is simply the number of words inthe gold standard that they system correctly recog-nized.
Unlike perplexity which only evaluates theperformance of language models, examining wordaccuracy and error rate requires running an entireASR system, i.e.
both the language and acousticmodels.We use the Sphinx system to train baseball specificacoustic models using parallel acoustic/text dataautomatically mined from our training set.
Follow-ing Jang and Hauptman (1999), we use an off theshelf acoustic model (the hub4 model) to generatean extremely noisy speech transcript of each gamein our training set, and use dynamic programmingto align these noisy outputs to the closed caption-ing stream for those same games.
Given these twotranscriptions, we then generate a paired acous-tic/text corpus by sampling the audio at the timecodes where the ASR transcription matches theclosed captioning transcription.For example, if the ASR output contains theterm sequence ??
and farther home run for Davidforty says??
and the closed captioning containsthe sequence ?
?another home run for DavidOrtiz?,?
the matched phrase ?home run forDavid?
is assumed a correct transcription for theaudio at the time codes given by the ASR system.Only looking at sequences of three words or more,12676.680.389.6707580859095switchboard interpolated groundedWordErrorRate(WER)31.325.415.105101520253035switchboard interpolated groundedWordAccuracy(%)Figure 3.
Word accuracy and error rates for ASR sys-tems using a grounded language model, a text basedlanguage model trained on the switchboard corpus, andthe switchboard model interpolated with a text basedmodel trained on baseball closed captions.we extract approximately 18 hours of clean paireddata from our 275 hour training corpus.
A con-tinuous acoustic model with 8 gaussians and 6000ties states is trained on this data using the Sphinxspeech recognizer.4Figure 3 shows the WERs and accuracy forthree ASR systems run using the Sphinx decoderwith the acoustic model described above and eitherthe grounded language model or the two baselinemodels described in section 4.1.
Note that per-formance for all of these systems is very poor dueto limited acoustic data and the large amount ofbackground crowd noise present in sports video(and particularly in sports highlights).
Even withthis noise, however, results indicate that the wordaccuracy and error rates when using the groundedlanguage model is significantly better than both theswitchboard model (absolute WER reduction of13%; absolute accuracy increase of 15.2%) and theswitchboard interpolated with the baseball specifictext based language model (absolute WER reduc-tion of 3.7%; absolute accuracy increase of 5.9%).4http://cmusphinx.sourceforge.net/html/cmusphinx.phpDrawing conclusions about the usefulness ofgrounded language models using word accuracy orerror rate alone is difficult.
As it is defined, thesemeasures penalizes a system that mistakes ?a?
for?uh?
as much as one that mistakes ?run?
for ?rum.
?When using ASR to support multimedia applica-tions (such as search), though, such substitutionsare not of equal importance.
Further, while visualinformation may be useful for distinguishing thelatter error, it is unlikely to assist with the former.Thus, in the next section we examine an extrinsicevaluation in which grounded language models arejudged not directly on their effect on word accu-racy or error rate, but based on their ability to sup-port video information retrieval.4.3 Precision of Information RetrievalOne of the most commonly used applications ofASR for video is to support information retrieval(IR).
Such video IR systems often use speech tran-scriptions to index segments of video in much thesame way that words are used to index text docu-ments (Wactlar et al, 1996).
For example, in thedomain of baseball, if a video IR system were is-sued the query ?home run,?
it would typically re-turn a set of video clips by searching its databasefor events in which someone uttered the phrase?home run.?
Because such systems rely on ASRoutput to search video, the performance of a videoIR system gives an indirect evaluation of theASR?s quality.
Further, unlike the case with wordaccuracy or error rate, such evaluations highlight asystems ability to recognize the more relevant con-tent words without being distracted by the morecommon stop words.Our metric for evaluation is the precision withwhich baseball highlights are returned in a videoIR system.
We examine three systems: one thatuses ASR with the grounded language model, abaseline system that uses ASR with the text onlyinterpolated language model, and finally a systemthat uses human produced closed caption transcrip-tions to index events.For each system, all 1200 events from the testset (not just the highlights) are indexed.
Queriesare generated artificially using a method similar toBerger and Lafferty (1999) and used in Fleischmanand Roy (2007).
First, each highlight is labeledwith the event?s type (e.g.
fly ball), the event?s lo-cation (e.g.
left field) and the event?s result (e.g.double play): 13 labels total.
Log likelihood ratios127are then used to find the phrases (unigram, trigram,and bigram) most indicative of each label (e.g.
?flyball?
for category fly ball).
For each label, thethree most indicative phrases are issued as queriesto the system, which ranks its results using the lan-guage modeling approach of Ponte and Croft(1998).
Precision is measured on how many of thetop five returned events are of the correct category.Figure 4 shows the precision of the video IRsystems based on ASR with the grounded languagemodel, ASR with the text-only interpolated lan-guage model, and closed captioning transcriptions.As with our previous evaluations, the IR resultsshow that the system using ASR with the groundedlanguage model performed better than the one us-ing ASR with the text-only language model (5.1%absolute improvement).
More notably, though,Figure 4 shows that the system using the groundedlanguage model performed better than the systemusing the hand generated closed captioning tran-scriptions (4.6% absolute improvement).
Althoughthis is somewhat counterintuitive given that handtranscriptions are typically considered gold stan-dards, these results follow from a limitation of us-ing text-based methods to index video.Unlike the case with text documents, the occur-rence of a query term in a video is often notenough to assume the video?s relevance to thatquery.
For example, when searching throughvideo of baseball games, returning all clips inwhich the phrase ?home run?
occurs, results pri-marily in video of events where a home run doesnot actually occur.
This follows from the fact thatin sports, as in life, people often talk not aboutwhat is currently happening, but rather, they talkabout what did, might, or will happen in the future.By taking into account non-linguistic contextduring speech recognition, the grounded languagemodel system indirectly circumvents some of thesefalse positive results.
This follows from the factthat an effect of using the grounded languagemodel is that when an announcer utters a phrase(e.g., ?fly ball?
), the system is more likely to rec-ognize that phrase correctly if the event it refers tois actually occurring (e.g.
if someone actually hit afly ball).
Because the grounded language modelsystem is biased to recognize phrases that describewhat is currently happening, it returns fewer falsepositives and gets higher precision.0.260.270.280.290.30.310.320.330.340.35ASR-LM CC ASR-GLMPrecisionof Top5Figure 4.
Precision of top five results of a video IR sys-tem based on speech transcriptions.
Three differenttranscriptions are compared: ASR-LM uses ASR with atext-only interpolated language model (trained on base-ball closed captioning and the switchboard corpus);ASR-GLM uses ASR with a grounded language model;CC uses human generated closed captioning transcrip-tions (i.e., no ASR).5 ConclusionsWe have described a method for improving speechrecognition in video.
The method uses groundedlanguage modeling, an extension of tradition lan-guage modeling in which the probability of a wordis conditioned not only on the previous word(s) butalso on the non-linguistic context in which theword is uttered.
Context is represented using hier-archical temporal patterns of low level featureswhich are mined automatically from a large unla-beled video corpus.
Hierarchical Bayesian modelsare then used to map these representations towords.
Initial results show grounded languagemodels improve performance on measures of per-plexity, word accuracy and error rate, and preci-sion on an information retrieval task.In future work, we will examine the ability ofgrounded language models to improve perform-ance for other natural language tasks that exploittext based language models, such as MachineTranslation.
Also, we are examining extendingthis approach to other sports domains such asAmerican football.
In theory, however, our ap-proach is applicable to any domain in which thereis discussion of the here-and-now (e.g., cookingshows, etc.).
In future work, we will examine thestrengths and limitations of grounded languagemodeling in these domains.128ReferencesAllen, J.F.
(1984).
A General Model of Action andTime.
Artificial Intelligence.
23(2).Barnard, K, Duygulu, P, de Freitas, N, Forsyth, D, Blei,D, and Jordan, M. (2003), Matching Words andPictures, Journal of Machine Learning Research,Vol 3.Berger, A. and Lafferty, J.
(1999).
InformationRetrieval as Statistical Translation.
In Proceed-ings of SIGIR-99.Blei, D. and Jordan, M. (2003).
Modeling annotateddata.
Proceedings of the 26th International Confer-ence on Research and Development in InformationRetrieval, ACM Press, 127?134.Blei, D. Ng, A., and Jordan, M (2003).
?Latent Dirichletallocation.?
Journal of Machine Learning Research3:993?1022.Bouthemy, P., Gelgon, M., Ganansia, F. (1999).
A uni-fied approach to shot change detection and cam-era motion characterization.
IEEE Trans.
onCircuits and Systems for Video Technology,9(7).Chen, S. F. and Goodman, J., (1998).
An EmpiricalStudy of Smoothing Techniques for Language Mod-eling, Tech.
Report TR-10-98, Computer ScienceGroup, Harvard U., Cambridge, MA.Fleischman M, Roy, D. (2007).
Situated Models ofMeaning for Sports Video Retrieval.
HLT/NAACL.Rochester, NY.Fleischman, M. and Roy, D. (2007).
Unsupervised Con-tent-Based Indexing of Sports Video Retrieval.
9thACM Workshop on Multimedia Information Retrieval(MIR).
Augsburg, Germany.Fleischman, M. B. and Roy, D.  (2005)  Why Verbs areHarder to Learn than Nouns: Initial Insights from aComputational Model of Intention Recognition inSituated Word Learning.
27th Annual Meeting of theCognitive Science Society, Stresa, Italy.Fleischman, M., DeCamp, P. Roy, D.  (2006).
MiningTemporal Patterns of Movement for Video ContentClassification.
ACM Workshop on Multimedia In-formation Retrieval.Fleischman, M., Roy, B., and Roy, D. (2007).
Tempo-ral Feature Induction for Sports Highlight Classifica-tion.
In Proceedings of ACM Multimedia.Augsburg, Germany.Gong, Y., Han, M., Hua, W., Xu, W.  (2004).
Maximumentropy model-based baseball highlight detection andclassification.
Computer Vision and Image Un-derstanding.
96(2).Hauptmann, A. , Witbrock, M., (1998) Story Segmenta-tion and Detection of Commercials in BroadcastNews Video, Advances in Digital Libraries.Hongen, S., Nevatia, R. Bremond, F. (2004).Video-based event recognition: activity repre-sentation and probabilistic recognition methods.Computer Vision and Image Understanding.96(2).Hsu , Bo-June (Paul).
(in review).
Generalized LinearInterpolation of Language Models.Jang, P., Hauptmann, A.
(1999).
Learning to RecognizeSpeech by Watching Television.
IEEE IntelligentSystems Magazine, 14(5), pp.
51-58.Mukherjee, N. and Roy, D.. (2003).
A Visual Context-Aware Multimodal System for Spoken LanguageProcessing.
Proc.
Eurospeech, 4 pages.Ponte, J.M., and Croft, W.B.
(1998).
A Language Mod-eling Approach to Information Retrieval.
In Proc.
ofSIGIR?98.Roy, D. (2005).
.
Grounding Words in Perception andAction: Insights from Computational Models.
TICS.Roy, D. and Pentland, A.
(2002).
Learning Words fromSights and Sounds: A Computational Model.
Cogni-tive Science, 26(1).Roy.
D. and Reiter, E. (2005).
.
Connecting Language tothe World.
Artificial Intelligence, 167(1-2), 1-12.Snoek, C.G.M.
and Worring, M.. (2005).
Multimodalvideo indexing: A review of the state-of-the-art.Multimedia Tools and Applications, 25(1):5-35.Steyvers, M., Smyth, P., Rosen-Zvi, M., & Griffiths, T.(2004).
Probabilistic Author-Topic Models for In-formation Discovery.
The Tenth ACM SIGKDD In-ternational Conference on Knowledge Discovery andData Mining.
Seattle, Washington.Stolcke, A., (2002).
SRILM - An Extensible LanguageModeling Toolkit, in Proc.
Intl.
Conf.
Spoken Lan-guage Processing, Denver, Colorado.Tardini, G. Grana C., Marchi, R., Cucchiara, R., (2005).Shot Detection and Motion Analysis for AutomaticMPEG-7 Annotation of Sports Videos.
In 13th In-ternational Conference on Image Analysis and Proc-essing.Wactlar, H., Witbrock, M., Hauptmann, A., (1996 ).Informedia: News-on-Demand Experiments inSpeech Recognition.
ARPA Speech RecognitionWorkshop, Arden House, Harriman, NY.Witten, I. and Frank, E. (2005).
Data Mining: Practicalmachine learning tools and techniques.
2nd Edition,Morgan Kaufmann.
San Francisco, CA.129
