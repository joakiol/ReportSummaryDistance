Tagset P.eduction Wi thout  In format ion LossThors ten  BrantsUn ivers i t~t  des Saar landesComputer l ingu is t ikD-66041 Saarbr f i cken,  Germanythorst en~coli, uni- sb.
deAbst ractA technique for reducing a tagset usedfor n-gram part-of-speech disambiguationis introduced and evaluated in an experi-ment.
The technique nsures that all in-formation that is provided by the originaltagset can be restored from the reducedone.
This is crucial, since we are intere-sted in the linguistically motivated tags forpart-of-speech disambiguation.
The redu-ced tagset needs fewer parameters for itsstatistical model and allows more accurateparameter estimation.
Additionally, thereis a slight but not significant improvementof tagging accuracy.1 Mot ivat ionStatistical part-of-speech disambiguation can be ef-ficiently done with n-gram models (Church, 1988;Cutting et al, 1992).
These models are equivalentto Hidden Markov Models (HMMs) (Rabiner, 1989)of order n - 1.
The states represent parts of speech(categories, tags), there is exactly one state for eachcategory, and each state outputs words of a particu-lar category.
The transition and output probabilitiesof the HMM are derived from smoothed frequencycounts in a text corpus.Generally, the categories for part-of-speech tag-ging are linguistically motivated and do not reflectthe probability distributions or co-occurrence pro-babilities of words belonging to that category.
It isan implicit assumption for statistical part-of-speechtagging that words belonging to the same categoryhave similar probability distributions.
But this as-sumption does not hold in many of the cases.Take for example the word cliff which could be aproper (NP) 1 or a common noun (NN) (ignoring ca-pitalization of proper nouns for the moment).
Thetwo previous words are a determiner (AT) and an1All tag names used in this paper are inspired bythose used for the LOB Corpus (Garside et al, 1987).adjective (J J).
The probability of cliff being a com-mon noun is the product of the respective contextualand lexical probabilities p(N N \]AT, JJ) ?
p(c//fflN N),regardless of other information provided by the ac-tual words (a sheer cliff vs. the wise Cliff).
Obvi-ously, information useful for probability estimationis not encoded in the tagset.On the other hand, in some cases information otneeded for probability estimation is encoded in thetagset.
The distributions for comparative and su-perlative forms of adjectives in the Susanne Corpus(Sampson, 1995) are very similar.
The number ofcorrect ag assignments is not affected when we com-bine the two categories.
However, it does not sufficeto assign the combined tag, if we are interested inthe distinction between comparative and superlativeform for further processing.
We have to ensure thatthe original (interesting) tag can be restored.There are two contradicting requirements.
On theone hand, more tags mean that there is more infor-mation about a word at hand, on the other hand,the more tags, the severer the sparse-data problemis and the larger the corpora that are needed fortraining.This paper presents a way to modify a given tag-set, such that categories with similar distributionsin a corpus are combined without losing informationprovided by the original tagset and without losingaccuracy.2 C lus ter ing  o f  TagsThe aim of the presented method is to reduce a tag-set as much as possible by combining (clustering)two or more tags without losing information and wi-thout losing accuracy.
The fewer tags we have, theless parameters have to be estimated and stored, andthe less severe is the sparse data problem.
Incomingtext will be disambiguated with the new reducedtagset, but we ensure that the original tag is stilluniquely ide:.ltified by the new tag.The basic idea is to exploit the fact that some ofthe categories have a very similar frequency distri-bution in a corpus.
If we combine categories with287similar distribution characteristics, there should beonly a small change in the tagging result.
The mainchange is that single tags are replaced by a clusterof tags, from which the original has to be identified.First experiments with tag clustering showed that,even for fully automatic identification of the originaltag, tagging accuracy slightly increased when the re-duced tagset was used.
This might be a result of ha-ving more occurrences per tag for a smaller tagset,and probability estimates are preciser.2.1 Un ique  Ident i f i cat ion  of  Or ig inal  TagsA crucial property of the reduced tagset is that theoriginal tag information can be restored from thenew tag, since this is the information we are intere-sted in.
The property can be ensured if we place aconstraint on the clustering of tags.Let )'V be the set of words, C the set of clusters(i.e.
the reduced tagset), and 7" the original tagset.To restore the original tag from a combined tag (clu-ster), we need a unique functionforia : W x C ~ 7-, (1)To ensure that there is such a unique function,we prohibit some of the possible combinations.
Acluster is allowed if and only if there is no word in thelexicon which can have two or more of the originaltags combined in one cluster.
Formally, seeing tagsas sets of words and clusters as sets of tags:VcEC, tl,t2Ec, t l~t2,wE}/Y: wEt l : :~w~t2(2)If this condition holds, then for all words w taggedwith a cluster e, exactly one tag two fulfillsw E twe A t~.e E c,yieldingfo .
, (w,  c) = t o.So, the original tag can be restored any time and noinformation from the original tagset is lost.Example: Assume that no word in the lexicon canbe both comparative (JJ R) and superlative adjective(JJT).
The categories are combined to {JJR,JJT}.When processing a text, the word easier is taggedas {JJR,JJT}.
Since the lexicon states that easiercan be of category J JR but not of category JJT, theoriginal tag must be J JR.2.2 Cr i te r ia  For Combin ing  TagsThe are several criteria that can determine the qua-lity of a particular clustering.1.
Compare the trigram probabilities p(BIXi , A),P(BIA, Xi), and p(XilA, B), i = 1, 2.
Combinetwo tags X1 and X2, if these probabilities coin-cide to a certain extent.2.
Maximize the probability that the training cor-pus is generated by the HMM which is describedby the trigram probabilities.3.
Maximize the tagging accuracy for a trainingcorpus.Criterion (1) establishes the theoretical basis,while criteria (2) and (3) immediately show the be-nefit of a particular combination.
A measure of si-milarity for (1) is currently under investigation.
Wechose (3) for our first experiments, ince it was theeasiest one to implement.
The only additional ef-fort is a separate, previously unused part of the trai-ning corpus for this purpose, the clustering part.
Wecombine those tags into clusters which give the bestresults for tagging of the clustering part.2.3 The  A lgor i thmThe total number of potential clusterings grows ex-ponential with the size of the tagset.
Since we areinterested in the reduction of large tagsets, a fullsearch regarding all potential clusterings is not fea-sible.
We compute the local maximum which can befound in polynomial time with a best-first search.We use a slight modification of the algorithmused by (Stolcke and Omohundro, 1994) for mergingHMMs.
Our task is very similar to theirs.
Stolckeand Omohundro start with a first order tIMM whereevery state represents a single occurrence of a wordin a corpus, and the goal is to maximize the a po-steriori probability of the model.
We start with asecond order HMM (since we use trigrams) whereeach state represents a part of speech, and our goalis to maximize the tagging accuracy for a corpus.The clustering algorithm works as follows:1.
Compute tagging accuracy for the clusteringpart with the original tagset.2.
Loop:(a) Compute a set of candidate clusters (obey-ing constraint (2) mentioned in section2.1), each consisting of two tags from theprevious tep.
(b) For each candidate cluster build the resul-ting tagset and compute tagging accuracyfor that tagset.
(c) If tagging accuracy decreases for all combi-nations of tags, break from the loop.
(d) Add the cluster which maximized the tag-ging accuracy to the tagset and remove thetwo tags previously used.3.
Output the resulting tagset.2.4 App l i ca t ion  of  Tag C lus ter ingTwo standard trigram tagging procedures wereperformed as the baseline.
Then clustering was per-formed on the same data and tagging was done withthe reduced tagset.
The reduced tagset was only in-ternally used, the output of the tagger consisted ofthe original tagset for all experiments.The Susanne Corpus has about 157,000 words anduses 424 tags (counting tags with indices denoting288Table 1: Tagging results for the test parts in the clustering experiments.
Exp.
1 and 2 are used as thebaseline.Training Clustering Testing Result (known words)1. parts A and B - part C 93.7% correct2.
parts A and C - part B 94.6% correct3.
part A part B part C 93.9% correct4.
part A part C part B 94.7% correctmulti-word lexemes as separate tags).
The tags arebased on the LOB tagset (Garside t al., 1987).Three parts are taken from the corpus.
Part Aconsists of about 127,000 words, part B of about10,000 words, and part C of about 10,000 words.The rest of the corpus, about 10,000 words, is notused for this experiment.
All parts are mutuallydisjunct.First, part A and B were used for training, andpart C for testing.
Then, part A and C were usedfor training, and part B for testing.
About 6% of thewords in the test parts did not occur in the trainingparts, i.e.
they are unknown.
For the moment weonly care about the known words and not about theunknown words (this is treated as a separate pro-blem).
Table 1 shows the tagging results for knownwords.Clustering was applied in the next steps.
In thethird experiment, part A was used for trigram trai-ning, part B for clustering and part C for testing.
Inthe fourth experiment, part A was used for trigramtraining, part C for clustering and part B for testing.The baseline xperiments u ed the clustering partfor the normal training procedure to ensure that bet-ter performance in the clustering experiments is notdue to information provided by the additional part.Clustering reduced the tagset by 33 (third exp.
),and 31 (fourth exp.)
tags.
The tagging results forthe known words are shown in table 1.The improvement i  the tagging result is too smallto be significant.
However, the tagset is reduced,thus also reducing the number of parameters withoutlosing accuracy.
Experiments with larger texts andmore permutations will be performed to get preciseresults for the improvement.3 Conc lus ionsWe have shown a method for reducing a tagset usedfor part-of-speech tagging without losing informa-tion given by the original tagset.
In a first expe-riment, we were able to reduce a large tagset andneeded fewer parameters for the n-gram model.
Ad-ditionally, tagging accuracy slightly increased, butthe improvement was not significant.
Further inve-stigation will focus on criteria for cluster selection.Can we use a similarity measure of probability dis-tributions to identify optimal clusters?
How far canwe reduce the tagset without losing accuracy?ReferencesKenneth Ward Church.
1988.
A stochastic partsprogram and noun phrase parser for unrestrictedtext.
In Proc.
Second Conference on Applied Na-tural Language Processing, pages 136-143, Austin,Texas, USA.Doug Cutting, Julian Kupiec, Jan Pedersen, and Pe-nelope Sibun.
1992.
A practical part-of-speechtagger.
In Proceedings of the 3rd Conference onApplied Natural Language Processing (ACL), pa-ges 133-140.R.
G. Garside, G. N. Leech, and G. R.
Sampson(eds.).
1987.
The Computationai Analysis of Eng-lish.
Longman.L.
R. Rabiner.
1989.
A tutorial on hidden markovmodels and selected applications in speech reco-gnition.
In Proceedings of the IEEE, volume 77(2),pages 257-285.Geoffrey Sampson.
1995.
English for the Computer.Oxford University Press, Oxford.Andreas Stolcke and Stephen M. Omohundro.
1994.Best-first model merging for hidden markov mo-del induction.
Technical Report TR-94-003, In-ternational Computer Science Institute, Berkeley,California, USA.289
