Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 143?152,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsWhat?s Cookin??
Interpreting Cooking Videos using Text, Speech and VisionJonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston,Andrew Rabinovich, and Kevin MurphyGoogle1600 Amphitheatre ParkwayMountain View, CA 94043malmaud@mit.edu{jonathanhuang, rathodv, nickj, amrabino, kpmurphy}@google.comAbstractWe present a novel method for aligning a se-quence of instructions to a video of some-one carrying out a task.
In particular, we fo-cus on the cooking domain, where the instruc-tions correspond to the recipe.
Our techniquerelies on an HMM to align the recipe stepsto the (automatically generated) speech tran-script.
We then refine this alignment usinga state-of-the-art visual food detector, basedon a deep convolutional neural network.
Weshow that our technique outperforms simplertechniques based on keyword spotting.
It alsoenables interesting applications, such as auto-matically illustrating recipes with keyframes,and searching within a video for events of in-terest.1 IntroductionIn recent years, there have been many successfulattempts to build large ?knowledge bases?
(KBs),such as NELL (Carlson et al, 2010), KnowItAll (Et-zioni et al, 2011), YAGO (Suchanek et al, 2007),and Google?s Knowledge Graph/ Vault (Dong et al,2014).
These KBs mostly focus on declarative facts,such as ?Barack Obama was born in Hawaii?.
Buthuman knowledge also encompasses procedural in-formation not yet within the scope of such declara-tive KBs ?
instructions and demonstrations of how todance the tango, for example, or how to change a tireon your car.
A KB for organizing and retrieving suchprocedural knowledge could be a valuable resourcefor helping people (and potentially even robots ?e.g., (Saxena et al, 2014; Yang et al, 2015)) learnto perform various tasks.In contrast to declarative information, proceduralknowledge tends to be inherently multimodal.
Inparticular, both language and perceptual informationare typically used to parsimoniously describe proce-dures, as evidenced by the large number of ?how-to?
videos and illustrated guides on the open web.To automatically construct a multimodal databaseof procedural knowledge, we thus need tools forextracting information from both textual and vi-sual sources.
Crucially, we also need to figure outhow these various kinds of information, which oftencomplement and overlap each other, fit together to aform a structured knowledge base of procedures.As a small step toward the broader goal of align-ing language and perception, we focus in this pa-per on the problem of aligning video depictions ofprocedures to steps in an accompanying text thatcorresponds to the procedure.
We focus on thecooking domain due to the prevalence of cookingvideos on the web and the relative ease of inter-preting their recipes as linear sequences of canon-ical actions.
In this domain, the textual source isa user-uploaded recipe attached to the video show-ing the recipe?s execution.
The individual steps ofprocedures are cooking actions like ?peel an onion?,?slice an onion?, etc.
However, our techniques canbe applied to any domain that has textual instruc-tions and corresponding videos, including videosat sites such as youtube.com, howcast.com,howdini.com or videojug.com.The approach we take in this paper leverages thefact that the speech signal in instructional videos isoften closely related to the actions that the personis performing (which is not true in more general143videos).
Thus we first align the instructional stepsto the speech signal using an HMM, and then refinethis alignment by using a state of the art computervision system.In summary, our contributions are as follows.First, we propose a novel system that combines text,speech and vision to perform an alignment betweentextual instructions and instructional videos.
Sec-ond, we use our system to create a large corpus of180k aligned recipe-video pairs, and an even largercorpus of 1.4M short video clips, each labeled witha cooking action and a noun phrase.
We evaluatethe quality of our corpus using human raters.
Third,we show how we can use our methods to supportapplications such as within-video search and recipeauto-illustration.2 Data and pre-processingWe first describe how we collected our corpus ofrecipes and videos, and the pre-processing steps thatwe run before applying our alignment model.
Thecorpus of recipes, as well as the results of the align-ment model, will be made available for download atgithub.com/malmaud/whats_cookin.2.1 Collecting a large corpus of cooking videoswith recipesWe first searched Youtube for videos whichhave been automatically tagged with the Freebasemids /m/01mtb (Cooking) and /m/0p57p (recipe),and which have (automatically produced) English-language speech transcripts, which yielded a collec-tion of 7.4M videos.
Of these videos, we kept thevideos that also had accompanying descriptive text,leaving 6.2M videos.Sometimes the recipe for a video is included inthis text description, but sometimes it is stored onan external site.
For example, a video?s text de-scription might say ?Click here for the recipe?.
Tofind the recipe in such cases, we look for sentencesin the video description with any of the followingkeywords: ?recipe?, ?steps?, ?cook?, ?procedure?,?preparation?, ?method?.
If we find any such to-kens, we find any URLs that are mentioned in thesame sentence, and extract the corresponding docu-ment, giving us an additional 206k documents.
Wethen combine the original descriptive text with anyClass Precision Recall F1Background 0.97 0.95 0.96Ingredient 0.93 0.95 0.94Recipe step 0.94 0.95 0.94Table 1: Test set performance of text-based recipe classifier.additional text that we retrieve in this way.Finally, in order to extract the recipe from the textdescription of a video, we trained a classifier thatclassifies each sentence into 1 of 3 classes: recipestep, recipe ingredient, or background.
We keeponly the videos which have at least one ingredientsentence and at least one recipe sentence.
This laststep leaves us with 180,000 videos.To train the recipe classifier, we need labeledexamples, which we obtain by exploiting the factthat many text webpages containing recipes usethe machine-readable markup defined at http://schema.org/Recipe.
From this we extract500k examples of recipe sentences, and 500k exam-ples of ingredient sentences.
We also sample 500ksentences at random from webpages to represent thenon-recipe class.
Finally, we train a 3-class na?
?veBayes model on this data using simple bag-of-wordsfeature vectors.
The performance of this model on aseparate test set is shown in Table 1.2.2 Parsing the recipe textFor each recipe, we apply a suite of in-house NLPtools, similar to the Stanford Core NLP pipeline.
Inparticular, we perform POS tagging, entity chunk-ing, and constituency parsing (based on a re-implementation of (Petrov et al, 2006)).1Following(Druck and Pang, 2012), we use the parse tree struc-ture to partition each sentence into ?micro steps?.
Inparticular, we split at any token categorized by theparser as a conjunction only if that token?s parent inthe sentence?s constituency parse is a verb phrase.Any recipe step that is missing a verb is considerednoise and discarded.We then label each recipe step with an optionalaction and a list of 0 or more noun chunks.
The ac-1Sometimes the parser performs poorly, because the lan-guage used in recipes is often full of imperative sentences, suchas ?Mix the flour?, whereas the parser is trained on newswiretext.
As a simple heuristic for overcoming this, we classify anytoken at the beginning of a sentence as a verb if it lexicallymatches a manually-defined list of cooking-related verbs.144tion label is the lemmatized version of the head verbof the recipe step.
We look at all chunked noun en-tities in the step which are the direct object of theaction (either directly or via the preposition ?of?, asin ?Add a cup of flour?
).We canonicalize these entities by computing theirsimilarity to the list of ingredients associated withthis recipe.
If an ingredient is sufficiently similar,that ingredient is added to this step?s entity list.
Oth-erwise, the stemmed entity is used.
For example,consider the step ?Mix tomato sauce and pasta?
; ifthe recipe has a known ingredient called ?spaghetti?,we would label the action as ?mix?
and the entitiesas ?tomato sauce?
and ?spaghetti?, because of itshigh semantic similarity to ?pasta?.
(Semantic sim-ilarity is estimated based on Euclidean distance be-tween word embedding vectors computed using themethod of (Mikolov et al, 2013) trained on generalweb text.
)In many cases, the direct object of a transitive verbis elided (not explicitly stated); this is known as the?zero anaphora?
problem.
For example, the text maysay ?Add eggs and flour to the bowl.
Mix well.?.
Theobject of the verb ?mix?
is clearly the stuff that wasjust added to the bowl (namely the eggs and flour),although this is not explicitly stated.
To handle this,we use a simple recency heuristic, and insert the en-tities from the previous step to the current step.2.3 Processing the speech transcriptThe output of Youtube?s ASR system is a sequenceof time-stamped tokens, produced by a standardViterbi decoding system.
We concatenate these to-kens into a single long document, and then apply ourNLP pipeline to it.
Note that, in addition to errors in-troduced by the ASR system2, the NLP system canintroduce additional errors, because it does not workwell on text that may be ungrammatical and which isentirely devoid of punctuation and sentence bound-ary markers.To assess the impact of these combined sources2According to (Liao et al, 2013), the Youtube ASR systemwe used, based on using Gaussian mixture models for the acous-tic model, has a word error rate of about 52% (averaged over allEnglish-language videos; some genres, such as news, had lowererror rates).
The newer system, which uses deep neural nets forthe acoustic model, has an average WER of 44%; however, thiswas not available to us at the time we did our experiments.Figure 1: Graphical model representation of the factoredHMM.
See text for details.of error, we also collected a much smaller set of 480cooking videos (with corresponding recipe text) forwhich the video creator had uploaded a manuallycurated speech transcript; this has no transcriptionerrors, it contains sentence boundary markers, andit also aligns whole phrases with the video (insteadof just single tokens).
We applied the same NLPpipeline to these manual transcripts.
In the resultssection, we will see that the accuracy of our end-to-end system is indeed higher when the speech tran-script is error-free and well-formed.
However, wecan still get good results using noisier, automaticallyproduced transcripts.3 MethodsIn this section, we describe our system for aligninginstructional text and video.3.1 HMM to align recipe with ASR transcriptWe align each step of the recipe to a correspondingsequence of words in the ASR transcript by using theinput-output HMM shown in Figure 1.
Here X(1 :K) represents the textual recipe steps (obtained us-ing the process described in Section 2.2); Y (1 : T )represent the ASR tokens (spoken words); R(t) ?
{1, .
.
.
,K} is the recipe step number for frame t;and B(t) ?
{0, 1} represents whether timestep t isgenerated by the background (B = 1) or foregroundmodel (B = 0).
This background variable is neededsince sometimes sequences of spoken words are un-related to the content of the recipe, especially at thebeginning and end of a video.The conditional probability distributions (CPDs)for the Markov chain is as follows:p(R(t) = r|R(t?
1) = r?)
=????
if r = r?+11?
?
if r = r?0.0 otherwisep(B(t) = b|B(t?
1) = b) = ?.145This encodes our assumption that the video fol-lows the same ordering as the recipe and that back-ground/foreground tokens tend to cluster together.Obviously these assumptions do not always hold,but they are a reasonable approximation.For each recipe, we set ?
= K/T , the ratio ofrecipe steps to transcript tokens.
This setting corre-sponds to an a priori belief that each recipe step isaligned with the same number of transcript tokens.The parameter ?
in our experiments is set by cross-validation to 0.7 based on a small set of manually-labeled recipes.For the foreground observation model, we gener-ate the observed word from the corresponding recipestep via:log p(Y (t) = y|R(t) = k,X(1 : K), B(t) = 0) ?max({WordSimilarity(y, x) : x ?
X(k)}),where X(k) is the set of words in the k?th recipestep, and WordSimilarity(s, t) is a measure of simi-larity between words s and t, based on word vectordistance.If this frame is aligned to the background, wegenerate it from the empirical distribution of words,which is estimated based on pooling all the data:p(Y (t) = y|R(t) = k,B(t) = 1) = p?
(y).Finally, the prior for p(B(t)) is uniform, andp(R(1)) is set to a delta function on R(1) = 1 (i.e.,we assume videos start at step 1 of the recipe).Having defined the model, we ?flatten?
it to astandard HMM (by taking the cross product of Rtand Bt), then estimate the MAP sequence using theViterbi algorithm.
See Figure 2 for an example.Finally, we label each segment of the video asfollows: use the segmentation induced by the align-ment, and extract the action and object from the cor-responding recipe step as described in Section 2.2.If the segment was labeled as background by theHMM, we do not apply any label to it.3.2 Keyword spottingA simpler approach to labeling video segments is tojust search for verbs in the ASR transcript, and thento extract a fixed-sized window around the times-tamp where the keyword occurred.
We call this ap-proach ?keyword spotting?.
A similar method from(Yu et al, 2014) filters ASR transcripts by part-of-speech tag and finds tokens that match a small vo-cabulary to create a corpus of video clips (extractedfrom instructional videos), each labeled with an ac-tion/object pair.In more detail, we manually define a whitelist of?200 actions (all transitive verbs) of interest, suchas ?add?, ?chop?, ?fry?, etc.
We then identify whenthese words are spoken (relying on the POS tags tofilter out non-verbs), and extract an 8 second videoclip around this timestamp.
(Using 2 seconds priorto the action being mentioned, and 6 seconds follow-ing.)
To extract the object, we take all tokens taggedas ?noun?
within 5 tokens after the action.3.3 Hybrid HMM + keyword spottingWe cannot use keyword spotting if the goal is toalign instructional text to videos.
However, if ourgoal is just to create a labeled corpus of video clips,keyword spotting is a reasonable approach.
Unfor-tunately, we noticed that the quality of the labels(especially the object labels) generated by keywordspotting was not very high, due to errors in the ASR.On the other hand, we also noticed that the recall ofthe HMM approach was about 5 times lower than us-ing keyword spotting, and furthermore, that the tem-poral localization accuracy was sometimes worse.To get the best of both worlds, we employ the fol-lowing hybrid technique.
We perform keyword spot-ting for the action in the ASR transcript as before,but use the HMM alignment to infer the correspond-ing object.
To avoid false positives, we only usethe output of the HMM for this video if at least halfof the recipe steps are aligned by it to the speechtranscript; otherwise we back off to the baseline ap-proach of extracting the noun phrase from the ASRtranscript in the window after the verb.3.4 Temporal refinement using visionIn our experiments, we noticed that sometimes thenarrator describes an action before actually perform-ing it (this was also noted in (Yu et al, 2014)).
Topartially combat this problem, we used computer vi-sion to refine candidate video segments as follows.We first trained visual detectors for a large collec-tion of food items (described below).
Then, givena candidate video segment annotated with an ac-tion/object pair (coming from any of the previous1461: In a bowl combine flour, chilli powder, cumin, paprika and five spice.
Once thoroughly mixed, add in chicken strips and coat in mixture.
?2: Heat oil in a wok or large pan on medium to high heat.
Add in chicken and cook until lightly brown for 3 -- 5 minutes.
?3: Add in chopped vegetables along with garlic, lime juice, hot sauce and Worcestershire sauce.
?4: Cook for a further 15 minutes on medium heat.
?5: As the mixture cooks, chop the tomatoes and add lettuce, and cucumber into a serving bowl.
?6: Once cooked, serve fajita mix with whole wheat wrap.
Add a spoonful of fajita mix into wrap with salsa and natural yogurt.
Wrap or roll up the tortilla and serve with side salad.in a bowl combine the flower chili powder paprika cumen and five-spice do 130 mixed add in the chicken strips and post in the flour mixture he's oil in a walk for large pan on medium to high heat add in the chicken and cook until lightly browned for three to five minutes add in chopped vegetables along with the garlic lime juice hot sauce and Worcestershire sauce dome cook for a further 15 minutes on medium peace and the mixture coax chop the tomatoes and as blessed tomato and cucumber into a serving bowl up we've cooked add a spoonful up the fajita mix into a wrap with the salsa and after yogurt throughout the rack and served with side salad this recipe makes to avalanche portions done they have just taken but he says and delicious introduction to Mexican flavors blue thatRecipe Steps Automatic Speech Transcription0 0.10.2 0.30.4 0.50.6 0.70.8 0.91Video PositionFried chickenTomatoStep 1Step 2Step 5Figure 2: Examples from a Chicken Fajitas recipe at https://www.youtube.com/watch?v=mGpvZE3udQ4 (figure bestviewed in color).
Top: Alignment between (left) recipe steps to (right) automatic speech transcript.
Tokens from the ASR areallowed to be classified as background steps (see e.g., the uncolored text at the end).
Bottom: Detector scores for two ingredientsas a function of position in the video.three methods), we find a translation of the window(of up to 3 seconds in either direction) for which theaverage detector score corresponding to the object ismaximized.
The intuition is that by detecting whenthe object in question is visually present in the scene,it is more likely that the corresponding action is ac-tually being performed.Training visual food detectors.
We trained adeep convolutional neural network (CNN) classi-fier (specifically, the 16 layer VGG model from (Si-monyan and Zisserman, 2014)) on the FoodFood-101 dataset of (Bossard et al, 2014), using the Caffeopen source software (Jia et al, 2014).
The Food-101 dataset contains 1000 images for 101 differentkinds of food.
To compensate for the small trainingset, we pretrained the CNN on the ImageNet dataset(Russakovsky et al, 2014), which has 1.2M images,and then fine-tuned on Food-101.
After a few hoursof fine tuning (using a single GPU), we obtained79% classification accuracy (assuming all 101 labelsare mutually exclusive) on the test set, which is con-sistent with the state of the art results.33In particular, the website https://www.metamind.io/vision/food (accessed on 2/25/15) claims they also got79% on this dataset.
This is much better than the 56.4% for aCNN reported in (Bossard et al, 2014).
We believe the mainreason for the improved performance is the use of pre-trainingon ImageNet.We then trained our model on an internal, propri-etary dataset of 220 million images harvested fromGoogle Images and Flickr.
About 20% of these im-ages contain food, the rest are used to train the back-ground class.
In this set, there are 2809 classes offood, including 1005 raw ingredients, such as avo-cado or beef, and 1804 dishes, such as ratatouille orcheeseburger with bacon.
We use the model trainedon this much larger dataset in the current paper, dueto its increased coverage.
(Unfortunately, we cannotreport quantitative results, since the dataset is verynoisy (sometimes half of the labels are wrong), sowe have no ground truth.
Nevertheless, qualitativebehavior is reasonable, and the model does well onFood-101, as we discussed above.
)Visual refinement pipeline.
For storage and timeefficiency, we downsample each video temporally to5 frames per second and each frame to 224 ?
224before applying the CNN.
Running the food detectoron each video then produces a vector of scores (oneentry for each of 2809 classes) per timeframe.There is not a perfect map from the names ofingredients to the names of the detector outputs.For example, an omelette recipe may say ?egg?,but there are two kinds of visual detectors, onefor ?scrambled egg?
and one for ?raw egg?.
Wetherefore decided to define the match score betweenan ingredient and a frame by taking the maximum147score for that frame over all detectors whose namesmatched any of the ingredient tokens (after lemma-tization and stopword filtering).Finally, the match score of a video segment toan object is computed by taking the average scoreof all frames within that segment.
By then scoringand maximizing over all translations of the candi-date segment (of up to three seconds away), we pro-duce a final ?refined?
segment.3.5 Quantifying confidence via vision andaffordancesThe output of the keyword spotting and/or HMMsystems is an (action, object) label assigned to cer-tain video clips.
In order to estimate how much con-fidence we have in that label (so that we can trade offprecision and recall), we use a linear combination oftwo quantities: (1) the final match score producedby the visual refinement pipeline, which measuresthe visibility of the object in the given video seg-ment, and (2) an affordance probability, measuringthe probability that o appears as a direct object of a.The affordance model allows us to, for example,prioritize a segment labeled as (peel, garlic) over asegment labeled as (peel, sugar).
The probabilitiesP (object = o|action = a) are estimated by firstforming an inverse document frequency matrix cap-turing action/object co-occurrences (treating actionsas documents).
To generalize across actions and ob-jects we form a low-rank approximation to this IDFmatrix using a singular value decomposition and setaffordance probabilities to be proportional to expo-nentiated entries of the resulting matrix.
Figure 3 vi-sualizes these affordance probabilities for a selectedsubset of frequently used action/object pairs.4 Evaluation and applicationsIn this section, we experimentally evaluate how wellour methods work.
We then briefly demonstratesome prototype applications.4.1 Evaluating the clip databaseOne of the main outcomes of our process is a set ofvideo clips, each of which is labeled with a verb (ac-tion) and a noun (object).
We generated 3 such la-beled corpora, using 3 different methods: keywordspotting (?KW?
), the hybrid HMM + keyword spot-ting (?Hybrid?
), and the hybrid system with visualFigure 3: Visualization of affordance model.
Entries (a, o) arecolored according to P (object = o | action = a).Score00.511.52Actions ObjectsKW Visual Refinement (alone)HybridHybrid (manual)Visual Refinement +recipeKW(manual)KW Visual Refinement (alone)HybridHybrid (manual)Visual Refinement +recipeKW(manual)Figure 4: Clip quality, as assessed by Mechanical Turk exper-iments on 900 trials.
Figure best viewed in color; see text fordetails.food detector (?visual refinement?).
The total num-ber of clips produced by each method is very similar,approximately 1.4 million.
The coverage of the clipsis approximately 260k unique (action, noun phrase)pairs.To evaluate the quality of these methods, we cre-ated a random subset of 900 clips from each corpususing stratified sampling.
That is, we picked an ac-tion uniformly at random, and then picked a corre-sponding object for that action from its support setuniformly at random, and finally picked a clip withthat (action, object) label uniformly at random fromthe clip corpuses produced in Section 3; this ensures1481.21.31.41.51.61.71.81.920 0.5 1 1.5Mean clip qualityNumber of clips MillionsActionObjectFigure 5: Average clip quality (precision) after filtering outlow confidence clips versus # clips retained (recall).# RatersRating-2 -1 0 1 21701211647670Figure 6: Histogram of human ratings comparing recipe stepsagainst ASR descriptions of a video clip.
?2?
indicate a strongpreference for the recipe step; ?-2?
a strong preference for thetranscript.
See text for details.the test set is not dominated by frequent actions orobjects.We then performed a Mechanical Turk experi-ment on each test set.
Each clip was shown to 3raters, and each rater was asked the question ?Howwell does this clip show the given action/object?
?.Raters then had to answer on a 3-point scale: 0means ?not at all?, 1 means ?somewhat?, and 2means ?very well?.The results are shown in Figure 4.
We see thatthe quality of the hybrid method is significantly bet-ter than the baseline keyword spotting method, forboth actions and objects.4While a manually curated4Inter-rater agreement, measured via Fleiss?s kappa by ag-gregating across all judgment tasks, is .41, which is statisticallysignificant at a p < .05 level.speech transcript indeed yields better results (see thebars labeled ?manual?
), we observe that automati-cally generated transcripts allow us to perform al-most as well, especially using our alignment modelwith visual refinement.Comparing accuracy on actions against that onobjects in the same figure, we see that keyword spot-ting is far more accurate for actions than it is forobjects (by over 30%).
This disparity is not surpris-ing since keyword spotting searches only for actionkeywords and relies on a rough heuristic to recoverobjects.
We also see that using alignment (whichextracts the object from the ?clean?
recipe text) andvisual refinement (which is trained explicitly to de-tect ingredients) both help to increase the relative ac-curacy of objects ?
under the hybrid method, forexample, the accuracy for actions is only 8% betterthan that of objects.Note that clips from the HMM and hybrid meth-ods varied in length between 2 and 10 seconds(mean 4.2 seconds), while clips from the keywordspotting method were always exactly 8 seconds.Thus clip length is potentially a confounding factorin the evaluation when comparing the hybrid methodto the keyword-spotting method; however, if there isa bias to assign higher ratings to longer clips (whichare a priori more likely to contain a depiction of agiven action than shorter clips), it would benefit thekeyword spoting method.Segment confidence scores (from Section 3.5) canbe used to filter out low confidence segments, thusimproving the precision of clip retrieval at the cost ofrecall.
Figure 5 visualizes this trade-off as we varyour confidence threshold, showing that indeed, seg-ments with higher confidences tend to have the high-est quality as judged by our human raters.
More-over, the top 167,000 segments as ranked by our con-fidence measure have an average rating exceeding1.75.We additionally sought to evaluate how wellrecipe steps from the recipe body could serve ascaptions for video clips in comparison to the oftennoisy ASR transript, which serves as a rough proxyfor evaluating the quality of the alignment model aswell as demonstration a potential application of ourmethod for ?cleaning up?
noisy ASR captions intocomplete grammatical sentences.
To that end, werandomly selected 200 clips from our corpus that149both have an associated action keyword from thetranscript as well as an aligned recipe step selectedby the HMM alignment model.
For each clip, threeraters on Mechanical Turk were shown the clip, thetext from the recipe step, and a fragment of the ASRtranscript (the keyword, plus 5 tokens to the left andright of the keyword).
Raters then indicated whichdescription they preferred: 2 indicates a strong pref-erence for the recipe step, 1 a weak preference, 0indifference, -1 a weak preference for the transcriptfragment, and -2 a strong preference.
Results areshown in Figure 6.
Excluding raters who indicatedindiffierence, 67% of raters preferred the recipe stepas the clip?s description.A potential confound for using this analysis asa proxy for the quality of the alignment model isthat the ASR transcript is generally an ungrammat-ical sentence fragment as opposed to the grammati-cal recipe steps, which is likely to reduce the raters?approval of ASR captions in the case when both ac-curately describe the scene.
However, if users stillon average prefer an ASR sentence fragment whichdescribes the clip correctly versus a full recipe stepwhich is unrelated to the scene, then this experimentstill provides evidence of the quality of the align-ment model.4.2 Automatically illustrating a recipeOne useful byproduct of our alignment method isthat each recipe step is associated with a segmentof the corresponding video.5We use a standardkeyframe selection algorithm to pick the best framefrom each segment.
We can then associate this framewith the corresponding recipe step, thus automati-cally illustrating the recipe steps.
An illustration ofthis process is shown in Figure 7.4.3 Search within a videoAnother application which our methods enable issearch within a video.
For example, if a user wouldlike to find a clip illustrating how to knead dough,we can simply search our corpus of labeled clips,5The HMM may assign multiple non-consecutive regions ofthe video to the same recipe step (since the background state canturn on and off).
In such cases, we just take the ?convex hull?of the regions as the interval which corresponds to that step.
Itis also possible for the HMM not to assign a given step to anyinterval of the video.Figure 8: Searching for ?knead dough?.
Note that the videoshave automatically been advanced to the relevant frame.and return a list of matches (ranked by confidence).Since each clip has a corresponding ?provenance?,we can return the results to the user as a set of videosin which we have automatically ?fast forwarded?
tothe relevant section of the video (see Figure 8 for anexample).
This stands in contrast to standard videosearch on Youtube, which returns the whole video,but does not (in general) indicate where within thevideo the user?s search query occurs.5 Related workThere are several pieces of related work.
(Yu et al,2014) performs keyword spotting in the speech tran-script in order to label clips extracted from instruc-tional videos.
However, our hybrid approach per-forms better; the gain is especially significant on au-tomatically generated speech transcripts, as shownin Figure 4.The idea of using an HMM to align instructionalsteps to a video was also explored in (Naim et al,2014).
However, their conditional model has to gen-erate images, whereas ours just has to generate ASRwords, which is an easier task.
Furthermore, theyonly consider 6 videos collected in a controlled labsetting, whereas we consider over 180k videos col-lected ?in the wild?.Another paper that uses HMMs to process recipetext is (Druck and Pang, 2012).
They use the HMMto align the steps of a recipe to the comments madeby users in an online forum, whereas we align thesteps of a recipe to the speech transcript.
Also, weuse video information, which was not considered inthis earlier work.
(Joshi et al, 2006) describes a system to automat-ically illustrate a text document, however they onlygenerate one image, not a sequence, and their tech-niques are very different.There is also a large body of other work on con-necting language and vision; we only have space150De-stem 2 medium plum tomatoes.
Cut them in half lengthwise and remove the seeds.
Finely chop the tomatoes, combining them with 1/4 cup of finely chopped red onion, 2 minced cloves of garlic, 1 tablespoon of olive oil, 2 tablespoons of fresh lime juice, and 1/8 teaspoon of black pepperCut an avocado into chunks and mash until it's smooth with just a few pieces intact.
Stir the mashed avocados into the other mixture for a homemade guacamole recipe that 's perfect for any occasion!Use this easy guacamole for parties, or serve chips with guacamole for an easy appetizer.
You could even add some cayenne, jalapenos, or ancho chili for even more kick to add to your Mexican food night!Figure 7: Automatically illustrating a Guacamole recipe from https://www.youtube.com/watch?v=H7Ne3s202lU.to briefly mention a few key papers.
(Rohrbach etal., 2012b) describes the MPII Cooking CompositeActivities dataset, which consists of 212 videos col-lected in the lab of people performing various cook-ing activities.
(This extends the dataset described intheir earlier work, (Rohrbach et al, 2012a).)
Theyalso describe a method to recognize objects and ac-tions using standard vision features.
However, theydo not leverage the speech signal, and their datasetis significantly smaller than ours.
(Guadarrama et al, 2013) describes a method forgenerating subject-verb-object triples given a shortvideo clip, using standard object and action detec-tors.
The technique was extended in (Thomason etal., 2014) to also predict the location/ place.
Further-more, they use a linear-chain CRF to combine thevisual scores with a simple (s,v,o,p) language model(similar to our affordance model).
They appliedtheir technique to the dataset in (Chen and Dolan,2011), which consists of 2000 short video clips, eachdescribed with 1-3 sentences.
By contrast, we focuson aligning instructional text to the video, and ourcorpus is significantly larger.
(Yu and Siskind, 2013) describes a technique forestimating the compatibility between a video clipand a sentence, based on relative motion of theobjects (which are tracked using HMMs).
Theirmethod is tested on 159 video clips, created undercarefully controlled conditions.
By contrast, we fo-cus on aligning instructional text to the video, andour corpus is significantly larger.6 Discussion and future workIn this paper, we have presented a novel method foraligning instructional text to videos, leveraging bothspeech recognition and visual object detection.
Wehave used this to align 180k recipe-video pairs, fromwhich we have extracted a corpus of 1.4M labeledvideo clips ?
a small but crucial step toward build-ing a multimodal procedural knowlege base.
In thefuture, we hope to use this labeled corpus to trainvisual action detectors, which can then be combinedwith the existing visual object detectors to interpretnovel videos.
Additionally, we believe that combin-ing visual and linguistic cues may help overcomelongstanding challenges to language understanding,such as anaphora resolution and word sense disam-biguation.Acknowledgments.
We would like to thank AlexGorban and Anoop Korattikara for helping withsome of the experiments, and Nancy Chang for feed-back on the paper.ReferencesLukas Bossard, Matthieu Guillaumin, and Luc Van Gool.2014.
Food-101 ?
mining discriminative componentswith random forests.
In Proc.
European Conf.
onComputer Vision.A.
Carlson, J. Betteridge, B. Kisiel, B.
Settles, E. Hr-uschka Jr., and T. Mitchell.
2010.
Toward an archi-tecture for never-ending language learning.
In Procs.AAAI.David L Chen and William B Dolan.
2011.
Collectinghighly parallel data for paraphrase evaluation.
In Proc.ACL, HLT ?11, pages 190?200, Stroudsburg, PA, USA.Association for Computational Linguistics.X.
Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao,K.
Murphy, T. Strohmann, S. Sun, and W. Zhang.2014.
Knowledge vault: A web-scale approach toprobabilistic knowledge fusion.
In Proc.
of the Int?lConf.
on Knowledge Discovery and Data Mining.Gregory Druck and Bo Pang.
2012.
Spice it up?
: Mining151refinements to online instructions from user generatedcontent.
In Proc.
ACL, pages 545?553.O.
Etzioni, A. Fader, J. Christensen, S. Soderland, andMausam.
2011.
Open Information Extraction: theSecond Generation.
In Intl.
Joint Conf.
on AI.S.
Guadarrama, N. Krishnamoorthy, G. Malkarnenkar,S.
Venugopalan, R. Mooney, T. Darrell, andK.
Saenko.
2013.
YouTube2Text: Recognizing anddescribing arbitrary activities using semantic hierar-chies and Zero-Shot recognition.
In Intl.
Conf.
onComputer Vision, pages 2712?2719.Yangqing Jia, Evan Shelhamer, Jeff Donahue, SergeyKarayev, Jonathan Long, Ross Girshick, SergioGuadarrama, and Trevor Darrell.
2014.
Caffe: Con-volutional architecture for fast feature embedding,20 June.Dhiraj Joshi, James Z Wang, and Jia Li.
2006.
The storypicturing Engine-A system for automatic text illus-tration.
ACM Trans.
Multimedia Comp., Comm.
andAppl., 2(1):1?22.Hank Liao, Erik McDermott, and Andrew Senior.
2013.Large scale deep neural network acoustic modelingwith semi-supervised training data for YouTube videotranscription.
In ASRU (IEEE Automatic SpeechRecognition and Understanding Workshop).Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word representa-tions in vector space.
http://arxiv.org/abs/1301.3781.I Naim, Y C Song, Q Liu, H Kautz, J Luo, and D Gildea.2014.
Unsupervised alignment of natural language in-structions with video segments.
In Procs.
of AAAI.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 433?440, Sydney, Aus-tralia, July.
Association for Computational Linguistics.M Rohrbach, S Amin, M Andriluka, and B Schiele.2012a.
A database for fine grained activity detectionof cooking activities.
In CVPR, pages 1194?1201.Marcus Rohrbach, Michaela Regneri, Mykhaylo An-driluka, Sikandar Amin, Manfred Pinkal, and BerntSchiele.
2012b.
Script data for Attribute-Based recog-nition of composite activities.
In Proc.
EuropeanConf.
on Computer Vision, pages 144?157.Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-drej Karpathy, Aditya Khosla, Michael Bernstein,Alexander C. Berg, and Li Fei-Fei.
2014.
Im-ageNet Large Scale Visual Recognition Challenge.http://arxiv.org/abs/1409.0575.Ashutosh Saxena, Ashesh Jain, Ozan Sener, Aditya Jami,Dipendra K Misra, and Hema S Koppula.
2014.RoboBrain: Large-Scale knowledge engine for robots.http://arxiv.org/pdf/1412.0691.pdf.Karen Simonyan and Andrew Zisserman.
2014.Very deep convolutional networks for Large-Scale image recognition, 4 September.http://arxiv.org/abs/1409.1556.F.
M. Suchanek, G. Kasneci, and G. Weikum.
2007.YAGO: A Large Ontology from Wikipedia and Word-Net.
J.
Web Semantics, 6:203217.J Thomason, S Venugopalan, S Guadarrama, K Saenko,and R Mooney.
2014.
Integrating language and visionto generate natural language descriptions of videos inthe wild.
In Intl.
Conf.
on Comp.
Linguistics.Yezhou Yang, Yi Li, Cornelia Ferm?uller, and YiannisAloimonos.
2015.
Robot learning manipulation ac-tion plans by watching unconstrained videos from theworld wide web.
In The Twenty-Ninth AAAI Confer-ence on Artificial Intelligence (AAAI-15).Haonan Yu and JM Siskind.
2013.
Grounded languagelearning from video described with sentences.
In Proc.ACL.Shoou-I Yu, Lu Jiang, and Alexander Hauptmann.
2014.Instructional videos for unsupervised harvesting andlearning of action examples.
In Intl.
Conf.
Multimedia,pages 825?828.
ACM.152
