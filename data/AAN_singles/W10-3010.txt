Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 70?77,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsA High-Precision Approach to Detecting Hedges and Their ScopesHalil Kilicoglu and Sabine BerglerDepartment of Computer Science and Software EngineeringConcordia University1455 de Maisonneuve Blvd.
WestMontre?al, Canada{h kilico,bergler}@cse.concordia.caAbstractWe extend our prior work on specula-tive sentence recognition and speculationscope detection in biomedical text to theCoNLL-2010 Shared Task on Hedge De-tection.
In our participation, we soughtto assess the extensibility and portabilityof our prior work, which relies on linguis-tic categorization and weighting of hedg-ing cues and on syntactic patterns in whichthese cues play a role.
For Task 1B,we tuned our categorization and weight-ing scheme to recognize hedging in bio-logical text.
By accommodating a smallnumber of vagueness quantifiers, we wereable to extend our methodology to de-tecting vague sentences in Wikipedia arti-cles.
We exploited constituent parse treesin addition to syntactic dependency rela-tions in resolving hedging scope.
Our re-sults are competitive with those of closed-domain trained systems and demonstratethat our high-precision oriented methodol-ogy is extensible and portable.1 IntroductionNatural language is imbued with uncertainty,vagueness, and subjectivity.
However, informa-tion extraction systems generally focus on ex-tracting factual information, ignoring the wealthof information expressed through such phenom-ena.
In recent years, the need for information ex-traction and text mining systems to identify andmodel such extra-factual information has increas-ingly become clear.
For example, online productand movie reviews have provided a rich contextfor analyzing sentiments and opinions in text (seePang and Lee (2008) for a recent survey), whiletentative, speculative nature of scientific writing,particularly in biomedical literature, has providedimpetus for recent research in speculation detec-tion (Light et al, 2004).
The term hedging is oftenused as an umbrella term to refer to an array ofextra-factual phenomena in natural language andis the focus of the CoNLL-2010 Shared Task onHedge Detection.The CoNLL-2010 Shared Task on Hedge De-tection (Farkas et al, 2010) follows in the stepsof the recent BioNLP?09 Shared Task on EventExtraction (Kim et al, 2009), in which one task(speculation and negation detection) was con-cerned with notions related to hedging in biomed-ical abstracts.
However, the CoNLL-2010 SharedTask differs in several aspects.
It sheds light onthe pervasiveness of hedging across genres and do-mains: in addition to biomedical abstracts, it isconcerned with biomedical full text articles as wellas with Wikipedia articles.
Both shared tasks havebeen concerned with scope resolution; however,their definitions of scope are fundamentally differ-ent: the BioNLP?09 Shared Task takes the scopeof a speculation instance to be an abstract seman-tic object (an event), thus a normalized logicalform.
The CoNLL-2010 Shared Task, on the otherhand, defines it as a textual unit based on syntac-tic considerations.
It is also important to note thathedging in scientific writing is a core aspect of thegenre (Hyland, 1998), while it is judged to be aflaw which has to be eradicated in Wikipedia ar-ticles.
Therefore, hedge detection in these genresserves different purposes: explicitly encoding thefactuality of a scientific claim (doubtful, probable,etc.)
versus flagging unreliable text.We participated in both tasks of the CoNLL-2010 Shared Task: namely, detection of sentenceswith uncertainty (Task 1) and resolution of uncer-tainty scope (Task 2).
Since we pursued both ofthese directions in prior work, one of our goals inparticipating in the shared task was to assess howour approach generalized to previously unseentexts, even genres.
Towards this goal, we adopted70an open-domain approach, where we aimed to usepreviously developed techniques to the extent pos-sible.
Among all participating groups, we distin-guished ourselves as the one that fully worked inan open-domain setting.
This approach workedreasonably well for uncertainty detection (Task 1);however, for the scope resolution task, we neededto extend our work more substantially, since thenotion of scope was fundamentally different thanwhat we adopted previously.
The performanceof our system was competitive; in terms of F-measure, we were ranked near the middle in Task1, while a more significant focus on scope reso-lution resulted in fourth place ranking among fif-teen systems.
We obtained the highest precisionin tasks focusing on biological text.
Consideringthat we chose not to exploit the training data pro-vided to the full extent, we believe that our systemis viable in terms of extensibility and portability.2 Related WorkSeveral notions related to hedging have been pre-viously explored in natural language processing.In the news article genre, these have includedcertainty, modality, and subjectivity.
For ex-ample, Rubin et al (2005) proposed a four di-mensional model to categorize certainty in newstext: certainty level, focus, perspective and time.In the context of TimeML (Pustejovsky et al,2005), which focuses on temporal expressionsin news articles, event modality is encoded us-ing subordination links (SLINKs), some of which(MODAL,EVIDENTIAL) indicate hedging (Saur??
etal., 2006).
Saur??
(2008) exploits modality andpolarity to assess the factuality degree of events(whether they correspond to facts, counter-facts orpossibilities), and reports on FactBank, a corpusannotated for event factuality (Saur??
and Puste-jovsky, 2009).
Wiebe et al (2005) considersubjectivity in news articles, and focus on thenotion of private states, encompassing specula-tions, opinions, and evaluations in their subjectiv-ity frames.The importance of speculative language inbiomedical articles was first acknowledged byLight et al (2004).
Following work in this areafocused on detecting speculative sentences (Med-lock and Briscoe, 2007; Szarvas, 2008; Kilicogluand Bergler, 2008).
Similar to Rubin et al?s(2005) work, Thompson et al (2008) proposeda categorization scheme for epistemic modality inbiomedical text according to the type of infor-mation expressed (e.g., certainty level, point ofview, knowledge type).
With the availability of theBioScope corpus (Vincze et al, 2008), in whichnegation, hedging and their scopes are annotated,studies in detecting speculation scope have alsobeen reported (Morante and Daelemans, 2009;O?zgu?r and Radev, 2009).
Negation and uncer-tainty of bio-events are also annotated to some ex-tent in the GENIA event corpus (Kim et al, 2008).The BioNLP?09 Shared Task on Event Extraction(Kim et al, 2009) dedicated a task to detectingnegation and speculation in biomedical abstracts,based on the GENIA event corpus annotations.Ganter and Strube (2009) elaborated on the linkbetween vagueness in Wikipedia articles indicatedby weasel words and hedging.
They exploitedword frequency measures and shallow syntacticpatterns to detect weasel words in Wikipedia ar-ticles.3 MethodsOur methodology for hedge detection is essen-tially rule-based and relies on a combination oflexical and syntactic information.
Lexical infor-mation is encoded in a simple dictionary, and rel-evant syntactic information is identified using theStanford Lexicalized Parser (Klein and Manning,2003).
We exploit constituent parse trees as wellas corresponding collapsed dependency represen-tations (deMarneffe et al, 2006), provided by theparser.3.1 Detecting Uncertainty in Biological TextFor detecting uncertain sentences in biological text(Task 1B), we built on the linguistically-inspiredsystem previously described in detail in Kilicogluand Bergler (2008).
In summary, this system relieson a dictionary of lexical speculation cues, derivedfrom a set of core surface realizations of hedgingidentified by Hyland (1998) and expanded throughWordNet (Fellbaum, 1998) synsets and UMLSSPECIALIST Lexicon (McCray et al, 1994) nom-inalizations.
A set of lexical certainty markers (un-hedgers) are also included, as they indicate hedg-ing when they are negated (e.g., know).
Thesehedging cues are categorized by their type (modalauxiliaries, epistemic verbs, approximative adjec-tives, etc.)
and are weighted to reflect their cen-tral/peripheral contribution to hedging, inspired bythe fuzzy model of Hyland (1998).
We use a scale71of 1-5, where 5 is assigned to cues most centralto hedging and 1 to those that are most periph-eral.
For example, the modal auxiliary may hasa weight of 5, while a relatively weak hedgingcue, the epistemic adverb apparently, has a weightof 2.
The weight sum of cues in a sentence incombination with a predetermined threshold de-termines whether the sentence in question is un-certain.
Syntax, generally ignored in other stud-ies on hedging, plays a prominent role in our ap-proach.
Certain syntactic constructions act as cues(e.g., whether- and if -complements), while othersstrengthen or weaken the effect of the cue associ-ated with them.
For example, a that-complementtaken by an epistemic verb increases the hedgingscore contributed by the verb by 2, while lack ofany complement decreases the score by 1.For the shared task, we tuned this categoriza-tion and weighting scheme, based on an analy-sis of the biomedical full text articles in trainingdata.
We also adjusted the threshold.
We elim-inated some hedging cue categories completelyand adjusted the weights of a small number ofthe remaining cues.
The eliminated cue categoriesincluded approximative adverbs (e.g., generally,largely, partially) and approximative adjectives(e.g., partial), often used to ?manipulate preci-sion in quantification?
(Hyland, 1998).
The othereliminated category included verbs of effort (e.g.,try, attempt, seek), also referred to as rationalisingnarrators (Hyland, 1998).
The motivation behindeliminating these categories was that cues belong-ing to these categories were never annotated ashedging cues in the training data.
The eliminationprocess resulted in a total of 147 remaining hedg-ing cues.
Additionally, we adjusted the weights ofseveral other cues that were not consistently anno-tated as cues in the training data, despite our viewthat they were strong hedging cues.
One exampleis the epistemic verb predict, previously assigned aweight of 4 based on Hyland?s analysis.
We foundits annotation in the training data somewhat incon-sistent, and lowered its weight to 3, thus requiringa syntactic strengthening effect (an infinitival com-plement, for example) for it to qualify as a hedgingcue in the current setting (threshold of 4).3.2 Detecting Uncertainty in WikipediaArticlesTask 1W was concerned with detecting uncer-tainty in Wikipedia articles.
Uncertainty in thiscontext refers more or less to vagueness indicatedby weasel words, an undesirable feature accord-ing to Wikipedia policy.
Analysis of Wikipediatraining data provided by the organizers revealedthat there is overlap between weasel words andhedging cues described in previous section.
We,therefore, sought to adapt our dictionary of hedg-ing cues to the task of detecting vagueness inWikipedia articles.
Similar to Task 1B, changesinvolved eliminating cue categories and adjustingcue weights.
In addition, however, we also addeda previously unconsidered category of cues, dueto their prominence in Wikipedia data as weaselwords.
This category (vagueness quantifiers (Lap-pin, 2000)) includes words, such as some, several,many and various, which introduce imprecisionwhen in modifier position.
For instance, in the ex-ample below, both some and certain contribute tovagueness of the sentence.
(1) Even today, some cultures have certain in-stances of their music intending to imitatenatural sounds.For Wikipedia uncertainty detection, eliminatedcategories included verbs and nouns concerningtendencies (e.g., tend, inclination) in addition toverbs of effort.
The only modal auxiliary consis-tently considered a weasel word was might; there-fore, we only kept might in this category and elim-inated the rest (e.g., may, would).
Approxima-tive adverbs, eliminated in detecting uncertaintyin biological text, not only were revived for thistask, but also their weights were increased as theywere more central to vagueness expressions.
Be-sides these changes in weighting and categoriza-tion, the methodology for uncertainty detection inWikipedia articles was essentially the same as thatfor biological text.
The threshold we used in oursubmission was, similarly, 4.3.3 Scope Resolution for Uncertainty inBiological TextTask 2 of the shared task involved hedging scoperesolution in biological text.
We previously tack-led this problem within the context of biologicaltext in the BioNLP?09 Shared Task (Kilicoglu andBergler, 2009).
That task defined the scope ofspeculation instances as abstract, previously ex-tracted bio-events.
Our approach relied on find-ing an appropriate syntactic dependency relationbetween the bio-event trigger word identified in72earlier steps and the speculation cue.
The cate-gory of the hedging cue constrained the depen-dency relations that are deemed appropriate.
Forexample, consider the sentence in (2a), where in-volves is a bio-event trigger for a Regulationevent and suggest is a speculation cue of epis-temic verb type.
The first dependency relationin (2b) indicates that the epistemic verb takes aclausal complement headed by the bio-event trig-ger.
The second indicates that that is the comple-mentizer.
This cue category/dependency combi-nation licenses the generation of a speculation in-stance where the event indicated by the event trig-ger represents the scope.
(2) (a) The results suggest that M-CSF induc-tion of M-CSF involves G proteins, PKCand NF kappa B.
(b) ccomp(suggest,involves)complm(involves,that)Several other cue category/dependency combi-nations sought for speculation scope resolution aregiven in Table 1.
X represents a token that is nei-ther a cue nor a trigger (aux: auxiliary, dobj: directobject, neg: negation modifier).Cue Category DependencyModal auxiliary (may) aux(Trigger,Cue)Conditional (if ) complm(Trigger,Cue)Unhedging noun dobj(X,Cue)(evidence) ccomp(X,Trigger)neg(Cue,no)Table 1: Cue categories with examples and the de-pendency relations to searchIn contrast to this notion of scope being an ab-stract semantic object, Task 2 (BioScope corpus,in general) conceptualizes hedge scope as a con-tinuous textual unit, including the hedging cue it-self and the biggest syntactic unit the cue is in-volved in (Vincze et al, 2008).
This fundamen-tal difference in conceptualization limits the di-rect applicability of our prior approach to thistask.
Nevertheless, we were able to use our workas a building block in extending scope resolutionheuristics.
We further augmented it by exploitingconstituent parse trees provided by Stanford Lex-icalized Parser.
These extensions are summarizedbelow.3.3.1 Exploiting parse treesThe constituent parse trees contribute to scoperesolution uniformly across all hedging cue cate-gories.
We simply determine the phrasal node thatdominates the hedging cue and consider the tokenswithin that phrase as being in the scope of the cue,unless they meet one of the following exclusioncriteria:1.
Exclude tokens within post-cue sententialcomplements (indicated by S and SBARnodes) introduced by a small number ofdiscourse markers (thus, whereas, because,since, if, and despite).2.
Exclude punctuation marks at the rightboundary of the phrase3.
Exclude pre-cue determiners and adverbs atthe left boundary of the phraseFor example, in the sentence below, the verbphrase that included the modal auxiliary may alsoincluded the complement introduced by thereby.Using the exclusion criteria 1 and 2, we excludedthe tokens following SPACER from the scope:(3) (a) .
.
.motifs may be easily compared withthe results from BEAM, PRISM andSPACER, thereby extending the SCOPEensemble to include a fourth class ofmotifs.
(b) CUE: maySCOPE: motifs may be easily comparedwith the results from BEAM, PRISMand SPACER3.3.2 Extending dependency-based heuristicsThe new scope definition was also accommodatedby extending the basic dependency-based heuris-tics summarized earlier in this section.
In additionto finding the trigger word that satisfies the ap-propriate dependency constraint with the hedgingcue (we refer to this trigger word as scope head,henceforth), we also considered the other depen-dency relations that the scope head was involvedin.
These relations, then, were used in right ex-pansion and left expansion of the scope.
Right ex-pansion involves finding the rightmost token thatis in a dependency relation with the scope head.Consider the sentence below:(4) The surprisingly low correlations betweenSig and accuracy may indicate that the ob-jective functions employed by motif finding73programs are only a first approximation to bi-ological significance.The epistemic verb indicate has as its scopehead the token approximation, due to the existenceof a clausal complement dependency (ccomp) be-tween them.
On the other hand, the rightmost to-ken of the sentence, significance, has a preposi-tional modifier dependency (prep to) with approx-imation.
It is, therefore, included in the scope ofindicate.
Two dependency types, adverbial clausemodifier (advcl) and conjunct (conj), were ex-cluded from consideration when the rightmost to-ken is sought, since they are likely to signal newdiscourse units outside the scope.In contrast to right expansion, which appliesto all hedging cue categories, left expansion ap-plies only to a subset.
Left expansion involvessearching for a subject dependency governed bythe scope head.
The dependency types descend-ing from the subject (subj) type in the Stanford de-pendency hierarchy are considered: nsubj (nom-inal subject), nsubjpass (passive nominal sub-ject), csubj (clausal subject) and csubjpass (pas-sive clausal subject).
In the following example,the first token, This, is added to the scope of likelythrough left expansion (cop: copula).
(5) (a) This is most likely a conservative esti-mate since a certain proportion of inter-actions remain unknown .
.
.
(b) nsubj(likely,This)cop(likely,is)Left expansion was limited to the following cuecategories, with the additional constraints given:1.
Modal auxiliaries, only when their scopehead takes a passive subject (e.g., they isadded to the scope of may in they may be an-notated as pseudogenes).2.
Cues in adjectival categories, when they arein copular constructions (e.g., Example (5)).3.
Cues in several adjectival ad verbal cate-gories, when they take infinitival comple-ments (e.g., this is added to the scope of ap-pears in However, this appears to add morenoise to the prediction without increasing theaccuracy).After scope tokens are identified using the parsetree as well as via left and right expansion, the al-gorithm simply sets as scope the continuous tex-tual unit that includes all the scope tokens and thehedging cue.
Since, likely is the hedging cue andThis and estimate are identified as scope tokens inExample (5), the scope associated with likely be-comes This is most likely a conservative estimate.We found that citations, numbers and punc-tuation marks occurring at the end of sentencescaused problems in scope resolution, specificallyin biomedical full text articles.
Since they arerarely within any scope, we implemented a simplestripping algorithm to eliminate them from scopesin such documents.4 Results and DiscussionThe official evaluation results regarding our sub-mission are given in Table 2.
These results wereachieved with the threshold 4, which was the opti-mal threshold on the training data.Prec.
Recall F-score RankTask 1B 92.07 74.94 82.62 12/24Task 1W 67.90 46.02 54.86 10/17Task 2 62.47 49.47 55.21 4/15Table 2: Evaluation resultsIn Task 1B, we achieved the highest precision.However, our relatively low recall led to the place-ment of our system in the middle.
Our system al-lows adjusting precision versus recall by settingthe threshold.
In fact, setting the threshold to 3 af-ter the shared task, we were able to obtain overallbetter results (Precision=83.43, Recall=84.81, F-score=84.12, Rank=8/24).
However, we explicitlytargeted precision, and in that respect, our submis-sion results were not surprising.
In fact, we iden-tified a new type of hedging signalled by coordi-nation (either .
.
.
or .
.
.
as well as just or) in thetraining data.
An example is given below:(6) (a) It will be either a sequencing error or apseudogene.
(b) CUE: either-orSCOPE: either a seqeuncing error or apseudogeneBy handling this class to some extent, we couldhave increased our recall, and therefore, F-score(65 out of 1,044 cues in the evaluation data forbiological text involved this class).
However, wedecided against treating this class, as we believeit requires a slightly different treatment due to itsspecial semantics.74In participating in Task 1W, our goal was totest the ease of extensibility of our system.
Inthat regard, our results show that we were ableto exploit the overlap between our hedging cuesand the weasel words.
The major difference wenoted between hedging in two genres was theclass of vagueness quantifiers, and, with little ef-fort, we extended our system to consider them.We also note that setting the threshold to 3 afterthe shared task, our recall and F-score improvedsignificantly (Precision=63.21, Recall=53.67, F-score=58.05, Rank=3/17).Our more substantial effort for Task 2 resultedin a better overall ranking, as well as the highestprecision in this task.
In contrast to Task 1, chang-ing the threshold in this task did not have a pos-itive effect on the outcome.
We also measuredthe relative contribution of the enhancements toscope resolution.
The results are presented in Ta-ble 3.
Baseline is taken as the scope resolution al-gorithm we developed in prior work.
These resultsshow that: a) scope definition we adopted earlieris essentially incompatible with the BioScope def-inition b) simply taking the phrase that the hedg-ing cue belongs to as the scope provides relativelygood results c) left and right expansion heuristicsare needed for increased precision and recall.Prec.
Recall F-scoreBaseline 3.29 2.61 2.91Baseline+ Left/right expansion25.18 20.03 22.31Parse tree 49.20 39.10 43.58Baseline+Parse tree50.66 40.27 44.87All 62.47 49.47 55.21Table 3: Effect of scope resolution enhancements4.1 Error AnalysisIn this section, we provide a short analysis of theerrors our system generated, focusing on biologi-cal text.Since our dictionary of hedging cues is incom-plete and we did not attempt to expand it for Task1B, we had a fair number of recall errors.
Aswe mentioned above, either-or constructions oc-cur frequently in the training and evaluation data,and we did not attempt to handle them.
Addition-ally, some lexical cues, such as feasible and im-plicate, do not appear in our dictionary, causingfurther recall errors.
The weighting scheme alsoaffects recall.
For example, the adjective appar-ent has a weight of 2, which is not itself sufficientto qualify a sentence as uncertain (with a thresh-old of 4) (7a).
On the other hand, when it takesa clausal complement, the sentence is considereduncertain (7b).
The first sentence (7a) causes a re-call error.
(7) (a) An apparent contradiction between thepreviously reported number of cyclinggenes .
.
.
(b) .
.
.
it is apparent that the axonal terminicontain a significantly reduced numberof varicosities .
.
.In some cases, syntactic constructions that playa role in determining the certainty status of a sen-tence cannot be correctly identified by the parser,often leading to recall errors.
For example, in thesentence below, the clausal complement construc-tion is missed by the parser.
Since the verb indi-cate has weight 3, this leads to a recall error in thecurrent setting.
(8) .
.
.
indicating that dMyc overexpression cansubstitute for PI3K activation .
.
.Adjusting the weights of cues worked well gen-erally, but also caused unexpected problems, dueto what seem like inconsistencies in annotation.The examples below highlight the effect of low-ering the weight of predict from 4 to 3.
Exam-ples (9a) and (9b) are almost identical on surfaceand our system predicted both to be uncertain, dueto the fact that predicted took infinitival comple-ments in both cases.
However, only (9a) was an-notated as uncertain, leading to a precision error in(9b).
(9) (a) .
.
.
include all protein pairs predicted tohave posterior odds ratio .
.
.
(b) Protein pairs predicted to have a poste-rior odds ratio .
.
.The error cases in scope resolution are morevaried.
Syntax has a larger role in this task, andtherefore, parsing errors tend to affect the resultsmore directly.
In the following example, dur-ing left-expanding the scope of the modal auxil-iary could, RNAi screens, rather than the full nounphrase fruit fly RNAi screens, is identified as thepassive subject of the scope head (associated), be-cause an appropriate modifier dependency cannot75be found between the noun phrase head screensand either of the modifiers, fruit and fly.
(10) .
.
.
was to investigate whether fruit fly RNAiscreens of conserved genes could be asso-ciated with similar tick phenotypes and tickgene function.In general, the simple mechanism to exploitconstituent parse trees was useful in resolvingscope.
However, it appears that a nuanced ap-proach based on cue categories could enhance theresults further.
In particular, the current mecha-nism does not contribute much to resolving scopesof adverbial cues.
In the following example, parsetree mechanism does not have any effect, leadingto both a precision and a recall error in scope res-olution.
(11) (a) .
.
.
we will consider tightening the defi-nitions and possibly splitting them intodifferent roles.
(b) FP: possiblyFN: possibly splitting them into differ-ent rolesLeft/right expansion strategies were based onthe analysis of training data.
However, we en-countered errors caused by these strategies wherewe found the annotations contradictory.
In Exam-ple (12a), the entire fragment is in the scope ofthought, while in (12b), the scope of suggesteddoes not include it was, even though on surfaceboth fragments are very similar.
(12) (a) .
.
.
the kinesin-5 motor is thought to playa key role.
(b) .
.
.
it was suggested to enhance the nu-clear translocation of NF-?B.Post-processing in the form of citation strippingwas simplistic, and, therefore, was unable to han-dle complex cases, as the one shown in the exam-ple below.
The algorithm is only able to removeone reference at the end.
(13) (a) .
.
.
it is possible that some other sig-nalling system may operate with Semasto confine dorsally projecting neurons todorsal neuropile [3],[40],[41].
(b) FP: may operate with Semas to con-fine dorsally projecting neurons to dor-sal neuropile [3],[40],FN: may operate with Semas to con-fine dorsally projecting neurons to dor-sal neuropile5 ConclusionsRather than developing a dedicated methodologythat exclusively relies on the data provided by or-ganizers, we chose to extend and refine our priorwork in hedge detection and used the trainingdata only in a limited manner: to tune our sys-tem in a principled way.
With little tuning, weachieved the highest precision in Task 1B.
Wewere able to capitalize on the overlap betweenhedging cues and weasel words for Task 1W andachieved competitive results.
Adapting our pre-vious work in scope resolution to Task 2, how-ever, was less straightforward, due to the incom-patible definitions of scope.
Nevertheless, by re-fining the prior dependency-based heuristics withleft and right expansion strategies and utilizing asimple mechanism for parse tree information, wewere able to accommodate the new definition ofscope to a large extent.
With these results, we con-clude that our methodology is portable and easilyextensible.While the results show that using the parse treeinformation for scope resolution benefited our per-formance greatly, error analysis presented in theprevious sections also suggests that a finer-grainedapproach based on cue categories could furtherimprove results, and we aim to explore this exten-sion further.ReferencesMarie-Catherine deMarneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation, pages 449?454.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to Detect Hedges andtheir Scope in Natural Language Text.
In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning (CoNLL-2010): SharedTask, pages 1?12, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Christiane Fellbaum.
1998.
WordNet: an electroniclexical database.
MIT Press, Cambridge, MA.Viola Ganter and Michael Strube.
2009.
FindingHedges by Chasing Weasels: Hedge Detection Us-ingWikipedia Tags and Shallow Linguistic Features.In Proceedings of the ACL-IJCNLP 2009 Confer-ence Short Papers, pages 173?176.76Ken Hyland.
1998.
Hedging in scientific research ar-ticles.
John Benjamins B.V., Amsterdam, Nether-lands.Halil Kilicoglu and Sabine Bergler.
2008.
Recogniz-ing speculative language in biomedical research ar-ticles: a linguistically motivated perspective.
BMCBioinformatics, 9 Suppl 11:s10.Halil Kilicoglu and Sabine Bergler.
2009.
Syntac-tic dependency based heuristics for biological eventextraction.
In Proceedings of Natural LanguageProcessing in Biomedicine (BioNLP) NAACL 2009Workshop, pages 119?127.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.2008.
Corpus annotation for mining biomedicalevents from literature.
BMC Bioinformatics, 9:10.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 Shared Task on Event Extraction.In Proceedings of Natural Language Processingin Biomedicine (BioNLP) NAACL 2009 Workshop,pages 1?9.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41th Meeting of the Association for ComputationalLinguistics, pages 423?430.Shalom Lappin.
2000.
An intensional parametric se-mantics for vague quantifiers.
Linguistics and Phi-losophy, 23(6):599?620.Marc Light, Xin Y. Qiu, and Padmini Srinivasan.
2004.The language of bioscience: facts, speculations, andstatements in between.
In BioLINK 2004: LinkingBiological Literature, Ontologies and Databases,pages 17?24.Alexa T. McCray, Suresh Srinivasan, and Allen C.Browne.
1994.
Lexical methods for managing vari-ation in biomedical terminologies.
In Proceedingsof the 18th Annual Symposium on Computer Appli-cations in Medical Care, pages 235?239.Ben Medlock and Ted Briscoe.
2007.
Weakly su-pervised learning for hedge classification in scien-tific literature.
In Proceedings of the 45th Meet-ing of the Association for Computational Linguis-tics, pages 992?999.Roser Morante and Walter Daelemans.
2009.
Learn-ing the scope of hedge cues in biomedical texts.
InProceedings of the BioNLP 2009 Workshop, pages28?36.Arzucan O?zgu?r and Dragomir R. Radev.
2009.
Detect-ing speculations and their scopes in scientific text.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1398?1407.Bo Pang and Lillian Lee.
2008.
Sentiment Analysisand Opinion Mining.
Now Publishers Inc, Boston,MA.James Pustejovsky, Robert Knippen, Jessica Littman,and Roser Saur??.
2005.
Temporal and event in-formation in natural language text.
Language Re-sources and Evaluation, 39(2):123?164.Victoria L. Rubin, Elizabeth D. Liddy, and NorikoKando.
2005.
Certainty identification in texts: Cat-egorization model and manual tagging results.
InJames G. Shanahan, Yan Qu, and Janyce Wiebe, ed-itors, Computing Attitude and Affect in Text: The-ories and Applications, volume 20, pages 61?76.Springer Netherlands, Dordrecht.Roser Saur??
and James Pustejovsky.
2009.
FactBank:a corpus annotated with event factuality.
LanguageResources and Evaluation, 43(3):227?268.Roser Saur?
?, Marc Verhagen, and James Pustejovsky.2006.
Annotating and recognizing event modality intext.
In Proceedings of 19th International FLAIRSConference.Roser Saur??.
2008.
A Factuality Profiler for Eventual-ities in Text.
Ph.D. thesis, Brandeis University.Gyo?rgy Szarvas.
2008.
Hedge classification inbiomedical texts with a weakly supervised selectionof keywords.
In Proceedings of the 46th Meetingof the Association for Computational Linguistics,pages 281?289.Paul Thompson, Giulia Venturi, John McNaught,Simonetta Montemagni, and Sophia Ananiadou.2008.
Categorising modality in biomedical texts.
InProceedings of LREC 2008 Workshop on Buildingand Evaluating Resources for Biomedical Text Min-ing.Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008.
The Bio-Scope corpus: biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9 Suppl 11:S9.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation, 39(2):165?210.77
