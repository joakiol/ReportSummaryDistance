Extracting Important Sentences with Support Vector MachinesTsutomu HIRAO and Hideki ISOZAKI and Eisaku MAEDANTT Communication Science Laboratories2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0237 Japan{hirao,isozaki,maeda}@cslab.kecl.ntt.co.jpYuji MATSUMOTOGraduate School of Information and Science, Nara Institute of Science and Technology8516-9, Takayama, Ikoma, Nara 630-0101 Japanmatsu@is.aist-nara.ac.jpAbstractExtracting sentences that contain important in-formation from a document is a form of textsummarization.
The technique is the key to theautomatic generation of summaries similar tothose written by humans.
To achieve such ex-traction, it is important to be able to integrateheterogeneous pieces of information.
One ap-proach, parameter tuning by machine learning,has been attracting a lot of attention.
This pa-per proposes a method of sentence extractionbased on Support Vector Machines (SVMs).
Toconfirm the method?s performance, we conductexperiments that compare our method to threeexisting methods.
Results on the Text Summa-rization Challenge (TSC) corpus show that ourmethod offers the highest accuracy.
Moreover,we clarify the different features effective for ex-tracting different document genres.1 IntroductionExtracting important sentences means extract-ing from a document only those sentences thathave important information.
Since some sen-tences are lost, the result may lack coherence,but important sentence extraction is one ofthe basic technologies for generating summariesthat are useful for humans to browse.
There-fore, this technique plays an important role inautomatic text summarization.Many researchers have been studied impor-tant sentence extraction since the late 1950?s(Luhn, 1958).
Conventional methods focus onsentence features and define significance scores.The features include key words, sentence posi-tion, and certain linguistic clues.
Edmundson(1969) and Nobata et al (2001) have proposedscoring functions to integrate heterogeneous fea-tures.
However, we can not tune the parametervalues by hand when the number of features islarge.When a large quantity of training data isavailable, tuning can be effectively realized bymachine learning.
In recent years, machinelearning has attracted attention in the field ofautomatic text summarization.
Aone et al(1998) and Kupiec et al (1995) employedBayesian classifiers, Mani et al (1998), Nomotoet al (1997), Lin (1999), and Okumura etal.
(1999) used decision tree learning.
How-ever, most machine learning methods overfit thetraining data when many features are given.Therefore, we need to select features carefully.Support Vector Machines (SVMs) (Vapnik,1995) is robust even when the number offeatures is large.
Therefore, SVMs haveshown good performance for text categoriza-tion (Joachims, 1998), chunking (Kudo andMatsumoto, 2001), and dependency structureanalysis (Kudo and Matsumoto, 2000).In this paper, we present an important sen-tence extraction technique based on SVMs.
Weverified the technique against the Text Summa-rization Challenge (TSC) (Fukushima and Oku-mura, 2001) corpus.2 Important Sentence Extractionbased on Support Vector Machines2.1 Support Vector Machines (SVMs)SVM is a supervised learning algorithm for 2-class problems.Training data is given by(x1, y1), ?
?
?
, (xu, yu), xj ?
Rn, yj ?
{+1,?1}.Here, xjis a feature vector of the j-thsample; yjis its class label, positive(+1) ornegative(?1).
SVM separates positive and neg-ative examples by a hyperplane defined byw ?
x + b = 0, w ?
Rn, b ?
R, (1)PositiveNegativemarginwx + b = 0wx + b = 1wx + b = -1Support VectorFigure 1: Support Vector Machines.where ???
represents the inner product.In general, such a hyperplane is not unique.Figure 1 shows a linearly separable case.
TheSVM determines the optimal hyperplane bymaximizing the margin.
A margin is the dis-tance between negative examples and positiveexamples.Since training data is not necessarily linearlyseparable, slack variables (?j) are introduced forall xj.
These ?jincur misclassification error,and should satisfy the following inequalities:w ?
xj + b ?
1?
?jw ?
xj + b ?
?1 + ?j .
(2)Under these constraints, the following objectivefunction is to be minimized.12||w||2 + Cu?j=1?j .
(3)The first term in (3) corresponds to the sizeof the margin and the second term representsmisclassification.By solving a quadratic programming prob-lem, the decision function f(x) = sgn(g(x)) canbe derived whereg(x) =( ?i=1?iyixi ?
x+ b).
(4)The decision function depends on only sup-port vectors (xi).
Training examples, exceptfor support vectors, have no influence on thedecision function.Non-linear decision surfaces can be realizedby replacing the inner product of (4) with a ker-nel function K(x ?
xi) :g(x) =( ?i=1?iyiK(xi,x) + b).
(5)In this paper, we use polynomial kernel func-tions that have been very effective when appliedto other tasks, such as natural language pro-cessing (Joachims, 1998; Kudo and Matsumoto,2001; Kudo and Matsumoto, 2000):K(x,y) = (x ?
y + 1)d. (6)2.2 Sentence Ranking by using SupportVector MachinesImportant sentence extraction can be regardedas a two-class problem: important or unimpor-tant.
However, the proportion of important sen-tences in training data will differ from that inthe test data.
The number of important sen-tences in a document is determined by a sum-marization rate that is given at run-time.
Asimple solution for this problem is to rank sen-tences in a document.
We use g(x) the distancefrom the hyperplane to x to rank the sentences.2.3 FeaturesWe define the boolean features discussed belowthat are associated with sentence Siby takingpast studies into account (Zechner, 1996; No-bata et al, 2001; Hirao et al, 2001; Nomotoand Matsumoto, 1997).We use 410 boolean variables for each Si.Where x = (x[1], ?
?
?
, x[410]).
A real-valued fea-ture normalized between 0 and 1 is representedby 10 boolean variables.
Each variable corre-sponds to an internal [i/10,(i + 1)/10) wherei = 0 to 9.
For example, Posd = 0.75 is rep-resented by ?0000000100?
because 0.75 belongsto [7/10,8/10).Position of sentencesWe define three feature functions for the posi-tion of Si.
First, Lead is a boolean that corre-sponds to the output of the lead-based methoddescribed below1 .
Second, Posd is Si?s positionin a document.
Third, Posp is Si?s position in aparagraph.
The first sentence obtains the high-est score, the last obtains the lowest score:1 When a sentence appears in the first N of document,we assign 1 to the sentence.
An N was given for eachdocument by TSC committee.Posd(Si) = 1?BD(Si)/|D(Si)|Posp(Si) = 1?BP (Si)/|P (Si)|.Here, |D(Si)| is the number of characters inthe document D(Si) that contains Si; BD(Si)is the number of characters before Siin D(Si);|P (Si)| is the number of characters of the para-graph P (Si) that contains Si, and BP (Si) isthe number of characters before Siin the para-graph.Length of sentencesWe define a feature function that addresses thelength of sentence asLen(Si) = |Si|/ maxSz?D(Si)|Sz|.Here, |Si| is the number of characters of sen-tence Si, and maxSz?D|Sz| is the maximumnumber of characters in a sentence that belongsto D(Si).In addition, the length of a previous sentenceLen?1(Si) = Len(Si?1) and the length of a nextsentence Len+1(Si) = Len(Si+1) are also fea-tures of sentence Si.Weight of sentencesWe defined the feature function that weightssentences based on frequency-based wordweighting asWf (Si) =?ttf(t, Si) ?
w(t, D(Si)).Here, Wf(Si) is the summention of weightingw(t, D(Si)) of words that appear in a sentence.tf(t, Si) is term frequency of t in Si.
We usedonly nouns.
In addition, we define word weightw(t, D(Si)) based on a specific field (Hara et al,1997):w(t, D(Si)) = ?(1TT?z=1?zVz)+?
(tf(t, D(Si))?t?
tf(t?, D(Si))).Here, T is the number of sentence in a docu-ment, and Vzis the number of words in sentenceSz?
D(Si) (repetitions are ignored).
Also, ?zisa boolean value: that is 1 when t appears inSz.The first term of the equation above is theweighting of a word in a specific field.
The sec-ond term is the occurrence probability of wordt.We set parameters ?
and ?
as 0.8, 0.2, re-spectively.
The weight of a previous sentenceWf?1(Si)=Wf (Si?1), and the weight of a nextsentence Wf +1(Si)=Wf (Si+1) are also featuresof sentence Si.Density of key wordsWe define the feature function Den(Si) thatrepresents density of key words in a sentenceby using Hanning Window function (fH(k, m)):Den(Si) = maxmm+Win/2?k=m?Win/2fH(k,m) ?
a(k, Si),where fH(k, m) is given byfH(k,m) ={12(1 + cos2?
k?mW in)(|k ?m| ?
Win/2)0 (|k ?m| > Win/2).The key words (KW ) are the top 30% ofwords in a document according to w(t, D(Si)).Also, m is the center position of the window,Win = |Si|/2.
In addition, a(k, Si) is defined asfollows:a(k, Si) =????
?w(t,D) Where a word t (?
KW ) beginsat k0 k is not the beginning positionof a word in KW.Named Entitiesx[r]=1 (1?r?8) indicates that a certain NamedEntity class appears in Si.
The number ofNamed Entity classes is 8 (Sekine and Eriguchi,2000), e.g., PERSON, LOCATION.
We useIsozaki?s NE recognizer (Isozaki, 2001).Conjunctionsx[r]=1 (9?r?61) if and only if a certain con-junction is used in the sentence.
The number ofconjunctions is 53.Functional wordsx[r]=1 (62?r?234) if and only if a certain func-tional word such as ga, ha, and ta is used inthe sentence.
The number of functional wordsis 173.Part of speechx[r]=1 (235?r?300) if and only if a certain partof speech such as ?Noun-jiritsu?
and ?Verb-jiritsu?
is used in the sentence.
The numberof part of speech is 66.Semantical depth of nounsx[r]=1 (301?r?311) if and only if Sicontainsa noun at a certain semantical depth accordingto a Japanese lexicon, Goi-Taikei (Ikehara et al,1997).
The number of depth levels is 11.
Forinstance, Semdep=2 means that a noun in Sibelongs to the second depth level.Document genrex[r]=1 (312?r?315) if and only if the docu-ment belongs to a certain genre.
The genre isexplicitly written in the header of each docu-ment.
The number of genres is four: General,National, Editorial, and Commentary.Symbolsx[r]=1 (r=316) if and only if sentence includesa certain symbol (for example: ?,&,).Conversationx[r]=1 (r=317) if and only if Siincludes a con-versation style expression.Assertive expressionsx[r]=1 (r=318) if and only if Siincludes an as-sertive expression.3 Experimental settings3.1 CorpusWe used the data set of TSC (Fukushima andOkumura, 2001) summarization collection forour evaluation.
TSC was established as a sub-task of NTCIR-2 (NII-NACSIS Test Collectionfor IR Systems).
The corpus consists of 180Japanese documents2 from the Mainichi News-papers of 1994, 1995, and 1998.
In each doc-ument, important sentences were manually ex-tracted at summarization rates of 10%, 30%,and 50%.
Note that the summarization ratesdepend on the number of sentences in a doc-ument not the number of characters.
Table 1shows the statistics.3.2 Evaluated methodsWe compared four methods: decision tree learn-ing, boosting, lead, and SVM.
At each summa-rization rate, we trained classifiers and classifiedtest documents.Decision tree learning methodWe used C4.5 (Quinlan, 1993) for our experi-ments with the default settings.
We used the2 Each document is presented in SGML style with sen-tence and paragraph separators attached.features described in section 2.
Sentences wereranked according to their certainty factors givenby C4.5.Boosting methodWe used C5.0, which applies boosting to deci-sion tree learning.
The number of rounds wasset to 10.
Sentences were ranked according totheir certainty factors given by C5.0.Lead-based methodThe first N sentences of a document were se-lected.
N was determined according to the sum-marization rates.SVM methodThis is our method as outlined in section 2.
Weused the second-order polynomial kernel, andset C (in equation (3)) as 0.0001.
We usedTinySVM3 .3.3 Measures for evaluationIn the TSC corpus, the number of sentences tobe extracted was explicitly given by the TSCcommittee.
When we extract sentences accord-ing to that number, Precision, Recall, and F-measure become the same value.
We call thisvalue Accuracy.
Accuracy is defined as follows:Accuracy = b/a ?
100,where a is the specified number of importantsentences, and b is the number of true impor-tant sentences that were contained in system?soutput.4 ResultsTable 2 shows the results of five-fold cross vali-dation by using all 180 documents.For all summarization rates and all genres,SVM achieved the highest accuracy, the lead-based method the lowest.
Let the null hypoth-esis be ?There are no differences among thescores of the four methods?.
We tested this nullhypothesis at a significance level of 1% by usingTukey?s method.
Although the SVM?s perfor-mance was best, the differences were not sta-tistically significant at 10%.
At 30% and 50%,SVM performed better than the other methodswith a statistical significance.3 http://cl.aist-nara.ac.jp/?taku-ku/software/TinySVM/Table 1: Details of data sets.General National Editorial Commentary# of documents 16 76 41 47# of sentences 342 1721 1362 1096# of important sentences (10%) 34 172 143 112# of important sentences (30%) 103 523 414 330# of important sentences (50%) 174 899 693 555Table 2: Evaluation results of cross validation.Summarization rate 10%Genre SVM C4.5 C5.0 LeadGeneral 55.7 55.2 52.4 47.9Editorial 34.2 33.6 27.9 31.6National 61.4 52.0 56.3 51.8Commentary 28.7 27.4 21.4 15.9Average 46.2 41.4 40.4 37.4Summarization rate 30%Genre SVM C4.5 C5.0 LeadGeneral 51.0 45.7 50.4 50.5Editorial 47.8 41.6 43.3 36.7National 55.9 44.1 49.3 54.3Commentary 48.7 39.4 40.1 32.4Average 51.6 42.4 45.7 44.2Summarization rate 50%Genre SVM C4.5 C5.0 LeadGeneral 65.2 63.0 60.2 60.4Editorial 60.6 54.1 54.6 51.0National 63.3 58.7 58.7 61.5Commentary 65.7 59.6 60.6 50.4Average 63.5 58.2 58.4 56.15 DiscussionTable 2 shows that Editorial and Commentaryare more difficult than the other genres.
Wecan consider two reasons for the poor scores ofEditorial and Commentary:?
These genres have no feature useful for dis-crimination.?
Non-standard features are useful in thesegenres.Accordingly, we conduct an experiment toclarify genre dependency4 .4 We did not use General because the number of doc-uments in this genre was insufficient.1 Extract 36 documents at random fromgenre i for training.2 Extract 4 documents at random from genrej for test.3 Repeat this 10 times for all combinationsof (i, j).Table 3 shows that the result implies thatnon-standard features are useful in Editorialand Commentary documents.Now, we examine effective features in eachgenre.
Since we used the second order polyno-mial kernel, we can expand g(x) as follows:g(x) = b+?i=1wi + 2?i=1wiu?k=1xi[k]x[k] +?i=1wiu?h=1u?k=1xi[h]xi[k]x[h]x[k ], (7)where ) is the number of support vectors, andwiequals ?iyi.We can rewrite it as follows when all vectorsare boolean:g(x) = W0+u?k=1W1[k]x[k] +u?1?h=1u?k=h+1W2[k, h]x[h]x[k] (8)whereW0 = b +?i=1 wi,W1[k] = 3?i=1 wixi[k], andW2[h, k] = 2?i=1 wixi[h]xi[k].Therefore, W1[k] indicates the significance ofan individual feature and W2[h, k] indicates thesignificance of a feature pair.
When |W1[k]| or|W2[h, k]| was large, the feature or the featurepair had a strong influence on the optimal hy-perplane.Table 3: Evaluation results for three genres.Training \ TestNational Editorial Commentary10% 30% 50% 10% 30% 50% 10% 30% 50%National 63.4 57.6 65.5 32.8 39.4 53.6 24.0 39.5 60.8Editorial 49.3 46.8 58.4 33.9 49.1 64.4 24.9 43.6 62.1Commentary 37.4 43.3 61.1 18.4 41.8 57.8 30.6 49.6 67.0Table 4: Effective features and their pairsSummarization rate 10%National Editorial CommentaryLead ?
ga 0.9?Posd?1.0 ?
0.7?Wf<0.8 0.9?P osd?1.0 ?
Semdep=20.9?Posd?1.0 ?
ga NE ?
de 0.5?Len+1<0.6 ?
Noun-hijiritsuLead ?
ta 0.9?Posd?1.0 ?
de 0.0?P osp<0.1 ?
0.5?Wf+1<0.60.9?Posd?1.0 ?
ta Lead ?
0.7?Wf<0.8 0.8?P osd<0.9 ?
ParticleSummarization rate 30%National Editorial CommentaryLead ?
Semdep=6 0.0?Posp<0.1 ?
ga Aux verb ?
Semdep=20.9?Posd?1.0 ?
Semdep=6 0.9?Posd?1.0 ?
NE Verb-jiritsu ?
Semdep=2Lead ?
ga Lead ?
NE Semdep=20.9?Posd?1.0 0.0?P osd<0.1 0.0?Posp<0.1 ?
0.5?Den<0.6Summarization rate 50%National Editorial CommentaryLead 0.0?Posp<0.1 ?
Semdep=6 0.0?Posp<0.1 ?
ParticleLead ?
ha 0.0?Posp<0.1 ?
ga 0.2?P osd<0.3Lead ?
Verb-jiritsu 0.0?Posp<0.1 0.4?Len<0.5Lead ?
ta 0.0?P osd<0.1 0.0?Posp<0.1Table 4 shows some of the effective featuresthat had large weights W1[k], W2[h, k] for eachgenre.Effective features common to three genres atthree rates were sentence positions.
Since Na-tional has a typical newspaper style, the begin-ning of the document was important.
More-over, ?ga?
and ?ta?
were important.
Thesefunctional words are used when a new event isintroduced.In Editorial and Commentary, the end of aparagraph and that of a document were impor-tant.
The reason for this result is that subtopicor main topic conclusions are common in thosepositions.
This implies that National has a dif-ferent text structure from Editorial and Com-mentary.Moreover, in Editorial, ?de?
and sentenceweight was important.
In Commentary, seman-tically shallow words, sentence weight and thelength of a next sentence were important.In short, we confirmed that the feature(s) ef-fective for discriminating a genre differ with thegenre.6 ConclusionThis paper presented a SVM-based importantsentence extraction technique.
Comparisonswere made using the lead-based method, deci-sion tree learning method, and boosting methodwith the summarization rates of 10%, 30%,and 50%.
The experimental results show thatthe SVM-based method outperforms the othermethods at all summarization rates.
Moreover,we clarified the effective features for three gen-res, and showed that the important featuresvary with the genre.In our future work, we would like to apply ourmethod to trainable Question Answering Sys-tem SAIQA-II developed in our group.AcknowledgementWe would like to thank all the members of theKnowledge Processing Research Group for valu-able comments and discussions.ReferencesC.
Aone, M. Okurowski, and J. Gorlinsky.
1998.Trainable Scalable Summarization Using Ro-bust NLP and Machine Learning.
Proc.
of the17th COLING and 36th ACL, pages 62?66.H.
Edmundson.
1969.
New methods inautomatic abstracting.
Journal of ACM,16(2):246?285.T.
Fukushima and M. Okumura.
2001.
TextSummarization Challenge Text summariza-tion evaluation in Japan.
Proc.
of theNAACL2001 Workshop on Automatic sum-marization, pages 51?59.M.
Hara, H. Nakajima, and T. Kitani.
1997.Keyword Extraction Using a Text Formatand Word Importance in a Specific Filed (inJapanese).
Transactions of Information Pro-cessing Society of Japan, 38(2):299?309.T.
Hirao, M. Hatayama, S. Yamada, andK.
Takeuchi.
2001.
Text Summarizationbased on Hanning Window and Dependencystructure analysis.
Proc.
of the 2nd NTCIRWorkshop, pages 349?354.S.
Ikehara, M. Miyazaki, S. Shirai, A. Yokoo,H.
Nakaiwa, K. Ogura, Y. Ooyama, andY.
Hayashi.
1997.
Goi-Taikei ?
A JapaneseLexicon (in Japanese).
Iwanami Shoten.H.
Isozaki.
2001.
Japanese Named EntityRecognition based on Simple Rule Generatorand Decision Tree Learning.
Proc.
of the 39thACL, pages 306?313.T.
Joachims.
1998.
Text Categorization withSupport Vector Machines: Learning withMany Relevant Features.
Proc.
of ECML,pages 137?142.T.
Kudo and Y. Matsumoto.
2000.
Japane De-pendency Structure Analysis Based on Su-port Vector Machines.
Proc.
of EMNLP andVLC, pages 18?25.T.
Kudo and Y. Matsumoto.
2001.
Chunkingwith Support Vector Machine.
Proc.
of the2nd NAACL, pages 192?199.J.
Kupiec, J. Pedersen, and F. Chen.
1995.
ATrainable Document Summarizer.
Proc.
ofthe 18th ACM-SIGIR, pages 68?73.Chin-Yew Lin.
1999.
Training a Selection Func-tion for Extraction.
Proc.
of the 18th ACM-CIKM, pages 55?62.H.
Luhn.
1958.
The Automatic Creation of Lit-erature Abstracts.
IBM Journal of Researchand Development, 2(2):159?165.I.
Mani and E. Bloedorn.
1998.
Machine Learn-ing of Generic and User-Focused Summariza-tion.
Proc.
of the 15th AAAI, pages 821?826.C.
Nobata, S. Sekine, M. Murata, K. Uchimoto,M.
Utiyama, and H. Isahara.
2001.
SentenceExtraction System Assembling Multiple Ev-idence.
Proc.
of the 2nd NTCIR Workshop,pages 319?324.T.
Nomoto and Y. Matsumoto.
1997.
The Reli-ability of Human Coding and Effects on Auto-matic Abstracting (in Japanese).
The SpecialInterest Group Notes of IPSJ (NL-120-11),pages 71?76.M.
Okumura, Y. Haraguchi, and H. Mochizuki.1999.
Some Observations on AutomaticText Summarization Based on Decision TreeLearning (in Japanese).
Proc.
of the 59th Na-tional Convention of IPSJ (5N-2), pages 393?394.J.
Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann.S.
Sekine and Y. Eriguchi.
2000.
JapaneseNamed Entity Extraction Evaluation - Anal-ysis of Results -.
Proc.
of the 18th COLING,pages 1106?1110.V.
Vapnik.
1995.
The Nature of StatisticalLearning Theory.
New York.K.
Zechner.
1996.
Fast Generation of Abstractsfrom General Domain Text Corpora by Ex-tracting Relevant Sentences.
Proc.
of the 16thCOLING, pages 986?989.
