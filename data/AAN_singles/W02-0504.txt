An HMM Approach to Vowel Restoration in Arabic and HebrewYa?akov GalDivision of Engineering and Applied SciencesHarvard UniversityCambridge, MA 02138gal@eecs.harvard.eduAbstractSemitic languages pose a problem toNatural Language Processing sincemost of the vowels are omitted fromwritten prose, resulting in consider-able ambiguity at the word level.However, while reading text, nativespeakers can generally vocalize eachword based on their familiarity withthe lexicon and the context of theword.
Methods for vowel restorationin previous work involving morpho-logical analysis concentrated on a sin-gle language and relied on a parsedcorpus that is difficult to create formany Semitic languages.
We showthat Hidden Markov Models are a use-ful tool for the task of vowel restora-tion in Semitic languages.
Our tech-nique is simple to implement, doesnot require any language specificknowledge to be embedded in themodel and generalizes well to bothHebrew and Arabic.
Using a publiclyavailable version of the Bible and theQur?an as corpora, we achieve a suc-cess rate of 86% for restoring the ex-act vowel pattern in Arabic and 81%in Hebrew.
For Hebrew, we also re-port on 87% success rate for restoringthe correct phonetic value of thewords.1 IntroductionIn both Hebrew and in Arabic, modern writ-ten texts are composed in script that leaves outmost of the vowels of the words.
Because manywords that have different vowel patterns mayappear identical in a vowel-less setting, consid-erable ambiguity exists at the word level.In Hebrew, Levinger et al (1995) computedthat 55% out of 40,000 word tokens taken froma corpus of the Israeli daily Ha?aretz were am-biguous.
For example, the non-voweled Hebrewword , written in Latin transliteration as SPR,may represent the noun ?book?
(pronounced/sepher/), the third person singular form of theverb ?to count?
(pronounced /saphar/) or at leastfour other possible interpretations.
In Arabic,there are almost five possible morphologicalanalyses per word on average (Beesley 1998).Take, for example, the Arabic word  , writ-ten in Latin transliteration as KTAAB.
One pos-sible interpretation is the noun ?book?
(pro-nounced /kitaab/) and another is the plural of thenoun ?secretary?, (pronounced /kuttaab/).
Fur-ther contributing to this ambiguity is the factthat Hebrew and Arabic morphology is com-plex: most words are derived from roots that arecast in templates that govern the ordering ofletters and provide semantic information.
Inaddition, prefixes and suffixes can also be at-tached to words in a concatenative manner, re-sulting in a single string that represents verbinflections, prepositions, pronouns, and connec-tives.Vowel restoration in Hebrew and Arabic textis a non-trivial task.
In both languages, vowelsare marked by both letters and diacritics.
InHebrew, there are twelve different vowel diacrit-ics, and in general, most diacritics are left out ofmodern script.
In Arabic, there are six vowels,which can be divided into three pairs consistingof a short vowel and a long vowel.
Each paircorresponds to a different phonetic value.
Inwritten Arabic text, the short vowels are gener-ally left out.Surprisingly, native speakers of Arabic orHebrew can, in most cases, accurately vocalizewords in text based on their context and thespeaker?s knowledge of the grammar and lexi-con of the language.
However, speakers of He-brew are not as successful in restoring the exactvowel diacritics of words.
Since many vowelshave the same pronunciation in modern Hebrew,and speakers of Hebrew generally use non-voweled script in reading and writing text, theyare not familiar with the precise vowel pattern ofwords.Throughout this paper, we refer to a wordthat is fully voweled,1  i.e.
supplied with its fulldiacritical marking, as diacritisized (Beesley1998).
A system that could restore the diacri-tisized form of scripts, i.e.
supply the full dia-critical markings, would greatly benefit non-native speakers, sufferers of dyslexia and couldassist in diacritisizing children?s and poetrybooks, a task that is currently done manually.2 A Statistical ApproachIdentifying contextual relationships is crucialin deciphering lexical ambiguities in both He-brew and Arabic and is commonly used by na-tive speakers.
Hidden Markov Models have beentraditionally used to capture the contextual de-pendencies between words (Charniak 1995).
Wedemonstrate the utility of Hidden Markov Mod-els for the restoration of vowels in Hebrew andArabic.
As we show, our model is straightfor-ward and simple to implement.
It consists ofhidden states that correspond to diacritisizedwords from the training corpus, in which eachhidden state has a single emission leading to anundiacritisized (non-voweled) word observation.Our model does not require any handcraftedlinguistic knowledge and is robust in the sensethat it generalizes well to other languages.
Therest of this paper is organized as follows: inSection 3, we provide an explanation of thecorpora we used in our experiment.
Section 4and 5 describe the models we designed as wellas our experimental setup for evaluating them.Section 6 describes related work done in mor-phological analysis and vowel restoration in1In literature relating to Hebrew morphologyanalysis, this is often refered to as a pointed word.Hebrew and in Arabic.
Finally, Section 7 dis-cusses future work.3 Evaluation MethodologyWe compare a baseline approach using aunigram model to a bigram model.
We trainboth models on a corpus of diacritisized text,and then check the models?
performance on anunseen test set, by removing the vowel diacriticsfrom part of the corpus.
For both Hebrew andArabic, we evaluate performance by measuringthe percentage of words in the test set whosevowel pattern was restored correctly, i.e.
thevowel pattern suggested by the system exactlymatched the original.
We refer to thisperformance measure as word accuracy.
ForHebrew, we also divided the vowel symbols intoseparate groups, each one corresponding to aspecific phonetic value.
We then measured thepercentage of words whose individual letterswere fitted with a vowel diacritic belonging tothe same phonetic group as the correct voweldiacritic in the test set.
In other words, therestored vowels, while perhaps not agreeingexactly with the original pattern, all belonged tothe correct phonetic group.
This performancemeasure, which corresponds to vocalization ofnon-voweled text, is useful for applications suchas text-to-speech systems.2  We refer to thisperformance measure as phonetic groupaccuracy.There is an unfortunate lack of data forvowel-annotated text in both modern Hebrewand Arabic.
The only easily accessible sourcesare the Hebrew Bible and the Qur?an, for whichon-line versions transliterated into Latin charac-ters are available.
Ancient Hebrew and Arabicbear enough syntactical and semantic resem-blance to their modern language equivalents tojustify usage of these ancient texts as corpora.For Hebrew, we used the Westminster HebrewMorphological Database (1998), a corpus con-taining a complete transcription of the graphicalform of the Massoretic text of the Hebrew Biblecontaining roughly 300,000 words.
For theQur?an, we used the transliterated version pub-licly available from the sacred text archive at2In modern Hebrew, it is generally sufficient toassociate each vowel symbol with its phonetic groupin order to vocalize the word correctly.ba-raelo-himmissing#www.sacred-texts.com.
This corpus containsroughly 90,000 words.For both languages, we tested our model on10% of the corpus.
We measured performanceby evaluating word accuracy for both Hebrewand Arabic.
In addition, we measured phoneticgroup accuracy for Hebrew.4 Baseline : A Unigram ModelTo assess the difficulty of the problem, wecounted the number of times each diacriticizedword appeared in the training set.
For each non-voweled word encountered in the test set, wesearched through all of the words with the samenon-voweled structure and picked thediacriticized word with the highest count in thetable.
Figure 1 shows the ambiguity distributionin the training set.Figure 1.
Ambiguity Distribution inTraining Set01020300 2 3 4 5 6 7 8 9 10 11No.
of Possible Interpretations per WordPercentageofTrainingSetHebrew ArabicNote that for both languages, only about 30%of the words in the training set wereunambiguous, i.e.
had a single interpretation.For the baseline model, we achieved a wordaccuracy rate of 68% for Hebrew and 74% forArabic.
We note that even though the size of theArabic training set was about a third of the sizeof the Hebrew training set, we still achieved ahigher success rate of restoring vowels inArabic.
We attribute this to the fact that thereare only three possible missing vowel diacriticsin modern Arabic text, compared to twelve inHebrew.5 A Bigram ModelWe constructed a bigram Hidden MarkovModel (HMM) where hidden states werevowel-annotated (diacritisized) words, and ob-servations were vowel-less words.
One exampleof a path through the HMM for reconstructing aHebrew sentence is given in Figure 2; ovalsrepresent hidden states that correspond to diacri-tisized words; rectangles represent observationsof vowel-less words; solid edges link the statesthat mark the transition through the model forgenerating the desired sentence; each edge car-ries with it a probability mass, representing theprobability of transitioning between the twohidden states connected by the edge.
This tech-nique was used for Arabic in a similar way.Our model consists of a set of hidden statesnTT ,..,1  where each hidden state corresponds toBR^$YTBRbe-re-shit^LWHYMFigure 2.
HMM path for the non-voweled phrase ?in the beginning god created?
?pronounced /be-reshit bara elohim/an observed word in our training corpus.
Thus,each hidden state corresponds to a word contain-ing its complete vowel pattern.
From each hid-den state iT , there is a single emission, whichsimply consists of the word in its non-voweledform.
If we make the assumption that the prob-ability of observing a given word depends onlyon the previous word, we can compute the prob-ability of observing a sentence nn wwW ,...,1,1 =by summing over all of the possible hiddenstates that the HMM traversed while generatingthe sentence, as denoted in the following equa-tion.
)1|()(1,1,1  ?+?=nTn TiTipiWpThese probabilities of transitions through thestates of the model are approximated by bigramcounts, as described below.
Note that the symbol?#?
in the figure serves to ?anchor?
the initialstate of the HMM and facilitate computation.Thereafter, the hidden states actually consist ofvowel-annotated bigrams.
The probability ofany possible path in our model that generatesthis phrase can be computed as follows:)|()( 1,1 ?
?= iin wwpiWpThis equation decomposes into the followingmaximum likelihood probability estimations,denoted by p?
,  in which c(word)  denotes thenumber of instances that word had occurred inthe training set and c(word1, word2)  denotes thenumber of joint occurrences of word1 andword2 in the training set.)(),()|(?)(),()|(?),()|(#?,1)|#(#?himelochimelorabachimelorabaprabacrabashitrebecrabashitrebepshitrebecshitrebepp???=??????=?????=?
?=In order to be able to compute the likelihood ofeach bigram, we kept a look-up table consistingof counts for all individual and joint occurrencesin the training set.
We implemented the Viterbialgorithm to find the most likely path transitionsthrough the hidden states that correspond to theobservations.
The likelihood of observing thesentence nW ,1  while traversing the hidden statepath nT ,1  is taken to be ),( ,1,1 nn TWp .
We  ig-nore the normalizing factor )(,1 nWp .
Moreformally, the most likely path through the modelis defined as),(maxarg)(),(maxarg)|(maxarg,1,1,1,1,1,1,1,1,1,1nnnTnnnnTnnnTTWpWpTWpWTpr==5.1 Dealing with Sparse DataBecause our bigram model is trained from afinite corpus, many words are bound to be miss-ing from it.
For example, in the unigram model,we found that as many as 16% of the Hebrewwords in the test set were not present.
Theamount of unseen bigrams was even higher, asmuch as 20 percent.
This is not surprising, as weexpect some unseen bigrams to consist of wordsthat were both seen before individually.
We didnot specifically deal with sparse data in the uni-gram base line model.As many of the unseen unigrams were nonambiguous, we would have liked to look up themissing words in a vowel-annotated dictionaryand copy the vowel pattern found in the diction-ary.
However, as noted in Section 2, morphol-ogy in both Hebrew and Arabic is non-concatenative.
Since dictionaries contain onlythe root form of verbs and nouns, without asound morphological analyzer we could notdecipher the root.
Therefore, proceeded as fol-lows: We employed a technique proposed byKatz (1987) that combines a discounting methodalong with a back-off method to obtain a goodestimate of unseen bigrams.
We use the Good-Turing discounting method (Gale & Sampson1991) to decide how much total probability massto set aside for all the events we haven?t seen,and a simple back-off algorithm to tell us how todistribute this probability.
Formally, we define)1|2()1()1|2()1|2()1|2(wwpwwwpwwPwwp d?==Here, dP  is the discounted estimate using theGood-Turing method, p is a probability esti-mated by the number of occurrences and )1(w?is a normalizing factor that divides the unknownif c(w2,w1)>0if c(w2,w1)=0probability mass of unseen bigrams beginningwith w1.=>?
?=0)2,1(:20)2,1(:2)1|2(1)1|2(1)1(wwcwwwcwdwwpwwPw?In order to compute Pd we create a separatediscounting model for each word in the trainingset.
The reason for this is simple: If we use onlyone model over all of the bigram counts, wewould really be approximating )1,2( wwPd .Because we wish to estimate )1|2( wwPd , wedefine the discounted frequency counts as fol-lows:)2,1(1)2,1()2,1()2,1(*wwcwwcnnwwcwwc++=where cn  is the number of different bigrams inthe corpus that have frequency c. FollowingKatz, we estimate the probability of unseen bi-grams to bep(w2|w1) ?If the missing bigram is composed of two in-dividually observed words, this technique allowsus to estimate the probability mass of the unseenbigram.
In some cases, the unseen bigram con-sists of individual words that have never beenseen.
In other words, w2 itself is unseen andc(w2)  cannot be computed.
In this case, we es-timate the probability for p(w2|w1) by comput-ing p(unseen|w1).
We do this by allocating someprobability mass to unseen words, keeping aspecial count for bigrams that were seen lessthen k times.3  We allocate a separate hiddenstate for unseen words, as depicted in Figure 2.In this case, we do not attempt to fit any vowelpattern to the unseen word; the word is left bareof its diacritics.
However, we can still assign aprobability mass, p(unseen|w1), to prevent theViterbi algorithm from computing a zero prob-3k  was arbitrarily set to three in our experiment.Alternatively, we could get a more exact estimationof the missing probability mass by discounting theunigram probabilities of w2.ability.
We can compute the probabilitiesp(w2|unseen) in a similar manner.5.2 ResultsFigure 3.
Results of Bigram Model6065707580859040 60 90Percentage of Training DataWordAccuracyPercentageHebrew 1 Hebrew2 ArabicFigure 3 presents our results using the bigramHMM model, where ?Hebrew 1?
measures wordaccuracy be in Hebrew, ?Hebrew 2?
measuresphonetic group accuracy, and ?Arabic?measures word accuracy in Arabic.
Using thebigram model for Hebrew, we achieved 81%word accuracy and 87% phonetic groupaccuracy.
For Arabic, we achieved 86% wordaccuracy.
For Hebrew, the system was moresuccessful in restoring the phonetic group vowelpattern than restoring the exact diacritics.
This isbecause the number of possible vowel symbolsin Hebrew is larger than in Arabic.
However, fortext-to-speech systems, it is sufficient toassociate each vowel with the correct phoneticgroup.
For word accuracy, most of the errors inHebrew (11%) and in Arabic (8%) were due towords that were not found in the training corpus.Therefore, we believe that acquiring asufficiently large modern corpus of the languagewould greatly improve performance.
However,the number of parameters for our model isquadratic in the number of word types in thetraining set.
Therefore, we suggest using limitedmorphological analysis to improve performanceof the system by attempting to identify the stemor root of the words in the test set, as well as theconjugation.
Since conjugation templates inSemitic languages have fixed vowel patterns,even limited success in morphological analyseswould greatly improve performance of thesystem, while not incurring a blowup in thenumber of parameters.p(w2)      if c(w2) > 0p(unseen|w1)      if c(w2) = 06  Related WorkPerforming a full morphological analysis of aHebrew or Arabic sentence would greatly assistthe vowel restoration problem.
That is, if wecould correctly parse each word in a sentence,we could eliminate ambiguity and restore thecorrect vowel pattern of the word according toits grammatical form and part of speech.For Arabic, a morphological analyzer,developed by the Xerox Research Centre Europe(Beesley 1998) is freely available.4  The systemuses finite state transducers, traditionally usedfor modeling concatenative morphology.
Sincethe system is word based, it cannot disambiguatewords in context and outputs all possibleanalyses for each word.
The system relies onhandcrafted rules and lexicon that govern Arabicmorphology.For Hebrew, a morphological analyzer calledNakdan Text  exists, as part of the Rav Milimproject for the processing of modern Hebrew(Choueka and Neeman 1995).
Given a sentencein modern Hebrew,  Nakdan  Text restores itsvowel diacritics by first finding all possiblemorphological analyses and vowel patterns ofevery word in the sentence.
Then, for every suchword, it chooses the correct context-dependentvowel pattern using short-context syntacticalrules as well as some probabilistic models.
Theauthors report 95% success rate in restoringvowel patterns.
It is not clear if this refers toword accuracy or letter accuracy.5Segel (1997) devised a statistical Hebrewlexical analyzer that takes contextual dependen-cies into account.
Given a non-voweled Hebrewtexts as input and achieves 95% word accuracyon test data extracted from the Israeli dailyHa?aretz.
However, this method requires fullyanalyzed Hebrew text to train on.
Segel used amorphological hand-analyzed training set con-sisting of only 500 sentences.
Because there iscurrently no tree bank of analyzed Hebrew text,this method is not applicable to other domains,such as novels or medical texts.4http://www.arabic-morphology.com/5This program was demonstrated at BISFAI-95, thefifth Bar Ilan international symposium on ArtificialIntelligence, but no summary or article was includedin its proceedings, and to the best of our knowledgeno article has been published describing the methodsof Nakdan text.Kontorovich and Lee (2001) use an HMMapproach to vocalizing Hebrew text.
Theirmodel consists of fourteen hidden states, withemissions for each word of the training set.Initially, the parameters of the model are chosenat random and training of the model is doneusing the EM algorithm.
They achieve a successrate of 81%, when unseen words are discardedfrom the test set.7 Future WorkSince most of the errors in the model can beattributed to missing words, we plan to addressthis problem from two perspectives.
First, weplan to include a letter-based HMM to be usedfor fitting an unseen word with a likely vowelpattern.
The model would be trained separatelyon words from the training set.
Its hidden stateswould correspond to vowels in a language, mak-ing this model language dependent.
We alsoplan to use a trigram model for the task of vowelrestoration, backing off to a bigram model forsparse trigrams.Second, we plan to use some degree of mor-phological analysis to assist us with the restora-tion of unseen words.
At the very least, we coulduse a morphological analyzer as a dictionary forwords that have unique diacritization, but aremissing from the model.
Since analyzers forArabic that are commonly available  (Beesley1998) are word based, they output all possiblemorphological combinations of the word, and itis still unclear how we could choose the mostlikely parse given the context.Finally, since the size of our corpora isrelatively small, we also plan to use crossvalidation to get a better estimate of the gener-alization error.8 ConclusionIn this paper, we demonstrated the use of astatistically based approach for vowel restora-tion in Semitic languages.
We wish to demon-strate that HMMs are a useful tool for computa-tional processing of Semitic languages, and thatthese models generalize to other languages.
Forthe task of vocalizing the vowels according totheir phonetic classification, the system we haveproposed achieves an accuracy of 87% for He-brew.
For the task of restoring the exact vowelpattern, we achieved an accuracy of 81% forHebrew texts and 86% for Arabic texts.
Thus,we have shown that the contextual informationgained by using HMMs is beneficial for thistask.AcknowledgmentsWe would like to thank Stuart Shieber, Chris-tian R. Lange, Jill Nickerson, Emir Kapanci,Wheeler Ruml and Chung-chieh Shan for usefuldiscussions.ReferencesAcademy of the Hebrew Language  1957.
?Therules for Hebrew-Latin transcription,?
InMemiors of the Academy of the Hebrew Lan-guage, pages 5-8 (in Hebrew)Beesley, K. 1998.
"Arabic finite-statemorphological analysis and generation," inCOLING-96 Proceedings 1 : 89-94,Copenhagen.Charniak, E. 1995.
Statistical Language Learn-ing, MIT Press.Choueka, Y. and Neeman, Y.
1995.
?Nakdan-Text, (an In-Context Text-Vocalizer for Mod-ern Hebrew),?
BISFAI-95, The Fifth Bar IlanSymposium for Artificial IntelligenceDagan, D., Pereira P., and Lee L. 1994.
?Simi-larity-based estimation of word cooccurrenceprobabilities,?
In Proceedings of the 32nd An-nual Meeting of the Association for Computa-tional Linguistics.Gale, W. A. and Sampson, G. 1995.
?Good-Turing Frequency Estimation Without Tears,?Journal of Quantitative Linguistics 2, 217-237.Katz, S., ?Estimation of probabilities fromsparse data for the language model componentof a speech recognizer,?
In IEEE Transactionson Acoustics, Speech, and Signal Processing.Kontorovich L.and Lee D. 2001.
?Problems inSemitic NLP,?
NIPS Workshop on MachineLearning Methods for Text and ImagesLevinger, M., Ornan U., Itai A.
1995.
?LearningMorpho-Lexical Probabilities from anUntagged Corpus with an Application toHebrew, ?
Computational Linguistics,21(3): 383-404Segel, A.
1997.
?A probabilistic MorphologicalAnalyzer for Hebrew undotted text,?
MSc the-sis, Israeli Institute of Technology, Technion.
(in Hebrew)Westminster Theological Seminar 1998.
"TheHebrew Morphological Database",Philadelphia, PA
