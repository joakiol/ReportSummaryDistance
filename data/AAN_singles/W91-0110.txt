I\[A Uniform Architecture for Parsing, Generation and TransferRdmi ZajacProject POLYGLOSS*IMS-CL/IfI-AIS, University of StuttgartKeplerstrai3e 17, D-7000 Stuttgart 1zajac@informatik.uni-stuttgart.deAbst rac tWe present a uniform computationalarchitecture for developing reversiblegrammars for parsing and generation, andfor bidirectipnal transfer in MT.
We sketchthe principles of a general reversible ar-chitecture and show how they are real-ized in the rewriting system for typed fea-ture structu:res developed at the Univer-sity of Stuttgart.
The reversibility of pars-ing and gen:eration, and the bidirection-ality of tralisfer rules fall out of generalproperties of the uniform architecture.1 PR INCIPLES  FOR A UNIFORM AR-CHITECTUREThe principles for a uniform architecturefor parsing/generation and bidirectional trans-fer are a l ready contained in some PROLOGimplementations: of logic grammars like DCGs.For example, \[Sliieber 88\] proposes to apply theidea of Earley deduction \[Pereira/Warren 83\] togeneration.
With the noticeable exception of\[Dymetman et al 90\], all of these approaches usea context-free based mapping to relate a stringof words with a semantic structure.
Almost allof these approaches also rely on some specificproperties of the grammars intended to be pro-cessed (semantic heads, guides, leading features,specific representation of subcategorization, etc.
).They are also dependent on the direction in whichthey are used: even if the grammar specifica-tion is the same, two different compilers gener-ate two different programs for parsing and gener-ation.
Using the PROLOG deduction mechanismto have a simple and direct implementation of aparser/generator, one has to solve some problemsdue to the PROLOG evaluation method, for ex-ample termination on uninstantiated goals: goalshave to be evaluated in a different order for pars-ing and generation.
A reordering of goals per-formed by a rule compiler can be based on a di-rect specification of the ordering by the grammarwriter \ [Dymetman/Isabel le 88\], or can be derived*Research reported in this paper is partly supported by the German Ministry of Research and Technology (BMFT,Bundesminister ffir Forschung und Technologic), under grant No.
08 B3116 3.
The views and conclusions contained hereinare those of the autl~or and should not be interpreted as representing official policies.71by a compiler by analysing the dataflow using onlyinput/output specifications \[Strzalkowski 90\].But if we regard the grammar as a set of con-straints to be satisfied, parsing and generation dif-fer only in the nature of the "input", and there isno reason to use two different programs.
An inter-esting approach which uses only one program isdescribed in \[Dymetman/Isabelle 88\].
Within thisapproach, a lazy evaluation mechanism, based onthe specification of input/output arguments, i  im-plemented, and the evaluation is completly data-driven: the same program parses or generates de-pending only on the form of the input term.
Fur-thermore, a reversible grammar need not to bebased only on constituency.
\[Dymetman et al 90\]describes a class of reversible grammars ("Lexi-cal Grammars") based on a few composition ruleswhich are very reminiscent of categorial gram-mars.
Other kinds of approaches can also be en-visaged, e.g.
using a dependency structure andlinear precedence relations \[Reape 90\] (see also\[Pollard/Sag 87\]).From these experiments, we can outline desir-able properties of a computational framework forimplementing reversible grammars:A unique general deductive mechanism isused.
Grammars define constraints on theset of acceptable structures, and there is nodistinction between "input" and "output".To abolish the input/output distinction, thesame kind of data structure is used to encodeboth the string and the linguistic structure,and they are embedded into one data struc-ture that represents he relation between thestring and the associated linguistic structure(c.f.
the HPSG sign \[Pollard/Sag 87\]).Specific mapping properties, based on con-stituency, linear precedence or flmctionalcomposition, are not part of the formalismitself but are encoded explicitly using theformalism.The deductive mechanism should be compu-tationally well-behaved, especially with re-spect to completeness.In the next section, we show how these prop-erties are realized in the Typed Feature Structurerewriting system implemented at the University ofStuttgart 1.
We then discuss the parsing and gen-eration problem, and bidirectionality oftransfer inMT.
Assuming that we have the proper machin-ery, problems in parsing or generation can ariseonly because of a deficiency in the grammar2: inthe last section, the termination problem and effi-ciency issues are addressed.2 A REWRITE  MACHINE FOR TYPEDFEATURE STRUCTURESThe basic motivation behind the Typed Fea-ture Structure rewriting system is to provide alanguage which has the same deductive and log-ical properties of logic programming languagessuch as PROLOG, but which is based on featureterms instead of first order terms \[Ai't-Kaci 84,A~t-Kaci 86, Emele/Zajac 90a\].
Such a languagehas a different semantics than the Herbrand se-mantics: this semantics i based on the notion ofapproximation, which captures in a computational1The TFS system has been implemented byMartin Emele and the author as part of the POLYGLOSS project.2As it is often the case in generation when using a grammar built initially for parsing.3See also \[Emele/Zajac 90a\] for a fixed-point semantics.72iframework the idea that feature structures repre-sent partial information \[Zajac 90b\] 3.
Of course,as in PROLOG,i problems of completeness and el- Lficiency have to be addressed.The universe: of feature terms is structured inan inheritance hierarchy which defines a partialordering on kinds of available information.
Thebackbone of the hierarchy is defined by a par-tial order _< on !a set of type symbols T. To thisset, we add two more symbols: T which repre-sents completly underspecified information, and_l_ which represents inconsistent information.
Twotype symbols have a common most general sub-type (Greatest Lower Bound - GLB): this sub-type inherits ale information associated with allits super-types.
We define a meet operation on twotype symbols A and B as A A B = glb(A, B).
For-mally, a type hierarchy defined as a tuple (T, <, A)iis a meet semi-lattice.
A technicality arises whentwo types A and B have more than one GLB: inthat case, the set of GLBs is interpreted as a dis-junction.As different shts of attribute-value pairs makesense for differen~t kind of objects, we divide ourfeature terms into different types.
Terms are closedin the sense that ~ach type defines aspecific associ-ation of features iand restrictions on their possiblevalues) which are I appropriate for it, expressed as afeature structure '(the definition of the type).
Sincetypes are organized in an inheritance hierarchy,a type inherits all the features and value restric-tions from all its 'super-types.
This type-disciplinefor feature structures enforces the following twoconstraints: a term cannot have a feature whichis not appropriate for its type 4 and conversely, apair of feature and value should always be definedfor some type.
Thus a feature term is always typedand it is not possible to introduce an arbitrary fea-ture in a term (by unification): all features addedto some term should be appropriate for its type.We use the attribute-value matrix (AVM) nota-tion for feature terms and we write the type sym-bol for each feature term in front of the openingsquare bracket of the AVM.
A type symbol whichdoes not have any feature defined for it is atomic.All others types are complex.A type definition has the following form: thetype symbol to be defined appears on the left-hand side of the equation.
The right-hand sideis an expression of conjunctions and disjunctionsof typed feature terms (Figure 1).
Conjunctionsare interpreted as meets on typed feature terms(implemented using a typed unification algorithm\[Emele 91\]).
The definition may have conditionalconstraints expressed as a logical conjunction offeature terms and introduced by ' : - ' .
The right-hand side feature term may contain the left-hand side type symbol in a subterm (or in thecondition), thus defining a recursive type equa-tion which gives the system the expressive powerneeded to describe complex linguistic structures.A subtype inherits all constraints of its super-types monotonically: the constraints expressed asan expression of feature terms are conjoined usingunification; the conditions are conjoined using thelogical and operation.i .
4Checked at  compde t ime.73II: LIST\]APPEND\[2: LIST\[.
\[.3: LISTJ1: \[rest:~APPEND0 23: \[~LIS'I APPENDI/2: \[~\]LIST:~ \[3: CONS\[first:It\[rest:JiM\]:- APPEND :LISTNIL NS LISTFigure 2: Type hierarchy for LIST and APPEND (T and .1_ omitted).A set of type definitions defines an inheri-tance hierarchy of feature terms which specifiesthe available approximations.
Such a hierarchy iscompiled into a rewriting system as follows: eachdirect link between a type A and a subtype Bgenerates a rewrite rule of the form A\[a\] ~ B\[b\]where \[a\] and \[b\] are the definitions of A and B,respectively.The interpreter is given a "query" (a featureterm) to evaluate: this input term is already anapproximation of the final solution, though a veryrough approximation.
The idea is to incremen-tally add more information to that term using therewrite rules in order to get step by step closerto the solution: we stop when we have the bestpossible approximation.A rewrite step for a term t is defined as follows:if u is a subterm of t of type A and there exists arewrite rule A\[a\] ~ B\[b\] such that A\[a\] N u ~ _l_,the right-hand side B\[b\] is unified with the sub-term u, giving a new term t' which is more spe-cific than t. This rewrite step is applied non-deterministically everywhere in the term until nofurther rule is applicable 5.
Actually, the rewritingprocess stops either when all types are minimaltypes or when all subterms in a term correspondexactly to some approximation defined by a typein the hierarchy.
A term is "solved" when any sub-term is either more specific than the definition ofa minimal type, or does not give more informationthan the definition of its type.This defines an if and only if condition for aterm to be a solved-form, where any addition ofinformation will not bring anything new and isimplemented using a lazy rewriting strategy: theapplication of a rule A\[a\] ~ B\[b\] at a subterm uis actually triggered only when A\[a\] N u U d\[a\].This lazy rewriting strategy implements a fullydata-driven computation scheme and avoids use-less branches of computation.
Thus, there is no5Conditions do not change this general scheme and are omitted from the presentation for the sake of simplicity.
Seefor example \[Dershowitz/Plaisted 88\], and \[Klop 90\] for a survey.74LISTLISTAPPEND : LIST /: LISTJ\[i: APPEND :LIST I LISTJ> NIL, CONS\[first: T T\]\[rest: LISAPPEND0 :\[i APPEND1 :NILLIST~rst:\[\]CONS rest:El\ [ \ ]  LISTCONS\[  first:IX\]\[rest:\[\]APPEND 2:I_ a:Figure 3: Rewrite rules for LIST and APPEND.need to have a special treatment to avoid what cor-responds to the evaluation of un-instantiated goalsin PROLOG, since a general treatment based onithe semantics oflthe formalism itself is built in theevaluation strategy of the interpreter.The choice of which subterm to rewrite isonly partly driven by the availability of infor-mation (using the lazy rewriting scheme).
Whenthere are several subterms that could be rewrit-ten, the computation rule is to choose the outer-most ones (inner-most strategies are usually non-terminating) 6.
Such an outer-most rewriting strat-egy has interesting termination properties, sincethere are problems where a TFS program willterminate when the corresponding PROLOG pro-gram will not z.For a given subterm , the choice of which rule toapply is done non-deterministically, and the searchspace is explored depth-first using a backtrack-ing scheme.
This strategy is not complete, thoughin association with the outer-most rule and withthe lazy evaluation scheme, it seems to terminateon any "well-defined" problem, i.e.
when termsintroduced by recursive definitions during exe-cution are strictly decreasing according to somemesure (for example, see the definition of guidesin \ [Dymetman et al 90\] for the parsing and gener-ation problems).
A complete breadth-first searchstrategy is planned for debugging purposes.The interpreter described above is implemented sand has been used to test several models suchas LFG, HPSG, or DCG on toy examples\[Emele/Zajac 90b, Emele et al 90, Zajac 90a\].8This outer-mos t rewriting strategy is similar to hyper-resolution i  logic programming.
The lazy evaluation mech-anism is related to the 'freeze' predicate of, e.g.
Prolog-II and Sicstus Prolog, though in Prolog, it has to be calledexplicitly.7e.g.
the problem: of left-recursive rules in naive PROLOG implementations of DCGsSA prototype version is publically available.753 PARSING,  GENERATION,  AND BIDI-RECT IONAL TRANSFER3.1 Pars ing /generat ionA grammar describes the relation betweenstrings of words and linguistic structures.
In or-der to implement a reversible grammar, we haveto encode both kinds of structure using the samekind of data structure provided by the TFS lan-guage: typed feature structures.
A linguistic struc-ture will be encoded using features and values, andthe set of valid linguistic structures has to be de-clared explicitly.
A string of words will be encodedas a list of word forms, using the same kind of def-initions as in Figure 1.To abolish the distinction between "input" and"output", the relation between a string and a lin-guistic structure will be encoded in a single termwith, for example, two features, s t r ing  and synand we can call the type of such a structure SIGN 9.The type SIGN is divided into several subtypescorresponding to different mappings between astring and a linguistic structure.
We will have atleast the classifcation bewteen phrases and words.The definition of a phrase will recursively relatesubphrases and substrings, and define the phraseas a composition of subphrases and the stringas the concatenation r of substrings.
The formal-ism does not impose constraints on how the re-lations between phrases and strings are defined,and the grammar writer has to define them ex-plicitly.
One possibility is to use context-free likemappings, using for example the same kind ofencoding as in DCGs for PATR-like gramars ortIPSG \[Emele/Zajac 90b\].
But other possibilitiesare available as well: using a kind of functionalcomposition reminiscent of categorial grammarsas in \[Dymetman et al 90\], or linear precedencerules \[Pollard/Sag 87, Reape 90\].For example, a rule like \[Shieber 86\] l?NP VP :(S head)= (YP head)(S headform) = finite(UP syncat first) = (NP)(VP syncat rest) -- (end).is encoded in TFS using a type S for the sentencetype with two features np and vp for encoding theconstituent structure, and similarly for NPs andVPs.
The string associated with each constituentis encoded under the feature s t r ing .
The stringassociated with the sentence is simply the concate-nation of the string associated with the VP andthe string associated with the NP: this constraintis expressed in a condition using the APPEND rela-tion on lists (Figure 4).The difference between the parsing and thegeneration problem is then only in the form of theterm given to the interpreter for evaluation.
Anunderspecified term where only the string is givendefines the parsing problem:An underspecified term where only the seman-tic form is given defines the generation problem:9This is of course very reminiscent of HPSG, and it should not come as a surprise: HPSG is so far the only formallinguistic theory based on the notion of typed feature structures \[Pollard/Sag 87\].
A computational formalism similar toTFS is currently under design at CMU for implementing HPSG \[Carpenter 90, Franz 90\].1?Using a more condensed notation for lists with angle brackets provided by the TFS syntax: a listCONS\[first: Mary, res t :  CONS\[first: s ings ,  res t :  NIL\]\] is written as <Mary sings>.76\[ r 111S head: |tran.
:/a.g,: .T.
R / / /L Larg2: CORSWALLJ j jis also specified in the condition part, and thesecontrastive definitions are defined separately fromthe lexical definitions.In both cases , the same interpreter uses thesame set of rewrite rules to fill in "missing in-formation" according to the grammar definitions.The result in both cases is exactly the same: a fullyspecified term containing the string, the semanticform, and also all other syntactic information likethe constituent Structure (Figure 5).The transfer problem for one direction or theother is stated in the same way as for parsing orgeneration: the input term is an under-specified"bilingual sign" where only one structure for onelanguage is given.
Using the contrastive grammar,the interpreter fills in missing information andbuilds a completely specified bilingual sign 11 .3.2 B i -d i rect iona l  t ransfer  in MTWe have sketrched above a very general frame-work for specifying mappings between a linguis-tic structure, effcoded as a feature structure anda string; also encoded as a feature structure.
Weapply a similar technique for specifying MT trans-fer rules, which we prefer to call "contrastiverules" since there is no directionality involved\[Zajac 89, Zajac;90a\].The idea is rather simple: assume we are work-ing with linguistic structures imilar to LFG'sfunctional structures for English and French\[Kaplan et al 8~\].
We define a translation rela-tion as a type TAU-LEX with two features, eng forthe English structure and f r  for the French struc-ture.
This "bilingual sign" is defined on the lexicalstructure: each shbtype of TAU-LEX defines a lexi-cal correspondence b tween a partial English lexi-cal structure and.
a partial French lexical structurefor a given lexical equivalence.
Such a lexical con-trastive definition also has to pair the argumentsrecursively, and this is expressed in the conditionpart of the definltion (Figure 6).
The translationof syntactic features, like tense or determination,4 THE TERMINATION PROBLEM ANDEFF IC IENCY ISSUESFor parsing and generation, since no constraintis imposed on the kind of mapping between thestring and the semantic form, termination hasto be proved for each class of grammar andthe for the particular evaluation mechanism usedfor either parsing or generation with this gram-mar.
If we restrict ourselves to class of grammarsfor which terminating evaluation algorithms areknown, we can implement those directly in TFS.However, the TFS evaluation strategy allows morenaive implementations of grammars and the outer-most evaluation of "sub-goals" terminates on astrictly larger class of programs than for corre-sponding logic programs implemented in a con-ventional PROLOG.
Furthermore, the grammarwriter does not need, and actually should not, beaware of the control which follows the shape of theinput rather than a fixed strategy, thanks to thelazy evaluation mechanism.tIPSG-style grammars do not cause any prob-lem: completeness and coherence as definedfor LFG, and extended to the general case11See also \[Reape ',90\] for a "Shake'n'Bake" approach to MT (Whitelock).77by \[Wedekind 88\], are implemented in HPSGusing the "subcategorization feature principle"\[Johnson 87\].
Termination conditions for parsingare well understood in the framework of context-free grammars.
For generation using feature struc-tures, one of the problems is that the input couldbe "extended" during processing, i.e.
arbitraryfeature structures could be introduced in the se-mantic part of the input by unification with thesemantic part of a rule.
However, if the semanticpart of the input is fully speficied according to aset of type definitions describing the set of well-formed semantic structures (and this condition iseasy to check), this cannot arise in a type-basedsystem.
A more general approach is described in\[Dymetman et al 90\] who define sufficient prop-erties for termination for parsing and generationfor the class of "Lexical Grammars" implementedin PROLOG.
These properties eem generalizableto other classes of grammars as well, and are alsoapplicable to TFS implementations.
The idea isrelatively simple and says that for parsing, eachrule must consume a non empty part of the string,and for generation, each rule must consume a nonempty part of the semantic form.
Since LexicalGrammars are implemented in PROLOG, left-recursion must be eliminated for parsing and forgeneration, but this does not apply to TFS imple-mentations.Termination for reversible transfer grammars idiscussed in \[van Noord 90\].
One of the problemsmentioned is the extension of the "input", as ingeneration, and the answer is similar (see above).Itowever, properties imilar to the "conservativeguides" of \[Dymetman et al 90\] have to hold inorder to ensure termination.The lazy evaluation mechanism has an al-most optimal behavior on the class of prob-lems that have an exponential complexitywhen using the "generate and test" method\[van Hentenryck/Dincbas 87, A\[t-Saci/Meyer 90\].It is driven by the availability of information: assoon as some piece of information is available, theevaluation of constraints in which this informationappears is triggered.
Thus, the search space is ex-plored "intelligently", never following branches ofcomputation that would correspond to uninstan-ciated PROLOG goals.
The lazy evaluation mech-anism is not yet fully implemented in the currentversion of TFS, but with the partial implementa-tion we have, a gain of 50% for parsing has alreadybeen achieved (in comparison with the previousimplementation using only the outer-most rewrit-ing strategy).The major drawback of the current implemen-tation is the lack of an efficient indexing schemefor objects.
Since the dictionaries are accessed us-ing unification only, each entry is tried one afterthe other, leading to an extremely inefficient be-havior with large dictionaries.
However, we thinkthat a general indexing scheme based on a com-bination of methods used in PROLOG implemen-tations and in object-oriented database systems isfeasible.CONCLUSIONWe have described a uniform constraint-basedarchitecture for the implementation of reversibleunification grammars.
The advantages of this ar-chitecture in comparison of more traditional logic(i.e.
PROLOG) based architectures are: the in-put /output  distinction is truly abolished; the eval-uation terminates on a strictly larger class of prob-lems; it is directly based on typed feature struc-tures, not first order terms; a single fully data-driven constraint evaluation scheme is used; the78Iconstraint evaluation scheme is directly derivedfrom the semantics of typed feature structures.Thus, the TFS i language allows a direct imple-mentation of reyersible unification grammars.
Ofcourse, it does not dispense the grammar designerwith the proof O f general formal properties thatany well-behaved grammar should have, but itdoes allow the grammar writer to develop gram-mars without thinking about any notion of controlor input/output: distinction.References\[A\[t-Kaei 84\] Hassan Ai't-Kaei.
A Lattice TheoreticApproach to Computation based on a Calculus ofPartially Ordered Types Structures.
Ph.D Disserta-tion, University of Pennsylvania.\[Ai't-Kaci 86\] Hassan Ait-Kaei.
"An Algebraic Seman-tics Approach to the Effective Resolution of TypeEquations".
Theoretical Computer Science 45, 293-351.\[Ai't-Kaei/Meyer 90\] Hassan Ai't-Kaei and RichardMeyer.
"Wild_E.IFE , a user manual".
PRL TechnicalNote 1, Digital~Equipement Corporation, Paris Re-search Laboratory, Rueil-Malmaison, France, 1990.\[Calder et al 89\] ~Jonathan Calder, Mike Reape andHenk Zeevat.
"An algorithm for generation i uni-fication grammars".
Proc.
of the 4th Conference ofthe European Chapter of the Association for Compu-tational LinguiStics, 10-12 April 1989, Manchester.\[Carpenter 90\] Bob Carpenter.
"Typed feature struc-tures: inheritance, (in)equality and extensionality".Proc.
of the Workshop on Inheritance in NaturalLanguage Processing, Institute for Language Tech-nology and AI, Tilburg University, Netherlands, Au-gust 1990.\[Dershowitz/Plais~ed88\] N. Dershowitz and D.A.Plaisted.
"Equational programming".
In Hayes,Michie and Riehards (eds.).
Machine Intelligence 11.Clarendon Press, Oxford, 1988.\[Dymetman/Isabelle 88\] Marc Dymetman and PierreIsabelle.
"Reversible logic grammars for machinetranslation".
Proc.
of the 2nd International Con-ference on Theoretical and Methodological Issuesin Machine Translation of Natural Language, June1988, Pittsburgh.\[Dymetman et al 90\] Mare Dymetman, Pierre Is-abelle and Francois Perrault.
"A symmetrical p-proach to parsing and generation".
Proe.
of the 13thInternational Conference on Computational Lin-guistics - COLING'90, Helsinki, August 1990.\[Emele 91\] Martin Emele.
"Unification with lazy non-redundant copying".
29th Annual Meeting of theACL, June 1991, Berkeley, CA.\[Emele/Zajac 90a\] Martin Emele and Rdmi Zajac.
"Afixed-point semantics for feature type systems".Proe.
of the 2nd Workshop on Conditional andTyped Rewriting Systems - CTRS'90, Montreal,June 1990.\[Emele/Zajae 90b\] Martin Emele and Rdmi Zajac.
"Typed Unification Grammars".
Proe.
of the 13thInternational Conference on Computational Lin-guistics - COLING'90, Helsinki, August 1990.\[Emele t al.
90\] Martin Emele, Ulrich Heid, StefanMomma and Rdmi Zajac.
"Organizing linguisticknowledge for multilingual generation".
Proe.
ofthe 13th International Conference on ComputationalLinguistics - COLING'90, Helsinki, August 1990.\[Franz 90\] Alex Franz.
"A parser for HPSG".
CMU re-port CMU-LCL-90-3, Laboratory for ComputationalLinguistics, Carnegie Mellon University, July 1990.\[Isabelleet al 88\] Pierre Isabelle, Mare Dymetmanand Eliot Maeklovitch.
"CRITTER: a translationsystem for agricultural market reports.".
Proc.
ofthe 12th International Conference on ComputationalLinguistics - COLING'88, August 1988, Budapest.\[Johnson 87\] Mark Johnson.
"Grammatical relationsin attribute-value grammars".
Proe.
of the WestCoast Conference on Formal Linguistics, Vol.6,Stanford, 198779\[Kaplan et al 89\] Ronald M. Kaplam, Klaus Netter,Jfirgen Wedekind, Annie Zaenen.
"Translation bystructural correspondences".
Proc.
of the 4th Euro-pean A CL Conference, Manchester, 1989.\[Klop 90\] Jan Willem Klop.
"Term rewriting systems".To appear in S. Abramsky, D. Gabbay and T.Maibaum.
Handbook of Logic in Computer Science,Vol.1, Oxford University Press.\[Newman 90\] P. Newman.
"Towards convenient bi-directional grammar formalisms".
Proc.
of the 13thInternational Conference on Computational Lin-guistics - COLING'90, August 1990, Helsinki.\[Pereira/Warren 83\] Fernando C.N.
Pereira and DavidWarren.
"Parsing as deduction".
Proc.
of the 21stAnnual Meeting of the ACL, 15-17 June 1983, Cam-bridge, MA.\[Pollard/Sag87\] Carl Pollard and Ivan A. Sag.Information-Based Syntax and Semantics.
CSLILecture Notes 13, Chicago University Press, 1987.\[Pollard/Moshier 89\] Carl Pollard and Drew Moshier.
"Unifiying partial descriptions of sets".
In P.
Hanson(ed.)
Information, Language and Cognition, Van-couver Studies in Cognitive Science 1, University ofBritish Columbia Press, Vancouver.\[Reape 90\] Mike Reape.
"Parsing semi-free word or-der and bounded discontinuous constituency and"shake 'n' bake" machine translation (or 'genera-tion as parsing')".
Presented at the InternationalWorkshop on Constraint Based Formalisms for Nat-ural Language Generation, Bad Teinach, Germany,November 1990.\[Russell et al 90\] Graham Russell, Susan Warwickand John Carroll.
"Asymmetry in parsing and gen-eration with unification grammars: case studies fromELU".
Proc.
of the 28th A nnual Meeting of the A CL,6-9 June 1990, Pittsburgh.\[Shieber 86\] Stuart Shieber.An Introduction to Unification-based Grammar For-malisms.
CSLI Lectures Notes 4, Chicago UniversityPress, 1986.\[Shieber 88\] Stuart Shieber.
"A uniform architecturefor parsing and generation".
Proc.
of the 12th Inter-national Conference on Computational Linguistics -COLING'88, August 1988, Budapest.\[Shieber et al 89\] Stuart Shieber, Gertjan van Noord,Robert Moore and Fernando Pereira.
"A uniform ar-chitecture for parsing and generation".
Proc.
of tile27th Annual Meeting of the ACL, 26-27 June 1989,Vancouver.\[Strzalkowski 90\] Tomek Strzalkowski.
"How to inverta natural language parser into an efficient gener-ator: an algorithm for logic grammars".
Proc.
ofthe 13th International Conference on ComputationalLinguistics - COLING'90, August 1990, Helsinki.\[van Hentenryck/Dincbas 87\] P. van Hentenryck andM.
Dincbas.
"Forward checking in logic program-ming".
Proc.
of the 4th International Conference onLogic Programming, Melbourne, May 1987.\[van Noord 90\] Gertjan van Noord.
"Reversible unifi-cation based machine translation".
Proc.
of the 13thInternational Conference on Computational Lin-guistics - COLING'90, August 1990, Helsinki.\[Wedekind 88\] Jiirgen Wedekind.
"Generation asstructure driven generation".
Proc.
of the 12th Inter-national Conference on Computational Linguistics -COLING'88, August 1988, Budapest.\[Zajac 89\] R6mi Zajac.
"A transfer model using atyped feature structure rewriting system with in-heritance".
Proc.
of the gTth Annual Meeting of theA CL, 26-27 June 1989, Vancouver.\[Zajac 90a\] Rdmi Zajac.
"A relational approach totranslation".
Proc.
of the 3rd International Con-ference on Theoretical and Methodological Issues inMachine Translation of Natural Language, 11-13June 1990, Austin.\[Zajac 90b\] R~mi Zajac.
"Computing partial informa-tion using approximations - Semantics of typedfeature structures".
Presented at the InternationalWorkshop on Constraint Based Formalisms for Nat-ural Language Generation, Bad Teinach, Germany,November 1990.80
