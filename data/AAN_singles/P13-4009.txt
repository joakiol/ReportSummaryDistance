Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 49?54,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsFudanNLP: A Toolkit for Chinese Natural Language ProcessingXipeng Qiu, Qi Zhang, Xuanjing HuangFudan University, 825 Zhangheng Road, Shanghai, Chinaxpqiu@fudan.edu.cn, qz@fudan.edu.cn, xjhuang@fudan.edu.cnAbstractThe growing need for Chinese naturallanguage processing (NLP) is largelyin a range of research and commer-cial applications.
However, most ofthe currently Chinese NLP tools orcomponents still have a wide rangeof issues need to be further improvedand developed.
FudanNLP is an opensource toolkit for Chinese natural lan-guage processing (NLP), which usesstatistics-based and rule-based meth-ods to deal with Chinese NLP tasks,such as word segmentation, part-of-speech tagging, named entity recogni-tion, dependency parsing, time phraserecognition, anaphora resolution and soon.1 IntroductionChinese is one of the most widely used lan-guages in this world, and the proportion thatChinese language holds on the Internet is alsoquite high.
Under the current circumstances,there are greater and greater demands for in-telligent processing and analyzing of the Chi-nese texts.Similar to English, the main tasks in Chi-nese NLP include word segmentation (CWS),part-of-speech (POS) tagging, named en-tity recognition (NER), syntactic parsing,anaphora resolution (AR), and so on.
Al-though the general ways are essentially thesame for English and Chinese, the implemen-tation details are different.
It is also non-trivial to optimize these methods for ChineseNLP tasks.There are also some toolkits to be usedfor NLP, such as Stanford CoreNLP1, ApacheOpenNLP2, Curator3 and NLTK4.
But thesetoolkits are developed mainly for English andnot optimized for Chinese.In order to customize an optimized systemfor Chinese language process, we implementan open source toolkit, FudanNLP5, which iswritten in Java.
Since most of the state-of-the-art methods for NLP are based on statisticallearning, the whole framework of our toolkitis established around statistics-based meth-ods, supplemented by some rule-based meth-ods.
Therefore, the quality of training datais crucial for our toolkit.
However, we findthat there are some drawbacks in currentlymost commonly used corpora, such as CTB(Xia, 2000) and CoNLL (Haji?
et al 2009)corpora.
For example, in CTB corpus, the setof POS tags is relative small and some cate-gories are derived from the perspective of En-glish grammar.
And in CoNLL corpus, thehead words are often interrogative particlesand punctuations, which are unidiomatic inChinese.
These drawbacks bring more chal-lenges to further analyses, such as informa-tion extraction and semantic understanding.Therefore, we first construct a corpus witha modified guideline, which is more in ac-cordance with the common understanding forChinese grammar.In addition to the basic Chinese NLP tasks1http://nlp.stanford.edu/software/corenlp.shtml2http://incubator.apache.org/opennlp/3http://cogcomp.cs.illinois.edu/page/software_view/Curator4http://www.nltk.org/5http://fudannlp.googlecode.com49Figure 1: System Structure of FudanNLPmentioned above, the toolkit also providesmany minor functions, such as text classifi-cation, dependency tree kernel, tree pattern-based information extraction, keywords ex-traction, translation between simplified andtraditional Chinese, and so on.Currently, our toolkit has been used bymany universities and companies for variousapplications, such as the dialogue system, so-cial computing, recommendation system andvertical search.The rest of the demonstration is organizedas follows.
We first briefly describe our systemand its main components in section 2.
Then weshow system performances in section 3.
Sec-tion 4 introduces three ways to use our toolkit.In section 5, we summarize the paper and givesome directions for our future efforts.2 System OverviewThe components of our system have threelayers of structure: data preprocessing, ma-chine learning and natural language process-ing, which is shown in Figure 1.
We will in-troduce these components in detail in the fol-lowing subsections.2.1 Data Preprocessing ComponentIn the natural language processing system,the original input is always text.
However,the statistical machine learning methods oftendeal with data with vector-based representa-tion.
So we firstly need to preprocess the inputtexts and transform them to the required for-mat.
Due to the fact that text data is usuallydiscrete and sparse, the sparse vector struc-ture is largely used.
Similar to Mallet (Mc-Callum, 2002), we use the pipeline structurefor a flexible transformation of various data.The pipeline consists of several serial or par-allel modules.
Each module, called ?pipe?, isaimed at a single and simple function.For example, when we transform a sentenceinto a vector with ?bag-of-words?, the trans-formation process would involve the followingserial pipes:1.
String2Token Pipe: to transform a stringinto word tokens.2.
Token2Index Pipe: to look up the wordalphabet to get the indices of the words.3.
WeightByFrequency Pipe: to calculatethe vector weight for each word accord-ing to its frequency of occurrence.With the pipeline structure, the data pre-processing component has good flexibility, ex-tensibility and reusability.2.2 Machine Learning ComponentThe outputs of NLP are often structured,so the structured learning is our core module.Structured learning is the task of assigning astructured label y to an input x.
The label ycan be a discrete variable, a sequence, a treeor a more complex structure.To illustrate by a sample x, we define thefeature as ?(x,y).
Thus, we can label x witha score function,y?
= arg maxyF (w,?
(x,y)), (1)where w is the parameter of function F (?
).The feature vector ?
(x,y) consists of lots ofoverlapping features, which is the chief benefitof a discriminative model.For example, in sequence labeling, both x =x1, .
.
.
, xL and y = y1, .
.
.
, yL are sequences.For first-order Markov sequence labeling, thefeature can be denoted as ?k(yi?1, yi,x, i),where i is the position in the sequence.
Thenthe score function can be rewritten asy?
= arg maxyF (L?i=1?kwk?k(yi?1, yi,x, i)), (2)where L is the length of x.Different algorithms vary in the definition ofF (?)
and the corresponding objective function.50F (?)
is usually defined as a linear or exponen-tial family function.
For example, in condi-tional random fields (CRFs) (Lafferty et al2001), F (?)
is defined as:Pw(y|x) =1Zwexp(wT?
(x,y)), (3)where Zw is the normalization constant suchthat it makes the sum of all the terms one.In FudanNLP, the linear function is univer-sally used as the objective function.
Eq.
(1) iswritten as:y?
= arg maxy< w,?
(x,y) > .
(4)2.2.1 TrainingIn the training stage, we use the passive-aggressive algorithm to learn the model pa-rameters.
Passive-aggressive (PA) algorithm(Crammer et al 2006) was proposed for nor-mal multi-class classification and can be easilyextended to structure learning (Crammer etal., 2005).
Like Perceptron, PA is an onlinelearning algorithm.2.2.2 InferenceFor consistency with statistical machinelearning, we call the process to calculate theEq.
(1) as ?inference?.
In structured learning,the number of possible solutions is very huge,so dynamic programming or approximate ap-proaches are often used for efficiency.
For NLPtasks, the most popular structure is sequence.To label the sequence, we use Viterbi dynamicprogramming to solve the inference problem inEq.
(4).Our system can support any order of Viterbidecoding.
In addition, we also implement aconstrained Viterbi algorithm to reduce thenumber of possible solutions by pre-definedrules.
For example, when we know the prob-able labels, we delete the unreachable statesfrom state transition matrix.
It is very usefulfor CWS and POS tagging with sequence la-beling.
When we have a word dictionary orknow the POS for some words, we can getmore accurate results.2.2.3 Other AlgorithmsApart from the core modules of structuredlearning, our system also includes several tra-ditional machine learning algorithms, such asPerceptron, Adaboost, kNN, k-means, and soon.2.3 Natural Language ProcessingComponentsOur toolkit provides the basic NLP func-tions, such as word segmentation, part-of-speech tagging, named entity recognition, syn-tactic parsing, temporal phrase recognition,anaphora resolution, and so on.
These func-tions are trained on our developed corpus.
Wealso develop a visualization module to display-ing the output.
Table 1 shows the output rep-resentation of our toolkit.2.3.1 Chinese Word SegmentationDifferent from English, Chinese sentencesare written in a continuous sequence of char-acters without explicit delimiters such as theblank space.
Since the meanings of most Chi-nese characters are not complete, words arethe basic syntactic and semantic units.
There-fore, it is indispensable step to segment thesentence into words in Chinese language pro-cessing.We use character-based sequence labeling(Peng et al 2004) to find the boundaries ofwords.
Besides the carefully chosen features,we also use the meaning of character drawnfrom HowNet(Dong and Dong, 2006), whichimproves the performance greatly.
Since un-known words detection is still one of main chal-lenges of Chinese word segmentation.
We im-plement a constrained Viterbi algorithm to al-low users to add their own word dictionary.2.3.2 POS taggingChinese POS tagging is very different fromthat in English.
There are no morphologicalchanges for a word among its different POStags.
Therefore, most of Chinese words mayhave multiple POS tags.
For example, thereare different morphologies in English for theword ???
(destroy)?, such as ?destroyed?,?destroying?
and ?destruction?.
But in Chi-nese, there is just one same form(Xia, 2000).There are two popular guidelines to tag theword?s POS: CTB (Xia, 2000) and PKU (Yuet al 2001).
We take into account boththe weaknesses and the strengths of these twoguidelines, and propose our guideline for bet-ter subsequent analyses, such as parser andnamed entity recognition.
For example, theproper name is labeled as ?NR?
in CTB, whilewe label it with one of four categories: person,51Input:???????????
1980 ?
?John is from Washington, and he was born in 1980.Output:..??
.??
.???
.?
.?
.??
.1980 ?
.
?.John .is from .Washington ., .he .was born in .1980 ...PER .VV .LOC .PU .PRN .NN .PU.1 .2 .3 .4 .5 .6 .7 .8RootSUBCS:COO1OBJPUNSUB OBJPUNNER:1 ?
PER3 ?
LOCAR:5 ?
1TIME:7 ?
19801 CS:COO means the coordinate complex sentence.Table 1: Example of the output representation of our toolkitlocation, organization and other proper name.Conversely, we merge the ?VC?
and ?VE?
into?VV?
since there is no link verb in Chinese.Finally, we use a tag set with 39 categories intotal.Since a POS tag is assigned to each word,not to each character, Chinese POS tag-ging has two ways: pipeline method or jointmethod.
Currently, the joint method is morepopular and effective because it uses more flex-ible features and can reduce the error propa-gation (Ng and Low, 2004).
In our system,we implement both methods for POS tagging.Besides, we also use some knowledge to im-prove the performance, such as Chinese sur-name and the common suffixes of the namesof locations and organizations.2.3.3 Named Entity RecognitionIn Chinese named entity recognition (NER),there are usually three kinds of named enti-ties (NEs) to be dealt with: names of per-sons (PER) , locations (LOC) and organiza-tions (ORG).
Unlike English, there is no obvi-ous identification for NEs, such as initial capi-tals.
The internal structures are also differentfor different kinds of NEs, so it is difficult tobuild a unified model for named entity recog-nition.Our NER is based on the results of POStagging and uses some customize features todetect NEs.
First, the number of NEs is verylarge and the new NEs are endlessly emerg-ing, so it is impossible to store them in dic-tionary.
Since the internal structures are rela-tively more important, we use language mod-els to capture the internal structures.
Second,we merge the continuous NEs with some rule-based strategies.
For example, we combine thecontinuous words ???/NN???/NN?
into?
????
?/LOC?.2.3.4 Dependency parsingOur syntactic parser is currently a depen-dency parser, which is implemented with theshift-reduce deterministic algorithm based onthe work in (Yamada and Matsumoto, 2003).The syntactic structure of Chinese is morecomplex than that of English, and semanticmeaning is more dominant than syntax in Chi-nese sentences.
So we select the dependencyparser to avoid the minutiae in syntactic con-stituents and wish to pay more attention tothe subsequent semantic analysis.
Since thestructure of the Chinese language is quite dif-ferent from that of English, we use more effec-tive features according to the characteristics ofChinese sentences.The common used corpus for Chinese de-pendency parsing is CoNLL corpus (Haji?
etal., 2009).
However, there are some illogicalcases in CoNLL corpus.
For example, thehead words are often interrogative particlesand punctuations.
Our guideline is based oncommon understanding for Chinese grammar.The Chinese syntactic components usually in-clude subject, predicate, object, attribute, ad-verbial modifier and complement.
Figure 2and 3 show the differences between the trees ofCoNLL and our Corpus.
Table 2 shows some52primary dependency relations in our guideline...?
.?
.???
.?
.?
.?
.
?.want to .go to .Hehuanshan .to see .the snow .
.
?.VV .VV .NR .VV .NN .SP .PURootCOMPADVCOMPCOMPCOMP UNKFigure 2: Dependency Tree in CoNLL Corpus..?
.?
.???
.?
.?
.?
.
?.want to .go to .Hehuanshan .to see .the snow .
.
?.MD .VV .LOC .VV .NN .SP .PURootADVOBJOBJOBJVOCPUNFigure 3: Dependency Tree in Our CorpusRelations Chinese DefinitionsSUB ??
SubjectPRED ??
PredicateOBJ ??
ObjectATT ??
AttributeADV ??
Adverbial ModifierCOMP ??
ComplementSVP ??
Serial Verb PhrasesSUB-OBJ ??
Pivotal ConstructionVOC ??
VoiceTEN ??
TensePUN ??
PunctuationTable 2: Some primary dependency relations2.3.5 Temporal Phrase Recognitionand NormalizationChinese temporal phrases is more flexiblethan English.
Firstly, there are two calendars:Gregorian and lunar calendars.
Both of themare frequently used.
Secondly, the forms ofsame temporal phrase are various, which oftenconsists of Chinese characters, Arabic numer-als and English letters, such as ???
10 ?
?and ?10:00 PM?.Different from the general process basedon machine learning, we implement the timephrase recognizer with a rule-based method.These rules include 376 regular expressionsand nearly a hundred logical judgments.After recognizing the temporal phrases, wenormalize them with a standard time format.For a phrase indicating a relative time , suchas ?????
and ?
????
?, we first find thebase time in the context.
If no base time isfound, or there is also no temporal phrase toindicate the base time (such as ????
), weset the base time to the current system time.Table 3 gives examples for our temporal phraserecognition module.Input:08 ????????
?8 ?
8 ?????????????????????
?The Beijing Olympic Games took place from Au-gust 8, 2008.
Four years later, the London OlympicGames took place from July 21.????????
9 ????????????
?I?m busy today, and have to come off duty after 9:00PM.
And I also have to work this Sunday.Output:08 ?
(2008) 20088 ?
8 ?
(August 8) 2008-8-8??????
(July 21) 2012-7-27??
(today) 2012-2-221??
9 ?
(9:00 PM) 2012-2-22 21:00??
(this Sunday) 2012-2-261 The base time is 2012-02-22 10:00AM.Table 3: Examples for Temporal PhraseRecognition2.3.6 Anaphora ResolutionAnaphora resolution is to detect the pro-nouns and find what they are referring to.We first find all pronouns and entity names,then use a classifier to predict whether thereis a relation between each pair of pronoun andentity name.
Table 4 gives examples for ouranaphora resolution module.Input:???????
1167 ?????????????????????????
?Oxford University is founded in 1167.
It is locatedin Oxford, UK.
The university has nurtured a lotof good students.Output:?
(It) ????????
(Theuniversity)????
(Oxford University)Table 4: Examples for Anaphora Resolution3 System PerformancesIn this section, we investigate the per-formances for the six tasks: Chinese wordsegmentation (CWS), POS tagging (POS),53named entity recognition (NER) and de-pendency parser(DePar), Temporal PhraseRecognition (TPR) and Anaphora Resolution(AR).
We use 5-fold cross validation on ourdeveloped corpus.
The corpus includes 65, 745sentences and 959, 846 words.
The perfor-mances are shown in Table 5.Task Accuracy Speed1 MemoryCWS 97.5% 98.9K 66MPOS 93.4% 44.5K 110MNER 98.40% 38K 30MDePar 85.3% 21.1 80MTPR 95.16% 22.9k 237KAR 70.3% 35.7K 52K1 characters per second.
Test environment:CPU 2.67GHz, JRE 7.Table 5: System Performances4 UsagesWe provide three ways to use our toolkit.Firstly, our toolkit can be used as library.Users can call application programming inter-faces (API) in their own applications.Secondly, users can also invoke the mainNLP modules to process the inputs (stringsor files) from the command line directly.Thirdly, the web services are providedfor platform-independent and language-independent use.
We use a REST (Represen-tational State Transfer) architecture, in whichthe web services are viewed as resources andcan be identified by their URLs.5 ConclusionsIn this demonstration, we have describedthe system, FudanNLP, which is a Java-basedopen source toolkit for Chinese natural lan-guage processing.
In the future, we will addmore functions, such as semantic parsing.
Be-sides, we will also optimize the algorithms andcodes to improve the system performances.AcknowledgmentsWe would like to thank all the people6involved with our FudanNLP project.
Thiswork was funded by NSFC (No.610030916https://code.google.com/p/fudannlp/wiki/Peopleand No.61073069) and 973 Program(No.2010CB327900).ReferencesK.
Crammer, R. McDonald, and F. Pereira.
2005.Scalable large-margin online learning for struc-tured classification.
In NIPS Workshop onLearning With Structured Outputs.
Citeseer.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
On-line passive-aggressive algorithms.
Journal ofMachine Learning Research, 7:551?585.Z.
Dong and Q. Dong.
2006.
Hownet And theComputation of Meaning.
World Scientific Pub-lishing Co., Inc. River Edge, NJ, USA.J.
Haji?, M. Ciaramita, R. Johansson, D. Kawa-hara, M.A.
Mart?, L. M?rquez, A. Meyers,J.
Nivre, S.
Pad?, J.
?t?p?nek, et al2009.
TheCoNLL-2009 shared task: Syntactic and seman-tic dependencies in multiple languages.
In Pro-ceedings of the Thirteenth Conference on Com-putational Natural Language Learning: SharedTask, pages 1?18.
Association for Computa-tional Linguistics.John D. Lafferty, Andrew McCallum, and Fer-nando C. N. Pereira.
2001.
Conditional ran-dom fields: Probabilistic models for segmentingand labeling sequence data.
In Proceedings ofthe Eighteenth International Conference on Ma-chine Learning.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.H.T.
Ng and J.K. Low.
2004.
Chinese part-of-speech tagging: one-at-a-time or all-at-once?word-based or character-based.
In Proceedingsof EMNLP, volume 4.F.
Peng, F. Feng, and A. McCallum.
2004.
Chi-nese segmentation and new word detection us-ing conditional random fields.
Proceedings of the20th international conference on ComputationalLinguistics.F.
Xia, 2000.
The part-of-speech tagging guidelinesfor the penn chinese treebank (3.0).H.
Yamada and Y. Matsumoto.
2003.
Statis-tical dependency analysis with support vectormachines.
In Proceedings of the InternationalWorkshop on Parsing Technologies (IWPT),volume 3.S.
Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun,H.
Wang, Q. Zhao, and W. Zhan.
2001.
Process-ing norms of modern chinese corpus.
Technicalreport, Technical report.54
