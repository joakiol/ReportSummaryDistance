Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2193?2203,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsExploiting Sentence Similarities for Better AlignmentsTao Li and Vivek SrikumarUniversity of Utah{tli,svivek}@cs.utah.eduAbstractWe study the problem of jointly aligning sen-tence constituents and predicting their sim-ilarities.
While extensive sentence similar-ity data exists, manually generating referencealignments and labeling the similarities of thealigned chunks is comparatively onerous.
Thisprompts the natural question of whether wecan exploit easy-to-create sentence level datato train better aligners.
In this paper, wepresent a model that learns to jointly alignconstituents of two sentences and also predicttheir similarities.
By taking advantage of bothsentence and constituent level data, we showthat our model achieves state-of-the-art per-formance at predicting alignments and con-stituent similarities.1 IntroductionThe problem of discovering semantic relationshipsbetween two sentences has given birth to severalNLP tasks over the years.
Textual entailment (Da-gan et al, 2013, inter alia) asks about the truth ofa hypothesis sentence given another sentence (ormore generally a paragraph).
Paraphrase identifi-cation (Dolan et al, 2004, inter alia) asks whethertwo sentences have the same meaning.
Foregoingthe binary entailment and paraphrase decisions, thesemantic textual similarity (STS) task (Agirre et al,2012) asks for a numeric measure of semantic equiv-alence between two sentences.
All three tasks haveattracted much interest in the form of shared tasks.While various approaches have been proposed topredict these sentence relationships, a commonlyemployed strategy (Das and Smith, 2009; Chang etGunmen abduct 7 foreign workers?
Seven foreign workers kidnappedEQUIEQUIFigure 1: Example constituent alignment.
The solidlines represent aligned constituents (here, both la-beled equivalent).
The chunk Gunmen is unaligned.al., 2010a) is to postulate an alignment between con-stituents of the sentences and use this alignment tomake the final prediction (a binary decision or a nu-meric similarity score).
The implicit assumption insuch approaches is that better constituent alignmentscan lead to better identification of semantic relation-ships between sentences.Constituent alignments serve two purposes.
First,they act as an intermediate representation for pre-dicting the final output.
Second, the alignments helpinterpret (and debug) decisions made by the over-all system.
For example, the alignment between thesentences in Figure 1 can not only be useful to deter-mine the equivalence of the two sentences, but alsohelp reason about the predictions.The importance of this intermediate representa-tion led to the creation of the interpretable seman-tic textual similarity task (Agirre et al, 2015a) thatfocuses on predicting chunk-level alignments andsimilarities.
However, while extensive resources ex-ist for sentence-level relationships, human annotatedchunk-aligned data is comparatively smaller.In this paper, we address the following question:can we use sentence-level resources to better pre-2193dict constituent alignments and similarities?
To an-swer this question, we focus on the semantic tex-tual similarity (STS) task and its interpretable vari-ant.
We propose a joint model that aligns con-stituents and integrates the information across thealigned edges to predict both constituent and sen-tence level similarity.
The key advantage of model-ing these two problems jointly is that, during train-ing, the sentence-level information can provide feed-back to the constituent-level predictions.We evaluate our model on the SemEval-2016 taskof interpretable STS.
We show that even without thesentence information, our joint model that uses con-stituent alignments and similarities forms a strongbaseline.
Further, our easily extensible joint modelcan incorporate sentence-level similarity judgmentsto produce alignments and chunk similarities that arecomparable to the best results in the shared task.In summary, the contributions of this paper are:1.
We present the first joint model for predictingconstituent alignments and similarities.
Ourmodel can naturally take advantage of the muchlarger sentence-level annotations.2.
We evaluate our model on the SemEval-2016task of interpretable semantic similarity andshow state-of-the-art results.2 Problem DefinitionIn this section, we will introduce the notationused in the paper using the sentences in Figure1 as a running example.
The input to the prob-lem is a pair of sentences, denoted by x. Wewill assume that the sentences are chunked (TjongKim Sang and Buchholz, 2000) into constituents.We denote the chunks using subscripts.
Thus,the input x consists of two sequences of chunkss = (s1, s2, ?
?
? )
and t = (t1, t2, ?
?
? )
respec-tively.
In our running example, we have s =(Gunmen, abduct, seven foreign workers) and t =(Seven foreign workers, kidnapped).The output consists of three components:1.
Alignment: The alignment between a pair ofchunks is a labeled, undirected edge that ex-plains the relation that exists between them.The labels can be one of EQUI (semanticallyequivalent), OPPO (opposite meaning in con-text), SPE1, SPE2 (the chunk from s is morespecific than the one from t and vice versa),SIMI (similar meaning, but none of the pre-vious ones) or REL (related, but none of theabove)1.
In Figure 1, we see two EQUI edges.A chunk from either sentence can be unaligned,as in the case of the chunk Gunmen.We will use y to denote the alignment for aninput x.
The alignment y consists of a se-quence of triples of the form (si, tj , l).
Here, siand tj denote a pair of chunks that are alignedwith a label l. For brevity, we will include un-aligned chunks into this format using a specialnull chunk and label to indicate that a chunk isunaligned.
Thus, the alignment for our runningexample contain the triple (Gunmen, ?, ?).2.
Chunk similarity: Every aligned chunk is as-sociated with a relatedness score between zeroand five, denoting the range from unrelatedto equivalent.
Note that even chunks labeledOPPO can be assigned a high score because thepolarity is captured by the label rather than thescore.
We will denote the chunk similarities us-ing z, comprising of numeric zi,j,l for elementsof the corresponding alignment y.
For an un-aligned chunk, the corresponding similarity zis fixed to zero.3.
Sentence similarity: The pair of sentences isassociated with a scalar score from zero to five,to be interpreted as above.
We will use r todenote the sentence similarity for an input x.Thus, the prediction problem is the following:Given a pair of chunked sentences x = (s, t), pre-dict the alignment y, the alignment similarities z andthe sentence similarity r. Note that this problem def-inition integrates the canonical semantic textual sim-ilarity task (only predicting r) and its interpretablevariant (predicting both y and z) into a single task.1We refer the reader to the guidelines of the task (Agirre etal., 2015a) for further details on these labels.
Also, for simplic-ity, in this paper, we ignore the factuality and polarity tags fromthe interpretable task.21943 Predicting Alignments and SimilaritiesThis section describes our model for predictingalignments, alignment scores, and the sentence sim-ilarity scores for a given pair of sentences.
We willassume that learning is complete and we have all thescoring functions we need and defer discussing theparameterization and learning to Section 4.We frame the problem of inference as an instanceof an integer linear program (ILP).
We will first seethe scoring functions and the ILP formulation inSection 3.1.
Then, in Section 3.2, we will see howwe can directly read off the similarity scores at bothchunk and sentence level from the alignment.3.1 Alignment via Integer Linear ProgramsWe have two kinds of 0-1 inference variables to rep-resent labeled aligned chunks and unaligned chunks.We will use the inference variables 1i,j,l to denotethe decision that chunks si and tj are aligned with alabel l. To allow chunks to be unaligned, the vari-ables 1i,0 and 10,j denote the decisions that si andtj are unaligned respectively.Every inference decision is scored by the trainedmodel.
Thus, we have score(i, j, l), score(i, 0)and score(0, j) for the three kinds of inferencevariables respectively.
All scores are of the formA(wT?
(?, s, t)), where w is a weight vector thatis learned, ?
(?, s, t) is a feature function whose ar-guments include the constituents and labels in ques-tion, and A is a sigmoidal activation function thatflattens the scores to the range [0, 5].
In all our ex-periments, we used the function A(x) = 51+e?x .The goal of inference is to find the assignment tothe inference variables that maximizes total score.That is, we seek to solvearg max1?C?i,j,lscore(i, j, l)1i,j,l+?iscore(i, 0)1i,0+?jscore(0, j)10,j (1)Here 1 represents all the inference variables togetherand C denotes the set of all valid assignments to thevariables, defined by the following set of constraints:1.
A pair of chunks can have at most one label.2.
Either a chunk can be unaligned or it shouldparticipate in a labeled alignment with exactlyone chunk of the other sentence.We can convert these constraints into linear in-equalities over the inference variables using stan-dard techniques for ILP inference (Roth and Yih,2004)2.
Note that, by construction, there is a one-to-one mapping from an assignment to the inferencevariables 1 and the alignment y.
In the rest of thepaper, we use these two symbols interchangeably,using 1 referring details of inference and y referringto the alignment as a sequence of labeled edges.3.2 From Alignments to SimilaritiesTo complete the prediction, we need to compute thenumeric chunk and sentence similarities given thealignment y.
In each case, we make modeling as-sumptions about how the alignments and similaritiesare related, as described below.Chunk similarities To predict the chunk similari-ties, we assume that the label-specific chunk similar-ities of aligned chunks are the best edge-weights forthe corresponding inference variables.
That is, for apair of chunks (si, tj) that are aligned with a labell, the chunk pair similarity zi,j,l is the coefficient as-sociated with the corresponding inference variable.If the alignment edge indicates an unaligned chunk,then the corresponding score is zero.
That is,zi,j,l ={A(wT?
(si, tj , l, s, t)) if l 6= ?0 if l = ?.
(2)But can chunk similarities directly be used to findgood alignments?
To validate this assumption, weperformed a pilot experiment on the chunk alignedpart of our training dataset.
We used the gold stan-dard chunk similarities as scores of the inferencevariables in the integer program in Eq.
1, with thevariables associated with unaligned chunks beingscored zero.
We found that this experiment givesa near-perfect typed alignment F-score of 0.9875.2While it may be possible to find the score maximizingalignment in the presence of these constraints using dynamicprogramming (say, a variant of the Kuhn-Munkres algorithm),we model inference as an ILP to allow us the flexibility to ex-plore more sophisticated output interactions in the future.2195The slight disparity is because the inference only al-lows 1-to-1 matches between chunks (constraint 2),which does not hold in a small number of examples.Sentence similarities Given the aligned chunks y,the similarity between the sentences s and t (i.e., inour notation, r) is the weighted average of the chunksimilarities (i.e., zi,j,l).
Formally,r = 1|y|?
(si,tj ,l)?y?lzi,j,l.
(3)Note that the weights ?l depend only on the labelsassociated with the alignment edge and are designedto capture the polarity and strength of the label.
Eq.3 bridges sentence similarities and chunk similari-ties.
During learning, this provides the feedbackfrom sentence similarities to chunk similarities.
Thevalues of the?
?s can be learned or fixed before learn-ing commences.
To simplify our model, we choosethe latter approach .
Section 5 gives more details.Features To complete the description of themodel, we now describe the features that define thescoring functions.
We use standard features from theSTS literature (Karumuri et al, 2015; Agirre et al,2015b; Banjade et al, 2015).For a pair of chunks, we extract the followingsimilarity features: (1) Absolute cosine similari-ties of GloVe embeddings (Pennington et al, 2014)of head words, (2) WordNet based Resnik (Resnik,1995), Leacock (Leacock and Chodorow, 1998) andLin (Lin, 1998) similarities of head words, (3) Jac-card similarity of content words and lemmas.
Inaddition, we also add indicators for: (1) the partof speech tags of the pair of head words, (2) thepair of head words being present in the lexical largesection of the Paraphrase Database (Ganitkevitch etal., 2013), (3) a chunk being longer than the otherwhile both are not named entity chunks, (4) a chunkhaving more content words than the other, (5) con-tents of one chunk being a part of the other, (6) hav-ing the same named entity type or numeric words,(7) sharing synonyms or antonyms, (8) sharing con-junctions or prepositions, (9) the existence of uni-gram/bigram/trigram overlap, (10) if only one chunkhas a negation, and (11) a chunk having extra con-tent words that are also present in the other sentence.For a chunk being unaligned, we conjoin an in-dicator that the chunk is unaligned with the part ofspeech tag of its head word.3.3 DiscussionIn the model proposed above, by predicting thealignment, we will be able to deterministically cal-culate both chunk and sentence level similarities.This is in contrast to other approaches for the STStask, which first align constituents and then extractfeatures from alignments to predict similarities in apipelined fashion.
The joint prediction of alignmentand similarities allows us to address the primary mo-tivation of the paper, namely using the abundant sen-tence level data to train the aligner and scorer.The crucial assumption that drives the joint modelis that the same set of parameters that can discovera good alignment can also predict similarities.
Thisassumption ?
similar to the one made by Chang et al(2010b) ?
and the associated model described above,imply that the goal of learning is to find parametersthat drive the inference towards good alignments andsimilarities.4 Learning the Alignment ModelUnder the proposed model, the alignment directlypredicts the chunk and sentence similarities as well.We utilize two datasets to learn the model:1.
The alignment dataset DA consists of fullyannotated aligned chunks and respective chunksimilarity scores.2.
The sentence dataset DS that consists of pairsof sentences where each pair is labeled with anumeric similarity score between zero and five.The goal of learning is to use these two datasetsto train the model parameters.
Note that unlike stan-dard multi-task learning problems, the two tasks inour case are tightly coupled both in terms of theirdefinition and via the model described in Section 3.We define three types of loss functions corre-sponding to the three components of the final out-put (i.e., alignment, chunk similarity and sentencesimilarity).
Naturally, for each kind of loss, we as-sume that we have the corresponding ground truth.We will denote ground truth similarity scores andalignments using asterisks.
Also, the loss functions2196defined below depend on the weight vector w, butthis is not shown to simplify notation.1.
The alignment loss La is a structured lossfunction that penalizes alignments that are faraway from the ground truth.
We used the struc-tured hinge loss (Taskar et al, 2004; Tsochan-taridis et al, 2005) for this purpose.La(s, t,y?)
= maxy wT?
(s, t,y)+?
(y,y?)?wT?
(s, t,y?)
.Here, ?
refers to the Hamming distance be-tween the alignments.2.
The chunk score loss Lc is designed to pe-nalize errors in predicted chunk level similar-ities.
To account for cases where chunk bound-aries may be incorrect, we define this loss asthe sum of squared errors of token similarities.However, neither our output nor the gold stan-dard similarities are at the granularity of tokens.Thus, to compute the loss, we project the chunkscores zi,j,l for an aligned chunk pair (si, tj , l)to the tokens that constitute the chunks byequally partitioning the scores among all pos-sible internal alignments.
In other words, for atoken wi in the chunk si and token wj in chunksj , we define token similarity scores asz(wi, wj , l) = zi,j,lN(si,tj)Here, the normalizing function N is the prod-uct of the number of tokens in the chunks3.Note that this definition of the token similarityscores applies to both predicted and gold stan-dard similarities.
Unaligned tokens are associ-ated with a zero score.We can now define the loss for a token pair(wi, wj) ?
(s, t) and a label l as the squarederror of their token similarity scores:l(wi, wj , l) = (z(wi, wj , l)?
z?
(wi, wj , l))23Following the official evaluation of the interpretable STStask, we also experimented with the max(|si|, |tj |) for the nor-malizer, but we found via cross validation that the product per-forms better.The chunk loss score Lc for a sentence pair isthe sum of all the losses over all pairs of tokensand labels.Lc(s, t,y,y?, z, z?)
=?wi,wj ,ll(wi, wj , l)3.
The sentence similarity loss Ls provides feed-back to the aligner by penalizing alignmentsthat are far away from the ground truth in theirsimilarity assessments.
For a pair of sentences(s, t), given the ground truth sentence simi-larity r?
and the predicted sentence similarityr (using Equation (3)), the sentence similarityloss is the squared error:Ls(s, t, r?)
= (r ?
r?
)2 .Our learning objective is the weighted combina-tion of the above three components and a `2 regular-izer on the weight vector.
The importance of eachtype of loss is controlled by a corresponding hyper-parameter: ?a, ?c and ?s respectively.Learning algorithm We have two scenarios toconsider: with only alignment dataset DA, and withboth DA and sentence dataset DS .
Note that evenif we train only on the alignment dataset DA, ourlearning objective is not convex because the activa-tion function is sigmoidal (in Section 3.1).In both cases, we use stochastic gradient descentwith minibatch updates as the optimizer.
In the firstscenario, we simply perform the optimization usingthe alignment and the chunk score losses.
We foundby preliminary experiments on training data that ini-tializing the weights to one performed best.Algorithm 1 Learning alignments and similarities,given alignment dataset DA and sentence similaritydataset DS .
See the text for more details.1: Initialize all weights to one.2: w0 ?
SGD(DA): Train an initial model3: Use w0 to predict alignments on examples inDS .
Call this D?S .4: w ?
SGD(DA ?
D?S): Train on both sets ofexamples.5: return wWhen we have both DA and DS (Algorithm 1),we first initialize the model on the alignment data2197only.
Using this initial model, we hypothesize align-ments on all examples in DS to get fully labeled ex-amples.
Then, we optimize the full objective (allthree loss terms) on the combined dataset.
Becauseour goal is to study the impact on the chunk levelpredictions, in the full model, the sentence loss doesnot play a part on examples from DA.5 Experiments and ResultsThe primary research question we seek to answer viaexperiments is: Can we better predict chunk align-ments and similarities by taking advantage of sen-tence level similarity data?Datasets We used the training and test data fromthe 2016 SemEval shared tasks of predicting seman-tic textual similarity (Agirre et al, 2016a) and inter-pretable STS (Agirre et al, 2016b), that is, tasks 1and 2 respectively.
For our experiments, we used theheadlines and images sections of the data.
The datafor the interpretable STS task, consisting of manu-ally aligned and scored chunks, provides the align-ment datasets for training (DA).
The headlines sec-tion of the training data consists for 756 sentencepairs, while the images section consists for 750 sen-tence pairs.
The data for the STS task acts as oursentence level training dataset (DS).
For the head-lines section, we used the 2013 headlines test setconsisting of 750 sentence pairs with gold sentencesimilarity scores.
For the images section, we usedthe 2014 images test set consisting of 750 exam-ples.
We evaluated our models on the official Task 2test set, consisting of 375 sentence pairs for both theheadlines and images sections.
In all experiments,we used gold standard chunk boundaries if they areavailable (i.e., for DA).Pre-processing We pre-processed the sentenceswith parts of speech using the Stanford CoreNLPtoolkit (Manning et al, 2014).
Since our setting as-sumes that we have the chunks as input, we usedthe Illinois shallow parser (Clarke et al, 2012) toextract chunks from DS .
We post-processed thepredicted chunks to correct for errors using the fol-lowing steps: 1.
Split on punctuation; 2.
Split onverbs in NP; 3.
Split on nouns in VP; 4.
MergePP+NP into PP; 5.
Merge VP+PRT into VP if thePRT chunk is not a preposition or a subordinatingconjunction; 6.
Merge SBAR+NP into SBAR; and7.
Create new contiguous chunks using tokens thatare marked as being outside a chunk by the shal-low parser.
We found that using the above post-processing rules, improved the F1 of chunk accuracyfrom 0.7865 to 0.8130.
We also found via cross-validation that this post-processing improved overallalignment accuracy.
The reader may refer to otherSTS resources (Karumuri et al, 2015) for furtherimprovements along this direction.Experimental setup We performed stochasticgradient descent for 200 epochs in our experiments,with a mini-batch size of 20.
We determined thethree ?
?s using cross-validation, with different hy-perparameters for examples fromDA andDS .
Table1 lists the best hyperparameter values.
For perform-ing inference, we used the Gurobi optimizer4.Setting ?a, ?c, ?sheadlines, DA 100, 0.01, N/Aheadlines, DS 0.5, 1, 50images, DA 100, 0.01, N/Aimages, DS 5, 2.5, 50Table 1: Hyperparameters for the various settings,chosen by cross-validation.
The alignment datasetdo not have a ?
associated with the sentence loss.As noted in Section 3.1, the parameter ?l com-bines chunk scores into sentence scores.
To findthese hyper-parameters, we used a set of 426 sen-tences from the from the headlines training data thathad both sentence and chunk annotation.
We sim-plified the search by assuming that ?Equi is always1.0 and all labels other than OPPO have the same ?.Using grid search over [?1, 1] in increments of 0.1,we selected ?
?s that gave us the highest Pearson cor-relation for sentence level similarities.
The best ?
?s(with a Pearson correlation of 0.7635) were:?l =????
?1, l = EQUI,?1, l = OPPO,0.7, otherwiseResults Following the official evaluation for theSemEval task, we evaluate both alignments and their4http://www.gurobi.com/2198Setting untyped typedali score ali scoreBaseline 0.8462 0.7610 0.5462 0.5461Rank 1 0.8194 0.7865 0.7031 0.6960DA 0.9257 0.8377 0.7350 0.6776DA +DS 0.9235 0.8591 0.7281 0.6948(a) Headlines resultsSetting untyped typedali score ali scoreBaseline 0.8556 0.7456 0.4799 0.4799Rank 1 0.8922 0.8408 0.6867 0.6708DA 0.8689 0.7905 0.6933 0.6411DA +DS 0.8738 0.8193 0.7011 0.6769(b) Imags resultsTable 2: F-score for headlines and images datasets.
These tables show the result of our systems, baselineand top-ranked systems.
DA is our strong baseline trained on interpretable STS dataset; DA +DS is trainedon interpretable STS as well as STS dataset.
The rank 1 system on headlines is Inspire (Kazmi and Schu?ller,2016) and UWB (Konopik et al, 2016) on images.
Bold are the best scores.corresponding similarity scores.
The typed align-ment evaluation (denoted by typed ali in the resultstable) measures F1 over the alignment edges wherethe types need to match, but scores are ignored.
Thetyped similarity evaluation (denoted by typed score)is the more stringent evaluation that measures F1 ofthe alignment edge labels, but penalizes them if thesimilarity scores do not match.
The untyped ver-sions of alignment and scored alignment evaluationsignore alignment labels.
These metrics, based onMelamed (1997), are tailored for the interpretableSTS task5.
We refer the reader to the guidelines ofthe task for further details.
We report both scores inTable 2.
We also list the performance of the base-line system (Sultan et al, 2014a) and the top rankedsystems from the 2016 shared task for each dataset6.By comparing the rows labeledDA andDA +DSin Table 2 (a) and Table 2 (b), we see that in both theheadlines and the images datasets, adding sentencelevel information improves the untyped score, liftingthe stricter typed score F1.
On the headlines dataset,incorporating sentence-level information degradesboth the untyped and typed alignment quality be-cause we cross-validated on the typed score metric.The typed score metric is the combination of un-typed alignment, untyped score and typed align-ment.
From the row DA +DS in Table 2(a), we ob-serve that the typed score F1 is slightly behind thatof rank 1 system while all other three metrics aresignificantly better, indicating that we need to im-prove our modeling of the intersection of the threeaspects.
However, this does not apply to images5In the SemEval 2016 shared task, the typed score is themetric used for system ranking.6http://alt.qcri.org/semeval2016/task2/dataset where the improvement on the typed scoreF1 comes from the typed alignment.Further, we see that even our base model thatonly depends on the alignment data offers strongalignment F1 scores.
This validates the utility ofjointly modeling alignments and chunk similarities.Adding sentence data to this already strong systemleads to performance that is comparable to or betterthan the state-of-the-art systems.
Indeed, our finalresults would have been ranked first on the imagestask and a close second on the headlines task in theofficial standings.The most significant feedback coming fromsentence-level information is with respect to thechunk similarity scores.
While we observed slightchange in the unscored alignment performance, forboth the headlines and the images datasets, we sawimprovements in both scored precision and recallwhen sentence level data was used.6 Analysis and DiscussionIn this section, first, we report the results of man-ual error analysis.
Then, we study the ability of ourmodel to handle data from different domains.6.1 Error AnalysisTo perform a manual error analysis, we selected40 examples from the development set of the head-lines section.
We classified the errors made by thefull model trained on the alignment and sentencedatasets.
Below, we report the four most significanttypes of errors:1.
Contextual implication: Chunks that aremeant to be aligned are not synonyms by them-2199selves but are implied by the context.
For in-stance, Israeli forces and security forces mightbe equivalent in certain contexts.
Out of the 16instances of EQUI being misclassified as SPE,eight were caused by the features?
inability toascertain contextual implications.
This also ac-counted for four out of the 15 failures to iden-tify alignments.2.
Semantic phrase understanding: These arethe cases where our lexical resources failed, e.g., ablaze and left burning.
This accounted forten of the 15 chunk alignment failures and nineof the 21 labeling errors.
Among these, someerrors (four alignment failures and four label-ing errors) were much simpler than others thatcould be handled with relatively simple fea-tures (e.g.
family reunions?
family unions).3.
Preposition semantics: The inability to ac-count for preposition semantics accounts forthree of the 16 cases where EQUI is mistaken asa SPE.
Some examples include at 91 ?
aged91 and catch fire?
after fire.4.
Underestimated EQUI score: Ten out of 14cases of score underestimation happened onEQUI label.Our analysis suggests that we need better contex-tual features and phrasal features to make furthergains in aligning constituents.6.2 Does the text domain matter?In all the experiments in Section 5, we used sentencedatasets belonging to the same domain as the align-ment dataset (either headlines or images).
Giventhat our model can take advantage of two separatedatasets, a natural question to ask is how the do-main of the sentence dataset influences overall align-ment performance.
Additionally, we can also askhow well the trained classifiers perform on out-of-domain data.
We performed a series of experimentsto explore these two questions.
Table 3 summarizesthe results of these experiments.The columns labeled Train and Test of the ta-ble show the training and test sets used.
Eachdataset can be either the headlines section (denotedby hdln), or the images section (img) or not used(?).
The last two columns report performance on thetest set.
The rows 1 and 5 in the table correspond tothe in-domain settings and match the results of typedalignment and score in Table 2.Id Train Test Typed F1DA DS ali score1.hdln?
hdln 0.7350 0.67762. img 0.6826 0.63473. ?
img 0.6547 0.59894. img 0.6161 0.58545.img?
img 0.6933 0.64116. hdln 0.7033 0.67937. ?
hdln 0.6702 0.62748. hdln 0.6672 0.6445Table 3: F-score for the domain adaptation experi-ments.
This table shows the performance of trainingon different dataset combinations.When the headlines data is tested on the imagessection, we see that there is the usual domain adap-tation problem (row 3 vs row 1) and using target im-ages sentence data does not help (row 4 vs row 3).In contrast, even though there is a domain adaptationproblem when we compare the rows 5 and 7, we seethat once again, using headlines sentence data im-proves the predicted scores (row 7 vs row 8).
Thisobservation can be explained by the fact that the im-ages sentences are relatively simpler and headlinesdataset can provide richer features in comparison,thus allowing for stronger feedback from sentencesto constituents.The next question concerns how the domain of thesentence dataset DS influences alignment and sim-ilarity performance.
To answer this, we can com-pare the results in every pair of rows (i.e., 1 vs 2,3 vs 4, etc.)
We see that when the sentence datafrom the image data is used in conjunction to theheadlines chunk data, it invariably makes the clas-sifiers worse.
In contrast, the opposite trend is ob-served when the headlines sentence data augmentsthe images chunk data.
This can once again beexplained by relatively simpler sentence construc-tions in the images set, suggesting that we can lever-age linguistically complex corpora to improve align-ment on simpler ones.
Indeed, surprisingly, we ob-tain marginally better performance on the images setwhen we use images chunk level data in conjunction2200with the headlines sentence data (row 6 vs the rowlabeled DA +DS in the Table 2(b)).7 Related WorkAligning words and phrases between pairs of sen-tences is widely studied in NLP.
Machine translationhas a rich research history of using alignments (fore.g., (Koehn et al, 2003; Och and Ney, 2003)), go-ing back to the IBM models (Brown et al, 1993).From the learning perspective, the alignments areoften treated as latent variables during learning, asin this work where we treated alignments in the sen-tence level training examples as latent variables.
Ourwork is also conceptually related to (Ganchev et al,2008), which asked whether improved alignment er-ror implied better translation.Outside of machine translation, alignments areemployed either explicitly or implicitly for recog-nizing textual entailment (Brockett, 2007; Changet al, 2010a) and paraphrase recognition (Das andSmith, 2009; Chang et al, 2010a).
Additionally,alignments are explored in multiple ways (tokens,phrases, parse trees and dependency graphs) as afoundation for natural logic inference (Chambers etal., 2007; MacCartney and Manning, 2007; Mac-Cartney et al, 2008).
Our proposed aligner can beused to aid such applications.For predicting sentence similarities, in both vari-ants of the task, word or chunk alignments have ex-tensively been used (Sultan et al, 2015; Sultan etal., 2014a; Sultan et al, 2014b; Ha?nig et al, 2015;Karumuri et al, 2015; Agirre et al, 2015b; Banjadeet al, 2015, and others).
In contrast to these sys-tems, we proposed a model that is trained jointly topredict alignments, chunk similarities and sentencesimilarities.
To our knowledge, this is the first ap-proach that combines sentence-level similarity datawith fine grained alignments to train a chunk aligner.8 ConclusionIn this paper, we presented the first joint frame-work for aligning sentence constituents and pre-dicting constituent and sentence similarities.
Weshowed that our predictive model can be trained us-ing both aligned constituent data and sentence simi-larity data.
Our jointly trained model achieves state-of-the-art performance on the task of predicting in-terpretable sentence similarities.AcknowledgmentsThe authors wish to thank the anonymous reviewersand the members of the Utah NLP group for theirvaluable comments and pointers to references.References[Agirre et al2012] Eneko Agirre, Mona Diab, Daniel Cer,and Aitor Gonzalez-Agirre.
2012.
SemEval-2012 task6: A pilot on semantic textual similarity.
In *SEM2012: The First Joint Conference on Lexical and Com-putational Semantics.
[Agirre et al2015a] Eneko Agirre, Carmen Banea, ClaireCardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, In?igo Lopez-Gazpio, MontseMaritxalar, Rada Mihalcea, German Rigau, LarraitzUria, and Janyce Wiebe.
2015a.
SemEval-2015 Task2: Semantic Textual Similarity, English, Spanish andPilot on Interpretability.
In Proceedings of the 9th In-ternational Workshop on Semantic Evaluation.
[Agirre et al2015b] Eneko Agirre, Aitor Gonzalez-Agirre, Inigo Lopez-Gazpio, Montse Maritxalar,German Rigau, and Larraitz Uria.
2015b.
UBC:Cubes for English Semantic Textual Similarity andSupervised Approaches for Interpretable STS.
InProceedings of the 9th International Workshop onSemantic Evaluation.
[Agirre et al2016a] Eneko Agirre, Carmen Banea, DanielCer, Mona Diab, Aitor Gonzalez-Agirre, Rada Mi-halcea, German Rigau, and Janyce Wiebe.
2016a.SemEval-2016 Task 1: Semantic Textual Similarity,Monolingual and Cross-lingual Evaluation.
In Pro-ceedings of the 10th International Workshop on Se-mantic Evaluation.
[Agirre et al2016b] Eneko Agirre, Aitor Gonzalez-Agirre, Inigo Lopez-Gazpio, Montse Maritxalar, Ger-man Rigau, and Larraitz Uria.
2016b.
SemEval-2016Task 2: Interpretable Semantic Textual Similarity.
InProceedings of the 10th International Workshop onSemantic Evaluation.
[Banjade et al2015] Rajendra Banjade, Nobal B Niraula,Nabin Maharjan, Vasile Rus, Dan Stefanescu, MihaiLintean, and Dipesh Gautam.
2015.
NeRoSim: ASystem for Measuring and Interpreting Semantic Tex-tual Similarity.
Proceedings of the 9th InternationalWorkshop on Semantic Evaluation.
[Brockett2007] Chris Brockett.
2007.
Aligning the RTE2006 corpus.
Technical Report MSR-TR-2007-77,Microsoft Research.2201[Brown et al1993] Peter F Brown, Vincent J Della Pietra,Stephen A Della Pietra, and Robert L Mercer.
1993.The mathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics.
[Chambers et al2007] Nathanael Chambers, Daniel Cer,Trond Grenager, David Hall, Chloe Kiddon, Bill Mac-Cartney, Marie-Catherine De Marneffe, Daniel Ram-age, Eric Yeh, and Christopher D Manning.
2007.Learning Alignments and Leveraging Natural Logic.In Proceedings of the ACL-PASCAL Workshop on Tex-tual Entailment and Paraphrasing.
Association forComputational Linguistics.
[Chang et al2010a] Ming-Wei Chang, Dan Goldwasser,Dan Roth, and Vivek Srikumar.
2010a.
Discrimi-native Learning over Constrained Latent Representa-tions.
In Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics.
[Chang et al2010b] Ming-Wei Chang, Vivek Srikumar,Dan Goldwasser, and Dan Roth.
2010b.
StructuredOutput Learning with Indirect Supervision.
In In Pro-ceedings of the 27th International Conference on Ma-chine Learning.
[Clarke et al2012] James Clarke, Vivek Srikumar, MarkSammons, and Dan Roth.
2012.
An NLP Curator(or: How I Learned to Stop Worrying and Love NLPPipelines).
In Proceedings of the Eighth InternationalConference on Language Resources and Evaluation(LREC-2012).
[Dagan et al2013] Ido Dagan, Dan Roth, Mark Sam-mons, and Fabio M. Zanzotto.
2013.
RecognizingTextual Entailment: Models and Applications.
Syn-thesis Lectures on Human Language Technologies.
[Das and Smith2009] Dipanjan Das and Noah A Smith.2009.
Paraphrase identification as probabilistic quasi-synchronous recognition.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing.
[Dolan et al2004] Bill Dolan, Chris Quirk, and ChrisBrockett.
2004.
Unsupervised Construction of LargeParaphrase Corpora: Exploiting Massively ParallelNews Sources.
In COLING 2004: Proceedings of the20th International Conference on Computational Lin-guistics.
[Ganchev et al2008] Kuzman Ganchev, Joao V Grac?a,and Ben Taskar.
2008.
Better Alignments= BetterTranslations?
Proceedings of ACL-08: HLT.
[Ganitkevitch et al2013] Juri Ganitkevitch, BenjaminVan Durme, and Chris Callison-Burch.
2013.
PPDB:The Paraphrase Database.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies.
[Ha?nig et al2015] Christian Ha?nig, Robert Remus, andXose De La Puente.
2015.
ExB Themis: ExtensiveFeature Extraction from Word Alignments for Seman-tic Textual Similarity.
Proceedings of the 9th Interna-tional Workshop on Semantic Evaluation.
[Karumuri et al2015] Sakethram Karumuri, ViswanadhKumar Reddy Vuggumudi, and Sai Charan Raj Chiti-rala.
2015.
UMDuluth-BlueTeam: SVCSTS-A Multi-lingual and Chunk Level Semantic Similarity System.Proceedings of the 9th International Workshop on Se-mantic Evaluation.
[Kazmi and Schu?ller2016] Mishal Kazmi and PeterSchu?ller.
2016.
Inspire at SemEval-2016 Task 2:Interpretable Semantic Textual Similarity Alignmentbased on Answer Set Programming.
In Proceedingsof the 10th International Workshop on SemanticEvaluation, June.
[Koehn et al2003] Philipp Koehn, Franz Josef Och, andDaniel Marcu.
2003.
Statistical Phrase-Based Trans-lation.
In Proceedings of the 2003 Human LanguageTechnology Conference of the North American Chap-ter of the Association for Computational Linguistics.
[Konopik et al2016] Miloslav Konopik, Ondrej Prazak,David Steinberger, and Toma?s?
Brychc??n.
2016.
UWBat SemEval-2016 Task 2: Interpretable SemanticTextual Similarity with Distributional Semantics forChunks.
In Proceedings of the 10th InternationalWorkshop on Semantic Evaluation, June.
[Leacock and Chodorow1998] Claudia Leacock and Mar-tin Chodorow.
1998.
Combining Local Contextand WordNet Similarity for Word Sense Identification.WordNet: An Electronic Lexical Database.
[Lin1998] Dekang Lin.
1998.
An Information-TheoreticDefinition of Similarity.
In In Proceedings of the 15thInternational Conference on Machine Learning.
[MacCartney and Manning2007] Bill MacCartney andChristopher D Manning.
2007.
Natural Logic for Tex-tual Inference.
In Proceedings of the ACL-PASCALWorkshop on Textual Entailment and Paraphrasing.
[MacCartney et al2008] Bill MacCartney, Michel Galley,and Christopher D Manning.
2008.
A Phrase-BasedAlignment Model for Natural Language Inference.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing.
[Manning et al2014] Christopher D Manning, Mihai Sur-deanu, John Bauer, Jenny Rose Finkel, StevenBethard, and David McClosky.
2014.
The StanfordCoreNLP Natural Language Processing Toolkit.
InProceedings of 52nd Annual Meeting of the Associa-tion for Computational Linguistics: System Demon-strations.
[Melamed1997] Dan Melamed.
1997.
Manual Annota-tion of Translational Equivalence: The Blinker Project.2202Technical report, Institute for Research in CognitiveScience, Philadelphia.
[Och and Ney2003] Franz Josef Och and Hermann Ney.2003.
A Systematic Comparison of Various StatisticalAlignment Models.
Computational Linguistics, Vol-ume 29, Number 1, March 2003.
[Pennington et al2014] Jeffrey Pennington, RichardSocher, and Christopher D Manning.
2014.
Glove:Global Vectors for Word Representation.
In Proceed-ings of the 2014 Conference on Empirical Methods inNatural Language Processing.
[Resnik1995] Philip Resnik.
1995.
Using InformationContent to Evaluate Semantic Similarity in a Taxon-omy.
In Proceedings of the 14th International JointConference on Artificial Intelligence.
[Roth and Yih2004] Dan Roth and Wen-Tau Yih.
2004.A Linear Programming Formulation for Global Infer-ence in Natural Language Tasks.
In HLT-NAACL 2004Workshop: Eighth Conference on Computational Nat-ural Language Learning (CoNLL-2004).
[Sultan et al2014a] Md Arafat Sultan, Steven Bethard,and Tamara Sumner.
2014a.
Back to Basics for Mono-lingual Alignment: Exploiting Word Similarity andContextual Evidence.
Transactions of the Associationof Computational Linguistics.
[Sultan et al2014b] Md Arafat Sultan, Steven Bethard,and Tamara Sumner.
2014b.
DLS@CU: Sentencesimilarity from word alignment.
In Proceedings of the8th International Workshop on Semantic Evaluation.
[Sultan et al2015] Md Arafat Sultan, Steven Bethard, andTamara Sumner.
2015.
DLS@CU: Sentence Sim-ilarity from Word Alignment and Semantic VectorComposition.
In Proceedings of the 9th InternationalWorkshop on Semantic Evaluation.
[Taskar et al2004] Ben Taskar, Carlos Guestrin, andDaphne Koller.
2004.
Max-Margin Markov Net-works.
In Advances in Neural Information ProcessingSystems 16.
[Tjong Kim Sang and Buchholz2000] Erik F TjongKim Sang and Sabine Buchholz.
2000.
Introductionto the CoNLL-2000 shared task: Chunking.
In FourthConference on Computational Natural LanguageLearning and the Second Learning Language in LogicWorkshop.
[Tsochantaridis et al2005] Ioannis Tsochantaridis,Thorsten Joachims, Thomas Hofmann, and YaseminAltun.
2005.
Large Margin Methods for Structuredand Interdependent Output Variables.
Journal ofMachine Learning Research, Volume 6.2203
