Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 78?87,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsOptimising Natural Language Generation Decision MakingFor Situated DialogueNina DethlefsDepartment of Linguistics,University of Bremendethlefs@uni-bremen.deHeriberto Cuaya?huitlGerman Research Centrefor Artificial Intelligence (DFKI)heriberto.cuayahuitl@dfki.deJette ViethenCentre for Language Technologyacquarie Universityjviethen@ics.mq.edu.auAbstractNatural language generators are faced with amultitude of different decisions during theirgeneration process.
We address the joint opti-misation of navigation strategies and referringexpressions in a situated setting with respect totask success and human-likeness.
To this end,we present a novel, comprehensive frameworkthat combines supervised learning, Hierarchi-cal Reinforcement Learning and a hierarchicalInformation State.
A human evaluation showsthat our learnt instructions are rated similarto human instructions, and significantly betterthan the supervised learning baseline.1 IntroductionNatural Language Generation (NLG) systems aretypically faced with a multitude of decisions dur-ing their generation process due to nondeterminacybetween a semantic input to a generator and its re-alised output.
This is especially true in situated set-tings, where sudden changes of context can occurat anytime.
Sources of uncertainty include (a) thesituational context, such as visible objects, or taskcomplexity, (b) the user, including their behaviourand reactions, and (c) the dialogue history, includ-ing shared knowledge or patterns of linguistic con-sistency (Halliday and Hasan, 1976) and alignment(Pickering and Garrod, 2004).Previous work on context-sensitive generation insituated domains includes Stoia et al (2006) andGaroufi and Koller (2010).
Stoia et al present asupervised learning approach for situated referringexpression generation (REG).
Garoufi and Kolleruse techniques from AI planning for the combinedgeneration of navigation instructions and referringexpressions (RE).
More generally, the NLG prob-lem of non-deterministic decision making has beenaddressed from many different angles, includingPENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langk-ilde and Knight, 1998), tree-based stochastic models(Bangalore and Rambow, 2000), maximum entropy-based ranking (Ratnaparkhi, 2000), combinatorialpattern discovery (Duboue and McKeown, 2001),instance-based ranking (Varges, 2003), chart gen-eration (White, 2004), planning (Koller and Stone,2007), or probabilistic generation spaces (Belz,2008) to name just a few.More recently, there have been several approachestowards using Reinforcement Learning (RL) (Rieseret al, 2010; Janarthanam and Lemon, 2010) or Hi-erarchical Reinforcement Learning (HRL) (Deth-lefs and Cuaya?huitl, 2010) for NLG decision mak-ing.
All of these approaches have demonstrated thatHRL/RL offers a powerful mechanism for learn-ing generation policies in the absence of completeknowledge about the environment or the user.
Itovercomes the need for large amounts of hand-crafted knowledge or data in rule-based or super-vised learning accounts.
On the other hand, RLcan have difficulties to find an optimal policy in alarge search space, and is therefore often limited tosmall-scale applications.
Pruning the search spaceof a learning agent by including prior knowledge istherefore attractive, since it finds solutions faster, re-duces computational demands, incorporates expertknowledge, and scales to complex problems.
Sug-78gestions to use such prior knowledge include Lit-man et al (2000) and Singh et al (2002), whohand-craft rules of prior knowledge obvious to thesystem designer.
Cuaya?huitl (2009) suggests us-ing Hierarchical Abstract Machines to partially pre-specify dialogue strategies, and Heeman (2007) usesa combination of RL and Information State (IS)to also pre-specify dialogue strategies.
Williams(2008) presents an approach of combining Partially-Observable Markov Decision Processes with con-ventional dialogue systems.
The Information Stateapproach is well-established in dialogue manage-ment (e.g., Bohlin et al (1999) and Larsson andTraum (2000)).
It allows the system designer tospecify dialogue strategies in a principled and sys-tematic way.
A disadvantage is that random designdecisions need to be made in cases where the bestaction, or sequence of actions, is not obvious.The contribution of this paper consists in a com-prehensive account of constrained Hierarchical Re-inforcement Learning through a combination witha hierarchical Information State (HIS), which is in-formed by prior knowledge induced from decisiontrees.
We apply our framework to the generationof navigation strategies and referring expressions ina situated setting, jointly optimised for task suc-cess and linguistic consistency.
An evaluation showsthat humans prefer our learnt instructions to the su-pervised learning-based instructions, and rate themequal to human instructions.
Simulation-based re-sults show that our semi-learnt approach learns morequickly than the fully-learnt baseline, which makesit suitable for large and complex problems.
Our ap-proach differs from Heeman?s in that we transfer itto NLG and to a hierarchical setting.
Although Hee-man was able to show that his combined approachlearns more quickly than pure RL, it is limited tosmall-scale systems.
Our ?divide-and-conquer?
ap-proach, on the other hand, scales up to large searchspaces and allows us to address complex problems.2 The Generation Tasks2.1 The GIVE-2 DomainOur domain is the generation of navigation instruc-tions and referring expressions in a virtual 3D worldin the GIVE scenario (Koller et al, 2010).
In thistask, two people engage in a ?treasure hunt?, wherean instruction giver (IG) navigates an instruction fol-lower (IF) through the world, pressing a sequence ofbuttons and completing the task by obtaining a tro-phy.
Pairs take part in three dialogues (in three dif-ferent worlds); after the first dialogue, they switchroles.
The GIVE-2 corpus (Gargett et al, 2010) pro-vides transcripts of such dialogues in English andGerman.
For this paper, we complemented the En-glish dialogues of the corpus with a set of seman-tic annotations.1 The feature set is organised in fivegroups (Table 1).
The first two groups cover manip-ulation instructions (i.e., instructions to press a but-ton), including distractors2 and landmarks (Gargettet al, 2010).
The third group describes high- andlow-level navigation, the fourth group describes theuser.
The fifth group finally contains grammaticalinformation.2.2 Navigation and Manipulation InstructionsNavigation instructions can take many forms, evenfor the same route.
For example, a way to anotherroom can be described as ?go to the room with thelamp?, ?go left and through the door?, or ?turn 90degrees, left, straight?.
Choosing among these vari-ants is a highly context- and speaker-dependent task.Figure 1 shows the six user strategies we identifiedfrom the corpus based on an analysis of the combi-nation of navigation level (?high?
vs.
?low?)
and con-tent (?destination?, ?direction?, ?orientation?, ?path?,?straight?).
User models are based on the navigationlevel and content decisions made in a sequence of in-structions, so that different sequences, with a certaindistribution, lead to different user model classifica-tions.
The proportions are shown in Figure 1.
Wefound that 75% of all speakers use the same strat-egy in consecutive rounds/games.
62.5% of pairsare consistent over all three dialogues, indicatinginter-speaker alignment.
These high measures ofhuman consistency suggest that this phenomenonis worth modelling in a learning agent, and there-fore provides the motivation of including linguis-tic consistency in our agent?s behaviour.
Manipula-tion instructions were treated as an REG task, whichneeds to be sensitive to the properties of the referentand distractors (e.g, size, colour, or spatial relation1The annotations are available on request.2Distractors are objects of the same type as the referent.79ID Feature Type Descriptionf1 absolute property(referent) boolean Is the colour of the referent mentioned?f2 absolute property(distractor) boolean Is the colour of the distractor mentioned?f3 discriminative colour(referent) boolean Is the colour of the referent discriminating?f4 discriminative colour(distractor) boolean Is the colour of the distractor discriminating?f5 mention(distractor) boolean Is a distractor mentioned?f6 first mention(referent) boolean Is this the first reference to the referent?f7 mention(macro landmark) boolean Is a macro (non-movable) landmark mentioned?f8 mention(micro landmark) boolean Is a micro (movable) landmark mentioned?f9 num(distractors) integer How many distractors are present?f10 num(micro landmarks) integer How many micro landmarks are present?f11 spatial rel(referent,obj) string Which spatial relation(s) are used in the RE?f12 taxonomic property(referent) boolean Is the type of the distractor mentioned?f13 within field of vision(referent) boolean Is the referent within the user?s field of vision?f14 mention(colour, lm) boolean Is the colour of a macro- / micro lm mentioned?f15 mention(size, lm) boolean Is the size of a macro- / micro lm mentioned?f16 abstractness(nav instruction) string Is the instruction explicit or implicit?f17 content(nav instruction) string Vals: destination, direction, orientation, path, straightf18 level(nav instruction) string Is the instruction high- or low-level?f19 position(user) string Is the user on track or off track?f20 reaction(user) string Vals: take action, take wrong action, wait, req helpf21 type(user) string Vals: likes waiting, likes exploring, in betweenf22 waits(user) boolean Is the user waiting for the next instruction?f23 model(user) string User model/navig.
strategy used (cf.
Fig.1)?f24 actor(instruction) boolean Is the actor of the instruction inserted?f25 mood(instruction) boolean Is the mood of the instruction inserted?f26 process(instruction) boolean Is the process of the instruction inserted?f27 locational phrase(instruction) boolean Is the loc.
phrase (path, straight, etc.)
inserted?Table 1: Corpus annotation features that were used as knowledge of the learning agent and the Information State.
Fea-tures are presented in groups, describing the properties of referents in the environment (f1...f13) and their distractors(f14...f15), features of high- and low-level navigation (f16...f18), the user (f19...f23), and grammatical informationabout constituents (f24...f27).with respect to the referent) to be natural and dis-tinguishing.
We also considered the visual salienceof objects, and the type of spatial relation involved,since recent studies indicate the potential relevanceof these features (Viethen and Dale, 2008).
Giventhese observations, we aim to optimise the task suc-cess and linguistic consistency of instructions.
Tasksuccess is measured from user reactions after eachinstruction (Section 5.1).
Linguistic consistency isachieved by rewarding the agent for generating in-structions that belong to the same user model as theprevious one.
The agent has the same probabilityfor choosing any pattern, but is then rewarded forconsistency.
Table 3 (in Section 5.2) presents an ex-ample dialogue generated by our system.3 Constrained Hierarchical ReinforcementLearning for NLG3.1 Hierarchical Reinforcement LearningOur idea of language generation as an optimisa-tion problem is as follows: given a set of genera-tion states, a set of actions, and an objective rewardfunction, an optimal generation strategy maximisesthe objective function by choosing the actions lead-ing to the highest reward for every reached state.Such states describe the system?s knowledge about80Figure 1: Decision tree for the classification of usermodels (UM) defined by the use of navigation level andcontent.
UM 0=high-level, UM 1=low-level (LL), UM2=orientation-based LL, UM 3=orientation-based mix-ture (M), UM 4=path-based M, UM 5=pure M.the generation task (e.g.
navigation strategy, or re-ferring expressions).
The action set describes thesystem?s capabilities (e.g.
?use high level naviga-tion strategy?, ?mention colour of referent?, etc.
).The reward function assigns a numeric value foreach action taken.
In this way, language generationcan be seen as a finite sequence of states, actionsand rewards {s0, a0, r1, s1, a1, ..., rt?1, st}, wherethe goal is to find an optimal strategy automatically.To do this we use RL with a divide-and-conquer ap-proach in order to optimise a hierarchy of generationpolicies rather than a single policy.
The hierarchy ofRL agents consists of L levels and N models perlevel, denoted as M ij , where j ?
{0, ..., N ?
1}and i ?
{0, ..., L ?
1}.
Each agent of the hierar-chy is defined as a Semi-Markov Decision Process(SMDP) consisting of a 4-tuple < Sij, Aij , T ij , Rij >.Sij is a set of states, Aij is a set of actions, T ij isa transition function that determines the next states?
from the current state s and the performed ac-tion a, and Rij is a reward function that specifiesthe reward that an agent receives for taking an ac-tion a in state s lasting ?
time steps.
The randomvariable ?
represents the number of time steps theagent takes to complete a subtask.
Actions can beeither primitive or composite.
The former yield sin-gle rewards, the latter correspond to SMDPs andyield cumulative discounted rewards.
The goal ofeach SMDP is to find an optimal policy that max-imises the reward for each visited state, according to?
?ij(s) = arg maxa?Aij Q?ij(s, a), where Q?ij (s, a)specifies the expected cumulative reward for exe-cuting action a in state s and then following pol-icy ?
?ij .
We use HSMQ-Learning (Dietterich, 1999)for learning a hierarchy of generation policies.
Thishierarchical approach has been applied successfullyto dialogue strategy learning by Cuaya?huitl et al(2010).3.2 Information StateThe notion of an Information State has traditionallybeen applied to dialogue, where it encodes all infor-mation relevant to the current state of the dialogue.This includes, for example, the context of the in-teraction, participants and their beliefs, and the sta-tus of grounding.
An IS consists of a set of infor-mational components, encoding the information ofthe dialogue, formal representations of these com-ponents, a set of dialogue moves leading to the up-date of the IS, a set of update rules which govern theupdate, and finally an update strategy, which speci-fies which update rule to apply in case more than oneapplies (Larsson and Traum (2000), p. 2-3).
In thispaper, we apply the theory of IS to language gener-ation.
For this purpose we define the informationalcomponents of an IS to represent the (situational andlinguistic) knowledge of the generator (Section 4.2).Update rules are triggered by generator actions, suchas the decision to insert a new constituent into thecurrent logical form, or the decision to prefer oneword order sequence over another.
We use the DIP-PER toolkit (Bos et al, 2003)3 for our implementa-tion of the IS.3.3 Combining Hierarchical ReinforcementLearning and Information StatePrevious work has suggested the HSMQ-Learningalgorithm for optimizing text generation strategies(Dethlefs and Cuaya?huitl, 2010).
Because such analgorithm uses all available actions in each state,an important extension is to constrain the actionsavailable with some prior expert knowledge, aim-ing to combine behaviour specified by human de-signers and behaviour automatically inferred by re-inforcement learning agents.
To that end, we sug-3http://www.ltg.ed.ac.uk/dipper81Figure 2: (Left:) Hierarchy of learning agents executed from top to bottom for generating instructions.
(Right:) Staterepresentations for the agents shown in the hierarchy on the left.
The features f1...f27 refer back to the features usedin the annotation given in the first column of Table 1.
Note that agents can share information across levels.gest combining the Information State approach withhierarchical reinforcement learning.
We thereforere-define the characterisation of each Semi-MarkovDecision Process (SMDP) in the hierarchy as a 5-tuple model M ij =< Sij, Aij , T ij , Rij , Iij >, whereSij , Aij , T ij and Rij are as before, and the additionalelement Iij is an Information State used as knowl-edge base and rule-based decision maker.
In this ex-tended model, action selection is based on a con-strained set of actions provided by the IS updaterules.
We assume that the names of update rulesin Iij represent the agent actions Aij .
The goal ofeach SMDP is then to find an optimal policy thatmaximises the reward for each visited state, accord-ing to ?
?ij(s) = arg maxa?Aij?Iij Q?ij(s, a), whereQ?ij (s, a) specifies the expected cumulative rewardfor executing constrained action a in state s and thenfollowing ?
?ij thereafter.
For learning such poli-cies we use a modified version of HSMQ-Learning.This algorithm receives subtask M ij and InformationState Iij used to initialise state s, performs similarlyto Q-Learning for primitive actions, but for compos-ite actions it invokes recursively with a child sub-task.
In contrast to HSMQ-Learning, this algorithmchooses actions from a subset derived by applyingthe IS update rules to the current state of the world.When the subtask is completed, it returns a cumu-lative reward rt+?
, and continues its execution untilfinding a goal state for the root subtask.
This processiterates until convergence occurs to optimal context-independent policies, as in HSMQ-Learning.4 Experimental Setting4.1 Hierarchy of AgentsFigure 2 shows a (hand-crafted) hierarchy of learn-ing agents for navigating and acting in a situated en-vironment.
Each of these agents represents an indi-vidual generation task.
Model M00 is the root agentand is responsible for ensuring that a set of naviga-tion instructions guide the user to the next referent,where an RE is generated.
Model M10 is responsiblefor the generation of the RE that best describes anintended referent.
Subtasks M20 ... M22 realise sur-face forms of possible distractors, or macro- / microlandmarks.
Model M12 is responsible for the gener-ation of navigation instructions which smoothly fitinto the linguistic consistency pattern chosen.
Partof this task is choosing between a low-level (modelM23 ) and a high-level (model M24 ) instruction.
Sub-tasks M30 ...M34 realise the actual instructions, des-tination, direction, orientation, path, and ?straight?,respectively.4 Finally, model M11 can repair previ-ous system utterances.4Note that navigation instructions and REs correspond to se-quences of actions, not to a single one.82Model(s) ActionsM00 navigation, manipulation, confirmation, stop, repair system act, repair no system actM10 insert distractor, insert no distractor, insert no absolute property, insert micro relatum, insert macro relatuminsert no taxonomic property, insert absolute property, insert no macro relatum, insert taxonomic propertyM12 choose high level, choose low level, get route, choose easy route, choose short routeM20 ... M22 exp head, exp no head, insert colour, insert no colour, insert size, insert no size, exp spatial relationM23 choose explicit abstractness, choose implicit abstractness, destination instruction, path instructionM24 choose explicit abstractness, choose implicit abstractness, direction instr, orientation instr, straight instrM30 ... M34 exp actor, exp no actor, exp mood, exp loc phrase, exp no loc phrase, exp process, exp no processTable 2: Action set of the learning agents and Information States.4.2 State and Action SetsThe HRL agent?s knowledge base consists of all sit-uational and linguistic knowledge the agent needsfor decision making.
Figure 2 shows the hierarchyof learning agents together with the knowledge baseof the learning agent with respect to the semanticfeatures shown in Table 1 that were used for the an-notation of the GIVE-2 corpus dialogues.
The firstcolumn of the table in Figure 2 indicates the respec-tive model, also referred to as agent, or subtask, andthe second column refers to the knowledge variableit uses (in the form of the feature index given in thefirst column of Table 1).
In the agent, boolean valuesand strings were represented as integers.
The HISshares all information of the learning agent, but hasan additional set of relational feature-value pairs foreach slot.
For example, if the agent knows that theslot content(nav instruction) has value 1 (mean-ing ?filled?
), the HIS knows also which value it wasfilled with, such as path.
Such additional knowledgeis required for the supervised learning baseline (Sec-tion 5).
The action set of the hierarchical learningagent and the hierarchical information state is givenin Table 2.
The state-action space size of a flat learn-ing agent would be |S ?A| = 1011, the hierarchicalsetting has a state-action space size of 2.4 ?
107.The average state-action space size of all subtasks is|S ?
A|/14 = 1.7 ?
107.
Generation actions canbe primitive or composite.
While the former corre-spond to single generation decisions, the latter rep-resent separate generation subtasks (Fig.
2).4.3 Prior KnowledgePrior knowledge can include decisions obvious tothe system designer, expert knowledge, or generalintuitions.
In our case, we use a supervised learn-ing approach to induce prior knowledge into ourHRL agent.
We trained decision trees on our anno-tated corpus data using Weka?s (Witten and Frank,2005) J48 decision tree classifer.
A separate treewas trained for each semantic attribute (cf.
Table1).
The obtained decision trees represent our super-vised learning baseline.
They achieved an accuracyof 91% in a ten-fold cross-validation.
For our semi-learnt combination of HRL and HIS, we performed amanual analysis of the resulting rules to assess theirimpact on a learning agent.5 In the end, the fol-lowing rules were used to constrain the agent?s be-haviour: (1) In REs, always use a referent?s colour,except in cases of repair when colour is not discrim-inating; (2) mention a distractor or micro landmark,if the colour of the referent is not discriminating;(3) in navigation, always make orientation instruc-tions explicit.
All remaining behaviour was subjectto learning.4.4 Reward FunctionWe use the following reward function to train the hi-erarchy of policies of our HRL agent.
It aims to re-duce discourse length at maximal task success6 us-ing a consistent navigation strategy.R =????????
?0 for reaching the goal state-2 for an already invoked subtask+1 for generating instruction u con-sistent with instruction u?1-1 otherwise.5We excluded rules that always choose the same value, sincethey would work against our aim of generating consistent, butvariable instructions.6Task success is addressed by that the user has to ?accept?each instruction for a state transition.83The third reward that encourages consistency of in-structions rewards a sequence of actions that allowthe last generated instruction to be classified as be-longing to the same navigation strategy/user modelas the previously generated instruction (cf.
2.2).5 Experiments and Results5.1 The Simulated EnvironmentThe simulated environment contains two kinds ofuncertainties: (1) uncertainty regarding the state ofthe environment, and (2) uncertainty concerning theuser?s reaction to a system utterance.
The first aspectis represented by a set of contextual variables de-scribing the environment, 7 and user behaviour.8 Al-together, this leads to 115 thousand different contex-tual configurations, which are estimated from data(cf.
Section 2.1).
The uncertainty regarding theuser?s reaction to an utterance is represented by aNaive Bayes classifier, which is passed a set ofcontextual features describing the situation, mappedwith a set of semantic features describing the utter-ance.9 From these data, the classifier specifies themost likely user reaction (after each system act) ofperform desired action, perform undesired action, waitand request help.10 The classifier was trained on theannotated data and reached an accuracy of 82% in aten-fold cross validation.5.2 Learnt PoliciesWith respect to REs, the fully-learnt policy (onlyHRL) uses colour when it is discriminating, and adistractor or micro landmark otherwise.
The semi-learnt policy (HRL with HIS) behaves as defined inSection 4.3.
The supervised learning policy (onlyHIS) uses the rules learnt by the decision trees.
Bothlearnt policies learn to maximise task success, andto generate consistent navigation strategies.11 The7previous system act, route length, route status(known/unknown), objects within vision, objects withindialogue history, number of instructions, alignment(proportion)8previous user reaction, user position, user wait-ing(true/false), user type(explorative/hesitant/medium)9navigation level(high / low), abstractness(implicit / ex-plicit), repair(yes / no), instruction type(destination / direction /orientation / path / straight)10User reactions measure the system?s task success.11They thereby also learn to adapt their semantic choices tothose most frequently made by humans.101 102 103 104?80?70?60?50?40?30?20?100AverageRewardEpisodesDeterministicSemi?LearntFully?LearntFigure 3: Comparison of fully-learnt, semi-learnt, and su-pervised learning (deterministic) behaviours.supervised learning policy generates successful in-structions from the start.
Note that we are not ac-tually learning dialogue strategies, but rather gen-eration strategies using dialogue features.
There-fore the described policies, fully-learnt, semi-learntand supervised-learning, exclusively guide the sys-tem?s behaviour in the interaction with the simulateduser.
An example dialogue is shown in Table 3.
Wecan observe that the agent starts using a low levelnavigation strategy, and then switches to high level.When the user gets confused, the system temporar-ily switches back to low level.
For referring expres-sions, it first attempts to locate the referent by ref-erence to a distractor, and then repairs by using amicro landmark.
The surface forms of instructionswere realised from templates, since the NLG systemso far only generates a sequence of content selectiondecisions.
We address surface realisation in Dethlefsand Cuaya?huitl (2011).We compared our semi-learnt policy against afully-learnt, and a supervised learning baseline.
Allpolicies were trained for 40 thousand episodes.
Fortraining, the step-size parameter ?, which indicatesthe learning rate, was initiated with 1 and then re-duced over time by ?
= 11+t , where t is the time step.The discount rate ?, which indicates the relevance offuture rewards in relation to immediate rewards, wasset to 0.99, and the probability of a random action ?was 0.01.
See (Sutton and Barto, 1998) for detailson these parameters.5.3 Comparison of PoliciesFigure 3 compares the rewards averaged over 10runs.
In terms of learning speed, we can see that84Agent Policy Action (composite in italics) UtteranceUsr request route How do I get to the trophy?Sys ?00, ?12 root, navigation, dontRepair, orientation?23, ?30 low level navigation, add direction+degrees First, turn 90 degrees left.
?23, ?31 low level navigation, ?straight?
Then go straight ahead.Usr perform desired actionSys ?00 confirmation OK.?24, ?34, ?32 high level navigation, destination, direction,?24, ?34, ?32 addLocation Turn left at the end of the hallway.Usr waitSys ?11 repair system act, change navigation strategy?23, ?32 low level navigation, direction Turn right?24, ?33 high level navigation, path and go down the hallway.Usr perform desired actionSys ?23, ?32 low level navigation, direction Now turn left.Usr perform desired actionSys ?00 confirmation OK.?24, ?34 high level navigation, destination,?24, ?34 addLocation Enter to the room in front of you.Usr perform desired actionSys ?10, ?21 manipulation, use colour, use distractor Press the red button beside the blue.Usr request helpSys ?11 repair system act, change granularity?24, ?10 high level navigation, manipulation?34, ?22 destination, use colour, use micro landmark Go to the red button beside the chair.Usr perform desired actionSys ?10 manipulation Press it.Usr perform desired actionSys ?00 confirmation Well done.Table 3: Sample dialogue in the GIVE-2 scenario showing the dynamics of generation policies.
See Figure 2 for thecorresponding hierarchy models, and Table 2 for the action set.
See Section 5.2 for an explantation of the dialogue.while the semi-learnt behaviour is able to follow anear-optimal policy from the beginning, the fully-learnt policy takes about 40 thousand episodes toreach the same performance.
In terms of simulatedtask success, we see that while the supervised learn-ing behaviour follows a good policy from the start,it is eventually beaten by the learnt policies.5.4 Human Evaluation StudyWe asked 11 participants12 to rate altogether 132sets of instructions, where each set contained a spa-tial graphical scene containing a person, mappedwith one human, one learnt, and one supervised126 female, 5 male with an age average of 26.4.learning instruction.
Instructions consisted of a nav-igation instruction followed by a referring expres-sion.
Subjects were asked to rate instructions on a1-5 Likert scale (where 5 is the best) for their help-fulness on guiding the displayed person from its ori-gin to pressing the intended button.
We selectedsix different scenarios for the evaluation: (a) onlyone button is present, (b) two buttons are present,the referent and a distractor of the same colour asthe referent, (c) two buttons are present, the referentand a distractor of a different colour than the refer-ent, (d) one micro landmark is present and one dis-tractor of the same colour as the referent, (e) onemicro landmark is present and one distractor of adifferent colour than the referent.
All scenarios oc-85Figure 4: Example scenario of the human evaluation study.curred twice in each evaluation sheet, their specificinstances were drawn from the GIVE-2 corpus atrandom.
Scenes and instructions were presented ina randomised order.
Figure 4 presents an exampleevaluation scene.
Finally, we asked subjects to cir-cle the object they thought was the intended refer-ent.
Subjects rated the human instructions with anaverage of 3.82, the learnt instructions with an aver-age of 3.55, and the supervised learning instructionswith an average of 2.39.
The difference between hu-man and learnt is not significant.
The difference be-tween learnt and supervised learning is significant atp < 0.003, and the difference between human andsupervised learning is significant at p < 0.0002.
In96% of all cases, users were able to identify the in-tended referent.6 Conclusion and DiscussionWe have presented a combination of HRL with a hi-erarchical IS, which was informed by prior knowl-edge from decision trees.
Such a combined frame-work has the advantage that it allows us to system-atically pre-specify (obvious) generation strategies,and thereby find solutions faster, reduce computa-tional demands, scale to complex domains, and in-corporate expert knowledge.
By applying HRL tothe remaining (non-obvious) action set, we are ableto learn a flexible, generalisable NLG policy, whichwill take the best action even under uncertainty.
Asan application of our approach and its generalisabil-ity across domains, we have presented the joint op-timisation of two separate NLG tasks, navigation in-structions and referring expressions, in situated dia-logue under the aspects of task success and linguis-tic consistency.
Based on an evaluation in a simu-lated environment estimated from data, we showedthat our semi-learnt behaviour outperformed a fully-learnt baseline in terms of learning speed, and a su-pervised learning baseline in terms of average re-wards.
Human judges rated our instructions signif-icantly better than the supervised learning instruc-tions, and close to human quality.
The study re-vealed a task success rate of 96%.
Future workcan transfer our approach to different applications toconfirm its benefits, and induce the agent?s rewardfunction from data to test in a more realistic setting.AcknowledgmentsThanks to the German Research Foundation DFGand the Transregional Collaborative Research Cen-tre SFB/TR8 ?Spatial Cognition?
and the EU-FP7project ALIZ-E (ICT-248116) for partial support ofthis work.
Also, thanks to John Bateman for com-ments on an earlier draft of this paper.ReferencesSrinivas Bangalore and Owen Rambow.
2000.
Exploit-ing a probabilistic hierarchical model for generation.In Proceedings of the 18th conference on Computa-tional linguistics - Volume 1, pages 42?48.Anja Belz.
2008.
Automatic generation ofweather forecast texts using comprehensive probabilis-tic generation-space models.
Natural Language Engi-neering, 1:1?26.86Peter Bohlin, Robin Cooper, Elisabet Engdahl, andStaffan Larsson.
1999.
Information states and di-alogue move engines.
In IJCAI-99 Workshop onKnowledge and Reasoning in Practical Dialogue Sys-tems.Johan Bos, Ewan Klein, Oliver Lemon, and Tetsushi Oka.2003.
DIPPER: Description and Formalisation of anInformation-State Update Dialogue System Architec-ture.
In 4th SIGDial Workshop on Discourse and Dia-logue, pages 115?124.Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, andHiroshi Shimodaira.
2010.
Evaluation of a hierar-chical reinforcement learning spoken dialogue system.Computer Speech and Language, 24(2):395?429.Heriberto Cuaya?huitl.
2009.
Hierarchical ReinforcementLearning for Spoken Dialogue Systems.
Ph.D. thesis,School of Informatics, University of Edinburgh.Nina Dethlefs and Heriberto Cuaya?huitl.
2010.
Hi-erarchical Reinforcement Learning for Adaptive TextGeneration.
Proceedings of INLG ?10.Nina Dethlefs and Heriberto Cuaya?huitl.
2011.
Hier-archical Reinforcement Learning and Hidden MarkovModels for Task-Oriented Natural Language Genera-tion.
In Proceedings of ACL-HLT 2011, Portland, OR.Thomas G. Dietterich.
1999.
Hierarchical reinforce-ment learning with the maxq value function decom-position.
Journal of Artificial Intelligence Research,13:227?303.Pablo A. Duboue and Kathleen R. McKeown.
2001.
Em-pirically estimating order constraints for content plan-ning in generation.
In ACL ?01, pages 172?179.Andrew Gargett, Konstantina Garoufi, Alexander Koller,and Kristina Striegnitz.
2010.
The give-2 corpus ofgiving instructions in virtual environments.
In LREC.Konstantina Garoufi and Alexander Koller.
2010.
Au-tomated planning for situated natural language gener-ation.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages1573?1582, July.Michael A. K. Halliday and Ruqaiya Hasan.
1976.
Co-hesion in English.
Longman, London.Peter Heeman.
2007.
Combining reinforcement learningwith information-state update rules.
In Human Tech-nology Conference (HLT), pages 268?275.Srinivasan Janarthanam and Oliver Lemon.
2010.
Learn-ing to adapt to unknown users: referring expressiongeneration in spoken dialogue systems.
In ACL ?10,pages 69?78.Alexander Koller and Matthew Stone.
2007.
Sentencegeneration as planning.
In Proceedings of ACL-07.Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-tine Cassell, Robert Dale, Johanna Moore, and JonOberlander.
2010.
The first challenge on generat-ing instructions in virtual environments.
In M. The-une and E. Krahmer, editors, Empirical Methodson Natural Language Generation, pages 337?361,Berlin/Heidelberg, Germany.
Springer.Irene Langkilde and Kevin Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InACL-36, pages 704?710.Staffan Larsson and David R. Traum.
2000.
Informa-tion state and dialogue management in the TRINDIdialogue move engine toolkit.
Nat.
Lang.
Eng., 6(3-4):323?340.Diane J. Litman, Michael S. Kearns, Satinder Singh, andMarilyn A. Walker.
2000.
Automatic optimization ofdialogue management.
In Proceedings of the 18th con-ference on Computational linguistics, pages 502?508.William Mann and Christian M I M Matthiessen.
1983.NIGEL: A systemic grammar for text generation.Technical report, ISI/RR-85-105.Martin J. Pickering and Simon Garrod.
2004.
Towarda mechanistc psychology of dialog.
Behavioral andBrain Sciences, 27.Adwait Ratnaparkhi.
2000.
Trainable methods for sur-face natural language generation.
In Proceedings ofNAACL, pages 194?201.Verena Rieser, Oliver Lemon, and Xingkun Liu.
2010.Optimising information presentation for spoken dia-logue systems.
In ACL ?10, pages 1009?1018.Satinder Singh, Diane Litman, Michael Kearns, and Mar-ilyn Walker.
2002.
Optimizing Dialogue Managementwith Reinforcement Learning: Experiments with theNJFun System.
Journal of Artificial Intelligence Re-search, 16:105?133.Laura Stoia, Darla Magdalene Shockley, Donna K. By-ron, and Eric Fosler-Lussier.
2006.
Noun phrase gen-eration for situated dialogs.
In Proceedings of INLG?06, pages 81?88.Richard S Sutton and Andrew G Barto.
1998.
Reinforce-ment Learning: An Introduction.
MIT Press, Cam-bridge, MA, USA.Sebastian Varges.
2003.
Instance-based Natural Lan-guage Generation.
Ph.D. thesis, School of Informat-ics, University of Edinburgh.Jette Viethen and Robert Dale.
2008.
The use of spatialrelations in referring expression generation.
In Pro-ceedings of INLG ?08, INLG ?08, pages 59?67.Michael White.
2004.
Reining in CCG chart realization.In In Proc.
INLG-04, pages 182?191.Jason Williams.
2008.
The best of both worlds: Uni-fying conventional dialog systems and POMDPs.
InInterspeech, Brisbane.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques.
2. edi-tion.87
