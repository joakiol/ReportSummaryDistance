Orthogonal Negation in Vector Spaces for ModellingWord-Meanings and Document RetrievalDominic Widdows ?Stanford Universitydwiddows@csli.stanford.eduAbstractStandard IR systems can process queriessuch as ?web NOT internet?, enabling userswho are interested in arachnids to avoiddocuments about computing.
The docu-ments retrieved for such a query should beirrelevant to the negated query term.
Mostsystems implement this by reprocessing re-sults after retrieval to remove documentscontaining the unwanted string of letters.This paper describes and evaluates a the-oretically motivated method for removingunwanted meanings directly from the orig-inal query in vector models, with the samevector negation operator as used in quan-tum logic.
Irrelevance in vector spaces ismodelled using orthogonality, so query vec-tors are made orthogonal to the negatedterm or terms.As well as removing unwanted terms, thisform of vector negation reduces the occur-rence of synonyms and neighbours of thenegated terms by as much as 76% comparedwith standard Boolean methods.
By alter-ing the query vector itself, vector negationremoves not only unwanted strings but un-wanted meanings.1 IntroductionVector spaces enjoy widespread use in informationretrieval (Salton and McGill, 1983; Baeza-Yates and?This research was supported in part by the ResearchCollaboration between the NTT Communication ScienceLaboratories, Nippon Telegraph and Telephone Corpo-ration and CSLI, Stanford University, and by EC/NSFgrant IST-1999-11438 for the MUCHMORE project.Ribiero-Neto, 1999), and from this original appli-cation vector models have been applied to seman-tic tasks such as word-sense acquisition (Landauerand Dumais, 1997; Widdows, 2003) and disambigua-tion (Schu?tze, 1998).
One benefit of these models isthat the similarity between pairs of terms or betweenqueries and documents is a continuous function, au-tomatically ranking results rather than giving just aYES/NO judgment.
In addition, vector models canbe freely built from unlabelled text and so are bothentirely unsupervised, and an accurate reflection ofthe way words are used in practice.In vector models, terms are usually combinedto form more complicated query statements by(weighted) vector addition.
Because vector additionis commutative, terms are combined in a ?bag ofwords?
fashion.
While this has proved to be effective,it certainly leaves room for improvement: any gen-uine natural language understanding of query state-ments cannot rely solely on commutative addition forbuilding more complicated expressions out of primi-tives.Other algebraic systems such as Boolean logic andset theory have well-known operations for buildingcomposite expressions out of more basic ones.
Set-theoretic models for the logical connectives ?AND?,?NOT?
and ?OR?
are completely understood by mostresearchers, and used by Boolean IR systems for as-sembling the results to complicated queries.
It isclearly desirable to develop a calculus which com-bines the flexible ranking of results in a vector modelwith the crisp efficiency of Boolean logic, a goalwhich has long been recognised (Salton et al, 1983)and attempted mainly for conjunction and disjunc-tion.
This paper proposes such a scheme for nega-tion, based upon well-known linear algebra, andwhich also implies a vector form of disjunction.
Itturns out that these vector connectives are preciselythose used in quantum logic (Birkhoff and von Neu-mann, 1936), a development which is discussed inmuch more detail in (Widdows and Peters, 2003).Because of its simplicity, our model is easy to under-stand and to implement.Vector negation is based on the intuition that un-related meanings should be orthogonal to one an-other, which is to say that they should have no fea-tures in common at all.
Thus vector negation gener-ates a ?meaning vector?
which is completely orthog-onal to the negated term.
Document retrieval ex-periments demonstrate that vector negation is notonly effective at removing unwanted terms: it isalso more effective than other methods at removingtheir synonyms and related terms.
This justifies theclaim that, by producing a single query vector for?a NOT b?, we remove not only unwanted stringsbut also unwanted meanings.We describe the underlying motivation behind thismodel and define the vector negation and disjunc-tion operations in Section 2.
In Section 3 we re-view other ways negation is implemented in Infor-mation Retrieval, comparing and contrasting withvector negation.
In Section 4 we describe experi-ments demonstrating the benefits and drawbacks ofvector negation compared with two other methodsfor negation.2 Negation and Disjunction inVector SpacesIn this section we use well-known linear algebra todefine vector negation in terms of orthogonality anddisjunction as the linear sum of subspaces.
Themathematical apparatus is covered in greater detailin (Widdows and Peters, 2003).
If A is a set (insome universe of discourse U), then ?NOT A?
corre-sponds to the complement A?
of the set A in U (bydefinition).
By a simple analogy, let A be a vectorsubspace of a vector space V (equipped with a scalarproduct).
Then the concept ?NOT A?
should corre-spond to the orthogonal complement A?
of A underthe scalar product (Birkhoff and von Neumann, 1936,?6).
If we think of a basis for V as a set of features,this says that ?NOT A?
refers to the subspace of Vwhich has no features in common with A.We make the following definitions.
Let V be a(real) vector space equipped with a scalar product.We will use the notation A ?
V to mean ?A is avector subspace of V .?
For A ?
V , define the or-thogonal subspace A?
to be the subspaceA?
?
{v ?
V : ?a ?
A, a ?
v = 0}.For the purposes of modelling word-meanings, wemight think of ?orthogonal?
as a model for ?com-pletely unrelated?
(having similarity score zero).This makes perfect sense for information retrieval,where we assume (for example) that if two wordsnever occur in the same document then they have nofeatures in common.Definition 1 Let a, b ?
V and A,B ?
V .
ByNOT A we mean A?
and by NOT a, we mean?a?
?, where ?a?
= {?a : ?
?
R} is the 1-dimensionalsubspace subspace generated by a.
By a NOT B wemean the projection of a onto B?
and by a NOT bwe mean the projection of a onto ?b?
?.We now show how to use these notions to performcalculations with individual term or query vectors ina form which is simple to program and efficient torun.Theorem 1 Let a, b ?
V .
Then a NOT b is repre-sented by the vectora NOT b ?
a ?
a ?
b|b|2 b.where |b|2 = b ?
b is the modulus of b.Proof.
A simple proof is given in (Widdows and Pe-ters, 2003).For normalised vectors, Theorem 1 takes the par-ticularly simple forma NOT b = a?
(a ?
b)b, (1)which in practice is then renormalised for consis-tency.
One computational benefit is that Theorem 1gives a single vector for a NOT b, so finding the sim-ilarity between any other vector and a NOT b is justa single scalar product computation.Disjunction is also simple to envisage, the expres-sion b1 OR .
.
.
OR bn being modelled by the sub-spaceB = {?1b1 + .
.
.+ ?nbn : ?i ?
R}.Theoretical motivation for this formulation can befound in (Birkhoff and von Neumann, 1936, ?1,?6)and (Widdows and Peters, 2003): for example, Bis the smallest subspace of V which contains the set{bj}.Computing the similarity between a vector a andthis subspace B is computationally more expensivethan for the negation of Theorem 1, because thescalar product of a with (up to) n vectors in an or-thogonal basis for B must be computed.
Thus thegain we get by comparing each document with thequery a NOT b using only one scalar product oper-ation is absent for disjunction.However, this benefit is regained in the case ofnegated disjunction.
Suppose we negate not only oneargument but several.
If a user specifies that theywant documents related to a but not b1, b2, .
.
.
, bn,then (unless otherwise stated) it is clear that theyonly want documents related to none of the un-wanted terms bi (rather than, say, the average ofthese terms).This motivates a process which can be thought ofas a vector formulation of the classical de Morganequivalence ?
a?
?
b ??
(a ?
b), by which theexpressiona AND NOT b1 AND NOT b2 .
.
.
AND NOT bnis translated toa NOT (b1 OR .
.
.
OR bn).
(2)Using Definition 1, this expression can be modelledwith a unique vector which is orthogonal to all ofthe unwanted arguments {b1}.
However, unless thevectors b1, .
.
.
, bn are orthogonal (or identical), weneed to obtain an orthogonal basis for the subspaceb1 OR .
.
.
OR bn before we can implement a higher-dimensional version of Theorem 1.
This is becausethe projection operators involved are in general non-commutative, one of the hallmark differences be-tween Boolean and quantum logic.In this way vector negation generates a meaning-vector which takes into account the similarities anddifferences between the negative terms.
A query forchip NOT computer, siliconis treated differently from a query forchip NOT computer, potato.Vector negation is capable of realising that for thefirst query, the two negative terms are referring tothe same general topic area, but in the second casethe task is to remove radically different meaningsfrom the query.
This technique has been used toremove several meanings from a query iteratively, al-lowing a user to ?home in on?
the desired meaning bysystematically pruning away unwanted features.2.1 Initial experiments modellingword-sensesOur first experiments with vector negation were todetermine whether the negation operator could finddifferent senses of ambiguous words by negating aword closely related to one of the meanings.
A vectorspace model was built using Latent Semantic Analy-sis, similar to the systems of (Landauer and Dumais,1997; Schu?tze, 1998).
The effect of LSA is to in-crease linear dependency between terms, and for thisreason it is likely that LSA is a crucial step in ourapproach.
Terms were indexed depending on theirco-occurrence with 1000 frequent ?content-bearingwords?
in a 15 word context-window, giving eachterm 1000 coordinates.
This was reduced to 100 di-mensions using singular value decomposition.
Lateron, document vectors were assigned in the usualmanner by summation of term vectors using tf-idfweighting (Salton and McGill, 1983, p. 121).
Vectorswere normalised, so that the standard (Euclidean)scalar product and cosine similarity coincided.
Thisscalar product was used as a measure of term-termand term-document similarity throughout our exper-iments.
This method was used because it has beenfound to be effective at producing good term-termsimilarities for word-sense disambiguation (Schu?tze,1998) and automatic lexical acquisition (Widdows,2003), and these similarities were used to generate in-teresting queries and to judge the effectiveness of dif-ferent forms of negation.
More details on the build-ing of this vector space model can be found in (Wid-dows, 2003; Widdows and Peters, 2003).suit suit NOT lawsuitsuit 1.000000 pants 0.810573lawsuit 0.868791 shirt 0.807780suits 0.807798 jacket 0.795674plaintiff 0.717156 silk 0.781623sued 0.706158 dress 0.778841plaintiffs 0.697506 trousers 0.771312suing 0.674661 sweater 0.765677lawsuits 0.664649 wearing 0.764283damages 0.660513 satin 0.761530filed 0.655072 plaid 0.755880behalf 0.650374 lace 0.755510appeal 0.608732 worn 0.755260Terms related to ?suit NOT lawsuit?
(NYT data)play play NOT gameplay 1.000000 play 0.779183playing 0.773676 playing 0.658680plays 0.699858 role 0.594148played 0.684860 plays 0.581623game 0.626796 versatility 0.485053offensively 0.597609 played 0.479669defensively 0.546795 roles 0.470640preseason 0.544166 solos 0.448625midfield 0.540720 lalas 0.442326role 0.535318 onstage 0.438302tempo 0.504522 piano 0.438175score 0.475698 tyrone 0.437917Terms related to ?play NOT game?
(NYT data)Table 1: First experiments with negation and word-sensesTwo early results using negation to find senses ofambiguous words are given in Table 1, showing thatvector negation is very effective for removing the ?le-gal?
meaning from the word suit and the ?sporting?meaning from the word play, leaving respectively the?clothing?
and ?performance?
meanings.
Note that re-moving a particular word also removes concepts re-lated to the negated word.
This gives credence tothe claim that our mathematical model is removingthe meaning of a word, rather than just a string ofcharacters.
This encouraged us to set up a largerscale experiment to test this hypothesis, which is de-scribed in Section 4.3 Other forms of Negation in IRThere have been rigourous studies of Boolean op-erators for information retrieval, including the p-norms of Salton et al (1983) and the matrix forms ofTurtle and Croft (1989), which have focussed partic-ularly on mathematical expressions for conjunctionand disjunction.
However, typical forms of negation(such as NOT p = 1?p) have not taken into accountthe relationship between the negated argument andthe rest of the query.Negation has been used in two main forms in IRsystems: for the removal of unwanted documents af-ter retrieval and for negative relevance feedback.
Wedescribe these methods and compare them with vec-tor negation.3.1 Negation by filtering results afterretrievalA traditional Boolean search for documents relatedto the query a NOT b would return simply those doc-uments which contain the term a and do not containthe term b.
More formally, let D be the documentcollection and let Di ?
D be the subset of docu-ments containing the term i.
Then the results to theBoolean query for a NOT b would be the set Da?D?b,where D?b is the complement of Db in D. Variants ofthis are used within a vector model, by using vectorretrieval to retrieve a (ranked) set of relevant docu-ments and then ?throwing away?
documents contain-ing the unwanted terms (Salton and McGill, 1983, p.26).
This paper will refer to such methods under thegeneral heading of ?post-retrieval filtering?.There are at least three reasons for preferring vec-tor negation to post-retrieval filtering.
Firstly, post-retrieval filtering is not very principled and is subjectto error: for example, it would remove a long docu-ment containing only one instance of the unwantedterm.One might argue here that if a document contain-ing unwanted terms is given a ?negative-score?
ratherthan just disqualified, this problem is avoided.
Thiswould leaves us considering a combined score,sim(d, a NOT b) = d ?
a ?
?d ?
bfor some parameter ?.
However, since this is thesame as d ?
(a ?
?b), it is computationally more ef-ficient to treat a ?
?b as a single vector.
This isexactly what vector negation accomplishes, and alsodetermines a suitable value of ?
from a and b. Thusa second benefit for vector negation is that it pro-duces a combined vector for a NOT b which enablesthe relevance score of each document to be computedusing just one scalar product operation.The third gain is that vector retrieval proves to bebetter at removing not only an unwanted term butalso its synonyms and related words (see Section 4),which is clearly desirable if we wish to remove notonly a string of characters but the meaning repre-sented by this string.3.2 Negative relevance feedbackRelevance feedback has been shown to improve re-trieval (Salton and Buckley, 1990).
In this process,documents judged to be relevant have (some multipleof) their document vector added to the query: docu-ments judged to be non-relevant have (some multipleof) their document vector subtracted from the query,producing a new query according to the formulaQi+1 = ?Qi + ??relDi|Di|?
?
?nonrelDi|Di|,where Qi is the ith query vector, Di is the set of doc-uments returned by Qi which has been partitionedinto relevant and non-relevant subsets, and ?, ?, ?
?R are constants.
Salton and Buckley (1990) reportbest results using ?
= 0.75 and ?
= 0.25.The positive feedback part of this process hasbecome standard in many search engines with op-tions such as ?More documents like this?
or ?Similarpages?.
The subtraction option (called ?negative rel-evance feedback?)
is much rarer.
A widely held opin-ion is that that negative feedback is liable to harmretrieval, because it may move the query away fromrelevant as well as non-relevant documents (Kowal-ski, 1997, p. 160).The concepts behind negative relevance feedbackare discussed instructively by Dunlop (1997).
Neg-ative relevance feedback introduces the idea of sub-tracting an unwanted vector from a query, but givesno general method for deciding ?how much to sub-tract?.
We shall refer to such methods as ?ConstantSubtraction?.
Dunlop (1997, p. 139) gives an anal-ysis which leads to a very intuitive reason for pre-ferring vector negation over constant subtraction.
Ifa user removes an unwanted term which the modeldeems to be closely related to the desired term, thisshould have a strong effect, because there is a sig-nificant ?difference of opinion?
between the user andthe model.
(From an even more informal point ofview, why would anyone take the trouble to removea meaning that isn?t there anyway?).
With any kindof constant subtraction, however, the removal of dis-tant points has a greater effect on the final query-statement than the removal of nearby points.Vector negation corrects this intuitive mismatch.Recall from Equation 1 that (using normalised vec-tors for simplicity) the vector a NOT b is given bya ?
(a ?
b)b.
The similarity of a with a NOT b isthereforea ?
(a ?
(a ?
b)b) = 1?
(a ?
b)2.The closer a and b are, the greater the (a ?
b)2 factorbecomes, so the similarity of a with a NOT b be-comes smaller the closer a is to b.
This coincides ex-actly with Dunlop?s intuitive view: removing a con-cept which in the model is very close to the originalquery has a large effect on the outcome.
Negativerelevance feedback introduces the idea of subtract-ing an unwanted vector from a query, but gives nogeneral method for deciding ?how much to subtract?.We shall refer to such methods as ?Constant Subtrac-tion?.4 Evaluation and ResultsThis section describes experiments which comparethe three methods of negation described above (post-retrieval filtering, constant subtraction and vectornegation) with the baseline alternative of no nega-tion at all.
The experiments were carried out usingthe vector space model described in Section 2.1.To judge the effectiveness of different methods atremoving unwanted meanings, with a large numberof queries, we made the following assumptions.
Adocument which is relevant to the meaning of ?terma NOT term b?
should contain as many references toterm a and as few references to term b as possible.Close neighbours and synonyms of term b are unde-sirable as well, since if they occur the document inquestion is likely to be related to the negated termeven if the negated term itself does not appear.4.1 Queries and results for negating singleand multiple terms1200 queries of the form ?term a NOT term b?
weregenerated for 3 different document collections.
Theterms chosen were the 100 most frequently occurring(non-stop) words in the collection, 100 mid-frequencywords (the 1001st to 1100th most frequent), and 100low-frequency words (the 5001st to 5100th most fre-quent).
The nearest neighbour (word with highestcosine similarity) to each positive term was takento be the negated term.
(This assumes that a useris most likely to want to remove a meaning closelyrelated to the positive term: there is no point in re-moving unrelated information which would not beretrieved anyway.)
In addition, for the 100 most fre-quent words, an extra retrieval task was performedwith the roles of the positive term and the negatedterm reversed, so that in this case the system was be-ing asked to remove the very most common words inthe collection from a query generated by their near-est neighbour.
We anticipated that this would bean especially difficult task, and a particularly real-istic one, simulating a user who is swamped withinformation about a ?popular topic?
in which theyare not interested.1 The document collections usedwere from the British National Corpus (published byOxford University, the textual data consisting of ca90M words, 85K documents), the New York TimesNews Syndicate (1994-96, from the North AmericanNews Text Corpus published by the Linguistic DataConsortium, ca 143M words, 370K documents) andthe Ohsumed corpus of medical documents (Hersh etal., 1994) (ca 40M words, 230K documents).The 20 documents most relevant to each querywere obtained using each of the following four tech-niques.?
No negation.
The query was just the positiveterm and the negated term was ignored.?
Post-retrieval filtering.
After vector retrieval us-ing only the positive term as the query term,documents containing the negated term wereeliminated.?
Constant subtraction.
Experiments were per-formed with a variety of subtraction constants.The query a NOT b was thus given the vectora?
?b for some ?
?
[0, 1].
The results recorded inthis paper were obtained using ?
= 0.75, whichgives a direct comparison with vector negation.?
Vector negation, as described in this paper.For each set of retrieved documents, the followingresults were counted.?
The relative frequency of the positive term.?
The relative frequency of the negated term.?
The relative frequency of the ten nearest neigh-bours of the negative term.
One slight subtletyhere is that the positive term was itself a close1For reasons of space we do not show the retrieval per-formance on query terms of different frequencies in thispaper, though more detailed results are available fromthe author on request.neighbour of the negated term: to avoid incon-sistency, we took as ?negative neighbours?
onlythose which were closer to the negated term thanto the positive term.?
The relative frequency of the synonyms of thenegated term, as given by the WordNet database(Fellbaum, 1998).
As above, words which werealso synonyms of the positive term were dis-counted.
On the whole fewer such synonymswere found in the Ohsumed and NYT docu-ments, which have many medical terms andproper names which are not in WordNet.Additional experiments were carried out to com-pare the effectiveness of different forms of negationat removing several unwanted terms.
The same 1200queries were used as above, and the next nearestneighbour was added as a further negative argument.For two negated terms, the post-retrieval filteringprocess worked by discarding documents containingeither of the negative terms.
Constant subtractionworked by subtracting a constant multiple of eachof the negated terms from the query.
Vector nega-tion worked by making the query vector orthogonalto the plane generated by the two negated terms, asin Equation 2.Results were collected in much the same way as theresults for single-argument negation.
Occurrencesof each of the negated terms were added together,as were occurrences of the neighbours and WordNetsynonyms of either of the negated words.The results of our experiments are collected inTable 2 and summarised in Figure 1.
The resultsfor a single negated term demonstrate the followingpoints.?
All forms of negation proved extremely goodat removing the unwanted words.
This is triv-ially true for post-retrieval filtering, which worksby discarding any documents that contain thenegated term.
It is more interesting that con-stant subtraction and vector negation performedso well, cutting occurrences of the negated wordby 82% and 85% respectively compared with thebaseline of no negation.?
On average, using no negation at all retrievedthe most positive terms, though not in everycase.
While this upholds the claim that any formof negation is likely to remove relevant as wellas irrelevant results, the damage done was onlyaround 3% for post-retrieval filtering and 25%for constant and vector negation.?
These observations alone would suggest thatpost-retrieval filtering is the best method forthe simple goal of maximising occurrences ofthe positive term while minimising the occur-rences of the negated term.
However, vec-tor negation and constant subtraction dramati-cally outperformed post-retrieval filtering at re-moving neighbours of the negated terms, andwere reliably better at removing WordNet syn-onyms as well.
We believe this to be goodevidence that, while post-search filtering is bydefinition better at removing unwanted strings,the vector methods (either orthogonal or con-stant subtraction) are much better at removingunwanted meanings.
Preliminary observationssuggest that in the cases where vector negationretrieves fewer occurrences of the positive termthan other methods, the other methods are of-ten retrieving documents that are still related inmeaning to the negated term.?
Constant subtraction can give similar results tovector negation on these queries (though thevector negation results are slightly better).
Thisis with queries where the negated term is theclosest neighbour of the positive term, and theassumption that the similarity between thesepairs is around 0.75 is a reasonable approxima-tion.
However, further experiments with a va-riety of negated arguments chosen at randomfrom a list of neighbours demonstrated that inthis more general setting, the flexibility providedby vector negation produced conclusively betterresults than constant subtraction for any singlefixed constant.In addition, the results for removing multiplenegated terms demonstrate the following points.?
Removing another negated term further reducesthe retrieval of the positive term for all forms ofnegation.
Constant subtraction is the worst af-fected, performing noticeably worse than vectornegation.?
All three forms of negation still remove manyoccurrences of the negated term.
Vector nega-tion and (trivially) post-search filtering performas well as they do with a single negated term.However, constant subtraction performs muchworse, retrieving more than twice as many un-wanted terms as vector negation.?
Post-retrieval filtering was even less effective atremoving neighbours of the negated term thanwith a single negated term.
Constant subtrac-tion also performed much less well.
Vector nega-tion was by far the best method for remov-ing negative neighbours.
The same observation1 negated term 2 negated termsBNC NYT Ohsumed BNC NYT OhsumedNo Negation Positive term 0.53 1.18 2.57 0.53 1.18 2.57Negated term 0.37 0.66 1.26 0.45 0.82 1.51Negative neighbours 0.49 0.74 0.45 0.69 1.10 0.71Negative synonyms 0.24 0.22 0.10 0.42 0.42 0.20Post-retrieval Positive term 0.61 1.03 2.51 0.58 0.91 2.35filtering Negated term 0 0 0 0 0 0Negative neighbours 0.31 0.46 0.39 0.55 0.80 0.67Negative synonyms 0.19 0.22 0.10 0.37 0.39 0.37Constant Positive term 0.52 0.82 1.88 0.42 0.70 1.38Subtraction Negated term 0.09 0.13 0.20 0.18 0.21 0.35Negative neighbours 0.08 0.11 0.14 0.30 0.33 0.18Negative synonyms 0.19 0.16 0.07 0.33 0.29 0.12Vector Positive term 0.50 0.83 1.85 0.45 0.69 1.51Negation Negated term 0.08 0.12 0.16 0.08 0.11 0.15Negative neighbours 0.10 0.10 0.10 0.17 0.16 0.16Negative synonyms 0.18 0.16 0.07 0.31 0.27 0.12Table 2: Table of results showing the percentage frequency of different terms in retrieved documentsAverage results across corpora for one negated term01No negation Post-retrieval filtering Constant Subtraction Vector negation% frequencyAverage results across corpora for two negated terms01No negation Post-retrieval filtering Constant Subtraction Vector negation% frequencyPositive Term Negated TermVector Neighbours of Negated Word WordNet Synonyms of Negated WordFigure 1: Barcharts summarising results of Table 2holds for WordNet synonyms, though the resultsare less pronounced.This shows that vector negation is capable of re-moving unwanted terms and their related words fromretrieval results, while retaining more occurrences ofthe original query term than constant subtraction.Vector negation does much better than other meth-ods at removing neighbours and synonyms, and wetherefore expect that it is better at removing doc-uments referring to unwanted meanings of ambigu-ous words.
Experiments with sense-tagged data areplanned to test this hypothesis.The goal of these experiments was to evaluate theextent to which the different methods could removeunwanted meanings, which we measured by count-ing the frequency of unwanted terms and conceptsin retrieved documents.
This leaves the problems ofdetermining the optimal scope for the negation quan-tifier for an IR system, and of developing a naturaluser interface for this process for complex queries.These important challenges are beyond the scope ofthis paper, but would need to be addressed to in-corporate vector negation into a state-of-the-art IRsystem.5 ConclusionsTraditional branches of science have exploited thestructure inherent in vector spaces and developedrigourous techniques which could contribute to nat-ural language processing.
As an example of this po-tential fertility, we have adapted the negation anddisjunction connectives used in quantum logic to thetasks of word-sense discrimination and informationretrieval.Experiments focussing on the use of vector nega-tion to remove individual and multiple terms fromqueries have shown that this is a powerful and ef-ficient tool for removing both unwanted terms andtheir related meanings from retrieved documents.Because it associates a unique vector to each querystatement involving negation, the similarity betweeneach document and the query can be calculated usingjust one scalar product computation, a considerablegain in efficiency over methods which involve someform of post-retrieval filtering.We hope that these preliminary aspects will beinitial gains in developing a concrete and effectivesystem for learning, representing and composing as-pects of lexical meaning.DemonstrationAn interactive demonstration of negation for wordsimilarity and document retrieval is publicly avail-able at http://infomap.stanford.edu/webdemo.ReferencesRicardo Baeza-Yates and Berthier Ribiero-Neto.1999.
Modern Information Retrieval.
AddisonWesley / ACM Press.Garrett Birkhoff and John von Neumann.
1936.
Thelogic of quantum mechanics.
Annals of Mathemat-ics, 37:823?843.Mark Dunlop.
1997.
The effect of accessing non-matching documents on relevance feedback.
ACMTransactions on Information Systems, 15(2):137?153, April.Christiane Fellbaum, editor.
1998.
WordNet: AnElectronic Lexical Database.
MIT Press, Cam-bridge MA.William Hersh, Chris Buckley, T. J. Leone, andDavid Hickam.
1994.
Ohsumed: An interactiveretrieval evaluation and new large test collectionfor research.
In Proceedings of the 17th AnnualACM SIGIR Conference, pages 192?201.Gerald Kowalski.
1997.
Information retrieval sys-tems: theory and implementation.
Kluwer aca-demic publishers, Norwell, MA.Thomas Landauer and Susan Dumais.
1997.
A solu-tion to plato?s problem: The latent semantic anal-ysis theory of acquisition.
Psychological Review,104(2):211?240.Gerard Salton and Chris Buckley.
1990.
Improv-ing retrieval performance by relevance feedback.Journal of the American society for informationscience, 41(4):288?297.Gerard Salton and Michael McGill.
1983.
Introduc-tion to modern information retrieval.
McGraw-Hill, New York, NY.Gerard Salton, Edward A.
Fox, and Harry Wu.
1983.Extended boolean information retrieval.
Commu-nications of the ACM, 26(11):1022?1036, Novem-ber.Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?124.Howard Turtle and W. Bruce Croft.
1989.
Inferencenetworks for document retrieval.
In Proceedings ofthe 13th Annual ACM SIGIR Conference, pages1?24.Dominic Widdows and Stanley Peters.
2003.
Wordvectors and quantum logic.
In Mathematics ofLanguage 8, Bloomington, Indiana.Dominic Widdows.
2003.
Unsupervised methods fordeveloping taxonomies by combining syntactic andstatistical information.
HLT-NAACL, Edmonton,Canada.
