Hierarchical Phrase-Based Translation withWeighted Finite-State Transducers andShallow-n GrammarsAdria` de Gispert?University of CambridgeGonzalo Iglesias?
?University of VigoGraeme Blackwood?University of CambridgeEduardo R.
Banga?
?University of VigoWilliam Byrne?University of CambridgeIn this article we describe HiFST, a lattice-based decoder for hierarchical phrase-based translationand alignment.
The decoder is implemented with standard Weighted Finite-State Transducer(WFST) operations as an alternative to the well-known cube pruning procedure.
We find thatthe use of WFSTs rather than k-best lists requires less pruning in translation search, resulting infewer search errors, better parameter optimization, and improved translation performance.
Thedirect generation of translation lattices in the target language can improve subsequent rescoringprocedures, yielding further gains when applying long-span language models and MinimumBayes Risk decoding.
We also provide insights as to how to control the size of the search spacedefined by hierarchical rules.
We show that shallow-n grammars, low-level rule catenation,and other search constraints can help to match the power of the translation system to specificlanguage pairs.1.
IntroductionHierarchical phrase-based translation (Chiang 2005) is one of the current promisingapproaches to statistical machine translation (SMT).
Hiero SMT systems are basedon probabilistic synchronous context-free grammars (SCFGs) whose translation rules?
University of Cambridge, Department of Engineering.
CB2 1PZ Cambridge, U.K.E-mail: {ad465,gwb24,wjb31}@eng.cam.ac.uk.??
University of Vigo, Department of Signal Processing and Communications, 36310 Vigo, Spain.E-mail: {giglesia,erbanga}@gts.tsc.uvigo.es.Submission received: 30 October 2009; revised submission received: 24 February 2010; accepted forpublication: 10 April 2010.?
2010 Association for Computational LinguisticsComputational Linguistics Volume 36, Number 3can be extracted automatically from word-aligned parallel text.
These grammars canproduce a very rich space of candidate translations and, relative to simpler phrase-based systems (Koehn, Och, and Marcu 2003), the power of Hiero is most evident intranslation between dissimilar languages, such as English and Chinese (Chiang 2005,2007).
Hiero is able to learn and apply complex patterns in movement and translationthat are not possible with simpler systems.
Hiero can also be used to good effect on?simpler?
problems, such as translation between English and Spanish (Iglesias et al2009c), even though there is not the same need for the full complexity of movement andtranslation.
If gains in using Hiero are small, however, the computational and modelingcomplexity involved are difficult to justify.
Such concerns would vanish if there werereliable methods to match Hiero complexity for specific translation problems.
Looselyput, it would be a good thing if the complexity of a system was somehow proportionalto the improvement in translation quality the system delivers.Another notable current trend in SMT is system combination.
Minimum BayesRisk decoding is widely used to rescore and improve hypotheses produced by indi-vidual systems (Kumar and Byrne 2004; Tromble et al 2008; de Gispert et al 2009),and more aggressive system combination techniques which synthesize entirely newhypotheses from those of contributing systems can give even greater translation im-provements (Rosti et al 2007; Sim et al 2007).
It is now commonplace to note that eventhe best available individual SMT system can be significantly improved upon by suchtechniques.
This puts a burden on the underlying SMT systems which is somewhatunusual in NLP.
The requirement is not merely to produce a single hypothesis thatis as good as possible.
Ideally, the SMT systems should generate large collections ofcandidate hypotheses that are simultaneously diverse and of good quality.Relative to these concerns, previously published descriptions of Hiero have notedcertain limitations.
Spurious ambiguity (Chiang 2005) was described asa situation where the decoder produces many derivations that are distinct yet have thesame model feature vectors and give the same translation.
This can result in n-best listswith very few different translations which is problematic for the minimum-error-ratetraining algorithm ...This is due in part to the cube pruning procedure (Chiang 2007), which enumerates alldistinct hypotheses to a fixed depth by means of k-best hypothesis lists.
If enumerationwas not necessary, or if the lists could be arbitrarily deep, there might still be manyduplicate derivations, but at least the hypothesis space would not be impoverished.Spurious ambiguity is also related to overgeneration (Varile and Lau 1988; Wu1997; Setiawan et al 2009).
For our purposes we say that overgeneration occurs whendifferent derivations based on the same set of rules give rise to different translations.An example is given in Figure 1.This process is not necessarily a bad thing in that it allows new translations to besynthesized from rules extracted from training data; a strong target language model,such as a high order n-gram, is typically relied upon to discard unsuitable hypotheses.Overgeneration does complicate translation, however, in that many hypotheses areintroduced only to be subsequently discarded.
The situation is further complicated bysearch errors.
Any search procedure which relies on pruning during search is at risk ofsearch errors and the risk is made worse if the grammars tend to introduce many similarscoring hypotheses.
In particular we have found that cube pruning is very prone tosearch errors, that is, the hypotheses produced by cube pruning are not the top scoringhypotheses which should be found under the Hiero grammar (Iglesias et al 2009b).506de Gispert et al Hierarchical Translation with WFSTs and Shallow-n GrammarsFigure 1Example of multiple translation sequences from a simple grammar fragment showing variabilityin reordering in translation of the source sequence abc.These limitations are clearly related to each other.
Moreover, they become moreproblematic as the amount of parallel text grows.
As the number of rules in the grammarincreases, the grammars become more expressive, but the ability to search them does notimprove.
This leads to a widening gap between the expressive power of the grammarand the ability to search it to find good and diverse hypotheses.In this article we describe the following two refinements to Hiero which are in-tended to address some of the limitations in its original formulation.Lattice-based hierarchical translation We describe how the cube pruning procedurecan be replaced by standard operations with Weighted Finite State Transducers(WFSTs) so that Hiero uses translation lattices rather than n-best lists in search.We find that keeping partial translation hypotheses in lattice form greatly reducessearch errors.
In some instances it is possible to perform translation withoutany pruning at all so that search errors are completely eliminated.
Consistentwith the observation by Chiang (2005), this leads to improvements in minimumerror rate training.
Furthermore, the direct generation of translation lattices canimprove gains from subsequent language model and Minimum Bayes Risk (MBR)rescoring.Shallow-n grammars and additional nonterminal categories Nonterminals can be in-corporated into hierarchical translation rules for the purpose of tuning the sizeof the Hiero search space for individual language pairs.
Shallow-n grammars aredescribed and shown to control the level of rule nesting, low-level rule catenation,and the minimum and maximum spans of individual translation rules.
In trans-lation experiments we find that a shallow-1 grammar (one level of rule nesting)is sufficiently expressive for Arabic-to-English translation, but that a shallow-3grammar is required in Chinese-to-English translation to match the performanceof a full Hiero system that allows arbitrary rule nesting.
These nonterminals areintroduced to control the Hiero search space and do not require estimation fromannotated?or parsed?parallel text, as can be required by translation systemsbased on linguistically motivated grammars.
We use this approach as the basisof a general approach to SMT modeling.
To control overgeneration, we revisitthe synchronous context-free grammar defined by hierarchical rules and take ashallow-1 grammar as a starting point.
We then increase the complexity of therules until the desired translation quality is found.507Computational Linguistics Volume 36, Number 3With these refinements we find that hierarchical phrase-based translation can be effi-ciently carried out with no (or minimal) search errors in large-data tasks and can achievestate-of-the-art translation performance.There are many benefits to formulating Hiero translation in terms of WFSTs.
Fol-lowing the manner in which Knight and Al-Onaizan (1998), Kumar, Deng, and Byrne(2006), and Graehl, Knight, and May (2008) elucidate other machine translation models,we can use WFST operations to make the operations of the Hiero decoder very clear.
Thesimplicity of the analysis makes it possible to focus on the underlying grammars andavoid the complexities of heuristic search procedures.
Once the decoder is formulated,implementation is mostly straightforward using standard WFST techniques developedfor language processing (Mohri, Pereira, and Riley 2002).
What difficulties arise are dueto using finite state techniques with grammars which are not themselves finite state.We will show, however, that the basic operations which need to be performed, such asextracting sufficient statistics for minimum error rate training, can be done relativelyeasily and naturally.1.1 OverviewIn Section 2 we describe HiFST, which is a hierarchical phrase-based translation systembased on the OpenFST WFST libraries (Allauzen et al 2007).
We describe how trans-lation lattices can be generated over the Cocke-Younger-Kasami (CYK) grid used forparsing under Hiero.
We also review some modeling issues needed for practical trans-lation, such as the efficient handling of source language deletions and the extraction ofstatistics for minimum error rate training.
This requires running HiFST in ?alignmentmode?
(Section 2.3) to find all the rule derivations that generate a given set of translationhypotheses.In Section 3 we investigate parameters that control the size and nature of thehierarchical phrase-based search space as defined by hierarchical translation rules.
Toefficiently explore the largest possible space and avoid pruning in search, we introduceways to easily adapt the grammar to the reordering needs of each language pair.
Wedescribe the use of additional nonterminal categories to limit the degree of rule nesting,and can directly control the minimum or maximum span each translation rule can cover.In Section 4 we report detailed translation results for Arabic-to-English andChinese-to-English, and review translation results for Spanish-to-English and Finnish-to-English translation.
In these experiments we contrast the performance of lattice-basedand cube pruning hierarchical decoding and we measure the impact on processingtime and translation performance due to changes in search parameters and grammarconfigurations.
We demonstrate that it is easy and feasible to compute the marginalinstead of the Viterbi probabilities when using WFSTs, and that this yields gains intranslation performance.
And finally, we show that lattice-based translation performssignificantly better than k-best lists for the task of combining translation hypothesesgenerated from alternative morphological segmentations of the data via lattice-basedMBR decoding.2.
Hierarchical Translation and Alignment with WFSTsHierarchical phrase-based rules define a synchronous context-free grammar (CFG) anda particular search space of translation candidates.
Table 1 shows the type of rules in-cluded in a standard hierarchical phrase-based grammar, where T denotes the terminals(words) and ?
is a bijective function that relates the source and target nonterminals of508de Gispert et al Hierarchical Translation with WFSTs and Shallow-n GrammarsTable 1Rules contained in the standard hierarchical grammar.standard hierarchical grammarS??X,X?
glue rule 1S?
?S X,S X?
glue rule 2X???,?,??
, ?,?
?
{X ?
T}+ hiero ruleseach rule (Chiang 2007).
This function is defined if there are at least two nonterminals,and for clarity of presentation may be omitted in general rule discussions.
When ?,?
?
{T}+, that is, the rule contains no nonterminals, the rule is a simple lexical phrase pair.The HiFST translation system is based on a variant of the CYK algorithm closelyrelated to CYK+ (Chappelier and Rajman 1998).
Parsing follows the description ofChiang (2005, 2007); it maintains back-pointers and employs hypothesis recombinationwithout pruning.
The underlying model is a probabilisitic synchronous CFG consistingof a set R = {Rr} of rules Rr : Nr ?
??r,?r?
/ pr, with ?glue?
rules, S ?
?X,X?
andS ?
?S X,S X?.
N denotes the set of nonterminal categories (examples are given inSection 3), and pr denotes the rule probability, typically transformed to a cost cr; unlessotherwise noted we use the tropical semiring, so cr = ?
log pr.
T denotes the terminals(words), and the grammar builds parses based on strings ?,?
?
{N ?
T}+.
Each cellin the CYK grid is specified by a nonterminal symbol and position in the CYK grid:(N, x, y), which spans sx+y?1x on the source sentence.In effect, the source language sentence is parsed using a CFG with rules N ?
?.
Thegeneration of translations is a second step that follows parsing.
For this second step, wedescribe a method to construct word lattices with all possible translations that can beproduced by the hierarchical rules.
Construction proceeds by traversing the CYK gridalong the back-pointers established in parsing.
In each cell (N, x, y) in the CYK grid, webuild a target language word lattice L(N, x, y).
This lattice contains every translation ofsx+y?1x from every derivation headed by N. These lattices also contain the translationscores on their arc weights.The ultimate objective is the word lattice L(S, 1, J) which corresponds to all theanalyses that cover the source sentence sJ1.
Once this is built, we can apply a target lan-guage model to L(S, 1, J) to obtain the final target language translation lattice (Allauzen,Mohri, and Roark 2003).2.1 Lattice Construction over the CYK GridIn each cell (N, x, y), the set of rule indices used by the parser is denoted R(N, x, y), thatis, for r ?
R(N, x, y), the rule N ?
??r,?r?
was used in at least one derivation involvingthat cell.For each rule Rr, r ?
R(N, x, y), we build a lattice L(N, x, y, r).
This lattice is derivedfrom the target side of the rule ?r by concatenating lattices corresponding to the ele-ments of ?r = ?r1...?r|?r|.
If an ?ri is a terminal, creating its lattice is straightforward.
If?ri is a nonterminal, it refers to a cell (N?, x?, y?)
lower in the grid identified by the back-pointer BP(N, x, y, r, i); in this case, the lattice used is L(N?, x?, y?).
Taken together,L(N, x, y, r) =?i=1..|?r|L(N, x, y, r, i) (1)509Computational Linguistics Volume 36, Number 3Figure 2Production of target lattice L(S, 1, 3) using translation rules within CYK grid for sentence s1s2s3.The grid is represented here in two dimensions (x, y).
In practice only the first column acceptsboth nonterminals (S,X).
For this reason it is divided into two subcolumns.L(N, x, y, r, i) ={A(?i) if ?i ?
TL(N?, x?, y?)
otherwise (2)where A(t), t ?
T returns a single-arc acceptor which accepts only the symbol t. Thelattice L(N, x, y) is then built as the union of lattices corresponding to the rules inR(N, x, y):L(N, x, y) =?r?R(N,x,y)L(N, x, y, r) (3)Lattice union and concatenation are performed using the?
and?WFST operations,respectively, as described by Allauzen et al (2007).
If a rule Rr has a cost cr, it is appliedto the exit state of the lattice L(N, x, y, r) prior to the operation of Equation (3).2.1.1 An Example of Phrase-based Translation.
Figure 2 illustrates this process for a three-word source sentence s1s2s3 under monotonic phrase-based translation.
The left-handside shows the state of the CYK grid after parsing using the rules R1 to R5.
These includethree standard phrases, that is, rules with only terminals (R1, R2, R3), and the glue rules(R4, R5).
Arrows represent back-pointers to lower-level cells.
We are interested in theuppermost S cell (S, 1, 3), as it represents the search space of translation hypothesescovering the whole source sentence.
Two rules (R4, R5) are in this cell, so the latticeL(S, 1, 3) will be obtained by the union of the two lattices found by the back-pointers ofthese two rules.
This process is explicitly derived in the right-hand side of Figure 2.2.1.2 An Example of Hierarchical Translation.
Figure 3 shows a hierarchical scenario forthe same sentence.
Three rules, R6,R7,R8, are added to the example of Figure 2, thusproviding two additional derivations.
This makes use of sublattices already producedin the creation of L(S, 1, 3, 5) and L(X, 1, 3, 1) in Figure 2; these are shown within {}.510de Gispert et al Hierarchical Translation with WFSTs and Shallow-n GrammarsFigure 3Translation as in Figure 2 but with additional rules R6,R7,R8.
Lattices previously derived appearwithin {}.2.2 A Procedure for Lattice ConstructionFigure 4 presents an algorithm to build the lattice for every cell.
The algorithm usesmemoization: If a lattice for a requested cell already exists, it is returned (line 2);otherwise it is constructed via Equations (1)?(3).
For every rule, each element of thetarget side (lines 3,4) is checked as terminal or nonterminal (Equation (2)).
If it is aterminal element (line 5), a simple acceptor is built.
If it is a nonterminal (line 6), thelattice associated to its back-pointer is returned (lines 7 and 8).
The complete latticeL(N, x, y, r) for each rule is built by Equation (1) (line 9).
The lattice L(N, x, y) for thiscell is then found by union of all the component rules (line 10, Equation (3)); this latticeis then reduced by standard WFST operations (lines 11, 12, 13).
It is important at thispoint to remove any epsilon arcs which may have been introduced by the various WFSTunion, concatenation, and replacement operations (Allauzen et al 2007).We now address several important aspects of efficient implementation.Figure 4Recursive lattice construction from a CYK grid.511Computational Linguistics Volume 36, Number 3Figure 5Delayed translation WFST with derivations from Figures 2 and 3 before (left) and afterminimization (right).2.2.1 Delayed Translation.
Equation (2) leads to the recursive construction of lattices inupper levels of the grid through the union and concatenation of lattices from lowerlevels.
If Equations (1) and (3) are actually carried out over fully expanded word lattices,the memory required by the upper lattices will increase exponentially.To avoid this, we use special arcs that serve as pointers to the low-level lattices.
Thiseffectively builds a skeleton for the desired lattice and delays the creation of the finalword lattice until a single replacement operation is carried out in the top cell (S, 1, J).To make this exact, we define a function g(N, x, y) which returns a unique tag for eachlattice in each cell, and use it to redefine Equation (2).
With the back-pointer (N?, x?, y?)
=BP(N, x, y, r, i), these special arcs are introduced asL(N, x, y, r, i) ={A(?i) if ?i ?
TA(g(N?, x?, y?))
otherwise (4)The resulting lattices L(N, x, y) are a mix of target language words and latticepointers (Figure 5, left).
Each still represents the entire search space of all translationhypotheses covering the span, however.
Importantly, operations on these lattices?suchas lossless size reduction via determinization and minimization (Mohri, Pereira, andRiley 2002)?can still be performed.
Owing to the existence of multiple hierarchical ruleswhich share the same low-level dependencies, these operations can greatly reduce thesize of the skeleton lattice; Figure 5 shows the effect on the translation example.
Thisprocess is carried out for the lattice at every cell, even at the lowest level where thereare only sequences of word terminals.
As stated, size reductions can be significant.
Notall redundancy is removed, however, because duplicate paths may arise through theconcatenation and union of sublattices with different spans.At the upper-most cell, the lattice L(S, 1, J) contains pointers to lower-level lattices.A single FST replace operation (Allauzen et al 2007) recursively substitutes all pointersby their lower-level lattices until no pointers are left, thus producing the completetarget word lattice for the whole source sentence.
The use of the lattice pointer arc wasinspired by the ?lazy evaluation?
techniques developed by Mohri, Pereira, and Riley(2000), closely related to Recursive Transition Networks (Woods 1970; Mohri 1997).
Itsimplementation uses the infrastructure provided by the OpenFST libraries for delayedcomposition.2.2.2 Top-level Pruning and Search Pruning.
The final translation lattice L(S, 1, J) canbe quite large after the pointer arcs are expanded.
We therefore apply a word-based512de Gispert et al Hierarchical Translation with WFSTs and Shallow-n GrammarsFigure 6Transducers for filtering up to one (left) or two (right) consecutive deletions.language model via WFST composition (Allauzen et al 2007) and perform likelihood-based pruning based on the combined translation and language model scores.
We callthis top-level pruning because it is performed over the topmost lattice.Pruning can also be performed on the sublattices in each cell during search.
Onesimple strategy is to monitor the number of states in the determinized lattices L(N, x, y).If this number is above a threshold, we expand any pointer arcs and apply a word-basedlanguage model via composition.
The resulting lattice is then reduced by likelihood-based pruning, after which the LM scores are removed.
These pruning strategies can bevery selective, for example allowing the pruning threshold to depend on the height ofthe cell in the grid.
In this way the risk of search errors can be controlled.The same n-gram language model can be used for top-level pruning and searchpruning, although different WFST realizations are required.
For top-level pruning, astandard implementation as described by Allauzen et al (2007) is appropriate.
Forsearch pruning, the WFST must allow for incomplete language model histories, becausemany sublattice paths are incomplete translation hypotheses which do not begin witha sentence-start marker.
The language model acceptor is constructed so that initialsubstrings of length less than the language model order are assigned no weight underthe language model.2.2.3 Constrained Source Word Deletion.
As a practical matter it can be useful to allow SMTsystems to delete some source words rather than to enforce their translation.
Deletionscan be allowed in Hiero by including in the grammar a set of special deletion rulesof the type: X??s,NULL?.
Unconstrained application of these rules can lead to overlylarge and complex search spaces, however.
We therefore limit the number of consecutivesource word deletions as we explore each cell of the CYK grid.
This is done by standardcomposition with an unweighted transducer that maps any word to itself, and up to kNULL tokens to  arcs.
In Figure 6 this simple transducer for k = 1 and k = 2 is drawn.Composition of the lattice in each cell with this transducer filters out all translationswith more than k consecutive deleted words.2.3 Hierarchical Phrase-Based Alignment with WFSTsWe now describe a method to apply our decoder in alignment mode.
The objective inalignment is to recover all the derivations which can produce a given translation.
We dothis rather than keep track of the rules used during translation, because we find it fasterand more efficient first to generate translations and then, by running the system as analigner with a constrained target space, to extract all the relevant derivations with theircosts.
As will be discussed in Section 2.3.1, this is useful for minimum error training,where the contribution of each feature to the overall hypothesis cost is required forsystem optimization.513Computational Linguistics Volume 36, Number 3Figure 7Transducer encoding simultaneously rule derivations R2R1R3R4 and R1R5R6, and the translationt5t8.
The input sentence is s1s2s3 and the grammar considered here contains the following rules:R1: S?
?X,X?, R2: S?
?S X,S X?
, R3: X?
?s1,t5?, R4: X?
?s2 s3,t8?, R5: X?
?s1 X s3,X t8?
and R6:X?
?s2,t5?.Conceptually, we would like to create a transducer that represents the mappingfrom all possible rule derivations to all possible translations, and then compose thistransducer with an acceptor for the translations of interest.
Creating this transducerwhich maps derivations to translations is not feasible for large translation grammars,so we instead keep track of rules as they are used to generate a particular translationoutput.
We introduce two modifications into lattice construction over the CYK griddescribed in Section 2.2:1.
In each cell transducers are constructed which map rule sequences to thetarget language translation sequences they produce.
In each transducer theoutput strings are all possible translations of the source sentence spancovered by that cell; the input strings are all the rule derivations thatgenerate those translations.
The rule derivations are expressed assequences of rule indices r given the set of rules R = {Rr}.2.
As these transducers are built they are composed with acceptors forsubsequences of the reference translations so that any translations notpresent in the given set of reference translations are removed.
In effect, thisreplaces the general target language model used in translation with anunweighted acceptor which accepts only specific sentences.For alignment, Equations (1) and (2) are redefined asL(N, x, y, r) = AT(r,)?i=1..|?r|L(N, x, y, r, i) (5)L(N, x, y, r, i) ={AT(,?i) if ?i ?
TL(N?, x?, y?)
otherwise (6)where AT(r, t), Rr ?
R, t ?
T returns a single-arc transducer which accepts the symbolr in the input language (rule indices) and the symbol t in the output language (targetwords).
The weight assigned to each arc is the same in alignment as in translation.
Withthese definitions the goal lattice L(S, 1, J) is now a transducer with rule indices in theinput symbols and target words in the output symbols.
A simple example is givenin Figure 7 where two rule derivations for the translation t5t8 are represented by thetransducer.As we are only interested in those rule derivations that generate the given targetreferences, we can discard non-desired translations via standard FST composition of514de Gispert et al Hierarchical Translation with WFSTs and Shallow-n GrammarsFigure 8Construction of a substring acceptor.
An acceptor for the strings t1t2t4 and t3t4 (left) and itssubstring acceptor (right).
In alignment the substring acceptor can be used to filter out undesiredpartial translations via standard FST composition operations.the lattice transducer with the given reference acceptor.
In principle, this would be donein the uppermost cell of the CYK, once the complete source sentence has been covered.However, keeping track of all possible rule derivations and all possible translations untilthe last cell may not be computationally feasible for many sentences.
It is more desirableto carry out this filtering composition in lower-level cells while constructing the latticeover the CYK grid so as to avoid storing an increasing number of undesired translationsand derivations in the lattice.
The lattice in each cell should contain translations formedonly from substrings of the references.To achieve this we build an unweighted substring acceptor that accepts all sub-strings of each target reference string.
For instance, given the reference string t1t2 .
.
.tJ, we build an acceptor for all substrings ti .
.
.
tj, where 1 ?
i ?
j ?
J.
This is appliedto lattices in all cells (x, y) that do not span the whole sentence.
In the uppermostcell we compose with the reference acceptor which accepts only complete referencestrings.
Given a lattice of target references, the unweighted substring acceptor isbuilt as:1. change all non-initial states into final states2.
add one initial state and add  arcs from it to all other statesFigure 8 shows an example of a substring acceptor for the two references t1t2t4 andt3t4.
The substring acceptor also accepts an empty string, accounting for those rulesthat delete source words, which in other words translate into NULL.
In some instancesthe final composition with the reference acceptor might return an empty lattice.
If thishappens there is no rule sequence in the grammar that can generate the given sourceand target sentences simultaneously.We have described the use of transducers to encode mappings from rule deriva-tions to translations.
These transducers are somewhat impoverished relative to parsetrees and parse forests, which are more commonly used to encode this relationship.
Itis easy to map from a parse tree to one of these transducers but the reverse essentiallyrequires re-parsing to recreate the tree structure.
The structures of the parse trees asso-ciated with a translation are not needed by many algorithms, however.
In particular,parameter optimization by MERT requires only the rules involved in translation.
Ourapproach keeps only what is needed by such algorithms.
This approach also has prac-tical advantages such as the ability to align directly to k-best lists or lattices of referencetranslations.515Computational Linguistics Volume 36, Number 3Figure 9One arc from a rule acceptor that assigns a vector of K feature weights to each rule (top) and theresult of composition with the transducer of Figure 7 (after weight-pushing) (bottom).
Thecomponents of the final K-dimensional weight vector agree with the feature weights of the rulesequence, for example ck = c2,k + c1,k + c3,k + c4,k for k = 1 .
.
.K.2.3.1 Extracting Feature Values from Alignments.
As described by Chiang (2007), scores areassociated with hierarchical translation rules through a factoring into features within alog-linear model (Och and Ney 2002).
We assume that we have a collection of K featuresand that the cost cr of each rule Rr is cr =?Kk=1 ?kcr,k, where cr,k is the value of the kthfeature value for the rth rule and ?k is the weight assigned to the kth feature for all rules.For a parse which makes use of the rules Rr1 .
.
.RrN , its cost?Nn=1 crn can thereforebe written as?Kk=1 ?k?Nn=1 crn,k.
The quantity?Nn=1 crn,k is the contribution by the kthfeature to the overall translation score for that parse.
These are the quantities whichneed to be extracted from alignment lattices for use in procedures such as minimumerror rate training for estimation of the feature weights ?k.The procedure described in Section 2.3 produces alignment lattices with scoresconsistent with the total parse score.
Further steps must be taken to factor this over-all score to identify the contribution due to individual features or translation rules.We introduce a rule acceptor which accepts sequences of rule indices, such as theinput sequences of the alignment transducer, and assigns weights in the form ofK-dimensional vectors.
Each component of the weight vector corresponds to the featurevalue for that rule.
Arcs have the form 0Rr/wr??
0 where wr = [cr,1, .
.
.
, cr,K].
An exampleof composition with this rule acceptor is given in Figure 9 to illustrate how feature scoresare mapped to components of the weight vector.
The same operations can be applied tothe (unweighted) alignment transducer on a much larger scale to extract the statisticsneeded for minimum error rate training.We typically apply this procedure in the tropical semiring (Viterbi likelihoods), sothat only the best rule derivation that generated each translation candidate is takeninto account when extracting feature contributions for MERT.
However, given thealignment transducer L, this could also be performed in the log semiring (marginallikelihoods), taking into account the feature contributions from all rule derivations, foreach translation candidate.
This would be adequate if the translation system also car-ried out decoding in the log semiring, an experiment which is partially explored inSection 4.4.We note that the contribution of the language model to the overall translation scorecannot be calculated in this scheme, since the language model score cannot be factoredin terms of rules.
To obtain the language model contribution, we simply carry outWFST composition (Allauzen et al 2007) between an unweighted acceptor of the target516de Gispert et al Hierarchical Translation with WFSTs and Shallow-n GrammarsFigure 10Hierarchical translation grammar example and two parse trees with different levels of rulenesting for the input sentence s1s2s3s4.sentences and the n-gram language model used in translation.
After determinization,the cost of each path in the acceptor is then the desired LM feature contribution.3.
Shallow-n Translation Grammars: Translation Search Space RefinementsIn this section we describe shallow-n grammars in order to reduce Hiero overgenerationand adapt the grammar complexity to specific language pairs; the ultimate goal is to de-fine the most constrained grammar that is capable of generating the desired movementand translation, so that decoding can be performed without search errors.Hiero can provide varying degrees of complexity in movement and translation.Consider the example shown in Figure 10, which shows a hierarchical grammar definedby six rules.
For the input sentence s1s2s3s4, there are two possible parse trees as shown;the rule derivations for each tree are R1R4R3R5 and R2R1R3R5R6.
Along with each treeis shown the translation generated and the phrase-level alignment.
Comparing the twotrees and alignments, the leftmost tree makes use of more reordering when translatingfrom source to target through the nested application of the hierarchical rules R3 and R4.For some language pairs this level of reordering may be required in translation, but forother language pairs it may lead to overgeneration of unwanted hypotheses.
Supposethe grammar in this example is modified as follows:1.
A nonterminal X0 is introduced into hierarchical translation rulesR3:X?
?X0 s3,t5 X0?R4:X?
?X0 s4,t3 X0?2.
Rules for lexical phrases are applied to X0R5:X0?
?s1 s2,t1 t2?R6:X0?
?s4,t7?These modifications exclude parses in which hierarchical translation rules generateother hierarchical rules, except at the 0th level which generate lexical phrases.
Con-sequently the left most tree of Figure 10 cannot be generated and t5t1t2t7 is the onlyallowable translation of s1s2s3s4.
We call this a ?shallow-1?
grammar: The maximum517Computational Linguistics Volume 36, Number 3degree of rule nesting allowed is 1 and only the glue rule can be applied above thislevel.The use of additional nonterminal categories is an elegant way to easily controlimportant aspects that can have a strong impact on the search space.
A shallow-ntranslation grammar can be formally defined as:1. the usual nonterminal S2.
a set of nonterminals {X0, .
.
.
,XN}3. two glue rules: S ?
?XN,XN?
and S ?
?S XN,S XN?4.
hierarchical translation rules for levels n = 1, .
.
.
,N:R: Xn???,?,??
, ?,?
?
{{Xn?1} ?
T}+with the requirement that ?
and ?
contain at least one Xn?15.
translation rules which generate lexical phrases:R: X0???,??
, ?,?
?
T+3.1 Avoiding Some Spurious AmbiguityThe added requirement in condition (4) in the definition of shallow-n grammars isincluded to avoid some instances in which multiple parses lead to the same translation.It is not absolutely necessary but it can be added without any loss in representationalcapability.
To see the effect of this constraint, consider the following example with asource sentence s1 s2 and a shallow-1 grammar defined by these four rules:R1: S?
?X1,X1?R2: X1?
?s1 s2,t2 t1?R3: X1?
?s1 X0,X0 t1?R4: X0?
?s2,t2?There are two derivations R1R2 and R1R3R4 which yield identical translations.
HoweverR2 would not be allowed under the constraint introduced here because it does notrewrite an X1 to an X0.3.2 Structured Long-Distance MovementThe basic formulation of shallow-n grammars allows only the upper-level nonterminalcategory S to act within the glue rule.
This can prevent some useful long-distancemovement, as might be needed to translate Arabic sentences in Verb-Subject-Objectorder into English.
It often happens that the initial Arabic verb requires long-distancemovement, but the subject which follows can be translated in monotonic order.
Forinstance, consider the following Romanized Arabic sentence:TAlb AlwzrA?
AlmjtmEyn Alywm fy dm$q <lY ...(CALLED) (the ministers) (gathered) (today) (in Damascus) (FOR) ...where the verb ?TAlb?
must be translated into English so that it follows the translationsof the five subsequent Arabic words ?AlwzrA?
AlmjtmEyn Alywm fy dm$q?, which518de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammarsare themselves translated monotonically.
A shallow-1 grammar cannot generate thismovement except in the relatively unlikely case that the five words following the verbcan be translated as a single phrase.A more powerful approach is to define grammars which allow low-level rules toform movable groups of phrases.
Additional nonterminals {Mk} are introduced to allowsuccessive generation of k nonterminals XN?1 in monotonic order for both languages,where K1 ?
k ?
K2.
These act in the same manner as the glue rule does in the uppermostlevel.
Applying Mk nonterminals at the N?1 level allows one hierarchical rule to performa long-distance movement over the tree headed by Mk.We further refine the definition of shallow-n grammars by specifying the allowablevalues of k for the successive productions of nonterminals XN?1.
There are many pos-sible ways to formulate and constrain these grammars.
If K2 = 1, then the grammaris equivalent to the previous definition of shallow-n grammars, because monotonicproduction is only allowed by the glue rule of level N. If K1 = 1 and K2 > 1, then thesearch space defined by the grammar is greater than the standard shallow-n grammaras it includes structured long-distance movement.
Finally, if K1 > 1 then the searchspace is different from standard shallow-n as the n level is only used for long-distancemovement.Introduction of Mk nonterminals redefines shallow-n grammars as:1. the usual nonterminal S2.
a set of nonterminals {X0, .
.
.
,XN}3. a set of nonterminals {MK1 , .
.
.
,MK2} for K1 = 1, 2; K1 ?
K24.
two glue rules: S ?
?XN,XN?
and S ?
?S XN,S XN?5.
hierarchical translation rules for level N:R: XN???,?,??
, ?,?
?
{{MK1 , .
.
.
,MK2} ?
T}+with the requirement that ?
and ?
contain at least one Mk6.
hierarchical translation rules for levels n = 1, .
.
.
,N ?
1:R: Xn???,?,??
, ?,?
?
{{Xn?1} ?
T}+with the requirement that ?
and ?
contain at least one Xn?17.
translation rules which generate lexical phrases:R: X0???,??
, ?,?
?
T+8.
rules which generate k nonterminals XN?1:if K1 == 2 :R: Mk?
?XN?1 Mk?1,XN?1 Mk?1?
, for k = 3, .
.
.
,K2R: M2?
?XN?1 XN?1,XN?1 XN?1,I?if K1 == 1 :R: Mk?
?XN?1 Mk?1,XN?1 Mk?1?
, for k = 2, .
.
.
,K2R: M1?
?XN?1,XN?1?where I denotes the identity function that enforces monotonocity in the nonterminals.For example, with a shallow-1 grammar, M3 leads to the monotonic production of threenonterminals X0, which leads to the production of three lexical phrase pairs; these can bemoved with a hierarchical rule of level 1.
This is graphically represented by the leftmosttree in Figure 11.
With a shallow-2 grammar, M2 leads to the monotonic production of519Computational Linguistics Volume 36, Number 3Figure 11Movement allowed by two grammars: shallow-1, with K1 = 1, K2 = 3 (left), and shallow-2, withK1 = 1, K2 = 3 (right).
Both grammars allow movement of the bracketed term as a unit.Shallow-1 requires that translation within the object moved be monotonic whereas shallow-2allows up to two levels of reordering.two nonterminals X1, a movement represented by the rightmost tree in Figure 11.
Thismovement cannot be achieved with a shallow-1 grammar.3.3 Minimum and Maximum Rule SpanIt is useful to define two parameters which further control the application of hierarchicaltranslation rules in generating the search space.
Parameters hmax and hmin specifythe maximum and minimum height at which any hierarchical translation rule can beapplied in the CYK grid.
In other words, a hierarchical rule can only be applied in cell(x, y) if hmin?
y ?hmax.
Note that these parameters can also be set independently foreach nonterminal category.3.4 Verb Movement Grammars for Arabic-to-English TranslationFollowing the discussion which motivated this section, we wish to model movement ofArabic verbs when translating into English.
We add to the shallow-n grammars a verbrestriction so that the hierarchical translation rules (5) apply only if the source languagestring ?
contains a verb.
This encourages translations in which the Arabic verb is movedat the uppermost level N.3.5 Grammars Used for SMT ExperimentsWe now define the hierarchical grammars for the translation experiments which wedescribe next.Shallow-1,2,3 : Shallow-n grammars for n = 1, 2, 3.
These grammars do not incorporateany monotonicity constraints, that is K1 = K2 = 1.Shallow-1, K1 = 1, K2 = 3 : hierarchical rules with one nonterminal can reorder amonotonic production of up to three target language phrases of level 0.520de Gispert et al Hierarchical Translation with WFSTs and Shallow-n GrammarsShallow-1, K1 = 1, K2 = 3, vo : hierarchical rules with one nonterminal can reorder amonotonic catenation of up to three target language phrases of level 0, but only ifone of the source terminals is tagged as a verb.Shallow-2, K1 = 2,K2 = 3, vo : two levels of reordering with monotonic productionof up to three target language phrases of level 1, but only if one of the sourceterminals is tagged as a verb.4.
Translation ExperimentsIn this section we report on hierarchical phrase-based translation experiments withWFSTs.
We focus mainly on the NIST Arabic-to-English and Chinese-to-English trans-lation tasks; some results for other language pairs are summarized in Section 4.6.Translation performance is evaluated using the BLEU score (Papineni et al 2001) asimplemented for the NIST 2009 evaluation.1 The experiments are organized as follows:- Lattice-based and cube pruning hierarchical decoding (Section 4.2)- Grammar configurations and search parameters and their effect ontranslation performance and processing time (Section 4.3)- Marginalization over translation derivations (Section 4.4)- Combining translation lattices obtained from alternative morphologicaldecompositions of the input (Section 4.5)4.1 Experimental FrameworkFor Arabic-to-English translation we use all allowed parallel corpora in the NIST MT08(and MT09) Arabic Constrained Data track (?150M words per language).
In addition toreporting results on the MT08 set itself, we make use of a development set mt02-05-tuneformed from the odd numbered sentences of the NIST MT02 through MT05 evaluationsets; the even numbered sentences form a validation set mt02-05-test.
The mt02-05-tuneset has 2,075 sentences.For Chinese-to-English translation we use all available parallel text for the GALE2008 evaluation;2 this is approximately 250M words per language.
We report translationresults on the NIST MT08 set, a development set tune-nw, and a validation set test-nw.These tuning and test sets contain translations produced by the GALE program andportions of the newswire sections of MT02 through MT05.
The tune-nw set has 1,755sentences, and test-nw set is similar.The parallel texts for both language pairs are aligned using MTTK (Deng and Byrne2008).
We extract hierarchical rules from the aligned parallel texts using the constraintsdeveloped by Chiang (2007).
We further filter the extracted rules by count and patternas described by Iglesias et al(2009a).
The following features are extracted from theparallel data and used to assign scores to translation rules: source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of theglue rule, source-to-target and target-to-source lexical models, and three rule countfeatures inspired by Bender et al (2007).1 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl2 See http://projects.ldc.upenn.edu/gale/data/catalog.html521Computational Linguistics Volume 36, Number 3We use two types of language model in translation.
In first-pass translation we use4-gram language models estimated over the English side of the parallel text (for eachlanguage pair) and a 965 million word subset of monolingual data from the EnglishGigaword Third Edition (LDC2007T07).
These are the language models used if pruningis needed during search.
The main language model is a zero-cutoff stupid-backoff(Brants et al 2007) 5-gram language model, estimated using 6.6B words of English textfrom the English Gigaword corpus.
These language models are converted to WFSTsas needed (Allauzen, Mohri, and Roark 2003); failure transitions are used for correctapplication of back-off weights.
In tuning the systems, standard MERT (Och 2003)iterative parameter estimation under IBM BLEU is performed on the development sets.4.2 Contrast between HiFST and Cube PruningWe contrast two hierarchical phrase-based decoders.
The first decoder, HCP, is a k-bestdecoder using cube pruning following the description by Chiang (2007); in our im-plementation, these k-best lists contain only unique hypotheses (Iglesias et al 2009a),which are obtained by extracting the 10,000 best candidates from each cell (includingthe language model cost), using a priority queue to explore the cross-product of thek-best lists from the cells pointed by nonterminals.
We find that deeper k-best lists(i.e., k = 100, 000) results in impractical decoding times and that fixed k-best list depthsyields better performance than use of a likelihood threshold parameter.
The seconddecoder, HiFST, is a lattice-based decoder implemented with WFSTs as described earlier.Hypotheses are generated after determinization under the tropical semiring so thatscores assigned to hypotheses arise from a single minimum cost/maximum likelihoodderivation.Translation proceeds as follows.
After Hiero translation with optimized featureweights and the first-pass language model, hypotheses are written to disk.
For HCP wesave translations as 10,000-best lists, whereas HiFST generates word lattices.
The first-pass results are then rescored with the main 5-gram language model.
In this operationthe first-pass language model scores are removed before the main language modelscores are applied.
We then perform MBR rescoring.
For the n-best lists we rescorethe top 1,000 hypotheses using the negative sentence-level BLEU score as the lossfunction (Kumar and Byrne 2004); we have found that using a deeper k-best list isimpractically slow.
For the HiFST lattices we use lattice-based MBR search proceduresdescribed by Tromble et al (2008) in an implementation based on standard WFSToperations (Allauzen et al 2007).4.2.1 Shallow-1 Arabic-to-English Translation.
We translate Arabic-to-English withshallow-1 hierarchical decoding: as described in Section 3, nonterminals are allowedonly to generate target language phrases.
Table 2 shows results for mt02-05-tune, mt02-05-test, and mt08.
In this experiment we use MERT to find optimized parameters forHCP and we use these parameter values in HiFST as well.
This allows for a closecomparison of decoder behavior, independent of parameter optimization.In these experiments, the first-pass translation quality of the two systems (Table 2a vs. b) is nearly identical.
The most notable difference in the first-pass behavior of thedecoders is their memory use.
For example, for an input sentence of 105 words, HCPuses 1.2Gb memory whereas HiFST takes only 900Mb under the same conditions.
Torun HCP successfully requires cube pruning with the first-pass 4-gram language model.By contrast, HiFST requires no pruning during lattice construction and the first passlanguage model is not applied until the lattice is fully built at the upper most cell of the522de Gispert et al Hierarchical Translation with WFSTs and Shallow-n GrammarsTable 2Contrastive Arabic-to-English translation results (lower-cased IBM BLEU) after first-passdecoding and subsequent rescoring steps.
Decoding time reported for mt02-05-tune is in secondsper word.
Both systems are optimized using MERT over the k-best lists generated by HCP.decoder time mt02-05-tune mt02-05-test mt08a HCP 1.1 52.5 51.9 42.8+5g - 53.4 52.9 43.5+5g+MBR - 53.6 53.0 43.6b HiFST 0.5 52.5 51.9 42.8+5g - 53.6 53.2 43.9+5g+LMBR - 54.3 53.7 44.8CYK grid.
For this grammar, HiFST is able to produce exact translations without anysearch errors.Search Errors Because both decoders are constrained to use exactly the same features,we can compare their search errors on a sentence-by-sentence basis.
A search error isassigned to one of the decoders if the other has found a hypothesis with lower cost.
Formt02-05-tune, we find that in 18.5% of the sentences HiFST finds a hypothesis with lowercost than HCP.
In contrast, HCP never finds any hypothesis with lower cost for anysentence.
This is as expected: The HiFST decoder requires no pruning prior to applyingthe first-pass language model, so search is exact.Lattice/k-best Quality It is clear from the results that the lattices produced by HiFSTyield better rescoring results than the k-best lists produced by HCP.
This is the case forboth 5-gram language model rescoring and MBR.
In MT08 rescoring, HCP k-best listsyield an improvement of 0.8 BLEU relative to the first-pass baseline, whereas rescoringHiFST lattices yield an improvement of 2.0 BLEU.
The advantage of maintaining a largesearch space in lattice form during decoding is clear.
The use of k-best lists in HCP limitsthe gains from subsequent rescoring procedures.Translation Speed HCP requires an average of 1.1 seconds per input word.
HiFSTcuts this time by half, producing output at a rate of 0.5 seconds per word.
It provesmuch more efficient to process compact lattices containing many hypotheses rather thanindependently processing each distinct hypothesis in k-best form.4.2.2 Fully Hierarchical Chinese-to-English Translation.
We translate Chinese-to-Englishwith full hierarchical decoding: nonterminals are allowed to generate other hierarchicalrules in recursion.We apply the constraint hmax=10 for nonterminal category X, as described in Sec-tion 2.2.2, so that only glue rules are allowed at upper levels of the CYK grid; this isapplied in both HCP and HiFST.In HiFST any lattice in the CYK grid is pruned if it covers at least three source wordsand contains more than 10,000 states.
The log-likelihood pruning threshold relative tothe best path in the sublattices is 9.0.Improved Optimization and Generalization Table 3 shows results for tune-nw, test-nw,and mt08.
The first two rows show results for HCP when using MERT parametersoptimized over k-best lists produced by HCP (row a) and by HiFST (row b); in the lattercase, we are tuning HCP parameters over the hypothesis list generated by HiFST.
Whenmeasured over test-nw this gives a 0.3 BLEU improvement.
HCP benefits from tuning523Computational Linguistics Volume 36, Number 3Table 3Contrastive Chinese-to-English translation results (lower-cased IBM BLEU) after first-passdecoding and subsequent rescoring steps.
The MERT k-best column indicates which decodergenerated the k-best lists used in MERT optimization.
The mt08 set contains 691 sentences ofnewswire and 666 sentences of Web text.decoder MERT k-best tune-nw test-nw mt08a HCP HCP 32.8 33.1 ?b HCP 32.9 33.4 28.2+5g HiFST 33.4 33.8 28.7+5g+MBR 33.6 34.0 28.9c HiFST 33.1 33.4 28.1+5g HiFST 33.8 34.3 29.0+5g+LMBR 34.5 34.9 30.2over the HiFST hypotheses and we conclude that using the k-best list obtained by theHiFST decoder yields better parameters in optimization.Search Errors Measured over the tune-nw development set, HiFST finds a hypothesiswith lower cost in 48.4% of the sentences.
In contrast, HCP never finds any hypothesiswith a lower cost for any sentence, indicating that the described pruning strategy forHiFST is much broader than that of HCP.
Note that HCP search errors are more frequentfor this language pair.
This is due to the larger search space required for full hierarchicaltranslation; the larger the search space, the more likely it is that search errors will beintroduced by the cube pruning algorithm.Lattice/k-best Quality Large LMs and MBR both benefit from the richer space oftranslation hypotheses contained in the lattices.
Relative to the first-pass baseline inMT08, rescoring HiFST lattices yields a gain of 2.1 BLEU, compared to a gain of 0.7BLEU with HCP k-best lists.4.2.3 Reliability of n-gram Posterior Distributions.
MBR decoding under linear BLEU(Tromble et al 2008) is driven mainly by the presence of high posterior n-grams inthe lattice; the low posterior n-grams have poor discriminatory power.
In the followingexperiment, we show that high posterior n-grams are more likely to be found in thereferences, and that using the full evidence space of the lattice is much better than evenvery large k-best lists for computing posterior probabilities.
Let Ni = {w1, .
.
.,w|Ni|}denote the set of n-grams of order i in the first-pass translation 1-best, and let Ri ={w1, .
.
.,w|Ri|} denote the set of n-grams of order i in the union of the references.For confidence threshold ?, let Ni,?
= {w ?
Ni : p(w|L) ?
?}
denote the set of alln-grams in Ni with posterior probability greater than or equal to ?, where p(w|L) isthe posterior probability of the n-gram w, that is, the sum of the posterior probabilitiesof all translations containing at least one occurrence of w. The precision at order i forthreshold ?
is the proportion of n-grams in Ni,?
found in the references:pi,?
=|Ri ?Ni,?||Ni,?|.
(7)The average per-sentence 4-gram precision at a range of posterior probability thresholds?
is shown in Figure 12.
The posterior probabilities are computed using either the full524de Gispert et al Hierarchical Translation with WFSTs and Shallow-n GrammarsFigure 124-gram precisions for Arabic-to-English mt02-05-tune first-pass 1-best translations computedusing the full evidence space of the lattice and k-best lists of various sizes.lattice L or a k-best list of the specified size.
The 4-gram precision of the 1-best trans-lations is approximately 0.35.
At higher values of ?, the reference precision increasesconsiderably.
Expanding the k-best list size from 1,000 to 10,000 hypotheses only slightlyimproves the precision but much higher precisions are observed when the full evidencespace of the lattice is used.
The improved precision results from more accurate estimatesof n-gram posterior probabilites and emphasizes once more the advantage of lattice-based decoding and rescoring techniques.4.3 Grammar Configurations and Search ParametersWe report translation performance and decoding speed as we vary hierarchical gram-mar depth and the constraints on low-level rule concatenation (see Section 3).
Unlessotherwise noted, hmin = 1 and hmax = 10 throughout (except for the ?S?
nonterminalcategory, where these constraints are not relevant).4.3.1 Grammars for Arabic-to-English Translation.
Table 4 reports Arabic-to-English trans-lation results using the alternative grammar configurations described in Section 3.5.Results are shown in first-pass decoding (HiFST rows), and in rescoring with a larger5-gram language model for the most promising configurations (+5g rows).
Decodingtime is reported for first-pass decoding only; rescoring time is negligible by comparison.As shown in the upper part of Table 4, translation under a shallow-2 grammar doesnot improve relative to a shallow-1 grammar, although decoding is much slower.
Thisindicates that the additional hypotheses generated when allowing a hierarchical depthof two are not useful in Arabic-to-English translation.
By contrast the shallow gram-mars that allow long-distance movement for verbs only (shallow-1+K1,K2 = 1, 3, vo andshallow-2+K1,K2 = 2, 3, vo), perform slightly better than shallow-1 grammar at a similardecoding time.
Performance differences increase when the larger 5-gram is applied525Computational Linguistics Volume 36, Number 3Table 4Contrastive Arabic-to-English translation results (lower-cased IBM BLEU) with variousgrammar configurations.
Decoding time reported in seconds per word for mt02-05-tune.grammar time mt02-05-tune mt02-05-test mt08HiFST shallow-1 0.8 52.7 52.0 42.9+K1,K2 = 1, 3 1.3 52.6 51.9 42.8+K1,K2 = 1, 3, vo 0.9 52.7 52.1 42.9shallow-2 4.2 52.7 51.9 42.6+K1,K2 = 2, 3, vo 1.8 52.8 52.2 43.0+5g shallow-1 - 53.9 53.4 44.9+K1,K2 = 1, 3, vo - 54.1 53.6 45.0shallow-2+K1,K2 = 2, 3, vo- 54.2 53.8 45.0(Table 4, bottom).
This is expected given that these grammars add valid translationcandidates to the search space with similar costs; a language model is needed to selectthe good hypotheses among all those introduced.4.3.2 Grammars for Chinese-to-English Translation.
Table 5 shows contrastive results inChinese-to-English translation for full hierarchical and shallow-n (n = 1, 2, 3) gram-mars.3 Unlike Arabic-to-English translation, Chinese-to-English translation improvesas the hierarchical depth of the grammar is increased (i.e., for larger n).
Decoding timealso increases significantly.
The shallow-1 grammar constraints which worked well forArabic-to-English translation are clearly inadequate for this task; performance degradesby approximately 1.0 BLEU relative to the full hierarchical grammar.However, we find that translation under the shallow-3 grammar yields performancenearly as good as that of the full hiero grammars; translation times are shorter andyield degradations of only 0.1 to 0.3 BLEU.
Translation can be made significantly fasterby constraining the shallow-3 search space with hmin = 9, 5, 2 for X2,X1, and X0, respec-tively; translation speed is reduced from 10.8 sec/word to 3.8 sec/word at a degradationof 0.2 to 0.3 BLEU relative to full Hiero.Shallow-3 grammars describe a restricted search-space but appear to have expressivepower in Chinese-to-English translation which is very similar to that of a full Hierogrammar.
Each cell (x, y) is represented by a bigger set of nonterminals; this allows formore effective pruning strategies during lattice construction.
We note also that hmaxvalues greater than 10 yield little improvement.
As shown in the five bottom rows ofTable 5, differences between grammar configurations tend to carry through after 5-gramrescoring.
In summary, a shallow-3 grammar and filtering with hmin = 9, 5, 2 lead to a0.4 degradation in BLEU relative to full Hiero.
As a final contrast, the mixed-case NISTBLEU-4 for the HiFST system on mt08 is 28.6.
This result is obtained under the sameevaluation conditions as the official NIST MT08 Constrained Training Track.43 We note that the scores in full hiero row do not match those of row c in Table 3 which were obtained witha slightly simplified version of HiFST and optimized according to the 2008 NIST implementation of IBMBLEU; here we use the 2009 implementation by NIST.4 Full MT08 results are available atwww.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html526de Gispert et al Hierarchical Translation with WFSTs and Shallow-n GrammarsTable 5Contrastive Chinese-to-English translation results (lower-cased IBM BLEU ) with variousgrammar configurations and search parameters.
Decoding time is reported in sec/word fortune-nw.grammar time tune-nw test-nw mt08 (nw)HiFST shallow-1 0.7 33.6 33.4 32.6shallow-2 5.9 33.8 34.2 32.7+hmin=5 5.6 33.8 34.1 32.9+hmin=7 4.0 33.8 34.3 33.0shallow-3 8.8 34.0 34.3 33.0+hmin=7 7.7 34.0 34.4 33.1+hmin=9 5.9 33.9 34.3 33.1+hmin=9,5,2 3.8 34.0 34.3 33.0+hmin=9,5,2+hmax=11 6.1 33.8 34.4 33.0+hmin=9,5,2+hmax=13 9.8 34.0 34.4 33.1full hiero 10.8 34.0 34.4 33.3+5g shallow-1 - 34.1 34.5 33.4shallow-2 - 34.3 35.1 34.0shallow-3 - 34.6 35.2 34.4+hmin=9,5,2 - 34.5 34.8 34.2full hiero - 34.5 35.2 34.64.4 Marginalization Over Translation DerivationsAs has been discussed earlier, the translation model in hierarchical phrase-based ma-chine translation allows for multiple derivations of a target language sentence.
Eachderivation corresponds to a particular combination of hierarchical rules and it has beenargued that the correct approach to translation is to accumulate translation probabilityby summing over the scores of all derivations (Blunsom, Cohn, and Osborne 2008).Computing this sum for each of the many translation candidates explored during de-coding is computationally difficult, however.
For this reason the translation probabilityis commonly computed using the Viterbi max-derivation approximation.
This is theapproach taken in the previous sections in which translations scores were accumulatedunder the tropical semiring.The use of WFSTs allows the sum over alternative derivations of a target stringto be computed efficiently.
HiFST generates a translation lattice realized as a weightedtransducer with output labels encoding words and input labels encoding the sequenceof rules corresponding to a particular derivation, and the cost of each path in the latticeis the negative log probability of the derivation that generated the hypothesis.Determinization applies the ?
operator to all paths with the same word se-quence (Mohri 1997).
When applied in the log semiring, this operator computes thesum of two paths with the same word sequence as x ?
y = ?log(e?x + e?y) so that theprobabilities of alternative derivations can be summed.Currently this operation is only performed in the top cell of the hierarchical decoderso it is still an approximation to the true translation probability.
Computing the truetranslation probability would require the same operation to be repeated in every cellduring decoding, which is very time consuming.
Note that the translation lattice wasgenerated with a language model and so the language model costs must be removed527Computational Linguistics Volume 36, Number 3Table 6Arabic-to-English results (lower-cased IBM BLEU) when determinizing the lattice at theupper-most CYK cell with alternative semirings.semiring mt02-05-tune mt02-05-test mt08tropical HiFST 52.8 52.2 43.0+5g 54.2 53.8 44.9+5g+LMBR 55.0 54.6 45.5log HiFST 53.1 52.6 43.2+5g 54.6 54.2 45.2+5g+LMBR 55.0 54.6 45.5before determinization to ensure that only the derivation probabilities are includedin the sum.
After determinization, the language model is reapplied and the 1-besttranslation hypothesis can be extracted from the logarc determinized lattices.Table 6 compares translation results obtained using the tropical semiring (Viterbilikelihoods) and the log semiring (marginal likelihoods).
First-pass translation showssmall gains in all sets: +0.3 and +0.4 BLEU for mt02-05-tune and mt02-05-test, and +0.2for mt08.
These gains show that the sum over alternative derivations can be easilyobtained in HiFST simply by changing semiring and that these alternative derivationsare beneficial to translation.
The gains carry through to the large language model 5-gramrescoring stage but after LMBR the final BLEU scores are unchanged.
The hypothesesselected by LMBR are in almost all cases exactly the same regardless of the choice ofsemiring.
This may be due to the fact that our current marginalization procedure is onlyan approximation to the true marginal likelihoods, since the log semiring determiniza-tion operation is applied only in the uppermost cell of the CYK grid and MERT trainingis performed using regular Viterbi likelihoods.We note that a close study of the interaction between LMBR and marginalizationover derivations is beyond the scope of this paper.
Our purpose here is to show howeasily these operations can be done using WFSTs.4.5 Combining Lattices Obtained from Alternative Morphological DecompositionsIt has been shown that MBR decoding is a very effective way of combining translationhypotheses obtained from alternative morphological decompositions of the same sourcedata.
In particular, de Gispert et al (2009) show gains for Arabic-to-English and Finnish-to-English when taking k-best lists obtained from two morphological decompositionsof the source language.
Here we extend this approach to the case of translation lat-tices and experiment with more than two alternative decompositions.
We will showthat working with translation lattices gives significant improvements relative to k-bestlists.In lattice-based MBR system combination, first-pass decoding results in a set of Idistinct translation lattices L(i), i = 1. .
.I for each foreign sentence, with each latticeproduced by translating one of the alternative morphological decompositions.
Theevidence space for MBR decoding is formed as the union of these lattices L =?Ii=1 L(i).The posterior probability of n-gram w in the union of lattices is computed as a simple528de Gispert et al Hierarchical Translation with WFSTs and Shallow-n GrammarsTable 7Arabic-to-English results (lower-cased IBM BLEU) when using alternative Arabicdecompositions, and their combination with k-best-based and lattice-based MBR.configuration mt02-05-tune mt02-05-test mt08a HiFST+5g 54.2 53.8 44.9b HiFST+5g 53.8 53.6 45.0c HiFST+5g 54.1 53.8 44.7a+b +MBR 55.1 54.7 46.1+LMBR 55.7 55.4 46.7a+c +MBR 55.4 54.9 46.5+LMBR 56.0 55.9 46.9a+b+c +MBR 55.3 54.9 46.5+LMBR 56.0 55.7 47.3linear interpolation of the posterior probabilities according to the evidence space of eachindividual lattice so thatp(w|L) =I?i=1?i pi(w|L(i) ), (8)where the interpolation parameters 0 ?
?i ?
1 such that?Ii=1 ?i = 1 specify the weightassociated with each system in the combination and are optimized with respect tothe tuning set.
The system-specific posteriors required for the interpolation are com-puted aspi(w|L(i) ) =?E?L(i)wPi(E|F), (9)where Pi(E|F) is the posterior probability of translation E given source sentence F andthe sum is taken over the subset L(i)w = {E ?
L(i) : #w(E) > 0} of the lattice L(i) containingpaths with at least one occurrence of the n-gram w. These posterior probabilities areused in MBR decoding under the linear approximation to the BLEU score describedin Tromble et al (2008).
We find that for system combination, decoding often producesoutput that is slightly shorter than required.
A fixed per-word factor optimized on thetuning set is applied when computing the gain and this results in output with improvedBLEU score and reduced brevity penalty.Table 7 shows translation results in Arabic-to-English using three alternative mor-phological decompositions of the Arabic text (upper rows a, b, and c).
For each decom-position an independent set of hierarchical rules is obtained from the respective parallelcorpus alignments.
The decompositions were generated by the MADA toolkit (Habashand Rambow 2005) with two alternative tokenization schemes, and by the Sakhr ArabicMorphological Tagger, developed by Sakhr Software in Egypt.The following rows show the results when combining with MBR the translationhypotheses obtained from two or three decompositions.
The table also shows a contrast529Computational Linguistics Volume 36, Number 3Figure 13Average per-sentence 4-gram reference precisions for Arabic-to-English mt02-05-tunesingle-system MBR 1-best translations and the 1-best obtained through MBR systemcombination.between decoding the joint k-best lists (rows named MBR, with k = 1, 000) and decod-ing the unioned translation lattices (rows named LMBR).
In line with the findings ofde Gispert et al (2009), we find significant gains from combining k-best lists with respectto using any one segmentation alone.
Interestingly, here we find further gains whenapplying lattice-based MBR instead of a k-best approach, obtaining consistent gains of0.6?0.8 BLEU across all sets.The results reported in Table 7 are very competitive.
The mixed-case NIST BLEU-4score for a+b+c LMBR system in MT08 is 44.9.
This result is obtained under the sameevaluation conditions as the official NIST MT08 Constrained Training Track.5 For MT09,the mixed-case BLEU-4 is 48.3, which ranks first in the Arabic-to-English NIST 2009Constrained Data Track.64.5.1 System Combination and Reference Precision.
We have demonstrated that MBR de-coding of multiple lattices generated from alternative morphological segmentationsleads to significant improvements in BLEU score.
We now show that one reason forthe improved performance is that lattice combination leads to better n-gram posteriorprobability estimates.
To combine two equally weighted lattices L(1) and L(2), the in-terpolation weights are ?1 = ?2 = 12 ; Equation (8) simplifies as p(w|L) = 12 (p1(w|L(1) )+p2(w|L(2))).
Figure 13 plots average per-sentence reference precisions for the 4-grams inthe MBR 1-best of systems a and b and their combination (labeled a+b) at a range ofposterior probability thresholds 0 ?
?
?
1.
Systems a and b have similar precisions atall values of ?, confirming that the optimal interpolation weights for this combinationshould be equal.
The precision obtained using n-gram posterior probabilities computed5 Full MT08 results are available atwww.nist.gov/speech/tests/mt/2008/doc/mt08 official results v0.html6 Full MT09 results are available at www.itl.nist.gov/iad/mig/tests/mt/2009/ResultsRelease530de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammarsfrom the combined lattices is higher than that of the individual systems.
A higherproportion of the n-grams assigned high posterior probability under the interpolateddistribution are found in the references and this is one of the reasons for the large gainsin BLEU in lattice-based MBR system combination.4.6 European Language TranslationThe HiFST described here has also been found to achieve competitive performance forother language pairs, such as Spanish-to-English and Finnish-to-English.For Spanish-to-English we carried out experiments on the shared task of the ACL2008 Workshop on Statistical Machine Translation (Callison-Burch et al 2008) based onthe Europarl corpus.
For the official test2008 evaluation set we obtain a BLEU score of34.2 using a shallow-1 grammar.
Similarly to the Arabic case, deeper grammars are notfound to improve scores for this task.In Finnish-to-English, we conducted translation experiments based on the Europarlcorpus using 3,000 sentences from the Q4/2000 period for testing with a single ref-erence.
In this case, the shallow-1 grammar obtained a BLEU score of 28.0, whereasthe full hierarchical grammar only achieved 27.6.
This is further evidence that full-hierarchical grammars are not appropriate in all instances.
In this case we suspect thatthe use of Finnish words without morphological decomposition leads to data sparsityproblems and complicates the task of learning complex translation rules.
The lack ofa large English language model suitable for this domain may also make it harder toselect the right hypothesis when the translation grammar produces many more Englishalternatives.5.
ConclusionsWe have described two linked investigations into hierarchical phrase-based translation.We investigate the use of weighted finite state transducers rather than k-best lists torepresent the space of translation hypotheses.
We describe a lattice-based Hiero de-coder, with which we find reductions in search errors, better parameter optimization,and improved translation performance.
Relative to these reductions in search errors,direct generation of target language translation lattices also leads to further translationimprovements through subsequent rescoring steps, such as MBR decoding and theapplication of large n-gram language models.
These steps can be carried out easily viastandard WFST operations.As part of the machinery needed for our experiments we develop WFST proceduresfor alignment and feature extraction so that statistics needed for system optimizationcan be easily obtained and represented as transducers.
In particular, we make use ofa lattice-based representation of sequences of rule applications, which proves usefulfor minimum error rate training.
In all instances we find that using lattices as compactrepresentations of translation hypotheses offers clear modeling advantages.We also investigate refinements in translation search space through shallow-n gram-mars, structured long-distance movement, and constrained word deletion.
We findthat these techniques can be used to fit the complexity of Hiero translation systems toindividual language pairs.
In translation from Arabic into English, shallow grammarsmake it possible to explore the entire search space and to do so more quickly but with thesame translation quality as the full Hiero grammar.
Even in complex translation tasks,such as Chinese to English, we find significant speed improvements with minimal loss531Computational Linguistics Volume 36, Number 3in performance using these methods.
We take the view that it is better to perform exactsearch of a constrained space than to risk search errors in translation.We note finally that Chiang introduced Hiero as a model ?based on a synchronousCFG, elsewhere known as a syntax-directed transduction grammar (Lewis and Stearns1968).?
We have taken this formulation as a starting point for the development ofnovel realizations of Hiero.
Our motivation has mainly been practical in that we seekimproved translation quality and efficiency through better models and algorithms.Our approach suggests close links between Hiero and Recursive Transition Net-works (Woods 1970; Mohri 1997).
Although this connection is beyond the scope of thispaper, we do note that Hiero translation requires keeping track of two grammars, onebased on the Hiero translation rules and the other based on n-gram language modelprobabilities.
These two grammars have very different dependencies which suggeststhat a full implementation of Hiero translation such as we have addressed does nothave a simple expression as an RTN.AcknowledgmentsThis work was supported in part by theGALE program of the Defense AdvancedResearch Projects Agency, Contract No.HR0011- 06-C-0022, and in part by theSpanish government and the ERDF underprojects TEC2006-13694-C03-03 andTEC2009-14094-C04-04.ReferencesAllauzen, Cyril, Mehryar Mohri, and BrianRoark.
2003.
Generalized algorithms forconstructing statistical language models.In Proceedings of ACL, pages 557?564,Sapporo.Allauzen, Cyril, Michael Riley, JohanSchalkwyk, Wojciech Skut, and MehryarMohri.
2007.
OpenFst: A general andefficient weighted finite-state transducerlibrary.
In Proceedings of CIAA,pages 11?23, Prague.Bender, Oliver, Evgeny Matusov, StefanHahn, Sasa Hasan, Shahram Khadivi,and Hermann Ney.
2007.
The RWTHArabic-to-English spoken languagetranslation system.
In Proceedings ofASRU, pages 396?401, Kyoto.Blunsom, Phil, Trevor Cohn, and MilesOsborne.
2008.
A discriminative latentvariable model for statistical machinetranslation.
In Proceedings of ACL-HLT,pages 200?208, Columbus, OH.Brants, Thorsten, Ashok C. Popat, Peng Xu,Franz J. Och, and Jeffrey Dean.
2007.Large language models in machinetranslation.
In Proceedings of EMNLP-ACL,pages 858?867, Prague.Callison-Burch, Chris, Cameron Fordyce,Philipp Koehn, Christof Monz, and JoshSchroeder.
2008.
Further meta-evaluationof machine translation.
In Proceedingsof the ACL Workshop on StatisticalMachine Translation, pages 70?106,Columbus, OH.Chappelier, Jean-Ce?dric and Martin Rajman.1998.
A generalized CYK algorithm forparsing stochastic CFG.
In Proceedings ofTAPD, pages 133?137, Paris.Chiang, David.
2005.
A hierarchicalphrase-based model for statistical machinetranslation.
In Proceedings of ACL,pages 263?270, Ann Arbor, MI.Chiang, David.
2007.
Hierarchicalphrase-based translation.
ComputationalLinguistics, 33(2):201?228.de Gispert, Adria`, Sami Virpioja, MikkoKurimo, and William Byrne.
2009.Minimum Bayes-Risk combination oftranslation hypotheses from alternativemorphological decompositions.
InProceedings of HLT/NAACL, CompanionVolume: Short Papers, pages 73?76,Boulder, CO.Deng, Yonggang and William Byrne.
2008.HMM word and phrase alignment forstatistical machine translation.
IEEETransactions on Audio, Speech, and LanguageProcessing, 16(3):494?507.Graehl, Jonathan, Kevin Knight, andJonathan May.
2008.
Training treetransducers.
Computational Linguistics,34(3):391?427.Habash, Nizar and Owen Rambow.
2005.Arabic tokenization, part-of-speechtagging and morphologicaldisambiguation in one fell swoop.
InProceedings of the ACL, pages 573?580,Ann Arbor, MI.Iglesias, Gonzalo, Adria` de Gispert,Eduardo R. Banga, and William Byrne.2009a.
Rule filtering by pattern for532de Gispert et al Hierarchical Translation with WFSTs and Shallow-n Grammarsefficient hierarchical translation.
InProceedings of the EACL, pages 380?388,Athens.Iglesias, Gonzalo, Adria` de Gispert, EduardoR.
Banga, and William Byrne.
2009b.Hierarchical phrase-based translation withweighted finite state transducers.
InProceedings of HLT/NAACL, pages 433?441,Boulder, CO.Iglesias, Gonzalo, Adria` de Gispert, EduardoR.
Banga, and William Byrne.
2009c.
TheHiFST system for the EuroparlSpanish-to-English task.
In Proceedings ofSEPLN, pages 207?214, Donosti.Knight, Kevin and Yaser Al-Onaizan.
1998.Translation with finite-state devices.
InProceedings of the Third Conference of theAMTA on Machine Translation and theInformation Soup, pages 421?437,Langhorne, PA.Koehn, Philipp, Franz Josef Och, and DanielMarcu.
2003.
Statistical phrase-basedtranslation.
In Proceedings of HLT-NAACL,pages 127?133, Edmonton.Kumar, Shankar and William Byrne.
2004.Minimum Bayes-risk decoding forstatistical machine translation.
InProceedings of HLT-NAACL, pages 169?176,Boston, MA.Kumar, Shankar, Yonggang Deng, andWilliam Byrne.
2006.
A weighted finitestate transducer translation templatemodel for statistical machine translation.Natural Language Engineering, 12(1):35?75.Lewis, P. M., II, and R. E. Stearns.
1968.Syntax-directed transduction.
Journal of theACM, 15(3):465?488.Mohri, Mehryar.
1997.
Finite-statetransducers in language and speechprocessing.
Computational Linguistics,23:269?311.Mohri, Mehryar, Fernando Pereira, andMichael Riley.
2000.
The design principlesof a weighted finite-state transducerlibrary.
Theoretical Computer Science,231:17?32.Mohri, Mehryar, Fernando Pereira, andMichael Riley.
2002.
Weighted finite-statetransducers in speech recognition.Computer Speech and Language, 16:69?88.Och, Franz J.
2003.
Minimum error ratetraining in statistical machine translation.In Proceedings of ACL, pages 160?167,Sapporo.Och, Franz Josef and Hermann Ney.
2002.Discriminative training and maximumentropy models for statistical machinetranslation.
In Proceedings of the ACL,pages 295?302, Philadelphia, PA.Papineni, Kishore, Salim Roukos, ToddWard, and Wei-Jing Zhu.
2001.
BLEU: Amethod for automatic evaluation ofmachine translation.
In Proceedings of ACL,pages 311?318, Toulouse.Rosti, Antti-Veikko, Necip Fazil Ayan, BingXiang, Spyros Matsoukas, RichardSchwartz, and Bonnie Dorr.
2007.Combining outputs from multiplemachine translation systems.
InProceedings of HLT-NAACL, pages 228?235,Rochester, NY.Setiawan, Hendra, Min Yen Kan, Haizhou Li,and Philip Resnik.
2009.
Topologicalordering of function words in hierarchicalphrase-based translation.
In Proceedingsof the ACL-IJCNLP, pages 324?332,Singapore.Sim, Khe Chai, William Byrne, Mark Gales,Hichem Sahbi, and Phil Woodland.
2007.Consensus network decoding for statisticalmachine translation system combination.In Proceedings of ICASSP, pages 105?108,Honolulu, HI.Tromble, Roy, Shankar Kumar, Franz J. Och,and Wolfgang Macherey.
2008.
LatticeMinimum Bayes-Risk decoding forstatistical machine translation.
InProceedings of EMNLP, pages 620?629,Honolulu, HI.Varile, Giovanni B. and Peter Lau.
1988.Eurotra practical experience with amultilingual machine translation systemunder development.
In Proceedings of theSecond Conference on Applied NaturalLanguage Processing, pages 160?167,Austin, TX.Woods, W. A.
1970.
Transition networkgrammars for natural language analysis.Communications of the ACM,13(10):591?606.Wu, Dekai.
1997.
Stochastic inversiontransduction grammars and bilingualparsing of parallel corpora.
ComputationalLinguistics, 23(3):377?403.533
