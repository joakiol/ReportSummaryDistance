The ICSI Meeting Recorder Dialog Act (MRDA) CorpusElizabeth Shriberg1,2,  Raj Dhillon1,  Sonali Bhagat1,Jeremy Ang1,  Hannah Carvey1,31International Computer Science Institute2SRI International3 CSU Hayward{ees,rdhillon,sonalivb,jca,hmcarvey}@icsi.berkeley.eduAbstractWe describe a new corpus of over 180,000 hand-annotated dialog act tags and accompanying adjacencypair annotations for roughly 72 hours of speech from 75naturally-occurring meetings.
We provide a brief sum-mary of the annotation system and labeling procedure,inter-annotator reliability statistics, overall distributionalstatistics, a description of auxiliary files distributed withthe corpus, and information on how to obtain the data.1 IntroductionNatural meetings offer rich opportunities for studying avariety of complex discourse phenomena.
Meetingscontain regions of high speaker overlap, affective varia-tion, complicated interaction structures, abandoned orinterrupted utterances, and other interesting turn-takingand discourse-level phenomena.
In addition, meetingsthat occur naturally involve real topics, debates, issues,and social dynamics that should generalize more readilyto other real meetings than might data collected usingartificial scenarios.
Thus meetings pose interesting chal-lenges to descriptive and theoretical models of dis-course, as well as to researchers in the speechrecognition community [4,7,9,13,14,15].We describe a new corpus of hand-annotated dialog actsand adjacency pairs for roughly 72 hours of naturallyoccurring multi-party meetings.
The meetings were re-corded at the International Computer Science Institute(ICSI) as part of the ICSI Meeting Recorder Project [9].Word transcripts and audio files from that corpus areavailable through the Linguistic Data Consortium(LDC).
In this paper, we provide a first description ofthe meeting recorder dialog act (MRDA) corpus, acompanion set of annotations that augment the wordtranscriptions with discourse-level segmentations, dia-log act (DA) information, and adjacency pair informa-tion.
The corpus is currently available online forresearch purposes [16], and we plan a future releasethrough the LDC.2 DataThe ICSI Meeting Corpus data is described in detail in[9].
It consists of 75 meetings, each roughly an hour inlength.
There are 53 unique speakers in the corpus, andan average of about 6 speakers per meeting.
Reflectingthe makeup of the Institute, there are more male thanfemale speakers (40 and 13, respectively).
There area28 native English speakers, although many of thenonnative English speakers are quite fluent.
Of the 75meetings, 29 are meetings of the ICSI meeting recorderproject itself, 23 are meetings of a research groupfocused on robustness in automatic speech recognition,15 involve a group discussing natural languageprocessing and neural theories of language, and 8 aremiscellaneous meeting types.
The last set includes 2very interesting meetings involving the corpustranscribers as participants (example included in [16]).3 AnnotationAnnotation involved three types of information:marking of DA segment boundaries, marking of DAsthemselves, and marking of correspondences betweenDAs (adjacency pairs, [12]).
Each type of annotation isdescribed in detail in [7].
Segmentation methods weredeveloped based on separating out speech regionshaving different discourse functions, but also payingattention to pauses and intonational grouping.
Todistinguish utterances that are prosodically one unit butwhich contain multiple DAs, we use a pipe bar ( | ) inthe annotations.
This allows the researcher to either splitor not split at the bar, depending on the research goals.We examined existing annotation systems, including[1,2,5,6,8,10,11], for similarity to the style of interactionin the ICSI meetings.
We found that SWBD-DAMSL[11], a system adapted from DAMSL [6], provided afairly good fit.
Although our meetings were natural, andthus had real agenda items, the dialog was less likehuman-human or human-machine task-oriented dialog(e.g., [1,2,10]) and more like human-human casualconversation  ([5,6,8,11]).
Since we were working withEnglish rather than Spanish, and did not view a large tagset as a problem, we preferred [6,11] over [5,8] for thiswork.
We modified the system in [11] a number ofways, as indicated in Figure 1 and as explained furtherin [7].
The MRDA system requires one ?general tag?per DA, and attaches a variable number of following?specific tags?.
Excluding nonlabelable cases, there are11 general tags and 39 specific tags.
There are two dis-ruption forms (%-, %--), two types of indecipherableutterances (x, %) and a non-DA tag to denote rising tone(rt).An interface allowed annotators to play regions ofspeech, modify transcripts, and enter DA and adjacencypair information, as well as other comments.
Meetingswere divided into 10 minute chunks; labeling time aver-aged about 3 hours per chunk, although this varied con-siderably depending on the complexity of the dialog.4 Annotated ExampleAn example from one of the meetings is shown in Fig-ure 2 as an illustration of some of the types of interac-tions we observe in the corpus.
Audio files andadditional sample excerpts are available from [16].
Inaddition to the obvious high degree of overlap?roughlyone third of all words are overlapped?note the explicitstruggle for the floor indicated by the two failed floorgrabbers (fg) by speakers c5 and c6.
Furthermore, 6 ofthe 19 total utterances express some form of agreementor disagreement (arp, aa, and nd) with previous utter-ances.
Also, of the 19 utterances within the excerpt, 9are incomplete due to interruption by another talker, asis typical of many regions in the corpus showing highspeaker overlap.
We find in related work that regions ofhigh overlap correlate with high speaker involvement,or ?hot spots?
[15].
The example also provides a tasteof the frequency and complexity of adjacency pair in-formation.
For example, within only half a minute,speaker c5 has interacted with speakers c3 and c6, andspeaker c6 has interacted with speakers c2 and c5.5 ReliabilityWe computed interlabeler reliability among the threelabelers for both segmentation (into DA units) and DAlabeling, using randomly selected excerpts from the 75labeled meetings.
Since agreement on DA segmentationdoes not appear to have standard associated metrics inthe literature, we developed our own approach.
Thephilosophy is that any difference in words at thebeginning and/or end of a DA could result in a differentlabel for that DA, and the more words that aremismatched, the more likely the difference in label.
Asa very strict measure of reliability, we used theTAG TITLESWBD -DAMSLMRDATAG TITLESWBD -DAMSLMRDATAG TITLESWBD -DAMSLMRDAIndecipherable%%Conventional - OpeningfpReformulationbfbsAbandoned% -% --Conventional - ClosingfcAppreciationbabaInterruption% -Topic Cha ngetcSympathybybyNonspeechxxExplicit - PerformativefxDownplayerbdbdSelf - Talkt1t1ExclamationfefeMisspeak Correctionbcbc3 rd - Party Talkt3t3Other - Forward - FunctionfoRhetorical - Question BackchannelbhbhTask - Managem entttThanksftftSignal NonunderstandingbrbrCommunication -ManagementcWelcomefwfwUnderstanding CheckbuStatementsdsApologyfafaDefending/ExplanationdfSubjective StatementsvsFloor - HolderfhMisspeak Self -Correct ionbscWh - QuestionqwqwFloor - Grabberfg"Follow Me"fY/N QuestionqyqyAccept, Yes Answersny, aaaaExpansion/Supporting additioneeOpen - Ended QuestionqoqoPartial AcceptaapaapNarrative - affirmative answersnanaOr QuestionqrqrPartial RejectarparpNarrative - negative answersngngOr Clause After Y/N QuestionqrrqrrMaybeamamNo knowledge answersnonoRhetorical QuestionqhqhReject, No Answersnn, ararDispreferred answersndndDeclarative - QuestionddHoldhhQuoted MaterialqTag QuestionggCollaborative -Completion22Humorous MaterialjOpen - OptionooBackchannelbbContinued from previous line+CommandadcoAcknowledgmentbkbkHedgehSuggestioncocsMimicmmNonlabeledzCommit (self - inclusive)ccccRepeatrFigure 1: Mapping of MRDA tags to SWBD-DAMSL tags.
Tags in boldface are not present in SWBD-DAMSL and wereadded in MRDA.
Tags in italics are based on the SWBD-DAMSL version but have had meanings modified for MRDA.
Theordering of tags in the table is explained as follows: In the mapping of DAMSL tags to SWBD-DAMSL tags in the SWBD-DAMSL manual, tags were ordered in categories such as ?Communication Status?, ?Information Requests?, and so on.
Inthe mapping of MRDA tags to SWBD-DAMSL tags here, we have retained the same overall ordering of tags within the table,but we do not explicitly mark the higher-level SWBD-DAMSL categories in order to avoid confusion, since categoricalstructure differs in the two systems (see [7]).following approach: (1) Take one labeler?s transcript asa reference.
(2) Look at each other labeler?s words.
Foreach word, look at the utterance it comes from and see ifthe reference has the exact same utterance.
(3) If it does,there is a match.
Match every word in the utterance, andthen mark the matched utterance in the reference so itcannot be matched again (this prevents felicitousmatches due to identical repeated words).
(4) Repeatthis process for each word in each reference-labelerpair, and rotate to the next labeler as the reference.
Notethat this metric requires perfect matching of the fullutterance a word is in for that word to be matched.
Forexample in the following case, labelers agree on 3 seg-mentation locations, but the agreement on our metric isonly 0.14, since only 1 of 7 words is matched:.
yeah  .
I agree     it?s a hard decision ..  yeah  .
I agree  .
it?s a hard decision .Overall segmentation results on this metric are providedby labeler pair in Table 1.We examined agreement on DA labels using the Kappastatistic [3], which adjusts for chance agreement.Because of the large number of unique full labelcombinations, we report Kappa values in Table 2 usingvarious class mappings distributed with the corpus.Values are shown by labeler pair.Table 1: Results for strict segmentation agreement metricReferenceLabelerComparisonLabelerAgree Total Agree%1 2 3004 4915 61.11 3 2797 3820 73.22 1 3004 4908 61.22 3 5253 7906 66.43 1 2797 3808 73.53 2 5253 7889 66.6Overall 22108 33246 66.5Table 2: Kappa values for DAs using different class mappings.Map 1: Disruptions vs. backchannels vs. fillers vs. statementsvs.
questions vs. unlabelable; does not break at the ?|?.
Map 2:Same as Map 1 but breaks at the ?|?.
Map 3: Same as Map 2but breaks down fillers and questions into further subclasses.See [16] for further details.Labeler Labeler Map 1 Map 2 Map 31 2 .75 .73 .721 3 .82 .81 .802 3 .82 .77 .75The overall value of Kappa for our basic, six-wayclassmap (Map1) is 0.80, representing good agreementfor this type of task.Time Chan DA AP Transcript2804-2810 c3 s^df^e.%- 34a i mean you can't just like print the - the vaues out in ascii and you know look atthem to see if they're ==2810-2811 c6 fg  well ==2810-2811 c5 s^arp^j 34b not unless you had a lot of time .2811-2812 c5 %-  and ==2811-2814 c6 s^bu 35a uh and also they're not - i mean as i understand it you ?
you don't have a  way tooptimize the features for the final word error .2814-2817 c6 qy^d^g^rt 35a+ right ?2818-2818 c2 s^aa 35b right .2818-2820 c6 s^bd  i mean these are just discriminative .2820-2823 c6 s.%- 36a but they're not um optimized for the final ==2822-2823 c2 s^nd 36b they're optimized for phone discrimination .2823-2825 c2 s^e.%-  not for ==2823-2835 c6 s^bk|s.%- 37a right | so it - there's always this question of  whether you might do better withthose features if there was a way to train it for the word error metric that you'reactually - that you're actually ==2824-2825 c5 s^aa  that's right .2829-2830 c5 s.%-  well the other ==2831-2832 c5 fg|%-  yeah | th- - the ==2833-2835 c2 %-  huh- - huh ==2834-2835 c5 s^nd 37b.38a well you actually are .2835-2837 c5 s^e 37b+.38a+ but ?
but it ?
but in an indirect way .2837-2840 c6 s^aa|s^df.%-  well right | it?s indirect so you don?t know ==Figure 2: Example from meeting Bmr023.
Time marks are truncated here; actual resolution is 10 msec.
?Chan?
: channel(speaker);  ?DA?
: full dialog act label (multiple tags are separated by ?^?
); ?==?
: incomplete DA;  ?xx  -  xx?
: disfluency inter-ruption point between words; ?xx-?
: incomplete word;  ?AP?
:  adjacency pairs (use arbitrary identifiers).
For purposes of illus-tration, overlapped speech regions are indicated in the figure by reverse font color.
Audio and other samples available from [16].6 Distributional StatisticsWe provide basic statistics based on the dialog actlabels for the 75 meetings.
If we ignore the tag markingrising intonation (rt), since this is not a DA tag, we find180,218 total tags.
Table 3 shows the distribution of thetags in more detail.Table 3: Distribution of tags.
Tags are listed in order ofdescending frequency; values are percentages of  the 180,218total tags.s 42.85 b 8.42 fh 4.65 %-- 4.39 bk 4.05aa 3.38 %- 3.33 qy 3.10 df 2.29 e 2.02d 1.74 fg 1.73 cs 1.69 ba 1.37 z 1.36bu 1.28 qw 1.15 na 0.97 g 0.89 % 0.69no 0.57 ar 0.53 j 0.49 2 0.48 co 0.46h 0.44 f 0.41 m 0.40 nd 0.39 tc 0.38r 0.34 t 0.33 fe 0.29 ng 0.28 bd 0.25cc 0.24 qh 0.23 qrr 0.22 am 0.21 t3 0.20x 0.18 t1 0.16 fa 0.16 aap 0.15 br 0.14qr 0.12 qo 0.11 arp 0.10 bsc 0.09 bs 0.09bh 0.09 ft 0.08 bc 0.03 by 0.01If instead we look at only the 11 obligatory general tags,for which there is one per DA, and if we split labels atthe pipe bar, the total is 113,560 (excluding tags thatonly include a disruption label).
The distribution ofgeneral tags is shown in Table 4.Table 4: Distribution of general tags; values are percentages of113,560 total general tags.s 68.00 b 13.37 fh 7.38 qy 4.91fg 2.74 qw 1.82 h 0.70 qh 0.36qrr 0.35 qr 0.20 qo 0.177 Auxiliary InformationWe include other useful information with the corpus.Word-level time information is available, based onalignments from an automatic speech recognizer.Annotator comments are also provided.
We suggestvarious ways to group the large set of labels into asmaller set of classes, depending on the research focus.Finally, the corpus contains information that may beuseful in for developing automatic modeling of prosody,such as hand-marked annotation of rising intonation.8 AcknowledgmentsWe thank Chuck Wooters, Don Baron, Chris Oei, and AndreasStolcke for software assistance, Ashley Krupski for contribu-tions to the annotation scheme, Andrei Popescu-Belis foranalysis and comments on a release of the 50 meetings, andBarbara Peskin and Jane Edwards for general advice and feed-back.
This work was supported by an ICSI subcontract to theUniversity of Washington on a DARPA Communicator pro-ject, ICSI NSF ITR Award IIS-0121396, SRI NASA AwardNCC2-1256, SRI NSF IRI-9619921, an SRI DARPA ROARproject, an ICSI award from the Swiss National Science Foun-dation through the research network IM2, and by the EUFramework 6 project on Augmented Multi-party Interaction(AMI).
The views are those of the authors and do not repre-sent the views of the funding agencies.References[1] Alexandersson, J., Buschbeck-Wolf, B., Fujinami, T., et al  Dialogue Acts in VERBMOBIL-2 Second Edition.
VM-Report226, DFKI Saarbr?cken, Germany, July 1998.
[2] Anderson, A. H., Bader, M., Bard, E. G., et al (1991).
TheHCRC Map Task Corpus.
Language and Speech, 34(4), 351-366.
[3] Carletta, J., 1996.
Assessing agreement on classification tasks:The Kappa Statistic.
Computational Linguistics, 22:2, 249-254.
[4]  Cieri, C., Miller, D. & Walker, K., 2002.
Research methodolo-gies, observations, and outcomes in conversational speech datacollection.
Proc.
HLT 2002.
[5]   Clark, A.
& Popescu-Belis, A., 2004.
Multi-level Dialogue ActTags.
In Proceedings of SIGDIAL ?04 (5th SIGDIAL Workshopon Discourse and Dialog).
Cambridge, MA.
[6] Core, M. & Allen, J., 1997.
Coding dialogs with the DAMSLannotation scheme.
Working Notes: AAAI Fall Symposium,AAAI, Menlo Park, CA, pp.
28-35.
[7]  Dhillon, R., Bhagat, S., Carvey, H., & Shriberg, E., 2004.
Meet-ing Recorder Project: Dialog Act Labeling Guide.
ICSI Techni-cal Report TR-04-002, International Computer Science Institute.
[8]  Finke, M., Lapata, M., Lavie, A., et al, 1998.
CLARITY:Inferring discourse structure from speech.
AAAI ?98 SpringSymposium Series, March 23-25, 1998, Stanford University,California.
[9] Janin, A. et al, 2003.
The ICSI Meeting Corpus.
Proc.
ICASSP-2003.
[10] Jekat, S., Klein, A., Maier, E., et al  Dialogue Acts in Verbmo-bil, Verbmobil-Report No.
65, April 1995.
[11] Jurafsky, D., Shriberg, E., & Biasca, D., 1997.
SwitchboardSWBD-DAMSL Labeling Project Coder?s Manual, Draft 13.Technical Report 97-02, Univ.
of Colorado Institute of Cogni-tive Science.
[12] Levinson, S., 1983.
Pragmatics.
Cambridge: Cambridge   Uni-versity Press.
[13] NIST meeting transcription project, www.nist.gov/speech/test_beds[14] Waibel, A., et al, 2001.
Advances in automatic meeting recordcreation and access.
Proc.
ICASSP-2001.
[15] Wrede, B.
& Shriberg, E., 2003.
The relationship between dia-logue acts and hot spots in meetings.
Proc.
IEEE Speech Rec-ognition and Understanding Workshop, St.
Thomas.
[16] www.icsi.berkeley.edu/~ees/dadb contains the annotationcorpus and sample (audio + annotations)  excerpts.
