An Efficient Clustering Algorithm for Class-based Language ModelsTakuya Matsuzaki Yusuke MiyaoDepartment of Computer Science, University of TokyoHongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPANCREST, JST (Japan Science and Technology Corporation)Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPANmatuzaki,yusuke,tsujii@is.s.u-tokyo.ac.jpJun?ichi TsujiiAbstractThis paper defines a general form for class-based probabilistic language models and pro-poses an efficient algorithm for clusteringbased on this.
Our evaluation experiments re-vealed that our method decreased computationtime drastically, while retaining accuracy.1 IntroductionClustering algorithms have been extensively studied inthe research area of natural language processing becausemany researchers have proved that ?classes?
obtained byclustering can improve the performance of various NLPtasks.
Examples have been class-based -gram models(Brown et al, 1992; Kneser and Ney, 1993), smooth-ing techniques for structural disambiguation (Li and Abe,1998) and word sense disambiguation (Shu?tze, 1998).In this paper, we define a general form for class-basedprobabilistic language models, and propose an efficientand model-theoretic algorithm for clustering based onthis.
The algorithm involves three operations, CLAS-SIFY, MERGE, and SPLIT, all of which decreases theoptimization function based on the MDL principle (Ris-sanen, 1984), and can efficiently find a point near the lo-cal optimum.
The algorithm is applicable to more generaltasks than existing studies (Li and Abe, 1998; Berkhinand Becher, 2002), and computational costs are signifi-cantly small, which allows its application to very largecorpora.Clustering algorithms may be classified into threetypes.
The first is a type that uses various heuristic mea-sure of similarity between the elements to be clusteredand has no interpretation as a probabilitymodel (Widdow,2002).
The resulting clusters from this type of methodare not guaranteed to work effectively as a componentof a statistical language model, because the similarityused in clustering is not derived from the criterion in thelearning process of the statistical model, e.g.
likelihood.The second type has clear interpretation as a probabilitymodel, but no criteria to determine the number of clusters(Brown et al, 1992; Kneser and Ney, 1993).
The perfor-mance of methods of this type depend on the number ofclusters that must be specified before the clustering pro-cess.
It may prove rather troublesome to determine theproper number of clusters in this type of method.
Thethird has interpretation as a probability model and usessome statistically motivated model selection criteria todetermine the proper number of clusters.
This type hasa clear advantage compared to the second.
AutoClass(Cheeseman and Stutz, 1996), the Bayesian model merg-ing method (Stolcke and Omohundro, 1996) and Li?smethod (Li, 2002) are examples of this type.
AutoClassand the Bayesian model merging are based on soft clus-tering models and Li?s method is based on a hard clus-tering model.
In general, computational costs for hardclustering models are lower than that for soft clusteringmodels.
However, the time complexity of Li?s method isof cubic order in the size of the vocabulary.
Therefore, itis not practical to apply it to large corpora.Our model and clustering algorithm provide a solutionto these problems with existing clustering algorithms.Since the model has clear interpretation as a probabilitymodel, the clustering algorithm uses MDL as clusteringcriteria and using a combination of top-down clustering,bottom-up clustering, and a K-means style exchange al-gorithm, the method we propose can perform the cluster-ing efficiently.We evaluated the algorithm through experiments ona disambiguation task of Japanese dependency analysis.In the experiments, we observed that the proposed algo-rithm?s computation time is roughly linear to the size ofthe vocabulary, and it performed slightly better than theexisting method.
Our main intention in the experimentswas to see improvements in terms of computational cost,not in performance in the test task.
We will show, in Sec-tions 2 and 3, that the proposed method can be appliedto a broader range of tasks than the test task we evalu-ate in the experiments in Section 4.
We need further ex-periments to determine the performance of the proposedmethod with more general tasks.2 Probability model2.1 Class-based language modelingOur probability model is a class-based model and it is anextension of the model proposed by Li and Abe (1998).We extend their two-dimensional class model to a multi-dimensional class model, i.e., we incorporate an arbitrarynumber of random variables in our model.Although our probabilitymodel and learning algorithmare general and not restricted to particular domains, wemainly intend to use them in natural language process-ing tasks where large amounts of lexical knowledge arerequired.
When we incorporate lexical information intoa model, we inevitably face the data-sparseness problem.The idea of ?word class?
(Brown et al, 1992) gives a gen-eral solution to this problem.
A word class is a groupof words which performs similarly in some linguisticphenomena.
Part-of-speech are well-known examples ofsuch classes.
Incorporating word classes into linguisticmodels yields good smoothing or, hopefully, meaningfulgeneralization from given samples.2.2 Model definitionLet us introduce some notations to define our model.
Inour model, we have considered  kinds of discrete ran-dom variables       and their joint distribu-tion.
denotes a set of possible values for the -th vari-able .
Our probability model assumes disjunctive par-titions of each , which are denoted by ?s.
A disjunc-tive partition          of  is a subset of, and satisfies   	   and   .We call elements in a partition classes of elements in.
, or for short, denotes a class in whichcontains an element   .With these notations, our probability model is ex-pressed as:              (1)In this paper, we have considered a hard clusteringmodel,i.e.,   for any   .
Li & Abe?s model(1998) is an instance of this joint probability model,where   .
Using more than 2 variables the model canrepresent the probability for the co-occurrence of triplets,such as subject, verb, object.2.3 Clustering criterionTo determine the proper number of classes in each par-tition      , we need criteria other than the maxi-mum likelihood criterion, because likelihood always be-come greater when we use smaller classes.
We can seethis class number decision problem as a model selectionproblem and apply some statistically motivated modelselection criteria.
As mentioned previously (followingLi and Abe (1998)) we used the MDL principle as ourclustering criterion.Assume that we have  samples of co-occurrencedata:               The objective function in both clustering and parame-ter estimations in our method is the description length,, which is defined as follows:   	 (2)where  denotes the model and  is the likelihoodof samples  under model  :      (3)The first term in Eq.2,  	, is called the datadescription length.
The second term, , is called themodel description length, and when sample size  islarge, it can be approximated as 	where  is the number of free parameters in model  .We used this approximated form throughout this paper.Given the number of classes,   for each       , we have  free parameters for jointprobabilities  .
Also, for each class , wehave   free parameters for conditional probabilities, where   .
Thus, we have      Our learning algorithm tries to minimize  byadjusting the parameters in the model, selecting partitionof each , and choosing the numbers of classes, in each partition .3 Clustering algorithmOur clustering algorithm is a combination of three ba-sic operations: CLASSIFY, SPLIT and MERGE.
We it-eratively invoke these until a terminate condition is met.Briefly, these three work as follows.
The CLASSIFYtakes a partition  in  as input and improves the par-tition by moving the elements in  from one class to an-other.
This operation is similar to one iteration in the K-means algorithm.
The MERGE takes a partition  as in-put and successively chooses two classes and from and replaces themwith their union,.
The SPLITtakes a class, , and tries to find the best division of into two new classes, which will decrease the descriptionlength the most.All of these three basic operations decrease the de-scription length.
Consequently, our overall algorithmalso decreases the description length monotonically andstops when all three operations cause no decrease in de-scription length.
Strictly, this termination does not guar-antee the resulting partitions to be even locally opti-mal, because SPLIT operations do not perform exhaus-tive searches in all possible divisions of a class.
Doingsuch an exhaustive search is almost impossible for a classof modest size, because the time complexity of such anexhaustive search is of exponential order to the size of theclass.
However, by properly selecting the number of tri-als in SPLIT, we can expect the results to approach somelocal optimum.It is clear that the way the three operations are com-bined affects the performance of the resulting class-basedmodel and the computation time required in learning.
Inthis paper, we basically take a top-down, divisive strat-egy, but at each stage of division we do CLASSIFY op-erations on the set of classes at each stage.
When wecannot divide any classes and CLASSIFY cannot moveany elements, we invoke MERGE to merge classes thatare too finely divided.
This top-down strategy can drasti-cally decrease the amount of computation time comparedto the bottom-up approaches used by Brown et al (1992)and Li and Abe (1998).The following is the precise algorithm for our mainprocedure:Algorithm 1 MAIN PROCEDURE()INPUT : an integer specifying the number of trials in aSPLIT operationOUTPUTPartitions   and estimated parameters in themodelPROCEDUREStep 0    	 INITIALIZE  Step 1 Do Step 2 through Step 3 until no change is madethrough one iterationStep 2 For     , do Step 2.1 through Step 2.2Step 2.1 Do Step 2.1.1 until no change occurs through itStep 2.1.1 For      , CLASSIFYStep 2.2 For each   ,  	 SPLIT Step 3 For     , MERGEStep 4 Return the resulting partitions with the parame-ters in the modelIn the Step 0 of the algorithm, INITIALIZE createsthe initial partitions of      .
It first divides each     into two classes and then applies CLASSIFYto each partition      one by one, while any ele-ments can move.The following subsections explain the algorithm forthe three basic operations in detail and show that theydecrease  monotonically.3.1 Iterative classificationIn this subsection, we explain a way of finding a localoptimum in the possible classification of elements in ,given the numbers of classes in partitions .Given the number of classes, optimization in terms ofthe description length (Eq.2) is just the same as optimiz-ing the likelihood (Eq.3).
We used a greedy algorithmwhich monotonically increases the likelihood whileupdating classification.
Our method is a generalizedversion of the previously reported K-means/EM-algorithm-style, iterative-classification methods inKneser and Ney (1993), Berkhin and Becher (2002) andDhillon et al (2002).
We demonstrate that the method isapplicable to more generic situations than those previ-ously reported, where the number of random variables isarbitrary.To explain the algorithmmore fully, we define ?counterfunctions?
 as follows:                                      where the hatch () denotes the cardinality of a set andis the -th variable in sample .
We used  	   ,in this subsection.Our classification method is variable-wise.
That is, toclassify elements in each      , we classified theelements in each in order.
The precise algorithm is asfollows:Algorithm 2 CLASSIFY()INPUT : a partition in OUTPUT An improved partition in PROCEDUREStep 1 Do steps 2.1 through 2.3 until no elements in can move from their current class to another one.Step 2.1 For each element   , choose a class  which satisfies the following two conditions:1.
 is not empty   	 , and2.
 maximizes following quantity  :             When the class containing  now, , maximizes ,select as even if some other classes also max-imize .Step 2.2 Update partition by moving each   tothe classes which were selected as  for  in Step2.1.Step 2.3 Update the parameters by maximum likelihoodestimation according to the updated partition.Step 3 Return improved partition .In Step 2.3, the maximum likelihood estimation of theparameters are given as follows:        (4)To see why this algorithm monotonically increases thelikelihood (Eq.3), it is sufficient to check that, for vari-able and any classification before Steps 2 and 3, do-ing Steps 2 and 3 positively changes the log likelihood(Eq.3).
We can show this as follows.First, assume    without loss of generality.
Let       and      denotethe partitions before/after Step 2, respectively.
Let anddenote the classes where an element belongs, before and after Step 2, respectively.Also, let   denote the class which was chosen forin Step 2.1 in the algorithm.
Note that  is differentfromas a set.
However, with these notations, it holdsthat if   , then.
We also use the suffixesin notations andas it holds that, if   , then.Using Eq.4, we can write the change in the log likeli-hood, 	 as follows:	              (5)To see the difference is  , we insert the intermediateterms into the right of Eq.5 and transform it as:	                                        (6)  (7)In the last expression, each term in the summation (7)is   according to the conditions in Step 2 of the al-gorithm.
Then, the summation (7) as a whole is always and only equals 0 if no elements are moved.
Wecan confirm that the summation (6) is positive, throughan optimization problem:maximize the following quantity    	  under the condition:    for any 	 . is   because 	, and    is always  .
Thus, the solution to this problem is givenby:      for any 	 .
Through this, we can concludethat the summation (6) is  .
Therefore, 	  holds, i.e., CLASSIFY increases log likelihoodmonoton-ically.3.2 SPLIT operationThe SPLIT takes a class as input and tries to find a wayto divide it into two sub-classes in such a way as to re-duce description length.
As mentioned earlier, to find thebest division in a class requires computation time that isexponential to the size of the class.
We will first use abrute-force approach here.
Let us simply try  randomdivisions, rearrange them with CLASSIFY and use thebest one.
If the best division does not reduce the descrip-tion length, we will not change the class at all.
It maypossible to use a more sophisticated initialization scheme,but this simple method yielded satisfactory results in ourexperiment.The following is the precise algorithm for SPLIT:Algorithm 3 SPLIT(, )INPUT : a class to be split : an integer specifying the number of trialsOUTPUTTwo new classes and on success, or  withno modifications on failurePROCEDUREStep 1 Do Steps 2.1 through 2.3 J timesStep 2.1 Randomly divide  into two classesStep 2.2 Apply CLASSIFY to these two classesStep 2.3 Record the resulting two classes in Step 2.2 withthe reduced description length produced by this splitStep 3 Find the maximum reduction in the recordsStep 4 If this maximum reduction  , return the corre-sponding two classes as output, or return  if themaximum  Clearly, this operation decreases  on successand does not change it on failure.3.3 MERGE operationThe MERGE takes partition  as input and successivelychooses two classes and from  and replaces themwith their union.
This operation thus reduces thenumber of classes in  and accordingly reduces the num-ber of parameters in the model.
Therefore, if we properlychoose the ?redundant?
classes in a partition, this mergingreduces the description length by the greater reduction inthe model description length which surpasses the loss inlog-likelihood.Our MERGE is almost the same procedure as that de-scribed by Li (2002).
We first compute the reduction indescription length for all possible merges and record theamount of reduction in a table.
We then do the merges inorder of reduction, while updating the table.The following is the precise algorithm for MERGE.In the pseudo code, ?denotes the reduction in which results in the merging of and .Algorithm 4 MERGE( )INPUT  : a partition in OUTPUT An improved partition in  on success, or thesame partition as the input on failurePROCEDUREStep 1 For each pair   in  compute ?andstore them in a table.Step 2 Do Step 3.1 through 3.5 until the termination con-dition in 3.2 is metStep 3.1 Find the maximum, ?, in all ?Step 3.2 If ?, return the updated partition, orelse go to Step 3.3.Step 3.3 Replace the class pair   which corre-sponds to ?, with their union   .Step 3.4 Delete all ??s which concern the mergedclasses or from the table.Step 3.5 For each in   , compute ?andstore them in the table.It is clear from the termination condition in Step 3.2that this operation reduces  on success but doesnot change it on failure.4 EvaluationThis section discusses the results of the evaluation ex-periment where we compared three clustering methods:i.e., our method, Li?s agglomerative method described inLi (2002), and a restricted version of ourmethod that onlyuses CLASSIFY.4.1 Evaluation taskWe used a simplified version of the dependency analysistask for Japanese for the evaluation experiment.In Japanese, a sentence can be thought of as an array ofphrasal units called ?bunsetsu?
and the dependency struc-ture of a sentence can be represented by the relationshipsbetween these bunsetsus.
A bunsetsu consists of one ormore content words and zero or more function words thatfollow these.For example, the Japanese sentenceRyoushi-ga kawa-de oyogu nezumi-wo utta.hunter-SUBJ river-in swim mouse-OBJ shot(A hunter shot a mouse which swam in the river.
)contains five bunsetsus  Ryoushi-ga, kawa-de, oyogu,nezumi-wo, utta  and their dependency relations are asfollows:Ryoushi-gautta kawa-deoyoguoyogunezumi-wo nezumi-wouttaOur task is, given an input bunsetsu, to output the cor-rect bunsetsu on which the input bunsetsu depends.
Inthis task, we considered the dependency relations of lim-ited types.
That is the dependency of types: noun-pppred , where noun is a noun, or the head of a compoundnoun, pp is one of 9 postpositions ga, wo, ni, de, to,he, made, kara, yori and pred is a bunsetsu which con-tains a verb or an adjective as its content word part.
Werestricted possible dependee bunsetsus to be those to theright of the input bunsetsus because in Japanese, basicallyall dependency relations are from left to right.
Thus, ourtest data is in the form noun-pp pred  pred  (8)where pred,...,pred is the set of all candidate de-pendee bunsetsus that are to the right of the input depen-dent bunsetsu noun-pp in a sentence.
The task is to selectthe correct dependee of noun-pp from pred,..,pred.Our training data is in the form , noun, pp, pred.A sample of this form represents two bunsetsus, noun-pp and pred within a sentence, in this order, and   denotes whether they are in a dependency relation( ), or not (  ).
From these types of samples,we want to estimate probability noun pp pred anduse these to approximate probability, where given thetest data in Eq.8, predis the correct answer, expressedas noun pp pred  noun pp predWe approximated the probability of occurrence forsample type    expressed as noun pp pred  noun pp predand estimated these from the raw frequencies.
For theprobability of type  , we treated a pair of pp andpred as one variable, pp:pred, expressed as noun pp pred noun pp:predand estimatednoun pp:pred from the training data.Thus, our decision rule given test data (Eq.8) is, toselect predwhere  is the index which maximizes thevaluenoun pp:pred pp predWe extracted the training samples and the test datafrom the EDR Japanese corpus (EDR, 1994).
We ex-tracted all the positive (i.e.,  ) and negative (  )relation samples and divided them into 10 disjunctive setsfor 10-fold cross validation.
When we divided the sam-ples, all the relations extracted from one sentence wereput together in one of 10 sets.
When a set was used asthe test data, these relations from one sentence were usedas the test data of the form (Eq.8).
Of course, we did notuse samples with only one pred.
In the results in the nextsubsection, the ?training data of size ?
means where weused a subset of positive samples that were covered by themost frequent  nouns and the most frequent  pp:predpairs.4.2 ResultsIn this experiments, we compared three methods: ours,Li?s described in Li (2002), and a restricted version ofour method that only uses CLASSIFY operations.
Thelast method is simply called ?the CLASSIFY method?in this subsection.
We used 10 as parameter  in ourmethod, which specifies the number of trials in initializa-tion and each SPLIT operation.
Li?s method (2002) usesthe MDL principle as clustering criteria and creates wordclasses in a bottom-up fashion.
Parametersandinhis method, which specify the maximum numbers of suc-cessive merges in each dimension, were both set to 100.The CLASSIFY method performs K-means style itera-tive clustering and requires that the number of clusters bespecified beforehand.
We set these to be the same as thenumber of clusters created by our method in each train-ing set.
By evaluating the differences in the performanceof ours and the CLASSIFY method, we can see advan-tages in our top-down approach guided by the MDL prin-ciple, compared to the K-means style approach that uses afixed number of clusters.We expect that these advantageswill remain when compared to other previously reported,K-means style methods (Kneser and Ney, 1993; Berkhinand Becher, 2002; Dhillon et al, 2002).In the results, precision refers to the ratio !!
"and coverage refers to the ratio !#, where !
and" denotethe numbers of correct and wrong predictions, and # de-notes the number of all test data.
All the ?ties cases?
were1101001000100001000001000 10000 100000computationtime(sec)size of vocabularyour methodLi?s methodCLASSIFYFigure 1: Computation time101001000100001000000 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45computationtime(sec)coverageour methodLi?s methodFigure 2: Coverage-Cost plottreated as wrong answers ("), where a ?tie case?
meansa situation where two or more predictions are made withthe same maximum probabilities.All digits are averages of results for ten training-testpairs, except for Li?s method where the training sets were8k or more.
The results of the Li?s method on trainingset of 8k were the averages over two training-test pairs.We could not do more trials with Li?s method due to timeconstraints.
All experiments were done on Pentium III1.2-GHz computers and the reported computation timesare wall-clock times.Figure 1 shows the computation time as a function ofthe size of the vocabulary, i.e., the number of nouns plusthe number of case frame slots (i.e., pp:pred) in the train-ing data.
We can clearly see the efficiency of our methodin the plot, compared to Li?s method.
The log-log plot re-veals our time complexity is roughly linear to the size ofthe vocabulary in these data sets.
This is about two orderslower than that for Li?s method.There is little relevance in comparing the speed of the0.740.750.760.770.780.790.80.810 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45precisioncoverageour methodLi?s methodCLASSIFYFigure 3: Coverage-precision plotCLASSIFY method to the speed of the other two meth-ods, because its computation time does not include thetime required to decide the proper number of classes.
Ofmore interest is to see its seeming speed-up in the largestdata sets.
This implies that, in large and sparse trainingdata, the CLASSIFY method was caught in some bad lo-cal optima at some early points on the way to better localoptima.Figure 2 has the computation times as a function ofthe coverage which is achieved using that computationtime.
From this, we would expect our method to reachhigher coverage within a realistic time if we used largerquantities of training data.
To determine this, we needother experiments using larger corpora, which we intendto do in the future.Table 1 lists the description lengths for training datafrom 1 to 32k and Table 2 shows the precision and cov-erage achieved by each method with this data.
In thesetables, we can see that our method works slightly betterthan Li?s method as an optimization method which min-imizes the description length, and also in the evaluationtasks.
Therefore, we can say that our method decreasedcomputational costs without losing accuracy.
We can alsosee that ours always performs better than the CLASSIFYmethod.
Both ours and the CLASSIFY method use ran-dom initializations, but from the results, it seems that ourtop-down, divisive strategy in combination with K-meanslike swapping and merging operations avoids the poor lo-cal optima where the CLASSIFY method was caught.Figure 3 also presents the results in terms of coverage-precision trade-off.
We can see that our method selectedalways better points in the trade-off than Li?s method orthe CLASSIFY method.From these results, we can conclude that our cluster-ing algorithm is more efficient and yields slightly betterresults than Li?s method, which uses the same cluster-ing criterion.
We can also expect that our combined ap-size of test data 1k 2k 3k 4k 5k 8k 16k 32kour method 1.15 1.88 2.38 2.76 3.13 3.77 5.03 6.21Li?s method 1.16 1.89 2.40 2.80 3.17 3.85 N/A N/ACLASSIFY 1.16 1.89 2.39 2.77 3.14 3.79 5.08 6.31Table 1: Description length in training data sets (unit: )size of training data 1k 2k 3k 4k 5k 8k 16k 32kour method precision 0.805 0.799 0.798 0.794 0.791 0.797 0.780 0.745coverage 0.043 0.076 0.109 0.136 0.163 0.245 0.362 0.429Li?s method precision 0.802 0.795 0.793 0.786 0.784 0.791 N/A N/Acoverage 0.043 0.076 0.109 0.135 0.162 0.242 N/A N/ACLASSIFY precision 0.797 0.792 0.789 0.785 0.786 0.789 0.768 0.741coverage 0.042 0.075 0.108 0.135 0.162 0.242 0.356 0.427Table 2: Performance of each method in the evaluation taskproach with the MDL principle will have advantages inlarge and sparse data compared to existing K-means styleapproaches where the number of the clusters is fixed.5 ConclusionThis paper proposed a general, class-based probabilitymodel and described a clustering algorithm for it, whichwe evaluated through experiments on a disambiguationtask of Japanese dependency analysis.
We obtained thefollowing results.
(1) Our clustering algorithm was muchmore efficient than the existing method that uses the sameobjective function and the same kind of model.
(2) Itworked better as an optimization algorithm for the de-scription length than the existing method.
(3) It per-formed better in the test task than an existing method andanother method that is similar to other existing methods.ReferencesAndreas Stolcke and Stephen M. Omohundro.
1994.Best-first Model Merging for Hidden Markov ModelInduction.
Technical Report TR-94-003, ComputerScience Division, University of California at Berkeleyand International Science Institute.Dominic Widdow and Beate Dorow.
2002.
A GraphModel for Unsupervised Lexical Acquisition.
Pro-ceedings of the 19th International Conference on Com-putational Linguistics, 1093?1099.EDR.
1994.
EDR (Japanese Electronic Dictionary Re-search Institute, Ltd) dictionary version 1.5 technicalguide.Hang Li.
2002.
Word Clustering and Disambiguationbased on Co-occurrence Data, Natural Language En-gineering, 8(1), 25-42.Hang Li and Naoki Abe.
1998.
Word Clustering andDisambiguation Based on Co-occurrence data.
Pro-ceedings of the 18th International Conference on Com-putational Linguistics and the 36th Annual Meeting ofAssociation for Computational Linguistics, 749?755.Hinrich Schu?tze.
1998.
Automatic Word Sense Discrim-ination Computational Linguistics, 24(1) 97?124.Inderjit S. Dhillon, SubramanyamMallela and Rahul Ku-mar.
2002.
Information Theoretic Feature Clusteringfor Text Classification.
The Nineteenth InternationalConference on Machine Learning, Workshop on TextLearning.Jorma Rissanen.
1984.
Universal Coding, Information,Prediction, and Estimation.
IEEE Transactions on In-formation theory, Vol.
IT-30(4):629?636Pavel Berkhin and Jonathan Becher.
2002.
LearningSimple Relations: Theory and Applications.
In Pro-ceedings of the Second SIAM International Conferenceon Data Mining, 420?436.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,Jennifer C. Lai and Robert L. Mercer.
1992.
Class-Based n-gram Models of Natural Language.
Compu-tational Linguistics 18(4):467-479.Peter Cheeseman and John Stutz.
1996.
Bayesian Clas-sification (AutoClass): Theory and Results.
In U.Fayyad, G. Piatetsky-Shapiro, P. Smyth and R. Uthu-rusamy (Eds.
), Advances in Knowledge Discovery andData Mining, 153?180.
AAAI Press.Reinherd Kneser and Hermann Ney.
1993.
ImprovedClustering Techniques for Class-Based Statistical Lan-guage Modelling.
In Proceedings of the 3rd EuropeanConference on Speech Communication and Technol-ogy, 973?976.
