Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 584?591,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Seed-driven Bottom-up Machine Learning Frameworkfor Extracting Relations of Various ComplexityFeiyu Xu, Hans Uszkoreit and Hong LiLanguage Technology Lab, DFKI GmbHStuhlsatzenhausweg 3, D-66123 Saarbruecken{feiyu,uszkoreit,hongli}@dfki.deAbstractA minimally supervised machine learningframework is described for extracting rela-tions of various complexity.
Bootstrappingstarts from a small set of n-ary relation in-stances as ?seeds?, in order to automati-cally learn pattern rules from parsed data,which then can extract new instances of therelation and its projections.
We propose anovel rule representation enabling thecomposition of n-ary relation rules on topof the rules for projections of the relation.The compositional approach to rule con-struction is supported by a bottom-up pat-tern extraction method.
In comparison toother automatic approaches, our rules can-not only localize relation arguments butalso assign their exact target argumentroles.
The method is evaluated in twotasks: the extraction of Nobel Prize awardsand management succession events.
Perfor-mance for the new Nobel Prize task isstrong.
For the management successiontask the results compare favorably withthose of existing pattern acquisition ap-proaches.1 IntroductionInformation extraction (IE) has the task to discovern-tuples of relevant items (entities) belonging to ann-ary relation in natural language documents.
Oneof the central goals of the ACE program1 is to de-velop a more systematically grounded approach toIE starting from elementary entities, binary rela-1 http://projects.ldc.upenn.edu/ace/tions to n-ary relations such as events.
Currentsemi- or unsupervised approaches to automaticpattern acquisition are either limited to a certainlinguistic representation (e.g., subject-verb-object),or only deal with binary relations, or cannot assignslot filler roles to the extracted arguments, or donot have good selection and filtering methods tohandle the large number of tree patterns (Riloff,1996; Agichtein and Gravano, 2000; Yangarber,2003; Sudo et al, 2003; Greenwood and Stevenson,2006; Stevenson and Greenwood, 2006).
Most ofthese approaches do not consider the linguistic in-teraction between relations and their projections onk dimensional subspaces where 1?k<n, which isimportant for scalability and reusability of rules.Stevenson and Greenwood (2006) present a sys-tematic investigation of the pattern representationmodels and point out that substructures of the lin-guistic representation and the access to the embed-ded structures are important for obtaining a goodcoverage of the pattern acquisition.
However, allconsidered representation models (subject-verb-object, chain model, linked chain model and sub-tree model) are verb-centered.
Relations embeddedin non-verb constructions such as a compoundnoun cannot be discovered:(1)  the 2005  Nobel Peace Prize(1) describes a ternary relation referring to threeproperties of a prize: year, area and prize name.We also observe that the automatically acquiredpatterns in Riloff (1996), Yangarber (2003), Sudoet al (2003), Greenwood and Stevenson (2006)cannot be directly used as relation extraction rulesbecause the relation-specific argument role infor-mation is missing.
E.g., in the management succes-sion domain that concerns the identification of jobchanging events, a person can either move into a584job (called Person_In) or leave a job (called Per-son_Out).
(2) is a simplified example of patternsextracted by these systems:(2) <subject: person> verb <object:organisation>In (2), there is no further specification of whetherthe person entity in the subject position is Per-son_In or Person_Out.The ambitious goal of our approach is to providea general framework for the extraction of relationsand events with various complexity.
Within thisframework, the IE system learns extraction pat-terns automatically and induces rules of variouscomplexity systematically, starting from samplerelation instances as seeds.
The arity of the seeddetermines the complexity of extracted relations.The seed helps us to identify the explicit linguisticexpressions containing mentionings of relation in-stances or instances of their k-ary projectionswhere 1?k<n.
Because our seed samples are notlinguistic patterns, the learning system is not re-stricted to a particular linguistic representation andis therefore suitable for various linguistic analysismethods and representation formats.
The patterndiscovery is bottom-up and compositional, i.e.,complex patterns can build on top of simple pat-terns for projections.We propose a rule representation that supportsthis strategy.
Therefore, our learning approach isseed-driven and bottom-up.
Here we use depend-ency trees as input for pattern extraction.
We con-sider only trees or their subtrees containing seedarguments.
Therefore, our method is much moreefficient than the subtree model of Sudo et al,(2003), where all subtrees containing verbs aretaken into account.
Our pattern rule ranking andfiltering method considers two aspects of a pattern:its domain relevance and the trustworthiness of itsorigin.
We tested our framework in two domains:Nobel Prize awards and management succession.Evaluations have been conducted to investigate theperformance with respect to the seed parameters:the number of seeds and the influence of data sizeand its redundancy property.
The whole systemhas been evaluated for the two domains consider-ing precision and recall.
We utilize the evaluationstrategy ?Ideal Matrix?
of Agichtein and Gravano(2000) to deal with unannotated test data.The remainder of the paper is organised as fol-lows: Section 2 provides an overview of the systemarchitecture.
Section 3 discusses the rule represen-tation.
In Section 4, a detailed description of theseed-driven bottom-up pattern acquisition is pre-sented.
Section 5 describes our experiments withpattern ranking, filtering and rule induction.
Sec-tion 6 presents the experiments and evaluations forthe two application domains.
Section 7 provides aconclusion and an outline of future work.2 System ArchitectureGiven the framework, our system architecturecan be depicted as follows:Figure 1.
ArchitectureThis architecture has been inspired by severalexisting seed-oriented minimally supervised ma-chine learning systems, in particular by Snowball(Agichtein and Gravano, 2000) and ExDisco(Yangarber et al, 2000).
We call our systemDARE, standing for ?Domain Adaptive RelationExtraction based on Seeds?.
DARE contains fourmajor components: linguistic annotation, classifier,rule learning and relation extraction.
The first com-ponent only applies once, while the last three com-ponents are integrated in a bootstrapping loop.
Ateach iteration, rules will be learned based on theseed and then new relation instances will be ex-tracted by applying the learned rules.
The new re-lation instances are then used as seeds for the nextiteration of the learning cycle.
The cycle termi-nates when no new relations can be acquired.The linguistic annotation is responsible for en-riching the natural language texts with linguisticinformation such as named entities and depend-ency structures.
In our framework, the depth of thelinguistic annotation can be varied depending onthe domain and the available resources.The classifier has the task to deliver relevantparagraphs and sentences that contain seed ele-ments.
It has three subcomponents: document re-585trieval, paragraph retrieval and sentence retrieval.The document retrieval component utilizes theopen source IR-system Lucene2.
A translation stepis built in to convert the seed into the proper IRquery format.
As explained in Xu et al (2006), wegenerate all possible lexical variants of the seedarguments to boost the retrieval coverage and for-mulate a boolean query where the arguments areconnected via conjunction and the lexical variantsare associated via disjunction.
However, the trans-lation could be modified.
The task of paragraphretrieval is to find text snippets from the relevantdocuments where the seed relation arguments co-occur.
Given the paragraphs, a sentence containingat least two arguments of a seed relation will beregarded as relevant.As mentioned above, the rule learning compo-nent constitutes the core of our system.
It identifiespatterns from the annotated documents inducingextraction rules from the patterns, and validatesthem.
In section 4, we will give a detailed expla-nation of this component.The relation extraction component applies thenewly learned rules to the relevant documents andextracts relation instances.
The validated relationinstances will then be used as new seeds for thenext iteration.3 DARE Rule RepresentationOur rule representation is designed to specify thelocation and the role of the arguments w.r.t.
thetarget relation in a linguistic construction.
In ourframework, the rules should not be restricted to aparticular linguistic representation and should beadaptable to various NLP tools on demand.
ADARE rule is allowed to call further DARE rulesthat extract a subset of the arguments.
Let us stepthrough some example rules for the prize awarddomain.
One of the target relations in the domain isabout a person who obtains a special prize in a cer-tain area in a certain year, namely, a quaternarytuple, see (3).
(4) is a domain relevant sentence.
(3) <recipient, prize, area, year>(4) Mohamed ElBaradei won the 2005 NobelPeace Prize on Friday for his efforts to limit thespread of atomic weapons.
(5) is a rule that extracts a ternary projection in-stance <prize, area, year>  from a  noun phrase2 http://www.lucene.decompound, while (6) is a rule which triggers (5) inits object argument and extracts all four arguments.
(5) and (6) are useful rules for  extracting argu-ments from (4).
(5)(6)Next we provide a definition of a DARE rule:A DARE rule has three components1.
rule name: ri;2. output: a set A containing the n argumentsof the n-ary relation, labelled with their ar-gument roles;3. rule body in AVM format containing:- specific linguistic labels or attributes(e.g., subject, object, head, mod), de-rived from the linguistic analysis, e.g.,dependency structures and the named en-tity information- rule: its value is a DARE rule which ex-tracts a subset of arguments of AThe rule in (6) is a typical DARE rule.
Its sub-ject and object descriptions call appropriate DARErules that extract a subset of the output relationarguments.
The advantages of this rule representa-tion strategy are that (1) it supports the bottom-uprule composition; (2) it is expressive enough forthe representation of rules of various complexity;(3) it reflects the precise linguistic relationshipamong the relation arguments and reduces thetemplate merging task in the later phase; (4) therules for the subset of arguments may be reused forother relation extraction tasks.The rule representation models for automatic orunsupervised pattern rule extraction discussed by586Stevenson and Greenwood (2006) do not accountfor these considerations.4 Seed-driven Bottom-up Rule LearningTwo main approaches to seed construction havebeen discussed in the literature: pattern-oriented(e.g., ExDisco) and semantics-oriented (e.g.,Snowball) strategies.
The pattern-oriented methodsuffers from poor coverage because it makes the IEtask too dependent on one linguistic representationconstruction (e.g., subject-verb-object) and hasmoreover ignored the fact that semantic relationsand events could be dispersed over different sub-structures of the linguistic representation.
In prac-tice, several tuples extracted by different patternscan contribute to one complex relation instance.The semantics-oriented method uses relation in-stances as seeds.
It can easily be adapted to all re-lation/event instances.
The complexity of the targetrelation is not restricted by the expressiveness ofthe seed pattern representation.
In Brin (1998) andAgichtein and Gravano (2000),  the semantics-oriented methods have proved to be effective inlearning patterns for some general binary relationssuch as booktitle-author and company-headquarterrelations.
In Xu et al (2006), the authors show thatat least for the investigated task it is more effectiveto start with the most complex relation instance,namely, with an n-ary sample for the target n-aryrelation as seed, because the seed arguments areoften centred in a relevant textual snippet wherethe relation is mentioned.
Given the bottom-upextracted patterns, the task of the rule induction isto cluster and generalize the patterns.
In compari-son to the bottom-up rule induction strategy (Califfand Mooney, 2004), our method works also in acompositional way.
For reasons of space this partof the work will be reported in Xu and Uszkoreit(forthcoming).4.1 Pattern ExtractionPattern extraction in DARE aims to find linguisticpatterns which do not only trigger the relations butalso locate the relation arguments.
In DARE, thepatterns can be extracted from a phrase, a clause ora sentence, depending on the location and the dis-tribution of the seed relation arguments.Figure 2.
Pattern extraction step 1Figure 3.
Pattern extraction step 2Figures 2 and 3 depict the general steps of bot-tom-up pattern extraction from a dependency tree twhere three seed arguments arg1, arg2 and arg3 arelocated.
All arguments are assigned their relationroles r1, r2 and r3.
The pattern-relevant subtrees aretrees in which seed arguments are embedded: t1, t2and t3.
Their root nodes are n1, n2 and n3.
Figure 2shows the extraction of a unary pattern n2_r3_i,while Figure 3 illustrates the further extraction andconstruction of a binary pattern n1_r1_r2_j and aternary pattern n3_r1_r2_r3_k.
In practice, not allbranches in the subtrees will be kept.
In the follow-ing, we give a general definition of our seed-drivenbottom-up pattern extraction algorithm:input:  (i) relation = <r1, r2, ..., rn>: the target rela-tion tuple with n argument roles.T: a set of linguistic analysis trees anno-tated with i seed relation arguments (1?i?n)output: P: a set of pattern instances which can ex-tract i or a subset of i arguments.Pattern extraction:for each tree t ?T587Step 1: (depicted in Figure 2)1. replace all terminal nodes that are instanti-ated with the seed arguments by newnodes.
Label these new nodes with theseed argument roles and possibly the cor-responding entity classes;2. identify the set of the lowest nonterminalnodes N1 in t that dominate only one ar-gument (possibly among other nodes).3. substitute N1 by nodes labelled with theseed argument roles and their entity classes4.
prune the subtrees dominated by N1 from tand add these subtrees into P. These sub-trees are assigned the argument role infor-mation and a unique id.Step2: For i=2 to n: (depicted in Figure 3)1. find the set of the lowest nodes N1 in t thatdominate in addition to other children onlyi seed arguments;2. substitute N1 by nodes labelled with the iseed argument role combination informa-tion (e.g., ri_rj) and with a unique id.3.
prune the subtrees Ti dominated by Ni in t;4. add Ti to P together with the argument rolecombination information and the unique idWith this approach, we can learn rules like (6) ina straightforward way.4.2 Rule Validation: Ranking and FilteringOur ranking strategy has incorporated the ideasproposed by Riloff (1996), Agichtein and Gravano(2000), Yangarber (2003) and Sudo et al (2003).We take two properties of a pattern into account: ?
domain relevance: its distribution in the rele-vant documents and irrelevant documents(documents in other domains);?
trustworthiness of its origin: the relevancescore of the seeds from which it is extracted.In Riloff (1996) and Sudo et al (2003), the rele-vance of a pattern is mainly dependent on its oc-currences in the relevant documents vs. the wholecorpus.
Relevant patterns with lower frequenciescannot float to the top.
It is known that some com-plex patterns are relevant even if they have lowoccurrence rates.
We propose a new method forcalculating the domain relevance of a pattern.
Weassume that the domain relevance of a pattern isdependent on the relevance of the lexical terms(words or collocations) constructing the pattern,e.g., the domain relevance of (5) and (6) are de-pendent on the terms ?prize?
and ?win?
respec-tively.
Given n different domains, the domain rele-vance score (DR) of a term t in a domain di is:DR(t, di)=0, if df(t, di) =0;df(t,di)N?D ?LOG(n?df(t,di)df(t,dj)j=1n?
), otherwisewhere?
df(t, di): is the document frequency of aterm t in the domain di?
D: the number of the documents in di?
N: the total number of the terms in diHere the domain relevance of a term is dependentboth on its document frequency and its documentfrequency distribution in other domains.
Termsmentioned by more documents within the domainthan outside are more relevant (Xu et al, 2002).In the case of n=3 such different domains mightbe, e.g., management succession, book review orbiomedical texts.
Every domain corpus should ide-ally have the same number of documents and simi-lar average document size.
In the calculation of thetrustworthiness of the origin, we follow Agichteinand Gravano (2000) and Yangarber (2003).
Thus,the relevance of a pattern is dependent on the rele-vance of its terms and the score value of the mosttrustworthy seed from which it origins.
Finally, thescore of a pattern p is calculated as follows:score(p)= }:)(max{)(0SeedsssscoretDRTii ??
?=where    |T|> 0 and ti ?
T?
T: is the set of the terms occur in p;?
Seeds: a set of seeds from which the pat-tern is extracted;?
score(s): is the score of the seed s;This relevance score is not dependent on the distri-bution frequency of a pattern in the domain corpus.Therefore, patterns with lower frequency, in par-ticular, some complex patterns, can be rankedhigher when they contain relevant domain terms orcome from reliable seeds.5885 Top down Rule ApplicationAfter the acquisition of pattern rules, the DAREsystem applies these rules to the linguistically an-notated corpus.
The rule selection strategy movesfrom complex to simple.
It first matches the mostcomplex pattern to the analyzed sentence in orderto extract the maximal number of relation argu-ments.
According to the duality principle (Yangar-ber 2001), the score of the new extracted relationinstance S is dependent on the patterns from whichit origins.
Our score method is a simplified versionof that defined by Agichtein and Gravano (2000):score(S)=1?
(1?
score(Pi )i=0P?
)where P={Pi} is the set of patterns that extract S.The extracted instances can be used as potentialseeds for the further pattern extraction iteration,when their scores are validated.
The initial seedsobtain 1 as their score.6 Experiments and EvaluationWe apply our framework to two application do-mains: Nobel Prize awards and management suc-cession events.
Table 1 gives an overview of ourtest data sets.Data Set Name Doc Number Data AmountNobel Prize A  (1999-2005) 2296 12,6 MBNobel Prize B (1981-1998)  1032 5,8 MBMUC-6 199 1 MBTable1.
Overview of Test Data Sets.For the Nobel Prize award scenario, we use twotest data sets with different sizes: Nobel Prize Aand Nobel Prize B.
They are Nobel Prize relatedarticles from New York Times, online BBC andCNN news reports.
The target relation for the ex-periment is a quaternary relation as mentioned in(3), repeated here again:<recipient, prize, area, year>Our test data is not annotated with target rela-tion instances.
However, the entire list of NobelPrize award events is available for the evaluationfrom the Nobel Prize official website3.
We use it asour reference relation database for building ourIdeal table (Agichtein and Gravano, 2000).For the management succession scenario, we usethe test data from MUC-6 (MUC-6, 1995) and de-3 http://nobelprize.org/fine a simpler relation structure than the MUC-6scenario template with four arguments:<Person_In, Person_Out, Position, Organisation>In the following tables, we use PI for Person_In,PO for Person_Out, POS for Position and ORG forOrganisation.
In our experiments, we attempt toinvestigate the influence of the size of the seed andthe size of the test data on the performance.
Allthese documents are processed by named entityrecognition (Drozdzynski et al, 2004) and depend-ency parser MINIPAR (Lin, 1998).6.1 Nobel Prize Domain EvaluationFor this domain, three test runs have been evalu-ated, initialized by one randomly selected relationinstance as seed each time.
In the first run, we usethe largest test data set Nobel Prize A.
In the sec-ond and third runs, we have compared two randomselected seed samples with 50% of the data each,namely Nobel Prize B.
For data sets in this do-main, we are faced with an evaluation challengepointed out by DIPRE (Brin, 1998) and Snowball(Agichtein and Gravano, 2000), because there is nogold-standard evaluation corpus available.
Wehave adapted the evaluation method suggested byAgichtein and Gravano, i.e., our system is success-ful if we capture one mentioning of a Nobel Prizewinner event through one instance of the relationtuple or its projections.
We constructed two tables(named Ideal) reflecting an approximation of themaximal detectable relation instances: one for No-bel Prize A and another for Nobel Prize B. TheIdeal tables contain the Nobel Prize winners thatco-occur with the word ?Nobel?
in the test corpus.Then precision is the correctness of the extractedrelation instances, while recall is the coverage ofthe extracted tuples that match with the Ideal table.In Table 2 we show the precision and recall of thethree runs and their random seed sample:Recall DataSetSeed Preci-sion total time intervalNobelPrize A[Zewail, Ahmed H],nobel, chemistry,199971,6% 50,7% 70,9%(1999-2005)NobelPrize B[Sen, Amartya], no-bel, economics, 199887,3% 31% 43%(1981-1998)NobelPrize B[Arias, Oscar],nobel, peace, 198783,8% 32% 45%(1981-1998)Table 2.
Precision, Recall against the Ideal TableThe first experiment with the full test data hasachieved much higher recall than the two experi-ments with the set Nobel Prize B.
The two experi-ments with the Nobel Prize B corpus show similar589performance.
All three experiments have betterrecalls when taking only the relation instances dur-ing the report years into account, because there aremore mentionings during these years in the corpus.Figure (6) depicts the pattern learning and newseed extracting behavior during the iterations forthe first experiment.
Similar behaviours are ob-served in the other two experiments.Figure 6.
Experiment with Nobel Prize A6.2 Management Succession DomainThe MUC-6 corpus is much smaller than the NobelPrize corpus.
Since the gold standard of the targetrelations is available, we use the standard IE preci-sion and recall method.
The total gold standardtable contains 256 event instances, from which werandomly select seeds for our experiments.
Table 3gives an overview of performance of the experi-ments.
Our tests vary between one seed, 20 seedsand 55 seeds.Initial Seed Nr.
Precision RecallA 12.6% 7.0% 1B 15.1% 21.8%20  48.4%  34.2%55  62.0% 48.0%Table 3.
Results for various initial seed setsThe first two one-seed tests achieved poor per-formance.
With 55 seeds, we can extract additional67 instances to obtain the half size of the instancesoccurring in the corpus.
Table 4 show evaluationsof the single arguments.
B works a little better be-cause the randomly selected single seed appears abetter sample for finding the pattern for extractingPI argument.Arg precision(A)precision(B)Recall(A)Recall(B)PI 10.9% 15.1% 8.6% 34.4%PO 28.6% - 2.3% 2.3%ORG 25.6% 100% 2.6% 2.6%POS 11.2% 11.2% 5.5% 5.5%Table 4.
Evaluation of one-seed tests (A and B)Table 5 shows the performance with 20 and 55seeds respectively.
Both of them are better than theone-seed tests, while 55 seeds deliver the best per-formance in average, in particular, the recall value.arg precision(20)precision(55)recall(20)recall(55)PI 84% 62.8% 27.9% 56.1%PO 41.2% 59% 34.2% 31.2%ORG 82.4% 58.2% 7.4% 20.2%POS 42% 64.8% 25.6% 30.6%Table 5.
Evaluation of 20 and 55 seeds testsOur result with 20 seeds (precision of 48.4% andrecall of 34.2%) is comparable with the best resultreported by Greenwood and Stevenson (2006) withthe linked chain model (precision of 0.434 and re-call of 0.265).
Since the latter model uses patternsas seeds, applying a similarity measure for patternranking, a fair comparison is not possible.
Our re-sult is not restricted to binary relations and ourmodel also assigns the exact argument role to thePerson role, i.e.
Person_In or Person_Out.We have also evaluated the top 100 event-independent binary relations such as Person-Organisation and Position-Organisation.
The preci-sion of these by-product relations of our IE systemis above 98%.7 Conclusion and Future WorkSeveral parameters are relevant for the successof a seed-based bootstrapping approach to relationextraction.
One of these is the arity of the relation.Another one is the locality of the relation instancein an average mentioning.
A third one is the typesof the relation arguments:  Are they  named entitiesin the classical sense?
Are they lexically marked?Are there several arguments of the same type?Both tasks we explored involved extracting quater-nary relations.
The Nobel Prize domain shows bet-ter lexical marking because of the prize name.
Themanagement succession domain has two slots ofthe same NE type, i.e., persons.
These differencesare relevant for any relation extraction approach.The success of the bootstrapping approach cru-cially depends on the nature of the training database.
One of the most relevant properties of thisdata base is the ratio of documents to relation in-stances.
Several independent reports of an instanceusually yield a higher number of patterns.The two tasks we used to investigate our methoddrastically differ in this respect.
The Nobel Prize590domain we selected as a learning domain for gen-eral award events since it exhibits a high degree ofredundancy in reporting.
A Nobel Prize triggersmore news reports than most other prizes.
Theachieved results met our expectations.
With onerandomly selected seed, we could finally extractmost relevant events in some covered time interval.However, it turns out that it is not just the aver-age number of reports per events that matters butalso the distribution of reportings to events.
Sincethe Nobel prizes data exhibit a certain type ofskewed distribution, the graph exhibits propertiesof scale-free graphs.
The distances between eventsare shortened to a few steps.
Therefore, we canreach most events in a few iterations.
The situationis different for the management succession taskwhere the reports came from a single newspaper.The ratio of events to reports is close to one.
Thislack of informational redundancy requires a highernumber of seeds.
When we started the bootstrap-ping with a single event, the results were ratherpoor.
Going up to twenty seeds, we still did notget the performance we obtain in the Nobel Prizetask but our results compare favorably to the per-formance of existing bootstrapping methods.The conclusion, we draw from the observed dif-ference between the two tasks is simple:  We shallalways try to find a highly redundant training dataset.
If at all possible, the training data should ex-hibit a skewed distribution of reports to events.Actually, such training data may be the only realis-tic chance for reaching a large number of rare pat-terns.
In future work we will try to exploit the webas training resource for acquiring patterns whileusing the parsed domain data as the source for ob-taining new seeds in bootstrapping the rules beforeapplying these to any other nonredundant docu-ment base.
This is possible because our seed tu-ples can be translated into simple IR queries andfurther linguistic processing is limited to the re-trieved candidate documents.AcknowledgementThe presented research was partially supported by agrant from the German Federal Ministry of Educationand Research to the project Hylap (FKZ: 01IWF02) andEU?funding for the project RASCALLI.
Our specialthanks go to Doug Appelt and an anonymous reviewerfor their thorough and highly valuable comments.ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: extract-ing relations from large plain-text collections.
InACM 2000, pages 85?94, Texas, USA.S.
Brin.
Extracting patterns and relations from theWorld-Wide Web.
In Proc.
1998 Int'l Workshop onthe Web and Databases (WebDB '98), March 1998.M.
E. Califf and R. J. Mooney.
2004.
Bottom-Up Rela-tional Learning of Pattern Matching Rules for Infor-mation Extraction.
Journal of Machine Learning Re-search, MIT Press.W.
Drozdzynski,  H.-U.Krieger, J.  Piskorski; U. Sch?-fer, and F. Xu.
2004.
Shallow Processing with Unifi-cation and Typed Feature Structures ?
Foundationsand Applications.
K?nstliche Intelligenz 1:17?23.M.
A. Greenwood and M. Stevenson.
2006.
ImprovingSemi-supervised Acquisition of Relation ExtractionPatterns.
In Proc.
of the Workshop on InformationExtraction Beyond  the Document, Australia.D.
Lin.
1998.
Dependency-based evaluation of  MINI-PAR.
In Workshop on the Evaluation of Parsing Sys-tems, Granada, Spain.MUC.
1995.
Proceedings of the Sixth Message Under-standing Conference (MUC-6), Morgan Kaufmann.E.
Riloff.
1996.
Automatically Generating ExtractionPatterns from Untagged Text.
In Proc.
of the Thir-teenth National Conference on Articial Intelligence,pages 1044?1049, Portland, OR, August.M.
Stevenson and Mark A. Greenwood.
2006.
Compar-ing Information Extraction Pattern Models.
In Proc.of the Workshop on Information Extraction Beyondthe Document, Sydney, Australia.K.
Sudo, S. Sekine, and R. Grishman.
2003.
An Im-proved Extraction Pattern Representation Model forAutomatic IE Pattern Acquisition.
In Proc.
of ACL-03, pages 224?231, Sapporo, Japan.R.
Yangarber, R. Grishman, P. Tapanainen, and S. Hut-tunen.
2000.
Automatic Acquisition of DomainKnowledge for Information Extraction.
In Proc.
ofCOLING 2000, Saarbr?cken, Germany.R.
Yangarber.
2003.
Counter-training in the Discoveryof Semantic Patterns.
In Proceedings of ACL-03,pages 343?350, Sapporo, Japan.F.
Xu, D. Kurz, J. Piskorski and S. Schmeier.
2002.
ADomain Adaptive Approach to Automatic Acquisitionof Domain Relevant Terms and their Relations withBootstrapping.
In Proc.
of LREC 2002, May 2002.F.
Xu, H. Uszkoreit and H. Li.
2006.
Automatic Eventand Relation Detection with Seeds of Varying Com-plexity.
In Proceedings of AAAI 2006 WorkshopEvent Extraction and Synthesis, Boston, July, 2006.591
