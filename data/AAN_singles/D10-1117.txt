Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1204?1213,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsUnsupervised Induction of Tree Substitution Grammarsfor Dependency ParsingPhil BlunsomComputing LaboratoryUniversity of OxfordPhil.Blunsom@comlab.ox.ac.ukTrevor CohnDepartment of Computer ScienceUniversity of SheffieldT.Cohn@dcs.shef.ac.ukAbstractInducing a grammar directly from text isone of the oldest and most challenging tasksin Computational Linguistics.
Significantprogress has been made for inducing depen-dency grammars, however the models em-ployed are overly simplistic, particularly incomparison to supervised parsing models.
Inthis paper we present an approach to depen-dency grammar induction using tree substi-tution grammar which is capable of learn-ing large dependency fragments and therebybetter modelling the text.
We define a hi-erarchical non-parametric Pitman-Yor Processprior which biases towards a small grammarwith simple productions.
This approach sig-nificantly improves the state-of-the-art, whenmeasured by head attachment accuracy.1 IntroductionGrammar induction is a central problem in Compu-tational Linguistics, the aim of which is to inducelinguistic structures from an unannotated text cor-pus.
Despite considerable research effort this un-supervised problem remains largely unsolved, par-ticularly for traditional phrase-structure parsing ap-proaches (Clark, 2001; Klein and Manning, 2002).Phrase-structure parser induction is made difficultdue to two types of ambiguity: the constituent struc-ture and the constituent labels.
In particular the con-stituent labels are highly ambiguous, firstly we don?tknow a priori how many there are, and secondly la-bels that appear high in a tree (e.g., an S categoryfor a clause) rely on the correct inference of all thelatent labels below them.
However recent work onthe induction of dependency grammars has provedmore fruitful (Klein and Manning, 2004).
Depen-dency grammars (Mel?c?uk, 1988) should be easier toinduce from text compared to phrase-structure gram-mars because the set of labels (heads) are directlyobserved as the words in the sentence.Approaches to unsupervised grammar induction,both for phrase-structure and dependency grammars,have typically used very simplistic models (Clark,2001; Klein and Manning, 2004), especially in com-parison to supervised parsing models (Collins, 2003;Clark and Curran, 2004; McDonald, 2006).
Sim-ple models are attractive for grammar induction be-cause they have a limited capacity to overfit, how-ever they are incapable of modelling many knownlinguistic phenomena.
We posit that more complexgrammars could be used to better model the unsuper-vised task, provided that active measures are takento prevent overfitting.
In this paper we present anapproach to dependency grammar induction usinga tree-substitution grammar (TSG) with a Bayesiannon-parametric prior.
This allows the model to learnlarge dependency fragments to best describe the text,with the prior biasing the model towards fewer andsmaller grammar productions.We adopt the split-head construction (Eisner,2000; Johnson, 2007) to map dependency parses tocontext free grammar (CFG) derivations, over whichwe apply a model of TSG induction (Cohn et al,2009).
The model uses a hierarchical Pitman-Yorprocess to encode a backoff path from TSG to CFGrules, and from lexicalised to unlexicalised rules.Our best lexicalised model achieves a head attach-ment accuracy of of 55.7% on Section 23 of the WSJdata set, which significantly improves over state-of-the-art and far exceeds an EM baseline (Klein andManning, 2004) which obtains 35.9%.1204CFG Rule DMV Distribution DescriptionS?
LH HR p(root = H) The head of the sentence is H .LH ?
Hl p(STOP |dir = L, head = H, val = 0) H has no left children.LH ?
L1H p(CONT |dir = L, head = H, val = 0) H has at least one left child.L?H ?
Hl p(STOP |dir = L, head = H, val = 1) H has no more left children.L?H ?
L1H p(CONT |dir = L, head = H, val = 1) H has another left child.HR?
Hr p(STOP |dir = R, head = H, val = 0) H has no right children.HR?
HR1 p(CONT |dir = R, head = H, val = 0) H has at least one right child.HR?
?
Hr p(STOP |dir = R, head = H, val = 1) H has no more right children.HR?
?
HR1 p(CONT |dir = R, head = H, val = 1) H has another right child.L1H ?
LC CMH?
p(C|dir = L, head = H) C is a left child of H .HR1?
H?MC CR p(C|dir = R, head = H) C is a right child of H .CMH?
?
CR L?H p = 1 UnambiguousH?MC ?
HR?
LC p = 1 UnambiguousTable 1: The CFG-DMV grammar schema.
Note that the actual CFG is created by instantiating these templates withpart-of-speech tags observed in the data for the variables H and C. Valency (val) can take the value 0 (no attachmentin the direction (dir) d) and 1 (one or more attachment).
L and R indicates child dependents left or right of the parent;superscripts encode the stopping and valency distributions, X1 indicates that the head will continue to attach morechildren and X?
that it has already attached a child.2 BackgroundThe most successful framework for unsuperviseddependency induction is the Dependency Modelwith Valence (DMV) (Klein and Manning, 2004).This model has been adapted and extended by anumber of authors and currently represents the state-of-the-art for dependency induction (Cohen andSmith, 2009; Headden III et al, 2009).
Eisner(2000) introduced the split-head algorithm whichpermits efficient O(|w|3) parsing complexity byreplicating (splitting) each terminal and processingleft and right dependents separately.
We employthe related fold-unfold representation of Johnson(2007) that defines a CFG equivalent of the split-head parsing algorithm, allowing us to easily adaptCFG-based grammar models to dependency gram-mar.
Table 1 shows the equivalent CFG grammar forthe DMV model (CFG-DMV) using the unfold-foldtransformation.
The key insight to understanding thenon-terminals in this grammar is that the subscriptsencode the terminals at the boundaries of the spanof that non-terminal.
For example the non-terminalLH encodes that the right most terminal spannedby this constituent is H (and the reverse for HR),while AMB encodes that A and B are the left-mostand right-most terminals of the span.
The ?
and 1superscripts are used to encode the valency of thehead, both indicate that the head has at least oneattached dependent in the specified direction.
Thisgrammar allows O(|w|3) parsing complexity whichfollows from the terminals of the dependency treebeing observed, such that each span of the parsechart uniquely specifies its possible heads (either theleftmost, rightmost or both) and therefore the num-ber of possible non-terminals for each span is con-stant.
The transform is illustrated in figures 1a and1c which show the CFG tree for an example sentenceand the equivalent dependency tree.Normally DMV based models have been trainedon part-of-speech tags of the words in a sentence,rather than the words themselves.
Headden III et al(2009) showed that performance could be improvedby including high frequency words as well as tagsin their model.
In this paper we refer to such mod-els as lexicalised; words which occur more than onehundred times in the training corpus are representedby a word/tag pair, while those less frequent are rep-resented simply by their tags.
We are also able toshow that this basic approach to lexicalisation im-proves the performance of our models.1205SLhates[V ]L1hates[V ]LNNlNMhates[V]?NRNrL?hates[V ]hates[V]lhates[V ]Rhates[V ]R1hates[V]?MNhates[V ]R?hates[V]rLNNlNRNr(a) A TSG-DMV derivation for the sentence George hates broc-coli.
George and broccoli occur less than the lexicalisation cutoffand are thus represented by the part-of-speech N, while hates iscommon and therefore is represented by a word/tag pair.
Boldnodes indicate frontier nodes of elementary trees.SLhates[V ]L1hates[V ]LN NMhates[V]?hates[V ]Rhates[V ]R1hates[V]?MN NR(b) A TSG-DMV elementary rule from Figure 1a.
This rule en-codes a dependency between the subject and object of hates thatis not present in the CFG-DMV.
Note that this rule doesn?t re-strict hates, or its arguments, to having a single left and rightchild.
More dependents can be inserted using additional rulesbelow the M/L/R frontier non-terminals.George hates broccoli ROOT(c) A traditional dependency tree representation of the parse treein Figure 1a before applying the lexicalisation cutoff.Figure 1: TSG-DMV representation of dependency trees.3 Lexicalised TSG-DMVThe models we investigate in this paper build uponthe CFG-DMV by defining a Tree SubstitutionGrammar (TSG) over the space of CFG rules.
ATSG is a 4-tuple,G = (T,N, S,R), where T is a setof terminal symbols, N is a set of non-terminal sym-bols, S ?
N is the distinguished root non-terminaland R is a set of productions (rules).
The produc-tions take the form of elementary trees ?
tree frag-ments of height ?
1, where each internal node islabelled with a non-terminal and each leaf is la-belled with either a terminal or a non-terminal.
Non-terminal leaves are called frontier non-terminals andform the substitution sites in the generative processof creating trees with the grammar.A derivation creates a tree by starting with theroot symbol and rewriting (substituting) it with anelementary tree, then continuing to rewrite fron-tier non-terminals with elementary trees until thereare no remaining frontier non-terminals.
We canrepresent derivations as sequences of elementarytrees, e, by specifying that during the generation ofthe tree each elementary tree is substituted for theleft-most frontier non-terminal.
Figure 1a shows aTSG derivation for the dependency tree in Figure 1cwhere bold nonterminal labels denote substitutionsites (root/frontier nodes in the elementary trees).The probability of a derivation, e, is the productof the probabilities of its component rules,P (e) =?c?e?eP (e|c) .
(1)where each rewrite is assumed conditionally inde-pendent of all others given its root nonterminal, c =root(e).
The probability of a tree, t, and string ofwords, w, areP (t) =?e:tree(e)=tP (e) and P (w) =?t:yield(t)=wP (t) ,respectively, where tree(e) returns the tree for thederivation e and yield(t) returns the string of termi-nal symbols at the leaves of t.A Probabilistic Tree Substitution Grammar(PTSG), like a PCFG, assigns a probability to eachrule in the grammar, denoted P (e|c).
The probabil-ity of a derivation, e, is the product of the proba-bilities of its component rules.
Estimating a PTSGrequires learning the sufficient statistics for P (e|c)in (1) based on a training sample.
Parsing involves1206finding the most probable tree for a given string(argmaxt P (t|w)).
This is typically approximatedby finding the most probable derivation which canbe done efficiently using the CYK algorithm.3.1 ModelIn this work we propose the Tree Substitution Gram-mar Dependency Model with Valence (TSG-DMV).We define a hierarchical non-parametric TSG modelon the space of parse trees licensed by the CFGgrammar in Table 1.
Our model is a generalisa-tion of that of Cohn et al (2009) and Cohn et al(2011).
We extend those works by moving from asingle level Dirichlet Process (DP) distribution overrules to a multi-level Pitman-Yor Process (PYP), andincluding lexicalisation.
The PYP has been shownto generate distributions particularly well suited tomodelling language (Teh, 2006; Goldwater et al,2006).
Teh (2006) used a hierarchical PYP to modelbackoff in language models, we leverage this samecapability to model backoff in TSG rules.
This ef-fectively allows smoothing from lexicalised to un-lexicalised grammars, and from TSG to CFG rules.Here we describe our deepest model which hasa four level hierarchy, depicted graphically in Table2.
In Section 5 we evaluate different subsets of thishierarchy.
The topmost level of our model describeslexicalised elementary elementary fragments (e) asproduced by a PYP,e|c ?
GcGc|ac, bc,Plcfg ?
PYP(ac, bc,Plcfg(?|c)) ,where ac and bc control the strength of the backoffdistribution Plcfg.
The space of lexicalised TSG ruleswill inevitably be very sparse, so the base distribu-tion Plcfg backs-off to calculating the probability ofa TSG rules as the product of the CFG rules it con-tains, multiplied by a geometric distribution over thesize of the rule.Plcfg(e|c) =?f?F(e)sfc?i?I(e)(1?
sic)?A(lex-cfg-rules(e|c))?|c ?
AcAc|alcfgc , blcfgc ,Pcfg ?
PYP(alcfgc , blcfgc ,Pcfg(?|c)),where I(e) are the set of internal nodes in e exclud-ing the root, F (e) are the set of frontier non-terminalnodes, and ci is the non-terminal symbol for nodei and sc is the probability of stopping expanding anode labelled c. The function lex-cfg-rules(e|c) re-turns the CFG rules internal to e, each of the formc?
?
?
; each CFG rule is drawn from the back-off distribution, Ac?
.
We treat sc as a parameterwhich is estimated during training, as described inSection 4.2.The next level of backoff (Pcfg) removes the lexi-calisation from the CFG rules, describing the gener-ation of a lexicalised rule by first generating an un-lexicalised rule from a PYP, then generating the lex-icalisaton from a uniform distribution over words:1Pcfg(?|c) = B(unlex(?)|unlex(c))?1|w||?|??|c?
?
Bc?Bc?
|acfgc?
, bcfgc?
,Psh ?
PYP(acfgc?
, bcfgc?
,Psh(?|c?
)),where unlex(?)
removes the lexicalisation from non-terminals leaving only the tags.The final base distribution over CFG-DMV rules(Psh) is inspired by the skip-head smoothing modelof Headden III et al (2009).
This model showed thatsmoothing the DMV by removing the heads from theCFG rules significantly improved performance.
Wereplicate this behavior through a final level in our hi-erarchy which generates the CFG rules without theirheads, then generates the heads from a uniform dis-tribution:Psh(?|c) = C(drop-head(c?
?
))?1|P |?|c ?
CcCc|ashc , bshc ?
PYP(ashc , bshc ,Uniform(?|c)),where drop-head(?)
removes the symbols that markthe head on the CFG rules, and P is the set of part-of-speech tags.
Each stage of backoff is illustrated inTable 2, showing the rules generated from the TSGelementary tree in Figure 1b.Note that while the supervised model of Cohn etal.
(2009) used a fixed back-off PCFG distribution,this model implicitly infers this distribution within1All unlexicalised words are actually given the generic UNKsymbol as their lexicalisation.1207Plcfg Pcfg PshSLhates[V ] hates[V ]RLhates[V ]L1hates[V ]SLV V RLVL1VSL?
?RL?L1?L1hates[V ]LN NMhates[V]?hates[V ]Rhates[V ]R1L1VLN NMV ?V RV R1L1?LN NM??
?R?R1hates[V ]R1hates[V]?MN NRV R1V ?MN NR?R1?
?MN NRTable 2: Backoff trees for the elementary tree in Figure 1b.its hierarchy, essentially learning the DMV modelembedded in the TSG.In this application to dependency grammar ourmodel is capable of learning tree fragments whichgroup CFG parameters.
As such the model can learnto condition dependency links on the valence, e.g.
bycombining LH ?
L1H and L1H ?
LC CMH?
rulesinto a single fragment the model can learn a pa-rameter that the leftmost child of H is C. By link-ing together multiple L1H or HR1 non-terminals themodel can learn groups of dependencies that occurtogether, e.g.
tree fragments representing the com-plete preferred argument frame of a verb.4 Inference4.1 TrainingTo train our model we use Markov Chain MonteCarlo sampling (Geman and Geman, 1984).
Whereprevious supervised TSG models (Cohn et al, 2009)permit an efficient local sampler, the lack of an ob-served parse tree in our unsupervised model makesthis sampler not applicable.
Instead we use a re-cently proposed blocked Metroplis-Hastings (MH)sampler (Cohn and Blunsom, 2010) which exploits afactorisation of the derivation probabilities such thatwhole trees can be sampled efficiently.
See Cohnand Blunsom (2010) for details.
That algorithm isapplied using a dynamic program over an observedtree, the generalisation to our situation of an insidepass over the space of all trees is straightforward.A final consideration is the initialisation of thesampler.
Klein and Manning (2004) emphasised theimportance of the initialiser for achieving good per-formance with their model.
We employ the sameharmonic initialiser as described in that work.
Theinitial derivations for our sampler are the Viterbiderivations under the CFG parameterised accordingto this initialiser.4.2 Sampling hyperparametersWe treat the hyper-parameters {(axc , bxc , sc) , c ?
N}as random variables in our model and infer their val-ues during training.
We choose quite vague priorsfor each hyper-parameter, encoding our lack of in-formation about their values.We place prior distributions on the PYP discountac and concentration bc hyperparamters and sam-ple their values using a slice sampler.
We use therange doubling slice sampling technique of (Neal,2003) to draw a new sample of a?c from its condi-tional distribution.2 For the discount parameters acwe employ a uniform Beta distribution, as we haveno strong prior knowledge of what its value shouldbe (ac ?
Beta(1, 1)).
Similarly, we treat the concen-tration parameters, bc, as being generated by a vaguegamma prior, bc ?
Gamma(1, 1), and sample a newvalue b?c using the same slice-sampling approach asfor ac:P (bc|z) ?
P (z|bc)?
Gamma(bc|1, 1).2We made use of the slice sampler included inMark Johnson?s Adaptor Grammar implementationhttp://www.cog.brown.edu/?mj/Software.htm.1208Corpus Words SentencesSections 2-21 (|x| ?
10) 42505 6007Section 22 (|x| ?
10) 1805 258Section 23 (|x| ?
10) 2649 398Section 23 (|x| ?
?)
49368 2416Table 3: Corpus statistics for the training and testing datafor the TSG-DMV model.
All models are trained on thegold standard part-of-speech tags after removing punctu-ation.We use a vague Beta prior for the stopping probabil-ities in Plcfg, sc ?
Beta(1, 1).All the hyper-parameters are resampled after ev-ery 10th sample of the corpus derivations.4.3 ParsingUnfortunately finding the maximising parse tree fora string under our TSG-DMV model is intractabledue to the inter-rule dependencies created by thePYP formulation.
Previous work has used MonteCarlo techniques to sample for one of the maxi-mum probability parse (MPP), maximum probabil-ity derivation (MPD) or maximum marginal parse(MMP) (Cohn et al, 2009; Bod, 2006).
We take asimpler approach and use the Viterbi algorithm tocalculate the MPD under an approximating TSG de-fined by the last set of derivations sampled for thecorpus during training.
Our results indicate that thisis a reasonable approximation, though the experi-ence of other researchers suggests that calculatingthe MMP under the approximating TSG may alsobe beneficial for DMV (Cohen et al, 2008).5 ExperimentsWe follow the standard evaluation regime for DMVstyle models by performing experiments on the textof the WSJ section of the Penn.
Treebank (Marcus etal., 1993) and reporting head attachment accuracy.Like previous work we pre-process the training andtest data to remove punctuation, training our unlex-icalised models on the gold-standard part-of-speechtags, and including words occurring more than 100times in our lexicalised models (Headden III et al,2009).
It is very difficult for an unsupervised modelto learn from long training sentences as they containa great deal of ambiguity, therefore the majority ofDMV based models have been trained on sentencesrestricted in length to ?
10 tokens.3 This has theadded benefit of decreasing the runtime for exper-iments.
We present experiments with this trainingscenario.
The training data comes from sections 2-21, while section 23 is used for evaluation.
An ad-vantage of our sampling based approach over pre-vious work is that we infer all the hyperparameters,as such we don?t require the use of section 22 fortuning the model.The models are evaluated in terms of head attach-ment accuracy (the percentage of correctly predictedhead indexes for each token in the test data), on twosubsets of the testing data.
Although we can arguethat unsupervised models are better learnt from shortsentences, it is much harder to argue that we don?tthen need to be able to parse long sentences with atrained model.
The most commonly employed testset mirrors the training data by only including sen-tences ?
10.
In this work we focus on the accuracyof our models on the whole of section 23, withoutany pruning for length.
The training and testing cor-pora statistics are presented in Table 3.
Subsequentto the evaluation reported in Table 4 we use section22 to report the correlation between heldout accu-racy and the model log-likelihood (LLH) for ana-lytic purposes.As we are using a sampler during training, the re-sult of any single run is non-deterministic and willexhibit a degree of variance.
All our reported resultsare the mean and standard deviation (?)
from fortysampling runs.5.1 DiscussionTable 4 shows the head attachment accuracy resultsfor our TSG-DMV, plus many other significant pre-viously proposed models.
The subset of hierarchicalpriors used by each model is noted in brackets.The performance of our models is extremely en-couraging, particularly the fact that it achieves thehighest reported accuracy on the full test set by aconsiderable margin.
On the |w| ?
10 test set al theTSG-DMVs are second only to the L-EVG modelof Headden III et al (2009).
The L-EVG modelextends DMV by adding additional lexicalisation,3See Spitkovsky et al (2010a) for an exception to this rule.1209Directed AttachmentAccuracy on WSJ23Model |w| ?
10 |w| ?
?Attach-Right 38.4 31.7EM (Klein and Manning, 2004) 46.1 35.9Dirichlet (Cohen et al, 2008) 46.1 36.9LN (Cohen et al, 2008) 59.4 40.5SLN, TIE V&N (Cohen and Smith, 2009) 61.3 41.4DMV (Headden III et al, 2009) 55.7?=8.0 -DMV smoothed (Headden III et al, 2009) 61.2?=1.2 -EVG smoothed (Headden III et al, 2009) 65.0?=5.7 -L-EVG smoothed (Headden III et al, 2009) 68.8?=4.5 -Less is More (Spitkovsky et al, 2010a) 56.2 44.1Leap Frog (Spitkovsky et al, 2010a) 57.1 45.0Viterbi EM (Spitkovsky et al, 2010b) 65.3 47.9Hypertext Markup (Spitkovsky et al, 2010c) 69.3 50.4Adaptor Grammar (Cohen et al, 2010) 50.2 -TSG-DMV (Pcfg) 65.9?=2.4 53.1?=2.4TSG-DMV (Pcfg, Psh) 65.1?=2.2 51.5?=2.0LexTSG-DMV (Plcfg, Pcfg) 67.2?=1.4 55.2?=2.2LexTSG-DMV (Plcfg, Pcfg, Psh) 67.7?=1.5 55.7?=2.0Supervised MLE (Cohen and Smith, 2009) 84.5 68.8Table 4: Mean and variance for the head attachment accu-racy of our TSG-DMV models (highlighted) with varyingbackoff paths, and many other high performing models.Citations indicate where the model and result were re-ported.
Our models labelled TSG used an unlexicalisedtop level Gc PYP, while those labelled LexTSG used thefull lexicalised Gc.valency conditioning, interpolated back-off smooth-ing and a random initialiser.
In particular Head-den III et al (2009) shows that the random initialiseris crucial for good performance, however this ini-tialiser requires training 1000 models to select a sin-gle best model for evaluation and results in consider-able variance in test set performance.
Note also thatour model exhibits considerably less variance thanthose induced using this random initialiser, suggest-ing that the combination of the harmonic initialiserand blocked-MH sampling may be a more practica-ble training regime.The recently proposed Adaptor Grammar DMVmodel of Cohen et al (2010) is similar in manyway to our TSG model, incorporating a Pitman Yorprior over units larger than CFG rules.
As such itis surprising that our model is performing signif-icantly better than this model.
We can identify anumber of differences that may impact these results:the Adaptor Grammar model is trained using vari-ational inference with the space of tree fragmentstruncated, while we employ a sampler which cannominally explore the full space of tree fragments;and the adapted tree fragments must be completesubtrees (i.e.
they don?t contain variables), whereasour model can make use of arbitrary tree fragments.An interesting avenue for further research would beto extend the variational algorithm of Cohen et al(2010) to our TSG model, possibly speeding infer-ence and allowing easier parallelisation.In Figure 2a we graph the model LLH on the train-ing data versus the head attachment accuracy on theheldout set.
The graph was generated by running160 models for varying numbers of samples andevaluating their accuracy.
This graph indicates thatthe improvements in the posterior probability of themodel are correlated with the evaluation, though thecorrelation is not as high as we might require in or-der to use LLH as a model selection criteria similarto Headden III et al (2009).
Further refinements tothe model could improve this correlation.The scaling perfomance of the model as the num-ber of samples is increased is shown in Figure 2b.Performance improves as the training data is sam-pled for longer, and continues to trend upwards be-yond 1000 samples (the point for which we?ve re-ported results in Table 4).
This suggests that longersampling runs ?
and better inference techniques ?could yield further improvements.For further analysis Table 5 shows the accuracyof the model at predicting the head for frequenttypes, while Table 6 shows the performance on de-pendencies of various lengths.
We emphasise thatthese results are for the single best performing sam-pler run on the heldout corpus and there is consid-erable variation in the analyses produced by eachsampler.
Unsurprisingly, the model appears to bemore accurate when predicting short dependencies,a result that is also reflected in the per type accura-cies.
The model is relatively good at identifying theroot verb in each sentence, especially those headedby past tense verbs (VBD, was), and to a lesser de-gree VBPs (are).
Conjunctions such as and posea particular difficulty when evaluating dependencymodels as the correct modelling of these remains a1210llll llll lllllllllll llllllllllllllllll lllllllll lllllllllllllllllllllll llllllllllllllllllllllllll lllllllllllll llllllllllllll lllll lllll lllllllllll lllllllllllllll?5.4 ?5.3 ?5.2 ?5.1 ?5.0 ?4.9 ?4.856586062646668Perplexity vs.
Accuracy CorrelationPYP.LLHDirected.Attachment.Accuracy(a) Correlation (R2 = 0.2) between the training LLH of thePYP Model and heldout directed head attachment accuracy(WSJ Section 22, |w| ?
10) for LexTSG-DMV (Plcfg, Pcfg, Psh).0 500 1000 1500 20005960616263646566Number of Samples vs. AccuracySamplesDirectedAttachmentAccuracylllll(b) Mean heldout directed head attachment accuracy (WSJ Sec-tion 22, |w| ?
10) versus the number of samples used duringtraining for LexTSG-DMV (Plcfg, Pcfg, Psh).Figure 2contentious linguistic issue and it?s not clear whatthe ?correct?
analysis should be.
Our model gets arespectable 75% accuracy for and conjunctions, butfor conjunctions (CC) as a whole, the model per-forms poorly (39%).Table 7 list the most frequent TSG rules lexi-calised with has.
The most frequent rule is sim-ply the single level equivalent of the DMV termi-nal rule for has.
Almost as frequent is rule 3, herethe grammar incorporates the terminal into a largerelementary fragment, encoding that it is the headof the past participle occuring immediately to it?sright.
This shows the model?s ability to learn theverb?s argument position conditioned on both thehead and child type, something lacking in DMV.Rule 7 further refines this preferred analysis for hasbeen by lexicalising both the head and child.
Rules(4,5,8,10) employ similar conditioning for properand ordinary nouns heading noun phrases to theleft of has.
We believe that it is the ability of theTSG to encode stronger constraints on argument po-sitions that leads to the model?s higher accuracyon longer sentences, while other models do wellon shorter sentences but relatively poorly on longerones (Spitkovsky et al, 2010c).6 ConclusionIn this paper we have made two significant contri-butions to probabilistic modelling and grammar in-duction.
We have shown that it is possible to suc-cessfully learn hierarchical Pitman-Yor models thatencode deep and complex backoff paths over highlystructured latent spaces.
By applying these modelsto the induction of dependency grammars we havealso been able to advance the state-of-the-art, in-creasing the head attachment accuracy on section 23of the Wall Street Journal Corpus by more than 5%.Further gains in performance may come from anexploration of the backoff paths employed within themodel.
In particular more extensive experimentationwith alternate priors and larger training data may al-low the removal of the lexicalisation cutoff which iscurrently in place to counter sparsity.We envisage that in future many grammar for-malisms that have been shown to be effective in su-pervised parsing, such as categorial, unification andtree adjoining grammars, will prove amenable tounsupervised induction using the hierarchical non-parametric modelling approaches we have demon-strated in this paper.1211Count LexTSG-DMV Rules1 94 L?has?V BZ ?
(L?has?V BZ has-VBZl)2 74 L1has?V BZ ?
(L1has?V BZ (LNN L1NN ) NNMhas?V BZ?
)3 71 has?V BZ?MV BN ?
(has?V BZ?MV BN (has?V BZR?
has-VBZr) LV BN )4 54 NNMhas?V BZ?
?
(NNMhas?V BZ?
NNR (L?has?V BZ has-VBZl))5 36 NNMhas?V BZ?
?
(NNMhas?V BZ?
NNR L?has?V BZ)6 36 has?V BZR?
?
(has?V BZR?
(has?V BZR1 has?V BZ?MV BN (V BNR VBNr)))7 30 has?V BZ?Mbeen?V BN ?
(has?V BZ?Mbeen?V BN (has?V BZR?
has-VBZr) Lbeen?V BN )8 27 NNPMhas?V BZ?
?
(NNPMhas?V BZ?
NNPR (L?has?V BZ has-VBZl))9 25 has?V BZR ?
(has?V BZR (has?V BZR1 has?V BZ?MNNS (NNSR NNSR1)))10 18 L1has?V BZ ?
(L1has?V BZ LNNP NNPMhas?V BZ?
)Table 7: The ten most frequent LexTSG-DMV rules in a final training sample that contain has.ReferencesRens Bod.
2006.
An all-subtrees approach to unsuper-vised parsing.
In Proc.
of the 44th Annual Meeting ofthe ACL and 21st International Conference on Compu-tational Linguistics (COLING/ACL-2006), pages 865?872, Sydney, Australia, July.Stephen Clark and James R. Curran.
2004.
Parsing theWSJ using CCG and log-linear models.
In Proc.
of the42nd Annual Meeting of the ACL (ACL-2004), pages103?110, Barcelona, Spain.Alexander Clark.
2001.
Unsupervised induction ofstochastic context-free grammars using distributionalclustering.
In ConLL ?01: Proceedings of the 2001workshop on Computational Natural Language Learn-ing, pages 1?8.
Association for Computational Lin-guistics.Shay B. Cohen and Noah A. Smith.
2009.
Shared lo-gistic normal distributions for soft parameter tying inunsupervised grammar induction.
In NAACL ?09: Pro-ceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages74?82, Morristown, NJ, USA.
Association for Com-putational Linguistics.Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.2008.
Logistic normal priors for unsupervised prob-abilistic grammar induction.
In Daphne Koller, DaleSchuurmans, Yoshua Bengio, and Lon Bottou, editors,NIPS, pages 321?328.
MIT Press.Shay B. Cohen, David M. Blei, and Noah A. Smith.2010.
Variational inference for adaptor grammars.In Human Language Technologies: The 11th AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics.Trevor Cohn and Phil Blunsom.
2010.
Blocked inferencein Bayesian tree substitution grammars.
In Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, page To Appear, Uppsala,Sweden.Trevor Cohn, Sharon Goldwater, and Phil Blunsom.2009.
Inducing compact but accurate tree-substitutiongrammars.
In NAACL ?09: Proceedings of HumanLanguage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics on ZZZ, pages 548?556,Morristown, NJ, USA.
Association for ComputationalLinguistics.Trevor Cohn, Phil Blunsom, and Sharon Goldwater.2011.
Inducing tree-substitution grammars.
Journalof Machine Learning Research.
To Appear.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Linguis-tics, 29(4):589?637.Jason Eisner.
2000.
Bilexical grammars and their cubic-time parsing algorithms.
In Harry Bunt and AntonNijholt, editors, Advances in Probabilistic and OtherParsing Technologies, pages 29?62.
Kluwer AcademicPublishers, October.Stuart Geman and Donald Geman.
1984.
Stochastic re-laxation, Gibbs distributions and the Bayesian restora-tion of images.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 6:721?741.Sharon Goldwater, Tom Griffiths, and Mark John-son.
2006.
Interpolating between types and tokensby estimating power-law generators.
In Y. Weiss,B.
Scho?lkopf, and J. Platt, editors, Advances in NeuralInformation Processing Systems 18, pages 459?466.MIT Press, Cambridge, MA.William P. Headden III, Mark Johnson, and David Mc-Closky.
2009.
Improving unsupervised dependencyparsing with richer contexts and smoothing.
In Pro-ceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages101?109, Boulder, Colorado, June.1212Child Tag Predicted AccuracyHead Correct (%)NN 181 0.64NNP 130 0.71DT 127 0.87NNS 108 0.72VBD 108 0.81JJ 106 0.80IN 81 0.55RB 65 0.61PRP 64 0.97VBZ 47 0.80VBN 36 0.86VBP 30 0.77CD 26 0.23VB 25 0.68the 42 0.88was 29 0.97The 25 0.83of 18 0.78a 18 0.90to 17 0.50in 16 0.89is 15 0.79n?t 15 0.83were 12 0.86are 11 0.92It 11 1.00for 9 0.64and 9 0.75?s 9 1.00Table 5: Per tag type predicted count and accuracy,for the most frequent 15 un/lexicalised tokens on theWSJ Section 22 |w| ?
10 heldout set (LexTSG-DMV(Plcfg,Pcfg,Psh)).Mark Johnson.
2007.
Transforming projective bilexicaldependency grammars into efficiently-parsable CFGswith unfold-fold.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Linguis-tics, pages 168?175, Prague, Czech Republic, June.Association for Computational Linguistics.Dan Klein and Christopher D. Manning.
2002.
A gener-ative constituent-context model for improved grammarinduction.
In Proceedings of 40th Annual Meeting ofthe Association for Computational Linguistics, pages128?135, Philadelphia, Pennsylvania, USA, July.
As-sociation for Computational Linguistics.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: models of de-pendency and constituency.
In ACL ?04: ProceedingsDistance Precision Recall F11 0.70 0.75 0.722 0.70 0.62 0.653 0.66 0.62 0.644 0.56 0.56 0.565 0.53 0.49 0.516 0.59 0.66 0.627 0.50 0.44 0.478 0.57 0.33 0.429 0.67 0.40 0.5010 1.00 0.17 0.29Table 6: Link distance precision, recall and f-score, onthe WSJ Section 22 |w| ?
10 heldout set.of the 42nd Annual Meeting on Association for Com-putational Linguistics, page 478.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: the Penn treebank.
ComputationalLinguistics, 19(2):313?330.Ryan McDonald.
2006.
Discriminative Training andSpanning Tree Algorithms for Dependency Parsing.Ph.D.
thesis, University of Pennsylvania.Igor?
A. Mel?c?uk.
1988.
Dependency Syntax: theory andpractice.
State University of New York Press, Albany.Radford Neal.
2003.
Slice sampling.
Annals of Statis-tics, 31:705?767.Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2010a.
From Baby Steps to Leapfrog: How?Less is More?
in unsupervised dependency parsing.In Human Language Technologies: The 11th AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics.Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky,and Christopher D. Manning.
2010b.
Viterbi trainingimproves unsupervised dependency parsing.
In Pro-ceedings of the Fourteenth Conference on Computa-tional Natural Language Learning (CoNLL-2010).Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-shawi.
2010c.
Profiting from mark-up: Hyper-textannotations for guided parsing.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics (ACL 2010).Y.
W. Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proceed-ings of the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of the As-sociation for Computational Linguistics, pages 985?992.1213
