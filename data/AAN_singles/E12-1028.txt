Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 276?285,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsAutomatic generation of short informative sentiment summariesAndrea Glaser and Hinrich Schu?tzeInstitute for Natural Language ProcessingUniversity of Stuttgart, Germanyglaseraa@ims.uni-stuttgart.deAbstractIn this paper, we define a new type ofsummary for sentiment analysis: a single-sentence summary that consists of a sup-porting sentence that conveys the overallsentiment of a review as well as a convinc-ing reason for this sentiment.
We present asystem for extracting supporting sentencesfrom online product reviews, based on asimple and unsupervised method.
We de-sign a novel comparative evaluation methodfor summarization, using a crowdsourcingservice.
The evaluation shows that oursentence extraction method performs betterthan a baseline of taking the sentence withthe strongest sentiment.1 IntroductionGiven the success of work on sentiment analy-sis in NLP, increasing attention is being focusedon how to present the results of sentiment analy-sis to the user.
In this paper, we address an im-portant use case that has so far been neglected:quick scanning of short summaries of a body ofreviews with the purpose of finding a subset ofreviews that can be studied in more detail.
Thisuse case occurs in companies that want to quicklyassess, perhaps on a daily basis, what consumersthink about a particular product.
One-sentencesummaries can be quickly scanned ?
similar tothe summaries that search engines give for searchresults ?
and the reviews that contain interestingand new information can then be easily identified.Consumers who want to quickly scan review sum-maries to pick out a few reviews that are helpfulfor a purchasing decision are a similar use case.For a one-sentence summary to be useful in thiscontext, it must satisfy two different ?informationneeds?
: it must convey the sentiment of the re-view, but it must also provide a specific reasonfor that sentiment, so that the user can make aninformed decision as to whether reading the en-tire review is likely to be worth the user?s time ?again similar to the purpose of the summary of aweb page in search engine results.We call a sentence that satisfies these two crite-ria a supporting sentence.
A supporting sentencecontains information on the sentiment as well asa specific reason for why the author arrived at thissentiment.
Examples for supporting sentences are?The picture quality is very good?
or ?The bat-tery life is 2 hours?.
Non-supporting sentencescontain opinions without such reasons such as ?Ilike the camera?
or ?This camera is not worth themoney?.To address use cases of sentiment analysis thatinvolve quick scanning and selective reading oflarge numbers of reviews, we present a simple un-supervised system in this paper that extracts onesupporting sentence per document and show thatit is superior to a baseline of selecting the sentencewith the strongest sentiment.One problem we faced in our experiments wasthat standard evaluations of summarization wouldhave been expensive to conduct for this study.
Wetherefore used crowdsourcing to perform a newtype of comparative evaluation method that is dif-ferent from training set and gold standard cre-ation, the dominant way crowdsourcing has beenused in NLP so far.In summary, our contributions in this paper areas follows.
We define supporting sentences, a newtype of sentiment summary that is appropriate insituations where both the sentiment of a reviewand a good reason for that sentiment need to be276conveyed succinctly.
We present a simple un-supervised method for extracting supporting sen-tences and show that it is superior to a baseline ina novel crowdsourcing-based evaluation.In the next section, we describe related workthat is relevant to our new approach.
In Section 3we present the approach we use to identify sup-porting sentences.
Section 4 describes the fea-ture representation of sentences and the classifi-cation method.
In Section 5 we give an overviewof the crowdsourcing evaluation.
Section 6 dis-cusses our experimental results.
In Sections 7 and8, we present our conclusions and plans for futurework.2 Related WorkBoth sentiment analysis (Pang and Lee, 2008;Liu, 2010) and summarization (Nenkova andMcKeown, 2011) are important subfields of NLP.The work most relevant to this paper is work onsummarization methods that addresses the spe-cific requirements of summarization in sentimentanalysis.
There are two lines of work in this veinwith goals similar to ours: (i) aspect-based andpro/con-summarization and (ii) approaches thatextract summary sentences from reviews.An aspect is a component or attribute of aproduct such as ?battery?, ?lens cap?, ?batterylife?, and ?picture quality?
for cameras.
Aspect-oriented summarization (Hu and Liu, 2004;Zhuang et al 2006; Kim and Hovy, 2006) col-lects sentiment assessments for a given set of as-pects and returns a list of pros and cons about ev-ery aspect for a review or, in some cases, on aper-sentence basis.Aspect-oriented summarization and pro/con-summarization differ in a number of ways fromsupporting sentence summarization.
First, as-pects and pros&cons are taken from a fixed in-ventory.
The inventory is typically small and doesnot cover the full spectrum of relevant informa-tion.
Second, in its most useful form, aspect-oriented summarization requires classification ofphrases and sentences according to the aspect theybelong to; e.g., ?The camera is very light?
hasto be recognized as being relevant to the aspect?weight?.
Developing a component that assignsphrases and sentences to their corresponding cat-egories is time-consuming and has to be redonefor each domain.
Any such component will makemistakes and undetected or incorrectly classifiedaspects can result in bad summaries.Our approach enables us to find strong support-ing sentences even if the reason given in that sen-tence does not fit well into the fixed inventory.
Nomanual work like the creation of an aspect inven-tory is necessary and there are no requirements onthe format of the reviews such as author-providedpros and cons.Aspect-oriented summarization also differs inthat it does not differentiate along the dimensionof quality of the reason given for a sentiment.
Forexample, ?I don?t like the zoom?
and ?The zoomrange is too limited?
both give reasons for why acamera gets a negative evaluation, but only the lat-ter reason is informative.
In our work, we evaluatethe quality of the reason given for a sentiment.The use case we address in this paper requiresa short, easy-to-read summary.
A well-formedsentence is usually easier to understand than apro/con table.
It also has the advantage that theinformation conveyed is accurately representingwhat the user wanted to say ?
this is not the casefor a presentation that involves several complexprocessing steps and takes linguistic material outof the context that may be needed to understand itcorrectly.Berend (2011) performs a form of pro/consummarization that does not rely on aspects.However, most of the problems of aspect-basedpro/con summarization also apply to this paper:no differentiation between good and bad reasons,the need for human labels to train a classifier, andinferior readability compared to a well-formedsentence.Two previous approaches that have attemptedto extract sentences from reviews in the contextof summarization are (Beineke et al 2004) and(Arora et al 2009).
Beineke et al(2004) traina classifier on rottentomatoes.com summary sen-tences provided by review authors.
These sen-tences sometimes contain a specific reason for theoverall sentiment of the review, but sometimesthey are just catchy lines whose purpose is todraw moviegoers in to read the entire review; e.g.,?El Bulli barely registers a pulse stronger than abook?s?
(which does not give a specific reason forwhy the movie does not register a strong pulse).Arora et al(2009) define two classes of sen-tences: qualified claims and bald claims.
A qual-ified claim gives the reader more details (e.g.,?This camera is small enough to fit easily in a277coat pocket?)
while a bald claim is open to inter-pretation (e.g., ?This camera is small?).
Quali-fied/bald is a dimension of classification of senti-ment statements that is to some extent orthogonalto quality of reason.
Qualified claims do not haveto contain a reason and bald claims can containan informative reason.
For example, ?I didn?t likethe camera, but I suspect it will be a great camerafor first timers?
is classified as a qualified claim,but the sentence does not give a good reason forthe sentiment of the document.
Both dimensions(qualified/bald, high-quality/low-quality reason)are important and can be valuable components ofa complete sentiment analysis system.Apart from the definition of the concept of sup-porting sentence, which we believe to be more ap-propriate for the application we have in mind thanrottentomatoes.com summary sentences and qual-ified claims, there are two other important differ-ences of our approach to these two papers.
First,we directly evaluate the quality of the reasons in acrowdsourcing experiment.
Second, our approachis unsupervised and does not require manual an-notation of a training set of supporting sentences.As we will discuss in Section 5, we proposea novel evaluation measure for summarizationbased on crowdsourcing in this paper.
The mostcommon use of crowdsourcing in NLP is to haveworkers label a training set and then train a super-vised classifier on this training set.
In contrast, weuse crowdsourcing to directly evaluate the relativequality of the automatic summaries generated bythe unsupervised method we propose.3 ApproachOur approach is based on the following threepremises.
(i) A good supporting sentence conveys boththe review?s sentiment and a supporting fact.We make this assumption because we wantthe sentence to be self-contained.
If it onlydescribes a fact about a product withoutevaluation, then it does not on its own ex-plain which sentiment is conveyed by the ar-ticle and why.
(ii) Supporting facts are most often expressed bynoun phrases.
We call a noun phrase that ex-presses a supporting fact a keyphrase.
Weare not assuming that all important wordsin the supporting sentence are nominal; theverb will be needed in many cases to accu-rately convey the reason for the sentimentexpressed.
However, it is a fairly safe as-sumption that part of the information is con-veyed using noun phrases since it is dif-ficult to convey specific information with-out using specific noun phrases.
Adjectivesare often important when expressing a rea-son, but frequently a noun is also mentionedor one would need to resolve a pronoun tomake the sentence a self-contained support-ing sentence.
In a sentence like ?It?s easyto use?
it is not clear what the adjective isreferring to.
(iii) Noun phrases that express supporting factstend to be domain-specific; they can beautomatically identified by selecting nounphrases that are frequent in the domain ?
ei-ther in relative terms (compared to a genericcorpus) or in absolute terms.
By makingthis assumption we may fail to detect sup-porting sentences that are worded in an orig-inal way using ordinary words.
However,in a specific domain there is usually a lotof redundancy and most good reasons oc-cur many times and are expressed by similarwords.Based on these assumptions, we select the sup-porting sentence in two steps.
In the first step, wedetermine the n sentences with the strongest sen-timent within every review by classifying the po-larity of the sentences (where n is a parameter).In the second step, we select one of the n sen-tences as the best supporting sentence by meansof a weighting function.Step 1: Sentiment ClassificationIn this step, we apply a sentiment classifier to allsentences of the review to classify sentences aspositive or negative.
We then select the n sen-tences with the highest probability of conformingwith the overall sentiment of the document.
Forexample, if the document?s polarity is negative,we select the n sentences that are most likely to benegative according to the sentiment classifier.
Werestrict the set of n sentences to sentences with the?right?
sentiment because even an excellent sup-porting sentence is not a good characterization of278the content of the review if it contradicts the over-all assessment given by the review.
Only in caseswhere there are fewer than n sentences with thecorrect sentiment, we also select sentences withthe ?wrong?
sentence to obtain a minimum of nsentences for each review.Step 2: Weighting FunctionBased on premises (ii) and (iii) above, we scorea sentence based on the number of noun phrasesthat occur with high absolute and relative fre-quency in the domain.
We only consider sim-ple nouns and compound nouns consisting oftwo nouns in this paper.
In general, compoundnouns are more informative and specific.
A com-pound noun may refer to a specific reason evenif the head noun does not (e.g., ?life?
vs.
?batterylife?).
This means that we need to compute scoresin a way that allows us to give higher weight tocompound nouns than to simple nouns.In addition, we also include counts of nounsand compounds in the scoring that do not havehigh absolute/relative frequency because fre-quency heuristics identify keyphrases with onlymoderate accuracy.
However, theses nouns andcompounds are given a lower weight.This motivates a scoring function that is aweighted sum of four variables: number of simplenouns with high frequency, number of infrequentsimple nouns, number of compound nouns withhigh frequency, and number of infrequent com-pound nouns.
High frequency is defined as fol-lows.
Let fdom(p) be the domain-specific abso-lute frequency of phrase p, i.e., the frequency inthe review corpus, and fwiki(p) the frequency ofp in the English Wikipedia.
We view the distribu-tion of terms in Wikipedia as domain-independentand define the relative frequency as in Equation 1.frel(p) =fdom(p)fwiki(p)(1)We do not consider nouns and compound nounsthat do not occur in Wikipedia for computingthe relative frequency.
A noun (resp.
compoundnoun) is deemed to be of high frequency if it isone of the k% nouns (resp.
compound nouns) withthe highest fdom(p) and at the same time is one ofthe k% nouns (resp.
compound nouns) with thehighest frel(p) where k is a parameter.Based on these definitions, we define four dif-ferent sets: F1 (the set of nouns with high fre-quency), I1 (the set of infrequent nouns), F2 (theset of compounds with high frequency), and I2(the set of infrequent compounds).
An infrequentnoun (resp.
compound) is simply defined as anoun (resp.
compound) that does not meet the fre-quency criterion.We define the score s of a sentence with n to-kens t1 .
.
.
tn (where the last token tn is a punctu-ation mark) as follows:s =n?1?i=1wf2 ?
[[(ti, ti+1) ?
F2]]+ wi2 ?
[[(ti, ti+1) ?
I2]]+ wf1 ?
[[ti ?
F1]]+ wi1 ?
[[ti ?
I1]](2)where [[?]]
= 1 if ?
is true and [[?]]
= 0 otherwise.Note that a noun in a compound will contribute tothe overall score in two different summands.The weights wf2 , wi2 , wf1 , and wi1 are deter-mined using logistic regression.
The training setfor the regression is created in an unsupervisedfashion as follows.
From each set of n sentences(one per review), we select the two highest scor-ing, i.e., the two sentences that were classifiedwith the highest confidence.
The two classes inthe regression problem are then the top rankedsentences vs. the sentences at rank 2.
Since tak-ing all sentences turned out to be too noisy, weeliminate sentence pairs where the top sentence isbetter than the second sentence on almost all ofthe set counts (i.e., count of members of F1, I1,F2, and I2).
Our hypothesis in setting up this re-gression was that the sentence with the strongestsentiment often does not give a good reason.
Ourexperiments confirm that this hypothesis is true.The weights wf2 , wi2 , wf1 , and wi1 estimatedby the regression are then used to score sentencesaccording to Equation 2.We give the same weight to all keyphrase com-pounds (and the same weight to all keyphrasenouns) ?
in future work one could attempt to givehigher weights to keyphrases with higher absoluteor relative frequency.
In this paper, our goal is toestablish a simple baseline for the task of extrac-tion of supporting sentences.After computing the overall weight for eachsentence in a review, the sentence with the highestweight is chosen as the supporting sentence ?
thesentence that is most informative for explainingthe overall sentiment of the review.2794 Experiments4.1 DataWe use part of the Amazon dataset from Jindaland Liu (2008).
The dataset consists of more than5.8 million consumer-written reviews of severalproducts, taken from the Amazon website.
Forour experiment we used the digital camera do-main and extracted 15,340 reviews covering a to-tal of 740 products.
See table 1 for key statisticsof the data set.Type NumberBrands 17Products 740Documents (all) 15,340Documents (cleaned) 11,624Documents (train) 9,880Documents (test) 1,744Short test documents 147Long test documents 1,597Average number of sents 13.36Median number of sents 10Table 1: Key statistics of our datasetIn addition to the review text, authors can givean overall rating (a number of stars) to the prod-uct.
Possible ratings are 5 (very positive), 4 (pos-itive), 3 (neutral), 2 (negative), and 1 (very nega-tive).
We unify ratings of 4 and 5 to ?positive?
andratings of 1 and 2 to ?negative?
to obtain polaritylabels for binary classification.
Reviews with arating of 3 are discarded.4.2 PreprocessingWe tokenized and part-of-speech (POS) taggedthe corpus using TreeTagger (Schmid, 1994).
Wesplit each review into individual sentences by us-ing the sentence boundaries given by TreeTag-ger.
One problem with user-written reviews isthat they are often not written in coherent En-glish, which results in wrong POS tags.
To ad-dress some of these problems, we cleaned thecorpus after the tokenization step.
We separatedword-punctuation clusters (e.g., word...word) andremoved emoticons, html tags, and all sentenceswith three or fewer tokens, many of which werea result of wrong tokenization.
We excluded allreviews with fewer than five sentences.
Short re-views are often low-quality and do not give goodreasons.
The cleaned corpus consists of 11,624documents.
Finally, we split the corpus into train-ing set (85%) and test set (15%) as shown in Table1.
The average number of sentences of a review is13.36 sentences, the median number of sentencesis 10.4.3 Sentiment ClassificationWe first build a sentence sentiment classifier bytraining the Stanford maximum entropy classifier(Manning and Klein, 2003) on the sentences in thetraining set.
Sentences occurring in positive (resp.negative) reviews are labeled positive (resp.
neg-ative).
We use a simple bag-of-words representa-tion (without punctuation characters and frequentstop words).
Propagating labels from documentsto sentences creates a noisy training set becausesome sentences have sentiment different from thesentiment in their documents; however, there isno alternative because we need per-sentence clas-sification decisions, but do not have per-sentencehuman labels.The accuracy of the classifier is 88.4% on?propagated?
sentence labels.We use the sentence classifier in two ways.First, it defines our baseline BL for extractingsupporting sentences: the baseline simply pro-poses the sentence with the highest sentimentscore that is compatible with the sentiment of thedocument as the supporting sentence.Second, the sentence classifier selects a subsetof candidate sentences that is then further pro-cessed using the scoring function in Equation 2.This subset consists of the n = 5 sentences withthe highest sentiment scores of the ?right?
polarity?
that is, if the document is positive (resp.
nega-tive), then the n = 5 sentences with the highestpositive (resp.
negative) scores are selected.4.4 Determining Frequencies and WeightsThe absolute frequency of nouns and compoundnouns simply is computed as their token fre-quency in the training set.
For computing the rel-ative frequency (as described in Section 3, Equa-tion 1), we use the 20110405 dump of the EnglishWikipedia.In the product review corpora we studied,the percentage of high-frequency keyphrase com-pound nouns was higher than that of simplenouns.
We therefore use two different thresh-olds for absolute and relative frequency.
We de-280fine F1 as the set of nouns that are in the topkn = 2.5% for both absolute and relative fre-quencies; and F2 as the set of compounds that arein the top kp = 5% for both absolute and rela-tive frequencies.
These thresholds are set to ob-tain a high density of good keyphrases with fewfalse positives.
Below the threshold there are stillother good keyphrases, but they cannot be sepa-rated easily from non-keyphrases.Sentences are scored according to Equation 2.Recall that the parameters wf2 , wi2 , wf1 , and wi1are determined using logistic regression.
The ob-tained parameter values (see table 2) indicate therelative importance of the four different types ofterms.
Compounds are the most important termand even those with a frequency below the thresh-old kp still provide more detailed information thansimple nouns above the threshold kn; the value ofwi2 is approximately twice the value wf1 for thisreason.
Non-keyphrase nouns are least importantand are weighted with only a very small value ofwi1 = 0.01.Phrase Par Valuekeyphrase compounds wf2 1.07non-keyphrase compounds wi2 0.89keyphrase nouns wf1 0.46non-keyphrase nouns wi1 0.01Table 2: Weight settingsThe scoring function with these parameter val-ues is applied to the n = 5 selected sentences ofthe review.
The highest scoring sentence is thenselected as the supporting sentence proposed byour system.For 1380 of the 1744 reviews, the sentence se-lected by our system is different from the baselinesentence; however, there are 364 cases (20.9%)where the two are the same.
Only the 1380 caseswhere the two methods differ are included in thecrowdsourcing evaluation to be described in thenext section.
As we will show below, our sys-tem selects better supporting sentences than thebaseline in most cases.
So if baseline and our sys-tem agree, then it is even more likely that the sen-tence selected by both is a good supporting sen-tence.
However, there could also be cases wherethe n = 5 sentences selected by the sentimentclassifier are all bad supporting sentences or caseswhere the document does not contain any goodsupporting sentences.5 Comparative Evaluation with AmazonMechanical TurkOne standard way to evaluate summarization sys-tems is to create hand-edited summaries and tocompute some measure of similarity (e.g., wordor n-gram overlap) between automatic and humansummaries.
An alternative for extractive sum-maries is to classify all sentences in the documentwith respect to their appropriateness as summarysentences.
An automatic summary can then bescored based on its ability to correctly identifygood summary sentences.
Both of these meth-ods require a large annotation effort and are mostlikely too complex to be outsourced to a crowd-sourcing service because the creation of manualsummaries requires skilled writers.
For the sec-ond type of evaluation, ranking sentences accord-ing to a criterion is a lot more time consumingthan making a binary decision ?
so ranking the13 or 14 sentences that a review contains on av-erage for the entire test set would be a signifi-cant annotation effort.
It would also be difficultto obtain consistent and repeatable annotation incrowdsourcing on this task due to its subtlety.We therefore designed a novel evaluationmethodology in this paper that has a much smallerstartup cost.
It is well known that relative judg-ments are easier to make on difficult tasks than ab-solute judgments.
For example, much recent workon relevance ranking in information retrieval re-lies on relative relevance judgments (one docu-ment is more relevant than another) rather than ab-solute relevance judgments.
We adopt this gen-eral idea and only request such relative judgmentson supporting sentences from annotators.
Unlikea complete ranking of the sentences (which wouldrequire m(m ?
1)/2 judgments where m is thelength of the review), we choose a setup wherewe need to only elicit a single relative judgmentper review, one relative judgment on a sentencepair (consisting of the baseline sentence and thesystem sentence) for each of the 1380 reviews se-lected in the previous section.
This is a manage-able annotation task that can be run on a crowd-sourcing service in a short time and at little cost.We use Amazon Mechanical Turk (AMT) forthis annotation task.
The main advantage of AMTis that cost per annotation task is very low, so thatwe can obtain large annotated datasets for an af-281Task:Sentence 1: This 5 meg camera meets all my requirements.Sentence 2: Very good pictures, small bulk, long battery life.Which sentence gives the more convincing reason?
Fill out exactly one field, please.Please type the blue word of the chosen sentence into the corresponding answer field.s1s2If both sentences do not give a convincing reason, type NOTCONV into this answerfield.XSubmitfile:///Users/hs0711/example2.html1 of 1 3/9/12 12:06 PMFigure 1: AMT interface for annotatorsfordable price.
The disadvantage is the level ofquality of the annotation which will be discussedat the end of this section.5.1 Task DesignWe created a HIT (Human Intelligence Task)template including detailed annotation guidelines.Every HIT consists of a pair of sentences.
Onesentence is the baseline sentence; the other sen-tence is the system sentence, i.e., the sentence se-lected by the scoring function.
The two sentencesare presented in random order to avoid bias.The workers are then asked to evaluate the rel-ative quality of the sentences by selecting one ofthe following three options:1.
Sentence 1 has the more convincing reason2.
Sentence 2 has the more convincing reason3.
Neither sentence has a convincing reasonIf both sentences contain reasons, the workerhas to compare the two reasons and choose thesentence with the more convincing reason.Each HIT was posted to three different workersto make it possible to assess annotator agreement.Every worker can process each HIT only onceso that the three assignments are always done bythree different people.Based on the worker annotations, we compute agold standard score for each sentence.
This scoreis simply the number of times it was rated bet-ter than its competitor.
The score can be 0, 1, 2or 3.
HITs for which the worker chooses the op-tion ?Neither sentence has a convincing reason?are ignored when computing sentence scores.The sentence with the higher score is then se-lected as the best supporting sentence for the cor-responding review.In cases of ties, we posted the sentence pair onemore time for one worker.
If one of the two sen-tences has a higher score after this reposting, wechoose it as the winner.
Otherwise we label thissentence pair ?no decision?
or ?N-D?.5.2 Quality of AMT AnnotationsSince our crowdsourcing based evaluation isnovel, it is important to investigate if human an-notators perform the annotation consistently andreproducibly.The Fleiss?
?
agreement score for the finalexperiment is 0.17.
AMT workers only havethe instructions given by the requesters.
If theyare not clear enough or too complicated, work-ers can misunderstand the task, which decreasesthe quality of the answers.
There are also AMTworkers who spam and give random answers totasks.
Moreover, ranking sentences according tothe quality of the given reason is a subjective task.Even if the sentence contains a reason, it mightnot be convincing for the worker.To ensure a high level of quality for our dataset,282Experiment # Docs BL SY N-D B=S1 AMT, first pass 1380 27.4 57.9 14.7 -2 AMT, second pass 203 46.8 45.8 7.4 -3 AMT final 1380 34.3 64.6 1.1 -4 AMT+[B=S] 1744 27.1 51.1 0.9 20.9Table 3: AMT evaluation results.
Numbers are percentages or counts.
BL = baseline, SY = system, N-D = nodecision, B=S = same sentence selected by baseline and systemwe took some precautions.
To force workers toactually read the sentences and not just click afew boxes, we randomly marked one word of eachsentence blue.
The worker had to type the wordof their preferred sentence into the correspondinganswer field or NOTCONV into the special field ifneither sentence was convincing.
Figure 1 showsour AMT interface design.For each answer field we have a gold stan-dard (the words we marked blue and the wordNOTCONV) which enables us to look for spam.The analysis showed that some workers mistypedsome words, which however only indicates thatthe worker actually typed the word instead ofcopying it from the task.
Some workers submit-ted inconsistent answers, for instance, they typeda random word or filled out all three answer fields.In such cases we reposted this HIT again to re-ceive a correct answer.After the task, we counted how often a workersaid that neither sentence is convincing since ahigh number indicates that the worker might haveonly copied the word for several sentence pairswithout checking the content of the sentences.
Wealso analyzed the time a worker needed for everyHIT.
Since no task was done in less than 10 sec-onds, the possibility of just copying the word wasrather low.6 Results and discussionThe results of the AMT experiment are shown intable 3.
As described above, each of the 1380sentence pairs was evaluated by three workers.Workers rated the system sentence as better for57.9% of the reviews, and the baselines sentenceas better for 27.4% of the reviews; for 14.7% ofreviews, the scores of the two sentences were tied(line 1 of Table 3).
The 203 reviews in this cate-gory were reposted one more time (as described inSection 5).
The responses were almost perfectlyevenly split: about 47% of workers preferred thebaseline system, 46% the system sentence; 7.4%of the responses were undecided (line 2).
Line 3presents the consolidated results where the 14.7%ties on line 1 are replaced by the ratings obtainedon line 2 in the second pass.The consolidated results (line 3) show that oursystem is clearly superior to the baseline of se-lecting the sentence with the strongest sentiment.Our system selected a better supporting sentencefor 64.6% of the reviews; the baseline selected abetter sentence for 34.3% of the reviews.
Theseresults exclude the reviews where baseline andsystem selected the same sentence.
If we as-sume that these sentences are also acceptable sen-tences (since they score well on the traditionalsentiment metrics as well as on our new con-tent keyword metric), then our system finds agood supporting sentence for 72.0% of reviews(51.1+20.9) whereas the baseline does so for only48.0% (27.1+20.9).6.1 Error AnalysisOur error analysis revealed that a significant pro-portion of system sentences that were worse thanbaseline sentences did contain a reason.
How-ever, the baseline sentence also contained a reasonand was rated better by AMT annotators.
Exam-ples (1) and (2) show two such cases.
The firstsentence is the baseline sentence (BL) which wasrated better.
The system sentence (SY) containsa similar or different reason.
Since rating reasonsis a very subjective task, it is impossible to de-fine which of these two sentences contains the bet-ter reason and depends on how the workers thinkabout it.
(1) BL:The best thing is that everything is just soeasily displayed and one doesn?t need amanual to start getting the work done.SY: The zoom is incredible, the video was soclear that I actually thought of making a15 min movie.283(2) BL:The colors are horrible, indoor shots arehorrible, and too much noise.SY: Who cares about 8 mega pixels and 1600iso when it takes such bad quality pic-tures.In example (3) the system sentence is an in-complete sentence consisting of only two nounphrases.
These cut-off sentences are mainlycaused by incorrect usage of grammar and punc-tuation by the reviewers which results in wronglydetermined sentence boundaries in the prepro-cessing step.
(3) BL:Gives peace of mind to have it fit per-fectly.SY: battery and SD card.In some cases, the two sentences that were pre-sented to the worker in the evaluation had a dif-ferent polarity.
This can have two reasons: (i) dueto noisy training input, the classifier misclassifiedsome of the sentences, and (ii) for short reviewswe also used sentences with the non-conformingpolarity.
Sentences with different polarity oftenconfused the workers and they tended to preferthe positive sentence even if the negative one con-tained a more convincing reason as can be seen inexample (4).
(4) BL:It shares same basic commands andsetup, so the learning curve was minimal.SY: I was not blown away by the image qual-ity, and as others have mentioned, theflash really is weak.A general problem with our approach is that theweighting function favors sentences with manynoun phrases.
The system sentence in example(5) contains many noun phrases, including somehighly frequent nouns (e.g., ?lens?, ?battery?
),but there is no convincing reason and the baselinesentence has been selected by the workers.
(5) BL:I have owned my cd300 for about 3 weeksand have already taken 700 plus pictures.SY: It has something to do with the lens be-cause the manual says it only happens tothe 300 and when I called Sony tech sup-port the guy tried to tell me the batterywas faulty and it wasn?t.Finally, there are a number of cases where ourassumption that good supporting sentences con-tain keyphrases is incorrect.
For example, sen-tence (6) does not contain any keyphrases indica-tive of good reasons.
The information that makesit a good supporting sentence is mainly expressedusing verbs and particles.
(6) I have had an occasional problem withthe camera not booting up and telling meto turn it off and then on again.7 ConclusionIn this work, we presented a system that ex-tracts supporting sentences, single-sentence sum-maries of a document that contain a convincingreason for the author?s opinion about a product.We used an unsupervised approach that extractskeyphrases of the given domain and then weightsthese keyphrases to identify supporting sentences.We used a novel comparative evaluation method-ology with the crowdsourcing framework Ama-zon Mechanical Turk to evaluate this novel tasksince no gold standard is available.
We showedthat our keyphrase-based system performs betterthan a baseline of extracting the sentence with thehighest sentiment score.8 Future workOur method failed for some of the about 35% ofreviews where it did not find a convincing reasonbecause of the noisiness of reviews.
Reviews areuser-generated content and contain grammaticallyincorrect sentences and are full of typographicalerrors.
This problem makes it hard to perform pre-processing steps like part-of-speech tagging andsentence boundary detection correctly and reli-ably.
We plan to address these problems in fu-ture work by developing a more robust processingpipeline.AcknowledgmentsThis work was supported by DeutscheForschungsgemeinschaft (Sonderforschungs-bereich 732, Project D7) and in part by theIST Programme of the European Community,under the PASCAL2 Network of Excellence,IST-2007-216886.
This publication only reflectsthe authors?
views.284ReferencesShilpa Arora, Mahesh Joshi, and Carolyn P. Rose?.2009.
Identifying types of claims in online cus-tomer reviews.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, Companion Volume:Short Papers, NAACL-Short ?09, pages 37?40,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Philip Beineke, Trevor Hastie, Christopher Manning,and Shivakumar Vaithyanathan.
2004.
Exploringsentiment summarization.
In Proceedings of theAAAI Spring Symposium on Exploring Attitude andAffect in Text: Theories and Applications.
AAAIPress.
AAAI technical report SS-04-07.Ga?bor Berend.
2011.
Opinion expression mining byexploiting keyphrase extraction.
In Proceedings of5th International Joint Conference on Natural Lan-guage Processing, pages 1162?1170, Chiang Mai,Thailand, November.
Asian Federation of NaturalLanguage Processing.Minqing Hu and Bing Liu.
2004.
Mining and sum-marizing customer reviews.
In Proceedings of theTenth ACM SIGKDD international conference onKnowledge discovery and data mining, KDD ?04,pages 168?177, New York, NY, USA.
ACM.Nitin Jindal and Bing Liu.
2008.
Opinion spamand analysis.
In WSDM ?08: Proceedings of theinternational conference on Web search and webdata mining, pages 219?230, New York, NY, USA.ACM.Soo-Min Kim and Eduard Hovy.
2006.
Automaticidentification of pro and con reasons in online re-views.
In Proceedings of the COLING/ACL onMain conference poster sessions, COLING-ACL?06, pages 483?490, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Bing Liu.
2010.
Sentiment analysis and subjectivity.Handbook of Natural Language Processing, 2nd ed.Christopher Manning and Dan Klein.
2003.
Opti-mization, maxent models, and conditional estima-tion without magic.
In Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics on Hu-man Language Technology: Tutorials - Volume 5,NAACL-Tutorials ?03, pages 8?8, Stroudsburg, PA,USA.
Association for Computational Linguistics.Ani Nenkova and Kathleen McKeown.
2011.
Auto-matic summarization.
Foundations and Trends inInformation Retrieval, 5(2-3):103?233.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1-2):1?135.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, Manchester, UK.Li Zhuang, Feng Jing, and Xiao-Yan Zhu.
2006.Movie review mining and summarization.
In Pro-ceedings of the 15th ACM international conferenceon Information and knowledge management, CIKM?06, pages 43?50, New York, NY, USA.
ACM.285
