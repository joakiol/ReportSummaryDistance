Anomaly Detecting within Dynamic Chinese Chat TextYunqing XiaDepartment of S.E.E.M.The Chinese University of Hong KongShatin, Hong Kongyqxia@se.cuhk.edu.hkKam-Fai WongDepartment of S.E.E.M.The Chinese University of Hong KongShatin, Hong Kongkfwong@se.cuhk.edu.hkAbstractThe problem in processing Chinese chattext originates from the anomalous char-acteristics and dynamic nature of such atext genre.
That is, it uses ill-edited termsand anomalous writing styles in chat text,and the anomaly is created and discardedvery quickly.
To handle this problem,one solution is to re-train the recognizerperiodically.
This costs a lot of man-power in producing the timely chat textcorpus.
The new approaches are pro-posed in this paper to detect the anomalywithin dynamic Chinese chat text by in-corporating standard Chinese corpora andchat corpus.
We first model standard lan-guage text using standard Chinese cor-pora and apply these models to detectanomalous chat text.
To improve detec-tion quality, we construct anomalous chatlanguage model using one static chat textcorpus and incorporate this model intothe standard language models.
Our ap-proaches calculate confidence and en-tropy for the input text and apply thresh-old values to help make the decisions.The experiments prove that performanceequivalent to the best ones produced bythe approaches in existence can beachieved stably with our approaches.1 IntroductionNetwork Informal Language (NIL) refers to thespecial human language widely used in thecommunity of network communication via plat-forms such as chat rooms/tools, mobile phoneshort message services (SMS), bulletin boardsystems (BBS), emails, blogs, etc.
NIL is ubiqui-tous due in special to the rapid proliferation ofInternet applications.
As one important type ofNIL text, chat text appears frequently within in-creasing volume of chat logs of online education(Heard-White, 2004) and customer relationshipmanagement (Gianforte, 2003) via chatrooms/tools.
In wed-based chat rooms and BBS alarge volume of NIL text is abused by (McCul-lagh, 2004).
A survey by the Global System forMobile Communication (GSM) showed thatGermans send 200 million messages a year(German News, 2004).
All the facts disclose thegrowing importance in processing NIL text.Chat text holds anomalous characteristics informing non-alphabetical characters, words, andphrases.
It uses ill-edited terms and anomalouswriting styles.
Typical examples of anomalousChinese chat terms can be found in (Xia et.
al.,2005a).
Besides the anomalous characteristics,our observations reveal remarkable dynamic na-ture of the chat text.
The anomaly is created anddiscarded very quickly.
Although there is no ideahow tomorrow?s chat text would look like, thechanging will never stop.
Instead, the changinggets faster and faster.The challenging issues originates from the dy-namic nature are two-fold.
On the one hand,anomalous chat terms and writing styles are fre-quently found in chat text.
Knowledge about chattext is urgently required to understand the anom-aly.
On the other hand, the dynamic nature of thechat text makes it nearly impossible to maintain atimely chat text knowledge base.
This claim hasbeen proved by (Xia et.
al., 2005a) in which ex-periments are conducted with an SVM classifier.The classifier is trained on chat text created in anearlier period and tested on chat text created in alater period.
In their experiments, performance ofthe SVM classifier becomes lower when the twoperiods are farther.
This reveals that chat text iswritten in such a style that changes constantlyalong with time.
A straightforward solution tothis problem is to re-train the SVM classifier pe-riodically with timely chat text collections.
Un-fortunately, this solution costs a lot of manpowerin producing new chat text corpora.
The super-48vised learning technique becomes ineffective inprocessing chat text.This paper proposes approaches to detectinganomaly in dynamic Chinese chat text by incor-porating standard Chinese corpora and a staticchat corpus.
The idea is basically error-driven.That is, we first create standard language modelsusing trigram on standard Chinese corpora.These corpora provide negative training samples.We then construct anomalous chat languagemodel using one static chat text corpus whichprovides positive training samples.
We incorpo-rate the chat language model with the standardlanguage models and calculate confidence andentropy to help make decisions whether inputtext is anomalous chat text.
We investigate twotypes of trigram, i.e.
word trigram and part-of-speech (POS) tag trigram in this work.The remaining sections of this paper are or-ganized as follow.
In Section 2, the works re-lated to this paper are addressed.
In Section 3,approaches of anomaly detection in dynamicChinese chat text with standard Chinese corporaare presented.
In Section 4, we incorporate theNIL corpus into our approaches.
In section 5,experiments are described to estimate thresholdvalues and to evaluate performance of the twoapproaches with various configurations.
Com-parisons and discussions are also reported.
Weconclude this paper and address future works inSection 6.2 Related WorksSome works had been carried out in (Xia et.
al.,2005a) in which an SVM classifier is imple-mented to recognize anomalous chat text terms.A within-domain open test is conducted on chattext posted in March 2005.
The SVM classifier istrained on five training sets which contain chattext posted from December 2004 to February2005.
The experiments show that performance ofthe SVM classifier increases when the trainingperiod and test period are closer.
This revealsthat chat text is written in a style that changesquickly with time.
Many anomalous popular chatterms in last year are forgotten today and newones replace them.
This makes SVM based pat-tern learning technique ineffective to reflect thechanges.The solution to this problem in (Xia et.
al.,2005b) is to re-train the SVM classifier periodi-cally.
This costs a lot of manpower in producingthe timely chat text corpora, in which each pieceof anomalous chat text should be annotated withseveral attributes manually.We argue that the anomalous chat text can beidentified using negative training samples instatic Chinese corpora.
Our proposal is that wemodel the standard natural language using stan-dard Chinese corpora.
We incorporate a staticchat text corpus to provide positive training sam-ples to reflect fundamental characteristics ofanomalous chat text.
We then apply the modelsto detect the anomalous chat text by calculatingconfidence and entropy.Regarding the approaches proposed in this pa-per, our arguments are, 1) the approaches canachieve performance equivalent to the best onesproduced by the approaches in existence; and 2)the good performance can be achieved stably.We prove these arguments in the following sec-tions.3 Anomaly Detection with StandardChinese CorporaChat text exhibits anomalous characteristics inusing or forming words.
We argue that theanomalous chat text, which is referred as anom-aly in this article, can be identified with languagemodels constructed on standard Chinese corporawith some statistical language modeling (SLM)techniques, e.g.
trigram model.The problem of anomaly detection can be ad-dressed as follows.
Given a piece of anomalouschat text, i.e.
},...,,{ 21 nwwwW = , and a languagemodel )}({ xpLM = , we attempt to recognize Was anomaly by the language model.
We proposetwo approaches to tackle this problem.
We de-sign a confidence-based approach to calculatehow likely that W  fits into the language model.Another approach is designed based on entropycalculation.
Entropy method was originally pro-posed to estimate how good a language model is.In our work we apply this method to estimatehow much the constructed language models areable to reflect the corpora properly based on theassumption that the corpora are sound and com-plete.Although there exist numerous statisticalmethods to construct a natural language model,the objective of them is one: to construct a prob-abilistic distribution model )(xp  which fits to themost extent into the observed language data inthe corpus.
We implement the trigram model andcreate language models with three Chinese cor-pora, i.e.
People?s Daily corpus, Chinese  Giga-word and Chinese Pen Treebank.
We investigate49quality of the language models produced withthese corpora.3.1 The N-gram Language ModelsN-gram model is the most widely used in statisti-cal language modeling nowadays.
Without lossof generality we express the probability of aword sequence },...,{ 1 nwwW =  of n words, i.e.
)(Wp  as?=?==niiin wwwwpwwpWp11101 ),...,,|(),...,()((1)where 0w  is chosen appropriately to handle theinitial condition.
The probability of the nextword iw  depends on the history ih  of wordsthat have been given so far.
With this factoriza-tion the complexity of the model grows exponen-tially with the length of the history.One of the most successful models of the pasttwo decades is the trigram model (n=3) whereonly the most recent two words of the history areused to condition the probability of the nextword.Instead of using the actual words, one can usea set of word classes.
Classes based on the POStags, or the morphological analysis of words, orthe semantic information have been tried.
Also,automatically derived classes based on some sta-tistical models of co-occurrence have been tried(Brown et.
al., 1990).
The class model can begenerally described as?=?
?=niiiiii cccpcwpWp112 ),|()|()(       (2)if the classes are non-overlapping.
These tri-classmodels have had higher perplexities than the cor-responding trigram model.
However, they haveled to a reduction in perplexity when linearlycombined with the trigram model.3.2 The Confidence-based ApproachGiven a piece of chat text },...,,{ 21 nwwwW =where each word iw  is obtained with a standardChinese word segmentation tool, e.g.
ICTCLAS.As ICTCLAS is a segmentation tool based onstandard vocabulary, it means that some un-known chat terms (e.g., ????)
would be brokeninto several element Chinese words (i.e., ??
?and ???
in the above case).
This does not hurtthe algorithm because we use trigram in thismethod.
A chat term may produce some anoma-lous word trigrams which are evidences foranomaly detection.We use non-zero probability for each trigramin this calculation.
This is very simple but na?ve.The calculation seeks to produce a so-called con-fidence, which reflects how much the given textfits into the training corpus in arranging its ele-ment Chinese words.
This is enlightened by theobservation that the chat terms use elementwords in anomalous manners which can not besimulated by the training corpus.The confidence-based value is defined as( ) KKi iTCWC11)( ?????
?= ?=                  (3)where K denotes the number of trigrams in chattext W  and iT  is the i-th order trigram.
( )iTC  isconfidence of trigram iT .
Generally ( )iTC  is as-signed probability of the trigram iT  in trainingcorpus, i.e.
( )iTp .
When a trigram is missing,linear interpolation is applied to estimate itsprobability.We empirically setup a confidence thresholdvalue to determine whether the input text con-tains chat terms, namely, it is a piece of chat text.The input is concluded to be stand text if its con-fidence is bigger than the confidence thresholdvalue.
Otherwise, the input is concluded to bechat text.
The confidence threshold value can beestimated with a training chat text collection.3.3 The Entropy-based ApproachThe idea beneath this approach comes from en-tropy based language modeling.
Given a lan-guage model, one can use the quantity of entropyto get an estimation of how good the languagemodel (LM) might be.
Denote by p the true dis-tribution, which is unknown to us, of a segmentof new text x of k words.
Then the entropy on aper word basis is defined as?
?= ?>?
xk xpxpkH )(ln)(1lim              (4)If every word in a vocabulary of size |V| isequally likely then the entropy would be||log 2 V ; ||ln VH ?
for other distributions ofthe words.Enlightened by the estimation method, wecompute the entropy-based value on a per tri-gram basis for the input chat text.
Given a stan-dard LM denoted by p~  which is modeled  bytrigram, the entropy-value is calculate as50?=?=KiiiK TpTpKH1)(~ln)(~1~             (5)where K denotes number of trigrams the inputtext contains.
Our goal is to find how much dif-ference the input text is compared against theLM.
Obviously, bigger entropy discloses a pieceof more anomalous chat text.
An empirical en-tropy threshold is again estimated on a trainingchat text collection.
The input is concluded to bestand text if its entropy is smaller than the en-tropy threshold value.
Otherwise, the input isconcluded to be chat text.4 Incorporating the Chat Text CorpusWe argue performance of the approaches can beimproved when an initial static chat text corpusis incorporated.
The chat text corpus providessome basic forms of the anomalous chat text.These forms we observe provide valuable heuris-tics in the trigram models.
Within the chat textcorpus, we only consider the word trigrams andPOS tag trigrams in which anomalous chat textappears.
We thus construct two trigram lists.Probabilities are produced for each trigram ac-cording to its occurrence.
One chat text exampleEXP1 is given below.EXP1: ?????????
?SEG1: ?
?
??
?
??
?
?
?SEG1 presents the word segments producedby ICTCLAS.
We generate chat text word tri-grams based on SEG1 as follow.TRIGRAM1:   (1)/?
?
??/(2)/?
??
?/(3)/??
?
?/(4)/?
?
?/For each input trigram iT , if it appears in thechat text corpus, we adjust the confidence andentropy values by incorporating its probability inchat text corpus.4.1 The Refined ConfidenceFor each ( )iTC , we assign a weight i?
, which iscalculated as)()( icin TpTpi e ?=?
(6)where )( in Tp  is probability of the trigram iT  instandard corpus and )( ic Tp  probability in chattext corpus.
Equation (3) therefore is re-writtenas( )( ) KKi inTpTpKKi iiTpeTCWCicin11)()(11')(??????=??????=?
?=?= ?
(7)The intention of inserting i?
into confidencecalculation is to decrease confidence of inputchat text when chat text trigrams are found.Normally, when a trigram iT  is found in chat texttrigram lists, )( in Tp  will be much lower than)( ic Tp ; therefore i?
will be much lower than 1 .By multiplying such a weight, confidence of in-put chat text can be decreased so that the text canbe easily detected.4.2 The Refined EntropyInstead of assigning a weight, we introduce theentropy-based value of the input chat text on thechat text corpus, i.e.
cKH~ , to produce a new equa-tion.
We denote nKH~  the entropy calculated withequation (5).
Similar to nKH~ , cKH~  is calculatedwith equation (8).
?=?=KiiciccK TpTpKH1)(~ln)(~1~              (8)We therefore re-write the entropy-based valuecalculation as follows.
( )?=+?=+=KiicicinincKnKKTpTpTpTpKHHH1)(~ln)(~)(~ln)(~1~~~(9)The intention of introducing cKH~  in entropycalculation is to increase the entropy of inputchat text when chat text trigrams are found.
Itcan be easily proved that KH~  is never smallerthan nKH .
As bigger entropy discloses a piece ofmore anomalous chat text, we believe moreanomalous chat texts can be correctly detectedwith equation (9).5 EvaluationsThree experiments are conducted in this work.The first experiment aims to estimate thresholdvalues from a real text collection.
The remainingexperiments seek to evaluate performance of theapproaches with various configurations.5.1 Data DescriptionWe use two types of text corpora to train our ap-proaches in the experiments.
The first type is51standard Chinese corpus which is used to con-struct standard language models.
We use Peo-ple?s Daily corpus, also know as Peking Univer-sity Corpus (PKU), the Chinese Gigaword(CNGIGA) and the Chinese Penn Treebank(CNTB) in this work.
Considering coverage,CNGIGA is the most excellent one.
However,PKU and CPT provide more syntactic informa-tion in their annotations.
Another type of trainingcorpus is chat text corpus.
We use NIL corpusdescribed in (Xia et.
al., 2005b).
In NIL corpuseach anomalous chat text is annotated with theirattributes.We create four test sets in our experiments.We use the test set #1 to estimate the thresholdvalues of confidence and entropy for our ap-proaches.
The values are estimated on two typesof trigrams in three corpora.
Test set #1 contains89 pieces of typical Chinese chat text selectedfrom the NIL corpus and 49 pieces of standardChinese sentences selected from online Chinesenews by hand.
There is no special considerationthat we select different number of chat texts andstandard sentences in this test set.The remaining three test sets are used to com-pare performance of our approaches on test datacreated in different time periods.
The test set #2is the earliest one and #4 the latest one accordingto their time stamp.
There are 10K sentences intotal in test set #2, #3 and #4.
In this collection,chat texts are selected from YESKY BBS system(http://bbs.yesky.com/bbs/) which cover BBStext in March and April 2005 (later than the chattext in the NIL corpus), and standard texts areextracted from online Chinese news randomly.We describe the four test sets in Table 1.Test set # of standard sentences# of chatsentences#1 49 89#2 1013 2320#3 1013 2320#4 1014 2320Table 1: Number of sentences in the four testsets.5.2 Experiment I: Threshold Values Esti-mation5.2.1 Experiment DescriptionThis experiment seeks to estimate the thresholdvalues of confidence and entropy for two typesof trigrams in three Chinese corpora.We first run the two approaches using onlystandard Chinese corpora on the 138 sentences inthe first test set.
We put the calculated values(confidence or entropy) into two arrays.
Notethat we already know type of each sentence inthe first test set.
So we are able to select in eacharray a value that produces the lowest error rate.In this way we obtain the first group of thresholdvalues for our approaches.We incorporate the NIL corpus to the two ap-proaches and run them again.
We then producethe second group of threshold values in the sameway to produce the first group of values.5.2.2 ResultsThe selected threshold values and correspondingerror rates are presented in Table 2~5.Trigram option Threshold Err rateword of CNGIGA 1.58E-07 0.092word of PKU 7.06E-07 0.098word of CNTB 2.09E-06 0.085POS tag of CNGIGA 0.0278 0.248POS tag of PKU 0.0143 0.263POS tag of CNTB 0.0235 0.255Table 2: Selected threshold values of confidencefor the approach using standard Chinese corporaand error rates.Trigram option Threshold Err rateword of CNGIGA 3.762E-056 0.099word of PKU 5.683E-048 0.112word of CNTB 2.167E-037 0.169POS tag of CNGIGA 0.00295 0.234POS tag of PKU 0.00150 0.253POS tag of CNTB 0.00239 0.299Table 3: Selected threshold values of entropy forthe approach using standard Chinese corpora anderror rates.Trigram option Threshold Err rateword of CNGIGA 4.26E-05 0.089word of PKU 3.75E-05 0.102word of CNTB 6.85E-05 0.092POS tag of CNGIGA 0.0398 0.257POS tag of PKU 0.0354 0.266POS tag of CNTB 0.0451 0.249Table 4: Selected threshold values of confidencefor the approach incorporating the NIL corpusand error rates.Trigram option Threshold Err rateword of CNGIGA 8.368E-027 0.102word of PKU 3.134E-019 0.096word of CNTB 5.528E-021 0.172POS tag of CNGIGA 0.00465 0.241POS tag of PKU 0.00341 0.251POS tag of CNTB 0.00532 0.282Table 5: Selected thresholds values of entropyfor the approach incorporating the NIL corpusand error rates.52We use the selected threshold values in ex-periment II and III to detect anomalous chat textwithin test set #2, #3 and #4.5.3 Experiment II: Anomaly Detection withThree Standard Chinese Corpora5.3.1 Experiment DescriptionIn this experiment, we run the two approachesusing the standard Chinese corpora on test set #2.The threshold values estimated in experiment Iare applied to help make decisions.Input text can be detected as either standardtext or chat text.
But we are only interested inhow correctly the anomalous chat text is de-tected.
Thus we calculate precision (p), recall (r)and 1F  measure (f) only for chat text.2rprpfbaarcaap +?
?=+=+=     (10)where a is the number of true positives, b thefalse negatives and c the false positives.5.3.2 ResultsThe experiment results for the approaches usingthe standard Chinese corpora on test set #2 arepresented in Table 6.5.3.3 DiscussionsTable 4 shows that, in most cases, the entropy-based approach outperforms the confidence-based approach slightly.
It can thus be concludethat the entropy-based approach is more effectivein anomaly detection.It is also revealed that both approaches per-form better with word trigrams than that withPOS tag trigrams.
This is natural for class basedtrigram model when number of class is small.Thirty-nine classes are used in ICTCLAS in POStagging Chinese words.When the three Chinese corpora are compared,the CNGIGA performs best in the confidence-based approach with word trigram model.
How-ever, it is not the case with POS tag trigrammodel.
Results of two approaches on CNTB arebest amongst the three corpora.
Although we areable to draw the conclusion that  bigger corporayields better performance with word trigram, thesame conclusion, however, does not work forPOS tag trigram.
This is very interesting.
Thereason we can address on this issue is that CNTBprobably provides highest quality POS tag tri-grams and other corpora contain more noisy POStag trigrams, which eventually decreases the per-formance.
An observation on word/POS tag listsfor three Chinese corpora verifies such a claim.Text in CNTB is best-edited amongst the three.5.4 Experiment III: Anomaly Detectionwith NIL Corpus Incorporated5.4.1 Experiment DescriptionIn this experiment, we incorporate one chat textcorpus, i.e.
NIL corpus, to the two approaches.We run them on test set #2, #3 and #4 with theestimated threshold values.
We use precision,recall and 1F  measure again to evaluate perform-ance of the two approaches.5.4.2 ResultsThe experiment results are presented in Table 7~Table 9 on test set #2,  #3 and #4 respectively.5.4.3 DiscussionsWe first compare the two approaches with dif-ferent running configurations.
All conclusionsmade in experiment II still work for experimentIII.
They are, i) the entropy-based approach out-performs the confidence-based approach slightlyin most cases; ii) both approach perform betterwith word trigram than POS tag trigram; iii) bothapproaches perform best on CNGIGA with wordtrigram model.
But with POS tag trigram model,CNTB produces the best results.An interesting comparison is conducted on 1Fmeasure between the approaches in experiment IIand experiment III on test set #2 in Figure 1 (theleft two columns).
Generally, 1F  measure ofanomaly detection with both approaches withword trigram model is improved when the NILcorpus is incorporated.
It is revealed in Table7~9 that same observation is found with POS tagtrigram model.We compare 1F  measure of the approacheswith word trigram model in experiment III ontest set #2, #3 and #4 in Figure 1 (the right threecolumns).
The graph in Figure 1 shows that 1Fmeasure on three test sets are very close to eachother.
This is also true the approaches with POStag trigram model as showed in Table 7~9.
Thisprovides evidences for the argument that the ap-proaches can produce stable performance withthe NIL corpus.
Differently, as reported in (Xiaet.
al., 2005a), performance achieved in SVMclassifier is rather unstable.
It performs poorlywith training set C#1 which contains BBS textposted several months ago, but much better withtraining set C#5 which contains the latest chattext.53Word trigram POS tag trigramconfidence entropy confidence entropy Corpusp r f p r f p r f p r fCNGIGA 0.685 0.737 0.710 0.722 0.761 0.741 0.614 0.654 0.633 0.637 0.664 0.650PKU 0.699 0.712 0.705 0.701 0.738 0.719 0.619 0.630 0.624 0.625 0.648 0.636CNTB 0.653 0.661 0.657 0.692 0.703 0.697 0.651 0.673 0.662 0.684 0.679 0.681Table 6: Results of anomaly detection using standard Chinese corpora on test set #2.Word trigram POS tag trigramconfidence entropy confidence entropy Corpusp r f p r f p r f p r fCNGIGA 0.821 0.836 0.828 0.857 0.849 0.853 0.653 0.657 0.655 0.672 0.678 0.675PKU 0.818 0.821 0.819 0.838 0.839 0.838 0.672 0.672 0.672 0.688 0.679 0.683CNTB 0.791 0.787 0.789 0.821 0.811 0.816 0.691 0.679 0.685 0.712 0.688 0.700Table 7: Results of anomaly detection incorporating NIL corpus on test set #2Word trigram POS tag trigramconfidence entropy confidence entropy Corpusp r f p r f p r f p r fCNGIGA 0.819 0.841 0.830 0.849 0.848 0.848 0.657 0.659 0.658 0.671 0.677 0.674PKU 0.812 0.822 0.817 0.835 0.835 0.835 0.663 0.671 0.667 0.687 0.681 0.684CNTB 0.801 0.783 0.792 0.822 0.803 0.812 0.689 0.677 0.683 0.717 0.689 0.703Table 8: Results of anomaly detection incorporating NIL corpus on test set #3Word trigram POS tag trigramconfidence entropy confidence entropy Corpusp r f p r f p r f p r fCNGIGA 0.824 0.839 0.831 0.852 0.845 0.848 0.651 0.654 0.652 0.674 0.674 0.674PKU 0.815 0.825 0.820 0.836 0.84 0.838 0.668 0.668 0.668 0.692 0.682 0.687CNTB 0.796 0.785 0.790 0.817 0.807 0.812 0.694 0.681 0.687 0.713 0.686 0.699Table 9: Results of anomaly detection incorporating NIL corpus on test set #40.000.100.200.300.400.500.600.700.800.90conf-CNGIGA-wordent-CNGIGA-wordconf-PKU-word ent-PKU-word conf-CNTB-word ent-CNTB-wordExp-II-#2Exp-III-#2Exp-III-#3Exp-III-#4Figure 1: Comparisons on 1F  measure of the approaches with word trigram on test set #2, #3 and #4 inexperiment II and experiment III.We finally compare performance of our ap-proaches against the one described in (Xia, et.al., 2005a).
The best 1F  measure achieved in ourwork, i.e.
0.
853, is close to the best one in theirwork, i.e.
0.871 with training corpus C#5.
Thisproves another argument that our approaches canproduce equivalent performance to the best onesachieved by the approaches in existence.6 ConclusionsThe new approaches to detecting anomalousChinese chat text are proposed in this paper.
Theapproaches calculate confidence and entropyvalues with the language models constructed onnegative training samples in three standard Chi-54nese corpora.
To improve detection quality, weincorporate positive training samples in NIL cor-pus in our approaches.
Two conclusions can bemade based on this work.
Firstly, 1F  measure ofanomaly detection can be improved by around0.10 when NIL corpus is incorporated into theapproaches.
Secondly, performance equivalent tothe best ones produced by the approaches in exis-tence can be achieved stably by incorporating thestandard Chinese corpora and the NIL corpus.We believe some strong evidences for ourclaims can be obtained by training our ap-proaches with more chat text corpora which con-tain chat text created in different time periods.We are conducting this experiment seeks to findout whether and how our approaches are inde-pendent of time.
This work is still progressing.
Areport on this issue will be available shortly.
Wealso plan to investigate how size of chat text cor-pus influences performance of our approaches.The goal is to find the optimal size of chat textcorpus which can achieve the best performance.The readers should also be noted that evaluationin this work is a within-domain test.
Due toshortage of chat text resources, no cross-domaintest is conducted.
In the future cross-domain test,we will investigate how our approaches are inde-pendent of domain.Eventual goal of chat text processing is to nor-malize the anomalous chat text, namely, convertit to standard text holding the same meaning.
Sothe work carried out in this paper is the first stepleading to this goal.
Approaches will be designedto locate the anomalous terms in chat text andmap them to standard words.AcknowledgementResearch described in this paper is partially sup-ported by the Chinese University of Hong Kongunder the Direct Grant Scheme (No: 2050330)and Strategic Grant Scheme project (No:4410001) respectively.ReferenceBrown, P. F., V. J. Della Pietra, P. V. de Souza, J. C.Lai, and R. L. Mercer.
1990.
Class-based n-grammodels of natural language.
In Proceedings of theIBM Natural Language ITL, Paris, France.Finkelhor, D., K. J. Mitchell, and J. Wolak.
2000.Online Victimization: A Report on the Nation'sYouth.
Alexandria, Virginia: National Center forMissing & Ex-ploited Children, page ix.German News.
2004.
Germans are world SMS cham-pions, 8 April 2004, http://www.expatica.com/source/site_article.asp?subchannel_id=52&story_id=6469.Gianforte, G.. 2003.
From Call Center to ContactCenter: How to Successfully Blend Phone, Email,Web and Chat to Deliver Great Service and SlashCosts.
RightNow Technologies.Heard-White, M., Gunter Saunders and Anita Pincas.2004.
Report into the use of CHAT in education.Final report for project of Effective use of CHATin Online Learning, Institute of Education, Univer-sity of London.McCullagh, D.. 2004.
Security officials to spy on chatrooms.
News provided by CNET Networks.
No-vember 24, 2004.Xia, Y., K.-F. Wong and W. Gao.
2005a.
NIL is notNothing: Recognition of Chinese Network Infor-mal Language Expressions, 4th SIGHAN Work-shop on Chinese Language Processing atIJCNLP'05, pp95-102.Xia, Y., K.-F. Wong and R. Luk.
2005b.
A Two-StageIncremental Annotation Approach to ConstructingA Network Informal Language Corpus.
In Proc.
ofNTCIR-5 Meeting, pp.
529-536.Zhang, Z., H. Yu, D. Xiong and Q. Liu.
2003.
HMM-based Chinese Lexical Analyzer ICTCLAS.SIGHAN?03 within ACL?03, pp.
184-187.55
