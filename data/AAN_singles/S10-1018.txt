Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 92?95,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsSUCRE: A Modular System for Coreference ResolutionHamidreza Kobdani and Hinrich Sch?utzeInstitute for Natural Language ProcessingUniversity of Stuttgart, Germanykobdani@ims.uni-stuttgart.deAbstractThis paper presents SUCRE, a new soft-ware tool for coreference resolution andits feature engineering.
It is able to sep-arately do noun, pronoun and full coref-erence resolution.
SUCRE introduces anew approach to the feature engineeringof coreference resolution based on a rela-tional database model and a regular featuredefinition language.
SUCRE successfullyparticipated in SemEval-2010 Task 1 onCoreference Resolution in Multiple Lan-guages (Recasens et al, 2010) for goldand regular closed annotation tracks of sixlanguages.
It obtained the best results inseveral categories, including the regularclosed annotation tracks of English andGerman.1 IntroductionIn this paper, we introduce a new software toolfor coreference resolution.
Coreference resolutionis the process of finding discourse entities (mark-ables) referring to the same real-world entity orconcept.
In other words, this process groups themarkables of a document into equivalence classes(coreference entities) so that all markables in anentity are coreferent.There are various publicly available systemsthat perform coreference resolution, such asBART (Versley et al, 2008) and GUITAR (Stein-berger et al, 2007).
A considerable engineeringeffort is needed for the full coreference resolutiontask, and a significant part of this effort concernsfeature engineering.
Thus, a system which is ableto extract the features based on a feature defini-tion language can help the researcher reduce theimplementation effort needed for feature extrac-tion.
Most methods of coreference resolution, ifproviding a baseline, usually use a feature set sim-ilar to (Soon et al, 2001) or (Ng and Cardie, 2002)and do the feature extraction in the preprocessingstage.
SUCRE has been developed to provide amore flexible method for feature engineering ofcoreference resolution.
It has a novel approach tomodel an unstructured text corpus in a structuredframework by using a relational database modeland a regular feature definition language to defineand extract the features.
Relational databases area well-known technology for structured data mod-eling and are supported by a wide array of soft-ware and tools.
Converting a text corpus to/fromits equivalent relational database model is straight-forward in our framework.A regular language for feature definition is avery flexible method to extract different featuresfrom text.
In addition to features defined di-rectly in SUCRE, it accepts also externally ex-tracted/generated features.
Its modular architec-ture makes it possible to use any externally avail-able classification method too.
In addition to linkfeatures (features related to a markable pair), itis also possible to define other kinds of features:atomic word and markable features.
This ap-proach to feature engineering is suitable not onlyfor knowledge-rich but also for knowledge-poordatasets.
It is also language independent.
The re-sults of SUCRE in SemEval-2010 Task 1 show thepromise of our framework.2 ArchitectureThe architecture of SUCRE has two main parts:preprocessing and coreference resolution.In preprocessing the text corpus is converted toa relational database model.
These are the mainfunctionalities in this stage:1.
Preliminary text conversion2.
Extracting atomic word features3.
Markable detection92Column CharacteristicWord TableWord-ID Primary KeyDocument-ID Foreign KeyParagraph-ID Foreign KeySentence-ID Foreign KeyWord-String AttributeWord-Feature-0 AttributeWord-Feature-1 Attribute... AttributeWord-Feature-N AttributeMarkable TableMarkable-ID Primary KeyBegin-Word-ID Foreign KeyEnd-Word-ID Foreign KeyHead-Word-ID Foreign KeyMarkable-Feature-0 AttributeMarkable-Feature-1 Attribute... AttributeMarkable-Feature-N AttributeLinks TableLink-ID Primary KeyFirst-Markable-ID Foreign KeySecond-Markable-ID Foreign KeyCoreference-Status AttributeStatus-Confidence-Level AttributeTable 1: Relational Database Model of Text Corpus4.
Extracting atomic markable featuresAfter converting (modeling) the text corpus tothe database, coreference resolution can be per-formed.
Its functional components are:1.
Relational Database Model of Text Corpus2.
Link Generator3.
Link Feature Extractor4.
Learning (Applicable on Train Data)5.
Decoding (Applicable on Test Data)2.1 Relational Database Model of TextCorpusThe Relational Database model of text thev cor-pus is an easy to generate format.
Three tables areneeded to have a minimum running system: Word,Markable and Link.Table 1 presents the database model of the textcorpus.
In the word table, Word-ID is the indexof the word, starting from the beginning of thecorpus.
It is used as the primary key to uniquelyidentify each token.
Document-ID, Paragraph-IDand Sentence-ID are each counted from the be-ginning of the corpus, and also act as the foreignkeys pointing to the primary keys of the docu-ment, paragraph and sentence tables, which areoptional (the system can also work without them).It is obvious that the raw text as well as any otherformat of the corpus can be generated from theword table.
Any word features (Word-Feature-#Xcolumns) can be defined and will then be addedto the word table in preprocessing.
In the mark-able table, Markable-ID is the primary key.
Begin-Word-ID, End-Word-ID and Head-Word-ID referto the word table.
Like the word features, themarkable features are not mandatory and in thepreprocessing we can decide which features areadded to the table.
In the link table, Link-ID isthe primary key; First-Markable-ID and Second-Markable-ID refer to the markable table.2.2 Link GeneratorFor training, the system generates a positive train-ing instance for each adjacent coreferent markablepair and negative training instances for a markablem and all markables disreferent with m that occurbefore m (Soon et al, 2001).
For decoding it gen-erates all the possible links inside a window of 100markables.2.3 Link Feature ExtractorThere are two main categories of features inSUCRE: Atomic Features and Link FeaturesWe first explain atomic features in detail andthen turn to link features and the extraction methodwe use.Atomic Features: The current version ofSUCRE supports the atomic features of wordsand markables but in the next versions we aregoing to extend it to sentences, paragraphs anddocuments.
An atomic feature is an attribute.
Forexample the position of the word in the corpusis an atomic word feature.
Atomic word featuresare stored in the columns of the word table calledWord-Feature-X.In addition to word position in the corpus, doc-ument number, paragraph number and sentencenumber, the following are examples of atomicword features which can be extracted in prepro-cessing: Part of speech tag, Grammatical Gen-der (male, female or neutral), Natural Gender(male or female), Number (e.g.
singular, plural orboth), Semantic Class, Type (e.g.
pronoun types:personal, reflexive, demonstrative ...), Case (e.g.nominative, accusative, dative or genitive in Ger-man) and Pronoun Person (first, second or third).Other possible atomic markable features include:93number of words in markable, named entity, alias,syntactic role and semantic class.For sentences, the following could be extracted:number of words in the sentence and sentencetype (e.g.
simple, compound or complex).
Forparagraphs these features are possible: number ofwords and number of sentences in the paragraph.Finally, examples of document features includedocument type (e.g.
news, article or book), num-ber of words, sentences and paragraphs in the doc-ument.Link Features: Link features are defined over apair of markables.
For link feature extraction, thehead words of the markables are usually used, butin some cases the head word may not be a suitablechoice.
For example, consider the two markablesthe books and a book.
In both cases book is thehead word, but to distinguish which markable isdefinite and which indefinite, the article must betaken into account.
Now consider the two mark-ables the university student from Germany and theuniversity student from France.
In this case, thehead words and the first four words of each mark-able are the same but they can not be coreferent;this can be detected only by looking at the lastwords.
Sometimes we need to consider all wordsin the two markables, or even define a feature fora markable as a unit.
To cover all such caseswe need a regular feature definition language withsome keywords to select different word combina-tions of two markables.
For this purpose, we de-fine the following variables.
m1 is the first mark-able in the pair.
m1b, m1e and m1h are the first,last and head words of the first markable in thepair.
m1a refers to all words of the first markablein the pair.
m2, m2b, m2e, m2h and m2a havethe same definitions as above but for the secondmarkable in the pair.In addition to the above keywords there aresome other keywords that this paper does not haveenough space to mention (e.g.
for accessing theconstant values, syntax relations or roles).
Thecurrently available functions are: exact- and sub-string matching (in two forms: case-sensitive andcase-insensitive), edit distance, alias, word rela-tion, markable parse tree path, absolute value.Two examples of link features are as follows:?
(seqmatch(m1a,m2a) > 0)&& (m1h.f0 == f0.N )&& (m2h.f0 == f0.N )means that there is at least one exact matchbetween the words of the markables and thatthe head words of both are nouns (f0 meansWord-Feature-0, which is part of speech inour system).?
(abs(m2b.stcnum?m1b.stcnum) == 0)&& (m2h.f3 == f3.reflexive)means that two markables are in the samesentence and that the type of the sec-ond markable head word is reflexive (f3means Word-Feature-3, which is morpholog-ical type in our system).2.4 LearningThere are four classifiers integrated in SUCRE:Decision-Tree, Naive-Bayes, Support Vector Ma-chine (Joachims, 2002) and Maximum-Entropy(Tsuruoka, 2006).When we compared these classifiers, the bestresults, which are reported in Section 3, wereachieved with the Decision-Tree.2.5 DecodingIn decoding, the coreference chains are created.SUCRE uses best-first clustering for this purpose.It searches for the best predicted antecedent fromright-to-left starting from the end of the document.3 ResultsTable 2 shows the results of SUCRE and the bestcompetitor system on the test portions of the sixlanguages from SemEval-2010 Task 1.
Four dif-ferent evaluation metrics were used to rank theparticipating systems: MUC (Vilain et al, 1995),B3(Bagga and Baldwin, 1998), CEAF (Luo,2005) and BLANC (Recasens and Hovy, in prep).SUCRE has the best results in regular closedannotation track of English and German (for allmetrics).
Its results for gold closed annotationtrack of both English and German are the bestin MUC and BLANC scoring metrics (MUC: En-glish +27.1 German +32.5, BLANC: English +9.5German +9.0) and for CEAF and B3(CEAF: En-glish -1.3 German -4.8, B3: English -2.1 German-4.8); in comparison to the second ranked sys-tem, the performance is clearly better in the firstcase and slightly better in the second.
This re-sult shows that SUCRE has been optimized in away that achieves good results on the four differentscoring metrics.
We view this good performanceas a demonstration of the strength of SUCRE: our94method of feature extraction, definition and tuningis uniform and can be optimized and applied to alllanguages and tracks.Results of SUCRE show a correlation betweenthe MUC and BLANC scores (the best MUCscores of all tracks and the best BLANC scores in11 tracks of a total 12), in our opinion this correla-tion is not because of the high similarity betweenMUC and BLANC, but it is because of the bal-anced scores.Language ca de en es it nlSystem SUCRE (Gold Annotation)MD-F1 100 100 100 100 98.4 100CEAF-F1 68.7 72.9 74.3 69.8 66.0 58.8MUC-F1 56.2 58.4 60.8 55.3 45.0 69.8B3-F1 77.0 81.1 82.4 77.4 76.8 67.0BLANC 63.6 66.4 70.8 64.5 56.9 65.3System SUCRE (Regular Annotation)MD-F1 69.7 78.4 80.7 70.3 90.8 42.3CEAF-F1 47.2 59.9 62.7 52.9 61.3 15.9MUC-F1 37.3 40.9 52.5 36.3 50.4 29.7B3-F1 51.1 64.3 67.1 55.6 70.6 11.7BLANC 54.2 53.6 61.2 51.4 57.7 46.9System Best Competitor (Gold Annotation)MD-F1 100 100 100 100 N/A N/ACEAF-F1 70.5 77.7 75.6 66.6 N/A N/AMUC-F1 42.5 25.9 33.7 24.7 N/A N/AB3-F1 79.9 85.9 84.5 78.2 N/A N/ABLANC 59.7 57.4 61.3 55.6 N/A N/ASystem Best Competitor (Regular Annotation)MD-F1 82.7 59.2 73.9 83.1 55.9 34.7CEAF-F1 57.1 49.5 57.3 59.3 45.8 17.0MUC-F1 22.9 15.4 24.6 21.7 42.7 8.3B3-F1 64.6 50.7 61.3 66.0 46.4 17.0BLANC 51.0 44.7 49.3 51.4 59.6 32.3Table 2: Results of SUCRE and the best competitor system.Bold F1 scores indicate that the result is the best SemEvalresult.
MD: Markable Detection, ca: Catalan, de: German,en:English, es: Spanish, it: Italian, nl: Dutch4 ConclusionIn this paper, we have presented a new modularsystem for coreference resolution.
In comparisonwith the existing systems the most important ad-vantage of our system is its flexible method of fea-ture engineering based on relational database and aregular feature definition language.
There are fourclassifiers integrated in SUCRE: Decision-Tree,Naive-Bayes, SVM and Maximum-Entropy.
Thesystem is able to separately do noun, pronoun andfull coreference resolution.
The system uses best-first clustering.
It searches for the best predictedantecedent from right-to-left starting from the endof the document.ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In In The First Interna-tional Conference on Language Resources and Eval-uation Workshop on Linguistics Coreference, pages563?566.Thorsten Joachims.
2002.
Learning to Classify TextUsing Support Vector Machines, Methods, Theory,and Algorithms.
Kluwer/Springer.Xiaoqiang Luo.
2005.
On coreference resolution per-formance metrics.
In HLT ?05: Proceedings ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Process-ing, pages 25?32, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the ACL, pages 104?111.Marta Recasens and Eduard Hovy.
in prep.
BLANC:Implementing the Rand Index for Coreference Eval-uation.Marta Recasens, Llu?
?s M`arquez, Emili Sapena,M.Ant`onia Mart?
?, Mariona Taul?e, V?eronique Hoste,Massimo Poesio, and Yannick Versley.
2010.SemEval-2010 Task 1: Coreference resolution inmultiple languages.
In Proceedings of the 5thInternational Workshop on Semantic Evaluations(SemEval-2010), Uppsala, Sweden.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.
InComputational Linguistics, pages 521?544.Josef Steinberger, Massimo Poesio, Mijail A. Kabad-jovb, and Karel Jezek.
2007.
Two uses of anaphoraresolution in summarization.
In Information Pro-cessing and Management, Special issue on Summa-rization, pages 1663?1680.Yoshimasa Tsuruoka.
2006.
A simple c++ libraryfor maximum entropy classification.
Tsujii labora-tory, Department of Computer Science, University ofTokyo.Yannick Versley, Simone Paolo Ponzetto, MassimoPoesio, Vladimir Eidelman, Alan Jern, Jason Smith,and Xiaofeng Yang.
2008.
Bart: A modular toolkitfor coreference resolution.
In Proceedings of the46nd Annual Meeting of the Association for Com-putational Linguistics, pages 9?12.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In MUC6?95: Proceedings of the 6th conference on Messageunderstanding, pages 45?52, Morristown, NJ, USA.Association for Computational Linguistics.95
