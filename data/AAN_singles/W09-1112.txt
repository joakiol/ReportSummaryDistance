Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 84?92,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsMinimally Supervised Model of Early Language AcquisitionMichael ConnorDepartment of Computer ScienceUniversity of Illinoisconnor2@uiuc.eduYael GertnerDepartment of PsychologyUniversity of Illinoisygertner@cyrus.psych.uiuc.eduCynthia FisherDepartment of PsychologyUniversity of Illinoiscfisher@cyrus.psych.uiuc.eduDan RothDepartment of Computer ScienceUniversity of Illinoisdanr@uiuc.eduAbstractTheories of human language acquisition as-sume that learning to understand sentences isa partially-supervised task (at best).
Insteadof using ?gold-standard?
feedback, we traina simplified ?Baby?
Semantic Role Labelingsystem by combining world knowledge andsimple grammatical constraints to form a po-tentially noisy training signal.
This combina-tion of knowledge sources is vital for learn-ing; a training signal derived from a singlecomponent leads the learner astray.
When thislargely unsupervised training approach is ap-plied to a corpus of child directed speech, theBabySRL learns shallow structural cues thatallow it to mimic striking behaviors found inexperiments with children and begin to cor-rectly identify agents in a sentence.1 IntroductionSentence comprehension involves assigning seman-tic roles to sentence constituents, determining whodoes what to whom.
How do young children be-gin learning to interpret sentences?
The structure-mapping view of early verb and syntax acquisitionproposes that children treat the number of nounsin the sentence as a cue to its semantic predicate-argument structure (Fisher, 1996), and represent lan-guage experience in an abstract format that promotesgeneralization to new verbs (Gertner et al, 2006).Theories of human language acquisition assumethat learning to understand sentences is naturallya partially-supervised task: the fit of the learner?spredicted meaning with the referential context andbackground knowledge provides corrective feed-back (e.g., Pinker (1989)).
But this feedback mustbe noisy; referential scenes provide ambiguous in-formation about the semantic roles of sentence par-ticipants.
For example, the same participant couldbe construed as an agent who ?fled?
or as a patientwho is ?chased?.In this paper, we address this problem by de-signing a Semantic Role Labeling system (SRL),equipped with shallow representations of sentencestructure motivated by the structure-mapping ac-count, that learns with no gold-standard feedback atall.
Instead, the SRL provides its own internally-generated feedback based on a combination of worldknowledge and linguistic constraints.
As a sim-ple stand-in for world knowledge, we assume thatthe learner has animacy information for some set ofnouns, and uses this knowledge to determine theirlikely roles.
In terms of linguistic constraints, thelearner uses simple knowledge about the possible ar-guments verbs can appear with.This approach has two goals.
The first is to in-form theories of language learning by investigatingthe utility of the proposed internally-generated feed-back as one component of the human learner?s tools.Second, from an NLP and Machine Learning per-spective we propose to inject information into a su-pervised learning algorithm through a channel otherthan labeled training data.
From both perspectives,our key question is whether the algorithm can usethese internally labeled examples to extract generalpatterns that can be applied to new cases.By building a model that uses shallow representa-tions of sentences and minimal feedback, but that84mimics features of language development in chil-dren, we can explore the nature of initial representa-tions of syntactic structure.1.1 BackgroundThe structure-mapping account of early verb andsyntax acquisition makes strong predictions.
First,it predicts early use of simple structural cues to sen-tence interpretation.
As soon as children can iden-tify some nouns, they should assign different in-terpretations to transitive and intransitive sentences,simply by assuming that each noun in the sentencebears a distinct semantic role.
Similarly, language-specific syntactic learning should transfer rapidly tonew verbs.
Second, however, this account predictsstriking errors.
In ?Fred and Ginger danced?, anintransitive verb occurs with two nouns.
If chil-dren interpret any two-noun sentence as if it weretransitive, they should mistakenly interpret the orderof two nouns in such conjoined-subject intransitivesentences as agent-patient.
Experiments with youngchildren support these predictions.
21-month-oldsuse the number of nouns to understand sentencescontaining new verbs (Yuan et al, 2007), generalizewhat they have learned about transitive word-orderto new verbs (Gertner et al, 2006), and make thepredicted error, treating intransitive sentences con-taining two nouns as if they were transitive (Gert-ner and Fisher, 2006).
By 25 months, children havelearned enough about English syntax to interpretconjoined-subject intransitives differently from tran-sitives (Naigles, 1990).Our previous computational experiments with asystem for automatic semantic role labeling (Con-nor et al, 2008) suggest that it is possible to learnto assign basic semantic roles based on the simplerepresentations proposed by the structure-mappingview.
The classifier?s features were limited to lexicalinformation (nouns and verbs only) and the numberand order of nouns in the sentence, and trained on asample of child-directed speech annotated in Prop-Bank (Kingsbury and Palmer, 2002) style.
Giventhis training, our classifier learned to label the firstof two nouns as an agent and the second as a patient.Even amid the variability of casual speech, simplyrepresenting the target word as the first or the secondof two nouns significantly boosts SRL performance(relative to a lexical baseline) on transitive sentencescontaining novel verbs.
This result depends on keyassumptions of the structure-mapping view, includ-ing abstract representations of semantic roles, andabstract but simple representations of sentence struc-ture.
Another approach was taken by (Alishahi andStevenson, 2007).
Their model learned to assign se-mantic roles without prior knowledge of abstract se-mantic roles.
Instead, it relied on built-in syntacticknowledge and a rich hierarchical representation ofsemantic knowledge to learn links between sentencestructure and meaning.However, our previous experimental design hasa serious drawback that limits its relevance to thestudy of how children learn their first language.In training, our SRL received gold standard feed-back consisting of correctly labeled sentences.
Thuswhen the SRL made a mistake in identifying the se-mantic role of any noun in a sentence, it receivedfeedback about the ?true?
semantic role of this noun.As noted above, this is an unrealistic assumption forthe input to human learners.Here we ask whether an SRL could learn to in-terpret simple sentences even without gold-standardfeedback by relying on world knowledge to gen-erate its own feedback.
This internally-generatedfeedback was based on the following assumptions.First, nouns referring to animate entities are likelyto be agents, and nouns referring to inanimate en-tities are not.
Second, each predicate takes at mostone agent.
Such role uniqueness constraints are typ-ically included in linguistic discussions of thematicroles (Bresnan, 1982; Carlson, 1998).
The animacyheuristic is not always correct, of course.
For ex-ample, in ?The door hit you?, an inanimate objectis the agent of action, and an animate being is thepatient.
Nevertheless, it is useful for two reasons.First, there is a strong cross-linguistic associationbetween agency and animacy (Aissen, 1999; Dowty,1991).
Second, from the first year of life, childrenhave strong expectations about the capacities of an-imate and inanimate entities (Baillargeon et al, inpress).
Given the universal tendency for speakers totalk about animate action on less animate objects,many sentences will present useful training data tothe SRL: In ordinary sentences such as ?You brokeit,?
feedback generated based on animacy will re-semble gold-standard feedback.852 Learning ModelOur learning task is similar to the full SRL task (Car-reras and Ma`rquez, 2004), except that we classifythe roles of individual words rather than full phrases.A full automatic SRL system (e.g.
(Punyakanok etal., 2005a)) typically involves multiple stages to 1)parse the input, 2) identify arguments, 3) classifythose arguments, and then 4) run inference to makesure the final labeling for the full sentence does notviolate any linguistic constraints.
Our simplifiedBabySRL architecture essentially replaces the firsttwo steps with developmentally plausible heuris-tics.
Rather than identifying arguments via a learnedclassifier with access to a full syntactic parse, theBabySRL treats each noun in the sentence as a can-didate argument and assigns a semantic role to it.
Asimple heuristic collapsed compound or sequentialnouns to their final noun, an approximation of thehead noun of the noun phrase.
For example, ?Mr.Smith?
was treated as the single noun ?Smith?.
Othercomplex noun phrases were not simplified in thisway.
Thus, a phrase such as ?the toy on the floor?would be treated as two separate nouns, ?toy?
and?floor?.
This represents the assumption that youngchildren know ?Mr.
Smith?
is a single name, butthey do not know all the predicating terms that maylink multiple nouns into a single noun phrase.
Thesimplified learning task of the BabySRL implementsa key assumption of the structure-mapping account:that at the start of multiword sentence comprehen-sion children can tell which words in a sentence arenouns (Waxman and Booth, 2001), and treat eachnoun as a candidate argument.We further simplify the SRL task such that clas-sification is between two macro-roles: A0 (agent)and A1 (non-agent; all non-A0 arguments).
We didso because we reason that this simplified feedbackscheme can be primarily informative for a first stageof learning in which learners identify how their lan-guage identifies agents vs. non-agents in sentences.In addition, this level of role granularity is more con-sistent across verbs (Palmer et al, 2005).For argument classification we use a linear clas-sifier trained with a regularized perceptron updaterule (Grove and Roth, 2001).
This learning algo-rithm provides a simple and general linear classifierthat works well in other language tasks, and allowsus to inspect the weights of key features to determinetheir importance for classification.For the final predictions, the classifier usespredicate-level inference to ensure coherent argu-ment assignments.
In our task the only active con-straints are that all nouns require a tag, and that theyhave unique labels, which for this restricted case ofA0 vs. not A0 means there will be only one agent.2.1 Training and FeedbackThe key feature of our BabySRL lies in the wayfeedback is provided.
Ordinarily, during training,SRL classifiers predict a semantic label for an argu-ment and receive gold-standard feedback about itscorrect semantic role.
Such accurate feedback is notavailable for the child learner.
Children must rely ontheir own error-prone interpretation of events to sup-ply feedback.
This internally-generated feedbacksignal is presumably derived from multiple infor-mation sources, including the plausibility of partic-ular combinations of argument-roles given the cur-rent situation (Chapman and Kohn, 1978).
Herewe model this process by combining backgroundknowledge with linguistic constraints to generatea training signal.
The ?unsupervised?
feedback isbased on: 1) nouns referring to animate entities areassumed to be agents, while nouns referring to inan-imate entities are non-agents and 2) each predicatecan have at most one agent.This internally-generated feedback bears somesimilarities to Inference Based Training (Pun-yakanok et al, 2005b).
In both cases the feedback tolocal supervised classifiers depends on global con-straints.
With IBT, feedback for mistakes is onlyconsidered after global inference, but for BabySRLthe global inference is applied to the feedback itself.Figure 1 gives an overview of the training and test-ing procedure, making clear the distinction betweentraining and testing inference.The training data were samples of parental speechto one child (?Sarah?
; (Brown, 1973), availablevia Childes (MacWhinney, 2000)).
We trainedon parental utterances in samples 1 through 80,recorded at child age 2;3-3;10 years.
All verb-containing utterances without symbols indicatinglong pauses or unintelligible words were automat-ically parsed with the Charniak parser (Charniak,1997) and annotated using an existing SRL sys-86tem (Punyakanok et al, 2005a).
In this initialpass, sentences with parsing errors that misidenti-fied argument boundaries were excluded.
Role la-bels were hand-corrected using the PropBank anno-tation scheme.
The child-directed speech trainingset consists of about 8300 tagged arguments over4700 sentences, of which a majority had a singleverb and two labeled nouns1.
The annotator agree-ment on this data set ranged between 95-97% at thelevel of arguments.
In the current paper these role-tagged examples provide a comparison point for theutility of animacy-based feedback during training.Our BabySRL did not receive these hand-corrected semantic roles during training.
Instead,for each training example it generated its own feed-back based in part on an animacy table.
To ob-tain the animacy table we coded the 100 most fre-quent nouns in our corpus (which constituted lessthan 15% of the total number of nouns, but 65%of noun occurrences).
We considered 84 of thesenouns to be unambiguous in animacy: Personal pro-nouns and nouns referring to people were coded asanimate (30).
Nouns referring to objects, body parts,locations, and times, were coded as inanimate (54).The remaining 16 nouns were excluded because theywere ambiguous in animacy (e.g., dolls, actions).We test 3 levels of feedback representing increas-ing amounts of linguistic knowledge used to gener-ate internal interpretations of the sentences.
Usingthe animacy table, Animacy feedback (Feedback 1)was generated as follows: for each noun in training,if it was coded as animate it was labeled A0, if it wascoded as inanimate it was labeled A1, otherwise nofeedback was given.
Because of the frequency of an-imate nouns this gives a skewed distribution of 4091animate agents and 1337 inanimate non-agents.
(Feedback 2) builds on Feedback 1 by adding an-other linguistic constraint: if a noun was not foundin the animacy-table and there is another noun in thesentence that is labeled A0, then the unknown nounis an A1.
In the training set this adds non-agenttraining examples, yielding 4091 A0 and 2627 A1examples.Feedback 1 and Feedback 2 allow two nouns ina sentence to be labeled with A0.
Feedback 3 pre-1Corpus available at http://l2r.cs.uiuc.edu/?cogcompvents this; it implements a unique agent constraintthat incorporates bootstrapping to make an ?intelli-gent guess?
about which noun is the correct agent.This decision is made based on the current predic-tions of the classifier.
Given a sentence with multi-ple animate nouns, the classifier predicts a label foreach, and the one with the highest score for A0 isdeclared the true agent and the rest are classified asnon-agent.
Note that we cannot apply role unique-ness to the A1 (not A0) role, given that this label en-compasses multiple non-agent roles.
This feedbackscheme, allowing at most one agent per sentence, re-duces the number of A0 examples and increases thenumber of A1 examples to 3019 A0 and 3699 A1.2.2 Feature SetsThe basic feature we propose is the noun pattern fea-ture (NPattern).
We hypothesize that children usethe number and order of nouns to represent argumentstructure.
The NPattern feature indicates how manynouns there are in the sentence and which noun thetarget is.
For example, in the two-noun sentence?Did you see it?
?, ?you?
has a feature active indicat-ing that it is the first noun of two.
Likewise, for ?it?
afeature is active indicating that it is the second of twonouns.
This feature is easy to compute once nounsare identified, and does not require fine-grained part-of-speech distinctions.We compare the noun pattern feature to a baselinelexical feature set (Words): the target noun and theroot form of the predicate.
The NPattern feature setincludes lexical features as well as features indicat-ing the number and order of the noun (first of two,second of three, etc.).
With gold-standard role feed-back, (Connor et al, 2008) found that the NPatternfeature allowed the BabySRL to generalize to newverbs: it increased the system?s tendency to predictthat the first of two nouns was A0 and the second oftwo nouns A1 for verbs not seen in training.To the extent that in child-directed speech the firstof two nouns tends to be an agent, and agents tendto be animate, we anticipate that with the NPat-tern feature the BabySRL will learn the same thing,even when provided with internally-generated feed-back based on animacy.
In Connor et al (2008) weshowed that, because this NPattern feature set repre-sents only the number and order of nouns, with thisfeature set the BabySRL reproduced the errors chil-87Algorithm BABYSRL TRAININGINPUT: Unlabeled Training SentencesOUTPUT: Trained Argument ClassifierFor each training sentenceGenerate Internal Feedback: Find interpreted meaningFeedback 1: Apply Animacy HeuristicFor each argument in the sentence (noun)If noun is animate?
mark as agentIf noun is inanimate?
mark as non-agentelse leave unknownendFeedback 2: Known agent constraintBeginning with Feedback 1If an agent was foundMark all unknown arguments as non-agentFeedback 3: Unique agent constraintBeginning with Feedback 2If multiple agents foundFind argument with highest agent predictionLeave this argument an agent, mark rest as non-agentTrain Supervised ClassifierPresent each argument to classifierUpdate if interpreted meaning does not matchclassifier predictionend(a) TrainingAlgorithm BABYSRL TESTINGINPUT: Unlabeled Testing SentencesOUTPUT: Role labels for each argumentFor each test sentencePredict roles for each argumentTest Inference:Find assignment to whole sentence with highest sum ofpredictions that doesn?t violate uniqueness constraintend(b) TestingFigure 1: BabySRL training and testing procedures.
In-ternal feedback is generated using animacy plus optionalconstraints.
This feedback is fed to a supervised learningalgorithm to create an agent-identification classifier.dren make as noted in the Introduction, mistakenlyassigning agent- and non-agent roles to the first andsecond nouns in intransitive test sentences contain-ing two nouns.
In the present paper, the linguisticconstraints provide an additional cause for this er-ror.
In addition, as a first step in examining recov-ery from the predicted error, Connor et al (2008)added a verb position feature (VPosition) specifyingwhether the target noun is before or after the verb.Given these features, the BabySRL?s classificationof transitive and two-noun intransitive test sentencesdiverged, because the gold-standard training sup-ported the generalization that pre-verbal nouns tendto be agents, and post-verbal nouns tend to be pa-tients.
In the present paper we include the VPositionfeature for comparison to Connor et al (2008).2.3 TestingTo evaluate the BabySRL we tested it with both aheld-out sample of child-directed speech, and withconstructed sentences containing novel verbs, likethose used in the experiments with children de-scribed above.
These sentences provide a morestringent test of generalization than the customarytest on a held-out section of the data.
Although theheld-out section of data contains unseen sentences,it may contain few unseen verbs.
In a held out sec-tion of our data, 650 out of 696 test examples containa verb that was encountered in training.
Therefore,the customary test cannot tell us whether the systemgeneralizes what it learned to novel verbs.All constructed test sentences contained a novelverb (?gorp?).
We constructed two test sentence tem-plates: ?A gorps B?
and ?A and B gorp?, where A andB were replaced with nouns that appeared more thantwice in training.
For each test sentence template webuilt a test set of 100 sentences by randomly sam-pling nouns in two different ways described next.Full distribution: The first nouns in the test sen-tences (A) are chosen from the set of all first nounsin our corpus, taking their frequency into accountwhen sampling.
The second nouns in the sentences(B) are chosen from the set of nouns appearing assecond nouns in the sentence of our corpus.
Thisway of sampling the nouns will maximize the SRL?stest performance based on the baseline feature setof lexical information alone (Words).
This is so be-cause in our data many sentences have an animatefirst noun and an inanimate second noun.
Based onthese words alone the SRL could learn to predict anA0-A1 role sequence for our test sentences.
Nev-ertheless, we expect that when the BabySRL is alsogiven the NPattern feature it should be able to per-form better than this high lexical baseline.Two animate nouns: In these test sentences theA and B nouns are chosen from our list of animatenouns.
We chose nouns from this list that werefairly frequent (ranging from 8 to 240 uses in the88corpus), and that occurred roughly equally as thefirst and second noun.
This mimics the sentencesused in the experiments with children (e.g., ?Thegirl is kradding the boy!?).
The lexical baseline sys-tem?s tendency to assign an A0-A1 sequence to thesenouns should be much lower for these test sentences.We therefore expect the contribution of the NPatternfeature to be more apparent in these test sentences.The test sentences with novel verbs ask whetherthe classifier transfers its learning about argumentrole assignment to unseen verbs.
Does it assumethe first of two nouns in a simple transitive sentence(?A gorps B?)
is the agent (A0) and the second isnot an agent (A1)?
In (Connor et al, 2008) weshowed that a system with the same feature and rep-resentations also over-generalized this rule to two-noun intransitives (?A and B gorp?
), mimicking chil-dren?s behavior.
In the present paper this error isover-determined, because the classifier learns onlyan agent/non-agent contrast, and the linguistic con-straints forbid duplicate agents in a sentence.
How-ever, for comparison to the earlier paper we test oursystem on the ?A and B gorp?
sentences as well.3 Experimental ResultsOur experiments use internally-generated feedbackto train simple, abstract structural features: theNPattern features that proved useful with gold-standard training in Connor et al (2008).
Sec-tion 3.1 tests the system on agent-identification inheld-out sentences from the corpus, and demon-strates that the animacy-based feedback is useful,yielding SRL performance comparable to that of asystem trained with 1000 sentences of gold-standardfeedback.
Section 3.2 presents the critical novel-verb test data, demonstrating that this system repli-cates key findings of (Connor et al, 2008) with nogold standard feedback.
Using only noisy internally-generated feedback, the BabySRL learned that thefirst of two nouns is an agent, and generalized thisknowledge to sentences with novel verbs.3.1 Comparing Self Generated Feedback withGold Standard FeedbackTable 1 reports for the varying feedback schemes,the A0 F1 performance for a system with either lex-ical baseline feature (Words) or structural featuresFeedback Words +NPattern1.
Just Animacy 0.72 0.732.
+ non A0 Inference 0.74 0.753.
+ unique A0 bootstrap 0.70 0.7410 Gold 0.43 0.47100 Gold 0.61 0.651000 Gold 0.75 0.76Table 1: Agent identification results (A0 F1) on held-out sections of the Sarah Childes corpus.
We comparea classifier trained with various amounts of gold labeleddata (averaging over 10 different samples at each levelof data).
For noun pattern features the internally gener-ated bootstrap feedback provides comparable accuracy totraining with between 100-1000 fully labeled examples.
(+NPattern) when tested on a held-out section ofthe Sarah Childes corpus section 84-90, recordedat child ages 3;11-4;1 years.
Agent identificationbased on lexical features is quite accurate given an-imacy feedback alone (Feedback 1).
As expected,because many agents are animate, the animacy tag-ging heuristic itself is useful.
As linguistic con-straints are added via non-A0 inference (Feedback2), performance increases for both the lexical base-line and NPattern feature-set, because the system ex-periences more non-A0 training examples.When the unique A0 constraint is added (Feed-back 3), the lexical baseline performance decreases,because for the first time animate nouns are beingtagged as non-agents.
With this feedback the NPat-tern feature set yields a larger improvement over lex-ical baseline, showing that it extracts more generalpatterns.
We discuss the source of these feedbackdifferences in the novel-verb test section below.We compared the usefulness of the internally-generated feedback to gold-standard feedback bytraining a classifier equipped with the same featureson labeled sentences.
We reduced the SRL labelingfor the training sentences to the binary agent/non-agent set, and trained the classifier with 10, 100,or 1000 labeled examples.
Surprisingly, the simplefeedback derived from 84 nouns labeled with ani-macy information yields performance equivalent tobetween 100 and 1000 hand-labeled examples.89Full Distribution Nouns Animate NounsFeedback Words NPattern VPosition Words NPattern VPosition?A gorps B?1.
Animacy 0.86 0.86 0.87 0.76 0.79 0.702.
+ non A0 Inference 0.87 0.92 0.90 0.63 0.86 0.853.
+ unique A0 bootstrap 0.87 0.95 0.89 0.63 0.82 0.66?A and B gorp?1.
Animacy 0.86 0.86 0.84 0.76 0.79 0.682.
+ non A0 Inference 0.87 0.92 0.85 0.63 0.86 0.663.
+ unique A0 bootstrap 0.87 0.95 0.86 0.63 0.82 0.63Table 2: Percentage of sentences interpreted as agent first (%A0-A1) by the BabySRL when trained on unlabeled datawith the 3 internally-generated feedback schemes described in the text.
Two different two-noun sentence structureswere used (?A gorps B?, ?A and B gorp?
), along with two different methods of sampling the nouns (Full Distribution,Animate Nouns) to create test sets with 100 sentences each.3.2 Comparing Structural Features withLexical FeaturesThe previous section shows that the BabySRLequipped with simple structural features can useinternally generated feedback to learn a simpleagent/non-agent classification, and apply it to un-seen sentences.
In this section we probe what theSRL has learned by testing generalization to newverbs in constructed sentences.
Table 2 summarizesthese experiments.
The results are broken down bothby what sentence structure is used in test (?A gorpsB?, ?A and B gorp?)
and how the nouns ?A?
and?B?
are sampled (Full Distribution, Animate Nouns).The results are presented in terms of %A0A1: thepercentage of test sentences that are assigned anAgent role for ?A?
and a non-Agent role for ?B?.For the transitive ?A gorps B?
sentences, A0A1 isthe correct interpretation; A should be the agent.
Aspredicted, when A and B are sampled from the fulldistribution of nouns, simply basing classification onthe Words feature-set aleady strongly predicts thisA0A1 ordering for the majority of cases.
This is be-cause the data (language in general, child directedspeech in particular here) are naturally distributedsuch that particular nouns that refer to animates tendto be agents, and tend to appear as first nouns, andthose that refer to inanimates tend to be non-agentsand second nouns.
Thus, a learner representing sen-tence information in terms of words only succeedswith full-distribution ?A gorps B?
test sentences evenwith the simplest animacy feedback (Feedback 1);the A and B nouns in these test sentences reproducethe learned distribution.
Also as predicted, given thissimple feedback, the additional higher-level features(NPattern, VPosition) do not improve much uponthe lexical baseline.
This is due to the strictly lexicalnature of the animacy feedback: each lexical item(e.g., ?you?
or ?it?)
will always either be animate orinanimate and therefore either A0 or A1.
Therefore,in this case lexical features are the best predictors.Also as expected, higher-level features (NPat-tern, and VPosition) improve performance with amore sophisticated self-generated feedback scheme.Adding inferred feedback to label unknown nounsas A1 when the sentence contains a known animatenoun (Feedback 2) decreases the ratio of A0 to non-A0 arguments.
This feedback is less lexically deter-mined: for nouns whose animacy is unknown, feed-back will be provided only if there is another ani-mate noun in the sentence.
This leaves room for theabstract structural features to play a role.Next we test a form of the unique-A0 constraint.In (Feedback 3), in addition to the non-A0 inferenceadded in (Feedback 2), the BabySRL intelligentlyselects one noun as A0 in sentences with multipleanimate nouns.
With this feedback we see a strikingincrease in test performance based on the noun pat-tern features over the lexical baseline.
In principle,this feedback mechanism might permit the classifierto start to learn that animate nouns are not alwaysagents.
Early in training, the noun pattern featurelearns that first nouns tend to be animate (and there-fore interpreted as agents), and it feeds this informa-90tion back into subsequent training examples, gen-erating new feedback that continues to interpret asagents those animate nouns that appear first in sen-tences containing two animates.For the nouns sampled from the full distributionwe see that structural features improve over the lex-ical baseline despite the high performance of thelexical baseline.
This finding tells us that simplerepresentations of sentence structure can be use-ful in learning to interpret sentences even with nogold-standard training.
Provided only with sim-ple internally-generated feedback based on animacyknowledge and linguistic constraints, the BabySRLlearned that the first of two nouns tends to be anagent, and the second of two does not.The results for the ?A B gorp?
test sentencesdemonstrate an important way in which predictionsbased on different simple structure representationscan diverge.
As expected, the NPattern featuremakes the same overgeneralization error seen bychildren and the system in (Connor et al, 2008).However, when the VPosition feature is added, dif-ferent results are obtained for the ?A gorp B?
and?A and B gorp?
sentences.
The SRL predicts fewerA0A1 for ?A and B gorp?
(it cannot predict the ex-pected A0A0 because of the uniqueness constraintused in test inference).Next, we replicate our findings by performing thesame experiments with test sentences in which both?A?
and ?B?
are animate.
Because lexical featuresalone cannot determine if ?A?
or ?B?
should be theagent, it is a more sensitive test of generalization.When we look at the lexical baseline for animatesentences, the agent-first percentage is lower com-pared to the full distribution results, because theword features indicate nearly evenly that both nounsshould be agents, so the Words baseline model mustrely on small, chance differences in its experiencewith particular words.
This percentage is still wellabove chance due to the method used to apply in-ference during testing.
Recall that the classifier usespredicate-level inference at test to ensure that onlyone argument is labeled A0.
This inference is imple-mented using a beam search that looks at argumentsin a fixed order and roles from A0 up.
Thus in thecase of ties there is a preference for first seen solu-tions, meaning A0A1 in this case.
This bias has alarge effect on the SRL?s baseline performance withthe test sentences containing two animate nouns.Despite this high baseline, however, because lexicalfeatures alone cannot determine if ?A?
or ?B?
shouldbe the agent, we are able to see more clearly the im-provement gained by including structural features.Regardless of our testing scheme, we see that asthe feedback incorporates more information, bothadded linguistic constraints and the SRL?s own priorlearning, the noun pattern structural feature is betterused to identify agents beyond the lexical baseline.The largest improvement over this lexical baseline isobtained by combining knowledge of animacy witha single-agent constraint and bootstrapping predic-tions based on prior learning.4 Conclusion and Future WorkConventional approaches to supervised learning re-quire creating large amounts of hand-labeled data.This is labor-intensive, and limits the relevance ofthe work to the study of how children learn lan-guages.
Children do not receive perfect feedbackabout sentence interpretation.
Here we found thatour simple SRL classifier can, to a surprising de-gree, attain performance comparable to training with1000 sentences of labeled data.
This suggests thatfully labeled training data can be supplemented by acombination of simple world knowledge (animatesmake good agents) and linguistic constraints (eachverb has only one agent).
The combination of thesesources provides an informative training signal thatallows our BabySRL to learn a high-level seman-tic task and generalize beyond the training data weprovided to it.
The SRL learned, based on the dis-tribution of animates in sentences of child-directedspeech, that the first of two nouns tends to be anagent.
It did so based on representations of sentencestructure as simple as the ordered set of nouns inthe sentence.
This demonstrates that it is possible tolearn how to correctly assign semantic roles basedon these very simple cues.
This together with exper-imental work (e.g.
(Fisher, 1996) suggests that suchrepresentations might play a role in children?s earlysentence comprehension.AcknowledgmentsThis research is supported by NSF grant BCS-0620257 and NIH grant R01-HD054448.91ReferencesJ.
Aissen.
1999.
Markedness and subject choice in opti-mality theory.
Natural Language and Linguistic The-ory, 17:673?711.A.
Alishahi and S. Stevenson.
2007.
A computationalusage-based model for learning general properties ofsemantic roles.
In Proceedings of the 2nd EuropeanCognitive Science Conference.R.
Baillargeon, D. Wu, S. Yuan, J. Li, and Y.
Luo.
(in press).
Young infants expectations about self-propelled objects.
In B.
Hood and L. Santos, editors,The origins of object knowledge.
Oxford UniversityPress, Oxford.J.
Bresnan.
1982.
The mental representation of gram-matical relations.
MIT Press, Cambridge MA.R.
Brown.
1973.
A First Language.
Harvard UniversityPress, Cambridge, MA.G.
Carlson.
1998.
Thematic roles and the individuationof events.
In S. D. Rothstein, editor, Events and Gram-mar, pages 35?51.
Kluwer, Dordrecht.X.
Carreras and L. Ma`rquez.
2004.
Introduction to theCoNLL-2004 shared tasks: Semantic role labeling.
InProceedings of CoNLL-2004, pages 89?97.
Boston,MA, USA.R.
S. Chapman and L. L. Kohn.
1978.
Comprehensionstrategies in two- and three-year-olds: Animate agentsor probable events?
Journal of Speech and HearingResearch, 21:746?761.E.
Charniak.
1997.
Statistical parsing with a context-free grammar and word statistics.
In Proc.
NationalConference on Artificial Intelligence.M.
Connor, Y. Gertner, C. Fisher, and D. Roth.
2008.Baby srl: Modeling early language acquisition.
InProc.
of the Annual Conference on ComputationalNatural Language Learning (CoNLL), Aug.D.
Dowty.
1991.
Thematic proto-roles and argument se-lection.
Language, 67:547?619.C.
Fisher.
1996.
Structural limits on verb mapping:The role of analogy in children?s interpretation of sen-tences.
Cognitive Psychology, 31:41?81.Y.
Gertner and C. Fisher.
2006.
Predicted errors in earlyverb learning.
In 31st Annual Boston University Con-ference on Language Development.Y.
Gertner, C. Fisher, and J. Eisengart.
2006.
Learningwords and rules: Abstract knowledge of word orderin early sentence comprehension.
Psychological Sci-ence, 17:684?691.A.
Grove and D. Roth.
2001.
Linear concepts and hiddenvariables.
Machine Learning, 42(1/2):123?141.P.
Kingsbury and M. Palmer.
2002.
From Treebank toPropBank.
In Proceedings of LREC-2002, Spain.B.
MacWhinney.
2000.
The CHILDES project: Toolsfor analyzing talk.
Third Edition.
Lawrence ElrbaumAssociates, Mahwah, NJ.L.
R. Naigles.
1990.
Children use syntax to learn verbmeanings.
Journal of Child Language, 17:357?374.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
In Computational Linguistics 31(1).S.
Pinker.
1989.
Learnability and Cognition.
Cam-bridge: MIT Press.V.
Punyakanok, D. Roth, and W. Yih.
2005a.
The neces-sity of syntactic parsing for semantic role labeling.
InProc.
of the International Joint Conference on Artifi-cial Intelligence (IJCAI), pages 1117?1123.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2005b.Learning and inference over constrained output.
InProc.
of the International Joint Conference on Artifi-cial Intelligence (IJCAI), pages 1124?1129.S.
R. Waxman and A. Booth.
2001.
Seeing pink ele-phants: Fourteen-month-olds?s interpretations of novelnouns and adjectives.
Cognitive Psychology, 43:217?242.S.
Yuan, C. Fisher, Y. Gertner, and J. Snedeker.
2007.Participants are more than physical bodies: 21-month-olds assign relational meaning to novel transitiveverbs.
In Biennial Meeting of the Society for Researchin Child Development, Boston, MA.92
