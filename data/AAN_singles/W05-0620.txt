Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 152?164, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsIntroduction to the CoNLL-2005 Shared Task:Semantic Role LabelingXavier Carreras and Llu?
?s Ma`rquezTALP Research CentreTechnical University of Catalonia (UPC){carreras,lluism}@lsi.upc.eduAbstractIn this paper we describe the CoNLL-2005 shared task on Semantic Role La-beling.
We introduce the specification andgoals of the task, describe the data sets andevaluation methods, and present a generaloverview of the 19 systems that have con-tributed to the task, providing a compara-tive description and results.1 IntroductionIn the few last years there has been an increasinginterest in shallow semantic parsing of natural lan-guage, which is becoming an important componentin all kind of NLP applications.
As a particular case,Semantic Role Labeling (SRL) is currently a well-defined task with a substantial body of work andcomparative evaluation.
Given a sentence, the taskconsists of analyzing the propositions expressed bysome target verbs of the sentence.
In particular, foreach target verb all the constituents in the sentencewhich fill a semantic role of the verb have to be rec-ognized.
Typical semantic arguments include Agent,Patient, Instrument, etc.
and also adjuncts such asLocative, Temporal, Manner, Cause, etc.Last year, the CoNLL-2004 shared task aimedat evaluating machine learning SRL systems basedonly on partial syntactic information.
In (Carrerasand Ma`rquez, 2004) one may find a detailed reviewof the task and also a brief state-of-the-art on SRLprevious to 2004.
Ten systems contributed to thetask, which was evaluated using the PropBank cor-pus (Palmer et al, 2005).
The best results werearound 70 in F1 measure.
Though not directly com-parable, these figures are substantially lower than thebest results published up to date using full parsingas input information (F1 slightly over 79).
In addi-tion to the CoNLL-2004 shared task, another evalua-tion exercise was conducted in the Senseval-3 work-shop (Litkowski, 2004).
Eight systems relying onfull parsing information were evaluated in that eventusing the FrameNet corpus (Fillmore et al, 2001).From the point of view of learning architectures andstudy of feature relevance, it is also worth mention-ing the following recent works (Punyakanok et al,2004; Moschitti, 2004; Xue and Palmer, 2004; Prad-han et al, 2005a).Following last year?s initiative, the CoNLL-2005shared task1 will concern again the recognition ofsemantic roles for the English language.
Comparedto the shared task of CoNLL-2004, the novelties in-troduced in the 2005 edition are:?
Aiming at evaluating the contribution of fullparsing in SRL, the complete syntactic treesgiven by two alternative parsers have been pro-vided as input information for the task.
Therest of input information does not vary and cor-responds to the levels of processing treated inthe previous editions of the CoNLL shared task,i.e., words, PoS tags, base chunks, clauses, andnamed entities.?
The training corpus has been substantially en-larged.
This allows to test the scalability of1The official CoNLL-2005 shared task web page, in-cluding data, software and systems?
outputs, is available athttp://www.lsi.upc.edu/?srlconll.152learning-based SRL systems to big datasets andto compute learning curves to see how muchdata is necessary to train.
Again, we concen-trate on the PropBank corpus (Palmer et al,2005), which is the Wall Street Journal partof the Penn TreeBank corpus enriched withpredicate?argument structures.?
In order to test the robustness of the pre-sented systems, a cross-corpora evaluation isperformed using a fresh test set from the Browncorpus.Regarding evaluation, two different settings weredevised depending if the systems use the informa-tion strictly contained in the training data (closedchallenge) or they make use of external sourcesof information and/or tools (open challenge).
Theclosed setting allows to compare systems understrict conditions, while the open setting aimed at ex-ploring the contributions of other sources of infor-mation and the limits of the current learning-basedsystems on the SRL task.
At the end, all 19 systemstook part in the closed challenge and none of themin the open challenge.The rest of the paper is organized as follows.
Sec-tion 2 describes the general setting of the task.
Sec-tion 3 provides a detailed description of training,development and test data.
Participant systems aredescribed and compared in section 4.
In particular,information about learning techniques, SRL strate-gies, and feature development is provided, togetherwith performance results on the development andtest sets.
Finally, section 5 concludes.2 Task DescriptionAs in the 2004 edition, the goal of the task was todevelop a machine learning system to recognize ar-guments of verbs in a sentence, and label them withtheir semantic role.
A verb and its set of argumentsform a proposition in the sentence, and typically, asentence contains a number of propositions.There are two properties that characterize thestructure of the arguments in a proposition.
First, ar-guments do not overlap, and are organized sequen-tially.
Second, an argument may appear split intoa number of non-contiguous phrases.
For instance,in the sentence ?
[A1 The apple], said John, [C?A1is on the table]?, the utterance argument (labeledwith type A1) appears split into two phrases.
Thus,there is a set of non-overlapping arguments labeledwith semantic roles associated with each proposi-tion.
The set of arguments of a proposition can beseen as a chunking of the sentence, in which chunksare parts of the semantic roles of the propositionpredicate.In practice, number of target verbs are markedin a sentence, each governing one proposition.
Asystem has to recognize and label the arguments ofeach target verb.
To support the role labeling task,sentences contain input annotations, that consist ofsyntactic information and named entities.
Section 3describes in more detail the annotations of the data.2.1 EvaluationEvaluation is performed on a collection of unseentest sentences, that are marked with target verbs andcontain only predicted input annotations.A system is evaluated with respect to precision,recall and the F1 measure of the predicted argu-ments.
Precision (p) is the proportion of argumentspredicted by a system which are correct.
Recall (r)is the proportion of correct arguments which are pre-dicted by a system.
Finally, the F1 measure com-putes the harmonic mean of precision and recall, andis the final measure to compare the performance ofsystems.
It is formulated as: F?=1 = 2pr/(p + r).For an argument to be correctly recognized, thewords spanning the argument as well as its semanticrole have to be correct.
2As an exceptional case, the verb argument of eachproposition is excluded from the evaluation.
This ar-gument is the lexicalization of the predicate of theproposition.
Most of the time, the verb correspondsto the target verb of the proposition, which is pro-vided as input, and only in few cases the verb par-ticipant spans more words than the target verb.
Ex-cept for non-trivial cases, this situation makes theverb fairly easy to identify and, since there is oneverb with each proposition, evaluating its recogni-tion over-estimates the overall performance of a sys-tem.
For this reason, the verb argument is excludedfrom evaluation.2The srl-eval.pl program is the official program toevaluate the performance of a system.
It is available at theShared Task web page.153And CC * (S* (S* * - (AM-DIS*) (AM-DIS*)to TO (VP* (S* (S(VP* * - * (AM-PNC*attract VB *) * (VP* * attract (V*) *younger JJR (NP* * (NP* * - (A1* *listeners NNS *) *) *)))) * - *) *), , * * * * - * *Radio NNP (NP* * (NP* (ORG* - (A0* (A0*Free NNP * * * * - * *Europe NNP *) * *) *) - *) *)intersperses VBZ (VP*) * (VP* * intersperse * (V*)the DT (NP* * (NP(NP* * - * (A1*latest JJS *) * *) * - * *in IN (PP*) * (PP* * - * *Western JJ (NP* * (NP* (MISC*) - * *rock NN * * * * - * *groups NNS *) * *)))) * - * *).
.
* *) *) * - * *Figure 1: An example of an annotated sentence, in columns.
Input consists of words (1st column), PoStags (2nd), base chunks (3rd), clauses (4th), full syntactic tree (5th) and named entities (6th).
The 7thcolumn marks target verbs, and their propositions are found in remaining columns.
According to thePropBank Frames, for attract (8th), the A0 annotates the attractor, and the A1 the thing attracted; forintersperse (9th), A0 is the arranger, and A1 the entity interspersed.2.2 Closed Challenge SettingThe organization provided training, developmentand test sets derived from the standard sections ofthe Penn TreeBank (Marcus et al, 1993) and Prop-Bank (Palmer et al, 2005) corpora.In the closed challenge, systems have to be builtstrictly with information contained in the trainingsections of the TreeBank and PropBank.
Since thiscollection contains the gold reference annotationsof both syntactic and predicate-argument structures,the closed challenge allows: (1) to make use of anypreprocessing system strictly developed within thissetting, and (2) to learn from scratch any annotationthat is contained in the data.
To support the former,the organization provided the output of state-of-the-art syntactic preprocessors, described in Section 3.The development set is used to tune the parame-ters of a system.
The gold reference annotations arealso available in this set, but only to evaluate the per-formance of different parametrizations of a system,and select the optimal one.
Finally, the test set isused to evaluate the performance of a system.
It isonly allowed to use predicted annotations in this set.Since all systems in this setting have had access tothe same training and development data, the evalua-tion results on the test obtained by different systemsare comparable in a fair manner.3 DataThe data consists of sections of the Wall Street Jour-nal part of the Penn TreeBank (Marcus et al, 1993),with information on predicate-argument structuresextracted from the PropBank corpus (Palmer et al,2005).
In this edition of the CoNLL shared task,we followed the standard partition used in syntacticparsing: sections 02-21 for training, section 24 fordevelopment, and section 23 for test.
In addition, thetest set of the shared task includes three sections ofthe Brown corpus (namely, ck01-03).
The predicate-argument annotations of the latter test material werekindly provided by the PropBank team, and are veryvaluable, as they allow to evaluate learning systemson a portion of data that comes from a differentsource than training.We first describe the annotations related to argu-ment structures.
Then, we describe the preprocess-ing systems that have been selected to predict theinput part of the data.
Figure 1 shows an example ofa fully-annotated sentence.3.1 PropBankThe Proposition Bank (PropBank) (Palmer et al,2005) annotates the Penn TreeBank with verb argu-ment structure.
The semantic roles covered by Prop-Bank are the following:154?
Numbered arguments (A0?A5, AA): Argu-ments defining verb-specific roles.
Their se-mantics depends on the verb and the verb us-age in a sentence, or verb sense.
The mostfrequent roles are A0 and A1 and, commonly,A0 stands for the agent and A1 corresponds tothe patient or theme of the proposition.
How-ever, no consistent generalization can be madeacross different verbs or different senses of thesame verb.
PropBank takes the definition ofverb senses from VerbNet, and for each verband each sense defines the set of possible rolesfor that verb usage, called the roleset.
The def-inition of rolesets is provided in the PropBankFrames files, which is made available for theshared task as an official resource to developsystems.?
Adjuncts (AM-): General arguments that anyverb may take optionally.
There are 13 types ofadjuncts:AM-ADV : general-purpose AM-MOD : modal verbAM-CAU : cause AM-NEG : negation markerAM-DIR : direction AM-PNC : purposeAM-DIS : discourse marker AM-PRD : predicationAM-EXT : extent AM-REC : reciprocalAM-LOC : location AM-TMP : temporalAM-MNR : manner?
References (R-): Arguments representing ar-guments realized in other parts of the sentence.The role of a reference is the same as the role ofthe referenced argument.
The label is an R- tagprefixed to the label of the referent, e.g.
R-A1.?
Verbs (V): Argument corresponding to the verbof the proposition.
Each proposition has exa-clty one verb argument.We used PropBank-1.0.
Most predicative verbswere annotated, although not all of them (for exam-ple, most of the occurrences of the verb ?to have?and ?to be?
were not annotated).
We applied proce-dures to check consistency of propositions, lookingfor overlapping arguments, and incorrect semanticrole labels.
Also, co-referenced arguments were an-notated as a single item in PropBank, and we au-tomatically distinguished between the referent andthe reference with simple rules matching pronomi-nal expressions, which were tagged as R arguments.Train.
Devel.
tWSJ tBrownSentences 39,832 1,346 2,416 426Tokens 950,028 32,853 56,684 7,159Propositions 90,750 3,248 5,267 804Verbs 3,101 860 982 351Arguments 239,858 8,346 14,077 2,177A0 61,440 2,081 3,563 566A1 84,917 2,994 4,927 676A2 19,926 673 1,110 147A3 3,389 114 173 12A4 2,703 65 102 15A5 68 2 5 0AA 14 1 0 0AM 7 0 0 0AM-ADV 8,210 279 506 143AM-CAU 1,208 45 73 8AM-DIR 1,144 36 85 53AM-DIS 4,890 202 320 22AM-EXT 628 28 32 5AM-LOC 5,907 194 363 85AM-MNR 6,358 242 344 110AM-MOD 9,181 317 551 91AM-NEG 3,225 104 230 50AM-PNC 2,289 81 115 17AM-PRD 66 3 5 1AM-REC 14 0 2 0AM-TMP 16,346 601 1,087 112R-A0 4,112 146 224 25R-A1 2,349 83 156 21R-A2 291 5 16 0R-A3 28 0 1 0R-A4 7 0 1 0R-AA 2 0 0 0R-AM-ADV 5 0 2 0R-AM-CAU 41 3 4 2R-AM-DIR 1 0 0 0R-AM-EXT 4 1 1 0R-AM-LOC 214 9 21 4R-AM-MNR 143 6 6 2R-AM-PNC 12 0 0 0R-AM-TMP 719 31 52 10Table 1: Counts on the data sets.A total number of 80 propositions were not compli-ant with our procedures (one in the Brown files, therest in WSJ) and were filtered out from the CoNLLdata sets.Table 1 provides counts of the number of sen-tences, tokens, annotated propositions, distinctverbs, and arguments in the four data sets.3.2 Preprocessing SystemsIn this section we describe the selected processorsthat computed input annotations for the SRL sys-tems.
The annotations are: part-of-speech (PoS)tags, chunks, clauses, full syntactic trees and namedentities.
As it has been noted, participants were also155allowed to use any processor developed within thesame WSJ partition.The preprocessors correspond to the followingstate-of-the-art systems:?
UPC processors, consisting of:?
PoS tagger: (Gime?nez and Ma`rquez,2003), based on Support Vector Machines,and trained on WSJ sections 02-21.?
Base Chunker and Clause Recognizer:(Carreras and Ma`rquez, 2003), based onVoted Perceptrons, trained on WSJ sec-tions 02-21.
These two processors form acoherent partial syntax of a sentence, thatis, chunks and clauses form a partial syn-tactic tree.?
Full parser of Collins (1999), with ?model 2?.Predicts WSJ full parses, with information ofthe lexical head for each syntactic constituent.The PoS tags (required by the parser) have beencomputed with (Gime?nez and Ma`rquez, 2003).?
Full parser of Charniak (2000).
Jointly predictsPoS tags and full parses.?
Named Entities predicted with the Maximum-Entropy based tagger of Chieu and Ng (2003).The tagger follows the CoNLL-2003 task set-ting (Tjong Kim Sang and De Meulder, 2003),and thus is not developed with WSJ data.
How-ever, we allowed its use because there is noavailable named entity recognizer developedwith WSJ data.
The reported performance onthe CoNLL-2003 test is F1 = 88.31, withPrec/Rec.
at 88.12/88.51.Tables 2 and 3 summarize the performance ofthe syntactic processors on the development and testsets.
The performance of full parsers on the WSJtest is lower than that reported in the correspond-ing papers.
The reason is that our evaluation fig-ures have been computed in a strict manner with re-spect to punctuation tokens, while the full parsingcommunity usually does not penalize for punctua-tion wrongly placed in the tree.3 As it can be ob-3Before evaluating Collins?, we raised punctuation to thehighest point in the tree, using a script that is available at theshared task webpage.
Otherwise, the performance would havePrec./Recall figures below 37.Dev.
tWSJ tBrownUPC PoS-tagger 97.13 97.36 94.73Charniak (2000) 92.01 92.29 87.89Table 2: Accuracy (%) of PoS taggers.served, the performance of all syntactic processorssuffers a substantial loss in the Brown test set.
No-ticeably, the parser of Collins (1999) seems to be themore robust when moving from WSJ to Brown.4 A Review of Participant SystemsNineteen systems participated in the CoNLL-2005shared task.
They approached the task in severalways, using different learning components and la-beling strategies.
The following subsections brieflysummarize the most important properties of eachsystem and provide a qualitative comparison be-tween them, together with a quantitative evaluationon the development and test sets.4.1 Learning techniquesUp to 8 different learning algorithms have been ap-plied to train the learning components of partici-pant systems.
See the ?ML-method?
column of ta-ble 4 for a summary of the following information.Log?linear models and vector-based linear classi-fiers dominated over the rest.
Probably, this is due tothe versatility of the approaches and the availabilityof very good software toolkits.In particular, 8 teams used the Maximum En-tropy (ME) statistical framework (Che et al, 2005;Haghighi et al, 2005; Park and Rim, 2005; TjongKim Sang et al, 2005; Sutton and McCallum, 2005;Tsai et al, 2005; Yi and Palmer, 2005; Venkatapathyet al, 2005).
Support Vector Machines (SVM) wereused by 6 teams.
Four of them with the standardpolynomial kernels (Mitsumori et al, 2005; TjongKim Sang et al, 2005; Tsai et al, 2005; Pradhan etal., 2005b), another one using Gaussian kernels (Oz-gencil and McCracken, 2005), and a last group usingtree-based kernels specifically designed for the task(Moschitti et al, 2005).
Another team used also a re-lated learning approach, SNoW, which is a Winnow-based network of linear separators (Punyakanok etal., 2005).Decision Tree learning (DT) was also represented156Devel.
Test WSJ Test BrownP(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1UPC Chunker 94.66 93.17 93.91 95.26 94.52 94.89 92.64 90.85 91.73UPC Clauser 90.38 84.73 87.46 90.93 85.94 88.36 84.21 74.32 78.95Collins (1999) 85.02 83.55 84.28 85.63 85.20 85.41 82.68 81.33 82.00Charniak (2000) 87.60 87.38 87.49 88.20 88.30 88.25 80.54 81.15 80.84Table 3: Results of the syntactic parsers on the development, and WSJ and Brown test sets.
Unlike in fullparsing, the figures have been computed on a strict evaluation basis with respect to punctuation.by Ponzetto and Strube (2005), who used C4.5.Ensembles of decision trees learned through theAdaBoost algorithm (AB) were applied by Ma`rquezet al (2005) and Surdeanu and Turmo (2005).
TjongKim Sang et al (2005) applied, among others,Memory-Based Learning (MBL).Regarding novel learning paradigms not appliedin previous shared tasks, we find Relevant VectorMachine (RVM), which is a kernel?based linear dis-criminant inside the framework of Sparse BayesianLearning (Johansson and Nugues, 2005) and TreeConditional Random Fields (T-CRF) (Cohn andBlunsom, 2005), that extend the sequential CRFmodel to tree structures.
Finally, Lin and Smith(2005) presented a proposal radically different fromthe rest, with very light learning components.
Theirapproach (Consensus in Pattern Matching, CPM)contains some elements of Memory-based Learningand ensemble classification.From the Machine Learning perspective, systemcombination is another interesting component ob-served in many of the proposals.
This fact, which isa difference from last year shared task, is explainedas an attempt of increasing the robustness and cover-age of the systems, which are quite dependent on in-put parsing errors.
The different outputs to combineare obtained by varying input information, chang-ing learning algorithm, or considering n-best solu-tion lists.
The combination schemes presented in-clude very simple voting-like combination heuris-tics, stacking of classifiers, and a global constraintsatisfaction framework modeled with Integer LinearProgramming.
Global models trained to re-rank al-ternative outputs represent a very interesting alter-native that has been proposed by two systems.
Allthese issues are reviewed in detail in section 4.2.4.2 SRL approachesSRL is a complex task, which may be decomposedinto a number of simpler decisions and annotatingschemes in order to be addressed by learning tech-niques.
Table 4 contains a summary of the mainproperties of the 19 systems presented.
In this sec-tion we will explain the contents of that table bycolumns (from left-to-right).One first issue to consider is the input structureto navigate in order to extract the constituents thatwill form labeled arguments.
The majority of sys-tems perform parse tree node labeling, searchingfor a one?to?one map between arguments and parseconstituents.
This information is summarized in the?synt?
column of Table 4.
?col?, ?cha?, ?upc?
standfor the syntactic parse trees (the latter is partial) pro-vided as input by the organization.
Additionally,some teams used lists of n-best parsings generatedby available tools (?n-cha?
by Charniak parser; ?n-bikel?
by Bikel?s implementation of Collins parser).Interestingly, Yi and Palmer (2005) retrained Rat-naparkhi?s parser using the WSJ training sectionsenriched with semantic information coming fromPropBank annotations.
These are referred to as ANand AM parses.
As it can be seen, Charniak parseswere used by most of the systems.
Collins parseswere used also in some of the best performing sys-tems based on combination.The exceptions to the hierarchical processing arethe systems by Pradhan et al (2005b) and Mitsumoriet al (2005), which perform a chunking-based se-quential tokenization.
As for the former, the systemis the same than the one presented in the 2004 edi-tion.
The system by Ma`rquez et al (2005) exploreshierarchical syntactic structures but selects, in a pre-process, a sequence of tokens to perform a sequen-tial tagging afterwards.157ML-method synt pre label embed glob post comb typepunyakanok SNoW n-cha,col x&p i+c defer yes no n-cha+col ac-ILPhaghighi ME n-cha ?
i+c dp-prob yes no n-cha re-rankmarquez AB cha,upc seq bio !need no no cha+upc s-joinpradhan SVM cha,col/chunk ?
c/bio ?
no no cha+col?chunk stacksurdeanu AB cha prun c g-top no yes no ?tsai ME,SVM cha x&p c defer yes no ME+SVM ac-ILPche ME cha no c g-score no yes no ?moschitti SVM cha prun i+c !need no no no ?tjongkimsang ME,SVM,TBL cha prun i+c !need no yes ME+SVM+TBL s-joinyi ME cha,AN,AM x&p i+c defer no no cha+AN+AM ac-joinozgencil SVM cha prun i+c g-score no no no ?johansson RVM cha softp i+c ?
no no no ?cohn T-CRF col x&p c g-top yes no no ?park ME cha prun i+c ?
no no no ?mitsumori SVM chunk no bio !need no no no ?venkatapathy ME col prun i+c frames yes no no ?ponzetto DT col prun c g-top no yes no ?lin CPM cha gt-para i+c !need no no no ?sutton ME n-bikel x&p i+c dp-prob yes no n-bikel re-rankTable 4: Main properties of the SRL strategies implemented by the participant teams, sorted by F1 per-formance on the WSJ+Brown test set.
synt stands for the syntactic structure explored; pre stands forpre-processing steps; label stands for the labeling strategy; embed stands for the technique to ensure non-embedding of arguments; glob stands for global optimization; post stands for post-processing; comb standsfor system output combination, and type stands for the type of combination.
Concrete values appearing inthe table are explained in section 4.1.
The symbol ???
stands for unknown values not reported by the systemdescription papers.In general, the presented systems addressed theSRL problem by applying different chained pro-cesses.
In Table 4 the column ?pre?
summarizes pre-processing.
In most of the cases this corresponds toa pruning procedure to filter out constituents that arenot likely to be arguments.
As in feature develop-ment, the related bibliography has been followed forpruning.
For instance, many systems used the prun-ing strategy described in (Xue and Palmer, 2004)(?x&p?)
and other systems used the soft pruningrules described in (Pradhan et al, 2005a) (?softp?
).Remarkably, Park and Rim (2005) parametrize thepruning procedure and then study the effect of be-ing more or less aggressive at filtering constituents.In the case of Ma`rquez et al (2005), pre-processingcorresponds to a sequentialization of syntactic hier-archical structures.
As a special case, Lin and Smith(2005) used the GT-PARA analyzer for convertingparse trees into a flat representation of all predicatesincluding argument boundaries.The second stage, reflected in column ?label?
ofTable 4, is the proper labeling of selected candi-dates.
Most of the systems used a two-step proce-dure consisting of first identifying arguments (e.g.,with a binary ?null?
vs. ?non-null?
classifier) andthen classifying them.
This is referred to as ?i+c?
inthe table.
Some systems address this phase in a sin-gle classification step by adding a ?null?
categoryto the multiclass problem (referred to as ?c?).
Themethods performing a sequential tagging use a BIOtagging scheme (?bio?).
As a special case, Mos-chitti et al (2005) subdivide the ?i+c?
strategy intofour phases: after identification, heuristics are ap-plied to assure compatibility of identified arguments;and, before classifying arguments into roles, a pre-classification into core vs. adjunct arguments is per-formed.
Venkatapathy et al (2005) use three labelsinstead of two in the identification phase : ?null?,?mandatory?, and ?optional?.Since arguments in a solution do not embed andmost systems identify arguments as nodes in a hier-archical structure, non-embedding constraints mustbe resolved in order to generate a coherent argu-ment labeling.
The ?embed?
column of Table 4 ac-counts for this issue.
The majority of systems ap-plied specific greedy procedures that select a subsetof consistent arguments.
The families of heuristicsto do that selection include prioritizing better scored158constituents (?g-score?
), or selecting the argumentsthat are first reached in a top-down exploration (?g-top?).
Some probabilistic systems include the non-embedding constraints within the dynamic program-ming inference component, and thus calculate themost probable coherent labeling (?dp-prob?).
The?defer?
value means that this is a combination sys-tem and that coherence of the individual system pre-dictions is not forced, but deferred to the later com-bination step.
As a particular case, Venkatapathy etal.
(2005) use PropBank subcategorization frames toforce a coherent solution.
Note that tagging-basedsystems do not need to check non-embedding con-straints (?!need?
value).The ?glob?
column of Table 4 accounts for the lo-cality/globality of the process used to calculate theoutput solution given the argument prediction candi-dates.
Systems with a ?yes?
value in that column de-fine some kind of scoring function (possibly proba-bilistic) that applies to complete candidate solutions,and then calculate the solution that maximizes thescoring using an optimization algorithm.Some systems use some kind of postprocessing toimprove the final output of the system by correct-ing some systematic errors, or treating some typesof simple adjunct arguments.
This information is in-cluded in the ?post?
column of Table 4.
In most ofthe cases, this postprocess is performed on the basisof simple ad-hoc rules.
However, it is worth men-tioning the work of Tjong Kim Sang et al (2005)in which spelling error correction techniques areadapted for improving the resulting role labeling.
Inthat system, postprocessing is applied before systemcombination.Most of the best performing systems included acombination of different base subsystems to increaserobustness of the approach and to gain coverage andindependence from parse errors.
Last 2 columns ofTable 4 present this information.
In the ?comb?
col-umn the source of the combination is reported.
Basi-cally, the alternative outputs to combine can be gen-erated by different input syntactic structures or n-best parse candidates, or by applying different learn-ing algorithms to the same input information.The type of combination is reported in the last col-umn.
Ma`rquez et al (2005) and Tjong Kim Sanget al (2005) performed a greedy merging of the ar-guments of base complete solutions (?s-join?).
Yiand Palmer (2005) did also a greedy merging of ar-guments but taking into account not complete so-lutions but all candidate arguments labeled by basesystems (?ac-join?).
In a more sophisticated way,Punyakanok et al (2005) and Tsai et al (2005) per-formed global inference as constraint satisfactionusing Integer Linear Programming, also taking intoaccount all candidate arguments (?ac-ILP?).
It isworth noting that the generalized inference appliedin those papers allows to include, jointly with thecombination of outputs, a number of linguistically-motivated constraints to obtain a coherent solution.Pradhan et al (2005b) followed a stacking ap-proach by learning a chunk-based SRL system in-cluding as features the outputs of two syntax-basedsystems.
Finally, Haghighi et al (2005) and Sut-ton and McCallum (2005) performed a different ap-proach by learning a re-ranking function as a globalmodel on top of the base SRL models.
Actually,Haghighi et al (2005) performed a double selectionstep: an inner re-ranking of n-best solutions comingfrom the base system on a single tree; and an outerselection of the final solution among the candidatesolutions coming from n-best parse trees.
The re-ranking approach allows to define global complexfeatures applying to complete candidate solutions totrain the rankers.4.3 FeaturesLooking at the description of the different systems, itbecomes clear that the general type of features usedin this edition is strongly based on previous work onthe SRL task (Gildea and Jurafsky, 2002; Surdeanuet al, 2003; Pradhan et al, 2005a; Xue and Palmer,2004).
With no exception, all systems have madeintensive use of syntax to extract features.
Whilemost systems work only on the output of a parser?Charniak?s being the most preferred?
some sys-tems depend on many syntactic parsers.
In the lattersituation, either a system is a combination of manyindividual systems (each working with a differentparser), or a system extracts features from many dif-ferent parse trees while exploring the nodes of onlyone parse tree.
Most systems have also considerednamed entities for extracting features.The main types of features seen in this SRL edi-tion can be divided into four general categories: (1)Features characterizing the structure of a candidate159sources argument verb arg?verb psynt ne at aw ab ac ai pp sd v sc rp di ps pv pi sf aspunyakanok cha,col,upc + + h + t + + ?
+ + + c + ?
+ + ?haghighi cha ?
+ h + p,s ?
+ + + + + t + + ?
?
+marquez cha,upc + + h + t + ?
+ + + + w,c + + ?
+ ?pradhan cha,col,upc + + h,c + p,s,t + + ?
+ + + c,t + + + + ?surdeanu cha + + h,c + p,s + ?
+ + + + w,t + + + ?
?tsai cha,upc + + h + p,s,t ?
?
?
+ + + w + ?
?
?
?che cha + + h + ?
?
+ ?
+ + + t + + ?
?
?moschitti cha ?
+ h + p + + ?
+ + + t + + ?
+ ?tjongkimsang cha + + ?
+ p,t ?
+ ?
+ + + w,t + + + ?
?yi cha,an,am ?
+ h,c ?
p,s ?
+ ?
+ + + w + ?
?
+ ?ozgencil cha ?
+ h ?
p ?
?
?
+ + + ?
+ + ?
?
?johansson cha,upc + + h ?
?
?
?
?
+ + + ?
+ + ?
?
?cohn col ?
+ h + p,s ?
+ ?
+ + + w + ?
+ + ?park cha ?
+ h,c ?
p ?
?
?
+ + + ?
+ ?
+ ?
?mitsumori upc,cha + + ?
+ t ?
?
+ + ?
+ c,t ?
+ ?
?
?venkatapathy col + + h + ?
?
?
?
+ ?
+ ?
+ ?
?
?
?ponzetto col,upc + + h + ?
+ ?
?
+ ?
?
w,c,t ?
?
+ ?
?lin cha ?
+ h + ?
?
?
?
+ ?
+ w ?
?
?
?
?sutton bik ?
+ h + p,s ?
?
?
+ ?
+ ?
+ ?
?
?
+Table 5: Main feature types used by the 19 participating systems in the CoNLL-2005 shared task, sorted byperformance on the WSJ+Brown test set.
Sources: synt: use of parsers, namely Charniak (cha), Collins(col), UPC partial parsers (upc), Bikel?s Collins model (bik) and/or argument-enriched parsers (an,am); ne:use of named entities.
On the argument: at: argument type; aw: argument words, namely the head (h)and/or content words (c); ab: argument boundaries, i.e.
form and PoS of first and/or last argument words; ac:argument context, capturing features of the parent (p) and/or left/right siblings (s), or the tokens surroundingthe argument (t); ai: indicators of the structure of the argument (e,g., on internal constituents, surround-ing/boundary punctuation, governing category, etc.
); pp: specific features for prepositional phrases; sd:semantic dictionaries.
On the verb: v: standard verb features (voice, word/lemma, PoS); sc: subcatego-rization.
On the arg-verb relation: rp: relative position; di: distance, based on words (w), chunks (c) orthe syntactic tree (t); ps: standard path; pv: path variations; pi: scalar indicator variables on the path (ofchunks, clauses, or other phrase types), common ancestor, etc.
; sf: syntactic frame (Xue and Palmer, 2004);On the complete proposition: as: sequence of arguments of a proposition.argument; (2) Features describing properties of thetarget verb predicate; (3) Features that capture therelation between the verb predicate and the con-stituent under consideration; and (4) Global featuresdescribing the complete argument labeling of a pred-icate.
The rest of the section describes the most com-mon feature types in each category.
Table 5 summa-rizes the type of features exploited by systems.To represent an argument itself, all systems makeuse of the syntactic type of the argument.
Almostall teams used the heuristics of Collins (1999) to ex-tract the head word of the argument, and used fea-tures that capture the form, lemma and PoS tag ofthe head.
In the same line, some systems also usefeatures of the content words of the argument, usingthe heuristics of Surdeanu et al (2003).
Very gen-erally also, many systems extract features from thefirst and last words of the argument.
Regarding thesyntactic elements surrounding the argument, manysystems working on full trees have considered theparent and siblings of the argument, capturing theirsyntactic type and head word.
Differently, othersystems have captured features from the left/righttokens surrounding the argument, which are typi-cally words, but can be chunks or general phrases insystems that sequentialize the task (Ma`rquez et al,2005; Pradhan et al, 2005b; Mitsumori et al, 2005).Many systems use a variety of indicator features thatcapture properties of the argument structure and itslocal syntactic annotations.
For example, indicatorsof the immediate syntactic types that form the argu-ment, flags raised by punctuation tokens in or nearbythe argument, or the governing category feature ofGildea and Jurafsky (2002).
It is also somewhat gen-160eral the use of specific features that apply when theconstituent is a prepositional phrase, such as look-ing for the head word of the noun phrase within it.A few systems have also built semantic dictionariesfrom training data, that collect words appearing fre-quently in temporal, locative or other arguments.To represent the predicate, all systems have usedfeatures codifying the form, lemma, PoS tag andvoice of the verb.
It is also of general use the subcat-egorization feature, capturing the syntactic rule thatexpands the parent of the predicate.
Some systemscaptured statistics related to the frequency of a verbin training data (not in Table 5).Regarding features related to an argument-verbpair, almost all systems use the simple feature de-scribing the relative position between them.
Toa lesser degree, systems have computed distancesfrom one to the other, based on the number of wordsor chunks between them, or based on the syntactictree.
Not surprisingly, all systems have extracted thepath from the argument to the verb.
While almostall systems use the standard path of (Gildea and Ju-rafsky, 2002), many have explored variations of it.A common one consists of the path from the argu-ment to the lowest common ancestor of the verb andthe argument.
Another variation is the partial path,that is built of chunks and clauses only.
Indicatorfeatures that capture scalar values of the path arealso common, and concentrate mainly on lookingat the common ancestor, capturing the difference ofclausal levels, or looking for punctuation and otherlinguistic elements in the path.
In this category, it isalso noticeable the use of the syntactic frame feature,proposed by Xue and Palmer (2004).Finally, in this edition two systems apply learn-ing at a global context (Haghighi et al, 2005; Sut-ton and McCallum, 2005) and, consequently, theyare able to extract features from a complete labelingof a predicate.
Basically, the central feature in thiscontext extracts the sequential pattern of predicatearguments.
Then, this pattern can be enriched withsyntactic categories, broken down into role-specificindicator variables, or conjoined with the predicatelemma.Apart from basic feature extraction, combinationof features has also been explored in this edition.Many of the combinations depart from the manuallyselected conjunctions of Xue and Palmer (2004).4.4 EvaluationA baseline rate was computed for the task.
Itwas produced using a system developed in the pastshared task edition by Erik Tjong Kim Sang, fromthe University of Amsterdam, The Netherlands.
Thebaseline processor finds semantic roles based on thefollowing seven rules:?
Tag target verb and successive particles as V.?
Tag not and n?t in target verb chunk asAM-NEG.?
Tag modal verbs in target verb chunk asAM-MOD.?
Tag first NP before target verb as A0.?
Tag first NP after target verb as A1.?
Tag that, which and who before target verbas R-A0.?
Switch A0 and A1, and R-A0 and R-A1 if thetarget verb is part of a passive VP chunk.
AVP chunk is considered in passive voice if itcontains a form of to be and the verb doesnot end in ing.Table 6 presents the overall results obtained bythe nineteen systems plus the baseline, on the de-velopment and test sets (i.e., Development, TestWSJ, Test Brown, and Test WSJ+Brown).
The sys-tems are sorted by the performance on the combinedWSJ+Brown test set.As it can be observed, all systems clearly outper-formed the baseline.
There are seven systems with afinal F1 performance in the 75-78 range, seven morewith performances in the 70-75 range, and five witha performance between 65 and 70.
The best perfor-mance was obtained by Punyakanok et al (2005),which almost reached an F1 at 80 in the WSJ testset and almost 78 in the combined test.
Their resultson the WSJ test equal the best results published sofar on this task and datasets (Pradhan et al, 2005a),though they are not directly comparable due to adifferent setting in defining arguments not perfectlymatching the predicted parse constituents.
Since theevaluation in the shared task setting is more strict,we believe that the best results obtained in the sharedtask represent a new breakthrough in the SRL task.It is also quite clear that the systems using com-bination are better than the individuals.
It is worthnoting that the first 4 systems are combined.
The161Development Test WSJ Test Brown Test WSJ+BrownP(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1punyakanok 80.05 74.83 77.35 82.28 76.78 79.44 73.38 62.93 67.75 81.18 74.92 77.92haghighi 77.66 75.72 76.68 79.54 77.39 78.45 70.24 65.37 67.71 78.34 75.78 77.04marquez 78.39 75.53 76.93 79.55 76.45 77.97 70.79 64.35 67.42 78.44 74.83 76.59pradhan 80.90 75.38 78.04 81.97 73.27 77.37 73.73 61.51 67.07 80.93 71.69 76.03surdeanu 79.14 71.57 75.17 80.32 72.95 76.46 72.41 59.67 65.42 79.35 71.17 75.04tsai 81.13 72.42 76.53 82.77 70.90 76.38 73.21 59.49 65.64 81.55 69.37 74.97che 79.65 71.34 75.27 80.48 72.79 76.44 71.13 59.99 65.09 79.30 71.08 74.97moschitti 74.95 73.10 74.01 76.55 75.24 75.89 65.92 61.83 63.81 75.19 73.45 74.31tjongkimsang 76.79 70.01 73.24 79.03 72.03 75.37 70.45 60.13 64.88 77.94 70.44 74.00yi 75.70 69.99 72.73 77.51 72.97 75.17 67.88 59.03 63.14 76.31 71.10 73.61ozgencil 73.57 71.87 72.71 74.66 74.21 74.44 65.52 62.93 64.20 73.48 72.70 73.09johansson 73.40 70.85 72.10 75.46 73.18 74.30 65.17 60.59 62.79 74.13 71.50 72.79cohn 73.51 68.98 71.17 75.81 70.58 73.10 67.63 60.08 63.63 74.76 69.17 71.86park 72.68 69.16 70.87 74.69 70.78 72.68 64.58 60.31 62.38 73.35 69.37 71.31mitsumori 71.68 64.93 68.14 74.15 68.25 71.08 63.24 54.20 58.37 72.77 66.37 69.43venkatapathy 71.88 64.76 68.14 73.76 65.52 69.40 65.25 55.72 60.11 72.66 64.21 68.17ponzetto 71.82 61.60 66.32 75.05 64.81 69.56 66.69 52.14 58.52 74.02 63.12 68.13lin 70.11 61.96 65.78 71.49 64.67 67.91 65.75 52.82 58.58 70.80 63.09 66.72sutton 64.43 63.11 63.76 68.57 64.99 66.73 62.91 54.85 58.60 67.86 63.63 65.68baseline 50.00 28.98 36.70 51.13 29.16 37.14 62.66 33.07 43.30 52.58 29.69 37.95Table 6: Overall precision, recall and F1 rates obtained by the 19 participating systems in the CoNLL-2005shared task on the development and test sets.
Systems sorted by F1 score on the WSJ+Brown test set.best individual system on the task is that of Sur-deanu and Turmo (2005), which obtained F1=75.04on the combined test set, about 3 points below thanthe best performing combined system.
On the de-velopment set, that system achieved a performaceof 75.17 (slightly below than the 75.27 reported byChe et al (2005) on the same dataset).
Accord-ing to the description papers, we find that otherindividual systems, from which the combined sys-tems are constructed, performed also very well.
Forinstance, Tsai et al (2005) report F1=75.76 for abase system on the development set, Ma`rquez et al(2005) report F1=75.75, Punyakanok et al (2005)report F1=74.76, and Haghighi et al (2005) reportF1=74.52.The best results in the CoNLL-2005 shared taskare 10 points better than those of last year edition.This increase in performance should be attributed toa combination of the following factors: 1) trainingsets have been substantially enlarged; 2) predictedparse trees are available as input information; and 3)more sophisticated combination schemes have beenimplemented.
In order to have a more clear idea ofthe impact of enriching the syntactic information,we refer to (Ma`rquez et al, 2005), who developedan individual system based only on partial parsing(?upc?
input information).
That system performedF1=73.57 on the development set, which is 2.18points below the F1=75.75 obtained by the same ar-chitecture using full parsing, and 4.47 points belowthe best performing combined system on the devel-opment set (Pradhan et al, 2005b).Comparing the results across development andWSJ test corpora, we find that, with two exceptions,all systems experienced a significant increase in per-formance (normally between 1 and 2 F1 points).This fact may be attributed to the different levels ofdifficulty found across WSJ sections.
The linguisticprocessors and parsers perform slightly worse in thedevelopment set.
As a consequence, the matchingbetween parse nodes and actual arguments is lower.Regarding the evaluation using the Brown testset, all systems experienced a severe drop in perfor-mance (about 10 F1 points), even though the base-line on the Brown test set is higher than that ofthe WSJ test set.
As already said in previous sec-tions, all the linguistic processors, from PoS tag-ging to full parsing, showed a much lower perfor-mance than in the WSJ test set, evincing that theirperformance cannot be extrapolated across corpora.Presumably, this fact is the main responsible of theperformace drop, though we do not discard an ad-ditional overfitting effect due to the design of spe-cific features that do not generalize well.
More im-162portantly, this results impose (again) a severe criti-cism on the current pipelined architecture for Natu-ral Language Processing.
Error propagation and am-plification through the chained modules make the fi-nal output generalize very badly when changing thedomain of application.5 ConclusionWe have described the CoNLL-2005 shared taskon semantic role labeling.
Contrasting with theCoNLL-2004 edition, the current edition has in-corporated the use of full syntax as input to theSRL systems, much larger training sets, and cross-corpora evaluation.
The first two novelties havemost likely contributed to an improvement of re-sults.
The latter has evinced a major drawback ofnatural language pipelined architectures.Nineteen teams have participated to the task, con-tributing with a variety of learning algorithms, la-beling strategies, feature design and experimenta-tion.
While, broadly, all systems make use of thesame basic techniques described in existing SRLliterature, some novel aspects have also been ex-plored.
A remarkable aspect, common in the fourtop-performing systems and many other, is thatof combining many individual SRL systems, eachworking on different syntactic structures.
Combin-ing systems improves robustness, and overcomesthe limitations in coverage that working with a sin-gle, non-correct syntactic structure imposes.
Thebest system, presented by Punyakanok et al (2005),achieves an F1 at 79.44 on the WSJ test.
This per-formance, of the same order than the best reportedin literature, is still far from the desired behavior ofa natural language analyzer.
Furthermore, the per-formance of such SRL module in a real applicationwill be about ten points lower, as demonstrated inthe evaluation on the sentences from Brown.We conclude with two open questions.
First, whatsemantic knowledge is needed to improve the qual-ity and performance of SRL systems.
Second, be-yond pipelines, what type of architectures and lan-guage learning methodology ensures a robust per-formance of processors.AcknowledgementsAuthors would like to thank the following people and institu-tions.
The PropBank team, and specially Martha Palmer andBenjamin Snyder, for making available PropBank-1.0 and theprop-banked Brown files.
The Linguistic Data Consortium, forissuing a free evaluation license for the shared task to use theTreeBank.
Hai Leong Chieu and Hwee Tou Ng, for runningtheir Named Entity tagger on the task data.
Finally, the teamscontributing to the shared task, for their great enthusiasm.This work has been partially funded by the European Com-munity (Chil - IP506909; PASCAL - IST-2002-506778) andthe Spanish Ministry of Science and Technology (Aliado,TIC2002-04447-C02).ReferencesXavier Carreras and Llu?
?s Ma`rquez.
2003.
Phrase recog-nition by filtering and ranking with perceptrons.
InProceedings of RANLP-2003, Borovets, Bulgaria.Xavier Carreras and Llu?
?s Ma`rquez.
2004.
Introductionto the CoNLL-2004 Shared Task: Semantic Role La-beling.
In Proceedings of CoNLL-2004.Eugene Charniak.
2000.
A maximum-entropy inspiredparser.
In Proceedings of NAACL-2000.Wanxiang Che, Ting Liu, Sheng Li, Yuxuan Hu, andHuaijun Liu.
2005.
Semantic role labeling systemusing maximum entropy classifier.
In Proceedings ofCoNLL-2005.Hai Leong Chieu and Hwee Tou Ng.
2003.
Named en-tity recognition with a maximum entropy approach.
InProceedings of CoNLL-2003, Edmonton, Canada.Trevor Cohn and Philip Blunsom.
2005.
Semantic rolelabelling with tree conditional random fields.
In Pro-ceedings of CoNLL-2005.Michael Collins.
1999.
Head-driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Charles J. Fillmore, Charles Wooters, and Collin F.Baker.
2001.
Building a large lexical databank whichprovides deep semantics.
In Proceedings of the Pa-cific Asian Conference on Language, Informa tion andComputation, Hong Kong, China.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2003.
Fast and accu-rate part-of-speech tagging: The svm approach revis-ited.
In Proceedings of RANLP-2003, Borovets, Bul-garia.163Aria Haghighi, Kristina Toutanova, and ChristopherManning.
2005.
A joint model for semantic role la-beling.
In Proceedings of CoNLL-2005.Richard Johansson and Pierre Nugues.
2005.
Sparsebayesian classification of predicate arguments.
In Pro-ceedings of CoNLL-2005.Chi-San Lin and Tony C. Smith.
2005.
Semantic rolelabeling via consensus in pattern-matching.
In Pro-ceedings of CoNLL-2005.Ken Litkowski.
2004.
Senseval-3 task: Automatic label-ing of semantic roles.
In Proceedings of the Senseval-3ACL-SIGLEX Workshop.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19.Llu?
?s Ma`rquez, Pere Comas, Jesu?s Gime?nez, and NeusCatala`.
2005.
Semantic role labeling as sequentialtagging.
In Proceedings of CoNLL-2005.Tomohiro Mitsumori, Masaki Murata, Yasushi Fukuda,Kouichi Doi, and Hirohumi Doi.
2005.
Semantic rolelabeling using support vector machines.
In Proceed-ings of CoNLL-2005.Alessandro Moschitti, Ana-Maria Giuglea, BonaventuraCoppola, and Roberto Basili.
2005.
Hierarchical se-mantic role labeling.
In Proceedings of CoNLL-2005.Alessandro Moschitti.
2004.
A study on convolutionkernel for shallow semantic parsing.
In Proceedingsof the 42nd Annual Conference of the Association forComputational Linguistics (ACL-2004).Necati Ercan Ozgencil and Nancy McCracken.
2005.Semantic role labeling using libSVM.
In Proceedingsof CoNLL-2005.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1).Kyung-Mi Park and Hae-Chang Rim.
2005.
Maximumentropy based semantic role labeling.
In Proceedingsof CoNLL-2005.Simone Paolo Ponzetto and Michael Strube.
2005.
Se-mantic role labeling using lexical statistical informa-tion.
In Proceedings of CoNLL-2005.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James Martin, and Daniel Jurafsky.2005a.
Support vector learning for semantic argu-ment classification.
Machine Learning.
Special issueon Speech and Natural Language Processing.
To ap-pear.Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.Martin, and Daniel Jurafsky.
2005b.
Semantic rolechunking combining complementary syntactic views.In Proceedings of CoNLL-2005.Vasin Punyakanok, Dan Roth, Wen-Tau Yih, and Dav Zi-mak.
2004.
Semantic role labeling via integer lin-ear programming inference.
In Proceedings of the In-ternational Conference on Computational Linguistics(COLING).Vasin Punyakanok, Peter Koomen, Dan Roth, and Wentau Yih.
2005.
Generalized inference with multi-ple semantic role labeling systems.
In Proceedings ofCoNLL-2005.Mihai Surdeanu and Jordi Turmo.
2005.
Semantic rolelabeling using complete syntactic analysis.
In Pro-ceedings of CoNLL-2005.Mihai Surdeanu, Sanda Harabagiu, John Williams, andPaul Aarseth.
2003.
Using predicate-argument struc-tures for information extraction.
In Proceedings ofACL 2003, Sapporo, Japan.Charles Sutton and Andrew McCallum.
2005.
Jointparsing and semantic role labeling.
In Proceedings ofCoNLL-2005.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proceedingsof CoNLL-2003.Erik Tjong Kim Sang, Sander Canisius, Antal van denBosch, and Toine Bogers.
2005.
Applying spelling er-ror correction techniques for improving semantic rolelabelling.
In Proceedings of CoNLL-2005.Tzong-Han Tsai, Chia-Wei Wu, Yu-Chun Lin, and Wen-Lian Hsu.
2005.
Exploiting full parsing informationto label semantic roles using an ensemble of me andsvm via integer linear programming.
In Proceedingsof CoNLL-2005.Sriram Venkatapathy, Akshar Bharati, and PrashanthReddy.
2005.
Inferring semantic roles using sub-categorization frames and maximum entropy model.In Proceedings of CoNLL-2005.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP).Szu-ting Yi and Martha Palmer.
2005.
The integration ofsyntactic parsing and semantic role labeling.
In Pro-ceedings of CoNLL-2005.164
