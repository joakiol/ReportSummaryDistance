Proceedings of NAACL HLT 2007, pages 308?315,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsExtracting Appraisal ExpressionsKenneth Bloom and Navendu Garg and Shlomo ArgamonComputer Science DepartmentIllinois Institute of Technology10 W. 31st St.Chicago, IL 60616{kbloom1,gargnav,argamon}@iit.eduAbstractSentiment analysis seeks to characterizeopinionated or evaluative aspects of nat-ural language text.
We suggest here thatappraisal expression extraction should beviewed as a fundamental task in sentimentanalysis.
An appraisal expression is a tex-tual unit expressing an evaluative stancetowards some target.
The task is to findand characterize the evaluative attributesof such elements.
This paper describes asystem for effectively extracting and dis-ambiguating adjectival appraisal expres-sions in English outputting a generic rep-resentation in terms of their evaluativefunction in the text.
Data mining on ap-praisal expressions gives meaningful andnon-obvious insights.1 IntroductionSentiment analysis, which seeks to analyze opin-ion in natural language text, has grown in interestin recent years.
Sentiment analysis includes a vari-ety of different problems, including: sentiment clas-sification techniques to classify reviews as positiveor negative, based on bag of words (Pang et al,2002) or positive and negative words (Turney, 2002;Mullen and Collier, 2004); classifying sentences ina document as either subjective or objective (Riloffand Wiebe, 2003; Pang and Lee, 2004); identifyingor classifying appraisal targets (Nigam and Hurst,2004); identifying the source of an opinion in a text(Choi et al, 2005), whether the author is expressingthe opinion, or whether he is attributing the opinionto someone else; and developing interactive and vi-sual opinion mining methods (Gamon et al, 2005;Popescu and Etzioni, 2005).
Much of this work hasutilized the fundamental concept of ?semantic orien-tation?, (Turney, 2002); however, sentiment analysisstill lacks a ?unified field theory?.We propose in this paper that a fundamental taskunderlying many of these formulations is the extrac-tion and analysis of appraisal expressions, definedas those structured textual units which express anevaluation of some object.
An appraisal expressionhas three main components: an attitude (which takesan evaluative stance about an object), a target (theobject of the stance), and a source (the person tak-ing the stance) which may be implied.The idea of appraisal extraction is a generaliza-tion of problem formulations developed in earlierworks.
Mullen and Collier?s (2004) notion of classi-fying appraisal terms using a multidimensional setof attributes is closely tied to the definition of anappraisal expression, which is classified along sev-eral dimensions.
In previous work (Whitelaw etal., 2005), we presented a related technique of find-ing opinion phrases, using a multidimensional setof attributes and modeling the semantics of mod-ifiers in these phrases.
The use of multiple textclassifiers by Wiebe and colleagues (Wilson et al,2005; Wiebe et al, 2004) for various kinds of senti-ment classification can also be viewed as a sentence-level technique for analyzing appraisal expressions.Nigam and Hurst?s (2004) work on detecting opin-ions about a certain topic presages our notion ofconnecting attitudes to targets, while Popescu andEtzioni?s (2005) opinion mining technique also fitswell into our framework.In this paper we describe a system for extractingadjectival appraisal expressions, based on a hand-built lexicon, a combination of heuristic shallowparsing and dependency parsing, and expectation-maximization word sense disambiguation.
Each ex-308tracted appraisal expression is represented as a set offeature values in terms of its evaluative function inthe text.
We have applied this system to two domainsof texts: product reviews, and movie reviews.
Man-ual evaluation of the extraction shows our system towork well, as well as giving some directions for im-provement.
We also show how straightforward datamining can give users very useful information aboutpublic opinion.2 Appraisal ExpressionsWe define an appraisal expression to be an elemen-tary linguistic unit that conveys an attitude of somekind towards some target.
An appraisal expressionis defined to comprise a source, an attitude, and atarget, each represented by various attributes.
Forexample, in ?I found the movie quite monotonous?,the speaker (the Source) expresses a negative Atti-tude (?quite monotonous?)
towards ?the movie?
(theTarget).
Note that attitudes come in different types;for example, ?monotonous?
describes an inherentquality of the Target, while ?loathed?
would describethe emotional reaction of the Source.Attitude may be expressed through nouns, verbs,adjectives and metaphors.
Extracting all of this in-formation accurately for all of these types of ap-praisal expressions is a very difficult problem.
Wetherefore restrict ourselves for now to adjectival ap-praisal expressions that are each contained in a sin-gle sentence.
Additionally, we focus here only onextracting and analyzing the attitude and the target,but not the source.
Even with these restrictions, weobtain interesting results (Sec.
7).2.1 Appraisal attributesOur method is grounded in Appraisal Theory, devel-oped by Martin and White (2005), which analyzesthe way opinion is expressed.
Following Martin andWhite, we define:Attitude type is type of appraisal beingexpressed?one of affect, appreciation, orjudgment (Figure 1).
Affect refers to anemotional state (e.g., ?happy?, ?angry?
), andis the most explicitly subjective type of ap-praisal.
The other two types express evaluationof external entities, differentiating betweenintrinsic appreciation of object properties (e.g.,?slender?, ?ugly?)
and social judgment (e.g.,?heroic?, ?idiotic?
).Orientation is whether the attitude is positiveAttitude TypeAppreciationCompositionBalance: consistent, discordant, ...Complexity: elaborate, convoluted, ...ReactionImpact: amazing, compelling, dull, ...Quality: beautiful, elegant, hideous, ...Valuation: innovative, profound, inferior, ...Affect: happy, joyful, furious, ...JudgmentSocial EsteemCapacity: clever, competent, immature, ...Tenacity: brave, hard-working, foolhardy, ...Normality: famous, lucky, obscure, ...Social SanctionPropriety: generous, virtuous, corrupt, ...Veracity: honest, sincere, sneaky, ...Figure 1: The Attitude Type taxonomy, with exam-ples of adjectives from the lexicon.(?good?)
or negative (?bad?
).Force describes the intensity of the appraisal.
Forceis largely expressed via modifiers such as?very?
(increased force), or ?slightly?
(de-creased force), but may also be expressed lex-ically, for example ?greatest?
vs. ?great?
vs.?good?.Polarity of an appraisal is marked if it is scoped ina polarity marker (such as ?not?
), or unmarkedotherwise.
Other attributes of appraisal are af-fected by negation; e.g., ?not good?
also has theopposite orientation from ?good?.Target type is a domain-dependent semantic typefor the target.
This attribute takes on valuesfrom a domain-dependent taxonomy, represent-ing important (and easily extractable) distinc-tions between targets in the domain.2.2 Target taxonomiesTwo domain-dependent target type taxonomies areshown in Figure 2.
In both, the primary distinctionis between a direct naming of a kind of ?Thing?
or adeictic/pronominal reference (e.g., ?those?
or ?it?
),since the system does not currently rely on corefer-ence resolution.
References are further divided intoreferences to the writer/reader (?interactants?)
and toother people or objects.The Thing subtrees for the two domains dif-fer somewhat.
In the movie domain, Things suchas ?this movie?, ?Nicholas Cage?, or ?cinematogra-phy?, are classified into six main categories: movies(the one being reviewed, or another one), people309Movie Target TypeMovie ThingAny MovieThis MovieOther MovieMovie PersonReal Person.
.
.CharacterMovie Aspect.
.
.CompanyMarketingReferenceInteractantFirst PersonSecond PersonOtherThird PersonDeicticProduct Target TypeProduct ThingAny ProductThis ProductOther ProductProduct PartIntegralReplaceableExperienceCompanyMarketingSupportReferenceInteractantFirst PersonSecond PersonOtherThird PersonDeicticFigure 2: Target taxonomies for movie and productreviews.
(whether characters, or real people involved in mak-ing the film), aspects of the movie itself (its plot,special effects, etc.
), the companies involved in mak-ing it, or aspects of marketing the movie (suchas trailers).
For target Things in product reviews,we replace ?Movie Person?
and ?Movie Aspect?
by?Product Part?
with two subcategories: ?Integral?, forparts of the product itself (e.g., wheels or lenses),and ?Replaceable?, for parts or supplies meant tobe periodically replaced (e.g., batteries or ink car-tridges).
The categories of ?Support?, for referencesto aspects of customer support, and ?Experience?
forthings associated with the experience of using theproduct (such as ?pictures?
or ?resolution?, were alsoadded.3 Appraisal ExtractionIn our system, appraisal extraction runs in several in-dependent stages.
First, the appraisal extractor findsappraisal expressions by finding the chunks of textthat express attitudes and targets.
Then, it links eachattitude group found to a target in the text.
Finally, ituses a probabilistic model to determine which atti-tude type should be assigned when attitude chunkswere ambiguous.3.1 ChunkingThe chunker is based on our earlier work (Whitelawet al, 2005), which finds attitude groups and tar-gets using a hand-built lexicon (Sec.
4).
This lexi-con contains head adjectives (which specify valuesfor the attributes attitude type, force, polarity, andorientation), and appraisal modifiers (which specifytransformations to the four attributes).
Some headadjectives are ambiguous, having multiple entries inthe lexicon with different attribute values.
In allcases, different entries for a given word have dif-ferent attitude types.
If the head adjective is am-biguous, multiple groups are created, to be disam-biguated later.
See our previous work (Whitelaw etal., 2005) for a discussion of the technique.Target groups are found by matching phrases inthe lexicon with corresponding phrases in the textand assigning the target type listed in the lexicon.3.2 LinkingAfter finding attitude groups and candidate targets,the system links each attitude to a target.
Eachsentence is parsed to a dependency representation,and a ranked list of linkage specifications is usedto look for paths in the dependency tree connectingsome word in the source to some word in the target.Such linkage specifications are hand-constructed,and manually assigned priorities, so that when twolinkage specifications match, only the highest prior-ity specification is used.
For example, the two high-est priority linkage specifications are:1.
targetnsubj????
xdobj???
yamod????
attitude2.
attitudeamod????
targetThe first specification selects the subject of a sen-tence where the appraisal modifies a noun in thepredicate, for example ?The Matrix?
in ?The Matrixis a good movie?.
The second selects the noun mod-ified by an adjective group, for example ?movie?
in?The Matrix is a good movie?.If no linkage is found connecting an attitude to acandidate target, the system goes through the link-age specifications again, trying to find any word inthe sentence connected to the appraisal group by aknown linkage.
The selected word is assigned thegeneric category ofmovie thing or product thing (de-pending on the domain of the text).
If no linkage isfound at all, the system assigns the default categorymovie thing or product thing, assuming that there isan appraised thing that couldn?t be found using thegiven linkage specifications.3103.3 DisambiguationAfter linkages are made, this information is used todisambiguate multiple senses that may be presentin a given appraisal expression.
Most cases areunambiguous, but in some cases two, or occasion-ally even three, senses are possible.
We bootstrapfrom the unambiguous cases, using a probabilisticmodel, to resolve the ambiguities.
The attitudetype places some grammatical/semantic constraintson the clause.
Two key constraints are the syntacticrelation with the target (which can differentiate af-fect from the other types of appraisal), and whetherthe target type has consciousness (which helps dif-ferentiate judgment and affect from appreciation).To capture these constraints, we model the proba-bility of a given attitude type being correct, giventhe target type and the linkage specification used toconnect the attitude to the target, as follows.The correct attitude type of an appraisal expres-sion is modeled by a random variable A, the set ofall attitude types in the system is denoted by A, anda specific attitude type is denoted by a.
As describedabove, other attributes besides attitude type mayalso vary between word senses, but attitude typealways changes between word senses, so when thesystem assigns a probability to an attitude type, itis assigning that probability to the whole word sense.We denote the linkage type used in a given ap-praisal expression by L, the set of all possible link-ages asL, and a specific linkage type by l. Note thatthe first attempt with a linkage specification (to finda chunked target) is considered to be different fromthe second attempt with the same linkage specifica-tion (which attempts to find any word).
Failure tofind an applicable linkage rule is considered as yetanother ?linkage?
for the probability model.
Sinceour system uses 29 different linkage specifications,there are a total of 59 different possible linkagestypes.The target type of a given appraisal expression isdenoted by T , the set of all target types by T , and aspecific target type by t. We consider an expressionto have a given target type T = t only if that is itsspecific target type; if its target type is a descendantof t, then its target type is not t in the model.
Edenotes the set of all extracted appraisal expressions.The term exp denotes a specific expression.Our goal is to estimate, for each appraisal expres-sion exp in the corpus, the probability of its attitudetype being a, given the expression?s target type tand linkage type lP (A = a|exp) = P (A = a|T = t, L = l)To do this, we define a model M of this probability,and then estimate the maximum likelihood modelusing Expectation-Maximization.We model PM (A = a|T = t, L = l) by firstapplying Bayes?
theorem:PM (A = a|T = t, L = l) =PM (T = t, L = l|A = a)PM (A = a)PM (T = t, L = l)Assuming conditional independence of target typeand linkage, this becomes:PM (T = t|A = a)PM (L = l|A = a)PM (A = a)PM (T = t)PM (L = l)M ?s parameters thus represent the conditional andmarginal probabilities on this right-hand-side.Given a set of (possibly ambiguous) appraisal ex-pressions E identified by chunking and linkage de-tection, we seek the maximum likelihood modelM?
= argmaxM?exp?E?a?AM(A = a|exp)M?
will be our best estimate of P , given the pro-cessed data in a given corpus.
The system esti-mates M?
using an implementation of Expectation-Maximization over the entire corpus.
The highest-probability attitude type (hence sense) according toM is then chosen for each appraisal expression.4 The LexiconAs noted above, attitude groups were identified via adomain-independent lexicon of appraisal adjectives,adverbs, and adverb modifiers.
1 For the moviedomain, appraised things were identified based ona manually constructed lexicon containing genericmovie words, as well as automatically constructedlexicons of proper names specific to each movie be-ing reviewed.
For each product type considered, wemanually constructed a lexicon containing genericproduct words; we did not find it necessary to con-struct product-specific lexicons.1All of the lexicons used in the paper can befound at http://lingcog.iit.edu/arc/appraisal lexicon 2007a.tar.gz311For adjectival attitudes, we used the lexicondeveloped we developed in our previous work(Whitelaw et al, 2005) on appraisal.
We reviewedthe entire lexicon to determine its accuracy andmade numerous improvements.Generic target lexicons were constructed by start-ing with a small sample of the kind of reviewsthat the lexicon would apply to.
We examinedthese manually to find generic words referring to ap-praised things to serve as seed terms for the lexiconand used WordNet (Miller, 1995) to suggest addi-tional terms to add to the lexicon.Since movie reviews often refer to the specificcontents of the movie under review by proper names(of actors, the director, etc.
), we also automaticallyconstructed a specific target lexicon for each moviein the corpus, based on lists of actors, characters,writers, directors, and companies listed for the filmat imdb.com.
Each such specific lexicon was onlyused for processing reviews of the movie it was gen-erated for, so the system had no specific knowledgeof terms related to other movies during processing.5 CorporaWe evaluated our appraisal extraction system on twocorpora.
The first is the standard publicly availablecollection of movie reviews constructed by Pang andLee (2004).
This standard testbed consists of 1000positive and 1000 negative reviews, taken from theIMDb movie review archives2.
Reviews with ?neu-tral?
scores (such as three stars out of five) were re-moved by Pang and Lee, giving a data set with onlyclearly positive and negative reviews.
The averagedocument length in this corpus is 764 words, and1107 different movies are reviewed.The second corpus is a collection of user prod-uct reviews taken from epinions.com suppliedin 2004 for research purposes by Amir Ashkenaziof Shopping.Com.
The base collection contains re-views for three types of products: baby strollers, dig-ital cameras, and printers.
Each review has a numer-ical rating (1?5); based on this, we labeled positiveand negative reviews in the same way as Pang andLee did for the movie reviews corpus.
The prod-ucts corpus has 15162 documents, averaging 442words long.
This comprises 11769 positive docu-ments, 1420 neutral documents, and 1973 negativedocuments.
There are 905 reviews of strollers, 57782See http://www.cs.cornell.edu/people/pabo/movie-review-data/reviews of ink-jet printers and 8479 reviews of digi-tal cameras, covering 516 individual products.Each document in each corpus was preprocessedinto individual sentences, lower-cased, and tok-enized.
We used an implementation of Brill?s (1992)part-of-speech tagger to find adjectives and modi-fiers; for parsing, we used the Stanford dependencyparser (Klein and Manning, 2003).6 Evaluating ExtractionWe performed two manual evaluations on the sys-tem.
The first was to evaluate the overall accuracyof the entire system.
The second was to specifi-cally evaluate the accuracy of the probabilistic dis-ambiguator.6.1 Evaluating AccuracyWe evaluated randomly selected appraisal expres-sions for extraction accuracy on a number of binarymeasures.
This manual evaluation was performedby the first author.We evaluated interrater reliabilitybetween this rater and another author on 200 ran-domly selected appraisal expressions (100 on eachcorpus).
The first rater rated an additional 120 ex-pressions (60 for each corpus), and combined thesewith his ratings for interrater reliability to computesystem accuracy, for a total of 320 expressions (160for each corpus).
The (binary) rating criteria were asfollows.
Relating to the appraisal group:APP Does the expression express appraisal at all?ARM If so, does the appraisal group have all rele-vant modifiers?HEM Does the appraisal group include extra mod-ifiers?
(Results are shown negated, so thathigher numbers are better.
)Relating to the target:HT If there is appraisal, is there an identifiable tar-get (even if the system missed it)?FT If there is appraisal, did the system identifysome target?
(Determined automatically.
)RT If so, is it the correct one?Relating to the expression?s attribute values (if it ex-presses appraisal):Att Is the attitude type assigned correct?Ori Is the orientation assigned correct?Pol Is the polarity assigned correct?Tar Is the target type assigned correct?Pre Is the target type the most precise value in thetaxonomy for this target?312Table 1: System accuracy at evaluated tasks.
95%confidence one-proportion z-intervals are reported.Measure Movies Products CombinedAPP 86% ?
3% 81% ?
3% 83% ?
2%ARM 94% ?
2% 95% ?
2% 95% ?
1%?
HEM 99% ?
1% 100% 99.6% ?
0.4%HT 91% ?
2% 97% ?
2% 94% ?
1%FT 96% ?
2% 94% ?
2% 95% ?
1%RT 77% ?
4% 73% ?
4% 75% ?
3%Att 78% ?
4% 80% ?
4% 79% ?
2%Ori 95% ?
2% 95% ?
2% 94% ?
1%Pol 97% ?
1% 96% ?
2% 97% ?
1%Tar 84% ?
3% 86% ?
3% 85% ?
2%Pre 70% ?
4% 77% ?
4% 73% ?
3%Table 2: Interrater reliability of manual evaluation.95% confidence intervals are reported.Measure Movies Products CombinedAPP 71% ?
9% 87% ?
7% 79% ?
6%ARM 95% ?
5% 91% ?
6% 93% ?
4%?
HEM 98% ?
3% 100% 99% ?
1%HT 97% ?
4% 99% ?
3% 98% ?
3%FT N/A N/A N/ART 94% ?
6% 97% ?
4% 96% ?
4%Att 79% ?
10% 86% ?
8% 83% ?
6%Ori 93% ?
6% 94% ?
5% 93% ?
4%Pol 96% ?
4% 94% ?
5% 95% ?
4%Tar 94% ?
6% 90% ?
7% 91% ?
5%Pre 86% ?
10% 90% ?
8% 88% ?
6%Results are given in Table 1, and interrater relia-bility is given in Table 2.
In nearly all cases agree-ment percentages are above 80%, indicating goodinter-rater consensus.
Regarding precision, we notethat most aspects of extraction seem to work quitewell.
The area of most concern in the system isprecision of target classification.
This may be im-proved with further development of the target lex-icons to classify more terms to specific leaves inthe target type hierarchy.
The other area of con-cern is the APP test, which encountered difficultieswhen a word could be used as appraisal in somecontexts, but not in others, particularly when an ap-praisal word appeared as a nominal classifier.6.2 Evaluating DisambiguationThe second experiment evaluated the accuracy ofEM in disambiguating the attitude type of appraisalexpressions.
We evaluated the same number of ex-pressions as used for the overall accuracy experi-ment (100 used for interrater reliability and accu-racy, plus 60 used only for accuracy on each corpus),each having two or more word senses, presenting allof the attitude types possible for each appraisal ex-pression, as well as a ?none of the above?
and a ?notappraisal?
option, asking the rater to select whichone applied to the selected expression in context.Baseline disambiguator accuracy, if the computerwere to simply pick randomly from the choicesspecified in the lexicon is 48% for both corpora.
In-terrater agreement was 80% for movies and 73% forproducts (taken over 100 expressions from each cor-pus.
)Considering just those appraisal expressionswhich the raters decided were appraisal, the dis-ambiguator achieved 58% accuracy on appraisal ex-pressions from the movies corpus and 56% accuracyon the products corpus.
Further analysis of the re-sults of the disambiguator shows that most of the er-rors occur when the target type is the generic cate-gory thing which occurs when the target is not in thetarget lexicon.
Performance on words recognized ashaving more specific target types is better: 68% formovies, and 59% for products.
This indicates thatspecific target type is an important indicator of at-titude type.7 Opinion MiningWe (briefly) demonstrate the usefulness of appraisalexpression extraction by using it for opinion mining.In opinion mining, we find large numbers of reviewsand perform data mining to determine which aspectsof a product people like or dislike, and in whichways.
To do this, we search for association rules de-scribing the appraisal features that can be found ina single appraisal expression.
We generally look forrules that contain attitude type, orientation, thingtype, and a product name, when these rules occurmore frequently than expected.The idea is similar to Agrawal andSrikant?s (1995) notion of generalized associa-tion rules.
We treat each appraisal expression asa transaction, with the attributes of attitude type,orientation, polarity, force, and thing type, as wellas the document attributes product name, producttype, and document classification (based on thenumber of stars the reviewer gave the product).We use CLOSET+ (Wang et al, 2003) to find allof the frequent closed itemsets in the data, with asupport greater than or equal to 20 occurrences.Let ?b, a1, a2, .
.
.
an?
or ?b, A?
denote the contentsof an itemset, and c (?b, A?)
denote the support forthis itemset.
For a given item b, pi(b) denotes itsimmediate parent its value taxonomy, or ?root?
forflat sets.313Table 3: The most interesting specific rules for products.b A Doc.Int.
Product Name Attitude Target Type Orientation Polarity class45.7 Peg Perego Pliko Matic (1) ?
quality this-product positive unmarked42.8 Lexmark Color JetPrinter 1100 ?
reaction this-product negative unmarked neg41.9 Peg Perego Milano XL ?
reaction this-product positive unmarked pos41.1 Peg Perego Pliko Matic ?
reaction this-product positive unmarked40.8 Peg Perego Milano XL ?
quality this-product positive unmarked pos37.5 Peg Perego Milano XL ?
reaction this-product positive unmarked37.1 Peg Perego Milano XL ?
quality this-product positive unmarked36.3 Agfa ePhoto Smile (2) ?
reaction experience negative unmarked neg36.0 Agfa ePhoto Smile (2) ?
reaction experience negative neg33.9 KB Gear KG-JC3S Jamcam ?
quality experience negative negTable 4: The most interesting oppositional rules for products.b A Doc.Int.
Product Name Attitude Target Orient.
Polarity class31.6 Lexmark Color JetPrinter 1100 (3) ?
reaction this-product positive neg31.5 Lexmark Color JetPrinter 1100 ?
quality this-product positive neg29.5 Lexmark Color JetPrinter 1100 ?
reaction this-product positive unmarked neg29.2 Lexmark Color JetPrinter 1100 ?
quality this-product positive unmarked neg28.9 Lexmark Color JetPrinter 1100 ?
appreciation this-product positive negFor each item set, we collect rules ?b, A?
andcompute their interestingness relative to the itemset?pi(b), A?.
Interestingness is defined as follows:Int =P (A|b)P (A|pi(b))=c(?b, A?)?
c(?pi(b)?
)c(?pi(b), A?)?
c(?b?
)Int is the relative probability of finding the childitemset in an appraisal expression, compared to find-ing it in a parent itemset.
Values greater than 1 in-dicate that the child itemset appears more frequentlythan we would expect.We applied two simple filters to the output, to helpfind more meaningful results.
Specificity requiresthat b be a product name, and that attitude type andthing type be sufficiently deep nodes in the hier-archy to describe something specific.
(For exam-ple, ?product thing?
gives no real information aboutwhat part of the product is being appraised.)
Oppo-sition chooses rules with a different rating than thereview as a whole, that is, document classificationis the opposite of appraisal orientation.
The filteralso ensures that thing type is sufficiently specific,as with specificity, and requires that b be a productname.We present the ten most ?interesting?
rules fromeach filter, for the products corpus.
Rules from thespecificity filter are shown in Table 3 and rules fromthe opposition filter are shown in Table 4.
We con-sider the meaning of some of these rules.The first specificity rule (1) describes a typical ex-ample of users who like the product very well over-all.
An example sentence that created this rule says?Not only is it an excellent stroller, because of it?s[sic] size it even doubled for us as a portable crib.
?The specificity rules for the Agfa ePhoto SmileDigital Camera (2) are an example of the kind ofrule we expect to see when bad user experience con-tributes to bad reviews.
The text of the reviews thatgave these rules quite clearly convey that users werenot happy specifically with the photo quality.In the oppositional rules for the Lexmark ColorJetPrinter 1100 (3), we see that users made positivecomments about the product overall, while neverthe-less giving the product a negative review.
Drillingdown into the text, we can see some examples of re-views like ?On the surface it looks like a good printerbut it has many flaws that cause it to be frustrating.
?8 ConclusionsWe have presented a new task, appraisal expres-sion extraction, which, we suggest, is a fundamentaltasks for sentiment analysis.
Shallow parsing basedon a set of appraisal lexicons, together with sparseuse of syntactic dependencies, can be used to ef-fectively address the subtask of extracting adjectivalappraisal expressions.
Indeed, straightforward datamining applied to appraisal expressions can yield in-sights into public opinion as expressed in patterns ofevaluative language in a corpus of product reviews.Immediate future work includes extending the ap-proach to include other types of appraisal expres-314sions, such as where an attitude is expressed via anoun or a verb.
In this regard, we will be examin-ing extension of existing methods for automaticallybuilding lexicons of positive/negative words (Tur-ney, 2002; Esuli and Sebastiani, 2005) to the morecomplex task of estimating also attitude type andforce.
As well, a key problem is the fact that eval-uative language is often context-dependent, and soproper interpretation must consider interactions be-tween a given phrase and its larger textual context.ReferencesRakesh Agrawal and Ramakrishnan Srikant.
1995.
Min-ing generalized association rules.
In Umeshwar Dayal,Peter M. D. Gray, and Shojiro Nishio, editors, Proc.21st Int.
Conf.
Very Large Data Bases, VLDB, pages407?419.
Morgan Kaufmann, 11?15 September.Eric Brill.
1992.
A simple rule-based part of speech tag-ger.
In Proc.
of ACL Conference on Applied NaturalLanguage Processing.Yejin Choi, Claire Cardie, Ellen Riloff, and SiddharthPatwardhan.
2005.
Identifying sources of opinionswith conditional random fields and extraction patterns.Human Language Technology Conference/Conferenceon Empirical Methods in Natural Language Process-ing (HLT-EMNLP 2005), October.Andrea Esuli and Fabrizio Sebastiani.
2005.
Determin-ing the semantic orientation of terms through glossanalysis.
In Proceedings of CIKM-05, the ACM SIGIRConference on Information and Knowledge Manage-ment, Bremen, DE.Michael Gamon, Anthony Aue, Simon Corston-Oliver,and Eric Ringger.
2005.
Pulse: Mining customeropinions from free text.
In Proceedings of IDA-05, the6th International Symposium on Intelligent Data Anal-ysis, Lecture Notes in Computer Science, Madrid, ES.Springer-Verlag.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stMeeting of the Association for Computational Linguis-tics.J.
R. Martin and P. R. R. White.
2005.
The Language ofEvaluation: Appraisal in English.
Palgrave, London.
(http://grammatics.com/appraisal/).George A. Miller.
1995.
Wordnet: A lexical database forEnglish.
Commun.
ACM, 38(11):39?41.TonyMullen and Nigel Collier.
2004.
Sentiment analysisusing support vector machines with diverse informa-tion sources.
In Proceedings of EMNLP-04, 9th Con-ference on Empirical Methods in Natural LanguageProcessing, Barcelon, ES.Kamal Nigam and Matthew Hurst.
2004.
Towards a ro-bust metric of opinion.
In Proceedings of the AAAISpring Symposium on Exploring Attitude and Affect inText: Theories and Applications, Standford, US.Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proc.
42nd ACL, pages271?278, Barcelona, Spain, July.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification using ma-chine learning techniques.
In Proceedings of EMNLP.Ana-Maria Popescu and Oren Etzioni.
2005.
Ex-tracting product features and opinions from reviews.In Proceedings of HLT-EMNLP-05, the Human Lan-guage Technology Conference/Conference on Empiri-cal Methods in Natural Language Processing, Vancou-ver, CA.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proceed-ings of EMNLP.Peter D. Turney.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proceedings 40th Annual Meetingof the ACL (ACL?02), pages 417?424, Philadelphia,Pennsylvania.Jianyong Wang, Jiawei Han, and Jian Pei.
2003.CLOSET+: searching for the best strategies for min-ing frequent closed itemsets.
In Pedro Domingos,Christos Faloutsos, Ted Senator, Hillol Kargupta,and Lise Getoor, editors, Proceedings of the ninthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining (KDD-03), pages236?245, New York, August 24?27.
ACM Press.Casey Whitelaw, Navendu Garg, and Shlomo Argamon.2005.
Using appraisal taxonomies for sentiment anal-ysis.
In Proceedings of CIKM-05, the ACM SIGIRConference on Information and Knowledge Manage-ment, Bremen, DE.Janyce Wiebe, Theresa Wilson, Rebecca Bruce, MatthewBell, and Melanie Martin.
2004.
Learning subjectivelanguage.
Computational Linguistics, 30(3).Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of Hu-man Language Technologies Conference/Conferenceon Empirical Methods in Natural Language Process-ing (HLT/EMNLP 2005), Vancouver, CA.315
