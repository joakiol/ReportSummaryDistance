Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 683?692,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsShift-Reduce CCG ParsingYue ZhangUniversity of CambridgeComputer Laboratoryyue.zhang@cl.cam.ac.ukStephen ClarkUniversity of CambridgeComputer Laboratorystephen.clark@cl.cam.ac.ukAbstractCCGs are directly compatible with binary-branching bottom-up parsing algorithms, inparticular CKY and shift-reduce algorithms.While the chart-based approach has been thedominant approach for CCG, the shift-reducemethod has been little explored.
In this paper,we develop a shift-reduce CCG parser usinga discriminative model and beam search, andcompare its strengths and weaknesses with thechart-based C&C parser.
We study differenterrors made by the two parsers, and show thatthe shift-reduce parser gives competitive accu-racies compared to C&C.
Considering our useof a small beam, and given the high ambigu-ity levels in an automatically-extracted gram-mar and the amount of information in the CCGlexical categories which form the shift actions,this is a surprising result.1 IntroductionCombinatory Categorial Grammar (CCG; Steedman(2000)) is a lexicalised theory of grammar which hasbeen successfully applied to a range of problems inNLP, including treebank creation (Hockenmaier andSteedman, 2007), syntactic parsing (Hockenmaier,2003; Clark and Curran, 2007), logical form con-struction (Bos et al, 2004) and surface realization(White and Rajkumar, 2009).
From a parsing per-spective, the C&C parser (Clark and Curran, 2007)has been shown to be competitive with state-of-the-art statistical parsers on a variety of test suites, in-cluding those consisting of grammatical relations(Clark and Curran, 2007), Penn Treebank phrase-structure trees (Clark and Curran, 2009), and un-bounded dependencies (Rimell et al, 2009).The binary branching nature of CCG means thatit is naturally compatible with bottom-up parsing al-gorithms such as shift-reduce and CKY (Ades andSteedman, 1982; Steedman, 2000).
However, theparsing work by Clark and Curran (2007), and alsoHockenmaier (2003) and Fowler and Penn (2010),has only considered chart-parsing.
In this paper wefill a gap in the CCG literature by developing a shift-reduce parser for CCG.Shift-reduce parsers have become popular for de-pendency parsing, building on the initial work of Ya-mada and Matsumoto (2003) and Nivre and Scholz(2004).
One advantage of shift-reduce parsers is thatthe scoring model can be defined over actions, al-lowing highly efficient parsing by using a greedyalgorithm in which the highest scoring action (or asmall number of possible actions) is taken at eachstep.
In addition, high accuracy can be maintainedby using a model which utilises a rich set of featuresfor making each local decision (Nivre et al, 2006).Following recent work applying global discrim-inative models to large-scale structured predictionproblems (Collins and Roark, 2004; Miyao andTsujii, 2005; Clark and Curran, 2007; Finkel etal., 2008), we build our shift-reduce parser using aglobal linear model, and compare it with the chart-based C&C parser.
Using standard developmentand test sets from CCGbank, our shift-reduce parsergives a labeled F-measure of 85.53%, which is com-petitive with the 85.45% F-measure of the C&Cparser on recovery of predicate-argument dependen-cies from CCGbank.
Hence our work shows that683transition-based parsing can be successfully appliedto CCG, improving on earlier attempts such as Has-san et al (2008).
Detailed analysis shows that ourshift-reduce parser yields a higher precision, lowerrecall and higher F-score on most of the commonCCG dependency types compared to C&C.One advantage of the shift-reduce parser is thatit easily handles sentences for which it is difficultto find a spanning analysis, which can happen withCCG because the lexical categories at the leaves of aderivation place strong contraints on the set of possi-ble derivations, and the supertagger which providesthe lexical categories sometimes makes mistakes.Unlike the C&C parser, the shift-reduce parser nat-urally produces fragmentary analyses when appro-priate (Nivre et al, 2006), and can produce sensiblelocal structures even when a full spanning analysiscannot be found.1Finally, considering this work in the wider pars-ing context, it provides an interesting comparisonbetween heuristic beam search using a rich set offeatures, and optimal dynamic programming searchwhere the feature range is restricted.
We are able toperform this comparison because the use of the CCGsupertagger means that the C&C parser is able tobuild the complete chart, from which it can find theoptimal derivation, with no pruning whatsoever atthe parsing stage.
In contrast, the shift-reduce parseruses a simple beam search with a relatively smallbeam.
Perhaps surprisingly, given the ambiguity lev-els in an automatically-extracted grammar, and theamount of information in the CCG lexical categorieswhich form the shift actions, the shift-reduce parserusing heuristic beam search is able to outperform thechart-based parser.2 CCG ParsingCCG, and the application of CCG to wide-coverageparsing, is described in detail elsewhere (Steedman,2000; Hockenmaier, 2003; Clark and Curran, 2007).Here we provide only a short description.During CCG parsing, adjacent categories are com-bined using CCG?s combinatory rules.
For example,a verb phrase in English (S\NP ) can combine with1See e.g.
Riezler et al (2002) and Zhang et al (2007) for chart-based parsers which can produce fragmentary analyses.an NP to its left using function application:NP S\NP ?
SCategories can also combine using functioncomposition, allowing the combination of ?may?
((S\NP)/(S\NP)) and ?like?
((S\NP)/NP) incoordination examples such as ?John may like butmay detest Mary?
:(S\NP)/(S\NP) (S\NP)/NP ?
(S\NP)/NPIn addition to binary rules, such as function appli-cation and composition, there are also unary ruleswhich operate on a single category in order tochange its type.
For example, forward type-raisingcan change a subject NP into a complex categorylooking to the right for a verb phrase:NP ?
S/(S\NP)An example CCG derivation is given in Section 3.The resource used for building wide-coverageCCG parsers of English is CCGbank (Hockenmaierand Steedman, 2007), a version of the Penn Tree-bank in which each phrase-structure tree has beentransformed into a normal-form CCG derivation.There are two ways to extract a grammar from thisresource.
One approach is to extract a lexicon,i.e.
a mapping from words to sets of lexical cat-egories, and then manually define the combinatoryrule schemas, such as functional application andcomposition, which combine the categories together.The derivations in the treebank are then used to pro-vide training data for the statistical disambiguationmodel.
This is the method used in the C&C parser.2The second approach is to read the completegrammar from the derivations, by extracting combi-natory rule instances from the local trees consistingof a parent category and one or two child categories,and applying only those instances during parsing.
(These rule instances also include rules to deal withpunctuation and unary type-changing rules, in addi-tion to instances of the combinatory rule schemas.
)This is the method used by Hockenmaier (2003) andis the method we adopt in this paper.Fowler and Penn (2010) demonstrate that the sec-ond extraction method results in a context-free ap-proximation to the grammar resulting from the first2Although the C&C default mode applies a restriction for effi-ciency reasons in which only rule instances seen in CCGbankcan be applied, making the grammar of the second type.684method, which has the potential to produce a mildly-context sensitive grammar (given the existence ofcertain combinatory rules) (Weir, 1988).
However,it is important to note that the advantages of CCG, inparticular the tight relationship between syntax andsemantic interpretation, are still maintained with thesecond approach, as Fowler and Penn (2010) argue.3 The Shift-reduce CCG ParserGiven an input sentence, our parser uses a stack ofpartial derivations, a queue of incoming words, anda series of actions?derived from the rule instancesin CCGbank?to build a derivation tree.
FollowingClark and Curran (2007), we assume that each inputword has been assigned a POS-tag (from the PennTreebank tagset) and a set of CCG lexical categories.We use the same maximum entropy POS-tagger andsupertagger as the C&C parser.
The derivation treecan be transformed into CCG dependencies or gram-matical relations by a post-processing step, whichessentially runs the C&C parser deterministicallyover the derivation, interpreting the derivation andgenerating the required output.The configuration of the parser, at each step ofthe parsing process, is shown in part (a) of Figure 1,where the stack holds the partial derivation trees thathave been built, and the queue contains the incomingwords that have not been processed.
In the figure,S(H) represents a category S on the stack with headword H, while Qi represents a word in the incomingqueue.The set of action types used by the parser is asfollows: {SHIFT, COMBINE, UNARY, FINISH}.Each action type represents a set of possible actionsavailable to the parser at each step in the process.The SHIFT-X action pushes the next incomingword onto the stack, and assigns the lexical categoryX to the word (Figure 1(b)).
The label X can be anylexical category from the set assigned to the wordbeing shifted by the supertagger.
Hence the shift ac-tion performs lexical category disambiguation.
Thisis in contrast to a shift-reduce dependency parser inwhich a shift action typically just pushes a word ontothe stack.The COMBINE-X action pops the top two nodesoff the stack, and combines them into a new node,which is pushed back on the stack.
The category ofFigure 1: The parser configuration and set of actions.the new node is X.
A COMBINE action correspondsto a combinatory rule in the CCG grammar (or one ofthe additional punctuation or type-changing rules),which is applied to the categories of the top twonodes on the stack.The UNARY-X action pops the top of the stack,transforms it into a new node with category X, andpushes the new node onto the stack.
A UNARY ac-tion corresponds to a unary type-changing or type-raising rule in the CCG grammar, which is applied tothe category on top of the stack.The FINISH action terminates the parsing pro-cess; it can be applied when all input words havebeen shifted onto the stack.
Note that the FINISHaction can be applied when the stack contains morethan one node, in which case the parser producesa set of partial derivation trees, each correspondingto a node on the stack.
This sometimes happenswhen a full derivation tree cannot be built due to su-pertagging errors, and provides a graceful solutionto the problem of producing high-quality fragmen-tary parses when necessary.685Figure 2: An example parsing process.Figure 2 shows the shift-reduce parsing processfor the example sentence ?IBM bought Lotus?.
Firstthe word ?IBM?
is shifted onto the stack as an NP;then ?bought?
is shifted as a transitive verb look-ing for its object NP on the right and subject NP onthe left ((S[dcl]\NP)/NP); and then ?Lotus?
is shiftedas an NP.
Then ?bought?
is combined with its ob-ject ?Lotus?
resulting in a verb phrase looking for itssubject on the left (S[dcl]\NP).
Finally, the resultingverb phrase is combined with its subject, resulting ina declarative sentence (S[dcl]).A key difference with previous work on shift-reduce dependency (Nivre et al, 2006) and CFG(Sagae and Lavie, 2006b) parsing is that, for CCG,there are many more shift actions ?
a shift action foreach word-lexical category pair.
Given the amountof syntactic information in the lexical categories, thechoice of correct category, from those supplied bythe supertagger, is often a difficult one, and oftena choice best left to the parsing model.
The C&Cparser solves this problem by building the completepacked chart consistent with the lexical categoriessupplied by the supertagger, leaving the selection ofthe lexical categories to the Viterbi algorithm.
Forthe shift-reduce parser the choice is also left to theparsing model, but in contrast to C&C the correctlexical category could be lost at any point in theheuristic search process.
Hence it is perhaps sur-prising that we are able to achieve a high parsing ac-curacy of 85.5%, given a relatively small beam size.4 DecodingGreedy local search (Yamada and Matsumoto, 2003;Sagae and Lavie, 2005; Nivre and Scholz, 2004)has typically been used for decoding in shift-reduceparsers, while beam-search has recently been ap-plied as an alternative to reduce error-propagation(Johansson and Nugues, 2007; Zhang and Clark,2008; Zhang and Clark, 2009; Huang et al, 2009).Both greedy local search and beam-search have lin-ear time complexity.
We use beam-search in ourCCG parser.To formulate the decoding algorithm, we define acandidate item as a tuple ?S,Q,F ?, where S repre-sents the stack with partial derivations that have beenbuilt, Q represents the queue of incoming words thathave not been processed, and F is a boolean valuethat represents whether the candidate item has beenfinished.
A candidate item is finished if and only ifthe FINISH action has been applied to it, and nomore actions can be applied to a candidate item af-ter it reaches the finished status.
Given an input sen-tence, we define the start item as the unfinished itemwith an empty stack and the whole input sentence asthe incoming words.
A derivation is built from thestart item by repeated applications of actions untilthe item is finished.To apply beam-search, an agenda is used to holdthe N -best partial (unfinished) candidate items ateach parsing step.
A separate candidate output is686function DECODE(input, agenda, list, N ,grammar, candidate output):agenda.clear()agenda.insert(GETSTARTITEM(input))candidate output = NONEwhile not agenda.empty():list.clear()for item in agenda:for action in grammar.getActions(item):item?
= item.apply(action)if item?.F == TRUE:if candidate output == NONE oritem?.score > candidate output.score:candidate output = item?else:list.append(item?
)agenda.clear()agenda.insert(list.best(N ))Figure 3: The decoding algorithm; N is the agenda sizeused to record the current best finished item that hasbeen found, since candidate items can be finished atdifferent steps.
Initially the agenda contains only thestart item, and the candidate output is set to none.
Ateach step during parsing, each candidate item fromthe agenda is extended in all possible ways by apply-ing one action according to the grammar, and a num-ber of new candidate items are generated.
If a newlygenerated candidate is finished, it is compared withthe current candidate output.
If the candidate outputis none or the score of the newly generated candi-date is higher than the score of the candidate output,the candidate output is replaced with the newly gen-erated item; otherwise the newly generated item isdiscarded.
If the newly generated candidate is un-finished, it is appended to a list of newly generatedpartial candidates.
After all candidate items from theagenda have been processed, the agenda is clearedand the N -best items from the list are put on theagenda.
Then the list is cleared and the parser moveson to the next step.
This process repeats until theagenda is empty (which means that no new itemshave been generated in the previous step), and thecandidate output is the final derivation.
Pseudocodefor the algorithm is shown in Figure 3.feature templates1 S0wp, S0c, S0pc, S0wc,S1wp, S1c, S1pc, S1wc,S2pc, S2wc,S3pc, S3wc,2 Q0wp, Q1wp, Q2wp, Q3wp,3 S0Lpc, S0Lwc, S0Rpc, S0Rwc,S0Upc, S0Uwc,S1Lpc, S1Lwc, S1Rpc, S1Rwc,S1Upc, S1Uwc,4 S0wcS1wc, S0cS1w, S0wS1c, S0cS1c,S0wcQ0wp, S0cQ0wp, S0wcQ0p, S0cQ0p,S1wcQ0wp, S1cQ0wp, S1wcQ0p, S1cQ0p,5 S0wcS1cQ0p, S0cS1wcQ0p, S0cS1cQ0wp,S0cS1cQ0p, S0pS1pQ0p,S0wcQ0pQ1p, S0cQ0wpQ1p, S0cQ0pQ1wp,S0cQ0pQ1p, S0pQ0pQ1p,S0wcS1cS2c, S0cS1wcS2c, S0cS1cS2wc,S0cS1cS2c, S0pS1pS2p,6 S0cS0HcS0Lc, S0cS0HcS0Rc,S1cS1HcS1Rc,S0cS0RcQ0p, S0cS0RcQ0w,S0cS0LcS1c, S0cS0LcS1w,S0cS1cS1Rc, S0wS1cS1Rc.Table 1: Feature templates.5 Model and TrainingWe use a global linear model to score candidateitems, trained discriminatively with the averagedperceptron (Collins, 2002).
Features for a (finishedor partial) candidate are extracted from each ac-tion that have been applied to build the candidate.Following Collins and Roark (2004), we apply the?early update?
strategy to perceptron training: at anystep during decoding, if neither the candidate out-put nor any item in the agenda is correct, decodingis stopped and the parameters are updated using thecurrent highest scored item in the agenda or the can-didate output, whichever has the higher score.Table 1 shows the feature templates used by theparser.
The symbols S0, S1, S2 and S3 in the ta-ble represent the top four nodes on the stack (if ex-istent), and Q0, Q1, Q2 and Q3 represent the frontfour words in the incoming queue (if existent).
S0Hand S1H represent the subnodes of S0 and S1 thathave the lexical head of S0 and S1, respectively.
S0Lrepresents the left subnode of S0, when the lexicalhead is from the right subnode.
S0R and S1R rep-resent the right subnode of S0 and S1, respectively,687when the lexical head is from the left subnode.
If S0is built by a UNARY action, S0U represents the onlysubnode of S0.
The symbols w, p and c represent theword, the POS, and the CCG category, respectively.These rich feature templates produce a large num-ber of features: 36 million after the first training it-eration, compared to around 0.5 million in the C&Cparser.6 ExperimentsOur experiments were performed using CCGBank(Hockenmaier and Steedman, 2007), which wassplit into three subsets for training (Sections 02?21),development testing (Section 00) and the final test(Section 23).
Extracted from the training data, theCCG grammar used by our parser consists of 3070binary rule instances and 191 unary rule instances.We compute F-scores over labeled CCG depen-dencies and also lexical category accuracy.
CCG de-pendencies are defined in terms of lexical categories,by numbering each argument slot in a complex cat-egory.
For example, the first NP in a transitive verbcategory is a CCG dependency relation, correspond-ing to the subject of the verb.
Clark and Curran(2007) gives a more precise definition.
We use thegenerate script from the C&C tools3 to transformderivations into CCG dependencies.There is a mismatch between the grammar thatgenerate uses, which is the same grammar as theC&C parser, and the grammar we extract from CCG-bank, which contains more rule instances.
Hencegenerate is unable to produce dependencies forsome of the derivations our shift-reduce parser pro-duces.
In order to allow generate to process allderivations from the shift-reduce parser, we repeat-edly removed rules that the generate script can-not handle from our grammar, until all derivationsin the development data could be dealt with.
Infact, this procedure potentially reduces the accuracyof the shift-reduce parser, but the effect is compar-atively small because only about 4% of the devel-opment and test sentences contain rules that are nothandled by the generate script.All experiments were performed using automati-3Available at http://svn.ask.it.usyd.edu.au/trac/candc/wiki; weused the generate and evaluate scripts, as well as theC&C parser, for evaluation and comparison.cally assigned POS-tags, with 10-fold cross valida-tion used to assign POS-tags and lexical categoriesto the training data.
At the supertagging stage, mul-tiple lexical categories are assigned to each word inthe input.
For each word, the supertagger assigns alllexical categories whose forward-backward proba-bility is above ?
?
max, where max is the highestlexical category probability for the word, and ?
is athreshold parameter.
To give the parser a reasonablefreedom in lexical category disambiguation, we useda small ?
value of 0.0001, which results in 3.6 lexi-cal categories being assigned to each word on aver-age in the training data.
For training, but not testing,we also added the correct lexical category to the listof lexical categories for a word in cases when it wasnot provided by the supertagger.Increasing the size of the beam in the parser beamsearch leads to higher accuracies but slower runningtime.
In our development experiments, the accu-racy improvement became small when the beam sizereached 16, and so we set the size of the beam to 16for the remainder of the experiments.6.1 Development test accuraciesTable 2 shows the labeled precision (lp), recall (lr),F-score (lf), sentence-level accuracy (lsent) and lex-ical category accuracy (cats) of our parser and theC&C parser on the development data.
We ran theC&C parser using the normal-form model (we re-produced the numbers reported in Clark and Cur-ran (2007)), and copied the results of the hybridmodel from Clark and Curran (2007), since the hy-brid model is not part of the public release.The accuracy of our parser is much better whenevaluated on all sentences, partly because C&Cfailed on 0.94% of the data due to the failure to pro-duce a spanning analysis.
Our shift-reduce parserdoes not suffer from this problem because it pro-duces fragmentary analyses for those cases.
Whenevaluated on only those sentences that C&C couldanalyze, our parser gave 0.29% higher F-score.
Ourshift-reduce parser also gave higher accuracies onlexical category assignment.
The sentence accuracyof our shift-reduce parser is also higher than C&C,which confirms that our shift-reduce parser producesreasonable sentence-level analyses, despite the pos-sibility for fragmentary analysis.688lp.
lr.
lf.
lsent.
cats.
evaluated onshift-reduce 87.15% 82.95% 85.00% 33.82% 92.77% all sentencesC&C (normal-form) 85.22% 82.52% 83.85% 31.63% 92.40% all sentencesshift-reduce 87.55% 83.63% 85.54% 34.14% 93.11% 99.06% (C&C coverage)C&C (hybrid) ?
?
85.25% ?
?
99.06% (C&C coverage)C&C (normal-form) 85.22% 84.29% 84.76% 31.93% 92.83% 99.06% (C&C coverage)Table 2: Accuracies on the development test data.606570758085900  5  10  15  20  25  30precision%dependency length (bins of 5)Precision comparison by dependency lengththis paperC&C5055606570758085900  5  10  15  20  25  30recall%dependency length (bins of 5)Recall comparison by dependency lengththis paperC&CFigure 4: P & R scores relative to dependency length.6.2 Error comparison with C&C parserOur shift-reduce parser and the chart-based C&Cparser offer two different solutions to the CCG pars-ing problem.
The comparison reported in this sec-tion is similar to the comparison between the chart-based MSTParser (McDonald et al, 2005) and shift-reduce MaltParser (Nivre et al, 2006) for depen-dency parsing.
We follow McDonald and Nivre(2007) and characterize the errors of the two parsersby sentence and dependency length and dependencytype.We measured precision, recall and F-score rel-ative to different sentence lengths.
Both parsersperformed better on shorter sentences, as expected.Our shift-reduce parser performed consistently bet-ter than C&C on all sentence lengths, and therewas no significant difference in the rate of perfor-mance degradation between the parsers as the sen-tence length increased.Figure 4 shows the comparison of labeled preci-sion and recall relative to the dependency length (i.e.the number of words between the head and depen-dent), in bins of size 5 (e.g.
the point at x=5 showsthe precision or recall for dependency lengths 1 ?
5).This experiment was performed using the normal-form version of the C&C parser, and the evaluationwas on the sentences for which C&C gave an anal-ysis.
The number of dependencies drops when thedependency length increases; there are 141, 180 and124 dependencies from the gold-standard, C&C out-put and our shift-reduce parser output, respectively,when the dependency length is between 21 and 25,inclusive.
The numbers drop to 47, 56 and 36 whenthe dependency length is between 26 and 30.
Therecall of our parser drops more quickly as the de-pendency length grows beyond 15.
A likely reasonis that the recovery of longer-range dependencies re-quires more processing steps, increasing the chanceof the correct structure being thrown off the beam.In contrast, the precision did not drop more quicklythan C&C, and in fact is consistently higher thanC&C across all dependency lengths, which reflectsthe fact that the long range dependencies our parsermanaged to recover are comparatively reliable.Table 3 shows the comparison of labeled precision(lp), recall (lr) and F-score (lf) for the most commonCCG dependency types.
The numbers for C&C arefor the hybrid model, copied from Clark and Curran(2007).
While our shift-reduce parser gave higherprecision for almost all categories, it gave higher re-call on only half of them, but higher F-scores for allbut one dependency type.6.3 Final resultsTable 4 shows the accuracies on the test data.
Thenumbers for the normal-form model are evaluatedby running the publicly available parser, while thosefor the hybrid dependency model are from Clarkand Curran (2007).
Evaluated on all sentences, theaccuracies of our parser are much higher than theC&C parser, since the C&C parser failed to produceany output for 10 sentences.
When evaluating both689category arg lp.
(o) lp.
(C) lr.
(o) lr.
(C) lf.
(o) lf.
(C) freq.N/N 1 95.77% 95.28% 95.79% 95.62% 95.78% 95.45% 7288NP/N 1 96.70% 96.57% 96.59% 96.03% 96.65% 96.30% 4101(NP\NP)/NP 2 83.19% 82.17% 89.24% 88.90% 86.11% 85.40% 2379(NP\NP)/NP 1 82.53% 81.58% 87.99% 85.74% 85.17% 83.61% 2174((S\NP)\(S\NP))/NP 3 77.60% 71.94% 71.58% 73.32% 74.47% 72.63% 1147((S\NP)\(S\NP))/NP 2 76.30% 70.92% 70.60% 71.93% 73.34% 71.42% 1058((S[dcl]\NP)/NP 2 85.60% 81.57% 84.30% 86.37% 84.95% 83.90% 917PP/NP 1 73.76% 75.06% 72.83% 70.09% 73.29% 72.49% 876((S[dcl]\NP)/NP 1 85.32% 81.62% 82.00% 85.55% 83.63% 83.54% 872((S\NP)\(S\NP)) 2 84.44% 86.85% 86.60% 86.73% 85.51% 86.79% 746Table 3: Accuracy comparison on the most common CCG dependency types.
(o) ?
our parser; (C) ?
C&C (hybrid)lp.
lr.
lf.
lsent.
cats.
evaluatedshift-reduce 87.43% 83.61% 85.48% 35.19% 93.12% all sentencesC&C (normal-form) 85.58% 82.85% 84.20% 32.90% 92.84% all sentencesshift-reduce 87.43% 83.71% 85.53% 35.34% 93.15% 99.58% (C&C coverage)C&C (hybrid) 86.17% 84.74% 85.45% 32.92% 92.98% 99.58% (C&C coverage)C&C (normal-form) 85.48% 84.60% 85.04% 33.08% 92.86% 99.58% (C&C coverage)F&P (Petrov I-5)* 86.29% 85.73% 86.01% ?
?
?
(F&P?
C&C coverage; 96.65% on dev.
test)C&C hybrid* 86.46% 85.11% 85.78% ?
?
?
(F&P?
C&C coverage; 96.65% on dev.
test)Table 4: Comparison with C&C; final test.
* ?
not directly comparable.parsers on the sentences for which C&C produces ananalysis, our parser still gave the highest accuracies.The shift-reduce parser gave higher precision, andlower recall, than C&C; it also gave higher sentence-level and lexical category accuracy.The last two rows in the table show the accuraciesof Fowler and Penn (2010) (F&P), who applied theCFG parser of Petrov and Klein (2007) to CCG, andthe corresponding accuracies for the C&C parser onthe same test sentences.
F&P can be treated as an-other chart-based parser; their evaluation is basedon the sentences for which both their parser andC&C produced dependencies (or more specificallythose sentences for which generate could pro-duce dependencies), and is not directly comparablewith ours, especially considering that their test set issmaller and potentially slightly easier.The final comparison is parser speed.
The shift-reduce parser is linear-time (in both sentence lengthand beam size), and can analyse over 10 sentencesper second on a 2GHz CPU, with a beam of 16,which compares very well with other constituencyparsers.
However, this is no faster than the chart-based C&C parser, although speed comparisonsare difficult because of implementation differences(C&C uses heavily engineered C++ with a focus onefficiency).7 Related WorkSagae and Lavie (2006a) describes a shift-reduceparser for the Penn Treebank parsing task whichuses best-first search to allow some ambiguity intothe parsing process.
Differences with our approachare that we use a beam, rather than best-first, search;we use a global model rather than local modelschained together; and finally, our results surpassthe best published results on the CCG parsing task,whereas Sagae and Lavie (2006a) matched the bestPTB results only by using a parser combination.Matsuzaki et al (2007) describes similar workto ours but using an automatically-extracted HPSG,rather than CCG, grammar.
They also use the gen-eralised perceptron to train a disambiguation model.One difference is that Matsuzaki et al (2007) use anapproximating CFG, in addition to the supertagger,to improve the efficiency of the parser.690Ninomiya et al (2009) (and Ninomiya et al(2010)) describe a greedy shift-reduce parser forHPSG, in which a single action is chosen at eachparsing step, allowing the possibility of highly ef-ficient parsing.
Since the HPSG grammar has rela-tively tight constraints, similar to CCG, the possibil-ity arises that a spanning analysis cannot be foundfor some sentences.
Our approach to this problemwas to allow the parser to return a fragmentary anal-ysis; Ninomiya et al (2009) adopt a different ap-proach based on default unification.Finally, our work is similar to the comparison ofthe chart-based MSTParser (McDonald et al, 2005)and shift-reduce MaltParser (Nivre et al, 2006) fordependency parsing.
MSTParser can perform ex-haustive search, given certain feature restrictions,because the complexity of the parsing task is lowerthan for constituent parsing.
C&C can perform ex-haustive search because the supertagger has alreadyreduced the search space.
We also found that ap-proximate heuristic search for shift-reduce parsing,utilising a rich feature space, can match the perfor-mance of the optimal chart-based parser, as well assimilar error profiles for the two CCG parsers com-pared to the two dependency parsers.8 ConclusionThis is the first work to present competitive resultsfor CCG using a transition-based parser, filling a gapin the CCG parsing literature.
Considered in termsof the wider parsing problem, we have shown thatstate-of-the-art parsing results can be obtained usinga global discriminative model, one of the few pa-pers to do so without using a generative baseline as afeature.
The comparison with C&C also allowed usto compare a shift-reduce parser based on heuristicbeam search utilising a rich feature set with an opti-mal chart-based parser whose features are restrictedby dynamic programming, with favourable resultsfor the shift-reduce parser.The complementary errors made by the chart-based and shift-reduce parsers opens the possibil-ity of effective parser combination, following sim-ilar work for dependency parsing.The parser code can be downloaded athttp://www.sourceforge.net/projects/zpar,version 0.5.AcknowledgementsWe thank the anonymous reviewers for their sugges-tions.
Yue Zhang and Stephen Clark are supportedby the European Union Seventh Framework Pro-gramme (FP7-ICT-2009-4) under grant agreementno.
247762.ReferencesA.
E. Ades and M. Steedman.
1982.
On the order ofwords.
Linguistics and Philosophy, pages 517 ?
558.Johan Bos, Stephen Clark, Mark Steedman, James R.Curran, and Julia Hockenmaier.
2004.
Wide-coveragesemantic representations from a CCG parser.
In Pro-ceedings of COLING-04, pages 1240?1246, Geneva,Switzerland.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics,33(4):493?552.Stephen Clark and James R. Curran.
2009.
Comparingthe accuracy of CCG and Penn Treebank parsers.
InProceedings of ACL-2009 (short papers), pages 53?56, Singapore.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceedingsof ACL, pages 111?118, Barcelona, Spain.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP, pages 1?8, Philadelphia, USA.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Feature-based, conditional randomfield parsing.
In Proceedings of the 46th Meeting ofthe ACL, pages 959?967, Columbus, Ohio.Timothy A. D. Fowler and Gerald Penn.
2010.
Ac-curate context-free parsing with Combinatory Catego-rial Grammar.
In Proceedings of ACL-2010, Uppsala,Sweden.H.
Hassan, K. Sima?an, and A.
Way.
2008.
A syntacticlanguage model based on incremental CCG parsing.In Proceedings of the Second IEEE Spoken LanguageTechnology Workshop, Goa, India.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Compu-tational Linguistics, 33(3):355?396.Julia Hockenmaier.
2003.
Data and Models for Statis-tical Parsing with Combinatory Categorial Grammar.Ph.D.
thesis, University of Edinburgh.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduce691parsing.
In Proceedings of the 2009 EMNLP Confer-ence, pages 1222?1231, Singapore.Richard Johansson and Pierre Nugues.
2007.
Incre-mental dependency parsing using online learning.
InProceedings of the CoNLL/EMNLP Conference, pages1134?1138, Prague, Czech Republic.Takuya Matsuzaki, Yusuke Miyao, and Jun ichi Tsu-jii.
2007.
Efficient HPSG parsing with supertaggingand CFG-filtering.
In Proceedings of IJCAI-07, pages1671?1676, Hyderabad, India.Ryan McDonald and Joakim Nivre.
2007.
Characteriz-ing the errors of data-driven dependency parsing mod-els.
In Proceedings of EMNLP/CoNLL, pages 122?131, Prague, Czech Republic.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd Meeting of theACL, pages 91?98, Michigan, Ann Arbor.Yusuke Miyao and Jun?ichi Tsujii.
2005.
Probabilisticdisambiguation models for wide-coverage HPSG pars-ing.
In Proceedings of the 43rd meeting of the ACL,pages 83?90, University of Michigan, Ann Arbor.Takashi Ninomiya, Takuya Matsuzaki, NobuyukiShimizu, and Hiroshi Nakagawa.
2009.
Deterministicshift-reduce parsing for unification-based grammarsby using default unification.
In Proceedings ofEACL-09, pages 603?611, Athens, Greece.Takashi Ninomiya, Takuya Matsuzaki, NobuyukiShimizu, and Hiroshi Nakagawa.
2010.
Deter-ministic shift-reduce parsing for unification-basedgrammars.
Journal of Natural Language Engineering,DOI:10.1017/S1351324910000240.J.
Nivre and M. Scholz.
2004.
Deterministic dependencyparsing of English text.
In Proceedings of COLING-04, pages 64?70, Geneva, Switzerland.Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,and Svetoslav Marinov.
2006.
Labeled pseudo-projective dependency parsing with support vector ma-chines.
In Proceedings of CoNLL, pages 221?225,New York, USA.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Proceedings ofHLT/NAACL, pages 404?411, Rochester, New York,April.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell III, and Mark John-son.
2002.
Parsing the Wall Street Journal using aLexical-Functional Grammar and discriminative esti-mation techniques.
In Proceedings of the 40th Meet-ing of the ACL, pages 271?278, Philadelphia, PA.Laura Rimell, Stephen Clark, and Mark Steedman.
2009.Unbounded dependency recovery for parser evalua-tion.
In Proceedings of EMNLP-09, pages 813?821,Singapore.Kenji Sagae and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proceed-ings of IWPT, pages 125?132, Vancouver, Canada.Kenji Sagae and Alon Lavie.
2006a.
A best-firstprobabilistic shift-reduce parser.
In Proceedings ofCOLING/ACL poster session, pages 691?698, Sydney,Australia, July.Kenji Sagae and Alon Lavie.
2006b.
Parser combinationby reparsing.
In Proceedings of HLT/NAACL, Com-panion Volume: Short Papers, pages 129?132, NewYork, USA.Mark Steedman.
2000.
The Syntactic Process.
The MITPress, Cambridge, Mass.David Weir.
1988.
Characterizing Mildly Context-Sensitive Grammar Formalisms.
Ph.D. thesis, Univer-sity of Pennsylviania.Michael White and Rajakrishnan Rajkumar.
2009.
Per-ceptron reranking for CCG realization.
In Proceedingsof the 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 410?419, Singapore.H Yamada and Y Matsumoto.
2003.
Statistical depen-dency analysis using support vector machines.
In Pro-ceedings of IWPT, Nancy, France.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-basedand transition-based dependency parsing using beam-search.
In Proceedings of EMNLP-08, Hawaii, USA.Yue Zhang and Stephen Clark.
2009.
Transition-basedparsing of the Chinese Treebank using a global dis-criminative model.
In Proceedings of IWPT, Paris,France, October.Yi Zhang, Valia Kordoni, and Erin Fitzgerald.
2007.
Par-tial parse selection for robust deep processing.
In Pro-ceedings of the ACL 2007 Workshop on Deep Linguis-tic Processing, Prague, Czech Republic.692
