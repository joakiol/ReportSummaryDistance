Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 69?74,Dublin, Ireland, August 23-24 2014.An analysis of textual inference in German customer emailsKathrin Eichler?, Aleksandra Gabryszak?, G?unter Neumann?
?German Research Center for Artificial Intelligence (DFKI), Berlin(kathrin.eichler|aleksandra.gabryszak@dfki.de)?German Research Center for Artificial Intelligence (DFKI), Saarbr?ucken(neumann@dfki.de)AbstractHuman language allows us to express thesame meaning in various ways.
Recogniz-ing that the meaning of one text can be in-ferred from the meaning of another can beof help in many natural language process-ing applications.
One such application isthe categorization of emails.
In this paper,we describe the analysis of a real-worlddataset of manually categorized customeremails written in the German language.We investigate the nature of textual infer-ence in this data, laying the ground for de-veloping an inference-based email catego-rization system.
This is the first analysisof this kind on German data.
We compareour results to previous analyses on Englishdata and present major differences.1 IntroductionA typical situation in customer support is thatmany customers send requests describing the sameissue.
Recognizing that two different customeremails refer to the same problem can help saveresources, but can turn out to be a difficult task.Customer requests are usually written in the formof unstructured natural language text, i.e., whenautomatically processing them, we are faced withthe issue of variability: Different speakers of a lan-guage express the same meanings using differentlinguistic forms.
There are, in fact, cases wheretwo sentences expressing the same meaning do notshare a single word:1.
?Bild und Ton sind asynchron.?
[Picture andsound are asynchronous.]2.
?Die Tonspur stimmt nicht mit dem Film?uberein.?
[The audio track does not match thevideo.
]Detecting the semantic equivalence of sentences1 and 2 requires several textual inference steps: Atthe lexical level, it requires mapping the word pic-ture to video and sound to audio track.
At thelevel of compositional semantics, it requires de-tecting the equivalence of the expressions A and Bare asynchronous and A does not match B.In this paper, we describe our analysis of a largeset of manually categorized customer emails, lay-ing the ground for developing an email catego-rization system based on textual inference.
Inour analysis, we compared each email text to thedescription of its associated category in order toinvestigate the nature of the inference steps in-volved.
In particular, our analysis aims to give an-swers to the following questions: What text repre-sentation is appropriate for the email categoriza-tion task?
What kind of inference steps are in-volved and how are they distributed in real-worlddata?
Answering these questions will not onlyhelp us decide, which existing tools and resourcesto integrate in an inference-based email catego-rization system, but also, which non-existing toolsmay be needed in addition.2 Related WorkThe task of email categorization has been ad-dressed by numerous people in the last decade.In the customer support domain, work to be men-tioned includes Eichler (2005), Wicke (2010), andEichler et al.
(2012).Approaching the task using textual inference re-lates to two tasks, for which active research is go-ing on: Semantic Textual Similarity, which mea-sures the degree of semantic equivalence (Agirreet al., 2012) of two texts, and Recognizing TextualEntailment (RTE), which is defined as recogniz-ing, given a hypothesis H and a text T, whether themeaning of H can be inferred from (is entailed in)T (Dagan et al., 2005).
The task of email catego-rization can be viewed as an RTE task, where T69refers to the email text and H refers to the cate-gory description.
The goal then is to find out if theemail text entails the category description, and ifso, assign it to the respective category.In connection with RTE, several groups haveanalyzed existing datasets in order to investigatethe nature of textual inference.
Bar-Haim (2010)introduces two levels of entailment, lexical andlexical-syntactic, and analyzes the contribution ofeach level and of individual inference mechanismswithin each level over a sample from the first RTEChallenge test set (Dagan et al., 2005).
He con-cludes that the main contributors are paraphrasesand syntactic transformations.Volokh and Neumann (2011) analyzed a subsetof the RTE-7 (Bentivogli et al., 2011) developmentdata to measure the complexity of the task.
Theydivide the T/H pairs into three different classes,depending on the type of knowledge required tosolve the problem: In class A, the relevant infor-mation is expressed with the same words in bothT and H. In class B, the words used in T aresynonyms to those used in H. In class C, recog-nizing entailment between H and T requires theuse of logical inference and/or world knowledge.They conclude that for two thirds of the data agood word-level analysis is enough, whereas theremainder of the data contains diverse phenomenacalling for a more sophisticated approach.A detailed analysis of the linguistic phenomenainvolved in semantic inferences in the T-H pairs ofthe RTE-5 dataset was presented by (Cabrio andMagnini, 2013).As the approaches described above, our anal-ysis aims at measuring the contribution of infer-ence mechanisms at different representation lev-els.
However, we focus on a different type of text(customer request as compared to news) and a dif-ferent language (German as compared to English).We thus expect our results to differ from the onesobtained in previous work.3 Setup3.1 DatasetWe analyzed a dataset consisting of a set of emailsand a set of categories associated to these emails.The emails contain customer requests sent to thesupport center of a multimedia software company,and mainly concern the products offered by thiscompany.
Each email was manually assigned toone or more matching categories by a customersupport agent (a domain expert).
These categories,predefined by the data provider, represent previ-ously identified problems reported by customers.All emails and category descriptions are written inGerman.
As is common for this type of data, manyemails contain spelling mistakes, grammatical er-rors or abbreviations, which make automatic textprocessing difficult.
An anonymized1version ofthe dataset is available online2.
Our data analysiswas done on the original dataset.
The data exam-ples we use in the following, however, are takenfrom the anonymized dataset.In our analysis, we manually compared theemail texts to the descriptions of their associatedcategories in order to investigate the nature of theinference steps involved.
In order to reduce thecomplexity of the task, we based our analysis onthe subset of categories, for which the categorytext described a single problem (a single H, speak-ing in RTE terms).
We also removed emails forwhich we were not able to relate the category de-scription to the email text.
However, we keptemails associated to several categories and ana-lyzed all of the assignments.
The reduced datasetwe used for our analysis consists of 369 emails as-sociated to 25 categories.
The email lengths varybetween 2 and 1246 tokens.
Category descriptionsusually consist of a single sentence or a phrase.3.2 Task definitionThe task of automatically assigning emails tomatching categories can be viewed as an RTE task,where T refers to the email text and H refers to thecategory description.
The goal then is to find out ifthe email text entails the category description, andif so, assign it to the respective category.For the analysis of inference steps involved, wedistinguish between two levels of inference: lexi-cal semantics and compositional semantics.
At thelexical level, we distinguish two different types oftext representation: First, the bag-of-tokens repre-sentation, where both the email text and the cate-gory description are represented as the set of con-tent word tokens contained in the respective text.1The anonymization step was performed to eliminate ref-erences to the data provider and anonymize personal dataabout the customers.
During this step, the data was trans-ferred into a different product domain (online auction sales).However, the anonymized version is very similar to the orig-inal one in terms of language style (including spelling errors,anglicisms, abbreviations, and special characters).2http://www.excitement-project.eu/attachments/article/97/omq_public_email_data.zip70Second, the bag-of-terms representation, where a?term?
can consist of one or more content tokensoccurring consecutively.
At this level, followingBar Haim (2010), we assume that entailment holdsbetween T (the email) and H (the category descrip-tion) if every token (term) in H can be matched bya corresponding entailing token (term) in T.At the level of compositional semantics, we rep-resent each text as the set of complex expressions(combinations of terms linked syntactically andsemantically) contained in it.
At this level, weassume that entailment holds between T and H ifevery term in H is part of at least one complex ex-pression that can be matched by a correspondingentailing expression in T.The data analysis was carried out by two peopleseparately (one of them an author of this paper),who analyzed each assignment of an email E toa category C based on predefined analysis guide-lines.
For each of the text representation types de-scribed above, the task of the annotators was tofind, for each expression in the description of C, asemantically equivalent or entailing expression inE.3If such an expression was found, all involvedinference steps were to be noted down in an anno-tation table.
The predefined list of possible infer-ence steps is explained in detail in the following.4 Inference steps4.1 Lexical semantics levelFor each of the three different types of represen-tation (token, term, complex expression), we dis-tinguish various inference steps.
At the lexicallevel, we distinguish among spelling, inflection,derivation, composition, lexical semantics at thetoken level and lexical semantics at the term level.This distinction was made based on the assump-tion that for each of these steps a different NLPtool or resource is required (e.g., a lemmatizer forinflection, a compound splitter for composition,a lexical-semantic net for lexical semantics).
Wealso distinguish between token and term level lexi-cal semantics, as, for term-level lexical semantics,we assume that a tool for detecting multi-tokenterms would be required.3A preanalysis of the data revealed that in some cases,the entailment direction seemed to be flipped: Expressionsin the category description entailed expressions in the emailtext, e.g.
?Video?
(video) ?
?Film?
(film).
In our analysis,we counted these as positive cases if the context suggestedthat both expressions were used to express the same idea.
Weconsider this an interesting issue to be further investigated.4.2 Compositional semantics levelAt the level of compositional semantics, we con-sider inference steps involving complex expres-sions.4These steps go beyond the lexical leveland would require the usage of at least a syntac-tic parser for detecting word dependencies and atool for recognizing entailment between two com-plex expressions.
At this level, we also record thefrequency of three particular phenomena: parti-cle verbs, negation, and light verb constructions,which we considered worth addressing separately.Particle verbs are important when processingGerman because, unlike in English, they can oc-cur both as one token or two, dependending on thesyntactic construction, in which they are embed-ded (e.g., ?aufnehmen?
and ?nehme [...] auf?
[(to)record].
Recognizing the scope of negation can berequired in cases where negation is expressed im-plicitly in one of the sentences, e.g., ?A und B sindnicht synchron?
[A and B are not synchronous]vs. ?Es kommt zu Versetzung zwischen A undB?
[There is a misaligment between A and B].
Bylight verbs we refer to verbs with little semanticcontent of their own, forming a linguistic unit witha noun or prepositional phrase, for which a singleverb with a similar meaning exists, e.g., ?Meldungkommt?
[message appears] vs. ?melden?
[notify].For example, for the text pair ?Das Brennenbricht ab mit der Meldung X?
[Burning breakswith message X] and ?Beim Brennen kommt dieFehlermeldung X?
[When burning, error messageX appears], the word ?Meldung?
[message] wasrecorded as inference at the token level becauseit can be derived from ?Fehlermeldung?
[errormessage] using decomposition.
The verb ?brichtab?
[break] was considered inference at the levelof compositional semantics because there is nolexical-semantic relation to the verb ?kommt?
[ap-pears].
The verb can thus only be matched by con-sidering the complete expression.4.3 Possible effects on precisionThe focus of the analysis described so far wason ways to improve recall in an email catego-rization system: We count the inference steps re-quired to increase the amount of mappable infor-mation (similar to query expansion in informa-tion retrieval).
However, the figures do not showthe impact of these mappings on precision, i.e.,4Additional lexical inference steps required at this levelare not recorded.71whether an inference step we take would nega-tively affect the precision of the system.
Takinga more precision-oriented view at the problem, wealso counted the number of cases for which a morecomplex representation could be ?helpful?
(albeitnot necessary).
For example, inferring the negatedexpression ?Programm kann die DVD nicht ab-spielen?
[Program cannot play the DVD] from?Programm kann die DVD nicht laden??
[Programdoes not load the DVD] is possible at the lexicallevel, assuming that ?abspielen?
[(to) play] entails?laden?
[(to) load].
However, knowing that bothverbal expressions are negated is expected to bebeneficial to precision, in order to avoid wronglyinferring a negated from a non-negated expression.5 Results5.1 Interannotator agreementOur analysis was done by two people separately,which allowed us to measure the reliability of theannotation for the different inference steps.
Thekappa coefficient (Cohen, 1960) for spelling, in-flection, derivation and composition ranged be-tween 0.46 and 0.67, i.e., moderate to substan-tial agreement according to the scale proposed byLandis and Koch (1977).
For lexical semantics,the value is only fair (0.38).
An analysis showedthat the identification of a lexical semantic rela-tion is often not straightforward, and may requirea good knowledge of the domain.
For example,the verbs ?aufrufen?
[call] and ?importieren?
[im-port], which would usually not be considered tobe semantically related, may in fact be used to de-scribe the same action in the computer domain, re-ferring to files.
Also for the more complex infer-ence steps, we measured only fair agreement, dueto the number of positive and negative cases beingvery skewed.
For the ?helpful?
cases, the valuesranged between 0.73 and 0.79 (substantial agree-ment).5.2 Distribution of inference stepsTable 1 summarizes the distribution of inferencesteps identified in our data for each text represen-tation type, ordered by their frequency of occur-rence.5For multi-token terms, particle verbs, andnegation, the number of ?helpful?
cases is given inbrackets.Our results show that the most important infer-ence step at the lexical level is lexical semantics.5Based on the steps agreed on after a consolidation phase.At the lexical level, we found 157 different wordmappings.
Only 26 of them correspond to a re-lation in GermaNet (Hamp and Feldweg, 1997),version 7.0.
48 of the involved words had no Ger-maNet entry at all, due to the word being an an-glicism (e.g., ?Error?
instead of ?Fehler?
), a non-lexicalized compound (e.g., ?Bildschirmbereich?
[screen area]) or a highly domain- or application-specific word (for only 37.5% of the words miss-ing in GermaNet, we found an entry in Wikipedia).In 72 cases, both words had a GermaNet entry,but no relation existed, usually because the rela-tion was too domain-specific.For more than 30% of the words (as comparedto 10.1% in Bar-Haim?s (2010) analysis on En-glish), a morphological transformation is required,which can be explained by the high complexity ofGerman morphology as compared to the morphol-ogy of English.
Spelling mistakes or differences,which are not considered in other analyses, arealso found in a considerable number of words, thereason being that customer emails are less well-formed than, for example, news texts.The significance of multi-token terms was sur-prisingly high for German, where word combina-tions are usually expressed in the form of com-pounds (i.e., a single token).
In our data, multi-token terms were usually compounds consistingof at least one anglicism (e.g., ?USB Anschluss?
[USB port]).
This suggests that texts written ina domain language with a high proportion of En-glish loan words may be more difficult to processthan general language texts, as multi-token termshave to be recognized.At the level of compositional semantics, itshould be noted that, in many cases, recogniz-ing the entailment relation between two expres-sions requires world or domain knowledge.
Sev-eral of the mappings involved particle verbs orlight verbs.
Detecting negation scope is expectedto be important in a precision-oriented system.5.3 Comparing text representationsWe also had a look at the amount of informationleft unmapped at each level.
For the lexical level,we determined for how many of the content tokens(terms) occurring in the category descriptions, nomatching expression was found in the associatedemails.
For the level of compositional semantics,we looked at each term left unmapped at the lexi-cal level and tried to map a complex expression in72Type of inference Data example Total (Share)Lexical semantics(Token)?Anfang?
[start]?
?Beginn?
[beginning] 310 (20.2%)Inflection ?startet?
[starts]?
?starten?
[start] 206 (13.4%)Derivation ?Import?
[import]?
?importieren?
[(to) import] 164 (10.7%)Composition ?Fehlermeldung?
[error message]?
?Meldung?
[message] 158 (10.3%)Spelling ?Dateine??
?Dateien?
[files] 47 (3.1%)Lexical semantics(Term)?MPEG Datei?
[MPEG file]?
?Video?
[video]60 (4.1%)[+124 (8.6%)]Particle verbs ?spielt [...] ab?
[play]?
?abspielen?
[play]26 (1.8%)[+34 (2.4%)]Light verbs ?Meldung kommt?
[message appears]?
?melden?
[notify] 17 (1.2%)Negation?Brennerger?at kann nicht gefunden werden?
[Burning device cannot be found]?
?Es wird kein Brenner gefunden?
[No burner is found ]8 (0.6%)[+121 (8.4%)]Other complexexpressions?Das Brennen bricht ab mit der Meldung X?
[Burning breaks with message X]?
?Beim Brennen kommt die Fehlermeldung X?
[Burning yields error message X ]83 (5.7%)Table 1: Distribution of inference steps in the dataset.which the term occurred.
If for none of these ex-pressions a matching expression was found in theemail, the term was counted as non-mappable atthis level.Representation Non-mappable ShareTokens 428 / 1538 27.8%Terms 365 /1446 25.2%Complex expressions 229 / 1446 15.8%The above table shows that the majority ofthe required inference relates to the lexical level.Choosing a representation that allows us to mapmore complex expressions, increases the amountof mappable terms by almost 10%.
However, evenwith this more complex representation, a consider-able amount of terms (15.8%) cannot be mappedat all because the email text does not contain allinformation specified in the category description.6 ConclusionsIn our analysis, we examined the inference stepsrequired to determine that the text of a category de-scription can be inferred from the text of a particu-lar email associated to this category.
We identifiedmajor inference phenomena and determined theirdistribution in a German real-world dataset.
Ouranalysis supports previous results for English datain that a large portion of the required inference re-lates to the lexical level.
Choosing a representa-tion that allows us to map more complex expres-sions significantly increases the amount of map-pable expressions, but some expressions simplycannot be mapped because the categorization wasdone relying on partial information in the email.Our results extend previous results by investi-gating inference steps specific to the German lan-guage (such as morphology, composition, and par-ticle verbs).
Some outcomes are unexpected forthe German language, such as the high share ofmulti-token terms.
Our analysis also stresses theimportance of inference steps relying on domain-specific resources, i.e., for this type of data, thedevelopment of tools and resources to support in-ference in highly specialized domains is crucial.We are currently using the results of our anal-ysis to build an email categorization system thatintegrates linguistic resources and tools to expandthe linguistic expressions in an incoming emailwith entailed expressions.
This will allow us tomeasure the performance of such a system, in par-ticular with respect to the effect on precision.AcknowledgementsThis work was partially supported by the EX-CITEMENT project (EU grant FP7 ICT-287923)and the German Federal Ministry of Education andResearch (Software Campus grant 01?S12050 ).We would like to thank OMQ GmbH for provid-ing the dataset, Britta Zeller and Jonas Placzek forthe data anonymization, and Stefania Racioppa forher help in the annotation phase.This work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/73ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
SemEval-2012 Task 6:A Pilot on Semantic Textual Similarity.
In *SEM2012: The First Joint Conference on Lexical andComputational Semantics (SemEval 2012), pages385?393, Montr?eal, Canada, 7-8 June.
Associationfor Computational Linguistics.Roy Bar-Haim.
2010.
Semantic Inference at theLexical-Syntactic Level.
Ph.D. thesis, Departmentof Computer Science, Bar Ilan University, RamatGan, Israel.Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa T. Dang,and Danilo Giampiccolo.
2011.
The Seventh PAS-CAL Recognizing Textual Entailment Challenge.
InProceedings of TAC.Elena Cabrio and Bernardo Magnini.
2013.
Decom-posing Semantic Inferences.
Linguistics Issues inLanguage Technology - LiLT.
Special Issues on theSemantics of Entailment, 9(1), August.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20(1):37.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The PASCAL Recognising Textual Entail-ment Challenge.
In Proceedings of the PASCALChallenges Workshop on Recognising Textual En-tailment.Kathrin Eichler, Matthias Meisdrock, and SvenSchmeier.
2012.
Search and Topic Detection inCustomer Requests - Optimizing a Customer Sup-port System.
KI, 26(4):419?422.Kathrin Eichler.
2005.
Automatic classification ofSwedish email messages.
Bachelor thesis, Eberhard-Karls-Universit?at, T?ubingen, Germany.Birgit Hamp and Helmut Feldweg.
1997.
GermaNet -a Lexical-Semantic Net for German.
In In Proceed-ings of ACL workshop Automatic Information Ex-traction and Building of Lexical Semantic Resourcesfor NLP Applications, pages 9?15.J.
R. Landis and G. G. Koch.
1977.
The Measurementof Observer Agreement for Categorical Data.
Bio-metrics, 33(1):159?174, March.Alexander Volokh and G?unter Neumann.
2011.
UsingMT-Based Metrics for RTE.
In Proceedings of the4th Text Analysis Conference (TAC 2011), Gaithers-burg, Maryland, USA, November.
National Instituteof Standards and Technology.Janine Wicke.
2010.
Automated Email Classificationusing Semantic Relationships.
Master thesis, KTHRoyal Institute of Technology, Stockholm, Sweden.74
