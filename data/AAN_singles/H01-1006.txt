Answering What-Is Questions by Virtual AnnotationJohn PragerIBM T.J. Watson Research CenterYorktown Heights, N.Y. 10598(914) 784-6809jprager@us.ibm.comDragomir RadevUniversity of MichiganAnn Arbor, MI 48109(734) 615-5225radev@umich.eduKrzysztof CzubaCarnegie-Mellon UniversityPittsburgh, PA 15213(412) 268 6521kczuba@cs.cmu.eduABSTRACTWe present the technique of Virtual Annotation as a specializationof Predictive Annotation for answering definitional What isquestions.
These questions generally have the property that thetype of the answer is not given away by the question, which posesproblems for a system which has to select answer strings fromsuggested passages.
Virtual Annotation uses a combination ofknowledge-based techniques using an ontology, and statisticaltechniques using a large corpus to achieve high precision.KeywordsQuestion-Answering, Information Retrieval, Ontologies1.
INTRODUCTIONQuestion Answering is gaining increased attention in both thecommercial and academic arenas.
While algorithms for generalquestion answering have already been proposed, we find that suchalgorithms fail to capture certain subtleties of particular types ofquestions.
We propose an approach in which different types ofquestions are processed using different algorithms.
We introduce atechnique named Virtual Annotation (VA) for answering one suchtype of question, namely the What is question.We have previously presented the technique of PredictiveAnnotation (PA) [Prager, 2000], which has proven to be aneffective approach to the problem of Question Answering.
Theessence of PA is to index the semantic types of all entities in thecorpus, identify the desired answer type from the question, searchfor passages that contain entities with the desired answer type aswell as the other query terms, and to extract the answer term orphrase.
One of the weaknesses of PA, though, has been in dealingwith questions for which the system cannot determine the correctanswer type required.
We introduce here an extension to PAwhich we call Virtual Annotation and show it to be effective forthose ?What is/are (a/an) X?
questions that are seeking hypernymsof X.
These are a type of definition question, which other QAsystems attempt to answer by searching in the document collectionfor textual clues similar to those proposed by [Hearst, 1998], thatare characteristic of definitions.
Such an approach does not usethe strengths of PA and is not successful in the cases in which adeeper understanding of the text is needed in order to identifythe defining term in question.We first give a brief description of PA.  We look at a certainclass of What is questions and describe our basic algorithm.Using this algorithm we develop the Virtual Annotationtechnique, and evaluate its performance with respect to both thestandard TREC and our own benchmark.
We demonstrate ontwo question sets that the precision improves from .15 and .33 to.78 and .83 with the addition of VA.2.
BACKGROUNDFor our purposes, a question-answering (QA) system is onewhich takes a well-formed user question and returns anappropriate answer phrase found in a body of text.
Thisgenerally excludes How and Why questions from consideration,except in the relatively rare cases when they can be answered bysimple phrases, such as ?by fermenting grapes?
or ?because ofthe scattering of light?.
In general, the response of a QA systemwill be a named entity such as a person, place, time, numericalmeasure or a noun phrase, optionally within the context of asentence or short paragraph.The core of most QA systems participating in TREC [TREC8,2000 & TREC9, 2001] is the identification of the answer typedesired by analyzing the question.
For example, Who questionsseek people or organizations, Where questions seek places,When questions seek times, and so on.
The goal, then, is to findan entity of the right type in the text corpus in a context thatjustifies it as the answer to the question.
To achieve this goal,we have been using the technique of PA to annotate the textcorpus with semantic categories (QA-Tokens) prior to indexing.Each QA-Token is identified by a set of terms, patterns, orfinite-state machines defining matching text sequences.
Thus?Shakespeare?
is annotated with ?PERSON$?, and the textstring ?PERSON$?
is indexed at the same text location as?Shakespeare?.
Similarly, ?$123.45?
is annotated with?MONEY$?.
When a question is processed, the desired QA-Token is identified and it replaces the Wh-words and theirauxiliaries.
Thus, ?Who?
is replaced by ?PERSON$?, and?How much?
+ ?cost?
are replaced by ?MONEY$?.
Theresulting query is then input to the search engine as a bag ofwords.
The expectation here is that if the initial question were?Who wrote Hamlet?, for example, then the modified query of?PERSON$ write Hamlet?
(after lemmatization) would be aperfect match to text that states ?Shakespeare wrote Hamlet?
or?Hamlet was written by Shakespeare?.The modified query is matched by the search engine againstpassages of 1-2 sentences, rather than documents.
The top 10passages returned are processed by our Answer Selection modulewhich re-annotates the text, identifies all potential answer phrases,ranks them using a learned evaluation function and selects the top5 answers (see [Radev et al, 2000]).The problem with ?What is/are (a/an) X?
questions is that thequestion usually does not betray the desired answer type.
All thesystem can deduce is that it must find a noun phrase (the QA-Token THING$).
The trouble with THING$ is that it is toogeneral and labels a large percentage of the nouns in the corpus,and so does not help much in narrowing down the possibilities.
Asecond problem is that for many such questions the desired answertype is not one of the approximately 50 high-level classes (i.e.
QA-Tokens) that we can anticipate at indexing; this phenomenon isseen in TREC9, whose 24 definitional What is questions are listedin the Appendix.
These all appear to be calling out for ahypernym.
To handle such questions we developed the techniqueof Virtual Annotation which is like PA and shares much of thesame machinery, but does not rely on the appropriate class beingknown at indexing time.
We will illustrate with examples from theanimal kingdom, including a few from TREC9.3.
VIRTUAL ANNOTATIONIf we look up a word in a thesaurus such as WordNet [Miller et al,1993]), we can discover its hypernym tree, but there is noindication which hypernym is the most appropriate to answer aWhat is question.
For example, the hypernym hierarchy for?nematode?
is shown in Table 1.
The level numbering countslevels up from the starting term.
The numbers in parentheses willbe explained later.Table 1.
Parentage of ?nematode?
according to WordNet.Level Synset0 {nematode, roundworm}1 {worm(13)}2 {invertebrate}3 {animal(2), animate being, beast, brute, creature,fauna}4 {life form(2), organism(3), being, living thing}5 {entity, something}At first sight, the desirability of the hypernyms seems to decreasewith increasing level number.
However, if we examine ?meerkat?we find the hierarchy in Table 2.We are leaving much unsaid here about the context of the questionand what is known of the questioner, but it is not unreasonable toassert that the ?best?
answer to ?What is a meerkat?
is either ?amammal?
(level 4) or ?an animal?
(level 7).
How do we get anautomatic system to pick the right candidate?Table 2.
Parentage of ?meerkat?
according to WordNetLevel Synset0 {meerkat, mierkat}1 {viverrine, viverrine mammal}2 {carnivore}3 {placental, placental mammal, eutherian, eutherianmammal}4 {mammal}5 {vertebrate, craniate}6 {chordate}7 {animal(2), animate being, beast, brute, creature,fauna}8 {life form, organism, being, living thing}9 {entity, something}It seems very much that what we would choose intuitively as thebest answer corresponds to Rosch et al?s basic categories[Rosch et al, 1976].
According to psychological testing, theseare categorization levels of intermediate specificity that peopletend to use in unconstrained settings.
If that is indeed true, thenwe can use online text as a source of evidence for this tendency.For example, we might find sentences such as ??
meerkats andother Y ?
?, where Y is one of its hypernyms, indicating that Yis in some sense the preferred descriptor.We count the co-occurrences of the target search term (e.g.?meerkat?
or ?nematode?)
with each of its hypernyms (e.g.?animal?)
in 2-sentence passages, in the TREC9 corpus.
Thesecounts are the parenthetical numbers in Tables 1 and 2.
Theabsence of a numerical label there indicates zero co-occurrences.Intuitively, the larger the count, the better the correspondingterm is as a descriptor.3.1  Hypernym Scoring and SelectionSince our ultimate goal is to find passages describing the targetterm, discovering zero co-occurrences allows elimination ofuseless candidates.
Of those remaining, we are drawn to thosewith the highest counts, but we would like to bias our systemaway from the higher levels.
Calling a nematode a life-form iscorrect, but hardly helpful.The top levels of WordNet (or any ontology) are by definitionvery general, and therefore are unlikely to be of much use forpurposes of definition.
However, if none of the immediateparents of a term we are looking up co-occur in our text corpus,we clearly will be forced to use a more general term that does.We want to go further, though, in those cases where theimmediate parents do occur, but in small numbers, and the verygeneral parents occur with such high frequencies that ouralgorithm would select them.
In those cases we introduce atentative level ceiling to prevent higher-level terms from beingchosen if there are suitable lower-level alternatives.We would like to use a weighting function that decreasesmonotonically with level distance.
Mihalcea and  Moldovan[1999], in an analogous context, use the logarithm of the numberof terms in a given term?s subtree to calculate weights, and theyclaim to have shown that this function is optimal.
Since it isapproximately true that the level population increasesexponentially in an ontology, this suggests that a linear function oflevel number will perform just as well.Our first step is to generate a level-adapted count (LAC) bydividing the co-occurrence counts by the level number (we areonly interested in levels 1 and greater).
We then select the besthypernym(s) by using a fuzzy maximum calculation.
We locatethe one or more hypernyms with greatest LAC, and then also selectany others with a LAC within a predefined threshold of it; in ourexperimentation we have found that a threshold value of 20%works well.
Thus if, for example, a term has one hypernym atlevel 1 with a count of 30, and another at level 2 with a count of50, and all other entries have much smaller counts, then since theLAC 25 is within 20% of the LAC 30, both of these hypernymswill be proposed.To prevent the highest levels from being selected if there is anyalternative, we tentatively exclude them from considerationaccording to the following scheme:If the top of the tree is at level N, where N <= 3, we set a tentativeceiling at N-1, otherwise if N<=5, we set the ceiling at N-2,otherwise we set the ceiling at N-3.
If no co-occurrences are foundat or below this ceiling, then it is raised until a positive value isfound, and the corresponding term is selected.If no hypernym at all co-occurs with the target term, then thisapproach is abandoned:  the ?What?
in the question is replaced by?THING$?
and normal procedures of Predictive Annotation arefollowed.When successful, the algorithm described above discovers one ormore candidate hypernyms that are known to co-occur with thetarget term.
There is a question, though, of what to do when thequestion term has more than one sense, and hence more than oneancestral line in WordNet.
We face a choice of either selecting thehypernym(s) with the highest overall score as calculated by thealgorithm described above, or collecting together the besthypernyms in each parental branch.
After some experimentationwe made the latter choice.
One of the questions that benefittedfrom this was ?What is sake?.
WordNet has three senses for sake:good (in the sense of welfare), wine (the Japanese drink) andaim/end, with computed scores of 122, 29 and 87/99 respectively.It seems likely (from the phrasing of the question) that the ?wine?sense is the desired one, but this would be missed entirely if onlythe top-scoring hypernyms were chosen.We now describe how we arrange for our Predictive Annotationsystem to find these answers.
We do this by using thesedescriptors as virtual QA-Tokens; they are not part of the searchengine index, but are tagged in the passages that the search enginereturns at run time.3.2 IntegrationLet us use H to represent either the single hypernym or adisjunction of the several hypernyms found through the WordNetanalysis.
The original question Q =?What is (a/an) X?is converted to Q?
=?DEFINE$ X H?where DEFINE$ is a virtual QA-Token that was never seen atindexing time, does not annotate any text and does not occur in theindex.
The processed query Q?
then will find passages thatcontain occurrences of both X and H; the token DEFINE$ willbe ignored by the search engine.
The top passages returned bythe search engine are then passed to Answer Selection, which re-annotates the text.
However, this time the virtual QA-TokenDEFINE$ is introduced and the patterns it matches are definedto be the disjuncts in H.  In this way, all occurrences of theproposed hypernyms of X in the search engine passages arefound, and are scored and ranked in the regular fashion.
Theend result is that the top passages contain the target term and oneof its most frequently co-occurring hypernyms in closeproximity, and these hypernyms are selected as answers.When we use this technique of Virtual Annotation on theaforementioned questions, we get answer passages such as?Such genes have been found in nematode wormsbut not yet in higher animals.
?and?South African golfer Butch Kruger had a goodround going in the central Orange Free State trials,until a mongoose-like animal grabbed his ball withits mouth and dropped down its hole.
Kruger wroteon his card: "Meerkat.
"?4 RESULTS4.1 EvaluationWe evaluated Virtual Annotation on two sets of questions ?
thedefinitional questions from TREC9 and similar kinds ofquestions from the Excite query log (seehttp://www.excite.com).
In both cases we were looking fordefinitional text in the TREC corpus.
The TREC questions hadbeen previously verified (by NIST) to have answers there; theExcite questions had no such guarantee.
We started with 174Excite questions of the form ?What is X?, where X was a 1- or2-word phrase.
We removed those questions that we felt wouldnot have been acceptable as TREC9 questions.
These werequestions where:o The query terms did not appear in the TREC corpus,and some may not even have been real words (e.g.
?What is a gigapop?
).1  37 questions.o The query terms were in the corpus, but there was nodefinition present (e.g ?What is a computermonitor?
).2  18 questions.o The question was not asking about the class of theterm but how to distinguish it from other members ofthe class (e.g.
?What is a star fruit?).
17 questions.o The question was about computer technology thatemerged after the articles in the TREC corpus werewritten (e.g.
?What is a pci slot?).
19 questions.o The question was very likely seeking an example, nota definition (e.g.
?What is a powerful adhesive?).
1question plus maybe some others ?
see the Discussion1 That is, after automatic spelling correction was attempted.2 The TREC10 evaluation in August 2001 is expected to containquestions for which there is no answer in the corpus(deliberately).
While it is important for a system to be able tomake this distinction, we kept within the TREC9 framework forthis evaluation.section later.
How to automatically distinguish thesecases is a matter for further research.Of the remaining 82 Excite questions, 13 did not have entries inWordNet.
We did not disqualify those questions.For both the TREC and Excite question sets we report twoevaluation measures.
In the TREC QA track, 5 answers aresubmitted per question, and the score for the question is thereciprocal of the rank of the first correct answer in these 5candidates, or 0 if the correct answer is not present at all.
Asubmission?s overall score is the mean reciprocal rank (MRR) overall questions.
We calculate MRR as well as mean binary score(MBS) over the top 5 candidates; the binary score for a question is1 if a correct answer was present in the top 5 candidates, 0otherwise.
The first sets of MBS and MRR figures are for our basesystem, the second set the system with VA.Table 3.
Comparison of base system and system with VA onboth TREC9 and Excite definitional questions.Source No.
ofQuestionsMBSw/oVAMRRw/oVAMBSwithVAMRRwithVATREC9(in WN)20 .3 .2 .9 .9TREC9(not in WN)4 .5 .375 .5 .5TREC9Overall24 .333 .229 .833 .833Excite(in WN)69 .101 .085 .855.824Excite(not in WN)13 .384 .295 .384 .295ExciteOverall82 .146 .118 .780 .740We see that for the 24 TREC9 definitional questions, our MRRscore with VA was the same as the MBS score.
This was becausefor each of the 20 questions where the system found a correctanswer, it was in the top position.By comparison, our base system achieved an overall MRR score of.315 across the 693 questions of TREC9.
Thus we see that withVA, the average score of definitional questions improves frombelow our TREC average to considerably higher.
While thepercentage of definitional questions in TREC9 was quite small, weshall explain in a later section how we plan to extend ourtechniques to other question types.4.2  ErrorsThe VA process is not flawless, for a variety of reasons.
One isthat the hierarchy in WordNet does not always exactly correspondto the way people classify the world.
For example, in WordNet adog is not a pet, so ?pet?
will never even be a candidate answer to?What is a dog?.When the question term is in WordNet, VA succeeds most of thetime.
One of the error sources is due to the lack of uniformity ofthe semantic distance between levels.
For example, the parentsof ?architect?
are ?creator?
and ?human?, the latter being whatour system answers to ?What is an architect?.
This istechnically correct, but not very useful.Another error source is polysemy.
This does not seem to causeproblems with VA very often ?
indeed the co-occurrencecalculations that we perform are similar to those done by[Mihalcea and Moldovan, 1999] to perform word sensedisambiguation ?
but it can give rise to amusing results.
Forexample, when asked ?What is an ass?
the system respondedwith ?Congress?.
Ass has four senses, the last of which inWordNet is a slang term for sex.
The parent synset contains thearchaic synonym congress (uncapitalized!).
In the TREC corpusthere are several passages containing the words ass andCongress, which lead to congress being the hypernym with thegreatest score.
Clearly this particular problem can be avoidedby using orthography to indicate word-sense, but the generalproblem remains.5  DISCUSSION AND FURTHER WORK5.1  DiscussionWhile we chose not to use Hearst?s approach of key-phraseidentification as the primary mechanism for answering What isquestions, we don?t reject the utility of the approach.
Indeed, acombination of VA as described here with a key-phrase analysisto further filter candidate answer passages might well reduce theincidence of errors such as the one with ass mentioned in theprevious section.
Such an investigation remains to be done.We have seen that VA gives very high performance scores atanswering What is questions ?
and we suggest it can beextended to other types ?
but we have not fully addressed theissue of automatically selecting the questions to which to applyit.
We have used the heuristic of only looking at questions ofthe form ?What is (a/an) X?
where X is a phrase of one or twowords.
By inspection of the Excite questions, almost all of thosethat pass this test are looking for definitions, but some - such as?What is a powerful adhesive?
- very probably do not.
Thereare also a few questions that are inherently ambiguous(understanding that the questioners are not all perfectgrammarians):  is ?What is an antacid?
asking for a definition ora brand name?
Even if it is known or assumed that a definitionis required, there remains the ambiguity of the state ofknowledge of the questioner.
If the person has no clue what theterm means, then a parent class, which is what VA finds, is theright answer.
If the person knows the class but needs to knowhow to distinguish the object from others in the class, forexample ?What is a star fruit?, then a very different approach isrequired.
If the question seems very specific, but uses commonwords entirely, such as the Excite question ?What is a yellowspotted lizard?, then the only reasonable interpretation seems tobe a request for a subclass of the head noun that has the givenproperty.
Finally, questions such as ?What is a nanometer?
and?What is rubella?
are looking for a value or more commonsynonym.5.2 Other Question TypesThe preceding discussion has centered upon What is questions andthe use of WordNet, but the same principles can be applied to otherquestion types and other ontologies.
Consider the question ?Whereis Chicago?, from the training set NIST supplied for TREC8.
Letus assume we can use statistical arguments to decide that, in avanilla context, the question is about the city as opposed to therock group, any of the city?s sports teams or the University.
Thereis still considerable ambiguity regarding the granularity of thedesired answer.
Is it:  Cook County?
Illinois?
The Mid-West?The United States?
North America?
The Western Hemisphere?
?There are a number of geographical databases available, whicheither alone or with some data massaging can be viewed asontologies with ?located within?
as the primary relationship.
Thenby applying Virtual Annotation to Where questions we can findthe enclosing region that is most commonly referred to in thecontext of the question term.
By manually applying our algorithmto ?Chicago?
and the list of geographic regions in the previousparagraph we find that ?Illinois?
wins, as expected, just beating out?The United States?.
However, it should be mentioned that a moreextensive investigation might find a different weighting schememore appropriate for geographic hierarchies.The aforementioned answer of ?Illinois?
to the question ?Where isChicago??
might be the best answer for an American user, but foranyone else, an answer providing the country might be preferred.How can we expect Virtual Annotation to take this into account?The ?hidden variable?
in the operation of VA is the corpus.
It isassumed that the user belongs to the intended readership of thearticles in the corpus, and to the extent that this is true, the resultsof VA will be useful to the user.Virtual Annotation can also be used to answer questions that areseeking examples or instances of a class.
We can use WordNetagain, but this time look to hyponyms.
These questions are morevaried in syntax than the What is kind;  they include, for examplefrom TREC9 again:?Name a flying mammal.?
?What flower did Vincent Van Gogh paint?
?and?What type of bridge is the Golden Gate Bridge??6.
SUMMARYWe presented Virtual Annotation, a technique to extend thecapabilities of PA to a class of definition questions in which theanswer type is not easily identifiable.
Moreover, VA can find textsnippets that do not contain the regular textual clues for presenceof definitions.
We have shown that VA can considerably improvethe performance of answering What is questions, and we indicatehow other kinds of questions can be tackled by similar techniques.7.
REFERENCES[1] Hearst, M.A.
?Automated Discovery of WordNet Relations?in WordNet: an Electronic Lexical Database, ChristianeFellbaum Ed, MIT Press, Cambridge MA, 1998.
[2] Mihalcea, R. and Moldovan, D. ?A Method for Word SenseDisambiguation of Unrestricted Text?.
Proceedings of the37th Annual Meeting of the Association for ComputationalLinguistics (ACL-99), pp.
152-158, College Park, MD, 1999.
[3] Miller, G. ?WordNet: A Lexical Database for English?,Communications of the ACM 38(11) pp.
39-41, 1995.
[4] Moldovan, D.I.
and Mihalcea, R. ?Using WordNet andLexical Operators to Improve Internet Searches?, IEEEInternet Computing, pp.
34-43, Jan-Feb 2000.
[5] Prager, J.M., Radev, D.R., Brown, E.W.
and Coden, A.R.
?The Use of Predictive Annotation for Question-Answeringin TREC8?, Proceedings of TREC8, Gaithersburg, MD,2000.
[6] Prager, J.M., Brown, E.W., Coden, A.R., and Radev, D.R.
"Question-Answering by Predictive Annotation",Proceedings of SIGIR 2000, pp.
184-191, Athens, Greece,2000.
[7] Radev, D.R., Prager, J.M.
and Samn, V. ?RankingSuspected Answers to Natural Language Questions usingPredictive Annotation?, Proceedings of ANLP?00, Seattle,WA, 2000.
[8] Rosch, E. et al ?Basic Objects in Natural Categories?,Cognitive Psychology 8, pp.
382-439, 1976.
[9] TREC8 - ?The Eighth Text Retrieval Conference?, E.M.Voorhees and D.K.
Harman Eds., NIST, Gaithersburg, MD,2000.
[10] TREC9 - ?The Ninth Text Retrieval Conference?, E.M.Voorhees and D.K.
Harman Eds., NIST, Gaithersburg, MD,to appear.APPENDIXWhat-is questions from TREC9617: What are chloroplasts?
(X)528: What are geckos?544: What are pomegranates?241: What is a caldera?
(X)358: What is a meerkat?434: What is a nanometer?
(X)354: What is a nematode?463: What is a stratocaster?447: What is anise?386: What is anorexia nervosa?635: What is cribbage?300: What is leukemia?305: What is molybdenum?644: What is ouzo?420: What is pandoro?
(X)228: What is platinum?374: What is porphyria?483: What is sake?395: What is saltpeter?421: What is thalassemia?438: What is titanium?600: What is typhoid fever?468: What is tyvek?539: What is witch hazel?Our system did not correctly answer the questions marked withan ?X?.
For all of the others the correct answer was the first ofthe 5 attempts returned.
