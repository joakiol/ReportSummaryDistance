Proceedings of the Second Workshop on Statistical Machine Translation, pages 232?239,Prague, June 2007. c?2007 Association for Computational LinguisticsEnglish-to-Czech Factored Machine TranslationOndr?ej BojarInstitute of Formal and Applied Linguistics?UFAL MFF UK, Malostranske?
na?me?st??
25CZ-11800 Praha, Czech Republicbojar@ufal.mff.cuni.czAbstractThis paper describes experiments withEnglish-to-Czech phrase-based machinetranslation.
Additional annotation of inputand output tokens (multiple factors) is usedto explicitly model morphology.
We varythe translation scenario (the setup of multi-ple factors) and the amount of informationin the morphological tags.
Experimentalresults demonstrate significant improvementof translation quality in terms of BLEU.1 IntroductionStatistical phrase-based machine translation (SMT)systems currently achieve top performing results.1Known limitations of phrase-based SMT includeworse quality when translating to morphologicallyrich languages as opposed to translating from them(Koehn, 2005).
One of the teams at the 2006 sum-mer engineering workshop at Johns Hopkins Uni-versity2 attempted to tackle these problems by in-troducing separate FACTORS in SMT input and/oroutput to allow explicit modelling of the underlyinglanguage structure.
The support for factored transla-tion models was incorporated into the Moses open-source SMT system3.In this paper, we report on experiments withEnglish-to-Czech multi-factor translation.
After abrief overview of factored SMT and our data (Sec-tions 2 and 3), we summarize some possible trans-lating scenarios in Section 4.
Section 5 studies the1http://www.nist.gov/speech/tests/mt/2http://www.clsp.jhu.edu/ws2006/3http://www.statmt.org/moses/level of detail useful for morphological representa-tion and Section 6 compares the results to a settingwith more data available, albeit out of domain.
Thesecond part (Section 7) is devoted to a brief analysisof MT output errors.1.1 Motivation for Improving MorphologyCzech is a Slavic language with very rich morphol-ogy and relatively free word order.
The Czech mor-phological system (Hajic?, 2004) defines 4,000 tagsin theory and 2,000 were actually seen in a bigtagged corpus.
(For comparison, the English PennTreebank tagset contains just about 50 tags.)
In ourparallel corpus (see Section 3 below), the Englishvocabulary size is 35k distinct token types but morethan twice as big in Czech, 83k distinct token types.To further emphasize the importance of morphol-ogy in MT to Czech, we compare the standardBLEU (Papineni et al, 2002) of a baseline phrase-based translation with BLEU which disregards wordforms (lemmatized MT output is compared to lem-matized reference translation).
The theoretical mar-gin for improving MT quality is about 9 BLEUpoints: the same MT output scores 12 points in stan-dard BLEU and 21 points in lemmatized BLEU.2 Overview of Factored SMTIn statistical MT, the goal is to translate a source(foreign) language sentence fJ1 = f1 .
.
.
fj .
.
.
fJinto a target language (Czech) sentence cI1 =c1 .
.
.
cj .
.
.
cI .
In phrase-based SMT, the assump-tion is made that the target sentence can be con-structed by segmenting source sentence into phrases,translating each phrase and finally composing the232target sentence from phrase translations, sK1 de-notes the segmentation of the input sentence intoK phrases.
Among all possible target languagesentences, we choose the sentence with the highestprobability,e?I?1 = argmaxI,cI1,K,sK1{Pr(cI1|fJ1 , sK1 )} (1)In a log-linear model, the conditional probabilityof cI1 being the translation of fJ1 under the segmenta-tion sK1 is modelled as a combination of independentfeature functions h1(?, ?, ?)
.
.
.
hM (?, ?, ?)
describingthe relation of the source and target sentences:Pr(cI1|fJ1 , sK1 ) =exp(?Mm=1 ?mhm(cI1, fJ1 , sK1 ))?c?I?1exp(?Mm=1 ?mhm(c?I?1 , fJ1 , sK1 ))(2)The denominator in 2 is used as a normalizationfactor that depends on the source sentence fJ1 andsegmentation sK1 only and is omitted during maxi-mization.
The model scaling factors ?M1 are trainedeither to the maximum entropy principle or opti-mized with respect to the final translation qualitymeasure.Most of our features are phrase-based and we re-quire all such features to operate synchronously onthe segmentation sK1 and independently of neigh-bouring segments.
In other words, we restrict theform of phrase-based features to:hm(cI1, fJ1 , sK1 ) =K?k=1h?m(c?k, f?k) (3)where f?k represents the source phrase and c?
repre-sents the target phrase k given the segmentation sK1 .2.1 Decoding StepsIn factored SMT, source and target words f and c arerepresented as tuples of F and C FACTORS, resp.,each describing a different aspect of the word, e.g.its word form, lemma, morphological tag, role in averbal frame.
The process of translation consists ofDECODING steps of two types: MAPPING steps andGENERATION steps.
If more steps contribute to thesame output factor, they have to agree on the out-come, i.e.
partial hypotheses where two decodingsteps produce conflicting values in an output factorare discarded.A MAPPING step from a subset of source fac-tors S ?
{1 .
.
.
F} to a subset of target factorsT ?
{1 .
.
.
C} is the standard phrase-based model(see e.g.
(Koehn, 2004a)) and introduces a featurein the following form:h?map:S?Tm (c?k, f?k) = log p(f?Sk |c?Tk ) (4)The conditional probability of f?Sk , i.e.
the phrasef?k restricted to factors S, given c?Tk , i.e.
the phrasec?k restricted to factors T is estimated from relativefrequencies: p(f?Sk |c?Tk ) = N(f?S, c?T )/N(c?T ) whereN(f?S, c?T ) denotes the number of co-occurrences ofa phrase pair (f?S, c?T ) that are consistent with theword alignment.
The marginal count N(c?T ) is thenumber of occurrences of the target phrase c?T in thetraining corpus.For each mapping step, the model is included inthe log-linear combination in source-to-target andtarget-to-source directions: p(f?T |c?S) and p(c?S |f?T ).In addition, statistical single word based lexica areused in both directions.
They are included to smooththe relative frequencies used as estimates of thephrase probabilities.A GENERATION step maps a subset of target fac-tors T1 to a disjoint subset of target factors T2,T1,2 ?
{1 .
.
.
C}.
In the current implementationof Moses, generation steps are restricted to word-to-word correspondences:h?gen:T1?T2m (c?k, f?k) = loglength(c?k)?i=1p(c?T1k,i|c?T2k,i) (5)where c?Tk,i is the i-th words in the k-th target phraserestricted to factors T .
We estimate the conditionalprobability p(c?T2k,i|c?T1k,i) by counting over words in thetarget-side corpus.
Again, the conditional probabil-ity is included in the log-linear combination in bothdirections.In addition to features for decoding steps, we in-clude arbitrary number of target language modelsover subsets of target factors, T ?
{1 .
.
.
C}.
Typi-cally, we use the standard n-gram language model:233hTLMn(fJ1 , cI1) = logI?i=1p(cTi |cTi?1 .
.
.
cTi?n+1) (6)While generation steps are used to enforce ?verti-cal?
coherence between ?hidden properties?
of out-put words, language models are used to enforce se-quential coherence of the output.Operationally, Moses performs a stack-basedbeam search very similar to Pharaoh (Koehn,2004a).
Thanks to the synchronous-phrases assump-tion, all the decoding steps can be performed duringa preparatory phase.
For each span in the input sen-tence, all possible translation options are constructedusing the mapping and generation steps in a user-specified order.
Low-scoring options are pruned al-ready during this phase.
Once all translation optionsare constructed, Moses picks source phrases (all out-put factors already filled in) in arbitrary order, sub-ject to a reordering limit, producing output in left-to-right fashion and scoring it using the specified lan-guage models exactly as Pharaoh does.3 Data UsedThe experiments reported in this paper were car-ried out with the News Commentary (NC) corpus asmade available for the SMT workshop4 of the ACL2007 conference.5The Czech part of the corpus was tagged and lem-matized using the tool by Hajic?
and Hladka?
(1998),the English part was tagged MXPOST (Ratnaparkhi,1996) and lemmatized using the Morpha tool (Min-nen et al, 2001).
After some final cleanup, thecorpus consists of 55,676 pairs of sentences (1.1MCzech tokens and 1.2M English tokens).
We use thedesignated additional tuning and evaluation sectionsconsisting of 1023, resp.
964 sentences.In all experiments, word alignment was obtainedusing the grow-diag-final heuristic for symmetriz-ing GIZA++ (Och and Ney, 2003) alignments.
Toreduce data sparseness, the English text was lower-cased and Czech was lemmatized for alignment es-timation.
Language models are based on the target4http://www.statmt.org/wmt07/5Our preliminary experiments with the Prague Czech-English Dependency Treebank, PCEDT v.1.0 ( ?Cmejrek et al,2004), 20k sentences, gave similar results, although with alower level of significance due to a smaller evaluation set.side of the parallel corpus only, unless stated other-wise.3.1 Evaluation Measure and MERTWe evaluate our experiments using the (lowercase,tokenized) BLEU metric and estimate the empiri-cal confidence using the bootstrapping method de-scribed in Koehn (2004b).6 We report the scoresobtained on the test section with model parameterstuned using the tuning section for minimum errorrate training (MERT, (Och, 2003)).4 Scenarios of Factored TranslationEnglish?CzechWe experimented with the following factored trans-lation scenarios.The baseline scenario (labelled T for translation)is single-factored: input (English) lowercase wordforms are directly translated to target (Czech) low-ercase forms.
A 3-gram language model (or moremodels based on various corpora) checks the streamof output word forms.
The baseline scenario thuscorresponds to a plain phrase-based SMT system:English Czechlowercase lowercase +LMlemma lemmamorphology morphologyIn order to check the output not only for word-level coherence but also for morphological coher-ence, we add a single generation step: input wordforms are first translated to output word forms andeach output word form then generates its morpho-logical tag.Two types of language models can be used simul-taneously: a (3-gram) LM over word forms and a(7-gram) LM over morphological tags.We used tags with various levels of detail, see sec-tion 5.
We call this the ?T+C?
(translate and check)scenario:6Given a test set of sentences, we perform 1,000 random se-lections with repetitions to estimate 1,000 BLEU scores on testsets of the same size.
The empirical 90%-confidence upper andlower bounds are obtained after removing top and bottom 5% ofscores.
For conciseness, we report the average of the distancebetween to standard BLEU value and the empirical upper andlower bound after the ???
symbol.234English Czechlowercase lowercase +LMlemma lemmamorphology morphology +LMAs a refinement of T+C, we also used T+T+Cscenario, where the morphological output stream isconstructed based on both output word forms and in-put morphology.
This setting should reinforce cor-rect translation of morphological features such asnumber of source noun phrases.
To reduce the riskof early pruning, the generation step operationallyprecedes the morphology mapping step.
Again,two types of language models can be used in this?T+T+C?
scenario:English Czechlowercase lowercase +LMlemma lemmamorphology morphology +LMThe most complex scenario we used is linguis-tically appealing: output lemmas (base forms) andmorphological tags are generated from input in twoindependent translation steps and combined in a sin-gle generation step to produce output word forms.The input English text was not lemmatized so weused English word forms as the source for produc-ing Czech lemmas.The ?T+T+G?
setting allows us to use three typesof language models.
Trigram models are used forword forms and lemmas and 7-gram language mod-els are used over tags:English Czechlowercase lowercase +LMlemma lemma +LMmorphology morphology +LM4.1 Experimental Results: Improved over TTable 1 summarizes estimated translation quality ofthe various scenarios.
In all cases, a 3-gram LM isused for word forms or lemmas and a 7-gram LMfor morphological tags.The good news is that multi-factored models al-ways outperform the baseline T.Unfortunately, the more complex multi-factoredscenarios do not bring any significant improvementover T+C.
Our belief is that this effect is caused bysearch errors: with multi-factored models, more hy-potheses get similar scores and future costs of partialBLEUT+T+G 13.9?0.7T+T+C 13.9?0.6T+C 13.6?0.6Baseline: T 12.9?0.6Table 1: BLEU scores of various translation scenar-ios.hypotheses might be estimated less reliably.
Withthe limited stack size (not more than 200 hypothe-ses of the same number of covered input words), thedecoder may more often find sub-optimal solutions.Moreover, the more steps are used, the more modelweights have to be tuned in the minimum error ratetraining.
Considerably more tuning data might benecessary to tune the weights reliably.5 Granularity of Czech Part-of-SpeechAs stated above, the Czech morphological tag sys-tem is very complex: in theory up to 4,000 differenttags are possible.
In our T+T+C scenario, we exper-iment with various simplifications of the system tofind the best balance between richness and robust-ness of the statistics available in our corpus.
(Themore information is retained in the tags, the moresevere data sparseness is.
)Full tags (1200 unique seen in the 56k corpus):Full Czech positional tags are used.
A tagconsists of 15 positions, each holding the valueof a morphological property (e.g.
number, caseor gender).7POS+case (184 unique seen): We simplify the tagto include only part and subpart of speech (dis-tinguishes also partially e.g.
verb tenses).
Fornouns, pronouns, adjectives and prepositions8 ,also the case is included.CNG01 (621 unique seen): CNG01 refines POS.For nouns, pronouns and adjectives we includenot only the case but also number and gender.7In principle, each of the 15 positions could be used as aseparate factor.
The set of necessary generation steps to encoderelevant dependencies would have to be carefully determined.8Some Czech prepositions select for a particular case, someare ambiguous.
Although the case is never shown on surface ofthe preposition, the tagset includes this information and Czechtaggers are able to infer the case.235CNG02 (791 unique seen): Tag for punctuation isrefined: the lemma of the punctuation symbolis taken into account; previous models disre-garded e.g.
the distributional differences be-tween a comma and a question mark.
Case,number and gender added to nouns, pronouns,adjectives, prepositions, but also to verbs andnumerals (where applicable).CNG03 (1017 unique seen): Optimized tagset:?
Tags for nouns, adjectives, pronouns andnumerals describe the case, number andgender; the Czech reflexive pronoun se orsi is highlighted by a special flag.?
Tag for verbs describes subpart of speech,number, gender, tense and aspect; the tagincludes a special flag if the verb was theauxiliary verb by?t (to be) in any of itsforms.?
Tag for prepositions includes the case andalso the lemma of the preposition.?
Lemma included for punctuation, parti-cles and interjections.?
Tag for numbers describes the ?shape?
ofthe number (all digits are replaced by thedigit 5 but number-internal punctuation iskept intact).
The tag thus distinguishes be-tween 4- or 5-digit numbers or the preci-sion of floating point numbers.?
Part of speech and subpart of speech forall other words.5.1 Experimental Results: CNG03 BestTable 2 summarizes the results of T+T+C scenariowith varying detail in morphological tag.BLEUBaseline: T (single-factor) 12.9?0.6T+T+C, POS+case 13.2?0.6T+T+C, CNG01 13.4?0.6T+T+C, CNG02 13.5?0.7T+T+C, full tags 13.9?0.6T+T+C, CNG03 14.2?0.7Table 2: BLEU scores of various granularities ofmorphological tags in T+T+C scenario.NC NC CzEngmixweighted = ?NC + ?mixScenario Phrases from LMs BLEUT NC NC 12.9?0.6T mix mix 11.8?0.6T mix weighted 11.8?0.6T+C CNG03 NC NC 13.7?0.7T+C CNG03 mix mix 13.1?0.7T+C CNG03 mix weighted 13.7?0.7T+C full tags NC NC 13.6?0.6T+C full tags mix mix 13.1?0.7T+C full tags mix weighted 13.8?0.7Figure 1: The effect of additional data in T and T+Cscenarios.Our results confirm improvement over the single-factored baseline.
Detailed knowledge of the mor-phological system also proves its utility: by choos-ing the most relevant features of tags and lemmasbut avoiding sparseness, we can improve on BLEUscore by about 0.3 absolute over T+T+C with fulltags.6 More Out-of-Domain Data in T and T+CScenariosIn order to check if the method scales up withmore parallel data available, we extend our train-ing data using the CzEng parallel corpus (Bojarand ?Zabokrtsky?, 2006).
CzEng contains sentence-aligned texts from the European Parliament (about75%), e-books and stories (15%) and open sourcedocumentation.
By ?Baseline?
corpus we denoteNC corpus only, by ?Large?
we denote the combi-nation of training sentences from NC and CzEng(1070k sentences, 13.9M Czech and 15.5 Englishtokens) where in-domain NC data amounts only to5.2% sentences.Figure 1 gives full details of our experiments withthe additional data.
We varied the scenario (T orT+C), the level of detail in the T+C scenario (fulltags vs. CNG03) and the size of the training corpus.We extract phrases from either the in-domain corpusonly (NC) or the mixed corpus (mix).
We use eitherone LM per output factor, varying the corpus size(NC or mix), or two LMs per output factors withweights trained independently in the MERT proce-236dure (weighted).
Independent weights allow us totake domain difference into account, but we exploitthis in the target LM only, not the phrases.The only significant difference is caused by thescenario: T+C outperforms the baseline T, regard-less of corpus size.
Other results (insignificantly)indicate the following observations:?
Ignoring the domain difference and using onlythe mixed domain LM in general performsworse than allowing MERT to optimize LMweights for in-domain and generic data sepa-rately.9?
CNG03 outperforms full tags only in small datasetting, with large data (treating the domain dif-ference properly), full tags perform better.7 Untreated Morphological ErrorsThe previous sections described improvementsgained on small data sets when checking morpho-logical agreement using T+T+C scenario (BLEUraised from 12.9% to 13.9% or up to 14.2% withmanually tuned tagset, CNG03).
However, the bestresult achieved is still far below the margin of lem-matized BLEU (21%), as mentioned in Section 1.1.When we searched for the unexploited morpho-logical errors, visual inspection of MT output sug-gested that local agreement (within 3-word span) isrelatively correct but Verb-Modifier relations are of-ten malformed causing e.g.
a bad case for the Mod-ifier.
To quantify this observation we performed amicro-study of our best MT output using an intu-itive metric.
We checked whether Verb-Modifier re-lations are properly preserved during the translationof 15 sample sentences.The source text of the sample sentences contained77 Verb-Modifier pairs.
Table 3 lists our observa-tions on the two members in each Verb-Modifierpair.
We see that only 56% of verbs are translatedcorrectly and 79% of nouns are translated correctly.The system tends to skip verbs quite often (27% ofcases).9In our previous experiments with PCEDT as the domain-specific data, the difference was more apparent because the cor-pus domains were more distant.
In the T scenario reported here,the weighted LMs did not bring any improvement over ?mix?and even performed worse than the baseline NC.
We attributethis effect to some randomness in the MERT procedure.Translation of Verb Modifier.
.
.
preserves meaning 56% 79%.
.
.
is disrupted 14% 12%.
.
.
is missing 27% 1%.
.
.
is unknown (not translated) 0% 5%Table 3: Analysis of 77 Verb-Modifier pairs in 15sample sentences.More importantly, our analysis has shown thateven in cases where both the Verb and the Modi-fier are lexically correct, the relation between themin Czech is either non-grammatical or meaning-disrupted in 56% of these cases.
Commented sam-ples of such errors are given in Figure 2 below.
Thefirst sample shows that a strong language model canlead to the choice of a grammatical relation that nev-ertheless does not convey the original meaning.
Thesecond sample illustrates a situation where two cor-rect options are available but the system choosesan inappropriate relation, most probably because ofbacking off to a generic pattern verb-nounaccusativeplural .This pattern is quite common for expressing the ob-ject role of many verbs (such as vydat, see Cor-rect option 2 in Figure 2), but does not fit wellwith the verb vybe?hnout.
While the target-sidedata may be rich enough to learn the generalizationvybe?hnout?s?instr, no such generalization is possi-ble with language models over word forms or mor-phological tags only.
The target side data will behardly ever rich enough to learn this particular struc-ture in all correct morphological and lexical variants:vybe?hl?s?reklamou, vybe?hla?s?reklamami, vybe?hl?s?prohla?s?en?
?m, vybe?hli?s?ozna?men?
?m, .
.
.
.
Wewould need a mixed model that combines verb lem-mas, prepositions and case information to properlycapture the relations.Unfortunately, our preliminary experiments thatmade use of automatic Czech dependency parsetrees to construct a factor explicitly highlighting theVerb (lexicalized) its Modifiers (case and the lemmaof the preposition, if present) and boundary sym-bols such as punctuation or conjunctions and usinga dummy token for all other words did not bring anyimprovement over the baseline.
A possible reason isthat we employed only a standard 7-gram languagemodel to this factor.
A more appropriate treatment237is to disregard the dummy tokens in the languagemodel at all and use an n-gram language model thatlooks at last n?
1 non-dummy items.8 Related ResearchClass-based LMs (Brown et al, 1992) or factoredLMs (Bilmes and Kirchhoff, 2003) are very similarto our T+C scenario.
Given the small differencesin all T+.
.
.
scenarios?
performance, class-based LMmight bring equivalent improvement.
Yang andKirchhoff (2006) have recently documented minorBLEU improvement using factored LMs in single-factored SMT to English.
The multi-factored ap-proach to SMT of Moses is however more general.Many researchers have tried to employ mor-phology in improving word alignment techniques(e.g.
(Popovic?
and Ney, 2004)) or machine trans-lation quality (Nie?en and Ney (2001), Koehn andKnight (2003), Zollmann et al (2006), among oth-ers, for various languages; Goldwater and McClosky(2005), Bojar et al (2006) and Talbot and Osborne(2006) for Czech), however, they focus on translat-ing from the highly inflectional language.Durgar El-Kahlout and Oflazer (2006) report pre-liminary experiments in English to Turkish single-factored phrase-based translation, gaining signifi-cant improvements by splitting root words and theirmorphemes into a sequence of tokens.
In might beinteresting to explore multi-factored scenarios fordifferent Turkish morphology representation sug-gested the paper.de Gispert et al (2005) generalize over verb formsand generate phrase translations even for unseen tar-get verb forms.
The T+T+G scenario allows a sim-ilar extension if the described generation step is re-placed by a (probabilistic) morphological generator.Nguyen and Shimazu (2006) translate from En-glish to Vietnamese but the morphological richnessof Vietnamese is comparable to English.
In fact theVietnamese vocabulary size is even smaller than En-glish vocabulary size in one of their corpora.
Theobserved improvement due to explicit modelling ofmorphology might not scale up beyond small-datasetting.As an alternative option to our verb-modifierexperiments, structured language models (Chelbaand Jelinek, 1998) might be considered to improveclause coherence, until full-featured syntax-basedMT models (Yamada and Knight (2002), Eisner(2003), Chiang (2005) among many others) aretested when translating to morphologically rich lan-guages.9 ConclusionWe experimented with multi-factored phrase-basedtranslation aimed at improving morphological co-herence in MT output.
We varied the setup of ad-ditional factors (translation scenario) and the levelof detail in morphological tags.
Our results onEnglish-to-Czech translation demonstrate signifi-cant improvement in BLEU scores by explicit mod-elling of morphology and using a separate morpho-logical language model to ensure the coherence.
Toour knowledge, this is one of the first experimentsshowing the advantages of using multiple factors inMT.Verb-modifier errors have been studied and a fac-tor capturing verb-modifier dependencies has beenproposed.
Unfortunately, this factor has yet to bringany improvement.10 AcknowledgementThe work on this project was partially sup-ported by the grants Collegium InformaticumGA ?CR 201/05/H014, grants No.
ME838 andGA405/06/0589 (PIRE), FP6-IST-5-034291-STP(Euromatrix), and NSF No.
0530118.ReferencesJeff A. Bilmes and Katrin Kirchhoff.
2003.
Factored languagemodels and generalized parallel backoff.
In Proc.
of NAACL2003, pages 4?6.Ondr?ej Bojar and Zdene?k ?Zabokrtsky?.
2006.
CzEng: Czech-English Parallel Corpus, Release version 0.5.
Prague Bul-letin of Mathematical Linguistics, 86:59?62.Ondr?ej Bojar, Evgeny Matusov, and Hermann Ney.
2006.Czech-English Phrase-Based Machine Translation.
In Proc.of FinTAL 2006, pages 214?224, Turku, Finland.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jen-nifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Computational Linguis-tics, 18(4):467?479.Ciprian Chelba and Frederick Jelinek.
1998.
Exploiting syntac-tic structure for language modeling.
In Proc.
of ACL 1998,pages 225?231, San Francisco, California.238Input: Keep on investing.MT output: Pokrac?ovalo investova?n??.
(grammar correct here!
)Gloss: Continued investing.
(Meaning: The investing continued.
)Correct: Pokrac?ujte v investova?n?
?.Input: brokerage firms rushed out ads .
.
.MT Output: brokerske?
firmy vybe?hl reklamyGloss: brokerage firmspl.fem ransg.masc adspl.voc,sg.genpl.nom,pl.accCorrect option 1: brokerske?
firmy vybe?hly s reklamamipl.instrCorrect option 2: brokerske?
firmy vydaly reklamypl.accFigure 2: Two sample errors in translating Verb-Modifier relation from English to Czech.David Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proc.
of ACL 2005, pages263?270.Martin ?Cmejrek, Jan Cur??
?n, Jir???
Havelka, Jan Hajic?, andVladislav Kubon?.
2004.
Prague Czech-English DependecyTreebank: Syntactically Annotated Resources for MachineTranslation.
In Proc.
of LREC 2004, Lisbon, Portugal.Adria` de Gispert, Jose?
B. Marin?o, and Josep M. Crego.
2005.Improving statistical machine translation by classifying andgeneralizing inflected verb forms.
In Proc.
of Eurospeech2005, pages 3185?3188, Lisbon, Portugal.
?Ilknur Durgar El-Kahlout and Kemal Oflazer.
2006.
Initial Ex-plorations in English to Turkish Statistical Machine Transla-tion.
In Proc.
of the Workshop on Statistical Machine Trans-lation, ACL 2006, pages 7?14, New York City.Jason Eisner.
2003.
Learning non-isomorphic tree mappingsfor machine translation.
In Proc.
of ACL 2003, CompanionVolume, pages 205?208, Sapporo, Japan.Sharon Goldwater and David McClosky.
2005.
Improvingstatistical MT through morphological analysis.
In Proc.
ofHLT/EMNLP 2005, pages 676?683.Jan Hajic?
and Barbora Hladka?.
1998.
Tagging Inflective Lan-guages: Prediction of Morphological Categories for a Rich,Structured Tagset.
In Proc.
of COLING/ACL 1998, pages483?490, Montreal, Canada.Jan Hajic?.
2004.
Disambiguation of Rich Inflection (Compu-tational Morphology of Czech).
Nakladatelstv??
Karolinum,Prague.Philipp Koehn and Kevin Knight.
2003.
Empirical methods forcompound splitting.
In Proc.
of EACL 2003, pages 187?193.Philipp Koehn.
2004a.
Pharaoh: A beam search decoder forphrase-based statistical machine translation models.
In Proc.of AMTA 2004, pages 115?124.Philipp Koehn.
2004b.
Statistical Significance Tests for Ma-chine Translation Evaluation.
In Proc.
of EMNLP 2004,Barcelona, Spain.Philipp Koehn.
2005.
Europarl: A Parallel Corpus for Statisti-cal Machine Translation.
In Proc.
of MT Summit X.Guido Minnen, John Carroll, and Darren Pearce.
2001.
Ap-plied morphological processing of English.
Natural Lan-guage Engineering, 7(3):207?223.T.P.
Nguyen and A. Shimazu.
2006.
Improving Phrase-BasedSMT with Morpho-Syntactic Analysis and Transformation.In Proc.
of AMTA 2006, pages 138?147.Sonja Nie?en and Hermann Ney.
2001.
Toward hierarchicalmodels for statistical machine translation of inflected lan-guages.
In Proc.
of Workshop on Data-driven methods inmachine translation, ACL 2001, pages 1?8.Franz Josef Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proc.
of ACL 2003, Sapporo,Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
BLEU: a Method for Automatic Evaluation ofMachine Translation.
In Proc.
of ACL 2002, pages 311?318.M.
Popovic?
and H. Ney.
2004.
Improving Word AlignmentQuality using Morpho-Syntactic Information.
In Proc.
ofCOLING 2004, Geneva, Switzerland.Adwait Ratnaparkhi.
1996.
A Maximum Entropy Part-Of-Speech Tagger.
In Proc.
of EMNLP 1996, Philadelphia,USA.David Talbot and Miles Osborne.
2006.
Modelling lexical re-dundancy for machine translation.
In Proc.
of COLING andACL 2006, pages 969?976, Sydney, Australia.Kenji Yamada and Kevin Knight.
2002.
A decoder for syntax-based statistical MT.
In Proc.
of ACL 2002, pages 303?310.Mei Yang and Katrin Kirchhoff.
2006.
Phrase-based backoffmodels for machine translation of highly inflected languages.In Proc.
of EACL 2006.Andreas Zollmann, Ashish Venugopal, and Stephan Vogel.2006.
Bridging the inflection morphology gap for arabic sta-tistical machine translation.
In Proc.
of HLT/NAACL.239
