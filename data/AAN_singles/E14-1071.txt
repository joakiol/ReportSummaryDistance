Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 673?682,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsAutomatic Food Categorization from Large Unlabeled Corpora and ItsImpact on Relation ExtractionMichael Wiegand and Benjamin Roth and Dietrich KlakowSpoken Language SystemsSaarland UniversityD-66123 Saarbru?cken, Germany{Michael.Wiegand|Benjamin.Roth|Dietrich.Klakow}@lsv.uni-saarland.deAbstractWe present a weakly-supervised induc-tion method to assign semantic informa-tion to food items.
We consider two tasksof categorizations being food-type classi-fication and the distinction of whether afood item is composite or not.
The cate-gorizations are induced by a graph-basedalgorithm applied on a large unlabeleddomain-specific corpus.
We show that theusage of a domain-specific corpus is vi-tal.
We do not only outperform a manuallydesigned open-domain ontology but alsoprove the usefulness of these categoriza-tions in relation extraction, outperformingstate-of-the-art features that include syn-tactic information and Brown clustering.1 IntroductionIn view of the large interest in food in many partsof the population and the ever increasing amountof new dishes/food items, there is a need of au-tomatic knowledge acquisition.
We approach thistask with the help of natural language processing.We investigate different methods to assign cate-gories to food items.
We focus on two categoriza-tions, being a classification of food items to cat-egories of the Food Guide Pyramid (U.S. Depart-ment of Agriculture, 1992) and a categorization ofwhether a food item is composite or not.We present a semi-supervised graph-based ap-proach to induce these food categorizations froman unlabeled domain-specific text corpus crawledfrom the Web.
The method only requires mini-mal manual guidance for the initialization of thealgorithm with seed terms.
It depends, however,on an automatically constructed high-quality sim-ilarity graph.
For that we choose a pattern-basedrepresentation that outperforms a distributional-based representation.
For initialization, we ex-amine some manually compiled seed words anda very few simple surface patterns to automati-cally induce such expressions.
As a hard baseline,we compare the effectiveness of using a general-purpose ontology for the same types of categoriza-tions.
Apart from an intrinsic evaluation, we alsoexamine the categories in relation extraction.The contributions of this paper are a method re-quiring minimal supervision for a comprehensiveclassification of food items and a proof of con-cept that the knowledge that can thus be gained isbeneficial for relation extraction.
Even though wefocus on a specific domain, the induction methodcan be easily translated to other domains.
In par-ticular, other life-style domains, such as fashion,cosmetics or home & gardening, show parallelssince comparable textual web data are availableand similar relation types (e.g.
that two items fittogether or can be substituted by each other) exist.Our experiments are carried out on German databut our findings should carry over to other lan-guages since the issues we address are (mostly)language universal.
For general accessibility, allexamples are given as English translations.2 Data & Annotation2.1 Domain-Specific Text CorpusIn order to generate a dataset for our experiments,we used a crawl of chefkoch.de1 (Wiegand et al.,2012b) consisting of 418, 558 webpages of food-related forum entries.
chefkoch.de is the largestGerman web portal for food-related issues.2.2 Food CategorizationAs a food vocabulary, we employ a list of 1888food items: 1104 items were directly extractedfrom GermaNet (Hamp and Feldweg, 1997), theGerman version of WordNet (Miller et al., 1990).The items were identified by extracting all hy-ponyms of the synset Nahrung (English: food).
By1www.chefkoch.de673Class Description Size Perc.MEAT meat and fish (products) 394 20.87BEVERAGE beverages (incl.
alcoholic drinks) 298 15.78VEGE vegetables (incl.
salads) 231 12.24SWEET sweets, pastries and snack mixes 228 12.08SPICE spices and sauces 216 11.44STARCH starch-based side dishes 185 9.80MILK milk products 104 5.51FRUIT fruits 94 4.98GRAIN grains, nuts and seeds 77 4.08FAT fat 41 2.18EGG eggs 20 1.06Table 1: The different food types (gold standard).consulting the relation tuples from Wiegand et al.
(2012c) a further 784 items were added.
We man-ually annotated this vocabulary w.r.t.
two tasks:2.2.1 Task I: Food TypesThe food type categories we chose are mainly in-spired by the Food Guide Pyramid (U.S. Depart-ment of Agriculture, 1992) that divides food itemsinto categories with similar nutritional properties.This categorization scheme not only divides theset of food items in many intuitive homogeneousclasses but it is also the scheme that is most com-monly agreed upon.
Table 1 lists the specific cat-egories we use.
For category assignment of com-plex dishes comprising different food items we ap-plied a heuristics: we always assign the categorythat dominates the dish.
A meat sauce, for exam-ple, would thus be assigned MEAT (even thoughthere may be other ingredients than meat).2.2.2 Task II: Dishes vs. Atomic Food ItemsIn addition to Task I, we include another catego-rization that divides food items into dishes andatomic food items (Table 2).
By dish, we mainlyunderstand food items that are composite fooditems made of other (atomic) food items.
Thiscategorization is orthogonal to the previous clas-sification of food items.
We refrained from addingdishes as a further category of food types in ?2.2.1,as we would have ended up with a very heteroge-neous class in the set of homogeneous food typecategories.
Thus, dishes that differ greatly in nu-trient content, such as Waldorf salad and chocolatecake, would have been subsumed by one class.3 Method3.1 Graph-based InductionWe propose a semi-supervised graph-based ap-proach to label food items with their respectiveClass Description Examples Perc.DISH composite food items cake, falafel, meat loaf 32.10ATOM non-composite food items apple, steak, potato 67.90Table 2: Distribution of dishes and atomic fooditems among the food vocabulary (gold standard).food categories.
The underlying data structureis a similarity graph connecting different fooditems.
Food items that belong to the same categoryshould be connected by highly weighted edges.
Inorder to infer the labels for each respective fooditem, one first needs to specify a small set of seedsfor each category and then apply a graph-basedclustering method that divides the graph into clus-ters that represent distinct food categories.
Ourmethod is a low-resource approach that can alsobe easily adapted to other domains.
The onlydomain-specific information required are an unla-beled corpus and a set of seeds.3.1.1 Construction of the Similarity GraphTo enable a graph-based induction, we generate asimilarity graph that connects similar food items.For that purpose, a list of domain-independentsimilarity-patterns was compiled.
Each pattern is alexical sequence that connects the mention of twofood items (Table 3).
Each pair of food items ob-served with any of those patterns is connected viaa weighted edge (the different patterns are treatedequally).
The weight is the total frequency of allpatterns co-occurring with a particular food pair.Due to the high precision of our patterns, withone or a few prototypical seeds we cannot expectto find all items of a food category within the setof items to which the seeds are directly connected.Instead, one also needs to consider transitive con-nectedness within the graph.
For example, in Fig-ure 1 banana and redberry are not directly con-nected but they can be reached via pear or rasp-berry.
However, by considering mediate relation-ships it becomes more difficult to determine themost appropriate category for each food item sincemost food items are connected to food items of dif-ferent categories (in Figure 1, there are not onlyedges between banana and other types of fruitsbut there is also some edge to some sweet, i.e.chocolate).
For a unique class assignment, we ap-ply a robust graph-based clustering algorithm.
(Itwill figure out that banana, pear, raspberry andredberry belong to the same category and choco-late belongs to another category, since it is mostly674Patterns food item1(or|or rather|instead of|?(?)
food item2Example {apple: pineapple, pear, fruit, strawberry, kiwi} {steak:schnitzel, sausage, roast, meat loaf, cutlet}Table 3: Domain-independent patterns for build-ing the similarity graph.Figure 1: Illustration of the similarity graph.linked to many other food items not being fruits.
)3.1.2 Semi-Supervised Graph OptimizationOur semi-supervised graph optimization (Belkinand Niyogi, 2004) is a robust algorithm that wasprimarily chosen since it only contains few freeparameters to adjust.
It is based on two principles:First, similar data points should be assigned simi-lar labels, as expressed by a similarity graph of la-beled and unlabeled data.
Second, for labeled datapoints the prediction of the learnt classifier shouldbe consistent with the (actual) gold labels.We construct a weighted transition matrix Wof the graph by normalization of the matrix withco-occurrence counts C which we obtain from thesimilarity graph (?3.1.1).
We use the commonnormalization by a power of the degree functiondi=?jCij: it defines Wij=Cijd?id?jif i 6= j,and Wii= 0.
The normalization weight ?
is thefirst of two parameters used in our experiments forsemi-supervised graph optimization.
For learningthe semi-supervised classifier, we use the methodof Zhou et al.
(2004) to find a classifying functionwhich is sufficiently smooth with respect to boththe structure of unlabeled and labeled points.Given a set of data points X = {x1, .
.
.
, xn}and label set L = {1, .
.
.
, c}, with xi:1?i?llabeledas yi?
L and xi:l+1?i?nunlabeled.
For predic-tion, a vectorial function F : X ?
Rc is estimatedassigning a vector Fiof label scores to every xi.The predicted labeling follows from these scoresas y?i= argmaxj?cFij.
Conversely, the gold la-beling matrix Y is a n ?
c matrix with Yij= 1 ifxiis labeled as yi= j and Yij= 0 otherwise.Minimizing the cost function Q aims at a trade-off between information from neighbours and ini-tial labeling information, controlled by parameterPatterns Categorization ExamplespatthearstFood Types food item is some food type,food type such as food item, .
.
.pattdishesDishes recipe for food itempattatomAtomic Food Items made of|contains food itemTable 4: List of patterns to extract seeds.?
(the second parameter used in our experiments):Q =12(?ni,j=1Wij?????1??iFi?1??jFj????
?+ ??ni=1?Fi?
Yi?
)where ?iis the degree function of W .The first term in Q is the smoothness constraint,its minimization leads to adjacent edges havingsimilar labels.
The second term is the fitting con-straint, its minimization leads to consistency of thefunction F with the labeling of the data.
The solu-tion to the above cost function is found by solvinga system of linear equations (Zhou et al., 2004).As we do not possess development data for thiswork, we set the two free parameters ?
= 0.5 and?
= 0.01.
This setting is used for both inductiontasks and all configurations.
It is a setting that pro-vided reasonable results without any notable biasfor any particular configuration we examine.3.1.3 Manually vs.
Automatically ExtractedSeedsWe explore two types of seed initializations: (a)a manually compiled list of seed food items and(b) a small set of patterns (Table 4) by the help ofwhich such seeds are automatically extracted.In order to extract seeds for Task I with thepattern-based approach, we apply the patternsfrom Hearst (1992).
These patterns have been de-signed for the acquisition of hyponyms.
Task I canalso be regarded as some type of hyponym extrac-tion.
The food types (fruit, meat, sweets) repre-sent the hypernyms for which we extract seed hy-ponyms (banana, beef, chocolate).In order to extract seeds for Task II, we applytwo domain-specific sets of patterns (pattdishandpattatom).
We rank the food items according to thefrequency of occurring with the respective patternset.
Since food items may occur in both rankings,we merge the two rankings in the following way:score(food item) = #pattdish(food it.
)?#pattatom(food it.
)The top end of this ranking represents disheswhile the bottom end represents atoms.3.2 Using a General-Purpose OntologyAs a hard baseline, we also make use of the seman-tic relationships encoded in GermaNet.
Our two675types of food categorization schemes can be ap-proximated with the hypernymy graph in that on-tology: We manually identify nodes that resembleour food categories (e.g.
fruit, meat or dish) andlabel any food item that is an immediate or a me-diate hyponym of these nodes (e.g.
apple for fruit)with the respective category label.
The downsideof this method is that a large amount of food itemsis missing from the GermaNet-database (?2.2).3.3 Other Baselines & Post-ProcessingIn addition to the previous methods we imple-ment a heuristic baseline (HEUR) that rests on theobservation that German food items of the samefood category often share the same suffix, e.g.Schokoladenkuchen (English: chocolate cake) andApfelkuchen (English: apple pie).
For HEUR, wemanually compiled a set of few typical suffixes foreach food type/dish category (ranging from 3 to 8suffixes per category).
For classification of a fooditem, we assign the food item the category labelwhose suffix matched with the food item.2We also examine an unsupervised baseline(UNSUP) that applies spectral clustering on thesimilarity graph following von Luxburg (2007):?
Input: a similarity matrix W and the number of categories to detect k.?
The laplacian L is constructed from W .
It is the symmetric laplacianL = I ?D1/2WD1/2, where D is a diagonal degree matrix.3?
A matrix U ?
Rn?k is constructed that contains as columns the firstk eigenvectors u1, .
.
.
, ukof L.?
The rows of U are interpreted as the new data points.
The final cluster-ing is obtained by k-means clustering of the rows of U .UNSUP (which is completely parameter-free)gives some indication about the intrinsic expres-siveness of the similarity graph as it lacks anyguidance towards the categories to be predicted.In graph-based food categorization, one canonly make predictions for food items that are con-nected (be it directly or indirectly) to seed fooditems within the similarity graph.
To expand labelsto unconnected food items, we apply some post-processing (POSTP).
Similarly to HEUR, it ex-ploits the suffix-similarity of food items.
It assignseach unconnected food item the label of the fooditem (that could be labeled by the graph optimiza-tion) that shares the longest suffix.
Due to theirsimilar nature, we refrain from applying POSTPon HEUR as it would produce no changes.2Unlike German food items, English food items are of-ten multi-word expressions.
Therefore, we assume that forEnglish, instead of analyzing suffixes the usage of the headof a multiword expression (i.e.
chocolate cake) would be anappropriate basis for a similar heuristic.3That is, Diiequals to the sum of the ith row.PLAIN +POSTPConfiguration graph Acc Prec Rec F1 Acc Prec Rec F1UNSUP X 46.2 43.1 35.7 36.0 56.1 41.0 42.5 38.4HEUR (plain) 25.5 87.9 32.2 42.9 N/A N/A N/A N/AHEUR X 56.4 73.6 52.1 54.7 68.7 72.3 64.3 60.7PAT-Top1 X 52.4 60.2 51.2 52.5 64.5 58.2 62.9 57.4PAT-Top5 X 61.1 70.7 61.9 64.4 74.5 67.9 76.0 69.7PAT-Top10 X 60.2 69.6 60.5 62.2 73.4 66.7 74.2 67.31-PROTO X 58.0 68.0 58.0 59.5 70.2 64.1 71.0 63.85-PROTO X 64.5 76.6 63.7 68.6 78.6 73.8 78.5 75.210-PROTO X 65.8 79.0 65.5 71.0 80.2 75.9 80.6 77.7GermaNet (plain) 52.1 94.0 52.0 65.7 75.4 73.2 75.0 72.4GermaNet X 68.3 84.7 63.4 71.6 82.7 81.8 77.7 79.1Table 5: Comparison of different food-type classi-fiers (graph indicates graph-based optimization).4 ExperimentsWe report precision, recall and F-score and accu-racy.4 For precision, recall and F-score, we list themacro-averaged score.4.1 Evaluation of Food Categorization4.1.1 Detection of Food TypesTable 5 compares different classifiers and configu-rations for the prediction of food types (against thegold standard from Table 1).
Apart from the pre-viously described baselines, we consider n man-ually selected prototypes (n-PROTO) and the topn food items produced by Hearst-patterns (PAT-Topn) as seeds for graph-based optimization.
Thetable shows that the semi-supervised graph-basedapproach with these seeds outperforms the base-lines UNSUP and HEUR.
Only as few as 5prototypical seeds (per category) are required toobtain performance that is even better than us-ing plain GermaNet.
The table also shows thatpost-processing (with our suffix-heuristics) con-sistently improves performance.
Manually choos-ing prototypes is more effective than instantiatingseeds via Hearst-patterns.
The quality of the out-put of Hearst-patterns degrades from top 10 on-wards.
However, considering that PAT-Topn doesnot include any manual intervention, it alreadyproduces decent results.
Finally, even GermaNetcan be effectively used as seeds.4.1.2 Detection of DishesTable 6 compares different classifiers for the de-tection of dishes (against the gold standard fromTable 2).
Dishes and atomic food items are very4All manually labeled resources are available at:www.lsv.uni-saarland.de/personalPages/michael/relFood.html676PLAIN +POSTPConfiguration graph Acc Prec Rec F1 Acc Prec Rec F1UNSUP X 54.5 59.6 40.2 37.3 67.9 59.0 50.0 40.6HEUR (plain) 74.1 84.3 59.9 58.6 N/A N/A N/A N/APAT-Top25 X 59.7 72.2 54.6 61.9 74.1 70.1 67.6 68.4PAT-Top50 X 60.9 74.4 55.6 63.1 75.9 72.7 69.2 70.3PAT-Top100 X 62.7 77.6 57.2 65.2 78.4 76.5 71.5 73.0PAT-Top250 X 59.6 71.8 55.1 62.2 74.2 70.3 68.7 69.3RAND-25 X 61.4 77.1 54.3 61.8 76.1 74.4 67.1 68.4RAND-50 X 62.6 76.3 60.1 67.2 77.2 74.0 76.8 74.4RAND-100 X 66.5 82.7 63.0 71.3 83.0 80.8 79.5 80.1GermaNet (plain) 49.5 81.3 46.5 59.3 79.0 75.9 75.5 75.7GermaNet X 60.8 79.4 51.3 57.6 75.9 78.2 64.4 65.4Table 6: Comparison of different classifiers dis-tinguishing between dishes and atomic food items(graph indicates graph-based optimization).PLAIN +POSTPConfiguration graph Acc Prec Rec F1 Acc Prec Rec F1PAT-Top100 (plain) 9.5 89.5 10.5 18.6 63.6 61.5 63.5 61.3PAT-Top100 X 62.7 77.6 57.2 65.2 78.4 76.5 71.5 73.0RAND-100 (plain) 10.6 100.0 12.2 21.4 70.2 69.7 69.0 69.0RAND-100 X 66.5 82.7 63.0 71.3 83.0 80.8 79.5 80.1Table 7: Impact of graph-based optimization(graph) for the detection of dishes.heterogeneous classes which is why more seedsare required for initialization.
This means thatwe cannot look for prototypes.
For simplicity,we resorted to randomly sample seeds from ourgold standard (RAND-n).
For HEUR, we couldnot find a small and intuitive set of suffixes thatare shared by many atomic food types, thereforewe considered all food types from our vocabularywhose suffix did not match a typical dish suffix asatomic.
As this leaves no unspecified food items inour vocabulary, we cannot use the output of HEURas seeds for graph-based optimization.In contrast to the previous experiment, HEUR isa more robust baseline.
But again, post-processingmostly improves performance, and patterns are notas good as manual (random) seeds yet the formerare notably better than HEUR w.r.t.
F-Score.
Un-like in the food-type classification, graph-basedoptimization applied on GermaNet does not resultin some improvement.
We assume that the preci-sion of plain GermaNet with 81.3% is too low.5Since GermaNet cannot effectively be used asseeds for the graph-based optimization and post-processing has already a strong positive effect, wemay wonder how effective the actual graph-based5For other seeds for which it worked, we usually mea-sured a precision of 90% or higher.optimization is for this classification task.
Af-ter all, significantly more seeds are required forthis classification task than for the previous task,so we need to show that it is not the mere seeds(+post-processing) that are required for a reason-able categorization.
Table 7 examines two keyconfigurations with and without graph-based op-timization.
It shows that also for this classificationtask, graph-based optimization produces a catego-rization superior to the mere seeds.
Moreover, thesuffix-based post-processing is complementary tothe improvement by the graph-based optimization.4.1.3 Comparison of Initialization MethodsTable 8 compares for each food type 5 manuallyselected prototypical seeds (i.e.
5-PROTO) andthe 5 food items most frequently been observedwith patthearst(Table 4).
While the manually cho-sen seeds represent the spectrum of food itemswithin each particular class (e.g.
for STARCH,some type of pasta, rice and potato was chosen),it is not possible to enforce such diversity withthe automatically extracted seeds.
However, mostfood items are correct.
Table 9 displays the 10most highly ranked dishes and atomic food itemsextracted with pattdishand pattatom(Table 4).
Un-like the previous task (Table 8), we obtain moreheterogeneous seeds within the same class.4.1.4 Distributional SimilaritySince many recent methods for related tasks, suchas noun classification, are based on so-called dis-tributional similarity (Riloff and Shepherd, 1997;Lin, 1998; Snow et al., 2004; Weeds et al., 2004;Yamada et al., 2009; Huang and Riloff, 2010;Lenci and Benotto, 2012), we also examine this asan alternative representation to the pattern-basedsimilarity graph (Table 3).
We represent each fooditem as a vector which itself is an aggregate ofthe contexts of all mentions of a particular fooditem.
We weighted the individual (context) wordsco-occurring with the food item at a fixed windowsize of 5 words with tf-idf.
We can now applygraph-based optimization on the similarity matrixencoding the cosine similarities between any pos-sible pair of vectors representing two food items.As seeds, we use the best configuration (not em-ploying GermaNet), i.e.
10-PROTO for food typeclassification and RAND-100 for the dish classi-fication.
Since, however, the graph clustering isnot actually necessary, as we have a full similar-ity matrix (rather than a sparse graph) that also al-677Class 5 Manually Chosen Seeds (5-PROTO) 5 Hearst-Pattern Seeds (PAT-Top5)MEAT schnitzel, rissole, bologna, redfish, trout salmon, beef, chicken, turkey hen, poultryBEVERAGE coffee, tea, water, beer, coke coffee, beer, mineral water, lemonade, teaVEGE peas, green salad, tomato, cauliflower, carrot zucchini, lamb?s salad, broccoli, leek, cauliflowerSWEET chocolate, torte, popcorn, apple pie, potato crisps wine gum, marzipan, custard, pancake, biscuitsSPICE pepper, cinnamon, salt, gravy, remoulade cinnamon, laurel, clove, tomato sauce, basilSTARCH spaghetti, basmati rice, white bread, potato, french fries au gratin potatoes, jacket potato, potato, pita, jamMILK yoghurt, gouda, cream cheese, cream, butter milk butter milk, bovine milk, soured milk, goat cheese, sour creamFRUIT banana, apple, strawberries, apricot, orange banana, strawberries, pear, melon, kiwiGRAIN hazelnut, pumpkin seed, rye flour, semolina, wheat sesame, spelt, wheat, millet, barleyFAT margarine, lard, colza oil, spread, butter margarine, lard, resolidified butter, coconut oil, tartarEGG scrambled eggs, fried eggs, chicken egg, omelette, pickled egg yolk, fried eggs, albumen, offal, easter eggTable 8: Comparison of different seed initializations for the food type categorization task (underlinedfood items represent erroneously extracted food items).lows us to compare any arbitrary pair of food itemsdirectly, we also employ a second classifier (forcomparison) based on the nearest neighbour prin-ciple.
We assign each food item the label of themost similar seed food item.Table 10 compares these two classifiers with thebest previous result.
It shows that the pattern-based representation consistently outperforms thedistributional representation.
The former may besparse but it produces high-precision similaritylinks.6 The vector representation, on the otherhand, may not be sparse but it contains a highdegree of noise.
The major problem is that notonly vectors of similar food items, such as chips(fries), potatoes and rice, are similar to each other,but also vectors of different food items that aretypically consumed with each other (e.g.
fishand chips).
This is because of their frequent co-occurrence (as in collocations like fish & chips).Unfortunately, these pairs belong to different foodtypes.
For the dish classification, however, thevector representation is less of a problem.7The distributional representation works betterwith the simple nearest neighbour classifier.
Weassume that graph-based optimization adds furthernoise to the classification since, unlike the nearestneighbour which only calculates the direct similar-ity between two vectors, it also incorporates indi-rect relationships (which may be more error-pronethan the direct relationships) between food items.4.1.5 Do we need a domain-specific corpus?In this section, we want to provide evidence thatapart from the similarity graph and seeds the tex-tual source for the graph, i.e.
our domain-specific6By the label propagation within the graph-based opti-mization, the sparsity problem is also mitigated.7Fish and chips are both atoms, so in the dish classifica-tion, it is no mistake to consider them similar food items.Class 10 Seeds Extracted with Patterns (PAT-Top10)DISH cookies, cake, praline, bread dumpling, jam, biscuit, cheesecake, black-and-whites, onion tart, pasta saladATOM marzipan, flour, potato, olive oil, water, sugar, cream, choco-late, milk, tomatoTable 9: Illustration of seed initialization for thedistinction between dishes and atomic food items.Task Similarity Classifier Acc F1Food Type distributional nearest neighbour 53.4 51.1distributional graph 25.6 25.6pattern-based graph 80.2 77.7Dish distributional nearest neighbour 76.8 75.2distributional graph 71.5 71.2pattern-based graph 83.0 80.1Table 10: Impact of the similarity representation.corpus (chefkoch.de), is also important.
For thatpurpose, we compare our current corpus againstan open-domain corpus.
We consider the Germanversion of Wikipedia since this resource also con-tains encyclopedic knowledge about food items.Table 11 compares the graph-based induction.
Asin the previous section, we only consider the bestprevious configuration.
The table clearly showsthat our domain-specific text corpus is a more ef-fective resource for our purpose than Wikipedia.4.2 Evaluation for Relation ExtractionWe now examine whether automatic food cate-gorization can be harnessed for relation extrac-tion.
The task is to detect instances of the relationtypes SuitsTo, SubstitutedBy and IngredientOf in-troduced Wiegand et al.
(2012b) (repeated in Ta-ble 12) and motivated in Wiegand et al.
(2012a).These relation types are highly relevant for cus-tomer advice/product recommendation.
In partic-ular, SuitsTo and SubstitutedBy are fairly domain-independent relation types.
Customers want to678know which items can be used together (SuitsTo),be it two food items that can be used as a mealor two fashion items that can be worn together.Substitutes are also relevant for situations in whichitem A is out of stock but item B can be offered asan alternative.
Therefore, insights from this workshould carry over to other domains.We randomly extracted 1500 sentences fromour text corpus (?2.1) in which (at least) two fooditems co-occur.
Each food pair mention was man-ually assigned one label.
In addition to the threerelation types from above, we introduce the la-bel Other for cases in which either another rela-tion between the target food items is expressed orthe co-occurrence is co-incidental.
On a subset of200 sentences, we measured a substantial inter-annotation agreement of Cohen?s ?
= 0.67 (Lan-dis and Koch, 1977).We train a supervised classifier and incorporatethe knowledge induced from our domain-specificcorpus as features.
We chose Support Vector Ma-chines with 5-fold cross-validation using SVMlight-multi-class (Joachims, 1999).Table 13 displays all features that we examinefor supervised classification.
Most features arewidely used throughout different NLP tasks.
Onespecial feature brown takes into consideration theoutput of Brown clustering (Brown et al., 1992)which like our graph-based optimization producesa corpus-driven categorization of words.
Simi-lar to UNSUP, this method is unsupervised but itconsiders the entire vocabulary of our text corpusrather than only food items.
Therefore, this in-formation can be considered as a generalizationof all contextual words.
Such type of informa-tion has been shown to be useful for named-entityrecognition (Turian et al., 2010) and relation ex-traction (Plank and Moschitti, 2013).For syntactic parsing, Stanford Parser (Raffertyand Manning, 2008) was used.
For Brown cluster-ing, the SRILM-toolkit (Stolcke, 2002) was used.Following Turian et al.
(2010), we induced 1000clusters (from our domain-specific corpus ?2.1).4.2.1 Why should food categories be helpfulfor relation extraction?All relation types we consider comprise pairs oftwo food items which makes these relation typeslikely to be confused.
Contextual information maybe used for disambiguation but there may also befrequent contexts that are not sufficiently informa-tive.
For example, 25% of the instances of Ingre-PLAIN +POSTPTask Corpus graph Acc F1 Acc F1Food TypeWikipedia X 40.3 49.4 61.4 59.8chefkoch.de X 65.8 71.0 80.2 77.7DishWikipedia X 50.4 53.1 75.4 71.1chefkoch.de X 66.5 71.3 83.0 80.1Table 11: Comparison of Wikipedia and domain-specific corpus as a source for the similarity graph.dientOf follow the lexical pattern food item1withfood item2(1).
However, the same pattern alsocovers 15% of the instances of SuitsTo (2).
(1) We had a stew with red lentils.
(Relation: IngredientOf)(2) We had salmon with broccoli.
(Relation: SuitsTo)The food type information we learned from ourtext corpus might tell us which of the food itemsare dishes.
Only in (1), there is a dish, i.e.
stew.So, one may infer that the presence of dishes isindicative of IngredientOf rather than SuitsTo.food item1and food item2is another ambigu-ous context.
It cannot only be observed with therelation SuitsTo, as in (3) (66% of all instantia-tions of that pattern), but also SubstitutedBy (20%of all mentions of that relation match that pattern),as in (4).
For SuitsTo, two food items that belongto two different classes (e.g.
MEAT and STARCHor MEAT and VEGE) are quite characteristic.
ForSubstitutedBy, the two food items are very often ofthe same category of the Food Guide Pyramid.
(3) I very often eat fish and chips.
(Relation: SuitsTo)(4) For these types of dishes you can offer both Burgundy wine andChampagne.
(Relation: SubstitutedBy)Since the second ambiguous context involvesthe two general relation types SuitsTo and Substi-tutedBy, resolving this ambiguity with automati-cally induced type information has some signifi-cance for other domains.
In particular, for otherlife-style domains, domain-specific type informa-tion could be obtained following our method from?3.1.
The disambiguation rule that two entities ofthe same type imply SubstitutedBy otherwise theyimply SuitsTo should also be widely applicable.4.2.2 ResultsTable 14 displays the performance of the differentfeature sets for relation extraction.
The featuresdesigned from graph-based induction (i.e.
graph)work slightly better than GermaNet.
The perfor-mance of patt is not impressively high.
However,one should consider that patt can be used directlywithout a supervised classifier (as each pattern is679Relation Description Example Freq.
Perc.SuitsTo food items that are typically consumed together My kids love the simple combination of fish fingerswith mashed potatoes.633 42.20SubstitutedBy similar food items commonly consumed in the same situations We usually buy margarine instead of butter.
336 22.40IngredientOf ingredient of a particular dish Falafel is made of chickpeas.
246 16.40Other other relation or co-occurrence of food items are co-incidental On my shopping list, I?ve got bread, cauliflower, ... 285 19.00Table 12: The different relation types and their respective frequency on our dataset.Features Descriptionpatt lexical surface patterns used in Wiegand et al.
(2012b)word bag-of-words features: all words within the sentencebrown features using Brown clustering: all features from word butwords are replaced by induced clusterspos part-of-speech sequence between target food items and tagsof the words immediately preceding and following themsynt path from syntactic parse tree from first target food item tosecond target food itemconj conjunctive features: patt with brown classes of target fooditems; pos sequence with brown classes of target food items;synt with brown classes of target food itemsgraph semantic food information induced by graph optimization(config.
: 10-PROTO(+POSTP) and RAND-100(+POSTP))germanet semantic food information derived from (plain) GermaNetTable 13: Description of the feature set.designed for a particular relation type, one canread off from the matching pattern which class ispredicted).
word is slightly better but, unlike patt,it is dependent on supervised learning.The only feature that individually manages tosignificantly outperform word is graph.
The tra-ditional features (i.e.
pos, synt and brown) onlyproduce some mild improvement when addedjointly to word along some conjunctive fea-tures.
When graph is added to this feature set(i.e.
word+patt+pos+synt+brown+conj), we ob-tain another significant improvement.
In con-clusion, the information we induced from ourdomain-specific corpus cannot be obtained byother NLP-features, including other state-of-the-art induction methods such as Brown clustering.5 Related WorkWhile many of the previous works on noun catego-rization also address the task of hypernym classifi-cation (Hearst, 1992; Caraballo, 1999; Widdows,2003; Kozareva et al., 2008; Huang and Riloff,2010; Lenci and Benotto, 2012) and some includeexamples involving food items (Widdows andDorow, 2002; Cederberg and Widdows, 2003),only van Hage et al.
(2005) and van Hage et al.
(2006) specifically focus on the classification offood items.
van Hage et al.
(2005) deal with on-tology mapping whereas van Hage et al.
(2006)explore part-whole relations.Features Acc Prec Rec F1germanet 45.3 41.3 37.2 37.3graph 46.0 39.4 39.7 38.6patt 59.8 49.8 41.1 38.7word 60.1 56.9 54.5 55.1word+patt 60.3 57.3 54.9 55.5word+brown 59.5 56.1 54.6 54.9word+synt 60.3 57.7 55.4 56.0word+pos 59.8 56.6 54.6 55.1word+germanet 61.3 58.6 56.0 56.7word+graph 62.9 59.2 57.6 58.1?word+patt+brown+synt+pos 60.4 57.3 56.2 56.5word+patt+brown+synt+pos+conj 61.7 59.0 57.8 58.2?word+patt+brown+synt+pos+conj+germanet 63.1 60.2 58.6 59.1?word+patt+brown+synt+pos+conj+graph 64.7 62.1 60.3 60.9?
?statistical significance testing (paired t-test): better than word ?
at p < 0.1/?
at p < 0.05; ?
better than word+patt+brown+synt+pos+conj at p < 0.05Table 14: Comparison of various features (Ta-ble 13) for (unrestricted) relation extraction.The task of data-driven lexicon expansion hasalso been explored before (Kanayama and Na-sukawa, 2006; Das and Smith, 2012), however,our paper presents the first attempt to carry outa comprehensive categorization for the food do-main.
For the first time, we also show that typeinformation can effectively improve the extractionof very common relations.
For the twitter domain,the usage of type information based on cluster-ing has already been found effective for supervisedlearning (Bergsma et al., 2013).6 ConclusionWe presented an induction method to assign se-mantic information to food items.
We consideredtwo types of categorizations being food-type infor-mation and information about whether a food itemis composite or not.
The categorization is inducedby graph-based optimization applied on a largeunlabeled domain-specific text corpus.
We pro-duce categorizations that outperform a manuallycompiled resource.
The usage of such a domain-specific corpus based on a pattern-based represen-tation is vital and largely outperforms other textcorpora or a distributional representation.
The in-duced knowledge improves relation extraction.680AcknowledgementsThis work was performed in the context of the Software-Cluster project SINNODIUM.
Michael Wiegand was fundedby the German Federal Ministry of Education and Research(BMBF) under grant no.
01IC12SO1X.
Benjamin Roth isa recipient of the Google Europe Fellowship in Natural Lan-guage Processing, and this research is supported in part bythis Google Fellowship.
The authors would like to thankStephanie Ko?ser for annotating the dataset presented in thispaper.ReferencesMikhail Belkin and Partha Niyogi.
2004.
Semi-supervised learning on Riemannian manifolds.
Ma-chine Learning, 56(1-3):209?239.Shane Bergsma, Mark Dredze, Benjamin VanDurme, Theresa Wilson, and David Yarowsky.2013.
Broadly Improving User Classificationvia Communication-Based Name and LocationClustering on Twitter.
In Proceedings of the HumanLanguage Technology Conference of the NorthAmerican Chapter of the ACL (HLT/NAACL), pages1010?1019, Atlanta, GA, USA.Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18:467?479.Sharon A. Caraballo.
1999.
Automatic constructionof a hypernym-labeled noun hierarchy from text.
InProceedings of the Annual Meeting of the Associ-ation for Computational Linguistics (ACL), pages120?126, College Park, MD, USA.Scott Cederberg and Dominic Widdows.
2003.
Us-ing LSA and Noun Coordination Information to Im-prove the Precision and Recall of Automatic Hy-ponymy Extraction.
In Proceedings of the Confer-ence on Computational Natural Language Learn-ing (CoNLL), pages 111?118, Edmonton, Alberta,Canada.Dipanjan Das and Noah A. Smith.
2012.
Graph-Based Lexicon Expansion with Sparsity-InducingPenalties.
In Proceedings of the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the ACL (HLT/NAACL), pages 677?687, Montre?al, Quebec, Canada.Birgit Hamp and Helmut Feldweg.
1997.
GermaNet -a Lexical-Semantic Net for German.
In Proceedingsof ACL workshop Automatic Information Extractionand Building of Lexical Semantic Resources for NLPApplications, pages 9?15, Madrid, Spain.Marti A. Hearst.
1992.
Automatic Acquisition ofHyponyms from Large Text Corpora.
In Pro-ceedings of the International Conference on Com-putational Linguistics (COLING), pages 539?545,Nantes, France.Ruihong Huang and Ellen Riloff.
2010.
InducingDomain-specific Semantic Class Taggers from (al-most) Nothing.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 275?285, Uppsala, Sweden.Thorsten Joachims.
1999.
Making Large-Scale SVMLearning Practical.
In B. Scho?lkopf, C. Burges,and A. Smola, editors, Advances in Kernel Meth-ods - Support Vector Learning, pages 169?184.
MITPress.Hiroshi Kanayama and Tetsuya Nasukawa.
2006.Fully Automatic Lexicon Expansion for Domain-oriented Sentiment Analysis.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 355?363, Syd-ney, Australia.Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.2008.
Semantic Class Learning from the Webwith Hyponym Pattern Linkage Graphs.
In Pro-ceedings of the Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 1048?1056, Columbus, OH, USA.J.
Richard Landis and Gary G. Koch.
1977.
TheMeasurement of Observer Agreement for Categor-ical Data.
Biometrics, 33(1):159?174.Alessandro Lenci and Guilia Benotto.
2012.
Identi-fying hypernyms in distributional semantic spaces.In Proceedings of the Joint Conference on Lexicaland Computational Semantics (*SEM), pages 75?79, Montre?al, Quebec, Canada.Dekang Lin.
1998.
Automatic Retrieval and Cluster-ing of Similar Words.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics and International Conference on Computa-tional Linguistics (ACL/COLING), pages 768?774,Montreal, Quebec, Canada.George Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine Miller.
1990.Introduction to WordNet: An On-line LexicalDatabase.
International Journal of Lexicography,3:235?244.Barbara Plank and Alessandro Moschitti.
2013.
Em-bedding Semantic Similarity in Tree Kernels for Do-main Adapation of Relation Extraction.
In Pro-ceedings of the Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 1498?1507, Sofia, Bulgaria.Anna Rafferty and Christopher D. Manning.
2008.Parsing Three German Treebanks: Lexicalized andUnlexicalized Baselines.
In Proceedings of the ACLWorkshop on Parsing German (PaGe), pages 40?46,Columbus, OH, USA.681Ellen Riloff and Jessica Shepherd.
1997.
ACorpus-Based Approach for Building SemanticLexicons.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 117?124, Providence, RI, USA.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2004.Learning Syntactic Patterns for Automatic Hyper-nym Discovery.
In Advances in Neural Informa-tion Processing Systems (NIPS), Vancouver, BritishColumbia, Canada.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Proceedings of the In-ternational Conference on Spoken Language Pro-cessing (ICSLP), pages 901?904, Denver, CO, USA.Joseph Turian, Lev Ratinov, and Yoshua Bengio.2010.
Word Representations: A Simple and GeneralMethod for Semi-supervised Learning.
In Proceed-ings of the Annual Meeting of the Association forComputational Linguistics (ACL), pages 384?394,Uppsala, Sweden.Human Nutrition Information Service U.S. Depart-ment of Agriculture.
1992.
The Food Guide Pyra-mid.
Home and Garden Bulletin 252, Washington,D.C., USA.Willem Robert van Hage, Sophia Katrenko, and GuusSchreiber.
2005.
A Method to Combine Linguis-tic Ontology-Mapping Techniques.
In Proceedingsof International Semantic Web Conference (ISWC),pages 732 ?
744, Galway, Ireland.
Springer.Willem Robert van Hage, Hap Kolb, and GuusSchreiber.
2006.
A Method for Learning Part-Whole Relations.
In Proceedings of InternationalSemantic Web Conference (ISWC), pages 723 ?
735,Athens, GA, USA.
Springer.Ulrike von Luxburg.
2007.
A Tutorial on SpectralClustering.
Statistics and Computing, 17:395?416.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising Measures of Lexical DistributionalSimilarity.
In Proceedings of the International Con-ference on Computational Linguistics (COLING),pages 1015?1021, Geneva, Switzerland.Dominic Widdows and Beate Dorow.
2002.
AGraph Model for Unsupervised Lexical Acquisition.In Proceedings of the International Conference onComputational Linguistics (COLING), pages 1093?1099, Taipei, Taiwan.Dominic Widdows.
2003.
Unsupervised methods fordeveloping taxonomies by combining syntactic andstatistical information.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the ACL (HLT/NAACL), pages197?204, Edmonton, Alberta, Canada.Michael Wiegand, Benjamin Roth, and DietrichKlakow.
2012a.
Knowledge Acquisition with Nat-ural Language Processing in the Food Domain: Po-tential and Challenges.
In Proceedings of the ECAI-Workshop on Cooking with Computers (CWC),pages 46?51, Montpellier, France.Michael Wiegand, Benjamin Roth, and DietrichKlakow.
2012b.
Web-based Relation Extractionfor the Food Domain.
In Proceedings of the In-ternational Conference on Applications of Natu-ral Language Processing to Information Systems(NLDB), pages 222?227, Groningen, the Nether-lands.
Springer.Michael Wiegand, Benjamin Roth, Eva Lasarcyk,Stephanie Ko?ser, and Dietrich Klakow.
2012c.
AGold Standard for Relation Extraction in the FoodDomain.
In Proceedings of the Conference onLanguage Resources and Evaluation (LREC), pages507?514, Istanbul, Turkey.Ichiro Yamada, Kentaro Torisawa, Jun?ichi Kazama,Kow Kuroda, Masaki Murata, Stijn De Saeger, Fran-cis Bond, and Asuka Sumida.
2009.
Hypernym Dis-covery Based on Distributional Similarity and Hi-erarchical Structures.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 929?927, Singapore.Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,Jason Weston, and Bernhard Scho?lkopf.
2004.Learning with Local and Global Consistency.
InAdvances in Neural Information Processing Systems(NIPS), Vancouver and Whistler, British Columbia,Canada.682
