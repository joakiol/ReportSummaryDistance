Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 32?39,Sydney, July 2006. c?2006 Association for Computational LinguisticsUsing Scenario Knowledge in Automatic Question AnsweringSanda Harabagiu and Andrew HicklLanguage Computer Corporation1701 North Collins BoulevardRichardson, Texas 75080 USAsanda@languagecomputer.comAbstractThis paper describes a novel frameworkfor using scenario knowledge in open-domain Question Answering (Q/A) appli-cations that uses a state-of-the-art textualentailment system (Hickl et al, 2006b) inorder to discover textual information rele-vant to the set of topics associated with ascenario description.
An intrinsic and anextrinsic evaluation of this method is pre-sented in the context of an automatic Q/Asystem and results from several user sce-narios are discussed.1 IntroductionUsers of today?s automatic question-answering(Q/A) systems generally have complex informa-tion needs that cannot be satisfied by asking singlequestions in isolation.
When users interact withQ/A systems, they often formulate sets of queriesthat they believe will help them gather the infor-mation that needed to perform one or more spe-cific tasks.
While human users are generally ableto identify their information needs independently,the information needs of organizations are oftenpresented in the form of short prose descriptions?
known as scenarios ?
which outline the rangeof knowledge sought by a customer in order toachieve a specific outcome or to accomplish a par-ticular task.
(An example of one scenario is pre-sented in Figure 1.
)Recent work in Q/A has sought to use in-formation derived from these kinds of scenar-ios in order to retrieve sets of answers that aremore relevant ?
and responsive ?
to a customer?sinformation needs.
While (Harabagiu et al,2005) used topic signatures (Lin and Hovy, 2000;Scenario DescriptionThe customer has commissioned a research project looking at theimpact of the outsourcing of American jobs on the United States?relationship with India.
After conducting research on U.S.companies currently doing business in India, the customer wantsto know why American corporations have sought to outsource jobsto India, the types of economic advantages that American companiescould gain from relocating to India, and the kinds of economic orpolitical inducements that India has offered to American companieslooking to outsource jobs there.
The customer is not interestedin demographic information on Indian employees of American firms.Table 1: Example of a User Scenario.Harabagiu, 2004) computed automatically fromcollections of documents relevant to a scenario inorder to approximate the semantic content of ascenario, (Narayanan and Harabagiu, 2004) em-ployed formal models of the interrelated events,actions, states, and relations implicit to a sce-nario in order to produce fine-grained, context-sensitive inferences that could be used to answerquestions.
Scenario knowledge was also includedin the form of axiomatic logic transformation de-veloped in (Moldovan et al, 2003).
Under thisapproach, information extracted from the scenarionarrative is converted to logical axioms that canused in conjunction with a logic prover in orderjustify answers returned for questions.In this paper, we propose that scenario-relevantpassages in natural language texts can be identifiedby recognizing a semantic relation, known as con-textual entailment (CE), that exists between a textpassage and one of a set of subquestions that areconventionally implied by a scenario.
Under thismodel, we expect that a scenario S can be consid-ered to contextually entail a passage t, when thereexists at least one subquestion q derived from Sthat textually entails the passage t. We show thatby using a state-of-the-art textual entailment sys-tem (Hickl et al, 2006b), we can provide Q/A sys-tems with another mechanism for approximatingthe inference between questions and relevant an-swers.
We show how each of these cases of con-32textual entailment can be computed and how it canbe used in the intrinsic and extrinsic evaluation ofa Q/A system.The remainder of the paper is organized in thefollowing way.
Section 2 introduces our notion ofcontextual entailment and provides a frameworkfor recognizing instances of CE between scenar-ios and both questions and answers.
Section 3 de-scribes the textual entailment system used at thecore of our CE system.
Sections 4 and 5 describeseparate frameworks for intrinsically and extrinsi-cally evaluating the impact of CE on current Q/Asystems.
Section 6 presents results from our evalu-ations, and Section 7 summarizes our conclusions2 Recognizing Contextual EntailmentWe define contextual entailment (CE) as a direc-tional relation that exists between a text passage tand one of a set of implicit subquestions q that canbe derived from a user?s interpretation of a sce-nario.
Informally, we consider that a scenario Scontextually entails a passage t when there existsat least one subquestion q implied by S that can beconsidered to entail t.We expect that the meaning of an information-seeking scenario S can be represented as a ques-tion under discussion (QUD) QS , which denotes apartially-ordered set of subquestions (q ?
QS) thatrepresent the entire set of questions that could po-tentially be asked in order to gather informationrelevant to S. Taken together, we expect thesesubquestions to represent the widest possible con-strual of a user?s information need given S.Entailment?ContextualEntailment?ContextualCASE 1 CASE 2 CASE 3AnswerQuestion QuestionUser ScenarioEntailment?AnswerQuestionUser ScenarioEntailment?AnswerUser ScenarioEntailment?ContextualEntailment?ContextualEntailment?Figure 1: Three types of Contextual EntailmentWe believe the set of subquestions implied byQS can be used to test whether a text passage isrelevant to S. Since the formal answerhood re-lation between a question and its answer(s) canbe cast in terms of (logical) entailment (Groe-nendijk, 1999; Lewis, 1988), we believe that sys-tems for recognizing textual entailment (Dagan etal., 2005) could be used in order to identify thosetext passages that should be considered when gath-ering information related to a scenario.
Based onthese assumptions, we expect that the set of textpassages that are textually entailed by subques-tions derived from a scenario represent informa-tion that is more likely to be relevant to the overalltopic of the scenario as a whole.We expect that there are three types of contex-tual entailment relationships that could prove use-ful for automatic Q/A systems.
First, as illustratedin Case 1 in 1, CE could exist between a scenarioand one of the set of answers returned by a Q/Asystem in response to a user?s question.
Second,as in Case 2, CE could be established directly be-tween a scenario and the question asked by theuser.
Finally, as in Case 3, CE could be establishedboth between a scenario and a user?s question aswell as between a scenario and one of the answersreturned by the Q/A system for that question.Figure 2 provides examples of each of thesethree types of contextual entailment using the sce-nario presented in Figure 1.CASE 1:Scenario:companies could gain from relocating to Indiathe types of economic advantages that AmericanAnswer:Question:GE and Dell have reported earnings growth afteroutsourcing jobs to both Indonesia and IndiaWhat U.S. companies are outsourcing jobs toIndonesia?Textual Entailment Contextual EntailmentScenario:companies could gain from relocating to Indiathe types of economic advantages that AmericanAnswer:Question:Scenario:companies could gain from relocating to Indiathe types of economic advantages that AmericanAnswer:Question:certain types of jobs to India?How could U.S. companies profit from movingHow could U.S. companies benefit by moving jobsto India?Outsourcing jobs to India saved the carrier $25million, enabling it to turn a profit for the first time.Despite public opposition to outsourcing jobs toIndia, political support has never been higher.CASE 3:CASE 2: S does not entail A, Q entails A, S entails QS entails A, Q entails A, S does not entail QS entails A, Q entails A, S entails QFigure 2: Examples of Contextual Entailment.In Case 1, the scenario textually entails themeaning of the answer passage, as earningsgrowth from outsourcing necessarily representsone of the types of economic advantages that canbe derived from outsourcing.
However, the sce-nario cannot be seen as entailing the user?s ques-tion, as the user?s interest in job outsourcing inIndonesia cannot be interpreted as being part ofthe topics that are associated with the scenario.In this case, recognition of contextual entailmentwould allow systems to be sensitive to the types of33scenario-relevant information that is encountered?
even when the user asks questions that are notentailed by the scenario itself.
We expect that thistype of contextual entailment would allow systemsto identify scenario-relevant knowledge through-out a user?s interaction with a system, regardlessof topic of a user?s last query.In Case 2, the user?s question is entailed bythe scenario, but no corresponding entailment re-lationship can be established between the scenarioand the answer passage identified by the Q/A sys-tem as an answer to the question.
While politicalsupport may be interpretable as one of the benefitsrealized by companies that outsource, it cannot beunderstood as one of the economic advantages ofoutsourcing.
Here, recognizing that contextual en-tailment could not be established between the sce-nario and the answer ?
but could be establishedbetween the scenario and the question ?
could beused to signal the Q/A system to consider addi-tional answers before moving on to the user?s nextquestion.
By identifying contextual entailmentrelationships between answers and elements in ascenario, systems could perform valuable forms ofanswer validation that could be used to select onlythe most relevant answers for a user?s considera-tion.Finally, in Case 3, entailment relationships existbetween the scenario and both the user?s questionand the returned answer, as saving $25 million canbe considered to be both an economic advantageand one of the ways that companies prot fromoutsourcing.
In this case, the establishment of con-textual entailment could be used to inform topicmodels that could be used to identify and extractother similarly relevant passages for the user.In order to capture these three types of CE re-lationships, we developed the architecture for rec-ognizing contextual entailment illustrated in Fig-ure 3.This architecture includes three basic types ofmodules: (1) a Context Discovery module, whichidentifies passages relevant to the concepts men-tioned in a scenario, (2) a Textual Entailment mod-ule, which recognizes implicational relationshipsbetween passages, and (3) a Entailment Merg-ing module, which ranks relevant passages ac-cording to their relevance to the scenario itself.In Context Discovery, document retrieval queriesare first extracted from each sentence found in ascenario.
Once a set of documents has been as-UserScenarioQuery ExtractionDocumentsRelevantDocumentsIrelevantTopic SignaturesSignatureAnswerSignatureAnswerSignatureAnswerSignatureAnswerSignatureAnswerEntailmentTextualQuestion/AnswerScenario ContextMerging Textual Entailment ResultsContextual Entailment Decision / ConfidenceFigure 3: Contextual Entailment Architecture.sembled, topic signatures (Lin and Hovy, 2000;Harabagiu 2004) are computed which identify theset of topic-relevant concepts ?
and relations be-tween concepts ?
that are found in the relevant setof documents.
Weights associated with each set oftopic signatures are then used to extract a set ofrelevant sentences ?
referred to as topic answers ?from each relevant document.
Once a set of topicanswers have been identified, each topic answer ispaired with a question submitted by a user and sentto the Textual Entailment system described in Sec-tion 2.
Topic answers that are deemed to be pos-itive entailments of the user question are assigneda confidence value by the TE system and are thensent to a Entailment Merging module, which useslogistic regression in order to rank passages ac-cording to their expected relevance to the user sce-nario.
Here, logistic regression is used to find a setof coefficients bj (where 0 ?
j ?
p) in order to fita variable x to a logistic transformation of a prob-ability q.logit(q) = log q1?
q = b0 +p?j=1bjxj + eWe believe that since logistic regression uses amaximum likelihood method, it is a suitable tech-nique for normalizing across range of confidencevalues output by the TE system.34CoreferenceCoreferenceNEAliasingConceptTextualInput 1TextualInput 2Lexical AlignmentParaphrase AcquisitionAlignment ModuleWWWTrainingCorporaClassifierYESNOFeaturesAlignmentDependencyFeaturesParaphraseFeaturesSemantic/PragmaticFeaturesFeature ExtractionClassification ModuleLexico?SemanticPoS/ NERSynonyms/AntonymsNormalizationSyntacticSemanticTemporalParsingModality Detection Speech Act RecognitionPragmaticsFactivity Detection Belief RecognitionPreprocessingFigure 4: Textual Entailment Architecture.3 Recognizing Textual EntailmentRecent work in computational seman-tics (Haghighi et al, 2005; Hickl et al, 2006b;MacCartney et al, 2006) has demonstrated theviability of supervised machine learning-basedapproaches to the recognition of textual en-tailment (TE).
While these approaches havenot incorporated the forms of structured worldknowledge featured in many logic-based TE sys-tems, classification-based approaches have beenconsistently among the top-performing systemsin both the 2005 and 2006 PASCAL RecognizingTextual Entailment (RTE) Challenges (Dagan etal., 2005), with the best systems (such as (Hicklet al, 2006b)) correctly identifying instances oftextual entailment more than 75% of the time.The architecture of our TE system is presentedin Figure 4.1 Pairs of texts are initially sent to aPreprocessing Module, which performs syntacticand semantic parsing of each sentence, resolvescoreference, and annotates entities and predicateswith a wide range of lexico-semantic and prag-1For more information on the TE system described in thissection, please see (Hickl et al, 2006b) and (Harabagiu andHickl, 2006).matic information, including named entity infor-mation, synonymy and antonymy information, andpolarity and modality information.Once preprocessing is complete, texts are thensent to an Alignment Module, which uses lexi-cal alignment module in conjunction with a para-phrase acquisition module in order to determinethe likelihood that pairs of elements selected fromeach sentence contain corresponding informationthat could be used in recognizing textual entail-ment.
Lexical Alignment is performed using aMaximum Entropy-based classifier which com-putes an alignment probability p(a) equal to thelikelihood that a term selected from one text cor-responds to an element selected from another text.Once these pairs of corresponding elements areidentified, alignment information is then used inorder to extract portions of texts that could berelated via one or more phrase-level alternationsor ?paraphrases?.
In order to acquire these al-ternations, the most likely pairs of aligned ele-ments were then sent to a Paraphrase Acquisitionmodule, which extracts sentences that contain in-stances of both aligned elements from the WorldWide Web.Output from these two modules are then com-bined in a final Classication Module, which usesfeatures derived from (1) lexico-semantic prop-erties, (2) semantic dependencies, (3) predicate-based features (including polarity and modality),(4) lexical alignment, and (5) paraphrase acquisi-tion in order learn a decision tree classifier capableof determining whether an entailment relationshipexists for a pair of texts.4 Intrinsic Evaluation of ContextualEntailmentSince we believe CE is intrinsic to the Q/A task,we have evaluated the impact of contextual en-tailment on our Q/A system in two ways.
First,we compared the quality of the answers produced,with and without contextual entailment.
Second,we evaluated the quality of the ranked list of para-graphs against the list of entailed paragraphs iden-tified by the CE system and the set of relevant an-swers identified by the Q/A system.
This compar-ison was performed for each of the three cases ofentailment presented in Figure 2.We have explored the impact of knowledgederived from the user scenario through differentforms of contextual entailment by comparing the35Topic Signatures Relevant AnswersGenerationAUTO?QUABList of QuestionsQuestionSimilarityENTAILMENTCONTEXTUALENTAILMENTCONTEXTUALProcessingQuestionModule(QP)KeywordsPassageRetrievalModule(PR)Ranked List of ParagraphsList of Entailed ParagraphsModuleAnswerProcessing(AP)QuestionUserScenarioAnswerSet1AnswerSet2ModuleAnswerProcessing(AP)AnswerSet3DocumentsFigure 5: Framework for Intrinsic Evaluation of Contextual Entailment in Q/A.results of such knowledge integration in a Q/Asystem against the usage of scenario knowledgereported in (Harabagiu et al, 2005).Topic signatures, derived from the user scenarioand from documents are used to establish text pas-sages that are relevant to the scenario, and thusconstitute relevant answers.
For each such an-swer, one or multiple questions were built auto-matically with the method reported in (Harabagiuet al, 2005).
When a new question was asked, itssimilarity to any of the questions generated basedon the knowledge of the scenario is computed, andits corresponding answer is provided as an answerfor the current question as well.
Since the ques-tions are ranked by similarity to the current ques-tion, the answers are also ranked and produce theAnswer Set1 illustrated in Figure 5.When a Q/A system is used for answering thequestion, the scenario knowledge can be used intwo ways.
First, the keywords extracted by theQuestion Processing module can be enhanced withconcepts from the topic signatures to produce aranked list of paragraphs, resulting from the Pas-sage Retrieval Module.
These passages togetherwith the question and the user scenario are usedin one of the contextual entailment configurationsto derive a list of entailed paragraphs from whichthe Answer Processing module can extract the an-swer set 2 illustrated in Figure 5.
In another way,the ranked list of paragraphs is passed to the An-swer Processing module, which provides a set ofranked answers to the contextual entailment con-figurations to derive a list of entailed answers, rep-resented as answer set 3 in Figure 5.
We evalu-ate the quality of each set of answers, and for theanswer set 2 and 3, we produce separate evalua-tion for each configuration for the contextual en-tailment.5 Extrinsic Evaluation of ContextualEntailmentQuestions asked in response to a user scenariotend to be complex.
Following work in (Hicklet al, 2004), we believe complex questions canbe answered in one of two ways: either by(1) using techniques (similar to the ones pro-posed in (Harabagiu et al, 2006)) for automati-cally decomposing complex questions into sets ofinformationally-simpler questions, or by (2) us-ing a multi-document summarization (MDS) sys-tem (such as the one described in (Lacatusu et al,2006)) in order to assemble a ranked list of pas-sages which contain information that is potentiallyrelevant to the user?s question.First, we expect that contextual entailment canbe used to select the decompositions of a complexquestion that are most closely related to a scenario.By assigning more confidence to the decomposi-tions that are contextually entailed by a scenario,systems can select a set of answers that are rel-evant to both the user scenario ?
and the user?squestion.
In contrast, contextual entailment can beused in conjunction with the output of a MDS sys-tem: once a summary has been constructed fromthe passages retrieved for a query, contextual en-36UserScenarioQuestionKeyword ExtractionSystemQuestion AnsweringEntailmentContextualQuestion Decomposition DocumentsEntailmentContextualMulti?DocumentSummarizationSystemCandidateSub?QuestionsQueryRelevantDocumentsCandidate AnswersSummaryAnswersRankedFigure 6: Framework for Extrinsic Evaluation of Contextual Entailment in Q/A.tailment can be used to select the most relevantsentences from the summary.The architecture of this proposed system is il-lustrated in Figure 6.When using contextual entailment for selectingquestion decompositions, we rely on the methodreported in (Harabagiu et al, 2006) which gener-ates questions by using a random walk on a bipar-tite graph of salient relations and answers.
In thiscase, the recognition of entailment between ques-tions operates as a filter, forcing questions that arenot entailed by any of the signature answers de-rived from the scenario context (see Figure 3) tobe dropped from consideration.When entailment information is used for re-ranking candidate answers, the summary is addedto the scenario context, each summary sentencebeing treated akin to a signature answer.
We be-lieve that the summary contains the most informa-tive information from both the question and thescenario, since the queries that produced it orig-inated both in the question and in the scenario.
Byadding summary sentences to the scenario context,we have introduced a new dimension to the pro-cessing of the scenario.
The contextual entailmentis based on the textual entailments between any ofthe texts from the scenario context and any of thecandidate answers.6 Experimental ResultsIn this section, we present preliminary results fromfour sets of experiments which show how forms oftextual ?
and contextual ?
entailment can enhancethe quality of answers returned by an automaticQ/A system.Questions used in these experiments were gath-ered from human interactions with the interactiveQ/A system described in (Hickl et al, 2006a).
Atotal of 6 users were asked to spend approximately90 minutes gathering information related to threedifferent information-gathering scenarios similarto the one in Table 1.
Each user researched twodifferent scenarios, resulting in a total of 12 to-tal research sessions.
Once all research sessionswere completed, linguistically well-formed ques-tions were extracted from the system logs for eachsession for use in our experiments; ungrammaticalquestions or keyword-style queries were not usedin our experiments.
Table 2 presents a breakdownof the total number of questions collected for eachof the 6 scenarios.Scenario Name Users Total Qs Avg.
Q/Session ?2S1 .
India Outsourcing 4 45 11.25 2.50S2 .
Chinese WMD Proliferation 4 38 9.50 6.45S3 .
Libyan Bioweapons Programs 4 63 15.75 2.22Total 12 146 12.17 1.23Table 2: Questions Collected from User Experi-ments.In order to evaluate the performance of our Q/Asystem under each of the experimental conditionsdescribed below, questions were re-submitted tothe Q/A system and the top 10 answers were re-trieved.
Two annotators were then tasked withjudging the correctness ?
or ?relevance?
?
of eachreturned answer to the original question.
If the an-swer could be considered to provide either a com-plete or partial answer to the original question, itwas marked as correct; if the answer contained in-formation that could not be construed as an answerto the original question, it was marked as incor-rect.6.1 Textual EntailmentFollowing (Harabagiu and Hickl, 2006), we usedTE information in order to filter answers identifiedby the Q/A system that were not entailed by theuser?s original question.
After filtering, the top-ranked entailed answer (as determined by the Q/Asystem) was returned as the system?s answer to thequestion.
Results from both a baseline version anda TE-enhanced version of our Q/A system are pre-sented in Table 4.Although no information from the scenario wasused in this experiment, performance of the Q/A37S1 S2 S3 Total# of Questions 45 38 63 146baseline top 1 8 (17.78%) 6 (15.79%) 11 (17.46%) 25 (17.12%)TE top 1 10 (22.22%) 8 (21.05%) 16 (25.40%) 34 (23.29%)baseline top 5 17 (37.78%) 16 (42.11%) 27 (42.86%) 60 (41.10%)TE top 5 20 (44.44%) 17 (44.74%) 32 (50.79%) 69 (47.26%)Table 3: Impact of Textual Entailment on Q/A.system increased by more than 6% over the base-line system for each of the three scenarios.
Theseresults suggest that TE can be used effectively inorder to boost the percentage of relevant answersfound in the top answers returned by a system:by focusing only on answers that are entailed bya user?s question, we feel that systems can betteridentify passages that might contain informationrelevant to a user?s information need.6.2 Contextual EntailmentIn order to evaluate the performance of our con-textual entailment system directly, two annota-tors were tasked with identifying instances of CEamongst the passages and answers returned byour Q/A system.
Annotators were instructed tomark a passage as being contextually entailed bya scenario only when the passage could be rea-sonably expected to be associated with one of thesubtopics they believed to be entailed by the com-plex scenario.
If the passage could not be associ-ated with the extension of any subtopic they be-lieved to be entailed by the scenario, annotatorswere instructed to mark the passage as not beingcontextually entailed by the scenario.
For evalua-tion purposes, only examples that were marked byboth annotators were considered as valid examplesof CE.Annotators were tasked with evaluating threetypes of output from our Q/A system: (1) theranked list of passages retrieved by our system?sPassage Retrieval module, (2) the list of passagesidentified as being CE by the scenario, and (3) theset of answers marked as being CE by the scenario(AnsSet3).
Results from the annotation of thesepassages are presented in Table 4.S1 S2 S3 Total# %Rel # %Rel # %Rel # %RelRanked Paragraphs 450 40.4% 380 31.3% 630 42.5% 1460 39.3%Entailed Paragraphs 112 46.5% 87 44.8% 149 52.4% 348 48.6%Answer Set 3 304 44.4% 188 39.9% 322 49.1% 814 45.2%Table 4: Distribution of CE.Annotators marked 39.3% of retrieved passagesas being CE by one of the three scenarios.
Thisnumber increased substantially when only pas-sages identified by the CE system were consid-ered, as annotators judged 48.6% of CE passagesand 45.2% of CE-filtered answers to be valid in-stances of contextual entailment.6.3 Intrinsic EvaluationIn order to evaluate the impact of CE on a Q/A sys-tem, we compared the quality of answers produced(1) when no CE information was used (AnsSet1),(2) when CE information was used to select alist of entailed paragraphs that were submitted toan Answer Processing module (AnsSet2), and (3)when CE information was used directly to selectanswers (AnsSet3).
Results from these three ex-periments are presented in Table 5.S1 S2 S3 Total# of Questions 45 38 63 146AnsSet1 top 1 12 (26.67%) 9 (23.68%) 19 (30.16%) 40 (27.39%)AnsSet2 top 1 16 (35.56%) 11 (28.95%) 26 (41.27%) 53 (36.30%)AnsSet3 top 1 14 (31.11%) 15 (39.47%) 31 (49.21%) 60 (41.09%)AnsSet1 top 5 21 (46.67%) 17 (44.74%) 30 (47.62%) 68 (46.58%)AnsSet2 top 5 24 (53.33%) 18 (47.37%) 35 (55.55%) 77 (52.74%)AnsSet3 top 5 29 (64.44%) 20 (52.63%) 39 (61.90%) 88 (60.27%)Table 5: Intrinsic Evaluation of CE on Q/A Per-formance.As with the TE-based experiments described inSection 7.1, we found that the Q/A system wasmore likely to return at least one relevant an-swer among the top-ranked answers when con-textual entailment information was used to eitherrank or select answers.
When CE was used torank passages for Answer Processing (AnsSet2),accuracy increased by nearly 9% over the base-line (AnsSet1), while accuracy increased by al-most 14% overall when CE was used to select an-swers directly (AnsSet3).6.4 Extrinsic EvaluationIn order to evaluate the performance of the frame-work illustrated in Figure 6, we compared the per-formance of a question-focused MDS system (firstdescribed in (Lacatusu et al, 2006)) that did notuse CE with a similar system that used CE to rankpassages for a summary answer.When CE was not used, sentences identified bythe system?s Q/A and MDS system for each ques-tion were combined and ranked based on numberof question keywords found in each sentence.
Inthe CE-enabled system (analogous to the systemdepicted in Figure 6), only the sentences that werecontextually entailed by the scenario were consid-ered; sentences were then ranked using the real-valued entailment condence computed by the CEsystem for each sentence.
Results from this sys-tem are presented in Table 6.Although the CE-enabled system was morelikely to return a scenario-relevant sentence in top38S1 S2 S3 Total# of Questions 45 38 63 146Without CE top 1 14 (31.11%) 15 (39.47%) 31 (49.21%) 60 (41.09%)With CE top 1 20 (44.44%) 16 (42.11%) 32 (50.79%) 68 (48.23%)Without CE top 5 29 (64.44%) 20 (52.63%) 39 (61.90%) 88 (60.27%)With CE top 5 29 (64.44%) 21 (55.26%) 40 (63.49%) 90 (61.64%)Table 6: Extrinsic Evaluation.position (48.23%) than the system that did notuse CE (41.09%), differences between the systemswere much less apparent when the top 5 answersreturned by each system were compared.7 ConclusionsThis paper introduced a new form of textual entail-ment, known as contextual entailment, which canbe used to recognize scenario-relevant informationin both the questions users ask and in the answersthat automatic Q/A systems return.
In additionto outlining a framework for recognizing contex-tual entailment in texts, we showed that contextualentailment information can significantly enhancethe quality of answers returned by a Q/A systemin response to users?
questions about a particularscenario.
In our evaluations, we found that usingcontextual entailment allowed Q/A systems to im-prove their accuracy by more than 10% overall.8 AcknowledgmentsThis material is based upon work funded in wholeor in part by the U.S. Government and any opin-ions, findings, conclusions, or recommendationsexpressed in this material are those of the authorsand do not necessarily reflect the views of the U.S.Government.ReferencesIdo Dagan, Oren Glickman, and Bernardo Magnini.
2005.The PASCAL Recognizing Textual Entailment Challenge.In Proceedings of the PASCAL Challenges Workshop.Jeroen Groenendijk.
1999.
The logic of interrogation: Clas-sical version.
In Proceedings of the Ninth Semantics andLinguistics Theory Conference (SALT IX), Ithaca, NY.Aria Haghighi, Andrew Ng, and Christopher Manning.
2005.Robust textual inference via graph matching.
In Pro-ceedings of Human Language Technology Conference andConference on Empirical Methods in Natural LanguageProcessing, Vancouver, British Columbia, Canada, Octo-ber.Sanda Harabagiu and Andrew Hickl.
2006.
Methodsfor using textual entailment in open-domain question-answering.
In Proceedings of the Joint InternationalConference on Computational Linguistics and AnnualMeeting of the Association for Computational Linguistics(COLING-ACL 2006), Sydney, Australia.Sanda Harabagiu, Andrew Hickl, John Lehmann, and DanMoldovan.
2005.
Experiments with Interactive Question-Answering.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics (ACL?05).Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl.
2006.Answering complex questions with random walk models.In 2006 ACM SIGIR Conference on Research and Devel-opment in Information Retrieval (SIGIR), Seattle, WA.Sanda Harabagiu.
2004.
Incremental Topic Representations.In Proceedings of the 20th COLING Conference, Geneva,Switzerland.Andrew Hickl, John Lehmann, John Williams, and SandaHarabagiu.
2004.
Experiments with Interactive Question-Answering in Complex Scenarios.
In Proceedings of theWorkshop on the Pragmatics of Question Answering atHLT-NAACL 2004, Boston, MA.Andrew Hickl, Patrick Wang, John Lehmann, and SandaHarabagiu.
2006a.
FERRET: Interactive Question-Answering for Real-World Environments.
In Proceed-ings of the Joint International Conference on Computa-tional Linguistics and Annual Meeting of the Associationfor Computational Linguistics (COLING-ACL 2006) In-teractive Presentations Program, Sydney, Australia.Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,Bryan Rink, and Ying Shi.
2006b.
Recognizing TextualEntailment with LCC?s Groundhog System.
In Proceed-ings of the Second PASCAL Challenges Workshop, Syd-ney, Australia.Finley Lacatusu, Andrew Hickl, Kirk Roberts, Ying Shi,Jeremy Bensley, Bryan Rink, Patrick Wang, and Lara Tay-lor.
2006.
LCC?s GISTexter at DUC 2006: Multi-StrategyMulti-Document Summarization.
In Proceedings of the2006 Document Understanding Conference (DUC 2006),New York, New York.David Lewis.
1988.
Relevant Implication.
Theoria,54(3):161?174.Chin-Yew Lin and Eduard Hovy.
2000.
The auto-mated acquisition of topic signatures for text summariza-tion.
In Proceedings of the 18th COLING Conference,Saarbru?cken, Germany.Bill MacCartney, Trond Grenager, Marie-Catherine de Marn-effe, Daniel Cer, and Christopher D. Manning.
2006.Learning to recognize features of valid textual entail-ments.
In Proceedings of the Joint Human LanguageTechnology Conference and Annual Meeting of the NorthAmerican Chapter of the Association for ComputationalLinguistics (HLT-NAACL 2006), New York, New York.Dan Moldovan, Christine Clark, Sanda Harabagiu, and SteveMaiorano.
2003.
COGEX: A Logic Prover for QuestionAnswering.
In Proceedings of HLT/NAACL-2003.Srini Narayanan and Sanda Harabagiu.
2004.
Question An-swering based on Semantic Structures.
In Proceedings ofCOLING-2004.39
