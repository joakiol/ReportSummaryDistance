Proceedings of ACL-08: HLT, pages 586?594,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsForest Reranking: Discriminative Parsing with Non-Local Features?Liang HuangUniversity of PennsylvaniaPhiladelphia, PA 19104lhuang3@cis.upenn.eduAbstractConventional n-best reranking techniques of-ten suffer from the limited scope of the n-best list, which rules out many potentiallygood alternatives.
We instead propose forestreranking, a method that reranks a packed for-est of exponentially many parses.
Since ex-act inference is intractable with non-local fea-tures, we present an approximate algorithm in-spired by forest rescoring that makes discrim-inative training practical over the whole Tree-bank.
Our final result, an F-score of 91.7, out-performs both 50-best and 100-best rerankingbaselines, and is better than any previously re-ported systems trained on the Treebank.1 IntroductionDiscriminative reranking has become a populartechnique for many NLP problems, in particular,parsing (Collins, 2000) and machine translation(Shen et al, 2005).
Typically, this method first gen-erates a list of top-n candidates from a baseline sys-tem, and then reranks this n-best list with arbitraryfeatures that are not computable or intractable tocompute within the baseline system.
But despite itsapparent success, there remains a major drawback:this method suffers from the limited scope of the n-best list, which rules out many potentially good al-ternatives.
For example 41% of the correct parseswere not in the candidates of ?30-best parses in(Collins, 2000).
This situation becomes worse withlonger sentences because the number of possible in-terpretations usually grows exponentially with the?
Part of this work was done while I was visiting Instituteof Computing Technology, Beijing, and I thank Prof. Qun Liuand his lab for hosting me.
I am also grateful to Dan Gildea andMark Johnson for inspirations, Eugene Charniak for help withhis parser, and Wenbin Jiang for guidance on perceptron aver-aging.
This project was supported by NSF ITR EIA-0205456.local non-localconventional reranking only at the rootDP-based discrim.
parsing exact N/Athis work: forest-reranking exact on-the-flyTable 1: Comparison of various approaches for in-corporating local and non-local features.sentence length.
As a result, we often see very fewvariations among the n-best trees, for example, 50-best trees typically just represent a combination of 5to 6 binary ambiguities (since 25 < 50 < 26).Alternatively, discriminative parsing is tractablewith exact and efficient search based on dynamicprogramming (DP) if all features are restricted to belocal, that is, only looking at a local window withinthe factored search space (Taskar et al, 2004; Mc-Donald et al, 2005).
However, we miss the benefitsof non-local features that are not representable here.Ideally, we would wish to combine the merits ofboth approaches, where an efficient inference algo-rithm could integrate both local and non-local fea-tures.
Unfortunately, exact search is intractable (atleast in theory) for features with unbounded scope.So we propose forest reranking, a technique inspiredby forest rescoring (Huang and Chiang, 2007) thatapproximately reranks the packed forest of expo-nentially many parses.
The key idea is to computenon-local features incrementally from bottom up, sothat we can rerank the n-best subtrees at all internalnodes, instead of only at the root node as in conven-tional reranking (see Table 1).
This method can thusbe viewed as a step towards the integration of dis-criminative reranking with traditional chart parsing.Although previous work on discriminative pars-ing has mainly focused on short sentences (?
15words) (Taskar et al, 2004; Turian and Melamed,2007), our work scales to the whole Treebank, where586VP1,6VBD1,2 blah NP2,6NP2,3 blah PP3,6be2 e1Figure 1: A partial forest of the example sentence.we achieved an F-score of 91.7, which is a 19% er-ror reduction from the 1-best baseline, and outper-forms both 50-best and 100-best reranking.
This re-sult is also better than any previously reported sys-tems trained on the Treebank.2 Packed Forests as HypergraphsInformally, a packed parse forest, or forest in short,is a compact representation of all the derivations(i.e., parse trees) for a given sentence under acontext-free grammar (Billot and Lang, 1989).
Forexample, consider the following sentence0 I 1 saw 2 him 3 with 4 a 5 mirror 6where the numbers between words denote string po-sitions.
Shown in Figure 1, this sentence has (atleast) two derivations depending on the attachmentof the prep.
phrase PP3,6 ?with a mirror?
: it can ei-ther be attached to the verb ?saw?,VBD1,2 NP2,3 PP3,6VP1,6 , (*)or be attached to ?him?, which will be further com-bined with the verb to form the same VP as above.These two derivations can be represented as a sin-gle forest by sharing common sub-derivations.
Sucha forest has a structure of a hypergraph (Klein andManning, 2001; Huang and Chiang, 2005), whereitems like PP3,6 are called nodes, and deductivesteps like (*) correspond to hyperedges.More formally, a forest is a pair ?V,E?, where Vis the set of nodes, and E the set of hyperedges.
Fora given sentence w1:l = w1 .
.
.
wl, each node v ?
Vis in the form of X i,j , which denotes the recogni-tion of nonterminal X spanning the substring frompositions i through j (that is, wi+1 .
.
.
wj).
Each hy-peredge e ?
E is a pair ?tails(e), head(e)?, wherehead(e) ?
V is the consequent node in the deduc-tive step, and tails(e) ?
V ?
is the list of antecedentnodes.
For example, the hyperedge for deduction (*)is notated:e1 = ?
(VBD1,2, NP2,3, PP3,6), VP1,6?We also denote IN (v) to be the set of incom-ing hyperedges of node v, which represent the dif-ferent ways of deriving v. For example, in the for-est in Figure 1, IN (VP1,6) is {e1, e2}, with e2 =?
(VBD1,2, NP2,6), VP1,6?.
We call |e| the arity ofhyperedge e, which counts the number of tail nodesin e. The arity of a hypergraph is the maximum ar-ity over all hyperedges.
A CKY forest has an arityof 2, since the input grammar is required to be bi-nary branching (cf.
Chomsky Normal Form) to en-sure cubic time parsing complexity.
However, in thiswork, we use forests from a Treebank parser (Char-niak, 2000) whose grammar is often flat in manyproductions.
For example, the arity of the forest inFigure 1 is 3.
Such a Treebank-style forest is eas-ier to work with for reranking, since many featurescan be directly expressed in it.
There is also a distin-guished root node TOP in each forest, denoting thegoal item in parsing, which is simply S0,l where S isthe start symbol and l is the sentence length.3 Forest Reranking3.1 Generic Reranking with the PerceptronWe first establish a unified framework for parsereranking with both n-best lists and packed forests.For a given sentence s, a generic reranker selectsthe best parse y?
among the set of candidates cand(s)according to some scoring function:y?
= argmaxy?cand(s)score(y) (1)In n-best reranking, cand(s) is simply a set ofn-best parses from the baseline parser, that is,cand(s) = {y1, y2, .
.
.
, yn}.
Whereas in forestreranking, cand(s) is a forest implicitly represent-ing the set of exponentially many parses.As usual, we define the score of a parse y to bethe dot product between a high dimensional featurerepresentation and a weight vector w:score(y) = w ?
f(y) (2)587where the feature extractor f is a vector of d func-tions f = (f1, .
.
.
, fd), and each feature fj mapsa parse y to a real number fj(y).
Following (Char-niak and Johnson, 2005), the first feature f1(y) =log Pr(y) is the log probability of a parse from thebaseline generative parser, while the remaining fea-tures are all integer valued, and each of them countsthe number of times that a particular configurationoccurs in parse y.
For example, one such featuref2000 might be a question?how many times is a VP of length 5 surroundedby the word ?has?
and the period?
?which is an instance of the WordEdges feature (seeFigure 2(c) and Section 3.2 for details).Using a machine learning algorithm, the weightvector w can be estimated from the training datawhere each sentence si is labelled with its cor-rect (?gold-standard?)
parse y?i .
As for the learner,Collins (2000) uses the boosting algorithm andCharniak and Johnson (2005) use the maximum en-tropy estimator.
In this work we use the averagedperceptron algorithm (Collins, 2002) since it is anonline algorithm much simpler and orders of magni-tude faster than Boosting and MaxEnt methods.Shown in Pseudocode 1, the perceptron algo-rithm makes several passes over the whole train-ing data, and in each iteration, for each sentence si,it tries to predict a best parse y?i among the candi-dates cand(si) using the current weight setting.
In-tuitively, we want the gold parse y?i to be picked, butin general it is not guaranteed to be within cand(si),because the grammar may fail to cover the goldparse, and because the gold parse may be prunedaway due to the limited scope of cand(si).
So wedefine an oracle parse y+i to be the candidate thathas the highest Parseval F-score with respect to thegold tree y?i :1y+i , argmaxy?cand(si)F (y, y?i ) (3)where function F returns the F-score.
Now we trainthe reranker to pick the oracle parses as often as pos-sible, and in case an error is made (line 6), performan update on the weight vector (line 7), by addingthe difference between two feature representations.1If one uses the gold y?i for oracle y+i , the perceptron willcontinue to make updates towards something unreachable evenwhen the decoder has picked the best possible candidate.Pseudocode 1 Perceptron for Generic Reranking1: Input: Training examples {cand(si), y+i }Ni=1 ?
y+i is theoracle tree for si among cand(si)2: w?
0 ?
initial weights3: for t?
1 .
.
.
T do ?
T iterations4: for i?
1 .
.
.
N do5: y?
= argmaxy?cand(si) w ?
f(y)6: if y?
6= y+i then7: w?
w + f(y+i )?
f(y?
)8: return wIn n-best reranking, since all parses are explicitlyenumerated, it is trivial to compute the oracle tree.2However, it remains widely open how to identify theforest oracle.
We will present a dynamic program-ming algorithm for this problem in Sec.
4.1.We also use a refinement called ?averaged param-eters?
where the final weight vector is the average ofweight vectors after each sentence in each iterationover the training data.
This averaging effect has beenshown to reduce overfitting and produce much morestable results (Collins, 2002).3.2 Factorizing Local and Non-Local FeaturesA key difference between n-best and forest rerank-ing is the handling of features.
In n-best reranking,all features are treated equivalently by the decoder,which simply computes the value of each one oneach candidate parse.
However, for forest reranking,since the trees are not explicitly enumerated, manyfeatures can not be directly computed.
So we firstclassify features into local and non-local, which thedecoder will process in very different fashions.We define a feature f to be local if and only ifit can be factored among the local productions in atree, and non-local if otherwise.
For example, theRule feature in Fig.
2(a) is local, while the Paren-tRule feature in Fig.
2(b) is non-local.
It is worthnoting that some features which seem complicatedat the first sight are indeed local.
For example, theWordEdges feature in Fig.
2(c), which classifiesa node by its label, span length, and surroundingwords, is still local since all these information areencoded either in the node itself or in the input sen-tence.
In contrast, it would become non-local if wereplace the surrounding words by surrounding POS2In case multiple candidates get the same highest F-score,we choose the parse with the highest log probability from thebaseline parser to be the oracle parse (Collins, 2000).588VPVBD NP PPSVPVBD NP PPVPVBZhasNP|?
5 words?|..VPVBDsawNPDTthe...(a) Rule (local) (b) ParentRule (non-local) (c) WordEdges (local) (d) NGramTree (non-local)?
VP?
VBD NP PP ?
?
VP?
VBD NP PP | S ?
?
NP 5 has .
?
?
VP (VBD saw) (NP (DT the)) ?Figure 2: Illustration of some example features.
Shaded nodes denote information included in the feature.tags, which are generated dynamically.More formally, we split the feature extractor f =(f1, .
.
.
, fd) into f = (fL; fN ) where fL and fN arethe local and non-local features, respectively.
For theformer, we extend their domains from parses to hy-peredges, where f(e) returns the value of a local fea-ture f ?
fL on hyperedge e, and its value on a parseyfactors across the hyperedges (local productions),fL(y) =?e?yfL(e) (4)and we can pre-compute fL(e) for each e in a forest.Non-local features, however, can not be pre-computed, but we still prefer to compute them asearly as possible, which we call ?on-the-fly?
com-putation, so that our decoder can be sensitive to themat internal nodes.
For instance, the NGramTree fea-ture in Fig.
2 (d) returns the minimum tree fragementspanning a bigram, in this case ?saw?
and ?the?, andshould thus be computed at the smallest common an-cestor of the two, which is the VP node in this ex-ample.
Similarly, the ParentRule feature in Fig.
2(b) can be computed when the S subtree is formed.In doing so, we essentially factor non-local featuresacross subtrees, where for each subtree y?
in a parsey, we define a unit feature f?(y?)
to be the part off(y) that are computable within y?, but not com-putable in any (proper) subtree of y?.
Then we have:fN (y) =?y?
?yf?N (y?)
(5)Intuitively, we compute the unit non-local fea-tures at each subtree from bottom-up.
For example,for the binary-branching node Ai,k in Fig.
3, theAi,kBi,jwi .
.
.
wj?1Cj,kwj .
.
.
wk?1Figure 3: Example of the unit NGramTree featureat node Ai,k: ?
A (B .
.
.
wj?1) (C .
.
.
wj) ?.unit NGramTree instance is for the pair ?wj?1, wj?on the boundary between the two subtrees, whosesmallest common ancestor is the current node.
Otherunit NGramTree instances within this span have al-ready been computed in the subtrees, except thosefor the boundary words of the whole node, wi andwk?1, which will be computed when this node is fur-ther combined with other nodes in the future.3.3 Approximate Decoding via Cube PruningBefore moving on to approximate decoding withnon-local features, we first describe the algorithmfor exact decoding when only local features arepresent, where many concepts and notations will bere-used later.
We will use D(v) to denote the topderivations of node v, where D1(v) is its 1-bestderivation.
We also use the notation ?e, j?
to denotethe derivation along hyperedge e, using the jith sub-derivation for tail ui, so ?e,1?
is the best deriva-tion along e. The exact decoding algorithm, shownin Pseudocode 2, is an instance of the bottom-upViterbi algorithm, which traverses the hypergraph ina topological order, and at each node v, calculatesits 1-best derivation using each incoming hyperedgee ?
IN (v).
The cost of e, c(e), is the score of its589Pseudocode 2 Exact Decoding with Local Features1: function VITERBI(?V, E?
)2: for v ?
V in topological order do3: for e ?
IN (v) do4: c(e)?
w ?
fL(e) +Pui?tails(e) c(D1(ui))5: if c(e) > c(D1(v)) then ?
better derivation?6: D1(v)?
?e,1?7: c(D1(v))?
c(e)8: return D1(TOP)Pseudocode 3 Cube Pruning for Non-local Features1: function CUBE(?V, E?
)2: for v ?
V in topological order do3: KBEST(v)4: return D1(TOP)5: procedure KBEST(v)6: heap ?
?
; buf ?
?7: for e ?
IN (v) do8: c(?e,1?)?
EVAL(e,1) ?
extract unit features9: append ?e,1?
to heap10: HEAPIFY(heap) ?
prioritized frontier11: while |heap| > 0 and |buf | < k do12: item?
POP-MAX(heap) ?
extract next-best13: append item to buf14: PUSHSUCC(item, heap)15: sort buf to D(v)16: procedure PUSHSUCC(?e, j?, heap)17: e is v ?
u1 .
.
.
u|e|18: for i in 1 .
.
.
|e| do19: j?
?
j + bi ?
bi is 1 only on the ith dim.20: if |D(ui)| ?
j?i then ?
enough sub-derivations?21: c(?e, j??)?
EVAL(e, j?)
?
unit features22: PUSH(?e, j?
?, heap)23: function EVAL(e, j)24: e is v ?
u1 .
.
.
u|e|25: return w ?
fL(e) + w ?
f?N (?e, j?)
+Pi c(Dji(ui))(pre-computed) local features w ?
fL(e).
This algo-rithm has a time complexity of O(E), and is almostidentical to traditional chart parsing, except that theforest might be more than binary-branching.For non-local features, we adapt cube pruningfrom forest rescoring (Chiang, 2007; Huang andChiang, 2007), since the situation here is analogousto machine translation decoding with integrated lan-guage models: we can view the scores of unit non-local features as the language model cost, computedon-the-fly when combining sub-constituents.Shown in Pseudocode 3, cube pruning worksbottom-up on the forest, keeping a beam of at most kderivations at each node, and uses the k-best pars-ing Algorithm 2 of Huang and Chiang (2005) tospeed up the computation.
When combining the sub-derivations along a hyperedge e to form a new sub-tree y?
= ?e, j?, we also compute its unit non-localfeature values f?N (?e, j?)
(line 25).
A priority queue(heap in Pseudocode 3) is used to hold the candi-dates for the next-best derivation, which is initial-ized to the set of best derivations along each hyper-edge (lines 7 to 9).
Then at each iteration, we popthe best derivation (lines 12), and push its succes-sors back into the priority queue (line 14).
Analo-gous to the language model cost in forest rescoring,the unit feature cost here is a non-monotonic score inthe dynamic programming backbone, and the deriva-tions may thus be extracted out-of-order.
So a bufferbuf is used to hold extracted derivations, which issorted at the end (line 15) to form the list of top-kderivations D(v) of node v. The complexity of thisalgorithm is O(E + V k log kN ) (Huang and Chi-ang, 2005), where O(N ) is the time for on-the-flyfeature extraction for each subtree, which becomesthe bottleneck in practice.4 Supporting Forest Algorithms4.1 Forest OracleRecall that the Parseval F-score is the harmonicmean of labelled precision P and labelled recall R:F (y, y?)
, 2PRP + R=2|y ?
y?||y|+ |y?| (6)where |y| and |y?| are the numbers of brackets in thetest parse and gold parse, respectively, and |y ?
y?|is the number of matched brackets.
Since the har-monic mean is a non-linear combination, we can notoptimize the F-scores on sub-forests independentlywith a greedy algorithm.
In other words, the optimalF-score tree in a forest is not guaranteed to be com-posed of two optimal F-score subtrees.We instead propose a dynamic programming al-gorithm which optimizes the number of matchedbrackets for a given number of test brackets.
For ex-ample, our algorithm will ask questions like,?when a test parse has 5 brackets, what is themaximum number of matched brackets?
?More formally, at each node v, we compute an ora-cle function ora[v] : N 7?
N, which maps an integert to ora[v](t), the max.
number of matched brackets590Pseudocode 4 Forest Oracle Algorithm1: function ORACLE(?V, E?, y?
)2: for v ?
V in topological order do3: for e ?
BS(v) do4: e is v ?
u1u2 .
.
.
u|e|5: ora[v]?
ora[v]?
(?iora[ui])6: ora[v]?
ora[v] ?
(1,1v?y?
)7: return F (y+, y?)
= maxt 2?ora[TOP](t)t+|y?| ?
oracle F1for all parses yv of node v with exactly t brackets:ora[v](t) , maxyv :|yv |=t|yv ?
y?| (7)When node v is combined with another node ualong a hyperedge e = ?
(v, u), w?, we need to com-bine the two oracle functions ora[v] and ora[u] bydistributing the test brackets of w between v and u,and optimize the number of matched bracktes.
Todo this we define a convolution operator ?
betweentwo functions f and g:(f ?
g)(t) , maxt1+t2=tf(t1) + g(t2) (8)For instance:t f(t)2 13 2?t g(t)4 45 4=t (f ?
g)(t)6 57 68 6The oracle function for the head node w is thenora[w](t) = (ora[v]?
ora[u])(t?
1)+1w?y?
(9)where 1 is the indicator function, returning 1 if nodew is found in the gold tree y?, in which case weincrement the number of matched brackets.
We canalso express Eq.
9 in a purely functional formora[w] = (ora[v]?
ora[u]) ?
(1,1w?y?)
(10)where ?
is a translation operator which shifts afunction along the axes:(f ?
(a, b))(t) , f(t?
a) + b (11)Above we discussed the case of one hyperedge.
Ifthere is another hyperedge e?
deriving node w, wealso need to combine the resulting oracle functionsfrom both hyperedges, for which we define a point-wise addition operator ?
:(f ?
g)(t) , max{f(t), g(t)} (12)Shown in Pseudocode 4, we perform these com-putations in a bottom-up topological order, and fi-nally at the root node TOP, we can compute the bestglobal F-score by maximizing over different num-bers of test brackets (line 7).
The oracle tree y+ canbe recursively restored by keeping backpointers foreach ora[v](t), which we omit in the pseudocode.The time complexity of this algorithm for a sen-tence of l words is O(|E| ?
l2(a?1)) where a is thearity of the forest.
For a CKY forest, this amountsto O(l3 ?
l2) = O(l5), but for general forests likethose in our experiments the complexities are muchhigher.
In practice it takes on average 0.05 secondsfor forests pruned by p = 10 (see Section 4.2), butwe can pre-compute and store the oracle for eachforest before training starts.4.2 Forest PruningOur forest pruning algorithm (Jonathan Graehl, p.c.
)is very similar to the method based on marginalprobability (Charniak and Johnson, 2005), exceptthat ours prunes hyperedges as well as nodes.
Ba-sically, we use an Inside-Outside algorithm to com-pute the Viterbi inside cost ?
(v) and the Viterbi out-side cost ?
(v) for each node v, and then compute themerit ??
(e) for each hyperedge:??
(e) = ?
(head(e)) +?ui?tails(e)?
(ui) (13)Intuitively, this merit is the cost of the best deriva-tion that traverses e, and the difference ?
(e) =??
(e) ?
?
(TOP) can be seen as the distance awayfrom the globally best derivation.
We prune awayall hyperedges that have ?
(e) > p for a thresh-old p. Nodes with all incoming hyperedges prunedare also pruned.
The key difference from (Charniakand Johnson, 2005) is that in this algorithm, a nodecan ?partially?
survive the beam, with a subset of itshyperedges pruned.
In practice, this method pruneson average 15% more hyperedges than their method.5 ExperimentsWe compare the performance of our forest rerankeragainst n-best reranking on the Penn English Tree-bank (Marcus et al, 1993).
The baseline parser isthe Charniak parser, which we modified to output a591Local instances Non-Local instancesRule 10, 851 ParentRule 18, 019Word 20, 328 WProj 27, 417WordEdges 454, 101 Heads 70, 013CoLenPar 22 HeadTree 67, 836Bigram?
10, 292 Heavy 1, 401Trigram?
24, 677 NGramTree 67, 559HeadMod?
12, 047 RightBranch 2DistMod?
16, 017Total Feature Instances: 800, 582Table 2: Features used in this work.
Those with a ?are from (Collins, 2000), and others are from (Char-niak and Johnson, 2005), with simplifications.packed forest for each sentence.35.1 Data PreparationWe use the standard split of the Treebank: sections02-21 as the training data (39832 sentences), sec-tion 22 as the development set (1700 sentences), andsection 23 as the test set (2416 sentences).
Follow-ing (Charniak and Johnson, 2005), the training set issplit into 20 folds, each containing about 1992 sen-tences, and is parsed by the Charniak parser with amodel trained on sentences from the remaining 19folds.
The development set and the test set are parsedwith a model trained on all 39832 training sentences.We implemented both n-best and forest rerankingsystems in Python and ran our experiments on a 64-bit Dual-Core Intel Xeon with 3.0GHz CPUs.
Ourfeature set is summarized in Table 2, which closelyfollows Charniak and Johnson (2005), except thatwe excluded the non-local features Edges, NGram,and CoPar, and simplified Rule and NGramTreefeatures, since they were too complicated to com-pute.4 We also added four unlexicalized local fea-tures from Collins (2000) to cope with data-sparsity.Following Charniak and Johnson (2005), we ex-tracted the features from the 50-best parses on thetraining set (sec.
02-21), and used a cut-off of 5 toprune away low-count features.
There are 0.8M fea-tures in our final set, considerably fewer than thatof Charniak and Johnson which has about 1.3M fea-3This is a relatively minor change to the Charniak parser,since it implements Algorithm 3 of Huang and Chiang (2005)for efficient enumeration of n-best parses, which requires stor-ing the forest.
The modified parser and related scripts for han-dling forests (e.g.
oracles) will be available on my homepage.4In fact, our Rule and ParentRule features are two specialcases of the original Rule feature in (Charniak and Johnson,2005).
We also restricted NGramTree to be on bigrams only.89.091.093.095.097.099.00  500  1000  1500  2000ParsevalF-score(%)average # of hyperedges or brackets per sentencep=10 p=20n=10n=50 n=1001-bestforest oraclen-best oracleFigure 4: Forests (shown with various pruningthresholds) enjoy higher oracle scores and morecompact sizes than n-best lists (on sec 23).tures in the updated version.5 However, our initialexperiments show that, even with this much simplerfeature set, our 50-best reranker performed equallywell as theirs (both with an F-score of 91.4, see Ta-bles 3 and 4).
This result confirms that our featureset design is appropriate, and the averaged percep-tron learner is a reasonable candidate for reranking.The forests dumped from the Charniak parser arehuge in size, so we use the forest pruning algorithmin Section 4.2 to prune them down to a reasonablesize.
In the following experiments we use a thresh-old of p = 10, which results in forests with an av-erage number of 123.1 hyperedges per forest.
Thenfor each forest, we annotate its forest oracle, andon each hyperedge, pre-compute its local features.6Shown in Figure 4, these forests have an forest or-acle of 97.8, which is 1.1% higher than the 50-bestoracle (96.7), and are 8 times smaller in size.5.2 Results and AnalysisTable 3 compares the performance of forest rerank-ing against standard n-best reranking.
For both sys-tems, we first use only the local features, and thenall the features.
We use the development set to deter-mine the optimal number of iterations for averagedperceptron, and report the F1 score on the test set.With only local features, our forest reranker achievesan F-score of 91.25, and with the addition of non-5http://www.cog.brown.edu/?mj/software.htm.
We followthis version as it corrects some bugs from their 2005 paperwhich leads to a 0.4% increase in performance (see Table 4).6A subset of local features, e.g.
WordEdges, is independentof which hyperedge the node takes in a derivation, and can thusbe annotated on nodes rather than hyperedges.
We call thesefeatures node-local, which also include part of Word features.592baseline: 1-best Charniak parser 89.72n-best rerankingfeatures n pre-comp.
training F1%local 50 1.7G / 16h 3 ?
0.1h 91.28all 50 2.4G / 19h 4 ?
0.3h 91.43all 100 5.3G / 44h 4 ?
0.7h 91.49forest reranking (p = 10)features k pre-comp.
training F1%local - 1.2G / 2.9h 3 ?
0.8h 91.25all 15 4 ?
6.1h 91.69Table 3: Forest reranking compared to n-best rerank-ing on sec.
23.
The pre-comp.
column is for featureextraction, and training column shows the numberof perceptron iterations that achieved best results onthe dev set, and average time per iteration.local features, the accuracy rises to 91.69 (with beamsize k = 15), which is a 0.26% absolute improve-ment over 50-best reranking.7This improvement might look relatively small, butit is much harder to make a similar progress withn-best reranking.
For example, even if we doublethe size of the n-best list to 100, the performanceonly goes up by 0.06% (Table 3).
In fact, the 100-best oracle is only 0.5% higher than the 50-best one(see Fig.
4).
In addition, the feature extraction stepin 100-best reranking produces huge data files andtakes 44 hours in total, though this part can be paral-lelized.8 On two CPUs, 100-best reranking takes 25hours, while our forest-reranker can also finish in 26hours, with a much smaller disk space.
Indeed, thisdemonstrates the severe redundancies as another dis-advantage of n-best lists, where many subtrees arerepeated across different parses, while the packedforest reduces space dramatically by sharing com-mon sub-derivations (see Fig.
4).To put our results in perspective, we also comparethem with other best-performing systems in Table 4.Our final result (91.7) is better than any previouslyreported system trained on the Treebank, although7It is surprising that 50-best reranking with local featuresachieves an even higher F-score of 91.28, and we suspect this isdue to the aggressive updates and instability of the perceptron,as we do observe the learning curves to be non-monotonic.
Weleave the use of more stable learning algorithms to future work.8The n-best feature extraction already uses relative counts(Johnson, 2006), which reduced file sizes by at least a factor 4.type system F1%DCollins (2000) 89.7Henderson (2004) 90.1Charniak and Johnson (2005) 91.0updated (Johnson, 2006) 91.4this work 91.7G Bod (2003) 90.7Petrov and Klein (2007) 90.1S McClosky et al (2006) 92.1Table 4: Comparison of our final results with otherbest-performing systems on the whole Section 23.Types D, G, and S denote discriminative, generative,and semi-supervised approaches, respectively.McClosky et al (2006) achieved an even higher ac-cuarcy (92.1) by leveraging on much larger unla-belled data.
Moreover, their technique is orthogonalto ours, and we suspect that replacing their n-bestreranker by our forest reranker might get an evenbetter performance.
Plus, except for n-best rerank-ing, most discriminative methods require repeatedparsing of the training set, which is generally im-pratical (Petrov and Klein, 2008).
Therefore, pre-vious work often resorts to extremely short sen-tences (?
15 words) or only looked at local fea-tures (Taskar et al, 2004; Henderson, 2004; Turianand Melamed, 2007).
In comparison, thanks to theefficient decoding, our work not only scaled to thewhole Treebank, but also successfully incorporatednon-local features, which showed an absolute im-provement of 0.44% over that of local features alone.6 ConclusionWe have presented a framework for reranking onpacked forests which compactly encodes many morecandidates than n-best lists.
With efficient approx-imate decoding, perceptron training on the wholeTreebank becomes practical, which can be done inabout a day even with a Python implementation.
Ourfinal result outperforms both 50-best and 100-bestreranking baselines, and is better than any previ-ously reported systems trained on the Treebank.
Wealso devised a dynamic programming algorithm forforest oracles, an interesting problem by itself.
Webelieve this general framework could also be appliedto other problems involving forests or lattices, suchas sequence labeling and machine translation.593ReferencesSylvie Billot and Bernard Lang.
1989.
The struc-ture of shared forests in ambiguous parsing.
InProceedings of ACL ?89, pages 143?151.Rens Bod.
2003.
An efficient implementation of anew DOP model.
In Proceedings of EACL.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine-grained n-best parsing and discriminativereranking.
In Proceedings of the 43rd ACL.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of NAACL.David Chiang.
2007.
Hierarchical phrase-based translation.
Computational Linguistics,33(2):201?208.Michael Collins.
2000.
Discriminative rerankingfor natural language parsing.
In Proceedings ofICML, pages 175?182.Michael Collins.
2002.
Discriminative trainingmethods for hidden markov models: Theory andexperiments with perceptron algorithms.
In Pro-ceedings of EMNLP.James Henderson.
2004.
Discriminative training ofa neural network statistical parser.
In Proceedingsof ACL.Liang Huang and David Chiang.
2005.
Better k-best Parsing.
In Proceedings of the Ninth Interna-tional Workshop on Parsing Technologies (IWPT-2005).Liang Huang and David Chiang.
2007.
Forestrescoring: Fast decoding with integrated languagemodels.
In Proceedings of ACL.Mark Johnson.
2006.
Features of statisti-cal parsers.
Talk given at the Joint Mi-crosoft Research and Univ.
of Washing-ton Computational Linguistics Colloquium.http://www.cog.brown.edu/?mj/papers/ms-uw06talk.pdf.Dan Klein and Christopher D. Manning.
2001.Parsing and Hypergraphs.
In Proceedings of theSeventh International Workshop on Parsing Tech-nologies (IWPT-2001), 17-19 October 2001, Bei-jing, China.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of English: the Penn Tree-bank.
Computational Linguistics, 19:313?330.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of the HLT-NAACL, New York City,USA, June.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training ofdependency parsers.
In Proceedings of the 43rdACL.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Proceedings ofHLT-NAACL.Slav Petrov and Dan Klein.
2008.
Discriminativelog-linear grammars with latent variables.
In Pro-ceedings of NIPS 20.Libin Shen, Anoop Sarkar, and Franz Josef Och.2005.
Discriminative reranking for machinetranslation.
In Proceedings of HLT-NAACL.Ben Taskar, Dan Klein, Michael Collins, DaphneKoller, and Chris Manning.
2004.
Max-marginparsing.
In Proceedings of EMNLP.Joseph Turian and I. Dan Melamed.
2007.
Scalablediscriminative learning for natural language pars-ing and translation.
In Proceedings of NIPS 19.594
