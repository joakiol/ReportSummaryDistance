Proceedings of NAACL HLT 2009: Short Papers, pages 77?80,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsGenerating Synthetic Children's Acoustic Models from Adult ModelsAndreas Hagen, Bryan Pellom, and Kadri HaciogluRosetta Stone Labs{ahagen, bpellom, khacioglu}@rosettastone.comAbstractThis work focuses on generating children?sHMM-based acoustic models for speech rec-ognition from adult acoustic models.
Collect-ing children?s speech data is more costlycompared to adult?s speech.
The patent-pending method developed in this work re-quires only adult data to estimate syntheticchildren?s acoustic models in any languageand works as follows: For a new languagewhere only adult data is available, an adultmale and an adult female model is trained.
Alinear transformation from each male HMMmean vector to its closest female mean vectoris estimated.
This transform is then scaled to acertain power and applied to the female modelto obtain a synthetic children?s model.
In apronunciation verification task the methodyields 19% and 3.7% relative improvement onnative English and Spanish children?s data, re-spectively, compared to the best adult model.For Spanish data, the new model outperformsthe available real children?s data based modelby 13% relative.1 IntroductionLanguage learning is becoming more and moreimportant in the age of globalization.
Dependingon their work or cultural situation some people areconfronted with various different languages on adaily basis.
While it is very desirable to learn lan-guages at any age, language learning, among otherlearning experiences, is comparably simpler forchildren than for adults and should therefore beencouraged at early ages.Even though the children?s language learning mar-ket is highly important, comprising effectivespeech recognition tools for pronunciation assess-ment is relatively hard due to the special characte-ristics of children?s speech and the limitedavailability of children?s speech data in many lan-guages in the speech research community.
Adultspeech data is usually easier to obtain.
By under-standing the characteristics of children?s speech theunconditional need for children?s speech data canbe lessened by altering adult acoustic models suchthat they are suitable for children?s speech.Children?s speech has higher pitch and formantsthan female speech.
Further, female speech hashigher pitch and formants than male speech.
Child-ren?s speech is more variable than female speech,and, as research has shown, female speech is morevariable than male speech (Lee et al, 1999).
Giventhis transitive chain of argumentation, the trans-formation from a male to a female acoustic modelcan be estimated for a language and applied (at acertain adjustable degree) to the female model.This process results in a synthetic children?sspeech model designed on the basis of the femalemodel.
Therefore, for a new language an effectivesynthetic children?s acoustic model can be derivedwithout the need of children?s data (Hagen et al,2008).2 Related WorkExtensive research has been done in the field ofchildren?s speech analysis and recognition in thepast few years.
A detailed overview of children?sspeech characteristics can be found in (Lee et al,1999).
The paper presents research results showingthe higher variability in speech characteristicsamong children compared to adult speech.
Theproperties of children?s speech that were re-searched were duration of vowels and sentences,pitch, and formant locations.When designing acoustic models specially suitedfor children, properties as the formant locationsand higher variability of children?s speech need tobe accounted for.
The best solution for buildingchildren?s speech models is to collect children?sspeech data and to train models from scratch (Ha-77gen et al, 2003, Cosi et al 2005).
Researchershave also tried to apply adult acoustic models us-ing speaker normalization techniques to recognizechildren?s speech (Elenius et al, 2005, Potamianoset al 1997).
Adult acoustic models were adaptedtowards children?s speech.
A limited amount ofchildren?s speech data was available for adapta-tion.
In (Gustafson et al, 2002) children?s voiceswere transformed before being sent to the recog-nizer using adult acoustic models.
In (Claes et al,1997) children?s acoustic models were built basedon a VTL adaptation of cepstral parameters basedon the third formant frequency.
The methodshowed to be effective for building children?sspeech models.3 Building Synthetic Children?s Modelsfrom Adult ModelsAs mentioned in Section 1, research has shownthat pitch and formants of children?s speech arehigher than for female speech.
Female speech hashigher pitch and formants than male speech.
Inorder to exploit these research results a transforma-tion from a male acoustic model to a female acous-tic model can be derived.
This transformation willmap a male model as close as possible to a femalemodel.
The transformation can be adjusted andapplied to the female model.
The resulting synthet-ic model can be tested on children?s data.Parameters that are subject to transformation inthis process are the mean vectors of the HMMstates.
The transformation can be represented as asquare matrix in the dimension of the mean vec-tors.
The transformation chosen in this approach istherefore linear and is for example capable ofrepresenting a vocal tract length adaptation as itwas shown in (Pitz et al, 2005).
Linear transfor-mations (i.e.
matrices) are also chosen in adapta-tion approaches as MAPLR and MLLR, whosebenefit has been shown to be additive to the benefitof VTLN in speaker adaptation applications.
Alinear transform in the form of a matrix is thereforewell suited due to its expressive power as well asits mathematical manageability.3.1 Transformation MatrixThe transformation matrix used in this approach isestimated by mapping the male to the femaleacoustic model, such that each HMM state meanvector in the male model is assigned a correspond-ing mean vector in the female model.
Informationused in the mapping process is the basic phonemeand context.
The resulting mean vector pairs areused as source and target features in the trainingprocess of the transformation matrix.
During train-ing the matrix is initialized as the identity matrixand the estimate of the mapping is refined by gra-dient descent.
In a typical acoustic model there areseveral hundred, sometimes thousands, of thesemean vector pairs to train the transformation ma-trix.
The expression that needs to be minimized is:2),()(minarg yAxTpairsyxA?= ?where T is the error-minimizing transformationmatrix; x is a male model?s source vector and y itcorresponding female model?s target vector.In this optimization process the Matrix A is initia-lized as the identity matrix.
Each matrix entry ija isupdated (to the new value 'ija ) in the following wayby gradient descent:( ) jiiijij xyxAkaa ?+='where iA  is the i-th line of matrix A and k deter-mines the descent step size (k<0 and incorporatesthe factor of 2 resulting from the differentiation).The gradient descent needs to be run multipletimes over all vector pairs (x,y) for the matrix toconverge to an acceptable approximation which iscalled the transformation matrix T.3.2 Synthetic Children?s Model CreationThe transformation matrix can be applied to thefemale model in order to create a new syntheticacoustic model which should suit children?s speechbetter than adult acoustic models.
It is unlikely thatthe transformation applied ?as is?
will result in thebest model possible, therefore the transformationcan be altered (amplified or weakened) in order toyield the best results.
An intuitive way to alter theimpact of the transformation is taking the matrix Tto a certain power p. Synthetic models can becreated by applying pT  to the female model1, forvarious values p. If children?s data is available forevaluation purposes, the best value of p can be de-termined.
The power p is claimed to be languageindependent.
It might vary in nuances, but experi-1Taking a matrix to the power of p is meant in the senseTTpp=/1, IdentityT =0 , TT =178ments have shown that a value around 0.25 is areasonable choice.3.3 Transformation AlgorithmThe previous section presented the theoreticalmeans necessary for the synthetic children?s modelcreation process.
The precise, patent-pending algo-rithm to create a synthetic children?s model in anew language is as follows (Hagen et al, 2008):1.
Train a male and a female acoustic model2.
Estimate the transform T from the maleto the female model3.
Determine the power p by which thetransform T should be adjusted4.
Apply pT  to the female acoustic modelto create the synthetic children?s modelStep 3, the determination of the power p, can bedone in two different ways.
If children?s test datain the relevant language is available, various mod-els based on different p-values can be evaluatedand the best one chosen.
If there is no children?sdata available in a new language, p can be esti-mated by evaluations in a language where there isenough male, female, and children?s speech dataavailable.
The claim here is that the power p is rel-atively language independent and estimating p in adifferent language is superior to a simple guess.4 ExperimentsThe algorithm was tested on two languages: USEnglish and Spanish.
For both languages sufficientmale, female, and children?s speech data wasavailable (more than 20 hours) in order to trainvalid acoustic models and to have reference child-ren?s acoustic models available.
For English testdata we used a corpus of 22 native speakers in theage range of 5 to 14.
The number of utterances is2,182.
For Spanish test data the corpus is com-prised of 19 speakers in the age range of 8 to 13years.
The number of utterances is 2,598.The transform from the male to the female modelwas estimated in English.
The power of p wasgradually increased and the transformation matrixwas adjusted.
With this adjusted matrix pT  a syn-thetic children?s model was built.
This syntheticchildren?s model was evaluated on children?s testdata and the results were compared to the referencechildren?s model?s and the female model?s perfor-mance.When speech is evaluated in a language learningsystem, the first step is utterance verification,meaning the task of evaluating if the user actuallytried to produce the desired utterance.
The EqualError Rate (EER) on the utterance level is a meansof evaluating this performance.
For each utterancean in- and out-of-grammar likelihood score is de-termined.
The EER operating points, determinedby the cutting point of the two distributions (in-grammar and out-of-grammar), are reported as anerror metric.
Figure 1 shows the EER values of thesynthetic model applied to children?s data.Figure 1: Synthetic model?s EER performance de-pending on the power p used for model creation.It can be seen that the best performance is reachedat about p=0.25.
The overview of the results isgiven in Table 1.Equal Error RateReal Children?s Model 1.90%Male Model 4.07%Female Model 2.92%Synthetic Model 2.36%Table 1: EER numbers when using a real children?smodel compared to a male, female, and syntheticmodel for children?s data evaluation.The results show that the synthetic children?s mod-el yields good classification results when appliedto children?s data.
The gold standard, the realchildren?s model application, results in the bestEER performance.If the same evaluation scenario is applied to Span-ish, a very similar picture evolves.
Figure 2 showsthe EER results versus transformation power p forSpanish children?s data.79Figure 2: Spanish synthetic model?s EER perfor-mance depending on the power p used for modelcreation.In Figure 2 it can be seen that the optimal settingfor p is about 0.27.
This value is very similar to theone found for US English, which supports, but cer-tainly does not prove, the language independenceclaim.
Results for Spanish are given in Table 2.Equal Error RateReal Children?s model 2.40%Male model 5.62%Female model 2.17%Synthetic model 2.09%Table 2: EER numbers for Spanish when using areal children?s model compared to a male, female,and synthetic model for Spanish children?s dataevaluation.Similar to English, the Spanish synthetic modelperforms better than the female model on child-ren?s speech.
Interestingly, the acoustic modelpurely trained on children?s data performs worsethan the female and the synthetic model.
It is notclear why the children?s model does not outper-form the female and the synthetic model; an expla-nation could be diverse and variable training datathat hurts classification performance.It can be seen that for US English and Spanish thepower p used to adjust the transformation is about0.25.
Therefore, for a new language where onlyadult data is available, the transformation from themale to the female model can be estimated andapplied to the female model (after being adjustedby p=0.25).
The resulting synthetic model willwork reasonably well and could be refined as soonas children?s data becomes available.5 ConclusionThis work presented a new technique to createchildren?s acoustic models from adult acousticmodels without the need for children?s trainingdata when applied to a new language.
While it canbe assumed that the availability of children?s datawould improve the resulting acoustic models, theapproach is effective if children?s data is not avail-able.
It will be interesting to see how performanceof this technique compares to adapting adult mod-els by adaptation techniques, i.e.
MLLR, when li-mited amounts of children?s data are available.Two scenarios are possible: With increasingamount of children?s data speaker adaptation willdraw even and/or be superior.
The other possibilityis that the presented technique yields better resultsregardless how much real children?s data is availa-ble, due to the higher variability and noise-pollution of children?s data.ReferencesClaes, T., Dologlou, I, ten Bosch, L., Van Compernolle,D.
1997.
New Transformations of Cepstral Parame-ters for Automatic Vocal Tract Length Normalizationin Speech Recognition, 5th Europ.
Conf.
on SpeechComm.
and Technology, Vol.
3: 1363-1366.Cosi, P., Pellom, B.
2005.
Italian children's speech rec-ognition for advanced interactive literacy tutors.Proceedings Interspeech, Lisbon, Portugal.Elenius, D. and Blomberg, M. 2005.
Adaptation andNormalization Experiments in Speech Recognitionfor 4 to 8 Year old Children.
Proceedings Inters-peech, Lisbon, Portugal.Gustafson, J., Sj?lander, K. 2002.
Voice transformationsfor improving children?s speech recognition in a pub-licly available dialogue system.
ICSLP, Denver.Hagen, A., Pellom, B., and Cole, R. 2003.
Children'sSpeech Recognition with Application to InteractiveBooks and Tutors.
Proceedings ASRU, USA.Lee, S., Potamianos, A., and Narayanan, S. 1999.Acoustics of children's speech: Developmentalchanges of temporal and spectral parameter.
J.Acoust.
Soc.
Am., Vol.
105(3):1455-1468.Pitz, M., Ney, H. 2005.
Vocal Tract NormalizationEquals Linear Transformation in Cepstral Space.IEEE Trans.
Speech & Audio Proc., 13(5): 930-944.Potamianos, A., Narayanan, S., and Lee, S. 1997.
Auto-matic Speech Recognition for Children.
ProceedingsEurospeech, Rhodes, Greece.Hagen, A., Pellom, B., and Hacioglu, K. 2008.
Methodfor Creating a Speech Model.
US Patent Pending.80
