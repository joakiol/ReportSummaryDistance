Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1426?1436,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA Discriminative Graph-Based Parserfor the Abstract Meaning RepresentationJeffrey Flanigan Sam Thomson Jaime Carbonell Chris Dyer Noah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{jflanigan,sthomson,jgc,cdyer,nasmith}@cs.cmu.eduAbstractAbstract Meaning Representation (AMR)is a semantic formalism for which a grow-ing set of annotated examples is avail-able.
We introduce the first approachto parse sentences into this representa-tion, providing a strong baseline for fu-ture improvement.
The method is basedon a novel algorithm for finding a maxi-mum spanning, connected subgraph, em-bedded within a Lagrangian relaxation ofan optimization problem that imposes lin-guistically inspired constraints.
Our ap-proach is described in the general frame-work of structured prediction, allowing fu-ture incorporation of additional featuresand constraints, and may extend to otherformalisms as well.
Our open-source sys-tem, JAMR, is available at:http://github.com/jflanigan/jamr1 IntroductionSemantic parsing is the problem of mapping nat-ural language strings into meaning representa-tions.
Abstract Meaning Representation (AMR)(Banarescu et al, 2013; Dorr et al, 1998) is asemantic formalism in which the meaning of asentence is encoded as a rooted, directed, acyclicgraph.
Nodes represent concepts, and labeled di-rected edges represent the relationships betweenthem?see Figure 1 for an example AMR graph.The formalism is based on propositional logic andneo-Davidsonian event representations (Parsons,1990; Davidson, 1967).
Although it does notencode quantifiers, tense, or modality, the set ofsemantic phenomena included in AMR were se-lected with natural language applications?in par-ticular, machine translation?in mind.In this paper we introduce JAMR, the first pub-lished system for automatic AMR parsing.
Thesystem is based on a statistical model whose pa-rameters are trained discriminatively using anno-tated sentences in the AMR Bank corpus (Ba-narescu et al, 2013).
We evaluate using theSmatch score (Cai and Knight, 2013), establishinga baseline for future work.The core of JAMR is a two-part algorithmthat first identifies concepts using a semi-Markovmodel and then identifies the relations that ob-tain between these by searching for the maximumspanning connected subgraph (MSCG) from anedge-labeled, directed graph representing all pos-sible relations between the identified concepts.
Tosolve the latter problem, we introduce an appar-ently novel O(|V |2log |V |) algorithm that is sim-ilar to the maximum spanning tree (MST) algo-rithms that are widely used for dependency pars-ing (McDonald et al, 2005).
Our MSCG algo-rithm returns the connected subgraph with maxi-mal sum of its edge weights from among all con-nected subgraphs of the input graph.
Since AMRimposes additional constraints to ensure seman-tic well-formedness, we use Lagrangian relaxation(Geoffrion, 1974; Fisher, 2004) to augment theMSCG algorithm, yielding a tractable iterative al-gorithm that finds the optimal solution subject tothese constraints.
In our experiments, we havefound this algorithm to converge 100% of the timefor the constraint set we use.The approach can be understood as an alterna-tive to parsing approaches using graph transduc-ers such as (synchronous) hyperedge replacementgrammars (Chiang et al, 2013; Jones et al, 2012;Drewes et al, 1997), in much the same way thatspanning tree algorithms are an alternative to us-ing shift-reduce and dynamic programming algo-rithms for dependency parsing.1While a detailed1To date, a graph transducer-based semanticparser has not been published, although the Bolinastoolkit (http://www.isi.edu/publications/licensed-sw/bolinas/) contains much of the neces-sary infrastructure.1426want-01boyvisit-01cityname?New?
?York?
?City?ARG0ARG1ARG0ARG1nameop1op2 op3(a) Graph.
(w / want-01:ARG0 (b / boy):ARG1 (g / visit-01:ARG0 b:ARG1 (c / city:name (n / name:op1 "New":op2 "York":op3 "City"))))(b) AMR annotation.Figure 1: Two equivalent ways of representing the AMRparse for the sentence, ?The boy wants to visit New YorkCity.
?comparison of these two approaches is beyond thescope of this paper, we emphasize that?as hasbeen observed with dependency parsing?a diver-sity of approaches can shed light on complex prob-lems such as semantic parsing.2 Notation and OverviewOur approach to AMR parsing represents an AMRparse as a graph G = ?V,E?
; vertices and edgesare given labels from sets LVand LE, respec-tively.
G is constructed in two stages.
The firststage identifies the concepts evoked by words andphrases in an input sentence w = ?w1, .
.
.
, wn?,each wia member of vocabulary W .
The secondstage connects the concepts by adding LE-labelededges capturing the relations between concepts,and selects a root in G corresponding to the focusof the sentence w.Concept identification (?3) involves segmentingw into contiguous spans and assigning to eachspan a graph fragment corresponding to a conceptfrom a concept set denoted F (or to ?
for wordsthat evoke no concept).
In ?5 we describe howF is constructed.
In our formulation, spans arecontiguous subsequences of w. For example, thewords ?New York City?
can evoke the fragmentrepresented by(c / city:name (n / name:op1 "New":op2 "York":op3 "City"))))We use a sequence labeling algorithm to identifyconcepts.The relation identification stage (?4) is similarto a graph-based dependency parser.
Instead offinding the maximum-scoring tree over words, itfinds the maximum-scoring connected subgraphthat preserves concept fragments from the firststage, links each pair of vertices by at most oneedge, and is deterministic2with respect to a spe-cial set of edge labels L?E?
LE.
The set L?Econsists of the labels ARG0?ARG5, and does notinclude labels such as MOD or MANNER, for ex-ample.
Linguistically, the determinism constraintenforces that predicates have at most one semanticargument of each type; this is discussed in moredetail in ?4.To train the parser, spans of words must be la-beled with the concept fragments they evoke.
Al-though AMR Bank does not label concepts withthe words that evoke them, it is possible to buildan automatic aligner (?5).
The alignments areused to construct the concept lexicon and to trainthe concept identification and relation identifica-tion stages of the parser (?6).
Each stage is adiscriminatively-trained linear structured predic-tor with rich features that make use of part-of-speech tagging, named entity tagging, and depen-dency parsing.In ?7, we evaluate the parser against gold-standard annotated sentences from the AMR Bankcorpus (Banarescu et al, 2013) under the Smatchscore (Cai and Knight, 2013), presenting the firstpublished results on automatic AMR parsing.3 Concept IdentificationThe concept identification stage maps spans ofwords in the input sentence w to concept graphfragments from F , or to the empty graph fragment?.
These graph fragments often consist of justone labeled concept node, but in some cases theyare larger graphs with multiple nodes and edges.32By this we mean that, at each node, there is at most oneoutgoing edge with that label type.3About 20% of invoked concept fragments are multi-concept fragments.1427Concept identification is illustrated in Figure 2 us-ing our running example, ?The boy wants to visitNew York City.
?Let the concept lexicon be a mapping clex :W??
2Fthat provides candidate graph frag-ments for sequences of words.
(The construc-tion of F and clex is discussed below.)
Formally,a concept labeling is (i) a segmentation of winto contiguous spans represented by boundariesb, giving spans ?wb0:b1,wb1:b2, .
.
.wbk?1:bk?, withb0= 0 and bk= n, and (ii) an assignment ofeach phrase wbi?1:bito a concept graph fragmentci?
clex (wbi?1:bi) ?
?.Our approach scores a sequence of spans b anda sequence of concept graph fragments c, both ofarbitrary length k, using the following locally de-composed, linearly parameterized function:score(b, c;?)
=?ki=1?>f(wbi?1:bi, bi?1, bi, ci)(1)where f is a feature vector representation of a spanand one of its concept graph fragments in context.The features are:?
Fragment given words: Relative frequency es-timates of the probability of a concept graphfragment given the sequence of words in thespan.
This is calculated from the concept-wordalignments in the training corpus (?5).?
Length of the matching span (number of to-kens).?
NER: 1 if the named entity tagger marked thespan as an entity, 0 otherwise.?
Bias: 1 for any concept graph fragment from Fand 0 for ?.Our approach finds the highest-scoring b andc using a dynamic programming algorithm: thezeroth-order case of inference under a semi-Markov model (Janssen and Limnios, 1999).
LetS(i) denote the score of the best labeling of thefirst i words of the sentence, w0:i; it can be calcu-lated using the recurrence:S(0) = 0S(i) = maxj:0?j<i,c?clex(wj:i)??
{S(j) + ?>f(wj:i, j, i, c)}The best score will be S(n), and the best scor-ing concept labeling can be recovered using back-pointers, as in typical implementations of theViterbi algorithm.
Runtime is O(n2).clex is implemented as follows.
When clex iscalled with a sequence of words, it looks up thesequence in a table that contains, for every wordsequence that was labeled with a concept fragmentin the training data, the set of concept fragments itwas labeled with.
clex also has a set of rules forgenerating concept fragments for named entitiesand time expressions.
It generates a concept frag-ment for any entity recognized by the named entitytagger, as well as for any word sequence matchinga regular expression for a time expression.
clexreturns the union of all these concept fragments.4 Relation IdentificationThe relation identification stage adds edges amongthe concept subgraph fragments identified in thefirst stage (?3), creating a graph.
We frame thetask as a constrained combinatorial optimizationproblem.Consider the fully dense labeled multigraphD = ?VD, ED?
that includes the union of all la-beled vertices and labeled edges in the conceptgraph fragments, as well as every possible labelededge u`??
v, for all u, v ?
VDand every ` ?
LE.4We require a subgraph G = ?VG, EG?
that re-spects the following constraints:1.
Preserving: all graph fragments (including la-bels) from the concept identification phase aresubgraphs of G.2.
Simple: for any two vertices u and v ?
VG,EGincludes at most one edge between u and v. Thisconstraint forbids a small number of perfectlyvalid graphs, for example for sentences such as?John hurt himself?
; however, we see that< 1%of training instances violate the constraint.
Wefound in preliminary experiments that includingthe constraint increases overall performance.53.
Connected: G must be weakly connected (ev-ery vertex reachable from every other vertex, ig-noring the direction of edges).
This constraintfollows from the formal definition of AMR andis never violated in the training data.4.
Deterministic: For each node u ?
VG, and foreach label ` ?
L?E, there is at most one outgoingedge in EGfrom u with label `.
As discussed in?2, this constraint is linguistically motivated.4To handle numbered OP labels, we pre-process the train-ing data to convert OPN to OP, and post-process the output bynumbering the OP labels sequentially.5In future work it might be treated as a soft constraint, orthe constraint might be refined to specific cases.1428Theboywants tovisit New YorkCity?
?boywant-01visit-01cityname?New??York?
?City?nameop1op2op3Figure 2: A concept labeling for the sentence ?The boy wants to visit New York City.
?One constraint we do not include is acyclicity,which follows from the definition of AMR.
Inpractice, graphs with cycles are rarely producedby JAMR.
In fact, none of the graphs produced onthe test set violate acyclicity.Given the constraints, we seek the maximum-scoring subgraph.
We define the score to decom-pose by edges, and with a linear parameterization:score(EG;?)
=?e?EG?>g(e) (2)The features are shown in Table 1.Our solution to maximizing the score in Eq.
2,subject to the constraints, makes use of (i) an al-gorithm that ignores constraint 4 but respects theothers (?4.1); and (ii) a Lagrangian relaxation thatiteratively adjusts the edge scores supplied to (i)so as to enforce constraint 4 (?4.2).4.1 Maximum Preserving, Simple, Spanning,Connected Subgraph AlgorithmThe steps for constructing a maximum preserving,simple, spanning, connected (but not necessar-ily deterministic) subgraph are as follows.
Thesesteps ensure the resulting graph G satisfies theconstraints: the initialization step ensures the pre-serving constraint is satisfied, the pre-processingstep ensures the graph is simple, and the core al-gorithm ensures the graph is connected.1.
(Initialization) Let E(0)be the union of theconcept graph fragments?
weighted, labeled, di-rected edges.
Let V denote its set of vertices.Note that ?V,E(0)?
is preserving (constraint 4),as is any graph that contains it.
It is also sim-ple (constraint 4), assuming each concept graphfragment is simple.2.
(Pre-processing) We form the edge set E by in-cluding just one edge from EDbetween eachpair of nodes:?
For any edge e = u`??
v in E(0), include e inE, omitting all other edges between u and v.?
For any two nodes u and v, include only thehighest scoring edge between u and v.Note that without the deterministic constraint,we have no constraints that depend on the labelof an edge, nor its direction.
So it is clear thatthe edges omitted in this step could not be partof the maximum-scoring solution, as they couldbe replaced by a higher scoring edge without vi-olating any constraints.Note also that because we have kept exactly oneedge between every pair of nodes, ?V,E?
is sim-ple and connected.3.
(Core algorithm) Run Algorithm 1, MSCG, on?V,E?
and E(0).
This algorithm is a (to ourknowledge novel) modification of the minimumspanning tree algorithm of Kruskal (1956).Note that the directions of edges do not matterfor MSCG.Steps 1?2 can be accomplished in one passthrough the edges, with runtime O(|V |2).
MSCGcan be implemented efficiently in O(|V |2log |V |)time, similarly to Kruskal?s algorithm, using adisjoint-set data structure to keep track of con-nected components.6The total asymptotic runtimecomplexity is O(|V |2log |V |).The details of MSCG are given in Algorithm 1.In a nutshell, MSCG first adds all positive edges tothe graph, and then connects the graph by greedilyadding the least negative edge that connects twopreviously unconnected components.Theorem 1.
MSCG finds a maximum spanning,connected subgraph of ?V,E?Proof.
We closely follow the original proof of cor-rectness of Kruskal?s algorithm.
We first show byinduction that, at every iteration of MSCG, thereexists some maximum spanning, connected sub-graph that contains G(i)= ?V,E(i)?
:6For dense graphs, Prim?s algorithm (Prim, 1957) isasymptotically faster (O(|V |2)).
We conjecture that usingPrim?s algorithm instead of Kruskall?s to connect the graphcould improve the runtime of MSCG.1429Name DescriptionLabel For each ` ?
LE, 1 if the edge has that labelSelf edge 1 if the edge is between two nodes in the same fragmentTail fragment root 1 if the edge?s tail is the root of its graph fragmentHead fragment root 1 if the edge?s head is the root of its graph fragmentPath Dependency edge labels and parts of speech on the shortest syntactic path between any twowords in the two spansDistance Number of tokens (plus one) between the two concepts?
spans (zero if the same)Distance indicators A feature for each distance value, that is 1 if the spans are of that distanceLog distance Logarithm of the distance feature plus one.Bias 1 for any edge.Table 1: Features used in relation identification.
In addition to the features above, the following conjunctions are used (Tail andHead concepts are elements of LV): Tail concept ?
Label, Head concept ?
Label, Path ?
Label, Path ?
Head concept, Path ?Tail concept, Path ?
Head concept ?
Label, Path ?
Tail concept ?
Label, Path ?
Head word, Path ?
Tail word, Path ?
Headword ?
Label, Path ?
Tail word ?
Label, Distance ?
Label, Distance ?
Path, and Distance ?
Path ?
Label.
To conjoin thedistance feature with anything else, we multiply by the distance.input : weighted, connected graph ?V,E?and set of edges E(0)?
E to bepreservedoutput: maximum spanning, connectedsubgraph of ?V,E?
that preservesE(0)let E(1)= E(0)?
{e ?
E | ?>g(e) > 0};create a priority queue Q containing{e ?
E | ?>g(e) ?
0} prioritized by scores;i = 1;while Q nonempty and ?V,E(i)?
is not yetspanning and connected doi = i+ 1;E(i)= E(i?1);e = argmaxe??Q?>g(e?
);remove e from Q;if e connects two previously unconnectedcomponents of ?V,E(i)?
thenadd e to E(i)endendreturn G = ?V,E(i)?
;Algorithm 1: MSCG algorithm.Base case: ConsiderG(1), the subgraph contain-ing E(0)and every positive edge.
Take any maxi-mum preserving spanning connected subgraph Mof ?V,E?.
We know that such an M exists be-cause ?V,E?
itself is a preserving spanning con-nected subgraph.
Adding a positive edge to Mwould strictly increase M ?s score without discon-necting M , which would contradict the fact thatM is maximal.
Thus M must contain G(1).Induction step: By the inductive hypothesis,there exists some maximum spanning connectedsubgraph M = ?V,EM?
that contains G(i).Let e be the next edge added to E(i)by MSCG.If e is in EM, then E(i+1)= E(i)?
{e} ?
EM,and the hypothesis still holds.Otherwise, since M is connected and does notcontain e, EM?
{e} must have a cycle containinge.
In addition, that cycle must have some edge e?that is not in E(i).
Otherwise, E(i)?
{e} wouldcontain a cycle, and e would not connect two un-connected components of G(i), contradicting thefact that e was chosen by MSCG.Since e?is in a cycle in EM?
{e}, removing itwill not disconnect the subgraph, i.e.
(EM?{e})\{e?}
is still connected and spanning.
The score ofe is greater than or equal to the score of e?, oth-erwise MSCG would have chosen e?instead of e.Thus, ?V, (EM?
{e}) \ {e?}?
is a maximum span-ning connected subgraph that containsE(i+1), andthe hypothesis still holds.When the algorithm completes, G = ?V,E(i)?is a spanning connected subgraph.
The maximumspanning connected subgraph M that contains itcannot have a higher score, because G containsevery positive edge.
Hence G is maximal.4.2 Lagrangian RelaxationIf the subgraph resulting from MSCG satisfies con-straint 4 (deterministic) then we are done.
Oth-erwise we resort to Lagrangian relaxation (LR).Here we describe the technique as it applies to ourtask, referring the interested reader to Rush andCollins (2012) for a more general introduction toLagrangian relaxation in the context of structuredprediction problems.In our case, we begin by encoding a graph G =?VG, EG?
as a binary vector.
For each edge e inthe fully dense multigraph D, we associate a bi-1430nary variable ze= 1{e ?
EG}, where 1{P} isthe indicator function, taking value 1 if the propo-sition P is true, 0 otherwise.
The collection of zeform a vector z ?
{0, 1}|ED|.Determinism constraints can be encoded as aset of linear inequalities.
For example, the con-straint that vertex u has no more than one outgoingARG0 can be encoded with the inequality:?v?V1{uARG0????
v ?
EG} =?v?VzuARG0????v?
1.All of the determinism constraints can collectivelybe encoded as one system of inequalities:Az ?
b,with each row AiinA and its corresponding entrybiin b together encoding one constraint.
For theprevious example we have a row Aithat has 1sin the columns corresponding to edges outgoingfrom u with label ARG0 and 0?s elsewhere, and acorresponding element bi= 1 in b.The score of graph G (encoded as z) can bewritten as the objective function ?>z, where ?e=?>g(e).
To handle the constraint Az ?
b, we in-troduce multipliers ?
?
0 to get the Lagrangianrelaxation of the objective function:L?
(z) = maxz (?>z+ ?>(b?Az)),z??
= argmaxz L?
(z).And the dual objective:L(z) = min??0L?
(z),z?= argmaxzL(z).Conveniently, L?
(z) decomposes over edges:L?
(z) = maxz (?>z+ ?>(b?Az))= maxz(?>z?
?>Az)= maxz((??A>?
)>z).So for any ?, we can find z?
?by assigning edgesthe new Lagrangian adjusted weights ?
?
A>?and reapplying the algorithm described in ?4.1.We can find z?by projected subgradient descent,by starting with ?
= 0, and taking steps in thedirection:??L???(z??)
= Az?
?.If any components of ?
are negative after taking astep, they are set to zero.L(z) is an upper bound on the unrelaxed ob-jective function ?>z, and is equal to it if andonly if the constraints Az ?
b are satisfied.
IfL(z?)
= ?>z?, then z?is also the optimal solu-tion to the constrained solution.
Otherwise, thereexists a duality gap, and Lagrangian relaxationhas failed.
In that case we still return the sub-graph encoded by z?, even though it might vio-late one or more constraints.
Techniques from in-teger programming such as branch-and-bound orcutting-planes methods could be used to find anoptimal solution when LR fails (Das et al, 2012),but we do not use these techniques here.
In ourexperiments, with a stepsize of 1 and max numberof steps as 500, Lagrangian relaxation succeeds100% of the time in our data.4.3 Focus IdentificationIn AMR, one node must be marked as the focus ofthe sentence.
We notice this can be accomplishedwithin the relation identification step: we add aspecial concept node root to the dense graph D,and add an edge from root to every other node,giving each of these edges the label FOCUS.
Werequire that root have at most one outgoing FO-CUS edge.
Our system has two feature types forthis edge: the concept it points to, and the shortestdependency path from a word in the span to theroot of the dependency tree.5 Automatic AlignmentsIn order to train the parser, we need alignments be-tween sentences in the training data and their an-notated AMR graphs.
More specifically, we needto know which spans of words invoke which con-cept fragments in the graph.
To do this, we builtan automatic aligner and tested its performance ona small set of alignments we annotated by hand.The automatic aligner uses a set of rules togreedily align concepts to spans.
The list of rulesis given in Table 2.
The aligner proceeds downthe list, first aligning named-entities exactly, thenfuzzy matching named-entities, then date-entities,etc.
For each rule, an entire pass through the AMRgraph is done.
The pass considers every concept inthe graph and attempts to align a concept fragmentrooted at that concept if the rule can apply.
Somerules only apply to a particular type of conceptfragment, while others can apply to any concept.For example, rule 1 can apply to any NAME con-cept and its OP children.
It searches the sentence1431for a sequence of words that exactly matches itsOP children and aligns them to the NAME and OPchildren fragment.Concepts are considered for alignment in the or-der they are listed in the AMR annotation (left toright, top to bottom).
Concepts that are not alignedin a particular pass may be aligned in subsequentpasses.
Concepts are aligned to the first match-ing span, and alignments are mutually exclusive.Once aligned, a concept in a fragment is never re-aligned.7However, more concepts can be attachedto the fragment by rules 8?14.We use WordNet to generate candidate lemmas,and we also use a fuzzy match of a concept, de-fined to be a word in the sentence that has thelongest string prefix match with that concept?s la-bel, if the match length is ?
4.
If the match lengthis < 4, then the concept has no fuzzy match.
Forexample the fuzzy match for ACCUSE-01 could be?accusations?
if it is the best match in the sen-tence.
WordNet lemmas and fuzzy matches areonly used if the rule explicitly uses them.
All to-kens and concepts are lowercased before matchesor fuzzy matches are done.On the 200 sentences of training data wealigned by hand, the aligner achieves 92% preci-sion, 89% recall, and 90% F1for the alignments.6 TrainingWe now describe how to train the two stages of theparser.
The training data for the concept identifi-cation stage consists of (X,Y ) pairs:?
Input: X , a sentence annotated with namedentities (person, organization, location, mis-ciscellaneous) from the Illinois Named EntityTagger (Ratinov and Roth, 2009), and part-of-speech tags and basic dependencies from theStanford Parser (Klein and Manning, 2003; deMarneffe et al, 2006).?
Output: Y , the sentence labeled with conceptsubgraph fragments.The training data for the relation identificationstage consists of (X,Y ) pairs:7As an example, if ?North Korea?
shows up twice inthe AMR graph and twice in the input sentence, then thefirst ?North Korea?
concept fragment listed in the AMR getsaligned to the first ?North Korea?
mention in the sentence,and the second fragment to the second mention (because thefirst span is already aligned when the second ?North Korea?concept fragment is considered, so it is aligned to the secondmatching span).1.
(Named Entity) Applies to name concepts and theiropn children.
Matches a span that exactly matches itsopn children in numerical order.2.
(Fuzzy Named Entity) Applies to name concepts andtheir opn children.
Matches a span that matches thefuzzy match of each child in numerical order.3.
(Date Entity) Applies to date-entity conceptsand their day, month, year children (if exist).Matches any permutation of day, month, year, (two digitor four digit years), with or without spaces.4.
(Minus Polarity Tokens) Applies to - concepts, andmatches ?no?, ?not?, ?non.?5.
(Single Concept) Applies to any concept.
Stripsoff trailing ?-[0-9]+?
from the concept (for examplerun-01 ?
run), and matches any exact matchingword or WordNet lemma.6.
(Fuzzy Single Concept) Applies to any concept.Strips off trailing ?-[0-9]+?, and matches the fuzzy matchof the concept.7.
(U.S.) Applies to name if its op1 child is unitedand its op2 child is states.
Matches a word thatmatches ?us?, ?u.s.?
(no space), or ?u.
s.?
(with space).8.
(Entity Type) Applies to concepts with an outgoingname edge whose head is an aligned fragment.
Up-dates the fragment to include the unaligned concept.Ex: continent in (continent :name (name:op1 "Asia")) aligned to ?asia.?9.
(Quantity) Applies to .
*-quantity concepts withan outgoing unit edge whose head is aligned.
Up-dates the fragment to include the unaligned concept.
Ex:distance-quantity in (distance-quantity:unit kilometer) aligned to ?kilometres.?10.
(Person-Of, Thing-Of) Applies to person andthing concepts with an outgoing .
*-of edge whosehead is aligned.
Updates the fragment to includethe unaligned concept.
Ex: person in (person:ARG0-of strike-02) aligned to ?strikers.?11.
(Person) Applies to person concepts with a sin-gle outgoing edge whose head is aligned.
Updatesthe fragment to include the unaligned concept.
Ex:person in (person :poss (country :name(name :op1 "Korea")))12.
(Goverment Organization) Applies to conceptswith an incoming ARG.
*-of edge whose tail is analigned government-organization concept.
Up-dates the fragment to include the unaligned concept.
Ex:govern-01 in (government-organization:ARG0-of govern-01) aligned to ?government.?13.
(Minus Polarity Prefixes) Applies to - conceptswith an incoming polarity edge whose tail is alignedto a word beginning with ?un?, ?in?, or ?il.?
Up-dates the fragment to include the unaligned concept.Ex: - in (employ-01 :polarity -) aligned to?unemployment.?14.
(Degree) Applies to concepts with an incomingdegree edge whose tail is aligned to a word endingis ?est.?
Updates the fragment to include the unalignedconcept.
Ex: most in (large :degree most)aligned to ?largest.
?Table 2: Rules used in the automatic aligner.1432?
Input: X , the sentence labeled with graph frag-ments, as well as named enties, POS tags, andbasic dependencies as in concept identification.?
Output: Y , the sentence with a full AMRparse.8Alignments are used to induce the concept label-ing for the sentences, so no annotation beyond theautomatic alignments is necessary.We train the parameters of the stages separatelyusing AdaGrad (Duchi et al, 2011) with the per-ceptron loss function (Rosenblatt, 1957; Collins,2002).
We give equations for concept identifica-tion parameters ?
and features f(X,Y ).
For asentence of length k, and spans b labeled with asequence of concept fragments c, the features are:f(X,Y ) =?ki=1f(wbi?1:bi, bi?1, bi, ci)To train with AdaGrad, we process examples inthe training data ((X1, Y1), .
.
.
, (XN, YN)) oneat a time.
At time t, we decode (?3) to get?Ytandcompute the subgradient:st= f(Xt,?Yt)?
f(Xt, Yt)We then update the parameters and go to the nextexample.
Each component i of the parameter vec-tor gets updated like so:?t+1i= ?ti????tt?=1st?isti?
is the learning rate which we set to 1.
Forrelation identification training, we replace ?
andf(X,Y ) in the above equations with ?
andg(X,Y ) =?e?EGg(e).We ran AdaGrad for ten iterations for conceptidentification, and five iterations for relation iden-tification.
The number of iterations was chosen byearly stopping on the development set.7 ExperimentsWe evaluate our parser on the newswire sectionof LDC2013E117 (deft-amr-release-r3-proxy.txt).Statistics about this corpus and our train/dev./testsplits are given in Table 3.8Because the alignments are automatic, some conceptsmay not be aligned, so we cannot compute their features.
Weremove the unaligned concepts and their edges from the fullAMR graph for training.
Thus some graphs used for trainingmay in fact be disconnected.Split Document Years Sentences TokensTrain 1995-2006 4.0k 79kDev.
2007 2.1k 40kTest 2008 2.1k 42kTable 3: Train/dev./test split.Train TestP R F1P R F1.92 .90 .91 .90 .79 .84Table 4: Concept identification performance.For the performance of concept identification,we report precision, recall, and F1of labeled spansusing the induced labels on the training and testdata as a gold standard (Table 4).
Our conceptidentifier achieves 84% F1on the test data.
Pre-cision is roughly the same between train and test,but recall is worse on test, implicating unseen con-cepts as a significant source of errors on test data.We evaluate the performance of the full parserusing Smatch v1.0 (Cai and Knight, 2013), whichcounts the precision, recall and F1of the conceptsand relations together.
Using the full pipeline(concept identification and relation identificationstages), our parser achieves 58% F1on the testdata (Table 5).
Using gold concepts with the re-lation identification stage yields a much higherSmatch score of 80% F1.
As a comparison, AMRBank annotators have a consensus inter-annotatoragreement Smatch score of 83% F1.
The runtimeof our system is given in Figure 3.The large drop in performance of 22% F1whenmoving from gold concepts to system conceptssuggests that joint inference and training for thetwo stages might be helpful.8 Related WorkOur approach to relation identification is inspiredby graph-based techniques for non-projective syn-tactic dependency parsing.
Minimum span-ning tree algorithms?specifically, the optimumbranching algorithm of Chu and Liu (1965) andEdmonds (1967)?were first used for dependencyparsing by McDonald et al (2005).
Later ex-Train Testconcepts P R F1P R F1gold .85 .95 .90 .76 .84 .80automatic .69 .78 .73 .52 .66 .58Table 5: Parser performance.14330 10 20 30 400.00.10.20.30.40.5sentence length (words)average runtime (seconds)Figure 3: Runtime of JAMR (all stages).tensions allow for higher-order (non?edge-local)features, often making use of relaxations to solvethe NP-hard optimization problem.
Mcdonald andPereira (2006) incorporated second-order features,but resorted to an approximate algorithm.
Oth-ers have formulated the problem as an integer lin-ear program (Riedel and Clarke, 2006; Martins etal., 2009).
TurboParser (Martins et al, 2013) usesAD3(Martins et al, 2011), a type of augmentedLagrangian relaxation, to integrate third-order fea-tures into a CLE backbone.
Future work might ex-tend JAMR to incorporate additional linguisticallymotivated constraints and higher-order features.The task of concept identification is similar inform to the problem of Chinese word segmenta-tion, for which semi-Markov models have success-fully been used to incorporate features based onentire spans (Andrew, 2006).While all semantic parsers aim to transform nat-ural language text to a formal representation ofits meaning, there is wide variation in the mean-ing representations and parsing techniques used.Space does not permit a complete survey, but wenote some connections on both fronts.Interlinguas (Carbonell et al, 1992) are an im-portant precursor to AMR.
Both formalisms areintended for use in machine translation, but AMRhas an admitted bias toward the English language.First-order logic representations (and exten-sions using, e.g., the ?-calculus) allow variablequantification, and are therefore more power-ful.
In recent research, they are often associ-ated with combinatory categorial grammar (Steed-man, 1996).
There has been much work on sta-tistical models for CCG parsing (Zettlemoyer andCollins, 2005; Zettlemoyer and Collins, 2007;Kwiatkowski et al, 2010, inter alia), usually usingchart-based dynamic programming for inference.Natural language interfaces for queryingdatabases have served as another driving applica-tion (Zelle and Mooney, 1996; Kate et al, 2005;Liang et al, 2011, inter alia).
The formalismsused here are richer in logical expressiveness thanAMR, but typically use a smaller set of concepttypes?only those found in the database.In contrast, semantic dependency parsing?inwhich the vertices in the graph correspond to thewords in the sentence?is meant to make semanticparsing feasible for broader textual domains.
Al-shawi et al (2011), for example, use shift-reduceparsing to map sentences to natural logical form.AMR parsing also shares much in commonwith tasks like semantic role labeling and frame-semantic parsing (Gildea and Jurafsky, 2002; Pun-yakanok et al, 2008; Das et al, 2014, inter alia).In these tasks, predicates are often disambiguatedto a canonical word sense, and roles are filledby spans (usually syntactic constituents).
Theyconsider each predicate separately, and producea disconnected set of shallow predicate-argumentstructures.
AMR, on the other hand, canonical-izes both predicates and arguments to a commonconcept label space.
JAMR reasons about all con-cepts jointly to produce a unified representation ofthe meaning of an entire sentence.9 ConclusionWe have presented the first published system forautomatic AMR parsing, and shown that it pro-vides a strong baseline based on the Smatch eval-uation metric.
We also present an algorithm forfinding the maximum, spanning, connected sub-graph and show how to incorporate extra con-straints with Lagrangian relaxation.
Our feature-based learning setup allows the system to be easilyextended by incorporating new feature sources.AcknowledgmentsThe authors gratefully acknowledge helpful cor-respondence from Kevin Knight, Ulf Hermjakob,and Andr?e Martins, and helpful feedback fromNathan Schneider, Brendan O?Connor, WaleedAmmar, and the anonymous reviewers.
Thiswork was sponsored by the U. S. Army ResearchLaboratory and the U. S. Army Research Officeunder contract/grant number W911NF-10-1-0533and DARPA grant FA8750-12-2-0342 funded un-der the DEFT program.1434ReferencesHiyan Alshawi, Pi-Chuan Chang, and Michael Ring-gaard.
2011.
Deterministic statistical mapping ofsentences to underspecified semantics.
In Proc.
ofICWS.Galen Andrew.
2006.
A hybrid markov/semi-markovconditional random field for sequence segmentation.In Proc.
of EMNLP.Laura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
2013.
Abstract meaning representationfor sembanking.
In Proc.
of the Linguistic Annota-tion Workshop and Interoperability with Discourse.Shu Cai and Kevin Knight.
2013.
Smatch: an eval-uation metric for semantic feature structures.
InProc.
of ACL.Jaime G. Carbonell, Teruko Mitamura, and Eric H. Ny-berg.
1992.
The KANT perspective: A critiqueof pure transfer (and pure interlingua, pure trans-fer, .
.
.
).
In Proc.
of the Fourth International Con-ference on Theoretical and Methodological Issuesin Machine Translation: Empiricist vs. RationalistMethods in MT.David Chiang, Jacob Andreas, Daniel Bauer,Karl Moritz Hermann, Bevan Jones, and KevinKnight.
2013.
Parsing graphs with hyperedgereplacement grammars.
In Proc.
of ACL.Y.
J. Chu and T. H. Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14:1396?1400.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and ex-periments with perceptron algorithms.
In Proc.
ofEMNLP.Dipanjan Das, Andr?e F. T. Martins, and Noah A. Smith.2012.
An exact dual decomposition algorithm forshallow semantic parsing with constraints.
In Proc.of the Joint Conference on Lexical and Computa-tional Semantics.Dipanjan Das, Desai Chen, Andr?e F. T. Martins,Nathan Schneider, and Noah A. Smith.
2014.Frame-semantic parsing.
Computational Linguis-tics, 40(1):9?56.Donald Davidson.
1967.
The logical form of actionsentences.
In Nicholas Rescher, editor, The Logic ofDecision and Action, pages 81?120.
Univ.
of Pitts-burgh Press.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProc.
of LREC.Bonnie Dorr, Nizar Habash, and David Traum.
1998.A thematic hierarchy for efficient generation fromlexical-conceptual structure.
In David Farwell, Lau-rie Gerber, and Eduard Hovy, editors, MachineTranslation and the Information Soup: Proc.
ofAMTA.Frank Drewes, Hans-J?org Kreowski, and Annegret Ha-bel.
1997.
Hyperedge replacement graph gram-mars.
In Handbook of Graph Grammars, pages 95?162.
World Scientific.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
Journal of MachineLearning Research, 12:2121?2159, July.Jack Edmonds.
1967.
Optimum branchings.
NationalBureau of Standards.Marshall L. Fisher.
2004.
The Lagrangian relaxationmethod for solving integer programming problems.Management Science, 50(12):1861?1871.Arthur M Geoffrion.
1974.
Lagrangean relaxation forinteger programming.
Springer.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Jacques Janssen and Nikolaos Limnios.
1999.
Semi-Markov Models and Applications.
Springer, Octo-ber.Bevan Jones, Jacob Andreas, Daniel Bauer,Karl Moritz Hermann, and Kevin Knight.
2012.Semantics-based machine translation with hyper-edge replacement grammars.
In Proc.
of COLING.Rohit J. Kate, Yuk Wah Wong, and Raymond J.Mooney.
2005.
Learning to transform natural toformal languages.
In Proc.
of AAAI.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proc.
of ACL.Joseph B. Kruskal.
1956.
On the shortest spanningsubtree of a graph and the traveling salesman prob-lem.
Proc.
of the American Mathematical Society,7(1):48.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing probabilis-tic CCG grammars from logical form with higher-order unification.
In Proc.
of EMNLP.Percy Liang, Michael I. Jordan, and Dan Klein.
2011.Learning dependency-based compositional seman-tics.
In Proc.
of ACL.Andr?e F. T. Martins, Noah A. Smith, and Eric P. Xing.2009.
Concise integer linear programming formula-tions for dependency parsing.
In Proc.
of ACL.1435Andr?e F. T. Martins, Noah A. Smith, Pedro M. Q.Aguiar, and M?ario A. T. Figueiredo.
2011.
Dualdecomposition with many overlapping components.In Proc.
of EMNLP.Andr?e F. T. Martins, Miguel Almeida, and Noah A.Smith.
2013.
Turning on the turbo: Fast third-ordernon-projective Turbo parsers.
In Proc.
of ACL.Ryan Mcdonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proc.
of EACL, page 81?88.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Haji?c.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proc.
ofEMNLP.Terence Parsons.
1990.
Events in the Semantics of En-glish: A study in subatomic semantics.
MIT Press.Robert C. Prim.
1957.
Shortest connection networksand some generalizations.
Bell System TechnologyJournal, 36:1389?1401.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2):257?287.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProc.
of CoNLL.Sebastian Riedel and James Clarke.
2006.
Incrementalinteger linear programming for non-projective de-pendency parsing.
In Proc.
of EMNLP.Frank Rosenblatt.
1957.
The perceptron?a perceivingand recognizing automaton.
Technical Report 85-460-1, Cornell Aeronautical Laboratory.Alexander M. Rush and Michael Collins.
2012.
Atutorial on dual decomposition and Lagrangian re-laxation for inference in natural language process-ing.
Journal of Artificial Intelligence Research,45(1):305?-362.Mark Steedman.
1996.
Surface structure and interpre-tation.
Linguistic inquiry monographs.
MIT Press.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In Proc.
of AAAI.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In Proc.
of UAI.Luke S. Zettlemoyer and Michael Collins.
2007.
On-line learning of relaxed CCG grammars for parsingto logical form.
In In Proc.
of EMNLP-CoNLL.1436
