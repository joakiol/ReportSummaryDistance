Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 820?830,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsAnalysing recall loss in named entity slot fillingGlen Pink Joel Nothman James R. Currane-lab, School of Information TechnologiesUniversity of SydneyNSW 2006, Australia{glen.pink,joel.nothman,james.r.curran}@sydney.edu.auAbstractState-of-the-art fact extraction is heavilyconstrained by recall, as demonstrated byrecent performance in TAC Slot Filling.We isolate this recall loss for NE slots bysystematically analysing each stage of theslot filling pipeline as a filter over correctanswers.
Recall is critical as candidatesnever generated can never be recovered,whereas precision can always be increasedin downstream processing.We provide precise, empirical confirma-tion of previously hypothesised sources ofrecall loss in slot filling.
While NE typeconstraints substantially reduce the searchspace with only a minor recall penalty, wefind that 10% to 39% of slot fills will beentirely ignored by most systems.
One insix correct answers are lost if coreferenceis not used, but this can be mostly retainedby simple name matching rules.1 IntroductionThe TAC Knowledge Base Population (KBP) SlotFilling (SF) consists of extracting named attributesfrom text.
Given a query, e.g.
John Kerry, a systemsearches a corpus for documents which contain theentity.
It then fills a list of slots, named attributessuch as (per:spouse, Teresa Heinz).The top TAC SF 2013 (TAC13) system scored37.3% F-score (Roth et al., 2013), and the medianF-score was 16.9% (Surdeanu, 2013).
Recall forSF systems is especially low, with many systemsusing precise extractors with low recall.
Precisionranges from 9% to 40% greater than recall for thetop 5 systems in TAC13, and unsurprisingly, Rothet al.
(2013) has the highest recall at 33%.
Closingthe recall gap without substantially increasing thesearch space is critical to improving SF results.Ji and Grishman (2011) and Min and Grishman(2012) identify many of the challenges of SF, andsuggest that inference, coreference and named en-tity recognition (NER) are key sources of error.Min and Grishman categorise the slot fills foundby human annotators but not found in the aggre-gated output of all systems.
However, this ap-proach only allows them to hypothesise the likelysource of recall loss.
For instance, it is impossibleto distinguish candidate generation errors from an-swer merging errors.
Roth et al.
(2014) categorisethese errors at a high level, without specific anal-ysis of candidate generation pipeline componentssuch as coreference.In this paper, we take this analysis further byperforming a systematic recall analysis that al-lows us to pinpoint the cause of every recall er-ror (candidates lost that can never be recovered)and estimate upper bounds on recall in existing ap-proaches.
We implement a collection of na?
?ve SFsystems utilizing a set of increasingly restrictivefilters over documents and named entities (NEs).TAC has three slot types: NE, string and value slots.We consider only those slots filled by NEs as thereare widely-used, high accuracy tools available forNER, and focusing on NEs only allows us to pre-cisely gauge performance of filters.
String slots donot have reliable classifiers, and value slots requiremore normalisation than directly returning a tokenspan.
Otherwise, this evaluation is not specificallydependent on the nature of NEs, and we expectsimilar results for other slot types.We focus on systems which first generate can-didates and then process them, the approach of themajority of TAC systems.
Our filters apply hardconstraints over NEs commonly used in the litera-ture, accounting for a typical SF candidate genera-tion pipeline?matching the query term, the formof candidate fills and the distance between thequery and the candidate?but not performing anyfurther scoring or thresholding.
We compare sev-820eral forms of coreference as filters, motivated bythe need for efficient coreference resolution whenprocessing large corpora.
Complementing theseunsupervised experiments, we implement a max-imum recall bootstrap to identify which fills arereachable from training data.We find ?10% of recall is ignored by most sys-tems due to NER bounds errors, and despite state-of-the-art coreference, 8% is lost when queriesand fills occur in different sentences.
Using NEtype constraints is very effective, reducing recallby only 2% for a search space reduction of 81%.Without any coreference, 16% of typed fills arelost, but 12% of this recall can be recovered us-ing fast na?
?ve name matching rules, reducing thesearch space to 59% that of full coreference.
15%of recall is lost if a SF approach, such as a boot-strapping, requires that dependency paths be non-unique in a corpus.
We show that most remainingcandidates are reachable via bootstrapping froma small number of seeds.
Our results providesystematic confirmation that effective coreferenceand NER are critical to high recall slot filling.2 Why focus on recall?In this work, we determine the recall loss causedby candidate generation constraints in SF systems.SF pipelines are typically implemented using acoarse-to-fine approach, where all possible candi-dates are generated and then filtered by hard con-straints and more sophisticated downstream pro-cesses.
Following this, we maximally generatecandidates and assume a high-precision but rela-tively costly downstream process selects the finalextractions.
While ultimately any system makesprecision-recall trade-offs, the recall of a system?scoarse candidate generation process sets a hardupper bound on performance, as candidates thatare not generated at all can never be recovered bydownstream processes.
SF systems could gener-ate every noun phrase in a corpus as potential can-didates, but they apply hard candidate generationconstraints for efficiency and precision.We implement these hard constraints as a se-ries of filters, and return every candidate whichpasses a filter without further ranking or threshold-ing.
These filters are comprised of generic com-ponents, such as NER, which are representative ofSF pipelines.
We are only interested in precisionin so much as it corresponds to the size of thesearch space (the candidates generated), assum-ing a small, fixed number of answers.
The searchspace determines the workload of later stages re-sponsible for extraction, merging and ranking.Precision can be improved by this post-processingof the candidate set, but recall cannot.3 BackgroundSlot filling (SF) is a query-oriented relation ex-traction (RE) task in the Knowledge Base Popu-lation (KBP) track of the Text Analysis Confer-ences (TAC) (McNamee and Dang, 2009).
A SFsystem is queried with a name and a predefinedrelation schema, or slots, and must seek instancesof any relations involving the query entity, and thecorresponding slot fills, from a corpus.Systems typically consist of several pipelinedstages (Ji et al., 2011), providing many potentiallocations for error.
The basic pipeline, in Fig-ure 1, consists of four stages (Ji and Grishman,2011): document retrieval, candidate generation,answer extraction, and answer merging and rank-ing.
The output of the second stage is a set of can-didates which are then usually ranked using REtechniques,1to precisely pinpoint answers.
TACpenalises redundant responses, requiring a finalanswer merging and ranking stage.
The first twostages are the focus of this work, as they inad-vertently filter correct answers that cannot be re-covered, and they determine the size of the searchspace for later stages.Min and Grishman (2012) conducted an analy-sis of the 140 TAC 2010 SF fills that were found byhuman annotators but not any system, and manu-ally look for evidence in the reference documentand categorise the hypothetical sources of error.They find inference, coreference and NER to bethe top sources of error, and that the most studiedcomponent (sentence-level RE) is not the domi-nant problem, contributing only 10% of recall loss.We precisely characterise the contribution of thesesources of error.We follow the SF literature in adopting RE tech-niques for filtering candidates.
RE focuses onidentifying relations between entities (or attributesof entities) as mentioned in text.
Both relationschema and training data are often provided, andextraction is done using learnt classifiers (Mintzet al., 2009; Surdeanu et al., 2012; Riedel et al.,1We note that question answering techniques have beenused directly by SF systems (Byrne and Dunnion, 2011) butRE techniques are the primary method for answer extraction.821answer extractionanswer merging and rankingdocumentretrievalaliasmatchexactmatchoracledocscandidate generationNEssentence filtercorefNNPcorefNNPnaivedependency filterslengthtypesnon-uniquenocorefNPn-gramsFigure 1: Candidate filters within the standard SF pipeline.
Arrows indicate a sequence of filters.2013; Zhang et al., 2013) or semi-supervised tech-niques (Agichtein and Gravano, 2000; Wang et al.,2011; Carlson et al., 2010).Relation phrases or patterns may be identifiedwithout labels (Fader et al., 2011; Mausam et al.,2012) or clustered (Yao et al., 2012) into types.Generating candidate entity pairs and using thesyntactic or surface path between them to decidewhether a relation exists are common threads inRE that also form part of the SF pipeline.
In someRE tasks, entities mentioned may already be iden-tified in a document and provided to a RE sys-tem; in general, automatic NER is required.
Sometasks are defined more generally to include com-mon noun phrases (Fader et al., 2011; Carlson etal., 2010).
SF specifically includes slots that canbe filled by arbitrary strings such as per:causeof death, which make up a large number ofslot fills but may require the use of different tech-niques for extraction, separate from names.
NERmay be further enhanced by resolving names toa KB (Mintz et al., 2009; Hoffmann et al., 2011;Surdeanu et al., 2012; Wang et al., 2011), reduc-ing noise in learning and extraction processes, butwe do not take this step in this work.Typically, a RE system will only consider enti-ties mentioned together in a sentence.
When seek-ing all instances of a given relation between knownentities, coreference resolution is necessary to sub-stantially expand the set of candidate pairs (Gab-bard et al., 2011).
Coreference resolution maynot be necessary where each relation is redun-dantly mentioned in a large corpus, as in SF; inthis vein, ?Open?
approaches prefer precision andavoid automatic coreference resolution (Banko etal., 2007).
Moreover, previous analysis attributedsubstantial SF error to these tools (Ji and Grish-man, 2011).
Our work evaluates NER, localityheuristics and coreference within a SF context.Classification features for RE typically encode:attributes of the entities; the surface form, depen-dency path, or phrase structure subtree betweenthem; and surrounding context (Zhou et al., 2005;Mintz et al., 2009; Zhang et al., 2013).
We eval-uate the length of dependency path between enti-ties as a variable affecting SF candidate recall, andapply na?
?ve entity pair bootstrapping (Brin, 1998;Agichtein and Gravano, 2000) to assess the gener-alisation over dependency paths from examples.4 Experimental setupWe begin with a set of queries (a query being aNE entity grounded in a mention in a document)and, for each query q, the documents Dqknown tocontain any slot fill for q, as determined by oracleinformation retrieval (IR) from human annotationand judged system output.
Filling every slot in qwith every n-gram in Dqconstitutes a system withnearly perfect recall.
We apply a series of increas-ingly restrictive filters over this set.
As in Figure 1,SF systems in practice must retrieve relevant docu-ments and generate candidates.
We propose filtersthat allow for analysis of recall lost during thesestages.
We ignore the remaining stages and evalu-ate the set of candidates directly.Filters define what documents or NEs are al-lowed to pass through, based on constraints im-posed by query matching, entity form, and sen-tence and syntactic context.
We combine these fil-ters in series in a number of configurations.
Theuse or absence of coreference varies across ourconfigurations, as the need to identify the querymention and terms that refer to the query mentionis critical.
Finally, we experiment with a boot-822strapping training process, to reflect constraintsimplicitly applied by a training approach.The SF typical system pipeline presented in Sec-tion 3 applies to most, but not all SF approaches.The following filters directly apply only to sys-tems that use NER as the method of candidate gen-eration, and where candidate generation is distinctfrom answer extraction.
Fourteen of the eighteenteams participating in TAC13 submitted system re-ports (Surdeanu, 2013).
Eleven of these systemsidentify NEs with NER and pass these to an answerextraction process.
The remaining three systemseither do not document whether they rely on or donot rely on NER for candidate generation for nameslots.
We include a high recall baseline based onnoun phrases (NPs) to cover these systems.4.1 FiltersThe first step in the SF pipeline is to find a relevantdocument and the query entity mentioned withinthat document.
We use oracle IR to find docu-ments Dq(ORACLE DOCS in Figure 1) but need tofind a reference to q in these documents for otherfilters and downstream stages (ALIAS MATCH inFigure 1).
An exact match to the query name istrivial, but some documents may not contain thequery verbatim.
This primarily occurs in caseswhere an alias is used, e.g.
where the query FyffesPLC is only mentioned as Fyffes in a document.SF systems typically implement a query expan-sion step prior to searching for relevant docu-ments, generating and extracting aliases based onthe corpus and external sources (Ji et al., 2011).For documents that do not mention the query ver-batim, we manually annotate the longest tokenspan which refers to the query.
All of our filtersare applied to this base setup.
To measure the ef-fect of our manual aliases on recall, we implementa na?
?ve EXACT MATCH filter, which allows a doc-ument only if a NE matches the query verbatim.Entity form filters are based on the form of theentities extracted from documents.
We initiallyconsider all substrings of all NPs for a high-recall,yet tractable, baseline.
The NP N-GRAMS filter al-lows every n-gram of every NP.
NES allows NEsonly; and for TYPES, fill NEs must be of a NERtype defined by the slot, e.g.
for per:city ofbirth only LOC NEs are allowed.Sentence filters require the query mention andfill to be in the same sentence, or to have mentionsin the same sentence.
Sentence filters are COREF:the query and the fill must be mentioned in thesame sentence; COREF NNP: as for COREF, butthe query and the fill must have coreferent propernoun mentions in the same sentence; NA?IVE NNP:as for COREF NNP, but instead of using a fullcoreference system and identifying proper nounmentions, we use a na?
?ve proper noun coreferenceprocess; and NOCOREF: the verbatim query andthe fill must be named in the same sentence.As dependency paths are often a key fea-ture for extracting relations, we apply furthersyntactic filters based on dependency paths be-tween NEs and mentions in sentences.
Wherewe use dependencies, we use the Stanford col-lapsed and propagated representation (de Marn-effe and Manning, 2008), e.g.
in Alice is an em-ployee of Bob and Charlie the collapsed and prop-agated dependency path between Alice and Charlieis?nsubj?employee?prep of?.Syntactic filters roughly capture the complex-ity of the syntactic configuration between queryand filler: LENGTH ?
N requires that the queryand fill are separated by a dependency path of atmost N arcs, e.g.
the above dependency path istwo arcs; VERB requires a verb to be present in thedependency path between the query and fill men-tions or names; and NON-UNIQUE requires the de-pendency path between the query and fill to occurmore than once in a corpus, modelling a hard con-straint on bootstrapping and other learning pro-cesses that require a shared dependency contextbetween training and test examples.4.2 Bootstrapping reachabilityIn addition to the upper bound set by these explicithard constraints, we want to reflect constraints thatare implicitly applied by an extraction process?are there fills that are never learnable given a set offeatures and a set of training data?
We extend ourevaluation to include a training process in a semi-supervised setting.
We treat this as a bootstrap-ping task (Agichtein and Gravano, 2000): giventraining pairs of NEs in text (each pair effectivelya query entity and a candidate slot fill, or vice-versa), extract the context of each pair, and findother pairs in the corpus that share that context.A pair is reachable, and hence learnable, if it canbe found by iterating this process.
We continue toevaluate maximum recall and do not apply thresh-olding or ranking that would typically be utilisedin a bootstrapping process.
We simply output all823Jim Senn (PER)Center for Global Business (ORG)Herb Gibson (PER)OSHA (ORG)Leslie Walker (PER)Massachusetts Correctional Legal Services (ORG)per:employee_of<-prep_of<-director<-appos<-<-prep_for<-director<-appos<-Figure 2: Bootstrapping.
The rightmost vertex is labelled with per:employee of after two iterations.possible candidates in order to measure recall loss:as with hard constraints applied by filters, if recallis lost it can never be recovered.Given a set of training data, we identify if wecan reach a test instance by bootstrapping, no mat-ter how remotely it is connected to training in-stances.
We use lemmatised dependency paths asthe context for this process as they are relativelyprecise and discriminative, compared to other fea-tures used for SF.
In order to simplify process-ing, we construct a graph of all pairs and pathsin the corpus first, and then bootstrap from train-ing instances over this graph.
Bootstrapping moregeneral features (e.g.
bag-of-words) results in thegraph becoming too large to process on our com-puting resources.The graph is constructed as follows.
Each ver-tex represents a typed pair of NEs that occur in thesame sentence in the TAC KBP Source Data (LDC,2010), collapsing vertices that have equal namesand types into a single vertex.
An edge existsbetween pairs that are connected at least once bythe same dependency path.
The constructed graphis equivalent to the EXACT MATCH + NOCOREF+ NON-UNIQUE filter.
Constructing a graph forCOREF (which requires many more edges thanNOCOREF) was impractical.Initially, pairs in training data are labelled withtheir corresponding slots (see Figure 2).
In eachbootstrap iteration, the labels of each vertex areadded to its neighbouring vertices.
There is no fil-tering or competition between labels on a vertex,they are all added.
We analyse performance aftereach iteration, evaluating by mapping the labelledgraph back to the equivalent SF queries.
This en-ables us to determine what fills are recoverablefrom the bootstrapping process.5 EvaluationWe evaluate our filters on the TAC KBP EnglishSlot Filling 2011 corpus, queries and task spec-ification.
As we aim to determine recall upperbounds and recall loss, we use only the documentsD from the TAC KBP Source Data (LDC, 2010)that are known to contain at least one correct slotfill in the TAC KBP 2011 English Slot Filling As-sessment Results (LDC, 2011).We restrict the assessment results and the eval-uation process to all slot types that are filled byname content types as opposed to value orstring.
We also do not evaluate the per:alt-ernate names or org:alternate namesslots, as extraction of fills for these slots typicallyfalls outside the RE task: while X also known as Yor similar may appear in text, X and Y are typicallymentioned independently across documents.There are 100 TAC11 queries, 50 PER and 50ORG.
There are 535 fills in our reduced evalua-tion, 1,171 correct responses over these fills: 56%of the original evaluation slots.
The distribution offills per slot is listed in Table 1.
The number of fillsper query ranges from 0 (one query has no namefills) to 71, with a median of 17.
D is comprisedof 1,351 documents.
The number of documentsper query ranges from 0 to 63, with a median of15.5.
We use TAC 2009 and 2010 results and an-notations as training data for bootstrapping, with4,647 relevant training examples.We evaluate ignoring case and without requir-ing a specific source document: nocase andanydoc in SF evaluation.
Note that each slotfill is an equivalence class of responses: e.g.
fororg:founded by the correct fills Clifford S. As-ness and Clifford Asness are equivalent.
Consis-tent with SF evaluation, we identify at what con-straint an entire equivalence class no longer hasany member proposed as a fill.We process documents with Stanford CoreNLP:tokenisation, POS tagging (Toutanova et al.,2003), NER (Finkel et al., 2005), parsing (Kleinand Manning, 2003), and coreference resolution(Lee et al., 2011), and these annotations form therelevant components of our filters.
Where we usedependency paths, we lemmatise tokens on the824slot #org:top members,employees 118per:employee of 71per:member of 47org:subsidiaries 32org:parents 24per:origin 23org:country of headquarters 22per:countries of residence 20org:city of headquarters 19org:shareholders 18slot #per:cities of residence 17per:children 17org:stateorprovince of headquarters 17per:schools attended 16per:stateorprovinces of residence 11org:member of 11per:spouse 8org:members 8org:founded by 7per:siblings 6slot #per:other family 6per:city of birth 6per:parents 3per:country of birth 3org:political,religious affiliation 2per:stateorprovince of birth 1per:country of death 1per:city of death 1Table 1: Number of fills for slots in the evaluation.path to increase generality and recall in furtheranalysis.
For example, for Alice employs Bob weextract the path?nsubj?employ?dobj?.The COREF NNP filter uses CoreNLP corefer-ence, limited to mentions which are headed byNNPs.
For NA?IVE NNP we use a na?
?ve rule-basedcoreference process (Pink et al., 2013), motivatedby efficiency reasons, as the full CoreNLP requiresparsing and a more complex model.
The rules donot require deep processing and can run quicklyover large volumes of text.
All NEs from a doc-ument are matched by processing in decreasinglength order.
Two names are marked coreferentwhere, ignoring titles and case: they match ex-actly; they have a matching final word; they havea matching initial word; or one is an acronym ofthe other.
If multiple conditions are matched, theearliest (the most strict match) is used.The NON-UNIQUE filter requires that a depen-dency path occurs more than once between NEsin the full TAC KBP Source Data (LDC, 2010),comprised of 1.8M documents and 318M NE pairs.There are 38.6M distinct lemmatised dependencypaths, 5M of which occur more than once.6 ResultsWe now analyse where the filters lose recall.
Re-sults for non-syntactic filters are listed in Table 2.Figure 3 illustrates our main pipeline which con-tains filters that would typically be implemented.NP n-grams We choose all n-grams of NPs(from the CoreNLP constituency parser) to be ourhighest recall filter, and so our highest baselinehas 3% recall loss.
We identify the reasons forloss at this filter.
There are four errors due tothe fill not existing verbatim in text, e.g.
Pinellasand Pasco counties does not contain Pinellas Countyverbatim.
Four errors occur where an NP is notcorrectly identified, which occurs in two differ-ent cases: where there is genuine error or wherethe sentence being parsed is actually a list or othersemi-structured data as opposed to an actual sen-tence.
four errors are where a correct answer hasnot been annotated as correct, we refer to this asANNOTATION error below, and one case where anincorrect response has been annotated as correct.While 97% recall is an excellent starting point,53M candidates is a huge, likely intractable searchspace for any downstream process.
Hence NER iscommonly used as the starting point for SF.NEs Most errors here are due to NER errors, andthese errors result in nearly a 10% recall loss.
25errors are caused where no token in the fill hasbeen tagged as part of a NE (NO NER); and 13where some tokens were missed (NER BOUNDS).There are two additional cases of ANNOTATIONdue to determiners not being included in an NE,where they perhaps should have also been anno-tated.
Hence, in agreement with previous analy-ses, NER error has a large impact on SF.On this data set we have 10% recall loss thatmost SF or RE approaches would never be able toextract.
However, it is still fairly unconstrainedand a high recall bound in comparison to the fol-lowing filters.
Recall errors could be substan-tially reduced if SF approaches were to take intoconsideration all NEs in documents as a set ofcandidates, and take a more document-based ap-proach to RE as opposed to sentence-based.
Whilethere has been some work in extracting relationsacross sentences without coreference (Swampillaiand Stevenson, 2011), RE across sentence bound-aries is effectively limited to coreference chainsbetween sentences.
Currently whole documentextraction is not a research focus for SF, andthe implementation of whole document techniquesthroughout SF pipelines would likely be beneficial.825NPngrams97%53 MNEs90%562 K88%109 Ktypes49K76%64%corefNNP naiveNNP corefnocoref80%78%43K29 K18 KFigure 3: Results for NP N-GRAMS + NES + TYPES, followed by sentence filters with a range of corefer-ence configurations.
Grey fill and % indicates recall after each filter, and the number in the arrow is thesize of the result set passed to the next filter or to the downstream process.experiment R (%) |search space|NP N-GRAMS 97 53966773. .
.
+ NEs 90 562318. .
.
+ TYPES (1) 88 109241. .
.
+ EXACT MATCH (2) 85 105764(1) + COREF 80 49170(1) + NNP COREF 78 43476(1) + NNP NA?IVE 76 29171(1) + NOCOREF 64 18331(2) + COREF 77 47439(2) + NNP COREF 73 30089(2) + NNP NA?IVE 73 27770(2) + NOCOREF 61 16978(1) + COREF + NON-UNIQUE 65 19958(1) + NNP COREF + NON-UNIQUE 62 17692(1) + NNP NA?IVE + NON-UNIQUE 61 13960(1) + NOCOREF + NON-UNIQUE 48 8084(2) + COREF + NON-UNIQUE 63 18953(2) + NNP COREF + NON-UNIQUE 60 16712(2) + NNP NA?IVE + NON-UNIQUE 56 13064(2) + NOCOREF + NON-UNIQUE 43 7236Table 2: Results on D given sets of filters config-urations.
The ellipses indicate the previous line.Exact match Requiring that the query name isexactly matched (EXACT MATCH) loses a further2% recall.
Effectively this is the recall error cre-ated by the IR component of SF.
Five error casesoccur when an alias is required, e.g.
Quds Forcefor IRGC-QF; Chris Bentley for Christopher Bentley.Eight errors occur where the query term is a refer-ence to an entity but not its name, all pertaining tothe query GMAC?s Residential Capital LLC.Types All errors created by the TYPES filter aredue to incorrect NER types on mentions proposedby CoreNLP.
We do not aggregate the NE type overthe coreference chain.
Applying this filter cutsdown the search space substantially, with minimalloss to recall.
Adding TYPES results in a recall lossof 2%, but cuts down the search space by 80%.Coref This filter is the starting point for manyrecent SF approaches: we consider entities that areeither named or mentioned in the same sentence.Table 3 shows that coreference is the largest cat-egory of recall error created by the COREF filter.NN COREF, NNP COREF and PRP COREF indicatefailure to resolve common noun, proper noun andpronoun coreference.The remainder of the errors are cases wherementions of the fills do not occur in the same sen-tence.
ROLE INF indicates that an individual?s roleis mentioned, e.g.
Gene Roberts, the executive editor,where The Inquirer is mentioned in a previous sen-tence.
LOC INF where additional location knowl-edge is required: a French company is headquar-tered in France.
The search space has been sub-stantially reduced, by a further 55% to 0.1% of theoriginal space.
However, the recall upper boundhas dropped to 80% of all fills.Coref NNP and naive NNP While coreferenceis important for high recall, more difficult coref-erence cases (common noun and pronoun coref-erence) may generate a large number of spuriouscases.
Using COREF NNP as the sentence filterloses 2% recall, to an upper bound of 78%, fora 12% reduction in the search space.
However,using a full coreference system generates maymore candidates than using simple NNP corefer-ence.
NA?IVE NNP has an upper bound of 76%.This is only 4% lower recall than COREF, butfor a 41% reduction in search space.
In addi-tion, CoreNLP coreference is much more expen-sive than our na?
?ve approach as it requires parsing.No coref Errors for NOCOREF are listed in Ta-ble 3.
INF indicates that inference or more sophis-ticated analysis is required to find the fill, such ascorrectly identifying the relation between entities826Experiment NN COREF NNP COREF PRP COREF ROLE INF LOC INF INF NO NER ANNOTATIONCOREF 9 6 13 4 3 0 8 1NOCOREF 16 52 20 4 3 2 14 3Table 3: Error types for COREF and NOCOREF.0 20 40 60 80 10002413Recall %Precision%COREFNOCOREFFigure 4: Effect of COREF.0 20 40 60 80 10002413Recall %Precision%NOCOREF + n ?
1-3NOCOREF + n ?
4-7Figure 5: Effect of short dependency paths, takingthe NOCOREF points from Figure 4.0 20 40 60 80 10001234Recall %Precision%COREFCOREF + VERBFigure 6: Effect of the VERB filter.referred to in an interview.
NOCOREF results in arecall upper bound of 64%.
While this gives us asmall search space, we are now losing a substan-tial proportion of the correct fills.Precision-recall curves for the dependency pathfilters are given in Figures 4, 5 and 6.
We chooseto report precision for simplicity, and note that thedownstream search space is the inverse of preci-sion multiplied by the number of correct fills.
Dotsfrom low recall to high recall indicate maximumdependency path length from n = 1 to n = 7.
De-pendency paths of length 7 give maximum recallin our experiments.
Results for the addition of theNON-UNIQUE constraint are given in Table 2.Use of coreference While critical for recall, useof coreference generates a large number of candi-dates and presents a key trade-off for SF, as indi-cated by Figure 4.
At maximum dependency pathlength, coreference gives 16% greater recall at acost of 1.1% precision, roughly half the precisionof no coreference.Higher precision indicates that fewer candidatesare generated.
Fewer candidates allows for SF ap-proaches to be scaled to larger amounts of data,and enables techniques that take advantage of re-dundancy or clustering to be used.
Hence thehigher precision no coreference approach may al-low for more precise learning methods to be used,which may provide better results overall than anapproach using coreference.Short dependency paths In all of our filter con-figurations, a short dependency path length is suf-ficient for extracting the majority of slot fills forthat particular configuration.
Improving precisionof fills found on short dependency paths may be amore effective and scalable approach to improvingF-score rather than focusing on long paths.In Figure 5 we consider NOCOREF.
Limiting thedependency path length to three loses 11% recall,but gains 0.7% precision.
While this loss of re-call is high, the reduction in unique dependencypaths is substantial.
For maximum path lengththree there are 10,732 paths (1,551 unique); for allpaths there are 17,394 paths (2,863 unique).Verb Figure 6 shows the VERB filters has lessimpact or recall or precision than some other de-pendency filters.
For COREF with all paths, addingthe VERB filter loses 6% recall for a 0.1% gain inprecision.
Some slots not included in this anal-ysis, such as per:title, tend to be described827by shorter paths that often do not include verbs.These slots are also frequent in the TAC11 dataset.Non-unique The frequency of a dependencypath may be a critical feature for learning, as pathsthat occur only once will not been seen by a boot-strapping process or may not be considered byother machine learning approaches.
Applying theNON-UNIQUE filter (Table 2) has a large effect onrecall: COREF loses 15% recall for a 41% reduc-tion in the size of the search space; NOCOREFloses 15% recall for a 44% reduction in searchspace.
To recover this recall, the strictness of thisfilter could be relaxed by further generalising de-pendency paths or using a different similarity met-ric to direct match of paths.
However, this is theupper bound for approaches which consider onlyexact dependency paths as a feature.Bootstrapping A small amount of training dataquickly finds slot fills via bootstrapping.
One it-eration has a recall of 24%, with 7,665 candidatesgenerated.
Two to four iterations have recall of37%?39% (maximum recall), with 31,702?37,797candidates.
The recall upper bound for these con-figurations is 43%?more training data will allowfor better precision, but will only minimally im-prove recall in this setup.
We note that limit-ing bootstrap to one or two iterations is ideal forthe best trade-off between recall and search space.However, closer analysis of discriminative paths isrequired for a full SF system.Note that even when bootstrapping through ev-ery dependency path in the corpus, there is an up-per bound on recall of 39%.
Even if we usedthe test data as additional training data the recallwould still be limited to 43%.
This demonstratesthat systems need distributional features, depen-dency tree kernels or other similarity comparisonas opposed to exact feature matching if depen-dency paths are to be a useful feature for SF.7 DiscussionWe present an analysis of SF recall bounds givenhard constraints applied by standard system com-ponents.
Pipeline error is common across all NLPtasks.
Our analysis suggests that high-precisionna?
?ve tools, e.g.
na?
?ve coreference, can lead tostate-of-the-art performance.However, the SF task is not strictly an exhaus-tive evaluation for each query, as the evaluationdata is comprised of the time-limited human anno-tation plus aggregated system output only.
Theremay be fills that are missed in the evaluation re-sults but are correct and returned by our high recallfilters?affecting our reported precisions.We manually evaluate a small sample of thequeries, the first five person and the first fiveorganization queries, to identify missed fills inthe COREF output (2,903 of 49,170 total fills, or5.9%).
For these fills, there were 29 fills in the as-sessment data.
Of these fills, 21 are returned byCOREF, however there are two correct fills foundby COREF that are not in the assessment data.
Oneof these two errors would be identified with cor-rect coreference, and the other requires complexlong range inference.
These additional correct fillsthat are identified will not have a large impact onthe absolute precision, as there are two of 2,903more fills.
However, the relative difference in truepositives, 21 to 23, results in some uncertainty inresults when comparing them relatively.8 ConclusionRecent TAC KBP Slot Filling results have shownthat state-of-the-art systems are substantially lim-ited by low recall.
In this work, we perform amaximum recall analysis of slot filling, providinga comprehensive analysis of recall error createdin the document retrieval and candidate generationstages.
We focus on recall error in candidate gen-eration as a performance limitation, as candidatesthat are lost in the pipeline cannot be recovered bydownstream processes.We find ?10% of recall is ignored by most slotfilling systems due to NER error, and while state-of-the-art coreference provides a substantial recallgain over no coreference, 8% of recall is still lostwhen queries and fills occur in different sentences.Using NE type constraints is very effective, reduc-ing recall by only 2% for a search space reduc-tion of 81%.
Without coreference, a further 16%of fills are lost, but 12% of this recall can be re-gained using efficient na?
?ve name matching rules,while still reducing the search space by 41%, mak-ing such an approach possibly preferable over fullcoreference.
We confirm that coreference and ac-curate NER are critical to high recall slot filling.We find that using maximum recall bootstrap-ping, 39% of test slots fills are reachable from theTAC09 and TAC10 training data, limited by an up-per bound on non-unique paths of 43%.In the future, we intend to assess how specific828slots are affected by recall and search space trade-off, and perform evaluation over all slot types:names, values and strings.
In addition, we in-tend to expand the bootstrapping experiments withvariations over the training data.This work highlights NER, coreference and typ-ing as the areas that have the most impact onslot filling recall, enabling researchers to focus onproblems that will most improve performance.AcknowledgementsWe would like to thank the anonymous review-ers for their useful feedback.
This work was sup-ported by an Australian Postgraduate Award, theCapital Markets CRC Computable News projectand Australian Research Council Discovery grantDP1097291.ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting Relations from Large Plain-text Collec-tions.
In Proceedings of the Fifth ACM Confer-ence on Digital Libraries, pages 85?94, San Anto-nio, Texas, USA.Michele Banko, Michael J. Cafarella, Stephen Soder-land, Matt Broadhead, and Oren Etzioni.
2007.Open Information Extraction from the Web.
In Pro-ceedings of IJCAI, pages 2670?2676, Hyderabad,India.Sergey Brin.
1998.
Extracting patterns and rela-tions from the World Wide Web.
In Proceedingsof the 1998 International Workshop on the Web andDatabases, Valencia, Spain.Lorna Byrne and John Dunnion.
2011.
UCD IIRG atTAC 2011.
In Proceedings of TAC, Gaithersburg,Maryland, USA.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., and Tom M.Mitchell.
2010.
Toward an Architecture for Never-Ending Language Learning.
In Proceedings ofAAAI, pages 1306?1313, Atlanta, Georgia, USA.Linguistic Data Consortium.
2010.
TAC KBP SourceData.
LDC2010E12.Linguistic Data Consortium.
2011.
TAC KBP2011 English Slot Filling Assessment Results.LDC2011E88.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford Typed DependenciesRepresentation.
In Proceedings of the Workshop onCross-Framework and Cross-Domain Parser Evalu-ation, pages 1?8, Manchester, United Kingdom.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying Relations for Open InformationExtraction.
In Proceedings of EMNLP, pages 1535?1545, Edinburgh, United Kingdom.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating Non-local Informa-tion into Information Extraction Systems by GibbsSampling.
In Proceedings of ACL, pages 363?370,Ann Arbor, Michigan, USA.Ryan Gabbard, Marjorie Freedman, and RalphWeischedel.
2011.
Coreference for Learning to Ex-tract Relations: Yes Virginia, Coreference Matters.In Proceedings of ACL, pages 288?293, Portland,Oregon, USA.Raphael Hoffmann, Congle Zhang, Xiao Ling,Luke Zettlemoyer, and Daniel S. Weld.
2011.Knowledge-based weak supervision for informationextraction of overlapping relations.
In Proceed-ings of ACL-HLT, pages 541?550, Portland, Oregon,USA.Heng Ji and Ralph Grishman.
2011.
KnowledgeBase Population: Successful Approaches and Chal-lenges.
In Proceedings of ACL-HLT, pages 1148?1158, Portland, Oregon.Heng Ji, Ralph Grishman, and Hoa Dang.
2011.Overview of the TAC2011 Knowledge Base Popu-lation Track.
In Proceedings of TAC, Gaithersburg,Maryland, USA.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of ACL,pages 423?430, Sapporo, Japan.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve corefer-ence resolution system at the CoNLL-2011 sharedtask.
In Proceedings of CONLL: Shared Task, pages28?34, Portland, Oregan, USA.Mausam, Michael Schmitz, Robert Bart, StephenSoderland, and Oren Etzioni.
2012.
Open lan-guage learning for information extraction.
In Pro-ceedings of EMNLP-CONLL, pages 523?534, JejuIsland, Korea.Paul McNamee and Hoa Dang.
2009.
Overview of theTAC 2009 Knowledge Base Population Track.
InProceedings of TAC, Gaithersburg, Maryland, USA.Bonan Min and Ralph Grishman.
2012.
Challengesin the Knowledge Base Population Slot Filling Task.In Proceedings of LREC, pages 1148?1158, Istan-bul, Turkey.Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In Proceedings of ACL-IJCNLP, pages 1003?1011, Singapore.829Glen Pink, Will Radford, Will Cannings, AndrewNaoum, Joel Nothman, Daniel Tse, and James R.Curran.
2013.
SYDNEY CMCRC at TAC 2013.
InProceedings of TAC, Gaithersburg, Maryland, USA.Sebastian Riedel, Limin Yao, Benjamin M. Marlin,and Andrew McCallum.
2013.
Relation Extractionwith Matrix Factorization and Universal Schemas.In Proceedings of HLT-NAACL, Atlanta, Georgia,USA.Benjamin Roth, Tassilo Barth, Michael Wiegand, Mit-tul Singh, and Dietrich Klakow.
2013.
EffectiveSlot Filling Based on Shallow Distant SupervisionMethods.
In Proceedings of TAC, Gaithersburg,Maryland, USA.Benjamin Roth, Tassilo Barth, Grzegorz Chrupa?a,Martin Gropp, and Dietrich Klakow.
2014.
Re-lationFactory: A Fast, Modular and Effective Sys-tem for Knowledge Base Population.
Proceedingsof EACL, pages 89?92.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,and Christopher D. Manning.
2012.
Multi-instancemulti-label learning for relation extraction.
In Pro-ceedings of EMNLP-CONLL, pages 455?465, JejuIsland, Korea.Mihai Surdeanu.
2013.
Overview of the TAC2013Knowledge Base Population Evaluation: EnglishSlot Filling and Temporal Slot Filling.
In Proceed-ings of TAC, Gaithersburg, Maryland, USA.Kumutha Swampillai and Mark Stevenson.
2011.
Ex-tracting Relations Within and Across Sentences.
InProceedings of RANLP, pages 25?32, Hissar, Bul-garia.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich Part-of-speech Tagging with a Cyclic Dependency Network.In Proceedings of HLT-NAACL, pages 173?180, Ed-monton, Canada.Yafang Wang, Bin Yang, Lizhen Qu, Marc Spaniol,and Gerhard Weikum.
2011.
Harvesting Facts fromTextual Web Sources by Constrained Label Propa-gation.
In Proceedings of CIKM, pages 837?846,Glasgow, Scotland, UK.Limin Yao, Sebastian Riedel, and Andrew McCallum.2012.
Unsupervised Relation Discovery with SenseDisambiguation.
In Proceedings of ACL, pages712?720, Jeju Island, Korea.Xingxing Zhang, Jianwen Zhang, Junyu Zeng, JunYan, Zheng Chen, and Zhifang Sui.
2013.
TowardsAccurate Distant Supervision for Relational FactsExtraction.
In Proceedings of ACL-HLT, pages 810?815, Jeju Island, Korea.Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005.
Exploring Various Knowledge in RelationExtraction.
In Proceedings of ACL, pages 427?434,Ann Arbor, Michigan, USA.830
