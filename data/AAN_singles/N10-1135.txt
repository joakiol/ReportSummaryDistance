Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 921?929,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsCross-lingual Induction of Selectional Preferenceswith Bilingual Vector SpacesYves PeirsmanQLVL, University of LeuvenResearch Foundation ?
Flanders (FWO)yves.peirsman@arts.kuleuven.beSebastian Pad?IMS, University of Stuttgartpado@ims.uni-stuttgart.deAbstractWe describe a cross-lingual method for the in-duction of selectional preferences for resource-poor languages, where no accurate monolin-gual models are available.
The method usesbilingual vector spaces to ?translate?
foreignlanguage predicate-argument structures intoa resource-rich language like English.
Theonly prerequisite for constructing the bilin-gual vector space is a large unparsed corpusin the resource-poor language, although themodel can profit from (even noisy) syntacticknowledge.
Our experiments show that thecross-lingual predictions correlate well withhuman ratings, clearly outperforming monolin-gual baseline models.1 IntroductionSelectional preferences capture the empirical observa-tion that not all words are equally good arguments toa given verb in a particular argument position (Wilks,1975; Resnik, 1996).
For instance, the subjects ofthe English verb to shoot are generally people, whilethe direct objects can be people or animals.
This isreflected in speakers?
intuitions.
Table 1 shows thatthe combination the hunter shot the deer is judgedmore plausible than the deer shot the hunter.
Selec-tional preferences do not only play an important rolein human sentence processing (McRae et al, 1998),but are also helpful for NLP tasks like word sensedisambiguation (McCarthy and Carroll, 2003) andsemantic role labeling (Gildea and Jurafsky, 2002).Computational models of selectional preferencespredict such plausibilities for triples of a predicate p,an argument position a, and a head word h, such asPredicate Relation Noun Plausibilityshoot subject hunter 6.9shoot object hunter 2.8shoot subject deer 1.0shoot object deer 6.4Table 1: Predicate-relation-noun triples with human plau-sibility judgments on a 7-point scale (McRae et al, 1998)(shoot,object,hunter).
All recent models take a two-step approach: (1), they extract all triples (p, a, h)from a large corpus; (2), they apply some type ofgeneralization to make predictions for unseen items.Clearly, the accuracy of these models relies cruciallyon the quality and coverage of the extracted triples,and thus on the syntactic analysis of the corpus.
Un-fortunately, corpora that are both large enough andhave a very good syntactic analysis are only availablefor a handful of Western and Asian languages, whichleaves all other languages without reliable selectionalpreference models.In this paper, we propose a cross-lingual knowl-edge transfer approach to this problem: We automat-ically translate triples (p, a, h) from resource-poorlanguages into English, where large and high-qualityparsed corpora are available and we can compute areliable plausibility estimate.
The translations areextracted from a bilingual semantic space, which canbe constructed via bootstrapping from large unparsedcorpora in the two languages, without the need forparallel corpora or bilingual lexical resources.Structure of the paper.
Section 2 reviews modelsfor selectional preferences.
In Section 3, we describeour approach.
Section 4 introduces our experimentalsetup, and Sections 5 and 6 present and discuss ourexperiments.
Section 7 wraps up.9212 Selectional PreferencesThe first broad-coverage model of selectional prefer-ences was developed by Resnik (1996).
To estimatethe plausibility of a triple (p, a, h), Resnik first ex-tracted all head words seen with predicate p in posi-tion a, Seena(p), from a corpus.
He then used theWordNet hierarchy to generalize over the head wordsand to create predictions for unseen ones.
A numberof studies has followed the same approach, exploringdifferent ways of using the structure of WordNet (Abeand Li, 1996; Clark and Weir, 2002).
While theseapproaches show good results, they can only makepredictions for argument heads that are covered byWordNet.
This is already a problem for English, andmuch more so in other languages, where comparableresources are often much smaller or entirely absent.A promising alternative approach is to derivethe generalizations from distributional informa-tion (Prescher et al, 2000; Pad?
et al, 2007; Bergsmaet al, 2008).
For example, the Pad?
et al (2007)model computes vector space representations for allhead words h and defines the plausibility of the triple(p, a, h) as a weighted mean of the vector space simi-larities between h and all h?
in Seena(p):Pl(p, a, h) =?h??Seena(p)w(h?)?
sim(h, h?)?h?
w(h?
)(1)where w(h?)
is a weight, typically frequency.In this model, the generalization is provided by dis-tributional similarity, which can be computed from alarge corpus, without the need for additional lexicalresources.
Pad?
et al found it to outperform Resnik?sapproach in an evaluation against human plausibilityjudgments.
However, note that competitive resultsare only obtained by representing the head words in?syntactic?
vector spaces whose dimensions consistof context words with their syntactic relation to thetarget rather than just context words.
This is not sur-prising: Presumably, hunter and deer share a domainand are likely to have similar word-based contextdistributions, even though they differ with regard totheir plausibility for particular predicate-argumentpositions.
Only when the vector space can capturetheir different syntactic co-occurrence patterns canthe model predict different plausibilities.English tripleGerman triple(schie?en,obj,Hirsch)monolingualselectionalpreferencemodelmonolingualselectionalpreferencemodel(shoot,obj,deer)bilingualvector spacedeerHirschschie?enshootFigure 1: Predicting selectional preferences for a sourcelanguage (e.g.
German) by translating into a target lan-guage (e.g.
English) with a bilingual vector space.3 Cross-lingual selectional preferencesIn order to compute reliable selectional preferencerepresentations, distributional models need to seeat least some head words for each (p, a) combina-tion.
Manually annotated treebank corpora, whichare becoming available for an increasing number oflanguages, are too small for this task.
We thereforeexplore the idea of predicting the selectional pref-erences for such languages by taking advantage oflarge corpora with high-quality syntactic analyses inresource-rich languages like English.
This idea fallsinto the general approach of cross-lingual knowledgetransfer (see e.g.
Hwa et al, 2005).
The applicationto selectional preferences was suggested by Agirre etal.
(2003), who demonstrated its feasibility by man-ual translation between Basque and English.
Weextend their experiments to an automatic model thatpredicts plausibility judgments in a resource-poorlanguage (source language) by exploiting a model ina resource-rich language (target language).Figure 1 sketches our method.
We assume thatthere is not enough high-quality data to build a mono-lingual selectional preference model for the sourcelanguage (shown by dotted lines).
However, we canuse a bilingual vector space, that is, a semantic spacein which words of both the source and the targetlanguage are represented, to translate each sourcelanguage word s into the target language by identify-ing its nearest (most similar) target word tr(s):tr(s) = argmaxt sim(s, t) (2)Now we can use a target language selectional prefer-ence model to obtain plausibilities for source triples:Pls(p, a, h) = Plt(tr(p), a, tr(h)) (3)where the superscript indicates the language.922Eq.
(3) gives rise to three questions: (1), How canwe construct the bilingual space to model tr?
(2), Istranslating actually the appropriate way of transfer-ring selectional preferences?
(3), Is it reasonable toretain the source language argument positions likesubject or object?
The following subsections discuss(1) and (2); we will address (3) in Sections 5 and 6.3.1 Bilingual Vector SpacesBilingual vector spaces are vector spaces in whichwords from two languages are represented (cf.
Fig.
2).The dimensions of this space are labeled with bilin-gual context word pairs (like secretly/heimlich andrifle/Gewehr for German?English) that are mutualtranslations.
By treating such context word pairs assingle dimensions, the vector space can represent tar-get words from both languages, counting the targetwords?
co-occurrences with the context words fromthe respective language.
In other words, a source-target word pair (s, t) will be assigned similar vectorsin the semantic space if the context words of s aretranslations of the context words of t. Cross-lingualsemantic similarity between words can be measuredusing standard vector space similarity (Lee, 1999).Importantly, bilingual vector spaces can be builton the basis of co-occurrences drawn from two un-related corpora for the source and target languages.Their construction does not require resources suchas parallel corpora or bilingual translation lexicons,which might not be available for resource-poor sourcelanguages.
Where parallel corpora exist, they oftencover specific domains (e.g., politics), while manybilingual lexicons are prone to ambiguity problems.The main challenge in constructing bilingual vec-tor spaces is determining the set of dimensions,i.e., bilingual word pairs, using as little knowledge aspossible.
Most often, such pairs are extracted fromsmall bilingual lexicons (Fung and McKeown, 1997;Rapp, 1999; Chiao and Zweigenbaum, 2002).
Asmentioned above, such resources might not be avail-able.
We thus follow an alternative approach by usingfrequent cognates, words that are shared between thetwo languages (Mark?
et al, 2005).
Cognates canbe extracted by simple string matching between thecorpora, and mostly share their meaning (Koehn andKnight, 2002).
However, they account for (at most) asmall percentage of all interesting translation pairs.To extend the set of dimensions available for theshoothitstalkrifle/Gewehrsecretly/heimlichschie?enanschleichenFigure 2: Sketch of a bilingual vector space for English(solid dots) and German (empty circles).bilingual space, we use these cognates merely as astarting point for a bootstrapping process: We builda bilingual vector space with the initial word pairs asdimensions, and identify nearest neighbors betweenthe two languages in the space.
These are added asdimensions of the bilingual space, and the processis repeated.
Since the focus is on identifying reli-able source-target word pairs rather than completecoverage as in Eq.
(2), we adopt a symmetrical defi-nition of translation that pairs up only mutual nearestneighbors, and allows words to remain untranslated:1trsym(s) = t iff tr(s) = t and tr(t) = s (4)From the second iteration onward, this process intro-duces dimensions that are not identical graphemes,such as Kind?child and Geschwindigkeit?speed, andis iterated until convergence.
Since each word ofeither language can only participate in at most onedimension, dimensions acquired in later steps can cor-rect wrong pairs from previous steps, like the ?falsefriend?
German Kind ?child?
?
English kind, whichis part of the initial set of cognates.3.2 Translation and Selectional PreferencesAs Figure 1 shows, the easiest way of exploiting abilingual semantic space is to identify for each sourceword the target language word with the highest se-mantic similarity.
For example, in Figure 2, the besttranslation of German schie?en is its English nearestneighbor, shoot.
However, it is risky to rely on thesingle nearest neighbor ?
it might simply be wrong.Even if it is correct, data sparsity is an issue: Thetranslations may be infrequent in the target language,or the two translations of p and h may form unlikelycollocates for target language-internal reasons (like1To avoid unreliable vectors, we also adopt only the 50%most frequent of the trsym pairs.
Frequency is defined as thegeometric mean of the two words?
monolingual frequencies.923difference in register) that do not reflect plausibility.A third issue are monolingual semantic phenomenalike polysemy and idioms: The implausible Germantriple (schie?en,obj,Brise) will be judged as very plau-sible due to the English idiom to shoot the breeze.A look at the broader neighborhood of schie?ensuggests that its second and third-best English neigh-bors, hit, and stalk, can be used to smooth plausibilityestimates for schie?en.
Instead of translating sourcelanguage words by their single nearest neighbor, wewill take its k nearest neighbors into account.
Thisis defensible also from a more fundamental point ofview, which suggests that the cross-lingual transfer ofselectional preferences does not require literal trans-lation in order to work.
First, ontological modelslike Resnik?s assume that synonymous words behavesimilarly with respect to selectional preferences.
Sec-ond, recent work by Chambers and Jurafsky (2009)has induced ?narrative chains?, i.e., likely sequencesof events, by their use of similar head words.
Thus,we expect that all k nearest neighbors of a sourcepredicate s are informative for the selectional prefer-ences of s (like schie?en) as long as they are eithersynonyms of its literal translation (shoot/hit) or comefrom the same narrative chain (stalk/kill/.
.
.
).It is also clear that smoothing does not alwaysequate better predictions.
Closeness in a word-basedvector space can also just reflect semantic association.For example, Spanish tenista ?tennis player?
is highlyassociated with English tennis, but is a bad translationin terms of selectional preferences.
We assume thatthis problem is more acute for nouns than for verbs:The context of verbs is dominated by their arguments,which is not true for nouns.
Consequently, closenouns in vector space can differ widely in ontologicaltype, while close verbs generally have one or moresimilar argument slots.
In our model, we will thusconsider several verb translations, but just the besthead word translation.
For details, see Section 5.4 Experimental SetupOur evaluation uses English as the target languageand two source languages: German (as a very closeneighbor of English) and Spanish (as a more distantone).
Neither of these languages are really resource-poor, but they allow us to compare our cross-lingualmodel against monolingual models, to emulate dif-ferent levels of ?resource poorness?
and to examinethe model?s learning curve.Plausibility Data.
For German, we used the plau-sibility judgments collected by Brockmann (2002).The dataset contains human judgments for ninetytriples sampled from the manually annotated 1 mil-lion word TiGer corpus (Brants et al, 2002): tenverbs with three argument positions (subject [SUBJ],direct object [DOBJ], and oblique (prepositional) ob-ject [POBJ]) combined with three head words.
Mod-els are evaluated against such datasets by correlatingpredicted plausibilities with the (not normally dis-tributed) human judgments using Spearman?s ?, anon-parametric rank-order correlation coefficient.We constructed a similar 90-triple data set forSpanish by sampling triples from two Spanish cor-pora (see below) using Brockmann?s (2002) crite-ria.
Human judgments for the triples were collectedthrough the Amazon Mechanical Turk (AMT) crowd-sourcing platform (Snow et al, 2008).
We askednative speakers of Spanish to rate the plausibility ofa simple sentence with the relevant verb-argumentcombination on a five-point Likert scale, obtainingbetween 12 and 17 judgments for each triple.
Foreach datapoint, we removed the single lowest andhighest judgments and computed the mean.
We as-sessed the reliability of our data by replicating Brock-mann?s experiment for German with our AMT setup.With a Spearman ?
of almost .90, our own judgmentscorrelate very well with Brockmann?s original data.Monolingual Prior Work and Baselines.
ForGerman, Brockmann and Lapata (2003) evaluatedontology-based models trained on TiGer triples andthe GermaNet ontology.
The results in Table 2 showthat while both models are able to predict the datasignificantly, neither of the models can predict all ofthe data.
We attribute this to the small size of TiGer.2To gauge the limits of monolingual knowledge-lean approaches, we constructed two monolingualdistributional models for German and Spanish ac-cording to the Pad?
et al (2007) model (Eq.
(1)).Recall that this model performs generalization in asyntax-based vector space model.
We computed vec-tor spaces from dependency-parsed corpora for the2For each of the three argument positions and ?all?, Brock-mann and Lapata report the results for the best parametrizationof the models, which explains the apparently inconsistent results.924Resnik Clark & WeirSUBJ .408* .268DOBJ .430* .611***POBJ .330 .597***all .374*** .232*Table 2: Monolingual baselines 1.
Spearman correla-tions for ontology-based models in German as reported byBrockmann and Lapata (2003).
*: p < .05; ***: p < .001Lang.
German SpanishCorpus Schulte?s HGC AnCora Encarta?
Cov.
?
Cov.
?
Cov.SUBJ .34?
90% .44* 80% .14 100%DOBJ .51** 97% .29 83% -.05 100%POBJ .41* 93% -.03 100% ?
?3all .33** 93% .16 88% .11 67%Table 3: Monolingual baselines 2.
Spearman correlationand coverage for distributional models.
?
: p < .1; *: p <.05; **: p < .01.two languages, using the 2,000 most frequent lemma-dependency relation pairs as dimensions and adopt-ing the popular pointwise mutual information metricas co-occurrence statistic.
For German, we usedSchulte im Walde?s verb frame resource (Schulte imWalde et al, 2001), which contains the frequency oftriples calculated from probabilistic parses of 30Mwords from the Huge German Corpus (HGC) ofnewswire.
For Spanish, we consulted two syntac-tically analyzed corpora: the AnCora (Taul?
et al,2008) and the Encarta corpus (Calvo et al, 2005).
At0.5M words, the AnCora corpus is small, but man-ually annotated, whereas the larger, automaticallyparsed Encarta corpus amounts to over 18M tokens.Table 3 shows the results for the distributionalmonolingual models.
For German, we get significantcorrelations for DOBJ and POBJ, an almost signif-icant correlation for SUBJs, and high significancefor the complete dataset (p < 0.01).
These figuresrival the performance of the ontological models (cf.Table 2), without using ontological information.
ForSpanish, the only significant correlation with humanjudgments is obtained for subjects, the most frequentargument position, with the clean AnCora data.
An-Cora is presumably too sparse for the other argumentpositions.
The large Encarta corpus, in turn, is verynoisy, supporting our concerns from Section 2.3Since the Encarta data consists of individual dependencyn noun adj verb allGerman 7340 .61 .57 .43 .56Spanish 4143 .62 .67 .41 .58Table 4: First-translation accuracy for German-Englishand Spanish-English translation (n: size of gold standard).Cross-lingual Selectional Preferences.
Our archi-tecture for the cross-lingual prediction of selectionalpreferences shown in Figure 1 consists of two com-ponents, namely the bilingual vector space and aselectional preference model in the target language.As our English selectional preference model, weagain use the Pad?
et al (2007) model, trained ona version of the BNC parsed with MINIPAR (Lin,1993).
The parameters of the syntactic vector spacewere the same as for the monolingual baseline mod-els.
The bilingual vector spaces were constructedfrom three large, unparsed, comparable monolin-gual corpora.
For German, we used the HGC de-scribed above.
For Spanish, we obtained a corpuswith around 100M words, consisting of 2.5 years ofcrawled text from two major Spanish newspapers.For English, we used the BNC.We first constructed initial sets of bilingual labels.For German?English, we identified 1064 graphem-ically identical word pairs that occurred more than4 times per million words.
Due to the larger lex-ical distance between Spanish and English, thereare fewer graphemically identical tokens for this lan-guage pair.
We therefore applied a Porter stemmerand found 2104 identical stems, at a higher risk of?false friends?.
We then applied the bootstrappingcycle from Section 3.1.
The set of dimensions con-verged after around five iterations.We evaluated the (asymmetric) nearest neighborpairs from the final spaces, (s, tr(s)), against twoonline dictionaries.4 Table 4 shows that 55% to 60%of the pairs are listed in the dictionaries, with paralleltendencies for both language pairs.
The bilingualspace performs fairly well for nouns and adjectives,but badly for verbs, which is a well-known weaknessof distributional models (Peirsman et al, 2008).Even taking into account the incompleteness ofdictionaries, this looks like a negative result: morerelations rather than trees, we could not model the POBJ data.4DE-EN: www.dict.cc; ES-EN: www.freelang.net.Pairs (s, tr(s)) were only evaluated if the dictionary listed s.925than half of all verb translations are incorrect.
How-ever, following up on our intuitions from Section 3.2,we performed an analysis of the ?incorrect?
transla-tions.
It revealed that many of the errors in Table 4are informative, semantically related words.
Near-est neighbor target language verbs in particular tendto represent the same event type and take the samekinds of arguments as the source verb.
Examplesare German gef?hrden ?threaten?
?
English affect,and German Neugier ?curiosity?
?
English enthusi-asm.
We concluded that literal translation quality isa misleading figure of merit for our task.Experimental rationale.
Section 3 introduced onemajor design decision of our model: the question ofhow to treat the argument position, which cannotbe translated by the bilingual vector space, in thecross-lingual transfer.
We present two experimentsthat investigate the model?s behavior in the absenceand presence of knowledge about argument positions.Experiment 1 uses no syntactic knowledge about thesource language whatsoever.
In this situation, thebest we can do is to assume that source languageargument positions like SUBJ will correspond to thesame argument position in the target language.
Exper-iment 2 attempts to identify, for each source languageargument position, the ?best fit?
position in the targetlanguage.
This results in better plausibility estimates,but also means that we need at least some syntac-tic information about the source language.
In bothexperiments, we vary the number of translations weconsider for each verb.5 Exp.
1: Induction without syntacticknowledge in the source languageThis experiment assumes that argument positionssimply carry over between languages.
While thisassumption clearly simplifies linguistic reality, it hasthe advantage of not needing any syntactic informa-tion about the source language.
We thus model Ger-man and Spanish SUBJ relations by English SUBJrelations and DOBJs by DOBJs.
In the case of (lex-icalized) POBJs, where we cannot assume identity,we compute plausibility scores for all English POBJsthat account for at least 10% of the predicate?s ar-gument tokens, and select the PP with the highestplausibility estimate.
The k best ?translations?
of thepredicate p, trk(p), are turned into a single predictionusing maximization, yielding the final model:Plsnosyn(p, a, h) = maxpt?trk(p)Plt(pt, a, tr(h)) (5)Note that this model does not use any source lan-guage information, except the bilingual vector space.The results of Experiment 1 are given in Table 5(coverage always 100%).
For German, all predictionscorrelate significantly with human ratings, and mosteven at p < 0.01, despite our naive assumption aboutthe cross-lingual argument position identity.
Theresults exceed both monolingual model types (onto-logical, Tab.
2, and distributional, Tab.
3), notablywithout the use of syntactic data.
In particular, theresults for the POBJs, notoriously difficult to modelmonolingually, are higher than for SUBJs or DOBJs.We attribute this to the cross-lingual generalizationwhich takes all prepositional arguments into account.The Spanish dataset is harder to model overall.We obtain significantly high correlations for SUBJ,but non-significant results for DOBJ and POBJ.
Thiscorresponds well to the patterns for the monolingualAnCora corpus (Table 3).
However, we outperformAnCora on the complete dataset, where it did notachieve significance, while the cross-lingual modeldoes at p < 0.01 ?
again, even without the use ofsyntactic analyses.
We attribute the overall lowerresults compared to German to systematic syntacticdifferences between English and Spanish.
For exam-ple, animate direct objects in Spanish are realizedas POBJs headed by the preposition a. Estimatingthe plausibility of such objects by looking at EnglishPOBJs is unlikely to yield good results.
The use ofa larger number of verb translations yields a clearincrease in correlation for the German data, but in-conclusive results for Spanish.6 Exp.
2: Induction with syntacticknowledge in the source languageAs discussed in Section 3.2, verbs that are semanti-cally similar in the bilingual vector space may verywell realize their (semantic) argument positions dif-ferently in the surface syntax.
For example, Germanteilnehmen is correctly translated to English attend,but the crucial event argument is realized differently,namely as a POBJ headed by an in German and asa DOBJ in English.
To address this problem, we926DE 1-best 2-best 3-best 4-best 5-bestSUBJ .44* .47** .45* .47** .54**DOBJ .39* .39* .52** .54** .55**POBJ .58** .61** .61** .61** .62**all .35** .37** .37** .38** .40**ES 1-best 2-best 3-best 4-best 5-bestSUBJ .58** .64** .64** .58** .58**DOBJ .13 .16 .11 .07 .07POBJ .13 .13 .09 .14 .14all .34** .36** .34** .32** .32**Table 5: Exp.1: Spearman correlation between syntaxlesscross-lingual model and human judgments for k best verbtranslations.
Best k for each argument position marked inboldface.
Coverage of all models: 100%.learn a mapping function m that identifies the argu-ment position at of a target language predicate ptthat corresponds best to an argument position a of apredicate p in the source language.
Our simple modelis in the same spirit as the cross-lingual plausibilitymodel itself: It returns the argument position at ofpt for which the seen head words of (p, a) are mostplausible when translated into the target language:5m(p, a, pt) = argmaxat?h?Seena(p)Plt(pt, at, tr(h))Parallel to Eq.
(5), the cross-lingual model is now:Plssyn(p, a, h) = maxpt?trk(p)Plt(pt,m(p, a, pt), tr(h))(6)This model can recover English argument positionsthat correspond better to the original ones than theidentity mapping.
For example, on our data, it discov-ers the mapping for teilnehmen an/attend discussedabove.
A second example concerns the incorrect, butinformative translation of stagnieren ?stagnate?
asboost.
Here the model recognizes that the SUBJ ofstagnieren (the stagnating entity) corresponds to theDOBJ of boost.Establishing m requires syntactic information inthe source language, in order to obtain the set ofseen head words Seenas(ps).
For this reason, Exp.
2uses the parsed subset of the HGC (German), and theAnCora and Encarta corpora (Spanish).
The resultsare shown in Table 6.
We generally improve over5To alleviate sparse data, we ignore argument positions ofEnglish verbs that represent less than 10% of its argument tokens.DE 1-best 2-best 3-best 4-best 5-bestSUBJ .55** .59** .49** .52** .54**DOBJ .52** .52** .66** .66** .68**POBJ .61** .68** .70** .69** .70**all .41** .44** .44* .46** .48**ES-A 1-best 2-best 3-best 4-best 5-bestSUBJ .52** .47* .42* .41* .42*DOBJ .52*c .64**c .54*c .42*c .42*cPOBJ .32?
.18 .13 .13 .24all .47** .41** .36** .33** .37**ES-E 1-best 2-best 3-best 4-best 5-bestSUBJ .40* .42* .39* .39* .41*DOBJ .21 .02 .06 .13 .20Table 6: Exp.2: Spearman correlation between syntax-aware cross-lingual model and human judgments for kbest verb translations.
ES-A: AnCora corpus, ES-E: En-carta corpus.
Best k for each argument position in bold-face.
Coverage of all models: 100%, except c: 60%.Exp.
1.
For German, every single model now corre-lates highly significantly with human judgments (p< 0.01), and the correlation for the complete datasetincreases from .40 to .48.
For Spanish, we see verygood results for the AnCora corpus.
Compared toExp.
1, we see a slight degradation for the SUBJs;however, the correlations remain significant for allvalues of k. Conversely, all predictions for DOBJsare now significant,6 and the POBJs have improved atleast numerically, which validates our analysis of theproblems in Exp.
1.
The best correlation for the com-plete dataset improves from .36 to .47.
The resultsfor the Encarta corpus disappoint, though.
SUBJsare significant, but worse than for AnCora, and theDOBJs remain non-significant throughout.
With re-gard to increasing the number of verb translations,Exp.
2 shows an almost universal benefit for Ger-man, but still mixed results for Spanish, which mayindicate that verb translations for Spanish are still?looser?
than the German ones.In fact, most remaining poor judgments are theresult of problematic translations, which stem fromthree main sources.
The first one is sparse data.
Infre-quent German and Spanish words often receive unre-liable vector representations.
Some examples are the6Note, however, that AnCora has an imperfect coverage forDOBJs (60%).
This is because our Spanish dataset containsverbs sampled from Encarta that do not occur in AnCora.927German Tau (?dew?, frequency of 180 in the HGC),translated as alley, and Reifepr?fung (German SAT,frequency 120), translated as affiliation.
Both of thesemay also be due to the difference in genre betweenthe HGC and the BNC.
A second problem is formedby nearest neighbors that are ontologically dissimi-lar, as in the tenista ?tennis player?/tennis examplefrom above.
A final issue relates to limitations of thePad?
et al (2007) model, whose architecture is sus-ceptible to polysemy-related problems.
For instance,the Spanish combination (excavar, obj, terreno) wasjudged by speakers as very plausible, but its Englishequivalent (excavate, obj, land) is assigned a verylow score by the model.
This might be due to thefact that in the BNC, land occurs often in its politicalmeaning, and forms an outlier among the head wordsfor (excavate,obj).How much syntactic information is necessary?The syntax-aware model requires syntactic infor-mation about the source language, which seems torun counter to our original motivation of developingmethods for resource-poor languages.
To address thispoint, we analyzed the behavior of the syntax-awaremodel for small syntactically analyzed corpora thatcontained only at most m occurrences for each pred-icate.
We obtained the m occurrences by samplingfrom the syntactically analyzed part of the HGC; iffewer than m occurrences were present in the corpus,we simply used these.
Figure 3 shows the trainingcurve with 1 verb translation, averaged over n rounds(n = 10 for 5 arguments, n = 5 for 10 arguments,n = 4 for 20, 50 and 100 arguments).
The generalpicture is clear: most of the benefit of the syntacticdata is drawn form the first five occurrences for eachargument position.
This shows that a small amount oftargeted syntactic annotation can improve the cross-lingual model substantially.7 ConclusionsIn this article, we have presented a first unsuper-vised cross-lingual model of selectional preferences.Our model proceeds by automatically translating(predicate, argument position, head word) triples forresource-poor source languages into a resource-richtarget language, where accurate selectional prefer-ence models are available.
The translation is based ona bilingual vector space, which can be bootstrappedll ll l lnumber of observed headsSpearman'srhol SUBJDOBJPOBJall0 5 10 20 50 1000.30.40.50.60.7Figure 3: Training curve for the bilingual German?Englishmodel as a function of the number of observed head wordsper argument position in the source language.from large unparsed corpora in the two languages.Our results indicate that bilingual methods can goa long way towards the modeling of selectional pref-erences in resource-poor languages, where bilinguallexicons, parallel corpora, or ontologies might not beavailable.
Our experiments have looked at Germanand Spanish, where the cross-lingual models rivaland even exceed monolingual methods that typicallyhave to rely on small, clean ?treebank?-style corporaor large, very noisy, automatically parsed corpora.We have also demonstrated that noisy syntactic datafrom the source language can be integrated in ourmodel, where it helps improve the cross-lingual han-dling of argument positions.
The linguistic distancebetween the languages can impact (1) the ability tofind accurate translations and (2) the degree of syntac-tic overlap; nevertheless, as Agirre et al (2003) show,the transfer is possible even for unrelated languages.In this paper, we have instantiated the selectionalpreference model in the target language (English)with the distributional model by Pad?
et al (2007).However, our approach is modular and can be com-bined with any other selectional preference model.We see two main avenues for future work: (1), Theconstruction of properly bilingual models wheresource language information can also help to fur-ther improve the target language model (Diab andResnik, 2002); (2), The extension of our cross-lingualmapping for the argument position to mappings thathold across multiple predicates as well as argument-dependent mappings like the Spanish direct objects,whose realization depends on their animacy.928ReferencesNaoki Abe and Hang Li.
1996.
Learning word associationnorms using tree cut pair models.
In Proc.
ICML, pages3?11, Bari, Italy.Eneko Agirre, Izaskun Aldezabal, and Eli Pociello.
2003.A pilot study of English selectional preferences andtheir cross-lingual compatibility with Basque.
In Proc.TSD, pages 12?19, Brno, Czech Republic.Shane Bergsma, Dekang Lin, and Randy Goebel.
2008.Discriminative learning of selectional preference fromunlabeled text.
In Proc.
EMNLP, pages 59?68, Hon-olulu, HI.Sabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The TIGER tree-bank.
In Proc.
Workshop on Treebanks and LinguisticTheories, Sozopol, Bulgaria.Carsten Brockmann and Mirella Lapata.
2003.
Evaluatingand combining approaches to selectional preferenceacquisition.
In Proc.
EACL, pages 27?34, Budapest,Hungary.Carsten Brockmann.
2002.
Evaluating and combining ap-proaches to selectional preference acquisition.
Master?sthesis, Universit?t des Saarlandes, Saarbr?cken.Hiram Calvo, Alexander Gelbukh, and Adam Kilgarriff.2005.
Distributional thesaurus vs. wordnet: A compari-son of backoff techniques for unsupervised PP attach-ment.
In Proc.
CICLing, pages 177?188, Mexico City,Mexico.Nathanael Chambers and Dan Jurafsky.
2009.
Unsuper-vised learning of narrative schemas and their partici-pants.
In Proc.
ACL, pages 602?610, Singapore.Yun-Chuang Chiao and Pierre Zweigenbaum.
2002.Looking for candidate translational equivalents in spe-cialized, comparable corpora.
In Proc.
COLING, pages1?5, Taipei, Taiwan.Stephen Clark and David Weir.
2002.
Class-based proba-bility estimation using a semantic hierarchy.
Computa-tional Linguistics, 28(2):187?206.Mona Diab and Philip Resnik.
2002.
An unsupervisedmethod for word sense tagging using parallel corpora.In Proc.
ACL, pages 255?262, Philadelphia, PA.Pascale Fung and Kathleen McKeown.
1997.
Findingterminology translations from non-parallel corpora.
InProc.
3rd Annual Workshop on Very Large Corpora,pages 192?202, Hong Kong.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguistics,28(3):245?288.Rebecca Hwa, Philipp Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural Language Engineering, 11(3):311?325.Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
In Proc.ACL-02 Workshop on Unsupervised Lexical Acquisi-tion, pages 9?16, Philadelphia, PA.Lillian Lee.
1999.
Measures of distributional similarity.In Proc.
ACL, pages 25?32, College Park, MD.Dekang Lin.
1993.
Principle-based parsing without over-generation.
In Proc.
ACL, pages 112?120.Korn?l Mark?, Stefan Schulz, Olena Medelyan, and UdoHahn.
2005.
Bootstrapping dictionaries for cross-language information retrieval.
In Proc.
SIGIR, pages528?535, Seattle, WA.Diana McCarthy and John Carroll.
2003.
Disambiguat-ing nouns, verbs and adjectives using automaticallyacquired selectional preferences.
Computational Lin-guistics, 29(4):639?654.Ken McRae, Michael Spivey-Knowlton, and MichaelTanenhaus.
1998.
Modeling the influence of thematicfit (and other constraints) in on-line sentence compre-hension.
Journal of Memory and Language, 38:283?312.Sebastian Pad?, Ulrike Pad?, and Katrin Erk.
2007.
Flex-ible, corpus-based modelling of human plausibilityjudgements.
In Proc.
EMNLP-CoNLL, pages 400?409,Prague, Czech Republic.Yves Peirsman, Kris Heylen, and Dirk Geeraerts.
2008.Size matters.
Tight and loose context definitions inEnglish word space models.
In Proc.
ESSLLI Workshopon Lexical Semantics, pages 9?16, Hamburg, Germany.Detlef Prescher, Stefan Riezler, and Mats Rooth.
2000.Using a probabilistic class-based lexicon for lexicalambiguity resolution.
In Proc.
COLING, pages 649?655, Saarbr?cken, Germany.Reinhard Rapp.
1999.
Automatic identification of wordtranslations from unrelated English and German cor-pora.
In Proc.
ACL, pages 519?526, College Park, MD.Philip Resnik.
1996.
Selectional constraints: Aninformation-theoretic model and its computational real-ization.
Cognition, 61:127?159.Sabine Schulte im Walde, Helmut Schmid, Mats Rooth,Stefan Riezler, and Detlef Prescher.
2001.
StatisticalGrammar Models and Lexicon Acquisition.
In Linguis-tic Form and its Computation, pages 389?440.
CSLIPublications, Stanford, CA.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Ng.
2008.
Cheap and fast ?
but is it good?Evaluating non-expert annotations for natural languagetasks.
In Proc.
EMNLP, pages 254?263, Honolulu, HI.Mariona Taul?, M. Ant?nia Mart?, and Marta Recasens.2008.
Ancora: Multilevel annotated corpora for Cata-lan and Spanish.
In Proc.
LREC, Marrakech, Morocco.Yorick Wilks.
1975.
Preference semantics.
In E. Keenan,editor, Formal Semantics of Natural Language.
Cam-bridge University Press.929
