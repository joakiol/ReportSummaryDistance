Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 804?812,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAn Unsupervised Aspect-Sentiment Model for Online ReviewsSamuel BrodyDept.
of Biomedical InformaticsColumbia Universitysamuel.brody@dbmi.columbia.eduNoemie ElhadadDept.
of Biomedical InformaticsColumbia Universitynoemie@dbmi.columbia.eduAbstractWith the increase in popularity of online re-view sites comes a corresponding need fortools capable of extracting the informationmost important to the user from the plain textdata.
Due to the diversity in products and ser-vices being reviewed, supervised methods areoften not practical.
We present an unsuper-vised system for extracting aspects and deter-mining sentiment in review text.
The methodis simple and flexible with regard to domainand language, and takes into account the in-fluence of aspect on sentiment polarity, an is-sue largely ignored in previous literature.
Wedemonstrate its effectiveness on both compo-nent tasks, where it achieves similar results tomore complex semi-supervised methods thatare restricted by their reliance on manual an-notation and extensive knowledge sources.1 IntroductionOnline review sites continue to grow in popularity asmore people seek the advice of fellow users regard-ing services and products.
Unfortunately, users areoften forced to wade through large quantities of writ-ten data in order to find the information they want.This has led to an increase in research in the areasof opinion mining and sentiment analysis, with theaim of providing systems that can automatically an-alyze user reviews and extract the information mostrelevant to the user.One example of such an application is generat-ing a summary of the important factors mentionedin the reviews of a product (see Lerman et al 2009).Another application is comparing two similar prod-ucts.
In this case, it is important to present to theuser the aspects in which the products differ, ratherthan just provide a general star rating.
A third exam-ple is systems for generating automatic recommen-dations, based on similarity between products, userreviews, and history of previous purchases.
Thesetypes of application require an underlying frame-work to identify the important aspects of the prod-uct (also known as features or attributes), and thesentiment expressed by the review writer.Unsupervised Methods are desirable for this task,for two reasons.
First, due to the wide range and va-riety of products and services being reviewed, theframework must be robust and easily transferablebetween domains.
The second reason is the nature ofthe data.
Online reviews are often short and unstruc-tured, and may contain many spelling and gram-matical errors, as well as slang or specialized jar-gon.
These factors often present a problem to meth-ods relying exclusively on dictionaries, manually-constructed knowledge resources, and gazetteers, asthey may miss out on an important aspect of theproduct or an indicator of sentiment.
Unsupervisedmethods, on the other hand, are not influenced bythe lexical form, and can handle unknown words orword-forms, provided they occur frequently enough.This insures that any emergent topic that is salient inthe data will be addressed by the system.In this paper, we present an unsupervised systemwhich addresses the core tasks necessary to enableadvanced applications to handle review data.
We in-troduce a local topic model, which works at the sen-tence level and employs a small number of topics, toautomatically infer the aspects.
For sentiment detec-tion, we present a method for automatically derivingan unsupervised seed set of positive and negative ad-jectives that replaces the manually constructed onescommonly used in the literature.
Our approach isspecifically designed to take into account the inter-804action between the two tasks.The rest of the paper is structured as follows.
InSec.
2 we provide relevant background, and placeour method in the context of previous work in thefield.
We describe the data we used in Sec.
3, and ourexperiments on the aspect and sentiment-polaritycomponents in Sec.
4 and 5, respectively.
We con-clude in Sec.
6 with a discussion of our results andfindings and directions for future research.2 Previous ApproachesIn this paper, we focus on the detection of two prin-ciple elements in the review text: aspects and sen-timent.
In previous work these elements have beentreated, for the most part, as two separate tasks.Aspect The earliest attempts at aspect detectionwere based on the classic information extraction (IE)approach of using frequently occurring noun phrases(e.g., Hu and Liu 2004).
Such approaches work wellin detecting aspects that are strongly associated witha single noun, but are less useful when aspects en-compass many low frequency terms (e.g., the foodaspect of restaurants, which involves many differ-ent dishes), or are abstract (e.g.
ambiance can bedescribed without using any concrete nouns at all).Common solutions to this problem involve cluster-ing with the help of knowledge-rich methods, in-volving manually-constructed rules, semantic hier-archies, or both (e.g., Popescu and Etzioni 2005,Fahrni and Klenner 2008).
Titov and McDonald(2008b) underline the need for unsupervised meth-ods for aspect detection.
However, according to theauthors, existing topic models, such as standard La-tent Dirichlet Allocation (LDA) (Blei et al, 2003),are not suited to the task of aspect detection in re-views, because they tend to capture global topicsin the data, rather than rateable aspects pertinentto the review.
To address this problem, they con-struct a multi-grain topic model (MG-LDA), whichattempts to capture two layers of topics - global andlocal, where the local topics correspond to rateableaspects.
MG-LDA distinguishes tens of local top-ics, but the many-to-one mapping between these andrateable aspects is not explicit in the system.
To re-solve this issue, the authors extend their model inTitov and McDonald (2008a) and attempt to infersuch a mapping with the help of aspect-specific rat-ings provided along with the review text.Sentiment Sentiment analysis has been the fo-cus of much previous research.
In this discussion,we will only mention work directly related to ourown.
For a comprehensive survey of the subject, thereader is directed to Pang and Lee (2008).Most previous approaches rely on a manuallyconstructed lexicon of terms which are strongly pos-itive or negative regardless of context.
This informa-tion on its own is usually insufficient, due to lackof coverage and the fact that sentiment is often ex-pressed through words whose polarity is highly do-main and context specific.
If a sentiment lexicon isavailable for one domain, domain adaptation can beused, provided the domains are sufficiently similar(Blitzer et al, 2007).
Another common solution isthrough bootstrapping - using a seed group of termswith known polarity to infer the polarity of domainspecific terms (e.g., Fahrni and Klenner 2008; Jijk-oun and Hofmann 2009).
The most minimalist ex-ample of this approach is Turney (2002), who usedonly a single pair of adjectives (good and poor) todetermine the polarity of other terms through mu-tual information.
For Chinese, Zagibalov and Carroll(2008) use a single seed word meaning good, and sixcommon indicators of negation in their bootstrap-ping approach.
Often, when using a context indepen-dent seed, large amounts of domain-specific data arerequired, in order to obtain sufficient co-occurrencestatistics.
Commonly, web queries are used to obtainsuch data.Independently of any specific task, Hatzivas-siloglou and McKeown (1997) present a completelyunsupervised method for determining the polarity ofadjectives in a large corpus.
A graph is created, inwhich adjectives are nodes, and edges between themare weighted according to a (dis)similarity functionbased primarily on whether the two adjectives oc-curred in a conjunction or disjunction in the corpus.A heuristic approach is then used to split the graphin two.
The group containing the adjectives with thehigher average frequency is labeled as positive, andthe other as negative.Combined Approaches Aspects can influencesentiment polarity within a single domain.
For ex-ample, in the restaurant domain, cheap is usuallypositive when discussing food, but negative whendiscussing the decor or ambiance.
Many otherwiseneutral terms (e.g., warm, heavy, soft) acquire a sen-timent polarity in the context of a specific aspect.805Recent work has addressed this interaction in differ-ent ways.
Mei et al (2007) present a form of do-main adaptation using an LDA model which treatspositive and negative sentiment as two additionaltopics.
Fahrni and Klenner (2008) directly addressthe specificity of sentiment to the word it is modi-fying.
Aspects are defined by a manually specifiedsubset of the Wikipedia category hierarchy.
For sen-timent, the authors use a seed set of positive andnegative adjectives, and iteratively propagate sen-timent polarity through conjunction relations (likethose used by Hatzivassiloglou and McKeown 1997,above).
Web queries are used to overcome the spar-sity issue of these highly-specific patterns.
In the IEsetting, Popescu and Etzioni (2005) extract frequentterms, and cluster them into aspects.
The sentimentdetection task is formulated as a Relaxation Label-ing problem of finding the most likely sentiment la-bels for opinion-bearing terms, while satisfying asmany local constraints as possible.
The authors usea variety of knowledge sources, web queries, andhand crafted rules to detect relations between terms(e.g., meronymy).
These relations are used both forthe clustering, and as a basis for the constraints.Our approach is designed to be as unsupervisedand knowledge-lean as possible, so as to make ittransferable across different types of products andservices, as well as across languages.
Aspects aredetermined via a local version of LDA, which oper-ates on sentences, rather than documents, and em-ploys a small number of topics that correspond di-rectly to aspects.
This approach overcomes the prob-lems of frequent-term methods, as well as the issuesraised by Titov andMcDonald (2008b).
We use mor-phological negation indicators to automatically cre-ate a seed set of highly relevant positive and nega-tive adjectives, which are guaranteed to be pertinentto the aspect at hand.
These automatically-derivedseed sets achieve comparable results to the use ofmanual ones, and the work of Zagibalov and Car-roll (2008) suggests that the use of negation can beeasily transfered to other languages.3 DataOur primary dataset is the publicly available corpusused in Ganu et al (2009).
It contains over 50,000restaurant reviews from Citysearch New York1.
Ad-1http://newyork.citysearch.com/ditionally, to demonstrate the domain independenceof our system, we collected 1086 reviews for fourleading netbook computers from Amazon.com.For evaluation purposes, we used the annotateddataset from Ganu et al (2009), which is a sub-set of 3,400 sentences from the Citysearch corpus.These sentences were manually labeled for aspectand sentiment.
There were six manually defined as-pect labels - Food & Drink, Service, Price, Atmo-sphere, Anecdotes and Miscellaneous.
A sentencecould contain multiple aspects, but, for our evalua-tion, we used only sentences with a single label.
Forsentiment, each sentence was given a single value -Positive, Negative, Neutral or Conflict (indicating amixture of positive and negative sentiment).We were also provided with a seed set of 128 pos-itive and 88 negative adjectives used by Fahrni andKlenner (2008), which were specifically selected tobe domain and target independent.For the purpose of the experiments presentedhere, we focused on sentences containing noun-adjective pairs.
Such pairs are one of the most com-mon way of expressing sentiment about an aspectand allow us to capture the interaction between thetwo.4 Aspect4.1 MethodologyIn order to infer the salient aspects in the data, weemployed the following steps:Local LDA We used a standard implementation2of LDA.
In order to prevent the inference of globaltopics and direct the model towards rateable aspects(see Sec.
2), we treated each sentence as a separatedocument.
The output of the model is a distributionover inferred aspects for each sentence in the data.The parameters we employed were standard, out-of-the-box settings (?
= 0.1,?
= 0.1, 3000 iterations),with no specific tuning to our data.
We ran the algo-rithm with the number of aspects ranging from 10 to20, and employed a cluster validation scheme (seebelow) to determine the optimal number.Model Order The issue of model order, i.e., deter-mining the correct number of clusters, is an impor-tant element in unsupervised learning.
A common2GibbsLDA++, by Xuan-Hieu Phan.
Available at http://gibbslda.sourceforge.net/.806approach (Levine and Domany, 2001; Lange et al,2004; Niu et al, 2007) is to use a cluster validationprocedure.
In such a procedure, different model or-ders are compared, and the one with the most con-sistent clustering is chosen.
For the purpose of thevalidation procedure, we have a cluster correspond-ing to each aspect, and we label each sentence asbelonging to the cluster of the most probable aspect.Given the collection of sentences in our data, D,and two connectivity matricesC and C?, where a celli, j contains 1 if sentences di and d j belong to thesame cluster, we define a consistency function F(following Niu et al 2007):F(C,C?)
=?i, j 1{Ci, j = C?i, j = 1,di,d j ?
D?
}?i, j 1{Ci, j = 1,di,d j ?
D?
}(1)We then employ the following procedure:1.
Run the LDA model with k topics on D to ob-tain connectivity matrixCk.2.
Create a comparison connectivity matrix Rkbased on uniformly drawn random assignmentsof the instances.3.
Sample random subset Di of size ?|D| from D.4.
Run the LDA model on Di to obtain connectiv-ity matrixCik.5.
Create a comparison matrix Rik based on uni-formly drawn random assignments of the in-stances in Di.6.
Calculate scorei(k) = F(C?,C)?F(R?,R) whereF is given in Eq.
1.7.
Repeat steps 3 to 6 q times.8.
Return the average score over q iterations.This procedure calculates the consistency of ourclustering solution, using a similar sized random as-signment for comparison.
It does this on q subsets toreduce the effects of chance.
The k with the high-est score is chosen.
In our experiments, we usedq= 5,?= 0.9.
For both our datasets (restaurants andnetbooks), the highest-scoring k was 14.Determining Representative Words For each as-pect, we list all the nouns in the data according to ascore based on their mutual information with regardto that aspect.Scorea(w) = p(w,a) ?
logp(w,a)p(w) ?
p(a)(2)Where p(w), p(a), p(w,a) are the probabilities, ac-cording to the LDA model, of the word w, the aspecta, and the wordw labeled with aspect a, respectively.We then select, for each aspect, the top ka rank-ing words, such that they cover 75% of the word-instances labeled by the LDA model with aspect la-bel a.
Due to the skewed frequency distribution ofwords, this is a relatively small portion of the words(typically 100-200).
This set of representative wordsfor each aspect is used in the sentiment componentof our system (see Sec.
5.1).4.2 Inferred AspectsTable 1 presents the aspects inferred by our systemfor the restaurant domain.
The inferred aspects coverall those defined in the manual annotation, but alsodistinguish between a finer granularity of aspects,based solely on the review text, e.g., between phys-ical environment and ambiance, and between the at-titude of the staff and the quality of the service.In order to demonstrate that our method can betransfered between very different domains and cat-egories of products, we also ran our algorithm onour set of netbook reviews.
The inferred aspectsare presented in Table 2.
The system identifies im-portant aspects relevant to our data.
Some of these(e.g., software, hardware) might be suggested by hu-man annotators, but some would probably be missedunless the annotators carefully read through all thereviews, e.g., theMemory aspect, which includes ad-vice about upgrading specific models.
This capabil-ity of our system is important, as it demonstrates thatour method can be used to produce customized com-parisons for the user and will take into account theimportant common factors, as well as the unique as-pects of each item.4.3 EvaluationTo determine the quality of our automatically in-ferred aspects, we compared the output of our sys-tem to the sentence-level manual annotation of Ganuet al (2009).
To each sentence in the data, the LDAmodel assigns a distribution {P(a)}a?A over the setA of inferred aspects.
By defining a threshold ta foreach aspect, we can label a sentence as belongingto aspect a if P(a) > ta.
By varying the threshold tawe created precision-recall curves for the top threerateable aspects in the restaurant domain, shown in807Inferred Aspect Representative Words Manual AspectMain Dishes chicken, sauce, rice, cheese, spicy, salad,Food & DrinkBakery hot, delicious, dessert, bagels, bread, chocolateFood - General menu, fresh, sushi, fish, chef, cuisineWine & Drinks wine, list, glass, drinks, beer, bottleAmbiance / Mood great, atmosphere, wonderful, music, experience, relaxedAtmospherePhysical Atmosphere bar, room, outside, seating, tables, cozy, loudStaff service, staff, friendly, attentive, busy, slowStaffService table, order, wait, minutes, reservation, forgotValue portions, quality, worth, size, cheap PriceAnecdotes dinner, night, group, friends, date, familyAnecdotesAnecdotes out, back, definitely, around, walk, blockGeneral best, top, favorite, city, NYCMisc.Misc.
- Location never, restaurant, found, Paris, (New) York, locationMisc.
place, eat, enjoy, big, often, stuffTable 1: List of automatically inferred aspects for the restaurant domain, with some representative words for eachaspect (middle), and the corresponding aspect label from the manual annotation (right).
Labels (left) were assigned bythe authors.Aspect Representative WordsPerformance power, performance, mode, fan, quietHardware drive, wireless, bluetooth, usb, speakers, webcamMemory ram, 2GB, upgrade, extra, 1GB, speedSoftware using, office, software, installed, works, programsUsability internet, video, web, movies, music, email, playPortability around, light, work, portable, weight, travelComparison netbooks, best, reviews, read, decided, researchAspect Representative WordsMouse mouse, right, touchpad, pad, buttons, leftGeneral great, little, machine, price, netbook, happyPurchase amazon, purchased, bought, weeks, orderedLooks looks, feel, white, finish, blue, solid, glossyOS windows, xp, system, boot, linux, vista, osBattery battery, life, hours, time, cell, lastSize screen, keyboard, size, small, enough, bigTable 2: List of automatically inferred aspects for the netbook dataset, with representative words for each aspect .Figure 13.
Although the data used in Titov and Mc-Donald (2008a) was unavailable for direct compar-ison, our method exhibits similar behavior and per-formance (compare Fig.
4, there) on a domain withsimilar characteristics (abstract aspects which en-compass many low frequency words).
This demon-strates that our local version of LDA with few top-ics overcomes the issues which confronted the au-thors of that work (i.e., global topics and many-to-one mapping of topics to aspects), without requiringspecially designed models or additional informationin the form of user-provided aspect-specific ratings(see Sec.
2).We believe the reason for this stems from thecomposition of online reviews.
Since many reviewshave similar mixtures of local topics (e.g., food, ser-vice), standard LDA prefers global topics, which3We combined the probabilities of all the inferred aspectsthat match a single manually assigned aspect, according to themapping in Table 1.distinguish more strongly between reviews (e.g., cui-sine type, restaurant type).
However, when em-ployed at the sentence level, local topics (corre-sponding to rateable aspects) provide a stronger wayto distinguish between individual sentences.5 Sentiment5.1 MethodologyFor determining sentiment polarity, we developedthe following procedure.
For each aspect, we ex-tracted the relevant adjectives, built a conjunctiongraph, automatically determined the seed set (orused a manual one, for comparison), and propagatedthe polarity scores to the rest of the adjectives.
De-tails of each step are described below.Extracting Adjectives As a pre-processing step,we parsed our data (using RASP, Briscoe and Car-roll 2002).
The parsed output was used to detectnegation and conjunction.
If an adjective A partic-808(a) (b) (c)Figure 1: Precision / Recall curves for the top three rateable aspects: (a) Food, (b) Service, and (c) Atmosphere.ipated in a negation in the sentence, it was replacedby a new adjective not-A.
We then extract all caseswhere an adjective modified a noun.
For example,from the sentence ?The food was tasty and hot, butour waiter was not friendly.?
we can extract the pairs(tasty, food), (hot, food), (not-friendly, waiter).Building the Graph Our method for determin-ing sentiment polarity is based on an adaptation ofHatzivassiloglou and McKeown (1997) (see Sec.
2).Several issues confronted us when attempting toadapt their method to our task.
In the original arti-cle, adjectives with no orientation were ignored.
Itis unclear how this can be easily done in an unsu-pervised fashion, and such sentiment-neutral adjec-tives are ubiquitous in real-world data.
Furthermore,adjectives whose orientation depended on the con-text were also ignored.
These are of particular in-terest in our task, and are likely to be missing orincorrectly labeled in standard sentiment dictionar-ies.
For our purposes, since we need to handle ad-jectives expressing various shades of sentiment, notonly strongly positive or negative ones, we are inter-ested in a scoring method, rather than a binary label-ing.
Also, we do not want to use a general corpus,but rather the text from the reviews themselves.
Thisusually means a much smaller corpus than the oneused in the original paper, but has the advantage ofbeing domain specific.Our method of building the polarity graph differedin several ways from the original.
First, we did notuse disjunctions (e.g., ?but?)
as indicators of oppositepolarity.
The reason for this was that, in our domainof online reviews, disjunctions often did not conveycontrast in polarity, but rather in perceived expecta-tions, e.g., ?dainty but strong necklace?, and ?cheapbut delicious food?.Instead of using regular expressions to capture ex-plicit conjunctions, we retrieved all cases where ourparser indicated that two adjectives modified a sin-gle noun in the same sentence.To ensure that aspect-specific adjectives are han-dled correctly, we built a separate graph for each as-pect, by selecting the cases where the modified nounwas one of the representative words for that aspect(see Sec.
4.1).Constructing a Seed Set We used morphologi-cal information and explicit negation to find pairs ofopposite polarity.
Specifically, adjective pairs whichwere distinguished only by one of the prefixes ?un?,?in?, ?dis?, ?non?, or by the negation marker ?not-?were selected for the seed set.
Starting with the mostfrequent pair, we assigned a positive polarity to themore frequent member of the pair.Then, in order of decreasing frequency, we as-signed polarity to the other seed pairs, based on theshortest path either of the members had to a previ-ously labeled adjective.
That member received itsneighbor?s polarity, and the other member of the pairreceived the opposite polarity.
When all pairs werelabeled, we corrected for misclassifications by iter-ating through the pairs and reversing the polarity ifthat improved consistency, i.e., if it caused the mem-bers of the pair to match the polarities of more oftheir neighbors.
Finally, we reverse the polarity ofthe seed groups if the negative group has a highertotal frequency.Propagating Polarity Our propagation method isbased on the label propagation algorithm of Zhuand Ghahramani (2002).
The adjectives in the posi-tive and negative seed groups are assigned a polarity809score of 1 and 0, respectively.
All the rest start witha score of 0.5.
Then, an update step is repeated.
Inupdate iteration t, for each adjective x that is not inthe seed, the following update rule is applied:pt(x) =?y?N(x)w(y,x) ?
pt?1(y)?y?N(x)w(y,x)(3)Where pt(x) is the polarity of adjective x at step t,N(x) is the set of the neighbors of x, andw(y,x) is theweight of the edge connecting x and y.
We set thisweight to be 1+ log(#mod(y,x)) where #mod(y,x)is the number of times y and x both modified a singlenoun.
The update step is repeated to convergence.5.2 Aspect-Specific Gold StandardTo evaluate the performance of the sentiment com-ponent of our system, we created an aspect-specificgold standard.
For each of the top eight automati-cally inferred aspects (corresponding to the Food,Service and Atmosphere aspects in the annotation),we constructed a polarity graph, as described inSec.
5.1.
We retrieved a list of all adjectives thatparticipated in five or more modifications of nounsfrom that specific aspect).
Table 3 lists the number ofsuch adjectives in each aspect.
We split the data intoten portions and, for each portion, asked two volun-teers to rate each adjective according to the polar-ity of the sentiment it expresses in the context of thespecified aspect.
The judges could select from thefollowing ratings: Strongly Negative, Weakly Nega-tive, Neutral, Weakly Positive, Strongly Positive, andN/A.
As expected, exact inter-annotator agreementwas low - only 54%, but when considering two ad-jacent ratings as equivalent (i.e, Strongly vs. WeaklyNegative or Positive, and Neutral vs. Weakly Neg-ative or Positive), agreement was 93.3%.
This indi-cates there is some difficulty distinguishing betweenthe fine-grained categories we specified, but highagreement at a coarser level, which advocates us-ing a ranking approach for evaluation (see also Pangand Lee 2005).
We therefore translated the annota-tor ratings to a numerical scale, from ?2 (StronglyNegative) to +2 (Strongly Positive) at unit intervals.After discarding adjectives where one or more anno-tators gave a ?N/A?
tag, we averaged the two annota-tor numerical scores, and used this data as the goldstandard for our evaluation.Aspect # Adj.
# Rated % Neu.Mood 293 206 17%Staff 155 122 3%Main Dishes 287 185 25%Physical Atmo.
161 103 21%Bakery 180 129 23%Food - General 192 144 28%Wine & Drinks 111 75 18%Service 89 57 5%Total 1468 1021 ?Table 3: For each aspect, the number of frequently oc-curring adjectives for each aspect (# Adj.
), number ofadjectives remaining after removing those labeled ?N/A?
(# Rated), and percent of rated adjectives labeled ?Neu-tral?
by both annotators (% Neu.).Auto.
ManualAspect ?k Dk ?k DkMood 0.53 0.23 0.56 0.22Staff 0.57 0.22 0.60 0.20Main Dishes 0.19 0.40 0.38 0.31Physical Atmo.
0.34 0.33 0.25 0.37Bakery 0.33 0.33 0.35 0.33Food - General 0.19 0.41 0.41 0.30Wine & Drinks 0.32 0.34 0.52 0.24Service 0.41 0.30 0.54 0.23Average 0.36 0.32 0.45 0.27Table 4: Kendall coefficient and distance scores for eightinferred aspects.5.3 Evaluation MeasuresKendall?s tau coefficient (?k) and Kendall?s distance(Dk) are commonly used (e.g., Jijkoun and Hofmann2009) to compare rankings.
These measures look atthe number of pairs of ranked items that agree ordisagree with the ordering in the gold standard.
Thevalue of ?k ranges from -1 (perfect disagreement) to1 (perfect agreement), with 0 indicating an almostrandom ranking.
The value ofDk ranges from 0 (per-fect agreement) to 1 (perfect disagreement).
It is im-portant to note that only pairs that are ordered in thegold standard are used in the comparison.5.4 Evaluation ResultsTable 4 reports Kendall?s coefficient (?k) and dis-tance (Dk) values for our method when using ourautomatically derived seed set (Auto.).
For com-parison, we ran our procedure using the manuallycompiled seed set (Manual) of Fahrni and Klenner810Food - General: Mexican, French, Eastern, Turkish,European, Tuscan, Mediterranean, American, Cuban,Thai, Peruvian, Spanish, Korean, Vietnamese, Indian,African, Japanese, Italian, Chinese, AsianMood: Vietnamese, Brazilian, Turkish, Eastern,Caribbean, Cuban, Italian, Spanish, Japanese, Euro-pean, Mediterranean, Colombian, Mexican, Asian,Indian, Thai, British, American, French, Korean,Chinese, Russian, MoroccanStaff: British, European, Chinese, Indian, American,Spanish, Asian, Italian, FrenchTable 5: Polarity ranking of cuisine adjectives (from mostpositive) for three aspects.(2008).
Using the manual seed set obtains resultsthat correspond better to our gold standard.
Our au-tomatic method also achieves good results, and canbe used when a manual seed set is not available.More importantly, correlation with the gold standardmay not indicate better suitability to the sentimentdetection task in reviews.
For instance, it is interest-ing to note that the worst correlation scores were onthe Main Dishes and Food - General aspects.
If wecompare to Table 3, we can see these aspects havethe highest percentage of adjectives rated as neutralby the annotators.
However, in many cases, these ad-jectives actually carry some sentiment in their con-text.
An example of this are adjectives describingthe type of cuisine, which are objective, and there-fore usually considered neutral by annotators.
Ta-ble 5 shows the automatic ranking of cuisine typefrom positive to negative in three aspects.
It is inter-esting to see that the rankings change according tothe aspect, and certain cuisines are strongly associ-ated with specific aspects and not with others.
Thisis supported by Ganu et al (2009), who observedduring the annotation that, in the restaurant corpus,French and Italian restaurants were strongly associ-ated with the service aspect.
This trend can be iden-tified automatically by our method, and at a muchmore detailed level than that noticed by a human an-alyzing the data.6 Discussion & Future WorkOur experiments confirm the value of a fully un-supervised approach to the tasks of aspect detec-tion and sentiment analysis.
The aspects are inferredfrom the data, and are more representative thanmanually derived ones.
For instance, in our restau-rant domain, the manually constructed aspect listomitted or over-generalized some important aspects,while over-representing others.
There was no sep-arate Drinks category, even though it was stronglypresent in the data.
The Service aspect, dealing withwaiting time, reservations, and mistaken orders, wasan important emergent aspect on its own, but wasgrouped under Staff in the manual annotation.Adjectives can convey different sentiments de-pending on the aspect being discussed.
For exam-ple, the adjective ?warm?
was ranked very positive inthe Staff aspect, but slightly negative in the GeneralFood aspect.
A knowledge-rich approach might ig-nore such adjectives, thereby missing important ele-ments of the review.Finally, as online reviews belong to an informalgenre, with inventive spelling and specialized jar-gon, it may be insufficient, for both aspect andsentiment, to rely only on lexicons.
For example,our restaurant reviews included spelling errors suchas desert, decour/decore, anti-pasta, creme-brule,sandwhich, omlette, exelent, tastey, as well as atleast six different common misspellings of restau-rant.
There were also specialized terms, such as Ko-rma, Edamame, Dosa and Pho, all of which do notappear in common dictionaries, and creative use ofadjectives, such as orgasmic and New-Yorky.This work has opened many avenues for future re-search and improvements.
So far, we focused on ad-jectives as sentiment indicators, however, there havebeen studies showing that other parts of speech canbe very helpful for this task (e.g., Pang et al 2002;Benamara et al 2007).
Also, it would be interestingto take a closer look at the interactions between as-pect and sentiment, especially at a multiple-sentencelevel (see Snyder and Barzilay 2007).
Finally, wefeel that the true test of the usability of our systemshould be through an application, and intend to pro-ceed in that direction.AcknowledgmentsWe?d like to thank Angela Fahrni and Manfred Klennerfor kindly allowing us access to their data and annotation.We also wish to thank the volunteer annotators.
This workwas partially supported by a Google Research Award.ReferencesBenamara, Farah, Carmine Cesarano, Antonio Picariello,Diego Reforgiato, and V. S. Subrahmanian.
2007.
Sen-timent analysis: Adjectives and adverbs are better than811adjectives alone.
In Proc.
of the International Confer-ence on Weblogs and Social Media (ICWSM).Blei, David M., Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of MachineLearning Research 3:993?1022.Blitzer, John, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InProc.
of the 45th Annual Meeting of the Association ofComputational Linguistics.
ACL, Prague, Czech Re-public, pages 440?447.Briscoe, Ted and John Carroll.
2002.
Robust accuratestatistical annotation of general text.
In Proc.
of the 3rdLREC.
Las Palmas, Gran Canaria, pages 1499?1504.Fahrni, Angela and Manfred Klenner.
2008.
Old Wineor Warm Beer: Target-Specific Sentiment Analysis ofAdjectives.
In Proc.of the Symposium on AffectiveLanguage in Human and Machine, AISB 2008 Con-vention.
pages 60 ?
63.Ganu, Gayatree, Noemie Elhadad, and Amelie Marian.2009.
Beyond the stars: Improving rating predictionsusing review text content.
In WebDB.Hatzivassiloglou, Vasileios and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proc.
of the 35th Annual Meeting of the Asso-ciation for Computational Linguistics.
ACL, Madrid,Spain, pages 174?181.Hu, Minqing and Bing Liu.
2004.
Mining and summariz-ing customer reviews.
In KDD ?04: Proc.
of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining.
ACM, NewYork, NY,USA, pages 168?177.Jijkoun, Valentin and Katja Hofmann.
2009.
Generating anon-english subjectivity lexicon: Relations that matter.In Proc.
of the 12th Conference of the European Chap-ter of the ACL (EACL 2009).
ACL, Athens, Greece,pages 398?405.Lange, Tilman, Volker Roth, Mikio L. Braun, andJoachim M. Buhmann.
2004.
Stability-based val-idation of clustering solutions.
Neural Comput.16(6):1299?1323.Lerman, Kevin, Sasha Blair-Goldensohn, and Ryan Mc-Donald.
2009.
Sentiment summarization: evaluatingand learning user preferences.
In EACL ?09: Proc.
ofthe 12th Conference of the European Chapter of theAssociation for Computational Linguistics.
ACL,Mor-ristown, NJ, USA, pages 514?522.Levine, Erel and Eytan Domany.
2001.
Resamplingmethod for unsupervised estimation of cluster validity.Neural Comput.
13(11):2573?2593.Mei, Qiaozhu, Xu Ling, Matthew Wondra, Hang Su, andChengXiang Zhai.
2007.
Topic sentiment mixture:modeling facets and opinions in weblogs.
In WWW?07: Proc.
of the 16th international conference onWorld Wide Web.
ACM, New York, NY, USA, pages171?180.Niu, Zheng-Yu, Dong-Hong Ji, and Chew-Lim Tan.
2007.I2r: three systems for word sense discrimination, chi-nese word sense disambiguation, and english wordsense disambiguation.
In SemEval ?07: Proc.
of the4th International Workshop on Semantic Evaluations.ACL, Morristown, NJ, USA, pages 177?182.Pang, Bo and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proc.
of the ACL.
pages115?124.Pang, Bo and Lillian Lee.
2008.
Opinion mining and sen-timent analysis.
Foundations and Trends in Informa-tion Retrieval 2(1-2):1?135.Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification using ma-chine learning techniques.
In EMNLP ?02: Proc.
of theconference on Empirical methods in natural languageprocessing.
ACL, Morristown, NJ, USA, pages 79?86.Popescu, Ana-Maria and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
InHLT ?05: Proc.
of the conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing.
ACL, Morristown, NJ, USA, pages339?346.Snyder, Benjamin and Regina Barzilay.
2007.
Multi-ple aspect ranking using the good grief algorithm.
InCandace L. Sidner, Tanja Schultz, Matthew Stone, andChengXiang Zhai, editors, HLT-NAACL.
The Associa-tion for Computational Linguistics, pages 300?307.Titov, Ivan and Ryan McDonald.
2008a.
A joint model oftext and aspect ratings for sentiment summarization.
InProc.
of ACL-08: HLT .
ACL, Columbus, Ohio, pages308?316.Titov, Ivan and RyanMcDonald.
2008b.
Modeling onlinereviews with multi-grain topic models.
In WWW ?08:Proc.
of the 17th international conference on WorldWide Web.
ACM, New York, NY, pages 111?120.Turney, Peter.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proc.
of 40th Annual Meeting ofthe Association for Computational Linguistics.
ACL,Philadelphia, Pennsylvania, USA, pages 417?424.Zagibalov, Taras and John Carroll.
2008.
Automatic seedword selection for unsupervised sentiment classifica-tion of chinese text.
In COLING ?08: Proc.
of the 22ndInternational Conference on Computational Linguis-tics.
ACL, Morristown, NJ, USA, pages 1073?1080.Zhu, X. and Z. Ghahramani.
2002.
Learning from labeledand unlabeled data with label propagation.
Technicalreport, CMU-CALD-02.812
