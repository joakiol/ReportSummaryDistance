A SYNTACTIC F ILTER ON PRONOMINAL  ANAPHORA FOR SLOT GRAMMARShalom Lappin and Michael McCordIBM T.J. Watson Research CenterP.O.
Box 704Yorktown Heights, NY 10598E-mail: Lappin/McCord@yktvmh.bitnetABS\]RACTWe propose a syntactic falter for identifyingnon-coreferential pronoun-NP pairs within asentence.
The filter applies to the output of aSlot Grammar parser and is formulated m termsof the head-argument structures which the parsergenerates.
It liandles control and unbounded e-pendency constructions without empty categoriesor binding chains, by virtue of the uniticationalnature of the parser.
The filter provides con-straints for a discourse semantics system, reducingthe search domain to which the inference rulesof the system's anaphora resolution componentapply.1.
INTRODUCTIONIn this paper we present an implemented al-gorithm which filters intra-sentential relations ofreferential dependence between pronouns andputative NP antecedents (both full and pronomi-nal NP's) for the syntactic representations pro-vided by an English Slot Grammar parser(McCord 1989b).
For  each parse of a sentence,the algorithm provides a list o7 pronoun-NP pairswhere referential dependence of the first elementon the second is excluded by syntactic con-straints.
The coverage of the filter has roughlythe same extension as conditions B and C OfChomsky's (1981, 1986) binding theory, tlow-ever, the formulation of the algorithm is sign!f" -icantly different from the conditions of thebinding theory, and from proposed implementa-tions of its conditions.
In particular, the filterformulates constraints on pronominal anaphorain terms of the head-argument structures providedby Slot Grammar syntactic representations ratherthan the configurational tree relations, partic-ularly c-command, .on which the binding theoryrelies.
As a result, the statements ofthe algorithmapply straightforwardly, and without special pro-vision, to a wide variety of constructions whichrecently proposed implementations of the bindingtheory do not handle without additional devices.Like the Slot Grammar whose input it applies to,the algorithm runs in Prolog, and it is stated inessentially declarative t rms.In Section 2 we give a brief description of SlotGrammar, and the parser we are employing.
Thesyntactic filter is presented in Section 3, firstthrough a statement of six constraints, each ofwhich is sufficient o rule out coreference, thenthrough a detailed description of the algorithmwhich implements these constraints.
We illus-trate the/algorithm with examples of the lists ofnon-corelerential p irs which it provides for par-ticular parses.
In Section 4 we compare our ap-proach to other proposals for syntactic filteringof pronominal anapliora which have appeared inthe literature.
We discuss Ilobbs algorithm, andwe take UP two recent implementations of thebinding theory.
Finally, in Section 5 we discussthe integration of our filter into other systems ofanaphora resolution.
We indicate how it can becombined with a VP anaphora lgorithm whichwe have recently completed.
We also outline theincorporation of our algorithm into LODUS(Bemth 1989), a system for discourse represen-tation.2.
SLOT GRAMMARThe original work on Slot Grammar was donearound 1976-78 and appeared in (McCord 1980).Recently, a new version (McCord 1989b) wasdeveloped in a logic programming framework, inconnection with fhe machine translation systemLMT (McCord 1989a,c,d).Slot Grammar is lexicalist and is dependen-cy-oriented.
Every phrase has a head word (witha given word sense and morphosyntactic fea-tures).
The constituents of a phrase besides tilehead word (also called the modifiers of the hcad)are obtained by "Idling" slots associated with thehead.
Slots are symbols like sub j, obj and iobjrepresenting grammatical relations, and are asso-ciated with a word (sense) in two ways.
Thelexical entry for the word specifies a set of com-plement slots (corresponding to arguments of tileword sense in logical form); and the grammarspecifies a set of ad/unct slots for each part of135speech.
A complement slot can be filled at mostonce, and an adjunct slot can by default be filledany number of times.The phenomena treated by augmented phrasestructure rules in some grammatical systems aretreated modularly by several different ypes ofrules in Slot Grammar.
The most important typeof rule is the (slot) filler rule, which gives condi-tions (expressed largely through unification) onthe filler phrase and its relations to the higherphrase.Filler rules are stated (normally) without ref-erence to conditions on order among constitu-ents.
But there are separately stated orderingrules, l Slot~head ordering rules state conditionson the position (left or fight) of the slot (fdler)relative to the head word.
Slot~slot ordering rulesplace conditions on the relative left-to-right orderof (the fillers of) two slots.A slot is obligatory (not optional) if it mustbe filled, either in the current phrase or in a raised~osition through left movement or coordination.djunct slots are always optional.
Complementslots are optional by default, but they may bespecified to be obligatory in a particular lexicalentry, or they may be so specifiedin the grammarby obligatory slot rules.
Such rules may be un-conditional or be conditional on the character-istics of the higher phrase.
They also may specifythat a slot is obligatory relative to the idling ofanother slot.
For example, the direct object slotin English.
may.
be d.eclared obligatory on theconditmn that the indirect object slot is filled bya noun phrase.One aim of Slot Grammar is to develop ap, owerful language-independent module, ashell", which can be used together with lan-guage-dependent modules, reducing the effort ofwriting grammars for new languages.
The SlotGrammar shell module includes the parser, whichis a bottom-up chart parser.
It also includes mostof the treatment of coordination, unbounded e-pendencies, controlled subjects, and punctuation.And the shell contains a system for evaluatingparses, extending tteidom's (1982)parse metric,which is used not only for ranking final parses butalso for pruning away unlikely partial analysesduring parsing, thus reducing the problem ofparse space explosion.
Parse evaluation expressespreferences for close attachment, for choice ofcomplements over adjuncts, and for parallelismin coordination.Although the shell contains most of the treat-ment of the above .phenomena (coordination,etc.
), a small part of their treatment is necessarilylanguage-dependent.
A (language-specific) gram-mar can include for instance (1) rules for coordi-nating feature structures that override the defaultsin the shell; (2) declarations of slots (called ex-traposer slots) that allow left extraposition ofother slots out oI their fdlers; (3) language-specificrules for punctuation that override defaults; and(4) language-specific controls over parse evalu-ation that override defaults.Currently, Slot Grammars are being devel-oped for English (ESG) by McCord, for Danish(DSG) by Arendse Bemth, and for German(GSG) by Ulrike Schwall.
ESG uses the UDIC'Flexicon (Byrd 1983, Klavans and Wacholder1989) having over 60,000 lemmas, with an inter-face that produces lot frames.
The fdter algo-r i thm has so far been successfully tested withESG and GSG.
(The adaptation to German wasdone by Ulrike Schwall.
)The algorithm applies in a second pass to theparse output, so the important hing in the re-mainder of this section is to describe Slot Gram-mar syntactic analysis tructures.A syntactic structure is a tree; each node ofthe tree represents a phrase in the sentence andhas a unique head word.
Formally, a phrase isrepresented by a termphrase(X,H,Sense,Features,s IotFrame,Ext,Hods),where the components are as follows: (1) X is alogical variable called the marker of the phrase.U/aifications of the marker play a crucial role inthe fdter algorithm.
(2) H is an integer epres-enting the position of the head word o f thephrase.
This integer identifies the phraseuniquely, and is used ha the fdter algorithm as theway of referring to phrases.
(3) Sense is theword sense of the head word.
(4) Features isthe feature structure of the head word and of thephrase.
It is a logic term (not an attribute-valuelist), which is generally rather sparse ha informa-tion, showing mainly the part of speech and in-flectional features of the head word.
(5)5 l otFrame is the list of complement slots, eachslot being ha the internal forms Iot(S iot,0b,X), where Slot is the slot name,0b shows whether it is an obligatory form ofSlot, and X is the slot marker.
The slot markeris unified (essentially) with the marker of the fillerphrase when the slot is fdled, even remotely, asin left movement or coordination.
Such unifica-tions are important for the filter algorithm.
(6)Ext is the list of slots that have been extraposedor raised to the level of the current phrase.
(7)The last component Hods represents he modifi-ers (daughters) of the phrase, and is of the formmods (LHods, RMods ) where LHods and RMods areTile distinction between slot filler rules and ordering constraints parallels the difference between Immediate Do-minance Rules and Linear Precedence Rules in GPSG.
See Gazdar et al(1985) for a characterization f ID andI,P rules in GPSG.
See (McCord 1989b) for more discussion of the relation of Slot Grammar to other systems.136Who did John say wanted to try to find him?subj(n)topsubj(n)auxcmp(inf(bare))obj(fin)preinfcomp(enlinfling)~ preinf obj(inf)obj(fin)who(X2) noundol(Xl,X3,X4) verbJohn(X3) nounsay(X4,X3,Xg,u) verbwant(X9,X2,X2,Xl2) verbpreinf(Xl2) preinftry(Xl2,X2,Xl3) verbpreinf(Xl3) preinffind(Xl3,X2,Xl4,u,u) verbhe(Xl4) nounFigure i.the lists of left modifiers and right modifiers, re-spectively.
Each member of a modifier list is ofthe form Slot:Phrase where Slot is a slot andPhrase is a phrase which flUs Slot.
Modifierlists reflect surface order, and a given slot mayappear more than once (if it is an adjunct).
Thusmodifier lists are not attribute-value lists.In Figure 1, a sample parse tree is shown,displayed by a procedure that uses only one lineper node and exhibits tree structure lines on theleft.
In this display, each line (representing anode) shows (1) the tree connection fines, (2) theslot filled by the node, (3) the word sensepredi-cation, and (4) the feature structure.
The featurestructure is abbreviated here by a display option,showin8 only the part of speech.
The word sensepredication consists of the sense name of the headword with the following arguments.
The first ar-gument is the marker variable for the phrase(node) itself; it is like an event or state variable forverbs.
The remaining arguments are the markervariables of the slots in the complement slotframe (u signifies "unbound").
As can be seen inthe display, the complement arguments are uni-fied with the marker variables of the fdler com-plement phrases., Note that in the example themarker X2 ol the who phrase is unified with thesubject variables of want, try, and find.
(There are also some unifications created by ad-junct slot Idling, which will not be describedhere.
)Forthe operation of the filter algorithm, thereis a prelim~ary step in which pertinent informa-tion about the parse tree is represented in a man-ner more convenient for the algorithm.
Asindicated above, nodes (phrases) t\]lemselves arerepresented by the word numbers of their headwords.
Properties of phrases and relations be-tween them are represented by unit clauses(predications) involving these integers (and otherdata), which are asserted into the Prolog work-space.
Because of this "dispersed" representationwith a collection of unit clauses, the originalphrase structure for the whole tree is firstgrounded (variables are bound to unique con-stants) before the unit clauses are created.As an example for this clausal representation,the clause has ar g (P, X) says that phrase P has Xone of its arguments; i.e., X is the slot markervariable for one of the complement slots of P.For the above sample parse, then, we would getclauseshasarg(5,'X2'), hasarg(5,'Xl2').as information about the "want' node (5).As another example, the clausephmarker(P,X) is added when phrase P hasmarker X.
Thus for the above sample, we wouldget the unit clausephmarker(I,'X2').An important predicate for the fdter algorithmis argm, defined byargm(P,Q) *- phmarker(P,X) &hasarg(Q,X).This says that phrase P is an argument of phraseQ.
This includes remote arguments and con-trolled subjects, because of the unifications ofmarker variables performed by the Slot Grammarparser.
Thus for the above parse, we would getargm(1,5), argm( 1,7).
argm( I ,9).showing that 'who' is an argument of 'want', "try',and "find'.3.
THE FILTER137A.A.I.B.B.I.C.C.l.a.b.C.d.e.
?.C.2.C.2.1.C.2.2.C.3.D.D.I.E.E.I.FoF.IThe Filter Algorithmnonrefdep(P,Q) ~ refpair(P,Q) & ncorefpair(P,Q).refpair(P,Q) ~ pron(p) & noun(Q) & P=/Q.ncorefpair(P,Q) ~ nonagr(P,Q) &/.nonagr(P,Q) ~ numdif(p,Q) I typedif(P,Q) I persdif(P,Q).ncorefpair(P,Q) ~ proncom(P,Q) &/.proncom(P,Q)argm(P,H) &(argm(Q,H) &/ I-pron(Q) &cont(Q,H) &(-subclcont(Q,T) I gt(Q,p)) &(~det(Q) I gt(Q,P))).cont_i(P,Q) ~ argm(P,Q) I adjunct(P,Q).cont(P,Q) ~ cont_i(P,Q).cont(P,Q) ~ cont_i(P,R) & R=/Q & cont(R,Q).subclcont(P,Q) ~ subconj(Q) & cont(P,Q).ncorefpair(P,Q) ~ prepcom(Q,P) &/.prepcom(Q,P) ~ argm(Q,H) & adjunct(R,H) & prep(R) & argm(P,R).ncorefpair(P,Q) ~ npcom(P,Q) &/.npcom(Q,P) ~ adjunct(Q,H) & noun(H) &(argm(P,H) \[adjunct(R,H) & prep(R) & argm(P,R)).ncorefpair(P,Q) ~ nppcom(P,Q) &/.nppcom(P,Q) ~ adjunct(P,H) & noun(H) &-pron(Q) & cont(Q,H).Figure 2.In preparation for stating the six constraints,we adopt the following definitions.
The agree-ment features of an NP are its number, personand gender features.
We will say that a phrase Pis in the argument domain of a phrase N iff P anN are both arguments of the same head.
We willalso say that P i s  in the adjunct domain of N iffN is an argument of a head tt, P is the object ofa preposition PREP, and PREP is an adjunct ofIt.
P is in the NP domain of N iff N is the det-erminer of a noun Qand (i) P is an argument ofQ, or (ii) P is the object of  a preposition PREPand Prep is an adjunct of Q.
The six constraintsare as follows.
A pronoun P is not coreferentialwith a noun phrase N if any of the followingconditions holds.I.
P and N have incompatible agreement features.II.
P is in the argument domain of N.III.
P is in the adjunct domain of N.IV.
P is an argument of a head H, N is not apronoun, and N is contained in tt.V.
P is in the NP domain of N.VI.
P is the determiner of a noun Q, and N iscontained in Q.The algorithm wlfich implements I-VI defines apredicate nonrefdep(P,q) wlfich is satisfied bya pair whose first element Is a pronoun and whosesecond element is an NP on which the pronouncannot be taken as referentially dependent, byvirtue of the syntactic relation between them.The main clauses of the algorithm are shown inFigure 2.Rule A specifies that the main goalnonrefdep(P,Q) is satisfied by <P ,Q> if this pairis a referential pair ( refpalr (P ,Q))  and a non-coreferential pair (neorefpair(P,Q)) .
A.1 de-frees a refpatr ,:P,Q> as one in which P is apronoun, Q'is a noun (either pronominal or non-pronominal), and P and Q are distinct.
Rules B,C, D, E, and F provide a disjunctive statementof the conditions under which the non-corefer-ence goal ncorefpair(P,Q) is satisfied, and soconst,tute the core of the algorithm.
Each ofthese rules concludes with a cut to prevent un-necessary backtracking which could generatelooping.Rule B, together with B. I, identifies the con-ditions under which constraint I holds.
In thefollowing example sentences, the pairs consistingof the second and the first coindexed expressionsin la-c (and in lc also the pair < T, 'she'> ) sat-isfy nonrefdep(P,Q) by virtue of rule B.la.
John i said that they i came.138b.
The woman i said that he i is funny.C.
I i believe that she  i is competent.? "
?
~ ,  t ,  ?
The algorithm Identifies they, John > as anonrefdep pair in la, which entails that 'they,cannot be taken as coreferential with John.However, (the referent of) "John" could of coursebe part of the reference set of 'they, and in suit-able discourses LODUS could identify this possi-bility.Rule C states that <P ,Q> is a non-coreferentialpl.~i.r, if it satisfies the pro ncom(P,Q) predicate.s holds under two conditions, correspondingto disjuncts C. 1.a-b and C.l.a,c-f.
The first con-dition specifies that the pronoun P and its puta-tive antecedent Q are both arguments of the samephrasal head, and so implements constraint II.This rules out referential dependence in 2a-b.2a.
Mary i likes her i.b.
She i tikes her i.Given the fact that Slot Grammar unifies the ar-gument and adjunct variables of a head with thephrases which t'dl these variable positions, it willalso exclude coreference in cases of control andunbounded ependency, as in 3a-c.3a.
Jo l t .
seems to want to see hirn~..b. Whi6h man i did he i see?
- -e. This is the girl i. Johh said she i saw.The second disjunct C.l.a,c-f covers cases inwhich the pronoun is an argument which ishigher up in the head-argument structure of thesentence than a non-pronominal noun.
This dis-junct corresponds to condition IV.
C.2-C.2.2provide a reeursive definition of containmentwithin aphrase.
This definition uses the relationof immediate containment, eont i (P ,Q), as thebase of the recursion, where con~ i (P ,Q) holdsif Q is either an argument or an adj'unct (modifieror determiner) of a head Q.
The second disjunctblocks coreference in 4a-c.4a.
He~ believes that the m.a% is amusing.b.
Who i did he i say Johr~.
hssed?c.
This Is the man i he i said John iwrote about.The wh-phrase in 4b and the head noun of therelative clause in 4c unify with variables in posi-tions contained within the phrase (more precise!y,the verb which heads the phrase) of which thepronoun is an argument.
Therefore, the algo-rithm identifies these nouns as impossible ante-cedents of the pronoun.The two final conditions of the second dis-junct, C. 1 .e and C. l.f, describe cases in which theantecedent of a pronoun is contained in a pre-ceding adjunct clause, and cases in which the an-tecedent is the determiner of an NP whichprecedes a pronoun, respectively.
These clausesprevent such structures from satisfying the non-coreference goal, and so permit referential de-pendence in 5a-b.5a.
After John i sang, he i danced.b.
Johni's motherlikes him i.Notice that because a determiner is an adjunct ofan NP and not an argument of the verb of whichthe NP is an argument, rule C. 1 also permits co-reference in 6.6.
His i mother likes John i.ltowever, C.l.a,c-e correctly excludes referentialdependence in 7, where the pronoun is an argu-ment which is higher than a noun adjunct.7.
He i likes Johni's mother.The algorithm permits backwards anaphora incases like 8, where the pronoun is not an argu-ment of a phrase 14 to wtfich its antecedent Q bearsthe con t (Q, fl ) relation.8.
After he i sang, John i danced.D-D.I block coreference between an NPwhich is the argument of a head H, and apronounthat is the object of a preposition heading a PPadjunct of 14, as in 9a-c.
These rules implementconstraint III.9a.
Sam.
i spoke about him i.b.
She i sat near her i.C.
Who i did he i ask for?Finally, E-E.I and F realize conditions V andVI, respectively, in NP internal non-coreferencecases like 10a-c.10a.
His i portrait of Jo .hnj.
is interesting.b.
JolL, i/s portrait of htrn i is interestmg.c.
Hisi description of the portrait by John iis interesting.Let us look at three examples of actual listsof pairs satisfying the nonrefdep redicate whichthe algorithm generates for particular parse treesof Slot Grammar.
The items in each pair areidentified by their words and word numbers, cor-responding to their sequential position in thestnng.When the sentence Who did John saywanted to try to find him?
is ~ven tothe system, the parse is as shown in Figure 1above, and the output of the filter is:Noncoref pairs:he.lO - who.l139Coreference ana lys i s  t ime = ii msec .Thus < "him','who' > is identified as a non-core-ferential pair, while coreference between 'John'and 'him is allowed.In Figure 3, the algorithm correctly lists< 'him ,'Bill > (6-3) as a non-coreferential pair,while permitting 'him' to take "John' as an ante-cedent.
In Fi~c~ure 4, it correctly excludes corefer-ence between him and 'John' (he.6-John.1), andallows him to be referentially dependent upon"Bill'.John expected Bill to impress him.IIsubj(n) John(X3) nountop expect(Xl,X3,X4,X5) verbobj Bill(X4) nounpreinf preinf(X5) preinfcomp(inf) impress(XS,X4,X6) verbobj he(X6) nounNoncoref pairs :he.6 - Bill.3Coreference analysis time = 5 msec.complement clause subiect, tlowever, in Figure4, the infinitival clause IS an adjunct of 'lectured'mid requires matrix subject control.4.
EXISTING PROPOSALS FOR CON-STRAINING PRONOMINAL ANAPHORAWe will discuss three suggestions which havebeen made in the computational literature forsyntactically constraining the relationship be-tween a pronoun and its set of possible antece.dents intra-sententially.
The first is Hobbs(1978) Algorithm, which performs a breadth-first,left-to-right search of the tree containing the pro-noun for possible antecedents.
The search is re-stricted to paths above the first NP or S nodecontaining the pronoun, and so the pronouncannot be boundby an antecedent in its minimalgoverning category.
If no antecedents are foundwithin the same tree as the pronoun, the trees ofthe previous entences in the text are searched inorder of proximity.
There are two main .difficul-ties with this approach.
First, it cannot be ap-plied to cases of  control in infinitival clauses, likethose given in Figures 3 and 4, or to unboundeddependencies, like those in Figure 1 and in ex-amples 3b-c and 4b-c, without significant modifi-cation.Figure 3.John lectured Bill to impress him.!
subj(n) John(X3) noun?
top lecture(Xl,X3,X4) verb\[ obj Bill(X4) noun~ preinf preinf(X5) preinfvnfvp impress(X5,X3,X6) verbobj he(X6) nounNoncoref pairs:he.6 - John.lCoreference analysis time = 5 msec.Figure 4.It makes this distinction by virtue of the differ-ences between the roles of the two infinitivalclauses in these sentences.
In Fi~gtjre 3, the infin-itival clause is a complement o1 "expected, andthis verb is marked for object control of theSecond, the algorithm is inefficient in designand violates modularity by virtue of the fact thatit computes both intra-sentential constraints onpronoriainal anaphora and inter-sentential nte-cedent possibilities each time it is invoked for anew pronoun in a tree.
Our system computes theset ofpronoun-NP pairs for which coreference issyntactically excluded in a single pass on a parsetree.
This set provides the input to a semantic-pragmatic discourse module which determinesanaphora by inference and preference rules.The other two proposals are presented inCorrea (1988), and in lngria and Stallard (1989).Both of these models are implementations oIChomsky's Binding theory which make use ofGovernment Binding type parsers.
They employessentially the same strategy.
This involves com-puting the set of possible antecedents of an ana-phor as the NP s which c-command the anaphorwithin a minimal domain (its minimal govet:ningcategory).
2 The minimal domain of an NP ischaracterized asthe first S, or the first NP withouta possessive subiect, in which it is contained.
Thepossible intra-sentential antecedents of a pronounare the set of NP's in the tree which are not in-cluded within this minimal domain.See Reinhart (1976) and (1983) for alternative definitions of c-command, and discussions of the role of this re-lation in determining the possibilities of anaphora.
See Lappin (1985) for additional discussion of the connectionbetween c-command and distinct varieties of pronominal anal3hora.
See Chomsky (1981), (1986a) and (1986b)for alternative definitions of the notion 'government' and 'rain,real governing category'.140This approach does sustain modularity bycomputing the set of possible antecedents for allpronouns within a tree in a single pass operation,prior to the application of inter-sentential searchprocedures.
The main difficulty with the modelis that because constraints on pronominal ana-phora are stated entirely in terms of configura-tional relations of tree geometry, specifically, interms of c-command and minimal dominating Sand NP domains, control and unbounded e-p endency structures can only be handled b~' ad- itional and fairly complex devices.
It isnecessary to generate mpty categories for PROand trace in appropriate positions in parse trees.Additional algorithms must be invoked to specifythe chains of  control (A-binding) for PRO, andoperator (A )-binding for trace in order to linkthese categories to the constituents which bindthem.
The algorithm which computes possibleantecedents for anaphors and pronouns must beformulated so that ii identifies the head of such achain as non-coreferential with a pronoun oranaphor (in the sense of the Binding theory), ifany element of the chain is excluded as a possibleantecedent.Neither empty categories nor binding chainsare required in our system.
In Slot Grammarparse representations, wh-phrases, heads of rela-tive clauses, and NP's which control the subjectsof inf'mitival clauses are unified with the variablescorresponding to the roles they bind in argumentpositions.
Tlierefore, the clauses of the algorithmapply to these constructions directly, and withoutadditional devices or stipulations)5.
THE INTEGRATION OF THE FILTERINTO OTHER SYSTEMS OF ANAPHORARESOLUTIONWe have recently implemented an algorithmfor the interpretation of intrasentential VP ana-phora structures like those in 1 la-c.1 l a. John arrived, and Mary did too.b.
Bill read every book which Sam saidhe did.c.
Max wrote a letter to Bill before Marydid to John.The VP anaphora lgorithm generates a secondtree which copies the antecedent verb into theposition of the head of the elliptical VP.
It alsolists the new arguments and adjuncts which thecopied verb inhei'its from its antecedent.
We haveintegrated our filter on pronominal anaphora intothis algorithm, so that the filter applies to the in-terpreted trees which the algorithm generates.consider12.
John likes to him, and Bill does too.If the \[dter applies to the parse of 11, it willidentify only .< him, John'> as a non-corefer-ential pair, gwen that the pair <'him','Bill'>doesn t satisfy any of the conditions of the filteralgorithm.
Ilowever, when the filter is applied tothe interpreted VP anaphora tree of 12, the filteralgorithm correctly identifies both pronoun-NPpairs, as shown in the VP output of the algorithmfor 12 given in Figure 5.John likes him, and Bi l ldoes too.Antecedent Verb-Elliptical Verb Pairs.like.2 - dol.7Elliptical Verb-New Argument Pairs.like.7 - he.3Interpreted VP anaphora tree.subj John(X9) noun~ iconj like(X8,X9,Xl0) verbobj he(Xl0) noun?
top and(Xi,X8,Xll) verb ~ subj BilI(XI2) noun rconj like(Xll,Xl2,Xl0) verbvadv too(Xll) advNon-Coreferential Pronoun-NP Pairs.he.3 - John.l, he.3 - Bill.6Coreference analysis time = 70 msec.Figure 5.Our filter also provides input to a discourseunderstanding system, LODUS, designed andimplemented by A. Bernth, and described in(..Bernth 1988, 1989).
LOI)US creates a singlediscourse structure from the analyses of the S|0tGrammar parser for several sentences.
It inter-prets each sentence analysis in the context con-sisting of the discourse processed so far, togetherwith domain knowledge, and it then embeds itinto the discourse structure.
The process of in-te.rpretation consists in applying rules of inferencewhich encode semantic and pragmatic (know-In fact, a more complicated algorithm with approximately tile same coverage as our lilter can be formulated fi, ra parser which produces configurational surlhce trees wiulout empty categories and binding chains, if the parserprovides deep grammatical roles at some level of representation.
The first author has implemented such an al-gorithm for the PEG parser.
For a general description of I'EG, see Jensen (1986).
The current version of \['E(;provides information on deep grammatical roles by means of second pass rules which apply to the initial parserecord structure.
The algorithm employs both c-command and reference to deep grammatical roles.141ledge-based) relations among lexical items, anddiscourse structures.
The fdter reduces the set oIpossible antecedents which the anaphora resol-ution component of LODUS considers for pro-nouns.
For example, this component will notconsider 'the cat or that' as a .p, ossible antece-dents for either occurrence of it in the secondsentence in 13, but only "the mouse' in the firstsentence of this discourse.
This is due to the factthat our fdter lists the excluded pairs togetherwith the parse tree of the second sentence.13.
The mouse ran in.The cat that saw it ate it.Thus, the fdter significantly reduces the searchspace which the anaphora resolution componentof LODUS must process.
The interface betweenour filter and LODUS embodies the sort of mo-dular interaction of syntactic and semantic-prag-matic components which we see as important tothe successful operation and efficiency of anyanaphora resolution system.ACKNOWLEDGMENTSWe are grateful to Arendse Bemth, MartinChodorow, and Wlodek Zadrozny for helpfulcomments and advice on proposals contained inthis paper.REFERENCESBemth, A.
(1988) Computational Discourse Se-mantics, Doctoral Dmsertation, U. Copenha-gen and IBM Research.Bemth, A.
(1989) "Discourse Understanding InLo~c", Proc.
North American Conference onLogic Programming, pp.
755-771, MIT Press.Byrd, R. J.
(1983) "Word Formation in NaturalLanguage Processing Systems," ProceedingsoflJCAI-VIII, pp.
704-706.Chomsky, N. (1981) Lectures on Government andBinding, Foils, Dordrecht.Chomsky, N. (1986a) Knowledge of Language:Its Nature, Origin, and Use, Praeger, NewYork.Chomsky, N. (1986b) Barriers, MIT Press,Cambridge, Mass.Correa, N. (1988) "A B'_m,,ding Rule for Govern-ment-Binding Parsing , COLING "88, Buda-pest, pp.
123-129.Gazdar, G., E. Klein, G. Pullum, and I. Sag,G1985) Generalized Phrase Structurerammar, Blackwell, Oxford.Heidorn, G. E. (1982) "Experience with an EasilyComputed Metric for Ranking AlternativeParses," Proceedings ofAnnual ACL Meeting,1982, pp.
82-84.I tobbs, J.
(1978) j'Resolving l'ronounReferences", Lingua 44, pp.
311-338.Ingria, R. and D. Stallard (1989) "A Computa-tional Mechanism for PronominalReference", Proceedings of the 27th AnnualMeeting of the Association for ComputationalLinguistics, Vancouver, pp.
262-271.Jensen, K. (,1986) "PEG: A Broad-CoverageComputatmnal Syntax of English," TechnicalReport, IBM T.J. Watson Research Center,Yorktown Heights, NY.Klavans, J. L. and Wacholder, N. (1989) "Doc-umentation of Features and Attributes inUDICT," Research Report RC14251, IBMT.J.
Watson Research Center, YorktownHeights, N.Y.Lappin, S. (1985) "Pronominal Binding and Co-reference", Theoretical Linguistics 12, pp.241-263.McCord, M. C. (1980) "Slot Grammars," Com-putational Linguistics, vol.
6, pp.
31-43.McCord, M. C. (1989a) "Design of LMT: AProlog-based Machine Translation System,"Computational Linguistics, vol.
15, pp.
33-52.McCord, M. C. (1989b) "A New Version of SlotGrammar," Research Report RC 14506, IBMResearch Division, Yorktown Iteights, NY10598.McCord, M. C. (198%) "A New Version of theMachine Translation System LMT," to ap-pear in Proc.
International Scientific Sympo-sium on Natural Language and Logic, SpringerLecture Notes in Computer Science, and inJ.
Literary and Linguistic Computing.McCord, M. C. (1989d) "LMT," Proceedings ofMT Summit II, pp.
94-99, Deutsche GeseU-schaft f'tir Dokumentation, Frankfurt.Reinhart, T. (1976) The Syntactic Domain ofAnaphora, Doctoral Dissertation, MIT, Cam-bridge, Mass.Reinhart, T. (1983) Anaphora, Croom Ilelm,London.142
