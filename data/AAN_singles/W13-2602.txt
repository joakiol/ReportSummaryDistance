Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 11?20,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsA model of generalization in distributional learning of phonetic categoriesBozena PajakBrain & Cognitive SciencesUniversity of RochesterRochester, NY 14627-0268bpajak@bcs.rochester.eduKlinton BicknellPsychologyUC San DiegoLa Jolla, CA 92093-0109kbicknell@ucsd.eduRoger LevyLinguisticsUC San DiegoLa Jolla, CA 92093-0108rlevy@ucsd.eduAbstractComputational work in the past decadehas produced several models accountingfor phonetic category learning from distri-butional and lexical cues.
However, therehave been no computational proposals forhow people might use another powerfullearning mechanism: generalization fromlearned to analogous distinctions (e.g.,from /b/?/p/ to /g/?/k/).
Here, we presenta new simple model of generalization inphonetic category learning, formalized ina hierarchical Bayesian framework.
Themodel captures our proposal that linguis-tic knowledge includes the possibility thatcategory types in a language (such asvoiced and voiceless) can be shared acrosssound classes (such as labial and velar),thus naturally leading to generalization.We present two sets of simulations thatreproduce key features of human perfor-mance in behavioral experiments, and wediscuss the model?s implications and di-rections for future research.1 IntroductionOne of the central problems in language acqui-sition is how phonetic categories are learned, anunsupervised learning problem involving mappingphonetic tokens that vary along continuous di-mensions onto discrete categories.
This task maybe facilitated by languages?
extensive re-use of aset of phonetic dimensions (Clements 2003), be-cause learning one distinction (e.g., /b/?/p/ vary-ing along the voice onset time (VOT) dimension)might help learn analogous distinctions (e.g., /d/?/t/, /g/?k/).
Existing experimental evidence sup-ports this view: both infants and adults general-ize newly learned phonetic category distinctions tountrained sounds along the same dimension (Mc-Claskey et al1983, Maye et al2008, Perfors& Dunbar 2010, Pajak & Levy 2011a).
However,while many models have been proposed to accountfor learning of phonetic categories (de Boer &Kuhl 2003, Vallabha et al2007, McMurray et al2009, Feldman et al2009, Toscano & McMur-ray 2010, Dillon et al2013), there have been nocomputational proposals for how generalizationto analogous distinctions may be accomplished.Here, we present a new simple model of gener-alization in phonetic category learning, formal-ized in a hierarchical Bayesian framework.
Themodel captures our proposal that linguistic knowl-edge includes the possibility that category typesin a language (such as voiced and voiceless) canbe shared across sound classes (defined as previ-ously learned category groupings, such as vowels,consonants, nasals, fricatives, etc.
), thus naturallyleading to generalization.One difficulty for the view that learning one dis-tinction might help learn analogous distinctions isthat there is variability in how the same distinc-tion type is implemented phonetically for differ-ent sound classes.
For example, VOT values areconsistently lower for labials (/b/?/p/) than for ve-lars (/g/?/k/) (Lisker & Abramson 1970), and thedurations of singleton and geminate consonantsare shorter for nasals (such as /n/?/nn/) than forvoiceless fricatives (such as /s/?/ss/) (Giovanardi& Di Benedetto 1998, Mattei & Di Benedetto2000).
Improving on our basic model, we imple-ment a modification that deals with this difficultyby explicitly building in the possibility for analo-gous categories along the same dimension to havedifferent absolute phonetic values along that di-mension (e.g., shorter overall durations for nasalsthan for fricatives).In Section 2 we discuss the relevant backgroundon phonetic category learning, including previ-ous modeling work.
Section 3 describes our ba-sic computational model, and Section 4 presentssimulations demonstrating that the model can re-11produce the qualitative patterns shown by adultlearners in cases when there is no phonetic vari-ability between sound classes.
In Section 5 wedescribe the extended model that accommodatesphonetic variability across sound classes, and inSection 6 we show that the improved model qual-itatively matches adult learner performance bothwhen the sound classes implement analogous dis-tinction types in identical ways, and when they dif-fer in the exact phonetic implementation.
Section 7concludes with discussion of future research.2 BackgroundOne important source of information for unsuper-vised learning of phonetic categories is the shapeof the distribution of acoustic-phonetic cues.
Forexample, under the assumption that each phoneticcategory has a unimodal distribution on a particu-lar cue, the number of modes in the distributionof phonetic cues can provide information aboutthe number of categories: a unimodal distributionalong some continuous acoustic dimension, suchas VOT, may indicate a single category (e.g., /p/,as in Hawaiian); a bimodal distribution may sug-gest a two-category distinction (e.g., /b/ vs. /p/, asin English); and a trimodal distribution implies athree-category distinction (e.g., /b/, /p/, and /ph/,as in Thai).
Infants extract this distributional infor-mation from the speech signal (Maye et al2002,2008) and form category representations focusedaround the modal values of categories (Kuhl 1991,Kuhl et al1992, Lacerda 1995).
Furthermore, in-formation about some categories bootstraps learn-ing of others: infants exposed to a novel bimodaldistribution along the VOT dimension for oneplace of articulation (e.g., alveolar) not only learnthat novel distinction, but also generalize it to ananalogous contrast for another (e.g., velar) placeof articulation (Maye et al2008).
This ability ispreserved beyond infancy, and is potentially usedduring second language learning, as adults are alsoable to both learn from distributional cues and usethis information when making category judgmentsabout untrained sounds along the same dimensions(Maye & Gerken 2000, 2001, Perfors & Dunbar2010, Pajak & Levy 2011a,b).The phonetic variability in how different soundclasses implement the same distinction type mightin principle hinder generalization across classes.However, there is evidence of generalization evenin cases when sound classes differ in the exactphonetic implementation of a shared distinctiontype.
For example, learning a singleton/geminatelength contrast for the class of voiceless fricatives(e.g., /s/?/ss/, /f/?/ff/) generalizes to the class ofsonorants (e.g., /n/?/nn/, /j/?/jj/) even when the ab-solute durations of sounds in the two classes aredifferent ?
overall longer for fricatives than forsonorants (Pajak & Levy 2011a) ?
indicating thatlearners are able to accomodate the variability ofphonetic cues across different sound classes.Phonetic categorization from distributional cueshas been modeled using Gaussian mixture mod-els, where each category is represented as a Gaus-sian distribution with a mean and covariance ma-trix, and category learning involves estimatingthe parameters of each mixture component and?
for some models ?
the number of components(de Boer & Kuhl 2003, Vallabha et al2007, Mc-Murray et al2009, Feldman et al2009, Toscano& McMurray 2010, Dillon et al2013).1 Thesemodels are successful at accounting for distribu-tional learning, but do not model generalization.We build on this previous work (specifically, themodel in Feldman et al2009) and implement gen-eralization of phonetic distinctions across differentsound classes.3 Basic generalization modelThe main question we are addressing here con-cerns the mechanisms underlying generalization.How do learners make use of information aboutsome phonetic categories when learning othercategories?
Our proposal is that learners expectcategory types (such as singleton and geminate,or voiced and voiceless) to be shared amongsound classes (such as sonorants and fricatives).We implement this proposal with a hierarchicalDirichlet process (Teh et al2006), which allowsfor sharing categories across data groups (here,sound classes).
We build on previous computa-tional work in this area that models phonetic cate-gories as Gaussian distributions.
Furthermore, wefollow Feldman et al(2009) in using Dirichletprocesses (Ferguson 1973), which allow the modelto learn the number of categories from the data,and implementing the process of learning fromdistributional cues via nonparametric Bayesian in-ference.1In Dillon et al(2013) each phoneme is modeled as amixture of Gaussians, where each component is an allophone.12HG0?Gc?0zicdic i ?
{1..nc}c ?
CFigure 1: The graphical representation of the basicmodel.H : ?
?
N (?0, ?2?0 )?2 ?
InvChiSq(?0,?20 )G0 ?
DP(?,H)Gc ?
DP(?0,G0)zic ?
Gcdic ?
N (?zic ,?2zic)fc ?
N (0,?2f )dic ?
N (?zic ,?2zic)+ fcFigure 2: Mathematical description of the model.The variables below the dotted line refer to the ex-tended model in Figure 6.3.1 Model detailsAs a first approach, we consider a simplified sce-nario of a language with a set of sound classes,each of which contains an unknown number ofphonetic categories, with perceptual token definedas a value along a single phonetic dimension.The model learns the set of phonetic categoriesin each sound class, and the number of categoriesinferred for one class can inform the inferencesabout the other class.
Here, we make the simpli-fying assumption that learners acquire a context-independent distribution over sounds, although themodel could be extended to use linguistic con-text (such as coarticulatory or lexical information;Feldman et al2009).Figure 1 provides the graphical representationof the model, and Figure 2 gives its mathematicalVariable ExplanationHbase distribution over means andvariances of categoriesG0distribution over possiblecategoriesGcdistribution over categories inclass c?,?0 concentration parameterszic category for datapoint dicdic datapoint (perceptual token)nc number of datapoints in class cC set of classesfc offset parameter?
f standard deviation of prior on fcTable 1: Key for the variables in Figures 1, 2,and 6.
The variables below the dotted line referto the extended model in Figure 6.description.
Table 1 provides the key to the modelvariables.
In the model, speech sounds are pro-duced by selecting a phonetic category zic, whichis defined as a mean ?zic and variance ?2zic alonga single phonetic dimension,2 and then samplinga phonetic value from a Gaussian with that meanand variance.
We assume a weak prior over cat-egories that does not reflect learners?
prior lan-guage knowledge (but we return to the possiblerole of prior language knowledge in the discus-sion).
Learners?
beliefs about the sound inventory(distribution over categories and mean and vari-ance of each category) are encoded through a hier-archical Dirichlet process.
Each category is sam-pled from the distribution Gc, which is the distri-bution over categories in a single sound class.
Inorder to allow sharing of categories across classes,the Gc distribution for each class is sampled from aDirichlet process with base distribution G0, whichis shared across classes, and concentration param-eter ?0 (which determines the sparsity of the dis-tribution over categories).
G0, then, stores the fullset of categories realized in any class, and it issampled from a Dirichlet process with concentra-tion parameter ?
and base distribution H, whichis a normal inverse chi-squared prior on category2Although we are modeling phonetic categories as havingvalues along a single dimension, the model can be straight-forwardly extended to multiple dimensions, in which case thevariance would be replaced by a covariance matrix ?zic .13means and variances.3 The parameters of the nor-mal inverse chi-squared distribution are: ?0 and ?0,which can be thought of as pseudo-observations,as well as ?0 and ?20 , which determine the priordistribution over means and variances, as in Fig-ure 2.3.2 InferenceThe model takes as input the parameters of thebase distribution H, the concentration parameters?0 and ?
, and the data, which is composed of alist of phonetic values.
The model infers a poste-rior distribution over category labels for each data-point via Gibbs sampling.
Each iteration of Gibbssampling resamples the assignments of each data-point to a lower-level category (in Gc) and also re-samples the assignments of lower-level categoriesto higher-level categories (in G0).
We marginalizeover the category means and variances.4 Simulations: basic modelThe first set of simulations has three goals: first,to establish that our model can successfully per-form distributional learning and second, to showthat it can use information about one type of classto influence judgements about another, in the casethat there is no variability in category structurebetween classes.
Finally, these simulations reveala limitation of this basic model, showing that itcannot generalize in the presence of substantialbetween-class variability in category realizations.We address this limitation in Section 5.4.1 The dataThe data we use to evaluate the model comefrom the behavioral experiments in Pajak & Levy(2011a).
Adult native English speakers were ex-posed to novel words, where the middle conso-nant varied along the length dimension from short(e.g., [ama]) to long (e.g., [amma]).
The distri-butional information suggested either one cate-gory along the length dimension (unimodal distri-bution) or two categories (bimodal distribution),as illustrated in Figure 3.
In Experiment 1, thetraining included sounds in the sonorant class (4continua: [n]-...-[nn], [m]-...-[mm], [j]-...-[jj], [l]-...-[ll]) with the duration range of 100?205msec.In Experiment 2 the training included sounds in3In the case of categories defined along multiple di-mensions, the base distribution would be a normal inverse-Wishart.l ll l04812160481216Expt1:sonorantsExpt2:fricatives100 120 140 160 180 200 220 240 260 280Stimuli length continuum (in msec)Familiarizationfrequencybimodal unimodalFigure 3: Experiment 1 & 2 training (Pajak andLevy 2011a).
The y axis reflects the frequencyof tokens from each training continuum.
The fourpoints indicate the values of the untrained data-points.the voiceless fricative class (4 continua: [s]-...-[ss], [f]-...-[ff], [T]-...-[TT], [S]-...-[SS]) with the du-ration range of 140?280msec.
The difference induration ranges between the two classes reflectedthe natural duration distributions of sounds inthese classes: generally shorter for sonorants andlonger for fricatives (Greenberg 1996, Giovanardi& Di Benedetto 1998, Mattei & Di Benedetto2000).Subsequently, participants?
expectations aboutthe number of categories in the trained class andanother untrained class were probed by asking forjudgments about tokens at the endpoints of thecontinua: participants were presented with pairsof words (e.g., sonorant [ama]?
[amma] or frica-tive [asa]?
[assa]) and asked whether these weretwo different words in this language or two rep-etitions of the same word.
As illustrated in Ta-ble 2, in the test phase of Experiment 1 the du-rations of both the trained and the untrained classwere identical (100msec for any short consonantand 205msec for any long consonant), whereasin the test phase of Experiment 2 the durationswere class-specific: longer for trained fricatives(140msec for a short fricative and 280msec for along fricative) and shorter for untrained sonorants(100msec for a short sonorant and 205msec for along sonorant).The experiment results are illustrated in Fig-ure 4.
The data from the ?trained?
condition showsthat learners were able to infer the number of cat-egories from distributional cues: they were more14Expt1?trained Expt1?untrainedProportion of 'different'responses0.00.20.40.60.81.0BimodalUnimodalExpt2?trained Expt2?untrainedProportion of 'different'responses0.00.20.40.60.81.0BimodalUnimodalFigure 4: Experiment 1 & 2 results: proportion of ?different?
responses on ?different?
trials (Pajak andLevy, 2011a).Expt.
1 Expt.
2trained(sonorants) (fricatives)100ms ?
205ms 140ms ?
280msuntrained(fricatives) (sonorants)100ms ?
205ms 100ms ?
205msTable 2: Experiment 1 & 2 test (Pajak and Levy,2011a).likely to posit two categories (i.e., respond ?dif-ferent?
on ?different?
trials) when the distributionwas bimodal than when the distribution was uni-modal.
In addition, as demonstrated by the ?un-trained?
condition, learners used the informationabout the trained class to make inferences aboutthe untrained class: they were more likely to ac-cept length-based category distinctions for frica-tives after learning the distinction for sonorants(Expt.
1), and vice versa (Expt.
2).
This general-ization occurred both (a) when each class imple-mented the distinction in exactly the same way(with the same absolute durations; Expt.
1), and(b) when the classes differed in how the shared dis-tinction type was implemented (the absolute dura-tions of the untrained class were shifted relative tothe trained class; Expt.
2).The model simulations described below at-tempt to replicate the key features of human per-formance: distributional learning and generaliza-tion.
We model both experiments of Pajak &Levy (2011a): (a) ?same durations?
across classes(Expt.
1), and (b) ?different durations?
acrossclasses (Expt.
2).
Thus, the datasets we usedwere closely modeled after their experimental de-sign: (1) Expt.
1 bimodal, (2) Expt.
1 unimodal,(3) Expt.
2 bimodal, and (4) Expt.
2 unimodal.
Ineach dataset, the data consisted of a list of pho-netic values (duration in msec), where each data-point was tagged as belonging to either the sono-rant or the fricative class.
The frequencies of the?trained?
class were as listed in Figure 3 (simu-lating a single training continuum).
In addition tothe ?trained?
class, each dataset included two dat-apoints from the ?untrained?
class with the valuesas listed in Table 2 in the ?untrained?
condition.These two datapoints were included in order toevaluate the model?s categorization of sounds forwhich no distributional evidence is available, thusassessing the extent of generalization.
We sim-ulated weak perceptual noise by adding to eachdatapoint normally-distributed error with standarddeviation of 0.3 times the distance between adja-cent continuum steps.4.2 MethodologyWe ran the basic model on each of the fourdatasets.
For each, we performed 1,000,000 iter-ations of Gibbs sampling, and analyzed the re-sults for the second half.
To assess convergence,we ran four Markov chains for each dataset, us-ing two overdispersed initializations: (1) assigningone category label to all datapoints, and (2) as-signing a different label to each datapoint.
Weused a weak prior base distribution H (?0 = .001;?0 = .001; ?20 = 1; ?0 was set to the overall meanof the data), and set the concentration parameters?
= ?0 = 1.4.3 Results and discussionThe simulation results are illustrated in Figure 5,4plotting the proportion of samples on which themodel assigned the datapoints to two different cat-egories, as opposed to a single category.5 Note that4All variables we report in all simulations appear to haveconverged to the posterior, as assessed by R?
values of 1.1 orless, calculated across the 4 chains (Gelman & Rubin 1992).5No models we report assign the trained category data-points to more than two categories more than 1% of the time.15trained untrained0.000.250.500.751.00bimodalunimodalbimodalunimodalProportionof2?categoryinferencesBasic model:Experiment 1trained untrained0.000.250.500.751.00bimodalunimodalbimodalunimodalProportionof2?categoryinferencesBasic model:Experiment 2Figure 5: Simulation results for the basic model.Error bars give 95% binomial confidence intervals,computed using the estimated number of effec-tively independent samples in the Markov chains.in the ?trained?
condition, this means categoriza-tion of all datapoints along the continuum.
In the?untrained?
condition, on the other hand, it is cat-egorization of two datapoints: one from each end-point of the continuum.The results in the ?trained?
conditions demon-strate that the model was able to learn from thedistributional cues, thus replicating the success ofprevious phonetic category learning models.Of most interest here are the results in the ?un-trained?
condition.
The figure on the left showsthe results modeling the ?same-durations?
exper-iment (Expt.
1), demonstrating that the model cat-egorizes the two datapoints in the untrained soundclass in exactly the same way as it did for thetrained sound class: two categories in the bimodalcondition, and one category in the unimodal con-dition.
Thus, these results suggest that we can suc-cessfully model generalization of distinction typesacross sound classes in phonetic category learningby assuming that learners have an expectation thatcategory types (such as short and long, or voice-less and voiced) may be shared across classes.The figure on the right shows the results model-ing the ?different-durations?
experiment (Expt.
2),revealing a limitation of the model: failure to gen-eralize when the untrained class has the same cat-egory structure but different absolute phonetic val-ues (overall shorter in the untrained class than inthe trained class).
Instead, the model categorizesboth untrained datapoints as belonging to a singlecategory.
This result diverges from the experimen-tal results, where learners generalize the learneddistinction type in both cases, whether the abso-lute phonetic values of the analogous categoriesare identical or not.
We address this problem inthe next section by implementing a modificationto the model that allows more flexibility in howeach class implements the same category types.5 Extended generalization modelThe goal of the extended model is to explicitly al-low for phonetic variability across sound classes.As a general approach, we could imagine func-tions that transform categories across classes sothat the same categories can be ?reused?
by be-ing translated around to different parts of the pho-netic space.
These functions would be specific op-erations representing any intrinsic differences be-tween sound classes.
Here, we use a very simplefunction that can account for one widely attestedtype of transformation: different absolute phoneticvalues for analogous categories in distinct soundclasses (Ladefoged & Maddieson 1996), such aslonger overall durations for voiceless fricativesthan for sonorants.
This type of transformationhas been successfully used in prior modeling workto account for learning allophones of a singlephoneme that systematically vary in phonetic val-ues along certain dimensions (Dillon et al2013).5.1 Model detailsWe implement the possibility for between-classvariability by allowing for one specific type ofidiosyncratic implementation of categories acrossclasses: learnable class-specific ?offsets?
by whichthe data in a class are shifted along the phoneticdimension, as illustrated in Figure 6 (the key forthe variables is in Table 1).5.2 InferenceEach iteration of MCMC now includes aMetropolis-Hastings step to resample the offsetparameters fc, which uses a zero-mean Gaussianproposal, with standard deviation ?p = range of data5 .6 Simulations: extended modelThis second set of simulations has two goals: (1) toestablish that the extended model can successfullyreplicate the performance of the basic model inboth distributional learning and generalization inthe no-variability case, and (2) to show that ex-plicitly allowing for variability across classes letsthe model generalize when there is between-classvariability in category realizations.16HG0?Gc?0zicdicfc?
fi ?
{1..nc}c ?
CFigure 6: The graphical representation of the ex-tended model.6.1 MethodologyWe used the same prior as in the first set of sim-ulations, and used a Gaussian prior on the offsetparameter with standard deviation ?
f = 1000.
Be-cause only the relative values of offset parametersare important for category sharing across classes,we set the offset parameter for one of the classesto zero.
The four Markov chains now crossed cate-gory initialization with two different initial valuesof the offset parameter.6.2 Results and discussionThe simulation results are illustrated in Fig-ure 7.
The figure on the left demonstrates thatthe extended model performs similarly to the ba-sic model in the case of no variability betweenclasses.
The figure on the right, on the otherhand, shows that ?
unlike the basic model ?the extended model succeeds in generalizing thelearned distinction type to an untrained soundclass when there is phonetic variability betweenclasses.
These results suggest that allowing forvariability in category implementations acrosssound classes may be necessary to account forhuman learning.
Taken together, these results areconsistent with our proposal that language learn-ers have an expectation that category types can beshared across sound classes.
Furthermore, learn-ers appear to have implicit knowledge of the waysthat sound classes can vary in their exact phoneticimplementations of different category types.
Thistrained untrained0.000.250.500.751.00bimodalunimodalbimodalunimodalProportionof2?categoryinferencesExtended model:Experiment 1trained untrained0.000.250.500.751.00bimodalunimodalbimodalunimodalProportionof2?categoryinferencesExtended model:Experiment 2Figure 7: Simulation results for the extendedmodel.
Error bars give 95% binomial confidenceintervals, computed using the estimated numberof effectively independent samples in the Markovchains.type of knowledge may include ?
as in our ex-tended generalization model ?
the possibility thatphonetic values of categories in one class can besystematically shifted relative to another.7 General discussionIn this paper we presented the first model of gen-eralization in phonetic category learning, in whichlearning a distinction type for one set of sounds(e.g., /m/?/mm/) immediately generalizes to an-other set of sounds (e.g., /s/?/ss/), thus reproduc-ing the key features of adult learner performancein behavioral experiments.
This extends previouscomputational work in phonetic category learn-ing, which focused on modeling the process oflearning from distributional cues, and did not ad-dress the question of generalization.
The basicpremise of the proposed model is that learners?knowledge of phonetic categories is representedhierarchically: individual sounds are grouped intocategories, and individual categories are groupedinto sound classes.
Crucially, the category struc-ture established for one sound class can be di-rectly shared with another class, although differ-ent classes can implement the categories in id-iosyncratic ways, thus mimicking natural variabil-ity in how analogous categories (e.g., short /m/ and/s/, or long /mm/ and /ss/) are phonetically imple-mented for different sound classes.The simulation results we presented succeedin reproducing the human pattern of generaliza-tion performance, in which the proportion of two-category inferences about the untrained class is17very similar to that for the trained class.
Note,however, that there are clear quantitative dif-ferences between the two in learning perfor-mance: the model learns almost perfectly fromthe available distributional cues (?trained?
condi-tion), while adult learners are overall very conser-vative in accepting two categories along the lengthdimension, as indicated by the overall low num-ber of ?different?
responses.
There are two mainreasons why the model might be showing moreextreme categorization preferences than humansin this particular task.
First, humans have cogni-tive limitations that the current model does not,such as those related to memory or attention.
Inparticular, imperfect memory makes it harder forhumans to integrate the distributional informationfrom all the trials in the exposure, and longer train-ing would presumably improve performance.
Sec-ond, adults have strong native-language biases thataffect learning of a second language (Flege 1995).The population tested by Pajak & Levy (2011a)consisted of adult native speakers of American En-glish, a language in which length is not used con-trastively.
Thus, the low number of ?different?
re-sponses in the experiments can be attributed toparticipants?
prior bias against category distinc-tions based on length.
The model, on the otherhand, has only a weak prior that was meant to beeasily overridden by data.This last point is of direct relevance for thearea of second language (L2) acquisition, whereone of the main research foci is to investigatethe effects of native-language knowledge on L2learning.
The model we proposed here can poten-tially be used to systematically investigate the roleof native-language biases when learning categorydistinctions in a new language.
In particular, an L2learner, whose linguistic representations includetwo languages, could be implemented by addinga language-level node to the model?s hierarchicalstructure (through an additional Dirichlet process).This extension will allow for category structures tobe shared not just within a language for differentsound classes, but also across languages, thus ef-fectively acting as a native-language bias.As a final note, we briefly discuss alternativeways of modeling generalization in phonetic cat-egory learning.
In the model we described in thispaper, whole categories are generalized from oneclass to another.
However, one might imagine an-other approach to this problem where generaliza-tion is a byproduct of learners?
attending moreto the dimension that they find to be relevant fordistinguishing between some categories in a lan-guage.
That is, learners?
knowledge would not in-clude the expectation that whole categories maybe shared across classes, as we argued here, butrather that a given phonetic dimension is likelyto be reused to distinguish between categories inmultiple sound classes.This intuition could be implemented in differ-ent ways.
In a Dirichlet process model of categorylearning, the concentration parameter ?
might belearned, and shared for all classes along a givenphonetic dimension, thus producing a bias to-ward having a similar number of categories acrossclasses.
Alternatively, the variance of categoriesalong a given dimension might be learned, andalso shared for all classes.
Under this scenario,learning category variance along a given dimen-sion would help categorize novel sounds along thatdimension.
That is, two novel datapoints would belikely categorized into separate categories if the in-ferred variance along the relevant dimension wassmaller than the distance between the datapoints,but into a single category if the inferred variancewas comparable to that distance.Finally, this model assumes that sound classesare given in advance, and that only the categorieswithin each class are learned.
While this assump-tion may seem warranted for some types of per-ceptually dissimilar sound classes (e.g., conso-nants and vowels), and also may be appropriatefor L2 acquisition, it is not clear that it is true forall sound classes that allow for generalization ininfancy.
It remains for future work to determinehow learners may generalize while simultaneouslylearning the sound classes.We plan to pursue all these directions in fu-ture work with the ultimate goal of improving ourunderstanding how human learners represent theirlinguistic knowledge and how they use it when ac-quiring a new language.AcknowledgmentsWe thank Gabriel Doyle and three anonymousCMCL reviewers for useful feedback.
This re-search was supported by NIH Training Grant T32-DC000041 from the Center for Research in Lan-guage at UC San Diego to B.P.
and NIH TrainingGrant T32-DC000035 from the Center for Lan-guage Sciences at University of Rochester to B.P.18Referencesde Boer, Bart & Patricia K. Kuhl.
2003.
Inves-tigating the role of infant-directed speech witha computer model.
Acoustic Research LettersOnline 4(4).
129?134.Clements, George N. 2003.
Feature economy insound systems.
Phonology 20.
287?333.Dillon, Brian, Ewan Dunbar & William Idsardi.2013.
A single-stage approach to learningphonological categories: Insights from Inukti-tut.
Cognitive Science 37.
344?377.Feldman, Naomi H., Thomas L. Griffiths &James L. Morgan.
2009.
Learning phonetic cat-egories by learning a lexicon.
In Proceedingsof the 31st Annual Conference of the CognitiveScience Society, 2208?2213.
Austin, TX: Cog-nitive Science Society.Ferguson, Thomas S. 1973.
A Bayesian analy-sis of some nonparametric problems.
Annals ofStatistics 1.
209?230.Flege, James E. 1995.
Second-language speechlearning: theory, findings and problems.
InWinifred Strange (ed.
), Speech perception andlinguistic experience: issues in cross-languageresearch, 229?273.
Timonium, MD: York Press.Gelman, Andrew & Donald B. Rubin.
1992.
In-ference from iterative simulation using multiplesequences.
Statistical Science 7.
457?511.Giovanardi, Maurizio & Maria-GabriellaDi Benedetto.
1998.
Acoustic analysis ofsingleton and geminate fricatives in Italian.The European Journal of Language and Speech(EACL/ESCA/ELSNET) 1998.
1?13.Greenberg, Steven.
1996.
The Switchboard tran-scription project.
Report prepared for the1996 CLSP/JHU Workshop on Innovative Tech-niques in Continuous Large Vocabulary SpeechRecognition.Kuhl, Patricia K. 1991.
Human adults and humaninfants show a ?perceptual magnet effect?
forthe prototypes of speech categories, monkeysdo not.
Perception and Psychophysics 50(2).93?107.Kuhl, Patricia K., Karen A. Williams, FranciscoLacerda, Kenneth N. Stevens & Bjo?rn Lind-blom.
1992.
Linguistic experience alters pho-netic perception in infants by 6 months of age.Science 255.
606?608.Lacerda, Francisco.
1995.
The perceptual magnet-effect: An emergent consequence of exemplar-based phonetic memory.
In K. Ellenius &P. Branderud (eds.
), Proceedings of the 13thInternational Congress of Phonetic Sciences,140?147.
Stockholm: KTH and Stockholm Uni-versity.Ladefoged, Peter & Ian Maddieson.
1996.
Thesounds of the world?s languages.
Oxford, UK;Cambridge, MA: Blackwell.Lisker, Leigh & Arthur S. Abramson.
1970.
Thevoicing dimensions: Some experiments in com-parative phonetics.
In Proceedings of the SixthInternational Congress of Phonetic Sciences,Prague: Academia.Mattei, Marco & Maria-Gabriella Di Benedetto.2000.
Acoustic analysis of singleton and gemi-nate nasals in Italian.
The European Journal ofLanguage and Speech (EACL/ESCA/ELSNET)2000.
1?11.Maye, Jessica & LouAnn Gerken.
2000.
Learningphonemes without minimal pairs.
In S. Cather-ine Howell, Sarah A.
Fish & Thea Keith-Lucas(eds.
), Proceedings of the 24th Annual BostonUniversity Conference on Language Develop-ment, 522?533.
Somerville, MA: CascadillaPress.Maye, Jessica & LouAnn Gerken.
2001.
Learn-ing phonemes: how far can the input take us?In A. H-J.
Do, L.
Dom?
?nguez & A.
Johansen(eds.
), Proceedings of the 25th Annual BostonUniversity Conference on Language Develop-ment, 480?490.
Somerville, MA: CascadillaPress.Maye, Jessica, Daniel J. Weiss & Richard N.Aslin.
2008.
Statistical phonetic learning ininfants: facilitation and feature generalization.Developmental Science 11(1).
122?134.Maye, Jessica, Janet F. Werker & LouAnn Gerken.2002.
Infant sensitivity to distributional infor-mation can affect phonetic discrimination.
Cog-nition 82.
B101?B111.McClaskey, Cynthia L., David B. Pisoni &Thomas D. Carrell.
1983.
Transfer of trainingof a new linguistic contrast in voicing.
Percep-tion and Psychophysics 34(4).
323?330.McMurray, Bob, Richard N. Aslin & Joseph C.Toscano.
2009.
Statistical learning of phoneticcategories: insights from a computational ap-proach.
Developmental Science 12(3).
369?378.Pajak, Bozena & Roger Levy.
2011a.
How ab-stract are phonological representations?
Evi-dence from distributional perceptual learning.19In Proceedings of the 47th Annual Meeting ofthe Chicago Linguistic Society, Chicago, IL:University of Chicago.Pajak, Bozena & Roger Levy.
2011b.
Phono-logical generalization from distributional evi-dence.
In L. Carlson, C. Ho?lscher & T. Ship-ley (eds.
), Proceedings of the 33rd Annual Con-ference of the Cognitive Science Society, 2673?2678.
Austin, TX: Cognitive Science Society.Perfors, Amy & David Dunbar.
2010.
Phonetictraining makes word learning easier.
In S. Ohls-son & R. Catrambone (eds.
), Proceedings of the32nd Annual Conference of the Cognitive Sci-ence Society, 1613?1618.
Austin, TX: Cogni-tive Science Society.Teh, Yee Whye, Michael I. Jordan, Matthew J.Beal & David M. Blei.
2006.
HierarchicalDirichlet processes.
Journal of the AmericanStatistical Association 101(476).
1566?1581.Toscano, Joseph C. & Bob McMurray.
2010.
Cueintegration with categories: Weighting acousticcues in speech using unsupervised learning anddistributional statistics.
Cognitive Science 34.434?464.Vallabha, Gautam K., James L. McClelland, Fer-ran Pons, Janet F. Werker & Shigeaki Amano.2007.
Unsupervised learning of vowel cate-gories from infant-directed speech.
Proceedingsof the National Academy of Sciences 104(33).13273?13278.20
