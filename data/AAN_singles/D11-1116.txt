Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1257?1268,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA Fast, Accurate, Non-Projective, Semantically-Enriched ParserStephen Tratz and Eduard HovyInformation Sciences InstituteUniversity of Southern CaliforniaMarina del Rey, California 90292{stratz,hovy}@isi.eduAbstractDependency parsers are critical componentswithin many NLP systems.
However, cur-rently available dependency parsers each ex-hibit at least one of several weaknesses, in-cluding high running time, limited accuracy,vague dependency labels, and lack of non-projectivity support.
Furthermore, no com-monly used parser provides additional shal-low semantic interpretation, such as prepo-sition sense disambiguation and noun com-pound interpretation.
In this paper, we presenta new dependency-tree conversion of the PennTreebank along with its associated fine-graindependency labels and a fast, accurate parsertrained on it.
We explain how a non-projectiveextension to shift-reduce parsing can be in-corporated into non-directional easy-first pars-ing.
The parser performs well when evalu-ated on the standard test section of the PennTreebank, outperforming several popular opensource dependency parsers; it is, to the bestof our knowledge, the first dependency parsercapable of parsing more than 75 sentences persecond at over 93% accuracy.1 IntroductionParsers are critical components within many natu-ral language processing (NLP) systems, includingsystems for information extraction, question answer-ing, machine translation, recognition of textual en-tailment, summarization, and many others.
Unfortu-nately, currently available dependency parsers suf-fer from at least one of several weaknesses includ-ing high running time, limited accuracy, vague de-pendency labels, and lack of non-projectivity sup-port.
Furthermore, few parsers include any sort ofadditional semantic interpretation, such as interpre-tations for prepositions, possessives, or noun com-pounds.In this paper, we describe 1) a new dependencyconversion (Section 3) of the Penn Treebank (Mar-cus, et al, 1993) along with the associated de-pendency label scheme, which is based upon theStanford parser?s popular scheme (de Marneffe andManning, 2008), and a fast, accurate dependencyparser with non-projectivity support (Section 4) andadditional integrated semantic annotation modulesfor automatic preposition sense disambiguation andnoun compound interpretation (Section 5).
We showhow Nivre?s (2009) swap-based reordering tech-nique for non-projective shift-reduce-style parsingcan be integrated into the non-directional easy-firstframework of Goldberg and Elhadad (2010) to sup-port non-projectivity, and we report the results of ourparsing experiments on the standard test section ofthe PTB, providing comparisons with several freelyavailable parsers, including Goldberg and Elhadad?s(2010) implementation, MALTPARSER (Nivre et al,2006), MSTPARSER (McDonald et al, 2005; Mc-Donald and Pereira, 2006), the Charniak (2000)parser, and the Berkeley parser (Petrov et al, 2006;Petrov and Klein, 2007).The experimental results show that the parser issubstantially more accurate than Goldberg and El-hadad?s original implementation, with fairly simi-lar overall speed.
Furthermore, the results provethat Stanford-granularity dependency labels can belearned by modern dependency parsing systemswhen using our Treebank conversion, unlike theStanford conversion, for which Cer et al (2010)show that this isn?t the case.The optional semantic annotation modules also1257perform well, with the preposition sense disam-biguation module exceeding the accuracy of the pre-vious best reported result for fine-grained preposi-tion sense disambiguation (85.7% vs Hovy et al?s(2010) 84.8%), the possessives interpretation sys-tem achieving over 85% accuracy, and the nouncompound interpretation system performing simi-larly to an earlier version described by Tratz andHovy (2010) at just over 79% accuracy.2 BackgroundThe NLP community has recently seen a surge ofinterest in dependency parsing, with several CoNLLshared tasks focusing on it (Buchholz and Marsi,2006; Nivre et al, 2007).
One of the main advan-tages of dependency parsing is the relative ease withwhich it can handle non-projectivity1.
Additionally,since each word is linked directly to its head via alink that, ideally, indicates the syntactic dependencytype, there is no difficulty in determining either thesyntactic head of a particular word or the syntacticrelation type, whereas these issues often arise whendealing with constituent parses2.Unfortunately, most currently available depen-dency parsers produce relatively vague labels or, inmany cases, produce no labels at all.
While theStanford fine-grain dependency scheme (de Marn-effe and Manning, 2008) has proven to be popular,recent experiments by Cer et al (2010) using theStanford conversion of the Penn Treebank indicatethat it is difficult for current dependency parsers tolearn.
Indeed, the highest scoring parsers trained us-ing the MSTPARSER (McDonald and Pereira, 2006)and MALTPARSER (Nivre et al, 2006) parsing suitesachieved only 78.8 and 81.1 labeled attachmentF1, respectively.
This contrasted with the muchhigher performance obtained using a constituent-to-dependency conversion approach with accurate, butmuch slower, constituency parsers such as the Char-niak and Johnson (2005) and Berkeley (Petrov etal., 2006; Petrov and Klein, 2007) parsers, whichachieved 89.1 and 87.9 labeled F1 scores, respec-tively.1A tree is non-projective if the sequence of words visited ina left-to-right, depth-first traversal of the sentence?s parse tree isdifferent than the actual word order of the sentence.2These latter two issues are not problems for constituentparses with binarized output and functional tags.Though there are many syntactic parsers than canreconstruct the grammatical structure of a text, thereare few, if any, accurate and widely accepted sys-tems that also produce shallow semantic analysis ofthe text.
For example, a parser may indicate that,in the case of ?ice statue?, ?ice?
modifies ?statue?
butwill not indicate that ?ice?
is the substance of thestatue.
Similarly, a parser will indicate which wordsa preposition connects but will not give any seman-tic interpretation (e.g., ?the boy with the pirate hat??
wearing or carrying, ?wash with cold water?
?means, ?shave with the grain?
?
in the same direc-tion as).
While, in some cases, it may be possible touse the output from a separate system for this pur-pose, doing so is often difficult in practice due to awide variety of complications, including program-ming language differences, alternative data formats,and, sometimes, other parsers.3 Dependency Conversion3.1 Relations and StructureMost recent English dependency parsers produceone of three sets of dependency types: unlabeled,some variant of the coarse labels used by theCoNLL dependency parsing shared-tasks (Buchholzand Marsi, 2006; Nivre et al, 2007) (e.g., ADV,NMOD, PMOD), or Stanford?s dependency labels(de Marneffe and Manning, 2008).
Unlabeled de-pendencies are clearly too impoverished for manytasks.
Similarly, the coarse labels of the CoNLLtasks are not very specific; for example, the same re-lation, NMOD, is used for determiners, adjectives,nouns, participle modifiers, relative clauses, etc.
thatmodify nouns.
In contrast, the Stanford relationsprovide a more reasonable level of granularity.Our dependency relation scheme is similar toStanford?s basic scheme but has several differ-ences.
It introduces several new relations includingccinit ?initial coordinating conjunction?, cleft ?cleftclause?, combo ?combined term?, extr ?extraposedelement?, infmark ?infinitive marker ?to?
?, objcomp?object complement?, postloc ?post-modifying lo-cation?, sccomp ?clausal complement of ?so?
?, vch?verbal chain?
and whadvmod ?wh- adverbial mod-ifier?.
The nsubjpass, csubjpass, and auxpass rela-tions of Stanford?s are left out because adding themup front makes learning more difficult and the fact1258abbrev abbreviation csubjpass clausal subject (passive) pobj prepositional objectacomp adjectival complement det determiner poss possessiveadvcl adverbial clause dobj direct object possessive possessive markeradvmod adverbial modifier extr extraposed element postloc post-modifying locationagent ?by?
agent expl ?there?
expletive preconj pre conjunctamod adjectival modifier infmark infinitive marker (?to?)
predet predeterminerappos appositive infmod infinite modifier prep prepositionattr attributive iobj indirect object prt particleaux auxillary mark subordinate clause marker punct punctuationauxpass auxillary (passive) measure measure modifier purpcl purpose clausecleft cleft clause neg negative quantmod quantifier modifiercc coordination nn noun compound rcmod relative clauseccinit initial CC nsubj nominal subject rel relativeccomp clausal complement nsubjpass nominal subject (passive) sccomp clausal complement of ?so?combo combination term num numeric modifier tmod temporal modifiercompl complementizer number compound number vch verbal chainconj conjunction objcomp object complement whadvmod wh- adverbialcop copula complement parataxis parataxis xcomp clausal complement w/o subjcsubj clausal subject partmod participle modifierTable 1: Dependency scheme with differences versus basic Stanford dependencies highlighted.
Bold indicates therelation does not exist in the Stanford scheme.
Italics indicate the relation appears in Stanford?s scheme but not ours.that a nsubj, csubj, or aux is passive can easily be de-termined from the final tree.
Stanford?s aux depen-dencies are replaced using verbal chain (vch) links;conversion of these to Stanford-style aux dependen-cies is also trivial as a post-processing step.3 The attrdependency is excluded because it is redundant withthe cop relation due to different handling of copula,and the dependency scheme does not have an abbrevlabel because this information is not provided by thePenn Treebank.
The dependency scheme with dif-ferences with Stanford highlighted is presented inTable 1.In addition to using a slightly different set of de-pendency names, a handful of relations, notably cop,conj, and cc, are treated in a different manner.
Thesedifferences are illustrated by Figure 1.
The Stan-ford scheme?s treatment of copula may be one rea-son why dependency parsers have trouble learningand applying it.
Normally, the head of the clauseis a verb, but, under Stanford?s scheme, if the verbhappens to be a copula, the complement of the cop-ula (cop) is treated as the head of the clause instead.3The parsing system includes an optional script that can con-vert vch arcs into aux and auxpass and the subject relations intocsubjpass and nsubjpass.Figure 1: Example comparing Stanford?s (top) handlingof copula and coordinating conjunctions with ours (bot-tom).3.2 Conversion ProcessA three-step process is used to convert the PennTreebank (Marcus, et al, 1993) from constituentparses into dependency trees labeled according tothe dependency scheme presented in the prior sec-tion.
The first step is to apply the noun phrasestructure patch created by Vadas and Curran (2007),which adds structure to the otherwise flat nounphrases (NPs) of the Penn Treebank (e.g., ?
(metalsoup pot cover)?
would become ?
(metal (soup pot)cover)?).
The second step is to apply a versionof Johansson and Nugues?
(2007) constituent-to-dependency converter with some head-finding rulemodifications; these rules, with changes highlighted1259(WH)?NP|NX|NML|NAC FW|NML|NN* JJR $|# CD|FW QP JJ|NAC JJS PRP ADJP RB[SR] VBG|DT|WPRB NP- S|SBAR|UCP|PP SINV|SBARQ|SQ UH VP|NP VB|VBPADJP|JJP NNS QP NN $|# JJ VBN VBG (AD|J)JP ADVP JJR NP|NML JJS DT FW RBR RBS SBAR RBADVP RB|RBR|JJ|JJR RBS FW ADVP TO CD IN NP|NML JJS NNPRN S* VP NN*|NX|NML NP W* PP|IN ADJP|JJ ADVP RB NAC VP INTJQP $|# NNS NN CD JJ RB DT NCD QP IN CC JJR JJSSBARQ SQ S SBARQ SINV FRAGSQ VBZ VBD VBP VB MD *-PRD SQ VP FRAG XUCP [QNVP]P|S*|UCP|NML|PR[NT]|RRC|NX|NAC|FRAG|INTJ|AD[JV]P|LST|WH*|XVP VBD|AUX VBN MD VBZ VB VBG VBP VP POS *-PRD ADJP JJ NN NNS NP|NMLWHADJP CC JJ WRB ADJPWHADVP CC WRB|RBX [QNVP]P|S*|UCP|NML|PR[NT]|RRC|NX|NAC|FRAG|INTJ|AD[JV]P|LST|WH*|X|CONJPLST LS : DT|NN|SYMFigure 2: Modified head-finding rules.
Underline indicates that the search is performed in a left-to-right fashion insteadof the default right-to-left order.
NML and JJP are both products of Vadas and Curran?s (2007) patch.
Bold indicatesan added or moved element; for the original rules, see the paper by Johansson and Nugues (2007).in bold, are provided in Figure 2.
Finally, an addi-tional script makes additional changes and convertsthe intermediate output into the dependency scheme.This dependency conversion has several advan-tages to it.
Using the modified head-finding rules forJohansson and Nugues?
(2007) converter results infewer buggy trees than were present in the CoNLLshared tasks, including fewer trees in which wordsare headed by punctuation marks.
For sections 2?21, there are far fewer generic dep/DEP relations(2,765) than with the Stanford conversion (34,134)or the CoNLL 2008 shared task conversion (23,811).Also, the additional conversion script contains vari-ous rules for correcting part-of-speech (POS) errorsusing the syntactic structure as well as additionalrules for some specific word forms, mostly commonwords with inconsistent taggings.
Many of thesechanges cover part-of-speech problems discussed byManning (2011), including VBD/VBN, VBZ/NNS,NNP/NNPS, and IN/WDT/DT issues.
In total, thescript changes over 9,500 part-of-speech tags, withthe most common change being to change preposi-tion tags (IN) into adverb tags (RB) for cases wherethere is no prepositional complement/object.
Thetop fifteen of these changes are presented in Table2.
The conversion script contains a variety of ad-ditional rules for modifying the parse structure andfixing erroneous trees as well, including cases whereone or more POS tags were incorrect and, as such,the initial dependency parse was flawed.
Quickmanual inspections of the changes suggested that thevast majority are accurate.In the final output from the conversion, the num-ber of sentences with one or more words dependenton non-projective arcs in sections 2?21 is 3,245?about 8.1% of the dataset.
About 1.3% of this, or556 of sentences, is due to the secondary conver-sion script, with sentences containing approximatecurrency amounts (e.g., about $ 10) comprising thebulk of difference.
For these, the quantifying text(e.g., about, over, nearly), is linked to the numberfollowing the currency symbol instead of to the cur-rency symbol as it was in the CoNLL 2008 task.Original New # of changesIN RB 1128JJ NN 787VBD VBN 601RB IN 462VBN VBD 441NN JJ 409NNPS NNP 405IN WDT 388VBG NN 223DT IN 220RB JJ 214VB VBP 184NN NNS 169RB NN 157NNS VBZ 148Table 2: Top 15 part-of-speech tag changes performed bythe conversion script.12604 Parser4.1 AlgorithmThe parsing approach is based upon the non-directional easy-first algorithm recently presentedby Goldberg and Elhadad (2010).
Their original al-gorithm behaves as follows.
For a sentence of lengthn, the algorithm performs a total of n steps.
In eachstep, one of the unattached tokens is added as a childto one of its current neighbors and is then removedfrom the list of unprocessed tokens.
When only onetoken remains unprocessed, it is designated as theroot.
Provided that only a constant number of po-tential attachments need to be re-evaluated after eachstep, which is the case if one restricts the context forfeature generation to a constant number of neigh-boring tokens, the algorithm can be implemented torun in O(n log n).
However, since only O(n) dotproducts must be calculated by the parser and thesehave a large constant associated with them, the run-ning time will rival O(n) parsers for any reasonablen, and, thus, a naive O(n2) implementation will benearly as fast as a priority queue implementation inpractice.4The algorithm has a couple potential advantagesover standard shift-reduce style parsing algorithms.The first advantage is that performing easy ac-tions first may make the originally difficult deci-sions easier.
The second advantage is that perform-ing parse actions in a more flexible order than left-to-right/right-to-left shift-reduce parsing reduces thechance of error propagation.Unfortunately, the original algorithm does notsupport non-projective trees.
To extend the algo-rithm to support non-projective trees, we introducemove-right and move-left operations similar to thestack-to-buffer swaps proposed by Nivre (2009) forshift-reduce style parsing.
Thus, instead of attachinga token to one of its neighbors at each step, the algo-rithm may instead decide to move a token past oneof its neighbors.
Provided that no node is allowedto be moved past a token in such a way that a previ-ous move operation is undone, there can be at mostO(n2) moves and the overall worst-case complexitybecomes O(n2 log n).
While theoretically slower,this has a limited impact upon actual parsing times4See Goldberg and Elhadad (2010) for more explanation.in practice, especially for languages with relativelyfixed word order such as English.5 Though Gold-berg and Elhadad?s (2010) original implementationonly supports unlabeled dependencies, the algorithmitself is in no way limited in this regard, and it issimple enough to add labeled dependency supportby treating each dependency label as a specific typeof attach operation (e.g., attach_as_nsubj), whichis the method used by this implementation.
Pseu-docode for the non-directional easy-first algorithmwith non-projective support is given in Algorithm 1.input : w1 ... wn, #the sentencem, #the modelk, #the context widthactions, #the list of parse actions?, #the feature generatoroutput: tree #a collection of dependency arcswords = copyOf(s);stale = copyOf (s);cache; #cache of action scoreswhile |words| > 1 dofor w ?
stale dofor act ?
actions docache[w,act] = score(act, ?(w,...
),m);stale.remove(w);best = argmaxa?actions&valid(a),w?wordscache[w, a]if isMove(best) theni =words.index(getTokenToMove(best));words.move (i, isMoveLeft(best) ?
-1: 1);elsearc = createArc(best);tree.add(arc);i = words.index(getChild(arc));words.remove(i);for x ?
-k,...,k dostale.add(words.get(index+x));return treeAlgorithm 1: Modified version of Goldberg andElhadad?s (2010) Easy-First Algorithm with non-projective support.5See Nivre (2009) for more information on the effect of re-ordering operations on parse time.12614.2 FeaturesOne of the key aspects of the parser is the complexset of features used.
The feature set is based offthe features used by Goldberg and Elhadad (2010)but has a significant number of extensions.
Variousfeature templates are specifically designed to pro-duce features that help with several syntactic issuesincluding preposition attachment, coordination, ad-verbial clauses, clausal complements, and relativeclauses.
Unfortunately, there is insufficient space inthis paper to describe them all here.
However, a listof feature templates will be provided with the parserdownload.Several of the feature templates use unsupervisedword clusters created with the Brown et al (1992)hierarchical clustering algorithm.
The use of this al-gorithm was inspired by Koo et al (2008), who usedthe top branches of the cluster hierarchy as features.However, unlike Koo et al?s (2008) parser, the fine-grained cluster identifiers are used instead of justthe top 4-6 branches of the cluster hierarchy.
The175 word clusters utilized by the parser were createdfrom the New York Times corpus (Sandhaus, 2008).Some examples from the clusters are presented inFigure 3.
The ideal number of such clusters was notthoroughly investigated.while where when although despite unless unlike ...why what whom whatever whoever whomever whence ...based died involved runs ended lived charged born ...them him me us himself themselves herself myself ...really just almost nearly simply quite fully virtually ...know think thought feel believe knew felt hope mean ...into through on onto atop astride Saturday/Early thru ...Ms. Mr. Dr. Mrs. Judge Miss Professor Officer Colonel ...John President David J. St. Robert Michael James George ...wife own husband brother sister grandfather beloved ...often now once recently sometimes clearly apparently ...everyone it everybody somebody anybody nobody hers ...around over under among near behind outside across ...Clinton Bush Johnson Smith Brown Williams King ...children companies women people men things students ...Figure 3: High frequency examples from 15 of the Brownclusters.4.3 TrainingThe parsing model is trained using a variant of thestructured perceptron training algorithm used in theoriginal Goldberg and Elhadad (2010) implementa-tion.
The general idea of the algorithm is to iterateover the sentences and, whenever the model predictsan incorrect action, update the model weights.
Fol-lowing Goldberg and Elhadad, parameter averagingis used to reduce overfitting.Our implementation varies slightly from that ofGoldberg and Elhadad (2010).
The difference isthat, at any particular step for a given sentence, thealgorithm continues to update the weight vector aslong as any invalid action is scored higher than anyvalid action, not just the highest scoring valid ac-tion; unfortunately, this change significantly sloweddown the training process.
In early experiments, thischange produced a slight improvement in accuracythough it also slowed training significantly.
In laterexperiments using additional feature templates, thischange ceased to have any notable impact on theoverall accuracy, but it was kept anyway.
6The oracle used to determine whether a move op-eration should be considered legal during the train-ing phase is similar to Nivre et al?s (2009) improvedoracle based upon maximal projective subcompo-nents.
As an additional restriction, during training,move actions were only considered valid either if noother action was valid or if the token to be movedalready had all its children attached and moving itcaused it to be adjacent to its parent.
This fits withNivre et al?s (2009) intuition that it is best to delayword reordering as long as possible.4.4 Speed EnhancementsTo enhance the speed for practical use, the parseruses constraints based upon the part-of-speech tagsof the adjacent word pairs to eliminate invalid de-pendencies from even being evaluated.
A rela-tion is only considered between a pair of words ifsuch a relation was observed in the training databetween a pair of words with the same parts-of-speech (with the exception of the generic dep de-pendency, which is permitted between any POS tagpair).
Early experiments utilizing similar constraintsshowed an improvement in parsing speed of about16% with no significant impact on accuracy, regard-less of whether the constraints were enforced duringtraining.6See Goldberg and Elhadad (2010) for more description ofthe general training procedure.1262System Arc Accuracy Perfect Sentences Non-Proj ArcsLabeled Unlabeled Labeled Unlabeled Labeled UnlabeledTHIS WORK 92.1 (93.3) 93.7 (94.3) 38.4 (42.5) 46.2 (48.5) 66.5 (69.7) 69.3 (71.7)THIS WORKno clusters 91.8 (93.1) 93.4 (94.1) 38.2 (42.3) 45.5 (47.3) 67.3 (70.9) 69.3 (72.5)THIS WORKmoves disabled 91.7 (92.9) 93.3 (93.9) 37.1 (40.8) 44.2 (46.2) 21.1 (21.1) 22.7 (21.9)NON-DIR EASY FIRST * 91.2 (92.0) * 37.8 (39.4) * 15.1 (16.3)EISNER?MST 90.9 (92.2) 92.8 (93.5) 32.1 (35.6) 40.6 (42.3) 62.5 (65.3) 63.7 (66.9)CHU-LIU-EDMONDSMST 90.0 (91.2) 91.8 (92.5) 28.4 (31.3) 35.0 (36.4) 62.9 (65.3) 64.1 (66.5)ARC-EAGERMalt 89.8 (91.1) 91.3 (92.1) 31.6 (34.2) 37.4 (38.5) 19.5 (19.5) 20.3 (19.9)ARC-STANDARDMalt 88.3 (89.5) 89.7 (90.4) 31.4 (34.1) 36.1 (37.3) 13.1 (12.0) 13.9 (12.7)STACK-EAGERMalt 90.0 (91.2) 91.5 (92.3) 34.5 (37.5) 40.4 (41.9) 51.8 (53.8) 53.8 (55.4)STACK-LAZYMalt 90.4 (91.7) 91.9 (92.8) 34.8 (37.7) 40.6 (42.5) 61.8 (63.3) 63.3 (65.3)CHARNIAK?
* 93.2 * 43.5 * 32.3BERKELEY?
* 93.3 * 43.6 * 34.3Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded).
Results in parentheses wereproduced using gold POS tags.
?Eisner (1996) algorithm with non-projective rewriting and second order features.
?Results not directly comparable; see text.
?Labeled dependencies not available/comparable.4.5 EvaluationThe following split of the Penn Treebank (Marcus,et al, 1993) was used for the experiments: sections2?21 for training, 22 for development, and 23 fortesting.For part-of-speech (POS) tagging, we used an in-house SVM-based POS tagger modeled after thework of Gim?nez and M?rquez (2004) 7.
The train-ing data was tagged in a 10-fold fashion; each foldwas tagged using a tagger trained from the nine re-maining folds.
The development and test sectionswere tagged by an instance of the tagger trained us-ing the entire training set.
The full details of thePOS tagger are outside the scope of this paper; it isincluded with the parser download.The final parser was trained for 31 iterations,which is the point at which its performance on thedevelopment set peaked.
One test run was per-formed with non-projectivity support disabled in or-der to get some idea of the impact of the move opera-tions on the parser?s overall performance; also, sincethe parsers used for comparison had no access to theunsupervised word clusters, an additional instanceof the parser was trained with every word treatedas belonging to the same cluster so as to facilitatea more fair comparison.Seven different dependency parsing models were797.42% accuracy on traditional POS evaluation (Penn Tree-bank WSJ sections 22-24).trained for comparison using the following opensource parsing packages: Goldberg and Elhadad?s(2010)?s non-directional easy-first parser, MALT-PARSER (Nivre et al, 2006), and MSTPARSER(McDonald and Pereira, 2006)8.
The model trainedusing Goldberg and Elhadad?s (2010) easy-firstparser serves as something of a baseline.
Thefour MALTPARSER parsing models used the arc-eager, arc-standard, stack-eager, and stack-lazy al-gorithms.
One of the MSTPARSER models usedthe Chu-Liu-Edmonds maximum spanning tree ap-proach, and the other used the Eisner (1996) al-gorithm with second order features and a non-projective rewriting post-processing step.Unfortunately, it is not possible to directly com-pare the parser?s accuracy with most popular con-stituent parsers such as the Charniak (2000) andBerkeley (Petrov et al, 2006; Petrov and Klein,2007) parsers9 both because they do not pro-duce functional tags for subjects, direct objects,etc., which are required for the final script of theconstituent-to-dependency conversion routine, andbecause they determine part-of-speech tags in con-junction with the parsing.
However, it is possible tocompute approximate unlabeled accuracy scores bytraining the constituent parsers on the NP-patched(Vadas and Curran, 2007) version of the data andthen running the test output through just the firstconversion script?that is, the modified version ofJohansson and Nugues?
(2007) converter.1263The results of the experiment are given in Ta-ble 3, including accuracy for individual arcs, non-projective arcs only, and full sentence match.
Punc-tuation is excluded in all the result computations.
Todetermine whether an arc is non-projective, the fol-lowing heuristic was used.
Traverse the sentence ina depth-first search, starting from the imaginary rootnode and pursuing child arcs in order of increasingabsolute distance from their parent.
Whenever anarc being traversed is found to cross a previously tra-versed arc, mark it as non-projective and continue.To evaluate the impact of part-of-speech tagging er-ror, results for parsing using the gold standard part-of-speech tags are also included.We also measured the speed of the parser on thevarious sentences in the test collection.
For reason-able sentence lengths, the parser scales quite well.The scatterplot depicting the relation between sen-tence length and parsing time is presented in Figure5.Figure 4: Parse times for Penn Treebank section 23 forthe parsers on a PC with a 2.4Ghz Q6600 processor and8GB RAM.
MALTPARSER ran substantially slower thanthe others, perhaps due to its use of polynomial kernels,and isn?t shown.
(C-L-E - Chu-Liu-Edmonds, G&E -Goldberg and Elhadad (2010)).4.5.1 Results DiscussionThe parser achieves 92.1% labeled and 93.7% un-labeled accuracy on the evaluation, a solid result andabout 2.5% higher than the original easy-first imple-mentation of Goldberg and Elhadad (2010).
Further-more, the parser processed the entire test section in8Versions 1.4.1, 0.4.3b, and 0.2, respectively9Versions 1.1 and 05Aug16, respectivelyjust over 30 seconds?a rate of over 75 sentences persecond, substantially faster than most of the otherparsers.Not surprisingly, the results for non-projectivearcs are substantially lower than the results for allarcs, and the systems that are designed to handlethem outperformed the strictly projective parsers inthis regard.The negative effect of part-of-speech tagging er-ror appears to impact the different parsers about thesame amount, with a loss of .6% to .8% in unlabeledaccuracy and 1.1% to 1.3% in labeled accuracy.The 93.2% and 93.3% accuracy scores achievedby the Charniak and Berkeley parsers are not toodifferent from the 93.7% result, but, of course, it isimportant to remember that these scores are not di-rectly comparable.Figure 5: Sentence length versus parse time.
Mediantimes for five runs over section 23.5 Shallow Semantic AnnotationTo create a more informative parse, the parser in-cludes four optional modules, a preposition sensedisambiguation (PSD) system, a work-in-progress?s-possessive interpretation system, a noun com-pound interpretation system, and a PropBank-basedsemantic role labeling system10.
Taken together,these integrated modules enable the parsing sys-tem to produce substantially more informative out-put than a traditional parser.Preposition Sense Disambiguation The PSDsystem is a newer version of the system described10Lack of space prohibits a sufficiently thorough discussionof these individual components and their evaluations, but addi-tional information will be available with the system download.1264by Tratz and Hovy (2009) and Hovy et al (2010); itachieves 85.7% accuracy on the SemEval-2007 fine-grain PSD task (Litkowski and Hargraves, 2007),which is a statistically significant (p<=0.05; upper-tailed z test) increase over the previous best reportedresult for this dataset, Hovy et al?s (2010) 84.8%.Noun Compound Interpretation The noun com-pound interpretation system is a newer version ofthe system described by Tratz and Hovy (2010) withsimilar accuracy (79.6% vs 79.3% using 10-foldcross-validation11).Possessives Interpretation The possessive inter-pretation system assigns interpretations to ?s pos-sessives (e.g., John?s arm ?
PART-OF, Mowgli?scapture ?
PATIENT/THEME).
The current systemachieves over 85.0% accuracy, but it is important tonote that the annotation scheme, automatic classifier,and dataset are all still under active development.PropBank SRL The PropBank-based semanticrole labeling system achieves 86.8 combined F1measure for automatically-generated parse trees cal-culated over both predicate disambiguation and ar-gument/adjunct classification (89.5 F1 on predicatedisambiguation, 85.6 F1 on argument and adjunctscorresponding to dependency links, and 86.8 F1);this score is not directly comparable to any previ-ous work due to some differences, including differ-ences in both the parse tree conversion and the Prop-Bank conversion.
The most similar work is that ofthe CoNLL shared task work (Surdeanu et al, 2008;Hajic?
et al, 2009).6 Related WorkNon-projectivity.
There are two main approachesused in recent NLP literature for handling non-projectivity in parse trees.
The first is to use an al-gorithm, like the one presented in this paper, thathas inherent support for non-projective trees.
Ex-amples of this include the Chu-Liu-Edmonds?
ap-proach for maximum spanning tree (MST) parsing(McDonald et al, 2005) and Nivre?s (2009) swap-based reordering method for shift-reduce parsing.The second approach is to create an initial projec-tive parse and then apply transformations to intro-11These accuracy figures are higher than what should be ex-pected for unseen datasets; see Tratz and Hovy (2010) for moredetail.duce non-projectivity into it.
Examples of this in-clude McDonald and Pereira?s (2006) rewriting ofprojective trees produced by the Eisner (1996) al-gorithm, and Nivre and Nilsson?s (2005) pseudo-projective approach that creates projective trees withspecially marked arcs that are later transformed intonon-projective dependencies.Descriptive dependency labels.
While most re-cent dependency parsing research has used eithervague labels, such as those of the CoNLL sharedtasks, or no labels at all, some descriptive depen-dency label schemes exist.
By far the most promi-nent of these is the Stanford typed dependencyscheme (de Marneffe and Manning, 2008).
An-other descriptive scheme that exists, but which isless widely used in the NLP community, is the oneused by Tapanainen and J?rvinen?s parser (1997).Unfortunately, the Stanford dependency conversionof the Penn Treebank has proven difficult to learn forcurrent dependency parsers (Cer et al, 2010), andthere is no publicly available dependency conversionaccording to Tapanainen and J?rvinen?s scheme.Faster parsing.
While the fastest reasonableparsing algorithms are the O(n) shift-reduce algo-rithms, such as Nivre?s (2003) algorithm and an ex-pected linear time dynamic programming approachpresented by Huang and Sagae (2010), a few otherfast alternatives exist.
Goldberg and Elhadad?s(2010) easy-first algorithm is one such example.
An-other example, is Roark and Hollingshead?s (2009)work that uses chart constraints to achieve lineartime complexity for constituency parsing.Effective features for parsing.
A variety of workhas investigated the use of more informative fea-tures for parsing.
This includes work that inte-grates second and even third order features (McDon-ald et al, 2006; Carreras, 2007; Koo and Collins,2010).
Also, some work has incorporated unsuper-vised word clusters as features, including that of Kooet al (2008) and Suzuki et al (2009), who utilizedunsupervised word clusters created using the Brownet al (1992) hierarchical clustering algorithm.Semantically-enriched output.
The 2008 and2009 CoNLL shared tasks (Surdeanu et al, 2008;Hajic?
et al, 2009), which required participants tobuild systems capable of both syntactic parsing andSemantic Role Labeling (SRL) (Gildea and Juraf-sky, 2002), are the most notable attempts to encour-1265age the development of parsers with additional se-mantic annotation.
These tasks relied upon Prop-Bank (2005) and NomBank (2004) for the seman-tic roles.
A variety of other systems have focusedon FrameNet-based (1998) SRL instead, includingthose that participated in the SemEval-2007 Task 19(Baker et al, 2007) and work by Das et al (2010).7 ConclusionIn this paper, we have described a new high-qualitydependency tree conversion of the Penn Treebank(Marcus, et al, 1993) along with its labeled depen-dency scheme and presented a parser that is fast, ac-curate, supports non-projective trees and providesrich output, including not only informative depen-dency labels similar to Stanford?s but also additionalsemantic annotation for prepositions, possessives,and noun compound relations.
We showed how theeasy-first algorithm of Goldberg and Elhadad (Gold-berg and Elhadad, 2010) can be extended to supportnon-projective trees by adding move actions similarto Nivre?s (2009) swap-based reordering for shift-reduce parsing and evaluated our parser on the stan-dard test section of the Penn Treebank, comparingwith several other freely available parsers.The Penn Treebank conversion process fixes anumber of buggy trees and part-of-speech tags andproduces dependency trees with a relatively smallpercentage of generic dep dependencies.
The ex-perimental results show that dependency parsers cangenerally produce Stanford-granularity labels withhigh accuracy when using the new dependency con-version of the Penn Treebank, something which, ac-cording to the findings of Cer et al (2010), doesnot appear to be the case when training and testingdependency parsers on the Stanford conversion.The parser achieves high labeled and unlabeledaccuracy in the evaluation, 92.1% and 93.7%, re-spectively.
The 93.7% result represents a 2.5% in-crease over the accuracy of Goldberg and Elhadad?s(2010) implementation.
Also, the parser proves tobe quite fast, processing section 23 of the Penn Tree-bank in just over 30 seconds (a rate of over 75 sen-tences per second).The parsing system is capable of not only produc-ing fine-grained dependency relations, but can alsoproduce shallow semantic annotations for preposi-tions, possessives, and noun compounds by usingseveral optional integrated modules.
The preposi-tion sense disambiguation (PSD) module achieves85.7% accuracy on the SemEval-2007 PSD task, ex-ceeding the previous best published result of 84.8%by a statistically significant margin, the possessivesmodule is over 85% accurate, the noun compoundinterpretation module achieves 79.6% accuracy onTratz and Hovy?s (2010) dataset.
The PropBankSRL module achieves 89.5 F1 on predicate disam-biguation and 85.6 F1 on argument and adjuncts cor-responding to dependency links, for an overall F1 of86.8.
Combined with the core parser, these modulesallow the system to produce a substantially more in-formative textual analysis than a standard parser.8 Future WorkThere are a variety of ways to extend and improveupon this work.
We would like to change our han-dling of coordinating conjunctions to treat the co-ordinating conjunction as the head because this hasfewer ambiguities than the current approach and alsoadd the ability to produce traces for WH- words.
Itwould also be interesting to examine the impact onfinal parsing accuracy of the various differences be-tween our dependency conversion and Stanford?s.To aid future NLP research work, the code,including the treebank converter, part-of-speechtagger, parser, and semantic annotation add-ons,will be made publicly available for download viahttp://www.isi.edu.AcknowledgementsWe would like to thank Richard Johansson forproviding us with the code for the pennconverterconsituent-to-dependency converter.
We would alsolike to thank Dirk Hovy and Anselmo Pe?as formany valuable dicussions and suggestions.ReferencesCollin Baker, and Michael Ellsworth and Katrin Erk.2007.
SemEval?07 task 19: Frame Semantic StructureExtraction.
In Proc.
of the 4th International Workshopon Semantic EvaluationsCollin Baker, Charles J. Fillmore and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proc.
of the126617th international conference on Computational lin-guisticsAdam L. Berger, Vincent J. Della Pietra, and Stephen A.Della Pietra.
1996.
A maximum entropy approach tonatural language processing.
In Computational Lin-guistics 22(1):39?71Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-Based n-gram Models of Natural Language.
Compu-tational Linguistics 18(4):467?479.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProc.
of CoNLL 2006.Xavier Carreras.
2007.
Experiments with a Higher-Order Projective Dependency Parser.
In Proc.
of theCoNLL Shared Task Session of EMNLP-CoNLL 2007.Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-sky, and Christopher D. Manning.
2010.
Parsing toStanford Dependencies: Trade-offs between speed andaccuracy.
In Proc.
of LREC 2010.Eugene Charniak.
2000.
A Maximum-Entropy-InspiredParser.
In Proc.
of NAACL 2000.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-find-grained n-best parsing and discriminative rerank-ing.
In Proc.
of ACL 2005.Michael A. Covington.
2001.
A Fundamental Algorithmfor Dependency Parsing.
In Proc.
of the 39th AnnualACM Southeast Conference.Dipanjan Das, Nathan Schneider, Desai Chen, and NoahA.
Smith.
2010.
Probabilistic Frame-Semantic Pars-ing.
In Proc.
of HLT-NAACL 2010.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies repre-sentation.
In COLING Workshop on Cross-frameworkand Cross-domain Parser Evaluation.Jason Eisner.
1996.
Three New Probabilistic Modelsfor Dependency Parsing: An Exploration.
In Proc.
ofCOLING 1996.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics.28(3):245?288.Jes?s Gim?nez and Llu?s M?rquez 2004.
SVMTool: AGeneral POS Tagger Generator Based on Support Vec-tor Machines.
In Proc.
of LREC 2004.Yoav Goldberg and Michael Elhadad.
2010.
An Ef-ficient Algorithm for Easy-First Non-Directional De-pendency Parsing.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the ACL.Yoav Goldberg and Michael Elhadad.
2009.
TheCoNLL-2009 Shared Task: Syntactic and SemanticDependencies in Multiple Languages.
In Proc.
ofthe Thirteenth Conference on Computational NaturalLanguage Learning: Shared Task.Dirk Hovy, Stephen Tratz, and Eduard Hovy.
2010.What?s in a Preposition?
?Dimensions of Sense Dis-ambiguation for an Interesting Word Class.
In Proc.
ofCOLING 2010.Liang Huang and Kenji Sagae.
2010.
Dynamic Program-ming for Linear-Time Shift-Reduce Parsing.
In Proc.of ACL 2010.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for english.
InProc.
of NODALIDA.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple Semi-supervised Dependency Parsing.
InProc.
of ACL 2008.Terry Koo and Michael Collins.
2010.
Efficient Third-order Dependency Parsers.
In Proc.
of ACL 2010.Ken Litkowski and Orin Hargraves.
2007.
SemEval-2007 Task 06: Word-Sense Disambiguation of Prepo-sitions.
In Proc.
of the 4th International Workshop onSemantic Evaluations.Christopher D. Manning.
2011.
Part-of-Speech Taggingfrom 97% to 100%: Is It Time for Some Linguistics?In Proc.
of the 12th International Conference on Intel-ligent Text Processing and Computational Linguistics(CICLing 2011).Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: the Penn TreeBank.
ComputationalLinguistics, 19(2):313?330.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-Projective Dependency ParsingUsing Spanning Tree Algorithms.
In Proc.
of HLT-EMNLP 2005.Ryan McDonald and Fernando Pereira.
2006.
OnlineLearning of Approximate Dependency Parsing Algo-rithms.
In Proc.
of EACL 2006.Adam Meyers, Ruth Reeves, Catherine Macleod, RachelSzekely, Veronika Zielinska, Brian Young and RalphGrishman.
2004.
The NomBank Project: An InterimReport.
In Proc.
of the NAACL/HLT Workshop onFrontiers in Corpus Annotation.Joakim Nivre.
2009.
Non-Projective Dependency Pars-ing in Expected Linear Time.
In Proc.
of the 47th An-nual Meeting of the ACL and the 4th IJCNLP of theAFNLP.Joakim Nivre.
2003.
An Efficient Algorithm for Projec-tive Dependency Parsing.
In Proc.
of the 8th Interna-tional Workshop on Parsing Technologies (IWPT).Joakim Nivre, Marco Kuhlmann, and Johan Hall.
2009.An Improved Oracle for Dependency Parsing with On-line Reordering.
In Proc.
of the 11th InternationalConference on Parsing Technologies (IWPT).Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 shared task on dependencyparsing.
In Proc.
of EMNLP-CoNLL 2007.1267Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
Malt-Parser: A Data-Driven Parser-Generator for Depen-dency Parsing.
In Proc.
of LREC 2006.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projectivedependency parsing.
In Proc.
of ACL-2005.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
In Computational Linguistics.31(1):71?106.Slav Petrov and Dan Klein.
2007.
Improved Inferencefor Unlexicalized Parsing.
In Proc.
of HLT-NAACL2007.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proc.
of COLING-ACL2006.Brian Roark and Kristy Hollingshead.
2009.
Linearcomplexity context-free parsing pipelines via chartconstraints.
In Proc.
of HLT-NAACL.Evan Sandhaus.
2008.
The New York Times AnnotatedCorpus.
Linguistic Data Consortium, Philadelphia.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?s M?rquez, and Joakim Nivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In Proc.
of the Twelfth Confer-ence on Computational Natural Language Learning.Jun Suzuki, Hideki Isozaki, Xavier Carrerras, andMichael Collins.
2009.
An Empirical Study of Semi-supervised Structured Conditional Models for Depen-dency Parsing.
In Proc.
of EMNLP.Pasi Tapanainen and Timo J?rvinen.
1997.
A non-projective dependency parser.
In Proc.
of the fifth con-ference on applied natural language processing.Stephen Tratz and Dirk Hovy.
2009.
Disambiguation ofPreposition Sense using Linguistically Motivated Fea-tures.
In Proc.
of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,Companion Volume: Student Research Workshop andDoctoral Consortium.Stephen Tratz and Eduard Hovy.
2010.
A Taxonomy,Dataset, and Classifier for Automatic Noun Com-pound Interpretation.
In Proc.
of ACL 2010.David Vadas and James R. Curran.
2007.
Adding NounPhrase Structure to the Penn Treebank.
In Proc.
ofACL 2007.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical Dependency Analysis With Support Vector Ma-chines.
In Proc.
of 8th International Workshop onParsing Technologies (IWPT).1268
