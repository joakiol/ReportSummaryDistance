First Joint Conference on Lexical and Computational Semantics (*SEM), pages 524?528,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsPolyUCOMP: Combining Semantic Vectors with Skip bigrams forSemantic Textual SimilarityJian Xu Qin Lu Zhengzhong LiuThe Hong Kong Polytechnic UniversityDepartment of ComputingHung Hom, Kowloon, Hong Kong{csjxu, csluqin, hector.liu}@comp.polyu.edu.hkAbstractThis paper presents the work of the HongKong Polytechnic University (PolyUCOMP)team which has participated in the SemanticTextual Similarity task of SemEval-2012.
ThePolyUCOMP system combines semantic vec-tors with skip bigrams to determine sentencesimilarity.
The semantic vector is used tocompute similarities between sentence pairsusing the lexical database WordNet and theWikipedia corpus.
The use of skip bigram isto introduce the order of words in measuringsentence similarity.1 IntroductionSentence similarity computation plays an im-portant role in text summarization, classification,question answering and social network applica-tions (Lin and Pantel, 2001; Erkan and Radev,2004; Ko et al, 2004; Ou et al, 2011).
TheSemEval 2012 competition includes a task targetedat Semantic Textual Similarity (STS) between sen-tence pairs (Eneko et al, 2012).
Given a set of sen-tence pairs, participants are required to assign toeach sentence pair a similarity score.Because a sentence has only a limited amount ofcontent words, it is not easy to determine sentencesimilarities because of the sparseness issue.Hatzivassiloglou et al (1999) proposed to use lin-guistic features as indicators of text similarity toaddress the problem of sparse representation ofsentences.
Mihalcea et al (2006) measured sen-tence similarity using component words in sen-tences.
Li et al (2006) proposed to incorporate thesemantic vector and word order to calculate sen-tence similarity.In our approach to the STS task, semantic vectoris used and the semantic relatedness betweenwords is derived from two sources: WordNet andWikipedia.
Because WordNet is limited in its cov-erage, Wikipedia is used as a candidate for deter-mining word similarity.Word order, however, is not considered in se-mantic vector.
As semantic information are codedin sentences according to its order of writing, andin our systems, content words may not be adjacentto each other, we proposed to use skip bigrams torepresent the structure of sentences.
Skip bigrams,generally speaking, are pairs of words in a sen-tence order with arbitrary gap (Lin and Och,2004a).
Different from the previous skip bigramstatistics which compare sentence similaritiesthrough overlapping skip bigrams (Lin and Och,2004a), the skip bigrams we used are weighted bya decaying factor of the skipping gap in a sentence,giving higher scores to closer occurrences of skipbigrams.
It is reasonable to assume that similarsentences should have more overlapping skip bi-grams, and the gaps in their shared skip bigramsshould also be similar.The rest of this paper is organized as followed.Section 2 describes sentence similarity using se-mantic vectors and the order-sensitive skip bigrams.Section 3 gives the performance evaluation.
Sec-tion 4 is the conclusion.2 Similarity between SentencesWords are used to represent a sentence in thevector space model.
Semantic vectors are con-structed for sentence representations with each en-try corresponding to a word.
Since the semanticvector does not consider word order, we furtherproposed to use skip bigrams to represent sentencestructure.
Moreover, these skip bigrams are524weighted by a decaying factor based on the socalled skip distance in the sentence.2.1 Sentence similarity using SemanticVectorGiven a sentence pair, S1 and S2, for example,S1: Chairman Michael Powell and FCC colleagues atthe Wednesday hearing.S2: FCC chief Michael Powell presides over hearingMonday.The term set of the vector space is first formedby taking only the content words in both sentences,T={chairman, chief, colleagues, fcc, hearing, michael,monday, powell, presides, wednesday }Each entry of the semantic vector corresponds toa word in the joint word set (Li et al, 2006).
Then,the vector for each sentence is formed in two steps:For a word both in the term set T and in the sen-tence, the value for this word entry is set to 1.
If aword is not in the sentence, the most similar wordin the sentence will then be identified, and the cor-responding path similarity value will be assignedto this entry.
Let T be the term set with a sorted listof content words, T=(t1, t2,?, tn).
Without loss ofgenerality, let a sentence S=(w1 w2?wm) where wjis a content word and wj is a word in T. Let thevector space of the sentence S be VSs = (v1, v2, ?,vn).
Then the value of vi is assigned as follows,where the similarity function SIM(ti, wj) is calcu-lated according to the path measure (Pedersen etal., 2004) using the WordNet, formally defined as,),(1),( jiji wtdistwtSIM ?where dist(ti, wj) is the shortest path from  ti, towj by counting nodes in the WordNet taxonomy.Based on this, the semantic vectors for the two ex-ample sentences will be,SVS1 = (1, 0.25, 1, 1, 1, 1, 0.33, 1, 0, 1) andSVS2 = (0.25, 1, 0, 1, 1, 1, 1, 1, 1, 0.33)Based on the two semantic vectors, the cosinemetric is used to measure sentence similarity.
Inthe WordNet, the entry chairman in the joint set ismost similar to the word chief in sentence S2.
Inpractice, however, this entry might be closer to theword presides than to the word chief.
Therefore,we try to obtain the semantic relatedness using theWikipedia for sentence T and find that the entrychairman is closest to the word presides.
The Wik-ipedia-based word relatedness utilizes the hyper-link structure (Milne & Witten, 2008).
It firstidentifies the candidate articles, a and b, that dis-cuss ti and wj respectively in this case and thencompute relatedness between these articles,|))||,log(min(||)log(|)log(|))||,log(max(|),( BAWBABAbarel ???
?where A and B are sets of articles that link to aand b. W is the set of all articles in the Wikipedia.Finally, two articles that represent ti and wj are se-lected and their relatedness score is assigned toSIM(ti, wj).2.2 Sentence Similarity by Skip bigramsSkip bigrams are pairs of words in a sentenceorder with arbitrary gaps.
They contain the order-sensitive information between two words.
The skipbigrams of a sentence are extracted as featureswhich will be stacked in a vector space.
Each skipbigram is weighted by a decaying factor with itsskip distances in the sentence.
To illustrate this,consider the following sentences S and T:S =  w1 w2 w1 w3 w4   and    T =  w2 w1 w4 w5 w4where w denotes a word.
It can be used morethan once in a sentence.
Each sentence above has aC(5, 2) 1 = 10 skip bigrams.The sentence S has the following skip bigrams:?w1w2?, ?w1w1?, ?w1w3?, ?w1w4?, ?w2w1?,?w2w3?
, ?w2w4?
, ?w1w3?, ?w1w4?, ?w3w4?The sentence T has the following skip bigrams:?
2w1?, ?w2w4?, ?w2w5?, ?w2w4?, ?w1w4?,?w1w5?
, ?w1w4?
, ?w4w5?, ?w4w4?, ?w5w4?In the sentence S, we have two repeated skip bi-grams ?w1w4?
and ?w1w3?.
In the sentence T, wehave ?w2w4?
and ?w1w4?
repeated twice.
In thiscase, the weight of the recurring skip bigrams willbe increased.
Hereafter, vectors for S and T will be1 Combination: C(5,2)=5!/(2!*3!
)=10.525formulated with each entry corresponding to a dis-tinctive skip bigram.VS = (?w1w2?, ?w1w1?, ?w1w3?, ?w1w4?, ?w2w1,?w2w3?, ?w2w4?, ?w3w4?
)?VT = (?w2w1?, ?w2w4?, ?w2w5?, ?w1w4?, ?w1w5?,?w4w5?, ?w4w4?, ?w5w4?
)?Now, the question remains how to weight theskip bigrams.
Given?
as a finite word set, letS=w1w2?w|S| be a sentence, wi?
?and 1?i?|S|.A skip bigram of S, denoted by u, is defined by anindex set I=(i1, i2) of S (1?i1<i2?|S| and u=S[I]).The skip distance of S[I] , denoted by du (I), is theskip distance of the first word and the second wordof u, calculated by i2-i1+1.
For example, if S is thesentence of w1w2w1w3w4 and u = w1w4, then thereare two index sets, I1=[3,5] and I2=[1,5] such thatu=S[3,5] and u=S[1,5], and the skip distances ofS[3,5] and S[1,5] are 3 and 5.
The weight of a skipbigram u for a sentence S with all its possible oc-currences, denoted by ( )u S?
, is defined as:( ): [ ]( ) ud Iu I u S IS?
???
?where ?
is the decay factor which penalizes thelonger skip distance of a skip bigram.
By doing so,for the sentence S, the complete word set is ?={w1,w2, w3,w4}.
The weights for the skip bigrams arelisted in Table 1:u)(Su?u)(Su?21ww   2?
12ww  2?11ww  3?
32ww  3?31ww  24 ??
?
42ww  4?41ww  35 ??
?
43ww  2?Table 1: Skip bigrams and their Weights in SIn Table 1, if ?
is set to 0.25, the weight of theskip bigram w1w2 in S is 0.252=0.0625, and w1w3 is0.254 +0.252=0.064.
Similarly, the skip bigramsand weights in the sentence T can be obtained.With the skip bigram-based vectors, cosine metricis then used to compute similarity between S and T.3 ExperimentsIn the STS task, three training datasets are avail-able: MSR-Paraphrase, MSR-Video andSMTeuroparl (Eneko et al, 2012).
The number ofsentence pairs for three dataset is 750, 750 and 734.In the following experiments, Let SWN, SWIKIandSSKIP denote similarity measures of the vector spacerepresentation using WordNet, Wikipedia and skipbigrams, respectively.
The three similaritymeasures are linearly combined as SCOMB:SKIPWIKIWNCOMB SSSS ????????
)1( ???
?where ?
and ?
are weight factors for SWN andSWIKI in the range [0,1].
If ?
is set to 1, only theWordNet-based similarity measure is used; if ?
is 0,the Wikipedia and skip bigram measures are used.Because each dataset has a different representa-tion for sentences, the parameter configurations forthem are different.
For the word similarity usingthe lexical resource WordNet, the path measure isused in experiments.
To get word relatedness fromthe English Wikipedia, the Wikipedia Miner tool2is used.
When computing sentence similarity basedon the skip bigrams, the decaying factor (DF) mustbe specified beforehand.
Hence, parameter config-urations for the three datasets are listed in Table 2:Table 2: Parameter ConfigurationsIn the testing phase, five testing dataset are pro-vided.
In addition to three test datasets drawn fromthe publicly available datasets used in the trainingphase, two surprise datasets are given.
They areSMTnews and OnWN (Eneko et al, 2012).SMTnews has 399 pairs of sentences and OnWNcontains 750 sentence pairs.
The parameter config-urations for these two surprise datasets are thesame as those for the dataset MSR-Paraphrase.The official scoring is based on Pearson correla-tion.
If the system gives the similarity scores closeto the reference answers, the system will attain ahigh correlation value.
Besides, three other evalua-tion metrics (ALL, ALLnrm, Mean) based on thePearson correlation are used (Eneko et al, 2012).Among the 89 submitted systems, the results ofour system are given in Table 3:Run ALL Rank ALLnrm RankNrm Mean RankMeanPolyUCOMP 0.6528 31 0.7642 59 0.5492 51Table 3: Performance using Different Metrics2 http://wikipedia-miner.cms.waikato.ac.nz/526Using the ALL metric, our system ranks 31, butfor ALLnrm and Mean metrics, our system rankingis decreased to 59 and 51.
In terms of ALL metric,our system achieves a medium performance, im-plying that our system correlates well with humanassessments.
In terms of ALLnrm and Mean met-rics, our system performance degrades a lot, imply-ing that our system is not well correlated with thereference answer when each dataset is normalizedinto the aggregated dataset using the least squareerror or the weighted mean across the datasets.To see how well each of the individual vectorspace models performed on the evaluation sets, weexperiment on the five datasets using vectors basedon WordNet, Wikipedia (Wiki), SkipBigram andPolyuCOMP (a combination of the three vectors).Table 4 gives detailed results of each dataset.Table 4: Pearson Correlation for each DatasetTable 4 shows that after combining three vectorrepresentations, each dataset obtains the best per-formance.
The WordNet-based approach gives abetter performance than Wikipedia-based approachin MSRvid dataset.
The two approaches, however,give similar performance in other four datasets.This is because the sentences in the MSRvid da-taset are too short with limited amount of contentwords.
It is difficult to capture the meaning of asentence without distinguishing words in consecu-tive positions.
This is why the order-sensitiveSkipBigram approach gives better performancethan the other two approaches.
For example,A woman is playing a game with a man.A man is playing piano.Using the semantic vectors, we will get highsimilarity scores, but the two sentences are dissimi-lar.
If the skip bigram approach is used, the simi-larity score between sentences will be 0, whichcorrelates with human judgment.
In parameter con-figurations for the MSRvid dataset, higher weight(1-0.123-0.01=0.867) is also given to skip bigrams.It is interesting to note that the decaying factor forthis dataset is 1.4 and is not in the range from 0 to1 inclusive.
This is because higher decaying factorhelps to capture semantic meaning between wordsthat span afar.
For example,A man is playing a flute.A man is playing a bamboo flute.In this sentence pair, the second sentence is en-tailed by the first one.
The similarity can be cap-tured by assigned larger decay factor to weigh theskip bigram ?playing flute?
in two sentences.Hence, if the value of the decay factor is greaterthan 1, the two sentences will become much moresimilar.
After careful investigation, these two sen-tences are similar to a large extent.
In this sense, ahigher decaying factor would help capture themeaning between sentence pairs.
This is quite dif-ferent from the other four datasets which focus onshared skip bigrams with smaller decaying factor.4 Conclusions and Future WorkIn the Semantic Textual Similarity task ofSemEval-2012, we proposed to combine the se-mantic vector with the order-sensitive skip bigramsto capture the meaning between sentences.
First, asemantic vector is derived from either theWordNet or Wikipedia.
The WordNet simulatesthe common human knowledge about word con-cepts.
However, WordNet is limited in its wordcoverage.
To remedy this, Wikipedia is used toobtain the semantic relatedness between words.Second, the proposed approach also considers theimpact of word order in sentence similarity by us-ing skip bigrams.
Finally, the overall sentence sim-ilarity is defined as a linear combination of thethree similarity metrics.
However, our system islimited in its approaches.
In future work, we wouldlike to apply machine learning approach in deter-mining sentence similarity.527ReferencesDavid Milne , Ian H. Witten.
2008.
An Effective, Low-cost Measure of Semantic Relatedness Obtained fromWikipedia Links.
In Proceedings of the first AAAIWorkshop on Wikipedia and Artificial Intelligence(WIKIAI'08), Chicago, I.LDekang Lin and Patrick Pantel.
2001.
Discovery of In-ference Rules for Question Answering.
Natural Lan-guage Engineering, 7(4):343-360.Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonza-lez-Agirre.
2012.
SemEval-2012 Task 6: A Pilot onSemantic Textual Similarity.
In Proceedings of the6th International Workshop on Semantic Evaluation(SemEval 2012), in conjunction with the First JointConference on Lexical and Computational Semantics(*SEM 2012).Gunes Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based Lexical Centrality as Salience in TextSummarization.
Journal of Artificial Intelligence Re-search, 22: 457?479.Lin, Chin-Yew and Franz Josef Och.
2004a.
AutomaticEvaluation of Machine Translation Quality UsingLongest Common Subsequence and Skip bigram Sta-tistics.
In Proceedings of the 42nd Annual Meeting ofthe Association for Computational Linguistics (ACL2004), Barcelona, Spain.Ou Jin, Nathan Nan Liu, Yong Yu and Qiang Yang.2011.
Transferring Topical Knowledge from Auxilia-ry Long Text for Short Text Understanding.
In: Pro-ceedings of the 20th ACM Conference onInformation and Knowledge Management (ACMCIKM 2011).
Glasgow, UK.Rada Mihalcea and Courtney Corley.
2006.
Corpus-based and Knowledge-based Measures of Text Se-mantic Similarity.
In Proceeding of the Twenty-FirstNational Conference on Artificial Intelligence andthe Eighteenth Innovative Applications of ArtificialIntelligence Conference.Ted Pedersen, Siddharth Patwardhan and JasonMichelizzi.
2004.
WordNet::Similarity?Measuringthe Relatedness of Concepts.
In Proceedings of the19th National Conference on Artificial Intelligence(AAAI, San Jose, CA), pages 144?152.Vasileios Hatzivassiloglou, Judith L. Klavans , EleazarEskin.
1999.
Detecting Text Similarity over ShortPassages: Exploring Linguistic Feature Combinationsvia Machine Learning.
In Proceeding of EmpiricalMethods in natural language processing and VeryLarge Corpora.Youngjoong Ko,  Jinwoo Park, and Jungyun Seo.
2004.Improving Text Categorization using the Importanceof Sentences.
Information Processingand Manage-ment, 40(1): 65?79.Yuhua Li, David Mclean, Zuhair B, James D. O'sheaand Keeley Crockett.
2006.
Sentence SimilarityBased on Semantic Nets and Corpus Statistics.
IEEETransactions on Knowledge and Data Engineering,18(8), 1138?1149.528
