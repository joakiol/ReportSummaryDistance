Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 991?1002,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsHarnessing different knowledge sources to measure semantic relatednessunder a uniform modelZiqi Zhang Anna Lisa Gentile Fabio CiravegnaDepartment of Computer Science, University of Sheffield211 Portobello, Regent CourtSheffield, S1 4DPz.zhang@dcs.shef.ac.uka.l.gentile@dcs.shef.ac.ukf.ciravegna@dcs.shef.ac.ukAbstractMeasuring semantic relatedness betweenwords or concepts is a crucial process tomany Natural Language Processing tasks.Exiting methods exploit semantic evidencefrom a single knowledge source, and arepredominantly evaluated only in thegeneral domain.
This paper introduces amethod of harnessing different knowledgesources under a uniform model formeasuring semantic relatedness betweenwords or concepts.
Using Wikipedia andWordNet as examples, and evaluated inboth the general and biomedical domains, itsuccessfully combines strengths from bothknowledge sources and outperforms state-of-the-art on many datasets.1    IntroductionSemantic relatedness (SR) measures how muchtwo (strings of) words or concepts are related byencompassing all kinds of relations between them(Strube and Ponzetto, 2006).
It is more generalthan semantic similarity.
SR is often an importantpre-processing step to many complex NaturalLanguage Processing (NLP) tasks, such as WordSense Disambiguation (Leacock and Chodorow,1998; Han and Zhao, 2010), and informationretrieval (Finkelstein et al, 2002).
In thebiomedical domain, SR is an important techniquefor discovering gene functions and interactions(Wu et al, 2005; Ye et al, 2005).There is an abundant literature on measuringSR between words or concepts.
Typically, thesemethods extract semantic evidence of words andconcepts from a background knowledge source,with which their relatedness is assessed.
Theknowledge sources can be unstructured documentsor (semi-)structured resources such as Wikipedia,WordNet, and domain specific ontologies (e.g., theGene Ontology1).In this paper, we identify two issues that havenot been addressed in the previous works.
First,existing works typically employ a singleknowledge source of semantic evidence.
Research(Strube and Ponzetto, 2006; Zesch and Gurevych,2010; Zhang et al, 2010) has shown that theaccuracy of an SR method differs depending on thechoice of the knowledge sources, and there is noconclusion which knowledge source is superior toothers.
Zhang et al (2010) argue that this indicatesdifferent knowledge sources may complement eachother.
Second, the majority of SR methods havebeen evaluated in general domains only, except afew earlier WordNet-based methods that have beenadapted to biomedical ontologies and evaluated inthat domain (Lord et al, 2003; Pedersen et al,2006; Pozo et al, 2008).
Given the significantattention that SR has received in specific domains(Pesquita et al, 2007), evaluation of SR methodsin specific domains is increasingly important.This paper addresses these issues by proposinga generic and uniform model for computing SRbetween words or concepts using multipleknowledge sources, and evaluating the proposedmethod in both general and specific domains.
Themethod combines and integrates semantic evidenceof words or concepts extracted from anyknowledge source in a generic graphrepresentation, with which the SR betweenconcepts or words is computed.
Using two of themost popular general-domain knowledge sources,1 http://www.geneontology.org/, last retrieved in Mar.
2011991Wikipedia and WordNet as examples, the methodis evaluated on 7 benchmarking datasets, includingthree datasets from the biomedical domain andfour from the general domain.
It has achievedexcellent results: compared to the baselines thatuse each single knowledge sources, combiningboth knowledge sources has improved the accuracyon all datasets by 2~11%; compared to state-of-the-art on the general domain datasets, the methodachieves the best results on three datasets; and onthe other three biomedical datasets, it obtains thebest result in one case; and second and third bestresults on the other two among eight participatingmethods, where all other competitors exploit somedomain-specific knowledge sources.The remainder of this paper is organized asfollows.
Section 2 discusses related work; Section3 presents the proposed method; Section 4describes the experiments and evaluation; Section5 discusses results and findings; Section 6concludes this paper.2    Related work2.1    SR methodsMethods for computing SR can be classified intopath based, Information Content (IC) based,statistical and hybrid methods.
Path basedmethods (Hirst and St-Onge, 1998; Leacock andChodorow, 1998; Pekar and Staab, 2002; Rada etal., 1989; Wu and Palmer, 1994) measure SRbetween words or concepts as a function of theirdistance in a semantic network, usually calculatedbased on the path connecting the words or conceptsby certain semantic (typically is-a) links.
IC basedmethods (Jiang and Conrath, 1997; Lin, 1998;Pirro et al, 2009; Resnik, 1995; Seco et al, 2004)assess relatedness between words or concepts bythe amount of information they share, usuallydetermined by a higher level concept thatsubsumes both concepts in a taxonomic structure.Statistical methods measure relatedness betweenwords or concepts based on their distribution ofcontextual evidence.
This can be formalized as co-occurrence statistics collected from unstructureddocuments (Chen et al, 2006; Cilibrasi andVitanyi, 2007; Matsuo et al, 2006), ordistributional concept or word vectors withfeatures extracted from either unstructureddocuments (Harrington, 2010; Wojtinnek andPulman, 2011) or (semi-)structured knowledgeresources (Agirre et al, 2009; Gabrilovich andMarkovitch, 2007; Gouws et al, 2010; Zesch andGurevych, 2007; Zhang et al, 2010).
Hybridmethods combine different purebred methods incertain ways.
For example Riensche et al (2007)employ both an IC based method (Resnik, 1995)and a statistical method (cosine vector similarity)in their study.
Pozo et al (2008) derive a taxonomyof terms from unstructured documents by applyinghierarchical clustering based on corpus statistics,then apply path based method on this taxonomy tocompute SR. Han and Zhao (2010) use one ICbased method and two statistical methods tocompute SR, then derive an aggregated score.2.2    SR knowledge sources and domainsComputing SR requires background knowledgeabout concepts or words, which can be extractedfrom unstructured corpora, semi-structured andstructured knowledge resources.
Unstructuredcorpora are easier to create and cheaper tomaintain, however, semantic relations betweenwords or concepts are implicit.
Methods (Chen etal., 2006; Cilibrasi and Vitanyi 2007; Matsuo et al,2006) that exploit unstructured corpora typicallydepend on distributional statistics, and thus mayignore important semantic evidences present in(semi-)structured knowledge sources (Pan andFarrell, 2007).
Recent studies (Harrington, 2010;Pozo et al, 2008; Wojtinnek and Pulman, 2011)propose to pre-process a corpus to learn a semanticnetwork, with which SR is computed.
This createshigh pre-processing cost; also, the choice of corpusand its size often have a direct correlation with theaccuracy of SR methods (Batet et al, 2010).
(Semi-)Structured knowledge sources on theother hand, organize semantic knowledge aboutconcepts and words explicitly and interlink themwith semantic relations.
They have been popularchoices in the studies of SR, and they includelexical resources such as WordNet, Wiktionary,and (semi-)structured encyclopedic resources suchas Wikipedia.
WordNet has been used in earlierstudies (Hirst and St-Onge, 1998; Jiang andConrath, 1997; Lin, 1998; Leacock and Chodorow1998; Resnik, 1995; Seco et al, 2004; Wu andPalmer, 1994) and is still a preferred knowledgesource in recent works (Agirre et al, 2009).However, its effectiveness may be hindered by itslack of coverage of specialized lexicons anddomain specific concepts (Strube and Ponzetto,9922006; Zhang et al, 2010).
Wikipedia andWiktionary are collaboratively maintained know-ledge sources and therefore may overcome thislimitation.
Wikipedia in particular, is found to havereasonable coverage of many domains (Hollowayet al, 2007; Halavais, 2008).
It has becomeincreasingly popular in SR studies recently.However, research (Zesch and Gurevych, 2010)have shown that methods based on Wikipedia haveno clear advantage over WordNet-based methodson some general domain datasets in terms ofaccuracy, while Zhang et al (2010) argue thatdifferent knowledge sources may complement eachother, and SR methods may benefit fromharnessing different knowledge sources.Several studies (Lord et al, 2003; Pedersen etal., 2006; Petrakis et al, 2006; Pozo et al, 2008)have adapted state-of-the-art to domain specificknowledge sources (e.g., the Gene Ontology, theMeSH2) and evaluated them therein.
Despite theseefforts, a large proportion of state-of-the-art is stillonly evaluated in the general domain.2.3    SR methods similar to this workFew works have attempted at combining differentknowledge sources in SR studies, especially (semi-)structured knowledge sources.
The closest studiesare Han and Zhao (2010) and Tsang and Stevenson(2010).
Han and Zhao firstly compute SR betweenwords using three state-of-the-art SR methodsseparately.
Next, one score is chosen subject to anarbitrary preference order, and used to create aconnected graph of weighted edges betweenwords.
A recursive function is then applied to thegraph to compute final SR scores between words.Essentially, each SR method is applied in isolationand features from different sources are usedseparately with each distinctive method.
Althoughthis retains advantages of each method, thelimitations of them are also combined.Tsang and Stevenson (2010) combine WordNetand unstructured documents by weighing eachword found in WordNet using its frequencyobserved in a large corpus.
The frequencieshowever, are sensitive to the choice of corpus, thusdifferent corpora may result in different accuracies.Furthermore, their method is only applicable tocomputing SR between pairs of sets of words orconcepts.2 http://www.nlm.nih.gov/mesh/ last retrieved in March 20113    MethodologyWe define a set of requirements for SR methodsthat harness different knowledge sources:?
It should improve over the same methodbased on a single knowledge source?
It should be generic and applicable to anyknowledge source?
It should be robust in dealing withknowledge source specific features butalso tolerate the quality and coverageissues of individual knowledge sourceOur method of harnessing different knowledgesources contains four steps.
Firstly (Section 3.1),each word or word segment is searched in eachknowledge source to identify their contexts that isspecific to that knowledge source.
We define acontext as the representation of meaning or aconcept for a word.
In the following, we say thateach context is associated with a distinct concept.Secondly (Section 3.2), for each concept of aninput word, features are extracted from its contextand a graph representation of each concept andtheir features is created.
Thirdly (Section 3.3),cross-source contexts are mapped where they referto the same concept, thus their features fromdifferent sources can be combined to derive anenriched representation.
This creates a final,uniform graph representation where input wordsare connected by shared features of theirunderlying candidate concepts.
Then (Section 3.4)the graph is submitted to a generic algorithm tocompute SR between words.In the following, we discuss details with respectto different types of knowledge sources, whilefocusing on Wikipedia and WordNet in ourexperiments for two reasons.
First, they are usedby the majority of SR methods and are thereforemost representative knowledge sources.
Second,they have strongly distinctive and complementarycharacteristics, which make ideal testbeds for therequirements.
On one hand, WordNet is a lexicalresource containing rich and strict semanticrelations between words, but lacks coverage ofspecialized vocabularies.
On the other hand,Wikipedia is a semi-structured resource with goodcoverage of domains and named entities, but thesemantic knowledge is organized in a looser way.9933.1    Context retrievalGiven a pair of words or word segments, we firstlyidentify contexts representing the underlyingmeanings or concepts from each knowledgesource.
For lexical resources, this could bedistinctive word senses.
In WordNet (WN), acontext corresponds to a single synset, whichcorresponds to a concept.
We search each word inWordNet and extract all possible synsets.
Let w bea word or word segment (e.g., ?cat?
), andbe the set of k concepts of wextracted from WordNet.Using Wikipedia (WK) as an example semi-structured resource, the context can be an articlethat describes a unique concept.
Thus we searchfor underlying articles that describe differentconcepts.
Firstly, we search w in Wikipedia, wherethree situations may be anticipated.
If a single non-disambiguation page describing a concept isreturned, the concept is selected and the retrieval iscomplete.
In the second case, a disambiguationpage linking to all possible concept pages may bereturned.
This page lists all underlying conceptsand entities referenced by w as links and a shortdescription with each link.
In this case, we alwayskeep the first concept page, which is found often tobe the most common sense of the word;additionally, we select other concept pages whoseshort descriptions contain the word w. We do notselect all linked pages because many of these infact link to a concept relevant to w, but notnecessarily a candidate sense of w. Thirdly, if nopages are returned for w, we search for the mostrelevant page using w as keyword(s) in an invertedindex of all Wikipedia pages (e.g., via searchengines).
We denote concepts retrieved fromWikipedia as.For unstructured sources such as documents, asimple approach could be defining a word contextas a text passage around each occurrence of w, andgrouping similar contexts of w as representation ofits underlying meanings, or concepts.
Alternatively,more complex approaches such as Pozo et al(2008) and Harrington (2010) may be applied toextract a lexical network of words, whereby similarmethods to WordNet can be applied.3.2 Feature extraction and representationNext, for each concept identified from aknowledge source, features are extracted from theircorresponding contexts.
In our case, for each, we follow the work by Zhang et al(2010) to extract four types of features from theircorresponding Wikipedia pages.
Figure 1 shows anexample representation of a concept and itsWikipedia features:?
Words from page titles and redirectionlinks (can be considered as synonyms)?
Words from categories, used as higherlevel hypernyms in some studies (Zesch etal., 2010; Strube and Ponzetto, 2006)?
Words from outgoing links?
Top n most frequent words from a pageFigure 1.
Representation of the concept ?cat, themammal?
using different types of featuresextracted from Wikipedia.
The shaded circlerepresents the concept; ovals represent featurevalues; edges connecting feature values to theconcept and <labels> represent feature typesFor each, we extract ten features fromWordNet: hypernyms, hyponyms, meronyms,holonyms, synonyms, antonyms, attributes, ?seealso?
words, ?related?
words, and gloss.
These arealso represented in the same way as in Figure 1.With unstructured sources, contextual wordscan be used as features.
Alternatively, if a lexicalnetwork is extracted, features may be extracted in asimilar way to those of WordNet.Additionally, with WordNet and Wikipedia, wealso propose several intra-resource feature mergingstrategies to study the effect of featurediversification.
This is because, while someapproaches (such as Agirre et al, 2009;Harrington, 2010; Yeh et al, 2009) do notdistinguish different feature types in graphconstruction, or adopt a bag-of-words featurerepresentation (such as Zesch and Gurevych,2010), others (such as Yazdani and Popescu-Belis,2010; Zhang et al, 2010) have used differentiated994feature types and weights in their model.
Wetherefore carry out studies to investigate this issue.Specifically, for the original four Wikipediafeatures, we create a bag-of-words feature thatsimply merges all feature types (i.e., all edges inFigure 1 will have the same label).
For the originalten WordNet features, we propose two mergedrepresentations corresponding to that of Wikipedia,so as to support the studies of feature enrichmentin the following section.
We introduce a bag-of-words feature that collapses all different featuretypes, and a four-feature representation as follow:?
wn-synant merges WordNet synonyms andantonyms.?
wn-hypoer merges WordNet hypernymsand hyponyms, collectively representingfeatures by ?is-a?
semantic relation?
wn-assc merges WordNet meronyms,holonyms, related and ?see also?, whichare features corresponding to associativerelations?
wn-dist merges WordNet gloss andattributes that generally describe a concept.3.3 Concept mapping and feature enrichmentOur method essentially harnesses differentknowledge sources by combining featuresextracted from different sources in a uniformmodel.
This requires two sub-processes: cross-source concept mapping and cross-sourcefeature enrichment.In cross-source concept mapping, conceptsextracted from different knowledge sources aremapped according to similar meanings such thatcross-source features can be combined.
To do so,we select the concepts from one knowledge sourceas the reference concept set; then concepts fromother knowledge sources are mapped to referenceconcepts of similar meanings.
There can bedifferent criteria of choosing reference knowledgesource concepts.
Empirically, we found itnecessary to choose the knowledge source withbroader coverage and richer features.
This will bediscussed later in Section 5.
Following thisstrategy, in our example,is chosen asreference concepts, and for eachweselect asuch thatandrefer tothe same meaning.
To do so, we apply a simplemaximum set overlap metric to their featurevalues.
Let F(c) be a function that returns allfeature values of c as bag-of-words, then for each, it is mapped to asuch thatis maximized among all.
The resulting concept candidates aredenoted as, where=is amapped set of concepts potentially referring to thesame meaning.
Ifthen.Next, cross-source feature enrichment createsa uniform feature representation for each mappedsets of concepts.
The process can be considered asenriching the features from one knowledge sourcewith others.
The most straightforward approach isto simply collect features extracted from eachknowledge source on to a single graph, retainingthe diversity in feature types.
For example, Figure2 shows a graph representation based on thecollection of the four Wikipedia features and thefour derived WordNet features.
We refer to thisapproach as ?feature combination?.Figure 2.
Representation of ?cat, the mammal?after concept mapping and feature combinationOn the other hand, cross-source features may bemerged according to their semantics.
For example,WordNet and Wikipedia contain features based onsynonyms of concepts; while Wikipedia andunstructured documents contain word distribution-al features.
Thus we define ?feature integration?as merging feature types from different knowledgesources into single types of features based on theirsimilarity in semantics.
With WordNet and Wiki-pedia, we integrate features as below (Figure 3):?
merged-synant merges Wikipedia pagetitles and redirection links with wn-synant?
merged-hypoer merges merges Wikipediacategories with wn-hypoer995?
merged-assc merges Wikipedia links withwn-assc.
We consider Wikipedia links bearother associative relations and aretherefore merged with features extractedby other WordNet relations?
merged-dist merges Wikipedia frequent nwords with wn-dist.Figure 3.
Representation of ?cat, the mammal?after concept mapping and feature integrationNote that the difference between cross-sourcefeature combination and integration is that theformer introduces more types of features, whereasthe latter retains same number of feature types butincreases feature values for each type.
Both havethe effect of establishing additional path (viafeatures) between concepts, but in different ways.With intra-resource feature diversification, cross-source feature combination and featureintegration, we create a total of nine intra- andcross-source feature representations to be testedwith the uniform random walk model:?
four types of Wikipedia features (wk-4F)?
one type of Wikipedia features (wk-1F)?
ten types of WordNet features (wn-10F)?
four types of WordNet features (wn-4F)?
one type of WordNet features (wn-1F)?
wk-4F combines wn-4F: wk-4F+wn4F,C?
wk-4F integrates wn-4F: wk-4F+wn4F,I?
wk-1F combines wn-1F: wk-1F+wn1F,C?
wk-1F integrates wn-1F: wk-1F+wn1F,I3.4 Computing SR using the graphThe algorithm for computing SR using the graph isbased on the idea of random walk.
It formalizes theidea that taking successive steps along the paths ina graph, the ?easier?
it is to arrive at a target nodestarting from a source node, the more related thetwo nodes are.
Following the previous steps, thefeature representations of all candidate conceptsrelevant to the input word pairs are joined, whichcreates a single undirected, weighted, bi-partitegraph.
Let G = (V, E) be the graph, where V is theset of nodes (concepts and feature values); E is theset of edges (feature types) that connect conceptsand features.
As shown in Figure 4, differentconcepts are connected if they share same valuesof same types of features, namely, there exists apath that connects one concept to another.Figure 4.
Paths are established between differentconcepts if they share values of same feature types<bold underlined>Using Figure 4 it is easier to comprehend thedifference between feature combination andintegration.
Since concept nodes can only beconnected by same types of edges (feature types),feature combination increases the chances ofconnectivity by adding in more types of edges,while integration merges similar types of edgesacross knowledge sources and increases thenumber of feature nodes connected by each type.From the graph, we start by building anadjacency matrix W of initial probabilitydistribution:??????????
??????
?
?otherwiseEjililEilwW Ll kkijk,0),(,|),(:),(|)( [1]Where Wij is the ith-line and jth-column entry of W,indexed by V; l(i, j) is a function that returns thetype of edge (i.e., type of feature) connectingnodes i and j; L is the set of all possible types; w(l)returns the weight for that type.
Essentially, L isthe collection of all feature types, and w(l) assigns996a weight to a particular feature type.
Next, wecompute the transition probability matrix P(t)(j|i) =[(D?1W)t]ij (Dii = ?kWik), which returns theprobability of reaching other nodes from a startingnode on the graph after t steps.
In this method, wefollow the work by Rowe and Ciravegna (2010) toset t=2 in order to preserve locally connectednodes.
Next, we extract the probability vectorscorresponding to concept nodes from P, andcompute pair-wise relatedness using the cosinefunction.
Effectively, this formalizes the notionthat two concepts related to a third concept is alsosemantically related, which is similar to thehypothesis proposed by Patwardhan and Pedersen(2006) in their method based on second-ordercontext vectors.
The final SR between the inputword pair is the maximum pair-wise concept SR.4    Experiment and evaluationWe evaluate the method based on correlationagainst human judgment (gold standard) on sevenbenchmarking datasets covering both general andtechnical domains.
These include four generaldomain datasets: the Rubenstein and Goodenough(1965) dataset containing 65 pairs of nouns(RG65); the Miller and Charles (1991) dataset thatis a subset of the RG-65 dataset and contains 30pairs (MC30); the Finkelstein et al (2002) datasetwith 353 pairs of words, including nouns, verbs,adjectives, as well as named entities.
This containstwo subsets, a set of 153 pairs (Fin153) and a set of200 (Fin200) pairs each annotated by a differentgroups of annotators.
Zesch and Gurevych (2010)show largely varying Inter-Annotator-Agreement(IAA) between the two sets (Table 1), and arguethat they should be treated as separate datasets.Three biomedical datasets are selected to evaluatedomain-specific performance of the proposedmethod.
These include a set of 36 MeSH term pairsin Petrakis et al (2006) (MeSH36), 30 pairs ofmedical terms annotated by a group of physiciansas in Pedersen et al (2006) (Ped30-p) and the sameset annotated by a different group of medicalcoders (Ped30-c).
Table 1 shows statistics of theseven datasets.The correlation is computed using theSpearman rank order coefficient for two reasons.First, it is a better metric than other alternatives(Zesch and Gurevych, 2010).
Second, it isconsistent with the majority of studies such thatresults can be compared.Dataset Size Domain IAAMC30 30 General 0.9RG65 65 General 0.8Fin153 153 General 0.73Fin200 200 General 0.55Ped30-p 30 Biomedical 0.68Ped30-c 30 Biomedical 0.78MeSH36 36 Biomedical -Table 1: Information of benchmarking datasetsWe distribute feature weights w(l) acrossdifferent feature types L evenly in each featurerepresentation.
Although Zhang et al (2010) showthat discriminated feature weights leads toimproved accuracy; this is not the focus of thisstudy.
Since we aim to investigate the effects ofharnessing different knowledge sources, weobtained baseline performances by applying themethod to those feature representations based onsingle knowledge sources (i.e., wk-4F, wk-1F, wn-10F, wn-4F, wn-1F).
Tables 2 and 3 show the bestresults obtained with baselines and correspondingknowledge sources and feature representation.Dataset Corr.
Feature Coverage (% pairs)MC30 0.77 wn-1F 77%RG65 0.71 wn-1F 65%Fin153 0.45 wn-4F 82%Fin200 0.35 wn-4F 76%Ped30-p 0.66 wn-4F 33%Ped30-c 0.8 wn-4F 33%MeSH36 0.49 wn-1F 50%Table 2: Correlation obtained using WordNet.Many word pairs are not covered due to sparsefeature space and lack of coverage.
Only coveredpairs are accounted.Dataset Corr.
FeatureMC30 0.74 wk-1FRG65 0.67 wk-1FFin153 0.7 wk-1FFin200 0.51 wk-4FPed30-p 0.53 wk-4FPed30-c 0.58 wk-4FMeSH36 0.73 wk-4FTable 3: Correlation obtained using onlyWikipedia.
All word pairs are 100% covered.997Tables 4 ?
6 show results obtained withenriched feature representation.Combination (C) Integration (I)Dataset wn-4F +wk-4Fwn-1F +wk-1Fwn-4F +wk-4Fwn-1F+ wk-1FMC30 0.77 0.8 0.8 0.79RG65 0.74 0.73 0.73 0.729Fin153 0.73 0.75 0.74 0.73Fin200 0.52 0.54 0.53 0.54Ped30-p 0.63 0.52 0.64 0.47Ped30-c 0.64 0.52 0.67 0.49MeSH36 0.7 0.694 0.75 0.7Table 4: Correlation obtained using bothknowledge sources.
Word pairs are 100% covered.KS and # of feature typesWN WK WK+WN,C WK+WN, IMC30 1 1 1 4RG65 1 1 4 4Fin153 4 1 1 4Fin200 4 4 1 1Ped30-p 4 4 4 4Ped30-c 4 4 4 4MeSH36 1 4 4 4Table 5: Number of feature types with which bestresults are obtained on each dataset.
KS:Knowledge SourceSingle KS Multiple KS Impr.Dataset Best corr.
Best corr.
StrategyMC30 0.74 0.8 C/I 0.06RG65 0.67 0.74 C 0.07Fin153 0.7 0.75 C 0.05Fin200 0.51 0.54 C/I 0.03Ped30-p 0.53 0.64 I 0.11Ped30-c 0.58 0.67 I 0.09MeSH36 0.73 0.75 I 0.02Table 6: Improvement achieved by harnessingmultiple KSs.
Best correlation with single KS isbased on Wikipedia, which provides 100%coverage of word pairs.Tables 7 and 8 compare our method against state-of-the-art.
For Table 8, figures for other state-of-the-art systems can be found in correspondingpublications; while we only list the bestperforming systems for comparison.MC30 RG65 Fin153 Fin200 KSbest ofWN+WK0.8 0.74 0.75 0.54 BothRad89* 0.75 0.79 0.33 0.24 WNLC98* 0.75 0.79 0.33 0.24 WNWP94* 0.77 0.78 0.38 0.24 WNHS98* 0.76 0.79 0.33 0.32 WNRes95* 0.72 0.74 0.35 0.26 WNJC97* 0.68 0.58 0.28 0.10 WNLin98* 0.67 0.60 0.27 0.17 WNZes07* 0.77 0.82 0.6 0.51 WKGM07* 0.67 0.75 0.69 0.51 WKZha10 0.71 0.76 0.71 0.46 WKTable 73: Comparison against state-of-the-art in thegeneral domain.
(* figures from Zesch andGurevych, 2010)Ped30-p Ped30-c MeSH36 KSbest ofWN+WK0.64 0.67 0.75 WN+WKPet06 best - - 0.74 MeSHPed06 best 0.84 0.75 - GO, DPed06 second 0.62 0.68 - GO, DTable 84: Comparison against state-of-the-art in thebiomedical domain.
GO ?
Gene Ontology; D ?document sets.Given the fact that some datasets (i.e., MC30,Ped30-p, Ped30-c, MeSH36) have a relatively lowsample size, we cannot always be sure thatcorrelation values are accurate or occurred bychance.
Therefore, we measure the statisticalsignificance of correlation by computing the p-value for the correlation values reported for oursystem in Tables 7 and 8.
For all cases, a p-valueof less than 0.001 is obtained, which indicates thatcorrelation values are statistically significant.3 Rada (1989) (Rad89); Leacock and Chodorow (1998)(LC98); Wu and Palmer (1994) (WP04); Hirst and St-Onge(1998) (HS98); Resnik (1995) (Res95); Jiang and Conrath(1997) (JC97); Lin (1998) (Lin98); Zesch and Gurevych(2007) (ZG07); Gabrilovich and Markovitch (2007) (GM07);Zhang et al (2010) (Zha10)4 Petrakis et al (2006) (Pet06); Pedersen et al (2006) (Ped06).Original participating systems can be found in these works.9985    DiscussionSingle v.s.
multiple knowledge sources As shownin Table 6, considering the best performancesacross all feature enrichment strategies and featuresets, the proposed method successfully harnesseddifferent knowledge sources and improved over thebaselines using single knowledge sources by 0.02~ 0.11.
The biggest improvement (0.11) is on adomain-specific dataset, on which the methodbased on single knowledge source performedpoorly in terms of coverage and accuracy.
The bestenrichment strategy that has consistently improvedthe baselines is wk-4F+wn-4F, Integration (Table4 v.s.
Table 3).
With features enriched frommultiple knowledge sources, the method alsoconsistently improved over their correspondingsingle-source features on all datasets, exceptMeSH36, on which wk-4F+wn-4F, Combination(Table 4) slightly reduced the accuracy obtainedwith wk-4F (Table 3) only.The large proportion of uncovered word pairsusing WordNet is due to its lack of coverage ofspecialized lexicons, and sparser semantic content.For example, of all 115 distinctive terms in thePed30 and MeSH36 datasets, 30% are not includedin WordNet.
And of all 447 distinctive words in allgeneral domain datasets, only 69% have multiplesynonyms.
Features such as attributes and ?seealso?
are present for less than 20 words.
This is thereason that some approaches using WordNet (e.g.,Agirre et al, 2009) require a graph of all WordNetlexicons to be built, thus intermediate words may?bridge?
input words even if they do not connectdirectly by their features.
Nevertheless, theimprovement in accuracy and 100% coverage afterharnessing both knowledge sources suggests thatthey complement each other well.
On one hand,Wikipedia brings its strength in domain andcontent coverage; on the other hand, WordNetbrings useful semantic evidences for words that arecovered.Concept mapping and feature enrichmentmethods While the set overlap based method forcross-source concept mapping using the referenceknowledge source concepts is simple and provedsuccessful, the accuracy of mapping and itscorrelation with the accuracy of the SR methodwas not studied.
This will be explored in the future.Also, alternative mapping methods will beinvestigated.
For example, Toral and Mu?oz (2006)describe a different method of mapping Wikipediaarticles to WordNet synsets; one could also adopt asimple disambiguation process to select the bestcandidate concept from each knowledge sourcesuited for the input word pairs, whereby cross-source concept mapping becomes straightforward.In terms of feature enrichment strategies, there isno strong indication (Table 6) of which (featurecombination v.s.
integration) is more effective,although the system consistently outperforms thebaselines (Table 4 v.s.
Table 3) with the wk-4F+wn-4F, Integration strategy.Feature diversification v.s.
unification Table5 suggests that in most cases, differentiatingfeature types leads to better results than mergingthem uniformly, despite the knowledge sourcesused.
This is consistent with the findings by Zhanget al (2010).
This can be understandable sincealthough unifying feature types effectivelyincreases possibility of sharing features, equally,this may also increase the proportion of noisyfeatures.
For example, consider the Wikipediaarticle of ?Horse?
(animal), which has a categorylabel ?livestock?
; and the article ?Famine?, whichhas an outgoing link ?livestock?
(in a sentencedescribing diseases that caused decline of livestockproduction).
By differentiating the feature types?has_category?
and ?has_outlink?, the twoconcepts will not be connected even if they bothhave the same word ?livestock?
in their featurerepresentation.
However, using a bag-of-wordsrepresentation where feature types areundistinguished, the strength of their relatedness isboosted by sharing this word, which may beuninteresting in this occasion.Compared against state-of-the-art, theproposed method has achieved promising results.Overall, by harnessing different knowledge sources,the method achieves, and in many cases,outperforms state-of-the-art.
In the general domain,it outperforms state-of-the-art on three out of fourdatasets.
It is worth noting that all methods basedon WordNet generally have poor performance onthe Fin153 and Fin200 datasets (Table 7).
Despitethe heterogeneity in these datasets, this may alsorelate to the quality of the feature space generatedwith WordNet.
In fact methods using Wikipediaperform better on these datasets.
With enrichedfeatures from both knowledge sources, theaccuracies are further improved.999In the biomedical domain, the proposed methodoutperforms state-of-the-art on one dataset andproduces competitive results on others.
Note thatall other methods exploit domain-specificontologies and corpora.
The Ped06 best and Ped06second methods also depend on a corpus of onemillion documents.
These results further confirmedthe benefits of our method: harnessing knowledgefrom general-purpose knowledge sources oflimited domain coverage, it is possible to achieveresults that rival methods based on well-curatedand specially tailored domain-specific knowledgesources.
This is an encouraging finding.
Althoughthere are abundant resources in the biomedicaldomain for this type of tasks, such resources maybe scarce in other domains and are expensive tobuild.
However, the results suggest that theproposed method offers a more affordableapproach that provides reasonable coverage andquality, even if individual general knowledgesources may be limited in themselves.Generality of the method.
The proposedmethod represents features extracted from differentknowledge sources in a generic manner, whichfacilitates cross-source feature enrichment andrequires generic algorithm computation.
Asdiscussed in Section 3, semantic evidence of wordsand concepts may be extracted from differentknowledge sources in different ways, whileharnessed in the generic model.
In contrast, othermethods using multiple knowledge sources (e.g.,Han and Zhao, 2010; Tsang and Stevenson, 2010)introduce algorithms that are bound to theknowledge sources, which may limit theiradaptability and portability.6    ConclusionThis paper introduced a generic method ofharnessing different knowledge sources to computesemantic relatedness.
We have shown empiricallythat different knowledge sources containcomplementary semantic evidence, which, whencombined together under a uniform model, canimprove the accuracy of SR methods.
Moreover,we have demonstrated its robustness in dealingwith knowledge sources of different quality andcoverage.
Several remaining issues will be studiedin the future.
First, additional knowledge sourceswill be studied, particularly unstructured corporaand domain-specific resources.
The experimentshave shown that although harnessing differentknowledge sources achieved encouraging resultson biomedical datasets, they are still far from beingperfect.
While it should be appreciated that theresults are obtained using only general purposeknowledge sources, it would be interesting toinvestigate whether harnessing domain specificknowledge sources (where available) furtherimproves the performance.
Second, differentmethods of concept mapping will be studied.
Wewill also design methods for assessing the qualityof mapping, and analyze their correlations with theSR methods.
Third, analyses will be carried out touncover the differences between featurecombination and integration that have led todifferent accuracies.AcknowledgmentsPart of this research has been funded under the EC7th Framework Program, in the context of theSmartProducts project (231204).ReferencesAgirre, E., Alfonseca, E., Hall, K., Kravalova, J., Pasca,M., Soroa, A.
2009.
A Study on Similarity andRelatedness Using Distributional and WordNet-based Approaches.
In Proceedings of NAACL?09Batet, M., S?nchez, D., Valls, A.
2010.
An ontology-based measure to compute semantic similarity inbiomedicine.
In Journal of Biomedical Informatics,44(1), 118-125Chen, H., Lin, M., Wei, Y.
2006.
Novel associationmeasures using web search with double checking.Proceedings of COLING?06-ACL?06, pp.
1009-1016Cilibrasi, R., Vitanyi, P. 2007.
The Google SimilarityDistance.
In IEEE Transactions on Knowledge andData Engineering.
19(3), 370-383Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E.,Solan, Z., Wolfman, G., and Ruppin, E. (2002).Placing search in context: the concept revisited.
InACM Transactions on Information Systems, 20 (1),pp.
116 ?
131Gabrilovich, E., Markovitch, S. 2007.
Computingsemantic relatedness using Wikipedia-based explicitsemantic analysis.
In proceeding of IJCAI'07Gouws, S., Rooyen, G., Engelbrecht, H. 2010.Measuring conceptual similarity by spreadingactivation over Wikipedia?s hyperlink structure.Proceedings of the 2nd Workshop on The People?sWeb Meets NLP: Collaboratively ConstructedSemantic Resources1000Halavais , A.
2008.
An Analysis of Topical Coverage ofWikipedia.
Journal of Computer-MediatedCommunication, 13(2)Han, X., Zhao, J.
2010.
Structural semantic relatedness:a knowledge-based method to named entitydisambiguation.
In the 48th Annual Meeting of theAssociation for Computational Linguistics.Harrington, B.
2010.
A semantic network approach tomeasuring relatedness.
In Proceedings of COLING?10Hirst, G., and St-Onge, D. 1998.
Lexical chains asrepresentation of context for the detection andcorrection malapropisms.
In Christiane Fellbaum(ed.
), WordNet: An Electronic Lexical Database andSome of Its Applications, pp.
305?332.
Cambridge,MA: The MIT Press.Holloway, T., Bozicevic, M., B?rner, K. 2007.Analyzing and visualizing the semantic coverage ofWikipedia and its authors.
In Journal of Complexity,Special issue on Understanding Complex Systems,12(3), 30-40Jiang, J. and D. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.Proceedings of the International Conference onResearch in Computational Linguistics, pp.
19-33Leacock, C., Chodorow, M. 1998.
Combining localcontext and WordNet similarity for word senseidentification.
In C. Fellbaum (Ed.
), WordNet.
AnElectronic Lexical Database, Chp.
11, pp.
265-283.Lin, D. 1998.
An information-theoretic definition ofsimilarity.
Proceedings of the Fifteenth InternationalConference on Machine Learning, pp.
296-304Lord, P., Stevens, R., Brass, A., Goble, C. 2003.Investigating semantic similarity measures acrossthe Gene Ontology: the relationship betweensequence and annotation.
In Bioinformatics, 19(10),pp.
1275?1283Matsuo, Y., T.
Sakaki., K., Uchiyama, M., Ishizuka.2006.
Graph-based word clustering using a websearch engine.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), pp.542-550Miller, G., Charles, W. 1991.
Contextual correlates ofsemantic similarity.
In Language and CognitiveProcesses, 6(1): 1-28Pan, F., Farrell, R. 2007.
Computing semantic similaritybetween skill statements for approximate matching.In Proceedings of NAACL-HLT?07, pp.
572-579Patwardhan, S., Pedersen, T. 2006.
Using WordNet-based context vectors to estimate the semanticrelatedness of concepts.
Proceedings of the EACL2006 Workshop on Making Sense of Sense:Bringing Computational Linguistics andPsycholinguistics TogetherPedersen, T., Pakhomov, S., Patwardhan, S., Chute, C.2006.
Measures of semantic similarity andrelatedness in the biomedical domain.
Journal ofBiomedical Informatics 40(3), 288-299Pekar, V., Staab, S. 2002.
Taxonomy learning: factoringthe structure of a taxonomy into a semanticclassification decision.
Proceedings of COLING?02.pp.
786-792Pesquita, C., Faria, D., Bastos, H., Falc?o, A., Couto, F.(2007).
Evaluating GO-based Semantic SimilarityMeasures.
ISMB/ECCB 2007 SIG Meeting ProgramMaterials, International Society for ComputationalBiology 2007Petrakis, E., Varelas, G., Hliaoutakis, A., Raftopoulou,P.
2006.
Design and evaluation of semanticsimilarity measures for concepts stemming from thesame or different ontologies.
In 4th Workshop onMultimedia Semantics (WMS'06), pp.
44-52.Pirro, G. 2009.
A semantic similarity metric combiningfeatures and intrinsic information content.
In Dataand Knowledge Engineering, 68(11), pp.
1289-1308Pozo A., Pazos F., Valencia, A.
2008.
Definingfunctional distances over gene ontology.
In BMCBioinformatics 9, pp.50Rada, R., Mili, H., Bicknell, E., Blettner, M. 1989.Development and application of a metric onsemantic nets.
In IEEE Transactions on Systems,Man and Cybernetics 19(1), pp.17-30Resnik, P. (1995).
Using information content to evaluatesemantic similarity in a taxonomy.
In Proceedings ofIJCAI-95, pp.
448-453Riensche, R., Baddeley, B., Sanfilippo, A., Posse, C.,Gopalan, B.
2007.
XOA: Web-Enabled Cross-Ontological Analytics.
IEEE Congress on Services,pp.
99-105Rowe, M., Ciravegna, F. 2010.
Disambiguating identityweb references using Web 2.0 data and semantics.M Rowe and F Ciravegna.
The Journal of WebSemantics.Rubenstein, H., Goodenough, J.
1965.
Contextualcorrelates of synonymy.
In Communications of theACM, 8(10):627-633Seco, N., and Hayes, T. 2004.
An intrinsic informationcontent metric for semantic similarity in WordNet.In Proceedings of the 16th European conference onArtificial IntelligenceStrube, M., Ponzetto, S. 2006.
WikiRelate!
Computingsemantic relatedness using Wikipedia.
InProceedings of the 21st national conference onArtificial intelligence (AAAI)Toral, A., Mu?oz, R. 2006.
A Proposal to AutomaticallyBuild and Maintain Gazetteers for Named EntityRecognition by using Wikipedia.
In Proceedings ofWorkshop on New Text, ACL?06.Tsang, V., Stevenson, S. 2010.
A graph-theoreticframework for semantic distance.
In Journal ofComputational Linguistics, 36(1).1001Wojtinnek, P., Pulman, S. 2011.
Semantic relatednessfrom automatically generated semantic networks.
InProceedings of the Ninth International Conferenceon Computational Semantics (IWCS?11)Wu, Z. Palmer, M. 1994.
Verbs semantics and lexicalselection.
Proceedings of the 32nd annual meetingon Association for Computational Linguistics, pp.133-138Wu, H., Su, Z., Mao, F., Olman, V., Xu, Y.
2005.Prediction of functional modules based oncomparative genome analysis and gene ontologyapplication.
Nucleic Acids Research, 33, pp.
2822?2837.Yazdani, M., Popescu-Belis, A.
2010.
A random walkframework to compute textual semantic similarity: aunified model for three benchmark tasks.
IEEEFourth International Conference on SemanticComputing (ICSC), pp.
424-429Ye, P., Peyser, B., Pan, X., Boek, J., Spencer, F., Bader,J.
2005.
Gene function prediction from congruentsynthetic lethal interactions in yeast.
In Molecularsystem biologyYeh, E., Ramage, D., Manning, C., Agirre, E., Soroa, A.2009.
WikiWalk: random walks on Wikipedia forsemantic relatedness.
In Proceedings of theTextGraphs-4, Workshop on Graph-based Methodsfor Natural Language Processing, ACL2009Zesch, T., and Gurevych, I.
2007.
Analysis of theWikipedia category graph for NLP applications.
InProceedings of the TextGraphs-2 Workshop(NAACL-HLT 2007), pp.
1?8Zesch, T., Gurevych, I.
2010.
Wisdom of crowds versuswisdom of linguists: measuring the semanticrelatedness of words.
In Journal of NaturalLanguage Engineering, 16, pp.
25-59Zhang, Z., Gentile, A., Xia, L., Iria, J., Chapman, S.2010.
A random graph walk based approach tocompute semantic relatedness using knowledge fromWikipedia.
In Proceedings of LREC?10.1002
