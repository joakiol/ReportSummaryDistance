Exper ience  w i th  a S tack  Decoder -Based  HMM CSRand  Back-Of f  N -Gram Language Mode ls  IDouglas B. PaulLincoln Laboratory,  M ITLexington,  Ma.
02173ABSTRACTStochastic language models are more useful than non-stochastic models because they contribute more informationthan a simple acceptance or rejection of a word sequence.Back-off N-gram language models\[ I l l  are an effective classof word based stochastic language model.
The first partof this paper describes our experiences using the back-offlanguage models in our time-synchronous decoder CSR.
Abigram back-off language model was chosen for the languagemodel to be used in the informal ATIS CSR baseline val-uation test\[13, 21\].The stack decoder\[2, 8  24\] is a promising control struc-ture for a speech understanding system because it can com-bine constraints from both the acoustic model and a longspan language model (such as a natural anguage processor(NLP)) into a single integrated search\[17\], h copy of theLincoln time-synchronous HMM CSR has been convertedto a stack decoder controlled search with stochastic lan-guage models.
The second part of this paper describes ourexperiences with our prototype stack decoder CSR usingno grammar, the word-pair grammar, and N-gram back-offlanguage models.N-GRAM BACK-OFF  LANGUAGEMODELSN-gram language models\[2, 10\] are an attractive methodfor estimating the probability of the sentence W by succes-sively estimating the probability of the next word in thesentence:p(w) = Hp(wd~_N,..., w~_,)iwhere N is the order of the model.
They are easily com-puted, highly effective in reducing the perplexity of therecognition task, and the probabilities can be estimatedfrom observed text.
They also have the advantage thatthey are purely data-driven and therefore can be trained ondatabases which are too large for human inspection.
The1This work was sponsored by the Defense Advanced ResearchProjects Agency.maximum likelihood (ML) estimate of the conditional prob-abilities is:C(w~-N .... , wi)p(wd~,-N .
.
.
.
.
~,-1) = c(w~-N+l, .
.
.
,  ~)where C is the number of times the gram was observedin the training data.
However, the number of parameterswhich must be estimated is V N where V is the number ofwords in the vocabulary.
This l imits the N-gram models totrigram (N = 3) or lower order models due to the difficultyin estimating these probabilities from obtainable amounts oftraining data.
There are a number of methods for smooth-ing the counts or probabilities to ensure non-zero probabil-ities for unobserved grams and to smooth the estimates ofthe observed grams\[lO\].
One such method of smoothing isthe N-gram back-off models\[I l l :p(wilw,-N .... , wi-1) =C(tei_ N ...... i) if O(Wi--N, ,Wl) > coastC(t .~ i_N, .
.
.
,w i _  I )  " .
?
_ _c'(~,~-N ...... ,) if 0 < C(w i -N , .
.
,wi) < coast C(~i__N,...,Wi__l)back_o f f  if C(Wi- -N, .
.
.
,  Wi) = 0whereback_o f f  = ol( wi--N .
.
.
.
.
wi--l )p (w i lw i -N+ l .
.
.
.
, wi--1),o~ is a back-off normalization weight, C ?
is the Good-Turingcorrected\[6\] estimate of U, and const  is some constant onthe order of five.
(The Good-Turing correction generallyyields a reduced effective count.)
The method can back offrecursively until the "zero-gram" is reached.
The detailedmathematics of the Good-Turing correction and computingthe c~s may be found in reference \[11\] and will not be re-peated here.
This model is intuitively appealing because ituses the most detailed model when it can and backs off toa less detailed model when it has insufficient raining data.In addition, the Good-Turing correction both improves theprobability estimates for low probability grams and "freesup" some "probability space" to allow backing off to thenext lower order model.
In particular, some probabilityspace is obtained from the unigrams for the zero-grams orunknown words, and therefore it can estimate the probabil-ity of an unknown word class.
However, the method is notwithout difficulties.284?
In essence, part of the gram space is covered by the N-gram pdf and the remainder is covered by the (back-off) N-1gram pdf.
(Thus the need for the back-off weight--the pdfover all of the gram space must sum to one.)
Therefore if thesum of the N-gram pdf for a particular left context is one, no"space" is left for the back-off.
Such a case might occur witha word pair such as "San Francisco" if it occurred more thanconst times and ~San" did not occur in any other context.The heuristic applied in the Lincoln implementation was toincrement the count on the context o break this deadlock.A similar difficulty occurs if one wishes to assign prob-abilities to known but unobserved (C = 0) words.
Theheuristic solution applied here is motivated by a commentby Katz\[ l l \ ]  to the effect that the probability of an unob-served word is similar to the probability of a word with acount of one.
Thus, any known, but unobserved words wereartificially given a unigram count of one.This, then, was how the bigram back-off model for theinformal ATIS CSR baseline valuation test\[13, 21\] was de-rived.
The basic theory with the above heuristics produceda language model with a non-zero probability for an un-known word class and non-zero probabilities for known butunobserved words.
To distribute this language model, amachine-independent text file format was designed for spec-ifying back-off models which simply lists the grams, theirprobabilities, and, for the lower order grams, their back-offweights.Text Modeling PerformanceThe N-gram back-off language models were initially testedand compared to some padded ML 2 (PML) models on sev-eral text databases as shown in Table 1.
These perplexitiesshow the effectiveness of this method when trained on lim-ited amounts of data.
Comparison between the back-off andPML models shows the back-off model never produced ahigher perplexity than the PML model, but sometimes pro-duced dramatically lower perplexities than the PML model.In one case (WSJ small, trigram), the back-off model pro-duced more than an order-of-magnitude lower perplexitythan did the PML model.Recognition PerformanceBigram back-off language models were installed in thetime-synchronous (TS) decoder\[21\].
The model order waslimited to bigram due to the structure of the TS decoder.
(The previous language models, such as the RM word-pairgrammar (WPG), were simply yes-no finite state grammars.
)Recognition results for the RM database are shown in Ta-ble 2.
(The results in Table 2 are speaker-dependent (SD)only, but SRI has confirmed several of the relations forSI models\[15\].)
The fair back-off model, compared to theWPG, produced a doubled error rate in spite of its muchlower perplexity.
To probe this effect, a cheating RM back-2The PML models are just ML models where each N-gram countis initialized with a small "count."
The appropriate bins are thensummed to produce the N-1 gram counts and the equation for MLprobabilities i used to compute the probability.
The method is sim-ple but can yield very poor probability estimates for low probabilitywords.off model, which was guaranteed not to assign an excessivelylow probability to the test data, was generated.
(This isdemonstrated by the large decrease in the perplexity foronly a small increase in the amount of training data.)
Itsrecognition performance was very similar to that of theWPG system.
BBN also observed that a classed bigrammodel of perplexity similar to that of the fair back-off modelproduced similar results to the WPG\[23\].
A pattern bigrammodel generated from the sentence patterns used to gener-ate the RM language\[22\] showed a perplexity of about 20and consistently produced lower error rates than the WPGin tests at CMU\[12\].Table 2 shows the WPG and pattern bigram modelsto produce better recognition results and the fair back-offmodel to produce poorer recognition results than wouldbe expected from the perplexity alone.
This occurred fortwo reasons: (1) the data trained models are full-branchingwhile the pattern trained models can disallow 94% of thetransitions without the possibility of error, and (2) the prob-ability estimates of the fair back-off model are "noisy" dueto the limited training data (8K sentences, from Table 1).The RM task sentence patterns\[22\] made liberal use of classes(e.g.
ship-name) in the patterns and therefore grouping thewords into similar classes in the language model is a sig-nificant source of additional information and significantlysmoothes the probability estimates.
(The back-off modelswere word based and knew nothing about the word classes.One could, of course, generate a classed back-off model.
)BBN observed a half bit reduction in the variance of theprobability estimates in a class-based model compared toa word-based model\[23\].
It is relatively simple to manuallygenerate word classes for simple tasks such as RM and ATISbut automatic methods using probabilistic classes \[8, 9\] areprobably required for more realistic natural tasks.Why then, did the class-based and the word-based bi-gram language models of the same perplexity exhibit signif-icantly different error rates?
A "noisy" model would over-estimate some probabilities and underestimate some others.Since the perplexity is a weighted geometric mean and thenoisiest components have the least weight, the noise levelwould generally have only a small effect on the perplexity.In contrast, recognition occurs on a word-by-word level andthe noisy probabilities would be expected to occasionallycause recognition errors.
In the context of a CSR whichotherwise shows a low error rate, these "occasional" ad-ditional errors could result in a significant increase in theoverall error rate.
The class smoothed (less noisy) model, ifthe classes are appropriately chosen, would produce fewerof these additional errors.The use of classes or other language model smoothingtechniques is another example of a trade-off that occursthroughout the design of speech recognizers--that of match-ing the complexity of the model to the amount of availabletraining data.
A prespecified-class classed model requiresless training data because it has fewer parameters, but be-cause it combines the words into groups, can never modelas much detail as the word-based models.
As the amountof training data is increased, there will be a point beyondwhich the word-based models will achieve a better perfor-285mance on natural tasks than the classed models.
(Of course,the word-based modeling can at best equal the performanceof a correctly-classed model if the language is truly classed.In general, one would expect only artificial languages to betruly classed.
)THE STACK DECODERA copy of the Lincoln time synchronous (TS) decoderHMM CSR\[18, 20, 21\] has been converted to a stack decoder\[2,8, 24\] using the optimal A* search\[19\].
The current pro-totype supports most of the features of the current TSrecognizer\[20, 21\].
It uses multiple observation streams, hasadaptive background model estimation, optional interwordsilences, and context dependent phone modeling.
The cur-rent implementation can be used with or without a languagemodel.
The language models are integrated using the CSR-NL interface\[17\].
Language model modules have been builtfor word-pair and bigram/tr igram back-off language mod-els.
Unlike the TS decoder implementation which is l imitedto outputting the single best sentence theory, the stack de-coder implementation can output a top-N sentence theorylist.The current prototype does not yet use tied mixtures\[4,7\] (TM).
For simplicity, it uses Top-1 observation pruningmode, which is equivalent o a discrete observation recog-nizer using observation pdfs generated by a TM trainer.
(This was done to delay dealing with the issue of cachingthe mixture sums.
The changes required to convert to aninefficient TM system are trivial.)
It also does not yet usecross-word phone modeling pending solution of several im-plementation issues.The system includes a tree-structured fast match\[l, 3\]to reduce the computation required by the detailed acous-tic match.
This fast match uses a beam-pruned TS searchof a phonetic tree built from HMM phone models to lo-cate the the possible one word extensions to the currenttheory (partial sentence).
To reduce computation, only theobservation pdfs are used--the transition probabilities areignored.
The current fast match reduces the number of pos-sible next words to about 15% of the vocabulary for the SD(RM) task using triphone models.The system has been tested in no-grammar (NG) modeusing SD triphone models on the RM task.
In some re-spects, the stack decoder does a better job than does theTS decoder.
It sometimes locates a higher probability paththrough the recognition etwork than does the TS decoderwith a reasonable pruning threshold.
(Unfortunately, thesepaths usually contain a recognition error.)
The fixed prun-ing threshold of the TS decoder terminates these paths,while the stack decoder continues to extend them.
If thepruning threshold of the TS system is increased to a valuethat would ordinarily be considered excessive, the TS sys-tem will also find these paths.
The stack decoder automat-ically finds these paths because it has, in effect, adaptivepruning which does not require any fixed thresholds.The system has also been tested using WPG and N-gramback-off language models through the CSR-NL interface.The potential search errors due to an interaction betweenthe acoustic and language models described and verified bysimulation in \[19\] have been observed on real speech data.
Infact, this search error can be caused by the word insertionpenalty alone.
(This appears to be rather infrequent-- itwas forced experimentally by using a relatively large inser-tion penalty.)
Initial checks for the search error, which usedthe WPG without probabilities, indicated that the inter-action was a minor problem.
(The WPG without proba-bilities gave better recognition results than the WPG with1 probabilities in the TS recognizer.)
branchino-\]actorIn contrast to NG and the WPG, the interaction be-comes a major problem when one of the stochastic (bigramor trigram back-off) language models is used.
A simple, butinefficient solution is to increase the tie-breaking factor inthe equation for the stack ordering criterion\[19\]:StSc,  =mtax L,(t) - l~bL(t) - etwhere Li(t) is the likelihood of theory i and lubL(t) is theleast-upper-bound-so-far on the theory likelihoods and e isthe tie-breaking factor which favors shorter over longer the-ories.
(For the NG case, ?
need only be a very small numbergreater than zero.)
This is a poor solution because a largevalue of ?
will also greatly increase the computation.
Whena sufficiently large value of ?
is used, the stack decoder func-tions as expected with either the trigram or bigram back-offlanguage models.
Another possible solution is to run the de-coder in top-N mode and select he best sentence after somenumber of sentences has been output.
This approach alsosignificantly increases the computation.It appears likely that the interaction problem is due tocombining two fundamentally different ypes of score (log-probabilities) together.
The HMM observation and transi-tion probabilities (i.e.
the acoustic scores) are accumulatedas a function of time and the language model probabilitiesare accumulated as a function of the number of states tra-versed.
The mixture of these dissimilar scores appears tobe damaging the estimation of the least upper bound usedto perform the A* search\[16\] in the stack decoder.On the average, the current prototype stack decoderruns significantly faster than does the TS decoder on a SDno-grammar task.
This is probably due to the adaptive-pruning-threshold like behavior of the stack decoder whichallows it to pursue the minimum number of theories requiredto decode the sentences--a small number on the "easy"sentences and a larger number on the "harder" sentences,whereas the TS decoder must always use a worst-case prun-ing threshold.
With the word-pair grammar, the two sys-tems run at similar speeds.
The strategies proposed abovefor combating the interaction problem with the stochasticlanguage models low the stack decoder sufficiently to causeit to run to significantly slower than the TS decoder.
Thestrategies also increase the number of theories which mustbe held on the stack.CONCLUSIONStochastic language models will be an important com-ponent in future speech recognition and understanding sys-tems.
The N-gram language models are a class of modelwhich is easily trained from observed data and provides286significant constraints to the recognition process and weretherefore chosen for use in the informal ATIS CSR base-line evaluation test.
The required number of parameters,however, is too large to be trained from practical amountsof data.
Backing off to lower order models to estimate theprobability of unobserved N-grams is an effective methodfor dealing with finite training data.
The fact that thesemodels are purely data driven is both an advantage anda disadvantage--they arefree from often erroneous humanbias, but also cannot incorporate human knowledge.
Onemethod of incorporating human knowledge in limited tasksis to smooth the probability estimates by grouping the wordsinto human-defined classes and estimating the language modelon the classes.The stack decoder is an attractive control strategy fora speech understanding system because it can combine in-formation from the acoustic matching and any of a varietyof language models/natural language systems into a sin-gle integrated search.
The current prototype is not matureenough to use in a practical recognition/understanding sys-tem, but is showing promise.
The no-grammar recognitionworks fairly well--but no-grammar recognition is not thegoal.
The goal of the effective integration of the languagemodel and the acoustic modeling has not yet been achieveddue to the interaction between the two knowledge sourcespreventing estimation of the proper least upper bound ofthe theory likelihoods.
Once this problem is overcome, thestack decoder should become apractical structure for speechrecognition.REFERENCES1.
A. Averbuch, L. Bahl, R. Bakis, P. Brown, A. Cole,G.
Daggett, S. Das, K. Davies, S. De Gennaro, P. deSouza, E. Epstein, D. Fraleigh, F. Jelinek, S. Katz,B.
Lewis, R. Mercer, A. Nadas, D. Nahamoo, M.Picheny, G. Shichman, and P. SpineUi, "An IBM-PC Based Large-Vocabulary Isolated-Utterance SpeechRecognizer," Proc.
ICASSP 86, Tokyo, April 1986.2.
L. R. Bahl, F. Jelinek, and R. L. Mercer, "A MaximumLikelihood Approach to Continuous Speech Recogni-tion," IEEE Trans.
Pattern Analysis and Machine In-telligence, PAMI-5, March 1983.3.
L. R. Bahl, P. S. Gopalakrishnam, D. Kanevsky, D. Na-hamoo, "Matrix Fast Match: A Fast Method for Iden-tifying a Short List of Candidate Words for Decoding,"Proc.
ICASSP 89, Glasgow, May 1989.4.
J.R. Bellegarda nd D.H. Nahamoo, "Tied MixtureContinuous Parameter Models for Large VocabularyIsolated Speech Recognition," Proc.
ICASSP 89, Glas-gow, May 1989.5.
A. Derr and R. Schwartz, "A Simple Statistical ClassGrammar for Measuring Speech Recognition Perfor-mance," Proceedings October, 1989 DARPA Speechand Natural Language Workshop, Morgan KaufmannPublishers, October, 1989.6.
I. J.
Good, "The Population Frequencies of Species andthe Estimation of Population Parameters," Biometrika,vol.
40, no.3 mad 4, 1953.7.
X. D. Huang and M.A.
Jack, "Semi-continuous Hid-den Markov Models for Speech Recognition," Com-puter Speech and Language, Vol.
3, 1989.8.
F. Jelinek, "A Fast Sequential Decoding Algorithm Us-ing a Stack," IBM J. Res.
Develop., vol.
13, November1969.9.
F. Jelinek, R. Mercer, and S. Roucos, "ClassifyingWords for Improved Statistical Language Models,"Proc.
ICASSP 90, Albuquerque, NM, April 1990.10.
F. Jelinek, "Self-Organized Language Modeling forSpeech Recognition," in Readin9 s in Speech Recogni-tion, A. Weibel and K. F. Lee, ed., Morgan KaufmannPublishers, 1990.11.
S. M. Katz, "Estimation of Probabilities from SparseData for the Language Model Component of ~ SpeechRecognizer," ASSP-35, pp 400-401, March 1987.12.
K. F. Lee, Automatic Speech Recognition: The Develop-ment of the SPHINX System, Kluwer Academic Pub-lishers, Norwell, MA, 1989.13.
F. Kubala, S. Austin, C. Barry, J. Makhoul, P.Plaeeway, R. Schwartz, "BYBLOS Speech RecognitionBenchmark Results," Proc.
DARPA Speech and Nat-ural Language Workshop, Morgan Kaufmann Publish-ers, Feb. 1991.14.
M. Liberman, "Text on Tap: the ACL/DCI," Proceed-ings October, 1989 DARPA Speech and Natural Lan-guage Workshop, Morgan Kaufmann Publishers, Octo-ber, 1989.15.
H. Murveit, personal communication.16.
N. J. Nilsson, "Problem-Solving Methods of ArtificialIntelligence," McGraw-Hill, New York, 1971.17.
D. B. Paul, "A CSR-NL Interface Specification," Pro-ceedings October, 1989 DARPA Speech and NaturalLanguage Workshop, Morgan Kaufmann Publishers,October, 1989.18.
D. B. Paul, "Speech Recognition using Hidden MarkovModels," Lincoln Laboratory Journal, Vol.
3, no.
1,Spring 1990.19.
D. B. Paul, "Algorithms for an Optimal A* Searchand Linearizing the Search in the Stack Decoder,"Proc.
DARPA Speech and Natural Language Work-shop, Morgan Kaufmann Publishers, June 1990.20.
D. B. Paul, "The Lincoln Tied-Mixture HMM Contin-uous Speech Recognizer," Proc.
DARPA Speech andNatural Language Workshop, Morgan Kaufmann Pub-lishers, June 1990.21.
D. B. Paul, "New Results with the Lincoln Tied-Mixture HMM CSR System," Proc.
DARPA Speechand Natural Language Workshop, Morgan KaufmannPublishers, Feb. 1991.22.
P. Price, W. fischer, J. Bernstein, and D. Pallett, "TheDARPA 1000-Word Resource Management Databasefor Continuous Speech Recognition," Proc.
ICASSP 88,New York, April 1988.23.
R. Schwartz, personal communication.24.
D. G. Sturtevant, "A Stack Decoder for ContinuousSpeech Recognition," Proc.
DARPA Speech and Nat-ural Language Workshop, Morgan Kaufmann Publish-ers, Oct. 1989.287Table  1: Perplexities of N-gram Back-off (BO) and PML Language ModelsPerplexityI 1-gram 2-gram 3-gram TrainingDatabase i BO \[ PML* BO \] PML* BO PML* WordsATIS, June 90 I 114 122 24 28 18 35 8600ATIS, baseline 125 18 - 13 45KRM, fair I 258 258 26 27 14 25 71KRM, cheat 254 16 - 6 89KWSJ, small 715 1608 365 1512 274 3926 130KWSJ, large 1215 1334 287 492 172 1541 4.8MTrainingVocab.UnknownTest Words552 1.3%1065 .2%991 0%991 0%13K 6.9%64K .8%* pad chosen to minimize test set perplexityJune 90 = June 90 ATIS training databaseline = "baseline" ATIS training data, see belowRM, fair = trained on RM1 + RM2 training dataRM, cheat = trained on "fair" + test dataWSJ = Wall Street Journal sampler\[14\]Table  2: RM SD Bigram Language Model Recognition ResultsI Trained i Modeling Test SetGrammar type \] fromWPG (cheat)back-off fair Iback-off cheat dataclass bigram\[5\] fair datapattern bigram (cheat) i patternsUnit Perplexity Word Errpatterns\[22\] word 60 1.7%data word 26 3.0%word 1624 class1.5%~WPG\[23\]\[22\] word 20112\] ~.7*WPG\[12\]Note: A "fair" language model is not trained on the test data, a "cheating" model is trained on the test data.
Therefore, thepattern trained models must be classed as cheating because they were trained on the entire RM language--which includesthe test data.288
