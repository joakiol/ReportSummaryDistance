Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 171?178,Vancouver, October 2005. c?2005 Association for Computational LinguisticsImproving Parsing Accuracy by Combining DiverseDependency ParsersDaniel Zeman and Zden k ?abokrtsk?
?stav form?ln?
a aplikovan?
lingvistiky, Univerzita KarlovaMalostransk?
n?m st?
25, CZ-11800  Praha{zeman|zabokrtsky}@ufal.mff.cuni.czAbstractThis paper explores the possibilities ofimproving parsing results by combiningoutputs of several parsers.
To some ex-tent, we are porting the ideas of Hender-son and Brill (1999) to the world ofdependency structures.
We differ fromthem in exploring context features moredeeply.
All our experiments were con-ducted on Czech but the method is lan-guage-independent.
We were able tosignificantly improve over the best pars-ing result for the given setting, known sofar.
Moreover, our experiments show thateven parsers far below the state of the artcan contribute to the total improvement.1 IntroductionDifficult and important NLP problems have theproperty of attracting whole range of researchers,which often leads to the development of severaldifferent approaches to the same problem.
If theseapproaches are independent enough in terms of notproducing the same kinds of errors, there is a hopethat their combination can bring further improve-ment to the field.
While improving any single ap-proach gets more and more difficult once somethreshold has been touched, exploring the potentialof approach combination should never be omitted,provided three or more approaches are available.Combination techniques have been successfullyapplied to part of speech tagging (van Halteren etal., 1998; Brill and Wu, 1998; van Halteren et al,2001).
In both cases the investigators were able toachieve significant improvements over the previ-ous best tagging results.
Similar advances havebeen made in machine translation (Frederking andNirenburg, 1994), speech recognition (Fiscus,1997), named entity recognition (Borthwick et al,1998), partial parsing (Inui and Inui, 2000), wordsense disambiguation (Florian and Yarowsky,2002) and question answering (Chu-Carroll et al,2003).Brill and Hladk?
(Haji  et al, 1998) have firstexplored committee-based dependency parsing.However, they generated multiple parsers from asingle one using bagging (Breiman, 1994).
Therehave not been more sufficiently good parsersavailable.
A successful application of voting and ofa stacked classifier to constituent parsing followedin (Henderson and Brill, 1999).
The authors haveinvestigated two combination techniques (constitu-ent voting and na?ve Bayes), and two ways of theirapplication to the (full) parsing: parser switching,and similarity switching.
They were able to gain1.6 constituent F-score, using their most successfultechnique.In our research, we focused on dependency pars-ing.
One of the differences against Henderson andBrill?s situation is that a dependency parser has toassign exactly one governing node (parent word) toeach word.
Unlike the number of constituents inconstituency-based frameworks, the number ofdependencies is known in advance, the parser onlyhas to assign a link (number 0 through N) to eachword.
In that sense, a dependency parser is similarto classifiers like POS taggers.
Unless it deliber-ately fails to assign a parent to a word (or assigns171several alternate parents to a word), there is noneed for precision & recall.
Instead, a single metriccalled accuracy is used.On the other hand, a dependency parser is not areal classifier: the number of its ?classes?
is theo-retically unlimited (natural numbers), and no gen-eralization can be drawn about objects belongingto the same ?class?
(words that ?
sometimes ?
ap-peared to find their parent at the position i).A combination of dependency parsers does notnecessarily grant the resulting dependency struc-ture being cycle-free.
(This contrasts to not intro-ducing crossing brackets in constituent parsing,which is granted according to Henderson andBrill.)
We address the issue in 4.4.The rest of this paper is organized as follows: inSections 2 and 3 we introduce the data and thecomponent parsers, respectively.
In Section 4 wediscuss several combining techniques, and in Sec-tion 5 we describe the results of the correspondingexperiments.
We finally compare our results to theprevious work and conclude.2 The dataTo test our parser combination techniques, we usethe Prague Dependency Treebank 1.0 (PDT; Hajiet al 2001).
All the individual parsers have beentrained on its analytical-level training section(73,088 sentences; 1,255,590 tokens).The PDT analytical d-test section has been parti-tioned into two data sets, Tune (last 77 files; 3646sentences; 63,353 tokens) and Test (first 76 files;3673 sentences; 62,677 tokens).
We used the Tuneset to train the combining classifiers if needed.
TheTest data were used to evaluate the approach.
Nei-ther the member parsers, nor the combining classi-fier have seen this data set during their respectivelearning runs.3 Component parsersThe parsers involved in our experiments are sum-marized in Table 1.
Most of them use uniquestrategies, the exception being thl and thr, whichdiffer only in the direction in which they processthe sentence.The table also shows individual parser accura-cies on our Test data.
There are two state-of-the artparsers, four not-so-good parsers, and one quitepoor parser.
We included the two best parsers(ec+mc) in all our experiments, and tested the con-tributions of various selections from the rest.The necessary assumption for a meaningfulcombination is that the outputs of the individualparsers are sufficiently uncorrelated, i.e.
that theparsers do not produce the same errors.
If someAccuracy Par-serAuthor Brief descriptionTune TestecEugeneCharniakA maximum-entropy inspired parser, home in constituency-basedstructures.
English version described in Charniak (2000), Czech ad-aptation 2002 ?
2003, unpublished.83.6 85.0mcMichaelCollinsUses a probabilistic context-free grammar, home in constituency-based structures.
Described in (Haji  et al, 1998; Collins et al,1999).81.7 83.3z?
Zden k ?abokrtsk?Purely rule-based parser, rules are designed manually, just a few lexi-cal lists are collected from the training data.
2002, unpublished.
74.3 76.2dz Daniel ZemanA statistical parser directly modeling syntactic dependencies as wordbigrams.
Described in (Zeman, 2004).
73.8 75.5thr 71.0 72.3thl 69.5 70.3thpTom?
?HolanThree parsers.
Two of them use a sort of push-down automata anddiffer from each other only in the way they process the sentence (left-to-right or right-to-left).
Described in (Holan, 2004).
62.0 63.5Table 1.
A brief description of the tested parsers.
Note that the Tune data is not the data used to train theindividual parsers.
Higher numbers in the right column reflect just the fact that the Test part is slightlyeasier to parse.172parsers produced too similar results, there wouldbe the danger that they push all their errorsthrough, blocking any meaningful opinion of theother parsers.To check the assumption, we counted (on theTune data set) for each parser in a given parser se-lection the number of dependencies that only thisparser finds correctly.
We show the results in Ta-ble 2.
They demonstrate that all parsers are inde-pendent on the others at least to some extent.4 Combining techniquesEach dependency structure consists of a number ofdependencies, one for each word in the sentence.Our goal is to tell for each word, which parser isthe most likely to pick its dependency correctly.By combining the selected dependencies we aim atproducing a better structure.
We call the complexsystem (of component parsers plus the selector) thesuperparser.Although we have shown how different strate-gies lead to diversity in the output of the parsers,there is little chance that any parser will be able topush through the things it specializes in.
It is verydifficult to realize that a parser is right if most ofthe others reject its proposal.
Later in this sectionwe assess this issue; however, the real power is inmajority of votes.4.1 VotingThe simplest approach is to let the member parsersvote.
At least three parsers are needed.
If there areexactly three, only the following situations reallymatter: 1) two parsers outvote the third one; 2) atie: each parser has got a unique opinion.
It wouldbe democratic in the case of a tie to select ran-domly.
However, that hardly makes sense once weknow the accuracy of the involved parsers on theTune set.
Especially if there is such a large gapbetween the parsers?
performance, the best parser(here ec) should get higher priority whenever thereParsers compared All 7 4 best 3 best ec+mc+dz 2 best 3 worstWho is correct How many times correctec 1.7 % 3.0 % 4.1 % 4.5 % 8.1 %z?
1.2 % 2.0 % 3.3 %mc 0.9 % 1.7 % 2.7 % 2.9 % 6.2 %thr 0.4 %     4.9 %thp 0.4 %     4.4 %dz 0.3 % 1.0 %  2.2 %a single parser(all other wrong)thl 0.3 %     4.3 %all seven parsers 42.5 %at least six 58.1 %at least five 68.4 %at least four 76.8 % 58.0 %at least three 84.0 % 75.1 % 63.6 % 64.7 %  50.6 %at least two 90.4 %  82.9 % 82.4 % 75.5 % 69.2 %at least one 95.8 % 94.0 % 93.0 % 92.0 % 89.8 % 82.7 %Table 2: Comparison of various groups of parsers.
All percentages refer to the share of the total words intest data, attached correctly.
The ?single parser?
part shows shares of the data where a single parser is theonly one to know how to parse them.
The sizes of the shares should correlate with the uniqueness of theindividual parsers?
strategies and with their contributions to the overall success.
The ?at least?
rows giveclues about what can be got by majority voting (if the number represents over 50 % of parsers compared)or by hypothetical oracle selection (if the number represents 50 % of the parsers or less, an oracle wouldgenerally be needed to point to the parsers that know the correct attachment).173is no clear majority of votes.
Van Halteren et al(1998) have generalized this approach for highernumber of classifiers in their TotPrecision votingmethod.
The vote of each classifier (parser) isweighted by their respective accuracy.
For in-stance, mc + z?
would outvote ec + thr, as 81.7 +74.3 = 156 > 154.6 = 83.6 + 71.0.4.2 StackingIf the world were ideal, we would have an oracle,able to always select the right parser.
In such situa-tion our selection of parsers would grant the accu-racy as high as 95.8 %.
We attempt to imitate theoracle by a second-level classifier that learns fromthe Tune set, which parser is right in which situa-tions.
Such technique is usually called classifierstacking.
Parallel to (van Halteren et al, 1998), weran experiments with two stacked classifiers,Memory-Based, and Decision-Tree-Based.
Thisapproach roughly corresponds to (Henderson andBrill, 1999)?s Na?ve Bayes parse hybridization.4.3 Unbalanced combiningFor applications preferring precision to recall, un-balanced combination ?
introduced by Brill andHladk?
in (Haji  et al, 1998) ?
may be of inter-est.
In this method, all dependencies proposed byat least half of the parsers are included.
The termunbalanced reflects the fact that now precision isnot equal to recall: some nodes lack the link totheir parents.
Moreover, if the number of memberparsers is even, a node may get two parents.4.4 SwitchingFinally, we develop a technique that considers thewhole dependency structure rather than each de-pendency alone.
The aim is to check that the result-ing structure is a tree, i.e.
that the dependency-selecting procedure does not introduce cycles.1Henderson and Brill prove that under certain con-ditions, their parse hybridization approach cannot1One may argue that ?treeness?
is not a necessary conditionfor the resulting structure, as the standard accuracy measuredoes not penalize non-trees in any way (other than that there isat least one bad dependency).
Interestingly enough, even someof the component parsers do not produce correct trees at alltimes.
However, non-trees are both linguistically and techni-cally problematic, and it is good to know how far we can getwith the condition in force.introduce crossing brackets.
This might seem ananalogy to our problem of introducing cycles ?but unfortunately, no analogical lemma holds.
As aworkaround, we have investigated a crossbreedapproach between Henderson and Brill?s ParserSwitching, and the voting methods describedabove.
After each step, all dependencies that wouldintroduce a cycle are banned.
The algorithm isgreedy ?
we do not try to search the space of de-pendency combinations for other paths.
If there areno allowed dependencies for a word, the wholestructure built so far is abandoned, and the struc-ture suggested by the best component parser isused instead.25 Experiments and results5.1 VotingWe have run several experiments where variousselections of parsers were granted the voting right.In all experiments, the TotPrecision voting schemeof (van Halteren et al, 1998) has been used.
Thevoting procedure is only very moderately affectedby the Tune set (just the accuracy figures on thatset are used), so we present results on both the Testand the Tune sets.Accuracy Voters Tune Testec (baseline) 83.6 85.0all seven 84.0 85.4ec+mc+dz 84.9 86.2all but thp 84.9 86.3ec+mc+z?+dz+thr 85.1 86.5ec+mc+z?
85.2 86.7ec+mc+z?+dz 85.6 87.0Table 3: Results of voting experiments.According to the results, the best voters poolconsists of the two best parsers, accompanied by2We have not encountered such situation in our test data.However, it indeed is possible, even if all the component pars-ers deliver correct trees, as can be seen from the followingexample.
Assume we have a sentence #ABCD and parsers P1(85 votes), P2 (83 votes), P3 (76 votes).
P1 suggests the treeA?D?B?C?#, P2 suggests B?D?A?C?#, P3 suggestsB?D?A?#, C?#.
Then the superparser P gradually intro-duces the following dependencies: 1.
A?D; 2.
B?D;3.
C?#; 4.
D?A or D?B possible but both lead to a cycle.174the two average parsers.
The table also suggeststhat number of diverse strategies is more importantthan keeping high quality standard with all theparsers.
Apart from the worst parser, all the othertogether do better than just the first two and thefourth.
(On the other hand, the first three parsersare much harder to beat, apparently due to the ex-treme distance of the strategy of z?
parser from allthe others.
)Even the worst performing parser combination(all seven parsers) is significantly3 better than thebest component parser alone.We also investigated some hand-invented votingschemes but no one we found performed betterthan the ec+mc+z?+dz combination above.Some illustrative results are given in the Ta-ble 4.
Votes were not weighted by accuracy inthese experiments, but accuracy is reflected in thepriority given to ec and mc by the human schemeinventor.Accuracy Voters Selectionscheme Tune Testall seven most votesor ec82.8 84.3all sevenat leasthalf, or ecif there isno absolutemajority84.4 85.8all sevenabsolutemajority,or ec+2, ormc+2, orec84.6 85.9Table 4: Voting under hand-invented schemes.5.2 Stacking ?
using contextWe explored several ways of using context inpools of three parsers.4 If we had only three parserswe could use context to detect two kinds of situa-tions:3All significance claims refer to the Wilcoxon Signed RanksTest at the level of p = 0.001.4Similar experiments could be (and have been) run for sets ofmore parsers as well.
However, the number of possible fea-tures is much higher and the data sparser.
We were not able togain more accuracy on context-sensitive combination of moreparsers.1.
Each parser has its own proposal and aparser other than ec shall win.2.
Two parsers agree on a common pro-posal but even so the third one shouldwin.
Most likely the only reasonable in-stance is that ec wins over mc + thethird one.?Context?
can be represented by a number offeatures, starting at morphological tags and endingup at complex queries on structural descriptions.We tried a simple memory-based approach, and amore complex approach based on decision trees.Within the memory-based approach, we use justthe core features the individual parsers themselvestrain on: the POS tags (morphological tags or m-tags in PDT terminology).
We consider the m-tagof the dependent node, and the m-tags of the gov-ernors proposed by the individual parsers.We learn the context-based strengths and weak-nesses of the individual parsers on their perform-ance on the Tune data set.
In the following table,there are some examples of contexts in which ec isbetter than the common opinion of mc + dz.Dep.tagGov.tag(ec)ContextoccurrencesNo.
oftimesec wasrightPercentcases ecwasrightJ^ # 67 44 65.7Vp J^ 53 28 52.8VB J^ 46 26 56.5N1 Z, 38 21 55.3Rv Vp 25 13 52.0Z, Z, 15 8 53.3A1 N1 15 8 53.3Vje J^ 14 9 64.3N4 Vf 12 9 75.0Table 5: Contexts where ec is better than mc+dz.J^ are coordination conjunctions, # is the root, V*are verbs, Nn are nouns in case n, R* are preposi-tions, Z* are punctuation marks, An are adjectives.For the experiment with decision trees, we usedthe C5 software package, a commercial version ofthe well-known C4.5 tool (Quinlan, 1993).
Weconsidered the following features:For each of the four nodes involved (the de-pendent and the three governors suggested by thethree component parsers):175?
12 attributes derived from the morpho-logical tag (part of speech, subcategory,gender, number, case, inner gender, in-ner number, person, degree of compari-son, negativeness, tense and voice)?
4 semantic attributes (such as Proper-Name, Geography etc.
)For each of the three governor-dependent pairsinvolved:?
mutual position of the two nodes (Left-Neighbor, RightNeighbor, LeftFar,RightFar)?
mutual position expressed numerically?
for each parser pair a binary flagwhether they do or do not share opin-ionsThe decision tree was trained only on situationswhere at least one of the three parsers was rightand at least one was wrong.Voters Scheme Accuracyec+mc+dz context free 86.2ec+mc+dz memory-based 86.3ec+mc+z?
context free 86.7ec+mc+z?
decision tree 86.9Table 6: Context-sensitive voting.
Contexts trainedon the Tune data set, accuracy figures apply to theTest data set.
Context-free results are given for thesake of comparison.It turns out that there is very low potential in thecontext to improve the accuracy (the improvementis significant, though).
The behavior of the parsersis too noisy as to the possibility of formulatingsome rules for prediction, when a particular parseris right.
C5 alone provided a supporting evidencefor that hypothesis, as it selected a very simple treefrom all the features, just 5 levels deep (see Fig-ure 1).Henderson and Brill (1999) also reported thatcontext did not help them to outperform simplevoting.
Although it is risky to generalize these ob-servations for other treebanks and parsers, our en-vironment is quite different from that of Hendersonand Brill, so the similarity of the two observationsis at least suspicious.5.3 Unbalanced combiningFinally we compare the balanced and unbalancedmethods.
Expectedly, precision of the unbalancedcombination of odd number of parsers rose whilerecall dropped slightly.
A different situation is ob-served if even number of parsers vote and morethan one parent can be selected for a node.
In suchcase, precision drops in favor of recall.Method Precision Recall F-measureec only(baseline) 85.0balanced(all seven) 85.4unbalanced(all seven) 90.7 78.6 84.2balanced(best four) 87.0unbalanced(best four) 85.4 87.7 86.5balanced(ec+mc+dz) 86.2unbalanced 89.5 84.0 86.7agreezzmc = yes: zz (3041/1058)agreezzmc = no::...agreemcec = yes: ec (7785/1026)agreemcec = no::...agreezzec = yes: ec (2840/601)agreezzec = no::...zz_case = 6: zz (150/54)zz_case = 3: zz (34/10)zz_case = X: zz (37/20)zz_case = undef: ec (2006/1102)zz_case = 7: zz (83/48)zz_case = 2: zz (182/110)zz_case = 4: zz (108/57)zz_case = 1: ec (234/109)zz_case = 5: mc (1)zz_case = root::...ec_negat = A: mc (117/65)ec_negat = undef: ec (139/65)ec_negat = N: ec (1)ec_negat = root: ec (2)Figure 1.
The decision tree for ec+mc+z?,learned by C5.
Besides pairwise agreement be-tween the parsers, only morphological case andnegativeness matter.176Method Precision Recall F-measure(ec+mc+dz)balanced(ec+mc+z?)
86.7unbalanced(ec+mc+z?)
90.2 84.7 87.3Table 7: Unbalanced vs. balanced combining.
Allruns ignored the context.
Evaluated on the Testdata set.5.4 SwitchingOut of the 3,673 sentences in our Test set, 91.6 %have been rendered as correct trees in the balanceddecision-tree based stacking of ec+mc+z?+dz (ourbest method).After we banned cycles, the accuracy droppedfrom 97.0 to 96.9 %.6 Comparison to related workBrill and Hladk?
in (Haji  et al, 1998) were able toimprove the original accuracy of the mc parser onPDT 0.5 e-test data from 79.1 to 79.9 (a nearly 4%reduction of the error rate).
Their unbalanced5 vot-ing pushed the F-measure from 79.1 to 80.4 (6%error reduction).
We pushed the balanced accuracyof the ec parser from 85.0 to 87.0 (13% error re-duction), and the unbalanced F-measure from 85.0to 87.7 (18% reduction).
Note however that therewere different data and component parsers (Hajiet al found bagging the best parser better thancombining it with other that-time-available pars-ers).
This is the first time that several strategicallydifferent dependency parsers have been combined.
(Henderson and Brill, 1999) improved their bestparser?s F-measure of 89.7 to 91.3, using their na-?ve Bayes voting on the Penn TreeBank constituentstructures (16% error reduction).
Here, even theframework is different, as has been explainedabove.7 ConclusionWe have tested several approaches to combining ofdependency parsers.
Accuracy-aware voting of thefour best parsers turned out to be the best method,as it significantly improved the accuracy of thebest component from 85.0 to 87.0 % (13 % error5Also alternatively called unrestricted.rate reduction).
The unbalanced voting lead to theprecision as high as 90.2 %, while the F-measureof 87.3 % outperforms the best result of balancedvoting (87.0).At the same time, we found that employing con-text to this task is very difficult even with a well-known and widely used machine-learning ap-proach.The methods are language independent, thoughthe amount of accuracy improvement may varyaccording to the performance of the available pars-ers.Although voting methods are themselves notnew, as far as we know we are the first to proposeand evaluate their usage in full dependency pars-ing.8 AcknowledgementsOur thanks go to the creators of the parsers usedhere for making their systems available.The research has been supported by the CzechAcademy of Sciences, the ?Information Society?program, project No.
1ET101470416.ReferencesAndrew Borthwick, John Sterling, Eugene Agichtein,Ralph Grishman.
1998.
Exploiting Diverse Knowl-edge Sources via Maximum Entropy in Named EntityRecognition.
In: Eugene Charniak (ed.
): Proceedingsof the 6th Workshop on Very Large Corpora, pp.152?160.
Universit?
de Montr?al, Montr?al, Qu?bec.Leo Breiman.
1994.
Bagging Predictors.
Technical Re-port 421, Department of Statistics, University ofCalifornia at Berkeley, Berkeley, California.Eric Brill, Jun Wu.
1998.
Classifier Combination forImproved Lexical Combination.
In: Proceedings ofthe 17th International Conference on ComputationalLinguistics (COLING-98), pp.
191?195.
Universit?de Montr?al, Montr?al, Qu?bec.Eugene Charniak.
2000.
A Maximum-Entropy-InspiredParser.
In: Proceedings of NAACL.
Seattle, Wash-ington.Jennifer Chu-Carroll, Krzysztof Czuba, John Prager,Abraham Ittycheriah.
2003.
In Question Answering,Two Heads Are Better Than One.
In: Proceedings ofthe HLT-NAACL.
Edmonton, Alberta.Michael Collins, Jan Haji   , Eric Brill, Lance Ramshaw,Christoph Tillmann.
1999.
A Statistical Parser ofCzech.
In: Proceedings of the 37th Meeting of the177ACL, pp.
505?512.
University of Maryland, CollegePark, Maryland.Jonathan G. Fiscus.
1997.
A Post-Processing System toYield Reduced Word Error Rates: Recognizer OutputVoting Error Reduction (ROVER).
In: EuroSpeech1997 Proceedings, vol.
4, pp.
1895?1898.
Rodos,Greece.Radu Florian, David Yarowsky.
2002.
Modeling Con-sensus: Classifier Combination for Word Sense Dis-ambiguation.
In: Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), pp.
25?32.
Philadelphia, Pennsylvania.Robert Frederking, Sergei Nirenburg.
1994.
ThreeHeads Are Better Than One.
In: Proceedings of the4th Conference on Applied Natural Language Proc-essing, pp.
95?100.
Stuttgart, Germany.Jan Haji   , Eric Brill, Michael Collins, Barbora Hladk?,Douglas Jones, Cynthia Kuo, Lance Ramshaw, OrenSchwartz, Christoph Tillmann, Daniel Zeman.
1998.Core Natural Language Processing Technology Ap-plicable to Multiple Languages.
The Workshop 98Final Report.
http://www.clsp.jhu.edu/ws98/projects/nlp/report/.
Johns Hopkins University, Baltimore,Maryland.Jan Haji   , Barbora Vidov?
Hladk?, Jarmila Panevov?,Eva Haji   ov?, Petr Sgall, Petr Pajas.
2001.
PragueDependency Treebank 1.0 CD-ROM.
Catalog #LDC2001T10, ISBN 1-58563-212-0.
Linguistic DataConsortium, Philadelphia, Pennsylvania.Hans van Halteren, Jakub Zav  el, Walter Daelemans.1998.
Improving Data-Driven Wordclass Tagging bySystem Combination.
In: Proceedings of the 17th In-ternational Conference on Computational Linguistics(COLING-98), pp.
491?497.
Universit?
de Montr?al,Montr?al, Qu?bec.Hans van Halteren, Jakub Zav  el, Walter Daelemans.2001.
Improving Accuracy in Word Class Taggingthrough the Combination of Machine Learning Sys-tems.
In: Computational Linguistics, vol.
27, no.
2,pp.
199?229.
MIT Press, Cambridge, Massachusetts.John C. Henderson, Eric Brill.
1999.
Exploiting Diver-sity in Natural Language Processing: CombiningParsers.
In: Proceedings of the Fourth Conference onEmpirical Methods in Natural Language Processing(EMNLP-99), pp.
187?194.
College Park, Maryland.Tom??
Holan.
2004.
Tvorba z?vislostn?ho syntaktick?hoanalyz?toru.
In: David Obdr?
?lek, Jana Teskov?(eds.
): MIS 2004 Josef v D  l, Sborn?k semin?  e.Matfyzpress, Praha, Czechia.Inui Takashi, Inui Kentaro.
2000.
Committee-BasedDecision Making in Probabilistic Partial Parsing.
In:Proceedings of the 18th International Conference onComputational Linguistics (COLING 2000), pp.348?354.
Universit?t des Saarlandes, Saarbr?cken,Germany.J.
Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo, California.Daniel Zeman.
2004.
Parsing with a Statistical Depend-ency Model (PhD thesis).
Univerzita Karlova, Praha,Czechia.178
