Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 16?23,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsPart-of-Speech Tagging with a Symbolic Full Parser:Using the TIGER Treebank to Evaluate FipsYves ScherrerLanguage Technology Laboratory (LATL)University of Geneva1211 Geneva 4, Switzerlandyves.scherrer@lettres.unige.chAbstractIn this paper, we introduce the German ver-sion of the multilingual Fips parsing system.We focus on the evaluation of its part-of-speech tagging component with the help of theTIGER treebank.
We explain how Fips can beadapted to the tagset used by TIGER and re-port first results of this study: currently, 87%of words are tagged correctly.
We also discusssome common errors and explore a possibleextension of this study to parsing.1 IntroductionFips is a parsing framework based on the main as-sumptions of Chomsky?s generative linguistics.
Ithas been designed as a multilingual framework,making it easy to add new languages.
Currently, itis available for six languages (English, French, Ger-man, Italian, Spanish and Greek).
While the Frenchversion (providing the best coverage) has taken partin evaluation campaigns (Adda et al, 1998; Gold-man et al, 2005), the other language modules haveonly been subject to internal qualitative evaluation.However, the availability of gold standard treebanksallows for quantitative evaluation of rule-based pars-ing systems.
In particular, we propose to use theTIGER treebank for the evaluation of the Germanversion of Fips.This paper reports on research in progress.
As apreliminary step towards a quantitative assessmentof parser performance, we focus on the task of Part-of-Speech (POS) tag comparison here.
This task isintended to yield a first appreciation of the quality ofthe German Fips component without having to dealwith the full parser output and its possible incom-patibilities due to underlying theoretical differences.Tag comparison operates on a word-by-word basisand provides binary measures of accuracy (tag iden-tity or difference).We extend our work to the tasks of lemma identi-fication and morphological analysis: Fips as well asthe TIGER treebank provide this information.Fips has been developed independently of theTIGER treebank.
Therefore, a large part of this pa-per deals with problems arising from mismatchesbetween the design decisions made for Fips and theannotation guidelines of TIGER.
In our view, a de-tailed discussion of these mismatches is essential fora fair assessment of the performances of Fips, butmay also be interesting for future research involvingevaluation.This paper is organized as follows.
In Section 2,we present the Fips framework.
In Section 3, werecall the main characteristics of the TIGER tree-bank, explain the adaptations we applied to the Fipstagger and give some information about the evalu-ation setup.
We go on to report the results for thethree main tasks: Part-of-Speech tagging (Section4), lemma identification (Section 5), and morpho-logical analysis (Section 6).
Section 7 compares ourwork to statistical POS tagging and to parser eval-uation.
We conclude by giving an overview of thebenefits of quantitative evaluation.2 The Fips frameworkFips (Wehrli, 2007) is a deep symbolic parser devel-oped at the University of Geneva.
It currently sup-ports six languages, and others are under develop-ment.
The parser is based on an adaption of gener-ative linguistics, borrowing concepts from the Min-imalist model (Chomsky, 1995), from the Simpler16CPTPVPPartanDPNPNObergrenzeDeineAdvPPPDPNPNKurenPbeiAdvPPPDPDZuzahlungenPf?rCdeuteteDPNPNMinisterDDerFigure 1: Example output of the German Fips parser.Syntax model (Culicover and Jackendoff, 2005), aswell as from Lexical Functional Grammar (Bresnan,2001).
Each syntactic constituent is represented as asimplified X-bar structure without intermediate lev-els, in the form [XPLXR].
X denotes a lexical cate-gory, L and R stand for (possibly empty) lists of leftand right subconstituents, respectively.The originality of Fips lies in its two-layer archi-tecture.
Fundamental properties and structures thatare common to all languages are defined in an ab-stract, language-independent layer.
On a theoreti-cal level, this layer can be associated to the con-cept of ?universal grammar?.
On top of this layer,a particular, language-dependent layer extends theabstract structures and adds language-specific gram-mar rules.
The Fips lexicon contains detailed mor-phosyntactic and semantic information such as se-lectional properties, subcategorization informationand syntactico-semantic features.
The parser is thusbased on a strong lexicalist framework.
In order toguide ambiguity resolution, numeric penalty valuescan be assigned to rules and lexemes.The German component of Fips contains around100 language-specific grammar rules.
The lexiconcontains 39 000 lexemes and 410 000 word forms.The word forms are generated by a rule-based mor-phological generator.
The lexicon also contains 500multi-word expressions and 1500 high-frequencycompound nouns.
Unknown compound nouns arechunked at runtime.Fips operates in two modes: parser (see Figure 1)and tagger (see Figure 2) output.1 The tagger out-put allows us to benefit from the rich information ofthe Fips lexicon, being at the same time more robustthan the parser.3 Experimental setup3.1 The TIGER treebankThe TIGER treebank contains about 50 000 sen-tences of newspaper text, covering all domains(Brants et al, 2002).
The annotation has been per-formed with the help of interactive tools.
Thismethodology allows the human annotator to easilyaccept or reject proposals made by the computer.Part-of-speech tags are proposed by a statistical tag-ger trained on a manually annotated corpus.
It usesthe Stuttgart-T?bingen-Tagset (STTS) (Thielen etal., 1999).
The parse trees were constructed inter-actively with the help of a statistical parser.
Figure 3shows an example of the TIGER export file.3.2 AdaptationsIn order to compare the Fips output with the TIGERtags, some adaptations had to be made.
First of all,the tagset had to be changed to match the STTStagset.
While this procedure was straightforwardfor most of the categories, it showed that the Ger-man tagging module of Fips had never been subject1The parser output is shown here for illustration ?
we do notuse it in the present study.Given the scope of this workshop, we forgo translating Ger-man examples into English.17der ART SIN-MAS-NOM 311000336 0 der SUBJminister NN SIN-MAS-NOM 311019783 3 Ministerdeutete VVFIN IND-KON-PRA-3-SIN 311021998 12 andeutenf?r APPR 311050006 20 f?rZuzahlungen NE INN-ING-NOM-ACC-DAT 0 24 Zuzahlungenbei APPR 311050009 36 beikuren NN PLU-FEM-NOM-ACC-DAT-GEN 311004912 40 Kureine ART SIN-FEM-NOM-ACC 311000346 46 ein OBJober?
NN SIN-MAS-NOM-ACC-DAT 311019956 51 Ober COMP-CHUNKgrenze NN SIN-FEM-NOM-ACC 311001176 55 Grenze COMP-HEADan PTKVZ 311050018 62 an.
$.
0 65 .Figure 2: Example output of the German Fips tagger.
The columns show: the word as found in the text; the POStag in the STTS tagset; morphological information in a proprietary tagset; the lexeme number of the internal database(0 stands for unknown words); the character position at which the word begins; the lemma.
The rightmost columncontains additional information like grammatical function and compound noun syntax.Note that the compound noun Obergrenze was automatically chunked and that the word Zuzahlungen was not foundin the lexicon; the particle an is attached to the lemma of the main verb deutete.to a rigorous evaluation.
For example, there wereno particular tags for pronominal prepositions (e.g.,dar?ber, deswegen), for prepositions with articles(e.g., beim, ins), and for the infinitival particle zu.Small adaptions concerned the replacement of ?by ss (Fips uses the Swiss Standard German orthog-raphy, lacking the letter ? )
and the different lemma-tization of the particle verbs: in TIGER and in con-trast to Fips, the particles are not attached to thelemma (see the verb andeuten in Figures 2 and 3).Finally, the Fips tagger contains a compoundnoun chunker which is automatically used for un-known words and which outputs one line for eachchunk.
These lines had to be reassembled to fit withthe unchunked TIGER output (cf.
the compoundnoun Obergrenze in Figures 2 and 3).3.3 EvaluationFrom the TIGER export file, we extracted the orig-inal sentences and submitted them to the Fips tag-ger.
Then, we compared its results with the informa-tion given in TIGER.
Overall, 792 885 words werecompared.
This number does not correspond to the888 578 tokens of the TIGER corpus, because theconcept of word is much more flexible in Fips thanin TIGER.
For example, the token 62j?hriger is splitinto two words 62 and j?hriger.
By contrast, vorallem is regarded as a single lexical item (adverb) byFips, but as two words by TIGER.
Moreover, for aTIGER Tag Fips Tag Number PercentageNN NE 12592 1.59KON ADV 8000 1.01ADJD ADV 6737 0.85ADV PTKA 4976 0.63NE NN 4782 0.60VAFIN VVFIN 3529 0.45ART PRELS 2935 0.37VVFIN VVIMP 1937 0.24VVINF VVFIN 1859 0.23VVPP VVFIN 1624 0.20Correct tags 692 386 87.32Tested words 792 885 100.00Table 1: Results of the part-of-speech tag comparison.The table shows the number of tags correctly predictedby Fips (second last line), as well as the ten most fre-quent erroneous predictions.
The first column shows thecorrect tag as given by TIGER, the second column showsthe erroneous tag assigned by Fips.currently unknown reason, some words do not showup in the output of the Fips tagger.4 Part-of-speech tagging resultsThe most important part of this evaluation concernsthe part-of-speech tags.
As explained above, wehave adapted Fips to generate STTS tags.
Table 1shows the number of correctly predicted tags, and18the ten most frequent tagging errors.
In the follow-ing sections, we discuss some of these errors.4.1 Proper and common nounsThe most common error is related to the distinctionbetween proper (NE) and common nouns (NN).
Thiserror affects 2.19% of words (see first and fifth linein Table 1) and accounts for 17.29% of all taggingerrors.
Currently, the distinction between proper andcommon nouns is implemented in Fips as follows.A noun is regarded as common noun if:?
it is present in the lexicon and not explic-itly marked as proper noun: Chemie, Hirsch,Konkurrenz, or?
it is a compound noun that can be analyzed intochunks which are present in the lexicon: Bun-des+bank, Finanz+markt, Sitz+platz.A noun is regarded as proper noun if:?
it is explicitly marked as such in the lexicon:Gregor, Berlin, Europa.?
it is not present in the lexicon and cannotbe fully analyzed as compound noun: Talk,Gaullismus, Kibbuzarbeiter.Tagging errors occur in two ways.
Words that areannotated as common nouns by TIGER are anno-tated as proper nouns by Fips (see first line in Ta-ble 1).
This happens for all common nouns that arenot present in the lexicon (e.g., Primadonna, Port-folio, Niedersachse, Gaullismus).
There are alsocompound nouns with a proper noun complement:Vichy-Zeiten, Spreearm.
While TIGER considersthese words as common nouns because the head isa common noun, Fips still analyzes them as propernouns.
For other words likeMarseillaise, the TIGERannotation as common noun may be questioned.In the other way, some TIGER proper nouns havebeen tagged by Fips as common nouns (cf.
fifth linein Table 1).
One common category of erroneous tag-ging is the case of homonymous proper and commonnouns.
For example, Kohl and Teufel are commonnouns, but also the names of German politicians andtherefore proper nouns.
These misinterpretations aredue to the fact that Fips does not contain any spe-cific Named Entity Recognition module.
While Fipssuccessfully relies on letter case to identify propernouns in other languages, this approach obviouslydoes not work in German.Some proper nouns exhibit a more subtle phe-nomenon: words like Mannheim, Wendland orKantstrasse are analyzed by Fips as commoncompound nouns (Mann+Heim, wenden+Land,Kante+Strasse).
Again, a Named Entity Recogni-tion system would prevent such unfortunate analy-ses.
Furthermore, we do not find it compelling to an-alyze Buddha, Bundesbank and Bundeskriminalamtas proper nouns.To sum up, the source of noun mistagging is three-fold.
First, the Fips lexicon contains some gaps.Second, the lack of a Named Entity Recognitionmodule in Fips causes an overgeneration of homo-graph common nouns where a proper noun would beappropriate.
Third, the distinction between properand common nouns is not clear-cut, and some diver-gences can be considered as normal.4.2 Conjunctions and adverbsConjunctions are frequently mistagged as adverbs.Above all, this error affects the words und, aber,denn, which can have an adverbial (ADV) or a con-junction (KON) reading.
In (1), the first occurrenceof und is erroneously tagged as adverb.
However, ifwe parse the first part of the sentence only (2), Fipsobtains the correct conjunction reading.
This sug-gests that the conjunction reading is available alsofor (1), but that the ranking mechanism is flawed andprefers the adverb reading.
(1) Automaten sind dort nur in Gesch?ften undRestaurants erlaubt und nicht wie in derBundesrepublik auch im Freien.
(2) Automaten sind dort nur in Gesch?ften undRestaurants erlaubt.In general, it seems that Fips gets the conjunctionsright in short sentences, while it easily gets confusedwith longer sentences.
However, the preference forthe adverbial reading can be easily explained.
In or-der to propose a conjunction, the parser must iden-tify two conjuncts of the same category, whereas anadverb does not have that requirement.
Thus, if theparser fails to find two suitable conjuncts, it will pro-pose the less constrained adverbial reading.19#BOS 47149 0 1088427994 0Der der ART Nom.Sg.Masc NK 500Minister Minister NN Nom.Sg.Masc NK 500deutete deuten VVFIN 3.Sg.Past.Ind HD 504f?r f?r APPR ?
AC 503Zuzahlungen Zuzahlung NN Acc.Pl.Fem NK 503bei bei APPR ?
AC 501Kuren Kur NN Dat.Pl.Fem NK 501eine ein ART Acc.Sg.Fem NK 502Obergrenze Obergrenze NN Acc.Sg.Fem NK 502an an PTKVZ ?
SVP 504. ?
$.
?
?
0#500 ?
NP ?
SB 504#501 ?
PP ?
MNR 503#502 ?
NP ?
OA 504#503 ?
PP ?
MO 504#504 ?
S ?
?
0#EOS 47149Figure 3: An example sentence of the TIGER corpus.
The #BOS and #EOS lines mark the beginning and the end ofa sentence.
The columns show: the word (or word component) as found in the text; the lemma; the POS tag in theSTTS tagset; the morphological features.
The fifth and sixth column, as well as the lines beginning with #50x, containinformation for the construction of the parse tree and are not relevant for our study.4.3 Adjectives and adverbsIn contrast to English or French, there is no for-mal difference in German between adjectives usedas predicates (e.g., Er ist schnell ) or as adverbs (e.g.,Er f?hrt schnell ).
This formal identity may have mo-tivated the developers of the STTS tagset to use thesame tag (ADJD) in both cases.
In contrast, the Ger-man Fips tagger is based on earlier work on Frenchand English, where distinct tags for adverbials andpredicatives are needed.
Therefore, it also uses dif-ferent tags for German.We tried to come up with a simple solution to thisproblem by assigning the ADJD tag to all adverbswhose base forms are homograph with an adjective.However, in this case, we also assigned the ADJDtag to words like ganz, nat?rlich, wirklich, whichare tagged as proper adverbs (ADV) in TIGER.
Inshort, we had the choice of either overgeneratingADV tags (keeping the Fips output as-is) or over-generating ADJD tags (with the homograph modi-fication).
Preliminary tests showed similar amountsof overgeneration in both cases.
We have thus cho-sen to stick to the original Fips analyses.4.4 Particles followed by adjectivesSTTS introduces a special tag (PTKA) for parti-cles ?followed by adjectives or adverbs?, for exam-ple am [sch?nsten], zu [schnell].
In Fips, the classof comparative adverbs also contains auch, so andmehr.
Of course, these words are not always fol-lowed by adjectives, and should thus not always begiven the PTKA tag.
While different readings areindeed available in the Fips lexicon, the results sug-gest that Fips overgeneralizes the comparative read-ing and assigns the PTKA tag even in cases wherea normal ADV tag would be adequate.
(3) shows asentence where Fips erroneously assigned the PTKAtag to auch.
(3) Der Verkehrssenator, wie er k?nftig auchhei?en m?ge, .
.
.4.5 PronounsThe seventh line refers to the homography of the def-inite determiner and the relative pronoun (PRELS)whenever Fips cannot find an agreement betweenthe determiner and the head of the noun phrase.
(4) Neue Debatte ?ber den Atomschild20In (4), the Fips lexicon only contains the neuterlexeme Schild (which serves as a head of the com-pound noun Atomschild ), but not the rarer mas-culine homograph lexeme.
This lexical gap pre-vents the masculine determiner den to be attachedto Atomschild as a determiner, and Fips resorts tothe relative pronoun analysis instead.4.6 Verb problemsVerb tagging seems to be a serious problem to Fips:four of the ten most frequent tagging errors involveverbs.The first type of error is related to the distinctionbetween auxiliary and full verbs.
The three auxiliaryverbs haben, sein, werden can also have full verbreadings, depending on the context.
We recently ob-served that Fips preferred the auxiliary reading evenin cases where a full verb reading is required, andsubsequently modified the constraints on the lexemeselection.
It now turns out that these constraints aretoo strong and lead to a massive overgeneration ofthe full verb reading.Then, Fips tends to overgenerate imperatives:third person singular forms are erroneously analyzedas imperative plurals (e.g., kommt, schreit).
Again,this is due to agreement constraints: the third personsingular requires an overt subject, while an imper-ative does not.
If Fips fails to find a subject thatagrees with the verb (for example because of an un-detected long distance dependency), it will resort toan imperative reading.
In the future development ofFips, further restrictions should be imposed on theuse of imperative forms as these are extremely rarein newspaper text.The last two lines in Table 1 reveal that finite verbforms are preferred to infinite forms: infinitives aremistagged as finite plural forms, and past partici-ples without ge- prefix are mistagged as third personsingular forms (for regular verbs) or as past pluralform (for irregular verbs with -en participle).
Thesephenomena depend on long distance relations andshould typically benefit from a full parsing approachlike the one used by Fips.
Two factors may explainwhy this is not the case.
First, many sentences inwhich such errors occur could not be parsed com-pletely by Fips; long distance relations are not fullydetected in these cases.
Second, the implementationof passive and modal sentences is incomplete andTIGER Base Form Fips Base Formdieser dieseanderer anderwelche welcherBeamte BeamterAngestellte AngestellterFigure 4: For some pronouns and nouns, TIGER and Fipsuse different base forms.lacks some essential constraints on verb form selec-tion.5 Lemmatizer resultsOn the whole TIGER corpus (792 885 words),94.32% of the words (747 855) were correctlylemmatized.
Most errors were due to diverg-ing base form choices.
This especially holds forpronouns and nominalized adjectives (cf.
Fig-ure 4), but also for pronouns.
In TIGER, femi-nine and neuter pronouns always refer to the mas-culine lemma, whereas Fips separates the gen-ders more strictly: der (Dat.Sg.Fem) refers to thelemma der (Nom.Sg.Masc) in TIGER, but to die(Nom.Sg.Fem) in Fips.
Moreover, participles usedas adjectives keep the infinitive as base form in Fips,but not in TIGER.Some lemma errors are due to wrong POS tag-ging.
For instance, we found that Fips overgeneratesimperatives.
For example, einig is not analyzed asadjective, but as the imperative singular (with elisionof final e) of sich einigen; the adjective n?tige is an-alyzed as the imperative singular of n?tigen.
How-ever, such awkward analyses should be easy to ironout.Globally, we find that very few errors are directlydue to the lemmatizer; most of them are either dueto different base forms or to POS tagging errors.6 Morphology resultsAfter the discussion of the part-of-speech tagger andlemmatizer functionalities of Fips, we now turn tothe last functionality, the morphological analyzer.We restricted our evaluation to the words that ob-tained correct POS tags: if the POS tag is alreadywrong, it is very likely that the morphology will bewrong as well.
Table 2 reports the results of the mor-21Type Number PercentageNumber mismatch 15617 2.26Case mismatch 12420 1.79Gender mismatch 8461 1.22Degree mismatch 514 0.07Person mismatch 108 0.02Correct analysisor no morphology 665 110 96.06Tested words 692 386 100.00Table 2: Results of the morphological analysis.
The tablepresents the numbers of words that have been correctlyanalyzed by Fips, and the types of errors that occurred.
Aword can present several mismatch types.phology evaluation.
Parts of speech without inflec-tion were considered as correctly analyzed.
We splitthe errors into five categories, according to the in-flection feature that Fips failed to predict correctly.The different mismatch types do not sum up to 100%because a word can show several mismatches (e.g., anoun can show case and number mismatch), and be-cause not all types of mismatch apply to all parts ofspeech (for instance, degree mismatch only appliesto adjectives).It is not easy to find recurrent patterns in the er-rors.
However, we found that most errors occurredin noun phrases.
Most inflected adjective and ar-ticle forms admit several morphological analyses,but the ambiguities can usually be reduced by thesyntactic context.
If the ambiguities are reduced inan incorrect way, this means that the syntactic con-text has been analyzed badly.
In other words, suchmorphology errors often reflect bad parses.
There-fore, it might be useful to address these errors be-fore evaluating the parsing performance of Fips.
An-other rather odd fact is that nouns with identical sin-gular and plural forms (for example, Minister, Un-ternehmen) prefer to be analyzed as plurals by Fips.Here again, these cases hint at bad parses.Degree mismatches result from a bug in Fips:comparative forms in predicative positions as in (5)are assigned the positive tag instead of the compara-tive one.
(5) .
.
.
um noch tiefer in den Kosmos blicken zuk?nnen.7 Related workIt may be interesting to compare Fips to a statisticalpart-of-speech tagger for German.
The TnT tagger(Brants, 2000) is based on Hidden Markov Models,and has been trained and tested on the NEGRA cor-pus (Skut et al, 1997); NEGRA is the predecessor ofTIGER and uses the same tagset.
Brants (2000) re-ports an overall accuracy of 96.7%.
However, TnT isnot directly comparable to Fips for several reasons.First, we showed that Fips originally used a differ-ent tagset, based on different linguistic assumptionsthan STTS.
Those conceptional differences make upa large part of the errors, as has been shown forthe distinction between the ADJD and ADV tags.By contrast, TnT has been trained directly over theSTTS tagset and should thus not present such errors.Second, the recurrence of certain error patternswith Fips illustrates the classical problem of manualrule ranking and weighting in rule-based systems.Third, Fips has been conceived as a parser in thefirst place, and its tagger functionality should ratherbe viewed as a by-product.
Hence, its algorithms arenot optimized for POS tagging.
While there may besimpler approaches to obtain high tagging accuracy,the method chosen for Fips seems theoretically moreplausible to us.As we pointed out at the beginning, this taggerevaluation has been started as a first step towards theevaluation of the Fips parser.
While POS tagging hasthe advantage of operating word-by-word and of be-ing rather theory-independent, these two propertiesdo not hold for parsing.The phrase trees in TIGER are rather flat, whilethe ones generated by Fips are deeper and closerto recent generative grammar frameworks.
We willthus need to define the type of constituents that canbe compared.
An even bigger issue is the allowanceof discontinuous phrases and crossing branches inTIGER, whereas Fips resolves these phenomena byresorting to projections and traces.
Further researchhas to show if these structural differences can beovercome in order to lead to a meaningful compar-ison.
The exact evaluation metric will also have tobe chosen.
While PARSEVAL (Black et al, 1991)is still one of the most important metrics, other mea-sures may be more adapted to our problem (Carrollet al, 2002; Rehbein and van Genabith, 2007).228 ConclusionAs we remarked above, this article reports on workin progress.
Until now, we have been able to showthat the general approach of evaluating Fips with thehelp of the TIGER treebank is valid.
With very littleadaptation work (see Section 3.2), we managed toobtain 87.32% of POS-tagging accuracy.
This is avery promising beginning, and the discussion of theerrors has shown that there are many ?low hangingfruits?
to improve the performance.In any way, we find that the quantitative evalu-ation of NLP systems can be quite rewarding: de-veloping rule-based systems is a complex task, of-ten guided by vague intuitions about parsing qual-ity.
Quantitative evaluation allows us to measurethe progress of the development and guarantees usthat improvements on one parameter do not yield un-wanted side effects on another.Finally, the quantitative evaluation of the POStagging performances yields important feedback onthe forces and weaknesses of Fips.
The result of theevaluation can be viewed as a sort of priority list forthe developer.
By working on the most common er-rors in a target-oriented way, (s)he is guaranteed toinvest his/her time in a maximally effective manner.Such guiding principles are very valuable for the fur-ther development of any rule-based parsing system,independently of the precise accuracy figures of theevaluation.
Even if the adaptation of two differenttagsets and tagging philosophies is not straightfor-ward, we plan to extend our evaluation to other lan-guages of the Fips project for which suitable goldstandard corpora exist.AcknowledgementsWe thank Eric Wehrli and for his precious supportfor this work and for his valuable comments on pre-vious versions of this paper.ReferencesG.
Adda, J. Mariani, J. Lecomte, P. Paroubek, and M. Ra-jman.
1998.
The GRACE French part-of-speech tag-ging evaluation task.
In Proceedings of the First In-ternational Conference on Language Resources andEvaluation (LREC), Granada.E.
Black, S. Abney, S. Flickenger, C. Gdaniec, C. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
Procedurefor quantitatively comparing the syntactic coverage ofEnglish grammars.
In HLT ?91: Proceedings of theWorkshop on Speech and Natural Language, pages306?311, Pacific Grove, California.S.
Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.2002.
The TIGER Treebank.
In Proceedings of theWorkshop on Treebanks and Linguistic Theories, So-zopol.T.
Brants.
2000.
TnT ?
a statistical part-of-speech tagger.In Proceedings of the Sixth Applied Natural LanguageProcessing (ANLP-2000), Seattle.J.
Bresnan.
2001.
Lexical Functional Syntax.
Blackwell,Oxford.J.
Carroll, A. Frank, D. Lin, D. Prescher, and H. Uszko-reit.
2002.
Beyond PARSEVAL ?
towards improvedevaluation measures for parsing systems.
In Proceed-ings of the LREC 2002 Workshop, Las Palmas, GranCanaria.N.
Chomsky.
1995.
The Minimalist Program.
MITPress, Cambridge, Mass.P.
W. Culicover and R. Jackendoff.
2005.
Simpler Syn-tax.
Oxford University Press, Oxford.J.-P. Goldman, C. Laenzlinger, G. Soare, and E. Wehrli.2005.
L?analyseur syntaxique multilingue Fips dans lacampagne EASy.
In Proceedings of TALN XII, vol-ume 2, pages 35?49, Dourdan.I.
Rehbein and J. van Genabith.
2007.
Treebankannotation schemes and parser evaluation for Ger-man.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP/CoNLL 2007), pages 630?639, Prague.W.
Skut, B. Krenn, T. Brants, and H. Uszkoreit.
1997.An annotation scheme for free word order languages.In Proceedings of the Fifth Conference on Applied Nat-ural Language Processing ANLP-97, Washington, DC.C.
Thielen, A. Schiller, S. Teufel, and C. St?ckert.
1999.Guidelines f?r das Tagging deutscher Textkorpora mitSTTS.
Technical report, University of Stuttgart andUniversity of T?bingen.E.
Wehrli.
2007.
Fips, a ?deep?
linguistic multilingualparser.
In Proceedings of the ACL 2007 Workshop onDeep Linguistic Processing, pages 120?127, Prague.23
