Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 219?224,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsQCRI-MES Submission at WMT13: Using Transliteration Mining toImprove Statistical Machine TranslationHassan Sajjad1, Svetlana Smekalova2, Nadir Durrani3,Alexander Fraser4, Helmut Schmid41Qatar Computing Research Institute ?
hsajjad@qf.org.qa2University of Stuttgart ?
smekalsa@ims.uni-stuttgart.de3University of Edinburgh ?
dnadir@inf.ed.ac.uk4Ludwig-Maximilians University Munich ?
(fraser|schmid)@cis.uni-muenchen.deAbstractThis paper describes QCRI-MES?s sub-mission on the English-Russian dataset tothe Eighth Workshop on Statistical Ma-chine Translation.
We generate improvedword alignment of the training data byincorporating an unsupervised translitera-tion mining module to GIZA++ and builda phrase-based machine translation sys-tem.
For tuning, we use a variation of PROwhich provides better weights by optimiz-ing BLEU+1 at corpus-level.
We translit-erate out-of-vocabulary words in a post-processing step by using a transliterationsystem built on the transliteration pairsextracted using an unsupervised translit-eration mining system.
For the Russianto English translation direction, we applylinguistically motivated pre-processing onthe Russian side of the data.1 IntroductionWe describe the QCRI-Munich-Edinburgh-Stuttgart (QCRI-MES) English to Russian andRussian to English systems submitted to theEighth Workshop on Statistical Machine Trans-lation.
We experimented using the standardPhrase-based Statistical Machine TranslationSystem (PSMT) as implemented in the Mosestoolkit (Koehn et al 2007).
The typical pipelinefor translation involves word alignment usingGIZA++ (Och and Ney, 2003), phrase extraction,tuning and phrase-based decoding.
Our system isdifferent from standard PSMT in three ways:?
We integrate an unsupervised transliterationmining system (Sajjad et al 2012) into theGIZA++ word aligner (Sajjad et al 2011).So, the selection of a word pair as a correctalignment is decided using both translationprobabilities and transliteration probabilities.?
The MT system fails when translating out-of-vocabulary (OOV) words.
We build a statis-tical transliteration system on the translitera-tion pairs mined by the unsupervised translit-eration mining system and transliterate themin a post-processing step.?
We use a variation of Pairwise Ranking Op-timization (PRO) for tuning.
It optimizesBLEU at corpus-level and provides betterfeature weights that leads to an improvementin translation quality (Nakov et al 2012).We participate in English to Russian and Rus-sian to English translation tasks.
For the Rus-sian/English system, we present experiments withtwo variations of the parallel corpus.
One set ofexperiments are conducted using the standard par-allel corpus provided by the workshop.
In the sec-ond set of experiments, we morphologically re-duce Russian words based on their fine-grainedPOS tags and map them to their root form.
Wedo this on the Russian side of the parallel corpus,tuning set, development set and test set.
This im-proves word alignment and learns better transla-tion probabilities by reducing the vocabulary size.The paper is organized as follows.
Section2 talks about unsupervised transliteration miningand its incorporation to the GIZA++ word aligner.In Section 3, we describe the transliteration sys-tem.
Section 4 describes the extension of PROthat optimizes BLEU+1 at corpus level.
Section5 and Section 6 present English/Russian and Rus-sian/English machine translation experiments re-spectively.
Section 7 concludes.2192 Transliteration MiningConsider a list of word pairs that consists of eithertransliteration pairs or non-transliteration pairs.A non-transliteration pair is defined as a wordpair where words are not transliteration of eachother.
They can be translation, misalignment,etc.
Transliteration mining extracts transliterationpairs from the list of word pairs.
Sajjad et al(2012) presented an unsupervised transliterationmining system that trains on the list of word pairsand filters transliteration pairs from that.
It modelsthe training data as the combination of a translit-eration sub-model and a non-transliteration sub-model.
The transliteration model is a joint sourcechannel model.
The non-transliteration model as-sumes no correlation between source and targetword characters, and independently generates asource and a target word using two fixed uni-gram character models.
The transliteration miningmodel is defined as an interpolation of the translit-eration model and the non-transliteration model.We apply transliteration mining to the list ofword pairs extracted from English/Russian paral-lel corpus and mine transliteration pairs.
We usethe mined pairs for the training of the translitera-tion system.2.1 Transliteration Augmented-GIZA++GIZA++ aligns parallel sentences at word level.
Itapplies the IBM models (Brown et al 1993) andthe HMM model (Vogel et al 1996) in both direc-tions i.e.
source to target and target to source.
Itgenerates a list of translation pairs with translationprobabilities, which is called the t-table.
Sajjadet al(2011) used a heuristic-based transliterationmining system and integrated it into the GIZA++word aligner.
We follow a similar procedure butuse the unsupervised transliteration mining systemof Sajjad et al(2012).We define a transliteration sub-model and trainit on the transliteration pairs mined by the unsuper-vised transliteration mining system.
We integrateit into the GIZA++ word aligner.
The probabil-ity of a word pair is calculated as an interpolationof the transliteration probability and the transla-tion probability stored in the t-table of the differ-ent alignment models used by the GIZA++ aligner.This interpolation is done for all iterations of allalignment models.2.1.1 Estimating Transliteration ProbabilitiesWe use the algorithm for the estimation of translit-eration probabilities of Sajjad et al(2011).
Wemodify it to improve efficiency.
In step 6 of Al-gorithm 1 instead of taking all f that coocur withe, we take only those that have a word length ra-tio in range of 0.8-1.2.1 This reduces cooc(e) bymore than half and speeds up step 9 of Algorithm1.
The word pairs that are filtered out from cooc(e)won?t have transliteration probability pti(f |e).
Wedo not interpolate in these cases and use the trans-lation probability as it is.Algorithm 1 Estimation of transliteration proba-bilities, e-to-f direction1: unfiltered data?
list of word pairs2: filtered data?transliteration pairs extracted using unsu-pervised transliteration mining system3: Train a transliteration system on the filtered data4: for all e do5: nbestTI(e) ?
10 best transliterations for e accord-ing to the transliteration system6: cooc(e)?
set of all f that cooccur with e in a parallelsentence with a word length in ratio of 0.8-1.27: candidateTI(e)?
cooc(e) ?
nbestTI(e)8: for all f do9: pmoses(f, e) ?
joint transliteration probability of eand f according to the transliterator10: Calculate conditional transliteration probabilitypti(f |e)?
pmoses(f,e)?f?
?CandidateTI(e) pmoses(f ?,e)2.1.2 Modified EM TrainingSajjad et al(2011) modified the EM training ofthe word alignment models.
They combined thetranslation probabilities of the IBM models andthe HMM model with the transliteration proba-bilities.
Consider pta(f |e) = fta(f, e)/fta(e) isthe translation probability of the word alignmentmodels.
The interpolated probability is calcu-lated by adding the smoothed alignment frequencyfta(f, e) to the transliteration probability weightby the factor ?.
The modified translation probabil-ities is given by:p?
(f |e) = fta(f, e) + ?pti(f |e)fta(e) + ?
(1)where fta(f, e) = pta(f |e)fta(e).
pta(f |e) is ob-tained from the original t-table of the alignmentmodel.
fta(e) is the total corpus frequency of e.?
is the transliteration weight which is defined asthe number of counts the transliteration model getsversus the translation model.
The model is not1We assume that the words with very different charactercounts are less likely to be transliterations.220very sensitive to the value of ?.
We use ?
= 50for our experiments.
The procedure we describedof estimation of transliteration probabilities andmodification of EM is also followed in the oppo-site direction f-to-e.3 Transliteration SystemThe unsupervised transliteration mining system(as described in Section 2) outputs a list of translit-eration pairs.
We consider transliteration wordpairs as parallel sentences by putting a space af-ter every character of the words and train a PSMTsystem for transliteration.
We apply the transliter-ation system to OOVs in a post-processing step onthe output of the machine translation system.Russian is a morphologically rich language.Different cases of a word are generally representedby adding suffixes to the root form.
For OOVsthat are named entities, transliterating the inflectedforms generates wrong English transliterations asinflectional suffixes get transliterated too.
To han-dle this, first we need to identify OOV named en-tities (as there can be other OOVs that are notnamed entities) and then transliterate them cor-rectly.
We tackle the first issue as follows: Ifan OOV word is starting with an upper case let-ter, we identify it as a named entity.
To correctlytransliterate it to English, we stem the named en-tity based on a list of suffixes ( , , , , , )and transliterate the stemmed form.
For morpho-logically reduced Russian (see Section 6.1), wefollow the same procedure as OOVs are unknownto the POS tagger too and are (incorrectly) not re-duced to their root forms.
For OOVs that are notidentified as named entities, we transliterate themwithout any pre-processing.4 PRO: Corpus-level BLEUPairwise Ranking Optimization (PRO) (Hopkinsand May, 2011) is an extension of MERT (Och,2003) that can scale to thousands of parameters.It optimizes sentence-level BLEU+1 which is anadd-one smoothed version of BLEU (Lin and Och,2004).
The sentence-level BLEU+1 has a biastowards producing short translations as add-onesmoothing improves precision but does not changethe brevity penalty.
Nakov et al(2012) fixed thisby using several heuristics on brevity penalty, ref-erence length and grounding the precision length.In our experiments, we use the improved versionof PRO as provided by Nakov et al(2012).
Wecall it PROv1 later on.5 English/Russian Experiments5.1 DatasetThe amount of bitext used for the estimation of thetranslation model is ?
2M parallel sentences.
Weuse newstest2012a for tuning and newstest2012b(tst2012) as development set.The language model is estimated using largemonolingual corpus of Russian ?
21.7M sen-tences.
We follow the approach of Schwenk andKoehn (2008) by training domain-specific lan-guage models separately and then linearly inter-polate them using SRILM with weights optimizedon the held-out development set.
We divide thetuning set newstest2012a into two halves and usethe first half for tuning and second for test in or-der to obtain stable weights (Koehn and Haddow,2012).5.2 Baseline SettingsWe word-aligned the parallel corpus usingGIZA++ (Och and Ney, 2003) with 5 iterationsof Model1, 4 iterations of HMM and 4 iterationsof Model4, and symmetrized the alignments us-ing the grow-diag-final-and heuristic (Koehn et al2003).
We built a phrase-based machine transla-tion system using the Moses toolkit.
Minimum er-ror rate training (MERT), margin infused relaxedalgorithm (MIRA) and PRO are used to optimizethe parameters.5.3 Main System SettingsOur main system involves a pre-processing step?
unsupervised transliteration mining, and a post-processing step ?
transliteration of OOVs.
For thetraining of the unsupervised transliteration min-ing system, we take the word alignments fromour baseline settings and extract all word pairswhich occur as 1-to-1 alignments (like Sajjad etal.
(2011)) and later refer to them as a list ofword pairs.
The unsupervised transliteration min-ing system trains on the list of word pairs andmines transliteration pairs.
We use the mined pairsto build a transliteration system using the Mosestoolkit.
The transliteration system is used in Algo-rithm 1 to generate transliteration probabilities ofcandidate word pairs and is also used in the post-processing step to transliterate OOVs.We run GIZA++ with identical settings as de-scribed in Section 5.2.
We interpolate for ev-221GIZA++ TA-GIZA++ OOV-TIMERT 23.41 23.51 23.60MIRA 23.60 23.73 23.85PRO 23.57 23.68 23.70PROv1 23.65 23.76 23.87Table 1: BLEU scores of English to Russian ma-chine translation system evaluated on tst2012 us-ing baseline GIZA++ alignment and translitera-tion augmented-GIZA++.
OOV-TI presents thescore of the system trained using TA-GIZA++ af-ter transliterating OOVsery iteration of the IBM Model1 and the HMMmodel.
We had problem in applying smoothingfor Model4 and did not interpolate transliterationprobabilities for Model4.
The alignments are re-fined using the grow-diag-final-and heuristic.
Webuild a phrase-based system on the aligned pairsand tune the parameters using PROv1.
OOVs aretransliterated in the post-processing step.5.4 ResultsTable 1 summarizes English/Russian results ontst2012.
Improved word alignment gives up to0.13 BLEU points improvement.
PROv1 improvestranslation quality and shows 0.08 BLEU pointincrease in BLEU in comparison to the parame-ters tuned using PRO.
The transliteration of OOVsconsistently improve translation quality by at least0.1 BLEU point for all systems.2 This adds to acumulative gain of up to 0.2 BLEU points.We summarize results of our systems trained onGIZA++ and transliteration augmented-GIZA++(TA-GIZA++) and tested on tst2012 and tst2013in Table 2.
Both systems use PROv1 for tuningand transliteration of OOVs in the post-processingstep.
The system trained on TA-GIZA++ per-formed better than the system trained on the base-line aligner GIZA++.6 Russian/English ExperimentsIn this section, we present translation experimentsin Russian to English direction.
We morphologi-cally reduce the Russian side of the parallel data ina pre-processing step and train the translation sys-tem on that.
We compare its result with the Rus-sian to English system trained on the un-processedparallel data.2We see similar gain in BLEU when using operation se-quence model (Durrani et al 2011) for decoding and translit-erating OOVs in a post-processing step (Durrani et al 2013).SYS tst2012 tst2013GIZA++ 23.76 18.4TA-GIZA++ 23.87 18.5*Table 2: BLEU scores of English to Russian ma-chine translation system evaluated on tst2012 andtst2013 using baseline GIZA++ alignment andtransliteration augmented-GIZA++ alignment andpost-processed the output by transliterating OOVs.Human evaluation in WMT13 is performed onTA-GIZA++ tested on tst2013 (marked with *)6.1 Morphological ProcessingThe linguistic processing of Russian involves POStagging and morphological reduction.
We first tagthe Russian data using a fine grained tagset.
Thetagger identifies lemmas and the set of morpholog-ical attributes attached to each word.
We reducethe number of these attributes by deleting someof them, that are not relevant for English (for ex-ample, gender agreement of verbs).
This gener-ates a morphologically reduced Russian which isused in parallel with English for the training ofthe machine translation system.
Further details onthe morphological processing of Russian are de-scribed in Weller et al(2013).6.1.1 POS TaggingWe use RFTagger (Schmid and Laws, 2008) forPOS tagging.
Despite the good quality of taggingprovided by RFTagger, some errors seem to be un-avoidable due to the ambiguity of certain gram-matical forms in Russian.
A good example ofthis is neuter nouns that have the same form inall cases, or feminine nouns, which have identi-cal forms in singular genitive and plural nomina-tive (Sharoff et al 2008).
Since Russian sentenceshave free word order, and the case of nouns can-not be determined on that basis, this imperfectioncan not be corrected during tagging or by post-processing the tagger output.6.1.2 Morphological ReductionEnglish in comparison to Slavic group of lan-guages is morphologically poor.
For example, En-glish has no morphological attributes for nounsand adjectives to express gender or case; verbs inEnglish have no gender either.
Russian, on thecontrary, has rich morphology.
It suffices to saythat the Russian has 6 cases and 3 grammaticalgenders, which manifest themselves in different222suffixes for nouns, pronouns, adjectives and someverb forms.When translating from Russian into English, alot of these attributes become meaningless and ex-cessive.
It makes sense to reduce the number ofmorphological attributes before the text is sup-plied for the training of the MT system.
We ap-ply morphological reduction to nouns, pronouns,verbs, adjectives, prepositions and conjunctions.The rest of the POS (adverbs, particles, interjec-tions and abbreviations) have no morphological at-tributes and are left unchanged.We apply morphological reduction to train,tune, development and test data.
We refer to thisdata set as morph-reduced later on.6.2 DatasetWe use two variations of the parallel corpus tobuild and test the Russian to English system.
Onesystem is built on the data provided by the work-shop.
For the second system, we preprocess theRussian side of the data as described in Section6.1.
Both the provided parallel corpus and themorph-reduced parallel corpus consist of 2M par-allel sentences each.
We use them for the estima-tion of the translation model.
We use large train-ing data for the estimation of monolingual lan-guage model ?
en?
287.3M sentences.
We followthe identical procedure of interpolated languagemodel as described in Section 5.1.
We use new-stest2012a for tuning and newstest2012b (tst2012)for development.6.3 System SettingsWe use identical system settings to those describedin Section 5.3.
We trained the systems sepa-rately on GIZA++ and transliteration augmented-GIZA++ to compare their results.
All systems aretuned using PROv1.
The translation output is post-processed to transliterate OOVs.6.4 ResultsTable 3 summarizes results of Russian to Englishmachine translation systems trained on the orig-inal parallel corpus and on the morph-reducedcorpus and using GIZA++ and transliterationaugmented-GIZA++ for word alignment.
The sys-tem using TA-GIZA++ for alignment shows thebest results for both tst2012 and tst2013.
The im-proved alignment gives a BLEU improvement ofup to 0.4 points.Original corpusSYS tst2012 tst2013GIZA++ 32.51 25.5TA-GIZA++ 33.40 25.9*Morph-reducedSYS tst2012 tst2013GIZA++ 31.22 24.30TA-GIZA++ 31.40 24.45Table 3: Russian to English machine translationsystem evaluated on tst2012 and tst2013.
Humanevaluation in WMT13 is performed on the systemtrained using the original corpus with TA-GIZA++for alignment (marked with *)The system built on the morph-reduced datashows degradation in results by 1.29 BLEU points.However, the percentage of OOVs reduces forboth test sets when using the morph-reduced dataset compared to the original parallel corpus.
Weanalyze the output of the system and find that themorph-reduced system makes mistakes in choos-ing the right tense of the verb.
This might be onereason for poor performance.
This implies that themorphological reduction is slightly damaging thedata, perhaps for specific parts of speech.
In thefuture, we would like to investigate this issue indetail.7 ConclusionIn this paper, we described the QCRI-Munich-Edinburgh-Stuttgart machine translation systemssubmitted to the Eighth Workshop on StatisticalMachine Translation.
We aligned the parallel cor-pus using transliteration augmented-GIZA++ toimprove the word alignments.
We built a phrase-based system using the Moses toolkit.
For tun-ing the feature weights, we used an improvementof PRO that optimizes for corpus-level BLEU.
Wepost-processed the output of the machine transla-tion system to transliterate OOV words.For the Russian to English system, we mor-phologically reduced the Russian data in a pre-processing step.
This reduced the vocabulary sizeand helped to generate better word alignments.However, the performance of the SMT systemdropped by 1.29 BLEU points in decoding.
Wewill investigate this issue further in the future.223AcknowledgmentsWe would like to thank the anonymous reviewersfor their helpful feedback and suggestions.
Wewould like to thank Philipp Koehn and Barry Had-dow for providing data and alignments.
NadirDurrani was funded by the European Union Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreement n ?
287658.
Alexander Fraserwas funded by Deutsche Forschungsgemeinschaftgrant Models of Morphosyntax for Statistical Ma-chine Translation.
Helmut Schmid was supportedby Deutsche Forschungsgemeinschaft grant SFB732.
This publication only reflects the authorsviews.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and R. L. Mercer.
1993.
The mathe-matics of statistical machine translation: parameterestimation.
Computational Linguistics, 19(2).Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A joint sequence translation model with in-tegrated reordering.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, Port-land, USA.Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-san Sajjad, and Richa?rd Farkas.
2013.
Munich-Edinburgh-Stuttgart submissions of OSM systems atWMT13.
In Proceedings of the Eighth Workshop onStatistical Machine Translation, Sofia, Bulgaria.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,Edinburgh, United Kingdom.Philipp Koehn and Barry Haddow.
2012.
Towardseffective use of training data in statistical machinetranslation.
In Proceedings of the Seventh Work-shop on Statistical Machine Translation, Montre?al,Canada.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceed-ings of the Human Language Technology and NorthAmerican Association for Computational Linguis-tics Conference, Edmonton, Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the Asso-ciation for Computational Linguistics, Demonstra-tion Program, Prague, Czech Republic.Chin-Yew Lin and Franz Josef Och.
2004.
OR-ANGE: a method for evaluating automatic evalua-tion metrics for machine translation.
In Proceed-ings of the 20th international conference on Compu-tational Linguistics, Geneva, Switzerland.Preslav Nakov, Francisco Guzma?n, and Stephan Vo-gel.
2012.
Optimizing for sentence-level BLEU+1yields short translations.
In Proceedings of the24th International Conference on ComputationalLinguistics, Mumbai, India.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1).Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, Sapporo, Japan.Hassan Sajjad, Alexander Fraser, and Helmut Schmid.2011.
An algorithm for unsupervised translitera-tion mining with an application to word alignment.In Proceedings of the 49th Annual Conference ofthe Association for Computational Linguistics, Port-land, USA.Hassan Sajjad, Alexander Fraser, and Helmut Schmid.2012.
A statistical model for unsupervised andsemi-supervised transliteration mining.
In Proceed-ings of the 50th Annual Conference of the Associa-tion for Computational Linguistics, Jeju, Korea.Helmut Schmid and Florian Laws.
2008.
Estimationof conditional probabilities with decision trees andan application to fine-grained pos tagging.
In Pro-ceedings of the 22nd International Conference onComputational Linguistics - Volume 1, Manchester,United Kingdom.Holger Schwenk and Philipp Koehn.
2008.
Large andDiverse Language Models for Statistical MachineTranslation.
In International Joint Conference onNatural Language Processing, Hyderabad, India.Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, AnnaFeldman, and Dagmar Divjak.
2008.
Designingand evaluating a russian tagset.
In Proceedings ofthe Sixth International Conference on Language Re-sources and Evaluation.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statisticaltranslation.
In 16th International Conference onComputational Linguistics, Copenhagen, Denmark.Marion Weller, Max Kisselew, Svetlana Smekalova,Alexander Fraser, Helmut Schmid, Nadir Durrani,Hassan Sajjad, and Richa?rd Farkas.
2013.
Munich-Edinburgh-Stuttgart submissions at WMT13: Mor-phological and syntactic processing for SMT.
InProceedings of the Eighth Workshop on StatisticalMachine Translation, Sofia, Bulgaria.224
