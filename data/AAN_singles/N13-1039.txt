Proceedings of NAACL-HLT 2013, pages 380?390,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsImproved Part-of-Speech Tagging for Online Conversational Textwith Word ClustersOlutobi Owoputi?
Brendan O?Connor?
Chris Dyer?Kevin Gimpel?
Nathan Schneider?
Noah A.
Smith?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA?Toyota Technological Institute at Chicago, Chicago, IL 60637, USACorresponding author: brenocon@cs.cmu.eduAbstractWe consider the problem of part-of-speechtagging for informal, online conversationaltext.
We systematically evaluate the use oflarge-scale unsupervised word clusteringand new lexical features to improve taggingaccuracy.
With these features, our systemachieves state-of-the-art tagging results onboth Twitter and IRC POS tagging tasks;Twitter tagging is improved from 90% to 93%accuracy (more than 3% absolute).
Quali-tative analysis of these word clusters yieldsinsights about NLP and linguistic phenomenain this genre.
Additionally, we contribute thefirst POS annotation guidelines for such textand release a new dataset of English languagetweets annotated using these guidelines.Tagging software, annotation guidelines, andlarge-scale word clusters are available at:http://www.ark.cs.cmu.edu/TweetNLPThis paper describes release 0.3 of the ?CMUTwitter Part-of-Speech Tagger?
and annotateddata.1 IntroductionOnline conversational text, typified by microblogs,chat, and text messages,1 is a challenge for natu-ral language processing.
Unlike the highly editedgenres that conventional NLP tools have been de-veloped for, conversational text contains many non-standard lexical items and syntactic patterns.
Theseare the result of unintentional errors, dialectal varia-tion, conversational ellipsis, topic diversity, and cre-ative use of language and orthography (Eisenstein,2013).
An example is shown in Fig.
1.
As a re-sult of this widespread variation, standard model-ing assumptions that depend on lexical, syntactic,and orthographic regularity are inappropriate.
There1Also referred to as computer-mediated communication.ikr!smhGheOaskedVfirPyoDlastAnameNsoPheOcanVaddVuOonPfb?lololol!Figure 1: Automatically tagged tweet showing nonstan-dard orthography, capitalization, and abbreviation.
Ignor-ing the interjections and abbreviations, it glosses as Heasked for your last name so he can add you on Facebook.The tagset is defined in Appendix A.
Refer to Fig.
2 forword clusters corresponding to some of these words.is preliminary work on social media part-of-speech(POS) tagging (Gimpel et al 2011), named entityrecognition (Ritter et al 2011; Liu et al 2011), andparsing (Foster et al 2011), but accuracy rates arestill significantly lower than traditional well-editedgenres like newswire.
Even web text parsing, whichis a comparatively easier genre than social media,lags behind newspaper text (Petrov and McDonald,2012), as does speech transcript parsing (McCloskyet al 2010).To tackle the challenge of novel words and con-structions, we create a new Twitter part-of-speechtagger?building on previous work by Gimpel etal.
(2011)?that includes new large-scale distribu-tional features.
This leads to state-of-the-art resultsin POS tagging for both Twitter and Internet RelayChat (IRC) text.
We also annotated a new dataset oftweets with POS tags, improved the annotations inthe previous dataset from Gimpel et al and devel-oped annotation guidelines for manual POS taggingof tweets.
We release all of these resources to theresearch community:?
an open-source part-of-speech tagger for onlineconversational text (?2);?
unsupervised Twitter word clusters (?3);?
an improved emoticon detector for conversationaltext (?4);380?
POS annotation guidelines (?5.1); and?
a new dataset of 547 manually POS-annotatedtweets (?5).2 MEMM TaggerOur tagging model is a first-order maximum en-tropy Markov model (MEMM), a discriminative se-quence model for which training and decoding areextremely efficient (Ratnaparkhi, 1996; McCallumet al 2000).2 The probability of a tag yt is condi-tioned on the input sequence x and the tag to its leftyt?1, and is parameterized by a multiclass logisticregression:p(yt = k | yt?1,x, t;?)
?exp(?
(trans)yt?1,k +?j ?
(obs)j,k fj(x, t))We use transition features for every pair of labels,and extract base observation features from token tand neighboring tokens, and conjoin them againstall K = 25 possible outputs in our coarse tagset(Appendix A).
Our feature sets will be discussedbelow in detail.Decoding.
For experiments reported in this paper,we use the O(|x|K2) Viterbi algorithm for predic-tion; K is the number of tags.
This exactly max-imizes p(y | x), but the MEMM also naturally al-lows a fasterO(|x|K) left-to-right greedy decoding:for t = 1 .
.
.
|x|:y?t ?
argmaxk p(yt = k | y?t?1,x, t;?
)which we find is 3 times faster and yields similar ac-curacy as Viterbi (an insignificant accuracy decreaseof less than 0.1% absolute on the DAILY547 test setdiscussed below).
Speed is paramount for social me-dia analysis applications?which often require theprocessing of millions to billions of messages?sowe make greedy decoding the default in the releasedsoftware.2Although when compared to CRFs, MEMMs theoreticallysuffer from the ?label bias?
problem (Lafferty et al 2001), oursystem substantially outperforms the CRF-based taggers of pre-vious work; and when comparing to Gimpel et alsystem withsimilar feature sets, we observed little difference in accuracy.This is consistent with conventional wisdom that the qualityof lexical features is much more important than the paramet-ric form of the sequence model, at least in our setting: part-of-speech tagging with a small labeled training set.This greedy tagger runs at 800 tweets/sec.
(10,000tokens/sec.)
on a single CPU core, about 40 timesfaster than Gimpel et als system.
The tokenizer byitself (?4) runs at 3,500 tweets/sec.3Training and regularization.
During training,the MEMM log-likelihood for a tagged tweet ?x,y?is the sum over the observed token tags yt, each con-ditional on the tweet being tagged and the observedprevious tag (with a start symbol before the first to-ken in x),`(x,y,?)
=?|x|t=1 log p(yt | yt?1,x, t;?
).We optimize the parameters ?
with OWL-QN, anL1-capable variant of L-BFGS (Andrew and Gao,2007; Liu and Nocedal, 1989) to minimize the regu-larized objectiveargmin??
1N??x,y?
`(x,y,?)
+R(?
)where N is the number of tokens in the corpus andthe sum ranges over all tagged tweets ?x,y?
in thetraining data.
We use elastic net regularization (Zouand Hastie, 2005), which is a linear combination ofL1 and L2 penalties; here j indexes over all features:R(?)
= ?1?j |?j |+12?2?j ?2jUsing even a very small L1 penalty eliminates manyirrelevant or noisy features.3 Unsupervised Word ClustersOur POS tagger can make use of any number of pos-sibly overlapping features.
While we have only asmall amount of hand-labeled data for training, wealso have access to billions of tokens of unlabeledconversational text from the web.
Previous work hasshown that unlabeled text can be used to induce un-supervised word clusters which can improve the per-formance of many supervised NLP tasks (Koo et al2008; Turian et al 2010; T?ckstr?m et al 2012, in-ter alia).
We use a similar approach here to improvetagging performance for online conversational text.We also make our induced clusters publicly avail-able in the hope that they will be useful for otherNLP tasks in this genre.3Runtimes observed on an Intel Core i5 2.4 GHz laptop.381Binary path Top words (by frequency)A1 111010100010 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lolololA2 111010100011 haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa ahahA3 111010100100 yes yep yup nope yess yesss yessss ofcourse yeap likewise yepp yesh yw yuup yusA4 111010100101 yeah yea nah naw yeahh nooo yeh noo noooo yeaa ikr nvm yeahhh nahh noooooA5 11101011011100 smh jk #fail #random #fact smfh #smh #winning #realtalk smdh #dead #justsayingB 011101011 u yu yuh yhu uu yuu yew y0u yuhh youh yhuu iget yoy yooh yuo yue juu dya youz yyouC 11100101111001 w fo fa fr fro ov fer fir whit abou aft serie fore fah fuh w/her w/that fron isn againsD 111101011000 facebook fb itunes myspace skype ebay tumblr bbm flickr aim msn netflix pandoraE1 0011001 tryna gon finna bouta trynna boutta gne fina gonn tryina fenna qone trynaa qonE2 0011000 gonna gunna gona gna guna gnna ganna qonna gonnna gana qunna gonne goonaF 0110110111 soo sooo soooo sooooo soooooo sooooooo soooooooo sooooooooo sooooooooooG1 11101011001010 ;) :p :-) xd ;-) ;d (; :3 ;p =p :-p =)) ;] xdd #gno xddd >:) ;-p >:d 8-) ;-dG2 11101011001011 :) (: =) :)) :] :?)
=] ^_^ :))) ^.^ [: ;)) ((: ^__^ (= ^-^ :))))G3 1110101100111 :( :/ -_- -.- :-( :?
( d: :| :s -__- =( =/ >.< -___- :-/ </3 :\ -____- ;( /: :(( >_< =[ :[ #fmlG4 111010110001 <3 xoxo <33 xo <333 #love s2 <URL-twitition.com> #neversaynever <3333Figure 2: Example word clusters (HMM classes): we list the most probable words, starting with the most probable, indescending order.
Boldfaced words appear in the example tweet (Figure 1).
The binary strings are root-to-leaf pathsthrough the binary cluster tree.
For example usage, see e.g.
search.twitter.com, bing.com/social andurbandictionary.com.3.1 Clustering MethodWe obtained hierarchical word clusters via Brownclustering (Brown et al 1992) on a large set ofunlabeled tweets.4 The algorithm partitions wordsinto a base set of 1,000 clusters, and induces a hi-erarchy among those 1,000 clusters with a series ofgreedy agglomerative merges that heuristically opti-mize the likelihood of a hidden Markov model with aone-class-per-lexical-type constraint.
Not only doesBrown clustering produce effective features for dis-criminative models, but its variants are better unsu-pervised POS taggers than some models developednearly 20 years later; see comparisons in Blunsomand Cohn (2011).
The algorithm is attractive for ourpurposes since it scales to large amounts of data.When training on tweets drawn from a singleday, we observed time-specific biases (e.g., nu-merical dates appearing in the same cluster as theword tonight), so we assembled our unlabeled datafrom a random sample of 100,000 tweets per dayfrom September 10, 2008 to August 14, 2012,and filtered out non-English tweets (about 60% ofthe sample) using langid.py (Lui and Baldwin,2012).5 Each tweet was processed with our to-4As implemented by Liang (2005), v. 1.3: https://github.com/percyliang/brown-cluster5https://github.com/saffsd/langid.pykenizer and lowercased.
We normalized all at-mentions to ?@MENTION?
and URLs/email ad-dresses to their domains (e.g.
http://bit.ly/dP8rR8 ?
?URL-bit.ly?).
In an effort to reducespam, we removed duplicated tweet texts (this alsoremoves retweets) before word clustering.
Thisnormalization and cleaning resulted in 56 millionunique tweets (847 million tokens).
We set theclustering software?s count threshold to only clusterwords appearing 40 or more times, yielding 216,856word types, which took 42 hours to cluster on a sin-gle CPU.3.2 Cluster ExamplesFig.
2 shows example clusters.
Some of the chal-lenging words in the example tweet (Fig.
1) are high-lighted.
The term lololol (an extension of lol for?laughing out loud?)
is grouped with a large numberof laughter acronyms (A1: ?laughing my (fucking)ass off,?
?cracking the fuck up?).
Since expressionsof laughter are so prevalent on Twitter, the algorithmcreates another laughter cluster (A1?s sibling A2),that tends to have onomatopoeic, non-acronym vari-ants (e.g., haha).
The acronym ikr (?I know, right??
)is grouped with expressive variations of ?yes?
and?no?
(A4).
Note that A1?A4 are grouped in a fairlyspecific subtree; and indeed, in this message ikr and382lololol are both tagged as interjections.smh (?shaking my head,?
indicating disapproval)seems related, though is always tagged in the an-notated data as a miscellaneous abbreviation (G);the difference between acronyms that are interjec-tions versus other acronyms may be complicated.Here, smh is in a related but distinct subtree from theabove expressions (A5); its usage in this exampleis slightly different from its more common usage,which it shares with the other words in its cluster:message-ending expressions of commentary or emo-tional reaction, sometimes as a metacomment on theauthor?s message; e.g., Maybe you could get a guyto date you if you actually respected yourself #smhor There is really NO reason why other girls shouldsend my boyfriend a goodmorning text #justsaying.We observe many variants of categories tradition-ally considered closed-class, including pronouns (B:u = ?you?)
and prepositions (C: fir = ?for?
).There is also evidence of grammatical categoriesspecific to conversational genres of English; clustersE1?E2 demonstrate variations of single-word con-tractions for ?going to?
and ?trying to,?
some ofwhich have more complicated semantics.6Finally, the HMM learns about orthographic vari-ants, even though it treats all words as opaque sym-bols; cluster F consists almost entirely of variantsof ?so,?
their frequencies monotonically decreasingin the number of vowel repetitions?a phenomenoncalled ?expressive lengthening?
or ?affective length-ening?
(Brody and Diakopoulos, 2011; Schnoebe-len, 2012).
This suggests a future direction to jointlymodel class sequence and orthographic informa-tion (Clark, 2003; Smith and Eisner, 2005; Blunsomand Cohn, 2011).We have built an HTML viewer to browse theseand numerous other interesting examples.73.3 Emoticons and EmojiWe use the term emoticon to mean a face or iconconstructed with traditional alphabetic or punctua-6One coauthor, a native speaker of the Texan English dialect,notes ?finna?
(short for ?fixing to?, cluster E1) may be an im-mediate future auxiliary, indicating an immediate future tensethat is present in many languages (though not in standard En-glish).
To illustrate: ?She finna go?
approximately means ?Shewill go,?
but sooner, in the sense of ?She is about to go.
?7http://www.ark.cs.cmu.edu/TweetNLP/cluster_viewer.htmltion symbols, and emoji to mean symbols renderedin software as small pictures, in line with the text.Since our tokenizer is careful to preserve emoti-cons and other symbols (see ?4), they are clusteredjust like other words.
Similar emoticons are clus-tered together (G1?G4), including separate clustersof happy [[ :) =) ?_?
]], sad/disappointed [[ :/ :(-_- </3 ]], love [[ ?xoxo ?.? ]]
and winking [[;) (?_-) ]] emoticons.
The clusters are not per-fectly aligned with our POS annotation guidelines;for example, the ?sad?
emoticon cluster includedemotion-bearing terms that our guidelines define asnon-emoticons, such as #ugh, #tear, and #fml (?fuckmy life?
), though these seem potentially useful forsentiment analysis.One difficult task is classifying different typesof symbols in tweets: our annotation guidelinesdifferentiate between emoticons, punctuation, andgarbage (apparently non-meaningful symbols or to-kenization errors).
Several Unicode character rangesare reserved for emoji-style symbols (including thethree Unicode hearts in G4); however, dependingon the user?s software, characters in these rangesmight be rendered differently or not at all.
Wehave found instances where the clustering algo-rithm groups proprietary iOS emoji symbols alongwith normal emoticons; for example, the characterU+E056, which is interpreted on iOS as a smilingface, is in the same G2 cluster as smiley face emoti-cons.
The symbol U+E12F, which represents a pic-ture of a bag of money, is grouped with the wordscash and money.3.4 Cluster-Based FeaturesSince Brown clusters are hierarchical in a binarytree, each word is associated with a tree path rep-resented as a bitstring with length ?
16; we use pre-fixes of the bitstring as features (for all prefix lengths?
{2, 4, 6, .
.
.
, 16}).
This allows sharing of statisti-cal strength between similar clusters.
Using prefixfeatures of hierarchical clusters in this way was sim-ilarly found to be effective for named-entity recog-nition (Turian et al 2010) and Twitter POS tag-ging (Ritter et al 2011).When checking to see if a word is associated witha cluster, the tagger first normalizes the word usingthe same techniques as described in ?3.1, then cre-ates a priority list of fuzzy match transformations383of the word by removing repeated punctuation andrepeated characters.
If the normalized word is notin a cluster, the tagger considers the fuzzy matches.Although only about 3% of the tokens in the devel-opment set (?6) did not appear in a clustering, thismethod resulted in a relative error decrease of 18%among such word tokens.3.5 Other Lexical FeaturesBesides unsupervised word clusters, there are twoother sets of features that contain generalized lexi-cal class information.
We use the tag dictionary fea-ture from Gimpel et al which adds a feature fora word?s most frequent part-of-speech tag.8 Thiscan be viewed as a feature-based domain adaptationmethod, since it gives lexical type-level informationfor standard English words, which the model learnsto map between PTB tags to the desired output tags.Second, since the lack of consistent capitalizationconventions on Twitter makes it especially difficultto recognize names?Gimpel et aland Foster etal.
(2011) found relatively low accuracy on propernouns?we added a token-level name list feature,which fires on (non-function) words from namesfrom several sources: Freebase lists of celebritiesand video games (Google, 2012), the Moby Wordslist of US Locations,9 and lists of male, female, fam-ily, and proper names from Mark Kantrowitz?s namecorpus.104 Tokenization and Emoticon DetectionWord segmentation on Twitter is challenging dueto the lack of orthographic conventions; in partic-ular, punctuation, emoticons, URLs, and other sym-bols may have no whitespace separation from textual8Frequencies came from the Wall Street Journal and Browncorpus sections of the Penn Treebank.
If a word has multiplePTB tags, each tag is a feature with value for the frequency rank;e.g.
for three different tags in the PTB, this feature gives a valueof 1 for the most frequent tag, 2/3 for the second, etc.
Coarseversions of the PTB tags are used (Petrov et al 2011).
While88% of words in the dictionary have only one tag, using rankinformation seemed to give a small but consistent gain over onlyusing the most common tag, or using binary features conjoinedwith rank as in Gimpel et al9http://icon.shef.ac.uk/Moby/mwords.html10http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/0.htmlwords (e.g.
no:-d,yes should parse as four tokens),and internally may contain alphanumeric symbolsthat could be mistaken for words: a naive split(/[^a-zA-Z0-9]+/) tokenizer thinks the words ?p?
and ?d?are among the top 100 most common words on Twit-ter, due to misanalysis of :p and :d. Traditional PennTreebank?style tokenizers are hardly better, oftenbreaking a string of punctuation characters into asingle token per character.We rewrote twokenize (O?Connor et al2010), a rule-based tokenizer, emoticon, and URLdetector, for use in the tagger.
Emoticons are es-pecially challenging, since they are open-class andproductive.
We revise O?Connor et als regular ex-pression grammar that describes possible emoticons,adding a grammar of horizontal emoticons (e.g.
-_-),known as ?Eastern-style,?11 though we observe highusage in English-speaking Twitter (Fig.
2, G2?G3).We also add a number of other improvements to thepatterns.
Because this system was used as prepro-cessing for the word clustering experiment in ?3, wewere able to infer the emoticon clusters in Fig.
2.Furthermore, whether a token matches the emoticonpattern is also used as a feature in the tagger (?2).URL recognition is also difficult, since the http://is often dropped, resulting in protocol-less URLslike about.me.
We add recognition patterns for theseby using a list of top-level and country domains.5 Annotated DatasetGimpel et al(2011) provided a dataset of POS-tagged tweets consisting almost entirely of tweetssampled from one particular day (October 27,2010).
We were concerned about overfitting to time-specific phenomena; for example, a substantial frac-tion of the messages are about a basketball gamehappening that day.We created a new test set of 547 tweets for eval-uation.
The test set consists of one random Englishtweet from every day between January 1, 2011 andJune 30, 2012.
In order for a tweet to be consideredEnglish, it had to contain at least one English wordother than a URL, emoticon, or at-mention.
We no-ticed biases in the outputs of langid.py, so weinstead selected these messages completely manu-11http://en.wikipedia.org/wiki/List_of_emoticons384ally (going through a random sample of one day?smessages until an English message was found).5.1 Annotation MethodologyGimpel et alprovided a tagset for Twitter (shown inAppendix A), which we used unmodified.
The orig-inal annotation guidelines were not published, but inthis work we recorded the rules governing taggingdecisions and made further revisions while annotat-ing the new data.12 Some of our guidelines reiter-ate or modify rules made by Penn Treebank annota-tors, while others treat specific phenomena found onTwitter (refer to the next section).Our tweets were annotated by two annotators whoattempted to match the choices made in Gimpel etal.
?s dataset.
The annotators also consulted the POSannotations in the Penn Treebank (Marcus et al1993) as an additional reference.
Differences werereconciled by a third annotator in discussion with allannotators.13 During this process, an inconsistencywas found in Gimpel et als data, which we cor-rected (concerning the tagging of this/that, a changeto 100 labels, 0.4%).
The new version of Gimpel etal.
?s data (called OCT27), as well as the newer mes-sages (called DAILY547), are both included in ourdata release.5.2 Compounds in Penn Treebank vs. TwitterRitter et al(2011) annotated tweets using an aug-mented version of the PTB tagset and presumablyfollowed the PTB annotation guidelines.
We wrotenew guidelines because the PTB conventions are in-appropriate for Twitter in several ways, as shown inthe design of Gimpel et als tagset.
Importantly,?compound?
tags (e.g., nominal+verbal and nomi-nal+possessive) are used because tokenization is dif-ficult or seemingly impossible for the nonstandardword forms that are commonplace in conversationaltext.For example, the PTB tokenization splits contrac-tions containing apostrophes: I?m?
I/PRP ?m/VBP.But conversational text often contains variants thatresist a single PTB tag (like im), or even chal-lenge traditional English grammatical categories12The annotation guidelines are available online athttp://www.ark.cs.cmu.edu/TweetNLP/13Annotators are coauthors of this paper.
(like imma or umma, which both mean ?I am go-ing to?).
One strategy would be to analyze theseforms into a PTB-style tokenization, as discussed inForsyth (2007), who proposes to analyze doncha asdo/VBP ncha/PRP, but notes it would be difficult.We think this is impossible to handle in the rule-based framework used by English tokenizers, giventhe huge (and possibly growing) number of largecompounds like imma, gonna, w/that, etc.
Theseare not rare: the word clustering algorithm discov-ers hundreds of such words as statistically coherentclasses (e.g.
clusters E1 and E2 in Fig.
2); and theword imma is the 962nd most common word in ourunlabeled corpus, more frequent than cat or near.We do not attempt to do Twitter ?normalization?into traditional written English (Han and Baldwin,2011), which we view as a lossy translation task.
Infact, many of Twitter?s unique linguistic phenomenaare due not only to its informal nature, but also a setof authors that heavily skews towards younger agesand minorities, with heavy usage of dialects that aredifferent than the standard American English mostoften seen in NLP datasets (Eisenstein, 2013; Eisen-stein et al 2011).
For example, we suspect thatimma may implicate tense and aspect markers fromAfrican-American Vernacular English.14 Trying toimpose PTB-style tokenization on Twitter is linguis-tically inappropriate: should the lexico-syntactic be-havior of casual conversational chatter by young mi-norities be straightjacketed into the stylistic conven-tions of the 1980s Wall Street Journal?
Instead, wewould like to directly analyze the syntax of onlineconversational text on its own terms.Thus, we choose to leave these word forms un-tokenized and use compound tags, viewing com-positional multiword analysis as challenging fu-ture work.15 We believe that our strategy is suf-ficient for many applications, such as chunking ornamed entity recognition; many applications suchas sentiment analysis (Turney, 2002; Pang and Lee,2008, ?4.2.3), open information extraction (Carl-son et al 2010; Fader et al 2011), and informa-tion retrieval (Allan and Raghavan, 2002) use POS14See ?Tense and aspect?
examples in http://en.wikipedia.org/wiki/African_American_Vernacular_English15For example, wtf has compositional behavior in ?Wtf justhappened??
?, but only debatably so in ?Huh wtf?.385#Msg.
#Tok.
Tagset DatesOCT27 1,827 26,594 App.
A Oct 27-28, 2010DAILY547 547 7,707 App.
A Jan 2011?Jun 2012NPSCHAT 10,578 44,997 PTB-like Oct?Nov 2006(w/o sys.
msg.)
7,935 37,081RITTERTW 789 15,185 PTB-like unknownTable 1: Annotated datasets: number of messages, to-kens, tagset, and date range.
More information in ?5,?6.3, and ?6.2.patterns that seem quite compatible with our ap-proach.
More complex downstream processing likeparsing is an interesting challenge, since contractionparsing on traditional text is probably a benefit tocurrent parsers.
We believe that any PTB-trainedtool requires substantial retraining and adaptationfor Twitter due to the huge genre and stylistic differ-ences (Foster et al 2011); thus tokenization conven-tions are a relatively minor concern.
Our simple-to-annotate conventions make it easier to produce newtraining data.6 ExperimentsWe are primarily concerned with performance onour annotated datasets described in ?5 (OCT27,DAILY547), though for comparison to previouswork we also test on other corpora (RITTERTW in?6.2, NPSCHAT in ?6.3).
The annotated datasetsare listed in Table 1.6.1 Main ExperimentsWe use OCT27 to refer to the entire dataset de-scribed in Gimpel et al it is split into train-ing, development, and test portions (OCT27TRAIN,OCT27DEV, OCT27TEST).
We use DAILY547 asan additional test set.
Neither OCT27TEST norDAILY547 were extensively evaluated against untilfinal ablation testing when writing this paper.The total number of features is 3.7 million, allof which are used under pure L2 regularization; butonly 60,000 are selected by elastic net regularizationwith (?1, ?2) = (0.25, 2), which achieves nearly thesame (but no better) accuracy as pure L2,16 and weuse it for all experiments.
We observed that it was16We conducted a grid search for the regularizer values onpart of DAILY547, and many regularizer values give the best ornearly the best results.
We suspect a different setup would haveyielded similar results.llll l l l1e+03 1e+05 1e+0775808590Number of Unlabeled TweetsTaggingAccuracylll ll l l1e+03 1e+05 1e+070.600.650.70Number of Unlabeled TweetsTokenCoverageFigure 3: OCT27 development set accuracy using onlyclusters as features.Model In dict.
Out of dict.Full 93.4 85.0No clusters 92.0 (?1.4) 79.3 (?5.7)Total tokens 4,808 1,394Table 3: DAILY547 accuracies (%) for tokens in and outof a traditional dictionary, for models reported in rows 1and 3 of Table 2.possible to get radically smaller models with onlya slight degradation in performance: (4, 0.06) has0.5% worse accuracy but uses only 1,632 features, asmall enough number to browse through manually.First, we evaluate on the new test set, training onall of OCT27.
Due to DAILY547?s statistical repre-sentativeness, we believe this gives the best view ofthe tagger?s accuracy on English Twitter text.
Thefull tagger attains 93.2% accuracy (final row of Ta-ble 2).To facilitate comparisons with previous work, weran a series of experiments training only on OCT27?straining and development sets, then report test re-sults on both OCT27TEST and all of DAILY547,shown in Table 2.
Our tagger achieves substantiallyhigher accuracy than Gimpel et al(2011).17Feature ablation.
A number of ablation tests in-dicate the word clusters are a very strong source oflexical knowledge.
When dropping the tag dictio-naries and name lists, the word clusters maintainmost of the accuracy (row 2).
If we drop the clus-ters and rely only on tag dictionaries and namelists,accuracy decreases significantly (row 3).
In fact,we can remove all observation features except forword clusters?no word features, orthographic fea-17These numbers differ slightly from those reported by Gim-pel et al due to the corrections we made to the OCT27 data,noted in Section 5.1.
We retrained and evaluated their tagger(version 0.2) on our corrected dataset.386Feature set OCT27TEST DAILY547 NPSCHATTESTAll features 91.60 92.80 91.19 1with clusters; without tagdicts, namelists 91.15 92.38 90.66 2without clusters; with tagdicts, namelists 89.81 90.81 90.00 3only clusters (and transitions) 89.50 90.54 89.55 4without clusters, tagdicts, namelists 86.86 88.30 88.26 5Gimpel et al(2011) version 0.2 88.89 89.17 6Inter-annotator agreement (Gimpel et al 2011) 92.2 7Model trained on all OCT27 93.2 8Table 2: Tagging accuracies (%) in ablation experiments.
OCT27TEST and DAILY547 95% confidence intervals areroughly ?0.7%.
Our final tagger uses all features and also trains on OCT27TEST, achieving 93.2% on DAILY547.tures, affix n-grams, capitalization, emoticon pat-terns, etc.
?and the accuracy is in fact still betterthan the previous work (row 4).18We also wanted to know whether to keep the tagdictionary and name list features, but the splits re-ported in Fig.
2 did not show statistically signifi-cant differences; so to better discriminate betweenablations, we created a lopsided train/test split ofall data with a much larger test portion (26,974 to-kens), having greater statistical power (tighter con-fidence intervals of ?
0.3%).19 The full system got90.8% while the no?tag dictionary, no-namelists ab-lation had 90.0%, a statistically significant differ-ence.
Therefore we retain these features.Compared to the tagger in Gimpel et al most ofour feature changes are in the new lexical featuresdescribed in ?3.5.20 We do not reuse the other lex-ical features from the previous work, including aphonetic normalizer (Metaphone), a name list con-sisting of words that are frequently capitalized, anddistributional features trained on a much smaller un-labeled corpus; they are all worse than our newlexical features described here.
(We did include,however, a variant of the tag dictionary feature thatuses phonetic normalization for lookup; it seemed toyield a small improvement.
)18Furthermore, when evaluating the clusters as unsupervised(hard) POS tags, we obtain a many-to-one accuracy of 89.2%on DAILY547.
Before computing this, we lowercased the textto match the clusters and removed tokens tagged as URLs andat-mentions.19Reported confidence intervals in this paper are 95% bino-mial normal approximation intervals for the proportion of cor-rectly tagged tokens: ?1.96?p(1?
p)/ntokens .
1/?n.20Details on the exact feature set are available in a technicalreport (Owoputi et al 2012), also available on the website.Non-traditional words.
The word clusters are es-pecially helpful with words that do not appear in tra-ditional dictionaries.
We constructed a dictionaryby lowercasing the union of the ispell ?American?,?British?, and ?English?
dictionaries, plus the stan-dard Unix words file from Webster?s Second Inter-national dictionary, totalling 260,985 word types.After excluding tokens defined by the gold stan-dard as punctuation, URLs, at-mentions, or emoti-cons,21 22% of DAILY547?s tokens do not appear inthis dictionary.
Without clusters, they are very dif-ficult to classify (only 79.2% accuracy), but addingclusters generates a 5.7 point improvement?muchlarger than the effect on in-dictionary tokens (Ta-ble 3).Varying the amount of unlabeled data.
A taggerthat only uses word clusters achieves an accuracy of88.6% on the OCT27 development set.22 We createdseveral clusterings with different numbers of unla-beled tweets, keeping the number of clusters con-stant at 800.
As shown in Fig.
3, there was initiallya logarithmic relationship between number of tweetsand accuracy, but accuracy (and lexical coverage)levels out after 750,000 tweets.
We use the largestclustering (56 million tweets and 1,000 clusters) asthe default for the released tagger.6.2 Evaluation on RITTERTWRitter et al(2011) annotated a corpus of 787tweets23 with a single annotator, using the PTB21We retain hashtags since by our guidelines a #-prefixed to-ken is ambiguous between a hashtag and a normal word, e.g.
#1or going #home.22The only observation features are the word clusters of atoken and its immediate neighbors.23https://github.com/aritter/twitter_nlp/blob/master/data/annotated/pos.txt387Tagger AccuracyThis work 90.0 ?
0.5Ritter et al(2011), basic CRF tagger 85.3Ritter et al(2011), trained on more data 88.3Table 4: Accuracy comparison on Ritter et als TwitterPOS corpus (?6.2).Tagger AccuracyThis work 93.4 ?
0.3Forsyth (2007) 90.8Table 5: Accuracy comparison on Forsyth?s NPSCHATIRC POS corpus (?6.3).tagset plus several Twitter-specific tags, referredto in Table 1 as RITTERTW.
Linguistic concernsnotwithstanding (?5.2), for a controlled comparison,we train and test our system on this data with thesame 4-fold cross-validation setup they used, attain-ing 90.0% (?0.5%) accuracy.
Ritter et als CRF-based tagger had 85.3% accuracy, and their best tag-ger, trained on a concatenation of PTB, IRC, andTwitter, achieved 88.3% (Table 4).6.3 IRC: Evaluation on NPSCHATIRC is another medium of online conversationaltext, with similar emoticons, misspellings, abbrevi-ations and acronyms as Twitter data.
We evaluateour tagger on the NPS Chat Corpus (Forsyth andMartell, 2007),24 a PTB-part-of-speech annotateddataset of Internet Relay Chat (IRC) room messagesfrom 2006.First, we compare to a tagger in the same setup asexperiments on this data in Forsyth (2007), trainingon 90% of the data and testing on 10%; we averageresults across 10-fold cross-validation.25 The fulltagger model achieved 93.4% (?0.3%) accuracy,significantly improving over the best result they re-port, 90.8% accuracy with a tagger trained on a mixof several POS-annotated corpora.We also perform the ablation experiments on thiscorpus, with a slightly different experimental setup:we first filter out system messages then split data24Release 1.0: http://faculty.nps.edu/cmartell/NPSChat.htm25Forsyth actually used 30 different 90/10 random splits; weprefer cross-validation because the same test data is never re-peated, thus allowing straightforward confidence estimation ofaccuracy from the number of tokens (via binomial sample vari-ance, footnote 19).
In all cases, the models are trained on thesame amount of data (90%).into 5,067 training and 2,868 test messages.
Resultsshow a similar pattern as the Twitter data (see finalcolumn of Table 2).
Thus the Twitter word clustersare also useful for language in the medium of textchat rooms; we suspect these clusters will be appli-cable for deeper syntactic and semantic analysis inother online conversational text mediums, such astext messages and instant messages.7 ConclusionWe have constructed a state-of-the-art part-of-speech tagger for the online conversational textgenres of Twitter and IRC, and have publicly re-leased our new evaluation data, annotation guide-lines, open-source tagger, and word clusters athttp://www.ark.cs.cmu.edu/TweetNLP.AcknowledgementsThis research was supported in part by the National Sci-ence Foundation (IIS-0915187 and IIS-1054319).A Part-of-Speech TagsetN common nounO pronoun (personal/WH; not possessive)^ proper nounS nominal + possessiveZ proper noun + possessiveV verb including copula, auxiliariesL nominal + verbal (e.g.
i?m), verbal + nominal (let?s)M proper noun + verbalA adjectiveR adverb!
interjectionD determinerP pre- or postposition, or subordinating conjunction& coordinating conjunctionT verb particleX existential there, predeterminersY X + verbal# hashtag (indicates topic/category for tweet)@ at-mention (indicates a user as a recipient of a tweet)~ discourse marker, indications of continuation acrossmultiple tweetsU URL or email addressE emoticon$ numeral, punctuationG other abbreviations, foreign words, possessive endings,symbols, garbageTable 6: POS tagset from Gimpel et al(2011) used in thispaper, and described further in the released annotationguidelines.388ReferencesJ.
Allan and H. Raghavan.
2002.
Using part-of-speechpatterns to reduce query ambiguity.
In Proc.
of SIGIR.G.
Andrew and J. Gao.
2007.
Scalable training of L1-regularized log-linear models.
In Proc.
of ICML.P.
Blunsom and T. Cohn.
2011.
A hierarchical Pitman-Yor process HMM for unsupervised part of speech in-duction.
In Proc.
of ACL.S.
Brody and N. Diakopoulos.
2011.Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!
: usingword lengthening to detect sentiment in microblogs.In Proc.
of EMNLP.P.
F. Brown, P. V. de Souza, R. L. Mercer, V. J.Della Pietra, and J. C. Lai.
1992.
Class-based n-grammodels of natural language.
Computational Linguis-tics, 18(4).A.
Carlson, J. Betteridge, B. Kisiel, B.
Settles, E. R. Hr-uschka Jr, and T. M. Mitchell.
2010.
Toward an archi-tecture for never-ending language learning.
In Proc.
ofAAAI.A.
Clark.
2003.
Combining distributional and morpho-logical information for part of speech induction.
InProc.
of EACL.J.
Eisenstein, N. A. Smith, and E. P. Xing.
2011.
Discov-ering sociolinguistic associations with structured spar-sity.
In Proc.
of ACL.J.
Eisenstein.
2013.
What to do about bad language onthe internet.
In Proc.
of NAACL.A.
Fader, S. Soderland, and O. Etzioni.
2011.
Identifyingrelations for open information extraction.
In Proc.
ofEMNLP.E.
N. Forsyth and C. H. Martell.
2007.
Lexical and dis-course analysis of online chat dialog.
In Proc.
of ICSC.E.
N. Forsyth.
2007.
Improving automated lexical anddiscourse analysis of online chat dialog.
Master?s the-sis, Naval Postgraduate School.J.
Foster, O. Cetinoglu, J. Wagner, J. L. Roux, S. Hogan,J.
Nivre, D. Hogan, and J. van Genabith.
2011.
#hard-toparse: POS tagging and parsing the Twitterverse.
InProc.
of AAAI-11 Workshop on Analysing Microtext.K.
Gimpel, N. Schneider, B. O?Connor, D. Das, D. Mills,J.
Eisenstein, M. Heilman, D. Yogatama, J. Flanigan,and N. A. Smith.
2011.
Part-of-speech tagging forTwitter: Annotation, features, and experiments.
InProc.
of ACL.Google.
2012.
Freebase data dumps.
http://download.freebase.com/datadumps/.B.
Han and T. Baldwin.
2011.
Lexical normalisation ofshort text messages: Makn sens a #twitter.
In Proc.
ofACL.T.
Koo, X. Carreras, and M. Collins.
2008.
Simple semi-supervised dependency parsing.
In Proc.
of ACL.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
of ICML.P.
Liang.
2005.
Semi-supervised learning for naturallanguage.
Master?s thesis, Massachusetts Institute ofTechnology.D.
C. Liu and J. Nocedal.
1989.
On the limited memoryBFGS method for large scale optimization.
Mathemat-ical programming, 45(1).X.
Liu, S. Zhang, F. Wei, and M. Zhou.
2011.
Recogniz-ing named entities in tweets.
In Proc.
of ACL.M.
Lui and T. Baldwin.
2012. langid.py: An off-the-shelf language identification tool.
In Proc.
of ACL.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2).A.
McCallum, D. Freitag, and F. Pereira.
2000.
Maxi-mum entropy Markov models for information extrac-tion and segmentation.
In Proc.
of ICML.D.
McClosky, E. Charniak, and M. Johnson.
2010.
Au-tomatic domain adaptation for parsing.
In Proc.
ofNAACL.B.
O?Connor, M. Krieger, and D. Ahn.
2010.TweetMotif: exploratory search and topic summariza-tion for Twitter.
In Proc.
of AAAI Conference on We-blogs and Social Media.O.
Owoputi, B. O?Connor, C. Dyer, K. Gimpel, andN.
Schneider.
2012.
Part-of-speech tagging for Twit-ter: Word clusters and other advances.
Technical Re-port CMU-ML-12-107, Carnegie Mellon University.B.
Pang and L. Lee.
2008.
Opinion mining and sentimentanalysis.
Now Publishers.S.
Petrov and R. McDonald.
2012.
Overview of the 2012shared task on parsing the web.
Notes of the FirstWorkshop on Syntactic Analysis of Non-CanonicalLanguage (SANCL).S.
Petrov, D. Das, and R. McDonald.
2011.
Auniversal part-of-speech tagset.
arXiv preprintarXiv:1104.2086.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proc.
of EMNLP.A.
Ritter, S. Clark, Mausam, and O. Etzioni.
2011.Named entity recognition in tweets: An experimentalstudy.
In Proc.
of EMNLP.T.
Schnoebelen.
2012.
Do you smile with your nose?Stylistic variation in Twitter emoticons.
University ofPennsylvania Working Papers in Linguistics, 18(2):14.N.
A. Smith and J. Eisner.
2005.
Contrastive estimation:Training log-linear models on unlabeled data.
In Proc.of ACL.O.
T?ckstr?m, R. McDonald, and J. Uszkoreit.
2012.Cross-lingual word clusters for direct transfer of lin-guistic structure.
In Proc.
of NAACL.389J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: A simple and general method for semi-supervised learning.
In Proc.
of ACL.P.
D. Turney.
2002.
Thumbs up or thumbs down?
: se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proc.
of ACL.H.
Zou and T. Hastie.
2005.
Regularization and vari-able selection via the elastic net.
Journal of the RoyalStatistical Society: Series B (Statistical Methodology),67(2):301?320.390
