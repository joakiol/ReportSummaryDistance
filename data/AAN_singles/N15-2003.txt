Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 17?24,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsAnalyzing Newspaper Crime Reports for Identification of Safe Transit PathsVasu Sharma1Rajat Kulshreshtha2Puneet Singh1Nishant Agrawal3Akshay Kumar4Indian Institute Of Technology,Kanpur1IIT,Guwahati2IIIT,Hyderabad3VIT Chennai4vasus@iitk.ac.in rk.kuls@gmail.com pun.singh92@gmail.com nash007@gmail.comakshay.kumar2011@vit.ac.inAbstractIn this paper, we propose a method to find thesafest path between two locations, based onthe geographical model of crime intensities.We consider the police records and news arti-cles for finding crime density of different areasof the city.
It is essential to consider news arti-cles as there is a significant delay in updatingpolice crime records.
We address this prob-lem by updating the crime intensities basedon current news feeds.
Based on the updatedcrime intensities, we identify the safest path.It is this real time updation of crime intensitieswhich makes our model way better than themodels that are presently in use.
Our modelwould also inform the user of crime spreesin a particular area thereby ensuring that useravoids these crime hot spots.Keywords: Crime detection, Hotspot identifi-cation, Safest Path, Topic Modeling, LatentDirichlet Allocation, Latent Semantic Analy-sis, Natural Language Processing.1 IntroductionIn today?s society, reports of criminal activity are onthe rise.
Newspapers each day are replete with newsarticles about incidents of crime from different partsof our cities.
Crime is not spread evenly across acity, the level of criminal activity varies with region.In traveling from one spot to another within a city,people naturally desire not to be a victim to criminalactivity.
In general, the likelihood of falling victimto criminal activity is greater in areas with elevatedcrime levels, hence the path one travels must prefer-entially avoid areas with higher levels of crime.Our objective in this paper, is to find the safestpossible path between any two points on the streetmap, based on actual or inferred knowledge of priorcriminal activity.
The map may be viewed as agraph, where junctions are vertices in the graph, andstreets are edges.
The problem of finding a pathfrom an origin to a destination is simply that of find-ing a path between the corresponding vertices in thegraph.
For the purpose of this paper we have focusedon the city of New Delhi, India, a city which has re-cently gained notoriety as being particularly unsafefor commuters especially women.We can now cast our ?safest-path?
problem as agraph search problem.
Each vertex and edge in thegraph can be assigned a risk.
The safest path be-tween junction A and junction B is the least riskypath, or, assuming the risk to be a cost, the least-costpath between the graph vertices a and b.
Thus nowwe can restate the problem as finding the least-costpath between vertices.Given a graph, the algorithm for finding the least-cost path between vertices is well known.
We use thewell known Dijkstra?s algorithm (Dijkstra, 1959).The greater challenge now is that of specifying thegraph.
The structure of the graph, as mentioned ear-lier, is simply the street map of the city.
The realchallenge becomes that of assigning costs to the ver-tices and edges, which reflect the risk of crime in thejunctions and streets they represent.
We will do soby assigning the cumulative count of the number ofinstances of crime that were reported at any street orjunction as the cost of the corresponding edge.We do not have a direct way of assigning thesecosts, since detailed, updated crime information is17generally not available for the city.
So we will tryto infer this information using a variety of sources.We will use police records to assign costs based onhistorical data, and to compute a priori informationfor further inference.
For more updated scores, wemine the newspaper reports.
However mining news-paper articles is not easy, since the articles are in nat-ural language.
Moreover, they are often imprecisein locating the reported crimes and don?t specify theroads or the junctions.
So, we use a Bayesian for-malism to determine the locations from the article.Following the above mentioned steps, we can as-sign costs to our graph and thus find the safest pathbetween any two locations.
However, for simplic-ity we have not considered the actual road networksfor finding the path, but do so based on neighbor-hoods, which we then map on to the road network.Our results show that we are able to infer locationfrom newspapers reports with relatively high accu-racy, and that moreover, the hypothesized paths arehighly plausible.The paper is organized as follows.
Related liter-ature is reviewed in Section 2.
Section 3 presentsour data collection strategy.
Sections 4-7 present de-tailed methodology.
Results and discussion are pre-sented in Section 8.
Our conclusions are presentedin Section 9.2 Literature ReviewThe majority of the literature on crime-data miningfocuses on analyzing data and crime records to iden-tify patterns and predict crime.
(Chen et al , 2004)propose a generic machine learning framework forvarious clustering and inference tasks that may beused to detect or predict crime based on observedactivity.
Other traditional approaches to crime datamining focus on finding relations between attributesof the crimes, or finding hot-spots from a set ofcrime incidents.
Another approach to detect patternswithin police records was presented in (Sukanya etal.
, 2012), where an expert based semi-supervisedlearning method was used to cluster police crimerecords based on their attributes.
Weights were in-troduced to the attributes, and various patterns wereidentified from subsets of attributes.
A step furtherin this direction is crime forecasting, which was pre-sented in (Yu et al , 2011), which developed amodel in collaboration with police, aiming at pre-dicting the location, time and likelihood of futureresidential burglary based on temporal and spacial(over grids) information taken from police records.Various classification techniques are used to developa model that best relates attributes to crimes.
This ispertinent to our model, as the group has investigateddata mining techniques to forecast crime.For our purpose, it is sufficient to identify newsarticles that pertain to relevant criminal activity, andfind the distribution of such crimes across the city.Our challenge, then, is to automatically identifynews articles that relate to specific types of crime,and to automatically locate the crime that is reportedwith sufficient specificity that we can build a ?path-safety map?.
As it turns out, little of the literatureon crime-data mining actually directly relates to thistask.
The task of identifying news reports has itsclosest analog in the literature on document classi-fication (Sebastiani , 2002), although we have notspecifically encountered many that relate in particu-lar to crime data.
(Chau et al , 2002) report on theuse of machine learning algorithms to derive namedentities from formal police reports, but do not spec-ify the techniques used.
As we see later from ourwork, we do not require sophisticated algorithms;simple classifiers can do this particular task quite ef-fectively.The current literature on crime mapping, e.g.
(Maltz et al , 2000) , (Leong et al , 2000) does notsignificantly address the issue of generating mapsfrom free-form text report.
An interesting idea isdivision of the city into a grid, which is an intuitivemethod of quantizing the locations.
In our model,we have assumed police stations to be a strong indi-cator of population (and consequently crime) den-sity, and have mapped each locality to it?s policestation.
Perhaps the most relevant work is done in(Mahendiran et al , 2011), where the problem ofidentifying patterns in combined data sources is ap-proached by inferring clusters from spatial and tem-poral distribution.
Bayesian Belief Networks areused to find a probabilistic relation between crime,time and location.
Creation of a ?heat map?
to repre-sent unsafe areas was suggested but not part of thisreport.
The group has collated crime reports fromvarious websites.
The distinguishing feature of ourmodel is that we have combined crime reports and18news feeds, and that we mapped our crime distribu-tion into a graph with edges weighted according tocrime intensities.3 Data CollectionFor the experiments reported in this paper we focuson Delhi, India, a city which has recently acquiredsome notoriety because of the spate of crimes re-ported from there, particularly against women.
Thisrecent notoriety particularly motivates people to tryto find safe routes from origin to destination, makingthe solution reported in this paper especially relevantthere.In our proposed system, we gather data from dis-parate sources such as the reports from the Delhi Po-lice Website1.
Here we have ignored the gravity ofcrime and used only the number of crimes for al-locating a cost to a location.
We have used 42768police crime records over a period of 3 years forthe state of Delhi to form our historical prior.
Weparse the records and extract the location and typeof crime from the records.
We now tag the recordsto their nearest police station and maintain countsof the number of crimes committed in the jurisdic-tion area of every police station.
This count is whatwe have considered as ?crime intensity?
for that area.These are used to derive the a priori probability dis-tribution of crime in the various precincts.
A total of162 locations were considered, one for each policestation in Delhi.We used a web crawler to obtain news articlesfrom various news paper websites2to get crime re-lated news articles.
A total of 32000 news articleswere obtained using the crawler out of which halfwere crime related and the other half were not crimerelated.
These articles formed the prior for our k-nearest neighbor and LDA based approach used forclassification as crime/non-crime and location iden-tification described in the later sections.1The police recoreds were obtained from : http://delhipolice.serverpeople.com/firwebtemp/Index.aspx2The newspaper articles were obtained from:?
http://timesofindia.indiatimes.com/(Times of India online portal)?
http://indiatoday.intoday.in/ (India Todaynews portal)?
http://www.ndtv.com (NDTV news portal)4 Classification of Article as Crime or NonCrimeThe news articles picked from news paper websitesare not annotated.
Besides, we are concerned onlywith crimes which affect safety of a person travel-ing through that region.
For example cyber crimes,suicides , etc., do not affect the safety of a persontraveling through a region and should not be classi-fied as commuter affecting crimes by the model.Therefore, in order to proceed with the ?safety-map?
generation, we must first classify the news ar-ticles as ?crime?
or ?non-crime?.
We find that thelanguage used to refer to such crime in the news ar-ticles is diverse, ranging from direct to oblique ref-erences.
Even among the direct references, a varietyof different vocabularies and constructs may be em-ployed.
Direct analysis of language and vocabularymay consequently require complicated classificationschemes to account for all possible variations.Instead, we work on a simpler hypothesis ?
wehypothesize that regardless of the manner in whichthe crimes are being referred to, there exist underly-ing semantic levels at which they are all similar, andthat by expressing the documents in terms of theirrepresentation within these levels, we must be ableto perform the requisite classification relatively sim-ply.Uncovering the underlying semantic structuremust be performed in an unsupervised manner.
Avariety of statistical models such as latent seman-tic analysis, probabilistic latent semantic analysis(Hoffmann , 1999), latent Dirichlet alocation (Bleiet al , 2003) etc.
have been proposed for this pur-pose.
We employ a relatively lightweight, simplealgorithm, latent semantic analysis (LSA)(Dumais, 2004).
LSA is a singular-value decomposition(SVD) (Kumar , 2009) based statistical model ofword usage that attempts to recover abstract equiv-alents of semantic structure from the co-occurrencestatistics of words in documents.
Given a collec-tion of documents, it first composes a term-countmatrix, where each column represents a document,and each row represents a particular word.
The totalnumber of columns represents the number of docu-ments in the collection being analyzed, and the to-tal number of rows represents the ?vocabulary?
ofwords being considered.
The (i, j)thentry of the19term-count matrix represents the number of timesthe ithword in the vocabulary occurs in the jthdoc-ument.
The term-count matrix is decomposed usingSVD.
The M most significant left singular vectorsrecovered, corresponding to the M highest singularvalues, are assumed to represent the M directions ofthe underlying latent semantic space.
Any documentcan be represented in this space as the projection ofthe term-count vector of the document (comprisinga vector of counts of the words from the vocabularyin the document) onto the set of M singular vectors.The projection is assumed to exist in the correspond-ing semantic space.To compute our model, we first stem our corpus,and eliminate all stop word such as ?a?, ?an?, ?the?,etc.
We compose a term-document matrix from thedocuments, and employ LSA to reduce the dimen-sionality of the data.
All documents are representedin the lower dimensional semantic space.We annotate our training data instances to iden-tify if they belong to the ?crime?
category or not.Subsequently, given any test document, we use a k-nearest neighbor classifier to classify it: we identifythe k closest training instances, where closeness iscomputed based on cosine distance.
If the majorityof the k instances are crime-related, we classify thearticle as a crime article, otherwise we classify it asnon-crime.5 Identification of Location of the ArticleAfter identifying crime-related articles, we mustnext identify the location where the reported crimeoccurred.
Again, we observe that newspaper articlesoften do not make explicit identification of the lo-cation of the crime, often not providing more thancity-level information explicitly.
The exact locationmust be inferred from the text used to describe thearea, and sometimes from other incidental informa-tion that the articles may contain.
Identification ofthe location thus becomes a challenging problem.Unlike the problem of identifying that the articlerefers to a crime, this is a closed-set problem in thatthe reported crime has indeed occurred, and hencemust have occurred in one of the areas of the city.Thus, we only need to identify which of the vari-ous locations in the city was the spot of occurrenceof the crime.
We do so by a combination of meth-ods.
In the First, we employ a named-entity extrac-tor to identify potential location-related words fromthe document, in case the location may be inferredfrom direct references.
Then we use a Naive Bayesclassifier based on a representation derived from la-tent Dirichlet alocation analysis (Blei et al , 2003)of the articles to identify the location.
We describeboth below.5.1 Named Entity RecognitionNamed Entity Recognition (Klein et al , 2003) isa Natural Language Processing technique which canidentify named entities like names, locations, orga-nizations etc.
from text.
Specifically, we use thetechnique described in the aforementioned work, toidentify locations from articles.
It uses decision treesand Conditional Random Fields(CRF?s) (Wallach ,2004) to identify named entities.
Conditional ran-dom fields (CRFs) are a class of statistical modelingmethod often applied in pattern recognition and ma-chine learning, where they are used for structuredprediction.
Whereas an ordinary classifier predicts alabel for a single sample without regard to ?neigh-boring?
samples, a CRF can take context into ac-count; e.g., the linear chain CRF popular in naturallanguage processing predicts sequences of labels forsequences of input samples.
Given the nature of ourproblem we determined this technique to be mostappropriate for our data.5.2 LDA-based Naive Bayes for LocationDeterminationNamed entity recognition cannot pull up location in-formation when it is not actually specified in the ar-ticle.
Even when it is mentioned, the reference maynot be unambiguous.
In order to deal with such ar-ticles we use a simple Naive Bayes classifier basedon features derived using Latent Dirichlet Allocation(LDA) (Blei et al , 2003).LDA is a well-known document-analysis tech-nique which assumes a ?latent?
or underlying pat-tern in the pattern of words in it.
The model as-sumes that documents are composed of topics.
Top-ics are distinguished by the probability distribu-tions of words associated with the topic ?
differ-ent topics have different distributions over words.For instance, a sports-related topic may have ahigher prevalence of sports-related words, while a20politics-related topic will have a higher prevalenceof politics-related words.
The generative model forLDA assume that in order to compose a document,for each word in the document a topic is selected ac-cording to a document-specific probability distribu-tion over topics, and subsequently a word is drawnfrom the topic.
Mathematically, the collection ofwords {w ?
D} in any articleA are assumed to havebeen drawn from a distribution P (w|t; ?
)P (t|A),where P (t|A) represents the probability distributionover topics t within article, and P (w|t) is the proba-bility distribution of words within topic t. The prob-ability distributions P (t|w) are learned from train-ing data.
The probability distribution P (t|w) of top-ics within any document is also drawn from an a pri-ori Dirichlet distribution, the parameters of whichare also learned from training data.We employ the distribution over topics as the fun-damental characterization of documents.
We derivea set of T topics from a training corpus comprisingcrime-related news reports.
Every article A is nowdecomposed into these topics.
The probability dis-tribution P (t|A) of topics t in the article, which isderived using LDA, is now used as a representationfor the documents.We view each document as bag of topics, andP (t|A) as a normalized count of the number of timesthe topic appears in the document.
Now we cast thelocation classification problem as follows.We associate locations with police stations.
Thecity is partitioned into regions, one corresponding tothe jurisdiction of each station.
We tag a number oftraining articles with the location of the crime theyreport.
We ensure that every station is adequatelyrepresented in the training set.
Each article is nowdecomposed into a topic histogram P (t|A).We now compute a probability distribution of top-ics with respect to each location to be identified us-ing the following maximum likelihood estimator:P (t|L) =1|{A ?
L}|?A?LP (t|A)where A ?
L represents the set of all training arti-cles that refer to crimes in location L.In order to appropriately represent the natural biasof crime in the city, we derive a priori probabilitydistribution of crime in the various precincts, P (L)from historical police FIR records asP (L) =|C ?
L|?L|C ?
L|where C ?
L represents the set of all FIR records ofcrimes reported at location L.We can now apply the following Bayesian clas-sifier to identify the location?L(A) of the crime re-ported in any article A:?L(A) = argmaxAP (L|A) (1)In other words, we are assigning the crime to thelocation that is most probable a posteriori, given theinformation in the article.Using the usual modification of the above equa-tion, the classification reduces to?L(A) = argmaxAP (A|L)P (L) (2)and working in the log domain, taking into accountthe monotonicity of the log function:?L(A) = argmaxAlogP (A|L) + logP (L) (3)log p(L) in the above equation is directly obtainedfrom the a priori probability distribution P (L).
Weonly need to compute logP (A|L) to perform thecomputation in Equation 3.
To do so, we assumethat the article being classified has been obtained bydrawing topics from the location specific topic dis-tribution P (t|L) repeatedly.
This leads us to the fol-lowing equation for P (A|L).p(A|L) =?tP (t|L)?P (t|A)logP (A|L) = ?
?tP (t|A) logP (t|L)where, as mentioned earlier, P (t|A) is the normal-ized count of the times topic t in the article A, ascomputed using LDA.
The term ?
is required be-cause we only know the normalized count of topicoccurrence; this must be scaled to obtain the truecounts.
The overall classification rule thus becomes?L(A) = argmaxA?
?tP (t|A) logP (t|L)+logP (L)(4)In principle ?
is article specific.
In practice, we de-rive a global value of ?
by optimizing over a devel-opment training set.216 Mapping Crime IntensitiesWe apply the combination of the document-identification and location-detection algorithms tonews articles and use it to generate a ?heat map?of crime for the city.
Every new incoming articlethat has been classified as relating to crime, and as-signed to any police station, is used to increment thecrime count for that station.
In our work we haveworked with a fixed number of articles, resulting ina fixed heat map; in practice, to prevent the entiremap from being saturated, a forgetting factor mustbe employed to assign greater weight to more recentcrimes.
We associate the total crime count for anystation with every junction in its jurisdiction.
Crimecounts for junctions that span multiple jurisdictionsaccumulate the counts of all the stations that coverthem.
This results in a crime-weighted street mapthat we can now use to find the safest path betweenlocations.7 Identifying Safest PathOnce the safety map showing the crime intensities isknown, we can convert the safest path problem to ashortest path problem by modeling the edge weightsas the sum of crime frequencies of the two connect-ing nodes.
Now that we have a graph with welldefined positive edge weights, we can apply Dijk-stra?s algorithm(Dijkstra, 1959) to identify the short-est path which is the safest path here.8 Results and ValidationThe validation of the model is two-fold.In the firststep we check the effectiveness of the classificationof the article as crime or non crime.
Then we checkhow well does the model identify the location of thearticle.8.1 Result of Crime/Non Crime ClassificationThe test for crime/non-crime classification was doneon 5000 articles (3000 crime and 2000 non-crime ar-ticles were taken) and various values of k were ex-perimented with.
The results of which are as fol-lows:Value of k Accuracy F-score1 82.14% 0.783 84.86% 0.815 86.52% 0.827 87.94% 0.839 89.36% 0.8411 87.60% 0.82Table 1: Results of Classifying articles intoCrime/Non-crime categoriesAs the experiments demonstrated the most suit-able value for k was found to be 9.8.2 Result of Identification of locationMethod Used Accuracy F-scoreNER 81.48% 0.78LDA 79.38% 0.75LDA+NER 83.64% 0.81Table 2: Location Identification resultsClearly the combination of LDA and NER tech-niques yields the best results.8.3 Result for Safest Path searchWe did a survey for 1200 commuters to use ourmodel for finding the safest transit path between twolocations and rate the path suggested by our modelon a scale of 1 to 10 based on their prior experienceof commuting between these locations.
We receivedan average rating of 8.75/10 from the 1200 users.9 ConclusionsThe model is able to predict the safest path between2 locations to a very high degree of accuracy.
Theaccuracy of the model depends on the correct classi-fication of the article as crime/non crime and on thecorrect identification of crime?s location from arti-cle.
Clearly the model achieves both of these withvery high degrees of accuracy as can be seen fromTables 1 and 2.
The model also maps this safestpath correctly on the map and informs the user ofthe route he should opt for to avoid crime prone re-gions.2210 Assumptions used and Future WorkOur model presently doesn?t take into account theactual road networks and instead gives the pathfrom one region(represented by that region?s policestation) to the other based on the assumption thata region is connected directly only to it?s nearestneighbors.In the near future we plan to do away with this as-sumption by incorporating the actual road networkin our model.Other future work includes identifying safest pathswhich also take into account the time of the day andthe traffic density of various routes.We also plan toidentify the exact type of crime and assign differentweights to different kinds of crimes in the nearfuture.11 AcknowledgmentsWe would like to acknowledge the efforts of Dr.Bhiksha Raj and Dr. Rita Singh of Carnegie MellonUniversity without whose constant support andguidance this project would not have been possible.References[Klein et al 2003] D. Klein and J. Smarr and H. Nguyenand C.D.
Manning 2003.
Named Entity Recognitionwith Character-Level Model.
Proceedings the SeventhConference on Natural Language Learning.
[Dumais 2004] Susan T. Dumais.
2004.
Latent SemanticAnalysis.
Annual Review of Information Science andTechnology[Hoffmann 1999] Thomas Hoffmann.
1999.
Probabilis-tic Latent Semantic Analysis Uncertainty in ArtificialIntelligence[Blei et al 2003] David M. Blei and Andrew Y. Ng andMichael I. Jordan 2003.
Latent Dirichlet Allocation.The Journal of Machine Learning Research,[Maltz et al 2000] Michael D. Maltz and Andrew C. Gor-don and Warren Friedman 2000.
Mapping Crimein Its Community Setting: Event Geography Analysis.Springer Verlag.
[Sebastiani 2002] Fabrizio Sebastian 2002.
Machinelearning in automated text categorization.
ACM Com-puting Surveys.
[Leong et al 2000] Kelvin Leong and Stephen Chan.2000.
A content analysis of web-based crime map-ping in the world?s top 100 highest GDP cities.
Map-ping Crime in Its Community Setting: Event Geogra-phy Analysis.
Springer Verlag[Ku et al 2011] Chih-Hao Ku and Gondy Leroy.
2011.
Acrime reports analysis system to identify related crimes.
Journal of the American Society for Information Sci-ence and Technology.
[Deerwester et al 1990] S. Deerwester and S. T Dumaisand G W Furnas and T K Landauer and R. Harshman.1990.
Indexing by Latent Semantic Analysis.
Journalof the American Society for Information Science.
[Kumar 2009] Ch.
Aswani Kumar.
2009.
Analysis ofUnsupervised Dimensionality Reduction Techniques .COMSIS.
[Wallach 2004] Hanna M. Wallach.
2004.
ConditionalRandom Fields: An Introduction .
University of Penn-sylvania CIS Technical Report MS-CIS-04-21.
[BeyondNormality 2013] BeyondNormality.
2013.Wikipedia Entry .
[Dijkstra1959] Dijkstra?s, E.W.
1959.
A note on twoproblems in connexion with graphs .
NumerischeMathematik.
[Chau et al 2002] Michael Chau and Jennifer J. Zu andHisnchun Chen.
2002.
Extracting meaningful entitiesfrom police narrative reports .
Proceedings of the 2002annual national conference on Digital government re-search.
[Zhang et al 2010] Yin Zhang,Rong Zim,Zhi Hua Zhou.2010.
Understanding Bag-of-Words Model: A Statis-tical Framework .
International Journal of MachineLearning and Computing.
[Wang et al 2004] Tong Wang and Cynthia Rudin andDaniel Wagner and Rich Sevieri.
2004.
Learningto Detect Patterns of Crime .
Machine Learning andKnowledge Discovery in Databases.
Springer BerlinHeidelberg[Hu 2013] Ruijuan Hu.
2013.
Data Mining in the Ap-plication of Criminal Cases Based on Decision Tree .International Journal of Engineering Sciences.
[Yu et al 2011] C.H.
Yu and Max W. Ward and M. Mora-bito and W. Ding.
2011.
Crime Forecasting UsingData Mining Techniques .
IEEE 11th InternationalConference on Data Mining Workshops.
[Mahendiran et al 2011] Aravindan Mahendiran andMichael Shuffett and Sathappan Muthiah and RimyMalla and Gaoqiang Zhang.
2011.
Forecasting CrimeIncidents using Cluster Analysis and Bayesian BeliefNetworks .
[Nath 2006] Shyam Varan Nath.
2006.
Crime PatternDetection Using Data Mining .
Web Intelligence andIntelligent Agent Technology Workshops, 2006.
WI-IAT 2006 Workshops.
2006 IEEE/WIC/ACM Interna-tional Conference.
[Bajpai 2012] Devesh Bajpai.
2012.
Emerging Trends inUtilization of Data Mining in Criminal Investigation:An Overview .
Springer.
[Sukanya et al 2012] Sukanya, M. and Kalaikumaran, T.23and Karthik, S.. 2012.
Criminals and crime hotspotdetection using data mining algorithms: clustering andclassification .
International Journal of AdvancedResearch in Computer Engineering and Technology(IJARCET).
[Chen et al 2004] Chen, H. and Chung, W. and Xu, J.J.and Wang, G. and Qin, Y. and Chau, M.. 2004.
Crimedata mining: A general framework and some examples.
IEEE Computer.24
