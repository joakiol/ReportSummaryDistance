Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 443?451,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsIs Machine Translation Getting Better over Time?Yvette Graham Timothy Baldwin Alistair Moffat Justin ZobelDepartment of Computing and Information SystemsThe University of Melbourne{ygraham,tbaldwin,ammoffat,jzobel}@unimelb.edu.auAbstractRecent human evaluation of machinetranslation has focused on relative pref-erence judgments of translation quality,making it difficult to track longitudinal im-provements over time.
We carry out alarge-scale crowd-sourcing experiment toestimate the degree to which state-of-the-art performance in machine translation hasincreased over the past five years.
To fa-cilitate longitudinal evaluation, we moveaway from relative preference judgmentsand instead ask human judges to providedirect estimates of the quality of individ-ual translations in isolation from alternateoutputs.
For seven European languagepairs, our evaluation estimates an aver-age 10-point improvement to state-of-the-art machine translation between 2007 and2012, with Czech-to-English translationstanding out as the language pair achiev-ing most substantial gains.
Our methodof human evaluation offers an economi-cally feasible and robust means of per-forming ongoing longitudinal evaluationof machine translation.1 IntroductionHuman evaluation provides the foundation for em-pirical machine translation (MT), whether humanjudges are employed directly to evaluate systemoutput, or via the use of automatic metrics ?validated through correlation with human judg-ments.
Achieving consistent human evaluationis not easy, however.
Annual evaluation cam-paigns conduct large-scale human assessment butreport ever-decreasing levels of judge consistency?
when given the same pair of translations torepeat-assess, even expert human judges will wor-ryingly often contradict both the preference judg-ment of other judges and their own earlier prefer-ence (Bojar et al., 2013).
For this reason, humanevaluation has been targeted within the commu-nity as an area in need of attention, with increasedefforts to develop more reliable methodologies.One standard platform for human evaluation isWMT shared tasks, where assessments have (since2007) taken the form of ranking five alternate sys-tem outputs from best to worst (Bojar et al., 2013).This method has been shown to produce more con-sistent judgments compared to fluency and ade-quacy judgments on a five-point scale (Callison-Burch et al., 2007).
However, relative preferencejudgments have been criticized for being a sim-plification of the real differences between trans-lations, not sufficiently taking into account thelarge number of different types of errors of vary-ing severity that occur in translations (Birch et al.,2013).
Relative preference judgments do not takeinto account the degree to which one translation isbetter than another ?
there is no way of knowing ifa winning system produces far better translationsthan all other systems, or if that system would haveranked lower if the severity of its inferior transla-tion outputs were taken into account.Rather than directly aiming to increase humanjudge consistency, some methods instead increasethe number of reference translations available toautomatic metrics.
HTER (Snover et al., 2006)employs humans to post-edit each system out-put, creating individual human-targeted referencetranslations which are then used as the basis forcomputing the translation error rate.
HyTER, onthe other hand, is a tool that facilitates creationof very large numbers of reference translations(Dreyer and Marcu, 2012).
Although both ap-proaches increase fairness compared to automaticmetrics that use a single generic reference transla-tion, even human post-editors will inevitably varyin the way they post-edit translations, and the pro-cess of creating even a single new reference trans-443lation for each system output is often too resource-intensive to be used in practice.With each method of human evaluation, a trade-off exists between annotation time and the numberof judgments collected.
At one end of the spec-trum, the WMT human evaluation collects largenumbers of quick judgments (approximately 3.5minutes per screen, or 20 seconds per label) (Bojaret al., 2013).1In contrast, HMEANT (Lo and Wu,2011) uses a more time-consuming fine-grainedsemantic-role labeling analysis at a rate of approx-imately 10 sentences per hour (Birch et al., 2013).But even with this detailed evaluation methodol-ogy, human judges are inconsistent (Birch et al.,2013).Although the trend appears to be toward morefine-grained human evaluation of MT output, itremains to be shown that this approach leads tomore reliable system rankings ?
with a main rea-son to doubt this being that far fewer judgmentswill inevitably be possible.
We take a counter-approach and aim to maintain the speed by whichassessments are collected in shared task evalua-tions, but modify the evaluation set-up in two mainways: (1) we structure the judgments as monolin-gual tasks, reducing the cognitive load involvedin assessing translation quality; and (2) we ap-ply judge-intrinsic quality control and score stan-dardization, to minimize noise introduced whencrowd-sourcing is used to leverage numbers of as-sessments and to allow for the fact that humanjudges will vary in the way they assess transla-tions.
Assessors are regarded as reliable as longas they demonstrate consistent judgments across arange of different quality translations.We elicit direct estimates of quality fromjudges, as a quantitative estimate of the magni-tude of each attribute of interest (Steiner and Nor-man, 1989).
Since we no longer look for rela-tive preference judgments, we revert back to theoriginal fluency and adequacy criteria last used inWMT 2007 shared task evaluation.
Instead of five-point fluency/adequacy scales, however, we usea (100-point) continuous rating scale, as this fa-cilitates more sophisticated statistical analyses ofscore distributions for judges, including worker-intrinsic quality control for crowd-sourcing.
Thelatter does not depend on agreement with ex-perts, and is made possible by the reduction in1WMT 2013 reports 361 hours of labor to collect 61,695labels, with approximately one screen of five pairwise com-parisons each yielding a set of 10 labels.information-loss when a continuous scale is used.In addition, translations are assessed in isolationfrom alternate system outputs, so that judgmentscollected are no longer relative to a set of fivetranslations.
This has the added advantage of elim-inating the criticism made of WMT evaluationsthat systems sometimes gain advantage from luck-of-the-draw comparison with low quality output,and vice-versa (Bojar et al., 2011).Based on our proposed evaluation methodology,human judges are able to work quickly, on averagespending 18 and 13 seconds per single segment ad-equacy and fluency judgment, respectively.
Addi-tionally, when sufficiently large volumes of suchjudgments are collected, mean scores reveal sig-nificant differences between systems.
Further-more, since human evaluation takes the form of di-rect estimates instead of relative preference judg-ments, our evaluation introduces the possibilityof large-scale longitudinal human evaluation.
Wedemonstrate the value of longitudinal evaluationby investigating the improvement made to state-of-the-art MT over a five year time period (be-tween 2007 and 2012) using the best participatingWMT shared task system output.
Since it is likelythat the test data used for shared tasks has variedin difficulty over this time period, we additionallypropose a simple mechanism for scaling systemscores relative to task difficulty.Using the proposed methodology for measur-ing longitudinal change in MT, we conclude that,for the seven European language pairs we evalu-ate, MT has made an average 10% improvementover the past 5 years.
Our method uses non-expertmonolingual judges via a crowd-sourcing portal,with fast turnaround and at relatively modest cost.2 Monolingual Human EvaluationThere are several reasons why the assessment ofMT quality is difficult.
Ideally, each judge shouldbe a native speaker of the target language, whileat the same time being highly competent in thesource language.
Genuinely bilingual people arerare, however.
As a result, judges are often peo-ple with demonstrated skills in the target language,and a working knowledge ?
often self-assessed ?of the source language.
Adding to the complexityis the discipline that is required: the task is cog-nitively difficult and time-consuming when doneproperly.
The judge is, in essence, being asked todecide if the supplied translations are what they444would have generated if they were asked to do thesame translation.The assessment task itself is typically structuredas follows: the source segment (a sentence ora phrase), plus five alternative translations and a?reference?
translation are displayed.
The judgeis then asked to assign a rank order to the fivetranslations, from best to worst.
A set of pairwisepreferences are then inferred, and used to generatesystem rankings, without any explicit formation ofstand-alone system ?scores?.This structure introduces the risk that judgeswill only compare translations against the refer-ence translation.
Certainly, judges will vary inthe degree they rely on the reference translation,which will in turn impact on inter-judge inconsis-tency.
For instance, even when expert judges doassessments, it is possible that they use the ref-erence translation as a substitute for reading thesource input, or do not read the source input atall.
And if crowd-sourcing is used, can we reallyexpect high proportions of workers to put the ad-ditional effort into reading and understanding thesource input when a reference translation (proba-bly in their native language) is displayed?
In re-sponse to this potential variability in how annota-tors go about the assessment task, we trial assess-ments of adequacy in which the source input is notdisplayed to human judges.
We structure assess-ments as a monolingual task and pose them in sucha way that the focus is on comparing the meaningof reference translations and system outputs.2We therefore ask human judges to assess the de-gree to which the system output conveys the samemeaning as the reference translation.
In this way,we focus the human judge indirectly on the ques-tion we wish to answer when assessing MT: doesthe translation convey the meaning of the source?The fundamental assumption of this approach isthat the reference translation accurately capturesthe meaning of the source; once that assumptionis made, it is clear that the source is not requiredduring the evaluation.Benefits of this change are that the task is botheasier to describe to novice judges, and easierto answer, and that it requires only monolingualspeakers, opening up the evaluation to a vastlylarger pool of genuinely qualified workers.With this set-up in place for adequacy, we also2This dimension of the assessment is similar but not iden-tical to the monolingual adequacy assessment in early NISTevaluation campaigns (NIST, 2002).re-introduce a fluency assessment.
Fluency rat-ings can be carried out without the presence of areference translation, reducing any remnant biastowards reference translations in the evaluationsetup.
That is, we propose a judgment regime inwhich each task is presented as a two-item fluencyand adequacy judgment, evaluated separately, andwith adequacy restructured into a monolingual?similarity of meaning?
task.When fluency and adequacy were originallyused for human evaluation, each rating used a 5-point adjective scale (Callison-Burch et al., 2007).However, adjectival scale labels are problematicand ratings have been shown to be highly depen-dent on the exact wording of descriptors (Seymouret al., 1985).
Alexandrov (2010) provides a sum-mary of the extensive problems associated with theuse of adjectival scale labels, including bias result-ing from positively- and negatively-worded itemsnot being true opposites of one another, and itemsintended to have neutral intensity in fact provingto have specific conceptual meanings.It is often the case, however, that the questioncould be restructured so that the rating scale nolonger requires adjectival labels, by posing thequestion as a statement such as The text is fluentEnglish and asking the human assessor to specifyhow strongly they agree or disagree with that state-ment.
The scale and labels can then be held con-stant across experimental set-ups for all attributesevaluated ?
meaning that if the scale is still biasedin some way it will be equally so across all set-ups.3 Assessor ConsistencyOne way of estimating the quality of a humanevaluation regime is to measure its consistency:whether or not the same outcome is achieved ifthe same question is asked a second time.
InMT, annotator consistency is commonly measuredusing Cohen?s kappa coefficient, or some variantthereof (Artstein and Poesio, 2008).
Originally de-veloped as a means of establishing assessor inde-pendence, it is now commonly used in the reversesense, with high numeric values being used as ev-idence of agreement.
Two different measurementscan be made ?
whether a judge is consistent withother judgments performed by themselves (intra-annotator agreement), and whether a judge is con-sistent with other judges (inter-annotator agree-ment).Cohen?s kappa is intended for use with categor-445ical judgments, but is also commonly used withfive-point adjectival-scale judgments, where theset of categories has an explicit ordering.
Oneparticular issue with five-point assessments is thatscore standardization cannot be applied.
As such,a judge who assigns two neighboring intervals isawarded the same ?penalty?
for being ?different?as the judge who chooses the extremities.
Thekappa coefficient cannot be directly applied tomany-valued interval or continuous data.This raises the question of how we should eval-uate assessor consistency when a continuous rat-ing scale is in place.
No judge, when given thesame translation to judge twice on a continuousrating scale, can be expected to give precisely thesame score for each judgment (where repeat as-sessments are separated by a considerable numberof intervening ones).
A more flexible tool is thusrequired.
We build such a tool by starting with twocore assumptions:A: When a consistent assessor is presented witha set of repeat judgments, the mean of theinitial set of assessments will not be signifi-cantly different from the mean score of repeatassessments.B: When a consistent judge is presented with aset of judgments for translations from twosystems, one of which is known to producebetter translations than the other, the meanscore for the better system will be signifi-cantly higher than that of the inferior system.Assumption B is the basis of our quality-controlmechanism, and allows us to distinguish betweenTurkers who are working carefully and those whoare merely going through the motions.
We use a100-judgment HIT structure to control same-judgerepeat items and deliberately-degraded systemoutputs (bad reference items) used for worker-intrinsic quality control (Graham et al., 2013).bad reference translations for fluency judgmentsare created as follows: two words in the translationare randomly selected and randomly re-insertedelsewhere in the sentence (but not as the initial orfinal words of the sentence).Since adding duplicate words will not degradeadequacy in the same way, we use an alternatemethod to create bad reference items for adequacyjudgments: we randomly delete a short sub-stringof length proportional to the length of the origi-nal translation to emulate a missing phrase.
Sincetotal fltrd Assum A total fltrdwrkrs wrkrs holds segs segsF 557 321 (58%) 314 (98.8%) 122k 78k (64%)A 542 283 (52%) 282 (99.6%) 102k 62k (61%)Table 1: Total quality control filtered workers andassessments (F = fluency; A = adequacy).this is effectively a new degradation scheme, wetested against experts.
For low-quality transla-tions, deleting just two words from a long sentenceoften made little difference.
The method we even-tually settled on removes a sequence of k words,as a function of sentence length n:2 ?
n ?
3 ?
k = 14 ?
n ?
5 ?
k = 26 ?
n ?
8 ?
k = 39 ?
n ?
15 ?
k = 416 ?
n ?
20 ?
k = 5n > 20 ?
k =?n5?To filter out careless workers, scores forbad reference pairs are extracted, and adifference-of-means test is used to calculatea worker-reliability estimate in the form of ap-value.
Paired tests are then employed using theraw scores for degraded and corresponding systemoutputs, using a reliability significance thresholdof p < 0.05.
If a worker does not demonstratethe ability to reliably distinguish between a badsystem and a better one, the judgments fromthat worker are discarded.
This methodologymeans that careless workers who habitually ratetranslations either high or low will be detected,as well as (with high probability) those that click(perhaps via robots) randomly.
It also has theadvantage of not filtering out workers who areinternally consistent but whose scores happen notto correspond particularly well to a set of expertassessments.Having filtered out users who are unable to reli-ably distinguish between better and worse sets oftranslations (p ?
0.05), we can now examine howwell Assumption A holds for the remaining users,i.e.
the extent to which workers apply consistentscores to repeated translations.
We compute meanscores for the initial and repeat items and look foreven very small differences in the two distribu-tions for each worker.
Table 1 shows numbers ofworkers who passed quality control, and also that446SiSi+51 bad reference its corresponding system output1 system output a repeat of it1 reference its corresponding system outputAbove in reverse for Siand Si+54 system outputs 4 system outputsTable 2: Control of repeat item pairs.
Sidenotesthe ithset of 10 translations assessed within a 100translation HIT.the vast majority (around 99%) of reliable work-ers have no significant difference between meanscores for repeat items.4 Five Years of Machine TranslationTo estimate the improvement in MT that tookplace between 2007 and 2012, we asked work-ers on Amazon?s Mechanical Turk (MTurk) to ratethe quality of translations produced by the best-reported participating system for each of WMT2007 and WMT 2012 (Callison-Burch et al., 2007;Callison-Burch et al., 2012).
Since it is likely thatthe test set has changed in difficulty over this timeperiod, we also include in the evaluation the orig-inal test data for 2007 and 2012, translated by asingle current MT system.
We use the latter to cal-ibrate the results for test set difficulty, by calcu-lating the average difference in rating, ?, betweenthe 2007 and 2012 test sets.
This is then addedto the difference in rating for the best-reportedsystems in 2012 and 2007, to arrive at an over-all evaluation of the 5-year gain in MT quality fora given language pair, separately for fluency andadequacy.Experiments were carried out for each of Ger-man, French and Spanish into and out of English,and also for Czech-to-English.
English-to-Czechwas omitted because of a low response rate onMTurk.
For language pairs where two systems tiedfor first place in the shared task, a random selec-tion of translations from both systems was made.HIT structureTo facilitate quality control, we construct eachHIT on MTurk as an assessment of 100 trans-lations.
Each individual translation is rated inisolation from other translations with workers re-quired to iterate through 100 translations withoutthe opportunity to revisit earlier assessments.
A100-translation HIT contains the following items:70 randomly selected system outputs made up ofroughly equal proportions of translations for eachevaluated system, 10 bad reference translations(each based on one of the 70 system outputs), 10exact repeats and 10 reference translations.
We di-vide a 100-translation HIT into 10 sets of 10 trans-lations.
Table 2 shows how the content of each setis determined.
Translations are then randomizedonly within each set (of 10 translations), with theoriginal sequence order of the sets preserved.
Inthis way, the order of quality control items is un-predictable but controlled so pairs are separated bya minimum of 40 intervening assessments (4 setsof translations).
The HIT structure results in 80%of assessed translations corresponding to genuineoutputs of a system (including exact repeat assess-ments), which is ultimately what we wish to ob-tain, with 20% of assessments belonging to qualitycontrol items (bad reference or reference transla-tions).Assessment set-upSeparate HITs were provided for evaluation of flu-ency and adequacy.
For fluency, a single systemoutput was displayed per screen, with a worker re-quired to rate the fluency of a translation on a 100-point visual analog scale with no displayed pointscores.
A similar set-up was used for adequacy butwith the addition of a reference translation (dis-played in gray font to distinguish it from the sys-tem output being assessed).
The Likert-type state-ment that framed the judgment was Read the textbelow and rate it by how much you agree that:?
[for fluency] the text is fluent English?
[for adequacy] the black text adequately ex-presses the meaning of the gray text.In neither case was the source language string pro-vided to the workers.Tasks were published on MTurk, with no re-gion restriction but the stipulation that only na-tive speakers of the target language should com-plete HITs, and with a qualification of an MTurkprior HIT-approval rate of at least 95%.
Instruc-tions were always presented in the target language.Workers were paid US$0.50 per fluency HIT, andUS$0.60 per adequacy HIT.33Since insufficient assessments were collected for Frenchand German evaluations in the initial run, a second and ulti-mately third set of HITs were needed for these languages withincreased payment per HIT of US$1.0 per 100-judgment ade-quacy HIT, US$0.65 per 100-judgment fluency HIT and lateragain to US$1.00 per 100-judgment fluency HIT.447Close to one thousand individual Turkers con-tributed to this experiment (some did both flu-ency and adequacy assessments), providing a to-tal of more than 220,000 translations, of which140,000 were provided by workers meeting thequality threshold.In general, it cost approximately US$30 to as-sess each system, with low-quality workers ap-proximately doubling the cost of the annotation.We rejected HITs where it was clear that random-clicking had taken place, but did not reject solelyon the basis of having not met the quality controlthreshold, to avoid penalizing well-intentioned butlow-quality workers.Overall change in performanceTable 3 shows the overall gain made in five years,from WMT 07 to WMT 12.
Mean scores for thetwo top-performing systems from each shared task(BEST07, BEST12) are included, as well as scoresfor the benchmark current MT system on the twotest sets (CURR07, CURR12).
For each languagepair, a 100-translation HIT was constructed byrandomly selecting translations from the pool of(3003+2007)?2 that were available, and this re-sults in apparently fewer assessments for the 2007test set.
In fact, numbers of evaluated translationsare relative to the size of each test set.
Average zscores for each system are also presented, based onthe mean and standard deviation of all assessmentsprovided by an individual worker, with positivevalues representing deviations above the mean ofworkers.
In addition, we include mean BLEU (Pa-pineni et al., 2001) and METEOR (Banerjee andLavie, 2005) automatic scores for the same systemoutputs.The CURR benchmark shows fluency scoresthat are 5.9 points higher on the 2007 data set thanthey are on the 2012 test data, with a larger dif-ference in adequacy of 8.3 points.
As such, the2012 test data is more challenging than the 2007test data.
Despite this, both fluency and adequacyscores for the best system in 2012 have increasedby 4.5 and 2.0 points respectively, amounting toestimated average gains of 10.4 points in fluencyand 10.3 points in adequacy for state-of-the-artMT across the seven language pairs.Looking at the standardized scores, it is appar-ent that the presence of the CURR translations forthe 2007 test set pushes the mean score for the2007 best systems below zero.
The presence inthe HITs of reference translations also shifts stan-dardized system evaluations below zero, becausethey are not attributable to any of the systems be-ing assessed.4Results for automatic metrics lead to similarconclusions: that the test set has indeed increasedin difficulty; and that, in spite of this, substantialimprovements have been made according to auto-matic metrics, +13.5 using BLEU, and +7.1 onaverage using METEOR.Language pairsTable 4 shows mean fluency and adequacy scoresby language pair for translation into English.
Rel-ative gains in both adequacy and fluency for the to-English language pairs are in agreement with theestimates generated through the use of the two au-tomatic metrics.
Most notably, Czech-to-Englishtranslation appears to have made substantial gainsacross the board, achieving more than double thegain made by some of the other language pairs; re-sults for best participating 2007 systems show thatthis may in part be caused by the fact that Czech-to-English translation had a lower 2007 baselineto begin with (BEST07F:40.8; A:41.7) in compar-ison to, for example, Spanish-to-English transla-tion (BEST07F:56.7; A:59.0).Another notable result is that although the testdata for each year?s shared task is parallel acrossfive languages, test set difficulty increases by dif-ferent degrees according to human judges and au-tomatic metrics, with BLEU scores showing sub-stantial divergence across the to-English languagepairs.
Comparing BLEU scores achieved by thebenchmark system for Spanish to English andCzech-to-English, for example, the benchmarksystem achieves close scores on the 2007 test datawith a difference of only |52.3 ?
51.2| = 1.1,compared to the score difference for the bench-mark scores for translation of the 2012 test data of|25.0 ?
38.3| = 13.3.
This may indicate that theincrease in test set difficulty that has taken placeover the years has made the shared task dispro-portionately more difficult for some language pairsthan for others.
It does seem that some languagepairs are harder to translate than others, and thedifferential change may be a consequence of thefact that increasing test set complexity for all lan-guages in parallel has a greater impact on transla-tion difficulty for language pairs that are intrinsi-cally harder to translate between.4Scores for reference translations can optionally be omit-ted for score standardization.448CURR07CURR12?BEST07BEST125-Year Gain(CURR07?
CURR12) (BEST12?
BEST07+ ?
)fluencyscore 64.1 58.2 5.9 53.5 58.0 (+4.5) 10.4z 0.18 0.00 0.18 ?0.16 0.00 (+0.16) 0.34n 12,334 18,654 12,513 18,579adequacyscore 65.0 56.7 8.3 54.0 56.0 (+2.0) 10.3z 0.18 ?0.07 0.25 ?0.16 ?0.09 (+0.07) 0.32n 10,022 14,870 10,049 14,979metricsBLEU 41.5 30.0 11.4 25.6 27.7 (+2.1) 13.5METEOR 49.2 41.1 8.1 41.1 40.1 (?1.0) 7.1Table 3: Average human evaluation results for all language pairs; mean and standardized z scores arecomputed in each case for n translations.
In this table, and in Tables 4 and 5, all reported fluency andadequacy values are in points relative to the 100-point assessment scale.CURR07CURR12?BEST07BEST125-Year Gain(CURR07?
CURR12) (BEST12?
BEST07+ ?
)DE-ENfluencyscore 65.3??
?57.9 7.4 52.8 55.0?
(+2.2) 9.6n 2,164 3,381 2,242 3,253adequacyscore 63.8??
?52.8 11.0 46.5 49.8??
(+3.3) 14.3n 1,458 2,175 1,454 2,193metricsBLEU 38.3 26.5 11.8 21.1 23.8 (+2.7) 14.5METEOR 40.3 32.7 7.6 33.4 31.7 (?1.7) 5.9FR-ENfluencyscore 65.9??
?58.0 7.9 57.8 60.2??
(+2.4) 10.3n 2,172 3,267 2,203 3,238adequacyscore 61.0??
?52.3 8.7 52.7 51.5 (?1.2) 7.5n 1,754 2,651 1,763 2,712metricsBLEU 39.4 32.0 7.4 28.6 31.5 (+2.9) 10.3METEOR 39.8 34.6 5.2 35.9 34.3 (?1.6) 3.6ES-ENfluencyscore 68.4??
?59.2 9.2 56.7 56.7 (+0.0) 9.2n 1,514 2,234 1,462 2,230adequacyscore 68.0??
?56.9 11.1 59.0??
?55.7 (?3.3) 7.8n 1,495 2,193 1,492 2,180metricsBLEU 51.2 38.3 12.9 35.1 33.5 (?1.6) 11.3METEOR 45.4 37.0 8.4 39.9 36.0 (?3.9) 4.5CS-ENfluencyscore 62.3??
?49.9 12.4 40.8 50.5???
(+9.7) 22.1n 1,873 2,816 1,923 2,828adequacyscore 62.4??
?47.5 14.9 41.7 47.4???
(+5.7) 20.6n 1,218 1,830 1,257 1,855metricsBLEU 52.3 25.0 27.3 25.1 22.4 (?2.7) 24.6METEOR 44.7 31.6 13.1 34.3 30.8 (?3.5) 9.6Table 4: Human evaluation of WMT 2007 and 2012 best systems for to-English language pairs.
Meanscores are computed in each case for n translations.
In this table and in Table 5,?denotes significance atp < 0.05;?
?significance at p < 0.01; and??
?significance at p < 0.001.Table 5 shows results for translation out-of En-glish, and once again human evaluation scores arein agreement with automatic metrics with English-to-Spanish translation achieving most substantialgains for the three out-of-English language pairs,an increase of 12.4 points for fluency, and 11.8points with respect to adequacy, while English-to-French translation achieves a gain of 8.8 for449CURR07CURR12?BEST07BEST125-Year Gain(CURR07?
CURR12) (BEST12?
BEST07+ ?
)EN-ESfluencyscore 77.2??
?73.4 3.8 63.3 71.9???
(+8.6) 12.4n 2,286 3,318 2,336 3,420adequacyscore 75.2??
?68.1 7.1 62.5 67.2 (+4.7) 11.8n 1,410 2,039 1,399 2,112metricsBLEU 48.2 38.7 9.5 29.1 35.3 (+6.2) 15.7METEOR 69.9 59.6 10.3 57.0 58.1 (+1.1) 11.4EN-FRfluencyscore 57.1 55.2 1.9 49.5 56.4 (+6.9) 8.8n 1,008 1,645 1,039 1,588adequacyscore 64.2?61.9 2.3 57.2 62.3 (+5.1) 7.4n 1,234 1,877 1,274 1,775metricsBLEU 37.2 30.8 6.4 25.3 29.9 (+4.6) 11.0METEOR 59.4 52.9 6.5 50.4 52.0 (+1.6) 8.1EN-DEfluencyscore 52.3 54.1?
?1.8 53.7 55.5 (+1.8) 0.0n 1,317 1,993 1,308 2,022adequacyscore 60.3?
?57.4 2.9 58.3 58.3 (+0.0) 2.9n 1,453 2,105 1,410 2,152metricsBLEU 23.6 18.7 4.9 14.6 17.2 (+2.6) 7.5METEOR 44.7 39.1 5.6 36.7 38.0 (+1.3) 6.9Table 5: Human evaluation of WMT 2007 and 2012 best systems for out of English language pairs.Mean scores are computed in each case for n translations.fluency and 7.4 points for adequacy.
English-to-German translation achieves the lowest gain ofall languages, with apparently no improvementin fluency, as the human fluency evaluation ofthe benchmark system on the supposedly easier2007 data receives a substantially lower score thanthe same system over the 2012 data.
This resultdemonstrates why fluency, evaluated without a ref-erence translation, should not be used to evalu-ate MT systems without an adequacy assessment,since it is entirely possible for a low-adequacytranslation to achieve a high fluency score.For all language pairs, Figure 1 plots the netgain in fluency, adequacy and F1against increasein test data difficulty.5 ConclusionWe carried out a large-scale human evaluationof best-performing WMT 2007 and 2012 sharedtask systems in order to estimate the improvementmade to state-of-the-art machine translation overthis five year time period.
Results show significantimprovements have been made in machine trans-lation of European language pairs, with Czech-to-English recording the greatest gains.
It is alsoclear from our data that the difficulty of the taskhas risen over the same period, to varying degrees0 5 10 15051015Best12 ?
Best 07?
( Curr  07?Curr12 ) de?enfr?enes?encs?enen?deen?fren?esF1FluencyAdequacyFigure 1: Mean fluency, adequacy and combinedF1scores for language pairs.for individual language pairs.Researchers interested in making use of thedataset are invited to contact the first author.Acknowledgments This work was supported bythe Australian Research Council.450ReferencesA.
Alexandrov.
2010.
Characteristics of single-itemmeasures in Likert scale format.
The ElectronicJournal of Business Research Methods, 8:1?12.R.
Artstein and M. Poesio.
2008.
Inter-coder agree-ment for computational linguistics.
ComputationalLinguistics, 34(4):555?596.S.
Banerjee and A. Lavie.
2005.
METEOR: An au-tomatic metric for mt evaluation with improved cor-relation with human judgements.
In Proc.
Wkshp.Intrinsic and Extrinsic Evaluation Measures for Ma-chine Translation and/or Summarization, pages 65?73, Ann Arbor, MI.A.
Birch, B. Haddow, U. Germann, M. Nadejde,C.
Buck, and P. Koehn.
2013.
The feasibility ofHMEANT as a human MT evaluation metric.
InProc.
8th Wkshp.
Statistical Machine Translation,pages 52?61, Sofia, Bulgaria.
ACL.O.
Bojar, M. Ercegov?cevic, M. Popel, and O. Zaidan.2011.
A grain of salt for the WMT manual evalua-tion.
In Proc.
6th Wkshp.
Statistical Machine Trans-lation, pages 1?11, Edinburgh, Scotland.
ACL.O.
Bojar, C. Buck, C. Callison-Burch, C. Federmann,B.
Haddow, P. Koehn, C. Monz, M. Post, R. Soricut,and L. Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Proc.8th Wkshp.
Statistical Machine Translation, pages1?44, Sofia, Bulgaria.
ACL.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz, andJ.
Schroeder.
2007.
(Meta-) evaluation of machinetranslation.
In Proc.
2nd Wkshp.
Statistical MachineTranslation, pages 136?158, Prague, Czech Repub-lic.
ACL.C.
Callison-Burch, P. Koehn, C. Monz, M. Post,R.
Soricut, and L. Specia.
2012.
Findings of the2012 Workshop on Statistical Machine Translation.In Proc.
7th Wkshp.
Statistical Machine Translation,pages 10?51, Montreal, Canada.
ACL.M.
Dreyer and D. Marcu.
2012.
HyTER: Meaning-equivalent semantics for translation evaluation.
InProc.
2012 Conf.
North American Chapter of theACL: Human Language Technologies, pages 162?171, Montreal, Canada.
ACL.Y.
Graham, T. Baldwin, A. Moffat, and J. Zobel.
2013.Continuous measurement scales in human evalua-tion of machine translation.
In Proc.
7th Linguis-tic Annotation Wkshp.
& Interoperability with Dis-course, pages 33?41, Sofia, Bulgaria.
ACL.C.
Lo and D. Wu.
2011.
MEANT: An inexpensive,high-accuracy, semi-automatic metric for evaluatingtranslation utility based on semantic roles.
In Proc.49th Annual Meeting of the ACL: Human LanguageTechologies, pages 220?229, Portland, OR.
ACL.NIST.
2002.
The 2002 NIST machine translationevaluation plan.
National Institute of Standards andTechnology.
http://www.itl.nist.gov/iad/894.01/tests/mt/2003/doc/mt03_evalplan.v2.pdf.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.2001.
BLEU: A method for automatic evaluationof machine translation.
Technical Report RC22176(W0109-022), IBM Research, Thomas J. WatsonResearch Center.R.
A. Seymour, J. M. Simpson, J. E. Charlton, andM.
E. Phillips.
1985.
An evaluation of length andend-phrase of visual analogue scales in dental pain.Pain, 21:177?185.M.
Snover, B. Dorr, R. Scwartz, J. Makhoul, andL.
Micciula.
2006.
A study of translation error ratewith targeted human annotation.
In Proc.
7th Bien-nial Conf.
of the Assoc.
Machine Translaiton in theAmericas, pages 223?231, Boston, MA.D.
L. Steiner and G. R. Norman.
1989.
Health Mea-surement Scales, A Practical Guide to their Devel-opment and Use.
Oxford University Press, Oxford,UK, fourth edition.451
