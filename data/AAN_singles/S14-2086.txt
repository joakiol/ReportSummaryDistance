Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 497?502,Dublin, Ireland, August 23-24, 2014.RTRGO: Enhancing the GU-MLT-LT Systemfor Sentiment Analysis of Short MessagesTobias G?untherRetresco GmbHretresco.deemail@tobias.ioJean Vancoppenolleferret go GmbHferret-go.comjean.vcop@gmail.comRichard JohanssonUniversity of Gothenburgwww.svenska.gu.serichard.johansson@gu.seAbstractThis paper describes the enhancementsmade to our GU-MLT-LT system (G?untherand Furrer, 2013) for the SemEval-2014re-run of the SemEval-2013 shared task onsentiment analysis in Twitter.
The changesinclude the usage of a Twitter-specific to-kenizer, additional features and sentimentlexica, feature weighting and random sub-space learning.
The improvements resultin an increase of 4.18 F-measure points onthis year?s Twitter test set, ranking 3rd.1 IntroductionAutomatic analysis of sentiment expressed in textis an active research area in natural language pro-cessing with obvious commercial interest.
In thesimplest formulation of the problem, sentimentanalysis is framed as a categorization problemover documents, where the set of categories istypically a set of polarity values, such as posi-tive, neutral, and negative.
Many approaches todocument-level sentiment classification have beenproposed.
For an overview see e.g.
Liu (2012).Text in social media and in particular microblogmessages are a challenging text genre for senti-ment classification, as they introduce additionalproblems such as short text length, spelling vari-ation, special tokens, topic variation, languagestyle and multilingual content.
Following Pang etal.
(2002), most sentiment analysis systems havebeen based on standard text categorization tech-niques, e.g.
training a classifier using some sort ofbag-of-words feature representation.
This is alsotrue for sentiment analysis of microblogs.
AmongThis work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/the first to work specifically with Twitter1datawere Go et al.
(2009), who use emoticons as labelsfor the messages.
Similarly, Davidov et al.
(2010),Pak and Paroubek (2010), and Kouloumpis et al.
(2011) use this method of distant supervision toovercome the data acquisition barrier.
Barbosaand Feng (2010) make use of three different senti-ment detection websites to label messages and usemostly non-lexical features to improve the robust-ness of their classifier.
Bermingham and Smeaton(2010) investigate the impact of the shortness ofTweets on sentiment analysis and Speriosu et al.
(2011) propagate information from seed labelsalong a linked structure that includes Twitter?sfollower graph.
There has also been work onlexicon-based approaches to sentiment analysis ofmicroblogs, such as O?Connor et al.
(2010), Thel-wall et al.
(2010) and Zhang et al.
(2011).
For adetailed discussion see G?unther (2013).In 2013, the International Workshop on Se-mantic Evaluation (SemEval) organized a sharedtask on sentiment analysis in Twitter (Nakov etal., 2013) to enable a better comparison of dif-ferent approaches for sentiment analysis of mi-croblogs.
The shared task consisted of two sub-tasks: one on recognizing contextual polarity ofa given subjective expression (Task A), and oneon document-level sentiment classification (TaskB).
For both tasks, the training sets consisted ofmanually labeled Twitter messages, while the testsets consisted of a Twitter part and an SMS partin order to test domain sensitivity.
Among thebest performing systems were Mohammad et al.
(2013), G?unther and Furrer (2013) and Becker etal.
(2013), who all train linear models on a vari-ety of task-specific features.
In this year the cor-pus resources were used for a re-run of the sharedtask (Rosenthal et al., 2014), introducing two newTwitter test sets, as well as LiveJournal data.1A popular microblogging service on the internet, its mes-sages are commonly referred to as ?Tweets.
?4972 System DesciptionThis section describes the details of our sentimentanalysis system, focusing on the differences to ourlast year?s implementation.
This year we only par-ticipated in the subtask on whole message polarityclassification (Subtask B).2.1 PreprocessingFor tokenization of the messages we use thetokenizer of Owoputi et al.
(2013)?s TwitterNLP Tools2, which include a tokenizer and part-of-speech tagger optimized for the usage withTweets.
The tokenizer contains a regular expres-sion grammar for recognizing emoticons, which isan especially valuable property in the context ofsentiment analysis due to the high emotional ex-pressiveness of emoticons.It is well known that the way word tokens arerepresented may have a significant impact on theperformance of a lexical classifier.
This is par-ticularly true in natural language processing ofsocial media, where we run into the problem ofspelling variation causing extreme lexical sparsity.To deal with this issue we normalize the tokenswith the following technique: First, all tokens areconverted to lowercase and the hashtag sign (#) isremoved if present.
If the token is not present inan English word list or any of the used sentimentlexica (see below), we remove all directly repeatedletters after the first repetition (e.g.
greeeeaaat?greeaat).
If the resulting token is still not presentin any of the lexical resources, we allow no directrepetition of letters at all.
While this might leadto lexical collisions in some cases (e.g.
goooodd?
goodd ?
god), it is an easy and efficient wayto remove some lexical sparsity.
While generatingall possible combinations of deletions and check-ing the resulting tokens against a lexical resourceis another option, a correct disambiguation of theintended word would require a method making useof context knowledge (e.g.
goooodd?
good, vs.goooodd?
god).2.2 FeaturesWe use the following set of features as input to oursupervised classifier:?
The normalized tokens as unigrams and bi-grams, where stopword and punctuation to-kens are excluded from bigrams2http://www.ark.cs.cmu.edu/TweetNLP?
The word stems of the normalized tokens,reducing inflected forms of a word to a com-mon form.
The stems were computed usingthe Porter stemmer algorithm (Porter, 1980)?
The IDs of the token?s word clusters.The clusters were generated by performingBrown clustering (Brown et al., 1992) on56,345,753 Tweets by Owoputi et al.
(2013)and are available online.2?
The presence of a hashtag or URL in the mes-sage (one feature each)?
The presence of a question mark token in themessage?
We use the opinion lexicon by Bing Liu (Huand Liu, 2004), the MPQA subjectivity lex-icon (Wiebe et al., 2005) and the Twitrratrwordlist, which all provide a list of positiveand negative words, to compute a prior polar-ity of the message.
For each of the three sen-timent lexica two features capture whetherthe majority of the tokens in the messagewere in the positive or negative sentiment list.The same is done for hashtags using the NRChashtag sentiment lexicon (Mohammad et al.,2013).?
We apply special handling to features in anegation context.
A token is considered asnegated if it occurs after a negation word (upto the next punctuation).
All token, stem andword cluster features are marked with a nega-tion prefix.
Additionally, the polarity for to-ken in a negation context is inverted whencomputing the prior lexicon polarity.?
We use the part-of-speech tags computed bythe part-of-speech tagger of the Twitter NLPtools by Owoputi et al.
(2013) to excludecertain tokens.
Assuming they do not carryany helpful sentiment information, no fea-tures are computed for token recognized asname (tag ?)
or user mention (tag @).?
We also employ feature weighting to givemore importance to certain features and indi-cation of emphasis by the author.
Normally,all features described above receive weight 1if they are present and weight 0 if they are ab-sent.
For each of the following cases we add+1 to the weight of a token?s unigram, stemand word cluster features:498?
The original (not normalized) token isall uppercase?
The original token has more than threeadjacent repetitions of one letter?
The token is an adjective or emoticon(according to its part-of-speech tag)Furthermore, the score of each token is di-vided in half, if the token occurs in a ques-tion context.
A token is considered to be ina question context, if it occurs before a ques-tion mark (up to the next punctuation).2.3 Machine Learning MethodsAll training was done using the open-source ma-chine learning toolkit scikit-learn3(Pedregosa etal., 2011).
Just as in our last year?s systemwe trained linear one-versus-all classifiers us-ing stochastic gradient descent optimization withhinge loss and elastic net regularization.4For fur-ther details see G?unther and Furrer (2013).
Thenumber of iterations was set to 1000 for the finalmodel and 100 for the experiments.It is widely observed that training on a lot oflexical features can lead to brittle NLP systems,that are easily overfit to particular domains.
In so-cial media messages the brittleness is particularlyacute due to the wide variation in vocabulary andstyle.
While this problem can be eased by usingcorpus-induced word representations such as thepreviously introduced word cluster features, it canalso be addressed from a learning point of view.Brittleness can be caused by the problem that verystrong features (e.g.
emoticons) drown out the ef-fect of other useful features.The method of random subspace learning(S?gaard and Johannsen, 2012) seeks to handlethis problem by forcing learning algorithms to pro-duce models with more redundancy.
It does thisby randomly corrupting training instances duringlearning, so if some useful feature is correlatedwith a strong feature, the learning algorithm hasa better chance to assign it a nonzero weight.
Weimplemented random subspace learning by train-ing the classifier on a concatenation of 25 cor-rupted copies of the training set.
In a corruptedcopy, each feature was randomly disabled with aprobability of 0.2.
Just as for the classifier, the hy-perparameters were optimized empirically.3Version 0.13.1, http://scikit-learn.org.4SGDClassifier(penalty=?elasticnet?,alpha=0.001, l1 ratio=0.85, n iter=1000,class weight=?auto?
)3 ExperimentsFor the experiments and the training of the finalmodel we used the joined training and develop-ment sets of subtask B.
We were able to retrieve10368 Tweets, of which we merged all sampleslabeled as objective into the neutral class.
This re-sulted in a training set of 3855 positive, 4889 neu-tral and 1624 negative tweets.
The results of theexperiments were obtained by performing 10-foldcross-validation, predicting positive, negative andneutral class.
Just as in the evaluation of the sharedtask the results are reported as average F-measure(F1) between positive and negative class.To be able to evaluate the contribution of thedifferent features groups to the final model we per-form an ablation study.
By disabling one featuregroup at the time one can easily compare the per-formance of the model without a certain feature tothe model using the complete feature set.
In Ta-ble 1 we present the results for the feature groupsbigrams (2gr), stems (stem), word clusters (wc),sentiment lexica (lex), negation (neg), excludingnames and user mentions (excl), feature weighting(wei) and random subspace learning (rssl).Negative Positive Avg.Prec Rec Prec Rec F1ALL 54.80 71.67 76.70 75.41 69.08-2gr -0.55 -0.49 -0.35 +0.20 -0.31-stem -1.47 -1.72 -0.49 -0.03 -0.92-wc -1.45 -1.60 -0.40 -1.66 -1.29-lex -1.73 -5.11 +1.06 -2.75 -1.99-neg -1.90 -3.14 -1.30 +0.36 -1.43-excl +0.31 -0.99 +0.59 +0.08 +0.08-wei -1.57 +0.43 -0.84 -0.34 -0.73-rssl +2.04 -4.37 +1.38 -2.88 -0.67Table 1: Feature ablation studyLooking at Table 1, we can see that removingthe sentiment lexica features causes the biggestdrop in performance.
This is especially true forthe recall of the negative class, which is underrep-resented in the training data and can thus profit themost from prior domain knowledge.
When com-paring to the features of our last year?s system, itbecomes clear that the used sentiment lexica canprovide a much bigger gain in performance thanthe previously used SentiWordNet.
Even thoughthey are outperformed by the sentiment lexica, theword cluster features still provide an additional in-499GU-MLT-LT (2013) RTRGO (2014)F1pos/neg F13-class Accuracy F1pos/neg F13-class AccuracyTwitter2013 65.42 68.13 70.42 69.10 70.92 72.54Twitter2014 65.77 66.59 69.40 69.95 69.99 72.53SMS2013 62.65 66.93 69.09 67.51 72.15 75.54LiveJournal2014 68.97 68.42 68.39 72.20 72.29 72.33Twitter2014Sarcasm 54.11 56.91 58.14 47.09 49.34 51.16Table 2: Final results of our submissions on the different test sets (Subtask B)crease in performance and can, in contrast to sen-timent lexica, be learned in a completely unsu-pervised manner.
Negation handling is an impor-tant feature to boost the precision of the classifier,while using random subspace learning increasesthe recall of the classes, which indicates that thetechnique indeed leads to more redundant models.Another interesting question in sentiment anal-ysis is, how machine learning methods com-pare to simple methods only relying on sentimentwordlists and how much training data is neededto outperform them.
Figure 1 shows the resultsof a training size experiment, in which we testedclassifiers, trained on different portions of a train-ing set, on the same test set (10-fold cross val-idated).
The two horizontal lines indicate theperformance of two simple classifiers, using theTwitrratr wordlist (359 entries, labeled TRR) orBing Liu opinion lexicon (6789 entries, labeledLIU) with a simple majority-vote strategy (choos-ing the neutral class in case of no hits or no ma-jority and including a polarity switch for token ina negation context).
The baseline of the machinelearning classifiers is a logistic regression4550556065Training samples per classAverage3?class F150 250 500 750 1000 1250TRRLIU+LEX+RESTBOWFigure 1: Training size experimentclassifier using only uni- and bigram features andnegation handling (labeled BOW).
To this baselinewe add either the lexicon features for the Bing Liuopinion lexicon and the Twitrratr wordlist (labeled+LEX) or all other features described in section2.2 excluding lexicon features (labeled +REST).Looking at the results, we can see that a simplebag of words classifier needs about 250 samplesof each class to outperform the TRR list and about700 samples of each class to outperform the LIUlexicon on the common test set.
Adding the fea-tures that can be obtained without having senti-ment lexica available (+REST) reduces the neededtraining samples about half.
It is worth noting thatfrom a training set size of 1250 samples per classthe +REST-classifier is able to match the results ofthe classifier combining bag of words and lexiconfeatures (+LEX).4 Results and ConclusionThe results of our system are presented in Table 2,where the bold column marks the results relevantto our submission to this year?s shared task.
Wealso give results for our last year?s system.
Be-side the average F-measure between positive andnegative class, on which the shared task is evalu-ated, we also provide the results of both systems asaverage F-measure over all three classes and accu-racy to create possibilities for better comparisonto other research.
In this paper we showed sev-eral ways to improve a machine learning classifierfor the use of sentiment analysis in Twitter.
Com-pared to our last year?s system we were able toincrease the performance about several F-measurepoints on all non-sarcastic datasets.AcknowledgementsWe would like to thank the organizers of theshared task for their effort, as well as the anony-mous reviewers for their helpful comments on thepaper.500ReferencesLuciano Barbosa and Junlan Feng.
2010.
Robust sen-timent detection on twitter from biased and noisydata.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,pages 36?44.
Association for Computational Lin-guistics.Lee Becker, George Erhart, David Skiba, and Valen-tine Matula.
2013.
Avaya: Sentiment analysis ontwitter with self-training and polarity lexicon expan-sion.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop onSemantic Evaluation (SemEval 2013), pages 333?340, Atlanta, Georgia, USA, June.
Association forComputational Linguistics.Adam Bermingham and Alan F Smeaton.
2010.
Clas-sifying sentiment in microblogs: is brevity an advan-tage?
In Proceedings of the 19th ACM internationalconference on Information and knowledge manage-ment, pages 1833?1836.
ACM.Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n-gram models of natural language.Computational linguistics, 18(4):467?479.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics:Posters, pages 241?249.
Association for Computa-tional Linguistics.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.CS224N Project Report, Stanford.Tobias G?unther and Lenz Furrer.
2013.
GU-MLT-LT: Sentiment analysis of short messages using lin-guistic features and stochastic gradient descent.
InSecond Joint Conference on Lexical and Computa-tional Semantics (*SEM), Volume 2: Proceedingsof the Seventh International Workshop on Seman-tic Evaluation (SemEval 2013), pages 328?332, At-lanta, Georgia, USA, June.
Association for Compu-tational Linguistics.Tobias G?unther.
2013.
Sentiment analysis of mi-croblogs.
Master?s thesis, University of Gothenburg,June.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 168?177.ACM.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: The goodthe bad and the omg.
In Proceedings of the Fifth In-ternational AAAI Conference on Weblogs and SocialMedia, pages 538?541.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.Saif Mohammad, Svetlana Kiritchenko, and XiaodanZhu.
2013.
Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets.
In Proceedingsof the seventh international workshop on Seman-tic Evaluation Exercises (SemEval-2013), Atlanta,Georgia, USA, June.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
Semeval-2013 task 2: Sentiment analysis intwitter.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop onSemantic Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, USA, June.
Association forComputational Linguistics.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R Routledge, and Noah A Smith.
2010.From tweets to polls: Linking text sentiment topublic opinion time series.
In Proceedings of theInternational AAAI Conference on Weblogs andSocial Media, pages 122?129.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL 2013.Alexander Pak and Patrick Paroubek.
2010.
Twitter asa corpus for sentiment analysis and opinion mining.In Proceedings of LREC, volume 2010.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in naturallanguage processing-Volume 10, pages 79?86.
As-sociation for Computational Linguistics.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-dre Passos, David Cournapeau, Matthieu Brucher,Matthieu Perrot, and?Edouard Duchesnay.
2011.Scikit-learn: Machine learning in Python.
Journalof Machine Learning Research, 12:2825?2830.Martin F Porter.
1980.
An algorithm for suffix strip-ping.
Program: electronic library and informationsystems, 14(3):130?137.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanova.
2014.
Semeval-2014 task 9:Sentiment analysis in twitter.
In Proceedings ofthe International Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland, August.501Anders S?gaard and Anders Johannsen.
2012.
Robustlearning in random subspaces: Equipping NLP forOOV effects.
In COLING (Posters), pages 1171?1180.Michael Speriosu, Nikita Sudan, Sid Upadhyay, andJason Baldridge.
2011.
Twitter polarity classifica-tion with label propagation over lexical links and thefollower graph.
In Proceedings of the First Work-shop on Unsupervised Learning in NLP, pages 53?63.
Association for Computational Linguistics.Mike Thelwall, Kevan Buckley, Georgios Paltoglou,Di Cai, and Arvid Kappas.
2010.
Sentimentstrength detection in short informal text.
Journal ofthe American Society for Information Science andTechnology, 61(12):2544?2558.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation, 39(2-3):165?210.Ley Zhang, Riddhiman Ghosh, Mohamed Dekhil, Me-ichun Hsu, and Bing Liu.
2011.
Combining lex-iconbased and learning-based methods for twittersentiment analysis.
HP Laboratories, Technical Re-port HPL-2011-89.502
