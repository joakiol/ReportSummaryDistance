META-COMPILING TEXT GRAMMARSAS A MODEL FOR HUMAN BEHAVIORSheldon KleinComputer Sciences DepartmentUniversity of WisconsinI.
BACKGROUNDIn our efforts to model the totality ofsynchronic and diachronic language behaviorin complex social groups, we developed ameta-symbolic simulation system thatincludes a powerful behavioral simulationprogramming language that models, generatesand manipulates events in the notation of asemantic network that changes through time,and a generalized, semantics-to-surfacestructure generation mechanism that candescribe changes in the semantic universe inthe syntax of any natural language for whicha grammar is supplied.
Because the systemis a meta-theoretical device, it can handlegenerative semantic grammars formulatedwithin a variety of theoretical frameworks.A key feature of the system is that thesemantic deep structure of the non-verbal,behavioral rules may be represented in thesame network notation as the semantics fornatural language grammars, and, as aconsequence, provide non-verbal context forlinguistic rules.We are also experimenting with anatural language meta-compiling capability,that is, the use of the semantic network togenerate productions in the simulationlanguage itself -- productions in the formof "texts" that may themselves be compiledas new behavioral rules during the flow ofthe simulation -- rules that may themselvescontrol the process of deriving new rules.This feature permits non-verbal behavioralrules to be derived from natural languageconversational inputs, and through inferencetechniques identical with those forinferring natural language generativesemantic grammars.
The total system has thepower of at least the 2nd order predicatecalculus, and will facilitate theformulation of highly abstract meta-modelsof discourse, including the logicalquantification of such models.Achievements with the generativeportion of the system include a text grammarmodel that generates 2100 word murdermystery stories in less than 19 secondseach, complete with calculation of the plotand specification of the deep structure aswell as the surface syntax (Klein et al1973).
The speed of this generation is 100to 1000 times faster than other existingprograms using transformational grammars.
(The algorithm for the semantics-to-surfacestructure generative component is such thatprocessing time increases only linearly as afunct ion  of sentence length and syntacticcomplexity.
)More recent achievements include modelsof portions of Levi-Strauss" mythology workin The Raw & the Cooked (Levi-Strauss 1969)and a model for Propp's Morphology of the84!Folktale (Propp 1968) which generated 50Russian fairytales, according to the ru les  ?of his text grammar, at an average speed of i 128 words a second, again including plotcomputations and specification of deepstructure as well as surface syntax (Klein iet al1974, Klein et al 1975).
IOur earliest automatic text generationwork used syntactic dependencynetwork/graphs with 2-valued labelling of ?edges as an approximation to semantic |network/graphs with multi-valued labellingof edges (Klein & Simmons 1963, Klein 1965a,1965b).
iOur work on automatic inference of Igrammars includes the world's first programfor learning context free, phrase structuregrammars, for both natural and artificiallanguages, and the first program forlearning transformational grammars (Klein1967, Klein et al1968, Klein & Kuppin1970).
More recent inference work includesthe formulation of techniques for automaticinference of generative semantic grammars(Klein 1973) and for the ontogeny of Pidginand Creole languages (Klein & Rozencvejg1974).IIIIThe text grammarian movement, centeredin Germany and Holland, includes work such Ias that of van Dijk, Ihwe, Pet~fi and Rieser l (1972), Pet6fi and Rieser (1973), Pet~fi(1973), van Dijk (1973), and van Dijk andPet6fi (1974).
The underlying motivation of Jthis group is the belief that Chomskian I derived linguistic theories are inadequateto handle the complexities of complexnarrative and discourse -- that morepowerful logical devices are needed.
An ?attempted refutation of the text grammarian Qposition appeared in Dascal & Margalit(1974).
Our own work on Propp andLevi-Strauss models refutes the refutation Jby demonstration (Klein et al1974).
I!To provide the reader with an intuitiveview of the nature of a text grammar, weoffer the following two Russian fairytalesgenerated by our automated model of Propp(Klein et al1974).
The same text grammarIIn formulating components for automaticinference of rules in the meta-symbolicsimulation system, we find that the commonnotation for the semantics of the non-verbalbehavioral simulation rules and naturallanguage means that the same learningheuristics may be used to infer behavioralrules as well as linguistic rules.
The ?implication is that the totality of human |verbal and non-verbal behavior, in complexsocial groups, both synchronically anddiachronically, may now be modelled withinthe same notational framework.
What for us I started as a generalized device for testingvarying theoretical models as part of aneffort to model language change andvariation (Klein 1974a, Klein & Rozencvejg I1974) now appears as the basis for a higher m level theory of the linguistic basis ofhuman behavior (Klein 1974b).iII.
WHAT IS A TEXT GRAMMAR?
Igenerated both s tor ies  from a structuralmodel at a level of abstract ion thatprovided a semantic unif icat ion of theapparent surface diversity.Tale ITHE BORISIEVICHES LIVE IN A DISTANCEPROVINCE.THE FATHER IS EMELYA.THE ONLY SON IS BORIS.MARTHA IS THE ONLY DAUGHTER.EMELYA HAS THE SHEEP.BORIS, MARTHA AND THE SHEEP ARE IN THEWOODS.BORIS SAYS MARTHA, DO NOT LEAVE THE WOODS.BORIS LEAVES TO GO BERRY GATHERING.MARTHA LEAVES THE WOODS.A WOLF APPEARS IN THE DISTANT PROVINCE.EMELYA ASKS THE WOLF WHERE IS YOURWISDOM.THE WOLF SAYS THAT MY WISDOM IS IN A MAGICEGG.THE WOLF PLUNDERS THE SHEEP.EMELYA SENDS MARTHA TO SEARCH FOR THE WOLF.MARTHA DECIDES TO SEARCH FOR THE WOLF.MARTHA LEAVES ON A SEARCH.MARTHA MEETS A WITCH ALONG THE WAY.THE WITCH PROPOSES THAT MARTHA LISTEN TO THEGUSLA WITHOUT FALLING ASLEEP.MARTHA RESPONDS BY STAYING AWAKE WHILELISTENING TO THE GUSLA.A MAGIC WAFER IS CONSUMED BY MARTHA.MARTHA OBTAINS SUPER-HUMAN STRENGTH.MARTHA TRAVELS TO THE LOCATION OF THE WOLFIN ANOTHER KINGDOM.MARTHA IS DIRECTED BY A HEDGEHOG.MARTHA FINDS THE WOLFTHEY FIGHT IN AN OPEN FIELD.MARTHA IS WOUNDED.MARTHA DEFEATS THE WOLF WITH THE AID OFSUPER-HUMAN STRENGTH.THE WOLF IS CAUGHT BY MARTHA.MARTHA STARTS BACK HOME.MARTHA RETURNS HOME.Tale 2THE MOREVNAS LIVE IN A DISTANT PROVINCE.THE FATHER IS EREMA.THE MOTHER IS VASILISA.THE OLDEST SON IS BALDAK.THE YOUNGER SON IS MARCO.THE YOUNGEST SON IS BORIS.THE OLDEST DAUGHTER IS MARIA.THE YOUNGER DAUGHTER IS KATRINA.THE YOUNGEST DAUGHTER IS MARTHA.NICHOLAS ALSO LIVES IN THE SAME LAND.NICHOLAS IS OF MIRACULOUS BIRTH.BALDAK HAS A MAGIC STEED.A BEAR APPEARS IN THE DISTANT PROVINCE.THE BEAR SEIZES THE MAGIC STEED.BALDAK CALLS FOR HELP FROM NICHOLAS.NICHOLAS DECIDES TO SEARCH FOR THE MAGICSTEED.NICHOLAS LEAVES ON A SEARCH.NICHOLAS MEETS A JUG ALONG THE WAY.THE JUG IS FIGHTING WITH ELENA OVER A MAGICBOW.THE JUG ASKS NICHOLAS TO DIVIDE THE MAGICBOW.NICHOLAS TRICKS THE DISPUTANTS INTO LEAVINGTHE MAGIC BOW UNPROTECTED.THE MAGIC BOW, A MAGIC CARPET AND A MAGICBOX ARE SEIZED BY NICHOLAS.NICHOLAS TRAVELS TO THE LOCATION OF THEMAGIC STEED IN ANOTHER KINGDOM.NICHOLAS BY THE MAGIC CARPET.NICHOLAS FINDS THE BEAR.NICHOLAS SURPRISES THE BEAR.NICHOLAS KILLS THE BEAR WITH THE AID OF THEMAGIC BOW.THE MAGIC STEED APPEARS FROM THE MAGIC BOX.NICHOLAS STARTS BACK HOME.THE BEAR'S FATHER CHASES AFTER NICHOLAS.NICHOLAS ESCAPES BY FLYING ON A FALCON.NICHOLAS RETURNS HOME.III.
THE KEY QUESTIONWe perceive the locus of theoret icalinterest to  be the process of verbal andnon-verbal  behavior transmission acrossgenerations.
Our work on model l ing speechcommunit ies includes designs for s imulat ionsin which many model led individuals, eachwith his own semantic network, his owngrammar(s),  his own behavior rules, interactwith each other according to the model ledrules of the social structure of the society(Klein 1974a).It is our hope to be able to model thet ransmiss ion process of all the rules in thesystem.
This means that newly born model ledindiv iduals  wil l  infer rules for naturallanguage and also for non-verba l  behaviorals imulat ion rules, as a function of inputs oftexts suppl ied by other model ledindividuals.
The texts may be verbaldiscourse, or non-verbal  sequences ofbehavior.
The learning individual wil lactual ly  compile and recompile new versionsof his own behavioral  rules as thes imulat ion process proceeds.
His own testproduct ions of behavior scenarios as well asnatural  language discourse wil l  be subjectto evaluat ion and possible correct ion byo ther  members of the model led community, andtheir react ions as well as the consequencesof the productions,  wil l  serve as a controlon the entire learning process.
And, asindicated earlier, the rules to be inferred,compiled and recompi led will include rulesthat govern the process of inference andcompi lat ion itself.IV.
LOGICAL QUANTIFICATION, SEMANTICPARSING, PRESUPPOSIT IONAL ANALYSISWe have ment ioned the 2nd order orhigher predicate calculus.
For ourpurposes, the essential  feature is that thelogical quant i f icat ion of the rules may bequant i f ied by the contents of the rulesthemselves.
Meta-compi l ing of rulesgoverning meta-compi l ing is an example ofthis process.There are other techniques avai lable.The behavioral  rules operate with h igh- levelclasses that make it possible to formulaterules that can treat objects, characters andcomplex act ions as mani festat ions of the85same abstract semantic Unit.
A major typeof behavior rule modif icat ion and extensionis the abi l i ty to requantify the rules as aheurist ic function of experience.
Theprocess does not involverecompi lat ion -- rather modif icat ion of thedomain of appl icabi l i ty  of an existing rule.One of the types of semantic parsingpossible in the system is the determinat ionof the presupposit ions of the semanticcontent of input text.
The scenario rulesthat could have generated the text haveprecondit ions, and these precondit ions alsohave their own precondit ions as specif ied byother rules.
In cases where the semanticcontent of an input text is not potent ia l lyderivable from exist ing behavioral  rules,the system can posit requant i f icat ion(assignments and reassignments to semanticclasses) to make the input text derivable.Or, if necessary, the same end can beachieved by compi l ing new rules that wouldmake the text plausible.General izat ion of the method makes itpossible to build complex learning modelsfor highly abstract, semantical ly  driventext grammars.
Perhaps the ult imate test isthe model l ing of the heurist ic processes ofLevi-Strauss.
We hope to be able to build amodel that learns text grammars witharb i t rar i ly  abstract semantics such as thatmanifested in Levi -Strauss (1969).
At themoment, we are working on model l ing the textgrammar he himself  has derived (Klein et al1975).
The potential  of our work is tohandle a degree and kind of abstract ion insemantics heretofore untouched byl inguistics, including the model l ing of theautomatic creation of text grammars fordreams and myths as a function of culturalrules.V.
GENERALITY OF THE META-SYMBOLICSIMULATION SYSTEM AS A THEORY TESTING DEVICEOur methodology and programming sty lehave y ie lded a system wherein all the rules,and even the form of the theories in whichthey are cast, are input as data.
As far aswe can determine, this permits us to encodein our system virtual ly  all the theoret icalmodels current ly prevalent in l inguistics,plus heretofore unformulated models ofvastly greater power.
(Prel iminary work inthe classroom, for example, indicates thatmodels of the work of Schank and hisstudents may easi ly be implemented in oursystem, with an increased speed of executionof about 50 to I in favor of our versions.)VI.
THE METHODOLOGICAL SIGNIFICANCE OF OURWORKOur work over the years has suggestedand reinforced the fol lowing methodologica lprinciples:I.
No s igni f icant theories can beformulated in L inguist ics that arenot computed based.862.
The theoret ical  foundations ofComputer Science are identical  withthose of Linguist ics.3.
Theoret ical  l inguistic models thatare not strongly l inked toobject ive tasks are meaningless.No semantics is meaningful  exceptin terms of the object ive tasks itfaci l itates.4.
The future of Linguistics,Computat ional  Linguist ics,Art i f ic ia l  Intel l igence,Psychological  models of humanbehavior, are in the future of theFoundat ions of ProgrammingLanguages and the Theory ofOperat ing Systems.
The human mindis at least as compl icated as anoperat ing system for a 4thgenerat ion computer.5.
An adequate l inguist ic theory mustaccount for the function oflanguage in social groups and itst ransmiss ion through time andspace.
At the same time, such atheory must account for the highestsemantic atta inments of the humanmind, including l i terature and art,and, in fact, the total i ty ofsymbolic processes.6.
Input/output equ iva lence  of modeland model led does not implyisomorphism between model andmodelled.
(Chomskian bel iefs tothe contrary have their roots inLeibniz" Theory of Monads and itsrequired ontological  argument.
)There are no models of performance,only models of competence which canbe compared, one against the other,for accuracy in predict ingrelat ions between input and outputin real world systems.VII.
THEORETICAL IMPLICATIONSA.
The Non- inateness of HumanStructuresMentalOur work const i tutes a refutat ion bycounter example of the necessity for acorrelat ion between models of human mentalstructures and the structure of the humanbrain.
(A software system can operate withno inherent isomorphisms with a part icularcomputer.)
Nothing need be innate except themeta-compi l ing capacity and the percept ionof time.Our work suggests the logicalposs ib i l i ty  that the human mind can learn tolearn, and learn how to learn to learn, andthat each human may do it dif ferently.
Thebasic pr inciples of language inference,which can be derived from a behavior ist icpsychological  framework, can alone accountfor the structur ing of mental processes as asoftware phenomenon, independent ofphysio logical  reality.
It fol lows thathumans can have dif ferent rules, di f ferentdata structures, di f ferent h ierarchicalD!!IIIi!II!!!!!
!iIIiorganizations, where the only control l ingfactor is the requirement that theinternal ized models permit the individualsto function and interact with the inputs andoutputs of other individuals in a socialgroup.B.
History as the Meta- language ofHistoryImplicit in our approach is analternat ive to the concept of an infinitehierarchy of meta- languages, as formulatedby Bertrand Russell in his Theory of Typesin Principia Mathematica (Whitehead &Russell 1911-1913).
The concept ofsuccessive states of time, each linked withthe possibi l i ty of defining (meta-compil ing)new rules of the universe for the nextstate, ( including the rules for defining newrules), suggests that there need be only asingle meta- language and a single languagein any state at any point in time, and thateach serves, in turn, as the meta- languagefor the other in successive time frames.This is not a stochastic process.It is the concepts of time andmeta-compi l ing that appear to be thefundamental  aspects of human cognition.
Theprinciple may be universal  for all humanbehavioral /symbol ic  processes, and studentsof the ph i losophy  of history wil l  nowrecognize our meta-symbol ic  s imulat ionsystem as equivalent to an automatedHegel ian dialectic phi losophy whichspecif ies that each successive state ofhistor ical  development is control led by themeta- language of its previous state, andbecomes the meta- language of its successorstate.REFERENCESDascal, M. and A. Margal it  1974.
A new"revolution" inl inguist ics?
-- "Text-grammars" vs."sentence-grammars."
Theoret icalLinguistics, 1:195-213van Dijk, T.A.
(ed), 1973, Text Grammar andNarrat ive Structures.
Poetics 3.van Dijk, T.A., J. Ihwe, J.S.
Pet~fi andH.
Rieser 1972, Zur Best immung vonnarrat iven Strukturen auf der Grundlagevon Textgrammatiken.Verlag.Hamburg: Buskevan Dijk, T.A.
and J. Pet~fi (eds) 1974.Grammars and Descriptions.
Ber l in- -NewYork: de Gruyter.Klein, S. 1965a.
Automatc paraphrasing inessay format.
Mechanical  Translat ion8.3/4:68-83..... 1965b.
Control of style with agenerat ive grammar.
Language 41:619-631.1967.
Current research in the computers imulat ion of histor ical  change inlanguage.
Actes d__uu X e Congres87Internat ional  des Linguistics.
Bucharest1967..... 1973.
Automatic inference of semanticdeep structure rules in generativesemantic Grammars.
Univ.
if WisconsinComp.
Sci.
Tech Report 180.
Also in1974.
Computat ional  and MathematicalLinguistics, Proceedings of the Int.Conf.
on Computat ional  Linguistics,Pisa, 1973.
A. Zampolli, ed., Florence:Olschki.... 1974a.
Computer simulation of languagecontact models.
In Towards Tomorrow'sLinguistics, Shuy & Bailey, editors,Washington, D.C.: Georgetown UniversityPress..... 1974b.
A computer model for thel inguist ic basis of the transmission ofculture.
Presented at 1974 Meeting ofAmerican Anthropological  Association,Mexico City, Nov. 1974.
(Final draft inpreparation)Klein, S., J.F.
Aeschlimann, D.F.Balsiger, S.L.
Converse, C. Court, M.Foster, R. Lao, J.D.
Oakley, and J.Smith 1973.
AUTOMATIC NOVEL WRITING: astatus report.
Univ.
of Wisc.
Comp.Sci.
Dept.
Tech Report 186.
Presentedat 1973 Int.
Conf.
on Computers in theHumanities.Klein, S., J.F.
Aeschlimann, M.A.Appelbaum, D.F.
Balsiger, E.J.
Curtis,M.
Foster, S.D.
Kalish, S.J.
Kamin,Y-D. Lee, L.A. Price, D.F.
Salsieder.1974.
Model l ing Propp and Levi-Straussin a Meta-symbol ic  Simulat ion System.Univ.
of Wisc.
Comp.
Sci.
Tech Report226.
In press in Patterns in OralLiterature, edited by Heda Jason andDimitr i  Segal as a retroact ivecontr ibut ion to this volume of the 1973Wolrd Conference of Anthropological  andEthnological  Sciences.
Chicago.Klein, S., W. Fabens, R. Herriot, W.Katke, M.A.
Kuppin, A. Towster 1968.The AUTOLING system.
Univ.
of Wisc.Comp.
Sci.
Dept.
Tech Report 43.Klein, S. and M.A.
Kuppin 1970.
Aninteractive, heurist ic program forlearning transformat ional  grammars.Computer Studies in the Humanit ies andVerbal Behavior, 3:144-162.Klein, S., L.A. Price, J.F.
Aeschlimann,D.A.
Bals iger and E.J.
Curtis, 1975.
AMeta-symbol ic  Simulat ion Model for FiveMyths from Levi -Strauss" The Raw and theCooked.
Univ.
of Wisc.
Comp.
Sci.Dept.
Tech Report Presented at 2nd Int.Conf.
on Computers in the Humanities,Chicago, Apri l  1975.Klein, S. and V. Rozencvejg 1974.
AComputer Model for the Ontogeny of Pidginand Creole Languages, Univ.
of Wisc.Comp.
Sci.
Dept.
Tech Report 238.Presented at the 1975 Int.
Conf.
onPidgins and Creoles, Hawaii, January1975.Klein, S. and R .F .
Simmons 1963.Syntactic dependence and the computergeneration of coherent discourse.Mechanical Translation 7:50-61.Levi-Strauss, C. 1969, The Raw and theCooked.
(English translation) New York:Harper & Row.PetSfi, J.S.
1973.
Toward an empiricallymotivated grammatical theory of verbaltexts.
In Studies in Text Grammar,edited by J.S.
Petofi & H. Rieser.Dordrecht: Reidel.Pet~fi, J.S.
& H. Rieser 1973.
Problemeder modelltheoretischen Interpretationyon Texten.
Hamburg: Buske Verlag.Propp, V. 1968.
Morphology of the Folkt~le(English translation) 2nd Edition,Austin: University of Texas Press.Whitehead, A.N.
and B. Russell 1911-1913.Princip~a Mathematica.
(3 volumesLondon: Cambridge University Press.88IItIIII
