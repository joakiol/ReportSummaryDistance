Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1225?1236,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsHuman Effort and Machine Learnability in Computer Aided TranslationSpence Green, Sida Wang, Jason Chuang,*Jeffrey Heer,*Sebastian Schuster,and Christopher D. ManningComputer Science Department, Stanford University{spenceg,sidaw,sebschu,manning}@stanford.edu*Computer Science Department, University of Washington{jcchuang,jheer}@uw.eduAbstractAnalyses of computer aided translation typi-cally focus on either frontend interfaces andhuman effort, or backend translation andmachine learnability of corrections.
How-ever, this distinction is artificial in prac-tice since the frontend and backend mustwork in concert.
We present the first holis-tic, quantitative evaluation of these issuesby contrasting two assistive modes: post-editing and interactive machine translation(MT).
We describe a new translator inter-face, extensive modifications to a phrase-based MT system, and a novel objectivefunction for re-tuning to human correc-tions.
Evaluation with professional bilin-gual translators shows that post-edit is fasterthan interactive at the cost of translationquality for French-English and English-German.
However, re-tuning the MT sys-tem to interactive output leads to larger, sta-tistically significant reductions in HTERversus re-tuning to post-edit.
Analysisshows that tuning directly to HTER resultsin fine-grained corrections to subsequentmachine output.1 IntroductionThe goal of machine translation has always been toreduce human effort, whether by partial assistanceor by outright replacement.
However, preoccupa-tion with the latter?fully automatic translation?atthe exclusion of the former has been a feature ofthe research community since its first nascent stepsin the 1950s.
Pessimistic about progress duringthat decade and future prospects, Bar-Hillel (1960,p.3) argued that more attention should be paid to a?machine-post-editor partnership,?
whose decisiveproblem is ?the region of optimality in the contin-uum of possible divisions of labor.?
Today, withhuman-quality, fully automatic machine translation(MT) elusive still, that decades-old recommenda-tion remains current.This paper is the first to look at both sides ofthe partnership in a single user study.
We comparetwo common flavors of machine-assisted transla-tion: post-editing and interactive MT.
We analyzeprofessional, bilingual translators working in bothmodes, looking first at user productivity.
Does theadditional machine assistance available in the inter-active mode affect translation time and/or quality?Then we turn to the machine side of the part-nership.
The user study results in corrections tothe baseline MT output.
Do these corrections helpthe MT system, and can it learn from them quicklyenough to help the user?
We perform a re-tuningexperiment in which we directly optimize humanTranslation Edit Rate (HTER), which correlateshighly with human judgments of fluency and ade-quacy (Snover et al., 2006).
It is also an intuitivemeasure of human effort, making fine distinctionsbetween 0 (no editing) and 1 (complete rewrite).We designed a new user interface (UI) for theexperiment.
The interface places demands on theMT backend?not the other way around.
The mostsignificant new MT system features are prefix de-coding, for translation completion based on a userprefix; and dynamic phrase table augmentation, tohandle target out-of-vocabulary (OOV) words.
Dis-criminative re-tuning is accomplished with a novelcross-entropy objective function.We report three main findings: (1) post-editingis faster than interactive MT, corroborating Koehn(2009a); (2) interactive MT yields higher qualitytranslation when baseline MT quality is high; and(3) re-tuning to interactive feedback leads to largerheld-out HTER gains relative to post-edit.
Togetherthese results show that a human-centered approachto computer aided translation (CAT) may involvetradeoffs between human effort and machinelearnability.
For example, if speed is the toppriority, then a design geared toward post-editing1225ABCD EFigure 1: Main translation interface.
The user sees the full document context, with French source inputs(A) interleaved with suggested English translations (B).
The sentence in focus is indicated by the bluerectangle, which can be moved via two hot keys.
Source coverage (C) of the user prefix?shaded inblue?updates as the user works, as do autocomplete suggestions (D) and a full completion (E).is appropriate.
However, if reductions in HTERultimately correspond to lower human effort, theninvesting slightly more time in the interactive mode,which results in more learnable output, may be op-timal.
Mixed UI designs may offer a compromise.Code and data from our experiments are available at:http://nlp.stanford.edu/software/phrasal/A holistic comparison with human subjects nec-essarily involves many moving parts.
Section 2briefly describes the interface, focusing on NLPcomponents.
Section 3 describes changes to thebackend MT system.
Section 4 explains the userstudy, and reports human translation time and qual-ity results.
Section 5 describes the MT re-tuningexperiment.
Analysis (section 6) and related work(section 7) round out the paper.2 New Translator User InterfaceFigure 1 shows the translator interface, which isdesigned for expert, bilingual translators.
Previ-ous studies have shown that expert translators workand type quickly (Carl, 2010), so the interface isdesigned to be very responsive, and to be primar-ily operated by the keyboard.
Most aids can beaccessed via either typing or four hot keys.
Thecurrent design focuses on the point of text entryand does not include conventional translator work-bench features such as workflow management, spellchecking, and text formatting tools.In the trivial post-edit mode, the interactive aidsare disabled and a 1-best translation pre-populatesthe text entry box.We have described the HCI-specific motivationsfor and contributions of this new interface in Greenet al.
(2014c).
This section focuses on interfaceelements built on NLP components.2.1 UI Overview and WalkthroughWe categorized interactions into three groups:source comprehension: word lookups, source cov-erage highlighting; target gisting: 1-best transla-tion, real-time target completion; target genera-tion: real-time autocomplete, target reordering, in-sert complete translation.
The interaction designsare novel; those in italic have, to our knowledge,never appeared in a translation workbench.Source word lookup When the user hovers overa source word, a menu of up to four ranked trans-lation suggestions appears (Figure 2).
The menuis populated by a phrase-table query of the wordplus one token of left context.
This query usuallyreturns in under 50ms.
The width of the horizontalbars indicates confidence, with the most confidentsuggestion ?regularly?
placed at the bottom, near-est to the cursor.
The user can insert a translationsuggestion by clicking.Source coverage highlighting The source cover-age feature (Figure 1C) helps the user quickly finduntranslated words in the source.
The interaction is1226Figure 2: Source word lookup and target autocom-plete menus.
The menus show different suggestions.The word lookupmenu (top) is not dependent on thetarget context Teachers, whereas the autocompletedropdown (bottom) is.based on the word alignments between source andtarget generated by the MT system.
We found thatthe raw alignments are too noisy to show users, sothe UI filters them with phrase-level heuristics.1-best translation The most common use of MToutput is gisting (Koehn, 2010, p.21).
The gray textbelow each black source input shows the best MTsystem output (Figure 1B).Real-time target completion When the user ex-tends the black prefix, the gray text will update tothe most probable completion (Figure 1E).
This up-date comes from decoding under the full translationmodel.
All previous systems performed inferencein a word lattice.Real-time autocomplete The autocompletedropdown at the point of text entry is the maintranslation aid (Figures 1D and 2).
Each real-timeupdate actually contains a distinct 10-best list forthe full source input.
The UI builds up a trie fromthese 10-best lists.
Up to four distinct suggestionsare then shown at the point of translation.
Thesuggestion length is based on a syntactic parse ofthe fixed source input.
As an offline, pre-processingstep, we parse each source input with StanfordCoreNLP (Manning et al., 2014).
The UI combinesthose parses with word alignments from the fulltranslation suggestions to project syntactic con-stituents to each item on the n-best list.
Syntacticprojection is a very old idea that underlies manyMT systems (see: Hwa et al.
(2002)).
Here wemake novel use of it for suggestion predictionfiltering.1Presently, we project noun phrases,verb phrases (minus the verbal arguments), andprepositional phrases.
Crucially, these units arenatural to humans, unlike statistical target phrases.Target Reordering Carl (2010) showed that ex-pert translators tend to adopt local planning: theyread a few words ahead and then translate in aroughly online fashion.
However, word order differ-ences between languages will necessarily requirelonger range planning and movement.
To that end,the UI supports keyboard-based reordering.
Sup-pose that the user wants to move a span in graytext to the insertion position for editing.
Typingthe prefix of this string will update the autocom-plete dropdown with matching strings from the graytext.
Consequently, sometimes the autocompletedropdown will contain suggestions from severalpositions in the full suggested translation.Insert complete translation The user can insertthe full completion via a hot key.
Notice that ifthe user presses this hot key immediately, all graytext becomes black, and the interface effectivelyswitches to post-edit mode.
This feature greatly ac-celerates translation when the MT is mostly correct,and the user only wants to make a few changes.2.2 User Activity LoggingA web application serves the Javascript-based in-terface, relays translation requests to the MT sys-tem, and logs user records to a database.
Each userrecord is a tuple of the form (f, e?, h, u), where fis the source sequence, e?
is the latest 1-best ma-chine translation of f , h is the correction of e?, andu is the log of interaction events during the transla-tion session.
Our evaluation corpora also includeindependently generated references e for each f .3 Interactive MT BackendNow we describe modifications to Phrasal (Greenet al., 2014b), the phrase-based MT system that sup-ports the interface.
Phrasal follows the log-linearapproach to phrase-based translation (Och and Ney,2004) in which the decision rule has the familiarlinear forme?
= arg maxew>?
(e, f) (1)1The classic TransType system included a probabilisticprediction length component (Foster et al., 2002), but we findthat the simpler projection technique works well in practice.1227where w ?
Rdis the model weight vector and?(?)
?
Rdis a feature map.3.1 DecodingThe default Phrasal search algorithm is cube prun-ing (Huang and Chiang, 2007).
In the post-edit con-dition, search is executed as usual for each sourceinput, and the 1-best output is inserted into the tar-get textbox.
However, in interactive mode, the fullsearch algorithm is executed each time the usermodifies the partial translation.
Machine sugges-tions e?
must match user prefix h. Define indicatorfunction pref(e?, h) to return true if e?
begins withh, and false otherwise.
Eq.
1 becomes:e?
= arg maxe s.t.pref(e,h)w>?
(e, f) (2)Cube pruning can be straightforwardly modified tosatisfy this constraint by simple string matching ofcandidate translations.
Also, the pop limit must besuspended until at least one legal candidate appearson each beam, or the priority queue of candidates isexhausted.
We call this technique prefix decoding.2There is another problem.
Human translators arelikely to insert unknown target words, includingnew vocabulary, misspellings, and typographicalerrors.
They might also reorder source text so as toviolate the phrase-based distortion limit.
To solvethese problems, we perform dynamic phrase tableaugmentation, adding new synthetic rules specificto each search.
Rules allowing any source word toalign with any unseen or ungeneratable (due to thedistortion limit) target word are created.3Thesesynthetic rules are given rule scores lower than anyother rules in the set of queried rules for that sourceinput f .
Then candidates are allowed to competeon the beam.
Candidates with spurious alignmentswill likely be pruned in favor of those that only turnto synthetic rules as a last resort.3.2 TuningWe choose BLEU (Papineni et al., 2002) for base-line tuning to independent references, and HTERfor re-tuning to human corrections.
Our rationaleis as follows: Cer et al.
(2010) showed that BLEU-tuned systems score well across automatic metricsand also correlate with human judgment better than2Och et al.
(2003) describe a similar algorithm for wordgraphs.3Ortiz-Mart?nez et al.
(2009) describe a related techniquein which all source and target words can align, with scores setby smoothing.systems tuned to other metrics.
Conversely, sys-tems tuned to edit-distance-based metrics like TERtend to produce short translations that are heavilypenalized by other metrics.When human corrections become available, weswitch to HTER, which correlates with human judg-ment and is an interpretable measure of editingeffort.
Whereas TER is computed as TER(e, e?
),HTER is HTER(h, e?).
HBLEU is an alternative,but since BLEU is invariant to some permutations(Callison-Burch et al., 2006), it is less interpretable.We find that it also does not work as well in practice.We previously proposed a fast, online tuning al-gorithm (Green et al., 2013b) based on AdaGrad(Duchi et al., 2011).
The default loss function isexpected error (EE) (Och, 2003; Cherry and Foster,2012).
Expected BLEU is an example of EE, whichwe found to be unstable when switching metrics.This may result from direct incorporation of theerror metric into the gradient computation.To solve this problem, we propose a cross-entropy loss which, to our knowledge, is new inMT.
Let?E = {e?i}ni=1be an n-best list rankedby a gold metric G(e, e?)
?
0.
Assume wehave a preference of a higher G (e.g., BLEU or1?HTER).
Define the model distribution over?Eas q(e?|f) ?
exp[w>?
(e?, f)] normalized so that?e??
?Eq(e?|f) = 1; q indicates howmuch the modelprefers each translation.
Similarly, define p(e?|f)based on any function of the gold metric so that?e??
?Ep(e?|f) = 1; p indicates how much the met-ric prefers each translation.
We choose a DCG-style4parameterization that skews the p distribu-tion toward higher-ranked items on the n-best list:p(e?i|f) ?
G(e, e?i)/ log(1 + i) for the ith rankeditem.
The cross-entropy (CE) loss function is:`CE(w;E) = Ep(e?|f)[?
log(q(e?|f)] (3)It turns out that if p is simply the posterior distribu-tion of the metric, then this loss is related to the logof the standard EE loss:5`EE(w;E) = ?
log[Ep(e?|f)[q(e?|f)]] (4)We can show that `CE?
`EEby applying Jensen?sinequality to the function ?
log(?).
So minimizing`CEalso minimizes a convex upper bound of thelog expected error.
This convexity given the n-4Discounted cumulative gain (DCG) is widely used in infor-mation retrieval learning-to-rank settings.
n-best MT learningis standardly formulated as a ranking task.5For expected error, p(e?i) = G(e, e?i) is not usually nor-malized.
Normalizing p adds a negligible constant.1228best list does not mean that the overall MT tuningloss is convex, since the n-best list contents andorder depend on the parameters w. However, allregret bounds and other guarantees of online con-vex optimization would now apply in the CE casesince `CE,t(wt?1;Et) is convex for each t. Thisis attractive compared to expected error, which isnon-convex even given the n-best list.
We empiri-cally observed that CE converges faster and is lesssensitive to hyperparameters than EE.Faster decoding trick We found that online tun-ing also permits a trick that speeds up decodingduring deployment.
Whereas the Phrasal defaultbeam size is 1,200, we were able to reduce the beamsize to 800 and run the tuner longer to achieve thesame level of translation quality.
For example, atthe default beam size for French-English, the algo-rithm converges after 12 iterations, whereas at thelower beam size it achieves that level after 20 itera-tions.
In our experience, batch tuning algorithmsseem to be more sensitive to the beam size.3.3 Feature TemplatesThe baseline system contains 19 dense feature tem-plates: the nine Moses (Koehn et al., 2007) baselinefeatures, the eight-feature hierarchical lexicalizedre-ordering model of Galley and Manning (2008),the (log) count of each rule in the bitext, and anindicator for unique rules.
We found that sparsefeatures, while improving translation quality, cameat the cost of slower decoding due to feature extrac-tion and inner products with a higher dimensionalfeature map ?.
During prototyping, we observedthat users found the system to be sluggish unlessit responded in approximately 300ms or less.
Thisbudget restricted us to dense features.When re-tuning to corrections, we extract fea-tures from the user logs u and add them to thebaseline dense model.
For each tuning input f ,the MT system produces candidate derivations d =(f, e?, a), where a is a word alignment.
The user logu also contains the last MT derivation6acceptedby the user du= (f, e?u, au).
We extract featuresby comparing d and du.
The heuristic we take isintersection: ?(d)?
?
(d) ?
?
(du).Lexicalized and class-based alignments Con-sider the alignment in Figure 3.
We find thatuser derivations often contain many unigram rules,6Extracting features from intermediate user editing actionsis an interesting direction for future work.tarcevaparvientainsi?stopperlacroissancetarcevawasthusabletohaltthegrowthFigure 3: User translation word alignment obtainedvia prefix decoding and dynamic phrase table aug-mentation.which are less powerful than larger phrases, butnonetheless provide high-precision lexical choiceinformation.
We fire indicators for both unigramlinks and multiword cliques.
We also fire class-based versions of this feature.Source OOV blanket Source OOVs are usuallymore frequent when adapting to a new domain.
Inthe case of European languages?our experimentalsetting?many of the words simply transfer to thetarget, so the issue is where to position them.
In Fig-ure 3, the proper noun tarceva is unknown, so the de-coder OOV model generates an identity translationrule.
We add features in which the source word isconcatenated with the left, right, and left/right con-texts in the target, e.g., {<s>-tarceva, tarceva-was, <s>-tarceva-was}.
We also add versionswith target words mapped to classes.3.4 Differences from Previous WorkOur backend innovations support the UI and enablefeature-based learning from human corrections.
Incontrast, most previous work on incremental MTlearning has focused on extracting new translationrules, language model updating, and modifyingtranslation model probabilities (see: Denkowskiet al.
(2014a)).
We regard these features as ad-ditive to our own work: certainly extracting new,unseen rules should help translation in a new do-main.
Moreover, to our knowledge, all previouswork on updating the weight vector w has consid-ered simulated post-editing, in which the indepen-dent references e are substituted for corrections h.Here we extract features from and re-tune to actualcorrections to the baseline MT output.12294 Translation User StudyWe conducted a human translation experiment witha 2 (translation conditions) ?
n (source sentences)mixed design, where n depended on the languagepair.
Translation conditions (post-edit and interac-tive) and source sentences were the independentvariables (factors).
Experimental subjects saw allfactor levels, but not all combinations, since oneexposure to a sentence would influence another.Subjects completed the experiment remotely ontheir own hardware.
They received personalizedlogin credentials for the translation interface, whichadministered the experiment.
Subjects first com-pleted a demographic questionnaire about prior ex-perience with CAT and language proficiency.
Next,they completed a training module that included a4-minute tutorial video and a practice ?sandbox?
fordeveloping proficiency with the UI.
Then subjectscompleted the translation experiment.
Finally, theycompleted an exit questionnaire.Unlike the experiment of Koehn (2009a), sub-jects were under time pressure.
An idle timer pre-vented subjects from pausing for more than threeminutes while the translator interface was open.This constraint eliminates a source of confound inthe timing analysis.We randomized the order of translation condi-tions and the assignment of sentences to conditions.At most five sentences appeared per screen, andthose sentences appeared in the source documentorder.
Subjects could move among sentences withina screen, but could not revise previous screens.
Sub-jects received untimed breaks both between trans-lation conditions and after about every five screenswithin a translation condition.4.1 Linguistic MaterialsWe chose two language pairs: French-English (Fr-En) and English-German (En-De).
Anecdotally,French-English is an easy language pair for MT,whereas English-German is very hard due to re-ordering and complex German morphology.We chose three text genres: software, medical,and informal news.
The software text came fromthe graphical interfaces of Autodesk AutoCAD andAdobe Photoshop.
The medical text was a drug re-view from the European Medicines Agency.
Thesetwo data sets came from TAUS7and included inde-pendent reference translations.
The informal newstext came from the WMT 2013 shared task test set7http://www.tausdata.org/(Bojar et al., 2013).
The evaluation corpus was con-structed from equal proportions of the three genres.The Fr-En dataset contained 3,003 source tokens(150 segments); the En-De dataset contained 3,002(173 segments).
As a rule of thumb, a human trans-lator averages about 2,700 source tokens per day(Ray, 2013, p.36), so the experiment was designedto replicate a slightly demanding work day.4.2 Selection of SubjectsFor each language pair, we recruited 16 profes-sional, freelance translators on Proz, which is thelargest online translation community.8We postedads for both language pairs at a fixed rate of $0.085per source word, an average rate in the industry.
Inaddition, we paid $10 to each translator for complet-ing the training module.
All subjects had significantprior experience with a CAT workbench.4.3 ResultsWe analyze the translation conditions in terms oftwo response variables: time and quality.
We ex-cluded one Fr-En subject and two En-De subjectsfrom the models.
One subject misunderstood the in-structions of the experiment and proceeded withoutclarification; another skipped the training moduleentirely.
The third subject had a technical problemthat prevented logging.
Finally, we also filteredsegment-level sessions for which the log of transla-tion time was greater than 2.5 standard deviationsfrom the mean.4.3.1 Translation TimeWe analyze time with a linear mixed effects model(LMEM) estimated with the lme4 (Bates, 2007) Rpackage.
When experimental factors are sampledfrom larger populations?e.g., humans, sentences,words?LMEMs are more robust to type II errors(see: Baayen et al.
(2008)).
The log-transformedtime is the response variable and translation condi-tion is the main independent variable.
The maximalrandom effects structure (Barr et al., 2013) containsintercepts for subject, sentence id, and text genre,each with random slopes for translation condition.We found significant main effects for translationcondition (Fr-En, p < 0.05; En-De, p < 0.01).The orientation of the coefficients indicates thatinteractive is slower for both language pairs.
For Fr-En, the LMEM predicts a mean time (intercept) of46.0 sec/sentence in post-edit vs. 54.6 sec/sentence8http://www.proz.com1230Fr-En En-DeTER HTER TER HTERpost-edit 47.32 23.51 56.16 37.15interactive 47.05 24.14 55.89 39.55Table 1: Automatic assessment of translation qual-ity.
Here we change the definitions of TER andHTER slightly.
TER is the human translations com-pared to the independent references.
HTER is thebaseline MT compared to the human corrections.in interactive, or 18.7% slower.
For En-De, themean is 51.8 sec/sentence vs. 63.3 sec/sentence ininteractive, or 22.1% slower.We found other predictive covariates that revealmore about translator behavior.
When subjects didnot edit the MT suggestion, they were significantlyfaster.
When token edit distance fromMT or sourceinput length increased, they were slower.
Subjectswere usually faster as the experiment progressed, aresult that may indicate increased proficiency withpractice.
Note that all subjects reported profes-sional familiarity with post-edit, whereas the in-teractive mode was entirely new to them.
In theexit survey many translators suggested that withmore practice, they could have been as fast in theinteractive mode.94.3.2 Translation QualityWe evaluated translation quality with both auto-matic and manual measures.
Table 1 shows thatin the interactive mode, TER is lower and HTERis higher: subjects created translations closer tothe references (lower TER), but performed moreediting (higher HTER).
This result suggests bettertranslations in the interactive mode.To confirm that intuition, we elicited judgmentsfrom professional human raters.
The setup followedthe manual quality evaluation of the WMT 2014shared task (Bojar et al., 2014).
We hired six raters?three for each language pair?who were paid be-tween $15?20 per hour.
The raters logged into Ap-praise (Federmann, 2010) and for each source seg-ment, ranked five randomly selected translations.From these 5-way rankings we extracted pairwisejudgments pi = {<,=}, where u1< u2indicatesthat subject u1provided a better translation thansubject u2for a given source input (Table 2).9See (Green et al., 2014c) for significance levels of theother covariates along with analysis of subject learning rates,subject behavior, and qualitative feedback.Fr-En En-De#pairwise 14,211 15,001#ties (=) 5,528 2,964IAA 0.419 (0.357) 0.407 (0.427)EW (inter.)
0.512 0.491Table 2: Pairwise judgments for the manual qual-ity assessment.
Inter-annotator agreement (IAA)?
scores are measured with the official WMT14script.
For comparison, the WMT14 IAA scoresare given in parentheses.
EW (inter.)
is expectedwins of interactive according to Eq.
(6).Fr-En En-Design p sign pui (interactive) + ?
?log edit distance ?
???
+ ??
?gender (female) ?
+ ?log session order ?
+ ?Table 3: LMEM manual translation quality resultsfor each fixed effect with contrast conditions forbinary predictors in ().
The signs of the coefficientscan be interpreted as in ordinary regression.
editdistance is token-level edit distance from baselineMT.
session order is the order in which the subjecttranslated the sentence during the experiment.
Sta-tistical significance was computed with a likelihoodratio test: ???
p < 0.001; ?
p < 0.05.In WMT the objective is to rank individual sys-tems; here we need only compare interface condi-tions.
However, we should control for translatorvariability.
Therefore, we build a binomial LMEMfor quality.
The model is motivated by the simpleand intuitive expected wins (EW) measure used atWMT.
Let S be the set of pairwise judgments andwins(u1, u2) = |{(u1, u2, pi) ?
S | pi = <}|.
Thestandard EW measure is:e(u1) =1|S|?u16=u2wins(u1, u2)wins(u1, u2) + wins(u2, u1)(5)Sakaguchi et al.
(2014) showed that, despite its sim-plicity, Eq.
(5) is nearly as effective as model-basedmethods given sufficient high-quality judgments.Since we care only about the two translation condi-tions, we reinterpret the uias interface conditions,i.e., u1= int and u2= pe.
We can then disregard1231the normalizing term to obtain:e(u1) =wins(u1, u2)wins(u1, u2) + wins(u2, u1)(6)which is the expected value of a Bernoulli distribu-tion (so e(u2) = 1 ?
e(u1)).
The intercept-termof the binomial LMEM will be approximately thisvalue subject to other fixed and random effects.To estimate the model, we convert each pairwisejudgment u1< u2to two examples where the re-sponse is 1 for u1and 0 for u2.
We add the fixedeffects shown in Table 3, where the numeric effectsare centered and scaled by their standard deviations.The maximal random effects structure contains in-tercepts for sentence id nested within subject alongwith random slopes for interface condition.Table 3 shows the p-values and coefficient orien-tations.
The models yield probabilities that can beinterpreted like Eq.
(6) but with all fixed predictorsset to 0.
For Fr-En, the value for post-edit is 0.472vs.
0.527 for interactive.
For En-De, post-edit is0.474 vs. 0.467 for interactive.
The difference isstatistically significant for Fr-En, but not for En-De.When MT quality was anecdotally high (Fr-En),high token-level edit distance from the initial sug-gestion decreased quality.
When MT was poor (En-De), significant editing improved quality.
FemaleEn-De translators were better than males, possiblydue to imbalance in the subject pool (12 females vs.4 males).
En-De translators seemed to improve withpractice (positive coefficient for session order).The Fr-En results are the first showing an inter-active UI that improves over post-edit.5 MT Re-tuning ExperimentThe human translators corrected the output of theBLEU-tuned, baseline MT system.
No updating ofthe MT system occurred during the experiment toeliminate a confound in the time and quality analy-ses.
Now we investigate re-tuning the MT systemto the corrections by simply re-starting the onlinelearning algorithm from the baseline weight vectorw, this time scoring with HTER instead of BLEU.Conventional incremental MT learning experi-ments typically resemble domain adaptation: small-scale baselines are trained and tuned on mostly out-of-domain data, and then re-tuned incrementallyon in-domain data.
In contrast, we start with large-scale systems.
This is more consistent with a pro-fessional translation environment where translatorsreceive suggestions from state-of-the-art systemslike Google Translate.Bilingual Monolingual#Segments #Tokens #TokensEn-De 4.54M 224M 1.7BFr-En 14.8M 842M 2.24BTable 4: Gross statistics of MT training corpora.En-De Fr-Enbaseline-tune 9,469 8,931baseline-dev 9,012 9,030int-tune 680 589int-test 457 368pe-tune 764 709pe-test 492 447Table 5: Tuning, development, and test corpora(#segments).
tune and dev were used for baselinesystem preparation.
Re-tuning was performed onint-tune and pe-tune, respectively.
We report held-out results on the two test data sets.
All sets aresupplied with independent references.5.1 DatasetsTable 4 shows the monolingual and parallel train-ing corpora.
Most of the data come from the con-strained track of the WMT 2013 shared task (Bojaret al., 2013).
We also added 61k parallel segmentsof TAUS data to the En-De bitext, and 26k TAUSsegments to the Fr-En bitext.
We aligned the par-allel data with the Berkeley Aligner (Liang et al.,2006) and symmetrized the alignments with thegrow-diag heuristic.
For each target language weused lmplz (Heafield et al., 2013) to estimate unfil-tered, 5-gram Kneser-Ney LMs from the concate-nation of the target side of the bitext and the mono-lingual data.
For the class-based features, we esti-mated 512-class source and target mappings withthe algorithm of Green et al.
(2014a).The upper part of Table 5 shows the baselinetuning and development sets, which also contained1/3 TAUS medical text, 1/3 TAUS software text,and 1/3 WMT newswire text (see section 4).The lower part of Table 5 shows the organizationof the human corrections for re-tuning and testing.Recall that for each unique source input, eight hu-man translators produced a correction in each con-dition.
First, we filtered all corrections for which alog u was not recorded (due to technical problems).Second, we de-duplicated the corrections so thateach h was unique.
Finally, we split the unique(f, h) tuples according to a natural division in the1232System tune BLEU?
TER?
HTERbaseline bleu 23.12 60.29 44.05re-tune hter 22.18 60.85 43.99re-tune+feat hter 21.73 59.71 42.35(a) En-De int-test results.System tune BLEU?
TER?
HTERbaseline bleu 39.33 45.29 28.28re-tune hter 39.99 45.73 26.96re-tune+feat hter 40.30 45.28 26.40(b) Fr-En int-test results.Table 6: Main re-tuning results for interactivedata.
baseline is the BLEU-tuned system usedin the translation user study.
re-tune is the base-line feature set re-tuned to HTER on int-tune.
re-tune+feat adds the human feature templates de-scribed in section 3.3. bold indicates statisticalsignificance relative to the baseline at p < 0.001;italic at p < 0.05 by the permutation test of Riezlerand Maxwell (2005).data.
There were five source segments per docu-ment, and each document was rendered as a singlescreen during the translation experiment.
Segmentorder was not randomized, so we could split thedata as follows: assign the first three segments ofeach screen to tune, and the last two to test.
This isa clean split with no overlap.This tune/test split has two attractive properties.First, if we can quickly re-tune on the first few sen-tences on a screen and provide better translationsfor the last few, then presumably the user experienceimproves.
Second, source inputs f are repeated?eight translators translated each input in each condi-tion.
This means that a reduction in HTER meansbetter average suggestions for multiple human trans-lators.
Contrast this experimental design with tun-ing to the corrections of a single human translator.There the system might overfit to one human style,and may not generalize to other human translators.5.2 ResultsTable 6 contains the main results for re-tuning to in-teractive MT corrections.
For both language pairs,we observe large statistically significant reductionsinHTER.However, the results for BLEU and TER?which are computed with respect to the independentreferences?are mixed.
The lower En-De BLEUscore is explained by a higher brevity penalty forthe re-tuned output (0.918 vs. 0.862).
However, there-tuned 4-gram and 3-gram precisions are signif-System HTER?
System HTER?int pebaseline 44.05 baseline 41.05re-tune (int) 43.99 re-tune (pe) 40.34re-tune+feat 42.35 ?
??
?1.80 ?0.71Table 7: En-De test results for re-tuning to post-edit(pe) vs. interactive (int).
Features cannot be ex-tracted from the post-edit data, so the re-tune+featsystem cannot be learned.
The Fr-En results aresimilar but are omitted due to space.icantly higher.
The unchanged Fr-En TER valuecan be explained by the observation that no humantranslators produced TER scores higher than thebaselineMT.
This odd result has also been observedfor BLEU (Culy and Riehemann, 2003), althoughhere we do observe a slight BLEU improvement.The additional features (854 for Fr-En; 847 forEn-De) help significantly and do not slow downdecoding.
We used the same L1regularizationstrength as the baseline, but feature growth couldbe further constrained by increasing this parame-ter.
Tuning is very fast at about six minutes for thewhole dataset, so tuning during a live user sessionis already practical.Table 7 compares re-tuning to interactive vs.post-edit corrections.
Recall that the int-test andpe-test datasets are different and contain differentreferences.
The post-edit baseline is lower becausehumans performed less editing in the baseline con-dition (see Table 1).
Features account for the great-est reduction in HTER.
Of course, the features arebased mostly on word alignments, which could beobtained for the post-edit data by running an onlineword alignment tool (see: Farajian et al.
(2014)).However, the interactive logs contain much richeruser state information that we could not exploit dueto data sparsity.
We also hypothesize that the fi-nal interactive corrections might be more usefulsince suggestions prime translators (Green et al.,2013a), and the MT system was able to refine itssuggestions.6 Re-tuning AnalysisTables 6 and 7 raise two natural questions: whataccounts for the reduction in HTER, and why arethe TER/BLEU results mixed?
Comparison of theBLEU-tuned baseline to the HTER re-tuned sys-tems gives some insight.
For both questions, fine-1233grained corrections appear to make the difference.Consider this French test example (with gloss):(1) uneonelignelinedeofchimioth?rapiechemotherapyant?rieurepreviousThe independent reference for une ligne de chimio-th?rapie is ?previous chemotherapy treatment?, andthe baseline produces ?previous chemotherapy line.
?The source sentence appears seven times with thefollowing user translations: ?one line or moreof chemotherapy?, ?one prior line of chemother-apy?, ?one previous line of chemotherapy?
(2), ?oneline of chemotherapy before?
(2), ?one protocol ofchemotherapy?.
The re-tuned, feature-based sys-tem produces ?one line of chemotherapy before?,matching two of the humans exactly, and six of thehumans in terms of idiomatic medical jargon (?lineof chemotherapy?
vs. ?chemotherapy treatment?
).However, the baseline output would have receivedbetter BLEU and TER scores.Sometimes re-tuning improves the translationswith respect to both the reference and the humancorrections.
This English phrase appears in theEn-De test set:(2) dependingabh?ngigonvonthederfiledateiThe baseline produces exactly the gloss shown in Ex.(2).
The human translators produced: ?je nach datei?
(6), ?das dokument?, and ?abh?ngig von der datei?.The re-tuned system rendered the phrase ?je nachdokument?, which is closer to both the independentreference ?je nach datei?
and the human corrections.This change improves TER, BLEU, and HTER.7 Related WorkThe process study most similar to ours is that ofKoehn (2009a), who compared scratch, post-edit,and simple interactive modes.
However, he used un-dergraduate, non-professional subjects, and did notconsider re-tuning.
Our experimental design withprofessional bilingual translators follows our previ-ous work Green et al.
(2013a) comparing scratchtranslation to post-edit.Many research translation UIs have been pro-posed including TransType (Langlais et al., 2000),Caitra (Koehn, 2009b), Thot (Ortiz-Mart?nez andCasacuberta, 2014), TransCenter (Denkowski etal., 2014b), and CasmaCat (Alabau et al., 2013).However, to our knowledge, none of these inter-faces were explicitly designed according to mixed-initiative principles from the HCI literature.Incremental MT learning has been investigatedseveral times, usually starting from no data (Bar-rachina et al., 2009; Ortiz-Mart?nez et al., 2010),via simulated post-editing (Mart?nez-G?mez et al.,2012; Denkowski et al., 2014a), or via re-ranking(W?schle et al., 2013).
No previous experimentscombined large-scale baselines, full re-tuning ofthe model weights, and HTER optimization.HTER tuning can be simulated by re-parameterizing an existing metric.
Snover etal.
(2009) tuned TERp to correlate with HTER,while Denkowski and Lavie (2010) did the samefor METEOR.
Zaidan and Callison-Burch (2010)showed how to solicit MT corrections for HTERfrom Amazon Mechanical Turk.Our learning approach is related to coactive learn-ing (Shivaswamy and Joachims, 2012).
Their basicpreference perceptron updates toward a correction,whereas we use the correction for metric scoringand feature extraction.8 ConclusionWe presented a new CAT interface that supportspost-edit and interactive modes.
Evaluation withprofessional, bilingual translators showed post-editto be faster, but prior subject familiarity with post-edit may have mattered.
For French-English, theinteractive mode enabled higher quality translation.Re-tuning the MT system to interactive correctionsalso yielded large HTER gains.
Technical contri-butions that make re-tuning possible are a cross-entropy objective, prefix decoding, and dynamicphrase table augmentation.
Larger quantities of cor-rections should yield further gains, but our currentexperiments already establish the feasibility of Bar-Hillel?s virtuous ?machine-post-editor partnership?which benefits both humans and machines.AcknowledgementsWe thank TAUS for access to their data reposi-tory.
We also thank John DeNero, Chris Dyer,Alon Lavie, and Matt Post for helpful conversa-tions.
The first author is supported by a NationalScience Foundation Graduate Research Fellowship.This work was also supported by the Defense Ad-vanced Research Projects Agency (DARPA) BroadOperational Language Translation (BOLT) programthrough IBM.
Any opinions, findings, and conclu-sions or recommendations expressed are those ofthe author(s) and do not necessarily reflect the viewof either DARPA or the US government.1234ReferencesV.
Alabau, R. Bonk, C. Buck, M. Carl, F. Casacuberta,M.
Garc?a-Mart?nez, et al.
2013.
Advanced com-puter aided translation with a web-based workbench.In 2nd Workshop on Post-Editing Technologies andPractice.R.H.
Baayen, D.J.
Davidson, and D.M.
Bates.
2008.Mixed-effects modeling with crossed random effectsfor subjects and items.
Journal of Memory and Lan-guage, 59(4):390?412.Y.
Bar-Hillel.
1960.
The present status of automatictranslation of languages.
Advances in Computers,1:91?163.D.
J. Barr, R. Levy, C. Scheepers, and H. J. Tily.
2013.Random effects structure for confirmatory hypothe-sis testing: Keep it maximal.
Journal of Memoryand Language, 68(3):255?278.S.
Barrachina, O. Bender, F. Casacuberta, J. Civera,E.
Cubel, S. Khadivi, et al.
2009.
Statistical ap-proaches to computer-assisted translation.
Compu-tational Linguistics, 35(1):3?28.D.
M. Bates.
2007. lme4: Linear mixed-effects models using S4 classes.
Technical re-port, R package version 1.1-5, http://cran.r-project.org/package=lme4.O.
Bojar, C. Buck, C. Callison-Burch, C. Federmann,B.
Haddow, P. Koehn, et al.
2013.
Findings of the2013 Workshop on Statistical Machine Translation.In WMT.O.
Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn,J.
Leveling, et al.
2014.
Findings of the 2014 Work-shop on Statistical Machine Translation.
In WMT.C.
Callison-Burch, M. Osborne, and P. Koehn.
2006.Re-evaluating the role of BLEU in machine transla-tion research.
In EACL.M.
Carl.
2010.
A computational framework for a cogni-tive model of human translation processes.
In AslibTranslating and the Computer Conference.D.
Cer, C. D.Manning, and D. Jurafsky.
2010.
The bestlexical metric for phrase-based statistical MT systemoptimization.
In NAACL.C.
Cherry and G. Foster.
2012.
Batch tuning strategiesfor statistical machine translation.
In NAACL.C.
Culy and S. Z. Riehemann.
2003.
The limits of n-gram translation evaluation metrics.
In MT SummitIX.M.
Denkowski and A. Lavie.
2010.
Extending the ME-TEOR machine translation evaluation metric to thephrase level.
In NAACL.M.
Denkowski, C. Dyer, and A. Lavie.
2014a.
Learn-ing from post-editing: Online model adaptation forstatistical machine translation.
In EACL.M.
Denkowski, A. Lavie, I. Lacruz, and C. Dyer.2014b.
Real time adaptive machine translation forpost-editing with cdec and TransCenter.
In Work-shop on Humans and Computer-assisted Translation.J.
Duchi, E. Hazan, and Y.
Singer.
2011.
Adaptive sub-gradient methods for online learning and stochasticoptimization.
JMLR, 12:2121?2159.M.
A. Farajian, N. Bertoldi, and M. Federico.
2014.Online word alignment for online adaptive machinetranslation.
InWorkshop on Humans and Computer-assisted Translation.C.
Federmann.
2010.
Appraise: An open-sourcetoolkit for manual phrase-based evaluation of trans-lations.
In LREC.G.
Foster, P. Langlais, and G. Lapalme.
2002.
User-friendly text prediction for translators.
In EMNLP.M.
Galley and C. D. Manning.
2008.
A simple andeffective hierarchical phrase reordering model.
InEMNLP.S.
Green, J. Heer, and C. D. Manning.
2013a.
The effi-cacy of human post-editing for language translation.In CHI.S.
Green, S. Wang, D. Cer, and C. D. Manning.
2013b.Fast and adaptive online training of feature-rich trans-lation models.
In ACL.S.
Green, D. Cer, and C. D. Manning.
2014a.
An em-pirical comparison of features and tuning for phrase-based machine translation.
In WMT.S.
Green, D. Cer, and C. D. Manning.
2014b.
Phrasal:A toolkit for new directions in statistical machinetranslation.
In WMT.S.
Green, J. Chuang, J. Heer, andC.D.Manning.
2014c.Predictive Translation Memory: A mixed-initiativesystem for human language translation.
In UIST.K.
Heafield, I. Pouzyrevsky, J. H. Clark, and P. Koehn.2013.
Scalable modified Kneser-Ney languagemodel estimation.
In ACL, Short Papers.L.
Huang and D. Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InACL.R.
Hwa, P. Resnik, A. Weinberg, and O. Kolak.
2002.Evaluating translational correspondence using anno-tation projection.
In ACL.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, et al.
2007.
Moses: Opensource toolkit for statistical machine translation.
InACL, Demonstration Session.P.
Koehn.
2009a.
A process study of computer-aidedtranslation.
Machine Translation, 23:241?263.P.
Koehn.
2009b.
A web-based interactive computeraided translation tool.
In ACL-IJCNLP, SoftwareDemonstrations.1235P.
Koehn.
2010.
Statistical Machine Translation.
Cam-bridge University Press.P.
Langlais, G. Foster, and G. Lapalme.
2000.TransType: a computer-aided translation typing sys-tem.
In Workshop on Embedded Machine Transla-tion Systems.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In NAACL.C.
Manning, M. Surdeanu, J. Bauer, J. Finkel,S.
Bethard, and D. McClosky.
2014.
The StanfordCoreNLP natural language processing toolkit.
InACL, System Demonstrations.P.
Mart?nez-G?mez, G. Sanchis-Trilles, and F. Casacu-berta.
2012.
Online adaptation strategies for sta-tistical machine translation in post-editing scenarios.Pattern Recognition, 45(9):3193?3203.F.
J. Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Compu-tational Linguistics, 30(4):417?449.F.
J. Och, R. Zens, and H. Ney.
2003.
Efficientsearch for interactive statistical machine translation.In EACL.F.
J. Och.
2003.
Minimum error rate training for statis-tical machine translation.
In ACL.D.
Ortiz-Mart?nez and F. Casacuberta.
2014.
The newThot toolkit for fully automatic and interactive statis-tical machine translation.
In EACL, System Demon-strations.D.
Ortiz-Mart?nez, I. Garc?a-Varea, and F. Casacuberta.2009.
Interactive machine translation based on par-tial statistical phrase-based alignments.
In RANLP.D.
Ortiz-Mart?nez, I. Garc?a-Varea, and F. Casacuberta.2010.
Online learning for interactive statistical ma-chine translation.
In NAACL.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In ACL.R.
Ray.
2013.
Ten essential research findings for 2013.In 2013 Resource Directory & Index.
Multilingual.S.
Riezler and J. T. Maxwell.
2005.
On some pitfalls inautomatic evaluation and significance testing in MT.In ACL Workshop on Intrinsic and Extrinsic Evalua-tion Measures for Machine Translation and/or Sum-marization.K.
Sakaguchi, M. Post, and B.
Van Durme.
2014.
Effi-cient elicitation of annotations for human evaluationof machine translation.
In WMT.P.
Shivaswamy and T. Joachims.
2012.
Online struc-tured prediction via coactive learning.
In ICML.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In AMTA.M.
Snover, N. Madnani, B. Dorr, and R. Schwartz.2009.
Fluency, adequacy, or HTER?
Exploring dif-ferent human judgments with a tunable MT metric.In WMT.K.
W?schle, P. Simianer, N. Bertoldi, S. Riezler, andM.
Federico.
2013.
Generative and discriminativemethods for online adaptation in SMT.
In MT Sum-mit XIV.O.
F. Zaidan and C. Callison-Burch.
2010.
Predictinghuman-targeted translation edit rate via untrained hu-man annotators.
In NAACL.1236
