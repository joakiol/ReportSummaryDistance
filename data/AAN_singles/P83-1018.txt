SYN'I'ACI IC CONSTI~,,\INTS AND F~FI:ICIFNI' I~AI(SAI~,II.I'I'YRobert C. BerwickRoom 820, MIT Artificial Intelligence l,aboratory545 Technology Square, Cambridge, MA 02139Amy S. WeinbergDeparuncnt of Linguistics, MITCambridge, MA 02139ABSTRACTA central goal of linguistic theory is to explain why naturallanguages are the way they are.
It has often been supposed thatcom0utational considerations ought to play a role in thischaracterization, but rigorous arguments along these lines have beendifficult to come by.
In this paper we show how a key "axiom" ofcertain theories of grammar, Subjacency, can be explained byappealing to general restrictions on on-line parsing plus naturalconstraints on the rule-writing vocabulary of grammars.
Theexplanation avoids the problems with Marcus' \[1980\] attempt toaccount for the same constraint.
The argument is robust with respectto machine implementauon, and thus avoids the problems that oftenarise wilen making detailed claims about parsing efficiency.
It has theadded virtue of unifying in the functional domain of parsing certaingrammatically disparate phenomena, s well as making a strong claimabout the way in which the grammar is actually embedded into anon-line sentence processor.I INTRODUCTIONIn its short history, computational linguistics has bccn driven bytwo distinct but interrelated goals.
On the one hand, it has aimed atcomputational explanations of distinctively human linguistic behavior-- that is, accounts of why natural languages are the way they areviewed from the perspective ofcomputation.
On the other hand, it hasaccumulated a stock of engineenng methods for building machines todeal with natural (and artificial) languages.
Sometimes a single bodyof research as combined both goals.
This was true of the work ofMarcus \[1980\].
for example.
But all too often the goals have remainedopposed -- even to the extent hat current ransformational theory hasbeen disparaged as hopelessly "intractable" and no help at all inconstructing working parsers.This paper shows that modern transformational grammar (the"Government-Binding" or "GB" theory as described in Chomsky\[1981\]) can contribute to both aims of computational linguistics.
Weshow that by combining simple assumptions about efficient parsabilityalong with some assumpti(ms about just how grammatical theory is tobe "embedded" in a model of language processing, one can actuallyexplain some key constraints of natural anguages, uch as Suhjacency.
(The a)gumcnt is differmlt frt)m that used in Marcus 119801.)
In fact,almost he entire pattern of cunstraints taken as "axioms" by the GBthct)ry can be accutmtcd tbr.
Second, contrary to what has sometimesbeen supposed, by exph)iting these constraints wc can ~how that aGll-based theory is particularly compatil)le v~idl efficient parsingdesigns, in particdlar, with extended I I~,(k,t) parsers (uf the sortdescribed by Marcus \[1980 D. Wc can extcnd thc I,R(k.t) design toaccommodate such phenomena s antecedent-PRO and pronominalbinding.
Jightward movement, gappiug, aml VP tlcletion.A, Functional Explanations o__f I,ocality PrinciplesLet us consider how to explain locality constraints in naturallanguages.
First of all, what exactly do we mean by a "localityconstraint"?
"\]'he paradigm case is that of Subjacency: the distancebetween a displaced constituent and its "underlying" canonicalargument position cannot be too large, where the distance is gauged (inEnglish) in terms of the numher of the number of S(entence) or NPphrase boundaries.
For example, in sentence (la) below, John (theso-called "antecedent") is just one S-boundary away from itspresumably "underlying" argument position (denoted "x", the"trace")) as the Subject of the embedded clause, and the sentence isfine:(la) John seems \[S x to like ice cream\].However, all we have to do ts to make the link between John and xextend over two S's, and the sentence is ill-formed:(lb) John seems \[S it is certain \[S x to like ice creamThis restriction entails a "successive cyclic" analysis oftransformational rules (see Chomsky \[1973\]).
In order to derive asentence like (lc) below without violating the Subjacency condition,we must move the NP from its canonical argument position throughthe empty Subject position in the next higher S and then to its surfaceslot:(lc) John seems tel to be certain x to get the ice cream.Since the intermediate subject position is filled in (lb) there is no licitderivation for this sentence.More precisely, we can state the Subjacency constraint as follows:No rule of grammar can involve X and Y in a configuration like thefollowing,\[ ...x...\[,, ...\[/r..Y...\]...l .X...\]where a and # are bounding nodes (in l.'nglish, S or NP phrases).
"Why should natural languages hc dcsigned Lhis way and not someother way?
Why, that is, should a constraint like Subjaccncy exist atall?
Our general result is that under a certain set of assumptions aboutgrammars and their relationship to human sentence processing one canactually expect the following pattern of syntactic igcality constraints:(l) The antecedent-trace relationship mustobey Subjaccncy, but other "binding"realtionships (e.g., NP--Pro) need not obeySubjaccncy.119(2) Gapping constructitms must be subjectto a bounding condition resemblingSubjacency.
but VP deletion ced not be.
(3) Rightward movemcnt must be stricdybounded.To the extent hat this predicted pattern of constraints i actuallyobserved -- as it is in English and other languages -- we obtain agenuine functional explanation of these constraints and support for theassumptions themselves.
The argument is different from Man:us'because it accounts for syntactic locality constraints (like Subjaceney),as the joint effect of a particular theory of grammar, a theory of howthat grammar is used in parsing, a criterion for efficient parsability.and a theory of of how the parser is builL In contrast, Marcusattempted to argue that Subjaceney could be derived from just the(independently justified) operating principles of a particular kind ofparser.B.
Assumptions.The assumptions we make are the following:(1) The grammar includes a level ofannotated surface structure indicating howconstituents have been displaced from theircanonical predicate argument positions.Further, sentence analysis is divided intotwo stages, along the lines indicated by tiletheory of Government and Binding: thefirst stage is a purely syntactic analysis thatrebuilds annotated surface structure; thesecond stage carries out the interpretationof variables, binds them to operators, allmaking use of the "referential indices" ofNPs.
(2) To be "visible" at a stage of analysis alinguistic representation must be written inthe vocabulary of that level.
For example,to be affected by syntactic operations, arepresentation must be expressed in asyntactic vocabulary (in the usual sense); tobe interpreted by operations at the secondstage, the NPs in a representation mustpossess referential indices.
(Thisassumption is not needed to derive theSubjaccncy constraint, but may be used toaccount for another "axiom" of currentgrammatical theory, the so-called"constituent command" constraint onantecedcnLs and the variables that theyhind.)
This "visibility" assumption is arather natural one.
(3) The rule-writing vocabulary of thegrammar cannot make use of arithmeticpredicates such as "one", "two" or "three".but only such predicates as "adjacent".Further, quzmtificational statements are notallowed m rt.les.
These two assumptionsare also rather standard.
It has often beennoted that grammars "do not count" -- thatgrammatical predicates are structurallybased.
There is no rule of grammar thattakes the just the fourth constituent of asentence and moves it, for example.
Incontrast, many different kinds of rules ofgrammar make reference to adjacentconstituents.
(This is a feature found inmorphological, phonological, and syntacticrules.
)(4) Parsing is no....!
done via a method thatcarries along (a representation) of allpossible derivations in parallel.
Inparticular, an Earley-type algorithm is ruledout.
To the extent that multiple optionsabout derivations are not pursued, the parseis "deterministic.
"(5) The left-context of the parse (as definedin Aho and Ullman \[19721) is literallyrepresented, rather than generativelyrepresented (as, e.g., a regular set).
Inparticular, just the symbols used by thegrammar (S, NP.
VP...) are part of theleft-context vocabulary, and not "complex"symbols serving as proxies for the set oflefl.-context s rings.
1 In effect, we make the(quite strong) assumption that the sentenceprocessor adopts a direct, transparentembedding of the grammar.Other theories or parsing methods do not meet these constraintsand fail to explain the existence of locality constraints with respect tothts particular set of assumpuons.
2 For example, as we show, there isno reason to expect a constraint like Subjacency in the GeneralizedPhrase Structure Grammars/GPSGsl of G,zdar 119811, because thereis no inherent barrier to eastly processing a sentence where anantecedent and a trace are !.mboundedly far t'rt~m each other.Similarly if a parsing method like Earlcy's algorithm were actuallyused by people, than Sub\]acency remains a my:;tcry on the functionalgrounds of efficient parsability.
(It could still be explained on otherfunctional grounds, e.g., that oflearnability.
)II PARSING AND LOCALITY PRINCIPLESTo begin the actual argument then, assume that on-line sentenceprocessing is done by something like a deterministic parser)Sentences like (2) cause trouble for such a parser:(2) What i do you think that John told Mary...mat newould like to eat %t.
Recall that he suoec.~i~'e lin s of a left- or right-most derivation i a context-freegrammar cnnstttute a regular Language.
~.~ shown m. e.g.. DcRemer \[19691.2.
Plainly.
one is free to imagine some other set of assumptions that would o the job.3.
If one a.ssumcs a backtracking parser, then the argument can also be made to gothrough, but only by a.,,,,~ummg that backtracking Ks vcr/co~tlS, ince this son of parserclearly ,,~ab:~umes th  IR(kPt,',pe machines under t/le right co,mrual of 'cost".
we makethe stronger assumption f I R(k)-ncss.120The problem is that on recognizing the verb eat the parser must decidewhether to expand the parse with a trace (the transitive reading) orwith no postverbal element (.the intransitive reading).
The ambiguitycannot be locally resolved since eat takes both readings.
It can only beresolved by checking to see whether there is an actual antecedent.Further, observe that this is indeed a parsing decision: the machinemust make some decision about how to tu build a portion of the parsetree.
Finally, given non-parallelism, the parser is not allowed to pursueboth paths at once: it must decide now how to build the parse tree (byinserting an empty NP trace or not).Therefore, assuming that the correct decision is to be made on-line(or that retractions of incorrect decisions are costly) there must be anactual parsing rule that expands a category as transitive iff there is animmediate postverbal NP in the string (no movement) or if an actualantecedent is present.
However, the phonologically overt antecedentcan be unboundedly far away from the gap.
Therefore, it would seemthat the relevant parsing rule would have to refer to a potentiallyunbounded left context.
Such a rule cannot be stated in the finitecontrol table of an I,R(k) parser.
Theretbre we must find some finiteway of expressing the domain over which the antecedent must besearched.There are two ways of accomplishing this.
First, one could expressall possible left-contexts as somc regular set and then carry thisrepresentation along in the finite control table of the I,R(k) machine.This is always pu,,;sible m the case of a contcxt-fiee grammar, and mfact is die "standard" approach.
4 However, m the case of (e.g.)
,,hmoven!enk this demands a generative encoding of the associated finitestate automaton, via the use of complex symbols like "S/wh"(denoting the "state" that a tvtt has been encountered) and rules to passking this nun-literal representation f the state of the parse.
Illisapproach works, since wc can pass akmg this state encoding throughthe VP (via the complex non-terminal symbol VP/wh) and finally intothe embedded S. This complex non-terminal is then used to trigger anexpansion of eat into its transitive form.
Ill fact, this is precisely thesolution method advocated by Gazdar.
We ~ce then that if one adoptsa non-terminal encoding scheme there should he no p,oblem inparsing any single long-distance gap-filler relationship.
That is, thereis no need for a constraint like Subjacency.
sSecond, the problem of unbounded left-context is directly avoidedif the search space is limited to some literally finite left context.
Butthis is just what the Sttbjacency c(mstraint does: it limits where anantecedent NP could be to an immediately adjacent S or S. Thisconstraint has a StlllpJe intcrprctatum an actual parser (like that builthy Murcus \[19};0 D. l'he IF-THEN pattern-action rules that make upthe Marcus parser's ~anite control "transi:ion table" must be finite inorder to he stored ioside a machine.
The rule actions themselves areliterally finite.
If the role patterns must be /herally stored (e.g., thepattern \[S \[S"\[S must be stored as an actual arbitrarily long string orsnodes, rather than as the regular set S+), then these patterns must beliterally finite.
That is, parsing patterns must refer to literally houndedright and left context (in terms of phrasal nodes).
6 Note  Further that4 Following the approactl of DcRemer \[\]969\], one budds a finHe stale automaton Lhatreco~nl/es exactly Ihe set of i?\[t-(OIIlext strings that cain arise during the course of aright-most derivation, the so-Gilled ch,melert.s l lcf ' .ni fe s/ale ClUlOmC~lott.5 l'laml} the same Imlds for a "hold cell" apploaeh \[o compulm 8 filler-gaprelallonshipi6.
Actually Uteri.
lhJ8 k;nd or device lall!
; lllto lJae (~itegoly of bounded contc;~t parsing.a.
'~ defiued b~.
I \]oyd f19(.
)4\].this constraint depends on the sheer represcntability of the parser'srule system in a finite machine, rather than on any details ofimplementation.
Therefore it will hold invariantly with respect tornactfine design -- no matter kind of machine we build, if" we assume aliteral representation of left-contexts, then some kind t)f finitenessconstraint is required.
The robustness of this result contrasts with theusual problems in applying "efficiency" results to explain grm'~T""'!calconstraints.
These often fail because it is difficult to consider allpossible implcmentauons simultaneously.
However, if the argument isinvariant with respect to machine desing, this problem is avoided.Given literal left-contexts and no (or costly) backtracking, theargument so far motivates ome bounding condition for ambiguoussentences like these.
However, to get the lull range of cases thesefunctional facts must interact with properties of the rule writing systemas defined by the grammar.
We will derive the litct that the Imundingcondition must be ~acency  (as opposed to tri- or quad-jaccncy) byappeal to the lhct that grammatical c~m~tramts and rules arc ~tated in avocabtdary which is non-c'vunmtg.
,',rithmetic predicates areforbidden.
But this means that since only the prediu~lte "ad\].cent" ispermitted, any literal I)ouuding rc,~trict\]oi\] must be c.xprc,~)cd m tcrlllSof adjacent domains: t~e~;ce Subjaccncy.
INert that ",djacent" is alsoan arithmetic predicate.)
l:urthcr.
Subjaccncy mu,,t appiy ~.o ,ill traces(not ju',t traces of,mlb=guously traw~itive/imransi\[ive vcrb,o in:cause arestriction to just the ambiguous cases would low)ire using cxistentmlquantilicati.n.
Ouantificatiomd predicates are barred in the rulewriting vocabulary of natural grammars.
7Next we extend the approach to NP movement and Gapping.Gapping is particularly interesting because it is difficult ~o explainwhy this construction (tmlike other deletiou rules) is bounded.
That is,why is (3) but not (4) grammatical:(3) John will hit Frank and Bill will \[ely P George.
*(4)John will hit Frank and I don't believe Bill will\[elvpGeorge.The problem with gapping constructions is that the attachment ofphonologically identical complements is governed by the verb that thecomplement follows.
Extraction tests show that in {5) the pilrase u/?erM'ao' attaches to V" whde in (6) it attaches to V" (See Hornstem andWemberg \[\]981\] for details.
}(5) John will mn aftcr Mary.
(6) John will arrivc after Mary.In gapping structures, however, the verb of the gapped constituent ,snot present in the string.
Therefore.
correct ,lltachrnent o( thecomplement can only be guaranteed by accessing the antecedent in theprevious clause.
If this is true however, then the boundlng argumentfor Suhjacency applies to this ease as well: given deterministic parsingof gapping done correctly, and a literal representation of  left-context,then gapping must be comext-bounded.
Note that this is a particularly7 Of course, there zs a anolhcr natural predic.atc Ihat would produce a finite bound onrule context: i\[ ~\]) alld Irate hod I .
bc in tile .ame S donlalll Prc~umahb', lhls is also anOptlllt3 ~l;iI could gel reah,ed in qOII|C n.'Ittlral ~rJoln'iai~: ll'ic resuhing languages wouldno( have ov,,:rt nlo~.eIIICill OUlside o\[ an S. %o(e lllal Lhc naltllal plcdJc;des simply givethe ranta?
of po~edble ndiulal granmlars.
\]lot hose actually rour~d.The elimination of quanllfil',.llion predic~les is supportable on grounds o(acquisltton.121interesting example bccause it shows how grammatically dissimilaroperations like wh-movement and gapping can "fall together" in thefunctional domain of parsing.NP-trace and gaplSing constructions contrast withantecedentY(pro)nominal binding, lexical anaphor elationships, andVP deletion.
These last three do not obey Subjacency.
For example, aNoun Phrase can be unboundedly far from a (phonologically empty)PRO.
even in tenns ofJohn i thought it was certain that... \[PRO i feeding himself\]would be easy.Note though that in these cases the expansion of the syntactic tree doesno._At depend on the presence or absence of an antecedent(Pro)nominals and Icxical anaphors are phonologically realized in thestring and can unambiguously tell the parser hew to expand the tree.
(After the tree is fully expanded the parser may search back to seewhether the element is bound to an antecedent, but this is not aparsing decision,) VP deletion sites are also always locally detectablefrom ~e simple fact that every sentence requires a VP.
The sameargument applies to PRO.
PRO is locally detectable as the onlyphonologically unrealized element that can appear in an ungovernedcontext, and the predicate "ungoverned" is local.
8 In short, there is noparsing decision that hinges on establishing the PRO-antecedent.
VPdeletion-antecedent, t)r lexical anaphor-antecedent relationship.
Butthen, we should not expect bounding principles to apply in thcse cases,and, in fact, we do not find these elements subject o bounding.
Onceagain then.
apparently diverse grammaucal phcnomc,m behave alikewithin a functional realm.To summarize, we can explain why Subjacency applies to exactlythose elements that the grammar stipulates it must apply to.
We dothis using both facts about the functional design of a parsing systemand properties of the formal rule writing vocabulary, l'o the extentthat the array of assumpuons about the grammar and parser actuallyexplain this observed constraint on human linguistic behavior, weobtain a powerful argument hat certain kinds of grammaticalrepresenumons and parsing dcstgns are actually implicated in humansentence processing.Chomsky, Noam \[19811 Lectures on Gove,nmem and Binding, ForisPublications.I)eRerner, Frederick \[1969\] Practical 7"nms,':m~sJbr IR(k) I.angu,ges,Phi) di.~scrtation, MIT Department of Electrical Engineering andComputer Science.Floyd, Robert \[1964\] "Bounded-context syntactic analysis.
"Communtcations of the Assoctatiotl for Computing ,l.lachinery, 7 pp,62-66.Gazdar, Gerald \[19811 "Unbounded dependencies and coordinatestructure," Linguistic Inquiry, 12:2 I55-184.Hornstein.
Norbert and Wcinherg, Amy \[19811 "Preposition strandingand case theory," LingutMic \[nquio,, 12:1.Marcus, Mitchell \[19801 A Theory of Syntactic Recognition for NaturalLanguage, MIT Press111 ACKNOWLEDGEIvlENTSThis report describes work done at the Artificial IntelligenceLaboratory of the Massachusetts Institute ofl'cchnt)logy.
Support forthe Laboratory's artificial intelligence research is prey)deal in part bytiac Advanced P, esearch ProjccLs Agency of the Department of Defenseunder Office ()f Naval Research Contract N00014-80-C-0505.IV REFERENCESAho, Alfred and Ullman, Jeffrey \[1972\] The Theory of ParsingTrnn.~lalion, attdCumpiiing, vo\[.
\[., Prentice-(-{all.Chumsky, Noam \[1973\] "Conditions on 'rransformations,"in S.Anders(m & P Kiparsky, eds.
A Feslschr(l'l \[or Morris Halle.
Holt,Rinehart and Winston.8 F;hlce ~ ~s ungovcNicd fff a ~ovct'llcd t:~ F;L\[:~c, and a go~c,'m~J is a bounded predicate,i hcmg Lcstrictcd Io mu~',dy a~in~i?
lllaX1111;il Drojcctlon (at worst al| S).122
