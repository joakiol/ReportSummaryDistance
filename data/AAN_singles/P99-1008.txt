Finding Parts in Very Large CorporaMatthew Berland, Eugene Charniakrob, ec @ cs.
brown, eduDepartment of Computer ScienceBrown University, Box 1910Providence, RI 02912AbstractWe present a method for extracting parts of objectsfrom wholes (e.g.
"speedometer" f om "car").
Givena very large corpus our method finds part words with55% accuracy for the top 50 words as ranked by thesystem.
The part list could be scanned by an end-userand added to an existing ontology (such as WordNet),or used as a part of a rough semantic lexicon.1 IntroductionWe present a method of extracting parts of objectsfrom wholes (e.g.
"speedometer" f om "car").
Tobe more precise, given a single word denoting someentity that has recognizable parts, the system findsand rank-orders other words that may denote partsof the entity in question.
Thus the relation foundis strictly speaking between words, a relation Miller\[1\] calls "meronymy."
In this paper we use the morecolloquial "part-of" terminology.We produce words with 55??
accuracy for the top50 words ranked by the system, given a very largecorpus.
Lacking an objective definition of the part-ofrelation, we use the majority judgment of five humansubjects to decide which proposed parts are correct.The program's output could be scanned by an end-user and added to an existing ontology (e.g., Word-Net), or used as a part of a rough semantic lexicon.To the best of our knowledge, there is no publishedwork on automatically finding parts from unlabeledcorpora.
Casting our nets wider, the work most sim-ilar to what we present here is that by Hearst \[2\] onacquisition of hyponyms ("isa" relations).
In that pa-per Hearst (a) finds lexical correlates to the hyponymrelations by looking in text for cases where known hy-ponyms appear in proximity (e.g., in the construction(NP, NP and (NP other NN)) as in "boats, cars, andother vehicles"), (b) tests the proposed patterns forvalidity, and (c) uses them to extract relations froma corpus.
In this paper we apply much the samemethodology to the part-of relation.
Indeed, in \[2\]Hearst states that she tried to apply this strategy tothe part-of relation, but failed.
We comment later onthe differences in our approach that we believe weremost important to our comparative success.Looking more widely still, there is an ever-growing literature on the use of statistical/corpus-based techniques in the automatic acquisition oflexical-semantic knowledge (\[3-8\]).
We take it as ax-iomatic that such knowledge is tremendously usefulin a wide variety of tasks, from lower-level tasks likenoun-phrase r ference, and parsing to user-level taskssuch as web searches, question answering, and digest-ing.
Certainly the large number of projects that useWordNet \[1\] would support his contention.
And al-though WordNet is hand-built, here is general agree-ment that corpus-based methods have an advantagein the relative completeness of their coverage, partic-ularly when used as supplements o the more labor-intensive methods.2 Finding Parts2.1 Par tsWebster's Dictionary defines "part" as "one of theoften indefinite or unequal subdivisions into whichsomething is or is regarded as divided and which to-gether constitute the whole."
The vagueness of thisdefinition translates into a lack of guidance on exactlywhat constitutes a part, which in turn translates intosome doubts about evaluating the results of any pro-cedure that claims to find them.
More specifically,note that the definition does not claim that partsmust be physical objects.
Thus, say, "novel" mighthave "plot" as a part.In this study we handle this problem by asking in-formants which words in a list are parts of some targetword, and then declaring majority opinion to be cor-rect.
We give more details on this aspect of the studylater.
Here we simply note that while our subjectsoften disagreed, there was fair consensus that whatmight count as a part depends on the nature of the57word: a physical object yields physical parts, an in-stitution yields its members, and a concept yields itscharacteristics and processes.
In other words, "floor"is part of "building" and "plot" is part of "book.
"2 .2  Pat ternsOur first goal is to find lexical patterns that tend toindicate part-whole relations.
Following Hearst \[2\],we find possible patterns by taking two words thatare in a part-whole relation (e.g, basement and build-ing) and finding sentences in our corpus (we used theNorth American News Corpus (NANC) from LDC)that have these words within close proximity.
Thefirst few such sentences are:... the basement of the bui ld ing.... the basement  in question isin a four-story apartment bu i ld ing ...... the basement of the apartment building.From the building's basement ...... the basement  of a bu i ld ing  ...... the basements  of bui ld ings ...From these examples we construct he five pat-terns shown in Table 1.
We assume here that partsand wholes are represented by individual lexical items(more specifically, as head nouns of noun-phrases) asopposed to complete noun phrases, or as a sequence of"important" noun modifiers together with the head.This occasionally causes problems, e.g., "conditioner"was marked by our informants as not part of "car",whereas "air conditioner" probably would have madeit into a part list.
Nevertheless, in most cases headnouns have worked quite well on their own.We evaluated these patterns by observing howthey performed in an experiment on a single example.Table 2 shows the 20 highest ranked part words (withthe seed word "car") for each of the patterns A-E.(We discuss later how the rankings were obtained.
)Table 2 shows patterns A and B clearly outper-form patterns C, D, and E. Although parts occur inall five patterns~ the lists for A and B are predom-inately parts-oriented.
The relatively poor perfor-mance of patterns C and E was ant!cipated, as manythings occur "in" cars (or buildings, etc.)
other thantheir parts.
Pattern D is not so obviously bad as itdiffers from the plural case of pattern B only in thelack of the determiner "the" or "a".
However, thisdifference proves critical in that pattern D tends topick up "counting" nouns such as "truckload."
Onthe basis of this experiment we decided to proceedusing only patterns A and B from Table 1.A.
whole NN\[-PL\] 's POS part NN\[-PL\].
.
.
bu i ld ing 's  basement  .
.
.B.
part NN\[-PL\] of PREP {theIa } DETroods \[JJINN\]* whole NN.
.
.
basement of a bu i ld ing.
.
.C.
part NN in PREP {thela } DETroods \[JJINN\]* whole NN.
.
.
basement in a building .
.
.D.
parts NN-PL of PREP wholes NN-PL.
.
.
basements  o f  bu i ld ings  .
.
.E.
parts NN-PL in PREP wholes NN-PL.
.
.
basements in buildings .
.
.Format: type_of_word TAG type_of_word TAG ...NN = Noun, NN-PL = Plural NounDET = Determiner, PREP = PrepositionPOS = Possessive, J J = AdjectiveTable h Patterns for partOf(basement,building)3 Algorithm3.1 InputWe use the LDC North American News Corpus(NANC).
which is a compilation of the wire outputof several US newspapers.
The total corpus is about100,000,000 words.
We ran our program on the wholedata set, which takes roughly four hours on our net-work.
The bulk of that time (around 90%) is spenttagging the corpus.As is typical in this sort of work, we assume thatour evidence (occurrences of patterns A and B) isindependently and identically distributed (lid).
Wehave found this assumption reasonable, but its break-down has led to a few errors.
In particular, a draw-back of the NANC is the occurrence of repeated ar-ticles; since the corpus consists of all of the articlesthat come over the wire, some days include multiple,updated versions of the same story, containing iden-tical paragraphs or sentences.
We wrote programsto weed out such cases, but ultimately found themof little use.
First, "update" articles still have sub-stantial variation, so there is a continuum betweenthese and articles that are simply on the same topic.Second, our data is so sparse that any such repeatsare very unlikely to manifest hemselves as repeatedexamples of part-type patterns.
Nevertheless sincetwo or three occurrences of a word can make it rankhighly, our results have a few anomalies that stemfrom failure of the iid assumption (e.g., quite appro-priately, "clunker").58Pattern Aheadlight windshield ignition shifter dashboard ra-diator brake tailpipe pipe airbag speedometer con-verter hood trunk visor vent wheel occupant en-gine tyrePattern Btrunk wheel driver hood occupant seat bumperbackseat dashboard jalopy fender ear roof wind-shield back clunker window shipment reenactmentaxlePattern Cpassenger gunmen leaflet hop houseplant airbaggun koran cocaine getaway motorist phone menindecency person ride woman detonator kid keyPattern Dimport caravan make dozen carcass hipment hun-dred thousand sale export model truckload queuemillion boatload inventory hood registration trunktenPattern Eairbag packet switch gem amateur device handgunpassenger fire smuggler phone tag driver weaponmeal compartment croatian defect refugee delayTable 2: Grammatical Pattern ComparisonOur seeds are one word (such as "car") and itsplural.
We do not claim that all single words wouldfare as well as our seeds, as we picked highly probablewords for our corpus (such as "building" and "hos-pital") that we thought would have parts that mightalso be mentioned therein.
With enough text, onecould probably get reasonable r sults with any nounthat met these criteria.3.2 S ta t i s t i ca l  MethodsThe program has three phases.
The first identifiesand records all occurrences ofpatterns A and B in ourcorpus.
The second filters out all words ending with"ing', "ness', or "ity', since these suffixes typicallyoccur in words that denote a quality rather than aphysical object.
Finally we order the possible partsby the likelihood that they are true parts accordingto some appropriate metric.We took some care in the selection of this met-ric.
At an intuitive level the metric should be some-thing like p(w \[ p).
(Here and in what follows wdenotes the outcome of the random variable gener-ating wholes, and p the outcome for parts.
W(w)states that w appears in the patterns AB as a whole,while P(p) states that p appears as a part.)
Met-rics of the form p(w I P) have the desirable propertythat they are invariant over p with radically differentbase frequencies, and for this reason have been widelyused in corpus-based lexical semantic research \[3,6,9\].However, in making this intuitive idea someone moreprecise we found two closely related versions:p(w, W(w) I P)p(w, w(~,) Ip, e(p))We call metrics based on the first of these "looselyconditioned" and those based on the second "stronglyconditioned".While invariance with respect o frequency is gen-erally a good property, such invariant metrics canlead to bad results when used with sparse data.
Inparticular, if a part word p has occurred only once inthe data in the AB patterns, then perforce p(w \[ P)= 1 for the entity w with which it is paired.
Thusthis metric must be tempered to take into accountthe quantity of data that supports its conclusion.
Toput this another way, we want to pick (w,p) pairsthat have two properties, p(w I P) is high and \[ w, plis large.
We need a metric that combines these twodesiderata in a natural way.We tried two such metrics.
The first is Dun-ning's \[10\] log-likelihood metric which measures how"surprised" one would be to observe the data countsI w,p\[,\[ -,w, pl ,  \[ w , - ,p land  I - 'w , - 'P l i foneassumes that p(w I P) = p(w).
Intuitively this will behigh when the observed p(w I P) >> p(w) and whenthe counts upporting this calculation are large.The second metric is proposed by Johnson (per-sonal communication).
He suggests asking the ques-tion: how far apart can we be sure the distributionsp(w \[ p)and p(w) are if we require a particular signif-icance level, say .05 or .01.
We call this new test the"significant-difference" test, or sigdiff.
Johnson ob-serves that compared to sigdiff, log-likelihood tendsto overestimate the importance of data frequency atthe expense of the distance between p(w I P) and3.3 Compar i sonTable 3 shows the 20 highest ranked words for eachstatistical method, using the seed word "car."
Thefirst group contains the words found for the methodwe perceive as the most accurate, sigdiff and strongconditioning.
The other groups show the differencesbetween them and the first group.
The + categorymeans that this method adds the word to its list, -means the opposite.
For example, "back" is on thesigdiff-loose list but not the sigdiff-strong list.In general, sigdiff worked better than surprise andstrong conditioning worked better than loose condi-tioning.
In both cases the less favored methods tendto promote words that are less specific ("back" over"airbag", "use" over "radiator").
Furthermore, the59Sigdiff, Strongairbag brake bumper dashboard driver fenderheadlight hood ignition occupant pipe radi-ator seat shifter speedometer tailpipe trunkvent wheel windshieldSigdiff, Loose+ back backseat oversteer rear roof vehicle visor- airbag brake bumper pipe speedometertailpipe ventSurprise,  Strong+ back cost engine owner price rear roof usevalue window- airbag bumper fender ignition pipe radiatorshifter speedometer tailpipe ventSurprise, Loose+ back cost engine front owner price rear roofside value version window- airbag brake bumper dashboard fender ig-nition pipe radiator shifter speedometertailpipe ventTable 3: Methods Comparisoncombination ofsigdiff and strong conditioning workedbetter than either by itself.
Thus all results in thispaper, unless explicitly noted otherwise, were gath-ered using sigdiff and strong conditioning combined.4 Results4.1 Test ing  HumansWe tested five subjects (all of whom were unawareof our goals) for their concept of a "part."
We askedthem to rate sets of 100 words, of which 50 were in ourfinal results set.
Tables 6 - 11 show the top 50 wordsfor each of our six seed words along with the numberbook10 820 1430 2040 2450 28102030405Ohosp i ta l716212326bui lding car712182129p lant510152022817232631schoo l1014202631Table 4: Result Scoresof subjects who marked the wordas a part of the seedconcept.
The score of individual words vary greatlybut there was relative consensus on most words.
Weput an asterisk next to words that the majority sub-jects marked as correct.
Lacking a formal definitionof part, we can only define those words as correctand the rest as wrong.
While the scoring is admit-tedly not perfect 1, it provides an adequate referenceresult.Table 4 summarizes these results.
There we showthe number of correct part words in the top 10, 20,30, 40, and 50 parts for each seed (e.g., for "book", 8of the top 10 are parts, and 14 of the top 20).
Over-all, about 55% of the top 50 words for each seed areparts, and about 70% of the top 20 for each seed.
Thereader should also note that we tried one ambigu-ous word, "plant" to see what would happen.
Ourprogram finds parts corresponding to both senses,though given the nature of our text, the industrial useis more common.
Our subjects marked both kinds ofparts as correct, but even so, this produced the weak-est part list of the six words we tried.As a baseline we also tried using as our "pattern"the head nouns that immediately surround our targetword.
We then applied the same "strong condition-ing, sigdiff" statistical test to rank the candidates.This performed quite poorly.
Of the top 50 candi-dates for each target, only 8% were parts, as opposedto the 55% for our program.4.2 WordNetWordNet+ door engine floorboard gear grille horn mirrorroof tailfin window- brake bumper dashboard river headlight ig-nition occupant pipe radiator seat shifterspeedometer tailpipe vent wheel windshieldTable 5: WordNet ComparisonWe also compared out parts list to those of Word-Net.
Table 5 shows the parts of "car" in WordNetthat are not in our top 20 (+) and the words in ourtop 20 that are not in WordNet ( - ) .
There are defi-nite tradeoffs, although we would argue that our top-20 set is both more specific and more comprehensive.Two notable words our top 20 lack are "engine" and"door", both of which occur before 100.
More gener-ally, all WordNet parts occur somewhere before 500,with the exception of "tailfin', which never occurswith car.
It would seem that our program would bel For instance, "shifter" is undeniably part of a car, while"production" is only arguably part of a plant.60a good tool for expanding Wordnet, as a person canscan and mark the list of part words in a few minutes.5 Discussion and ConclusionsThe program presented here can find parts of objectsgiven a word denoting the whole object and a largecorpus of unmarked text.
The program is about 55%accurate for the top 50 proposed parts for each of sixexamples upon which we tested it.
There does notseem to be a single cause for the 45% of the casesthat are mistakes.
We present here a few problemsthat have caught our attention.Idiomatic phrases like "a jalopy of a car" or "theson of a gun" provide problems that are not easilyweeded out.
Depending on the data, these phrasescan be as prevalent as the legitimate parts.In some cases problems arose because of taggermistakes.
For example, "re-enactment" would befound as part of a "car" using pattern B in thephrase "the re-enactment of the car crash" if "crash"is tagged as a verb.The program had some tendency to find qualitiesof objects.
For example, "driveability" is stronglycorrelated with car.
We try to weed out most of thequalities by removing words with the suffixes "hess","ing', and "ity.
"The most persistent problem is sparse data, whichis the source of most of the noise.
More data wouldalmost certainly allow us to produce better lists,both because the statistics we are currently collectingwould be more accurate, but also because larger num-bers would allow us to find other reliable indicators.For example, idiomatic phrases might be recognizedas such.
So we see "jalopy of a car" (two times) butnot, of course, "the car's jalopy".
Words that appearin only one of the two patterns are suspect, but to usethis rule we need sufficient counts on the good wordsto be sure we have a representative sample.
At 100million words, the NANC is not exactly small, butwe were able to process it in about four hours withthe machines at our disposal, so still larger corporawould not be out of the question.Finally, as noted above, Hearst \[2\] tried to findparts in corpora but did not achieve good results.She does not say what procedures were used, but as-suming that the work closely paralleled her work onhyponyms, we suspect hat our relative success wasdue to our very large corpus and the use of more re-fined statistical measures for ranking the output.6 AcknowledgmentsThis research was funded in part by NSF grant IRI-9319516 and ONR Grant N0014-96-1-0549.
Thanksto the entire statistical NLP group at Brown, andparticularly to Mark Johnson, Brian Roark, GideonMann, and Ann-Maria Popescu who provided invalu-able help on the project.References\[1\] George Miller, Richard Beckwith, Cristiane Fell-baum, Derek Gross & Katherine J. Miller, "Word-Net: an on-line lexicai database," InternationalJournal of Lexicography 3 (1990), 235-245.\[2\] Marti Hearst, "Automatic acquisition of hy-ponyms from large text corpora," in Proceed-ings of the Fourteenth International Conferenceon Computational Linguistics,, 1992.\[3\] Ellen Riloff & Jessica Shepherd, "A corpus-basedapproach for building semantic lexicons," in Pro-ceedings of the Second Conference on EmpiricalMethods in Natural Language Processing, 1997,117-124.\[4\] Dekang Lin, "Automatic retrieval and cluster-ing of similar words," in 36th Annual Meetingof the Association for Computational Linguisticsand 17th International Conference on Computa-tional Linguistics, 1998, 768-774.\[5\] Gregory Grefenstette, "SEXTANT: extracting se-mantics from raw text implementation details,"Heuristics: The Journal of Knowledge Engineer-ing (1993).\[6\] Brian Roark & Eugene Charniak, "Noun-phraseco-occurrence statistics for semi-automatic se-mantic lexicon construction," in 36th AnnualMeeting of the Association for ComputationalLinguistics and 17th International Conference onComputational Linguistics, 1998, 1110-1116.\[7\] Vasileios Hatzivassiloglou & Kathleen R. McKe-own, "Predicting the semantic orientation of ad-jectives," in Proceedings of the 35th Annual Meet-ing of the ACL, 1997, 174-181.\[8\] Stephen D. Richardson, William B. Dolan & LucyVanderwende, "MindNet: acquiring and structur-ing semantic information from text," in 36th An-nual Meeting of the Association for Computa-tional Linguistics and 17th International Confer-ence on Computational Linguistics, 1998, 1098-1102.\[9\] William A. Gale, Kenneth W. Church & DavidYarowsky, "A method for disambiguating wordsenses in a large corpus," Computers and the Hu-manities (1992).\[10\] Ted Dunning, "Accurate methods for the statis-tics of surprise and coincidence," ComputationalLinguistics 19 (1993), 61-74.61Ocr.8532311471235951220125103613454691648228912451635783613113035394423856154723683353567Frame3069484141696310324993053196116072812277114169324012432108001751512366102312123138236029513901633042522908120721842656973674522140276252611136481943OOWordauthorsubtitleco-authorforewordpublicationepigraphco-editorcovercopypagetitleauthorshipmanuscriptchapterepiloguepublisherjacketsubjectdouble-pagesaleexcerptcontentplotgalleyeditionprotagonistco-publisherspinepremiserevelationthemefallacyeditortranslationcharactertoneflawsectionintroductionreleasediaristprefacenarratorformatfacsimilemock-upessaybackheroinepleasureTable 6: bookx/55*4*4*5*23*4*5*25*5*225*5*4*5*5*0025*5*23*4*3*5*12225*25*224*5*104*4*20125*4*0Ocr.
Frame72 154527 211642 15685 456100 5779 2332 16228 15212 4549 3337 2030 25014 8914 9310 6023 2254 910 6236 4327 3782 144923 27637 57212 1203 613 1569 8332 635219 66127 5811 1432 22 22 247 14049 11514 285129 561617 40425 73015 3583 116 723 1237 152010 20739 16462 338 17364 31Wordrubble~oorfacadebasementroofatriumexteriortenantrooftopwreckagestairwellshelldemolitionbalconyhallwayrenovationjanitorrotundaentrancehulkwallruinlobbycourtyardtenancydebrispipeinteriorfrontelevatorevacuationweb-siteairshaftcorniceconstructionlandlordoccupantownerreardestructionsuperintendentstairwaycellarhalf-milestepcorridorwindowsubbasementdoorspireTable 7: bui ld ingx/505*4*5*5*4*5*14*15*005*5*015*3*05*05*4*0123*4*5*104*3*21113*115*5*05*5*5*5*4*3*62Ocr.922712137094311964371556381171083336428233204675210937181911533116187154226Frame215712430318212108801362858312184428336198556616465772784404196836483216179131176357613347318183761259806326885155151Wordtrunkwindshielddashboardheadlightwheelignitionhooddriverradiatorshifteroccupantbrakeventfendertailpipebumperpipeairbagseatspeedometerconverterbackseatwindowroof .jalopyenginerearvisordeficiencybackoversteerplatecigaretteclunkerbatteryinteriorspeedshipmentre-enactmentconditioneraxletankattributelocationcostpaintantennasocketcorsatireTable 8: carx/54*5*5*5*5*4*5*15*115*3*5*5*5*3*5*4*4*25*5*5*05*4*3*0213*105*3*10025*5*0114*5*005*Oct.43323317318163368441119156253572100532044293231421713451583421451524162293Frame302729943411711692211654043352432123710412072905501537411236923588953472993061394414933156507335714746864167456612220019045742631587576434651887884825606276WordwardradiologisttrograncicmortuaryhopewellclinicaneasthetistgroundpatientfloorunitroomentrancedoctoradministratorcorridorstaffdepartmentbedpharmacistdirectorsuperintendentstoragechieflawncompoundheadnurseswitchboarddebrisexecutivepediatricianboardareaceoyardfrontreputationinmateprocedureoverheadcommitteemilecenterpharmacylaboratoryprogramshahpresidentruinTable 9: hospitalx/55*5*04*05*5*14*4*4*24*5*5*4*3*5*5*4*5*3*3*22005*4*024*11223*11204*014*5*102163Ocr.1855238102196412217222612211924263124238881792355024242940949416213326528357Frame140412311721222459621663844645965125738798O8564411519205065152225325430911774131966131632625532564347856165777793636027626884854043372331371169296632Wordconstructionstalkreactoremissionmodernizationmeltershutdownstart-upworkerrootclosurecompletionoperatorinspectionlocationgatesproutleafoutputturbineequipmentresiduezenfoliageconversionworkforceseeddesignfruitexpansionpollutioncosttouremployeesiteownerroofmanageroperationcharacteristicproductionshootunittowerco-ownerinstrumentationgroundfianceeeconomicsenergyTable 10: p lantx/524*3*3*13*1023*004*223*3*5*23*3*104*013*4*5*22105*13*4*3*3*13*01113*2012Oer.525164134117161948253138911538755610458284227211117875573926105166251736246Fralne10514455382412617952213438740578218552146210221001526716031722655252034231151085660130232341128788711120144283720135553144WorddeanprincipalgraduatepromheadmistressMumnicurriculumseventh-gradergymnasiumfacultycritendowment~umn~cadetenrollmentinfwmaryvaledictoriancommandantstudentfeetauditoriumjamiesonyearbookcafeteriateachergraderwennbergjeffepupilcampusclasstrusteecounselorbenefactorberthhallwaymascotfounderraskinplaygroundprogramgroundcourtyardhallchampionshipaccreditationfellowfreundrectorclassroomTable 1 I: school5*3*3*4*3*5*3*5*5*03*2024*4*05*03*4*5*20o'3*4*5*3*4*204*3*104*3*3*3*4*121024*64
