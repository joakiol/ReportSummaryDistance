Proceedings of the EACL 2009 Workshop on Cognitive Aspects of Computational Language Acquisition, pages 34?41,Athens, Greece, 31 March 2009. c?2009 Association for Computational LinguisticsCategorizing Local Contexts as a Step in Grammatical CategoryInductionMarkus DickinsonIndiana UniversityBloomington, IN USAmd7@indiana.eduCharles JochimIndiana UniversityBloomington, IN USAcajochim@indiana.eduAbstractBuilding on the use of local contexts, orframes, for human category acquisition,we explore the treatment of contexts ascategories.
This allows us to examine andevaluate the categorical properties that lo-cal unsupervised methods can distinguishand their relationship to corpus POS tags.From there, we use lexical informationto combine contexts in a way which pre-serves the intended category, providing aplatform for grammatical category induc-tion.1 Introduction and MotivationIn human category acquisition, the immediate lo-cal context of a word has proven to be a reliableindicator of its grammatical category, or part ofspeech (e.g., Mintz, 2002, 2003; Redington et al,1998).
Likewise, category induction techniquescluster word types together (e.g., Clark, 2003;Schu?tze, 1995), using similar information, i.e.,distributions of local context information.
Thesemethods are successful and useful (e.g.
Koo et al,2008), but in both cases it is not always clearwhether errors in lexical classification are due to aproblem in the induction algorithm or in what con-texts count as identifying the same category (cf.Dickinson, 2008).
The question we ask, then, is:what role does the context on its own play in defin-ing a grammatical category?
Specifically, when dotwo contexts identify the same category?Many category induction experiments start bytrying to categorize words, and Parisien et al(2008) categorize word usages, a combination ofa word and its context.
But to isolate the effect thecontext has on the word, we take the approach ofcategorizing contexts as a first step towards clus-tering words.
By separating out contexts for wordclustering, we can begin to speak of better dis-ambiguation models as a foundation for induc-tion.
We aim in this paper to thoroughly investi-gate what category properties contexts can or can-not distinguish by themselves.With this approach, we are able to more thor-oughly examine the categories used for evaluation.Evaluation of induction methods is difficult, due tothe variety of corpora and tagsets in existence (seediscussion in Clark, 2003) and the variety of po-tential purposes for induced categories (e.g., Kooet al, 2008; Miller et al, 2004).
Yet improving theevaluation of category induction is vital, as eval-uation does not match up well with grammar in-duction evaluation (Headden III et al, 2008).
Formany evaluations, POS tags have been mappedto a smaller tagset (e.g., Goldwater and Griffiths,2007; Toutanova and Johnson, 2008), but therehave been few criteria for evaluating the qualityof these mappings.
By isolating contexts, we caninvestigate how each mapping affects the accuracyof a method and the lexicon.Using corpus annotation also allows us to ex-plore the relation between induced categoriesand computationally or theoretically-relevant cat-egories (e.g., Elworthy, 1995).
While human cate-gory acquisition results successfully divide a lexi-con into categories, these categories are not neces-sarily ones which are appropriate for many com-putational purposes or match theoretical syntacticanalysis.
This work can also serve as a platform tohelp drive the design of new tagsets, or refinementof old ones, by outlining which types of categoriesare or are not applicable for category induction.After discussing some preliminary issues in sec-tion 2, in section 3 we examine to what extent con-texts by themselves can distinguish different cat-egory properties and how this affects evaluation.Namely, we propose that corpus tagsets shouldbe clear about identifying syntactic/distributionalproperties and about how tagset mappings forevaluation should outline how much information34is lost by mapping.
In section 4, in more prelimi-nary work, we add lexical information to contexts,in order to merge them together and see which stillidentify the same category.2 Preliminaries2.1 BackgroundResearch on language acquisition has addressedhow humans learn categories of words, and we usethis as a starting point.
Mintz (2002) shows thatlocal context, in the form of a frame of two wordssurrounding a target word, leads to the target?scategorization in adults, and Mintz (2003) showsthat frequent frames supply category informationin child language corpora.
A frame is not decom-posed into its left and right sides (cf., e.g., Reding-ton et al, 1998; Clark, 2003; Schu?tze, 1995), butis taken as their joint occurrence (Mintz, 2003).1For category acquisition, frequent frames areused, those with a frequency above a certainthreshold.
These predict category membership, asthe set of words appearing in a given frame shouldrepresent a single category.
The frequent frameyou it, for example, largely identifies verbs, asshown in (1), taken from child-directed speech inthe CHILDES database (MacWhinney, 2000).
Forfrequent frames in six subcorpora of CHILDES,Mintz (2003) obtains both high type and token ac-curacy in categorizing words.
(1) a. you put itb.
you see itThe categories do not reflect fine-grained lin-guistic distinctions, though, nor do they fully ac-count for ambiguous words.
Indeed, accuraciesslightly degrade when moving from ?Standard La-beling?2 to the more fine-grained ?Expanded La-beling,?3 from .98 to .91 in token accuracy andfrom .93 to .91 in type accuracy.
In scaling themethod beyond child-directed speech, it wouldbe beneficial to use annotated data, which allowsfor ambiguity and distinguishes a word?s cate-gory across corpus instances.
Furthermore, eventhough many frames identify the same category,1This use of frame is different than that used for subcate-gorization frames, which are also used to induce word classes(e.g., Korhonen et al, 2003).2Categories = noun, verb, adjective, preposition, adverb,determiner, wh-word, not, conjunction, and interjection.3Nouns split into nouns and pronouns; verbs split intoverbs, auxiliaries, and copulathe method does not thoroughly specify how to re-late them.It has been recognized for some time that widercontexts result in better induction models (e.g.,Parisien et al, 2008; Redington et al, 1998), butmany linguistic distinctions rely on lexical infor-mation that cannot be inferred from additionalcontext (Dickinson, 2008), so focusing on shortcontexts can provide many insights.
The use offrames allows for frequent recurrent contexts anda way to investigate corpus categories, or POS tags(cf., e.g., Dickinson and Jochim, 2008).
An addedbenefit of starting with this method is that it can beconverted to a model of online acquisition (Wangand Mintz, 2007).
For this paper, however, weonly investigate the type of information input intothe model.2.2 Some definitionsFrequency The core idea of using frames is thatwords used in the same context are associated witheach other, and the more often these contexts oc-cur, the more confidence we have that the frame in-dicates a category.
Setting a threshold to obtain the45 most frequent frames in each subcorpus (about80,000 words on average), (Mintz, 2003) allows aframe to occur often enough to be meaningful andhave a variety of target words in the frame.To determine what category properties framespinpoint (section 3), we use two thresholds to de-fine frequent.
Singly occurring frames cannot pro-vide any information about groupings of words,so we first consider frames that occur more thanonce.
This gives a large number of frames, cover-ing much of the corpus (about 970,000 tokens), butframes with few instances have very little informa-tion.
For the other threshold, frequent frames arethose which have a frequency of 200, about 0.03%of the total number of frames in the corpus.
Onecould explore more thresholds, but for compar-ing tagset mappings, these provide a good picture.The higher threshold is appropriate for combiningcontexts (section 4), as we need more informationto tell whether two frames behave similarly.Accuracy To evaluate, we need a measure of theaccuracy of each frame.
Mintz (2003) and Red-ington et al (1998) calculate accuracy by countingall pairs of words (types or tokens) that are fromthe same category, divided by all possible pairs ofwords in a grouping.
This captures the idea thateach word should have the same category as every35other word in its category set.Viewing the task as disambiguating contexts(see section 3), however, this measurement doesnot seem to adequately represent cases with a ma-jority label.
For example, if three words havethe tag X and one Y , pairwise comparison re-sults in an accuracy of 50%, even though X isdominant.
To account for this, we measure theprecision of the most frequent category instancesamong all instances, e.g., 75% for the above ex-ample (cf.
the notion of purity in Manning et al,2008).
Additionally, we only use measurementsof token precision.
Token precision naturally han-dles ambiguous words and is easy to calculate in aPOS-annotated corpus.3 Categories in local contextsIn automatic category induction, a category is of-ten treated as a set, or cluster, of words (Clark,2003; Schu?tze, 1995), and category ambiguity isrepresented by the fact that words can appear inmore than one set.
Relatedly, one can cluster wordusages, a combination of a word and its context(Parisien et al, 2008).
An erroneous classificationoccurs when a word is in an incorrect set, and onesource of error is when the contexts being treatedas indicative of the same category are actually am-biguous.
For example, in a bigram model, the con-text be identifies nouns, adjectives, and verbs,among others.Viewed in this way, it is important to gaugethe precision of contexts for distinguishing a cat-egory (cf.
also Dickinson, 2008).
In other words,how often does the same context identify the samecategory?
And how fine-grained is the categorythat the context distinguishes?
To test whethera frame defines a single category in non-child-directed speech, we focus on which categoricalproperties frames define, and for this we use aPOS-annotated corpus.
Due to its popularity forunsupervised POS induction research (e.g., Gold-berg et al, 2008; Goldwater and Griffiths, 2007;Toutanova and Johnson, 2008) and its often-usedtagset, for our initial research, we use the WallStreet Journal (WSJ) portion of the Penn Treebank(Marcus et al, 1993), with 36 tags (plus 9 punc-tuation tags), and we use sections 00-18, leavingheld-out data for future experiments.4Defining frequent frames as those occurring at4Even if we wanted child-directed speech, the CHILDESdatabase (MacWhinney, 2000) uses coarse POS tags.least 200 times, we find 79.5% token precision.Additionally, we have 99 frames, identifying 14types of categories as the majority tag (commonnoun (NN) being the most prevalent (37 frames)).For a threshold of 2, we have 77.3% precision for67,721 frames and 35 categories.5 With precisionbelow 80%, we observe that frames are not fullyable to disambiguate these corpus categories.3.1 Frame-defined categoriesThese corpus categories, however, are composedof a variety of morphological and syntactic fea-tures, the exact nature of which varies from tagsetto tagset.
By merging different tags, we can factorout different types of morphological and syntac-tic properties to determine which ones are more orless easily identified by frames.
Accuracy will ofcourse improve by merging tags; what is importantis for which mappings it improves.We start with basic categories, akin to thosein Mintz (2003).
Despite the differences amongtagsets, these basic categories are common, andmerging POS tags into basic categories can showthat differences in accuracy have more to do withstricter category labels than language type.
Wemerged tags to create basic categories, as in table 1(adapted from Hepple and van Genabith (2000);see appendix A for descriptions).6Category Corpus tagsDeterminer DT, PDT, PRP$Adjective JJ, JJR, JJSNoun NN, NNS, PRP, NNP, NNPSAdverb RB, RBR, RBSVerb MD, VB, VBD, VBG, VBN,VBP, VBZWh-Det.
WDT, WP$Table 1: Tag mappings into basic categoriesThese broader categories result in the accuraciesin table 2, and we also record accuracies for thesimilar PTB-17 tagset used in a variety of unsu-pervised tagging experiments (Smith and Eisner,2005), which mainly differs by treating VBG andVBN uniquely.
With token precision around 90%,it seems that frame-based disambiguation is gener-ally identifying basic categories, though with less5LS (List item marker) is not identified; UH (interjection)appears in one repeating frame, and SYM (symbol) in two.6The 13 other linguistic tags were not merged, i.e., CC,CD, EX, FW, IN, LS, POS, RP, SYM, TO, UH, WP, WRB.36accuracy than in Mintz (2003).?
2 ?
200Orig.
77.3% 79.5%Merged 85.9% 91.0%PTB-17 85.1% 89.7%Table 2: Effect of mappings on precisionBut which properties of the tagset do theframe contexts accurately capture and which dothey not?
To get at this question, we ex-plore linguistically-motivated mappings betweenthe original tagset and the fully-merged tagset intable 1.
Given the predominance of verbs andnouns, we focus on distinguishing linguistic prop-erties within these categories.
For example, sim-ply by merging nouns and leaving all other orig-inal tags unchanged, we move from 79.5% tokenprecision to 88.4% (for the threshold of 200).Leaving all other mappings as in table 1, wemerge nouns and verbs along two dimensions:their common syntactic properties or their com-mon morphological properties.
Ideally, we pre-fer frames to pick out syntactic properties, sincemorphological properties can assumedly be deter-mined from word-internal properties (see Clark,2003; Christiansen and Monaghan, 2006).Specifically, we can merge nouns by nountype (PRP [pronoun], NN/NNS [common noun],NNP/NNPS [proper noun]) or by noun form, inthis case based on grammatical number (PRP[pronoun], NN/NNP [singular noun], NNS/NNPS[plural noun]).
We can merge verbs by finite-ness (MD [modal], VBP/VBZ/VBD [finite verb],VB/VBG/VBN [nonfinite verb]) or by verb form(MD [modal], VB/VBP [base], VBD/VBN [-ed],VBG [-ing], VBZ [-s]).
In the latter case, verbswith consistently similar forms are grouped?e.g.,see can be a baseform (VB) or a present tense verb(VBP).The results are given in tables 3 and 4.
Wefind that merging verbs by finiteness and nouns bynoun type results in higher precision.
This con-firms that contexts can better distinguish syntactic,but not necessarily morphological, properties.
Aswe will see in the next section, this mapping alsomaintains distinctions in the lexicon.
Such use oflocal contexts, along with tag merging, can be usedto evaluate tagsets which claim to be distributional(see, e.g., Dickinson and Jochim, 2008).It should be noted that we have only exploredNoun type Noun formFiniteness 82.9% 81.2%Verb form 81.2% 79.5%Table 3: Mapping precision (freq.
?
2)Noun type Noun formFiniteness 86.4% 85.3%Verb form 84.5% 83.4%Table 4: Mapping precision (freq.
?
200)category mappings which merge tags, ignoringpossible splits.
While splitting a tag like TO (to)into prepositional and infinitival uses would beideal, we do not have the information automati-cally available.
We are thus limited in our eval-uation by what the tagset offers.
Some tag splitscan be automatically recovered (e.g., splitting PRPbased on properties such as person), but if it is au-tomatically recoverable from the lexicon, we donot necessarily need context to identify it, an ideawe turn to in the next section.3.2 Evaluating tagset mappingsSome of the category distinctions made by framesare more or less important for the context to make.For example, it is detrimental if we conflate VBand VBP because this is a prominent ambiguity formany words (e.g., see).
On the other hand, thereare no words which can be both VBP (e.g., see)and VBZ (e.g., sees).
Ideally, induction methodswould be able to distinguish all these cases?justas they often make distinctions beyond what is in atagset?but there are differences in how problem-atic the mappings are.
If we group VB and VBPinto one tag, there is no way to recover that distinc-tion; for VBP and VBZ, there are at least differentwords which inherently take the different tags.Thus, a mapping is preferred which does notconflate tags that vary for individual words.
Tocalculate this, we compare the original lexiconwith a mapped lexicon and count the number ofwords which lose a distinction.
Consider thewords accept and accepts: accept varies betweenVB and VBP; accepts is only VBZ.
When we maptags based on verb form, we count 1 for accept,as VB and VBP are now one tag (Verb).
Whenwe map verbs based on finiteness, we count 0 forthese two words, as accept still has two tags (V-nonfin, V-fin) and accepts has one tag (V-fin).37We evaluate our mappings in table 5 by enumer-ating the number of word types whose distinctionsare lost by a particular mapping (out of 44,520word types); we also repeat the token precisionvalues for comparison.
Perhaps unsurprisingly,grouping words based on form results in high con-fusability (cf.
the discussion of see in section 3.1).On the other hand, merging nouns by type andverbs by finiteness results in something of a bal-ance between precision and non-confusability.
Itis thus these types of categorizations which we canreasonably expect induction models to capture.Lost PrecisionMapping tags ?
2 ?
200All mappings 3003 85.9% 91.0%PTB-17 2038 85.1% 89.7%N.
form/V.
form 2699 79.5% 83.4%N.
type/V.
form 2148 81.2% 84.5%N.
form/Finite 951 81.2% 85.3%N.
type/Finite 399 82.9% 86.4%No mappings 0 77.3% 79.5%Table 5: Confusable word typesFor induction evaluation, in addition to an ac-curacy metric, a metric such as the one we havejust proposed is important to gauge how much cor-pus annotation information is lost when perform-ing tagset mappings.
For example, the PTB-17mapping (Smith and Eisner, 2005) is commonlyused for evaluating category induction (Goldwa-ter and Griffiths, 2007; Toutanova and Johnson,2008), yet it loses distinctions for 2038 words.We could also define mappings which lose nodistinctions in the lexicon.
Initial experimentsshow that this allows no merging of nouns, andthat the resulting precision is only minimally bet-ter than no mapping at all.
We should also notethat the number of confusable words may be toohigh, given errors in the lexicon (cf.
Dickinson,2008).
For example, removing tags occurring lessthan 10% of the time for a word results in only 305confusable words for the Noun type/Finiteness(NF) mapping and 1575 for PTB-17.4 Combining contextsWe have narrowly focused on identical contexts,or frames, for identifying categories, but this couldleave us with as many categories as frames (67,721for ?
2, 99 for ?
200, instead of 35 and 30).
Weneed to reduce the number of categories withoutinappropriately merging them (cf.
the notion of?completeness?
in Mintz, 2003; Christiansen andMonaghan, 2006).
Thus far, we have not utilizeda frame?s target words; we turn to these now, inorder to better gauge the effectiveness of framesfor identifying categories.
Although the work issomewhat preliminary, our goal is to continue toinvestigate when contexts identify the same cate-gory.
This merging of contexts is different thanclustering words (e.g., Clark, 2000; Brown et al,1992), but is applicable, as word clustering relieson knowing which contexts identify the same cat-egory.4.1 Word-based combinationOn their own, frames at best distinguish only verybroad categorical properties.
This is perhaps un-surprising, as the finer-grained distinctions in cor-pora seem to be based on lexical properties morethan on additional context (see, e.g., Dickinson,2008).
If we want to combine contexts in a waywhich maps to corpus tagsets, then, we need toexamine the target words.
It is likely that two setsshare the same tag if they contain the same words(cf.
overlap in Mintz, 2003).
In fact, the more aframe?s word set overlaps with another?s word set,the more likely it is unambiguous in the first place,as the other set provides corroborating evidence.Therefore, we use overlap of frames?
word sets asa criterion to combine them.This allows us to combine frames which do notshare context words.
For example, in (2) we findframes identifying baseform verbs (VB) (2a) andframes identifying cardinal numbers (CD) (2b),despite having a variety of context words.
Theirtarget word sets, however, are sufficiently similar.
(2) a. will to, will the, to the, to up,would the, to their, n?t the,to a, to its, to that, to tob.
or cents, $ million, rose %,a %, about %, to %, $ a,$ billionBy viewing frames as categories, in the fu-ture we could also investigate splitting cate-gories, based on subsets of words, morpho-logical/phonological cues (e.g., Christiansen andMonaghan, 2006), or on additional context words,better handling frames that are ambiguous.Calculating overlap We merge frames whoseword sets overlap, using a simple weighted fre-38quency distance metric.
We define sufficient over-lap as the case where a given percent of the wordsin one frame?s word set are found in the other?sword set.
We define this test in either direction,as smaller sets can be a subset of a larger set.
Forexample, the frames the on (224 tokens) and theof (4304 tokens) have an overlap of 78 tokens;overlap here is 34.8% (78/224).
While we coulduse a more sophisticated form of clustering (see,e.g., Manning et al, 2008), this will help deter-mine the viability of this general approach.Of course, two sets may share a category withrelatively few shared words, and so we transitivelycombine sets of contexts.
If the overlap of framesA andB meet our overlap criterion and the overlapof frames A and C also meet the criterion, then allthree sets are merged, even if B and C have onlya small amount of overlap.7Using the threshold of 200, we test criteria of30%, 40%, and 50% overlap and consider theframes?
overlap calculated as a percentage of wordtypes or as a percentage of word tokens.
For exam-ple, if a word type occurs 10 times in one word setand 20 in the other, the overlap of types is 1, andthe overlap of tokens is 10.
Token overlap bettercaptures similarities in distributions of words.4.2 EvaluationTable 6 shows the number of categories for the30%, 40%, and 50% type-based (TyB) and token-based (ToB) overlap criteria for merging.
As wecan see, the overlap based on tokens in word setsresults in more categories, i.e., fewer merges.% TyB ToB50% 59 7540% 42 6430% 27 50Table 6: Number of categories by conditionThe precision of each of these criteria is givenin table 7, evaluating on both the original tagsetand the noun type/finiteness (NF) mapping.
Wecan see that the token-based overlap is consistentlymore accurate than type-based overlap, and thereis virtually no drop in precision for any of thetoken-based conditions.8 Thus, for the rest of theevaluation, we use only the token-based overlap.7We currently do not consider overlap of already mergedsets, e.g., between A+B and C.8Experiments at 20% show a noticeable drop in precision.% Tags Frames TyB ToB50% Orig.
79.5% 76.4% 79.5%NF 86.4% 82.8% 86.4%40% Orig.
79.5% 75.7% 79.3%NF 86.4% 81.8% 86.1%30% Orig.
79.5% 74.7% 79.1%NF 86.4% 81.7% 86.1%Table 7: Precision of merged framesWe mentioned that if frame word sets overlap,the less ambiguous their category should be.
Wecheck this by looking at the difference betweenmerged and unmerged frames, as shown in table 8.The number of categories are also given in paren-theses; for example, for 30% overlap, 41 framesare unmerged, and the remaining 58 make up 9categories.
These results confirm for this data thatframes which are merged have a higher precision.Merged Unmerged Overall50% 93.4% (7) 79.9% (68) 86.4% (75)40% 89.7% (10) 81.1% (54) 86.1% (64)30% 89.7% (9) 77.4% (41) 86.1% (50)Table 8: Precision of merged & unmerged framesfor NF mapping (with number of categories)But are we only merging a select, small set ofwords?
To gauge this, we measure how muchof the corpus is categorized by the 99 most fre-quent frames.
Namely, 46,874 tokens occur as tar-gets in our threshold of 99 frequent frames out of663,608 target tokens in the entire corpus,9 a re-call of 7.1%.
Table 9 shows some recall figures forthe frequent frames.
There are 9621 word types inthe set of target words for the 99 frequent frames,which is 27.2% of the target lexicon.
Crucially,though, these 9621 are realized as 523,662 targettokens in the corpus, or 78.9%.
The words cate-gorized by the frequent frames extend to a largeportion of the corpus (cf.
also Mintz, 2003).Tokens Types CoverageMerged (30%) 5.0% 20.0% 61.5%Unmerged (30%) 2.0% 11.5% 65.9%Total Overlap 7.1% 27.2% 78.9%Table 9: Recall of frames9Because we remove frames which contain punctuation,the set of target tokens is a subset of all words in the corpus.394.2.1 Qualitative analysisTo better analyze what is happening for futurework, we look more closely at 30% overlap.
Ofthe 58 frames merged into 9 categories, 54 of themhave the samemajority tag after merging.
The fourframes which get merged into a different categoryare worth investigating, to see the method?s limi-tations and potential for improvement.Of the four frames which lose their majoritytag after merging, two can be ignored when map-ping to the NF tags.
The frame it the with ma-jority tag VBZ becomes VBD when merged, butboth are V-fin.
Likewise, n?t to changes fromVB to VBN, both cases of V-nonfin.
The thirdcase reveals an evaluation problem with the orig-inal tagset: the frames million $ (IN) and %$ (TO) are merged into a category labeled TO.The tag TO is for the word to and is not split intoprepositional and infinitival uses.
Corpus cate-gories such as these, which overlap in their def-initions yet cannot be merged (due to their non-overlapping uses), are particularly problematic forevaluation.The final case which does not properly merge isthe most serious.
The frame is the (37% of to-kens as preposition (IN)) merges with is a (41%of tokens as VBG); the merged VBG category hasan precision of 34%.
The distribution of tags is rel-atively similar, the highest percentages being forIN and VBG in both.
This highlights the pointmade earlier, that more information is needed, tosplit the word sets.4.2.2 TIGER CorpusTo better evaluate frequent frames for determin-ing categories, we also test them on the GermanTIGER corpus (Brants et al, 2002), version 2,to see how the method handles data with freerword order and more morphological complexity.We use the training data, with the data split asin Dubey (2004).
The frequency threshold forthe WSJ (0.03% of all frames) leaves us withonly 60 frames in the TIGER corpus, and 51 ofthese frames have a majority tag of NN.10 Thus,we adjusted the threshold to 0.02% (102 mini-mum occurrences), thereby obtaining 119 frequentframes, with a precision of 82.0%.
For the 30%token-based overlap (the best result for English),frames merged into 81 classes, with 79.1% pre-cision.
These precision figures are on a par with10We use no tagset mappings for our TIGER experiments.English (cf.
table 7).11 Part of this might be dueto the fact that NN is still a large majority (76% ofthe frames).
Additionally, we find that, althoughthe frame tokens make up only 5.2% of the corpusand the types make up 15.9% of the target lexi-con, those types correspond to 67.2% of the targetcorpus tokens.5 Summary and OutlookBuilding on the use of frames for human categoryacquisition, we have explored the benefits of treat-ing contexts?in this case, frames?as categoriesand analyzed the consequences.
This allowed usto examine a way to evaluate tagset mappings andprovide feedback on distributional tagset design.From there, we explored using lexical informationto combine contexts in a way which generally pre-serves the intended category.We evaluated this on English and German, but,to fully verify our findings, a high priority is toperform similar experiments on more corpora, em-ploying different tagsets, for different languages.Additionally, we need to expand the definition ofa context to more accurately categorize contexts,while at the same time not lowering recall.AcknowledgementsWe wish to thank the Indiana University Compu-tational Linguistics discussion group for feedback,as well as the three anonymous reviewers.A Some Penn Treebank POS tagsDT DeterminerJJ AdjectiveJJR Adjective, comparativeJJS Adjective, superlativeMD ModalNN Noun, singular or massNNS Noun, pluralNNP Proper noun, singularNNPS Proper noun, pluralPDT PredeterminerPRP Personal pronounPRP$ Possessive pronounRB AdverbRBR Adverb, comparativeRBS Adverb, superlativeVB Verb, base formVBD Verb, past tenseVBG Verb, gerund or present participleVBN Verb, past participleVBP Verb, non-3rd person singular presentVBZ Verb, 3rd person singular presentWDT Wh-determinerWP$ Possessive wh-pronoun11Interestingly, thresholds of 20% and 10% result in simi-larly high precision.40ReferencesBrants, Sabine, Stefanie Dipper, Silvia Hansen,Wolfgang Lezius and George Smith (2002).
TheTIGER Treebank.
In Proceedings of TLT-02.Sozopol, Bulgaria.Brown, Peter F., Peter V. deSouza, Robert L. Mer-cer, T. J. Watson, Vincent J. Della Pietra andJenifer C. Lai (1992).
Class-Based n-gramModels of Natural Language.
ComputationalLinguistics 18(4), 467?479.Christiansen, Morten H. and Padraic Monaghan(2006).
Discovering verbs through multiple-cueintegration.
In Action Meets Word: How Chil-dren Learn Verbs, Oxford: OUP.Clark, Alexander (2000).
Inducing Syntactic Cat-egories by Context Distribution Clustering.
InProceedings of CoNLL-00.
Lisbon, Portugal.Clark, Alexander (2003).
Combining Distribu-tional and Morphological Information for Partof Speech Induction.
In Proceedings of EACL-03.
Budapest.Dickinson, Markus (2008).
Representations forcategory disambiguation.
In Proceedings ofColing 2008.
Manchester.Dickinson, Markus and Charles Jochim (2008).
ASimple Method for Tagset Comparison.
In Pro-ceedings of LREC 2008.
Marrakech, Morocco.Dubey, Amit (2004).
Statistical Parsing for Ger-man: Modeling syntactic properties and anno-tation differences.
Ph.D. thesis, Saarland Uni-versity, Germany.Elworthy, David (1995).
Tagset Design and In-flected Languages.
In Proceedings of the ACL-SIGDAT Workshop.
Dublin.Goldberg, Yoav, Meni Adler and Michael Elhadad(2008).
EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start).
In Pro-ceedings of ACL-08.
Columbus, OH.Goldwater, Sharon and Tom Griffiths (2007).
Afully Bayesian approach to unsupervised part-of-speech tagging.
In Proceedings of ACL-07.Prague.Headden III, William P., David McClosky andEugene Charniak (2008).
Evaluating Unsu-pervised Part-of-Speech Tagging for GrammarInduction.
In Proceedings of Coling 2008.Manchester.Hepple, Mark and Josef van Genabith (2000).Experiments in Structure-Preserving GrammarCompaction.
In 1st Meeting on Speech Tech-nology Transfer.
Seville, Spain.Koo, Terry, Xavier Carreras and Michael Collins(2008).
Simple Semi-supervised DependencyParsing.
In Proceedings of ACL-08.
Columbus,OH.Korhonen, Anna, Yuval Krymolowski and ZvikaMarx (2003).
Clustering Polysemic Subcatego-rization Frame Distributions Semantically.
InProceedings of ACL-03.
Sapporo.MacWhinney, Brian (2000).
The CHILDESproject: Tools for analyzing talk.
Mahwah, NJ:Lawrence Erlbaum Associates, third edn.Manning, Christopher D., Prabhakar Raghavanand Hinrich Schu?tze (2008).
Introduction to In-formation Retrieval.
CUP.Marcus, M., Beatrice Santorini and M. A.Marcinkiewicz (1993).
Building a large anno-tated corpus of English: The Penn Treebank.Computational Linguistics 19(2), 313?330.Miller, Scott, Jethran Guinness and Alex Zama-nian (2004).
Name Tagging with Word Clustersand Discriminative Training.
In Proceedings ofHLT-NAACL 2004.
Boston, MA.Mintz, Toben H. (2002).
Category inductionfrom distributional cues in an artificial lan-guage.
Memory & Cognition 30, 678?686.Mintz, Toben H. (2003).
Frequent frames as acue for grammatical categories in child directedspeech.
Cognition 90, 91?117.Parisien, Christopher, Afsaneh Fazly and SuzanneStevenson (2008).
An Incremental BayesianModel for Learning Syntactic Categories.
InProceedings of CoNLL-08.
Manchester.Redington, Martin, Nick Chater and Steven Finch(1998).
Distributional Information: A PowerfulCue for Acquiring Syntactic Categories.
Cogni-tive Science 22(4), 425?469.Schu?tze, Hinrich (1995).
Distributional Part-of-Speech Tagging.
In Proceedings of EACL-95.Dublin, Ireland.Smith, Noah A. and Jason Eisner (2005).
Con-trastive Estimation: Training Log-Linear Mod-els on Unlabeled Data.
In Proceedings ofACL?05.
Ann Arbor, MI.Toutanova, Kristina and Mark Johnson (2008).A Bayesian LDA-based Model for Semi-Supervised Part-of-speech Tagging.
In Pro-ceedings of NIPS 2008.
Vancouver.Wang, Hao and Toben H. Mintz (2007).
A Dy-namic Learning Model for Categorizing WordsUsing Frames.
In Proceedings of BUCLD 32.pp.
525?536.41
