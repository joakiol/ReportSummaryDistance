//II//I/l//////////The segmentation problem in morphology learningChr i s topher  D .
Mann ingUniversity of  Sydneycmanning@mail.usyd.edu.auRecently there has been a large literature onvarious approaches to learning morphology, andthe success and cognitive plausibility of differentapproaches (Rumelhart and McClelland (1986),MacWhinney and Leinbach (1991) arguing forconnectionist models, Pinker and Prince (1988),Lachter and Bever (1988), Marcus et al (1992)arguing against connectionist models, Ling andMarinov (1993), Ling (1994) using ID3/C4.5 de-cision trees, and Mooney and Califf (1995, 1996)using inductive logic programming/decision lists,among others).
However - except for a couple offorays into German - this literature has been ex-clusively concerned with the learning of the En-glish past tense.
This has not worried some.
Lingis happy to describe it as "a landmark task".
Butwhile the English past tense has some interestingfeatures in its combination of regular ules withsemi-productive strong verb patterns, it is in manyother respects a very trivial morphological system- reflecting the generally vestigal nature of inflec-tional morphology within modem English.In this paper, I briefly discuss ome experimentson learning morphological forms in languages withmuch richer morphological paradigms.
Such lan-guages are common throughout much of the globe(from Latin and Greek to Inuit and Cashinahuaor Anmajere and Kayardild - to finish with someAustralian examples).
Attempting to learn mor-phology in languages with rich morphology raisesquite different problems from those discussed inthe work above, issues discussed - if rather naivelyand unsatisfactorily from a computational view-point - in earlier work such as Pinker (1984),MacWhinney (1978) and Peters (1983).
Foremostamong these is the segmentation problem of howone cuts the complex morphological forms into bitswith meanings identified.
Note that I assume herethat the child has already figured out the mean-ings of words.
This is a big assumption, but it isreasonable for a model to focus on one aspect ofthe learning problem - and at any rate the learn-ing task is still much broader and more realisticthan that attempted by the recent English pasttense literature.
It may not even be unrealistic;see Pinker (1984:29-30) for a general defense ofassuming some form of "semantic bootstrapping"and MacWhinney (1978:70-71) who for argumentsfor the learning of word meanings before gaininga productive understanding of them ("it appearsthat the use of inflections in amalgams i  stabilizedsemantically before these amalgams are analyzedmorphologically").
Thus the learning task whichI am attempting to address could be stated thus:Given a set of words and a representationof their meanings, determine an internalizedrepresentation that will allow heard and (reg-ular) unheard forms to be successfully pre-dicted and parsed.The  segmentat ion  prob lem isd i f f i cu l tThere are many morphological issues that makethe segmentation problem difficult.
If a learnerworks on-line, then it has to be careful not to besent down wrong tracks.
For example, suppose alearner already knows that the English past tenseis regular ly/t /after  a voiceless ound.
If it en-counters the past of burst we would expect thefollowing analysis to be generated:(1) Mea,~i,~a Word S~,~ Tense\[PRIED: burst.
TENSE: PAST\] burst burs tThe stem is wrongly found to be/burs/, and couldonly perhaps be fixed after observing the presentand deciding that some form of reanalysis i nec-essary.Many languages have ~sional morphologywhere one morpheme expresses multiple semanticManning 299 The segmentation problem in morphology learningChristopher D. Manning (1998) The segmentation problem in morphology learning.
In D.M.W.
Powers (ed.
)NeMLaP3/CoNLL98 Workshop on Paradigms and Grounding inLanguage Learning, ACL, pp 299-305.components.
This means that looking for a con-sistent phonetic exponent for one meaning compo-nent will be in vain.
For example, consider tensein the following data from Pocomchi:(2) 'to see' Present Past\[SUB J: I, OBJ: YOU\] tiwil ~atwil\[SUBJ: I, OBJ: THEM\] kiwil ~iwilThe account of MacWhinney (1978) does not ad-dress fusional morhology, Pinker (1984) attemptsto but various flaws in his proposed segmentationprocedures mean that fusional morphology is fre-quently mishandled, and due to the simplicity ofthe English past tense task, none of the more re-cent work addresses this problem.Further problems are created by inflectionalclasses (declensions or conjugations).
For exam-ple, if one starts with a bunch of words in theLatin ablative singular:(3) mensa table.ABL.SGserv5 slave.ABL.SGurbe city.ABL.SGmanu hand.ABL.SGr~ thing.ABL.SGThen there is no (fusional) morpheme that ex-presses ablative singular.
It has different allo-morphs for different inflectional classes.However, if the learning procedure just looksat stem-specific paradigms in isolation, and thencompares the results to see if they happen to besimilar (as Pinker (1984) suggested), there is noth-ing to make the learner hunt out similarity, tolook deeper for alternative analyses that would ex-pose common underlying structure (much as a lin-guist does).
It is only this latter sort of approachthat will allow us to postulate general phono-logical rules.
Although a symbolic morphologylearner presumably must start with stem-specificparadigms, we need to have a counterbalancingprinciple of paradigm economy (Carstairs 1988),which collapses together stem-specific paradigmswhere possible, even when this wasn't the obvi-ous analysis at first.
For example, consider theconsonant-stem declension of Greek or Latin (theexamples here are from Koin4 Greek).
If we seethe forms:(4) himas thong.NOM.SOhimanta thong.ACC.SGhimantos thong.GEN.SGthen (if it were not for any prior knowledge ofGreek or Latin), the obvious analysis would be:(5) hima- \[pred: thong\]- s  \[case: nora, num: sg\]-nta \[case: acc, num: sg\]-ntos \[case: gen, hum: sg\]and we will find other words that appear to declinesimilarly.
However, when we see a reasonable col-lection of words of another kind:(6) skolops Stake.NOM.SCskolopa stake.ACC.SGskolopos stake.GEN.SGwe can decide it would be better to reanalyze theforms above thus:(7) hima- \[pred: thong\] / _ shimant- \[pred: thong\] / elsewhereskolop- \[pred: stake\]-s \[case: nora, num: sg\]-a \[case: acc, num: sg\]-os \[case: gen, num: sg\]The key to discovering the phonological rule thatdeletes alveolars before/s/ is a notion of paradigmeconomy that suggests the reanalysis shown in (7).For identifying allomorphs of morphemes,Pinker (1984) depends heavily on a notion of "pho-netic material in common".
However, he merelysuggests that the definition of this notion shouldbe drawn from an appropriate theory of phonol-ogy.
But in general a theory of phonology cannotjust take two words and tell one what their "pho-netic material in common" is.
To consider an ex-ample from Latin nouns raised by Braine (1987),given the noun forms on/o and on/inem, the pho-netic material in common is going to be  On/.
Itrequires a more sophisticated level of theory for-mation to determine that the desired root form forthis word is actually on~in.
Even in simpler casesof sandhi (word internal phonological changes), itwill not be immediately apparent what the stemof a word (or other morphemes within it) is.
Con-sider the Japanese verb forms in (8):(8) nomu drink (present)nonda drank (past)nomitai want to drinknomimasu drink (present honorific)Is the stem 'drink' no, nora, or even nomi?
Sucha question cannot in general be answered simplyusing a notion of common phonetic material, butmust.
be answered in terms of a broader under-standing of the paradigmatic system of the lan-guage as a whole.Manning 300 The segmentation problem in morphology learningImmmmmmmmmmmmmmm/lII//lIl/I/////lMacWhinney (1978) does provide an explicit,if simplistic, theory of phonetic similarity.
In it,parts of words match only if they are string iden-tical.
But this notion is insufficient o accountfor not only sandhi effects but also many of thephenomena that inspired autosegmental phonol-ogy, that is, melodies being stretched or squashedto fit onto a skeleton.
In particular, considervowel lengthening of the sort shown in (9), fromHungarian: 1(9) SG PLwater viiz vizekfire tfiiiz tiizekbird madaar madarakIt is clearly necessary for a learner to be able toidentify the stems of these words as v/z, t~z andmadar, despite the fact that they are not segmen-tally identical in their two appearances.
This willnever happen if segments are simply matched one-for-one.We see that getting a start on the segmenta-tion problem seems to have two main components:working out what the allomorphs and/or underly-ing forms in the data are and working out the envi-ronments in which different allomorphs occur.
Forthe first segmentation problem, we saw that nei-ther aUomorphs nor especially underlying formscan be correctly determined by just looking for"phonetic material in common".
Indeed, we deter-mined the stronger esult that appropriate stemsoften cannot be determined by looking at a stem-specific paradigm at all, but can only be deter-mined by comparisons across the morphologicalsystem, invoking some notion of paradigm econ-omy.
For the second problem, we can use exist-ing classification techniques, which have been ex-plored in the English past tense work.
For exam-ple, one can use ID3, as I do here, as an algorithmthat can find conditioning features while still be-ing reasonably tolerant of noise (that is, irregularforms) in the data.An  imp lemented  symbol i cmorpho logy  learnerMy model works from being given pairs of a sur-face (allophonic) form and a representation f itsmeaning (this essentially consists of just encoding1In Hungarian orthography long vowels axe indi-cated by acute accents, but here I write them as dou-ble vowels, roughly approximating the phonetic inputto the child.a word's position within paradigmatic dimensionsof contrast, by giving it a meaning such as \[PRED:apple, NUM: SG, CASE: ACC\]).
It works essen-tially as an azT-tx-stripping model of morphologi-cal processing with a back-end environment cat-egorization system based on the ID3 algorithm.My model and indeed all the models mentionedabove, connectionist and symbolic alike, assumethat morphemes and words can be satisfactorilyrepresented asa linear sequence of segments.
Thisflies in the face of much recent work in phonology(e.g., Goldsmith 1990), but works for 90% of lan-guages, and is a useful simplifying assumption atthis stage.
However, I will introduce mechanismsthat allow conditioning by nearest consonants orvowels, and the stretching of melodies, which ac-tually allow us to capture some (though not all)of the features of an autosegmental analysis.The model I will present here, like all Englishpast tense models, is one of conditioned allomor-phy that attempts to provide asolution to the twoproblems mentioned at the end of the last section:determining what the allomorphs of morphemesare and the environments where they occur.
Thisis still somewhat less than a complete theory ofphonology.
So long as productive phonologicalchanges are confined to inflectional endings, such atheory is in fact sufficient.
However, if productivephonological rules change stems, then somethingmore is needed: one must postulate phonologicalrules that can then be applied to generate the al-lomorphs of newly heard stems.
This last task isnot attempted here.
However, it seems reasonableto suppose that this is a higher-order inductivestep that would build on the results of a theory oflearning conditioned allomorphs.Chopping words into morphemes Wordsand their paradigmatic meanings axe collected un-til a reasonable percentage of the forms for a par-ticular stem-specific paradigm have been seen.At this point a stem-specific paradigm is ana-lyzed.
The model (heuristically) determines likelycandidates for the first or last morph in all wordsthat contain the appropriate s mantic feature (fea-tures are here things like TENSE or SUBJ.NUM) bylooking at words that share a certain feature, andseeing if they are all phonetically similar at oneend or the other.
For each such candidate in turn,the model determines candidate guesses for eachmorpheme that expresses this feature.The model uses both similarity matching be-tween all words sharing a morpheme, and differ-ence matching from the other end with all wordsManning 301 The segmentation problem in morphology learningthat have the same meaning except in the value ofthe morpheme in question to determine candidatemorpheme values, as indicated in (10):(10)b.a.
Given carries and carried, one can at-tempt to learn \[PRED: CARRY\] by similaritymatching.Again given carries and carried, one canattempt o learn either PAST or PRES.3SGby difference matching (since the rest of themorphemes in these words are identical).In the presence of word internal sandhi, using bothsame and difference matching will generally serveto delimit the boundary region wherein sandhi ef-fects are occurring, and the model considers thepossibility of a morpheme break anywhere withinthis sandhi region.
For example, given the follow-ing data:(11) 'foot' 'house''my' kepina yotna'your' kepika yotdathe program determines/a/as   value for 'your'by same matching, but /ka /and/dR/by  differencematching by looking at the two forms for 'foot' and'house' respectively.
These two boundary pointsmark out the sandhi region within which the valueof 'your' must be found (i.e., it is either /a/  of/Ca / fo r  some consonant).To determine whether two strings of segmentsmight reasonably be two allomorphs of a mor-pheme, the model uses a similarity condition.
Thisis measured by counting a mismatch in phono-logical features.
The model uses fairly standardphonological features (based on those in Halle andClements 1983).
This requirement of surface sim-ilarity between morphs is similar to, but weakerthan having a Unique Underlier Condition.
Acrossdifferent word-specific paradigms, the form of amorpheme can vary at will - the similarity condi-tion only applies when analyzing a word-speciflcparadigm, or a group of such paradignas when at-tempting inflectional class formation.
Within aparadigm, if a solution satisfying the similaritycondition cannot be found, then fusional morphsmust be postulated.As well as allowing a certain amount of mis-match of features between 'matching' segments,the similarity marcher was also built to handlethe stretching (or squeezing) of melodies.
Whena segment occurs multiple times in one form,the matching routine will nondeterministically at-tempt to match any number of copies of that seg-ment in one word with the segment in other words.In this way the Hungarian stem allomorphs dis-cussed in (9) can make it past the similarity con-dition.When a proposed form has been found for eachvalue of a feature (i.e., each case of a case featureor whatever), these affixes are then stripped fromthe correct end of all words that contain them, andthe above analysis procedure can then be appliedrecursively to the remaining partial words.
Withluck, this procedure will correctly analyze words,but in cases of sandhi where the learner has had tomake guesses, there may be mistakes.
The modelincludes a number of obvious heuristics to tell itthat a mistake has been made:?
If values have been assigned to all features, butthere are still some segments left unassigned asa residue, then an error has occurred.?
If a stem is null an error has occurred.
(Since most analysis is done on word-specificparadigms, which give no evidence of contrast-ing stems, this can be a useful heuristic.)?
An  initial pass examines words that differ inone feature and if those words are different, themodel notes that the values of the feature con-cerned must be different.
If a solution then triesto assign an identical value to these differentmorphs then an error has occurred.In cases of error, certain potential segmentationsare dimiuated (where multiple possible segmen-tations have been generated, as in the presenceof sandhi effects).
The limiting case is when nopossible way of chopping the word into morphssucceeds.
As mentioned above, this is indicativeof fusion, which was defined as a last resort whenthere is no available analysis of multiple featuresinto separate morphemes (allomorphs).
In suchcases all possible analyses should fail in this firstphase, and the model will then recursively attempthigher level analyses that postulate first partiallyand then finally totally fusional analyses (so that,for example, instead of trying to find a morphemerepresenting each case of a CASE feature, the modelwill be trying to find a morpheme representingeach value of the crossproduct of two or more fea-tures, for example a value for each case and num-ber combination).On  completion of an analysis of this sort, thehistory of the morpheme stripping order can be re-constructed to give the morpheme order in words.Manning 302 The segmentation problem in morphology learningIIIIIIIIIIIIIIIIII!iIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIAdditionally the program notes whether each fea-ture appears to be compulsorily expressed or op-tional in the words that it has been trained on.No more subtle ordering information than this iscurrently learned.Forming Inflectional Classes The above givesa plausible first attempt at a model that chopswords into morphemes.
But earlier, I argued thatthe correct chop point cannot always be discov-ered while looking at just a single word-specificparadigm.
My program attempts to solve suchproblems by a process of inflectional class forma-tion.
After a second stem-specific paradigm hasbeen analyzed, the model examines the two sets ofendings that have been generated, and determineswhether they are similar.
2 If the endings appearsimilar, the analysis procedure described above isthen applied to words belonging to both stems si-multaneously.
If this analysis ucceeds (proposingat most the same amount of fusion as when ex-amining the stem-specific paradigms), then thisreanalysis for the two words is recorded.
Such areanalysis can move the morpheme boundaries incases uch as the Greek consonant stem declensiondiscussed above (4).Learning phonological condit ioning Oncewords are (hopefully correctly) segmented intomorphemes, there may still be several allomorphsof a morpheme, and there remains the problem ofdetermining which allomorph occurs when.
Themodel assumes two possible forms of allomorphconditioning, phonological conditioning and lex-ical conditioning (where the stem lexeme deter-mines which allomorph occurs), and uses a de-cision tree based learning system (with pruning)that can handle noisy input and disjunctive classdescriptions i employed.
To operate, the ID3algorithm needs a list of possible features thatcan condition changes.
The list used here is thefollowing: an allomorph can be conditioned byany phonological feature (cons, son, ant, etc.
)of any of the preceding or following segment orthe preceding or following \[-cons\] or \[-syl\] seg-ment.
This captures autosegmental-phonology-like affects, since we are allowing the nearest con-sonant and vowel to also be 'adjacent' for the pur-poses of conditioning.
If the decision tree falls to2The model uses an heuristic measure of similar-ity that focuses on the 'nucleus' of morphemes.
Thatis, due to  mis takes  in segmentation, the margins ofmorphemes may well be different, but if they reallybelong to the same inflectional class, they should havea common core.find phonological conditioning features, then lexi-cal conditioning is assumed.The output decision trees are then converted tosomething more similar to conventional phonolog-ical rules.
However, in this model, all environ-ments are surface conditions, so we cannot com-pact rule systems by using rule ordering (to selec-tively bleed/feed various rules).
Instead a systemof rule priorities was implemented, sothat groupsof rules form default hierarchies (Holland et al1986).
This notion is the same as having elsewhereconditions on rules, as in the notion of disjunc-tive rule ordering.
So, rather than having eitherthe decision tree in (12a) or the equivalent ruleset in (12b), the use of a default hierarchy lets ususe the representation shown in (12c).
Rules pre-ceded by a number have a higher priority (equalto that number) and will apply in preference toother (usually but not necessarily more general)rules.
Rules not preceded by a number can be re-garded as having priority 1.
Thus a word endingin a \[-cont, +cor, +ant\] sound will take the allo-morph \[~d\], while all other sounds will receive theallomorph \[t\].
(12) a.-{-CONT -CONTt --COR +CORrt --ANT +ANT\[ it ~1b.
\[tense: past\] --~ t /\[tense: past\] --+ t /\[tense: past\] ~ t /\[tense: past\] -+ ~d /XX _ \[- co T\]COR JX _COR /ANT JXCOR /ANTc.
\[tense: past\] --+ tManning 303 The segmentation problem in morphology learning2: \[tense: past\] --+ ~d / X \[i co T1COR /ANT JFinally the model includes asimple parser/generator which can use the ruleslearned by the preceding processes to parse andgenerate morphological forms.Exper imenta l  ResultsI've done small studies with my model on por-tions of the morphological systems of a numberof languages.
Provided the language phenomenastay within the bounds of what the model cancope with (i.e., avoiding semitic and similar tem-platic languages), it is a fairly robust learner.
Iassure the reader that my model can also learnthe English past tense - essentially duplicatingLing's results, but actually doing the segmentationwork rather than just a classification task.
Here Iwill present a small study of the tense endings ofAnmajere verbs (Anmajere is an Australian lan-guage; data is from Avery Andrews (p.c., 1989)).In addition, small studies have shown that themodel can learn the following examples which Ihave mentioned previously:?
Latin nouns (this involves learning fusional caseand number morphemes)?
Greek consonant stems (this involves learningparadigm reanalyses for data such as (4)?
Japanese verb morphology (word internalsandhi obscures morpheme size)?
Hungarian nouns (9) - this requires the stretch-ing of melodiesFigure 7?
shows verb forms from Anmajere.The digraphs rr, rl, rn and rd represent a sin-gle sound (a trill for rr, the rest are apical reflex-ives), using the usual orthography for Australianlanguages (Dixon 1980).
While all these verbs areregular, they demonstrate more subtle phonologi-cal conditioning than in the English past tense.
Afinal labial stop of verb stems is voiced or voice-less depending on the voicing of the first conso-nant (not the next sound) of the inflection (seethe verbs 'depart' and 'leave alone').
In the in-flections, the recent past has two allomorphs, hav-ing an apico-alveolar /n/ when the stem ends in a\[-COR\] consonant (for example, with arlk- 'yell'),Verb Present Recent pastyell arlkeme arlkendepart albeme albenleave alone imbeme imbenhear aweme awencut akeme akenspeak agkeme agkencook ideme idernsit aneme anerntake ineme inerushape ardeme arderncome out arrademe arradernrun arrjaneme arrjanernchuck iwemepop ardemeFigure 1.
Anmajere verb formsPastarlkekealpekeimpekeawekeakekeagkekeand the retroflex /rn/ when it ends in a \[+cor\]consonant (for example, with id- 'cook').My model can successfully earn these distribu-tions, producing rules such as these:(13) a.
\[pred: yell\] --+ "arlk"b.
\[pred: depart\] --+ "alb" / _  C\[+ soN\]c. \[pred: depart\] --, "alp" / _  C\[- so \]d. \[tense: reel --~ "ern" / X _ _\[+ coale.
\[tense: rec\] ~ "en" / X _ _\[- coR\]f. \[tense: pres\] --~ "eme"g. \[tense: past\] --+ "eke"The model chose \[+son\] rather than \[+voiced\] asthe distinguishing feature for the first alternation,which gives non-distinct results for the data thatwas given.
This knowledge is sufficient for themodel to be able to fill in the remaining entries inthe above table (as a transfer test).
However, asnoted before, the model would need to go one stagefurther and learn universally applicable phonologi-cal rules to be able to extend its knowledge of stemallomorphy from known verbs to new or nonceverbs.Manning 304 The segmentation problem in morphology learningRRmImm|mmmm|mmmmmIIIIIIIIIIIIIIIIIIIIIIIIConclusionThis work introduces a more substantial nd re-alistic problem domain for morphology learningprograms, and demonstrates a ymbolic morphol-ogy learner that can learn an interesting range ofthe complex morphological systems found in theworld's languages.
On the other hand, it is notthe final word, and more work still has to be doneon generalizing its representations and algorithmsso that it is capable of learning the morphology ofall human languages.REFERENCESArchangeli, D. 1988.
Aspects of underspecifica-tion theory.
Phonology 5:183-208.Braine, M. D. S. 1987.
What is learned in ac-quiring word classes--a step toward an acquisi-tion theory.
In B. MacWhinney (Ed.
), Mecha-nisms of Language Acquisition, 65-87.
Hillsdale,N J: Lawrence Erlbaum.Carstairs, A.
1988.
Nonconcatenative inflec-tion and paradigm economy.
In M. Hammondand M. Noonan (Eds.
), Theoretical Morphology:Approaches in Modern Linguistics, 71-77.
SanDiego, CA: Academic Press.Dixon, R. M. W. 1980.
The Languages of Aus-tralia.
Cambridge: Cambridge University Press.Goldsmith, J.
A.
1990.
Autosegmental and Met-rical Phonology.
Oxford: Basil Blackwell.Halle, M., and G. N. Clements.
1983.
Problembook in phonology.
Cambridge, MA: MIT Press.Holland, J.
It., K. J. Holyoak, R. E. Nisbett, andP.
R. Thagard.
1986.
Induction: proceses of in-ference, learning and discovery.
Cambridge, MA:MIT Press.Lachter, J., and T. G. Bever.
1988.
The relationbetween linguistic structure and associative theo-ries of language l arning--a constructive critiqueof some connectionist learning models.
Cognition28:195-247.Ling, C. X.
1994.
Learning the past tense of en-glish verbs: the symbolic pattern associator vs.connectionist models.
Journal of Artificial Intel-ligence Research 1:209--229.Ling, C. X., and M. Marinov.
1993.
Answeringthe connectionist challenge: a symbolic model oflearning the past ense of english verbs.
Cognition49:235-290.MacWhinney, B.
1978.
The acquisition of mor-phophonology.
Monographs of the Society forResearch in Child Development, 43(1-2, SerialNo.
174).MacWhinney, B., and J. Leinbac~.
1991.
Imple-mentations are not conceptualizations: Revisingthe verb learning model.
Cognition 40:121-157.Marcus, G. F., S. Pinker, M. Ullman, M. Hol-lander, T. J. Rosen, and F. Xu.
1992.
Overregu-larization in Language Acquisition.
Chicago, IL:University of Chicago Press.Mooney, R. J., and M. E. Califf.
1995.
Inductionof first-order decision lists: Results on learningthe past tense of english verbs.
Journal of Arti-ficial Intelligence Research 3:1-24.Mooney, R. J., and M. E. Califf.
1996.
Learn-ing the past tense of english verbs using induc-tive logic programming.
In S. Wermter, E. Riloff,and G. Scheler (Eds.
), Symbolic, Connectionist,and Statistical Approaches to Learning for Natu-ral Language Processing.
Springer Verlag.Peters, A. M. 1983.
The units of language acqui-sition.
Cambridge: Cambridge University Press.Pinker, S. 1984.
Language Learnability and Lan-guage Development.
Cambridge, MA: HarvardUniversity Press.Pinker, S., and A.
Prince.
1988.
On language andconnectionism: Analysis of a parallel distributedprocessing model of language acquisition.
Cogni-tion 28:73-193.Rumelhart, D. E., and J. L. McClelland.
1986.On learning the past tenses of English verbs.
InJ.
L. McClelland and D. E. Rumelhart (Eds.
),Parallel Distributed Processing: Explorations inthe Mierostructure of Cognition, Vol.
2,216-271.Cambridge, MA: MIT Press.Manning 305 The segmentation problem in morphology learningmmmmmmmmmmmmmmmmmm
