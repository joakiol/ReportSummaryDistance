Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 137?142,Prague, June 2007. c?2007 Association for Computational LinguisticsAn Extensible Probabilistic Transformation-based Approachto the Third Recognizing Textual Entailment ChallengeStefan HarmelingInstitute of Adaptive and Neural ComputationSchool of Informatics, Edinburgh University, Scotlandstefan.harmeling@ed.ac.ukAbstractWe introduce a system for textual entail-ment that is based on a probabilistic modelof entailment.
The model is defined usingsome calculus of transformations on depen-dency trees, which is characterized by thefact that derivations in that calculus preservethe truth only with a certain probability.
Wealso describe a possible set of transforma-tions (and with it implicitly a calculus) thatwas successfully applied to the RTE3 chal-lenge data.
However, our system can be im-proved in many ways and we see it as thestarting point for a promising new approachto textual entailment.1 IntroductionTextual entailment recognition asks the questionwhether a piece of text likeThe Cassini Spacecraft has taken imagesfrom July 22, 2006 that show rivers andlakes present on Saturn?s moon Titan.implies a hypothesis likeThe Cassini Spacecraft reached Titan.There exists already many interesting approaches tothis problem, see (Dagan et al, 2005; Bar-Haim etal., 2006) for various recent efforts and our paperwont try to fully reinvent the wheel.
Instead it willpresent some work in progress that tries to model theprobability of entailment in terms of ideas motivatedby approaches like the edit-distance (Kouylekov andMagnini, 2005; Kouylekov and Magnini, 2006; Tatuet al, 2006; Adams, 2006).
However, instead ofdefining some distance based on edits, we will gen-erate derivations in some calculus that is able totransform dependency parse trees.
The special prop-erty of our calculus is that the truth is only preservedwith a certain probability along its derivations.
Thismight sound like a disadvantage.
However, in com-monsense reasoning there is usual a lot of uncer-tainty due the fact that it is impossible to formal-ize all world knowledge.
We think that probabili-ties might help us in such situations where it is im-possible to include everything into the model, but inwhich nonetheless we want to do reasoning.2 Main ideaFirst of all, let us assume that the text and the hy-pothesis of an textual entailment example are repre-sented as dependency trees T and H .
We would liketo formalize the probability that T entails H withsome model p?
(T |= H) parametrized by a vector?.
In order to define p?
(T |= H) we first introducethe probability of preserving truth along syntacticderivations in some calculus T `?
H which we in-formally introduce next.Suppose we are given n transformationsTF1, .
.
.
,TFn that are designed to modify depen-dency trees.
For each such transformation TFj ,the probability of preserving truth is modelled as aconstant value ?j independent of the dependencytree T it is applied to, i.e.p?
(T `TFjTFj(T )) = ?j for all T , (1)with parameter ?
being the vector of all ?j .
The idea137is that applying a transformation to T could also cre-ate a dependency tree that is sometimes not entailedby T anymore.
Consider e.g.
the transformation thatextracts an appositive and adds a new sentence forit.
Usually this is correct, but there are situations inwhich the appositive appears inside a quote, whereit might lead to a wrong conclusion.
Thus it makessense to consider probabilities to deal with imperfectcalculi.We call an n-tuple of such transformations aderivation, which we denote by ?
with `(?)
= n.Let ?j count the number of times TFj appears in?
.
Furthermore, let ?
(T ) be the result of applyingthe transformations in ?
to some dependency tree T ,e.g.
for ?
= (TF3,TF3,TF17) with `(?)
= 3 wehave ?
(T ) = TF17(TF3(TF3(T ))).Suppose that a derivation ?
= (t1, .
.
.
, t`(?))
de-rives H from T , i.e.
?
(T ) = H .
Then we define theprobability of preserving the truth along the deriva-tion ?
as the product of the preservation probabilitiesof the transformations involved1:p?
(T `?H) =`(?)?1?i=1p?
(Ti `tiTi+1) =n?j=1?
?jj (2)with T1 = T , Ti+1 = ti(Ti) and T`(?)
= H .
Notethat even though for a certain dependency tree T ap-plying different derivations ?
and ?
can result in thesame tree, i.e.
?
(T ) = ?
(T ), their probabilities ofpreserving truth can be different, since the probabil-ities depend only the transformations applied.In the previous paragraphs we have defined proba-bilities of preserving truth for all finite length deriva-tions in the calculus.
This allows us now to definethe probability of T |=H to be the maximal proba-bility over all possible derivations,p?
(T |=H) = max?
:?
(T )=Hp?
(T `?H) = max?
:?
(T )=Hn?j=1?
?jj .
(3)In the following we introduce a set of transforma-tions that is able to transform any text into any hy-pothesis and we will propose a heuristic that gener-ates such derivations.1Note, that this definition is similar to the idea of ?transitivechaining?
introduced in (Dagan and Glickman, 2004).3 Details3.1 Preprocessing and parsingFor preprocessing we apply the following steps tothe text string and the hypothesis string:(1) Remove white space, dots and quotationsmarks at beginning and end.
(2) Remove trailing points of abbreviations.
(3) Remove space between names, e.g.
?Pat Smith?becomes ?PatSmith?.
(4) Unify numbers, e.g.
resolve ?million?.
(5) Unify dates, e.g.
?5 June?
becomes ?June 5?.We then split the text string into sentences simply bysplitting at all locations containing a dot followed bya space.
The resulting strings are fed to the StanfordParser (de Marneffe et al, 2006; Klein and Man-ning, 2003) with its included pretrained model andoptions ?-retainTmpSubcategories?
and ?-splitTMP1?.
This allows us to generate dependency trees thenodes of which contain a single stemmed word, itspart-of-speech tag and its dependency tag (as pro-duced using the parser?s output options ?wordsAnd-Tags?
and ?typedDependencies?, see (de Marneffe etal., 2006)).
For the stemming we apply the function?morphy?
of the NLTK-LITE toolbox (Bird, 2005).If the text string contains more than one sentence,they will be combined to a single dependency treewith a common root node.
Let us from now on referto the dependency trees of text and hypothesis by Tand H .3.2 Generating derivationsThe heuristic described in the following generates aderivation that transforms T into H .
For brevity wewill use in the text the abbreviations of the transfor-mations as listed in Tab.
1.
(1) Resolve appositives and relative clauses.
Allderivations for some T and H start by convertingany existing appositives and relative clauses in T tonew sentences that are added to T .
For each sen-tence that was added in this step, the applied trans-formation, ATS or RTS, is appended to ?
.
(2) Calculate howH can clamp to T .
Often thereare several possibilities to assign all or some of thewords in H to words in T .
For simplicity our systemcurrently ignores certain grammatical parts which138SS substitute synonymSN substitute numberSNE substitute named entitySI substitute identitySHE substitute hypernymSHO substitute hyponymSC substitute currencySP substitute pronounGTC grammar tag changeCP change prepSA substitute antonymDOS del other sentsRUP remove unclamped partsRUN remove unclamped negsRUNO remove unclamped negs oddityMCU move clamped upRRN restructure remove nounRAN restructure add nounRRV restructure remove verbRAV restructure add verbRPD restructure pos depthRND restructure neg depthRHNC restructure h neg countRHNO restructure h neg oddityATP active to passivePTA passive to activeATS appos to sentRTS rcmod to sentTable 1: Current transformations with their abbr.are auxiliaries (?aux?
), determiners (?det?
), preposi-tions (?prep?)
and possessives (?poss?).
Furthermore,we currently ignore words of those parts of speech(POS) that are not verbs (?VB?
), nouns (?NN?
), ad-jectives (?JJ?
), adverbs (?RB?
), pronouns (?PR?
), car-dinals (?CD?)
or dollar signs (?$?).
For all otherwords wH in H and words wT in T we calculatewhether wT can be substituted by wH .
For this weemploy amongst simple heuristics also WordNet 2.1(Fellbaum, 1998) as described next:(1) Are the tokens and POS tags of wT and wHidentical?
If yes, return (1, ?identity?).
(2) If the POS tags of wT and wH indicate thatboth words appear in WordNet continue with(3) otherwise with (8).
(3) Are they antonyms in WordNet?
If yes, return(2, ?antonym?).
(4) Are they synonyms in WordNet?
If yes, return(2, ?synonym?).
(5) Does wH appear in the hypernym hierarchy ofwT in WordNet?
If yes, return (z, ?hyponym?
)with z being the distance, i.e.
wT is a hyponymof wH .
(6) Does wT appear in the hypernym hierarchy ofwH in WordNet?
If yes, return (z, ?hypernym?
)with z being the distance, i.e.wT is a hypernymof wH(7) Are they named entities that share certain partsof their strings?
If yes, return (z, ?named en-tity?)
with z being larger dependent on how dif-ferent they are.
(8) Is wT a pronoun and wH a noun?
If yes, return(2, ?pronoun?).
(9) Are wT and wH exactly matching cardinals?
Ifyes, return (1, ?number?).
(10) Are wT and wH identical currencies?
If yes,return (1, ?currency?).
(11) Are wT and wH both currencies?
If yes, return(2, ?currency?
).Note that along the hierarchy in WordNet we alsolook one step along the ?derived form?
pointer to al-low a noun like ?winner?
be substitutable by the verb?win?.
If a word wT is substitutable by a word wH ,we say that wT and wH are clamped.
We call thewhole assignment that assigns some or all words ofH to words in T a clamp.
Since usually a singleword wH is clamped to several words in T , we willoften have several different clamps.
E.g.
if H hasthree words each of which is clamped to four wordsin T there are sixty-four possible clamps in total,i.e.
sixty-four possible ways to clamp the words inH to words in T .Each of these different clamps gives rise to a dif-ferent derivation.
However, let us for simplicity con-tinue to focus on a single clamp and see how to com-plete a single derivation ?
.
(3) Substitute the clamped words.
If wH and wTare clamped, we know what their relationship is:e.g.
(3, hypernym) means that we have to go threesteps up wH ?s hypernym-hierarchy in WordNet toreach wT .
Thus we have to apply three times thetransformation SHE to substitute wT by wH , whichwe reflect in ?
by appending three times SHE to it.Similarly, we add other transformations for other re-lations.
The substitution of wT with wH might alsotrigger other transformations, such as PTA, ATP, CPand GTC which try to adjust the surrounding gram-matical structure.
All applied transformations willbe appended to the derivation ?
.139(4) Pick the sentence with the most clamps.
Af-ter substituting all clamped words, we simply pickthe sentence in T with the most clamped words anddelete the others using DOS.
E.g.
if T consists ofthree sentences, after this step T will only containthe single sentence with the most clamps and DOSwill be appended twice to ?
.
(5) Remove subtrees that do not contain clampednodes.
After this step we add for each removednode the transformation RUP to ?
.
Then we addRUN for each removed negation modifier (?neg?
)and additionally RUNO if the number of removednegation is odd.
RUNO is a somewhat artificialtransformation and acts more like a flag.
This mightbe changed in future sets of transformation to bettercomply with the transformation metaphor.
(6) Move the clamped nodes closer to the rootand remove unclamped subtree.
Again we dosome counting before and after this step, which de-termines the transformations to add to ?
.
In partic-ular we count how many verbs are passed by mov-ing clamped nodes up.
For each passed verb we addMCU to ?
.
(7) Restructure and add the missing pieces.
Thedefinition in Eq.
(3) requires that any T can be trans-formed into anyH , otherwise the maximum is unde-fined.
In the last step we will thus remove all wordsin T which are not needed for H and add all miss-ing words to T and restructure until T becomes H .For the bookkeeping we count the number of nouns,verbs and negation modifier that have to be addedand removed.
Furthermore, we count how many lev-els up or down we need to move words in T suchthat they match the structure in H .
For all thesecountings we add accordingly as many transforma-tions RRN, RRV, RAN, RAV, RPD, RND, RHNC,RHNO (see Tab.
1 for short explanations).Finally, the completed derivation ?
with ?
(T ) =H is converted to a 28-dimensional feature vector[?1, .
.
.
, ?28]> using the notion of ?j which has beendefined in Sec.
2.3.3 Estimating the parametersLet Dtr = {(T1,H1, y1), .
.
.
, (T800,H800, y800)}be the training examples with yi ?
{0, 1} indicat-ing entailment.
For brevity we definefi(?)
= p?
(Ti |=Hi) (4)to abbreviate the probability of entailment modelledas outline in Sec.
2.
Then the data likelihood can bewritten as:p?
(Dtr) =800?i=1fi(?)yi(1?fi(?
))(1?yi) (5)We would like to maximize p?
(Dtr) in term of thevector ?.
However, the maxima in Eq.
(3) makethis optimization difficult.
For the submission to theRTE3 challenge we choose the following way to ap-proximate it:(1) Generate for each example pair several deriva-tions (as described in the previous section) andchoose the eight shortest ones.
If there are lessthan eight derivations available, copy the short-est ones to end up with eight (some of whichcould be identical).
(2) There are now 8 ?
800 derivations in total.
Wedenote the corresponding feature vectors byx1, .
.
.
, x6400.
Note that xi is a vector contain-ing the countings of the different transforma-tions.
E.g.
if the corresponding derivation was?
, then xij = ?j .
(3) Similarly copy the training labels yi to matchthose 6400 feature vectors, i.e.
now our databecomes Dtr = {(x1, y1), .
.
.
, (x6400, y6400)}.
(4) Replacing fi(?)
bygi(?)
=?j?xijj (6)the data likelihood becomes:p?
(Dtr) =6400?i=1gi(?)yi(1?gi(?
))(1?yi) (7)(5) Replace furthermore each ?j by?
(zj) =11 + exp(?zj)(8)with ?
being the sigmoidal function, which en-sures that the values for ?j stay between zeroand one.
(6) Maximize pz(Dtr) in terms of z =[z1, .
.
.
, zn]> using gradient ascent.
(7) Calculate ?j = ?
(zj) for all j.1403.4 Classifying the test dataHaving estimated the parameter vector ?
we can ap-ply the trained model to the test data Dte to inferits unknown labels.
Since we only generate somederivations and can not try all possible?as wouldbe required by Eq.
(3)?we again transform the testdata into 6400 feature vectors x1, .
.
.
, x6400.
Notethat x1, .
.
.
, x8 are the feature vectors belonging tothe first test example (T1,H1), and so forth.
To ap-proximate the probability of entailment we take themaximum over the eight feature vectors assigned toeach test example, e.g.
for (T1,H1),p?
(T1 |=H1) ?
maxi?
{1,...,8}28?j=1?xijj (9)and analogously for the other test examples.
Theclass label and herewith the answer to the ques-tion whether Ti entails Hi is obtained be checkingwhether p?
(Ti |= Hi) is above a certain threshold,which we can determine using the training set.
Thiscompletes the description of the system behind ourfirst run of our RTE3 submission.3.5 Logistic RegressionThe second run of our RTE3 submission is moti-vated by the following observation: introducing aweight vector with entries wj = log ?j and usinglogarithms, we can rewrite Eq.
(3) aslog p?
(T |=H) = max?
:?
(T )=Hn?j=1?jwj .
(10)The probability of entailment becomes the maxi-mum of several linear expressions with the addi-tional constraint wj < 0 for all j which ensuresthat ?j is a probability.
In order to compare with an-other linear classifier we applied as the second runlogistic regression to the data.
Again we used eightderivations/feature vectors per training example toestimate the parameters of the logistic regression.Also with the test data we applied the weight vectorto eight derivations/feature vectors per test exampleand choose the largest result which was then thresh-old to obtain a label.4 ResultsThe first fact we see from the official RTE3 resultsin Tab.
3 is that our system is better than random.RTE 2 overall IE IR QA SUMAccTr 0.5950 0.5700 0.5850 0.5500 0.6750AccTe 0.5675 0.5000 0.5850 0.5600 0.6250AccTr 0.6050 0.5700 0.5550 0.5800 0.7150AccTe 0.5725 0.5000 0.5800 0.5800 0.6300Table 2: Results for the RTE2 data.
Shown are theaccuracies on the training and test sets.
First twolines for the first run (transformation-based model)and the next two lines for the second run (logisticregression).RTE 3 overall IE IR QA SUMAccTr 0.6475 0.5750 0.6350 0.7600 0.6200AccTe 0.5600 0.4700 0.6250 0.6450 0.5000PreTe 0.5813 0.5162 0.6214 0.6881 0.5607AccTr 0.6550 0.5600 0.6300 0.7850 0.6450AccTe 0.5775 0.5000 0.6300 0.6700 0.5100PreTe 0.5952 0.5562 0.6172 0.7003 0.5693Table 3: Official results on the RTE3 test dataand inofficial results on the corresponding trainingdata.
Shown are the accuracies and average preci-sion on the test data.
First three lines for the firstrun (transformation-based model) and the next threelines for the second run (logistic regression).However, with 56% and 57.75% it is not much bet-ter.
From the task specific data we see that it com-pletely failed on the information extraction (IE) andthe summarization (SUM) data.
On the other handit has reached good results well above 60% for theinformation retrieval (IR) and question answering(QA) data.
From the accuracies of the training datain Tab.
3 we see that there was some overfitting.We also applied our system to the RTE2 challengedata.
The results are shown in Tab.
2 and show thatour system is not yet competitive with last year?sbest systems.
It is curious that in the RTE2 data theSUM task appears simpler than the other tasks whilein this year?s data IR and QA seem to be the easiest.5 Future work and conclusionAs already mentioned, this paper presents work inprogress and we hope to improve our system inthe near future.
For the RTE3 challenge our maingoal was to get a system running before the dead-line.
However, we had to make a lot of compro-mises/simplifications to achieve that goal.Even though our current results suggest that rightnow our system might not be able to compete with141the better systems from last year?s challenge we seethe potential that our architecture provides a usefulplatform on which one can test and evolve differentsets of transformations on dependency trees.
Again,we note that such a set of transformations can beseen as a calculus that preserves truth only with acertain probability, which is an interesting conceptto follow up on.
Furthermore, this idea of a prob-abilistic calculus is not limited to dependency treesbut could equally well applied to other representa-tions of text.Besides working on more powerful and faithfultransformations, our system might be improved alsosimply by replacing our ad hoc solutions for the pre-processing and sentence-splitting.
We should alsotry different parsers and see how they compare forour purposes.
Since our approach is based on aprobabilistic model, we could also try to incorporateseveral optional parse trees (as a probabilistic parsermight be able to create) with their respective proba-bilities and create a system that uses probabilities ina consistent way all the way from tagging/parsing toinferring entailment.AcknowledgementThe author is grateful for valuable discussions withChris Williams and Amos Storkey.
This researchwas supported by the EU-PASCAL network of ex-cellence (IST- 2002-506778) and through a Euro-pean Community Marie Curie Fellowship (MEIF-CT-2005-025578).
Furthermore, the author is thank-ful to the organizers of the RTE challenges and to thecreators of WordNet, NLTK-LITE and the StanfordParser for sharing their software with the scientificcommunity.ReferencesR.
Adams.
2006.
Textual entailment through extendedlexical overlap.
In Proceedings of the Second PASCALChallenges Workshop on Recognising Textual Entail-ment.R.
Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampic-colo, B. Magnini, and I. Szpektor, editors.
2006.
Pro-ceedings of the Second PASCAL Challenges Workshopon Recognising Textual Entailment.S.
Bird.
2005.
NLTK-Lite: Efficient scripting for naturallanguage processing.
In 4th International Conferenceon Natural Language Processing, pages 1?8.I.
Dagan and O. Glickman.
2004.
Probabilistic tex-tual entailment: Generic applied modeling of languagevariability.
In PASCAL Workshop on Learning Meth-ods for Text Understanding and Mining, Grenoble.I.
Dagan, O. Glickman, and B. Magnini, editors.
2005.Proceedings of the PASCAL Challenges Workshop onRecognising Textual Entailment.M.-C. de Marneffe, B. MacCartney, and C.D.
Man-ning.
2006.
Generating typed dependency parses fromphrase structure parses.
In International Conferenceon Language Resources and Evaluation (LREC).C.
Fellbaum.
1998.
WordNet: an electronic lexicaldatabase.
MIT Press.D.
Klein and C.D.
Manning.
2003.
Accurate unlexical-ized parsing.
In Proceedings of the 41st Meeting of theAssociation for Computational Linguistics.M.
Kouylekov and B. Magnini.
2005.
Recognizing tex-tual entailment with tree edit distance algorithms.
InProceedings of the PASCAL Challenges Workshop onRecognising Textual Entailment.M.
Kouylekov and B. Magnini.
2006.
Tree edit distancefor recognizing textual entailment: Estimating the costof insertion.
In Proceedings of the Second PASCALChallenges Workshop on Recognising Textual Entail-ment.M.
Tatu, B. Iles, J. Slavick, A. Novischi, andD.
Moldovan.
2006.
COGEX at the second recog-nizing textual entailment challenge.
In Proceedings ofthe Second PASCAL Challenges Workshop on Recog-nising Textual Entailment.142
