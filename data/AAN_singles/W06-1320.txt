Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 144?151,Sydney, July 2006. c?2006 Association for Computational LinguisticsAn Analysis of Quantitative Aspects in the Evaluation of ThematicSegmentation AlgorithmsMaria GeorgesculISSCO/TIM, ETIUniversity of Geneva1211 Geneva, Switzerlandmaria.georgescul@eti.unige.chAlexander ClarkDepartment of Computer ScienceRoyal Holloway University of LondonEgham, Surrey TW20 0EX, UKalexc@cs.rhul.ac.ukSusan ArmstrongISSCO/TIM, ETIUniversity of Geneva1211 Geneva, Switzerlandsusan.armstrong@issco.unige.chAbstractWe consider here the task of linear the-matic segmentation of text documents, byusing features based on word distributionsin the text.
For this task, a typical and of-ten implicit assumption in previous stud-ies is that a document has just one topicand therefore many algorithms have beentested and have shown encouraging resultson artificial data sets, generated by puttingtogether parts of different documents.
Weshow that evaluation on synthetic data ispotentially misleading and fails to give anaccurate evaluation of the performance onreal data.
Moreover, we provide a criti-cal review of existing evaluation metrics inthe literature and we propose an improvedevaluation metric.1 IntroductionThe goal of thematic segmentation is to iden-tify boundaries of topically coherent segmentsin text documents.
Giving a rigorous definitionof the notion of topic is difficult, but the taskof discourse/dialogue segmentation into thematicepisodes is usually described by invoking an ?in-tuitive notion of topic?
(Brown and Yule, 1998).Thematic segmentation also relates to several no-tions such as speaker?s intention, topic flow andcohesion.Since it is elusive what mental representationshumans use in order to distinguish a coherenttext, different surface markers (Hirschberg andNakatani, 1996; Passonneau and Litman, 1997)and external knowledge sources (Kozima and Fu-rugori, 1994) have been exploited for the purposeof automatic thematic segmentation.
Halliday andHasan (1976) claim that the text meaning is re-alised through certain language resources and theyrefer to these resources by the term of cohesion.The major classes of such text-forming resourcesidentified in (Halliday and Hasan, 1976) are: sub-stitution, ellipsis, conjunction, reiteration and col-location.
In this paper, we examine one form oflexical cohesion, namely lexical reiteration.Following some of the most prominent dis-course theories in literature (Grosz and Sidner,1986; Marcu, 2000), a hierarchical representationof the thematic episodes can be proposed.
Thebasis for this is the idea that topics can be re-cursively divided into subtopics.
Real texts ex-hibit a more intricate structure, including ?seman-tic returns?
by which a topic is suspended at onepoint and resumed later in the discourse.
However,we focus here on a reduced segmentation prob-lem, which involves identifying non-overlappingand non-hierarchical segments at a coarse level ofgranularity.Thematic segmentation is a valuable initialtool in information retrieval and natural languageprocessing.
For instance, in information ac-cess systems, smaller and coherent passage re-trieval is more convenient to the user than whole-document retrieval and thematic segmentation hasbeen shown to improve the passage-retrieval per-formance (Hearst and Plaunt, 1993).
In cases suchas collections of transcripts there are no headersor paragraph markers.
Therefore a clear separa-tion of the text into thematic episodes can be usedtogether with highlighted keywords as a kind of?quick read guide?
to help users to quickly navi-gate through and understand the text.
Moreoverautomatic thematic segmentation has been shownto play an important role in automatic summariza-tion (Mani, 2001), anaphora resolution and dis-144course/dialogue understanding.In this paper, we concern ourselves with the taskof linear thematic segmentation and are interestedin finding out whether different segmentation sys-tems can perform well on artificial and real datasets without specific parameter tuning.
In addi-tion, we will refer to the implications of the choiceof a particular error metric for evaluation results.This paper is organized as follows.
Section 2and Section 3 describe various systems and, re-spectively, different input data selected for ourevaluation.
Section 4 presents several existingevaluation metrics and their weaknesses, as wellas a new evaluation metric that we propose.
Sec-tion 5 presents our experimental set-up and showscomparisons between the performance of differentsystems.
Finally, some conclusions are drawn inSection 6.2 Comparison of SystemsCombinations of different features (derived for ex-ample from linguistic, prosodic information) havebeen explored in previous studies like (Galley etal., 2003) and (Kauchak and Chen, 2005).
Inthis paper, we selected for comparison three sys-tems based merely on the lexical reiteration fea-ture: TextTiling (Hearst, 1997), C99 (Choi, 2000)and TextSeg (Utiyama and Isahara, 2001).
In thefollowing, we briefly review these approaches.2.1 TextTiling AlgorithmThe TextTiling algorithm was initially developedby Hearst (1997) for segmentation of exposi-tory texts into multi-paragraph thematic episodeshaving a linear, non-overlapping structure (as re-flected by the name of the algorithm).
TextTilingis widely used as a de-facto standard in the eval-uation of alternative segmentation systems, e.g.
(Reynar, 1998; Ferret, 2002; Galley et al, 2003).The algorithm can briefly be described by the fol-lowing steps.Step 1 includes stop-word removal, lemmatiza-tion and division of the text into ?token-sequences?(i.e.
text blocks having a fixed number of words).Step 2 determines a score for each gap betweentwo consecutive token-sequences, by computingthe cosine similarity (Manning and Schu?tze, 1999)between the two vectors representing the frequen-cies of the words in the two blocks.Step 3 computes a ?depth score?
for each token-sequence gap, based on the local minima of thescore computed in step 2.Step 4 consists in smoothing the scores.Step 5 chooses from any potential boundariesthose that have the scores smaller than a certain?cutoff function?, based on the average and stan-dard deviation of score distribution.2.2 C99 AlgorithmThe C99 algorithm (Choi, 2000) makes a linearsegmentation based on a divisive clustering strat-egy and the cosine similarity measure between anytwo minimal units.
More exactly, the algorithmconsists of the following steps.Step 1: after the division of the text into min-imal units (in our experiments, the minimal unitis an utterance1), stop words are removed and astemmer is applied.The second step consists of constructing a sim-ilarity matrix Sm?m, where m is the number ofutterances and an element sij of the matrix corre-sponds to the cosine similarity between the vectorsrepresenting the frequencies of the words in the i-th utterance and the j-th utterance.Step 3: a ?rank matrix?
Rm?m is computed, bydetermining for each pair of utterances, the num-ber of neighbors in Sm?m with a lower similarityvalue.In the final step, the location of thematic bound-aries is determined by a divisive top-down cluster-ing procedure.
The criterion for division of thecurrent segment B into b1, ...bm subsegments isbased on the maximisation of a ?density?
D, com-puted for each potential repartition of boundariesasD =?mk=1 sumk?mk=1 areak,where sumk and areak refers to the sum of rankand area of the k-th segment in B, respectively.2.3 TextSeg AlgorithmThe TextSeg algorithm (Utiyama and Isahara,2001) implements a probabilistic approach to de-termine the most likely segmentation, as brieflydescribed below.The segmentation task is modeled as a problemof finding the minimum cost C(S) of a segmenta-tion S. The segmentation cost is defined as:C(S) ?
?logPr(W|S)Pr(S),1Occasionally within this document we employ the termutterance to denote either a sentence or an utterance in itsproper sense.145where W = w1w2...wn represents the text con-sisting of n words (after applying stop-words re-moval and stemming) and S = S1S2...Sm is a po-tential segmentation of W in m segments.
Theprobability Pr(W|S) is defined using Laplacelaw, while the definition of the probability Pr(S)is chosen in a manner inspired by information the-ory.A directed graph G is defined such that a pathin G corresponds to a possible segmentation ofW .
Therefore, the thematic segmentation pro-posed by the system is obtained by applying a dy-namic programming algorithm for determining theminimum cost path in G.3 Input DataWhen evaluating a thematic segmentation systemfor an application, human annotators should pro-vide the gold standard.
The problem is that theprocedure of building such a reference corpus isexpensive.
That is, the typical setting involves anexperiment with several human subjects, who areasked to mark thematic segment boundaries basedon specific guidelines and their intuition.
Theinter-annotator agreement provides the referencesegmentation.
This expense can be avoided byconstructing a synthetic reference corpus by con-catenation of segments from different documents.Therefore, the use of artificial data for evaluationis a general trend in many studies, e.g.
(Ferret,2002; Choi, 2000; Utiyama and Isahara, 2001).In our experiment, we used artificial and realdata, i.e.
the algorithms have been tested on thefollowing data sets containing English texts.3.1 Artificially Generated DataChoi (2000) designed an artificial dataset, built byconcatenating short pieces of texts that have beenextracted from the Brown corpus.
Any test samplefrom this dataset consists of ten segments.
Eachsegment contains the first n sentences (where 3 ?n ?
11) of a randomly selected document fromthe Brown corpus.
From this dataset, we randomlychose for our evaluation 100 test samples, wherethe length of a segment varied between 3 and 11sentences.3.2 TDT DataOne of the commonly used data sets for topic seg-mentation emerged from the Topic Detection andTracking (TDT) project, which includes the taskof story segmentation, i.e.
the task of segmentinga stream of news data into topically cohesive sto-ries.
As part of the TDT initiative several datasetsof news stories have been created.
In our evalua-tion, we used a subset of 28 documents randomlyselected from the TDT Phase 2 (TDT2) collection,where a document contains an average of 24.67segments.3.3 Meeting TranscriptsThe third dataset used in our evaluation contains25 meeting transcripts from the ICSI-MR corpus(Janin et al, 2004).
The entire corpus containshigh-quality close talking microphone recordingsof multi-party dialogues.
Transcriptions at wordlevel with utterance-level segmentations are alsoavailable.
The gold standard for thematic segmen-tations has been kindly provided by (Galley etal., 2003) and has been chosen by considering theagreement between at least three human annota-tions.
Each meeting is thus divided into contigu-ous major topic segments and contains an averageof 7.32 segments.Note that thematic segmentation of meetingdata is a more challenging task as the thematictransitions are subtler than those in TDT data.4 Evaluation MetricsIn this section, we will look in detail at the errormetrics that have been proposed in previous stud-ies and examine their inadequacies.
In addition,we propose a new evaluation metric that we con-sider more appropriate.4.1 Pk Metric(Passonneau and Litman, 1996; Beeferman et al,1999) underlined that the standard evaluation met-rics of precision and recall are inadequate for the-matic segmentation, namely by the fact that thesemetrics did not account for how far away is a hy-pothesized boundary (i.e.
a boundary found bythe automatic procedure) from a reference bound-ary (i.e.
a boundary found in the reference data).On the other hand, it is desirable that an algorithmthat places for instance a boundary just one utter-ance away from the reference boundary to be pe-nalized less than an algorithm that places a bound-ary two (or more) utterances away from the ref-erence boundary.
Hence (Beeferman et al, 1999)proposed a new metric, called PD, that allows fora slight vagueness in where boundaries lie.
More146specifically, (Beeferman et al, 1999) define PDas follows2:PD(ref, hyp) =?1?i?j?N D(i, j)[?ref (i, j) ?
?hyp(i, j)].N is the number of words in the reference data.The function ?ref (i, j) is evaluated to one if thetwo reference corpus indices specified by its pa-rameters i and j belong in the same segment, andzero otherwise.
Similarly, the function ?hyp(i, j)is evaluated to one, if the two indices are hypothe-sized by the automatic procedure to belong in thesame segment, and zero otherwise.
The ?
opera-tor is the XNOR function ?both or neither?.
D(i, j)is a ?distance probability distribution over the setof possible distances between sentences chosenrandomly from the corpus?.
In practice, a distri-bution D having ?all its probability mass at a fixeddistance k?
(Beeferman et al, 1999) was adoptedand the metric PD was thus renamed Pk.In the framework of the TDT initiative, (Allanet al, 1998) give the following formal definitionof Pk and its components:Pk = PMiss ?
Pseg + PFalseAlarm ?
(1?
Pseg),where:PMiss =PN?ki=1 [?hyp(i,i+k)]?[1?
?ref (i,i+k)]PN?ki=1 [1?
?ref (i,i+k)],PFalseAlarm =PN?ki=1 [1??hyp(i,i+k)]?
[?ref (i,i+k)]PN?ki=1 ?ref (i,i+k),and Pseg is the a priori probability that inthe reference data a boundary occurs within aninterval of k words.
Therefore Pk is calculated bymoving a window of a certain width k, where k isusually set to half of the average number of wordsper segment in the gold standard.Pevzner and Hearst (2002) highlighted severalproblems of the Pk metric.
We illustrate belowwhat we consider the main problems of the Pkmetric, based on two examples.Let r(i, k) be the number of boundaries be-tween positions i and i + k in the gold standardsegmentation and h(i, k) be the number of bound-aries between positions i and i+k in the automatichypothesized segmentation.?
Example 1: If r(i, k) = 2 and h(i, k) = 1then obviously a missing boundary should2Let ref be a correct segmentation and hyp be a segmen-tation proposed by a text segmentation system.
We will keepthis notations in equations introduced below.be counted in Pk, i.e.
PMiss should be in-creased.?
Example 2: If r(i, k) = 1 and h(i, k) =2 then obviously PFalseAlarm should be in-creased.However, considering the first example, we willobtain ?ref (i, i + k) = 0, ?hyp(i, i + k) = 0and consequently PMiss is not increased.
By tak-ing the case from the second example we obtain?ref (i, i + k) = 0 and ?hyp(i, i + k) = 0, involv-ing no increase of PFalseAlarm.In (TDT, 1998), a slightly different defini-tion is given for the Pk metric: the definition ofmiss and false alarm probabilities is replaced with:P ?Miss =PN?ki=1 [1??hyp(i,i+k)]?[1?
?ref (i,i+k)]PN?ki=1 [1?
?ref (i,i+k)],P ?FalseAlarm =PN?ki=1 [1??hyp(i,i+k)]?
[?ref (i,i+k)]PN?ki=1 ?ref (i,i+k),where:?hyp(i, i+ k) ={1, if r(i, k) = h(i, k),0, otherwise.We will refer to this new definition of Pk byP ?k.
Therefore, by taking the definition ofP ?k and the first example above, we obtain?ref (i, i+ k) = 0 and ?hyp(i, i+ k) = 0 and thusP ?Miss is correctly increased.
However for the caseof example 2 we will obtain ?ref (i, i + k) = 0and ?hyp(i, i + k) = 0, involving no increase ofP ?FalseAlarm and erroneous increase of P ?Miss.4.2 WindowDiff metricPevzner and Hearst (2002) propose the alternativemetric called WindowDiff.
By keeping our nota-tions concerning r(i, k) and h(i, k) introduced inthe subsection 4.1, WindowDiff is defined as:WindowDiff =PN?ki=1 [|r(i,k)?
h(i,k)|>0]N?k .Similar to both Pk and P ?k, WindowDiff isalso computed by moving a window of fixed sizeacross the test set and penalizing the algorithmmisses or erroneous algorithm boundary detec-tions.
However, unlike Pk and P ?k, WindowDifftakes into account how many boundaries fallwithin the window and is penalizing in ?howmany discrepancies occur between the referenceand the system results?
rather than ?determininghow often two units of text are incorrectly labeled147as being in different segments?
(Pevzner andHearst, 2002).Our critique concerning WindowDiff is thatmisses are less penalised than false alarms andwe argue this as follows.
WindowDiff can berewritten as:WindowDiff = WDMiss +WDFalseAlarm,where:WDMiss =PN?ki=1 [r(i,k)>h(i,k)]N?k ,WDFalseAlarm =PN?ki=1 [r(i,k)<h(i,k)]N?k .Hence both misses and false alarms are weightedby 1N?k .Note that, on the one hand, there are indeed (N-k) equiprobable possibilities to have a false alarmin an interval of k units.
On the other hand, how-ever, the total number of equiprobable possibil-ities to have a miss in an interval of k units issmaller than (N-k) since it depends on the num-ber of reference boundaries (i.e.
we can have amiss in the interval of k units only if in that intervalthe reference corpus contains at least one bound-ary).
Therefore misses, being weighted by 1N?k ,are less penalised than false alarms.Let Bref be the number of thematic boundariesin the reference data.
Let?s say that the refer-ence data contains about 20% boundaries and 80%non-boundaries from the total number of potentialboundaries.
Therefore, since there are relativelyfew boundaries compared with non-boundaries, astrategy introducing no false alarms, but introduc-ing a maximum number of misses (i.e.
k ?
Brefmisses) can be judged as being around 80% cor-rect by the WindowDiff measure.
On the otherhand, a segmentation with no misses, but with amaximum number of false alarms (i.e.
(N ?
k)false alarms) is judged as being 100% erroneousby the WindowDiff measure.
That is, misses andfalse alarms are not equally penalised.Another issue regarding WindowDiff is that it isnot clear ?how does one interpret the values pro-duced by the metric?
(Pevzner and Hearst, 2002).4.3 Proposal for a New MetricIn order to address the inadequacies of Pk andWindowDiff, we propose a new evaluation metric,defined as follows:Prerror = Cmiss ?
Prmiss + Cfa ?
Prfa,where:Cmiss (0 ?
Cmiss ?
1) is the cost of a miss, Cfa(0 ?
Cfa ?
1) is the cost of a false alarm,Prmiss =PN?ki=1 [?ref hyp(i,k)]PN?ki=1 [?ref (i,k)],P rfa =PN?ki=1 [?ref hyp(i,k)]N?k ,?ref hyp(i, k) ={1, if r(i, k) > h(i, k)0, otherwise?ref hyp(i, k) ={1, if r(i, k) < h(i, k)0, otherwise.
?ref (i, k) ={1, if r(i, k) > 00, otherwise.Prmiss could be interpreted as the probabilitythat the hypothesized segmentation contains lessboundaries than the reference segmentation in aninterval of k units3, conditioned by the fact thatthe reference segmentation contains at least oneboundary in that interval.
Analogously Prfa isthe probability that the hypothesized segmentationcontains more boundaries than the reference seg-mentation in an interval of k units.For certain applications where misses are moreimportant than false alarms or vice versa, thePrerror can be adjusted to tackle this trade-off viathe Cfa and Cmiss parameters.
In order to havePrerror ?
[0, 1], we suggest that Cfa and Cmissbe chosen such that Cfa + Cmiss = 1.
By choos-ing Cfa=Cmiss=12 , the penalization of misses andfalse alarms is thus balanced.
In consequence, astrategy that places no boundaries at all is penal-ized as much as a strategy proposing boundarieseverywhere (i.e.
after every unit).
In other words,both such degenerate algorithms will have an errorrate Prerror of about 50%.
The worst algorithm,penalised as having an error rate Prerror of 100%when k = 2, is the algorithm that places bound-aries everywhere except the places where refer-ence boundaries exist.5 Results5.1 Test ProcedureFor the three datasets we first performed twocommon preprocessing steps: common words areeliminated using the same stop-list and remainingwords are stemmed by using Porter?s algorithm(1980).
Next, we ran the three segmenters de-scribed in Section 2, by employing the default val-ues for any system parameters and by letting the3A unit can be either a word or a sentence / an utterance.148systems estimate the number of thematic bound-aries.We also considered the fact that C99 andTextSeg algorithms can take into account a fixednumber of thematic boundaries.
Even if the num-ber of segments per document can vary in TDTand meeting reference data, we consider that in areal application it is impossible to provide to thesystems the exact number of boundaries for eachdocument to be segmented.
Therefore, we ran C99and TextSeg algorithms (for a second time), byproviding them only the average number of seg-ments per document in the reference data, whichgives an estimation of the expected level of seg-mentation granularity.Four additional naive segmentations were alsoused for evaluation, namely: no boundaries,where the whole text is a single segment; allboundaries, i.e.
a thematic boundary is placed af-ter each utterance; random known, i.e.
the samenumber of boundaries as in gold standard, distrib-uted randomly throughout text; and random un-known: the number of boundaries is randomlyselected and boundaries are randomly distributedthroughout text.
Each of the segmentations wasevaluated with Pk, P ?k and WindowDiff, as de-scribed in Section 4.5.2 Comparative Performance ofSegmentation SystemsThe results of applying each segmentation algo-rithm to the three distinct datasets are summa-rized in Figures 1, 2 and 3.
Percent error valuesare given in the figures and we used the follow-ing abbreviations: WD to denote WindowDiff er-ror metric; TextSeg KA to denote the TextSeg algo-rithm (Utiyama and Isahara, 2001) when the av-erage number of boundaries in the reference datawas provided to the algorithm; C99 KA to denotethe C99 algorithm (Choi, 2000) when the aver-age number of boundaries in the reference datawas provided to the algorithm; N0 to denote the al-gorithm proposing a segmentation with no bound-aries; All to denote the algorithm proposing the de-generate segmentation all boundaries; RK to de-note the algorithm that generates a random knownsegmentation; and RU to denote the algorithm thatgenerates a random unknown segmentation.5.2.1 Comparison of System Performancefrom Artificial to Realistic DataFrom the artificial data to the more realisticdata, we expect to have more noise and thus thealgorithms to constantly degrade, but as our ex-periments show a reversal of the assessment canappear.
More exactly: as can be seen from Figure1, both C99 and TextSeg algorithms significantlyoutperformed TextTiling algorithm on the artifi-cially created dataset, when the number of seg-ments was determined by the systems.
A com-parison between the error rates given in Figure1 and Figure 2 show that C99 and TextSeg havea similar trend, by significantly decreasing theirperformance on TDT data, but still giving bet-ter results than TextTiling on TDT data.
Whencomparing the systems by Prerror, C99 has simi-lar performance with TextTiling on meeting data(see Figure 3).
Moreover, when assessment isdone by using WindowDiff, Pk or P ?k, both C99and TextSeg came out worse than TextTiling onmeeting data.
This demonstrates that rankings ob-tained when evaluating on artificial data are dif-ferent from those obtained when evaluating on re-alistic data.
An alternative interpretation can begiven by taking into account that the degenerativeno boundaries segmentation has an error rate ofonly 30% by the WindowDiff, Pk and P ?k metricson meeting data.
That is, we could interpret thatall three systems give completely wrong segmen-tations on meeting data (due to the fact that topicshifts are subtler and not as abrupt as in TDT andartificial data).
Nevertheless, we tend to adopt thefirst interpretation, given the weaknesses of Pk, P ?kand WindowDiff (where misses are less penalisedthan false alarms), as discussed in Section 4.5.2.2 The Influence of the Error Metric onAssessmentBy following the quantitative assessment givenby the WindowDiff metric, we observe that thealgorithm labeled N0 is three times better thanthe algorithm All on meeting data (see Figure 3),while the same algorithm N0 is considered onlytwo times better than All on the artificial data (seeFigure 1).
This verifies the limitation of the Win-dowDiff metric discussed in Section 4.The four error metrics described in detail inSection 4 have shown that the effect of knowingthe average number of boundaries on C99 is posi-tive when testing on meeting data.
However if wewant to take into account all the four error met-149020406080100120Error ratePk 34.75 11.01 7.89 10 7.15 44.12 55.5 47.71 52.51P'k 35.1 13.21 8.55 10.94 7.87 44.13 99.58 48.85 80.84WD 35.73 13.58 9.21 11.34 8.59 43.1 99.59 48.89 80.63Pr_error 33.33 9.1 7.71 9.34 6.87 49.87 49.79 41.61 45.01TextTiling C99 TextSeg C99_KA TextSeg_KA N0 All RK RUFigure 1: Error rates of the segmentation systems on artificial data, where k = 42 and Pseg = 0.44.020406080100120ErrorratePk 40.7 21.36 13.97 18.83 11.33 36.02 63.93 37.03 60.04P'k 44.92 29.5 20.37 27.69 21.4 36.04 100 45.28 89.93WD 44.76 36.28 30.3 40.26 31.46 46.69 100 53.75 91.92Pr_error 34.09 25.69 25.62 27.17 21.05 49.96 50 44.89 48.31TextTiling C99 TextSeg C99_KA TextSeg_KA N0 All RK RUFigure 2: Error rates of the segmentation systems on TDT data, where k = 55 and Pseg = 0.3606.rics, it is difficult to draw definite conclusions re-garding the influence of knowing the average num-ber of boundaries on TextSeg and C99 algorithms.For example, when tested on TDT data, C99 KAseems to work better than C99 by Pk and P ?k met-rics, while the WindowDiff metric gives a contra-dictory assessment.6 ConclusionsBy comparing the performance of three systemsfor thematic segmentation on different kinds ofdata, we address two important issues in a quan-titative evaluation.
Strong emphasis was put onthe kind of data used for evaluation and we havedemonstrated experimentally that evaluation onsynthetic data is potentially misleading.
The sec-ond major issue addressed in this paper concernsthe choice of a valuable error metric and its sideeffects on the evaluation assessment.AcknowledgmentsThis work is supported by the InteractiveMultimodal Information Management project(http://www.im2.ch/).
Many thanks to AndreiPopescu-Belis and the anonymous reviewers fortheir valuable comments.
We are grateful to theInternational Computer Science Institute (ICSI),University of California for sharing the data withus.
We also wish to thank Michael Galley whokindly provided us the thematic annotations ofICSI data.ReferencesJames Allan, Jaime Carbonell, George Doddington,Jonathan Yamron, and Yiming Yang.
1998.
TopicDetection and Tracking Pilot Study: Final Re-port.
In DARPA Broadcast News Transcription andUnderstanding Workshop, pages 194?218, Lands-downe, VA. Morgan Kaufmann.Doug Beeferman, Adam Berger, and John Lafferty.1999.
Statistical Models for Text Segmentation.Machine Learning, 34(Special Issue on Natural Lan-guage Learning):177?210.Gillian Brown and George Yule.
1998.
DiscourseAnalysis.
(Cambridge Textbooks in Linguistics),Cambridge.Freddy Choi.
2000.
Advances in Domain IndependentLinear Text Segmentation.
In Proceedings of the 1st150020406080100120ErrorrateP_k 38.22 54.62 40.82 35.65 35.94 30.82 69.09 45.42 68.48P'_k 39.12 66.78 45.66 39.04 39.6 30.89 100 47.97 95.99WD 40.82 69.41 49.27 41.98 42.48 29.31 100 49.64 95.48Pr_error 40.17 40.27 35.45 35.83 36.61 49.8 50 50.7 53.38TextTiling C99 TextSeg C99_KA TextSeg_KA N0 All RK RUFigure 3: Error rates of the segmentation systems on meeting data, where k = 85 and Pseg = 0.3090.Conference of the North American Chapter of theAssociation for Computational Linguistics, Seattle,USA.Olivier Ferret.
2002.
Using Collocations for TopicSegmentation and Link Detection.
In The 19th In-ternational Conference on Computational Linguis-tics, Taipei, Taiwan.Michael Galley, Kathleen McKeown, Eric Fosler-Luissier, and Hongyan Jing.
2003.
Discourse Seg-mentation of Multy-Party Conversation.
In AnnualMeeting of the Association for Computational Lin-guistics, pages 562?569.Barbara J. Grosz and Candace L. Sidner.
1986.
At-tention, Intentions and the Structure of Discourse.Computational Linguistics, 12:175?204.Michael A. K. Halliday and Ruqaiya Hasan.
1976.
Co-hesion in English.
Longman, London.Marti Hearst and Christian Plaunt.
1993.
SubtopicStructuring for Full-Length Document Access.In Proceedings of the 16th Annual InternationalACM/SIGIR Conference, pages 59?68, Pittsburgh,Pennsylvania, United States.Marti Hearst.
1997.
TextTiling: Segmenting Text intoMulti-Paragraph Subtopic Passages.
ComputationalLinguistics, 23(1):33?64.Julia Hirschberg and Christine Nakatani.
1996.A Prosodic Analysis of Discourse Segments inDirection-Giving Monologues.
In Proceedings ofthe 34th Annual Meeting on Association for Com-putational Linguistics, pages 286 ?
293, Santa Cruz,California.Adam Janin, Jeremy Ang, Sonali Bhagat, RajdipDhillon, Jane Edwards, Javier Macias-Guarasa, Nel-son Morgan, Barbara Peskin, Elizabeth Shriberg,Andreas Stolcke, Chuck Wooters, and Britta Wrede.2004.
The ICSI Meeting Project: Resources and Re-search.
In ICASSP 2004 Meeting Recognition Work-shop (NIST RT-04 Spring Recognition Evaluation),Montreal.David Kauchak and Francine Chen.
2005.
Feature-based segmentation of narrative documents.
In Pro-ceedings of the ACL Workshop on Feature Engi-neering for Machine Learning in Natural LanguageProcessing, pages 32?39, Ann Arbor; MI; USA.Hideki Kozima and Teiji Furugori.
1994.
SegmentingNarrative Text into Coherent Scenes.
Literary andLinguistic Computing, 9:13?19.Inderjeet Mani.
2001.
Automatic Summarization.John Benjamins Pub Co.Chris Manning and Hinrich Schu?tze.
1999.
Foun-dations of Statistical Natural Language Processing.MIT Press Cambridge, MA, USA.Daniel Marcu.
2000.
The Theory and Practice ofDiscourse Parsing and Summarization.
MIT PressCambridge, MA, USA.Rebecca J. Passonneau and Diane J. Litman.
1996.Empirical Analysis of Three Dimensions of SpokenDiscourse: Segmentation, Coherence and LinguisticDevices.Rebecca J. Passonneau and Diane J. Litman.
1997.Discourse Segmentation by Human and AutomatedMeans.
Computational Linguistics, 23(1).Lev Pevzner and Marti Hearst.
2002.
A Critique andImprovement of an Evaluation Metric for Text Seg-mentation.
Computational Linguistics, 16(1):19?36.Martin Porter.
1980.
An Algorithm for Suffix Strip-ping.
Program, 14:130 ?
137.Jeffrey Reynar.
1998.
Topic Segmentation: Algorithmsand Applications.
Ph.D. thesis, University of Penn-sylvania.TDT.
1998.
The Topic Detection and Tracking - Phase2 Evaluation Plan.
Available from World Wide Web:http://www.nist.gov/speech/tests/tdt/tdt98/index.htm.Masao Utiyama and Hitoshi Isahara.
2001.
A Statisti-cal Model for Domain-Independent Text Segmenta-tion.
In ACL/EACL, pages 491?498.151
