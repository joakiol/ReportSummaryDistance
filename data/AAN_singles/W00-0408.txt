A Comparison of Rankings Produced by SummarizationEvaluation MeasuresRober t  L. DonawayDepartment  of Defense9800 Savage Rd.
STE 6409Ft.
Meade, MD 20755-6409rldonaw@super, o gKev in  W.  DrummeyDepartment  of Defense9800 Savage Rd.
STE 6341Ft.
Meade, MD 20755-6341kwdrumm @super.
orgLaura  A.  MatherLa Jolla Research LabBritannica.com, Inc.3253 Holiday Ct. Suite 208La Jolla, CA 92037mather@us, britannica, cornAbst rac tSummary evaluation measures produce a rankingof all possible extract summaries of a document.,Recall-based evaluation measures, which depend oncostly human-generated ground truth summaries,produce uncorrelated rankings when ground truthis varied.
This paper proposes using sentence-rank-based and content-based measures for evaluating ex-tract summaries, and compares these with recall-based evaluation measures.
Content-based measuresincrease the correlation of rankings induced by syn-onymous ground truths, and exhibit other desirableproperties.1 In t roduct ionThe bulk of active research in the automatictext summarization community centers on de-veloping algorithms to produce extract sum-maries, e. g. (Schwarz, 1990), (Paice and Jones,.1993), (Kupiec et al, 1995), (Marcu, 1997),(Strzalkowski et al, 1998), and (Goldstein et:al., 1999).
Yet understanding how to evalu-ate their output has received less attention.
In.?
1997, T IPSTER sponsored a conference (SUM-MAC) where various text summarization algo-rithms were evaluated for their performance invarious tasks (Mani et al, 1999; Firmin andChrzanowski, 1999).
While extrinsic evalua-tion measures such as these are often very Con-crete, the act of designing the task and scor-ing the results of the task introduces bias andsubject-based variability.
These factors mayconfound the comparison of summarization al-gorithms.
Machine-generated summaries alsomay be evaluated intrinsically by comparingthem with "ideal" human-generated summaries.However, there is often little agreement as towhat constitutes the ideal summary of a docu-ment.Both intrinsic and extrinsic methods requiretime consuming, expert human input in orderto evaluate summaries.
While the traditionalmethods have many advantages, they are costly,and human assessors cannot always agree onsummary quality.
If a numerical measure wereavailable which did not depend on human judge-ment, researchers and developers would be ableto immediately assess the effect of modificationsto summary generation algorithms?
Also, sucha measure might be free of the bias that is in-troduced by human assessment.This paper investigates the properties of vari-ous numerical measures for evaluating the qual-ity of generic, indicative document summaries.As explained by Mani et al (1999), a genericsummary is not topic-related, but "is aimed ata broad readership community" and an indica-tive summary tells "what topics are addressedin the source text, and thus can be used toalert the user as to source content."
Section 2discusses the properties of numerical evaluationmeasures, points out several drawbacks associ-ated with intrinsic measures and introduces newmeasures developed by the authors.
An exper-iment was devised to compare the new evalua-tion measures with the traditional ones.
The de-sign of this experiment is discussed in Section 3and its results are presented in Section 4.
Thefinal section includes conclusions and a state-ment of the future work related to these evalu-ation measures.2 Eva luat ion  MeasuresAn evaluation measure produces a numericalscore which can be used to compare differentsummaries of the same document.
The scoresare used to assess ummary quality across a col-lection of test documents in order to producean average for an algorithm or system.
How-ever, it must be emphasized that the scores are{}9most significant when considered per document.For example, two different summaries of a doc-ument may have been produced by two differ-ent summarization algorithms.
Presumably, thesummary with the higher score indicates thatthe system which produced it performed bet-ter than the other system.
Obviously, if onesystem consistently produces higher scores thananother system, its average score will be higher,and one has reason to believe that it is a bet-ter system.
Thus, the important feature of anysummary evaluation measure is not the value ofits score, but rather the ranking its score im-poses on a set of extracts of a document.To compare two evaluation measures, whosescores may have very different ranges and distri-butions, one must compare the order in whichthe measures rank various summaries of a docu-ment.
For instance, suppose a summary scoringfunction Y is completely dependent upon theoutput of another scoring function X, such asY -- 2 X.
Since Y is an increasing function of X,both X and Y will produce the same ranking ofany set of summaries.
However, the scores pro-duced by Y will have a very different distribu-tion than those of X and the two sets of scoreswill not be correlated since the dependence of Yon X is non-linear.
Therefore, in order to com-pare the scores two different measures assign toa set of summaries, one must compare the ranks.
they assign, not the actual scores.The ranks assigned by an evaluation mea-sure  produce equivalence classes of extractsummaries; each rank equivalence class con-tains summaries which received the same score.When a measure produces the same score fortwo different summaries of a document, here isa tie, and the equivalence class will contain morethan one summary.
All summaries in an equiv-alence class must share the same rank; let thisrank be the midrank of the range of ranks thatwould have be assigned if each score were dis-tinct.
An evaluation measure should posses thefollowing properties: (i) higher-ranking sum-maries are more effective or are of higher qualitythan lower-ranking summaries, and (ii) all of thesummaries in a rank equivalence class are more-or-less equally effective.The following sections contrast he rankingproperties of three types of evaluation measures:recall-based measures, a sentence-rank-basedmeasure and content-based measures.
Thesetypes of measures are defined, their propertiesare described and their use is explained.2.1 Reca l l -Based  Eva luat ion  MeasuresRecall-based evaluation measures are intrin-sic.
They compare machine-generated sum-maries with sentences previously extracted byhuman assessors or judges.
From each docu-ment, the judges extract sentences that theybelieve make up the best extract summary ofthe document.
A summary of a document gen-erated by a summarization algorithm is typi-cally compared to one of these "ground truth"summaries by counting the number of sentencesthe ground truth summary and the algorithm'ssummary have in common.
Thus, the more sen-tences a summary has recalled from the groundtruth, the higher its score will be.
See work byGoldstein et al (1999) and Jing et al (1998)for examples of the use of this measure.The recall-based measures introduce a biassince they are based on the Opinions of a smallnumber of assessors.
It is widely acknowledged(Jing et al, 1998; Kupiec et al, 1995; Voorhees,1998) that assessor agreement is typically quitelow.
There are at least two sources of this dis-agreement.
First, it is possible that one humanassessor will pick a particular sentence for in-clusion in their summary when the content ofanother sentence or set of sentences i approx-imately equivalent.
J ing et al (1998) agree:"...precision and recall are not the best mea-sures for computing document quality.
This isdue to the fact that a small change in the sum-mary output (e.g., replacing one sentence withan equally good equivalent which happens notto match majority opinion \[of the assessors\]) candramatical ly affect a system's core."
We callthis source of summary disagreement 'disagree-ment due to synonymy.'
Here is an example oftwo human-generated xtracts from the same1991 Wall Street Journal article which containdifferent sentences, but still seem to be describ-ing an article about violin playing in a film:EXTRACT 1:  Both Ms. Streisand's filmhusband, played by Jeroen Krabbe, andher film son, played by her real son Ja-son Gould, are, for the purposes of thescreenplay, violinists.
The actual sound- what might be called a fiddle over - wasproduced off camera by Pinchas Zucker-"70IIIiIIIIIIIIIIIIIIIman.
The violin program in "Prince ofTides" eliminates the critic's usual edgeand makes everyone fall back on his basicpair of ears.EXTRACT 2: Journalistic ethics forbidme from saying if I think "Prince of Tides"is as good as "Citizen Kane," but I don'tthink it's wrong to reveal that the filmhas some very fine violin playing.
Butmoviegoers will hear Mr. Zuckerman castoff the languor that too often makes himseem like the most bored of great violin-ists.
With each of these pieces, Mr. Zuck-erman takes over the movie and showswhat it means to play his instrument withsupreme dash.Another source of disagreement can arise fromjudges' differing opinions about the true focus ofthe original document.
In other words, judgesdisagree on what the document is about.
Wecall this second source 'disagreement due to fo-cus.'
Here is a human-generated xtract of thesame article which seems to differ in focus:EXTRACT 3: Columbia Pictures has de-layed the New York City and Los Angelesopenings of "Prince Of Tides" for a week.So Gothamites and Angelenos, along withthe rest of the country, will have to waituntil Christmas Day to see this film ver-sion of the Pat Conroy novel about aSouthern football coach (Nick Nolte) dal-lying with a Jewish female psychothera-pist (Barbra Streisand) in the Big Apple.Perhaps the postponement is a sign thatthe studio is looking askance at this ex-pensive product directed and co-produced".by its female lead.Whatever the source, disagreements at thesentence level are prevalent.
This has seri-ous implications for measures based on a sin-gle opinion, when a slightly different opinionwould result in a significantly different score(and rank) for many summaries.For example, consider the following three-sentence ground truth extract of a 37-sentence1994 Los Angeles Times article from the TRECcollection.
It contains sentences 1, 2 and 13.
(1) Clinton Trade Initiative Sinks UnderG-7 Criticism.
(2) President Clinton cameto the high-profile Group of Seven sum-mit to demonstrate new strength in for-71eign policy but instead watched his pre-mier initiative sink Saturday under a waveof sharp criticism.
(13) The negative re-action to the president's trade proposalcame as a jolt after administration offi-cials had built it up under the forward-looking name of "Markets 2000" and hadportrayed it as evidence of his interest inleading the other nations to more opentrade practices.An extract that replaces entence 13 with sen-tence 5:(5) In its most elementary form, it woul~dhave set up a one-year examination of im-' prediments to world trade, but it wouldhave also set an agenda for liberalizingtrade rules in entirely new areas, such asfinancial services, telecommunications a dinvest ment.will receive the same recall score as one whichreplaces entence 13 with sentence 32:(32) Most nations have yet to go throughthis process, which they hope to completeby January.These two alternative summaries both have thesame recall rank, but are obviously of very dif-ferent quality.Considered quantitatively, the only impor-tant component of either precision or recall isthe 'sentence agreement' J , the number of sen-tences a summary has in common with theground truth summary.
Following Goldsteinet al (1999), let M be the number of sen-tences in a ground truth extract summary andlet K be the number of sentences in a sum-mary to be evaluated.
With precision P =J /K  and recall R = J IM as usual, and F1 =2PR/(P + R); then elementary algebra showsthat F1 = 2J / (M?K).
Often, a fixed summarylength K is used.
(In terms of word count, thisrepresents varying compression rates.)
When aparticular ground truth of a given document ischosen, then precision, recall and F1 are all con-stant multiples of J .
As such, these measuresproduce different scores, but the same rankingof all the K-sentence extracts from the docu-ment.
Since only this ranking is of interest, it isnot necessary to examine more than one of P,R and F1.The sentence agreement J can only take oninteger values between 0 and M,  so J ,  P ,  R,and F1 are all discrete variables.
Therefore, al-though there may be thousands of possible ex-tract summaries of a document, only M + 1 dif-ferent scores are possible.
This will obviouslycreate a large number of ties in rankings pro-duced by the P, R, and F1 scores, and willgreatly increase the probability that radicallydifferent summaries will be given the same scoreand rank.
On the other hand, two summarieswhich express the same ideas using different sen-tences will be given very different scores.
Bothof these problems are illustrated by the exam-ple above.
Furthermore, if a particular groundtruth includes a large proportion of the doc-ument's sentences (perhaps it is ~ very con-cise document), shorter summaries will likely in-clude only sentences which appear in the groundtruth.
Consequently, even a randomly selectedcollection of sentences might obtain the largestpossible score.
Thus, recall-based measures arelikely to violate both properties (i) and (ii), dis-cussed at the beginning of Section 2.
These in-herent weaknesses in recall-based measures willbe further explored in Section 4.2.2 A Sentence-Rank-Based MeasureOne way to produce ground truth summaries ito ask judges to rank the sentences of a docu-.ment in order of their importance in a generic,indicative summary.
This is often a difficulttask for which it is nearly impossible to obtainconsistent results.
However, sentences whichappear early in a document are often more in-dicative of the content of the document hanare other sentences.
This is particularly truein newspaper articles, whose authors frequentlytry, to give the main points in the first para-graph (Brandow et al, 1995).
Similarly, adja-cent sentences are more likely to be related toeach other than to those which appear furtheraway in the text.
Thus, sentence position alonemay be an effective way to rank the importanceof sentences.To account for sentence importance within aground truth, a summary comparison measurewas developed which treats an extract as a rank-ing of the sentences of the document.
For ex-ample, a document with five sentences can beexpressed as (1, 2, 3, 4, 5).
A particular extractmay include sentences 2 and 3.
Then if sen-tence 2 is more important han sentence 3, thesentence ranks are given by (4, 1, 2, 4, 4).
Sen-tences 1, 4, and 5 all rank fourth, since 4 is themidrank of ranks 3, 4 and 5.
Such rank vectorscan be compared using Kendall's tau statistic(Sheskin, 1997), thus quantifying a summary'sagreement with a particular ground truth.
Aswill be shown in Section 4, sentence rank mea-sures result in a smaller number of ties than dorecall-based evaluation measures.Although it is also essentially recall-based,the sentence rank measure has another slightadvantage over recall.
Suppose a ground truthsummary of a 20-sentence document consistsof sentences (2, 3, 5}.
The machine-generatedsummaries consisting of sentences (2, 3, 4} and{2, 3, 9} would receive the same recall score, but(2, 3, 4} would receive a higher tau score (5 iscloser to 4 than to 9).
Of course, this higherscore may not be warranted if the content ofsentence 9 is more similar to that of sentence 5.The use of the tau statistic may be more ap-propriate for ground truths produced by classi-fying all of the sentences of the original docu-ment in terms of their importance to an indica-tive summary.
Perhaps four different categoriescould be used, ranging from 'very important'  to'not important.'
This would allow comparisonof a ranking with four equivalence classes (rep-resenting the document) to one with just twoequivalence classes (representing inclusion andexclusion from the summary to be evaluated).2.3 Content -Based  MeasuresSince indicative summaries alert users to doc-ument content, any measure that evaluates thequality of an indicative summary ought to con-sider the similarity of the content of the sum-mary to the content of the full document.
Thisconsideration should be independent of exactlywhich sentences are chosen for the summary.The content of the summary need only cap-ture the general ideas of the original docu-ment.
If human-generated xtracts are avail-able, machine-generated extracts may be evalu-ated alternatively by comparing their contentsto these ground truths.
This section definescontent-based measures by comparing the termfrequency (tf) vectors of extracts to tf vectorsof the full text or to tf  vectors of a ground truthextract.
When the texts and summaries are to-kenized and token aliases are determined by athesaurus, umrriaries that disagree due to syn-onymy are likely to have similarly-distributed72!
!tIiIii11IIIlIIiIlIIIIIIIIIIiIiIiIIi!term frequencies.
Also, summaries which hap-pen to use synonyms appearing infrequently inthe text will not be penalized in a summary-to-full-document comparison.
Note that termfrequencies can always be used to compare anextract with its full text, since the two will al-ways have terms in common, but without a the-saurus or some form of term aliasing, term fre-quencies cannot be used to compare abstractswith extracts.The vector space model of information re-trieval as described by Salton (1989) uses theinner product of document vectors to measurethe content similarity sirn(dl,d2) of two docu-ments dl and d2.
Geometrically, this similaritymeasure gives the cosine of the angle betweenthe two document vectors.
Since cos 0 = 1, doc-uments with high cosine similarity are deemedsimilar.
We apply this concept to summaryevaluation by computing document-summarycontent similarity sim(d, s) or ground truth-summary content similarity sire(g, s).Note that when comparing a summary withits document, a prior human assessment is notnecessary.
This may serve to eliminate the am-biguity of a human assessor's bias towards cer-tain types of summaries or sentences.
How-ever, the scores produced by such evaluationmeasures cannot be used reliably to comparesummaries of drastically different lengths, sincea much longer summary is more likely than ashort summary to produce a term frequency.vector which is similar to the full document's"tf vector, despite the normalization of the twovectors.
(This contrasts with the bias of recalltowards hort summaries.
)This similarity measure can be enhanced in anumber of ways.
For example, using term fre-quency counts for a large corpus of documents,term weighting (such as log-entropy (Dumais,199!)
or tf-idf (Salton, 1989)) can be used toweight he terms in the document and summaryvectors.
This may improve the performance ofthe similarity measure by increasing the weightsof content-indicative terms and decreasing theweights of those terms that are not indicativeof content.
It is demonstrated in Section 4 thatterm weighting caused a significant increase inthe correlation of the rankings produced by dif-ferent ground truths; however, it is n.ot clearthat this weighting increases the scores of highquality summaries.There are two potential problems with using -the cosine measure to evaluate the performanceof a summarization algorithm.
First of all, itis likely that the summary vector will be verysparse compared to the document vector sincethe summary will probably contain many fewerterms than the document.
Second, it is possi-ble that the summary will use key terms thatare not used often in the document.
For exam-ple, a document about the merger of two banks,may use the term "bank" frequently, and use therelated (yet not exactly synonymous) term "fi-nancial institution" only a few times.
It is pos-silJle that a high quality extract would have alow cosine similarity with the full document if itcontained only those few sentences that use theterm "financial institution" instead of "bank.
"Both of these problems can be addressed withanother common tool in information retrieval:latent semantic indexing or LSI (Deerwester tal., 1990).LSI is a method of reducing the dimensionof the vector space model using the singularvalue decomposition.
Given a corpus of doc-uments, create a term-by-document matrix Awhere each row corresponds to a term in thedocument set and each column corresponds toa document.
Thus, the columns of A representall the documents from the corpus, expressedin a particular term-weighting scheme.
(In ourtesting, the document vectors' entries are therelative frequencies of the terms.)
Compute thesingular value decomposition (SVD) of this ma-trix (for details ee Golub and van Loan (1989)).Retain some number of the largest singular val-ues of A and discard the rest.
In general, re-moving singular values serves as a dimensionreduction technique.
While the SVD computa-tion may be time-consuming when the corpus islarge, it needs to be performed only once to pro-duce a new term-document matrix and a pro-jection matrix.
To calculate the similarity of asummary and a document, he summary vectors must also be mapped to this low-dimensionalspace.
This involves computing a vector-matrixproduct, which can be done quickly.The effect of using the scaled, reduced-dimension document and summary vectors istwo-fold.
First, each coordinate of both the doc-ument and summary vector will contribute to73the overall similarity of the summary to the doc-ument (unlike the original vector space model,where only terms that occur in the summarycontribute to the cosine similarity score).
Sec-ond, the purpose of using LSI is to reduce theeffect of near-synonymy on the similarity score.If a term occurs infrequently in the documentbut is highly indicative of the content of thedocument, as in the case where the infrequentterm is synonymous with a frequent erm, thesummary will be penalized less in the reduced-dimension model for using the infrequent term-than it would be in the original vector spacemodel.
This reduction in penalty occurs be-cause LSI essentially averages the weights of,terms that co-occur frequently with other terms(both "bank" and "financial institution" oftenoccur with the term "account").
This shouldimprove the accuracy of the cosine similaritymeasure for determining the quality of a sum-mary of a document.3 Exper imenta l  Des ignThis section describes the experiment that testshow well these summary evaluation metrics per-form.
Fifteen documents from the Text Re-trieval Conference (TREC) collection were usedin the experiment.
These documents are part ofa corpus of 103 newspaper articles.
Each of thedocuments was tokenized by a language process-ing algorithm, which performed token aliasing.In our experiments, the term set was comprisedof all the aliases appearing in the full corpus of103 documents.
This corpus was used for thepurposes of term weighting.
Four expert judgescreated extract summaries (ground truths) foreach of the documents.
A list of the first 15documents, along with some of their numeri-cal features is found in Table 1.
The judgeswere instructed to select as many sentences aswere necessary to make an "ideal" indicative x-tract summary of the document.
In terms ofthe count of sentences in the ground truth, thelengths of the summaries varied from documentto document.
Ground truth compression rateswere generally between 10 and 20 percent.
Theinter-assessor agreement also varied, but was of-ten quite high.
We measured this by calculatingthe average pairwise recall in the collection offour ground truths.A suite of summary evaluation measures {Ek }which produce a score for a summary was de-veloped.
These measures may depend on none,one, or all of the collection of ground t ru thsummaries {gj}.
Measures which do not de-pend on ground truth compute the summary-document similarity sire(s, d).
Content-basedmeasures which depend on a single ground truthgi compute the summary-ground truth similar-ity sim(s, gi).
A measure which depends onall of the ground truths g l , .
- .
,ga ,  computesa summary's similarity with each ground truthseparately and averages these values.
Table 2enumerates the 28 different evaluation measuresthat were compared in this experiment.
Notethat the Recall and Kendall measures require aground truth.In this study, the measures will be used toevaluate xtract summaries of a fixed sentencelength K. In all of our tests, K = 3 for rea-sons of scale which will become clear.
A sum-mary length of three sentences represents vary-ing proportions of the number of sentences inthe full text document, but this length was usu-ally comparable to the lengths of the human-generated ground truths.
For each document,the collection {Sj} was generated.
This is theset of all possible K-sentence xtracts from thedocument.
If the document has N sentencestotal, there will be N choose KN)  N!g = K!
( /~L g ) !extracts in the exhaustive collection {Sj}.
Thefocus now is only on the set of all possible sum-maries and the evaluation measures, and not Onany particular summarization algorithm.
Foreach document, each of the measures in {Ek}was used to rank the sets {Sj}.
(Note that themeasures which do notdepend on ground truthscould, in fact, be used to generate summaries ifit were possible to produce and rank the exhaus-tive set of fixed-length summaries in real time.Despite the authors' access to impressive com-puting power, the process took several hoursfor each document!)
The next section comparesthese different rankings of the exhaustive set ofextracts for each document.4 Exper imenta l  Resu l t sOne way to compare the different rankings pro-duced by two different evaluation measures i to74IIIIIIIIIIiI %iIIIIiIIIIIIIiIIiIIiIIIIiIDoc.No.123456789101112131415Table 1: Test Document & Summary StatisticsSent.TREC File Name CountWSJ911211-0057 34wsJg00608-0126 34WSJ900712-0047 " 18latwp940604.0027 23latwp940621.0116 27latwp940624.0094 17latwp940707.0400 33latwp940709.0051 37latwp940713.0013 34latwp940713.0014 30latwp940721.0080 28latwp940725.0030 36latwp940725.0128 18latwp940729.0109 25' l'atwp940801.0010 28Token Gnd.
TruthCount Sent.
Cnt.667 3, 4, 12, 3603 4, 4, 9, 3364 2, 3, 5, 2502 4, 5, 5, 4579 12, 11, 10460 5, 5, 5, 4503 6, 9, 8, 8877 3, 5, 5, 4702 9, 4, 5, 8528 6, 5, 7, 5793 3, 3, 5, 2690 9, 2, 7, 5438 6, 3, 5, 5I682 4, 3, 4 ,3474 4, 5, 4, 5Gnd.
TruthAvg.
Recall44%54%7S%69%84%79%52%53%35%88%88%45%63%96%43%Table 2: Evaluation MeasuresSimilarityMeasure DetailsRecall J J  M , Ji = # ( s N gi )Kendall Tau see Section 2.2tf Cosine sirn(s, d or gi) o, tf voctors" t f - id f  Cosine sim(s,-) on tf-idf weighted vectorsSVD tf Cosine sire(s,  .)
o, iow-ai .
.
.
.
.
to,sN.A.
E1 E2 E3 E4 E5N.A.
E6 E7 Es E9 ElOEll E12 E13 E14 EIS E16El7 ElS El9 E20 E21 E22E23 E24 E25 E26 E27\] E2sCalculate their Spearman rank correlation coef-'ficient.
When two evaluation measures producenearly the same ranking of the summary set, therank correlation will be near 1 and a scatterplotof the two rankings will show points nearly ly-?
ing on a line with slope 1.
When there is littlecorrelation between two rankings, the statisticwill be near 0 and the scatterplot will appear tohave randomly-distributed points.
A negativecorrelation indicates that one ranking often re-verses the rankings of the other and in this casea rank scatterplot will show points nearly lyingon a line with negative slope.Table 3 compares the Spearman correlationof the rankings produced by a specific pair ofground truths.
The first row contains the cor-relations of two highly similar ground truth ex-tracts of document 14.
Both of these extractsconsisted of three sentences; two of the sen-tences were common to both extracts.
Not sur-prisingly, the correlation is high regardless ofwhat measure produced the rankings.
The sec-ond row demonstrates an increase (across therow) in correlation between rankings producedby two different ground truth summaries of doc-ument 8.
These two ground truths did not dis-agree in focus, but did disagree due to synonymy- -  they contain just one sentence in common.In general, the correlation among the rankingsproduced by synonymous ground truths was in-creased most by using the SVD content-basedcomparison.
Figure 1 illustrates the correla-tion increase graphically for this pair of groundtruths.
By contrast, the third row of Table 3displays a decrease (across the row) in correla-tion between rankings produced by two differ-ent ground truths.
In this case, the two groundtruths disagreed in .focus: they are Extracts 2and 3 contrasted in Section 2.1.
Again, the cor-relation among the rankings produced by thefour ground truths was decreased most by us-ing a weighted content-based comparison such75Table 3: Correlation of Ground Truths Depends on Level of DisagreementIt recall i tan I tf cosine I tf-idf I SVD IAgree Sentences 0.87 0.96 0.95 0.87 0.99Disagree synonymy 0.34 0.37 0.53 0.72 0.96Disagree focus 0.22 0.31 0.32 0.20 -0.29 ias tf-idf or SVD.
These patterns were typical forrankings produced by ground truths which dif-fered in focus, allaying the fear that applyingthe SVD weighting would produce correlatedrankings based on any two ground truths.Of course, the lack of correlation amongrecall-based rankings whenever ground truthsdid not contain exactly the same sentences im-plies that a different collection of extracts wouldrank highly if one ground truth were replacedwith the other.
This effect would surely carrythrough to system averages across a set of doc-uments.
To exemplify the size of this effect,for each document, he summaries which scoredhighest using one ground truth were scored (us-ing recall) against a second ground truth.
Withthe first ground truths, these high-scoring sum-maries averaged over 75% recall; using the sec-ond ground truths, the same summaries aver-aged just over 25% recall.
Thus, by simplychanging judges, an automatic system whichproduced these summaries would appear tohave a very different success rate.
This dispar-.
ity is lessened when content-based measures areused, but the outcomes are still disparate.Evidence suggests that the content-basedmeasures which do not rely on a ground truth?
may be an acceptable substitute to those whichdo'.- Over the set of 15 documents, the aver-age within-document inter-assessor correlationis 0.61 using term frequency, 0.72 using tf-idf,and 0.67 using SVD.
The average correlationof the ground truth dependent measures withthose that perform summary-document com-parisons is 0.48 using term frequency, 0.70 usingtf-idf, and 0.56 using SVD.
This means that onaverage, the rankings based on single groundtruths are only slightly more correlated to eachother than they are to the rankings that do notdepend on any ground truth.As noted in Section 2.1, the recall-basedmeasures exhibit unfavorable scoring proper-ties.
Figure 2 shows the histogram of scoresassigned to the exhaustive summary set for doc-Figure 1: Synonymy: Content-based MeasuresIncrease Rank Correlationii' e. |?D~J~ll8 ~ Sca~ GTIv~ GT2{P~d)~' J ' '  I (~ !.-"~io : ,?
o ~I m xHDocuml8 R~ ~@d: GTI~ GT2 ~dd)I i i~;i i i i iiii'l~ : : ~', !
!
!,,~r.,"~I 2D ~ nl m~E6~Oocuml8 R~ ~ct  GTI m GI2~)~ I  " ' '  ' ' 'iill '" ' " II~n~EI2~D~uT~8 Sc~e~ct GTI ~ GT2 ~4CF Cen)i !l ,,,,,,,,-,,,: .... amwi ll,, I, .
,,.
~.., ~%.~,,r~,~it t~~SP~Sca~GII~GT2NC~)m ~ m76IIiIIiI!
!1I!|iIII!Iument 14 by five different measures.
Each ofthese measures was based on the same groundtruth summary of this document, which con-tained four sentences.
Clearly, the measuresbased on a more sophisticated parsing methodhave a much greater ability to discriminate be-tween summaries.
By contrast, the recail met-ric can assign one of only four scores to a length3 summary, based on the value of Ji Elemen-tary combinatorics shows that 4 extracts willreceive the highest possible score (and thus willrank first), 126 summaries will rank second, 840 -summaries will rank third, and 1330 summarieswill rank last (with a score of 0).
This accountsfor all of the 2300 three-sentence extracts that ,are possible.
It seems very unlikely that all ofthe second-ranking summaries are equally effec-tive.
The histogram depicting this distributionis shown at the top of Figure 2.
This is fol-lowed by the histograms for the Kendall met-ric, and the content-based metrics using termfrequency, tf-idf, and SVD weighted vectors, re-spectively.
The tf-idf and SVD weighted mea-sures produced a very fine distribution of scores,particularly near the top of the range.
That is,these metrics are able to distinguish betweendifferent high-scoring summaries.
These pat-terns in the score histograms were typical acrossthe 15 documents.5 Conc lus ions  and  Future  Work'There is wide variation in the rankings pro-duced by recall scores from non-identical groundtruths.
This difference in scores is reflected in?
averages computed across documents.
The lowinter-assessor correlation of ranks based on re-call measures is distressing, and indicates thatthese measures cannot be effectively used tocompare performances of summarization sys-tems.
Measures which gauge content similarityproduce more highly correlated rankings when-ever ground truths do not disagree in focus.Content-based measures assign different rank-ings when ground truths do disagree in focus.
Inaddition, these measures provide a finer grainedscore with which to compare summaries.Moreover, the content-based measures whichrely on a ground truth are only slightly morecorrelated to each other than theyare  to themeasures which perform summary-documentcomparisons.
This suggests that the effective-77Figure 2: Score Histograms for Document 14l~Oillerd 14 S~re H'=do~ GT4 ~3~)!
=i!il .Ofx~t t 4 ~re F~?~ GT4 (Xer~)i!
l i , _  Ii,?
-- B - -U U U~S:R(~cun~ 14 ~ ~'~T GT4 ~ ~),dlllh,,.,,,,,,,..,.. .
.
.
.
.
.1 \]oU 02 ~ U U,LI U~tS~I~cume~ 14,~m I,~to~a~ ~4 (SVD Co~/e)IllllilUildJJ i!l _..,,,bang liiiDiad,,.I.I ~ I i  O.t U Q LI~TSeo~ness of summarization algorithms could be mea-sured without the use of human judges.
Sincethe cosine measure is easy to calculates feed-back of summary quality can be almost instan-taneous.The properties of these content-based mea-sures need to be further investigated.
For ex-ample, it is not clear that content-based mea-sures satisfy properties (i) and (ii), discussed inSection 2.
Also, while they do produce far fewerties than either recall or tau, such a fine distinc-tion in summary quality is probably not justi-fied.
When human-generated ground truths areavailable, perhaps some combination of recalland the content-based measures could be used.For instance, whenever recall is not perfect, thecontent of the non-overlapping sentences couldbe compared with the missed ground truth sen-tences.
Also, the effects of compression rate,summary length, and document style are notknown.The authors are currently performing furtherexperiments o see if users prefer summariesthat rank highly with content-based measuresover other summaries.
Also, the outcomesof extrinsic evaluation techniques will be com-pared with each of these scoring methods.
Inother words, do the high-ranking summarieshelp users to perform various tasks better thanlower-ranking summaries do?6 AcknowledgementsThe authors would like to thank Mary EllenOkurowski and Duncan Buell for their sup- jport, encouragement, and advice throughoutthis project.
Thanks go also to Tomek Strza-lkowski, Inderjeet Mani, Donna Harman, andHal Wilson for their suggestions of how to im-prove the design of the experiment.
We greatlyappreciate the fine editing advice Oksana Las-sowsky provided.
Finally, we are especiallygrateful to the four expert judges, Benay, Ed,MEO, and Toby, who produced our groundtruth summaries.ReferencesRonald Brandow, Karl Mitze, and Lisa F. Ran.1995.
Automatic ondensation f electronicpublications by sentence selection.
Informa-tion Processing and Management, 31 (5):675-685.Scott Deerwester, Susan T. Dumais, George W.Furnas, Thomas K. Landauer, and Richard?
ttarshman.
1990.
Indexing by latent seman-tic analysis.
Journal of the American Societyfor Information Science, 41(6):391-407.Susan T. Dumais.
1991.
Improving the retrievalof information from external sources.
Behav-ior Research Methods, Instruments ~ Com-puters, 23(2):229-236.Therese Firmin and Michael J. Chrzanowski.1999.
An evaluation of automatic text sum-marization systems.
In Advances in Au-tomatic Text Summarization, chapter 21,pages 325-336.
MIT Press, Cambridge, Mas-sachusetts.Jade Goldstein, Mark Kantrowitz, Vibhu Mit-tal, and Jaime Carbonell-.
1999.
Summa-rizing text documents: Sentence selection78and evaluation metrics.
In Proceedings of theACM SIGIR, pages 121-128.Gene H. Golub and Charles F. van Loan.
1989.Matrix Computations.
The Johns HopkinsUniversity Press, Baltimore.Hongyan Jing, Kathleen McKeown, ReginaBarzilay, and Michael Elhadad.
1998.
Sum-marization evaluation methods: Experimentsand analysis.
In American Association forArtificial Intelligence Spring Symposium Se-ries, pages 60-68.Julian Kupiec, Jan Pederson, and FrancineChen.
1995.
A trainable document Summa-rizer.
In Proceedings of the A CM SIGIR,' pages 68-73.Inderjeet Mani, David House, Gary Klein,Lynette Hirschman, Therese Firmin, andBeth Sundheim.
1999.
The TIPSTER SUM-MAC text summarization evaluation.
In Pro-ceedings of the Ninth Conference of the Euro-pean Chapter of the A CL, pages 77-85.Daniel Marcu.
1997.
From discourse struc-ture to text summaries.
In Proceedings ofthe A CL '97/EA CL'97 Workshop on Intelli-gent Scalable Text Summarization, pages 82-88.C.
D. Paice and P. A. Jones.
1993.
The identifi-cation of important concepts in highly struc-tured technical papers.
In Proceedings of theA CM SIGIR, pages 69-78.Gerard Salton.
1989.
Automatic Text Pro-cessing.
Addison-Wesley Publishers, Mas-sachusetts.C.
Schwarz.
1990.
Content based text han-dling.
Information Processing and Manage-ment, 26(2):219-226.David J. Sheskin.
1997.
Handbook of Paramet-ric and Nonparametric Statistical Procedures.CRC Press LLC, United States.T.
Strzalkowski, J. Wang, and B.
Wise.
1998.A robust practical text summarizaton sys-tem.
In AAAI Intelligent Text Summariza-tion Workshop, pages 26-30.Ellen M. Voorhees.
1998.
Variations in rele-vance judgements and the measurement ofretrieval effectiveness.
In Proceedings of theA CM SIGIR, pages 315-323.IIIIIIIIIIIIiIIIII
