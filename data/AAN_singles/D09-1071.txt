Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 678?687,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPThe infinite HMM for unsupervised PoS taggingJurgen Van GaelDepartment of EngineeringUniversity of Cambridgejv249@cam.ac.ukAndreas VlachosComputer LaboratoryUniversity of Cambridgeav308@cl.cam.ac.ukZoubin GhahramaniDepartment of EngineeringUniversity of Cambridgezoubin@eng.cam.ac.ukAbstractWe extend previous work on fully unsu-pervised part-of-speech tagging.
Usinga non-parametric version of the HMM,called the infinite HMM (iHMM), we ad-dress the problem of choosing the numberof hidden states in unsupervised Markovmodels for PoS tagging.
We experi-ment with two non-parametric priors, theDirichlet and Pitman-Yor processes, on theWall Street Journal dataset using a paral-lelized implementation of an iHMM in-ference algorithm.
We evaluate the re-sults with a variety of clustering evalua-tion metrics and achieve equivalent or bet-ter performances than previously reported.Building on this promising result we eval-uate the output of the unsupervised PoStagger as a direct replacement for the out-put of a fully supervised PoS tagger for thetask of shallow parsing and compare thetwo evaluations.1 IntroductionMany Natural Language Processing (NLP) tasksare commonly tackled using supervised learningapproaches.
These learning methods rely on theavailability of labeled datasets which are usuallyproduced by expensive manual annotation.
Forsome tasks, we have the choice to use unsuper-vised learning approaches.
While they do not nec-essarily achieve the same level of performance,they are appealing as unlabeled data is usuallyabundant.
In particular, for the purpose of ex-ploring new domains and languages, obtainininglabeled material can be prohibitively expensiveand unsupervised learning methods are a very at-tractive choice.
Recent work (Johnson, 2007;Goldwater and Griffiths, 2007; Gao and Johnson,2008) explored the task of part-of-speech tagging(PoS) using unsupervised Hidden Markov Models(HMMs) with encouraging results.
PoS tagging isa standard component in many linguistic process-ing pipelines, so any improvement on its perfor-mance is likely to impact a wide range of tasks.It is important to point out that a completelyunsupervised learning method will discover thestatistics of a dataset according to a particularmodel choice but these statistics might not cor-respond exactly to our intuition about PoS tags.Johnson (2007) and Gao & Johnson (2008) as-sume that words are generated by a hidden Markovmodel and find that the resulting states stronglycorrelate with POS tags.
Nonetheless, identifyingthe HMM states with appropriate POS tags is hard.Because many evaluation methods often requirePOS tags (rather than HMM states) this identifica-tion problem makes unsupervised systems difficultto evaluate.One potential solution is to add a small amountof supervision as in Goldwater & Griffiths (2007)who assume a dictionary of frequent words asso-ciated with possible PoS tags extracted from a la-beled corpus.
Although this technique improvesperformance, in this paper we explore the com-pletely unsupervised approach.
The reason for thisis that better unsupervised approaches provide uswith better starting points from which to explorehow and where to incorporate supervision.In previous work on unsupervised PoS tagginga main question was how to set the number of hid-den states appropriately.
Johnson (2007) reportsresults for different numbers of hidden states but itis unclear how to make this choice a priori, whileGoldwater & Griffiths (2007) leave this questionas future work.It is not uncommon in statistical machine learn-ing to distinguish between parameters of a modeland the capacity of a model.
E.g.
in a clusteringcontext, the choice for the number of clusters (ca-pacity) and the parameters of each cluster are often678treated differently: the latter are estimated usingalgorithms like EM, MCMC or Variational Bayeswhile the former is chosen using common sense,heuristics or in a Bayesian framework maybe us-ing evidence maximization.Non-parametric Bayesian methods are a class ofprobability distributions which explicitly treat thecapacity of a model as ?just another parameter?.Potential advantages are?
the model capacity can automatically adjustto the amount of data: e.g.
when clusteringa very small dataset, it is unlikely that manyfine grained clusters can be distinguished,?
inference can be more efficient: e.g.
insteadof running full inference for different modelcapacities and then choosing the best ca-pacity (according to some choice of ?best?
),inference in non-parametric Bayesian meth-ods integrates the capacity search in one al-gorithm.
This is particularly advantageouswhen parameters other than capacity need tobe explored, since it reduces signifcantly thenumber of experiments needed.None of these potential advantages are guaranteedand in this paper we investigate these two aspectsfor the task of unsupervised PoS tagging.The contributions in this paper extend previouswork on unsupervised PoS tagging in five ways.First, we introduce the use of a non-parametricversion of the HMM, namely the infinite HMM(iHMM) (Beal et al, 2002) for unsupervised PoStagging.
This answers an open problem fromGoldwater & Griffiths (2007).
Second, we care-fully implemented a parallelized version of theinference algorithms for the iHMM so we coulduse it on the Wall Street Journal Penn Treebankdataset.
Third, we introduce a new variant ofthe iHMM that builds on the Pitman-Yor process.Fourth, we evaluate the results with a variety ofclustering evaluation methods and achieve equiv-alent or better performances than previously re-ported.
Finally, building on this promising resultwe use the output of the unsupervised PoS taggeras a direct replacement for the output of a fully su-pervised PoS tagger for the task of shallow pars-ing.
This evaluation enables us to assess the appli-cability of an unsupervised PoS tagging methodand provides us with means of comparing its per-formance against a supervised PoS tagger.The rest of the paper is structured as follows:in section 2 we introduce the iHMM as a non-parametric version of the Bayesian HMM usedin previous work on unsupervised PoS tagging.Then, in section 3 we describe some details ofour implementation of the iHMM.
In section 4 wepresent a variety of evaluation metrics to compareour results with previous work.
Finally, in sec-tion 5 we report our experimental results.
We con-clude this paper with a discussion of ongoing workand experiments.2 The Infinite HMMIn this section, we describe a non-parametric hid-den Markov model known as the infinite HMM(iHMM) (Beal et al, 2002; Teh et al, 2006).
Aswe show below, this model is flexible in the num-ber of hidden states which it can accomodate.
Inother words, the capacity is an uncertain quantitywith an a priori infinite range that is a posterioriinferred by the data.
It is instructive to first re-view the finite HMM and its Bayesian treatment:for one, it is the model that has been used in previ-ous work on unsupervised PoS tagging, secondlyit allows us to better understand the iHMM.The Bayesian HMM A finite first-order HMMconsists of a hidden state sequence s =(s1, s2, .
.
.
, sT) and a corresponding observationsequence y = (y1, y2, .
.
.
, yT).
Each state vari-able stcan take on a finite number of states, say1 .
.
.K.
Transitions between states are governedby Markov dynamics parameterized by the tran-sition matrix pi, where piij= p(st= j|st?1=i), while the initial state probabilities are pi0i=p(s1= i).
For each state st?
{1 .
.
.K} thereis a parameter ?stwhich parameterizes the obser-vation likelihood for that state: yt|st?
F (?st).Given the parameters {pi0,pi,?,K} of the HMM,the joint distribution over hidden states s and ob-servations y can be written (with s0= 0):p(s,y|pi0,pi,?,K) =T?t=1p(st|st?1)p(yt|st)As Johnson (2007) clearly explained, training theHMM with EM leads to poor results in PoS tag-ging.
However, we can easily treat the HMM in afully Bayesian way (MacKay, 1997) by introduc-ing priors on the parameters of the HMM.
Withno further prior knowledge, a typical prior for thetransition (and initial) probabilities are symmet-ric Dirichlet distributions.
This corresponds to our679belief that, a priori, each state is equally likely totransition to every other state.
Also, it is com-monly known that the parameter of a Dirichletdistribution controls how sparse its samples are.In other words, by making the hyperprior on theDirichlet distribution for the rows of the transi-tion matrix small, we can encode our belief thatany state (corresponding to a PoS tag in this ap-plication context) will only be followed by a smallnumber of other states.
As we explain below, wewill be able to include this desirable property inthe non-parametric model as well.
Secondly, weneed to introduce a prior on the observation pa-rameters ?k.
Without any further prior knowl-edge, a convenient choice here is another sym-metric Dirichlet distribution with sparsity induc-ing hyperprior.
This encodes our belief that onlya subset of the words correspond to a particularstate.The Infinite HMM A first na?
?ve way to obtaina non-parametric HMM with an infinite numberof states might be to use symmetric Dirichlet pri-ors over the transition probabilities with parameter?/K and take K ?
?.
This approach unfortu-nately does not work: ?/K ?
0 when K ?
?and hence the rows of the matrix will become ?in-finitely sparse?.
Since the sum of the entries mustsum to one, the rows of the transition matrix willbe zero everywhere and all its mass in a randomlocation.
Unfortunately, this random location isout of an infinite number of possible locations andhence with probability 1 will be different for allthe rows.
As a consequence, at each timestep theHMM moves to a new state and will never revisitold states.
As we shall see shortly, we can fix thisby using a hierarchical Bayesian formalism wherethe Dirichlet priors on the rows have a shared pa-rameter.Before moving on to the iHMM, let us look atthe finite HMM from a different perspective.
Thefinite HMM of length T with K hidden states canbe seen as a sequence of T finite mixture models.The following equation illustrates this idea: con-ditioned on the previous state st?1, the marginalprobability of observation ytcan be written as:p(yt|st?1= k) =K?st=1p(st|st?1= k)p(yt|st),=K?st=1pik,stp(yt|?st).
(1)The variable st?1= k specifies the mixingweights pik,?for the mixture distribution, while stindexes the mixture component generating the ob-servation yt.
In other words, equation (1) says thateach row of the transition matrix pi specifies a dif-ferent mixture distribution over the same set of Kmixture components ?.Our second attempt to define a non-parametricversion of the hidden Markov model is to replacethe finite mixture by an infinite mixture.
Thetheory of Dirichlet process mixtures (Antoniak,1974) tells us exactly how to do this.
A drawG ?
DP (?,H) from a Dirichlet process (DP)with base measure H and concentration parame-ter ?
?
0 is a discrete distribution which can bewritten as an infinite mixture of atomsG(?)
=??i=1pii??i(?
)where the ?iare i.i.d.
draws from the base mea-sure H , ??i(?)
represents a point distribution at?iand pii= vi?i?1l=1(1 ?
vl) where each vl?Beta(1, ?).
The distribution over piiis called astick breaking construction and is essentially aninfinite dimensional version of the Dirichlet dis-tribution.
We refer to Teh et al (2006) for moredetails.Switching back to the iHMM our next step is tointroduce a DP Gjfor each state j ?
{1 ?
?
??
};we write Gj(?)
=??i=1piji??ji(?).
There is nowa parameter for each state j and each index i ?
{1, 2, ?
?
?
,?}.
Next, we draw the datapoint attimestep t given that the previous datapoint was instate st?1by drawing from DP Gst?1.
We first se-lect a mixture component stfrom the vector pist?1,?and then sample a datapoint yt?
F (?st?1,st) sowe get the following distribution for ytp(yt|?, st?1) =?
?st=1pist?1,stp(yt|?st?1,st).This is almost the non-parametric equivalent ofequation (1) but there is a subtle difference: eachGjselects their own set of parameters ?j?.
Thisis unfortunate as it means that the output distribu-tion would not be the same for each state, it woulddepend on which state we were moving to!
Luck-ily, we can easily fix this: by introducing an in-termediate distribution G0?
DP (?,H) and letGj?
DP (?,G0) we enforce that the i.i.d.
draws?j?are draws from a discrete distribution (sinceG0680is a draw from a Dirichlet process) and hence allGjwill share the same infinite set of atoms as cho-sen byG0.
Figure 1 illustrates the graphical modelfor the iHMM.The iHMM with Pitman-Yor Prior TheDirichlet process described above defines a veryspecific distribution over the number of statesin the iHMM.
One particular generalization ofthe Dirichlet process that has been studied in theNLP literature before is the Pitman-Yor process.Goldwater et al (2006) have shown that thePitman-Yor distribution can more accurately cap-ture power-law like distributions that frequentlyoccur in natural language.More specifically, a draw G ?
PY (d, ?,H)from a Pitman-Yor process (PY) with base mea-sure H , discount parameter 0 ?
d < 1 and con-centration parameter ?
> ?d is a discrete distri-bution which can be written as an infinite mixtureof atomsG(?)
=??i=1pii??i(?
)where the ?iare i.i.d.
draws from the base mea-sure H , ??i(?)
represents a point distribution at?iand pii= vi?i?1l=1(1 ?
vl) where each vl?Beta(1?d, ?+ ld).
Note the similarity to the DP:in fact, the DP is a special case of PY with d = 0.In our experiments, we constructed an iHMMwhere the DP (?,H) base measure G0is re-placed with its two parameter generalizationPY (d, ?,H).
Because the Dirichlet and Pitman-Yor processes only differ in the way pi is con-structed, without loss of generality we will de-scribe hyper-parameter choice and inference in thecontext of the iHMM with Dirichlet process basemeasure.Hyperparameter Choice The descriptionabove shows that there are 4 parameters whichwe must specify: the base measure H , theoutput distribution p(yt|?st), the discount1andconcentration2parameters d, ?
for G0and theconcentration parameter ?
for the DP?sGj.
Just asin the finite case, the base measure H is the priordistribution on the parameter ?
of p(yt|?st).
Wechose to use a symmetric Dirichlet distributionwith parameter ?
over the word types in ourcorpus.
Since we do not know the sparsity level ?of the output distributions we decided to learn this1for Pitman-Yor base measure2for both Dirichlet and Pitman-Yor base measuresparameter from the data.
We initially set a vagueGamma prior over ?
but soon realized that as weexpect hidden states in the iHMM to correspondto PoS tags, it is unrealistic to expect each stateto have the same sparsity level.
Hence we chosea Dirichlet process as the prior for ?
; this waywe end up with a small discrete set of sparsitylevels: e.g.
we can learn that states correspondingto verbs and nouns share one sparsity levelwhile states correpsonding to determiners havetheir own (much sparser) sparsity level.
For theoutput distribution p(yt|?st) we chose a simplemultinomial distribution.The hyperparameters d and ?
mostly control thenumber of states in the iHMM while - as we dis-cussed above - ?
controls the sparsity of the tran-sition matrix.
In the experiments below we reportboth fixing the two parameters and learning themby sampling (using vague Gamma hyperpriors).Because of computational constraints, we chose touse vague Bayesian priors for all hyperparametersrather than run the whole experiment over a grid of?reasonable?
parameter settings and use the bestones according to cross validation.3 InferenceThe Wall Street Journal part of the Penn Tree-bank that was used for our experiments containsabout one million words.
In the non-parametricBayesian literature not many algorithms have beendescribed that scale into this regime.
In this sec-tion we describe our parallel implementation ofthe iHMM which can easily handle a dataset ofthis scale.There is a wealth of evidence (Scott, 2002; Gaoand Johnson, 2008) in the machine learning litera-ture that Gibbs sampling for Markov models leadsto slow mixing times.
Hence we decided our start-ing point for inference needs to be based on dy-namic programming.
Because we didn?t have agood idea for the number of states that we were go-ing to end up with, we prefered the beam samplerof Van Gael et al (2008) over a finite truncationof the iHMM.
Moreover, the beam sampler alsointroduces a certain amount of sparsity in the dy-namic program which can speed up computations(potentially at the cost of slower mixing).The beam sampler is a blocked Gibbs samplerwhere we alternate between sampling the param-eters (transition matrix, output parameters), thestate sequence and the hyperparameters.
Sam-681k = 1 ?
?
?1s0s1s2y1y2?k?k??
?HFigure 1: The graphical model for the iHMM.
The variable ?
represents the mixture for the DP G0.pling the transition matrix and output distribu-tion parameters requires computing their sufficientstatistics and sampling from a Dirichlet distribu-tion; we refer to the beam sampling paper for de-tails.
For the hyperparameters we use standardGibbs sampling.
We briefly sketch the resam-pling step for the state sequence for a single se-quence of data (sentence of words).
Running stan-dard dynamic programming is prohibitive becausethe state space of the iHMM is infinitely large.The central idea of the beam sampler is to adap-tively truncate the state space of the iHMM andrun dynamic programming.
In order to truncatethe state space, we sample an auxilary variable utfor each word in the sequence from the distribu-tion ut?
Uniform(0, pist?1st) where pi representsthe transition matrix.Intuitively, when we sample u1:T|s1:Taccord-ing to the distribution above, the only valid sam-ples are those for which the utare smaller thanthe transition probabilities of the state sequences1:T. This means that when we sample s1:T|u1:Tat a later point, it must be the case that the ut?sare still smaller than the new transition probabil-ities.
This significantly reduces the set of validstate sequences that we need to consider.
Morespecifically, Van Gael et al (2008) show that wecan compute p(st|y1:t, u1:t) using the followingdynamic programming recursion p(st|y1:t, u1:t) =p(yt|st)?st?1:ut<pist?1,stp(st?1|y1:t?1, u1:t?1).The summation?st?1:ut<pist?1,stensures that thiscomputation remains finite.
When we computep(st|y1:t, u1:t) for t ?
{1 ?
?
?T}, we can easilysample sTand using Bayes rule backtrack sampleevery other st.
It can be shown that this procedureproduces samples from the exact posterior.Notice that the dynamic program only needs toperform computation when ut< pist?1,st.
A care-ful implementation of the beam sampler consistsof preprocessing the transition matrix pi and sort-ing its elements in descending order.
We can theniterate over the elements of the transition matrixstarting from the largest element and stop oncewe reach the first element of the transition matrixsmaller than ut.
In our experiments we found thatthis optimization reduces the amount of computa-tion per sentence by an order of magnitutde.A second optimization which we introducedis to use the map-reduce paradigm (Dean andGhemawat, 2004) to parallelize our computations.More specifically, after we preprocess the transi-tion matrix, the dynamic program computationsare independent for each sentence in the dataset.This means we can perform each dynamic pro-gram in parallel; in other words our ?map?
con-sists of running the dynamic program on one sen-tence in the dataset.
Next, we need to resamplethe transition matrix and output distribution pa-rameters.
In order to do so we need to computetheir sufficient statistics: the number of transitionsfrom state to state and the number of emissions ofeach word out of each state.
Our ?reduce?
func-tion consists of computing the sufficient statisticsfor each sentence and then aggregating the statis-tics for the whole dataset.
Our implementationruns on a quad-core shared memory architectureand we find an almost linear speedup going fromone to four cores.4 EvaluationEvaluating unsupervised PoS tagging is rather dif-ficult mainly due to the fact that the output of such682systems are not actual PoS tags but state identi-fiers.
Therefore it is impossible to evaluate per-formance against a manually annotated gold stan-dard using accuracy.
Recent work (Goldwater andGriffiths, 2007; Johnson, 2007; Gao and Johnson,2008) on this task explored a variety of method-ologies to address this issue.The most common approach followed in pre-vious work is to evaluate unsupervised PoS tag-ging as clustering against a gold standard usingthe Variation of Information (VI) (Meil?a, 2007).VI assesses homogeneity and completeness us-ing the quantities H(C|K) (the conditional en-tropy of the class distribution in the gold stan-dard given the clustering) and H(K|C) (the con-ditional entropy of clustering given the class dis-tribution in the gold standard).
However, as Gao& Johnson (2008) point out, VI is biased to-wards clusterings with a small number of clus-ters.
A different evaluation measure that usesthe same quantities but weighs them differently isthe V-measure (Rosenberg and Hirschberg, 2007),which is defined in Equation 2 by setting the pa-rameter ?
to 1.h = 1?H(C|K)H(C)c = 1?H(K|C)H(K)V?=(1 + ?
)hc(?h) + c(2)Vlachos et al (2009) noted that V-measure favorsclusterings with a large number of clusters.
Bothof these biases become crucial in our experiments,since the number of clusters (states of the iHMM)is not fixed in advance.
Vlachos et al proposed avariation of the V-measure, V-beta, that adjusts thebalance between homogeneity and completenessusing the parameter ?
in Eq.
2.It is worth mentioning that, unlike V-measureand V-beta, VI scores are not normalizedand therefore they are difficult to interpret.Meil?a (2007) presented two normalizations,acknowledging the potential disadvantagesthey have.
The first one normalizes VI by2 log(max(|K|, |C|)), which is inappropriatewhen the number of clusters discovered |K|changes between experiments.
The secondnormalization involves the quantity logN whichis appropriate when comparing different algo-rithms on the same dataset (N is the numberof instances).
However, this quantity dependsexclusively on the size of the dataset and hence ifthe dataset is very large it can result in normalizedVI scores misleadingly close to 100%.
This doesnot affect rankings, i.e.
a better VI score will alsobe translated into a better normalized VI score.
Inour experiments, we report results only with theun-normalized VI scores, V-measure and V-beta.All the evaluation measures mentioned so farevaluate PoS tagging as a clustering task againsta manually annotated gold standard.
While thisis reasonable, it still does not provide means ofassessing the performance in a way that wouldallow comparisons with supervised methods thatoutput actual PoS tags.
Even for the normalizedmeasures V-measure and V-beta, it is unclear howtheir values relate to accuracy levels.
Gao & John-son (2008) partially addressed this issue by map-ping states to PoS tags following two differentstrategies, cross-validation accuracy, and greedy1-to-1 mapping, which both have shortcomings.We argue that since an unsupervised PoS tagger istrained without taking any gold standard into ac-count, it is not appropriate to evaluate against aparticular gold standard, or at least this should notbe the sole criterion.
The fact that different authorsuse different versions of the same gold standard toevaluate similar experiments (e.g.
Goldwater &Griffiths (2007) versus Johnson (2007)) supportsthis claim.
Furthermore, PoS tagging is seldomlya goal in itself, but it is a component in a linguisticpipeline.In order to address these issues, we perform anextrinsic evaluation using a well-explored task thatinvolves PoS tags.
While PoS tagging is consid-ered a pre-processing step in many natural lan-guage processing pipelines, the choice of task isrestricted by the lack of real PoS tags in the out-put of our system.
For our purposes we need atask that relies on discriminating between PoS tagsrather than the PoS tag semantics themselves, inother words, a task in which knowing whether aword is tagged as noun instead of a verb is equiv-alent to knowing it is tagged as state 1 instead ofstate 2.
Taking these considerations into account,in Section 5 we experiment with shallow pars-ing in the context of the CoNLL-2000 shared task(Tjong Kim Sang and Buchholz, 2000) in whichvery good performances were achieved using onlythe words with their PoS tags.
Our intuition is thatif the iHMM (or any unsupervised PoS tagging683method) has a reasonable level of performance, itshould improve on the performance of a systemthat does not use PoS tags.
Moreover, if the per-formance is very good indeed, it should get closeto the performance of a system that uses real PoStags, provided either by human annotation or by agood supervised system.
Similar extrinsic evalu-ation was performed by Biemann et al (2007).
Itis of interest to compare the results between theclustering evaluation and the extrinsic one.A different approach in evaluating non-parametric Bayesian models for NLP is state-splitting (Finkel et al, 2007; Liang et al, 2007).In this setting, the model is used in order to re-fine existing annotation of the dataset.
While thisapproach can provide us with some insights andinterpretable results, the use of existing annotationinfluences the output of the model.
In this work,we want to verify whether the output of the iHMM(without any supervision) can be used instead ofthat of a supervised system.5 ExperimentsIn all our experiments, the Wall Street Journal(WSJ) part of the Penn Treebank was used.
As ex-plained in Section 4, we evaluate the output of theiHMM in two ways, as clustering with respect to agold standard and as direct replacement of the PoStags in the task of shallow parsing.
In each experi-ment, we obtain a sample from the iHMM over allthe sections of WSJ.
The states for sections 15-18and 20 of the WSJ (training and testing sets re-spectvely in the CoNLL shared task) are used forthe evaluation based on shallow parsing, while theremaining sections are used for evaluation againstthe WSJ gold standard PoS tags using clusteringevaluation measures.As described in Section 2 we performed threeruns with the iHMM: one run with DP prior andfixed ?, ?, one with PY prior and fixed d, ?, ?
andone with DP prior but where we learn the hyper-parameters ?, ?
from the data.
Our inference algo-rithm uses 1000 burn-in iterations after which wecollect a sample every 1000 iterations.
Our infer-ence procedure is annealed during the first 1000burnin and 2400 iterations by powering the likeli-hood of the output distribution with a number thatsmoothly increases from 0.4 to 1.0 over the 3400first iterations.
The numbers of iterations reportedin the remainder of the section refer to the itera-tions after burn-in.
We initialized the sampler by:a) sampling the hyperparameters from the priorwhere applicable, b) uniformly assign each wordone out of 20 iHMM states.
For the DP run withfixed parameters, we chose ?
= 0.8 to encouragesome sparsity in the transition matrix and ?
= 5.0to allow for enough hidden states.
For the PY runwith fixed parameters, we chose ?
= 0.8 for simi-lar reasons and d = 0.1 and ?
= 1.0.
We point outthat one weakness of MCMC methods is that theyare hard to test for convergence.
We chose to runthe simulations until they became prohibitively ex-pensive to obtain a new sample.First, we present results using clustering eval-uation measures which appear in the figures ofTable 1.
The three runs exhibit different behav-ior.
The number of states reached by the iHMMwith fixed parameters using the DP prior stabilizesclose to 50 states, while for the experiment withlearnt hyperparameters the number of states growsmore rapidly, reaching 194 states after 8,000 iter-ations.
With the PY prior, the number of statesreached grows less rapidly reaching 90 states.
Allruns achieve better performances with respect toall the measures used as the number of iterationsgrows.
An exception is that VI scores tend to in-crease (lower VI scores are better) when the num-ber of states grows larger than the gold standard.It is interesting to notice how the measures exhibitdifferent biases, in particular that VI penalizes thelarger numbers of states discovered in the DP runwith learnt parameters as well as the run with thePY prior, compared to the more lenient scores pro-vided by V-measure and V-beta.
The latter thoughassigns lower scores to the DP run with learnt pa-rameters because it takes into account that the highhomogeneity is achieved using even more states.Finally, the interpretability of these scores presentssome interest.
For example, in the run with fixedparameters using the DP prior, after burn-in VIwas 4.6, which corresponds to 76.65% normalizedVI score, while V-measure and V-beta were 12.7%and 9% respectively.
In 8,000 iterations after burn-in, VI was 3.94 (80.3% when normalized), whileV-measure and V-beta were 53.3%, since the num-ber of states was almost the same as the number ofunique PoS tags in the gold standard.The closest experiment to ours is the one byGao & Johnson (2008) who run their BayesianHMM over the whole WSJ and evaluated againstthe full gold standard, the only difference beingis that we exclude the CoNLL shared task sec-6840204060801001201401601802000  1  2  3  4  5  6  7  8statesDP-learntDP-fixedPY-fixed0102030405060700  1  2  3  4  5  6  7  8homogeneityDP-learntDP-fixedPY-fixed25303540455055600  1  2  3  4  5  6  7  8completenessDP-learntDP-fixedPY-fixed3.63.844.24.44.64.855.20  1  2  3  4  5  6  7  8VIDP-learntDP-fixedPY-fixed10152025303540455055600  1  2  3  4  5  6  7  8V-measureDP-learntDP-fixedPY-fixed510152025303540455055600  1  2  3  4  5  6  7  8V-betaDP-learntDP-fixedPY-fixedTable 1: Performance of the three iHMM runs according to clustering evaluation measures against num-ber of iteretions (in thousands).93.293.493.693.89494.294.494.60  1  2  3  4  5  6  7  8accuracyDP-learntDP-fixedPY-fixed88.58989.59090.5910  1  2  3  4  5  6  7  8F-scoreDP-learntDP-fixedPY-fixedTable 2: Performance of the output of the three iHMM runs when used in shallow parsing against numberof iteretions (in thousands).tions from our evaluation, which leaves us with 19sections instead of 24.
Their best VI score was4.03886 which they achieved using the collapsed,sentence-blocked Gibbs sampler with the numberof states fixed to 50.
The VI score achieved by theiHMM with fixed parameters using the PY priorreaches 3.73, while using the DP prior VI reaches4.32 with learnt parameters and 3.93 with fixed685parameters.
These results, even if they are notdirectly comparable, are on par with the state-of-the-art, which encouraged us to proceed with theextrinsic evaluation.For the experiments with shallow parsing weused the CRF++ toolkit3which has an efficientimplementation of the model introduced by Sha &Pereira (2003) for this task.
First we ran an experi-ment using the words and the PoS tags provided inthe shared task data and the performances obtainedwere 96.07% accuracy and 93.81% F-measure.The PoS tags were produced using the Brill tag-ger (Brill, 1994) which employs tranformation-based learning and was trained using the WSJ cor-pus.
Then we ran an experiment removing thePoS tags altogether, and the performances were93.25% accuracy and 88.58% F-measure respec-tively.
This gave us some indication as to what thecontribution of the PoS tags is in the context of theshallow parsing task at hand.The experiments using the output of the iHMMas PoS tags for shallow parsing are presented inTable 2.
The best performance achieved was94.48% and 90.98% in accuracy and F-measure,which is 1.23% and 2.4% better respectively thanjust using words, but worse by 1.57% and 2.83%compared to using the supervised PoS tagger out-put.
Given that the latter is trained on WSJ we be-lieve that this is a good result.
Interestingly, thiswas obtained by using the last sample from theiHMM run using the DP prior with learnt param-eters which has worse overall clustering evalua-tion scores, especially in terms of VI.
This samplethough has the best homogeneity score (69.39%).We believe that homogeneity is more importantthan the overall clustering score due to the factthat, in the application considered, it is probablyworse to assign tokens that belong to different PoStags to the same state, e.g.
verb and adverbs, ratherthan generate more than one state for the samePoS.
This is likely to be the case in tasks wherewe are interested in distinguishing between PoStags rather than the actual tag itself.
Also, clus-tering evaluation measures tend to score lenientlyconsistent mixing of members of different classesin the same cluster.
However, such mixing resultsin consistent noise when the clustering output be-comes input to a machine learning method, whichis harder to deal with.3http://crfpp.sourceforge.net/6 Conclusions - Future WorkIn the context of shallow parsing we saw that theperformance of the iHMM does not match theperformance of a supervised PoS tagger but doeslead to a performance increase over a model us-ing only words as features.
Given that it was con-structed without any need for human annotation,we believe this is a good result.
At the same timethough, it suggests that it is still some way frombeing a direct drop-in replacement for a supervisedmethod.
We argue that the extrinsic evaluation ofunsupervised PoS tagging performed in this paperis quite informative as it allowed us to assess ourresults in a more realistic context.
In this work weused shallow parsing for this, but we are consider-ing other tasks in which we hope that PoS taggingperformance will be more crucial.Our experiments also suggest that the number ofstates in a Bayesian non-parametric model can berather unpredictable.
On one hand, this is a strongwarning towards inference algorithms which per-form finite truncation of non-parametric models.On the other hand, the remarkable difference inbehavior between the DP with fixed and learnedpriors suggests that more research is needed to-wards understanding the influence of hyperparam-eters in Bayesian non-parametric models.We are currently experimenting with a semi-supervised PoS tagger where we let the transi-tion matrix of the iHMM depend on annotatedPoS tags.
This model allows us to: a) use an-notations whenever they are available and do un-supervised learning otherwise; b) use the powerof non-parametric methods to possibly learn morefine grained statistical structure than tag sets cre-ated manually.On the implementation side, it would be in-teresting to see how our methods scale in a dis-tributed map-reduce architecture where networkcommunication overhead becomes an issue.Finally, the ultimate goal of our investigation isto do unsupervised PoS tagging using web-scaledatasets.
Although the WSJ corpus is reasonablysized, our computational methods do not currentlyscale to problems with one or two order of magni-tude more data.
We will need new breakthroughsto unleash the full potential of unsupervised learn-ing for NLP.686ReferencesCharles E. Antoniak.
1974.
Mixtures of dirichlet pro-cesses with applications to bayesian nonparametricproblems.
The Annals of Statistics, 2(6):1152?1174.M.
J. Beal, Z. Ghahramani, and C. E. Rasmussen.2002.
The infinite hidden markov model.
Advancesin Neural Information Processing Systems, 14:577 ?584.Chris Biemann, Claudio Giuliano, and Alfio Gliozzo.2007.
Unsupervised part-of-speech tagging support-ing supervised methods.
In Proceedings of RANLP.Eric Brill.
1994.
Some advances in transformation-based part of speech tagging.
In National Confer-ence on Artificial Intelligence, pages 722?727.Jeffrey Dean and Sanjay Ghemawat.
2004.
Mapre-duce: Simplified data processing on large clusters.In Sixth Symposium on Operating System Designand Implementation.Jenny Rose Finkel, Trond Grenager, and Christo-pher D. Manning.
2007.
The infinite tree.
In Pro-ceedings of the 45th Annual Meeting of the Associ-ation of Computational Linguistics, pages 272?279,Prague, Czech Republic, June.
Association for Com-putational Linguistics.J.
Van Gael, Y. Saatci, Y. W. Teh, and Z. Ghahra-mani.
2008.
Beam sampling for the infinite hiddenmarkov model.
In Proceedings of the 25th interna-tional conference on Machine learning, volume 25,Helsinki.Jianfeng Gao and Mark Johnson.
2008.
A compari-son of bayesian estimators for unsupervised hiddenmarkov model pos taggers.
In Proceedings of the2008 Conference on Empirical Methods in NaturalLanguage Processing, pages 344?352.Sharon Goldwater and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speechtagging.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 744?751, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.S.
Goldwater, T. Griffiths, and M. Johnson.
2006.
In-terpolating between types and tokens by estimatingpower-law generators.
Advances in Neural Informa-tion Processing Systems, 18.Mark Johnson.
2007.
Why Doesn?t EM Find GoodHMM POS-Taggers?
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, pages 296?305.P.
Liang, S. Petrov, M. I. Jordan, and D. Klein.
2007.The infinite PCFG using hierarchical Dirichlet pro-cesses.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning.D.
J. C. MacKay.
1997.
Ensemble learning for hiddenMarkov models.
Technical report, Cavendish Labo-ratory, University of Cambridge, 1997.Marina Meil?a.
2007.
Comparing clusterings?an in-formation based distance.
Journal of MultivariateAnalysis, 98(5):873?895.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clus-ter evaluation measure.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, pages 410?420, Prague, CzechRepublic, June.Steven L. Scott.
2002.
Bayesian methods for hiddenmarkov models: Recursive computing in the 21stcentury.
Journal of the American Statistical Asso-ciation, 97(457):337?351, March.Fei Sha and Fernando Pereira.
2003.
Shallow pars-ing with conditional random fields.
In Human Lan-guage Technology Conference and the 4th Meetingof the North American Association for Computa-tional Linguistics.Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,and D. M. Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.Erik F. Tjong Kim Sang and Sabine Buchholz.2000.
Introduction to the CoNLL-2000 shared task:Chunking.
In Claire Cardie, Walter Daelemans,Claire Nedellec, and Erik Tjong Kim Sang, editors,Proceedings of the Fourth Conference on Computa-tional Natural Language Learning, pages 127?132.Lisbon, Portugal, September.Andreas Vlachos, Anna Korhonen, and ZoubinGhahramani.
2009.
Unsupervised and ConstrainedDirichlet Process Mixture Models for Verb Cluster-ing.
In Proceedings of the EACL workshop on GEo-metrical Models of Natural Language Semantics.687
