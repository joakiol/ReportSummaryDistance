Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 927?936,Honolulu, October 2008. c?2008 Association for Computational LinguisticsQuestion Classification using Head Words and their HypernymsZhiheng HuangEECS DepartmentUniversity of Californiaat BerkeleyCA 94720-1776, USAzhiheng@cs.berkeley.eduMarcus ThintIntelligent Systems Research CenterBritish Telecom GroupChief Technology Officemarcus.2.thint@bt.comZengchang QinEECS DepartmentUniversity of Californiaat BerkeleyCA 94720-1776, USAzqin@cs.berkeley.eduAbstractQuestion classification plays an important rolein question answering.
Features are the key toobtain an accurate question classifier.
In con-trast to Li and Roth (2002)?s approach whichmakes use of very rich feature space, we pro-pose a compact yet effective feature set.
Inparticular, we propose head word feature andpresent two approaches to augment semanticfeatures of such head words using WordNet.In addition, Lesk?s word sense disambigua-tion (WSD) algorithm is adapted and the depthof hypernym feature is optimized.
With fur-ther augment of other standard features suchas unigrams, our linear SVM and MaximumEntropy (ME) models reach the accuracy of89.2% and 89.0% respectively over a standardbenchmark dataset, which outperform the bestpreviously reported accuracy of 86.2%.1 IntroductionAn important step in question answering (QA) andother dialog systems is to classify the question tothe anticipated type of the answer.
For example, thequestion of Who discovered x-rays should be classi-fied into the type of human (individual).
This infor-mation would narrow down the search space to iden-tify the correct answer string.
In addition, this infor-mation can suggest different strategies to search andverify a candidate answer.
For instance, the classifi-cation of question What is autism to a definition typequestion would trigger the search strategy specificfor definition type (e.g., using predefined templateslike: Autism is ... or Autism is defined as...).
In fact,the combination of QA and the named entity recog-nition is a key approach in modern question answer-ing systems (Voorhees and Dang, 2005).The question classification is by no means trivial:Simply using question wh-words can not achievesatisfactory results.
The difficulty lies in classify-ing the what and which type questions.
Consideringthe example What is the capital of Yugoslavia, it isof location (city) type, while What is the pH scaleis of definition type.
Considering also examples (Liand Roth, 2006) What tourist attractions are there inReims, What are the names of the tourist attractionsin Reims, What do most tourists visit in Reims, Whatattracts tourists to Reims, and What is worth seeingin Reims, all these reformulations are of the sameanswer type of location.
Different wording and syn-tactic structures make it difficult for classification.Many QA systems used manually constructed setsof rules to map a question to a type, which is not effi-cient in maintain and upgrading.
With the increasingpopularity of statistical approaches, machine learn-ing plays a more and more important role in thistask.
A salient advantage of machine learning ap-proach is that one can focus on designing insightfulfeatures, and rely on learning process to efficientlyand effectively cope with the features.
In addition, alearned classifier is more flexible to reconstruct thana manually constructed system because it can betrained on a new taxonomy in a very short time.
Ear-lier question classification work includes Pinto et al(2002) and Radev et at.
(2002), in which languagemodel and Rappier rule learning were employedrespectively.
More recently, Li and Roth (2002)have developed a machine learning approach whichuses the SNoW learning architecture (Khardon et al,9271999).
They have compiled the UIUC question clas-sification dataset 1 which consists of 5500 trainingand 500 test questions.
The questions in this datasetare collected from four sources: 4,500 English ques-tions published by USC (Hovy et al, 2001), about500 manually constructed questions for a few rareclasses, 894 TREC 8 and TREC 9 questions, andalso 500 questions from TREC 10 which serve as thetest dataset.
All questions in the dataset have beenmanually labeled by them according to the coarseand fine grained categories as shown in Table 3, withcoarse classes (in bold) followed by their fine classrefinements.
In addition, the table shows the dis-tribution of the 500 test questions over such cate-gories.
Li and Roth (2002) have made use of lexicalwords, part of speech tags, chunks (non-overlappingphrases), head chunks (the first noun chunk in aquestion) and named entities.
They achieved 78.8%accuracy for 50 fine grained classes.
With a hand-built dictionary of semantically related words, theirsystem is able to reach 84.2%.The UIUC dataset has laid a platform for thefollow-up research.
Hacioglu and Ward (2003) usedlinear support vector machines with question wordbigrams and error-correcting output to obtain accu-racy of 80.2% to 82.0%.
Zhang and Lee (2003)used linear SVMs with all possible question wordgrams, and obtained accuracy of 79.2%.
Later Liand Roth (2006) used more semantic informationsources including named entities, WordNet senses,class-specific related words, and distributional sim-ilarity based categories in question classificationtask.
With all these semantic features plus the syn-tactic ones, their model was trained on 21?500 ques-tions and was able to achieve the best accuracy of89.3% on a test set of 1000 questions (taken fromTREC 10 and TREC 11) for 50 fine classes.
Mostrecently, Krishnan et al (2005) used a short (typ-ically one to three words) subsequence of questiontokens as features for question classification.
Theirmodel can reach the accuracy of 86.2% using UIUCdataset over fine grained question categories, whichis the highest reported accuracy on UIUC dataset.In contrast to Li and Roth (2006)?s approachwhich makes use of a very rich feature set, wepropose to use a compact yet effective feature set.In particular, we propose head word feature and1available at http://12r.cs.uiuc.edu/?cogcomp/Data/QA/QCpresent two approaches to augment semantic fea-tures of such head words using WordNet.
In addi-tion, Lesk?s word sense disambiguation (WSD) al-gorithm is adapted and the depth of hypernym fea-ture is optimized.
With further augment of otherstandard features such as unigrams, we can obtainaccuracy of 89.2% using linear SVMs, or 89.0% us-ing ME for 50 fine classes.2 ClassifiersIn this section, we briefly present two classifiers,support vector machines and maximum entropymodel, which will be employed in our experiments.These two classifiers perform roughly identical inthe question classification task.2.1 Support Vector MachinesSupport vector machine (Vapnik, 1995) is a usefultechnique for data classification.
Given a training setof instance-labeled pairs (xi, yi), i = 1, .
.
.
, l wherexi ?
Rn and y ?
{1,?1}l , the support vector ma-chines (SVM) require the solution of the followingoptimization problem: minw,b,?
12wTw+C?li=1 ?isubject to yi(wT?
(xi) + b) ?
1 ?
?i and ?i ?
0.Here training vectors xi are mapped into a higher(maybe infinite) dimensional space by the function?.
Then SVM finds a linear separating hyperplanewith the maximal margin in this higher dimensionalspace.
C > 0 is the penalty parameter of the er-ror term.
Furthermore, K(xi,xj) ?
?(xi)T?
(xi) iscalled the kernel function.
There are four basic ker-nels: linear, polynomial, radial basis function, andsigmoid.
In the question classification context, xiis represented by a set of binary features, for in-stance, the presence or absence of particular words.yi ?
{1,?1} indicates wether a question is of aparticular type or not.
Due to the large number offeatures in question classification, one may not needto map data to a higher dimensional space.
It hasbeen commonly accepted that the linear kernel ofK(xi,xj) = xiTxi is good enough for questionclassification.
In this paper, we adopt the LIBSVM(Chang and Lin, 2001) implementation in our exper-iments.2.2 Maximum Entropy ModelsMaximum entropy (ME) models (Berger et al,1996; Manning and Klein, 2003), also known as928log-linear and exponential learning models, providea general purpose machine learning technique forclassification and prediction which has been suc-cessfully applied to natural language processing in-cluding part of speech tagging, named entity recog-nition etc.
Maximum entropy models can inte-grate features from many heterogeneous informa-tion sources for classification.
Each feature corre-sponds to a constraint on the model.
In the context ofquestion classification, a sample feature could be thepresence of a particular word associated with a par-ticular question type.
The maximum entropy modelis the model with maximum entropy of all modelsthat satisfy the constraints.
In this paper, we adoptStanford Maximum Entropy (Manning and Klein,2003) implementation in our experiments.3 FeaturesEach question is represented as a bag of featuresand is feeded into classifiers in training stage.
Wepresent five binary feature sets, namely question wh-word, head word, WordNet semantic features forhead word, word grams, and word shape feature.The five feature sets will be separately used by theclassifiers to determine their individual contribution.In addition, these features are used in an incrementalfashion in our experiments.3.1 Question wh-wordThe wh-word feature is the question wh-word ingiven questions.
For example, the wh-word ofquestion What is the population of China is what.We have adopted eight question wh-words, namelywhat, which, when, where, who, how, why, and rest,with rest being the type does not belong to any ofthe previous type.
For example, the question Namea food high in zinc is a rest type question.3.2 Head WordLi and Roth (2002;2006) used head chunks as fea-tures.
The first noun chunk and the first verb chunkafter the question word in a sentence are definedas head chunks in their approach.
Krishnan et al(2005) used one contiguous span of tokens which isdenoted as the informer span as features.
In bothapproaches, noisy information could be introduced.For example, considering the question of What is agroup of turkeys called, both the head chunk and in-former span of this question is group of turkeys.
Theword of turkeys in the chunk (or span) contributes tothe classification of type ENTY:animal if the hyper-nyms of WordNet are employed (as described in nextsection).
However, the extra word group would in-troduce ambiguity to misclassify such question intoHUMAN:group, as all words appearing in chunk aretreated equally.
To tackle this problem, we pro-pose the feature of head word, which is one singleword specifying the object that the question seeks.In the previous example What is a group of turkeyscalled, the head word is exactly turkeys.
In doingso, no misleading word group is augmented.
An-other example is George Bush purchased a small in-terest in which baseball team.
The head chunk, in-former span and head word are baseball team, base-ball team and team respectively.
The extra wordbaseball in the head chunk and informer span maylead the question misclassified as ENTY:sport ratherthan HUM:group.
In most cases, the head chunkor informer span include head words.
The headchunk feature or informer span feature would bebeneficiary so long as the useful information plays astronger role than the misleading one.
Nevertheless,this is not as effective as the introduction of one headword.To obtain the head word feature, a syntactic parseris required.
A syntactic parser is a model that out-puts the grammatical structure of given sentences.There are accurate parsers available such as Cha-niak parser (Charniak and Johnson, 2005), Stan-ford parser (Klein and Manning, 2003) and Berkeleyparser (Petrov and Klein, 2007), among which weuse the Berkeley parser 2 to help identify the headword.
Figure 1 shows two example parse trees forquestions What year did the Titanic sink and What isthe sales tax in Minnesota respectively.Collins rules (Collins, 1999) can be applied toparse trees to extract the syntactic head words.
Forexample, the WHNP phrase (Wh-noun phrase) inthe top of Figure 1 takes its WP child as its headword, thus assigning the word what (in the bracket)which is associated with WP tag to the syntactichead word of WHNP phrase.
Such head word as-signment is carried out from the bottom up and theword did is extracted as the head word of the wholequestion.
Similarly, the word is is extracted as the2available at http://nlp.cs.berkeley.edu/Main.html#parsing929MinnesotaVPVBsinkROOTSBARQ(did)SQ(did)WHNP(What) .WP NNWhat year didVBD NP(Titanic)DT NNP?the TitanicROOTSBARQ(is)WHNP(What) .WPWhatSQ(is)VBZisNP(tax)NP(tax) PP(in)NNS NNDT?IN NPNNPsalesthe tax inFigure 1: Two example parse trees and their head wordsassignmentsyntactic head word in the bottom of Figure 1.Collins head words finder rules have been modi-fied to extract semantic head word (Klein and Man-ning, 2003).
To better cover the question sentences,we further re-define the semantic head finder rulesto fit our needs.
In particular, the rules to find thesemantic head word of phrases SBARQ (Clause in-troduced by subordinating conjunction), SQ (sub-constituent of SBARQ excluding wh-word or wh-phrase), VP (verb phrase) and SINV (declarativesentence with subject-aux inversion) are redefined,with the head preference of noun or noun phraserather than verb or verb phrase.
The new headword assignments for the previous two examples areshown in Figure 2.If the head word is any of name, type or kind etc,post fix is required to identify the real head word ifnecessary.
In particular, we compile a tree patternas shown in the left of Figure 3.
If this pattern ismatched against a given parse question parse tree,the head word is re-assigned to the head word of NPnode in the tree pattern.
For example, the initial headword extracted from parse tree of question What isthe proper name for a female walrus is name.
Assuch parse tree (as shown partially in the right ofFigure 3) matches the compiled tree pattern, the postoperation shall fix it to walrus, which is the headword of the NP in the tree pattern.
This post fix helpsclassify the question to ENTY:animal.MinnesotaVPVBsinkROOTSBARQ(year)SQ(Titanic)WHNP(year) .WP NNWhat year didVBD NP(Titanic)DT NNP?the TitanicROOTSBARQ(tax)WHNP(What) .WPWhatSQ(tax)VBZisNP(tax)NP(tax) PP(in)NNS NNDT?IN NPNNPsalesthe tax inFigure 2: Two example parse trees and their revised headwords assignmentwalrusPPIN NP*nametypekindgenregroupNPNPPPDT JJ NN IN NPDT JJ NNthe nameproper fora femaleFigure 3: Post fix for the head word assignmentIn addition to the question head word as describedabove, we introduce a few regular expression pat-terns to help question head word identification.
Notethat these patterns depend on the question type tax-onomy as shown in Table 3.
For example, consid-ering the questions of What is an atom and Whatare invertebrates, the head word of atom and in-vertebrates do not help classify such questions toDESC:def.
To resolve this, we create a binary fea-ture using a string regular expression which beginswith what is/are and follows by an optional a, an, orthe and then follows by one or two words.
If a ques-tion matches this regular expression, a binary feature(a placeholder word is used in implementation, forinstance DESC:def 1 in this case) would be insertedto the feature set of the question.
This feature, if itis beneficial, would be picked up by the classifiers(SVMs or MEs) in training.
We list all regular ex-pression patterns which are used in our experimentsas following:DESC:def pattern 1 The question begins with what is/are and follows930by an optional a, an, or the and then follows by one or two words.DESC:def pattern 2 The question begins with what do/does and endswith mean.ENTY:substance pattern The question begins with what is/are andends with composed of/made of/made out of.DESC:desc pattern The question begins with what does and endswith do.ENTY:term The question begins with what do you call.DESC:reason pattern 1 The question begins with what causes/cause.DESC:reason pattern 2 The question begins with What is/are andends with used for.ABBR:exp pattern The question begins with What does/do and endswith stand for.HUM:desc pattern The question begins with Who is/was and followsby a word starting with a capital letter.It is worth noting that all these patterns serve asfeature generators for given questions: the featurebecomes active if the pattern matches the ques-tions.
The algorithm to extract question head wordis shown in Algorithm 1.
There is no head wordreturned for when, where or why type questions, asthese hw-words are informative enough; the inclu-sion of other words would introduce noisy informa-tion.
If the question is of type how, the word follow-ing how is returned as head word.
The patterns arethen attempted to match the question if it is of typewhat or who.
If there is a match, the placehold wordfor such pattern (e.g., HUM:desc for HUM:desc pat-tern) is returned as head word.
If none of the abovecondition is met, the candidate head word is ex-tracted from the question parse tree using the rede-fined head finder rules.
Such extracted head word isreturned only if it has noun or noun phrase tag; oth-erwise the first word which has noun or noun phrasetag is returned.
The last step is a back up plan incase none of the previous procedure happens.3.3 WordNet Semantic FeatureWordNet (Fellbaum, 1998) is a large English lexiconin which meaningfully related words are connectedvia cognitive synonyms (synsets).
The WordNet isa useful tool for word semantics analysis and hasbeen widely used in question classification (Krish-nan et al, 2005; Schlaefer et al, 2007).
A naturalway to use WordNet is via hypernyms: Y is a hy-pernym of X if every X is a (kind of) Y.
For exam-ple, the question of What breed of hunting dog didAlgorithm 1 Question head word extractionRequire: Question qEnsure: Question head word1: if q.type == when|where|why then2: return null3: end if4: if q.type == how then5: return the word following word ?how?6: end if7: if q.type == what then8: for any aforementioned regular expression r (except HUM:descpattern) do9: if(q matches r)10: return r.placehold-word11: end for12: end if13: if q.type == who && q matches HUM:desc pattern then14: return ?HUM:desc?15: end if16: String candidate = head word extracted from question parse tree17: if candidate.tag starts with NN then18: return candidate19: end if20: return the first word whose tag starts with NNthe Beverly Hillbillies own requires the knowledgeof animal being the hypernym of dog.
In this paper,we propose two approaches to augment WordNet se-mantic features, with the first augmenting the hyper-nyms of head words as extracted in previous sectiondirectly, and the second making use of a WordNetsimilarity package (Seco et al, 2004), which implic-itly employs the structure of hypernyms.3.3.1 Direct Use of HypernymsIn WordNet, senses are organized into hierarchieswith hypernym relationships, which provides a nat-ural way to augment hypernyms features from theoriginal head word.
For example, the hierarchies fora noun sense of domestic dog is described as: dog?
domestic animal ?
animal, while another nounsense (a dull unattractive unpleasant girl or woman)is organized as dog ?
unpleasant woman ?
un-pleasant person.
In addition, a verb sense of dog isorganized as dog ?
pursue ?
travel.
In our first ap-proach, we attempt to directly introduce hypernymsfor the extracted head words.
The augment of hyper-nyms for given head word can introduce useful in-formation, but can also bring noise if the head wordor the sense of head word are not correctly identi-fied.
To resolve this, three questions shall be ad-dressed: 1) which part of speech senses should beaugmented?
2) which sense of the given word isneeded to be augmented?
and 3) how many depth931are required to tradeoff the generality (thus moreinformative) and the specificity (thus less noisy).The first question can be answered by mappingthe Penn Treebank part of speech tag of the givenhead word to its WordNet part of speech tag, whichis one of POS.NOUN, and POS.ADJECTIVE,POS.ADVERB and POS.VERB.
The second ques-tion is actually a word sense disambiguation (WSD)problem.
The Lesk algorithm (Lesk, 1986) is a clas-sical algorithm for WSD.
It is based on the assump-tion that words in a given context will tend to sharea common topic.
A basic implementation of the TheLesk algorithm is described as following:1.
Choosing pairs of ambiguous words within acontext2.
Checks their definitions in a dictionary3.
Choose the senses as to maximize the numberof common terms in the definitions of the cho-sen wordsIn our head word sense disambiguation, the contextwords are words (except the head word itself) in thequestion, and the dictionary is the gloss of a sensefor a given word.
Algorithm 2 shows the adaptedLesk algorithm which is employed in our system.Basically, for each sense of given head word, thisAlgorithm 2 Head word sense disambiguationRequire: Question q and its head word hEnsure: Disambiguated sense for h1: int count = 02: int maxCount = -13: sense optimum = null4: for each sense s for h do5: count = 06: for each context word w in q do7: int subMax = maximum number of common words in sdefinition (gloss) and definition of any sense of w8: count = count + sumMax9: end for10: if count > maxCount then11: maxCount = count12: optimum = s13: end if14: end for15: return optimumalgorithm computes the maximum number of com-mon words between gloss of this sense and gloss ofany sense of the context words.
Among all headword senses, the sense which results in the maxi-mum common words is chosen as the optimal senseto augment hypernyms later.
Finally the third ques-tion is answered via trail and error based on evaluat-ing randomly generated 10% data from the trainingdataset.
Generally speaking, if the identification ofthe head word is not accurate, it would brings signif-icant noisy information.
Our experiments show thatthe use of depth six produces the best results overthe validation dataset.
This indirectly proves that ourhead word feature is very accurate: the hypernymsintroduction within six depths would otherwise pol-lute the feature space.3.3.2 Indirect Use of HypernymsIn this approach, we make use of the WordNetSimilarity package (Seco et al, 2004), which im-plicitly employs WordNet hypernyms.
In particu-lar, for a given pair of words, the WordNet similar-ity package models the length of path traveling fromone word to the other over the WordNet network.It then computes the semantic similarity based onthe path.
For example, the similarity between carand automobile is 1.0, while the similarity betweenfilm and audience is 0.38.
For each question, weuse the WordNet similarity package to compute thesimilarity between the head word of such questionand each description word in a question categoriza-tion.
The description words for a question categoryare a few words (usually one to three) which explainthe semantic meaning of such a question category3.
For example, the descriptions words for categoryENTY:dismed are diseases and medicine.
The ques-tion category which has the highest similarity to thehead word is marked as a feature.
This is equal toa mini question classifier.
For example, as the headword walrus of question What is the proper namefor a female walrus has the highest similarity mea-sure to animals, which is a description word of cat-egory ENTY:animal, thus the ENTY:animal is in-serted into the feature set of the given question.3.4 N-GramsAn N-gram is a sub-sequence of N words from agiven question.
Unigram forms the bag of wordsfeature, and bigram forms the pairs of words fea-ture, and so forth.
We have considered unigram, bi-gram, and trigram features in our experiments.
Thereason to use such features is to provide word sense3available at http://12r.cs.uiuc.edu/?cogcomp/Data/QA/QC/definition.html932disambiguation for questions such as How long didRip Van Winkle sleep, as How long (captured bywh-word and head word features) could refer to ei-ther NUM:dist or NUM:period.
The word feature ofsleep help determine the NUM:period classification.3.5 Word ShapeWord shape in a given question may be useful forquestion classification.
For instance, the questionWho is Duke Ellington has a mixed shape (begins awith capital letter and follows by lower case letters)for Duke, which roughly serves as a named entityrecognizer.
We use five word shape features, namelyall upper case, all lower case, mixed case, all digits,and other.
The experiments show that this featureslightly boosts the accuracy.4 Experimental ResultsWe designed two experiments to test the accuracyof our classifiers.
The first experiment evaluates theindividual contribution of different feature types toquestion classification accuracy.
In particular, theSVM and ME are trained from the UIUC 5500 train-ing data using the following feature sets: 1) wh-word + head word, 2) wh-word + head word + directhypernym, 3) wh-wod + head word + indirect hyper-nym, 4) unigram, 5) bigram, 6) trigram, and 7) wordshape.
We set up the tests of 1), 2) and 3) due to thefact that wh-word and head word can be treated as aunit, and hypernym depends on head word.
In thesecond experiment, feature sets are incrementallyfeeded to the SVM and ME.
The parameters for bothSVM and ME classifiers (e.g., the C in the SVM)are all with the default values.
In order to facilitatethe comparison with previously reported results, thequestion classification performance is measured byaccuracy, i.e., the proportion of the correctly classi-fied questions among all test questions.4.1 Individual Feature ContributionTable 1 shows the question classification accuracyof SVM and ME using individual feature sets for6 coarse and 50 fine classes.
Among all featuresets, wh-word + head word proves to be very infor-mative for question classification.
Our first Word-Net semantic feature augment, the inclusion of di-rect hypernym, can further boost the accuracy in thefine classes for both SVM and ME, up to four per-Table 1: Question classification accuracy of SVM andME using individual feature sets for 6 and 50 classes overUIUC dataset6 class 50 classSVM ME SVM MEwh-word + head word 92.0 92.2 81.4 82.0wh-word + depth=1 92.0 91.8 84.6 84.8head word + depth = 3 92.0 92.2 85.4 85.4direct hypernym depth = 6 92.6 91.8 85.4 85.6wh-word + head 91.8 92.0 83.2 83.6+ indirect hypernymunigram 88.0 86.6 80.4 78.8bigram 85.6 86.4 73.8 75.2trigram 68.0 57.4 39.0 44.2word shape 18.8 18.8 10.4 10.4cent.
This phenomena conforms to Krishnan et al(2005) that WordNet hypernym benefits mainly onthe 50 fine classes classification.
Li and Roth (2006)made use of semantic features including named en-tities, WordNet senses, class-specific related words,and distributional similarity based categories.
Theirsystem managed to improve around 4 percent withthe help of those semantic features.
They reportedthat WordNet didn?t contribute much to the system,while our results show that the WordNet signifi-cantly boosts the accuracy.
The reason may be thattheir system expanded the hypernyms for each wordin the question, while ours only expanded the headword.
In doing so, the augmentation does not intro-duce much noisy information.
Notice that the inclu-sion of various depth of hypernyms results in differ-ent accuracy.
The depth of six brings the highest ac-curacy of 85.4% and 85.6% for SVM and ME under50 classes, which is very competitive to the previ-ously reported best accuracy of 86.2% (Krishnan etal., 2005).Our second proposed WordNet semantic feature,the indirect use of hypernym, does not perform asgood as the first approach; it only contributes theaccuracy gain of 1.8 and 1.6 in the fine classes forSVM and ME respectively.
The reason may be twofold: 1) the description words (usually one to threewords) of question categories are not representativeenough, and 2) the indirect use of hypernyms viathe WordNet similarity package is not as efficient asdirect use of hypernyms.Among the surface words features, unigram fea-ture perform the best with accuracy of 80.4% forSVM under 50 classes, and 88.0% for SVM under6 classes.
It is not surprising that the word shape933feature only achieves small gain in question classi-fication, as the use of five shape type does not pro-vide enough information for question classification.However, this feature is treated as an auxiliary one toboost a good classifier, as we will see in the secondexperiment.4.2 Incremental Feature ContributionBased on the individual feature contribution, wethen trained the SVMs and MEs using wh-word,head word, direct hypernyms (with depth 6) of headword, unigram, and word shape incrementally.
Table2 shows the question classification accuracy (bro-ken down by question types) of SVM and ME for 6coarse and 50 fine classes.
As can be seen, the maindifficulty for question classification lies in the whattype questions.
SVM and ME perform roughly iden-tical if they use the same features.
For both SVMand ME, the baseline using the wh-head word andhead word results in 81.4% and 82.0% respectivelyfor 50 fine class classification (92.0% and 92.2% for6 coarse classes).
The incremental use of hypernymfeature within 6 depths boost about four percent forboth SVM and ME under 50 classes, while slightgain or slight loss for SVM and ME for 6 coarseclasses.
The further use of unigram feature leads toanother three percent gain for both SVM and ME in50 classes.
Finally, the use of word shape leads toanother 0.6% accuracy increase for both SVM andME in 50 classes.
The best accuracy achieved for50 classes is 89.2% for SVM and 89.0% for ME.For 6 coarse classes, SVM and ME achieve the bestaccuracy of 93.4% and 93.6% respectively.Our best result feature space only consists of13?697 binary features and each question has 10 to30 active features.
Compared to the over feature sizeof 200?000 in Li and Roth (2002), our feature spaceis much more compact, yet turned out to be moreinformative as suggested by the experiments.Note that if we replace the bigram with unigram,SVM and ME achieve the overall accuracy of 88.4%and 88.0% respectively for 50 fine classes, and theuse of trigram leads SVM and ME to 86.6% and86.8% respectively.
The inclusion of unigram, bi-gram and trigram together won?t boost the accu-racy, which reflects the fact that the bigram and tri-gram features cannot bring more information giventhat unigram, wh-word and head word features arepresent.
This is because the useful informationwhich are supposed to be captured by bigram or tri-gram are effectively captured by wh-word and headword features.
The unigram feature thus outper-forms bigram and trigram due to the fact that it isless sparse.
In addition, if we replace the indirectuse of hypernym with the direct use of hypernym,the overall accuracy is 84.6% and 84.8% for SVMand ME respectively.
All these experiments conformto the individual features contributions as shown inTable 1.For a better understanding of the error distribu-tion with respect to the 50 question categories, Ta-ble 3 shows the precision and recall for each ques-tion type in the best result (89.2%) using SVM.It is not surprising that some of the categories aremore difficult to predict such as ENTY:other andENTY:product, while others are much easier suchas HUMAN:individual, since the former are moresemantically ambiguous than the latter.Table 3: Precision and recall for fine grained questioncategoriesClass # P R Class # P RABBR 9 desc 7 75.0 85.7abb 1 100 100 manner 2 100 100exp 8 88.9 100 reason 6 85.7 100ENTITY 94 HUMAN 65animal 16 94.1 100 group 6 71.4 83.3body 2 100 50.0 individual 55 94.8 100color 10 100 100 title 1 0.0 0.0creative 0 100 100 desc 3 100 100currency 6 100 100 LOC 81dis.med.
2 40.0 100 city 18 100 77.8event 2 100 50.0 country 3 100 100food 4 100 50.0 mountain 3 100 66.7instrument 1 100 100 other 50 83.9 94.0lang 2 100 100 state 7 85.7 85.7letter 0 100 100 NUM 113other 12 45.5 41.7 code 0 100 100plant 5 100 100 count 9 81.8 100product 4 100 25.0 date 47 95.9 100religion 0 100 100 distance 16 100 62.5sport 1 100 100 money 3 100 33.3substance 15 88.9 53.3 order 0 100 100symbol 0 100 100 other 12 85.7 50.0technique 1 100 100 period 8 72.7 100term 7 100 85.7 percent 3 75.0 100vehicle 4 100 75.0 speed 6 100 83.3word 0 100 100 temp 5 100 60.0DESC 138 size 0 100 100definition 123 89.0 98.4 weight 4 100 75.0Table 4 shows the summary of the classificationaccuracy of all models which were applied to UIUCdataset.
Note (1) that SNoW accuracy without therelated word dictionary was not reported.
Withthe semantically related word dictionary, it achieved91%.
Note (2) that SNoW with a semantically re-lated word dictionary achieved 84.2% but the otheralgorithms did not use it.
Our results are summa-rized in the last two rows.Our classifiers are able to classify some chal-934Table 2: Question classification accuracy of SVM and ME using incremental feature sets for 6 and 50 classes6 coarse classesType #Quest wh+headword +headword hypernym +unigram +word shapeSVM ME SVM ME SVM ME SVM MEwhat 349 88.8 89.1 89.7 88.5 89.7 90.3 90.5 91.1which 11 90.9 90.9 100 100 100 100 100 100when 26 100 100 100 100 100 100 100 100where 27 100 100 100 100 100 100 100 100who 47 100 100 100 100 100 100 100 100how 34 100 100 100 100 100 100 100 100why 4 100 100 100 100 100 100 100 100rest 2 100 100 50.0 50.0 100 50.0 100 50.0total 500 92.0 92.2 92.6 91.8 92.8 93.0 93.4 93.650 fine classesType #Quest wh+headword +headword hypernym +unigram +word shapeSVM ME SVM ME SVM ME SVM MEwhat 349 77.4 77.9 82.8 82.5 85.4 85.1 86.2 86.0which 11 81.8 90.9 81.8 90.9 90.9 100 90.9 100when 26 100 100 100 100 100 100 100 100where 27 92.6 92.6 92.6 92.6 92.6 92.6 92.6 92.6who 47 100 100 100 100 100 100 100 100how 34 76.5 76.5 76.5 79.4 97.1 91.2 97.1 91.2why 4 100 100 100 100 100 100 100 100rest 2 0.0 0.0 50.0 50.0 0.0 50.0 0.0 50.0total 500 81.4 82.0 85.4 85.6 88.6 88.4 89.2 89.0lenge questions.
For instance, the question Whatis the proper name for a female walrus has beencorrectly classified as ENTY:animal.
However, itstill has nearly ten percent error rate for 50 fineclasses.
The reason is three fold: 1) there are in-herently ambiguity in classifying a question.
Forinstance, the question What is mad cow disease, itcould be either of type DESC:def or ENTY:dismed;2) there are inconsistent labeling in the training dataand test data.
For instance, What is the popula-tion of Kansas is labeled with the type NUM:otherwhile What is the population of Arcadia , Floridais labeled with type NUM:count.
Another exam-ple, What county is Chicago in is labeled with typeLOC:other while What county is Phoenix , AZ in islabeled with type LOC:city; and 3) The parser canproduce incorrect parse tree which would result inwrong head word extraction.
For instance, the headword extracted from What is the speed humming-birds fly is hummingbirds (the correct one should bespeed), thus leading to the incorrect classification ofENTY:animal (rather than the correct NUM:speed).5 ConclusionIn contrast to Li and Roth (2006)?s approach whichmakes use of very rich feature space, we proposeda compact yet effective feature set.
In particular,we proposed head word feature and presented twoTable 4: Classification accuracy of all models which wereapplied to UIUC datasetAlgorithm 6 class 50 classLi and Roth, SNoW ?
(1) 78.8(2)Hacioglu et al, SVM+ECOC ?
80.2-82Zhang & Lee, Linear SVM 87.4 79.2Zhang & Lee, Tree SVM 90.0 ?Krishnan et al, SVM+CRF 93.4 86.2Linear SVM 93.4 89.2Maximum Entropy Model 93.6 89.0approaches to augment semantic features of suchhead words using WordNet.
In addition, Lesk?sword sense disambiguation algorithm was adaptedand the depth of hypernym feature was optimizedthrough cross validation, which was to introduceuseful information while not bringing too muchnoise.
With further augment of wh-word, unigramfeature, and word shape feature, we can obtain ac-curacy of 89.2% using linear SVMs, or 89.0% usingME for 50 fine classes.6 AcknowledgmentsThis research was supported by British Telecomgrant CT1080028046 and BISC Program of UCBerkeley.935ReferencesBerger, A. L., V. J. D. Pietra, and S. A. D. Pietra.
1996.A Maximum Entropy Approach to Natural LanguageProcessing.
Computational Linguistics, 22(1):39?71.Chang, C. C and C. J. Lin.
2001.
LIBSVM: a li-brary for support vector machines.
Software availableat http://www.csie.ntu.edu.tw/?cjlin/libsvm/.Charniak, E. and M. Johnson.
2005.
Coarse-to-Fine N-Best Parsing and MaxEnt Discriminative Reranking.The 43rd Annual Meeting on Association for Compu-tational Linguistics.Collins, M. 1999.
Head-Driven Statistical Models forNatural Language Parsing.
PhD thesis, University ofPennsylvania.Fellbaum, C. 1998.
An Electronic Lexical Database.The MIT press.Hacioglu, K. and W. Ward.
2003.
Question Classifica-tion with Support Vector Machines and Error Correct-ing Codes.
The Association for Computational Lin-guistics on Human Language Technology, vol.
2, pp.28?30.Hovy, E., L. Gerber, U. Hermjakob, C. Y. Lin, and D.Ravichandran.
2001.
Towards Semantics-based An-swer Pinpointing.
The conference on Human languagetechnology research (HLT), pp.
1?7.Khardon, R., D. Roth, and L. G. Valiant.
1999.
Rela-tional Learning for NLP using Linear Threshold Ele-ments.
The Conference on Artificial Intelligence, pp.911?919.Klein, D. and C. Manning.
2003.
Accurate Unlexical-ized Parsing.
The Association for Computational Lin-guistics, vol.
1, pp.
423?430.Krishnan, V., S. Das, and S. Chakrabarti.
2005.
En-hanced Answer Type Inference from Questions usingSequential Models.
The conference on Human Lan-guage Technology and Empirical Methods in NaturalLanguage Processing.Lesk, Michael.
1986.
Automatic Sense Disambiguationusing Machine Readable Dictionaries: how to tell apine cone from an ice cream cone.
ACM Special Inter-est Group for Design of Communication Proceedingsof the 5th annual international conference on Systemsdocumentation, pp.
24?26.Li, X. and D. Roth.
2002.
Learning Question Classifiers.The 19th international conference on Computationallinguistics, vol.
1, pp.
1?7.Li, X. and D. Roth.
2006.
Learning Question Classifiers:the Role of Semantic Information.
Natural LanguageEngineering, 12(3):229?249.Manning, C. and D. Klein.
2003.
Optimization, MaxentModels, and Conditional Estimation without Magic.Tutorial at HLT-NAACL 2003 and ACL 2003.Petrov, S. and D. Klein.
2007.
Improved Inference forUnlexicalized Parsing.
HLT-NAACL.Pinto, D., M. Branstein, R. Coleman, W. B. Croft, M.King, W. Li, and X. Wei.
2002.
QuASM: A Systemfor Question Answering using semi-structured DataThe 2nd ACM/IEEE-CS joint conference on Digital li-braries..Radev, D., W. Fan, H. Qi, H. Wu, and A. Grewal.
2002.Probabilistic question answering on the web.
The 11thinternational conference on World Wide Web.Schlaefer, N., J. Ko, J. Betteridge, S. Guido, M. Pathak,and E. Nyberg.
2007.
Semantic Extensions of theEphyra QA System for TREC 2007.
The SixteenthText REtrieval Conference (TREC).Seco, N., T. Veale, and J. Hayes.
2004.
An IntrinsicInformation Content Metric for Semantic Similarity inWordNet.
Proceedings of the European Conference ofArtificial Intelligence.Vapnik, V. N. 1995.
The Nature of Statistical LearningTheory.
Springer-Verlag New York.Voorhees, E. M. and H. T. Dang.
2005.
Overview ofthe TREC 2005 Question Answering Track.
The TextRetrieval Conference (TREC2005).Zhang D. and W. S. Lee.
2003.
Question Classificationusing Support Vector Machines.
The ACM SIGIR con-ference in informaion retrieval, pp.
26?32.936
