Evaluation and Extension of Maximum Entropy Modelswith Inequality ConstraintsJun?ichi Kazama?kazama@is.s.u-tokyo.ac.jp?Department of Computer ScienceUniversity of TokyoHongo 7-3-1, Bunkyo-ku,Tokyo 113-0033, JapanJun?ichi Tsujii?
?tsujii@is.s.u-tokyo.ac.jp?CREST, JST(Japan Science and Technology Corporation)Honcho 4-1-8, Kawaguchi-shi,Saitama 332-0012, JapanAbstractA maximum entropy (ME) model is usu-ally estimated so that it conforms to equal-ity constraints on feature expectations.However, the equality constraint is inap-propriate for sparse and therefore unre-liable features.
This study explores anME model with box-type inequality con-straints, where the equality can be vio-lated to reflect this unreliability.
We eval-uate the inequality ME model using textcategorization datasets.
We also proposean extension of the inequality ME model,which results in a natural integration withthe Gaussian MAP estimation.
Experi-mental results demonstrate the advantageof the inequality models and the proposedextension.1 IntroductionThe maximum entropy model (Berger et al, 1996;Pietra et al, 1997) has attained great popularity inthe NLP field due to its power, robustness, and suc-cessful performance in various NLP tasks (Ratna-parkhi, 1996; Nigam et al, 1999; Borthwick, 1999).In the ME estimation, an event is decomposedinto features, which indicate the strength of certainaspects in the event, and the most uniform modelamong the models that satisfy:Ep?
[fi] = Ep[fi], (1)for each feature.
Ep?
[fi] represents the expectationof feature fi in the training data (empirical expec-tation), and Ep[fi] is the expectation with respectto the model being estimated.
A powerful and ro-bust estimation is possible since the features can beas specific or general as required and does not needto be independent of each other, and since the mostuniform model avoids overfitting the training data.In spite of these advantages, the ME model stillsuffers from a lack of data as long as it imposes theequality constraint (1), since the empirical expecta-tion calculated from the training data of limited sizeis inevitably unreliable.
A careful treatment is re-quired especially in NLP applications since the fea-tures are usually very sparse.
In this study, text cat-egorization is used as an example of such tasks withsparse features.Previous work on NLP proposed several solutionsfor this unreliability such as the cut-off, which sim-ply omits rare features, the MAP estimation withthe Gaussian prior (Chen and Rosenfeld, 2000), thefuzzy maximum entropy model (Lau, 1994), and fatconstraints (Khudanpur, 1995; Newman, 1977).Currently, the Gaussian MAP estimation (com-bined with the cut-off) seems to be the most promis-ing method from the empirical results.
It succeededin language modeling (Chen and Rosenfeld, 2000)and text categorization (Nigam et al, 1999).
Asdescribed later, it relaxes constraints like Ep?
[fi] ?Ep[fi] =?i?2, where ?i is the model?s parameter.This study follows this line, but explores the fol-lowing box-type inequality constraints:Ai ?
Ep?
[fi] ?
Ep[fi] ?
?Bi, Ai, Bi > 0.
(2)Here, the equality can be violated by the widths Aiand Bi.
We refer to the ME model with the aboveinequality constraints as the inequality ME model.This inequality constraint falls into a type of fat con-straints, ai ?
Ep[fi] ?
bi, as suggested by (Khudan-pur, 1995).
However, as noted in (Chen and Rosen-feld, 2000), this type of constraint has not yet beenapplied nor evaluated for NLPs.The inequality ME model differs from the Gaus-sian MAP estimation in that its solution becomessparse (i.e., many parameters become zero) as a re-sult of optimization with inequality constraints.
Thefeatures with a zero parameter can be removed fromthe model without changing its prediction behavior.Therefore, we can consider that the inequality MEmodel embeds feature selection in its estimation.Recently, the sparseness of the solution has been rec-ognized as an important concept in constructing ro-bust classifiers such as SVMs (Vapnik, 1995).
Webelieve that the sparse solution improves the robust-ness of the ME model as well.We also extend the inequality ME model so thatthe constraint widths can move using slack vari-ables.
If we penalize the slack variables by their 2-norm, we obtain a natural integration of the inequal-ity ME model and the Gaussian MAP estimation.While it incorporates the quadratic stabilization ofthe parameters as in the Gaussian MAP estimation,the sparseness of the solution is preserved.We evaluate the inequality ME models empiri-cally, using two text categorization datasets.
Theresults show that the inequality ME models outper-form the cut-off and the Gaussian MAP estimation.Such high accuracies are achieved with a fairly smallnumber of active features, indicating that the sparsesolution can effectively enhance the performance.
Inaddition, the 2-norm extended model is shown to bemore robust in several situations.2 The Maximum Entropy ModelThe ME estimation of a conditional model p(y|x)from the training examples {(xi, yi)} is formulatedas the following optimization problem.1maximizepH(p) =?xp?
(x)?yp(y|x) log p(y|x)subject to Ep?[fi]?
Ep[fi] = 0 1 ?
i ?
F. (3)1To be precise, we have also the constraintsPyp(y|x) ?1 = 0 x ?
X .
Note that although we explain using a condi-tional model throughout the paper, the discussion can be appliedeasily to a joint model by considering the condition x is fixed.The empirical expectations and model expectationsin the equality constraints are defined as follows.Ep?
[fi] =?x p?
(x)?y p?
(y|x)fi(x, y), (4)Ep[fi] =?x p?
(x)?y p(y|x)fi(x, y), (5)p?
(x) = c(x)/L, p?
(y|x) = c(x, y)/c(x), (6)where c(?)
indicates the number of times ?
occurredin the training data, and L is the number of trainingexamples.By the Lagrange method, p(y|x) is found to havethe following parametric form:p?
(y|x) =1Z(x)exp(?i?ifi(x, y)), (7)where Z(x) =?y exp(?i ?ifi(x, y)).
The dualobjective function becomes:L(?)
=?x p?
(x)?y p?
(y|x)?i ?ifi(x, y) (8)?
?x p?
(x) log?y exp(?i ?ifi(x, y)).The ME estimation becomes the maximization ofL(?).
And it is equivalent to the maximization of thelog-likelihood: LL(?)
= log?x,y p?(y|x)p?
(x,y).This optimization can be solved using algo-rithms such as the GIS algorithm (Darroch and Rat-cliff, 1972) and the IIS algorithm (Pietra et al,1997).
In addition, gradient-based algorithms canbe applied since the objective function is concave.Malouf (2002) compares several algorithms for theME estimation including GIS, IIS, and the limited-memory variable metric (LMVM) method, which isa gradient-based method, and shows that the LMVMmethod requires much less time to converge for realNLP datasets.
We also observed that the LMVMmethod converges very quickly for the text catego-rization datasets with an improvement in accuracy.Therefore, we use the LMVM method (and its vari-ant for the inequality models) throughout the exper-iments.
Thus, we only show the gradient when men-tioning the training.
The gradient of the objectivefunction (8) is computed as:?L(?)?
?i= Ep?[fi]?
Ep[fi].
(9)3 The Inequality ME ModelThe maximum entropy model with the box-type in-equality constraints (2) can be formulated as the fol-lowing optimization problem:maximizep?xp?
(x)?yp(y|x) log p(y|x),subject to Ep?[fi]?
Ep[fi]?
Ai ?
0, (10)Ep[fi]?
Ep?[fi]?
Bi ?
0.
(11)By using the Lagrange method for optimizationproblems with inequality constraints, the followingparametric form is derived.p?,?
(y|x) =1Z(x)exp(?i(?i ?
?i)fi(x, y)),?i ?
0, ?i ?
0, (12)where parameters ?i and ?i are the Lagrange mul-tipliers corresponding to constraints (10) and (11).The Karush-Kuhn-Tucker conditions state that, atthe optimal point,?i(Ep?[fi]?
Ep[fi]?
Ai) = 0,?i(Ep[fi]?
Ep?[fi]?
Bi) = 0.These conditions mean that the equality constraint ismaximally violated when the parameter is non-zero,and if the violation is strictly within the widths, theparameter becomes zero.
We call a feature upperactive when ?i > 0, and lower active when ?i > 0.When ?i?
?i = 0, we call that feature active.2 Inac-tive features can be removed from the model withoutchanging its behavior.
Since Ai >0 and Bi >0, anyfeature should not be upper active and lower activeat the same time.3The inequality constraints together with the con-straints?y p(y|x)?
1 = 0 define the feasible re-gion in the original probability space, on which theentropy varies and can be maximized.
The largerthe widths, the more the feasible region is enlarged.Therefore, it can be implied that the possibility of afeature becoming inactive (the global maximal pointis strictly within the feasible region with respectto that feature?s constraints) increases if the corre-sponding widths become large.2The term ?active?
may be confusing since in the ME re-search, a feature is called active when fi(x, y) > 0 for anevent.
However, we follow the terminology in the constrainedoptimization.3This is only achieved with some tolerance in practice.The solution for the inequality ME model wouldbecome sparse if the optimization determines manyfeatures as inactive with given widths.
The relationbetween the widths and the sparseness of the solu-tion is shown in the experiment.The dual objective function becomes:L(?, ?)
=?x p?
(x)?y p?
(y|x)?i(?i ?
?i)fi(x, y)?
?x p?
(x) log?y exp(?i(?i ?
?i)fi(x, y))?
?i ?iAi ?
?i ?iBi.
(13)Thus, the estimation is formulated as:maximize?i?0,?i?0L(?, ?
).Unlike the optimization in the standard maximumentropy estimation, we now have bound constraintson parameters which state that parameters must benon-negative.
In addition, maximizing L(?, ?)
is nolonger equivalent to maximizing the log-likelihoodLL(?, ?).
Instead, we maximize:LL(?, ?)
?
?i ?iAi ?
?i ?iBi.
(14)Although we can use many optimization algorithmsto solve this dual problem since the objective func-tion is still concave, a method that supports boundedparameters must be used.
In this study, we use theBLMVM algorithm (Benson and More?, ), a variantof the limited-memory variable metric (LMVM) al-gorithm, which supports bound constraints.4The gradient of the objective function is:?L(?,?)?
?i= Ep?
[fi] ?
Ep[fi] ?
Ai,?L(?,?)?
?i= Ep[fi] ?
Ep?
[fi] ?
Bi.
(15)4 Soft Width ExtensionIn this section, we present an extension of the in-equality ME model, which we call soft width.
Thesoft width allows the widths to move as Ai + ?iand ?Bi ?
?i using slack variables, but with somepenalties in the objective function.
This soft widthextension is analogous to the soft margin extensionof the SVMs, and in fact, the mathematical discus-sion is similar.
If we penalize the slack variables4Although we consider only the gradient-based method hereas noted earlier, an extension of GIS or IIS to support boundedparameters would also be possible.by their 2-norm, we obtain a natural combination ofthe inequality ME model and the Gaussian MAP es-timation.
We refer to this extension using 2-normpenalty as the 2-norm inequality ME model.
As theGaussian MAP estimation has been shown to be suc-cessful in several tasks, it should be interesting em-pirically, as well as theoretically, to incorporate theGaussian MAP estimation into the inequality model.We first review the Gaussian MAP estimation in thefollowing, and then we describe our extension.4.1 The Gaussian MAP estimationIn the Gaussian MAP ME estimation (Chen andRosenfeld, 2000), the objective function is:LL(?)
?
?i(12?2i)?2i , (16)which is derived as a consequence of maximizingthe log-likelihood of the posterior probability, usinga Gaussian distribution centered around zero withthe variance ?2i as a prior on parameters.
The gra-dient becomes:?L(?)?
?i= Ep?[fi]?
Ep[fi]??i?2i.
(17)At the optimal point, Ep?
[fi] ?
Ep[fi] ?
?i?2i= 0.Therefore, the Gaussian MAP estimation can also beconsidered as relaxing the equality constraints.
Thesignificant difference between the inequality MEmodel and the Gaussian MAP estimation is that theparameters are stabilized quadratically in the Gaus-sian MAP estimation (16), while they are stabilizedlinearly in the inequality ME model (14).4.2 2-norm penalty extensionOur 2-norm extension to the inequality ME model isas follows.5maximizep,?,?H(p)?
C1?i ?i2?
C2?i ?2i ,subject to Ep?
[fi] ?
Ep[fi] ?
Ai ?
?i, (18)Ep[fi] ?
Ep?
[fi] ?
Bi ?
?i, (19)5It is also possible to impose 1-norm penalties in the objec-tive function.
It yields an optimization problem which is iden-tical to the inequality ME model except that the parameters areupper-bounded as 0 ?
?i?
C1and 0 ?
?i?
C2.
We will notinvestigate this 1-norm extension in this paper and leave it forfuture research.where C1and C2is the penalty constants.
The para-metric form is identical to the inequality ME model(12).
However, the dual objective function becomes:LL(?, ?)
?
?i(?iAi +?2i4C1)?
?i(?iBi +?2i4C2).Accordingly, the gradient becomes:?L(?,?)?
?i= Ep?
[fi] ?
Ep[fi] ?
(Ai +?i2C1),?L(?,?)?
?i= Ep[fi]?
Ep?[fi]?
(Bi +?i2C2).
(20)It can be seen that this model is a natural combina-tion of the inequality ME model and the GaussianMAP estimation.
It is important to note that the so-lution sparseness is preserved in the above model.5 Calculation of the Constraint WidthThe widths, Ai and Bi, in the inequality constraintsare desirably widened according to the unreliabilityof the feature (i.e., the unreliability of the calculatedempirical expectation).
In this paper, we examinetwo methods to determine the widths.The first is to use a common width for all featuresfixed by the following formula.Ai = Bi = W ?1L, (21)where W is a constant, width factor, to control thewidths.
This method can only capture the global re-liability of all the features.
That is, only the reli-ability of the training examples as a whole can becaptured.
We call this method single.The second, which we call bayes, is a method thatdetermines the widths based on the Bayesian frame-work to differentiate between the features dependingon their reliabilities.For many NLP applications including text catego-rization, we use the following type of features.fj,i(x, y) = hi(x) if y = yj, 0 otherwise.
(22)In this case, if we assume the approximation,p?
(y|x) ?
p?
(y|hi(x) > 0), the empirical expectationcan be interpreted as follows.6Ep?
[fj,i]=?x: hi(x)>0p?(x)p?
(y = yj|hi(x)>0)hi(x).6This is only for estimating the unreliability, and is not usedto calculate the actual empirical expectations in the constraints.Here, a source of unreliability is p?(y|hi(x)>0).
Weconsider p?
(y|hi(x) > 0) as the parameter ?
of theBernoulli trials.
That is, p(y|hi(x) > 0) = ?
andp(y?|hi(x)>0) = 1 ?
?.
Then, we estimate the pos-terior distribution of ?
from the training examplesby Bayesian estimation and utilize the variance ofthe distribution.
With the uniform distribution as theprior, k times out of n trials give the posterior distri-bution: p(?)
= Be(1+k, 1+n?k), where Be(?, ?
)is the beta distribution.
The variance is calculated asfollows.V [?]
=(1+k)(1+n?k)(2+n)2(n+3).
(23)Letting k = c(fj,i(x, y)>0) and n = c(hi(x)>0),we obtain fine-grained variances narrowed accord-ing to c(hi(x) > 0) instead of a single value, whichjust captures the global reliability.
Assuming the in-dependence of training examples, the variance of theempirical expectation becomes:V[Ep?
[fj,i]]=[?x: hi(x)>0 {p?
(x)hi(x)}2]V [?j,i].Then, we calculate the widths as follows:Ai = Bi = W ??V[Ep?[fj,i]].
(24)6 ExperimentsFor the evaluation, we use the ?Reuters-21578, Dis-tribution 1.0?
dataset and the ?OHSUMED?
dataset.The Reuters dataset developed by David D. Lewisis a collection of labeled newswire articles.7 Weadopted ?ModApte?
split to split the collection,and we obtained 7, 048 documents for training, and2, 991 documents for testing.
We used 112 ?TOP-ICS?
that actually occurred in the training set as thetarget categories.The OHSUMED dataset (Hersh et al, 1994) is acollection of clinical paper abstracts from the MED-LINE database.
Each abstract is manually assignedMeSH terms.
We simplified a MeSH term, like?A/B/C ?
A?, and used the most frequent 100simplified terms as the target categories.
We ex-tracted 9, 947 abstracts for training, and 9, 948 ab-stracts for testing from the file ?ohsumed.91.
?A documents is converted to a bag-of-words vec-tor representation with TFIDF values, after the stop7Available from http://www.daviddlewis.com/resources/words are removed and all the words are downcased.Since the text categorization task requires that mul-tiple categories are assigned if appropriate, we con-structed a binary categorizer, pc(y ?
{+1,?1}|d),for each category c. If the probability pc(+1|d) isgreater than 0.5, the category is assigned.
To con-struct a conditional maximum entropy model, weused the feature function of the form (22), wherehi(d) returns the TFIDF value of the i-th word ofthe document vector.We implemented the estimation algorithms as anextension of an ME estimation tool, Amis,8 usingthe Toolkit for Advanced Optimization (TAO) (Ben-son et al, 2002), which provides the LMVM and theBLMVM optimization modules.
For the inequal-ity ME estimation, we added a hook that checks theKKT conditions after the normal convergence test.9We compared the following models:?
ME models only with cut-off (cut-off ),?
ME models with cut-off and the Gaussian MAPestimation (gaussian),?
Inequality ME models (ineq),?
Inequality ME models with 2-norm extensiondescribed in Section 4 (2-norm),10For the inequality ME models, we compared the twomethods to determine the widths, single and bayes,as described in Section 5.
Although the GaussianMAP estimation can use different ?i for each fea-ture, we used a common variance ?
for gaussian.Thus, gaussian roughly corresponds to single in theway of dealing with the unreliability of features.Note that, for inequality models, we started withall possible features and rely on their ability to re-move unnecessary features automatically by solu-tion sparseness.
The average maximum number offeatures in a categorizer is 63, 150.0 for the Reutersdataset and 116, 452.0 for the OHSUMED dataset.8Developed by Yusuke Miyao so as to support variousME estimations such as the efficient estimation with compli-cated event structures (Miyao and Tsujii, 2002).
Available athttp://www-tsujii.is.s.u-tokyo.ac.jp/?yusuke/amis9The tolerance for the normal convergence test (relative im-provement) and the KKT check is 10?4.
We stop the training ifthe KKT check has been failed many times and the ratio of thebad (upper and lower active) features among the active featuresis lower than 0.01.10Here, we fix the penalty constants C1= C2= 1016.0.80.8050.810.8150.820.8250.830.8350.840.8450.851e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1Accuracy(F-score)Width FactorABCDA: ineq + singleB: 2-norm + singleC: ineq + bayesD: 2-norm + bayescut-off bestgaussian best(a) Reuters0.540.550.560.570.580.590.60.610.621e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1 100Accuracy(F-score)Width FactorABCDA: ineq + singleB: 2-norm + singleC: ineq + bayesD: 2-norm + bayescut-off bestgaussian best(b) OHSUMEDFigure 1: Accuracies as a function of the width factor W for the development sets.0100002000030000400005000060000700001e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1#ofActiveFeaturesWidth FactorABCDA: ineq + singleB: 2-norm + singleC: ineq + bayesD: 2-norm + bayes(a) Reuters0200004000060000800001000001200001e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1 100#ofActiveFeaturesWidth FactorABCDA: ineq + singleB: 2-norm + singleC: ineq + bayesD: 2-norm + bayes(b) OHSUMEDFigure 2: The average number of active features as a function of width factor W .6.1 ResultsWe first found the best values for the control param-eters of each model, W , ?, and the cut-off threshold,by using the development set.
We show that the in-equality models outperform the other methods in thedevelopment set.
We then show that these values arevalid for the evaluation set.
We used the first half ofthe test set as the development set, and the secondhalf as the evaluation set.Figure 1 shows the accuracies of the inequalityME models for various width factors.
The accura-cies are presented by the ?micro averaged?
F-score.The horizontal lines show the highest accuracies ofcut-off and gaussian models found by exhaustivesearch.
For cut-off, we varied the cut-off thresh-old and found the best threshold.
For gaussian, wevaried ?
with each cut-off threshold, and found thebest ?
and cut-off combination.
We can see thatthe inequality models outperform the cut-off methodand the Gaussian MAP estimation with an appro-priate value for W in both datasets.
Although theOHSUMED dataset seems harder than the Reutersdataset, the improvement in the OHSUMED datasetis greater than that in the Reuters dataset.
This maybe because the OHSUMED dataset is more sparsethan the Reuters dataset.
The 2-norm extensionboosts the accuracies, especially for bayes, at themoderate W s (i.e., with the moderate numbers ofactive features).
However, we can not observe theapparent advantage of the 2-norm extension in termsof the highest accuracy here.Figure 2 shows the average number of active fea-tures of each inequality ME model for various widthfactors.
We can see that active features increase0.790.80.810.820.830.840.85100  1000  10000Accuracy(F-score)# of Active FeaturesBDFEB: 2-norm + singleD: 2-norm + bayesE: cut-offF: gaussian(a) Reuters0.540.550.560.570.580.590.60.610.621000  10000  100000Accuracy(F-score)# of Active FeaturesBDF EB: 2-norm + singleD: 2-norm + bayesE: cut-offF: gaussian(b) OHSUMEDFigure 3: Accuracies as a function of the average number of active features for the development sets.
Forgaussian, the accuracy with the best ?
found by exhaustive search is shown for each cut-off threshold.when the widths become small as expected.Figure 3 shows the accuracy of each model as afunction of the number of active features.
We cansee that the inequality ME models achieve the high-est accuracy with a fairly small number of active fea-tures, removing unnecessary features on their own.Besides, they consistently achieve much higher ac-curacies than the cut-off and the Gaussian MAP es-timation with a small number of features.Table 1 summarizes the above results includingthe best control parameters for the development set,and shows how well each method performs for theevaluation set with these parameters.
We can see thatthe best parameters are valid for the evaluation sets,and the inequality ME models outperform the othermethods in the evaluation set as well.
This meansthat the inequality ME model is generally superiorto the cut-off method and the Gaussian MAP estima-tion.
At this point, the 2-norm extension shows theadvantage of being robust, especially for the Reutersdataset.
That is, the 2-norm models outperform thenormal inequality models in the evaluation set.
Tosee the reason for this, we show the average crossentropy of each inequality model as a function ofthe width factor in Figure 4.
The average cross en-tropy was calculated as ?
1C?c1L?i log pc(yi|di),where C is the number of categories.
The cross en-tropy of the 2-norm model is consistently more sta-ble than that of the normal inequality model.
Al-though there is no simple relation between the abso-lute accuracy and the cross entropy, this consistentdifference can be one explanation for the advantageof the 2-norm extension.
Besides, it is possible thatthe effect of 2-norm extension appears more clearlyin the Reuters dataset because the robustness is moreimportant in the Reuters dataset since the develop-ment set is rather small and easy to overfit.Lastly, we could not observe the advantage ofbayes method in these experiments.
However, sinceour method is still in development, it is prematureto conclude that the idea of using different widthsaccording to its unreliability is not successful.
It ispossible that the uncertainty of p?
(x), which were notconcerned about, is needed to be modeled, or theBernoulli trial assumption is inappropriate.
Furtherinvestigation on these points must be done.7 Conclusion and Future WorkWe have shown that the inequality ME modelsoutperform the cut-off method and the GaussianMAP estimation, using the two text categoriza-tion datasets.
Besides, the inequality ME modelsachieved high accuracies with a small number offeatures due to the sparseness of the solution.
How-ever, it is an open question how the inequality MEmodel differs from other sophisticated methods offeature selection based on other criteria.Future work will investigate the details of the in-equality model including the effect of the penaltyconstants of the 2-norm extension.
Evaluations onother NLP tasks are also planned.
In addition, weneed to analyze the inequality ME model further toTable 1: The summary of the experiments.Reuters OHSUMEDbest setting # active feats acc (dev) acc (eval) best setting # active feats acc (dev) acc (eval)cut-off cthr=2 16, 961.9 83.24 86.38 cthr=0 116, 452.0 58.83 58.35gaussian cthr=3, ?=4.22E3 12, 326.6 84.01 87.04 cthr=8, ?=2.55E3 10, 154.7 59.53 59.08ineq+single W =1.78E?11 9, 479.9 84.47 87.41 W =4.22E?2 1, 375.5 61.23 61.102-norm+single W =5.62E?11 6, 611.1 84.35 87.59 W =4.50E?2 1, 316.5 61.26 61.23ineq+bayes W =3.16E?15 63, 150.0 84.21 87.37 W =9.46 1, 136.6 60.65 60.312-norm+bayes W =3.16E?9 10, 022.3 84.01 87.57 W =9.46 1, 154.5 60.67 60.3200.020.040.060.080.10.120.140.160.181e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1Avg.EntropyWidth FactorABCDA: ineq + singleB: 2-norm + singleC: ineq + bayesD: 2-norm + bayes(a) Reuters00.20.40.60.811.21.41.61.821e-16 1e-14 1e-12 1e-10 1e-08 1e-06 1e-04 0.01 1 100Avg.EntropyWidth FactorABCDA: ineq + singleB: 2-norm + singleC: ineq + bayesD: 2-norm + bayes(b) OHSUMEDFigure 4: W vs. the average cross entropy for the development sets.clarify the reasons for its success.Acknowledgments We would like to thankYusuke Miyao, Yoshimasa Tsuruoka, and theanonymous reviewers for many helpful comments.ReferencesS.
J. Benson and J. J.
More?.
A limited memory variable metricmethod for bound constraint minimization.
Technical Re-port ANL/MCS-P909-0901, Argonne National Laboratory.S.
Benson, L. C. McInnes, J. J.
More?, and J. Sarich.
2002.TAO users manual.
Technical Report ANL/MCS-TM-242-Revision 1.4, Argonne National Laboratory.A.
L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996.
Amaximum entropy approach to natural language processing.Computational Linguistics, 22(1):39?71.A.
Borthwick.
1999.
A maximum entropy approach to namedentity recognition.
Ph.D. Thesis.
New York University.S.
F. Chen and R. Rosenfeld.
2000.
A survey of smoothingtechniques for ME models.
IEEE Trans.
on Speech and Au-dio Processing, 8(1):37?50.J.
N. Darroch and D. Ratcliff.
1972.
Generalized iterativescaling for log-linear models.
The Annals of MathematicalStatistics, 43:1470?1480.W.
Hersh, C. Buckley, T.J. Leone, and D. Hickam.
1994.OHSUMED: An interactive retrieval evaluation and newlarge test collection for research.
In Proc.
of the 17th An-nual ACM SIGIR Conference, pages 192?201.S.
Khudanpur.
1995.
A method of ME estimation with re-laxed constraints.
In Johns Hopkins Univ.
Language Model-ing Workshop, pages 1?17.R.
Lau.
1994.
Adaptive statistical language modeling.
A Mas-ter?s Thesis.
MIT.R.
Malouf.
2002.
A comparison of algorithms for maximumentropy parameter estimation.
In Proc.
of the sixth CoNLL.Y.
Miyao and J. Tsujii.
2002.
Maximum entropy estimation forfeature forests.
In Proc.
of HLT 2002.W.
Newman.
1977.
Extension to the ME method.
In IEEETrans.
on Information Theory, volume IT-23, pages 89?93.K.
Nigam, J. Lafferty, and A. McCallum.
1999.
Using maxi-mum entropy for text classification.
In IJCAI-99 Workshopon Machine Learning for Information Filtering, pages 61?67.S.
Pietra, V. Pietra, and J. Lafferty.
1997.
Inducing features ofrandom fields.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 19(4):380?393.A.
Ratnaparkhi.
1996.
A maximum entropy model for part-of-speech tagging.
In Proc.
of the EMNLP, pages 133?142.V.
Vapnik.
1995.
The Nature of Statistical Learning Theory.Springer Verlag.
