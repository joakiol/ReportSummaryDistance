Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 625?630,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Scalable Probabilistic Classifier for Language ModelingJoel LangInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB, UKJ.Lang-3@sms.ed.ac.ukAbstractWe present a novel probabilistic classifier,which scales well to problems that involve alarge number of classes and require training onlarge datasets.
A prominent example of such aproblem is language modeling.
Our classifieris based on the assumption that each featureis associated with a predictive strength, whichquantifies how well the feature can predict theclass by itself.
The predictions of individualfeatures can then be combined according totheir predictive strength, resulting in a model,whose parameters can be reliably and effi-ciently estimated.
We show that a generativelanguage model based on our classifier consis-tently matches modified Kneser-Ney smooth-ing and can outperform it if sufficiently richfeatures are incorporated.1 IntroductionA Language Model (LM) is an important compo-nent within many natural language applications in-cluding speech recognition and machine translation.The task of a generative LM is to assign a probabil-ity p(w) to a sequence of words w = w1 .
.
.
wL.
Itis common to factorize this probability asp(w) =L?i=1p(wi|wi?N+1 .
.
.
wi?1) (1)Thus, the central problem that arises from thisformulation consists of estimating the probabilityp(wi|wi?N+1 .
.
.
wi?1).
This can be viewed as aclassification problem in which the target word Wicorresponds to the class that must be predicted,based on features extracted from the conditioningcontext, e.g.
a word occurring in the context.This paper describes a novel approach for mod-eling such conditional probabilities.
We propose aclassifier which is based on the assumption that eachfeature has a predictive strength, quantifying howwell the feature can predict the class (target word)by itself.
Then the predictions made by individualfeatures can be combined into a mixture model, inwhich the prediction of each feature is weighted ac-cording to its predictive strength.
This reflects thefact that certain features (e.g.
certain context words)are much more predictive than others but the pre-dictive strength for a particular feature often doesn?tvary much across classes and can thus be assumedconstant.
The main advantage of our model is that itis straightforward to incorporate rich features with-out sacrificing scalability or reliability of parame-ter estimation.
In addition, it is simple to imple-ment and no feature selection is required.
Section 3shows that a generative1 LM built with our classi-fier is competitive to modified Kneser-Ney smooth-ing and can outperform it if sufficiently rich featuresare incorporated.The classification-based approach to languagemodeling was introduced by Rosenfeld (1996) whoproposed an optimized variant of the maximum-entropy classifier (Berger et al, 1996) for the task.Unfortunately, data sparsity resulting from the largenumber of classes makes it difficult to obtain reli-able parameter estimates, even on large datasets andthe high computational costs make it difficult trainmodels on large datasets in the first place2.
Scal-1While the classifier itself is discriminative, i.e.
condition-ing on the contextual features, the resulting LM is generative.See Roark et al (2007) for work on discriminative LMs.2For example, using a vocabulary of 20000 words Rosen-feld (1994) trained his model on up to 40M words, howeveremploying heavy feature pruning and indicating that ?the com-putational load, was quite severe for a system this size?.625ability is however very important, since moving tolarger datasets is often the simplest way to obtaina better model.
Similarly, neural probabilistic LMs(Bengio et al, 2003) don?t scale very well to largedatasets.
Even the more scalable variant proposedby Mnih and Hinton (2008) is trained on a datasetconsisting of only 14M words, also using a vocabu-lary of around 20000 words.
Van den Bosch (2005)proposes a decision-tree classifier which has beenapplied to training datasets with more than 100Mwords.
However, his model is non-probabilistic andthus a standard comparison with probabilistic mod-els in terms of perplexity isn?t possible.N-Gram models (Goodman, 2001) obtain esti-mates for p(wi|wi?N+1 .
.
.
wi?1) using counts ofN-Grams.
Because directly using the maximum-likelihood estimate would result in poor predictions,smoothing techniques are applied.
A modified inter-polated form of Kneser-Ney smoothing (Kneser andNey, 1995) was shown to consistently outperforma variety of other smoothing techniques (Chen andGoodman, 1999) and currently constitutes a state-of-the-art3 generative LM.2 ModelWe are concerned with estimating a probability dis-tribution p(Y |x) over a categorical class variableY with range Y , conditional on a feature vectorx = (x1, .
.
.
, xM ), containing the feature values xiof M features.
While generalizations are conceiv-able, we will restrict the features Xk to be binary,i.e.
xk ?
{0, 1}.
For language modeling the classvariable Y corresponds to the target word Wi whichis to be predicted and thus ranges over all possiblewords of some vocabulary.
The binary input fea-tures x are extracted from the conditioning contextwi?N+1 .
.
.
wi?1.
The specific features we use forlanguage modeling are given in Section 3.We assume sparse features, such that typicallyonly a small number of the binary features take value1.
These features are referred to as the active fea-tures and predictions are based on them.
We in-troduce a bias feature which is active for every in-stance, in order to ensure that the set of active fea-tures is non-empty for each instance.
Individually,each active feature Xk is predictive of the class vari-able and predicts the class through a categorical dis-3The model of Wood et al (2009) has somewhat higher per-formance, however, again due to high computational costs themodel has only been trained on training sets of at most 14Mwords.tribution4 distribution, which we denote as p(Y |xk).Since instances typically have several active featuresthe question is how to combine the individual pre-dictions of these features into an overall prediction.To this end we make the assumption that each fea-ture Xk has a certain predictive strength ?k ?
R,where larger values indicate that the feature is morelikely to predict correctly.
The individual predic-tions can then be combined into a mixture model,which weights individual predictions according totheir predictive strength:p(Y |x, ?)
=?k?A(x)vk(x)p(Y |xk) (2)wherevk(x) =e?k?k?A(x)e?k(3)Here A(x) denotes the index-set of active featuresfor instance (y, x).
Note that since the set of activefeatures varies across instances, so do the mixingproportions vk(x) and thus this is not a conventionalmixture model, but rather a variable one.
We willtherefore refer to our model as the variable mixturemodel (VMM).
In particular, our model differs fromlinear or log-linear interpolation models (Klakow,1998), which combine a typically small number ofcomponents that are common across instances.In order to compare our model to the maximum-entropy classifier and other (generalized) linearmodels, it is beneficial to rewrite Equation 2 asp(Y = y|x, ?)
=1Q(x)M?k=1|Y|?j=1?j,k(y, x)?j,k (4)=1Q(x)?>?
(y, x) (5)where ?j,k(y, x) is a sufficient statistics indicatingwhether feature Xk is active and class y = yj and?j,k = e?k+log p(yj |xk) (6)Q(x) =?k?A(x)e?k (7)Table 1 shows the main differences between theVMM, the maximum-entropy classifier and the per-ceptron (Collins, 2002).4commonly referred to as a multinomial distribution626VMM Maximum Entropy Perceptronp(y|x, ?)
= 1Q(x)?>?
(y, x) p(y|x, ?)
= 1Q(x)e?>?
(y,x) score(y|x, ?)
= ?>?
(y, x)Q(x) =?k?A(x) e?k Q(x) =?|Y|j=1 e?>?
(yj ,x)Table 1: A comparison between the VMM, the maximum-entropy classifier and the perceptron.
Like the perceptronand in contrast to the maximum-entropy classifier, the VMM directly uses a predictor ?>?
(y, x).
For the VMM thesufficient statistics ?
(y, x) correspond to binary indicator variables and the parameters ?
are constrained according toEquation 6.
This results in a partition function Q(x) which can be efficiently computed, in contrast to the partitionfunction of the maximum-entropy classifier, which requires a summation over all classes.2.1 Parameter EstimationThe VMM has two types of parameters:1. the categorical parameters ?j,k = p(yj |xk)which determine the likelihood of class yj inpresence of feature Xk;2. the parameters ?k quantifying the predictivestrength of each feature Xk.The two types of parameters are estimated from atraining dataset, consisting of instances (y(h), x(h)).Parameter estimation proceeds in two separatestages, resulting in a simple and efficient procedure.In a first stage, the categorical parameters are com-puted independently for each feature, as the maxi-mum likelihood estimates, smoothed using absolutediscounting (Chen and Rosenfeld, 2000):?j,k = p(yj |xk) =c?j,kckwhere c?j,k is the smoothed count of how many timesY takes value yj when Xk is active, and ck isthe count of how many times Xk is active.
Thesmoothed count is computed asc?j,k ={cj,k ?D if cj,k > 0D?NZkZkif cj,k = 0where cj,k is the raw count for class yj and fea-ture Xk, NZk is the number of classes for whichthe raw count is non-zero, and Zk is the number ofclasses for which the raw count is zero.
D is thediscount constant chosen in [0, 1].
The smoothingthus subtracts D from each non-zero count and re-distributes the so-obtained mass evenly amongst allzero counts.
If all counts are non-zero no mass isredistributed.Once the categorical parameters have been com-puted, we proceed by estimating the predictivestrengths ?
= (?1, .
.
.
, ?M ).
We can do so by con-ducting a search for the parameter vector ??
whichmaximizes the log-likelihood of the training data:??
= argmax?ll(?
)= argmax?
?hlog p(y(h)|x(h), ?
)While any standard optimization method couldbe applied, we use stochastic gradient ascent (SGA,Bottou (2004)) as this results in a particularly conve-nient and efficient procedure that requires only oneiteration over the data (see Section 3).
SGA is anonline optimization method which iteratively com-putes the gradient ?
for each instance and takes astep of size ?
in the direction of that gradient:?
(t+1) ?
?
(t) + ??
(8)The gradient ?
= (?ll(h)?
?1, .
.
.
, ?ll(h)?
?M) computed forSGA contains the first-order derivatives of the datalog-likelihood of a particular instance with respectto the ?-parameters which are given by??
?klog p(y|x, ?)
=vk(x)p(y|x, ?)[p(y|xk)?
p(y|x, ?
)](9)The resulting parameter-update Equation 8 hasthe following intuitive interpretation.
If the predic-tion of a particular active feature Xk is higher thanthe current overall prediction, the term in squarebrackets in Equation 9 becomes positive and thusthe predictive strength ?k for that feature is increasedand conversely for the case where the prediction isbelow the overall prediction.
The magnitude of the627Type Extracted FeaturesStandard N-Grams (BA,SR,LR)* * * (bias)Mr Thompson said* Thompson said* * saidSkip N-Grams (SR,LR)Mr * saidMr Thompson *Mr * ** Thompson *Unigram Bag Features (SR,LR)MrThompsonsaidLong-Range Unigram Bag Features (LR)YesterdayatthepressconferenceTable 2: Feature types and examples for a model of orderN=4 and for the context Yesterday at the pressconference Mr Thompson said.
For each fea-ture type we write in parentheses the feature sets whichinclude that type of feature.
The wildcard symbol * isused as a placeholder for arbitrary regular words.
Thebias feature, which is active for each instance is writtenas * * *.
In standard N-Gram models the bias featurecorresponds to the unigram distribution.update depends on how much overall and featureprediction differ and on the scaling factor vk(x)p(y|x,?)
.In order to improve generalization, we estimatethe categorical parameters based on the counts fromall instances, except the one whose gradient is beingcomputed for the online update (leave-one-out).
Inother words, we subtract the counts for a particularinstance before computing the update (Equation 8)and add them back when the update has been ex-ecuted.
In total, training only requires two passesover the data, as opposed to a single pass (plussmoothing) required by N-Gram models.3 ExperimentsAll experiments were conducted using the SRI Lan-guage Modeling Toolkit (SRILM, Stolcke (2002)),i.e.
we implemented5 the VMM within SRILM andcompared to default N-Gram models supplied withSRILM.
The experiments were run on a 64-bit, 2.2GHz dual-core machine with 8GB RAM.Data The experiments were carried out on datafrom the Reuters Corpus Version 1 (Lewis et al,5The code can be downloaded from http://code.google.com/p/variable-mixture-model .2004), which was split into sentences, tokenized andconverted to lower case, not removing punctuation.All our models were built with the same 30367-word vocabulary, which includes the sentence-endsymbol and a special symbol for out-of-vocabularywords (UNK).
The vocabulary was compiled by se-lecting all words which occur more than four timesin the data of week 31, which was not otherwiseused for training or testing.
As development set weused the articles of week 50 (4.1M words) and astest set the articles of week 51 (3.8M words).
Fortraining we used datasets of four different sizes: D1(week 1, 3.1M words), D2 (weeks 1-3, 10M words),D3 (weeks 1-10, 37M words) and D4 (weeks 1-30,113M words).Features We use three different feature sets in ourexperiments.
The first feature set (basic, BA) con-sists of all features also used in standard N-Grammodels, i.e.
all subsequences up to a length N ?
1immediately preceding the target word.
The sec-ond feature set (short-range, SR) consists of all ba-sic features as well as all skip N-Grams (Ney et al,1994) that can be formed with theN ?1 length con-text.
Moreover, all words occurring in the contextare included as bag features, i.e.
as features whichindicate the occurrence of a word but not the partic-ular position.
The third feature set (long-range, LR)is an extension of SR which also includes longer-distance features.
Specifically, this feature set ad-ditionally includes all unigram bag features up to adistance d = 9.
The feature types and examples ofextracted features are given in Table 2.Model Comparison We compared the VMM tomodified Kneser-Ney (KN, see Section 1).
The or-der of a VMM is defined through the length of thecontext from which the basic and short-range fea-tures are extracted.
In particular, VM-BA of a cer-tain order uses the same features as the N-Grammodels of the same order and VM-SR uses the sameconditioning context as the N-Gram models of thesame order.
VM-LR in addition contains longer-distance features, beyond the order of the corre-sponding N-Gram models.
The order of the modelswas varied between N = 2 .
.
.
5, however, for thelarger two datasets D3 and D4 the order 5 modelswould not fit into the available RAM which is whyfor order 5 we can only report scores for D1 and D2.We could resort to pruning, but since this would havean effect on performance it would invalidate a directcomparison, which we want to avoid.628D1 D2 D3 D4Model N 3.1M 10M 37M 113MKN2 209.2 178.2 155.3 139.33 164.9 127.7 98.9 78.14 160.9 122.2 91.4 68.45 164.5 124.6 ?
?VM-BA2 217.9 209.8 162.8 144.73 174.1 159.7 114.3 87.34 164.9 147.7 102.7 78.25 163.2 144.2 ?
?VM-SR2 215.1 210.1 161.9 144.43 180.1 137.3 112.7 84.64 157.8 117.7 94.8 68.85 147.8 109.7 ?
?VM-LR2 207.5 170.8 147.4 128.23 160.6 124.7 103.2 79.34 146.7 112.1 89.8 66.05 141.4 107.1 ?
?Table 3: The test set perplexities of the models for ordersN=2..5 on training datasets D1-D4.Model Parametrization We used the develop-ment set to determine the values for the absolute dis-counting parameter D (defined in Section 2.1) andthe number of iterations for stochastic gradient as-cent.
This resulted in a value D = 0.1.
Stochas-tic gradient yields best results with a single passthrough all instances.
More iterations result in over-fitting, i.e.
decrease training data log-likelihood butincrease the log-likelihood on the development data.The step size was kept fixed at ?
= 1.0.Results The results of our experiments are givenin Table 3, which shows that for sufficiently highorders VM-SR matches KN on each dataset.
As ex-pected, the VMM?s strength partly stems from thefact that compared to KN it makes better use ofthe information contained in the conditioning con-text, as indicated by the fact that VM-SR matchesKN whereas VM-BA doesn?t.
At orders 4 and 5,VM-LR outperforms KN on all datasets, bringingimprovements of around 10% for the two smallertraining datasets D1 and D2.
Comparing VM-BAand VM-SR at order 4 we see that the 7 additionalfeatures used by VM-SR for every instance signifi-cantly improve performance and the long-range fea-tures further improve performance.
Thus richer fea-ture sets consistently lead to higher model accuracy.Similarly, the performance of the VMM improves asone moves to higher orders, thereby increasing theamount of contextual information.
For orders 2 and3 VM-SR is inferior to KN, because the SR featureset at order 2 contains no additional features overKN and at order 3 it only contains one additionalfeature per instance.
At order 4 VM-SR matchesKN and, while KN gets worse at order 5, the VMMimproves and outperforms KN by around 14%.The training time (including disk IO) of the or-der 4 VM-SR on the largest dataset D4 is about 30minutes, whereas KN takes about 6 minutes to train.4 ConclusionsThe main contribution of this paper consists of anovel probabilistic classifier, the VMM, which isbased on the idea of combining predictions made byindividual features into a mixture model whose com-ponents vary from instance to instance and whosemixing proportions reflect the predictive strength ofeach component.
The main advantage of the VMMis that it is straightforward to incorporate rich fea-tures without sacrificing scalability or reliability ofparameter estimation.
Moreover, the VMM is sim-ple to implement and works ?out-of-the-box?
with-out feature selection, or any special tuning or tweak-ing.Applied to language modeling, the VMM re-sults in a state-of-the-art generative language modelwhose relative performance compared to N-Grammodels gets better as one incorporates richer fea-ture sets.
It scales almost as well to large datasetsas standard N-Gram models: training requires onlytwo passes over the data as opposed to a single passrequired by N-Gram models.
Thus, the experimentsprovide empirical evidence that the VMM is basedon a reasonable set of modeling assumptions, whichtranslate into an accurate and scalable model.Future work includes further evaluation of theVMM, e.g.
as a language model within a speechrecognition or machine translation system.
More-over, optimizing memory usage, for example viafeature pruning or randomized algorithms, would al-low incorporation of richer feature sets and wouldlikely lead to further improvements, as indicated bythe experiments in this paper.
We also intend to eval-uate the performance of the VMM on other lexicalprediction tasks and more generally, on other classi-fication tasks with similar characteristics.Acknowledgments I would like to thankMirella Lapata and Charles Sutton for theirfeedback on this work and Abby Levenberg for thepreprocessed datasets.629ReferencesY.
Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003.A Neural Probabilistic Language Model.
Journal ofMachine Learning Research, 3:1137?1155.A.
Berger, V. Della Pietra, and S. Della Pietra.
1996.A Maximum Entropy Approach to Natural LanguageProcessing.
Computational Linguistics, 22(1):39?71.L.
Bottou.
2004.
Stochastic Learning.
In AdvancedLectures on Machine Learning, Lecture Notes in Ar-tificial Intelligence, pages 146?168.
Springer Verlag,Berlin/Heidelberg.S.
Chen and J. Goodman.
1999.
An Empirical Study ofSmoothing Techniques for Language Modeling.
Com-puter Speech and Language, 13:359?394.S.
Chen and R. Rosenfeld.
2000.
A Survey of Smooth-ing Techniques for ME Models.
IEEE Transactions onSpeech and Audio Processing, 8(1):37?50.M.
Collins.
2002.
Discriminative Training Methodsfor Hidden Markov Models: Theory and Experimentswith Perceptron Algorithms.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 1?8, Philadelphia, PA, USA.J.
Goodman.
2001.
A Bit of Progress in Language Mod-eling (Extended Version).
Technical report, MicrosoftResearch, Redmond, WA, USA.D.
Klakow.
1998.
Log-Linear Interpolation of LanguageModels.
In Proceedings of the 5th International Con-ference on Spoken Language Processing, pages 1694?1698, Sydney, Australia.R.
Kneser and H. Ney.
1995.
Improved Backing-offfor M-Gram Language Modeling.
In Proceedings ofthe International Conference on Acoustics, Speech andSignal Processing, pages 181?184, Detroit, MI, USA.D.
Lewis, Y. Yang, T. Rose, and F. Li.
2004.
RCV1:A New Benchmark Collection for Text CategorizationResearch.
Journal of Machine Learning Research,5:361?397.A.
Mnih and G. Hinton.
2008.
A Scalable HierarchicalDistributed Language Model.
In Advances in NeuralInformation Processing Systems 21.H.
Ney, U. Essen, and R. Kneser.
1994.
On Structur-ing Probabilistic Dependences in Stochastic LanguageModeling.
Computer, Speech and Language, 8:1?38.B.
Roark, M. Saraclar, and M. Collins.
2007.
Discrimi-native n-gram Language Modeling.
Computer, Speechand Language, 21:373?392.R.
Rosenfeld.
1994.
Adaptive Statistical Language Mod-elling: A Maximum Entropy Approach.
Ph.D. thesis,Carnegie Mellon University.R.
Rosenfeld.
1996.
A Maximum Entropy Approach toAdaptive Statistical Language Modeling.
Computer,Speech and Language, 10:187?228.A.
Stolcke.
2002.
SRILM ?
An Extensible LanguageModeling Toolkit.
In Proceedings of the 7th Inter-national Conference on Spoken Language Processing,pages 901?904, Denver, CO, USA.A.
Van den Bosch.
2005.
Scalable Classification-basedWord Prediction and Confusible Correction.
Traite-ment Automatique des Langues, 42(2):39?63.F.
Wood, C. Archambeau, J. Gasthaus, L. James, andY.
Teh.
2009.
A Stochastic Memoizer for SequenceData.
In Proceedings of the 24th International Con-ference on Machine learning, pages 1129?1136, Mon-treal, Quebec, Canada.630
