Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
649?657, Prague, June 2007. c?2007 Association for Computational LinguisticsA Graph-based Approach to Named Entity Categorization in WikipediaUsing Conditional Random FieldsYotaro Watanabe, Masayuki Asahara and Yuji MatsumotoGraduate School of Information ScienceNara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara, 630-0192, Japan{yotaro-w,masayu-a,matsu}@is.naist.jpAbstractThis paper presents a method for catego-rizing named entities in Wikipedia.
InWikipedia, an anchor text is glossed in alinked HTML text.
We formalize named en-tity categorization as a task of categorizinganchor texts with linked HTML texts whichglosses a named entity.
Using this repre-sentation, we introduce a graph structure inwhich anchor texts are regarded as nodes.In order to incorporate HTML structure onthe graph, three types of cliques are definedbased on the HTML tree structure.
We pro-pose a method with Conditional RandomFields (CRFs) to categorize the nodes onthe graph.
Since the defined graph may in-clude cycles, the exact inference of CRFs iscomputationally expensive.
We introduce anapproximate inference method using Tree-based Reparameterization (TRP) to reducecomputational cost.
In experiments, our pro-posed model obtained significant improve-ments compare to baseline models that useSupport Vector Machines.1 IntroductionNamed and Numeric Entities (NEs) refer to propernouns (e.g.
PERSON, LOCATION and ORGANI-ZATION), time expressions, date expressions and soon.
Since a large number of NEs exist in the world,unknown expressions appear frequently in texts, andthey become hindrance to real-world text analysis.To cope with the problem, one effective ways to adda large number of NEs to gazetteers.In recent years, NE extraction has been performedwith machine learning based methods.
However,such methods cannot cover all of NEs in texts.Therefore, it is necessary to extract NEs from ex-isting resources and use them to identify more NEs.There are many useful resources on the Web.
We fo-cus on Wikipedia1 as the resource for acquiring NEs.Wikipedia is a free multilingual online encyclope-dia and a rapidly growing resource.
In Wikipedia,a large number of NEs are described in titles of ar-ticles with useful information such as HTML treestructures and categories.
Each article links to otherrelated articles.
According to these characteristics,they could be an appropriate resource for extractingNEs.Since a specific entity or concept is glossed in aWikipedia article, we can regard the NE extractionproblem as a document classification problem of theWikipedia article.
In traditional approaches for doc-ument classification, in many cases, documents areclassified independently.
However, the Wikipediaarticles are hypertexts and they have a rich structurethat is useful for categorization.
For example, hyper-linked mentions (we call them anchor texts) whichare enumerated in a list tend to refer to the articlesthat describe other NEs belonging to the same class.It is expected that improved NE categorization is ac-complished by capturing such dependencies.We structure anchor texts and dependencies be-tween them into a graph, and train graph-basedCRFs to obtain probabilistic models to estimate cat-egories for NEs in Wikipedia.So far, several statistical models that can cap-1http://wikipedia.org/649ture dependencies between examples have been pro-posed.
There are two types of classification meth-ods that can capture dependencies: iterative classi-fication methods (Neville and Jensen, 2000; Lu andGetoor, 2003b) and collective classification methods(Getoor et al, 2001; Taskar et al, 2002).
In thispaper, we use Conditional Random Fields (CRFs)(Lafferty et al, 2001) for NE categorization inWikipedia.The rest of the paper is structured as follows.
Sec-tion 2 describes the general framework of CRFs.Section 3 describes a graph-based CRFs for NE cat-egorization in Wikipedia.
In section 4, we showthe experimental results.
Section 5 describes relatedwork.
We conclude in section 6.2 Conditional Random FieldsConditional Random Fields (CRFs) (Lafferty et al,2001) are undirected graphical models that give aconditional probability distribution p(y|x) in a formof exponential model.CRFs are formalized as follows.
Let G = {V,E}be an undirected graph over random variables y andx, where V is a set of vertices, and E is a setof edges in the graph G. When a set of cliquesC = {{yc,xc}} are given, CRFs define the con-ditional probability of a state assignment given anobservation set.p(y|x) = 1Z(x)?c?C?
(xc,yc) (1)where ?
(xc,yc) is a potential function definedover cliques, and Z(x) =?y?c?C ?
(xc,yc) isthe partition function.The potentials are factorized according to the setof features {fk}.?
(xc,yc) = exp(?k?kfk(xc,yc))(2)where F = {f1, ..., fK} are feature functions onthe cliques, ?
= {?1, ..., ?K ?
R} are the modelparameters.
The parameters ?
are estimated itera-tive scaling or quasi-Newton method from labeleddata.The original paper (Lafferty et al, 2001) fo-cused on linear-chain CRFs, and applied them topart-of-speech tagging problem.
McCallum et al(2003), Sutton et al(2004) proposed Dynamic Con-ditional Random Fields (DCRFs), the generaliza-tion of linear-chain CRFs, that have complex graphstructure (include cycles).
Since DCRFs modelstructure contains cycles, it is necessary to use ap-proximate inference methods to calculate marginalprobability.
Tree-based Reparameterization (TRP)(Wainwright et al, 2003), a schedule for loopy be-lief propagation, is used for approximate inferencein these papers.3 Graph-based CRFs for NECategorization in WikipediaIn this section we describe how to apply CRFs forNE categorization in Wikipedia.Each Wikipedia article describes a specific entityor concept by a heading word, a definition, and oneor more categories.
One possible approach is to clas-sify each NE described in an article into an appropri-ate category by exploiting the definition of the arti-cle.
This process can be done one by one withoutconsidering the relationship with other articles.On the other hand, articles in Wikipedia aresemi-structured texts.
Especially lists (<UL> or<OL>) and tables (<TABLE>) have an importantcharacteristics, that is, occurrence of elements inthem have some sort of dependencies.
Structuralcharacteristics, such as lists (<UL> or <OL>) ortables (<TABLE>), are useful becase their ele-ments have some sort of dependencies.Figure 2 shows an example of an HTML segmentand the corresponding tree structure.
The first an-chor texts in each list tag (<LI>) tend to be in thesame NE category.
Such characteristics are usefulfeature for the categorization task.
In this paper wefocus on lists which appear frequently in Wikipedia.Furthermore, there are anchor texts in articles.Anchor texts are glossed entity or concept describedwith links to other pages.
With this in mind, our NEcategorization problem can be regarded as NE cat-egory labeling problem for anchor texts in articles.Exploiting dependencies of anchor texts that are in-duced by the HTML structure is expected to improvecategorization performance.We use CRFs for categorization in which anchortexts correspond to random variables V in G and de-650Sibling ES = {(vTi , vTj )|vTi , vTj ?
V T , d(vTi , ca(vTi , vTj )) = d(vTj , ca(vTi , vTj )) = 1, vTj = ch(pa(vTj , 1), k),vTi = ch(pa(vTi , 1), max{l|l < k})}Cousin EC = {(vTi , vTj )|vTi , vTj ?
V T , d(vTi , ca(vTi , vTj )) = d(vTj , ca(vTi , vTj )) ?
2, vTi = ch(pa(vTi ), k),vTj = ch(pa(vTj ), k), pa(vTj , d(vTj , ca(vTi , vTj ))?
1) = ch(pa(vTj , d(vTj , ca(vTi , vTj ))), k),pa(vTi , d(vTi , ca(vTi , vTj ))?
1) = ch(pa(vTi , ca(vTi , vTj )),max{l|l < k})}Relative ER = {(vTi , vTj )|vTi , vTj ?
V T , d(vTi , ca(vTi , vTj )) = 1, d(vTj , ca(vTi , vTj )) = 3,pa(vTj , 2) = ch(pa(vTj , 3), k), vTi = ch(pa(vTi , 1),max{l|l < k})}Figure 1: The definitions of sibling, cousin and relative cliques, where ES , EC , ER correspond to sets whichconsist of anchor text pairs that have sibling, cousin and relative relations respectively.pendencies between anchor texts are treated as edgesE in G. In the next section, we describe the concreteway to construct graphs.3.1 Constructing a graph from an HTML treeAn HTML document is an ordered tree.
We de-fine a graph G = (V G , EG) on an HTML treeT HTML = (V T , ET ): the vertices V G are anchortexts in the HTML text; the edges E are limited tocliques of Sibling, Cousin, and Relative, which wewill describe later in the section.
These cliques areintended to encode a NE label dependency betweenanchor texts where the two NEs tend to be in thesame or related class, or one NE affects the otherNE label.Let us consider dependent anchor text pairs inFigure 2.
First, ?Dillard & Clark?
and ?countryrock?
have a sibling relation over the tree structure,and appearing the same element of the list.
The latterelement in this relation tends to be an attribute or aconcept of the other element in the relation.
Second,?Dillard & Clark?
and ?Carpenters?
have a cousinrelation over the tree structure, and they tend to havea common attribute such as ?Artist?.
The elements inthis relation tend to belong to the same class.
Third,?Carpenters?
and ?Karen Carpenter?
have a relationin which ?Karen Carpenter?
is a sibling?s grandchildin relation to ?Carpenters?
over the tree structure.The latter elements in this relation tends to be a con-stituent part of the other element in the relation.
Wecan say that the model can capture dependencies bydealing with anchor texts that depend on each otheras cliques.
Based on the observations as above, wetreat a pair of anchor texts as cliques which satisfythe condtions in Figure 1.<UL><LI><A><LI> <LI><A><A><UL><A>Dillard &ClarkcountryrockCarpentersKarenCarpenterSiblingCousinRelative Dillard & Clark ??
?country rock? Carpenters Karen CarpenterFigure 2: Correspondence between tree structureand defined cliques.Now, we define the three sorts of edges given anHTML tree.
Consider an HTML tree T HTML =(V T , ET ), where V T and ET are nodes and edgesover the tree.
Let d(vTi , vTj ) be the number of edgesbetween vTi and vTj where vTi , vTj ?
V T , pa(vTi , k)be k-th generation ancestor of vTi , ch(vTi , k) bevTi ?s k-th child, ca(vTi , vTj ) be a common ances-tor of vTi , vTj ?
V T .
Precise definitions of cliques,namely Sibling, Cousin, and Relative, are given inFigure 1.
A set of cliques used in our graph-basedCRFs are edges defined in Figure 1 and vertices, i.e.C = ES ?
EC ?
ER ?
V .
Note that they are re-stricted to pairs of the nearest vertices to keep thegraph simple.3.2 ModelWe introduce potential functions for cliques to de-fine conditional probability distribution over CRFs.Conditional distribution over label set y given ob-651servation set x is defined as:p(y|x) = 1Z(x)???
(vi,vj)?ES?EC?ER?SCR(yi, yj)????
?vi?V?V (yi,x)??
(3)where ?SCR(yi, yj) is the potential over sibling,cousin and relative edges, ?V (yi,x) is the potentialover the nodes, and Z(x) is the partition function.The potentials ?SCR(yi, yj) and ?V (yi,x) factor-ize according to the features fk and weights ?k as:?SCR (yi, yj) = exp(?k?kfk(yi, yj))(4)?V (yi,x) = exp(?k??k?fk?
(yi,x))(5)fk(yi, yj) captures co-occurrences between labels,where k ?
{(yi, yj)|Y ?
Y} corresponds to the par-ticular element of the Cartesian product of the labelset Y .
fk?
(yi,x) captures co-occurrences betweenlabel yi ?
Y and observation features, where k?
cor-responds to the particular element of the label setand observed features.The weights of a CRF, ?
= {?k, .
.
.
, ?k?
, .
.
.
}are estimated to maximize the conditional log-likelihood of the graph in a training datasetD = {?x(1), y(1)?, ?x(2), y(2)?, .
.
.
, ?x(N), y(N)?
}The log-likelihood function can be defined as fol-lows:L?
=N?d=1[?
(vi,vj)?E(d)S ?E(d)C ?E(d)R?k?kfk(yi, yj)+?vi?V (d)?k??k?fk?
(yi,x(d)) ?
logZ(x(d))]?
?k?2k2?2 ??k?
?2k?2?2 (6)where the last two terms are due to the Gaussianprior (Chen and Rosenfeld, 1999) used to reduceoverfitting.
Quasi-Newton methods, such as L-BFGS (Liu and Nocedal, 1989) can be used for max-imizing the function.3.3 Tree-based ReparameterizationSince the proposed model may include loops, it isnecessary to introduce an approximation to calculatemariginal probabilities.
For this, we use Tree-basedReparameterization (TRP) (Wainwright et al, 2003)for approximate inference.
TRP enumerates a set ofspanning trees from the graph.
Then, inference isperformed by applying an exact inference algorithmsuch as Belief Propagation to each of the spanningtrees, and updates of marginal probabilities are con-tinued until they converge.4 Experiments4.1 DatasetOur dataset is a random selection of 2300 articlesfrom the Japanese version of Wikipedia as of Octo-ber 2005.
All anchor texts appearing under HTML<LI> tags are hand-annotated with NE class la-bel.
We use the Extended Named Entity Hierar-chy (Sekine et al, 2002) as the NE class labelingguideline, but reduce the number of classes to 13from the original 200+ by ignoring fine-grained cat-egories and nearby categories in order to avoid datasparseness.
We eliminate examples that consist ofless than two nodes in the SCR model.
There are16136 anchor texts with 14285 NEs.
The numberof Sibling, Cousin and Relative edges in the datasetare |ES | = 4925, |EC | = 13134 and |ER| = 746respectively.4.2 Experimental settingsThe aims of experiments are the two-fold.
Firstly,we investigate the effect of each cliques.
The sev-eral graphs are composed with the three sorts ofedges.
We also compare the graph-based modelswith a node-wise method ?
just MaxEnt method notusing any edge dependency.
Secondly, we com-pare the proposed method by CRFs with a baselinemethod by Support Vector Machines (SVMs) (Vap-nik, 1998).The experimental settings of CRFs and SVMs areas follows.CRFs In order to investigate which type of cliqueboosts classification performance, we perform ex-periments on several CRFs models that are con-structed from combinations of defined cliques.
Re-652SCR SC SR CR# of loopy examples 318 (36%) 324 (32%) 101 (1%) 42 (2%)# of linear chain or tree examples 555 (64%) 631 (62%) 2883 (27%) 1464 (54%)# of one node examples 0 (0%) 60 (6%) 7800 (72%) 1176 (44%)# of total examples 873 1015 10784 2682average # of nodes per example 18.5 15.8 1.5 6.0S C R I# of loopy examples 0 (0%) 0 (0%) 0 (0%) 0 (0%)# of linear chain or tree examples 2913 (26%) 1631 (54%) 237 (2%) 0 (0%)# of one node examples 8298 (74%) 1380 (46%) 15153 (98%) 16136 (100%)# of total examples 11211 3011 15390 16136average # of nodes per example 1.4 5.4 1.05 1Table 1: The dataset details constructed from each model.sulting models of CRFs evaluated on this experi-ments are SCR, SC, SR, CR, S, C, R and I (indepen-dent).
Figure 3 shows representative graphs of theeight models.
When the graph are disconnected byreducing the edges, the classification is performedon each connected subgraph.
We call it an example.We name the examples according the graph struc-ture: ?loopy examples?
are subgraphs including atleast one cycle; ?linear chain or tree examples?
aresubgraphs including not a cycle but at least an edge;?one node examples?
are subgraphs without edges.Table 1 shows the distribution of the examples ofeach model.
Since SCR, SC, SR and CR model haveloopy examples, TRP approximate inference is nec-essary.
To perform training and testing with CRFs,we use GRMM (Sutton, 2006) with TRP.
We set theGaussian Prior variances for weights as ?2 = 10 inall models.SC modelCCC CS SSSCSCR modelCCC CS SSSRRCSR modelS SSSRRCR modelCCC CRRCS modelS SSSC modelCCC CCR modelRRI modelFigure 3: An example of graphs constructed bycombination of defined cliques.
S, C, R in themodel names mean that corresponding model hasSibling, Cousin, Relative cliques respectively.
Ineach model, classification is performed on each con-nected subgraph.SVMs We introduce two models by SVMs (modelI and model P).
In model I, each anchor text is clas-sified independently.
In model P, we ordered theanchor texts in a linear-chain sequence.
Then, weperform a history-based classification along the se-quence, in which j ?
1-th classification result isused in j-th classification.
We use TinySVM witha linear-kernel.
One-versus-rest method is used formulti-class classification.
To perform training andtesting with SVMs, we use TinySVM 2 with a linear-kernel, and one-versus-rest is used for multi-classclassification.
We used the cost of constraint vio-lation C = 1.Features for CRFs and SVMs The features usedin the classification with CRFs and SVMs are shownin Table 2.
Japanese morphological analyzer MeCab3 is used to obtain morphemes.4.3 EvaluationWe evaluate the models by 5 fold cross-validation.Since the number of examples are different in eachmodel, the datasets are divided taking the examples?
namely, connected subgraphs ?
in SCR model.The size of divided five sub-data are roughly equal.We evaluate per-class and total extraction perfor-mance by F1-value.4.4 Results and discussionTable 3 shows the classification accuracy of eachmodel.
The second column ?N?
stands for the num-ber of nodes in the gold data.
The second last row?ALL?
stands for the F1-value of all NE classes.2http://www.chasen.org/?taku/software/TinySVM/3http://mecab.sourceforge.net/653types feature SVMs CRFsobservation definition (bag-of-words) ?
?
(V )features heading of articles?
?
(V )heading of articles (morphemes) ?
?
(V )categories articles?
?
(V )categories articles (morphemes) ?
?
(V )anchor texts?
?
(V )anchor texts (morphemes) ?
?
(V )parent tags of anchor texts?
?
(V )text included in the last header of anchor texts?
?
(V )text included in the last header of anchor texts(morphemes) ?
?
(V )label features between-label feature?
(S,C,R)previous label?Table 2: Features used in experiments.
???
means that the corresponding features are used in classification.The V , S, C and R in CRFs column corresponds to the node, sibling edges, cousin edges and relative edgesrespectively.CRFs SVMsNE CLASS N C CR I R S SC SCR SR I PPERSON 3315 .7419 .7429 .7453 .7458 .7507 .7533 .7981 .7515 .7383 .7386TIMEX/NUMEX 2749 .9936 .9944 .9940 .9936 .9938 .9931 .9933 .9940 .9933 .9935FACILITY 2449 .8546 .8541 .8540 .8516 .8500 .8530 .8495 .8495 .8504 .8560PRODUCT 1664 .7414 .7540 .7164 .7208 .7130 .7371 .7418 .7187 .7154 .7135LOCATION 1480 .7265 .7239 .6989 .7048 .6974 .7210 .7232 .7033 .7022 .7132NATURAL OBJECTS 1132 .3333 .3422 .3476 .3513 .3547 .3294 .3304 .3316 .3670 .3326ORGANIZATION 991 .7122 .7160 .7100 .7073 .7122 .6961 .5580 .7109 .7141 .7180VOCATION 303 .9088 .9050 .9075 .9059 .9150 .9122 .9100 .9186 .9091 .9069EVENT 121 .2740 .2345 .2533 .2667 .2800 .2740 .2759 .2667 .3418 .3500TITLE 42 .1702 .0889 .2800 .2800 .3462 .2083 .1277 .3462 .2593 .2642NAME OTHER 24 .0000 .0000 .0000 .0000 .0000 .0000 .0000 .0000 .0690 .0000UNIT 15 .2353 .1250 .2353 .2353 .2353 .1250 .1250 .2353 .3333 .3158ALL 14285 .7846 .7862 .7806 .7814 .7817 .7856 .7854 .7823 .7790 .7798ALL (no articles) 3898 .5476 .5495 .5249 .5274 .5272 .5484 .5465 .5224 .5278 .5386Table 3: Comparison of F1-values of CRFs and SVMs.654The last row ?ALL (no article)?
stands for the F1-value of all NE classes which have no gloss texts inWikipedia.Relational vs.
Independent Among the modelsconstructed by combination of defined cliques, thebest F1-value is achieved by CR model, followed bySC, SCR, C, SR, S, R and I.
We performed McNe-mar paired test on labeling disagreements betweenCR model of CRFs and I model of CRFs.
Thedifference was significant (p < 0.01).
These re-sults show that considering dependencies work pos-itively in obtaining better accuracy than classify-ing independently.
The Cousin cliques provide thehighest accuracy improvement among the three de-fined cliques.
The reason may be that the Cousincliques appear frequently in comparison with theother cliques, and also possess strong dependenciesamong anchor texts.
As for PERSON, better accu-racy is achieved in SC and SCR models.
In fact,the PERSON-PERSON pairs frequently appear inSibling cliques (435 out of 4925) and in Cousincliques (2557 out of 13125) in the dataset.
Also, asfor PRODUCT and LOCATION, better accuracy isachieved in the models that contain Cousin cliques(C, CR, SC and SCR model).
1072 PRODUCT-PRODUCT pairs and 738 LOCATION-LOCATIONpairs appear in Cousin cliques.
?All (no article)?row in Table 3 shows the F1-value of nodes whichhave no gloss texts.
The F1-value difference be-tween CR and I model of CRF in ?ALL (no article)?row is larger than the difference in ?All?
row.
Thefact means that the dependency information helps toextract NEs without gloss texts in Wikipedia.
Weattempted a different parameter tying in which theSCR potential functions are tied with a particular ob-servation feature.
This parameter tying is introducedby Ghamrawi and McCallum (2005).
However, wedid not get any improved accuracy.CRFs vs. SVMs The best model of CRFs (CRmodel) outperforms the best model of SVMs (Pmodel).
We performed McNemar paired test on la-beling disagreements between CR model of CRFsand P model of SVMs.
The difference was signifi-cant (p < 0.01).
In the classes having larger num-ber of examples, models of CRFs achieve better F1-values than models of SVMs.
However, in severalclasses having smaller number of examples such as0.4 0.5 0.6 0.7 0.80.800.850.900.95RecallPrecisionCR model of CRFsFigure 4: Precision-Recall curve obtained by vary-ing the threshold ?
of marginal probability from 1.0to 0.0.EVENT and UNIT, models of SVMs achieve signif-icantly better F1-values than models of CRFs.Filtering NE Candidates using Marginal Prob-ability The precision-recall curve obtained bythresholding the marginal probability of the MAPestimation in the CR models is shown in Figure 4.The curve reaches a peak at 0.57 in recall, and theprecision value at that point is 0.97.
This preci-sion and recall values mean that 57% of all NEs canbe classified with approximately 97% accuracy on aparticular thresholding of marginal probability.
Thisresults suggest that the extracted NE candidates canbe filtered with fewer cost by exploiting the marginalprobability.Training Time The total training times of allCRFs and SVMs models are shown in Table 4.
Thetraining time tends to increase in case models havecomplicated graph structure.
For instance, modelSCR has complex graph structure compare to modelI, therefore the SCR?s training time is three timeslonger than model I.
Training the models by SVMsare faster than training the models by CRFs.
The dif-ference comes from the implementation issues: C++655CRFs SVMsC CR I R S SC SCR SR I PTraining Time (minutes) 207 255 97 90 138 305 316 157 28 29Table 4: Training Time (minutes)vs. Java, differences of feature extraction modules,and so on.
So, the comparing these two is not theimportant issue in this experiment.5 Related WorkWikipedia has become a popular resource for NLP.Bunescu and Pasca used Wikipedia for detecting anddisambiguating NEs in open domain texts (2006).Strube and Ponzetto explored the use of Wikipediafor measuring Semantic Relatedness between twoconcepts (2006), and for Coreference Resolution(2006).Several CRFs have been explored for informa-tion extraction from the web.
Tang et al pro-posed Tree-structured Conditional Random Fields(TCRFs) (2006) that capture hierarchical structureof web documents.
Zhu et al proposed Hierar-chical Conditional Random Fields (HCRFs) (2006)for product information extraction from Web docu-ments.
TCRFs and HCRFs are similar to our ap-proach described in section 4 in that the model struc-ture is induced by page structure.
However, themodel structures of these models are different fromour model.There are statistical models that capture depen-dencies between examples.
There are two types ofclassification approaches: iterative (Lu and Getoor,2003b; Lu and Getoor, 2003a) or collective (Getooret al, 2001; Taskar et al, 2002).
Lu et al (2003a;2003b) proposed link-based classification methodbased on logistic regression.
This model iterates lo-cal classification until label assignments converge.The results vary from the ordering strategy of lo-cal classification.
In contrast to iterative classifica-tion methods, collective classification methods di-rectly estimate most likely assignments.
Getooret al proposed Probabilistic Relational Models(PRMs) (2001) which are built upon Bayesian Net-works.
Since Bayesian Networks are directed graph-ical models, PRMs cannot model directly the caseswhere instantiated graph contains cycles.
Taskar etal.
proposed Relational Markov Networks (RMNs)(2002).
RMNs are the special case of ConditionalMarkov Networks (or Conditional Random Fields)in which graph structure and parameter tying are de-termined by SQL-like form.As for the marginal probability to use as a confi-dence measure shown in Figure 4, Peng et al (2004)has applied linear-chain CRFs to Chinese word seg-mentation.
It is calculated by constrained forward-backward algorithm (Culotta and McCallum, 2004),and confident segments are added to the dictionaryin order to improve segmentation accuracy.6 ConclusionIn this paper, we proposed a method for categorizingNEs in Wikipedia.
We defined three types of cliquesthat are constitute dependent anchor texts in con-struct CRFs graph structure, and introduced poten-tial functions for them to reflect classification.
Theexperimental results show that the effectiveness ofcapturing dependencies, and proposed CRFs modelcan achieve significant improvements compare tobaseline methods with SVMs.
The results also showthat the dependency information from the HTMLtree helps to categorize entities without gloss textsin Wikipedia.
The marginal probability of MAP as-signments can be used as confidence measure of theentity categorization.
We can control the precisionby filtering the confidence measure as PR curve inFigure 4.
The measure can be also used as a con-fidence estimator in active learning in CRFs (Kimet al, 2006), where examples with the most uncer-tainty are selected for presentation to human anno-tators.In future research, we plan to explore NE catego-rization with more fine-grained label set.
For NLPapplications such as QA, NE dictionary with fine-grained label sets will be a useful resource.
How-ever, generally, classification with statistical meth-ods becomes difficult in case that the label set islarge, because of the insufficient positive examples.It is an issue to be resolved in the future.656ReferencesRazvan Bunescu and Marius Pasca.
2006.
Using ency-clopedic knowledge for named entity disambiguation.In Proceedings of the 11th Conference of the EuropeanChapter of the Association for Computational Linguis-tics.Stanley F. Chen and Ronald Rosenfeld.
1999.
A gaussianprior for smoothing maximum entropy models.
Tech-nical report, Carnegie Mellon University.Aron Culotta and Andrew McCallum.
2004.
Confi-dence estimation for information extraction.
In Pro-ceedings of Human Language Technology Conferenceand North American Chapter of the Association forComputational Linguistics (HLT-NAACL).Lise Getoor, Eran Segal, Ben Taskar, and Daphne Koller.2001.
Probabilistic models of text and link structurefor hypertext classification.
In IJCAI Workshop onText Learning: Beyond Supervision, 2001.Nadia Ghamrawi and Andrew McCallum.
2005.
Col-lective multi-label classification.
In Fourteenth Con-ference on Information and Knowledge Management(CIKM).Juanzi Li Jie Tang, Mingcai Hong and Bangyong Liang.2006.
Tree-structured conditional random fields forsemantic annotation.
In Proceedings of 5th Interna-tional Conference of Semantic Web (ISWC-06).Seokhwan Kim, Yu Song, Kyungduk Kim, Jeong-WonCha, and Gary Geunbae Lee.
2006.
MMR-based ac-tive machine learning for bio named entity recogni-tion.
In Proceedings of the Human Language Technol-ogy Conference/North American chapter of the Asso-ciation for Computational Linguistics annual meeting(HLT-NAACL06).John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
In Pro-ceedings of the 18th International Conference on Ma-chine Learning, pages 282?289.
Morgan Kaufmann,San Francisco, CA.Dong C. Liu and Jorge Nocedal.
1989.
The limited mem-ory BFGS methods for large scale optimization.
InMathematical Programming 45.Qing Lu and Lise Getoor.
2003a.
Link-based classifica-tion using labeled and unlabeled data.
In Proceedingsof the International Conference On Machine Learning,Washington DC, August.Qing Lu and Lise Getoor.
2003b.
Link-based text clas-sification.
In Proceedings of the International JointConference on Artificial Intelligence.Andrew McCallum, Khashayar Rohanimanesh, andCharles Sutton.
2003.
Dynamic conditional randomfields for jointly labeling multiple sequences.
In NIPSWorkshop on Syntax, Semantics, and Statistics, De-cember.J.
Neville and D. Jensen.
2000.
Iterative classificationin relational data.
In Proceedings of AAAI-2000 Work-shop on Learning Statistical Models from RelationalData, pages 13?20.
AAAI Press.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detectionusing conditional random fields.
In Proceedings ofThe 20th International Conference on ComputationalLinguistics (COLING).Simone Paolo Ponzetto and Michael Strube.
2006.
Ex-ploiting semantic role labeling, wordnet and wikipediafor coreference resolution.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata.2002.
Extended named entity hierarchy.
In Proceed-ings of the LREC-2002.Michael Strube and Simone Paolo Ponzetto.
2006.Wikirelate!
computing semantic relatedness usingwikipedia.
In Proceedings of the 21st National Con-ference on Artificial Intelligence (AAAI-06).Charles Sutton, Khashayar Rohanimanesh, and AndrewMcCallum.
2004.
Dynamic conditional randomfields: Factorized probabilistic models for labeling andsegmenting sequence data.
In Proceedings of the 21thInternational Conference on Machine Learning.Charles Sutton.
2006.
GRMM: A graphical modelstoolkit.
http://mallet.cs.umass.edu.Ben Taskar, Pieter Abbeel, and Daphne Koller.
2002.Discriminative probabilistic models for relational data.In Proceedings of the 18th Conference on Uncertaintyin Artificial Intelligence.
Morgan Kaufmann.Vladimir Vapnik.
1998.
Statistical Learning Theory.Wiley Interscience.Martin Wainwright, Tommi Jaakkola, and Alan Will-sky.
2003.
Tree-based reparameterization frame-work for analysis of sum-product and related algo-rithms.
IEEE Transactions on Information Theory,45(9):1120?1146.Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, and Wei-Ying Ma.
2006.
Simultaneous record detection andattribute labeling in web data extraction.
In Proceed-ings of the 12th ACM SIGKDD International Confer-ence on Knowledge Discovery and Data Mining.657
