The information-processing difficulty of incremental parsingJohn HaleDepartment of Linguistics and LanguagesMichigan State UniversityEast Lansing, MI 48824-1027jthale@msu.eduAbstractWhen an incremental parser gets the next word,its expectations about upcoming grammatical struc-tures can change.
When a word greatly constrainsthese grammatical expectations, uncertainty is re-duced.
This elimination of possibilities constitutesinformation processing work.
Formalizing this no-tion of information processing work yields a com-plexity metric that predicts human repetition ac-curacy scores across a systematic class of linguis-tic phenomena, the Accessibility Hierarchy of rela-tivizable grammatical relations.1 IntroductionAn attractive hypothesis in psycholinguistics, dat-ing back at least to the 1950s, has been thatthe degree of predictability of words in sentencesis somehow related to understandability (Taylor,1953), production difficulty (Goldman-Eisler, 1958)or, more recently, eye-movements (McDonald andShillcock, 2003).
However, since the 1950s, inte-grating this hypothesis with realistic models of lin-guistic structure has remained a challenge.Lounsbury (1954) appreciated the formal char-acter of the problem.
He defined a finite, artificiallanguage, endowed with a rudimentary phonology,morphology and syntax, and showed that a word?sinformational contribution could be formally de-fined as the entropy reduction brought about by itsaddition to the end of a sentence fragment.
He qual-ified the significance of his achievement, sayingAn entropy reduction analysis presupposesthat the number of possible messages is finite,and that the probabilities of each of the mes-sages is known....Thus it appears that the en-tropy reduction analysis could be applied onlyto limited classes of natural language mes-sages since the number of messages in nearlyall languages is indefinitely large(Lounsbury, 1954, 108)A fuller presentation of this work can be found inHale (forthcoming).The present paper extends Lounsbury?s originalidea to infinite languages, by applying two classi-cal ideas in (probabilistic) formal language theory:Grenander?s (1967) closed-form solution for the en-tropy of a nonterminal in a probabilistic context-freephrase structure grammar, and Lang?s (1974; 1988)insight that an intermediate parser state is itself aspecification of a grammar.This extension permits the psycholinguistic hy-pothesis ERH to be examined.Entropy Reduction Hypothesis (ERH) a per-son?s processing difficulty at a word in asentence is directly related to the number ofbits signaled to the person by that word withrespect to a probabilistic grammar the personknows.In section 2 a method for calculating the entropyreduction of a word in a sentence generated by aprobabilistic grammar is presented.
Section 3 de-scribes the empirical domain of interest, the Acces-sibility Hierarchy (Keenan and Comrie, 1977).
Sec-tion 4 goes on to describe two probabilistic gram-mars in the class of mildly context-sensitive Min-imalist Grammars (Stabler, 1997).
One expressesthe ?promotion analysis?
(Kayne, 1994) of relativeclauses while the other expresses the more standard?adjunction analysis?
(Chomsky, 1977).
The pre-dictions of these grammars through the lens of theERH are considered in sections 5 through 7, whereit is shown that predictions derived from the pro-motion analysis match human repetition accuracyscores better than predictions derived from the ad-junction analysis.
Section 8 concludes.2 Entropy ReductionThe idea of the entropy reduction of a word is thatuncertainty about grammatical continuations fluctu-ates as new words come in.
The ERH is the pro-posal that fluctuations in this value be taken as psy-cholinguistic predictions.
This proposal is foundedon the possibility of viewing nonterminal symbolsin probabilistic grammars as random variables.
Forinstance, in the rules given below,0.87 NP?
the boy0.13 NP?
the tall boythe nonterminal NP can be viewed as a randomvariable that has two alternative outcomes.
Indeed,nonterminals generally in probabilistic context-freephrase structure grammars (PCFGs) can be viewedthis way.
Since their outcomes are discrete, theirentropy H is easily calculatedH(X) = ?
?x?Xp(x) log2 p(x) (1)H(NP) = ?
[(0.87?
log2 0.87)+(0.13?
log2 0.13)]?
0.56 bitsThere is just over half a bit of uncertainty abouthow NP is going to rewrite, because the outcomeis so heavily weighted towards the first alternative.In this simple example there is no recursion, so thegenerated language is finite.
To obtain the uncer-tainty about infinite PCFG languages, a recursiverelation due to Grenander (1967) can be used to cal-culate the entropy of the start symbol S which be-gins all derivations.2.1 Entropy of nonterminals in a PCFGGrenander?s theorem is a recurrence relation thatgives the entropy of each nonterminal in a PCFG Gas the sum of two terms.
Let the set of productionrules in G be ?
and the subset rewriting nontermi-nal ?
be ?(?).
Denote by pr the probability of a ruler having daughters ?j1 , ?j2 , .
.
.. Thenh(?i) = ??r??
(?i)pr log2 prH(?i) = h(?i) +?r??
(?i)pr [H(?j1)+H(?j2) + ?
?
?
](Grenander, 1967, 19)the first term, lowercase h, is simply the definitionof entropy for a discrete random variable.
The sec-ond term, uppercase H , is the recurrence.
It ex-presses the intuition that derivational uncertainty ispropagated from children to parents.For PCFGs that define a probability distribution,the solution to this recurrence can be written as amatrix equation where I is the identity matrix, ~hthe vector of the h(?i) and A is a matrix whose(i, j)th component gives the expected number ofnonterminals of type j resulting from nonterminalsof type i.H = (I ?A)?1~h (2)2.2 Incomplete sentencesGrenander?s theorem supplies the entropy for anyPCFG nonterminal in one step by inverting a ma-trix.
To determine the contribution of a particu-lar word, one would like to be able to look at thechange in uncertainty about compatible derivationsas a given prefix string is lengthened.
When this set,the set of derivations generating a given string w =w0w1 .
.
.
wn as a left prefix, is finite, it can be ex-pressed as a list.
In the case of a recursive grammarthis set is not finite and some other representation isnecessary.Lang and Billot observe (1974; 1988; 1989) thatthe incremental state of a parser can be describedby another, related grammar.
They view parsing asthe intersection of a grammar with a regular lan-guage, of which ordinary strings are but the simplestexamples.
This perspective readily accommodatesincomplete sentences as regular languages whosemembers all have the same initial n words but con-tinue with all possible words of the terminal vocabu-lary, for all possible lengths.
If L(G) is the languageof the grammar G, parsing an initial substring w isthe intersection depicted in 3 where the period de-notes any terminal symbol of G and the Kleene starindicates any number of repetitions.w(.)?
?
L(G) (3)The result of this intersection is a new context-free grammar describing just the derivations whoseyield begins with the string w. By generalizing theinput from a single string to a regular set of strings,the grammatical continuations can be captured inthe new, output grammar.
These grammars are eas-ily read off of chart parsers?
internal data structuresby attaching position indices to nonterminal names,thus distinguishing recognized constituents in dif-ferent positions.The uncertainty associated with the the start sym-bol of this new, resultant grammar is the conditionalentropy H(S|w1, w2, ?
?
?wn).
The entropy reduc-tion of word wn+1 then is the downward change inthis value as the string w is made one word longer.The proposal of the ERH is that these changes mea-sure the disambiguation work the comprehender hasperformed by ruling out possible syntactic analyses.SUBJECT ?
DIR.
OBJECT ?
INDIR.
OBJECT ?
OBLIQUE ?
GENITIVE ?
OCOMPFigure 1: The Accessibility Hierarchy of relativizable grammatical relations3 The Accessibility HierarchyThis paper examines the processing predictions ofthe ERH on a systematic class of relative clausetypes, the Accessibility Hierarchy (AH) shown infigure 1.
The AH is an implicational markednesshierarchy of grammatical relations discovered byKeenan and Comrie in (1977).
The implication isthat if a language has a relative-clause formationrule applicable to grammatical relations at somepoint x on the AH, then it can also form relativeclauses on grammatical relations listed at all pointsbefore x.This hierarchy shows up in a variety of mod-ern syntactic theories that have been influenced byRelational Grammar (Perlmutter and Postal, 1974).In Head-driven Phrase Structure Grammar (Pollardand Sag, 1994) the hierarchy corresponds to theorder of elements on the SUBCAT list, and inter-acts with other principles in explanations of bind-ing facts.
The hierarchy also figures in Lexical-Functional Grammar (Bresnan, 1982) where it isknown as Syntactic Rank.Keenan and Comrie speculated that their typo-logical generalization might have a basis in per-formance factors.
This idea was examined ina repetition-accuracy experiment carried out in1974 but not published until 1987.
Subjects in thisstudy repeated back stimulus sentences after a delaywhile under the additional memory load of a digit-memory task.
Stimuli were subject-modifying rel-ative clauses embedded in one of four carrier sen-tence frames, exemplified in figure 2.subject extracted they had forgotten that the boy whotold the story was so youngdirect object extracted the fact that the cat whichDavid showed to the man likes eggs is strangeindirect object extracted I know that the man whoStephen explained the accident to is kindoblique extracted he remembered that the food whichChris paid the bill for was cheapgenitive subject extracted they had forgotten that thegirl whose friend bought the cake was waitinggenitive object extracted the fact that the sailor whoseship Jim took had one leg is importantFigure 2: Relative clauses in each of four carriersentence typesThe results of the human study, given in figure 3,SU DO IO OBL GenS GenOrepetitionaccuracy 406 364 342 279 167 171Figure 3: results from Keenan & Hawkins (1987)show that repetition accuracy1 declines across theAH.
Keenan and Hawkins (1987) note however that?It remains unexplained just why RCs should bemore difficult to comprehend-produce as they areformed on positions lower on the AH.
?The ERH, if correct, would offer just such an ex-planation.
If a person?s difficulty on each word ofa sentence is related to derivational information sig-naled by that word, then the total difficulty readinga sentence ought to be the sum of the difficulty oneach word2.4 Minimalist GrammarsIf correct, the ERH would explain the increasingdifficulty across the AH in terms of greater or lesseruncertainty about intermediate parser states.
To cal-culate these predictions, some assumption must bemade about what those structures are.4.1 Two analyses of relativizationToward this end, two grammars covering theKeenan and Hawkins stimuli were written in theMinimalist Grammars (Stabler, 1997) formalism.These grammars were exactly the same except fortheir treatment of relative clauses.One grammar expresses the usual analysis of rel-ative clauses as right-adjoined modifiers (Chomsky,1977).
The other expresses the promotion analysisof relative clause.
The analysis, which dates back tothe 1960s, is revived in Kayne (1994).
For reasonshaving to do with Kayne?s general theory of phrasestructure, he proposes that, in a sentence like 1, theunderlying form of the subject is akin to 2.1Each response was coded for accuracy on a 0-2 scale where2 means perfect repetition and 1 suggests minor, grammaticalerrors.
A score of 0 was assigned when the response did notinclude a relative clause of the indicated grammatical function.Cf.Keenan and Hawkins (1987)2Summation naturally extends the word-by-word complex-ity metric ERH to the sentence level.
In word-by-word self-paced reading, evidence for the Accessibility Hierarchy is lim-ited (cf.
chapter 5 of Hale (2003)).
(1) the boy who the father explained the answerto was honest(2) [IP the father explained the answer to[DP[+wh] who boy[+f] ] ]According to Kayne, at an early stage (2) of syn-tactic derivation, the determiner phrase (DP) ?whoboy?
occupies what will eventually be the gap posi-tion.
This DP moves to a specifier position of the en-closing, empty-headed (C0) complementizer phrase(CP), thereby checking a feature +wh as indicatedin 3.
(3) [CP [DP who boy[+f] ]i C0 [IP the father ex-plained the answer to ti ] ]In a second movement, ?boy?
evacuates from DP,moving to another specifier (perhaps that of thesilent agreement morpheme, Agr) as in 4 ?
checkinga different feature, +f.
(4) [AgrP boyj Agr [CP [DP who tj ]i C0 [IP thefather explained the answer to ti ] ] ]The entire structure becomes a complement of a de-terminer to yield a larger DP in 5.
(5) [DP the [AgrP boyj Agr [CP [DP who tj ]i C0[IP the father explained the answer to ti ] ] ]]No adjunction is used in this derivation, and, un-conventionally, the leftmost ?the?
and ?boy?
do notshare an exclusive common constituent.
Nor is thewh-word ?who?
co-indexed with anything.
Struc-tural descriptions involving both the Kaynian anal-ysis and the more standard adjunction analysis areshown in figures 4 and 5 respectively3.
The otherlinguistic assumptions suggested by these diagramsare discussed in chapter 4 of Hale (2003).4.2 Formal grammars of relativizationThe Minimalist Grammars (MG) formalism (cf.Stabler and Keenan (2003) for a systematic pre-sentation) facilitates the relatively transparent im-plementation of ideas like movement and fea-ture checking that figure prominently in the twoanalyses of relativization discussed in the previ-ous subsection.
MGs define a set of sentencesby closing the structure-building functions mergeand move on a finite set of lexical entries; how-ever, this does not mean that parsing must happenbottom-up.
A fundamental result, obtained inde-pendently by Harkema (2001) and Michaelis (2001)3The X-bar structures depicted in figures 4 and 5 are drawnusing tools developed by Edward Stabler and colleagues.is that MGs are equivalent to Multiple context-free grammars (Seki et al, 1991).
Multiple context-free grammars generalize standard context-freegrammars by allowing the string yields of daugh-ter categories to be manipulated by a function otherthan simple concatenation.
As in Tree Adjoin-ing Grammar (Joshi et al, 1975) a record of thesemanipulations is kept at each node of an MG deriva-tion tree, while a picture of the result is manifestedin derived trees such as the ones in figures 4 and 5.The derivation tree on the promotion grammar isshown4 in figure 6 for the substring ?the boy whothe father explained the answer to.
?d -case::=c_rel d -case c_rel+wh_rel c_rel,-wh_rel::=t +wh_rel c_rel t,-wh_rel+case t,-case,-wh_rel::=>little_v +case t little_v,-case,-wh_rel=d little_v,-wh_rel d -case::=>v =d little_v v,-wh_rel+case v,-case,-wh_rel=d +case v,-wh_rel d -case::=p_to =d +case v p_to,-wh_rel::=>Pto p_to Pto,-wh_rel+case Pto,-case -wh_rel::=d +case Pto d -case -wh_rel+f d -case -wh_rel,-f::=Num +f d -case -wh_rel Num,-f::=n Num ::n -f::=Num d -case Num::=n Num ::n::=Num d -case Num::=n Num ::nFigure 6: Derivation tree on promotion grammar.The derivation trees encode everything there is toknow about MG derivations, and can be parsed ina variety of orders.
Most importantly, if equippedwith weights on their branches, they can be gener-ated by probabilistic context-free grammars.4These derivation trees are drawn using tools developed byMaxime Amblard.cPc?
((((((chhhhhhtPdP(4)d?dtheXXXc relPdP(1)nP(0)n?nboyd?dwhoNumPNum?NumnPt(0)`` ``c rel?c rel``t`P(((((dP(3)d?dtheNumPNum?NumnPn?nfatherhhhhht?t!
!little vvexplainlittle vaat-ed`` `little vPdPt(3)PPlittle v?little vtPPvPdP(2)d?dtheNumPNum?NumnPn?nanswerPPv?
"dPt(2)bv?vtp toPp to?p toPtotop toPPPtoPdP(1)t(1)HPto?PtotdPt(1)t?!
!tBebet-edaaBePBe?BetaPdPt(4)a?a APA?AhonestFigure 4: Kaynian promotion analysiscPc?
((((((chhhhhhtPdP(3)dPd?dtheNumPNum?NumnPn?nboyXXXc relPdP(0)d?dwhoXXXc rel?c rel``t`P(((((dP(2)d?dtheNumPNum?NumnPn?nfatherhhhhht?t!
!little vvexplainlittle vaat-edXXXlittle vPdPt(2)XXXlittle v?little vtPPvPdP(1)d?dtheNumPNum?NumnPn?nanswerPPv?
"dPt(1)bv?vtp toPp to?p toPtotop toPPPtoPdP(0)t(0)HPto?PtotdPt(0)t?!
!tBebet-edaaBePBe?BetaPdPt(3)a?a APA?AhonestFigure 5: more standard adjunction analysis5 ProcedureDerivation trees on both grammars were obtained5for each of Keenan and Hawkins?
(1987) twenty-four stimulus sentences6.
Branches of these deriva-tion trees were viewed as PCFG rules with probabil-ities set according to the usual relative-frequency es-timation technique (Chi, 1999).
However, becausethe stimuli were intentionally constructed to have5Derivations were obtained using a parser described in Ap-pendix A of Hale (2003)6To eliminate number agreement as a source of derivationaluncertainty, the results were calculated using a modified stim-ulus set in which four noun phrases were changed from pluralto singular.exactly four examples of each structure, these sen-tences were weighted in accordance with a corpusstudy (Keenan, 1975) to make their relative frequen-cies more realistic.6 ResultsThe summed entropy reductions exhibit a signifi-cant correlation with the repetition accuracy scorescollected by Keenan and Hawkins (1987).The correlation in figure 7(a) obtains only on thegrammar expressing the Kaynian promotion anal-ysis, and not on the grammar expressing the stan-dard adjunction analysis (figure 7(b)).
Nor do log-probabilities for stimulus sentences on the grammar250 300 350 400 450 500error score303540455055totalbits reducedAccessibility Hierarchypromotion grammarr2=0.45, p<0.001250 300 350 400 450 500error score505560657075totalbits reducedAccessibility Hierarchyadjunction grammarr2=0.02, n.s.Figure 7: Predictions of two probabilistic Minimalist Grammars through the lens of the ERHexhibit a significant correlation with repetition ac-curacy scores.7 DiscussionFrom the perspective of the ERH, the difference be-tween the promotion and adjunction grammars re-sides in the uncertainty of particular states an incre-mental parser would pass through on the way to acomplete analysis.On the Keenan and Hawkins?
(1987) stimuli,these grammars specify incremental parser statesthat support explanations for some of the observedrepetition accuracy asymmetries, abbreviated <.SU < IO subject extracted relatives are easier thanindirect object extracted relatives, because aleft-to-right incremental parser evades, in justsubject extracted relatives, the uncertainty as-sociated with questions like?
which internal argument is the gap??
did dative shift happen?These questions are defined by alterna-tive derivation-subtrees associated with theverb phrase.
For the DO stimuli that use poten-tially ditransitive embedded verbs the same ex-planation is available, however only two out offour items in the Keenan and Hawkins (1987)set qualify.IO < OBL there is only one type of extractionfrom indirect object, whereas on these gram-mars, the head of the oblique phrase (?for??with?
?on?
or ?in?)
signals which of four cat-egorically separate kinds of extraction has oc-curred.
These alternatives correspond to fourdifferent derivation-nonterminals.OBL < GEN both grammars analyze ?whose?
astaking a common noun argument, for example?whose ship.?
But in just the promotion gram-mar, ?whose?
is further analyzed as the ordi-nary ?who?
morpheme plus a complex pos-sessive phrase headed by ?-s?
(McDaniel etal., 1998).
Because of the recursive charac-ter of this possessor category, the structure of?whose?s?
common noun argument introducesadditional uncertainty not present in the indi-rect object extracted relatives.Strikingly, the two grammars disagree on six out-liers in figure 7(b) where just the adjunction gram-mar predicts very great difficulty in conjunctionwith the ERH.
These outlier predictions are made onjust the sentences that use the nominal carrier framebeginning with ?the fact that...?
Because the adjunc-tion grammar analyzes relative clauses with an MGrule analogous to the phrase structure rule (4),DP ?
DP CPrel.
(4)all DPs are available for modification by any num-ber of stacked relative clauses.
The nominal frameintroduces an additional DP, not present in the otherstimuli, that can be modified in this way.By contrast, the promotion grammar does not in-clude a +f promotion feature on any lexical en-try for ?fact,?
precluding the possibility of suchmodification.
Moreover, even with such a feature,the promotion grammar assigns different categoriesto the outermost versus successive relative clausemodifiers.
Because only one relative clause is everstacked in the Keenan and Hawkins (1987) stimulusset, the relevant recursion is not attested, yielding acategory of caseless subject DP that is more certainthan it is in the adjunction grammar.An ERH account that avoids predicting these out-liers on the Keenan and Hawkins (1987) stimuliseems to require a grammar where the probabilityof 2nd and subsequent stacked relative clause modi-fiers is closer to 0 (its value on the trained promotiongrammar) than to 0.31 (its value on the trained ad-junction grammar).
Beyond these particular stimuli,this modeling motivates a general question aboutthe scale of structural expectations in human sen-tence processing.
Does disconfirmation of a morecomplicated structural alternative (such as stackedrelative clauses) induce greater processing difficultythan disconfirmation of a simpler one?
Such em-pirical issues go beyond the scope of this paper butsuggest particular kinds of future work.8 ConclusionBy extending Lounsbury?s (1954) entropy reductionidea to infinite languages, it has become possibleto relate predictability and processing difficulty in away that takes into account linguistic structures de-fined by one kind of mildly context-sensitive gram-mar formalism.
This relation is the linking hypoth-esis ERH.On this linking hypothesis, a grammar express-ing the promotion analysis of relative clauses yieldswhole-sentence predictions more closely approxi-mating human repetition accuracy results than doesa grammar expressing the standard adjunction anal-ysis.If the ERH is true, this result suggests that onegrammar carries a kind of greater psychological va-lidity than the other.
On the other hand, to theextent that the promotion grammar correctly char-acterizes human linguistic competence, this con-firms the ERH as a linking hypothesis.
In any case,the information-processing difficulty of incrementalparsing can now be given a more specific definition.AcknowledgmentsThe author wishes to thank Paul Smolensky, Ed Sta-bler and Ted Gibson.ReferencesSylvie Billot and Bernard Lang.
1989.
The struc-ture of shared forests in ambiguous parsing.
InProceedings of the 1989 Meeting of the Associa-tion for Computational Linguistics.Joan Bresnan, editor.
1982.
The Mental Repre-sentation of Grammatical Relations.
MIT Press,Cambridge, MA.Zhiyi Chi.
1999.
Statistical properties of prob-abilistic context-free grammars.
Computa-tional Linguistics, 25(1):131?160.Noam Chomsky.
1977.
On Wh-Movement.
In Pe-ter Culicover, Thomas Wasow, and Adrian Ak-majian, editors, Formal Syntax, pages 71?132.Academic Press, New York.Frieda Goldman-Eisler.
1958.
Speech produc-tion and the predictability of words in context.Quarterly Journal of Experimental Psychology,10:96?106.Ulf Grenander.
1967.
Syntax-controlled probabili-ties.
Technical report, Brown University Divisionof Applied Mathematics, Providence, RI.John Hale.
2003.
Grammar, uncertainty and sen-tence processing.
Ph.D. thesis, Johns HopkinsUniversity, Baltimore, Maryland.Henk Harkema.
2001.
Parsing Minimalist Gram-mars.
Ph.D. thesis, UCLA.Aravind K. Joshi, Leon S. Levy, and Masako Taka-hashi.
1975.
Tree adjunct grammars.
Journal ofComputer and System Sciences, 10:136?163.Richard S. Kayne.
1994.
The Antisymmetry of Syn-tax.
MIT Press.Edward L. Keenan and Bernard Comrie.
1977.Noun phrase accessibility and universal grammar.Linguistic Inquiry, 8(1):63?99.Edward L. Keenan and Sarah Hawkins.
1987.
Thepsychological validity of the Accessibility Hi-erarchy.
In Edward L. Keenan, editor, Univer-sal Grammar: 15 Essays, pages 60?85, London.Croom Helm.Edward L. Keenan.
1975.
Variation in universalgrammar.
In R.W.
Shuy and R.W.
Fasold, edi-tors, Analyzing Variation in Language.
George-town University Press.Bernard Lang.
1974.
Deterministic techniques forefficient non-deterministic parsers.
In J. Loeckx,editor, Proceedings of the 2nd Colloquium onAutomata, Languages and Programming, num-ber 14 in Springer Lecture Notes in ComputerScience, pages 255?269, Saarbruu?cken.Bernard Lang.
1988.
Parsing incomplete sentences.In Proceedings of the 12th International Confer-ence on Computational Linguistics, pages 365?371.Floyd G. Lounsbury.
1954.
Transitional proba-bility, linguistic structure and systems of habit-family hierarchies.
In C. E. Osgood and T. A.Sebeok, editors, Psycholinguistics: a survey oftheory and research.
Indiana University Press.Dana McDaniel, Cecile McKee, and Judy B. Bern-stein.
1998.
How children?s relatives solve aproblem for minimalism.
Language, pages 308?334.Scott A. McDonald and Richard C. Shillcock.
2003.Eye movements reveal the on-line computation oflexical probabilities during reading.
Psychologi-cal Science, 14:648?652.Jens Michaelis.
2001.
On Formal Properties ofMinimalist Grammars.
Ph.D. thesis, PotsdamUniversity.David Perlmutter and Paul Postal.
1974.
Lectureson Relational Grammar.
LSA Linguistic Insti-tute, UMass Amherst.Carl Pollard and Ivan A.
Sag.
1994.
Head-driven Phrase Structure Grammar.
University ofChicago Press.Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,and Tadao Kasami.
1991.
On multiple context-free grammars.
Theoretical Computer Science,88:191?229.Edward Stabler and Edward Keenan.
2003.
Struc-tural similarity.
Theoretical Computer Science,293:345?363.Edward P. Stabler.
1997.
Derivational minimal-ism.
In Christian Retore?, editor, Logical As-pects of Computational Linguistics, pages 68?95.Springer.Wilson Taylor.
1953.
Cloze procedure: a new toolfor measuring readability.
Journalism Quarterly,30:415?433.
