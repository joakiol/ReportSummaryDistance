Construction of Japanese Nominal Semantic Dictionaryusing "A NO B" Phrases in CorporaSadao Kurohash i ,  Masak i  Murata ;  Yasunor i  Yata~Mi tsunobu Sh imada I and Makoto  NagaoGraduate School of Infomatics, Kyoto UniversityYoshida-honmachi, Sakyo, Kyoto 606-8501, Japankuro?
i ,  kyot  o-u.
ac.
jpAbst rac tThis paper describes a method of constructingJapanese nominal semantic dictionary, which isindispensable for text analysis, especially for in-direct anaphora resolution.
The main idea is touse noun phrases of "A NO(postposition) B" incorpora.
Two nouns A and B in "A NO B" canhave several semantic relations.
By collecting"A NO B" phrases form corpora, analyzing theirsemantic relations, and arranging them for each"B" and each semantic relation, we can obtaina nominal semantic dictionary.
The dictionarywe constructed from 130M characters corporaby this method has 22,252 entries, which can beconsidered as a practically useful coverage.
Ourmethod for analyzing "A NO B" phrase is alsooriginal which uses a thesaurus as an attributefor decision tree.1 In t roduct ionThe role of dictionary is undoubtedly importantin Natural Language Processing (NLP).So far, research in NLP has mainly concernedthe analysis of individual sentences.
The analy-sis of a sentence is to clarify which element in ithas relation with which by what relation.
To dosuch an analysis, a verbal semantic dictionary,in other words, case frame dictionary is neces-sary.
A case frame dictionary describes whatkind of cases each verb has and what kinds ofnoun can fill a case slot.
Condition on case slotscan be expressed by semantic markers and/orexample nouns.
For example, a case frame forthe verb "YOMU(read)" can be as follows:"Now at Communications Research Laboratory.
E-mail: murata@crl.go.jptNow at Sharp Corporation.YOMU(read)agent: human beings, like KARE(he),KEN(ken), SENSEI(teacher)object : something to be read, likeHON(book), SHOSETSU(novel)Such dictionaries with a practically usefulcoverage have been compiled in many institutes,mainly by hand, and used in many NLP systems(EDR, 1993; NTT, 1997).These days, the main target of NLP has beenshifting from individual sentences to a series ofsentences, that is, a text.
Human beings uselanguage to communicate, and the unit of com-munication is not a sentence, but a text in mostcases, especially in the case of written language.The NLP system can only catch enough infor-mation when it handles a text as a whole.Similar to sentence analysis, the main part oftext analysis is to clarify the relation among itsconstituents:?
discourse relation between text segments(cause-effect, elaboration, exemplification,etc.),?
maintaining and changing of topics,?
recovery of omission,?
direct anaphora resolution,?
indirect anaphora resolution.To do such analyses, not only a verbal se-mantic dictionary, but also many other types ofknowledge have to be employed, one of whichis a nominal semantic dictionary.
Similar to averbal semantic dictionary, a nominal semanticdictionary describes what kind of nouns havewhat relation with each noun obligately (likeobligate cases of a verb) as follows:33KAKAKU(pr ice)?
an attribute of something like KU-RUMA(car), PASOKON(personal com-puter), RINGO(apple), NIKU(meat)YANE(roof)?
a part of a building like IE(house), KOYA(hut)SENSEI(teacher)?
belongs to some institute likeSHOGAKKO (elementary school),KOUKOU(high school), and?
teaches omething like SUGAKU (mathe-matics), ONGAKU(music)A nominal semantic dictionary is necessaryfor indirect anaphora resolution.
Indirectanaphora is not an special, exceptional phe-nomena in texts, but very often used and it isvery important o handle it properly for textunderstanding.
A typical example of indirectanaphora is as follows:XXX announced the release of a newlap-top computer.
The price is $800.In order to find the relation between "the price"and "a new lap-top computer", a nominal se-mantic information about "price" has to be em-ployed.It is, however, almost impossible to compilea nominal semantic dictionary automatically.Then, the qustion is how we can construct a dic-tionary semi-automatically, or support the hu-man compilation sufficiently.
Comparing withthe case of verbal dictionary, the number ofnoun is very big.
Furthermore, if technicalterms should be included, they become unlim-itedly large.Since case elements of a verb appear by theverb in a sentence, we can collect possible caseelemtns of a verb by a simple parsing, or justby detecting adjoining noun and verb.
On theother hand, since an anaphor and its anchor ofindirect anaphora ppear far away, it is almostimpossible to collect them by a simple methodautomatically.This paper presents how to solve this prob-lem, namely, how to construct a nominal se-mantic dictionary semi-automatically.2 Use  of  "A  NO B"  Phrases  inCorporaIn Japanese, two nouns, A and B, in a phrase"A NO(postposition) B" have several semanticrelations.
Some relations among them can be aslot of a nominal semantic dictionary.For example, "price" is the price of some-thing, and in Japanese corpora we canfind several phrases like "KURUMA(car) NOKAKAKU(price)" and "RINGO(apple) NOKAKAKU(price)".
That is, we can obtain use-ful data for the entry "B" in a nominal semanticdictionay only by collecting phrases of "A NOB" from corpora.However, all phrases of "A NO B" are notuseful.
For example, even if "MEIKA(maker)NO KAKAKU(price)" exists in corpora,"MEIKA(maker)" is not traded at some price,in normal cases.
In other case, the phrase"WATASHI(I) NO HON(book)" does not nec-essarily indicate that the noun "HON(book)"has obiligate relation with "WATASHI(I)" orhuman beings.Furthermore, when a phrase "A NO B" is aproper data for a nominal dictionary, it is desir-able to place "A" to a proper slot of the entry"B".These classification can be realized by the se-mantic analysis of "A NO B" described in thenext section.3 Semant ic  Ana lys i s  o f  "A  NO B"Japanese noun phrase "A NO B" can have oneof many semantic relations listed in Table 1.The semantic analysis of "A NO B" has beena hard problem in Japanese NLP.
For this prob-lem, Sumita et al proposed an example-basedanalysis method (Sumita et al, 1990):1.
Collect many example phrases of ~A NOB",2.
Give proper semantic relation to each ex-ample by hand,3.
Given an input, detect he most similar ex-ample to the input,4.
Assign the relation given to the most simi-lar example to the input.This is the first work that implemented anexample-based method in NLP, being much34Table 1: Semantic relation of "A NO B".1. possession (in a wide sense)possessionwhole-part*belong*relatives*product/produceattribute*ex.
WATASHI(I) NO HON(book)ex.
KURUMA(car) NO ENJIN(engine)ex.
HOTEL(hotel) NO JYUGYOIN(employee)ex.
KEN(Ken) NO ANE(sister)ex.
NIHON(Japan) NO KOME(rice)ex.
KURUMA(car) NO KAKAKU(price)2.
A modifies BA:natureA:action/B:agentA:action/B:objectA:action/B:place*A:action/B:time*A:action/B:methodA:cause/B:effect*A:effect/B:cause*A:object/B:agent*A:field*ex.
TANPATU(short hair) NO JYOSEI(lady)ex.
SANPO(walk) NO HITO(man)ex.
YUNYU(import) NO RINGO(apple)ex.
SOTSUGYOUSHIKI(graduation ceremony) NO KAIJYO(place)ex.
SOTSUGYOU(graduation) NO JIKI(time)ex.
TSUKIN(travel to work) NO SHUDAN(way)ex.
JISHIN(earthquake) NO HIGAI(damage)ex.
JISHIN(earthquake) NO GENIN(cause)ex.
SUUGAKU(mathematics) NO SENSEI(teacher)ex.
BENGOSHI(lawyer) NO SHIKAKU(qualification)3.
B is actionA:agent/B:actionA:object/B:action*A:goal/B:action*A:place/B:actionA:time/B:actionA:method/B:actionex.
KAZOKU(family) NO SHOUDAKU(approval)ex.
KURUMA(car) NO HANBAI(sale)ex.
KYOTO(Kyoto) NO TOUCHAKU(arrivai)ex.
OKUGAI(outdoor) NO ASOBI(play)ex.
5JI(5 o'clock) NO HEITEN(close)ex.
DENSHA(train) NO TSUKIN(travel to work)4.
A is place/timeA:place ex.
20SEIKI(20th centry) NO ASIA(Asia)A:time ex.
KYODAI(Kyoto University) NO TOKEIDAI(clock tower)5.
Exceptionsidiomatic phrase ex.
CHA-NO-MA(living room)fraction ex.
3BUN-NO-2(two-third)more robust and easy to maintain than the con-ventional rule-based NLP.The problem of example-based NLP is howto define the similarity between an input andan example.
Sumita et al caluculated the sim-ilarity between an input "Ai NO Bi" and anexample "Ae NO Be" as follows:w A ?
s im(A i ,  Ae) + wB" s im(Bi ,  Be),where s im(Ai ,  Ae) is the similarity between Aiand Ae calculated based on the distance of thetwo words in a thesaurus tree, s im(B i ,Be)  isthe same for Bi and Be, WA and WB are weightsshowing which similarity should be consideredmore relevant, Ai and Ae or Bi and Be.Such a way of caluculating words' similarityand combining them has been widely used bymany researchers.
However, it only has somequalitative ground, but no quantitative one.For such a problem, Jiri and Nagao proposeda method using a thesaurus as an attribute fordecision tree, being able to optimize the sys-tem on training set of examples (Jiri and Na-gao, 1997).
Although their method treated PPattachment ambiguity, it can be applicable tothe analysis of "A NO B".Let us explain the decision tree method for"A NO B" analysis here.35Select T (A)\ /gOH = 0.56Select 'I TM\ /gOIt ; 0.99Figure 1: Selecting an attribute for the decision tree expansion.Dec is ion  Tree  Induct ionEach example phrase "A NO B" is expressedby a triple (TA, TB, Rj), where TA and TB arethe position (node) in a thesaurus matching theword A and B, respectively, Rj is the semanticrelation of the phrase given by hand.Each node in the decision tree, D, corre-sponds to the information expressed by a triple(T (A), T (B), S),  where T (A) and T (B) are theposition (node) in A-side thesaurus and B-sidethesaurus, respectively, S is a subset of examplephrases.At first, the root node of the decision tree,Droot, corresponds to the triple (T~(A~, T~(o~,Salt), and it is given to the step 1 below.1.
Suppose the given decision tree node, D,corresponds to the triple (T (A), T B, S).If the percentage of the major relation inthe set S is greater than a threshold value(90% in our experiment), that is, S is ho-mogenous enough, stop expanding D (Dbecomes a leaf of the decision tree), andthe major relation of the set S is given toD.
Otherwise, go to step 2.2.
Select the more informative attribute, T(A)or T (B).
We consider the more informativeattribute to be the one which split the nodeD to more homogenous subnodes, when wetry T (A), we split the node into subnodes,each associated with a child node of T (A),T: A), and containing a set (Si) of exampleswhose TA is a descendant of T: A).
Then,we calculate the following formula, whichshows a kind of overall heterogeneity of theresulting subnodes:gi - -  Nq Nq.on = L logsi 3where N is the number of examples in S,Ni is the number of examples in Si, andNq is the number of examples in Si whichis given the j-th semantic relation.
We alsocalculate the OH value for T (B), and weselect one with the lower OH.. For the selected attribute, make subnodesas in step 2, and call the same algorithmon each subnode.Figure 1 shows a simplified example of theabove algorithm.
Suppose "A NO B" phrasescan be classified into only two relations, R1 andR2, and a given decision tree node contains 50Rl-examples and 50 R2-examples.
If we selectT (A) (suppose T (A) has three child nodes), weobtain the OH, 0.56, as in the left hand sideof Figure 1; if we select T (s) (suppose T(B) hastwo child nodes), we obtain the OH, 0.99, as inthe right hand side of Figure 1.
Consequently,we select T (A), and split the node D into threesubnodes.
In the next step, the first and thirdnodes are analyzed by the same algorithm; thesecond node is not expanded any more (it ishomogeneous enough).36ClassificationClassification algorithm of an unseen phrase,"Ai NO Bi", using the induced ecision tree isvery simple.
A path is traversed in the deci-sion tree, starting at its root.
At each internalnode D, we follow the branch depending on theD's selected attribute (T (A) or T (s)) and thethesaurus position of the input nouns (TA~ orTB~).
When the path reaches to a leaf of thedecision tree, the phrase is assigned the major-ity relation of the leaf.
When we cannot followany branch at any decision tree node, D, thephrase is assigned the majority relation of D.(For example, when TA~ is relatively high in thethesaurus, and at some point the decision treetries to expand the node.
)Exper imentWe did an experiment to see how well theabove method works.
As a thesaurus, we usedEDR Concept Dictionary (EDR, 1993).
We col-lected about 20,000 example phrases of "A NOB" from several corpora, and gave one of thesemantic relations listed in Table 1 by hand.Then, we did experiments on twelve differenttest sets: each time, we partitioned the wholeexample into a training set of 19,500 phrasesand a test set of 500 phrases, made a decisiontree using the training set, analyzed the test set,and compared the result with the original rela-tion given by hand.
The average accuracy ofthe analysis was about 80%.4 Const ruct ion  of Nomina l  Semant icD ic t ionaryOur proposed method of constructing a nominalsemantic dictionary is as follows: 11.
Collect phrases of "A NO B" from corpora,excluding syntactically ambiguous phraseslike "A NO B NO C" and "A NO B C"(in both cases, "A" may modify "C", not"B").2.
For each phrase "A NO B", decide the se-mantic relation using the decision tree al-gorithm described in the previous ection.IFor an action noun, like "sale", "arrival"; it is pos-sible to utilize a verbal semantic information about itsverbal form ("sell" and "arrive").
However, in this paoper, we limit the discussion to the method only using "ANO B" phrases.3.
All examples are arranged for each "B" andeach relation.
"B" becomes an entry wordof our dictionary, and each entry is classi-fied by semantic relations.
If the relation isnot among relations marked '. '
in Table 1,it is discarded from the entry 2In our experiments, we used Mainichi News-paper Articles in 1995 (60M characters), andHeibonsha's World Encyclopedia (70M charac-ters) as corpora.From these 130M characters corpora, we col-lected about 620,000 types of "A NO B".
Then,we analyzed these phrases, and the resultingdictionary consists of 22,252 entries, each entryhas 1.5 slots on average, and each slot has 6.6words on overage (it means that each entry has9.9 words (= 1.5x6.6) on average).
We can saythat the resulting dictionary has a practicallyuseful wide-coverage.Table 2 shows a couple of entries of the dic-tionary.
We could find an interesting featurein our corpus-based dictionary.
In the entry ofthe word "UDE (arm)", human lexicographerswould make a slot of part-whole relation with"KARADA (body)" at first.
In the automaticconstructed dictionary, however, the major slotis field, with examples of "TENIS (tennis)","SHODOU (calligraphy)".
This reflects the factthat a metaphoric usage of "UDE (arm)" mean-ing ability or skill is much more frequent thanthe literal usage in real corpora.
Such an adapt-ability to the real usage of words is an advantageof a corpus-based dictionary.The remaining problem is how to clean upthe dictionary.
As mentioned in the previoussection, the accuracy of the semantic analysisof "A NO B" is about 80%, resulting in manyinappropriate words in the dictionary slots.
Forexample, the entry of "KAKAKU(price)" in Ta-ble 2, attribute slot includes "URITE(seller)"and "KAITE(buyer)".
These are the re-sults of incorrect analysis of "URITE(seller)NO KAKAKU(price)" and "KAITE(buyer) NOKAKAKU(price)".
One way of cleaning up is tointroduce some machine learning method, aim-ing at more automatic process.
However, thecurrent dictionary is not so bad, and it's not so2Currently we consider semantic relations marked ' , 'in Table 1 can be a relation between an anaphor and theanchor.
However, more investigation is necessary for thiscriteria.37Table 2: Example entries in the automatic onstructed nominal dictionary.KAKAKU(price)attribute: RINGO(apple), BUTANIKU(port), KIN(gold), URITE(seller), MEM-ORI(memory), KAITE(buyer), KURUMA(car) ...SENSEI(teacher)agent-object : BIJYUTU(art), GOLF(golf), ONGAKU(music) .--belong : KOUKOU(high school), SHOUGAKKOU(elementary school),JYUKU(crammer) ---YANE(roof)part-whole : JYUTAKU(house), KURUMA(car), STADIUM(stadium), KOYA(hut)UDE(arm)field : TENNIS(tennis), SHODOU(calligraphy), KARATE(karate), EN-SOU(musical performance) -..IHUKU(clothes), NINGYOU(doll) part-whole :hard to clean up it by hand.5 Conc lus ionIn this paper, we described a method of con-structing Japanese nominal semantic dictionaryusing noun phrases of "A NO B" in corpora.The resulting dictionary we constructed from130M characters corpora by this method has22,252 entries, which can be considered as apractically useful coverage.What we have to do next is to clean up thedictionary, since the automatic analysis of "ANO B" phrase has some errors.
Another targetis to employ the resulting dictionary in our textanalysis ystem which handles direct anaphora,indirect anaphora, and omission simultaneously.Re ferencesJapan Electronic Dictionary Research InstituteLtd.
1993.
EDR Electronic Dictionary Spec-ifications Guide.Ikehara, S., Miyazaki, M., Shirai, S., Yokoo,A., Nakalwa, H,, Ogura, K., Oyama, Y.and Hayashi, Y.
1997.
Japanese Lexicon.Iwanami Publishing.Sumita, E., Iida, H. and Kohyama, H. 1990.Translating with Examples: A New Approachto Machine Translation.
Proc.
of 3rd TMI.Jiri Stetina and Makoto Nagao.
1997.
CorpusBased PP-Attachment Ambiguity Resolutionwith a Semantic Dictionary.
Proc.
5th Work-shop on Very Large Corpora, Hongkong.38
