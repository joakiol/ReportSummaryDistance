Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 624?635,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsUnsupervised Spectral Learning of WCFG as Low-rank Matrix CompletionRaphae?l Bailly?
Xavier Carreras?
Franco M. Luque?
Ariadna Quattoni??
Universitat Polite`cnica de CatalunyaBarcelona, 08034rbailly,carreras,aquattoni@lsi.upc.edu?
Universidad Nacional de Co?rdoba and CONICETX500HUA Co?rdoba, Argentinafrancolq@famaf.unc.edu.arAbstractWe derive a spectral method for unsupervisedlearning of Weighted Context Free Grammars.We frame WCFG induction as finding a Han-kel matrix that has low rank and is linearlyconstrained to represent a function computedby inside-outside recursions.
The proposed al-gorithm picks the grammar that agrees with asample and is the simplest with respect to thenuclear norm of the Hankel matrix.1 IntroductionWeighted Context Free Grammars (WCFG) definean important class of languages.
Their expressivitymakes them good candidates for modeling a widerange of natural language phenomena.
This expres-sivity comes at a cost: unsupervised learning ofWCFG seems to be a particularly hard task.
Andwhile it is a well-studied problem, it is still to a greatextent unsolved.Several methods for unsupervised learning ofWCFG have been proposed.
Some rely on heuristicsthat are used to build incrementally an approxima-tion of the unknown grammar (Adriaans et al 2000;Van Zaanen, 2000; Tu and Honavar, 2008).
Othermethods are based on maximum likelihood estima-tion, searching for the grammar that has the largestposterior given the training corpus (Baker, 1979;Lari and Young, 1990; Pereira and Schabes, 1992;Klein and Manning, 2002).
Several Bayesian in-ference approaches have also been proposed (Chen,1995; Kurihara and Sato, 2006; Liang et al 2007;Cohen et al 2010).
These approaches perform pa-rameter estimation by exploiting Markov samplingtechniques.Recently, for the related problem of unsuperviseddependency parsing, Gormley and Eisner (2013)proposed a new way of framing the max-likelihoodestimation.
In their formulation the problem is ex-pressed as an integer quadratic program subject tonon-linear constraints.
They exploit techniques frommathematical programming to solve the resultingoptimization.In spirit, the work by Clark (2001; 2007) is prob-ably the most similar to our approach since both ap-proaches share an algebraic view of the problem.
Inhis case the key idea is to work with an algebraicrepresentation of a WCFG.
The problem of recover-ing the constituents of the grammar is reduced to theproblem of identifying its syntactic congruence.In the last years, multiple spectral learning algo-rithms have been proposed for a wide range of mod-els (Hsu et al 2009; Bailly et al 2009; Bailly et al2010; Balle et al 2011; Luque et al 2012; Cohenet al 2012).
Since the spectral approach provides agood thinking tool to reason about distributions over?
?, the question of whether they can be used for un-supervised learning of WCFG seems natural.
Still,while spectral algorithms for unsupervised learningof languages can learn regular languages, tree lan-guages and simple dependency grammars, the fron-tier to WCFG seems hard to reach.In fact, the most recent theoretical results on spec-tral learning of WCFG do not seem to be very en-couraging.
Recently, Hsu et al(2012) showed thatthe problem of recovering the joint distribution overPCFG derivations and their yields is not identifiable.624Although, for some simple grammar subclasses (e.g.independent left and right children), identification inthe weaker sense (over the yields of the grammar)implies strong identification (e.g.
over joint distri-bution of yields and derivations).
In their paper, theypropose a spectral algorithm based on a generaliza-tion of the method of moments for these restrictedsubclasses.Thus one open direction for spectral research con-sists on defining subclasses of context free lan-guages that can be learned (in the strong sense) fromobservations of yields.
Yet, an alternative researchdirection is to consider learnability in the weakersense.
In this paper we take the second road, andfocus on the problem of approximating the distribu-tion over yields generated by a WCFG.Our main contribution is to present a spectral al-gorithm for unsupervised learning of WCFG.
Fol-lowing ideas from Balle et al(2012), the algo-rithm is framed as a convex optimization where wesearch for a low-rank matrix satisfying two typesof constraints: (1) Constraints derived from observ-able statistics over yields; and (2) Constraints de-rived from certain recurrence relations satisfied by aWCFG.
Our derivations of the learning algorithm il-lustrate the main ingredients behind the spectral ap-proach to learning functions over ??
which are: (1)to exploit the recurrence relations satisfied by thetarget family of functions and (2) provide algebraicformulations of these relations.We alert the reader that although we are able toframe the problem as a convex optimization, thenumber of variables involved is quite large and pro-hibits a practical implementation of the method ona realistic scenario.
The experiments we presentshould be regarded as examples designed to illus-trate the behavior of the method.
More researchis needed to make the optimization more efficient,and we are optimistic that such improvements canbe achieved by exploiting problem-specific proper-ties of the optimization.
Regardless of this, ours isa novel way of framing the grammatical inferenceproblem.The rest of the paper is organized as follows.
Sec-tion 2 gives preliminaries on WCFG and the type offunctions we will learn.
Section 3 establishes thatspectral methods can learn a WCFG from a Han-kel matrix containing statistics about context-freecuts.
Section 4 presents the unsupervised algorithm,where we formulate grammar induction as a low-rank optimization.
Section 5 presents experiments,and finally we conclude the paper.Notation Let ?
be an alphabet.
We use ?
to de-note an arbitrary symbol in ?.
The set of all fi-nite strings over ?
is denoted by ?
?, where wewrite ?
for the empty string.
We also use the set?+ = ??
\ {?
}.We use bold letters to represent column vectorsv and matrices M .
We use In to denote the n-dimensional identity matrix.
We use M+ to de-note the Moore-Penrose pseudoinverse of some ma-trixM .
M?M ?
is the Kronecker product betweenmatricesM ?
Rm?n andM ?
?
Rp?q resulting in amatrix in Rmp?nq.
The rest of notation will be givenas needed.2 Weighted Context Free GrammarsIn this section we define Weighted Context FreeGrammars (WCFG).
We start with a classic defini-tion and then describe an algebraic form of WCFGthat will be used throughout the paper.
We also de-scribe the fundamental recursions in WCFG.2.1 WCFG in Classic FormA WCFG over ?
is a tuple G?
=?V,R, T, w?, wT , wR?
where?
V is the set of non-terminal symbols.
We as-sume that V = {1, .
.
.
, n} for some naturalnumber n, and that V ?
?
= ?.?
R is a set of binary rules of the form i ?
j kwhere i, j, k ?
V .?
T is a set of unary rules of the form i ?
?where i ?
V and ?
?
?.?
w?
: V ?
R, with w?
(i) being the weight ofstarting a derivation with non-terminal i.?
wT : V ?
?
?
R, with wT (i ?
?)
being theweight of rule rewriting i into ?.?
wR : V ?
V ?
V ?
R, with wR(i ?
j k)being the weight of rewriting i into j k.A WCFG G?
computes a function gG?
: ?+ ?
Rdefined asgG?
(x) =?i?Vw?(i)??G?(i?=?
x) , (1)625where we define the inside function ??G?
: V ?
?+ ?R recursively:??G?(i?=?
?)
= wT (i?
?)
(2)??G?(i?=?
x) =?j,k?Vx1,x2??+s.t.
x=x1x2wR(i?
j k) (3)??G?(j?=?
x1)??G?(k?=?
x2) ,where in the second case we assume |x| > 1.
Theinside function ??G?(i?=?
x) exploits the fundamen-tal inside recursion in WCFG (Baker, 1979; Lari andYoung, 1990).
We will find useful to define the out-side function ??G?
: ??
?V ???
?
R defined recur-sively as:??G?(?
; i;?)
= w?
(i) (4)??G?
(x; i; y) =?j,k?Vx1???,x2??+s.t.
x=x1x2wR(j ?
k i)?
(5)??G?
(x1; j; y) ?
??G?(k?=?
x2)+?j,k?Vy1??+,y2???s.t.
y=y1y2wR(j ?
i k)???G?
(x; j; y2) ?
??G?(k?=?
y1) ,where in the second case we assume that eitherx 6= ?
or y 6= ?.For x, z ?
??
and y ?
?+ we have that?i?V??G?
(x; i; z) ?
??G?(i?=?
y) (6)is the weight that the grammar G?
assigns to a stringxyz that has a cut or bracketing around y. Techni-cally, it corresponds to the sum of the weights of allderivations that have a constituent spanning y. Inparticular we have thatgG?
(x) =?i??G?(?
; i;?)
?
??G?(i?=?
x) .If x is a string of lengthm, and x[t:t?]
is the substringof x from positions t to t?, it also happens thatgG?
(x) =?i??G?
(x[1:t?1]; i;x[t+1:m])???G?(i?=?
x[t]))for any t between 1 and m.In this paper we will make frequent use of insideand outside quantities.
Notationally, for outsides thesemi-colon between two strings, i.e.
x; z, will sim-bolize a cut where we can insert an inside string y.Finally, we note that Probabilistic Context FreeGrammars (PCFG) are a special case of WCFGwhere: w?
(i) is the probability to start a derivationwith non-terminal i; wR(i ?
j k) is the condi-tional probability of rewriting nonterminal i into jand k; wT (i ?
?)
is the probability of rewriting iinto symbol ?;?iw?
(i) = 1; and for each i ?
V ,?j,k wR(i ?
j k) +??
wT (i ?
?)
= 1.
Un-der these conditions the function gG?
is a probabilitydistibution over ?+.2.2 WCFG in Algebraic FormWe now define a WCFG in algebraic form.
AWeighted Context Free Grammar (WCFG) over ?with n states is a tuple G = ??
?, {??},A?
with:?
An initial vector ??
?
Rn.?
Terminal vectors ??
?
Rn for ?
?
?.?
A bilinear operatorA ?
Rn?n2.A WCFG G computes a function gG : ??
?
Rdefined asgG(x) = ?>?
?G(x) (7)where the inside function ?G : ?+ ?
Rn is?G(?)
= ??
(8)?G(x) =?x1,x2??+x=x1x2A(?G(x1)?
?G(x2)) (9)We will define the outside function ?G : ??
???
?
Rn as:?G(?;?)
= ??
(10)?G(x; z)> =?x1???,x2?
?+x=x1x2?G(x1; z)>A(?G(x2)?
In)+?z1??+,z2??
?z=z1z2?G(x; z2)>A(In ?
?G(z1)) (11)For x, z ?
??
and y ?
?+ we have that?G(x; z)>?G(y) (12)is the weight that the grammar assigns to the stringxyz with a cut around y.
In particular, gG(x) =?G(?;?
)>?G(x).626Let us make clear that a WCFG is the samedevice in classic or algebraic forms.
If G?
=?V,R, T, w?, wT , wR?
and G = ??
?, {??
},A?, themapping is:w?
(i) = ??
(i) (13)wT (i?
?)
= ??
[i] (14)wR(i?
j k) = A[i, j, k] (15)??G?(i?=?
x) = ?G(x)[i] (16)??G?
(x; i; z) = ?G(x; z)[i] (17)See Section A.1 for a proof of Eq.
16 and 17.3 WCFG and Hankel MatricesIn this section we describe Hankel matrices forWCFG.
These matrices explicitly capture inside-outside recursions employed by WCFG functions,and are key to a derivation of a spectral learning al-gorithm that learns a grammar G using statistics ofa training sample.Let us define some sets.
We say that I1 = ?+is the set of inside strings.
The set of composed in-side strings I2 is the set of elements (x, x?
), wherex, x?
?
?+.
Intuitively (x, x?)
represents two adja-cent spans with an operation, i.e., it keeps the traceof the operation that composes x with x?
and yieldsxx?.
We will use the set I = I1 ?
I2.The set of outside contextsO is the set containingelements ?x; z?, where x, z ?
??.
Intuitively, ?x; z?represents a context where we can insert an insideelement y in between x and z, yielding xyz.Consider a function f : O ?
I ?
R. The Hankelmatrix of f is the bi-infinite matrix Hf ?
RO?Isuch thatHf (o, i) = f(o, i).In practice we will work with finite sub-blocks ofHf .
To this end we will employ the notion of basisB = (P,S), where {?
?, ??}
?
P ?
O is a setof outside contexts and ?
?
S ?
I1 is a set ofinside strings.
We will use p = |P| and s = |S|.Furthermore, we define the inside completion of Sas the set S?
= {(x, x?)
| x, x?
?
S}.
Note thatS?
?
I2.
We say that B?
= (P,S?)
is the insidecompletion of B.The sub-block of Hf defined by B is the p ?
smatrix HB ?
RP?S with HB(o, i) = Hf (o, i) =f(o, i).
In addition toHB, we are interested in theseadditional finite vectors and matrices:?
h?
?
RS is the s-dimensional vector with co-ordinates h?
(x) = f(?
?, ?
?, x).?
h?
?
RP is the p-dimensional vector with co-ordinates h?
(o) = f(o, ?).?
HA ?
RP?S?with HA(o, (x1, x2)) =f(o, (x1, x2)).3.1 Hankel FactorizationsIf f is computed by a WCFG G, then Hf has rankn factorization.
To see this, consider the follow-ing matrices.
First a matrix S ?
Rn?I1of insidevectors for all strings, with column x taking valueSx = ?G(x).
Then a matrix P ?
RO?n of out-side vectors for all contexts, with row ?x; z?
tak-ing value P ?x;z?
= ?G(x; z).
It is easy to see thatHf = PS, since Hf (?x; z?, y) = P ?x;z?Sy =?G(x; z)>?G(y).
ThereforeHf has rank n.The same happens for sub-blocks.
If HB is thesub-block associated with basis B = (P,S), thenthe sub-blocks P B ?
RP?n and SB ?
Rn?S of Pand S also accomplish that HB = P BSB .
It alsohappens thath>?
= ?>?
SB (18)h?
= P B??
(19)HA = P BA(SB ?
SB) .
(20)We say that a basis B is complete for f ifrank(HB) = rank(Hf ).
The following is a keyresult for spectral methods.Lemma 1.
Let B = (P,S) be a complete basis ofdimension n for a function f and let HB ?
RP?Sbe the Hankel sub-block of f for B.
Let h?, h?
andHA be the additional matrices for B. IfHB = PSis a rank n factorization, then the WCFG G =??
?, {??},A?
with?>?
= h>?
S+ (21)??
= P+h?
(22)A = P+HA(S ?
S)+ (23)computes f .See proof in Section A.2.6273.2 Supervised Spectral Learning of WCFGThe spectral learning method directly exploitsLemma 1.
In a nutshell, the spectral method is:1.
Choose a complete basis B = (P,S) and a di-mension n.2.
Use training data to compute estimates of thenecessary Hankel matrices: HB, h?, h?,HA.3.
Compute the SVD ofHB,HB = U?V >.4.
Create a truncated rank n factorization of HBasP nSn, havingP n = Un?n andSn = V >n ,where we only consider the top n singular val-ues/vectors of ?,U ,V .5.
Use Lemma 1 to computeG, usingP n and Sn.Because of Lemma 1, if B is complete and wehave access to the trueHB, h?, h?,HA of a WCFGtarget function g?, then the algorithm will computea G that exactly computes g?.
In practice, we onlyhave access to empirical estimates of the Hankel ma-trices.
In this case, there exist PAC-style samplecomplexity bounds that state that gG will be a closeapproximation to g?
(Hsu et al 2009; Bailly et al2009; Bailly et al 2010).The parameters of the algorithm are the basis andthe dimension of the grammar n. One typically em-ploys some validation strategy using held-out data.Empirically, the performance of these methods hasbeen shown to be good, and similar to that of EM(Luque et al 2012; Cohen et al 2013).
It is alsoimportant to mention that in the case that the targetg?
is a probability distribution, the function gG willbe close to g?, but it will only define a distribution inthe limit: in practice it will not sum to one, and forsome inputs it might return negative values.
This is apractical difficulty of spectral methods, for exampleto apply evaluation metrics like perplexity which areonly defined for distributions.4 Unsupervised Learning of WCFGIn the previous section we have exposed that if wehave access to estimates of a Hankel matrix of aWCFG G, we can recover G. However, the statis-tics in the Hankel require access to strings that haveinformation about context-free cuts.
We will assumethat we only have access to statistics about plainstrings of a distribution, i.e.
p(x), which we callobservations.
In this scenario, one natural idea isto search for a Hankel matrix that agrees with theobservations.
The method we present in this sec-tion frames this problem as a low-rank matrix op-timization problem.
We first characterize the spaceof solutions to our problem, i.e.
Hankel matricesassociated with WCFG that agree with observablestatistics.
Then we present the method.4.1 Characterization of a WCFG HankelIn this section we describe valid WCFG Hankel ma-trices using linear constraints.We first describe an inside-outside basis that isan extension of the one in the previous section.
In-side elements are the same, namely I = I1 ?
I2,where I1 are strings (x) and I2 are composedstrings (x, x?).
The set of outside contexts O1 isthe set containing elements ?x; z?, defined as be-fore.
The set of composed outside contexts has el-ements ?x, x?
; z?, and ?x; z?, z?, where x, z ?
?
?and x?, z?
?
?+.
These outside contexts keep anoperation open in one of the sides.
For example, ifwe consider ?x; z?, z?
and insert a string y, we obtainx(y, z?
)z, where we use (y, z?)
to explicitly denote acomposed inside string.
We will use O = O1 ?
O2.In this section, we will assume that I and O arefinite and closed.
By closed, we mean that:?
(x) ?
I ?
(x1, x2) ?
I for x = x1x2?
(x1, x2) ?
I ?
x1 ?
I, x2 ?
I?
?x; z?
?
O ?
?x1, x2; z?
?
O for x = x1x2?
?x; z?
?
O ?
?x; z1, z2?
?
O for z = z1z2?
?x1, x2; z?
?
O ?
(x2) ?
I?
?x; z1, z2?
?
O ?
(z1) ?
IWe will consider a Hankel matrix H ?
RO?I .Some entries of this matrix will correspond to ob-servable quantities.
Specifically, for any string x ?I1 for which we know p(x) we can define the fol-lowing observable constraint:p(x) = H(??;?
?, (x)) (24)The rest of entries of H correspond to a stringwith an inside-outside cut, and these are not ob-servable.
Our method will infer the values of theseentries.
The following constraints will ensure thatthe matrix H is a well defined Hankel matrix forWCFG:628?
Hankel constraints: ?
?x; z?
?
O, (y1, y2) ?
IH(?x; z?, (y1, y2)) = H(?x, y1; z?, (y2))= H(?x; y2, z?, (y1)) (25)?
Inside constraints: ?
o ?
O, (x) ?
IH(o, (x)) =?x=x1x2H(o, (x1, x2)) (26)?
Outside constraints: ?
?x; z?
?
O, i ?
IH(?x; z?, i) =?x=x1x2H(?x1, x2; z?, i)+?z=z1z2H(?x; z1, z2?, i) (27)Constraint (25) states that composition operationsthat result in the same structure should have the samevalue.
Constraints (26) and (27) ensure that the val-ues in the Hankel follow the inside-outside recur-sions that define the computations of a WCFG func-tion.
The following lemma formalizes this concept.LetH?
be the sub-block ofH restricted toO1?I1,i.e.
without compositions.Lemma 2.
If H satisfies constraints (25),(26) and(27), and if rank(H) = rank(H?)
then there existsa WCFG that generatesH?.See proof in Section A.3.4.2 Convex OptimizationWe now present the core optimization program be-hind our method.
Let vec(H) be a vector in R|O|?|I|corresponding to all coefficients of H in columnvector form.
Let O be a matrix such that O ?vec(H) = z represents the observation constraints.For example, if i-th row of O corresponds to theHankel coefficientH(??;?
?, (x)) then z(i) = p(x).Let K be a matrix such that K ?
vec(H) = 0 rep-resents the constraints (25), (26) and (27).The optimization problem is:minimizeHrank(H)subject to ?O ?
vec(H)?
z?2 ?
?K ?
vec(H) = 0?H?2 ?
1.
(28)Intuitively, we look for H that agrees with the ob-servable statistics and satisfies the inside-outsideconstraints.
?
is a parameter of the method that con-trols the degree of error in fitting the observables z.The ?H?2 ?
1 is satisfied by any Hankel matrixderived from a true distribution, and is used to avoidincoherent solutions.The above optimization problem, however, iscomputationally hard because of the rank objective.We employ a common relaxation of the rank objec-tive, based on the nuclear norm as in (Balle et al2012).
The optimization is:minimizeH?H?
?subject to ?O ?
vec(H)?
z?2 ?
?K ?
vec(H) = 0?H?2 ?
1.
(29)To optimize (29) we employ a projected gradientstrategy, similar to the FISTA scheme proposed byBeck and Teboulle (2009).
The method alternatesbetween separate projections for the observable con-straints, the `2 norm, the inside-outside constraints,and the nuclear norm.
Of these, the latter two are themost expensive.Elsewhere, we develop theoretical properties ofthe optimization (28) applied to finite-state transduc-tions (Bailly et al 2013).
One can prove that there istheoretical identifiability of the rank and the param-eters of an FST distribution, using a rank minimiza-tion formulation.
However, this problem is NP-hard,and it remains open whether there exists a polyno-mial method with identifiability results.
These re-sults should generalize to WCFG.5 ExperimentsIn this section we describe some experiments withthe learning algorithms for WCFG.
Our goal isto verify that the algorithms can learn some basiccontext-free languages, and to study the possibilityof using them on real data.5.1 Synthetic ExperimentsWe performed experiments on synthetic data, ob-tained by choosing a PCFG with random parameters(?
[0, 1]), with a normalization step in order to geta probability distribution.
We built the Hankel ma-trix from the inside basis {(x)}x??
and outside basis6291e-061e-050.00010.0010.010.11100  1000  10000  100000  1e+06KLdivergenceSample sizeUnsupervised SpectralSupervised SpectralUnsupervised EMSupervised EMFigure 1: KL divergence for spectral and EM methods,unsupervised and supervised, for different sizes of learn-ing sample, on log-log scales.
Results are averages over50 random target PCFG with 2 states and 2 symbols.{??;??}
?
{?x;?
?, ??;x?}x??.
The composed in-sides for the operator matrix are thus {(x, y)}x,y?
?.The matrix in the optimizer has the following struc-tureH =?????
(y) ?
?
?
(y, z)??;??
(?
; y;?)
?
?
?
(?
; y, z;?)?x;??
(x; y;?)
?
?
?
(x; y, z;?)??;x?
(?
; y;x) ?
?
?
(?
; y, z;x)... ?
?
?
?
?
?
?
?
?????
?The constraints we use are:K ={H((x; y;?))
= H((?
;x; y))}x,y???{H((?
;x; y)) = H((?
;x, y;?))}x,y???
{H((x; y;?))
= H((?
;x, y;?))}x,y?
?andO ={H((?;x;?))
= pS(x)}x??
?{H((?
;x; y)) = pS(xy)}x,y??
?
{H((x; y, z;?))
+H((?
;x, y; z)) = pS(xyz)}x,y,z?
?We use pS to denote the empirical distribution.Those are simplified versions of the Hankel, inside,outside and observation constraints.
The set O isbuilt from the following remarks: (1) (xy) = (x, y)and (2) (xyz) = (xy, z)+(x, yz).
The method usesstatistics for sequences up to length 3.The algorithm we use for the unsupervised spec-tral method is a simplified version: we use alter-natively a hard projection on the constraints (by1e-081e-071e-061e-050.00010.0010.010.11000  10000  100000  1e+06  1e+07  1e+08  1e+09KLdivergenceSample sizeUnsupervised SpectralSupervised SpectralFigure 2: KL divergence for unsupervised and supervisedspectral methods, for different sizes of learning sample,on log-log scales.
Results are averages over 50 randomtarget PCFG with 3 states and 6 symbols.projecting iteratively on each constraint), and athresholding-shrinkage operation for the target di-mension.
We use the same trick as FISTA for theupdate.
We finally use the regular spectral methodon this matrix to get our model.We compare this method with an unsupervisedEM, and also with supervised versions of spectralmethod and EM.
We compare the accuracy of thedifferent models in terms of KL-divergence for se-quences up to length 10.
We run 50 optimizationsteps for the unsupervised spectral method, and 200iterations for the EM methods.
Figure 1 shows theresults, corresponding the the geometric mean over50 experiments on random targets of 2 symbols and2 states.For sample size greater than 105, the unsupervisedspectral method seems to provide better solutionsthan both EM and supervised EM.
The solution, interms of KL-divergence, is comparable to the oneobtained with the supervised spectral method.
Thecomputation time of unsupervised spectral methodis almost constant w.r.t.
the sample size, around1.67s, while computation time of unsupervised EM(resp.
supervised EM) is 6.103s (resp.
2.104s) forsample size 106.Figure 2 presents learnings curve for random tar-gets with 3 states and 6 symbols.
One can see that,for big sample sizes (109), the unsupervised spectralmethod is losing accuracy compared to the super-vised method.
This is due to a lack of information,63000.20.40.60.811.21.41.60  0.01  0.02  0.03  0.04  0.05L1distanceBasis factorSpectral WFAUnsupervised SpectralSupervised SpectralFigure 3: Learning errors for different models in terms ofthe size of the basis.and could be overcome by considering a greater ba-sis (e.g.
inside sequences up to length 2 or 3).5.2 Dyck LanguagesWe now present experiments using the followingPCFG:S ?
S S (0.2) | aS b (0.4) | a b (0.4)This PCFG generates a probabilistic version of thewell-known Dyck language or balanced parenthesislanguage, an archetypical context-free language.We do experiments with the following models andalgorithms:?
WFA: a Weighted Finite Automata learned us-ing spectral methods as described in (Luque etal., 2012).
Parameters: number of states andsize of basis.?
Supervised Spectral: a WCFG learned fromstructured strings using the algorithm of sec-tion 3.2.
We choose as basis the most frequentinsides and outsides observed in the trainingdata.
The size of the basis is determined by aparameter f called the basis factor, that deter-mines the proportion of total insides and out-sides that will be in the basis.?
Unsupervised Spectral: a WCFG learned fromstrings using the algorithm of Section 4.
Thebasis is like in the supervised case, but sincecontext-free cuts in the strings are not observed,basis size ofH obs.
i/o ctr.1 ?
11 39 ?
159 34 1626 ?
14 1,163 ?
764 146 6,36012 ?
18 4,462 ?
2,239 322 25,37418 ?
22 9,124 ?
4,149 479 52,52424 ?
26 15,755 ?
6,858 657 89,71830 ?
29 19,801 ?
8,545 769 112,37436 ?
34 27,989 ?
11,682 916 156,69042 ?
37 3,638 ?
15,026 1,035 200,34648 ?
41 45,192 ?
18,235 1,157 244,39854 ?
45 53,741 ?
21,196 1,281 284,46660 ?
48 60,844 ?
23,890 1,382 318,354Table 1: Problem sizes for the WSJ10 training corpus.basis / n 5 10 15 201 ?
11 1.265 10?36 ?
14 7.06 10?4 6.92 10?412 ?
18 7.30 10?4 6.28 10?4 6.01 10?418 ?
22 7.31 10?4 6.29 10?4 5.84 10?4 5.59 10?424 ?
26 7.35 10?4 6.39 10?4 5.88 10?4 5.31 10?430 ?
29 7.34 10?4 6.41 10?4 5.86 10?4 5.30 10?4Table 2: Experiments with the unsupervised spectralmethod on the WSJ10 corpus.
Results are in terms ofexpected L1 on the training set, for different basis andnumbers of states.all possible inside and outsides of the sample(i.e.
all possible substrings and contexts) areconsidered.We generate a training set by sampling 4,000strings from the target PCFG and counting the rel-ative frequency of each.
For the supervised model,we generate strings paired with their context-freederivation.
To measure the quality of the learnedmodels, we use the L1 distance to the target distri-bution over a fixed set of strings ?
?n, for n = 7.1Figure 3 shows the results for the different mod-els and for different basis sizes (in terms of the basisfactor f ).
Here we can clearly see that the WCFGmodels, even the unsupervised one, outperform theWFA in reproducing the target distribution.5.3 Natural Language ExperimentsNow we present some preliminar tests using naturallanguage data.
For these tests, we used the WSJ10subset of the Penn Treebank, as Klein and Manning(2002).
This dataset consists of the sentences oflength ?
10 after filtering punctuation and currency.We removed lexical items and mapped the POS tags1Given two functions f1 and f2 over strings, the L1 distanceis the sum of the absolute difference over all strings in a set:?x |f1(x)?
f2(x)|.631to the Universal Part-of-Speech Tagset (Petrov et al2012), reducing the alphabet to a set of 11 symbols.Table 1 shows the size of the problem for differ-ent basis sizes.
As described in the previous sub-section for the unsupervised case, we obtain the ba-sis by taking the most frequent observed substringsand contexts.
We then compute all yields that canbe generated with this basis, and close the basis toinclude all possible insides and outsides with oper-ations completions, such that we create a Hankel asdescribed in Section 4.1.
Table 1 shows, for eachbase, the size of H we induce, the number of ob-servable constraints (i.e.
sentences we train from),and the number of inside-outside constraints.With the current implementation of the optimizerwe were only able to run the unsupervised learningfor small basis sizes.
Table 2 shows the expected L1on training data.
For a fixed basis, as we increasethe number of states we see that the error decreases,showing that the method is inducing a Hankel matrixthat explains the observable statistics.6 ConclusionsWe have presented a novel approach for unsuper-vised learning of WCFG.
Our method combines in-gredients of spectral learning with low-rank convexoptimization methods.Our method optimizes over a matrix that, even if itgrows polynomially with respect to the size of train-ing, results in a large problem.
To scale the methodto learn languages of the complexity of natural lan-guages we would need to identify optimization algo-rithms specially suited for this problem.A ProofsA.1 Proof of Inside-Outside Eq.
16 and 17For the inside function, the base case is trivial.
Byinduction:?G(x)[i] =?x=x1x2A(?G(x1)?
?G(x2))[i]=?j,k?Vx=x1x2A[i, j, k] ?
?G(x1)[j] ?
?G(x2)[k]=?j,k?Vx=x1x2wR(i?
j k) ?
??G?(j?=?
x1) ?
??G?(k?=?
x2)= ??G?(i?=?
x)For the outside function, let ei be an n-dimensional vector with coordinate i to 1 and therest to 0.
We reformulate the mapping as:?G(x; z)>ei = ??G?
(x; i; z) (30)The base case is trivial by definitions.
We use theproperty of Kronecker products that (v ?
In)v?
=(v?
v?)
and (In?
v)v?
= (v??
v) for v,v?
?
Rn.We first look at one of the terms of ?G(x; z)>ei:?G(x1; z)>A(?G(x2)?
In)ei= ?G(x1; z)>A(?G(x2)?
ei)=?j,k?V(?G(x1; z)>ej) ?A[j, k, i] ?
?G(x2)[k]=?j,k?V??G?
(x1; j; z) ?
wR(j ?
k i) ?
??G?(k?=?
x2)Applying the distributive property in ?G(x; z)>ei itis easy to see that all terms are mapped to the corre-sponding term in ??G?
(x; i; z).A.2 Proof of Lemma 1Let G?
= ???
?, {???},A??
be a WCFG for f that in-duces a rank factorizationH = P ?S?.
We first showthat there exists an invertible matrixM that changesthe basis of the operators of G into those of G?.Define M = S?S+ and note that P+P ?S?S+ =P+HS+ = I implies that M is invertible withM?1 = P+P ?.
We now check that the operatorsof G correspond to the operators of G?
under thischange of basis.
First we see thatA = P+HA(S ?
S)+= P+P ?A?(S?
?
S?
)(S ?
S)+= M?1A?
(S?S+ ?
S?S+)= M?1A?
(M ?M) .Now, since h?
= ??>?
S?
and h?
= P ????
, it followsthat ?>?
= ??
?>M and ??
= M?1??
?.Finally we check that G and G?
compute thesame function, namely f(o, i) = ?G(o)>?G(i) =?G?(o)>?G?(i).
We first see that ?G(x) =632M?1?G?(x):?G(?)
= ??
= M?1???
(31)?G(x) =?x=x1x2A(?G(x1)?
?G(x2)) (32)=?x=x1x2M?1A?
(M ?M)(?G(x1)?
?G(x2))= M?1?x=x1x2A?
(M?G(x1)?M?G(x2))= M?1?x=x1x2A?(?G?(x1)?
?G?
(x2))It can also be shown that ?G(x; z)> =?G?
(x; z)>M .
One must see that in any term:?G(x1; z)>A(?G(x2)?
In) (33)= ?G(x1; z)>M?1A?
(M ?M)(?G(x2)?
In)= ?G?
(x1; z)>A?
(M?G(x2)?MIn)= ?G?
(x1; z)>A?(?G?(x2)?
In)Mand the relation follows.
Finally:?G(x; z)>?G(y) (34)= ?G?
(x; z)>MM?1?G?
(y)= ?G?
(x; z)>?G?
(y)A.3 Proof of Lemma 2We will use the following sub-blocks ofH:?
H?
is the sub-block restricted to O1 ?
I1, i.e.without compositions.?
HA is the sub-block restricted to O1 ?
I2, i.e.inside compositions.?
H ?A is the sub-block restricted to O2 ?
I1, i.e.outside compositions.?
h>?
?
RI1 is the row ofH?
for ??;??.?
h(x) ?
RO1is the column ofH?
for (x).?
h(x1,x2) ?
RO1 is the column of HA for(x1, x2).?
h??x;z?
?
RI1 is the row of h?
for ?x; z?.?
h??x1,x2;z?
and h??x;z1,z2?
be the rows in RI1 ofh?A for ?x1, x2; z?
and ?x; z1, z2?
).One supposes that rank(H?)
= rank(H).
We de-fine G as?>?
= h>?H+?
, ?a = h(a),A = HA(H+?
?H+?
)Lemma 3.
One has that ?G(x) = h(x), and?G(x1, x2) = h(x1,x2).Proof.
By induction.
For sequences of size 1, onehas ?G(x) = ?x = h(x).
For the recursive case,let e(x) be a vector in RI1with 1 in the coordinateof (x) in H?.
Let e(x,y) be a vector in RI2with 1in the coordinate of (x, y) in HA.
For ?G(x, y),one has H+?
?G(x) = e(x), and H+?
?G(y) =e(y), thus H+?
?G(x) ?
H+?
?G(y) = e(x,y) andHA(H+?
?G(x) ?
H+?
?G(y)) = h(x,y).
Finally,one has that ?G(x) =?x=x1x2 ?G(x1, x2) =?x=x1x2 h(x1,x2) = h(x1x2x3) by the equation(26).One has a symmetric result for outside vectors.
Wedefine G?
as?>?
= h>?
, ?a = H+?
h(a),A = H+?HALemma 4.
One has that ?G?
(?x; z?
)> =h?
?x;z?, ?G?
(?x1, x2; z?
)> = h??x1,x2;z?
and?G?
(?x; z1, z2?
)> = h??x;z1,z2?.Proof.
(Sketch) Equation (31) is used in the sameway than (27) before.
Equation (25) is used to en-sure a link betweenH ?A andHA.Let g be the mapping computed by G andG?.
One has that g(o, i) = ?G?(o)>?G?
(i) =?G(o)>?G(i) = ?G?(o)>H+?
?G(i) = H?
(o, i).Acknowledgements We are grateful to Borja Balleand the anonymous reviewers for providing us with help-ful comments.
This work was supported by a GoogleResearch Award, and by projects XLike (FP7-288342),ERA-Net CHISTERA VISEN, TACARDI (TIN2012-38523-C02-02), BASMATI (TIN2011-27479-C04-03),SGR-GPLN (2009-SGR-1082) and SGR-LARCA (2009-SGR-1428).
Xavier Carreras was supported by theRamo?n y Cajal program of the Spanish Government(RYC-2008-02223).
Franco M. Luque was supported bythe National University of Co?rdoba and by a Postdoc-toral fellowship of CONICET, Argentinian Ministry ofScience, Technology and Productive Innovation.633ReferencesPieter Adriaans, Marten Trautwein, and Marco Vervoort.2000.
Towards high speed grammar induction on largetext corpora.
In SOFSEM 2000: Theory and Practiceof Informatics, pages 173?186.
Springer.Raphae?l Bailly, Franois Denis, and Liva Ralaivola.
2009.Grammatical inference as a principal component anal-ysis problem.
In Le?on Bottou and Michael Littman,editors, Proceedings of the 26th International Confer-ence on Machine Learning, pages 33?40, Montreal,June.
Omnipress.Raphae?l Bailly, Amaury Habrard, and Franc?ois Denis.2010.
A spectral approach for probabilistic grammat-ical inference on trees.
In Proceedings of the 21stInternational Conference Algorithmic Learning The-ory, Lecture Notes in Computer Science, pages 74?88.Springer.Raphae?l Bailly, Xavier Carreras, and Ariadna Quattoni.2013.
Unsupervised spectral learning of finite-statetransducers.
In Advances in Neural Information Pro-cessing Systems 26.James K. Baker.
1979.
Trainable grammars for speechrecognition.
In D. H. Klatt and J. J. Wolf, editors,Speech Communication Papers for the 97th Meetingof the Acoustical Society of America, pages 547?550.Borja Balle, Ariadna Quattoni, and Xavier Carreras.2011.
A spectral learning algorithm for finite statetransducers.
In Proceedings of ECML PKDD, pages156?171.Borja Balle, Ariadna Quattoni, and Xavier Carreras.2012.
Local loss optimization in operator models: Anew insight into spectral learning.
In John Langfordand Joelle Pineau, editors, Proceedings of the 29th In-ternational Conference on Machine Learning (ICML-2012), ICML ?12, pages 1879?1886, New York, NY,USA, July.
Omnipress.Amir Beck and Marc Teboulle.
2009.
A fast iter-ative shrinkage-thresholding algorithm for linear in-verse problems.
SIAM J. Img.
Sci., 2(1):183?202,March.Stanley F Chen.
1995.
Bayesian grammar induction forlanguage modeling.
In Proceedings of the 33rd annualmeeting on Association for Computational Linguistics,pages 228?235.
Association for Computational Lin-guistics.Alexander Clark.
2001.
Unsupervised induction ofstochastic context-free grammars using distributionalclustering.
In Proceedings of the 2001 workshop onComputational Natural Language Learning-Volume 7,page 13.
Association for Computational Linguistics.Alexander Clark.
2007.
Learning deterministic contextfree grammars: The omphalos competition.
MachineLearning, 66(1):93?110.Shay B. Cohen, David M. Blei, and Noah A. Smith.2010.
Variational inference for adaptor grammars.In Human Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 564?572, Los Angeles, California, June.
Association forComputational Linguistics.Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.Foster, and Lyle Ungar.
2012.
Spectral learning oflatent-variable pcfgs.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 223?231,Jeju Island, Korea, July.
Association for Computa-tional Linguistics.Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.Foster, and Lyle Ungar.
2013.
Experiments with spec-tral learning of latent-variable pcfgs.
In Proceedingsof the 2013 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 148?157, At-lanta, Georgia, June.
Association for ComputationalLinguistics.Matthew Gormley and Jason Eisner.
2013.
Nonconvexglobal optimization for latent-variable models.
In Pro-ceedings of the 51st Annual Meeting of the Associationfor Computational Linguistics (ACL), Sofia, Bulgaria,August.
11 pages.Daniel Hsu, Sham M. Kakade, and Tong Zhang.
2009.
Aspectral algorithm for learning hidden markov models.In Proceedings of the Annual Conference on Compu-tational Learning Theory (COLT).Daniel Hsu, Sham Kakade, and Percy Liang.
2012.Identifiability and unmixing of latent parse trees.
InP.
Bartlett, F.C.N.
Pereira, C.J.C.
Burges, L. Bottou,and K.Q.
Weinberger, editors, Advances in Neural In-formation Processing Systems 25, pages 1520?1528.Dan Klein and Christopher D Manning.
2002.
A genera-tive constituent-context model for improved grammarinduction.
In Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, pages128?135.
Association for Computational Linguistics.Kenichi Kurihara and Taisuke Sato.
2006.
Variationalbayesian grammar induction for natural language.
InGrammatical Inference: Algorithms and Applications,pages 84?96.
Springer.Karim Lari and Steve J Young.
1990.
The estimationof stochastic context-free grammars using the inside-outside algorithm.
Computer speech & language,4(1):35?56.Percy Liang, Slav Petrov, Michael I Jordan, and DanKlein.
2007.
The infinite PCFG using hierarchicaldirichlet processes.
In EMNLP-CoNLL, pages 688?697.634Franco M. Luque, Ariadna Quattoni, Borja Balle, andXavier Carreras.
2012.
Spectral learning for non-deterministic dependency parsing.
In Proceedingsof the 13th Conference of the European Chapter ofthe Association for Computational Linguistics, pages409?419, Avignon, France, April.
Association forComputational Linguistics.Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimation from partially bracketed corpora.In Proceedings of the 30th Annual Meeting of the As-sociation for Computational Linguistics, pages 128?135, Newark, Delaware, USA, June.
Association forComputational Linguistics.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proceedings ofLREC, May.Kewei Tu and Vasant Honavar.
2008.
Unsupervisedlearning of probabilistic context-free grammar usingiterative biclustering.
In Grammatical Inference: Al-gorithms and Applications, pages 224?237.
Springer.Menno Van Zaanen.
2000.
Abl: Alignment-based learn-ing.
In Proceedings of the 18th conference on Compu-tational linguistics-Volume 2, pages 961?967.
Associ-ation for Computational Linguistics.635
