Lexical Semantic Techniques for CorpusAnalysisJames Pustejovsky*Brandeis UniversityPeter Anick~Digital Equipment CorporationSabine Bergler tConcordia UniversityIn this paper we outline a research program for computational linguistics, making extensive useof text corpora.
We demonstrate how a semantic framework for lexical knowledge can suggestricher relationships among words in text beyond that of simple co-occurrence.
The work suggestshow linguistic phenomena such as metonymy and polysemy might be exploitable for semantictagging of lexical items.
Unlike with purely statistical collocational analyses, the frameworkof a semantic theory allows the automatic onstruction of predictions about deeper semanticrelationships among words appearing in collocational systems.
We illustrate the approach forthe acquisition oflexical information for several classes of nominals, and how such techniques canfine-tune the lexical structures acquired from an initial seeding of a machine-readable dictionary.In addition to conventional lexical semantic relations, we show how information concerninglexical presuppositions and preference r lations can also be acquired from corpora, when analyzedwith the appropriate semantic tools.
Finally, we discuss the potential that corpus studies have forenriching the data set for theoretical linguistic research, as well as helping to confirm or disconfirmlinguistic hypotheses.1.
IntroductionThe proliferation of on-line textual information poses an interesting challenge to lin-guistic researchers for several reasons.
First, it provides the linguist with sentence andword usage information that has been difficult to collect and consequently argelyignored by linguists.
Second, it has intensified the search for efficient automated in-dexing and retrieval techniques.
FulMext indexing, in which all the content words ina document are used as keywords, is one of the most promising of recent automatedapproaches, yet its mediocre precision and recall characteristics indicate that there ismuch room for improvement (Croft 1989).
The use of domain knowledge can enhancethe effectiveness of a full-text system by providing related terms that can be used tobroaden, narrow, or refocus a query at retrieval time (Debili, Fluhr, and Radasua 1988;Anick et al 1989.
Likewise, domain knowledge may be applied at indexing time todo word sense disambiguation (Krovetz and Croft 1989) or content analysis (Jacobs1991).
Unfortunately, for many domains, such knowledge, even in the form of a the-saurus, is either not available or is incomplete with respect o the vocabulary of thetexts indexed.
* Computer Science Department, Brandeis University, Waltham MA 02254.t Computer Science Department, Concordia University, Montreal, Quebec H3G 1M8, Canada.Digital Equipment Corporation, 111 Locke Drive LM02-1/D12, Marlboro MA 01752.
(~) 1993 Association for Computational LinguisticsComputational Linguistics Volume 19, Number 2In this paper we examine how linguistic phenomena such as metonymy and po-lysemy might be exploited for the semantic tagging of lexical items.
Unlike purelystatistical collocational analyses, employing a semantic theory allows for the auto-matic construction of deeper semantic relationships among words appearing in collo-cational systems.
We illustrate the approach for the acquisition of lexical informationfor several classes of nominals, and how such techniques can fine-tune the lexicalstructures acquired from an initial seeding of a machine-readable dictionary.
In addi-tion to conventional lexical semantic relations, we show how information concerninglexical presuppositions and preference r lations (Wilks 1978) can also be acquired fromcorpora, when analyzed with the appropriate semantic tools.
Finally, we discuss thepotential that corpus studies have for enriching the data set for theoretical linguisticresearch, as well as helping to confirm or disconfirm linguistic hypotheses.The aim of our research is to discover what kinds of knowledge can be reliablyacquired through the use of these methods, exploiting, as they do, general inguis-tic knowledge rather than domain knowledge.
In this respect, our program is simi-lar to Zernik's (1989) work on extracting verb semantics from corpora using lexicalcategories.
Our research, however, differs in two respects: first, we employ a moreexpressive lexical semantics; second, our focus is on all major categories in the lan-guage, and not just verbs.
This is important since for full-text information retrieval,information about nominals is paramount, as most queries tend to be expressed asconjunctions of nouns.
From a theoretical perspective, we believe that the contribu-tion of the lexical semantics of nominals to the overall structure of the lexicon has beensomewhat neglected, relative to that of verbs.
While Zernik (1989) presents ambiguityand metonymy as a potential obstacle to effective corpus analysis, we believe thatthe existence of motivated metonymic structures actually provides valuable clues forsemantic analysis of nouns in a corpus.We will assume, for this paper, the general framework of a generative lexicon asoutlined in Pustejovsky (1991).
In particular, we make use of the principles of typecoercion and qualia structure.
This model of semantic knowledge associated withwords is based on a system of generative devices that is able to recursively definenew word senses for lexical items in the language.
These devices and the associateddictionary make up a generative l xicon, where semantic information is distributedthroughout the lexicon to all categories.
The general framework assumes four basiclevels of semantic description: argument structure, qualia structure, lexical inheritancestructure, and event structure.Connecting these different levels is a set of generative devices that provide forthe compositional interpretation of words in context.
The most important of thesedevices is a semantic transformation called type coercion--analogous to coercion inprogramming languages--which captures the semantic relatedness between syntacti-cally distinct expressions.
As an operation on types within a A-calculus, type coercioncan be seen as transforming a monomorphic language into one with polymorphictypes (cf.
Cardelli and Wegner 1985).
Argument, event, and qualia types must con-form to the well-formedness conditions defined by the type system defined by thelexical inheritance structure when undergoing operations of semantic omposition.
~1 The details of type coercion need not concern us here.
Briefly, however, whenever there exists agrammatical environment where more than one syntactic type satisfies the semantic type selected bythe governing element, the governing element can be analyzed as coercing a range of surface typesinto a single semantic type.
An example of subject ype coercion is a causative verb, semanticallyselecting an event as subject (as in (i)), but syntactically permitting a nonevent denoting NP (as in (ii)):i.
The flood killed the grass.ii.
The herbicide killed the grass.332James Pustejovsky et al Lexical Semantic Techniques for Corpus AnalysisOne component of this approach, the qualia structure, specifies the different as-pects of a word's meaning through the use of subtyping.
These include the subtypesCONSTITUTIVE, FORMAL, TELIC, and AGENTIVE.
To illustrate how these are used, thequalia structure for book is given below.
2?
book(x ,y)  .
\]GONST = in fo rmat ion(y l  \]FORMAL = physobj(x) \[TELIC = read(T,w,y) \[AGENTIVE = write(T,z,y)  JThis structured representation allows one to use the same lexical entry in differentcontexts, where the word refers to different qualia of the noun's denotation.
For ex-ample, the sentences in (1)-(3) below refer to different aspects (or qualia) of the generalmeaning of book.
3Example 1This book weighs four ounces.Example 2John finished a book.Example 3This is an interesting book.Example 1 makes reference to the formal role, while 3 refers to the constitutive role.Example 2, however, can refer to either the telic or the agentive aspects given above.The utility of such knowledge for information retrieval is readily apparent.
This theoryclaims that noun meanings should make reference to related concepts and the relationsinto which they enter.
The qualia structure, thus, can be viewed as a kind of generictemplate for structuring this knowledge.
Such information about how nouns relate toother lexical items and their concepts might prove to be much more useful in full-textinformation retrieval than what has come from standard statistical techniques.To illustrate how such semantic structuring might be useful, consider the generalclass of artifact nouns.
A generative view of the lexicon predicts that by classifyingan element into a particular category, we can generate many aspects of its semanticstructure, and hence, its syntactic behavior.
For example, the representation above forbook refers to several word senses, all of which are logically related by the semantictemplate for an artifactual object.
That is, it contains information, it has a materialextension, it serves some function, and it is created by some particular act or event.2 Briefly, the qualia can be defined as follows:?
CONSTITUTIVE: the relation between an object and its constituent parts;?
FORMAL: that which distinguishes it within a larger domain;?
TELIC: its purpose and function;?
AGENTIVE: factors involved in its origin or "bringing it about.
"In the qualia structures given below, we adopt the convention that \[c~, r\] denotes conjunction offormulas within the feature structure, while \[a; r\] will denote disjunction.3 A related approach for expressing the different semantic relations of nominals in distinguished contextsis given in Bierwisch (1983).333Computational Linguistics Volume 19, Number 2Such an analysis allows us to minimally structure objects according to these fourqualia.As an example of how objects cluster according to these dimensions, we willbriefly consider three object ypes: (1) containers (of information), e.g., book, tape, record;(2) instruments, e.g., gun, hammer, paintbrush; and (3) figure-ground objects, e.g., door,room, fireplace.
Because of how their qualia structures differ, these classes appear invastly different grammatical contexts.As with containers in general, information containers permit metonymic exten-sions between the container and the material contained within it.
Collocations uchas those in Examples 4 through 7 indicate that this metonymy is grammaticalizedthrough specific and systematic head-PP constructions.Example 4read a bookExample 5read a story in a bookExample 6read a tapeExample 7read the information on the tapeInstruments, on the other hand, display classic agent-instrument causative alter-nations, such as those in Examples 8 through 11 (cf.
Fillmore 1968; Lakoff 1968, 1970).Example 8... smash the vase with the hammerExample 9The hammer smashed the vase.Example 10... kill him with a gunExample 11The gun killed him.Finally, figure-ground nominals (Pustejovsky and Anick 1988) permit perspectiveshifts such as those in Examples 12 through 15.
These are nouns that refer to physicalobjects as well as the specific enclosure or aperture associated with it.Example 12John painted the door.Example 13John walked through the door.334James Pustejovsky et al Lexical Semantic Techniques for Corpus AnalysisExample 14John is scrubbing the fireplace.Example 15The smoke filled the fireplace.That is, paint and scrub are actions on physical objects while walk through andfill are processes in spaces.
These collocational patterns, we argue, are systematicallypredictable from the lexical semantics of the noun, and we term such sets of collocatedstructures lexical conceptual paradigms (LCPs).
4To make this point clearer, let us consider a specific example of an LCP fromthe computer science domain, namely for the noun tape.
Because of the particularmetonymy observed for a noun like tape, we will classify it as belonging to the con-tainer/containee LCP.
This general class is represented as follows, where P and 0 arepredicate variables: 5container(x,y) \]CONST = P(y)FORMAL = Q(x)TELIC = hold(S,x,y)The LCP is a generic qualia structure that captures not only the semantic relationshipbetween arguments types of a relation, but also, through corpus-tuning, the collocationrelations that realize these roles.
The telic function of a container, for example, is therelation hold, but this underspecifies which spatial prepositions would adequatelysatisfy this semantic relation (e.g.
in, on, inside, etc.
).In this view, a noun such as tape would have the following qualia structure:tape(x,y)CONST = information(y)FORMAL = physobj(x),2-dimen(x)TELIC = contain(S,x,y)AGENTIVE = write(T,z,y)This states that a tape is an "information container" that is also a two-dimensionalphysical object, where the information is written onto the object.
6With such nouns, alogical metonymy exists (as the result of type coercion), when the logical argument ofa semantic type, which is selected by a function of some sort, denotes the semantictype itself.
Thus, in this example, the type selected for by a verb such as read refers tothe "information" argument for tape, while a verb such as carry would select for the"physical object" argument.
They are, however, logically related, since the noun itselfdenotes a relation.The representation above simply states that any semantics for tape must logicallymake reference to the object itself (formal), what it can contain (const), what purpose4 This relates to Mel'~uk's lexical functions and the syntactic structures they associate with an element.See Mel'~uk (1988) and references therein.
Cruse (1986, 1992) and Nunberg (1978) discuss theforegrounding and backgrounding of information with respect o similar examples.5 Within the qualia structure for a term, FORMAL and CONST roles typically refer to the object domainwhile TELI?
and ACENTIVE refer to events.
Hence, the first parameter in the latter two roles refers to anevent sort, i.e., a state (s), process (p), or transition (T).6 The appropriate selection of a surface spatial preposition will follow from its formal type specificationas a 2-dimen object.
Cf.
Pustejovsky (in press) for details.335Computational Linguistics Volume 19, Number 2it serves (telic), and how it arises (agentive).
This provides us with a semantic repre-sentation that can capture the multiple perspectives a single lexical item may assumein different contexts.
Yet, the qualia for a lexical item such as tape are not isolatedvalues for that one word, but are integrated into a global knowledge base indicatinghow these senses relate to other lexical items and their senses.
This is the contributionof inheritance and the hierarchical structuring of knowledge (cf.
Evans and Gazdar1990; Copestake and Briscoe 1992; Russell et al 1992).
In Pustejovsky (1991) it is sug-gested that there are two types of relational structures for lexical knowledge; a fixedinheritance similar to that of an i s -a  hierarchy (cf.
Touretzky 1986); and a dynamicstructure that operates generatively from the qualia structure of a lexical item to createa relational structure for ad hoc categories.
7Reviewing briefly, the basic idea is that semantics allows for the dynamic cre-ation of arbitrary concepts through the application of certain transformations to lexicalmeanings.
Thus for every predicate, Q, we can generate its opposition, =Q.
Similarly,these two predicates can be related temporally to generate the transition events defin-ing this opposition.
These operations include but may not be limited to: -~, negation;_<, temporal precedence; >, temporal succession; =, temporal equivalence; and act, anoperator adding agency to an argument.
We will call the concept space generated bythese operations the Projective Conclusion Space of a specific quale for a lexical item.
Toreturn to the example of tape above, the predicates read and copy are related to the telicvalue by just such an operation, while predicates such as mount and dismount--i.e, un-mount--are related to the formal role.
Following the previous discussion, with mountedas the predicate Q, successive applications of the negation and temporal precedenceoperators derives the transition verbs mount and dismount.
8 We return to a discussionof this in Section 3, and to how this space relates to statistically significant collocationsin text.It is our view that the approach outlined above for representing lexical knowl-edge can be put to use in the service of information retrieval tasks.
In this respect,our proposal can be compared to attempts at object classification in information sci-ence.
One approach, known as faceted classification (Vickery 1975) proceeds roughlyas follows: collect all terms lying within a field; then group the terms into facets byassigning them to categories.
Typical examples of this are state, property, reaction, anddevice.
However, each subject area is likely to have its own sets of categories, whichmakes it difficult to re-use a set of facet classifications.
9Even if the relational information provided by the qualia structure and inheri-tance would improve performance in information retrieval tasks, one problem stillremains, namely that it would be very time-consuming to hand-code such structuresfor all nouns in a domain.
Since it is our belief that such representations are genericstructures across all domains, it is our long-term goal to develop methods for auto-matically extracting these relations and values from on-line corpora.
In the sectionsthat follow, we describe several experiments indicating that the qualia structures do, infact, correlate with well-behaved collocational patterns, thereby allowing us to performstructure-matching operations over corpora to find these relations.7 This is similar to thesauruslike structures, within the IR community, cf.
for example Sparck Jones (1981).8 Details of the derivation are as follows.
Let Q be mounted, then ~Q gives ~mounted, and K applied tothese two states gives Q < -~Q, which is lexicalized as dismount.
A similar derivation exists for mount.Cf.
Pustejovsky (1991) for details.9 This is reflected in the sublanguage work of Grishman, Hirschman, and Nhan (1986), whose automateddiscovery procedures are aimed at clustering nouns into categories like diagnosis and symptom.336James Pustejovsky et al Lexical Semantic Techniques for Corpus Analysis2.
Seeding Lexical Structures from MRDsIn this section we discuss briefly how a lexical semantic theory can help in extract-ing information from machine-readable dictionaries (MRDs).
We describe research onconversion of a machine-tractable dictionary (Wilks et al 1993) into a usable lexicalknowledge base (Boguraev 1991).
Although the results here are preliminary, it is im-portant to mention the process of converting an MRD into a lexical knowledge base, sothat the process of corpus-tuning is put into the proper perspective.
The initial seed-ing of lexical structures i being done independently both from the Oxford AdvancedLearners Dictionary (OALD) and from lexical entries in the Longman Dictionary ofContemporary English (Procter, Ilson, and Ayto 1978).
These are then automaticallyadapted to the format of generative l xical structures.
It is these lexical structures thatare then statistically tuned against the corpus, following the methods outlined in Anickand Pustejovsky (1990) and Pustejovsky (1992).Previous work by Amsler (1980), Calzolari (1984), Chodorow, Byrd, and Heidorn(1985), Byrd et al (1987), Markowitz, Ahlswede, and Evens (1986), and Nakamuraand Nagao (1988) showed that taxonomic information and certain semantic relationscan be extracted from MRDs using fairly simple techniques.
Later work by Veronisand Ide (1991), Klavans, Chodorow, and Wacholder (1990), and Wilks et aL (1992)provides us with a number of techniques for transfering information from MRDs to arepresentation language such as that described in the previous ection.
Our goal is toautomate, to the extent possible, the initial construction of these structures.Extensive research as been done on the kind of information eeded by naturallanguage programs and on the representation f that information (Wang, Vanden-dorpe, and Evens 1985; Ahlswede and Evens 1988).
Following Boguraev et al (1989)and Wilks et al of 1989), we believe that much of what is needed for NLP lexicons canbe found either explicitly or implicitly in a dictionary, and empirical evidence suggeststhat this information gives rise to a sufficiently rich lexical representation for use inextracting information from texts.
Techniques for identifying explicit information inmachine-readable dictionaries have been developed by many researchers (Boguraevet al 1989; Slator 1988; Slator and Wilks 1987; Guthrie et al 1990) and are well under-stood.
Many properties of a word sense or the semantic relationships between wordsenses are available in MRDs, but this information can only be identified computa-tionally through some analysis of the definition text of an entry (Atkins 1991).
Someresearch as already been done in this area.
Alshawi (1987), Boguraev et al (1989),Vossen, Meijs, and den Broeder (1989), and the work described in Wilks et al (1992)have made explicit some kinds of implicit information found in MRDs.
Here we pro-pose to refine and merge some of the previous techniques to make explicit he implicitinformation specified by a theory of generative l xicons.Given what we described above for the lexical structures for nominals, we canidentify these semantic relations in the OALD and LDOCE by pattern matching on theparse trees of definitions.
To illustrate what specific information can be derived by au-tomatic seeding from machine-readable dictionaries, consider the following examples) ?For example, the LDOCE definition for book is:"a collection of sheets of paper fastened together as a thing to be read,or to be written in"10 The following lexical entries, termed gls's, are taken from the lexical databases derived from the OALDusing tools developed by Peter Dilworth, and from LDOCE using a combination f tools developed byLouise Guthrie, Gees Stein, and Pete Dilworth.337Computational Linguistics Volume 19, Number 2while the OALD provides a somewhat different definition:"number of sheet of papers, either printed or blank, fastened togetherin a cover.
"Note that both definitions are close to, but not identical to the information structuresuggested in the previous ection, using a qualia structure for nominals.
LDOCE sug-gests write in rather than write as the value for the telic role, while the OALD suggestsnothing for this role.
Furthermore, although the physical contents of a book as "acollection of sheets of paper" is mentioned, nowhere is information made reference toin the definition.
When the dictionary fails to provide the value for a semantic role,the information must be either hand-entered or the lexical structure must be tunedagainst a large corpus, in the hope of extracting such features automatically.
We turnto this issue in the next two sections.Although the two dictionaries differ in substantial respects, it is remarkable howsystematic the definition structures are for extracting semantic information, if there is aclear idea how this information should be structured.
For example, from the followingOALD definition for cigarette,cigarette n roll of shredded tobacco enclosed in thin paper for smok-ing.the initial lexical structure below is generated.gls (cigarette,syn( \[type (n),code(C)\] ),qualia ( \[formal( \[roll\] ),telic ( \[smoking\] ),const ( \[tobacco ,paper\] ),agent ( \[enclosed\] )\] ),cospec ( \[\] ) ) .Parsing the LDOCE entry for the same noun results in a different lexical structure:c igarette  n finely cut shredded tobacco rolled in a narrow tube ofthin paper for smoking.gls (cigarette,syn( \[type (n),code(C),ldoce id(cigarette 0_1)\]),qualia( \[formal( \[tube\] ),telic( \[smoking\] ),const ( \[tobacco ,paper\] ),agent ( \[rolled\] )\] ),cospec ( \[\] ) ) .One obvious problem with the above representation is that there is no informationindicating how the word being defined binds to the relations in the qualia.
Currently,338James Pustejovsky et al Lexical Semantic Techniques for Corpus Analysissubsequent routines providing for argument binding analyze the relational structurefor particular aspects of noun meaning, giving us a lexical structure fairly close towhat we need for representation a d retrieval purposes, although the result is in noway ideal or uniform over all nominal forms.
(cf.
Cowie, Guthrie, and Pustejovsky\[1992\] for details of this operation on LDOCE.
): 11cigarette(x)CONST = tobacco(y),shredded(y),paper(z)FOaMAL = roll(x)TELIC = smoke(T,w,x)AGENTIVE = artifact(x)In a related set of experiments performed while constructing a large lexical data-base for data extraction purposes, we seeded a lexicon with 6000 verbs from LDOCE.This process and the corpus tuning for both argument typing and subcategorizationacquisition are described in Cowie, Guthrie, and Pustejovsky (1992) and Pustejovskyet al (1992).In summary, based on a theory of lexical semantics, we have discussed how anMRD can be useful as a corpus for automatically seeding lexical structures.
Rather thanaddressing the specific problems inherent in converting MRDs into useful lexicons,we have emphasized how it provides us, in a sense, with a generic vocabulary fromwhich to begin lexical acquisition over corpora.
In the next section, we will addressthe problem of taking these initial, and often very incomplete l xical structures, andenriching them with information acquired from corpus analysis.
As mentioned inthe previous ection, the power of a generative l xicon is that it takes much of theburden of semantic interpretation ff of the verbal system by supplying a much richersemantics for nouns and adjectives.
This makes the lexical structures ideal as an initialrepresentation for knowledge acquisition and subsequent information retrieval tasks.3.
Knowledge Acquisition from CorporaA machine-readable dictionary provides the raw material from which to constructcomputationally useful representations of the generic vocabulary contained within it.The lexical structures discussed in the previous ection are one example of how suchinformation can be exploited.
Many sublanguages, however, are poorly represented inon-line dictionaries, if represented atall.
Vocabularies geared to specialized omainswill be necessary for many applications, uch as text categorization a d informationretrieval.
The second area of our research program that we discuss is aimed at devel-oping techniques for building sublanguage l xicons via syntactic and statistical corpusanalysis coupled with analytic techniques based on the tenets of generative l xicontheory.To understand fully the experiments described in the next two sections, we willrefer to several semantic notions introduced in previous ections.
These include typecoercion, where a lexical item requires a specific type specification for its argument, and11 As one reviewer correctly pointed out, more than simple argument binding is involved here.
Forexample, the model must know that paper can enclose shredded tobacco, but not the reverse.
Suchinformation, typically part of commonsense knowledge, is well outside the domain of lexicalsemantics, as envisioned here.
One approach to this problem, consistent with our methodology, is toexamine the corpus and the collocations that result from training on specific qualia relations.
Furtherwork will hopefully clarify the nature of this problem, and whether it is best treated lexically or not.339Computational Linguistics Volume 19, Number 2the argument is able to change type accordingly--this explains the behavior of logicalmetonymy and the syntactic variation seen in complements o verbs and nominals;and cospecification, a semantic tagging of what collocational patterns the lexical itemmay enter into.Metonymy, in this view, can be seen as a case of the "licensed violation" of selec-tional restrictions.
For example, while the verb announce selects for a human subject,sentences like The Dow Corporation announced third quarter losses are not only an accept-able paraphrase of the selectionally correct form Mr. Dow Jr. announced third quarterlosses for Dow Corp, but they are the preferred form in the corpora being examined.This is an example of subject ype coercion, where the semantics for Dow Corp as acompany must specify that there is a human typically associated with such officialpronouncements (see Section 5).
12For one set of experiments, we used a corpus of approximately 3,000 articles writ-ten by Digital Equipment Corporation's Customer Support Specialists for an on-linecomputer troubleshooting library.
The articles, each one- to two-page long descriptionsof a problem and its solution, comprise about I million words.
Our analysis proceedsin two phases.
In the first phase, we pre-process the corpus to build a database ofphrasal relationships.
This consists briefly of the following steps:1.
Perform unknown word resolution to the corpus.
The corpus issearched for strings that are not members of a 25,000 word genericon-line English lexicon.
Morphological analysis is then applied to theseunknown strings to identify candidate citation forms and their likelymorphological paradigms.
Unless morphological evidence indicatesotherwise, we enter unknown words into the lexicon as regular nouns; ifthere is evidence of some other morphological paradigm, such as verbalor adjectival suffixes, the word is entered into the lexicon accordingly.2.
Corpus tagging.
Once the lexicon is updated to include the new singleword forms in the domain, the corpus is tagged with part-of-speechindicators.
Any words that are ambiguous with respect o category aredisambiguated according to a set of several dozen ordereddisambiguation heuristics, which choose a category based on thecategories of the words immediately preceding and following theambiguous term.3.
Partial parsing.
The tagged corpus is then segmented into a fiatsequence of phrasal groupings, using closed class words such asprepositions and determiners, as well as certain part-of-speechtransitions, to indicate likely phrase boundaries.
No attempt is made toconstruct a full parse tree or resolve prepositional phrase attachment,conjunction scoping, etc.
A concordance is constructed, identifying, foreach word appearing in the corpus, the set of sentences, phrases, andphrase locations in which the word appears.12 Within the current framework, a distinction is made between logical metonymy, where the metonymicextension or relation is transparent from the lexical semantics of the coerced phrase, and conventionalmetonymy, where the relation may not be directly calculated from information provided grammatically.For example, in the sentence "The Boston office called today," it is not clear from logical metonymywhat relation Boston bears to office other than location; i.e., it is not obvious that it is a branch office.This is well beyond lexical semantics (cf.
Lakoff 1987 and Martin 1990).340James Pustejovsky et al Lexical Semantic Techniques for Corpus AnalysisThe database of partially parsed sentences provides the raw material for a numberof sublanguage analyses.
This begins the second phase of analysis:1.
Noun compound recognition and bracketing.
In technical sublanguages,noun compounds are often employed to expand the working vocabularywithout he invention of new word forms.
It is therefore useful inapplications such as lexicon-assisted full-text information retrieval (Anick1992) to include such noun compounds as lexical items for bothquerying and thesaurus browsing.
We construct bracketed nouncompounds from our database of partial parses in a two-step rocess.The first simply searches the corpus for (recurring) contiguous sequencesof nouns.
Then, to bracket each compound that includes more than twonouns, we test whether possible subcomponents of the phrase exist ontheir own (as complete noun compounds) elsewhere in the corpus.Sample bracketed compounds derived from the computertroubleshooting database include \[ \[system anagement\] u t i l i ty \ ] ,\[TK50 \[tape drive\]\], \[\[database management\] system\].2.
Generation of taxonomic relationships on the basis of collocationalinformation.
Technical sublanguages often express ubclass relationshipsin noun compounds of the form <instance-name> <class-name>, as in"Unix operating system" and "C language."
Unfortunately, nouncompounds are also employed to express numerous other relationships,as in "Unix kernel" and "C debugger."
We have found, however, thatcollocational evidence can be employed to suggest which nouncompounds reflect axonomic relationships, using a strategy similar tothat employed by Hindle (1990) for detecting synonyms.
Given a term T,we extract from the phrase database those nouns Ni that appear as thehead of any phrase in which T is the immediately preceding term.
Thesenouns represent candidate classes of which T may be a member.
We thengenerate the set of verbs that take T as direct object and calculate themutual information value for each verb/T collocation (cf.
Hindle 1990).We do the same for each noun Ni.
Under the assumption that instanceand class nouns are likely to co-occur with the same verbs, we computea similarity score between T and each noun Ni, by summing the productof the mutual information values for those verbs occurring with bothnouns.
(Verbs with negative mutual information values are left out of thecalculation.)
The noun with the highest similarity score is often the classof which T is an instance, as illustrated by the sample results in Figure 1.For each word displayed in Figure 1, its "class" is the head noun withthe highest similarity score.
Other head nouns occurring with the wordas modifier are listed as well.As with all the automated procedures described here, this algorithmyields useful, but imperfect results.
The class chosen for "VMS," forexample, is incorrect, and may reflect he fact that in a DECtroubleshooting database, authors ee no need to further specify VMS as"VMS operating system."
A more interesting observation is that, amongthe collocations associated with the terms, there are often several thatmight qualify as classes of which the term is an instance, e.g.,DECWindows could also be classified as "software"; TK50 might alsoqualify as "tape."
From a generative l xicon perspective, thesealternative classifications reflect multiple inheritance through the noun's341Computational Linguistics Volume 19, Number 2word class score other collocationsHSC controller 27.69BACKUP operation 34.18RL02 disk 15.93TK50 cartridge 39.17ACCVIO error 14.35VAX product 23.28VMS support 7.98upgrade procedure 12.27DCL level 9.14CHECKSUM value 4.45EDT editor 11.58TPU command 3.62RTL error 1.58DECWindows environment 75.46device, disk, path, messagedisk, tape, process, savesetmedia, kit, packtape, kit, density, formatproblemconfiguration, ode, editor, hardwareproduct, upgrade, installationphase, option, support, prerequisitecommand, procedure, access, errorcharacter, operation, errorsession, conversion, search, problemeditor, session, function, debuggerroutine, libraryimage, application, intrinsics, softwareFigure 1Classification of nouns from a computer troubleshooting corpus.qualia.
That is, "cartridge" is further specifying the formal role of tapefor TK50.
DECWindows is functionally an "environment," its telic role,while "software" characterizes its formal quale.3.
Extraction of information relating to noun's qualia.
Under certaincircumstances, it may be possible to elicit information about a noun'squalia from automated procedures on a corpus.
In this line of research,we haved employed the notion of "lexical conceptual paradigm"described above.
An LCP relates a set of syntactic behaviors to thelexical semantic structures of the participating lexical items.For example, the set of expressions involving the word "tape" in thecontext of its use as a secondary storage device suggests that it fits thecontainer artifact schema of the qualia structure, with "information" and"file" as its containees:(a) read information from tape(b) write file to tape(c) read information on tape(d) read tape(e) write tapeAs mentioned in Section 1, containers tend to appear as objects of theprepositions to, from, in, and on as well as in direct object position, inwhich case they are typically serving metonymically for the containee.Thus, the container LCP relates the set of generalized syntactic patternsV i Nj {to ,  froin~ on}  X kvi NjriNkto the underlying lexical semantic structure given below.container(x,y) \]CONST = P(y)FORMAL ~- Q(x)TELIC = hold(S,x,y)342James Pustejovsky et al Lexical Semantic Techniques for Corpus Analysisverb MI countunload 5.43 5position 3.92 5mount 3.77 29initialize 3.18 10dismount 2.88 5read 1.40 7load 1.18 4restore 0.80 3write -0.24 2copy -2.55 1Figure 2Verbs associated with direct object ape as direct object.This LCP includes a nominal alternation between the container andcontainee in the object position of verbs.
For tape, this alternation ismanifested for verbs that predicate the telic role of data storage but notthe formal role of physical object, which refers to the object as a wholeregardless of its contents:TEL IC  : data-storage(a) read (tape/data from tape)(b) write (tape/data on tape)(c) copy (tape/data from tape)FORMAL= physical object(a) mount (tape)(b) dismount (tape)We have explored the use of heuristics to distinguish those predicatesthat relate to the Telic quale of the noun.
Consider the word tape, whichoccurs as the direct object in 107 sentences in our corpus.
It appears witha total of 34 different verbs.
By applying the mutual information metric(MI) to the verb-object pairs, we can sort the verbs accordingly, giving usthe table of verbs most highly associated with tape, shown in Figure 2.While the mutual information statistic does a good job of identifyingverbs that semantically relate to the word tape, it provides no informationabout how the verbs relate to the noun's qualia structure.
That is, verbssuch as unload, position, and mount are selecting for the formal quale oftape, a physical object hat can be physically manipulated with respect oa tape drive.
Read, write, and copy, on the other hand, relate to the telicrole, the function of a tape as a medium for storing information.Our hypothesis was that the nominal alternation can help todistinguish the two sets of verbs.
We reasoned that, if the alternation isbased on the container/containee metonymy, then it will be those verbsthat apply to the telic role of the direct object hat participate in thealternation.
We tested this hypothesis as follows.We generated a candidate set of containees for tape by identifying allthe nouns that appeared in the corpus to the left of the adjunct on tape.343Computational Linguistics Volume 19, Number 2$1&81 NS2S l -&Verbs with tape as objectVerbs with a containee of tape as object{restore, create, write, read, copy, replace}{mount, initialize, unload, position, dismount, load, allocate }818281Sln&-&Verbs with disk as objectVerbs with a containee of disk as object{compress, restore, disable, rebuild, modify, recover, search, copy}{mount, initialize, boot, dismount, serve, }Sl82S in&$1 -- ~2Verbs with directory as objectVerbs with containee of directory as object{create, recreate, delete, store, rename, check}{own, miss, search, review}Figure 3Intersection and set difference for three container nouns.Then we took the set of verbs that had one of these containee nouns as adirect object and compared this set to the set of verbs that had thecontainer noun tape as a direct object in the corpus.
According to ourhypothesis, verbs applying to the telic role should appear in theintersection of these two sets (as a result of the alternation), while thoseapplying to the formal role will appear in the set difference {verbs withcontainers as direct object}--{verbs with containees as direct object}.
Thedifference operation should serve to remove any verbs that co-occur withcontainee objects.
Figure 3 shows the results of intersection and setdifference for three container nouns tape, disk, and directory.The results indicate that the container LCP is able to differentiatenouns with respect o their telic and formal qualia, for the nouns tapeand disk but not for directory.
The poor discrimination in the latter casecan be attributed to the fact that a directory is a recursive container.
Adirectory contains files, and a directory is itself a file.
Therefore, verbsthat apply to the formal role of directory are likely to apply to the formalrole of objects contained in directories (such as other directories).
Thiscan be seen as a shortcoming of the container LCP for the task at hand,but may be a useful way of diagnosing when containers contain objectsfunctionally similar to themselves.The result of this corpus acquisition procedure is a kind of minimal faceted analysisfor the noun tape, as illustrated below, showing only the qualia that are relevant o thediscussion} 3tape(x,y) 1 CONST = information(y);file(y)FORMAL = mount(z,x);dismount(z,x)TELIC = read(w,y);write(w,y);copy(w,y);contain(w,y)13 Because the technique was sensitive to grammatical position of the object NP, the argument can bebound to the appropriate variable in the relation expressed in the qualia.
It should be pointed out thatthese qualia values do not carry event place variables, since such discrimination was beyond the scopeof this experiment.344James Pustejovsky et al Lexical Semantic Techniques for Corpus AnalysisWhat is interesting about the qualia values is how close they are to the concepts inthe projective conclusion space of tape, as mentioned in Section 1.To illustrate this procedure on another semantic ategory, consider the term mousein its computer artifact sense.
In our corpus, it appears in the object position of theverb use in a "use NP to" construction, as well as the object of the preposition withfollowing a transitive verb and its object:1. use the mouse to set breakpoints2.
use the mouse anywhere3.
move a window with the mouse4.
click on it with the mouse .
.
.These constructions are symptomatic of its role as an instrument; and the VPcomplement of to as well as the VP dominating the with-PP identify the telic predicatesfor the noun.
Other verbs, for which mouse appears as a direct object are currentlydefaulted into the formal role, resulting in an entry for mouse as follows:mouse(x) \]CONST = button(y)FORMAL = physobj(x)TEL IG  = set(x,breakpoint);move(x,window);click-on(x,z)The above experiments have met with limited success, enough to warrant continu-ing our application of lexical semantic theory to knowledge acquisition from corpora,but not enough to remove the human from the loop.
As they currently exist, thealgorithms described here can be used as tools to help the knowledge ngineer ex-tract useful information from on-line textual sources, and in some applications (e.g.,a "related terms" thesaurus for full text information retrieval) may provide a usefulway to heuristically organize sublanguage t rminology when human resources areunavailable.4.
Semantic Type Induction from Syntactic FormsThe purpose of the research described in this section is to experiment with the au-tomatic acquisition of semantic tags for words in a sublanguage, tags well beyondthat available from the seeding of MRDs.
The identification of semantic tags is theresult of type coercion on known syntactic forms, to induce a semantic feature, suchas \[+event\] or \[+object\].4.1 Coercive Environments in CorporaA pervasive xample of type coercion is seen in the complements of aspectual verbssuch as begin and finish, and verbs such as enjoy.
That is, in sentences such as "Johnbegan the book," the normal complement expected is an action or event of some sort,most often expressed by a gerundive or infinitival phrase: "John began reading thebook," "John began to read the book."
In Pustejovsky (1991) it was argued that in suchcases, the verb need not have multiple subcategorizations, butonly one deep semantictype, in this case, an event.
Thus, the verb coerces its complement (e.g.
"the book")into an event related to that object.
Such information can be represented by means ofa representational schema called qualia structure, which, among other things, specifiesthe relations associated with objects.345Computational Linguistics Volume 19, Number 2Figure 4Counts for objects of begin/V.count verb object205 begin career176 begin day159 begin work140 begin talk120 begin campaign113 begin investigation106 begin process92 begin program85 begin operation85 begin negotiation66 begin strike64 begin production59 begin meeting59 begin term50 begin visit45 begin test39 begin construction31 begin debate29 begin trialIn related work being carried out with Mats Rooth of the University of Stuttgart,we are exploring what the range of coercion types is, and what environments heymay appear in, as discovered in corpora.
Some of our initial data suggest hat thehypothesis of deep semantic selection may in fact be correct, as well as indicatingwhat the nature of the coercion rules may be.
Using techniques described in Churchand Hindle (1990), Church and Hanks (1990), and Hindle and Rooth (1991), Figure 4shows some examples of the most frequent V-O pairs from the AP corpus.Corpus studies confirm similar results for "weakly intensional contexts" such asthe complement of coercive verbs such as veto.
These are interesting because regard-less of the noun type appearing as complement, it is embedded within a semanticinterpretation f "the proposal to," thereby clothing the complement within an inten-sional context.
The examples in Figure 5 with the verb veto indicate two things: first,that such coercions are regular and pervasive in corpora; second, that almost anythingcan be vetoed, but that the most frequently occurring objects are closest o the typeselected by the verb.What these data show is that the highest count complement types match the typerequired by the verb; namely, that one vetoes a bill or proposal to do something, notthe thing itself.
These nouns can therefore be used with some predictive certainty forinducing the semantic type in coercive nvironments such as "veto the expedition.
"This work is still preliminary, however, and requires further examination (Pustejovskyand Rooth \[unpublished\]).4.2 Induction of Semantic Relations from Syntactic FormsIn this section, we present another experiment indicating the feasibility of inducingsemantic tags for lexical items from Corpora.
14 Imagine being able to take the V-O pairs14 This section presents an abridged version of material reported on in Pustejovsky (1992).346James Pustejovsky et al Lexical Semantic Techniques for Corpus AnalysisFigure 5Counts for objects of veto/V.count verb object303 veto bill84 veto legislation58 veto measure35 veto resolution21 veto law14 veto item12 veto decision9 veto proposal9 veto plan7 veto package6 veto increase5 veto sanction5 veto penalty4 veto notice4 veto idea4 veto appropriation4 veto mission4 veto attempt3 veto search3 veto cut3 veto deal1 veto expeditionsuch as those given in Section 4.1, and then applying semantic tags to the verbs thatare appropriate to the role they play for that object (i.e., induction of the qualia rolesfor that noun).
This is similar to the experiment reported on in Section 3.
Here weapply a similar technique to a much larger corpus, in order to induce the agentive rolefor nouns; that is, the semantic predicate associated with bringing about he object.In this example we look at the behavior of noun phrases and the prepositionalphrases that follow them.
In particular, we look at the co-occurrence ofnominals withbetween, with, and to.
Table 1 shows results of the conflating noun plus prepositionpatterns.
The percentage shown indicates the ratio of the particular collocation to thekey word.
Mutual information (MI) statistics for the two words in collocation are alsoshown.
What these results indicate is that induction of semantic type from conflatingsyntactic patterns is possible.
Based on the semantic types for these prepositions, thesyntactic evidence suggests that there is an equivalence class where each prepositionmakes reference to a symmetric relation between the arguments in the following twopatterns:?
Z with y = ARzAx3y\ [Rz(x ,y)  A Rz(y, x)\]?
Z between x and y=,XRz3x, y\[Rz(x, y)/x Rz(y, x)\]We then take these results and, for those nouns where the association ratios for Nwith and N between are similar, we pair them with the set of verbs governing these"NP PP" combinations in corpus, effectively partitioning the original V-O set into\[+agentive\] predicates and \[-agentive\] predicates.These are semantic n-grams rather than direct interpretations of the prepositions.347Computational Linguistics Volume 19, Number 2Table 1Mutual information for noun + preposition patterns.Word Word Word Word Word WordWord + to + with + between Word + to + with + between(%)/MI (%)/MI (%)/MI (%)/MI (%)/MI (%)/MIagreement .117 .159 .028 expansion .013 .007 01.512 3.423 3.954 -.666 .381 n/aannouncement .010 .003 0 impasse 0 .064 .096-.918 -.409 n/a n/a 2.520 5.192barrier .215 0 .030 interactions 0 0 .2502.117 n/a 4.046 n/a n/a 6.141competition .019 .028 .021 market .013 .006 .000-.269 1.701 3.666 -.637 .240 -.500confrontation .029 .283 .074 range .005 .002 .020.141 4.000 4.932 -1.533 -.618 3.663contest .052 .052 .039 relations .009 .217 .103.715 2.323 4.301 -1.017 3.736 5.254contract .066 .060 .002 settlement .013 .091 .012.947 2.463 1.701 -.626 2.868 3.142deal .028 .193 .004 talks .029 .218 .030.086 3.616 2.015 .138 3.740 4.043dialogue 0 .326 .152 venture .032 .105 .035n/a 4.140 5.644 .226 3.008 4.185difference .017 .009 .348 war .010 .041 .015-.410 .638 6.474 -.937 2.079 3.372What these expressions in effect indicate is the range of semantic environments theywill appear in.
That is, in sentences like those in Example 16, the force of the relationalnouns agreement and talks is that they are unsaturated for the predicate bringing aboutthis relation.
In 17, on the other hand, the NPs headed by agreement and talks aresaturated in this respect.Example 16a.
John made an agreement with Mary.b.
Apple opened talks with IBM.Example 17a.
This is an agreement between John and Mary.b.
Those were the first talks between Apple and IBM.If our hypothesis is correct, we expect that verbs governing nominals collocated witha with-phrase will be mostly those predicates referring to the agentive quale of thenominal.
This is because the with-phrase is unsaturated as a predicate, and acts to348James Pustejovsky et al Lexical Semantic Techniques for Corpus Analysiscount verb objectFigure 6Verb-object pairs with prep = with.19 form venture3 announce venture3 enter venture2 discuss venture1 be venture1 abandon venture1 begin venture1 complete venture1 negotiate venture1 start venture1 expect ventureidentify the agent of the verb as its argument (cf.
Nilsen (1973)).
This is confirmed byour data, shown in Figure 6.Conversely, verbs governing nominals collocating with a between-phrase will notrefer to the agentive since the phrase is saturated already.
Indeed, the only verb occur-ring in this position with any frequency is the copula be, namely with the followingcounts: 12 be/V venture/0.
Thus, weak semantic types can be induced on the basisof syntactic behavior.There is a growing literature on corpus-based acquisition and tuning (Smadja1991a; Zernik and Jacobs 1991; Brent 1991; as well as Grishman and Sterling 1992).We share with these researchers a general dependence on well-behaved collocationalpatterns and distributional structures.
Probably the main distinguishing feature of ourapproach is its reliance on a fairly well studied semantic framework to aid and guidethe semantic induction process itself, whether it involves selectional restrictions orsemantic types.5.
Lexical Presuppositions and PreferencesIn the previous ection we presented algorithms for extracting collocational informa-tion from corpora, in order to supplement and fine-tune the lexical structures eededby a machine-readable dictionary.
In this section we demonstrate hat, in addition toconventional lexical semantic relations, it is also possible to acquire information con-cerning lexical presuppositions and preferences from corpora, when analyzed withthe appropriate semantic tools.
In particular, we will discuss a phenomenon we calldiscourse polarity, and how corpus-based experiments provide clues toward the repre-sentation of this phenomenon, as well as information on preference relations.As we have seen, providing a representational system for lexical semantic relationsis a nontrivial task.
Representing presuppositional information, however, is even moredaunting.
Nevertheless, there are some systematic semantic generalizations a sociatedwith such subtle lexical inferences.
To illustrate this, consider the following examplestaken from the Wall Street Journal Corpus, involving the verb insist.Example 18But Mr. Fourtou insisted that the restructuring plans hadn't played a role in his decision.349Computational Linguistics Volume 19, Number 2Example 19But so far, the majority is insisting that a daily paper in the home is an essentialeducational resource that Mr. Oshry must have, like it or not.Example 20But Mr. Nishi insists there is a common theme to his scattered projects: to improveand spread personal computers.Example 21"Mister, Djemaa is a crazy place for you," insists the first of many young men, clutchinga visitor's sleeve.Example 22But the BNL sources yesterday insisted that the head office was aware of only a smallportion of the credits to Iraq made by Atlanta.Example 23Mr.
Smale, who ordinarily insists on a test market before a national roll-out, told theteam to go ahead--a l though he said he was skeptical that Pringle's could survive, Mr.Tucker says.Example 24The Cantonese insist that their fish be "fresh," though one whiff of Hong Kong harborand the visitor may yearn for something shipped from distant seas.Example 25Money isn't the issue, Mr. Bush insists.From analyzing these and similar data, a pattern emerges concerning the use of verbslike insist in discourse; namely, the co-occurrence with discourse markers denotingnegative affect, such as although and but, as well as literal negatives, e.g., no and not.This is reminiscent of the behavior of negative polarity items such as any more and at all.Such lexical items occur only in the context of negatives within a certain structuralconfiguration.
15 In a similar way, verbs such as insist seem to require an overt orimplicit negation within the immediate discourse context, rather than within the clause.For this reason, we will call such verbs discourse polarity items.For our purposes, the significance of such data is twofold: first, experiments oncorpora can test and confirm linguistic intuitions concerning a subtle semantic judg-ment; second, if such knowledge is in fact so systematic, then it must be at leastpartially represented in the lexical semantics of the verb.To test whether the intuitions supported by the above data could be confirmedin corpora, Bergler (1991) derived the statistical co-occurrence of insist with discoursepolarity markers in the 7 mill ion-word corpus of Wall Street Journal articles.
She derivedthe statistics reported in Figure 7.Let us assume, on the basis of this preliminary data 16 presented in Bergler (1992)that these verbs in fact do behave as discourse polarity items.
The question then15 There is a rich literature on this topic.
For discussion see Ladusaw (1980) and Linebarger (1980).16 Overlap between the categories occurs in less than 35 cases.350James Pustejovsky et al Lexical Semantic Techniques for Corpus AnalysisKeywordsinsistinsist oninsist & butinsist & negationCount586109insist & subjunctiveFigure 7117186159Commentsoccurrences throughout the corpusthese have been cleaned by hand and are actually oc-currences of the idiom insist on rather than accidentalco-occurrences.occurrences ofboth insist and but in the same sentenceincludes not and n'tincludes would, could, should, and beNegative markers with insist in WSJC.immediately arises as to how we represent this type of knowledge.
Using the languageof the qualia structure discussed above, we can make explicit reference to the polaritybehavior, in the following informal but intuitive representation for the verb insist.
17insist(x:ind,y:prop) \]FORMAL = REPORTING-VERB--LCPTELIC = say(x,true(y)) & presupposed(~b) & y = ~bThis entry states that in the REPORTING-VERB sense of the word, insist is a relationbetween an individual and a statement that is the negation of a proposition, ~b, pre-supposed in the context of the utterance.
As argued in Pustejovsky (1991) and Millerand Fellbaum (1991), such simple oppositional predicates form a central part of ourlexicalization of concepts.
Semantically motivated collocations uch as these extractedfrom large corpora can provide presupposit ional information for words that wouldotherwise be missing from the lexical semantics of an entry.
While full automatic ex-traction of semantic collocations is not yet feasible, some recent research in relatedareas is promising.Hindle (1990) reports interesting results of this kind based on literal collocations,where he parses the corpus (Hindle 1983) into predicate-argument structures andapplies a mutual information measure (Fano 1961; Magerman and Marcus 1990) to weighthe association between the predicate and each of its arguments.
For example, as alist of the most frequent objects for the verb drink in his corpus, Hindle found beer,tea, Pepsi, and champagne.
Based on the distributional hypothesis that the degree ofshared contexts is a similarity measure for words, he develops a similarity metricfor nouns based on their substitutability in certain verb contexts.
Hindle thus findssets of semantically similar nouns based on syntactic co-occurrence data.
The setshe extracts are promising; for example, the ten most similar nouns to treaty in hiscorpus are agreement, plan, constitution, contract, proposal, accord, amendment, rule, law,and legislation.This work is very close in spirit to our own investigation here; the emphasis onsyntactic o-occurrence enables Hindle to extract his similarity lists automatically; they17 For illustration, we use an abbreviated version of the lexical entries under discussion, highlighting onlycertain qualia for the verbs.
For the most recent representation f verbal semantics in this framework,see Pustejovsky (1993).351Computational Linguistics Volume 19, Number 2are therefore asy to compile for different corpora, different sublanguages, etc.
Herewe are attempting to use these techniques together with a model of lexical meaning,to capture deeper lexical semantic ollocations; e.g., the generalization that the list ofobjects occurring for the word drink contains only liquids.In the final part of this section, we turn to how the analysis of corpora can providelexical semantic preferences for verb selection.
As discussed above, there is a growingbody of research on deriving collocations from corpora (cf.
Church and Hanks 1990;Klavans, Chodorow, and Wacholder 1990; Wilks et al 1993; Smadja 1991a, 1991b; Cal-zolari and Bindi 1990).
Here we employ the tools of semantic analysis from Section 1to examine the behavior of metonymy with reporting verbs.
We will show, on the ba-sis of corpus analysis, how verbs display marked differences in the ability to licensemetonymic operations over their arguments.
Such information, we argue, is part ofthe preference semantics for a sublanguage, asautomatically derived from corpus.Metonymy can be seen as a case of "licensed violation" of selectional restric-tions.
For example, while the verb announce selects for a human subject, sentences likeThe Phantasie Corporation announced third quarter losses are not only an acceptable para-phrase of the selectionally correct form Mr. Phantasie Jr. announced third quarter lossesfor Phantasie Corp, but they are the preferred form in the Wall Street Journal).
This is anexample of subject ype coercion, as discussed in Section 1.
For example, the qualiastructure for a noun such as corporation might be represented asbelow:corporation(x)CONST = group(y),spokesperson(w),executive(z)FORMAL = organization(x)TELIC = execute(z, decisions)AGENTIVE = incorporate(y,x)The metonymic extension i  this example is straightforward: a spokesman, executive,or otherwise legitimate representative "speaking for" a company or institution can bemetonymically replaced by that company or institution.
TMWe find that this type of metonymic extension for the subject is natural and indeedvery frequent with reporting verbs Bergler (1991), such as announce, report, release, andclaim, while it is in general not possible with other verbs selecting human subjects, e.g.,the verbs of contemplation (such as contemplate, consider, and think).
However, thereare subtle differences in the occurrence of such metonymies for the different membersof the same semantic verb class that arise from corpus analysis.A reporting verb is an utterance verb that is used to relate the words of a source.
Ina careful study of seven reporting verbs on a 250,000-word corpus of Time magazinearticles from 1963, we found that the preference for different metonymic extensionsvaries considerably within this field (Bergler 1991).
Figure 8 shows the findings for thewords insist, deny, admit, claim, announce, said, and told for two metonymic extensions,namely where a group stands for an individual (Analysts aid... ) and where a companyor other institution stands for the individual (IBM announced ... ).19The difference in patterns of metonymic behavior is quite striking: semanticallysimilar verbs seem to pattern similarly over all three categories; admit, insist, and denyshow a closer resemblance to each other than to any of the others, while said and18 Note, however, that the metonymic extension is not quite as simple as extending from any employee tothe whole company or institution, but that a form of legitimation has to be involved.)
For more detailsee Bergler (1992).19 The data for Figure 8 have been screened to ensure that only occurrences that constitute reportingcontexts were used.352James Pustejovsky et al Lexical Semantic Techniques for Corpus Analysisadmitdenyinsistannounceclaimsaidtoldperson64%59%57%51%35%69%19%11%24%10%21%14%19%16%31%38%other2%11%3%8%6%8%16%Figure 8Preference for different metonymies in subject position.- - t  personWSJ 49%TIME 83%group15%6%institution other34% 2%4% 8%Figure 9Preference for metonymies for said in a 160,000-word fragment of the Wall Street Journalcorpus.told form a category by themselves.
There may be a purely semantic explanation whysaid and told seem not to prefer the metonymic use in subject position; e.g., perhapsthese verbs relate more closely to the act of uttering, or perhaps they are too informal,stylistically.
Evidence from other corpora, however, suggests that such information isaccurately characterized as lexical preference.
An initial experiment on a subset of theWall Street Journal Corpus, for example, shows that said has a quite different metonymicdistribution there, reported in Figure 9.In this corpus we discovered that subject selection for an individual person ap-peared in only 50% of the sentences, while a company/institution appeared in 34%of the cases.
This difference could either be attributed to a difference in style betweenTime magazine and the Wall Street Journal or perhaps to a difference in general usagebetween 1963 and 1989.
The statistics presented here can of course not determine thereason for the difference, but rather help establish the lexical semantic preferences thatexist in a certain corpus and sublanguage.An important question related to the extraction of preference information is whatthe corpus should be.
Recent effort has been spent constructing balanced corpora, con-taining text from different styles and sources, such as novels, newspaper texts, scien-tific journal articles, etc.
The assumption is of course that given a representative mix ofsamples of language use, we can extract he general properties and usage of words.But if we gain access to sophisticated automatic orpus analysis tools such as those353Computational Linguistics Volume 19, Number 2discussed above, and indeed if we have specialized algorithms for sublanguage ex-traction, then homogeneous corpora might provide better data.
The few examples oflexical preference mentioned in this section might not tell us anything conclusive forthe definitive usage of a word such as said, if there even exists such a notion.
Nev-ertheless the statistics provide an important tool for text analysis within the corpusfrom which they are derived.
Because we can systematically capture the violation ofselectional restrictions (as semantically predicted), there is no need for a text analy-sis system to perform extensive commonsense inferencing.
Thus, such presuppositionand preference statistics are vital to efficient processing of real text.6.
Summary and DiscussionIn this paper we have presented a particularly directed program of research for howtext corpora can contribute to linguistics and computational linguistics.
We first pre-sented a representation language for lexical knowledge, the generative l xicon, anddemonstrated how it facilitates the structuring of lexical relations among words, look-ing in particular at the problems of metonymy and polysemy.Such a framework for lexical knowledge suggests that there are richer relation-ships among words in text beyond that of simple co-occurrence that can be extractedautomatically.
The work suggests how linguistic phenomena such as metonymy andpolysemy might be exploited for knowledge acquisition for lexical items.
Unlike purelystatistical collocational nalyses, the framework of a semantic theory allows the auto-matic construction of predictions about deeper semantic relationships among wordsappearing in collocational systems.We illustrated the approach for the acquisition of lexical information for severalclasses of nominals, and how such techniques can fine-tune the lexical structures ac-quired from an initial seeding of a machine-readable dictionary.
In addition to conven-tional lexical semantic relations, we then showed how information concerning lexicalpresuppositions and preference relations can also be acquired from corpora, whenanalyzed with the appropriate semantic tools.In conclusion, we feel that the application of computational resources to the anal-ysis of text corpora has and will continue to have a profound effect on the directionof linguistic and computational linguistic research.
Unlike previous attempts at cor-pus research, the current focus is supported and guided by theoretical tools, and notmerely statistical techniques.
We should furthermore welcome the ability to expandthe data set used for the confirmation of linguistic hypotheses.
At the same time, wemust remember that statistical results themselves reveal nothing, and require carefuland systematic nterpretation by the investigator to become linguistic data.AcknowledgmentsThis research was supported by DARPAcontract MDA904-91-C-9328.
We would liketo thank Scott Waterman for his assistancein preparing the statistics.
We would alsolike to thank Mats Rooth, Scott Waterman,and four anonymous reviewers for usefulcomments and discussion.ReferencesAhlswede, T., and Evens, M.
(1988).
"Generating a relational lexicon from amachine-readable dictionary."
InternationalJournal of Lexicography, 1(3), 214-237.Alshawi, H. (1987).
"Processing dictionarydefinitions with phrasal patternhierarchies."
Computational Linguistics, 13,3-4.Alshawi, H.; Boguraev, B.; and Briscoe, T.(1985).
"Towards a dictionary supportenvironment for real time parsing."
InProceedings, European Conference onComputational Linguistics.
Geneva,Switzerland.Amsler, R. A.
(1980).
The structure of theMerriam-Webster Pocket Dictionary.Doctoral dissertation, University of Texas.354James Pustejovsky et al Lexical Semantic Techniques for Corpus AnalysisAmsler, R. A.
(1989).
"Third generationcomputational lexicology."
In Proceedings,First International Lexical AcquisitionWorkshop.
Detroit, Michigan, August 1989.Amsler, R. A., and White, J. S.
(1979).
"Development of a computationalmethodology for deriving naturallanguage semantic structures via analysisof machine-readable dictionaries."
NSFTechnical Report MCS77-01315.Anick, P. (1992).
"Lexicon assistedinformation retrieval for the help-desk.
"In Proceedings, IEEE CAIA-92 Workshop onAI and Help-Desks.
Monterey, California.Anick, P.; Brennan, J.; Flynn, R.; Hanssen,D.
; Alvey, B.; and Robbins, J.
(1989).
"Adirect manipulation i terface for Booleaninformation retrieval via natural languagequery."
In Proceedings, SIGIR '89.Anick, P., and Pustejovsky, J.
(1990).
"Anapplication of lexical semantics toknowledge acquisition from corpora."
InProceedings, 13th International Conference ofComputational Linguistics.
Helsinki,Finland.Atkins, B. T. (1991).
"Building a lexicon:Reconciling anisomorphic sensedifferentiations in machine-readabledictionaries."
International Journal ofLexicography.Atkins, B. T., and Levin, B.
(1991).
"Admitting impediments."
In LexicalAcquisition: Using On-Line Resources toBuild a Lexicon, edited by U. Zernik.
LEA.Bergler, S. (1991).
"The semantics ofcollocational patterns for reportingverbs."
In Proceedings, Fifth Conference ofthe European Chapter of the Association forComputational Linguistics.
Berlin, Germany,April 1991.Bergler, S. (1992).
"Evidential analysis ofreported speech."
Doctoral dissertation,Brandeis University.Bierwisch, M. (1983).
"Semantische undkonzeptuelle Repr~isentationenlexikalischer Einheiten."
InUntersuchungen zur Semantik, edited byR.
Ruzicka and W. Motsch.Akademische-Verlag.Binot, J.-L., and Jensen, K. (1987).
"Asemantic expert using an online standarddictionary."
In Proceedings, lOthInternational Joint Conference on Artifi'cialIntelligence (IJCAI-87).
Milan, Italy,709-714.Boguraev, B.
(1979).
"Automatic resolutionof linguistic ambiguities."
TechnicalReport No.
11, University of CambridgeComputer Laboratory, Cambridge, U.K.Boguraev, B.
(1991).
"Building a lexicon: Thecontribution of computers."
InternationalJournal of Lexicography, 4(3).Boguraev, B., and Briscoe, T.
(1989)."Introduction."
In ComputationalLexicography for Natural LanguageProcessing, edited by B. Boguraev andT.
Briscoe.
Longman Group UK.Boguraev, B., and Briscoe, T. (1987).
"Largelexicons for natural anguage processing:Exploring the grammar coding system ofLDOCE."
Computational Linguistics, 13.Boguraev, B.; Byrd, R.; Klavans, J.; and Neff,M.
(1989).
"From machine readabledictionaries to a lexical knowledge base.
"In Proceedings, First International LexicalAcquisition Workshop.
Detroit, Michigan,August 1989.Boguraev, B., and Pustejovsky, J.
(1990).
"Lexical ambiguity and the role ofknowledge representation in lexicondesign."
In Proceedings, 13th InternationalConference ofComputational Linguistics.Helsinki, Finland, August 1990.Brent, M. (1991).
"Automatics semanticclassification of verbs from their syntacticcontexts: An implemented classifier forstativity."
In Proceedings, Fifth Conference ofthe European Chapter of the Association forComputational Linguistics.
Berlin, Germany,April 1991.Briscoe, E.; Copestake, A.; and Boguraev, B.(1990).
"Enjoy the paper: Lexicalsemantics via lexicology."
In Proceedings,13th International Conference onComputational Linguistics.
Helsinki,Finland, 42-47.Byrd, R.; Calzolari, N.; Chodorow, M.;Klavans, J.; Neff, M.; and Rizk, O.
(1987).
"Tools and methods for computationallexicology."
Computational Linguistics,13(3-4), 219-240.Byrd, R. (1989).
"Discovering relationshipsamong word senses."
In Proceedings, FifthAnnual Conference ofthe UW Centre for theNew Oxford English Dictionary.
Oxford,U.K., 67-80.Calzolari, N. (1984).
"Detecting patterns in alexical database."
In Proceedings, SeventhInternational Conference on ComputationalLinguistics (COLING-84).
Stanford,California.Calzolari, N., and Bindi, R.
(1990).
"Acquisition of lexical information from alarge textual Italian corpus."
InProceedings, 13th International Conference onComputational Linguistics (COLING-90).Helsinki, Finland.Cardelli, L., and Wegner, P. (1985).
"Onunderstanding types, data abstraction,and polymorphism."
ACM ComputingSurveys, 17(4), 471-522.Chodorow, M.; Byrd, R.; and Heidorn, G.355Computational Linguistics Volume 19, Number 2(1985).
"Extracting semantic hierarchiesfrom a large on-line dictionary."
InProceedings, 23rd Annual Meeting of the,ACL.
Chicago, Illinois, 299-304.Church, K. (1988).
"A stochastic partsprogram and noun phrase parser forunrestricted text."
In Proceedings, SecondConference on Applied Natural LanguageProcessing.
Austin, Texas.Church, K., and Hanks, P. (1990).
"Wordassociation orms, mutual information,and lexicography."
ComputationalLinguistics, 16(1), 22-29.Church, K., and Hindle, D.
(1990).
"Collocational constraints andcorpus-based linguistics."
In WorkingNotes of the AAAI Symposium: Text-BasedIntelligent Systems.
Stanford, California.Copestake, Ann (in press).
"Defaults in theLKB."
In Default Inheritance in the Lexicon,edited by T. Briscoe and A. Copestake.Cambridge University Press.Copestake, A., and Briscoe, E.
(1992).
"Lexical operations in a unification-basedframework."
In Lexical Semantics andKnowledge Representation, edited byJ.
Pustejovsky and S. Bergler.
SpringerVerlag.Cowie, J.; Guthrie, L.; and Pustejovsky, J.(1992).
"Description of the MUCBRUCEsystem as used for MUC-4."
InProceedings, Fourth Message UnderstandingConference (MUC-4).
Morgan Kaufmann.Croft, W. B.
(1989).
"Automatic indexing.
"In Indexing: The State of Our Knowledge andthe State of Our Ignorance, edited byB.
H. Weinberg, 87-100.
LearnedInformation, Inc.Cruse, D. A.
(1986).
Lexical Semantics.Cambridge University Press.Cruse, D. A.
(1992).
"Polysemy and relatedphenomena from a cognitive linguisticviewpoint."
In Proceedings, Second ToulouseWorkshop on Lexical Semantics, edited byP.
Saint-Dizier and E. Viegas.
Toulouse,France.Debili, F.; Fluhr, C.; and Radasoa, P.
(1988).
"About reformulation i full-text IRS."
InProceedings, RIAO-88, 343-357.Evans, R., and Gazdar, G., editors.
(1990).
"The DATR papers: February 1990.
"Cognitive Science Research Paper CSRP139, School of Cognitive and ComputingSciences, University of Sussex.Evens, M. (1987).
Relational Models of theLexicon.
Cambridge University Press.Fano, R. (1961).
Transmission ofInformation: AStatistical Theory of Communications.
MITPress.Fillmore, C. (1968).
"The case for case."
InUniversals in Linguistics Theory, edited byE.
Bach and R. Harms.
Holt, Rinehart andWinston.Grishman, R.; Hirschman, L.; and Nhan,N.
T. (1986).
"Discovery procedures forsublanguage s lectional patterns: Initialexperiments."
Computational Linguistics,12(3), 205-215.Grishman, R., and Sterling, J.
(1992).
"Acquisition of selectional patterns."
InProceedings, 14th International Conference onComputational Linguistics (COLING-92).Nantes, France, July 1992.Guthrie, L.; Slator, B.; Wilks, Y.; and Bruce,R.
(1990).
"Is there content in EmptyHeads?"
In Proceedings, 13th InternationalConference ofComputational Linguistics(COLING-90).
Helsinki, Finland.Hindle, D. 1983.
"Deterministic parsing ofsyntactic non-fluencies."
In Proceedings,21st Annual Meeting of the Association forComputational Linguistics, Cambridge,Massachusetts, June 1983, 123-128.Hindle, D. (1990).
"Noun classification frompredicate-argument structures."
InProceedings, 28th Annual Meeting of theAssociation for Computational Linguistics,Pittsburgh, Pennsylvania, June 1990,268-275.Hindle, D., and Rooth, M.
(1991).
"Structural ambiguity and lexicalrelations."
In Proceedings ofthe ACL.Jacobs, P. (1991).
"Making sense of lexicalacquisition."
In Lexical Acquisition: UsingOn-Line Resources to Build a Lexicon, editedby U. Zernik, LEA.Klavans, J.; Chodorow, M.; and Wacholder,N.
(1990).
"From dictionary to knowledgebase via taxonomy."
In Proceedings, SixthConference ofthe UW Centre for the NewOED.
Waterloo, 110-132.Krovetz, R., and Croft, W. B.
(1989).
"Wordsense disambiguation usingmachine-readable dictionaries."
InProceedings, SIGIR.
127-136.Ladusaw, W. (1980).
Polarity Sensitivity asInherent Scope Relations.
Indiana UniversityLinguistics Club.Lakoff, G. (1968).
"Instrumental dverbsand the concept of deep structure.
"Foundations of Language, 4, 4-29.Lakoff, G. (1970).
Irregularity in Syntax.
Holt,Rinehart, and Winston.Lakoff, G. (1987).
Women, Fire, and DangerousObjects.
University of Chicago Press.Linebarger, M. (1980).
"The grammar ofnegative polarity."
Doctoral dissertation,MIT, Cambridge MA.Maarek, Y. S., and Smadja, F. Z.
(1989).
"Fulltext indexing based on lexical relations,an application: Software libraries."
InProceedings, SIGIR.
127-136.356James Pustejovsky et al Lexical Semantic Techniques for Corpus AnalysisMarkowitz, J.; Ahlswede, T.; and Evens, M.(1986).
"Semantically significant patternsin dictionary definitions."
In Proceedings,24th Annual Meeting of the Association forComputational Linguistics.
New York, NewYork, 112-119.Magerman, D., and Marcus, M.
(1990).
"Parsing a natural anguage using mutualinformation statistics."
In Proceedings,Eighth National Conference on ArtificialIntelligence (AAAI-90).
Boston,Massachusetts.Martin, J.
(1990).
A Computational Model ofMetaphor Interpretation.
Academic Press.Mel'~uk, I.
(1988).
Dependency S ntax.
SUNYPress.Miller, G., and Fellbaum, C. (1991).
"Verbsin WordNet."
Cognition.Nakamura, J., and Nagao, M.
(1988).
"Extraction of semantic information froman ordinary English dictionary and itsevaluation."
In Proceedings, COLING-88.Budapest, Hungary, 459-464.Nilsen, D. L. F. (1973).
The Instrumental Casein English: Syntactic and SemanticConsiderations.
Mouton.Nirenburg, S., and Nirenburg, I.
(1988).
"Aframework for lexical selection in naturallanguage generation."
In Proceedings,COLING-88.
Budapest, Hungary.Nunberg, G. (1978).
The Pragmatics ofReference.
Indiana University LinguisticsClub.Procter, P.; Ilson, R. F.; and Ayto, J.
(1978).Longman Dictionary of ContemporaryEnglish.
Longman Group Limited.Pustejovsky, J.
(1989).
"Issues incomputational lexical semantics."
InProceedings, Fourth Conference oftheEuropean Chapter of the ACL.
Manchester,England, April 1989.Pustejovsky, J.
(1991).
"The generativelexicon."
Computational Linguistics, 17(4),409-441.Pustejovsky, J.
(1992).
"The acquisition oflexical semantic knowledge from largecorpora."
In Proceedings, DARPA Spokenand Written Language Workshop.
MorganKaufmann.Pustejovsky, J.
(1993).
"Linguistic onstraintson type coercion."
In Computational LexicalSemantics, edited by P. Saint-Dizier andE.
Viegas.
Cambridge University Press.Pustejovsky, J.
(in press).
The GenerativeLexicon: A Theory of Computational LexicalSemantics.
MIT Press.Pustejovsky, J. and Anick, P. (1988).
"Thesemantic interpretation f nominals."
InProceedings, 12th International Conference ofComputational Linguistics.
Budapest,Hungary, August 1988.Pustejovsky, J. and Bergler, S. (1987).
"Theacquisition of conceptual structure for thelexicon."
In Proceedings, Sixth NationalConference on Artificial Intelligence.
Seattle,Washington.Pustejovsky, J. and Boguraev, B.
(1993).
"Lexical knowledge representation a dnatural anguage processing."
ArtificialIntelligence.Pustejovsky, J. and Rooth, M.(unpublished).
"Type coerciveenvironments in corpora.
"Pustejovsky, J.; Waterman, S.; Cowie, J.; andStein, G. (1992).
"Overview of theDIDEROT system for the Tipster textextraction project."
In Proceedings, DARPATIPSTER 12-Month Evaluation.
San Diego,California, September 1992.Rau, L., and Jacobs, P. (1988).
"Integratingtop-down and bottom-up strategies in atext processing system."
In Proceedings,Second Conference on Applied NaturalLanguage Processing, Austin Texas,February 1988, 129-135.Russell, G.; Ballim, A.; Carroll, J.; andWarwick-Armstrong, S. (1992).
"Apractical approach to multiple defaultinheritance for unification-basedlexicons."
Computational Linguistics, 18(3),311-337.Slator, B. M. (1988).
"Constructingcontextually organized lexical semanticknowledge-bases."
In Proceedings, ThirdAnnual Rocky Mountain Conference onArtificial Intelligence.
Denver, Colorado,June 1988, 142-148.Slator, B. M., and Wilks, Y.
A.
(1987).
"Toward semantic structures fromdictionary entries."
In Proceedings, SecondAnnual Rocky Mountain Conference onArtificial Intelligence.
Boulder, Colorado,85-96.Smadja, F. (1991a).
"Macrocoding thelexicon with co-occurrence knowledge.
"In Lexical Acquisition: Using On-LineResources to Build a Lexicon, edited byU.
Zernik.
Lawrence Erlbaum Associates.Smadja, F. (1991b).
"From n-grams tocollocations: an evaluation of xtract."
InProceedings, 29th Annual Meeting of theAssociation for Computational Linguistics.Berkeley, California, June 1991, 279-284.Sparck Jones, K., editor.
(1981).
InformationRetrieval Experiments.
Butterworth.Sparck Jones, K. (1986).
Synonymy andSemantic Classification.
EdinburghInformation Technology Series (EDITS).Edinburgh University Press.Talmy, L. (1985).
"Lexicalization patterns.
"In Language Typology and SyntacticDescription 3: Grammatical Categories and the357Computational Linguistics Volume 19, Number 2Lexicon, edited by T. Shopen, 57-149.Cambridge University Press.Touretzky, D. S. (1986).
The Mathematics ofInheritance Systems.
Morgan Kaufmann.Veronis, J., and Ide, N. (1991).
"Anassessment of semantic informationautomatically extracted from machinereadable dictionaries."
In Proceedings, FifthConference ofthe European Chapter of theAssociation for Computational Linguistics.Berlin, Germany, April 1991.Vickery, B. C. (1975).
Classi~cation a dIndexing in Science.
Butterworth and Co.Walker, D. E., and Amsler, R. A.
(1986).
"The use of machine-readable dictionariesin sublanguage analysis."
In AnalyzingLanguage in Restricted Domains, edited byR.
Grishman and R. Kittredge.
LawrenceErlbaum Associates.Vossen, P.; Meijs, W.; and den Broeder, M.(1989).
"Meaning and structure indictionary definitions."
In ComputationalLexicography for Natural LanguageProcessing, edited by B. Boguraev andT.
Briscoe.
Longman.Wang, Y.-C.; Vandendorpe, J.; and Evens, M.(1985).
"Relational thesauri in informationretrieval."
Journal of the American Society forInformation Science, 36, 15-27.Wilensky, R.; Chin, D. N.; Luria, M.; Martin,J.
; Mayfield, J.; and Wu, D. (1988).
"TheBerkeley UNIX consultant project.
"Computational Linguistics, 14(4), 35-84.Wilks, Y.
A.
(1978).
"Making preferencesmore active."
Artificial Intelligence, 10,75-97.Wilks, Y.
A.; Fass, D.; Guo, C.-M.;McDonald, J. E.; Plate, T.; and Slator,B.
M. (1989).
"A tractable machinedictionary as a resource for computationalsemantics."
In Computational Lexicographyfor Natural Language Processing, edited byB.
Boguraev and T. Briscoe, 193-228.Longman.Wilks, Y.; Fass, D.; Guo, C.-M.; McDonald,J.
E.; Plate, T.; and Slator, B. M.
(1993).
"Providing machine tractable dictionarytools."
In Semantics and the Lexicon, editedby J. Pustejovsky.
Kluwer AcademicPublishers.Zernik, U.
(1989).
"Lexicon acquisition:Learning from corpus by exploitinglexical categories."
In Proceedings,IJCAI-89.358
