Proceedings of EACL '99Automat ic  Verb Class i f icat ion Us ingD is t r ibut ions  of  Grammat ica l  FeaturesSuzanne StevensonDept  of  Computer  Scienceand Center  for Cogn i t ive  Science (RuCCS)Rutgers  Un ivers i tyCoRE Building, Busch CampusNew Brunswick, NJ 08903U.S.A.suzanne@ruccs, rutgers, eduPao la  Mer loLATL-Depar tment  of  L inguist icsUn ivers i ty  of  Geneva2 rue de Cando l le1211 Gen~ve 4SWITZERLANDmerlo@lettres, unige, chAbstractWe apply machine learning techniquesto classify automatically a set of verbsinto lexical semantic classes, based ondistributional approximations of diathe-ses, extracted from a very large anno-tated corpus.
Distributions of four gram-matical features are sufficient o reduceerror rate by 50% over chance.
We con-clude that corpus data is a usable repos-itory of verb class information, and thatcorpus-driven extraction of grammaticalfeatures is a promising methodology forautomatic lexical acquisition.1 IntroductionRecent years have witnessed a shift in grammardevelopment methodology, from crafting largegrammars, to annotation of corpora.
Correspond-ingly, there has been a change from developingrule-based parsers to developing statistical meth-ods for inducing grammatical knowledge from an-notated corpus data.
The shift has mostly oc-curred because building wide-coverage grammarsis time-consuming, error prone, and difficult.
Thesame can be said for crafting the rich lexical rep-resentations that are a central component of lin-guistic knowledge, and research in automatic lex-ical acquisition has sought to address this ((Dorrand Jones, 1996; Dorr, 1997), among others).Yet there have been few attempts to learn fine-grained lexical classifications from the statisti-cal analysis of distributional data, analogously tothe induction of syntactic knowledge (though see,e.g., (Brent, 1993; Klavans and Chodorow, 1992;Resnik, 1992)).
In this paper, we propose such anapproach for the automatic lassification of verbsinto lexical semantic lasses.
1We can express the issues raised by this ap-proach as follows.1.
Which linguistic distinctions among lexicalclasses can we expect o find in a corpus?2.
How easily can we extract he frequency dis-tributions that approximate the relevant lin-guistic properties?3.
Which frequency distributions work best todistinguish the verb classes?In exploring these questions, we focus on verbclassification for several reasons.
Verbs are veryimportant sources of knowledge in many languageengineering tasks, and the relationships amongverbs appear to play a major role in the orga-nization and use of this knowledge: Knowledgeabout verb classes is crucial for lexical acquisitionin support of language generation and machinetranslation (Dorr, 1997), and document classifica-tion (Klavans and Kan, 1998).
Manual classifica-tion of large numbers of verbs is a difficult andresource intensive task (Levin, 1993; Miller et ah,1990; Dang et ah, 1998).To address these issues, we suggest hat one canautomatically classify verbs by using statisticalapproximations to verb diatheses, to train an au-tomatic classifier.
We use verb diatheses, follow-ing Levin and Dorr, for two reasons.
First, verbdiatheses are syntactic cues to semantic lasses,~We are aware that a distributional pproach restson one strong assumption on the nature of the rep-resentations under study: semantic notions and syn-tactic notions are correlated, at least in part.
Thisassumption is not uncontroversial (Briscoe and Copes-take, 1995; Levin, 1993; Dorr and Jones, 1996; Dorr,1997).
We adopt it here as a working hypothesis with-out further discussion.45Proceedings of EACL '99hence they can be more easily captured by corpus-based techniques.
Second, using verb diatheses re-duces noise.
There is a certain consensus (Briscoeand Copestake, 1995; Pustejovsky, 1995; Palmer,1999) that verb diatheses are regular sense exten-sions.
Hence focussing on this type of classifica-tion allows one to abstract from the problem ofword sense disambiguation and treat residual dif-ferences in word senses as noise in the classifica-tion task.We present an in-depth case study, in which weapply machine learning techniques to automati-cally classify a set of verbs based on distribu-tions of grammatical indicators of diatheses, ex-tracted from a very large corpus.
We look at threevery interesting classes of verbs: unergatives, un-accusatives, and object-drop verbs (Levin, 1993).These are interesting classes because they all par-ticipate in the transitivity alternation, and theyare minimal pairs - that is, a small number ofwell-defined istinctions differentiate their transi-tive/intransitive behavior.
Thus, we expect thedifferences in their distributions to be small, en-tailing a fine-grained iscrimination task that pro-vides a challenging testbed for automatic lassifi-cation.The specific theoretical question we investigateis whether the factors underlying the verb classdistinctions are reflected in the statistical distri-butions of lexical features related to diatheses pre-sented by the individual verbs in the corpus.
Indoing this, we address the questions above by de-termining what are the lexical features that coulddistinguish the behavior of the classes of verbswith respect to the relevant diatheses, which ofthose features can be gleaned from the corpus,and which of those, once the statistical distribu-tions are available, can be used successfully by anautomatic lassifier.We follow a computational experimentalmethodology by investigating as indicated eachof the hypotheses below:HI: Linguistically and psychologically motivatedfeatures for distinguishing the verb classes are ap-parent within linguistic experience.We analyze the three classes based on prop-erties of the verbs that have been shown tobe relevant for linguistic classification (Levin93), or for disambiguation i syntactic pro-cessing (MacDonald94, Trueswel196) to deter-mine potentially relevant distinctive features.We then count those features (or approxima-tions to them) in a very large corpus.H2: The distributional patterns of (some of) thosefeatures contribute to learning the classificationsof the verbs.We apply machine learning techniques to de-termine whether the features support thelearning of the classifications.H3: Non-overlapping features are the most effec-tive in learning the classifications of the verbs.We analyze the contribution of different fea-tures to the classification process.To preview, we find that, related to (HI), lin-guistically motivated features (related to diathe-ses) that distinguish the verb classes can be ex-tracted from an annotated, and in one case parsed,corpus.
In relation to (H2), a subset of thesefeatures is sufficient o halve the error rate com-pared to chance in automatic verb classification,suggesting that distributional data provides use-ful knowledge to the classification of verbs.
Fur-thermore, in relation to (H3) we find that featuresthat are distributionally predictable, because theyare highly correlated to other features, contributelittle to classification performance.
We concludethat the usefulness of distributional features to thelearner is determined by their informativeness.2 Determin ing  the  FeaturesIn this section, we present motivation for the fea-tures that we investigate in terms of their role inlearning the verb classes.
We first present he lin-guistically derived features, then turn to evidencefrom experimental psycholinguistics to extend theset of potentially relevant features.2.1 Features  of  the  Verb ClassesThe three verb classes under investigation -unergatives, unaccusatives, and object-drop - dif-fer in the properties of their transitive/intransitivealternations, which are exemplified below.Unergative:(la) The horse raced past the barn.
(lb) The jockey raced the horse past the barn.Unaccusative:(2a) The butter melted in the pan.
(2b) The cook melted the butter in the pan.Object-drop:(3a) The boy washed the hall.
(3b) The boy washed.The sentences in (1) use an unergative verb, raced.Unergatives are intransitive action verbs whosetransitive form is the causative counterpart of the46Proceedings of EACL '99intransitive form.
Thus, the subject of the in-transitive (la) becomes the object of the transi-tive (lb) (Brousseau and Ritter, 1991; Hale andKeyser, 1993; Levin and Rappaport Hovav, 1995).The sentences in (2) use an unaccusative verb,melted.
Unaccusatives are intransitive change ofstate verbs (2a); like unergatives, the transitivecounterpart for these verbs is also causative (2b).The sentences in (3) use an object-drop verb,washed; these verbs have a non-causative transi-tive/intransitive alternation, in which the objectis simply optional.Both unergatives and unaccusatives have acausative transitive form, but differ in the seman-tic roles that they assign to the participants in theevent described.
In an intransitive unergative, thesubject is an Agent (the doer of the event), andin an intransitive unaccusative, the subject is aTheme (something affected by the event).
Therole assignments to the corresponding semanticarguments of the transitive forms--i.e., the di-rect objects--are the same, with the addition of aCausal Agent (the causer of the event) as subjectin both cases.
Object-drop verbs simply assignAgent to the subject and Theme to the optionalobject.We expect the differing semantic role assign-ments of the verb classes to be reflected in theirsyntactic behavior, and consequently in the distri-butional data we collect from a corpus.
The threeclasses can be characterized by their occurrencein two alternations: the transitive/intransitive al-ternation and the causative alternation.
Unerga-tives are distinguished from the other classes inbeing rare in the transitive form (see (Steven-son and Merlo, 1997) for an explanation of thisfact).
Both unergatives and unaccusatives are dis-tinguished from object-drop in being causative intheir transitive form, and similarly we expect histo be reflected in amount of detectable causativeuse.
Furthermore, since the causative is a transi-tive use, and the transitive use of unergatives iexpected to be rare, causativity should primar-ily distinguish unaccusatives from object-drops.In conclusion, we expect the defining features ofthe verb classes--the intransitive/transitive andcausative alternations--to lead to distributionaldifferences in the observed usages of the verbs inthese alternations.2.2 Features  of  the MV/RR A l ternat ivesNot only do the verbs under study differ in theirthematic properties, they also differ in their pro-cessing properties.
Because these verbs can occurboth in a transitive and an intransitive form, theyhave been particularly studied in the context ofthe main verb/reduced relative (MV/RR) ambi-guity illustrated below (Bever, 1970):The horse raced past the barn fell.The verb raced can be interpreted as either a pasttense main verb, or as a past participle within areduced relative clause (i.e., the horse \[that was\]raced past the barn).
Because fell is the main verb,the reduced relative interpretation of raced is re-quired for a coherent analysis of the complete sen-tence.
But the main verb interpretation of raced isso strongly preferred that people experience greatdifficulty at the verb fell, unable to integrate itwith the interpretation that has been developedto that point.
However, the reduced relative in-terpretation is not difficult for all verbs, as in thefollowing example:The boy washed in the tub was angry.The difference in ease of interpreting the resolu-tions of this ambiguity has been shown to be sen-sitive to both frequency differentials (MacDonald,1994; Trueswell, 1996) and to verb class distinc-tions (?
).Consider the features that distinguish the tworesolutions of the MV/RR ambiguity:Main Verb: The horse raced past the barn quickly.Reduced Relative: The horse raced past the barnfell.In the main verb resolution, the ambiguous verbraced is used in its intransitive form, while inthe reduaed relative, it is used in its transitive,causative form.
These features correspond di-rectly to the defining alternations of the threeverb classes under study (intransitive/transitive,causative).
Additionally, we see that other re-lated features to these usages erve to distinguishthe two resolutions of the ambiguity.
The mainverb form is active and a main verb part-of-speech(labeled as VBD by automatic POS taggers);by contrast, the reduced relative form is passiveand a past participle (tagged as VBN).
Althoughthese properties are redundant with the intran-sitive/transitive distinction, recent work in ma-chine learning (Ratnaparkhi, 1997; Ratnaparkhi,1998) has shown that using overlapping featurescan be beneficial for learning in a maximum en-tropy framework, and we want to explore it in thissetting to test H3 above.
2 In the next section,2These properties are redundant with the intran-sitive/transitive distinction, as passive implies tran-sitive use, and necessarily entails the use of a pastparticiple.
We performed a correlation analysis that47Proceedings of EACL '99we describe how we compile the corpus counts foreach of the four properties, in order to approxi-mate the distributional information of these alter-nations.3 F requency  D is t r ibut ions  o f  theFeaturesWe assume that currently available large cor-pora are a reasonable approximation to lan-guage (Pullum, 1996).
Using a combined cor-pus of 65-million words, we measured the rel-ative frequency distributions of the linguisticfeatures (VBD/VBN, active/passive, intransi-tive/transitive, causative/non-causative) over asample of verbs from the three lexical semanticclasses.3.1 Mater ia l sWe chose a set of 20 verbs from each class - di-vided into two groups each, as will be explainedbelow - based primarily on the classification ofverbs in (Levin, 1993).The unergatives are manner of motion verbs:jumped, rushed, marched, leaped, floated, raced,hurried, wandered, vaulted, paraded (group 1);galloped, glided, hiked, hopped, jogged, scooted,scurried, skipped, tiptoed, trotted (group 2).The unaccusatives are verbs of change of state:opened, exploded, flooded, dissolved, cracked,hardened, boiled, melted, fractured, solidified(group 1); collapsed, cooled, folded, widened,changed, cleared, divided, simmered, stabilized(group 2).The object-drop verbs are unspecified object al-ternation verbs: played, painted, kicked, carved,reaped, washed, danced, yelled, typed, knitted(group 1); borrowed, inherited, organised, rented,sketched, cleaned, packed, studied, swallowed,called (group 2).The verbs were selected from Levin's classes onthe basis of our intuitive judgment hat they arelikely to be used with sufficient frequency to befound in the corpus we had available.
Further-more, they do not generally show massive depar-tures from the intended verb sense in the corpus.
(Though note that there are only 19 unaccusativesbecause ripped, which was initially counted ingroup 2 of unaccusatives, was then excluded fromthe analysis as it occurred mostly in a differentusage in the corpus; ie, as a verb plus particle.
)yielded highly significant R=.44 between intransitiveand active use, and R=.36 between intransitive andmain verb (VBD) use.
We discuss the effects of fea-ture overlap in the experimental section.Most of the verbs can occur in the transitiveand in the passive.
Each verb presents the sameform in the simple past and in the past participle,entailing that we can extract both active and pas-sive occurrences by searching on a single token.In order to simplify the counting procedure, wemade the assumption that counts on this singleverb form would approximate the distribution ofthe features across all forms of the verb.Most counts were performed on the tagged ver-sion of the Brown Corpus and on the portion of theWall Street Journal distributed by the ACL/DCI(years 1987, 1988, 1989), a combined corpus inexcess of 65 million words, with the exception ofcausativity which was counted only for the 1988year of the WSJ, a corpus of 29 million words.3.2 MethodWe counted the occurrences of each verb tokenin a transitive or intransitive use (INTR), in anactive or passive use (ACT), in a past participleor simple past use (VBD), and in a causative ornon-causative use (CAUS).
3 More precisely, thefollowing occurrences were counted in the corpus.INTR: the closest nominal group following theverb token was considered to be a potential ob-ject of the verb.
A verb occurrence immmediatelyfollowed by a potential object was counted as tran-sitive.
If no object followed, the occurrence wascounted as intransitive.ACT: main verb (ie, those tagged VBD) werecounted as active.
Tokens with tag VBN were alsocounted as active if the closest preceding auxiliarywas have, while they were counted as passive if theclosest preceding auxiliary was be.VBD: A part-of-speech tagged corpus was used,hence the counts for VBD/VBN were simply donebased on the POS label according to the taggedcorpus.
?AUS: The causative feature was approximatedby the following steps.
First, for each verb occur-rence subjects and objects were extracted froma parsed corpus (Collins 1997).
Then the propor-3In performing this kind of corpus analysis, onehas to take into account he fact that current corpusannotations do not distinguish verb senses.
However,in these counts, we did not distinguish a core senseof the verb from an extended use of the verb.
So,for instance, the sentence Consumer spending jumped1.7 ~o in February after a sharp drop the month be-fore (WSJ 1987) is counted as an occurrence of themanner-of-motion verb jump in its intransitive form.This kind of extension of meaning does not modifysubcategorization distributions (Roland and Jurafsky,1998), although it might modify the rate of causativ-ity, but this is an unavoidable imitation at the currentstate of annotation of corpora.48Proceedings of EACL '99tion of overlap between the two multisets of nounswas calculated, meant to capture the property ofthe causative construction that the subject of theintransitive can occur as the object of the transi-tive.
We define overlap as the largest multiset ofelements belonging to both the subjects and theobject multisets, e.g.
{a, a, a, b} A {a} = {a, a, a}.The proportion is the ratio between the overlapand the sum of the subject and object multisets.The verbs in group 1 had been used in an earlierstudy, in which it was important to minimize noisydata, so they generally underwent greater man-ual intervention i  the counts.
In adding group 2for the classification experiment, we chose to min-imize the intervention, in order to demonstratethat the classification process is robust enough towithstand the resulting noise in the data.For transitivity and voice, the method of countdepended on the group.
For group 1, the countswere done automatically by regular expressionpatterns, and then corrected, partly by hand andpartly automatically.
For group 2, the counts weredone automatically without any manual interven-tion.
For causativity, the same counting scriptswere used for both groups of verbs, but the in-put to the counting programs was determined bymanual inspection of the corpus for verbs belong-ing to group 1, while it was extracted automati-cally from a parsed corpus for group 2 (WSJ 1988,parsed with the parser from (Collins, 1997).Each count was normalized over all occurrencesof the verb, yielding a total of four relative fre-quency features: VBD (%VBD tag), ACT (%activeuse), INTR (%intransitive use), CAUS (%causativeuse) .44 Exper iments  in  C lus ter ing  andC lass i f i ca t ionOur goal was to determine whether statistical in-dicators can be automatically combined to de-termine the class of a verb from its distribu-tional properties.
We experimented both withself-aggregating and supervised methods.
The fre-quency distributions of the verb alternation fea-tures yield a vector for each verb that representsthe relative frequency values for the verb on eachdimension; the set of 59 vectors constitute thedata for our machine learning experiments.Vector template: \[verb, VBD, ACT, INTK, CAUS\]Example: \[opened, .793, .910, .308, .158\]4 All raw and normalized corpus data  are availablefrom the authors.Table 1: Accuracy of the Verb Clustering Task.Features Accuracy1.
VBD ACT INTI~ CAUS 52%"2.
VBD ACT CAUS 54%3.
VBD ACT INTR 45%'4.
ACT INTR.
CAUS 47%5.
VBD INTB.
CAUS 66%We must now determine which of the distri-butions actually contribute to learning the verbclassifications.
First we describe computationalexperiments in unsupervised learning, using hi-erarchical clustering, then we turn to supervisedclassification.4.1 Unsuperv ised  Learn ingOther work in automatic lexical semantic lassifi-cation has taken an approach in which clusteringover statistical features is used in the automaticformation of classes (Pereira et al, 1993; Pereiraet al, 1997; Resnik, 1992).
We used the hierar-chical clustering algorithm available in SPlus5.0,imposing a cut point that produced three clus-ters, to correspond to the three verb classes.
Ta-ble 1 shows the accuracy achieved using the fourfeatures described above (row 1), and all three-feature subsets of those four features (rows 2-5).
Note that chance performance in this task (athree-way classification) is 33% correct.The highest accuracy in clustering, of 66%--or half the error rate compared to chance--is ob-tained only by the triple of features in row 5 inthe table: VBD, INTR., and CANS.
All other sub-sets of features yield a much lower accuracy, of 45-54%.
We can conclude that some of the featurescontribute useful information to guide clustering,but the inclusion of ACT actually degrades perfor~mance.
Clearly, having fewer but more relevantfeatures is important o accuracy in verb classi-fication.
We will return to the issue in detail ofwhich features contribute most to learning in ourdiscussion of supervised learning below.A problem with analyzing the clustering perfor-mance is that it is not always clear what counts asa misclassification.
We cannot actually know whatthe identity of the verb class is for each cluster.In the above results, we imposed a classificationbased on the class of the majority of verbs in acluster, but often there was a tie between classeswithin a cluster, and/or the same class was themajority class in more than one cluster.
To evalu-ate better the effects of the features in learning, wetherefore turned to a supervised learning method,49Proceedings of EACL '99Table 2: Accuracy of the Verb Classification Task.i Decision Trees Rule SetsFeatures Accuracy Standard Error Accuracy Standard Error1.
VBD ACT INTR.
CAUS 64.2% 1.7% 64.9% 1.6%2.
VBD ACT CADS 55.4% 1.5% 55.7% 1.4%-3.
VBD ACT INTR'4.
ACT INTR CADS5.
VBD INTR.
CADS54.4% 1.4%59.8% 1.2%56.7% 1.5%58.9% 0.9%60.9% 1.2% 62.3% 1.2%where the classification of each verb in a test setis unambiguous.4.2 Superv ised  learn ingFor our supervised learning experiments, we usedthe publicly available version of the C5.0 ma-chine learning algorithm, 5 a newer version of C4.5(Quinlan, 1992), which generates decision treesfrom a set of known classifications.
We also hadthe system extract rule sets automatically fromthe decision trees.
For all reported experiments,we ran a 10-fold cross-validation repeated tentimes, and the numbers reported are averages overall the runs.
6Table 2 shows the results of our experiments onthe four features we counted in the corpora (VBD,ACT, INTR., CADS), as well as all three-feature sub-sets of those four.
As seen in the table, classifi-cation based on the four features performs at 64-65%, or 31% over chance.
(Recall that this is a3-way decision, hence baseline is 33%).Given the resources needed to extract the fea-tures from the corpus and to annotate the cor-pus itself, we need to understand the relative con-tribution of each feature to the results - one ormore of the features may make little or no con-tribution to the successful classification behavior.Observe that when either the INTR or CADS fea-ture is removed (rows 2 and 3, respectively, of Ta-ble 2), performance degrades considerably, with adecrease in accuracy of 8-10% from the maximumachieved with the four features (row 1).
However,when the VBD feature is removed (row 4), thereis a smaller decrease in accuracy, of 4-6%.
Whenthe ACT feature is removed (row 5), there is an5Available for a number of platforms fromhttp ://www.
rulequest, com/.6A 10-fold cross-validation means that the systemrandomly divides the data into ten parts, and runs tentimes on a different 90%-training-data/t0%-test-datasplit, yielding an average accuracy and standard error.This procedure is then repeated for 10 different ran-dom divisions of the data, and accuracy and standarderror are again averaged across the ten runs.even smaller decrease, of 2-4%.
In fact, the accu-racy here is very close to the accuracy of the four-feature results when the standard error is takeninto account.
We conclude then that INTR andCADS contribute the most to the accuracy of theclassification, while ACT seems to contribute little.
(Compare the clustering results, in which the bestperformance was achieved with the subset of fea-tures excluding ACT.)
This shows that not all thelinguistically relevant features are equally usefulin learning.We think that this pattern of results is relatedto the combination of the feature distributions:some distributions are highly correlated, whileothers are not.
According to our calculations,CADS is not significantly correlated with any otherfeature; of the features that are significantly cor-related, VBD is more highly correlated with ACTthan with INTI~ (R=.67 and g=.36 respectively),while INTR is more highly correlated with ACTthan with VBD (R=.44 and R=.36 respectively).We expect combinations of features that are notcorrelated to yield better classification accuracy.If we compare the accuracy of the 3-feature com-binations in Table 2 (rows 2-5), this hypothesis isconfirmed.
The three combinations that containthe feature CADS (rows 2, 4 and 5)- - the uncorre-lated feature--have better performance than thecombination that does not (row 3), as expected.Now consider the subsets of three features thatinclude CADS with a pair of the other correlatedfeatures.
The combination containing VBD andINTR (row 5)- - the least correlated pair of the fea-tures VBD, INTR, and ACT--has the best accuracy,while the combination containing the highly cor-related VBD and ACT (row 2) has the worst ac-curacy.
The accuracy of the subset {vso, INTR,CADS} (row 5) is also better than the accuracy ofthe subset {ACT, INTa, CADS} (row 4), becauseINTR overlaps with VBD less than with ACT.
77We suspect that another factor comes into play,namely how noisy the feature is.
The similarity inperformance using INTR or CADS in combination with50Proceedings of EACL '995 ConclusionsIn this paper, we have presented an in-depth casestudy, in which we apply machine learning tech-niques to automatically classify a set of verbs,based on distributional features extracted from avery large corpus.
Results show that a small num-ber of linguistically motivated grammatical fea-tures are sufficient to halve the error rate overchance.
This leads us to conclude that corpusdata is a usable repository of verb class infor-mation.
On one hand, we observe that seman-tic properties of verb classes (such as causativity)may be usefully approximated through countablefeatures.
Even with some noise, lexical proper-ties are reflected in the corpus robustly enoughto positively contribute in classification.
On theother hand, however, we remark that deep lin-guistic analysis cannot be eliminated.
In our ap-proach, it is embedded in the selection of the fea-tures to count.
We also think that using linguisti-cally motivated features makes the approach veryeffective and easily scalable: we report a 50% re-duction in error rate, with only 4 features that arerelatively straightforward to count.AcknowledgementsThis research was partly sponsored by the SwissNational Science Foundation, under fellowship8210-46569 to P. Merlo, and by the US NationalScience Foundation, under grants ~:9702331 and~9818322 to S. Stevenson.
We thank MarthaPalmer for getting us started on this work andMichael Collins for giving us acces to the outputof his parser.ReferencesThomas G. Bever.
1970.
The cognitive basis forlinguistic structure.
In J. R. Hayes, editor, Cog-nition and the Development of Language.
JohnWiley, New York.Michael Brent.
1993.
From grammar to lexicon:Unsupervised learning of lexical syntax.
Com-putational Linguistics, 19(2):243-262.Edward Briscoe and Ann Copestake.
1995.
Lex-icaI rules in the TDFS framework.
Technicalreport, Acquilex-II Working Papers.VBD and ACT (rows 2 and 3) might be due to the factthat  the counts for CAUS are a more noisy approxima-tion of the actual feature distribution than the countsfor INTR.
We reserve defining a precise model of noise,and its interaction with the other features, for futureresearch.Anne-Marie Brousseau and Elizabeth Ritter.1991.
A non-unified analysis of agentive verbs.In West Coast Conference on Formal Linguis-tics, number 20, pages 53-64.Michael John Collins.
1997.
Three generative,lexicalised models for statistical parsing.
InProc.
of the 35th Annual Meeting of the ACL,pages 16-23.Hoa Trang Dang, Karin Kipper, Martha Palmer,and Joseph Rosenzweig.
1998.
Investigat-ing regular sense extensions based on intere-sective Levin classes.
In Proc.
of the 36th An-nual Meeting of the ACL and the 17th Interna-tional Conference on Computational Linguistics(COLING-A CL '98), pages 293-299, Montreal,Canada.
Universit~ de Montreal.Bonnie Dorr and Doug Jones.
1996.
Role of wordsense disambiguation i  lexical acquisition: Pre-dicting semantics from syntactic ues.
In Proc.of the 16th International Conference on Com-putational Linguistics, pages 322-327, Copen-hagen, Denmark.Bonnie Dorr.
1997.
Large-scale dictionary con-struction for foreign language tutoring and in-terlingual machine translation.
Machine Trans-lation, 12:1-55.Ken Hale and Jay Keyser.
1993.
On argumentstructure and the lexical representation f syn-tactic relations.
In K. Hale and J. Keyser, edi-tors, The View from Building 20, pages 53-110.MIT Press.Judith L. Klavans and Martin Chodorow.
1992.Degrees of stativity: The lexical representationof verb aspect.
In Proceedings of the Four-teenth International Conference on Computa-tional Linguistics.Judith Klavans and Min-Yen Kan. 1998.
Role ofverbs in document analysis.
In Proc.
of the 36thAnnual Meeting of the ACL and the 17th Inter-national Conference on Computational Linguis-tics (COLING-A CL '98), pages 680-686, Mon-treal, Canada.
Universit~ de Montreal.Beth Levin and Malka Rappaport Hovav.
1995.Unaccusativity.
MIT Press, Cambridge, MA.Beth Levin.
1993.
English Verb Classes and Al-ternations.
Chicago University Press, Chicago,IL.Maryellen C. MacDonald.
1994.
Probabilisticconstraints and syntactic ambiguity resolution.Language and Cognitive Processes, 9(2):157-201.51Proceedings of EACL '99George Miller, R. Beckwith, C. Fellbaum,D.
Gross, and K. Miller.
1990.
Five papers onWordnet.
Technical report, Cognitive ScienceLab, Princeton University.Martha Palmer.
1999.
Consistent criteria forsense distinctions.
Computing for the Humani-ties.Fernando Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional c ustering of english words.In Proc.
of the 31th Annual Meeting of the ACL,pages 183-190.Fernando Pereira, Ido Dagan, and Lillian Lee.1997.
Similarity-based methods for word sensedisambiguation.
In Proc.
of the 35th AnnualMeeting of the ACL and the 8th Conf.
of theEA CL (A CL/EA CL '97), pages 56 -63.Geoffrey K. Pullum.
1996.
Learnability, hyper-learning, and the poverty of the stimulus.
InJan Johnson, Matthew L. Juge, and Jeri L.Moxley, editors, 22nd Annual Meeting of theBerkeley Linguistics Society: General Sessionand Parasession on the Role of Learnability inGrammatical Theory, pages 498-513, Berkeley,California.
Berkeley Linguistics Society.James Pustejovsky.
1995.
The Generative Lezi-con.
MIT Press.J.
Ross Quinlan.
1992.
C~.5 : Programs for Ma-chine Learning.
Series in Machine Learning.Morgan Kaufmann, San Mateo, CA.Adwait Ratnaparkhi.
1997.
A linear observedtime statistical parser based on maximum en-tropy models.
In 2nd Conf.
on Empirical Meth-ods in NLP, pages 1-10, Providence, RI.Adwait Ratnaparkhi.
1998.
Statistical models forunsupervised prepositional phrase attachment.In Proc.
of the 36th Annual Meeting of the A CL,Montreal, CA.Philip Resnik.
1992.
Wordnet and distributionalanalysis: a class-based approach to lexical dis-covery.
In AAAI Workshop in Statistically-based NLP Techniques, pages 56-64.Doug Roland and Dan Jurafsky.
1998.
Howverb subcategorization frequencies are affectedby corpus choice.
In Proc.
of the 36th AnnualMeeting of the ACL, Montreal, CA,Suzanne Stevenson and Paola Merlo.
1997.
Lexi-cal structure and parsing complexity.
Languageand Cognitive Processes, 12(2/3):349-399.John Trueswell.
1996.
The role of lexical fre-quency in syntactic ambiguity resolution.
J. ofMemory and Language, 35:566-585.52
