A Procedure for Mult i -Class Discr iminat ion and some LinguisticAppl icat ionsVlad imi r  Per i c l ievIns t i tu te  of Mathemat ics  &: In format icsAcad.
G. Bonchev Str.,  bl.
8,1113 Sofia, Bu lgar iaper i?math ,  acad .
bgRad l  E .
Va ld4s -P~rezComputer  Science Depar tmentCarnegie Mel lon Univers i tyP i t t sburgh ,  PA 15213, USAva ldes?cs ,  cmu.
eduAbst rac tThe paper describes a novel computa-tional tool for multiple concept learn-ing.
Unlike previous approaches, whosemajor goal is prediction on unseen in-stances rather than the legibility of theoutput, our MPD (Maximally Parsimo-nious Discrimination) program empha-sizes the conciseness and intelligibilityof the resultant class descriptions, usingthree intuitive simplicity criteria to thisend.
We illustrate MPD with applica-tions in componential nalysis (in lexicol-ogy and phonology), language typology,and speech pathology.1 In t roduct ionA common task of knowledge discovery is multi-ple concept learning, in which from multiple givenclasses (i.e.
a typology) the profiles of these classesare inferred, such that every class is contrasted fromevery other class by feature values.
Ideally, goodprofiles, besides making good predictions on futureinstances, should be concise, intelligible, and com-prehensive (i.e.
yielding all alternatives).Previous approaches like ID3 (Quinlan, 1983) orC4.5 (Quinlan, 1993), which use variations on greedysearch, i.e.
localized best-next-step search (typi-cally based on information-gain heuristics), have astheir major goal prediction on unseen instances, andtherefore do not have as an explicit concern theconciseness, intelligibility, and comprehensiveness ofthe output.
In contrast to virtually all previousapproaches to multi-class discrimination, the MPD(Maximally Parsimonious Discrimination) programwe describe here aims at the legibility of the resul-tant class profiles.
To do so, it (1) uses a minimalnumber of features by carrying out a global opti-mization, rather than heuristic greedy search; (2)produces conjunctive, or nearly conjunctive, profilesfor the sake of intelligibility; and (3) gives all alterna-tive solutions.
The first goal stems from the familiar1034requirement that classes be distinguished by jointlynecessary and sufficient descriptions.
The second ac-cords with the also familiar thesis that conjunctivedescriptions are more comprehensible (they are thenorm for typological classification (Hempel, 1965),and they are more readily acquired by experimen-tal subjects than disjunctive ones (Bruner et.
al.,1956)), and the third expresses the usefulness, for adiversity of reasons, of having all alternatives.
Lin-guists would generally subscribe to all three require-ments, hence the need for a computational tool withsuch focus3In this paper, we briefly describe the MPD system(details may be found in Valdrs-P@rez and Pericliev,1997; submitted) and focus on some linguistic appli-cations, including componential nalysis of kinshipterms, distinctive feature analysis in phonology, lan-guage typology, and discrimination of aphasic syn-dromes from coded texts in the CHILDES database.For further interesting application areas of similaralgorithms, cf.
Daelemans et.
al., 1996 and Tanaka,1996.2 Overv iew o f  the  MPD programThe Maximally Parsimonious Discrimination pro-gram (MPD) is a general computational tool forinferring, given multiple classes (or, a typology),with attendant instances of these classes, the pro-files (=descriptions) of these classes uch that everyclass is contrasted from all remaining classes on thebasis of feature values.
Below is a brief descriptionof the program.2.1 Expressing contrastsThe MPD program uses Boolean, nominal and nu-meric features to express contrasts, as follows:~The profiling of multiple types, in actual fact, is ageneric task of knowledge discovery, and the programwe describe has found substantial pplications in areasoutside of linguistics, as e.g., in criminology, audiology,and datasets from the UC Irvine repository.
However,we shall not discuss these applications here.?
Two classes C1 and C2 are contrasted by aBoolean or nominal feature if the instances ofC1 and the instances of C2 do not share a value.?
Two classes C1 and C2 are contrasted by a nu-meric feature if the ranges of the instances ofC1 and of C2 do not overlap.
2MPD distinguishes two types of contrasts: (1) ab.solute contrasts when all the classes can be cleanlydistinguished, and (2) partial contrasts when no ab-solute contrasts are possible between some pairwiseclasses, but absolute contrasts can nevertheless beachieved by deleting up to N per cent of the in-stances, where N is specified by the user.The program can also invent derived features--inthe case when no successful (absolute) contrasts areso far achieved--the key idea of which is to expressinteractions between the given primitive features.Currently we have implemented inventing novel de-rived features via combining two primitive features(combining three or more primitive features is alsopossible, but has not so far been done owing to thelikelihood of a combinatorial explosion):?
Two Boolean features P and Q are combinedinto a set of two-place functions, none of whichis reducible to a one-place function or to thenegation of another two-place function in theset.
The resulting set consists of P-and-Q, P-or-Q, P-iff-Q, P-implies-Q, and Q-implies-P.?
Two nominal features M and N are combinedinto a single two-place nominal function MxN.?
Two numeric features X and Y are combinedby forming their product and their quotient.
3Both primitive and derived features are treatedanalogously in deciding whether two classes are con-trasted by a feature, since derived features are legit-imate Boolean, nominal or numeric features.It will be observed that contrasts by a nominalor numeric feature may (but will not necessarily)introduce a slight degree of disjunctiveness, which isto a somewhat greater extent the case in contrastsaccomplished by derived features.Missing values do not present much problem,since they can be ignored without any need to es-timate a value nor to discard the remaining infor-mative features values of the instance.
In the caseof nominal features, missing values can be treated asjust another legitimate feature value.2.2 The  s impl ic i ty  c r i te r iaMPD uses three intuitive criteria to guarantee theuncovering of the most parsimonious discriminationamong classes:2Besides these atomic feature values we may also sup-port (hierarchically) structured values, but this will beof no concern here.~Analogously to the Bacon program's invention oftheoretical terms Langley et.
al., 1987.1.
Minimize overall features.
A set of classes maybe demarcated using a number of overall fea-ture sets of different cardinality; this criterionchooses those overall feature sets which havethe smallest cardinality (i.e.
are the shortest).2.
Minimize profiles.
Given some overall featureset, one class may be demarcated--using onlyfeatures from this set- -by a number of profilesof different cardinality; this criterion choosesthose profiles having the smallest cardinality.3.
Maximize coordination.
This criterion maxi-mizes the coherence between class profiles inone discrimination model, 4 in the case whenalternative profiles remain even after the appli-cation of the two previous implicity criteria.
5Due to space limitations, we cannot enter into theimplementation details of these global optimizationcriteria, in fact the most expensive mechanism ofMPD.
Suffice it to say here that they are imple-mented in a uniform way (in all three cases by con-verting a logic formula - either CNF or somethingmore complicated - into a DNF formula), and all canuse both sound and unsound (but good) heuristicsto deal successfully with the potentially explosivecombinatorics inherent in the conversion to DNF.2.3 An  i l lus t ra t ionBy way of (a simplified) illustration, let us considerthe learning of the Bulgarian translational equiva-lents of the English verb feed on the basis of thecase frames of the latter.
Assume the following fea-tures/values, corresponding to the verbal slots: (1)NPl={hum,beast,phys-obj}, (2) VTR (binary fea-ture denoting whether the verb is transitive or not),(3) NP2 (same values as NP1), (4) PP (binary fea-ture expressing the obligatory presence of a prepo-sitional phrase).
An illustrative input to MPD isgiven in Table 1 (the sentences in the third columnof the table are not a part of the input, and are onlygiven for the sake of clarity, though, of course, wouldnormally serve to deriving the instances by parsing).The output of the program is given in Table 2.MPD needs to find 10 pairwise contrasts between the5 classes (i.e.
N-choose-2, calculable by the formulaN(N-1)/2 ), and it has successfully discriminated all4 In a "discrimination model" each class is describedwith a unique profile.SBy way of an abstract example, denote features byF1...Fn, and let Class 1 have the profiles: (1) F1 F2,(2) F1 F3, and Class 2: (1) F4 F2, (2) F4 F5, (3) F4F6.
Combining freely all alternative profiles with oneanother, we should get 6 discrimination models.
How-ever, in Class 1 we have a choice between \[F2 F3\] (F1must be used), and in Class 2 between \[F2 F5 F6\] (F4must be used); this criterion, quite analogously to theprevious two, will minimize this choice, selecting F2 inboth cases, and hence yield the unique model Class 1:F1 F2, and Class 2:F4 F2.1035Classes1.otglezdam2.xranja3.xranja-se4.zaxranvam5.podavamI ns tances1.
NP1--hum VTR NP2=beast ~PP2.
NP l=hum VTR NP2=beast~PP1.
NP l=hum VTR NP2=hum~PP2.
NP1---beast VTR NP2=beast ~PPI.
NPl-----beast ~VTR PP2.
NP l=beast  ~VTR PPI.
NP l - -hum VTR NP2----phys-obj PP2.
NP l - -hum VTR NP2=phys-obj PP1.
NPl=phys*obj VTR NP2=phys-obj PP2.
NPl=phys*obj VTR NP2=phys-obj PP3.
NP l=hum VTR NP2=phys-ob i PPI l l us t ra t ions1.He feeds pigs2.
Jane feeds cat t lel .Nurses feed invalids2.Wi ld animals feed theircubs regularlyl .Horses feed on gr~ss2.Cows feed on hayl .Farmers  feed corn to fowls2.This family feeds meatto their  dogl ,The production line feedscloth in the machine2.The trace feeds paperto the printer3.Jim feeds coal to afurnaceTable 1: Classes and InstancesClasses1.otg lezdam2.xranja3.xranja-se4.zaxranvam5.podavamProfiles~PP NPlxNP2={{hum beast\])~PP NPlxNP2=(\[hum hum\] V \[beast beast\])NP lfbeast PPNPl=hum PP66.6% NP1--phys-ob~ PPTable 2: Classes and their Profilesclasses.
This is done by the overall feature set {NP1,PP, NPlxNP2}, whose first two features are primi-tive, and the third is a derived nominal feature.
Notall classes are absolutely discriminated: Class 4 (za-xranvam) and Class 5 (podavam) are only partiallycontrasted by the feature NP1.
Thus, Class 5 is66.6% NPl=phys-obj since we need to retract 1/3of its instances (particularly, sentence (3) from Ta-ble 1 whose NPl=hum) in order to get a clean con-trast by that feature.
Class 1 (otglezdam) and Class2 (xranja) use in their profiles the derived nominalfeature NPlxNP2; they actually contrast because allinstances of Class 1 have the value 'hum' for NP1and the value 'beast' for NP2, and hence the "de-rived value" \[hum beast\], whereas neither of the in-stances of Class 2 has an identical derived value (in-deed, referring to Table 1, the first instance of Class2 has NPlxNP2=\[hum hum\] and the second instanceNPlxNP2=\[beast beast\]).
The resulting profiles inTable 2 is the simplest in the sense that there areno more concise overall feature sets that discrimi-nate the classes, and the profiles--using only fea-tures from the overall feature set--are the shortest.3 Component ia l  ana lys i s3.1 In lexlcologyOne of the tasks we addressed with MPD is se-mantic omponential nalysis, which has well-knownlinguistic implications, e.g., for (machine) trans-lation (for a familiar early reference, cf.
Nida,1971).
More specifically, we were concerned withthe componential nalysis of kinship terminologies,a common area of study within this trend.
KIN-SHIP is a specialized computer program, having asinput the kinterms (=classes) of a language, andtheir attendant kintypes (=instances).
6 It com-putes the feature values of the kintypes, and thenfeeds the result to the MPD component to makethe discrimination between the kinterms of the lan-guage.
Currently, KINSHIP uses about 30 features,of all types: binary (e.g., male={+/-}),  nominal(e.g., lineal={lineal, co-lineal, ablineal}), and nu-meric (e.g., generation={1,2,..,n}).In the long history of this area of study, prac-titioners of the art have come up with explicit re-quirements as regards the adequacy of analysis: (1)Parsimony, including both overall features and kin-term descriptions (=profiles).
(2) Conjunctivenessof kinterm descriptions.
(3) Comprehensiveness indisplaying all alternative componential models.As seen, these requirements fit nicely with mostof the capabilities of MPD.
This is not accidental,since, historically, we started our investigations byautomating the important discovery task of com-ponential analysis, and then, realizing the genericnature of the discrimination subtask, isolated thispart of the program, which was later extended withthe mechanisms for derived features and partial con-trasts.Some of the results of KINSHIP are worth sum-marizing.
The program has so far been applied tomore than 20 languages of different language fami-lies.
In some cases, the datasets were partial (onlyconsanguineal, or blood) kin systems, but in oth-ers they were complete systems comprising 40-50classes with several hundreds of instances.
The pro-gram has re-discovered some classical analyses (ofthe Amerindian language Seneca by Lounsbury),has successfully analyzed previously unanalyzed lan-guages (e.g., Bulgarian), and has improved on pre-vious analyses of English.
For English, the mostparsimonious model has been found, and the onlyone giving conjunctive class profiles for all kinterms,which sounds impressive considering the massive f-forts concentrated on analyzing the English kinship6Examples of English kinterms are lather, uncle, andof their respective kintypes are: Fa (father); FaBr (fa-ther's brother) MoBr (mother's brother) FaFaSo (fa-ther's father's on) and a dozen of others.1036system.
7Most importantly, MPD has shown that the hugenumber of potential componential (-discrimination)models--a menace to the very foundations of theapproach, which has made some linguists proposealternative analytic tools-- are in fact reduced to(nearly) unique analyses by our 3 simplicity crite-ria.
Our 3rd criterion, ensuring the coordination be-tween equally simple alternative profiles, and withno precedence in the linguistic literature, proved es-sential in the pruning of solutions (details of KIN-SHIP are reported in Pericliev and Vald&-P@rez,1997; Pericliev and Vald~s-P~rez, forthcoming).3.2 In phonologyComponential analysis in phonology amounts tofinding the distinctive features of a phonemic sys-tem, differentiating any phoneme from all the rest.The adequacy requirements are the same as in theabove subsection, and indeed they have been bor-rowed in lexicology (and morphology for that mat-ter) from phonological work which chronologicallypreceded the former.
We applied MPD to the Rus-sian phonemic system, the data coming from a paperby Cherry et.
al., 1953, who also explicitly state asone of their goals the finding of minimal phonemedescriptions.The data consisted of 42 Russian phonemes, i.e.the transfer of feature values from instances (=allo-phones) to their respective classes (--phonemes) hasbeen previously performed.
The phonemes were de-scribed in terms of the following 11 binary features:(1) vocalic, (2) consonantal, (3) compact, (4) dif-fuse, (5) grave, (6) nasal, (7) continuant, (8) voiced,(9) sharp, (10) strident, (11) stressed.
MPD con-firmed that the 11 primitive overall features are in-deed needed, but it found 11 simpler phoneme pro-files than those proposed in this classic article (cf.Table 3).
Thus, the average phoneme profile turnsout to comprise 6.14, rather than 6.5, componentsas suggested by Cherry et.
al.The capability of MPD to treat not just binary,but also non-binary (nominal) features, it should benoted, makes it applicable to datasets of a newertrend in phonology which are not limited to us-ing binary features, and instead exploit multivaluedsymbolic features as legitimate phonological build-ing blocks.4 Language typo logyWe have used MPD for discovery of linguistic ty-pologies, where the classes to be contrasted are in-dividual anguages or groups of languages (languagefamilies).7We also found errors in analyses performed by lin-guists, which is understandable for a computationallycomplex task like this.Classes  I 2 3 4 5 6 7 8 9 I0  I Ik - -  + + +k --  + + + +g - + + + - + -a + + + - + +x - + + + +C l + + - -  I- + + - + -- + + - + +t - + -t - + - + -d - -  + - -  - -  - -  + - -d + -- -- -- + +, - + - +s - + - + - +z - + - - + + -z - + - - + + +- + - +n -- + -- -- + --n - + - - + +p - + - +p - + - + +b -- + -- + -- + --b + - + - + +f - + - + +f - + - + + - +v - + - + + + -v - + - + + + +m - + - + + -m --  + - + + +'u  + + +u + + +' o  + +'e +' i  + + --i + + -' a  + - ++ - +r + + - -r + + - +1 + + + -I + + + +JTable 3: Russian phonemes and their profilesIn one application, MPD was run on the datasetfrom the seminal paper by Greenberg (1966) on wordorder universals.
This corpus has previously beenused to uncover linguistic universals, or similarities;we now show its feasibility for the second fundamen-tal typological task of expressing the differences be-tween languages.
The data consist of a sample of 30languages with a wide genetic and areal coverage.The 30 classes to be differentiated are described interms of 15 features, 4of which are nominal, and theremaining 11 binary.
Running MPD on this datasetshowed that from 435 (30-Choose-2) pairwise dis-criminations to be made, just 12 turned out to beimpossible, viz.
the pairs:(berber,zapotec), (berber,welsh)(berber,hebrew), (fulani,swahili)(greek,serbian), (greek,maya)(hebrew,zapotec), (japanese,turkish)(japanese,kannada), (kannada,turkish)(malay,yoruba), (maya,serbian)The contrasts (uniquely) were made with a minimalset of 8 features: {SubjVerbObj-order, Adj < N,Genitive < N, Demonstrative < N, Numeral < N,Aux < V, Adv < Adj, affixation}.In the processed ataset, for a number of lan-guages there were missing values, esp.
for features1037(12) through (14).
The linguistic reasons for thiswere two-fold: (i) lack of reliable information; or (ii)non-applicability of the feature for a specific lan-guage (e.g., many languages lack particles for ex-pressing yes-no questions, i.e.
feature (12)).
Theabove results reflect our default treatment of miss-ing values as making no contribution to the contrastof language pairs.
Following the other alternativepath, and allowing 'missing' as a distinct value, willresult in the successful discrimination of most lan-guage pairs.
Greek and Serbian would remain in-discriminable, which is no surprise given their arealand genetic affinity.5 Speech production in aphasicsThis application concerns the discrimination of dif-ferent forms of aphasia on the basis of their languagebehaviour.SWe addressed the profiling of aphasic patients, us-ing the CAP dataset from the CHILDES database(MacWhinney, 1995), containing (among others) 22English subjects; 5 are control and the others sufferfrom anomia (3 patients), Broca's disorder (6), Wer-nicke's disorder (5), and nonfluents (3).
The patientsare grouped into classes according to their fit to aprototype used by neurologists and speech pathol-ogists.
The patients' records--verbal responses topictorial stimuli--are transcribed in the CHILDESdatabase and are coded with linguistic errors froman available set that pertains to phonology, morphol-ogy, syntax and semantics.As a first step in our study, we attempted to pro-file the classes using just the errors as they werecoded in the transcripts, which consisted of a set of26 binary features, based on the occurrence or non-occurrence of an error (feature) in the transcript ofeach patient.
We ran MPD with primitive featuresand absolute contrasts and found that from a total of10 pairwise contrasts to be made between 5classes, 7were impossible, and only 3 possible.
We then usedderived features and absolute contrasts, but still onepair (Broca's and Wernicke's patients) remained un-contrasted.
We obtained 80 simplest models with 5features (two primitive and three derived) discrimi-nating the four remaining classes.We found this profiling unsatisfactory from a do-main point of view for several reasons 9 which led usSWe are grateful to Prof. Brian MacWhinney fromthe Psychology Dpt.
of CMU for helpful discussions onthis application of MPD.
?First, one pair remained uncontrasted.
Second, only3 pairwise contrasts were made with absolute primitivefeatures, which are as a rule most intuitively acceptableas regards the comprehensibility of the demarcations (inthis specific case they correspond to "standard" errors,priorly and independently identified from the task underconsideration).
And, third, some of the derived featuresnecessary for the profiling lacked the necessary plausibil-ClassesControlSubjectsAnomicSubjectsBroc&PsSubjectsWernicke'sSubjectsNon fluentSubjectsProfi lessverage errors=\[O, 1.3\]average errors--\[l.7, 4.6\]prolixity--J7, 7.5\]fluency~fluency87% ~semi-intelligibleprolixity=\[12, 30.1\]fluency~fluencysemi-intelli$ibleTable 4: Profiles of Aphasic Patients with AbsoluteFeatures and Partial Contraststo re-examining the transcripts (amounting roughlyto 80 pages of written text) and adding manuallysome new features that could eventually result inmore intelligible profiling.
These included:(1) Prolixity.
This feature is intended to simu-late an aspect of the Grice's maxim of manner, viz.
"Avoid unnecessary prolixity".
We try to modelit by computing the average number of words pro-nounced per individual pictorial stimulus, so eachpatient is assigned a number (at present, each word-like speech segment is taken into account).
Wer-nicke's patients eem most prolix, in general.
(2) Truthfulness.
This feature attempts to sim-ulate Grices' Maxim of Quality: "Be truthful.
Donot say that for which you lack adequate vidence".Wernicke's patients are most persistent in violatingthis maxim by fabricating things not seen in the pic-torial stimuli.
All other patients eem to conform tothe maxim, except the nonfluents whose speech isdifficult to characterize either way (so this feature isconsidered irrelevant for contrasting).
(3) Fluency.
By this we mean general fluency, nor-mal intonation contour, absence of many and longpauses, etc.
The Broca's and non-fluent patientshave negative value for this feature, in contrast oall others.
(4) Average number of errors.
This is the sec-ond numerical feature, besides prolixity.
It countsthe average number of errors per individual stimu-lus (picture).
Included are all coder's markings inthe patient's text, some explicitly marked as errors,others being pauses, retracings, etc.Re-running MPD with absolute primitive featureson the new data, now having more than 30 fea-tures, resulted in 9 successful demarcations out of 10.Two sets of primitive features were used to this end:{average rrors, fluency, prolixity} and {average r-rors, fluency, truthfulness}.
The Broca's patientsand the nonfluent ones, which still resisted iscrim-ination, could be successfully handled with nine al-ternative derived Boolean features, formed from dif-ferent combinations of the coded errors (a handfulof which are also plausible).
We also ran MPD withprimitive features and partial contrasts (cf.
Table 4).Retracting one of the six Broca's subjects allows allity for domain scientists.1038classes to be completely discriminated.These results may be considered satisfactory fromthe point of view of aphasiology.
First of all, nowall disorders are successfully discriminated, mostcleanly, and this is done with the primitive features,which, furthermore, make good sense to domain spe-cialists: control subjects are singled out by the leastnumber of mistakes they make, Wernicke's patientsare contrasted from anomic ones by their greaterprolixity, anomics contrast Broca's and nonfluentpatients by their fluent speech, etc.6 MPD in  the  context  o f  d iverseapp l i ca t ion  typesA learning program can profitably be viewed alongtwo dimensions: (1) according to whether the outputof the program is addressed to a human or servesas input to another program; and (2) according towhether the program is used for prediction of futureinstances or not.
This yields four alternatives:type (i) (+human/-prediction),type (ii) (+human/+prediction),type (iii) (-human/+prediction), andtype (iv) (-human/-prediction).We may now summarize MPD's mechanisms inthe context of the diverse application types.
Theseobservations will clear up some of the discussion inthe previous ections, and may also serve as guide-lines in further specific applications of the program.Componential analysis falls under type (i):a componential model is addressed to a lin-guist/anthropologist, and there is no prediction ofunseen instances, since all instances (e.g., kintypesin kinship analysis) are as a rule available at theoutset.
10The aphasics discrimination task can be classedas type (ii): the discrimination model aims to makesense to a speech pathologist, but it should also havegood predictive power in assigning future patients tothe proper class of disorder.Learning translational equivalents from verbalcase frames belongs to type (iii) since the output ofthe learner will normally be fed to other subroutinesand this output model should make good predictionsas to word selection in the target language, encoun-tering future sentences in the source language.We did not discuss here a case of type (iv), so wejust mention an example.
Given a grammar G, thelearner should find "look-aheads", specifying whichof the rules of G should be fired firstJ 1 In this task,l?We note that componential nalysis in phonologycan alternatively be viewed of type (iii) if its ultimategoal is speech recognition.llA trivial example is G, having rules: (i) sl--+np, vp,\['2\] ; (ii) s2-~vp, \['!
'\] ; (iii) s3-~aux, np, v, \['?
'\], wherethe classes are the LHS, the instances are the RHS, andthe profiling should decide which of the 3 rules to usethe output of the learner can be automatically in-corporated as an additional rule in G (an hence beof no direct human use), and it should make no pre-dictions since it applies to the specific G, and not toany other grammar.For tasks of types (i) and (ii), a typical scenarioof using MPD would be:Using all 3 simplicity criteria, and find-ing all alternative models, follow the fea-ture/contrast hierarchy: primitive fea-tures & absolute contrasts > derived &absolute > primitive & partial > derived& partialwhich reflects the desiderata of conciseness, compre-hensiveness, and intelligibility (as far as the latteris concerned, the primitive features (normally user-supplied) are preferable to the computer-invented,possibly disjunctive, derived features).However, in some specific tasks, another hierarchyseems preferable, which the user is free to follow.E.g., in kinship under type (i), the inability of MPDto completely discriminate the kinterms may verywell be due to noise in the instances, a situationby no means infrequent, esp.
in data for "exotic"languages.
In a type (ii) task, an analogous situationmay hold (e.g., a patient may be erroneously classedunder some impairment), all this leading to tryingfirst the primitive & partial heuristic.
There may beother reasons to change the order of heuristics in thehierarchy as well.We see no clear difference between types (i)-(ii)tasks, placing the emphasis in (ii) on the human ad-dressee subtask rather than on prediction subtask,because it is not unreasonable tosuppose that a con-cise and intelligible model has good chances of rea-sonably high predictive power.
12We have less experience in applying MPD on tasksof types (iii) and (iv) and would therefore refrainfrom suggesting typical scenarios for these types.
Weoffer instead some observations on the role of MPD'smechanisms in the context of such tasks, showing atsome places their different meaning/implication incomparison with the previous two tasks:(1) Parsimony, conceived as a minimality of classprofiles, is essential in that it generally contributes toreducing the cost of assigning an incoming instanceto a class.
(In contrast o tasks of types (i)-(ii), theMaximize-Coordination criterion has no clear mean-ing here, and the Minimize-Features may well behaving as input say Come here/.12By way of a (non-linguistic) illustration, we haveturned the MPD profiles into classification rules and havecarried out an initial experiment on the LED-24 datasetfrom the UC Irvine repository.
MPD classified 1000 un-seen instances at 73 per cent, using five features, whichcompares well with a seven features classifier eportedin the literature, as well as with other citations in therepository entry.1039sacrificed in order to get shorter profiles).
13(2) Conjunctiveness i  of less importance herethan in tasks of type (i)-(ii), but a better legibil-ity of profiles is in any case preferable.
The derivedfeatures mechanism can be essential in achieving in-tuitive contrasts, as in verbal case frame learning,where the interaction between features nicely fits thetask of learning "slot dependencies" (Li and Abe,1996).
(3) All alternative profiles of equal simplicity arenot always a necessity as in tasks of type (i)-(ii), butare most essential in many tasks where there are dif-ferent costs of finding the feature values of unseeninstances (e.g., computing a syntactic feature, gen-erally, would be much less expensive than computingsay a pragmatic one).The important point to emphasize here is thatMPD generally leaves these mechanisms as programparameters tobe set by the user, and thus, by chang-ing its inductive bias, it may be tailored to the spe-cific needs that arise within the 4 types of tasks.7 Conc lus ionThe basic contributions of this paper are: (1) to in-troduce anovel flexible multi-class learning program,MPD, that emphasizes the conciseness and intelligi-bility of the class descriptions; (2) to show some usesof MPD in diverse linguistic fields, at the same timeindicating some prospective modes of using the pro-gram in the different application types; and (3) todescribe substantial results that employed the pro-gram.A basic limitation of MPD is of course its inabilityto handle inherently disjunctive concepts, and thereare indeed various tasks of this sort.
Also, despiteits efficient implementation, the user may sometimesbe forced to sacrifice conciseness (e.g., choose twoprimitive features instead of just one derived thatcan validly replace them) in order to evade combi-natorial problems.
Nevertheless in our experiencewith linguistic (and not only linguistic) tasks MPDhas proved a successful tool for solving significantpractical problems.
As far as our ongoing researchis concerned, we basically are focussing on findingnovel application areas.Acknowledgments.
This work was supported by agrant #IRI-9421656 from the (USA) National Sci-ence Foundation and by the NSF Division of Inter-national Programs.13E.g., instead of the profile \[xranja-se: NPl=beastPP\] in Table 2, one may choose the valid shorter profile\[xranja-se: -~VTR\], even though that would increase thenumber of overall features used.Re ferencesC.
Cherry, M. Halle, and R, Jakobson.
1953.
To-ward the logical description of languages in theirphonemic aspects.
Language 29:34-47.W.
Daelemans, P. Berck, and S. Gillis.
1996.
Un-supervised iscovery of phonological categoriesthrough supervised learning of morphologicalrules.
COLING96, Copenhagen, pages 95-100.J.
Bruner, J. Goodnow, and G. Austin.
1956.
AStudy of Thinking.
John Wiley, New York.J.
Greenberg.
1966.
Some universals of grammarwith particular eference to the order of meaning-ful elements.
In J. Greenberg, ed.
Universals ofLanguage, MIT Press, Cambridge, Mass.C.
Hempel.
1965.
Aspects of Scientific Explanation.The Free Press, New York.P.
Langley, H. Simon, G. Bradshaw, and J, Zytkow.1987.
Scientific Discovery: Computational Explo-rations of the Creative Process.
The MIT Press,Cambridge, Mass.Hang Li and Naoki Abe.
1996.
Learning depen-dencies between case frame slots.
COLING96,Copenhagen, pages 10-15.B.
MacWhinney.
1995.
The CHILDES Project:Tools for Analyzing Talk.
Lawrence Erlbaum, N.J.E.
Nida.
1971.
Semantic omponents in translationtheory.
In G. Perren and J.
Trim (eds.)
Appli-cations of Linguistics, pages 341-348.
CambridgeUniversity Press, Cambridge, England.V.
Pericliev and R. E. Vald~s-P~rez.
1997.
A dis-covery system for componential nalysis of kin-ship terminologies.
In B. Caron (ed.)
16th Inter-national Congress of Linguists, Paris, July 1997,Elsevier.V.
Pericliev and R. E. Vald~s-P~rez.
forthcoming.Automatic componential nalysis of kinship se-mantics with a proposed structural solution to theproblem of multiple models.
Anthropological Lin-guistics.J.
R. Quinlan.
1986.
Induction of decision trees.Machine Learning, 1:81-106.J.
R. Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann.H.Tanaka.
1996.
Decision tree learning algorithmwith structured attributes: Application to verbalcase frame acquisition.
COLING96, Copenhagen,pages 943-948.R.
E. Vald~s-P~rez and V. Pericliev.
1997.
Maxi-mally parsimonious discrimination: a task fromlinguistic discovery.
AAAI97, Providence, RI,pages 515-520.R.
E. Vald~s-P~rez and V. Pericliev.
1998.
Concise,intelligible, and approximate profiling of numer-ous classes.
Submitted for publication.1040
