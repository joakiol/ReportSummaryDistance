On the Applicability of Global Index GrammarsJose?
M. Castan?oComputer Science DepartmentBrandeis Universityjcastano@cs.brandeis.eduAbstractWe investigate Global Index Gram-mars (GIGs), a grammar formalismthat uses a stack of indices associatedwith productions and has restrictedcontext-sensitive power.
We discusssome of the structural descriptionsthat GIGs can generate compared withthose generated by LIGs.
We showalso how GIGs can represent structuraldescriptions corresponding to HPSGs(Pollard and Sag, 1994) schemas.1 IntroductionThe notion of Mildly context-sensitivity was in-troduced in (Joshi, 1985) as a possible modelto express the required properties of formalismsthat might describe Natural Language (NL)phenomena.
It requires three properties:1 a)constant growth property (or the stronger semi-linearity property); b) polynomial parsability;c) limited cross-serial dependencies, i.e.
somelimited context-sensitivity.
The canonical NLproblems which exceed context free power are:multiple agreements, reduplication, crossing de-pendencies.2Mildly Context-sensitive Languages (MCSLs)have been characterized by a geometric hierar-chy of grammar levels.
A level-2 MCSL (eg.1See for example, (Joshi et al, 1991), (Weir, 1988).2However other phenomena (e.g.
scrambling, Geor-gian Case and Chinese numbers) might be considered tobe beyond certain mildly context-sensitive formalisms.TALs/LILs) is able to capture up to 4 countingdependencies (includes L4 = {anbncndn |n ?
1}but not L5 = {anbncndnen |n ?
1}).
They wereproven to have recognition algorithms with timecomplexity O(n6 ) (Satta, 1994).
In general fora level-k MCSL the recognition problem is inO(n3 ?2 k?1 ) and the descriptive power regard-ing counting dependencies is 2k (Weir, 1988).Even the descriptive power of level-2 MCSLs(Tree Adjoining Grammars (TAGs), Linear In-dexed Grammars (LIGs), Combinatory Catego-rial Grammars (CCGs) might be considered in-sufficient for some NL problems, therefore therehave been many proposals3 to extend or modifythem.
On our view the possibility of modelingcoordination phenomena is probably the mostcrucial in this respect.In (Castan?o, 2003) we introduced Global In-dex Grammars (GIGs) - and GILs the corre-sponding languages - as an alternative grammarformalism that has a restricted context sensitivepower.
We showed that GIGs have enough de-scriptive power to capture the three phenomenamentioned above (reduplication, multiple agree-ments, crossed agreements) in their generalizedforms.
Recognition of the language generated bya GIG is in bounded polynomial time: O(n6 ).We presented a Chomsky-Schu?tzenberger repre-sentation theorem for GILs.
In (Castan?o, 2003c)we presented the equivalent automaton model:LR-2PDA and provided a characterization the-3There are extensions or modifications of TAGs,CCGs, IGs, and many other proposals that would beimpossible to mention here.orems of GILs in terms of the LR-2PDA andGIGs.
The family of GILs is an Abstract Fam-ily of Language.The goal of this paper is to show the relevanceof GIGs for NL modeling and processing.
Thisshould not be understood as claim to proposeGIGs as a grammar model with ?linguistic con-tent?
that competes with grammar models suchas HPSG or LFG.
It should be rather seen asa formal language resource which can be usedto model and process NL phenomena beyondcontext free, or beyond the level-2 MCSLs (likethose mentioned above) or to compile grammarscreated in other framework into GIGs.
LIGsplayed a similar role to model the treatment ofthe SLASH feature in GPSGs and HPSGs, andto compile TAGs for parsing.
GIGs offer addi-tional descriptive power as compared to LIGsor TAGs regarding the canonical NL problemsmentioned above, and the same computationalcost in terms of asymptotic complexity.
Theyalso offer additional descriptive power in termsof the structural descriptions they can generatefor the same set of string languages, being ableto produce dependent paths.4This paper is organized as follows: section 2reviews Global Index Grammars and their prop-erties and we give examples of its weak descrip-tive power.
Section 3 discusses the relevanceof the strong descriptive power of GIGs.
Wediscuss the structural description for the palin-drome, copy and the multiple copies languages{ww+|w ?
??}.
Finally in section 4 we discusshow this descriptive power can be used to en-code HPSGs schemata.2 Global Index Grammars2.1 Linear Indexed GrammarsIndexed grammars, (IGs) (Aho, 1968), andLinear Index Grammars, (LIGs;LILs) (Gazdar,1988), have the capability to associate stacks ofindices with symbols in the grammar rules.
IGsare not semilinear.
LIGs are Indexed Grammarswith an additional constraint in the form of theproductions: the stack of indices can be ?trans-4For the notion of dependent paths see for instance(Vijay-Shanker et al, 1987) or (Joshi, 2000).mitted?
only to one non-terminal.
As a con-sequence they are semilinear and belong to theclass of MCSGs.
The class of LILs contains L4but not L5 (see above).A Linear Indexed Grammar is a 5-tuple(V, T, I, P, S), where V is the set of variables,T the set of terminals, I the set of indices, Sin V is the start symbol, and P is a finite setof productions of the form, where A,B ?
V ,?, ?
?
(V ?
T )?, i ?
I:a.
A[..] ?
?
B[..] ?
b.
A[i..] ?
?
B[..] ?c.
A[..] ?
?B[i..] ?Example 1 L(Gwcw ) = {wcw |w ?
{a, b}?
},Gww = ({S,R}, {a, b}, {i, j}, S, P ) and P is:1.S[..] ?
aS[i..] 2.S[..] ?
bS[j..]3.S[..] ?
cR[..] 4.R[i..] ?
R[..]a5.R[j..] ?
R[..]b 6.
R[] ?
?2.2 Global Indexed GrammarsGIGs use the stack of indices as a global con-trol structure.
This formalism provides a globalbut restricted context that can be updated atany local point in the derivation.
GIGs are akind of regulated rewriting mechanisms (Dassowand Pa?un, 1989) with global context and his-tory of the derivation (or ordered derivation) asthe main characteristics of its regulating device.The introduction of indices in the derivation isrestricted to rules that have terminals in theright-hand side.
An additional constraint thatis imposed on GIGs is strict leftmost derivationwhenever indices are introduced or removed bythe derivation.Definition 1 A GIG is a 6-tuple G =(N,T, I, S,#, P ) where N,T, I are finite pair-wise disjoint sets and 1) N are non-terminals2) T are terminals 3) I a set of stack indices 4)S ?
N is the start symbol 5) # is the start stacksymbol (not in I,N ,T ) and 6) P is a finite set ofproductions, having the following form,5 where5The notation in the rules makes explicit that oper-ation on the stack is associated to the production andneither to terminals nor to non-terminals.
It also makesexplicit that the operations are associated to the com-putation of a Dyck language (using such notation asused in e.g.
(Harrison, 1978)).
In another notation: a.1[y..]A ?
[y..]?, a.2 [y..]A ?
[y..]?, b.
[..]A ?
[x..]a ?and c. [x..]A ?
[..]?x ?
I, y ?
{I ?#}, A ?
N , ?, ?
?
(N ?T )?
anda ?
T .a.i A ??
?
(epsilon)a.ii A ?[y]?
(epsilon with constraints)b.
A ?x a ?
(push)c. A ?
?x ?
a ?
(pop)Note the difference between push (type b) andpop rules (type c): push rules require the right-hand side of the rule to contain a terminal in thefirst position.
Pop rules do not require a termi-nal at all.
That constraint on push rules is acrucial property of GIGs.
Derivations in a GIGare similar to those in a CFG except that it ispossible to modify a string of indices.
We de-fine the derives relation ?
on sentential forms,which are strings in I?#(N ?T )?
as follows.
Let?
and ?
be in (N ?
T )?, ?
be in I?, x in I, w bein T ?
and X i in (N ?
T ).1.
If A ??
X1 ...Xn is a production of type (a.)(i.e.
?
= ?
or ?
= [x], x ?
I) then:i.
?#?A?
??
?#?X1 ...Xn?ii.
x?#?A?
??
x?#?X1 ...Xn?2.
If A ??
aX1 ...Xn is a production of type(b.)
or push: ?
= x, x ?
I, then:?#wA?
??
x?#waX1 ...Xn?3.
If A ??
X1 ...Xn is a production of type (c.)or pop : ?
= x?, x ?
I, then:x?#wA?
??
?#wX1 ......Xn?The reflexive and transitive closure of ?
isdenoted, as usual by ??.
We define the languageof a GIG, G, L(G) to be: {w|#S ??
#w and wis in T ?
}The main difference between, IGs, LIGs andGIGs, corresponds to the interpretation of thederives relation relative to the behavior of thestack of indices.
In IGs the stacks of indices aredistributed over the non-terminals of the right-hand side of the rule.
In LIGs, indices are asso-ciated with only one non-terminal at right-handside of the rule.
This produces the effect thatthere is only one stack affected at each deriva-tion step, with the consequence of the semilin-earity property of LILs.
GIGs share this unique-ness of the stack with LIGs: there is only onestack to be considered.
Unlike LIGs and IGs thestack of indices is independent of non-terminalsin the GIG case.
GIGs can have rules where theright-hand side of the rule is composed only ofterminals and affect the stack of indices.
Indeedpush rules (type b) are constrained to start theright-hand side with a terminal as specified in(6.b) in the GIG definition.
The derives def-inition requires a leftmost derivation for thoserules ( push and pop rules) that affect the stackof indices.
The constraint imposed on the pushproductions can be seen as constraining the con-text sensitive dependencies to the introductionof lexical information.
This constraint preventsGIGs from being equivalent to a Turing Machineas is shown in (Castan?o, 2003c).2.2.1 ExamplesThe following example shows that GILs con-tain a language not contained in LILs, nor in thefamily of MCSLs.
This language is relevant formodeling coordination in NL.Example 2 (Multiple Copies) .L(Gwwn) = {ww+ | w ?
{a, b}?
}Gwwn = ({S,R,A,B,C, L}, {a, b}, {i, j}, S,#, P )and where P is: S ?
AS | BS | C C ?
RC | LR ?
?iRA R ?
?jRB R ?
[#]?A ?ia B ?jb L ?
?iLa | a L ?
?jLb | bThe derivation of ababab:#S ?
#AS ?
i#aS ?
i#aBS ?
ji#abS ?ji#abC ?
ji#abRC ?
i#abRBC ?
#abRABC ?#abABC ?
i#abaBC ?
ji#ababC ?
ji#ababL ?i#ababLb ?
#abababThe next example shows the MIX (or Bach)language.
(Gazdar, 1988) conjectured the MIXlanguage is not an IL.
GILs are semilinear,(Castan?o, 2003c) therefore ILs and GILs couldbe incomparable under set inclusion.Example 3 (MIX language) .L(Gmix ) ={w|w ?
{a, b, c}?
and |a|w = |b|w = |c|w ?
1}Gmix = ({S,D, F, L}, {a, b, c}, {i, j, k, l,m, n}, S,#, P )where P is:S ?
FS | DS | LS | ?
F ?ic F ?jb F ?kaD ?
?iaSb | bSa D ?
?jaSc | cSa D ?
?kbSc | cSbD ?laSb | bSa D ?maSc | cSa D ?nbSc | cSbL ?
?lc L ?
?mb L ?
?naThe following example shows that the familyof GILs contains languages which do not belongto the MCSL family.Example 4 (Multiple dependencies)L(Ggdp) = { an(bncn)+ | n ?
1},Ggdp = ({S,A,R,E,O,L}, {a, b, c}, {i}, S,#, P )and P is:S ?
AR A ?
aAE A ?
a E ?ibR ?ib L L ?
OR | C C ?
?ic C | cO ?
?ic OE | cThe derivation of the string aabbccbbcc showsfive dependencies.#S ?
#AR ?
#aAER ?
#aaER ?
i#aabR ?ii#aabbL ?
ii#aabbOR ?
i#aabbcOER ?#aabbccER ?
i#aabbccbR ?
ii#aabbccbbL ?ii#aabbccbbC ?
i#aabbccbbcC ?
#aabbccbbcc2.3 GILs RecognitionThe recognition algorithm for GILs we presentedin (Castan?o, 2003) is an extension of Earley?s al-gorithm (cf.
(Earley, 1970)) for CFLs.
It has tobe modified to perform the computations of thestack of indices in a GIG.
In (Castan?o, 2003) agraph-structured stack (Tomita, 1987) was usedto efficiently represent ambiguous index opera-tions in a GIG stack.
Earley items are modifiedadding three parameters ?, c, o:[?, c,o, A ?
?
?A?, i, j]The first two represent a pointer to an activenode in the graph-structured stack ( ?
?
I andc ?
n).
The third parameter (o ?
n) is usedto record the ordering of the rules affecting thestack.The O(n6 ) time-complexity of this algorithmreported in (Castan?o, 2003) can be easily ver-ified.
The complete operation is typically thecostly one in an Earley type algorithm.
It canbe verified that there are at most n6 instances ofthe indices (c1 , c2 , o, i, k, j) involved in this oper-ation.
The counter parameters c1 and c2 , mightbe state bound, even for grammars with ambigu-ous indexing.
In such cases the time complex-ity would be determined by the CFG backboneproperties.
The computation of the operationson the graph-structured stack of indices are per-formed at a constant time where the constant isdetermined by the size of the index vocabulary.O(n6 ) is the worst case; O(n3 ) holds for gram-mars with state-bound indexing (which includesunambiguous indexing)6; O(n2 ) holds for unam-biguous context free back-bone grammars withstate-bound indexing and O(n) for bounded-state7 context free back-bone grammars withstate-bound indexing.3 GIGs and structural description(Gazdar, 1988) introduces Linear IndexedGrammars and discusses its applicability to Nat-ural Language problems.
This discussion is ad-dressed not in terms of weak generative capac-ity but in terms of strong-generative capacity.Similar approaches are also presented in (Vijay-Shanker et al, 1987) and (Joshi, 2000) (see(Miller, 1999) concerning weak and strong gen-erative capacity).
In this section we review someof the abstract configurations that are argued forin (Gazdar, 1988).3.1 The palindrome languageCFGs can recognize the language {wwR|w ???}
but they cannot generate the structural de-scription depicted in figure 1 (we follow Gazdar?snotation: the leftmost element within the brack-ets corresponds to the top of the stack):a[..] [a] [b,a] [c,b,a]bcd[d,c,b,a]dc[b,a]ba[a][..][c,b,a]Figure 1: A non context-free structural descrip-tion for the language wwR (Gazdar, 1988)Gazdar suggests that such configurationwould be necessary to represent Scandinavian6Unambiguous indexing should be understood asthose grammars that produce for each string in the lan-guage a unique indexing derivation.7Context Free grammars where the set of items in eachstate set is bounded by a constant.unbounded dependencies.Such an structure canbe obtained using a GIG (and of course a LIG).But the mirror image of that structure can-not be generated by a GIG because it wouldrequire to allow push productions with a nonterminal in the first position of the right-handside.
However the English adjective construc-tions that Gazdar argues that can motivate theLIG derivation, can be obtained with the follow-ing GIG productions as shown in figure 2.Example 5 (Comparative Construction) .AP ?
AP NP AP ?
A?
A?
?
A?
AA ?ia A ?jb A ?kcNP ?
?ia NP NP ?
?jb NP NP ?
?kc NPNPNPAAAA AAPAPAPAPNPA ba[a,b,c]a NPb NPNPcc[..][b,c][b,c][c][..][c][..]Figure 2: A GIG structural description for thelanguage wwRIt should be noted that the operations on indicesfollow the reverse order as in the LIG case.
Onthe other hand, it can be noticed also that theintroduction of indices is dependent on the pres-ence of lexical information and its transmissionis not carried through a top-down spine, as inthe LIG or TAG cases.
The arrows show theleftmost derivation order that is required by theoperations on the stack.3.2 The Copy LanguageGazdar presents two possible LIG structural de-scriptions for the copy language.
Similar struc-tural descriptions can be obtained using GIGs.However he argues that another tree structurecould be more appropriate for some NaturalLanguage phenomenon that might be modeledwith a copy language.
Such structure cannotbe generated by a LIG, and can by an IG (see(Castan?o, 2003b) for a complete discussion andcomparasion of GIG and LIG generated trees).GIGs cannot produce this structural descrip-tion, but they can generate the one presented infigure 3, where the arrows depict the leftmostderivation order.
GIGs can also produce similarstructural descriptions for the language of mul-tiple copies (the language {ww+| w ?
??}
asshown in figure 4, corresponding to the gram-mar shown in example 2.
[ ][ ]b[a]a [b,a][a]abcd[b,a]ab[a,b,a][b,a,b,a] [b,a,b,a][a,b,a]Figure 3: A GIG structural description for thecopy language[ ][ ][ ][ ][a]?
[a][a][c,b,a][b,a][b,a][b,a]a b[a] [b,a]ab?ab[b,a][a] [b,a]b[a]aba[a,b,a][b,a,b,a][a,b,a] [b,a,b,a] [b,a,b,a][b,a,b,a][a,b,a] [b,a,b,a] [b,a,b,a][a,b,a] [a,b,a]aba abbFigure 4: A GIG structural description for themultiple copy language4 GIGs and HPSGsWe showed in the last section how GIGs canproduce structural descriptions similar to thoseof LIGs, and others which are beyond LIGs andTAGs descriptive power.
Those structural de-scriptions corresponding to figure 1 were corre-lated to the use of the SLASH feature in GPSGsand HPSGs.
In this section we will show howthe structural description power of GIGs, is notonly able to capture those phenomena but alsoadditional structural descriptions, compatiblewith those generated by HPSGs.
This followsfrom the ability of GIGs to capture dependen-cies through different paths in the derivation.There has been some work compiling HPSGsinto TAGs (cf.
(Kasper et al, 1995), (Beckerand Lopez, 2000)).
One of the motivationswas the potential to improve the processingefficiency of HPSG, performing HPSG deriva-tions at compile time.
Such compilation processallowed to identify significant parts of HPSGgrammars that were mildly context-sensitive.We will introduce informally some slight mod-ifications to the operations on the stacks per-formed by a GIG.
We will allow the productionsof a GIG to be annotated with finite stringsin I ?
I?
instead of single symbols.
This doesnot change the power of the formalism.
It is astandard change in PDAs (cf.
(Harrison, 1978))to allow to push/pop several symbols from thestack.
Also the symbols will be interpreted rel-ative to the elements in the top of the stack(as a Dyck set).
Therefore different derivationsmight be produced using the same productionaccording to what are the topmost elements ofthe stack.
This is exemplified with the produc-tions X ?
?nv x and X ?
[n]v x, in particular in thefirst three cases where different actions are taken(the actions are explained in the parenthesis) :nn?#wX?
?
?nv vn?#wx?
(pop n and push v)nv??#wX?
?
?nv ?#wx?
(pop n and v?)vn?#wX?
?
?nv vn?vn?#wx?
(push n?
and v)n?#wX?
?[n]vvn?#wx?
( check and push)We exemplify how GIGs can generate similarstructural descriptions as HPSGs do, in a veryoversimplified and abstract way.
We will ignoremany details and try give an rough idea on howthe transmission of features can be carried outfrom the lexical items by the GIG stack, obtain-ing very similar structural descriptions.Head-Subj-SchemaFigure 5 depicts the tree structure corre-sponding to the Head-Subject Schema in HPSG(Pollard and Sag, 1994).HHEAD 12 HEAD< >SUBJSUBJSUBJ12< >Figure 5: Head-Subject SchemaFigure 6 shows an equivalent structural de-scription corresponding to the GIG produc-tions and derivation shown in the next exam-ple (which might correspond to an intransitiveverb).
The arrows indicate how the transmis-sion of features is encoded in the leftmost deriva-tion order, an how the elements contained in thestack can be correlated to constituents or lexicalitems (terminal symbols) in a constituent recog-nition process.xXXPXPYPYy[n..][n..][..][v..][v..][v..]Figure 6: Head-Subject in GIG formatExample 6 (Intransitive verb) XP ?
Y P XPXP ?
X Y P ?
Y X ?
?nvx Y ?ny#XP ?
#Y PXP ?
#yXP ?
n#Y XP ?n#yX ?
v#yxHead-Comps-Schema Figure 7 shows thetree structure corresponding to the Head-Complement schema in HPSG.HEAD1HEAD < 2 >H<      >13, nC C1 n-23 n2COMPCOMPFigure 7: Head-Comps Schema tree representa-tionThe following GIG productions generate thestructural description corresponding to figure 8,where the initial configuration of the stack isassumed to be [n]:Example 7 (transitive verb) .XP ?
X CP CP ?
Y CP X ?
?nvn?x CP ?
?Y ?nyThe derivation:n#XP ?
n#XCP ?
n?v#xCP ?
n?v#xY CP ?v#xyCP ?
v#xyCPXPXx CPYy[n][n v][n v]?
[ v ][ v ][ v ]Figure 8: Head-Comp in GIG formatThe productions of example 8 (which usesome of the previous examples) generate thestructural description represented in figure 9,corresponding to the derivation given in exam-ple 8.
We show the contents of the stack wheneach lexical item is introduced in the derivation.Example 8 (SLASH in GIG format) .XP ?
Y P XP XP ?
X CP XP ?
X XPCP ?
Y P CP X ?
?nvn?hates CP ?
?X ?
?nv?know X ?
?nvv?claimsY P ?nKim|Sandy|Dana|weA derivation of ?Kim we know Sandy claims Danahates?
:#XP ?
#Y P XP ?
n#Kim XP ?n#Kim Y P XP ?
nn#Kim we XP ?nn#Kim we X XP ?
v?n#Kim we know XP ?v?n#Kim we know Y P XP ?nv?n#Kim we know Sandy XP ?nv?n#Kim we know Sandy X XP ?v?n#Kim we know Sandy claims XP ?v?n#Kim we know Sandy claims Y P XP ?nv?n#Kim we know Sandy claims Dana XP ?
?#Kim we know Sandy claims Dana hatesFinally the last example and figure 10 showhow coordination can be encoded.Example 9 (SLASH and Coordination)XP ?
Y P XP XP ?
X CP XP ?
X XPCP ?
Y P CP CP ?
?
X ?
[nv?n]cvisitX ?
?nvn?talk to C ?
and CXP ?
XP CXPCXP ?
C XP X ?
?nv?did Y P ?nWho|you5 ConclusionsWe presented GIGs and GILs and showed thedescriptive power of GIGs is beyond CFGs.CFLs are properly included in GILs by def-inition.
We showed also that GIGs includeXYP XPXPXPXPXPXPXPXYPXYPYP [n][nn][ n v n ][ n v n ][ ]weknowSandyclaimsDanahatesKim?
[n]CP[ v n ][ v n ][ ]Figure 9: SLASH in GIG formatsome languages that are not in the LIL/TALfamily.
GILs do include those languages thatare beyond context free and might be requiredfor NL modelling.
The similarity between GIGsand LIGs, suggests that LILs might be includedin GILs.
We presented a succinct comparisonof the structural descriptions that can be gen-erated both by LIGs and GIGs, we have shownthat GIGs generate structural descriptions forthe copy language which can not be generatedby LIGs.
We showed also that this is thecase for other languages that can be generatedby both LIGs and GIGs.
This correspondsto the ability of GIGs to generate dependentpaths without copying the stack.
We haveshown also that those non-local relationshipsthat are usually encoded in HPSGs as featuretransmission, can be encoded in GIGs using itsstack, exploiting the ability of Global stacks toencode dependencies through dependent pathsand not only through a spine.Acknowledgments:Thanks to J. Pustejovsky for his continuous support andencouragement on this project.
Many thanks also to theanonymous reviewers who provided many helpful com-ments.
This work was partially supported by NLM Grant[ ]XPXPXPXPYP[nv]XdidWhoyouYPvisitCXPCXPandCXPtalk to[ n v n ]??
[ n v n][ ][n][ n v n ]CP[ c n v n ]Figure 10: SLASH in GIG formatR01 LM06649-02.ReferencesA.
V. Aho.
1968.
Indexed grammars - an extensionof context-free grammars.
Journal of the Associ-ation for Computing Machinery, 15(4):647?671.T.
Becker and P. Lopez.
2000.
Adapting hpsg-to-tagcompilation to wide-coverage grammars.J.
Castan?o.
2003.
GIGs: Restricted context-sensitive descriptive power in boundedpolynomial-time.
In Proc.
of Cicling 2003,Mexico City, February 16-22.J.
Castan?o.
2003b.
Global index grammars and de-scriptive power.
In R. Oehrle and J. Rogers, edi-tors, Proc.
of Mathematics of Language, MOL 8.Bloomington, Indiana, June.J.
Castan?o.
2003c.
LR Parsing for Global Index Lan-guages (GILs).
In In Proceeding of CIAA 2003,Santa Barbara,CA.J.
Dassow and G. Pa?un.
1989.
Regulated Rewrit-ing in Formal Language Theory.
Springer, Berlin,Heidelberg, New York.J.
Earley.
1970.
An Efficient Context-free ParsingAlgorithm.
Communications of the ACM, 13:94?102.G.
Gazdar.
1988.
Applicability of indexed grammarsto natural languages.
In U. Reyle and C. Rohrer,editors, Natural Language Parsing and LinguisticTheories, pages 69?94.
D. Reidel, Dordrecht.M.
H. Harrison.
1978.
Introduction to Formal Lan-guage Theory.
Addison-Wesley Publishing Com-pany, Inc., Reading, MA.A.
Joshi, K. Vijay-Shanker, and D. Weir.
1991.
Theconvergence of mildly context-sensitive grammat-ical formalisms.
In Peter Sells, Stuart Shieber,and Thomas Wasow, editors, Foundational issuesin natural language processing, pages 31?81.
MITPress, Cambridge, MA.A.
Joshi.
1985.
Tree adjoining grammars: How muchcontext-sensitivity is required to provide reason-able structural description?
In D. Dowty, L. Kart-tunen, and A. Zwicky, editors, Natural languageprocessing: psycholinguistic, computational andtheoretical perspectives, pages 206?250.
ChicagoUniversity Press, New York.A.
Joshi.
2000.
Relationship between strong andweak generative power of formal systems.
InProceedings of the Fifth International Workshopon Tree Adjoining Grammars and Related For-malisms (TAG+5), pages 107?114, Paris, France.R.
Kasper, B. Kiefer, K. Netter, and K. Vijay-Shanker.
1995.
Compilation of HPSG into TAG.In Proceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics, pages92?99.
Cambridge, Mass.P.
Miller.
1999.
Strong Generative Capacity.
CSLIPublications, Stanford University, Stanford CA,USA.C.
Pollard and I.
A.
Sag.
1994.
Head-driven PhraseStructure Grammar.
University of Chicago Press,Chicago, IL.G.
Satta.
1994.
Tree-adjoining grammar parsing andboolean matrix multiplication.
Computational lin-guistics, 20, No.
2.M.
Tomita.
1987.
An efficiente augmented-context-free parsing algorithm.
Computational linguistics,13:31?46.K.
Vijay-Shanker, D. J. Weir, and A. K. Joshi.
1987.Characterizing structural descriptions producedby various grammatical formalisms.
In Proc.
ofthe 25th ACL, pages 104?111, Stanford, CA.D.
Weir.
1988.
Characterizing mildly context-sensitive grammar formalisms.
Ph.D. thesis, Uni-versity of Pennsylvania.
