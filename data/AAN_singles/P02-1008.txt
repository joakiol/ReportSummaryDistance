Comprehension and Compilation in Optimality Theory?Jason EisnerDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, MD, USA 21218-2691jason@cs.jhu.eduAbstractThis paper ties up some loose ends in finite-state OptimalityTheory.
First, it discusses how to perform comprehension un-der Optimality Theory grammars consisting of finite-state con-straints.
Comprehension has not been much studied in OT; weshow that unlike production, it does not always yield a regularset, making finite-state methods inapplicable.
However, aftergiving a suitably flexible presentation of OT, we show care-fully how to treat comprehension under recent variants of OTin which grammars can be compiled into finite-state transduc-ers.
We then unify these variants, showing that compilation ispossible if all components of the grammar are regular relations,including the harmony ordering on scored candidates.
A sidebenefit of our construction is a far simpler implementation ofdirectional OT (Eisner, 2000).1 IntroductionTo produce language is to convert utterances fromtheir underlying (?deep?)
form to a surface form.Optimality Theory or OT (Prince and Smolensky,1993) proposes to describe phonological productionas an optimization process.
For an underlying x,a speaker purportedly chooses the surface form zso as to maximize the harmony of the pair (x, z).Broadly speaking, (x, z) is harmonic if z is ?easy?to pronounce and ?similar?
to x.
But the precise har-mony measure depends on the language; accordingto OT, it can be specified by a grammar of rankeddesiderata known as constraints.According to OT, then, production maps each un-derlying form to its best possible surface pronuncia-tion.
It is akin to the function that maps each child xto his or her most flattering outfit z.
Different chil-dren look best in different clothes, and for an oddlyshaped child x, even the best conceivable outfit zmay be an awkward compromise between style andfit?that is, between ease of pronunciation and sim-ilarity to x.Language comprehension is production in re-verse.
In OT, it maps each outfit z to the set of chil-?Thanks to Kie Zuraw for asking about comprehension; toRon Kaplan for demanding an algebraic construction before hebelieved directional OT was finite-state; and to others whosequestions convinced me that this paper deserved to be written.dren x for whom that outfit is optimal, i.e., is at leastas flattering as any other outfit z?
:PRODUCE(x) = {z : (@z?)
(x, z?)
> (x, z)}COMPREHEND(z) = {x : z ?
PRODUCE(x)}= {x : (@z?)
(x, z?)
> (x, z)}In general z and z?
may range over infinitely manypossible pronunciations.
While the formulas aboveare almost identical, comprehension is in a sensemore complex because it varies both the underlyingand surface forms.
While PRODUCE(x) considersall pairs (x, z?
), COMPREHEND(z) must for each xconsider all pairs (x, z?).
Of course, this nested def-inition does not preclude computational shortcuts.This paper has three modest goals:1.
To show that OT comprehension does in factpresent a computational problem that productiondoes not.
Even when the OT grammar is required tobe finite-state, so that production can be performedwith finite-state techniques, comprehension cannotin general be performed with finite-state techniques.2.
To consider recent constructions that cut throughthis problem (Frank and Satta, 1998; Karttunen,1998; Eisner, 2000; Gerdemann and van Noord,2000).
By altering or approximating the OTformalism?that is, by hook or by crook?these con-structions manage to compile OT grammars intofinite-state transducers.
Transducers may readily beinverted to do comprehension as easily as produc-tion.
We carefully lay out how to use them for com-prehension in realistic circumstances (in the pres-ence of correspondence theory, lexical constraints,hearer uncertainty, and phonetic postprocessing).3.
To give a unified treatment in the extended finite-state calculus of the constructions referenced above.This clarifies their meaning and makes them easy toimplement.
For example, we obtain a transparent al-gebraic version of Eisner?s (2000) unbearably tech-nical automaton construction for his proposed for-malism of ?directional OT.
?Computational Linguistics (ACL), Philadelphia, July 2002, pp.
56-63.Proceedings of the 40th Annual Meeting of the Association forThe treatment shows that all the constructionsemerge directly from a generalized presentation ofOT, in which the crucial fact is that the harmony or-dering on scored candidates is a regular relation.2 Previous Work on ComprehensionWork focusing on OT comprehension?or evenmentioning it?has been surprisingly sparse.
Whilethe recent constructions mentioned in ?1 can easilybe applied to the comprehension problem, as we willexplain, they were motivated primarily by a desire topare back OT?s generative power to that of previousrewrite-rule formalisms (Johnson, 1972).Fosler (1996) noted the existence of the OT com-prehension task and speculated that it might suc-cumb to heuristic search.
Smolensky (1996) pro-posed to solve it by optimizing the underlying form,COMPREHEND(z)?= {x : (@x?)
(x?, z) > (x, z)}Hale and Reiss (1998) pointed out in response thatany comprehension-by-optimization strategy wouldhave to arrange for multiple optima: after all, phono-logical comprehension is a one-to-many mapping(since phonological production is many-to-one).1The correctness of Smolensky?s proposal (i.e.,whether it really computes COMPREHEND) dependson the particular harmony measure.
It can be madeto work, multiple optima and all, if the harmonymeasure is constructed with both production andcomprehension in mind.
Indeed, for any phonology,it is trivial to design a harmony measure that bothproduction and comprehension optimize.
(Just de-fine the harmony of (x, z) to be 1 or 0 accordingto whether the mapping x 7?
z is in the language!
)But we are really only interested in harmony mea-sures that are defined by OT-style grammars (rank-ings of ?simple?
constraints).
In this case Smolen-sky?s proposal can be unworkable.
In particular, ?4will show that a finite-state production grammar inclassical OT need not be invertible by any finite-statecomprehension grammar.1Hale & Reiss?s criticism may be specific to phonologyand syntax.
For some phenomena in semantics, pragmatics,and even morphology, Blutner (1999) argues for a one-to-oneform-meaning mapping in which marked forms express markedmeanings.
He deliberately uses bidirectional optimization torule out many-to-one cases: roughly speaking, an (x, z) pair isgrammatical for him only if z is optimal given x and vice-versa.3 A General Presentation of OTThis section (graphically summarized in Fig.
1) laysout a generalized version of OT?s theory of produc-tion, introducing some notational and representa-tional conventions that may be useful to others andwill be important below.
In particular, all objectsare represented as strings, or as functions that mapstrings to strings.
This will enable us to use finite-state techniques later.The underlying form x and surface form z arerepresented as strings.
We often refer to these stringsas input and output.
Following Eisner (1997), eachcandidate (x, z) is also represented as a string y.The notation (x, z) that we have been using so farfor candidates is actually misleading, since in factthe candidates y that are compared encode more thanjust x and z.
They also encode a particular alignmentor correspondence between x and z.
For example,if x = abdip and z = a[di][bu], then a typicalcandidate would be encodedy = aab0[ddii][pb0u]which specifies that a corresponds to a, b wasdeleted (has no surface correspondent), voiceless psurfaces as voiced b, etc.
The harmony of y mightdepend on this alignment as well as on x and z (justas an outfit might fit worse when worn backwards).Because we are distinguishing underlying andsurface material by using disjoint alphabets ?
={a,b, .
.
.}
and ?
= {[,],a,b, .
.
.
},2 it is easy toextract the underlying and surface forms (x and z)from y.Although the above example assumes that x andz are simple strings of phonemes and brackets, noth-ing herein depends on that assumption.
Autoseg-mental representations too can be encoded as strings(Eisner, 1997).In general, an OT grammar consists of 4 com-ponents: a constraint ranking, a harmony ordering,and generating and pronouncing functions.
The con-straint ranking is the language-specific part of thegrammar; the other components are often supposedto be universal across languages.The generating function GEN maps any x ?
?
?to the (nonempty) set of candidates y whose under-lying form is x.
In other words, GEN just inserts2An alternative would be to distinguish them by odd andeven positions in the string.x???
?underlying form x???GEN??
Y0(x)C1??
Y1(x)C2??
Y2(x) ?
?
?Cn??
Yn(x)?
??
?sets of candidates y?(???)?PRON??Z(x)?
??
?set of surface forms z??
?where Yi?1(x)Ci??
Yi(x) really means Yi?1(x)?
??
?y?(???)?Ci??
Y?i(x)prune??
optimal subset of Y?i(x)?
??
?y??(????{?
})?delete ???
Yi(x)?
??
?y?(???
)?Figure 1: This paper?s view of OT production.
In the second line,Ci inserts ?
?s into candidates; then the candidates with suboptimalstarrings are pruned away, and finally the ?
?s are removed from the survivors.arbitrary substrings from ??
amongst the charac-ters of x, subject to any restrictions on what consti-tutes a legitimate candidate y.3 (Legitimacy mightfor instance demand that y?s surface material z havematched, non-nested left and right brackets, or eventhat z be similar to x in terms of edit distance.
)A constraint ranking is simply a sequenceC1, C2, .
.
.
Cn of constraints.
Let us take eachCi to be a function that scores candidates y byannotating them with violation marks ?.
For ex-ample, a NODELETE constraint would map y =aab0c0[ddii][pb0u] to y?
=NODELETE(y) =aab?0c?0[ddii][pb0u], inserting a ?
after eachunderlying phoneme that does not correspond to anysurface phoneme.
This unconventional formulationis needed for new approaches that care about the ex-act location of the ??s.
In traditional OT only thenumber of ?
?s is important, although the locationsare sometimes shown for readability.Finally, OT requires a harmony orderingon scored candidates y?
?
(?
?
?
?
{?})?.
Intraditional OT, y?
is most harmonic when it con-tains the fewest ??s.
For example, among candi-dates scored by NODELETE, the most harmonicones are the ones with the fewest deletions; manycandidates may tie for this honor.
?6 considersother harmony orderings, a possibility recognizedby Prince and Smolensky (1993) ( corresponds totheir H-EVAL).
In general  may be a partial or-der: two competing candidates may be equally har-monic or incomparable (in which case both cansurvive), and candidates with different underlyingforms never compete at all.Production under such a grammar is a matter ofsuccessive filtering by the constraints C1, .
.
.
Cn.Given an underlying form x, letY0(x) = GEN(x) (1)3It is never really necessary for GEN to enforce such restric-tions, since they can equally well be enforced by the top-rankedconstraint C1 (see below).Yi(x) = {y ?
Yi?1(x) : (2)(@y?
?
Yi?1(x)) Ci(y?)
Ci(y)}The set of optimal candidates is now Yn(x).
Ex-tracting z from each y ?
Yn(x) gives the set Z(x)or PRODUCE(x) of acceptable surface forms:Z(x) = {PRON(y) : y ?
Yn(x)} ?
??
(3)PRON denotes the simple pronunciation functionthat extracts z from y.
It is the counterpart to GEN:just as GEN fleshes out x ?
??
into y by insertingsymbols of ?, PRON slims y down to z ?
??
byremoving symbols of ?.Notice that Yn ?
Yn?1 ?
.
.
.
?
Y0.
The onlycandidates y ?
Yi?1 that survive filtering by Ci arethe ones that Ci considers most harmonic.The above notation is general enough to handlesome of the important variations of OT, such asParadigm Uniformity and Sympathy Theory.
In par-ticular, one can define GEN so that each candidatey encodes not just an alignment between x and z,but an alignment among x, z, and some other stringsthat are neither underlying nor surface.
These otherstrings may represent the surface forms for othermembers of the same morphological paradigm, orintermediate throwaway candidates to which z issympathetic.
Production still optimizes y, whichmeans that it simultaneously optimizes z and theother strings.4 Comprehension in Finite-State OTThis section assumes OT?s traditional harmony or-dering, in which the candidates that survive filteringby Ci are the ones into which Ci inserts fewest ?
?s.Much computational work on OT has been con-ducted within a finite-state framework (Ellison,1994), in keeping with a tradition of finite-statephonology (Johnson, 1972; Kaplan and Kay, 1994).44The tradition already included (inviolable) phonologicalFinite-state OT is a restriction of the formal-ism discussed above.
It specifically assumes thatGEN, C1, .
.
.
Cn, and PRON are all regular relations,meaning that they can be described by finite-statetransducers.
GEN is a nondeterministic transducerthat maps each x to multiple candidates y.
The othertransducers map each y to a single y?
or z.These finite-state assumptions were proposed(in a different and slightly weaker form) byEllison (1994).
Their empirical adequacy has beendefended by Eisner (1997).In addition to having the right kind of power lin-guistically, regular relations are closed under vari-ous relevant operations and allow (efficient) parallelprocessing of regular sets of strings.
Ellison (1994)exploited such properties to give a production algo-rithm for finite-state OT.
Given x and a finite-stateOT grammar, he used finite-state operations to con-struct the set Yn(x) of optimal candidates, repre-sented as a finite-state automaton.Ellison?s construction demonstrates that Yn is al-ways a regular set.
Since PRON is regular, it followsthat PRODUCE(x) = Z(x) is also a regular set.We now show that COMPREHEND(z), in con-strast, need not be a regular set.
Let ?
= {a,b},?
= {[,],a,b, .
.
.}
and suppose that GEN allowscandidates like the ones in ?3, in which parts of thestring may be bracketed between [ and ].
The cru-cial grammar consists of two finite-state constraints.C2 penalizes a?s that fall between brackets (by in-serting ?
next to each one) and also penalizes b?sthat fall outside of brackets.
It is dominated by C1,which penalizes brackets that do not fall at eitheredge of the string.
Note that this grammar is com-pletely permissive as to the number and location ofsurface characters other than brackets.If x contains more a?s than b?s, then PRODUCE(x)is the set ???
of all unbracketed surface forms, where??
is ?
minus the bracket symbols.
If x containsfewer a?s than b?s, then PRODUCE(x) = [???
].And if a?s and b?s appear equally often in x, thenPRODUCE(x) is the union of the two sets.Thus, while the x-to-z mapping is not a regularrelation under this grammar, at least PRODUCE(x)is a regular set for each x?just as finite-state OTconstraints, notably Koskenniemi?s (1983) two-level model,which like OT used finite-state constraints on candidates y thatencoded an alignment between underlying x and surface z.guarantees.
But for any unbracketed z ?
??
?, suchas z = abc, COMPREHEND(z) is not regular: it isthe set of underlying strings with # of a?s?
# of b?s.This result seems to eliminate any hope of han-dling OT comprehension in a finite-state frame-work.
It is interesting to note that both OT andcurrent speech recognition systems construct finite-state models of production and define comprehen-sion as the inverse of production.
Speech recog-nizers do correctly implement comprehension viafinite-state optimization (Pereira and Riley, 1997).But this is impossible in OT because OT has a morecomplicated production model.
(In speech recog-nizers, the most probable phonetic or phonologicalsurface form is not presumed to have suppressed itscompetitors.
)One might try to salvage the situation by barringconstraints like C1 or C2 from the theory as linguis-tically implausible.
Unfortunately this is unlikelyto succeed.
Primitive OT (Eisner, 1997) already re-stricts OT to something like a bare minimum of con-straints, allowing just two simple constraint familiesthat are widely used by practitioners of OT.
Yet eventhese primitive constraints retain enough power tosimulate any finite-state constraint.
In any case, C1and C2 themselves are fairly similar to ?domain?constraints used to describe tone systems (Cole andKisseberth, 1994).
While C2 is somewhat odd inthat it penalizes two distinct configurations at once,one would obtain the same effect by combining threeseparately plausible constraints: C2 requires a?s be-tween brackets (i.e., in a tone domain) to receive sur-face high tones, C3 requires b?s outside brackets toreceive surface high tones, and C4 penalizes all sur-face high tones.5Another obvious if unsatisfying hack would im-pose heuristic limits on the length of x, for exam-ple by allowing the comprehension system to returnthe approximation COMPREHEND(z) ?
{x : |x| ?2 ?
|z|}.
This set is finite and hence regular, so per-5Since the surface tones indicate the total number of a?s andb?s in the underlying form, COMPREHEND(z) is actually a finiteset in this version, hence regular.
But the non-regularity argu-ment does go through if the tonal information in z is not avail-able to the comprehension system (as when reading text with-out diacritics); we cover this case in ?5.
(One can assume thatsome lower-ranked constraints require a special suffix before ],so that the bracket information need not be directly available tothe comprehension system either.
)haps it can be produced by some finite-state method,although the automaton to describe the set might belarge in some cases.Recent efforts to force OT into a fully finite-statemold are more promising.
As we will see, they iden-tify the problem as the harmony ordering , ratherthan the space of constraints or the potential infini-tude of the answer set.5 Regular-Relation ComprehensionSince COMPREHEND(z) need not be a regular setin traditional OT, a corollary is that COMPREHENDand its inverse PRODUCE are not regular relations.That much was previously shown by Markus Hillerand Paul Smolensky (Frank and Satta, 1998), usingsimilar examples.However, at least some OT grammars ought to de-scribe regular relations.
It has long been hypothe-sized that all human phonologies are regular rela-tions, at least if one omits reduplication, and this isnecessarily true of phonologies that were success-fully described with pre-OT formalisms (Johnson,1972; Koskenniemi, 1983).Regular relations are important for us becausethey are computationally tractable.
Any regular rela-tion can be implemented as a finite-state transducerT , which can be inverted and used for comprehen-sion as well as production.
PRODUCE(x) = T (x) =range(x ?
T ), and COMPREHEND(z) = T?1(z) =domain(T ?
z).We are therefore interested in compiling OTgrammars into finite-state transducers?by hook orby crook.
?6 discusses how; but first let us see howsuch compilation is useful in realistic situations.Any practical comprehension strategy must rec-ognize that the hearer does not really perceive theentire surface form.
After all, the surface form con-tains phonetically invisible material (e.g., syllableand foot boundaries) and makes phonetically imper-ceptible distinctions (e.g., two copies of a tone ver-sus one doubly linked copy).
How to comprehend inthis case?The solution is to modify PRON to ?go all theway?
?to delete not only underlying material butalso phonetically invisible material.
Indeed, PRONcan also be made to perform any purely phoneticprocessing.
Each output z of PRODUCE is now not aphonological surface form but a string of phonemesor spectrogram segments.
So long as PRON is a reg-ular relation (perhaps a nondeterministic or prob-abilistic one that takes phonetic variation into ac-count), we will still be able to construct T and use itfor production and comprehension as above.6How about the lexicon?
When the phonology canbe represented as a transducer, COMPREHEND(z) isa regular set.
It contains all inputs x that could haveproduced output z.
In practice, many of these in-puts are not in the lexicon, nor are they possiblenovel words.
One should restrict to inputs that ap-pear in the lexicon (also a regular set) by intersectingCOMPREHEND(z) with the lexicon.
For novel wordsthis intersection will be empty; but one can find thepossible underlying forms of the novel word, forlearning?s sake, by intersecting COMPREHEND(z)with a larger (infinite) regular set representing allforms satisfying the language?s lexical constraints.There is an alternative treatment of the lexicon.GEN can be extended ?backwards?
to incorporatemorphology just as PRON was extended ?forwards?to incorporate phonetics.
On this view, the inputx is a sequence of abstract morphemes, and GENperforms morphological preprocessing to turn x intopossible candidates y. GEN looks up each abstractmorpheme?s phonological string ?
??
from the lex-icon,7 then combines these phonological strings byconcatenation or template merger, then nondeter-ministically inserts surface material from ??.
Sucha GEN can plausibly be built up (by composition)as a regular relation from abstract morpheme se-quences to phonological candidates.
This regularity,as for PRON, is all that is required.Representing a phonology as a transducer T hasadditional virtues.
T can be applied efficientlyto any input string x, whereas Ellison (1994) orEisner (1997) requires a fresh automaton construc-tion for each x.
A nice trick is to build T without6Pereira and Riley (1997) build a speech recognizer by com-posing a probabilistic finite-state language model, a finite-statepronouncing dictionary, and a probabilistic finite-state acousticmodel.
These three components correspond precisely to the in-put to GEN, the traditional OT grammar, and PRON, so we aresimply suggesting the same thing in different terminology.7Nondeterministically in the case of phonologically condi-tioned allomorphs: INDEFINITE APPLE 7?
{?
?pl, ?n?pl} ???.
This yields competing candidates that differ even in theirunderlying phonological material.PRON and apply it to all conceivable x?s in paral-lel, yielding the complete set of all optimal candi-dates Yn(??)
=?x???
Yn(x).
If Y and Y ?
denotethe sets of optimal candidates under two grammars,then (Y ?
?Y ?)
?
(Y ?
?
?Y ) yields the candidatesthat are optimal under only one grammar.
ApplyingGEN?1 or PRON to this set finds the regular set ofunderlying or surface forms that the two grammarswould treat differently; one can then look for empir-ical cases in this set, in order to distinguish betweenthe two grammars.6 Theorem on Compiling OTWhy are OT phonologies not always regular re-lations?
The trouble is that inputs may be arbi-trarily long, and so may accrue arbitrarily largenumbers of violations.
Traditional OT (?4) issupposed to distinguish all such numbers.
Con-sider syllabification in English, which prefersto syllabify the long input bibambam .
.
.bam?
??
?k copiesas [bi][bam][bam] .
.
.
[bam] (with k codas)rather than [bib][am][bam] .
.
.
[bam] (withk + 1 codas).
NOCODA must therefore distinguishannotated candidates y?
with k ?
?s (which are opti-mal) from those with k + 1 ?
?s (which are not).
Itrequires a (?
k + 2)-state automaton to make thisdistinction by looking only at the ?
?s in y?.
And if kcan be arbitrarily large, then no finite-state automa-ton will handle all cases.Thus, constraints like NOCODA do not allow anupper bound on k for all x ?
??.
Of course, the min-imal number of violations k of a constraint is fixedgiven the underlying form x, which is useful in pro-duction.8 But comprehension is less fortunate: wecannot bound k given only the surface form z. Inthe grammar of ?4, COMPREHEND(abc) includedunderlying forms whose optimal candidates had ar-bitrarily large numbers of violations k.Now, in most cases, the effect of an OT gram-mar can be achieved without actually counting any-thing.
(This is to be expected since rewrite-rule8Ellison (1994) was able to construct PRODUCE(x) from x.One can even build a transducer for PRODUCE that is correct onall inputs that can achieve?
K violations and returns ?
on otherinputs (signalling that the transducer needs to be recompiledwith increased K).
Simply use the construction of (Frank andSatta, 1998; Karttunen, 1998), composed with a hard constraintthat the answer must have ?
K violations.grammars were previously written for the samephonologies, and they did not use counting!)
Thisis possible despite the above arguments becausefor some grammars, the distinction between opti-mal and suboptimal y?
can be made by looking atthe non-?
symbols in y?
rather than trying to countthe ??s.
In our NOCODA example, a surface sub-string such as .
.
.ib?][a.
.
.
might signal that y?
issuboptimal because it contains an ?unnecessary?coda.
Of course, the validity of this conclusiondepends on the grammar and specifically the con-straints C1, .
.
.
Ci?1 ranked above NOCODA, sincewhether that coda is really unnecessary depends onwhether Y?i?1 also contains the competing candidate.
.
.i][ba .
.
.
with fewer codas.But as we have seen, some OT grammars do haveeffects that overstep the finite-state boundary (?4).Recent efforts to treat OT with transducers havetherefore tried to remove counting from the formal-ism.
We now unify such efforts by showing that theyall modify the harmony ordering .
?4 described finite-state OT grammars as oneswhere GEN, PRON, and the constraints are regularrelations.
We claim that if the harmony orderingis also a regular relation on strings of (????{?
})?,then the entire grammar (PRODUCE) is also regular.We require harmony orderings to be compatiblewith GEN: an ordering must treat y?
?, y?
as incompa-rable (neither is  the other) if they were producedfrom different underlying forms.9To make the notation readable let us denote therelation by the letter H .
Thus, a transducer for Haccepts the pair (y?
?, y?)
if y??
y?.The construction is inductive.
Y0 = GEN is reg-ular by assumption.
If Yi?1 is regular, then so is Yisince (as we will show)Yi = (Y?i ?
?range(Y?i ?H)) ?D (4)where Y?idef= Yi?1 ?
Ci and maps x to the set ofstarred candidates that Ci will prune; ?
denotes thecomplement of a regular language; and D is a trans-ducer that removes all ??s.
Therefore PRODUCE =Yn ?
PRON is regular as claimed.9For example, the harmony ordering of traditional OT is{(y?
?, y?)
: y??
has the same underlying form as, but containsfewer ?
?s than, y?}.
If we were allowed to drop the same-underlying-form condition then the ordering would become reg-ular, and then our claim would falsely imply that all traditionalfinite-state OT grammars were regular relations.It remains to derive (4).
Equation (2) impliesCi(Yi(x)) = {y?
?
Y?i(x) : (@y??
?
Y?i(x)) y??
y?}
(5)= Y?i(x)?
{y?
: (?y??
?
Y?i(x)) y??
y?}
(6)= Y?i(x)?H(Y?i(x)) (7)One can read H(Y?i(x)) as ?starred candidates thatare worse than other starred candidates,?
i.e., subop-timal.
The set difference (7) leaves only the optimalcandidates.
We now see(x, y?)
?
Yi ?
Ci ?
y?
?
Ci(Yi(x)) (8)?
y?
?
Y?i(x), y?
6?
H(Y?i(x)) [by (7)] (9)?
y?
?
Y?i(x), (@z)y?
?
H(Y?i(z)) [see below](10)?
(x, y?)
?
Y?i, y?
6?
range(Y?i ?H) (11)?
(x, y?)
?
Y?i ?
?range(Y?i ?H) (12)therefore Yi ?
Ci = Y?i ?
?range(Y?i ?H) (13)and composing both sides with D yields (4).
To jus-tify (9)?
(10) we must show when y?
?
Y?i(x) thaty?
?
H(Y?i(x)) ?
(?z)y?
?
H(Y?i(z)).
For the ?direction, just take z = x.
For ?, y?
?
H(Y?i(z))means that (?y??
?
Y?i(z))y??
y?
; but then x = z(giving y?
?
H(Y?i(x))), since if not, our compatibil-ity requirement on H would have made y??
?
Y?i(z)incomparable with y?
?
Y?i(x).Extending the pretty notation of (Karttunen,1998), we may use (4) to define a left-associativegeneralized optimality operator ooH :Y ooH Cdef= (Y ?C?
?range(Y ?C?H))?D (14)Then for any regular OT grammar, PRODUCE =GEN ooH C1 ooH C2 ?
?
?
ooH Cn ?
PRONand can be inverted to get COMPREHEND.
Moregenerally, different constraints can usefully be ap-plied with different H?s (Eisner, 2000).The algebraic construction above is inspired by aversion that Gerdemann and van Noord (2000) givefor a particular variant of OT.
Their regular expres-sions can be used to implement it, simply replacingtheir add_violation by our H .Typically, H ignores surface characters whencomparing starred candidates.
So H can be writtenas elim(?)?G?elim(?
)?1 where elim(?)
is atransducer that removes all characters of ?.
To sat-isfy the compatibility requirement on H , G shouldbe a subset of the relation (?| ?
|( : ?)|(?
: ))?.1010This transducer regexp says to map any symbol in ??
{?
}to itself, or insert or delete ?
?and then repeat.We now summarize the main proposals from theliterature (see ?1), propose operator names, and castthem in the general framework.?
Y o C: Inviolable constraint (Koskenniemi,1983; Bird, 1995), implemented by composition.?
Y o+ C: Counting constraint (Prince andSmolensky, 1993): more violations is more dishar-monic.
No finite-state implementation possible.?
Y oo C: Binary approximation (Karttunen,1998; Frank and Satta, 1998).
All candidates withany violations are equally disharmonic.
Imple-mented by G = (??
( : ?)??
)+, which relates un-derlying forms without violations to the same formswith violations.?
Y oo3 C: 3-bounded approximation (Karttunen,1998; Frank and Satta, 1998).
Like o+ , but allcandidates with ?
3 violations are equally dishar-monic.
G is most easily described with a transducerthat keeps count of the input and output ?
?s so far, ona scale of 0, 1, 2, ?
3.
Final states are those whoseoutput count exceeds their input count on this scale.?
Y o?
C: Matching or subset approximation(Gerdemann and van Noord, 2000).
A candidate ismore disharmonic than another if it has stars in allthe same locations and some more besides.11 HereG = ((?|?)?
( : ?)(?|?)?)+.?
Y o> C: Left-to-right directional evaluation (Eis-ner, 2000).
A candidate is more disharmonic thananother if in the leftmost position where they differ(ignoring surface characters), it has a ?.
This revisesOT?s ?do only when necessary?
mantra to ?do onlywhen necessary and then as late as possible?
(evenif delaying ?
?s means suffering more of them later).Here G = (?|?)?
(( : ?)|((?
: ?)(?|?)?)).
Unlikethe other proposals, here two forms can both be op-timal only if they have exactly the same pattern ofviolations with respect to their underlying material.?
Y <o C: Right-to-left directional evaluation.
?Do only when necessary and then as early as possi-ble.?
Here G is the reverse of the G used in o> .The novelty of the matching and directional pro-posals is their attention to where the violations fall.Eisner?s directional proposal (o>, <o) is the only11Many candidates are incomparable under this ordering, soGerdemann and van Noord also showed how to weaken the no-tation of ?same location?
in order to approximate o+ better.
(a) x =bantodibo[ban][to][di][bo][ban][ton][di][bo][ban][to][dim][bon][ban][ton][dim][bon](b) NOCODAban?todiboban?to?diboban?todi?bo?ban?to?di?bo?
(c) C1 NOCODA*!
*+ *****!***!
*(d) C1 ?1 ?2 ?3 ?4*!
** *!+ * * ** *!
* *Figure 2: Counting vs. directionality.
[Adapted from (Eisner, 2000).]
C1 is some high-ranked constraint that kills the most faithfulcandidate; NOCODA dislikes syllable codas.
(a) Surface material of the candidates.
(b) Scored candidates for G to compare.Surface characters but not ?
?s have been removed by elim(?).
(c) In traditional evaluation o+ , G counts the ??s.
(d) Directionalevaluation o> gets a different result, as if NOCODA were split into 4 constraints evaluating the syllables separately.
Moreaccurately, it is as if NOCODA were split into one constraint per underlying letter, counting the number of ?
?s right after that letter.one defended on linguistic as well as computationalgrounds.
He argues that violation counting (o+) isa bug in OT rather than a feature worth approximat-ing, since it predicts unattested phenomena such as?majority assimilation?
(Bakovic?, 1999; Lombardi,1999).
Conversely, he argues that comparing viola-tions directionally is not a hack but a desirable fea-ture, since it naturally predicts ?iterative phenom-ena?
whose description in traditional OT (via Gener-alized Alignment) is awkward from both a linguisticand a computational point of view.
Fig.
2 contraststhe traditional and directional harmony orderings.Eisner (2000) proved that o> was a regular op-erator for directional H , by making use of a ratherdifferent insight, but that machine-level constructionwas highly technical.
The new algebraic construc-tion is simple and can be implemented with a fewregular expressions, as for any other H .7 ConclusionSee the itemized points in ?1 for a detailed summary.In general, this paper has laid out a clear, generalframework for finite-state OT systems, and used it toobtain positive and negative results about the under-studied problem of comprehension.
Perhaps theseresults will have some bearing on the developmentof realistic learning algorithms.The paper has also established sufficient condi-tions for a finite-state OT grammar to compile into afinite-state transducer.
It should be easy to imaginenew variants of OT that meet these conditions.ReferencesEric Bakovic?.
1999.
Assimilation to the unmarked.
Rut-gers Optimality Archive ROA-340., August.Steven Bird.
1995.
Computational Phonology: AConstraint-Based Approach.
Cambridge.Reinhard Blutner.
1999.
Some aspects of optimality innatural language interpretation.
In Papers on Optimal-ity Theoretic Semantics.
Utrecht.J.
Cole and C. Kisseberth.
1994.
An optimal domainstheory of harmony.
Studies in the Linguistic Sciences,24(2).Jason Eisner.
1997.
Efficient generation in primitive Op-timality Theory.
In Proc.
of ACL/EACL.Jason Eisner.
2000.
Directional constraint evaluation inOptimality Theory.
In Proc.
of COLING.T.
Mark Ellison.
1994.
Phonological derivation in Opti-mality Theory.
In Proc.
of COLINGJ.
Eric Fosler.
1996.
On reversing the generation processin Optimality Theory.
Proc.
of ACL Student Session.R.
Frank and G. Satta.
1998.
Optimality Theory and thegenerative complexity of constraint violability.
Com-putational Linguistics, 24(2):307?315.D.
Gerdemann and G. van Noord.
2000.
Approxima-tion and exactness in finite-state Optimality Theory.
InProc.
of ACL SIGPHON Workshop.Mark Hale and Charles Reiss.
1998.
Formal and empir-ical arguments concerning phonological acquisition.Linguistic Inquiry, 29:656?683.C.
Douglas Johnson.
1972.
Formal Aspects of Phonolog-ical Description.
Mouton.R.
Kaplan and M. Kay.
1994.
Regular models of phono-logical rule systems.
Comp.
Ling., 20(3).L.
Karttunen.
1998.
The proper treatment of optimalityin computational phonology.
In Proc.
of FSMNLP.Kimmo Koskenniemi.
1983.
Two-level morphology: Ageneral computational model for word-form recogni-tion and production.
Publication 11, Dept.
of GeneralLinguistics, University of Helsinki.Linda Lombardi.
1999.
Positional faithfulness and voic-ing assimilation in Optimality Theory.
Natural Lan-guage and Linguistic Theory, 17:267?302.Fernando C. N. Pereira and Michael Riley.
1997.
Speechrecognition by composition of weighted finite au-tomata.
In E. Roche and Y. Schabes, eds., Finite-StateLanguage Processing.
MIT Press.A.
Prince and P. Smolensky.
1993.
Optimality Theory:Constraint interaction in generative grammar.
Ms.,Rutgers and U. of Colorado (Boulder).Paul Smolensky.
1996.
On the comprehen-sion/production dilemma in child language.
LinguisticInquiry, 27:720?731.
