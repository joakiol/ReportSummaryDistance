SEMANTIC-HEAD-DRIVEN GENERATIONStuart M. ShieberAiken Computation LaboratoryDivision of Applied SciencesHarvard UniversityCambridge, MA 02138Gertjan van NoordDepartment of LinguisticsRijksuniversiteit UtrechtUtrecht, The NetherlandsFernando C. N. PereiraAT & T Bell LaboratoriesMurray Hill, NJ  07974Robert C. MooreArtificial Intelligence CenterSRI InternationalMenlo Park, CA 94025We present an algorithm for generating strings from logical form encodings that improves upon previousalgorithms in that it places fewer restrictions on the class of grammars to which it is applicable.
In particular,unlike a previous bottom-up generator, it allows use of semlantically nonmonotonic grammars, yet unliketop-down methods, it also permits left-recursion.
The enabling design feature of the algorithm is its implicittraversal of the analysis tree for the string being generated in a semantic-head-driven fashion.1 INTRODUCTIONThe problem of generating a well-formed natural anguageexpression from an encoding of its meaning possesses prop-erties that distinguish it from the converse problem ofrecovering a meaning encoding from a given natural an-guage xpression.
This much is axiomatic.
In previous work(Shieber 1988), however, one of us attempted tocharacter-ize these differing properties in such a way that a singleuniform architecture, appropriately parameterized, mightbe used for both natural anguage processes.
In particular,we developed an architecture inspired by the Earley deduc-tion work of Pereira and Warren (1983), but which gener-alized that work allowing for its use in both a parsing andgeneration mode merely by setting the values of a smallnumber of parameters.As a method for generating natural language xpres-sions, the Earley deduction method is reasonably successfulalong certain dimensions.
It is quite simple, general in itsapplicability to a range of unification-based and logic gram-mar formalisms, and uniform, in that it places only onerestriction (discussed below) on the form of the linguisticanalyses allowed by the grammars used in generation.
Inparticular, generation from grammars with recursions whosewell-foundedness relies on lexical information will termi-nate; top-down generation regimes uch as those of Wede-kind (1988) or Dymetman and Isabelle (1988) lack thisproperty; further discussion can be found in Section 2.1.Unfortunately, the bottom-up, left-to-right processingregime of Earley generation--as it might be called---has itsown inherent frailties.
Efficiency considerations requirethat only grammars possessing a property of semanticmonotonicity can be effectively used, and even for thosegrammars, processing can become overly nondeterministic.Tile algorithm described in this paper is an attempt oresolve these problems in a satisfactory manner.
Althoughwe believe that this algorithm could be seen as an instanceof a uniform architecture for parsing and generation--justas tile extended Earley parser (Shieber, 1985b) and thebottom-up generator were instances of the generalizedEarley deduction architecture--our efforts to date havebeen aimed foremost toward the development of the algo-rithm for generation alone.
We will mention efforts towardthis end in Section 5.1.1 APPLICABILITY OF THE ALGORITHMAs does the Earley-based generator, the new algorithmassumes that the grammar is a unification-based or logicgrammar with a phrase structure backbone and complexnonterminals.
Furthermore, and again consistent with pre-vious work, we assume that the nonterminals associate tothe phrases they describe logical expressions encoding theirpossible meanings.
Beyond these requirements common tologic-based formalisms, the methods are generally applica-ble.A variant of our method is used in Van Noord's BUG(Bottom-Up Generator) system, part of MiMo2, an experi-mental machine translation system for translating interna-tional news items of Teletext, which uses a Prolog version of30 Computational Linguistics Volume 16, Number 1, March 1990Shieber et al Semantic Head-Driven GrammarPATR-II similar to that of Hirsh (1987).
According toMartin Kay (personal communication), the STREP ma-chine translation project at the Center for the Study ofLanguage and Information uses a version of our algorithmto generate with respect to grammars based on head-drivenphrase structure grammar (HPSG).
Finally, Calder et al(1989) report on a generation algorithm for unificationcategorial grammar that appears to be a special case ofours.1.2 PRELIMINARIESDespite the general applicability of the algorithm, we will,for the sake of concreteness, describe it and other genera-tion algorithms in terms of their implementation for definite-clause grammars (DCG).
For ease of exposition, the encod-ing will be a bit more cumbersome than is typically found inProlog DCG interpreters.
The standard DCG encoding inProlog uses the notation(cat o) --> (cat I ) .
.
.
.
.
(cat,).where the (cat i) are terms representing the grammaticalcategory of an expression and its subconstituents.
Terminalsymbols are introduced into rules by enclosing them in listbrackets, for examplesbar/S --> \[that\], s/S.Such rules can be translated into Prolog directly using adifference list encoding of string positions; we assumereaders are familiar with this technique (Pereira and Shie-ber, 1985).Because we concentrate on the relationship betweenexpressions in a language and their logical forms, we willassume that the category terms have both a syntactic and asemantic omponent.
In particular, the infix function sym-bol / will be used to form categories of the form Syn/Semwhere Syn is the syntactic ategory of the expression andSere is an encoding of its semantics as a logical form; theprevious rule uses this notation, for example.
From a DCGperspective, all the rules involve the single nonterminal/,with the given intended interpretation.
Furthermore, therepresentation f grammars that we will postulate includesthe threading of string positions explicitly, so that a nodedescription will be of the form node (Syn/Sem, PO-P).The first argument of the node functor is the category,divided into its syntactic and semantic omponents; thesecond argument is the difference list encoding of thesubstring it covers.
In summary, a DCG grammar rule willbe encoded as the clausenode((syno) / (semo), PO-P)--->\[node( ( syn I) / ( semi ).
PO-P 1) .
.
.
.
,node( syn.)
/ (sem.)
, Pn-I-P\].We use the functor '--->' to distinguish this node encodingfrom the standard one.
The right-hand-side elements arekept as a Prolog list for easier manipulation by the interpret-ers we will build.We turn now to the issue of terminal symbols on theright-hand sides of rules in the node encoding.
During thecompilation process from the standard encoding to the nodeencoding, the right-hand side of a rule is converted from alist of categories and terminal strings to a list of nodesconnected together by the difference-list threading tech-nique used for standard DCG compilation.
At that point,terminal strings can be introduced into the string threadingand need never be considered further.
For instance, theprevious rule becomesnode(sbar/S, \[that\[P0\]-P) ---> node(s/S, P0-P).Throughout, we will alternate between the two encod-ings, using the standard one for readability and the nodeencoding as the actual data for grammar interpretation.
Asthe latter, more cumbersome, representation is algorithmi-cally generable from the former, no loss of generalityensues from using both.2 PROBLEMS WITH EXISTING GENERATORSExisting generation algorithms have efficiency or termina-tion problems with respect to certain classes of grammars.We review the problems of both top-down and bottom-upregimes in this section.2.1 PROBLEMS WITH TOP-DOWN GENERATORSConsider a naive top-down generation mechanism thattakes as input the semantics to generate from and a corre-sponding syntactic ategory and builds a complete tree,top-down, left-to-right by applying rules of the grammarnondeterministically to the fringe of the expanding tree.This control regime is realized, for instance, when runninga DCG "backwards" as a generator.Concretely, the following DCG interpreter--written inProlog and taking as its data the grammar in encodedform--implements such a generation method.gen(LF, Sentence) :- generate(node~s/LF, Sentence-\[ \])).generate(Node) :-(Node - -> Children),generate _ children(Children).generate_ children(\[ ]).generate_ children(\[ChildlRest\]) :-generate(Child),generate_ children(Rest).Clearly, such a generator may not terminate.
For exam-ple, consider agrammar that includes the ruless/S --> np/NP, vp(NP)/S.np/NP --> det(N)/NP, n/N.det(N)/NP -> np/NP0, poss(NP0,N)/NP.np/john --> \[john\].poss(NP0,N)/mod(N,NP0)--> Is\].n/father --> \[father\].vp(NP)/left(NP) --> \[left\].Computational Linguistics Volume 16, Number 1, March 1990 31Shieber et al Semantic Head-Driven GrammarThis grammar admits entences like "John left" and "John'sfather left" with logical form encodings left(john) andleft(mod(father, john)), respectively.
The technique usedhere to build the logical forms is well-known in logicgrammars.lGeneration with the goal gen(left(john), Sent) using thegenerator above will result in application of the first rule tothe node node(s/left(john), Sent-\[ \]).
A subgoal for thegeneration of a node node(np/NP, Sent-P) will result.
Tothis subgoal, the second rule will apply, leading to a subgoalfor generation of the node node(det(N)/NP, Sent-Pl),which itself, by virtue of the third rule, leads to anotherinstance of the NP node generation subgoal.
Of course, theloop may now be repeated an arbitrary number of times.Graphing the tree being constructed by the traversal of thisalgorithm, as in Figure 1, immediately exhibits the poten-tial for nontermination i  the control structure.
(The re-peated goals along the left branch are presented inboldfacein the figure.
Dashed lines indicate portions of the tree yetto be generated.
)This is an instance of the general problem familiar fromlogic programming that a logic program may not terminatewhen called with a goal less instantiated than what wasintended by the program's designer.
Several researchershave noted that a different ordering of the branches in thetop-down traversal would, in the case at hand, remedy thenontermination problem.
For the example above, the solu-tion is to generate the VP first--using the goal generate(node(vp(NP)/left(john), PI-\[ \]))--in the course of whichthe variable NP will become bound so that the generationfrom node(np/NP, Sent-P 1) will terminate.We might allow for reordering of the traversal of thechildren by sorting the nodes before generating them.
Thiscan be simply done, by modifying the first clause of gener-ate.generate(Node) :-(Node - ->  Children),sort_ children(Children, SortedChildren),generate_ children(SortedChildren).s/left (john)np/NP vp (NP)/le ft ( j ohn)?
?s ??
?det (N)/NP n/NSM %S S %%np/NP0 poss (NP0,N)/NP?
?
?# ?%Figure 1 Tree Constructed Top-Down byLeft-Recursive Grammar.Here, we have introduced a predicate sort_children toreorder the child nodes before generating.
Dymetman andIsabelle (1988) propose a node-ordering solution to thetop-down nontermination problem; they allow the gram-mar writer to specify a separate goal ordering for parsingand for generation by annotating the rules by hand.Strzalkowski (1989) develops an algorithm for generatingsuch annotations automatically.
In both of these cases, thenode ordering is known a priori, and can be thought of asapplying to the rules at compile time.Wedekind (1988) achieves the reordering by first gener-ating nodes that are connected, that is, whose semantics iinstantiated.
Since the NP is not connected in this sense,but the VP is, the latter will be expanded first.
In essence,the technique is a kind of goal freezing (Colmerauer 1982)or implicit wait declaration (Naish 1986).
This method ismore general, as the reordering is dynamic; the ordering ofchild nodes can, in principle at least, be different fordifferent uses of the same rule.
The generality seems neces-sary; for cases in which the a priori ordering of goals isinsufficient, Dymetman and Isabelle also introduce goalfreezing to control expansion.Although vastly superior to the naive top-down algo-rithm, even this sort of amended top-down approach togeneration based on goal freezing under one guise or an-other is insufficient with respect o certain linguisticallyplausible analyses.
The symptom is an ordering paradox inthe sorting.
For example, the "complements" rule given byShieber (1985a) in the PATR-II formalismVP 1 --* VP 2 X(VPl head) = (VP2 head)(VP2 syncat first) = (X)<VP2 syncat rest) = (VPI syncat)can be encoded as the DCG rule:vp(Head, Syncat)/VP ->,~(Head, \[Compl/LFlSyncat\])/VP, Compl/LF.Top-down generation using this rule will be forced toexpand the lower VP before its complement, since LF isuninstantiated initially.
Any of the reordering methodsmust choose to expand the child VP node first.
But in thatcase, application of the rule can recur indefinitely, leadingto nontermination.
Thus, no matter what ordering of sub-goals is chosen, nontermination results.Of course, if one knew ahead of time that the subcatego-rization list being built up as the value for Syncat wasbounded in size, then an ad hoc solution would be to limitrecursive use of this rule when that limit had been reached.But even this ad hoc solution is problematic, as there maybe no principled bound on the size of the subcategorizationlist.
For instance, in analyses of Dutch cross-serial verbconstructions (Evers 1975; Huybrechts 1984), subcategori-zation lists may be concatenated bysyntactic rules (Moort-32 Computational Linguistics Volume 16, Number 1, March 1990Shieber et al Semantic Head-Driven Grammargat 1984; Fodor et al 1985; Pollard 1988), resulting inarbitrarily long lists.
Consider the Dutch sentencedat \[Jan \[Marie \[de oppasser \[de olifanten \[zag helpenthat John Mary the keeper the elephants saw helpvoeren\]\]\]\]feedthat John saw Mary help the keeper feed the elephantsThe string of verbs is analyzed by appending their subcate-gorization lists as in Figure 2.
Subcategorization lists underthis analysis can have any length, and it is impossible topredict from a semantic structure the size of its correspond-ing subcategorization listmerely by examining the lexicon.Strzalkowski refers to this problem quite aptly as consti-tuting a deadlock situation.
He notes that by combiningdeadlock-prone rules (using a technique akin to partialexecution 2)many deadlock-prone rules can be replaced byrules that allow reordering; however, he states that "thegeneral solution to this normalization problem is still underinvestigation."
We think that such a general solution isunlikely because of cases like the one above in which nofinite amount of partial execution can necessarily bringsufficient information to bear on the rule to allow ordering.The rule would have to be partially executed with respect toitself and all verbs so as to bring the lexical informationthat well-founds the ordering to bear on the orderingproblem.
In general, this is not a finite process, as theprevious Dutch example reveals.
This does not deny thatcompilation methods may be able to convert a grammarinto a program that generates without termination prob-lems.
In fact, the partial execution techniques described bytwo of us (Pereira and Shieber 1985) could form the basisof a compiler built by partial execution of the new algo-rithm we propose below relative to a grammar.
However,the compiler will not generate a program that generatestop-down, as Strzalkowski's does.v \[c,k,mj\]V \[mj\]Izag V \[k,m\] V \[e,k\]saw \[helpen voerenhelp feedFigure 2 Schematic of Verb SubeategorizationLists for Dutch Example.V \[c,k,m\]In summary, top-down generation algorithms, even ifcontrolled by the instantiation status of goals, can fail toterminate on certain grammars.
The critical property of theexample given above is that the well-foundedness of thegeneration process resides in lexical information unavail-able to top-down regimes.
This property is the hallmark ofseveral inguistically reasonable analyses based on lexicalencoding of grammatical information such as are found incategorial grammar and its unification-based and combina-torial variants, in head-driven phrase-structure grammar,and in lexical-functional grammar.2.2 PROBLEMS WITH BOTTOM-UP GENERATORSThe bottom-up Earley-deduction generator does not fallprey to these problems of nontermination i  the face ofrecursion, because lexical information is available immedi-ately.
However, several important frailties of the Earleygeneration method were noted, even in the earlier work.For efficiency, generation using this Earley deductionmethod requires an incomplete search strategy, filteringthe search space using semantic information.
The semanticfilter makes generation from a logical form computation-ally feasible, but preserves completeness of the generationprocess only in the case of semantically monotonic gram-mars--those grammars in which the semantic omponentof each right-hand-side nonterminal subsumes ome por-tion of the semantic omponent of the left-hand-side.
Thesemantic monotonicity constraint i self is quite restrictive.As stated in the original Earley generation paper (Shieber1988), "perhaps the most immediate problem raised by\[Earley generation\] is the strong requirement of semanticmonotonicity .
.
.
.
Finding a weaker constraint on gram-mars that still allows efficient processing is thus an impor-tant research objective."
Although it is intuitively plausiblethat the semantic ontent of subconstituents ought o play arole in the semantics of their combination--this is just akind of compositionality claim--there are certain cases inwhich reasonable linguistic analyses might violate thisintuition.
In general, these cases arise when a particularlexical item is stipulated to occur, the stipulation beingeither lexical (as in the case of particles or idioms) orgrammatical (as in the case of expletive xpressions).Second, the left-to-right scheduling of Earley parsing,geared as it is toward the structure of the string rather thanthat of its meaning, is inherently more appropriate forparsing than generation.
3 This manifests itself in an overlyhigh degree of nondeterminism in the generation process.For instance, various nondeterministic possibilities for gen-erating a noun phrase (using different cases, say) might beentertained merely because the NP occurs before the verbwhich would more fully specify, and therefore limit, theoptions.
This nondeterminism has been observed in prac-tice.2.3 SOURCE OF THE PROBLEMSWe can think of a parsing or generation process as discover-ing an analysis tree, 4 one admitted by the grammar andComputational Linguistics Volume 16, Number 1, March 1990 33Shieber et ai.
Semantic Head-Driven Grammarsatisfying certain syntactic or semantic onditions, by tra-versing a virtual tree and constructing the actual treeduring the traversal.
The conditions to be satisfied--possessing a given yield in the parsing case, or having a rootnode labeled with given semantic information in the case ofgeneration--reflect the different premises of the two typesof problems.
This perspective purposely abstracts i sues ofnondeterminism in the parsing or generation process, as itassumes an oracle to provide traversal steps that happen tomatch the ethereal virtual tree being constructed.
It is thisabstraction that makes it a useful expository device, butshould not be taken literally as a description of an algo-rithm.From this point of view, a naive top-down parser orgenerator performs a depth-first, left-to-right traversal ofthe tree.
Completion steps in Earley's algorithm, whetherused for parsing or generation, correspond to a post-ordertraversal (with prediction acting as a pre-order filter).
Theleft-to-right raversal order of both of these methods isgeared towards the given information in a parsing problem,the string, rather than that of a generation problem, thegoal logical form.
It is exactly this mismatch betweenstructure of the traversal and structure of the problempremise that accounts for the profligacy of these ap-proaches when used for generation.Thus, for generation, we want a traversal order geared tothe premise of the generation problem, that is, to thesemantic structure of the sentence.
The new algorithm isdesigned to reflect such a traversal strategy respecting thesemantic structure of the string being generated, ratherthan the string itself.3 THE NEW ALGORITHMGiven an analysis tree for a sentence, we define the pivotnode as the lowest node in the tree such that it and allhigher nodes up to the root have the same semantics.Intuitively speaking, the pivot serves as the semantic headof the root node.
Our traversal will proceed both top-downand bottom-up from the pivot, a sort of semantic-head-driven traversal of the tree.
The choice of this traversalallows a great reduction in the search for rules used to buildthe analysis tree.To be able to identify possible pivots, we distinguish asubset of the rules of the grammar, the chain rules, inwhich the semantics of some right-hand-side element isidentical to the semantics of the left-hand-side.
The right-hand-side lement will be called the rule's semantic head.The traversal, then, will work top-down from the pivotusing a nonchain rule, for if a chain rule were used, thepivot would not be the lowest node sharing semantics withthe root.
Instead, the pivot's emantic head would be.
Afterthe nonchain rule is chosen, each of its children must begenerated recursively.The bottom-up steps to connect he pivot to the root ofthe analysis tree can be restricted to chain rules only, as thepivot (along with all intermediate nodes) has the samesemantics as the root and must therefore be the semantichead.
Again, after a chain rule is chosen to move up onenode in the tree being constructed, the remaining (non-semantic-head) children must be generated recursively.The top-down base case occurs when the nonchain rulehas no nonterminal children; that is, it introduces lexicalmaterial only.
The bottom-up base case occurs when thepivot and root are trivially connected because they are oneand the; same node.An :interesting side issue arises when there are tworight-hand-side elements that are semantically identical tothe left-hand-side.
This provides ome freedom in choosingthe semantic head, although the choice is not withoutramifications.
For instance, in some analyses of NP struc-ture, a rule such asnp/NP --> det/NP, nbar/NP.is postulated.
In general, a chain rule is used bottom-upfrom its semantic head and top-down on the non-semantic-head siblings.
Thus, if a non-semantic-head subconstituenthas the same semantics as the left-hand-side, a recursivetop-down generation with the same semantics will be in-voked.
In theory, this can lead to nontermination, unlesssyntactic factors eliminate the recursion, as they would inthe rule above regardless of which element is chosen assemantic head.
In a rule for relative clause introductionsuch a,; the following (in highly abbreviated form)nbar/N--> nbar/N, sbar/N.we can (and must) choose the nominal as semantic head toeffect 'termination.
However, there are other problematiccases, such as verb-movement analyses of verb-second lan-guages.
We discuss this topic further in Section 4.3.3.1 A DCG IMPLEMENTATIONTo make the description more explicit, we will develop aProlog implementation of the algorithm for DCGs, alongthe way introducing some niceties of the algorithm previ-ously glossed over.As before, a term of the form node(Cat, P0-P) representsa phrase with the syntactic and semantic information givenby Cat starting at position P0 and ending at position P inthe string being generated.
As usual for DCGs, a stringposition is represented by the list of string elements afterthe position.
The generation process starts with a goalcategory and attempts to generate an appropriate node, inthe process instantiating the generated string.gen(Cat, String) :- generate(node(Cat, String.-\[ \])).To generate from a node, we nondeterministically choosea nonchain rule whose left-hand-side will serve as the pivot.For each right-hand-side element, we recursively generate,and then connect he pivot to the root.34 Computational Linguistics Volume 16, Number 1, March 1990Shieber et al Semantic Head-Driven Grammargenerate(Root) :-% choose nonchain ruleapplicable _ non _ chain _ rule(Root, Pivot, RHS),% generate all subconstituentsgenerate_ rhs(RHS),% generate material on path to rootconnect(Pivot, Root).The processing within generate_ rhs is a simple iteration.generate_ rhs(\[ \]).generate_ rhs(\[First \[ Rest\]) :-generate(First),generate_ rhs(Rest).The connection of a pivot to the root, as noted before,requires choice of a chain rule whose semantic head matchesthe pivot, and the recursive generation of the remainder ofits right-hand side.
We assume a predicate appl ica-ble_ chain_ rule(SemHead, LHS, Root, RHS) that holdsif there is a chain rule admitting a node LHS as theleft-hand side, SoreHead as its semantic head, and RHS asthe remaining right-hand-side nodes, such that the left-hand-side node and the root node Root can themselves beconnected.connect(Pivot, Root) :-% choose chain ruJeapplicable_ chain_ rule(Pivot, LHS, Root, RHS),% generate remaining siblingsgenerate _ rhs(RHS),% connect the newperent to the rootoonnect(LHS, Root).The base case occurs when the root and the pivot are thesame.
To implement the generator correctly, identity checkslike this one must use a sound unification algorithm withthe occurs check.
(The default unification in most Prologsystems is unsound in this respect.)
The reason is simple.Consider, for example, a grammar with a gap-threadingtreatment of wh-movement (Pereira 1981; Pereira andShieber 1985), which might include the rulenp(Affr, \[np(Agr)/SemJX\]-X)/Sem - -> \[ \].stating that an NP with agreement Agr and semantics Serecan be empty provided that the list of gaps in the NP can berepresented as the difference list \[np(Agr)/SemlX\]-X, thatis, the list containing an NP gap with the same agreementfeatures Agr.
Because the above rule is a nonchain rule, itwill be considered when trying to generate any nongap NP,such as the proper noun np(3-sing, G-G)/john.
The basecase of connect will try to unify that term with the head ofthe rule above, leading to the attempted unification of Xwith \[np(Agr)/SemlX\], an occurs-check failure that wouldnot be caught by the default Prolog unification algorithm.The base case, incorporating the explicit call to a soundunification algorithm, is therefore as follows:connect(Pivot, Root) :-% trivially connect pivot to rootunify(Pivot, Root).Now, we need only define the notion of an applicablechain or nonchain rule.
A nonchain rule is applicable if thesemantics of the left-hand side of the rule (which is tobecome the pivot) matches that of the root.
Further, werequire a top-down check that syntactically the pivot canserve as the semantic head of the root.
For this purpose, weassume a predicate chained_ nodes that codifies the tran-sitive closure of the semantic head relation over categories.This is the correlate of the link relation used in left-cornerparsers with top-down filtering; we direct the reader to thediscussion by Matsumoto et al (1983) or Pereira andShieber (1985) for further information.app l icab le  _ non _ cha in  _ rule(Root, Pivot, RHS) :-% semant ics  o f root  andp ivot  ere seraenode_  semantics(Root, Sem),node_ semantics(Pivot, Sere),% choose a nonchain ru\]enon _ chain _ rulo(LHS, RHS),% .
.
.
whose lhs matches the pivotunify(Pivot, LHS),% make sttre tile categories can connectchained_ nodes(Pivot, Root).A chain rule is applicable to connect a pivot to a root if thepivot can serve as the semantic head of the rule and theleft-hand side of the rule is appropriate for linking to theroot.applicable_ chain_ rule(Pivot, Parent, Root, RHS) :-% choose a chain rulechain_ rule(Parent, RHS, SemHead),% .
.
.
whose sere.
headmatchespivotunify(Pivot, SemHead),% make sure the categories can connectchained_ nodes(Parent, Root).The information eeded to guide the generation (givenas the predicates chain_ rule, non_ chain_ rule, andchained_ nodes) can be computed automatically from thegrammar.
A program to compile a DCG into these tableshas in fact been implemented.
The details of the processwill not be discussed further; interested readers may writeto the first author for the required Prolog code.3.2 A S IMPLE  EXAMPLEWe turn now to a simple example to give a sense of theorder of processing pursued by this generation algorithm.As in previous examples, the grammar fragment in Figure3 uses the infix operator / to separate syntactic and seman-tic category information, and subcategorization f rcomple-ments is performed lexically.Consider the generation from the category sentence /decl(call_ up(john,friends)).
The analysis tree that we willbe implicitly traversing in the course of generation is givenComputational Linguistics Volume 16, Number 1, March 1990 35Shieber et al Semantic Head-Driven Grammarsentence/decl(S)---> s(finite)/S.
(1)sentence/imp(S) --> vp(nonfinite,\[np(_ )/you\])/S.s(form)/S ---> Subj, vp(Form,\[Subj\])/S.
(2)vp(Forrn, Subcat)/S --->vp(Form,\[CompllSubcat\])/S, Compl.
(3)vp(Form,\[Subj\])/S ---> vp(Form,\[Subj\])/VP,adv(VP)/S.vp(finite,\[np(_)/O, np(3-sing)/S\])/ love(S,O)---> \[loves\].vp(finite,\[np(_)/O,p/up,np(3-sing)/Sl)/call-up(S,O) --->\[callsl.
(4)vp(finite,\[np(3-sing)/S\])/leave(S) --> \[leaves\].np(3-sinq)/john---> \[john\].
(5)np(3-pl)/friends ---> \[friends\].
(6)adv(VP)/often(VP)---> \[often\].det(3-sinq,X,P)/qterrn(every, X,P)---> \[every\].n(3-sing, X)/friend(X)--> \[friend\].Figure 3 Grammar Fragment for Simple Example?in Figure 4.
The rule numbers are keyed to the grammar.The pivots chosen during generation and the branchescorresponding to the semantic head relation are shown inboldface.We begin by attempting to find a nonchain rule that willdefine the pivot.
This is a rule whose left-hand-side s man-tics matches the root semantics decl(cal l_up( john,sentence\[a\] /dacl (call__up (John, friends) )S (finite)~\] /call_up(John,friends)np(3-s?ng)\[c\] / JohnJohnvp(finite,\[np(3-sing)/John\])\[d\] /call_up(John, friends)vp(finite,\[p/up, np(3-sing)/John\])\[?\] /call_up(John, friends)vp ( f in i te ,  \[np (3 -pZ) / f z iends ,  np (3-p l )p/up,  np (3 -s ing) / John \ ]  ) / fc?ends/ ca l l _up  ( J ohn, f f i ends  )p /up<7)\[g\] upFigure 4 Analysis Tree for Simple Example.friends)) (although its syntax may differ).
In fact, the onlysuc, h nonchain rule issentence/decl(S)---> s(finite)/S.
(1)We conjecture that the pivot is labeled sentence/decl(call_ up(j ohn, friends)).
In terms of the tree traversal,we arc: implicitly choosing the root node \[a\] as the pivot.We recursively generate from the child's node \[b\], whosecategory is s(finite)/call_up(john, friends).
For this cate-gory, the pivot (which will turn out to be node \[f\]) will bedefined by the nonchain rulevp(finite,\[np(_)/O,p/up, n (3-sinq)/S\]/call_up(S,O) ---> \[caUs\].
(4)(If there were other forms of the verb, these would bepotential candidates, but most would be eliminated by thechained_nodes check, as the semantic head relation re-quires identity of the verb form of a sentence and its VPhead.
See Section 4.2 for a technique for further educingthe nondeterminism in lexical item selection.)
Again, werecursively generate for all the nonterminal e ements of theright-hand side of this rule, of which there are none.We must therefore connect the pivot \[f\] to the root \[b\].
Achain rule whose semantic head matches the pivot must bechosen.
The only choice is the rulevp(Form,Subcat)/S ---> vp(Form,\[Cornpl\[Subcat\])/S, Cornpl.
(3)Unifying the pivot in, we find that we must recursivelygenerate the remaining RHS element np(_)/friends, andthen connect the left-hand-side node \[e\] with categoryvp(finite,\[lex/up, np(3-sinq}/john\])/call_up(john, frie ds)tO the same root \[b\].
The recursive generation yields a nodecovering the string "friends" following the previously gen-erated string "calls".
The recursive connection will use thesame chain rule, generating the particle "up", and the newnode to be connected \[d\].
This node requires the chain rules(Form)/S ---> Subj, vp(Form,\[Subj\])/S.
(2)for connection.
Again, the recursive generation for thesubject yields the string "John", and the new node to beconnected s(finite)/call_up(john, friends).
This last nodeconnects to the root \[b\] by virtue of identity.This completes the process of generating top-down fromthe original pivot sentence/decl(call_up(john,friends)).All that remains is to connect this pivot to the original root.Again., the process is trivial, by virtue of the base case forconnection.
The generation process is thus completed, yield-ing the string "John calls friends up".
The drawing inFigure 4 summarizes the generation process by showingwhich steps were performed top-down or bottom-up byarrows on the analysis tree branches.3.3 IMPORTANT PROPERTIES OF THE ALGORITHMThe grammar presented here was forced for expositoryreasons to be trivial.
(We have developed more extensiveexper!imental grammars that can generate relative clauseswith gaps and sentences with quantified NPs from quanti-36 Computational Linguistics Volume 16, Number 1, March 1990Shieber et al Semantic Head-Drlven Grammarfled logical forms by using a version of Cooper storage\[Cooper, 1983\].
An outline of our treatment of quantifica-tion is provided in Section 3.4.)
Nonetheless, everal impor-tant properties of the algorithm are exhibited even in thepreceding simple example.First, the order of processing isnot left-to-right.
The verbwas generated before any of its complements.
Because ofthis, full information about the subject, including agree-ment information, was available before it was generated.Thus, the nondeterminism that is an artifact of left-to-rightprocessing, and a source of inefficiency in the Earley gener-ator, is eliminated.
Indeed, the example here was com-pletely deterministic; all rule choices were forced.In addition, the semantic information about the particle"up" was available, even though this information appearsnowhere in the goal semantics.
That is, the generatoroperated appropriately despite a semantically nonmono-tonic grammar.Finally, even though much of the processing is top-down,left-recursive rules, even deadlock-prone rules (e.g.
rule(3)), are handled in a constrained manner by the algo-rithm.For these reasons, we feel that the semantic-head-drivenalgorithm is a significant improvement over top-down meth-ods and the previous bottom-up method based on Earleydeduction.3.4 A MORE COMPLEX EXAMPLE: QUANTIFIERSTORAGEWe will outline here how the new algorithm can generate,from a quantified logical form, sentences with quantifiedNPs one of whose readings is the original ogical form; thatis, how it performs quantifier lowering automatically.
Forthis, we will associate a quantifier store with certain catego-ries and add to the grammar suitable store manipulationrules.Each category whose constituents may create store ele-ments will have a store feature.
Furthermore, for each suchcategory whose semantics can be the scope of a quantifier,there will be an optional nonchain rule to take the topelement of an ordered store and apply it to the semantics ofthe category.
For example, here is the rule for sentences:s(Form, GO-G, Store)/quani(O,X,R,S) ---> (8)s(Form, GO-G, \[qterm(Q,X,R)lStore\])/S.The term quant(Q,X,R,S) represents a quantified formulawith quantifier Q, bound variable X, restriction R, andscope S; qterm(Q,X,R) is the corresponding store element.In addition, some mechanism is needed to combine thestores of the immediate constituents ofa phrase into a storefor the phrase.
For example, the combination of subject andcomplement s ores for a verb into a clause store is done inone of our test grammars by lexical rules such asvp(finite, \[np(_, SO)/O, np(3-sing, SS)/S\], SC)/gen(S,O) --> (9)\[generates\], I huffle(SS, SO, SC)}.which states that the store SC of a clause with main verb"love" and the stores SS and SO of the subject and objectthe verb subcategorizes for satisfy the constraint shuffle(SS, SO, SC), meaning that SC is an interleaving of ele-ments of SS and SO in their original order: Constraints ingrammar ules such as the one above are handled in thegenerator by the clausegenerate(lGoals}) :- call(Goals).which passes the conditions to Prolog for execution.
Thisextension must be used with great care, because it is ingeneral difficult to know the instantion state of such goalswhen they are called from the generator, and as notedbefore underinstantiated goals may lead to nontermination.A safer scheme would rely on delaying the execution ofgoals until their required instantiation patterns are satisfied(Naish 1986).Finally, it is necessary to deal with the noun phrases thatcreate store elements.
Ignoring the issue of how to treatquantifiers from within complex noun phrases, we needlexical rules for determiners, of the formdet(3-sing, X P,\[qterra(every, X,P)D/X -->\[every\].
(10)stating that the semantics of a quantified NP is simply thevariable bound by the store element arising from the NP.For rules of this form to work properly, it is essential thatdistinct bound logical-form variables be represented asdistinct constants in the terms encoding the logical forms.This is an instance of the problem of coherence discussed inSection 4.1.Figure 5 shows the analysis tree traversal for generatingthe sentence "No program generates every sentence" fromthe logical formdecl(quant(no,p,prog(p),quant(every, s,sent(s),gen(p,s))))The numbers labeling nodes in the figure correspond totree traversal order.
We will only discuss the aspects of thetraversal involving the new grammer rules given above.
Theremaining rules are like the ones in Figure 3, except hatnonterminals have an additional store argument wherenecessary.Pivot nodes \[b\] and \[c\] result from the application of rule(8) to reverse the unstoring of the quantifiers in the goallogical form.
The next pivot node is node \[j\], where rule (9)is applied.
For the application of this rule to terminate, it isnecessary that at least either the first two or the lastargument of the shuffle condition be instantiated.
Thepivot node must obtain the required store instantiationfrom the goal node being generated.
This happens automat-ically in the rule applicability check that identified thepivot, since the table chained_nodes identifies the storevariables for the goal and pivot nodes.
Given the sentencestore, the shuffle predicate nondeterministically generatesComputational Linguistics Volume 16, Number 1, March 1990 37Shieber et al Semantic Head-Driven Grammarsentence /\[a\] des1 (quant (no, p, prog (p),quant (eve~, s, sent (s), gen (p, s} } ) }TS (finite, \[\] } /\[b\] quant (no, p, prog (p),q~ant (eve~y, s, sent (s), gen (p, s) ) )~1 (8)\[el s ( f in i te ,  \ [~erm (no, p, pro~ (p)) \] ) 1quant (evQ~, ?, sent (s), gln (p, s) )~1 (s)\[d\] s (finite, \[qterm (no, p, prog (p)) ,c/term (every, s, sent (s} } \] )/gen 1\[p, s}\[c\] np(3-sing, \ [ q t e r m ~\[qt|no, p, prog (p) T ~ (finite, \[np(3-sing, \[qtermlno,p,prog (p)) \] )/p\],Y \[h\] n (3-sing, p)/prog (s} \[i\] \[qterm (no, p, prog (p) }, no ~ qterm (every, s, sent (s)) \] )/gen (p, s)~1 vPlfinite, \[npl3-slng, (qtemlevtry, s,stnt(sl) If/s, ~\] npl3-sing, \[qterm(every,s, sent (el) \])/snp (3-sing, (qterm (no, p, pzog (p)) \] ) /P l ,(qterm (no, p, prog (p)),q~ (ev?ry, s, s~ (=)) \] )/gs~ (p, e)~ (9) \[l\] det(3-slng, s,sent(s), \[m\] nbar(3-sing, s)/sent(s)\[qtem(*v.*~/,., .
*at (.))
\] ) I= tgen~es ~ (10) \[n\] n (3-s,:l.ng, 8)/Bast (.
)evcryFigure 5 Analysis Tree for Sentence with Quantifiers.the substores for the constituents subcategorized for by theverb.The next interesting event occurs at pivot node \[1\], whererule (10) is used to absorb the store for the object quantifiednoun phrase.
The bound variable for the stored quantifier,in this case s, must be the same as the meaning of the nounphrase and determiner.
6 This condition was already used tofilter out inappropriate shuffle results when node \[1\] wasselected as pivot for a noun phrase goal, again through thenonterminal argument identifications included in thechained_ nodes table.The rules outlined here are less efficient han they mightbe because during the distribution of store elements amongthe subject and complements of a verb no check is per-formed as to whether the variable bound by a store elementactually appears in the semantics of the phrase to which itis being assigned, leading to many dead ends in the genera-tion process.
Also, the rules are sound for generation butnot for analysis, because they do not enforce the constraintthat every occurrence of a variable in logical form beoutscoped by the variable's binder.
Adding appropriate sideconditions to the rules, following the constraints discussedby Hobbs and Shieber (1987) would not be difficult.4 EXTENSIONSTile basic semantic-head-driven g eration algorithm canbe augmented in various ways so as to encompass someimportant analyses and constraints.
In particular, we dis-cuss the incorporation of?
completeness and coherence constraints,?
the postponing of lexical choice, and?
the ability to handle certain problematic empty-headedphrases4.1 COMPLETENESS AND COHERENCEWedckind (1988) defines completeness and coherence of ageneration algorithm as follows.
Suppose a generator de-rives a string w from a logical form s, and the grammarassigns to w the logical form a.
The generator iscomplete ifs always subsumes a and coherent if a always subsumes s.The generator defined in Section 3.1 is not coherent orcomplete in this sense; it requires only that a and s becompatible, that is, unifiable.If the logical-form language and semantic interpretationsystem provide a sound treatment of variable binding and38 Computational Linguistics Volume 16, Number 1, March 1990Shieber et al Semantic Head-Driven Grammarscope, abstraction and application, then completeness andcoherence will be irrelevant because the logical form of anyphrase will not contain free variables.
However, neithersemantic projections in lexical-functional grammar (LFG;Halvorsen and Kaplan 1988) nor definite-clause grammarsprovide the means for such a sound treatment: logical-formvariables or missing arguments of predicates are bothencoded as unbound variables (attributes with unspecifiedvalues in the LFG semantic projection) at the descriptionlevel.
Under such conditions, completeness and coherencebecome important.
For example, suppose a grammar asso-ciated the following strings and logical forms.eat(john, X)'John ate'eat(john, banana)'John ate a banana'eat(john, nice(yellow(banana)))'John ate a nice yellow banana'The generator of Section 3.1 would generate any of thesesentences for the logical form eat(john, X) (because of itsincoherence) and would generate "John ate" for the logicalform eat(john, banana) (because of its incompleteness).Coherence can be achieved by removing the confusionbetween object-level and metalevel variables mentionedabove; that is, by treating logical-form variables as con-stants at the description level.
In practice, this can beachieved by replacing each variable in the semantics fromwhich we are generating by a new distinct constant (forinstance with the numbervars predicate built into someimplementations of Prolog).
These new constants will notunify with any augmentations to the semantics.
A suitablemodification of our generator would begen(Cat, String) :-cat _ semantics(Cat, Sere),numbervars(Sem,O, _)generate(node(Cat,String,\[ \])).This leaves us with the completeness problem.
Thisproblem arises when there are phrases whose semantics arenot ground at the description level, but instead subsume thegoal logical form or generation.
For instance, in our hypo-thetical example, the string "John eats" will be generatedfor semantics eat(john, banana).
The solution is to test atthe end of the generation procedure whether the featurestructure that is found is complete with respect o theoriginal feature structure.
However, because of the way inwhich top-down information is used, it is unclear whatsemantic information is derived by the rules themselves,and what semantic information is available because ofunifcations with the original semantics.
For this reason,"shadow" variables are added to the generator that repre-sent the feature structure derived by the grammar itself.Furthermore, a copy of the semantics of the original fea-ture structure ismade at the start of the generation process.Completeness is achieved by testing whether the semanticsof the shadow is subsumed by the copy.4.2 POSTPONING LEXICAL CHOICEAs it stands, the generation algorithm chooses particularlcxical forms on-line.
This approach can lead to a certainamount of unnecessary nondetcrminism.
The choice of aparticular form depends on the available semantic andsyntactic information.
Sometimes there is not enough infor-mation available to choose a form deterministically.
Forinstance, the choice of verb form might depend on syntacticfeatures of the verb's subject available only after the sub-ject has been generated.
This nondeterminism can be elim-inated by deferring lexical choice to a postprocess.
Inflec-tional and orthographical rules arc only applied when thegeneration process is finished and all syntactic features areknown.
In short, the generator will yield a list of lexicalitems instead of a list of words.
To this list the inflectionaland orthographical rules are applied.The MiMe2 system incorporates such a mechanism intothe previous generation algorithm quite successfully.
Exper-iments with particular grammars of Dutch, Spanish, andEnglish have shown that the delay mechanism results in agenerator that is faster by a factor of two or three on shortsentences.
Of course, the same mechanism could be addedto any of the other generation techniques discussed in thispaper; it is independent ofthe traversal order.The particular approach to delaying lcxical choice foundin the MiMe2 system relies on the structure of the system'smorphological component as presented in Figure 6.
Thefigure shows how inflectional rules, orthographical rules,morphology and syntax are related: orthographical rulesare applied to the results of inflectional rules.
These infec-tional rules are applied to the results of the morphologicalrules.
The result of the orthographical part are then inputfor the syntax.I Grammar of syntax and semantics.'/.:::..
:.$$I I ???
Two-level orthography :.~i~I I N?g4 Paradigmatic inflection N~ ..:.~.::'-:.
:.~::i:~:~:t.~.I Morphological unification grammar for Iderivations, compounds and lexical roles I!Lexicon of stems \[IFigure 6 Relation between MorphologicalComponents for Lexical Choice Delaying.Computational Linguistics Volume 16, Number 1, March 1990 39Shieber et al Semantic Head-Driven GrammarHowever, in the lexical-delayed scheme the inflectionaland orthographical rules are delayed.
During the genera-tion process the results of the morphological grammar areused directly.
We emphasize that this is possible onlybecause the inflectional and orthographical rules are mono-tonic, in the sense that they only further instantiate thefeature structure of a lexical item but do not change it.
Thisimplies, for example, that a rule that relates an active and apassive variant of a verb will not be an inflectional rule butrather a rule in the morphological grammar, although therule that builds a participle from a stem may in fact be aninflectional rule if it only instantiates the feature vform.When the generation process proper is finished the delayedrules are applied and the correct forms can be chosendeterministically.The delay mechanism is useful in the following twogeneral cases:First, the mechanism is useful if an inflectional variantdepends on syntatic features that are not yet available.
Theparticular choice of whether a verb has singular or pluralinflection depends on the syntactic agreement features ofits subject; these are only available after the subject hasbeen generated.
Other examples may include the particularchoice of personal and relative pronouns, and so forth.Second, delaying lexical choice is useful when there areseveral variants for some word that are equally possiblebecause they are semantically and syntactically identical.For example, a word may have several spelling variants.
Ifwe delay orthography then the generation process com-putes with only one "abstract" variant.
After the genera-tion process is completed, several variants can be filled infor this abstract one.
Examples from English include wordsthat take both regular and irregular tense forms (e.g.
"burned/burnt"); and variants uch as "traveller/traveler,"realize/realise," etc.4.3 EMPTY HEADSThe success of the generation algorithm presented herecomes about because lexical information is available assoon as possible.
Returning to the Dutch examples inSection 2. l, the list of subcategorization elements i usuallyknown in time.
Semantic heads can then deterministicallypick out their arguments.An example in which this is not the case is an analysis ofGerman and Dutch, where the position of the verb in rootsentences (the second position) is different from its positionin subordinates (the last position).
In most traditionalanalyses it is assumed that the verb in root sentences hasbeen "moved" from the final position to the second position.Koster (1975) argues for this analysis of Dutch.
Thus, asimple root sentence in German and Dutch is analyzed as inthe following examples:Vandaag kusti de man de vrouw, 6Today kisses the man the womanVandaag heefti de man de vrouw ?i gekustToday has the man the woman kissedVandaag \[ziet en hoort\]ide man de vrouw ~iToday sees and hears the man the womanIn DCG such an analysis can easily be defined by unifyingtile information on the verb in second position to someempty verb in final position, as exemplified by the simplegrammar for a Dutch fragment in Figure 7.
In this gram-mar, a special empty element is defined corresponding totile missing verb.
All information on the verb in secondposition is percolated through the rules to this empty verb.Therefore the definition of the several VP rules is valid forboth root and subordinate clauses.
7 The problem comesabout because the generator can (and must) at some pointpredict the empty verb as the pivot of the construction.However, in the definition of this empty verb no informa-tion (such as the list of complements) will get instantiated.Therefore, the VP complement rule (11) can be applied anunbounded number of times.
The length of the lists ofcomplements now is not known in advance, and the genera-tor will not terminate.Van Noord (1989a) proposes an ad hoc solution thatassumes that the empty verb is an inflectional variant of averb.
As inflection rules are delayed, the generation processacts as if the empty verb is an ordinary verb, therebycircumventing the problem.
However, this solution onlyworks if the head that is displaced is always lexical.
This isnot the case in general.
In Dutch the verb second positioncan not only be filled by lexical verbs but also by a conjunc-tion of verbs.
Similarly, Spanish clause structure can beanalyzed by assuming the "movement" of complex verbalconstructions to the second position.
Finally, in German itis possible to topicalize a verbal head.s2/Sem - - ->  adv(Arg)/Sem, e l /Arg .s l /Sem - - ->  v (A ,B ,n i l ) /V ,  sO(v(A,B)/V) /Sem.sO(V)/Sem - - ->  np/Np, vp(np/Np, \[\] ,V)/Sem.vp (Subj ,  T, V)/LF - -  ->np/H, vp(Subj,\[np/HlT\],V)/LF.vp(A,B.C)/D ---> v(A,B.C)/D.vp(A.B.C)/Sem ---> adv(Arg)/Sem, vp(A.B.C)/Arg.v(A,B.v(A.B)/Sem)/Sem---> \[\].np/ john-- -> \[john\].np/mary-- -> \[mary\].adv(Ar g)/today(Ar E ) ---> \[vandaag\] .v(np/S,\[np/O\],nil)/kisses(S,O) ---> \[kust\].Figure 7 Dutch Grammar Fragment.
(11)(12)40 Computational Linguistics Volume 16, Number 1, March 1990Shieber et al Semantic Head-Driven GrammarNote that in these problematic cases the head that lackssufficient information (the empty verb anaphor) is overtlyrealized in a position where there is enough information(the antecedent).
Thus it appears that the problem mightbe solved if the antecedent is generated before the anaphor.This is the case if the antecedent is the semantic head of theclause; the anaphor will then be instantiated via top-downinformation through the chained_nodes predicate.
How-ever, in the example grammar the antecedent is not neces-sarily the semantic head of the clause because of the VPmodifier ule (12).Typically, there is a relation between the empty anaphorand some antecedent expressed implicitly in the grammar;in the case at hand, it comes about by percolating theinformation through different rules from the antecedent tothe anaphor.
We propose to make this relation explicit bydefining an empty head with a Prolog clause using thepredicate head_ gap.head_gap(v(A,B, nil)/Sem,v(A,B,v(A,B)/Sem)/Sem).Such a definition can intuitively be understood as follows:once there is some node X (the first argument of head-_gap), then there could just as well have been the emptynode Y (the second argument of head_gap).
Note that alot of information is shared between the two nodes, therebymaking the relation between anaphor and antecedent ex-plicit.
Such rules can be incorporated in the generator byadding the following clause for connect:connect(Pivot, Root) :-head- gap(Pivot, Gap), connect(Gap, Boot).Note that the problem is now solved because the gap willonly be selected after its antecedent has been built.
Someparts of this antecedent are then unified with some parts ofthe gap.
The subcategorization list, for example, will thusbe instantiated in time.5 FURTHER RESEARCHWe mentioned earlier that, although the algorithm asstated is applicable specifically to generation, we expectthat it could be thought of as an instance of a uniformarchitecture for parsing and generation, as the Earleygeneration algorithm was.
Two pieces of evidence point thisway.First, Martin Kay (1990) has developed a parsing algo-rithm that seems to be the parsing correlate to the genera-tion algorithm presented here.
Its existence might point theway toward a uniform architecture.Second, one of us (van Noord 1989b) has developed ageneral proof procedure for Horn clauses that can serve asa skeleton for both a semantic-head-driven g erator and aleft-corner parser.
However, the parameterization is muchmore broad than for the uniform Earley architecture (Shie-ber 1988).Further enhancements to the algorithm are envisioned.First, any system making use of a tabular link predicateover complex nonterminals (like the chained_ nodes pred-icate used by the generation algorithm and including thelink predicate used in the BUP parser; Matsumoto et al1983) is subject to a problem of spurious redundancy inprocessing if the elements in the link table are not mutuallyexclusive.
For instance, asingle chain rule might be consid-ered to be applicable twice because of the nondeterminismof the call to chained_ nodes.
This general problem has todate received little attention, and no satisfactory solution isfound in the logic grammar literature.More generally, the backtracking regimen of our imple-mentation of the algorithm may lead to recomputation fresults.
Again, this is a general property of backtrackmethods and is not particular to our application.
The use ofdynamic programming techniques, as in chart parsing,would be an appropriate augmentation to the implementa-tion of the algorithm.
Happily, such an augmentationwould serve to eliminate the redundancy caused by thelinking relation as well.Finally, to incorporate a general facility for auxiliaryconditions in rules, some sort of delayed evaluation trig-gered by appropriate instantiation (e.g.
wait declarations;Naish 1986) would be desirable, as mentioned in Section3.4.
None of these changes, however, constitutes restructur-ing of the algorithm; rather, they modify its realization insignificant and important ways.ACKNOWLEDGMENTSThe research reported herein was primarily completed while Shieber andPereira were at the Artificial Intelligence C nter, SRI International.
Theyand Moore were supported in this work by a contract with the NipponTelephone and Telegraph Corporation and by a gift from the SystemsDevelopment Foundation aspart of a coordinated research effort with theCenter for the Study of Language and Information, Stanford University;van Noord was supported by the European Community and the Neder-lands Bureau veer Bibliotheekwezen en Informatieverzorgin through theEurotra project.
We would like to thank Mary Dalrymple and Louis desTombe for their helpful discussions regarding this work, the ArtificialIntelligence Center for their support of the research, and the participantsin the MiMe2 project, a research machine translation project of somemembers of Eurotra-Utrecht.REFERENCESCalder, J.; Reape, M.; and Zeevat, H. 1989 "An Algorithm for Genera-tion in Unification Categorial Grammar."
InProceedings of the 4thConference ofthe European Chapter of the Association for Computa-tional Linguistics, 233-240.Colmerauer, A.
1982 PROLOG II: Manuel de R6ference etMod61eTh6orique.
Technical report, Groupe de'Intelligence Artificielle, Fa-cult6 des Sciences de Luminy, Marseille, France.Cooper, R. 1983 "Quantification a d Syntactic Theory," Volume 21 ofSynthese Language Library.
D. Reidel, Dordrecht, the Netherlands.Dymetman, M. and Isabelle, P. 1988 "Reversible Logic Grammars forMachine Translation."
In Proceedings of the Second InternationalConference on Theoretical nd Methodological Issues in MachineTranslation of Natural Languages.Computational Linguistics Volume 16, Number 1, March 1990 41Shieber et al Semantic Head-Driven GrammarEvers, A.
1975.
The Transformational Cycle in German and Dutch.Ph.D.
Thesis, University of Utrecht, Utrecht, the Netherlands.Fodor, J. D. In press.
"Cross Serial Dependencies and SubcategorizationPercolation."
In R. Rieber (ed.
), CUNYForum, Volume 15.
CityUniversity of New York, New York.Halvorsen, P.-K. and Kaplan, R.M.
1988 "Projections and SemanticDescription in Lexieal-Functional Grammar."
In Proceedings of theInternational Conference on Fifth Generation Computer Systems,Tokyo, Japan, 1116-1122.Hirsh, S. 1987 "P-PATR, a Compiler for Unification Based Grammars,"In V. Dahl and P. Saint-Dizier (eds.
), Natural Language Understand-ing and Logic Programming, H. Elsevier Science Publishers, NewYork, NY: 63-78.Hobbs, J.R. and Shieber, S.M.
1987 An Algorithm for GeneratingQuantifier Scopings."
Computational Linguistics, 13:47-63.Huybrechts, R.A.C.
1984 "The Weak Inadequacy ofContext-Free PhraseStructure Grammars," In G. de Haan, M. Trommelen, and W.Zonneveld (eds.
), Van Periferie naar Kern.
Foris, Dordrecht, theNetherlands.Kay, M. 1990 "Head-Driven Parsing."
In M. Tomita (ed.
), Currentlssuesin Parsing Technology.
Klumer Academic Publishers, Dordrecht, heNetherlands.Koster, J.
1975 "Dutch as an SOV Language."
Linguistic Analysis,1:(2):111-136.Matsumoto, Y.; Tanaka, H.; Hirakawa, H.; Miyoshi, H.; and Yasukawa,H.
1983 "BUP: A Bottom-Up Parser Embedded in Prolog."
NewGeneration Computing, 1 (2): 145-158.Moortgat, M. 1984 "A Fregean Restriction on Meta-Rules" In Proceed-ings of New England Linguistic Society, 14:306-325.Naish, L. 1986 "Negation and Control in Prolog," Volume 238 of LectureNotes in Computer Science.
Springer-Verlag Berlin, F.R.G.van Noord, G. 1989a "BUG: A Directed Bottom-Up Generator forUnification Based Formalisms."
Working Papers in Natural LanguageProcessing 4, Katholieke Universiteit Leuven, Stichting Taaltechnolo-gie Utrecht, Utrecht, the Netherlands.van Noord, G. 1989b "An Overview of Head-Driven Bottom-UpGeneration."
In Proceedings of the Second European Workshop onNatural Language Generation, Edinburgh, Scotland.Pereira, F.C.N.
and Shieber, S.M.
1985 "Prolog and Natural-LanguageAnalysis," Volume 10 of CSLI Lecture Notes.
Center for the Study ofLanguage and Information, Stanford, CA.Pereira, F.C.N.
and Warren, D.H.D.
1983 "Parsing as Deduction."
InProceedings of the 21st Annual Meeting of the Association for Compu-tational Linguistics, 137-144.Pereira, F.C.N.
1981 "Extraposition Grammars."
Computational Linguis-tics, 7(4):243-256.Po'ilard, C. 1988 "Categorial Grammar and Phrase Structure Grammar:An \]Excursion on the Syntax-Semantics Frontier," In R. Oehrle, E.Bach, and D. Wheeler (eds.
), Categorial Grammars and NaturalLanguage Structures.
D. Reidel, Dordrecht, he Netherlands.Shieber, S.M.
1985a "An Introduction to Unification-Based Approachesto Grammar," Volume 4 of CSLI Lecture Notes.
Center for the Studyof Language and Information, Stanford, CA.Shieber, S.M.
1985b "Using Restriction to Extend Parsing Algorithms forComplex-Feature-Based Formalisms."
In Proceedings of the 23rd An-nual Meeting of the Association for Computational Linguistics, 145-152.Shieber, S.M.
1988 "A Uniform Architecture for Parsing and Generation.
"In Proceedings of the 12th International Conference on ComputationalLinguistics, 614-619.Stcedman, M. 1985 "Dependency and Coordination in the Grammar ofDutch and English."
Language, 61 (3):523-568.Strzalkowski, T. 1989 Automated Inversion of a Unification Parser into aUnification Generator.
Technical Report 465, Department of Com-puter Science, New York University, New York.Wedekind, J.
1988 "Generation as Structure Driven Derivation."
InProceedings of the 12th International Conference on ComputationalLinguistics, 732-737.NOTES1.
See for instance the text by Pereira and Shieber (1985) for anoverview and further eferences.2.
Again, see the text by Pereira and Shieber (1985, p.
172ff.)
andreferences therein.3.
Pereira and Warren (1983) point out that Earley deduction is notrestricted toa left-to-right expansion of goals, but this suggestion wasnot followed up with a specific algorithm addressing the problemsdiscussed here.4.
We use the term "analysis tree" rather than the more familiar "parsetree" to make clear that the source of the tree is not necessarily aparsing process; rather the tree serves only to codify a particularanalysis of the structure of the string.5.
Further details of the use of shuffle in scoping are given by Pereira ndShieber (1985).6.
This compels us to represent logical form bound variables as Prologconstants, in contrast to the standard practice in logic grammars.7.
For simplicity the grammar does not handle topicalization, but (coun-te:rfactually) assumes that the topic is some adverbial constituent.Topicalization can be handled by gap-threading (Pereira 1981; Pereiraarid Shieber 1985).42 Computational Linguistics Volume 16, Number 1, March 1990
