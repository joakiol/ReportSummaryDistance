Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1041?1050,Honolulu, October 2008. c?2008 Association for Computational LinguisticsUnsupervised Multilingual Learning for POS TaggingBenjamin Snyder and Tahira Naseem and Jacob Eisenstein and Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology77 Massachusetts Ave., Cambridge MA 02139{bsnyder, tahira, jacobe, regina}@csail.mit.eduAbstractWe demonstrate the effectiveness of multilin-gual learning for unsupervised part-of-speechtagging.
The key hypothesis of multilin-gual learning is that by combining cues frommultiple languages, the structure of each be-comes more apparent.
We formulate a hier-archical Bayesian model for jointly predictingbilingual streams of part-of-speech tags.
Themodel learns language-specific features whilecapturing cross-lingual patterns in tag distri-bution for aligned words.
Once the parame-ters of our model have been learned on bilin-gual parallel data, we evaluate its performanceon a held-out monolingual test set.
Our evalu-ation on six pairs of languages shows consis-tent and significant performance gains over astate-of-the-art monolingual baseline.
For onelanguage pair, we observe a relative reductionin error of 53%.1 IntroductionIn this paper, we explore the application of multilin-gual learning to part-of-speech tagging when no an-notation is available.
This core task has been studiedin an unsupervised monolingual framework for overa decade and is still an active area of research.
In thispaper, we demonstrate the effectiveness of multilin-gual learning when applied to both closely relatedand distantly related language pairs.
We further ana-lyze the language features which lead to robust bilin-gual performance.The fundamental idea upon which our work isbased is that the patterns of ambiguity inherent inpart-of-speech tag assignments differ across lan-guages.
At the lexical level, a word with part-of-speech tag ambiguity in one language may corre-spond to an unambiguous word in the other lan-guage.
For example, the word ?can?
in English mayfunction as an auxiliary verb, a noun, or a regularverb.
However, each of the corresponding functionsin Serbian is expressed with a distinct lexical item.Languages also differ in their patterns of structuralambiguity.
For example, the presence of an articlein English greatly reduces the ambiguity of the suc-ceeding tag.
In Serbian, a language without articles,this constraint is obviously absent.
The key idea ofmultilingual learning is that by combining cues frommultiple languages, the structure of each becomesmore apparent.While multilingual learning can address ambigu-ities in each language, it must be flexible enoughto accommodate cross-lingual variations such as taginventory and syntactic structure.
As a result ofsuch variations, two languages often select and ordertheir tags differently even when expressing the samemeaning.
A key challenge of multilingual learningis to model language-specific structure while allow-ing information to flow between languages.We jointly model bilingual part-of-speech tag se-quences in a hierarchical Bayesian framework.
Foreach word, we posit a hidden tag state which gen-erates the word as well as the succeeding tag.
Inaddition, the tags of words with common seman-tic or syntactic function in parallel sentences arecombined into bilingual nodes representing the tagpair.
These joined nodes serve as anchors that cre-ate probabilistic dependencies between the tag se-1041quences in each language.
We use standard toolsfrom machine translation to discover aligned word-pairs, and thereafter our model treats the alignmentsas observed data.Our model structure allows language-specific taginventories.
Additionally, it assumes only that thetags at joined nodes are correlated; they need not beidentical.
We factor the conditional probabilities ofjoined nodes into two individual transition probabil-ities as well as a coupling probability.
We definepriors over the transition, emission, and couplingparameters and perform Bayesian inference usingGibbs sampling and the Metropolis-Hastings algo-rithm.We evaluate our model on a parallel corpus offour languages: English, Bulgarian, Serbian, andSlovene.
For each of the six language pairs, wetrain a bilingual model on this corpus, and evaluate iton held-out monolingual test sets.
Our results showconsistent improvement over a monolingual baselinefor all languages and all pairings.
In fact, for onelanguage pair ?
Serbian and Slovene ?
the error isreduced by over 53%.
Moreover, the multilingualmodel significantly reduces the gap between unsu-pervised and supervised performance.
For instance,in the case of Slovene this gap is reduced by 71%.We also observe significant variation in the level ofimprovement across language pairs.
We show that across-lingual entropy measure corresponds with theobserved differentials in performance.2 Related WorkMultilingual Learning A number of approachesfor multilingual learning have focused on induc-ing cross-lingual structures, with applications tomachine translation.
Examples of such effortsinclude work on the induction of synchronousgrammars (Wu and Wong, 1998; Chiang, 2005)and learning multilingual lexical resources (Genzel,2005).Another thread of work using cross-lingual linkshas been in word-sense disambiguation, wheresenses of words can be defined based on their trans-lations (Brown et al, 1991; Dagan et al, 1991;Resnik and Yarowsky, 1997; Ng et al, 2003).When annotations for a task of interest are avail-able in a source language but are missing in thetarget language, the annotations can be projectedacross a parallel corpus (Yarowsky et al, 2000;Diab and Resnik, 2002; Pado?
and Lapata, 2006; Xiand Hwa, 2005).
In fact, projection methods havebeen used to train highly accurate part-of-speechtaggers (Yarowsky and Ngai, 2001; Feldman et al,2006).
In contrast, our own work assumes that an-notations exist for neither language.Finally, there has been recent work on applyingunsupervised multilingual learning to morphologi-cal segmentation (Snyder and Barzilay, 2008).
Inthis paper, we demonstrate that unsupervised mul-tilingual learning can be successfully applied to thesentence-level task of part-of-speech tagging.Unsupervised Part-of-Speech Tagging Sincethe work of Merialdo (1994), the HMM has been themodel of choice for unsupervised tagging (Bankoand Moore, 2004).
Recent advances in theseapproaches include the use of a fully BayesianHMM (Johnson, 2007; Goldwater and Griffiths,2007).
In very recent work, Toutanova and John-son (2008) depart from this framework and proposean LDA-based generative model that groups wordsthrough a latent layer of ambiguity classes therebyleveraging morphological features.
In addition, anumber of approaches have focused on develop-ing discriminative approaches for unsupervised andsemi-supervised tagging (Smith and Eisner, 2005;Haghighi and Klein, 2006).Our focus is on developing a simple model thateffectively incorporates multilingual evidence.
Weview this direction as orthogonal to refining mono-lingual tagging models for any particular language.3 ModelWe propose a bilingual model for unsupervised part-of-speech tagging that jointly tags parallel streamsof text in two languages.
Once the parameters havebeen learned using an untagged bilingual paralleltext, the model is applied to a held-out monolingualtest set.Our key hypothesis is that the patterns of ambigu-ity found in each language at the part-of-speech levelwill differ in systematic ways; by considering multi-ple language simultaneously, the total inherent am-biguity can be reduced in each language.
The modelis designed to permit information to flow across the1042I love fishJ' adore les poissonsx1y1x2y2 y3 y4x3I love fishJ' adore les poissonsx1/y1 x1/y1 x1/y1y3(a) (b)Figure 1: (a) Graphical structure of two standard monolingual HMM?s.
(b) Graphical structure of our bilingual modelbased on word alignments.language barrier, while respecting language-specificidiosyncrasies such as tag inventory, selection, andorder.
We assume that for pairs of words that sharesimilar semantic or syntactic function, the associ-ated tags will be statistically correlated, though notnecessarily identical.
We use such word pairs asthe bilingual anchors of our model, allowing cross-lingual information to be shared via joint tagging de-cisions.
We use standard tools from machine trans-lation to identify these aligned words, and thereafterour model treats them as fixed and observed data.To avoid cycles, we remove crossing edges from thealignments.For unaligned parts of the sentence, the tag andword selections are identical to standard monolin-gual HMM?s.
Figure 1 shows an example of thebilingual graphical structure we use, in comparisonto two independent monolingual HMM?s.We formulate a hierarchical Bayesian model thatexploits both language-specific and cross-lingualpatterns to explain the observed bilingual sentences.We present a generative story in which the observedwords are produced by the hidden tags and modelparameters.
In Section 4, we describe how to in-fer the posterior distribution over these hidden vari-ables, given the observations.3.1 Generative ModelOur generative model assumes the existence of twotagsets, T and T ?, and two vocabularies W and W ?,one of each for each language.
For ease of exposi-tion, we formulate our model with bigram tag de-pendencies.
However, in our experiments we useda trigram model, which is a trivial extension of themodel discussed here and in the next section.1.
For each tag t ?
T , draw a transition distri-bution ?t over tags T , and an emission distri-bution ?t over words W , both from symmetricDirichlet priors.12.
For each tag t ?
T ?, draw a transition distri-bution ?
?t over tags T ?, and an emission distri-bution ?
?t over words W ?, both from symmetricDirichlet priors.3.
Draw a bilingual coupling distribution ?
overtag pairs T ?
T ?
from a symmetric Dirichletprior.4.
For each bilingual parallel sentence:(a) Draw an alignment a from an alignmentdistribution A (see the following para-graph for formal definitions of a and A),(b) Draw a bilingual sequence of part-of-speech tags (x1, ..., xm), (y1, ..., yn) ac-cording to:P (x1, ..., xm, y1, ..., yn|a, ?, ?
?, ?).
2This joint distribution is given in equa-tion 1.1The Dirichlet is a probability distribution over the simplex,and is conjugate to the multinomial (Gelman et al, 2004).2Note that we use a special end state rather than explicitlymodeling sentence length.
Thus the values of m and n dependon the draw.1043(c) For each part-of-speech tag xi in the firstlanguage, emit a word from W : ei ?
?xi ,(d) For each part-of-speech tag yj in the sec-ond language, emit a word from W ?
: fj ??
?yj .We define an alignment a to be a set of one-to-one integer pairs with no crossing edges.
Intuitively,each pair (i, j) ?
a indicates that the words ei andfj share some common role in the bilingual paral-lel sentences.
In our experiments, we assume thatalignments are directly observed and we hold themfixed.
From the perspective of our generative model,we treat alignments as drawn from a distribution A,about which we remain largely agnostic.
We onlyrequire that A assign zero probability to alignmentswhich either: (i) align a single index in one languageto multiple indices in the other language or (ii) con-tain crossing edges.
The resulting alignments arethus one-to-one, contain no crossing edges, and maybe sparse or even possibly empty.
Our technique forobtaining alignments that display these properties isdescribed in Section 5.Given an alignment a and sets of transition param-eters ?
and ?
?, we factor the conditional probabilityof a bilingual tag sequence (x1, ...xm), (y1, ..., yn)into transition probabilities for unaligned tags, andjoint probabilities over aligned tag pairs:P (x1, ..., xm, y1, ..., yn|a, ?, ?
?, ?)
=?unaligned i?xi?1(xi) ?
?unaligned j?
?yj?1(yj) ??
(i,j)?aP (xi, yj |xi?1, yj?1, ?, ?
?, ?
)(1)Because the alignment contains no crossingedges, we can model the tags as generated sequen-tially by a stochastic process.
We define the dis-tribution over aligned tag pairs to be a product ofeach language?s transition probability and the cou-pling probability:P (xi, yj |xi?1, yj?1, ?, ?
?, ?)
=?xi?1(xi) ?
?yj?1(yj) ?
(xi, yj)Z (2)The normalization constant here is defined as:Z =?x,y?xi?1(x) ?
?yj?1(y) ?
(x, y)This factorization allows the language-specific tran-sition probabilities to be shared across aligned andunaligned tags.
In the latter case, the addition ofthe coupling parameter ?
gives the tag pair an addi-tional role: that of multilingual anchor.
In essence,the probability of the aligned tag pair is a productof three experts: the two transition parameters andthe coupling parameter.
Thus, the combination ofa high probability transition in one language and ahigh probability coupling can resolve cases of inher-ent transition uncertainty in the other language.
Inaddition, any one of the three parameters can ?veto?a tag pair to which it assigns low probability.To perform inference in this model, we predictthe bilingual tag sequences with maximal probabil-ity given the observed words and alignments, whileintegrating over the transition, emission, and cou-pling parameters.
To do so, we use a combination ofsampling-based techniques.4 InferenceThe core element of our inference procedure isGibbs sampling (Geman and Geman, 1984).
Gibbssampling begins by randomly initializing all unob-served random variables; at each iteration, each ran-dom variable zi is sampled from the conditional dis-tribution P (zi|z?i), where z?i refers to all variablesother than zi.
Eventually, the distribution over sam-ples drawn from this process will converge to theunconditional joint distribution P (z) of the unob-served variables.
When possible, we avoid explic-itly sampling variables which are not of direct inter-est, but rather integrate over them?this techniqueis known as ?collapsed sampling,?
and can reducevariance (Liu, 1994).We sample: (i) the bilingual tag sequences (x,y),(ii) the two sets of transition parameters ?
and ?
?,and (iii) the coupling parameter ?.
We integrate overthe emission parameters ?
and ?
?, whose priors areDirichlet distributions with hyperparameters ?0 and??0.
The resulting emission distribution over wordsei, given the other words e?i, the tag sequences x1044and the emission prior ?0, can easily be derived as:P (ei|x, e?i, ?0) =?
?xi?xi(ei)P (?xi |?0) d?xi= n(xi, ei) + ?0n(xi) + Wxi?0(3)Here, n(xi) is the number of occurrences of thetag xi in x?i, n(xi, ei) is the number of occurrencesof the tag-word pair (xi, ei) in (x?i, e?i), and Wxiis the number of word types in the vocabulary Wthat can take tag xi.
The integral is tractable dueto Dirichlet-multinomial conjugacy (Gelman et al,2004).We will now discuss, in turn, each of the variablesthat we sample.
Note that in all cases we condi-tion on the other sampled variables as well as theobserved words and alignments, e, f and a, whichare kept fixed throughout.4.1 Sampling Part-of-speech TagsThis section presents the conditional distributionsthat we sample from to obtain the part-of-speechtags.
Depending on the alignment, there are severalscenarios.
In the simplest case, both the tag to besampled and its succeeding tag are not aligned toany tag in the other language.
If so, the samplingdistribution is identical to the monolingual case, in-cluding only terms for the emission (defined in equa-tion 3), and the preceding and succeeding transi-tions:P (xi|x?i, y, e, f, a, ?, ?
?, ?, ?0, ?
?0) ?P (ei|x, e?i, ?0) ?xi?1(xi) ?xi(xi+1).For an aligned tag pair (xi, yj), we sample theidentity of the tags jointly.
By applying the chainrule we obtain terms for the emissions in both lan-guages and a joint term for the transition probabili-ties:P (xi, yj |x?i, y?j , e, f, a, ?, ?
?, ?, ?0, ?
?0) ?P (ei|x, e?i, ?0)P (fj |y, f?j , ?
?0)P (xi, yj |x?i, y?j , a, ?, ?
?, ?
)The expansion of the joint term depends on thealignment of the succeeding tags.
In the case thatthe successors are not aligned, we have a product ofthe bilingual coupling probability and four transitionprobabilities (preceding and succeeding transitionsin each language):P (xi, yj |x?i, y?j , a, ?, ?
?, ?)
??
(xi, yj)?xi?1(xi) ?
?yj?1(yj) ?xi(xi+1) ?
?yj (yj+1)Whenever one or more of the succeeding tags isaligned, the sampling formulas must account for theeffect of the sampled tag on the joint probabilityof the succeeding tags, which is no longer a sim-ple multinomial transition probability.
We give theformula for one such case?when we are samplingan aligned tag pair (xi, yj), whose succeeding tags(xi+1, yj+1) are also aligned to one another:P (xi, yj |x?i, y?j , a, ?, ?
?, ?)
?
?
(xi, yj)?
?xi?1(xi)??yj?1(yj)[?xi(xi+1)?
?yj (yj+1)?x,y ?xi(x)?
?yj (y)?
(x, y)]Similar equations can be derived for cases wherethe succeeding tags are not aligned to each other, butto other tags.4.2 Sampling Transition Parameters and theCoupling ParameterWhen computing the joint probability of an alignedtag pair (Equation 2), we employ the transition pa-rameters ?, ??
and the coupling parameter ?
in a nor-malized product.
Because of this, we can no longerregard these parameters as simple multinomials, andthus can no longer sample them using the standardclosed formulas.Instead, to resample these parameters, we re-sort to the Metropolis-Hastings algorithm as a sub-routine within Gibbs sampling (Hastings, 1970).Metropolis-Hastings is a Markov chain samplingtechnique that can be used when it is impossible todirectly sample from the posterior.
Instead, sam-ples are drawn from a proposal distribution and thenstochastically accepted or rejected on the basis of:their likelihood, their probability under the proposaldistribution, and the likelihood and proposal proba-bility of the previous sample.We use a form of Metropolis-Hastings known asan independent sampler.
In this setup, the proposaldistribution does not depend on the value of theprevious sample, although the accept/reject decision1045does depend on the previous model likelihood.
Moreformally, if we denote the proposal distribution asQ(z), the target distribution as P (z), and the previ-ous sample as z, then the probability of accepting anew sample z?
?
Q is set at:min{1, P (z?)
Q(z)P (z) Q(z?
)}Theoretically any non-degenerate proposal distri-bution may be used.
However, a higher acceptancerate and faster convergence is achieved when theproposal Q is a close approximation of P .
For a par-ticular transition parameter ?x, we define our pro-posal distribution Q to be Dirichlet with parametersset to the bigram counts of the tags following x inthe sampled tag data.
Thus, the proposal distribu-tion for ?x has a mean proportional to these counts,and is thus likely to be a good approximation to thetarget distribution.Likewise for the coupling parameter ?, we de-fine a Dirichlet proposal distribution.
This Dirichletis parameterized by the counts of aligned tag pairs(x, y) in the current set of tag samples.
Since thissets the mean of the proposal to be proportional tothese counts, this too is likely to be a good approxi-mation to the target distribution.4.3 Hyperparameter Re-estimationAfter every iteration of Gibbs sampling the hyper-parameters ?0 and ?
?0 are re-estimated using a singleMetropolis-Hastings move.
The proposal distribu-tion is set to a Gaussian with mean at the currentvalue and variance equal to one tenth of the mean.5 Experimental Set-UpOur evaluation framework follows the standard pro-cedures established for unsupervised part-of-speechtagging.
Given a tag dictionary (i.e., a set of possi-ble tags for each word type), the model has to selectthe appropriate tag for each token occurring in a text.We also evaluate tagger performance when only in-complete dictionaries are available (Smith and Eis-ner, 2005; Goldwater and Griffiths, 2007).
In bothscenarios, the model is trained only using untaggedtext.In this section, we first describe the parallel dataand part-of-speech annotations used for system eval-uation.
Next we describe a monolingual base-line and our procedures for initialization and hyper-parameter setting.Data As a source of parallel data, we use Orwell?snovel ?Nineteen Eighty Four?
in the original Englishas well as translations to three Slavic languages ?Bulgarian, Serbian and Slovene.
This data is dis-tributed as part of the Multext-East corpus whichis publicly available.
The corpus provides detailedmorphological annotation at the world level, includ-ing part-of-speech tags.
In addition a lexicon foreach language is provided.We obtain six parallel corpora by consideringall pairings of the four languages.
We computeword level alignments for each language pair usingGiza++.
To generate one-to-one alignments at theword level, we intersect the one-to-many alignmentsgoing in each direction and automatically removecrossing edges in the order in which they appear leftto right.
This process results in alignment of abouthalf the tokens in each bilingual parallel corpus.
Wetreat the alignments as fixed and observed variablesthroughout the training procedure.The corpus consists of 94,725 English words (seeTable 2).
For every language, a random three quar-ters of the data are used for learning the model whilethe remaining quarter is used for testing.
In the testset, only monolingual information is made availableto the model, in order to simulate future performanceon non-parallel data.Tokens Tags/TokenSR 89,051 1.41SL 91,724 1.40BG 80,757 1.34EN 94,725 2.58Table 2: Corpus statistics: SR=Serbian, SL=Slovene,EN=English, BG=BulgarianTagset The Multext-East corpus is manually an-notated with detailed morphosyntactic information.In our experiments, we focus on the main syntac-tic category encoded as a first letter of the labels.The annotation distinguishes between 13 parts-of-speech, of which 11 are common for all languages1046Random Monolingual Unsupervised Monolingual Supervised Trigram EntropyEN 56.24 90.71 96.97 1.558BG 82.68 88.88 96.96 1.708SL 84.70 87.41 97.31 1.703SR 83.41 85.05 96.72 1.789Table 1: Monolingual tagging accuracy for English, Bulgarian, Slovene, and Serbian for two unsupervised baselines(random tag selection and a Bayesian HMM (Goldwater and Griffiths, 2007)) as well as a supervised HMM.
Inaddition, the trigram part-of-speech tag entropy is given for each language.in our experiments.3In the Multext-East corpus, punctuation marks arenot annotated.
We expand the tag repository bydefining a separate tag for all punctuation marks.This allows the model to make use of any transitionor coupling patterns involving punctuation marks.We do not consider punctuation tokens when com-puting model accuracy.Table 2 shows the tag/token ratio for these lan-guages.
For Slavic languages, we use the tag dic-tionaries provided with the corpus.
For English,we use a different process for dictionary construc-tion.
Using the original dictionary would result inthe tag/token ratio of 1.5, in comparison to the ra-tio of 2.3 observed in the Wall Street Journal (WSJ)corpus.
To make our results on English tagging morecomparable to previous benchmarks, we expand theoriginal dictionary of English tags by merging itwith the tags from the WSJ dictionary.
This processresults in a tag/token ratio of 2.58, yielding a slightlymore ambiguous dictionary than the one used in pre-vious tagging work.
4Monolingual Baseline As our monolingual base-line we use the unsupervised Bayesian HMM modelof Goldwater and Griffiths (2007) (BHMM1).
Thismodel modifies the standard HMM by adding pri-ors and by performing Bayesian inference.
Its is inline with state-of-the-art unsupervised models.
Thismodel is a particulary informative baseline, sinceour model reduces to this baseline model when thereare no alignments in the data.
This implies that anyperformance gain over the baseline can only be at-3The remaining two tags are Particle and Determiner; TheEnglish tagset does not include Particle while the other threelanguages Serbian, Slovene and Bulgarian do not have Deter-miner in their tagset.4We couldn?t perform the same dictionary expansion for theSlavic languages due to a lack of additional annotated resources.tributed to the multilingual aspect of our model.
Weused our own implementation after verifying that itsperformance on WSJ was identical to that reportedin (Goldwater and Griffiths, 2007).Supervised Performance In order to provide apoint of comparison, we also provide supervised re-sults when an annotated corpus is provided.
We usethe standard supervised HMM with Viterbi decod-ing.Training and Testing Framework Initially, allwords are assigned tags randomly from their tagdictionaries.
During each iteration of the sam-pler, aligned tag pairs and unaligned tags are sam-pled from their respective distributions given in Sec-tion 4.1 above.
The hyperparameters ?0 and ?
?0 areinitialized with the values learned during monolin-gual training.
They are re-estimated after every iter-ation of the sampler using the Metropolis Hastingsalgorithm.
The parameters ?
and ??
are initiallyset to trigram counts and the ?
parameter is set totag pair counts of aligned pairs.
After every 40 it-erations of the sampler, a Metropolis Hastings sub-routine is invoked that re-estimates these parametersbased on the current counts.
Overall, the algorithmis run for 1000 iterations of tag sampling, by whichtime the resulting log-likelihood converges to stablevalues.
Each Metropolis Hastings subroutine sam-ples 20 values, with an acceptance ratio of around1/6, in line with the standard recommended values.After training, trigram and word emission prob-abilities are computed based on the counts of tagsassigned in the final iteration.
For smoothing, thefinal sampled values of the hyperparameters areused.
The highest probability tag sequences for eachmonolingual test set are then predicted using trigramViterbi decoding.
We report results averaged overfive complete runs of all experiments.10476 ResultsComplete Tag Dictionary In our first experiment,we assume that a complete dictionary listing the pos-sible tags for every word is provided in each lan-guage.
Table 1 shows the monolingual results of arandom baseline, an unsupervised Bayesian HMMand a supervised HMM.
Table 3 show the resultsof our bilingual models for different language pair-ings while repeating the monolingual unsupervisedresults from Table 1 for easy comparison.
The finalcolumn indicates the absolute gain in performanceover this monolingual baseline.Across all language pairs, the bilingual modelconsistently outperforms the monolingual baseline.All the improvements are statistically significant bya Fisher sign test at p < 0.05.
For some lan-guage pairs, the gains are quite high.
For instance,the pairing of Serbian and Slovene (two closely re-lated languages) yields absolute improvements of6.7 and 7.7 percentage points, corresponding to rel-ative reductions in error of 51.4% and 53.2%.
Pair-ing Bulgarian and English (two distantly related lan-guages) also yields large gains: 5.6 and 1.3 percent-age points, corresponding to relative reductions inerror of 50% and 14%, respectively.5When we compare the best bilingual result foreach language (Table 3, in bold) to the monolin-gual supervised results (Table 1), we find that forall languages the gap between supervised and un-supervised learning is reduced significantly.
For En-glish, this gap is reduced by 21%.
For the Slavic lan-guages, the supervised-unsupervised gap is reducedby even larger amounts: 57%, 69%, and 78% forSerbian, Bulgarian, and Slovene respectively.While all the languages benefit from the bilin-gual learning framework, some language combina-tions are more effective than others.
Slovene, for in-stance, achieves a large improvement when pairedwith Serbian (+7.7), a closely related Slavic lan-guage, but only a minor improvement when coupled5The accuracy of the monolingual English tagger is rela-tively high compared to the 87% reported by (Goldwater andGriffiths, 2007) on the WSJ corpus.
We attribute this discrep-ancy to the slight differences in tag inventory used in our data-set.
For example, when Particles and Prepositions are mergedin the WSJ corpus (as they happen to be in our tag inventoryand corpus), the performance of Goldwater?s model on WSJ issimilar to what we report here.Entropy Mono- Bilingual Absolutelingual GainEN 0.566 90.71 91.01 +0.30SR 0.554 85.05 90.06 +5.03EN 0.578 90.71 92.00 +1.29BG 0.543 88.88 94.48 +5.61EN 0.571 90.71 92.01 +1.30SL 0.568 87.41 88.54 +1.13SL 0.494 87.41 95.10 +7.69SR 0.478 85.05 91.75 +6.70BG 0.568 88.88 91.95 +3.08SR 0.588 85.05 86.58 +1.53BG 0.579 88.88 90.91 +2.04SL 0.609 87.41 88.20 +0.79Table 3: The tagging accuracy of our bilingual modelson different language pairs, when a full tag dictionary isprovided.
The Monolingual Unsupervised results fromTable 1 are repeated for easy comparison.
The first col-umn shows the cross-lingual entropy of a tag when thetag of the aligned word in the other language is known.The final column shows the absolute improvement overthe monolingual Bayesian HMM.
The best result for eachlanguage is shown in boldface.with English (+1.3).
On the other hand, for Bulgar-ian, the best performance is achieved when couplingwith English (+5.6) rather than with closely relatedSlavic languages (+3.1 and +2.4).
As these resultsshow, an optimal pairing cannot be predicted basedsolely on the family connection of paired languages.To gain a better understanding of this variationin performance, we measured the internal tag en-tropy of each language as well as the cross-lingualtag entropy of language pairs.
For the first measure,we computed the conditional entropy of a tag de-cision given the previous two tags.
Intuitively, thisshould correspond to the inherent structural uncer-tainty of part-of-speech decisions in a language.
Infact, as Table 1 shows, the trigram entropy is a goodindicator of the relative performance of the mono-lingual baseline.
To measure the cross-lingual tagentropies of language pairs, we considered all bilin-gual aligned tag pairs, and computed the conditionalentropy of the tags in one language given the tagsin the other language.
This measure should indi-cate the amount of information that one language ina pair can provide the other.
The results of this anal-1048Mono- Bilingual Absolutelingual GainEN 63.57 68.22 +4.66SR 41.14 54.73 +13.59EN 63.57 71.34 +7.78BG 53.19 62.55 +9.37EN 63.57 66.48 +2.91SL 49.90 53.77 +3.88SL 49.90 59.68 +9.78SR 41.14 54.08 +12.94BG 53.19 54.22 +1.04SR 41.14 56.91 +15.77BG 53.19 55.88 +2.70SL 49.90 58.50 +8.60Table 4: Tagging accuracy for Bilingual models with re-duced dictionary: Lexicon entries are available for onlythe 100 most frequent words, while all other words be-come fully ambiguous.
The improvement over the mono-lingual Bayesian HMM trained under similar circum-stances is shown.
The best result for each language isshown in boldface.ysis are given in the first column of Table 3.
We ob-serve that the cross-lingual entropy is lowest for theSerbian and Slovene pair, corresponding with theirlarge gain in performance.
Bulgarian, on the otherhand, has lowest cross-lingual entropy when pairedwith English.
This corresponds with the fact thatEnglish provides Bulgarian with its largest perfor-mance gain.
In general, we find that the largest per-formance gain for any language is achieved whenminimizing its cross-lingual entropy.Reduced Tag Dictionary We also conducted ex-periments to investigate the impact of the dictio-nary size on the performance of the bilingual model.Here, we provide results for the realistic scenariowhere only a very small dictionary is present.
Ta-ble 4 shows the performance when a tag dictionaryfor the 100 most frequent words is present in eachlanguage.
The bilingual model?s results are consis-tently and significantly better than the monolingualbaseline for all language pairs.7 ConclusionWe have demonstrated the effectiveness of multilin-gual learning for unsupervised part-of-speech tag-ging.
The key hypothesis of multilingual learn-ing is that by combining cues from multiple lan-guages, the structure of each becomes more appar-ent.
We formulated a hierarchical Bayesian modelfor jointly predicting bilingual streams of tags.
Themodel learns language-specific features while cap-turing cross-lingual patterns in tag distribution.
Ourevaluation shows significant performance gains overa state-of-the-art monolingual baseline.AcknowledgmentsThe authors acknowledge the support of the NationalScience Foundation (CAREER grant IIS-0448168 andgrant IIS-0835445) and the Microsoft Research FacultyFellowship.
Thanks to Michael Collins, Amir Glober-son, Lillian Lee, Yoong Keok Lee, Maria Polinsky andthe anonymous reviewers for helpful comments and sug-gestions.
Any opinions, findings, and conclusions or rec-ommendations expressed above are those of the authorsand do not necessarily reflect the views of the NSF.ReferencesMichele Banko and Robert C. Moore.
2004.
Part-of-speech tagging in context.
In Proceedings of the COL-ING, pages 556?561.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1991.
Word-sense dis-ambiguation using statistical methods.
In Proceedingsof the ACL, pages 264?270.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe ACL, pages 263?270.Ido Dagan, Alon Itai, and Ulrike Schwall.
1991.
Twolanguages are more informative than one.
In Proceed-ings of the ACL, pages 130?137.Mona Diab and Philip Resnik.
2002.
An unsupervisedmethod for word sense tagging using parallel corpora.In Proceedings of the ACL, pages 255?262.Anna Feldman, Jirka Hana, and Chris Brew.
2006.A cross-language approach to rapid creation of newmorpho-syntactically annotated resources.
In Pro-ceedings of LREC, pages 549?554.Andrew Gelman, John B. Carlin, Hal .S.
Stern, and Don-ald .B.
Rubin.
2004.
Bayesian data analysis.
Chap-man and Hall/CRC.S.
Geman and D. Geman.
1984.
Stochastic relaxation,Gibbs distributions, and the Bayesian restoration ofimages.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 6:721?741.1049Dmitriy Genzel.
2005.
Inducing a multilingual dictio-nary from a parallel multitext in related languages.
InProceedings of HLT/EMNLP, pages 875?882.Sharon Goldwater and Thomas L. Griffiths.
2007.A fully Bayesian approach to unsupervised part-of-speech tagging.
In Proceedings of the ACL, pages744?751.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
Proceedings of HLT-NAACL, pages 320?327.W.
K. Hastings.
1970.
Monte carlo sampling meth-ods using Markov chains and their applications.Biometrika, 57:97?109.Mark Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers?
In Proceedings of EMNLP/CoNLL,pages 296?305.Jun S. Liu.
1994.
The collapsed Gibbs sampler inBayesian computations with applications to a generegulation problem.
Journal of the American Statis-tical Association, 89(427):958?966.Bernard Merialdo.
1994.
Tagging english text witha probabilistic model.
Computational Linguistics,20(2):155?171.Hwee Tou Ng, Bin Wang, and Yee Seng Chan.
2003.
Ex-ploiting parallel texts for word sense disambiguation:an empirical study.
In Proceedings of the ACL, pages455?462.Sebastian Pado?
and Mirella Lapata.
2006.
Optimal con-stituent alignment with edge covers for semantic pro-jection.
In Proceedings of ACL, pages 1161 ?
1168.Philip Resnik and David Yarowsky.
1997.
A perspectiveon word sense disambiguation methods and their eval-uation.
In Proceedings of the ACL SIGLEX Workshopon Tagging Text with Lexical Semantics: Why, What,and How?, pages 79?86.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: Training log-linear models on unlabeled data.In Proceedings of the ACL, pages 354?362.Benjamin Snyder and Regina Barzilay.
2008.
Unsuper-vised multilingual learning for morphological segmen-tation.
In Proceedings of the ACL/HLT, pages 737?745.Kristina Toutanova and Mark Johnson.
2008.
ABayesian lda-based model for semi-supervised part-of-speech tagging.
In Advances in Neural InformationProcessing Systems 20, pages 1521?1528.
MIT Press.Dekai Wu and Hongsing Wong.
1998.
Machine trans-lation with a stochastic grammatical channel.
In Pro-ceedings of the ACL/COLING, pages 1408?1415.Chenhai Xi and Rebecca Hwa.
2005.
A backoff modelfor bootstrapping resources for non-english languages.In Proceedings of EMNLP, pages 851 ?
858.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual pos taggers and np bracketers via robust pro-jection across aligned corpora.
In Proceedings of theNAACL, pages 1?8.David Yarowsky, Grace Ngai, and Richard Wicentowski.2000.
Inducing multilingual text analysis tools via ro-bust projection across aligned corpora.
In Proceedingsof HLT, pages 161?168.1050
