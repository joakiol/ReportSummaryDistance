Japanese Morphological Analyzer using Word Co-occurrence- -  JTAG-Takeshi FUCHINTT Information and CommunicationSystems LaboratoriesHikari-no-oka 1-1Yokosuka 239-0847, Japan,fuchi@isl.ntt.co.jpShinichiro TAKAGINTr Information and Communication SystemsLaboratoriesHikari-no-oka 1-1Yokosuka 239-0847, Japan,takagi@nttnly.isl.ntt.co.jpAbstractWe developed a Japanese morphologicalanalyzer that uses the co-occurrence ofwords to select the correct sequence ofwords in an unsegmented Japanese sentence.The co-occurrence information can beobtained from cases where the systemincorrectly analyzes sentences.
As theamount of information increases, theaccuracy of the system increases with asmall risk of degradation.
Experimentalresults show that the proposed systemassigns the correct phonologicalrepresentations to unsegmented Japanesesentences more precisely than do otherpopular systems.IntroductionIn natural language processing for Japanese text,morphological analysis is very important.Currently, there are two main methods forautomatic part-of-speech tagging, namely, corpus-based and rule-based methods.
The corpus-basedmethod is popular for European languages.Samuelsson and Voutilainen (1997), however,show significantly higher achievement of a rule-based tagger than that of statistical taggers forEnglish text.
On the other hand, most Japanesetaggers I are rule-based.
In previous Japanesetaggers, it was difficult o increase the accuracy ofthe analysis.
Takeuchi and Matsumoto (1995)combined a rule-based and a corpus-based method,i In this paper, a tagger is identical to amorphological nalyzer.resulting in a marginal increase in the accuracy oftheir taggers.
However, this increase is stillinsufficient.
The source of the trouble is thedifficulty in adjusting the grammar and parameters.Our tagger is also rule-based.
By using the co-occurrence of words, it reduces the difficulty andgenerates a continuous increase in its accuracy.The proposed system analyzes unsegmentedJapanese sentences and segments hem into words.Each word has a part-of-speech and phonologicalrepresentation.
Our tagger has the co-occurrenceinformation of words in its dictionary.
Theinformation can be adjusted concretely by hand ineach case of incorrect analysis.
Concreteadjustment is different from detailed adjustment.
Imust be easy to understand for people who makeadjustments to the system.
The effect of oneadjustment is concrete but small.
Therefore, muchmanual work is needed.
However, the work is sosimple and easy.Section 1 shows the drawbacks to previoussystems.
Section 2 describes the outline of theproposed system.
In Section 3, the accuracy of thesystem is compared with that of others.
In addition,we show the change in the accuracy while thesystem is being adjusted.1 Previous Japanese MorphologicalAnalyzersMost Japanese morphological analyzers uselinguistic grammar, generate possible sequences ofwords from an input string, and select a sequence.The following axe methods for selecting thesequence:?
Choose the sequence that has a longer word onthe right-hand side.
(right longest matchprinciple)409/?
Choose the sequence that has a longer word onthe left-hand side.
(left longest matchprinciple)?
Choose the sequence that has the least numberof phrases.
(least number of phrasesprinciple)?
Choose the sequence that has the leastconnective-cost of words.
(least connective-cost principle)?
Use pattern matching of words and/or parts-of-speech to specify the priority of sequences.?
Choose the sequence that contains modifiersand modifiees.?
Choose the sequence that contains words usedfrequently.In practice, combinations of the above methodsare used.Using these methods, many Japanesemorphological analyzers have been created.However, the accuracy cannot increasecontinuously in spite of careful manualadjustments and statistical adjustments.
The causeof incorrect analyses is not only unregisteredwords, in fact, many sentences are analyzedincorrectly even though there is a sufficientvocabulary for the sentences in their dictionaries.In this case, the system generates a correctsequence but does not select it.
Parameters such asthe priorities of words and connective-costsbetween parts-of-speech, can be adjusted so thatthe correct sequence is selected.
However, thisadjustment often causes incorrect side effects andthe system analyzes other sentences incorrectlythat have already been analyzed correctly.
Thisphenomenon is called 'degrading'.In addition to parameter adjustment, parts-of-speech may need to be expanded.
Both operationsare almost impossible to complete by people whoare not very familiar with the system.
If thesystem uses a complex algorithm to select asequence of words, even the system developer canhardly grasp the behaviour of the system.These operations begin to become more thanwhat a few experts can handle becausevocabularies in the systems are big.
Even to addan unregistered word to a dictionary, operatorsmust have good knowledge of parts-of-speech, t epriorities of words, and word classification formodifiers and modifiees.
In this situation, it isdifficult to increase the number of operators.
Thisis situation with previous analyzers.Unfortunately, current statistical taggers cannotavoid this situation.
The tuning of the systems isvery subtle.
It is hard to predict the effect ofparameter tuning of the systems.
To avoid thissituation, our tagger uses the co-occurrence ofwords whose effect is easy to understand.2 Overview of our systemWe developed the Japanese morphologicalanalyzer, JTAG, paying attention to simplealgorithm, straightforward adjustment, andflexible grammar.The features of JTAG are the followings.?
An attribute value is an atom.In our system, each word has several attributevalues.
An attribute value is limited so as not tohave structure.
Giving an attribute value to wordsis equivalent to naming the words as a group.?
New attribute values can be introduced easily.An attribute value is a simple character string.When a new attribute value is required, the userwrites a new string in the attribute field of a recordin a dictionary.?
The number of attribute values is unlimited.?
A part-of-speech is a kind of attribute value.?
Grammar is a set of connection rules.Grammar is implemented with connection rulesbetween attribute values.
List 1 is an example 2.One connection rule is written in one line.
Thefields are separated by commas.
Attribute valuesof a word on the left are written in the first field.Attribute values of a word on the right are writtenin the second field.
In the last field, the cost 3 of therule is written.
Attribute values are separated bycolons.
A minus sign '-' means negation.For example, the fn'st rule shows that a wordwith 'Noun' can be followed by a word withNoun,  Case:ConVerb, 50Noun:Name, Postfix:Noun, 100Noun:-Name, Postfix:Noun, 90Copula:de, VerbStem:Lde, 50List 1: Connection rules.2 Actual rules use Japanese characters.3 The cost figures were intuitively determined.
Thegrammar is used mainly to generate possible sequencesof words, so the determination f the cost figures wasnot very subtle.
The precise selection of the correct410VocabularyStandard WordsOutput WordsSegmentationSegmentation &Part-of-SpeechSegmentation &PhonemeSegmentation &Phoneme &Part-of-SpeechJTAG350K 710K 115K118091185598.9% 199.3%98.8% 199.2%98.8% 199.2%98.7% 1 99.1%9830986498.9% 1 99.3%98.3% 198.7%98.2% 198.6%98.0 % 1 98.3 %9901994898.5% 198.9%97.6% 198.1%97.5% 197.9%97.1% 197.6%Table H: Accuracy per word (precision I recall)'Case' and 'ConVerb'.
The cost of the rule is 50.The second rule shows that a word with 'Noun'and 'Name' can be followed by a word with'Postfix' and 'Noun'.
The cost is 100.
The thirdrule shows that a word that has 'Noun' and doesnot have 'Name' can be followed by a word with'Postfix' and 'Noun'.
The cost is 90.Only the word '"C' has the combination of'Copula' and 'de', so the fourth rule is specific to?
The co-occurrence of words.In our system, the sequence of words thatincludes the maximum number of co-occurrenceof words is selected.
Table I shows examples ofrecords in a dictionary.
'~ '  means 'amount', 'frame', 'forehead' or ahuman name 'Gaku'.
In the co-occurrence field,words are presented irectly.
If there are no co-occurrence words in a sentence that includes '~\[~','amount' is selected because its cost is thesmallest.
If ' ,~'(picture) is in the sentence,'frame' is selected.?
Selection AlgorithmJTAG selects the correct sequence of wordsusing connective-cost, the number of co-occurrences, the priority of words, and the lengthof words.
The precise description of the algolithmis shown in the Appendix.This algolithrn is too simple to analyzeJapanese sentences perfectly.
However, it issufficient in practice.sequence is done by the co-occurrence of words.3 EvaluationIn this section, Japanese morphologicalanayzers are evaluated using the following :?
Segmentation?
Part-of-speech tagging?
Phonological representationFLAG, is compared with JUMAN 4 andCHASEN 5.
A single "correct analysis" ismeaningless because these taggers use differentparts-of-speech, grammars, and segmentationpolicies.
We checked the outputs of each andselected the incorrect analyses that the grammarmaker of each system must not expect.3.1 ComparisonTo make the output of each system comparable,we reduce them to 21 parts-of-speech and 14 verb-inflection-types.
In addition, we assume that thepart-of-speech of unrecognized words is Noun.The segmentation policies are not unified.Therefore, the number of words in sentences idifferent from each other.Table II shows the system accuracy.
We used500 sentences 6 (19,519 characters) in the EDR 7corpus.
For segmentation, the accuracy of JTAG is4 JUMAN Version 3.4.http://www-nagao.kuee.kyoto-u.ac.jp/index-e~tml5 CHASEN Version 1.5.1.http://cactus.aist-nara.ac.jp/lab/nlt/chasen.html6 The sentences do not include Arabic numeralsbecause ~ 'MAN and CHASEN do not assignphonological representation to them.7 Japan Electronic Dictionary Research Institute.http://www.iijnet.or.jp/edr/411I ~ JTAG I JUMAN CHASENI C?nversi?nRati?
I 88"5% I 71.7% 72.3%Processin~ Time 86see 576see 335seeTable HI: Correct phonological representationper sentence.
Average 38 characters in onesentence.
Sun Ultra-1 170Mhz.the same as that of JUMAN.
Table II shows thatJTAG assigns the correct phonologicalrepresentations to unsegmented Japanesesentences more precisely than do the othersystems.Table 1TI shows the ratio of sentences that areconverted to the correct phonologicalrepresentation where segmentation errors areignored.
80,000 sentences s (3,038,713 characters,no Arabic numerals) were used in the EDR corpus.The average number of characters in one sentenceis 38.
JTAG converts 88.5% of sentences correctly.The ratio is much higher than that of the othersystems.Table III also shows the processing time ofeach system.
JTAG analyzes Japanese text morethan do four times faster than the other taggers.The simplicity of the JTAG selection algorithmcontributes tothe fast processing speed.3.2 Ad jus tment  ProcessTo show the adjustablity of JTAG, we tuned itfor a specific set of 10,000 sentences 9.
Theaverage number of words in a sentence is 21.Graph 1 shows the transition of the number ofsentences converted correctly to theirphonological representation.
We finished theadjustment when the system could no longer betuned in the framework of JTAG.
The lastaccuracy rating (99.8% per sentence) shows themaximum ability of JTAG.The feature of each phase of the adjustment isdescribed below.Phase I .
In this phase, the grammar of JTAG waschanged.
New attribute values were introducedand the costs of connection rules were changed.s In the EDR corpus, 2.3% of sentences have errorsand 1.5% of sentences have phonologicalrepresentation i consistencies.
In this case, thesentences are not revised.9 311,330 characters without Arabic numerals.Average 31 characters per sentence.
In this case, wefixed all errors of the sentences and the inconsistencyof their phonological representation.02OZI H HI IV100013 ~9800970096009500 ~940093009200 q9 lO0 ustment9000o 50 IOO 150 200Duration of Adjustment (honr~Graph 1: Transition of the number ofsentences correctly converted tophonological representation.These adjustments caused large occurrences ofdegradation i  our tagger.Phase \]l. The grammar was almost fixed.
One ofthe authors added unregistered words to thedictionaries, changed the costs of registered words,and supplied the information of the co-occurrenceof words.
The changes in the costs of wordscaused asmall degree of degradation.Phase II1.
In this phase, all unrecognized wordswere registered together.
The unrecognized wordswere extracted automatically and checkedmanually.
The time taken for this phase is theduration of the checking.Phase IV.
Mainly, co-occurrence information wassupplied.
This phase caused some degradation, butthese instances were very small.Graph 1 shows that JTAG converts 91.9% ofopen sentences to the correct phonologicalrepresentation, and 99.8% of closed sentences.Without he co-occurrence information, the ratio is97.5%.
Therefore, the co-occurrence informationcorrects 2.3% of the sentences.
Without newregistered words, the ratio is 95.6%, sounrecognized words caused an error in 4.2% of theonversions:urrence~nal wordsSentences ErrorsUnrecognized Words 4.2% 52%Co-occurrence 2.3% 28%Others 1.6% 20%Total 8.1% 100%Table IV: Causes of errors.412sentences.
Table IV shows the percentages of thecauses .ConclusionWe developed a Japanese morphologicalanalyzer that analyzes unsegmented Japanesesentences more precisely than other popularanalyzers.
Our system uses the co-occurrence ofwords to select he correct sequence of words.
Theefficiency of the co-occurrence information wasshown through experimental results.
The precisionof our current agger is 98.7% and the recall is99.1%.
The accuracy of the tagger can beexpected to increase because the risk ofdegradation is small when using the co-occurrenceinformation.ReferencesYoshimura K, Hitaka T. and Yoshida S. (1983)Morphological Analysis of Non-marked-off JapaneseSentences by the Least BUNSETSU's NumberMethod.
Trans.
IPSJ, Vol.24, No.l, pp.40-46.
(inJapanese)Miyazaki M. and Ooyama Y.
(1986) Linguistic Methodfor a Japanese Text to Speech System.
Trans.
IPSJ,Voi.27, No.1 I, pp.1053-1059.
(in Japanese)Hisamitsu T. and Nitta Y.
(1990) MorphologicalAnalysis by Minimum Connective-Cost Method.SIGNLC 90-8, IEICE, pp.17-24.
(in Japanese)Brill E. (1992) A simple rule-based part of speechtagger.
Procs.
Of 3 'd Conference on Applied NauralLanguage Processing, ACL.Maruyama M. and Ogino S. (1994) JapaneseMorphological Analysis Based on Regular Grammar.Trans.
IPSJ, Vol.35, No.7, pp.1293-1299.
(inJapanese)Nagata M. (1994) A Stochastic JapaneseMorphological Analyzer Using a Forward-DPBackward-A* N-Best Search Algorithm.Computational Linguistics, COLING, pp.201-207.Fuchi T. and Yonezawa M. (1995) A MorphemeGrammar for Japanese Morphological Analyzers.Journal of Natural Language Processing, TheAssociation for Natural Language Processing, Vo12,No.4, pp.37-65.Pierre C. and Tapanainen P. (1995) Tagging French -comparing a statical and a constraint-based method.Procs.
Of 7 ~ Conference of the European Chapter ofthe ACL, ACL, pp.149-156.Takeuehi K. and Matsumoto Y.
(1995) HMMParameter Learning for Japanese MorphologicalAnalyzer.
Proes.
Of 10 ~ Pacific Asia ConferenceLanguage, Information and Computation, pp.163-172.Voutilainen A.
(1995) A syntax-based part of speechanalyser.
Procs.
Of 7 ~ Conference of the EuropeanChapter of the Association for ComputationalLinguistics, ACL, pp.157-164.Matsuoka K., Takeishi E. and Asano H. (1996) NaturalLanguage Processing in a Japanese Text-To-SpeechSystem for Written-style Texts.
Procs.
Of 3 ~ IEEEWorkshop On Interactive Voice Technology ForTelecommunications Applications, IEEE, pp.33-36.Samuelsson C. and Voutilainen A.
(1997) Comparing aLinguistic and a Stochastic Tagger.
Procs.
Of 35 ~Annual Meeting of the Association forComputational Linguistics, ACL.AppendixELEMENT se lec t ion(SET  sequences)  \[ELEMENT selected;int best_ to ta l _connect ive_cost  -MAX_ INT;int best_number_o f_cooc  - -1;int best_ to ta l _word_cost  - -i;int best_number_o f_2character_word  - -i;foreach s (sequences) {s .
to ta l _connect ive_cost- sum_of_connect ive_cost (s ) ;if (bes t_ to ta l _connect ive_cost> s .
to ta l _connect ive_cost )  \[bes t_ to ta l _connect ive_cost- s .
to ta l _connect ive_cost ;se lected  - s; \]}foreach s (sequences) \[if ( s .
to ta l _connect ive_cost- bes t_ to ta l _connect ive_cost> PRUNE_RANGE)  \[sequcences .de le te (s ) ;  \]\]foreaoh s (sequences) \[s .number_o f_cooc= count_cooccurence_o f_words(s ) ;if (bes t_number_o f_cooc< s .number_o f_cooc)  \[bes t_number_o f_cooc- s .number_of_cooc;se lected - s; \]\]foreaoh s (sequences) \[if ( s .number_o f_cooc< best_number_o f_cooc)  \[sequoences .de le te (s ) ;  \]}foreach s (sequences) \[s .
to ta l _word_cost- sum_of_word_cost (s ) ;if (bes t_ to ta l _word_cost> s .
to ta l _word_cost )  \[bes t_ to ta l _word_cost- s .
to ta l _word_cost ;se lected  - s; }}foreach s (sequences) \[if ( s .
to ta l _word_cost> best_ to ta l _word_cost )  {sequcences .de le te (s ) ;  }\]foreach s (sequences) \[s .number_o f_2character_word- count_2character_word(s ) ;if (bes t_number_o f_2character_word< s .number_o f_2character_word)  {best_number_o f_2character_word- s .number_o f_2character_word ;se lected  - s; \]\]return selected;413
