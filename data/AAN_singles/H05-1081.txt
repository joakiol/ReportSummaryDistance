Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 644?651, Vancouver, October 2005. c?2005 Association for Computational LinguisticsA Robust Combination Strategy for Semantic Role LabelingLlu?
?s Ma`rquez, Mihai Surdeanu, Pere Comas, and Jordi TurmoTechnical University of CatalunyaBarcelona, Spain{lluism,surdeanu,pcomas,turmo}@lsi.upc.eduAbstractThis paper focuses on semantic role la-beling using automatically-generated syn-tactic information.
A simple and robuststrategy for system combination is pre-sented, which allows to partially recoverfrom input parsing errors and to signif-icantly boost results of individual sys-tems.
This combination scheme is alsovery flexible since the individual systemsare not required to provide any informa-tion other than their solution.
Extensiveexperimental evaluation in the CoNLL-2005 shared task framework supports ourprevious claims.
The proposed architec-ture outperforms the best results reportedin that evaluation exercise.1 IntroductionThe task of Semantic Role Labeling (SRL), i.e.the process of detecting basic event structuressuch as who did what to whom, when and where,has received considerable interest in the past fewyears (Gildea and Jurafsky, 2002; Surdeanu et al,2003; Xue and Palmer, 2004; Pradhan et al, 2005a;Carreras and Ma`rquez, 2005).
It was shown thatthe identification of such event frames has a signif-icant contribution for many Natural Language Pro-cessing (NLP) applications such as Information Ex-traction (Surdeanu et al, 2003) and Question An-swering (Narayanan and Harabagiu, 2004).Most current SRL approaches can be classifiedin one of two classes: approaches that take ad-vantage of complete syntactic analysis of text, pi-oneered by Gildea and Jurafsky (2002), and ap-proaches that use partial syntactic analysis, cham-pioned by previous evaluations performed withinthe Conference on Computational Natural LanguageLearning (CoNLL) (Carreras and Ma`rquez, 2004).The wisdom extracted from this volume of work in-dicates that full syntactic analysis has a significantcontribution to the SRL performance, when usinghand-corrected syntactic information.On the other hand, when only automatically-generated syntax is available, the quality of the in-formation provided through full syntax decreasesbecause the state-of-the-art of full parsing is lessrobust and performs worse than the tools used forpartial syntactic analysis.
Under such real-worldconditions, the difference between the two SRL ap-proaches (with full or partial syntax) is not that high.More interestingly, the two SRL strategies performbetter for different semantic roles.
For example,models that use full syntax recognize better agentand theme roles, whereas models based on partialsyntax are better at recognizing explicit patient roles,which tend to be farther from the predicate and accu-mulate more parsing errors (Ma`rquez et al, 2005).The above observations motivate the work pre-sented in this paper.
We introduce a novel semanticrole labeling approach that combines several indi-vidual SRL systems.
Intuitively, our approach canbe separated in two stages: a candidate generationphase, where the solutions provided by several indi-vidual models are combined into a pool of candidatearguments, and an inference phase, where the candi-dates are filtered using a binary classifier, and possi-644The luxury auto maker last year sold 1,214 cars in the U.S.PPNPVPNPNPSARG0 ARGM?TMP P ARG1 ARGM?LOCFigure 1: Sample PropBank sentence.ble conflicts with domain knowledge constraints areresolved to obtain the final solution.For robustness, the inference model uses onlyglobal attributes extracted from the solutions pro-vided by the individual systems, e.g., the sequenceof role labels generated by each system for the cur-rent predicate.
We do not use any attributes spe-cific to the individual models, not even the confi-dence assigned by the individual classifiers.
Besidessimplicity, the consequence of this decision is thatour approach does not impose any restrictions on theindividual SRL strategies, as long as one solutionis provided for each predicate.
On the other hand,probabilistic inference processes, which have beensuccessfully used for SRL (Koomen et al, 2005),mandate that each individual candidate argument beassociated with its raw activation, or confidence, inthe given model.
However, this information is notdirectly available in two out of three of our individ-ual models, which classify argument chunks and notentire arguments.Despite its simplicity, our approach obtains en-couraging results: the combined system outperformsany of the individual systems and, using exactly thesame data, it is also competitive with the best SRLsystems that participated in the latest CoNLL sharedtask evaluation (Carreras and Ma`rquez, 2005).2 Semantic CorporaIn this paper we report results using PropBank, anapproximately one-million-word corpus annotatedwith predicate-argument structures (Kingsbury etal., 2002).
To date, PropBank addresses mainlypredicates lexicalized by verbs and a small num-ber of predicates lexicalized by verb nominalizationsand adjectives.The arguments of each predicate are numbered se-quentially from ARG0 to ARG5.
Generally, ARG0stands for agent, ARG1 for theme or direct ob-ject, and ARG2 for indirect object, benefactive orinstrument, but mnemonics tend to be verb spe-cific.
Additionally, predicates might have ?adjunc-tive arguments?, referred to as ARGMs.
For example,ARGM-LOC indicates a locative and ARGM-TMP in-dicates a temporal.
Figure 1 shows a sample sen-tence where one predicate (?sold?)
has 4 arguments.In a departure from ?traditional?
SRL approachesthat train on the hand-corrected syntactic trees as-sociated with PropBank, we do not use any syn-tactic information from PropBank.
Instead, wedevelop our models using automatically-generatedsyntax and named-entity (NE) labels, made avail-able by the CoNLL shared task evaluation (Carrerasand Ma`rquez, 2005).
From the CoNLL data, ourindividual models based on full syntactic analysisuse the trees generated by the Charniak parser.
Thepartial-syntax model uses the chunk?
i.e.
basic syn-tactic phrase ?
labels and clause boundaries.
All in-dividual models make use of the provided NE labels.Following the CoNLL-2005 setting we evaluatedour system also on a fresh test set, derived from theBrown corpus.
This second evaluation allows us tore-enforce our robustness claim.3 Approach OverviewThe proposed architecture, summarized in Figure 2,consists of two stages: a candidate generation phaseand an inference stage.In the candidate generation step, we merge the so-lutions of three individual SRL models into a uniquepool of candidate arguments.
The proposed modelsrange from complete reliance on full parsing to us-ing only partial syntactic information.
The first twomodels, Model 1 and 2, are developed as sequentialtaggers (using the BIO tagging scheme) on a sharedframework.
The major difference between the twomodels is that Model 1 uses only partial syntacticinformation (basic phrases and clause boundaries),whereas Model 2 uses complete syntactic informa-tion.
To maximize diversity, Model 3 implementsa different strategy: it models only arguments thatmap into exactly one syntactic constituent.
Section 4details all three individual models.The inference stage starts with candidate filtering,645Candidate FilteringReliance on full syntaxModel 1 Model 2 Model 3Conflict ResolutionInferenceCandidateGenerationFigure 2: Architecture of the proposed system.which reduces the number of candidate argumentsin the pool using a single binary classifier.
Usingthis classifier?s confidence values and a number ofdomain-specific constraints, e.g.
no two argumentscan overlap, the conflict resolution component en-forces the consistency of the final solution using astraightforward greedy strategy.
The complete in-ference model is detailed in Section 5.4 Individual SRL ModelsModels 1 and 2.
These models approach SRL asa sequential tagging task.
In a pre-process step, theinput syntactic structures are traversed in order toselect a subset of constituents organized sequentially(i.e.
non embedding).
Model 1 makes use only ofthe partial tree defined by base chunks and clauseboundaries, while Model 2 explores full parse trees.Precisely, the sequential tokens are selected as fol-lows.
First, the input sentence is splitted into dis-joint segments by considering the clause boundariesgiven by the syntactic structure.
Second, for eachsegment, the set of top-most non-overlapping syn-tactic constituents completely falling inside the seg-ment are selected as tokens.
Note that this strategyprovides a set of sequential tokens covering the com-plete sentence.
Also, it is independent of the syn-tactic annotation explored, given it provides clauseboundaries ?
see (Ma`rquez et al, 2005) for moredetails.Due to this pre-processing stage, the upper-boundrecall figures are 95.67% for Model 1 and 90.32%for Model 2 using the datasets defined in Section 6.The nodes selected are labeled with B-I-O tags(depending if they are at the beginning, inside, oroutside of a predicate argument) and they are con-verted into training examples by considering a richset of features, mainly borrowed from state-of-the-art systems.
These features codify properties from:(a) the argument constituent, (b) the target predicate,Constituent type and head: extracted using common head-word rules.
If the first element is a PP chunk, then thehead of the first NP is extracted.First and last words and POS tags of the constituent.POS sequence: if it is less than 5 tags long.2/3/4-grams of the POS sequence.Bag-of-words of nouns, adjectives, and adverbs.TOP sequence: sequence of types of the top-most syntacticelements in the constituent (if it is less than 5 elements long).In the case of full parsing this corresponds to the right-handside of the rule expanding the constituent node.2/3/4-grams of the TOP sequence.Governing category as in (Gildea and Jurafsky, 2002).NamedEnt, indicating if the constituent embeds orstrictly matches a named entity along with its type.TMP, indicating if the constituent embeds or strictly matchesa temporal keyword (extracted from AM-TMP arguments ofthe training set).Previous and following words and POS of the constituent.The same features characterizing focus constituents areextracted for the two previous and following tokens, providedthey are inside the clause boundaries of the codified region.Table 1: Constituent structure features: Models 1/2Predicate form, lemma, and POS tag.Chunk type and type of verb phrase in which verb isincluded: single-word or multi-word.The predicate voice.
We currently distinguish five voicetypes: active, passive, copulative, infinitive, and progressive.Binary flag indicating if the verb is a start/end of a clause.Sub-categorization rule, i.e.
the phrase structure rule thatexpands the predicate immediate parent.Table 2: Predicate structure features: Models 1/2and (c) the distance between the argument and pred-icate.
The three feature sets are listed in Tables 1, 2,and 3, respectively.1Regarding the learning algorithm, we used gener-alized AdaBoost with real-valued weak classifiers,which constructs an ensemble of decision trees offixed depth (Schapire and Singer, 1999).
We con-sidered a one-vs-all decomposition into binary prob-lems to address multi-class classification.
AdaBoostbinary classifiers are then used for labeling test se-quences, from left to right, using a recurrent slidingwindow approach with information about the tag as-signed to the preceding token.
This tagging moduleenforces some basic constraints, e.g., BIO correctstructure, arguments cannot overlap with clause norchunk boundaries, discard ARG0-5 arguments notpresent in PropBank frames for a certain verb, etc.1Features extracted from partial parsing and Named Entitiesare common to Model 1 and 2, while features coming from fullparse trees only apply to Model 2.646Relative position, distance in words and chunks, and level ofembedding (in #clause-levels) with respect to the constituent.Constituent path as described in (Gildea and Jurafsky, 2002)and all 3/4/5-grams of path constituents beginning at theverb predicate or ending at the constituent.Partial parsing path as described in (Carreras et al, 2004)and all 3/4/5-grams of path elements beginning at theverb predicate or ending at the constituent.Syntactic frame as described by Xue and Palmer (2004)Table 3: Predicate?constituent features: Models 1/2The syntactic label of the candidate constituent.The constituent head word, suffixes of length 2, 3, and 4,lemma, and POS tag.The constituent content word, suffixes of length 2, 3, and4, lemma, POS tag, and NE label.
Content words, whichadd informative lexicalized information different fromthe head word, were detected using the heuristicsof (Surdeanu et al, 2003).The first and last constituent words and their POS tags.NE labels included in the candidate phrase.Binary features to indicate the presence of temporal cuewords, i.e.
words that appear often in AM-TMP phrasesin training.For each TreeBank syntactic label we added a feature toindicate the number of such labels included in thecandidate phrase.The sequence of syntactic labels of the constituentimmediate children.The phrase label, head word and POS tag of theconstituent parent, left sibling, and right sibling.Table 4: Constituent structure features: Model 3Model 3.
The third individual SRL model makesthe strong assumption that each predicate argumentmaps to one syntactic constituent.
For example, inFigure 1 ARG0 maps to a noun phrase, ARGM-LOCmaps to a prepositional phrase etcetera.
This as-sumption holds well on hand-corrected parse treesand simplifies significantly the SRL process becauseonly one syntactic constituent has to be correctlyclassified in order to recognize one semantic argu-ment.
On the other hand, this approach is limitedwhen using automatically-generated syntactic trees.For example, only slightly over 91% of the argu-ments can be mapped to one of the syntactic con-stituents produced by the Charniak parser.Using a bottom-up approach, Model 3 maps eachargument to the first syntactic constituent that hasthe exact same boundaries and then climbs as high aspossible in the tree across unary production chains.We currently ignore all arguments that do not mapto a single syntactic constituent.The predicate word and lemma.The predicate voice.
Same definition as Models 1 and 2.A binary feature to indicate if the predicate is frequent(i.e., it appears more than twice in the training data) or not.Sub-categorization rule.
Same def.
as Models 1 and 2.Table 5: Predicate structure features: Model 3The path in the syntactic tree between the argument phraseand the predicate as a chain of syntactic labels along withthe traversal direction (up or down).The length of the above syntactic path.The number of clauses (S* phrases) in the path.The number of verb phrases (VP) in the path.The subsumption count, i.e.
the difference between thedepths in the syntactic tree of the argument and predicateconstituents.
This value is 0 if the two phrases share thesame parent.The governing category, which indicates if NParguments are dominated by a sentence (typical forsubjects) or a verb phrase (typical for objects).We generalize syntactic paths with more than 3elements using two templates:(a) Arg ?
Ancestor ?
Ni ?
Pred, where Arg is theargument label, Pred is the predicate label, Ancestoris the label of the common ancestor, and Ni is instantiatedwith all the labels between Pred and Ancestor inthe full path; and(b) Arg ?
Ni ?
Ancestor ?
Pred, where Ni isinstantiated with all the labels between Arg andAncestor in the full path.The surface distance between the predicate and theargument phrases encoded as: the number of tokens, verbterminals (VB*), commas, and coordinations (CC) betweenthe argument and predicate phrases, and a binary feature toindicate if the two constituents are adjacent.A binary feature to indicate if the argument starts with apredicate particle, i.e.
a token seen with the RP* POStag and directly attached to the predicate in training.Table 6: Predicate?constituent features: Model 3Once the mapping process completes, Model 3extracts a rich set of lexical, syntactic, and seman-tic features.
Tables 4, 5, and 6 present these featuresorganized in the same three categories as the previ-ous Models 1 and 2 ?
see (Surdeanu and Turmo,2005) for more details.Similarly with Models 1 and 2, Model 3 trainsone-vs-all classifiers using AdaBoost for the mostcommon argument labels.
To reduce the samplespace, Model 3 selects training examples (both posi-tive and negative) only from: (a) the first clause thatincludes the predicate, or (b) from phrases that ap-pear to the left of the predicate in the sentence.
Morethan 98% of the argument constituents fall into oneof these classes.At prediction time the classifiers are combined us-ing a simple greedy technique that iteratively assigns647to each predicate the argument classified with thehighest confidence.
For each predicate we consideras candidates all AM attributes, but only numberedattributes indicated in the corresponding PropBankframe.
Additionally, this greedy strategy enforces alimited number of domain knowledge constraints inthe generated solution: (a) arguments can not over-lap in any form, and (b) no duplicate arguments areallowed for ARG0-5.5 The Inference ModelThe most important component of our inferencemodel is candidate filtering, which decides if a can-didate argument should be maintained in the globalsolution or not.
Candidate filtering is implementedas a single binary classifier that uses only featuresextracted from the solutions provided by the individ-ual systems.
For robustness, we do not use any fea-tures that are specific to any of the individual mod-els, nor the confidence value of their classifiers.Table 7 lists the features extracted from each can-didate argument by the filtering classifier.
For sim-plicity we have focused only on attributes that: (a)are readily available in the solutions proposed by theindividual classifiers, and (b) allow the gathering ofsimple and robust statistics.
For example, the fil-tering classifier might learn that a candidate is to betrusted if: (a) two individual systems proposed it, (b)if its label is ARG2 and it was generated by Model 1,or (c) if it was proposed by Model 2 within a certainargument sequence.The candidate arguments that pass the filteringstage are incorporated in the global solution by theconflict resolution module, which enforces severaldomain specific constraints.
We have currently im-plemented two constraints: (a) arguments can notoverlap or embed other arguments; and (b) no du-plicate arguments are allowed for the numbered ar-guments ARG0-5.
Theoretically, the set of con-straints can be extended with any other rules, but inour particular case, we know that some constraints,e.g.
providing only arguments indicated in the cor-responding PropBank frame, are already guaranteedby the individual models.
Conflicts are solved witha straightforward greedy strategy: the pool of candi-date arguments is inspected in descending order ofthe confidence values assigned by the filtering clas-The label of the candidate argument.The number of systems that generated an argument withthis label and span.The unique ids, e.g.
M1 and M2, of all the systems thatgenerated an argument with this label and span.The argument sequence for this predicate for all the systemsthat generated an argument with this label and span.
Forexample, the argument sequence for the propositionillustrated in Figure 1 is: ARG0 - ARGM-TMP - P -ARG1 - ARGM-LOC.The number and unique ids of all the systems that generatedan argument with the same span but different label.The number and unique ids of all the systems that generatedan argument included in the current argument.The number and unique ids of all the systems that generatedan argument that contains the current argument.The number and unique ids of all the systems that generatedan argument that overlaps the current argument.Table 7: Features used by the candidate filteringclassifier.sifier, and candidates are appended to the global so-lution only if they do not violate any of the domainconstraints with the arguments already selected.
Ourinference system currently has a sequential architec-ture, i.e.
no feedback is sent from the conflict reso-lution module to candidate filtering.6 Experimental ResultsWe trained the individual models using the completeCoNLL-2005 training set (PropBank/TreeBank sec-tions 2 to 21).
All models were developed usingAdaBoost with decision trees of depth 4 (i.e.
eachbranch may represent a conjunction of at most 4 ba-sic features).
Each classification model was trainedfor up to 2,000 rounds.We applied some simplifications to keep trainingtimes and memory requirements inside admissiblebounds: (a) we have limited the number of nega-tive examples in Model 3 to the first 500,000; (b)we have trained only the most frequent argument la-bels: top 41 for Model 1, top 35 for Model 2, andtop 24 for Model 3; and (c) we discarded all featuresoccurring less than 15 times in the training set.The models were tuned on a separate develop-ment partition (TreeBank section 24) and evaluatedon two corpora: (a) TreeBank section 23, whichconsists of Wall Street Journal (WSJ) documents,and (b) on three sections of the Brown corpus, se-mantically annotated by the PropBank team for theCoNLL 2005 shared task evaluation.
Table 8 sum-648WSJ PProps Precision Recall F?=1Model 1 48.45% 78.76% 72.44% 75.47 ?0.8Model 2 52.04% 79.65% 74.92% 77.21 ?0.8Model 3 45.28% 80.32% 72.95% 76.46 ?0.6Brown PProps Precision Recall F?=1Model 1 30.85% 67.72% 58.29% 62.65 ?2.1Model 2 36.44% 71.82% 64.03% 67.70 ?1.9Model 3 29.48% 72.41% 59.67% 65.42 ?2.1Table 8: Overall results of the individual models onthe WSJ and Brown test sets.marizes the results of the three models on the WSJand Brown corpora.
In that table we include thepercentage of perfect propositions detected by eachmodel (?PProps?
), i.e.
predicates recognized withall their arguments, the overall precision, recall, andF?=1 measure2.The results summarized in Table 8 indicate thatall individual systems have a solid performance.
Al-though none of them would rank in the top 3 in thisyear?s CoNLL evaluation (Carreras and Ma`rquez,2005), their performance is comparable to the bestindividual systems presented at that evaluation exer-cise3.
As expected, the models based on full parsing(2 and 3) perform better than the model based onpartial syntax.
But, interestingly, the difference isnot large (e.g., less than 2 points in F?=1 in the WSJcorpus), evincing that having base syntactic chunksand clause boundaries is enough to obtain a compet-itive performance with a simple system.Consistently with other systems evaluated on theBrown corpus, all our models experience a severeperformance drop in this corpus, due to the lowerperformance of the linguistic processors.6.1 Performance of Combination SystemsWe have trained the candidate filtering binary classi-fier on one third of the training partition.
Its trainingdata was generated using individual models trainedon the other two thirds of the training partition.
Theclassifier was developed using Support Vector Ma-chines (SVM) with a polynomial kernel of degree 2.We trained combined models for all 4 possible com-binations of our 3 individual models.2The significance intervals for the F1 measure have been ob-tained using bootstrap resampling (Noreen, 1989).
F1 rates out-side of these intervals are assumed to be significantly differentfrom the related F1 rate (p < 0.05).3The best performing SRL systems at CoNLL were a com-bination of several subsystems.
See section 7 for details.Table 9 summarizes the performance of the com-bined systems on the WSJ and Brown corpora.4The combined systems are compared against a base-line combination system, which merges all the argu-ments generated by the individual systems.
For con-flict resolution, the baseline uses the greedy strategyintroduced in Section 5, but using as argument or-dering criterion a radix sort that orders the candidatearguments in descending order of: number of mod-els that agreed on this argument; argument length intokens; and performance of the individual system5.Table 9 indicates that our combination strategy isalways successful: the results of all combined sys-tems improve upon their individual models and theyare better the baseline when using the same num-ber of individual models.
As expected, the highestscoring combined system includes all three individ-ual models.
Its F?=1 measure is 2.35 points higherthan the best individual model (Model 2) in the WSJtest set and 1.30 points higher in the Brown testset.
Somewhat surprisingly, the highest percentageof perfect propositions is not obtained by the over-all best combination, but by the system that com-bines the two models based on full parsing (Models2 and 3).
This happens because Model 1 is the weak-est performing of the bunch, hence its arguments,while providing useful information to the filteringclassifier, decrease the number of perfect proposi-tions when selected.We consider these results encouraging given thesimplicity of our inference model and the limitedamount of training data used to train the candidatefiltering classifier.
Additionally, they compare fa-vorably with respect to the best performing systemsat CoNLL-2005 shared task (see Section 7).6.2 Upper Limit of the Combination StrategyTo explore the potential of our approach we haveconstructed a hypothetical system where our candi-date filtering module is replaced with a perfect clas-sifier that selects only correct arguments and dis-cards all others.
Table 10 lists the results obtainedon the WSJ and Brown corpora by this hypotheticalsystem using all three individual models.4For conciseness, in Table 9 we introduced the notationM1+2+3 to indicate the combination of Models 1, 2, and 35This combination produced the highest-scoring baselinemodel.649WSJ PProps Prec.
Recall F?=1M1+2 51.30% 81.30% 74.13% 77.55 ?0.7M1+3 47.26% 81.21% 73.36% 77.08 ?0.8M2+3 52.65% 81.55% 75.30% 78.30 ?0.7M1+2+3 51.64% 84.89% 74.87% 79.56 ?0.7baseline 51.09% 77.29% 78.67% 77.98 ?0.7Brown PProps Prec.
Recall F?=1M1+2 35.95% 73.70% 62.93% 67.89 ?2.0M1+3 28.98% 72.83% 58.84% 65.09 ?2.2M2+3 37.06% 73.89% 63.30% 68.18 ?2.2M1+2+3 34.20% 78.66% 61.46% 69.00 ?2.1baseline 33.58% 67.66% 66.01% 66.82 ?1.8Table 9: Overall results of the combination modelson the WSJ and Brown test sets.Perfect props Precision Recall F?=1WSJ 70.76% 99.12% 85.22% 91.64Brown 51.87% 99.63% 74.32% 85.14Table 10: Performance upper limit on the test sets.Table 10 indicates that the upper limit of proposedapproach is relatively high: the F?=1 of this hy-pothetical system is over 12 points higher than ourbest combined system in the WSJ test set, and over16 points higher in the Brown corpus.
These re-sults indicate that the potential of our combinationstrategy is high, especially when compared with re-ranking strategies, which are limited to the perfor-mance of the best complete solution in the candidatepool.
By allowing the re-combination of argumentsfrom the individual candidate solutions we raise thisthreshold significantly.
Table 11 lists the contribu-tion of the individual models to this upper limit onthe WSJ corpus.
For conciseness, we list only the?core?
numbered arguments.
??
of 3?
indicates thepercentage of correct arguments where all 3 mod-els agreed, ??
of 2?
indicates the percentage of cor-rect arguments where any 2 models agreed, and theother columns indicate the percentage of correct ar-guments detected by a single model.
Table 11 indi-cates that, as expected, two or more individual mod-els agreed on a large percentage of the correct argu-ments.
Nevertheless, a significant number of correctarguments, e.g.
over 22% of ARG3, come from asingle individual system.
This proves that, in orderto achieve maximum performance, one has to lookbeyond simple voting strategies that favor argumentswith high agreement between individual systems.?
of 3 ?
of 2 M1 M2 M3ARG0 80.45% 12.10% 3.47% 2.14% 1.84%ARG1 69.82% 17.83% 7.45% 2.77% 2.13%ARG2 56.04% 22.32% 12.20% 4.95% 4.49%ARG3 56.03% 21.55% 12.93% 5.17% 4.31%ARG4 65.85% 20.73% 6.10% 2.44% 4.88%Table 11: Contribution of the individual systems tothe upper limit, for ARG0?ARG4 in the WSJ test set.WSJ BrownPProps F?=1 PProps F?=1koomen 53.79% 79.44 ?0.8 32.34% 67.75 ?1.8haghighi 56.52% 78.45 ?0.8 37.06% 67.71 ?2.0pradhan 50.14% 77.37 ?0.7 36.44% 67.07 ?2.0Table 12: Results of the best combined systems atCoNLL-2005.7 Related WorkThe best performing systems at the CoNLL-2005shared task included a combination of different basesubsystems to increase robustness and to gain cover-age and independence from parse errors.
Therefore,they are closely related to the work of this paper.Table 12 summarizes their results under exactly thesame experimental setting.Koomen et al (2005) used a 2 layer architecturesimilar to ours.
The pool of candidates is generatedby running a full syntax SRL system on alternativeinput information (Collins parsing, and 5-best treesfrom Charniak?s parser).
The combination of can-didates is performed in an elegant global inferenceprocedure as constraint satisfaction, which, formu-lated as Integer Linear Programming, can be solvedefficiently.
Interestingly, the generalized inferencelayer allows to include in the objective function,jointly with the candidate argument scores, a num-ber of linguistically-motivated constraints to obtaina coherent solution.
Differing from the strategy pre-sented in this paper, their inference layer does notinclude learning.
Also, they require confidence val-ues from individual classifiers.
This is the best per-forming system at CoNLL-2005.Haghighi et al (2005) implemented a double re-ranking model on top of the base SRL models to se-lect the most probable solution among a set of can-didates.
The re-ranking is performed, first, on a setof n-best solutions obtained by the base system runon a single parse tree, and, then, on the set of best-candidates coming from the n-best parse trees.
The650re-ranking approach allows to define global complexfeatures applying to complete candidate solutions totrain the rankers.
The main drawback, compared toour approach, is that re-ranking does not permit tocombine different solutions since it is forced to se-lect a complete candidate solution.
This fact impliesthat the performance upper limit strongly dependson the ability of the base model to generate the com-plete correct solution in the set of n-best candidates.Finally, Pradhan et al (2005b) followed a stack-ing approach by learning two individual systemsbased on full syntax, whose outputs are used togenerate features to feed the training stage of a fi-nal chunk-by-chunk SRL system.
Although the finegranularity of the chunking-based system allows torecover from parsing errors, we find this combina-tion scheme quite ad-hoc because it forces to breakargument candidates into chunks in the last stage.8 ConclusionsThis paper introduces a novel, robust combinationstrategy for semantic role labeling.
Our approachis separated in two stages: a candidate generationphase, which combines the solutions generated byseveral individual models into a pool of candidate ar-guments, followed by a simple inference model thatfilters the candidate arguments using a single binaryclassifier and then enforces an arbitrary number ofdomain-specific constraints.The proposed approach has several advantages.First, because it combines the solutions provided bythe individual models, the inference model can re-cover from errors produced in the generation phase.Second, due to the diversity of the individual modelsemployed, the candidate pool contains a high per-centage of the correct arguments.
And lastly, ourapproach is flexible and robust: it can incorporateany SRL model in the candidate generation stagebecause it does not require that the individual SRLmodels provide any information, e.g.
classificationconfidence values, other than an argument solution.Our results are better than the state of the art us-ing automatically-generated syntactic information.These results are encouraging considering the sim-plicity of the proposed approach.AcknowledgmentsThis research has been partially supported by theEuropean Commission (CHIL project, IP-506909).Mihai Surdeanu is a research fellow within theRamo?n y Cajal program of the Spanish Ministry ofEducation and Science.ReferencesX.
Carreras and L. Ma`rquez.
2004.
Introduction to the CoNLL-2004 shared task: Semantic role labeling.
In Proceedings ofCoNLL 2004.X.
Carreras and L. Ma`rquez.
2005.
Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling.
In Proceedingsof CoNLL-2005.X.
Carreras, L. Ma`rquez, and G. Chrupa?a.
2004.
Hierarchicalrecognition of propositional arguments with perceptrons.
InProceedings of CoNLL 2004 shared task.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling of seman-tic roles.
Computational Linguistics, 28(3).A.
Haghighi, K. Toutanova, and C. Manning.
2005.
A jointmodel for semantic role labeling.
In Proceedings of CoNLL-2005 shared task.P.
Kingsbury, M. Palmer, and M. Marcus.
2002.
Adding se-mantic annotation to the Penn Treebank.
In Proceedings ofthe Human Language Technology Conference.P.
Koomen, V. Punyakanok, D. Roth, and W. Yih.
2005.
Gen-eralized inference with multiple semantic role labeling sys-tems.
In Proceedings of CoNLL-2005 shared task.L.
Ma`rquez, P. Comas, J. Gime?nez, and N. Catala`.
2005.
Se-mantic role labeling as sequential tagging.
In Proceedings ofCoNLL-2005 shared task.S.
Narayanan and S. Harabagiu.
2004.
Question answeringbased on semantic structures.
In International Conferenceon Computational Linguistics (COLING 2004).E.
W. Noreen.
1989.
Computer-Intensive Methods for TestingHypotheses.
John Wiley & Sons.S.
Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Martin, andD.
Jurafsky.
2005a.
Support vector learning for semanticargument classification.
Machine Learning, to appear.S.
Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and D. Juraf-sky.
2005b.
Semantic role chunking combining complemen-tary syntactic views.
In Proceedings of CoNLL-2005.R.
E. Schapire and Y.
Singer.
1999.
Improved boosting algo-rithms using confidence-rated predictions.
Machine Learn-ing, 37(3).M.
Surdeanu and J. Turmo.
2005.
Semantic role labeling usingcomplete syntactic analysis.
In Proceedings of CoNLL-2005shared task.M.
Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.
2003.Using predicate-argument structures for information extrac-tion.
In Proceedings of ACL 2003.N.
Xue and M. Palmer.
2004.
Calibrating features for semanticrole labeling.
In Proceedings of EMNLP-2004.651
