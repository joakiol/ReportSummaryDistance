Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 190?198,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPDEPEVAL(summ): Dependency-based Evaluation for AutomaticSummariesKarolina OwczarzakInformation Access DivisionNational Institute of Standards and TechnologyGaithersburg, MD 20899karolina.owczarzak@nist.govAbstractThis paper presents DEPEVAL(summ),a dependency-based metric for automaticevaluation of summaries.
Using a rerank-ing parser and a Lexical-Functional Gram-mar (LFG) annotation, we produce aset of dependency triples for each sum-mary.
The dependency set for eachcandidate summary is then automaticallycompared against dependencies generatedfrom model summaries.
We examine anumber of variations of the method, in-cluding the addition of WordNet, par-tial matching, or removing relation la-bels from the dependencies.
In a teston TAC 2008 and DUC 2007 data, DE-PEVAL(summ) achieves comparable orhigher correlations with human judg-ments than the popular evaluation metricsROUGE and Basic Elements (BE).1 IntroductionEvaluation is a crucial component in the area ofautomatic summarization; it is used both to rankmultiple participant systems in shared summariza-tion tasks, such as the Summarization track at TextAnalysis Conference (TAC) 2008 and its Docu-ment Understanding Conference (DUC) predeces-sors, and to provide feedback to developers whosegoal is to improve their summarization systems.However, manual evaluation of a large numberof documents necessary for a relatively unbiasedview is often unfeasible, especially in the contextswhere repeated evaluations are needed.
Therefore,there is a great need for reliable automatic metricsthat can perform evaluation in a fast and consistentmanner.In this paper, we explore one such evaluationmetric, DEPEVAL(summ), based on the compar-ison of Lexical-Functional Grammar (LFG) de-pendencies between a candidate summary andone or more model (reference) summaries.
Themethod is similar in nature to Basic Elements(Hovy et al, 2005), in that it extends beyonda simple string comparison of word sequences,reaching instead to a deeper linguistic analysisof the text.
Both methods use hand-written ex-traction rules to derive dependencies from con-stituent parses produced by widely available PennII Treebank parsers.
The difference betweenDEPEVAL(summ) and BE is that in DEPE-VAL(summ) the dependency extraction is accom-plished through an LFG annotation of Cahill etal.
(2004) applied to the output of the rerankingparser of Charniak and Johnson (2005), whereasin BE (in the version presented here) dependen-cies are generated by the Minipar parser (Lin,1995).
Despite relying on a the same concept, ourapproach outperforms BE in most comparisons,and it often achieves higher correlations with hu-man judgments than the string-matching metricROUGE (Lin, 2004).A more detailed description of BE and ROUGEis presented in Section 2, which also gives an ac-count of manual evaluation methods employed atTAC 2008.
Section 3 gives a short introduction tothe LFG annotation.
Section 4 describes in moredetail DEPEVAL(summ) and its variants.
Sec-tion 5 presents the experiment in which we com-pared the perfomance of all three metrics on theTAC 2008 data (consisting of 5,952 100-wordssummaries) and on the DUC 2007 data (1,620250-word summaries) and discusses the correla-tions these metrics achieve.
Finally, Section 6presents conclusions and some directions for fu-ture work.2 Current practice in summaryevaluationIn the first Text Analysis Conference (TAC 2008),as well as its predecessor, the Document Under-standing Conference (DUC) series, the evaluation190of summarization tasks was conducted using bothmanual and automatic methods.
Since manualevaluation is still the undisputed gold standard,both at TAC and DUC there was much effort toevaluate manually as much data as possible.2.1 Manual evaluationManual assessment, performed by human judges,usually centers around two main aspects of sum-mary quality: content and form.
Similarly to Ma-chine Translation, where these two aspects are rep-resented by the categories of Accuracy and Flu-ency, in automatic summarization evaluation per-formed at TAC and DUC they surface as (Content)Responsiveness and Readability.
In TAC 2008(Dang and Owczarzak, 2008), however, ContentResponsiveness was replaced by Overall Respon-siveness, conflating these two dimensions and re-flecting the overall quality of the summary: thedegree to which a summary was responding tothe information need contained in the topic state-ment, as well as its linguistic quality.
A sepa-rate Readability score was still provided, assess-ing the fluency and structure independently of con-tent, based on such aspects as grammaticality, non-redundancy, referential clarity, focus, structure,and coherence.
Both Overall Responsiveness andReadability were evaluated according to a five-point scale, ranging from ?Very Poor?
to ?VeryGood?.Content was evaluated manually by NIST asses-sors using the Pyramid framework (Passonneau etal., 2005).
In the Pyramid evaluation, assessorsfirst extract all possible ?information nuggets?, orSummary Content Units (SCUs) from the fourhuman-crafted model summaries on a given topic.Each SCU is assigned a weight in proportion to thenumber of model summaries in which it appears,on the assumption that information which appearsin most or all human-produced model summariesis more essential to the topic.
Once all SCUs areharvested from the model summaries, assessorsdetermine how many of these SCUs are presentin each of the automatic peer summaries.
Thefinal score for an automatic summary is its totalSCUweight divided by the maximum SCUweightavailable to a summary of average length (wherethe average length is determined by the mean SCUcount of the model summaries for this topic).All types of manual assessment are expensiveand time-consuming, which is why it can be rarelyprovided for all submitted runs in shared taskssuch as the TAC Summarization track.
It is alsonot a viable tool for system developers who ide-ally would like a fast, reliable, and above all au-tomatic evaluation method that can be used to im-prove their systems.
The creation and testing ofautomatic evaluation methods is, therefore, an im-portant research venue, and the goal is to produceautomatic metrics that will correlate with manualassessment as closely as possible.2.2 Automatic evaluationAutomatic metrics, because of their relative speed,can be applied more widely than manual evalua-tion.
In TAC 2008 Summarization track, all sub-mitted runs were scored with the ROUGE (Lin,2004) and Basic Elements (BE) metrics (Hovy etal., 2005).ROUGE is a collection of string-comparisontechniques, based on matching n-grams betweena candidate string and a reference string.
Thestring in question might be a single sentence (asin the case of translation), or a set of sentences(as in the case of summaries).
The variations ofROUGE range from matching unigrams (i.e.
sin-gle words) to matching four-grams, with or with-out lemmatization and stopwords, with the optionsof using different weights or skip-n-grams (i.e.matching n-grams despite intervening words).
Thetwo versions used in TAC 2008 evaluations wereROUGE-2 and ROUGE-SU4, where ROUGE-2calculates the proportion of matching bigrams be-tween the candidate summary and the referencesummaries, and ROUGE-SU4 is a combination ofunigram match and skip-bigram match with skipdistance of 4 words.BE, on the other hand, employs a certain de-gree of linguistic analysis in the assessment pro-cess, as it rests on comparing the ?Basic Elements?between the candidate and the reference.
Basic El-ements are syntactic in nature, and comprise theheads of major syntactic constituents in the text(noun, verb, adjective, etc.)
and their modifiersin a dependency relation, expressed as a triple(head, modifier, relation type).
First, the input textis parsed with a syntactic parser, then Basic Ele-ments are extracted from the resulting parse, andthe candidate BEs are matched against the refer-ence BEs.
In TAC 2008 and DUC 2008 evalua-tions the BEs were extracted with Minipar (Lin,1995).
Since BE, contrary to ROUGE, does not191rely solely on the surface sequence of words to de-termine similarity between summaries, but delvesinto what could be called a shallow semantic struc-ture, comprising thematic roles such as subject andobject, it is likely to notice identity of meaningwhere such identity is obscured by variations inword order.
In fact, when it comes to evaluationof automatic summaries, BE shows higher corre-lations with human judgments than ROUGE, al-though the difference is not large enough to bestatistically significant.
In the TAC 2008 evalua-tions, BE-HM (a version of BE where the wordsare stemmed and the relation type is ignored) ob-tained a correlation of 0.911 with human assess-ment of overall responsiveness and 0.949 with thePyramid score, whereas ROUGE-2 showed corre-lations of 0.894 and 0.946, respectively.While using dependency information is an im-portant step towards integrating linguistic knowl-edge into the evaluation process, there are manyways in which this could be approached.
Sincethis type of evaluation processes information instages (constituent parser, dependency extraction,and the method of dependency matching betweena candidate and a reference), there is potentialfor variance in performance among dependency-based evaluation metrics that use different com-ponents.
Therefore, it is interesting to compareour method, which relies on the Charniak-Johnsonparser and the LFG annotation, with BE, whichuses Minipar to parse the input and produce de-pendencies.3 Lexical-Functional Grammar and theLFG parserThe method discussed in this paper rests on theassumptions of Lexical-Functional Grammar (Ka-plan and Bresnan, 1982; Bresnan, 2001) (LFG).
InLFG sentence structure is represented in terms ofc(onstituent)-structure and f(unctional)-structure.C-structure represents the word order of the sur-face string and the hierarchical organisation ofphrases in terms of trees.
F-structures are re-cursive feature structures, representing abstractgrammatical relations such as subject, object,oblique, adjunct, etc., approximating to predicate-argument structure or simple logical forms.
C-structure and f-structure are related by means offunctional annotations in c-structure trees, whichdescribe f-structures.While c-structure is sensitive to surface rear-rangement of constituents, f-structure abstractsaway from (some of) the particulars of surface re-alization.
The sentences John resigned yesterdayand Yesterday, John resigned will receive differ-ent tree representations, but identical f-structures.The f-structure can also be described in terms of aflat set of triples, or dependencies.
In triples for-mat, the f-structure for these two sentences is rep-resented in 1.
(1)subject(resign,john)person(john,3)number(john,sg)tense(resign,past)adjunct(resign,yesterday)person(yesterday,3)number(yesterday,sg)Cahill et al (2004), in their presentation ofLFG parsing resources, distinguish 32 types ofdependencies, divided into two major groups: agroup of predicate-only dependencies and non-predicate dependencies.
Predicate-only dependen-cies are those whose path ends in a predicate-value pair, describing grammatical relations.
Forinstance, in the sentence John resigned yester-day, predicate-only dependencies would include:subject(resign, john) and adjunct(resign, yester-day), while non-predicate dependencies are per-son(john,3), number(john,sg), tense(resign,past),person(yesterday,3), num(yesterday,sg).
Otherpredicate-only dependencies include: apposition,complement, open complement, coordination, de-terminer, object, second object, oblique, secondoblique, oblique agent, possessive, quantifier, rel-ative clause, topic, and relative clause pronoun.The remaining non-predicate dependencies are:adjectival degree, coordination surface form, fo-cus, complementizer forms: if, whether, and that,modal, verbal particle, participle, passive, pro-noun surface form, and infinitival clause.These 32 dependencies, produced by LFG an-notation, and the overlap between the set of de-pendencies derived from the candidate summaryand the reference summaries, form the basis of ourevaluation method, which we present in Section 4.First, a summary is parsed with the Charniak-Johnson reranking parser (Charniak and Johnson,2005) to obtain the phrase-structure tree.
Then,a sequence of scripts annotates the output, trans-lating the relative phrase position into f-structuraldependencies.
The treebank-based LFG annota-tion used in this paper and developed by Cahill etal.
(2004) obtains high precision and recall rates.As reported in Cahill et al (2008), the version of192the LFG parser which applies the LFG annotationalgorithm to the earlier Charniak?s parser (Char-niak, 2000) obtains an f-score of 86.97 on the WallStreet Journal Section 23 test set.
The LFG parseris robust as well, with coverage levels exceeding99.9%, measured in terms of complete spanningparse.4 Dependency-based evaluationOur dependency-based evaluation method, simi-larly to BE, compares two unordered sets of de-pendencies: one bag contains dependencies har-vested from the candidate summary and the othercontains dependencies from one or more referencesummaries.
Overlap between the candidate bagand the reference bag is calculated in the formof precision, recall, and the f-measure (with pre-cision and recall equally weighted).
Since forROUGE and BE the only reported score is recall,we present recall results here as well, calculated asin 2:(2) DEPEVAL(summ) Recall = |Dcand|?|Dref ||Dref |where Dcand are the candidate dependenciesand Dref are the reference dependencies.The dependency-based method using LFG an-notation has been successfully employed in theevaluation of Machine Translation (MT).
InOwczarzak (2008), the method achieves equal orhigher correlations with human judgments thanMETEOR (Banerjee and Lavie, 2005), one of thebest-performing automatic MT evaluation metrics.However, it is not clear that the method can be ap-plied without change to the task of assessing au-tomatic summaries; after all, the two tasks - ofsummarization and translation - produce outputsthat are different in nature.
In MT, the unit oftext is a sentence; text is translated, and the trans-lation evaluated, sentence by sentence.
In auto-matic summarization, the output unit is a sum-mary with length varying depending on task, butwhich most often consists of at least several sen-tences.
This has bearing on the matching pro-cess: with several sentences on the candidate andreference side each, there is increased possibilityof trivial matches, such as dependencies contain-ing function words, which might inflate the sum-mary score even in the absence of important con-tent.
This is particularly likely if we were to em-ploy partial matching for dependencies.
Partialmatching (indicated in the result tables with thetag pm) ?splits?
each predicate dependency intotwo, replacing one or the other element with avariable, e.g.
for the dependency subject(resign,John) we would obtain two partial dependenciessubject(resign, x) and subject(x, John).
This pro-cess helps circumvent some of the syntactic andlexical variation between a candidate and a refer-ence, and it proved very useful in MT evaluation(Owczarzak, 2008).
In summary evaluation, aswill be shown in Section 5, it leads to higher cor-relations with human judgments only in the caseof human-produced model summaries, because al-most any variation between two model summariesis ?legal?, i.e.
either a paraphrase or another, butequally relevant, piece of information.
For au-tomatic summaries, which are of relatively poorquality, partial matching lowers our method?s abil-ity to reflect human judgment, because it results inoverly generous matching in situations where theexamined information is neither a paraphrase norrelevant.Similarly, evaluating a summary against theunion of all references, as we do in the base-line version of our method, increases the poolof possible matches, but may also produce scoreinflation through matching repetitive informationacross models.
To deal with this, we produce aversion of the score (marked in the result tableswith the tag one) that counts only one ?hit?
for ev-ery dependency match, independent of how manyinstances of a given dependency are present in thecomparison.The use of WordNet1 module (Rennie, 2000)did not provide a great advantage (see resultstagged with wn), and sometimes even lowered ourcorrelations, especially in evaluation of automaticsystems.
This makes sense if we take into consid-eration that WordNet lists all possible synonymsfor all possible senses of a word, and so, givena great number of cross-sentence comparisons inmulti-sentence summaries, there is an increasedrisk of spurious matches between words which,despite being potentially synonymous in certaincontexts, are not equivalent in the text.Another area of concern was the potential noiseintroduced by the parser and the annotation pro-cess.
Due to parsing errors, two otherwise equiv-alent expressions might be encoded as differ-ing sets of dependencies.
In MT evaluation,the dependency-based method can alleviate parser1http://wordnet.princeton.edu/193noise by comparing n-best parses for the candidateand the reference (Owczarzak et al, 2007), but thisis not an efficient solution for comparing multi-sentence summaries.
We have therefore attemptedto at least partially counteract this issue by remov-ing relation labels from the dependencies (i.e.
pro-ducing dependencies of the form (resign, John) in-stead of subject(resign, John)), which did providesome improvement (see results tagged with norel).Finally, we experimented with a predicate-onlyversion of the evaluation, where only the predi-cate dependencies participate in the comparison,excluding dependencies that provide purely gram-matical information such as person, tense, or num-ber (tagged in the results table as pred).
Thismove proved beneficial only in the case of systemsummaries, perhaps by decreasing the number oftrivial matches, but decreased the method?s corre-lation for model summaries, where such detailedinformation might be necessary to assess the de-gree of similarity between two human summaries.5 Experimental resultsThe first question we have to ask is: which ofthe manual evaluation categories do we want ourmetric to imitate?
It is unlikely that a single au-tomatic measure will be able to correctly reflectboth Readability and Content Responsiveness, asform and content are separate qualities and needdifferent measures.
Content seems to be the moreimportant aspect, especially given that Readabil-ity can be partially derived from Responsiveness(a summary high in content cannot be very lowin readability, although some very readable sum-maries can have little relevant content).
ContentResponsiveness was provided in DUC 2007 data,but not in TAC 2008, where the extrinsic Pyra-mid measure was used to evaluate content.
It is,in fact, preferable to compare our metric againstthe Pyramid score rather than Content Responsive-ness, because both the Pyramid and our methodaim to measure the degree of similarity betweena candidate and a model, whereas Content Re-sponsiveness is a direct assessment of whether thesummary?s content is adequate given a topic anda source text.
The Pyramid is, at the same time,a costly manual evaluation method, so an auto-matic metric that successfully emulates it wouldbe a useful replacement.Another question is whether we focus onsystem-level or summary-level evaluation.
Thecorrelation values at the summary-level are gener-ally much lower than on the system-level, whichmeans the metrics are better at evaluating sys-tem performance than the quality of individualsummaries.
System-level evaluations are essen-tial to shared summarization tasks; summary-levelassessment might be useful to developers whowant to test the effect of particular improvementsin their system.
Of course, the ideal evaluationmetric would show high correlations with humanjudgment on both levels.We used the data from the TAC 2008 andDUC 2007 Summarization tracks.
The first setcomprised 58 system submissions and 4 human-produced model summaries for each of the 96 sub-topics (there were 48 topics, each of which re-quired two summaries: a main and an update sum-mary), as well as human-produced Overall Re-sponsiveness and Pyramid scores for each sum-mary.
The second set included 32 system submis-sions and 4 human models for each of the 45 top-ics.
For fair comparison of models and systems,we used jackknifing: while each model was evalu-ated against the remaining three models, each sys-tem summary was evaluated four times, each timeagainst a different set of three models, and the fourscores were averaged.5.1 System-level correlationsTable 1 presents system-level Pearson?s cor-relations between the scores provided by ourdependency-based metric DEPEVAL(summ),as well as the automatic metrics ROUGE-2,ROUGE-SU4, and BE-HM used in the TACevaluation, and the manual Pyramid scores, whichmeasured the content quality of the systems.It also includes correlations with the manualOverall Responsiveness score, which reflectedboth content and linguistic quality.
Table 3 showsthe correlations with Content Responsivenessfor DUC 2007 data for ROUGE, BE, and thosefew select versions of DEPEVAL(summ) whichachieve optimal results on TAC 2008 data (fora more detailed discussion of the selection seeSection 6).The correlations are listed for the following ver-sions of our method: pm - partial matching fordependencies; wn - WordNet; pred - matchingpredicate-only dependencies; norel - ignoring de-pendency relation label; one - counting a matchonly once irrespective of how many instances of194TAC 2008 Pyramid Overall ResponsivenessMetric models systems models systemsDEPEVAL(summ): Variationsbase 0.653 0.931 0.883 0.862pm 0.690 0.811 0.943 0.740wn 0.687 0.929 0.888 0.860pred 0.415 0.946 0.706 0.909norel 0.676 0.929 0.880 0.861one 0.585 0.958* 0.858 0.900DEPEVAL(summ): Combinationspm wn 0.694 0.903 0.952* 0.839pm pred 0.534 0.880 0.898 0.831pm norel 0.722 0.907 0.936 0.835pm one 0.611 0.950 0.876 0.895wn pred 0.374 0.946 0.716 0.912wn norel 0.405 0.941 0.752 0.905wn one 0.611 0.952 0.856 0.897pred norel 0.415 0.945 0.735 0.905pred one 0.415 0.953 0.721 0.921*norel one 0.600 0.958* 0.863 0.900pm wn pred 0.527 0.870 0.905 0.821pm wn norel 0.738 0.897 0.931 0.826pm wn one 0.634 0.936 0.887 0.881pm pred norel 0.642 0.876 0.946 0.815pm pred one 0.504 0.948 0.817 0.907pm norel one 0.725 0.941 0.905 0.880wn pred norel 0.433 0.944 0.764 0.906wn pred one 0.385 0.950 0.722 0.919wn norel one 0.632 0.954 0.872 0.896pred norel one 0.452 0.955 0.756 0.919pm wn pred norel 0.643 0.861 0.940 0.800pm wn pred one 0.486 0.932 0.809 0.890pm pred norel one 0.711 0.939 0.881 0.891pm wn norel one 0.743* 0.930 0.902 0.870wn pred norel one 0.467 0.950 0.767 0.918pm wn pred norel one 0.712 0.927 0.887 0.880Other metricsROUGE-2 0.277 0.946 0.725 0.894ROUGE-SU4 0.457 0.928 0.866 0.874BE-HM 0.423 0.949 0.656 0.911Table 1: System-level Pearson?s correlation between auto-matic and manual evaluation metrics for TAC 2008 data.a particular dependency are present in the candi-date and reference.
For each of the metrics, in-cluding ROUGE and BE, we present the correla-tions for recall.
The highest result in each categoryis marked by an asterisk.
The background gradi-ent indicates whether DEPEVAL(summ) correla-tion is higher than all three competitors ROUGE-2, ROUGE-SU4, and BE (darkest grey), two of thethree (medium grey), one of the three (light grey),or none (white).
The 95% confidence intervals arenot included here for reasons of space, but theircomparison suggests that none of the system-leveldifferences in correlation levels are large enoughto be significant.
This is because the intervalsthemselves are very wide, due to relatively smallnumber of summarizers (58 automatic and 8 hu-man for TAC; 32 automatic and 10 human forDUC) involved in the comparison.5.2 Summary-level correlationsTables 2 and 4 present the same correlations,but this time on the level of individual sum-maries.
As before, the highest level in eachcategory is marked by an asterisk.
Contrary tosystem-level, here some correlations obtained byDEPEVAL(summ) are significantly higher thanthose achieved by the three competing metrics,ROUGE-2, ROUGE-SU4, and BE-HM, as de-termined by the confidence intervals.
The let-ters in parenthesis indicate that a given DEPE-VAL(summ) variant is significantly better at cor-relating with human judgment than ROUGE-2 (=R2), ROUGE-SU4 (= R4), or BE-HM (= B).6 Discussion and future workIt is obvious that none of the versions performsbest across the board; their different character-istics might render them better suited either formodels or for automatic systems, but not forboth at the same time.
This can be explained ifwe understand that evaluating human gold stan-dard summaries and automatically generated sum-maries of poor-to-medium quality is, in a way, notthe same task.
Given that human models are bydefault well-formed and relevant, relaxing any re-straints on matching between them (i.e.
allowingpartial dependencies, removing the relation label,or adding synonyms) serves, in effect, to accept ascorrect either (1) the same conceptual informationexpressed in different ways (where the differencemight be real or introduced by faulty parsing),or (2) other information, yet still relevant to thetopic.
Accepting information of the former typeas correct will ratchet up the score for the sum-mary and the correlation with the summary?s Pyra-mid score, which measures identity of informationacross summaries.
Accepting the first and secondtype of information will raise the score and thecorrelation with Responsiveness, which measuresrelevance of information to the particular topic.However, in evaluating system summaries such re-laxation of matching constraints will result in ac-cepting irrelevant and ungrammatical informationas correct, driving up the DEPEVAL(summ) score,but lowering its correlation with both Pyramid andResponsiveness.
In simple words, it is okay to givea model summary ?the benefit of doubt?, and ac-cept its content as correct even if it is not match-ing other model summaries exactly, but the samestrategy applied to a system summary might causemass over-estimation of the summary?s quality.This substantial difference in the nature ofhuman-generated models and system-producedsummaries has impact on all automatic means ofevaluation, as long as we are limited to methodsthat operate on more shallow levels than a full195TAC 2008 Pyramid Overall ResponsivenessMetric models systems models systemsDEPEVAL(summ): Variationsbase 0.436 (B) 0.595 (R2,R4,B) 0.186 0.373 (R2,B)pm 0.467 (B) 0.584 (R2,B) 0.183 0.368 (B)wn 0.448 (B) 0.592 (R2,B) 0.192 0.376 (R2,R4,B)pred 0.344 0.543 (B) 0.170 0.327norel 0.437 (B) 0.596* (R2,R4,B) 0.186 0.373 (R2,B)one 0.396 0.587 (R2,B) 0.171 0.376 (R2,R4,B)DEPEVAL(summ): Combinationspm wn 0.474 (B) 0.577 (R2,B) 0.194* 0.371 (R2,B)pm pred 0.407 0.537 (B) 0.153 0.337pm norel 0.483 (R2,B) 0.584 (R2,B) 0.168 0.362pm one 0.402 0.577 (R2,B) 0.167 0.384 (R2,R4,B)wn pred 0.352 0.537 (B) 0.182 0.328wn norel 0.364 0.541 (B) 0.187 0.329wn one 0.411 0.581 (R2,B) 0.182 0.384 (R2,R4,B)pred norel 0.351 0.547 (B) 0.169 0.327pred one 0.325 0.542 (B) 0.171 0.347norel one 0.403 0.589 (R2,B) 0.176 0.377 (R2,R4,B)pm wn pred 0.415 0.526 (B) 0.167 0.337pm wn norel 0.488* (R2,R4,B) 0.576 (R2,B) 0.168 0.366 (B)pm wn one 0.417 0.563 (B) 0.179 0.389* (R2,R4.B)pm pred norel 0.433 (B) 0.538 (B) 0.124 0.333pm pred one 0.357 0.545 (B) 0.151 0.381 (R2,R4,B)pm norel one 0.437 (B) 0.567 (R2,B) 0.174 0.369 (B)wn pred norel 0.353 0.541 (B) 0.180 0.324wn pred one 0.328 0.535 (B) 0.179 0.346wn norel one 0.416 0.584 (R2,B) 0.185 0.385 (R2,R4,B)pred norel one 0.336 0.549 (B) 0.169 0.351pm wn pred norel 0.428 (B) 0.524 (B) 0.120 0.334pm wn pred one 0.363 0.525 (B) 0.164 0.380 (R2,R4,B)pm pred norel one 0.420 (B) 0.533 (B) 0.154 0.375 (R2,R4,B)pm wn norel one 0.452 (B) 0.558 (B) 0.179 0.376 (R2,R4,B)wn pred norel one 0.338 0.544 (B) 0.178 0.349pm wn pred norel one 0.427 (B) 0.522 (B) 0.153 0.379 (R2,R4,B)Other metricsROUGE-2 0.307 0.527 0.098 0.323ROUGE-SU4 0.318 0.557 0.153 0.327BE-HM 0.239 0.456 0.135 0.317Table 2: Summary-level Pearson?s correlation between automatic and manualevaluation metrics for TAC 2008 data.DUC 2007 Content ResponsivenessMetric models systemsDEPEVAL(summ) 0.7341 0.8429DEPEVAL(summ) wn 0.7355 0.8354DEPEVAL(summ) norel 0.7394 0.8277DEPEVAL(summ) one 0.7507 0.8634ROUGE-2 0.4077 0.8772ROUGE-SU4 0.2533 0.8297BE-HM 0.5471 0.8608Table 3: System-level Pearson?s correlationbetween automatic metrics and Content Respon-siveness for DUC 2007 data.
For model sum-maries, only DEPEVAL correlations are signif-icant (the 95% confidence interval does not in-clude zero).
None of the differences betweenmetrics are significant at the 95% level.DUC 2007 Content ResponsivenessMetric models systemsDEPEVAL(summ) 0.2059 0.4150DEPEVAL(summ) wn 0.2081 0.4178DEPEVAL(summ) norel 0.2119 0.4185DEPEVAL(summ) one 0.1999 0.4101ROUGE-2 0.1501 0.3875ROUGE-SU4 0.1397 0.4264BE-HM 0.1330 0.3722Table 4: Summary-level Pearson?s correlationbetween automatic metrics and Content Respon-siveness for DUC 2007 data.
ROUGE-SU4 andBE correlations for model summaries are notstatistically significant.
None of the differencesbetween metrics are significant at the 95% level.semantic and pragmatic analysis against human-level world knowledge.
The problem is twofold:first, our automatic metrics measure identity ratherthan quality.
Similarity of content between a can-didate summary and one or more references is act-ing as a proxy measure for the quality of the can-didate summary; yet, we cannot forget that the re-lation between these two features is not purely lin-ear.
A candidate highly similar to the referencewill be, necessarily, of good quality, but a candi-date which is dissimilar from a reference is notnecessarily of low quality (vide the case of par-allel model summaries, which almost always con-tain some non-overlapping information).The second problem is the extent to which ourmetrics are able to distinguish content throughthe veil of differing forms.
Synonyms, para-phrases, or pragmatic features such as the choiceof topic and focus render simple string-matchingtechniques ineffective, especially in the area ofsummarization where the evaluation happens ona supra-sentential level.
As a result, then, a lotof effort was put into developing metrics thatcan identify similar content despite non-similarform, which naturally led to the application oflinguistically-oriented approaches that look be-yond surface word order.Essentially, though, we are using imperfectmeasures of similarity as an imperfect stand-in forquality, and the accumulated noise often causesa divergence in our metrics?
performance withmodel and system summaries.
Much like the in-verse relation of precision and recall, changes andadditions that improve a metric?s correlation withhuman scores for model summaries often weakenthe correlation for system summaries, and viceversa.
Admittedly, we could just ignore this prob-lem and focus on increasing correlations for auto-matic summaries only; after all, the whole pointof creating evaluation metrics is to score and rankthe output of systems.
Such a perspective can berather short-sighted, though, given that we expectcontinuous improvement from the summarizationsystems to, ideally, human levels, so the same is-sues which now prevent high correlations for mod-els will start surfacing in evaluation of system-produced summaries as well.
Using metrics thatonly perform reliably for low-quality summariesmight prevent us from noticing when those sum-maries become better.
Our goal should be, there-fore, to develop a metric which obtains high cor-relations in both categories, with the assumptionthat such a metric will be more reliable in evaluat-ing summaries of varying quality.196Since there is no single winner among all 32variants of DEPEVAL(summ) on TAC 2008 data,we must decide which of the categories is most im-portant to a successful automatic evaluation met-ric.
Correlations with Overall Responsiveness arein general lower than those with the Pyramid score(except in the case of system-level models).
Thismakes sense, if we rememeber that Overall Re-sponsiveness judges content as well as linguisticquality, which are two different dimensions and soa single automatic metric is unlikely to reflect itwell, and that it judges content in terms of its rel-evance to topic, which is also beyond the reachof contemporary metrics which can at most judgecontent similarity to a model.
This means that thePyramid score makes for a more relevant metric toemulate.The last dilemma is whether we choose to focuson system- or summary-level correlations.
Thisties in with the purpose which the evaluation met-ric should serve.
In comparisons of multiple sys-tems, such as in TAC 2008, the value is placedin the correct ordering of these systems; whilesummary-level assessment can give us importantfeedback and insight during the system develop-ment stage.The final choice among all DEPEVAL(summ)versions hinges on all of these factors: we shouldprefer a variant which correlates highly with thePyramid score rather than with Responsiveness,which minimizes the gap between model and au-tomatic peer correlations while retaining relativelyhigh values for both, and which fulfills these re-quirements similarly well on both summary- andsystem-levels.
Three such variants are the base-line DEPEVAL(summ), the WordNet version DE-PEVAL(summ) wn, and the version with removedrelation labels DEPEVAL(summ) norel.
Both thebaseline and norel versions achieve significant im-provement over ROUGE and BE in correlationswith the Pyramid score for automatic summaries,and over BE for models, on the summary level.
Infact, almost in all categories they achieve highercorrelations than ROUGE and BE.
The only ex-ceptions are the correlations with Pyramid for sys-tems at the system-level, but there the results areclose and none of the differences in that categoryare significant.
To balance this exception, DE-PEVAL(summ) achieves much higher correlationswith the Pyramid scores for model summaries thaneither ROUGE or BE on the system level.In order to see whether the DEPEVAL(summ)advantage holds for other data, we examined themost optimal versions (baseline, wn, norel, aswell as one, which is the closest counterpartto label-free BE-HM) on data from DUC 2007.Because only a portion of the DUC 2007 datawas evaluated with Pyramid, we chose to lookrather at the Content Responsiveness scores.
Ascan be seen in Tables 3 and 4, the same pat-terns hold: decided advantage over ROUGE/BEwhen it comes to model summaries (especiallyat system-level), comparable results for automaticsummaries.
Since DUC 2007 data consisted offewer summaries (1,620 vs 5,952 at TAC) andfewer submissions (32 vs 57 at TAC), some resultsdid not reach statistical significance.
In Table 3, inthe models category, only DEPEVAL(summ) cor-relations are significant.
In Table 4, in the modelcategory, only DEPEVAL(summ) and ROUGE-2correlations are significant.
Note also that thesecorrelations with Content Responsiveness are gen-erally lower than those with Pyramid in previoustables, but in the case of summary-level compari-son higher than the correlations with Overall Re-sponsiveness.
This is to be expected given ourearlier discussion of the differences in what thesemetrics measure.As mentioned before, the dependency-basedevaluation can be approached from different an-gles, leading to differences in performance.
Thisis exemplified in our experiment, where DEPE-VAL(summ) outperforms BE, even though boththese metrics rest on the same general idea.
Thenew implementation of BE presented at the TAC2008 workshop (Tratz and Hovy, 2008) introducestransformations for dependencies in order to in-crease the number of matches among elements thatare semantically similar yet differ in terms of syn-tactic structure and/or lexical choices, and addsWordNet for synonym matching.
Its core moduleswere updated as well: Minipar was replaced withthe Charniak-Johnson reranking parser (Charniakand Johnson, 2005), Named Entity identificationwas added, and the BE extraction is conducted us-ing a set of Tregex rules (Levy and Andrew, 2006).Since our method, presented in this paper, alsouses the reranking parser, as well as WordNet, itwould be interesting to compare both methods di-rectly in terms of the performance of the depen-dency extraction procedure.197ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments.
In Pro-ceedings of the ACL 2005 Workshop on Intrinsic andExtrinsic Evaluation Measures for MT and/or Sum-marization, pages 65?73, Ann Arbor, MI, USA.Joan Bresnan.
2001.
Lexical-Functional Syntax.Blackwell, Oxford.Aoife Cahill, Michael Burke, Ruth O?Donovan, Josefvan Genabith, and Andy Way.
2004.
Long-distance dependency resolution in automatically ac-quired wide-coverage PCFG-based LFG approxima-tions.
In Proceedings of the 42th Annual Meetingof the Association for Computational Linguistics,pages 320?327, Barcelona, Spain.Aoife Cahill, Michael Burke, Ruth O?Donovan, StefanRiezler, Josef van Genabith, and Andy Way.
2008.Wide-coverage deep statistical parsing using auto-matic dependency structure annotation.
Comput.Linguist., 34(1):81?124.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In ACL 2005: Proceedings of the 43rdAnnual Meeting of the Association for Computa-tional Linguistics, pages 173?180, Morristown, NJ,USA.
Association for Computational Linguistics.Eugene Charniak.
2000.
A maximum entropy inspiredparser.
In Proceedings of the 1st Annual Meeting ofthe North American Chapter of the Association forComputational Linguistics, pages 132?139, Seattle,WA, USA.Hoa Trang Dang and Karolina Owczarzak.
2008.Overview of the tac 2008 summarization track: Up-date task.
In to appear in: Proceedings of the 1stText Analysis Conference (TAC).Eduard Hovy, Chin-Yew Lin, and Liang Zhou.
2005.Evaluating DUC 2005 using Basic Elements.
InProceedings of the 5th Document UnderstandingConference (DUC).Ronald M. Kaplan and Joan Bresnan, 1982.
The Men-tal Representation of Grammatical Relations, chap-ter Lexical-functional Grammar: A Formal Systemfor Grammatical Representation.
MIT Press, Cam-bridge, MA, USA.Roger Levy and Galen Andrew.
2006.
Tregex and tsur-geon: Tools for querying and manipulating tree datastructures.
In Proceedings of the 5th InternationalConference on Language Resources and Evaluation.Dekang Lin.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In Proceedingsof the 14th International Joint Conference on Artifi-cial Intelligence, pages 1420?1427.Chin-Yew Lin.
2004.
ROUGE: A package for au-tomatic evaluation of summaries.
In Proceedingsof the ACL 2004 Workshop: Text SummarizationBranches Out, pages 74?81.Karolina Owczarzak, Josef van Genabith, and AndyWay.
2007.
Evaluating Machine Translation withLFG dependencies.
Machine Translation, 21(2):95?119.Karolina Owczarzak.
2008.
A novel dependency-based evaluation metric for Machine Translation.Ph.D.
thesis, Dublin City University.Rebecca J. Passonneau, Ani Nenkova, Kathleen McK-eown, and Sergey Sigelman.
2005.
Applyingthe Pyramid method in DUC 2005.
In Proceed-ings of the 5th Document Understanding Conference(DUC).Jason Rennie.
2000.
Wordnet::querydata: aPerl module for accessing the WordNet database.http://people.csail.mit.edu/ jrennie/WordNet.Stephen Tratz and Eduard Hovy.
2008.
Summariza-tion evaluation using transformed Basic Elements.In Proceedings of the 1st Text Analysis Conference(TAC).198
