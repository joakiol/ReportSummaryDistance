Extending a thesaurus by classifying wordsTokunaga Takenobu Fuji i  A tsush iSakurai Naoyuki Tanaka  HozumiDepar tment  of  Computer  ScienceTokyo  Ins t i tu te  of Techno logytake~cs, t i t  ech.
ac.
jpIwayama MakotoAdvanced Research Lab.H i tachi  Ltd.Abst ractThis paper proposes a method for extending anexisting thesaurus through classification of newwords in terms of that thesaurus.
New wordsare classified on the basis of relative probabili-ties of.a word belonging to a given word class,with the probabilities calculated using noun-verb co-occurrence pairs.
Experiments usingthe Japanese Bunruigoihy5 thesaurus on about420,000 co-occurrences showed that new wordscan be classified correctly with a max imum ac-curacy of more than 80%.1 In t roduct ionFor most natural language processing (NLP) systems,thesauri comprise indispensable linguistic knowledge.Roger's International Thesaurus \[Chapman, 1984\] andWordNet \[Miller et al, 1993\] are typical English the-sauri which have been widely used in past NLP re-search \[Resnik, 1992; Yarowsky, 1992\].
They are hand-crafted, machine-readable and have fairly broad cover-age.
However, since these thesauri were originally com-piled for human use, they are not always suitable forcomputer-based natural language processing.
Limita-tions of handcrafted thesauri can be summarized as fol-lows \[Hatzivassiloglou and McKeown, 1993; Uramoto,1996; Hindle, 1990\].?
limited vocabulary size?
unclear classification criteria?
building thesauri by hand requires considerable timeand effortThe vocabulary size of typical handcrafted thesauriranges from 50,000 to 100,000 words, including generalwords in broad domains.
From the viewpoint of NLPsystems dealing with a particular domain, however, thesethesauri include many unnecessary (general) words anddo not include necessary domain-specific words.The second problem with handcrafted thesauri is thattheir classification is based on the intuition of lexicogra-phers, with their classification criteria not always beingclear.
For the purposes of NLP systems, their classifi-cation of words is sometimes too coarse and does notprovide sufficient distinction between words, or is sometimes unnecessarily detailed.Lastly, building thesauri by hand requires significantamounts of time and effort even for restricted omains.Furthermore, this effort is repeated when a system isported to another domain.This criticism leads us to automatic approaches forbuilding thesauri from large corpora \[Hirschman et al,1975; Hindle, 1990; Hatzivassiloglou and McKeown,1993; Pereira et al, 1993; Tokunaga et aL, 1995; Ush-ioda, 1996\].
Past attempts have basically taken the fol-lowing steps \[Charniak, 1993\].
(1) extract word co-occurrences(2) define similarities (distances) between words on thebasis of co-occurrences(3) cluster words on the basis of similaritiesThe most crucial part of this approach is gathering wordco-occurrence data.
Co-occurrences are usually gath-ered on the basis of certain relations uch as predicate-argument, modifier-modified, adjacency, or mixture ofthese.
However, it is very difficult to gather sufficientco-occurrences to calculate similarities reliably \[Resnik,1992; Basili et al, 1992\].
It is sometimes impracticalto build a large thesaurus from scratch based on onlyco-occurrence data.Based on this observation, a third approach as beenproposed, namely, combining linguistic knowledge andco-occurrence data \[Resnik, 1992; Uramoto, 1996\].
Thisapproach aims at compensating the sparseness of co~occurrence data by using existing linguistic knowledge,such as WordNet.
This paper follows this line of researchand proposes a method to extend an existing thesaurusby classifying new words in terms of that thesaurus.
Inother words, the proposed method identifies appropriate16word classes of the thesaurus for a new word which isnot included in the thesaurus.
This search process is fa-cilitated based on the probability that a word belongsto a given word class.
The probability is calculatedbased on word co'occurrences.
As such, this methodcould also suffer from the data sparseness problem.
AsResnik pointed out, however, using the thesaurus struc-ture (classes) can remedy this problem \[Resnik, 1992\].2 Core  thesaurusBunruigoihy$ (BGH for short) \[Hayashi, 1966\] is a typ-ical Japanese thesaurus, which has been used for muchNLP research on Japanese.
BGH includes 87,743 words,each of which is assigned an 8 digit class code.
Somewords are assigned more than one class code.
The cod-ing system of BGH has a hierarchical structure, that is,the first digit represents he part(s) of speech of the word(1: noun, 2:verb, 3: adjective, 4: others), and the seconddigit classifies words sharing the same first digit and soon.
Thus BGH can be considered as four trees, each ofwhich has 8 levels in depth (see figure 1), with each leafas a set of words.1 2 3 4.1 .1 .
.~rb)  (a~) (otheO11 12 13 14 15!
| !
| ~/ f i f~xx ~.
1588levels !1500 ... 150915000 ???
15004150040 ?..
150049words wordsFig.
1 Structure of Bunruigoihy6 (BGH)This paper focuses on classifying only nouns in terms ofa class code based on the first 5 digits, namely, up to thefifth level of the noun tree.
Table 1 shows the numberof words (#words) and the number of 5 digit class codes(#classes) with respect o each part of speech.Table 1 Outline of Bunruigoihy3 (BGH)POS noun I verb adj other total#words 55,443 i 21,669 9,890 741 87,743~classes 544 165 190 24 8423 Co-occur rence  dataAppropriate word classes for a new word are identi-fied based on the probability that the word belongs todifferent word classes.
This probability is calculatedbased on co-occurrences of nouns and verbs.
The co-occurrences were extracted from the RWC text baseRWC-DB-TEXT-95-1 \[Real World Computing Partner-ship, 1995\].
This text base consists of 4 years worth ofMainiti Shimbun \[Mainichi Shimbun, 1991-1994\] news-paper articles, which have been automatically annotatedwith morphological tags.
The total number of mor-phemes is about 100 million.
Instead of conducting fullparsing on the texts, several heuristics were used in or-der to obtain dependencies between ouns and verbs inthe form of tuples (frequency, noun, postposition, verb).Among these tuples, only those which include the post-position "WO" (typically marking accusative case) wereused.
Further, tuples containing nouns in BGH were se-lected.
In the case of a compound noun, the noun wastransformed into the maximal leftmost string containedin BGH 1.
As a result, 419,132 tuples remained includ-ing 23,223 noun types and 9,151 verb types.
These wereused in the experiments described in section 5.4 Ident i fy ing  appropr ia te  word  c lasses4.1 P robab i l i s t i c  mode lThe probabilistic model used in this paper is the SVMVmodel \[Iwayama and Tokunaga, 1994\].
This modelwas originally developed for document categorization, iwhich a new document is classified into certain prede-fined categories.
For the purposes of this paper, a newword (noun) not appearing in the thesaurus i treatedas a new document, and a word class in the thesauruscorresponds to a predefined ocument category.
Eachnoun is represented by a set of verbs co-occurring withthat noun.
The probability P(c, Iw) is calculated for eachword class c,, and the proper classes for a word w aredetermined based on it.
The SVMV model formalizesthe probability P(clw ) as follows.Conditioning P(clw ) on each possible vent givesP(clw) = ~ P(clw, V = v,)P(V = v, lw).
(1)O,Assuming conditional independence b tween c and V =v, given w, that is P(clw, V = %) = P(clV = v,), weobtainP(clw) = Z P(c\]V = vOP(V = %\[w).
(2)Using Bayes' theorem, this becomesP(clw ) = P(c) E P(V = v, lc)P(V = v, lw)~, P(V  = v,) (3)All the probabilities in (3) can be estimated from train-ing data based on the following equations.
In the follow-ing, fr(w, v) denotes the frequency that a noun w and averb v are co-occurring.1For Japanese compound noun, the final word tends to be asemantic head.17P(V  = v~lc ) is the probability that a randomly ex-tracted verb co-occurring with a noun is v~, given thatthe noun belongs to word class c. This is estimated fromthe relative frequency of v~ co-occurring with the nounsin word class c, namely,E eo/r(w,v,) (4)P(v =  ,lc) = E, Ewe?P(V  = v, lw ) is the probability that a randomly ex-tracted verb co-occurring with a noun w is vs.
This isestimated from the relative frequency of v, co-occurringwith noun w, namely,P(V = = (5)P(V  = v,) is the prior probability that a randomly ex-tracted verb co-occurring with a randomly selected nounis v~.
This is estimated from the relative frequency of v~in the whole training data, namely,(6) P(vP(c) is the prior probability that a randomly selectednoun belongs to c. This is estimated from the relativefrequency of a verb co-occurring with any noun in classc 2, namely,P(c) = EwEoEEo E eo(7)4.2 Search ing  through the  thesaurusAs is documented by the fact that we employ the proba-bilistic model used in document categorization, classify-ing words in a thesaurus i basically the same as docu-ment categorization 3.
Document categorization strate-gies can be summarized according to the following threetypes \[Iwayama and Tokunaga, 1995\].?
the k-nearest neighbor (k-nn) or Memory based rea-soning (MBR) approach?
the category-based approach?
the cluster-based approachThe k-nn approach searches for the k documents mostsimilar to a target document in training data, and as-signs that category with the highest distribution in the kdocuments \[Weiss and Kulikowski, 1991\].
Although the2This calculation seems be counterintuitive.
A more straight-forward calculation would be one based on the relative frequencyof words belonging to class c. However, the given estimation is nec-essary in order to normalize the sum of the probabilities P(clw ) toone.3As Uramoto mentioned, this task is also similar to word sensedisambiguation except for the size of search space \[Uramoto, 1996\].k-nn approach as been promising for document catego-rization \[Masand et al, 1992\], it requires ignificant com-putational resources to calculate the similarity betweena target document and every document in training data.In order to overcome the drawback of the k-nn ap-proach, the category-based approach first makes a clus-ter for each category consisting of documents assignedthe same category, then calculates the similarity betweena target document and each of these document clusters.The number of similarity calculations can be reduced tothe number of clusters (categories), saving on computa-tional resources.Another alternative is the cluster-based approach,which first constructs clusters from training data byusing some clustering algorithm, then calculates imi-larities between a target document and those clusters.The main difference between category-based and cluster-based approaches resides in the cluster construction.The former uses categories which have been assigned todocuments when constructing clusters, while the latterdoes not.
In addition, clusters are structured in a treewhen a hierarchical clustering algorithm is used for thelatter approach.
In this case, one can adopt a top-downtree search strategy for similar clusters, saving furthercomputational overhead.In this paper, all these approaches are evaluated forword classification, in which a target document corre-sponds to a target word and a document category corre-sponds to a thesaurus class code.5 Exper imentsIn our experiments, the 23,223 nouns described in sec-tion 3 were classified in terms of the core thesaurus,BGH, using the three search strategies described in theprevious ection.
Classification was conducted for eachstrategy as follows.k-nn Each noun is considered as a singleton cluster, andthe probability that a target noun is classified intoeach of the non-target noun clusters is calculated.category-based 10-fold cross validation was conductedfor the category-based and cluster-based strategies,in that, 23,223 nouns were randomly divided into10 groups, and one group of nouns was used for testdata while the rest was used for training.
The testgroup was rotated 10 times, and therefore, all nounswere used as a test case.
The results were averagedover these 10 trials.
Each noun in the training datawas categorized according to its BGH 5 digit classcode, generating 544 category clusters (see Table 1).The probability of each noun in the test data beingclassified into each of these 544 cluster was calcu-lated.cluster-based In the case of the category-based ap-proach, each noun in the training data was catego-rized into the leaf clusters of the BGH tree, that is,18the 5 digit class categories 4.
For the cluster-basedapproach, the nouns were also categorized into theintermediate class categories, that is, the 2 to 4 digitclass categories.
Since we use the BGH hierarchystructure instead of constructing a duster hierarchyfrom scratch, in a strict sense, this does not coincidewith the cluster-based approach as described in theprevious section.
However, searching through theBGH tree structure in a top down manner still en-ables us to save greatly on computational resources.A simple top down search, in which the cluster withthe highest probability is followed at each level, al-lows only one path leading to a single leaf (5 digitclass code).
In order to take into account multi-ple word senses, we followed several paths at thesame time.
More precisely, the difference betweenthe probability of each cluster and the highest prob-ability value for that level was calculated, and clus-ters for which the difference was within a certainthreshold were left as candidate paths.
The thresh-old was set to 0.2 in this experiments.The performance of each approach was evaluated onthe basis of the number of correctly assigned class codes.Tables 2 to 4 show the results of each approach.
Columnsshow the maximum number of class codes assigned toeach target word.
For example, the column "10" meansthat a target word is assigned to up to 10 class codes.If the correct class code is contained in these assignedcodes, the test case is considered to be assigned the cor-rect code.
Rows show the distribution word numbers onthe basis of occurrence frequencies in the training data.Each value in the table is the number of correct caseswith its percentage in the parentheses.Tab le  2 Results for the k-nn approachfreq\k 5 I0 20 30 total,.~ 10 1,733 2,581 3,934 4,902 12,719(13.6) (20.3) (30.9) (38.5)10 N 1,817 2,638 3,594 4,231 7,550100 (24.1) (34.9) (47.6) (56.0)100 ,,~ 658 949 1,260 1,455 2,208500 (29.8) (43.0) (57.1) (65.9)500 N 132 199 254 300 4011000 (32.9) (49.6) (63.3) (74.8)1000 ~ 149 187 236 264 345(43.2) (54.2) (68.4) (76.5)total 4,489 6,554 9,278 11,152 23,223(19.3) (28.2) (40.0) (48.0)4Note that we ignore lower digits, and therefore, lea\] means thecategories formed by 5 digit class code.Table 3 Results for the category-based approach~eq\k,,~ I0I0I00100 ,.~500500i0001000total5 10 20 30 total2,304 3,442 4,778 5,689 12,719(18.1) (27.1) (37.6) (44.7)2,527 3,458 4,449 5,025 7,550(33.5) (45.8) (58.9) (66.6)922 1,231 1,511 1,657 2,208(41.8) (55 8) (68.4) (75.0)204 250 298 327(50.9) (62.3) (74.3) (81.5)181 231 264 289(52.5) (67.0) (76.5) (83.8)4013456,138 8,612 11,300 12,987 23,223(26.4) (37.1) (48.7) (55.9)Table 4 Results for the cluster-based approach~eq\k1010 N100100 N500500 N10001000 ,.~5 10 20 30 total1,982 2,534 3,026 3,240 12,719(15.6) (19.9) (23.8) (25.5)2,385 3,011 3,490 3,690 7,550(31.6) (39.9) (46.2)(48.9)8877 1,077 1,205 1,264 2,208(40.2) (48.8) (54.6) (57.2)201 227 251 259(50.1) (56.6)(62.6) (64.6)401183 209 231 239(53.0) (60.6) (67.0) (69.3)345total 5,638 7,058 8,203 8,692 23,223(24.3) (30.4) (35.3) (37.4)6 D iscuss ionOverall, the category-based approach shows the best per-formance, followed by the cluster-based approach, k-nnshows the worst performance.
This result contradictspast research \[Iwayama and Tokunaga, 1995; Masandet al, 1992\].
One possible xplanation for this contra-diction may be that the basis of the classification forBGH and our probabilistic model is very different.
Inother words, co-occurrences with verbs may not havecaptured the classification basis of BGH very well.The performance ofk-nn is noticeably worse than thatof the others for low frequent words.
This may be dueto data sparseness.
Generalizing individual nouns byconstructing clusters remedies this problem.When b is small, namely only categories withhigh probabilities are assigned, the category-based andduster-based approaches show comparable performance.When k becomes bigger, however, the category-basedapproach becomes uperior.
Since a beam search wasadopted for the cluster-based approach, there was a pos-sibility of falling to follow the correct path.7 Re la ted  workThe goal of this paper is the same as that forUramoto \[Uramoto, 1996\], that is, identifying appro-priate word classes for an unknown word in terms of anexisting thesaurus.
The significant difference betweenUramoto and our research can be summarized as follows.19?
The core thesaurus is different.
Uramoto usedISAMAP \[Tanaka and Nisina, 1987\], which containsabout 4,000 words.?
We adopted a probabilistic model, which has asounder foundation than the Uramoto's.
He usedseveral factors, such as similarity between a targetword and words in each classes, class levels and soforth.
These factors are combined into a score bycalculating their weighted sum.
The weight for eachfactor is determined by using held out data.?
We restricted our co-occurrence data to that in-cluded the "WO" postposition, which typicallymarks the accusative case, while Uramoto used sev-eral grammatical relations in tandem.
There areclaims that words behave differently depending ontheir grammatical role, and that they should there-fore be classified into different word classes whenthe role is different \[Tokunaga et al, 1995\].
Thisviewpoint should be taken into account when weconstruct a thesaurus from scratch.
In our case,however, since we assume a core thesaurus, thereis room for argument as to whether we should con-sider this claim.
Further investigation on this pointis needed.?
Our evaluation scheme is more rigid and based ona larger dataset.
We conducted cross validationon nouns appearing in BGH and the judgement ofcorrectness was done automatically, while Uramotoused unknown words as test cases and decided thecorrectness on a subjective basis.
The number of histest cases was 250, ours is 23223.
The performanceof his method was reported to be from 65% to 85%in accuracy, which seems better than ours.
How-ever, it is difficult to compare these two in an ab-solute sense, because both the evaluation data andcode assignment scheme are different.
We identifiedclass codes at the fifth level of BGH, while Uramotosearched for a set of class codes at various levels.Nakano proposed a method of assigning a BGH classcode to new words \[Nakano, 1981\].
His approach is verydifferent from ours and Uramoto's.
He utilized charac-teristics of Japanese character classes.
There are threecharacter classes used in writing Japanese, Kanzi, Hira-gana and Katakana.
A Kanzi character is an ideogramand has a distinct stand-alone meaning, to a certain ex-tent.
On the other hand, Hiragana and Katakana char-acters are phonograms.
Nakano first constructed a Kanzimeaning dictionary from BGH by extracting words in-cluding a single Kanzi character.
He defined the classcode of each Kanzi character to the code of words includ-ing only that Kanzi.
He then assigned class codes to newwords based on this Kanzi meaning dictionary.
For ex-ample, if the class codes of Kanzi Ks and K s are ~1,  c~2}and {c31 , c32 ,c~3} respectively, then a word including K,and K~ is assigned the codes {Ctl,Cs2,C31,C32,C33 }.
Weapplied Nakano's method on the data used in section 55,obtaining the accuracy of 54.6% for 17,736 words.
Theaverage number of codes assigned was 5.75.
His methodhas several advantages over ours, such as:?
no co-occurrence data is required,?
not so much computational overhead is required.However, there are obvious limitations, such as:?
it can not handle words not including Kanzi,?
ranking or preference of assigned codes is not ob-tained,?
not applicable to languages other than Japanese.We investigated the overlap of words that were as-signed correct classes for our category-based method andNakano's method.
The parameter k was set to 30 forour method.
The number of words that were assignedcorrect classes by both methods was 5,995, which repre-sents 46% of the words correctly classified by our methodand 62% of the words correctly classified by Nakano'smethod.
In other words, of the words correctly clas-sifted by one method, only about half can also be alsoclassified correctly by the other method.
This result sug-gests that these two methods are complementary to eachother, rather than competitive, and that the overall per-formance can be improved by combining them.8 Conc lus ionThis paper proposed a method for extending an ex-isting thesaurus by classifying new words in terms ofthat thesaurus.
We conducted experiments using theJapanese Bunruigoihy5 thesaurus and about 420,000 co-occurrence pairs of verbs and nouns, related by the WOpostposition.
Our experiments showed that new wordscan be classified correctly with a maximum accuracy ofmore than 80% when the category-based search strategywas used.We only used co-occurrence data including the WO re-lation (accusative case).
However, as mentioned in com-parison with Uramoto's work, the use of other relationsshould be investigated.This paper focused on only 5 digit class codes.
This ismainly because of the data sparseness of co-occurrencedata.
We would be able to classify words at deeper lev-els if we obtained more co-occurrence data.
Anotherapproach would be to construct a hierarchy from a setof words of each class, using a clustering algorithm.5Nakano's original work used an old version of BGH, whichcontains 36,263 words.20References\[Basili et al, 1992\] Basili, R., Pazienza, M., andVelardi, P. Computational lexicons: The neat examplesand the odd exemplars.
In Proceedings ofthirdconference on Applied Natural Language Processing, pp.96--103.\[Chapman, 1984\] Chapman, L. R. Roger's InternationalThesaurus (Fourth Edition).
Harper & Row.\[Charniak, 1993\] Charniak, E. Statistical LanguageLearning.
MIT Press.\[Hatzivassiloglou and McKeown, 1993\] Hatzivassiloglou,V., and McKeown, K. R. Towards the automaticidentification of adjectival scales: Clustering adjectivesaccording to meaning.
In Proceedings of31st AnnualMeeting of the Association for ComputationalLinguistics, pp.
172-182.\[Hayashi, 1966\] Hayashi, O. Bunruigoihy&Syueisyuppan.
(In Japanese).\[Hindle, 1990\] Hindle, D. Noun classification frompredicate-argument structures.
In Proceedings of 28thAnnual Meeting of the Association for ComputationalLinguistics, pp.
268-275.\[Hirschman etal., 1975\] Hirschman, L., Grishman, R.,and Sager, N. Grammatically-based automatic wordclass formation.
Information Processing 8IManagement, 11, 39-57.\[Iwayama and Tokunaga, 1994\] Iwayama, M., andTokunaga, T. A probabilistic model for textcategorization: Based on a single random variable withmultiple values.
In Proceedings of 4th Conference onApplied Natural Language Processing.\[Iwayama and Tokunaga, 1995\] Iwayama, M., andTokunaga, T. Cluster-based text categorization: Acomparison of category search strategies.
InProceedings of A CM SIGIR'95, pp.
273-280.\[Masand et aL 1992\] Masand, B., Linoff, G., and Waltz,D.
Classifying news stories using memory basedreasoning.
In Proceedings of ACM SIGIR '9~, pp.59-65.\[Miller et al, 1993\] Miller, G. A., Bechwith, R.,Fellbaum, C., Gross, D., Miller, K., and Tengi, R. FivePapers on WordNet.
Tech.
rep. CSL Report 43,Cognitive Science Laboratory, Princeton University.Revised version.\[Nakano, 1981\] Nakano, H. Word classification supportsystem.
IPSJ-SIGCL, 25.\[Pereira et al, 1993\] Pereira, F., Tishby, N., and Lee, L.Distributional clustering of English words.
InProceedings of 31st Annual Meeting of the Associationfor Computational Linguistics, pp.
183-190.\[Real World Computing Partnership, 1995\] Real WorldComputing Partnership.
RWC text database.http ://www.
rwcp.
or.
j p/wswg, html.\[Resnik, 1992\] Resnik, P. A class-based approach tolexical discovery.
In Proceedings of 30th AnnualMeeting of the Association for ComputationalLinguistics, pp.
327-329.\[Mainichi Shimbun, 1991-1994\] Mainichi ShimbunCD-ROM '91-'94.\[Tanaka nd Nisina, 1987\] Construction of a thesaurusbased on superordinate/subordinate relations.IPSJ-SIGNL, NL64-~, 25-44.
(In Japanese).\[Tokunaga etaL, 1995\] Tokunaga, T., Iwayama, M.,and Tanaka, H. Automatic thesaurus constructionbased on grammatical relations.
In Proceedings offIJCAI '95, pp.
1308-1313.\[Uramoto, 1996\] Uramoto, N. Positioning unknownwords in a thesaurus by using information extractedfrom a corpus.
In Proceedings ofCOLING '96, pp.956-961.\[Ushioda, 1996\] Ushioda, A. Hierarchical c ustering ofwords.
In Proceedings off COLING '96, pp.
1159-1162.\[Weiss and Kulikowski, 1991\] Weiss, S. M., andKulikowsld, C. Computer Systems That Learn.
MorganKaufmann.\[Yarowsky, 1992\] Yarowsky, D. Word-sensedisambiguation using statistical models of Roget'scategories trained on large corpora.
In Proceedings ofCOLING '9& Vol.
2, pp.
454-460.21
