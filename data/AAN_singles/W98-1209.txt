III|/I!I!|IIIII|IIIIKnowledge Extraction and Recurrent Neural Networks:An Analysis of an Elman Network trained on a Natural Language LearningTaskIngo Schellhammer*#, Joachim Diederich*, Michael Towsey*,Claudia Brugman*** Neurocomputing Research Centre, Queensland University of Technology, QLD, 4001, Australia# Dept of Information Systems, University of Muenster, D-48149 Muenster, Germany**School of Languages, University of Otago, NewZealandj oachim@fit, qut.
edu.
auAbstractWe present results of experiments with Elman recurrentneural networks (Elman, 1990) trained on a naturallanguage processing task.
The task was to learn sequencesof word categories in a text derived from a primaryschool reader.
The grammar induced by the network wasmade xplicit by cluster analysis which revealed both therepresentations formed uring learning and enabled theconstruction ofstate-transition diagrams representing thegrammar.
A network initialised with weights based on aprior knowledge of the text's tatistics, learned slightlyfaster than the original network.In this paper we focus on the extraction ofgrammatical rules from trained Artificial NeuralNetworks and, in particular, Elman-type recurrentnetworks (Elman, 1990).
Unlike Giles & Omlin (1993a,b) who used an ANN to simulate adeterministic FiniteState Automaton (FSA) representing a regular grammar,we have extracted FSA's from a network trained on anatural anguage corpus.
The output of k-means clusteranalysis is converted to state-transition diagrams whichrepresent the grammar learned by the network.
Weanalyse the prediction and generalisation performanceof the grammar.1.
IntroductionSince their renaissance in the mid-1980s, ArtificialNeural Network (ANN) techniques have beensuccessfully applied across abroad spectrum of problemdomains such as pattern recognition and functionapproximation.
However despite these capabilities, to anend user an ANN is an arcane web of interconnectedinput, hidden, and output units.
Moreover an ANNsolution manifests itself entirely as sets of numbers inthe form of activation function parameters and weightvectors.
As such a trained ANN offers little or noinsight into the process by which it has arrived at agiven result nor, in general, the totality of "knowledge"actually embedded therein.
This lack of a capacity toprovide a "human comprehensible" explanation is seenas a clear impediment toa more widespread acceptanceof ANNs.In order to redress this situation, recently considerableeffort has been directed towards providing ANNs withthe requisite explanation capability.
In particular anumber of mechanisms, procedures, and techniqueshave been proposed and developed to extract theknowledge mbedded in a trained ANN as a set ofsymbolic rules which in effect mimic the behaviour ofthe ANN.
A recent survey conducted by Andrews et al(1995) offered an insight into the modus operandi of abroad cross-section fsuch techniques.2.
Methods2.1.
The dataThe data for these experiments were obtained from afirst-year primary school reader published circa 1950's(Hume).
To keep this initial task simple, sentences withembedded structures (relative clauses) and a length ofmore than eight words were eliminated.
The resultingcorpus consists of 106 sentences ranging from three toeight words in length, average length 5.06 words.
Thewords were converted to 10 lexieal categories, includinga sentence boundary marker.
The categories, theirabbreviations as used in the text and their percentfrequencies are shown in Table 1.
The resulting dataconsist of a string of 643 categories in 106 sentences.There are 62 distinct sentence sequences of which 43occur only once, the rest being replicated.
Themaximum replication of any sequence is eight-fold.Where sequences, uch as PR,VB,AR, are referred to inthe text, AR is the current input, VB the previous input(at time step t-I) and PR the input at time step t-2.2.2.
The networkElman simple recurrent networks (SRN), with ten inputand ten output units representing the sparse codedlexical categories, were trained on the categorysequence.
The task was to predict the next lexicalcategory given the current category.Schellhammer, Diederich, Towsey and Brugraan 73 Knowledge Extraction and Recurrent Neural NetsIngo Schellhammer, Joachim Diederich, Michael Towsey and Claudia Brugman (1998) Knowledge Extraction and RecurrentNeural Networks: An Analysis of an Elman Network trained on a Natural Language Learning Task.
In D.M.W.
Powers(ed.)
NeMLaP3/CoNLL98: New Methods in Language Processing and Computational N tural Language Learning, ACL, pp 73-78.The networks were trained by standard- backpropagation with momentum and state unitactivations were NOT reset o zero on presentation of asentence boundary.
Two networks were trained, onehaving two hidden units and the other nine, untilprediction error stopped declining.
The network withtwo hidden units had learned 51% of the training dataand that with nine hidden units had learned 69% of thedata.
By way of comparison, 48%, 62%, 72% and 76%correct predictions could be obtained using bi-, tri-, 4-and 5-gram models of the training data respectively.
Atthe end of training, the networks performed one passthrough the data without learning in order to recovertheir hidden unit activations.
Cluster analysis of the 642output vectors was performed by graphical means forthe two-hidden unit case and by k-means clustering forthe nine-hidden unit case.
Clusters from the latter casewere used to prepare FSA's.TABLE 1: Percent frequencies ofthe ten lexicalcategories in the text.Lexical Category %frequencyArticle AR 8%Conjunction CC 1%Preposition IN 7%Adjective JJ 4%Noun NN 30%Pronoun PR 10%Possessive (%) PS 2%Adverb RB 1%Verb VB 20%Sentence boundary /S 17%2.3.
Cluster Analysis and Preparation ofFinite State Automata(i) K-means cluster analysis oftware was used to labelthe 642 hidden unit activation vectors with clusternumbers between 1 and k. Each vector was thusassigned to a state, S~, where 1 < i < k and t uniquelyidentifies the time step for each member of cluster i.
(ii) For every current input, x' and previous tate, S I -I ,there is a transition to a new state, S I with a resultingoutput, ot.
A transition table was created from this data.
(iii) If the same input lead to more than one transitionfrom a given state, the transition having highestfrequency was chosen.
Similarly, if any transitionbrought about by a given input, generated more than onepossible output, the most frequent output was chosen.
(iv) The transition rules so derived were used toconstruct deterministic FSA's having k statescorresponding to the k clusters.
We generated ten FSA'swith k taking values in the range 6 to 22.
(v) Each automaton was tested on the string of 643categories used to train the original network.
They werescored for total correct predictions, the fraction ofmissing transitions and score on the non-missingtransitions.
(vi) In some experiments, low frequency transitions(having less than 5 occurrences) were pruned from theautomaton and the resulting automaton again tested forits performance on the original data sequence.
Missingtransitions were handled by jumping to a predefined'rescue' state and producing a predefined 'rescue'output.
In the default instance, the rescue state was thestate, whose preceding inputs had earliest position in thesentence.
The rescue output was always NN, thecategory having highest frequency.2.4.
Weight luitialisation with DomainKnowledgeFrom an examination of bigram probabilities derivedfrom the data sequence, it was determined that outputcategories NN and/S have the highest predictive rate.This knowledge can be used to initialise an Elmannetwork with non-random weights in the expectationthat training error should decline more rapidly than ifthe all the weights are initialised randomly.
Weinitialised an Elman net having 11 hidden units withrandom weights between -0.1 and 0.1, and thenmanually set to a value of +4.0 some of the weightslinking the hidden layer to the input units coding for NNand/S.
We refer to these as the set links.
In differenttrials, we set 0, 1, 5, 8, or 11 of both the NN and/S linksin such a way as to minimise the number of hidden unitshaving two set links.
Zero set links means that none ofthe original random weights were changed.3.
Results3.1.
Graphical cluster analysis of thenetwork having two hidden units.Graphical cluster analysis for the 2-hidden unit case isshown in Figure 1.
Clusters are labeled with the currentinput.
There is marked separation of clustersrepresenting the high frequency inputs, NN, VB,/S, PRand IN.
There is overlap of those clusters representinglow frequency inputs.
Although only 51% of thetraining set was learned by the network, there isevidence of further clustering based on the current andprevious inputs.
For example, Figure 2 shows clusterformation when NN is the current input and either AILNN, PR, PS, VB or/S is the previous input.
The PR,NNsub-cluster could be further broken down into sub-sub-clusters, representing the three input sequences/S,PR,NN and IN, PR,NN and VB,PR,NN.Schellhammer, Diederich, Towsey and Brugman 74 Knowledge Extraction and Recurrent Neural NetsIIIIIIIIIIII II I!
iI II II II II iI II II iI II II II II I10.80.60.40.200.441 .
?
.4 -  T ?
.
.Bo  4-m m m0.4?+J r .
?
?
?TI4- ++X X)~IEXm m0.2 0.6 0.8Unit I*CCIx INoJJxNNIo~ ?
PRI-PS IoRBI1 +VBI- ISFigure 1: Hidden unit activations (of an Elman network with two hidden units) labeled according to which of the teninput categories i the current input producing that activation.
The activations tend to be clustered according to theinput.
Clusters representing high frequency categories such as NN, VB and/S are more dispersed and broken intosub-clusters that represent both the current and previous inputs.
?qm ==0.50.40.30.20.100At& ~DO0.2 0.4 0.6 0.8 1unit I& VB,NNo/S,NN- AR,NN?/S,PR,NN= PS,NN?
NN, NNVB,PR,NNn IN,PR, NNFigure 2: Hidden unit activations of an Elman network (with two hidden units only) when the current input categoryis NN and the previous category is either VB,/S, AR, PR, PS or NN.3.2.
Analysis of the FSA'sThe performance of FSA's having 6 to 22 states isdisplayed in Table 2.
The second column gives the totalnumber of transitions permitted by the FSA.
The thirdcolumn gives the percent prediction score on thetraining data.
Best score is 60% which compares with69% of the training data learned by the original Elmannetwork from which the hidden unit activations wereobtained.
The total prediction score tends to increasewith the number of states.The fourth column of Table 2 gives the percentageof the 642 transitions in the data not permitted by theFSA's.
The number of missing transitions i  small, in allbut two cases less than 2%.
When a missing transitionoccurs, the FSA defaults to a 'rescue' state.
The percentcorrect predictions for non-missing transitions areshown in the rightmost column of Table 2.
They arelittle different from the total scores in most cases,simply because the number of missing transitions is sofew.The transition diagram for the 8-state FSA is shown inFigure 3.
The table in the top fight of the figure shows:(i) the number of visits to each state when the FSA istested, (ii) the percentage of correct predictionsassociated with a transition to that state and (iii) theaverage word-position in the sentence of the inputsleading to that state.Schellhammer, Diederich, Towsey and Brugman 75 Knowledge Extraction and Recurrent Neural NetsPR/NN state f req.
% correct position ~ VB/IN St 152 55.92% t.30$2 96 86.46% 3.06\ [~:C~ $3 155 49.68% 3.35(~2.P R/VB ~ ~ J ~  $4 42 83.33% 4.38S / N ~ ~ ' ~ / N ' ~ ~ N  ~ S5 34 70.59% 5.50~"" - -~ ~ .
.
.
.
.
.
I f  I N ~ $6 64 79.69% 5.67-- \" :.
: " .2 .
.
.
.
.
.
.
I I  I \ ~ $7 53 92.45% 5.68" ~ ' ~  v e/vff/N N/PS~ ~ ~ S8 46 86.96% 6.46 \ .
.
.
.N/NNFigure 3: The FSA having 8 states.
The double circle indicates an accept state.
If  the FSA is in an accept state and theinput is/S (end of sentence) then it returns to the start state, S1, with the output of NN.Transitions with thick arrows have a frequency count>20, transitions displayed with thin arrows have afrequency count of 5 to 20 and transitions with afrequoney count <5 are not shown to preserve clarity.The states have been numbered in sequence accordingto the average word position of their associated inputs.For example states 2, 6 and 8 all occur following inputof the NN category but they are distinguished in clusteranalysis by the NN having an average word-position ithe sentence of 3.1, 5.7 and 6.5 respectively.Table 2Performance ofFSA's prepared from k-means clusterNo.ofstates6 34 438 39 5410 45 5311 46 5612 53 5714 54 5616 59 5918 62 6020 67 6022 68 59analysis of hidden unit activations.%# of totaltransitions score% score% missing on non-transitions missingtransitions0.9 431.4 541.7 530.5 560.5 583.9 571.6 591.6 601.6 604.2 60No.
oftransitionsTable 3The effect of removing low frequency transitions froman FSA having 10 states% score% total % missing on non-missingscore transitions transitions45 53 1.7 5323 52 10.3 51The states having highest correct prediction rate, $7and $8, are associated with the ends of sentences.
$7 isreached when the last category in a sentence is predictedto be NN and $8 occurs when the end-of-sentence ispredicted.Many of the transitions in the FSA's occur with lowfrequency and could be primed with minimal loss ofperformance.
For example, the FSA with ten states has45 permitted transitions.
When transitions having afrequency <5 are pruned, the number of missingtransitions jumps from 1.7% to 10.28% but theprediction score drops only slightly from 53% to 51%(Table 3).Finally we look at the effect of the state chosen as therescue state for the FSA having 10 states.
The defaultstate is the state closest o the beginning of the sentence,in this case state 2.Schellhammer, Diederich, Towsey and Brugman 76 Knowledge Extraction and Recurrent Neural NetsIIIIIiIIIIIIIIIIilIIIIIIilIIIIIIIIIIIIIIIIIIIII!IIIIIIIIIIII/IIThe percent score of correct predictions i greater onlyin two other cases, that is when states 7 and 9 are usedas the 'rescue' state.
Changing the 'rescue' state alsochanges the number of transitions that the FSA does notrecognise.
However only in ilae case of rescue state 8 isthis number less than for rescue state 2.
It is apparentthat a decrease inthe number of missing transitions doesnot necessarily ead to a higher score.3.3.
Weight initialisation using domainknowledgeSetting links between the hidden layer and the NN and/S input units has a beneficial effect during the early?
stages of network training.
As indicated by the fasterinitial decrease inprediction error, the optimum numberof set links from inputs NN and/S was 5 or 8 (Figure 3).Table 4The effect of choice of'rescue' state on the)efformanceAverageRescue wordstate positionof inputsS2 1.36$5 2.94$8 3.30$3 4.18$7 4.37SlO 5.53$9 5.55$4 5.92$6 5.85SI 6.48ofthe resultinl~ 10-state FSA.% score%totalscore52505049535254495249% missing on non-transitions missingtransitions10.3 5115.3 539.7 4912.8 4816.2 5417.3 5511.5 5519.8 5318.9 5419.8 531.10.90.8 ~.7  .
.
.
.0 50 100 150 200epochs---e---O links --4a---1 link ~ 5 links ~, 8 links --e---11 links \]Figure 4: Output error of an Elman network over 200 training epochs following different weight initialisationprocedures.
'0-1inks' means that all links between hidden layer and input units are randomly initialised to values in \[-0.1, 0.1\].
The network has 11 hidden units.
Therefore 'll-links' means that every hidden unit has a set link (seemethods ection 2.4 for definition of this term) to the NN and the/S input units.
I, 5 and 8 links means that thisnumber of hidden units has a set link to the NN and/S input units.4.
DiscussionAlthough an Elman network with two hidden unitscould learn only 51% of the training data, neverthelessgraphical analysis reveals hierarchical clustering ofhidden unit activations.
There are dusters associatedwith each of the ten word categories (Figure I),although clusters associated with low frequency inputssuch as AR, CC and JJ tend to overlap.
Clusters labeledwith the high frequency inputs revealed obvious sub-clusters and sub-sub-clusters such as those shown inFigure 2.
In other words, the network had acquiredinternal representations of temporal sequences of atleast length 3.
However because the hidden unit spacehad such low dimensionality, it could not be partitionedby the output layer to achieve accurate prediction.An FSA with 18 states derived from k-meansclustering of hidden unit activations scored 60% on theoriginal training data (Table 2).
This compares with ascore of 69% by the original network and a score of62% when predicting with a trigram model.
Although intheory, the trigram model requires the calculation ofSchellhammer, Diederich, Towsey and Brugman 77 Knowledge Extraction and Recurrent Neural NetsI0,000 transition probabilities, it reduces to 42transition rules.
This compares with of 62 transitionsrules incorporated into the 18-state FSA.
Thus thetrigram model is a more compact definition of thegrammar.
However, low frequency transitions can betrimmed fi'om the FSA's with minimal loss ofperformance asis demonstrated for the I 0-state FSA inTable 3.Correct choice of the 'rescue' state is important forthe efficient performance of an FSA because itdetermines the FSA's ability to pick up the sentencestructure at~er amissing transition.
In order to automatethe production of FSA's following cluster analysis, werequire a heuristic for the choice of 'rescue' state.
Ourinitial choice, that state whose inputs on average areclosest o the beginning of the sentence, seems to be areasonable heuristic in the absence of other information(Table 4).
Likewise, choosing the highest frequencycategory (in our case, NN) as the 'rescue' output is alsoconfirmed by our results because the FSA scoresachieved on non-missing transitions are not much betterthan the total scores, despite 10-20% of missingtransitions (Table 4).The use of domain knowledge, such as categoryfrequencies, tobias weight initialisation is successful inreducing error faster in the early stages of learning.
Ofcourse if training is continued for long enough, then anymemory of the initial bias will be erased.
Best resultswere achieved when five links were set (such that nohidden unit had a set link to both the NN and/S inputs)or eight links were set (such that only five hidden unitshad set links to both the/fiN and/S inputs).5.
ConclusionsThis study has demonstrated one method for extractingthe knowledge ncoded in a trained neural network.Quite omen knowledge xtracted from neural networksis in the form of propositional rules (Andrews et al,1995) but these are not always the most appropriateformat for explication of network learning.
For examplewhere the network has been required to induce agrammar, cluster analysis of hidden unit activations andpreparation of an FSA is a powerful technique toexplicate the learned grammar.
However, for thisparticular task, there is a trade-off betweencomprehensibility of the FSA (fewer states means morecomprehensible) and its predictive performancecompared to the original neural network.
In theseexperiments an FSA with 18-states performed almost aswell as a trigram model.
The trigram model had theadvantage of compactness, but the FSA had theadvantage ofcomprehensibility.6.
ReferencesAndrews, tL, Diederich, J., & Tickle A.B.
(1995).
Asurvey and critique of techniques for extracting rulesfrom trained artificial neural networks.
Knowledge-Based Systems, 8(6); 373-389Elman, J.L.
(1990).
Finding Structure in Time.Cognitive Science 14, 179-211.Giles, C.L.
& Omlin, C.W.
(1993a).
Rule refinementwith recurrent neural networks.
Proceedings.
of the1EEE International Conference on Neural Networks(pp.
801-806).
San Francisco, CA.Giles, C.L.
& Omlin, C.W.
(1993b).
Extraction,insertion, and refinement of symbolic rules indynamically driven recurrent networks.
ConnectionScience, 5(3 & 4), 307-328.IrSchellhammer, Diederich, Towsey and Brugman 78 Knowledge Extraction and Recurrent Neural NetsIiIIIIIIIIIIIIIIIIIIIIIIIIi l
