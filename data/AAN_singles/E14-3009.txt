Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 76?84,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsUsing Minimal Recursion Semantics for Entailment RecognitionElisabeth LienDepartment of Informatics, University of Osloelien@ifi.uio.noAbstractThis paper describes work on using Mini-mal Recursion Semantics (MRS) repre-sentations for the task of recognising tex-tual entailment.
I use entailment datafrom a SemEval-2010 shared task to de-velop and evaluate an entailment recog-nition heuristic.
I compare my results tothe shared task winner, and discuss dif-ferences in approaches.
Finally, I run mysystem with multiple MRS representationsper sentence, and show that this improvesthe recognition results for positive entail-ment sentence pairs.1 IntroductionSince the first shared task on Recognising Text-ual Entailment (RTE) (Dagan et al., 2005) wasorganised in 2005, much research has been doneon how one can detect entailment between naturallanguage sentences.
A range of methods withinstatistical, rule based, and logical approaches havebeen applied.
The methods have exploited knowl-edge on lexical relations, syntactic and semanticknowledge, and logical representations.In this paper, I examine the benefits and pos-sible disadvantages of using rich semantic repre-sentations as the basis for entailment recognition.More specifically, I use Minimal Recursion Se-mantics (MRS) (Copestake et al., 2005) represen-tations as output by the English Resource Gram-mar (ERG) (Flickinger, 2000).
I want to investi-gate how logical-form semantics compares to syn-tactic analysis on the task of determining the en-tailment relationship between two sentences.
Tomy knowledge, MRS representations have so farnot been extensively used for this task.To this end, I revisit a SemEval shared task from2010 that used entailment recognition as a meansto evaluate parser output.
The shared task datawere constructed so as to require only syntacticanalysis to decide entailment for a sentence pair.The MRSs should perform well on such data, asthey abstract over irrelevant syntactic variation, asfor example use of active vs. passive voice, ormeaning-preserving variation in constituent order,and thus normalise at a highly suitable level of?who did what to whom?.
The core idea of myapproach is graph alignment over MRS represen-tations, where successful alignment of MRS nodesis treated as an indicator of entailment.This work is part of an ongoing dissertationproject, where the larger goal is to look moreclosely at correspondences between logical andtextual entailment, and the use of semantic repre-sentations in entailment recognition.Besides using MRS, one novel aspect of thiswork is an investigation of using n-best lists ofparser outputs in deciding on entailment relations.In principle, the top-ranked (i.e., most probable)parser output should correspond to the intendedreading, but in practise this may not always bethe case.
To increase robustness in our approachto imperfect parse ranking, I generalise the sys-tem to operate over n-best lists of MRSs.
Thissetup yields greatly improved system performanceand advances the state of the art on this task, i.e.,makes my system retroactively the top performerin this specific competition.The rest of this paper is organised as follows: insection 2, I describe the task of recognising text-ual entailment.
I also briefly describe MRS rep-resentations, and mention previous work on RTEusing MRS.
In section 3, I analyse the shared taskdata, and implement an entailment decision com-ponent which takes as input MRS representationsfrom the ERG.
I then analyse the errors that thecomponent makes.
Finally, I compare my resultsto the actual winner of the 2010 shared task.
Insection 4, I generalise my approach to 10-best listsof MRSs.762 BackgroundIn the following, I briefly review the task of recog-nising entailment between natural language sen-tences.
I also show an example of an MRS rep-resentation, and mention some previous work onentailment recognition that has used MRSs.2.1 Recognising Textual EntailmentResearch on automated reasoning has always beena central topic in computer science, with much fo-cus on logical approaches.
Although there hadbeen research on reasoning expressed in naturallanguage, the PASCAL Recognising Textual En-tailment (RTE) Challenge (Dagan et al., 2005)spurred wide interest in the problem.
In the taskproposed by the RTE Challenge, a system is re-quired to recognise whether the meaning of onetext can be inferred from the meaning of anothertext.
Their definition of inference, or textual en-tailment, is based on the everyday reasoning abili-ties of humans rather than the logical properties oflanguage.The RTE Challenge evolved from the relativelysimple task of making binary decisions about sen-tence pairs into more complex variants with manycategories and multi-sentence texts.
The data setsissued by the organisers over the years providevaluable research material.
However, they con-tain a wide range of inference phenomena, and re-quire both ontological and world knowledge.
Thedata set that I have used for the present work, thePETE data set, focusses on syntactic phenomena,and does not require any knowledge about the stateof the world or ontological relations.2.2 Minimal Recursion SemanticsMinimal Recursion Semantics (MRS) (Copestakeet al., 2005) is a framework for computational se-mantics which can be used for both parsing andgeneration.
MRS representations are expressive,have a clear interface with syntax, and are suitablefor processing.
MRSs can be underspecified withregard to scope in order to allow a semanticallyambiguous sentence to be represented with a sin-gle MRS that captures every reading.
MRS is inte-grated with the HPSG English Resource Grammar(ERG) (Flickinger, 2000).An MRS representation contains a multiset ofrelations, called elementary predications (EPs).An EP usually corresponds to a single lexeme, butcan also represent general grammatical features.Each EP has a predicate symbol which, in the caseof lexical predicates, encodes information aboutlemma, part-of-speech, and sense distinctions.
AnEP also has a label (also called handle) attachedto it.
Each EP contains a list of numbered argu-ments: ARG0, ARG1, etc.
The value of an ar-gument can be either a scopal variable (a handlewhich refers to another EP?s label) or a non-scopalvariable (events or states, or entities).The ARG0 position of the argument list hasthe EP?s distinguished variable as its value.
Thisvariable denotes either an event or state, or areferential or abstract entity (eior xi, respec-tively).
Each non-quantifier EP has its unique dis-tinguished variable.Finally, an MRS has a set of handle constraintswhich describe how the scopal arguments of theEPs can be equated with EP labels.
A constrainthi=qhjdenotes equality modulo quantifier in-sertion.
In addition to the indirect linking throughhandle constraints, EPs are directly linked by shar-ing the same variable as argument values.
The re-sulting MRS forms a connected graph.In figure 2, we see an MRS for the sentenceSomebody denies there are barriers from thePETE development data (id 4116)1.
The topmostrelation of the MRS is deny v to, which hastwo non-empty arguments: x5and h10.
x5is thedistinguished variable of the relations some qand person, which represent the pronoun some-body.
A handle constraint equates the senten-tial variable h10with h11, which is the label ofbe v there.
This last relation has x13as itssole argument, which is the distinguished variableof udef q and barrier n to, the representa-tion of barriers.2.3 Previous Work on RTE using MRSTo my knowledge, MRS has not been used exten-sively in entailment decision systems.
Notable ex-amples of approaches that use MRSs are Wotzlawand Coote (2013), and Bergmair (2010).In Wotzlaw and Coote (2013), the authorspresent an entailment recognition system whichcombines high-coverage syntactic and semantictext analysis with logical inference supported byrelevant background knowledge.
Their systemcombines deep and shallow linguistic analysis,and transforms the results into scope-resolved1The event and entity variables of the EPs often havegrammatical features attached to them.
I have removed thesefeatures from the MRS for the sake of readability.77?h1,h4:proper q?0:5?
(ARG0 x6, RSTR h5, BODY h7),h8:named?0:5?
(ARG0 x6, CARG Japan),h2: deny v to?6:12?
(ARG0 e3, ARG1 x6, ARG2 h10, ARG3 i9),h11: be v there?19:22?
(ARG0 e12, ARG1 x13),h14:udef q?23:37?
(ARG0 x13, RSTR h15, BODY h16),h17: real a 1?23:27?
(ARG0 e18, ARG1 x13),h17: barrier n to?28:37?
(ARG0 x13, ARG1 i19){h15=qh17, h10=qh11, h5=qh8, h1=qh2} ?Figure 1: MRS for the sentence Japan denies there are real barriers.?h1,h4:person?0:8?
(ARG0 x5),h6: some q?0:8?
(ARG0 x5, RSTR h7, BODY h8),h2: deny v to?9:15?
(ARG0 e3, ARG1 x5, ARG2 h10, ARG3 i9),h11: be v there?22:25?
(ARG0 e12, ARG1 x13),h14:udef q?26:35?
(ARG0 x13, RSTR h15, BODY h16),h17: barrier n to?26:35?
(ARG0 x13, ARG1 i18){h15=qh17, h10=qh11, h7=qh4, h1=qh2} ?Figure 2: MRS for the sentence Somebody denies there are barriers.MRS representations.
The MRSs are in turn trans-lated into another semantic representation for-mat, which, enriched with background knowledge,forms the basis for logical inference.In Bergmair (2010), we find a theory-driven ap-proach to textual entailment that uses MRS as anintermediate format in constructing meaning rep-resentations.
The approach is based on the as-sumptions that the syllogism is a good approx-imation of natural language reasoning, and thata many-valued logic provides a better model ofnatural language semantics than bivalent logics do.MRSs are used as a step in the translation of natu-ral language sentences into logical formulae thatare suitable for processing.
Input sentences areparsed with the ERG, and the resulting MRSs aretranslated into ProtoForms, which are fully recur-sive meaning representations that are closely re-lated to MRSs.
These ProtoForms are then decom-posed into syllogistic premises that can be pro-cessed by an inference engine.3 Recognising Syntactic Entailmentusing MRSsIn this section, I briefly review the SemEval-2010shared task that used entailment decision as ameans of evaluating parsers.
I then describe theentailment system I developed for the shared taskdata, and compare its results to the winner of theoriginal task.3.1 The PETE Shared TaskParser Evaluation using Textual Entailments(PETE) was a shared task in the SemEval-2010 Evaluation Exercises on Semantic Evalua-tion (Yuret et al., 2010).
The task involved build-ing an entailment system that could decide entail-ment for sentence pairs based on the output of aparser.
The organisers proposed the task as an al-ternative way of evaluating parsers.
The parserevaluation method that currently dominates thefield, PARSEVAL (Black et al., 1991), comparesthe phrase-structure bracketing of a parser?s outputwith the gold annotation of a treebank.
This makesthe evaluation both formalism-dependent and vul-nerable to inconsistencies in human annotations.The PETE shared task proposes a different eval-uation method.
Instead of comparing parser outputdirectly to a gold standard, one can evaluate in-directly by examining how well the parser outputsupports the task of entailment recognition.
Thisstrategy has several advantages: the evaluation isformalism-independent, it is easier for annotatorsto agree on entailment than on syntactic categoriesand bracketing, and the task targets semanticallyrelevant phenomena in the parser output.
The dataare constructed so that syntactic analysis of the78sentences is sufficient to determine the entailmentrelationship.
No background knowledge or rea-soning ability is required to solve the task.It is important to note that in the context of thePETE shared task, entailment decision is not agoal in itself, it is just a tool for parser evaluation.The PETE organisers created two data sets forthe task: a development set of 66 sentence pairs,and a test set of 301 pairs.
The data sets were builtby taking a selection of sentences that contain syn-tactic dependencies that are challenging for state-of-the-art parsers, and constructing short entail-ments that (in the case of positive entailment pairs)reflect these dependencies.
The resulting sentencepairs were annotated with entailment judgementsby untrained annotators, and only sentence pairswith a high degree of inter-annotator agreementwere kept.20 systems from 7 teams participated in thePETE task.
The best scoring system was the Cam-bridge system (Rimell and Clark, 2010), with anaccuracy of 72.4 %.3.2 The SystemMy system consists of an entailment decisioncomponent that processes MRS representations asoutput by the ERG2.
The entailment decision com-ponent is a Python implementation I developed af-ter analysing the PETE development data.The core idea is based on graph alignment,seeking to establish equivalence relations betweencomponents of MRS graphs.
In a nutshell, if allnodes of the MRS corresponding to the hypothesiscan be aligned with nodes of the MRS of the text,then we will call this relation MRS inclusion, andtreat it as an indicator for entailment.3Further-more, the PETE data set employs a limited rangeof ?robust?
generalisations in hypothesis strings,for example replacing complex noun phrases fromthe text by an underspecified pronoun like some-body.
To accomodate such variation, my graphalignment procedure supports a number of ?ro-bust?
equivalences, for example allowing an arbi-trarily complex sub-graph to align with the graphfragment corresponding to expressions like some-body.
These heuristic generalisations were de-signed in response to an in-depth analysis of thePETE development corpus, where I made the fol-2I used the 1212 release of the ERG, in combination withthe PET parser (Callmeier, 2000).3On this view, bidirectional inclusion indicates that thetwo MRS graphs are isomorphic, i.e., logically equivalent.lowing observations for the sentences of positiveentailment pairs (I use Tsentto mean the text sen-tence, and Hsentto mean the hypothesis sentence):?
Hsentis always shorter than Tsent.?
In some cases, Hsentis completely includedin Tsent.?
Mostly, Hsentis a substructure of Tsentwithminor changes:?
Tsentis an active sentence, while Hsentis passive.?
A noun phrase in Tsenthas been re-placed by somebody, someone or some-thing in Hsent.?
The whole of Hsentcorresponds to acomplex noun phrase in Tsent.In addition, I noted that the determiner or defi-niteness of a noun phrase often changes from textto hypothesis without making any difference forthe entailment.
I also noted that, in accordancewith the PETE design principles, the context pro-vided by the text sentence does not influence theentailment relationship.In the negative entailment pairs the hypothesisis usually a combination of elements from the textthat does not match semantically with the text.I examined treebanked MRS representations ofthe PETE development data in order to developan entailment recognition heuristic.
I found thatby taking the EPs that have an event variable astheir distinguished variable, I would capture thesemantically most important relations in the sen-tence (the verbs).
The heuristic picks out all EPswhose ARG0 is an event variable from both thetext and hypothesis MRSs?let us call them eventrelations.
Then it tries to match all the event re-lations of the hypothesis to event relations in thetext.
In the following, Tmrsmeans the MRS forthe text sentence, and Hmrsthe MRS for the hy-pothesis.
We say that two event relations matchif:1. they are the same or similar relations.
Twoevent relations are the same or similar if theyshare the same predicate symbol, or if theirpredicate symbols contain the same lemmaand part-of-speech.2.
and all their arguments match.
Two argu-ments in the same argument position matchif:79?
they are the same relation; or?
the argument in Tmrsrepresents a nounphrase and the argument in Hmrsissomebody/someone/something; or?
the argument in Tmrsis either a scopalrelation or a conjunction relation, andthe argument in the hypothesis is an ar-gument of this relation; or?
the argument in Hmrsis not expressed.Let us see how the heuristic works for the fol-lowing sentence pair (PETE id 4116):4116 Tsent: The U.S. wants the removal of whatit perceives as barriers to investment; Japandenies there are real barriers.4116 Hsent: Somebody denies there are barriers.Figure 2 shows the MRS for 4116 Hsent.
Fig-ure 1 shows an MRS for the part of 4116 Tsentthat entails 4116 Hsent: Japan denies there arereal barriers.
The heuristic picks out two rela-tions in 4116 Hmrsthat have an event variableas their distinguished variable: deny v to andbe v there.
It then tries to find a match forthese relations in the set of event relations in4116 Tmrs:?
The relation deny v to also appears in4116 Tmrs, and all its argument variables canbe unified since their relations match accord-ing to the heuristic:?
x5unifies with x6, since some q andperson (which represent somebody)match proper q and named (whichrepresent Japan4)?
h10unifies with h10, since they both (viathe handle constraints) lead to the rela-tion be v there.?
The variables i9and i9both representunexpressed arguments, and so are triv-ially unified.?
The relation be v there matches the cor-responding relation in 4116 Tmrs, since theirsingle argument x13denotes the same rela-tions: udef q and barrier n to.4According to the heuristic, any proper name matches thepronoun somebody, so we do not have to consider the actualproper name involved.This strategy enables us to capture all the corerelations of the hypothesis.
When examining thedata one can see that, contrary to the design prin-ciples for the PETE data, some sentence pairs dorequire reasoning.
The heuristic will fail to cap-ture such pairs.The ERG is a precision grammar and does notoutput analyses for sentences that are ungrammat-ical.
Some of the sentences in the PETE data setsare arguably in a grammatical gray zone, and con-sequently the ERG will not give us MRS represen-tations for such sentences.
In some cases, errors inan MRS can also cause the MRS processing in thesystem to fail.
Therefore, my system must havea fallback strategy for sentence pairs were MRSsare lacking or processing fails.
The system answerNO in such cases, since it has no evidence for anentailment relationship.For the development process I used both tree-banked and 1-best MRSs.3.3 Error analysisTables 1 and 2 show the entailment decision re-sults for 1-best MRSs for the PETE developmentand test data.
The ERG parsed 61 of the 66 pairsin the development set, and 285 of the 301 pairs inthe test set.
The five development set pairs that didnot get a parse were all negative entailments pairs.Of the 16 test pairs that failed to parse, 10 werenegative entailment pairs.
The system?s fallbackstrategy labels these as NO.gold YES: 38 gold NO: 28sys YES 25 2sys NO 13 26Table 1: The results for 1-best MRSs for the PETEdevelopment data.gold YES: 156 gold NO: 145sys YES 78 10sys NO 78 135Table 2: The results for 1-best MRSs for the PETEtest data.The implementation of the heuristic is fine-grained in its treatment of the transformationsfrom text to hypothesis that I found in the PETEdevelopment sentences.
Although I tried to antici-pate possible variations in the test data set, it in-evitably contained cases that were not covered by80the code.
This meant that occasionally the systemwas not able to recognise an entailment.However, most of the incorrect judgementswere caused either by errors in the MRSs, or byfeatures of the MRSs or the PETE sentence pairsthat are outside the scope of my heuristic:1.
Recognising the entailment depends on infor-mation about coreferring expressions, whichis not part of the MRS analyses.2.
The entailment (or non-entailment) relation-ship depends on something other than syntac-tic structure.
Recognising the entailment re-quires background knowledge and reasoning.This means the entailment is really outsidethe stated scope of the PETE task.3.
For some of the PETE sentence pairs, thegold annotation can be discussed.
The fol-lowing pair (PETE id 2079) is labeled NO,but is structurally similar to sentence pairsin the data set that are labeled YES: Also,traders are in better shape today than in 1987to survive selling binges.
?
Binges are sur-vived.3.4 Results and Comparison to Shared TaskWinnerAt this point, we are ready to compare the resultswith the winner of the PETE shared task.
Of the 20systems that took part in the shared task, the bestscoring participant was the Cambridge system, de-veloped by Laura Rimell and Stephen Clark ofthe University of Cambridge (Rimell and Clark,2010).
Their system had an overall accuracy of72.4 %.
My focus here is on comparing the perfor-mance of the entailment systems, not the parsers.The Cambridge system: The system consists ofa parser and an entailment system.
Rimell andClark used the C&C parser, which can produceoutput in the form of grammatical relations, that is,labelled head-dependencies.
They used the parserwith the Stanford Dependency scheme (de Marn-effe et al., 2006), which defines a hierarchy of 48grammatical relations.The Cambridge entailment system was based onthe assumption that the hypothesis is a simplifiedversion of the text.
In order to decide entailment,one can then compare the grammatical relations?the SDs?of the two sentences5.
If the SDs ofthe hypothesis are a subset of the SDs of the text,then the text entails the hypothesis.
However, be-cause the hypotheses in the PETE data are oftennot a direct substructure of the text, Rimell andClark used heuristics to deal with alterations be-tween sentences (in the following, I use TsdandHsdto mean the grammatical relations of text andhypothesis sentences, respectively):1.
If a SD in the hypothesis contains a to-ken which is not in the text, this SD is ig-nored.
This means that passive auxiliaries,pronouns, determiners and expletive subjectsthat are in Hsdbut not in Tsdare ignored.2.
Passive subjects are equated with direct ob-jects.
This rule handles the PETE pairs wherethe active verb of the text has become a pas-sive in the hypothesis.3.
When checking whether the SDs in Hsdare asubset of the SDs in Tsd, only subject and ob-ject relations are considered (core relations).4.
The intersection of SDs in Tsdand Hsdhasto be non-empty (this is not restricted to sub-jects and objects).To sum up: if core(Hsd)?
core(Tsd) and Hsd?Tsd6= ?, then Tsententails Hsent.Results for 1-best (automatically generated)test data: We can now compare the results fromthe system for 1-best test data with those of Cam-bridge.In order to compare the test data results frommy system with those of Rimell & Clark, I haveto account for those sentence pairs that the ERGcould not parse (16) and the MRS pairs that mysystem could not process (1).
I use the same fall-back strategy as Rimell & Clark, and let the en-tailment decision be NO for those sentence pairsthe system cannot handle.
For comparison, I alsoinclude the results for SCHWA (University of Syd-ney), the second highest scorer of the systems thatparticipated in the shared task.From the results in table 3 we can see that mysystem would have done well in the shared task.An accuracy of 70.7 % places the system a little5In Rimell and Clark (2010), the authors used the abbre-viation GR to mean the grammatical relations of the StanfordDependency scheme.
I use SD instead, to avoid confusionwith the term GR as used by Carroll et al.
(1999)81System A P R F1Cambridge 72.4 79.6 62.8 70.2My system 70.7 88.6 50.0 63.9SCHWA 70.4 68.3 80.1 73.7Table 3: The two top systems from the PETEshared task (Yuret et al., 2010) compared to mysystem.
Accuracy (A) gives the percentage of cor-rect answers for both YES and NO.
Precision (P),recall (R) and F1 are calculated for YES.ahead of SCHWA, the second best system.
Wealso note that my system has a significantly higherprecision on the YES judgements than the othertwo systems.Resuls for gold/treebanked development data:In order to evaluate their entailment system,Rimell & Clark ran their system on manually an-notated grammatical relations.
Given a valid en-tailment decision approach and correct SDs, thesystem could in theory achieve 100 % accuracy.Cambridge achieved 90.9 % accuracy on thesegold data.
The authors noted that one incorrectdecision was due to a PETE pair requiring coref-erence resolution, three errors were caused by cer-tain transformations between text and hypothesisthat were not covered by their heuristic, and twoerrors occured because the heuristic ignored someSDs that were crucial for recognising the entail-ments.When I ran my system on treebanked MRSs forthe PETE development data, it achieved an accu-racy of 92.4 %, which is slightly better than theaccuracy for Cambridge.MRSs vs. grammatical relations: The infor-mation that the Cambridge system uses is worddependencies that are typed with grammatical re-lations.
More specifically, Cambridge uses subjectand object relations between words to decide en-tailment.
Because the relations are explicit?weknow exactly what type of grammatical relationthat holds between two words?it is easy to selectthe relations in Hsdthat one wants to check.The EPs of MRSs are a mixture of lexical re-lations, and various syntactic and semantic re-lations.
A lot of the grammatical informationthat is explicitly represented as SDs in the Stan-ford scheme is implicitly represented in MRS EPsas argument-value pairs.
For example, the sub-ject relation between he and the verb in he runsis represented as (nsubj run he) in Stan-ford notation.
The corresponding representationin an MRS is [ run v 1 LBL: h ARG0: eARG1: x ], where ARG1 denotes the proto-agent of the verb.
The assignment of semanticroles to arguments in EPs is not affected by pas-sivisation or dative shift, whereas such transforma-tions can cause differences in SDs.
For sentencepairs where these phenomena occur, it is easierto match EPs and their arguments than the corre-sponding grammatical relations.Cambridge heuristic vs. my heuristic: TheCambridge system checks whether the subject andobject relations in Hsdalso appear in Tsd.
How-ever, because their heuristic ignores tokens in thehypothesis that are not in the text, the system incertain cases does not check core relations that arecrucial to the entailment relationship.My system checks whether the event relationsin Hmrsalso appear in Tmrs, and whether theirarguments can be matched.
Whereas the Cam-bridge system ignores tokens in the hypothesis thathave no match in the text, my heuristic has ex-plicit rules for matching arguments that are dif-ferent.
It makes my system more vulnerable tounseen cases, but at the same time makes the pos-itive entailment decisions more well-founded.
Itleads my system to make fewer mistakes on theNO entailments than both the Cambridge systemand SCHWA.In their paper, Rimell & Clark do not providean error analysis for the PETE test set, so I can-not do a comparative error analysis with my sys-tem.
However, they go into detail on some analy-ses and mention some errors that the system madeon the development data (both automatically gen-erated and gold-standard), and I can compare theseto my own results on the development data.
(I willonly look at those analyses where there are signif-icant differences between Cambridge and my sys-tem.
)PETE id 5019: He would wake up in the mid-dle of the night and fret about it.
?
He wouldwake up.
The Cambridge system recognises thiscorrectly, but the decision is based only on the sin-gle SD match (nsubj would he).
The otherSDs are ignored, since they are non-core accord-ing to the heuristic.
In my system, the YES de-cision is based on matching of both the relationwould v modal which has wake v up as itsscopal argument, and wake v up itself with its82pronoun argument.PETE id 3081.N: Occasionally, the childrenfind steamed, whole-wheat grains for cereal whichthey call ?buckshot?.
?
Grains are steamed.
Thetransformation of steamed from an adjective inTsentto a passive in Hsentwas not accounted forin the Cambridge heuristic, and the system failedto recognise the entailment.
In the MRS analysesfor these sentences, steamed gets exactly the samerepresentation, and my entailment system can eas-ily match the two.The Cambridge paper mentions that two of theerrors the entailment system made were due to thefact that a non-core relation or a pronoun in thehypothesis, which Cambridge ignores, was crucialfor recognising an entailment.
The paper does notmention which sentences these were, but it seemslikely that they would not pose a problem to mysystem.4 Using 10-best MRSsSo far, I have used only one MRS per sentencein the entailment decision process.
The entail-ment decisions were based on the best MRSs for asentence pair, either chosen manually (treebankedMRSs) or automatically (1-best MRSs).
In bothcases, it can happen that the MRS chosen for asentence is not actually the best interpretation, ei-ther because of human error during treebanking,or because the best MRS is not ranked as numberone.I also noticed that many of the incorrect deci-sions that the system made were caused either byerrors in the MRSs or by incompatible analysesfor a sentence pair.
In both cases, the correct orcompatible MRS could possibly be found furtherdown the list of analyses produced by the ERG.These shortcomings can perhaps be remedied byexamining more MRS analyses for each sentencein a pair.When doing n-best parsing on the PETE datasets, we can expect a high number of analysesfor the text sentences, and fewer analyses for theshorter hypotheses.
By setting n to 10, I hope tocapture a sufficient number of the best analyses.With 10-best parsing, I get on average 9 analysesfor the text sentences, and 3 analyses for the hy-potheses.I use a simple strategy for checking entailmentbetween a set of MRSs for the text and a set ofMRSs for the hypothesis: If I can find one caseof entailment between two MRSs, then I concludethat the text entails the hypothesis.In table 4, I compare my previous results withthose that I get with 10-best MRSs.
As we can see,the system manages to recognise a higher numberof positive entailment pairs, but the precision goesdown a little.
Using 10-best MRSs ensures that wedo not miss out on positive entailment pairs wherean incorrect MRS is ranked as number one.
How-ever, it also increases the number of spurious en-tailments caused by MRSs whose event relationsaccidentally match.
Variation of n allows trad-ing off precision and recall, and n can possibly betuned separately for texts and hypotheses.When we compare 10-best entailment checkingto the PETE shared task results, we see that myresults improve substantially over the previouslyhighest reported performance.
My system scoresabout 4 accuracy points higher than the system ofRimell & Clark, and more than 5 points for F1.System A P R F1One MRS 70.7 88.6 50.0 63.910-best 76.4 81.4 70.5 75.5Table 4: Here I compare system results for oneMRS and 10-best MRSs.
Accuracy (A) gives thepercentage of correct answers for both YES andNO.
Precision (P), recall (R) and F1 are calculatedfor YES.5 Conclusions and Future WorkIn this paper, I have demonstrated how to buildan entailment system from MRS graph alignment,combined with heuristic ?robust?
generalisations.I compared my results to the winner of the 2010PETE shared task, the Cambridge system, whichused grammatical relations as the basis for entail-ment decision.
I performed an in-depth compar-ison of types and structure of information rele-vant to entailment in syntactic dependencies vs.logical-form meaning representations.
The systemachieved competitive results to the state of the art.Results on gold-standard parser output suggestssubstantially better performance in my entailmentsystem than the PETE shared task winner.I also generalised the approach to using n-best lists of parser outputs.
Using 1-best out-put makes entailment decision vulnerable to in-correct MRS analyses being ranked as numberone.
Using n-best can counterbalance this prob-83lem.
With 10-best MRSs, a significant improve-ment was achieved in the performance of the en-tailment decision system.
The n-best setup offersthe flexibility of trading off precision and recall.With the 10-best MRS lists, I used a simplestrategy for entailment decision: if one MRS pairsupports a YES decision, we say that we have en-tailment.
It would be interesting to explore morecomplex strategies, such as testing all the MRScombinations for a sentence pair for a certain n,and decide for the majority vote.
One could alsomake use of the conditional probabilities on parseroutputs, for instance by multiplying the probabil-ities for each MRS pair, summing up for YES vs.NO decisions, and setting a threshold for the finaldecision.AcknowledgmentsI am grateful to my supervisors Jan Tore L?nningand Stephan Oepen for suggesting this task, andfor their valuable advice on my work.
I also ap-preciate the thorough comments made by the threeanonymous reviewers.ReferencesRichard Bergmair.
2010.
Monte Carlo Semantics: Ro-bust Inference and Logical Pattern Processing withNatural Language Text.
Ph.D. thesis, University ofCambridge.E.
Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A Pro-cedure for Quantitatively Comparing the SyntacticCoverage of English Grammars.
In Speech and nat-ural language: proceedings of a workshop, held atPacific Grove, California, February 19-22, 1991,page 306.
Morgan Kaufman Pub.Ulrich Callmeier.
2000.
PET.
A platform for ex-perimentation with efficient HPSG processing tech-niques.
Journal of Natural Language Engineering,6(1):99108, March.John A. Carroll, Guido Minnen, and Ted Briscoe.1999.
Corpus annotation for parser evaluation.
Pro-ceedings of the EACL workshop on Linguistically In-terpreted Corpora (LINC).Ann Copestake, Dan Flickinger, Carl Pollard, andIvan A.
Sag.
2005.
Minimal Recursion Semantics:An Introduction.
Research on Language & Compu-tation, 3(2):281?332.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The PASCAL Recognising Textual Entail-ment Challenge.
In Joaquin Qui?nonero Candela, IdoDagan, Bernardo Magnini, and Florence d?Alch?eBuc, editors, MLCW, volume 3944 of Lecture Notesin Computer Science, pages 177?190.
Springer.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InProceedings of LREC, Genoa, Italy.Dan Flickinger.
2000.
On building a more effcientgrammar by exploiting types.
Natural LanguageEngineering, 6(1):15?28.Laura Rimell and Stephen Clark.
2010.
Cam-bridge: Parser Evaluation using Textual Entailmentby Grammatical Relation Comparison.
In Proceed-ings of the 5th International Workshop on SemanticEvaluation, ACL 2010.Andreas Wotzlaw and Ravi Coote.
2013.
A Logic-based Approach for Recognizing Textual EntailmentSupported by Ontological Background Knowledge.CoRR, abs/1310.4938.Deniz Yuret, Aydin Han, and Zehra Turgut.
2010.SemEval-2010 Task 12: Parser Evaluation usingTextual Entailments.
In Proceedings of the 5thInternational Workshop on Semantic Evaluation,pages 51?56.
Association for Computational Lin-guistics.84
