Optimization in Multimodal InterpretationJoyce Y. Chai*         Pengyu Hong+ Michelle X. Zhou?
Zahar Prasov**Computer Science and EngineeringMichigan State UniversityEast Lansing, MI 48824{jchai@cse.msu.edu,prasovz@cse.msu.edu}+Department of StatisticsHarvard UniversityCambridge, MA 02138hong@stat.harvard.edu?Intelligent Multimedia InteractionIBM T. J. Watson Research Ctr.Hawthorne, NY 10532mzhou@us.ibm.comAbstractIn a multimodal conversation, the way userscommunicate with a system depends on theavailable interaction channels and the situatedcontext (e.g., conversation focus, visual feedback).These dependencies form a rich set of constraintsfrom various perspectives such as temporalalignments between different modalities,coherence of conversation, and the domainsemantics.
There is strong evidence thatcompetition and ranking of these constraints isimportant to achieve an optimal interpretation.Thus, we have developed an optimization approachfor multimodal interpretation, particularly forinterpreting multimodal references.
A preliminaryevaluation indicates the effectiveness of thisapproach, especially for complex user inputs thatinvolve multiple referring expressions in a speechutterance and multiple gestures.1 IntroductionMultimodal systems provide a natural andeffective way for users to interact with computersthrough multiple modalities such as speech,gesture, and gaze (Oviatt 1996).
Since the firstappearance of ?Put-That-There?
system (Bolt1980), a variety of multimodal systems haveemerged, from early systems that combine speech,pointing (Neal et al, 1991), and gaze (Koons et al1993), to systems that integrate speech with peninputs (e.g., drawn graphics) (Cohen et al, 1996;Wahlster 1998; Wu et al, 1999), and systems thatengage users in intelligent conversation (Cassell etal., 1999; Stent et al, 1999; Gustafson et al, 2000;Chai et al, 2002; Johnston et al, 2002).One important aspect of building multimodalsystems is multimodal interpretation, which is aprocess that identifies the meanings of user inputs.In a multimodal conversation, the way userscommunicate with a system depends on theavailable interaction channels and the situatedcontext (e.g., conversation focus, visual feedback).These dependencies form a rich set of constraintsfrom various aspects (e.g., semantic, temporal, andcontextual).
A correct interpretation can only beattained by simultaneously considering theseconstraints.
In this process, two issues areimportant: first, a mechanism to combineinformation from various sources to form anoverall interpretation given a set of constraints; andsecond, a mechanism that achieves the bestinterpretation among all the possible alternativesgiven a set of constraints.
The first issue focuses onthe fusion aspect, which has been well studied inearlier work, for example, through unification-based approaches (Johnston 1998) or finite stateapproaches (Johnston and Bangalore, 2000).
Thispaper focuses on the second issue of optimization.As in natural language interpretation, there isstrong evidence that competition and ranking ofconstraints is important to achieve an optimalinterpretation for multimodal language processing.We have developed a graph-based optimizationapproach for interpreting multimodal references.This approach achieves an optimal interpretationby simultaneously applying semantic, temporal,and contextual constraints.
A preliminaryevaluation indicates the effectiveness of thisapproach, particularly for complex user inputs thatinvolve multiple referring expressions in a speechutterance and multiple gestures.
In this paper, wefirst describe the necessities for optimization inmultimodal interpretation, then present our graph-based optimization approach and discuss how ourapproach addresses key principles in OptimalityTheory used for natural language interpretation(Prince and Smolensky 1993).2 Necessities for Optimization inMultimodal InterpretationIn a multimodal conversation, the way a userinteracts with a system is dependent not only onthe available input channels (e.g., speech andgesture), but also upon his/her conversation goals,the state of the conversation, and the multimediafeedback from the system.
In other words, there isa rich context that involves dependencies frommany different aspects established during theinteraction.
Interpreting user inputs can only besituated in this rich context.
For example, thetemporal relations between speech and gesture areimportant criteria that determine how theinformation from these two modalities can becombined.
The focus of attention from the priorconversation shapes how users refer to thoseobjects, and thus, influences the interpretation ofreferring expressions.
Therefore, we need tosimultaneously consider the temporal relationsbetween the referring expressions and the gestures,the semantic constraints specified by the referringexpressions, and the contextual constraints fromthe prior conversation.
It is important to have amechanism that supports competition and rankingamong these constraints to achieve an optimalinterpretation, in particular, a mechanism to allowconstraint violation and support soft constraints.We use temporal constraints as an example toillustrate this viewpoint1.
The temporal constraintsspecify whether multiple modalities can becombined based on their temporal alignment.
Inearlier work, the temporal constraints areempirically determined based on user studies(Oviatt 1996).
For example, in the unification-based approach (Johnston 1998), one temporalconstraint indicates that speech and gesture can becombined only when the speech either overlapswith gesture or follows the gesture within a certaintime frame.
This is a hard constraint that has to besatisfied in order for the unification to take place.If a given input does not satisfy these hardconstraints, the unification fails.In our user studies, we found that, although themajority of user temporal alignment behavior maysatisfy pre-defined temporal constraints, there are1 We implemented a system using real estate as an applicationdomain.
The user can interact with a map using both speechand gestures to retrieve information.
All the user studies men-tioned in this paper were conducted using this system.some exceptions.
Table 1 shows the percentage ofdifferent temporal relations collected from our userstudies.
The rows indicate whether there is anoverlap between speech referring expressions andtheir accompanied gestures.
The columns indicatewhether the speech (more precisely, the referringexpressions) or the gesture occurred first.Consistent with the previous findings (Oviatt et al1997), in most cases (85% of time), gesturesoccurred before the referring expressions wereuttered.
However, in 15% of the cases the speechreferring expressions were uttered before thegesture occurred.
Among those cases, 8% had anoverlap between the referring expressions and thege),al al(i )in fin e1 nin 6m ?ssp dw dah eusspbeteacT00.20.40.60.811 2 3 4 5 6 7UserPercentageNon-overlap Speech First Non-overlap Gesture FirstOverlap Speech First Overlap Gesture FirstFigure 1: Temporal relations between speech and gesturefor individual users100%85%15%Total48%40%8%Overlap52%45%7%Non-overlapTotalGesture FirstSpeech FirstTable 1: Overall temporal relations between speech andgesture sture and 7% had no overlap.Furthermore, as shown in (Oviatt et al, 2003though multimodal behaviors such as sequenti.e., non-overlap) or simultaneous (e.g., overlaptegration are quite consistent during the course oteraction, there are still some exceptions.
Figurshows the temporal alignments from sevedividual users in our study.
User 2 and Useraintained a consistent behavior in that User 2eech referring expressions always overlappeith gestures and User 6?s gesture always occurreead of the speech expressions.
The other fivers exhibited varied temporal alignment betweeneech and gesture during the interaction.
It willdifficult for a system using pre-definedmporal constraints to anticipate andcommodate all these different behaviors.herefore, it is desirable to have a mechanism thatallows violation of these constraints and supportsoft or graded constraints.3 A Graph-based Optimization ApproachTo address the necessities described above, wedeveloped an optimization approach forinterpreting multimodal references using graphmatching.
The graph representation captures bothsalient entities and their inter-relations.
The graphmatching is an optimization process that finds thebest matching between two graphs based onconstraints modeled as links or nodes in thesegraphs.
This type of structure and process isespecially useful for interpreting multimodalreferences.
One graph can represent all thereferring expressions and their inter-relations, andthe other graph can represent all the potentialreferents.
The question is how to match themtogether to achieve a maximum compatibilitygiven a particular context.3.1 OverviewGraph-based RepresentationAttribute Relation Graph (ARG) (Tsai and Fu, 1979)is used to represent information in our approach.An ARG consists of a set of nodes that areconnected by a set of edges.
Each node representsan entity, which in our case is either a referringexpression to be resolved or a potential referent.Each node encodes the properties of thecorresponding entity including:?
Semantic information that indicates thesemantic type, the number of potential referents,and the specific attributes related to thecorresponding entity (e.g., extracted from thereferring expressions).?
Temporal information that indicates the timewhen the corresponding entity is introduced intothe discourse (e.g., uttered or gestured).Each edge represents a set of relations betweentwo entities.
Currently we capture temporalrelations and semantic type relations.
A temporalrelation indicates the temporal order between tworelated entities during an interaction, which mayhave one of the following values:?
Precede: Node A precedes Node B if the entityrepresented by Node A is introduced into thediscourse before the entity represented by Node B.?
Concurrent: Node A is concurrent with Node B ifthe entities represented by them are referred to ormentioned simultaneously.?
Non-concurrent: Node A is non-concurrent withNode B if their corresponding objects/referencescannot be referred/mentioned simultaneously.?
Unknown: The temporal order between two entitiesis unknown.
It may take the value of any of theabove.A semantic type relation indicates whether tworelated entities share the same semantic type.
Itcurrently takes the following discrete values: Same,Different, and Unknown.
It could be beneficial in thefuture to consider a continuous function measuringthe rate of compatibility instead.Specially, two graphs are generated.
One graph,called the referring graph, captures referringexpressions from speech utterances.
For example,suppose a user says Compare this house, the greenhouse, and the brown one.
Figure 2 show a referringgraph that represents three referring expressionsfrom this speech input.
Each node captures thesemantic information such as the semantic type(i.e., Semantic Type), the attribute (Color), thenumber (Number) of the potential referents, as wellas the temporal information about when thisreferring expression is uttered (BeginTime andEndTime).
Each edge captures the semantic (e.g.,SemanticTypeRelation) and temporal relations (e.g.,TemporalRelation) between the referring expressions.In this case, since the green house is uttered beforethe brown one, there is a temporal Precederelationship between these two expressions.Furthermore, according to our heuristic thatobjects-to-be-compared should share the samesemantic type, therefore, the SemanticTypeRelationbetween two nodes is set to Same.Node 1this houseNode 2the greenhouseNode 3the brownoneSemanticType: HouseNumber.
: 1Attribute: Color = $GreenBeginTime: 32244242msEndTime: ??
?SemanticTypeRelation: SameTemporalRelation: PrecedeDirection: Node 2 -> Node 3Speech: Compare this house, the green houseand the brown oneFigure 2: An example of a referring graphSimilarly, the second graph, called the referentgraph, represents all potential referents frommultiple sources (e.g., from the last conversation,gestured by the user, etc).
Each node captures thesemantic and temporal information about apotential referent (e.g., the time when the potentialreferent is selected by a gesture).
Each edgecaptures the semantic and temporal relationsbetween two potential referents.
For instance,suppose the user points to one position and thenpoints to another position.
The correspondingreferent graph is shown in Figure 3.
The objectsinside the first dashed rectangle correspond to thepotential referents selected by the first pointinggesture and those inside the second dashedrectangle correspond to the second pointing gesture.Each node also contains a probability that indicatesthe likelihood of its corresponding object beingselected by the gesture.
Furthermore, the salientobjects from the prior conversation are alsoincluded in the referent graph since they could alsobe the potential referents (e.g., the rightmostdashed rectangle in Figure 32).To create these graphs, we apply a grammar-based natural language parser to process speechinputs and a gesture recognition component toprocess gestures.
The details are described in (Chaiet al 2004a).2 Each node from the conversation context is linked to everynode corresponding to the first pointing and the second point-ing.Graph-matching ProcessGiven these graph representations, interpretingmultimodal references becomes a graph-matchingproblem.
The goal is to find the best matchbetween a referring graph (Gs) and a referent graph(Gr).
Suppose ?
A referring graph Gs = ?
{?m}, {?mn}?, where {?m} arenodes and {?mn} are edges connecting nodes ?m and ?n.
Nodes in Gs are named referring nodes.?
A referent graph Gr = ?
{ax}, {rxy}?, where {ax} arenodes and {rxy} are edges connecting nodes ax and ay.Nodes in Gr are named referent nodes.The following equation finds a match thatachieves the maximum compatibility between Grand Gs:),(),(),(),(),(),(mnxynymxx y m nmxmxx msrrEdgeSimaPaPaNodeSimaPGGQ??????
?
?
??
?
+=  (1)In Equation (1), Q(Gr,Gs) measures the degree ofthe overall match between the referent graph andthe referring graph.
P(ax,?m) is the matchingprobability between a node ax in the referent graphand a node ?m in the referring graph.
The overallcompatibility depends on the similarities betweennodes (NodeSim) and the similarities betweenedges (EdgeSim).
The function NodeSim(ax,?m)measures the similarity between a referent node axand a referring node ?m by combining semanticconstraints and temporal constraints.
The functionEdgeSim(rxy,?mn) measures the similarity betweenrxy and ?mn, which depends on the semantic andtemporal constraints of the corresponding edges.These functions are described in detail in the nextsection.We use the graduated assignment algorithm(Gold and Rangarajan, 1996) to maximize Q(Gr,Gs)in Equation (1).
The algorithm first initializesP(ax,?m) and then iteratively updates the values ofP(ax,?m) until it converges.
When the algorithmconverges, P(ax,?m) gives the matchingprobabilities between the referent node ax and thereferring node ?m that maximizes the overallcompatibility function.
Given this probabilitymatrix, the system is able to assign the mostprobable referent(s) to each referring expression.3.2 Similarity FunctionsAs shown in Equation (1), the overallcompatibility between a referring graph and areferent graph depends on the node similarityOssiningChappaquaObject ID: MLS2365478SemanticType: HouseAttribute: Color = $BrownBeginTime: 32244292 msSelectionProb: 0.65?
?Semantic Type Relation: DiffTemporal relation: SameDirection:Gesture: Point to one position and point toanother positionFirst pointing Second pointing ConversationContextFigure 3: An example of referent graphfunction and the edge similarity function.
Next wegive a detailed account of how we defined thesefunctions.
Our focus here is not on the actualdefinitions of those functions (since they may varyfor different applications), but rather a mechanismthat leads to competition and ranking of constraints.Node Similarity FunctionGiven a referring expression (represented as ?min the referring graph) and a potential referent(represented as ax in the referent graph), the nodesimilarity function is defined based on thesemantic and temporal information captured in axand ?m through a set of individual compatibilityfunctions:NodeSim(ax,?m) = Id(ax,?m) SemType(ax,?m)?k Attrk(ax,?m) Temp(ax,?m)Currently, in our system, the specific returnvalues for these functions are empiricallydetermined through iterative regression tests.Id(ax,?m) captures the constraint of thecompatibilities between identifiers specified in axand ?m.
It indicates that the identifier of thepotential referent, as expressed in a referringexpression, should match the identifier of the truereferent.
This is particularly useful for resolvingproper nouns.
For example, if the referringexpression is house number eight, then the correctreferent should have the identifier number eight.We currently define this constraint as follows:Id(ax,?m) = 0 if the object identities of ax and ?mare different.
Id(ax,?m) = 100 if they are the same.Id(ax,?m) = 1 if at least one of the identities of axand ?m is unknown.
The different return valuesenforce that a large reward is given to the casewhere the identifiers from the referring expressionsmatch the identifiers from the potential referents.SemType(ax,?m) captures the constraint ofsemantic type compatibility between  ax and ?m.
Itindicates that the semantic type of a potentialreferent as expressed in the referring expressionshould match the semantic type of the correctreferent.
We define the following: SemType(ax,?m)= 0 if the semantic types of ax and ?m are different.SemType(ax,?m) = 1 if they are the same.SemType(ax,?m) = 0.5 if at least one of thesemantic types of ax and ?m is unknown.
Note thatthe return value given to the case where semantictypes are the same (i.e., ?1?)
is much lower thanthat given to the case where identifiers are thesame (i.e., ?100?).
This was designed to supportconstraint ranking.
Our assumption is that theconstraint on identifiers is more important than theconstraint on semantic types.
Because identifiersare usually unique, the corresponding constraint isa greater indicator of node matching if theidentifier expressed from a referring expressionmatches the identifier of a potential referent.Attrk(ax,?m) captures the domain specificconstraint concerning a particular semantic feature(indicated by the subscription k).
This constraintindicates that the expected features of a potentialreferent as expressed in a referring expressionshould be compatible with features associated withthe true referent.
For example, in the referringexpression the Victorian house, the style feature isVictorian.
Therefore, an object can only be apossible referent if the style of that object isVictorian.
Thus, we define the following: Ak(ax,?m)= 1 if both ax and ?m share the kth feature with thesame value.
Ak(ax,?m) = 0 if both ax and ?m havethe feature k and the values of the feature k are notequal.
Otherwise, when the kth feature is notpresent in either ax or ?m, then Ak (ax,?m) = 0.1.Note that these feature constraints are dependenton the specific domain model for a particularapplication.Temp(ax,?m) captures the temporal constraintbetween a referring expression ?m and a potentialreferent ax.
As discussed in Section 2, a hardconstraint concerning temporal relations betweenreferring expressions and gestures will beincapable of handling the flexibility of usertemporal alignment behavior.
Thus the temporalconstraint in our approach is a graded constraint,which is defined as follows:)2000|)()(|exp(),( mxmxBeginTimeaBeginTimeaTemp ??
?
?=This constraint indicates that the closer areferring expression and a potential referent interms of their temporal alignment (regardless ofthe absolute precedence relationship), the morecompatible they are.Edge Similarity FunctionThe edge similarity function measures thecompatibility of relations held between referringexpressions (i.e., an edge ?mn in the referring graph)and relations between the potential referents (i.e.,an edge rxy in the referent graph).
It is defined bytwo individual compatibility functions as follows:EdgeSim(rxy, ?mn) = SemType(rxy, ?mn) Temp(rxy, ?mn)SemType(rxy, ?mn) encodes the semantic typecompatibility between an edge in the referringgraph and an edge in the referent graph.
It isdefined in Table 2.
This constraint indicates thatthe relation held between referring expressionsshould be compatible with the relation heldbetween two correct referents.
For example,consider the utterance How much is this green houseand this blue house.
This utterance indicates that thereferent to the first expression this green houseshould share the same semantic type as the referentto the second expression this blue house.
As shownin Table 2, if the semantic type relations of rxy and?mn are the same, SemType(rxy, ?mn) returns 1.
Ifthey are different, SemType(rxy, ?mn) returns zero.
Ifeither rxy or ?mn is unknown, then it returns 0.5.Temp(rxy, ?mn) captures the temporalcompatibility between an edge in the referringgraph and an edge in the referent graph.
It isdefined in Table 3.
This constraint indicates thatthe temporal relationship between two referringexpressions (in one utterance) should becompatible with the relations of theircorresponding referents as they are introduced intothe context (e.g., through gesture).
The temporalrelation between referring expressions (i.e., ?mn) iseither Precede or Concurrent.
If the temporalrelations of rxy and ?mn are the same, then Temp(rxy,?mn) returns 1.
Because potential references couldcome from prior conversation, even if rxy and ?mnare not the same, the function does not return zerowhen ?mn is Precede.Next, we discuss how these definitions and theprocess of graph matching address optimization, inparticular, with respect to key principles ofOptimality Theory for natural languageinterpretation.3.3 Optimality TheoryOptimality Theory (OT) is a theory of languageand grammar, developed by Alan Prince and PaulSmolensky (Prince and Smolensky, 1993).
InOptimality Theory, a grammar consists of a set ofwell-formed constraints.
These constraints areapplied simultaneously to identify linguisticstructures.
Optimality Theory does not restrict thecontent of the constraints (Eisner 1997).
Aninnovation of Optimality Theory is the conceptionof these constraints as soft, which means violableand conflicting.
The interpretation that arises foran utterance within a certain context maximizes thedegree of constraint satisfaction and isconsequently the best alternative (hence, optimalinterpretation) among the set of possibleinterpretations.The key principles or components of OptimalityTheory can be summarized as the following threecomponents (Blutner 1998): 1) Given a set of input,Generator creates a set of possible outputs for eachinput.
2) From the set of candidate output, Evaluatorselects the optimal output for that input.
3) There isa strict dominance in term of the ranking of constraints.Constraints are absolute and the ranking of theconstraints is strict in the sense that outputs thathave at least one violation of a higher rankedconstraint outrank outputs that have arbitrarilymany violations of lower ranked constraints.Although Optimality Theory is a grammar-basedframework for natural language processing, its keyprinciples can be applied to other representations.At a surface level, our approach addresses thesemain principles.First, in our approach, the matching matrixP(ax,?m) captures the probabilities of all thepossible matches between a referring node ?m anda referent node ax.
The matching process updatesthese probabilities iteratively.
This processcorresponds to the Generator component inOptimality Theory.Second, in our approach, the satisfaction orviolation of constraints is implemented via returnvalues of compatibility functions.
These0.50.50.5Unknown0.510Different0.501Same?mnUnknown DifferentSamerxySemType(rxy, ?mn)Table 2: Definition of SemType(rxy, ?mn)0.5010Concurrent0.50.70.51Precede?mnUnknown Non-concurrentConcurrentPrecedingrxyTemp(rxy, ?mn)Table 3: Definition of Temp(rxy, ?mn)constraints can be violated during the matchingprocess.
For example, functions Id(ax,?m),SemType(ax,?m), and Attrk(ax,?m) return zero if thecorresponding intended constraints are violated.
Inthis case, the overall similarity function will returnzero.
However, because of the iterative updatingnature of the matching algorithm, the system willstill find the most optimal match as a result of thematching process even some constraints areviolated.
Furthermore, A function that neverreturns zero such as Temp(ax,?m) in the nodesimilarity function implements a gradient.,.3.4 EvaluationWe conducted several user studies to evaluatethe performance of this approach.
Users couldinteract with our system using both speech anddeictic gestures.
Each subject was asked tocomplete five tasks.
For example, one task was tofind the cheapest house in the most populated town.Data from eleven subjects was collected andanalyzed.Table 4 shows the evaluation results of 219inputs.
These inputs were categorized in terms ofthe number of referring expressions in the speechinput and the number of gestures in the gestureinputs.
Out of the total 219 inputs, 137 inputs hadtheir referents correctly interpreted.
For theremaining 82 inputs in which the referents werenot correctly identified, the problem did not comefrom the approach itself, but rather from othersources such as speech recognition and languageunderstanding errors.
These were two major errorsources, which were accounted for 55% and 20%of total errors respectively (Chai et al 2004b).In our studies, the majority of user referenceswere simple in that they involved only onereferring expression and one gesture as in earlierfindings (Kehler 2000).
It is trivial for ourapproach to handle these simple inputs since thesize of the graph is usually very small and there isonly one node in the referring graph.
However, wedid find 23% complex inputs (the row S3 and thecolumn G3 in Table 4), which involved multiplereferring expressions from speech utterancesand/or multiple gestures.
Our optimizationapproach is particularly effective to interpret thesecomplex inputs by simultaneously consideringsemantic, temporal, and contextual constraints.4 ConclusionAs in natural language interpretation addressedby Optimality Theory, the idea of optimizingconstraints is beneficial and there is evidence infavor of competition and constraint ranking inmultimodal language interpretation.
We developeda graph-based approach to address optimization formultimodal interpretation; in particular,interpreting multimodal references.
Our approachsimultaneously applies temporal, semantic, andcontextual constraints together and achieves thebest interpretation among all alternatives.
Althoughcurrently the referent graph corresponds to gesture129(111)90(26)20(15),19(2)102(91),65(22)7(5),6(2)Total Num15(9),16(1)12(8),8(0)3(1),7(1)0(0),1(0)S3: Multiple referringexpressions110(90),74(25)8(7),11(2)96(89),58(21)6(4),5(2)S2: One referringexpression4(2),0(0)03(1),0(0)1(1),0(0)S1:No referringexpressionTotalNumG3: Multi-GesturesG2: OneGestureG1: NoGestureTable 4: Evaluation Results.
In each entry form ?a(b), c(d)?,?a?
indicates the number of inputs in which the referringexpressions were correctly recognized by the speech recog-nizer; ?b?
indicates the number of inputs in which the refer-ring expressions were  correctly recognized and werecorrectly resolved; ?c?
indicates the number of inputs inwhich the referring expressions were not correctly recog-nized; ?d?
indicates the number of inputs in which the refer-ring expressions also were not correctly recognized, butwere correctly resolved.
The sum of ?a?
and ?c?
gives thetotal number of inputs with a particular combination ofspeech and gesture.
constraint in Optimality Theory.
Given thesecompatibility functions, the graph-matchingalgorithm provides an optimization process to findthe best match between two graphs.
This processcorresponds to the Evaluator component ofOptimality Theory.Third, in our approach, different compatibilityfunctions return different values to address theConstraint Ranking component in Optimality TheoryFor example, as discussed earlier, once ax and ?mshare the same identifier, Id(ax,?m) returns 100.
Ifax and ?m share the same semantic typeSemType(ax,?m) returns 1.
Here, we consider thecompatibility between identifiers is more importantthan the compatibility between semantic typesHowever, currently we have not yet addressed thestrict dominance aspect of Optimality Theory.input and conversation context, it can be easilyextended to incorporate other modalities such asgaze inputs.We have only taken an initial step to investigateoptimization for multimodal language processing.Although preliminary studies have shown theeffectiveness of the optimization approach basedon graph matching, this approach also has itslimitations.
The graph-matching problem is a NPcomplete problem and it can become intractableonce the size of the graph is increased.
However,we have not experienced the delay of systemresponses during real-time user studies.
This isbecause most user inputs were relatively concise(they contained no more than four referringexpressions).
This brevity limited the size of thegraphs and thus provided an opportunity for suchan approach to be effective.
Our future work willaddress how to extend this approach to optimizethe overall interpretation of user multimodal inputs.AcknowledgementsThis work was partially supported by grant IIS-0347548 from the National Science Foundationand grant IRGP-03-42111 from Michigan StateUniversity.
The authors would like to thank JohnHale and anonymous reviewers for their helpfulcomments and suggestions.ReferencesBolt, R.A. 1980.
Put that there: Voice and Gesture at theGraphics Interface.
Computer Graphics, 14(3): 262-270.Blutner, R., 1998.
Some Aspects of Optimality In NaturalLanguage Interpretation.
Journal of Semantics, 17, 189-216.Cassell, J., Bickmore, T., Billinghurst, M., Campbell, L.,Chang, K., Vilhjalmsson, H. and Yan, H. 1999.
Embodi-ment in Conversational Interfaces: Rea.
In Proceedings ofthe CHI'99 Conference, 520-527.Chai, J., Prasov, Z, and Hong, P. 2004b.
Performance Evalua-tion and Error Analysis for Multimodal Reference Resolu-tion in a Conversational System.
Proceedings of HLT-NAACL 2004 (Companion Volumn).Chai, J. Y., Hong, P., and Zhou, M. X.
2004a.
A ProbabilisticApproach to Reference Resolution in Multimodal User In-terfaces, Proceedings of 9th International Conference onIntelligent User Interfaces (IUI): 70-77.Chai, J., Pan, S., Zhou, M., and Houck, K. 2002.
Context-based Multimodal Interpretation in Conversational Systems.Fourth International Conference on Multimodal Interfaces.Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J.,Smith, I., Chen, L., and Clow, J.
1996.
Quickset: Multimo-dal Interaction for Distributed Applications.
Proceedings ofACM Multimedia.Eisner, Jason.
1997.
Efficient Generation in Primitive Opti-mality Theory.
Proceedings of ACL?97.Gold, S. and Rangarajan, A.
1996.
A Graduated AssignmentAlgorithm for Graph-matching.
IEEE Trans.
PatternAnalysis and Machine Intelligence, vol.
18, no.
4.Gustafson, J., Bell, L., Beskow, J., Boye J., Carlson, R., Ed-lund, J., Granstrom, B., House D., and Wiren, M.  2000.AdApt ?
a Multimodal Conversational Dialogue System inan Apartment Domain.
Proceedings of 6th InternationalConference on Spoken Language Processing (ICSLP).Johnston, M, Cohen, P., McGee, D., Oviatt, S., Pittman, J. andSmith, I.
1997.
Unification-based Multimodal Integration,Proceedings of ACL?97.Johnston, M. 1998.
Unification-based Multimodal Parsing,Proceedings of COLING-ACL?98.Johnston, M. and Bangalore, S. 2000.
Finite-state MultimodalParsing and Understanding.
Proceedings of COLING?00.Johnston, M.,  Bangalore, S.,  Visireddy G., Stent, A., Ehlen,P., Walker, M., Whittaker, S., and Maloor, P. 2002.MATCH: An Architecture for Multimodal Dialog Systems,Proceedings of ACL?02, Philadelphia, 376-383.Kehler, A.
2000.
Cognitive Status and Form of Reference inMultimodal Human-Computer Interaction, Proceedings ofAAAI?01, 685-689.Koons, D. B., Sparrell, C. J. and Thorisson, K. R. 1993.
Inte-grating Simultaneous Input from Speech, Gaze, and HandGestures.
In Intelligent Multimedia Interfaces, M. Maybury,Ed.
MIT Press: Menlo Park, CA.Neal, J. G., and Shapiro, S. C.  1991.
Intelligent MultimediaInterface Technology.
In Intelligent User Interfaces, J. Sul-livan & S. Tyler, Eds.
ACM: New York.Oviatt, S. L. 1996.
Multimodal Interfaces for Dynamic Inter-active Maps.
In Proceedings of Conference on Human Fac-tors in Computing Systems: CHI '96, 95-102.Oviatt, S., DeAngeli, A., and Kuhn, K., 1997.
Integration andSynchronization of Input Modes during Multimodal Hu-man-Computer Interaction, In Proceedings of Conferenceon Human Factors in Computing Systems: CHI '97.Oviatt, S., Coulston, R., Tomko, S., Xiao, B., Bunsford, R.Wesson, M., and Carmichael, L. 2003.
Toward a Theory ofOrganized Multimodal Integration Patterns during Human-Computer Interaction.
In Proceedings of Fifth InternationalConference on Multimodal Interfaces, 44-51.Prince, A. and Smolensky, P. 1993.
Optimality Theory.
Con-straint Interaction in Generative Grammar.
ROA 537.http://roa.rutgers.edu/view.php3?id=845.Stent, A., J. Dowding, J. M. Gawron, E. O. Bratt, and R.Moore.
1999.
The Commandtalk Spoken Dialog System.Proceedings of ACL?99,  183?190.Tsai, W.H.
and Fu, K.S.
1979.
Error-correcting Isomorphismof Attributed Relational Graphs for Pattern Analysis.
IEEETransactions on Systems, Man and Cybernetics., vol.
9.Wahlster, W., 1998.
User and Discourse Models for Multimo-dal Communication.
Intelligent User Interfaces, M.Maybury and W. Wahlster (eds.
),  359-370.Wu, L., Oviatt, S., and Cohen, P. 1999.
Multimodal Integra-tion ?
A Statistical View, IEEE Transactions on Multime-dia, Vol.
1, No.
4, 334-341.
