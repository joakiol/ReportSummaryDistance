Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1579?1588,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsPrototype Synthesis for Model LawsMatthew BurgessUniversity of Michiganmattburg@umich.eduEugenia GiraudyUC Berkeley & YouGoveugenia.giraudy@gmail.comEytan AdarUniversity of Michiganeadar@umich.eduAbstractState legislatures often rely on existingtext when drafting new bills.
Resource andexpertise constraints, which often drivethis copying behavior, can be taken ad-vantage of by lobbyists and special inter-est groups.
These groups provide modelbills, which encode policy agendas, withthe intent that the models become actuallaw.
Unfortunately, model legislation isoften opaque to the public?both in sourceand content.
In this paper we presentLOBBYBACK, a system that reverse en-gineers model legislation from observedtext.
LOBBYBACK identifies clusters ofbills which have text reuse and gener-ates ?prototypes?
that represent a canon-ical version of the text shared between thedocuments.
We demonstrate that LOBBY-BACK accurately reconstructs model leg-islation and apply it to a dataset of over550k bills.1 IntroductionBeginning in 2005, a number of states beganpassing ?Stand Your Ground?
laws?legal protec-tions for the use of deadly force in self-defense.Within a few years, at least two dozen states im-plemented a version of the this legislation (Gar-rett and Jansa, 2015).
Though each state passedits own variant, there is striking similarity in thetext of the legislation.
While seemingly ?viral?the expedient adoption of these laws was not theresult of an organic diffusion process, but rathermore centralized efforts.
An interest group, theAmerican Legislative Exchange Council (ALEC),drafted model legislation (in this case modeled onFlorida?s law) and lobbied to have the model lawenacted in other states.
While the influence of thelobbyists through model laws grows, the hiddennature of their original text (and source) creates atroubling opacity.Reconstructing such hidden text through analy-sis of observed, potentially highly mutated, copiesposes an interesting and challenging NLP prob-lem.
We refer to this as the Dark Corpora prob-lem.
Since legislatures are not required to cite thesource of the text that goes into a drafted bill, thebills that share text are unknown beforehand.
Thefirst problem therein lies in identifying clusters ofbills with reused text.
Once a cluster is identi-fied, a second challenge is the reconstruction ofthe original or prototype bill that corresponds tothe observed text.
The usual circumstances underwhich a model law is adopted by individual statesinvolves ?mutation.?
This may be as simple asmodifying parameters to the existing policy (e.g.,changing the legal limit allowed of medical mar-ijuana possession to 3.0 ounces from 2.5) or canbe more substantial, with significant additions ordeletions of different conditions of a policy.
Inter-estingly, the need to maintain the objectives of thelaw creates a pressure to retain a legally meaning-ful structure and precise language?thus changesneed to satisfy the existing laws of the state butcarry out the intent of the model.
Both subtlechanges of this type, and more dramatic ones, areof great interest to political scientists.
A specificapplication, for example, may be predicting likelyspots for future modifications as additional statesadopt the law.
Our challenge is to identify and rep-resent ?prototype?
sentences that capture the sim-ilarity of observed sentences while also capturingthe variation.In this paper we propose LOBBYBACK, a sys-tem that automatically identifies clusters of docu-ments that exhibit text reuse, and generates ?pro-totypes?
that represent a canonical version of textshared between the documents.
In order to syn-1579thesize the prototypes, LOBBYBACK first extractsclusters of sentences, where each sentence per-tains to the same policy but can exhibit variation.LOBBYBACK then uses a greedy multi-sequencealignment algorithm to identify an approximationof the optimal alignment between the sentences.Prototype sentences are synthesized by comput-ing a consensus sentence from the multi-sentencealignment.
As sentence variants are critical in un-derstanding the effect of the model legislation, wecan not simply generate a single ?common?
sum-mary sentence as a prototype.
Rather, LOBBY-BACK creates a data structure that captures thisvariability in text for display to the end-user.With LOBBYBACK, end-users can quicklyidentify clusters of text reuse to better understandwhat type of policies are diffused across states.In other applications, sentence prototypes can beused by journalists and researchers to discoverpreviously unknown model legislation and the in-volvement of lobbying organizations.
For exam-ple, prototype text can be compared to the lan-guage or policy content of interest groups docu-ments and accompanied with qualitative researchit can help discover which lobbyists have draftedthis legislation.We evaluated LOBBYBACK on the task of re-constructing 122 known model legislation docu-ments.
Our system was able to achieve an aver-age of 0.6 F1 score based on the number of pro-totype sentences that had high similarity with sen-tences from the model legislation.
We have alsorun LOBBYBACK on the entire corpus of state leg-islation (571,000 documents) from openstates.orgas an open task.
The system identified 4,446 clus-ters for which we generated prototype documents.We have released the resulting data set and codeat http://github.com/mattburg/LobbyBack.
LOB-BYBACK is novel in fully automating and scal-ing the pipeline of model-legislation reconstruc-tion.
The output of this pipeline captures both thelikely ?source sentences?
but also the variations ofthose sentences.2 Related WorkWhile no specific system or technique has fo-cused on the problem of legislative document re-construction, we find related work in a number ofdomains.
Multi-document summarization (MDS),for example, can be used to partially model the un-derlying problem?generating a representative doc-ument from multiple sources.
Extractive MDS, inparticular, is promising in that representative sen-tences are identified.Early work in extractive summarization includegreedy approaches such as that proposed by Car-bonell and Goldstein (1998).
The algorithm usesan objective function which trades off between rel-evance and redundancy.
Global optimization tech-niques attempt to generate ?summaries?
(selectedsets of sentences or utterances) that maximize anobjective based on informativeness, redundancyand/or length of summary.
These have shown su-perior performance to greedy algorithms (Yih etal., 2007; Gillick et al, 2009).
Approaches basedon neural networks have recently been proposedfor ranking candidate sentences (Cao et al, 2015).Graph based methods, such as LexRank (Erkanand Radev, 2004), have also proven effective forMDS.
Extensions to this approach combine sen-tence ranking with clustering in order to minimizeredundancy (Qazvinian and Radev, 2008; Wan andYang, 2008; Cai and Li, 2013).
The C-LexRankalgorithm (Qazvinian and Radev, 2008), in par-ticular, uses this combination and inspired ourhigh level design.
Similar to our approach, Wangand Cardie (2013) propose a method for generat-ing templates of meeting summaries using multi-sequence alignment.Though related, it is important to note that theobjectives of summarization (informativeness, re-duced redundancy, etc.)
are not entirely consistentwith our task.
For example, using the n-gram co-occurrence based ROUGE score would not be suf-ficient at evaluating LOBBYBACK.
Our goal is toaccurately reconstruct entire sentences of a hiddendocument given observed mutations of that docu-ment.
Additionally, our goal is not simply to finda representative sentence that reflects the originaldocument, but to capture the similarity and vari-ability of the text within a given ?sentence cluster.
?Within the political science and legal studiescommunities research has focused on manual ap-proaches to both understanding how model legis-lation impacts law and how policy ideas diffusebetween bill text.
As these studies are time con-suming, there is no large-scale or broad analysis oflegislative materials.
Rather, researchers have lim-ited their workload by focusing on a single topic(e.g., abortion (Patton, 2003) and crime (Kent andCarmichael, 2015)) or a single lobbying group(e.g., ALEC (Jackman, 2013).
Similarly, those1580studying policy diffusion across US states havealso limited their analysis to a few topics (e.g.,same-sex marriage (Haider-Markel, 2001)).Recent attempts to automate the analysis ofmodel legislation has had similar problems, asmost researchers have limited their analysis to oneinterest group or a few relevant topics (Hertel-Fernandez and Kashin, 2015; Jansa et al, 2015).Hertel-Fernandez and Kashin proposed a super-vised model in which they train on hand labeledexamples of state bills that borrow text and/or con-cepts from ALEC bills.
The problem they focuson is different from ours.
The motivation behindLOBBYBACK is that there exists many model leg-islation which we don?t have access to and the goalis to try and reconstruct these documents with-out labeled training data.
Jansa et al propose atechnique for inferring a network of policy diffu-sion for manually labeled clusters of bills.
BothJansa et al and Hertel-Fernandaz and Kashin pro-pose techniques that only look at the problem ofinferring whether two bills exhibit text reuse butunlike LOBBYBACK they do not attempt to inferwhether specific policies (sentences) in the docu-ments are similar/different.A related ?dark corpora?
problem, though at afar smaller scale, is the biblical ?Q source?
wherehidden oral sources are reconstructed through tex-tual analysis (Mournet, 2005).3 Problem DefinitionPolicy diffusion is a common phenomenon instate bills (Gray, 1973; Shipan and Volden, 2008;Berry and Berry, 1990; Haider-Markel, 2001).Unlike members of the (Federal) Congress, fewstate legislators have the expertise, time, and staffto draft legislation.
It is far easier for a legisla-tor to adapt existing legislative text than to writea bill from scratch.
As a consequence, state leg-islatures have an increased willingness to adoptlegislation drafted by interest groups or by legis-lators in other states (Jansa et al, 2015).
In addi-tion to states borrowing text from other legislatorsand lobbyists, another reason why bills can exhibittext reuse is when a new federal law passes andeach state needs to modify its existing policy toconform with the new federal law.The result of legislative copying, whethercaused by diffusion between states, influence froma lobby or the passing of a new federal law, is sim-ilar: a cluster of bills will share very similar text,5ed4a descriptiondraft or preliminary55ed4aaaaadescriptiondescription-draftdraftoror-oror---preliminarypreliminarySec.Sec.Sec.Sec.Sec.Sec.- --Figure 1: Visualization of a multi-sentence align-ment (fragment) and resulting prototype sentence.often varying only by implementation details of agiven policy.
The goal in constructing a prototypedocument?a representation of the ?original?
text?is to synthesize this document from the modifiedcopies.
In the case when the bill cluster was in-fluenced by one external source, such as lobby orpassage of a federal bill, the ideal prototype doc-ument would capture the language that each billborrowed from the source document.
In the casewhen their is not one single document that influ-enced a cluster of bills, the prototype will still givea summary of a concise description of the diffusedtext between bills, providing fast insight into whattext was shared and changed within a bill cluster.3.1 State Legislation CorpusWe obtained the entire openstates.org corpus ofstate legislation, which includes 550,000 bills and200,000 resolutions for all 50 states.
While forsome states this corpus includes data since 2007,for the majority of states we have data from 2010on.
We do not include data from Puerto Rico,where the text is in Spanish, and from Washing-ton DC, which includes many idiosyncrasies (e.g.,correspondence from city commissions introducedas bills).
On average, each state introduced 10,524bills, with an average length of 1205 words.4 LOBBYBACK ArchitectureLOBBYBACK consists of 3 major components.The first component identifies clusters of bills thathave text reuse.
Then for each of these bill clus-ters, LOBBYBACK extracts and clusters the sen-tences from all documents.
For each of the sen-tence clusters, LOBBYBACK synthesizes proto-type sentences in order to capture the similarityand variability of the sentences in the cluster.15814.1 Clustering BillsGroups of bills with significant text reuse repre-sent candidates for analysis as they have may haveall copied from the same model legislation.
Thereare a number of ways one could identify such clus-ters through text mining.
In our implementation,we have opted to generate a network representa-tion of the bills and then use a network cluster-ing (i.e., ?community-finding?)
algorithm to gen-erate the bill clusters.
In our network representa-tion each node represents a state bill and weightededges represent the degree to which two bills ex-hibit substantive text reuse.
Since most pairs ofbills do not have text reuse, we chose to use anetwork model because community finding algo-rithms work well on sparse data and do not requireany parameter choice for the number of clusters.In the context of this paper, text reuse occurs whentwo state bills share:1.
Long passages of text, e.g.
(sections of bills)that can differ in details.2.
These passages contain text of substantivenature to the topic of the bill (i.e., text thatis not boilerplate).In addition to text that describes policy, statebills also contain boilerplate text that is commonto all bills from a particular state or to a particu-lar topic.
Examples of legislative boilerplate in-clude: ?Read first time 01/29/16.
Referred toCommittee on Higher Education?
(meta-data de-scribing where the bill is in the legislative pro-cess); and ?Safety clause.
The general assemblyhereby finds, determines, and declares .
.
.
?
(astandard clause included in nearly all legislationfrom Colorado, stating the eligibility of a bill tobe petitioned with a referendum).In order to identify pairs of bills that exhibit textreuse, we created an inverted index that containedn-grams ranging from size 4-8.
We use Elastic-Search to implement the inverted index and com-puted the similarity between bills using the ?Morelike This?
(MLT) query (Elastic, 2016).
The MLTquery first selects the 100 highest scoring TF*IDFn-grams from a given document and uses thoseto form a search query.
The MLT query is ableto quickly compute the similarity between docu-ments and since it ranks the query terms by usingTF*IDF the query text is more likely to be sub-stantive rather then boilerplate.
The MLT querywe used was configured to only return documentsthat matched at least 50% of the query?s shinglesand returned at most 100 documents per query.By implementing the similarity search using aTF*IDF cutoff we were able to scale the similar-ity computation while still maintaining our desireto identify reuse of substantive text.The edges of the bill similarity network arecomputed by calculating pairwise similarity.
Eachbill is submitted as input for an MLT query andscored matches are returned by the search engine.Since the MLT query extracts n-grams only forthe query document, the similarity function be-tween two documents diand djis not symmet-ric.
We construct a symmetric bill similarity net-work by taking the average score of each (di, dj)and its reciprocal (dj, di).
A non-existent edgeis represented as an edge with score 0.
We fur-ther reduce the occurrence of false-positive edgesby removing all edges with a score lower than0.1.
The resulting network is very sparse, consist-ing of 35,346 bills that have 1 or more neighbors,125,401 edges, and 3534 connected componentsthat contain an average of 10 bills.A specific connected component may containmore than one bill cluster.
To isolate these clustersin the bill network we use the InfoMap commu-nity detection algorithm (Rosvall and Bergstrom,2008).
We use the InfoMap algorithm because ithas been to shown to be one of the best commu-nity detection algorithms and it is able to detectclusters at a finer granularity than other methods.Our corpus contains both bills that have passedand those that have not.
Bills can often be re-introduced in their entirety after failing the previ-ous year.
As we do not want to bias the clusterstowards bills that are re-introduced more than oth-ers, we filter the clusters such that they only in-clude the earliest bill from each state.4.2 Prototype SynthesisOnce we have identified a cluster of bills thathave likely emerged from a single ?source?
wewould like to construct a plausible representationof that source.
The prototype synthesizer achievesthis by constructing a canonical document thatcaptures the similarity and variability of the con-tent in a given bill cluster.
The two main steps inprototype synthesis consists of clustering bill sen-tences and generating prototype sentences fromthe clusters.1582Figure 2: An sample state bill segment fromMichigan Senate Bill 5714.2.1 Sentence ClusteringMost state bills have a common structure, con-sisting of an introduction that describes the intentof the bill followed by sections that contain thelaw to be implemented.
Each section of a bill iscomprised of self-contained policy, usually con-sisting of a long sentence that describes the pol-icy and the implementation details of that policy.Each document is segmented into these policy sen-tences using the standard Python NLTK sentenceextractor.
Sentences are cleaned by removing spu-rious white space characters, surrounding punctu-ation and lower-casing each word.
Once we haveextracted all of the sentences for a given bill clus-ter, we compute the cosine similarity between allpairs of sentences which are represented using aunigram bag-of-words model.
We used a simpleunweighted bag of words model because in le-gal text stop words can be import1In this case,we are generating a similarity ?matrix?
capturingsentence?sentence similarity.Given the similarity matrix, our next goal isto isolate clusters of variant sentences that likelycame from the same source sentence.
We electedto use the DBSCAN (Ester et al, 1996) algorithmto generate these clusters.
The DBSCAN algo-rithm provides us with tunable parameters that canisolate better clusters.
Specifically, the parameter controls the maximum distance between any twopoints in the same neighborhood.
By varying we are able to control both the number of clus-ters and the amount of sentence variation within acluster.
A second reason for selecting DBSCAN isthat the algorithm automatically deals with noisydata points, placing all points that are not closeenough to other points in a separate cluster labeled1The difference between the words ?shall?
and ?may?
forinstance is important, the former requires that a specific ac-tion be put on a states budget while the later does not?noise.?
Since many sentences in a given bill clus-ter do not contribute to the reused text betweenbills, the noise cluster is useful for grouping thosesentences together rather than having them be out-siders in ?good?
clusters.4.2.2 Multi-Sequence AlignmentOnce we have sentence clusters we then syn-thesize a ?prototype?
sentence from all of the sen-tences in a given cluster.
An ideal prototype ?sen-tence?
is one that simultaneously captures the sim-ilarity between each sentence in the cluster (thecommon sentence structures) and the variation be-tween the sentences in a cluster.
For a simple pairof (partial) sentences, ?The Department of MotorVehicles retains the right to .
.
.
?
and ?The Depart-ment of Transportation retains the right to .
.
.
?, aprototype might be of the form, ?The Departmentof { Motor Vehicles, Transportation } retains therights to .
.
.
?
Our ?sentence?
is not strictly a sin-gle linear piece of text.
Rather, we have a datastructure that describes alternative sub-strings andcaptures variant text.To generate this structure we propose an algo-rithm that computes an approximation of the opti-mal multi-sentence alignment (MSA) in the clus-ter and then generates a prototype sentence repre-senting a ?consensus?
for sentences in the MSA.We generate an MSA using a modified versionof the iterative pairwise alignment algorithm de-scribed in (Gusfield, 1997).
The greedy algorithmbuilds a multi-alignment by iteratively applyingthe Needleman-Wunsch pairwise global alignmentalgorithm.
Needleman-Wunsch computes the op-timal pairwise alignment by maximizing the align-ment score between two sentences.
An align-ment score is calculated based on three param-eters: word matches, word mismatches (when aword appears in one sentence but not the other),and gaps (when the algorithm inserts a space inone of the sentences).An MSA is generated by the following steps:1.
Construct an edit-distance matrix for all pairsof sentences2.
Construct an initial alignment between thetwo sentences with the smallest edit distance3.
Repeat this step k times:(a) Select the sentence with the smallest av-erage edit distance to the current MSA.
(b) Add the chosen sentence to the MSA byaligning it to the existing MSA.1583The algorithm stops after the alignment hasreached a size that is determined as a free param-eter k (user configured, but can be chosen to bethe number of sentences in the cluster).
As the al-gorithm execution is ordered on the edit distancebetween the current MSA and the next sentence tobe added, the larger the MSA, the more variationwe are allowing in the prototype sentence.4.2.3 Synthesizing Prototype SentencesWe synthesize a prototype sentence by findinga consensus sentence from all of the aligned sen-tences in the MSA for a given cluster.
We achievethis by going through each ?column?
of the MSAand using the following rules to decide which to-ken will be used in the prototype.
A token can beeither a word in one of the sentences or a ?space?that was inserted during the alignment process.1.
If there is a token that occurs in the major-ity of alignments (> 50%) then that token ischosen.2.
If no token appears in a majority, then a spe-cial variable token is constructed that dis-plays all of the possible tokens in each sen-tence.
For example the 1stand 3rdcolumnsin Figure 1.3.
If a space is the majority token chosen then itis shown as a variable token with the secondmost common token.5 EvaluationWe provide experiments that evaluate all threecomponents of LOBBYBACK.5.1 Model Legislation CorpusTo test the effectiveness of LOBBYBACK inrecreating a model bill, we first identified a setof known model bills.
Our model legislation cor-pus consists of 1846 model bills that we found bysearching on Google using the keywords ?modellaw?, ?model policy?
and ?model legislation.
?Most of the corpus is comprised of bills fromthe conservative lobby group, American Legisla-tive Exchange Council (ALEC, 708 documents),its liberal counterpart, the State Innovative Ex-change (SIX, 269 documents) and the non-partisanCouncil of State Governments (CSG, 470 docu-ments), and the remainder (399) from smaller in-terest groups that focus on specific issues.Using the clusters we previously described(Section 4.1), we found the most similar cluster toeach model bill.
This was done by first computingthe set of neighbors (ego-network) for a model billusing the same procedure used in creating the billsimilarity network.
We then matched a bill clusterto the model legislation by finding the bill clus-ter that had the highest Jaccard similarity (basedon the overlapping bills in each cluster) with theneighbor set of a model bill.
Each test example inour evaluation data set consists of model bill andits corresponding bill cluster.
The total number ofmodel legislation documents that had matches inthe state bill corpus was 360 documents.Once we have an evaluation data set comprisedof model bill/cluster pairs our goal is to comparethe prototype sentences we infer for a cluster tothe model bill that matches that cluster.
Since wedo not have ground truth on which sentences fromthe model bill match sentences in the documentsthat comprise the cluster we need to infer such la-bels.
In order to identify which sentences from themodel legislation actually get re-used in the billcluster, we take the following steps:1.
Extract all sentences from each of the bills ina cluster and the sentences in the correspond-ing model legislation.2.
Compute the pairwise cosine similarity be-tween bill sentences and each of the modelbill sentences using the same unigram bag-of-words model described in Section 4.2.13.
Compute the ?oracle?
matchingM?using theMunkres algorithm (Munkres, 1957)The Munkres algorithm gives the best possibleone-to-one matching between the sentences in themodel legislation and the sentences in the bill clus-ters.
There are some sentences in the model billthat are never used in actual state legislation (e.g.,sentences that describe the intent of a law or in-structions of how to implement a model policy).Therefore we label model bill sentences in M?that match a bill sentence with a score greater than0.85 as true matches (S?)2.
The final set of 122evaluation examples consists of all model legis-lation/bill cluster pairs where more than 50% ofmodel bill sentences have true matches.2A threshold of 0.85 was found effective in priorwork (Garrett and Jansa, 2015) and we observed good sep-aration between matching sentences and non-matches in ourdata-set.15840.0 0.2 0.4 0.6 0.8 1.0Threshold0.00.20.40.60.81.0RecallInfoMap Louvain0.0 0.2 0.4 0.6 0.8 1.0Threshold0.00.20.40.60.81.0Precision0.0 0.2 0.4 0.6 0.8 1.0Threshold0.00.20.40.60.81.0F1Figure 3: Precision, Recall, and F1 scores for the bill clustering component of LOBBYBACK configuredwith both the Louvain and InfoMap clustering algorithms.5.2 BaselinesWhile no specific baseline exists for our prob-lem, we implemented two alternatives to testagainst.
The first, Random-Baseline, was imple-mented simply to show the performance of ran-domly constructing prototype documents.
Thesecond, LexRank-Baseline, implements a popularextractive summarization method.Random-Baseline ?
The random-baseline randomlysamples sentences from a given bill cluster.
Thenumber of sentences it samples is equal to thenumber of sentences in the optimal matching |M?|LexRank-Baseline ?
The LexRank baseline usesthe exact same clustering algorithm as LOBBY-BACK except instead of synthesizing prototypesentences, it uses the LexRank algorithm (Erkanand Radev, 2004) to pick the most salient sentencefrom each of the sentence clusters.5.3 Evaluating Bill ClustersAs described in Section 5.1, each model billis associated with a set of bills that comprise itsneighbors in the bill network.
In order to evaluatehow LOBBYBACK clusters bills we compare theinferred clusters to the corresponding neighbor setfor each model bill.The inferred cluster for a given model bill canexhibit false positive and false negative errors.False negatives, in which a bill that exists in theneighbor set of a model bill but is not containedin the inferred cluster, are easy to identify (allow-ing us to calculate recall).
Precision, on the otherhand, is more difficult as any ?extra?
bills in theinferred cluster are not necessarily incorrect.
It ispossible that there are bills which do exhibit textre-use with the model legislation but did not matchvia the ElasticSearch query used to construct thenetwork.
We have found that in practice the sec-ond component of LOBBYBACK, which clustersthe sentences extracted from a bill cluster, is robustto false-positives due to DBSCAN?s treatment of?noisy?
data points.
Because of this, we are moreconcerned with recall in the bill clustering moduleas any data lost in this step propagates through therest of the system.Figure 3 shows the precision, recall, and F1scores for LOBBYBACK coupled with both theLouvain (Blondel et al, 2008) and InfoMap (Ros-vall and Bergstrom, 2008) network clustering al-gorithms.
Because we do not know with abso-lute certainty which state bills are derived frommodel legislation, we would like to test our ap-proach at different levels of confidence.
To dothis we vary the threshold on the similarity scores(edge weights) of the ego-network determined foreach model legislation.
A normalized weight iscomputed by taking the score provided by Elas-ticSearch and dividing that edge weight weightby the maximum edge weight in the neighbor set(to control for the unbounded scores provided byElasticSearch).
We vary the threshold for thisweight (ranging from 0 to 1) and calculate the pre-cision, recall, and F1 on a neighborhood set thatis comprised of all bills that have a weight greaterthan the threshold.
Higher threshold values meansfewer state bills are included in the set, but theyare increasingly similar to the model bill.As shown in Figure 3, recall stays high for themajority of threshold values.
This indicates thatboth clustering algorithms do a good job at re-covering the bills that are the most similar to themodel legislation.
However, the two clustering al-gorithms differ somewhat in precision.
Louvain,in particular performs worse, as it suffer from ?res-olution?
problems.
While effective for networkswith large clusters, Louvain can not isolate smallgroups, of which we have many.
For this reason,1585Figure 4: Precision, Recall, and F1 scores of LOBBYBACK and baselines.we chose InfoMap as the method to use for the fi-nal implementation of LOBBYBACK.5.4 Evaluating Sentence ClustersWe first evaluate the quality of the sentenceclusters using the optimal matching M?describedabove.
For each test example in the evaluation setwe generate a prototype document using LOBBY-BACK and each of the baselines described above.We then compute a matching M between the pro-totype sentences and the model bill sentences (us-ing the same procedure described in 5.1), where Sis the set of sentences in the prototype and S0.85is the set of sentences that match with a scoregreater than 0.85.
We compute precision, P , asP = |S0.85|/|S|, and recall, R, as R = |S|/|S?|.Figure 4 shows precision, recall and F1 scoresfor both baselines and LOBBYBACK.
Each curveis generated by averaging the precision/recall/F1scores computed for each of the examples in thetest set.
The x-axis represents the minimum billcluster size of the test examples for which thescore is computed.
For example, a minimum clus-ter size would average over all test examples withat least 2 bills in a cluster.
LOBBYBACK re-lies on the fact that if text was borrowed from amodel bill, then it would have been borrowed bymany of the bills in the cluster.
By analyzing howLOBBYBACK performs with respect to the min-imum cluster size, we can determine how muchevidence LOBBYBACK needs in order to constructclusters that correspond to sentences in the modelbills.
While the performance of LOBBYBACK andthe LexRank baseline substantially improves overthe random baseline, the different between LOB-BYBACK and LexRank for this task is negligi-ble.
Since our cut-off similarity is 0.85, all sen-tences above the threshold are treated as true posi-tives, making the distinction between the LexRankbaseline and system small.
LOBBYBACK per-forms a little worse than LexRank for large clustersizes because it is penalized for having space andvariable tokens which don?t occur in model bills.Space and variable tokens occur more frequentlyin prototype sentences in larger clusters becausethere is more variation in the sentence clusters.5.5 Evaluating Sentence SynthesisThe experiment in the previous section evalu-ated the quality of the sentence clusters by treat-ing all matching sentences with a similarity greaterthan 0.85 as true positives.
Here we providean evaluation of the synthesized sentences thatLOBBYBACK generates and compare them to theLexRank baseline, which chooses the most salientsentence from each cluster.
We evaluate the qual-ity of the synthesized prototype sentences by com-puting the word-based edit-distance between theprototype sentence with its corresponding modelbill sentence in S for each test example.Since the prototypes contain variable and spacetokens which do not occur in the model bill sen-tences we modify the standard edit distance algo-rithm by not penalizing space tokens and allow-ing for any of the tokens that comprise a variableto be positive matches.
In addition, we removepunctuation and lowercase all words in all of thesentences, regardless of method.
We generate theresults in Table 1 by averaging the edit distancefor a configuration of LOBBYBACK or LexRankover sentence clusters produced for each test ex-ample.
LOBBYBACK was configured to run withthe number of iterations set to the size of the sen-tence cluster.We compared both the performance of LOBBY-BACK and LexRank for DBSCAN  values of 0.1and 0.15 as well as computing the average edit dis-tance for different minimum sizes of cluster val-ues.
As the table shows, LOBBYBACK obtains alower edit distance than LexRank in every config-1586uration and as the size of the clusters increase thegap between the two increases.
The goal of LOB-BYBACK is not to be a better summarization algo-rithm than LexRank.
By comparing to LexRankand showing that the edit distances are smaller onaverage, we can conclude that the prototype sen-tences created by LOBBYBACK are capturing thetext that is ?central?
or similar within a given clus-ter.
In addition, the prototype sentences producedby LOBBYBACK are superior because they alsocapture and describe in a succinct way, the vari-ability of the sentences within a cluster.6 DiscussionOne assumption that we made about the natureof state adoption of model legislation is that thelegislatures make modifications that largely pre-serve the model language in an effort to preservepolicy.
However, we currently do not considercases in which a legislature has intentionally ob-scured the text while still retaining the same mean-ing.
While not as frequent as common text reuse,Hertel-Fernandez and Kashin (2015) observed thatsome legislatures almost completely changed thetext while reusing the concepts.
One area of futurework would be to try and extend LOBBYBACK tobe more robust to these cases.
One strategy wouldbe to allow for a more flexible representation oftext, such as word vector embeddings.
The em-beddings might even be used to extend the multi-sentence alignment to include a penalty based onword distance in embedding space.LOBBYBACK performs well on reconstructingmodel legislation from automatically generatedbill clusters.
However, there are a number of im-provements that can refine part of the pipeline.
Apotential change, but one that is more computa-tionally costly, would be to use a deeper parsing ofthe sentences that we extract from the documents.We used a simple unigram model when comput-ing sentence similarities because we wanted to en-sure that stop words were included?due to theirimportance in legal text.
We suspect that by usinga parser we could, for example, weight the sim-ilarity of noun-phrases, yielding a better similar-ity matrix and potentially higher precision/recall.Currently LOBBYBACK does not consider the or-der of the sentences when attempting to constructa prototype document.
We envision a future ver-sion of LOBBYBACK that tries to reconstruct theoriginal ordering of the prototype sentences.Min Cluster SizeMethod  2 4 6 8LOBBYBACK 0.1 24.4 20.4 18.2 17.2LexRank 0.1 25.4 22.5 20.3 19.4LOBBYBACK 0.15 25.5 21.6 19.4 17.9LexRank 0.15 27.3 25.6 25.0 24.1Table 1: Mean edit distance scores for LOBBY-BACK and LexRank.7 ConclusionIn this paper we present LOBBYBACK, a systemto reconstruct the ?dark corpora?
that is comprisedof model bills which are copied (and modified)by resource constrained state legislatures.
LOB-BYBACK first identifies clusters of text reuse in alarge corpora of state legislation and then gener-ates prototype sentences that summarizes the sim-ilarity and variation of the copied text in a billcluster.
We believe that by open-sourcing LOB-BYBACK and releasing our data of prototype billsto the public, journalists and legal scholars can useour findings to better understand the origination ofU.S state laws.AcknowledgementsThe authors would like to thank Mike Cafarella,Joe Walsh and Rayid Ghani for helpful discus-sions, the Sunlight Foundation for providing ac-cess to data and the anonymous reviewers for theirhelpful comments.ReferencesFrances Stokes Berry and William D. Berry.
1990.State lottery adoptions as policy innovations: Anevent history analysis.
American Political ScienceReview, 84:395?415, 6.Vincent D Blondel, Jean-Loup Guillaume, RenaudLambiotte, and Etienne Lefebvre.
2008.
Fast un-folding of communities in large networks.
Journalof Statistical Mechanics: Theory and Experiment,2008(10):P10008.Xiaoyan Cai and Wenjie Li.
2013.
Rankingthrough clustering: An integrated approach tomulti-document summarization.
IEEE Transac-tions on Audio, Speech, and Language Processing,21(7):1424?1433.Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, and MingZhou.
2015.
Ranking with recursive neural net-works and its application to multi-document sum-marization.
In AAAI Conference on Artificial Intel-ligence, AAAI?15, pages 2153?2159.
AAAI Press.1587Jaime Carbonell and Jade Goldstein.
1998.
The use ofMMR, diversity-based reranking for reordering doc-uments and producing summaries.
In ACM SIGIRConference on Research and Development in Infor-mation Retrieval, SIGIR ?98, pages 335?336.Elastic.
2016.
Elasticsearch reference.https://www.elastic.co/guide/en.G?unes Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based lexical centrality as salience in textsummarization.
Journal of Artificial IntelligenceResearch, 22(1):457?479, December.Martin Ester, Hans-Peter Kriegel, Jrg Sander, and Xi-aowei Xu.
1996.
A density-based algorithm fordiscovering clusters in large spatial databases withnoise.
In Knowledge Discovery and Data Mining,KDD?96, pages 226?231.
AAAI Press.Kristin N. Garrett and Joshua M. Jansa.
2015.
Interestgroup influence in policy diffusion networks.
StatePolitics & Policy Quarterly, 15(3):387?417.Dan Gillick, Korbinian Riedhammer, Benoit Favre, andDilek Hakkani-Tur.
2009.
A global optimizationframework for meeting summarization.
In IEEEInternational Conference on Acoustics, Speech andSignal Processing, ICASSP ?09, pages 4769?4772,Washington, DC, USA.
IEEE Computer Society.Virginia Gray.
1973.
Innovation in the states: A dif-fusion study.
American Political Science Review,67(04):1174?1185.Dan Gusfield.
1997.
Algorithms on Strings, Trees, andSequences: Computer Science and ComputationalBiology.
Cambridge University Press, New York,NY, USA.Donald P Haider-Markel.
2001.
Policy diffusion asa geographical expansion of the scope of politicalconflict: Same-sex marriage bans in the 1990s.
StatePolitics & Policy Quarterly, 1(1):5?26.Alexander Hertel-Fernandez and Konstantin Kashin.2015.
Capturing business power across the stateswith text reuse.
In Annual Conference of the Mid-west Political Science Association, pages 16?19.Molly Jackman.
2013.
Alecs influence over lawmak-ing in state legislatures.
Brookings Institute Blog,December 6.Joshua M Jansa, Eric R Hansen, and Virginia H Gray.2015.
Copy and paste lawmaking: The diffusion ofpolicy language across american state legislatures.Working Paper.Stephanie L Kent and Jason T Carmichael.
2015.
Leg-islative responses to wrongful conviction: Do parti-san principals and advocacy efforts influence state-level criminal justice policy?
Social science re-search, 52:147?160.Terence C Mournet.
2005.
Oral tradition and literarydependency: Variability and stability in the synoptictradition and Q, volume 195.
Mohr Siebeck.J.
Munkres.
1957.
Algorithms for the assignmentand transportation problems.
Journal of the Societyof Industrial and Applied Mathematics, 5(1):32?38,March.Dana Jill Patton.
2003.
The Effect of U.S. SupremeCourt Intervention on the Innovation and Diffu-sion of Post-Roe Abortion Policies in the AmericanStates, 1973-2000.
Ph.D. thesis, University of Ken-tucky.Vahed Qazvinian and Dragomir R. Radev.
2008.
Sci-entific paper summarization using citation summarynetworks.
In International Conference on Compu-tational Linguistics, COLING ?08, pages 689?696,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Martin Rosvall and Carl T. Bergstrom.
2008.
Maps ofrandom walks on complex networks reveal commu-nity structure.
Proceedings of the National Academyof Sciences, 105(4):1118?1123.Charles R Shipan and Craig Volden.
2008.
The mech-anisms of policy diffusion.
American Journal of Po-litical Science, 52(4):840?857.Xiaojun Wan and Jianwu Yang.
2008.
Multi-documentsummarization using cluster-based link analysis.
InACM SIGIR Conference on Research and Develop-ment in Information Retrieval, SIGIR ?08, pages299?306.Lu Wang and Claire Cardie.
2013.
Domain-independent abstract generation for focused meetingsummarization.
In Annual Meeting of the Associa-tion for Computational Linguistics, ACL?13, pages1395?1405.Wen-tau Yih, Joshua Goodman, Lucy Vanderwende,and Hisami Suzuki.
2007.
Multi-document summa-rization by maximizing informative content-words.In International Joint Conference on Artifical Intel-ligence, IJCAI?07, pages 1776?1782.1588
