Proceedings of NAACL HLT 2009: Short Papers, pages 9?12,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsEfficient Extraction of Oracle-best Translations from HypergraphsZhifei Li and Sanjeev KhudanpurCenter for Language and Speech Processing and Department of Computer ScienceThe Johns Hopkins University, Baltimore, MD 21218, USAzhifei.work@gmail.com and khudanpur@jhu.eduAbstractHypergraphs are used in several syntax-inspired methods of machine translation tocompactly encode exponentially many trans-lation hypotheses.
The hypotheses closest togiven reference translations therefore cannotbe found via brute force, particularly for pop-ular measures of closeness such as BLEU.
Wedevelop a dynamic program for extracting theso called oracle-best hypothesis from a hyper-graph by viewing it as the problem of findingthe most likely hypothesis under an n-gramlanguage model trained from only the refer-ence translations.
We further identify and re-move massive redundancies in the dynamicprogram state due to the sparsity of n-gramspresent in the reference translations, resultingin a very efficient program.
We present run-time statistics for this program, and demon-strate successful application of the hypothe-ses thus found as the targets for discriminativetraining of translation system components.1 IntroductionA hypergraph, as demonstrated by Huang and Chi-ang (2007), is a compact data-structure that can en-code an exponential number of hypotheses gener-ated by a regular phrase-based machine translation(MT) system (e.g., Koehn et al (2003)) or a syntax-based MT system (e.g., Chiang (2007)).
While thehypergraph represents a very large set of transla-tions, it is quite possible that some desired transla-tions (e.g., the reference translations) are not con-tained in the hypergraph, due to pruning or inherentdeficiency of the translation model.
In this case, oneis often required to find the translation(s) in the hy-pergraph that are most similar to the desired transla-tions, with similarity computed via some automaticmetric such as BLEU (Papineni et al, 2002).
Suchmaximally similar translations will be called oracle-best translations, and the process of extracting themoracle extraction.
Oracle extraction is a nontrivialtask because computing the similarity of any onehypothesis requires information scattered over manyitems in the hypergraph, and the exponentially largenumber of hypotheses makes a brute-force linearsearch intractable.
Therefore, efficient algorithmsthat can exploit the structure of the hypergraph arerequired.We present an efficient oracle extraction algo-rithm, which involves two key ideas.
Firstly, weview the oracle extraction as a bottom-up modelscoring process on a hypergraph, where the model is?trained?
on the reference translation(s).
This is sim-ilar to the algorithm proposed for a lattice by Dreyeret al (2007).
Their algorithm, however, requiresmaintaining a separate dynamic programming statefor each distinguished sequence of ?state?
words andthe number of such sequences can be huge, mak-ing the search very slow.
Secondly, therefore, wepresent a novel look-ahead technique, called equiv-alent oracle-state maintenance, to merge multiplestates that are equivalent for similarity computation.Our experiments show that the equivalent oracle-state maintenance technique significantly speeds up(more than 40 times) the oracle extraction.Efficient oracle extraction has at least three im-portant applications in machine translation.Discriminative Training: In discriminative train-ing, the objective is to tune the model parameters,e.g.
weights of a perceptron model or conditionalrandom field, such that the reference translations arepreferred over competitors.
However, the referencetranslations may not be reachable by the translationsystem, in which case the oracle-best hypothesesshould be substituted in training.9System Combination: In a typical system combi-nation task, e.g.
Rosti et al (2007), each compo-nent system produces a set of translations, whichare then grafted to form a confusion network.
Theconfusion network is then rescored, often employ-ing additional (language) models, to select the fi-nal translation.
When measuring the goodness of ahypothesis in the confusion network, one requiresits score under each component system.
However,some translations in the confusion network may notbe reachable by some component systems, in whichcase a system?s score for the most similar reachabletranslation serves as a good approximation.Multi-source Translation: In a multi-sourcetranslation task (Och and Ney, 2001) the input isgiven in multiple source languages.
This leadsto a situation analogous to system combination,except that each component translation system nowcorresponds to a specific source language.2 Oracle Extraction on a HypergraphIn this section, we present the oracle extraction al-gorithm: it extracts one or more translations in a hy-pergraph that have the maximum BLEU score1 withrespect to the corresponding reference translation(s).The BLEU score of a hypothesis h relative to areference r may be expressed in the log domain as,log BLEU(r, h) = min[1?
|r||h| , 0]+4?n=114 log pn.The first component is the brevity penalty when|h|<|r|, while the second component corresponds tothe geometric mean of n-gram precisions pn (withclipping).
While BLEU is normally defined at thecorpus level, we use the sentence-level BLEU forthe purpose of oracle extraction.Two key ideas for extracting the oracle-best hy-pothesis from a hypergraph are presented next.2.1 Oracle Extraction as Model ScoringOur first key idea is to view the oracle extractionas a bottom-up model scoring process on the hy-pergraph.
Specifically, we train a 4-gram languagemodel (LM) on only the reference translation(s),1We believe our method is general and can be extended toother metrics capturing only n-gram dependency and other com-pact data structures, e.g.
lattices.and use this LM as the only model to do a Viterbisearch on the hypergraph to find the hypothesis thathas the maximum (oracle) LM score.
Essentially,the LM is simply a table memorizing the counts ofn-grams found in the reference translation(s), andthe LM score is the log-BLEU value (instead of log-probability, as in a regular LM).
During the search,the dynamic programming (DP) states maintainedat each item include the left- and right-side LMcontext, and the length of the partial translation.To compute the n-gram precisions pn incrementallyduring the search, the algorithm also memorizes ateach item a vector of maximum numbers of n-grammatches between the partial translation and the ref-erence(s).
Note however that the oracle state of anitem (which decides the uniqueness of an item) de-pends only on the LM contexts and span lengths, noton this vector of n-gram match counts.The computation of BLEU also requires thebrevity penalty, but since there is no explicit align-ment between the source and the reference(s), wecannot get the exact reference length |r| at an inter-mediate item.
The exact value of brevity penalty isthus not computable.
We approximate the true refer-ence length for an item with a product between thelength of the source string spanned by that item anda ratio (which is between the lengths of the wholereference and the whole source sentence).
Anotherapproximation is that we do not consider the effectof clipping, since it is a global feature, making thestrict computation intractable.
This does not signifi-cantly affect the quality of the oracle-best hypothesisas shown later.
Table 1 shows an example how theBLEU scores are computed in the hypergraph.The process above may be used either in a first-stage decoding or a hypergraph-rescoring stage.
Inthe latter case, if the hypergraph generated by thefirst-stage decoding does not have a set of DP statesthat is a superset of the DP states required for ora-cle extraction, we need to split the items of the first-stage hypergraph and create new items with suffi-ciently detailed states.It is worth mentioning that if the hypergraph itemscontain the state information necessary for extract-ing the oracle-best hypothesis, it is straightforwardto further extract the k-best hypotheses in the hyper-graph (according to BLEU) for any k ?
1 using thealgorithm of Huang and Chiang (2005).10Item |h| |r?| matches log BLEUItem A 5 6.2 (3, 2, 2, 1) -0.82Item B 10 9.8 (8, 7, 6, 5) -0.27Item C 17 18.3 (12, 10, 9, 6) -0.62Table 1: Example computation when items A and B arecombined by a rule to produce item C. |r?| is the approxi-mated reference length as described in the text.2.2 Equivalent Oracle State MaintenanceThe process above, while able to extract the oracle-best hypothesis from a hypergraph, is very slow dueto the need to maintain a dedicated item for each or-acle state (i.e., a combination of left-LM state, right-LM state, and hypothesis length).
This is especiallytrue if the baseline system uses a LM whose order issmaller than four, since we need to split the items inthe original hypergraph into many sub-items duringthe search.
To speed up the extraction, our secondkey idea is to maintain an equivalent oracle state.Roughly speaking, instead of maintaining a dif-ferent state for different language model words, wecollapse them into a single state whenever it does notaffect BLEU.
For example, if we have two left-sideLM states a b c and a b d, and we know thatthe reference(s) do not have any n-gram ending withthem, then we can reduce them both to a b and ig-nore the last word.
This is because the combinationof neither left-side LM state (a b c or a b d) cancontribute an n-gram match to the BLEU computa-tion, regardless of which prefix in the hypergraphthey combine with.
Similarly, if we have two right-side LM states a b c and d b c, and if we knowthat the reference(s) do not have any n-gram startingwith either, then we can ignore the first word and re-duce them both to b c. We can continue this reduc-tion recursively as shown in Figures 1 and 2, whereIS-A-PREFIX(emi ) (or IS-A-SUFFIX(ei1)) checks ifemi (resp.
ei1) is a prefix (suffix) of any n-gram inthe reference translation(s).
For BLEU, 1 ?
n ?
4.This equivalent oracle state maintenance tech-nique, in practice, dramatically reduces the numberof distinct items preserved in the hypergraph for or-acle extraction.
To understand this, observe that ifall hypotheses in the hypergraph together contain munique n-grams, for any fixed n, then the total num-ber of equivalent items takes a multiplicative factorthat is O(m2) due to left- and right-side LM stateEQ-L-STATE (em1 )1 els?
em12 for i?
m to 1  right to left3 if IS-A-SUFFIX(ei1)4 break  stop reducing els5 else6 els?
ei?11  reduce state7 return elsFigure 1: Equivalent Left LM State Computation.EQ-R-STATE (em1 )1 ers?
em12 for i?
1 to m  left to right3 if IS-A-PREFIX (emi )4 break  stop reducing ers5 else6 ers?
emi+1  reduce state7 return ersFigure 2: Equivalent Right LM State Computation.maintenance of Section 2.1.
This multiplicative fac-tor under the equivalent state maintenance above isO(m?2), where m?
is the number of unique n-gramsin the reference translations.
Clearly, m?
m byseveral orders of magnitude, leading to effectivelymuch fewer items to process in the chart.One may view this idea of maintaining equivalentstates more generally as an outside look-ahead dur-ing bottom-up inside parsing.
The look-ahead usessome external information, e.g.
IS-A-SUFFIX(?
), toanticipate whether maintaining a detailed state nowwill be of consequence later; if not then the in-side parsing eliminates or collapses the state intoa coarser state.
The technique proposed by Li andKhudanpur (2008a) for decoding with large LMs isa special case of this general theme.3 Experimental ResultsWe report experimental results on a Chinese to En-glish task, for a system that is trained using a similarpipeline and data resource as in Chiang (2007).3.1 Goodness of the Oracle-Best TranslationsTable 2 reports the average speed (seconds/sentence)for oracle extraction.
Hypergraphs were generatedwith a trigram LM and expanded on the fly for 4-gram BLEU computation.11Basic DP Collapse equiv.
states speed-up25.4 sec/sent 0.6 sec/sent ?
42Table 2: Speed of oracle extraction from hypergraphs.The basic dynamic program (Sec.
2.1) improves signifi-cantly by collapsing equivalent oracle states (Sec.
2.2).Table 3 reports the goodness of the oracle-best hy-potheses on three standard data sets.
The highestachievable BLEU score in a hypergraph is clearlymuch higher than in the 500-best unique strings.This shows that a hypergraph provides a much betterbasis, e.g., for reranking than an n-best list.As mentioned in Section 2.1, we use several ap-proximations in computing BLEU (e.g., no clippingand approximate reference length).
To justify theseapproximations, we first extract 500-best unique or-acles from the hypergraph, and then rerank the ora-cles based on the true sentence-level BLEU.
The lastrow of Table 3 reports the reranked one-best oracleBLEU scores.
Clearly, the approximations do nothurt the oracle BLEU very much.Hypothesis space MT?04 MT?05 MT?061-best (Baseline) 35.7 32.6 28.3500-unique-best 44.0 41.2 35.1Hypergraph 52.8 51.8 37.8500-best oracles 53.2 52.2 38.0Table 3: Baseline and oracle-best 4-gram BLEU scoreswith 4 references for NIST Chinese-English MT datasets.3.2 Discriminative Hypergraph-RerankingOracle extraction is a critical component forhypergraph-based discriminative reranking, wheremillions of model parameters are discriminativelytuned to prefer the oracle-best hypotheses over oth-ers.
Hypergraph-reranking in MT is similar to theforest-reranking for monolingual parsing (Huang,2008).
Moreover, once the oracle-best hypothesisis identified, discriminative models may be trainedon hypergraphs in the same way as on n-best lists(cf e.g.
Li and Khudanpur (2008b)).
The results inTable 4 demonstrate that hypergraph-reranking witha discriminative LM or TM improves upon the base-line models on all three test sets.
Jointly trainingboth the LM and TM likely suffers from over-fitting.Test Set MT?04 MT?05 MT?06Baseline 35.7 32.6 28.3Discrim.
LM 35.9 33.0 28.2Discrim.
TM 36.1 33.2 28.7Discrim.
TM+LM 36.0 33.1 28.6Table 4: BLEU scores after discriminative hypergraph-reranking.
Only the language model (LM) or the transla-tion model (TM) or both (LM+TM) may be discrimina-tively trained to prefer the oracle-best hypotheses.4 ConclusionsWe have presented an efficient algorithm to extractthe oracle-best translation hypothesis from a hyper-graph.
To this end, we introduced a novel techniquefor equivalent oracle state maintenance, which sig-nificantly speeds up the oracle extraction process.Our algorithm has clear applications in diverse taskssuch as discriminative training, system combinationand multi-source translation.ReferencesD.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33(2):201-228.M.
Dreyer, K. Hall, and S. Khudanpur.
2007.
Compar-ing Reordering Constraints for SMT Using EfficientBLEU Oracle Computation.
In Proc.
of SSST.L.
Huang.
2008.
Forest Reranking: Discriminative Pars-ing with Non-Local Features.
In Proc.
of ACL.L.
Huang and D. Chiang.
2005.
Better k-best parsing.
InProc.
of IWPT.L.
Huang and D. Chiang.
2007.
Forest Rescoring: FasterDecoding with Integrated Language Models.
In Proc.of ACL.P.
Koehn, F. J. Och, and D. Marcu.2003.
Statisticalphrase-based translation.
In Proc.
of NAACL.Z.
Li and S. Khudanpur.
2008a.
A Scalable Decoder forParsing-based Machine Translation with EquivalentLanguage Model State Maintenance.
In Proc.
SSST.Z.
Li and S. Khudanpur.
2008b.
Large-scale Discrimina-tive n-gram Language Models for Statistical MachineTranslation.
In Proc.
of AMTA.F.
Och and H. Ney.
2001.
Statistical multisource transla-tion.
In Proc.
MT Summit VIII.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL.A.I.
Rosti, S. Matsoukas, and R. Schwartz.
2007.
Im-proved word-level system combination for machinetranslation.
In Proc.
of ACL.12
