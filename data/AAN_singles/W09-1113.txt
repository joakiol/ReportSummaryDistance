Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 93?101,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsLearning Where to Look: Modeling Eye Movements in ReadingMattias NilssonDepartment of Linguistics and PhilologyUppsala Universitymattias.nilsson@lingfil.uu.seJoakim NivreDepartment of Linguistics and PhilologyUppsala Universityjoakim.nivre@lingfil.uu.seAbstractWe propose a novel machine learning task thatconsists in learning to predict which words ina text are fixated by a reader.
In a first pilotexperiment, we show that it is possible to out-perform a majority baseline using a transition-based model with a logistic regression classi-fier and a very limited set of features.
We alsoshow that the model is capable of capturingfrequency effects on eye movements observedin human readers.1 IntroductionAny person engaged in normal skilled reading pro-duces an alternating series of rapid eye movementsand brief fixations that forms a rich and detailed be-havioral record of the reading process.
In the lastfew decades a great deal of experimental evidencehas accumulated to suggest that the eye movementsof readers are reflective of ongoing language pro-cessing and thus provide a useful source of infor-mation for making inferences about the linguisticprocesses involved in reading (Clifton et al, 2007).In psycholinguistic research, eye movement data isnow commonly used to study how experimental ma-nipulations of linguistic stimuli manifest themselvesin the eye movement record.Another related strand of research primarily at-tempts to understand what determines when andwhere the eyes move during reading.
This line ofresearch has led to mathematically well specified ac-counts of eye movement control in reading beinginstantiated as computational models (Legge et al,1997; Reichle et al, 1998; Salvucci, 2001; Engbertet al, 2002; McDonald et al, 2005; Feng, 2006;Reilly and Radach, 2006; Yang, 2006).
(For a re-cent overview, see (Reichle, 2006).)
These modelsreceive text as input and produce predictions for thelocation and duration of eye fixations, in approxima-tion to human reading behavior.
Although there aresubstantial differences between the various models,they typically combine both mechanisms of visuo-motor control and linguistic processing.
Two impor-tant points of divergence concern the extent to whichlanguage processing influences eye movements andwhether readers process information from more thanone word at a time (Starr and Rayner, 2001).
Moregenerally, the models that have emerged to date arebased on different sets of assumptions about the un-derlying perceptual and cognitive mechanisms thatcontrol eye movements.
The most influential modelso far, the E-Z Reader model (Reichle et al, 1998;Reichle et al, 2003; Pollatsek et al, 2006), rests onthe assumptions that cognitive / lexical processing isthe engine that drives the eyes through the text andthat words are identified serially, one at a time.Although eye movement models typically haveparameters that are fitted to empirical data sets, theyare not based on machine learning in the standardsense and their predictions are hardly ever tested onunseen data.
Moreover, their predictions are nor-mally averaged over a whole group of readers orwords belonging to a given frequency class.
In thisstudy, however, we investigate whether saccadic eyemovements during reading can be modeled usingmachine learning.
The task we propose is to learnto predict the eye movements of an individual readerreading a specific text, using as training data the eye93movements recorded for the same person readingother texts.Predicting the eye movements of an individualreader on new texts is arguably a hard problem, andwe therefore restrict the task to predicting word-based fixations (but not the duration of these fixa-tions) and focus on a first pilot experiment inves-tigating whether we can outperform a reasonablebaseline on this task.
More precisely, we present ex-perimental results for a transition-based model, us-ing a log-linear classifier, and show that the modelsignificantly outperforms the baseline of always pre-dicting the most frequent saccade.
In addition, weshow that even this simple model is able to capturefrequency effects on eye movements observed in hu-man readers.We want to emphasize that the motivation for thismodeling experiment is not to advance the state ofthe art in computational modeling of eye movementsduring reading.
For this our model is far too crudeand limited in scope.
The goal is rather to propose anovel approach to the construction and evaluation ofsuch models, based on machine learning and modelassessment on unseen data.
In doing this, we wantto establish a reasonable baseline for future researchby evaluating a simple model with a restricted setof features.
In future studies, we intend to inves-tigate how results can be improved by introducingmore complex models as well as a richer featurespace.
More generally, the machine learning ap-proach explored here places emphasis on modelingeye movement behavior with few a priori assump-tions about underlying cognitive and physiologicalmechanisms.The rest of the paper is structured as follows.
Sec-tion 2 provides a brief background on basic charac-teristics of eye movements in reading.
The emphasisis on saccadic eye movements rather than on tempo-ral aspects of fixations.
Section 3 defines the noveltask of learning to predict fixations during readingand discusses different evaluation metrics for thistask.
Section 4 presents a transition-based modelfor solving this task, using a log-linear classifier topredict the most probable transition after each fixa-tion.
Section 5 presents experimental results for themodel using data from the Dundee corpus (Kennedyand Pynte, 2005), and Section 6 contains conclu-sions and suggestions for future research.2 Eye Movements in ReadingPerhaps contrary to intuition, the eyes of readers donot move smoothly across a line or page of text.
It isa salient fact in reading research that the eyes makea series of very rapid ballistic movements (calledsaccades) from one location to another.
In betweensaccades, the eyes remain relatively stationary forbrief periods of time (fixations).
Most fixations lastabout 200-300 ms but there is considerable variabil-ity, both between and within readers.
Thus, somefixations last under 100 ms while others last over500 ms (Rayner, 1998).
Much of the variability infixation durations appears associated to processingease or difficulty.The number of characters that is within the re-gion of effective vision on any fixation is known asthe perceptual span.
For English readers, the per-ceptual span extends approximately four charactersto the left and fifteen characters to the right of thefixation.
Although readers fixate most words in atext, many words are also skipped.
Approximately85% of the content words are fixated and 35% ofthe function words (Carpenter and Just, 1983).
Vari-ables known to influence the likelihood of skippinga word are word length, frequency and predictabil-ity.
Thus, more frequent words in the language areskipped more often than less frequent words.
This istrue also when word length is controlled for.
Simi-larly, words that occur in constrained contexts (andare thus more predictable) are skipped more oftenthan words in less constrained contexts.Although the majority of saccades in reading isrelatively local, i.e., target nearby words, more dis-tant saccades also occur.
Most saccades move theeyes forward approximately 7?9 character spaces.Approximately 15% of the saccades, however, areregressions, in which the eyes move back to earlierparts of the text (Rayner, 1998).
It has long beenestablished that the length of saccades is influencedby both the length of the fixated word and the wordto the right of the fixation (O?Regan, 1979).
Re-gressions often go back one or two words, but occa-sionally they stretch further back.
Such backwardmovements are often thought to reflect linguisticprocessing difficulty, e.g., because of syntactic pars-ing problems.
Readers, however, are often unawareof making regressions, especially shorter ones.943 The Learning TaskWe define a text T as a sequence of word tokens(w1, .
.
.
, wn), and we define a fixation sequenceF for T as a sequence of token positions in T(i1, .
.
.
, im) (1 < ik < n).
The fixation set S(F )corresponding to F is the set of token positions thatoccur in F .
For example, the text Mary had a lit-tle lamb is represented by T = (Mary, had, a, little,lamb); a reading of this text where the sequence offixations is Mary ?
little ?
Mary ?
lamb is repre-sented by F = (1, 4, 1, 5); and the correspondingfixation set is S(F ) = {1, 4, 5}.The task we now want to consider is the oneof predicting the fixation sequence F for a spe-cific reading event E involving person P readingtext T .
The training data consist of fixation se-quences F1, .
.
.
, Fk for reading events distinct fromE involving the same person P but different textsT1, .
.
.
, Tk.
The performance of a model M is eval-uated by comparing the predicted fixation sequenceFM to the fixation sequence FO observed in a read-ing experiment involving P and T .
Here are someof the conceivable metrics for this evaluation:1.
Fixation sequence similarity: How similarare the sequences FM and FO, as measured, forexample, by some string similarity metric?2.
Fixation accuracy: How large is the agree-ment between the sets S(FM ) and S(FO), asmeasured by 0-1-loss over the entire text, i.e.,how large is the proportion of positions that areeither in both S(FM ) and S(FO) (fixated to-kens) or in neither (skipped tokens).
This canalso be broken down into precision and recallfor fixated and skipped tokens, respectively.3.
Fixation distributions: Does the model pre-dict the correct proportion of fixated andskipped tokens, as measured by the differencebetween |S(FM )|/|T | and |S(FO)|/|T |?
Thiscan also be broken down by frequency classesof words, to see if the model captures frequencyeffects reported in the literature.These evaluation metrics are ordered by an implica-tional scale from hardest to easiest.
Thus, a modelthat correctly predicts the exact fixation sequencealso makes correct predictions with respect to theset of words fixated and the number of words fixated(but not vice versa).
In the same fashion, a modelthat correctly predicts which words are fixated (butnot the exact sequence) also correctly predicts thenumber of words fixated.In the experiments reported in Section 5, we willuse variants of the latter two metrics and comparethe performance of our model to the baseline of al-ways predicting the most frequent type of saccadefor the reader in question.
We will report resultsboth for individual readers and mean scores over allreaders in the test set.
The evaluation of fixation se-quence similarity (the first type of metric) will beleft for future work.4 A Transition-Based ModelWhen exploring a new task, we first have to decidewhat kind of model to use.
As stated in the introduc-tion, we regard this as a pilot experiment to establishthe feasibility of the task and have therefore chosento start with one of the simplest models possible andsee whether we can beat the baseline of always pre-dicting the most frequent saccade.
Since the taskconsists in predicting a sequence of different actions,it is very natural to use a transition-based model,with configurations representing fixation states andtransitions representing saccadic movements.
Givensuch a system, we can train a classifier to predict thenext transition given the information in the currentconfiguration.
In order to derive a complete tran-sition sequence, we start in an initial configuration,representing the reader?s state before the first fixa-tion, and repeatedly apply the transition predicted bythe classifier until we reach a terminal state, repre-senting the reader?s state after having read the entiretext.
At an abstract level, this is essentially the sameidea as in transition-based dependency parsing (Ya-mada and Matsumoto, 2003; Nivre, 2006; Attardi,2006).
In the following subsections, we discuss thedifferent components of the model in turn, includingthe transition system, the classifier used, the featuresused to represent data, and the search algorithm usedto derive complete transition sequences.4.1 Transition SystemA transition system is an abstract machine consist-ing of a set of configurations and transitions between95configurations.
A configuration in the current sys-tem is a triple C = (L,R, F ), where1.
L is a list of tokens representing the left con-text, including the currently fixated token andall preceding tokens in the text.2.
R is a list of tokens representing the right con-text, including all tokens following the cur-rently fixated token in the text.3.
F is a list of token positions, representing thefixation sequence so far, including the currentlyfixated token.For example, if the text to be read is Mary had alittle lamb, then the configuration([Mary,had,a,little], [lamb], [1,4])represents the state of a reader fixating the word littleafter first having fixated the word Mary.For any text T = w1 .
.
.
wn, we define initial andterminal configurations as follows:1.
Initial: C = ([ ], [w1, .
.
.
, wn], [ ])2.
Terminal: C = ([w1, .
.
.
, wn], [ ], F )(for any F )We then define the following transitions:11.
Progress(n):([?|wi], [wi+1, .
.
.
, wi+n|?
], [?|i])?
([?|wi, wi+1, .
.
.
, wi+n], ?, [?|i, i+n])2.
Regress(n):([?|wi?n, .
.
.
, wi?1, wi], ?, [?|i])?
([?|wi?n], [wi?n+1, .
.
.
, wi|?
], [?|i, i?n])3.
Refixate:([?|wi], ?, [?|i])?
([?|wi], ?, [?|i, i])The transition Progress(n) models progressive sac-cades of length n, which means that the next fixatedword is n positions forward with respect to the cur-rently fixated word (i.e., n?1 words are skipped).In a similar fashion, the transition Regress(n) mod-els regressive saccades of length n. If the parameter1We use the variables ?, ?
and ?
for arbitrary sublists of L,R and F , respectively, and we write the L and F lists with theirtails to the right, to maintain the natural order of words.n of either Progress(n) or Regress(n) is greater thanthe number of words remaining in the relevant di-rection, then the longest possible movement is madeinstead, in which case Regress(n) leads to a terminalconfiguration while Progress(n) leads to a configu-ration that is similar to the initial configuration inthat it has an empty L list.
The transition Refixate,finally, models refixations, that is, cases where thenext word fixated is the same as the current.To illustrate how this system works, we may con-sider the transition sequence corresponding to thereading of the text Mary had a little lamb used asan example in Section 3:2Init ?
([ ], [Mary, .
.
.
, lamb], [ ])P(1) ?
([Mary], [had, .
.
.
, lamb], [1])P(3) ?
([Mary, .
.
.
, little], [lamb], [1,4])R(3) ?
([Mary], [had, .
.
.
, lamb], [1,4,1])P(4) ?
([Mary, .
.
.
, lamb], [ ], [1,4,1,5])4.2 Learning TransitionsThe transition system defined in the previous sectionspecifies the set of possible saccade transitions thatcan be executed during the reading of a text, but itdoes not say anything about the probability of dif-ferent transitions in a given configuration, nor doesit guarantee that a terminal configuration will everbe reached.
The question is now whether we canlearn to predict the most probable transition in sucha way that the generated transition sequences modelthe behavior of a given reader.
To do this we needto train a classifier that predicts the next transitionfor any configuration, using as training data the ob-served fixation sequences of a given reader.
Beforethat, however, we need to decide on a feature repre-sentation for configurations.Features used in this study are listed in Table 1.We use the notation L[i] to refer to the ith tokenin the list L and similarly for R and F .
The firsttwo features refer to properties of the currently fix-ated token.
Length is simply the character lengthof the word, while frequency class is an index ofthe word?s frequency of occurrence in representativetext.
Word frequencies are based on occurrences inthe Bristish National Corpus (BNC) and divided into2We abbreviate Progress(n) and Regress(n) to P(n) andR(n), respectively.96Feature DescriptionCURRENT.LENGTH The length of the token L[1]CURRENT.FREQUENCYCLASS The frequency class of the token L[1]NEXT.LENGTH The length of the token R[1]NEXT.FREQUENCYCLASS The frequency class of the token R[1]NEXTPLUSONE.LENGTH The length of the token R[2]NEXTPLUSTWO.LENGTH The length of the token R[3]DISTANCE.ONETOTWO The distance, in tokens, between F [1] and F [2]DISTANCE.TWOTOTHREE The distance, in tokens, between F [2] and F [3]Table 1: Features defined over fixation configurations.
The notation L[i] is used to denote the ith element of list L.five classes.
Frequencies were computed per millionwords in the ranges 1?10, 11?100, 101?1000, 1001?10000, and more than 10000.The next four features define features of tokensto the right of the current fixation.
For the to-ken immediately to the right, both length and fre-quency are recorded whereas only length is con-sidered for the two following tokens.
The lasttwo features are defined over tokens in the fixa-tion sequence built thus far and record the historyof the two most recent saccade actions.
The firstof these (DISTANCE.ONETOTWO) defines the sac-cade distance, in number of tokens, that led upto the token currently being fixated.
The second(DISTANCE.TWOTOTHREE), defines the next mostrecent saccade distance, that led up to the previousfixation.
For these two features the following holds.If the distance is positive, the saccade is progressive,if the distance is negative, the saccade is regressive,and if the distance amounts to zero, the saccade is arefixation.The small set of features used in the current modelwere chosen to reflect experimental evidence on eyemovements in reading.
Thus, for example, as notedin section 2, it is a well-documented fact that short,frequent and predictable words tend to be skipped.The last two features are included in the hope ofcapturing some of the dynamics in eye movementbehavior, for example, if regressions are more likelyto occur after longer progressive saccades, or if thenext word is skipped more often if the current wordis refixated.
Still, it is clear that this is only a tinysubset of the feature space that might be considered,and it remains an important topic for future researchto further explore this space and to study the impactof different features.Given our feature representation, and given sometraining data derived from reading experiments, itis straightforward to train a classifier for predictingthe most probable transition out of any configura-tion.
There are many learning algorithms that couldbe used for this purpose, but in the pilot experimentswe only make use of logistic regression.4.3 Search AlgorithmOnce we have trained a classifier f that predicts thenext transition f(C) out of any configuration C, wecan simulate the eye movement behavior of a personreading the text T = (w1, .
.
.
, wn) using the follow-ing simple search algorithm:1.
Initialize C to ([ ], [w1, .
.
.
, wn], [ ]).2.
While C is not terminal, apply f(C) to C.3.
Return F of C.It is worth noting that search will always terminateonce a terminal configuration has been reached, eventhough there is nothing in the transition system thatforbids transitions out of terminal configurations.
Inother words, while the model itself allows regres-sions and refixations after the last word of the texthas been fixated, the search algorithm does not.
Thisseems like a reasonable approximation for this pilotstudy.5 Experiments5.1 Experimental SetupThe experiments we report are based on data fromthe English section of the Dundee corpus.
This sec-97Fixation Accuracy Fixations SkipsReader # sentences Baseline Model Prec Rec F1 Prec Rec F1a 136 53.3 70.0 69.9 73.8 71.8 69.0 65.8 67.4b 156 55.7 66.5 65.2 85.8 74.1 70.3 80.4 75.0c 151 59.9 70.9 72.5 82.8 77.3 67.4 53.1 59.4d 162 69.0 78.9 84.7 84.8 84.7 66.0 65.8 65.9e 182 51.7 71.8 69.1 78.4 73.5 75.3 65.2 69.9f 157 63.5 67.9 70.9 83.7 76.8 58.7 40.2 47.7g 129 43.3 56.6 49.9 80.8 61.7 72.2 38.1 49.9h 143 57.6 66.9 69.4 76.3 72.7 62.8 54.3 58.2i 196 56.4 69.1 69.6 80.3 74.6 68.2 54.7 60.7j 166 66.1 76.3 82.2 81.9 82.0 65.0 65.4 65.2Average 157.8 57.7 69.5 70.3 80.9 75.2 67.5 58.3 62.6Table 2: Fixation and skipping accuracy on test data; Prec = precision, Rec = recall, F1 = balanced F measure.tion contains the eye tracking record of ten partici-pants reading editorial texts from The Independentnewspaper.
The corpus contains 20 texts, each ofwhich were read by all participants.
Participants alsoanswered a set of multiple-choice comprehensionquestions after having finished reading each text.The corpus consists of 2379 sentences, 56212 tokensand 9776 types.
The data was recorded using a Dr.Bouis Oculometer Eyetracker, sampling the positionof the right eye every millisecond (see Kennedy andPynte, 2005, for further details).For the experiments reported here, the corpus wasdivided into three data sets: texts 1-16 for training(1911 sentences), texts 17-18 for development andvalidation (237 sentences), and the last two texts 19-20 for testing (231 sentences).Since we want to learn to predict the observedsaccade transition for any fixation configuration,where configurations are represented as feature vec-tors, it is not possible to use the eye tracking datadirectly as training and test data.
Instead, we simu-late the search algorithm on the corpus data of eachreader in order to derive, for each sentence, the fea-ture vectors over the configurations and the tran-sitions corresponding to the observed fixation se-quence.
The instances to be classified then consist offeature representations of configurations while theclasses are the possible transitions.To somewhat simplify the learning task in thisfirst study, we removed all instances of non-localsaccades prior to training.
Progressions stretchingfurther than five words ahead of the current fixationwere removed, as were regressions stretching furtherback than two words.
Refixations were not removed.Thus we reduced the number of prediction classes toeight.
Removal of the non-local saccade instancesresulted in a 1.72% loss over the total number of in-stances in the training data for all readers.We trained one classifier for each reader using lo-gistic regression, as implemented in Weka (Wittenand Eibe, 2005) and default options.
In addition, wetrained majority baseline classifiers for all readers.These models always predict the most frequent sac-cadic eye movement for a given reader.The classifiers were evaluated with respect to theaccuracy achieved when reading previously unseentext using the search algorithm in 4.3.
To ensurethat test data were consistent with training data, sen-tences including any saccade outside of the localrange were removed prior to test.
This resultedin removal of 18.9% of the total number of sen-tences in the test data for all readers.
Accuracy wasmeasured in three different ways.
First, we com-puted the fixation accuracy, that is, the proportionof words that were correctly fixated or skipped bythe model, which we also broke down into precisionand recall for fixations and skips separately.3 Sec-ondly, we compared the predicted fixation distribu-3Fixation/skip precision is the proportion of tokens fix-ated/skipped by the model that were also fixated/skipped bythe reader; fixation/skip recall is the proportion of tokens fix-ated/skipped by the reader that were also fixated/skipped by themodel.98tions to the observed fixation distributions, both overall words and broken down into the same five fre-quency classes that were used as features (see Sec-tion 4).
The latter statistics, averaged over all read-ers, allow us to see whether the model correctly pre-dicts the frequency effect discussed in section 2.5.2 Results and DiscussionTable 2 shows the fixation accuracy, and precision,recall and F1 for fixations and skips, for each of theten different models and the average across all mod-els (bottom row).
Fixation accuracy is compared tothe baseline of always predicting the most frequentsaccade type (Progress(2) for readers a and e, andProgress(1) for the rest).If we consider the fixation accuracy, we see thatall models improve substantially on the baselinemodels.
The mean difference between models andbaselines is highly significant (p < .001, paired t-test).
The relative improvement ranges from 4.4 per-centage points in the worst case (model of reader f )to 20.1 percentage points in the best case (model ofreader e).
The highest scoring model, the model ofreader d, has an accuracy of 78.9%.
The lowest scor-ing model, the model of reader g, has an accuracyof 56.6%.
This is also the reader for whom thereis the smallest number of sentences in the test data(129), which means that a large number of sentenceswere removed prior to testing because of the greaternumber of non-local saccades made by this reader.Thus, this reader has an unusually varied saccadicbehaviour which is particularly hard to model.Comparing the precision and recall for fixationand skips, we see that while precision tends to beabout the same for both categories (with a few no-table exceptions), recall is consistently higher forfixations than for skips.
We believe that this is dueto a tendency of the model to overpredict fixations,especially for low-frequency words.
This has a greatimpact on the F1 measure (unweighted harmonicmean of precision and recall), which is considerablyhigher for fixations than for skips.Figure 1 shows the distributions of fixationsgrouped by reader and model.
The models appearreasonably good at adapting to the empirical fixa-tion distribution of individual readers.
However, themodels typically tend to look at more words than thereaders, as noted above.
This suggests that the mod-els lack sufficient information to learn to skip wordsmore often.
This might be overcome by introducingfeatures that further encourage skipping of words.
Inaddition to word length and word frequency, that arealready accounted for, n-gram probability could beincluded as a measure of predictability, for example.We also note that there is a strong linear relationbetween the capability of fitting the empirical dis-tribution well and achieving high fixation accuracy(Pearson?s r: -0.91, as measured by taking the dif-ferences of each pair of distributions and correlatingthem with the fixation accuracy of the models).Figure 2 shows the mean observed and predictedfixation and skipping probability as a function ofword frequency class, averaged over all readers.
Asseen here, model prediction is responsive to fre-quency class in a fashion comparable to the read-ers, although the predictions typically tend to exag-gerate the observed frequency effect.
In the lowerto medium classes (1?3), almost every word is fix-ated.
Then there is a clear drop in fixation proba-bility for words in frequency class 4 which fits wellwith the observed fixation probability.
Finally thereis another drop in fixation probability for the mostfrequent words (5).
The skipping probabilities forthe different classes show the corresponding reversetrend.6 ConclusionIn this paper we have defined a new machine learn-ing task where the goal is to learn the saccadic eyemovement behavior of individual readers in orderto predict the sequence of word fixations for novelreading events.
We have discussed different evalua-tion metrics for this task, and we have established afirst benchmark by training and evaluating a simpletransition-based model using a log-linear classifierto predict the next transition.
The evaluation showsthat even this simple model, with features limited toa few relevant properties in a small context window,outperforms a majority baseline and captures someof the word frequency effects on eye movements ob-served in human readers.This pilot study opens up a number of direc-tions for future research.
With respect to mod-eling, we need to explore more complex models,richer feature spaces, and alternative learning algo-99a b c d e f g h i jReaderModelProportion0.00.20.40.60.8Figure 1: Proportion of fixated tokens grouped by reader and modelF FFFF1 2 3 4 50.00.20.40.60.81.0F F FFFS SSSSS SSSSFixation probabilityFrequency classFFSSFixation ?
ObservedFixation ?
PredictedSkipping ?
ObservedSkipping ?
PredictedFigure 2: Mean observed and predicted fixation and skipping probability for five frequency classes of wordsrithms.
For example, given the sequential natureof the task, it seems natural to explore probabilisticsequence models such as HMMs (see for exampleFeng (2006)).
With respect to evaluation, we needto develop metrics that are sensitive to the sequentialbehavior of models, such as the fixation sequencesimilarity measure discussed in Section 3, and in-vestigate to what extent results can be generalizedacross readers.
With respect to the task itself, weneed to introduce additional aspects of the readingprocess, in particular the duration of fixations.
Bypursuing these lines of research, we should be ableto gain a better understanding of how machine learn-ing methods in eye movement modeling can informand advance current theories and models in readingand psycholinguistic research.100ReferencesGiuseppe Attardi.
2006.
Experiments with a multilan-guage non-projective dependency parser.
In Proceed-ings of the 10th Conference on Computational NaturalLanguage Learning (CoNLL), pages 166?170.Patricia A. Carpenter and Marcel A.
Just.
1983.
Whatyour eyes do while your mind is reading.
In KeithRayner, editor, Eye movements in reading: Perceptualand language processes, pages 275?307.
New York:Academic Press.Charles Clifton, Adrian Staub, and Keith Rayner.
2007.Eye movements in reading words and sentences.
InRoger van Gompel, editor, Eye movements: A windowon mind and brain, pages 341?372.
Amsterdam: Else-vier.Ralf Engbert, Andr?
Longtin, and Reinhold Kliegl.
2002.A dynamical model of saccade generation in readingbased on spatially distributed lexical processing.
Vi-sion Research, 42:621?636.Gary Feng.
2006.
Eye movements as time-series randomvariables: A stochastic model of eye movement con-trol in reading.
Cognitive Systems Research, 7:70?95.Alan Kennedy and Jo?l Pynte.
2005.
Parafoveal-on-foveal effects in normal reading.
Vision research,45:153?168.Gordon E. Legge, Timothy S. Klitz, and Bosco S. Tjan.1997.
Mr. Chips: An ideal-observer model of reading.Psychological Review, 104:524?553.Scott A. McDonald, R.H.S.
Carpenter, and Richard C.Schillcock.
2005.
An anatomically-constrained,stochastic model of eye movement control in reading.Psychological Review, 112:814?840.Joakim Nivre.
2006.
Inductive Dependency Parsing.Springer.J.
Kevin O?Regan.
1979.
Eye guidance in reading: Evi-dence for the linguistic control hypothesis.
Perception& Psychophysics, 25:501?509.Alexander Pollatsek, Erik Reichle, and Keith Rayner.2006.
Tests of the E-Z Reader model: Exploring theinterface between cognition and eye movements.Keith Rayner.
1998.
Eye movements in reading and in-formation processing: 20 years of research.
Psycho-logical Bulletin, 124:372?422.Erik Reichle, Alexander Pollatsek, Donald Fisher, andKeith Rayner.
1998.
Toward a model of eyemovement control in reading.
Psychological Review,105:125?157.Erik Reichle, Keith Rayner, and Alexander Pollatsek.2003.
The E-Z Reader model of eye-movement con-trol in reading: Comparisons to other models.
Behav-ioral and Brain Sciences, 26:445?476.Eric Reichle, editor.
2006.
Cognitive Systems Research.7:1?96.
Special issue on models of eye-movementcontrol in reading.Ronan Reilly and Ralph Radach.
2006.
Some empiricaltests of an interactive activation model of eye move-ment control in reading.
Cognitive Systems Research,7:34?55.Dario D. Salvucci.
2001.
An integrated model of eyemovements and visual encoding.
Cognitive SystemsResearch, 1:201?220.Matthew Starr and Keith Rayner.
2001.
Eye movementsduring reading: some current controversies.
Trends inCognitive Sciences, 5:156?163.Ian H. Witten and Frank Eibe.
2005.
Data Mining: Prac-tical machine learning tools and techniques.
MorganKaufmann.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisti-cal dependency analysis with support vector machines.In Proceedings of the 8th International Workshop onParsing Technologies (IWPT), pages 195?206.Shun-nan Yang.
2006.
A oculomotor-based model ofeye movements in reading: The competition/activationmodel.
Cognitive Systems Research, 7:56?69.101
