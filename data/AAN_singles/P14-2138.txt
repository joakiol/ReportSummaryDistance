Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 854?859,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsDoes the Phonology of L1 Show Up in L2 Texts?Garrett Nicolai and Grzegorz KondrakDepartment of Computing ScienceUniversity of Alberta{nicolai,gkondrak}@ualberta.caAbstractThe relative frequencies of character bi-grams appear to contain much informationfor predicting the first language (L1) of thewriter of a text in another language (L2).Tsur and Rappoport (2007) interpret thisfact as evidence that word choice is dic-tated by the phonology of L1.
In order totest their hypothesis, we design an algo-rithm to identify the most discriminativewords and the corresponding character bi-grams, and perform two experiments toquantify their impact on the L1 identifica-tion task.
The results strongly suggest analternative explanation of the effectivenessof character bigrams in identifying the na-tive language of a writer.1 IntroductionThe task of Native Language Identification (NLI)is to determine the first language of the writer of atext in another language.
In a ground-breaking pa-per, Koppel et al (2005) propose a set of featuresfor this task: function words, character n-grams,rare part-of-speech bigrams, and various types oferrors.
They report 80% accuracy in classifying aset of English texts into five L1 languages using amulti-class linear SVM.The First Shared Task on Native LanguageIdentification (Tetreault et al, 2013) attracted sub-missions from 29 teams.
The accuracy on a setof English texts representing eleven L1 languagesranged from 31% to 83%.
Many types of fea-tures were employed, including word length, sen-tence length, paragraph length, document length,sentence complexity, punctuation and capitaliza-tion, cognates, dependency parses, topic mod-els, word suffixes, collocations, function word n-grams, skip-grams, word networks, Tree Substi-tution Grammars, string kernels, cohesion, andpassive constructions (Abu-Jbara et al, 2013; Li,2013; Brooke and Hirst, 2013; Cimino et al, 2013;Daudaravicius, 2013; Goutte et al, 2013; Hender-son et al, 2013; Hladka et al, 2013; Bykh et al,2013; Lahiri and Mihalcea, 2013; Lynum, 2013;Malmasi et al, 2013; Mizumoto et al, 2013; Nico-lai et al, 2013; Popescu and Ionescu, 2013; Swan-son, 2013; Tsvetkov et al, 2013).
In particular,word n-gram features appear to be particularly ef-fective, as they were used by the most competitiveteams, including the one that achieved the highestoverall accuracy (Jarvis et al, 2013).
Furthermore,the most discriminative word n-grams often con-tained the name of the native language, or coun-tries where it is commonly spoken (Gebre et al,2013; Malmasi et al, 2013; Nicolai et al, 2013).We refer to such words as toponymic terms.There is no doubt that the toponymic termsare useful for increasing the NLI accuracy; how-ever, from the psycho-linguistic perspective, weare more interested in what characteristics of L1show up in L2 texts.
Clearly, L1 affects the L2writing in general, and the choice of words in par-ticular, but what is the role played by the phonol-ogy?
Tsur and Rappoport (2007) observe that lim-iting the set of features to the relative frequency ofthe 200 most frequent character bigrams yields arespectable 66% accuracy on a 5-language classi-fication task.
The authors propose the followinghypothesis to explain this finding: ?the choice ofwords [emphasis added] people make when writ-ing in a second language is strongly influenced bythe phonology of their native language?.
As theorthography of alphabetic languages is at least par-tially representative of the underlying phonology,character bigrams may capture these phonologicalpreferences.In this paper, we provide evidence against theabove hypothesis.
We design an algorithm to iden-tify the most discriminative words and the char-acter bigrams that are indicative of such words,854and perform two experiments to quantify their im-pact on the NLI task.
The results of the first ex-periment demonstrate that the removal of a rela-tively small set of discriminative words from thetraining data significantly impairs the accuracy ofa bigram-based classifier.
The results of the sec-ond experiment reveal that the most indicative bi-grams are quite similar across different languagesets.
We conclude that character bigrams are ef-fective in determining L1 of the author becausethey reflect differences in L2 word usage that areunrelated to the phonology of L1.2 MethodTsur and Rappoport (2007) report that characterbigrams are more effective for the NLI task thaneither unigrams or trigrams.
We are interested inidentifying the character bigrams that are indica-tive of the most discriminative words in order toquantify their impact on the bigram-based classi-fier.We follow both Koppel et al (2005) and Tsurand Rappoport (2007) in using a multi-class SVMclassifier for the NLI task.
The classifier computesa weight for each feature coupled with each L1language by attempting to maximize the overallaccuracy on the training set.
For example, if wetrain the classifier using words as features, withvalues representing their frequency relative to thelength of the document, the features correspond-ing to the word China might receive the followingweights:Arabic Chinese Hindi Japanese Telugu-770 1720 -276 -254 -180These weights indicate that the word providesstrong positive evidence for Chinese as L1, as op-posed to the other four languages.We propose to quantify the importance of eachword by converting its SVM feature weights intoa single score using the following formula:WordScorei=???
?N?j=1wij2where N is the number of languages, and wijis the feature weight of word i in language j.The formula assigns higher scores to words withweights of high magnitude, either positive or neg-ative.
We use the Euclidean norm rather than theAlgorithm 1 Computing the scores of words andbigrams in the data.1: create list of words in training data2: train SVM using words as features3: for all words i do4: WordScorei=?
?Nj=1wij25: end for6: sort words by WordScore7: NormValue = WordScore2008: create list of 200 most frequent bigrams9: for bigrams k = 1 to 200 do10: BigramScorek=?k?iWordScoreiNormV alue11: end for12: sort character bigrams by BigramScoresum of raw weights because we are interested inthe discriminative power of the words.We normalize the word scores by dividing themby the score of the 200th word.
Consequently,only the top 200 words have scores greater thanor equal to 1.0.
For our previous example, the200thword has a word score of 1493, while Chinahas a word score of 1930, which is normalized to1930/1493 = 1.29.
On the other hand, the 1000thword gets a normalized score of 0.43.In order to identify the bigrams that are indica-tive of the most discriminative words, we promotethose that appear in the high-scoring words, anddowngrade those that appear in the low-scoringwords.
Some bigrams that appear often in thehigh-scoring words may be very common.
For ex-ample, the bigram an occurs in words like Japan,German, and Italian, but also by itself as a deter-miner, as an adjectival suffix, and as part of theconjunction and.
Therefore, we calculate the im-portance score for each character bigram by multi-plying the scores of each word in which the bigramoccurs.Algorithm 1 summarizes our method of identi-fying the discriminative words and indicative char-acter bigrams.
In line 2, we train an SVM on thewords encountered in the training data.
In lines 3and 4, we assign the Euclidean norm of the weightvector of each word as its score.
Starting in line7, we determine which character bigrams are rep-resentative of high scoring words.
In line 10, wecalculate the bigram scores.8553 ExperimentsIn this section, we describe two experiments aimedat quantifying the importance of the discriminativewords and the indicative character bigrams that areidentified by Algorithm 1.3.1 DataWe use two different NLI corpora.
We follow thesetup of Tsur and Rappoport (2007) by extractingtwo sets, denoted I1 and I2 (Table 1), from theInternational Corpus of Learner English (ICLE),Version 2 (Granger et al, 2009).
Each set con-sists of 238 documents per language, randomly se-lected from the ICLE corpus.
Each of the docu-ments corresponds to a different author, and con-tains between 500 and 1000 words.
We follow themethodology of the paper in performing 10-foldcross-validation on the sets of languages used bythe authors.For the development of the method described inSection 2, we used a different corpus, namely theTOEFL Non-Native English Corpus (Blanchard etal., 2013).
It consists of essays written by nativespeakers of eleven languages, divided into threeEnglish proficiency levels.
In order to maintainconsistency with the ICLE sets, we extracted threesets of five languages apiece (Table 1), with eachset including both related and unrelated languages:European languages that use Latin script (T1),non-European languages that use non-Latin scripts(T2), and a mixture of both types (T3).
Each sub-corpus was divided into a training set of 80%, anddevelopment and test sets of 10% each.
The train-ing sets are composed of approximately 700 docu-ments per language, with an average length of 350words per document.
There are over 5000 wordtypes per language, and over 1000 character bi-grams in total.
The test sets include approximately90 documents per language.
We report results onthe test sets, after training on both the training anddevelopment sets.3.2 SetupWe replicate the experiments of Tsur and Rap-poport (2007) by limiting the features to the 200most frequent character bigrams.1The feature val-ues are set to the frequency of the character bi-1Our development experiments suggest that using the fullset of bigrams results in a higher accuracy of a bigram-basedclassifier.
However, we limit the set of features to the 200most frequent bigrams for the sake of consistency with previ-ous work.ICLE:I1 Bulgarian Czech French Russian SpanishI2 Czech Dutch Italian Russian SpanishTOEFL:T1 French German Italian Spanish TurkishT2 Arabic Chinese Hindi Japanese TeluguT3 French German Japanese Korean TeluguTable 1: The L1 language sets.grams normalized by the length of the document.We use these feature vectors as input to the SVM-Multiclass classifier (Joachims, 1999).
The resultsare shown in the Baseline column of Table 2.3.3 Discriminative WordsThe objective of the first experiment is to quantifythe influence of the most discriminative words onthe accuracy of the bigram-based classifier.
UsingAlgorithm 1, we identify the 100 most discrimi-native words, and remove them from the trainingdata.
The bigram counts are then recalculated, andthe new 200 most frequent bigrams are used asfeatures for the character-level SVM.
Note that thenumber of the features in the classifier remains un-changed.The results are shown in the DiscriminativeWords column of Table 2.
We see a statisticallysignificant drop in the accuracy of the classifierwith respect to the baseline in all sets except T3.The words that are identified as the most discrim-inative include function words, punctuation, verycommon content words, and the toponymic terms.The 10 highest scoring words from T1 are: indeed,often, statement, : (colon), question, instance, .
.
.
(ellipsis), opinion, conclude, and however.
In ad-dition, France, Turkey, Italian, Germany, and Italyare all found among the top 70 words.For comparison, we attempt to quantify the ef-fect of removing the same number of randomly-selected words from the training data.
Specifically,we discard all tokens that correspond to 100 wordtypes that have the same or slightly higher fre-quency as the discriminative words.
The resultsare shown in the Random Words column of Ta-ble 2.
The decrease is much smaller for I1, I2, andT1, while the accuracy actually increases for T2and T3.
This illustrates the impact that the mostdiscriminative words have on the bigram-basedclassifier beyond simple reduction in the amountof the training data.856Set Baseline Random Discriminative Random IndicativeWords Words Bigrams BigramsI1 67.5 ?0.2 ?3.6 ?1.0 ?2.2I2 66.9 ?2.5 ?5.5 ?0.7 ?2.8T1 60.7 ?3.3 ?7.7 ?2.5 ?3.9T2 60.6 +0.5 ?3.8 ?1.1 ?5.9T3 62.2 +0.3 ?0.0 ?0.5 ?4.1Table 2: The impact of subsets of word types and bigram features on the accuracy of a bigram-based NLIclassifier.3.4 Indicative BigramsUsing Algorithm 1, we identify the top 20 charac-ter bigrams, and replace them with randomly se-lected bigrams.
The results of this experiment arereported in the Indicative Bigrams column of Ta-ble 2.
It is to be expected that the replacement ofany 20 of the top bigrams with 20 less useful bi-grams will result in some drop in accuracy, regard-less of which bigrams are chosen for replacement.For comparison, the Random Bigrams column ofTable 2 shows the mean accuracy over 100 trialsobtained when 20 bigrams randomly selected fromthe set of 200 bigrams are replaced with randombigrams from outside of the set.The results indicate that our algorithm indeedidentifies 20 bigrams that are on average more im-portant than the other 180 bigrams.
What is reallystriking is that the sets of 20 indicative characterbigrams overlap substantially across different sets.Table 3 shows 17 bigrams that are common acrossthe three TOEFL corpora, ordered by their score,together with some of the highly scored words inwhich they occur.
Four of the bigrams consistof punctuation marks and a space.2The remain-ing bigrams indicate function words, toponymicterms like Germany, and frequent content wordslike take and new.The situation is similar in the ICLE sets, wherelikewise 17 out of 20 bigrams are common.
Theinter-fold overlap is even greater, with 19 out of20 bigrams appearing in each of the 10 folds.
Inparticular, the bigrams fr and bu can be tracedto both the function words from and but, and thepresence of French and Bulgarian in I1.
However,the fact that the two bigrams are also on the list for2It appears that only the relatively low frequency of mostof the punctuation bigrams prevents them from dominatingthe sets of the indicative bigrams.
When using all bigramsinstead of the top 200, the majority of the indicative bigramscontain punctuation.Bigram Words,,..u you Teluguf ofny any many Germanyyo you yourw now howi Iy you yourew new knewkn know knewey they Turkeywh what why where etc.of ofak make takeTable 3: The most indicative character bigrams inthe TOEFL corpus (sorted by score).the I2 set, which does not include these languages,suggests that their importance is mostly due to thefunction words.3.5 DiscussionIn the first experiment, we showed that the re-moval of the 100 most discriminative words fromthe training data results in a significant drop in theaccuracy of the classifier that is based exclusivelyon character bigrams.
If the hypothesis of Tsurand Rappoport (2007) was true, this should not bethe case, as the phonology of L1 would influencethe choice of words across the lexicon.In the second experiment, we found that the ma-jority of the most indicative character bigrams areshared among different language sets.
The bi-grams appear to reflect primarily high-frequencyfunction words.
If the hypothesis was true, this857should not be the case, as the diverse L1 phonolo-gies would induce different sets of bigrams.
Infact, the highest scoring bigrams reflect punctu-ation patterns, which have little to do with wordchoice.4 ConclusionWe have provided experimental evidence againstthe hypothesis that the phonology of L1 stronglyaffects the choice of words in L2.
We showedthat a small set of high-frequency function wordshave disproportionate influence on the accuracy ofa bigram-based NLI classifier, and that the major-ity of the indicative bigrams appear to be indepen-dent of L1.
This suggests an alternative explana-tion of the effectiveness of a bigram-based classi-fier in identifying the native language of a writer?
that the character bigrams simply mirror differ-ences in the word usage rather than the phonologyof L1.Our explanation concurs with the findings ofDaland (2013) that unigram frequency differencesin certain types of phonological segments betweenchild-directed and adult-directed speech are due toa small number of word types, such as you, what,and want, rather than to any general phonologicalpreferences.
He argues that the relative frequencyof sounds in speech is driven by the relative fre-quency of words.
In a similar vein, Koppel et al(2005) see the usefulness of character n-grams as?simply an artifact of variable usage of particularwords, which in turn might be the result of differ-ent thematic preferences,?
or as a reflection of theL1 orthography.We conclude by noting that our experimental re-sults do not imply that the phonology of L1 has ab-solutely no influence on L2 writing.
Rather, theyshow that the evidence from the Native LanguageIdentification task has so far been inconclusive inthis regard.AcknowledgmentsWe thank the participants and the organizers ofthe shared task on NLI at the BEA8 workshop forsharing their reflections on the task.
We also thankan anonymous reviewer for pointing out the studyof Daland (2013).This research was supported by the NaturalSciences and Engineering Research Council ofCanada and the Alberta Innovates Technology Fu-tures.ReferencesAmjad Abu-Jbara, Rahul Jha, Eric Morley, andDragomir Radev.
2013.
Experimental results onthe native language identification shared task.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 82?88.Daniel Blanchard, Joel Tetreault, Derrick Higgins,Aoife Cahill, and Martin Chodorow.
2013.TOEFL11: A Corpus of Non-Native English.
Tech-nical report, Educational Testing Service.Julian Brooke and Graeme Hirst.
2013.
Using otherlearner corpora in the 2013 NLI shared task.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 188?196.Serhiy Bykh, Sowmya Vajjala, Julia Krivanek, andDetmar Meurers.
2013.
Combining shallow andlinguistically motivated features in native languageidentification.
In Proceedings of the Eighth Work-shop on Innovative Use of NLP for Building Educa-tional Applications, pages 197?206.Andrea Cimino, Felice Dell?Orletta, Giulia Venturi,and Simonetta Montemagni.
2013.
Linguistic pro-filing based on general?purpose features and na-tive language identification.
In Proceedings of theEighth Workshop on Innovative Use of NLP forBuilding Educational Applications, pages 207?215.Robert Daland.
2013.
Variation in the input: a casestudy of manner class frequencies.
Journal of ChildLanguage, 40(5):1091?1122.Vidas Daudaravicius.
2013.
VTEX system descrip-tion for the NLI 2013 shared task.
In Proceedings ofthe Eighth Workshop on Innovative Use of NLP forBuilding Educational Applications, pages 89?95.Binyam Gebrekidan Gebre, Marcos Zampieri, PeterWittenburg, and Tom Heskes.
2013.
Improving na-tive language identification with TF-IDF weighting.In Proceedings of the Eighth Workshop on Innova-tive Use of NLP for Building Educational Applica-tions, pages 216?223.Cyril Goutte, Serge L?eger, and Marine Carpuat.
2013.Feature space selection and combination for na-tive language identification.
In Proceedings of theEighth Workshop on Innovative Use of NLP forBuilding Educational Applications, pages 96?100.Sylvaine Granger, Estelle Dagneaux, Fanny Meunier,and Magali Paquot.
2009.
INTERNATIONALCORPUS OF LEARNER ENGLISH: VERSION 2.John Henderson, Guido Zarrella, Craig Pfeifer, andJohn D. Burger.
2013.
Discriminating non-nativeEnglish with 350 words.
In Proceedings of theEighth Workshop on Innovative Use of NLP forBuilding Educational Applications, pages 101?110.858Barbora Hladka, Martin Holub, and Vincent Kriz.2013.
Feature engineering in the NLI shared task2013: Charles University submission report.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 232?241.Scott Jarvis, Yves Bestgen, and Steve Pepper.
2013.Maximizing classification accuracy in native lan-guage identification.
In Proceedings of the EighthWorkshop on Innovative Use of NLP for BuildingEducational Applications, pages 111?118.Thorsten Joachims.
1999.
Making large-scale supportvector machine learning practical.
In Advances inkernel methods, pages 169?184.
MIT Press.Moshe Koppel, Jonathan Schler, and Kfir Zigdon.2005.
Determining an author?s native language bymining a text for errors.
In Proceedings of theeleventh ACM SIGKDD international conference onKnowledge discovery in data mining, pages 624?628, Chicago, IL.
ACM.Shibamouli Lahiri and Rada Mihalcea.
2013.
Using n-gram and word network features for native languageidentification.
In Proceedings of the Eighth Work-shop on Innovative Use of NLP for Building Educa-tional Applications, pages 251?259.Baoli Li.
2013.
Recognizing English learners.
na-tive language from their writings.
In Proceedings ofthe Eighth Workshop on Innovative Use of NLP forBuilding Educational Applications, pages 119?123.Andr?e Lynum.
2013.
Native language identificationusing large scale lexical features.
In Proceedings ofthe Eighth Workshop on Innovative Use of NLP forBuilding Educational Applications, pages 266?269.Shervin Malmasi, Sze-Meng Jojo Wong, and MarkDras.
2013.
NLI shared task 2013: MQ submis-sion.
In Proceedings of the Eighth Workshop on In-novative Use of NLP for Building Educational Ap-plications, pages 124?133.Tomoya Mizumoto, Yuta Hayashibe, Keisuke Sak-aguchi, Mamoru Komachi, and Yuji Matsumoto.2013.
NAIST at the NLI 2013 shared task.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 134?139.Garrett Nicolai, Bradley Hauer, Mohammad Salameh,Lei Yao, and Grzegorz Kondrak.
2013.
Cognateand misspelling features for natural language iden-tification.
In Proceedings of the Eighth Workshopon Innovative Use of NLP for Building EducationalApplications, pages 140?145.Marius Popescu and Radu Tudor Ionescu.
2013.
Thestory of the characters, the DNA and the native lan-guage.
In Proceedings of the Eighth Workshop onInnovative Use of NLP for Building Educational Ap-plications, pages 270?278.Ben Swanson.
2013.
Exploring syntactic representa-tions for native language identification.
In Proceed-ings of the Eighth Workshop on Innovative Use ofNLP for Building Educational Applications, pages146?151.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.2013.
A Report on the First Native Language Iden-tification Shared Task.
In Proceedings of the EighthWorkshop on Innovative Use of NLP for BuildingEducational Applications.Oren Tsur and Ari Rappoport.
2007.
Using Classi-fier Features for Studying the Effect of Native Lan-guage on the Choice of Written Second LanguageWords.
In Proceedings of the Workshop on Cog-nitive Aspects of Computational Language Acquisi-tion, pages 9?16, Prague, Czech Republic.Yulia Tsvetkov, Naama Twitto, Nathan Schneider,Noam Ordan, Manaal Faruqui, Victor Chahuneau,Shuly Wintner, and Chris Dyer.
2013.
Identifyingthe L1 of non-native writers: the CMU-Haifa sys-tem.
In Proceedings of the Eighth Workshop on In-novative Use of NLP for Building Educational Ap-plications, pages 279?287.859
