Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 1?28,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsFindings of the 2009 Workshop on Statistical Machine TranslationChris Callison-BurchJohns Hopkins Universityccb cs jhu eduPhilipp KoehnUniversity of Edinburghpkoehn inf ed ac ukChristof MonzUniversity of Amsterdamchristof science uva nlJosh SchroederUniversity of Edinburghj schroeder ed ac ukAbstractThis paper presents the results of theWMT09 shared tasks, which included atranslation task, a system combinationtask, and an evaluation task.
We con-ducted a large-scale manual evaluation of87 machine translation systems and 22system combination entries.
We used theranking of these systems to measure howstrongly automatic metrics correlate withhuman judgments of translation quality,for more than 20 metrics.
We present anew evaluation technique whereby systemoutput is edited and judged for correctness.1 IntroductionThis paper presents the results of the shared tasksof the 2009 EACL Workshop on Statistical Ma-chine Translation, which builds on three previ-ous workshops (Koehn and Monz, 2006; Callison-Burch et al, 2007; Callison-Burch et al, 2008).There were three shared tasks this year: a transla-tion task between English and five other Europeanlanguages, a task to combine the output of multiplemachine translation systems, and a task to predicthuman judgments of translation quality using au-tomatic evaluation metrics.
The performance oneach of these shared task was determined after acomprehensive human evaluation.There were a number of differences betweenthis year?s workshop and last year?s workshop:?
Larger training sets ?
In addition to annualincreases in the Europarl corpus, we releaseda French-English parallel corpus verging on 1billion words.
We also provided large mono-lingual training sets for better language mod-eling of the news translation task.?
Reduced number of conditions ?
Previ-ous workshops had many conditions: 10language pairs, both in-domain and out-of-domain translation, and three types of man-ual evaluation.
This year we eliminatedthe in-domain Europarl test set and definedsentence-level ranking as the primary type ofmanual evaluation.?
Editing to evaluate translation quality ?Beyond ranking the output of translation sys-tems, we evaluated translation quality by hav-ing people edit the output of systems.
Later,we asked annotators to judge whether thoseedited translations were correct when shownthe source and reference translation.The primary objectives of this workshop are toevaluate the state of the art in machine transla-tion, to disseminate common test sets and pub-lic training data with published performance num-bers, and to refine evaluation methodologies formachine translation.
All of the data, translations,and human judgments produced for our workshopare publicly available.1 We hope they form avaluable resource for research into statistical ma-chine translation, system combination, and auto-matic evaluation of translation quality.2 Overview of the shared translation andsystem combination tasksThe workshop examined translation between En-glish and five other languages: German, Spanish,French, Czech, and Hungarian.
We created a testset for each language pair by translating news-paper articles.
We additionally provided trainingdata and a baseline system.1http://statmt.org/WMT09/results.html12.1 Test dataThe test data for this year?s task was created byhiring people to translate news articles that weredrawn from a variety of sources during the pe-riod from the end of September to mid-Octoberof 2008.
A total of 136 articles were selected, inroughly equal amounts from a variety of Czech,English, French, German, Hungarian, Italian andSpanish news sites:2Hungarian: hvg.hu (10), Napi (2), MNO (4),Ne?pszabadsa?g (4)Czech: iHNed.cz (3), iDNES.cz (4), Li-dovky.cz (3), aktua?lne?.cz (2), Novinky (1)French: dernieresnouvelles (1), Le Figaro (2),Les Echos (4), Liberation (4), Le Devoir (9)Spanish: ABC.es (11), El Mundo (12)English: BBC (11), New York Times (6), Timesof London (4),German: Su?ddeutsche Zeitung (3), FrankfurterAllgemeine Zeitung (3), Spiegel (8), Welt (3)Italian: ADN Kronos (5), Affari Italiani (2),ASCA (1), Corriere della Sera (4), Il Sole 24ORE (1), Il Quotidiano (1), La Republica (8)Note that Italian translation was not one of thisyear?s official translation tasks.The translations were created by the membersof EuroMatrix consortium who hired a mix ofprofessional and non-professional translators.
Alltranslators were fluent or native speakers of bothlanguages.
Although we made efforts to proof-read all translations, many sentences still containminor errors and disfluencies.
All of the transla-tions were done directly, and not via an interme-diate language.
For instance, each of the 20 Hun-garian articles were translated directly into Czech,English, French, German, Italian and Spanish.The total cost of creating the test sets consistingof roughly 80,000 words across 3027 sentences inseven languages was approximately 31,700 euros(around 39,800 dollars at current exchange rates,or slightly more than $0.08/word).Previous evaluations additionally used test setsdrawn from the Europarl corpus.
Our rationale be-hind discontinuing the use of Europarl as a test setwas that it overly biases towards statistical systemsthat were trained on this particular domain, and2For more details see the XML test files.
The docidtag gives the source and the date for each document in thetest set, and the origlang tag indicates the original sourcelanguage.that European Parliament proceedings were less ofgeneral interest than news stories.
We focus on asingle task since the use of multiple test sets in thepast spread our resources too thin, especially in themanual evaluation.2.2 Training dataAs in past years we provided parallel corpora totrain translation models, monolingual corpora totrain language models, and development sets totune parameters.
Some statistics about the train-ing materials are given in Figure 1.109 word parallel corpusTo create the large French-English parallel cor-pus, we conducted a targeted web crawl of bilin-gual web sites.
These sites came from a variety ofsources including the Canadian government, theEuropean Union, the United Nations, and otherinternational organizations.
The crawl yielded onthe order of 40 million files, consisting of morethan 1TB of data.
Pairs of translated documentswere identified using a set of simple heuristics totransform French URLs into English URLs (for in-stance, by replacing fr with en).
Documents thatmatched were assumed to be translations of eachother.All HTML and PDF documents were convertedinto plain text, which yielded 2 million Frenchfiles paired with their English equivalents.
Textfiles were split so that they contained one sen-tence per line and had markers between para-graphs.
They were sentence-aligned in batches of10,000 document pairs, using a sentence alignerthat incorporates IBM Model 1 probabilities in ad-dition to sentence lengths (Moore, 2002).
Thedocument-aligned corpus contained 220 millionsegments with 2.9 billion words on the French sideand 215 million segments with 2.5 billion wordson the English side.
After sentence alignment,there were 177 million sentence pairs with 2.5 bil-lion French words and 2.2 billion English words.The sentence-aligned corpus was cleaned to re-move sentence pairs which consisted only of num-bers or paragraph markers, or where the Frenchand English sentences were identical.
The laterstep helped eliminate documents that were notactually translated, which was necessary becausewe did not perform language identification.
Aftercleaning, the parallel corpus contained 105 millionsentence pairs with 2 billion French words and 1.8billion English words.2Europarl Training CorpusSpanish?
English French?
English German?
EnglishSentences 1,411,589 1,428,799 1,418,115Words 40,067,498 41,042,070 44,692,992 40,067,498 39,516,645 37,431,872Distinct words 154,971 108,116 129,166 107,733 320,180 104,269News Commentary Training CorpusSpanish?
English French?
English German?
English Czech?
EnglishSentences 74,512 64,223 82,740 79,930Words 2,052,186 1,799,312 1,831,149 1,560,274 2,051,369 1,977,200 1,733,865 1,891,559Distinct words 56,578 41,592 46,056 38,821 92,313 43,383 105,280 41,801109 Word Parallel CorpusFrench?
EnglishSentences 22,520,400Words 811,203,407 668,412,817Distinct words 2,738,882 2,861,836Hunglish Training Corpus CzEng Training CorpusHungarian?
EnglishSentences 1,517,584Words 26,114,985 31,467,693Distinct words 717,198 192,901Czech?
EnglishSentences 1,096,940Words 15,336,783 17,909,979Distinct words 339,683 129,176Europarl Language Model DataEnglish Spanish French GermanSentence 1,658,841 1,607,419 1,676,435 1,713,715Words 44,983,136 45,382,287 50,577,097 41,457,414Distinct words 117,577 162,604 138,621 348,197News Language Model DataEnglish Spanish French German Czech HungarianSentence 21,232,163 1,626,538 6,722,485 10,193,376 5,116,211 4,209,121Words 504,094,159 48,392,418 167,204,556 185,639,915 81,743,223 86,538,513Distinct words 1,141,895 358,664 660,123 1,668,387 929,318 1,313,578News Test SetEnglish Spanish French German Czech Hungarian ItalianSentences 2525Words 65,595 68,092 72,554 62,699 55,389 54,464 64,906Distinct words 8,907 10,631 10,609 12,277 15,387 16,167 11,046News System Combination Development SetEnglish Spanish French German Czech Hungarian ItalianSentences 502Words 11,843 12,499 12,988 11,235 9,997 9,628 11,833Distinct words 2,940 3,176 3,202 3,471 4,121 4,133 3,318Figure 1: Statistics for the training and test sets used in the translation task.
The number of words isbased on the provided tokenizer and the number of distinct words is the based on lowercased tokens.3In addition to cleaning the sentence-aligned par-allel corpus we also de-duplicated the corpus, re-moving all sentence pairs that occured more thanonce in the parallel corpus.
Many of the docu-ments gathered in our web crawl were duplicatesor near duplicates, and a lot of the text is repeated,as with web site navigation.
We further elimi-nated sentence pairs that varied from previous sen-tences by only numbers, which helped eliminatetemplate web pages such as expense reports.
Weused a Bloom Filter (Talbot and Osborne, 2007) todo de-duplication, so it may have discarded moresentence pairs than strictly necessary.
After de-duplication, the parallel corpus contained 28 mil-lion sentence pairs with 0.8 billion French wordsand 0.7 billion English words.Monolingual news corporaWe have crawled the news sources that were thebasis of our test sets (and a few more additionalsources) since August 2007.
This allowed us toassemble large corpora in the target domain to bemainly used as training data for language mod-eling.
We collected texts from the beginning ofour data collection period to one month before thetest set period, segmented these into sentences andrandomized the order of the sentences to obviatecopyright concerns.2.3 Baseline systemTo lower the barrier of entry for newcomers to thefield, we provided Moses, an open source toolkitfor phrase-based statistical translation (Koehn etal., 2007).
The performance of this baseline sys-tem is similar to the best submissions in last year?sshared task.
Twelve participating groups used theMoses toolkit for the development of their system.2.4 Submitted systemsWe received submissions from 22 groups from20 institutions, as listed in Table 1, a similarturnout to last year?s shared task.
Of the 20groups that participated with regular system sub-missions in last year?s shared task, 12 groups re-turned this year.
A major hurdle for many wasa DARPA/GALE evaluation that occurred at thesame time as this shared task.We also evaluated 7 commercial rule-based MTsystems, and Google?s online statistical machinetranslation system.
We note that Google did notsubmit an entry itself.
Its entry was created bythe WMT09 organizers using Google?s online sys-tem.3 In personal correspondence, Franz Ochclarified that the online system is different fromGoogle?s research system in that it runs at fasterspeeds at the expense of somewhat lower transla-tion quality.
On the other hand, the training dataused by Google is unconstrained, which meansthat it may have an advantage compared to the re-search systems evaluated in this workshop, sincethey were trained using only the provided materi-als.2.5 System combinationIn total, we received 87 primary system submis-sions along with 42 secondary submissions.
Thesewere made available to participants in the sys-tem combination shared task.
Based on feedbackthat we received on last year?s system combina-tion task, we provided two additional resources toparticipants:?
Development set: We reserved 25 articlesto use as a dev set for system combina-tion (details of the set are given in Table1).
These were translated by all participatingsites, and distributed to system combinationparticipants along with reference translations.?
n-best translations: We requested n-bestlists from sites whose systems could producethem.
We received 25 100-best lists accom-panying the primary system submissions, and5 accompanying the secondary system sub-missions.In addition to soliciting system combination en-tries for each of the language pairs, we treated sys-tem combination as a way of doing multi-sourcetranslation, following Schroeder et al (2009).
Forthe multi-source system combination task, we pro-vided all 46 primary system submissions from anylanguage into English, along with an additional 32secondary systems.Table 2 lists the six participants in the systemcombination task.3 Human evaluationAs with past workshops, we placed greater em-phasis on the human evaluation than on the auto-matic evaluation metric scores.
It is our contention3http://translate.google.com4ID ParticipantCMU-STATXFER Carnegie Mellon University?s statistical transfer system (Hanneman et al, 2009)COLUMBIA Columbia University (Carpuat, 2009)CU-BOJAR Charles University Bojar (Bojar et al, 2009)CU-TECTOMT Charles University Tectogramatical MT (Bojar et al, 2009)DCU Dublin City University (Du et al, 2009)EUROTRANXP commercial MT provider from the Czech RepublicGENEVA University of Geneva (Wehrli et al, 2009)GOOGLE Google?s production systemJHU Johns Hopkins University (Li et al, 2009)JHU-TROMBLE Johns Hopkins University Tromble (Eisner and Tromble, 2006)LIMSI LIMSI (Allauzen et al, 2009)LIU Linko?ping University (Holmqvist et al, 2009)LIUM-SYSTRAN University of Le Mans / Systran (Schwenk et al, 2009)MORPHO Morphologic (Nova?k, 2009)NICT National Institute of Information and Comm.
Tech., Japan (Paul et al, 2009)NUS National University of Singapore (Nakov and Ng, 2009)PCTRANS commercial MT provider from the Czech RepublicRBMT1-5 commercial systems from Learnout&Houspie, Lingenio, Lucy, PROMT, SDLRWTH RWTH Aachen (Popovic et al, 2009)STUTTGART University of Stuttgart (Fraser, 2009)SYSTRAN Systran (Dugast et al, 2009)TALP-UPC Universitat Politecnica de Catalunya, Barcelona (R. Fonollosa et al, 2009)UEDIN University of Edinburgh (Koehn and Haddow, 2009)UKA University of Karlsruhe (Niehues et al, 2009)UMD University of Maryland (Dyer et al, 2009)USAAR University of Saarland (Federmann et al, 2009)Table 1: Participants in the shared translation task.
Not all groups participated in all language pairs.ID ParticipantBBN-COMBO BBN system combination (Rosti et al, 2009)CMU-COMBO Carnegie Mellon University system combination (Heafield et al, 2009)CMU-COMBO-HYPOSEL CMU system comb.
with hyp.
selection (Hildebrand and Vogel, 2009)DCU-COMBO Dublin City University system combinationRWTH-COMBO RWTH Aachen system combination (Leusch et al, 2009)USAAR-COMBO University of Saarland system combination (Chen et al, 2009)Table 2: Participants in the system combination task.5Language Pair Sentence Ranking Edited Translations Yes/No JudgmentsGerman-English 3,736 1,271 4,361English-German 3,700 823 3,854Spanish-English 2,412 844 2,599English-Spanish 1,878 278 837French-English 3,920 1,145 4,491English-French 1,968 332 1,331Czech-English 1,590 565 1,071English-Czech 7,121 2,166 9,460Hungarian-English 1,426 554 1,309All-English 4,807 0 0Multisource-English 2,919 647 2184Totals 35,786 8,655 31,524Table 3: The number of items that were judged for each task during the manual evaluation.that automatic measures are an imperfect substi-tute for human assessment of translation quality.Therefore, we define the manual evaluation to beprimary, and use the human judgments to validateautomatic metrics.Manual evaluation is time consuming, and it re-quires a large effort to conduct it on the scale ofour workshop.
We distributed the workload acrossa number of people, including shared-task partic-ipants, interested volunteers, and a small numberof paid annotators.
More than 160 people partic-ipated in the manual evaluation, with 100 peopleputting in more than an hour?s worth of effort, and30 putting in more than four hours.
A collectivetotal of 479 hours of labor was invested.We asked people to evaluate the systems?
outputin two different ways:?
Ranking translated sentences relative to eachother.
This was our official determinant oftranslation quality.?
Editing the output of systems without dis-playing the source or a reference translation,and then later judging whether edited transla-tions were correct.The total number of judgments collected for thedifferent modes of annotation is given in Table 3.In all cases, the output of the various translationoutputs were judged on equal footing; the outputof system combinations was judged alongside thatof the individual system, and the constrained andunconstrained systems were judged together.3.1 Ranking translations of sentencesRanking translations relative to each other is a rea-sonably intuitive task.
We therefore kept the in-structions simple:Rank translations from Best to Worst rel-ative to the other choices (ties are al-lowed).In our the manual evaluation, annotators wereshown at most five translations at a time.
For mostlanguage pairs there were more than 5 systemssubmissions.
We did not attempt to get a com-plete ordering over the systems, and instead reliedon random selection and a reasonably large samplesize to make the comparisons fair.Relative ranking is our official evaluation met-ric.
Individual systems and system combinationsare ranked based on how frequently they werejudged to be better than or equal to any other sys-tem.
The results of this are reported in Section 4.Appendix A provides detailed tables that containpairwise comparisons between systems.3.2 Editing machine translation outputWe experimented with a new type of evaluationthis year where we asked judges to edit the outputof MT systems.
We did not show judges the refer-ence translation, which makes our edit-based eval-uation different than the Human-targeted Trans-lation Error Rate (HTER) measure used in theDARPA GALE program (NIST, 2008).
Ratherthan asking people to make the minimum numberof changes to the MT output in order capture thesame meaning as the reference, we asked them to6edit the translation to be as fluent as possible with-out seeing the reference.
Our hope was that thiswould reflect people?s understanding of the out-put.The instructions that we gave our judges werethe following:Correct the translation displayed, mak-ing it as fluent as possible.
If no correc-tions are needed, select ?No correctionsneeded.?
If you cannot understand thesentence well enough to correct it, select?Unable to correct.
?Each translated sentence was shown in isolationwithout any additional context.
A screenshot isshown in Figure 2.Since we wanted to prevent judges from see-ing the reference before editing the translations,we split the test set between the sentences usedin the ranking task and the editing task (becausethey were being conducted concurrently).
More-over, annotators edited only a single system?s out-put for one source sentence to ensure that their un-derstanding of it would not be influenced by an-other system?s output.3.3 Judging the acceptability of edited outputHalfway through the manual evaluation period, westopped collecting edited translations, and insteadasked annotators to do the following:Indicate whether the edited transla-tions represent fully fluent and meaning-equivalent alternatives to the referencesentence.
The reference is shown withcontext, the actual sentence is bold.In addition to edited translations, unedited itemsthat were either marked as acceptable or as incom-prehensible were also shown.
Judges gave a sim-ple yes/no indication to each item.
A screenshot isshown in Figure 3.3.4 Inter- and Intra-annotator agreementIn order to measure intra-annotator agreement10% of the items were repeated and evaluatedtwice by each judge.
In order to measure inter-annotator agreement 40% of the items were ran-domly drawn from a common pool that was sharedacross all annotators so that we would have itemsthat were judged by multiple annotators.INTER-ANNOTATOR AGREEMENTEvaluation type P (A) P (E) KSentence ranking .549 .333 .323Yes/no to edited output .774 .5 .549INTRA-ANNOTATOR AGREEMENTEvaluation type P (A) P (E) KSentence ranking .707 .333 .561Yes/no to edited output .866 .5 .732Table 4: Inter- and intra-annotator agreement forthe two types of manual evaluationWe measured pairwise agreement among anno-tators using the kappa coefficient (K) which is de-fined asK =P (A)?
P (E)1?
P (E)where P (A) is the proportion of times that the an-notators agree, and P (E) is the proportion of timethat they would agree by chance.For inter-annotator agreement we calculatedP (A) for the yes/no judgments by examining allitems that were annotated by two or more anno-tators, and calculating the proportion of time theyassigned identical scores to the same items.
Forthe ranking tasks we calculated P (A) by examin-ing all pairs of systems which had been judged bytwo or more judges, and calculated the proportionof time that they agreed that A > B, A = B, orA < B. Intra-annotator agreement was computedsimilarly, but we gathered items that were anno-tated on multiple occasions by a single annotator.Table 4 gives K values for inter-annotator andintra-annotator agreement.
These give an indi-cation of how often different judges agree, andhow often single judges are consistent for repeatedjudgments, respectively.
The interpretation ofKappa varies, but according to Landis and Koch(1977), 0 ?
.2 is slight, .2 ?
.4 is fair, .4 ?
.6 ismoderate, .6?
.8 is substantial and the rest almostperfect.Based on these interpretations the agreement foryes/no judgments is moderate for inter-annotatoragreement and substantial for intra-annotatoragreement, but the inter-annotator agreement forsentence level ranking is only fair.We analyzed two possible strategies for improv-ing inter-annotator agreement on the ranking task:First, we tried discarding initial judgments to give7Edit MT OutputYou have judged 19 sentences for WMT09 Multisource-English News Editing, 468 sentences total taking 74.4 seconds per sentence.Original: They are often linked to other alterations sleep as nightmares, night terrors, the nocturnal enuresis (pee in bed) or the sleepwalking, but it is notalways the case.Edit:Reset EditEdited.No corrections needed.Unable to correct.Annotator: ccb Task: WMT09 Multisource-English News EditingInstructions:Correct the translation displayed, making it as fluent as possble.
If no corrections are needed, select "No corrections needed."
If you cannot understandthe sentence well enough to correct it, select "Unable to correct.
"They are often linked to other sleep disorders, such as nightmares, night terrors, the nocturnal enuresis (bedwetting) or sleepwalking, but this isnot always the case.http://www.statmt.org/wmt09/judge/do_task.phpWMT09 Manual EvaluationFigure 2: This screenshot shows an annotator editing the output of a machine translation system.http://www.statmt.org/wmt09/judge/do_task.phpWMT09 Manual EvaluationJudge Edited MT OutputYou have judged 84 sentences for WMT09 French-English News Edit Acceptance, 459 sentences total taking 64.9 seconds per sentence.Source: Au m?me moment, les gouvernements belges, hollandais et luxembourgeois ont en parti nationalis?
le conglom?rat europ?en financier, Fortis.Les analystes de Barclays Capital ont d?clar?
que les n?gociations fr?n?tiques de ce week end, conclues avec l'accord de sauvetage" semblent ne pas avoirr?ussi ?
faire revivre le march?
".Alors que la situation ?conomique se d?t?riorasse, la demande en mati?res premi?res, p?trole inclus, devrait se ralentir.
"la prospective d'?quit?
globale, de taux d'int?r?t et d'?change des march?s, est devenue incertaine" ont ?crit les analystes de Deutsche Bank dans unelettre ?
leurs investisseurs.
""nous pensons que les mati?res premi?res ne pourront ?chapper ?
cette contagion.Reference: Meanwhile, the Belgian, Dutch and Luxembourg governments partially nationalized the European financial conglomerate Fortis.Analysts at Barclays Capital said the frantic weekend negotiations that led to the bailout agreement "appear to have failed to revive market sentiment.
"As the economic situation deteriorates, the demand for commodities, including oil, is expected to slow down.
"The outlook for global equity, interest rate and exchange rate markets has become increasingly uncertain," analysts at Deutsche Bank wrote in a note toinvestors.
"We believe commodities will be unable to escape the contagion.Translation VerdictWhile the economic situation is deteriorating, demand for commodities, including oil, should decrease.Yes NoWhile the economic situation is deteriorating, the demand for raw materials, including oil, should slow down.Yes NoAlors que the economic situation deteriorated, the request in rawmaterial enclosed, oil, would have to slow down.Yes  NoWhile the financial situation damaged itself, the first matters affected, oil included, should slow down themselves.Yes  NoWhile the economic situation is depressed, demand for raw materials, including oil, will be slow.Yes NoAnnotator: ccb Task: WMT09 French-English News Edit AcceptanceInstructions:Indicate whether the edited translations represent fully fluent and meaning-equivalent alternatives to the reference sentence.The reference is shown with context, the actual sentence is bold.Figure 3: This screenshot shows an annotator judging the acceptability of edited translations.80.320.330.340.350.360.370.380.390.400.410.10.20.30.40.50.60.70.80.91.00.510.520.530.540.550.560.570.580.590.600.61Inter-annotator agreementIntra-annotator agreementProportion of judgments retainedFigure 4: The effect of discarding every annota-tors?
initial judgments, up to the first 50 items0.320.330.340.350.360.370.380.390.400.410.10.20.30.40.50.60.70.80.91.00.510.520.530.540.550.560.570.580.590.600.61Inter-annotator agreementIntra-annotator agreementProportion of judgments retainedFigure 5: The effect of removing annotators withthe lowest agreement, disregarding up to 40 anno-tators9annotators a chance to learn to how to performthe task.
Second, we tried disregarding annota-tors who have very low agreement with others, bythrowing away judgments for the annotators withthe lowest judgments.Figures 4 and 5 show how the K values im-prove for intra- and inter-annotator agreement un-der these two strategies, and what percentage ofthe judgments are retained as more annotators areremoved, or as the initial learning period is madelonger.
It seems that the strategy of removing theworst annotators is the best in terms of improv-ing inter-annotator K, while retaining most of thejudgments.
If we remove the 33 judges with theworst agreement, we increase the inter-annotatorK from fair to moderate, and still retain 60% ofthe data.For the results presented in the rest of the paper,we retain all judgments.4 Translation task resultsWe used the results of the manual evaluation toanalyze the translation quality of the different sys-tems that were submitted to the workshop.
In ouranalysis, we aimed to address the following ques-tions:?
Which systems produced the best translationquality for each language pair??
Did the system combinations produce bettertranslations than individual systems??
Which of the systems that used only the pro-vided training materials produced the besttranslation quality?Table 6 shows best individual systems.
We de-fine the best systems as those which had no othersystem that was statistically significantly betterthan them under the Sign Test at p ?
0.1.4 Multi-ple systems are listed for many language pairs be-cause it was not possible to draw a statistically sig-nificant difference between the systems.
Commer-cial translation software (including Google, Sys-tran, Morphologic, PCTrans, Eurotran XP, andanonymized RBMT providers) did well in each ofthe language pairs.
Research systems that utilized4In one case this definition meant that the system that wasranked the highest overall was not considered to be one ofthe best systems.
For German-English translation RBMT5was ranked highest overall, but was statistically significantlyworse than RBMT2.only the provided data did as well as commercialvendors in half of the language pairs.The table also lists the best systems amongthose which used only the provided materials.To determine this decision we excluded uncon-strained systems which employed significant ex-ternal resources.
Specifically, we ruled out all ofthe commercial systems, since Google has accessto significantly greater data sources for its statisti-cal system, and since the commercial RBMT sys-tems utilize knowledge sources not available toother workshop participants.
The remaining sys-tems were research systems that employ statisti-cal models.
We were able to draw distinctionsbetween half of these for each of the languagepairs.
There are some borderline cases, for in-stance LIMSI only used additional monolingualtraining resources, and LIUM/Systran used addi-tional translation dictionaries as well as additionalmonolingual resources.Table 5 summarizes the performance of thesystem combination entries by listing the bestranked combinations, and by indicating whetherthey have a statistically significant difference withthe best individual systems.
In general, systemcombinations performed as well as the best indi-vidual systems, but not statistically significantlybetter than them.
Moreover, it was hard to drawa distinction between the different system combi-nation strategies themselves.
There are a numberof possibilities as to why we failed to find signifi-cant differences:?
The number of judgments that we collectedwere not sufficient to find a difference.
Al-though we collected several thousand judg-ments for each language pair, most pairs ofsystems were judged together fewer than 100times.?
It is possible that the best performing indi-vidual systems were sufficiently better thanthe other systems and that it is difficult to im-prove on them by combining them.?
Individual systems could have been weightedincorrectly during the development stage,which could happen if the automatic evalu-ation metrics scores on the dev set did notstrongly correlate with human judgments.?
The lack of distinction between differentcombinations could be due to the fact that10Language Pair Best system combinations Entries Significantly different thanbest individual systems?German-English RWTH-COMBO, BBN-COMBO,CMU-COMBO, USAAR-COMBO5 BBN-COMBO>GOOGLE, SYSTRAN,USAAR-COMBO<RMBT2,no difference for othersEnglish-German USAAR-COMBO 1 worse than 3 best systemsSpanish-English CMU-COMBO, USAAR-COMBO,BBN-COMBO3 each better than one of the RBMTsystems, but there was no differencewith GOOGLE, TALP-UPCEnglish-Spanish USAAR-COMBO 1 no differenceFrench-English CMU-COMBO-HYPOSEL,DCU-COMBO, CMU-COMBO5 no differenceEnglish-French USAAR-COMBO, DCU-COMBO 2 USAAR-COMBO>UKA,DCU-COMBO>SYSTRAN, LIMSI,no difference with othersCzech-English CMU-COMBO 2 no differenceHungarian-English CMU-COMBO-HYPOSEL,CMU-COMBO3 both worse than MORPHOMultisource-English RWTH-COMBO 3 n/aTable 5: A comparison between the best system combinations and the best individual systems.
It wasgenerally difficult to draw a statistically significant differences between the two groups, and between thecombinations themselves.there is significant overlap in the strategiesthat they employ.Improved system combination warrants further in-vestigation.
We would suggest collecting addi-tional judgments, and doing oracle experimentswhere the contributions of individual systems areweighted according to human judgments of theirquality.UnderstandabilityOur hope is that judging the acceptability of editedoutput as discussed in Section 3 gives some indi-cation of how often a system?s output was under-standable.
Figure 6 gives the percentage of timesthat each system?s edited output was judged tobe acceptable (the percentage also factors in in-stances when judges were unable to improve theoutput because it was incomprehensible).The edited output of the best perform-ing systems under this evaluation model weredeemed acceptable around 50% of the timefor French-English, English-French, English-Spanish, German-English, and English-German.For Spanish-English the edited output of the bestsystem was acceptable around 40% of the time, forEnglish-Czech it was 30% and for Czech-Englishand Hungarian-English it was around 20%.This style of manual evaluation is experimentaland should not be taken to be authoritative.
Somecaveats about this measure:?
Editing translations without context is diffi-cult, so the acceptability rate is probably anunderestimate of how understandable a sys-tem actually is.?
There are several sources of variance that aredifficult to control for: some people are betterat editing, and some sentences are more dif-ficult to edit.
Therefore, variance in the un-derstandability of systems is difficult to pindown.?
The acceptability measure does not stronglycorrelate with the more established method ofranking translations relative to each other forall the language pairs.5Please also note that the number of correctedtranslations per system are very low for somelanguage pairs, as low as 23 corrected sentencesper system for the language pair English?French.5The Spearman rank correlation coefficients for how thetwo types of manual evaluation rank systems are .67 for de-en, .67 for fr-en, .06 for es-en, .50 for cz-en, .36 for hu-en,.65 for en-de, .02 for en-fr, -.6 for en-es, and .94 for en-cz.11French?English625?836 judgments per systemSystem C?
?othersGOOGLE ?
no .76DCU ?
yes .66LIMSI ?
no .65JHU ?
yes .62UEDIN ?
yes .61UKA yes .61LIUM-SYSTRAN no .60RBMT5 no .59CMU-STATXFER ?
yes .58RBMT1 no .56USAAR no .55RBMT3 no .54RWTH ?
yes .52COLUMBIA yes .50RBMT4 no .47GENEVA no .34English?French422?517 judgments per systemSystem C?
?othersLIUM-SYSTRAN ?
no .73GOOGLE ?
no .68UKA ??
yes .66SYSTRAN ?
no .65RBMT3 ?
no .65DCU ??
yes .65LIMSI ?
no .64UEDIN ?
yes .60RBMT4 no .59RWTH yes .58RBMT5 no .57RBMT1 no .54USAAR no .48GENEVA no .38Hungarian?English865?988 judgments per systemSystem C?
?othersMORPHO ?
no .75UMD ?
yes .66UEDIN yes .45German?English651?867 judgments per systemSystem C?
?othersRBMT5 no .66USAAR ?
no .65GOOGLE ?
no .65RBMT2 ?
no .64RBMT3 no .64RBMT4 no .62STUTTGART ??
yes .61SYSTRAN ?
no .60UEDIN ?
yes .59UKA ?
yes .58UMD ?
yes .56RBMT1 no .54LIU ?
yes .50RWTH yes .50GENEVA no .33JHU-TROMBLE yes .13English?German977?1226 judgments per systemSystem C?
?othersRBMT2 ?
no .66RBMT3 ?
no .64RBMT5 ?
no .64USAAR no .58RBMT4 no .58RBMT1 no .57GOOGLE no .54UKA ?
yes .54UEDIN ?
yes .51LIU ?
yes .49RWTH ?
yes .48STUTTGART yes .43Czech?English1257?1263 judgments per systemSystem C?
?othersGOOGLE ?
no .75UEDIN ?
yes .57CU-BOJAR ?
yes .51Spanish?English613?801 judgments per systemSystem C?
?othersGOOGLE ?
no .70TALP-UPC ??
yes .59UEDIN ?
yes .56RBMT1 ?
no .55RBMT3 ?
no .55RBMT5 ?
no .55RBMT4 ?
no .53RWTH ?
yes .51USAAR no .51NICT yes .37English?Spanish632?746 judgments per systemSystem C?
?othersRBMT3 ?
no .66UEDIN ??
yes .66GOOGLE ?
no .65RBMT5 ?
no .64RBMT4 no .61NUS ?
yes .59TALP-UPC yes .58RWTH yes .51RBMT1 no .25USAAR no .48English?Czech4626?4784 judgments per systemSystem C?
?othersPCTRANS ?
no .67EUROTRANXP ?
no .67GOOGLE no .66CU-BOJAR ?
yes .61UEDIN yes .53CU-TECTOMT yes .48Systems are listed in the order of how often their translations were ranked higher than or equal to anyother system.
Ties are broken by direct comparison.C?
indicates constrained condition, meaning only using the supplied training data and possibly standardmonolingual linguistic tools (but no additional corpora).?
indicates a win in the category, meaning that no other system is statistically significantly better atp-level?0.1 in pairwise comparison.?
indicates a constrained win, no other constrained system is statistically better.For all pairwise comparisons between systems, please check the appendix.Table 6: Official results for the WMT09 translation task, based on the human evaluation (ranking trans-lations relative to each other)12Given these low numbers, the numbers presentedin Figure 6 should not be read as comparisons be-tween systems, but rather viewed as indicating thestate of machine translation for different languagepairs.5 Shared evaluation task overviewIn addition to allowing us to analyze the transla-tion quality of different systems, the data gath-ered during the manual evaluation is useful forvalidating the automatic evaluation metrics.
Lastyear, NIST began running a similar ?Metricsfor MAchine TRanslation?
challenge (Metrics-MATR), and presented their findings at a work-shop at AMTA (Przybocki et al, 2008).In this year?s shared task we evaluated a numberof different automatic metrics:?
Bleu (Papineni et al, 2002)?Bleu remainsthe de facto standard in machine translationevaluation.
It calculates n-gram precision anda brevity penalty, and can make use of multi-ple reference translations as a way of captur-ing some of the allowable variation in trans-lation.
We use a single reference translationin our experiments.?
Meteor (Agarwal and Lavie, 2008)?Meteormeasures precision and recall for unigramsand applies a fragmentation penalty.
It usesflexible word matching based on stemmingand WordNet-synonymy.
meteor-ranking isoptimized for correlation with ranking judg-ments.?
Translation Error Rate (Snover et al,2006)?TER calculates the number of ed-its required to change a hypothesis transla-tion into a reference translation.
The possi-ble edits in TER include insertion, deletion,and substitution of single words, and an editwhich moves sequences of contiguous words.Two variants of TER are also included: TERp(Snover et al, 2009), a new version which in-troduces a number of different features, and(Bleu ?
TER)/2, a combination of Bleu andTranslation Edit Rate.?
MaxSim (Chan and Ng, 2008)?MaxSimcalculates a similarity score by comparingitems in the translation against the reference.Unlike most metrics which do strict match-ing, MaxSim computes a similarity scorefor non-identical items.
To find a maxi-mum weight matching that matches each sys-tem item to at most one reference item, theitems are then modeled as nodes in a bipar-tite graph.?
wcd6p4er (Leusch and Ney, 2008)?a mea-sure based on cder with word-based substitu-tion costs.
Leusch and Ney (2008) also sub-mitted two contrastive metrics: bleusp4114,a modified version of BLEU-S (Lin andOch, 2004), with tuned n-gram weights, andbleusp, with constant weights.
wcd6p4eris an error measure and bleusp is a qualityscore.?
RTE (Pado et al, 2009)?The RTE metricfollows a semantic approach which appliesrecent work in rich textual entailment to theproblem of MT evaluation.
Its predictions arebased on a regression model over a featureset adapted from an entailment systems.
Thefeatures primarily model alignment qualityand (mis-)matches of syntactic and semanticstructures.?
ULC (Gime?nez and Ma`rquez, 2008)?ULCis an arithmetic mean over other automaticmetrics.
The set of metrics used includeRouge, Meteor, measures of overlap betweenconstituent parses, dependency parses, se-mantic roles, and discourse representations.The ULC metric had the strongest correlationwith human judgments in WMT08 (Callison-Burch et al, 2008).?
wpF and wpBleu (Popovic and Ney, 2009) -These metrics are based on words and part ofspeech sequences.
wpF is an n-gram based F-measure which takes into account both wordn-grams and part of speech n-grams.
wp-BLEU is a combnination of the normal Bluescore and a part of speech-based Bleu score.?
SemPOS (Kos and Bojar, 2009) ?
the Sem-POS metric computes overlapping words, asdefined in (Gime?nez and Ma`rquez, 2007),with respect to their semantic part of speech.Moreover, it does not use the surface repre-sentation of words but their underlying formsobtained from the TectoMT framework.13refbbn-cgooglecmu-combocu-bojaruedin0.140.160.180.230.250.98Czech-Englishrefgooglecmu-statxbbn-combocolumbiacmu-combocmu-combo-hypdcu-comboukarwthrbmt4limsirbmt1lium-systrnusaar-combodcuuedinjhurbmt3rbmt5usaargeneva0.210.280.280.280.290.300.310.330.330.340.340.340.350.380.390.400.410.410.470.500.520.85French-Englishrefrwth-ccmu-combobbn-comborbmt5usaar-cgooglecmu-cmb-husaarrbmt3stuttgartrbmt4rbmt1ukarwthumduedinrbmt2systranliugenevajhu-tromble0.030.060.200.210.250.260.260.270.280.300.300.310.310.320.330.340.350.360.370.410.470.83German-Englishrefusaar-cgooglerbmt5rwthtalp-upcuedinusaarrbmt3nusrbmt4rbmt10.080.100.190.210.270.270.280.320.330.380.520.69English-Spanishrefusaar-cgooglerbmt5rbmt3bbn-cmbuedintalp-upcrbmt1rwthcmu-crbmt4nictusaar0.230.260.270.280.280.280.280.310.310.360.370.380.410.88Spanish-Englishrefgooglepctranseurotranxpuedincu-bojarcu-tectomt0.190.210.230.260.320.320.91English-Czechrefgoogleukalimsiusaar-crbmt3rbmt5uedinusaarrbmt4lium-sysrwthdcu-cmbdcusystranrbmt1geneva0.080.100.220.270.300.310.320.340.370.400.400.430.440.450.480.490.79English-Frenchrefmorphocmu-cmb-hcmu-comboumduedinbbn-cmb0.110.120.150.190.210.220.93Hungarian-Englishrefrbmt5rbmt3googlerbmt1rbmt2usaarusaar-cmbrbmt4rwthukauedinstuttgartliu0.120.180.190.260.280.310.310.320.330.350.370.420.470.85English-Germanrefrwthbbncmu0.250.270.320.90Multsource-EnglishFigure 6: The percent of time that each system?s edited output was judged to be an acceptable translation.These numbers also include judgments of the system?s output when it was marked either incomprehen-sible or acceptable and left unedited.
Note that the reference translation was edited alongside the systemoutputs.
Error bars show one positive and one negative standard deviation for the systems in that lan-guage pair.145.1 Measuring system-level correlationWe measured the correlation of the automatic met-rics with the human judgments of translation qual-ity at the system-level using Spearman?s rank cor-relation coefficient ?.
We converted the raw scoresassigned to each system into ranks.
We assigneda human ranking to the systems based on the per-cent of time that their translations were judged tobe better than or equal to the translations of anyother system in the manual evaluation.When there are no ties ?
can be calculated usingthe simplified equation:?
= 1?6?d2in(n2 ?
1)where di is the difference between the rank forsystemi and n is the number of systems.
The pos-sible values of ?
range between 1 (where all sys-tems are ranked in the same order) and ?1 (wherethe systems are ranked in the reverse order).
Thusan automatic evaluation metric with a higher abso-lute value for ?
is making predictions that are moresimilar to the human judgments than an automaticevaluation metric with a lower absolute ?.5.2 Measuring sentence-level consistencyBecause the sentence-level judgments collectedin the manual evaluation are relative judgmentsrather than absolute judgments, it is not possi-ble for us to measure correlation at the sentence-level in the same way that previous work has done(Kulesza and Shieber, 2004; Albrecht and Hwa,2007a; Albrecht and Hwa, 2007b).Rather than calculating a correlation coefficientat the sentence-level we instead ascertained howconsistent the automatic metrics were with the hu-man judgments.
The way that we calculated con-sistency was the following: for every pairwisecomparison of two systems on a single sentence bya person, we counted the automatic metric as beingconsistent if the relative scores were the same (i.e.the metric assigned a higher score to the higherranked system).
We divided this by the total num-ber of pairwise comparisons to get a percentage.Because the systems generally assign real num-bers as scores, we excluded pairs that the humanannotators ranked as ties.de-en(21systems)fr-en(21systems)es-en(13systems)cz-en(5systems)hu-en(6systems)Averageulc .78 .92 .86 1 .6 .83maxsim .76 .91 .98 .7 .66 .8rte (absolute) .64 .91 .96 .6 .83 .79meteor-rank .64 .93 .96 .7 .54 .75rte (pairwise) .76 .59 .78 .8 .83 .75terp -.72 -.89 -.94 -.7 -.37 -.72meteor-0.6 .56 .93 .87 .7 .54 .72meteor-0.7 .55 .93 .86 .7 .26 .66bleu-ter/2 .38 .88 .78 .9 -.03 .58nist .41 .87 .75 .9 -.14 .56wpF .42 .87 .82 1 -.31 .56ter -.43 -.83 -.84 -.6 -.01 -.54nist (cased) .42 .83 .75 1 -.31 .54bleu .41 .88 .79 .6 -.14 .51bleusp .39 .88 .78 .6 -.09 .51bleusp4114 .39 .89 .78 .6 -.26 .48bleu (cased) .4 .86 .8 .6 -.31 .47wpbleu .43 .86 .8 .7 -.49 .46wcd6p4er -.41 -.89 -.76 -.6 .43 -.45Table 7: The system-level correlation of the au-tomatic evaluation metrics with the human judg-ments for translation into English.en-de(13systems)en-fr(16systems)en-es(11systems)en-cz(5systems)Averageterp .03 -.89 -.58 -.4 -.46ter -.03 -.78 -.5 -.1 -.35bleusp4114 -.3 .88 .51 .1 .3bleusp -.3 .87 .51 .1 .29bleu -.43 .87 .36 .3 .27bleu (cased) -.45 .87 .35 .3 .27bleu-ter/2 -.37 .87 .44 .1 .26wcd6p4er .54 -.89 -.45 -.1 -.22nist (cased) -.47 .84 .35 .1 .2nist -.52 .87 .23 .1 .17wpF -.06 .9 .58 n/a n/awpbleu .07 .92 .63 n/a n/aTable 8: The system-level correlation of the au-tomatic evaluation metrics with the human judg-ments for translation out of English.15SemPOS .4 BLEUtecto .3Meteor .4 BLEU .3GTM(e=0.5)tecto .4 NISTlemma .1GTM(e=0.5)lemma .4 NIST .1GTM(e=0.5) .4 BLEUlemma .1WERtecto .3 WERlemma -.1TERtecto .3 WER -.1PERtecto .3 TERlemma -.1F-measuretecto .3 TER -.1F-measurelemma .3 PERlemma -.1F-measure .3 PER -.1NISTtecto -.3Table 9: The system-level correlation for auto-matic metrics ranking five English-Czech systems6 Evaluation task results6.1 System-level correlationTable 7 shows the correlation of automatic met-rics when they rank systems that are translatinginto English.
Note that TERp, TER and wcd6p4erare error metrics, so a negative correlation is bet-ter for them.
The strength of correlation varied forthe different language pairs.
The automatic met-rics were able to rank the French-English systemsreasonably well with correlation coefficients in therange of .8 and .9.
In comparison, metrics per-formed worse for Hungarian-English, where halfof the systems had negative correlation.
The ULCmetric once again had strongest correlation withhuman judgments of translation quality.
This wasfollowed closely by MaxSim and RTE, with Me-teor and TERp doing respectably well in 4th and5th place.
Notably, Bleu and its variants were theworst performing metrics in this translation direc-tion.Table 8 shows correlation for metrics which op-erated on languages other than English.
Most ofthe best performing metrics that operate on En-glish do not work for foreign languages, becausethey perform some linguistic analysis or rely ona resource like WordNet.
For translation into for-eign languages TERp was the best system overall.The wpBleu and wpF metrics also did extremelywell, performing the best in the language pairs thatthey were applied to.
wpBleu and wpF were notapplied to Czech because the authors of the met-ric did not have a Czech tagger.
English-Germanproved to be the most problematic language pairto automatically evaluate, with all of the metricshaving a negative correlation except wpBleu andTER.Table 9 gives detailed results for how well vari-ations on a number of automatic metrics do forthe task of ranking five English-Czech systems.6These systems were submitted by Kos and Bojar(2009), and they investigate the effects of usingPrague Dependency Treebank annotations duringautomatic evaluation.
They linearizing the Czechtrees and evaluated either the lemmatized forms ofthe Czech (lemma) read off the trees or the Tec-togrammatical form which retained only lemma-tized content words (tecto).
The table also demon-strates SemPOS, Meteor, and GTM perform betteron Czech than many other metrics.6.2 Sentence-level consistencyTables 10 and 11 show the percent of times that themetrics?
scores were consistent with human rank-ings of every pair of translated sentences.7 Sincewe eliminated sentence pairs that were judged tobe equal, the random baseline for this task is 50%.Many metrics failed to reach the baseline (includ-ing most metrics in the out-of-English direction).This indicates that sentence-level evaluation ofmachine translation quality is very difficult.
RTEand ULC again do the best overall for the into-English direction.
They are followed closely bywpF and wcd6p4er, which considerably improvetheir performance over their system-level correla-tions.We tried a variant on measuring sentence-levelconsistency.
Instead of using the scores assignedto each individual sentence, we used the system-level score and applied it to every sentence thatwas produced by that system.
These can bethought of as a metric?s prior expectation abouthow a system should preform, based on their per-formance on the whole data set.
Tables 12 and 13show that using the system-level scores in placeof the sentence-level scores results in considerablyhigher consistency with human judgments.
Thissuggests an interesting line of research for improv-ing sentence-level predictions by using the perfor-mance on a larger data set as a prior.7 SummaryAs in previous editions of this workshop we car-ried out an extensive manual and automatic eval-uation of machine translation performance fortranslating from European languages into English,6PCTRANS was excluded from the English-Czech systemsbecause its SGML file was malformed.7Not all metrics entered into the sentence-level task.16fr-en(6268pairs)de-en(6382pairs)es-en(4106pairs)cz-en(2251pairs)hu-en(2193pairs)xx-en(1952pairs)Overall(23152pairs)ulc .55 .56 .51 .50 .51 .51 .54rte (absolute) .54 .56 .51 .50 .55 .51 .53wpF .54 .55 .50 .47 .48 .51 .52wcd6p4er .54 .54 .49 .48 .48 .50 .52maxsim .53 .55 .49 .47 .50 .49 .52bleusp .54 .55 .49 .47 .46 .50 .51bleusp4114 .53 .55 .48 .47 .46 .50 .51rte (pairwise) .49 .48 .52 .53 .55 .52 .51terp .52 .53 .48 .46 .45 .48 .50meteor-0.6 .50 .53 .46 .48 .47 .47 .49meteor-rank .50 .52 .46 .48 .47 .47 .49meteor-0.7 .49 .52 .46 .48 .47 .47 .49ter .48 .47 .43 .41 .40 .42 .45wpbleu .46 .45 .46 .39 .35 .45 .44Table 10: Sentence-level consistency of the auto-matic metrics with human judgments for transla-tions into English.
Italicized numbers fall belowthe random-choice baseline.en-fr(2967pairs)en-de(6563pairs)en-es(3249pairs)en-cz(11242pairs)Overall(24021pairs)wcd6p4er .57 .47 .52 .49 .50bleusp4114 .57 .46 .54 .49 .50bleusp .57 .46 .53 .48 .49ter .50 .41 .45 .37 .41terp .51 .39 .48 .27 .36wpF .57 .46 .54 n/a .51wpbleu .53 .37 .46 n/a .43Table 11: Sentence-level consistency of the auto-matic metrics with human judgments for transla-tions out of English.
Italicized numbers fall belowthe random-choice baseline.fr-en(6268pairs)de-en(6382pairs)es-en(4106pairs)cz-en(2251pairs)hu-en(2193pairs)Overall(21200pairs)Oracle .61 .63 .59 .61 .67 .62rte (absolute) .60 .61 .59 .57 .65 .61ulc .61 .62 .58 .61 .59 .60maxsim .61 .62 .59 .57 .61 .60meteor-rank .61 .61 .59 .57 .61 .60meteor-0.6 .61 .61 .58 .57 .60 .60rte (pairwise) .56 .61 .57 .59 .64 .59terp .60 .61 .59 .57 .56 .59meteor-0.7 .61 .61 .58 .57 .55 .59ter .60 .59 .57 .55 .51 .58wpF .60 .59 .57 .61 .46 .58bleusp .61 .59 .56 .55 .48 .57bleusp4114 .61 .59 .56 .55 .46 .57wcd6p4er .61 .59 .57 .55 .44 .57wpbleu .60 .59 .57 .57 .43 .57Table 12: Consistency of the automatic met-rics when their system-level ranks are treated assentence-level scores.
Oracle shows the consis-tency of using the system-level human ranks thatare given in Table 6.en-fr(2967pairs)en-de(6563pairs)en-es(3249pairs)en-cz(11242pairs)Overall(24021pairs)Oracle .62 .59 .63 .60 .60terp .62 .50 .59 .53 .54ter .61 .51 .58 .50 .53bleusp .62 .48 .59 .50 .52bleusp4114 .63 .48 .59 .50 .52wcd6p4er .62 .46 .58 .50 .52wpbleu .63 .51 .60 n/a .56wpF .63 .50 .59 n/a .55Table 13: Consistency of the automatic met-rics when their system-level ranks are treated assentence-level scores.
Oracle shows the consis-tency of using the system-level human ranks thatare given in Table 6.17and vice versa.The number of participants remained stablecompared to last year?s WMT workshop, with22 groups from 20 institutions participating inWMT09.
This year?s evaluation also included 7commercial rule-based MT systems and Google?sonline statistical machine translation system.Compared to previous years, we have simpli-fied the evaluation conditions by removing the in-domain vs. out-of-domain distinction focusing onnews translations only.
The main reason for thiswas eliminating the advantage statistical systemshave with respect to test data that are from thesame domain as the training data.Analogously to previous years, the main focusof comparing the quality of different approachesis on manual evaluation.
Here, also, we reducedthe number of dimensions with respect to whichthe different systems are compared, with sentence-level ranking as the primary type of manual eval-uation.
In addition to the direct quality judgmentswe also evaluated translation quality by havingpeople edit the output of systems and have as-sessors judge the correctness of the edited output.The degree to which users were able to edit thetranslations (without having access to the sourcesentence or reference translation) served as a mea-sure of the overall comprehensibility of the trans-lation.Although the inter-annotator agreement in thesentence-ranking evaluation is only fair (as mea-sured by the Kappa score), agreement can be im-proved by removing the first (up to 50) judgmentsof each assessor, focusing on the judgments thatwere made once the assessors are more familiarwith the task.
Inter-annotator agreement with re-spect to correctness judgments of the edited trans-lations were higher (moderate), which is proba-bly due to the simplified evaluation criterion (bi-nary judgments versus rankings).
Inter-annotatoragreement for both conditions can be increasedfurther by removing the judges with the worstagreement.
Intra-annotator agreement on the otherhand was considerably higher ranging betweenmoderate and substantial.In addition to the manual evaluation criteria weapplied a large number of automated metrics tosee how they correlate with the human judgments.There is considerably variation between the differ-ent metrics and the language pairs under consid-eration.
As in WMT08, the ULC metric had thehighest overall correlation with human judgmentswhen translating into English, with MaxSim andRTE following closely behind.
TERp and wpBleuwere best when translating into other languages.Automatically predicting human judgments atthe sentence-level proved to be quite challeng-ing with many of the systems performing aroundchance.
We performed an analysis that showedthat if metrics?
system-level scores are used inplace of their scores for individual sentences, thatthey do quite a lot better.
This suggests that priorprobabilities ought to be integrated into sentence-level scoring.All data sets generated by this workshop, in-cluding the human judgments, system translationsand automatic scores, are publicly available forother researchers to analyze.8AcknowledgmentsThis work was supported in parts by the EuroMa-trix project funded by the European Commission(6th Framework Programme), the GALE programof the US Defense Advanced Research ProjectsAgency, Contract No.
HR0011-06-C-0022, andthe US National Science Foundation under grantIIS-0713448.We are grateful to Holger Schwenk and PreslavNakov for pointing out the potential bias in ourmethod for ranking systems when self-judgmentsare excluded.
We analyzed the results and foundthat this did not hold.
We would like to thankMaja Popovic for sharing thoughts about how toimprove the manual evaluation.
Thanks to CamFordyce for helping out with the manual evalua-tion again this year.An extremely big thanks to Sebastian Pado forhelping us work through the logic of segment-levelscoring of automatic evaluation metric.ReferencesAbhaya Agarwal and Alon Lavie.
2008.
Meteor, M-BLEU and M-TER: Evaluation metrics for high-correlation with human rankings of machine trans-lation output.
In Proceedings of the Third Workshopon Statistical Machine Translation, pages 115?118,Columbus, Ohio, June.
Association for Computa-tional Linguistics.Joshua Albrecht and Rebecca Hwa.
2007a.
A re-examination of machine learning approaches for8http://www.statmt.org/wmt09/results.html18sentence-level MT evaluation.
In Proceedings of the45th Annual Meeting of the Association for Compu-tational Linguistics (ACL-2007), Prague, Czech Re-public.Joshua Albrecht and Rebecca Hwa.
2007b.
Regres-sion for sentence-level MT evaluation with pseudoreferences.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics(ACL-2007), Prague, Czech Republic.Alexandre Allauzen, Josep Crego, Aure?lien Max, andFran cois Yvon.
2009.
LIMSI?s statistical transla-tion systems for WMT?09.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, Athens, Greece, March.
Association for Com-putational Linguistics.Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Mar-tin Popel, Jan Pta?c?ek, Jan Rous?, and Zdene?kZ?abokrtsky?.
2009.
English-Czech MT in 2008.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, Athens, Greece, March.
Asso-ciation for Computational Linguistics.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2007.
(Meta-) evaluation of machine translation.
In Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation (WMT07), Prague, Czech Repub-lic.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further meta-evaluation of machine translation.
InProceedings of the Third Workshop on StatisticalMachine Translation (WMT08), Colmbus, Ohio.Marine Carpuat.
2009.
Toward using morphologyin French-English phrase-based SMT.
In Proceed-ings of the Fourth Workshop on Statistical MachineTranslation, Athens, Greece, March.
Association forComputational Linguistics.Yee Seng Chan and Hwee Tou Ng.
2008.
An automaticmetric for machine translation evaluation based onmaximum similary.
In In the Metrics-MATR Work-shop of AMTA-2008, Honolulu, Hawaii.Yu Chen, Michael Jellinghaus, Andreas Eisele,Yi Zhang, Sabine Hunsicker, Silke Theison, Chris-tian Federmann, and Hans Uszkoreit.
2009.
Com-bining multi-engine translations with moses.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, Athens, Greece, March.
Associa-tion for Computational Linguistics.Jinhua Du, Yifan He, Sergio Penkale, and Andy Way.2009.
MATREX: The DCU MT system for WMT2009.
In Proceedings of the Fourth Workshop onStatistical Machine Translation, Athens, Greece,March.
Association for Computational Linguistics.Lo?
?c Dugast, Jean Senellart, and Philipp Koehn.2009.
Statistical post editing and dictionary ex-traction: Systran/Edinburgh submissions for ACL-WMT2009.
In Proceedings of the Fourth Workshopon Statistical Machine Translation, Athens, Greece,March.
Association for Computational Linguistics.Chris Dyer, Hendra Setiawan, Yuval Marton, andPhilip Resnik.
2009.
The University of Mary-land statistical machine translation system for thefourth workshop on machine translation.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, Athens, Greece, March.
Associa-tion for Computational Linguistics.Jason Eisner and Roy W. Tromble.
2006.
Localsearch with very large-scale neighborhoods for op-timal permutations in machine translation.
In Pro-ceedings of the Human Language Technology Con-ference of the North American chapter of the Associ-ation for Computational Linguistics (HLT/NAACL-2006), New York, New York.Christian Federmann, Silke Theison, Andreas Eisele,Hans Uszkoreit, Yu Chen, Michael Jellinghaus, andSabine Hunsicker.
2009.
Translation combina-tion using factored word substitution.
In Proceed-ings of the Fourth Workshop on Statistical MachineTranslation, Athens, Greece, March.
Association forComputational Linguistics.Alexander Fraser.
2009.
Experiments in morphosyn-tactic processing for translating to and from German.In Proceedings of the Fourth Workshop on StatisticalMachine Translation, Athens, Greece, March.
Asso-ciation for Computational Linguistics.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2007.
Linguis-tic features for automatic evaluation of heterogenousMT systems.
In Proceedings of ACL Workshop onMachine Translation.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2008.
A smor-gasbord of features for automatic MT evaluation.In Proceedings of the Third Workshop on StatisticalMachine Translation, pages 195?198.Greg Hanneman, Vamshi Ambati, Jonathan H. Clark,Alok Parlikar, and Alon Lavie.
2009.
Animproved statistical transfer system for French-English machine translation.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, Athens, Greece, March.
Association for Com-putational Linguistics.Kenneth Heafield, Greg Hanneman, and Alon Lavie.2009.
Machine translation system combinationwith flexible word ordering.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, Athens, Greece, March.
Association for Com-putational Linguistics.Almut Silja Hildebrand and Stephan Vogel.
2009.CMU system combination for WMT?09.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, Athens, Greece, March.
Associa-tion for Computational Linguistics.19Maria Holmqvist, Sara Stymne, Jody Foo, and LarsAhrenberg.
2009.
Improving alignment for SMT byreordering and augmenting the training corpus.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, Athens, Greece, March.
Asso-ciation for Computational Linguistics.Philipp Koehn and Barry Haddow.
2009.
Edin-burgh?s submission to all tracks of the WMT2009shared task with reordering and speed improvementsto Moses.
In Proceedings of the Fourth Workshopon Statistical Machine Translation, Athens, Greece,March.
Association for Computational Linguistics.Philipp Koehn and Christof Monz.
2006.
Manual andautomatic evaluation of machine translation betweenEuropean languages.
In Proceedings of NAACL2006 Workshop on Statistical Machine Translation,New York, New York.Philipp Koehn, Nicola Bertoldi, Ondrej Bojar, ChrisCallison-Burch, Alexandra Constantin, BrookeCowan, Chris Dyer, Marcello Federico, EvanHerbst, Hieu Hoang, Christine Moran, Wade Shen,and Richard Zens.
2007.
Open source toolkit forstatistical machine translation: Factored translationmodels and confusion network decoding.
CLSPSummer Workshop Final Report WS-2006, JohnsHopkins University.Kamil Kos and Ondr?ej Bojar.
2009.
Evaluation of Ma-chine Translation Metrics for Czech as the TargetLanguage.
Prague Bulletin of Mathematical Lin-guistics, 92. in print.Alex Kulesza and Stuart M. Shieber.
2004.
A learn-ing approach to improving sentence-level MT evalu-ation.
In Proceedings of the 10th International Con-ference on Theoretical and Methodological Issues inMachine Translation, Baltimore, MD, October 4?6.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33:159?174.Gregor Leusch and Hermann Ney.
2008.
BLEUSP,PINVWER, CDER: Three improved MT evaluationmeasures.
In In the Metrics-MATR Workshop ofAMTA-2008, Honolulu, Hawaii.Gregor Leusch, Evgeny Matusov, and Hermann Ney.2009.
The RWTH system combination system forWMT 2009.
In Proceedings of the Fourth Workshopon Statistical Machine Translation, Athens, Greece,March.
Association for Computational Linguistics.Zhifei Li, Chris Callison-Burch, Chris Dyer, JuriGanitkevitch, Sanjeev Khudanpur, Lane Schwartz,Wren Thornton, Jonathan Weese, and Omar Zaidan.2009.
Joshua: An open source toolkit for parsing-based machine translation.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, Athens, Greece, March.
Association for Com-putational Linguistics.Chin-Yew Lin and Franz Josef Och.
2004.
Auto-matic evaluation of machine translation quality us-ing longest common subsequence and skip-bigramstatistics.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics(ACL-2004), Barcelona, Spain.Robert C. Moore.
2002.
Fast and accurate sentencealignment of bilingual corpora.
In Proceedings ofthe 5th Biennial Conference of the Association forMachine Translation in the Americas (AMTA-2002),Tiburon, California.Preslav Nakov and Hwee Tou Ng.
2009.
NUSat WMT09: Domain adaptation experiments forEnglish-Spanish machine translation of news com-mentary text.
In Proceedings of the Fourth Work-shop on Statistical Machine Translation, Athens,Greece, March.
Association for Computational Lin-guistics.Jan Niehues, Teresa Herrmann, Muntsin Kolss, andAlex Waibel.
2009.
The Universita?t Karlsruhetranslation system for the EACL-WMT 2009.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, Athens, Greece, March.
Asso-ciation for Computational Linguistics.NIST.
2008.
Evaluation plan for gale go/no-go phase3 / phase 3.5 translation evaluations.
June 18, 2008.Attila Nova?k.
2009.
Morphologic?s submission forthe WMT 2009 shared task.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, Athens, Greece, March.
Association for Com-putational Linguistics.Sebastian Pado, Michel Galley, Dan Jurafsky, andChristopher D. Manning.
2009.
Machine transla-tion evaluation with textual entailment features.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, Athens, Greece, March.
Asso-ciation for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
Bleu: A method for auto-matic evaluation of machine translation.
In Pro-ceedings of the 40th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL-2002),Philadelphia, Pennsylvania.Michael Paul, Andrew Finch, and Eiichiro Sumita.2009.
NICT@WMT09: Model adaptation andtransliteration for Spanish-English SMT.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, Athens, Greece, March.
Associa-tion for Computational Linguistics.Maja Popovic and Hermann Ney.
2009.
Syntax-oriented evaluation measures for machine transla-tion output.
In Proceedings of the Fourth Workshopon Statistical Machine Translation, Athens, Greece,March.
Association for Computational Linguistics.20Maja Popovic, David Vilar, Daniel Stein, Evgeny Ma-tusov, and Hermann Ney.
2009.
The RWTH ma-chine translation system for WMT 2009.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, Athens, Greece, March.
Associa-tion for Computational Linguistics.Mark Przybocki, Kay Peterson, and Se-bastien Bronsart.
2008.
Official resultsof the NIST 2008 ?Metrics for MAchineTRanslation?
challenge (MetricsMATR08).http://nist.gov/speech/tests/metricsmatr/2008/results/.Jose?
A. R. Fonollosa, Maxim Khalilov, Marta R.
Costa-jussa?, Jose?
B. Marin?o, Carlos A. Henra?quez Q.,Adolfo Herna?ndez H., and Rafael E. Banchs.
2009.The TALP-UPC phrase-based translation systemfor EACL-WMT 2009.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, Athens, Greece, March.
Association for Com-putational Linguistics.Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2009.
Incremental hy-pothesis alignment with flexible matching for build-ing confusion networks: BBN system descriptionfor WMT09 system combination task.
In Proceed-ings of the Fourth Workshop on Statistical MachineTranslation, Athens, Greece, March.
Association forComputational Linguistics.Josh Schroeder, Trevor Cohn, and Philipp Koehn.2009.
Word lattices for multi-source translation.In 12th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL-2009), Athens, Greece.Holger Schwenk, Sadaf Abdul Rauf, Loic Barrault, andJean Senellart.
2009.
SMT and SPE machine trans-lation systems for WMT?09.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, Athens, Greece, March.
Association for Com-putational Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the 7th Biennial Conference of theAssociation for Machine Translation in the Ameri-cas (AMTA-2006), Cambridge, Massachusetts.Matthew Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Fluency, adequacy,or HTER?
exploring different human judgmentswith a tunable MT metric.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, Athens, Greece, March.
Association for Com-putational Linguistics.David Talbot and Miles Osborne.
2007.
SmoothedBloom filter language models: Tera-scale lms onthe cheap.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), Prague, Czech Repub-lic.Eric Wehrli, Luka Nerima, and Yves Scherrer.2009.
Deep linguistic multilingual translationand bilingual dictionaries.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, Athens, Greece, March.
Association for Com-putational Linguistics.21A Pairwise system comparisons by human judgesTables 14?24 show pairwise comparisons between systems for each language pair.
The numbers in eachof the tables?
cells indicate the percentage of times that the system in that column was judged to be betterthan the system in that row.
Bolding indicates the winner of the two systems.
The difference between100 and the sum of the complimentary cells is the percent of time that the two systems were judged tobe equal.Because there were so many systems and data conditions the significance of each pairwise compar-ison needs to be quantified.
We applied the Sign Test to measure which comparisons indicate genuinedifferences (rather than differences that are attributable to chance).
In the following tables ?
indicates sta-tistical significance at p ?
0.10, ?
indicates statistical significance at p ?
0.05, and ?
indicates statisticalsignificance at p ?
0.01, according to the Sign Test.B Automatic scoresTables 26 and 25 give the automatic scores for each of the systems.GENEVAGOOGLEJHU-TROMBLELIURBMT1RBMT2RBMT3RBMT4RBMT5RWTHSTUTTGARTSYSTRANUEDINUKAUMDUSAARBBN-COMBOCMU-COMBOCMU-COMBO-HYPOSELRWTH-COMBOUSAAR-COMBOGENEVA .76?
.08?
.63?
.54 .69?
.73?
.83?
.78?
.49?
.77?
.75?
.74?
.57?
.74?
.69?
.75?
.84?
.60 .84?
.71?GOOGLE .15?
.03?
.23?
.50 .43 .24?
.39 .42 .39 .43 .33 .27?
.29?
.38 .48 .57?
.44 .32 .35 .36JHU-TROMBLE .75?
.90?
.77?
.81?
.84?
.91?
.94?
.88?
.79?
.83?
.83?
.93?
.89?
.92?
.90?
.94?
.90?
.95?
.91?
.83?LIU .29?
.65?
.12?
.49 .63 .63?
.57 .63?
.41 .49 .46 .50 .49 .50 .41 .66?
.53 .59?
.62?
.53RBMT1 .32 .43 .11?
.46 .42 .46 .50 .61?
.34 .46 .58 .51 .42 .42 .56 .47 .53 .49 .58 .54RBMT2 .25?
.46 .09?
.37 .45 .33 .45 .23?
.3 .28 .47 .42 .31?
.34 .39 .49 .61 .4 .32 .29?RBMT3 .17?
.59?
.02?
.26?
.35 .46 .27 .45 .27 .36 .46 .42 .43 .26?
.49 .4 .48 .58 .29 .31RBMT4 .12?
.47 .07?
.37 .4 .45 .52 .60?
.39 .39 .45 .39 .31?
.29?
.44 .54 .45 .37 .43 .30RBMT5 .13?
.34 .07?
.30?
.24?
.57?
.41 .29?
.31 .50 .34 .3 .28?
.43 .30 .49 .57 .3 .49 .21RWTH .21?
.55 .10?
.41 .49 .55 .46 .46 .60 .44 .57 .48 .51?
.41 .56 .64?
.54 .56?
.74?
.59?STUTTGART .17?
.43 .13?
.39 .43 .55 .39 .36 .33 .34 .38 .42 .52 .42 .49 .49 .28 .35 .56 .46SYSTRAN .11?
.63 .06?
.42 .37 .47 .50 .32 .58 .34 .55 .36 .44 .35 .43 .61?
.46 .41 .33 .44UEDIN .10?
.50?
.03?
.35 .49 .46 .39 .52 .55 .29 .39 .52 .35 .33 .42 .58?
.43 .56 .59?
.55UKA .29?
.58?
.04?
.32 .47 .63?
.55 .54?
.64?
.24?
.28 .39 .50 .29 .50 .48 .36 .57?
.45 .45UMD .16?
.53 .08?
.38 .49 .43 .63?
.68?
.49 .38 .39 .41 .50 .49 .46 .54 .44 .38 .46 .50USAAR .19?
.44 ?
.41 .34 .49 .4 .44 .33 .36 .33 .45 .39 .32 .41 .46 .41 .31 .42 .11BBN-COMBO .14?
.31?
.06?
.26?
.44 .44 .48 .36 .38 .23?
.35 .26?
.29?
.34 .36 .37 .32 .23?
.38 .32CMU-COMBO .10?
.36 .07?
.37 .37 .36 .48 .40 .30 .28 .53 .41 .4 .43 .28 .34 .50 .33 .53 .44CMU-COMBO-H .3 .46 ?
.10?
.39 .43 .40 .48 .57 .27?
.41 .47 .28 .26?
.38 .49 .65?
.46 .41 .47RWTH-COMBO .06?
.38 ?
.19?
.36 .54 .43 .43 .30 .10?
.33 .56 .22?
.27 .23 .42 .32 .31 .41 .29USAAR-COMBO .20?
.55 .17?
.3 .39 .57?
.45 .59 .32 .27?
.33 .47 .32 .33 .27 .16 .55 .44 .4 .50> OTHERS .22 .51 .06 .38 .44 .52 .49 .49 .50 .33 .44 .48 .44 .42 .41 .47 .56 .48 .46 .51 .43>= OTHERS .33 .65 .13 .50 .54 .64 .64 .62 .66 .50 .61 .60 .59 .58 .56 .65 .68 .63 .62 .70 .62Table 14: Sentence-level ranking for the WMT09 German-English News Task22GOOGLELIURBMT1RBMT2RBMT3RBMT4RBMT5RWTHSTUTTGARTUEDINUKAUSAARUSAAR-COMBOGOOGLE .34?
.56 .51 .55?
.44 .56?
.37 .41 .42 .45 .45 .43LIU .58?
.62?
.55?
.55?
.61?
.59?
.37 .38 .47 .43 .58?
.44RBMT1 .39 .33?
.56?
.44 .50?
.57?
.41 .32?
.37?
.35?
.45 .42RBMT2 .35 .34?
.34?
.43 .37?
.40 .25?
.25?
.31?
.36?
.37?
.32?RBMT3 .31?
.35?
.41 .35 .37?
.41 .24?
.25?
.33?
.43 .49 .36?RBMT4 .48 .33?
.33?
.56?
.55?
.47 .37 .35?
.34?
.45 .44 .38RBMT5 .36?
.35?
.33?
.50 .53 .33 .36?
.32?
.35?
.31?
.25?
.32?RWTH .51 .46 .50 .60?
.65?
.51 .60?
.38 .47 .48 .52 .54STUTTGART .50 .47 .62?
.65?
.64?
.57?
.62?
.46 .52?
.54?
.66?
.53UEDIN .50 .37 .53?
.64?
.62?
.60?
.55?
.45 .28?
.41 .53 .35UKA .47 .42 .57?
.58?
.46 .44 .62?
.35 .32?
.36 .46 .41USAAR .46 .36?
.46 .55?
.42 .42 .48?
.42 .28?
.39 .44 .41USAAR-COMBO .37 .45 .54 .55?
.55?
.53 .61?
.39 .40 .39 .46 .52> OTHERS .44 .38 .48 .55 .53 .47 .54 .37 .33 .39 .42 .48 .41>= OTHERS .54 .49 .57 .66 .64 .58 .64 .48 .43 .51 .54 .58 .52Table 15: Sentence-level ranking for the WMT09 English-German News TaskGOOGLENICTRBMT1RBMT3RBMT4RBMT5RWTHTALP-UPCUEDINUSAARBBN-COMBOCMU-COMBOUSAAR-COMBOGOOGLE .21?
.40 .40 .41 .38 .23?
.35 .31?
.25?
.36 .14 .21NICT .74?
.52 .53 .63?
.64?
.55?
.61?
.65?
.59?
.62?
.78?
.66?RBMT1 .56 .40 .34 .44 .46 .35 .48 .42 .42 .57?
.52 .54RBMT3 .40 .39 .40 .34 .36 .42 .4 .55 .50 .57?
.48 .62?RBMT4 .55 .32?
.41 .46 .47 .39 .49 .49 .48 .54 .57?
.54RBMT5 .54 .30?
.35 .44 .38 .45 .50 .49 .23 .51 .51 .66?RWTH .64?
.29?
.50 .53 .53 .49 .42 .46 .43 .44 .51 .58?TALP-UPC .48 .24?
.44 .47 .41 .36 .39 .36 .32?
.47 .45 .50UEDIN .61?
.16?
.48 .42 .41 .46 .44 .43 .44 .49 .51 .41USAAR .69?
.28?
.47 .44 .38 .35 .43 .60?
.48 .64?
.58?
.56?BBN-COMBO .35 .20?
.32?
.36?
.39 .37 .36 .39 .32 .31?
.50 .40CMU-COMBO .19 .15?
.33 .39 .32?
.37 .36 .31 .37 .21?
.35 .31USAAR-COMBO .23 .20?
.42 .31?
.39 .25?
.27?
.35 .35 .32?
.36 .29> OTHERS .50 .26 .42 .42 .42 .42 .39 .44 .43 .37 .49 .49 .50>= OTHERS .70 .37 .55 .55 .53 .55 .51 .59 .56 .51 .64 .70 .69Table 16: Sentence-level ranking for the WMT09 Spanish-English News TaskGOOGLENUSRBMT1RBMT3RBMT4RBMT5RWTHTALP-UPCUEDINUSAARUSAAR-COMBOGOOGLE .39 .21?
.49 .36 .48 .34?
.39 .33 .36?
.21NUS .50 .11?
.62?
.51 .51 .35 .25 .47 .36 .43RBMT1 .76?
.80?
.79?
.79?
.83?
.64?
.76?
.80?
.67?
.64?RBMT3 .42 .31?
.16?
.30?
.43 .34 .29?
.56 .24?
.32RBMT4 .47 .32 .11?
.52?
.49 .38 .36 .51 .39 .38RBMT5 .42 .40 .11?
.49 .35 .31?
.39 .47 .18?
.47RWTH .59?
.52 .26?
.54 .51 .61?
.46 .56?
.39 .55?TALP-UPC .49 .41 .17?
.63?
.52 .51 .29 .45?
.39 .41UEDIN .50 .32 .17?
.36 .37 .46 .30?
.29?
.32?
.36USAAR .58?
.56 .23?
.67?
.53 .47?
.51 .49 .61?
.58?USAAR-COMBO .31 .45 .21?
.54 .49 .50 .30?
.43 .43 .33?> OTHERS .50 .45 .17 .56 .47 .53 .38 .42 .52 .37 .43>= OTHERS .65 .59 .25 .66 .61 .64 .51 .58 .66 .48 .61Table 17: Sentence-level ranking for the WMT09 English-Spanish News Task23CMU-STATXFERCOLUMBIADCUGENEVAGOOGLEJHULIMSILIUM-SYSTRANRBMT1RBMT3RBMT4RBMT5RWTHUEDINUKAUSAARBBN-COMBOCMU-COMBOCMU-COMBO-HYPOSELDCU-COMBOUSAAR-COMBOCMU-STATXFER .37 .44 .17?
.63?
.47 .46 .58?
.34 .32 .25?
.42 .48 .46 .28 .38 .58?
.47 .39 .41 .35COLUMBIA .56 .56?
.37 .71?
.48 .56?
.35 .45 .28?
.38 .42 .41 .33 .58 .50 .64?
.52 .64?
.71?
.58?DCU .27 .29?
.15?
.67?
.45 .33 .34 .29 .31 .29 .27?
.24 .37 .21?
.39 .61?
.4 .36 .37 .1GENEVA .76?
.54 .73?
.71?
.65?
.73?
.62?
.66?
.76?
.46 .79?
.57 .74?
.72?
.67?
.69?
.52 .71?
.67?
.64?GOOGLE .23?
.17?
.12?
.13?
.21?
.35 .09?
.20?
.27?
.31?
.44 .16?
.21?
.33 .27?
.28 .30 .34 .37 .16?JHU .40 .26 .38 .22?
.60?
.31 .44 .27 .37 .29?
.41 .33 .37 .48 .48 .53 .47 .31 .47 .29LIMSI .4 .16?
.38 .19?
.56 .49 .29 .37 .27 .20?
.38 .23?
.33 .29 .38 .61?
.47 .31 .36 .26?LIUM-SYSTRAN .23?
.30 .42 .33?
.61?
.27 .45 .48 .31 .41 .44 .32 .35 .41 .39 .54?
.61?
.24 .67?
.36RBMT1 .53 .23 .42 .19?
.57?
.46 .51 .45 .47 .33 .46 .33 .41 .30 .61 .77?
.51 .41 .50 .41RBMT3 .57 .63?
.55 .15?
.69?
.44 .57 .52 .41 .22?
.38 .51 .43 .43 .31 .57?
.46 .47 .38 .55RBMT4 .58?
.35 .51 .36 .67?
.60?
.63?
.35 .41 .59?
.40 .55 .50 .71?
.52?
.63?
.65?
.65?
.66?
.38RBMT5 .42 .49 .54?
.09?
.38 .49 .49 .37 .27 .29 .34 .38 .39 .51 .18 .42 .58 .48 .50 .60?RWTH .38 .39 .45 .32 .63?
.46 .51?
.34 .56 .39 .32 .52 .48 .46 .46 .66?
.62?
.61?
.66?
.54?UEDIN .41 .21 .31 .19?
.68?
.46 .42 .35 .41 .38 .31 .46 .33 .34 .41 .41 .35 .44 .63?
.37UKA .40 .31 .54?
.19?
.51 .37 .44 .33 .52 .51 .17?
.27 .32 .49 .34 .39 .53 .36 .44 .29USAAR .44 .43 .52 .26?
.62?
.48 .46 .30 .30 .58 .17?
.24 .44 .47 .41 .65?
.52 .70?
.55 .41BBN-COMBO .21?
.21?
.12?
.23?
.26 .32 .28?
.23?
.12?
.26?
.22?
.49 .09?
.34 .23 .19?
.44 .49?
.28 .21?CMU-COMBO .41 .36 .4 .28 .30 .35 .47 .21?
.29 .42 .23?
.31 .17?
.49 .25 .42 .31 .37 .29 .25CMU-COMBO-H .24 .21?
.38 .23?
.37 .39 .31 .24 .31 .41 .28?
.31 .14?
.33 .34 .24?
.18?
.3 .29 .27DCU-COMBO .41 .13?
.42 .20?
.37 .29 .50 .19?
.44 .49 .23?
.46 .20?
.21?
.37 .39 .31 .26 .46 .19?USAAR-COMBO .41 .25?
.18 .28?
.66?
.53 .52?
.48 .41 .38 .53 .17?
.21?
.42 .42 .47 .58?
.58 .47 .63?> OTHERS .40 .31 .41 .23 .56 .43 .46 .36 .37 .41 .30 .40 .33 .41 .40 .40 .50 .47 .46 .49 .36>= OTHERS .58 .5 .66 .34 .76 .62 .65 .60 .56 .54 .47 .59 .52 .61 .61 .55 .73 .66 .71 .67 .57Table 18: Sentence-level ranking for the WMT09 French-English News TaskDCUGENEVAGOOGLELIMSILIUM-SYSTRANRBMT1RBMT3RBMT4RBMT5RWTHSYSTRANUEDINUKAUSAARDCU-COMBOUSAAR-COMBODCU .12?
.39 .47 .44 .33 .44 .27 .45 .24?
.49 .24 .46 .26?
.39 .33GENEVA .62?
.73?
.69?
.80?
.50?
.71?
.50?
.52?
.56?
.66?
.46?
.56?
.57 .74?
.84?GOOGLE .46 .15?
.28 .42 .26 .44 .26?
.34 .29?
.44 .24 .32 .29 .36 .32LIMSI .25 .16?
.45 .48 .23?
.43 .30 .45 .27 .42 .34 .4 .36 .53?
.38LIUM-SYSTRAN .24 ?
.45 .32 .17?
.29 .17?
.21?
.38 .29 .17?
.35 .17?
.41 .41RBMT1 .39 .25?
.51 .51?
.53?
.46 .40 .29 .52 .36 .60?
.63?
.41 .44 .60?RBMT3 .36 .11?
.37 .37 .52 .24 .25?
.27?
.31 .44 .43 .32 .27?
.53 .44RBMT4 .36 .19?
.58?
.37 .57?
.23 .61?
.42 .32 .50 .22 .39 .44 .53 .56?RBMT5 .41 .17?
.53 .39 .61?
.38 .58?
.30 .41 .52?
.41 .48 .13 .54 .60RWTH .59?
.21?
.63?
.50 .47 .29 .44 .37 .31 .37 .35 .51 .16?
.50?
.57?SYSTRAN .35 .20?
.33 .39 .38 .40 .22 .29 .26?
.44 .47 .33 .32 .60?
.45UEDIN .38 .11?
.41 .28 .77?
.33?
.51 .44 .49 .32 .37 .30 .31 .56 .56?UKA .36 .09?
.46 .4 .45 .23?
.50 .39 .29 .29 .47 .26 .19?
.41 .56?USAAR .66?
.27 .52 .49 .70?
.31 .61?
.29 .32 .64?
.62 .51 .61?
.76?
.65?DCU-COMBO .32 .11?
.30 .18?
.45 .22 .29 .33 .29 .13?
.27?
.26 .41 .12?
.21USAAR-COMBO .40 ?
.39 .17 .26 .17?
.28 .20?
.28 .20?
.39 .04?
.06?
.08?
.39> OTHERS .41 .15 .47 .39 .52 .29 .45 .32 .35 .35 .45 .34 .42 .28 .51 .49>= OTHERS .65 .38 .68 .64 .73 .54 .65 .59 .57 .58 .65 .60 .66 .48 .74 .77Table 19: Sentence-level ranking for the WMT09 English-French News TaskCU-BOJARGOOGLEUEDINBBN-COMBOCMU-COMBOCU-BOJAR .54?
.44 .45?
.52?GOOGLE .28?
.32?
.18?
.23UEDIN .38 .51?
.38 .45?BBN-COMBO .31?
.39?
.32 .38?CMU-COMBO .28?
.29 .27?
.24?> OTHERS .31 .43 .34 .31 .40>= OTHERS .51 .75 .57 .65 .73Table 20: Sentence-level ranking for the WMT09 Czech-English News Task24CU-BOJARCU-TECTOMTEUROTRANXPGOOGLEPCTRANSUEDINCU-BOJAR .31?
.45?
.43?
.48?
.30?CU-TECTOMT .51?
.54?
.56?
.58?
.42?EUROTRANXP .35?
.26?
.39 .38 .29?GOOGLE .31?
.30?
.42 .43?
.26?PCTRANS .33?
.27?
.36 .38?
.30?UEDIN .42?
.37?
.52?
.50?
.53?> OTHERS .38 .30 .46 .45 .48 .31>= OTHERS .61 .48 .67 .66 .67 .53Table 21: Sentence-level ranking for the WMT09 English-Czech News TaskMORPHOUEDINUMDBBN-COMBOCMU-COMBOCMU-COMBO-HYPOSELMORPHO .21?
.28?
.24?
.27?
.28?UEDIN .70?
.59?
.45?
.55?
.50?UMD .61?
.26?
.21?
.29 .38BBN-COMBO .67?
.23?
.48?
.41?
.52?CMU-COMBO .59?
.25?
.35 .29?
.42CMU-COMBO-HYPOSEL .55?
.15?
.34 .27?
.34> OTHERS .62 .22 .41 .29 .37 .42>= OTHERS .75 .45 .66 .54 .62 .68Table 22: Sentence-level ranking for the WMT09 Hungarian-English News TaskGOOGLECZGOOGLEESGOOGLEFRRBMT2 DERBMT3 DERBMT3 ESRBMT3 FRRBMT5 ESRBMT5 FRBBN-COMBOCZBBN-COMBODEBBN-COMBOESBBN-COMBOFRBBN-COMBOHUBBN-COMBOXXCMU-COMBO-HYPOSELDECMU-COMBO-HYPOSELHUCMU-COMBOCZCMU-COMBOHUCMU-COMBOXXDCU-COMBOFRRWTH-COMBODERWTH-COMBOXXUSAAR-COMBOESGOOGLECZ .61?
.54?
.47 .52 .51 .47 .61?
.42 .38 .52 .55 .54 .11?
.51 .48 .34 .49 .32 .53 .52 .50 .59 .53GOOGLEES .33?
.42 .37 .38 .41 .35 .49 .45 .11?
.39 .25 .36 .18?
.26?
.36 .22?
.32 .18?
.38 .4 .4 .38 .22GOOGLEFR .27?
.42 .26?
.36 .43 .47 .33 .35 .29?
.23?
.50 .23 .14?
.29?
.21?
.11?
.17?
.22?
.39 .48 .32 .36 .27RBMT2DE .33 .49 .61?
.41 .43 .25?
.52 .38 .33 .41 .4 .55 .20?
.66?
.62?
.18?
.55 .35 .35 .58 .54 .61?
.57?RBMT3DE .37 .60 .54 .41 .42 .38 .45 .61 .48 .39 .40 .63?
.32 .43 .25?
.35 .35 .25?
.56 .69?
.46 .49 .46RBMT3ES .34 .52 .46 .51 .54 .43 .36 .38 .30?
.54 .41 .47 .25?
.50 .42 .26?
.43 .27?
.52 .57 .47 .46 .26?RBMT3FR .40 .58 .37 .63?
.53 .57 .54 .50 .36 .64?
.44 .55 .13?
.60 .64?
.4 .53 .31 .46 .48 .44 .52 .42RBMT5ES .29?
.41 .55 .31 .48 .36 .33 .39 .16?
.44 .50 .68?
.23?
.35 .48 .38 .37 .41 .60?
.51 .51 .65?
.32RBMT5FR .47 .52 .45 .50 .33 .51 .34 .42 .29 .59 .44 .49 ?
.49 .61?
.28?
.19?
.35 .58?
.60?
.27 .59 .57BBN-COMBOCZ .41 .74?
.65?
.55 .44 .67?
.56 .80?
.46 .46 .58 .70?
.22?
.73?
.63?
.32 .38 .48 .65?
.72?
.66?
.70?
.58BBN-COMBODE .39 .54 .58?
.41 .49 .44 .31?
.44 .28 .49 .49 .52 .16?
.52 .36 .22?
.38 .33?
.41 .68?
.34 .52 .56BBN-COMBOES .38 .40 .41 .43 .47 .55 .46 .25 .51 .31 .43 .44 .20?
.50 .42 .30?
.32 .29?
.36 .62 .47 .44 .38BBN-COMBOFR .38 .52 .35 .36 .27?
.53 .40 .26?
.33 .24?
.44 .36 .12?
.47 .47 .32 .44 .27?
.41 .42 .33 .60?
.35BBN-COMBOHU .84?
.75?
.78?
.60?
.57 .70?
.71?
.62?
.84?
.65?
.72?
.63?
.85?
.78?
.69?
.60?
.71?
.50 .85?
.78?
.87?
.86?
.75?BBN-COMBOXX .4 .54?
.63?
.34?
.50 .47 .32 .45 .39 .20?
.39 .45 .41 .14?
.24?
.21?
.3 .21?
.46 .40 .47 .41 .41CMU-CMB-HYPDE .48 .43 .68?
.29?
.64?
.46 .31?
.30 .30?
.23?
.41 .39 .32 .19?
.74?
.21?
.32 .31 .50 .74?
.38 .56?
.53CMU-CMB-HYPHU .63 .75?
.78?
.70?
.55 .63?
.46 .58 .59?
.50 .61?
.70?
.59 .13?
.68?
.69?
.65?
.39 .75?
.71?
.82?
.80?
.68?CMU-COMBOCZ .32 .59 .81?
.36 .50 .46 .41 .50 .60?
.28 .54 .52 .47 .20?
.55 .56 .26?
.13?
.55 .69?
.57 .66?
.55CMU-COMBOHU .62 .76?
.69?
.58 .68?
.67?
.59 .54 .54 .48 .67?
.64?
.70?
.32 .74?
.60 .50 .77?
.66?
.72?
.61 .82?
.82?CMU-COMBOXX .4 .50 .33 .51 .37 .43 .44 .29?
.24?
.32?
.56 .43 .39 .13?
.39 .39 .16?
.30 .32?
.39 .4 .46 .4DCU-COMBOFR .44 .57 .29 .32 .25?
.29 .26 .35 .27?
.19?
.23?
.38 .42 .15?
.34 .20?
.12?
.19?
.17?
.50 .55 .49 .30?RWTH-COMBODE .41 .43 .52 .37 .39 .53 .50 .35 .53 .25?
.40 .47 .54 .10?
.47 .41 .07?
.38 .30 .53 .38 .56 .49RWTH-COMBOXX .31 .38 .44 .26?
.41 .39 .31 .26?
.32 .18?
.29 .44 .19?
.10?
.36 .25?
.11?
.28?
.15?
.39 .42 .28 .44USAAR-COMBOES .37 .37 .54 .21?
.4 .58?
.39 .47 .31 .32 .34 .28 .55 .11?
.38 .38 .20?
.38 .18?
.44 .67?
.43 .44> OTHERS .41 .54 .54 .43 .45 .49 .41 .44 .44 .32 .46 .46 .50 .16 .51 .45 .26 .40 .29 .52 .57 .48 .55 .47>= OTHERS .52 .67 .70 .55 .55 .57 .52 .58 .58 .43 .57 .59 .62 .27 .62 .58 .37 .52 .36 .63 .68 .59 .69 .62Table 23: Sentence-level ranking for the WMT09 All-English News Task25BBN-COMBOCMU-COMBORWTH-COMBOBBN-COMBO .37 .40?CMU-COMBO .41 .44?RWTH-COMBO .32?
.34?> OTHERS .36 .35 .42>= OTHERS .62 .58 .67Table 24: Sentence-level ranking for the WMT09 Multisource-English News Task26RANKBLEUBLEU-CASEDBLEU-TERBLEUSPBLEUSP4114MAXSIMMETEOR-0.6METEOR-0.7METEOR-RANKINGNISTNIST-CASEDRTE-ABSOLUTERTE-PAIRWISETERTERPULCWCD6P4ERWPFWPBLEUGerman-English News TaskBBN-COMBO 0.68 0.24 0.22 ?0.17 0.29 0.31 0.51 0.55 0.6 0.41 7.08 6.78 0.13 0.1 0.54 0.63 0.31 0.45 0.36 0.31CMU-COMBO 0.63 0.22 0.21 ?0.19 0.28 0.29 0.49 0.54 0.58 0.4 6.95 6.71 0.12 0.09 0.56 0.66 0.29 0.47 0.35 0.29CMU-COMBO-HYPOSEL 0.62 0.23 0.21 ?0.19 0.28 0.3 0.49 0.54 0.57 0.4 6.79 6.5 0.11 0.09 0.57 0.66 0.29 0.47 0.35 0.3GENEVA 0.33 0.1 0.09 ?0.33 0.17 0.18 0.38 0.43 0.44 0.30 4.88 4.65 0.03 0.04 0.71 0.86 0.22 0.58 0.25 0.17GOOGLE 0.65 0.21 0.20 ?0.2 0.27 0.28 0.48 0.54 0.57 0.39 6.85 6.65 0.11 0.11 0.56 0.65 0.29 0.48 0.35 0.28JHU-TROMBLE 0.13 0.07 0.06 ?0.38 0.09 0.1 0.34 0.43 0.41 0.29 4.90 4.25 0.02 0.02 0.81 1 0.19 0.61 0.22 0.12LIU 0.50 0.19 0.18 ?0.22 0.25 0.27 0.46 0.51 0.54 0.38 6.35 6.02 0.06 0.05 0.61 0.72 0.27 0.49 0.33 0.26RBMT1 0.54 0.14 0.13 ?0.29 0.20 0.21 0.43 0.50 0.53 0.37 5.30 5.07 0.04 0.04 0.67 0.76 0.26 0.55 0.29 0.22RBMT2 0.64 0.17 0.16 ?0.26 0.23 0.24 0.48 0.52 0.55 0.38 6.06 5.75 0.1 0.12 0.63 0.70 0.29 0.51 0.31 0.24RBMT3 0.64 0.17 0.16 ?0.25 0.23 0.25 0.48 0.52 0.55 0.38 5.98 5.71 0.09 0.09 0.61 0.68 0.29 0.51 0.32 0.25RBMT4 0.62 0.16 0.14 ?0.27 0.21 0.23 0.45 0.5 0.52 0.36 5.65 5.36 0.06 0.07 0.65 0.72 0.27 0.52 0.30 0.23RBMT5 0.66 0.16 0.15 ?0.26 0.22 0.24 0.47 0.51 0.54 0.37 5.76 5.52 0.07 0.06 0.63 0.70 0.28 0.52 0.31 0.24RWTH 0.50 0.19 0.18 ?0.21 0.25 0.26 0.45 0.50 0.53 0.36 6.44 6.24 0.06 0.03 0.60 0.74 0.27 0.49 0.33 0.26RWTH-COMBO 0.7 0.23 0.22 ?0.18 0.29 0.30 0.50 0.55 0.59 0.41 7.06 6.81 0.11 0.07 0.54 0.63 0.30 0.46 0.36 0.31STUTTGART 0.61 0.2 0.18 ?0.22 0.26 0.27 0.48 0.52 0.56 0.38 6.39 6.11 0.1 0.06 0.60 0.69 0.29 0.49 0.33 0.27SYSTRAN 0.6 0.19 0.17 ?0.22 0.24 0.26 0.47 0.52 0.55 0.38 6.40 6.08 0.08 0.07 0.60 0.71 0.28 0.5 0.33 0.26UEDIN 0.59 0.20 0.19 ?0.22 0.26 0.27 0.47 0.52 0.55 0.38 6.47 6.24 0.07 0.04 0.61 0.70 0.27 0.49 0.34 0.27UKA 0.58 0.21 0.2 ?0.20 0.27 0.28 0.47 0.52 0.56 0.38 6.66 6.43 0.08 0.04 0.58 0.69 0.28 0.48 0.34 0.28UMD 0.56 0.21 0.19 ?0.19 0.26 0.28 0.47 0.52 0.56 0.38 6.74 6.42 0.08 0.04 0.56 0.69 0.28 0.48 0.34 0.27USAAR 0.65 0.17 0.15 ?0.26 0.23 0.24 0.47 0.51 0.54 0.38 5.89 5.64 0.06 0.05 0.64 0.71 0.28 0.52 0.31 0.24USAAR-COMBO 0.62 0.17 0.16 ?0.25 0.23 0.24 0.47 0.51 0.55 0.38 5.99 6.85 0.07 0.06 0.64 0.70 0.28 0.51 0.32 0.25Spanish-English News TaskBBN-COMBO 0.64 0.29 0.27 ?0.13 0.34 0.35 0.53 0.57 0.62 0.43 7.64 7.35 0.16 0.13 0.51 0.61 0.33 0.42 0.4 0.35CMU-COMBO 0.7 0.28 0.27 ?0.13 0.33 0.35 0.53 0.58 0.62 0.43 7.65 7.46 0.21 0.2 0.51 0.60 0.34 0.42 0.40 0.36GOOGLE 0.70 0.29 0.28 ?0.13 0.34 0.35 0.53 0.58 0.62 0.43 7.68 7.50 0.23 0.22 0.5 0.59 0.34 0.42 0.41 0.36NICT 0.37 0.22 0.22 ?0.19 0.27 0.29 0.48 0.54 0.57 0.39 6.91 6.74 0.1 0.1 0.60 0.71 0.3 0.46 0.36 0.3RBMT1 0.55 0.19 0.18 ?0.24 0.25 0.26 0.49 0.54 0.57 0.40 6.07 5.93 0.11 0.12 0.62 0.69 0.3 0.49 0.34 0.28RBMT3 0.55 0.20 0.2 ?0.22 0.26 0.27 0.50 0.54 0.58 0.41 6.24 6.08 0.13 0.14 0.60 0.65 0.31 0.48 0.36 0.29RBMT4 0.53 0.2 0.19 ?0.22 0.25 0.27 0.48 0.53 0.57 0.4 6.20 6.03 0.10 0.11 0.60 0.67 0.3 0.48 0.35 0.28RBMT5 0.55 0.20 0.2 ?0.22 0.26 0.27 0.5 0.54 0.58 0.40 6.26 6.10 0.12 0.11 0.6 0.65 0.31 0.48 0.36 0.29RWTH 0.51 0.24 0.23 ?0.16 0.3 0.31 0.49 0.54 0.58 0.4 7.12 6.95 0.11 0.08 0.56 0.68 0.31 0.45 0.37 0.32TALP-UPC 0.59 0.26 0.25 ?0.15 0.31 0.33 0.51 0.56 0.6 0.41 7.28 7.02 0.13 0.11 0.54 0.64 0.32 0.44 0.38 0.33UEDIN 0.56 0.26 0.25 ?0.15 0.32 0.33 0.51 0.56 0.60 0.42 7.25 7.04 0.16 0.1 0.55 0.64 0.32 0.43 0.39 0.34USAAR 0.51 0.2 0.19 ?0.22 0.25 0.27 0.48 0.54 0.57 0.4 6.31 6.14 0.11 0.09 0.62 0.67 0.3 0.48 0.34 0.28USAAR-COMBO 0.69 0.29 0.27 ?0.13 0.34 0.35 0.53 0.58 0.62 0.43 7.58 7.25 0.20 0.13 0.51 0.6 0.34 0.42 0.4 0.35French-English News TaskBBN-COMBO 0.73 0.31 0.3 ?0.11 0.36 0.38 0.54 0.59 0.64 0.45 7.88 7.58 0.14 0.12 0.2 0.20 0.36 0.40 0.41 0.37CMU-COMBO 0.66 0.3 0.29 ?0.12 0.35 0.36 0.53 0.58 0.63 0.44 7.72 7.57 0.15 0.12 0.24 0.26 0.35 0.41 0.41 0.37CMU-COMBO-HYPOSEL 0.71 0.28 0.26 ?0.14 0.33 0.35 0.53 0.57 0.61 0.43 7.40 7.15 0.1 0.08 0.31 0.33 0.34 0.42 0.4 0.35CMU-STATXFER 0.58 0.24 0.23 ?0.18 0.29 0.31 0.49 0.54 0.58 0.40 6.89 6.75 0.08 0.07 0.38 0.42 0.31 0.46 0.37 0.32COLUMBIA 0.50 0.23 0.22 ?0.18 0.29 0.30 0.49 0.54 0.58 0.40 6.85 6.68 0.07 0.07 0.36 0.39 0.31 0.46 0.36 0.31DCU 0.66 0.27 0.25 ?0.15 0.32 0.34 0.52 0.56 0.61 0.42 7.29 6.94 0.09 0.07 0.32 0.34 0.33 0.43 0.38 0.34DCU-COMBO 0.67 0.31 0.31 ?0.11 0.36 0.37 0.54 0.59 0.64 0.44 7.84 7.69 0.14 0.12 0.21 0.22 0.35 0.41 0.42 0.38GENEVA 0.34 0.14 0.14 ?0.29 0.21 0.22 0.43 0.49 0.52 0.36 5.32 5.15 0.05 0.05 0.54 0.52 0.26 0.53 0.29 0.22GOOGLE 0.76 0.31 0.30 ?0.10 0.36 0.37 0.54 0.58 0.63 0.44 8 7.84 0.17 0.13 0.17 0.2 0.36 0.41 0.42 0.38JHU 0.62 0.27 0.23 ?0.15 0.32 0.33 0.51 0.56 0.6 0.41 7.23 6.68 0.08 0.05 0.33 0.36 0.32 0.43 0.37 0.32LIMSI 0.65 0.26 0.25 ?0.16 0.30 0.32 0.51 0.56 0.60 0.42 7.02 6.87 0.09 0.07 0.35 0.36 0.33 0.44 0.38 0.33LIUM-SYSTRAN 0.60 0.27 0.26 ?0.15 0.32 0.33 0.51 0.56 0.60 0.42 7.26 7.10 0.10 0.06 0.33 0.36 0.33 0.43 0.39 0.35RBMT1 0.56 0.18 0.18 ?0.25 0.24 0.25 0.48 0.53 0.57 0.4 5.89 5.73 0.07 0.06 0.51 0.45 0.3 0.50 0.34 0.26RBMT3 0.54 0.2 0.19 ?0.22 0.25 0.27 0.48 0.53 0.56 0.39 6.12 5.96 0.07 0.06 0.45 0.45 0.30 0.49 0.35 0.28RBMT4 0.47 0.19 0.18 ?0.24 0.24 0.26 0.48 0.52 0.56 0.39 5.97 5.83 0.07 0.06 0.46 0.45 0.3 0.49 0.34 0.27RBMT5 0.59 0.19 0.19 ?0.24 0.25 0.26 0.49 0.54 0.57 0.40 6.03 5.9 0.09 0.07 0.46 0.43 0.31 0.49 0.35 0.28RWTH 0.52 0.25 0.24 ?0.16 0.30 0.32 0.5 0.55 0.59 0.40 7.09 6.94 0.07 0.03 0.35 0.39 0.32 0.44 0.38 0.32UEDIN 0.61 0.25 0.24 ?0.16 0.31 0.32 0.50 0.55 0.59 0.41 7.04 6.85 0.08 0.04 0.35 0.38 0.32 0.44 0.38 0.33UKA 0.61 0.26 0.25 ?0.15 0.31 0.33 0.51 0.55 0.6 0.41 7.17 7.00 0.08 0.04 0.34 0.37 0.32 0.44 0.38 0.34USAAR 0.55 0.19 0.18 ?0.24 0.24 0.26 0.48 0.54 0.57 0.4 6.08 5.92 0.07 0.06 0.46 0.44 0.3 0.49 0.34 0.26USAAR-COMBO 0.57 0.26 0.25 ?0.16 0.31 0.33 0.51 0.55 0.59 0.41 7.13 6.85 0.08 0.02 0.33 0.35 0.32 0.44 0.38 0.33Czech-English News TaskBBN-COMBO 0.65 0.22 0.20 ?0.19 0.27 0.29 0.47 0.52 0.56 0.39 6.74 6.45 0.24 0.3 0.52 0.60 0.29 0.47 0.34 0.29CMU-COMBO 0.73 0.22 0.20 ?0.2 0.27 0.29 0.47 0.53 0.57 0.39 6.72 6.46 0.34 0.34 0.53 0.60 0.29 0.47 0.35 0.29CU-BOJAR 0.51 0.16 0.15 ?0.26 0.22 0.24 0.43 0.5 0.52 0.36 5.84 5.54 0.26 0.28 0.61 0.69 0.26 0.52 0.31 0.24GOOGLE 0.75 0.21 0.20 ?0.19 0.26 0.28 0.46 0.52 0.55 0.38 6.82 6.61 0.32 0.33 0.53 0.62 0.29 0.47 0.35 0.28UEDIN 0.57 0.2 0.19 ?0.23 0.25 0.27 0.45 0.50 0.54 0.37 6.2 6 0.22 0.25 0.56 0.63 0.27 0.49 0.33 0.27Hungarian-English News TaskBBN-COMBO 0.54 0.14 0.13 ?0.29 0.19 0.21 0.38 0.45 0.46 0.32 5.46 5.2 0.16 0.18 0.71 0.83 0.23 0.55 0.27 0.2CMU-COMBO 0.62 0.14 0.13 ?0.29 0.19 0.21 0.39 0.46 0.47 0.32 5.52 5.24 0.28 0.22 0.71 0.82 0.23 0.55 0.28 0.2CMU-COMBO-HYPOSEL 0.68 0.14 0.12 ?0.29 0.19 0.21 0.39 0.45 0.46 0.32 5.51 5.16 0.25 0.25 0.71 0.82 0.23 0.55 0.27 0.2MORPHO 0.75 0.1 0.09 ?0.36 0.15 0.17 0.39 0.45 0.46 0.32 4.75 4.55 0.34 0.49 0.79 0.83 0.23 0.6 0.26 0.17UEDIN 0.45 0.12 0.11 ?0.32 0.18 0.19 0.37 0.42 0.43 0.30 4.95 4.74 0.12 0.12 0.75 0.87 0.21 0.58 0.27 0.19UMD 0.66 0.13 0.12 ?0.28 0.18 0.2 0.36 0.44 0.45 0.30 5.41 5.12 0.21 0.13 0.68 0.85 0.22 0.55 0.27 0.18Table 25: Automatic evaluation metric scores for translations into English27RANKBLEUBLEU-CASEDBLEU-TERBLEUSPBLEUSP4114NISTNIST-CASEDTERTERPWCD6P4ERWPFWPBLEUEnglish-German News TaskGOOGLE 0.54 0.15 0.14 ?0.29 0.20 0.22 5.36 5.25 0.62 0.74 0.54 0.3 0.23LIU 0.49 0.14 0.13 ?0.29 0.2 0.21 5.35 5.18 0.65 0.78 0.54 0.3 0.23RBMT1 0.57 0.11 0.11 ?0.32 0.17 0.19 4.69 4.59 0.67 0.81 0.57 0.28 0.21RBMT2 0.66 0.13 0.13 ?0.30 0.19 0.21 5.08 4.99 0.62 0.75 0.55 0.30 0.23RBMT3 0.64 0.12 0.12 ?0.29 0.2 0.21 4.8 4.71 0.62 0.76 0.54 0.31 0.25RBMT4 0.58 0.11 0.10 ?0.33 0.17 0.18 4.66 4.57 0.7 0.84 0.57 0.27 0.2RBMT5 0.64 0.13 0.12 ?0.3 0.19 0.20 5.03 4.94 0.64 0.79 0.55 0.3 0.23RWTH 0.48 0.14 0.13 ?0.28 0.2 0.21 5.51 5.41 0.62 0.78 0.53 0.3 0.23STUTTGART 0.43 0.12 0.12 ?0.31 0.18 0.20 5.06 4.82 0.67 0.79 0.55 0.29 0.21UEDIN 0.51 0.15 0.15 ?0.27 0.21 0.23 5.53 5.42 0.63 0.77 0.53 0.31 0.24UKA 0.54 0.15 0.15 ?0.27 0.21 0.22 5.6 5.48 0.62 0.75 0.52 0.31 0.24USAAR 0.58 0.12 0.11 ?0.33 0.18 0.19 4.83 4.71 0.69 0.8 0.57 0.28 0.21USAAR-COMBO 0.52 0.16 0.15 ?0.27 0.21 0.23 5.6 5.39 0.62 0.75 0.52 0.31 0.24English-Spanish News TaskGOOGLE 0.65 0.28 0.27 ?0.15 0.33 0.34 7.27 7.07 0.36 0.42 0.42 0.37 0.31NUS 0.59 0.25 0.23 ?0.17 0.30 0.31 6.96 6.67 0.48 0.59 0.44 0.34 0.28RBMT1 0.25 0.15 0.14 ?0.27 0.20 0.22 5.32 5.17 0.55 0.66 0.51 0.24 0.16RBMT3 0.66 0.18 0.17 ?0.18 0.28 0.3 5.79 5.63 0.49 0.59 0.45 0.33 0.27RBMT4 0.61 0.21 0.2 ?0.20 0.26 0.28 6.47 6.28 0.52 0.64 0.47 0.31 0.25RBMT5 0.64 0.22 0.21 ?0.2 0.27 0.29 6.53 6.34 0.52 0.64 0.46 0.32 0.26RWTH 0.51 0.22 0.21 ?0.18 0.27 0.29 6.83 6.63 0.50 0.65 0.46 0.32 0.26TALP-UPC 0.58 0.25 0.23 ?0.17 0.3 0.31 6.96 6.69 0.47 0.58 0.44 0.34 0.28UEDIN 0.66 0.25 0.24 ?0.17 0.30 0.31 6.94 6.73 0.48 0.59 0.44 0.34 0.29USAAR 0.48 0.20 0.19 ?0.21 0.26 0.27 6.36 6.16 0.54 0.66 0.47 0.30 0.24USAAR-COMBO 0.61 0.28 0.26 ?0.14 0.33 0.34 7.36 6.97 0.39 0.48 0.42 0.36 0.31English-French News TaskDCU 0.65 0.24 0.22 ?0.19 0.29 0.30 6.69 6.39 0.63 0.72 0.47 0.38 0.34DCU-COMBO 0.74 0.28 0.27 ?0.15 0.33 0.34 7.29 7.12 0.58 0.67 0.44 0.42 0.38GENEVA 0.38 0.15 0.14 ?0.27 0.20 0.22 5.59 5.39 0.68 0.82 0.53 0.32 0.25GOOGLE 0.68 0.25 0.24 ?0.17 0.30 0.31 6.90 6.71 0.62 0.7 0.46 0.40 0.36LIMSI 0.64 0.25 0.24 ?0.17 0.3 0.31 6.94 6.77 0.60 0.71 0.46 0.4 0.35LIUM-SYSTRAN 0.73 0.26 0.24 ?0.17 0.31 0.32 7.02 6.83 0.61 0.71 0.45 0.40 0.36RBMT1 0.54 0.18 0.17 ?0.23 0.24 0.26 6.12 5.96 0.65 0.76 0.5 0.35 0.29RBMT3 0.65 0.22 0.20 ?0.20 0.27 0.28 6.48 6.29 0.63 0.72 0.48 0.38 0.33RBMT4 0.59 0.18 0.17 ?0.24 0.24 0.25 6.02 5.86 0.66 0.77 0.50 0.35 0.3RBMT5 0.57 0.20 0.19 ?0.21 0.26 0.27 6.31 6.15 0.63 0.74 0.49 0.36 0.31RWTH 0.58 0.22 0.21 ?0.19 0.27 0.28 6.67 6.51 0.62 0.75 0.48 0.38 0.32SYSTRAN 0.65 0.23 0.22 ?0.19 0.28 0.29 6.7 6.47 0.63 0.74 0.47 0.39 0.34UEDIN 0.60 0.24 0.23 ?0.18 0.29 0.30 6.75 6.57 0.62 0.71 0.47 0.39 0.35UKA 0.66 0.24 0.23 ?0.18 0.29 0.30 6.82 6.65 0.61 0.71 0.46 0.39 0.35USAAR 0.48 0.19 0.18 ?0.23 0.24 0.26 6.16 5.98 0.66 0.76 0.5 0.34 0.29USAAR-COMBO 0.77 0.27 0.25 ?0.15 0.32 0.33 7.24 6.93 0.59 0.69 0.44 0.41 0.37English-Czech News TaskCU-BOJAR 0.61 0.14 0.13 ?0.28 0.21 0.23 5.18 4.96 0.63 0.82 0.01 n/a n/aCU-TECTOMT 0.48 0.07 0.07 ?0.35 0.14 0.16 4.17 4.03 0.71 0.96 0.01 n/a n/aEUROTRANXP 0.67 0.1 0.09 ?0.33 0.16 0.18 4.38 4.26 0.7 0.93 0.01 n/a n/aGOOGLE 0.66 0.14 0.13 ?0.30 0.20 0.22 4.96 4.84 0.66 0.82 0.01 n/a n/aPCTRANS 0.67 0.09 0.09 ?0.34 0.17 0.18 4.34 4.19 0.71 0.90 0.01 n/a n/aUEDIN 0.53 0.14 0.13 ?0.29 0.21 0.22 5.04 4.9 0.64 0.84 0.01 n/a n/aEnglish-Hungarian News TaskMORPHO 0.79 0.08 0.08 ?0.37 0.15 0.16 4.04 3.92 0.83 1 0.6 n/a n/aUEDIN 0.32 0.1 0.09 ?0.33 0.17 0.18 4.48 4.32 0.78 1 0.56 n/a n/aTable 26: Automatic evaluation metric scores for translations out of English28
