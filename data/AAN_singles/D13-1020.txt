Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 193?203,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsMCTest: A Challenge Dataset for the Open-DomainMachine Comprehension of TextMatthew RichardsonMicrosoft ResearchOne Microsoft WayRedmond, WA 98052mattri@microsoft.comChristopher J.C. BurgesMicrosoft ResearchOne Microsoft WayRedmond, WA 98052cburges@microsoft.comErin RenshawMicrosoft ResearchOne Microsoft WayRedmond, WA 98052erinren@microsoft.comAbstractWe present MCTest, a freely available set ofstories and associated questions intended forresearch on the machine comprehension oftext.
Previous work on machine comprehen-sion (e.g., semantic modeling) has made greatstrides, but primarily focuses either on lim-ited-domain datasets, or on solving a more re-stricted goal (e.g., open-domain relationextraction).
In contrast, MCTest requires ma-chines to answer multiple-choice readingcomprehension questions about fictional sto-ries, directly tackling the high-level goal ofopen-domain machine comprehension.
Read-ing comprehension can test advanced abilitiessuch as causal reasoning and understandingthe world, yet, by being multiple-choice, stillprovide a clear metric.
By being fictional, theanswer typically can be found only in the sto-ry itself.
The stories and questions are alsocarefully limited to those a young child wouldunderstand, reducing the world knowledgethat is required for the task.
We present thescalable crowd-sourcing methods that allowus to cheaply construct a dataset of 500 storiesand 2000 questions.
By screening workers(with grammar tests) and stories (with grad-ing), we have ensured that the data is the samequality as another set that we manually edited,but at one tenth the editing cost.
By beingopen-domain, yet carefully restricted, we hopeMCTest will serve to encourage research andprovide a clear metric for advancement on themachine comprehension of text.1 Reading ComprehensionA major goal for NLP is for machines to be able tounderstand text as well as people.
Several researchdisciplines are focused on this problem: for exam-ple, information extraction, relation extraction,semantic role labeling, and recognizing textual en-tailment.
Yet these techniques are necessarilyevaluated individually, rather than by how muchthey advance us towards the end goal.
On the otherhand, the goal of semantic parsing is the machinecomprehension of text (MCT), yet its evaluationrequires adherence to a specific knowledge repre-sentation, and it is currently unclear what the bestrepresentation is, for open-domain text.We believe that it is useful to directly tackle thetop-level task of MCT.
For this, we need a way tomeasure progress.
One common method for evalu-ating someone?s understanding of text is by givingthem a multiple-choice reading comprehensiontest.
This has the advantage that it is objectivelygradable (vs. essays) yet may test a range of abili-ties such as causal or counterfactual reasoning,inference among relations, or just basic under-standing of the world in which the passage is set.Therefore, we propose a multiple-choice readingcomprehension task as a way to evaluate progresson MCT.
We have built a reading comprehensiondataset containing 500 fictional stories, with 4 mul-tiple choice questions per story.
It was built usingmethods which can easily scale to at least 5000stories, since the stories were created, and the cura-tion was done, using crowd sourcing almost entire-ly, at a total of $4.00 per story.
We plan to perio-dically update the dataset to ensure that methodsare not overfitting to the existing data.
The datasetis open-domain, yet restricted to concepts andwords that a 7 year old is expected to understand.This task is still beyond the capability of today?scomputers and algorithms.193By restricting the concept space, we gain the dif-ficulty of being an open-domain problem, withoutthe full complexity of the real world (for example,there will be no need for the machine to understandpolitics, technology, or to have any domain specif-ic expertise).
The multiple choice task avoids am-biguities (such as when the task is to find asentence that best matches a question, as in someearly reading comprehension tasks: see Section 2),and also avoids the need for additional grading,such as is needed in some TREC tasks.
The storieswere chosen to be fictional to focus work on find-ing the answer in the story itself, rather than inknowledge repositories such as Wikipedia; the goalis to build technology that actually understandsstories and paragraphs on a deep level (as opposedto using information retrieval methods and the re-dundancy of the web to find the answers).We chose to use crowd sourcing, as opposed to,for example, contracting teachers or paying forexisting standardized tests, for three reasons,namely: (1) scalability, both for the sizes of da-tasets we can provide, and also for the ease of reg-ularly refreshing the data; (2) for the variety instory-telling that having many different authorsbrings; and (3) for the free availability that can on-ly result from providing non-copyrighted data.
Thecontent is freely available at http://research.micro-soft.com/mct, and we plan to use that site to trackpublished results and provide other resources, suchas labels of various kinds.2 Previous WorkThe research goal of mapping text to meaning rep-resentations in order to solve particular tasks has along history.
DARPA introduced the Airline Trav-el Information System (ATIS) in the early 90?s:there the task was to slot-fill flight-related infor-mation by modeling the intent of spoken language(see Tur et al 2010, for a review).
This data con-tinues to be a used in the semantic modeling com-munity (see, for example, Zettlemoyer and Collins,2009).
The Geoquery database contains 880 geo-graphical facts about the US and has played a simi-lar role for written (as opposed to spoken) naturallanguage queries against a database (Zelle andMooney, 1996) and it also continues to spur re-search (see for example Goldwasser et al 2011),as does the similar Jobs database, which providesmappings of 640 sentences to a listing of jobs(Tang and Mooney, 2001).
More recently, Zweigand Burges (2012) provided a set of 1040 sentenc-es that comprise an SAT-style multiple choice sen-tence completion task.The idea of using story-based reading compre-hension questions to evaluate methods for machinereading itself goes back over a decade, whenHirschmann et al(1999) showed that a bag ofwords approach, together with some heuristic lin-guistic modeling, could achieve 40% accuracy forthe task of picking the sentence that best matchesthe query for ?who / what / when / where / why?questions, on a small reading comprehension da-taset from Remedia.
This dataset spurred severalresearch efforts, for example using reinforcementlearning (Grois and Wilkins, 2005), named entityresolution (Harabagiu et al 2003) and mappingquestions and answers to logical form (Wellner etal., 2006).
Work on story understanding itself goesback much further, to 1972, when Charniak pro-posed using a background model to answer ques-tions about children?s stories.
Similarly, the TREC(and TAC) Question Answering tracks (e.g., Voor-hees and Tice, 1999) aim to evaluate systems ontheir ability to answer factual questions such as?Where is the Taj Mahal?.
The QA4MRE task alsoaims to evaluate machine reading systems throughquestion answering (e.g., Clark et al 2012).
Earli-er work has also aimed at controlling the scope bylimiting the text to children?s stories: Breck et al(2001) collected 75 stories from the CanadianBroadcasting Corporation?s web site for children,and generated 650 questions for them manually,where each question was answered by a sentencein the text.
Leidner et al(2003) both enriched theCBC4kids data by adding several layers of annota-tion (such as semantic and POS tags), and meas-ured QA performance as a function of questiondifficulty.
For a further compendium of resourcesrelated to the story comprehension task, seeMueller (2010).The task proposed here differs from the abovework in several ways.
Most importantly, the datacollection is scalable: if the dataset proves suffi-ciently useful to others, it would be straightforwardto gather an order of magnitude more.
Even thedataset size presented here is an order of magni-tude larger than the Remedia or the CBC4kids dataand many times larger than QA4MRE.
Second, themultiple choice task presents less ambiguity (and isconsequently easier to collect data for) than the194task of finding the most appropriate sentence, andmay be automatically evaluated.
Further, our sto-ries are fictional, which means that the informationto answer the question is contained only in the sto-ry itself (as opposed to being able to directly lever-age knowledge repositories such as Wikipedia).This design was chosen to focus the task on themachine understanding of short passages, ratherthan the ability to match against an existingknowledge base.
In addition, while in theCBC4kids data each answer was a sentence fromthe story, here we required that approximately halfof the questions require at least two sentences fromthe text to answer; being able to control complexityin this way is a further benefit of using multiplechoice answers.
Finally, as explained in Section 1,the use of free-form input makes the problem opendomain (as opposed to the ATIS, Geoquery andJobs data), leading to the hope that solutions to thetask presented here will be easier to apply to novel,unrelated tasks.3 Generating the Stories and QuestionsOur aim was to generate a corpus of fictional storysets1 that could be scaled with as little expert inputas possible.
Thus, we designed the process to begated by cost, and keeping the costs low was ahigh priority.
Crowd-sourcing seemed particularlyappropriate, given the nature of the task, so weopted to use Amazon Mechanical Turk2 (AMT).With over 500,000 workers3, it provides the workforce required to both achieve scalability and,equally importantly, to provide diversity in the sto-ries and types of questions.
We restricted our taskto AMT workers (workers) residing in the UnitedStates.
The average worker is 36 years old, moreeducated than the United States population in gen-eral (Paolacci et al 2010), and the majority ofworkers are female.3.1 The Story and QuestionsWorkers were instructed to write a short (150-300words) fictional story, and to write as if for a childin grade school.
The choice of 150-300 was madeto keep the task an appropriate size for workerswhile still allowing for complex stories and ques-tions.
The workers were free to write about anytopic they desired (as long as it was appropriate fora young child), and so there is a wide range, in-cluding vacations, animals, school, cars, eating,gardening, fairy tales, spaceships, and cowboys.1 We use the term ?story set?
to denote the fictional storytogether with its multiple choice questions, hypothetical an-swers, and correct answer labels.2 http://www.mturk.com3 https://requester.mturk.com/tourJames the Turtle was always getting in trouble.Sometimes he'd reach into the freezer and empty outall the food.
Other times he'd sled on the deck and geta splinter.
His aunt Jane tried as hard as she could tokeep him out of trouble, but he was sneaky and gotinto lots of trouble behind her back.One day, James thought he would go into town andsee what kind of trouble he could get into.
He went tothe grocery store and pulled all the pudding off theshelves and ate two jars.
Then he walked to the fastfood restaurant and ordered 15 bags of fries.
He did-n't pay, and instead headed home.His aunt was waiting for him in his room.
She toldJames that she loved him, but he would have to startacting like a well-behaved turtle.After about a month, and after getting into lots oftrouble, James finally made up his mind to be a betterturtle.1) What is the name of the trouble making turtle?A) FriesB) PuddingC) JamesD) Jane2) What did James pull off of the shelves in the gro-cery store?A) puddingB) friesC) foodD) splinters3) Where did James go after he went to the grocerystore?A) his deckB) his freezerC) a fast food restaurantD) his room4) What did James do after he ordered the fries?A) went to the grocery storeB) went home without payingC) ate themD) made up his mind to be a better turtleFigure 1.
Sample Story and Questions (chosen random-ly from MC500 train set).195Workers were also asked to provide four readingcomprehension questions pertaining to their storyand, for each, four multiple-choice answers.
Com-ing up with incorrect alternatives (distractors) is adifficult task (see, e.g., Agarwal, 2011) but work-ers were requested to provide ?reasonable?
incor-rect answers that at least include words from thestory so that their solution is not trivial.
For exam-ple, for the question ?What is the name of thedog?
?, if only one of the four answers occurs in thestory, then that answer must be the correct one.Finally, workers were asked to design theirquestions and answers such that at least two of thefour questions required multiple sentences from thestory to answer them.
That is, for those questions itshould not be possible to find the answer in anyindividual sentence.
The motivation for this was toensure that the task could not be fully solved usinglexical techniques, such as word matching, alone.Whilst it is still possible that a sophisticated lexicalanalysis could completely solve the task, requiringthat answers be constructed from at least two dif-ferent sentences in the story makes this much lesslikely; our hope is that the solution will insteadrequire some inference and some form of limitedreasoning.
This hope rests in part upon the obser-vation that standardized reading comprehensiontests, whose goal after all is to test comprehension,generally avoid questions that can be answered byreading a single sentence.3.2 Automatic ValidationBesides verifying that the story and all of the ques-tions and answers were provided, we performedthe following automatic validation before allowingthe worker to complete the task:Limited vocabulary: The lowercase words in thestory, questions, and answers were stemmed andchecked against a vocabulary list of approximately8000 words that a 7-year old is likely to know(Kuperman et al 2012).
Any words not on the listwere highlighted in red as the worker typed, andthe task could not be submitted unless all of thewords satisfied this vocabulary criterion.
To allowthe use of arbitrary proper nouns, capitalized wordswere not checked against the vocabulary list.Multiple-sentence questions: As described earli-er, we required that at least two of the questionsneed multiple sentences to answer.
Workers weresimply asked to mark whether a question needs oneor multiple sentences and we required that at leasttwo are marked as multiple.3.3 The WorkersWorkers were required to reside in the UnitedStates and to have completed 100 HITs with anover 95% approval rate4.
The median worker took22 minutes to complete the task.
We paid workers$2.50 per story set and allowed each to do a maxi-mum of 8 tasks (5 in MC500).
We did not experi-ment with paying less, but this rate amounts to$6.82/hour, which is approximately the rate paidby other writing tasks on AMT at the time, thoughis also significantly higher than the median wageof $1.38 found in 2010 (Horton and Chilton,2010).
Workers could optionally leave feedback onthe task, which was overwhelmingly positive ?
themost frequent non-stopword in the comments was?fun?
and the most frequent phrase was ?thankyou?.
The only negative comments (in <1% ofsubmissions) were when the worker felt that a par-ticular word should have been on the allowed vo-cabulary list.
Given the positive feedback, it maybe possible to pay less if we collect more data inthe future.
We did not enforce story length con-straints, but some workers interpreted our sugges-tion that the story be 150-300 words as a hardconstraint, and some asked to be able to write alonger story.The MCTest corpus contains two sets of stories,named MC160 and MC500, and containing 160and 500 stories respectively.
MC160 was gatheredfirst, then some improvements were made beforegathering MC500.
We give details on the differ-ences between these two sets below.3.4 MC160: Manually Curated for QualityIn addition to the details described above, MC160workers were given a target elementary gradeschool level (1-4) and a sample story matching thatgrade level5.
The intent was to produce a set ofstories and questions that varied in difficulty sothat research work can progress grade-by-grade ifneeded.
However, we found little difference be-tween grades in the corpus..After gathering the stories, we manually curatedthe MC160 corpus by reading each story set and4 The latter two are the default AMT requirements.5 From http://www.englishforeveryone.org/.196correcting errors.
The most common mistakes weregrammatical, though occasionally questions and/oranswers needed to be fixed.
66% of the storieshave at least one correction.
We provide both thecurated and original corpuses in order to allow re-search on reading comprehension in the presenceof grammar, spelling, and other mistakes.3.5 MC500: Adding a Grammar TestThough the construction of MC160 was successful,it requires a costly curation process which will notscale to larger data sets (although the curation wasuseful, both for improving the design of MC500,and for assessing the effectiveness of automatedcuration techniques).
To more fully automate theprocess, we added two more stages: (1) A grammartest that automatically pre-screens workers forwriting ability, and (2) a second Mechanical Turktask whereby new workers take the reading com-prehension tests and rate their quality.
We will dis-cuss stage (2) in the next section.The grammar test consisted of 20 sentences, halfof which had one grammatical error (see Figure 2).The incorrect sentences were written using com-mon errors such as you?re vs. your, using ?s to in-dicate plurality, incorrect use of tense, it?s vs. its,less vs. fewer, I vs. me, etc.
Workers were requiredto indicate for each sentence whether it wasgrammatically correct or not, and had to pass withat least 80% accuracy in order to qualify for thetask.
The 80% threshold was chosen to trade offworker quality with the rate at which the taskswould be completed; initial experiments using athreshold of 90% indicated that collecting 500 sto-ries would take many weeks instead of days.
Notethat each worker is allowed to write at most 5stores, so we required at least 100 workers to passthe qualification test.To validate the use of the qualification test, wegathered 30 stories requiring the test (qual) and 30stories without.
We selected a random set of 20stories (10 from each), hid their origin, and thengraded the overall quality of the story and ques-tions from 1-5, meaning do not attempt to fix, badbut rescuable, has non-minor problems, has onlyminor problems, and has no problems, respective-ly.
Results are shown in Table 1.
The difference isstatistically significant (p<0.05, using the two-tailed t-test).
The qual stories were also more di-verse, with fewer of them about animals (the mostcommon topic).Additional Modifications: Based on our experi-ence curating MC160, we also made the followingmodifications to the task.
In order to eliminate triv-ially-answerable questions, we required that eachanswer be unique, and that either the correct an-swer did not appear in the story or, if it did appear,that at least two of the incorrect answers also ap-peared in the story.
This is to prevent questionsthat are trivially answered by checking which an-swer appears in the story.
The condition on wheth-er the correct answer appears is to allow questionssuch as ?How many candies did Susan eat?
?,where the total may never appear in the story, eventhough the information needed to derive it does.An answer is considered to appear in the story if atleast half (rounded down) of its non-stopword1.
We went to visit the Smith?s at their house.2.
I altered their suits for them.3.
You're car is very old.4.
Jim likes to run, hike, and going kayaking.5.
He should of come to work on time.6.
I think its best to wash lots of apples.7.
Are people who write "ping" thinking of subma-rines?8.
Smoke filled the room, making it hard to breathe.9.
Alert yet alf - that's you.10.
They wanted they're money back.11.
Hawks and eagles like to fly high in the sky.12.
Don't let her wear them down.13.
The cat particularly liked the greasy plate.14.
The company is less successful because we haveless employees.15.
The hamster belongs to Sam and I.16.
No one landed on the air strip today.17.
He was very effected by her tears.18.
You are a tired piece of toast, metaphoricallyspeaking.19.
Anne plays bass and sings.20.
Him and me met at the park.Figure 2.
Grammar test for qualifying workers.Quality(1-5)AboutanimalsNo Grammar Test 3.2 73%Grammar Test 4.3  30%Table 1.
Pre-screening workers using a grammar testimproves both quality and diversity of stories.
Bothdifferences are significant using the two-tailed t-test(p<0.05 for quality and p<0.01 for animals).197terms appear in the story (ignoring word endings).This check is done automatically and must be satis-fied before the worker is able to complete the task.Workers could also bypass the check if they felt itwas incorrect, by adding a special term to theiranswer.We were also concerned that the sample storymight bias the workers when writing the story set,particularly when designing questions that requiremultiple sentences to answer.
So, we removed thesample story and grade level from the task.Finally, in order to encourage more diversity ofstories, we added creativity terms, a set of 15nouns chosen at random from the allowed vocabu-lary set.
Workers were asked to ?please consider?using one or more of the terms in their story, butuse of the words was strictly optional.
On average,workers used 3.9 of the creativity terms in theirstories.4 Rating the Stories and QuestionsIn this section we discuss the crowd-sourced ratingof story sets.
We wished to ensure story set qualitydespite the fact that MC500 was only minimallymanually curated (see below).
Pre-qualifyingworkers with a grammar test was one step of thisprocess.
The second step was to have additionalworkers on Mechanical Turk both evaluate eachstory and take its corresponding test.
Each storywas evaluated in this way by 10 workers, each ofwhom provided scores for each of age-appropriateness (yes/maybe/no), grammaticality(few/some/many errors), and story clarity (excel-lent/reasonable/poor).
When answering the fourreading comprehension questions, workers couldalso mark a question as ?unclear?.
Each story setwas rated by 10 workers who were each paid $0.15per set.Since we know the purportedly correct answer,we can estimate worker quality by measuring whatfraction of questions that worker got right.
Work-ers with less than 80% accuracy (ignoring thosequestions marked as unclear) were removed fromthe set.
This constituted just 4.1% of the raters and4.2% of the judgments (see Figure 3).
Only onerater appeared to be an intentional spammer, an-swering 1056 questions with only 29% accuracy.The others primarily judged only one story.
Onlyone worker fell between, answering 336 questionswith just 75% accuracy.For the remaining workers (those who achievedat least 80% accuracy), we measured median storyappropriateness, grammar, and clarity.
For eachcategory, stories for which less than half of theratings were the best possible (e.g., excellent storyclarity) were inspected and optionally removedfrom the data set.
This required inspecting 40(<10%) of the stories, only 2 of which weredeemed poor enough to be removed (both of whichhad over half of the ratings all the way at the bot-tom end of the scale, indicating we could potential-ly have inspected many fewer stories with the sameresults).
We also inspected questions for which atleast 5 workers answered incorrectly, or answered?unclear?.
In total, 29 questions (<2%) were in-spected.
5 were fixed by changing the question, 8by changing the answers, 2 by changing both, 6 bychanging the story, and 8 were left unmodified.Note that while not fully automated, this processof inspecting stories and repairing questions tookone person one day, so is still scalable to at least anorder of magnitude more stories.5 Dataset AnalysisIn Table 2, we present results demonstrating thevalue of the grammar test and curation process.
Asexpected, manually curating MC160 resulted inincreased grammar quality and percent of ques-tions answered correctly by raters.
The goal ofMC500 was to find a more scalable method toachieve the same quality as the curated MC160.
AsTable 2 shows, the grammar test improved storygrammar quality from 1.70 to 1.77 (both uncurat-ed).
The rating and one-day curation process in-Figure 3.
Just 4.1% of raters had an accuracy below80% (constituting 4.2% of the judgments).198Set AgeAp Clarity Grammar Correct160  1.88 1.63 1.70 95.3500 1.92 1.65 1.77 95.3500 curated 1.94 1.71 1.79 96.9160 curated 1.91 1.67 1.84?97.7Table 2.
Average age appropriateness, story clarity,grammar quality (0-2, with 2 being best), and percent ofquestions answered correctly by raters, for the originaland curated versions of the data.
Bold indicates statisti-cal significance vs. the original version of the same set,using the two-sample t-test with unequal variance.
The ?indicates the only statistical difference between 500curated and 160 curated.Baseline AlgorithmsRequire: Passage P, set of passage words PW, ith word inpassage Pi, set of words in question Q, set of words inhypothesized answers A1..4, and set of stop words U,Define:  ( )  ?
(    )Define:   ( )     (( )).Algorithm 1 Sliding Windowfor i = 1 to 4 do| |?
{(    )| |end forreturnAlgorithm 2 Distance Basedfor i = 1 to 4 do(    )((     )   )if |  |    or |   |else| |(   ),where   (   ) is the minimum number ofwords between an occurrence of q and anoccurrence of a in P, plus one.end ifend forreturnAlgorithm SWReturnAlgorithm SW+DReturnFigure 4.
The two lexical-based algorithms used for thebaselines.creases this to 1.79, whereas a fully manual cura-tion results in a score of 1.84.
Curation also im-proved the percent of questions answered correctlyfor both MC160 and MC500, but, unlike withgrammar, there is no significant difference be-tween the two curated sets.
Indeed, the only statis-tically significant difference between the two is ingrammar.
So, the MC500 grammar test and cura-tion process is a very scalable method for collect-ing stories of nearly the quality of the costlymanual curation of MC160.We also computed correlations between thesemeasures of quality and various factors such asstory length and time spent writing the story.
OnMC500, there is a mild correlation between aworker?s grammar test score and the judgedgrammar quality of that worker?s story (correlationof 0.24).
Interestingly, this relation disappearedonce MC500 was curated, likely due to repairingthe stories with the worst grammar.
On MC160,there is a mild correlation between the clarity andthe number of words in the question and answer(0.20 and 0.18).
All other correlations were below0.15.
These factors could be integrated into an es-timate for age-appropriateness, clarity, and gram-mar, potentially reducing the need for raters.Table 3 provides statistics on each corpus.MC160 and MC500 are similar in average numberof words per story, question, and answer, as well asthe median writing time.
The most commonly usednouns in MC500 are: day, friend, time, home,house, mother, dog, mom, school, dad, cat, tree,and boy.
The stories vary widely in theme.
Thefirst 10 stories of the randomly-ordered MC500 setare about: travelling to Miami to visit friends, wak-ing up and saying hello to pets, a bully on aschoolyard, visiting a farm, collecting insects atGrandpa?s house, planning a friend?s birthday par-ty, selecting clothes for a school dance, keepinganimals from eating your ice cream, animals order-ing food, and adventures of a boy and his dog.Corpus Stories MedianwritingtimeAverage Words Per:Story Question AnswerMC160 160 26 min 204 8.0 3.4MC500 500 20 min 212 7.7 3.4Table 3.
Corpus statistics for MC160 and MC500.199We randomly divided MC160 and MC500 intotrain, development, and test sets of 70, 30, and 60stories and 300, 50, and 150 stories, respectively.6 Baseline System and ResultsWe wrote two baseline systems, both using onlysimple lexical features.
The first system used asliding window, matching a bag of words con-structed from the question and hypothesized an-swer to the text.
Since this ignored long rangedependencies, we added a second, word-distancebased algorithm.
The distance-based score wassimply subtracted from the window-based score toarrive at the final score (we tried scaling the dis-tance score before subtraction but this did not im-prove results on the MC160 train set).
Thealgorithms are summarized in Figure 4.
A coin flipis used to break ties.
The use of inverse wordcounts was inspired by TF-IDF.Results for MC160 and MC500 are shown inTable 4 and Table 5.
The MC160 train and devel-opment sets were used for tuning.
The baselinealgorithm was authored without seeing any portionof MC500, so both the MC160 test set and all ofMC500 were used for testing (although we never-theless report results on the train/test split).
Notethat adding the distance based algorithm improvedaccuracy by approximately 10% absolute onMC160 and approximately 6% on MC500.
Over-all, error rates on MC500 are higher than onMC160, which agrees with human performance(see Table 2), suggesting that MC500?s questionsare more difficult.7 Recognizing Textual Entailment ResultsWe also tried using a ?recognizing textual entail-ment?
(RTE) system to answer MCTest questions.The goal of RTE (Dagan et al 2005) is to deter-mine whether a given statement can be inferredfrom a particular text.
We can cast MCTest as anRTE task by converting each question-answer pairinto a statement, and then selecting the answerwhose statement has the highest likelihood of be-ing entailed by the story.
For example, in the sam-ple story given in Figure 1, the second question canbe converted into four statements (one for eachanswer), and the RTE system should select thestatement ?James pulled pudding off of the shelvesin the grocery store?
as the most likely one.For converting question-answer pairs to state-ments, we used the rules employed in a web-basedquestion answering system (Cucerzan andAgichtein, 2005).
For RTE, we used BIUTEE(Stern and Dagan, 2011), which performs betterthan the median system in the past four RTE com-petitions.
We ran BIUTEE both in its default con-figuration, as well as with its optional additionaldata sources (FrameNet, ReVerb, DIRT, and othersas found on the BIUTEE home page).
The defaultconfiguration performed better so we present itsresults here.
The results in Table 6 show that theRTE method performed worse than the baseline.MC160 Train and Dev:400 Q?sTest:240 Q?sSW SW+D SW SW+DSingle 59.46 68.11 64.29 75.89Multi 59.53 67.44 48.44 57.81All 59.50 67.75 55.83 66.25Table 4.
Percent correct for the multiple choice ques-tions for MC160.
SW: sliding window algorithm.SW+D: combined results with sliding window anddistance based algorithms.
Single/Multi: questionsmarked by worker as requiring a single/multiple sen-tence(s) to answer.
All differences between SW andSW+D are significant (p<0.01 using the two-tailedpaired t-test).MC500Train and Dev:1400 Q?sTest:600 Q?sAllSW SW+D SW SW+D SW+DSingle 55.13 61.77 51.10 57.35 60.44Multi 49.80 55.28 51.83 56.10 55.53All 52.21 58.21 51.50 56.67 57.75Table 5.
Percent correct for the multiple choice ques-tions for MC500, notation as above.
All differencesbetween SW and SW+D are significant (p<0.01, test-ed as above).MC160 Test MC500 TestBaseline (SW+D) 66.25 56.67RTE 59.79?53.52Combined 67.60 60.83?Table 6.
Percent correct for MC160 and MC500 testsets.
The ?
indicates statistical significance vs. baseline(p<0.01 using the two-tailed paired t-test).
MC160combined vs. baseline has p-value 0.063.200We also combined the baseline and RTE systemby training BIUTEE on the train set and using thedevelopment set to optimize a linear combinationof BIUTEE with the baseline; the combined sys-tem outperforms either component system onMC500.It is possible that with some tuning, an RTE sys-tem will outperform our baseline system.
Never-theless, these RTE results, and the performance ofthe baseline system, both suggest that the readingcomprehension task described here will not be triv-ially solved by off-the-shelf techniques.8 Making Data and Results an OngoingResourceOur goal in constructing this data is to encourageresearch and innovation in the machine compre-hension of text.
Thus, we have made both MC160and MC500 freely available for download athttp://research.microsoft.com/mct.
To our knowl-edge, these are the largest copyright-free readingcomprehension data sets publicly available.
Tofurther encourage research on these data, we willbe continually updating the webpage with the best-known published results to date, along with point-ers to those publications.One of the difficulties in making progress on aparticular task is implementing previous work inorder to apply improvements to it.
To mitigate thisdifficulty, we are encouraging researchers who usethe data to (optionally) provide per-answer scoresfrom their system.
Doing so has three benefits: (a)a new system can be measured in the context of theerrors made by the previous systems, allowingeach research effort to incrementally add usefulfunctionality without needing to also re-implementthe current state-of-the-art; (b) it allows systemperformance to be measured using paired statisticaltesting, which will substantially increase the abilityto determine whether small improvements are sig-nificant; and (c) it enables researchers to performerror analysis on any of the existing systems, sim-plifying the process of identifying and tacklingcommon sources of error.
We will also periodicallyensemble the known systems using standard ma-chine learning techniques and make those resultsavailable as well (unless the existing state-of-the-art already does such ensembling).The released data contains the stories and ques-tions, as well as the results from workers who ratedthe stories and took the tests.
The latter may beused, for example, to measure machine perfor-mance vs. human performance on a per-questionbasis (i.e., does your algorithm make similar mis-takes to humans?
), or vs. the judged clarity of eachstory.
The ratings, as well as whether a questionneeds multiple sentences to answer, should typical-ly only be used in evaluation, since such infor-mation is not generally available for most text.
Wewill also provide an anonymized author id for eachstory, which could allow additional research suchas using other works by the same author when un-derstanding a story, or research on authorship at-tribution (e.g., Stamatatos, 2009).9 Future WorkWe plan to use this dataset to evaluate approachesfor machine comprehension, but are making itavailable now so that others may do the same.
IfMCTest is used we will collect more story sets andwill continue to refine the collection process.
Oneinteresting research direction is ensuring that thequestions are difficult enough to challenge state-of-the-art techniques as they develop.
One idea forthis is to apply existing techniques automaticallyduring story set creation to see whether a questionis too easily answered by a machine.
By requiringauthors to create difficult questions, each data setwill be made more and more difficult (but still an-swerable by humans) as the state-of-the-art meth-ods advance.
We will also experiment with timingthe raters as they answer questions to see if we canfind those that are too easy for people to answer.Removing such questions may increase the diffi-culty for machines as well.
Additionally, any di-vergence between how easily a person answers aquestion vs. how easily a machine does may pointtoward new techniques for improving machinecomprehension; we plan to conduct research in thisdirection as well as make any such data availablefor others.10 ConclusionWe present the MCTest dataset in the hope that itwill help spur research into the machine compre-hension of text.
The metric (the accuracy on thequestion sets) is clearly defined, and on that metric,lexical baseline algorithms only attain approxi-mately 58% correct on test data (the MC500 set) as201opposed to the 100% correct that the majority ofcrowd-sourced judges attain.
A key component ofMCTest is the scalable design: we have shown thatdata whose quality approaches that of expertly cu-rated data can be generated using crowd sourcingcoupled with expert correction of worker-identifiederrors.
Should MCTest prove useful to the com-munity, we will continue to gather data, both toincrease the corpus size, and to keep the test setsfresh.
The data is available at http://research.micro-soft.com/mct and any submitted results will beposted there too.
Because submissions will be re-quested to include the score for each test item, re-searchers will easily be able to compare theirsystems with those of others, and investigation ofensembles comprised of components from severaldifferent teams will be straightforward.
MCTestalso contains supplementary material that re-searchers may find useful, such as worker accura-cies on a grammar test and crowd-sourcedmeasures of the quality of their stories.AcknowledgmentsWe would like to thank Silviu Cucerzan and LucyVanderwende for their help with converting ques-tions to statements and other useful discussions.ReferencesM.
Agarwal and P. Mannem.
2011.
Automatic Gap-fillQuestion Generation from Text Books.
In Proceed-ings of the Sixth Workshop on Innovative Use of NLPfor Building Educational Applications, 56?64.E.
Breck, M. Light, G.S.Mann, E. Riloff, B.
Brown, P.Anand, M. Rooth M. Thelen.
2001.
Looking underthe hood: Tools for diagnosing your question answer-ing engine.
In Proceedings of the workshop on Open-domain question answering, 12, 1-8.E.
Charniak.
1972.
Toward a Model of Children?s StoryComprehension.
Technical Report, 266, MIT Artifi-cial Intelligence Laboratory, Cambridge, MA.P.
Clark, P. Harrison, and X. Yao.
An Entailment-BasedApproach to the QA4MRE Challenge.
2012.
In Pro-ceedings of the Conference and Labs of the Evalua-tion Forum (CLEF) 2012.S.
Cucerzan and E. Agichtein.
2005.
Factoid QuestionAnswering over Unstructured and Structured Contenton the Web.
In Proceedings of the Fourteenth TextRetrieval Conference (TREC).I.
Dagan, O. Glickman, and B. Magnini.
2006.
ThePASCAL Recognising Textual Entailment Chal-lenge.
In J. Qui?onero-Candela, I. Dagan, B. Magni-ni, F. d'Alch?-Buc (Eds.
), Machine LearningChallenges.
Lecture Notes in Computer Science, Vol.3944, pp.
177-190, Springer.D.
Goldwasser, R. Reichart, J. Clarke, D. Roth.
2011.Confidence Driven Unsupervised Semantic Parsing.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics, 1486-1495.E.
Grois and D.C. Wilkins.
2005.
Learning Strategiesfor Story Comprehension: A Reinforcement LearningApproach.
In Proceedings of the Twenty Second In-ternational Conference on Machine Learning, 257-264.S.M.
Harabagiu, S.J.
Maiorano, and M.A.
Pasca.
2003.Open-Domain Textual Question Answering Tech-niques.
Natural Language Engineering, 9(3):1-38.Cambridge University Press, Cambridge, UK.L.
Hirschman, M. Light, E. Breck, and J.D.
Burger.1999.
Deep Read: A Reading Comprehension Sys-tem.
In Proceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics (ACL),325-332.J.
Horton and L. Chilton.
2010.
The labor economics ofpaid crowdsourcing.
In Proceedings of the 11th ACMConference on Electronic Commerce, 209-218.V.
Kuperman, H. Stadthagen-Gonzalez, M. Brysbaert.2012.
Age-of-acquisition ratings for 30,000 Englishwords.
Behavior Research Methods, 44(4):978-990.J.L.
Leidner, T. Dalmas, B. Webber, J. Bos, C. Grover.2003.
Automatic Multi-Layer Corpus Annotation forEvaluating Question Answering Methods:CBC4Kids.
In Proceedings of the 3rd InternationalWorkshop on Linguistically Interpreted Corpora.E.T.
Mueller.
2010.
Story Understanding Resources.http://xenia.media.mit.edu/~mueller/storyund/storyres.html.G.
Paolacci, J. Chandler, and P. Iperirotis.
2010.
Run-ning experiments on Amazon Mechanical Turk.Judgment and Decision Making.
5(5):411-419.E.
Stamatatos.
2009.
A survey of modern authorshipattribution methods.
J.
Am.
Soc.
Inf.
Sci., 60:538?556.A.
Stern and I. Dagan.
2011.
A Confidence Model forSyntactically-Motivated Entailment Proofs.
In Pro-ceedings of Recent Advances in Natural LanguageProcessing (RANLP).L.R.
Tang and R.J. Mooney.
2001.
Using MultipleClause Constructors in Inductive Logic Programmingfor Semantic Parsing.
In Proceedings of the 12th Eu-ropean Conference on Machine Learning (ECML),466-477.G.
Tur, D. Hakkani-Tur, and L.Heck.
2010.
What is leftto be understood in ATIS?
Spoken Language Tech-nology Workshop, 19-24.E.M.
Voorhees and D.M.
Tice.
1999.
The TREC-8Question Answering Track Evaluation.
In Proceed-ings of the Eighth Text Retrieval Conference (TREC-8).202B.
Wellner, L. Ferro, W. Greiff, and L. Hirschman.2005.
Reading comprehension tests for computer-based understand evaluation.
Natural Language En-gineering, 12(4):305-334.
Cambridge UniversityPress, Cambridge, UK.J.M.
Zelle and R.J. Mooney.
1996.
Learning to ParseDatabase Queries using Inductive Logic Program-ming.
In Proceedings of the Thirteenth NationalConference on Artificial Intelligence (AAAI), 1050-1055.L.S.
Zettlemoyer and M. Collins.
2009.
Learning Con-text-Dependent Mappings from Sentences to LogicalForm.
In Proceedings of the 47th Annual Meeting ofthe Association for Computation Linguistics (ACL),976-984.G.
Zweig and C.J.C.
Burges.
2012.
A Challenge Set forAdvancing Language Modeling.
In Proceedings ofthe Workshop on the Future of Language Modelingfor HLT, NAACL-HLT.203
