Identifying Cross-Document Relations between SentencesYasunari Miyabe ??
Hiroya Takamura ?
Manabu Okumura ?
?Interdisciplinary Graduate School of Science and Engineering,Tokyo Institute of Technology, Japan?Precision and Intelligence Laboratory,Tokyo Institute of Technology, Japanmiyabe@lr.pi.titech.ac.jp, {takamura,oku}@pi.titech.ac.jpAbstractA pair of sentences in different newspaperarticles on an event can have one of sev-eral relations.
Of these, we have focused ontwo, i.e., equivalence and transition.
Equiv-alence is the relation between two sentencesthat have the same information on an event.Transition is the relation between two sen-tences that have the same information exceptfor values of numeric attributes.
We pro-pose methods of identifying these relations.We first split a dataset consisting of pairsof sentences into clusters according to theirsimilarities, and then construct a classifierfor each cluster to identify equivalence re-lations.
We also adopt a ?coarse-to-fine?
ap-proach.
We further propose using the identi-fied equivalence relations to address the taskof identifying transition relations.1 IntroductionA document generally consists of semantic unitscalled sentences and various relations hold betweenthem.
The analysis of the structure of a document byidentifying the relations between sentences is calleddiscourse analysis.The discourse structure of one document hasbeen the target of the traditional discourse anal-ysis (Marcu, 2000; Marcu and Echihabi, 2002;Yokoyama et al, 2003), based on rhetorical struc-ture theory (RST) (Mann and Thompson, 1987).
?Yasunari Miyabe currently works at Toshiba Solutions Cor-poration.Inspired by RST, Radev (2000) proposed thecross-document structure theory (CST) for multi-document analysis, such as multi-document summa-rization, and topic detection and tracking.
CST takesthe structure of a set of related documents into ac-count.
Radev defined relations that hold betweensentences across the documents on an event (e.g., anearthquake or a traffic accident).Radev presented a taxonomy of cross-documentrelations, consisting of 24 types.
In Japanese, Etohet al (2005) redefined 14 CST types based onRadev?s taxonomy.
For example, a pair of sentenceswith an ?equivalence relation?
(EQ) has the sameinformation on an event.
EQ can be considered tocorrespond to the identity and equivalence relationsin Radev?s taxonomy.
A sentence pair with a ?tran-sition relation?
(TR) contains the same numeric at-tributes with different values.
TR roughly corre-sponds to the follow-up and fulfilment relations inRadev?s taxonomy.
We will provide examples ofCST relations:1.
ABC telephone company announced on the 9ththat the number of users of its mobile-phoneservice had reached one million.
Users can ac-cess the Internet, reserve train tickets, as wellas make phone calls through this service.2.
ABC said on the 18th that the number ofusers of its mobile-phone service had reached1,500,000.
This service includes Internet ac-cess, and enables train-ticket reservations andtelephone calls.The pair of the first sentence in 1 and the first sen-tence in 2 is in TR, because the number of users141has changed from one million to 1.5 millions, whileother things remain unchanged.
The pair of the sec-ond sentence in 1 and the second sentence in 2 isin EQ, because these two sentences have the sameinformation.Identification of CST relations has attracted moreattention since the study of multi-document dis-course emerged.
Identified CST types are helpfulin various applications such as multi-document sum-marization and information extraction.
For example,EQ is useful for detecting and eliminating redundantinformation in multi-document summarization.
TRcan be used to visualize time-series trends.We focus on the two relations EQ and TR in theJapanese CST taxonomy, and present methods fortheir identification.
For the identification of EQpairs, we first split a dataset consisting of sentencepairs into clusters according to their similarities, andthen construct a classifier for each cluster.
In addi-tion, we adopt a coarse-to-fine approach, in which amore general (coarse) class is first identified beforethe target fine class (EQ).
For the identification of TRpairs, we use variable noun phrases (VNPs), whichare defined as noun phrases representing a variablewith a number as its value (e.g., stock prices, andpopulation).2 Related WorkHatzivassiloglou et al (1999; 2001) proposed amethod based on supervised machine learning toidentify whether two paragraphs contain similar in-formation.
However, we found it was difficult toaccurately identify EQ pairs between two sentencessimply by using similarities as features.
Zhang etal.
(2003) presented a method of classifying CSTrelations between sentence pairs.
However, theirmethod used the same features for every type ofCST, resulting in low recall and precision.
We thusselect better features for each CST type, and for eachcluster of EQ.The EQ identification task is apparently related toTextual Entailment task (Dagan et al, 2005).
Entail-ment is asymmetrical while EQ is symmetrical, inthe sense that if a sentence entails and is entailed byanother sentence, then this sentence pair is in EQ.However in the EQ identification, we usually needto find EQ pairs from an extremely biased dataset ofsentence pairs, most of which have no relation at all.3 Identification of EQ pairsThis section explains a method of identifying EQpairs.
We regarded the identification of a CST re-lation as a standard binary classification task.
Givena pair of sentences that are from two different butrelated documents, we determine whether the pairis in EQ or not.
We use Support Vector Machines(SVMs) (Vapnik, 1998) as a supervised classifier.Please note that one instance consists of a pair of twosentences.
Therefore, a similarity value between twosentences is only given to one instance, not two.3.1 Clusterwise ClassificationAlthough some pairs in EQ have quite high similar-ity values, others do not.
Simultaneously using bothof these two types of pairs for training will adverselyaffect the accuracy of classification.
Therefore, wepropose splitting the dataset first according to sim-ilarities of pairs, and then constructing a classifierfor each cluster (sub-dataset).
We call this methodclusterwise classification.We use the following similarity in the cosine mea-sure between two sentences (s1, s2):cos(s1, s2) = u1 ?
u2/|u1||u2|, (1)where u1 and u2 denote the frequency vectors ofcontent words (nouns, verbs, adjectives) for respec-tive s1 and s2.
The distribution of the sentence pairsaccording to the cosine measure is summarized inTable 1.
From the table, we can see a large dif-ference in distributions of EQ and no-relation pairs.This difference suggests that the clusterwise classi-fication approach is reasonable.We split the dataset into three clusters: high-similarity cluster, intermediate-similarity cluster,and low-similarity cluster.
Intuitively, we ex-pected that a pair in the high-similarity clusterwould have many common bigrams, that a pair inthe intermediate-similarity cluster would have manycommon unigrams but few common bigrams, andthat a pair in the low-similarity cluster would havefew common unigrams or bigrams.3.2 Two-Stage Identification MethodThe number of sentence pairs in EQ in theintermediate- or low-similarity clusters is much142Table 1: The distribution of sentence pairs according to the cosine measure (NO indicates pairs with norelation.
The pairs with other relations are not on the table due to the space limitation)cos (0.0, 0.1] (0.1, 0.2] (0.2, 0.3] (0.3, 0.4] (0.4, 0.5] (0.5, 0.6] (0.6, 0.7] (0.7, 0.8] (0.8, 0.9] (0.9, 1.0]EQ 12 13 21 25 37 61 73 61 69 426summary 5 5 25 19 22 13 16 6 6 0refinement 3 4 15 11 12 15 6 6 3 2NO 194938 162221 68283 28152 11306 4214 1379 460 178 455Figure 1: Method of identifying EQ pairssmaller than the total number of sentence pairs asshown in Table 1.
These two clusters also containmany pairs that belong to a ?summary?
and a ?re-finement?
relation, which are very much akin to EQ.This may cause difficulties in identifying EQ pairs.We gave a generic name, GEN(general)-EQ, tothe union of EQ, ?summary?, and ?refinement?
re-lations.
For pairs in the intermediate- or low-similarity clusters, we propose a two-stage methodusing GEN-EQ on the basis of the above observa-tions, which first identifies GEN-EQ pairs betweensentences, and then identifies EQ pairs from GEN-EQ pairs.This two-stage method can be regarded as acoarse-to-fine approach (Vanderburg and Rosenfeld,1977; Rosenfeld and Vanderbrug, 1977), which firstidentifies a coarse class and then finds the target fineclass.
We used the coarse-to-fine approach on top ofthe clusterwise classification method as in Fig.
1.There are by far less EQ pairs than pairs withoutrelation.
This coarse-to-fine approach will reducethis bias, since GEN-EQ pairs outnumber EQ pairs.3.3 Features for identifying EQ pairsInstances (i.e., pairs of sentences) are represented asbinary vectors.
Numeric features ranging from 0.0to 1.0 are discretized and represented by 10 binaryfeatures (e.g., a feature value of 0.65 is transformedinto the vector 0000001000).
Let us first explain ba-sic features used in all clusters.
We will then explainother features that are specific to a cluster.3.3.1 Basic features1.
Cosine similarity measures: We use unigram, bi-gram, trigram, bunsetsu-chunk1 similarities at all thesentence levels, and unigram similarities at the para-graph and the document levels.
These similaritiesare calculated by replacing u1 and u2 in Eq.
(1) withthe frequency vectors of each sentence level.2.
Normalized lengths of sentences: Given an in-stance of sentence pair s1 and s2, we can define fea-tures normL(s1) and normL(s2), which represent(normalized) lengths of sentences, as:normL(s) = len(s)/EventMax(s), (2)where len(s) is the number of characters ins.
EventMax(s) is maxs?
?event(s) len(s?
), whereevent(s) is the set of sentences in the event thatdoc(s) describes.
doc(s) is the document contain-ing s.3.
Difference in publication dates: This feature de-pends on the interval between the publication datesof doc(s1) and doc(s2) and is defined as:DateDiff(s1, s2) = 1 ?|Date(s1) ?
Date(s2)|EventSpan(s1, s2), (3)where Date(s) is the publication date of an arti-cle containing s, and EventSpan(s1, s2) is the timespan of the event, i.e., the difference between thepublication dates for the first and the last articles thatare on the same event.
For example, if doc(s1) ispublished on 1/15/99 and doc(s2) on 1/17/99, andif the time span of the event ranges from 1/1/99 to1/21/99, then the feature value is 1-2/20 = 0.9.1Bunsetsu-chunks are Japanese phrasal units usually con-sisting of a pair of a noun phrase and a case marker.1434.
Positions of sentences in documents (Edmund-son, 1969): This feature is defined asPosit(s) = lenBef(s)/len(doc(s)), (4)where lenBef(s) is the number of characters be-fore s in the document, and len(doc(s)) is the totalnumber of characters in doc(s).5.
Semantic similarities: This feature is measured byEq.
(1) with u1 and u2 being the frequency vectorsof semantic classes of nouns, verbs, and adjectives.We used the semantic classes in a Japanese thesauruscalled ?Goi-taikei?
(Ikehara et al, 1997).6.
Conjunction (Yokoyama et al, 2003): Each of 55conjunctions corresponds to one feature.
If a con-junction appears at the beginning of the sentence,the feature value is 1, otherwise 0.7.
Expressions at the end of sentences: Yokoyamaet al (2003) created rules that map sentence endingsto their functions.
Each function corresponds to afeature.
If a function appears in the sentence, thevalue of the feature for the function is 1, otherwise 0.Functions of sentence endings are past, present, as-sertion, existence, conjecture, interrogation, judge-ment, possibility, reason, request, description, duty,opinion, continuation, causation, hearsay, and mode.8.
Named entity: This feature represents sim-ilarities measured through named entities in thesentences.
Its value is measured by Eq.
(1)with u1 and u2 being the frequency vectors of thenamed entities.
We used the named-entity chun-ker bar2.
The types of named entities are ARTI-FACT?DATE?ORGANIZATION?MONEY?LO-CATION?TIME?PERCENT?and PERSON.9.
Types of named entities with particle: This fea-ture represents the occurrence of types of named en-tities accompanied by a case marker (particle).
Weused 11 different case markers.3.3.2 Additional features to identify fine classWe will next explain additional features used inidentifying EQ pairs from GEN-EQ pairs.1.
Numbers of words (morphemes) and phrases:These features represent the closeness of the num-bers of words and bunsetsu-chunks in the two sen-tences.
This feature is defined as:2http://chasen.naist.jp/?masayu-a/p/bar/NumW (s1, s2) = 1 ?|frqW (s1) ?
frqW (s2)|max(frqW (s1), frqW (s2)), (5)where frqW (s) indicates the number of words ins.
Similarly, NumP (s1, s2) is obtained by replac-ing frqW in Eq.
(5) with frqP , where frqP (s)indicates the number of phrases in s.2.
Head verb: There are three features of this kind.The first indicates whether the two sentences havethe same head verb or not.
The second indicateswhether the two sentences have a semantically sim-ilar head verb or not.
If the two verbs have thesame semantic class in a thesaurus, they are re-garded as being semantically similar.
The last in-dicates whether both sentences have a verb or not.The head verbs are extracted using rules proposedby Hatayama (2001).3.
Salient words: This feature indicates whether thesalient words of the two sentences are the same ornot.
We approximate the salient word with the ga-or the wa-case word that appears first.4.
Numeric expressions and units (Nanba et al,2005): The first feature indicates whether the twosentences share a numeric expression or not.
Thesecond feature is similarly defined for numeric units.4 Experiments on identifying EQ pairsWe used the Text Summarization Challenge (TSC) 2and 3 corpora (Okumura et al, 2003) and the Work-shop on Multimodal Summarization for Trend Infor-mation (Must) corpus (Kato et al, 2005).
These twocorpora contained 115 sets of related news articles(10 documents per set on average) on various events.A document contained 9.9 sentences on average.Etoh et al (2005) annotated these two corpora withCST types.
There were 471,586 pairs of sentencesand 798 pairs of these had EQ.
We conducted theexperiments with 10-fold cross-validation (i.e., ap-proximately 425,000 pairs on average, out of whichapproximately 700 pairs are in EQ, are in the train-ing dataset for each fold).
The average, maximum,and minimum lengths of the sentences in the wholedatset are shown in Table 2.
We used precision,recall, and F-measure as evaluation measures.
Weused a Japanese morphological analyzer ChaSen3 to3http://chasen.naist.jp/hiki/Chasen/144Table 2: Average, max, min lengths of the sentencesin the datasetaverage max min# of words 33.27 458 1# of characters 111.22 1107 2extract parts-of-speech.
and a dependency analyzerCaboCha4 to extract bunsetsu-chunks.4.1 Estimation of thresholdWe split the set of sentence pairs into clusters ac-cording to their similarities in identifying EQ pairsas explained.
We used 10-fold cross validation againwithin the training data (i.e., the approximately425,000 pairs above are split into a temporary train-ing dataset and a temporary test dataset 10 times) toestimate the threshold to split the set, to select thebest feature set, and to determine the degree of thepolynomial kernel function and the value for soft-margin parameter C in SVMs.
No training instancesare used in the estimation of these parameters.4.1.1 Threshold between high- andintermediate-similarity clustersWe will first explain how to estimate the thresholdbetween high- and intermediate-similarity clusters.We expected that a pair in high-similarity clusterwould have many common bigrams, and that a pairin intermediate-similarity cluster would have manycommon unigrams but few common bigrams.
Wetherefore assumed that bigram similarity would beineffective in intermediate-similarity cluster.We determined the threshold in the following wayfor each fold of cross-validation.
We decreased thethreshold by 0.01 from 1.0.
We carried out 10-foldcross-validation within the training data, excludingone of the 14 features (6 cosine similarities and otherbasic features) for each value of the threshold.
Ifthe exclusion of a feature type deteriorates both av-erage precision and recall obtained by the cross-validation within the training data, we call it ineffec-tive.
We set the threshold to the minimum value forwhich bigram similarity is not ineffective.
We obtaina threshold value for each fold of cross-validation.The average value of threshold was 0.87.4http://chasen.naist.jp/?taku/software/cabocha/Table 3: Ineffective feature types for each thresholdthreshold ineffective features0.90 particle, bunsetsu-chunk similarity, semantic similarity0.89semantic similarity, expression at end of sentences,bigram similarity, particle0.88 bigram similarity0.87difference in publication dates, similarity between documents,expression at end of sentences, number of tokens,bigram similarity, similarity between paragraphs,positions of sentences, particle0.86 particle, similarity between documents, bigram similarityTable 4: F-measure calculated by cross-validationwithin the training data for each threshold in?intermediate-similarity cluster?threshold precision recall F-measure0.60 49,71 14.95 22.990.59 52.92 15.05 23.440.58 55.08 16.64 25.560.57 52.81 16.93 25.640.56 49.15 14.45 22.340.55 51.51 14.84 23.040.54 51.89 15.21 23.520.53 54.59 13.61 21.78As an example, we show the table of obtainedineffective feature types for one fold of cross-validation (Table 3).
The threshold was set to 0.90in this fold.4.1.2 Threshold between intermediate- andlow-similarity clustersWe will next explain how to estimate the thresholdbetween intermediate- and low-similarity clusters.There are numerous no-relation pairs in low-similarity pairs.
We expected that this imbalancewould adversely affect classification.
We thereforesimply attemted to exclude low-similarity pairs.
Wedecreased the threshold by 0.01 from the thresholdbetween high- and intermediate-similarity clusters.We chose a value that yielded the best average F-measure calculated by the cross-validation withinthe training data.
The average value of the thresh-old was 0.57.
Table 4 is an example of thresholdsand F-measures for one fold.4.2 Results of identifying EQ pairsThe results of EQ identification are shown in Ta-ble 5.
We tested the following models:Bow-cos: This is the simplest baseline we used.
We representedsentences with bag-of-words model.
Instances with the cosinesimilarity in Eq.
(1) larger than a threshold were classified asEQ.
The threshold that yielded the best F-measure in the test145Table 5: Results of identifying EQ pairsprecision recall F-measureBow-cos 87.29 57.35 69.22basic featuresClusterwise 81.98 59.40 68.88Non-Clusterwise 86.10 59.49 70.36ClusterC2F 94.96 62.27 75.22with additional featuresClusterwise 80.93 59.74 68.63Non-Clusterwise 86.11 60.16 70.84ClusterC2F 94.99 62.65 75.50Table 6: Results with basic featuresResults for ?high-similarity cluster?precision recall F-measureClusterwise 94.23 96.83 95.51Non-clusterwise 95.51 96.29 95.90ClusterC2F 94.23 96.83 95.51Results for ?intermediate-similarity cluster?Clusterwise 42.77 23.03 29.94Non-clusterwise 53.46 25.31 34.36ClusterC2F 100.00 36.29 53.25data was chosen.Non-Clusterwise: This is a supervised method without theclusterwise approach.
One classifier was constructed regard-less of the similarity of the instance.
We used the second degreepolynomial kernel.
Soft margin parameter C was set to 0.01.Clusterwise: This is a clusterwise method without the coarse-to-fine approach.
The second degree polynomial kernel wasused.
Soft margin parameter C was set to 0.1 for high-similaritycluster and 0.01 for the other clusters.ClusterC2F: This is our model, which integrates clusterwiseclassification with the coarse-to-fine approach (Figure 1).Table 5 shows that ClusterC2F yielded the bestF-measure regardless of presence of additional fea-tures.
The difference between ClusterC2F and theothers was statistically significant in the Wilcoxonsigned rank sum test with 5% significance level.4.3 Results for each clusterWe examined the results for each cluster.
The re-sults with basic features are summarized in Table 6and those with basic features plus additional fea-tures are in Table 7.
The tables show that thereare no significant differences among the modelsfor high-similarity cluster.
However, there are sig-nificant differences for intermediate-similarity clus-ter.
We thus concluded that the proposed model(ClusterC2F) works especially well in intermediate-similarity cluster.Table 7: Results with additional featuresResults for ?high-similarity cluster?precision recall F-measureClusterwise 94.23 96.83 95.51Non-clusterwise 95.70 96.76 96.23ClusterC2F 94.23 96.83 95.51Results for ?intermediate-similarity cluster?Clusterwise 39.77 22.93 29.09Non-clusterwise 55.61 26.81 36.18ClusterC2F 100.00 38.06 55.135 Identification of TR pairsWe regarded the identification of the relations be-tween sentences as binary classification, whether apair of sentences is classified into TR or not.
Weused SVMs (Vapnik, 1998).The sentence pairs in TR have the same numericattributes with different values, as mentioned in In-troduction.
Therefore, VNPs will be good clues forthe identification.5.1 Extraction of VNPsWe extract VNPs in the following way.1.
Search for noun phrases that have numeric ex-pressions (we call them numeric phrases).2.
Search for the phrases that the numeric phrasesdepend on (we call them predicate phrases).3.
Search for the noun phrases that depend on thepredicate phrases.4.
Extract the noun phrases that depend on thenoun phrases found in step 3, except for date expres-sions.
Both the extracted noun phrases and the nounphrases found in step 3 were regarded as VNPs.In the example in Introduction, ?one million?
and?1,500,000?
are numeric phrases, and ?had reached?is a predicate phrase.
Then, ?the number of users ofits mobile-phone service?
is a VNP.5.2 Features for identifying TR pairsWe used some features used in EQ identification:sentence-level uni-, bi-, tirgrams, and bunsetsu-chunk unigrams, normalized lengths of sentences,difference in publication dates, position of sentencesin documents, semantic similarities, conjunctions,expressions at the end of sentences, and named enti-ties.
In addition, we use the following features.1.
Similarities through VNPs: The cosine similarityof the frequency vectors of nouns in the VNPs in s1146and s2 is used.
If there are more than one VNP, thelargest cosine similarity is chosen.2.
Similarities through bigrams and trigrams inVNPs: These features are defined similarly to theprevious feature, but each VNP is represented by thefrequency vector of word bi- and trigrams.3.
Similarities of noun phrases in nominative case:Instances in TR often have similar subjects.
A nounphrase containing a ga-, wa-, or mo-case is regardedas the subject phrase of a sentence.
The similarity iscalculated by Eq.
(1) with the frequency vectors ofnouns in the phrase.4.
Changes in value of numeric attributes: This fea-ture is 1 if the values of the numeric phrases in thetwo sentences are different, otherwise 0.5.
Presence of numerical units: If a numerical unitis present in both sentences, the value of the featureis 1, otherwise 0.6.
Expressions that mean changes in value: In-stances in TR often contain those expressions, suchas ?reduce?
and ?increase?
(Nanba et al, 2005).
Wehave three features for each of these expressions.The first feature is 1 if both sentences have the ex-pression, otherwise 0.
The second is 1 if s1 has theexpression, otherwise 0.
The third is 1 if s2 has theexpression, otherwise 0.7.
Predicates: We define one feature for a predicate.The value of this feature is 1 if the predicate appearsin the two sentences, otherwise 0.8.
Reporter: This feature represents who is report-ing the incident.
This feature is represented by thecosine similarity between the frequency vectors ofnouns in phrases respectively expressing reporters ins1 and s2.
The subjects of verbs such as ?report?
and?announce?
are regarded as phrases of the reporter.5.3 Use of EQA pair of sentences in TR often has a high degreeof similarity.
Such pairs are likely to be confusedwith pairs in EQ.
We used the identified EQ pairs forthe identification of TR in order to circumvent thisconfusion.
Pairs classified as EQ with our methodwere excluded from candidates for TR.Table 8: Results of identifying TR pairsprecision recall F-measureBow-cos 27.44 41.26 32.96NANBA 19.85 45.96 27.73WithoutEq 42.41 47.06 44.61WithEq 43.13 48.51 45.67WithEqActual 43.06 48.55 45.646 Experiments on identifying TR pairsMost experimental settings are the same as in the ex-periments of EQ identification.
Sentence pairs with-out numeric expressions were excluded in advanceand 55,547 pairs were left.
This exclusion processdoes not degrade recall at all, because TR pairs bydefinition contain numberic expressions.We used precision, recall and F-measure for eval-uation.
We employed 10-fold cross validation.6.1 Results of identifying TR pairsThe results of the experiments are summarized inTable 8.
We compared four following models withours.
A linear kernel was used in SVMs and softmargin parameter C was set to 1.0 for all models:Bow-cos (baseline): We calculated the similarity throughVPNs.
If the similarity was larger than a threshold and the twosentences had the same expressions meaning changes in valueand had different values, then this pair was classified as TR.
Thethreshold was set to 0.7, which yielded the best F-measure in thetest data.NANBA (Nanba et al, 2005): If the unigram cosine similaritybetween the two sentences was larger than a threshold and thetwo sentences had expressions meaning changes in value, thenthis pair was classified as TR.
The value of the threshold was setto 0.42, which yielded the best F-measure in the test data.WithEq (Our method): This model uses the identified EQpairs.WithoutEq: This model uses no information on EQ.WithEqActual: This model uses the actual EQ pairs given byoracle.The results in Table 8 show that bow-cos is betterthan NANBA in F-measure.
This result suggests thatfocusing on VNPs is more effective than a simplebag-of-words approach.WithEq and WithEqActual were better than With-outEq.
This suggests that we successfully excludedEQ pairs, which are TR look-alikes.
WithEq andWithEqActual yielded almost the same F-measure.This means that our EQ identifier was good enough147to improve the identification of TR pairs.7 ConclusionWe proposed methods for identifying EQ and TRpairs in different newspaper articles on an event.We empirically demonstrated that the methods workwell in this task.Although we focused on resolving a bias in thedataset, we can expect that the classification perfor-mance will improve by making use of methods de-veloped in different but related tasks such as TextualEntailment recognition on top of our method.ReferencesIdo Dagan, Oren Glickman, and Bernardo Magnini.2005.
The pascal recognising textual entailment chal-lenge.
In Proceedings of the PASCAL ChallengesWorkshop on Recognising Textual Entailment, pages177?190.Harold Edmundson.
1969.
New methods in automaticextracting.
Journal of ACM, 16(2):246?285.Junji Etoh and Manabu Okumura.
2005.
Makingcross-document relationship between sentences cor-pus.
In Proceedings of the Eleventh Annual Meetingof the Association for Natural Language Processing(in Japanese), pages 482?485.Mamiko Hatayama, Yoshihiro Matsuo, and Satoshi Shi-rai.
2001.
Summarizing newspaper articles using ex-tracted information and functional words.
In 6th Natu-ral Language Processing Pacific Rim Symposium (NL-PRS2001), pages 593?600.Vasileios Hatzivassiloglou, Judith L. Klavans, andEleazar Eskin.
1999.
Detecting text similarity overshort passages: Exploring linguistic feature combi-nations via machine learning.
In Proceedings of theEmpirical Methods for Natural Language Processing,pages 203?212.Vasileios Hatzivassiloglou, Judith L. Klavans, Melissa L.Holcombe, Regina Barzilay, Min-Yen Kan, and Kath-leen R. McKeown.
2001.
Simfinder: A flexible clus-tering tool for summarization.
In Proceedings of theWorkshop on Automatic Summarization, pages 41?49.Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, AkioYokoo, Hiromi Nakaiwa, Kentaro Ogura, YoshifumiOyama, and Yoshihiko Hayashi.
1997.
Goi-Taikei ?
AJapanese Lexicon (in Japanese).
Iwanami Shoten.Tsuneaki Kato, Mitsunori Matsushita, and NorikoKando.
2005.
Must:a workshop on multimodal sum-marization for trend information.
In Proceedings ofthe NTCIR-5 Workshop Meeting, pages 556?563.William Mann and Sandra Thompson.
1987.
Rhetoricalstructure theory: Description and construction of textstructures.
In Gerard Kempen, editor, Natural Lan-guage Generation: New Results in Artificial Intelli-gence, Psychology, and Linguistics, pages 85?96.
Ni-jhoff, Dordrecht.Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse rela-tions.
In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics, pages368?375.Daniel Marcu.
2000.
The rhetorical parsing of un-restricted texts a surface-based approach.
Computa-tional Linguistics, 26(3):395?448.Hidetsugu Nanba, Yoshinobu Kunimasa, ShihoFukushima, Teruaki Aizawa, and Manabu Oku-mura.
2005.
Extraction and visualization of trendinformation based on the cross-document structure.In Information Processing Society of Japan, SpecialInterest Group on Natural Language Processing(IPSJ-SIGNL), NL-168 (in Japanese), pages 67?74.Manabu Okumura, Takahiro Fukushima, and HidetsuguNanba.
2003.
Text summarization challenge 2 -text summarization evaluation at ntcir workshop 3.In HLT-NAACL 2003 Workshop: Text Summarization(DUC03), pages 49?56.Dragomir Radev.
2000.
A common theory of infor-mation fusion from multiple text sources, step one:Cross-document structure.
In Proceedings of the 1stACL SIGDIAL Workshop on Discourse and Dialogue,pages 74?83.Azriel Rosenfeld and Gorden Vanderbrug.
1977.Coarse-fine template matching.
IEEE transactionsSystems, Man, and Cybernetics, 7:104?107.Gorden Vanderburg and Azriel Rosenfeld.
1977.
Two-stage template matching.
IEEE transactions on com-puters, 26(4):384?393.Vladimir Vapnik.
1998.
Statistical Learning Theory.John Wiley, New York.Kenji Yokoyama, Hidetsugu Nanba, and Manabu Oku-mura.
2003.
Discourse analysis using support vectormachine.
In Information Processing Society of Japan,Special Interest Group on Natural Language Process-ing (IPSJ-SIGNL), 2003-NL-155 (in Japanese), pages193?200.Zhu Zhang, Jahna Otterbacher, and Dragomir R.Radev.2003.
Learning cross-document structural relation-ships using boosting.
In Proceedings of the 12th Inter-national Conference on Information and KnowledgeManagement, pages 124?130.148
