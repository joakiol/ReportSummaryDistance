Duke's Trainable Information and Meaning Extraction System(Duke TIMES) *1 In t roduct ion  and BackgroundThe explosion in the amount of free text materialson the Internet, and the use of this information by .~people from all walks of life, has made the issue ofgeneralized information extraction a central one inNatural Language Processing.
Many systems includ- o ..~,Amit  BaggaJoyce Yue Cha iDepartment  of Computer  ScienceBox 90129, Duke UniversityDurham, NC 27708-0129Internet: {amit, chai}@cs.duke.eduTraining, ArticleTokenizer,Preprocessor &Partial ParserI Training Interfaceing ones from NYU, BBN, SRI, SRA, and MITREhave taken steps to make the process of customizinga system for a particular domain an easy one.We have built a system that attempts to provideany user with the ability to efficiently create andcustomize, for his or her own application, an infor-mation extraction system with competitive precisionand recall statistics.More details about the system can be found in(Bagga, 1997).2 Sys tem Arch i tec tureAs illustrated in Figure 1, there are three mainstages in the running of the system: the TrainingProcess, Rule Generalization, and the Scanning Pro-cess.
During the Training Process, the user, with thehelp of a graphical user interface, takes a few pro-totypical articles from the domain that the systemis being trained on, and creates rules (patterns) forthe target information contained in the training arti-cles.
These rules are specific to the training articlesand they are generalized so that they can be run onother articles from the domain.
The Rule General-ization routines, with the help of WordNet 1 (Miller,1990), generalize the specific rules generated by theTraining Process.
The system can now be run on alarge number of articles from the domain (ScanningProcess).
The output of the Scanning Process, foreach article, is a semantic network for that articlewhich can then be used by a Postprocessor to fillSupported by Fellowships from IBM Corporation.lWordNet is an on-line lexical reference system de-veloped by George Miller at Princeton University.t....- ~ .
WordNet \[ Role Generalization Routines~ r a l i z e d ~r,D New ArticleT?
en'z r' *Preprocessor &Partial ParserSense ClassiNer\] Rule Matching Routines \ ] JSemantic NetworkFigure 1: The Architecturetemplates, answer queries, or generate abstracts.2.1 Tools Used By the SystemIn addition to WordNet, the system uses IBM'sLanguageWare English Dictionary, IBM's Comput-ing Terms Dictionary, and a local dictionary of ourchoice.
The system also uses a gazetteer consist-ing of approximately 250 names of cities, states, andcountries.2.2 The Tokenizer, the Preprocessor, andthe Partial ParserThe Tokenizer accepts ASCII characters as inputand produces a stream of tokens (words) as output.It also determines sentence boundaries.The preprocessor tries to identify some importantentities like names of companies, proper names, etc.contained in the article.
Groups of words that com-prise these entities are collected together and con-7sidered as one item for all future processing.The Partial Parser produces a sequence of non-overlapping phrases as output.
The headword ofeach phrase is also identified.
The parser recognizesnoun groups, verb groups and preposition groups 2(Hobbs, 1993).2.3 The  Tra in ing  In ter faceThere are two parts to the Training Process: identi-fication of the (WordNet) sense usage of headwordsof interest, and the building of specific rules.
Train-ing is done by a user with the help of a graphicaluser Training Interface.3 GeneralizationRules created as a result of the Training Processare very specific and can only be applied to exactlythe same patterns as the ones present during thetraining.
Generalization consists of replacing eachconcept in a rule by a more generalized concept (ob-tained from WordNet).
Figure 2 shows the differentdegrees of generalization of the concept "IBM Cor-poration.
"sp = (IBM Corporation, NG, l, company)generalized at degree 1Generalize(sp, 1) = { business, concem}generalized at degree 2Generalize(sp, 2) = {enterprise}generalized at degree 3Generalize(sp, 3)= {organization}generalized at degree 5Generalize(sp, 5) = { group, social group }Figure 2: Degrees of Generalization4 ExperimentsWe designed an experiment to investigate how train-ing and the generalization strategy affect meaningextraction.
We trained our system on three sets ofarticles from the triangle.jobs USENET newsgroup,with emphasis on the following seven facts: Com-pany Name, Position/Title, Experience/Skill, Loca-tion, Benefit, Salary, and Contact Information.The first training set contained 8 articles; thesecond set contained 16 articles including the firstset; and the third set contained 24 articles includ-ing those in the first two sets.
For rules from eachtraining set, seven levels of generalization were per-formed.
Based on the generalized rules at each level,2We wish to thank Jerry Hobbs of SRI for providingus with the finite-state rules for the parser.u~1009590858075706560555040302010, ,.
.
.
.
.
.
.
.
.
.
.
: - - - : : : : -~!
-.
-_--.-iI I I I I1 2 3 4 5degree of generalizationFigure 3: Precision vs.
Degree of Generalization1009590858075706560555040302010i i i i i8 train-arts - -16 train-arts .
.
.
.
.24 t ra in -ar ts  .
.
.
.
.
.I I I I I1 2 3 4 5degree of generalizationFigure 4: Recall vs.
Degree of Generalizationthe system was run on 80 unseen articles from thesame newsgroup to test its performance on the ex-traction of the seven facts.The precision and recall curves with respect o thedegree of generalization are shown in Figure 3 andFigure 4 respectively.ReferencesBagga, Amit, and Joyce Y. Chai.
1997.
A TrainableMessage Understanding System, Submitted to theFifteenth International Joint Conference on Arti-ficial Intelligence (IJCAI'97).Hobbs, J., et al 1995.
FASTUS: A system for Ex-tracting Information from Text, Human LanguageTechnology, pp.
133-137, 1993.Miller, G.A., et al 1990.
Five Papers on WordNet,Cognitive Science Laboratory, Princeton Univer-sity, No.
43, July 1990.8
