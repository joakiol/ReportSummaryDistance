Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 190?200,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsCollecting Highly Parallel Data for Paraphrase EvaluationDavid L. ChenDepartment of Computer ScienceThe University of Texas at AustinAustin, TX 78712, USAdlcc@cs.utexas.eduWilliam B. DolanMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USAbilldol@microsoft.comAbstractA lack of standard datasets and evaluationmetrics has prevented the field of paraphras-ing from making the kind of rapid progressenjoyed by the machine translation commu-nity over the last 15 years.
We address bothproblems by presenting a novel data collectionframework that produces highly parallel textdata relatively inexpensively and on a largescale.
The highly parallel nature of this dataallows us to use simple n-gram comparisons tomeasure both the semantic adequacy and lex-ical dissimilarity of paraphrase candidates.
Inaddition to being simple and efficient to com-pute, experiments show that these metrics cor-relate highly with human judgments.1 IntroductionMachine paraphrasing has many applications fornatural language processing tasks, including ma-chine translation (MT), MT evaluation, summaryevaluation, question answering, and natural lan-guage generation.
However, a lack of standarddatasets and automatic evaluation metrics has im-peded progress in the field.
Without these resources,researchers have resorted to developing their ownsmall, ad hoc datasets (Barzilay and McKeown,2001; Shinyama et al, 2002; Barzilay and Lee,2003; Quirk et al, 2004; Dolan et al, 2004), andhave often relied on human judgments to evaluatetheir results (Barzilay and McKeown, 2001; Ibrahimet al, 2003; Bannard and Callison-Burch, 2005).Consequently, it is difficult to compare different sys-tems and assess the progress of the field as a whole.Despite the similarities between paraphrasing andtranslation, several major differences have preventedresearchers from simply following standards thathave been established for machine translation.
Pro-fessional translators produce large volumes of bilin-gual data according to a more or less consistent spec-ification, indirectly fueling work on machine trans-lation algorithms.
In contrast, there are no ?profes-sional paraphrasers?, with the result that there areno readily available large corpora and no consistentstandards for what constitutes a high-quality para-phrase.
In addition to the lack of standard datasetsfor training and testing, there are also no standardmetrics like BLEU (Papineni et al, 2002) for eval-uating paraphrase systems.
Paraphrase evaluationis inherently difficult because the range of potentialparaphrases for a given input is both large and unpre-dictable; in addition to being meaning-preserving,an ideal paraphrase must also diverge as sharply aspossible in form from the original while still sound-ing natural and fluent.Our work introduces two novel contributionswhich combine to address the challenges posed byparaphrase evaluation.
First, we describe a frame-work for easily and inexpensively crowdsourcing ar-bitrarily large training and test sets of independent,redundant linguistic descriptions of the same seman-tic content.
Second, we define a new evaluationmetric, PINC (Paraphrase In N-gram Changes), thatrelies on simple BLEU-like n-gram comparisons tomeasure the degree of novelty of automatically gen-erated paraphrases.
We believe that this metric,along with the sentence-level paraphrases providedby our data collection approach, will make it possi-190ble for researchers working on paraphrasing to com-pare system performance and exploit the kind ofautomated, rapid training-test cycle that has drivenwork on Statistical Machine Translation.In addition to describing a mechanism for collect-ing large-scale sentence-level paraphrases, we arealso making available to the research community85K parallel English sentences as part of the Mi-crosoft Research Video Description Corpus 1.The rest of the paper is organized as follows.
Wefirst review relevant work in Section 2.
Section 3then describes our data collection framework and theresulting data.
Section 4 discusses automatic evalua-tions of paraphrases and introduces the novel metricPINC.
Section 5 presents experimental results estab-lishing a correlation between our automatic metricand human judgments.
Sections 6 and 7 discuss pos-sible directions for future research and conclude.2 Related WorkSince paraphrase data are not readily available, var-ious methods have been used to extract parallel textfrom other sources.
One popular approach exploitsmultiple translations of the same data (Barzilay andMcKeown, 2001; Pang et al, 2003).
Examples ofthis kind of data include the Multiple-TranslationChinese (MTC) Corpus 2 which consists of Chinesenews stories translated into English by 11 transla-tion agencies, and literary works with multiple trans-lations into English (e.g.
Flaubert?s Madame Bo-vary.)
Another method for collecting monolingualparaphrase data involves aligning semantically par-allel sentences from different news articles describ-ing the same event (Shinyama et al, 2002; Barzilayand Lee, 2003; Dolan et al, 2004).While utilizing multiple translations of literarywork or multiple news stories of the same event canyield significant numbers of parallel sentences, thisdata tend to be noisy, and reliably identifying goodparaphrases among all possible sentence pairs re-mains an open problem.
On the other hand, multipletranslations on the sentence level such as the MTCCorpus provide good, natural paraphrases, but rela-1Available for download at http://research.microsoft.com/en-us/downloads/38cf15fd-b8df-477e-a4e4-a4680caa75af/2Linguistic Data Consortium (LDC) Catalog NumberLDC2002T01, ISBN 1-58563-217-1.tively little data of this type exists.
Finally, some ap-proaches avoid the need for monolingual paraphrasedata altogether by using a second language as thepivot language (Bannard and Callison-Burch, 2005;Callison-Burch, 2008; Kok and Brockett, 2010).Phrases that are aligned to the same phrase in thepivot language are treated as potential paraphrases.One limitation of this approach is that only wordsand phrases are identified, not whole sentences.While most work on evaluating paraphrase sys-tems has relied on human judges (Barzilay andMcKeown, 2001; Ibrahim et al, 2003; Bannard andCallison-Burch, 2005) or indirect, task-based meth-ods (Lin and Pantel, 2001; Callison-Burch et al,2006), there have also been a few attempts at creat-ing automatic metrics that can be more easily repli-cated and used to compare different systems.
Para-Metric (Callison-Burch et al, 2008) compares theparaphrases discovered by an automatic system withones annotated by humans, measuring precision andrecall.
This approach requires additional human an-notations to identify the paraphrases within paral-lel texts (Cohn et al, 2008) and does not evalu-ate the systems at the sentence level.
The morerecently proposed metric PEM (Paraphrase Evalu-ation Metric) (Liu et al, 2010) produces a singlescore that captures the semantic adequacy, fluency,and lexical dissimilarity of candidate paraphrases,relying on bilingual data to learn semantic equiva-lences without using n-gram similarity between can-didate and reference sentences.
In addition, the met-ric was shown to correlate well with human judg-ments.
However, a significant drawback of this ap-proach is that PEM requires substantial in-domainbilingual data to train the semantic adequacy evalu-ator, as well as sample human judgments to train theoverall metric.We designed our data collection framework foruse on crowdsourcing platforms such as Amazon?sMechanical Turk.
Crowdsourcing can allow inex-pensive and rapid data collection for various NLPtasks (Ambati and Vogel, 2010; Bloodgood andCallison-Burch, 2010a; Bloodgood and Callison-Burch, 2010b; Irvine and Klementiev, 2010), includ-ing human evaluations of NLP systems (Callison-Burch, 2009; Denkowski and Lavie, 2010; Zaidanand Callison-Burch, 2009).
Of particular relevanceare the paraphrasing work by Buzek et al (2010)191and Denkowski et al (2010).
Buzek et al automati-cally identified problem regions in a translation taskand had workers attempt to paraphrase them, whileDenkowski et al asked workers to assess the validityof automatically extracted paraphrases.
Our work isdistinct from these earlier efforts both in terms ofthe task ?
attempting to collect linguistic descrip-tions using a visual stimulus ?
and the dramaticallylarger scale of the data collected.3 Data CollectionSince our goal was to collect large numbers of para-phrases quickly and inexpensively using a crowd,our framework was designed to make the tasks short,simple, easy, accessible and somewhat fun.
For eachtask, we asked the annotators to watch a very shortvideo clip (usually less than 10 seconds long) anddescribe in one sentence the main action or eventthat occurred in the video clipWe deployed the task on Amazon?s MechanicalTurk, with video segments selected from YouTube.A screenshot of our annotation task is shown in Fig-ure 1.
On average, annotators completed each taskwithin 80 seconds, including the time required towatch the video.
Experienced annotators were evenfaster, completing the task in only 20 to 25 seconds.One interesting aspect of this framework is thateach annotator approaches the task from a linguisti-cally independent perspective, unbiased by the lexi-cal or word order choices in a pre-existing descrip-tion.
The data thus has some similarities to parallelnews descriptions of the same event, while avoidingmuch of the noise inherent in news.
It is also simi-lar in spirit to the ?Pear Stories?
film used by Chafe(1997).
Crucially, our approach allows us to gatherarbitrarily many of these independent descriptionsfor each video, capturing nearly-exhaustive cover-age of how native speakers are likely to summarizea small action.
It might be possible to achieve sim-ilar effects using images or panels of images as thestimulus (von Ahn and Dabbish, 2004; Fei-Fei et al,2007; Rashtchian et al, 2010), but we believed thatvideos would be more engaging and less ambiguousin their focus.
In addition, videos have been shownto be more effective in prompting descriptions ofmotion and contact verbs, as well as verbs that aregenerally not imageable (Ma and Cook, 2009).Watch and describe a short segment of a videoYou will be shown a segment of a video clip and asked to describe the main action/event in that segment inONE SENTENCE.Things to note while completing this task:The video will play only a selected segment by default.
You can choose to watch the entire clip and/orwith sound although this is not necessary.Please only describe the action/event that occurred in the selected segment and not any other parts ofthe video.Please focus on the main person/group shown in the segmentIf you do not understand what is happening in the selected segment, please skip this HIT and moveonto the next oneWrite your description in one sentenceUse complete, grammatically-correct sentencesYou can write the descriptions in any language you are comfortable withExamples of good descriptions:A woman is slicing some tomatoes.A band is performing on a stage outside.A dog is catching a Frisbee.The sun is rising over a mountain landscape.Examples of bad descriptions (With the reasons why they are bad in parentheses):Tomato slicing(Incomplete sentence)This video is shot outside at night about a band performing on a stage(Description about the video itself instead of the action/event in the video)I like this video because it is very cute(Not about the action/event in the video)The sun is rising in the distance while a group of tourists standing near some railings are takingpictures of the sunrise and a small boy is shivering in his jacket because it is really cold(Too much detail instead of focusing only on the main action/event)Segment starts: 25 | ends: 30 | length: 5 secondsPlay Segment ?
Play Entire VideoPlease describe the main event/action in the selected segment (ONE SENTENCE):Note: If you have a hard time typing in your native language on an English keyboard, you may findGoogle's transliteration service helpful.http://www.google.com/transliterateLanguage you are typing in (e.g.
English, Spanish, French, Hindi, Urdu, Mandarin Chinese, etc):Your one-sentence description:Please provide any comments or suggestions you may have below, we appreciate your input!Figure 1: A screenshot of our annotation task as it wasdeployed on Mechanical Turk.3.1 Quality ControlOne of the main problems with collecting data usinga crowd is quality control.
While the cost is very lowcompared to traditional annotation methods, work-ers recruited over the Internet are often unqualifiedfor the tasks or are incentivized to cheat in order tomaximize their rewards.To encourage native and fluent contributions, weasked annotators to write the descriptions in the lan-guage of their choice.
The result was a significantamount of translation data, unique in its multilingualparallelism.
While included in our data release, weleave aside a full discussion of this multilingual datafor future work.192To ensure the quality of the annotations being pro-duced, we used a two-tiered payment system.
Theidea was to reward workers who had shown the abil-ity to write quality descriptions and the willingnessto work on our tasks consistently.
While everyonehad access to the Tier-1 tasks, only workers who hadbeen manually qualified could work on the Tier-2tasks.
The tasks were identical in the two tiers buteach Tier-1 task only paid 1 cent while each Tier-2task paid 5 cents, giving the workers a strong incen-tive to earn the qualification.The qualification process was done manually bythe authors.
We periodically evaluated the workerswho had submitted the most Tier-1 tasks (usually onthe order of few hundred submissions) and grantedthem access to the Tier-2 tasks if they had performedwell.
We assessed their work mainly on the gram-maticality and spelling accuracy of the submitted de-scriptions.
Since we had hundreds of submissions tobase our decisions on, it was fairly easy to identifythe cheaters and people with poor English skills 3.Workers who were rejected during this process werestill allowed to work on the Tier-1 tasks.While this approach requires significantly moremanual effort initially than other approaches suchas using a qualification test or automatic post-annotation filtering, it creates a much higher qualityworkforce.
Moreover, the initial effort is amortizedover time as these quality workers are retained overthe entire duration of the data collection.
Many ofthem annotated all the available videos we had.3.2 Video CollectionTo find suitable videos to annotate, we deployed aseparate task.
Workers were asked to submit short(generally 4-10 seconds) video segments depictingsingle, unambiguous events by specifying links toYouTube videos, along with the start and end times.We again used a tiered payment system to rewardand retain workers who performed well.Since the scope of this data collection effort ex-tended beyond gathering English data alone, we3Everyone who submitted descriptions in a foreign languagewas granted access to the Tier-2 tasks.
This was done to encour-age more submissions in different languages and also becausewe could not verify the quality of those descriptions other thanusing online translation services (and some of the languageswere not available to be translated).??
Someone?is?coa?ng?a?pork?chop?in?a?glass?bowl?of?flour.???
A?person?breads?a?pork?chop.???
Someone?is?breading?a?piece?of?meat?with?a?white?powdery?substance.???
A?chef?seasons?a?slice?of?meat.???
Someone?is?pu?g?flour?on?a?piece?of?meat.???
A?woman?is?adding?flour?to?meat.???
A?woman?is?coa?ng?a?piece?of?pork?with?breadcrumbs.???
A?man?dredges?meat?in?bread?crumbs.???
A?person?breads?a?piece?of?meat.???
A?woman?is?breading?some?meat.???
Someone?is?breading?meat.???
A?woman?coats?a?meat?cutlet?in?a?dish.???
A?woman?is?coa?ng?a?pork?loin?in?bread?crumbs.???
The?laldy?coated?the?meat?in?bread?crumbs.???
The?woman?is?breading?pork?chop.???
A?woman?adds?a?mixture?to?some?meat.???
The?lady?put?the?ba?er?on?the?meat.
?Figure 2: Examples of English descriptions collected fora particular video segment.tried to collect videos that could be understoodregardless of the annotator?s linguistic or culturalbackground.
In order to avoid biasing lexicalchoices in the descriptions, we muted the audio andexcluded videos that contained either subtitles oroverlaid text.
Finally, we manually filtered the sub-mitted videos to ensure that each met our criteria andwas free of inappropriate content.3.3 DataWe deployed our data collection framework on Me-chanical Turk over a two-month period from July toSeptember in 2010, collecting 2,089 video segmentsand 85,550 English descriptions.
The rate of datacollection accelerated as we built up our workforce,topping 10K descriptions a day when we ended ourdata collection.
Of the descriptions, 33,855 werefrom Tier-2 tasks, meaning they were provided byworkers who had been manually identified as goodperformers.
Examples of some of the descriptionscollected are shown in Figure 2.Overall, 688 workers submitted at least one En-glish description.
Of these workers, 113 submittedat least 100 descriptions and 51 submitted at least500.
The largest number of descriptions submittedby a single worker was 3496 4.
Out of the 688 work-ers, 50 were granted access to the Tier-2 tasks.
The4This number exceeds the total number of videos becausethe worker completed both Tier-1 and Tier-2 tasks for the samevideos193Tier 1 Tier 2pay $0.01 $0.05# workers (English) 683 50# workers (total) 835 94# submitted (English) 51510 33829# submitted (total) 68578 55682# accepted (English) 51052 33825# accepted (total) 67968 55658Table 1: Statistics for the two video description taskssuccess of our data collection effort was in part dueto our ability to retain these good workers, building areliable and efficient workforce.
Table 1 shows somestatistics for the Tier-1 and Tier-2 tasks 5.
Overall,we spent under $5,000 including Amazon?s servicefees, some pilot experiments and surveys.On average, 41 descriptions were produced foreach video, with at least 27 for over 95% of thevideos.
Even limiting the set to descriptions pro-duced from the Tier-2 tasks, there are still 16 de-scriptions on average for each video, with at least 12descriptions for over 95% of the videos.
For mostclusters, then, we have a dozen or more high-qualityparallel descriptions that can be paired with one an-other to create monolingual parallel training data.4 Paraphrase Evaluation MetricsOne of the limitations to the development of ma-chine paraphrasing is the lack of standard metricslike BLEU, which has played a crucial role in driv-ing progress in MT.
Part of the issue is that agood paraphrase has the additional constraint thatit should be lexically dissimilar to the source sen-tence while preserving the meaning.
These can be-come competing goals when using n-gram overlapsto establish semantic equivalence.
Thus, researchershave been unable to rely on BLEU or some deriva-tive: the optimal paraphrasing engine under theseterms would be one that simply returns the input.To combat such problems, Liu et al (2010) haveproposed PEM, which uses a second language aspivot to establish semantic equivalence.
Thus, non-gram overlaps are required to determine the se-mantic adequacy of the paraphrase candidates.
PEM5The numbers for the English data are slightly underesti-mated since the workers sometimes incorrectly filled out theform when reporting what language they were using.also separately measures lexical dissimilarity andfluency.
Finally, all three scores are combined us-ing a support vector machine (SVM) trained on hu-man ratings of paraphrase pairs.
While PEM wasshown to correlate well with human judgments, ithas some limitations.
It only models paraphrasing atthe phrase level and not at the sentence level.
Fur-ther, while it does not need reference sentences forthe evaluation dataset, PEM does require suitablebilingual data to train the metric.
The result is thattraining a successful PEM becomes almost as chal-lenging as the original paraphrasing problem, sinceparaphrases need to be learned from bilingual data.The highly parallel nature of our data suggestsa simpler solution to this problem.
To measuresemantic equivalence, we simply use BLEU withmultiple references.
The large number of referenceparaphrases capture a wide space of sentences withequivalent meanings.
While the set of reference sen-tences can of course never be exhaustive, our datacollection method provides a natural distribution ofcommon phrases that might be used to describe anaction or event.
A tight cluster with many simi-lar parallel descriptions suggests there are only fewcommon ways to express that concept.In addition to measuring semantic adequacy andfluency using BLEU, we also need to measure lexi-cal dissimilarity with the source sentence.
We intro-duce a new scoring metric PINC that measures howmany n-grams differ between the two sentences.
Inessence, it is the inverse of BLEU since we want tominimize the number of n-gram overlaps betweenthe two sentences.
Specifically, for source sentences and candidate sentence c:PINC(s, c) =1NN?n=11?
| n-grams ?
n-gramc || n-gramc |where N is the maximum n-gram considered and n-grams and n-gramc are the lists of n-grams in thesource and candidate sentences, respectively.
Weuse N = 4 in our evaluations.The PINC score computes the percentage of n-grams that appear in the candidate sentence but notin the source sentence.
This score is similar to theJaccard distance, except that it excludes n-grams thatonly appear in the source sentence and not in thecandidate sentence.
In other words, it rewards candi-194dates for introducing new n-grams but not for omit-ting n-grams from the original sentence.
The resultsfor each n are averaged arithmetically.
PINC eval-uates single sentences instead of entire documentsbecause we can reliably measure lexical dissimilar-ity at the sentence level.
Also notice that we do notput additional constraints on sentence length: whileextremely short and extremely long sentences arelikely to score high on PINC, they still must main-tain semantic adequacy as measured by BLEU.We use BLEU and PINC together as a 2-dimensional scoring metric.
A good paraphrase, ac-cording to our evaluation metric, has few n-gramoverlaps with the source sentence but many n-gramoverlaps with the reference sentences.
This is con-sistent with our requirement that a good paraphraseshould be lexically dissimilar from the source sen-tence while preserving its semantics.Unlike Liu et al (2010), we treat these two cri-teria separately, since different applications mighthave different preferences for each.
For example,a paraphrase suggestion tool for a word processingsoftware might be more concerned with semanticadequacy, since presenting a paraphrase that doesnot preserve the meaning would likely result in anegative user experience.
On the other hand, a queryexpansion algorithm might be less concerned withpreserving the precise meaning so long as additionalrelevant terms are added to improve search recall.5 ExperimentsTo verify the usefulness of our paraphrase corpusand the BLEU/PINC metric, we built and evaluatedseveral paraphrase systems and compared the auto-matic scores to human ratings of the generated para-phrases.
We also investigated the pros and cons ofcollecting paraphrases using video annotation ratherthan directly eliciting them.5.1 Building paraphrase modelsWe built 4 paraphrase systems by training English toEnglish translation models using Moses (Koehn etal., 2007) with the default settings.
Using our para-phrase corpus to train and to test, we divided the sen-tence clusters associated with each video into 90%for training and 10% for testing.
We restricted ourattention to sentences produced from the Tier-2 tasks1?5?10?all?68.9?69?69.1?69.2?69.3?69.4?69.5?69.6?69.7?69.8?69.9?44.5?
45?
45.5?
46?
46.5?
47?
47.5?
48?
48.5?BLEU?PINC?Figure 3: Evaluation of paraphrase systems trained ondifferent numbers of parallel sentences.
As more trainingpairs are used, the model produces more varied sentences(PINC) but preserves the meaning less well (BLEU)in order to avoid excessive noise in the datasets, re-sulting in 28,785 training sentences and 3,367 testsentences.
To construct the training examples, werandomly paired each sentence with 1, 5, 10, orall parallel descriptions of the same video segment.This corresponds to 28K, 143K, 287K, and 449Ktraining pairs respectively.
For the test set, we usedeach sentence once as the source sentence with allparallel descriptions as references (there were 16references on average, with a minimum of 10 and amaximum of 31.)
We also included the source sen-tence as a reference for itself.Overall, all the trained models produce reasonableparaphrase systems, even the model trained on just28K single parallel sentences.
Examples of the out-puts produced by the models trained on single paral-lel sentences and on all parallel sentences are shownin Table 2.
Some of the changes are simple wordsubstitutions, e.g.
rabbit for bunny or gun for re-volver, while others are phrasal, e.g.
frying meat forbrowning pork or made a basket for scores in a bas-ketball game.
One interesting result of using videosas the stimulus to collect training data is that some-times the learned paraphrases are not based on lin-guistic closeness, but rather on visual similarity, e.g.substituting cricket for baseball.To evaluate the results quantitatively, we used theBLEU/PINC metric.
The performance of all thetrained models is shown in Figure 3.
Unsurprisingly,there is a tradeoff between preserving the meaning195Original sentence Trained on 1 parallel sentence Trained on all parallel sentencesa bunny is cleaning its paw a rabbit is licking its paw a rabbit is cleaning itselfa man fires a revolver a man is shooting targets a man is shooting a guna big turtle is walking a huge turtle is walking a large tortoise is walkinga guy is doing a flip over a park bench a man does a flip over a bench a man is doing stunts on a benchmilk is being poured into a mixer a man is pouring milk into a mixer a man is pouring milk into a bowlchildren are practicing baseball children are doing a cricket children are playing cricketa boy is doing karate a man is doing karate a boy is doing martial artsa woman is browning pork in a pan a woman is browning pork in a pan a woman is frying meat in a pana player scores in a basketball game a player made a basketball game a player made a basketTable 2: Examples of paraphrases generated by the trained models.and producing more varied paraphrases.
Systemstrained on fewer parallel sentences are more con-servative and make fewer mistakes.
On the otherhand, systems trained on more parallel sentences of-ten produce very good paraphrases but are also morelikely to diverge from the original meaning.
As acomparison, evaluating each human description asa paraphrase for the other descriptions in the samecluster resulted in a BLEU score of 52.9 and a PINCscore of 77.2.
Thus, all the systems performed verywell in terms of retaining semantic content, althoughnot as well in producing novel sentences.To validate the results suggested by the automaticmetrics, we asked two fluent English speakers torate the generated paraphrases on the following cate-gories: semantic, dissimilarity, and overall.
Seman-tic measures how well the paraphrase preserves theoriginal meaning while dissimilarity measures howmuch the paraphrase differs from the source sen-tence.
Each category is rated from 1 to 4, with 4being the best.
A paraphrase identical to the sourcesentence would receive a score of 4 for meaning and1 for dissimilarity and overall.
We randomly se-lected 200 source sentences and generated 2 para-phrases for each, representing the two extremes: oneparaphrase produced by the model trained with sin-gle parallel sentences, and the other by the modeltrained with all parallel sentences.
The averagescores of the two human judges are shown in Ta-ble 3.
The results confirm our finding that the sys-tem trained with single parallel sentences preservesthe meaning better but is also more conservative.5.2 Correlation with human judgmentsHaving established rough correspondences betweenBLEU/PINC scores and human judgments of se-Semantic Dissimilarity Overall1 3.09 2.65 2.51All 2.91 2.89 2.43Table 3: Average human ratings of the systems trained onsingle parallel sentences and on all parallel sentences.mantic equivalence and lexical dissimilarity, wequantified the correlation between these automaticmetrics and human ratings using Pearson?s corre-lation coefficient, a measure of linear dependencebetween two random variables.
We computed theinter-annotator agreement as well as the correlationbetween BLEU, PINC, PEM (Liu et al, 2010) andthe average human ratings on the sentence level.
Re-sults are shown in Table 4.In order to measure correlation, we need to scoreeach paraphrase individually.
Thus, we recomputedBLEU on the sentence level and left the PINC scoresunchanged.
While BLEU is typically not reliable atthe single sentence level, our large number of ref-erence sentences makes BLEU more stable even atthis granularity.
Empirically, BLEU correlates fairlywell with human judgments of semantic equiva-lence, although still not as well as the inter-annotatoragreement.
On the other hand, PINC correlates aswell as humans agree with each other in assessinglexical dissimilarity.
We also computed each met-ric?s correlation with the overall ratings, althoughneither should be used alone to assess the overallquality of paraphrases.PEM had the worst correlation with human judg-ments of all the metrics.
Since PEM was trained onnewswire data, its poor adaptation to this domain isexpected.
However, given the large amount of train-ing data needed (PEM was trained on 250K Chinese-196Semantic Dissimilarity OverallJudge A vs. B 0.7135 0.6319 0.4920BLEU vs. Human 0.5095 N/A 0.2127PINC vs. Human N/A 0.6672 0.0775PEM vs. Human N/A N/A 0.0654PINC vs. Human (BLEU > threshold)threshold = 0 N/A 0.6541 0.1817threshold = 30 N/A 0.6493 0.1984threshold = 60 N/A 0.6815 0.3986threshold = 90 N/A 0.7922 0.4350Combined BLEU and PINC vs. HumanArithmetic Mean N/A N/A 0.3173Geometric Mean N/A N/A 0.3003Harmonic Mean N/A N/A 0.3036PINC ?Sigmoid(BLEU) N/A N/A 0.3532Table 4: Correlation between the human judges as wellas between the automatic metrics and the human judges.English sentence pairs and 2400 human ratings ofparaphrase pairs), it is difficult to use PEM as a gen-eral metric.
Adapting PEM to a new domain wouldrequire sufficient in-domain bilingual data to sup-port paraphrase extraction.
In contrast, our approachonly requires monolingual data, and evaluation canbe performed using arbitrarily small, highly-paralleldatasets.
Moreover, PEM requires sample humanratings in training, thereby lessening the advantageof having automatic metrics.Since lexical dissimilarity is only desirable whenthe semantics of the original sentence is unchanged,we also computed correlation between PINC and thehuman ratings when BLEU is above certain thresh-olds.
As we restrict our attention to the set of para-phrases with higher BLEU scores, we see an in-crease in correlation between PINC and the humanassessments.
This confirms our intuition that PINCis a more useful measure when semantic content hasbeen preserved.Finally, while we do not believe any single scorecould adequately describe the quality of a para-phrase outside of a specific application, we experi-mented with different ways of combining BLEU andPINC into a single score.
Almost any simple combi-nation, such as taking the average of the two, yieldeddecent correlation with the human ratings.
The bestcorrelation was achieved by taking the product ofPINC and a sigmoid function of BLEU.
This followsthe intuition that semantic preservation is closer to a-??0.1?0?0.1?0.2?0.3?0.4?0.5?0.6?1?
2?
3?
4?
5?
6?
7?
8?
9?
10?
11?
12?
All?Pearson's?Correla?on?Number?of?references?for?BLEU?BLEU?with?source?vs.?Seman?c?
BLEU?without?source?vs.?Seman?c?BLEU?with?source?vs.?Overall?
BLEU?without?source?vs.
?Overall?Figure 4: Correlation between BLEU and human judg-ments as we vary the number of reference sentences.binary decision (i.e.
a paraphrase either preservesthe meaning or it does not, in which case PINC doesnot matter at all) than a linear function.
We usedan oracle to pick the best logistic function in ourexperiment.
In practice, some sample human rat-ings would be required to tune this function.
Othermore complicated methods for combining BLEUand PINC are also possible with sample human rat-ings, such as using a SVM as was done in PEM.We quantified the utility of our highly paralleldata by computing the correlation between BLEUand human ratings when different numbers of refer-ences were available.
The results are shown in Fig-ure 4.
As the number of references increases, thecorrelation with human ratings also increases.
Thegraph also shows the effect of adding the source sen-tence as a reference.
If our goal is to assess seman-tic equivalence only, then it is better to include thesource sentence.
If we are trying to assess the overallquality of the paraphrase, it is better to exclude thesource sentence, since otherwise the metric will tendto favor paraphrases that introduce fewer changes.5.3 Direct paraphrasing versus videoannotationIn addition to collecting paraphrases through videoannotations, we also experimented with the moretraditional task of presenting a sentence to an anno-tator and explicitly asking for a paraphrase.
We ran-domly selected a thousand sentences from our dataand collected two paraphrases of each using Me-chanical Turk.
We conducted a post-annotation sur-197vey of workers who had completed both the videodescription and the direct paraphrasing tasks, andfound that paraphrasing was considered more diffi-cult and less enjoyable than describing videos.
Ofthose surveyed, 92% found video annotations moreenjoyable, and 75% found them easier.
Based onthe comments, the only drawback of the video an-notation task is the time required to load and watchthe videos.
Overall, half of the workers preferred thevideo annotation task while only 16% of the workerspreferred the paraphrasing task.The data produced by the direct paraphrasing taskalso diverged less, since the annotators were in-evitably biased by lexical choices and word orderin the original sentences.
On average, a direct para-phrase had a PINC score of 70.08, while a paralleldescription of the same video had a score of 78.75.6 Discussions and Future WorkWhile our data collection framework yields usefulparallel data, it also has some limitations.
Findingappropriate videos is time-consuming and remains abottleneck in the process.
Also, more abstract ac-tions such as reducing the deficit or fighting for jus-tice cannot be easily captured by our method.
Onepossible solution is to use longer video snippets orother visual stimuli such as graphs, schemas, or il-lustrated storybooks to convey more complicated in-formation.
However, the increased complexity isalso likely to reduce the semantic closeness of theparallel descriptions.Another limitation is that sentences produced byour framework tend to be short and follow simi-lar syntactic structures.
Asking annotators to writemultiple descriptions or longer descriptions wouldresult in more varied data but at the cost of morenoise in the alignments.
Other than descriptions, wecould also ask the annotators for more complicatedresponses such as ?fill in the blanks?
in a dialogue(e.g.
?If you were this person in the video, whatwould you say at this point??
), their opinion of theevent shown, or the moral of the story.
However, aswith the difficulty of aligning news stories, findingparaphrases within these more complex responsescould require additional annotation efforts.In our experiments, we only used a subset of ourcorpus to avoid dealing with excessive noise.
How-ever, a significant portion of the remaining data isuseful.
Thus, an automatic method for filtering thosesentences could allow us to utilize even more of thedata.
For example, sentences from the Tier-2 taskscould be used as positive examples to train a stringclassifier to determine whether a noisy sentence be-longs in the same cluster or not.We have so far used BLEU to measure seman-tic adequacy since it is the most common MT met-ric.
However, other more advanced MT metricsthat have shown higher correlation with human judg-ments could also be used.In addition to paraphrasing, our data collectionframework could also be used to produces usefuldata for machine translation and computer vision.By pairing up descriptions of the same video in dif-ferent languages, we obtain parallel data without re-quiring any bilingual skills.
Another application forour data is to apply it to computer vision tasks suchas video retrieval.
The dataset can be readily usedto train and evaluate systems that can automaticallygenerate full descriptions of unseen videos.
As far aswe know, there are currently no datasets that containwhole-sentence descriptions of open-domain videosegments.7 ConclusionWe introduced a data collection framework that pro-duces highly parallel data by asking different an-notators to describe the same video segments.
De-ploying the framework on Mechanical Turk over atwo-month period yielded 85K English descriptionsfor 2K videos, one of the largest paraphrase data re-sources publicly available.
In addition, the highlyparallel nature of the data allows us to use standardMT metrics such as BLEU to evaluate semantic ad-equacy reliably.
Finally, we also introduced a newmetric, PINC, to measure the lexical dissimilaritybetween the source sentence and the paraphrase.AcknowledgmentsWe are grateful to everyone in the NLP group atMicrosoft Research and Natural Language Learninggroup at UT Austin for helpful discussions and feed-back.
We thank Chris Brockett, Raymond Mooney,Katrin Erk, Jason Baldridge and the anonymous re-viewers for helpful comments on a previous draft.198ReferencesVamshi Ambati and Stephan Vogel.
2010.
Can crowdsbuild parallel corpora for machine translation systems?In Proceedings of the NAACL HLT 2010 Workshop onCreating Speech and Language Data with Amazon?sMechanical Turk.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics (ACL-05).Regina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of Human Lan-guage Technology Conference / North American Asso-ciation for Computational Linguistics Annual Meeting(HLT-NAACL-2003).Regina Barzilay and Kathleen McKeown.
2001.
Extract-ing paraphrases from a parallel corpus.
In Proceed-ings of the 39th Annual Meeting of the Association forComputational Linguistics (ACL-2001).Michael Bloodgood and Chris Callison-Burch.
2010a.Bucking the trend: Large-scale cost-focused activelearning for statistical machine translation.
In Pro-ceedings of the 48th Annual Meeting of the Associationfor Computational Linguistics (ACL-2010).Michael Bloodgood and Chris Callison-Burch.
2010b.Using Mechanical Turk to build machine translationevaluation sets.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk.Olivia Buzek, Philip Resnik, and Benjamin B. Beder-son.
2010.
Error driven paraphrase annotation us-ing Mechanical Turk.
In Proceedings of the NAACLHLT 2010 Workshop on Creating Speech and Lan-guage Data with Amazon?s Mechanical Turk.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine trans-lation using paraphrases.
In Proceedings of HumanLanguage Technology Conference / North AmericanChapter of the Association for Computational Linguis-tics Annual Meeting (HLT-NAACL-06).Chris Callison-Burch, Trevor Cohn, and Mirella Lap-ata.
2008.
Parametric: An automatic evaluation met-ric for paraphrasing.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics(COLING-2008).Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP-2008).Chris Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using Amazon?s Me-chanical Turk.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP-2009).Wallace L. Chafe.
1997.
The Pear Stories: Cognitive,Cultural and Linguistic Aspects of Narrative Produc-tion.
Ablex, Norwood, NJ.Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.2008.
Constructing corpora for the development andevaluation of paraphrase systems.
Computational Lin-guistics, 34:597?614, December.Michael Denkowski and Alon Lavie.
2010.
Explor-ing normalization techniques for human judgments ofmachine translation adequacy collected using AmazonMechanical Turk.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk.Michael Denkowski, Hassan Al-Haj, and Alon Lavie.2010.
Turker-assisted paraphrasing for English-Arabic machine translation.
In Proceedings of theNAACL HLT 2010 Workshop on Creating Speech andLanguage Data with Amazon?s Mechanical Turk.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.
InProceedings of the 20th International Conference onComputational Linguistics (COLING-2004).Li Fei-Fei, Asha Iyer, Christof Koch, and Pietro Perona.2007.
What do we perceive in a glance of a real-worldscene?
Journal of Vision, 7(1):1?29.Ali Ibrahim, Boris Katz, and Jimmy Lin.
2003.
Extract-ing structural paraphrases from aligned monolingualcorpora.
In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics (ACL-03).Ann Irvine and Alexandre Klementiev.
2010.
Using Me-chanical Turk to annotate lexicons for less commonlyused languages.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics (ACL-07).Stanley Kok and Chris Brockett.
2010.
Hitting the rightparaphrases in good time.
In Proceedings of HumanLanguage Technologies: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL-HLT-2010).Dekang Lin and Patrick Pantel.
2001.
DIRT-discoveryof inference rules from text.
In Proceedings of the199Seventh ACM SIGKDD International Conference onKnowledge Discovery and Data Mining (KDD-2001).Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010.PEM: A paraphrase evaluation metric exploiting par-allel texts.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing(EMNLP-2010).Xiaojuan Ma and Perry R. Cook.
2009.
How well do vi-sual verbs work in daily communication for young andold adults.
In Proceedings of ACM CHI 2009 Confer-ence on Human Factors in Computing Systems.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations: Ex-tracting paraphrases and generating new sentences.
InProceedings of Human Language Technology Confer-ence / North American Association for ComputationalLinguistics Annual Meeting (HLT-NAACL-2003).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics (ACL-2002), pages 311?318,Philadelphia, PA, July.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual machine translation for paraphrase gen-eration.
In Proceedings of the 2004 Conference onEmpirical Methods in Natural Language Processing(EMNLP-2004).Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-lia Hockenmaier.
2010.
Collecting image annotationsusing Amazon?s Mechanical Turk.
In Proceedings ofthe NAACL HLT 2010 Workshop on Creating Speechand Language Data with Amazon?s Mechanical Turk.Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, andRalph Grishman.
2002.
Automatic paraphrase acqui-sition from news articles.
In Proceedings of HumanLanguage Technology Conference.Luis von Ahn and Laura Dabbish.
2004.
Labeling im-ages with a computer game.
In Proceedings of ACMCHI 2004 Conference on Human Factors in Comput-ing Systems.Omar F. Zaidan and Chris Callison-Burch.
2009.
Feasi-bility of human-in-the-loop minimum error rate train-ing.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing(EMNLP-2009).200
