Example-based machine translation using DP-matching betweenword sequencesEiichiro SumitaATR Spoken Language Translation Research Laboratories2-2 Hikaridai, Seika, Soraku,Kyoto 619-0288,Japansumita@slt.atr.co.jpAbstractWe propose a new approach under theexample-based machine translationparadigm.
First, the proposed approachretrieves the most similar example bycarrying out DP-matching of the inputsentence and example sentences whilemeasuring the semantic distance of thewords.
Second, the approach adjuststhe gap between the input and the mostsimilar example by using a bilingualdictionary.
We show the results of acomputational experiment.1 IntroductionKnowledge acquisition from corpora is viable formachine translation.
The background is as follows: Demands have been increasing for machinetranslation systems to handle a wider range oflanguages and domains. MT requires bulk knowledge consisting of rulesand dictionaries. Building knowledge consumes considerabletime and money. Bilingual/multilingual translations have becomewidely available.There are two approaches in corpus-basedtranslation:1.
Statistical Machine Translation (SMT): SMTlearns models for translation from corpora anddictionaries and searches for the best translationaccording to the models in run-time (Brown etal., 1990; Knight, 1997; Ney et al, 2000).2.
Example-Based Machine Translation (EBMT):EBMT uses the corpus directly.
EBMT retrievesthe translation examples that are best matchedto an input expression and adjusts the examplesto obtain the translation (Nagao, 1981; Sadler1989; Sato and Nagao, 1990; Sumita and Iida,1991; Kitano, 1993; Furuse et al, 1994;Watanabe and Maruyama, 1994; Cranias et al,1994; Jones, 1996; Veale and Way, 1997; Carl,1999, Andriamanankasina et al, 1999; Brown,2000).Figure 1 Configuration(1) Sentence-AlignedBilingualCorpus(2) BilingualDictionary(3) ThesauriRetrieval + AdjustmentInputsentenceTargetsentenceThis paper pursues EBMT and proposes anew approach by using the distance betweenword sequences.
The following sections showthe algorithm, experimental results, andimplications and prospects.2 The proposed method2.1 ConfigurationAs shown in Figure 1, our resources are (1) abilingual corpus, in which sentences are alignedbeforehand; (2) a bilingual dictionary, which is usedfor word alignment and translation; and (3) thesauriof both languages, which are used for aiding wordalignment and incorporating the semantic distancebetween words into the word sequence distance.2.2 AlgorithmThe translation process consists of four steps:1I.
Retrieve the most similar translation pair;II.
Generate translation patterns;III.
Select the best translation pattern;IV.
Substitute target words for source words.Here, we illustrate the algorithm using translationfrom Japanese to English step by step.2.2.1 Retrieval - Step IThis step scans the source parts of all examplesentences in the bilingual corpus.
By measuring thedistance (dist shown below) between the wordsequences of the input and example sentences, itretrieves the examples with the minimum distance,provided the distance is smaller than the giventhreshold.
Otherwise, the whole translation fails withno output.According to equation (1), dist is calculated asfollows: The counts of the Insertion (I), Deletion (D),and Substitution (S) operations are summed up andthe total is normalized by the sum of the length of thesource and example sequences.1 Step I corresponds to Retrieval in Figure 1 and steps II,III, and IV correspond to Adjustment.Substitution (S) considers the semantic distancebetween two substituted words and is calledSEMDIST.
SEMDIST is defined as the division of K(the level of the least common abstraction in thethesaurus of two words) by N (the height of thethesaurus) according to equation (2) (Sumita and Iida,1991).
It ranges from 0 to 1.Let?s observe the following two sentences,2 (1-j)the input and (2-j) the source sentence of thetranslation example, where the hatched partsrepresent the differences between the two sentences.
(1-j) iro/ga/ki/ni/iri/masen[color/SUB/favor/OBJ/enter/POLITE-NOT]{I do not care for the color.
}(2-j) dezain/ga/ki/ni/iri/masen[design/SUB/favor/OBJ/enter/ POLITE-NOT]{I do not care for the design.
}Because ?iro?
and ?dezain?
are completely dissimilarin the thesauri used in the experiment, SEMDIST is 1,and therefore, the dist between them is (0+0+2*1) /(6+6) = 0.167.
The dist is calculated efficiently by astandard dynamic programming technique (Cormen1989).This step is an application of the so-called DP-matching, which is often used in speech recognitionresearch.2.2.2 Pattern Generation - Step IIFirst, the step stores the hatched parts of the inputsentence in memory for the following translation.Second, the step aligns the hatched parts of sourcesentence (2-j) to corresponding target sentence (2-e)of the translation example by using lexical resources.3 We do not align non-hatched parts word by word.We assume that non-hatched parts correspondtogether as a whole.
This keeps most parts of theexample unchanged in order to avoid mixing errorsor unnaturalness in the translation.
(2-j) dezain/ga/ki/ni/iri/masen(2-e) I do not like the design.2 A Japanese sentence has no word boundary marker suchas the blank character in English so we put ?
/ ?
betweenJapanese words.
The brackets show the English literaltranslation word by word and the braces show the sentencetranslation in English.3 We do not consider on the alignment mechanism in thisproposal.
We have a free hand in selecting an appropriatealignment method out of a spectrum (Manning and Hinrich,1999) ranging from statistical to lexical types.
In theexperiment, we rely on a bilingual dictionary and thesauriin both languages.NKSEMDISTSEMDISTDIdist LL exampleinput=+++=)2(2)1(We obtain the following translation pattern,where the variable X is used to connect source(2-j-p) and target (2-e-p) and store instance (1-j-b) in the input sentence.
(2-j-p) X/ga/ki/ni/iri/masen(2-e-p) I do not like the X(1-j-b) X = ?iro?2.2.3 Pattern Selection - Step IIIWe may retrieve more than one example, and,moreover, translation patterns can differ.
We have toselect the most suitable one from among thesetranslation patterns.
We use a heuristic rule for thispurpose.1.
Maximize the frequency of the translationpattern.2.
If this cannot be determined, maximize the sumof the frequency of words in the generatedtranslation patterns.3.
If this cannot be determined, select onerandomly as a last resort.2.2.4 Word Substitution - Step IVThis step is straightforward.
By translating the sourceword of the variable using the bilingual dictionary,and instantiating the variable within the target part ofthe selected translation pattern by target word (1-e-b),we finally get target sentence (1-e).
(1-e-b) X = ?color?
(1-e) I do not like the color.3 ExperimentTo see whether this rough approach works or not, weconducted a computational experiment using a large-scale bilingual corpus.
In this section, we show theexperimental conditions, performance, and erroranalysis.Table 1 Corpus StatisticsSentences 204,108(J) 8.3Sentence Length(E) 6.1(J) 1,689,449Words(E) 1,235,747(J) 19,640Vocabulary(E) 15,3743.1 Experimental ConditionsBilingual CorpusWe built a collection of Japanese sentences and theirEnglish translations, which are usually found inphrasebooks for foreign tourists.
Because thetranslations were made sentence by sentence, thecorpus was sentence-aligned by birth.
We lemmatizedand POS-tagged both the Japanese and Englishsentences using our morphological analysis programs.The total sentence count was about 200 K. 4  Thestatistics are summarized in Table 1.Test setA quality evaluation was done for 500 sentencesselected randomly from the above-mentioned corpusand the remaining sentences were used as translationexamples for the experiment.Bilingual DictionaryWe also used a bilingual dictionary previouslydeveloped for another MT system in the traveldomain (Sumita et al1999).ThesaurusWe used thesauri whose hierarchies are based on theKadokawa Ruigo-shin-jiten (Ohno 1984) for distancecalculation and word alignment.3.2 ResultsHere, we show coverage and accuracy results asevidence that our proposed machine translationsystem works.3.2.1 CoverageOur approach does not produce any translation whenthere is no example whose dist is within the giventhreshold, which was 1/3 in the experiment.Table 2 Coverage and Sentence Length% Average lengthExactly 46.4 5.6Approximately 42.8 7.7No Output 10.8 11.0Total 100.0 7.0Our approach covers about 90% of 500randomly selected sentences.
As shown in Table 2,one half of 90% is matched exactly and the other halfis matched approximately (dist < 1/3).4  We call a sequence of sentences uttered by a singlespeaker an utterance.
Our corpus is in fact aligned utteranceby utterance.
Strictly speaking, ?sentence?
in this papershould be replaced by ?utterance.
?The characteristics of no output sentencesare clearly explained by the average length.
Ourapproach is not good with longer sentences becauseour algorithm has no explicit step of decomposing aninput sentence into sub-sentences and because thelonger the sentence, the smaller the possibility thatthere exists a similar sentence in the exampledatabase.We assume that a coverage of 90% isimportant because this means that if 200 K sentenceswere input into the system, the system would producea translation 90% of the time.
In other words, thesystem would help the user 90% of the time tocommunicate with foreign people (assuming the userto be in a foreign country).3.2.2 AccuracyQuality RankingEach translation is graded into one of four ranks5(described below) by a bilingual human translatorwho is a native speaker of the target language,American English:(A) Perfect: no problems in either information orgrammar; (B) Fair: easy-to-understand with someunimportant information missing or flawed grammar;(C) Acceptable: broken but understandable witheffort; (D) Nonsense: important information has beentranslated incorrectly.Table 3 Translation Accuracy5  This ranking was developed for evaluation in spokenlanguage translation.
For more details, see (Sumita et al,1999).ResultAs shown in Table 3, our proposal achieved a highaccuracy of about 80% (A, B, C ranks in total).
Theremaining 20% are divided into ranks D and F (Nooutput).Long Sentence ProblemFigure 2 shows6 that the accuracy clearly decreasesas the dist increases.
This implies two points: (1)dist can indicate the quality of the producedtranslation, in contrast with the fact that MT systemsusually do not provide any confidence factor on theirresults.
The user is safe if he/she confineshimself/herself to using translations with a small distvalue; (2) The current algorithm has a problem inhandling distant examples, which usually relate to thelong sentence problem.3.2.3 Error AnalysisAs shown in the previous two subsections, the mostdominant problem is in dealing with relatively longersentences.
We point out here that even for shortersentences there are problems, although they are lessfrequent, as follows: Idioms or collocationsEven when the dist between the two sentences issmall, i.e., they are quite similar in the sourcelanguage, the meanings of the sentences can vary andthe translation can be different in the target language.This case is not so frequent, but is possible by idiomsor collocations as exemplified in the followingsample.6 The horizontal axis indicates the number of sentences.Rank %A 41.4B 25.2GoodC 11.8D 10.8 BadF(No output) 10.80 50 100 150 200 2501/3> d >=0.30.3> d >=0.20.2> d >=0.10.1> d >=0EXACTABCDFFigure 2 Accuracy by Distance1.
kata/o/tsume/teitadake/masu/ka[shoulders/OBJ/shorten-or-sit-closely/REQUEST/POLITE/QUESTION]{Could you tighten the shoulders up?}2.
seki/o/tsume/teitadake/masu/ka[seat/OBJ/shorten-or-close-up/REQUEST/POLITE/QUESTION]{Could you move over a little?
}The replaceability between ?kata?
and ?seki?
doesnot hold for these two similar sentences.
To avoidthis problem, a feedback mechanism of erroneoustranslations built into the system is one possiblesolution. Noise in dataThe proposed approach accepts the translationexample blindly.
If the translation is wrong orinappropriate, the output is directly made defective.The next two translations show contextualinappropriateness.
The source parts of the twoexamples are exactly the same, but the target part ofthe first example is neutral and that of the secondexample is specific, i.e., valid only in a specialsituation.
Preventing this requires cleaning theexample database, preferably by machine, orcollecting sufficiently large-scale data to suppress theinfluence of noisy examples.1.
hai/ari/masu = Yes, we do.[yes/exist/POLITE]2.
hai/ari/masu = Yes, we have a shuttle bus.
[yes/exist/POLITE]4 DiscussionHere, we explain the implications of the experimentalresults and discuss the future extension.4.1 LimitationsOur proposal has the limitations listed below, but wewould like to note that we have obtained highcoverage and accuracy for the phrasebook task. Database limitation: If a nearest neighborwithin the threshold does not exist in theexample database, we cannot performtranslation.
One positive note is that we wereable to build the necessary example database forthe phrasebook task, which is not a toy. Context limitation: We cannot translate context-dependent words, because contexts are oftenhard to embed in an example database.
Forexample, Japanese ?konnichiwa?
corresponds to?Good morning?
or  ?Good afternoon?
in Englishdepending on the time of utterance.
It is ingeneral difficult to embed such kinds ofsituational information into the exampledatabase. Implementation limitation: We have no methodfor dividing an input into chunks (such asclauses) at present, so long sentences cannot bedealt with.
In addition, no investigation hasbeen made on robustness with respect torecognition errors yet.
However, DP-matchingis expected to be effective.4.2 Generality vs. QualityThere is no commercial system that can translatephrasebook sentences at this level of accuracy.Figure 3 shows a comparison of our proposal and acommercial machine translation system that acceptstravel conversations.
Table 4 shows sampletranslations by the above two systems.
The uppertranslation was produced by our proposed system andthe lower translation was produced by thecommercial system.The reason behind the performance differencefor this task is that the commercial one was built as ageneral-purpose system and phrasebook sentencesare not easy to translate into high-quality results byusing such a general-purpose architecture.However, we must admit that general-purposearchitectures are effective and we do not mean to0% 20% 40% 60% 80% 100%A commercial MTProposed MT ABCDFFigure 3 Comparison of Proposed EBMT and a Commercial MTcriticize them by using this comparison.
It isreasonable to suggest that general-purposearchitectures are not the most suitable option forachieving high-quality translations in restricteddomains.4.3 Development Cost and Its Reduction inthe FutureWe do not need grammars and transfer rules but wedo need a bilingual corpus and lexical resources.The adaptability of our approach tomultilingual translation is promising.
Because wehave already succeeded in J-to-E, one of the mostdifficult translation pairs, we have little concernabout other pairs.
If we can create an n lingual corpus,we can make n(n-1) MT systems.To enable such a dream within a shortertimeframe, we have to reduce the necessary resourcessuch as bilingual dictionaries and thesauri byautomating the construction of lexical knowledge.We are aiming at such additional costreduction.We also want to eliminate restrictions, e.g.,sentence-aligned and morphologically taggedexample database.
By doing so, the applicability ofour approach can be increased.
This is anotherimportant challenge.A further challenging goal is to establishtechnology enabling the use of a small-scale corpus.4.4 Related ResearchHere, we would like to compare our proposal andrelated research in four points: level of knowledge,application of dynamic programming, the use ofthesauri, and the task.Knowledge of EBMTMany EBMT studies (Sato and Nagao, 1990; Sato,1991; Furuse et al, 1994; Sadler, 1989) assume theexistence of a bank of aligned bilingual trees or a setof translation patterns.
However, building suchknowledge is done by humans and is very expensive.Methods for automating knowledge building are stillbeing developed.
In contrast, our proposal does notrely on such a high-level analysis of the corpus andrequires only word-level knowledge, i.e.,morphological tags and dictionaries.Dynamic programmingDynamic programming has been used within theEBMT paradigm (1) for technical term translation(Sato, 1993), and (2) for translation support (Craniaset al, 1994).Sato translates technical terms, which areusually compound nouns, while we translatesentences.
He uses a corpus in which translation unitsof a pair of technical terms are aligned, while we donot require the alignment of translation units.
Hedefines the matching score and we define thedistance between word sequences, which aredifferent.
However, both are computed by a standarddynamic programming technique.Based on surface structures and contentwords, Cranias defined a similarity score betweentexts and introduced the idea of clustering thetranslation memory to speed up the retrieval ofsimilar translation examples.
The score is againcomputed by a standard dynamic programmingtechnique, but Cranias provides not a translation butonly a retrieval.But made to want to change these yen into dollars.I?d like to change yen into dollars.korera/no/en/wo/doru/ni/ryougae/shi/tai/n/desu/ga[these/of/yen/OBJ/dollar/IND-OBJ/exchange/do/want/PARTICLE/be/but]English isn?t good.I?m not good at English.eigo/wa/tokui/dehaari/masen[English/TOPIC/strong/be/ POLITE/NEGATION]Don?t I let me know a way of using a washing machine.Will you show me how to use the washing machine?Sentaku/ki/no/tsukai/kata/o/oshie/tekure/masen/ka[washing/machine/of/use/way/OBJ/teach/REQUEST/POLITE/NEGATION/QUESTION]The roasting addition and subtraction of the steak.How would like your steak?suteeki/no/yaki/kagen/wa[steak/of/grill/degree/TOPIC]Is it possible to have bread more?Could I have some more bread?pan/o/motto/itadake/masu/ka[bread/OBJ/more/get/POLITE/QUESTION]Table 4 Sample translations by two MTsThesaurusBrown (2000) uses equivalence classes tosuccessfully improve the coverage of EBMT.
Heproposed a method of automatically generatingequivalence classes using clustering techniques,while we use hand-coded thesauri (in the experiment).Such automation is very attractive, and the author isplanning to follow in Brown?s line, in spite of  a fearthat low frequent words will not be dealt witheffectively by clustering techniques.
Brown uses ahard condition, i.e., whether a word is included in anequivalence class or not, while we provide therelative distance between two words.
It is unknownwhich method is better for EBMT.
We do not plan onsticking with the current implementation using hand-coded thesauri, as we realize that further research onthese open problems is indispensable.Phrasebook taskThe phrasebook task was first advocated for the taskof speech translation by (Stentiford and Steer, 1987).They pointed out that when communicating within alimited domain such as international telephonecommunications, it is nearly possible to specify all ofthe required message concepts.
They used a keyword-based approach to access concepts to overcomespeech recognition errors.
On the other hand, we useDP-matching techniques for this end.7 The scalabilityof the keyword-based approach has raised questionsbecause enlarging a corpus directly increases thechances of conflict in identifying the concepts to beconveyed.5 Concluding RemarksWe proposed a new approach using DP-matching forretrieving examples within EBMT and demonstratedits coverage and accuracy through a computationalexperiment for a restricted domain, i.e., a phrasebooktask for foreign tourists.There is much room for our translationmethod to improve: (1) decomposing input sentenceswill improve the coverage, and (2) indexing orclustering the example database will drasticallyimprove the efficiency of the current na?veimplementation.AcknowledgementFirst, the author thanks anonymous reviewers fortheir useful comments.
The author?s heartfelt thanksgo to Kadokawa-Shoten for providing the Ruigo-Shin-Jiten.
Thanks also go to Dr. Seiichi7  We believe our approach is robust against speechrecognition errors, but we have not yet applied it to speechrecognition results.YAMAMOTO, President and Mr. Satoshi SHIRAI,Department Head, for providing the author with thechance to pursue this research.ReferencesAndriamanankasina, T., Araki, K. and Tochinai, T.1999.
Example-Based Machine Translation of Part-Of-Speech Tagged Sentences by Recursive Division.Proceedings of MT SUMMIT VII.
Singapore.Brown, P. F., Cocke, J., Della Pietra, S. A.,  DellaPietra, V. J., Jelinek, F., Lafferty, J.,  Mercer, R. L.,and Roossin, P. S. 1990.
A Statistical Approach toMachine Translation.
Computational Linguistics16(2).Brown, R. D. 2000.
Automated Generalization ofTranslation Examples.
In Proceedings of theEighteenth International Conference onComputational Linguistics (COLING-2000), pp.
125-131.
Saarbr?cken, Germany, August 2000.Carl, M.  1999.
Inducing Translation Templates forExample-Based Machine Translation, Proc.
Of MT-Summit VII.Cormen, H. T., Leiserson, C. E. and Rivest, L. R.1989.
Introduction to Algorithms, MIT Press, p. 1028.Cranias, L., Papageorgiou, H. and Piperidis, S. 1994.A Matching Technique in Example-Based MachineTranslation.
Institute for Language and SpeechProcessing, Greece.
Paper presented to Computationand Language.Furuse, O., Sumita, E. and Iida, H. 1994.
Transfer-Driven Machine Translation Utilizing EmpiricalKnowledge.
Transactions of IPSJ, Vol.
35, No.
3, pp.414-425 (in Japanese).Jones, D. 1996.
Analogical Natural LanguageProcessing.
UCL Press.
London, 155p.Kitano, H. 1993.
A Comprehensive and PracticalModel of Memory-Based Machine Translation.
Proc.of IJCAI-93.
pp.
1276-1282.Knight, K. 1997.
Automating Knowledge Acquisitionfor Machine Translation, AI Magazine, 18/4.Manning, D., C. and Hinrich, S. (1999) Chapter 5 ofFoundations of statistical natural languageprocessing, MIT Press, p. 680.Nagao, M. 1981.
A Framework of a MechanicalTranslation between Japanese and English byAnalogy Principle, in Artificial and HumanIntelligence, A. Elithorn and R. Banerji (eds.)
North-Holland, pp.
173-180, 1984.Ney, H., Och, F. J. and Vogel, S. 2000.
StatisticalTranslation of Spoken Dialogues in the VermobilSystem, Proc.
Of MSC2000, pp.
69-74.Ohno, S. and Hamanishi, M. 1984.
Ruigo-Shin-Jiten,Kadokawa, p. 932 (in Japanese).Sadler, V. Working with Analogical Semantics, 1989).Foris Publications, p. 256.Sato, S. and Nagao, M. 1990.
Toward Memory-basedTranslation.
In the proceedings of the InternationalConference on Computational Linguistics, COLING-90, Helsinki, Finland, August 1990.Sato, S. 1991.
MBT2: A Method for CombiningFragments of Examples in Example-BasedTranslation.
JJSAI, Vol.
6, No.
6, pp.
861-871 (inJapanese).Sato, S. 1993.
Example-Based Translation ofTechnical Terms.
Proc.
of TMI-93, pp.
58-68,.Stentiford, F. M. W. and Steer, M. G. 1987.
A SpeechDriven Language Translation System, Proc.
ofEuropean Conference on Speech Technology, Vol.
2,pp.
418-421.Sumita, E. and Iida, H. 1991.
Experiments andProspects of Example-Based Machine Translation.Proc.
of ACL-91, pp.
185-192.Sumita, E., Yamada, S., Yamamoto, K., Paul, M.,Kashioka, H., Ishikawa, K. and Shirai, S. 1999.Solutions to Problems Inherent in Spoken-languageTranslation: The ATR-MATRIX Approach, Proc.
of7th MT Summit, pp.
229-235.Veale, T. and Way, A.
1997.
Gaijin: A Template-Driven Bootstrapping Approach to Example-BasedMachine Translation, in the Proceedings ofNeMNLP'97, New Methods in Natural LanguageProcessing, Sofia, Bulgaria.Watanabe, H. and Maruyama, H. 1994.
ATransfer System Using Example-Based Approach.IEICE Transactions on Information and Systems, Vol.E77-D, No.
2, pp.
247-257.
