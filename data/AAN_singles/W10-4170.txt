ISCAS?A System for Chinese Word Sense Induction Based onK-means AlgorithmZhenzhong Zhang*           Le Sun?
Wenbo Li?
*Institute of Software, Graduate UniversityChinese Academy of Scienceszhenzhong@nfs.iscas.ac.cn?Institute of SoftwareChinese Academy of Sciences{sunle,wenbo02}@iscas.ac.cnAbstractThis paper presents an unsupervisedmethod for automatic Chinese wordsense induction.
The algorithm is basedon clustering the similar words accordingto the contexts in which they occur.
First,the target word which needs to bedisambiguated is represented as thevector of its contexts.
Then, reconstructthe matrix constituted by the vectors oftarget words through singular valuedecomposition (SVD) method, and usethe vectors to cluster the similar words.Our system participants in CLP2010back off task4-Chinese word senseinduction.1 IntroductionIt has been shown that using word senses insteadof surface word forms could improveperformance on many nature languageprocessing tasks such as information extraction(Joyce and Alan, 1999), information retrieval(Ozlem et al, 1999) and machine translation(David et al, 2005).
Historically, word sensesare represented as a fixed-list of definitionscoming from a manually complied dictionary.However, there seem to be some disadvantagesassociated with such fixed-list of sensesparadigm.
Since dictionaries usually containgeneral definitions and lack explicit semantic,they can?t reflect the exact content of the contextwhere the target word appears.
Anotherdisadvantage is that the granularity of sensedistinctions is fixed, so it may not be entirelysuitable for different applications.In order to overcome these limitations, sometechniques like word sense induction (WSI) havebeen proposed for discovering words?
sensesautomatically from the unannotated corpus.
Theword sense induction algorithms are usually baseon the Distributional Hypothesis, proposed by(Zellig, 1954), which showed that words withsimilar meanings appear in similar contexts(Michael, 2009).
And the hypothesis is alsopopularized with the phrase ?a word characte-rized by the company it keeps?
(John, 1957).This concept shows us a method to automatical-ly discover senses of words by clustering thetarget words with similar contexts (Lin, 1998).The word sense induction can be regarded as anunsupervised clustering problem.
First, selectsome features to be used when comparing simi-larity between words.
Second, represent disam-biguated words as vectors of selected featuresaccording to target words?
contexts.
Third, clus-ter the similar words using the vectors.
Butcompared with European languages such as Eng-lish, Chinese language has its own characteris-tics.
For example, Chinese ideographs havesenses while the English alphabets don?t have.So the methods which work well in English maynot be entirely suitable for Chinese.This paper proposes a method for Chineseword sense induction, which contains two stageprocesses: features selecting and context cluster-ing.
Chinese ideographs and Chinese wordswhich have two or more Chinese ideographs areused different strategies when selecting features.The vectors of target word?s instances are puttogether to constitute a matrix, whose row is in-stances and column is features.
Reconstruct thematrix through singular value decomposition toget a new vector for each instance.
Then, K-means clustering algorithm is employed to clus-ter the vectors of disambiguated words?
contexts.Each cluster to which some instances belong toidentifies a sense of corresponding target word.Our system participants in CLP2010 back offtask4 - Chinese word sense induction.The remainder of this paper is organized asfollows.
Section 2 presents the Chinese wordsenses induction algorithm.
Section 3 presentsthe evaluation sheme and the results of oursystem.
Section 4 gives some discussions andconclusions.2 Chinese Word Senses InductionThis section will present the strategies of select-ing features for disambiguated Chinese wordsand k-means algorithm for clustering vectors ofthe contexts.2.1 Features SelectionSince the input instances of target words are un-structured, it's necessary to select features andtransform them into structured format to fit theautomatic clustering algorithm.
Following theexample in (Ted, 2007), words are chosen asfeatures to represent the contexts where targetwords appear.
A word w in the context of thetarget word can be represented as a vector whoseith component is the average of the calculatedconditional probabilities of w and wj.The target words are usually removed fromthe corpus in the task of English word sense in-duction.
But Chinese language is very differentfrom European languages such as English.
Chi-nese ideographs usually have meanings of theirown while English   alphabets don?t have.
InChinese word senses induction tasks, the targetword may be a Chinese word which could haveone or more Chinese ideographs or a Chineseideograph.
And the meaning of Chinese ideo-graphs is determined by the Chinese word whereit appears.
The following example shows us thiscase.z ???????????????
162???
?z ???????????????????????????
?In this example, the target word is Chineseideograph ???
displayed in italic in the con-texts.
In the first context, its meaning is paddywhich is determined by the Chinese word ???
?, and similarly in the second context itsmeaning is valley determined by ????.
Sincethe meaning of the Chinese ideograph ???
isdetermined by the word where it appears, it maynot be appropriate to remove it from the con-texts simply while the others of the word are left.Different strategies are employed to remove tar-get words.
If the target word contains two ormore Chinese ideographs, it will be removedfrom the context.
Otherwise it will be kept.To solve the problem of data sparseness, weextracted extra 100 instances for each targetword from Sogou Data and also used thethesauruses (TongYiCi CiLin of HIT) to reducethe dimensionality of the word space (featurespace).
Two filtering heuristics are applied whenselecting features.
The first one is the minimumfrequency p1 of words, and the second one is themaximum frequency p2 of words.Each selected word (feature) should be as-signed a weight, which indicates the relative fre-quency of two co-occurring words.
Using condi-tional probabilities for weighting for object/verband subject/verb pairs is better than point-wisemutual information (Philipp et al, 2005).
So weused conditional probabilities for weightingwords pairs.
Let numi,j denote the number of theinstances where the word i and word j co-occur ,and numi denote the number of the instances inwhich the word i appears.
Then the jth compo-nent of the vector of the word i can be calculatedusing the following equation.,( | ) ( | )2i jp j i p i jw+=Where,( | ) i jjn u mp i jn u m=The contexts of each target word are representedas the centroid of the vectors of the words occur-ring in the target contexts.
Figure 1 shows anexample of context vector, where the Chineseword ????
co-occurs with Chinese words ???
?and ???
?.Figure 1: An example of  a context vector for???
?, calculated as the centroid of vectors of????
and ???
?.2.2 Clustering AlgorithmK-means algorithm is applied to cluster the vec-tors of the target word.
It assigns each element toone of K clusters according to which centroidthe element is close to by the similarity function.The cosine function is used to measure the simi-larity between two vectors V and W:12 21 1( , )| | | |ni iin ni ii iVWV Wsim V WV WV W== =?= =???
?where n is the number of features in each vector.Before clustering the vectors of instances, weput together the vectors of instances in the cor-pus and obtain a co-occurrence matrix of in-stances and words.
Singular value decomposi-tion is applied to reduce the dimensionality ofthe resulting multidimensional space and findsthe major axes of variation in the word space(Golub and Van Loan, 1989).
After the reduc-tion, the similarity between two instances can bemeasured using the cosine function mentioned asabove between the corresponding vectors.
Theclustering algorithm stops when the centroid ofeach cluster does not change or the iteration ofthe algorithm exceed a user-defined threshold p3.And the number of the clusters is determined bythe corpus where the target word appears.
Eachcluster to which some instances belongrepresents one senses of the target wordrepresented by the vector.We also employed a graph-based clusteringalgorithm -Chinese Whispers (CW) (Chris, 2006)to deal with the task of Chinese WSI.
CW doesnot require any input parameters and has a goodperformance in WSI (Chris, 2006).
For moredetails about CW algorithm please refer to(Chris, 2006).
We first constructed a graph,whose vertexes were instances of target wordand edges?
weight was the similarity of the cor-responding two vertexes.
Then we removed theedges with minimum weight until the percentageof the kept edges?
sum respect the total was be-low a threshold p4.
CW algorithm was employedto cluster the graph and each clusters representeda sense of target word.3 EvaluationThis section presents the evaluation scheme, setof parameters and the result of our system.3.1 Evaluation SchemeWe use standard cluster evaluation methods tomeasure the performance of our WSI system.Following the former practice (Zhao and Kary-pis, 2005), we consider the FScore measure forassessing WSI methods.
The FScore is used in asimilar fashion to Information Retrieval exercis-es.Let we assume that the size of a particularclass sr is nr, the size of a particular cluster hj isnj and the size of their common instances set isnr,j.
The precision can be calculated as follow:,( , ) r jr jjnP s hn=The recall value can be defined as:,( , ) r jr jrnR s hn=Then FScore of this class and cluster is definedto be:2 ( , ) ( , )( , )( , ) ( , )r j r jr jr j r jP s h R s hF s hP s h R s h?
?= +The FScore of class sr, F(sr), is the maximumF(sr, hj) value attained by any cluster, and it isdefined as:( ) max( ( , ))jr r jhF s F s h=Finally, the FScore of the entire clustering solu-tion is defined as the weighted average FScoreof each class:1( )q r rrn F sFScoren=?=?Where q is the number of classes and n is thetotal number of the instances where target wordappears.3.2 Tuning the ParametersWe tune the parameters of our system on thetraining data.
But because of time restrictions,we do not optimize these parameters.
The max-imum frequency of a word (p2) and the maxi-mum number of the K-means?
iteration (p3) aretuned on the training data.
The minimum fre-quency of a word (p1) was set to two followingour intuition.
The last parameter K -the numberof the clusters is determined by the test data inwhich the target word appears.
When tuning pa-rameters, we first fixed the parameter p3 andfound the best value of parameter p2, whichcould lead to the best performance.
The resultshave been shown in Table 1 and Table 2.Parameters FScoreP3=300,p2=35 0.7502P3=400,p2=40 0.7523P3=500,p2=40 0.7582Table 1: The results of K-means with SVDParameters FScoreP3=300,p2=40 0.7454P3=400,p2=40 0.7493P3=500,p2=45 0.7404Table 2: The results of K-meansThe performance of CW algorithm is shownin Table 3.
The parameter p4 is a threshold forpruning graph as describing in section 2.2.Parameter FScoreP4=0.55 0.6325P4=0.6 0.6321P4=0.65 0.6278P4=0.7 0.6393P4=0.75 0.6289P4=0.8 0.6345P4=0.85 0.6326P4=0.9 0.6342P4=0.95 0.6355Table 3: The results of CW.The result shows that the K-means algorithmhas a better performance than CW.
That maybecause CW can?t use the information of thenumber of clusters, but K-means could.
Anotherproblem for CW is that the size of corpus issmall and the constructed graph can?t reflect theinherent relation between the instances.Based on the result of experiments, we em-ployed K-means algorithm for our system andthe parameters is shown in Table 4.Parameters ValueP1: Minimum frequency of a word 2P2: Maximum frequency of a word 40P3: Maximum number of K-means ite-ration500K: the number of the cluster -Table 4: Parameters for the system.
The last pa-rameter K is provided by the test data.3.3 ResultOur system participants in the CLP2010 back-off task4 and disambiguate 100 target words,total 5000 instances.
The F-score of our systemon the test data is 0.7209 against the F-score0.7933 of the best system.4 ConclusionWe have presented a model for Chinese wordsense induction.
Different strategies are appliedto deal with Chinese ideographs and Chinesewords that contain two or more Chinese ideo-graphs.
After selecting the features ?words, sin-gular value decomposition is used to find themajor axes of variation in the feature space andreconstruct the vector of each context.
Then weemploy k-means cluster algorithm to cluster thevectors of contexts.
Result shows that our sys-tem is able to induce correct senses.
One draw-back of our system is that it overlooks the infre-quent senses because of lacking enough data.And our system only uses the information ofword co-occurrences.
So in the future we wouldlike to integrate different kinds of informationsuch as topical information, syntactic informa-tion and semantic information, and see if wecould get a better result.AcknowledgementThis work has been partially funded by NationalNatural Science Foundation of China undergrant #60773027, #60736044 and #90920010and by ?863?
Key Projects #2006AA010108,?863?
Projects #2008AA01Z145.
We would liketo thank anonymous reviewers for their detailedcomments.ReferencesChris Biemann, 2006.
Chinese whispers - an efficientgraph clustering algorithm and its application tonatural language processing problems, In Pro-ceedings of TextGraphs, pp.
73?80, New York,USA.David Vickrey, Luke Biewald, Marc Teyssley, andDaphne Koller.
2005.
Word-sense disambiguationfor machine translation.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,pages 771-778, Vancouver, British Columbia,CanadaDekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 17th inter-national conference on Computational linguistics,volume 2, pages 768-774, Montreal, Quebec, Can-adaGolub, G. H. and Van Loan, C. F. 1989.
MatrixComputations.
The John Hopkins University Press,Baltimore, MDJohn, R., Firth.
1957.
A Synopsis of Linguistic Theory1930-1955, pages 1-32.Joyce Yue Chai and Alan W. Biermann.
1999.
Theuse of word sense disambiguation in an informa-tion extraction system.
In Proceedings of the six-teenth national conference on Artificial intelli-gence and the eleventh Innovative applications ofartificial intelligence conference innovative appli-cations of artificial intelligence, pages 850-855,Orlando, Florida, United States.Michael Denkowski.
2009.
A Survey of Techniquesfor Unsupervised Word Sense Induction.Ozlem Uzuner, Boris Katz, and Deniz Yuret.
1999.Word sense disambiguation for information re-trieval.
In Proceedings of the sixteenth nationalconference on Artificial intelligence and the ele-venth Innovative applications of artificial intelli-gence conference innovative applications of artifi-cial intelligence, page 985, Orlando, Florida, Unit-ed States.Philipp Cimiano, Andreas Hotho, and Steffen Staab,2005.
Learning concept hierarchies from text cor-pora using formal concept analysis, Journal of Ar-tificial Intelligence Research (JAIR), 24, 305?339.Ted Pedersen, 2007.
Umnd2: Senseclusters applied tothe sense induction task of senseval-4.
In Proceed-ings of the Fourth International Workshop on Se-mantic Evaluations, pages 394?397, Prague, CzechRepublic.Zellig Harris.
1954.
Distributional Structure, pages146-162.Ying Zhao and George Karypis.
2005.
Hierarchicalclustering algorithms for document datasets.
DataMining and Knowledge Discovery, 10(2):141.168.
