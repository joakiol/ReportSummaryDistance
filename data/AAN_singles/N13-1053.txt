Proceedings of NAACL-HLT 2013, pages 497?501,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsNegative Deceptive Opinion SpamMyle Ott Claire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853{myleott,cardie}@cs.cornell.eduJeffrey T. HancockDepartment of CommunicationCornell UniversityIthaca, NY 14853jeff.hancock@cornell.eduAbstractThe rising influence of user-generated onlinereviews (Cone, 2011) has led to growing in-centive for businesses to solicit and manufac-ture DECEPTIVE OPINION SPAM?fictitiousreviews that have been deliberately written tosound authentic and deceive the reader.
Re-cently, Ott et al(2011) have introduced anopinion spam dataset containing gold standarddeceptive positive hotel reviews.
However, thecomplementary problem of negative deceptiveopinion spam, intended to slander competitiveofferings, remains largely unstudied.
Follow-ing an approach similar to Ott et al(2011), inthis work we create and study the first datasetof deceptive opinion spam with negative sen-timent reviews.
Based on this dataset, we findthat standard n-gram text categorization tech-niques can detect negative deceptive opinionspam with performance far surpassing that ofhuman judges.
Finally, in conjunction withthe aforementioned positive review dataset,we consider the possible interactions betweensentiment and deception, and present initialresults that encourage further exploration ofthis relationship.1 IntroductionConsumer?s purchase decisions are increasingly in-fluenced by user-generated online reviews of prod-ucts and services (Cone, 2011).
Accordingly,there is a growing incentive for businesses to so-licit and manufacture DECEPTIVE OPINION SPAM?fictitious reviews that have been deliberately writ-ten to sound authentic and deceive the reader (Ott etal., 2011).
For example, Ott et al(2012) has esti-mated that between 1% and 6% of positive hotel re-views appear to be deceptive, suggesting that somehotels may be posting fake positive reviews in orderto hype their own offerings.In this work we distinguish between two kinds ofdeceptive opinion spam, depending on the sentimentexpressed in the review.
In particular, reviews in-tended to promote or hype an offering, and whichtherefore express a positive sentiment towards theoffering, are called positive deceptive opinion spam.In contrast, reviews intended to disparage or slandercompetitive offerings, and which therefore express anegative sentiment towards the offering, are callednegative deceptive opinion spam.
While previousrelated work (Ott et al 2011; Ott et al 2012) hasexplored characteristics of positive deceptive opin-ion spam, the complementary problem of negativedeceptive opinion spam remains largely unstudied.Following the framework of Ott et al(2011), weuse Amazon?s Mechanical Turk service to producethe first publicly available1 dataset of negative de-ceptive opinion spam, containing 400 gold standarddeceptive negative reviews of 20 popular Chicagohotels.
To validate the credibility of our decep-tive reviews, we show that human deception detec-tion performance on the negative reviews is low, inagreement with decades of traditional deception de-tection research (Bond and DePaulo, 2006).
We thenshow that standard n-gram text categorization tech-niques can be used to detect negative deceptive opin-ion spam with approximately 86% accuracy ?
far1Dataset available at: http://www.cs.cornell.edu/?myleott/op_spam.497surpassing that of the human judges.In conjunction with Ott et al(2011)?s positive de-ceptive opinion spam dataset, we then explore theinteraction between sentiment and deception withrespect to three types of language features: (1)changes in first-person singular use, often attributedto psychological distancing (Newman et al 2003),(2) decreased spatial awareness and more narrativeform, consistent with theories of reality monitor-ing (Johnson and Raye, 1981) and imaginative writ-ing (Biber et al 1999; Rayson et al 2001), and (3)increased negative emotion terms, often attributed toleakage cues (Ekman and Friesen, 1969), but per-haps better explained in our case as an exaggerationof the underlying review sentiment.2 DatasetOne of the biggest challenges facing studies of de-ception is obtaining labeled data.
Recently, Ott etal.
(2011) have proposed an approach for generat-ing positive deceptive opinion spam using Amazon?spopular Mechanical Turk crowdsourcing service.
Inthis section we discuss our efforts to extend Ott etal.
(2011)?s dataset to additionally include negativedeceptive opinion spam.2.1 Deceptive Reviews from Mechanical TurkDeceptive negative reviews are gathered from Me-chanical Turk using the same procedure as Ott etal.
(2011).
In particular, we create and divide 400HITs evenly across the 20 most popular hotels inChicago, such that we obtain 20 reviews for eachhotel.
We allow workers to complete only a singleHIT each, so that each review is written by a uniqueworker.2 We further require workers to be locatedin the United States and to have an average past ap-proval rating of at least 90%.
We allow a maximumof 30 minutes to complete the HIT, and reward ac-cepted submissions with one US dollar ($1).Each HIT instructs a worker to imagine that theywork for the marketing department of a hotel, andthat their manager has asked them to write a fakenegative review of a competitor?s hotel to be postedonline.
Accompanying each HIT is the name and2While Mechanical Turk does not provide a convenientmechanism for ensuring the uniqueness of workers, this con-straint can be enforced with Javascript.
The script is availableat: http://uniqueturker.myleott.com.URL of the hotel for which the fake negative re-view is to be written, and instructions that: (1) work-ers should not complete more than one similar HIT,(2) submissions must be of sufficient quality, i.e.,written for the correct hotel, legible, reasonable inlength,3 and not plagiarized,4 and, (3) the HIT is foracademic research purposes.Submissions are manually inspected to ensurethat they are written for the correct hotel and toensure that they convey a generally negative senti-ment.5 The average accepted review length was 178words, higher than for the positive reviews gatheredby Ott et al(2011), who report an average reviewlength of 116 words.2.2 Truthful Reviews from the WebNegative (1- or 2-star) truthful reviews are minedfrom six popular online review communities: Expe-dia, Hotels.com, Orbitz, Priceline, TripAdvisor, andYelp.
While reviews mined from these communitiescannot be considered gold standard truthful, recentwork (Mayzlin et al 2012; Ott et al 2012) suggeststhat deception rates among travel review portals isreasonably small.Following Ott et al(2011), we sample a subsetof the available truthful reviews so that we retain anequal number of truthful and deceptive reviews (20each) for each hotel.
However, because the truthfulreviews are on average longer than our deceptive re-views, we sample the truthful reviews according toa log-normal distribution fit to the lengths of our de-ceptive reviews, similarly to Ott et al(2011).63 Deception Detection PerformanceIn this section we report the deception detection per-formance of three human judges (Section 3.1) andsupervised n-gram Support Vector Machine (SVM)classifiers (Section 3.2).3We define ?reasonable length?
to be ?
150 characters.4We use http://plagiarisma.net to determinewhether or not a review is plagiarized.5We discarded and replaced approximately 2% of the sub-missions, where it was clear that the worker had misread theinstructions and instead written a deceptive positive review.6We use the R package GAMLSS (Rigby and Stasinopou-los, 2005) to fit a log-normal distribution (left truncated at 150characters) to the lengths of the deceptive reviews.498TRUTHFUL DECEPTIVEAccuracy P R F P R FHUMANJUDGE 1 65.0% 65.0 65.0 65.0 65.0 65.0 65.0JUDGE 2 61.9% 63.0 57.5 60.1 60.9 66.3 63.5JUDGE 3 57.5% 57.3 58.8 58.0 57.7 56.3 57.0METAMAJORITY 69.4% 70.1 67.5 68.8 68.7 71.3 69.9SKEPTIC 58.1% 78.3 22.5 35.0 54.7 93.8 69.1Table 1: Deception detection performance, incl.
(P)recision, (R)ecall, and (F)1-score, for three human judges and twometa-judges on a set of 160 negative reviews.
The largest value in each column is indicated with boldface.3.1 Human PerformanceRecent large-scale meta-analyses have shown hu-man deception detection performance is low, withaccuracies often not much better than chance (Bondand DePaulo, 2006).
Indeed, Ott et al(2011) foundthat two out of three human judges were unable toperform statistically significantly better than chance(at the p < 0.05 level) at detecting positive decep-tive opinion spam.
Nevertheless, it is important tosubject our reviews to human judgments to validatetheir convincingness.
In particular, if human detec-tion performance is found to be very high, then itwould cast doubt on the usefulness of the Mechan-ical Turk approach for soliciting gold standard de-ceptive opinion spam.Following Ott et al(2011), we asked three vol-unteer undergraduate university students to read andmake assessments on a subset of the negative reviewdataset described in Section 2.
Specifically, we ran-domized all 40 deceptive and truthful reviews fromeach of four hotels (160 reviews total).
We thenasked the volunteers to read each review and markwhether they believed it to be truthful or deceptive.Performance for the three human judges appearsin Table 1.
We additionally show the deception de-tection performance of two meta-judges that aggre-gate the assessments of the individual human judges:(1) the MAJORITY meta-judge predicts deceptivewhen at least two out of three human judges predictdeceptive (and truthful otherwise), and (2) the SKEP-TIC meta-judge predicts deceptive when at least oneout of three human judges predicts deceptive (andtruthful otherwise).A two-tailed binomial test suggests that JUDGE 1and JUDGE 2 both perform better than chance (p =0.0002, 0.003, respectively), while JUDGE 3 fails toreject the null hypothesis of performing at-chance(p = 0.07).
However, while the best human judgeis accurate 65% of the time, inter-annotator agree-ment computed using Fleiss?
kappa is only slightat 0.07 (Landis and Koch, 1977).
Furthermore,based on Cohen?s kappa, the highest pairwise inter-annotator agreement is only 0.26, between JUDGE1 and JUDGE 2.
These low agreements suggestthat while the judges may perform statistically betterthan chance, they are identifying different reviewsas deceptive, i.e., few reviews are consistently iden-tified as deceptive.3.2 Automated Classifier PerformanceStandard n-gram?based text categorization tech-niques have been shown to be effective at detect-ing deception in text (Jindal and Liu, 2008; Mihal-cea and Strapparava, 2009; Ott et al 2011; Feng etal., 2012).
Following Ott et al(2011), we evaluatethe performance of linear Support Vector Machine(SVM) classifiers trained with unigram and bigramterm-frequency features on our novel negative de-ceptive opinion spam dataset.
We employ the same5-fold stratified cross-validation (CV) procedure asOtt et al(2011), whereby for each cross-validationiteration we train our model on all reviews for 16hotels, and test our model on all reviews for the re-maining 4 hotels.
The SVM cost parameter, C, istuned by nested cross-validation on the training data.Results appear in Table 2.
Each row lists the sen-timent of the train and test reviews, where ?CrossVal.?
corresponds to the cross-validation proceduredescribed above, and ?Held Out?
corresponds toclassifiers trained on reviews of one sentiment andtested on the other.
The results suggest that n-gram?based SVM classifiers can detect negative decep-tive opinion spam in a balanced dataset with perfor-mance far surpassing that of untrained human judges(see Section 3.1).
Furthermore, our results show that499TRUTHFUL DECEPTIVETrain Sentiment Test Sentiment Accuracy P R F P R FPOSITIVE POSITIVE (800 reviews, Cross Val.)
89.3% 89.6 88.8 89.2 88.9 89.8 89.3(800 reviews) NEGATIVE (800 reviews, Held Out) 75.1% 69.0 91.3 78.6 87.1 59.0 70.3NEGATIVE POSITIVE (800 reviews, Held Out) 81.4% 76.3 91.0 83.0 88.9 71.8 79.4(800 reviews) NEGATIVE (800 reviews, Cross Val.)
86.0% 86.4 85.5 85.9 85.6 86.5 86.1COMBINED POSITIVE (800 reviews, Cross Val.)
88.4% 87.7 89.3 88.5 89.1 87.5 88.3(1600 reviews) NEGATIVE (800 reviews, Cross Val.)
86.0% 85.3 87.0 86.1 86.7 85.0 85.9Table 2: Automated classifier performance for different train and test sets, incl.
(P)recision, (R)ecall and (F)1-score.classifiers trained and tested on reviews of differ-ent sentiments perform worse, despite having moretraining data,7 than classifiers trained and tested onreviews of the same sentiment.
This suggests thatcues to deception differ depending on the sentimentof the text (see Section 4).Interestingly, we find that training on the com-bined sentiment dataset results in performance thatis comparable to that of the ?same sentiment?
classi-fiers (88.4% vs. 89.3% accuracy for positive reviewsand 86.0% vs. 86.0% accuracy for negative reviews).This is explainable in part by the increased trainingset size (1,280 vs. 640 reviews per 4 training folds).4 Interaction of Sentiment and DeceptionAn important question is how language features op-erate in our fake negative reviews compared with thefake positive reviews of Ott et al(2011).
For exam-ple, fake positive reviews included less spatial lan-guage (e.g., floor, small, location, etc.)
because in-dividuals who had not actually experienced the ho-tel simply had less spatial detail available for theirreview (Johnson and Raye, 1981).
This was also thecase for our negative reviews, with less spatial lan-guage observed for fake negative reviews relative totruthful.
Likewise, our fake negative reviews hadmore verbs relative to nouns than truthful, suggest-ing a more narrative style that is indicative of imag-inative writing (Biber et al 1999; Rayson et al2001), a pattern also observed by Ott et al(2011).There were, however, several important differ-ences in the deceptive language of fake negative rel-ative to fake positive reviews.
First, as might beexpected, negative emotion terms were more fre-7?Cross Val.?
classifiers are effectively trained on 80% ofthe data and tested on the remaining 20%, whereas ?Held Out?classifiers are trained and tested on 100% of each data.quent, according to LIWC (Pennebaker et al 2007),in our fake negative reviews than in the fake posi-tive reviews.
But, importantly, the fake negative re-viewers over-produced negative emotion terms (e.g.,terrible, disappointed) relative to the truthful re-views in the same way that fake positive reviewersover-produced positive emotion terms (e.g., elegant,luxurious).
Combined, these data suggest that themore frequent negative emotion terms in the presentdataset are not the result of ?leakage cues?
that re-veal the emotional distress of lying (Ekman andFriesen, 1969).
Instead, the differences suggest thatfake hotel reviewers exaggerate the sentiment theyare trying to convey relative to similarly-valencedtruthful reviews.Second, the effect of deception on the pattern ofpronoun frequency was not the same across posi-tive and negative reviews.
In particular, while firstperson singular pronouns were produced more fre-quently in fake reviews than truthful, consistent withthe case for positive reviews, the increase was di-minished in the negative reviews examined here.
Inthe positive reviews reported by Ott et al(2011),the rate of first person singular in fake reviews(M=4.36%, SD=2.96%) was twice the rate observedin truthful reviews (M=2.18%, SD=2.04%).
In con-trast, the rate of first person singular in the deceptivenegative reviews (M=4.47%, SD=2.83%) was only57% greater than for truthful reviews (M=2.85%,SD=2.23%).
These results suggest that the empha-sis on the self, perhaps as a strategy of convinc-ing the reader that the author had actually been tothe hotel, is not as evident in the fake negative re-views, perhaps because the negative tone of the re-views caused the reviewers to psychologically dis-tance themselves from their negative statements, aphenomenon observed in several other deceptionstudies, e.g., Hancock et al(2008).5005 ConclusionWe have created the first publicly-available corpusof gold standard negative deceptive opinion spam,containing 400 reviews of 20 Chicago hotels, whichwe have used to compare the deception detection ca-pabilities of untrained human judges and standardn-gram?based Support Vector Machine classifiers.Our results demonstrate that while human deceptiondetection performance is greater for negative ratherthan positive deceptive opinion spam, the best detec-tion performance is still achieved through automatedclassifiers, with approximately 86% accuracy.We have additionally explored, albeit briefly, therelationship between sentiment and deception byutilizing Ott et al(2011)?s positive deceptive opin-ion spam dataset in conjunction with our own.
Inparticular, we have identified several features of lan-guage that seem to remain consistent across senti-ment, such as decreased awareness of spatial detailsand exaggerated language.
We have also identifiedother features that vary with the sentiment, such asfirst person singular use, although further work is re-quired to determine if these differences may be ex-ploited to improve deception detection performance.Indeed, future work may wish to jointly model sen-timent and deception in order to better determine theeffect each has on language use.AcknowledgmentsThis work was supported in part by NSF Grant BCS-0904822, a DARPA Deft grant, the Jack Kent CookeFoundation, and by a gift from Google.
We also thankthe three Cornell undergraduate volunteer judges, as wellas the NAACL reviewers for their insightful comments,suggestions and advice on various aspects of this work.ReferencesD.
Biber, S. Johansson, G. Leech, S. Conrad, E. Finegan,and R. Quirk.
1999.
Longman grammar of spoken andwritten English, volume 2.
MIT Press.C.F.
Bond and B.M.
DePaulo.
2006.
Accuracy of de-ception judgments.
Personality and Social PsychologyReview, 10(3):214.Cone.
2011.
2011 Online Influence Trend Tracker.
On-line: http://www.coneinc.com/negative-reviews-online-reverse-purchase-decisions, August.P.
Ekman and W.V.
Friesen.
1969.
Nonverbal leakageand clues to deception.
Psychiatry, 32(1):88.Song Feng, Ritwik Banerjee, and Yejin Choi.
2012.
Syn-tactic stylometry for deception detection.
In Proceed-ings of the 50th Annual Meeting of the Associationfor Computational Linguistics: Short Papers-Volume2, pages 171?175.
Association for Computational Lin-guistics.J.T.
Hancock, L.E.
Curry, S. Goorha, and M. Woodworth.2008.
On lying and being lied to: A linguistic anal-ysis of deception in computer-mediated communica-tion.
Discourse Processes, 45(1):1?23.N.
Jindal and B. Liu.
2008.
Opinion spam and analysis.In Proceedings of the international conference on Websearch and web data mining, pages 219?230.
ACM.M.K.
Johnson and C.L.
Raye.
1981.
Reality monitoring.Psychological Review, 88(1):67?85.J.R.
Landis and G.G.
Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33(1):159.Dina Mayzlin, Yaniv Dover, and Judith A Chevalier.2012.
Promotional reviews: An empirical investiga-tion of online review manipulation.
Technical report,National Bureau of Economic Research.R.
Mihalcea and C. Strapparava.
2009.
The lie detector:Explorations in the automatic recognition of deceptivelanguage.
In Proceedings of the ACL-IJCNLP 2009Conference Short Papers, pages 309?312.
Associationfor Computational Linguistics.M.L.
Newman, J.W.
Pennebaker, D.S.
Berry, and J.M.Richards.
2003.
Lying words: Predicting deceptionfrom linguistic styles.
Personality and Social Psychol-ogy Bulletin, 29(5):665.M.
Ott, Y. Choi, C. Cardie, and J.T.
Hancock.
2011.Finding deceptive opinion spam by any stretch of theimagination.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies-Volume 1, pages 309?319.
Association for Computational Linguistics.Myle Ott, Claire Cardie, and Jeff Hancock.
2012.
Es-timating the prevalence of deception in online reviewcommunities.
In Proceedings of the 21st internationalconference on World Wide Web, pages 201?210.
ACM.J.W.
Pennebaker, C.K.
Chung, M. Ireland, A. Gonzales,and R.J. Booth.
2007.
The development and psycho-metric properties of LIWC2007.
Austin, TX: LIWC(www.liwc.net).P.
Rayson, A. Wilson, and G. Leech.
2001.
Grammaticalword class variation within the British National Cor-pus sampler.
Language and Computers, 36(1):295?306.R.
A. Rigby and D. M. Stasinopoulos.
2005.
Generalizedadditive models for location, scale and shape,(with dis-cussion).
Applied Statistics, 54:507?554.501
