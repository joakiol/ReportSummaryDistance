Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 573?581,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsType-Based MCMCPercy LiangUC Berkeleypliang@cs.berkeley.eduMichael I. JordanUC Berkeleyjordan@cs.berkeley.eduDan KleinUC Berkeleyklein@cs.berkeley.eduAbstractMost existing algorithms for learning latent-variable models?such as EM and existingGibbs samplers?are token-based, meaningthat they update the variables associated withone sentence at a time.
The incremental na-ture of these methods makes them suscepti-ble to local optima/slow mixing.
In this paper,we introduce a type-based sampler, which up-dates a block of variables, identified by a type,which spans multiple sentences.
We show im-provements on part-of-speech induction, wordsegmentation, and learning tree-substitutiongrammars.1 IntroductionA long-standing challenge in NLP is the unsu-pervised induction of linguistic structures, for ex-ample, grammars from raw sentences or lexiconsfrom phoneme sequences.
A fundamental propertyof these unsupervised learning problems is multi-modality.
In grammar induction, for example, wecould analyze subject-verb-object sequences as ei-ther ((subject verb) object) (mode 1) or (subject(verb object)) (mode 2).Multimodality causes problems for token-basedprocedures that update variables for one example ata time.
In EM, for example, if the parameters al-ready assign high probability to the ((subject verb)object) analysis, re-analyzing the sentences in E-steponly reinforces the analysis, resulting in EM gettingstuck in a local optimum.
In (collapsed) Gibbs sam-pling, if all sentences are already analyzed as ((sub-ject verb) object), sampling a sentence conditioned2 1 2 2 12 2 2 1 21 2 1 2 22 1 2 2 12 2 2 1 21 2 1 2 22 1 2 2 12 2 2 1 21 2 1 2 2(a) token-based (b) sentence-based (c) type-basedFigure 1: Consider a dataset of 3 sentences, each oflength 5.
Each variable is labeled with a type (1 or 2).
Theunshaded variables are the ones that are updated jointlyby a sampler.
The token-based sampler updates the vari-able for one token at a time (a).
The sentence-based sam-pler updates all variables in a sentence, thus dealing withintra-sentential dependencies (b).
The type-based sam-pler updates all variables of a particular type (1 in this ex-ample), thus dealing with dependencies due to commonparameters (c).on all others will most likely not change its analysis,resulting in slow mixing.To combat the problems associated with token-based algorithms, we propose a new sampling algo-rithm that operates on types.
Our sampler would, forexample, be able to change all occurrences of ((sub-ject verb) object) to (subject (verb object)) in onestep.
These type-based operations are reminiscent ofthe type-based grammar operations of early chunk-merge systems (Wolff, 1988; Stolcke and Omohun-dro, 1994), but we work within a sampling frame-work for increased robustness.In NLP, perhaps the the most simple and popu-lar sampler is the token-based Gibbs sampler,1 usedin Goldwater et al (2006), Goldwater and Griffiths(2007), and many others.
By sampling only one1In NLP, this is sometimes referred to as simply the col-lapsed Gibbs sampler.573variable at a time, this sampler is prone to slow mix-ing due to the strong coupling between variables.A general remedy is to sample blocks of coupledvariables.
For example, the sentence-based samplersamples all the variables associated with a sentenceat once (e.g., the entire tag sequence).
However, thisblocking does not deal with the strong type-basedcoupling (e.g., all instances of a word should betagged similarly).
The type-based sampler we willpresent is designed exactly to tackle this coupling,which we argue is stronger and more important todeal with in unsupervised learning.
Figure 1 depictsthe updates made by each of the three samplers.We tested our sampler on three models: aBayesian HMM for part-of-speech induction (Gold-water and Griffiths, 2007), a nonparametricBayesian model for word segmentation (Goldwateret al, 2006), and a nonparametric Bayesian model oftree substitution grammars (Cohn et al, 2009; Postand Gildea, 2009).
Empirically, we find that type-based sampling improves performance and is lesssensitive to initialization (Section 5).2 Basic Idea via a Motivating ExampleThe key technical problem we solve in this paper isfinding a block of variables which are both highlycoupled and yet tractable to sample jointly.
Thissection illustrates the main idea behind type-basedsampling on a small word segmentation example.Suppose our dataset x consists of n occurrencesof the sequence a b.
Our goal is infer z =(z1, .
.
.
, zn), where zi = 0 if the sequence is oneword ab, and zi = 1 if the sequence is two, aand b.
We can model this situation with a simplegenerative model: for each i = 1, .
.
.
, n, gener-ate one or two words with equal probability.
Eachword is drawn independently based on probabilities?
= (?a, ?b, ?ab) which we endow with a uniformprior ?
?
Dirichlet(1, 1, 1).We marginalize out ?
to get the following standardexpression (Goldwater et al, 2009):p(z | x) ?1(m)1(m)1(n?m)3(n+m)def= g(m), (1)where m =?ni=1 zi is the number of two-word se-quences and a(k) = a(a + 1) ?
?
?
(a + k ?
1) is the200 400 600 8001000m-1411.4-1060.3-709.1-358.0-6.8logg(m)2 4 6 8 10iteration2004006008001000mTokenType(a) bimodal posterior (b) sampling runFigure 2: (a) The posterior (1) is sharply bimodal (notethe log-scale).
(b) A run of the token-based and type-based samplers.
We initialize both samplers with m = n(n = 1000).
The type-based sampler mixes instantly(in fact, it makes independent draws from the posterior)whereas the token-based sampler requires five passesthrough the data before finding the high probability re-gion m u 0.ascending factorial.2 Figure 2(a) depicts the result-ing bimodal posterior.A token-based sampler chooses one zi to updateaccording to the posterior p(zi | z?i,x).
To illus-trate the mixing problem, consider the case wherem = n, i.e., all sequences are analyzed as twowords.
From (1), we can verify that p(zi = 0 |z?i,x) = O( 1n).
When n = 1000, this means thatthere is only a 0.002 probability of setting zi = 0,a very unlikely but necessary first step to take to es-cape this local optimum.
Indeed, Figure 2(b) showshow the token-based sampler requires five passesover the data to finally escape.Type-based sampling completely eradicates thelocal optimum problem in this example.
Let us takea closer look at (1).
Note that p(z | x) only dependson a single integer m, which only takes one of n+ 1values, not on the particular z.
This shows that thezis are exchangeable.
There are(nm)possible val-ues of z satisfying m =?i zi, each with the sameprobability g(m).
Summing, we get:p(m | x) ?
?z:m=Pi zip(x, z) =(nm)g(m).
(2)A sampling strategy falls out naturally: First, samplethe number m via (2).
Conditioned on m, choose2The ascending factorial function arises from marginaliz-ing Dirichlet distributions and is responsible the rich-gets-richerphenomenon: the larger n is, more we gain by increasing it.574the particular z uniformly out of the(nm)possibili-ties.
Figure 2(b) shows the effectiveness of this type-based sampler.This simple example exposes the fundamentalchallenge of multimodality in unsupervised learn-ing.
Both m = 0 and m = n are modes due to therich-gets-richer property which arises by virtue ofall n examples sharing the same parameters ?.
Thissharing is a double-edged sword: It provides us withclustering structure but also makes inference hard.Even though m = n is much worse (by a factor ex-ponential in n) than m = 0, a na?
?ve algorithm caneasily have trouble escaping m = n.3 SetupWe will now present the type-based sampler in fullgenerality.
Our sampler is applicable to any modelwhich is built out of local multinomial choices,where each multinomial has a Dirichlet process prior(a Dirichlet prior if the number of choices is finite).This includes most probabilistic models in NLP (ex-cluding ones built from log-linear features).As we develop the sampler, we will pro-vide concrete examples for the Bayesian hiddenMarkov model (HMM), the Dirichlet process uni-gram segmentation model (USM) (Goldwater et al,2006), and the probabilistic tree-substitution gram-mar (PTSG) (Cohn et al, 2009; Post and Gildea,2009).3.1 Model parametersA model is specified by a collection of multino-mial parameters ?
= {?r}r?R, where R is an in-dex set.
Each vector ?r specifies a distribution overoutcomes: outcome o has probability ?ro.?
HMM: Let K is the number of states.
The setR = {(q, k) : q ?
{T,E}, k = 1, .
.
.
,K}indexes the K transition distributions {?
(T,k)}(each over outcomes {1, .
.
.
,K}) and K emis-sion distributions {?
(E,k)} (each over the set ofwords).?
USM: R = {0}, and ?0 is a distribution over (aninfinite number of) words.?
PTSG: R is the set of grammar symbols, andeach ?r is a distribution over labeled tree frag-ments with root label r.R index set for parameters?
= {?r}r?R multinomial parameters?
= {?r}r?R base distributions (fixed)S set of sitesb = {bs}s?S binary variables (to be sampled)z latent structure (set of choices)z?s choices not depending on site szs:b choices after setting bs = b?zs:b zs:b\z?s: new choices from bs = bS ?
S sites selected for samplingm # sites in S assigned bs = 1n = {nro} counts (sufficient statistics of z)Table 1: Notation used in this paper.
Note that there is aone-to-one mapping between z and (b,x).
The informa-tion relevant for evaluating the likelihood is n. We usethe following parallel notation: n?s = n(z?s),ns:b =n(zs:b),?ns = n(?zs).3.2 Choice representation of latent structure zWe represent the latent structure z as a set of localchoices:3?
HMM: z contains elements of the form(T, i, a, b), denoting a transition from statea at position i to state b at position i + 1; and(E, i, a, w), denoting an emission of word wfrom state a at position i.?
USM: z contains elements of the form (i, w), de-noting the generation of word w at character po-sition i extending to position i+ |w| ?
1.?
PTSG: z contains elements of the form (x, t), de-noting the generation of tree fragment t rooted atnode x.The choices z are connected to the parameters ?as follows: p(z | ?)
=?z?z ?z.r,z.o.
Each choicez ?
z is identified with some z.r ?
R and out-come z.o.
Intuitively, choice z was made by drawingdrawing z.o from the multinomial distribution ?z.r.3.3 PriorWe place a Dirichlet process prior on ?r (Dirichletprior for finite outcome spaces): ?r ?
DP(?r, ?r),where ?r is a concentration parameter and ?r is afixed base distribution.3We assume that z contains both a latent part and the ob-served input x, i.e., x is a deterministic function of z.575Let nro(z) = |{z ?
z : z.r = r, z.o = o}| be thenumber of draws from ?r resulting in outcome o, andnr?
=?o nro be the number of times ?r was drawnfrom.
Let n(z) = {nro(z)} denote the vector ofsufficient statistics associated with choices z. Whenit is clear from context, we simply write n for n(z).Using these sufficient statistics, we can write p(z |?)
=?r,o ?nro(z)ro .We now marginalize out ?
using Dirichlet-multinomial conjugacy, producing the following ex-pression for the likelihood:p(z) =?r?R?o (?ro?ro)(nro(z))?r(nr?
(z)), (3)where a(k) = a(a+1) ?
?
?
(a+k?1) is the ascendingfactorial.
(3) is the distribution that we will use forsampling.4 Type-Based SamplingHaving described the setup of the model, we nowturn to posterior inference of p(z | x).4.1 Binary RepresentationWe first define a new representation of the latentstructure based on binary variables b so that there isa bijection between z and (b,x); z was used to de-fine the model, b will be used for inference.
We willuse b to exploit the ideas from Section 2.
Specifi-cally, let b = {bs}s?S be a collection of binary vari-ables indexed by a set of sites S.?
HMM: If the HMM hasK = 2 states, S is the setof positions in the sequence.
For each s ?
S , bsis the hidden state at s. The extension to generalK is considered at the end of Section 4.4.?
USM: S is the set of non-final positions in thesequence.
For each s ?
S , bs denotes whethera word boundary exists between positions s ands+ 1.?
PTSG: S is the set of internal nodes in the parsetree.
For s ?
S, bs denotes whether a tree frag-ment is rooted at node s.For each site s ?
S, let zs:0 and zs:1 denote thechoices associated with the structures obtained bysetting the binary variable bs = 0 and bs = 1, re-spectively.
Define z?sdef= zs:0 ?
zs:1 to be the setof choices that do not depend on the value of bs, andn?sdef= n(z?s) be the corresponding counts.?
HMM: z?s includes all but the transitions intoand out of the state at s plus the emission at s.?
USM: z?s includes all except the word ending ats and the one starting at s+ 1 if there is a bound-ary (bs = 1); except the word covering s if noboundary exists (bs = 0).?
PTSG: z?s includes all except the tree fragmentrooted at node s and the one with leaf s if bs = 1;except the single fragment containing s if bs = 0.4.2 Sampling One SiteA token-based sampler considers one site s at a time.Specifically, we evaluate the likelihoods of zs:0 andzs:1 according to (3) and sample bs with probabilityproportional to the likelihoods.
Intuitively, this canbe accomplished by removing choices that dependon bs (resulting in z?s), evaluating the likelihood re-sulting from setting bs to 0 or 1, and then adding theappropriate choices back in.More formally, let ?zs:bdef= zs:b\z?s be the newchoices that would be added if we set bs = b ?
{0, 1}, and let ?ns:bdef= n(?zs:b) be the corre-sponding counts.
With this notation, we can writethe posterior as follows:p(bs = b | b\bs) ?
(4)?r?R?o (?ro?ro + n?sro )(?ns:bro )(?r + n?sr?
)(?ns:br?
).The form of the conditional (4) follows from thejoint (3) via two properties: additivity of counts(ns:b = n?s + ?ns:b) and a simple property of as-cending factorials (a(k+?)
= a(k)(a+ k)(?
)).In practice, most of the entries of ?ns:b are zero.For the HMM, ns:bro would be nonzero only forthe transitions into the new state (b) at position s(zs?1 ?
b), transitions out of that state (b?
zs+1),and emissions from that state (b?
xs).4.3 Sampling Multiple SitesWe would like to sample multiple sites jointly as inSection 2, but we cannot choose any arbitrary subsetS ?
S, as the likelihood will in general depend onthe exact assignment of bSdef= {bs}s?S , of which576a b c a a b c a b c b(a) USM1 1 2 2 1 1 2 2a b a b c b b e(b) HMMaba ab cd ecdb cea b(c) PTSGFigure 3: The type-based sampler jointly samples all vari-ables at a set of sites S (in green boxes).
Sites in S arechosen based on types (denoted in red).
(a) HMM: twosites have the same type if they have the same previousand next states and emit the same word; they conflict un-less separated by at least one position.
(b) USM: two siteshave the same type if they are both of the form ab|c orabc; note that occurrences of the same letters with othersegmentations do not match the type.
(c) PTSG: analo-gous to the USM, only for tree rather than sequences.there are an exponential number.
To exploit the ex-changeability property in Section 2, we need to findsites which look ?the same?
from the model?s pointof view, that is, the likelihood only depends on bSvia mdef=?s?S bs.To do this, we need to define two notions, type andconflict.
We say sites s and s?
have the same type ifthe counts added by setting either bs or bs?
are thesame, that is, ?ns:b = ?ns?
:b for b ?
{0, 1}.
Thismotivates the following definition of the type of sites with respect to z:t(z, s)def= (?ns:0,?ns:1), (5)We say that s and s?
have the same type if t(z, s) =t(z, s?).
Note that the actual choices added (?zs:band ?zs?
:b) are in general different as s and s?
cor-respond to different parts of the latent structure, butthe model only depends on counts and is indifferentto this.
Figure 3 shows examples of same-type sitesfor our three models.However, even if all sites in S have the sametype, we still cannot sample bS jointly, since chang-ing one bs might change the type of another site s?
;indeed, this dependence is reflected in (5), whichshows that types depend on z.
For example, s, s?
?S conflict when s?
= s + 1 in the HMM or whens and s?
are boundaries of one segment (USM) orone tree fragment (PTSG).
Therefore, one additionalconcept is necessary: We say two sites s and s?
con-flict if there is some choice that depends on both bsand bs?
; formally, (z\z?s) ?
(z\z?s?)
6= ?.Our key mathematical result is as follows:Proposition 1 For any set S ?
S of non-conflictingsites with the same type,p(bS | b\bS) ?
g(m) (6)p(m | b\bS) ?
(|S|m)g(m), (7)for some easily computable g(m), where m =?s?S bs.We will derive g(m) shortly, but first note from(6) that the likelihood for a particular setting of bSdepends on bS only via m as desired.
(7) sumsover all(|S|m)settings of bS with m =?s?S bs.The algorithmic consequences of this result is thatto sample bS , we can first compute (7) for eachm ?
{0, .
.
.
, |S|}, sample m according to the nor-malized distribution, and then choose the actual bSuniformly subject to m.Let us now derive g(m) by generalizing (4).Imagine removing all sites S and their dependentchoices and adding in choices corresponding tosome assignment bS .
Since all sites in S are non-conflicting and of the same type, the count contribu-tion ?ns:b is the same for every s ?
S (i.e., sitesin S are exchangeable).
Therefore, the likelihoodof the new assignment bS depends only on the newcounts:?nS:mdef= m?ns:1 + (|S| ?m)?ns:0.
(8)Using these new counts in place of the ones in (4),we get the following expression:g(m) =?r?R?o (?ro?ro + nro(z?S))(?nS:mro )?r + nr?(z?S)(?nS:mr?
).
(9)4.4 Full AlgorithmThus far, we have shown how to sample bS givena set S ?
S of non-conflicting sites with the sametype.
To complete the description of the type-based577Type-Based Samplerfor each iteration t = 1, .
.
.
, T :?for each pivot site s0 ?
S:?
?S ?
TB(z, s0) (S is the type block centered at s0)?
?decrement n and remove from z based on bS?
?sample m according to (7)?
?sample M ?
S with |M | = m uniformly at random?
?set bs = I[s ?M ] for each s ?
S?
?increment n and add to z accordinglyFigure 4: Pseudocode for the general type-based sampler.We operate in the binary variable representation b of z.Each step, we jointly sample |S| variables (of the sametype).sampler, we need to specify how to choose S. Ourgeneral strategy is to first choose a pivot site s0 ?
Suniformly at random and then set S = TB(z, s0) forsome function TB.
Call S the type block centered ats0.
The following two criteria on TB are sufficientfor a valid sampler: (A) s0 ?
S, and (B) the typeblocks are stable, which means that if we change bSto any b?S (resulting in a new z?
), the type block cen-tered at s0 with respect to z?
does not change (thatis, TB(z?, s0) = S).
(A) ensures ergodicity; (B),reversibility.Now we define TB as follows: First set S = {s0}.Next, loop through all sites s ?
S with the same typeas s0 in some fixed order, adding s to S if it doesnot conflict with any sites already in S. Figure 4provides the pseudocode for the full algorithm.Formally, this sampler cycles over |S| transitionkernels, one for each pivot site.
Each kernel (in-dexed by s0 ?
S) defines a blocked Gibbs move,i.e.
sampling from p(bTB(z,s0) | ?
?
?
).Efficient Implementation There are two oper-ations we must perform efficiently: (A) loopingthrough sites with the same type as the pivot site s0,and (B) checking whether such a site s conflicts withany site in S. We can perform (B) in O(1) time bychecking if any element of ?zs:bs has already beenremoved; if so, there is a conflict and we skip s. Todo (A) efficiently, we maintain a hash table mappingtype t to a doubly-linked list of sites with type t.There is anO(1) cost for maintaining this data struc-ture: When we add or remove a site s, we just needto add or remove neighboring sites s?
from their re-spective linked lists, since their types depend on bs.For example, in the HMM, when we remove site s,we also remove sites s?1 and s+1.For the USM, we use a simpler solution: main-tain a hash table mapping each word w to a list ofpositions where w occurs.
Suppose site (position) sstraddles words a and b.
Then, to perform (A), weretrieve the list of positions where a, b, and ab occur,intersecting the a and b lists to obtain a list of posi-tions where a b occurs.
While this intersection isoften much smaller than the pre-intersected lists, wefound in practice that the smaller amount of book-keeping balanced out the extra time spent intersect-ing.
We used a similar strategy for the PTSG, whichsignificantly reduces the amount of bookkeeping.Skip Approximation Large type blocks meanlarger moves.
However, such a block S is also sam-pled more frequently?once for every choice of apivot site s0 ?
S. However, we found that empir-ically, bS changes very infrequently.
To eliminatethis apparent waste, we use the following approxi-mation of our sampler: do not consider s0 ?
S asa pivot site if s0 belongs to some block which wasalready sampled in the current iteration.
This way,each site is considered roughly once per iteration.4Sampling Non-Binary Representations We cansample in models without a natural binary represen-tation (e.g., HMMs with with more than two states)by considering random binary slices.
Specifically,suppose bs ?
{1, .
.
.
,K} for each site s ?
S .We modify Figure 4 as follows: After choosing apivot site s0 ?
S , let k = bs0 and choose k?
uni-formly from {1, .
.
.
,K}.
Only include sites in oneof these two states by re-defining the type block tobe S = {s ?
TB(z, s0) : bs ?
{k, k?
}}, and sam-ple bS restricted to these two states by drawing fromp(bS | bS ?
{k, k?
}|S|, ?
?
?
).
By choosing a randomk?
each time, we allow b to reach any point in thespace, thus achieving ergodicity just by using thesebinary restrictions.5 ExperimentsWe now compare our proposed type-based samplerto various alternatives, evaluating on marginal like-4A site could be sampled more than once if it belonged tomore than one type block during the iteration (recall that typesdepend on z and thus could change during sampling).578lihood (3) and accuracy for our three models:?
HMM: We learned a K = 45 state HMM onthe Wall Street Journal (WSJ) portion of the PennTreebank (49208 sentences, 45 tags) for part-of-speech induction.
We fixed ?r to 0.1 and ?r touniform for all r.For accuracy, we used the standard metric basedon greedy mapping, where each state is mappedto the POS tag that maximizes the number of cor-rect matches (Haghighi and Klein, 2006).
We didnot use a tagging dictionary.?
USM: We learned a USM model on theBernstein-Ratner corpus from the CHILDESdatabase used in Goldwater et al (2006) (9790sentences) for word segmentation.
We fixed ?0 to0.1.
The base distribution ?0 penalizes the lengthof words (see Goldwater et al (2009) for details).For accuracy, we used word token F1.?
PTSG: We learned a PTSG model on sections 2?21 of the WSJ treebank.5 For accuracy, we usedEVALB parsing F1 on section 22.6 Note this is asupervised task with latent-variables, whereas theother two are purely unsupervised.5.1 Basic ComparisonFigure 5(a)?
(c) compares the likelihood and accu-racy (we use the term accuracy loosely to also in-clude F1).
The initial observation is that the type-based sampler (TYPE) outperforms the token-basedsampler (TOKEN) across all three models on bothmetrics.We further evaluated the PTSG on parsing.
Ourstandard treebank PCFG estimated using maximumlikelihood obtained 79% F1.
TOKEN obtained an F1of 82.2%, and TYPE obtained a comparable F1 of83.2%.
Running the PTSG for longer continued to5Following Petrov et al (2006), we performed an initial pre-processing step on the trees involving Markovization, binariza-tion, and collapsing of unary chains; words occurring once arereplaced with one of 50 ?unknown word?
tokens, using basedistributions {?r} that penalize the size of trees, and samplingthe hyperparameters (see Cohn et al (2009) for details).6To evaluate, we created a grammar where the rule proba-bilities are the mean values under the PTSG distribution: thisinvolves taking a weighted combination (based on the concen-tration parameters) of the rule counts from the PTSG samplesand the PCFG-derived base distribution.
We used the decoderof DeNero et al (2009) to parse.improve the likelihood but actually hurt parsing ac-curacy, suggesting that the PTSG model is overfit-ting.To better understand the gains from TYPEover TOKEN, we consider three other alterna-tive samplers.
First, annealing (TOKENanneal) isa commonly-used technique to improve mixing,where (3) is raised to some inverse temperature.7In Figure 5(a)?
(c), we see that unlike TYPE,TOKENanneal does not improve over TOKEN uni-formly: it hurts for the HMM, improves slightly forthe USM, and makes no difference for the PTSG.
Al-though annealing does increase mobility of the sam-pler, this mobility is undirected, whereas type-basedsampling increases mobility in purely model-drivendirections.Unlike past work that operated on types (Wolff,1988; Brown et al, 1992; Stolcke and Omohun-dro, 1994), type-based sampling makes stochasticchoices, and moreover, these choices are reversible.Is this stochasticity important?
To answer this, weconsider a variant of TYPE, TYPEgreedy: insteadof sampling from (7), TYPEgreedy considers a typeblock S and sets bs to 0 for all s ?
S if p(bS =(0, .
.
.
, 0) | ?
?
? )
> p(bS = (1, .
.
.
, 1) | ?
?
?
); elseit sets bs to 1 for all s ?
S. From Figure 5(a)?
(c),we see that greediness is disastrous for the HMM,hurts a little for USM, and makes no difference onthe PTSG.
These results show that stochasticity canindeed be important.We consider another block sampler, SENTENCE,which uses dynamic programming to sample allvariables in a sentence (using Metropolis-Hastingsto correct for intra-sentential type-level coupling).For USM, we see that SENTENCE performs worsethan TYPE and is comparable to TOKEN, suggestingthat type-based dependencies are stronger and moreimportant to deal with than intra-sentential depen-dencies.5.2 InitializationWe initialized all samplers as follows: For the USMand PTSG, for each site s, we place a boundary (setbs = 1) with probability ?.
For the HMM, we set bsto state 1 with probability ?
and a random state with7We started with a temperature of 10 and gradually de-creased it to 1 during the first half of the run, and kept it at 1thereafter.5793 6 9 12time (hr.
)-1.1e7-0.9e7-9.1e6-7.9e6-6.7e6log-likelihood3 6 9 12time (hr.
)0.10.20.40.50.6accuracy2 4 6 8time (min.
)-3.7e5-3.2e5-2.8e5-2.4e5-1.9e5log-likelihood TokenTokenannealTypegreedyTypeSentence2 4 6 8time (min.
)0.10.20.40.50.6F 13 6 9 12time (hr.
)-6.2e6-6.0e6-5.8e6-5.7e6-5.5e6log-likelihood(a) HMM (b) USM (c) PTSG0.2 0.4 0.6 0.8 1.0?-7.1e6-7.0e6-6.9e6-6.8e6-6.7e6log-likelihood0.2 0.4 0.6 0.8 1.0?0.20.30.40.50.6accuracy0.2 0.4 0.6 0.8 1.0?-3.5e5-3.1e5-2.7e5-2.3e5-1.9e5log-likelihood0.2 0.4 0.6 0.8 1.0?0.20.30.40.50.6F 10.2 0.4 0.6 0.8 1.0?-5.7e6-5.6e6-5.6e6-5.5e6-5.5e6log-likelihood(d) HMM (e) USM (f) PTSGFigure 5: (a)?
(c): Log-likelihood and accuracy over time.
TYPE performs the best.
Relative to TYPE, TYPEgreedytends to hurt performance.
TOKEN generally works worse.
Relative to TOKEN, TOKENanneal produces mixed results.SENTENCE behaves like TOKEN.
(d)?
(f): Effect of initialization.
The metrics were applied to the current sample after15 hours for the HMM and PTSG and 10 minutes for the USM.
TYPE generally prefers larger ?
and outperform theother samplers.probability 1 ?
?.
Results in Figure 5(a)?
(c) wereobtained by setting ?
to maximize likelihood.Since samplers tend to be sensitive to initializa-tion, it is important to explore the effect of initial-ization (parametrized by ?
?
[0, 1]).
Figure 5(d)?
(f)shows that TYPE is consistently the best, whereasother samplers can underperform TYPE by a largemargin.
Note that TYPE favors ?
= 1 in general.This setting maximizes the number of initial types,and thus creates larger type blocks and thus enableslarger moves.
Larger type blocks also mean moredependencies that TOKEN is unable to deal with.6 Related Work and DiscussionBlock sampling, on which our work is built, is a clas-sical idea, but is used restrictively since samplinglarge blocks is computationally expensive.
Pastwork for clustering models maintained tractabil-ity by using Metropolis-Hastings proposals (Dahl,2003) or introducing auxiliary variables (Swendsenand Wang, 1987; Liang et al, 2007).
In contrast,our type-based sampler simply identifies tractableblocks based on exchangeability.Other methods for learning latent-variable modelsinclude EM, variational approximations, and uncol-lapsed samplers.
All of these methods maintain dis-tributions over (or settings of) the latent variables ofthe model and update the representation iteratively(see Gao and Johnson (2008) for an overview in thecontext of POS induction).
However, these methodsare at the core all token-based, since they only up-date variables in a single example at a time.8Blocking variables by type?the key idea ofthis paper?is a fundamental departure from token-based methods.
Though type-based changes havealso been proposed (Brown et al, 1992; Stolcke andOmohundro, 1994), these methods operated greed-ily, and in Section 5.1, we saw that being greedy ledto more brittle results.
By working in a samplingframework, we were able bring type-based changesto fruition.8While EM technically updates all distributions over latentvariables in the E-step, this update is performed conditioned onmodel parameters; it is this coupling (made more explicit incollapsed samplers) that makes EM susceptible to local optima.580ReferencesP.
F. Brown, V. J. D. Pietra, P. V. deSouza, J. C. Lai, andR.
L. Mercer.
1992.
Class-based n-gram models ofnatural language.
Computational Linguistics, 18:467?479.T.
Cohn, S. Goldwater, and P. Blunsom.
2009.
Inducingcompact but accurate tree-substitution grammars.
InNorth American Association for Computational Lin-guistics (NAACL), pages 548?556.D.
B. Dahl.
2003.
An improved merge-split sampler forconjugate Dirichlet process mixture models.
Techni-cal report, Department of Statistics, University of Wis-consin.J.
DeNero, M. Bansal, A. Pauls, and D. Klein.
2009.Efficient parsing for transducer grammars.
In NorthAmerican Association for Computational Linguistics(NAACL), pages 227?235.J.
Gao and M. Johnson.
2008.
A comparison ofBayesian estimators for unsupervised hidden Markovmodel POS taggers.
In Empirical Methods in NaturalLanguage Processing (EMNLP), pages 344?352.S.
Goldwater and T. Griffiths.
2007.
A fully Bayesianapproach to unsupervised part-of-speech tagging.
InAssociation for Computational Linguistics (ACL).S.
Goldwater, T. Griffiths, and M. Johnson.
2006.
Con-textual dependencies in unsupervised word segmenta-tion.
In International Conference on ComputationalLinguistics and Association for Computational Lin-guistics (COLING/ACL).S.
Goldwater, T. Griffiths, and M. Johnson.
2009.
ABayesian framework for word segmentation: Explor-ing the effects of context.
Cognition, 112:21?54.A.
Haghighi and D. Klein.
2006.
Prototype-driven learn-ing for sequence models.
In North American Associ-ation for Computational Linguistics (NAACL), pages320?327.P.
Liang, M. I. Jordan, and B. Taskar.
2007.
Apermutation-augmented sampler for Dirichlet processmixture models.
In International Conference on Ma-chine Learning (ICML).S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In International Conference on Computa-tional Linguistics and Association for ComputationalLinguistics (COLING/ACL), pages 433?440.M.
Post and D. Gildea.
2009.
Bayesian learning of atree substitution grammar.
In Association for Com-putational Linguistics and International Joint Confer-ence on Natural Language Processing (ACL-IJCNLP).A.
Stolcke and S. Omohundro.
1994.
Inducing prob-abilistic grammars by Bayesian model merging.
InInternational Colloquium on Grammatical Inferenceand Applications, pages 106?118.R.
H. Swendsen and J. S. Wang.
1987.
Nonuniversalcritical dynamics in MC simulations.
Physics ReviewLetters, 58:86?88.J.
G. Wolff.
1988.
Learning syntax and meaningsthrough optimization and distributional analysis.
InCategories and processes in language acquisition,pages 179?215.581
