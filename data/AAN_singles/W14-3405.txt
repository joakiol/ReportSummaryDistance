Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 29?37,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsDecomposing Consumer Health QuestionsKirk Roberts, Halil Kilicoglu, Marcelo Fiszman, and Dina Demner-FushmanNational Library of MedicineNational Institutes of HealthBethesda, MD 20894robertske@nih.gov, {kilicogluh,fiszmanm,ddemner}@mail.nih.govAbstractThis paper presents a method for decom-posing long, complex consumer healthquestions.
Our approach largely decom-poses questions using their syntactic struc-ture, recognizing independent questionsembedded in clauses, as well as coordi-nations and exemplifying phrases.
Addi-tionally, we identify elements specific todisease-related consumer health questions,such as the focus disease and backgroundinformation.
To achieve this, our approachcombines rank-and-filter machine learningmethods with rule-based methods.
Ourresults demonstrate significant improve-ments over the heuristic methods typicallyemployed for question decomposition thatrely only on the syntactic parse tree.1 IntroductionNatural language questions provide an intuitivemethod for consumers (non-experts) to query forhealth-related content.
The most intuitive wayfor consumers to formulate written questions isthe same way they write to other humans: multi-sentence, complex questions that contain back-ground information and often more than one spe-cific question.
Consider the following:?
Will Fabry disease affect a transplanted kidney?Previous to the transplant the disease was be-ing managed with an enzyme supplement.
Willthis need to be continued?
What cautions or ad-ditional treatments are required to manage thedisease with a transplanted kidney?This complex question contains three questionsentences and one background sentence.
The fo-cus (Fabry disease) is stated in the first questionbut is necessary for a full understanding of theother questions as well.
The background sentenceis necessary to understand the second question:the anaphor this must be resolved to an enzymetreatment, and the predicate continue?s implicit ar-gument that must be re-constructed from the dis-course (i.e., continue after a kidney transplant).The final question sentence uses a coordinationto ask two separate questions (cautions and addi-tional treatments).
A decomposition of this com-plex question would then result in four questions:1.
Will Fabry disease affect a transplanted kidney?2.
Will enzyme treatment for Fabry disease need tobe continued after a kidney transplant?3.
What cautions are required to manage Fabrydisease with a transplanted kidney?4.
What additional treatments are required to man-age Fabry disease with a transplanted kidney?Each question above could be independently an-swered by a question answering (QA) system.While previous work has discussed methods forresolving co-reference and implicit arguments inconsumer health questions (Kilicoglu et al., 2013),it does not address question decomposition.In this work, we propose methods for auto-matically recognizing six annotation types use-ful for decomposing consumer health questions.These annotations distinguish between sentencesthat contain questions and background informa-tion.
They also identify when a question sentencecan be split in multiple independent questions, and29when they contain optional or coordinated infor-mation embedded within a question.For each of these decomposition annotations,we propose a combination of machine learning(ML) and rule based methods.
The ML methodslargely take the form of a 3-step rank-and-filterapproach, where candidates are generated, rankedby an ML classifier, then the top-ranked candidateis passed through a separate ML filtering classi-fier.
We evaluate each of these methods on a set of1,467 consumer health questions related to geneticand rare diseases.2 BackgroundQA in the biomedical domain has been well-studied (Demner-Fushman and Lin, 2007; Cairnset al., 2011; Cao et al., 2011) as a means for re-trieving medical information.
This work has typ-ically focused, however, on questions posed bymedical professionals, and the methods proposedfor question analysis generally assume a single,concise question.
For example, Demner-Fushmanand Abhyankar (2012) propose a method for ex-tracting frames from queries for the purpose ofcohort retrieval.
Their method assumes syntacticdependencies exist between the necessary frameelements, and is thus not well-suited to handlelong, multi-sentence questions.
Similarly, Ander-sen et al.
(2012) proposes a method for convertinga concise question into a structured query.
How-ever, many medical questions require backgroundinformation that is difficult to encode in a singlequestion sentence.
Instead, it is often more naturalto ask multiple questions over several sentences,providing background information to give contextto the questions.
Yu and Cao (2008) use a MLmethod to recognize question types in professionalhealth questions.
Their method can identify morethan one type per complex question.
Without de-composing the full question into its sub-questions,however, the type cannot be associated with itsspecific span, or with other information specific tothe sub-question.
This other information can in-clude answer types, question focus, and other an-swer constraints.
By decomposing multi-sentencequestions, these question-specific attributes can beextracted, and the discourse structure of the largerquestion can be better understood.Question decomposition has been utilized be-fore in open-domain QA approaches, but rarelyevaluated on its own.
Lacatusu et al.
(2006)demonstrates how question decomposition can im-prove the performance of a multi-sentence sum-marization system.
They perform what we referto as syntactic question decomposition, where thesyntactic structure of the question is used to iden-tify sub-questions that can be answered in isola-tion.
A second form of question decomposition issemantic decomposition, which can semanticallybreak individual questions apart to answer themin stages.
For instance, the question ?When didthe third U.S. President die??
can be semanticallydecomposed ?Who was the third U.S.
President?
?and ?When did X die?
?, where the answer to thefirst question is substituted into the second.
Katzand Grau (2005) discusses this kind of decompo-sition using the syntactic structure, though it is notempirically validated.
Hartrumpf (2008) proposesa decomposition method using only the deep se-mantic structure.
Finally, Harabagiu et al.
(2006)proposes a different type of question decomposi-tion based on a random walk over similar ques-tions extracted from a corpus.
In our work, wefocus on syntactic question decomposition.
Wedemonstrate the importance of empirical evalua-tion of question decomposition, notably the pit-falls of heuristic approaches that rely entirely onthe syntactic parse tree.
Syntactic parsers trainedon Treebank are particularly poor at both analyz-ing questions (Judge et al., 2006) and coordinationboundaries (Hogan, 2007).
Robust question de-composition methods, therefore, must be able toovercome many of these difficulties.3 Consumer Health QuestionDecompositionOur goal is to decompose multi-sentence, multi-faceted consumer health questions into concisequestions coupled with important contextual in-formation.
To this end, we utilize a set of an-notations that identify the decomposable elementsand important contextual elements.
A more de-tailed description of these annotations is providedin Roberts et al.
(2014).
The annotations are pub-licly available at our institution website1.
Here, webriefly describe each annotation:(1) BACKGROUND - a sentence indicating usefulcontextual information, but lacks a question.
(2) QUESTION - a sentence or clause that indi-cates an independent question.1http://lhncbc.nlm.nih.gov/project/consumer-health-question-answering30Sentence SplittingRequestQuestionSentenceIgnoreSentenceBackgroundSentenceCandidate GenerationUMLSSVM Candidate RankingBoundary FixingFocusFocus RecognitionSentence ClassificationBackground ClassificationSVM Comorbidity ClassificationSVM Diagnosis ClassificationSVM Family History ClassificationSVM ISF ClassificationSVM Lifestyle ClassificationSVM Symptom ClassificationSVM Test ClassificationCandidate GenerationSVM Candidate FilterQuestion RecognitionSVM Sentence ClassificationQuestionCandidate GenerationSVM Candidate RankingExemplification RecognitionCandidate FilterCandidate GenerationSVM Candidate RankingCoordination RecognitionSVM Candidate Filter CoordinationExemplificationStanfordParserWordNetSVM Treatment ClassificationFigure 1: Question Decomposition Architecture.
Modules with solid green lines indicate machine learn-ing classifiers.
Modules with dotted green lines indicate rule-based classifiers.
(3) COORDINATION - a phrase that spans a set ofdecomposable items.
(4) EXEMPLIFICATION - a phrase that spans anoptional item.
(5) IGNORE - a sentence indicating nothing ofvalue is present.
(6) FOCUS - an NP indicating the theme of theconsumer health question.Further explanations of each annotation are pro-vided in Sections 4-9.
To convert these annota-tions into separate, decomposed questions, a sim-ple set of recursive rules is used.
The rules enu-merate all ways of including one conjunct fromeach COORDINATION as well as whether or notto include the phrase within an EXEMPLIFICA-TION.
These rules must be applied recursively tohandle overlapping annotations (e.g., a COORDI-NATION within an EXEMPLIFICATION).
Our im-plementation is straight-forward and not discussedfurther in this paper.
The BACKGROUND and FO-CUS annotations do not play a direct role in thisprocess, though they provide important contextualelements and are useful for co-reference, and arethus still considered part of the overall decompo-sition process.It should also be noted that some questions aresyntactically decomposable, but doing so alterstheir original meaning.
Consider the followingtwo question sentences:?
Can this disease be cured or can we only treatthe symptoms??
Are males or females worse affected?While the first example contains two ?Can...?questions and the second example contains the co-ordination ?males or females?, both questions areproviding a choice between two alternatives anddecomposing them would alter the semantic na-ture of the original question.
In these cases, we donot consider the questions to be decomposable.Data We use a set of consumer health ques-tions collected from the Genetic and Rare Dis-eases Information Center (GARD), which main-tains a website2with publicly available consumer-submitted questions and professionally-authoredanswers about genetic and rare diseases.
We col-lected 1,467 consumer health questions, consist-ing of 4,115 sentences, 1,713 BACKGROUND sen-tences, 37 IGNORE sentences, 2,465 QUESTIONs,367 COORDINATIONs, 53 EXEMPLIFICATIONs,and 1,513 FOCUS annotations.
Questions withmore than one FOCUS are generally concernedwith the relation between diseases.
Further infor-mation about the corpus and the annotation pro-cess can be found in Roberts et al.
(2014).System Architecture The architecture of ourquestion decomposition method is illustrated in2http://rarediseases.info.nih.gov/gard31Figure 1.
To avoid confusion, in the rest of thispaper we refer to a complex consumer health ques-tion simply as a request.
Requests are sent tothe independent FOCUS recognition module (Sec-tion 4), and then proceed through a pipeline thatincludes the classification of sentences (Section 5),the identification of separate QUESTIONs withina question sentence (Section 6), the recognitionof COORDINATIONs (Section 7) and EXEMPLIFI-CATIONs (Section 8), and the sub-classification ofBACKGROUND sentences (Section 9).Experimental Setup The remainder of this pa-per describes the individual modules in Figure 1.For simplicity, we show results on the GARD datafor each task in its corresponding section.
In allcases, experiments are conducted using a 5-foldcross-validation on the GARD data.
The cross-validation folds are organized at the request levelso that no two items from the same request will besplit between the training and testing data.4 Identifying the Focal DiseaseThe FOCUS is the condition that disease-centeredquestions are centered around.
Many other dis-eases may be mentioned, but the FOCUS is the dis-ease of central concern.
This is similar to the as-sumption made about a central disease in Medlineabstracts (Demner-Fushman and Lin, 2007).
Of-ten the FOCUS is stated in the first sentence (typ-ically a BACKGROUND) of the request while thequestions are near the end.
The questions can-not generally be answered outside the context ofthe FOCUS, however, so its identification is a crit-ical part of decomposition.
As shown in Figure 1,we use a 3-step process: (1) a high-recall methodidentifies potential FOCUS diseases in the data, (2)a support vector machine (SVM) ranks the FO-CUS candidates, and (3) the highest-ranking can-didate?s boundary is modified with a set of rules tobetter match our annotation standard.To identify candidates for the FOCUS, we use alexicon constructed from UMLS (Lindberg et al.,1993).
UMLS includes very generic terms, such asdisease and cancer, that are too general to exactlymatch a FOCUS in our data.
We allow these termsto be candidates so as to not miss any FOCUS thatdoesn?t exactly match an entry in UMLS.
Whensuch a general term is selected as the top-rankedFOCUS, the rules described below are capable ofexpanding the term to the full disease name.To rank candidates, we utilize an SVM (Fan etE/R P R F11st UMLS DisorderE 19.6 19.0 19.3R 28.2 27.4 27.8SVME 56.4 54.7 55.6R 89.2 86.5 87.9SVM + RulesE 74.8 72.5 73.6R 89.5 86.8 88.1Table 1: FOCUS recognition results.
E = exactmatch; R = relaxed match.al., 2008) with a small number of feature types:?
Unigrams.
Identifies generic words such as dis-ease and syndrome that indicate good FOCUScandidates, while also recognizing noisy UMLSterms that are often false positives.?
UMLS semantic group (McCray et al., 2001).?
UMLS semantic type.?
Sentence Offset.
The FOCUS is typically in thefirst sentence, and is far more likely to be at thebeginning of the request than the end.?
Lexicon Offset.
The FOCUS is typically the firstdisease mentioned.During training, the SVM considers any candidatethat overlaps the gold FOCUS to be correct.
Thisenables our approach to train on FOCUS examplesthat do not perfectly align with a UMLS concept.At test time, all candidates are classified, rankedby the classifier?s confidence, and the top-rankedcandidate is considered the FOCUS.As mentioned above, there are differences be-tween how a FOCUS is annotated in our data andhow it is represented in the UMLS.
We thereforeuse a series of heuristics to alter the boundary to amore usable FOCUS after it is chosen by the SVM.The rules are applied iteratively to widen the FO-CUS boundary until it cannot be expanded any fur-ther.
If a generic disease word is the only tokenin the FOCUS, we add the token to the left.
Con-versely, if the token on the right is a generic dis-ease word, it is added as well.
If the word to theleft is capitalized, it is safe to assume it is part ofthe disease?s name and so it is added as well.
Fi-nally, several rules recognize the various ways inwhich a disease sub-type might be specified (e.g.,Behcet?s syndrome vascular type, type 2 diabetes,Charcot-Marie-Tooth disease type 2C).We evaluate FOCUS recognition with both anexact match, where the gold and automatic FOCUSboundaries must line up perfectly, and a relaxedmatch, which only requires a partial overlap.
As abaseline, we compare our results against a fullyrule-based system where the first UMLS Disor-der term in the request is considered the FOCUS.32We also evaluate the effectiveness of our bound-ary altering rules by measuring performance with-out these rules.
The results are shown in Table 1.The baseline method shows significant problemsin precision and recall.
It is not able to ignorenoisy UMLS terms (e.g., aim is both a gene anda treatment).
The SVM improves upon the rule-based method by over 50 points in F1for relaxedmatching.
Adding the boundary fixing rules haslittle effect on relaxed matching, but greatly im-proves exact matching: precision and recall areimproved by 18.4 and 17.8 points, respectively.5 Classifying SentencesBefore precise question boundaries can be rec-ognized, we first identify sentences that con-tain QUESTIONs, as distinguished from BACK-GROUND and IGNORE sentences.
It should benoted that many of the question sentences in ourdata are not typical wh-word questions.
About20% of the questions in our data end in a period.For instance:?
Please tell me more about this condition.?
I was wondering if you could let me know whereI can find more information on this topic.?
I would like to get in contact with other familiesthat have this illness.We consider a sentence to be a question if it con-tains any information request, explicit or implicit.After sentence splitting, we identify sentencesusing a multi-class SVM with three feature types:?
Unigrams with parts-of-speech (POS).
Reducesunigram ambiguities, such as what-WP (a pro-noun, indicative of a question) versus what-WDT (a determiner, not indicative).?
Bigrams.?
Parse tree tags.
All Treebank tags from the syn-tactic parse tree.
Captures syntactic questionclues such as the phrase tags SQ (question sen-tence) and WHNP (wh-word noun phrase).The SVM classifier performs at 97.8%.
For com-parison, an SVM with only unigram features per-forms at 97.2%.
While the unigram model does agood job classifying sentences, suggesting this isa very easy task, the improved feature set reducesthe number of errors by 20%.6 Identifying QuestionsQUESTION recognition is the task of identifyingwhen a conjunction like and joins two independentquestions into a single sentence:?
[What causes the condition]QUESTION[and whattreatment is available?]QUESTION?
[What is this disease]QUESTION[and what stepscan I take to protect my daughter?
]QUESTIONWe consider the identification of separate QUES-TIONs within a single sentence to be a differ-ent task from COORDINATION recognition, whichfinds phrases whose conjuncts can be treated in-dependently.
Linguistically, these tasks are quitesimilar, but the distinction lies in whether theright-conjunct syntactically depends on anythingto its left.
For instance:?
I would like to learn [more about this conditionand what the prognosis is for a baby born withit]COORDINATION.Here, the right-conjunct starts with a questionstem (what), but is not a complete, grammaticalquestion on its own.
Alternatively, this could bere-formed into two separate QUESTIONs:?
[I would like to learn more about thiscondition,]QUESTION[and what is the prognosisis for a baby born with it.
]QUESTIONWe make this distinction because the QUESTIONrecognition task requires one fewer step since theboundaries extend to the entire sentence, prevent-ing error propagation from an input module.
Fur-ther, the features that differentiate our QUESTIONand COORDINATION annotations are different.The two-step process for recognizing QUES-TIONs includes: (1) a high-recall candidate gener-ator, and (2) an SVM to eliminate candidates thatare not separate QUESTIONs.
The candidates forQUESTION recognition are simply all the ways asentence can be split by the conjunctions and, or,as well as, and the forward slash (?/?).
In our data,this candidate generation process has a recall of98.6, as a few examples were missed where candi-dates were not separated by one of the above con-junctions.To filter candidates, we use an SVM with threefeatures types:?
The conjunction separating the QUESTIONs.?
Unigrams in the left-conjunct.
Identifies whenthe left-conjunct is not a QUESTION, or when aquestion is part of a COORDINATION.?
The right-conjunct?s parse tree tag.
Recog-nizes when the right-conjunct is an independentclause that may safely be split.33P R F1QUESTION split recognitionBaseline 24.7 82.4 38.0SVM 67.7 64.7 66.2Overall QUESTION recognitionBaseline 87.3 92.8 90.0SVM 97.7 97.4 97.5Table 2: QUESTION recognition results.For evaluation, we measure both the F1scorefor correct candidates, and the overall F1for allQUESTION annotations (i.e., all QUESTION sen-tences).
We also evaluate a baseline method thatutilizes the parse tree to recognize separate QUES-TIONs by splitting sentences where a conjunctionseparates independent clauses.
The results areshown in Table 2.
The baseline method has goodrecall for recognizing where a sentence should besplit into multiple QUESTIONs, but it lacks preci-sion.
This is largely because it is unable to differ-entiate clausal COORDINATIONs such as the aboveexample, as well as when the left-conjunct is notactually a separate question.
For instance:?
Our grandson was diagnosed recently with thisdisease and I am wondering if you could sendme information on it.The SVM-based method can overcome this prob-lem by looking at the words in the left-conjunct.Both methods, however, fail to recognize whentwo independent question clauses are asking thesame question but providing alternative answers:?
Will this condition be with him throughout hislife, or is it possible that it will clear up?While there are methods for handling this issuefor COORDINATION recognition, addressed be-low, recognizing non-splittable QUESTIONs re-quires far deeper semantic understanding whichwe leave to future work.7 Identifying CoordinationsCOORDINATION recognition is the task of identi-fying when a conjunction joins phrases within aQUESTION that can in be separate questions:?
How can I learn more about [treatments andclinical trials]COORDINATION??
Are [muscle twitching, muscle cramps, andmuscle pain]COORDINATIONeffects of having sil-icosis?Unlike QUESTION recognition, the boundaries ofa COORDINATION need to be determined as wellas whether the conjuncts can semantically be splitinto separate questions.
We thus use a three-stepprocess for recognizing COORDINATIONs: (1) ahigh-recall candidate generator, (2) an SVM torank all the candidates for a given conjunction, and(3) an SVM to filter out top-ranked candidates.Candidate generation begins with the identifica-tion of valid conjunctions within a QUESTION an-notation.
We use the same four conjunctions as inQUESTION recognition: and, or, as well as, andthe forward slash.
For each of these, all possi-ble left and right boundaries are generated, so ina QUESTION with 4 tokens on either side of theconjunction, there would be 16 candidates.
Addi-tionally, two adjectives separated by a comma andimmediately followed by a noun are considered acandidate (e.g., ?a [safe, permanent]COORDINATIONtreatment?).
In our data, this candidate generationprocess has a recall of 98.9, as a few instances ex-ist in which a conjunction is not used, such as:?
I am looking for any information you haveabout heavy metal toxicity, [treatment,outcomes]EXEMPLIFICATION+COORDINATION.To rank candidates, we use an SVM with thefollowing feature types:?
If the left-conjunct is congruent with the high-est node in the syntactic parse tree whose right-most leaf is also the right-most token in the left-conjunct.
Essentially, this is equivalent to say-ing whether or not the syntactic parser agreeswith the left-conjunct?s boundary.?
The equivalent heuristic for the right-conjunct.?
If a noun is in both, just the left conjunct, justthe right conjunct, or neither conjunct.?
The Levenshtein distance between the POS tagsequences for the left- and right-conjuncts.The first two features encode the information arule-based method would use if it relied entirelyon the syntactic parse tree.
The remaining featureshelp the classifier overcome cases where the parsermay be wrong.At training time, all candidates for a given con-junction are generated and only the candidate thatmatches the gold COORDINATION is considereda positive example.
Additionally, we annotatedthe boundaries for negative COORDINATIONs (i.e.,syntactic coordinations that do not fit our annota-tion standard).
There were 203 such instances inthe GARD data.
These are considered gold CO-ORDINATIONs for boundary ranking only.To filter the top-ranked candidates, we use anSVM with several feature types:34E/R P R F1BaselineE 28.1 36.5 31.8R 62.9 75.8 68.7Rank + FilterE 38.2 34.8 36.4R 78.5 69.0 73.5Table 3: COORDINATION recognition results.E = exact match; R = relaxed match.?
The conjunction.?
Unigrams in the left-conjunct.?
POS of the first word in both conjuncts.
CO-ORDINATIONs often have the same first POS inboth conjuncts.?
The word immediately before the candidate.E.g., between is a good negative indicator.?
Unigrams in the question but not the candidate.?
If the candidate takes up almost the entire ques-tion (all but 3 tokens).
Typically, COORDINA-TIONs are much smaller than the full question.?
If more than one conjunction is in the candidate.?
If a word in the left-conjunct has an antonymin the right conjunct.
Antonyms are recognizedvia WordNet (Fellbaum, 1998).At training time, the positive examples are drawnfrom the annotated COORDINATIONs, while thenegative examples are drawn from the 203 non-gold annotations mentioned above.In addition to evaluating this method, weevaluate a baseline method that relies entirelyon the syntactic parse to identify COORDINA-TION boundaries without filtering.
The resultsare shown in Table 3.
The rank-and-filter ap-proach shows significant gains over the rule-basedmethod in precision and F1.
As can be seen inthe difference between exact and relaxed match-ing, most of the loss for both the baseline and MLmethods come in boundary detection.
Most meth-ods overly rely upon the syntactic parser, whichperforms poorly both on questions and coordina-tions.
The ML method, though, is sometimes ableto overcome this problem.8 Identifying ExemplificationsEXEMPLIFICATION recognition is the task of iden-tifying when a phrase provides an optional, exem-plifying example with a more specific type of in-formation than that asked by the rest of the ques-tion.
For instance, the following contains both anEXEMPLIFICATION and a COORDINATION:?
Is there anything out there that can helphim [such as [medications or alternativetherapies]COORDINATION]EXEMPLIFICATION?We could consider this to denote 3 questions:?
Is there anything out there that can help him??
Is there anything out there that can help himsuch as medications??
Is there anything out there that can help himsuch as alternative therapies?In the latter two questions, we consider the phrasesuch as to now denote a mandatory constraint onthe answer to each question, whereas in the origi-nal question it would be considered optional.EXEMPLIFICATION recognition is similar toCOORDINATION recognition, and its three-stepprocess is thus similar as well: (1) a high-recallcandidate generator, (2) an SVM to rank all thecandidates for a given trigger phrase, and (3) a setof rules to filter out top-ranked candidates.Candidate generation begins with the identifica-tion of valid trigger words and phrases.
These in-clude: especially, including, particularly, specifi-cally, and such as.
For each of these, all possibleright boundaries are generated, thus EXEMPLIFI-CATIONs have far fewer candidates than COORDI-NATIONs.
Additionally, all phrases within paren-theses are added as EXEMPLIFICATIONs.
In ourdata, this candidate generation process has a recallof 98.1, missing instances without a trigger (seethe example also missed by COORDINATION can-didate generation in Section 7).To rank candidates, we use an SVM with thefollowing feature types:?
If the right-conjunct is the highest parse nodeas defined in the COORDINATION boundary fea-ture.?
If a dependency relation crosses from the right-conjunct to any word outside the candidate.?
POS of the word after the candidate.As with COORDINATIONs, we annotated bound-aries for negative EXEMPLIFICATIONs matchingthe trigger words and used them as positive exam-ples for boundary ranking.To filter the top-ranked candidates, we use twosimple rules.
First, EXEMPLIFICATIONs withinparentheses are filtered if they are acronyms oracronym expansions.
Second, cases such as thebelow example are removed by looking at thewords before the candidate:?
I am particularly interested in learning moreabout genetic testing for the syndrome.In addition to evaluating this method, we eval-uate a baseline method that relies entirely on the35E/R P R F1BaselineE 28.9 62.3 39.5R 39.5 84.9 53.9Rank + FilterE 60.8 58.5 59.6R 80.4 77.4 78.8Table 4: EXEMPLIFICATION recognition results.E = exact match; R = relaxed match.syntactic parser to identify EXEMPLIFICATIONboundaries and performs no filtering.
The re-sults are shown in Table 4.
The rank-and-filterapproach shows significant gains over the rule-based method in precision and F1, more than dou-bling precision for both exact and relaxed match-ing.
There is still a drop in performance when go-ing from relaxed to exact matching, again largelydue to the reliance on the syntactic parser.9 Classifying Background InformationBACKGROUND sentences contain contextual in-formation, such as whether or not a patient hasbeen diagnosed with the focal disease or whatsymptoms they are experiencing.
This informa-tion was annotated at the sentence level, partly be-cause of annotation convenience, but also becausephrase boundaries are not always clear for medicalconcepts (Hahn et al., 2012; Forbush et al., 2013).A difficult factor in this task, and especially onthe GARD dataset, is that consumers are not al-ways asking about a disease for themselves.
In-stead, often they ask on behalf of another individ-ual, often a family member.
The BACKGROUNDtypes are thus annotated based on the person ofinterest, who we refer to as the patient (in the lin-guistic sense).
For instance, if a mother has a dis-ease but is asking about her son (e.g., asking aboutthe probability of her son inheriting the disease),that sentence would be a FAMILY HISTORY, asopposed to a DIAGNOSIS sentence.The GARD corpus is annotated with eightBACKGROUND types:?
COMORBIDITY?
DIAGNOSIS?
FAMILY HISTORY?
ISF (informationsearch failure)?
LIFESTYLE?
SYMPTOM?
TEST?
TREATMENTISF sentences indicate previous attempts to findthe requested information have failed, and are agood signal to the QA system to enable more in-depth search strategies.
LIFESTYLE sentences de-scribe the patient?s life habits (e.g., smoking, ex-ercise).
Currently, the automatic identification ofType P R F1# AnnsCOMORBIDITY 0.0 0.0 0.0 23DIAGNOSIS 80.8 80.3 80.5 690FAMILY HISTORY 67.4 38.4 48.9 151ISF 75.0 65.9 70.1 41LIFESTYLE 0.0 0.0 0.0 13SYMPTOM 76.6 48.1 59.1 320TEST 37.5 4.9 8.7 61TREATMENT 87.3 35.0 50.0 137Overall: Micro-F1: 67.3 Macro-F1: 39.7Table 5: BACKGROUND results.BACKGROUND types has not been a major focusof our effort as no handling exists for it within ourQA system.
We report a baseline method and re-sults here to provide some insight into the diffi-culty of the task.BACKGROUND types are a multi-labeling prob-lem, so we use eight binary classifiers, one foreach type.
Each classifier utilizes only unigramand bigram features.
The results for the mod-els are shown in Table 5.
COMORBIDITY andLIFESTYLE are too rare in the data (23 and 13instances, respectively) for the classifier to iden-tify.
DIAGNOSIS questions are identified fairlywell because this is the most common type (690instances) and because of the constrained vocabu-lary for expressing a diagnosis.
The performanceof the rest of the types is largely proportional tothe number of instances in the data, though ISFperforms quite well given only 41 instances.10 ConclusionWe have presented a method for decomposingconsumer health questions by recognizing six an-notation types.
Some of these types are generalenough to use in open-domain question decom-position (BACKGROUND, IGNORE, QUESTION,COORDINATION, EXEMPLIFICATION), while oth-ers are targeted specifically at consumer healthquestions (FOCUS and the BACKGROUND sub-types).
We demonstrate that ML methods canimprove upon heuristic methods relying on thesyntactic parse tree, though parse errors are of-ten difficult to overcome.
Since significant im-provements in performance would likely requiremajor advances in open-domain syntactic parsing,we instead envision further integration of the keytasks in consumer health question analysis: (1) in-tegration of co-reference and implicit argument in-formation, (2) improved identification of BACK-GROUND types, and (3) identification of discourserelations within questions to further leverage ques-tion decomposition.36AcknowledgementsThis work was supported by the intramural re-search program at the U.S. National Library ofMedicine, National Institutes of Health.
We wouldadditionally like to thank Stephanie M. Morri-son and Janine Lewis for their help accessing theGARD data.ReferencesUlrich Andersen, Anna Braasch, Lina Henriksen,Csaba Huszka, Anders Johannsen, Lars Kayser,Bente Maegaard, Ole Norgaard, Stefan Schulz, andJ?urgen Wedekind.
2012.
Creation and use of Lan-guage Resources in a Question-Answering eHealthSystem.
In Proceedings of the Eighth InternationalConference on Language Resources and Evaluation,pages 2536?2542.Brian L. Cairns, Rodney D. Nielsen, James J. Masanz,James H. Martin, Martha S. Palmer, Wayne H. Ward,and Guergana K. Savova.
2011.
The MiPACQ Clin-ical Question Answering System.
In Proceedings ofthe AMIA Annual Symposium, pages 171?180.YongGang Cao, Feifan Liu, Pippa Simpson, LamontAntieau, Andrew Bennett, James J. Cimino, JohnEly, and Hong Yu.
2011.
AskHERMES: An on-line question answering system for complex clini-cal questions.
Journal of Biomedical Informatics,44:277?288.Dina Demner-Fushman and Swapna Abhyankar.
2012.Syntactic-Semantic Frames for Clinical CohortIdentification Queries.
In Data Integration in theLife Sciences, volume 7348 of Lecture Notes inComputer Science, pages 100?112.Dina Demner-Fushman and Jimmy Lin.
2007.
An-swering Clinical Questions with Knowledge-Basedand Statistical Techniques.
Computational Linguis-tics, 33(1).Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A Library for Large Linear Classification.
Journalof Machine Learning Research, 9:1871?1874.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Tyler B. Forbush, Adi V. Gundlapalli, Miland N.Palmer, Shuying Shen, Brett R. South, Guy Divita,Marjorie Carter, Andrew Redd, Jorie M. Butler, andMatthew Samore.
2013.
?Sitting on Pins and Nee-dles?
: Characterization of Symptom Descriptions inClinical Notes.
In AMIA Summit on Clinical Re-search Informatics, pages 67?71.Udo Hahn, Elena Beisswanger, Ekaterina Buyko, ErikFaessler, Jenny Traum?uller, Susann Schr?oder, andKerstin Hornbostel.
2012.
Iterative Refinementand Quality Checking of Annotation Guidelines ?How to Deal Effectively with Semantically SloppyNamed Entity Types, such as Pathological Phenom-ena.
In Proceedings of the Eighth InternationalConference on Language Resources and Evaluation,pages 3881?3885.Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl.2006.
Answer Complex Questions with RandomWalk Models.
In Proceedings of the 29th AnnualACM SIGIR Conference on Research and Develop-ment in Information Retrieval, pages 220?227.Sven Hartrumpf.
2008.
Semantic Decompositionfor Question Answering.
In Proceedings on the18th European Conference on Artificial Intelligence,pages 313?317.Dierdre Hogan.
2007.
Coordinate Noun Phrase Dis-ambiguation in a Generative Parsing Model.
In Pro-ceedings of the 45th Annual Meeting of the Associa-tion for Computational Linguistics, pages 680?687.John Judge, Aoife Cahill, and Josef van Genabith.2006.
QuestionBank: Creating a Corpus of Parse-Annotated Questions.
In Proceedings of the 21st In-ternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 497?504.Yarden Katz and Bernardo C. Grau.
2005.
Repre-senting Qualitative Spatial Information in OWL-DL.Proceedings of OWL: Experiences and Directions.Halil Kilicoglu, Marcelo Fiszman, and Dina Demner-Fushman.
2013.
Interpreting Consumer HealthQuestions: The Role of Anaphora and Ellipsis.
InProceedings of the 2013 BioNLP Workshop, pages54?62.Finley Lacatusu, Andrew Hickl, and Sanda Harabagiu.2006.
Impact of Question Decomposition on theQuality of Answer Summaries.
In Proceedings ofLREC, pages 1147?1152.Donald A.B.
Lindberg, Betsy L. Humphreys, andAlexa T. McCray.
1993.
The Unified Medical Lan-guage System.
Methods of Information in Medicine,32(4):281?291.Alexa T McCray, Anita Burgun, and Olivier Boden-reider.
2001.
Aggregating UMLS Semantic Typesfor Reducing Conceptual Complexity.
In Studiesin Health Technology and Informatics (MEDINFO),volume 84(1), pages 216?220.Kirk Roberts, Kate Masterton, Marcelo Fiszman, HalilKilicoglu, and Dina Demner-Fushman.
2014.
An-notating Question Decomposition on Complex Med-ical Questions.
In Proceedings of LREC.Hong Yu and YongGang Cao.
2008.
AutomaticallyExtracting Information Needs from Ad Hoc Clini-cal Questions.
In Proceedings of the AMIA AnnualSymposium.37
