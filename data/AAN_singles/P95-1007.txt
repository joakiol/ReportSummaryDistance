Corpus Statistics Meet the Noun Compound:Some Empirical ResultsMark  LauerMicrosoft Inst i tute65 Epping Road,North Ryde NSW 2113Austra l iat -mark l?microso f t ,  comAbst rac tA variety of statistical methods for nouncompound anMysis are implemented andcompared.
The results support wo mainconclusions.
First, the use of conceptualassociation ot only enables a broad cove-rage, but also improves the accuracy.
Se-cond, an analysis model based on depen-dency grammar issubstantially more accu-rate than one based on deepest constitu-ents, even though the latter is more preva-lent in the literature.1 Background1.1 Compound NounsIf parsing is taken to be the first step in taming thenatural language understanding task, then broad co-verage NLP remains a jungle inhabited by wild be-asts.
For instance, parsing noun compounds appearsto require detailed world knowledge that is unavaila-ble outside a limited domain (Sparek Jones, 1983).Yet, far from being an obscure, endangered species,the noun compound is flourishing in modern lan-guage.
It has already made five appearances in thisparagraph and at least one diachronic study showsa veritable population explosion (Leonard, 1984).While substantial work on noun compounds exists inboth linguistics (e.g.
Levi, 1978; Ryder, 1994) andcomputational linguistics (Finin, 1980; McDonald,1982; Isabelle, 1984), techniques suitable for broadcoverage parsing remain unavailable.
This paper ex-plores the application of corpus tatistics (Charniak,1993) to noun compound parsing (other computa-tional problems are addressed in Arens el al, 1987;Vanderwende, 1993 and Sproat, 1994).The task is illustrated in example 1:Example 1(a) \[womanN \[aidN workerN\]\](b) \[\[hydrogenN ionN\] exchangeN\]The parses assigned to these two compounds dif-fer, even though the sequence of parts of speech areidentical.
The problem is analogous to the prepo-sitional phrase attachment task explored in Hindleand Rooth (1993).
The approach they propose in-volves computing lexical associations from a corpusand using these to select he correct parse.
A similararchitecture may be applied to noun compounds.In the experiments below the accuracy of such asystem is measured.
Comparisons are made acrossfive dimensions:?
Each of two analysis models are applied: adja-cency and dependency.?
Each of a range of training schemes are em-ployed.?
Results are computed with and without uningfactors uggested in the literature.?
Each of two parameterisations areused: asso-ciations between words and associations bet-ween concepts.?
Results are collected with and without machinetagging of the corpus.1.2 Training SchemesWhile Hindle and Rooth (1993) use a partial par-ser to acquire training data, such machinery appearsunnecessary for noun compounds.
Brent (1993) hasproposed the use of simple word patterns for the ac-quisition of verb subcategorisation nformation.
Ananalogous approach to compounds i used in Lauer(1994) and constitutes one scheme valuated below.While such patterns produce false training examp-les, the resulting noise often only introduces minordistortions.A more liberal alternative is the use of a co-occurrence window.
Yarowsky (1992) uses a fixed100 word window to collect information used forsense disambiguation.
Similarly, Smadja (1993) usesa six content word window to extract significant col-locations.
A range of windowed training schemes areemployed below.
Importantly, the use of a windowprovides a natural means of trading off the amountof data against its quality.
When data sparseness un-dermines the system accuracy, a wider window may47admit a sufficient volume of extra accurate data tooutweigh the additional noise.1.3 Noun Compound Analys isThere are at least four existing corpus-based al-gorithms proposed for syntactically analysing nouncompounds.
Only two of these have been subjectedto evaluation, and in each case, no comparison toany of the other three was performed.
In fact all au-thors appear unaware of the other three proposals.I will therefore briefly describe these algorithms.Three of the algorithms use what I will call theADJACENCY MODEL, an analysis procedure that goesback to Marcus (1980, p253).
Therein, the proce-dure is stated in terms of calls to an oracle whichcan determine if a noun compound is acceptable.
Itis reproduced here for reference:Given three nouns nl, n2 and nz:?
If either \[nl n2\] or In2 n~\] is not semanticallyacceptable then build the alternative structure;?
otherwise, if \[n2 n3\] is semantically preferableto \[nl n2\] then build In2 nz\];?
otherwise, build \[nl n2\].Only more recently has it been suggested that cor-pus statistics might provide the oracle, and this ideais the basis of the three algorithms which use theadjacency model.
The simplest of these is repor-ted in Pustejovsky et al(1993).
Given a three wordcompound, a search is conducted elsewhere in thecorpus for each of the two possible subcomponents.Whichever is found is then chosen as the more closelybracketed pair.
For example, when backup compilerdisk is encountered, the analysis will be:Example  2(a) \[backupN \[compilerN diskN\]\]when compiler disk appears elsewhere(b) \[\[backupN compilerN\] diskN\]when backup compiler appears elsewhereSince this is proposed merely as a rough heuristic,it is not stated what the outcome is to be if neitheror both subcomponents appear.
Nor is there anyevaluation of the algorithm.The proposal of Liberman and Sproat (1992) ismore sophisticated and allows for the frequency ofthe words in the compound.
Their proposal invol-ves comparing the mutual information between thetwo pairs of adjacent words and bracketing togetherwhichever pair exhibits the highest.
Again, there isno evaluation of the method other than a demon-stration that four examples work correctly.The third proposal based on the adjacency modelappears in Resnik (1993) and is rather more complexagain.
The SELECTIONAL ASSOCIATION between apredicate and a word is defined based on the con-tribution of the word to the conditional entropy ofthe predicate.
The association between each pairof words in the compound is then computed by ta-king the maximum selectional association from allpossible ways of regarding the pair as predicate andargument.
Whilst this association metric is compli-cated, the decision procedure still follows the out-line devised by Marcus (1980) above.
Resnik (1993)used unambiguous noun compounds from the parsedWall Stree~ Journal (WSJ) corpus to estimate theassociation ~alues and analysed a test set of around160 compounds.
After some tuning, the accuracywas about 73%, as compared with a baseline of 64%achieved by always bracketing the first two nounstogether.The fourth algorithm, first described in Lauer(1994), differs in one striking manner from the otherthree.
It uses what I will call the DEPENDENCY MO-DEL.
This model utilises the following procedurewhen given three nouns at, n2 and n3:?
Determine how acceptable the structures \[nl n2\]and \[nl n3\] are;?
if the latter is more acceptable, build \[n2 nz\]first;?
otherwise, build In1 rig.\] first.Figure 1 shows a graphical comparison of the twoanalysis models.In Lauer (1994), the degree of acceptability isagain provided by statistical measures over a cor-pus.
The metric used is a mutual information-likemeasure based on probabilities of modification rela-tionships.
This is derived from the idea that parsetrees capture the structure of semantic relationshipswithin a noun compound.
1The dependency model attempts to choose a parsewhich makes the resulting relationships as accepta-ble as possible.
For example, when backup compilerdisk is encountered, the analysis will be:Example  3(a) \[backupN \[compilerN diskN\]\]when backup disk is more acceptable(b) \[\[backupN compilerN\] diskN\]when backup compiler is more acceptableI claim that the dependency model makes moreintuitive sense for the following reason.
Considerthe compound calcium ion exchange, which is typi-cally left-branching (that is, the first two words arebracketed together).
There does not seem to be anyreason why calcium ion should be any more frequentthan ion exchange.
Both are plausible compoundsand regardless of the bracketing, ions are the objectof an exchange.
Instead, the correct parse dependson whether calcium characterises the ions or media-tes the exchange.Another significant difference between the modelsis the predictions they make about the proportion1Lauer and Dras (1994) give a formal constructionmotivating the algorithm given in Lauer (1994).48LN2tRAdjacencyN3tPreferleft-branchingigL is moreacceptablethan RLN1 N2 N3t tRDependencyFigure 1: Two analysis models and the associations they compareof left and right-branching compounds.
Lauer andDras (1994) show that under a dependency mo-del, left-branching compounds hould occur twiceas often as right-branching compounds (that is two-thirds of the time).
In the test set used here andin that of Resnik (1993), the proportion of left-branching compounds is 67% and 64% respectively.In contrast, the adjacency model appears to predicta proportion of 50%.The dependency model has also been proposed byKobayasi et al(1994) for analysing Japanese nouncompounds, apparently independently.
Using a cor-pus to acquire associations, they bracket sequencesof Kanji with lengths four to six (roughly equiva-lent to two or three words).
A simple calculationshows that using their own preprocessing hueristicsto guess a bracketing provides a higher accuracy ontheir test set than their statistical model does.
Thisrenders their experiment inconclusive.2 Method2.1 Ext ract ing  a Test  SetA test set of syntactically ambiguous noun com-pounds was extracted from our 8 million word Gro-lier's encyclopedia corpus in the following way.
2 Be-cause the corpus is not tagged or parsed, a some-what conservative strategy of looking for unambi-guous sequences of nouns was used.
To distinguishnouns from other words, the University of Penn-sylvania morphological nalyser (described in Karpet al 1992) was used to generate the set of wordsthat can only be used as nouns (I shall henceforthcall this set AZ).
All consecutive sequences of thesewords were extracted, and the three word sequencesused to form the test set.
For reasons made clearbelow, only sequences consisting entirely of wordsfrom Roget's thesaurus were retained, giving a totalof 308 test triples.
3These triples were manually analysed using ascontext he entire article in which they appeared.
In2We would like to thank Grolier's for permission touse this material for research purposes.3The 1911 version of Roget's used is available on-lineand is in the public domain.some cases, the sequence was not a noun compound(nouns can appear adjacent o one another acrossvarious constituent boundaries) and was marked asan error.
Other compounds exhibited what Hin-die and Rooth (1993) have termed SEMANTIC INDE-TERMINACY where the two possible bracketings can-not be distinguished in the context.
The remainingcompounds were assigned either a left-branching orright-branching analysis.
Table 1 shows the numberof each kind and an example of each.Accuracy figures in all the results reported be-low were computed using only those 244 compoundswhich received a parse.2.2 Conceptua l  Assoc ia t ionOne problem with applying lexical association tonoun compounds is the enormous number of para-meters required, one for every possible pair of nouns.Not only does this require a vast amount of memoryspace, it creates a severe data sparseness problemsince we require at least some data about each pa-rameter.
Resnik and Hearst (1993) coined the termCONCEPTUAL ASSOCIATION to refer to associationvalues computed between groups of words.
By assu-ming that all words within a group behave similarly,the parameter space can be built in terms of thegroups rather than in terms of the words.In this study, conceptual association is used withgroups consisting of all categories from the 1911 ver-sion of Roget's thesaurus.
4 Given two thesaurus ca-tegories tl and t~, there is a parameter which re-presents the degree of acceptability of the structure\[nine\] where nl is a noun appearing in tl and n2appears in t2.
By the assumption that words withina group behave similarly, this is constant given thetwo categories.
Following Lauer and Dras (1994) wecan formally write this parameter as Pr(t l  ~ t2)where the event tl ~ t2 denotes the modification ofa noun in t2 by a noun in tl.2.3 T ra in ingTo ensure that the test set is disjoint from the trai-ning data, all occurrences of the test noun com-pounds have been removed from the training corpus.4It contains 1043 categories.49TypeErrorIndeterminateLeft-branchingRight-branchingNumber293516381Proportion9%11%53%26%ExampleIn monsoon regions rainfall does not .
.
.Most advanced aircraft have precision navigation systems.. .
.escaped punishment by the Allied war crimes tribunals.Ronald Reagan, who won two landslide election victories, .
.
.Table 1: Test set distributionTwo types of training scheme are explored in thisstudy, both unsupervised.
The first employs a pat-tern that follows Pustejovsky (1993) in counting theoccurrences of subcomponents.
A training instanceis any sequence of four words WlW2W3W 4 wherewl, w4 ~ .h/and w2, w3 E A/'.
Let county(n1, n2) bethe number of times a sequence wln ln2w4 occurs inthe training corpus with wl, w4 ~ At'.The second type uses a window to collect traininginstances by observing how often a pair of nouns co-occur within some fixed number of words.
In thisstudy, a variety of window sizes are used.
For n > 2,let countn(nl, n2) be the number of times a sequencen lw l .
.
.w ins  occurs in the training corpus wherei < n - 2.
Note that windowed counts are asym-metric.
In the case of a window two words wide,this yields the mutual information metric proposedby Liberman and Sproat (1992).Using each of these different raining schemes toarrive at appropriate counts it is then possible toestimate the parameters.
Since these are expressedin terms of categories rather than words, it is ne-cessary to combine the counts of words to arrive atestimates.
In all cases the estimates used are:1 count(wl, w2) Vr(tl --, t2) = ~ambig(wl) ambig(w2)wl f i t lw2qt2count(wl, w2)where ~ =  ~,~j?
ambig(wl)ambig(w~)w2Et2Here ambig(w) is the number of categories inwhich w appears.
It has the effect of dividing theevidence from a training instance across all possi-ble categories for the words.
The normaliser ensuresthat all parameters for a head noun sum to unity.2.4 Ana lys ing  the  Test  SetGiven the high level descriptions in section 1.3 itremains only to formalise the decision process usedto analyse a noun compound.
Each test compoundpresents a set of possible analyses and the goal is tochoose which analysis is most likely.
For three wordcompounds it suffices to compute the ratio of twoprobabilities, that of a left-branching analysis andthat of a right-branching one.
If this ratio is greaterthan unity, then the left-branching analysis is cho-sen.
When it is less than unity, a right-branchinganalysis is chosen.
~ If the ratio is exactly unity, theanalyser guesses left-branching, although this is fai-rly rare for conceptual association as shown by theexperimental results below.For the adjacency model, when the given com-pound is WlW2W3, we can estimate this ratio as:Ra4i : ~-~t,~cats(..~ Pr(t l  ---* t2) (1)~-'~t,ecats(-b Pr(t2 ---* t3)For the dependency model, the ratio is:Rdep = ~-~,,ec~ts(~,) Pr(Q ---* t~) Pr(t~ ---* ta) (2))-~t,ec~ts(~) Pr(~l ---* t3) Pr(t2 ~ ta)In both cases, we sum over all possible categoriesfor the words in the compound.
Because the de-pendency model equations have two factors, theyare affected more severely by data sparseness.
Ifthe probability estimate for Pr(t2 ~ t3) is zero forall possible categories t2 and t3 then both the nu-merator and the denominator will be zero.
Thiswill conceal any preference given by the parame-ters involving Q.
In such cases, we observe that thetest instance itself provides the information that theevent t2 --~ t3 can occur and we recalculate the ra-tio using Pr(t2 ---* t3) = k for all possible categoriest2,t a where k is any non-zero constant.
However, nocorrection is made to the probability estimates forPr(t l  --~ t2) and Pr(Q --* t3) for unseen cases, thusputting the dependency model on an equal footingwith the adjacency model above.The equations presented above for the dependencymodel differ from those developed in Lauer and Dras(1994) in one way.
There, an additional weightingfactor (of 2.0) is used to favour a left-branching ana-lysis.
This arises because their construction is ba-sed on the dependency model which predicts thatleft-branching analyses hould occur twice as often.Also, the work reported in Lauer and Dras (1994)uses simplistic estimates of the probability of a wordgiven its thesaurus category.
The equations aboveassume these probabilities are uniformly constant.Section 3.2 below shows the result of making thesetwo additions to the method.sit either probability estimate is zero, the other ana-lysis is chosen.
If both are zero the analysis is made asif the ratio were exactly unity.50Accuracy(%)8580757065605550I I I I I I I IDependency ModelAdjacency Model oGuess Left .
.
.
.Pattern 2 3 4 5 10 50 100Training scheme (integers denote window widths)Figure 2: Accuracy of dependency and adjacency model for various training schemes3 Resu l ts3.1 Dependency  meets  AdjacencyEight different raining schemes have been used toestimate the parameters and each set of estimatesused to analyse the test set under both the adjacencyand the dependency model.
The schemes used are:?
the pattern given in section 2.3; and?
windowed training schemes with window widthsof 2, 3, 4, 5, 10, 50 and 100 words.The accuracy on the test set for all these expe-riments is shown in figure 2.
As can be seen, thedependency model is more accurate than the adja-cency model.
This is true across the whole spec-trum of training schemes.
The proportion of casesin which the procedure was forced to guess, eitherbecause no data supported either analysis or becauseboth were equally supported, is quite low.
For thepattern and two-word window training schemes, theguess rate is less than 4% for both models.
In thethree-word window training scheme, the guess ratesare less than 1%.
For all larger windows, neithermodel is ever forced to guess.In the case of the pattern training scheme, thedifference between 68.9% for adjacency and 77.5%for dependency is statistically significant at the 5%level (p = 0.0316), demonstrating the superiority ofthe dependency model, at least for the compoundswithin Grolier's encyclopedia.In no case do any of the windowed training sche-mes outperform the pattern scheme.
It seems thatadditional instances admitted by the windowed sche-mes are too noisy to make an improvement.Initial results from applying these methods to theEMA corpus have been obtained by Wilco ter Stal(1995), and support he conclusion that the depen-dency model is superior to the adjacency model.3.2 TuningLauer and Dras (1994) suggest two improvements tothe method used above.
These are:?
a factor favouring left-branching which arisesfrom the formal dependency construction; and?
factors allowing for naive estimates of the varia-tion in the probability of categories.While these changes are motivated by the depen-dency model, I have also applied them to the adja-cency model for comparison.
To implement them,equations 1 and 2 must be modified to incorporate1 in each term of the sum and the a factor ofentire ratio must be multiplied by two.
Five trai-ning schemes have been applied with these extensi-ons.
The accuracy results are shown in figure 3.
Forcomparison, the untuned accuracy figures are shownwith dotted lines.
A marked improvement is obser-ved for the adjacency model, while the dependencymodel is only slightly improved.3.3 Lexical Associat ionTo determine the difference made by conceptual s-sociation, the pattern training scheme has been re-trained using lexical counts for both the dependencyand adjacency model, but only for the words inthe test set.
If the same system were to be app-lied across all of Af (a total of 90,000 nouns), thenaround 8.1 billion parameters would be required.Left-branching is favoured by a factor of two as de-scribed in the previous ection, but no estimates forthe category probabilities are used (these being mea-ningless for the lexical association method).Accuracy and guess rates are shown in figure 4.Conceptual association outperforms lexical associa-tion, presumably because of its ability to generalise.3.4 Using a TaggerOne problem with the training methods given in sec-tion 2.3 is the restriction of training data to nounsin .Af.
Many nouns, especially common ones, haveverbal or adiectival usages that preclude them frombeing in .Af.
Yet when they occur as nouns, theystill provide useful training information that the cur-rent system ignores.
To test whether using tagged51Accuracy(%)8580757065605550I I l ITuned Dependency-.
.
.
.
.
.
.
.
.
.
.
.
.
.
Tuned Adjacency o?
.
.
.
.
.
.
.
.I I IPattern 2 3 5 10Training scheme (integers denote window widths)Figure 3: Accuracy of tuned dependency and adjacency model for various training schemesA c c u r a c y(%)8580757065605550I !Conceptual ?Lexical O _Dependency Adjacency30252OGuess Rate (%) 1510I IConceptual ?Lexical \[\]I IDependency AdjacencyFigure 4: Accuracy and Guess Rates of Lexical and Conceptual Association5285 i lAccuracy(%)80757065605550Tagged DependencyTagged Adjacency oI IPattern 3Training scheme (integers denote window widths)Figure 5: Accuracy using a tagged corpus for various training schemesdata would make a difference, the freely availableBrill tagger (Brill, 1993) was applied to the corpus.Since no manually tagged training data is availablefor our corpus, the tagger's default rules were used(these rules were produced by Brill by training onthe Brown corpus).
This results in rather poor tag-ging accuracy, so it is quite possible that a manuallytagged corpus would produce better esults.Three training schemes have been used and thetuned analysis procedures applied to the test set.Figure 5 shows the resulting accuracy, with accuracyvalues from figure 3 displayed with dotted lines.
Ifanything, admitting additional training data basedon the tagger introduces more noise, reducing theaccuracy.
However, for the pattern training schemean improvement was made to the dependency model,producing the highest overall accuracy of 81%.4 Conc lus ionThe experiments above demonstrate a number of im-portant points.
The most general of these is thateven quite crude corpus statistics can provide infor-mation about he syntax of compound nouns.
At thevery least, this information can be applied in broadcoverage parsing to assist in the control of search.
Ihave also shown that with a corpus of moderate sizeit is possible to get reasonable r sults without usinga tagger or parser by employing a customised trai-ning pattern.
While using windowed co-occurrencedid not help here, it is possible that under more datasparse conditions better performance ould be achie-ved by this method.The significance of the use of conceptual associa-tion deserves ome mention.
I have argued that wit-hout it a broad coverage system would be impossible.This is in contrast o previous work on conceptualassociation where it resulted in little improvementon a task which could already be performed.
In thisstudy, not only has the technique proved its worthby supporting enerality, but through generalisationof training information it outperforms the equivalentlexical association approach given the same informa-tion.Amongst all the comparisons performed in theseexperiments one stands out as exhibiting the grea-test contrast.
In all experiments he dependencymodel provides a substantial dvantage over the ad-jacency model, even though the latter is more pre-valent in proposals within the literature.
This re-sult is in accordance with the informal reasoning i-ven in section 1.3.
The model also has the furthercommendation that it predicts correctly the obser-ved proportion of left-branching compounds foundin two independently extracted test sets.In all, the most accurate technique achieved an ac-curacy of 81% as compared to the 67% achieved byguessing left-branching.
Given the high frequency ofoccurrence of noun compounds in many texts, thissuggests tha; the use of these techniques in proba-bilistic parsers will result in higher performance inbroad coverage natural anguage processing.5 AcknowledgementsThis work has received valuable input from peopletoo numerous to mention.
The most significant con-tributions have been made by Richard Buckland,Robert Dale and Mark Dras.
I am also indebtedto Vance Gledhill, Mike Johnson, Philip Resnik, Ri-chard Sproat, Wilco ter Stal, Lucy Vanderwende andWayne Wobcke.
Financial support is gratefully ack-53nowledged from the Microsoft Institute and the Au-stralian Government.Re ferencesArens, Y., Granacki, J. and Parker, A.
1987.
Phra-sal Analysis of Long Noun Sequences.
In Procee-dings of the 25th Annual Meeting of the Associa-tion for Computational Linguistics, Stanford, CA.pp59-64.Brent, Michael.
1993.
From Grammar to Lexi-con: Unsupervised Learning of Lexical Syntax.
InComputational Linguistics, Vol 19(2), Special Is-sue on Using Large Corpora II, pp243-62.Brill, Eric.
1993.
A Corpus-based Approach to Lan-guage Learning.
PhD Thesis, University of Penn-sylvania, Philadelphia, PA..Charniak, Eugene.
1993.
Statistical Language Lear-ning.
MIT Press, Cambridge, MA.Finin, Tim.
1980.
The Semantic Interpretation ofCompound Nominals.
PhD Thesis, Co-ordinatedScience Laboratory, University of Illinois, Urbana,IL.Hindle, D. and Rooth, M. 1993.
Structural Am-biguity and Lexical Relations.
In ComputationalLinguistics Vol.
19(1), Special Issue on UsingLarge Corpora I, ppl03-20.Isabelle, Pierre.
1984.
Another Look At NominalCompounds.
In Proceedings of COLING-84, Stan-ford, CA.
pp509-16.Karp, D., Schabes, Y., Zaidel, M. and Egedi, D.1992.
A Freely Available Wide Coverage Mor-phological Analyzer for English.
In Proceedings ofCOLING-92, Nantes, France, pp950-4.Kobayasi, Y., Tokunaga, T. and Tanaka, H. 1994.Analysis of Japanese Compound Nouns usingCollocational Information.
In Proceedings ofCOLING-94, Kyoto, Japan, pp865-9.Lauer, Mark.
1994.
Conceptual Association forCompound Noun Analysis.
In Proceedings of the32nd Annual Meeting of the Association for Com-putational Linguistics, Student Session, Las Cru-ces, NM.
pp337-9.Lauer, M. and Dras, M. 1994.
A Probabilistic Mo-del of Compound Nouns.
In Proceedings of the 7thAustralian Joint Conference on Artificial Intelli-gence, Armidale, NSW, Australia.
World Scienti-fic Press, pp474-81.Leonard, Rosemary.
1984.
The Interpretation ofEnglish Noun Sequences on the Computer.
North-Holland, Amsterdam.Levi, Judith.
1978.
The Syntax and Semantics ofComplex Nominals.
Academic Press, New York.Liberman, M. and Sproat, R. 1992.
The Stress andStructure of Modified Noun Phrases in English.In Sag, I. and Szabolcsi, A., editors, Lexical Mat-ters CSLI Lecture Notes No.
24.
University ofChicago Press, ppl31-81.Marcus, Mit~.hell.
1980.
A Theory of Syntactic Re-cognition for Natural Language.
MIT Press, Cam-bridge, MA.McDonald, David B.
1982.
Understanding NounCompounds.
PhD Thesis, Carnegie-Mellon Uni-versity, Pittsburgh, PA.Pustejovsky, J., Bergler, S. and Anick, P. 1993.
Le-xical Semantic Techniques for Corpus Analysis.
InComputational Linguistics Vol 19(2), Special Is-sue on Using Large Corpora II, pp331-58.Resnik, Philip.
1993.
Selection and Informa-tion: A Class.Based Approach to Lexical Relati-onships.
PhD dissertation, University of Pennsyl-vania, Philadelphia, PA.Resnik, P. and Hearst, M. 1993.
Structural Ambi-guity and Conceptual Relations.
In Proceedings ofthe Workshop on Very Large Corpora: Academicand Industrial Perspectives, June 22, Ohio StateUniversity, pp58-64.Ryder, Mary Ellen.
1994.
Ordered Chaos: The In-terpretation of English Noun-Noun Compounds.University of California Press Publications in Lin-guistics, Vol 123.Smadja, Frank.
1993.
Retrieving Collocations fromText: Xtract.
In Computational Linguistics, Vol19(1), Special Issue on Using Large Corpora I,pp143-177.Sparck Jones, Karen.
1983.
Compound NounInterpretation Problems.
In Fallside, F. andWoods, W.A., editors, Computer Speech Proces-sing.
Prentice-Hall, NJ.
pp363-81.Sproat, Richard.
1994.
English noun-phrase accentprediction for text-to-speech.
In Computer Speechand Language, Vol 8, pp79-94.Vanderwende, Lucy.
1993.
SENS: The System forEvaluating Noun Sequences.
In Jensen, K., Hei-dorn, G. and Richardson, S., editors, Natural Lan-guage Processing: The PLNLP Approach.
KluwerAcademic, pp161-73.ter Stal, Wilco.
1995.
Syntactic Disambiguation fNominal Compounds Using Lexical and Concep-tual Association.
Memorandum UT-KBS-95-002,University of Twente, Enschede, Netherlands.Yarowsky, David.
1992.
Word-Sense Disambigua-tion Using Statistical Models of Roget's Catego-ries Trained on Large Corpora.
In Proceedings ofCOLING-92, Nantes, France, pp454-60.54
