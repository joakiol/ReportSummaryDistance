Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 593?602,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLearning Soft Linear Constraints with Application to Citation FieldExtractionSam Anzaroot Alexandre Passos David Belanger Andrew McCallumDepartment of Computer ScienceUniversity of Massachusetts, Amherst{anzaroot, apassos, belanger, mccallum}@cs.umass.eduAbstractAccurately segmenting a citation stringinto fields for authors, titles, etc.
is a chal-lenging task because the output typicallyobeys various global constraints.
Previouswork has shown that modeling soft con-straints, where the model is encouraged,but not require to obey the constraints, cansubstantially improve segmentation per-formance.
On the other hand, for impos-ing hard constraints, dual decompositionis a popular technique for efficient predic-tion given existing algorithms for uncon-strained inference.
We extend dual decom-position to perform prediction subject tosoft constraints.
Moreover, with a tech-nique for performing inference given softconstraints, it is easy to automatically gen-erate large families of constraints and learntheir costs with a simple convex optimiza-tion problem during training.
This allowsus to obtain substantial gains in accuracyon a new, challenging citation extractiondataset.1 IntroductionCitation field extraction, an instance of informa-tion extraction, is the task of segmenting and la-beling research paper citation strings into theirconstituent parts, including authors, editors, year,journal, volume, conference venue, etc.
This taskis important because citation data is often pro-vided only in plain text; however, having an ac-curate structured database of bibliographic infor-mation is necessary for many scientometric tasks,such as mapping scientific sub-communities, dis-covering research trends, and analyzing networksof researchers.
Automated citation field extrac-tion needs further research because it has not yetreached a level of accuracy at which it can be prac-tically deployed in real-world systems.Hidden Markov models and linear-chain condi-tional random fields (CRFs) have previously beenapplied to citation extraction (Hetzner, 2008; Pengand McCallum, 2004) .
These models support ef-ficient dynamic-programming inference, but onlymodel local dependencies in the output label se-quence.
However citations have strong global reg-ularities not captured by these models.
For exam-ple many book citations contain both an authorsection and an editor section, but none have twodisjoint author sections.
Since linear-chain mod-els are unable to capture more than Markov depen-dencies, the models sometimes mislabel the editoras a second author.
If we could enforce the globalconstraint that there should be only one authorsection, accuracy could be improved.One framework for adding such global con-straints into tractable models is constrained infer-ence, in which at inference time the original modelis augmented with restrictions on the outputs suchthat they obey certain global regularities.
Whenhard constraints can be encoded as linear equa-tions on the output variables, and the underlyingmodel?s inference task can be posed as linear opti-mization, one can formulate this constrained infer-ence problem as an integer linear program (ILP)(Roth and Yih, 2004).
Alternatively, one can em-ploy dual decomposition (Rush et al, 2010).
Dualdecompositions?s advantage over ILP is is that itcan leverage existing inference algorithms for theoriginal model as a black box.
Such a modularalgorithm is easy to implement, and works quitewell in practice, providing certificates of optimal-ity for most examples.The above two approaches have previously beenapplied to impose hard constraints on a model?soutput.
On the other hand, recent work has demon-strated improvements in citation field extractionby imposing soft constraints (Chang et al, 2012).Here, the model is not required obey the globalconstraints, but merely pays a penalty for their vi-5934 .ref-marker[ J.firstD.middleMonk ,last person]authors[ Cardinal Functions on Boolean Algebra , ]title[ Lectures in Mathematics, ETH Zurich ,seriesBirkhause Verlag ,publisherBasel , Boston , Berlin ,address1990 .year date]venueFigure 1: Example labeled citationolation.This paper introduces a novel method for im-posing soft constraints via dual decomposition.We also propose a method for learning the penal-ties the prediction problem incurs for violatingthese soft constraints.
Because our learningmethod drives many penalties to zero, it allowspractitioners to perform ?constraint selection,?
inwhich a large number of automatically-generatedcandidate global constraints can be considered andautomatically culled to a smaller set of useful con-straints, which can be run quickly at test time.Using our new method, we are able to incor-porate not only all the soft global constraints ofChang et al (2012), but also far more com-plex data-driven constraints, while also provid-ing stronger optimality certificates than their beamsearch technique.
On a new, more broadly rep-resentative, and challenging citation field extrac-tion data set, we show that our methods achieve a17.9% reduction in error versus a linear-chain con-ditional random field.
Furthermore, we demon-strate that our inference technique can use andbenefit from the constraints of Chang et al (2012),but that including our data-driven constraints ontop of these is beneficial.
While this paper fo-cusses on an application to citation field extrac-tion, the novel methods introduced here wouldeasily generalize to many problems with globaloutput regularities.2 Background2.1 Structured Linear ModelsThe overall modeling technique we employ is toadd soft constraints to a simple model for whichwe have an existing efficient prediction algorithm.For this underlying model, we employ a chain-structured conditional random field (CRF), sinceCRFs have been shown to perform better thanother simple unconstrained models like hiddenmarkov models for citation extraction (Peng andMcCallum, 2004).
We produce a prediction byperforming MAP inference (Koller and Friedman,2009).The MAP inference task in a CRF be can ex-pressed as an optimization problem with a lin-ear objective (Sontag, 2010; Sontag et al, 2011).Here, we define a binary indicator variable foreach candidate setting of each factor in the graph-ical model.
Each of these indicator variables isassociated with the score that the factor takes onwhen it has the indictor variable?s correspondingvalue.
Since the log probability of some y in theCRF is proportional to sum of the scores of all thefactors, we can concatenate the indicator variablesas a vector y and the scores as a vectorw and writethe MAP problem asmax.
?w, y?s.t.
y ?
U ,(1)where the set U represents the set of valid config-urations of the indicator variables.
Here, the con-straints are that all neighboring factors agree onthe components of y in their overlap.Structured Linear Models are the general fam-ily of models where prediction requires solving aproblem of the form (1), and they do not alwayscorrespond to a probabilistic model.
The algo-rithms we present in later sections for handlingsoft global constraints and for learning the penal-ties of these constraints can be applied to gen-eral structured linear models, not just CRFs, pro-vided we have an available algorithm for perform-ing MAP inference.2.2 Dual Decomposition for GlobalConstraintsIn order to perform prediction subject to variousglobal constraints, we may need to augment theproblem (1) with additional constraints.
Dual De-composition is a popular method for performingMAP inference in this scenario, since it lever-ages known algorithms for MAP in the base prob-lem where these extra constraints have not beenadded (Komodakis et al, 2007; Sontag et al,2011; Rush and Collins, 2012).
In this case, theMAP problem can be formulated as a structuredlinear model similar to equation (1), for which wehave a MAP algorithm, but where we have im-posed some additional constraints Ay ?
b thatno longer allow us to use the algorithm.
In other594Algorithm 1 DD: projected subgradient for dualdecomposition with hard constraints1: while has not converged do2: y(t)= argmaxy?U?w +AT?, y?3: ?
(t)= ?0??[?(t?1)?
?
(t)(Ay ?
b)]words, we consider the problemmax.
?w, y?s.t.
y ?
UAy ?
b,(2)for an arbitrary matrix A and vector b.
We canwrite the Lagrangian of this problem asL(y, ?)
= ?w, y?+ ?T(Ay ?
b).
(3)Regrouping terms and maximizing over the primalvariables, we have the dual problemmin.?D(?)
= maxy?U?w +AT?, y??
?Tb.
(4)For any ?, we can evaluate the dual objectiveD(?
), since the maximization in (4) is of the sameform as the original problem (1), and we assumedwe had a method for performing MAP in this.
Fur-thermore, a subgradient ofD(?)
isAy?
?b, for any?which maximizes this inner optimization prob-lem.
Therefore, we can minimize D(?)
with theprojected subgradient method (Boyd and Vanden-berghe, 2004), and the optimal y can be obtainedwhen evaluating D(??).
Note that the subgradientof D(?)
is the amount by which each constraint isviolated by ?
when maximizing over y.Algorithm 1 depicts the basic projected subgra-dient descent algorithm for dual decomposition.The projection operator ?
consists of truncatingall negative coordinates of ?
to 0.
This is neces-sary because ?
is a vector of dual variables for in-equality constraints.
The algorithm has convergedwhen each constraint is either satisfied by y(t)withequality or its corresponding component of ?
is 0,due to complimentary slackness (Boyd and Van-denberghe, 2004).3 Soft Constraints in DualDecompositionWe now introduce an extension of Algorithm 1to handle soft constraints.
In our formulation, asoft-constrained model imposes a penalty for eachunsatisfied constraint, proportional to the amountby which it is violated.
Therefore, our derivationparallels how soft-margin SVMs are derived fromhard-margin SVMs by introducing auxiliary slackvariables (Cortes and Vapnik, 1995).
Note thatwhen performing MAP subject to soft constraints,optimal solutions might not satisfy some con-straints, since doing so would reduce the model?sscore by too much.Consider the optimization problems of theform:max.
?w, y?
?
?c, z?s.t.
y ?
UAy ?
b ?
z?z ?
0,(5)For positive ci, it is clear that an optimal ziwillbe equal to the degree to which aTiy ?
biis vio-lated.
Therefore, we pay a cost citimes the degreeto which the ith constraint is violated, which mir-rors how slack variables are used to represent thehinge loss for SVMs.
Note that cihas to be pos-itive, otherwise this linear program is unboundedand an optimal value can be obtained by setting zito infinity.Using a similar construction as in section 2.2 wewrite the Lagrangian as:(6)L(y, z, ?, ?)
= ?w, y?
?
?c, z?+ ?T(Ay ?
b?
z) + ?T(?z).The optimality constraints with respect to z tell usthat ?c?
??
?
= 0, hence ?
= ?c?
?.
Substi-tuting, we haveL(y, ?)
= ?w, y?+ ?T(Ay ?
b), (7)except the constraint that ?
= ?c?
?
implies thatfor ?
to be positive ?
?
c.Since this Lagrangian has the same form asequation (3), we can also derive a dual problem,which is the same as in equation (4), with the ad-ditional constraint that each ?ican not be biggerthan its cost ci.
In other words, the dual problemcan not penalize the violation of a constraint morethan the soft constraint model in the primal wouldpenalize you if you violated it.This optimization problem can still be solvedwith projected subgradient descent and is depictedin Algorithm 2.
The only modifications to Al-gorithm 1 are replacing the coordinate-wise pro-jection ?0?
?with ?0??
?cand how we check forconvergence.
Now, we check for the KKT con-ditions of (5), where for every constraint i, eitherthe constraint is satisfied with equality, ?i= 0, or?i= ci.595Algorithm 2 Soft-DD: projected subgradient fordual decomposition with soft constraints1: while has not converged do2: y(t)= argmaxy?U?w +AT?, y?3: ?
(t)= ?0???c[?(t?1)?
?
(t)(Ay ?
b)]Therefore, implementing soft-constrained dualdecomposition is as easy as implementing hard-constrained dual decomposition, and the per-iteration complexity is the same.
We encouragefurther applications of soft-constraint dual decom-position to existing and new NLP problems.3.1 Learning PenaltiesOne consideration when using soft v.s.
hard con-straints is that soft constraints present a new train-ing problem, since we need to choose the vectorc, the penalties for violating the constraints.
Animportant property of problem (5) in the previoussection is that it corresponds to a structured lin-ear model over y and z.
Therefore, we can applyknown training algorithms for estimating the pa-rameters of structured linear models to choose c.All we need to employ the structured perceptronalgorithm (Collins, 2002) or the structured SVMalgorithm (Tsochantaridis et al, 2004) is a black-box procedure for performing MAP inference inthe structured linear model given an arbitrary costvector.
Fortunately, the MAP problem for (5) canbe solved using Soft-DD, in Algorithm 2.Each penalty cihas to be non-negative; other-wise, the optimization problem in equation (5) isill-defined.
This can be ensured by simple mod-ifications of the perceptron and subgradient de-scent optimization of the structured SVM objec-tive simply by truncating c coordinate-wise to benon-negative at every learning iteration.Intuitively, the perceptron update increases thepenalty for a constraint if it is satisfied in theground truth and not in an inferred prediction, anddecreases the penalty if the constraint is satisfiedin the prediction and not the ground truth.
Sincewe truncate penalties at 0, this suggests that wewill learn a penalty of 0 for constraints in three cat-egories: constraints that do not hold in the groundtruth, constraints that hold in the ground truth butare satisfied in practice by performing inferencein the base CRF model, and constraints that aresatisfied in practice as a side-effect of imposingnon-zero penalties on some other constraints .
Asimilar analysis holds for the structured SVM ap-proach.Therefore, we can view learning the values ofthe penalties not just as parameter tuning, but as ameans to perform ?constraint selection,?
since con-straints that have a penalty of 0 can be ignored.This property allows us to consider large familiesof constraints, from which the useful ones are au-tomatically identified.We found it beneficial, though it is not theoreti-cally necessary, to learn the constraints on a held-out development set, separately from the othermodel parameters, as during training most con-straints are satisfied due to overfitting, which leadsto an underestimation of the relevant penalties.4 Citation Extraction DataWe consider the UMass citation dataset, first intro-duced in Anzaroot and McCallum (2013).
It hasover 1800 citation from many academic fields, ex-tracted from the arXiv.
This dataset contains bothcoarse-grained and fine-grained labels; for exam-ple it contains labels for the segment of all authors,segments for each individual author, and for thefirst and last name of each author.
There are 660citations in the development set and 367 citationin the test set.The labels in the UMass dataset are a con-catenation of labels from a hierarchically-definedschema.
For example, a first name of an author istagged as: authors/person/first.
In addition, indi-vidual tokens are labeled using a BIO label schemafor each level in the hierarchy.
BIO is a commonlyused labeling schema for information extractiontasks.
BIO labeling allows individual labels ontokens to label segmentation information as wellas labels for the segments.
In this schema, labelsthat begin segments are prepended with a B, la-bels that continue a segment are prepended withan I, and tokens that don?t have a labeling in thisschema are given an O label.
For example, in a hi-erarchical BIO label schema the first token in thefirst name for the second author may be labeled as:I-authors/B-person/B-first.An example labeled citation in this dataset canbe viewed in figure 1.5 Global Constraints for CitationExtraction5.1 Constraint TemplatesWe now describe the families of global constraintswe consider for citation extraction.
Note these596constraints are all linear, since they depend onlyon the counts of each possible conditional ran-dom field label.
Moreover, since our labels areBIO-encoded, it is possible, by counting B tags,to count how often each citation tag itself appearsin a sentence.
The first two families of constraintsthat we describe are general to any sequence la-beling task while the last is specific to hierarchicallabeling such as available in the UMass dataset.Our sequence output is denoted as y and an ele-ment of this sequence is yk.We denote [[yk= i]] as the function that outputs1 if ykhas a 1 at index i and 0 otherwise.
Here, ykrepresents an output tag of the CRF, so if [[yk= i]]= 1, then we have that ykwas given a label withindex i.5.2 Singleton ConstraintsSingleton constraints ensure that each label canappear at most once in a citation.
These are sameglobal constraints that were used for citation fieldextraction in Chang et al (2012).
We define s(i)to be the number of times the label with index i ispredicted in a citation, formally:s(i) =?yk?y[[yk= i]]The constraint that each label can appear atmost once takes the form:s(i) <= 15.3 Pairwise ConstraintsPairwise constraints are constraints on the countsof two labels in a citation.
We define z1(i, j) to bez1(i, j) =?yk?y[[yk= i]] +?yk?y[[yk= j]]and z2(i, j) to bez2(i, j) =?yk?y[[yk= i]]?
?yk?y[[yk= j]]We consider all constraints of the forms:z(i, j) ?
0, 1, 2, 3 and z(i, j) ?
0, 1, 2, 3.Note that some pairs of these constraints are re-dundant or logically incompatible.
However, weare using them as soft constraints, so these con-straints will not necessarily be satisfied by the out-put of the model, which eliminates concern overenforcing logically impossible outputs.
Further-more, in section 3.1 we described how our proce-dure for learning penalties will drive some penal-ties to 0, which effectively removes them from ourset of constraints we consider.
It can be shown, forexample, that we will never learn non-zero penal-ties for certain pairs of logically incompatible con-straints using the perceptron-style algorithm de-scribed in section 3.1 .5.4 Hierarchical Equality ConstraintsThe labels in the citation dataset are hierarchicallabels.
This means that the labels are the concate-nation of all the levels in the hierarchy.
We cancreate constraints that are dependent on only oneor couple of elements in the hierarchy.We define C(x, i) as the function that returns 1if the output x contains the label i in the hierarchyand 0 otherwise.
We define e(i, j) to bee(i, j) =?yk?y[[C(yk, i)]]?
?yk?y[[C(yk, j)]]Hierarchical equality constraints take the forms:e(i, j) ?
0 (8)e(i, j) ?
0 (9)5.5 Local constraintsWe constrain the output labeling of the chain-structured CRF to be a valid BIO encoding.This both improves performance of the underly-ing model when used without global constraints,as well as ensures the validity of the global con-straints we impose, since they operate only onB labels.
The constraint that the labeling isvalid BIO can be expressed as a collection ofpairwise constraints on adjacent labels in the se-quence.
Rather than enforcing these constraintsusing dual decomposition, they can be enforceddirectly when performing MAP inference in theCRF by modifying the dynamic program of theViterbi algorithm to only allow valid pairs of adja-cent labels.5.6 Constraint PruningWhile the techniques from section 3.1 can easilycope with a large numbers of constraints at train-ing time, this can be computationally costly, spe-cially if one is considering very large constraintfamilies.
This is problematic because the size597Unconstrained[17]ref-marker[ D.firstSivia ,last personJ.firstSkilling ,last person]authors[ Data Analysis : A Bayesian Tutorial,booktitleOxford University Press ,publisher2006year date]venueConstrained[17]ref-marker[ D.firstSivia ,last personJ.firstSkilling ,last person]authorsData Analysis : A Bayesian Tutorial,title[ Oxford University Press ,publisher2006year date]venueUnconstrained[ Sobol?
,lastI.firstM.middle person]authors[ (1990) .year]date[On sensitivity estimation for nonlinear mathe-matical models .
]title[ Matematicheskoe Modelirovanie ,journal2volume(1) :number112?118 .pages( In Russian) .status]venueConstrained[ Sobol?
,lastI.firstM.middle person]authors[ (1990) .year]date[On sensitivity estimation for nonlinear mathe-matical models .
]title[ Matematicheskoe Modelirovanie ,journal2volume(1) :number112?118 .pages( In Russian) .language]venueFigure 2: Two examples where imposing soft global constraints improves field extraction errors.
Soft-DD converged in 1 iteration on the first example, and 7 iterations on the second.
When a reference isciting a book and not a section of the book, the correct labeling of the name of the book is title.
Inthe first example, the baseline CRF incorrectly outputs booktitle, but this is fixed by Soft-DD, whichpenalizes outputs based on the constraint that booktitle should co-occur with an address label.
In thesecond example, the unconstrained CRF output violates the constraint that title and status labels shouldnot co-occur.
The ground truth labeling also violates a constraint that title and language labels shouldnot co-occur.
At convergence of the Soft-DD algorithm, the correct labeling of language is predicted,which is possible because of the use of soft constraints.Constraints F1 score Sparsity # of consBaseline 94.44Only-one 94.62 0% 3Hierarchical 94.55 56.25% 16Pairwise 95.23 43.19% 609All 95.39 32.96% 628All DD 94.60 0% 628Table 1: Set of constraints learned and F1 scores.The last row depicts the result of inference usingall constraints as hard constraints.of some constraint families we consider growsquadratically with the number of candidate labels,and there are about 100 in the UMass dataset.Such a family consists of constraints that the sumof the counts of two different label types has tobe bounded (a useful example is that there can?tbe more than one out of ?phd thesis?
and ?jour-nal?).
Therefore, quickly pruning bad constraintscan save a substantial amount of training time, andcan lead to better generalization.To do so, we calculate a score that estimateshow useful each constraint is expected to be.
Ourscore compares how often the constraint is vio-lated in the ground truth examples versus our pre-dictions.
Here, prediction is done with respect tothe base chain-structured CRF tagger and does notinclude global constraints.
Note that it may makesense to consider a constraint that is sometimes vi-olated in the ground truth, as the penalty learningalgorithm can learn a small penalty for it, whichwill allow it to be violated some of the time.
Ourimportance score is defined as, for each constraintc on labeled set D,imp(c) =?d?D[[maxywTdy]]c?d?D[[yd]]c, (10)where [[y]]cis 1 if the constraint is violated on out-put y and 0 otherwise.
Here, yddenotes the groundtruth labeling and wdis the vector of scores for theCRF tagger.We prune constraints by picking a cutoff valuefor imp(c).
A value of imp(c) above 1 impliesthat the constraint is more violated on the pre-dicted examples than on the ground truth, andhence that we might want to keep it.We also find that the constraints that have thelargest imp values are semantically interesting.6 Related WorkThere are multiple previous examples of augment-ing chain-structured sequence models with termscapturing global relationships by expanding thechain to a more complex graphical model withnon-local dependencies between the outputs.
In-ference in these models can be performed, forexample, with loopy belief propagation (Bunescuand Mooney, 2004; Sutton and McCallum, 2004)or Gibbs sampling (Finkel et al, 2005).
Be-lief propagation is prohibitively expensive in our598model due to the high cardinalities of the out-put variables and of the global factors, which in-volve all output variables simultaneously.
Thereare various methods for exploiting the combi-natorial structure of these factors, but perfor-mance would still have higher complexity than ourmethod.
While Gibbs sampling has been shownto work well tasks such as named entity recogni-tion (Finkel et al, 2005), our previous experimentsshow that it does not work well for citation extrac-tion, where it found only low-quality solutions inpractice because the sampling did not mix well,even on a simple chain-structured CRF.Recently, dual decomposition has become apopular method for solving complex structuredprediction problems in NLP (Koo et al, 2010;Rush et al, 2010; Rush and Collins, 2012; Pauland Eisner, 2012; Chieu and Teow, 2012).
Softconstraints can be implemented inefficiently usinghard constraints and dual decomposition?
by in-troducing copies of output variables and an aux-iliary graphical model, as in Rush et al (2012).However, at every iteration of dual decomposition,MAP must be run in this auxiliary model.
Further-more the copying of variables doubles the num-ber of iterations needed for information to flowbetween output variables, and thus slows conver-gence.
On the other hand, our approach to softconstraints has identical per-iteration complexityas for hard constraints, and is a very easy modifi-cation to existing hard constraint code.Initial work in machine learning for citation ex-traction used Markov models with no global con-straints.
Hidden Markov models (HMMs), wereoriginally employed for automatically extractinginformation from research papers on the CORAdataset (Seymore et al, 1999; Hetzner, 2008).Later, CRFs were shown to perform better onCORA, improving the results from the Hmm?stoken-level F1 of 86.6 to 91.5 with a CRF(Pengand McCallum, 2004).Recent work on globally-constrained inferencein citation extraction used an HMMCCM, which isan HMM with the addition of global features thatare restricted to have positive weights (Chang etal., 2012).
Approximate inference is performedusing beam search.
This method increased theHMM token-level accuracy from 86.69 to 93.92on a test set of 100 citations from the CORAdataset.
The global constraints added into themodel are simply that each label only occursonce per citation.
This approach is limited in itsuse of an HMM as an underlying model, as ithas been shown that CRFs perform significantlybetter, achieving 95.37 token-level accuracy onCORA (Peng and McCallum, 2004).
In our ex-periments, we demonstrate that the specific globalconstraints used by Chang et al (2012) help on theUMass dataset as well.7 Experimental ResultsOur baseline is the one used in Anzaroot andMcCallum (2013), with some labeling errors re-moved.
This is a chain-structured CRF trainedto maximize the conditional likelihood using L-BFGS with L2 regularization.We use the same features as Anzaroot and Mc-Callum (2013), which include word type, capital-ization, binned location in citation, regular expres-sion matches, and matches into lexicons.
In addi-tion, we use a rule-based segmenter that segmentsthe citation string based on punctuation as well asprobable start or end segment words (e.g.
?in?
and?volume?).
We add a binary feature to tokens thatcorrespond to the start of a segment in the outputof this simple segmenter.
This final feature im-proves the F1 score on the cleaned test set from94.0 F1 to 94.44 F1, which we use as a baselinescore.We then use the development set to learn thepenalties for the soft constraints, using the percep-tron algorithm described in section 3.1.
MAP in-ference in the model with soft constraints is per-formed using Soft-DD, shown in Algorithm 2.We instantiate constraints from each template insection 5.1, iterating over all possible labels thatcontain a B prefix at any level in the hierarchy andpruning all constraints with imp(c) < 2.75 cal-culated on the development set.
We asses perfor-mance in terms of field-level F1 score, which isthe harmonic mean of precision and recall for pre-dicted segments.Table 1 shows how each type of constraint fam-ily improved the F1 score on the dataset.
Learningall the constraints jointly provides the largest im-provement in F1 at 95.39.
This improvement in F1over the baseline CRF as well as the improvementin F1 over using only-one constraints was shownto be statistically significant using the Wilcoxonsigned rank test with p-values < 0.05.
In theall-constraints settings, 32.96% of the constraintshave a learned parameter of 0, and therefore only599Stop F1 score Convergence Avg Iterations1 94.44 76.29% 1.02 95.07 83.38% 1.245 95.12 95.91% 1.6110 95.39 99.18% 1.73Table 2: Performance from terminating Soft-DDearly.
Column 1 is the number of iterations weallow each example.
Column 3 is the % of testexamples that converged.
Column 4 is the aver-age number of necessary iterations, a surrogate forthe slowdown over performing unconstrained in-ference.421 constraints are active.
Soft-DD converges,and thus solves the constrained inference prob-lem exactly, for all test set examples after at most41 iterations.
Running Soft-DD to convergencerequires 1.83 iterations on average per example.Since performing inference in the CRF is by farthe most computationally intensive step in the iter-ative algorithm, this means our procedure requiresapproximately twice as much work as running thebaseline CRF on the dataset.
On examples whereunconstrained inference does not satisfy the con-straints, Soft-DD converges after 4.52 iterationson average.
For 11.99% of the examples, theSoft-DD algorithm satisfies constraints that werenot satisfied during unconstrained inference, whilein the remaining 11.72% Soft-DD converges withsome constraints left unsatisfied, which is possiblesince we are imposing them as soft constraints.We could have enforced these constraints ashard constraints rather than soft ones.
This exper-iment is shown in the last row of Table 1, whereF1 only improves to 94.6.
In addition, runningthe DD algorithm with these constraints takes 5.21iterations on average per example, which is 2.8times slower than Soft-DD with learned penalties.In Figure 2, we analyze the performance ofSoft-DD when we don?t necessarily run it to con-vergence, but stop after a fixed number of itera-tions on each test set example.
We find that a largeportion of our gain in accuracy can be obtainedwhen we allow ourselves as few as 2 dual decom-position iterations.
However, this only amounts to1.24 times as much work as running the baselineCRF on the dataset, since the constraints are satis-fied immediately for many examples.In Figure 2 we consider two applications of ourSoft-DD algorithm, and provide analysis in thecaption.We train and evaluate on the UMass dataset in-stead of CORA, because it is significantly larger,has a useful finer-grained labeling schema, and itsannotation is more consistent.
We were able to ob-tain better performance on CORA using our base-line CRF than the HMMCCMresults presentedin Chang et al (2012), which include soft con-straints.
Given this high performance of our basemodel on CORA, we did not apply our Soft-DDalgorithm to the dataset.
Furthermore, since thedataset is so small, learning the penalties for ourlarge collection of constraints is difficult, and testset results are unreliable.
Rather than compare ourwork to Chang et al (2012) via results on CORA,we apply their constraints on the UMass data us-ing Soft-DD and demonstrate accuracy gains, asdiscussed above.7.1 Examples of learned constraintsWe now describe a number of the useful con-straints that receive non-zero learned penaltiesand have high importance scores, defined in Sec-tion 5.6.
The importance score of a constraint pro-vides information about how often it is violatedby the CRF, but holds in the ground truth, and anon-zero penalty implies we enforce it as a softconstraint at test time.The two singleton constraints with highest im-portance score are that there should only be atmost one title segment in a citation and that thereshould be at most one author segment in a cita-tion.
The only one author constraint is particu-larly useful for correctly labeling editor segmentsin cases where unconstrained inference mislabelsthem as author segments.
As can be seen in Table3, editor fields are among the most improved withour new method, largely due to this constraint.The two hierarchical constraints with the high-est importance scores with non-zero learnedpenalties constrain the output such that numberof person segments does not exceed the numberof first segments and vice-versa.
Together, theseconstraints penalize outputs in which the numberof person segments do not equal the number offirst segments, i.e., every author should have a firstname.One important pairwise constraint penalizesoutputs in which thesis segments don?t co-occurwith school segments.
School segments label thename of the university that the thesis was submit-ted to.
The application of this constraint increasesthe performance of the model on school segments600Label U C +venue/series 35.29 66.67 31.37venue/editor/person/first 66.67 94.74 28.07venue/school 40.00 66.67 26.67venue/editor/person/last 75.00 94.74 19.74venue/editor 77.78 90.00 12.22venue/editor/person/middle 81.82 91.67 9.85Table 3: Labels with highest improvement in F1.U is in unconstrained inference.
C is the results ofconstrained inference.
+ is the improvement in F1.dramatically, as can be seen in table 3.An interesting form of pairwise constraints pe-nalize outputs in which some labels do not co-occur with other labels.
Some examples of con-straints in this form enforce that journal segmentsshould co-occur with pages segments and thatbooktitle segments should co-occur with addresssegments.
An example of the latter constraint be-ing employed during inference is the first examplein Figure 2.
Here, the constrained inference pe-nalizes output which contains a booktitle segmentbut no address segment.
This penalization leadsallows the constrained inference to correctly labelthe booktitle segment as a title segment.The above example constraints are almost al-ways satisfied on the ground truth, and would beuseful to enforce as hard constraints.
However,there are a number of learned constraints that areoften violated on the ground truth but are still use-ful as soft constraints.
Take, for example, the con-straint that the number of number segments doesnot exceed the number of booktitle segments, aswell as the constraint that it does not exceed thenumber of journal segments.
These constraintsare moderately violated on ground truth examples,however.
For example, when booktitle segmentsco-occur with number segments but not with jour-nal segments, the second constraint is violated.
Itis still useful to impose these soft constraints, asstrong evidence from the CRF allows us to violatethem, and they can guide the model to good pre-dictions when the CRF is unconfident.8 ConclusionWe introduce a novel modification to the stan-dard projected subgradient dual decomposition al-gorithm for performing MAP inference subject tohard constraints to one for performing MAP in thepresence of soft constraints.
In addition, we offeran easy-to-implement procedure for learning thepenalties on soft constraints.
This method drivesmany penalties to zero, which allows users to auto-matically discover discriminative constraints fromlarge families of candidates.We show via experiments on a recent substantialdataset that using soft constraints, and selectingwhich constraints to use with our penalty-learningprocedure, can lead to significant gains in accu-racy.
We achieve a 17% gain in accuracy overa chain-structured CRF model, while only need-ing to run MAP in the CRF an average of lessthan 2 times per example.
This minor incremen-tal cost over Viterbi, plus the fact that we obtaincertificates of optimality on 100% of our test ex-amples in practice, suggests the usefulness of ouralgorithm for large-scale applications.
We encour-age further use of our Soft-DD procedure for otherstructured prediction problems.AcknowledgmentsThis work was supported in part by the Cen-ter for Intelligent Information Retrieval, in partby DARPA under agreement number FA8750-13-2-0020, in part by NSF grant #CNS-0958392,and in part by IARPA via DoI/NBC contract#D11PC20152.
The U.S. Government is autho-rized to reproduce and distribute reprint for Gov-ernmental purposes notwithstanding any copy-right annotation thereon.
Any opinions, findingsand conclusions or recommendations expressed inthis material are those of the authors and do notnecessarily reflect those of the sponsor.ReferencesSam Anzaroot and Andrew McCallum.
2013.
A newdataset for fine-grained citation field extraction.
InICML Workshop on Peer Reviewing and PublishingModels.Stephen Poythress Boyd and Lieven Vandenberghe.2004.
Convex optimization.
Cambridge universitypress.Razvan Bunescu and Raymond J Mooney.
2004.Collective information extraction with relationalmarkov networks.
In Proceedings of the 42nd An-nual Meeting on Association for Computational Lin-guistics, page 438.
Association for ComputationalLinguistics.M.
Chang, L. Ratinov, and D. Roth.
2012.
Structuredlearning with constrained conditional models.
Ma-chine Learning, 88(3):399?431, 6.Hai Leong Chieu and Loo-Nin Teow.
2012.
Com-bining local and non-local information with dual de-composition for named entity recognition from text.601In Information Fusion (FUSION), 2012 15th Inter-national Conference on, pages 231?238.
IEEE.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing-Volume 10, pages 1?8.Association for Computational Linguistics.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine learning, 20(3):273?297.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 363?370.
Association for Computational Lin-guistics.Erik Hetzner.
2008.
A simple method for citationmetadata extraction using hidden markov models.
InProceedings of the 8th ACM/IEEE-CS joint confer-ence on Digital libraries, pages 280?284.
ACM.Daphne Koller and Nir Friedman.
2009.
Probabilisticgraphical models: principles and techniques.
TheMIT Press.Nikos Komodakis, Nikos Paragios, and Georgios Tzir-itas.
2007.
Mrf optimization via dual decomposi-tion: Message-passing revisited.
In Computer Vi-sion, 2007.
ICCV 2007.
IEEE 11th InternationalConference on, pages 1?8.
IEEE.Terry Koo, Alexander M Rush, Michael Collins,Tommi Jaakkola, and David Sontag.
2010.
Dualdecomposition for parsing with non-projective headautomata.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 1288?1298.
Association for Compu-tational Linguistics.Michael J Paul and Jason Eisner.
2012.
Implicitly in-tersecting weighted automata using dual decompo-sition.
In Proceedings of the 2012 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 232?242.
Association for Computa-tional Linguistics.Fuchun Peng and Andrew McCallum.
2004.
Accu-rate information extraction from research papers us-ing conditional random fields.
In Daniel Marcu Su-san Dumais and Salim Roukos, editors, HLT-NAACL2004: Main Proceedings, pages 329?336, Boston,Massachusetts, USA, May 2 - May 7.
Associationfor Computational Linguistics.Dan Roth and Wen-tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
Defense Technical Information Center.Alexander M. Rush and Michael Collins.
2012.
A tu-torial on dual decomposition and lagrangian relax-ation for inference in natural language processing.J.
Artif.
Intell.
Res.
(JAIR), 45:305?362.Alexander M Rush, David Sontag, Michael Collins,and Tommi Jaakkola.
2010.
On dual decompositionand linear programming relaxations for natural lan-guage processing.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1?11.
Association for Computa-tional Linguistics.Alexander M Rush, Roi Reichart, Michael Collins, andAmir Globerson.
2012.
Improved parsing and postagging using inter-sentence consistency constraints.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 1434?1444.Kristie Seymore, Andrew McCallum, Roni Rosenfeld,et al 1999.
Learning hidden markov model struc-ture for information extraction.
In AAAI-99 Work-shop on Machine Learning for Information Extrac-tion, pages 37?42.David Sontag, Amir Globerson, and Tommi Jaakkola.2011.
Introduction to dual decomposition for in-ference.
In Suvrit Sra, Sebastian Nowozin, andStephen J. Wright, editors, Optimization for Ma-chine Learning.
MIT Press.David Sontag.
2010.
Approximate Inference in Graph-ical Models using LP Relaxations.
Ph.D. thesis,Massachusetts Institute of Technology, Departmentof Electrical Engineering and Computer Science.Charles Sutton and Andrew McCallum.
2004.
Col-lective segmentation and labeling of distant entitiesin information extraction.
Technical report, DTICDocument.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2004.
Support vectormachine learning for interdependent and structuredoutput spaces.
In Proceedings of the twenty-first in-ternational conference on Machine learning, page104.
ACM.602
