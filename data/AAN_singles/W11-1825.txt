Proceedings of BioNLP Shared Task 2011 Workshop, pages 155?163,Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational LinguisticsMSR-NLP Entry in BioNLP Shared Task 2011Chris Quirk, Pallavi Choudhury, Michael Gamon, and Lucy VanderwendeMicrosoft ResearchOne Microsoft WayRedmond, WA 98052 USA{chrisq,pallavic,mgamon,lucyv}@microsoft.comAbstractWe describe the system from the NaturalLanguage Processing group at MicrosoftResearch for the BioNLP 2011 SharedTask.
The task focuses on event extraction,identifying structured and potentiallynested events from unannotated text.
Ourapproach follows a pipeline, firstdecorating text with syntactic information,then identifying the trigger words ofcomplex events, and finally identifying thearguments of those events.
The resultingsystem depends heavily on lexical andsyntactic features.
Therefore, we exploredmethods of maintaining ambiguities andimproving the syntactic representations,making the lexical information less brittlethrough clustering, and of exploring novelfeature combinations and feature reduction.The system ranked 4th in the GENIA taskwith an F-measure of 51.5%, and 3rd in theEPI task with an F-measure of 64.9%.1 IntroductionWe describe a system for extracting complexevents and their arguments as applied to theBioNLP-2011 shared task.
Our goal is to exploregeneral methods for fine-grained informationextraction, to which the data in this shared task isvery well suited.
We developed our system usingonly the data provided for the GENIA task, butthen submitted output for two of the tasks, GENIAand EPI, training models on each datasetseparately, with the goal of exploring how generalthe overall system design is with respect to textdomain and event types.
We used no externalknowledge resources except a text corpus used totrain cluster features.
We further describe severalsystem variations that we explored but which didnot contribute to the final system submitted.
Wenote that the MSR-NLP system consistently isamong those with the highest recall, but needsadditional work to improve precision.2 System DescriptionOur event extraction system is a pipelinedapproach, closely following the structure used bythe best performing system in 2009 (Bj?rne et al,2009).
Given an input sentence along withtokenization information and a set of parses, wefirst attempt to identify the words that triggercomplex events using a multiclass classifier.
Nextwe identify edges between triggers and proteins, orbetween triggers and other triggers.
Finally, givena graph of proteins and triggers, we use a rule-based post-processing component to produceevents in the format of the shared task.2.1 Preprocessing and Linguistic AnalysisWe began with the articles as provided, with anincluded tokenization of the input andidentification of the proteins in the input.
However,we did modify the token text and the part-of-speech tags of the annotated proteins in the input tobe PROT after tagging and parsing, as we foundthat it led to better trigger detection.The next major step in preprocessing was toproduce labeled dependency parses for the input.Note that the dependencies may not form a tree:there may be cycles and some words may not beconnected.
During feature construction, thisparsing graph was used to find paths between155words in the sentence.
Since proteins may consistof multiple words, for paths we picked a singlerepresentative word for each protein to act as itsstarting point and ending point.
Generally this wasthe token inside the protein that is closest to theroot of the dependency parse.
In the case of ties,we picked the rightmost such node.2.1.1 McClosky-Charniak-Stanford parsesThe organizers provide parses from a version ofthe McClosky-Charniak parser, MCCC (McCloskyand Charniak, 2008), which is a two-stageparser/reranker trained on the GENIA corpus.
Inaddition, we used an improved set of parsingmodels that leverage unsupervised data, MCCC-I(McClosky, 2010).
In both cases, the StanfordParser was used to convert constituency trees in thePenn Treebank format into labeled dependencyparses: we used the collapsed dependency format.2.1.2 Dependency posteriorsEffectively maintaining and leveraging theambiguity present in the underlying parser hasimproved task accuracy in some downstream tasks(e.g., Mi et al 2008).
McClosky-Charniak parsesin two passes: the first pass is a generative modelthat produces a set of n-best candidates, and thesecond pass is a discriminative reranker that uses arich set of features including non-localinformation.
We renormalized the outputs fromthis log-linear discriminative model to get aposterior distribution over the 50-best parses.
Thisset of parses preserved some of the syntacticambiguity present in the sentence.The Stanford parser deterministically convertsphrase-structure trees into labeled dependencygraphs (de Marneffe et al, 2006).
We convertedeach constituency tree into a dependency graphseparately and retained the probability computedabove on each graph.One possibility was to run feature extraction oneach of these 50 parses, and weight the resultingfeatures in some manner.
However, this caused asignificant increase in feature count.
Instead, wegathered a posterior distribution over dependencyedges: the posterior probability of a labeleddependency edge was estimated by the sum of theprobability of all parses containing that edge.Gathering all such edges produced a single labeledgraph that retained much of the ambiguity of theinput sentence.
Figure 1 demonstrates this processon a simple example.
We applied a threshold of 0.5and retained all edges above that threshold,although there are many alternative ways to exploitthis structure.Figure 1: Example sentence from the GENIA corpus.
(a) Two of the top 50 constituency parses from the MCCC-Iparser; the first had a total probability mass of 0.43 and the second 0.25 after renormalization.
Nodes that differbetween parses are shaded and outlined.
(b) The dependency posteriors (labels omitted due to space) afterconversion of 50-best parses.
Solid lines indicate edges with posterior > 0.95; edges with posterior < 0.05 wereomitted.
Most of the ambiguity is in the attachment of ?elicited?.156As above, the resulting graph is likely no longera connected tree, though it now may also be cyclicand rather strange in structure.
Most of thedependency features were built on shortest pathsbetween words.
We used the algorithm in Cormenet al (2002, pp.595) to find shortest paths in acyclic graph with non-negative edge weights.
Theshortest path algorithm used in feature finding wassupplied uniform positive edge weights.
We couldalso weight edges by the negative log probabilityto find the shortest, most likely path.2.1.3 ENJUWe also experimented with the ENJU parses(Miyao and Tsujii, 2008) provided by the sharedtask organizers.
The distribution contained theoutput of the ENJU parser in a format consistentwith the Stanford Typed Dependencyrepresentation .2.1.4 Multiple parsersWe know that even the best modern parsers areprone to errors.
Including features from multipleparsers helps mitigate these errors.
When differentparsers agree, they can reinforce certainclassification decisions.
The features that wereextracted from a dependency parse have namesthat include an identifier for the parser thatproduced them.
In this way, the machine learningalgorithm can assign different weights to featuresfrom different parsers.
For finding heads of multi-word entities, we preferred the ENJU parser ifpresent in that experimental condition, then fellback to MCCC parses, and finally MCCC-I.2.1.5 Dependency conversion rulesWe computed our set of dependency features (see2.2.1) from the collapsed, propagated StanfordTyped Dependency representation (seehttp://nlp.stanford.edu/software/dependencies_manual.pdf and de Marneffe et al, 2006), madeavailable by the organizers.
We chose this form ofrepresentation since we are primarily interested incomputing features that hold between contentwords.
Consider, for example, the noun phrase?phosphorylation of TRAF2?.
A dependencyrepresentation would specify head-modifierrelations for the tuples (phosphorylation, of) and(of, TRAF2).
Instead of head-modifier, a typeddependency representation specifies PREP andPPOBJ as the two grammatical relations:PREP(phosphorylation-1, of-2) and PPOBJ(of-2,TRAF2-3).
A collapsed representation has a singletriplet specifying the relation between the contentwords directly, PREP_OF(phosphorylation-1,TRAF2-3); we considered this representation to bethe most informative.We experimented with a representation thatfurther normalized over syntactic variation.
Thesystem submitted for the GENIA subtask does notuse these conversion rules, while the systemsubmitted for the EPI subtask does use these rules.See Table 2 for further details.
While for someapplications it may be useful to distinguishwhether a given relation was expressed in theactive or passive voice, or in a main or a relativeclause, we believe that for this application it isbeneficial to normalize over these types ofsyntactic variation.
Accordingly, we had a set ofsimple renaming conversion rules, followed by arule for expansion; this list was our first effort andcould likely be improved.
We modeled thisnormalized level of representation on the logicalform, described in Jensen (1993), though we wereunable to explore NP-or VP-anaphoraRenaming conversion rules:1.
ABBREV -> APPOS2.
NSUBJPASS -> DOBJ3.
AGENT -> NSUBJ4.
XSUBJ -> NSUBJ5.
PARTMOD(head, modifier where last 3characters are "ing") -> NSUBJ(modifier, head)6.
PARTMOD(head, modifier where last 3characters are "ed") -> DOBJ(modifier, head)Expansion:1.
For APPOS, find all edges that point to the head(gene-20) and duplicate those edges, butreplacing the modifier with the modifier of theAPPOS relation (kinase-26).Thus, in the 2nd sentence in PMC-1310901-01-introduction, ?...
leading to expression of a bcr-ablfusion gene, an aberrant activated tyrosine kinase,....?, there are two existing grammatical relations:PREP_OF(expression-15, gene-20)APPOS(gene-20, kinase-26)to which this rule adds:PREP_OF(expression-15, kinase-26)1572.2 Trigger DetectionWe treated trigger detection as a multi-classclassification problem: each token should beannotated with its trigger type or with NONE if itwas not a trigger.
When using the feature setdetailed below, we found that an SVM(Tsochantaridis et al, 2004) outperformed amaximum entropy model by a fair margin, thoughthe SVM was sensitive to its free parameters.
Alarge value of C, the penalty incurred duringtraining for misclassifying a data point, wasnecessary to achieve good results.2.2.1 Features for Trigger DetectionOur initial feature set for trigger detection wasstrongly influenced by features that weresuccessful in Bj?rne et al, (2009).Token Features.
We included stems of singletokens from the Porter stemmer (Porter, 1980),character bigrams and trigrams, a binary indicatorfeature if the token has upper case letters, anotherindicator for the presence of punctuation, and afinal indicator for the presence of a number.
Wegathered these features for both the current tokenas well as the three immediate neighbors on boththe left and right hand sides.We constructed a gazetteer of possible triggerlemmas in the following manner.
First we used arule-based morphological analyzer (Heidorn, 2000)to identify the lemma of all words in the training,development, and test corpora.
Next, for each wordin the training and development sets, we mapped itto its lemma.
We then computed the number oftimes that each lemma occurred as a trigger foreach type of event (and none).
Lemmas that actedas a trigger more than 50% of the time were addedto the gazetteer.During feature extraction for a given token, wefound the lemma of the token, and then look upthat lemma in the gazetteer.
If found, we includeda binary feature to indicate its trigger type.Frequency Features.
We included as featuresthe number of entities in the sentence, a bag ofwords from the current sentence, and a bag ofentities in the current sentence.Dependency Features.
We used primarily a setof dependency chain features that were helpful inthe past (Bj?rne et al, 2009); these features walkthe Stanford Typed Dependency edges up to adistance of 3.We also found it helpful to have features aboutthe path to the nearest protein, regardless ofdistance.
In cases of multiple shortest paths, wetook only one, exploring the dependency treegenerally in left to right order.
For each potentialtrigger, we looked at the dependency edge labelsleading to that nearest protein.
In addition we had afeature including both the dependency edge labelsand the token text (lowercased) along that path.Finally, we had a feature indicating whether sometoken along that path was also in the triggergazetteer.
The formulation of this set of features isstill not optimal especially for the ?binding?
eventsas the training data will include paths to more thanone protein argument.
Nevertheless, in Table 3,Key Relation Value Key Relation Valuequantities child(left, NNS?JJ) measurable measurable child-1(left, NNS?JJ) quantitiesfound child(after, VBN?NNS) hours hours child-1(after, VBN?NNS) foundfound child(after, VBN?NN) ingestion ingestion child-1(after, VBN?NN) foundFigure 2: A sample PubMed sentence along with its dependency parse, and some key/relation/value triplesextracted from that parse for computation of distributional similarity.
Keys with a similar distribution of valuesunder the same relation are likely semantically related.
Inverse relations are indicated with a superscript -1.Prepositions are handled specially: we add edges labeled with the preposition from its parent to each child(indicated by dotted edges).158we can see that this set of features contributed toimproved precision.Cluster Features.
Lexical and stem featureswere crucial for accuracy, but were unfortunatelysparse and did not generalize well.
To mitigatethis, we incorporated word cluster features.
Inaddition to the lexical item and the stem, we addedanother feature indicating the cluster to which eachword belongs.
To train clusters, we downloaded allthe PubMed abstracts (http://pubmed.gov), parsedthem with a simple dependency parser (areimplementation of McDonald, 2006 trained onthe GENIA corpus), and extracted dependencyrelations to use in clustering: words that occur insimilar contexts should fall into the same cluster.An example sentence and the relations that wereextracted for distributional similarity computationare presented in Figure 2.
We ran a distributionalsimilarity clustering algorithm (Pantel et al, 2009)to group words into clusters.Tfidf features.
This set of features was intendedto capture the salience of a term in the medical and?general?
domain, with the aim of being able todistinguish domain-specific terms from moreambiguous terms.
We calculated the tf.idf score foreach term in the set of all PubMed abstracts anddid the same for each term in Wikipedia.
For eachtoken in the input data, we then produced threefeatures: (i) the tf.idf value of the token in PubMedabstracts, (ii) the tf.idf value of the token inWikipedia, and (iii) the delta between the twovalues.
Feature values were rounded to the closestinteger.
We found, however, that adding thesefeatures did not improve results.2.2.2 Feature combination and reductionWe experimented with feature reduction andfeature combination within the set of featuresdescribed here.
For feature reduction we tried anumber of simple approaches that typically workwell in text classification.
The latter is similar tothe task at hand, in that there is a very large butsparse feature set.
We tried two feature reductionmethods: a simple count cutoff, and selection ofthe top n features in terms of log likelihood ratio(Dunning, 1993) with the target values.
For a countcutoff, we used cutoffs from 3 to 10, but we failedto observe any consistent gains.
Only low cutoffs(3 and occasionally 5) would ever produce anysmall improvements on the development set.
Usinglog likelihood ratio (as determined on the trainingset), we reduced the total number of features tobetween 10,000 and 75,000.
None of theseexperiments improved results, however.
Onepotential reason for this negative result may be thatthere were a lot of features in our set that capturethe same phenomenon in different ways, i.e.
whichcorrelate highly.
By retaining a subset of theoriginal feature set using a count cutoff or loglikelihood ratio we did not reduce this featureoverlap in any way.
Alternative feature reductionmethods such as Principal Component Analysis, onthe other hand, would target the feature overlapdirectly.
For reasons of time we did not experimentwith other feature reduction techniques but webelieve that there may well be a gain still to be had.For our feature combination experiments theidea was to find highly predictive Booleancombinations of features.
For example, while thefeatures a and b may be weak indicators for aparticular trigger, the cases where both a and b arepresent may be a much stronger indicator.
A linearclassifier such as the one we used in ourexperiments by definition is not able to take suchBoolean combinations into account.
Someclassifiers such as SVMs with non-linear kernelsdo consider Boolean feature combinations, but wefound the training times on our data prohibitivewhen using these kernels.
As an alternative, wedecided to pre-identify feature combinations thatare predictive and then add those combinationfeatures to our feature inventory.
In order to pre-identify feature combinations, we trained decisiontree classifiers on the training set, and treated eachpath from the root to a leaf through the decisiontree classifier as a feature combination.
We alsoexperimented with adding all partial paths throughthe tree (as long as they started from the root) inaddition to adding all full paths.
Finally, we triedto increase the diversity of our combinationfeatures by using a ?bagging?
approach, where wetrained a multitude of decision trees on randomsubsets of the data.
Again, unfortunately, we didnot find any consistent improvements.
Twoobservations that held relatively consistently acrossour experiments with combination features anddifferent feature sets were: (i) only adding fullpaths as combination features sometimes helped,while adding partial paths did not, and (ii) bagginghardly ever led to improvements.1592.3 Edge DetectionThis phase of the pipeline was again modeled asmulti-class classification.
There could be an edgeoriginating from any trigger word and ending inany trigger word or protein.
Looking at the set ofall such edges, we trained a classifier to predict thelabel of this edge, or NONE if the edge was notpresent.
Here we found that a maximum entropyclassifier performed somewhat better than an SVM,so we used an in-house implementation of amaximum entropy trainer to produce the models.2.3.1 Features for Edge DetectionAs with trigger detection, our initial feature set foredge detection was strongly influenced by featuresthat were successful in Bj?rne et al (2009).Additionally, we included the same dependencypath features to the nearest protein that we used fortrigger detection, described in 2.2.1.
Further, for aprospective edge between two entities, where theentities are either a trigger and a protein, or atrigger and a second trigger, we added a featurethat indicates (i) if the second entity is in the pathto the nearest protein, (ii) if the head of the secondentity is in the path to the nearest protein, (iii) thetype of the second entity.2.4 Post-processingGiven the set of edges, we used a simpledeterministic procedure to produce a set of events.This step is not substantially different from thatused in prior systems (Bj?rne et al, 2009).2.4.1 Balancing Precision and RecallAs in Bj?rne et al (2009), we found that the triggerdetector had quite low recall.
Presumably this isdue to the severe class imbalance in the trainingdata: less than 5% of the input tokens are triggers.Thus, our classifier had a tendency to overpredictNONE.
We tuned a single free parameter ?
?
??
(the ?recall booster?)
to scale back the scoreassociated with the NONE class before selectingthe optimal class.
The value was tuned for whole-system F-measure; optimal values tended to fall inthe range 0.6 to 0.8, indicating that only a smallshift toward recall led to the best results.Development Set  Test SetEvent Class Count Recall Precision F1 Count Recall Precision F1Gene_expression 749 76.37 81.46 78.83 1002 73.95 73.22 73.58Transcription 158 49.37 73.58 59.09 174 41.95 65.18 51.05Protein_catabolism 23 69.57 80.00 74.42 15 46.67 87.50 60.87Phosphorylation 111 73.87 84.54 78.85 185 87.57 81.41 84.37Localization 67 74.63 75.76 75.19 191 51.31 79.03 62.22=[SVT-TOTAL]= 1108 72.02 80.51 76.03 1567 68.99 74.03 71.54Binding 373 47.99 50.85 49.38 491 42.36 40.47 41.39=[EVT-TOTAL]= 1481 65.97 72.73 69.18 2058 62.63 65.46 64.02Regulation 292 32.53 47.05 38.62 385 24.42 42.92 31.13Positive_Regulation 999 38.74 51.67 44.28 1443 37.98 44.92 41.16Negative_Regulation 471 35.88 54.87 43.39 571 41.51 42.70 42.10=[REG-TOTAL]= 1762 36.95 51.79 43.13 2399 36.64 44.08 40.02ALL-Total 3243 50.20 62.60 55.72 4457 48.64 54.71 51.50Table 1: Approximate span matching/approximate recursive matching on development and test datasets for GENIA Shared Task -1 with our system.TriggerDetectionFeaturesTriggerLoss Recall Prec.
F1B 2.14 48.44 64.08 55.18B + TI 2.14 48.17 62.49 54.40B + TI + C 2.14 50.32 60.90 55.11B + TI + C + PI 2.03 50.20 62.60 55.72B + TI + C + PI+D2.02 49.21 62.75 55.16Table 2: Recall/Precision/F1 on the GENIAdevelopment set using MCCC-I + Enju parse;adding different features for Trigger Detection.B = Base set Features, TI = Trigger inflectforms,1603 ResultsOf the five evaluation tracks in the shared task, weparticipated in two: the GENIA core task, and theEPI (Epigenetics and Post-translationalmodifications) task.
The systems used in each trackwere substantially similar; differences are calledout below.
Rather than building a systemcustomized for a single trigger and event set, ourgoal was to build a more generalizable frameworkfor event detection.3.1 GENIA TaskUsing F-measure performance on the developmentset as our objective function, we trained the finalsystem for the GENIA task with all the featuresdescribed in section 2, but without the conversionrules and without either feature combination orreduction.
Furthermore, we trained the clusterfeatures using the full set of PubMed documents(as of  January 2011).
The results of our finalsubmission are summarized in Table 1.
Overall, wesaw a substantial degradation in F-measure whenmoving from the development set to the test set,though this was in line with past experience fromour and other systems.We compared the results for different parsers inTable 3.
MCCC-I is not better in isolation but doesproduce higher F-measures in combination withother parsers.
Although posteriors were notparticularly helpful on the development set, we ranParserSVT-Total Binding REG-Total All-TotalRecall Prec.
F1 Recall Prec.
F1 Recall Prec.
F1 Recall Prec.
F1MCCC 70.94 82.72 76.38 45.04 55.26 49.63 34.39 51.88 41.37 48.10 64.39 55.07MCCC-I 68.59 82.59 74.94 42.63 58.67 49.38 32.58 52.76 40.28 46.06 65.50 54.07Enju 71.66 82.18 76.56 40.75 51.01 45.31 32.24 49.39 39.01 46.69 62.70 53.52MCCC-I +Posteriors70.49 78.87 74.44 47.72 51.59 49.58 35.64 50.40 41.76 48.94 61.47 54.49MCCC +Enju71.84 82.04 76.60 44.77 53.02 48.55 34.96 53.15 42.18 48.69 64.59 55.52MCCC-I +Enju72.02 80.51 76.03 47.99 50.85 49.38 36.95 51.79 43.13 50.20 62.60 55.72Table 3: Comparison of Recall/Precision/F1 on the GENIA Task-1 development set using variouscombinations of parsers: Enju, MCCC (Mc-Closky Charniak), and MCCC-I (Mc-Closky CharniakImproved self-trained biomedical parsing model) with Stanford collapsed dependencies were used forevaluation.
Results on Simple, Binding and Regulation and all events are shown.Development Set  Test SetEvent Class Count Recall Precision F1 Count Recall Precision F1Hydroxylation 31 25.81 61.54 36.36 69 30.43 84.00 44.68Dehydroxylation 0 100.00 100.00 100.00 0 100.00 100.00 100.00Phosphorylation 32 71.88 85.19 77.97 65 72.31 85.45 78.33Dephosphorylation 1 0.00 0.00 0.00 4 0.00 0.00 0.00Ubiquitination 76 63.16 75.00 68.57 180 67.78 81.88 74.16Deubiquitination 8 0.00 0.00 0.00 10 0.00 0.00 0.00DNA_methylation 132 72.73 72.18 72.45 182 71.43 73.86 72.63DNA_demethylation 9 0.00 0.00 0.00 6 0.00 0.00 0.00Glycosylation 70 61.43 67.19 64.18 169 39.05 69.47 50.00Deglycosylation 7 0.00 0.00 0.00 12 0.00 0.00 0.00Acetylation 65 89.23 75.32 81.69 159 87.42 85.28 86.34Deacetylation 19 68.42 92.86 78.79 24 62.50 93.75 75.00Methylation 65 64.62 75.00 69.42 193 62.18 73.62 67.42Demethylation 7 0.00 0.00 0.00 10 0.00 0.00 0.00Catalysis 60 3.33 15.38 5.48 111 4.50 33.33 7.94====[TOTAL]==== 582 57.22 72.23 63.85 1194 55.70 77.60 64.85Table 4: Approximate span matching/approximate recursive matching on development and test datasets for EPI CORE Task with our system161a system consisting of MCCC-I with posteriors(MCCC-I + Posteriors) on the test set after thefinal results were submitted, and found that it wascompetitive with our submitted system (MCCC-I +ENJU).
We believe that ambiguity preservationhas merit, and hope to explore more of this area inthe future.
Diversity is important: although theENJU parser alone was not the best, combining itwith other parsers led to consistently strong results.Table 2 explores feature ablation: TI appears todegrade performance, but clusters regain that loss.Protein depth information was helpful, butdependency rule conversion was not.
Thereforethe B+TI+C+PI combination was our finalsubmission on GENIA.3.2 EPI TaskWe trained the final system for the Epigeneticstask with all the features described in section 2.Further, we produced the clusters for theEpigenetics task using only the set of GENIAdocuments provided in the shared task.In contrast to GENIA, we found that thedependency rule conversions had a positive impacton development set performance.
Therefore, weincluded them in the final system.
Otherwise thesystem was identical to the GENIA task system.4 DiscussionAfter two rounds of the BioNLP shared task, in2009 and 2011, we wonder whether it might bepossible to establish an upper-bound on recall andprecision.
There is considerable diversity amongthe participating systems, so it would be interestingto consider whether there are some annotations inthe development set that cannot be predicted byany of the participating systems1.
If this is the case,then those triggers and edges would present aninteresting topic for discussion.
This might resulteither in a modification of the annotation protocols,or an opportunity for all systems to learn more.After a certain amount of feature engineering,we found it difficult to achieve furtherimprovements in F1.
Perhaps we need a significantshift in architecture, such as a shift to jointinference (Poon and Vanderwende, 2010).
Oursystem may be limited by the pipeline architecture.1 Our system output for the 2011development set can bedownloaded from http://research.microsoft.com/bionlp/MWEs (multi-word entities) are a challenge.Better multi-word triggers accuracy may improvesystem performance.
Multi-word proteins often ledto incorrect part-of-speech tags and parse trees.Cursory inspection of the Epigenetics taskshows that some domain-specific knowledgewould have been beneficial.
Our system hadsignificant difficulties with the rare inverse eventtypes, e.g.
?demethylation?
(e.g., there are 319examples for ?methylation?
in the combinedtraining/development set, but only 12 examples for?demethylation?).
Each trigger type was treatedindependently, thus we did not share informationbetween an event and its related inverse event type.Furthermore, our system also failed to identifyedges for these rare events.
One approach wouldbe to share parameters between types that differonly in a prefix, e.g., ?de?.
In general, someknowledge about the hierarchy of events may letthe learner generalize among related events.5 Conclusion and Future WorkWe have described a system designed for fine-grained information extraction, which we show tobe general enough to achieve good performanceacross different sets of event types and domains.The only domain-specific characteristic is the pre-annotation of proteins as a special class of entities.We formulated some features based on thisknowledge, for instance the path to the nearestprotein.
This would likely have analogues in otherdomains, given that there is often a special class oftarget items for any Information Extraction task.As the various systems participating in theshared task mature, it will be viable to apply theautomatic annotations in an end-user setting.Given a more specific application, we may haveclearer criteria for balancing the trade-off betweenrecall and precision.
We expect that fully-automated systems coupled with reasoningcomponents will need very high precision, whilesemi-automated systems, designed for informationvisualization or for assistance in curatingknowledge bases, could benefit from high recall.We believe that the data provided for the sharedtasks will support system development in eitherdirection.
As mentioned in our discussion, though,we find that improving recall continues to be amajor challenge.
We seek to better understand thedata annotations provided.162Our immediate plans to improve our systeminclude semi-supervised learning and systemcombination.
We will also continue to explorenew levels of linguistic representation tounderstand where they might provide furtherbenefit.
Finally, we plan to explore models of jointinference to overcome the limitations of pipeliningand deterministic post-processing.AcknowledgmentsWe thank the shared task organizers for providingthis interesting task and many resources, the TurkuBioNLP group for generously providing theirsystem and intermediate data output, and PatrickPantel and the MSR NLP group for their help andsupport.ReferencesJari Bj?rne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala and Tapio Salakoski.
2009.Extracting Complex Biological Events with RichGraph-Based Feature Sets.
In Proceedings of  theWorkshop on BioNLP: Shared Task.Thomas Cormen, Charles Leiserson, and Ronald Rivest.2002.
Introduction to Algorithms.
MIT Press.Ted Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
ComputationalLinguistics, 19(1), pp.
61-74.George E. Heidorn, 2000.
Intelligent WritingAssistance.
In Handbook of Natural LanguageProcessing, ed.
Robert Dale, Hermann Moisl, andHarold Somers.
Marcel Dekker Publishers.Karen Jensen.
1993. PEGASUS: Deriving ArgumentStructures after Syntax.
In Natural LanguageProcessing: the PLNLP approach, ed.
Jensen, K.,Heidorn, G.E., and Richardson, S.D.
KluwerAcademic Publishers.Marie-Catherine de Marneffe, Bill MacCartney andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InLREC 2006.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature forestmodels for probabilistic HPSG parsing.Computational Linguistics 34(1): 35-80.David McClosky and Eugene Charniak.
2008.
Self-Training for Biomedical Parsing.
In Proceedings ofthe Association for Computational Linguistics 2008.David McClosky.
2010.
Any Domain Parsing:Automatic Domain Adaptation for Natural LanguageParsing.
Ph.D. thesis, Department of ComputerScience, Brown University.Ryan McDonald.
2006.
Discriminative training andspanning tree algorithms for dependency parsing.
Ph.D. Thesis.
University of Pennsylvania.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based Translation.
In Proceedings of ACL 2008,Columbus, OH.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu and Vishnu Vyas.
2009.
Web-ScaleDistributional Similarity and Entity Set Expansion.
InProceedings of EMNLP 2009.Hoifung Poon and Lucy Vanderwende.
2010.
Jointinference for knowledge extraction from biomedicalliterature.
In Proceedings of NAACL-HLT 2010.Martin.F.
Porter, 1980, An algorithm for suffixstripping, Program, 14(3):130?137.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Alton.
2004.
Support vectormachine learning for interdependent and structuredoutput spaces.
In ICML 2004.163
