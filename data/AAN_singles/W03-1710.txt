Modeling of Long Distance Context Dependency in ChineseGuoDong ZHOUInstitute for Infocomm Research21 Heng Mui Keng TerraceSingapore, 119613zhougd@i2r.a-star.edu.sgAbstractNgram modeling is simple in languagemodeling and has been widely used in manyapplications.
However, it can only capture theshort distance context dependency within anN-word window where the largest practical Nfor natural language is three.
In the meantime,much of context dependency in naturallanguage occurs beyond a three-word window.In order to incorporate this kind of longdistance context dependency, this paperproposes a new MI-Ngram modeling approach.The MI-Ngram model consists of twocomponents: an ngram model and an MImodel.
The ngram model captures the shortdistance context dependency within an N-wordwindow while the MI model captures the longdistance context dependency between the wordpairs beyond the N-word window by using theconcept of mutual information.
It is found thatMI-Ngram modeling has much betterperformance than ngram modeling.
Evaluationon the XINHUA new corpus of 29 millionwords shows that inclusion of the best1,600,000 word pairs decreases the perplexityof the MI-Trigram model by 20 percentcompared with the trigram model.
In themeanwhile, evaluation on Chinese wordsegmentation shows that about 35 percent oferrors can be corrected by using theMI-Trigram model compared with the trigrammodel.1 IntroductionLanguage modeling is the attempt to characterize,capture and exploit the regularities and constraintsin natural language.
Among various languagemodeling approaches, ngram modeling has beenwidely used in many applications, such as speechrecognition, machine translation (Katz 1987;Jelinek 1989; Gale and Church 1990; Brown et al1992; Yang et al 1996; Bai et al1998; Zhou et al1999; Rosenfeld 2000; Gao et al2002).
Althoughngram modeling is simple in nature and easy to use,it has obvious deficiencies.
For instance, ngrammodeling can only capture the short distancecontext dependency within an N-word windowwhere currently the largest practical N for naturallanguage is three.In the meantime, it is found that there alwaysexist many preferred relationships between words.Two highly associated word pairs are ??/??
(?not only/but also?)
and ?
?
/ ?
?(?doctor/nurse?).
Psychological experiments inMeyer et al (1975) indicated that the human?sreaction to a highly associated word pair wasstronger and faster than that to a poorly associatedword pair.
Such preference information is veryuseful for natural language processing (Church etal.
1990; Hiddle et al 1993; Rosenfeld 1994; Zhouet al1998; Zhou et al1999).
Obviously, thepreference relationships between words can expandfrom a short to long distance.
While we can usetraditional ngram modeling to capture the shortdistance context dependency, the long distancecontext dependency should also be exploitedproperly.The purpose of this paper is to propose a newMI-Ngram modeling approach to capture thecontext dependency over both a short distance and along distance.
Experimentation shows that this newMI-Ngram modeling approach can significantlydecrease the perplexity of the new MI-Ngrammodel compared with traditional ngram model.
Inthe meantime, evaluation on Chinese wordsegmentation shows that this new approach cansignificantly reduce the error rate.This paper is organized as follows.
In section 2,we describe the traditional ngram modelingapproach and discuss its main property.
In section 3,we propose the new MI-Ngram modeling approachto capture context dependency over both a shortdistance and a long distance.
In section 4, wemeasure the MI-Ngram modeling approach andevaluate its application in Chinese wordsegmentation.
Finally we give a summary of thispaper in section 5.And the probability P  can be estimatedby using maximum likelihood estimation (MLE)principle:)|( 1?ii ww)()()|(111???
=iiiii wCwwCwwP             (2.5)Where )(?C  represents the number of times thesequence occurs in the training data.
In practice, dueto the data sparseness problem, some smoothingtechniques, such as linear interpolation (Jelinek1989; Chen and Goodman 1999) and back-offmodeling (Katz 1987), are applied.2 Ngram ModelingLet , where ?s are the wordsthat make up the hypothesis, the probability of theword string P  can be computed by using thechain rule:mm wwwwS ...211 ==)(Siw Obviously, an ngram model assumes that theprobability of the next word w is independent ofword string w  in the history.
The differencebetween bigram, trigram and other ngram models isthe value of N. The parameters of an ngram are thusthe probabilities:ini?1?=?=miii wwPwPSP2111 )|()()(                  (2.1))...|( 11 ?nn wwwP    For all Vwww nw ?,...,,1 .
By taking log function to both sides of equation(2.1), we have the log probability of the wordstring log : )(SP Given mwwwS ...21= , an ngram modelestimates the log probability of the word stringby re-writing equation (2.2): )(SP)|(log)(log)(log1121?=?+=iimiwwPwPSP(2.2)??=?+?
?=?++=mniiniiniiingramwwPwwPwPSP)|(log)|(log)(log)(log1112111(2.6) So, the classical task of statistical languagemodeling becomes how to effectively andefficiently predict the next word, given the previouswords, that is to say, to estimate expressions of theform  .
For convenience, Pis often written as , where , iscalled history.
)|( 11?ii wwP )|(11?ii ww11?= iwh)|( hwP iWhere  is the string length, w  is the -th wordin string .mSi iFrom equation (2.3),  we have: )|()|( 1 111?+??
?
i niiii wwPwwPNgram modeling has been widely used in estimating .
Within an ngram model, theprobability of a word occurring next is estimatedbased on the  previous words.
That is to say,)|( hwP i1?n )()()()(11111111?+??+????
iniiiniiiiwPwwPwPwwP)|()|( 1 111?+??
?
i niiii wwPwwP                 (2.3))()()()()()(11111111iiniiiniiiiiwPwPwwPwPwPwwP?+??+????
For example, in bigram model (n=2), theprobability of a word is assumed to depend only onthe previous word:)()()(log)()()(log 11111111iiniiiniiiiiwPwPwwPwPwPwwP?+??+????
(2.7))|()|( 111 ??
?
iiii wwPwwP            (2.4)Obviously, we can get)1,,()1,,( 1 111 =?= ?
+??
dwwMIdwwMI ii niii   (2.8)where)()()(log)1,,( 111111iiiiiiwPwPwwPdwwMI ???
==)iwisthe mutual information between the word string pairand ,( 11iw ?
)()()(log)1,, 111111iiniiiniiini wPwPwwPdw ?+??+??+?
==),1 iw d(wMI( 1i niw?+?is themutual information between the word string pair.
is the distance of two word stringsin  the word string pair and is equal to 1 when thetwo word strings are adjacent.For a word string pair (  over a distancewhere  and), BAd A B  are word strings, mutualinformation  reflects the degree ofpreference relationship between the two stringsover a distance .
Several properties of mutualinformation are apparent:)( dAMId,, B?
For the same distance ,.d),,(),,( dABMIdBAMI ??
For different distances d  and ,.1)2d,,(),,( 21 dBAMIdBAMI ??
If  and A B  are independent over a distance,  then  .
d 0),,( =dBAMI),,( dBAMI  reflects the change of  theinformation content when the word strings A  andB   are correlated.
That is to say, the higher thevalue of ,  the stronger affinity  and ),d,( BAMI AB  have.
Therefore, we can use mutual informationto measure the preference relationship degreebetween a word string pair.From the view of mutual information, an ngrammodel assumes the mutual informationindependency between ( .
Using analternative view of equivalence, an ngram model isone that partitions the data into equivalence classesbased on the last n-1 words in the history.
),1 ini ww ?As trigram model is most widely used in currentresearch, we will mainly consider the trigram-basedmodel.
By re-writing equation (2.6), the trigrammodel estimates the log probability of the stringas: )(SP?=?
?++=miiiiTrigramwwPwwwPSP312121)|(log)|log()(log)(log(2.9)3 MI-Ngram ModelingGiven history H , we canassume .
Then we have12111 ...
??
== ii wwww1321 ...
??
= iwww2= iwXXwH 1=                      (3.1)and)|()|( 1XwwPHwP ii = .
(3.2)Since)1,,()(log)()()(log)(log)()(log)|(logHwMIwPwPHPHwPwPHPHwPHwPiiiiiii+=+==(3.3)Here we assume),,()1,,()1,,(1 idwwMIdwXMIdwHMIiii=+===(3.4)where  ,  and i .
That is tosay, the mutual information of the next word withthe history is assumed equal to the summation ofthat of the next word with the first word in thehistory and that of the next word with the rest wordstring in the history.11?= iwH 12?= iwX N>We can re-writing equation (3.3) by usingequation (3.4):)|(log HwP i)1,,()(log ii wHMIwP +=),,()1,,()(log 1 iwwMIwXMIwP iii ++=),,()()()(log)(log 1 iwwMIXPwPXwPwP iiii ++=  ?==?+=niiii wwwP2111 )|log()(log)|(log 21nn wwP ++),,()()(log 1 iwwMIXPXwPii +=)1,,( 11 ++ + nwwMI n )|(log 112?+=?+ iimniwwP),,()|(log 1 iwwMIXwP ii +=                       (3.5)L L L Then we have?==?+=niiii wwwP2111 )|log()(log?+=?
?+mniiii wwP112 )|(log(3.6)),,()|(log)|(log11211iwwMIwwPwwPiiiii+= ?
?By applying equation (3.6) repeatedly, we have:),,()|(log)|(log11211iwwMIwwPwwPiiiii+= ???
?+=?==+?+mninikkik kiwwMI1 1)1,,(                        (3.8))|(log 13?= ii wwP)1,,( 2 iwwMI i ?+ ),,( 1 iwwMI i+L L L)|(log 1 1?+?= i nii wwP?
?==+?+nikkik kiwwMI1)1,,(                              (3.7)Obviously, the first item in equation (3.7)contributes to the log probability of ngram within anN-word window while the second item is thesummation of mutual information whichcontributes to the long distance context dependencyof the next word w  with the individual previousword  over the longdistance outside the N-word window.ii ?
),1( NiNjwj >?
?logIn equation (3.8), the first three items are thevalues computed by the trigram model as shown inequation (2.9) and the forth itemcontributes tosummation of the mutual information of the nextword with the words over the long distance outsidethe N-word window.
That is, the new model asshown in equation (3.8) consists of twocomponents: an ngram model and an MI model.Therefore, we call equation (3.8) as an MI-Ngrammodel and equation (3.8) can be re-written as:?
?+=?==+?mninikkik kiwwMI1 1)1,,(?
?+=?==?+?+=mninikkikNgramNgramMIkiwwMISPSP1 1)1,,()(log)((3.9)By using equation (3.7), equation (2.2) can bere-written as:As a special case N=3, the MI-Trigram modelestimate the log probability of the string as follows:)|(log)(log)(log 1121?=?+= iimiwwPwPSP?
?=?==?+?+=miikkikTrigramTrigramMIkiwwMISPSP431)1,,()(log)(log(3.10)?==?+=niiii wwwP2111 )|log()(log)|(log 11nn wwP ++ (log2+=?+ mniP )| 11?ii ww  Compared with traditional ngram modeling,MI-Ngram modeling incorporates the long distancecontext dependency by computing mutualinformation of the long distance dependent word)()(),,(log),,()()(),,(log),,()()(),,(log),,()()(),,(log),,(),,(BPAPdBAPdBAPBPAPdBAPdBAPBPAPdBAPdBAPBPAPdBAPdBAPdBAAMI+++=(3.11)pairs.
Since the number of possible long distancedependent word pairs may be very huge, it isimpossible for MI-Ngram modeling to incorporateall of them.
Therefore, for MI-Ngram modeling tobe practically useful, how to select a reasonablenumber of word pairs becomes very important.Here two approaches are used (Zhou et al1998 and1999).
One is to restrict the window size of possibleword pairs by computing and comparing theperplexities1 (Shannon C.E.
1951) of various longdistance bigram models for different distances.
It isfound that the bigram perplexities for differentdistances outside the 10-word window becomestable.
Therefore, we only consider MI-Ngrammodeling with a window size of 10 words.
Anotheris to adapt average mutual information to select areasonable number of long distance dependent wordpairs.
Given distance d and two words A and B, itsaverage mutual information is computed as:Compared with mutual information, averagemutual information takes joint probabilities intoconsideration.
In this way, average mutualinformation prefers frequently occurred word pairs.In our paper, different numbers of long distancedependent word pairs will be considered inMI-Ngram modeling within a window size of 10words to evaluate the effect of different MI modelsize.4 ExperimentationAs trigram modeling is most widely used in currentresearch, only MI-Trigram modeling is studiedhere.
Furthermore, in order to demonstrate theeffect of different numbers of word pairs inMI-Trigram modeling, various MI-Trigram modelswith different numbers of word pairs and the samewindow size of 10 words are trained on theXINHUA news corpus of 29 million words whilethe lexicon contains about 56,000 words.
Finally,various MI-Trigram models are tested on the sametask of Chinese word segmentation using theChinese tag bank PFR1.0 2  of 3.69M Chinesecharacters (1.12M Chinese Words).1  Perplexity is a measure of the average number ofpossible choices there are for a random variable.
Theperplexity PP  of a random variable X  with entropyis defined as: )(XH)(2)( XHXPP =Entropy is a measure of uncertainty about a randomvariable.
If a random variable X  occurs with aprobability distribution P x( ) , then the entropy Hof that event is defined as:)(X??
?=XxxPxPXH )(log)()( 2x xlog2 0?Since  as x ?
00 0 02log =, it is conventional  touse the relation  when computing entropy.Table 1 shows the perplexities of variousMI-Trigram models and their performances onChinese word segmentation.
Here, the precision (P)measures the number of correct words in the answerfile over the total number of words in the answer fileand the recall (R) measures the number of correctwords in the answer file over the total number of The units of entropy are bits of information.
This isbecause the entropy of a random variable corresponds tothe average number of bits per event needed to encode atypical sequence of event samples from that randomvariable?
s distribution.2  PFR1.0 is developed by Institute of ComputationalLinguistics at Beijing Univ.
Here, only the wordsegmentation annotation is used.words in the key file.
F-measure is the weightedharmonic mean of precision and recall:PRRPF ++= 22 )1(?
?with =1.
2?Table 1 shows that?
The perplexity and the F-measure rise quicklyas the number of word pairs in MI-Trigrammodeling increases from 0 to 1,600,000 andthen rise slowly.
Therefore, the best 1,600,000word pairs should at least be included.?
Inclusion of the best 1,600,000 word pairsdecreases the perplexity of MI-Trigrammodeling by about 20 percent compared withthe pure trigram model.?
The performance of Chinese wordsegmentation using the MI-Trigram model with1,600,000 word pairs is 0.8 percent higher thanusing the pure trigram model (MI-Trigram with0 word pairs).
That is to say, about 35 percent oferrors can be corrected by incorporating only1,600,000 word pairs to the MI-Trigram modelcompared with the pure trigram model.?
For Chinese word segmentation task, recalls areabout 0.7 percent higher than precisions.
Themain reason may be the existence of unknownwords.
In our experimentation, unknown wordsare segmented into individual Chinesecharacters.
This makes the number ofsegmented words in the answer file higher thanthat in the key file.It is clear that MI-Ngram modeling has muchbetter performance than ngram modeling.
Oneadvantage of MI-Ngram modeling is that its numberof parameters is just a little more than that of ngrammodeling.
Another advantage of MI-Ngrammodeling is that the number of the word pairs can bereasonable in size without losing too much of itsmodeling power.
Compared to ngram modeling,MI-Ngram modeling also captures thelong-distance context dependency of word pairsusing the concept of mutual information.Table 1: The effect of different numbers of word pairs in MI-Trigram modeling with the same window sizeof 10 words on Chinese word segmentationNumber of  word pairs  Perplexity Precision Recall F-measure0 316 97.5 98.2 97.8100,000 295 97.9 98.4 98.1200,000 281 98.1 98.6 98.3400,000 269 98.2 98.7 98.4800,000 259 98.2 98.8 98.51,600,000 250 98.4 98.8 98.63,200,000 245 98.3 98.9 98.66,400,000 242 98.4 98.9 98.66  ConclusionThis paper proposes a new MI-Ngram modelingapproach to capture the context dependency overboth a short distance and a long distance.
This isdone by incorporating long distance dependentword pairs into traditional ngram model by usingthe concept of mutual information.
It is found thatMI-Ngram modeling has much better performancethan ngram modeling.Future works include the explorations of thenew MI-Trigram modeling approach in otherapplications, such as Mandarin speech recognitionand PINYIN to Chinese character conversion.ReferencesBai S.H., Li H.Z., Lin Z.M.
and Yuan B.S.
1989.Building class-based language models withcontextual statistics.
Proceedings of InternationalConference on Acoustics, Speech and SignalProcessing (ICASSP?1998).
pages173-176.Brown P.F.
et al 1992.
Class-based ngram models ofnatural language.
Computational Linguistics  18(4),467-479.Chen S.F.
and Goodman J.
1999.
An empirical studyof smoothing technique for language modeling.Computer, Speech and Language.
13(5).
359-394.Church K.W.
et al 1991.
Enhanced good Turing andCat-Cal: two new methods for estimatingprobabilities of English bigrams.
Computer, Speechand Language  5(1), 19-54.Gale W.A.
and Church K.W.
1990.
Poor estimates ofcontext are worse than none.
Proceedings ofDARPA Speech and Natural Language Workshop,Hidden Valley, Pennsylvania, pages293-295.Gao J.F., Goodman J.T., Cao G.H.
and Li H. 2002.Exploring asymmetric clustering for statisticallanguage modelling.
Proceedings of the FortiethAnnual Meeting of the Association forComputational Linguistics (ACL?2002).Philadelphia.
pages183-190.Hindle D. et al 1993.
Structural ambiguity and lexicalrelations.
Computational Linguistics  19(1),103-120.Jelinek F. 1989.
Self-organized language modeling forspeech recognition.
In Readings in SpeechRecognition.
Edited by Waibel A. and Lee K.F.Morgan Kaufman.
San Mateo.
CA.
pages450-506.Katz S.M.
1987. ?
Estimation of Probabilities fromSparse Data for the Language Model Componentof a Speech Recognizer?.
IEEE Transactions onAcoustics.
Speech and Signal Processing.
35.400-401.Meyer D.  et al 1975.
Loci of contextual effects onvisual word recognition.
In Attention andPerformance V, edited by P.Rabbitt and S.Dornie.pages98-116.
Acdemic Press.Rosenfeld R. 1994.
Adaptive statistical languagemodeling: A Maximum Entropy Approach.
Ph.D.Thesis, Carneige Mellon University.Rosenfeld R. 2000.
Two decades of languagemodelling: where do we go from here.
Proceedingsof IEEE.
88:1270-1278.
August.Shannon C.E.
1951.
Prediction and entropy of printedEnglish.
Bell Systems Technical Journal   30, 50-64.Yang Y.J.
et al 1996.
Adaptive linguistic decodingsystem for Mandarin speech recognitionapplications.
Computer Processing of Chinese &Oriental Languages  10(2), 211-224.Zhou GuoDong and Lua Kim Teng, 1998.
WordAssociation and MI-Trigger-based LanguageModeling.
Proceedings of the Thirtieth-sixthAnnual Meeting of the Association forComputational Linguistics and the SeventeenthInternational Conference on ComputationalLinguistics (COLING-ACL?1998).
Montreal,Canada.
pages10-14.
August.Zhou GuoDong and Lua KimTeng.
1999.Interpolation of N-gram and MI-based TriggerPair Language Modeling in Mandarin SpeechRecognition, Computer, Speech and Language,13(2), 123-135.
