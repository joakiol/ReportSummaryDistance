Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 410?419,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsCross-Domain Co-Extraction of Sentiment and Topic LexiconsFangtao Li?, Sinno Jialin Pan?, Ou Jin?, Qiang Yang?
and Xiaoyan Zhu?
?Department of Computer Science and Technology, Tsinghua University, Beijing, China?
{fangtao06@gmail.com, zxy-dcs@tsinghua.edu.cn}?Institute for Infocomm Research, Singapore?jspan@i2r.a-star.edu.sg?Hong Kong University of Science and Technology, Hong Kong, China?
{kingomiga@gmail.com, qyang@cse.ust.hk}AbstractExtracting sentiment and topic lexicons is im-portant for opinion mining.
Previous workshave showed that supervised learning methodsare superior for this task.
However, the perfor-mance of supervised methods highly relies onmanually labeled training data.
In this paper,we propose a domain adaptation frameworkfor sentiment- and topic- lexicon co-extractionin a domain of interest where we do not re-quire any labeled data, but have lots of labeleddata in another related domain.
The frame-work is twofold.
In the first step, we gener-ate a few high-confidence sentiment and topicseeds in the target domain.
In the secondstep, we propose a novel Relational AdaptivebootstraPping (RAP) algorithm to expand theseeds in the target domain by exploiting thelabeled source domain data and the relation-ships between topic and sentiment words.
Ex-perimental results show that our domain adap-tation framework can extract precise lexiconsin the target domain without any annotation.1 IntroductionIn the past few years, opinion mining and senti-ment analysis have attracted much attention in Natu-ral Language Processing (NLP) and Information Re-trieval (IR) (Pang and Lee, 2008; Liu, 2010).
Senti-ment lexicon construction and topic lexicon extrac-tion are two fundamental subtasks for opinion min-ing (Qiu et al, 2009).
A sentiment lexicon is a listof sentiment expressions, which are used to indicatesentiment polarity (e.g., positive or negative).
Thesentiment lexicon is domain dependent as users mayuse different sentiment words to express their opin-ion in different domains (e.g., different products).
Atopic lexicon is a list of topic expressions, on whichthe sentiment words are expressed.
Extracting thetopic lexicon from a specific domain is importantbecause users not only care about the overall senti-ment polarity of a review but also care about whichaspects are mentioned in review.
Note that, similarto sentiment lexicons, different domains may havevery different topic lexicons.Recently, Jin and Ho (2009) and Li et al (2010a)showed that supervised learning methods canachieve state-of-the-art results for lexicon extrac-tion.
However, the performance of these meth-ods highly relies on manually annotated trainingdata.
In most cases, the labeling work may be time-consuming and expensive.
It is impossible to anno-tate each domain of interest to build precise domain-dependent lexicons.
It is more desirable to automat-ically construct precise lexicons in domains of inter-est by transferring knowledge from other domains.In this paper, we focus on the co-extraction taskof sentiment and topic lexicons in a target domainwhere we do not have any labeled data, but haveplenty of labeled data in a source domain.
Ourgoal is to leverage the knowledge extracted from thesource domain to help lexicon co-extraction in thetarget domain.
To address this problem, we proposea two-stage domain adaptation method.
In the firststep, we build a bridge between the source and tar-get domains by identifying some common sentimentwords as sentiment seeds in the target domain, suchas ?good?, ?bad?, ?nice?, etc.
After that, we gener-ate topic seeds in the target domain by mining somegeneral syntactic relation patterns between the sen-timent and topic words from the source domain.
Inthe second step, we propose a Relational AdaptivebootstraPping (RAP) algorithm to expand the seedsin the target domain.
Our proposed method can uti-410lize useful labeled data from the source domain aswell as exploit the relationships between the topicand sentiment words to propagate information forlexicon construction in the target domain.
Experi-mental results show that our proposed method is ef-fective for cross-domain lexicon co-extraction.In summary, we have three main contributions: 1)We give a systematic study on cross-domain senti-ment analysis in word level.
While, most of previouswork focused on document level; 2) A new two-stepdomain adaptation framework, with a novel RAP al-gorithm for seed expansion, is proposed.
3) We con-duct extensive evaluation, and the experimental re-sults demonstrate the effectiveness of our methods.2 Related Work2.1 Sentiment or Topic Lexicon ExtractionSentiment or topic lexicon extraction is to iden-tify the sentiment or topic words from text.
In thepast, many machine learning techniques have beenproposed for this task.
Hu and Liu et al (2004)proposed an association-rule-based method to ex-tract topic words and a dictionary-based method toidentify sentiment words, independently.
Wiebe etal.
(2004) and Rioff et al (2003) proposed toidentify subjective adjectives and nouns using wordclustering based on their distributional similarity.Popescu and Etzioni (2005) proposed a relaxed la-beling approach to utilize linguistic rules for opinionpolarity detection.
Some researchers also proposedto use topic modeling to identify implicit topics andsentiment words (Mei et al, 2007; Titov and Mc-Donald, 2008; Zhao et al, 2010; Li et al, 2010b),where a topic is a cluster of words, which is differ-ent from our fine-grained topic-word extraction.Jin and Ho (2009) and Li et al (2010a) both pro-posed to use supervised sequential labeling methodsfor topic and opinion extraction.
Experimental re-sults showed that the supervised learning methodscan achieve state-of-the-art performance on lexiconextraction.
However, these methods need to manu-ally annotate a lot of training data in each domain.Recently, Qiu et al (2009) proposed a rule-basedsemi-supervised learning methods for lexicon ex-traction.
However, their method requires to manu-ally define some general syntactic rules among sen-timent and topic words.
In addition, it still requiressome annotated words in the target domain.
In thispaper, we do not assume any predefined rules andlabeled data be available in the target domain.2.2 Domain AdaptationDomain adaptation aims at transferring knowledgeacross domains where data distributions may be dif-ferent (Pan and Yang, 2010).
In the past few years,domain adaptation techniques have been widely ap-plied to various NLP tasks, such as part-of-speechtagging (Ando and Zhang, 2005; Jiang and Zhai,2007; Daume?
III, 2007), named-entity recognitionand shallow parsing (Daume?
III, 2007; Jiang andZhai, 2007; Wu et al, 2009).
There are alsolots of studies for cross-domain sentiment analy-sis (Blitzer et al, 2007; Tan et al, 2007; Li et al,2009; Pan et al, 2010; Bollegala et al, 2011; Heet al, 2011; Glorot et al, 2011).
However, mostof them focused on coarse-grained document-levelsentiment classification, which is different from ourfine-grained word-level extraction.
Our work is sim-ilar to Jakob and Gurevych (2010) which proposed aConditional Random Field (CRF) for cross-domaintopic word extraction.
However, the performanceof their method highly depends on the manually de-signed features.
In our experiments, we compare ourmethod with theirs, and find that ours can achievemuch better results on cross-domain lexicon extrac-tion.
Note that our work is also different from a re-cent work (Du et al, 2010), which focused on identi-fying the polarity of adjective words by using cross-domain knowledge.
While we extract both topic andsentiment words and allow non-adjective sentimentwords, which is more practical.3 Cross-Domain Lexicon Co-Extraction3.1 Problem DefinitionRecall that, we focus on the setting where we haveno labeled data in the target domain, while we haveplenty of labeled data in the source domain.
De-note DS = {(wSi , ySi)}n1i=1 the source domain data,where wSi represents a word in the source domain.ySi ?
Y is the corresponding label of wSi .
Simi-larly, we denote DT = {wTj}n2j=1 the target domaindata, where the input wTj is a word in the target do-main.
In lexicon extraction, Y ?
{1, 2, 3}, whereyi = 1 denotes the corresponding word wi a sen-timent word, yi = 2 denotes wi a topic word, andyi = 3 denotes wi neither a sentiment nor topicword.
Our goal is to predict labels on DT to extracttopic and sentiment words for constructing topic and411sentiment lexicons, respectively.3.2 Motivating ExamplesIn this section, we use some examples to introducethe motivation behind our proposed method.
Table 1shows several reviews from two domains: movie andcamera.
From the table, we can observe that thereare some common sentiment words across differentdomains, such as ?great?, ?excellent?
and ?amaz-ing?.
However, the topic words may be different.For example, in the movie domain, topic words in-clude ?movie?
and ?script?.
While in the camera do-main, topic words include ?camera?
and ?photos?.Domain ReviewcameraThe camera is great.it is a very amazing product.i highly recommend this camera.takes excellent photos.photos had some artifacts and noise.movieThis movie has good script, greatcasting, excellent acting.I love this movie.Godfather was the most amazing movie.The movie is excellent.Table 1: Reviews in camera and movie domains.
Bold-faces are topic words and Italics are sentiment words.Based on the observations, we can build a connec-tion between the source and target domains by iden-tifying the common sentiment words.
Furthermore,intuitively, there are some general syntactic relation-ships or patterns between topic and sentiment wordsacross different domains.
Therefore, if we can minethe patterns from the source and target domain data,then we are able to construct an indirect connectionbetween topic words across domains by using thecommon sentiment words as a bridge, which makesknowledge transfer across domains possible.Figure 1 shows two dependency trees for the sen-tence ?the camera is great?
in the camera domainand the sentence ?the movie is excellent?
in themovie domain, respectively.
As can be observed, therelationships between the topic and sentiment wordsin the two sentences are the same.
They both sharea ?TOPIC-nsubj-SENTIMENT?
relation.
Let thecamera domain be the source domain and the moviedomain be the target domain.
If the word ?excel-lent?
is identified as a common sentiment word, andthe ?TOPIC-nsubj-SENTIMENT?
relation extractedfrom the camera domain is recognized as a commonsyntactic pattern, then the word ?movie?
can be pre-dicted as a topic word in the movie domain with highprobability.
After new topic words are extracted inthe movie domain, we can apply the same syntac-tic pattern or other syntactic patterns to extract newsentiment and topic words iteratively.greatcamera isThensubj copdet(a) Camera domain.excellentmovie isThensubj copdet(b) Movie domain.Figure 1: Examples of dependency tree structure.More specifically, we use the shortest path be-tween a topic word and a sentiment word in the cor-responding dependency tree to denote the relationbetween them.
To get more general paths, we donot take original words in the path into considera-tion, but use their POS tags instead, such as ?NN?,?VB?, ?JJ?, etc.
As an example shown in Figure 2,we can extract two paths or relationships betweentopic and sentiment words from the dependency treeof the sentence ?The movie has good script?
: ?NN-amod-JJ?
from ?script?
and ?good?, and ?NN-nsubj-VB-dobj-NN-amod-JJ?
from ?movie?
and ?good?.has(VB)script(NN)the(DT)movie(NN)good(JJ)dobj nsubjamod detFigure 2: Example of pattern extraction.In the following sections, we present the proposedtwo-stage domain adaptation framework: 1) gener-ating some sentiment and topic seeds in the targetdomain; and 2) expanding the seeds in the target do-main to construct sentiment and topic lexicons.4 Seed GenerationOur basic idea is to first identify several commonsentiment words across domains as sentiment seeds.Meanwhile, we mine some general patterns betweensentiment and topic words from the source domain.Finally, we use the sentiment seeds and general pat-terns to generate topic seeds in the target domain.4124.1 Sentiment Seed GenerationTo identify common sentiment words across do-mains, we extract all sentiment words from thesource domain as candidates.
For each candidate,we calculate its score based on the following metric:S1(wi) = (pS(wi) + pT (wi)) e(?|pS(wi)?pT (wi)|), (1)where pS(wi) and pT (wi) are the probabilities of theword wi occurring in the source and target domains,respectively.
If a word wi has high S1 score, whichimplies that the word wi occurs frequently and simi-larly in both domains, then it can be considered as acommon sentiment word (Pan et al, 2010; Blitzer etal., 2007).
We select top r candidates with highestS1 scores as sentiment seeds.4.2 Topic Seed GenerationWe extract all patterns between sentiment and topicwords in the source domain as candidates.
For eachpattern candidate, we calculate its score based on ametric defined in AutoSlog-TS (Riloff, 1996):S2(Rj) = Acc(Rj)?
log2(Freq(Rj)), (2)where Acc(Rj) is the accuracy of the pattern Rj inthe source domain, and Freq(Rj) is the frequencyof the pattern Rj observed in target domain.
Thismetric aims to identify the patterns that are precisein the source domain and observed frequently in thetarget domain.
We also select the top r patternswith highest S2 scores.
With the patterns and sen-timent seeds, we extract topic-word candidates andmeasure their scores based on a variant metric ofquadratic combination (Zhang and Ye, 2008):S3(wk) =?Rj?A, wi?B(S2(Rj)?
S1(wi)) , (3)where B is a set of sentiment seeds and A is a set ofpatterns which the words wi and wk satisfy.
We thenselect the top r candidates as topic seeds.5 Seed ExpansionAfter generating the topic and sentiment seeds, weaim to expand them in the target domain to constructtopic and sentiment lexicons.
In this section, we pro-pose a new bootstrapping-based method to addressthis problem.Bootstrapping is the process of improving the per-formance of a weak classifier by iteratively addingtraining data and retraining the classifier.
Morespecifically, bootstrapping starts with a small setof labeled ?seeds?, and iteratively adds unlabeleddata that are labeled by the classifier to the train-ing set based on some selection criterion, and retrainthe classifier.
Many bootstrapping-based algorithmshave been proposed to information extraction andother NLP tasks (Blum and Mitchell, 1998; Riloffand Jones, 1999; Jones et al, 1999; Wu et al, 2009).One important issue in bootstrapping is how todesign a criterion to select unlabeled data to beadded to the training set iteratively.
Our proposedbootstrapping for cross-domain lexicon extractionis based on the following two observations: 1) Al-though the source and target domains are different,part of source domain labeled data is still useful forlexicon extraction in the target domain after someadaptation; 2) The syntactic relationships amongsentiment and topic words can be used to expand theseeds in the target domain for lexicon construction.Based on the two observations, we propose anew bootstrapping-based method named RelationalAdaptive bootstraPping (RAP), as summarized inAlgorithm 1, for expanding lexicons across do-mains.
In each iteration, we employ a cross-domainclassifier trained on the source domain lexicons andthe extracted target domain lexicons to predict thelabels of the target unlabeled data, and select top k2predicted topic and sentiment words as candidatesbased on confidence.
With the extracted syntacticpatterns in the previous iterations, we construct abipartite graph between sentiment and topic wordson the extracted target domain lexicons and candi-dates.
After that, a graph-based score refinement al-gorithm is performed on the graph, and the top k1candidates are added to the extracted lexicons basedon the final scores.
Accordingly, with the new ex-tracted lexicons, we update the syntactic patterns ineach iteration.
The details of RAP are presented inthe following sections.5.1 Cross-Domain ClassifierIn this paper, we employ Transfer AdaBoost (TrAd-aBoost) (Dai et al, 2007) as the cross-domain learn-ing algorithm in RAP.
In TrAdaBoost, each wordwSi (or wTj ) is represented by a feature vector xSi(or xTj ).
A classifier trained on the source domaindata DS = {(xSi , ySi)} may perform poor on xTjbecause of domain difference.
The main idea ofTrAdaBoost is to re-weight the source domain databased on a few of target domain labeled data, whichis referred to as seeds in our task.
The re-weighting413aims to reduce the effect of the ?bad?
source do-main data while encourage the ?good?
ones to geta more precise classifier in target domain.
In eachiteration of RAP, we train cross-domain classifiersfTO and fTP for sentiment- and topic- word extrac-tion using TrAdaBoost separately (taking sentimentor topic words as positive instances).
We use linearSupport Vector Machines (SVMs) as the base clas-sifier in TrAdaBoost.
For features to represent eachword, we use lexicon features, such as the previous,current and next words, and POS tag features, suchas the previous, current and next words?
POS tags.Algorithm 1 Relational Adaptive bootstraPpingRequire: Target domain data DT = DlT?DuT , where DlTconsists of sentiment seeds B and topic seeds C and theirinitial scores S1(wi), ?wi ?
B and S3(wj), ?wj ?
C, DuTis the set of unlabeled target domain data; labeled sourcedomain data DS ; a cross-domain classifier; iteration num-ber M and candidate selection number k1, k2.Ensure: Expand C and B in the target domain.1: Initialize a pattern set A = ?, S?1(wi) = S1(wi), wi ?
Band S?3(wj) = S3(wj), wj ?
C. Consider all patternsobserved in the source domain as pattern candidates P .2: for m = 1 .
.
.M do3: Extract new pattern candidates to P with DlT in targetdomain, update pattern score S?2(Rj), where Rj ?
P ,based on Eq.
(4), and select the top k1 patterns to thepattern set A.4: Learn the cross-domain classifiers fTO and fTP forsentiment- and topic- word extraction with DS?DlTseparately.
Predict the sentiment score hTfO (wTj ) andtopic score hTfP (wTj ) on DuT , and select k2 sentimentwords and topic words with highest scores as candidates.5: Construct a bipartite graph between sentiment and topicwords on DlT and the k2 sentiment- and topic- word can-didates, and calculate the normalized weights ?ij?s foreach edge of the graph.6: Refine the scores S?1 and S?3 of the k2 sentiment andtopic word candidates using Eqs.
(5) and (6) iteratively.7: Select k1 new sentiment words and k1 new topic wordswith the final scores, and add them to lexicons B and C.Update S?1(wi) and S?3(wj) accordingly.8: end for9: return Expanded lexicons B and C.5.2 Graph ConstructionBased on the cross-domain classifiers fTO and fTP ,we can predict the sentiment label score hTfO(wTi)and topic label score hTfP (wTi) for the target domaindata wTi .
According to all predicted values, we re-spectively select top k2 new sentiment- and topic-words as candidates.
Together with the extractedsentiment and topic lexicons in the target domain,we build a bipartite graph among them as shown inFigure 3.
In the bipartite graph, one set of nodesrepresents topic words, including new topic candi-dates and words in the lexicon C, and the other setof nodes represents sentiment words, including newsentiment candidates and words in the lexicon B.For a pair of sentiment and topic words wOTi and wPTj ,if there is a pattern Rj in the pattern set A that theycan satisfy, then there exists an edge eij betweenthem.
Furthermore, each edge eij is associated witha nonnegative weight ?ij , which is measured as fol-lows, ?ij =?Rk?E S?2(Rk), where S?2 is the patternscore.
Similar to the metric defined in Eq.
(3), thepattern score is defined as:S?2(Rj) =?{wi,wk}?E(S?1(wi)?
S?3(wk)), (4)where E = {{wi, wj}|, wi ?
B,wj ?
C andwi, wj satisfy Rj , Rj ?
A}.
Note that in the be-ginning of each iteration, S?2 is updated based on thenew sentiment score S?1 and topic score S?3.
We fur-ther normalize ?ij by ?
?ij = ?ij/(?ij ?ij).Topic words Sentiment wordsmusicmovierecommendgoodboringscriptNN-nsubj-VB-dobj-NN-amod-JJNN-amod-JJNN-nsubj-JJNN-amod-JJNN-dobj-VBFigure 3: Topic and sentiment word graph.5.3 Score ComputationWe construct the bipartite graph to exploit the re-lationships between sentiment and topic words topropagate information for lexicon extraction.
Weuse the following reinforcement formulas to itera-tively update the final sentiment score S?1(wTj ) andtopic score S?3(wTi), respectively:S?1(wTj ) = ??iS?3(wTi)?
?ij + (1?
?
)hTfO (wTj ), (5)S?3(wTi) = ?
?jS?1(wTj )?
?ij + (1?
?
)hTfP (wTi), (6)where ?
is a trade-off parameter between the pre-dicted value by cross-domain classifier and the re-inforcement scores from other nodes connected by414edge eij .
Here ?
is empirically set to be 0.5.
WithEqs.
(5) and (6), the sentiment scores and topicscores are iteratively refined until the state of thegraph trends to be stable.
This can be consideredas an extension to the HITS algorithm(Kleinberg,1999).
Finally, we select k1 ?
k2 sentiment andtopic words from the k2 candidates based on theirrefined scores, and add them to the target domainlexicons, respectively.
We also update the sentimentscore S?1 and topic score S?3 for next iteration.5.4 Special CasesWe now introduce two special cases of the RAP al-gorithm.
In Eqs.
(5) and (6), if the parameter ?
= 1,then RAP only uses the relationships between sen-timent and topic words with their patterns to propa-gate label information in the target domain withoutusing the cross-domain classifier.
We call this reduc-tion relational bootstrapping.
If ?
= 0, then RAPonly utilizes useful source domain labeled data to as-sist learning of the target domain classifier withoutconsidering the relationships between sentiment andtopic words.
We call this reduction adaptive boot-strapping, which can be considered as a bootstrap-ping version of TrAdaBoost.
We also empiricallystudy these two special cases in experiments.6 Experiments on Lexicon Evaluation6.1 Data Set and Evaluation CriteriaWe use the review dataset from (Li et al, 2010a),which contains 500 movie and 601 product reviews,for evaluation.
The sentiment and topic words aremanually annotated.
In this dataset, all types ofsentiment words are annotated instead of adjectivewords only.
For example, the verbs, such as ?like?,?recommend?, and nouns, such as ?masterpiece?,are also labeled as sentiment words.
We constructtwo cross-domain lexicon extraction tasks: ?prod-uct vs. movie?
and ?movie vs.
product?, where theword before ?vs.?
corresponds with the source do-main and the word after ?vs.?
corresponds with thetarget domain.
We evaluate our methods in terms ofprecision, recall and F-score (F1).6.2 BaselinesThe results of in-domain classifiers, which aretrained on plenty of target domain labeled data, canbe treated as upper-bounds.
We denote iSVM andiCRF the in-domain SVM and CRF classifiers inexperiments, and compare our proposed methods,RAP, relational bootstrapping, and adaptive boot-strapping, with the following baselines,Unsupervised Method (Un) we implement a rule-based method for lexicon extraction based on (Huand Liu, 2004), where adjective words that matcha rule is recognized as sentiment words, and nounsthat match a rule are recognized as topic words.Semi-Supervised Method (Semi) we implementthe double propagation model proposed in (Qiu etal., 2009).
Since this method requires some targetdomain labeled data, we manually label 30 senti-ment words in the target domain.Cross-Domain CRF (Cross-CRF) we implementa cross-domain CRF algorithm proposed by (Jakoband Gurevych, 2010).TrAdaBoost We apply TrAdaBoost (Dai et al,2007) on the source domain labeled data and thegenerated seeds in the target domain to train a lexi-con extractor.6.3 Comparison ResultsComparison results on lexicon extraction are shownin Table 2 and Table 3.
From Table 2, we can ob-serve that our proposed methods are effective forsentiment lexicon extraction.
The relational boot-strapping method performs better than the unsuper-vised method, TrAdaBoost and the cross-domainCRF algorithm, and achieves comparable resultswith the semi-supervised method.
However, com-pared to the semi-supervised method, our proposedrelational bootstrapping method does not require anylabeled data in the target domain.
We can also ob-serve that the adaptive bootstrapping method and theRAP method perform much better than other meth-ods in terms of F-score.
The reason is that part ofthe source domain labeled data may be useful forlearning the target classifier after reweighting.
Inaddition, we also observe that embedding the TrAd-aBoost algorithm into a bootstrapping process canfurther boost the performance of the classifier forsentiment lexicon extraction.Table 3 shows the comparison results on topic lex-icon extraction.
From the table, we can observe thatdifferent from the sentiment lexicon extraction task,the relational bootstrapping method performs betterthan the adaptive bootstrapping method slightly.
Thereason may be that for the sentiment lexicon extrac-tion task, there exist some common sentiment words415product vs. movie movie vs. productPrec.
Rec.
F1 Prec.
Rec.
F1Un 0.82 0.31 0.45 0.74 0.23 0.35Semi 0.71 0.44 0.54 0.62 0.45 0.52Cross-CRF 0.69 0.40 0.51 0.65 0.34 0.45Tradaboost 0.73 0.41 0.52 0.72 0.42 0.52Adaptive 0.68 0.53 0.59 0.63 0.52 0.57Relational 0.55 0.51 0.53 0.57 0.51 0.54RAP 0.69 0.59 0.64 0.66 0.59 0.62iSVM 0.82 0.60 0.70 0.80 0.61 0.68iCRF 0.80 0.66 0.72 0.80 0.62 0.69Table 2: Results on sentiment lexicon extraction.
Num-bers in boldface denote significant improvement.product vs. movie movie vs. productPrec.
Rec.
F1 Prec.
Rec.
F1Un 0.41 0.32 0.36 0.53 0.35 0.41Semi 0.54 0.59 0.56 0.75 0.50 0.60Cross-CRF 0.70 0.23 0.34 0.80 0.24 0.37Tradaboost 0.64 0.45 0.53 0.57 0.47 0.51Adaptive 0.76 0.44 0.56 0.70 0.52 0.59Relational 0.57 0.58 0.58 0.61 0.57 0.59RAP 0.80 0.56 0.66 0.73 0.58 0.65iSVM 0.83 0.73 0.78 0.85 0.70 0.77iCRF 0.84 0.78 0.81 0.87 0.73 0.80Table 3: Results on topic lexicon extraction.
Numbers inboldface denote significant improvement.across domains, thus part of the labeled source do-main data may be useful for the target learning task.However, for the topic lexicon extraction task, thetopic words may be totally different, and as a result,we may not be able to find useful source domainlabeled data to boost the performance for lexiconextraction in the target domain.
In this case, mu-tual label propagation between sentiment and topicwords may be more reasonable for knowledge trans-fer.
RAP absorbs the advantages of the adaptive andrelational bootstrapping methods, thus can get thebest results in both lexicon extraction tasks.We also observe that relational bootstrapping canget better recall, but lower precision, compared toadaptive bootstrapping.
This is because relationalbootstrapping only utilizes the patterns to propagatelabel information, which may cover more topic andsentiment seeds, but include some noisy words.
Forexample, given two phases ?like the camera?
and?recommend the camera?, we can extract a pattern?VB-dobj-NN?.
However, by using this pattern andthe topic word ?camera?, we may extract ?take?
asa sentiment word from another phase ?take the cam-era?, which is incorrect.
The adaptive bootstrappingmethod can utilize various features to make predic-tions more precisely, which may have higher preci-sion, but encounter the lower recall problem.
For ex-ample, ?flash?
is not identified as a topic word in thetarget product domain (camera domain).
Our RAPmethod can exploit both relationships between sen-timent and topic words and part of labeled sourcedomain data for cross-domain lexicon extraction.
Itcan correctly identify the above two cases.6.3.1 Parameter Sensitivity StudyIn this section, we conduct experiments to studythe effect of different parameter settings.
There areseveral parameters in the framework: the numberof generated seeds r, the number of new candidatesk2 and the number of selections k in each iteration,and the number of iterations M (?
is empirically setto 0.5 ).
For the parameter k2, we just set it to alarge number (k2 = 100) such that have rich candi-dates to build the bipartite graph.
In the experimentsreported in the previous section, we set r = 20,k1 = 10 and M = 50.
Figures 4(a) and 4(b) showthe results under varying values of r in the ?productvs.
movie?
task.
Observe that for sentiment wordextraction, the results of the proposed methods arenot sensitive to the values of r. While for the topicword extraction, the proposed methods perform wellwhen r falls in the range from 15 to 20.5 10 15 20 25 300.450.50.550.60.650.7Values of rF?scoreRelationalAdaptiveRAP(a) Sentiment word extraction5 10 15 20 25 300.450.50.550.60.650.7Values of rF?scoreRelationalAdaptiveRAP(b) Topic word extractionFigure 4: Results on varying values of r.0 10 20 30 40 500.40.450.50.550.60.65Number of iterationsF?scoreRelationalAdaptiveRAP(a) Sentiment word extraction0 10 20 30 40 500.40.450.50.550.60.650.7Number of iterationsF?scoreRelationalAdaptiveRAP(b) Topic word extractionFigure 5: Results on varying values of M .We also test the sensitivity of the parameter k1and find that the proposed methods work well androbust when k1 falls in the range from 10 to 20.416Figures 5(a) and 5(b) show the results under vary-ing numbers of iterations in the ?product vs. movie?task.
As we can see, our proposed methods convergewell when M ?
40.7 Application: Sentiment ClassificationTo further verify the usefulness of the lexicons ex-tracted by the RAP method, we apply the extractedsentiment lexicon for sentiment classification.7.1 Experiment SettingOur work is motivated by the work of (Pang andLee, 2004), which only used subjective sentencesfor document-level sentiment classification, insteadof using all sentences.
In this experiment, we onlyuse sentiment related words as features to representopinion documents for classification, instead of us-ing all words.
Our goal is compare the sentimentlexicon constructed by the RAP method with othergeneral lexicons on the impact of for sentiment clas-sification.
The general lexicons used for comparisonare described in Table 4.We use the dataset from (Blitzer et al, 2007) forsentiment classification.
It contains a collection ofproduct reviews from Amazon.com.
The reviews areabout four product domains: books, dvds, electron-ics and kitchen appliance.
In each domain, there are1000 positive and 1000 negative reviews.
To con-struct domain specific sentiment lexicons, we applyRAP on each product domain with the movie domaindescribed in Section 6.1 as the source domain.
Fi-nally, we use linear SVM as the classifier and theclassification accuracy as the evaluate criterion.Lexicon Name Size DescriptionSenti-WordNet 6957 Words with a subjective score > 0.6(Esuli and Sebastiani, 2006)HowNet 4619 Eng.
translation of subj.
Chinesewords (Dong and Dong, 2006)Subj.
Clues 6878 Lexicons from (Wilson et al, 2005)Table 4: Description of different lexicons.7.2 Experimental ResultsExperimental results on sentiment classification areshown in Table 5, where we denote ?All?
using allunigram and bigram features instead of using sub-jective words.
As we can see that a classifier trainedwith features constructed by our RAP method per-formance best in all domains.
Note that the num-ber of features (sentiment words) constructed by ourmethod is much smaller than that of all unigramand bigram features, which can reduce the classi-fier training time dramatically.
These promising re-sults imply that our RAP can be applied for senti-ment classification effectively and efficiently.All Senti HowNet Subj.
Clue Oursdvd 82.55 79.80 80.57 80.93 84.05book 80.71 76.22 78.22 79.48 81.65electronic 84.43 82.42 83.05 83.22 86.71kitchen 87.70 81.78 84.17 84.23 88.83Table 5: Sentiment classification results (accuracy in %).Numbers in boldface denotes significant improvement.8 ConclusionsIn this paper, we propose a two-stage framework forco-extraction of sentiment and topic lexicons acrossdomains where we have no labeled data in the tar-get domain but have plenty of labeled data in an-other domain.
In the first stage, we propose a sim-ple strategy to generate a few high-quality sentimentand topic seeds for the target domain.
In the secondstage, we propose a novel Relational Adaptive boot-straPping (RAP) method to expand the seeds, whichcan exploit the relationships between topic and opin-ion words, and make use of part of useful source do-main labeled data for help.
Extensive experimentalresults show our proposed method can extract pre-cise sentiment and topic lexicons from the target do-main.
Furthermore, the extracted sentiment lexiconcan be applied to sentiment classification effectively.In the future work, besides the heterogeneousrelationships between topic and sentiment words,we intend to investigate the homogeneous relation-ships among topic words and those among sentimentwords (Qiu et al, 2009) to further boost the perfor-mance of RAP method.
Furthermore, in our frame-work, we do not identify the polarity of the extractedsentiment lexicon.
We also plan to embed this com-ponent into our unified framework.
Finally, it is alsointeresting to exploit multi-domain knowledge (Liand Zong, 2008; Bollegala et al, 2011) for cross-domain lexicon extraction.9 AcknowledgementThis work was supported by the Chinese Natu-ral Science Foundation No.60973104, National KeyBasic Research Program 2012CB316301, and HongKong RGC GRF Projects 621010 and 621211.417ReferencesRie K. Ando and Tong Zhang.
2005.
A framework forlearning predictive structures from multiple tasks andunlabeled data.
J. Mach.
Learn.
Res., 6:1817?1853.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 432?439,Prague, Czech Republic.
ACL.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of the 11th Annual Conference on ComputationalLearning Theory, pages 92?100.Danushka Bollegala, David Weir, and John Carroll.2011.
Using multiple sources to construct a sentimentsensitive thesaurus for cross-domain sentiment clas-sification.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 132?141, Port-land, Oregon.
ACL.Wenyuan Dai, Qiang Yang, Guirong Xue, and Yong Yu.2007.
Boosting for transfer learning.
In Proceed-ings of the 24th International Conference on MachineLearning, pages 193?200, Corvalis, Oregon, USA,June.
ACM.Hal Daume?
III.
2007.
Frustratingly easy domain adapta-tion.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 256?263, Prague, Czech Republic.
ACL.Zhendong Dong and Qiang Dong, editors.
2006.HOWNET and the computation of meaning.
WorldScientific Publishers, Norwell, MA, USA.Weifu Du, Songbo Tan, Xueqi Cheng, and XiaochunYun.
2010.
Adapting information bottleneck methodfor automatic construction of domain-oriented senti-ment lexicon.
In Proceedings of the 3rd ACM inter-national conference on Web search and data mining,pages 111?120, New York, NY, USA.
ACM.Andrea Esuli and Fabrizio Sebastiani.
2006.
SENTI-WORDNET: A publicly available lexical resource foropinion mining.
In In Proceedings of the 5th Confer-ence on Language Resources and Evaluation, pages417?422.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In Pro-ceedings of the 28th International Conference on Ma-chine Learning, pages 513?520, Bellevue, Washing-ton, USA.Yulan He, Chenghua Lin, and Harith Alani.
2011.
Auto-matically extracting polarity-bearing topics for cross-domain sentiment classification.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 123?131, Portland, Oregon.
ACL.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 168?177, Seat-tle, WA, USA.
ACM.Niklas Jakob and Iryna Gurevych.
2010.
Extractingopinion targets in a single- and cross-domain settingwith conditional random fields.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1035?1045, Cambridge,Massachusetts, USA.
ACL.Jing Jiang and ChengXiang Zhai.
2007.
Instance weight-ing for domain adaptation in NLP.
In Proceedings ofthe 45th Annual Meeting of the Association of Com-putational Linguistics, pages 264?271, Prague, CzechRepublic.
ACL.Wei Jin and Hung Hay Ho.
2009.
A novel lexical-ized HMM-based learning framework for web opinionmining.
In Proceedings of the 26th Annual Interna-tional Conference on Machine Learning, pages 465?472, Montreal, Quebec, Canada.
ACM.Rosie Jones, Andrew Mccallum, Kamal Nigam, andEllen Riloff.
1999.
Bootstrapping for text learningtasks.
In In IJCAI-99 Workshop on Text Mining: Foun-dations, Techniques and Applications, pages 52?63.Jon M. Kleinberg.
1999.
Authoritative sources in a hy-perlinked environment.
J. ACM, 46:604?632, Sept.Shoushan Li and Chengqing Zong.
2008.
Multi-domainsentiment classification.
In Proceedings of the 46thAnnual Meeting of the Association for ComputationalLinguistics on Human Language Technologies: ShortPapers, pages 257?260, Columbus, Ohio, USA.
ACL.Tao Li, Vikas Sindhwani, Chris Ding, and Yi Zhang.2009.
Knowledge transformation for cross-domainsentiment classification.
In Proceedings of the 32ndinternational ACM SIGIR conference on Research anddevelopment in information retrieval, pages 716?717,Boston, MA, USA.
ACM.Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,Ying-Ju Xia, Shu Zhang, and Hao Yu.
2010a.Structure-aware review mining and summarization.
InProceedings of the 23rd International Conference onComputational Linguistics, pages 653?661, Beijing,China.Fangtao Li, Minlie Huang, and Xiaoyan Zhu.
2010b.Sentiment analysis with global topics and local de-pendency.
In Proceedings of the Twenty-Fourth AAAIConference on Artificial Intelligence, Atlanta, Geor-gia, USA.
AAAI Press.418Bing Liu.
2010.
Sentiment analysis and subjectivity.Handbook of Natural Language Processing, SecondEdition.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, andChengXiang Zhai.
2007.
Topic sentiment mixture:modeling facets and opinions in weblogs.
In Pro-ceedings of the 16th international conference on WorldWide Web, pages 171?180, Banff, Alberta, Canada.ACM.Sinno Jialin Pan and Qiang Yang.
2010.
A surveyon transfer learning.
IEEE Trans.
Knowl.
Data Eng.,22(10):1345?1359, Oct.Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, QiangYang, and Chen Zheng.
2010.
Cross-domain senti-ment classification via spectral feature alignment.
InProceedings of the 19th International Conference onWorld Wide Web, pages 751?760, Raleigh, NC, USA,Apr.
ACM.Bo Pang and Lillian Lee.
2004.
A sentimental edu-cation: sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, Barcelona, Spain.
ACL.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.Ana-Maria Popescu and Oren Etzioni.
2005.
Extractingproduct features and opinions from reviews.
In Pro-ceedings of Human Language Technology Conferenceand Conference on Empirical Methods in Natural Lan-guage Processing, pages 339?346, Vancouver, BritishColumbia, Canada.
ACL.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009.Expanding domain sentiment lexicon through doublepropagation.
In Proceedings of the 21st internationaljont conference on Artifical intelligence, pages 1199?1204, Pasadena, California, USA.
Morgan KaufmannPublishers Inc.Ellen Riloff and Rosie Jones.
1999.
Learning dictio-naries for information extraction by multi-level boot-strapping.
In Proceedings of the 6th national con-ference on Artificial intelligence, pages 474?479, Or-lando, Florida, United States.
AAAI.Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003.Learning subjective nouns using extraction patternbootstrapping.
In Proceedings of the 7th conferenceon natural language learning, pages 25?32, Edmon-ton, Canada.
ACL.Ellen Riloff.
1996.
Automatically generating extrac-tion patterns from untagged text.
In Proceedings ofthe Thirteenth National Conference on Artificial In-telligence, pages 1044?1049, Portland, Oregon, USA.AAAI Press/MIT Press.Songbo Tan, Gaowei Wu, Huifeng Tang, and XueqiCheng.
2007.
A novel scheme for domain-transferproblem in the context of sentiment analysis.
In Pro-ceedings of the 16th ACM conference on Conferenceon information and knowledge management, pages979?982, Lisbon, Portugal.
ACM.Ivan Titov and Ryan McDonald.
2008.
A joint model oftext and aspect ratings for sentiment summarization.In Proceedings of the 46th Annual Meeting of the As-sociation of Computational Linguistics: Human Lan-guage Technologies, pages 308?316, Columbus, Ohio,USA.
ACL.Janyce Wiebe, Theresa Wilson, Rebecca Bruce, MatthewBell, and Melanie Martin.
2004.
Learning subjectivelanguage.
Comput.
Linguist., 30:277?308, Sept.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the conferenceon Human Language Technology and Empirical Meth-ods in Natural Language Processing, pages 347?354,Vancouver, British Columbia, Canada.
ACL.Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.2009.
Domain adaptive bootstrapping for named en-tity recognition.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 1523?1532, Singapore.
ACL.Min Zhang and Xingyao Ye.
2008.
A generationmodel to unify topic relevance and lexicon-based sen-timent for opinion retrieval.
In Proceedings of the31st annual international ACM SIGIR conference onResearch and development in information retrieval,pages 411?418, Singapore.
ACM.Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-ing Li.
2010.
Jointly modeling aspects and opin-ions with a MaxEnt-LDA hybrid.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 56?65, Cambridge, Mas-sachusetts, USA.
ACL.419
