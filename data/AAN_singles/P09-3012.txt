Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 96?104,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPCreating a Gold Standard for Sentence Clustering in Multi-DocumentSummarizationJohanna GeissUniversity of CambridgeComputer Laboratory15 JJ Thomson AvenueCambridge, CB3 0FD, UKjohanna.geiss@cl.cam.ac.ukAbstractSentence Clustering is often used as a firststep in Multi-Document Summarization(MDS) to find redundant information.
Allthe same there is no gold standard avail-able.
This paper describes the creationof a gold standard for sentence cluster-ing from DUC document sets.
The proce-dure of building the gold standard and theguidelines which were given to six humanjudges are described.
The most widelyused and promising evaluation measuresare presented and discussed.1 IntroductionThe increasing amount of (online) information andthe growing number of news websites lead to a de-bilitating amount of redundant information.
Dif-ferent newswires publish different reports aboutthe same event resulting in information overlap.Multi-Document Summarization (MDS) can helpto reduce the amount of documents a user has toread to keep informed.
In contrast to single doc-ument summarization information overlap is oneof the biggest challenges to MDS systems.
Whilerepeated information is a good evidence of im-portance, this information should be included ina summary only once in order to avoid a repeti-tive summary.
Sentence clustering has thereforeoften been used as an early step in MDS (Hatzi-vassiloglou et al, 2001; Marcu and Gerber, 2001;Radev et al, 2000).
In sentence clustering se-mantically similar sentences are grouped together.Sentences within a cluster overlap in information,but they do not have to be identical in meaning.In contrast to paraphrases sentences in a cluster donot have to cover the same amount of information.One sentence represents one cluster in the sum-mary.
Either a sentences from the cluster is se-lected (Aliguliyev, 2006) or a new sentence isregenerated from all/some sentences in a cluster(Barzilay and McKeown, 2005).
Usually the qual-ity of the sentence clusters are only evaluated in-directly by judging the quality of the generatedsummary.
There is still no standard evaluationmethod for summarization and no consensus in thesummarization community how to evaluate a sum-mary.
The methods at hand are either superficialor time and resource consuming and not easily re-peatable.
Another argument against indirect evalu-ation of clustering is that troubleshooting becomesmore difficult.
If a poor summary was created it isnot clear which component e.g.
information ex-traction through clustering or summary generation(using for example language regeneration) is re-sponsible for the lack of quality.However there is no gold standard for sentenceclustering available to which the output of a clus-tering systems can be compared.
Another chal-lenge is the evaluation of sentence clusters.
Thereare a lot of evaluation methods available.
Each ofthem focus on different properties of a set of clus-ters.
We will discuss and evaluate the most widelyused and most promising measures.
In this paperthe main focus is on the development of a goldstandard for sentence clustering using DUC clus-ters.
The guidelines and rules that were given tothe human annotators are described and the inter-judge agreement is evaluated.2 Related WorkSentence Clustering is used for different applica-tion in NLP.
Radev et al (2000) use it in theirMDS system MEAD.
The centroids of the clustersare used to create a summary.
Only the summaryis evaluated, not the sentence clusters.
The sameapplies to Wang et al (2008).
They use symmet-ric matrix factorisation to group similar sentencestogether and test their system on DUC2005 andDUC2006 data set, but do not evaluate the clus-terings.
However Zha (2002) created a gold stan-96dard relying on the section structure of web pagesand news articles.
In this gold standard the sec-tion numbers are assumed to give the true clusterlabel for a sentence.
In this approach only sen-tences within the same document and even withinthe same paragraph are clustered together whereasour approach is to find similar information be-tween documents.A gold standard for event identification wasbuilt by Naughton (2007).
Ten annotators taggedevents in a sentence.
Each sentence could be as-signed more than one event number.
In our ap-proach a sentence can only belong to one cluster.For the evaluation of SIMFINDER Hatzivas-siloglou et al (2001) created a set of 10.535 man-ually marked pairs of paragraphs.
Two human an-notator were asked to judge if the paragraphs con-tained ?common information?.
They were giventhe guideline that only paragraphs that describedthe same object in the same way or in which thesame object was acting the same are to be consid-ered similar.
They found significant disagreementbetween the judges but the annotators were able toresolve their differences.
Here the problem is thatonly pairs of paragraphs are annotated whereas wefocus on whole sentences and create not pairs butclusters of similar sentences.3 Data Set for ClusteringThe data used for the creation of the gold stan-dard was taken from the Document UnderstandingConference (DUC)1document sets.
These doc-ument clusters were designed for the DUC taskswhich range from single-/multi-document summa-rization to update summaries, where it is assumedthat the reader has already read earlier articlesabout an event and requires only an update of thenewer development.
Since DUC has moved toTAC in 2008 they focus on the update task.
Inthis paper only clusters designed for the generalmulti-document summarization task are used.Our clustering data set consists of four sen-tence sets.
They were created from the docu-ment sets d073b (DUC 2002), D0712C (DUC2007), D0617H (DUC 2006) and d102a (DUC2003).
Especially the newer document clusterse.g.
from DUC 2006 and 2007 contain a lot of doc-uments.
In order to build good sentence clustersthe judges have to compare each sentence to each1DUC has now moved to the Text Analysis Conference(TAC)other sentence and maintain an overview of thetopics within the documents.
Because of humancognitive limitations the number of documents andsentences have to be reduced.
We defined a set ofconstraints for a sentence set: (i) from one set, (ii)a sentence set should consist of 150 ?
200 sen-tences2.
To obtain sentence sets that comply withthese requirements we designed an algorithm thattakes the number of documents in a DUC set, thedate of publishing, the number of documents pub-lished on the same day and the number of sen-tences in a document into account.
If a documentset includes articles published on the same daythey were given preference.
Furthermore shorterdocuments (in terms of number of sentences) werefavoured.
The properties of the resulting sentencesets are listed in table 1.
The documents in a setwere ordered by date and split into sentences us-ing the sentence boundary detector from RASP(Briscoe et al, 2006).name DUC DUC id docs senVolcano 2002 D073b 5 162Rushdie 2007 D0712C 15 103EgyptAir 2006 D0617H 9 191Schulz 2003 d102a 5 248Table 1: Properties of sentence sets4 Creation of the Gold StandardEach sentence set was manually clustered by atleast three judges.
In total there were six judgeswhich were all volunteers.
They are all second-language speakers of English and hold at least aMaster?s degree.
Three of them (Judge A, Judge Jand Judge O) have a background in computationallinguistics.
The judges were given a task descrip-tion and a list of guidelines.
They were only usingthe guidelines given and worked independently.They did not confer with each other or the author.Table 2 gives details about the set of clusters eachjudge created.4.1 GuidelinesThe following guidelines were given to the judges:1.
Each cluster should contain only one topic.2.
In an ideal cluster the sentences are very similar.2If a DUC set contains only 5 documents all of them areused to create the sentence set, even if that results in morethan 200 sentences.
If the DUC set contains more than 15documents, only 15 documents are used for clustering even ifthe number of 150 sentences is not reached.97judge Rushdie Volcano EgyptAir Schulzs c s/c s c s/c s c s/c s c s/cJudge A 70 15 4.6 92 30 3 85 28 3 54 16 3.4Judge B 41 10 4.1 57 21 2.7 44 15 2.9 38 11 3.5Judge D 46 16 2.9Judge H 74 14 5.3 75 19 3.9Judge J 120 7 17.1Judge O 53 20 2.6Table 2: Details of manual clusterings: s number of sentences in a set, c number of clusters, s/c averagenumber of sentences in a cluster3.
The information in one cluster should come fromas many different documents as possible.
Themore different sources the better.
Clusters of sen-tences from only one document are not allowed.4.
There must be at least two sentences in a cluster,and more than two if possible.5.
Differences in numbers in the same cluster areallowed (e.g.
vagueness in numbers (300,000 -350,000), update (two killed - four dead))6.
Break off very similar sentences from one clusterinto their own subcluster, if you feel the cluster isnot homogeneous.7.
Do not use too much inference.8.
Partial overlap ?
If a sentence has parts that fit intwo clusters, put the sentence in the more impor-tant cluster.9.
Generalisation is allowed, as long as the sen-tences are about the same person, fact or event.The guidelines were designed by the author andher supervisor ?
Dr Simone Teufel.
The startingpoint was a single DUC document set which wasclustered by the author and her supervisor with thetask in mind to find clusters of sentences that rep-resent the main topics in the documents.
The mini-mal constraint was that each cluster is specific andgeneral enough to be described in one sentence(see rule 1 and 2).
By looking at the differencesbetween the two manual clustering and reviewingthe reasons for the differences the other rules weregenerated and tested on another sentence set.One rule that emerged early says that a topic canonly be included in the summary of a documentset if it appears in more than one document (rule3).
From our understanding of MDS and our defi-nition of importance only sentences that depict atopic which is present in more than one sourcedocument can be summary worthy.
From thisit follows that clusters must contain at least twosentences which come from different documents.Sentences that are not in any cluster of at least twoare considered irrelevant for the MDS task (rule4).
We defined a spectrum of similarity.
In an idealcluster the sentences would be very similar, almostparaphrases.
For our task sentences that are notparaphrases can be in the same cluster (see rule 5,8, 9).
In general there are several constraints thatpull against each other.
The judges have to find thebest compromise.We also gave the judges a recommended proce-dure:1.
Read all documents.
Start clustering from thefirst sentence in the list.
Put every sentence thatyou think will attract other sentences into an initialcluster.
If you feel, that you will not find any similarsentences to a sentence, put it immediately aside.Continue clustering and build up the clusters whileyou go through the list of sentences.2.
You can rearrange your clusters at any point.3.
When you are finished with clustering check thatall important information from the documents iscovered by your clusters.
If you feel that a veryimportant topic is not expressed in your clusters,look for evidence for that information in the text,even in secondary parts of a sentence.4.
Go through your sentences which do not belongto any cluster and check if you can find a suitablecluster.5.
Do a quality check and make sure that you wrotedown a sentence for each cluster and that the sen-tences in a cluster are from more than one docu-ment.6.
Rank the clusters by importance.4.2 Differences in manual clusteringsEach judge clustered the sentence sets differently.No two judges came up with the same separationinto clusters or the same amount of irrelevant sen-tences.
When analysing the differences betweenthe judges we found three main categories:Generalisation One judge creates a cluster thatfrom his point of view is homogeneous:1.
Since then, the Rushdie issue has turned into abig controversial problem that hinders the rela-tions between Iran and European countries.2.
The Rushdie affair has been the main hurdle inIran?s efforts to improve ties with the EuropeanUnion.983.
In a statement issued here, the EU said the Iraniandecision opens the way for closer cooperation be-tween Europe and the Tehran government.4.
?These assurances should make possible a muchmore constructive relationship between the UnitedKingdom, and I believe the European Union, withIran, and the opening of a new chapter in our re-lations,?
Cook said after the meeting.Another judge however puts these sentences intotwo separate cluster (1,2) and (3,4).The first judgechooses a more general approach and created acluster about the relationship between Iran andthe EU, whereas the other judge distinguishes be-tween the improvement of the relationship and thereason for the problems in the relationship.Emphasise Two judges can emphasise on differ-ent parts of a sentence.
For example the sentence?All 217 people aboard the Boeing 767-300 died when itplunged into the Atlantic off the Massachusetts coast onOct.
31, about 30 minutes out of New York?s KennedyAirport on a night flight to Cairo.?
was clustered to-gether with other sentence about the number of ca-sualties by one judge.
Another judge emphasisedon the course of events and put it into a differentcluster.Inference Humans use different level of inter-ference.
One judge clustered the sentence ?Schulz,who hated to travel, said he would have been happy liv-ing his whole life in Minneapolis.?
together with othersentences which said that Schulz is from Min-nesota although this sentence does not clearly statethis.
This judge interfered from ?he would have beenhappy living his whole life in Minneapolis?
that he actu-ally is from Minnesota.5 Evaluation measuresThe evaluation measures will compare a set ofclusters to a set of classes.
An ideal evaluationmeasure should reward a set of clusters if the clus-ters are pure or homogeneous, so that it only con-tains sentences from one class.
On the other handit should also reward the set if all/most of the sen-tences of a class are in one cluster (completeness).If sentences that in the gold standard make up oneclass are grouped into two clusters, the measureshould penalise the clustering less than if a lot ofirrelevant sentences were in the same cluster.
Ho-mogeneity is more important to us.D is a set of N sentences daso that D = {da|a =1, ..., N}.
A set of clusters L = {lj|j = 1, ..., |L|}is a partition of a data set D into disjoint subsetscalled clusters, so that lj?
lm= ?.
|L| is the num-ber of clusters in L. A set of clusters that containsonly one cluster with all the sentences ofD will becalled Lone.
A cluster that contains only one ob-ject is called a singleton and a set of clusters thatonly consists of singletons is called Lsingle.A set of classes C = {ci|i = 1, ..., |C|} is a par-tition of a data set D into disjoint subsets calledclasses, so that ci?
cm= ?.
|C| is the number ofclasses in C. C is also called a gold standard of aclustering of data set D because this set containsthe ?ideal?
solution to a clustering task and otherclusterings are compared to it.5.1 V -measure and VbetaThe V-measure (Rosenberg and Hirschberg, 2007)is an external evaluation measure based on condi-tional entropy:V (L,C) =(1 + ?
)hc?h+ c(1)It measures homogeneity (h) and completeness (c)of a clustering solution (see equation 2 where nijis the number of sentences ljand cishare, nithenumber of sentences in ciand njthe number ofsentences in lj)h = 1?H(C|L)H(C)c = 1?H(L|C)H(L)H(C|L) = ?|L|?j=1|C|?i=1nijNlognijnjH(C) = ?|C|?i=1niNlogniNH(L) = ?|L|?j=1njNlognjNH(L|C) = ?|C|?i=1|L|?j=1nijNlognijni(2)A cluster set is homogeneous if only objects froma single class are assigned to a single cluster.
Bycalculating the conditional entropy of the class dis-tribution given the proposed clustering it can bemeasured how close the clustering is to completehomogeneity which would result in zero entropy.Because conditional entropy is constrained by thesize of the data set and the distribution of the classsizes it is normalized by H(C) (see equation 2).Completeness on the other hand is achieved if all99data points from a single class are assigned to asingle cluster which results in H(L|C) = 0.The V -measure can be weighted.
If ?
> 1the completeness is favoured over homogeneitywhereas the weight of homogeneity is increasedif ?
< 1.Vlachos et al (2009) proposes Vbetawhere ?
is setto|L||C|.
This way the shortcoming of the V-measureto favour cluster sets with many more clusters thanclasses can be avoided.
If |L| > |C| the weightof homogeneity is reduced, since clusterings withlarge |L| can reach high homogeneity quite eas-ily, whereas |C| > |L| decreases the weight ofcompleteness.
V -measure and Vbetacan range be-tween 0 and 1, they reach 1 if the set of clusters isidentical to the set of classes.5.2 Normalized Mutual InformationMutual Information (I) measures the informationthat C and L share and can be expressed by usingentropy and conditional entropy:I = H(C) +H(L)?H(C,L) (3)There are different ways to normalise I .
Manninget al (2008) usesNMI =I(L,C)H(L)+H(C)2= 2I(L,C)H(L) +H(C)(4)which represents the average of the two uncer-tainty coefficients as described in Press et al(1988).Generalise NMI to NMI?=(1+?)I?H(L)+H(C).
ThenNMI?is actually the same as V?
:h = 1?H(C|L)H(C)?
H(C)h = H(C)?H(C|L)= H(C)?H(C,L) +H(L) = Ic = 1?H(L|C)H(L)?
H(L)c = H(L)?H(L|C)= H(L)?H(L,C) +H(C) = IV =(1 + ?
)hc?h+ c=(1 + ?
)H(L)H(C)hc?H(L)H(C)h+H(L)H(C)c(5)H(C)h and H(L)c are substituted by I:(1 + ?
)I2?H(L)I +H(C)I=(1 + ?
)I?H(L) +H(C)= NMI?V1= 2IH(L) +H(C)= NMI(6)5.3 Variation of Information (V I) andNormalized V IThe V I-measure (Meila, 2007) also measurescompleteness and homogeneity using conditionalentropy.
It measure the distance between twoclusterings and thereby the amount of informationgained in changing from C to L. For this measurethe conditional entropies are added up:V I(L,C) = H(C|L) +H(L|C) (7)Remember small conditional entropies mean thatthe clustering is near to complete homogene-ity/ completeness, so the smaller V I the better(V I = 0 if L = C).
The maximum of V I islog N e.g.
for V I(Lsingle, Cone).
V I can be nor-malized, then it can range from 0 (identical clus-ters) to 1.NV I(L,C) =1log NV I(L,C) (8)V -measure, Vbetaand V I measure both com-pleteness and homogeneity, no mapping betweenclasses and clusters is needed (Rosenberg andHirschberg, 2007) and they are only dependenton the relative size of the clusters (Vlachos et al,2009).5.4 Rand Index (RI)The Rand Index (Rand, 1971) compares two clus-terings with a combinatorial approach.
Each pairof objects can fall into one of four categories:?
TP (true positives) = objects belong to oneclass and one cluster?
FP (false positives) = objects belong to dif-ferent classes but to the same cluster?
FN (false negatives) = objects belong to thesame class but to different clusters?
TN (true negatives) = objects belong to dif-ferent classes and to different clusterBy dividing the total number of correctly clusteredpairs by the number of all pairs, RI gives the per-centage of correct decisions.RI =TP + TNTP + FP + TN + FN(9)RI can range between 0 and 1 where 1 correspondsto identical clusterings.
Meila (2007) mentionsthat in practise RI concentrates in a small intervalnear 1 (for more detail see section 5.7).
Anothershortcoming is that RI gives equal weight to FPsand FNs.1005.5 Entropy and PurityEntropy and Purity are widely used evaluationmeasures (Zhao and Karypis, 2001).
They bothcan be used to measure homogeneity of a cluster.Both measures give better values when the num-ber of clusters increase, with the best result forLsingle.
Entropy ranges from 0 for identical clus-terings or Lsingleto log N e.g.
for CsingleandLone.
The values of P can range between 0 and 1,where a value close to 0 represents a bad cluster-ing solution and a perfect clustering solution getsa value of 1.Entropy =|L|?j=1njN??
?1log |C||C|?i=1nijnjlognijnj?
?Purity =1N|L|?j=1maxi(nij)(10)5.6 F -measureThe F -measure is a well known metric from IR,which is based on Recall and Precision.
The ver-sion of the F -score (Hess and Kushmerick, 2003)described here measures the overall Precision andRecall.
This way a mapping between a cluster anda class is omitted which may cause problems if |L|is considerably different to |C| or if a cluster couldbe mapped to more than one class.
Precision andRecall here are based on pairs of objects and noton individual objects.P =TPTP + FPR =TPTP + FNF (L,C) =2PRP +R(11)5.7 Discussion of the Evaluation measuresWe used one cluster set to analyse the behaviourand quality of the evaluation measures.
Variationsof that cluster set were created by randomly split-ting and merging the clusters.
These modified setswere then compared to the original set.
This ex-periment will help to identify the advantages anddisadvantages of the measures, what the values re-veal about the quality of a set of clusters and howthe measures react to changes in the cluster set.We used the set of clusters created by Judge A forthe Rushdie sentence set.
It contains 70 sentencesin 15 clusters.
This cluster set was modified bysplitting and merging the clusters randomly untilwe got Lsinglewith 70 clusters and Lonewith onecluster.
The original set of clusters (CA) was com-pared to the modified versions of the set (see figure1).
The evaluation measures reach their best val-ues if CA= 15 clusters is compared to itself.The F -measure is very sensitive to changes.
Itis the only measure which uses its full measure-ment range.
F = 0 if CAis compared toLA?single, which means that the F -measure con-siders LA?singleto be the opposite of CA.
UsuallyLoneand LA?singleare considered to be observeand a measure should only reach its worst possiblevalue if these sets are compared.
In other wordsthe F -measure might be too sensitive for our task.The RI stays most of the time in an interval be-tween 0.84 and 1.
Even for the comparison be-tween CAand LA?singlethe RI is 0.91.
This be-haviour was also described in Meila (2007) whoobserved that the RI concentrates in a small inter-val near 1.As described in section 5.5 Purity and Entropyboth measure homogeneity.
They both react tochanges slowly.
Splitting and merging have al-most the same effect on Purity.
It reaches ?
0.6when the clusters of the set were randomly split ormerged four times.
As explained above our idealevaluation measure should punish a set of clusterswhich puts sentences of the same class into twoclusters less than if sentences are merged with ir-relevant ones.
Homogeneity decreases if unrelatedclusters are merged whereas a decline in complete-ness follows from splitting clusters.
In other wordsfor our task a measure should decrease more if twoclusters are merged than if a cluster is split.Entropy for example is more sensitive to merg-ing than splitting.
But Entropy only measures ho-mogeneity and an ideal evaluation measure shouldalso consider completeness.The remaining measures Vbeta, V0.5and NV I/V Iall fulfil our criteria of a good evaluation measure.All of them are more affected by merging than bysplitting and use their measuring range appropri-ately.
V0.5favours homogeneity over complete-ness, but it reacts to changes less than Vbeta.
TheV -measure can also be inaccurate if the |L| is con-siderably different to |C|.
Vbeta(Vlachos et al,2009) tries to overcome this problem and the ten-dency of the V -measure to favour clusterings witha large number of clusters.Since V I is measured in bits with an upper boundof log N , values for different sets are difficult tocompare.
NV I tries to overcome this problem by10100.20.40.60.811 2 4 8 15 30 48 61 70 012345evaluationmeasuresVImeasurenumber of clustersVbetaV 0.5 VINVI RIF EPureFigure 1: Behaviour of evaluation measure when randomly changed sets of clusters are compared to theoriginal set.normalising V I by dividing it by log N .
As Meila(2007) pointed out, this is only convenient if thecomparison is limited to one data set.In this paper Vbeta, V0.5and NV I will be used forevaluation purposes.6 Comparability of ClusteringsFollowing our procedure and guidelines the judgeshave to filter out all irrelevant sentences that arenot related to another sentence from a differentdocument.
The number of these irrelevant sen-tences are different for every sentence set and ev-ery judge (see table 2).
The evaluation measuresrequire the same number of sentences in each setof clusters to compare them.
The easiest way toensure that each cluster set for a sentence set hasthe same number of sentences is to add the sen-tences that were filtered out by the judges to thecorresponding set of clusters.
There are differentways to add these sentences:1. singletons: Each irrelevant sentence is addedto set of clusters as a cluster of its own2.
bucket cluster: All irrelevant sentences areput into one cluster which is added to the setof clusters.Adding each irrelevant sentence as a singletonseems to be the most intuitive way to handle theproblem with the sentences that were filtered out.However this approach has some disadvantages.The judges will be rewarded disproportionatelyhigh for any singleton they agreement on.
Therebythe disagreement on the more important clusteringwill be less punished.
With every singleton thejudges agree on the completeness and homogene-ity of the whole set of clusters increases.On the other hand the sentences in a bucket clusterare not all semantically related to each other andthe cluster is not homogeneous which is contradic-tory to our definition of a cluster.
Since the irrel-evant sentences are combined to only one cluster,the judges will not be rewarded disproportionatelyhigh for their agreement.
However two bucketclusters from two different sets of clusters willnever be exactly the same and therefore the judgeswill be punished more for the disagreement on theirrelevant sentencesWe have to considers these factors when we in-terpret the results of the inter-judge agreement.7 Inter-Judge AgreementWe added the irrelevant sentences to each set ofclusters created by the judges as described in sec-tion 6.
These modified sets were then compared toeach other in order to evaluate the agreement be-tween the judges.
The results are shown in table 3.For each sentence set 100 random sets of clusterswere created and compared to the modified sets (intotal 1300 comparisons for each method of addingirrelevant sentences).
The average values of these102set judges singleton clusters bucket clusterVbetaV0.5NVI VbetaV0.5NVIVolcano A-B 0.92 0.93 0.13 0.52 0.54 0.39A-D 0.92 0.93 0.13 0.44 0.49 0.4B-D 0.95 0.95 0.08 0.48 0.48 0.31Rushdie A-B 0.87 0.88 0.19 0.3 0.31 0.59A-H 0.86 0.86 0.2 0.69 0.69 0.32B-H 0.85 0.87 0.2 0.25 0.27 0.64EgyptAir A-B 0.94 0.95 0.1 0.41 0.45 0.34A-H 0.93 0.93 0.12 0.57 0.58 0.31A-O 0.94 0.94 0.11 0.44 0.46 0.36B-H 0.93 0.94 0.11 0.44 0.46 0.3B-O 0.96 0.96 0.08 0.42 0.43 0.28H-O 0.93 0.94 0.12 0.44 0.44 0.34Schulz A-B 0.98 0.98 0.04 0.54 0.56 0.15A-J 0.89 0.9 0.17 0.39 0.4 0.34B-J 0.89 0.9 0.18 0.28 0.31 0.35base 0.66 0.75 0.44 0.29 0.28 0.68Table 3: Inter-judge agreement for the four sentence set.comparisons are used as a baseline.The inter-judge agreement is most of the timehigher than the baseline.
Only for the Rushdiesentence set the agreement between Judge B andJudge H is lower for Vbetaand V0.5if the bucketcluster method is used.As explained in section 6 the two methods foradding sentences that were filtered out by thejudges have a notable influence on the values ofthe evaluation measures.
When adding single-tons to the set of clusters the inter-judge agree-ment is considerably higher than with the bucketcluster method.
For example the agreement be-tween Judge A and Judge B is 0.98 for VbetaandV0.5and 0.04 forNV I when singletons are added.Here the judges filter out the same 185 sentenceswhich is equivalent to 74.6% of all sentences inthe set.
In other words 185 clusters are alreadyconsidered to be homogen and complete, whichgives the comparison a high score.
Five of the 15clusters Judge A created contain only sentencesthere were marked as irrelevant by Judge B.
In to-tal 25 sentences are used in clusters by Judge Awhich are singletons in Judge B?s set.
Judge B in-cluded nine other sentences that are singletons inthe set of Judge A.
Four of the clusters are exactlythe same in both sets, they contain 16 sentences.To get from Judge A?s set to the set of Judge B37 sentences would have to be deleted, added ormoved.With the bucket cluster method Judge A andJudge H for the Rushdie sentence set have the bestinter-judge agreement.
At the same time this com-bination receives the worst V0.5and NV I val-ues with the singleton method.
The two judgesagree on 22 irrelevant sentences, which accountfor 21.35% of all sentences.
Here the singletonshave far less influence on the evaluation measuresthen the first example.
Judge A includes 7 sen-tences that are filtered out by Judge H who usesanother 11 sentences.
Only one cluster is exactlythe same in both sets.
To get from Judge A?s set toJudge H?s cluster 11 sentences have to be deleted,7 to be added, one cluster has to be split in two and11 sentences have to be moved from one cluster toanother.Although the two methods of adding irrelevantsentences to the sets of cluster result in differ-ent values for the inter-judge agreement, we canconclude that the agreement between the judgesis good and (almost) always exceed the baseline.Overall Judge B seems to have the highest agree-ment throughout all sentence sets with all otherjudges.8 Conclusion and Future WorkIn this paper we presented a gold standard for sen-tence clustering for Multi-Document Summariza-tion.
The data set used, the guidelines and pro-cedure given to the judges were discussed.
Weshowed that the agreement between the judges insentence clustering is good and exceeds the base-line.
This gold standard will be used for further ex-periments on clustering for Multi-Document Sum-marization.
The next step will be to compared theoutput of a standard clustering algorithm to thegold standard.103ReferencesRamiz M. Aliguliyev.
2006.
A novel partitioning-based clustering method and generic document sum-marization.
In WI-IATW ?06: Proceedings of the2006 IEEE/WIC/ACM international conference onWeb Intelligence and Intelligent Agent Technology,Washington, DC, USA.Regina Barzilay and Kathleen R. McKeown.
2005.Sentence Fusion for Multidocument News Sum-mariation.
Computational Linguistics, 31(3):297?327.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The Second Release of the RASP System.
In COL-ING/ACL 2006 Interactive Presentation Sessions,Sydney, Australien.
The Association for ComputerLinguistics.Vasileios Hatzivassiloglou, Judith L. Klavans,Melissa L. Holcombe, Regina Barzilay, Min-Yen Kan, and Kathleen R. McKeown.
2001.SIMFINDER: A Flexible Clustering Tool forSummarization.
In NAACL Workshop on AutomaticSummarization, pages 41?49.
Association forComputational Linguistics.Andreas Hess and Nicholas Kushmerick.
2003.
Au-tomatically attaching semantic metadata to web ser-vices.
In Proceedings of the 2nd International Se-mantic Web Conference (ISWC 2003), Florida, USA.Christopher D. Manning, Prabhakar Raghavan, andHeinrich Sch?utze.
2008.
Introduction to Informa-tion Retrieval.
Cambridge University Press.Daniel Marcu and Laurie Gerber.
2001.
An inquiryinto the nature of multidocument abstracts, extracts,and their evaluation.
In Proceedings of the NAACL-2001 Workshop on Automatic Summarization, Pitts-burgh, PA.Marina Meila.
2007.
Comparing clusterings?an in-formation based distance.
Journal of MultivariateAnalysis, 98(5):873?895.Martina Naughton.
2007.
Exploiting structure forevent discovery using the mdi algorithm.
In Pro-ceedings of the ACL 2007 Student Research Work-shop, pages 31?36, Prague, Czech Republic, June.Association for Computational Linguistics.William H. Press, Brian P. Flannery, Saul A. Teukol-sky, and William T. Vetterling.
1988.
NumericalRecipies in C: The art of Scientific Programming.Cambridge University Press, Cambridge, England.Dragomir R. Radev, Hongyan Jing, and MalgorzataBudzikowska.
2000.
Centroid-based summariza-tion of multiple documents: sentence extraction,utility-based evaluation, and user studies.
In InANLP/NAACL Workshop on Summarization, pages21?29, Morristown, NJ, USA.
Association for Com-putational Linguistics.William M. Rand.
1971.
Objective criteria for the eval-uation of clustering methods.
American StatisticalAssociation Journal, 66(336):846?850.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clus-ter evaluation measure.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 410?420.Andreas Vlachos, Anna Korhonen, and ZoubinGhahramani.
2009.
Unsupervised and ConstrainedDirichlet Process Mixture Models for Verb Cluster-ing.
In Proceedings of the EACL workshop on GEo-metrical Models of Natural Language Semantics.Dingding Wang, Tao Li, Shenghuo Zhu, and ChrisDing.
2008.
Multi-document summarization viasentence-level semantic analysis and symmetric ma-trix factorization.
In SIGIR ?08: Proceedings of the31st annual international ACM SIGIR conference onResearch and development in information retrieval,pages 307?314, New York, NY, USA.
ACM.Hongyuan Zha.
2002.
Generic Summarization andKeyphrase Extraction using Mutual ReinforcementPrinciple and Sentence Clustering.
In Proceedingsof the 25th Annual ACM SIGIR Conference, pages113?120, Tampere, Finland.Ying Zhao and George Karypis.
2001.
Criterionfunctions for document clustering: Experiments andanalysis.
Technical report, Department of ComputerScience, University of Minnesota.
(Technical Re-port #01-40).104
