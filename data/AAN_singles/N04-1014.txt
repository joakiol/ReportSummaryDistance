Training Tree TransducersJonathan GraehlInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292graehl@isi.eduKevin KnightInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292knight@isi.eduAbstractMany probabilistic models for natural languageare now written in terms of hierarchical treestructure.
Tree-based modeling still lacks manyof the standard tools taken for granted in (finite-state) string-based modeling.
The theory of treetransducer automata provides a possible frame-work to draw on, as it has been worked out in anextensive literature.
We motivate the use of treetransducers for natural language and addressthe training problem for probabilistic tree-to-tree and tree-to-string transducers.1 IntroductionMuch of natural language work over the past decade hasemployed probabilistic finite-state transducers (FSTs)operating on strings.
This has occurred somewhat underthe influence of speech recognition, where transducingacoustic sequences to word sequences is neatly capturedby left-to-right stateful substitution.
Many conceptualtools exist, such as Viterbi decoding (Viterbi, 1967) andforward-backward training (Baum and Eagon, 1967), aswell as generic software toolkits.
Moreover, a surprisingvariety of problems are attackable with FSTs, from part-of-speech tagging to letter-to-sound conversion to nametransliteration.However, language problems like machine transla-tion break this mold, because they involve massive re-ordering of symbols, and because the transformation pro-cesses seem sensitive to hierarchical tree structure.
Re-cently, specific probabilistic tree-based models have beenproposed not only for machine translation (Wu, 1997;Alshawi, Bangalore, and Douglas, 2000; Yamada andKnight, 2001; Gildea, 2003; Eisner, 2003), but also forThis work was supported by DARPA contract F49620-00-1-0337 and ARDA contract MDA904-02-C-0450.summarization (Knight and Marcu, 2002), paraphras-ing (Pang, Knight, and Marcu, 2003), natural languagegeneration (Langkilde and Knight, 1998; Bangalore andRambow, 2000; Corston-Oliver et al, 2002), and lan-guage modeling (Baker, 1979; Lari and Young, 1990;Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001;Klein and Manning, 2003).
It is useful to understandgeneric algorithms that may support all these tasks andmore.
(Rounds, 1970) and (Thatcher, 1970) independentlyintroduced tree transducers as a generalization of FSTs.Rounds was motivated by natural language.
The Roundstree transducer is very similar to a left-to-right FST, ex-cept that it works top-down, pursuing subtrees in paral-lel, with each subtree transformed depending only on itsown passed-down state.
This class of transducer is oftennowadays called R, for ?Root-to-frontier?
(G?cseg andSteinby, 1984).Rounds uses a mathematics-oriented example of an Rtransducer, which we summarize in Figure 1.
At eachpoint in the top-down traversal, the transducer choosesa production to apply, based only on the current stateand the current root symbol.
The traversal continuesuntil there are no more state-annotated nodes.
Non-deterministic transducers may have several productionswith the same left-hand side, and therefore some freechoices to make during transduction.An R transducer compactly represents a potentially-infinite set of input/output tree pairs: exactly those pairs(T1, T2) for which some sequence of productions appliedto T1 (starting in the initial state) results in T2.
This issimilar to an FST, which compactly represents a set ofinput/output string pairs, and in fact, R is a generalizationof FST.
If we think of strings written down vertically, asdegenerate trees, we can convert any FST into an R trans-ducer by automatically replacing FST transitions with Rproductions.R does have some extra power beyond path followingFigure 1: A sample R tree transducer that takes thederivative of its input.and state-based record keeping.
It can copy whole sub-trees, and transform those subtrees differently.
It can alsodelete subtrees without inspecting them (imagine by anal-ogy an FST that quits and accepts right in the middle ofan input string).
Variants of R that disallow copying anddeleting are called RL (for linear) and RN (for nondelet-ing), respectively.One advantage of working with tree transducers is thelarge and useful body of literature about these automata;two excellent surveys are (G?cseg and Steinby, 1984) and(Comon et al, 1997).
For example, R is not closed undercomposition (Rounds, 1970), and neither are RL or F (the?frontier-to-root?
cousin of R), but the non-copying FLis closed under composition.
Many of these compositionresults are first found in (Engelfriet, 1975).R has surprising ability to change the structure of aninput tree.
For example, it may not be initially obvioushow an R transducer can transform the English structureS(PRO, VP(V, NP)) into the Arabic equivalent S(V, PRO,NP), as it is difficult to move the subject PRO into posi-tion between the verb V and the direct object NP.
First, Rproductions have no lookahead capability?the left-hand-side of the S production consists only of q S(x0, x1), al-though we want our English-to-Arabic transformation toapply only when it faces the entire structure q S(PRO,VP(V, NP)).
However, we can simulate lookahead usingstates, as in these productions:- q S(x0, x1) ?
S(qpro x0, qvp.v.np x1)- qpro PRO ?
PRO- qvp.v.np VP(x0, x1) ?
VP(qv x0, qnp x1)By omitting rules like qpro NP?
..., we ensure that theentire production sequence will dead-end unless the firstchild of the input tree is in fact PRO.
So finite lookaheadis not a problem.
The next problem is how to get the PROto appear in between the V and NP, as in Arabic.
This canbe carried out using copying.
We make two copies of theEnglish VP, and assign them different states:- q S(x0,x1) ?
S(qleft.vp.v x1, qpro x0,qright.vp.np x1)- qpro PRO ?
PRO- qleft.vp.v VP(x0, x1) ?
qv x0- qright.vp.np VP(x0, x1) ?
qnp x1While general properties of R are understood, thereare many algorithmic questions.
In this paper, we takeon the problem of training probabilistic R transducers.For many language problems (machine translation, para-phrasing, text compression, etc.
), it is possible to collecttraining data in the form of tree pairs and to distill lin-guistic knowledge automatically.Our problem statement is: Given (1) a particulartransducer with productions P, and (2) a finite training setof sample input/output tree pairs, we want to produce (3)a probability estimate for each production in P such thatwe maximize the probability of the output trees given theinput trees.As organized in the rest of this paper, we accomplishthis by intersecting the given transducer with each in-put/output pair in turn.
Each such intersection produces aset of weighted derivations that are packed into a regulartree grammar (Sections 3-5), which is equivalent to a treesubstitution grammar.
The inside and outside probabili-ties of this packed derivation structure are used to com-pute expected counts of the productions from the original,given transducer (Sections 6-7).
Section 9 gives a sampletransducer implementing a published machine translationmodel; some readers may wish to skip to this section di-rectly.2 TreesT?
is the set of (rooted, ordered, labeled, finite) trees overalphabet ?.
An alphabet is just a finite set.T?
(X) are the trees over alphabet ?, indexed by X?the subset of T?
?X where only leaves may be labeled byX .
(T?(?)
= T?.)
Leaves are nodes with no children.The nodes of a tree t are identified one-to-one with itspaths: pathst ?
paths ?
N?
??
?i=0 Ni (A0 ?
{()}).The path to the root is the empty sequence (), and p1extended by p2 is p1 ?
p2, where ?
is concatenation.For p ?
pathst, rankt(p) is the number of chil-dren, or rank, of the node at p in t, and labelt(p) ??
?
X is its label.
The ranked label of a node is thepair labelandrankt(p) ?
(labelt(p), rankt(p)).
For1 ?
i ?
rankt(p), the ith child of the node at p islocated at path p ?
(i).
The subtree at path p of t ist ?
p, defined by pathst?p ?
{q | p ?
q ?
pathst} andlabelandrankt?p(q) ?
labelandrankt(p ?
q).The paths to X in t are pathst(X) ?
{p ?pathst | labelt(p) ?
X}.
A frontier is a set of pathsf that are pairwise prefix-independent:?p1, p2 ?
f, p ?
paths : p1 = p2 ?
p =?
p1 = p2A frontier of t is a frontier f ?
pathst.For t, s ?
T?
(X), p ?
pathst, t[p?
s] is the substitu-tion of s for p in t, where the subtree at path p is replacedby s. For a frontier f of t, the mass substitution of Xfor the frontier f in t is written t[p ?
X, ?p ?
f ] andis equivalent to substituting the X(p) for the p serially inany order.Trees may be written as strings over ?
?
{(, )}in the usual way.
For example, the tree t =S(NP,VP(V,NP)) has labelandrankt((2)) = (VP, 2)and labelandrankt((2, 1)) = (V, 0).
For t ?
T?, ?
?
?,?
(t) is the tree whose root has label ?
and whose singlechild is t.The yield of X in t is yieldt(X), the string formed byreading out the leaves labeled with X in left-to-right or-der.
The usual case (the yield of t) is yieldt ?
yieldt(?).?
= {S, NP, VP, PP, PREP, DET, N, V, run, the, of, sons,daughters}N = {qnp, qpp, qdet, qn, qprep}S = qP = {q?1.0 S(qnp, VP(V(run))),qnp?0.6 NP(qdet, qn),qnp?0.4 NP(qnp, qpp),qpp?1.0 PP(qprep, qnp),qdet?1.0 DET(the),qprep?1.0 PREP(of),qn?0.5 N(sons),qn?0.5 N(daughters)}Figure 2: A sample weighted regular tree grammar(wRTG)3 Regular Tree GrammarsIn this section, we describe the regular tree grammar, acommon way of compactly representing a potentially in-finite set of trees (similar to the role played by the finite-state acceptor FSA for strings).
We describe the version(equivalent to TSG (Schabes, 1990)) where the generatedtrees are given weights, as are strings in a WFSA.A weighted regular tree grammar (wRTG) G is aquadruple (?, N, S, P ), where ?
is the alphabet, N isthe finite set of nonterminals, S ?
N is the start (or ini-tial) nonterminal, and P ?
N?T?
(N)?R+ is the finiteset of weighted productions (R+ ?
{r ?
R | r > 0}).
Aproduction (lhs, rhs, w) is written lhs?w rhs.
Produc-tions whose rhs contains no nonterminals (rhs ?
T?
)are called terminal productions, and rules of the formA ?w B, for A,B ?
N are called ?-productions, orepsilon productions, and can be used in lieu of multipleinitial nonterminals.Figure 2 shows a sample wRTG.
This grammar ac-cepts an infinite number of trees.
The tree S(NP(DT(the),N(sons)), VP(V(run))) comes out with probability 0.3.We define the binary relation?G (single-step derivesin G) on T?(N)?
(paths?P )?, pairs of trees and deriva-tion histories, which are logs of (location, productionused):?G?
{((a, h), (b, h ?
(p, (l, r, w)))??
(l, r, w) ?
P ?
p ?
pathsa({l}) ?
b = a[p?
r]}where (a, h)?G (b, h ?
(p, (l, r, w))) iff tree b may bederived from tree a by using the rule l ?w r to replacethe nonterminal leaf l at path p with r. For a derivationhistory h = ((p1, (l1, r1, w1)), .
.
.
, (pn, (l1, r1, w1))),the weight of h is w(h) ?
?ni=1 wi, and call h leftmost ifL(h) ?
?1 ?
i < n : pi+1 ?lex pi.11() <lex (a), (a1) <lex (a2) iff a1 < a2, (a1) ?
b1 <lex(a2) ?
b2 iff a1 < a2 ?
(a1 = a2 ?
b1 <lex b2)The reflexive, transitive closure of?G is written?
?G(derives in G), and the restriction of ?
?G to leftmostderivation histories is?L?G (leftmost derives in G).The weight of a becoming b in G is wG(a, b) ?
?h:(a,())?L?G (b,h)w(h), the sum of weights of all unique(leftmost) derivations transforming a to b, and the weightof t in G is WG(t) = wG(S, t).
The weighted regu-lar tree language produced by G is LG ?
{(t, w) ?T?
?
R+ |WG(t) = w}.For every weighted context-free grammar, there is anequivalent wRTG that produces its weighted derivationtrees with yields being the string produced, and the yieldsof regular tree grammars are context free string languages(G?cseg and Steinby, 1984).What is sometimes called a forest in natural languagegeneration (Langkilde, 2000; Nederhof and Satta, 2002)is a finite wRTG without loops, i.e., ?n ?
N(n, ()) ?
?G(t, h) =?
pathst({n}) = ?.
Regular tree languagesare strictly contained in tree sets of tree adjoining gram-mars (Joshi and Schabes, 1997).4 Extended-LHS Tree Transducers (xR)Section 1 informally described the root-to-frontier trans-ducer class R. We saw that R allows, by use of states,finite lookahead and arbitrary rearrangement of non-sibling input subtrees removed by a finite distance.
How-ever, it is often easier to write rules that explicitly repre-sent such lookahead and movement, relieving the burdenon the user to produce the requisite intermediary rulesand states.
We define xR, a convenience-oriented gener-alization of weighted R. Because of its good fit to natu-ral language problems, xR is already briefly touched on,though not defined, in (Rounds, 1970).A weighted extended-lhs root-to-frontier tree trans-ducer X is a quintuple (?,?, Q,Qi, R) where ?
is theinput alphabet, and ?
is the output alphabet, Q is a fi-nite set of states, Qi ?
Q is the initial (or start, or root)state, and R ?
Q ?
XRPAT?
?
T?
(Q ?
paths) ?
R+is a finite set of weighted transformation rules, written(q, pattern) ?w rhs, meaning that an input subtreematching pattern while in state q is transformed intorhs, with Q?
paths leaves replaced by their (recursive)transformations.
The Q?paths leaves of a rhs are callednonterminals (there may also be terminal leaves la-beled by the output tree alphabet ?).XRPAT?
is the set of finite tree patterns: predicatefunctions f : T?
?
{0, 1} that depend only on the la-bel and rank of a finite number of fixed paths their in-put.
xR is the set of all such transducers.
R, the setof conventional top-down transducers, is a subset of xRwhere the rules are restricted to use finite tree patternsthat depend only on the root: RPAT?
?
{p?,r(t)} wherep?,r(t) ?
(labelt(()) = ?
?
rankt(()) = r).Rules whose rhs are a pure T?
with no states/pathsfor further expansion are called terminal rules.
Rulesof the form (q, pat) ?w (q?, ()) are ?-rules, or epsilonrules, which substitute state q?
for state q without produc-ing output, and stay at the current input subtree.
Multipleinitial states are not needed: we can use a single startstate Qi, and instead of each initial state q with startingweight w add the rule (Qi,TRUE) ?w (q, ()) (whereTRUE(t) ?
1, ?t).We define the binary relation?X for xR tranducer Xon T????Q?
(paths?R)?, pairs of partially transformed(working) trees and derivation histories:?X?
{((a, h), (b, h ?
(i, (q, pat, r, w))))??
(q, pat, r, w) ?
R ?
i ?
pathsa ?q = labela(i) ?
pat(a ?
(i ?
(1))) = 1 ?b = a[i?
r[p?
q?
(a ?
(i ?
(1) ?
i?
)),?p : labelr(p) = (q?, i?
)]]}That is, b is derived from a by application of a rule(q, pat) ?w r to an unprocessed input subtree a ?
iwhich is in state q, replacing it by output given by r, withits nonterminals replaced by the instruction to transformdescendant input subtrees at relative path i?
in state q?.The sources of a rule r = (q, l, rhs, w) ?
R are the input-path parts of the rhs nonterminals:sources(rhs) ??i???
?p ?
pathsrhs(Q?
paths),q?
?
Q : labelrhs(p) = (q?, i?
)?If the sources of a rule refer to input paths that do notexist in the input, then the rule cannot apply (becausea ?
(i ?
(1) ?
i?)
would not exist).
In the traditional state-ment of R, sources(rhs) is always {(1), .
.
.
, (n)}, writ-ing xi instead of (i), but in xR, we identify mapped inputsubtrees by arbitrary (finite) paths.An input tree is transformed by starting at the rootin the initial state, and recursively applying output-generating rules to a frontier of (copies of) input subtrees(each marked with their own state), until (in a completederivation, finishing at the leaves with terminal rules) nostates remain.Let ?
?X , ?L?X , and wX(a, b) follow from ?X ex-actly as in Section 3.
Then the weight of (i, o) in Xis WX(i, o) ?
wX(Qi(i), o).
The weighted tree trans-duction given by X is XX ?
{(i, o, w) ?
T?
?
T?
?R+|WX(i, o) = w}.5 Parsing a Tree TransductionDerivation trees for a transducer X = (?,?, Q,Qi, R)are trees labeled by rules (R) that dictate the choice ofrules in a complete X-derivation.
Figure 3 shows deriva-tion trees for a particular transducer.
In order to generateFigure 3: Derivation trees for an R tree transducer.derivation trees for X automatically, we build a modifiedtransducer X ?.
This new transducer produces derivationtrees on its output instead of normal output trees.
X ?
is(?, R,Q,Qi, R?
), withR?
?
{(q, pattern, rule(yieldrhs(Q?
paths)), w) |rule = (q, pattern, rhs, w) ?
R}That is, the original rhs of rules are flattened into atree of depth 1, with the root labeled by the original rule,and all the non-expanding ?-labeled nodes of the rhs re-moved, so that the remaining children are the nonterminalyield in left to right order.
Derivation trees deterministi-cally produce a single weighted output tree.The derived transducer X ?
nicely produces derivationtrees for a given input, but in explaining an observed(input/output) pair, we must restrict the possibilities fur-ther.
Because the transformations of an input subtreedepend only on that subtree and its state, we can (Al-gorithm 1) build a compact wRTG that produces ex-actly the weighted derivation trees corresponding to X-transductions (I, ()) ?
?X (O, h) (with weight equal towX(h)).6 Inside-Outside for wRTGGiven a wRTG G = (?, N, S, P ), we can computethe sums of weights of trees derived using each produc-tion by adapting the well-known inside-outside algorithmfor weighted context-free (string) grammars (Lari andYoung, 1990).The inside weights using G are given by ?G : T?
?(R?R?
), giving the sum of weights of all tree-producingderivatons from trees with nonterminal leaves:?G(t) ?????????
(t,r,w)?Pw ?
?G(r) if t ?
N?p?pathst(N)?G(labelt(p)) otherwiseBy definition, ?G(S) gives the sum of the weights ofall trees generated by G. For the wRTG generated byDERIV(X, I,O), this is exactly WX(I,O).Outside weights ?G for a nonterminal are the sums ofweights of trees generated by the wRTG that have deriva-tions containing it, but excluding its inside weights (thatis, the weights summed do not include the weights ofrules used to expand an instance of it).
?G(n ?
N) ?
1 if n = S, else:uses of n in productionsz }| {Xp,(n?,r,w)?P :labelr(p)=nw ?
?G(n?)
?Yp??pathsr(N)?{p}?G(labelr(p?
))| {z }sibling nonterminalsAlgorithm 1: DERIVInput: xR transducer X = (?,?, Q,Qi, R) and ob-served tree pair I ?
T?, O ?
T?.Output: derivation wRTG G = (R,N ?
Q?
pathsI ?pathsO, S, P ) generating all weighted deriva-tion trees for X that produce O from I .
Returnsfalse instead if there are no such trees.beginS ?
(Qi, (), ()), N ?
?, P ?
?if PRODUCEI,O(S) thenreturn (R,N, S, P )elsereturn falseendmemoized PRODUCEI,O(q, i, o) returns boolean?beginanyrule??
falsefor r = (q, pattern, rhs, w) ?
R : pattern(I ?
i) =1 ?MATCHO,?
(rhs, o) do(o1, .
.
.
, on)?
pathsrhs(Q?
paths) sorted byo1 <lex .
.
.
<lex on//n = 0 if there are nonelabelandrankderivrhs(())?
(r, n)for j ?
1 to n do(q?, i?)?
labelrhs(oj)c?
(q?, i ?
i?, o ?
oi)if ?PRODUCEI,O(c) then next rlabelandrankderivrhs((j))?
(c, 0)anyrule??
trueP ?
P ?
{((q, i, o), derivrhs, w)}if anyrule?
then N ?
N ?
{(q, i, o)}return anyrule?endMATCHt,?
(t?, p) ?
?p?
?
path(t?)
: label(t?, p?)
??
=?
labelandrankt?(p?)
= labelandrankt(p ?
p?
)The possible derivations for a givenPRODUCEI,O(q, i, o) are constant and need not becomputed more than once, so the function is memoized.We have in the worst case to visit all |Q| ?
|I| ?
|O|(q, i, o) pairs and have all |R| transducer rules match ateach of them.
If enumerating rules matching transducerinput-patterns and output-subtrees has cost L (constantgiven a transducer), then DERIV has time complexityO(L ?
|Q| ?
|I| ?
|O| ?
|R|).Finally, given inside and outside weights, the sumof weights of trees using a particular production is?G((n, r, w) ?
P ) ?
?G(n) ?
w ?
?G(r).Computing ?G and ?G for nonrecursive wRTG is astraightforward translation of the above recursive defi-nitions (using memoization to compute each result onlyonce) and is O(|G|) in time and space.7 EM TrainingEstimation-Maximization training (Dempster, Laird, andRubin, 1977) works on the principle that the corpus like-lihood can be maximized subject to some normalizationconstraint on the parameters by repeatedly (1) estimatingthe expectation of decisions taken for all possible ways ofgenerating the training corpus given the current parame-ters, accumulating parameter counts, and (2) maximizingby assigning the counts to the parameters and renormal-izing.
Each iteration is guaranteed to increase the like-lihood until a local maximum is reached.Algorithm 2 implements EM xR training, repeatedlycomputing inside-outside weights (using fixed transducerderivation wRTGs for each input/output tree pair) to ef-ficiently sum each parameter contribution to likelihoodover all derivations.
Each EM iteration takes time linearin the size of the transducer and linear in the size of thederivation tree grammars for the training examples.
Thesize of the derivation trees is at worst O(|Q|?|I|?|O|?|R|).For a corpus of K examples with average input/outputsize M , an iteration takes (at worst) O(|Q| ?
|R| ?K ?M2)time?quadratic, like the forward-backward algorithm.8 Tree-to-String Transducers (xRS)We now turn to tree-to-string transducers (xRS).
In theautomata literature, these were first called generalizedsyntax-directed translations (Aho and Ullman, 1971) andused to specify compilers.
Tree-to-string transducershave also been applied to machine translation (Yamadaand Knight, 2001; Eisner, 2003).We give an explicit tree-to-string transducer examplein the next section.
Formally, a weighted extended-lhsroot-to-frontier tree-to-string transducer X is a quintuple(?,?, Q,Qi, R) where ?
is the input alphabet, and ?is the output alphabet, Q is a finite set of states, Qi ?Q is the initial (or start, or root) state, and R ?
Q ?XRPAT??
(??
(Q?paths))?
?R+ are a finite set ofweighted transformation rules, written (q, pattern) ?wrhs.
A rule says that to transform (with weight w) aninput subtree matching pattern while in state q, replaceit by the string of rhs with its nonterminal (Q ?
paths)letters replaced by their (recursive) transformation.xRS is the same as xR, except that the rhs are stringscontaining some nonterminals instead of trees containingnonterminal leaves (so the intermediate derivation objectsAlgorithm 2: TRAINInput: xR transducer X = (?,?, Q,Qd, R), observedweighted tree pairs T ?
T?
?
T?
?
R+, normal-ization function Z({countr | r ?
R}, r?
?
R),minimum relative log-likelihood change for con-vergence ?
?
R+, maximum number of iterationsmaxit ?
N, and prior counts (for a so-calledDirichlet prior) {priorr | r ?
R} for smoothingeach rule.Output: New rule weights W ?
{wr | r ?
R}.beginfor (i, o, w) ?
T dodi,o ?DERIV(X, i, o)//Alg.
1if di,o = false thenT ?
T ?
{(i, o, w)}warn(more rules are needed to explain (i,o))compute inside/outside weights for di,o andremove all useless nonterminals n whose?di,o(n) = 0 or ?di,o(n) = 0itno?
0, lastL?
?
?, ?
?
?for r = (q, pat, rhs, w) ?
R do wr ?
wwhile ?
?
?
?
itno < maxit dofor r ?
R do countr ?
priorrL?
0for (i, o, wexample) ?
T//Estimatedolet D ?
di,o ?
(R,N, S, P )compute ?D, ?D using latestW ?
{wr | r ?
R}//see Section 6for prod = (n, rhs, w) ?
P do?D(prod)?
?D(n) ?
w ?
?D(rhs)let rule ?
labelrhs(())countrule ?
countrule+wexample ?
?D(prod)?D(S)L?
L + log ?D(S) ?
wexamplefor r = (q, pattern, rhs, w) ?
R//Maximizedowr ?countrZ({countr|r ?
R}, r)//e.g.Z((q, a, b, c)) ??r=(q,d,e,f)?Rcountr?
?
L?
lastL|L|lastL?
L, itno?
itno+ 1endare strings containing state-marked input subtrees).
Wehave developed an xRS training procedure similar to thexR procedure, with extra computational expense to con-sider how different productions might map to differentspans of the output string.
Space limitations prohibit adetailed description; we refer the reader to a longer ver-sion of this paper (submitted).
We note that this algo-rithm subsumes normal inside-outside training of PCFGon strings (Lari and Young, 1990), since we can alwaysfix the input tree to some constant for all training exam-ples.9 ExampleIt is possible to cast many current probabilistic naturallanguage models as R-type tree transducers.
In this sec-tion, we implement the translation model of (Yamadaand Knight, 2001).
Their generative model providesa formula for P(Japanese string | English tree), in termsof individual parameters, and their appendix gives spe-cial EM re-estimation formulae for maximizing the prod-uct of these conditional probabilities across the wholetree/string corpus.We now build a trainable xRS tree-to-string transducerthat embodies the same P(Japanese string | English tree).First, we need start productions like these, where q is thestart state:- q x:S ?
q.TOP.S x- q x:VP ?
q.TOP.VP xThese set up states like q.TOP.S, which means ?translatethis tree, whose root is S.?
Then every q.parent.child pairgets its own set of three insert-function-word productions,e.g.
:- q.TOP.S x ?
i x, r x- q.TOP.S x ?
r x, i x- q.TOP.S x ?
r x- q.NP.NN x ?
i x, r x- q.NP.NN x ?
r x, i x- q.NP.NN x ?
r xState i means ?produce a Japanese function word out ofthin air.?
We include an i production for every Japaneseword in the vocabulary, e.g.
:- i x ?
de- i x ?
kuruma- i x ?
waState r means ?re-order my children and then recurse.
?For internal nodes, we include a production for ev-ery parent/child-sequence and every permutation thereof,e.g.
:- r NP(x0:CD, x1:NN) ?
q.NP.CD x0, q.NP.NN x1- r NP(x0:CD, x1:NN) ?
q.NP.NN x1, q.NP.CD x0The rhs sends the child subtrees back to state q for re-cursive processing.
However, for English leaf nodes, weinstead transition to a different state t, so as to prohibitany subsequent Japanese function word insertion:- r NN(x0:car) ?
t x0- r CC(x0:and) ?
t x0State t means ?translate this word,?
and we have a produc-tion for every pair of co-occurring English and Japanesewords:- t car ?
kuruma- t car ?
wa- t car ?
*e*This follows (Yamada and Knight, 2001) in also allowingEnglish words to disappear, or translate to epsilon.Every production in the xRS transducer has an associ-ated weight and corresponds to exactly one of the modelparameters.There are several benefits to this xRS formulation.First, it clarifies the model, in the same way that (Knightand Al-Onaizan, 1998; Kumar and Byrne, 2003) eluci-date other machine translation models in easily-graspedFST terms.
Second, the model can be trained withgeneric, off-the-shelf tools?versus the alternative ofworking out model-specific re-estimation formulae andimplementing custom training software.
Third, we caneasily extend the model in interesting ways.
For exam-ple, we can add productions for multi-level and lexicalre-ordering:- r NP(x0:NP, PP(IN(of), x1:NP)) ?
q x1, no, q x0We can add productions for phrasal translations:- r NP(JJ(big), NN(cars)) ?
ooki, kurumaThis can now include crucial non-constituent phrasaltranslations:- r S(NP(PRO(there),VP(VB(are), x0:NP) ?
q x0, ga,arimasuWe can also eliminate many epsilon word-translationrules in favor of more syntactically-controlled ones, e.g.
:- r NP(DT(the),x0:NN) ?
q x0We can make many such changes without modifying thetraining procedure, as long as we stick to tree automata.10 Related WorkTree substitution grammars or TSG (Schabes, 1990)are equivalent to regular tree grammars.
xR transduc-ers are similar to (weighted) Synchronous TSG, exceptthat xR can copy input trees (and transform the copiesdifferently), but does not model deleted input subtrees.
(Eisner, 2003) discusses training for Synchronous TSG.Our training algorithm is a generalization of forward-backward EM training for finite-state (string) transducers,which is in turn a generalization of the original forward-backward algorithm for Hidden Markov Models.11 AcknowledgmentsThanks to Daniel Gildea and Kenji Yamada for commentson a draft of this paper, and to David McAllester for help-ing us connect into previous work in automata theory.ReferencesAho, A. V. and J. D. Ullman.
1971.
Translations of a context-free grammar.Information and Control, 19:439?475.Alshawi, Hiyan, Srinivas Bangalore, and Shona Douglas.
2000.
Learning de-pendency translation models as collections of finite state head transducers.Computational Linguistics, 26(1):45?60.Baker, J. K. 1979.
Trainable grammars for speech recognition.
In D. Klatt andJ.
Wolf, editors, Speech Communication Papers for the 97th Meeting of theAcoustical Society of America.
Boston, MA, pages 547?550.Bangalore, Srinivas and Owen Rambow.
2000.
Exploiting a probabilistic hierar-chical model for generation.
In Proc.
COLING.Baum, L. E. and J.
A. Eagon.
1967.
An inequality with application to statisticalestimation for probabilistic functions of Markov processes and to a model forecology.
Bulletin of the American Mathematicians Society, 73:360?363.Charniak, Eugene.
2001.
Immediate-head parsing for language models.
In Proc.ACL.Chelba, C. and F. Jelinek.
2000.
Structured language modeling.
ComputerSpeech and Language, 14(4):283?332.Collins, Michael.
1997.
Three generative, lexicalised models for statistical pars-ing.
In Proc.
ACL.Comon, H., M. Dauchet, R. Gilleron, F. Jacquemard, D. Lugiez, S. Tison, andM.
Tommasi.
1997.
Tree automata techniques and applications.
Available onwww.grappa.univ-lille3.fr/tata.
release October, 1st 2002.Corston-Oliver, Simon, Michael Gamon, Eric K. Ringger, and Robert Moore.2002.
An overview of Amalgam, a machine-learned generation module.
InProc.
IWNLG.Dempster, A. P., N. M. Laird, and D. B. Rubin.
1977.
Maximum likelihood fromincomplete data via the EM algorithm.
Journal of the Royal Statistical Society,Series B, 39(1):1?38.Eisner, Jason.
2003.
Learning non-isomorphic tree mappings for machine trans-lation.
In Proc.
ACL (companion volume).Engelfriet, J.
1975.
Bottom-up and top-down tree transformations?a compari-son.
Math.
Systems Theory, 9(3):198?231.G?cseg, F. and M. Steinby.
1984.
Tree Automata.
Akad?miai Kiad?, Budapest.Gildea, Daniel.
2003.
Loosely tree-based alignment for machine translation.
InProc.
ACL.Joshi, A. and Y. Schabes.
1997.
Tree-adjoining grammars.
In G. Rozenberg andA.
Salomaa, editors, Handbook of Formal Languages (Vol.
3).
Springer, NY.Klein, Dan and Christopher D. Manning.
2003.
Accurate unlexicalized parsing.In Proc.
ACL.Knight, K. and Y. Al-Onaizan.
1998.
Translation with finite-state devices.
InProc.
AMTA.Knight, K. and D. Marcu.
2002.
Summarization beyond sentence extraction?a probabilistic approach to sentence compression.
Artificial Intelligence,139(1).Kumar, S. and W. Byrne.
2003.
A weighted finite state transducer implemen-tation of the alignment template model for statistical machine translation.
InProceedings of HLT-NAACL.Langkilde, I.
2000.
Forest-based statistical sentence generation.
In Proc.
NAACL.Langkilde, I. and K. Knight.
1998.
Generation that exploits corpus-based statisti-cal knowledge.
In Proc.
ACL.Lari, K. and S. J.
Young.
1990.
The estimation of stochastic context-free gram-mars using the inside-outside algorithm.
Computer Speech and Language, 4.Nederhof, Mark-Jan and Giorgio Satta.
2002.
Parsing non-recursive CFGs.
InProc.
ACL.Pang, Bo, Kevin Knight, and Daniel Marcu.
2003.
Syntax-based alignment ofmultiple translations extracting paraphrases and generating new sentences.
InProc.
HLT/NAACL.Rounds, William C. 1970.
Mappings and grammars on trees.
MathematicalSystems Theory, 4(3):257?287.Schabes, Yves.
1990.
Mathematical and Computational Aspects of LexicalizedGrammars.
Ph.D. thesis, Department of Computer and Information Science,University of Pennsylvania.Thatcher, J. W. 1970.
Generalized2 sequential machine maps.
J. Comput.
SystemSci., 4:339?367.Viterbi, A.
1967.
Error bounds for convolutional codes and an asymptoticallyoptimum decoding algorithm.
IEEE Trans.
Information Theory, IT-13.Wu, Dekai.
1997.
Stochastic inversion transduction grammars and bilingual pars-ing of parallel corpora.
Computational Linguistics, 23(3):377?404.Yamada, Kenji and Kevin Knight.
2001.
A syntax-based statistical translationmodel.
In Proc.
ACL.
