Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 199?207,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005A Recursive Statistical Translation Model?Juan Miguel VilarDpto.
de Lenguajes y SistemasInforma?ticosUniversitat Jaume ICastello?n (Spain)jvilar@lsi.uji.esEnrique VidalDpto.
de Sistemas Informa?ticosy Computacio?nUniversidad Polite?cnica de ValenciaInstituto Tecnolo?gico de Informa?ticaValencia (Spain)evidal@iti.upv.esAbstractA new model for statistical translation ispresented.
A novel feature of this modelis that the alignments it produces are hier-archically arranged.
The generative pro-cess begins by splitting the input sen-tence in two parts.
Each of the parts istranslated by a recursive application ofthe model and the resulting translationare then concatenated.
If the sentenceis small enough, a simpler model (in ourcase IBM?s model 1) is applied.The training of the model is explained.
Fi-nally, the model is evaluated using the cor-pora from a large vocabulary shared task.1 IntroductionSuppose you were to find an English translation fora Spanish sentence.
One possible approach is to as-sume that every English sentence is a candidate butthat different English sentences have different prob-abilities of being the correct translation.
Then, thetranslation task can be divided in two parts: definean adequate probability distribution that answers tothe question ?given this English sentence, which isthe probability that it is a good translation of thatSpanish sentence??
; and use that distribution in or-der to find the most likely translation of your inputsentence.
?Work partially supported by Bancaixa through the project?Sistemas Inductivos, Estad?
?sticos y Estructurales, para la Tra-duccio?n Automa?tica (Siesta)?.This approach is referred to as the statistical ap-proach to machine translation.
The usual approachis to define an statistical model and train its parame-ters from a training corpus consisting in pairs of sen-tences that are known to be translation of each other.Different models have been presented in the litera-ture, see for instance (Brown et al, 1993; Och andNey, 2004; Vidal et al, 1993; Vogel et al, 1996).Most of them rely on the concept of alignment: amapping from words or groups of words in a sen-tence into words or groups in the other (in the caseof (Vidal et al, 1993) the mapping goes from rulesin a grammar for a language into rules of a grammarfor the other language).
This concept of alignmenthas been also used for tasks like authomatic vocab-ulary derivation and corpus alignment (Dagan et al,1993).A new statistical model is proposed in this pa-per, which was initially introduced in (Vilar Torres,1998).
This model is designed so that the align-ment between two sentences can be seen in an struc-tured manner: each sentence is divided in two partsand they are put in correspondence; then each ofthose parts is similarly divided and related to itstranslation.
This way, the alignment can be seen asa tree structure which aligns progressively smallersegments of the sentences.
This recursive proceduregives its name to the model: MAR, which comesfrom ?Modelo de Alineamiento Recursivo?, whichis Spanish for ?Recursive Alignment Model?.The rest of the paper is structured as follows: af-ter a comment on previous works, we introduce thenotation that we will use throughout the paper, thenwe briefly explain the model 1 from IBM, next we199introduce our model, then we explain the processof parameter estimation, and how to use the modelto translate new test sentences.
Finally, we presentsome experiments and results, together with conclu-sions.2 Previous worksThe initial formulation of the proposed model,including the training procedures, was presentedin (Vilar Torres, 1998), along with preliminary ex-periments in a small translation task which providedencouraging results.This model shares some similarities with thestochastic inversion transduction grammars (SITG)presented by Wu in (Wu, 1997).
The main pointin common is the type of possible alignments con-sidered in both models.
Some of the propertiesof these alignments are studied in (Zens and Ney,2003).
However, the parametrizations of SITGs andthe MAR are completely different.
The generativeprocess of SITGs produces simultaneously the in-put and output sentences and the parameters of themodel refer to the rules of the nonterminals.
Thisprovides a symmetry to both input and output sen-tences.
In contrast, our model clearly distinguishesthe input and output sentences and the parametersare based on observable properties of the strings(their lengths and the words composing them).
Onthe other hand, the MAR idea of splitting the sen-tences until a simple structure is found, also ap-pears in the Divisive Clustering approach presentedin (Deng et al, 2004).
Again, the main differencelies in the probabilistic modeling of the alignments.In Divisive Clustering a uniform distribution on thealignments is assumed while MAR uses a explicitparametrization.3 Some notationIn the rest of the paper, we use the following nota-tion.
Sentences are taken as concatenations of sym-bols (words) and represented using a letter and asmall bar, like in x?.
The individual words are de-signed by the name of the sentence and a subindexindicating the position, so x?
= x1x2 .
.
.
xn.
Thelength of a sentence is indicated by |x?|.
Segmentsof a sentence are denoted by x?ji = xi .
.
.
xj .
For thesubstrings of the form x?|x?|i we use the notation x?.i.Consistently, x?
denotes the input sentence and y?its translation and both are assumed to have at leastone word.
The input and output vocabularies are Xand Y , respectively.
Finally, we assume that we arepresentend a set M for training our models.
The ele-ments of this set are pairs (x?, y?)
where y?
is a possibletranslation for x?.4 IBM?s model 1IBM?s model 1 is the simplest of a hierarchy of fivestatistical models introduced in (Brown et al, 1993).Each model of the hierarchy can be seen as a refine-ment of the previous ones.
Although model 1, whichwe study here, relies on the concept of alignment,its formulation allows an interpretation of it as a re-lationship between multisets of words (the order ofthe words is irrelevant in the final formula).A word of warning is in order here.
The model weare going to present has an important difference withthe original: we do not use the empty word.
This isa virtual word which does not belong to the vocabu-lary of the task and that is added to the beginning ofeach sentence in order to allow words in the outputthat cannot be justified by the words in the input.
Wehave decided not to incorporate it because of the usewe are going to make of the model.
As we will see,model 1 is going to be used repeatedly over differentsubstrings of the input sentence in order to analyzetheir contribution to the total translation.
This meansthat we would have an empty word in each of thesesubstrings.
We have decided to avoid this ?prolifer-ation?
of empty words.
Future work may introducethe concept in a more appropriate way.The model 1 makes two assumptions.
That astochastic dictionary can be employed to model theprobability that word y is the translation of word xand that all the words in the input sentence have thesame weight in producing a word in the output.
Thisleads to:pI(y?
| x?)
=?
(|x?|, |y?|)|x?||y?||y?|?j=1|x?|?i=1t(yj | xi).
(1)Where t is the stochastic dictionary and ?
representsa table that relates the length of the alignment withthe length of the input sentence (we assume thatthere is a finite range of possible lengths).
This ex-plicit relations between the lengths is not present in200the original formulation of the model, but we preferto include it so that the probabilities are adequatelynormalized.Clearly, this model is not adequate to describecomplex translations in which complicated patternsand word order changes may appear.
Nevertheless,this model can do a good job to describe the transla-tion of short segments of texts.
For example, it canbe adequate to model the translation of the Spanish?gracias?
into the English ?thank you?.5 A Recursive Alignment ModelTo overcome that limitation of the model we willtake the following approach: if the sentence is com-plex enough, it will be divided in two and the twohalves will be translated independently and joinedlater; if the sentence is simple, the model 1 will beused.Let us formalize this intuition for the generativemodel.
We are given an input sentence x?
and the firstdecission is whether x?
is going to be translated byIBM?s model 1 or it is complex enough to be trans-lated by MAR.
In the second case, three steps aretaken: a cut point of x?
is defined, each of the result-ing parts are translated, and the corresponding trans-lations are concatenated.
For the translation of thesecond step, the same process is recursively applied.The concatenation of the third step can be done ina ?direct?
way (the translation of the first part andthen the translation of the second) or in an ?inverse?way (the translation of the second part and then thetranslation of the first).
The aim of this choice is toallow for the differences in word order between theinput and ouput languages.So, we are proposing an alignment model inwhich IBM?s model 1 will account for translationof elementary segments or individual words whiletranslation of larger and more complex segments orwhole sentences will rely on a hierarchical align-ment pattern in which model 1 alignments will beon the lowest level of the hierarchy.Following this discussion, the model can be for-mally described through a series of four random ex-periments:?
The first is the selection of the model.
It hastwo possible outcomes: IBM and MAR, withobvious meanings.?
The second is the choice of b, a cut point of x?.The segment x?b1 will be used to generate one ofthe parts of the translation, the segment x?.b+1will generate the other.
It takes values from 1to |x?| ?
1.?
The third is the decision about the order of theconcatenation.
It has two possible outcomes:D (for direct) and I (for inverse).?
The fourth is the translation of each of thehalves of x?.
They take values in Y+.The translation probability can be approximatedas follows:pT (y?
| x?)
= Pr(M = IBM | x?)pI(y?
| x?
)+ Pr(M = MAR | x?
)pM (y?
| x?
).The value of pI(y?
| x?)
corresponds to IBM?smodel 1 (Equation 1).
To derive pM (y?
| x?
), we ob-serve that:pM (y?
| x?)
=|x?|?1?b=1Pr(b | x?)?d?
{D,I}Pr(d | b, x?
)?y?1?Y+Pr(y?1 | b, d, x?
)?y?2?Y+Pr(y?2 | b, d, x?, y?1) Pr(y?
| d, b, x?, y?1, y?2).Note that the probability that y?
is generated froma pair (y?1, y?2) is 0 if y?
6= y?1y?2 and 1 if y?
= y?1y?2, sothe last two lines can be rewritten as:?y?1?Y+Pr(y?1 | b, d, x?
)?y?2?Y+Pr(y?2 | b, d, x?, y?1) Pr(y?
| b, d, x?, y?1, y?2)=?y?1,y?2?Y+y?=y?1y?2Pr(y?1 | b, d, x?)
Pr(y?2 | b, d, x?, y?1)=?y?1 ?
pref(y?)?
y?Pr(y?1 | b, d, x?)
Pr(y?
?11 y?
| b, d, x?, y?1)=|y?|?1?c=1Pr(y?c1 | b, d, x?)
Pr(y?.c+1 | b, d, x?, y?c1),201where pref(y?)
is the set of prefixes of y?.
And finally:pM (y?
| x?)
=|x?|?1?b=1Pr(b | x?)?d?
{D,I}Pr(d | b, x?
)|y?|?1?c=1Pr(y?c1 | b, d, x?)
Pr(y?.c+1 | b, d, x?, y?c1).
(2)The number of parameters of this model is verylarge, so it is necessary to introduce some simplifi-cations in it.
The first one relates to the decision ofthe translation model: we assume that it can be donejust on the basis of the length of the input sentence.That is, we cat set up two tables, MI and MM , sothatPr(M = IBM | x?)
?
MI(|x?|),Pr(M = MAR | x?)
?
MM (|x?|).Obviously, for any x?
?
X+, we will haveMI(|x?|)+MM (|x?|) = 1.
On the other hand, since it is notpossible to break a one word sentence, we defineMI(1) = 1.
This restriction comes in the line men-tioned before: the translation of longer sentenceswill be structured whereas shorter ones can be trans-lated directly.In order to decide the cut point, we will assumethat the probability of cutting the input sentence ata given position b is most influenced by the wordsaround it: xb and xb+1.
We use a table B such that:Pr(b | x?)
?B(xb, xb+1)?|x?|?1i=1 B(xi, xi+1).This can be interpreted as having a weight for eachpair of words and normalizing these weights in eachsentence in order to obtaing a proper probability dis-tribution.Two more tables, DD and DI , are used to store theprobabilities that the alignment be direct or inverse.As before, we assume that the decission can be madeon the basis of the symbols around the cut point:Pr(d = D | b, x?)
= DD(xb, xb+1),Pr(d = I | b, x?)
= DI(xb, xb+1).Again, we have DD(xb, xb+1) + DI(xb, xb+1) = 1for every pair of words (xb, xb+1).Finally, a probability must be assigned to thetranslation of the two halves.
Assuming that they areindependent we can apply the model in a recursivemanner:Pr(y?c1 | b, d, x?)
?
{pT (y?c1 | x?b1) if d = D,pT (y?c1 | x?.b+1) if d = I ,Pr(y?.c+1 | b, d, x?, y?c1) ?
{pT (y?.c+1 | x?.b+1) if d = D,pT (y?.c+1 | x?b1) if d = I .Finally, we can rewrite (2) as:pM (y?
| x?)
=|x?|?1?b=1B(xb, xb+1)?|x?|?1i=1 B(xi, xi+1)?
(DD(xb, xb+1)|y?|?1?c=1pT (y?c1 | x?b1)pT (y?.c+1 | x?.b+1)+DI(xb, xb+1)|y?|?1?c=1pT (y?.c+1 | x?b1)pT (y?c1 | x?.b+1)).The final form of the complete model is then:pT (y?
| x?)
=MI(|x?|)pI(y?
| x?
)+MM (|x?|)|x?|?1?b=1B(xb, xb+1)?|x?|?1i=1 B(xi, xi+1)?
(DD(xb, xb+1)|y?|?1?c=1pT (y?c1 | x?b1)pT (y?.c+1 | x?.b+1)+DI(xb, xb+1)|y?|?1?c=1pT (y?.c+1 | x?b1)pT (y?c1 | x?.b+1)).
(3)6 Parameter estimationOnce the model is defined, it is necessary to finda way of estimating its parameters given a trainingcorpus M. We will use maximun likelihood estima-tion.
In our case, the likelihood of the sample corpusis:V =?(x?,y?
)?MpT (y?
| x?
).202In order to maximize V , initial values are givento the parameters and they are reestimated using re-peatedly Baum-Eagon?s (Baum and Eagon, 1967)and Gopalakrishnan?s (Gopalakrishnan et al, 1991)inequalities.
Let P be a parameter of the model (ex-cept for those in B) and let F(P ) be its ?family?
(i.e.the set of parameters such that?Q?F(P ) Q = 1).Then, a new value of P can be computed as follows:N (P ) =P?
V?
P?Q?F(P )Q?
V?
Q=?(x?,y?
)?MPpT (y?
| x?)?
pT (y?
| x?)?
P?Q?F(P )?(x?,y?
)?MQpT (y?
| x?)?
pT (y?
| x?)?
Q=C(P )?Q?F(P )C(Q),(4)whereC(P ) =?(x?,y?
)?MPpT (y?
| x?)?
pT (y?
| x?)?
P, (5)are the ?counts?
of parameter P .
This is correct aslong as V is a polynomial in P .
However, we have aproblem for B since V is a rational function of theseparameters.
We can solve it by assuming, withoutlose of generality, that?x1,x2?X B(x1, x2) = 1.Then Gopalakrishnan?s inequality can be appliedsimilarly and we get:N (P ) =C + C(P )?Q?F(P )C + C(Q), (6)where C is an adequate constant.
Now it is easyto design a reestimation algorithm.
The algorithmgives arbitrary initial values to the parameters (typi-cally those corresponding to uniform probabilities),computes the counts of the parameters for the corpusand, using either (4) or (6), gets new values for theparameters.
This cycle is repeated until a stoppingcriterion (in our case a prefixed number of iterations)is met.
This algorithm can be seen in Figure 17 Some notes on efficiencyEstimating the parameters as discussed above entailshigh computational costs: computing pT (y?
| x?)
re-quires O(mn) arithmetic operations involving thevalues of pT (y?ji | x?lk) for every possible value ofi, j, k and l, which are O(m2n2).
This results in aglobal cost of O(m3n3).
On the other hand, com-puting ?
pT?
P costs as much as computing pT .
So it isinteresting to keep the number of computed deriva-tives low.7.1 Reduction of the parameters to trainIn the experiments we have followed some heuristicsin order not to reestimate certain parameters:?
The values of MI ?and, consequently,of MM?
for lengths higher than a thresholdare assumed to be 0 and therefore there is noneed to estimate them.?
As a consequence, the values of ?
for lengthsabove the same threshold, need not be reesti-mated.?
The values of t for pairs of words with countsunder a certain threshold are not reestimated.Furthermore, during the computation of counts, therecursion is cut on those substring pairs where thevalue of the probability for the translation is verysmall.7.2 Efficient computation of model 1Other source of optimization is the realization thatfor computing pT (y?
| x?
), it is necessary to com-pute the value of pI for each possible pair (x?ieib, y?oeob)(where ib, ie, ob and oe stand for input begin, in-put end, output begin and output end, respectively).Fortunately, it is possible to accelerate this compu-tations.
First, define:I(ib, ie, ob, oe) =pI(x?ieib, y?oeob)?(ie?
ib + 1, oe?
ob + 1)=1(ie?
ib + 1)oe?ob+1oe?j=obie?i=ibt(y?j | x?i).Now letS(ib, ie, j) =ie?i=ibt(y?j | x?i).203Algorithm Maximum likelihood estimationgive initial values to the parameters;repeatinitialize the counts to 0;for each (x?, y?)
?
M docompute pT (y?
| x?
);for each parameter P involved in the alignment of (x?, y?)
doCP := CP +PpT (y?
| x?)?
pT (y?
| x?)?
P;endforendforfor each parameter P doreestimate P using (4) or (6);endforuntil the stopping criterion is met;End Maximum likelihood estimationFigure 1: Algorithm for maximum likelihood estimation of the parameters of MARThis leads toI(ib, ie, ob, oe) = S(ib, ie, ob),if ob = oe, and toI(ib, ie, ob, oe) =I(ib, ie, ob, oe?
1)S(ib, ie, ob)(ie?
ib + 1),if ob 6= oe.So we can compute all values of I with the algo-rithm in Figure 2.7.3 Splitting the corporaAnother way of reducing the costs of training hasbeen the use of a heuristic to split long sentencesinto smaller parts with a length less than l words.Suppose we are to split sentences x?
and y?.
Webegin by aligning each word in y?
to a word in x?.Then, a score and a translation is assigned to eachsubstring x?ji with a length below l. The translation isproduced by looking for the substring of y?
which hasa length below l and which has the largest numberof words aligned to positions between i and j. Thepair so obtained is given a score equal to sum of: (a)the square of the length of x?ji ; (b) the square of thenumber of words in the output aligned to the input;and (c) minus ten times the sum of the square of thenumber of words aligned to a nonempty position outof x?ji and the number of words outside the segmentchosen that are aligned to x?ji .After the segments of x?
are so scored, the partitionof x?
that maximizes the sum of scores is computedby dynamic programming.8 Translating the test sentencesThe MAR model can be used to obtain adequatebilingual templates which can be used to translatenew test sentences using an appropriate template-based translation system.
Here we have adopted thepharaoh program (Koehn, 2004).8.1 Finding the templatesThe parameters of the MAR were trained using thealgorithm above: first ten IBM model 1 iterationswere used for giving initial values to the dictionaryprobabilities and then five more iterations for re-training the dictionary together with the rest of theparameters.The alignment of a pair has the form of a tree sim-ilar to the one in Figure 3 (this is one of the sen-tences from the Spanish-English part of the trainingcorpus).
Each interior node has two children corre-sponding to the translation of the two parts in whichthe input sentence is divided.
The leaves of the treecorrespond to those segments that were translated bymodel 1.
The templates generated were those de-fined by the leaves.
Further templates were obtainedby interpreting each pair of words in the dictionaryas a template.204Algorithm all IBMfor ob := 1 to |y?| dofor oe := ob to |y?| dofor ib := 1 to |x?| doS := 0;for ie := ib to |x?| doS := S + t(yoe | xie);I(ib, ie, ob, oe) :={S/(ie?
ib + 1) if ob = oe,I(ib, ie, ob, oe?
1)?
S/(ie?
ib + 1) otherwise;End all IBMFigure 2: Efficient computation of different values of IBM?s model 1.Equipos a presi?n transportablesTransportable pressure equipmentEquiposequipmenta presi?n transportablesTransportable pressurea presi?npressuretransportablesTransportableFigure 3: A sample alignment represented as a tree.Each template was assigned four weights1 in or-der to use the pharaoh program.
For the templatesobtained from the alignments, the first weight wasthe probability assigned to it by MAR, the secondweight was the count for the template, i.e., the num-ber of times that template was found in the corpus,the third weight was the normalized count, i.e., thenumber of times the template appeared in the corpusdivided by the number of times the input part waspresent in the corpus, finally, the fourth weight wasa small constant (10?30).
The intention of this lastweight was to ease the combination with the tem-plates from the dictionary.
For these, the first threeweights were assigned the same small constant andthe fourth was the probability of the translation ofthe pair obtained from the stochastic dictionary.
Thisweighting schema allowed to separate the influenceof the dictionary in smoothing the templates.1They should have been probabilities, but in two of the casesthere was no normalization and in one they were even greaterthan one!Table 1: Statistics of the training corpora.
Thelanguages are German (De), English (En), Span-ish (Es), Finnish (Fi) and French (Fr).Languages Sentences Words (input/output)De-En 751 088 15 257 871 / 16 052 702Es-En 730 740 15 725 136 / 15 222 505Fi-En 716 960 11 318 863 / 15 493 334Fr-En 688 031 15 599 184 / 13 808 5059 ExperimentsIn order to test the model, we have decided to par-ticipate in the shared task for this workshop.9.1 The taskThe aim of the task was to translate a set of 2,000sentences from German, Spanish, Finnish andFrench into English.
Those sentences were ex-tracted from the Europarl corpus (Koehn, Unpub-lished).
As training material, four different corporawere provided, one for each language pair, compris-ing around 700 000 sentence pairs each.
Some de-tails about these corpora can be seen in Table 1.
Anautomatic alignment for each corpus was also pro-vided.The original sentence pairs were splitted using thetechniques discussed in section 7.3.
The total num-ber of sentences after the split is presented in Ta-ble 2.
Two different alignments were used: (a) theone provided in the definition of the task and (b)one obtained using GIZA++ (Och and Ney, 2003)to train an IBM?s model 4.
As it can be seen, thenumber of parts is very similar in both cases.
The205Table 2: Number of training pairs after splitting toa maximum length of ten.
?Provided?
refers to thealignment provided in the task, ?GIZA++?
to thoseobtained with GIZA++.Sentence pairsLanguages Provided GIZA++De-En 2 351 121 2 282 316Es-En 2 160 039 2 137 301Fi-En 2 099 634 2 017 130Fr-En 2 112 931 2 080 200Table 3: Number of templates for each languagepair: ?Alignment?
shows the number of templatesderived from the alignments; ?dictionary?, those ob-tained from the dictionary; and ?total?
is the sum.
(a) Using the alignments provided with the task.Lang.
Alignment Dictionary TotalDe-En 2 660 745 1 840 582 4 501 327Es-En 2 241 344 1 385 086 3 626 430Fi-En 2 830 433 2 852 583 5 683 016Fr-En 2 178 890 1 222 266 3 401 156(b) Using GIZA++.Lang.
Alignment Dictionary TotalDe-En 2 672 079 1 796 887 4 468 966Es-En 2 220 533 1 350 526 3 571 059Fi-En 2 823 769 2 769 929 5 593 698Fr-En 2 140 041 1 181 990 3 322 031number of pairs after splitting is roughly three timesthe original.Templates were extracted as described in sec-tion 8.1.
The number of templates we obtained canbe seen in Table 3.
Again, the influence of thetype of alignment was small.
Except for Finnish,the number of dictionary templates was roughly twothirds of the templates extracted from the align-ments.9.2 Obtaining the translationsOnce the templates were obtained, the developmentcorpora were used to search for adequate values ofTable 4: Best weights for each language pair.
Thecolumns are for the probability given by the model,the counts of the templates, the normalized countsand the weight given to the dictionary.
(a) Using the alignments provided with the task.Languages Model Count Norm DictDe-En 0.0 3.0 0.0 0.3Es-En 0.0 2.9 0.0 0.4Fi-En 0.0 7.0 0.0 0.0Fr-En 0.0 7.0 1.0 1.0(b) Using GIZA++.Languages Model Count Norm DictDe-En 0.0 3.0 0.0 0.0Es-En 0.0 2.9 0.0 0.4Fi-En 0.0 3.0 1.5 0.0Fr-En 0.0 3.0 1.0 0.4Table 5: BLEU scores of the translations.BLEULanguages Provided GIZA++De-En 18.08 18.89Es-En 21.65 21.48Fi-En 13.31 13.79Fr-En 21.25 19.86the weights that pharaoh uses for each template(these are the weights passed to option weight-t,the other weights were not changed as an initial ex-ploration seemed to indicate that they had little im-pact).
As expected, the best weights differed be-tween language pairs.
The values can be seen intable 4.It is interesting to note that the probabilities as-signed by the model to the templates seemed tobe better not taken into account.
The most impor-tant feature was the counts of the templates, whichsometimes were helped by the use of the dictionary,although that effect was small.
Normalization ofcounts also had little impact.20610 Results and discussionThe results over the test sets can be seen in Table 5.It can be seen that, except for French, the influenceof the initial alignment is very small.
Also, the bestresults are obtained for Spanish and French, whichare more similar to English that German or Finnish.There are still many open questions that deservemore experimentation.
The first is the influence ofthe split of the original corpora.
Although the simi-larity of results seem to indicate that it has little in-fluence, this has to be tested.
Two more relevant as-pects are whether the weighting schema is the bestfor the decoder.
In particular, it is surprising that thenormalization of counts had so little effect.Finally, the average number of words per templateis below two, which probably is too low.
It is inter-esting to find alternate ways of obtaining the tem-plates, for instance using internal nodes up to a givenheight or covering portions of the sentences up to apredefined number of words.11 ConclusionsA new translation model has been presented.
Thismodel produces translations in a recursive way: theinput sentence is divided in two parts, each is trans-lated using the same procedure recursively and thetranslations are concatenated.
The model has beenused for finding the templates in a large vocabularytranslation task.
This involved using several heuris-tics to improve training time, including a method forsplitting the input before training the models.
Fi-nally, the influence of using a stochastic dictionarytogether with the templates as a means of smoothinghas been explored.ReferencesLeonard E. Baum and J.
A. Eagon.
1967.
An inequal-ity with applications to statistical estimation for prob-abilistic functions of Markov processes and to a modelfor ecology.
Bulletin of the American MathematicalSociety, 73:360?363.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?311,June.Ido Dagan, Kenneth W. Church, and William A. Gale.1993.
Robust bilingual word alignment for machineaided translation.
In Proceedings of the Workshop onVery Large Corpora, Columbus, Ohio (USA).
ACL.Yonggang Deng, Shankar Kumar, and William Byrne.2004.
Bitext chunk alignment for statistical machinetranslation.
Research Note 50, CLSP Johns HopkinsUniversity, April.P.
S. Gopalakrishnan, Dimitri Kanevsky, Arthur Na?das,and David Nahamoo.
1991.
An inequality for ra-tional functions with applications to some statisticalproblems.
IEEE Transactions on Information Theory,37(1):107?113, January.Philipp Koehn.
2004.
Pharaoh: A beam search decoderfor phrase-based statistical machine translation mod-els.
In AMTA, pages 115?124.Philipp Koehn.
Unpublished.
Europarl: A multilingualcorpus for evaluation of machine translation.
Draft.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Joseph Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449, De-cember.Enrique Vidal, Roberto Pieraccini, and Esther Levin.1993.
Learning associations between grammars: Anew approach to natural language understanding.
InProceedings of the EuroSpeech?93, pages 1187?1190,Berlin (Germany).Juan Miguel Vilar Torres.
1998.
Aprendizaje de Tra-ductores Subsecuenciales para su empleo en tareasde dominio restringido.
Ph.D. thesis, Departamentode Sistemas Informa?ticos y Computacio?n, UniversidadPolite?cnica de Valencia, Valencia (Spain).
(in Span-ish).Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of the COLING?96, pages 836?841, Copenhagen (Denmark), August.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Richard Zens and Hermann Ney.
2003.
A comparativestudy on reordering constraints in statistical machinetranslation.
In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics, pages144?151, Sapporo (Japan), July.
Association for Com-putational Lingustics.207
