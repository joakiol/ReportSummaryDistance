D-Theory: Talking about Talking about TreesMitchell P. MarcusDonald HindleMargaret M. FleckBell LaboratoriesMurray Hill, New Jersey 07974Linguists, including computational linguists, have always beenfond of talking about trees.
In this paper, we outline a theory oflinguistic structure which talks about talking about trees; we callthis theory Description theory (D-theory).
While importantissues must be resolved before a complete picture of D-theoryemerges (and also before we can build programs which utilizeit), we believe that this theory will ultimately provide aframework for explaining the syntax and semantics of naturallanguage in a manner which is intrinsically computational.
Thispaper will focus primarily on one set of motivations for thistheory, those engendered by attempts to handle certain syntacticphenomena within the framework of deterministic parsing.1.
D-Theory: An IntroductionThe key idea of D-theory is that a syntactic analysis of asentence of English (or other natural language) consists of adescription of its syntactic structure.
Such a descriptioncontains information which differs from that contained in astandard tree structure in two crucial ways:1) The primitive predicate for indicating hierarchical structurein a D-theory description is "dominates" rather than "directlydominates".
(A node A is said to dominate a node B if A issome ancestor of B; A is said to directly dominate B if A is theimmediate parent of B.)
A D-theory analysis thus expressesdirectly only what structures are contained (somewhere) withinlarger structures, but does indicate per se what the immediateconstituents of any particular constituent are.A tree structure, on the other hand, encodes which nodes aredirectly dominated by other nodes in the analysis; it indicatesdirectly the immediate constituents of each node.
In a standardparse tree, the topmost S node might directly dominate xactly aNoun Phrase node, an Aux node and a Verb Phrase node; it isthus made up of three subparts: .that NP, that Aux, and thatVP.2) A D-theory description uses names to make statements aboutentities, and does not contain the entities themselves.Furthermore, there is no distinguished set of names which aretaken to be standard names or rigid designators; i.e.
given only aname, one cannot tell what particular .syntactic entity it refersto.
(This is the primary reason that we view D-theoryrepresentations as descriptions and not merely as directedacyclic graphs.
)Because there are no standard names, if one is presented withtwo descriptions, each in terms of a different name, one can tellwith certainty only if the two names refer to different entities,but never (for sure) if they refer to the same entity.
In thelatter case, there is always potential ambiguity.
To take acommonplace example, given that "John has red hair" and "Mr.Jones has black hair', one can be sure that John is not Mr.Jones.
But if one is told "John has red hair" and "Mr. Joneswears glasses" and nothing more about either John or Mr.Jones, then it is impossible to tell whether John is or is not Mr.Jones.
In the domain of syntax, if a D-theory description saysthatXisan NP;Zisan NPY is an Adjective PhraseW is a nounX dominates YZ dominates Wand nothing else is stated about W, X, Y or Z, then it cannot bedetermined whether X and Z are aliases for the same NP nodeor are names for two distinct nodes, if an additional statementis added to the description that "Y dominates Z", then it must bethe case that X and Z name distinct entities.
We will show inwhat follows that the use of names has important ramificationsfor linguistic theory and the theory of parsing.The structure of the rest of this paper is roughly as follows: Wewill first sketch the computational framework we build on, inessence that of \[Marcus 80\], and explore briefly what a parserfor this kind of grammar might look like; in appearance, its datastructures and grammar will be Iittle different from thatdeveloped in \[Berwick 82\].
A series of syntactic phenomena willthen be explored which resist elegant account within the earlierframework.
For each phenomenon, we will present a simple D-theoretic solution together with exposition of the relevant aspectsof D-theory.One final introductory comment: That D-theory expressessyntactic structure in terms of dominance rather than directdominance may be reminiscent of \[Lasnik & Kupin 1977\](henceforth L-K), but our use of the dominance predicate differsfundamentally from the L-K formulation both in the primacy ofthe predicate to the theory, and in the theory of syntax implied.Lasnik and Kupin's formalization of the Extended StandardTheory der:ves domino.tion relations from their primaryrepresentation f linguistic structure, namely a set of strings ofterminals and nonterminals with specified properties.
D-theorystructures are expressed directly in terms of dominancerelations; the linear order of constituents is only directlyexpressed for items in the lexical string.
Despite appearances,D-theory and the Lasnik-Kupin formalization are not inter-definable.
We discuss the properties of the Lasnik-Kupinformalization at length in a forthcoming paper.\[2920 DeterminLqgic Tree-Building: The Old TheoryD-theory grows out of earlier work on deterministic parsing asdeterministic tree building (as in e.g.
\[Marcus 19801, \[Church801 and \[Berwick 82\]).
The essence of that work is thehypothesis that natural language can be analyzed by someprocess which builds a syntactic analysis indelibly (borrowing aterm from \[McDonald 83\]); i.e.
that any structure built by theparser is part of the correct analysis of the input.
Again, in thecontext of this earlier theory, the form of the indelible syntacticanalysis was that of a tree.One key idea of this earlier tree-building theory that we retain isthe notion that a natural language parser can buffer andexamine some small number (e.g.
up to three) unattachedconstituents before being forced to add to its existing structures.
(In D-theory, the node named X is attached to Y if the parser'sdescription of the existing structure includes a predication of theform "Y dominates X', or, as we will henceforth write,"D(Y,X)."
X is unattached if the parser's description of theexisting structure includes no predication of the form "D(Y, X)' ,for any name Y.)
We thus assume that such a parser will havethe two principle data structures of these earlier deterministicparsers, a stack and a buffer.
However, the stack and the bufferin a D-theory parser will contain names rather than constituents,and these data structures will be augmented by a data basewhere the description of the syntactic structure itself is built upby the parser.
(While this might sound novel, a moment'sreflection on LISP implementation techniques should assure thereader that this structure is far less different from that of olderparsers like Parsifal and Fidditch \[Hindle 831 than it mightsound.
)As we shall see below, however, a parser which embodies D-theory can recover (in some sense) from some of theconstructions which would terminally confuse (or "garden path')a parser based on the deterministic tree-building theory.
ForD-theory to be psychologically valid, of course, it must be thecase that just those constructions which do garden path a D-theory parser garden path people as well.
(We might note inpassing that recent experimental paradigms which explore onlinesyntactic processing using eye-tracking technology promise toprovide delicate tests of these hypotheses, e.g.
\[Rayner &Frazier 831.
)Another goal of this earlier work was to find some way ofprocedurally representing grammars of natural languages whichis brief and perspicuous, and which allows (and perhaps evenforces) grammatical generalizations to be stated in a naturalway.
As is often argued, such a representation must beembodied by our language understanding faculty, given that thegrammar of a language is learned incrementally and quickly bychildren given only limited evidence.
(To recast this point froman engineering point of view, this property is also a prerequisiteto writing a grammar for a subset of some given naturallanguage which remains extensible, so that new constructionscan be added to the grammar without global changes, and sothat these new constructions will interact robustly with the oldgrammar.
)Following \[Shipman 78\], as refined in \[Berwick 82\].
we assumethat the grammar is organized into a set of context free rules,which we will call base templates, and a set of pattern-actionrules.
As in Parsifal, each pattern consists of up to fourelements, each of which is a partial description of an element inthe buffer, or the accessible node in the stack (the "currentactive node').
Loosely following \[Berwick 82\], we assume thatthe action of each rule consists of exactly one of some small setof limited actions which might include the following:?
Attach a node in the buffer to the current active node.?
Switch the nodes in the first two buffer positions.?
Insert a specified lexical item into a specified buffer slot.?
Create a new current active node.?
Insert an empty NP into the first buffer slot.
(Where "attachment" is as defined above, and "create" meanssomething like coin a new node name, and push it onto theactive node stack.)
Each rule is associated with some position inone of the base templates.
So, for example, in figure 1 below,one base template is given, a highly simplified template for asentence.
Associated with the NP in the subject position of thesentence are several rules.
The first rule says that if the firstbuffer position holds a name which is asserted to be an NP(informally: if there is an NP in the first buffer slot), then(informally) it is dominated by the S. The second says that ifthere is an auxiliary verb in the first slot followed by an NP,then switch them.
And so on.Note that while a D-the0ry parser itself has no predicate withwhich to express direct dominance, the base templates explicitlyencode just such information.
Insofar as the parser makes itsassertions of dominance on the basis of the phrase structurerules, the parser will behave very similarly to deterministic treeS .> NP VP PP*{ \ [NP I -> Attach}{ \ [auxv l \ [NP \ ] -> Switch}{\[v, tenselessl ->  lnsert(NP, 0)}Figure 1.
A simplified base template forS, with associated NP rules.building parsers.
In fact, the parser will typically (although, aswe will see below, not always) behave in just such a fashion.3.
The Problem of Misleading Leading EdgesBy and large, we believe that a significant subset of thegrammar of English has been successfully embedded within thedeterministic tree-building model.
However, a residue ofsyntactic phenomena remain which defy simple explicationwithin this framework.
Some of these phenomena are particularproblems for the deterministic tree-building framework.
Others,for example coordination and gapping phenomena, have defiedadequate xplication within any existing theory of grammar.In the remainder of this paper we will explore a range of suchphenomena, and argue that D-theory provides a consistentapproach which yields simple accounts for the range ofphenomena we have considered to date.
We will first argue fortaking "dominates', not "directly dominates" as primitive, andthen later argue why the use of names is justified.
(Our viewthat this representation should be viewed as a description hangson the use of names.
In this section and in section 5 we argueonly for a representation which is a particular kind of directedacyclic graph.
Only with the arguments of section 7 is theposition that this is a kind of description at all defensible.
)One particularly interesting class of sentences which seems todefy deterministic accounts is exemplified by (2).
(2) I drove my aunt from Peoria's car.130Sentences like (2) contain a constituent which has a misleading*leading edge', an initial right-embedded subconstituent whichcould itself be the next constituent of whatever structure is beingbuilt at the next level up.
For example, while analyzing (2), aparser which deterministically builds old-fashioned trees mightjust take "my aunt" to be the object of "drove', attaching it asthe object of the VP, only to discover (too late) that this phrasefunctions instead as genitive determiner of the full NP "my auntfrom Peoria's car'.In fact, the existing grammar for Parsifal causes exactly thisbehavior, and for good reason: This parser constructs NPs onlyup to the head noun before deciding on their role within thelarger context; only after attaching an NP will Parsifal constructthe post-modifiers of the NP and attach them, (This involves amechanism called node reactivation; it is described in \[Shipman& Marcus 79\].)
One reason for this within the earlierframework is that, given a PP which immediately follows thehead of an NP, it cannot be determined whether that PP shouldbe attached to the preceding NP or to some constituent whichdominates the NP until the role of that NP itself has beendetermined.
In the specific case of (2), the parser will attach"my aunt" as the object of the verb "drove" so that it can decidewhere to attach the PP beginning with "from'.
Only after it istoo late will the parser see the genitive marker on "Peoria's" andboggle.
While one could attempt to overcome this particularmotivation for the two-stage parsing of NPs with some variantof the notion of pseudo-attachment (first used in \[Church 801),this and related approaches have their problems too, as Churchnotes.Potential pseudo-attachment solutions aside, the upshot is thatsentences like (2) will cause deterministic tree building parsersto garden path.
However, it is our strong intuition that suchcases are not "garden paths'; we believe that such cases shouldbe analyzed correctly by a deterministic parser rather than bythe (putative) mechanism which recovers from garden paths.The D-theoretic solution to the problem of misleading "leadingedges" hinges on one formal property of this problem: Theinitial analysis of this class of examples is incorrect only in thatsome constituent is attached in the parse tree at a higher pointin the surrounding structure than is correct.
Crucially, theparser neither creates structures of the wrong kind nor does itattach the structure that it builds to some structure which doesnot dominate it.
In the misanalysis of (2), the parser initiallyerrs only in attaching the NP "my aunt', which is indeeddominated by the VP whose head is "drove', too high in thestructure.This class of examples is handled by D-theory without difficultyexactly because syntactic analyses are expressed in terms ofdomination rather than direct domination.
The developingdescription of the structure of (2) in a D-theory parser at thepoint at which the parser had analyzed "my aunt', but nofurther, might include the following predications:(3.1) D(vpl, npl)(3.2) D(vpl, vl)where the verb node named vl dominates "drove', and the NPnode named npl dominates the lexical material "my aunt'.Let us assume for the sake of simplicity that while building thePP "from Peoria's', the parser detects a genitive marker on theproper noun "Peoria's" and knows (magically, for now) that"Peoria's car" is not the correct analysis.
Given this, the genitivemust mark the entire NP "my aunt from Peoria" and thus "myaunt from Peoria" must serve not as the object of the verb"drove" but as the determiner of some larger NP which itselfmust be the object of "drove'.
(Unless it is followed by agenitive marker, in which case....) The question we are centrallyinterested in here is not how the parser comes to the realizationthat it has erred, but rather what can be done to remedy thesituation.
(Actually how the parser must resolve "..L firstproblem is a complex and interesting story in and of itself, withthe punchline being that exactly one (but only one) of (2) and(4) I drove my aunt from Peoria's suburbs home.must cause a garden path.
The details of this await furtherresearch on the control of D-theory parsing.
)The description (3) is easy fixed, given that "D" is read"dominates', and not "directly dominates'.
Several furtherpredications can merely be added to (3), namely those of (5),which state that npl is dominated by a determiner node nameddetl, which itself is dominated by a new np node; np2, and thatnp2 is dominated by vpl.
(5.1) D(npl, detl)(5.2) D(detl, np2)(5.3) D(np2, vpl)Adding these new predications does not make the predications of(3) false; it merely adds to them.
The node named npl is stilldominated by vpl as stated in (3.1), because the relation "D" istransitive.
Given the predications in (5), (3.1) is redundant, butit is not false.The general point is this: D-theory allows nodes to be attachedinitially by a parser to some point which will turn out to behigher than its lowest point of attachment (for the more generalsense of attachment defined above) without such initial statescausing the parser to garden path.
Because of the nature of "D'.the parser can in this sense "lower" a constituent withoutfalsifying a previous predication.
The earlier predicationremains indelible.4.
Semantic Interpretation: The Standard ReferentBut how can such a list of domination predications beinterpreted?
It would seem that compositional semantics mustdepend upon being able to determine exactly what theimmediate constituents of any given structure are: if themeaning of a phrase determined from the meanings of its parts,then it must be determined exactly what its parts are.We assume that semantic interpretation of a D-theory analysisis done by taking such an analysis as describing the minimaltree possible, i.e.
by taking "D" to mean directly dominateswherever possible but only for semantic analysis.
For example.if the analysis of a structure includes the predications that Xdominates Y, Y dominates Z and X also dominates Z, then thesemantic interpreter will assume that X directly dominates Yand that Y directly dominates Z.
We will call such aninterpretation of a D-theoretic analysis the standard referent ofthe analysis.
(We further assume that the description producedby a D-theory parser will have at each stage of the analysis oneand only one standard referent, and the complex situation wheretwo or more chains of domination must be merged to arrive at asingle standard referent will not arise in the operation of a D-theory parser.
Substantiation of these assumptions awaits theconstruction of a parser and a sizable grammar.
)This notion of "standard referent" means that addingpredications to the (partial) analysis of a sentence may very well131change the standard referent of that analysis as viewed by thesemantic interpreter.
The key idea here is that from the pointof view of semantics, the structure built by the parser mayappear to change, but from the parser's point of view, thedescription remains indelible.The situation we describe is not far from that which occurs asthe usual case in the communication of descriptions of objectsbetween individuals.
Suppose Don says to you, standing beforeyou wearing a brown tweed jacket, "My coat is too warm".
Thephrase "my coat" can refer to any coat that Don owns, yet youwill undoubtedly take the phrase to refer to the brown tweedjacket.
Given that descriptions are always necessarily partial,there must always be a conventional standard referent for adescription.
But now suppose that Don says "My blue coat istoo warm'.
He merely adds "blue" to the phrase "my coat", butthe set of possible referents changes, and in fact shrinks.
Moreto the point, you will now take the referent of the phrase "myblue coat" to mean some blue coat or other which Don owns; i.e.adding to the description changes the standard referent.The key notion here is that because descriptions are alwaysunderspecified, there must be some set of conventions forchoosing the intended single referent out of the often large (andsometimes infinite) class of objects that any given description istrue of.
Thus, once we claim that the output of syntacticanalysis is a description, it is not surprising that there must besome restrictive conventions to determine xactly what such adescription refers to.
Given this, the convention we assumeseems a simple and natural one.5.
On the Re.analysis of Indelible Strucmre~Another problematic class of constructions for deterministictree-building theories are those for which it is argued that somekind of active reanalysis process must occur.
For each of theseconstructions, there is linguistic evidence (of varied force) whichsuggests (recast in processing terms) that different syntacticstructures must be assigned to that construction at differentpoints during grammatical processing.
In other words, it can bedemonstrated that each of these constructions has propertieswhich provide evidence for one particular structure at one stageof processing, while displaying properties which argue for aquite different structure at a later stage of processing.
But ifthis reanalysis account is the correct account for any of theseconstructions, then the deterministic tree building theory mustbe wrong somewhere, for changing a structural analysis is theone thing that indelible systems cannot do, ex hypothesLOne class of examples widely assumed to involve some kind ofreanatysis i the class of verb complement structures which haveso-called "pseudo-passives".
These verbs seem to have twopassive forms, one of which has an NP in subject position whichserves in the same role as that served by the seeming object ofthe active form, while the other passive form seems to have anunderlying prepositional object in subject position.
For example,there are two passives which correspond to the active sentence(6.1), a "normal" passive (6.3), and a passive which seems topull the object of "of" into subject position, namely, (6.2).
(6.1) Past owners had made a mess of the house.
(6.2) The house had been made a mess of.
(6.3) A mess had been made of the house.One fairly common view is that the phrase "made a mess of.functions as a single idiomatic verb, so that "the house" in (6.1)and (6.
2) can be simply viewed as the object of the verb "madea mess of..
But then to account for (6.3), it must be assumedthat "made" is first treated as a normal verb with "a mess" asobject.
This means that either (6.3) has a different underlyingsyntactic structure than (6.1-2), or that the syntactic analysisassigned to the string "made of" (or perhaps "made <trace>of') changes after the passive is accounted for.
To get aconsistent syntactic analysis for these sentences, one can argueeither that reanalysis always or never takes place.
The positionthat we find most tenable, given the evidence, is that reanalysissometimes takes place.
(Of course, the fact that purely lexicalaccounts (see, e.g.
\[Bresnan 82\]) seem plausible leaves the oldertree-building theories on not entirely untenable ground.)
Buthow can any reanalysis at all be reconciled with the determinismhypothesis?Consider the analysis that a D-theory parser will have built upafter having parsed "made a mess', but before noticing "of'.
Atthis point the parser should assign the sentence a non-idiomaticreading, with "a mess" the real object of "made".
Some of thepredications in the analysis will be(7.1) D(vpl, vl)(7,2) D(vpl, npl)where vpl is a vp node dominating "made" and npl is an npnode dominating "a mess ~.
(Note that'in(8.1) The children made a mess, but then cleaned it up.
"it" refers to a mess, but that one cannot say(8.2) *The children made a mess of their bedrooms,but then cleaned it up.which seems to indicate that the phrase "a mess" is opaque toanaphoric reference in the idiomatic reading, and that therefore(8.1) is not idiomatic in the same sense.
)We assume here that the preposition "of" is lexically marked forthe idiomatic verb "make a mess', i.e.
it is lexically specified forthe idiom, but it is not itself a part of the idiom.
Evidence forthis includes sentences like (9), in which the preposition cannotbe reanalyzed into the verb, given D-theory, as we will seebelow.
(9) Of what did the children make a mess'?From a parsing point of view, this means that the presence ofthe preposition "of.
will serve as a trigger to the reanalysis of"make a mess", without being part of the reanalysed materialitself.
(Thanks to Chris Halverson for pointing out a problemcaused by (9) for an earlier analysis.
)Returning to the analysis of (6.1), the preposition "of" triggersexactly such a reanalysis.
Given D-theory, this can be effectedsimply by adding the additional predication (10) to (7.1-2)above:(10) D(vl, npl)Given this new predication, the standard referent of thedescription ow has npl directly dominated by vl, i.e.
it is nowpart of the verb.
And now when "a house" is noticed by theparser, it will be attached as the first NP after the verb vl, i.e.as its object.
Once again, the predications (7.1-2) are notfalsified by the additional predication; they remain indelibly true- npl remains dominated by vpl, although no longer directlydominated by it.
But, to repeat the point, the parser is(blissfully) unaware of this notion; the standard referent is anotion meaningful only to semantics.132The analysis of (6.2) proceeds as follows: After parsing "made"as a verb and "a mess" as its object and noticing the trigger "of"sitting in the buffer, the parser will add an extra predicationeffecting just the same "reanalysis" as was done for (6.1).
Weassume that the passive rule inserts a trace either immediatelyafter a verb, or after the preposition immediately following averb, if  that preposition is lexically specified for that verb.
Wewill not argue for this analysis here; suffice it to say that thisanalysis is motivated by facts which also motivate recentsomewhat similar analyses of passive, e.g.
\[Hornstein andWeinberg 811 and \[Bresnan 82\].
Given this analysis, the parserwill now drop a passive trace for the subject "the house" into thebuffer after the lexically specified preposition "of", and the parsewill then move to completion.
(One issue that remains open,though, is exactly how the parser knows not to drop the passivetrace after "made'.
The solution to this particular problem mustinteract correctly with many such control problems involvingpassive.
Resolving this entire set of issues in a consistent fashionawaits the pending implementation f a parser to serve as a toolin the investigation of these control issues.
)How is (6.3) parsed?
Here we assume that the parser will dropa passive trace after the verb "made'.
Because we assume thatthe parser cannot access the binding of the trace, and thereforecannot access the lexical material "a mess', it must be the casethat reanalysis will not take place in this case.
While thisasymmetry may seem unpleasant, we note that there is noevidence that syntactic reanatysis has taken place here.
Instead,.we assume that semantic processing will simply add anadditional domination predicate after it notices the binding ofthe passive trace.
Thus, the reanalysis here is semantic, notsyntactic.
(Note that there are other cases, e.g.
rightdislocation, where it is clear that additional dominationpredicates are added by post-syntactic processes.
We believethat semantics can add domination predicates, but cannotconstruct new nodes.
)As an example of the kind of operation that is ruled out by D-theory, let us return to our assertion above that the preposition"of" cannot always be part of the idiomatic verb "make a mess'.Consider (9) above.
In this sentence, the analysis will includesome assertions that "of" is dominated by a PP, which itself isdominated by COMP.
But if an assertion is then added to thisdescription asserting that "of" is also dominated by a verb node,then there is no consistent interpretation f this structure at all,since the COMP cannot dominate the verb node and the verbnode cannot dominate the COMP.
Put more simply, there is noway something can merely be "lowered" from a COMP node intothe verb.Another possibility similarly ruled out by D-theory is that insentences like (6.1) there is initially a PP node which dominatesboth "of" and the NP "the house", but that "of" is reanalyzedinto the idiomatic verb.
For "of" to be dominated by a verbnode, given that it is already dominated by the PP node, eitherthe PP node must be dominated by the verb or the verb by thePP node, if the dominance relations are to be consistent.
But itmakes no sense for the PP node to have a standard referentwhere it immediately dominates only a verb and an NP, but nopreposition.
And if the verb dominates the PP, then the verbalso dominates the NP which serves as the object of the VP,which is impossible.In this sense, D-theory is clearly more restrictive than the theoryof \[Lasnik and Kupin 771, at least as interpreted by \[Chomsky81 \], where reanalysis i done by adding an additional monostringto the existing Restricted Phrase Marker and eliminating others.In this case, the dominationrelations implied by the newanalysis need not be consistent with those implicit in the pre-re, analysis RPM.6.
Constraints on D-theory: a brief discussionWhile we will not discuss this issue here at length, our currentaccount of D-theory includes a set of stipulated constro;-'- 'hatfurther restrict where new domination predications can be addedto a description.
These constraints include the following: TheRightmost Daughter Constraint, that only the rightmostdaughter of a node can be lowered under a sibling node at anygiven point in the parsing process; and The No CrossoverConstraint, that no node can be lowered under a sibling which isnot contiguous to it, and some others.As viewed from the point of view of the standard referent, webelieve that a D-theory parser will appear to operate, by andlarge, just like a tree building deterministic parser, until itcreates some structure whose standard referent must bechanged.
From the parser's point of view, it will scan basetemplates left-to-right for the most part, initiating some in atop-down manner, some in a bottom-up manner, until it findsitself unable to fill the next template slot somehow or other.
Atthis point some mechanism must decide what additionalpredications to add to allow the parser to proceed.
Thefunctional force of the stipulations discussed above is to sevelelyrestrict he range of possibilities that can be considered in such asituation.
Indeed, we would be delighted if it turned out to bethe case that the parser can never consider more than severalpossibilities at any point that such an operation will beperformed.It is particularly worthy of note that these two constraintsinteract o predict that the range of constructions that can bereanalyzed in the manner discussed in the last section is severelycircumscribed, and that this prediction is borne out (see {Quirk,Greenbaum, Leech & Svartvik 72\], ?12.64).
These twoconstraints together predict that verb reanalysis is possible onlywhen a single constituent precedes the trigger for reanalysis:Suppose that there were two constituents which preceded thetrigger for reanalysis, i.e.
that the order of constituents in theVP isVCI  C2Twhere C1 and C2 are the two constituents, and T is the trigger.Then these two constituents would be attached to the VP whosehead is V before T is encountered, causing the parser (beforeattaching T) to assert two new predications which would havethe force of shifting the two constituents into the verb.
Butwhich predication could be parser add first?
If it asserts thatD(V, CI), this violates the Rightmost Daughter Constraint,because only C2 can be lowered under a sibling.
But if theparser first asserts D(V, C2) then C2 crosses over CI, which isprohibited by the No Crossover Constraint.
Therefore, onlyconstituent can have been attached before the reanalysis occurs.7.
A DETERMINISTIC APPROACH TO COORDINATIONWe now turn from the consequences of expressing syntacticstructure in terms of domination to the use of names within D-theory.
As stated above, it is this use of names which reallymakes D-theory analyses descriptions, and not merely directedacyclic graphs.
The power of naming can be demonstrated mostclearly by investigating some implications of the use of names133for the representation of coordinate constructions, i.e.conjunction phenomena and the like.7,1 ~ Problem of Coordimtte StructureCoordinate constructions are infamous for being highlyambiguous given only syntactic onstraints; standard techniquesfor parsing coordinate structures, e.g.
\[Woods 73\], are highlycombinatoric, and it would seem inherent in the phenomenonthat tree-building parsers must do extensive search to build allsyntactically possible analyses.
(See, e.g.
the analysis of\[Church & Patil 1982\].
)One widely-used approach which eliminates much of thisseemingly inherent search is to use extensive semantic andpragmatic interaction interleaved with the parsing process toquickly prune unpromising search paths.
While Parsifal madeuse of exactly such interactions in other contexts, e.g.
tocorrectly place prepositional phrases, such interactions eem todemand at least implicitly building syntactic structure which isdiscarded after some choice is made by higher-level cognitivecomponents.
Because this is counter to at least the spirit of thedeterminism hypothesis, it would be interesting if the syntacticanalysis of coordinate structures could be made autonomous ofhigher-level processes.There are more central problems for a deterministic analysis ofconjunction, however.
Techniques which make use of the look-ahead provided by buffering constituents can deterministicallyhandle a perhaps surprising range of coordinate phenomena, asfirst demonstrated by the YAP parser \[Church 80\], but thereappear to be fundamental limitations to what can be analyzed inthis way.
The central problem is that a tree buildingdeterministic parser cannot examine the context necessary todetermine what is conjoined to what without constructing nodeswhich may turn out to be spurious, given the (ultimate) correctanalysis.In what follows, we will illustrate each of these problems inmore detail and sketch an approach to the analysis of coordinatestructures which we believe can be extended to handle suchstructures deterministically and without semantic interaction.7.2 Names and Appropriste VaguenessConsider the problem of analyzing sentences like (11.1-2).These two sentences are identical at the level of preterminalsymbols; they differ only in the particular lexical items chosen asnouns, with the schematic lexical structure indicated by (11.3).However, (11.1) has the favored reading that the apples, pearsand cherries are all ripe and from local orchards, while in(11.2), only the cheese is ripe and only the cider is from localorchards.
From this, it is clear that (11.1) is read as aconjunction of three nouns within one NP, while (11.2) is readas a conjunction of three individual NPs, with structures asindicated by (l l .
Ia,2a).
We assume here, crucially, thatconstituents in coordination are all attached to the sameconstituent; they can be thought of as "stacking" in a planeorthogonal to the standard referent, as \[Chomsky 82\] suggests.The conjunction itself is attached to the rightmost of thecoordinate structures.
(l l .1) They sell ripe apples, pears, and cherries from localorchards.
(1 l. la) They sell \[NP ripe \[N apples\], \[N pears\], \[N and cherries\]from local orchards\].
(11.2) They sell ripe cheese, bread, and cider from localorchards.
(11.2a) They sell \[Np ripe cheese\], \[uP bread\], \[uP and cherriesfrom local orchards\].
(11.3) They sell ripe NI,  N2, and N3 from local orchards.Thus, it would seem that to determine the level at which thestructures are conjoined requires much pragmatic knowledgeabout fruit, flowers and the like.Note also that while (11.1-2) have particular primary readings,one needs to consider these sentences carefully to decide whatthe primary reading is.
This is suggestive of the kind ofsyntactic vagueness that VanLehn argues characterizes manyjudgements of quantifier scope \[VanLehn 78\].
Note, however,that most evidence suggests that quantifier scope is notrepresented irectly in syntactic structure, but is interpretedfrom that structure.
For the readings of (11.1-2) to be vague inthis way, the structures of (I l.la-2a) must be interpreted fromsyntactic structure, and not be part of it.
It turns out that D-theory, coupled with the assumption that the parser does notinteract with semantic and pragmatic processing, provides anaccount which is consistent with these intuitions.But consider the D-theoretic analysis of (11.1); there are somesurprises in store.
Its representation will include predicationslike those of (12.1-8), where we are now careful to "unpack"informal names like "npl" to show that they consist of acontent-free identifier and predications about the type of entitythe identifier names.
(12.1) D(vpl, npl); VP(vpl); NP(npl)(12.2) D(vpl, np2); NP(np2)(12.3) D(vpl, np3); NP(np3)(12.4) D(npl, apl); D(apl, adjl); ADJ(adjl)(12.5) D(npl, hi); NOUN(hi)(12.6) D(np2, n2); NOUN(n2)(12.7) D(np3, n3); NOUN(n3)(12.8) D(np3, ppl): D(ppl, prept); PREP(prepl)(12.9) adjl < nl < n2 < n3 < preplHere vpl is the name of a node whose head is "sell", apl anadjective phrase dominating "ripe", and ppl the PP "from localorchards."
The analysis will also include predications about, theleft-to-right order of the terminal string, which has beeninformally represented in (12.9); +X < Y" is to be read +X isthe left of Y".
We indicate the order of nonterminals here onlyfor the sake of brevity; we usenl <n2as a shorthand forD(nl, 'cheese'); D(n2, 'bread'); 'cheese' < 'bread'.In particular, a D-theory analysis contains no explicitpredications about left-right order of non-terminals.But given only the predications in (12), what can be said aboutthe identities of the nodes named npl, np2, and np3?
Underthis description, the descriptions of npl, np2 and np3 arecompatible descriptions; they are potentially descriptions of thesame individual.
They are all dominated by vpl, and each is an134NP, so there is no conflict here, Each dominates a differentnoun, but several constituents of the same type can bedominated by the same node if they are in a coordinate structure(given the analysis of coordinate structures we assume) and ifthey are string adjacent.
NI, n2 and n3 are string adjacent(given only (12)), so the fact that the nodes named npl, np2and np3 dominate nouns which may turn out to be different doesnot make the descriptions of the NPs incompatible.
(Indeed, ifthe nouns are viewed as a coordinate structure, then thestructure of the nouns is the same as that of (11.1).
)Furthermore, adjl is immediately to the left of and ppl isimmediately to the right of all the nouns, so these constituentscould be dominated by the same single NP that might dominatehi,  n2 and n3 as well.
Thus there is no information here thatcan distinguish npl from np2 from np3.The fact that the conjunction "and" is dominated by np3 doesnot block the above analysis.
The addition of one dominationpredicate leaves it dominated by n3 (as well as np3, of course),thereby making n l, n2 and n3 a perfect coordinate structure,and leaving no barrier to npl, np2 and np3 being co-referent,But this means that the D-theory analysis of (11.1) has asstandard referents both it and (11.2)!
(This modifies ourstatement earlier in this paper about the uniqueness of thestandard referent; we now must say that for each possible"stacking" of nodes, there is one standard referent.)
For if npl,np2 and np3 corefer, then the analysis above shows that thestructure described is exactly that of (11.2).
There is also thepossibility that just npl and np2 corefer, given the aboveanalysis, which yields a reading where np2 is an appositive tonpl, with npl and np3 coordinate structures (the structure ofappositives is similar to that of coordinate structures, weassume); and the possibility that just np2 and np3 corefer,yielding a reading with npl and np2 coordinate structures, andnp3 in apposition to np2.
(The fact that we use a simplifiedphrase structure here is not an important fact.
The analysisgoes through equally as well with a full X-bar theoretic phrasecomponent; he story is just much longer.
)The upshot of this is that upon encountering constructions like(11), the parser can proceed by simply assuming that thestructures are conjoined at the highest level possible, usingdifferent names for each of the potential highest levelconstituents.
It can then analyze the (potentially) coordinatestructures entirely independently of feedback from pragmaticand semantic knowledge sources.
When higher cognitiveprocessing of this description requires distinguishing at whatlevel the structures are conjoined, pragmatics can be invokedwhere needed, but there need be no interaction with syntacticprocesses themselves.
This is because, once again, it turns out ifit is syntactically possible that structures hould be conjoined ata lower level than that initially posited, the names of thepotentially separate constituents simply can be viewed as aliasesof the one node that does exist in the corresponding standardreferent; in this case all predications about whatever node isnamed by the alias remain true, and thus once again nopredications need to be revoked.We now see how it is that D-theory gives an account of theintuition that the fine structure of coordinations in vague, in thesense of VanLehn.
For we have seen that pragmatics does notneed to determine whether (e.g.)
all the fruits in (11.1) are ripeor not for the syntactic analysis to be completeddeterministically, exactly because the D-theory analysis leavesall (and, we also claim, only) the syntactically correctpossibilities open.
Thus the description given in (12) isappropriately vague between possible syntactic analyses ofsentences like those schematized in (11.3).
Thus, this newrepresentation pens the way for a simple formal expression ofthe notion that some sentences may be vague in certain welldefined ways, even though they are believed to be understood,and that this vagueness may not be resolved until a hearer'sattention is called to the unresolved ecision.7.3 The Problem of Nodes That Aren't There.While we can give only the briefest sketch here (the full story isquite long and complicated), exactly this use of names resolvesyet another problem for the deterministic analysis of coordinatestructures: To examine enough context (in the buffer) to decidewhat kind of structure is conjoined with what, a troe-buildingparser will often have to go out on a limb and posit the existenceof nodes which may turn out not to exist after all.
For example,if a tree-building parser has analyzed the inputs shown in(13.1-2) up to "worms" and has seen "and" and "frogs" in the(13.1) Birds eat small worms and frogs eat small flies.
(13.2) Birds eat small worms and frogs.buffer, it will need to posit that "frogs" is a full NP to check tosee if the pattern\[conjunction\] \[NPI \[verblis fulfilled, and thus if an S should be created with the NP as itshead.
But if the input is not as in (13.1), but as in (13.2), thenpositing the NP might be incorrect, because the correct analysismay be a noun-noun conjunction of "worms" and "frogs', (withthe reading that birds eat worms and frogs, both of which aresmall).Of course, there is a second problem here for a tree-buildingparser, namely that (13.2) has a second reading which is an"NP and NP" conjunction.
As we have seen above, there is nocorresponding problem for a D-theory parser, because if i tmerely posits an NP dominating "frogs', the structure which willresult for (13.2) is appropriately vague between both the NPreading and the noun reading of "frogs" (i.e.
between thereadings where the frogs are just plain frogs and where the frogsare small.
)But the solution to the second problem for a D-theory parser isalso a solution to the first!
After seeing "and" and "frogs" in itsbuffer, a D-theory parser can simply posit an NP nodedominating "frogs" and continue.
If the input proceeds as in(13.1), then the parser will introduce an S node and assert thatit dominates the new NP.
This will make the descriptions of theNPs dominating "worms" and dominating "frogs" incompatible,i.e.
this will assure that there really are two NPs in the standardreferent.
If the input proceeds as in (13.2), a D-theory parserwill state that the node referred to by the new name isdominated by the previous VP, resulting in the structuredescribed immediately above.
To summarize, where a tree-building parser might be misled into creating a node whichmight not exist at all, there is no corresponding problem for aD-theory parser.8.
SUMMING UP'.
D-Theory on One FootThis paper has described a new theory of natural languagesyntax and parsing which argues that the proper output ofsyntactic analysis is not a tree structure per se, but rather adescription of such structures.
Rather than constructing a tree,a natural language parser based on these ideas will construct a135single description which can be viewed as a partial descriptionof each of a family of trees.The two key ideas that we have presented here arc:(1) An analysis of a syntactic structure consists primarily ofpredications of the form "node X dominates node Y', and notthe more traditional "node.
X immediately dominates node Y';syntactic analysis never says more than that node X issomewhere above node Y.
(2) Because this is a description, two names used to refer tosyntactic structures can always co-refer if their descriptions arecompatible, and furthermore, it is impossible to block thepossibility of coreferenec if the descriptions are compatible.These two ideas, taken together, imply that during the process ofanalyzing the structure of a given utterance, merely adding tothe emerging description may change the set of trees ultimatelydescribed (just as adding "honest" to the phrase "all politicians"may radically change the set described).
We have also sketchedsome implications of this theory that not only suggest a newanalysis of coordinate structures, but also suggest thatcoordinate structures might be much easier to analyze thancurrent parsing techniques would suggest.We are currently working to flesh out the analyses presentedabove.
We arc also working on an analysis of gapping andelision phenomena which seems to fall naturally out of thisframework.
This new analysis is surprising in that it makescrucially use of descriptions even less fully specified than thosewe have discussed in this paper, by using the notations we haveintroduced here to fuller advantage.
These emerging analysesmove yet further away from the traditional view of either treesor phrase markers as an appropriate framework for expressingsyntactic generalizations.9.
ReferencesBerwick, R. (1982) Locality Principles and the Acquisition ofSyntactic Knowledge, MIT PhD thesis.Bresnan, J.
(1982) -The Passive in Lexical Theory," in J.Bresnan (ed.)
The Mental Representation of GrammaticalRelations, MIT Press, pp.
3-86.Chomsky, N. (1981) Lectures on Government and Binding,Foris Publications.Chomsky, N. (1982) Some Concepts and Consequences of theTheory of Government and Binding, MIT Press.Church, K. (1980) "On Memory Limitations in NaturalLanguage Processing," MIT Masters thesis, MIT/LCS/TR-245.Church, K. and R. Patil (1982) "Coping with SyntacticAmbiguity or How to Put the Block in the Box on the Table,"MIT/LCS/TM-216.Hindle, D. (1983) "Deterministic Parsing of Syntactic Non-fluencies," this proceedings.Horustein, N. and A. Weinberg (1981) "Case Theory andPreposition Stranding," Linguistic Inquiry, 12.1, pp.
55-91.Lasnik, H. and J. Kapin (1977) "A Restrictive Theory ofTransformational Grammar," Theoretical Linguistics, vol.
4, pp.173-196.McDonald, D. (1983) "Natural Language Generation as aComputational Problem: an Introduction," in M. Brady and R.Berwick (eds.)
Computational Models of Discourse, MIT  Press,pp.
209-265.Marcus, M. (1980) A Theory of Syntactic Recognition forNatural Language, MIT Press.Quirk, R., S. Greenbaum, G. Leech and J. Svartik (1972) ,4Grammar of Contemporary English, Longman.Shipman, D. (1979) "Phrase Structure Rules for Parsifal', MITAI Lab Working Paper 182Shipman, D. and M. Marcus (1979) "Towards Minimal DataStructures for Deterministic Parsing,' IJCAI79.VanLehn, K.A.
(1978) "Determining the Scope of EnglishQuantifiers', MIT AI-TR-483.Woods, W.A.
(1973).
"An Experimental Parsing System forTransition Network Grammars."
in R. Rustin, ed., NaturalLanguage Processing, Algorithmics Press.136
