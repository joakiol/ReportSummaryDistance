Proceedings of ACL-08: HLT, pages 807?815,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsAn Unsupervised Approach to Biography Production using WikipediaFadi Biadsy,?
Julia Hirschberg?
and Elena Filatova*?Department of Computer ScienceColumbia University, New York, NY 10027, USA{fadi,julia}@cs.columbia.edu*InforSense LLCCambridge, MA 02141, USAefilatova@inforsense.comAbstractWe describe an unsupervised approach tomulti-document sentence-extraction basedsummarization for the task of producingbiographies.
We utilize Wikipedia to auto-matically construct a corpus of biographicalsentences and TDT4 to construct a corpusof non-biographical sentences.
We build abiographical-sentence classifier from thesecorpora and an SVM regression model forsentence ordering from the Wikipedia corpus.We evaluate our work on the DUC2004evaluation data and with human judges.Overall, our system significantly outperformsall systems that participated in DUC2004,according to the ROUGE-L metric, and ispreferred by human subjects.1 IntroductionProducing biographies by hand is a labor-intensivetask, generally done only for famous individuals.The process is particularly difficult when persons ofinterest are not well known and when informationmust be gathered from a wide variety of sources.
Wepresent an automatic, unsupervised, multi-documentsummarization (MDS) approach based on extractivetechniques to producing biographies, answering thequestion ?Who is X?
?There is growing interest in automatic MDS ingeneral due in part to the explosion of multilingualand multimedia data available online.
The goal ofMDS is to automatically produce a concise, well-organized, and fluent summary of a set of docu-ments on the same topic.
MDS strategies have beenemployed to produce both generic summaries andquery-focused summaries.
Due to the complexityof text generation, most summarization systems em-ploy sentence-extraction techniques, in which themost relevant sentences from one or more docu-ments are selected to represent the summary.
Thisapproach is guaranteed to produce grammatical sen-tences, although they must subsequently be orderedappropriately to produce a coherent summary.In this paper we describe a sentence-extractionbased MDS procedure to produce biographies fromonline resources automatically.
We make use ofWikipedia, the largest free multilingual encyclope-dia on the internet, to build a biographical-sentenceclassifier and a component for ordering sentences inthe output summary.
Section 2 presents an overviewof our system.
In Section 3 we describe our cor-pus and in Section 4 we discuss the components ofour system in more detail.
In Section 5, we presentan evaluation of our work on the Document Under-standing Conference of 2004 (DUC2004), the biog-raphy task (task 5) test set.
In Section 6 we com-pare our research with previous work on biographygeneration.
We conclude in Section 7 and identifydirections for future research.2 System OverviewIn this section, we present an overview of our biog-raphy extraction system.
We assume as input a set ofdocuments retrieved by an information retrieval en-gine from a query consisting of the name of the per-son for whom the biography is desired.
We furtherassume that these documents have been tagged withNamed Entities (NE)s with coreferences resolved807using a system such as NYU?s 2005 ACE system(Grishman et al, 2005), which we used for our ex-periments.
Our task is to produce a concise biogra-phy from these documents.First, we need to select the most ?important?
bio-graphical sentences for the target person.
To do so,we first extract from the input documents all sen-tences that contain some reference to the target per-son according to the coreference assignment algo-rithm; this reference may be the target?s name ora coreferential full NP or pronominal referring ex-pression, such as the President or he.
We call thesesentences hypothesis sentences.
We hypothesize thatmost ?biographical?
sentences will contain a refer-ence to the target.
However, some of these sentencesmay be irrelevant to a biography; therefore, we filterthem using a binary classifier that retains only ?bio-graphical?
sentences.
These biographical sentencesmay also include redundant information; therefore,we cluster them and choose one sentence from eachcluster to represent the information in that cluster.Since some of these sentences have more salient bi-ographical information than others and since manu-ally produced biographies tend to include informa-tion in a certain order, we reorder our summary sen-tences using an SVM regression model trained onbiographies.
Finally, the first reference to the tar-get person in the initial sentence in the reorderingis rewritten using the longest coreference in our hy-pothesis sentences which contains the target?s fullname.
We then trim the output to a threshold to pro-duce a biography of a certain length for evaluationagainst the DUC2004 systems.3 Training DataOne of the difficulties inherent in automatic biog-raphy generation is the lack of training data.
Onemight collect training data by manually annotatinga suitable corpus containing biographical and non-biographical data about a person, as in (Zhou et al,2004).
However, such annotation is labor intensive.To avoid this problem, we adopt an unsupervised ap-proach.
We use Wikipedia biographies as our corpusof ?biographical?
sentences.
We collect our ?non-biographical?
sentences from the English newswiredocuments in the TDT4 corpus.1 While each corpus1http://projects.ldc.upenn.edu/TDT4may contain positive and negative examples, we as-sume that most sentences in Wikipedia biographiesare biographical and that the majority of TDT4 sen-tences are non-biographical.3.1 Constructing the Biographical CorpusTo automatically collect our biographical sentences,we first download the xml version of Wikipediaand extract only the documents whose authors usedthe Wikipedia biography template when creatingtheir biography.
There are 16,906 biographies inWikipedia that used this template.
We next applysimple text processing techniques to clean the text.We select at most the first 150 sentences from eachpage, to avoid sentences that are not critically impor-tant to the biography.
For each of these sentences weperform the following steps:1.
We identify the biography?s subject from its ti-tle, terming this name the ?target person.?2.
We run NYU?s 2005 ACE system (Grish-man et al, 2005) to tag NEs and do coref-erence resolution.
There are 43 unique NEtags in our corpora, including PER Individual,ORG Educational, and so on, and TIMEX tagsfor all dates.3.
For each sentence, we replace each NE by itstag name and type ([name-type subtype]) as as-signed by the NYU tagger.
This modified sen-tence we term a class-based/lexical sentence.4.
Each non-pronominal referring expression(e.g., George W. Bush, the US president) thatis tagged as coreferential with the target per-son is replaced by our own [TARGET PER] tagand every pronoun P that refers to the targetperson is replaced by [TARGET P], where P isthe pronoun itself.
This allows us to general-ize our sentences while retaining a) the essen-tial distinction between this NE (and its role inthe sentence) and all other NEs in the sentence,and b) the form of referring expressions.5.
Sentences containing no reference to the tar-get person are assumed to be irrelevant and re-moved from the corpus, as are sentences with808fewer than 4 tokens; short sentences are un-likely to contain useful information beyond thetarget reference.For example, given sentences from the Wikipediabiography of Martin Luther King, Jr. we produceclass-based/lexical sentences as follows:Martin Luther King, Jr., was born on January 15, 1929, in Atlanta,Georgia.
He was the son of Reverend Martin Luther King, Sr. andAlberta Williams King.
He had an older sister, Willie Christine(September 11, 1927) and a younger brother, Albert Daniel.
[TARGET PER], was born on [TIMEX], in [GPE PopulationCenter].
[TARGET HE] was the son of [PER Individual] and [PER Individual].
[TARGET HE] had an older sister, [PER Individual] ([TIMEX]) and ayounger brother, [PER Individual].3.2 Constructing the Non-Biographical CorpusWe use the TDT4 corpus to identify non-biographical sentences.
Again, we run NYU?s 2005ACE system to tag NEs and do coreference resolu-tion on each news story in TDT4.
Since we haveno target name for these stories, we select an NEtagged as PER Individual at random from all NEs inthe story to represent the target person.
We excludeany sentence with no reference to this target personand produce class-based/lexical sentences as above.4 Our Biography Extraction System4.1 Classifying Biographical SentencesUsing the biographical and non-biographical cor-pora described in Section 3, we train a binary classi-fier to determine whether a new sentence should beincluded in a biography or not.
For our experimentswe extracted 30,002 sentences from Wikipedia bi-ographies and held out 2,108 sentences for test-ing.
Similarly.
we extracted 23,424 sentences fromTDT4, and held out 2,108 sentences for testing.For each sentence, we then extract the frequency ofthree class-based/lexical features ?
unigram, bia-gram, and trigram ?
and two POS features ?
thefrequency of unigram and bigram POS.
To reducethe dimensionality of our feature space, we first sortthe features in decreasing order of Chi-square statis-tics computed from the contingency tables of the ob-served frequencies from the training data.
We thentake the highest 30-80% features, where the num-ber of features used is determined empirically forClassifier Accuracy F-MeasureSVM 87.6% 0.87M.
na?
?ve Bayes 84.1% 0.84C4.5 81.8% 0.82Table 1: Binary classification results: Wikipedia bi-ography class-based/lexical sentences vs. TDT4 class-based/lexical sentenceseach feature type.
This process identifies featuresthat significantly contribute to the classification task.We extract 3K class-based/lexical unigrams, 5.5Kbigrams, 3K trigrams, 20 POS unigrams, and 166POS bigrams.Using the training data described above, we ex-perimented with three different classification algo-rithms using the Weka machine learning toolkit(Witten et al, 1999): multinomial na?
?ve Bayes,SVM with linear kernel, and C4.5.
Weka also pro-vides a classification confidence score that repre-sents how confident the classifier is on each classi-fied sample, which we will make use of as well.Table 1 presents the classification results on our4,216 held-out test-set sentences.
These results arequite promising.
However, we should note that theymay not necessarily represent the successful clas-sification of biographical vs. non-biographical sen-tences but rather the classification of Wikipedia sen-tences vs. TDT4 sentences.
We will validate theseresults for our full systems in Section 5.4.2 Removing Redundant SentencesTypically, redundancy removal is a standard com-ponent in MDS systems.
In sentence-extractionbased summarizers, redundant sentences are definedas those which include the same information with-out introducing new information and identified bysome form of lexically-based clustering.
We usean implementation of a single-link nearest neighborclustering technique based on stem-overlap (Blair-Goldensohn et al, 2004b) to cluster the sentencesclassified as biographical by our classifier, and thenselect the sentence from each cluster that maximizesthe confidence score returned by the classifier as therepresentative for that cluster.4.3 Sentence ReorderingIt is essential for MDS systems in the extractionframework to choose the order in which sentences809should be presented in the final summary.
Present-ing more important information earlier in a sum-mary is a general strategy for most domains, al-though importance may be difficult to determine re-liably.
Similar to (Barzilay and Lee, 2004), we au-tomatically learn how to order our biographical sen-tences by observing the typical order of presentationof information in a particular domain.
We observethat our Wikipedia biographies tend to follow a gen-eral presentation template, in which birth informa-tion is mentioned before death information, infor-mation about current professional position and af-filiations usually appear early in the biography, andnuclear family members are typically mentioned be-fore more distant relations.
Learning how to orderinformation from these biographies however wouldrequire that we learn to identify particular types ofbiographical information in sentences.We directly use the position of each sentence ineach Wikipedia biography as a way of determin-ing where sentences containing similar informationabout different target individuals should appear intheir biographies.
We represent the absolute posi-tion of each sentence in its biography as an inte-ger and train an SVM regression model with RBFkernel, from the class/lexical features of the sen-tence to its position.
We represent each sentence bya feature vector whose elements correspond to thefrequency of unigrams and bigrams of class-baseditems (e.g., GPE, PER) (cf.
Section 3) and lexicalitems; for example, the unigrams born, became, and[GPE State-or-Province], and the bigrams was born,[TARGET PER] died and [TARGET PER] joinedwould be good candidates for such features.To minimize the dimensionality of our regres-sion space, we constrained our feature choice tothose features that are important to distinguish bi-ographical sentences, which we term biographicalterms.
Since we want these biographical terms toimpact the regression function, we define these tobe phrases that consist of at least one lexical itemthat occurs in many biographies but rarely more thanonce in any given biography.
We compute the bio-graphical term score as in the following equation:bio score(t)= | Dt || D | ?
?d?Dt(1?n(t)dmaxt(n(t)d) )| D | (1)where D is the set of 16,906 Wikipedia biographies,n(t)d is the number of occurrences of term t in doc-ument d, and Dt = {d ?
D : t ?
d}.
The left factorrepresents the document frequency of term t, and theright factor calculates how infrequent the term is ineach biography that contains t at least once.2 We or-der the unigrams and bigrams in the biographies bytheir biographical term scores and select the high-est 1K unigrams and 500 bigrams; these thresholdswere determined empirically.4.4 Reference RewritingWe observe that news articles typically mention bio-graphical information that occurs early in Wikipediabiographies when they mention individuals for thefirst time in a story (e.g.
Stephen Hawking, the Cam-bridge University physicist).
We take advantage ofthe fact that the coreference resolution system weuse tags full noun phrases including appositives aspart of NEs.
Therefore, we initially search for thesentence that contains the longest identified NE (oftype PER) that includes the target person?s full nameand is coreferential with the target according to thereference resolution system; we denote this NE NE-NP.
If this sentence has already been classified asa biographical sentence by our classifier, we simplyboost its rank in the summary to first.
Otherwise,when we order our sentences, we replace the refer-ence to the target person in the first sentence by NE-NP.
For example, if the first sentence in the biogra-phy we have produced for Jimmy Carter is He wasborn in 1947 and a sentence not chosen for inclusionin our biography Jimmy Carter, former U.S. Presi-dent, visited the University of California last year.contains the NE-NP, and Jimmy Carter and He arecoreferential, then the first sentence in our biographywill be rewritten as Jimmy Carter, former U.S. Presi-dent, was born in 1947.
Note that, in the evaluationspresented in Section 5, sentence order was modifiedby this process in only eight summaries.5 EvaluationTo evaluate our biography generation system, we usethe document sets created for the biography evalua-2We considered various approaches to feature selection here,such as comparing term frequency between our biographicaland non-biographical corpora.
However, terms such as killedand died, which are useful biographical terms, also occur fre-quently in our non-biographical corpus.810ROUGE-L Average_F0.250.2750.30.3250.350 1 2 3 4 5 6 7 8 9 10 11 12SVMreg.
onlytop-DUC2004 C4.5 SVM SVM  + SVM reg.
MNB +SVM regMNBC4.5  +  SVM reg.
SVM + baseline orderC4.5 +   baseline order MNB +   baseline orderFigure 1: Comparing our approaches against the top performing system in DUC2004 according to ROUGE-L (dia-mond).tion (task 5) of DUC2004.3 The task for systemsparticipating in this evalution was ?
Given each doc-ument cluster and a question of the form ?Who isX?
?, where X is the name of a person or group ofpeople, create a short summary (no longer than 665bytes) of the cluster that responds to the question.
?NIST assessors chose 50 clusters of TREC docu-ments such that all the documents in a given clusterprovide at least part of the answer to this question.Each cluster contained on average 10 documents.NIST had 4 human summaries written for each clus-ter.
A baseline summary was also created for eachcluster by extracting the first 665 bytes of the mostrecent document in the cluster.
22 systems partici-pated in the competition, producing a total of 22 au-tomatic summaries (restricted to 665 bytes) for eachcluster.
We evaluate our system against the top per-forming of these 22 systems, according to ROUGE-L, which we denote top-DUC2004.45.1 Automatic Evaluation Using ROUGEAs noted in Section 4.1, we experimented with anumber of learning algorithms when building ourbiographical-sentence classifier.
For each machinelearning algorithm tested, we build a system that ini-tially classifies the input list of sentences into bio-graphical and non-biographical sentences and then3http://duc.nist.gov/duc20044Note that this system out-performed 19 of the 22 systemson ROUGE-1 and 20 of 22 on ROUGE-L and ROUGE-W-1.2(p < .05) (Blair-Goldensohn et al, 2004a).
No ROUGE metricproduced scores where this system scored significantly worsethan any other system.
See Figure 2 below for a comparisonof all DUC2004 systems with our top system where all systemsare evaluated using ROUGE-L-1.5.5.removes redundant sentences.
Next, we producethree versions of each system: one which imple-ments a baseline ordering procedure, in which sen-tences from the clusters are ordered by their ap-pearance in their source document (e.g.
any sen-tence which occurred first in its original documentis placed first in the summary, with ties ordered ran-domly within the set), a second which orders thebiographical sentences by the confidence score ob-tained from the classifier, and a third which uses theSVM regression as the reordering component.
Fi-nally, we run our reference rewriting component oneach and trim the output to 665 bytes.We evaluate first using the ROUGE-L metric (Linand Hovy, 2003) with a 95% (ROUGE computed)confidence interval for all systems and comparedthese to the ROUGE-L score of the best-performingDUC2004 system.5 The higher the ROUGE score,the closer the summary is to the DUC2004 humanreference summaries.
As shown in Figure 1, ourbest performing system is the multinomial na?
?veBayes classifier (MNB) using the classifier confi-dence scores to order the sentences in the biography.This system significantly outperforms the top rankedDUC2004 system (top-DUC2004).6 The success ofthis particularly learning algorithm on our task maybe due to: (1) the nature of our feature space ?
n-gram frequencies are modeled properly by a multi-nomial distribution; (2) the simplicity of this classi-fier particularly given our large feature dimensional-5We used the same version (1.5.5) of the ROUGE metric tocompute scores for the DUC systems and baseline also.6Significance for each pair of systems was determined bypaired t-test and calculated at the .05 significance level.811ity; and (3) the robustness of na?
?ve Bayes with re-spect to noisy data: Not all sentences in Wikipediabiographies are biographical sentences and somesentences in TDT4 are biographical.While the SVM regression reordering componenthas a slight negative impact on the performanceof the MNB system, the difference between thetwo versions is not significant.
Note however, thatboth the C4.5 and the SVM versions of our systemare improved by the SVM regression sentence re-ordering.
While neither performs better than top-DUC2004 without this component, the C4.5 systemwith SVM reordering is significantly better than top-DUC2004 and the performance of the SVM sys-tem with SVM regression is comparable to top-DUC2004.
In fact, when we use only the SVMregression model to rank the hypothesis sentences,without employing any classifier, then remove re-dundant sentences, rewrite and trim the results, wefind that, interestingly, this approach also outper-forms top-DUC2004, although the difference is notstatistically significant.
However, we believe thatthis is an area worth pursuing in future, with moresophisticated features.The following biography of Brian Jones was pro-duced by our MNB system and then the sentenceswere ordered using the SVM regression model:Born in Bristol in 1947, Brian Jones, the co-pilot on theBreitling mission, learned to fly at 16, dropping out ofschool a year later to join the Royal Air Force.
After earn-ing his commercial balloon flying license, Jones becamea ballooning instructor in 1989 and was certified as an ex-aminer for balloon flight licenses by the British Civil Avi-ation Authority.
He helped organize Breitling?s most re-cent around-the-world attempts, in 1997 and 1998.
Jones,52, replaces fellow British flight engineer Tony Brown.Jones, who is to turn 52 next week, is actually the team?sthird co-pilot.
After 13 years of service, he joined a cater-ing business and, in the 1980s,...Figure 2 illustrates the performance of our MNBsystem with classifier confidence score sentence or-dering when compared to mean ROUGE-L-1.5.5scores of DUC2004 human-generated summariesand the 22 DUC2004 systems?
summaries across allsummary tasks.
Human summaries are labeled A-H, DUC2004 systems 1-22, and our MNB systemis marked by the rectangle.
Results are sorted bymean ROUGE-L score.
Note that our system perfor-mance is actually comparable in ROUGE-L score toone of the human summary generators and is signif-icantly better that all DUC2004 systems, includingtop-DUC2004, which is System 1 in the figure.5.2 Manual EvaluationROUGE evaluation is based on n-gram overlap be-tween the automatically produced summary and thehuman reference summaries.
Thus, it is not able tomeasure how fluent or coherent a summary is.
Sen-tence ordering is one factor in determining fluencyand coherence.
So, we conducted two experimentsto measure these qualities, one comparing our top-performing system according to ROUGE-L score(MNB) vs. the top-performing DUC2004 system(top-DUC2004) and another comparing our top sys-tem with two different ordering methods, classifier-based and SVM regression.7 In each experiment,summaries were trimmed to 665 bytes.In the first experiment, three native American En-glish speakers were presented with the 50 questions(Who is X?).
For each question they were given apair of summaries (presented in random order): onewas the output of our MNB system and the otherwas the summary produced by the top-DUC2004system.
Subjects were asked to decide which sum-mary was more responsive in form and content to thequestion or whether both were equally responsive.85.3% (128/150) of subject judgments preferred onesummary over the other.
100/128 (78.1%) of thesejudgments preferred the summaries produced by ourMNB system over those produced by top-DUC2004.If we compute the majority vote, there were 42/50summaries in which at least two subjects made thesame choice.
37/42 (88.1%) of these majority judg-ments preferred our system?s summary (using bino-mial test, p = 4.4e?7).
We used the weighted kappastatistic with quadratic weighting (Cohen, 1968)to determine the inter-rater agreement, obtaining amean pairwise ?
of 0.441.Recall from Section 5.1 that our SVM regressionreordering component slightly decreases the aver-age ROUGE score (although not significantly) forour MNB system.
For our human evaluations, wedecided to evaluate the quality of the presentationof our summaries with and without this compo-7Note that top-DUC2004 was ranked sixth in the DUC 2004manual evaluation, with no system performing significantlybetter for coverage and only 1 system performing significantlybetter for responsiveness.812ROUGE-L Average_F0.20.250.30.350.40.45B E H G F A D C * 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 BL 18 19 20 21 22Figure 2: ROUGE-L scores for DUC2004 human summaries (A-H), our MNB system (rectangle), and the DUC2004competing systems (1-22 anonymized), with the baseline system labeled BL.nent to see if this reordering component affected hu-man judgments even if it did not improve ROUGEscores.
For each question, we produced two sum-maries from the sentences classified as biographi-cal by the MNB classifier, one ordered by the con-fidence score obtained by the MNB, in decreasingorder, and the other ordered by the SVM regressionvalues, in increasing order.
Note that, in three cases,the summary sentences were ordered identically byboth procedures, so we used only 47 summariesfor this evaluation.
Three (different) native Amer-ican English speakers were presented with the 47questions for which sentence ordering differed.
Foreach question they were given the two summaries(presented in random order) and asked to determinewhich biography they preferred.We found inter-rater agreement for these judg-ments using Fleiss?
kappa (Fleiss, 1971) to be onlymoderate (?=0.362).
However, when we computedthe majority vote for each question, we found that61.7% (29/47) preferred the SVM regression order-ing over the MNB classifier confidence score order-ing.
Although this difference is not statistically sig-nificant, again we find the SVM regression orderingresults encouraging enough to motivate our furtherresearch on improving such ordering procedures.6 Related WorkThe DUC2004 system achieving the highest over-all ROUGE score, our top-DUC2004 in Section 5,was Blair-Goldensohn et al (2004a)?s DefScriber,which treats ?Who is X??
as a definition questionand targets definitional themes (e.g.
genus-species)found in the input document collections which in-clude references to the target person.
Extracted sen-tences are then rewritten using a reference rewritingsystem (Nenkova and McKeown, 2003) which at-tempts to shorten subsequent references to the tar-get.
Sentences are ordered in the summary basedon a weighted combination of topic centrality, lex-ical cohesion, and topic coverage scores.
A simi-lar approach is explored in Biryukov et al (2005),which uses Topic Signatures (Lin and Hovy, 2000)constructed around the target individual?s name toidentify sentences to be included in the biography.Zhou et al (2004)?s biography generation system,like ours, trains biographical and non-biographicalsentence classifiers to select sentences to be includedin the biography.
Their system is trained on a hand-annotated corpus of 130 biographies of 12 people,tagged with 9 biographical elements (e.g., bio, ed-ucation, nationality) and uses binary unigram andbigram lexical and unigram part-of-speech featuresfor classification.
Duboue et al (2003) also ad-dress the problem of learning content selection rulesfor biography.
They learn rules from two corpora,a semi-structured corpus with lists of biographicalfacts about show business celebrities and a corpusof free-text biographies about the same celebrities.Filatova et al (2005) learn text features typicalof biographical descriptions by deducing biograph-ical and occupation-related activities automaticallyby compariing descriptions of people with differ-ent occupations.
Weischedel et al (2004) modelskernel-fact features typical for biographies using lin-guistic and semantic processing.
Linguistic features813are derived from predicate-argument structures de-duced from parse trees, and semantic features are theset of biography-related relations and events definedin the ACE guidelines (Doddington et al, 2004).Sentences containing kernel facts are ranked usingprobabilities estimated from a corpus of manuallycreated biographies, including Wikipedia, to esti-mate the conditional distribution of relevant materialgiven a kernel fact and a background corpus.The problem of ordering sentences and preserv-ing coherence in MDS is addressed by Barzi-lay et al (2001), who combine chronological order-ing of events with cohesion metrics.
SVM regres-sion has recently been used by (Li et al, 2007) forsentence ranking for general MDS.
The authors cal-culated a similarity score for each sentence to thehuman summaries and then regress numeric features(e.g., the centroid) from each sentence to this score.Barzilay and Lee (2004) use HMMs to capture topicshift within a particular domain; sequence of topicshifts then guides the subsequent ordering of sen-tences within the summary.7 Discussion and Future WorkIn this paper, we describe a MDS system for produc-ing biographies, given a target name.
We present anunsupervised approach using Wikipedia biographypages and a general news corpus (TDT4) to automat-ically construct training data for our system.
We em-ploy a NE tagger and a coreference resolution sys-tem to extract class-based and lexical features fromeach sentence which we use to train a binary classi-fier to identify biographical sentences.
We also trainan SVM regression model to reorder the sentencesand then employ a rewriting heuristic to create thefinal summary.We compare versions of our system based uponthree machine learning algorithms and two sentencereordering strategies plus a baseline.
Our best per-forming system uses the multinomial na?
?ve Bayes(MNB) classifier with classifier confidence score re-ordering.
However, our SVM regression reorder-ing improves summaries produced by the other twoclassifiers and is preferred by human judges.
Wecompare our MNB system on the DUC2004 bi-ography task (task 5) to other DUC2004 systemsand to human-generated summaries.
Our systemout-performs all DUC2004 systems significantly,according to ROUGE-L-1.5.5.
When presentedwith summaries produced by our system and sum-maries produced by the best-performing (accordingto ROUGE scores) of the DUC2004 systems, humanjudges (majority vote of 3) prefer our system?s bi-ographies in 88.1% of cases.In addition to its high performance, our approachhas the following advantages: It employs no manualannotation but relies upon identifying appropriatelydifferent corpora to represent our training corpus.It employs class-based as well as lexical featureswhere the classes are obtained automatically froman ACE NE tagger.
It utilizes automatic corefer-ence resolution to identify sentences containing ref-erences to the target person.
Our sentence reorder-ing approaches make use of either classifier confi-dence scores or ordering learned automatically fromthe actual ordering of sentences in Wikipedia biogra-phies to determine the order of presentation of sen-tences in our summaries.Since our task is to produce concise summaries,one focus of our future research will be to simplifythe sentences we extract before classifying themas biographical or non-biographical.
This proce-dure should also help to remove irrelevant informa-tion from sentences.
Recall that our SVM regres-sion model for sentence ordering was trained usingonly biographical class-based/lexical items.
In fu-ture, we would also like to experiment with morelinguistically-informed features.
While Wikipediadoes not enforce any particular ordering of infor-mation in biographies, and while different biogra-phies may emphasize different types of information,it would appear that the success of our automaticallyderived ordering procedures may capture some un-derlying shared view of how biographies are written.The same underlying views may also apply to do-mains such as organization descriptions or types ofhistorical events.
In future we plan to explore such ageneralization of our procedures to such domains.AcknowledgmentsWe thank Kathy McKeown, Andrew Rosenberg, Wisam Dakka, and theSpeech and NLP groups at Columbia for useful discussions.
This mate-rial is based upon work supported by the Defense Advanced ResearchProjects Agency (DARPA) under Contract No.
HR001106C0023 (ap-proved for public release, distribution unlimited).
Any opinions, find-ings and conclusions or recommendations expressed in this material arethose of the authors and do not necessarily reflect the views of DARPA.814ReferencesRegina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Proceedings ofNAACL-HLT.Regina Barzilay, Noemie Elhadad, and Kathleen McKe-own.
2001.
Sentence ordering in multidocument sum-marization.
In Proceedings of the First Human Lan-guage Technology Conference, San Diego, California.Maria Biryukov, Roxana Angheluta, and Marie-FrancineMoens.
2005.
Multidocument question answeringtext summarization using topic signatures.
In Pro-ceedings of the 5th Dutch-Belgium Information Re-trieval Workshop, Utrecht, the Netherlands.Sasha Blair-Goldensohn, David Evans, Vasileios Hatzi-vassiloglou, Kathleen McKeown, Ani Nenkova, Re-becca Passonneau, Barry Schiffman, Andrew Schlaik-jer, Advaith Siddharthan, and Sergey Siegelman.2004a.
Columbia University at DUC 2004.
In Pro-ceedings of the 4th Document Understanding Confer-ence, Boston, Massachusetts, USA.Sasha Blair-Goldensohn, Kathy McKeown, and AndrewSchlaikjer.
2004b.
Answering definitional questions:A hybrid approach.
In Mark Maybury, editor, NewDirections In Question Answering, chapter 4.
AAAIPress.J.
Cohen.
1968.
Weighted kappa: Nominal scale agree-ment with provision for scaled disagreement or partialcredit.
volume 70, pages 213?220.George Doddington, Alexis Mitchell, Mark Przybocki,Lance Ramshaw, Stephanie Strassel, and RalphWeischedel.
2004.
The automatic content extractionprogram - tasks, data, and evaluation.
In Proceedingsof the LREC Conference, Canary Islands, Spain, July.Pablo Duboue and Kathleen McKeown.
2003.
Statisticalacquisition of content selection rules for natural lan-guage generation.
In Proceedings of the Conferenceon Empirical Methods for Natural Language Process-ing, pages 121?128, Sapporo, Japan, July.Elena Filatova and John Prager.
2005.
Tell me whatyou do and I?ll tell you what you are: Learningoccupation-related activities for biographies.
In Pro-ceedings of the Joint Human Language TechnologyConference and Conference on Empirical Methods inNatural Language Processing, pages 113?120, Van-couver, Canada, October.J.
L. Fleiss.
1971.
Measuring nominal scale agreementamong many raters.
volume 76, No.
5, pages 378?382.Ralph Grishman, David Westbrook, and Adam Meyers.2005.
Nyu?s english ace 2005 system description.
InACE 05 Evaluation Workshop, Gaithersburg, MD.Sujian Li, You Ouyang, Wei Wang, and Bin Sun.
2007.Multi-document summarization using support vectorregression.
In http://duc.nist.gov/pubs/2007papers.Chin-Yew Lin and Eduard Hovy.
2000.
The auto-mated acquisition of topic signatures for text summa-rization.
In Proceedings of the 18th International Con-ference on Computational Linguistics, pages 495?501,Saarbru?cken, Germany, July.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic evalu-ation of summaries using n-gram co-occurrence statis-tics.
In Proceedings of the 2003 Language TechnologyConference, Edmonton, Canada.Ani Nenkova and Kathleen McKeown.
2003.
Referencesto named entities: A corpus study.
In Proceedingsof the Joint Human Language Technology Conferenceand North American chapter of the Association forComputational Linguistics Annual Meeting, Edmon-ton, Canada, May.Ralph Weischedel, Jinxi Xu, and Ana Licuanan.
2004.
Ahybrid approach to answering biographical questions.In Mark Maybury, editor, New Directions In QuestionAnswering, chapter 5.
AAAI Press.I.
Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, andS.
Cunningham.
1999.
Weka: Practical machinelearning tools and techniques with java implementa-tion.
In International Workshop: Emerging Knowl-edge Engineering and Connectionist-Based Informa-tion Systems, pages 192?196.Liang Zhou, Miruna Ticrea, and Eduard Hovy.
2004.Multi-document biography summarization.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 434?441,Barcelona, Spain.815
