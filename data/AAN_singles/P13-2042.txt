Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 233?237,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsModeling of term-distance and term-occurrence information for im-proving n-gram language model performanceTze Yuang Chong1,2, Rafael E. Banchs3, Eng Siong Chng1,2, Haizhou Li1,2,31Temasek Laboratory, Nanyang Technological University, Singapore 6397982School of Computer Engineering, Nanyang Technological University, Singapore 6397983Institute for Infocomm Research, Singapore 138632tychong@ntu.edu.sg, rembanchs@i2r.a-star.edu.sg,aseschng@ntu.edu.sg, hli@i2r.a-star.edu.sgAbstractIn this paper, we explore the use of distanceand co-occurrence information of word-pairsfor language modeling.
We attempt to extractthis information from history-contexts of up toten words in size, and found it complementswell the n-gram model, which inherently suf-fers from data scarcity in learning long histo-ry-contexts.
Evaluated on the WSJ corpus, bi-gram and trigram model perplexity were re-duced up to 23.5% and 14.0%, respectively.Compared to the distant bigram, we show thatword-pairs can be more effectively modeled interms of both distance and occurrence.1 IntroductionLanguage models have been extensively studiedin natural language processing.
The role of a lan-guage model is to measure how probably a (tar-get) word would occur based on some given evi-dence extracted from the history-context.
Thecommonly used n-gram model (Bahl et al 1983)takes the immediately preceding history-wordsequence, of length   1 , as the evidence forprediction.
Although n-gram models are simpleand effective, modeling long history-contextslead to severe data scarcity problems.
Hence, thecontext length is commonly limited to as short asthree, i.e.
the trigram model, and any useful in-formation beyond this window is neglected.In this work, we explore the possibility ofmodeling the presence of a history-word in termsof: (1) the distance and (2) the co-occurrence,with a target-word.
These two attributes will beexploited and modeled independently from eachother, i.e.
the distance is described regardless theactual frequency of the history-word, while theco-occurrence is described regardless the actualposition of the history-word.
We refer to thesetwo attributes as the term-distance (TD) and theterm-occurrence (TO) components, respectively.The rest of this paper is structured as follows.The following section presents the most relevantrelated works.
Section 3 introduces and moti-vates our proposed approach.
Section 4 presentsin detail the derivation of both TD and TO modelcomponents.
Section 5 presents some perplexityevaluation results.
Finally, section 6 presents ourconclusions and proposed future work.2 Related WorkThe distant bigram model (Huang et.al 1993,Simon et al 1997, Brun et al 2007) disassemblesthe n-gram into (n?1) word-pairs, such that eachpair is modeled by a distance-k bigram model,where 1      1 .
Each distance-k bigrammodel predicts the target-word based on the oc-currence of a history-word located k positionsbehind.Zhou & Lua (1998) enhanced the effective-ness of the model by filtering out those word-pairs exhibiting low correlation, so that only thewell associated distant bigrams are retained.
Thisapproach is referred to as the distance-dependenttrigger model, and is similar to the earlier pro-posed trigger model (Lau et al 1993, Rosenfeld1996) that relies on the bigrams of arbitrary dis-tance, i.e.
distance-independent.Latent-semantic language model approaches(Bellegarda 1998, Coccaro 2005) weight wordcounts with TFIDF to highlight their semanticimportance towards the prediction.
In this type ofapproach, count statistics are accumulated fromlong contexts, typically beyond ten to twentywords.
In order to confine the complexity intro-duced by such long contexts, word ordering isignored (i.e.
bag-of-words paradigm).Other approaches such as the class-based lan-guage model (Brown 1992, Kneser & Ney 1993)233use POS or POS-like classes of the history-wordsfor prediction.
The structured language model(Chelba & Jelinek 2000) determines the ?heads?in the history-context by using a parsing tree.There are also works on skipping irrelevant his-tory-words in order to reveal more informative n-grams (Siu & Ostendorf 2000, Guthrie et al2006).
Cache language models exploit temporalword frequencies in the history (Kuhn & Mori1990, Clarkson & Robinson 1997).3 Motivation of the Proposed ApproachThe attributes of distance and co-occurrence areexploited and modeled differently in each lan-guage modeling approach.
In the n-gram model,for example, these two attributes are jointly takeninto account in the ordered word-sequence.
Con-sequently, the n-gram model can only be effec-tively implemented within a short history-context(e.g.
of size of three or four).Both, the conventional trigger model and thelatent-semantic model capture the co-occurrenceinformation while ignoring the distance informa-tion.
It is reasonable to assume that distance in-formation at far contexts is less likely to be in-formative and, hence, can be discarded.
Howev-er, intermediate distances beyond the n-grammodel limits can be very useful and should notbe discarded.On the other hand, distant-bigram models anddistance-dependent trigger models make use ofboth, distance and co-occurrence, information upto window sizes of ten to twenty.
They achievethis by compromising inter-dependencies amonghistory-words (i.e.
the context is represented asseparated word-pairs).
However, similarly to n-gram models, distance and co-occurrence infor-mation are implicitly tied within the word-pairs.In our proposed approach, we attempt to ex-ploit the TD and TO attributes, separately, to in-corporate distant context information into the n-gram, as a remedy to the data scarcity problemwhen learning the far context.4 Language Modeling with TD and TOA language model estimates word probabilitiesgiven their history, i.e.
| ,where  denotes the target word anddenotes itscorresponding history.
Let the word located at ithposition,, be the target-word and its precedingword-sequence 	 ?  oflength   1, be its history-context.
Also, in or-der to alleviate the data scarcity problem, we as-sume the occurrences of the history-words to beindependent from each other, conditioned to theoccurrence of the target-word, i.e. |, where ,, and    .
Theprobability can then be approximated as: | ?
|! "  (1)where " is a normalizing term, andindicates that is the word at position  kth.4.1 Derivation of the TD-TO ModelIn order to define the TD and TO components forlanguage modeling, we express the observationof an arbitrary history-word, at the kth posi-tion behind the target-word, as the joint of twoevents: i) the word occurs within the histo-ry-context:, and ii) it occurs at distance   from the target-word: ? 	  , (?
  forbrevity); i.e.
 $  % ?
.Thus, the probability in Eq.1 can be written as: | ?
, ?
|! "  (2)where the likelihood , ?
|measures how likely the joint event ,?
 would be observed given the target-word.
This can be rewritten in terms of the likelih-ood function of the distance event (i.e.
?
 )and the occurrence event (i.e.), whereboth of them can be modeled and exploited sepa-rately, as follows: |&?
?
|, !?
|! '"(3)The formulation above yields three terms, re-ferred to as the prior, the TD likelihood, and theTO likelihood, respectively.In Eq.3, we have decoupled the observation ofa word-pair into the events of distance and co-occurrence.
This allows for independently mod-eling and exploiting them.
In order to controltheir contributions towards the final prediction ofthe target-word, we weight these components: |& ()?
?
|, ! (*?
|! (+ '"(4)234where , , ,- , and ,.
are the weights for theprior, TD and TO models, respectively.Notice that the model depicted in Eq.4 is thelog-linear interpolation (Klakow 1998) of thesemodels.
The prior, which is usually implementedas a unigram model, can be also replaced with ahigher order n-gram model as, for instance, thebigram model: |& |()?
?
|, ! (*?
|! (+ '"(5)Replacing the unigram model with a higherorder n-gram model is important to compensatethe damage incurred by the conditional indepen-dence assumption made earlier.4.2 Term-Distance Model ComponentBasically, the TD likelihood measures how likelya given word-pair would be separated by a givendistance.
So, word-pairs possessing consistentseparation distances will favor this likelihood.The TD likelihood for a distance  given the co-occurrence of the word-pair ,  can beestimated from counts as follows: ?
|, 	 C, , ?
C,   (6)The above formulation of the TD likelihoodrequires smoothing for resolving two problems:i) a word-pair at a particular distance has a zerocount, i.e.
C, , ?
 	 0 , whichresults in a zero probability, and ii) a word-pair isnot seen at any distance within the observationwindow, i.e.
zero co-occurrence C,  	 0, which results in a division by zero.For the first problem, we have attempted toredistribute the counts among the word-pairs atdifferent distances (as observed within the win-dow).
We assumed that the counts of word-pairsare smooth in the distance domain and that theinfluence of a word decays as the distance in-creases.
Accordingly, we used a weighted mov-ing-average filter for performing the smoothing.Similar approaches have also been used in otherworks (Coccaro 2005, Lv & Zhai 2009).
Notice,however, that this strategy is different from otherconventional smoothing techniques (Chen &Goodman 1996), which rely mainly on the count-of-count statistics for re-estimating and smooth-ing the original counts.For the second problem, when a word-pairwas not seen at any distance (within the win-dow), we arbitrarily assigned a small probabilityvalue, ?
|,  	 0.01 , to pro-vide a slight chance for such a word-pair  , to occur at close distances.4.3 Term-Occurrence Model ComponentDuring the decoupling operation (from Eq.2 toEq.3), the TD model held only the distance in-formation while the count information has beenignored.
Notice the normalization of word-paircounts in Eq.6.As a complement to the TD model, the TOmodel focuses on co-occurrence, and holds onlycount information.
As the distance information iscaptured by the TD model, the co-occurrencecount captured by the TO model is independentfrom the given word-pair distance.In fact, the TO model is closely related to thetrigger language model (Rosenfeld 1996), as theprediction of the target-word (the triggered word)is based on the presence of a history-word (thetrigger).
However, differently from the triggermodel, the TO model considers all the word-pairs without filtering out the weak associatedones.
Additionally, the TO model takes into ac-count multiple co-occurrences of the same histo-ry-word within the window, while the triggermodel would count them only once (i.e.
consid-ers binary counts).The word-pairs that frequently co-occur at ar-bitrary distances (within an observation window)would favor the TO likelihood.
It can be esti-mated from counts as:| 	 C, C  (7)When a word-pair did not co-occur (within theobservation window), we assigned a small prob-ability value, | 	 0.01, to pro-vide a slight chance for the history word to occurwithin the history-context of the target word.5 Perplexity EvaluationA perplexity test was run on the BLLIP WSJcorpus (Charniak 2000) with the standard 5Kvocabulary.
The entire WSJ ?87 data (740K sen-tences 18M words) was used as train-set to trainthe n-gram, TD, and TO models.
The dev-set andthe test-set, each comprising 500 sentences andabout 12K terms, were selected randomly fromWSJ ?88 data.
We used them for parameter fine-tuning and performance evaluation.2355.1 Capturing Distant InformationIn this experiment, we assessed the effectivenessof the TD and TO components in reducing the n-gram?s perplexity.
Following Eq.5, we interpo-lated n-gram models (of orders from two to six)with the TD, TO, and the both of them (referredto as TD-TO model).By using the dev-set, optimal interpolationweights (i.e.
,, ,-, and ,.)
for the three combi-nations (n-gram with TD, TO, and TD-TO) werecomputed.
The resulting interpolation weightswere as follows: n-gram with TD = (0.85, 0.15),n-gram with TO = (0.85, 0.15), and n-gram withTD-TO = (0.80, 0.07, 0.13).The history-context window sizes were opti-mized too.
Optimal sizes resulted to be 7, 5 and 8for TD, TO, and TD-TO models, respectively.
Infact, we observed that the performance is quiterobust with respect to the window?s length.
De-viating about two words from the optimumlength only worsens the perplexity less than 1%.Baseline models, in each case, are standard n-gram models with modified Kneser-Ney interpo-lation (Chen 1996).
The test-set results are de-picted in Table 1.N NG NG-TDRed.(%)NG-TORed.(%)NG-TDTORed.
(%)2 151.7 134.5 11.3 119.9 21.0 116.0 23.53 99.2 92.9 6.3 86.7 12.6 85.3 14.04 91.8 86.1 6.2 81.4 11.3 80.1 12.75 90.1 84.7 6.0 80.2 11.0 79.0 12.36 89.7 84.4 5.9 79.9 10.9 78.7 12.2Table 1.
Perplexities of the n-gram model (NG)of order (N) two to six and their combinationswith the TD, TO, and TD-TO models.As seen from the table, for lower order n-grammodels, the complementary information capturedby the TD and TO components reduced the per-plexity up to 23.5% and 14.0%, for bigram andtrigram models, respectively.
Higher order n-gram models, e.g.
hexagram, observe history-contexts of similar lengths as the ones observedby the TD, TO, and TD-TO models.
Due to theincapability of n-grams to model long history-contexts, the TD and TO components are stilleffective in helping to enhance the prediction.Similar results were obtained by using the stan-dard back-off model (Katz 1987) as baseline.5.2 Benefit of Decoupling Distant-BigramIn this second experiment, we examined whetherthe proposed decoupling procedure leads to bet-ter modeling of word-pairs compared to the dis-tant bigram model.
Here we compare the per-plexity of both, the distance-k bigram model anddistance-k TD model (for values of k rangingfrom two to ten), when combined with a standardbigram model.In order to make a fair comparison, withouttaking into account smoothing effects, we trainedboth models with raw counts and evaluated theirperplexities over the train-set (so that no zero-probability will be encountered).
The results aredepicted in Table 2.k 2 4 6 8 10DBG 105.7 112.5 114.4 115.9 116.8TD 98.5 106.6 109.1 111.0 112.2Table 2.
Perplexities of the distant bigram (DBG)and TD models when interpolated with a stan-dard bigram model.The results from Table 2 show that the TDcomponent complements the bigram model bet-ter than the distant bigram itself.
Firstly, theseresults suggest that the distance information (asmodeled by the TD) offers better cue than thecount information (as modeled by the distant bi-gram) to complement the n-gram model.The normalization of distant bigram counts, asindicated in Eq.6, aims at highlighting the infor-mation provided by the relative positions ofwords in the history-context.
This has beenshown to be an effective manner to exploit thefar context.
By also considering the results inTable 1, we can deduce that better performancecan be obtained when the TO attribute is alsoinvolved.
Overall, decoupling the word history-context into the TD and TO components offers agood approach to enhance language modeling.6 ConclusionsWe have proposed a new approach to computethe n-gram probabilities, based on the TD andTO model components.
Evaluated on the WSJcorpus, the proposed TD and TO models reducedthe bigram?s and trigram?s perplexities up to23.5% and 14.0%, respectively.
We have shownthe advantages of modeling word-pairs with TDand TO, as compared to the distant bigram.As future work, we plan to explore the useful-ness of the proposed model components in actualnatural language processing applications such asmachine translation and speech recognition.
Ad-ditionally, we also plan to develop a more prin-cipled framework for dealing with TD smoothing.236ReferencesBahl, L., Jelinek, F. & Mercer, R. 1983.
A statisticalapproach to continuous speech recognition.
IEEETrans.
Pattern Analysis and Machine Intelligence,5:179-190.Bellegarda, J. R. 1998.
A multispan language model-ing framework for larfge vocabulary speech recog-nition.
IEEE Trans.
on Speech and AudioProcessing, 6(5): 456-467.Brown, P.F.
1992 Class-based n-gram models of natu-ral language.
Computational Linguistics, 18: 467-479.Brun, A., Langlois, D. & Smaili, K. 2007.
Improvinglanguage models by using distant information.
InProc.
ISSPA 2007, pp.1-4.Cavnar, W.B.
& Trenkle, J.M.
1994.
N-gram-basedtext categorization.
Proc.
SDAIR-94, pp.161-175.Charniak, E., et al 2000.
BLLIP 1987-89 WSJ Cor-pus Release 1.
Linguistic Data Consortium, Phila-delphia.Chen, S.F.
& Goodman, J.
1996.
An empirical studyof smoothing techniques for language modeling.In.
Proc.
ACL ?96, pp.
310-318.Chelba, C. & Jelinek, F. 2000.
Structured languagemodeling.
Computer Speech & Language, 14: 283-332.Clarkson, P.R.
& Robinson, A.J.
1997.
Languagemodel adaptation using mixtures and an exponen-tially decaying cache.
In Proc.
ICASSP-97, pp.799-802.Coccaro, N. 2005.
Latent semantic analysis as a toolto improve automatic speech recognition perfor-mance.
Doctoral Dissertation, University of Colo-rado, Boulder, CO, USA.Guthrie, D., Allison, B., Liu, W., Guthrie, L., &Wilks, Y.
2006.
A closer look at skip-gram model-ling.
In Proc.
LREC-2006, pp.1222-1225.Huang, X. et al 1993.
The SPHINX-II speech recog-nition system: an overview.
Computer Speech andLanguage, 2: 137-148.Katz, S.M.
1987.
Estimation of probabilities fromsparse data for the language model component of aspeech recognizer.
IEEE Trans.
on Acoustics,Speech, & Signal Processing, 35:400-401.Klakow, D. 1998.
Log-linear interpolation of lan-guage model.
In Proc.
ICSLP 1998, pp.1-4.Kneser, R. & Ney, H. 1993.
Improving clusteringtechniques for class-based statistical languagemodeling.
In Proc.
EUROSPEECH ?93, pp.973-976.Kuhn, R. & Mori, R.D.
1990.
A cache-based naturallanguage model for speech recognition.
IEEETrans.
Pattern Analysis and Machine Intelligence,12(6): 570-583.Lau, R. et al 1993.
Trigger-based language models: amaximum-entropy approach.
In Proc.
ICASSP-94,pp.45-48.Lv Y.
& Zhai C. 2009.
Positional language models forinformation retrieval.
In Proc.
SIGIR?09, pp.299-306.Rosenfeld, R. 1996.
A maximum entropy approach toadaptive statistical language modelling.
ComputerSpeech and Language, 10: 187-228.Simons, M., Ney, H. & Martin S.C. 1997.
Distantbigram language modelling using maximum entro-py.
In Proc.
ICASSP-97, pp.787-790.Siu, M. & Ostendorf, M. 2000.
Variable n-grams andextensions for conversational speech languagemodeling.
IEEE Trans.
on Speech and AudioProcessing, 8(1): 63-75.Zhou G. & Lua K.T.
1998.
Word association and MI-trigger-based language modeling.
In Proc.
COL-ING-ACL, 1465-1471.237
