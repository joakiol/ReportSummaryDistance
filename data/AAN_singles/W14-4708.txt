Zock/Rapp/Huang (eds.
): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 60?63,Dublin, Ireland, August 23, 2014.Retrieving Word Associations with a Simple Neighborhood Algorithmin a Graph-Based ResourceGemma Bel-EnguixLIFAix Marseille Universit?,13288 Marseillegemma.belenguix@gmail.comAbstractThe paper explains the procedure to obtain word associations starting from a graph that has notbeen specifically built for that purpose.
Our goal is being able to simulate human word asso-ciations by using the simplest possible methods, including the basic tools of a co-occurrencenetwork from a non-annotated corpus, and a very simple search algorithm based on neighbor-hood.
The method has been tested in the Cogalex shared task, revealing the difficulty ofachieving word associations without semantic annotation.1 IntroductionBuilding annotated computational resources for natural language is a difficult and time-consumingtask that not always produces the desired results.
A good alternative to semantic annotation by handcould be using statistics and graph-based operations in corpora.
In order to implement a system capa-ble to work with such methods we have designed co-occurrence networks from large existing corpora,like Wikipedia or the British National Corpus (Burnard & Aston, 1998).
The underlying idea is thatsystems based on mathematics and statistics can achieve comparable results to the ones obtained withmore sophisticated methods relying on semantic processing.Non-annotated networks have been suggested and implemented, for example, by Ferrer-i-Canchoand Sol?
(2001).
The authors suggested non-semantically annotated graphs, building exclusively syn-tagmatic networks.
By this method, they reduced the syntagmatic-paradigmatic relations.
The authorsused the BNC corpus to build two graphs G1 and G2.
First, a so-called co-occurrence graph G1 inwhich words are linked if they co-occur in at least one sentence within a span of maximal three tokens.Then a collocation graph G2 is extracted in which only those links of G1 are retained whose end verti-ces co-occur more frequent than expected by chance.A non-annotated graph built from a large corpus (Bel-Enguix and Zock, 2013) is a good representa-tion to allow for the discovery of a large number of word relationships.
It can be used for a number oftasks, one of them being computing word associations.
To test the consistence of the results obtainedby our method, they will be compared with the Edinburgh Association Thesaurus, a collection of 8000words whose association norms were produced by presenting each of the stimulus words to about 100subjects each, and by collecting their responses.
The subjects were 17 to 22 year old British students.To perform the tests, we take a sample (EAT: http://www.eat.rl.ac.uk/) consisting in 100 words.For building a network to deal with the specific task of producing word associations we have usedthe British National Corpus (BNC) as a source.The way the network has been constructed has also some interest and impact in the final results.Firstly, for the sake of simplicity, we removed all words other than Nouns and Adjectives.
Nouns havebeen normalized to singular form.
After this pre-processing, a graph has been built where the nounsThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/60and adjectives in the corpus are the nodes, and where the edges between these nodes are zero at thebeginning, and are incremented by 1 whenever the two respective words co-occur in the corpus as di-rect neighbors (i.e.
more distant neighborhood was not taken into account).
That is, after processingthe corpus the weight of each edge represents the number of times the respective words (nodes) co-occur.To build the graph our system runs through a pipeline of four modules:?
document cleaning (deletion of stop-words), extracting only ?Nouns?
and ?Adjectives?;?
lemmatisation of word forms to avoid duplicates (horse, horses);?
computation of the (un-directed) graph's edges.
Links are created between direct neighbours;?
computation of the edges?
weights.
The weight of an edge is equal to the number of its occur-rences.
We only use absolute values.?
computation of the node?s weights.
As in the edges, the weight of a node is the number of itoccurrences.The graph has been implemented with Python.The resultant network has 427668 nodes (different words).
Of them, 1894 are happax (occur onlyonce), only the 0,5%.
There are 13654814 edges.
From them, 9836987 with weight one; and 3817827have a weight higher than one, on a percentage relation 72/28.
The average degree of the nodes of thenetwork is 31, 92.2 Searching methodThe search of the target word in the graph has two different steps:1.
Determining the set of common neighbors of the clues,2.
Ranking the set of nodes obtained in 1, and picking the ?best result?.2.1 Search of neighborsThe search of the target word T in a graph G, is done via some clues c1, c2,?, cn, which act as in-puts.
G=(V, E) stands for the graph, with V expressing the set of vertices (words) and E the set ofedges (co-occurrences).
The clues c1, c2,?, cn ?V.
N(i) expresses the neighbourhood of a node i ?V,and is defined as 'every j?V | ei,j ?E.
The search algorithm is as follows:?
Define the neighbourhood of c1, c2,?, cn as N(c1), N(c2),?, N(cn);?
Get the set of nodes VT = N(c1) ?
N(c2) ?
?
?N(cn) and consider Vc={c1, c2,?, cn} to be theset of nodes representing the clues.
We define a subgraph of G, GT, that is a complete bipar-tite graph, where every element of VT is connected to every element of Vc;In the Cogalex shared task, five clues have been given, belonging to any grammatical category, andin different inflected forms (ie., am, be, been or horse, horses).
Since the graph has the limitation ofcontaining only Nouns and Adjectives, the system dismisses every word not belonging to the set ofnodes V and uses only the remaining clues.
And being the words lemmatized, inflected forms are re-duced to only one.
Therefore, the application will never find ?be?
from ?am?, ?been?, ?is?.To build the graph and perform the search, a Python module has been used, Networkx(https://networkx.github.io/), that is extremely fast and efficient.2.2 Ranking the nodesThis task has been designed with a very simple algorithm.
Let?s consider C the number of finalstimulus words; wc1,wc2,?,wcn is the weight in the graph of every node c ?
VC; wt1,wt2,?,wtn theweight in the graph of every one of the nodes t ?
VT; wetc the weight for every edge of GT, where c ?VC and t?
VT.The nodes of the graph are gathered in groups in a logarithmic scale: up to 101, 102, 103, 104, 105,106.
We name a the power of 10, ie., for 106, a=6.The nodes of VT are ranked with a simple algorithm, consisting in calculating Wt for every t ?
VT,so asThe nodes are ranked according to the values of W.613 ResultsIn some initial tests, the results were compared with the ones obtained in a sample of the EdinburghAssociation Thesaurus (EAT: http://www.eat.rl.ac.uk/) consisting in 100 words.
The EAT (Kiss et al.,1973) has 8000 words, and the 100 selected for the test were all of them nouns or adjectives, whatmade the working easier for our system.
There were 15 words that match the ones observed in the Ed-inburgh Associative Thesaurus (EAT) as Primary Response (PR).
There is a partial coincidence ?
theword given has not a 0 in the EAT ?
in 54 of the outputs.
This means that in more than 50% of thecases the method retrieves a word corresponding to the one produced by a human in the associationexperiment.
This does not imply though that it is the most popular one.Some other methods of evaluation (Evert & Krenn, 2001) have been applied to the system (Bel-Enguix et al., 2014), showing that the outcomes provided by the graph-based method are quite consis-tent with human responses, and even optimize them in some specific classes.In contrast with these results, the ones obtained in the Cogalex shared task were clearly worse.From a total of 200 items, the number of matches was 182, which means an accuracy of the 9,10%.There are several reasons for that: a) some of the targets were not Nouns or Adjectives, what makesthem not retrievable for the system, b) many stimulus words were not Nouns or Adjectives, whatmakes the algorithm weaker, because such words are dismissed as clues, c) stimulus were not lemma-tized and the lemmatization process for words without a context is not easy for the python lemmatiza-tion module, d) probably many of the words of the first tested sample were very well-known relations,while the ones in the Cogalex shared task could be less well-connected nodes, e) the ranking algorithmcan be clearly improved in order to retrieve the best word, not only one in the list, because we havebeen asked only full matches.4 Conclusions: strengths and weakness of the methodEven though the results obtained were not good, there are several strengths that make this systemworth to be improved in the future.Firstly, the network is easy to built and program is very fast.
We have used the python package?networkx?
to build the graph, integrating its commands into the python script.
The result is that in lessthan one minute the system can compute the two thousand associations that were required.
Therefore,while an important improvement is needed in the ranking algorithm, there is room for it, because theperformance of the method can afford it.Secondly, the system works with any co-occurrence graph made from any corpus.
This allows us touse specialized corpora as a basis, as well as collections of texts closer to the time the human associa-tions have been produced.However, there are important weaknesses in the procedure.
In the first place, it is necessary to use anetwork resource including other grammatical categories, at least verbs and adverbs.
Even thoughsuch graph exists, the difficulty in the application of the current ranking algorithm makes it not-usableso far for this specific task.
There is still another clear difficulty in the method, related to the one wejust stated: the lack of clustering.
Not using semantic annotations is one of our axioms, because itmakes the system heavier.
Nevertheless, a way to detect which words are more related is needed.
Thisis currently the strongest weakness of this graph-based algorithm.
We propose for the future a verysimple clustering based on WordNet synsets (Miller, 1990), in a way the search can be oriented to-wards the best choices for every word connection, even though their weight in the graph is lower.5 AknowledgementsI am very grateful to my colleagues Michael Zock and Reinhard Rapp for their expertise, commentsand constant support.This work has been supported by the European Commission under a Marie Curie Fellowship, pro-ject DynNetLAc.62ReferencesBel-Enguix, G., Rapp, R. and Zock, M. (2014) A Graph-Based Approach for Computing Free Word Associa-tions, Proceedings of LREC 2014, Ninth International Conference on Language Resources and Evaluation,3027-3033.Bel-Enguix, G. and Zock, M. (2013).
Lexical Access via a Simple Co-occurrence Network, Proceedings ofTALN-RECITAL 2013, 596-603.Burnard, L. and Aston, G. (1998).
The BNC Handbook: Exploring the British National Corpus.
Edinburgh: Ed-inburgh University PressEvert, S. and Krenn, B.
(2001).
Methods for qualitative evaluation of lexical association measures.
In Proceed-ings of the 39th Annual Meeting of the Association of Computational Linguistics, Toulouse, France, 188-915.Ferrer-Cancho, R., Sol?, R. (2001).
The small-world of human language.
Proceedings of the Royal Society ofLondon.
Series B, Biological Sciences 268 (2001) 2261-2265.Kiss, G.R., Armstrong, C., Milroy, R., and Piper, J.
(1973).
An associative thesaurus of English and its computeranalysis.
In: A. Aitken, R. Beiley, N. Hamilton-Smith (eds.
): The Computer and Literary Studies.
EdinburghUniversity Press.Miller, G. (1990).
Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4).63
