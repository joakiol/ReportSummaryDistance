Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1346?1356, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsOpinion Target Extraction Using Word-Based Translation ModelKang Liu, Liheng Xu, Jun ZhaoNational Laboratory of Pattern Recognition,Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China{kliu, lhxu, jzhao}@nlpr.ia.ac.cnAbstractThis paper proposes a novel approach toextract opinion targets based on word-based translation model (WTM).
At first,we apply WTM in a monolingual scenarioto mine the associations between opiniontargets and opinion words.
Then, a graph-based algorithm is exploited to extractopinion targets, where candidate opinionrelevance estimated from the minedassociations, is incorporated with candidateimportance to generate a global measure.By using WTM, our method can captureopinion relations more precisely, especiallyfor long-span relations.
In particular,compared with previous syntax-basedmethods, our method can effectively avoidnoises from parsing errors when dealingwith informal texts in large Web corpora.By using graph-based algorithm, opiniontargets are extracted in a global process,which can effectively alleviate the problemof error propagation in traditionalbootstrap-based methods, such as DoublePropagation.
The experimental results onthree real world datasets in different sizesand languages show that our approach ismore effective and robust than state-of-artmethods.1 IntroductionWith the rapid development of e-commerce, mostcustomers express their opinions on various kindsof entities, such as products and services.
Thesereviews not only provide customers with usefulinformation for reference, but also are valuable formerchants to get the feedback from customers andenhance the qualities of their products or services.Therefore, mining opinions from these vastamounts of reviews becomes urgent, and hasattracted a lot of attentions from many researchers.In opinion mining, one fundamental problem isopinion target extraction.
This task is to extractitems which opinions are expressed on.
In reviews,opinion targets are usually nouns/noun phrases.For example, in the sentence of ?The phone has acolorful and even amazing screen?, ?screen?
is anopinion target.
In online product reviews, opiniontargets often are products or product features, sothis task is also named as product featureextraction in previous work (Hu et al 2004; Dinget al 2008; Liu et al 2005; Popescu et al 2005;Wu et al 2005; Su et al 2008).To extract opinion targets, many studiesregarded opinion words as strong indicators (Hu etal., 2004; Popescu et al 2005; Liu et al 2005;Qiu et al 2011; Zhang et al 2010), which isbased on the observation that opinion words areusually located around opinion targets, and thereare associations between them.
Therefore, mostpervious methods iteratively extracted opiniontargets depending upon the associations betweenopinion words and opinion targets (Qiu et al 2011;Zhang et al 2010).
For example, ?colorful?
and?amazing?
is usually used to modify ?screen?
inreviews about cell phone, so there are strongassociations between them.
If ?colorful?
and?amazing?
had been known to be opinion words,?screen?
is likely to be an opinion target in thisdomain.
In addition, the extracted opinion targetscan be used to expand more opinion wordsaccording to their associations.
It?s a mutualreinforcement procedure.Therefore, mining associations between opiniontargets and opinion words is a key for opinion1346target extraction (Wu et al 2009).
To this end,most previous methods (Hu et al 2004; Ding et al2004; Wang et al 2008), named as adjacentmethods, employed the adjacent rule, where anopinion target was regarded to have opinionrelations with the surrounding opinion words in agiven window.
However, because of the limitationof window size, opinion relations cannot becaptured precisely, especially for long-spanrelations, which would hurt estimating associationsbetween opinion targets and opinion words.
Toresolve this problem, several studies exploitedsyntactic information such as dependency trees(Popescu et al 2005; Qiu et al 2009; Qiu et al2011; Wu et al 2009; Zhang et al 2010).
If thesyntactic relation between an opinion word and anopinion target satisfied a designed pattern, thenthere was an opinion relation between them.Experiments consistently reported that syntax-based methods could yield better performance thanadjacent methods for small or medium corpora(Zhang et al 2010).
The performance of syntax-based methods heavily depends on the parsingperformance.
However, online reviews are ofteninformal texts (including grammar mistakes, typos,improper punctuations etc.).
As a result, parsingmay generate many mistakes.
Thus, for largecorpora from Web including a great deal ofinformal texts, these syntax-based methods maysuffer from parsing errors and introduce manynoises.
Furthermore, this problem maybe moreserious on non-English language reviews, such asChinese reviews, because that the performances ofparsing on these languages are often worse thanthat on English.To overcome the weakness of the two kinds ofmethods mentioned above, we propose a novelunsupervised approach to extract opinion targetsby using word-based translation model (WTM).We formulate identifying opinion relationsbetween opinion targets and opinion words as aword alignment task.
We argue that an opiniontarget can find its corresponding modifier throughmonolingual word alignment.
For example inFigure 1, the opinion words ?colorful?
and?amazing?
are aligned with the target ?screen?through word alignment.
To this end, we use WTMto perform monolingual word alignment for miningassociations between opinion targets and opinionwords.
In this process, several factors, such asword co-occurrence frequencies, word positionsetc., can be considered globally.
Compared withadjacent methods, WTM doesn?t identify opinionrelations between words in a given window, solong-span relations can be effectively captured(Liu et al 2009).
Compared with syntax-basedmethods, without using parsing, WTM caneffectively avoid errors from parsing informal texts.So it will be more robust.
In addition, by usingWTM, our method can capture the ?one-to-many?or ?many-to-one?
relations (?one-to-many?
meansthat, in a sentence one opinion word modifiesseveral opinion targets, and ?many-to-one?
meansseveral opinion words modify one opinion target).Thus, it?s reasonable to expect that WTM is likelyto yield better performance than traditionalmethods for mining associations between opiniontargets and opinion words.Based on the mined associations, we extractopinion targets in a ranking framework.
Allnouns/noun phrases are regarded as opinion targetcandidates.
Then a graph-based algorithm isexploited to assign confidences to each candidate,in which candidate opinion relevance andimportance are incorporated to generate a globalmeasure.
At last, the candidates with higher ranksare extracted as opinion targets.
Compared withmost traditional methods (Hu et al2004; Liu et al2005; Qiu et al 2011), we don?t extract opiniontargets iteratively based on the bootstrappingstrategy, such as Double Propagation (Qiu et al2011), instead all candidates are dynamicallyranked in a global process.
Therefore, errorpropagation can be effectively avoided and theperformance can be improved.Figure 1: Word-based translation model foropinion relation identificationThe main contributions of this paper are asfollows.1) We formulate the opinion relationidentification between opinion targets andopinion words as a word alignment task.
Toour best knowledge, none of previous methodsdeal with this task using monolingual wordalignment model (in Section 3.1).TranslationThe phone has a colorful and even amazing screenThe phone has a colorful and even amazing screen13472) We propose a graph-based algorithm foropinion target extraction in which candidateopinion relevance and importance areincorporated into a unified graph to estimatecandidate confidence.
Then the candidateswith higher confidence scores are extracted asopinion targets (in Section 3.2).3) We have performed experiments on threedatasets in different sizes and languages.
Theexperimental results show that our approachcan achieve performance improvement overthe traditional methods.
(in Section 4).The rest of the paper is organized as follows.
Inthe next section, we will review related work inbrief.
Section 3 describes our approach in detail.Then experimental results will be given in Section4.
At the same time, we will give some analysisabout the results.
Finally, we give the conclusionand the future work.2 Related WorkMany studies have focused on the task of opiniontarget extraction, such as (Hu et al 2004; Ding etal., 2008; Liu et al 2006; Popescu et al 2005;Wu et al 2005; Wang et al 2008; Li et al 2010;Su et al 2008; Li et al 2006).
In general, theexisting approaches can be divided into two maincategories: supervised and unsupervised methods.In supervised approaches, the opinion targetextraction task was usually regarded as a sequencelabeling task (Jin et al2009; Li et al2010; Wu etal., 2009; Ma et al2010; Zhang et al 2009).
Jin etal.
(2009) proposed a lexicalized HMM model toperform opinion mining.
Li et al(2010) proposeda Skip-Tree CRF model for opinion targetextraction.
Their methods exploited threestructures including linear-chain structure,syntactic structure, and conjunction structure.
Inaddition, Wu et al(2009) utilized a SVM classifierto identify relations between opinion targets andopinion expressions by leveraging phrasedependency parsing.
The main limitation of thesesupervised methods is that labeling training datafor each domain is impracticable because of thediversity of the review domains.In unsupervised methods, most approachesregarded opinion words as the important indicatorsfor opinion targets (Hu et al 2004; Popsecu et al2005; Wang et al 2008; Qiu et al 2011; Zhang etal., 2010).
The basic idea was that reviewers oftenuse the same opinion words when they commenton the similar opinion targets.
The extractionprocedure was often a bootstrapping process whichextracted opinion words and opinion targetsiteratively, depending upon their associations.Popsecu et al(2005) used syntactic patterns toextract opinion target candidates.
After that theycomputed the point-wise mutual information (PMI)score between a candidate and a product categoryto refine the extracted results.
Hu et al(2004)exploited an association rule mining algorithm andfrequency information to extract frequent explicitproduct features.
The adjective nearest to thefrequent explicit feature was extracted as anopinion word.
Then the extracted opinion wordswere used to extract infrequent opinion targets.Wang et al(2008) adopted the similar idea, buttheir method needed a few seeds to weaklysupervise the extraction process.
Qiu et al(2009,2011) proposed a Double Propagation method toexpand a domain sentiment lexicon and an opiniontarget set iteratively.
They exploited directdependency relations between words to extractopinion targets and opinion words iteratively.
Themain limitation of Qiu?s method is that the patternsbased on dependency parsing tree may introducemany noises for the large corpora (Zhang et al2010).
Meanwhile, Double Propagation is abootstrapping strategy which is a greedy processand has the problem of error propagation.
Zhang etal.
(2010) extended Qiu?s method.
Besides thepatterns used in Qiu?s method, they adopted someother patterns, such as phrase patterns, sentencepatterns and ?no?
pattern, to increase recall.
Inaddition they used the HITS (Klernberg et al 1999)algorithm to compute the feature relevance scores,which were simply multiplied by the log of featurefrequencies to rank the extracted opinion targets.
Inthis way, the precision of result can be improved.3 Opinion Target Extraction UsingWord-Based Translation Model3.1 Method FrameworkAs mentioned in the first section, our approach foropinion target extraction is composed of thefollowing two main components:1) Mining associations between opinion targetsand opinion words: Given a collection ofreviews, we adopt a word-based translation1348model to identify potential opinion relations inall sentences, and then the associationsbetween opinion targets and opinion words areestimated.2) Candidate confidence estimation: Based onthese associations, we exploit a graph-basedalgorithm to compute the confidence of eachopinion target candidate.
Then the candidateswith higher confidence scores are extracted asopinion targets.3.2 Mining associations between opiniontargets and opinion words using Word-based Translation ModelThis component is to identify potential opinionrelations in sentences and estimate associationsbetween opinion targets and opinion words.
Weassume opinion targets and opinion wordsrespectively to be nouns/noun phrases andadjectives, which have been widely adopted inprevious work (Hu et al 2004; Ding et al 2008;Wang et al 2008; Qiu et al 2011).
Thus, our aimis to find potential opinion relations betweennouns/noun phrases and adjectives in sentences,and calculate the associations between them.
Asmentioned in the first section, we formulateopinion relation identification as a word alignmenttask.
We employ the word-based translation model(Brown et al1993) to perform monolingual wordalignment, which has been widely used in manytasks, such as collocation extraction (Liu et al2009), question retrieval (Zhou et al 2011) and soon.
In our method, every sentence is replicated togenerate a parallel corpus, and we apply thebilingual word alignment algorithm to themonolingual scenario to align a noun/noun phasewith its modifier.Given a sentence with n words1 2{ , ,..., }nS w w w?
, the word alignment{( , ) | [1, ]}iA i a i n?
?
can be obtained bymaximizing the word alignment probability of thesentence as follows.
?=arg max ( | )AA P A S(1)where ( , )ii a  means that a noun/noun phrase atpositioni  is aligned with an adjective at position ia .If we directly use this alignment model to our task,a noun/noun phrase may align with the irrelevantwords other than adjectives, like prepositions orconjunctions and so on.
Thus, in the alignmentprocedure, we introduce some constrains: 1)nouns/noun phrases (adjectives) must be alignedwith adjectives (nouns/noun phrases) or null words;2) other words can only align with themselves.Totally, we employ the following 3 WTMs (IBM1~3) to identify opinion relations.11( | ) ( | )jnIBM j ajP A S t w w???
?21( | ) ( | ) ( | , )jnIBM j a jjP A S t w w d j a n???
?31 1( | ) ( | ) ( | ) ( | , )jn nIBM i i j a ji jP A S n w t w w d j a n???
???
?
(2)There are three main factors: ( | )jj at w w,( | , )jd j a nand ( | )i in w?
, which respectivelymodels different information.1) ( | )jj at w wmodels the co-occurrenceinformation of two words in corpora.
If anadjective co-occurs with a noun/noun phrasefrequently in the reviews, this adjective has highassociation with this noun/noun phrase.
Forexample, in reviews of cell phone, ?big?
often co-occurs with ?phone?s size?, so ?big?
has highassociation with ?phone?s size?.2) ( | , )jd j a lmodels word position information,which describes the probability of a word inpositionja aligned with a word in position j .3) ( | )i in w?
models the fertility of words, whichdescribe the ability of a word for ?one-to-many?alignment.i?
denotes the number of words that arealigned withiw .
For example, ?Iphone4 hasamazing screen and software?.
In this sentence,?amazing?
is used to modify two words: ?screen?and ?software?.
So?
equals to 2 for ?amazing?.Therefore, in Eq.
(2),1( | )IBMP A S?
only modelsword co-occurrence information.2 ( | )IBMP A S?additionally employs word position information.Besides these two information,3( | )IBMP A S?considers the ability of a word for ?one-to-many?alignment.
In the following experiments section,we will discuss the performance difference amongthese models in detail.
Moreover, these models1349may capture ?one-to-many?
or ?many-to-one?opinion relations (mentioned in the first section).In our knowledge, it isn?t specifically consideredby previous methods including adjacent methodsand syntax-based methods.
Meanwhile ?
thealignment results may contain empty-wordalignments, which means a noun/noun phrase hasno modifier or an adjective modify nothing in thesentence.After gathering all word pairs from the reviewsentences, we can estimate the translationprobabilities between nouns/noun phrases andadjectives as follows.
( , )( | ) ( )N AN AACount w wp w w Count w?
(3)where ( | )N Ap w w means the translationprobabilities from adjectives to nouns/nounphrases.
Similarly, we can obtain translationprobability ( | )A Np w w .
Therefore, similar to (Liuet al2009), the association between a noun/nounphrase and an adjective is estimated as follows.1| |( , )( ( ) (1 ) ( ))N AN NA AAssociation w wt p w w t p w w ??
?
?
(4)where t is the harmonic factor to combine thesetwo translation probabilities.
In this paper, we set0.5t ?
.
For demonstration, we give someexamples in Table 1.
We can see that our methodusing WTM can successfully capture associationsbetween opinion targets and opinion words.battery life sound softwarewonderful 0.000 0.042 0.000poor 0.032 0.000 0.026long 0.025 0.000 0.000Table 1: Examples of associations between opiniontargets and opinion words.3.3 Candidate Confidence EstimationIn this component, we compute the confidence ofeach opinion target candidate and rank them.
Thecandidates with higher confidence are regarded asthe opinion targets.
We argue that the confidenceof a candidate is determined by two factors: 1)Opinion Relevance; 2) Candidate Importance.Opinion Relevance reflects the degree that acandidate is associated to opinion words.
If anadjective has higher confidence to be an opinionword, the noun/noun phrase it modifies will havehigher confidence to be an opinion target.Similarly, if a noun/noun phrase has higherconfidence to be an opinion target, the adjectivewhich modifies it will be highly possible to be anopinion word.
It?s an iterative reinforcementprocess, which indicates that existing graph-basedalgorithms are applicable.Candidate Importance reflects the salience of acandidate in the corpus.
We assign an importancescore to an opinion target candidate f according toits -tf idf score, which is further normalized by thesum of -tf idf scores of all candidates.- ( )( )- ( )ctf idf cImportance ctf idf c??
(5)where c represents a candidate, tf is the termfrequency in the dataset, and df is computed byusing the Google n-gram corpus1.To model these two factors, a bipartite graph isconstructed, the vertices of which include allnouns/noun phrases and adjectives.
As shown inFigure 2, the white vertices represent nouns/nounphrases and the gray vertices represent adjectives.An edge between a noun/noun phrase and anadjective represents that there is an opinionrelation between them.
The weight on the edgesrepresents the association between them, which areestimated by using WTM, as shown in Eq.
(4).To estimate the confidence of each candidate onthis bipartite graph, we exploit a graph-basedalgorithm, where we use C to represent candidateconfidence vector, a 1n?
vector.
We set thecandidate initial confidence with candidateimportance score, i.e.
0C S?
, where S is thecandidate initial confidence vector and each itemin S is computed using Eq.
(5).Figure 2: Bipartite graph for modeling relationsbetween opinion targets and opinion words1 http://books.google.com/ngrams/datasets..........Opinion Word Candidates (adjectives)Opinion Target Candidates (nouns/noun phrases)1350Then we compute the candidate confidence byusing the following iterative formula.1t T tC M M C?
?
?
?
(6)where tC is the candidate confidence vector attime t , and 1tC ?
is the candidate confidencevector at time 1t ?
.
M is an opinion relevancematrix, a m n?
matrix, where ,i jM is theassociated weight between a noun/noun phrasei and an adjective j .To consider the candidate importance scores, weintroduce a reallocate condition: combining thecandidate opinion relevance with the candidateimportance at each step.
Thus we can get the finalrecursive form of the candidate confidence asfollows.1 (1 )t T tC M M C S?
??
?
?
?
?
?
?
?
(7)where [0,1]??
is the proportion of candidateimportance in the candidate confidence.
When1?
?
, the candidate confidence is completelydetermined by the candidate importance; and when0?
?
, the candidate confidence is determined bythe candidate opinion relevance.
We will discussits effect in the section of experiments.To solve Eq.
(7), we rewrite it as the followingform.1( (1 ) )TC I M M S?
?
??
?
?
?
?
?
?
(8)where I is an identity matrix.
To handle theinverse of the matrix, we expand the Eq.
(8) as apower series as following.
[ ]kC I B B S??
?
?
?
?
?
(9)where (1 ) TB M M??
?
?
?and [0, )k?
?
is anapproximate factor.
In experiments, we set100k ?
.
Using this equation, we estimateconfidences for opinion target candidates.
Thecandidates with higher confidence scores than thethreshold will be extracted as the opinion targets.4 Experiments4.1 Datasets and Evaluation MetricsIn our experiments, we select three real worlddatasets to evaluate our approach.
The first datasetis COAE2008 dataset22, which contains Chinesereviews of four different products.
The detailed2 http://ir-china.org.cn/coae2008.htmlinformation can be seen in Table 2.
Moreover, toevaluate our method comprehensively, we collect alarger collection named by Large, which includesthree corpora from three different domains anddifferent languages.
The detailed statisticalinformation of this dataset is also shown in Table 2.Restaurant is crawled from the Chinese Web site:www.dianping.com.
The Hotel and MP3 3  wereused in (Wang et al 2011), which are respectivelyclawed from www.tripadvisor.com andwww.amazon.com.
For each collection, weperform random sampling to generate testingdataset, which include 6,000 sentences for eachdomain.
Then the opinion targets in Large weremanually annotated as the gold standard forevaluations.
Three annotators are involved in theannotation process as follows.
First, everynoun/noun phrase and its contexts in reviewsentences are extracted.
Then two annotators wererequired to judge whether every noun/noun phraseis opinion target or not.
If a conflict happens, athird annotator will make judgment for finialresults.
The inter-agreement was 0.72.
In total, werespectively obtain 1,112, 1,241 and 1,850 opiniontargets in Hotel, MP3 and Restaurant.
The thirddataset is Customer Review Datasets 4  (Englishreviews of five products), which was also used in(Hu et al 2004; Qiu et al 2011).
They havelabeled opinion targets.
The detailed informationcan be found in (Hu et al 2004).Domain Language #Sentence #ReviewsCamera Chinese 2075 137Car Chinese 4783 157Laptop Chinese 1034 56Phone Chinese 2644 123(a) COAE2008 dataset2Domain Language #Sentence #ReviewsHotel English 1,855,351 185,829MP3 English 289,931 30,837Restaurant Chinese 1,683,129 395,124(b) LargeTable 2: Experimental Data Sets, # denotes the sizeof the reviews/sentencesIn experiments, each review is segmented intosentences according to punctuations.
Thensentences are tokenized and the part-of-speech of3 http://sifaka.cs.uiuc.edu/~wang296/Data/index.html4 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html1351MethodsCamera Car Laptop PhoneP R F P R F P R F P R FHu 0.63 0.65 0.64 0.62 0.58 0.60 0.51 0.67 0.58 0.69 0.60 0.64DP 0.71 0.70 0.70 0.72 0.65 0.68 0.58 0.69 0.63 0.78 0.66 0.72Zhang 0.71 0.78 0.74 0.69 0.68 0.68 0.57 0.80 0.67 0.80 0.71 0.75Ours 0.75 0.81 0.78 0.71 0.71 0.71 0.61 0.85 0.71 0.83 0.74 0.78Table 3: Experiments on COAE2008 dataset2MethodsHotel MP3 RestaurantP R F P R F P R FHu 0.60  0.65  0.62  0.61  0.68  0.64  0.64  0.69  0.66DP 0.67  0.69  0.68  0.69  0.70  0.69  0.74  0.72  0.73Zhang 0.67  0.76  0.71  0.67  0.77  0.72  0.75  0.79  0.77Ours 0.71  0.80  0.75  0.70  0.82  0.76  0.80  0.84  0.82Table 4: Experiments on LargeMethodsD1 D2 D3 D4 D5P R F P R F P R F P R F P R FHu 0.75  0.82  0.78  0.71  0.79  0.75  0.72  0.76  0.74  0.69  0.82  0.75  0.74  0.80  0.77DP 0.87  0.81  0.84  0.90  0.81  0.85  0.90  0.86  0.88  0.81  0.84  0.82  0.92  0.86  0.89Zhang 0.83  0.84  0.83  0.86  0.85  0.85  0.86  0.88  0.87  0.80  0.85  0.83  0.86  0.86  0.86Ours 0.84  0.85  0.84  0.87  0.85  0.86  0.88  0.89  0.88  0.81  0.85  0.83  0.89  0.87  0.88Table 5: Experiments on Customer Review Dataseteach word is assigned.
Stanford NLP tool5 is usedto perform POS-tagging and dependency parsing.The method in (Zhu et al 2009) is used to identifynoun phrases.
We select precision, recall and F-measure as the evaluation metrics.
We alsoperform a significant test, i.e., a t-test with adefault significant level of 0.05.4.2 Our Methods vs. State-of-art MethodsTo prove the effectiveness of our method, weselect the following state-of-art unsupervisedmethods as baselines for comparison.1) Hu is the method described in (Hu et al 2004),which extracted opinion targets by using adjacentrule.2) DP is the method described in (Qiu et al 2011),which used Double Propagation algorithm toextract opinion targets depending on syntacticrelations between words.3) Zhang is the method described in (Zhang et al2010), which is an extension of DP.
They extractedopinion targets candidates using syntactic patternsand other specific patterns.
Then HITS (Kleinberg1999) algorithm combined with candidatefrequency is employed to rank the results foropinion target extraction.Hu is selected to represent adjacent methods foropinion target extraction.
And DP and Zhang are5 http://nlp.stanford.edu/software/tagger.shtmlselected to represent syntax-based methods.
Theparameter settings in these three baselines are thesame as the original papers.
In special, for DP andZhang, we used the same patterns for differentlanguage reviews.
The overall performance resultsare shown in Table 3, 4 and 5, respectively, where?P?
denotes precision, ?R?
denotes recall and ?F?denotes F-measure.
Ours denotes full model of ourmethod, in which we use IBM-3 model foridentifying opinion relations between words.Moreover, we setmax 2?
?
in Eq.
(2) and 0.3?
?
inEq.
(7).
From results, we can make the followingobservations.1) Ours achieves performance improvement overother methods.
This indicates that our methodbased on word-based translation model iseffective for opinion target extraction.2) The graph-based methods (Ours and Zhang)outperform the methods using DoublePropagation (DP).
Similar observations havebeen made by Zhang et al(2010).
The reasonis that graph-based methods extract opiniontargets in a global framework and they caneffectively avoid the error propagation madeby traditional methods based on DoublePropagation.
Moreover, Ours outperformsZhang.
We believe the reason is that Oursconsider the opinion relevance and thecandidate importance in a unified graph-basedframework.
By contrast, Zhang only simply1352plus opinion relevance with frequency todetermine the candidate confidence.3) In Table 4, the improvement made by Ours onRestaurant (Chinese reviews) is larger thanthat on Hotel and MP3 (English reviews).
Thesame phenomenon can be found when wecompare the improvement made by Ours inTable 3 (Chinese reviews) with that in Table 5(English reviews).
We believe that reason isthat syntactic patterns used in DP and Zhangwere exploited based on English grammar,which may not be suitable to Chinese language.Moreover, another reason is that theperformance of parsing on Chinese texts is notbetter than that on English texts, which willhurt the performance of syntax-based methods(DP and Zhang).4) Compared the results in Table 3 with theresults in Table 4, we can observe that Oursobtains larger improvements with the increaseof the data size.
This indicates that our methodis more effective for opinion target extractionthan state-of-art methods, especially for largecorpora.
When the data size increase, themethods based on syntactic patterns willintroduce more noises due to the parsing errorson informal texts.
On the other side, Ours usesWTM other than parsing to identify opinionrelations between words, and the noises madeby inaccurate parsing can be avoided.
Thus,Ours can outperform baselines.5) In Table 5, Ours makes comparable resultswith baselines in Customer Review Datasets,although there is a little loss in precision insome domains.
We believe the reason is thatthe size of Customer Review Datasets is toosmall.
As a result, WTM may suffer from datasparseness for association estimation.Nevertheless, the average recall is improved.An Example In Table 6, we show top 10 opiniontargets extracted by Hu, DP, Zhang and Ours inMP3 of Large.
In Hu and DP, since they didn?trank the results, their results are ranked accordingto frequency in this experiment.
The errors aremarked in bold face.
From these examples, we cansee Ours extracts more correct opinion targets thanothers.
In special, Ours outperforms Zhang.
Itindicates the effectiveness of our graph-basedmethod for candidate confidence estimation.Moreover, Ours considers candidate importancebesides opinion relevance, so some specificopinion targets are ranked to the fore, such as?voice recorder?, ?fm radio?
and ?lcd screen?.4.3 Effect of Word-based Translation ModelIn this subsection, we aim to prove theeffectiveness of our WTM for estimatingassociations between opinion targets and opinionwords.
For comparison, we select two baselines forcomparison, named as Adjacent and Syntax.
Thesebaselines respectively use adjacent rule (Hu et al2004; Wang et al 2008) and syntactic patterns(Qiu et al 2009) to identify opinion relations insentences.
Then the same method (Eq.3 and Eq.4)is used to estimate associations between opiniontargets and opinion words.
At last the same graph-based method (in Section 3.3) is used to extractopinion targets.
Due to the limitation of the space,the experimental results only on COAE2008dataset2 and Large are shown in Figure 3.Figure 3: Experimental comparison amongdifferent relation identification methodsHu quality, thing, drive, feature, battery, sound,time, music, priceDP quality, battery, software, device, screen, file,thing, feature, battery lifeZhang quality, size, battery life, hour, version, function,upgrade, number, musicOurs quality, battery life, voice recorder, video, fmradio, battery, file system, screen, lcd screenTable 6: Top 10 opinion targets extracted bydifferent methods.In Figure 3, we observe that Ours using WTMmakes significant improvements compared with1353two baselines, both on precision and recall.
Itindicates that WTM is effective for identifyingopinion relations, which makes the estimation ofthe associations be more precise.4.4 Effect of Our Graph-based MethodIn this subsection, we aim to prove theeffectiveness of our graph-based method foropinion target extraction.
We design two baselines,named as WTM_DP and WTM_HITS.
BothWTM_DP and WTM_HITS use WTM to mineassociations between opinion targets and opinionwords.
Then, WTM_DP uses Double Propagationadapted in (Wang et al2008; Qiu et al2009) toextract opinion targets, which only consider thecandidate opinion relevance.
WTM_HITS uses agraph-based method of Zhang et al(2010) toextract opinion targets, which consider bothcandidate opinion relevance and frequency.
Figure4 gives the experimental results on COAE2008dataset2 and Large.
In Figure 4, we can observethat our graph-based algorithm outperforms notonly the method based on Double Propagation, butalso the previous graph-based approach.Figure 4: Experimental Comparison betweendifferent ranking algorithms4.5 Parameter Influences4.5.1 Effect of Different WTMsIn section 3, we use three different WTMs in Eq.
(2) to identify opinion relations.
In this subsection,we make comparison among them.
Experimentalresults on COAE2008 dataset2 and Large areshown in Figure 5.
Ours_1, Ours_2 and Ours_3respectively denote our method using differentWTMs (IBM 1~3).
From the results in Figure 5,we observe that Ours_2 outperforms Ours_1,which indicates that word position is useful foridentifying opinion relations.
Furthermore, Ours_3outperforms other models, which indicates thatconsidering the fertility of a word can producebetter performance.4.5.2 Effect of ?In our method, when we employ Eq.
(7) to assignconfidence score to each candidate,[0,1]??
decides the proportion of candidateimportance in our method.
Due to the limitation ofspace, we only show the F-measure of Ours onCOAE2008 dataset2 and Large when varying ?
inFigure 6.In Figure 6, curves increase firstly, and decreasewith the increase of ?
.
The best performance isobtained when ?
is around 0.3.
It indicates thatcandidate importance and candidate opinionrelevance are both important for candidateconfidence estimation.
The performance of opiniontarget extraction benefits from their combination.Figure 5.
Experimental results by using differentword-based translation model.Figure 6.
Experimental results when varying ?13545 Conclusions and Future WorkThis paper proposes a novel graph-based approachto extract opinion targets using WTM.
Comparedwith previous adjacent methods and syntax-basedmethods, by using WTM, our method can captureopinion relations more precisely and therefore bemore effective for opinion target extraction,especially for large informal Web corpora.In future work, we plan to use other wordalignment methods, such as discriminative model(Liu et al 2010) for this task.
Meanwhile, we willadd some syntactic information into WTM toconstrain the word alignment process, in order toidentify opinion relations between words moreprecisely.
Moreover, we believe that there aresome verbs or nouns can be opinion words andthey may be helpful for opinion target extraction.And we think that it?s useful to add some priorknowledge of opinion words (sentiment lexicon) inour model for estimating candidate opinionrelevance.AcknowledgementsThe work is supported by the National NaturalScience Foundation of China (Grant No.61070106), the National Basic Research Programof China (Grant No.
2012CB316300), TsinghuaNational Laboratory for Information Science andTechnology (TNList) Cross-discipline Foundationand the Opening Project of Beijing Key Laboratoryof Internet Culture and Digital DisseminationResearch (Grant No.
5026035403).
We thank theanonymous reviewers for their insightfulcomments.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguistics,19(2): 263-311.Xiaowen Ding, Bing Liu and Philip S. Yu.
2008.
AHolistic Lexicon-Based Approach to Opinion Mining.In Proceedings of WSDM 2008.Xiaowen Ding and Bing Liu.
2010.
Resolving Objectand Attribute Reference in Opinion Mining.
InProceedings of COLING 2010.Mingqin Hu and Bing Liu.
2004.
Mining andSummarizing Customer Reviews.
In Proceedings ofKDD 2004Minqing Hu and Bing Liu.
2004.
Mining OpinionFeatures in Customer Reviews.
In Proceedings ofAAAI-2004, San Jose, USA, July 2004.Wei Jin and Huang Hay Ho.
A Novel LexicalizedHMM-based Learning Framework for Web OpinionMining.
In Proceedings of ICML 2009.Jon Klernberg.
1999.
Authoritative Sources inHyperlinked Environment.
Journal of the ACM 46(5):604-632Zhuang Li, Feng Jing, Xiao-yan Zhu.
2006.
MovieReview Mining and Summarization.
In Proceedingsof CIKM 2006Fangtao Li, Chao Han, Minlie Huang and Xiaoyan Zhu.2010.
Structure-Aware Review Mining andSummarization.
In Proceedings of COLING 2010.Zhichao Li, Min Zhang, Shaoping Ma, Bo Zhou, YuSun.
Automatic Extraction for Product FeatureWords from Comments on the Web.
In Proceedingsof AIRS 2009.Bing Liu, Hu Mingqing and Cheng Junsheng.
2005.Opinion Observer: Analyzing and ComparingOpinions on the Web.
In Proceedings of WWW 2005Bing Liu.
2006.
Web Data Mining: ExploringHyperlinks, contents and usage data.
Springer, 2006Bing Liu.
2010.
Sentiment analysis and subjectivity.Handbook of Natural Language Processing, secondedition, 2010.Yang Liu, Qun Liu, and Shouxun Lin.
2010.Discriminative word alignment by linear modeling.Computational Linguistics, 36(3):303?339.Zhanyi Liu, Haifeng Wang, Hua Wu and Sheng Li.2009.
Collocation Extraction Using MonolingualWord Alignment Model.
In Proceedings of EMNLP2009.Tengfei Ma and Xiaojun Wan.
2010.
Opinion TargetExtraction in Chinese News Comments.
InProceedings of COLING 2010.Popescu, Ana-Maria and Oren, Etzioni.
2005.Extracting produt fedatures and opinions fromreviews.
In Proceedings of EMNLP 2005Guang Qiu, Bing Liu., Jiajun Bu and Chun Che.
2009.Expanding Domain Sentiment Lexicon throughDouble Popagation.
In Proceedings of IJCAI 2009Guang Qiu, Bing Liu, Jiajun Bu and Chun Chen.
2011.Opinion Word Expansion and Target Extraction1355through Double Propagation.
ComputationalLinguistics, March 2011, Vol.
37, No.
1: 9.27Qi Su, Xinying Xu., Honglei Guo, Zhili Guo, Xian Wu,Xiaoxun Zhang, Bin Swen and Zhong Su.
2008.Hidden Sentiment Association in Chinese WebOpinion Mining.
In Proceedings of WWW 2008Bo Wang, Houfeng Wang.
Bootstrapping both ProductFeatures and Opinion Words from Chinese CustomerReviews with Cross-Inducing.
In Proceedings ofIJCNLP 2008.Hongning Wang, Yue Lu and Chengxiang Zhai.
2011.Latent Aspect Rating Analysis without AspectKeyword Supervision.
In Proceedings of KDD 2011.Yuanbin Wu, Qi Zhang, Xuangjing Huang and LideWu, 2009, Phrase Dependency Parsing For OpinionMining, In Proceedings of EMNLP 2009Lei Zhang, Bing Liu, Suk Hwan Lim and EamonnO?Brien-Strain.
2010.
Extracting and RankingProduct Features in Opinion Documents.
InProceedings of COLING 2010.Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara,Joseph Johnson, Xuanjing Huang.
2009.
MiningProduct Reviews Based on Shallow DependencyParsing, In Proceedings of SIGIR 2009.Guangyou Zhou, Li Cai, Jun Zhao and Kang Liu.
2011.Phrase-based Translation Model for QuestionRetrieval in Community Question Answer Archives.In Proceedings of ACL 2011.Jingbo Zhu, Huizhen Wang, Benjamin K. Tsou andMuhua Zhu.
2009.
Multi-aspect Opinion Pollingfrom Textual Reviews.
In Proceedings of CIKM2009.1356
