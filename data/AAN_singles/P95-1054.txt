Quantifying lexical influence:Giving direction to contextV Kr ip~sundarkripa~cs, buffalo, eduCEDAR & Dept.
of Computer  ScienceSUNY at BuffaloBuffalo NY 14260, USAAbst rac tThe relevance of context in disambiguat-ing natural anguage input has been widelyacknowledged in the literature.
However,most attempts at formalising the intuitivenotion of context tend to treat the word andits context symmetrically.
We demonstratehere that traditional measures such as mu-tual information score are likely to overlooka significant fraction of all co-occurrencephenomena in natural language.
We alsopropose metrics for measuring directed lex-ical influence and compare performances.Keywords:  contextual post-processing,defining context, lexical influence, direc-tionality of context1 In t roduct ionIt is widely accepted that context plays a significantrole in shaping all aspects of language.
Indeed, com-prehension would be utterly impossible without theextensive application of contextual information.
Ev-idence from psycholinguistic and cognitive psycho-logical studies also demonstrates that contextual in-formation affects the activation levels of lexical can-didates during the process of perception (Weinreich,1980; McClelland, 1987).
Garvin (1972) describesthe role of context as follows:\[The meaning of\] a particular text \[is\] notthe system-derived meaning as a whole, butthat part of it which is included in the con-textually and situationally derived mean-ing proper to the text in question.
(p. 69-70)In effect, this means that the context of a word servesto restrict its sense.The problem addressed in this research is thatof improving the performance of a natural-languagerecogniser (such as a recognition system for hand-written or spoken language).
The recogniser out-put typically consists of an ordered set of candidatewords (word-choices) for each word position in theinput stream.
Since natural language abounds incontextual information, it is reasonable to utilise thisin improving the performance of the recogniser (bydisambiguating among the word-choices).The word-choices (together with their confidencevalues) constitute a confusion set.
The recognisermay further associate a confidence-value with each ofits word choices to communicate finer resolution inits output.
The language module must update theseconfidence values to reflect contextual knowledge.2 L ingu is t i c  post -process ingThe language module can, in principle, performseveral types of "post-processing" on the word-candidate lists that the recogniser outputs for thedifferent word-positions.
The most promising possi-bilities are:?
re-ranking the confusion set (and assigning newconfidence-values to its entries), and,?
deleting low-confidence entries from the confu-sion set (after applying contextual knowledge)Several researchers in NLP have acknowledged therelevance of context in disambiguating natural an-guage input ((Evett et al, 1991); (Zernik, 1991);(Hindle & Rooth, 1993); (Rosenfeld, 1994)).
In fact,the recent revival of interest in statistical languageprocessing is partly because of its (comparative) suc-cess in modelling context.
However, a theoreticallysound definition of context is needed to ensure thatsuch re-ranking and deleting of word-choices helpsand not hinders (Gale & Church, 1990).Researchers in information theory have come upwith many inter-related formalisations ofthe ideas ofcontext and contextual influence, such as mutual in-formation and joint entropy.
However, to our knowl-edge, all attempts at arriving at a theoretical basisfor formalising the intuitive notion of context havetreated the word and its context symmetrically.Many researchers ((Smadja, 1991); (Srihari & Bal-tus, 1993)) have suggested that the information-theoretic notion of mutual information score (MIS)directly captures the idea of context.
However, MIS332is deficient in its ability to detect one-sided correla-tions (cf.
Table 1), and our research indicates thatasymmetric nfluence measures are required to prop-erly handle them (Krip?sundar, 1994).For example, it seems quite unlikely that anysymmetric information measure can accurately cap-ture the co-occurrence r lationship between the twowords 'Paleolithic' and 'age' in the phrase 'Pale-olithic age'.
The suggestion that 'age' exerts as muchinfluence on 'Paleolithic' as vice versa seems ridicu-lous, to say the least.
What is needed here is a di-rected (ie, one-sided)influence measure  (DIM), some-thing that serves as a measure of influence of oneword on another, rather than as a simple, symmet-ric, "co-existence probability" of two words.
Table 1illustrates how a DIM can be effective in detectinglexical and lexico-semantic associations.3 Compar ing  measures  o f  lex ica lin f luenceWe used a section of the Wall Street Journal (WSJ)corpus containing 102K sentences (over two millionwords) as the training corpus for the partial resultsdescribed here.
The lexicon used was a simple 30K-word superset of the vocabulary of the training cor-pus.The results shown here serve to strengthen ourhypothesis that non-standard information measuresare needed for the proper utilisation of linguisticcontext.
Table 1 shows some pairs of words thatexhibit differing degrees of influence on each other.It also demonstrates very effectively that one-sidedinformation measures are much better than sym-metric measures at utilising context properly.
Thearrow between each pair of words in the table in-dicates the direction of influence (or flow of infor-mation).
The preponderance of word-pairs that ex-hibit only one direction of significant influence (eg,'according'---~'to') shows that no symmetric scorecould have captured the correlations in all of thesephrases.Our formulation of directed influence is still evolv-ing.
The word-pairs in Table 1 have been selectedrandomly from the test-set with the criterion thatthey scored "significantly" (ie, > 0.9) on at leastone of the three measures D1, D2 and D3.
The fourmeasures (including MIS) are defined as follows:?
, "  P(w,w2) MIS(wlw2) = log\[e(,$,)e(w2) jDl(wl/w2) = P(w~) = #~2D2(wl/w2) = ste~l  ( w /w1~ ~ n lr" k~Cmax\ ]  ""D3(wl/w2) = ote,,O?
~--v-x_~--z~ ,, r~l ~''\ #Cmax\] .
.
.
.In these definitions, #wlw2 denotes the frequencyof co-occurrence of the words wl and w2,1 while1Note that the exact word order of wl and w2 is ir-relevant here.#Wl, and #w~ represent (respectively) the frequen-cies of their (unconditional) occurrence.#Cmax a~--!
max(@wlw2) is defined to be theWlt~2maximum co-occurrence frequency in the corpus,and appears to be a better normalisation factor thanthe size of the corpus itself.The definition of MIS implicitly incorporates thesize of the corpus, since it has two P0  terms in thedenominator, and only one in the numerator.
TheDIM's, on the other hand, have balanced fractions.Therefore, we have not included a log-term in thedefinitions of D1, D2, and D3 above.D1 is a straightforward estimation of the condi-tional probability of co-occurrence.
It forms a base-line for performance valuations, but is prone tosparse data problems (Dunning, 1993).The s tep( )  functions in D2 and D3 represent twoattempts at minimising such errors.
These functionsare piecewise-linear mappings of the normalised co-occurrence frequency, and are used as scaling factors.Their effect is apparent in Table 1, especially in thebottom third of the table, where the low frequencyof the primer pushes D3 down to insignificant levels.The metrics D2 and D3 can and should be nor-mMised, perhaps to the 0-1 range, in order to fa-cilitate integration with other metrics such as therecogniser's confidence value.
Similarly, the lack ofnormalisation of MIS hampers direct comparison ofscores with the three DIM's.4 D iscuss ionOf the several different ypes of word-level associ-ations, lexical and lexico-semantic associations areamong the most significant local associations.
Lexi-cal (or associative) context is characterised by rigidword order, and usually implies that the primer andthe primed together act as one lexical unit.
Lexico-semantic associations are exemplified by phrasal verbs(eg, 'fix up'), and are characterised by morphologicalcomplexity in the verb part and spatial flexibility inthe phrase as a whole.It is noteworthy that all the three DIM's capturethe notions of lexical (ie, fixed) and lexico-semanticassociations in one formula (albeit to differing de-grees of success).
Thus we have 'staff' and 're-porter' influencing each other almost equally, whilethe asymmetric influence on 'in' from its right con-text ('addition') is also detected by the DIM's.It is our contention that symmetric measuresconstrain the re-ranking/proposing process signifi-cantly, since they are essentially blind to a signif-icant fraction (perhaps more than ha/f) of all co-occurrence phenomena in natural anguage.5 Summary  and  Future  WorkThe preliminary results described in this work es-tablish clearly that non-standard metrics of lexical333Word-pa~r WL WRnew *-- yor -b -~according --* tostaff *- reporterstaff --* reporternew ~ yorkon -* thevice --* presidentat *-- leastcompared --* with-~6927,2697,2338"~ 5 .5510.8663.4633.463(1084, 54580, 1083) II 3"62910"99912.99612.996 II(1613, 1205, 1157) II 7.111 10.96012.87912.879 II(1613, 1205, 1157) II 7"11101"71712"15012-150 II(6927, 2697, 2338) II 5.551 10.3371 1.3481 1.348 II(13025, 116356, 3483) \[I 1-554 I 0-267 I 1-3341 1.334 II(1017, 2678, 784) II 6"38410"7701 .5401 1.285 II(11158, 795, 665) II 5.03910.8361 .6711 1.247 II585, 11362, 551)Table 1: Asymmetry  in co-occurrence relationships: Word-pairs with "significant" influence in eitherdirection have been selected randomly from the test-set.
Note that very few of these pairs exhibit comparableinfluence on each other.
The arrows indicate the direction of lexical influence (or information flow).
A DIMscore of 1 or more implies a significant association, whereas an MIS below 4 is considered a chance association.influence bear much promise.
In fact, what we re-ally need is a generalised information score, a measurethat takes into account several factors, such as:?
directionality in correlation?
multiple words participating in a lexical rela-tionship?
different (morphological) forms of words, and,?
spatial flexibility in the components of a collo-cationThe generalised information score would capture allthe variations that are introduced by the above fac-tors, and allow for the variants so as to reflect a"normalised" measure of contextual influence.We have also been working with experimentalmeasures which attach higher significance to thecollocation frequency, (measures which, in essence,"trust" the recogniser more often).
Our future workwill involve bringing these various factors togetherinto one integrated formalism.ReferencesMax Coltheart, editor.
1987.
Attention and Perfor-mance XII: The Psychology of Reading.
LawrenceErlbaum.Ted Dunning.
1993.
Accurate methods for thestatistics of surprise and coincidence.
Computa-tional Linguistics, 19:1:61-74.LJ Evett, CJ Wells, FG Keenan, T Rose, andPd Whitrow.
1991.
Using linguistic informationto aid handwriting recognition.
Proceedings ofthe International Workshop on Frontiers in Hand-writing Recognition, pages 303-311.WilliamA Gale and Kenneth W Church.
1990.
Poorestimates of context are worse than none.
In Pro-ceedings of the DARPA Speech and Natural Lan-guage Workshop, pages 283-287.Paul L Garvin.
1972.
On Machine Translation.Mouton.Donald ttindle and Mats Rooth.
1993.
Structuralambiguity and lexical relations.
ComputationalLinguistics, 19:1:103-120.V Kriphsundar.
1994.
Drawing on Linguistic Con-text to Resolve Ambiguities oR How to imrove re-congition in noisy domains.
Ph.D. thesis, Com-puter Science, SUNY@Buffalo.
(proposal).James L McClelland.
1987.
The case for interaction-ism in language processing.
In (Coltheart, 1987).Lawrence Erlbaum.Ronald Rosenfeld.
1994.
A hybrid approach toadaptive statistical language modeling.
Proceed-ings of the ARPA workshop on human languagetechnology, pages 76-81.Frank Smadja.
1991.
Macrocoding the lexicon withco-occurrence knowledge, in (Zernik, 1991), pages165-190.RShi .ni K Srihari and Charlotte M Baltus.
1993.
Useof language models in on-line recognition of hand-written sentences.
Proceedings of the Third Inter-national Workshop on Frontiers in HandwritingRecognition (IWFIIR III).SN Srihari, JJ IIull, and R Chaudhari.
1983.
In-tegrating diverse knowledge sources in text recog-nition.
ACM Transactions on Office InformationSystems, 1:1:68-87.RM Warren.
1970.
Perceptual restoration of missingspeech sounds.
Science, 167:392-393.Uriel Weinreich.
1980.
On Semantics.
University ofPennsylvania Press.Uri Zernik, editor.
1991.
Lezical Acquisition: Ex-ploiting On-line Resources to Build a Lexicon.Lawrence Erlbaum.334
