The NOMAD System:Expectation-Based Detection andCorrection of Errors during Understanding ofSyntactically and Semantically Ill-Formed Text 1Richard  H. GrangerArtif icial Intel l igence Project, Computer  Science Depar tmentand Cognit ive Sciences ProgramUniversity of CaliforniaIrvine, CA 92717Most large text-understanding systems have been designed under the assumption thatthe input text will be in reasonably "neat" form (for example, newspaper stories and otheredited texts).
However, a great deal of natural language text (for example, memos,messages, rough drafts, conversation transcripts, etc.)
have features that differ significantlyfrom "neat" texts, posing special problems for readers, such as misspelled words, missingwords, poor syntactic construction, unclear or ambiguous interpretation, missing crucialpunctuation, etc.
Our solution to these problems is to make use of expectations, based bothon knowledge of surface English and on world knowledge of the situation being described.These syntactic and semantic expectations can be used to figure out unknown words fromcontext, constrain the possible word senses of words with multiple meanings (ambiguity), fillin missing words (ellipsis), and resolve referents (anaphora).
This method of using expecta-tions to aid the understanding of "scruffy" texts has been incorporated into a workingcomputer program called NOMAD, which understands scruffy texts in the domain of Navyship-to-shore messages.1.
IntroductionThe NOMAD system takes unedited English input in aconstrained omain, and works interactively with theuser to encode the message into database-readableform.
The unedited texts in this domain are Navalship-to-shore messages, written in 'telegraphic' Eng-lish, often leaving out nouns and verbs, crucial punctu-ation (such as periods), and making use of ad hocabbreviations of words.
In addition to these problemsof surface-text processing, these texts can containproblems of interpretation - that is, which of severalobjects is being referred to, or which possible goalinference is implied.
These semantic processing prob-lems are not easily detectable or solvable based on thesurface text alone but rather require a data base of1 This research was supported in part by the Naval OceanSystems Center under contracts N-00123-81-C-1078 and N66001-83-C-0255, and by the National Science Foundation under grantIST-81-20685.knowledge about the domain of discourse, in this caseship movements.Here are examples of each of these two types ofproblems.
First, one with a number of surface-texterrors:(1) 'Locked on open fired destroyed'Example (1) is missing crucial punctuation (no bound-aries separating the three clauses from each other), ismissing subjects and objects for all three verb phrases,and has a tense mismatch in the middle phrase ( 'openfired').
(This is an actual message in the corpus pro-vided to us by the Navy, not a constructed example.
)NOMAD's output from this example is:We aimed at an unknown object.We fired at the object.The object was destroyed.A second message has, in addition to some surfaceproblems, a goal-based interpretation problem:Copyright 1984 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is grantedprovided that the copies are not made for direct commercial dvantage and the Journal reference and this copyright notice are included onthe first page.
To copy otherwise, or to republish, requires afee and/or specific permission.0362-613X/83/030188-09503.00188 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983Richard H. Granger The NOMAD System(2) 'Returned bombs to Kashin.
'In addition to the surface problem of a missing sub-ject, this example is apparently missing mention ofsome previous event, implied by the use of 'returned';and it describes an ambiguous event, that is, either thepeaceable delivery of bombs to the Kashin ship (atype of enemy ship) or a battle action of firing bombsin retaliation.
Since the input is ambiguous withoutthe previous message, NOMAD returns a number ofalternative possible outputs to the user, marking oneas "preferred":(Preferred Interpretation):We fired some bombs at a Kashin ship.
(Inferred):The Kashin ship fired at us previously.
(Alternate Interpretation):We delivered some bombs to a Kashin ship.
(Inferred):The Kashin ship had delivered some bombs tous previously.NOMAD is interactive: it produces multiple inter-pretations when necessary, and lets the message-sender choose among these alternatives.
A typicalscenario is:?
the user (message-sender) will enter a 'telegraphic'message;?
NOMAD will produce two different possible inter-pretations of the message in corrected English, andpresent hem to the user;?
the user will then choose one of the interpretations;and?
a database-readable version of the correctly-interpreted message is then forwarded from theship to a central data base.Many of the approaches to understanding ill-formed input focus on syntactic errors separately fromsemantic errors (for example, Hayes and Mouradian1981 and Kwasny and Sondheimer 1981).
Both ofthese efforts essentially attempt o increase the flexi-bility of an ATN syntactic parser: the first by using'parse suspension and continuation', relaxing con-straints on consistency and permitting matches out oftheir correct order, and the second by relaxing theconstraints required to traverse an ATN arc, and thenproviding 'deviance notes' specifying the differencesbetween what was expected and what was actuallyseen.
These efforts attempt to correct the surfaceform of the input, that is, to perform a transformationfrom an ill-formed English text to a well-formed Eng-lish text.
Their goals are not to produce a meaningrepresentation of the input, and hence cannot be saidto 'understand' the input.
This also leads to the ina-bility of these systems to generate alternative interpre-tations of text; once these systems have guessed at aparse, they cannot back up and re-parse in response toinformation from a user.The approach taken by Hayes and Carbonell(1981) is closer to that described in this paper, in thatthey do build meaning representations.
However,there are still shortcomings; in particular, their systemscannot understand texts in which a missing or un-known word is the one that would have built the mainsemantic ase frame.
As will be seen below, NOMADbuilds on the FOUL-UP system (Granger 1977) tohandle such cases (which are frequent in our domain).Furthermore, like the systems described above, theirsystems cannot re-interpret a text when its initial in-terpretation turns out to be incorrect.We propose an integrated system of syntactic andsemantic processing, in which world knowledge andsyntactic knowledge are both applied during text proc-cessing to provide a number of possible interpretationsof a text.
Our focus is on interpretations: the goal ofthe system is to give rise to an unambiguous meaningrepresentation.
If surface-text problems occur duringprocessing but an unambiguous interpretation can beprovided and confirmed by the user, then the surface-text problems are ignored.
It is only when interpreta-tion problems arise that any noted surface-text prob-lems will be consulted to see if they might have beenthe source of the interpretation problem.
That is, weare attempting to attack the overall problem of proc-essing text, of which the processing of ill-formed textis a necessary subpart.
Our approach implies that theprocessing of ill-formed text 'falls out' of normal textprocessing, via the application of generalized error-correction processes that operate equally on syntax,semantics, and pragmatics, and are not designed spe-cifically for the processing of ill-formed surface text.NOMAD builds on previous work on conceptualanalysis (Riesbeck and Schank 1976, Birnbaum andSelfridge 1979), and on error detection and correctionduring conceptual analysis (Granger 1977, 1980,1982a).
Selfridge and Engelberg (1984), Lebowitz(1984), and Dyer (1983) have also recently takenapproaches that are similar to the one proposed here,attempting to fully exploit the power of integratedunderstanding.
NOMAD incorporates and integrateserror detection and correction algorithms based onboth syntactic and pragmatic error types, and is there-fore capable of correctly processing a wide range ofill-formed texts within the knowledge domain of Navymessages.
NOMAD has actually been installed and isbeing used for message processing by the Naval OceanSystems Center (NOSC) at San Diego.American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 189Richard H. Granger The NOMAD System2.
Background: Tolerant Text Processing2.1.
FOUL-UP figured out unknown words fromcontextThe FOUL-UP program (Figuring Out Unknown Lex-emes in the Understanding Process; Granger 1977)was the first program that could figure out meaningsof unknown words encountered during text under-standing.
FOUL-UP was an attempt o model the cor-responding human ability commonly known at"figuring out a word from context".
FOUL-UP workedwith the SAM system (Cull ingford 1977), using theexpectations generated by scripts (Schank and Abelson1977) to restrict the possible meanings of a word,based on what object or action would have occurred inthat position according to the script for the story.For instance, consider the following excerpt from anewspaper report of a car accident:(1) Friday, a car swerved off Route 69.
The vehiclestruck an embankment.The word "embankment"  was unknown to the SAMsystem, but it had encoded predictions about certainattributes of the expected conceptual object of thePROPEL action (the object that the vehicle struck);namely, that it would be a physical object, and wouldfunction as an "obstruct ion"  in the vehicle-accidentscript.
(In addition, the conceptual analyzer (ELI -Riesbeck and Schank 1976) had the expectation thatthe word in that sentence position would be a noun.
)Hence, when the unknown word was encountered,FOUL-UP would make use of those expected attributesto construct a memory entry for the word"embankment" ,  indicating that it was a noun, a physi-cal object, and an "obstruct ion"  in vehicle-accidentsituations.
It would then create a dictionary definitionthat the system would use from then on whenever theword was encountered in this context.2.2.
Syntactic (surface) and semantic(interpreation) text errorsBut even if the SAM system had known the word"embankment" ,  it would not have been able to handlea less edited version of the story, such as this'telegraphic' message, which might have been sent inby an on-the-scene reporter:(2) Vehcle ace Rt69; car strck embankment;  drivrdead one psngr inj; ser dmg to car full rptfrthcmng.While human readers would have little difficulty un-derstanding this text, no existing computer programscould do so.The scope of this problem is wide; examples oftexts that present "scruf fy"  difficulties to readers arecompletely unedited texts, such as messages composedin a hurry, with little or no re-writing, rough drafts,memos, transcripts of conversations, etc.
Such textsmay contain these problems, among others: missingwords, ad hoc abbreviations of words, poor syntax,confusing order of presentation of ideas, misspellings,lack of punctuation.
Even edited texts such as news-paper stories often contain misspellings, words un-known to the reader, and ambiguities; and even appar-ently very simple texts may contain alternative possi-ble interpretations, which can cause a reader to con-struct erroneous initial inferences that must later becorrected (see Granger 1980, 1981a, 1981b).The following sections describe the NOMAD sys-tem, which incorporates FOUL-UP's abilities as well assignificantly extended abilities to use syntactic andsemantic expectations to resolve these difficulties, inthe domain of Navy messages.
NOMAD's processing isdivided into two major categories:(1) blame assignment, that is, the detection of an er-ror and the attr ibution of that error to somesource; and(2) error correction, the remedy for the source of theerror.3.
How NOMAD Recognizes and CorrectsErrors3.1.
IntroductionNOMAD incorporates ideas from, and builds on, earlierwork on conceptual analysis (for example, Reisbeckand Schank 1976, Birnbaum and Selfridge 1979), situ-ation and intention inference (for example, Cullingford1977, Wilensky 1978), and English generation (forexample, Goldman 1973, McGuire 1980).
What dif-ferentiates NOMAD significantly from its predecessorsare its error recognition and error correction abilities,which enable it to read texts more complex than thosethat can be handled by other text understanding sys-tems.NOMAD operates by attempting to process textsleft to right, with each word capable of suggesting newexpectations (for example, a verb will follow, the pre-vious noun group should serve as actor of the currentact, etc.
), and applying those suggested expectations tonew inputs.
When expectations are met, they result inadditions to the ongoing meaning representation of thetext; when they are not met, they result in 'surface-text alerts', which are collected for potential ater cor-rective processing.There are two types of 'errors'  in NOMAD: surface-text errors and interpretation errors.
Surface-texterrors are potential problems that can be readily de-tected at surface-text processing time, including, forexample, unknown words and any surface expectationviolations, whether syntactic or semantic.
For in-stance, a syntactic expectation failure such as 'no noun190 American Journal of Computat ional  Linguistics, Volume 9, Numbers 3-4, July-December 1983Richard H. Granger The NOMAD Systemgroup appearing where one was expected' is a surfacealert, but so is a semantic/pragmatic expectation fail-ure such as 'target noun group was expected to de-scribe an animate actor, but described an inanimateobject instead'.
Each of these is equally an expecta-tion failure, and no difference need be drawn at thisstage of processing between syntactic or semantictypes.
It will be seen later that, depending on the typeof surface alert, different suggestions will be made asto where to look to 'assign blame' for the problem,and how to attempt o correct it.Interpretation failures, on the other hand, are de-fined as those that cannot be easily ascribable to thefailure of some particular pre-defined surface expecta-tion; these arise after some conceptual analysis hasbeen successfully performed and the resulting repre-sentation fails to match pragmatic hecks such as goal-based or script-based knowledge of the situation beingdescribed.Following is a list of nine categories of problemswe have identified that occur often in scruffy uneditedtexts, five surface-text problems and four interpreta-tion problems.
Each problem is illustrated by a briefexample from the domain of Navy messages.
It willbe seen that these errors often occur in pairs, withsurface-text problems sometimes giving rise to inter-pretation problems.
Note that while these problemsare often referred to in this paper as 'errors' in factsome are not actual 'errors',  strictly speaking, but arerather potent ia l  problem indicators that NOMAD recog-nizes, which may give rise to subsequent interpretationproblems.Surface-text problems1.
Unknown words.Enemy "scudded" bombs at us.
- the verb is un-known to the system.2.
Missing subject, object, etc.
of sentences.Sighted enemy ship.
Fired.
- the actor who fired isnot explicitly stated.3.
Missing sentence and clause boundaries.Locked  on opened f ire.
- two actions, aiming andfiring.4.
Ambiguous word usage.Returned  bombs to Kash in .
- " returned"  in thesense of retaliation after a previous attack, or" returned"  in the sense of "peaceably  deliveredto"?5.
Lack of tense agreement.Open f i red.
- the intended tense of 'open'  is trans-ferred to 'fire'.Interpretation problems1.
Causality violation.Ship  s ighted overhead.
- ships can't  fly; probablemessage-sending error.2.
Goal violation.Returned  bombs to Kashin .
- one of two ambiguousinterpretations of ' returned'  (peaceably delivered)gives rise to apparent goal violation (deliveringweapons to enemy).3.
User confirmation failure.NOMAD's failure is not confirmed by user.
(Notethat this is considered by NOMAD to be an inter-pretation problem even thought it may be due tothe user's idiosyncrasies, as opposed to violation ofsome known semantic rule - the effect is thesame.
)4 .
Object or event referenced out of known eventsequence.Midway lost contact on Kashin.
- no previous con-tact mentioned; this often arises when typicalknown situations are mentioned in other than ster-eotypical (scripty) order.When these problems arise in a message, NOMADmust first recognize what the problem(s) is(are)(which is often difficult to do), and then attempt tocorrect the error(s).
The following section outlinesthe overall processing algorithms NOMAD uses toprocess these errors.3.2.
NOMAD's  e r ro r -detect ion  a lgor i thmNOMAD's algorithm for detection and solution of er-rors follows a four-step process:1.
Set 'alert '  flags wherever potential surface-textproblems are detected.2.
Do only partial processing of surface text if neces-sary due to missing or ambiguous information (thatis, do as much normal processing as possible in theface of missing information).3.
Check for interpretation problems (causal, goal,sequencing (script), or user confirmation errors)after surface sentence processing.4.
Try solutions based on surface 'alert' flag catego-ries.To illustrate this process, consider an ambiguoustext, 'contact gained on kashin'.
During the process-ing of this text, some surface-text alerts arise (forexample, 'contact'  can be either a noun or a verb'  ifit's a verb, then there's either a missing subject or anexpected passive subject coming, etc.
), and an inter-pretation ambiguity: the text can be interpreted asmeaning either(a) We established visual or radar contact with akashin ship.
(b) Our contact (that is, a ship in contact with us)increased its speed in a chase after a kashin ship.In the case of 'contact gained on kashin', NOMAD'sblame assignment algorithm moves through the abovesteps as follows:1.
(a) Set both 'ambiguous-word-sense'  and 'ambig-uous-part-of-speech' alerts for the word 'contact':it might be either a noun (that is, the ship that iscurrently our contact) or a verb (to establish radaror visual contact).American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 191Richard H. Granger The NOMAD System(b) Set 'ambiguous-word-sense'  alert for word'gained': it might mean either 'establ ished' as in'gained (established) radar contact',  or 'advanced'as in 'gained (advanced) on enemy during chase'.2.
Product alternate interpretations based on alternateassumptions about word senses: 'established radaror visual contact with kashin', and 'our contact shipadvanced on kashin'.3.
(a) Look for possible causality or goal violations:none found.
(b) Ask user for confirmation: user confirms oneinterpretation but not the other.4.
Solution: Select interpretation confirmed by user.Consider another example, 'Returned bombs toKashin'.
As noted above, one of two ambiguous inter-pretations of ' returned'  in this text (that is, the'(peaceably) delivered' interpretation) gives rise to anapparent goal violation (delivering weapons to enemy).In the case of ' returned bombs to kashin',  theb lame assignment algorithm acts as follows:1.
(a) Set 'ambiguous-word-sense'  al rt for the word'returned': it might have either of two categories ofmeaning, corresponding to 're-do a previously-doneact ion  (as in 'return the favor' ,  ' return atransmission') or 're-deliver a previously-deliveredobject  (as in 'return a (borrowed) book') .
(b) Set 'ambiguous-word-sense'  alert for word'bombs':  it might mean either the verb 'to bomb' ,present tense, or the plural noun.
The former in-terpretation (that bomb is a verb) also gives rise toa 'missing-clause-boundary'  surface alert, sincethen the ' returned'  and 'bombs '  verbs would benext to each other.2.
Produce alternate interpretations based on alternateassumptions about word senses: 'Delivered object(bombs) to kashin' (after they had delivered someto us) or 'fired on kashin' (after they had fired onus).
(The error-r idden alternate interpretationsthat arise from the verb sense of 'bombs'  are alsogenerated.)3.
(a) Look for possible causality or goal violations:With the 'delivery' interpretation, a potential viola-tion of one of NOMAD's known goals is found:Actors of class (enemies) transferring possession ofobjects of class (weapons) to recipients of class(friends), and vice versa.
(b) Order the interpretations in order of prefer-ence, based on both surface-class and interpreta-tion-class errors; the goal-violation case above isnot preferred, and the 'bombs-as-verb '  case is notpreferred, while the 'firing back at kashin' interpre-tation is preferred.
(e) Present preferred interpretation to user; con-firmed.
(If this had failed, then unpreferred inter-pretations would have been presented.)4.
Solution: Select confirmed interpretation.4.
Blame Ass ignment  in NOMADAs evidenced in the above examples, there is no simplerelationship between types of errors in the interpreta-tion of the input, and possible solutions to those er-rors.
This is primarily because the source of an inter-pretation error is difficult to identify.
In general, in-terpretation problems can arise from any of a numberof surface-text problems, including:1. words with multiple word sensesReturned  bombs to Kash in .
- see above discussion;2. missing clause boundariesCha l lenged ship re fused  to heave to.
- can be inter-preted in any of the following ways: (a) We chal-lenged a ship.
They refused to heave to.
(b) Wechallenged a ship.
We refused to heave to.
(c)The challenged ship refused to heave to.3.
elliptical or telegraphic sentence constructionContact  ga ined  on Kash in .
- can be interpreted as:(a) We established visual or radar contact with akashin ship.
(b) Our contact (that is, a ship incontact with us) increased its speed in a chase aftera kashin ship).As mentioned earlier, NOMAD's goal is to producecorrect, unambiguous interpretations of input texts.
Itsability to handle il l-formed surface text arises from aneed to be able to find surface-text problems that giverise to interpretation problems; it attends to surface-text problems not because they are useful in their ownright but only because they may be useful later insolving an interpretation problem.
NOMAD collectsboth surface-text problems and interpretation prob-lems as it processes a text, and for each interpretationproblem, it attempts to find a corresponding surfaceproblem that gave rise to it.
Once it has an interpreta-tion problem - surface problem pair, it suggests a solu-tion for the overall problem based on the characteris-tics of both the surface problem and the interpretationproblem.
In cases where only a surface problem existsand no interpretation problem has arisen, the surfaceproblem is simply ignored as being irrelevant to thetrue understanding oal of producing a correct, unam-biguous interpretation.
In cases where an interpreta-tion problem exists but no surface-text problem can belinked to it, NOMAD suggests possible solutions to theinterpretation problem that do not depend on surfaceproblems.The 'blame assignment chart'  below illustrates omeof NOMAD's heuristics for finding surface-text alertsthat might correspond to a given interpretation prob-lem.NOMAD's blame assignment algorithm is at thecenter of its ability to handle syntactically and seman-tically i l l-formed text.
Blame assignment in NOMAD iscapable of dealing with problems at both the surface-text level and the interpretation level, especially whereinterpretation problems arise indirectly from surface-192 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, Ju ly -December 1983Richard H. Granger The NOMAD SystemINTERPRETATION PROBLEMOnly partial representationconstructedCausality violation,Goal violation,User confirmation failureActor or object referenceout of sequenceEvent referencedout of sequenceSUGGESTED SURFACE-TEXT ALERTUnknown wordWord with multipleword sensesExpectation failures:-Syntactic (word)-Semantic-Boundary (phrase)(No surface alert)(No surface alert) /Blame Assignment ChartSUGGESTED POTENTIAL  SOLUTIONSFOUL-UP:Expectations and Act-PreferenceTry alternate word senseTry inferring clause breakTry situation-frame inferencelevel decisions; in general, there is no simple relation-ship among surface-text problems, interpretation prob-lems, and potential solutions for these problems.4,1.
Recogn iz ing  and cor rec t ing  sur face  er rorsFor each of the five categories of surface problemshandled by the system, NOMAD's method of recogniz-ing and correcting the problem is briefly describedhere, along with actual English input and output fromNOMAD.1.
INPUT:ENEMY SCUDDED BOMBS AT US.Problem: Unknown word.
The unknown word"scudded" is trivial to recognize as being unknown,since it is the only word without a dictionary entry.Once it has been recognized, NOMAD checks it tosee if it could be (a) a misspelling, (b) an abbrevia-tion, or (c) a regular verb-tense of some knownword.Solution: Use expectations to figure out wordmeaning from context.
When the spelling checkersfail, a FOUL-UP mechanism is called that uses syn-tactic expectation (and some morphological analy-sis) to infer that 'scudded' is probably a verb, andthen uses pragmatic knowledge of what actions canbe done by an 'enemy' ACTOR, to a 'weapon'OBJECT, direct TO us.
At this point, NOMAD usesa mechanism we term 'ACT-preference' (Granger1977), which exploits both pragmatic knowledge ofwhat enemies tend to do with weapons, and word-order knowledge that we have derived of how par-ticular triads of prepositions, noun-categories, andverb-categories tend to combine (for example,'BLAGHED <weapon> AT <ship>'  will give riseto a different inference than 'BLAGHED<weapon> TO <ship>',  or 'BLAGHED <weapon>FOR <ship>' ,  etc.).
This process, detailed inGranger (1977), arrives at an inference that theaction is probably a 'PROPEL' (see Schank andAbelson 1977).
Again, this is only an educatedguess by the system, and may have to be correctedlater on the basis of further information (see Gran-ger 1980, 1981b).Finally, NOMAD produces an interpretation ofthe input, which the user may or may not confirm.In the event that the user does not confirmNOMAD's initial interpretation, a number of alter-native interpretations are produced (see Granger1981a, 1982c) until one is confirmed, or the proc-ess fails.
In this and the following examples,NOMAD's 'preferred' interpretation is confirmed bythe user.NOMAD OUTPUT:An enemy ship fired bombs at our ship.2.
INPUT:MIDWAY SIGHTED ENEMY.
FIRED.Problem: Missing subject and objects.
'Fired'builds a PROPEL, and expects a subject and objectsto play the conceptual roles of ACTOR (who didthe PROPELing), OBJECT (what got PROPELed)and RECIPIENT (who got PROPELed at).
Howev-er, no surface subjects or objects are presentedhere.Solution: Use expectations to fill in conceptualcases.
NOMAD uses situational (script-based) ex-pectations from the known typical sequence ofevents in an "ATTACK" - which consists of amovement (PTRANS), a sighting (ATTEND) andAmerican Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 193Richard H. Granger The NOMAD Systemfiring (PROPEL) (as in other script-based under-standers; see Cullingford 1978).
Those expecta-tions say (among other things) that the actor andrecipient of the PROPEL will be the same as theactor and direction of the ATTEND, and that theOBJECT that got PROPELed will be some kind ofprojectile, which is not further specified here.NOMAD OUTPUT:We sighted an enemy ship.
We fired at the ship.3.
INPUT:LOCKED ON OPENED FIRE.Problem: Missing sentence boundaries.
NOMADhas no expectations for a new verb ("opened") toappear immediately after the completed clause"locked on".
It tries but fails to connect "opened"to the phrase "locked on".Solution: Assume the syntactic expectationsfailed because a clause boundary was not adequate-ly marked in the message; assume such a boundaryis there.
NOMAD assumes that there may havebeen an intended sentence separation or clausebreak before "opened",  since no expectations canaccount for the word in this sentence position.Hence, NOMAD saves "locked on" as one clause,and continues to process the rest of the text as anew sentence.NOMAD OUTPUT:We aimed at an unknown object.
We fired at theobject.4.
INPUT:RETURNED BOMBS TO ENEMY SHIP.Problem: Multiple word senses of 'returned',resulting in ambiguous interpretation of action.NOMAD cannot tell whether the action here is"returning" fire to the enemy, that is, firing back atthem (after they presumably had fired at us), orpeaceably delivery bombs, with no firing implied.Solution: Use expectations of probable goals ofactors.
NOMAD first interprets the sentence as"peaceably delivering" some bombs to the ship.However, NOMAD contains the knowledge thatenemies do not transfer control of weapons, infor-mation, personnel, etc., to each other.
Hence itattempts to find an alternative interpretation of thesentence, in this case finding the "returned fire"interpretation, which does not violate any ofNOMAD's knowledge about goals.
It then infers, asin the above example, that the enemy ship musthave previously fired on us.NOMAD OUTPUT:An unknown enemy ship fired on us.
Then wefired bombs at them.5.
INPUT:OPEN FIRED.Problem: Lack of tense agreement between'open' and 'fired'.Solution: Use morphological analyzer to correcttense of word.
NOMAD identifies the phrase 'openfire', and assumes that past tense was intended (bydefault); and so constructs a phrase that correctlyincorporates the tense into the phrase, to make it'opened fire'.
NOMAD then adds the inferred miss-ing actor.
(Note that were this not a known phraseto NOMAD then the tense agreement would nothave been corrected at the surface level, but ratherthe semantic ontent of the two words would havecontributed to a meaning representation, whichwould hae been used to generate a 'corrected' ver-sion of the input.NOMAD OUTPUT:We fired bombs at an unspecified target.4.2.
Recognizing and correcting interpretationerrorsThe four interpretation error-types given above were:1. causal violations,2.
goal violations,3.
user confirmation failure, and4.
out-of-sequence event or object reference.The process of detecting or correcting these errortypes is different in principle from the five surfacetypes, for the simple reason that, as opposed to sur-face errors, which can only be attributed to themessage-sender himself, there are many possible dif-ferent sources of interpretation errors.
In particular,some surface errors can give rise to apparent interpre-tation errors.
To see this, recall the 'returned bombsto kashin' example above.
In this case, NOMAD'sdefault selection of a word sense for an ambiguousword ( 'returned') can give rise to an apparent goalviolation error (delivery weapons to an enemy, as op-posed to firing at an enemy).
Hence, the task ofblame assignment here is problematic: an earlysurface-processing decision of NOMAD's can give riseto an apparent later interpretation problem.Similarly, a 'user confirmation' error (that is, theuser will not confirm any of the interpretations offeredby NOMAD) might be due to any of a number ofthings: the user mistyped the original message,NOMAD made an erroneous surface-text decision, orNOMAD failed to detect a surface or interpretationproblem in the text.
And, a 'causal violation' error(that is, 'ship sighted overhead': ships can't fly, so theerror is apparently a user error) can be due either touser errors or to NOMAD's own interpretation errors.Finally, an object or event apparently referenced outof sequence can be due to either user error or an erro-neous inference by NOMAD.194 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983Richard H. Granger The NOMAD System5.
Summary  and Conclusions5.1.
NOMAD's  l imitat ions and shortcomingsNOMAD has proved to be a capable analyzer of ill-formed text.
Some of the standard problems of script-and plan-based understanders have been satisfactorilyaddressed in NOMAD, most notably, the handling ofunknown words (via the FOUL-UP mechanism); andthe script-selection problem, that is, knowing whichscripts to apply monitoring when they go wrong (viathe mechanisms of supplanting incorrect inferences(Granger 1980), and producing a set of alternate in-terpretations of a text (Granger 1981a, 198282a,1982c).The most important drawback of NOMAD is its lackof extensibility.
Since the system's knowledge is main-ly embedded in word-level routines, adding a newword to the system requires writing a new routine,possibly duplicating information elsewhere in NOMAD,and possibly introducing new errors into otherwise-working NOMAD code.
Any new word routine shouldideally take into account interactions with all theword-level routines already present in the system;some of those routines may have to be modified inlight of the new entry.In practice, we do not check every routine when anew word is added.
Rather, we test the system andmake corrections only when a bad interaction is found.Thus, the system is not guaranteed to be self-consistent.
Since NOMAD has more than a thousand-word vocabulary, it is impractical to check the entiresystem when a new word is added.Encoding grammatical knowledge at the word levelis also cumbersome.
For example, the routine fornearly every verb makes its own checks for active orpassive usage.
A more centralized grammatical mech-anism would eliminate this kind of redundancy.
Inprinciple, the knowledge currently encoded in theword-level routines could be made declarative (that is,stored as data), so as to be more centralized and usa-ble by other parts of the system.5.2.
VOX: A VOcabulary eXtension systemTo make the NOMAD system more extensible, we arecurrently building a new system that uses not word-level but phrasal analysis.
We call this new systemVOX (for vocabulary extension system).
Our goal isto make this system extensible by interaction with auser, rather than by adding to the data base program-matically.Our ideas about phrasal analysis originate from thework on the PHRAN system (Wilensky and Arens1982).
Phrasal analysis consists of matching the inputto one or more phrase-level patterns stored in aknowledge data base.
When the input has beenmatched, it is said to be understood.
Semantic actionscan be associated with each phrase, so that whenever aphrase is matched to part of the input a correspondingmeaning representat ion for the phrase may be con-structed.To extend the knowledge base of the system, wesimply add new patterns to the data base.
Ideally,patterns are independent entities whose interactionintroduces no side effects, so that new phrases can beeasily added to or removed from the data base.
Aworking prototype of VOX is already up and running(see Granger, Meyers, Yoshii, and Taylor 1983 andMeyers 1983), incorporating syntactic and grammati-cal analyses, semantic analyses and blame assignment,morphological analysis, and error detection and cate-gorization.
VOX's phrase knowledge base alreadyconsists of hundreds of phrases, and is being exten-sively tested.
Furthermore, VOX's data base can beinteractively 'edited' by a trained 'tutor '  to add newinformation, including new vocabulary, new syntacticcategories and constructions, and new meanings.Hence, we hope that VOX may be a first step towardsa ' trainable'  language-processing system.
Granger,Meyers, Yoshii, and Taylor (1983) and Meyers (1983)present extensive descriptions of the state of VOX andthe theories underlying it.5.3.
Summary:  Surface text  and itsinterpretat ionsThe ability to understand text is dependent on theability to understand what is being described in thetext.
Hence, a reader of English must have applicableknowledge of both the situations that may be de-scribed in texts (for example, actions, states, se-quences of events, goals, methods of achieving goals,etc.
), and the surface structures that appear in thelanguage, that is, the relations between the surfaceorder of words and phrases, and their correspondingmeaning structures.
The process of text understandingis the combined application of these knowledgesources as a reader proceeds through a text.
This factbecomes clearest when we investigate the understand-ing of i l l-formed texts, texts that present particularproblems to a reader.
The line between correct andincorrect English is often unclear, so a system thatcannot handle erroneous input is of limited use.Human understanding is inherently tolerant; peopleare naturally able to ignore and deal with many typesof errors, omissions, poor constructions, etc., and getstraight to the meaning of the text.
Our theories havetried to take this ability into account by includingknowledge and mechanisms of error noticing and cor-recting as implicit parts of our process models of lan-guage understanding.
NOMAD and VOX are primarilyengineering applications incorporating a series of theo-retical results in language understanding, includingscript-based and goal-based understanding, and inte-grated error-monitoring and supplanting during under-standing.
The NOMAD and VOX systems are the lat-American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983 195Richard H. Granger The NOMAD Systemest in a line of 'tolerant' language understanders, be-ginning with FOUL-UP, all based on the use of knowl-edge of syntax, semantics, and pragmatics at all stagesof the understanding process to cope with errors.AcknowledgmentsRika Yoshii, Chris Staros, Greg Taylor, and AmnonMeyers all contributed to the construction of theNOMAD system; Amnon Meyers has been responsiblefor the construction of VOX.ReferencesBirnbaum, L. and Selfridge, M. 1980 Conceptual Analysis ofNatural Language.
In Schank, R. and Riesbeck, C., Eds., InsideComputer Understanding.
Lawrence Erlbaum Associates, Hills-dale, New Jersey.Cullingford, R. 1977 Controlling Inferences in Story Understand-ing.
Proceedings of the Fifth International Joint Conference onArtificial Intelligence (1JCA1).
Cambridge, Massachusetts.DeJong, G. 1979 Skimming Stories in Real Time: An Experimentin Integrated Understanding.
Ph.D. Thesis.
Computer ScienceDepartment, Yale University, New Haven, Connecticut.Goldman, N. 1973 The Generation of English Sentences from aDeep Conceptual Base.
Ph.D. Thesis.
Stanford University,Stanford, California.Granger, R.H. 1977 FOUL-UP: A Program that Figures OutMeanings of Words from Context.
Proceedings of the FifthInternational Joint Conference on Artificial Intelligence (IJCAI).Cambridge, Massachusetts.Granger, R.H. 1980 When Expectation Fails: Toward a Self-Correcting Inference System.
Proceedings of the First NationalConference on Artificial Intelligence.
Stanford University, Stan-ford, California.Granger, R.H. 1981a Directing and Re-directing Inference Pur-suit: Extra-textual Influences on Text Interpretation.
Proceed-ings of the Seventh International Joint Conference on ArtificialIntelligence (IJCAI).
Vancouver, British Columbia.Granger, R.H. 1981b Shaping Explanations: Effects of Question-ing on Text Interpretation.
Proceedings of the Third AnnualConference of the Cognitive Science Society.
Berkeley, California:193-196.Granger, R.H. 1982a Judgmental Inference: Inferential Decision-Making during Understanding.
Technical Report #182.
Com-puter Science Department, University of California, Irvine,California.Granger, R.H. 1982b Scruffy Text Understanding: Design andImplementation of 'Tolerant' Understanders.
Proceedings of the20th Annual Meeting of the Association for ComputationalLinguistics.
Toronto, Ontario, Canada: 157-160.Granger, R.H. 1982c Inference Decisions in Text Understanding.Proceedings of the Fourth Annual Conference of the CognitiveScience Society.
Ann Arbor, Michigan.Granger, R.H.; Meyers, A.; Yoshii, R.; and Taylor, G. 1983 AnExtensible Natural Language Understanding System.
Proceed-ings of the Artificial Intelligence Conference.
Oakland University,Rochester, Michigan.Hayes, P.J.
and Mouradian, G.V.
1981 Flexible Parsing.
Ameri-can Journal of Computation Linguistics 7(4): 232-242.Kwasny, S.C. and Sondheimer, N.K.
1981 Relaxation Techniquesfor Parsing Grammatically Ill-Formed Input in Natural Lan-guage Understanding Systems.
American Journal of ComputationLinguistics 7(2): 99-108.Lebowitz, M. 1981 Generalization and Memory in an IntegratedUnderstanding System.
Computer Science Research Report186.
Yale University, New Haven, Connecticut.Meyers, A.
1983 Conceptual Grammar.
Computer Science Tech-nical Report #215.
University of California, Irvine, California.McGuire, R. 1980 Political Primaries and Words of Pain.
Unpubl-ished manuscript.
Department of Computer Science, YaleUniversity, New Haven, Connecticut.Riesbeck, C. and Schank, R. 1976 Comprehension by Computer:Expectation-based Analysis of Sentences in Context.
ComputerScience Research Report 78.
Yale University, New Haven,Connecticut.Schank, R.C.
and Abelson, R. 1977 Scripts, Plans, Goals, andUnderstanding.
Lawrence Erlbaum Associates, Hillsdale, NewJersey.Small, S. 1980 Word Expert Parsing: A Theory of DistributedWord-Based Natural Language Understanding.
Technical Re-port TR-954.
University of Maryland, College Park, Maryland.Wilensky, R. 1978 Understanding Goal-Based Stories.
ComputerScience Technical Report 140.
Yale University, New Haven,Connecticut.Wilensky, R. and Arens, Y.
1982 PHRAN: A Phrasal Analyzer.EECS Technical Report.
University of California, Berkeley,California.APPENDIX:  Some Statistics on NOMAD'sOperation1.
Timing: NOMAD uses about 3 cpu seconds perword when analyzing Navy messages.2.
Vocabulary size and structure: NOMAD is based onCA (Birnbaum and Selfridge 1979), and incorpo-rates 'word-expert '  routines (Small 1980).
Eachword-expert routine can process a whole class ofwords, not just an individual word.
There are 152word-expert routines; there are 440 words, inflect-ed forms, and phrases in NOMAD's dictionary.
(There are 330 words and phrases, not countinginflections.)3.
Knowledge: There are 16 situation frames, corre-sponding roughly to: battle, communication,location-change, sight, attack, report, command,communicate, manate, detect, project, aim, ptrans,patrol, state-change, causal-result.4.
Benchmarks: NOMAD has successfully processedabout 4000 Navy messages of lengths varying from1 line to 17 lines of text each.
No statistics havebeen compiled on NOMAD's overall success versusfailure rate on all Navy texts.196 American Journal of Computational Linguistics, Volume 9, Numbers 3-4, July-December 1983
