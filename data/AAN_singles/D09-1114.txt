Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1096?1104,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPThe Feature Subspace Method for SMT System CombinationNan Duan1, Mu Li2, Tong Xiao3, Ming Zhou21Tianjin University       2Microsoft Research Asia       3Northeastern UniversityTianjin, China                    Beijing, China                     Shenyang, China{v-naduan,muli,v-toxiao,mingzhou}@microsoft.comAbstractRecently system combination has been shownto be an effective way to improve translationquality over single machine translation sys-tems.
In this paper, we present a simple and ef-fective method to systematically derive an en-semble of SMT systems from one baseline li-near SMT model for use in system combina-tion.
Each system in the resulting ensemble isbased on a feature set derived from the fea-tures of the baseline model (typically a subsetof it).
We will discuss the principles to deter-mine the feature sets for derived systems, andpresent in detail the system combination mod-el used in our work.
Evaluation is performedon the data sets for NIST 2004 and NIST 2005Chinese-to-English machine translation tasks.Experimental results show that our method canbring significant improvements to baselinesystems with state-of-the-art performance.1 IntroductionResearch on Statistical Machine Translation(SMT) has shown substantial progress in recentyears.
Since the success of phrase-based methods(Och and Ney, 2004; Koehn, 2004), modelsbased on formal syntax (Chiang, 2005) or lin-guistic syntax (Liu et al, 2006; Marcu et al,2006) have also achieved state-of-the-art perfor-mance.
As a result of the increasing numbers ofavailable machine translation systems, studies onsystem combination have been drawing more andmore attention in SMT research.There have been many successful attempts tocombine outputs from multiple machine transla-tion systems to further improve translation quali-ty.
A system combination model usually takes n-best translations of single systems as input, anddepending on the combination strategy, differentmethods can be used.
Sentence-level combina-tion methods directly select hypotheses fromoriginal outputs of single SMT systems (Sim etal., 2007; Hildebrand and Vogel, 2008), whilephrase-level or word?level combination methodsare more complicated and could produce newtranslations different from any translations in theinput (Bangalore et al, 2001; Jayaraman and La-vie, 2005; Matusov et al, 2006; Sim et al,2007).Among all the factors contributing to the suc-cess of system combination, there is no doubtthat the availability of multiple machine transla-tion systems is an indispensable premise.
Al-though various approaches to SMT system com-bination have been explored, including enhancedcombination model structure (Rosti et al, 2007),better word alignment between translations(Ayan et al, 2008; He et al, 2008) and improvedconfusion network construction (Rosti et al,2008), most previous work simply used the en-semble of SMT systems based on different mod-els and paradigms at hand and did not tackle theissue of how to obtain the ensemble in a prin-cipled way.
To our knowledge the only workdiscussed this problem is Macherey and Och(2007), in which they experimented with build-ing different SMT systems by varying one ormore sub-models (i.e.
translation model or dis-tortion model) of an existing SMT system, andobserved that changes in early-stage model train-ing introduced most diversities in translationoutputs.In this paper, we address the problem of build-ing an ensemble of diversified machine transla-tion systems from a single translation engine forsystem combination.
In particular, we propose anovel Feature Subspace method for the ensembleconstruction based on any baseline SMT modelwhich can be formulated as a standard linearfunction.
Each system within the ensemble isbased on a group of features directly derivedfrom the baseline model with minimal efforts(which is typically a subset of the features usedin the baseline model), and the resulting systemis optimized in the derived feature space accor-dingly.We evaluated our method on the test sets forNIST 2004 and NIST 2005 Chinese-to-English1096machine translation tasks using two baselineSMT systems with state-of-the-art performance.Experimental results show that the feature sub-space method can bring significant improve-ments to both baseline systems.The rest of the paper is organized as follows.The motivation of our work is described on Sec-tion 2.
In Section 3, we first give a detailed de-scription about feature subspace method, includ-ing the principle to select subspaces from allpossible options, and then an n-gram consensus ?based sentence-level system combination methodis presented.
Experimental results are given inSection 4.
Section 5 discusses some related is-sues and concludes the paper.2 MotivationOur motivations for this work can be describedin the following two aspects.The first aspect is related to the cost of build-ing single systems for system combination.
Inprevious work, the SMT systems used in combi-nation differ mostly in two ways.
One is the un-derlying models adopted by individual systems.For example, using an ensemble of systems re-spectively based on phrase-based models, hierar-chical models or even syntax-based models is acommon practice.
The other is the methods usedfor feature function estimation such as using dif-ferent word alignment models, language modelsor distortion models.
For the first solution, build-ing a new SMT system with different methodol-ogy is by no means an easy task even for an ex-perienced SMT researcher, because it requiresnot only considerable effects to develop but alsoplenty of time to accumulate enough experiencesto fine tune the system.
For the second alterna-tive, usually it requires time-consuming re-training for word alignment or language models.Also some of the feature tweaking in this solu-tion is system or language specific, thus for anynew systems or language pairs, human engineer-ing has to be involved.
For example, using dif-ferent word segmentation methods for Chinesecan generate different word alignment results,and based on which a new SMT system can bebuilt.
Although this may be useful to combina-tion of Chinese-to-English translation, it is notapplicable to most of other language pairs.Therefore it will be very helpful if there is alight-weight method that enables the SMT sys-tem ensemble to be systematically constructedbased on an existing SMT system.Sourcesentence??
??
??
?
??
????
??
?
?ReftranslationChina's largest sea water desalini-zation project settles in ZhoushanDefaulttranslationChina 's largest desalinationproject in Zhoushan?????
?translationChina 's largest sea waterdesalination project in ZhoushanTable 1: An example of translations generatedfrom the same decoder but with different featuresettings.Chinese English ?
?
?1 ??
??
desalination 0.40002 ??
sea water 0.17483 ??
desalination 0.0923Table 2: Parameters of related phrases for exam-ples in Table 1.The second aspect motivating our work comesfrom the subspace learning method in machinelearning literature (Ho, 1998), in which an en-semble of classifiers are trained on subspaces ofthe full feature space, and final classification re-sults are based on the vote of all classifiers in theensemble.
Lopez and Resnik (2006) also showedthat feature engineering could be used to over-come deficiencies of poor alignment.
To illu-strate the usefulness of feature subspace in theSMT task, we start with the example shown inTable 1.
In the example, the Chinese source sen-tence is translated with two settings of a hierar-chical phrase-based system (Chiang, 2005).
Inthe default setting all the features are used asusual in the decoder, and we find that the transla-tion of the Chinese word ??
(sea water) ismissing in the output.
This can be explained withthe data shown in Table 2.
Because of noises andword alignment errors in the parallel trainingdata, the inaccurate translation phrase??
??
?
????????????
is assigned with ahigh value of the phrase translation probabilityfeature ?(?|?).
Although the correct translationcan also be composed by two phrases ??
????
?????
and ??
?
???????????
?, its over-all translation score cannot beat the incorrect onebecause the combined phrase translation proba-bility of these two phrases are much smallerthan  ?(????????????|??
??)
.
However, ifwe intentionally remove the ?(?|?)
feature fromthe model, the preferred translation can be gener-ated as shown in the result of ??????
because in1097this way the bad estimation of ?(?|?)
for thisphrase is avoided.This example gives us the hint that buildingdecoders based on subspaces of a standard modelcould help with working around some negativeimpacts of inaccurate estimations of feature val-ues for some input sentences.
The subspace-based systems are expected to work similarly tostatistical classifiers trained on subspaces of afull feature space ?
though the overall accuracyof baseline system might be better than any indi-vidual systems, for a specific sentence some in-dividual systems could generate better transla-tions.
It is expected that employing an ensembleof subspace-based systems and making use ofconsensus between them will outperform thebaseline system.3 Feature Subspace Method for SMTSystem Ensemble ConstructionIn this section, we will present in detail the me-thod for systematically deriving SMT systemsfrom a standard linear SMT model based on fea-ture subspaces for system combination.3.1 SMT System Ensemble GenerationNowadays most of the state-of-the-art SMT sys-tems are based on linear models as proposed inOch and Ney (2002).
Let ??
(?, ?)
be a featurefunction, and ??
be its weight, an SMT model ?can be formally written as:??
= argmax?????
(?, ?)?
(1)Noticing that Equation (1) is a general formu-lation independent of any specific features, tech-nically for any subset of features used in ?
, anew SMT system can be constructed based on it,which we call a sub-system.Next we will use ?
to denote the full featurespace defined by the entire set of features usedin ?, and ?
?
?
is a feature subset that belongsto ?(?
), the power set of ?.
The derived sub-system based on subset ?
?
?
is denoted by ??
.Although in theory we can use all the sub-systems derived from every feature subsetin ?(?
), it is still desirable to use only some ofthem in practice.
The reasons for this are two-fold.
First, the number of possible sub-systems(2 ? )
is exponential to the size of ?.
Even whenthe number of features in ?
is relatively small,i.e.
10, there will be up to 1024 sub-systems intotal, which is a large number for combinationtask.
Larger feature sets will make the systemcombination practically infeasible.
Second, notevery sub-system could contribute to the systemcombination.
For example, feature subsets onlycontaining very small number of features willlead to sub-systems with very poor performance;and the language model feature is too importantto be ignored for a sub-system to achieve reason-ably good performance.In our work, we only consider feature sub-spaces with only one difference from the featuresin ?.
For each non- language model feature ??
, asub-system ??
is built by removing ??
from  ?
.Allowing for the importance of the languagemodel (LM) feature to an SMT model, we do notremove any LM feature from any sub-system.Instead, we try to weaken the strength of a LMfeature by lowering its n-gram order.
For exam-ple, if a 4-gram language model is used in thebaseline system ?, then a trigram model can beused in one sub-system, and a bigram model canbe used in another.
In this way more than onesub-system can be derived based on one LM fea-ture.
When varying a language model feature, theone-feature difference principle is still kept: ifwe lower the order of a language model feature,no other features are removed or changed.The remaining issue of using weakened LMfeatures is that the resulting ensemble is no long-er strictly based on subspace of ?.
However, thistheoretical imperfection can be remedied by in-troducing ??
, a super-space of ?
to include alllower-order LM features.
In this way, an aug-mented baseline system ??
can be built basedon  ??
, and the baseline system ?
itself can alsobe viewed as a sub-system of ??.
We will showin the experimental section that ??
actually per-forms even slightly better than the original base-line system ?, but results of sub-system combi-nation are significantly better that both ?
and ??
.After the sub-system ensemble is constructed,each sub-system tunes its feature weights inde-pendently to optimize the evaluation metrics onthe development set.Let ?
= {?1 ,?
,??}
be the set of sub-systemsobtained by either removing one non-LM featureor changing the order of a LM feature, and ??
bethe n-best list produced by ??
.
Then ?(?
), thetranslation candidate pool to the system combi-nation model can be written as:?(?)
= ???
(2)The advantage of this method is that it allowsus to systematically build an ensemble of SMTsystems at a very low cost.
From the decoding1098perspective, all the sub-systems share a commondecoder, with minimal extensions to the baselinesystems to support the use of specified subset offeature functions to compute the overall score fortranslation hypotheses.
From the model trainingperspective, all the non-LM feature functions canbe estimated once for all sub-systems.
The onlyexception is the language model feature, whichmay be of different values across multiple sub-systems.
However, since lower-order modelshave already been contained in higher-ordermodel for the purpose of smoothing in almost allstatistical language model implementations, thereis also no extra training cost.3.2 System Combination SchemeIn our work, we use a sentence-level systemcombination model to select best translation hy-pothesis from the candidate pool  ?(?)
.
Thismethod can also be viewed to be a hypotheses re-ranking model since we only use the existingtranslations instead of performing decoding overa confusion network as done in the word-levelcombination method (Rosti et al, 2007).The score function in our combination modelis formulated as follows:??
= ?????????
???????
?
+ ???
+ ?(?,?(?
))(3)where ???
?
is the language model score for ?,?
is the length of ?, and ?(?,?(?))
is a transla-tion consensus ?based scoring function.
Thecomputation of ?(?,?(?))
is further decom-posed into weighted linear combination of a setof n-gram consensus ?based features, which aredefined in terms of the order of n-gram to bematched between current candidate and othertranslation in ?(?
).Given a translation candidate  ?
, the n-gramagreement feature between ?
and other transla-tions in the candidate pool is defined as:??+(?,?
? )
=  ??
?, ???
?
??
?
,?
???
(4)where the function  ??
?, ??
counts the occur-rences of n-grams of ?
in ?
?
:??
?, ??
= ?(???+?
?1, ?
?)?
?
?+1?=1(5)Here ?(?,?)
is the indicator function -?
???+?
?1 , ?
?
is 1 when the n-gram ???+?
?1  ap-pears in ?
?
, otherwise it is 0.In order to give the combination model an op-portunity to penalize long but inaccurate transla-tions, we also introduce a set of n-gram disa-greement features in the combination model:???(?,?
? )
=  ( ?
?
?
+ 1?
??
(?, ??))?
?
??
?
,?
???
(6)Because each order of n-gram match introduc-es two features, the total number of features inthe combination model will be 2?
+ 2 if ?
or-ders of n-gram are to be matched in computing?(?,?(?)).
Since we also adopt a linear scor-ing function in Equation (3), the feature weightsof our combination model can also be tuned on adevelopment data set to optimize the specifiedevaluation metrics using the standard MinimumError Rate Training (MERT) algorithm (Och2003).Our method is similar to the work proposed byHildebrand and Vogel (2008).
However, exceptthe language model and translation length, weonly use intra-hypothesis n-gram agreement fea-tures as Hildebrand and Vogel did and use addi-tional intra-hypothesis n-gram disagreement fea-tures as Li et al (2009) did in their co-decodingmethod.4 Experiments4.1 DataExperiments were conducted on the NIST evalu-ation sets of 2004 (MT04) and 2005 (MT05) forChinese-to-English translation tasks.
Both corpo-ra provide 4 reference translations per sourcesentence.
Parameters were tuned with MERTalgorithm (Och, 2003) on the NIST evaluationset of 2003 (MT03) for both the baseline systemsand the system combination model.
Translationperformance was measured in terms of case-insensitive NIST version of BLEU score whichcomputes the brevity penalty using the shortestreference translation for each segment, and allthe results will be reported in percentage num-bers.
Statistical significance is computed usingthe bootstrap re-sampling method proposed byKoehn (2004).
Statistics of the data sets aresummarized in Table 3.Data set #Sentences #WordsMT03 (dev) 919 23,782MT04 (test) 1,788 47,762MT05 (test) 1,082 29,258Table 3: Data set statistics.1099We use the parallel data available for theNIST 2008 constrained track of Chinese-to-English machine translation task as bilingualtraining data, which contains 5.1M sentencepairs, 128M Chinese words and 147M Englishwords after pre-processing.
GIZA++ toolkit (Ochand Ney, 2003) is used to perform word align-ment in both directions with default settings, andthe intersect-diag-grow method is used to gener-ate symmetric word alignment refinement.
Thelanguage model used for all systems is a 5-grammodel trained with the English part of bilingualdata and Xinhua portion of LDC English Giga-word corpus version 3.
In experiments, multiplelanguage model features with the order rangingfrom 2 to 5 can be easily obtained from the 5-gram one without retraining.4.2 System DescriptionTheoretically our method is applicable to all li-near model ?based SMT systems.
In our experi-ments, two in-house developed systems are usedto validate our method.
The first one (SYS1) is asystem based on the hierarchical phrase-basedmodel as proposed in (Chiang, 2005).
Phrasalrules are extracted from all bilingual sentencepairs, while hierarchical rules with variables areextracted from selected data sets includingLDC2003E14, LDC2003E07, LDC2005T06 andLDC2005T10, which contain around 350,000sentence pairs, 8.8M Chinese words and 10.3MEnglish words.
The second one (SYS2) is a re-implementation of a phrase-based decoder withlexicalized reordering model based on maximumentropy principle proposed by Xiong et al(2006).
All bilingual data are used to extractphrases up to length 3 on the source side.In following experiments, we only considerremoving common features shared by both base-line systems for feature subspace generation.Rule penalty feature and lexicalized reorderingfeature, which are particular to SYS1 and SYS2,are not used.
We list the features in considerationas follows:?
PEF and PFE: phrase translation probabili-ties ?
?
?
and ?
?
??
PEFLEX and PFELEX: lexical weights????
?
?
and ????
?
??
PP: phrase penalty?
WP: word penalty?
BLP: bi-lexicon pair counting how manyentries of a conventional lexicon co-occurring in a given translation pair?
LM-n: language model with order nBased on the principle described in Section3.1, we generate a number of feature subspacesfor each baseline system as follows:?
For non-LM features (PEF, PFE, PEFLEX,PFELEX, PP, WP and BLP), we remove oneof them from the full feature space eachtime.
Thus 7 feature subspaces are generated,which are denoted as  ??????
, ??????
,?????????
, ?????????
, ?????
, ?????
and??????
respectively.
The 5-gram LM featureis used in each of them.?
For LM features (LM-n), we change the or-der from 2 to 5 with all the other non-LMfeatures present.
Thus 4 LM-related featuresubspaces are generated, which are denotedas ????
?2, ????
?3 , ????
?4  and ????
?5 re-spectively.
????
?5 is essentially the full fea-ture space of  baseline system.For each baseline system, we construct a totalof 11 sub-systems by using above feature sub-spaces.
The baseline system is also containedwithin them because of using ?????5.
We callall sub-systems are non-baseline sub-systemsexcept the one derived by using ????
?5.By default, the beam size of 60 is used for allsystems in our experiments.
The size of n-bestlist is set to 20 for each sub-system, and for base-line systems, this size is set to 220, which equalsto the size of the combined n-best list generatedby total 11 sub-systems.
The order of n-gramagreement and disagreement features used insentence-level combination model ranges fromunigram to 4-gram.4.3 Evaluation of Oracle TranslationsWe first evaluate the oracle performance on then-best lists of baseline systems and on the com-bined n-best lists of sub-systems generated fromeach baseline system.The oracle translations are obtained by usingthe metric of sentence-level BLEU score (Ye etal., 2007).
Table 4 shows the evaluation results,in which Baseline stands for baseline systemwith a 5-gram LM feature, and FS stands for 11sub-systems derived from the baseline system.SYS1 SYS2BLEU/TER BLEU/TERMT04Baseline  49.68/0.6411 49.50/0.6349FS 51.05/0.6089 50.53/0.6056MT05Baseline 48.89/0.5946 48.37/0.5944FS 50.69/0.5695 49.81/0.5684Table 4: Oracle BLEU and TER scores on base-line systems and their generated sub-systems.1100For both SYS1 and SYS2, feature subspacemethod achieves higher oracle BLEU and lowerTER scores on both MT04 and MT05 test sets,which gives the feature subspace method morepotential to achieve higher performance than thebaseline systems.We then investigate the ratio of translationcandidates in the combined n-best lists of non-baseline sub-systems that are not included in thebaseline?s n-best list.
Table 5 shows the statistics.MT04 MT05SYS1 69.71% 69.69%SYS2 59.07% 58.54%Table 5: Ratio of unique translation candidatesfrom non-baseline sub-systems.From Table 5 we can see that only less thanhalf of the translation candidates of sub-systemsoverlap with those the of baseline systems.
Thisresult, together with the oracle BLEU and TERscore estimation, helps eliminate the concern thatno diversities or better translation candidates canbe obtained by using sub-systems.4.4 Feature Subspace Method on SingleSMT SystemNext we validate the effect of feature subspacemethod on single SMT systems.Figure 1 shows the evaluation results of dif-ferent systems on the MT05 test set.
From thefigure we can see that the overall accuracy ofbaseline systems is better than any of their de-rived sub-systems, and except the sub-systemderived by using ????
?2, the performance of allthe systems are fairly similar.Figure 1: Performances of different systems.We then evaluate the system combination me-thod proposed in Section 3.2 with all the sub-systems for each baseline system.
Table 6 showsthe results on both MT04 and MT05 data sets, inwhich FS-Comb denotes the system combinationusing 11 sub-systems.From Table 6 we can see that by using FS-Comb we obtain about 1.1~1.3 points of BLEUgains over baseline systems.
We also include inTable 6 the results for Baseline+mLM, whichstands for the augmented baseline system as de-scribed in Section 3.1 using a bunch of LM fea-tures from bigram to 5-gram.
It can be seen thatboth augmented baseline systems outperformtheir corresponding baseline systems slightly butconsistently on both data sets.MT04 MT05SYS1Baseline 39.07 38.72Baseline+mLM 39.34+ 39.14+FS-Comb 40.43++ 39.79++SYS2Baseline 38.84 38.30Baseline+mLM 38.95* 38.63+FS-Comb 39.92++ 39.49++Table 6: Translation results of Baseline, Base-line+mLM and FS-Comb (+: significant betterthan baseline system with ?
< 0.05; ++: signifi-cant better than baseline system with ?
< 0.01; *:no significant improvement).We also investigate the results when we in-crementally add the n-best list of each sub-system into a candidate pool to see the effectswhen different numbers of sub-systems are usedin combination.
In order to decide the sequenceof sub-systems to add, we first evaluate the per-formance of pair-wise combinations betweeneach sub-system and its baseline system on thedevelopment set.
That is, for each sub-system,we combine its n-best list with the n-best list ofits baseline system and perform system combina-tion for MT03 data set.
Then we rank the sub-systems by the pair-wise combination perfor-mance from high to low, and use this ranking asthe sequence to add n-best lists of sub-systems.Each time when a new n-best list is added, thecombination performance based on the enlargedcandidate pool is evaluated.
Figure 2 shows theresults on both MT04 and MT05 test sets, inwhich SYS1-fs and SYS2-fs denote the sub-systems derived from SYS1 and SYS2 respec-tively, and X-axis is the number of sub-systemsused for combination each time and Y-axis is theBLEU score.
From the figure we can see thatalthough in some cases the performance slightlydrops when a new sub-system is added, generallyusing more sub-systems always leads to betterresults.313233343536373839SYS1 SYS2BaselineFS-PEFFS-PFEFS-PEFLEXFS-PFELEXFS-PPFS-WPFS-BLPFS-LM-2FS-LM-3FS-LM-41101Next we examine the performance of baselinesystems when different beam sizes are used indecoding.
The results on MT05 test set areshown in Figure3, where X-axis is the beam size.In Figure 3, SYS1+mLM and SYS2+mLM de-note augmented baseline systems of SYS1 andSYS2 with multiple LM features.From Figure 3 we can see that augmentedbaseline systems (with multiple LM features)outperform the baseline systems (with only oneLM feature) for all beam sizes ranging from 20to 220.
In this experiment we did not observe anysignificant performance improvements when us-ing larger beam sizes than the default setting, butusing more sub-systems in combination almostalways bring improvements.Figure 2: Performances on different numbers ofsub-systems.Figure 3: Performances on different beam sizes.MT04 MT05SYS1-fs 44.63% 46.12%SYS2-fs 47.54% 44.73%Table 7: Ratio of final translations coming fromnon-baseline sub-systems.Finally, we investigate the ratio of final trans-lations coming from the n-best lists of non-baseline sub-systems only.
Table 7 shows theresults on both MT04 and MT05 test sets, whichindicate that almost half of the final translationsare contributed by the non-baseline sub-systems.4.5 The Impact of n-best List SizeIn order to find the optimal size of n-best list forcombination, we compare the combination re-sults of using list sizes from 10-best up to 500-best for each sub-system.In this experiment, system combination wasperformed on the combined n-best list from total11 sub-systems with different list size each time.Figure 4 shows the results on the MT03 dev setand the MT04 and MT05 test sets for both SYS1and SYS2.
X-axis is the n-best list size of eachsub-system.Figure 4: Performances on different n-best sizes.We can see from the figure that for all datasets the optimal n-best list size is around 50, butthe improvements are not significant over theresults when 20-best translations are used.
Thereason for the small optimal n-best list size couldbe that the low-rank hypotheses might introducemore noises into the combined translation candi-date pool for sentence-level combination (Hasanet al, 2007; Hildebrand and Vogel, 2008).4.6 Feature Subspace Method on MultipleSMT SystemsIn the last experiment, we investigate the effectof feature subspace method when multiple SMTsystems are used in system combination.Evaluation results are reported in Table 8.
Thesystem combination method described in Section3.2 is used to combine outputs from two baselinesystems (with only one 5-gram LM feature) andsub-systems generated from both baseline sys-tems (22 in total), with their results denoted asBaseline Comb (both) and FS Comb (both) re-spectively.
We also include the combination re-sults of sub-systems based on one baseline sys-tem for reference in the table.38.038.539.039.540.040.51 2 3 4 5 6 7 8 9 10 11SYS1-fs-05SYS2-fs-05SYS1-fs-04SYS2-fs-0438.038.539.039.520406080100120140160180200220SYS1SYS2SYS1-mLMSYS2-mLM39.039.540.040.541.041.542.010 20 50 100 200 500SYS1-fs-05SYS2-fs-05SYS1-fs-04SYS2-fs-04SYS1-fs-03SYS2-fs-031102On both MT04 and MT05 test sets, the resultsof system combination based on sub-systems aresignificantly better than those of baseline sys-tems, which show that our method can also helpwith system combination when more than onesystem are used.
We can also see that using mul-tiple systems based on different SMT models andusing our subspace based method can help eachother: the best performance can only be achievedwhen both are employed.MT04 MT05Baseline Comb (both) 39.98 39.43FS-Comb (SYS1) 40.43 39.79FS-Comb (SYS2) 39.92 39.49FS Comb (both) 40.96 40.38Table 8: Performances of sentence-level combi-nation on multiple SMT systems.5 ConclusionIn this paper, we have presented a novel and ef-fective Feature Subspace method for the con-struction of an ensemble of machine translationsystems based on a baseline SMT model whichcan be formulated as a standard linear function.Each system within the ensemble is based on asubset of features derived from the baselinemodel, and the resulting ensemble can be used insystem combination to improve translation quali-ty.
Experimental results on NIST Chinese-to-English translation tasks show that our methodcan bring significant improvements to two base-line systems with state-of-the-art performance,and it is expected that our method can be em-ployed to improve any linear model -based SMTsystems.
There is still much room for improve-ments in the current work.
For example, we stilluse a simple one-feature difference principle forfeature subspace generation.
In the future, wewill explore more possibilities for feature sub-spaces selection and experiment with our methodin a word-level system combination model.ReferencesNecip Fazil Ayan, Jing Zheng, and Wen Wang.
2008.Improving alignments for better confusion net-works for combining machine translation systems.In Proc.
COLING, pages 33-40.Srinivas Bangalore, German Bordel, and GiuseppeRiccardi.
2001.
Computing consensus translationfrom multiple machine translation systems.
InProc.
ASRU, pages 351-354.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Proc.ACL, pages 263-270.Xiaodong He, Mei Yang, Jianfeng Gao, PatrickNguyen, and Robert Moore.
2008.
Indirect-hmm-based hypothesis for combining outputs from ma-chine translation systems.
In Proc.
EMNLP, pages98-107.Almut Silja Hildebrand and Stephan Vogel.
2008.Combination of machine translation systems viahypothesis selection from combined n-best lists.
In8th AMTA conference, pages 254-261.Tin Kam Ho.
1998.
The random subspace method forconstructing decision forests.
In IEEE Transactionson Pattern Analysis and Machine Intelligence,pages 832-844.Sasa Hasan, Richard Zens, and Hermann Ney.
2007.Are very large n-best lists useful for SMT?
InProc.
NAACL, Short paper, pages 57-60.S.
Jayaraman and A. Lavie.
2005.
Multi-Engine Ma-chine Translation Guided by Explicit Word Match-ing.
In 10th EAMT conference, pages 143-152.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
EMNLP,pages 388-395.Philipp Koehn.
2004.
Phrase-based Model for SMT.In Computational Linguistics, 28(1): pages 114-133.Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, andMing Zhou.
2009.
Collaborative Decoding: PartialHypothesis Re-Ranking Using Translation Consen-sus between Decoders.
In Proc.
ACL-IJCNLP.Adam Lopez and Philip Resnik.
2006.
Word-BasedAlignment, Phrase-Based Translation: What?s thelink?
In 7th AMTA conference, pages 90-99.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical MachineTranslation.
In Proc.
ACL, pages 609-616.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target language phras-es.
In Proc.
EMNLP, pages 44-52.Wolfgang Macherey and Franz Och.
2007.
An Empir-ical Study on Computing Consensus Translationsfrom Multiple Machine Translation Systems.
InProc.
EMNLP, pages 986-995.Evgeny Matusov, Nicola Ueffi ng, and Hermann Ney.2006.
Computing consensus translation from mul-tiple machine translation systems using enhancedhypotheses alignment.
In Proc.
EACL, pages 33-40.Franz Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statis-1103tical machine translation.
In Proc.
ACL, pages 295-302.Franz Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proc.
ACL, pages160-167.Franz Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1): pages 19-51.Franz Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine transla-tion.
Computational Linguistics, 30(4): pages 417-449.Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,Spyros Matsoukas, Richard Schwartz, and BonnieDorr.
2007.
Combining outputs from multiple ma-chine translation systems.
In Proc.
NAACL, pages228-235.Antti-Veikko Rosti, Spyros Matsoukas, and RichardSchwartz.
2007.
Improved Word-Level SystemCombination for Machine Translation.
In Proc.ACL, pages 312-319.Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2008.
Incremental hypothe-sis alignment for building confusion networks withapplication to machine translation system combina-tion.
In Proc.
Of the Third ACL Workshop on Sta-tistical Machine Translation, pages 183-186.K.C.
Sim, W. Byrne, M. Gales, H. Sahbi, and P.Woodland.
2007.
Consensus network decoding forstatistical machine translation system combination.In ICASSP, pages 105-108.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Max-imum entropy based phrase reordering model forstatistical machine translation.
In Proc.
ACL, pages521-528.Yang Ye, Ming Zhou, and Chin-Yew Lin.
2007.
Sen-tence level Machine Translation Evaluation as aRanking Problem: One step aside from BLEU.
InProc.
Of the Second ACL Workshop on StatisticalMachine Translation, pages 240-247.1104
