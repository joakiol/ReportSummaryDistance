Coling 2010: Poster Volume, pages 1498?1506,Beijing, August 2010Automatic Temporal Expression Normalization with ReferenceTime Dynamic-ChoosingXujian Zhao, Peiquan Jin, and Lihua YueSchool of Computer Science and TechnologyUniversity of Science and Technology of Chinanonozxj@mail.ustc.edu.cn, {jpq,llyue}@ustc.edu.cnAbstractTemporal expressions in texts containsignificant temporal information.
Under-standing temporal information is veryuseful in many NLP applications, such asinformation extraction, documents sum-marization and question answering.Therefore, the temporal expression nor-malization which is used for transform-ing temporal expressions to temporal in-formation has absorbed many research-ers?
attentions.
But previous works,whatever the hand-crafted rules-based orthe machine-learnt rules-based, all cannot address the actual problem abouttemporal reference in real texts effective-ly.
More specifically, the reference timechoosing mechanism employed by theseworks is not adaptable to the universalimplicit times in normalization.
Aimingat this issue, we introduce a new refer-ence time choosing mechanism for tem-poral expression normalization, calledreference time dynamic-choosing, whichassigns the appropriate reference times todifferent classes of implicit temporal ex-pressions dynamically when normalizing.And then, the solution to temporal ex-pression defuzzification by scenario de-pendences among temporal expressionsis discussed.
Finally, we evaluate thesystem on a substantial corpus collectedby Chinese news articles and obtainedmore promising results than comparedmethods.1 IntroductionTemporal expression normalization is very im-portant for temporal information processing be-cause it is in charge of transforming temporalexpressions in surface texts to temporal informa-tion behind surface texts.
Temporal informationis defined as the knowledge about time or dura-tion, which can be abstracted into some objectsdefined as temporal attributes in TIMEX2 Stan-dard [Ferro et al, 2005].
Human being can taketemporal relation reasoning and anchor eventson the time line with this information.
Mean-while, temporal expressions are defined aschunks of texts which convey explicit or implicittemporal information.
So TERN evaluation plan1gives the task of temporal expression normaliza-tion that is annotating the appropriate temporalattributes for each temporal expression in texts.For example, a simple temporal expression,?May 1, 2009?, can be normalized as <TIMEX2VAL = ?2009-05-01?> May 1, 2009</TIMEX2>.Unfortunately, temporal expressions in realtexts are more complicated because they containa large number of Implicit Times besides Expli-cit Times.
Here,(1) Explicit Time: Explicit Time can directlybe laid in the timeline.
Basically, it is a directentry in the timeline and need not to be trans-formed.
E.g., ?May 1, 2009?.
(2) Implicit Time: Implicit Time can bemapped as an entry in the timeline with help ofreal contexts and some predefined knowledgeand need to be transformed.
E.g., ?May 1?, ?to-morrow?
and ?two day ago?.Consequently, temporal expression normali-zation is mainly aiming at Implicit Times that1 http://timex2.mitre.org/tern.html1498need to be transformed with referring to somespecific times.
However, the previous works ontemporal expression normalization which basi-cally adopt two mechanisms for choosing refer-ence time, static time-value [Mani and Wilson,2000; Wu et al, 2005; Wu et al, 2005] and stat-ic choosing-rules [Vozov, 2001; Jang et al, 2004;Lin et al, 2008], are not compatible with the realtexts.
The static time-value mechanism refers totaking the report time or publication time of thedocument as the fixed reference time for thewhole text when normalizing.
And the staticchoosing-rules mechanism means that the ma-chine always uses fixed rules by contexts tochoose reference time for each Implicit Timewhatever its temporal semantics is.
The rulebased on the nearest narrative time [Lin et al,2008] is the most typical and effective one,which uses the nearest narrative time in textabove as the reference time all the while.
Butactually the context-free assumption or the roteoperation is unsuitable for universal ImplicitTimes.
For example, a news report is as Figure 1shows:(Beijing, May 6, 2009) B company took over A companytotally on March 8, 2000.
After one week, B companylisted in Hong Kong, and became the first listed companyin that industry.
However, owing to the decision-makingmistakes in the leadership and the company later poormanagement, B company got into debt for several hun-dred million dollars, and was forced to announce bank-ruptcy this Monday.Figure 1.
Example of news reportsFor these two Implicit Times in the text, ?afterone week?
and ?this Monday?, obviously therewill be critical conflicts when using these twomechanisms referred above to choose referencetime.
The static time-value is unsuited for the?after one week?, and ?this Monday?
makesmistakes when taking the nearest narrative time(i.e., ?after one week?)
as the reference time tonormalize according to the static choosing-rules.Motivated by this issue, we propose a new ref-erence time choosing mechanism for temporalexpression normalization.
Firstly, we segmentthe Implicit Time into two parts, modifier andtemporal noun, and then train a classifier withreferential features of these two parts to classifyImplicit Times.
As a result, we choose the cor-responding reference time for each temporal ex-pression depending on its class when normaliz-ing.
Meanwhile an acceptable defuzzificationsolution is introduced to normalize fuzzy timesin our method.
And the contributions of this pa-per are:(1) We introduce a simple but effective refer-ence time choosing method, called dynamic-choosing mechanism, which can choose the ap-propriate reference times automatically for uni-versal Implicit Times as well as be compatiblewith the dynamically changeable contexts.
(2) Going beyond traditional normalizationapproaches, we develop a new way to deal withthe defuzzification in order to figure out thefuzzy reference time (the reference time is vagueor has imprecise start and end in timeline),which makes the normalization robust and im-prove the accuracy of reference times.The rest of this paper is organized as follows.Section 2 discusses related works.
In section 3we describe the reference time dynamic-choosing mechanism.
The temporal expressionnormalization is presented in section 4.
Section 5gives the description about experiments andevaluations.
Finally, conclusion and future workare presented in section 6.2 Related WorkIn general, several research works onnormalizing temporal expressions, which areinvolved in English [Mani and Wilson, 2000],French [Vozov, 2001], Spanish [Saquete et al,2002], Korean [Jang et al, 2004] and Chinese[Wu et al, 2005; Lin et al, 2008], have beenreported in recent years.
Among them, the hand-crafted rules-based methods [Saquete et al, 2002;Schilder and Habel, 2001; Mani and Wilson,2000] can deal with various temporalexpressions, but the procedure to build a robustrules system is quite time-consuming.
Withregard to the machine learning for normalization[Jang et al, 2004; Wu et al, 2005; Vicente-Diezet al, 2008], the potential task is theclassification which is deciding one explanationof a temporal expression from severalalternatives.However, these works on temporal expressionnormalization do not give an effective referencetime choosing method for Implicit Times in realtexts.
More specifically, the pioneer work byLacarides [1992] investigated various contextualeffects on different temporal-reference relations.Then Hitzeman et al [1995] discussed the refer-1499ence-choosing taking into account the effects oftense, aspect, temporal adverbials and rhetoricalrelations.
Dorr and Gaasterland [2002] presentedthe enhanced one in addition considering theconnecting words.
But they are theoretical innature and heavily dependent on languages.
Cur-rently, the static time-value mechanism [Maniand Wilson, 2000; Wu et al, 2005; Wu et al,2005] and the static choosing-rules mechanism[Vozov, 2001; Jang et al, 2004; Lin et al, 2008]for reference time choosing are applied intosome systems widely.
Nevertheless, as the dis-cussion in section 1, these two ways are notadaptable to universal Implicit Times.
In addi-tion, Vicente-Diez et al [2008; 2009] discussedthe reference date for relative times, but the al-ternative rules are not effective in experiments.Lin et al [2008] considered the condition thatthere is no report time or publication time whenchoosing reference time.Referring to the defuzzification, TIMEX2Standard [Ferro et al, 2005] takes the X place-holder to express fuzzy times?
value, so the re-lated works [Jang et al, 2004; Lin et al, 2008;Vicente-Diez and Martinez, 2009] almost followthis vague expressing way.
However, this me-thod can not address the actual situation that thefuzzy time is referred to by other times.
Basedon the human cognitive psychology, Anderson etal.
[1983] presented a classical scenario-timeshifting model that discussed the time includesthe fuzzy time is the clue to scenario shiftingwhen people reading.
Inspired by this issue andbased on our experiments, we find all times in asame scenario own strong dependences in tem-poral granularity, which can effectively help usdetermine granularity in defuzzification.
Andmore details are discussed in section 4.2.Aiming at solving these challenges above, weestablish a temporal expression normalizationsystem for real texts, which improves the accu-racy of temporal reference normalization re-markably by the dynamic-choosing mechanism.3 Reference Time Dynamic-choosingMechanism3.1 Referential feature in Implicit TimeIn this paper, we define the Implicit Time con-sists of the modifier and the temporal nounwhich is modified by modifiers.
And here weextend the modifier based on the TIMEX2 Stan-dard, which include verb, conjunction, adverband preposition that quantify or modify temporalnouns.
For example, ?ten days?
is a temporalnoun, but ?ten days ago?
is modified after add-ing the modifier ?ago?.Meanwhile we find no matter how long orhow many modifiers modify the temporal noun,the whole temporal expression holds the originaltemporal reference inferred from the temporalnoun.
Moreover, the key point of normalizingtemporal expressions is choosing the appropriatereference time according to the real context ra-ther than deciding the right direction or compu-ting the measurable offset.
For instance, withregard to these two Implicit Times in Figure 1,?after one week?
and ?this Monday?, we canachieve the referential direction easily from themodifiers through some mapping rules.
Mean-while, the offsets are able to be understood di-rectly by machine with pattern matching.
But forthe reference time, we must build the context-depending reference reasoning to trace it.
Thereference link is described as Figure 2 shows.Figure 2.
Example of reference linkFrom the reference reasoning, we can see thefull temporal reference comes from two parts:modifier reference and temporal noun reference.Because the former is inferred from the latter,the temporal noun reference reasoning playsmore important roles in normalization.
In otherwords, the reference reasoning of the whole Im-plicit Time strongly depends on the temporalnoun.
Furthermore, in the practical operation, weindeed take the report time or the nearest narra-tive time in text above as the reference time ofthe temporal noun when normalizing a wholeImplicit Time.
Therefore, we consider the classi-1500fication of the Implicit Time based on the classesof temporal noun?s reference time.
Basically wetag the Implicit Time as the same class as itstemporal noun?s under classifying temporalnouns into two classes according to the referen-tial feature.
(1) Global Temporal Noun: Global TemporalNoun takes the report time or publication time ofthe document as the reference time when norma-lizing.
Basically, it is independent of the localcontext.
(2) Local Temporal Noun: Local TemporalNoun makes reference to the nearest narrativetime in text above in normalization due to de-pending on the local context.Table 1 and 2 give some examples of GlobalTemporal Noun and Local Temporal Noun inreal texts.Consequently, here we denote the ImplicitTime consists of the Global Temporal Noun andthe modifier(s) by Global Time or GT, and ac-cordingly the Local Temporal Noun correspondsto Local Time or LT.Class Sub-class ExamplesGlobal TemporalNounyear last yearmonth next monthday this Fridayhour tonightfuzzy lately2Table 1.
Common Global Temporal Noun ex-pressionsClass Sub-class ExamplesLocal TemporalNounyear that yearmonth Octoberday the second dayhour morningfuzzy thenduration one monthTable 2.
Common Local Temporal Noun expres-sions3.2 Na?ve Bayesian ClassifierA variety of machine learning classifiers are de-signed to resolve the classification problem,such as SVM classifier, ME classifier and theDecision Tree family.
But the performance ofthese classifiers is greatly depending on the fea-tures selection.
Based on the observation andanalysis in our experiments, we find the referen-2 Some single temporal adverbs are taken as temporal noun,e.g.
recently, currently and so on.tial feature holds in the temporal noun is hard toexpress with some explicit denotations.
For ex-ample, ?that year?
and ?this year?
are nearlyidentical in surface feature, but the former is lo-cally context-depending while the latter is local-ly context-free.
So the Na?ve Bayesian Classifierthat assumes independence among feature deno-tations is suitable to be applied to our method.We take the single word in the temporal nounas the object attribute ix after removing the Ex-plicit Time in the whole text.
Given the classlabel c , the classifier learns the conditional prob-ability of each attribute ix from training data.Meanwhile, achieving the practical instanceof X , classification is then performed by apply-ing Bayes rules to compute the probability of c ,and then predicting the class with the highestposterior probability.1 2arg max ( | , , , )o n icc grade c x x x x X= ?L  (1)1 21 21 2( | , , , )( | , , , )( | , , , )nnnp c x x xgrade c x x xp c x x x= LLL(2)Applying Bayes rules to (2), we have:1 21 21 211( , , , | ) ( )( | , , , )( , , , | ) ( )( | ) ( )( | ) ( )nnnniiniip x x x c p cgrade c x x xp x x x c p cp x c p cp x c p c====?
?LLL(3)Actually, we estimate ( | )ip x c and ( | )ip x c byMaximum Likelihood Estimation (MLE) fromtraining data with Dirichlet Smoothing method[Li et al, 2004].1( , )( | )( , )ii njjnum x cp x cnum x c n?
?=+=+ ??
(4)1( , )( | )( , )ii njjnum x cp x cnum x c n?
?=+=+ ??
(5)3.3 Reference Time ChoosingIn our approach, there is a reference time table isused to hold full reference times for the wholetext, and we need to update and maintain it dy-namically after each normalizing processing.1501The time table consists of two parts: Global Ref-erence Time and Local Reference Time.
(1) Global Reference Time: Global ReferenceTime (GRT) is a type of reference time which isreferred to by the Global Time.
Specifically, it isthe report time or the publication time of thedocument.
(2) Local Reference Time: Local ReferenceTime (LRT) is made reference to by the LocalTime.
It will be updated dynamically after eachnormalizing.Figure 3 shows a sample of the interaction be-tween reference times and target times.Figure 3.
Interaction between reference timesand target timesIn Figure 3, we notice that different classes oftime dynamically and automatically choose ref-erences based on their respective classes ratherthan do it using the fixed value or the inconside-rate rule under the static mechanism.
And thereference time table is updated in real time fi-nishing each normalizing, which makes the tem-poral situation compliable with dynamicallychangeable contexts.4 Temporal Expression Normalization4.1 Basic Normalizing AlgorithmIn the beginning, we need to achieve the reporttime (RT) or the publication time (PT) of thedocument to initialize the GRT and LRT.
Addi-tionally, the fuzzy time can be referred to byother times in the normalization, but we mustsolve the defuzzification problem before takingit as the reference time.
With respect to this issue,we will discuss it in the next section.
Conse-quently, the practical normalizing algorithm is asfollows.Algorithm: TimeNormalizeInput: temporal expression ti in textOutput: regular time list TListBegin//initialize the GRT and LRT with RT or PT of thisdocumentGRT ?
Initialize (RT|PT)LRT ?
Initialize (RT|PT)for each ti in text do//segment ti into modifier and temporal nounti???SegmentTemporal?
?ti?if IsExplicitTime (ti) is true//update the time table with tiLRT ??UpdateTime??ti??????????
?//insert ti into regular time list directlyTList ?
InsertList (ti)elseif?IsLocalTime??ti??
?is?true?//retrieve the latest LRT from time table and thennormalize ti?Ti???RegularizeTemporal??ti?
, LRT?else?//retrieve GRT from time table and thennormalize ti?Ti???RegularizeTemporal??ti?
, GRT?LRT ??UpdateTime?
?Ti?end ifTList ?
InsertList (Ti)end ifreturn TListEnd Begin4.2 Temporal Expression DefuzzificationIn general, the defuzzification for fuzzy timesfaces two problems: deciding granularity andchoosing offset.
Here we introduce some know-ledge on the human cognitive psychology andthe empirical method to figure out these two is-sues respectively.
Based on the scenario-timeshifting model referred in related works, we getthe conclusion that once the scenario is shifting,the time is shifting.
More specifically, the timeshifting is reflected in the temporal granularitybetween two different scenarios.
So referring towriters, they will choose a few temporal expres-sions own the same granularity to render the co-herent temporal dimensionality in one scenarioin order to avoid generating improper scenarioshifting for readers.
Figure 4 describes the varia-tion process of the temporal granularity betweentwo different scenarios through scenario-timeshifting.1502Figure 4.
Variation process of temporal granu-larityAs conveyed in Figure 4, temporal expressionsin the same scenario are constrained by the sce-nario depending.
Hence fuzzy times should keeppace with scenario-correlative times in granu-larity.
For example, two sentences in differentscenarios:?He was in Hong Kong yesterday, but now he isin Beijing.?
?He was in Hong Kong last year, but now he isin Beijing.
?Obviously, the first ?now?
means ?today?
inthat scenario, and it has the same temporal gra-nularity with ?yesterday?.
Meanwhile it will bemore appropriate for the second ?now?
choosing?year?
as the temporal granularity than choosing?day?
because of the dependence to the scena-rio-correlative time.
In narrative, the paragraphis normally considered as the minimum unit ofthe scenario, so scenario-correlative relationsshould stand on the one paragraph at least.But for the first temporal expression in the pa-ragraph, we need to think about two specificconditions: when it appears in the first paragraphand in the non-first paragraph if it is a fuzzy time.Because there is no scenario shifting to the firstparagraph, we employ a dictionary to initializethe algorithm when the first time included in thefirst paragraph is fuzzy time.
The defuzzificationprocess is outlined as follows.Algorithm: TempGranuarityDefuzzifyInput: temporal expression ti in textOutput: precise-granularity time ti?Begin//obtain the granularity of tigranularity ?
GetGranularity (ti)//decide whether ti is a fuzzy timeif granularity is not nullti?
?
tielseif ti is not first temporal expression in current para-graph//assign the former?s granularity to the tigranularity ?
GetGranularity (ti-1)elseif ti is not included in first paragraph//decide which granularity is assigned to ti be-tween ti-1 and ti+1if IsSameGranularity (GetGranularity(ti-1), GetGranularity (ti+1)) is not truegranularity ?
GetGranularity (Coar-serCompare (ti-1, ti+1))elsegranularity ?
GetGranularity (ti+1)end ifelse//retrieve default granularity from dictionarygranularity ?
FindGranularityInDict (ti)end ifend if//retrieve default offset from dictionaryoffset ?
FindOffsetInDict (ti)//update and intact all temporal attributes of titi?
?
ModifyTimeAttribute (ti, granularity, offset)end ifreturn ti?End BeginIt?s possible for the first temporal expressionto correlate with forenamed times in last para-graph in real texts, so we choose the coarser gra-nularity for the fuzzy time when appearing con-flicts in granularity between the last temporalexpression and the next temporal expression.Additionally, an empirical fuzzy time dictionaryis constructed as the default in order to figureout the offset problem.
For example, ?lately?
isdenoted in dictionary as below.LatelyCommon synonyms: recently, latterly, late, of lateDefault granularity: dayDefault offset: 7 unitsFinishing the defuzzification for the whole text,the basic normalizing algorithm is evoked then.In the experiments, we find that the temporalexpression defuzzified can clearly improve theaccuracy of reference times besides discoveringthe implicit temporal information much more.5 Evaluation5.1 SetupBecause the normalization for temporal expres-sions is independent of the language [Wilson etal., 2001], we take the formal Chinese news3 as3 People?s Daily news corpus (January, 1998), supported byInstitute of Computational Linguistics (ICL), Peking Uni-versity.1503the experimental corpus, which consist of 3148Chinese news articles.
The data collection con-tains 2,816,612 characters/967,884 words and21,176 manually annotated temporal nouns.Among this corpus, 2518 articles (80%) include13,835 temporal expressions are used as trainingdata for the classification, and the rest (20%) astest data.
Then the whole corpus is tested for thenormalization.
Event-anchored expressions arerelevant with a specific event and it is hard torepresent the exact meaning of them, so in oursystem, event-anchored expressions are notnormalized.5.2 ResultsResults on Implicit Times classification: Wefirstly choose some temporal expressions classi-fied in advance by crafted, and manually extendthem in expressing patterns as the original train-ing samples.
For example, ?last month?
will ex-tend to ?this month?
and ?next month?, whichall belong to Global Times.
Actually there areonly 16,104 temporal expressions in our experi-ment because integrated temporal expressions incorpus are segmented into several parts, and wecombine them together again before operating.Using classifier trained by training data, we get2,264 Global Times and 998 Local Times fromtesting collections, where there are 1,705 GlobalTimes and 804 Local Times are correct respec-tively by manual statistics.
Table 3 gives the de-tails of classification.From the experiment data, we find the preci-sion and the recall almost below 80%, and theclassification performance is not expected.
Thereason is that we do not consider some specialapplication situations beforehand, which resultin classifying errors.
For example, the GlobalTime should be taken as the Local Time when itappears in the dialog or speech that marks boun-daries by a pair of quotation marks.
So we intro-duce some revising patches shown in Table 4 todeal with this issue.
Here the second and thefourth patches make corresponding temporalexpressions be treated as non-target times thatneed not be processed.
In addition, Time Set andNon-Specific are taken as the other classes ex-cept the Implicit Time and the Explicit Time.The final results with revising patches are shownin Table 5.
Obviously, revising patches make theclassification be more adapted for the real texts,and the performance evaluation is promising.Class #Correct Precision (%)Recall(%)F-measure(%)Global Time 1705 75.31 78.64 76.94Local Time 804 80.56 79.45 80.00Sum/Average 2509 77.94 79.05 78.47Table 3.
Results of classificationID Patch Type Patterns Operations1 Dialog/Speech ?XXX?
Time ?
LT2 Book/Movie?XXX?XXXBe omitted3 Time Set quantifier + XXXTime ?
oth-ers4 Proper Noun e.g.
October Revolution Be omitted5 Non-Specific e.g.
child-hoodTime ?
oth-ersTable 4.
Revising patches for classificationClass #Correct Precision (%)Recall(%)F-meas-ure(%)Global Time 1879 88.69 86.67 87.67Local Time 918 90.55 90.71 90.63Sum/Average 2797 89.62 88.69 89.15Table 5.
Results of classification with revisingpatchesResults on temporal expression normalization:For evaluating our algorithm objectively, wecompare the experiment result with other twomethods on the same testing corpus.
The firstcompared method which is adopted in many tra-ditional systems [Li et al, 2004; Wu et al, 2005]applies the static time-value mechanism to de-termine the reference time.
The nearest narrativetime [Lin et al, 2008; Vicente-Diez and Marti-nez, 2009] that represents the static choosing-rules mechanism is taken as the second com-pared method.
Table 6 presents the results.MethodAveragereferentupdating/articleAccuracy(%)ErrorsReferent(%)Others(%)STVM 0 68.42 22.84 8.74SCRM 7.8 76.19 11.25 12.56Ourmethod 4.2 83.55 7.33 9.12*STVM: Static Time-Value MechanismSCRM: Static Choosing-Rules MechanismTable 6.
Results of normalization1504The data shows that our method exceeds thecompared ones evidently.
The accuracy increas-es by 15.13% at most, and the errors by referentdecreases by 3.92% at least.
In contrast to theSCRM, we avoid the limitation that SCRM onlyconcentrates on the nearest distance for choosingreferent.
Meanwhile, because the SCRM pays noattention to the normalization for fuzzy temporalexpressions, the error by others (e.g.
granularity)is greater than ours.
Additionally, the STVMmethod applies the report time or the publicationtime of the document as the reference time forthe whole text, so there is no referent updating inprocess.
We mark all errors as referent errors aslong as they involve with false reference time inresults analysis, therefore, the STVM gets thehighest referent errors ratio.With respect to the defuzzification, we eva-luate it on fuzzy times separately.
All defuzzi-fied fuzzy times are assessed by human, andthen decided whether they are acceptable to thecontext.
The evaluation results are shown in Ta-ble 7.Type #Acceptable Acceptable ratio (%)As refe-rent (%)Global Time 687 80.14 18.39Local Time 159 92.61 6.43Sum/Average 846 86.38 12.41Table 7.
Evaluations on temporal expressiondefuzzificationFor the fuzzy temporal expression in LocalTime, it is much fewer and easier than the one inGlobal Time in number and expression respec-tively, so the defuzzification in Local Timesachieves more expected results.
On the otherhand, the fuzzy time in Global Time is often thefirst temporal expression in the first paragraph,and the corresponding dictionary-based methodcertainly affects the experiment results.
Accord-ing to the percentages that the temporal expres-sions defuzzified successfully account for in theall reference times, it demonstrates that the de-fuzzification makes contributions to the referen-tial normalization besides discovering the inter-nal temporal information in the fuzzy time.6 ConclusionIn this paper, we present an approach to auto-matically normalizing temporal expressions un-der the reference time dynamic-choosing me-chanism.
The referential feature in temporalnouns is applied to classify Implicit Times.Based on this, different classes of times can benormalized according to their respective classes.Meanwhile, we introduce the scenario-timeshifting model to deal with the defuzzificationproblem.
The experiment shows that our ap-proach achieves more promising evaluation re-sults, and makes the automatic normalizationmore adaptable to real texts than the prior works.However, the neglect on the event-anchored ex-pression certainly restricts the whole system inapplications, so the event-anchored expressionwill be our research focus in future.Acknowledgement This work is supported bythe National Natural Science Foundation of Chi-na under the grant no.
60776801 and 70803001,the Open Projects Program of National Labora-tory of Pattern Recognition (no.20090029), theKey Laboratory of Advanced InformationScience and Network Technology of Beijing (no.xdxx1005), the National High Technology Re-search and Development Program ("863" Pro-gram) of China (Grant No.
2009AA12Z204).ReferencesAnderson, A., Garrod, S.C. and Sandford, A.J.
1983.The Accessibility of Pronominal Antecedents As aFunction of Episode Shifts in Narrative Text.Quarterly Journal of Experimental Psychology,35a, pp.
427-440.Dorr, B. and Gaasterland, T. 2002.
Constraints on theGeneration of Tense, Aspect, and ConnectingWords from Temporal Expressions.
Technical Re-port CS-TR-4391,UMIACS-TR-2002-71, LAMP-TR-091, University of Maryland, College Park,MD, 2002.Ferro, L., Gerber, L., Mani, I., et al 2005.
TIDES2005 Standard for the Annotation of TemporalExpressions [EB/O L].
(2005-09)http: //timex2.mitre.org.Hitzeman, J., Moens, M. and Grover, C. 1995.
Algo-rithms for Analyzing the Temporal Structure ofDiscourse.
In Proceedings of the 7th EuropeanMeeting of the Association for Computational Lin-guistics, pp.
253-260.Jang, S.B., Baldwin, J. and Mani, I.
2004.
AutomaticTIMEX2 Tagging of Korean News.
ACM Trans-actions on Asian Language Informationprocessing 3(1), 51-65.1505Lascarides, A., Asher, N. and Oberlander, J.
1992.Inferring Discourse Relations in Context.
In Pro-ceedings of the 30th Meeting of the Association forComputational Linguistics, pp.
1-8.Li, W.J., Wong, K.F., Cao, G.H.
et al 2004.
Apply-ing Machine Learning to Chinese Temporal Rela-tion Resolution.
In Proceedings of the 42nd An-nual Meeting on Association for ComputationalLinguistics, pp.
582-588.Lin, J., Cao, D.F.
and Yuan, C.F.
2008.
AutomaticTIMEX2 tagging of Chinese temporal information.Journal of Tsinghua University 48(1), 117-120.Mani, I. and Wilson, G. 2000.
Robust TemporalProcessing of News.
In Proceedings of the 38thAnnual Meeting of the Association for Computa-tional Linguistics, pp.
69-76.Saquete, E., Martinez-Barco, Patricio and Munoz, R.2002.
Recognizing and Tagging Temporal Expres-sions in Spanish, in Proceedings of Workshop onAnnotation Standards for Temporal Information inNatural Language, pp.
44-51.Schilder, F. and Habel, C. 2001.
From Temporal Ex-pressions to Temporal Information: Semantic Tag-ging of News Messages.
In Proceedings of theACL 2001 Workshop on Temporal and Spatial In-formation Processing, pp.
65-72.Vazov, N. 2001.
A System for Extraction of Tempor-al Expressions from French Texts based on Syn-tactic and Semantic Constraints.
In Proceedings ofthe ACL Workshop on Temporal and Spatial In-formation Processing, pp.
96-103.Vicente-D?ez, M.T., Samy, D. and Mart?nez, P. 2008.An Empirical Approach to a Preliminary Success-ful identification and Resolution of Temporal Ex-pressions in Spanish News Corpora.
In Proceed-ings of the Sixth International Language Re-sources and Evaluation, pp.
2153-2158.Vicente-D?ez, M.T., Martinez, P. 2009.
TemporalSemantics Extraction for Improving Web Search.In Proceedings of the Workshop on Database andExpert Systems Application, pp.
69-73.Wilson, G., Mani, I., Sundheim, B. et al 2001.
AMultilingual Approach to Annotating and Extract-ing Temporal Information.
In Proceedings of theWorkshop on Temporal and Spatial InformationProcessing, pp.
1-7.Wu, M.L., Li, W.J., Lu, Q. and Li, B.L.
2005.CTEMP: A Chinese temporal parser for extractingand normalizing temporal information.
In Pro-ceedings of International Joint Conference onNatural Language Processing, pp.
694-706.Wu, M.L., Li, W.J., Chen, Q. and Lu, Q.
2005.
Nor-malizing Chinese Temporal Expressions with Mul-ti-label Classification.
In Proceedings of NaturalLanguage Processing and Knowledge Engineering,pp.
318-323.1506
