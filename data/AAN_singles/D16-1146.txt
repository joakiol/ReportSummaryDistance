Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1389?1399,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLifted Rule Injection for Relation EmbeddingsThomas DemeesterGhent University - iMindsGhent, Belgiumtdmeeste@intec.ugent.beTim Rockta?schel and Sebastian RiedelUniversity College LondonLondon, UK{t.rocktaschel,s.riedel}@cs.ucl.ac.ukAbstractMethods based on representation learning cur-rently hold the state-of-the-art in many naturallanguage processing and knowledge base in-ference tasks.
Yet, a major challenge is how toefficiently incorporate commonsense knowl-edge into such models.
A recent approach reg-ularizes relation and entity representations bypropositionalization of first-order logic rules.However, propositionalization does not scalebeyond domains with only few entities andrules.
In this paper we present a highly ef-ficient method for incorporating implicationrules into distributed representations for au-tomated knowledge base construction.
Wemap entity-tuple embeddings into an approxi-mately Boolean space and encourage a partialordering over relation embeddings based onimplication rules mined from WordNet.
Sur-prisingly, we find that the strong restrictionof the entity-tuple embedding space does nothurt the expressiveness of the model and evenacts as a regularizer that improves general-ization.
By incorporating few commonsenserules, we achieve an increase of 2 percentagepoints mean average precision over a matrixfactorization baseline, while observing a neg-ligible increase in runtime.1 IntroductionCurrent successful methods for automated knowl-edge base construction tasks heavily rely on learneddistributed vector representations (Nickel et al,2012; Riedel et al, 2013; Socher et al, 2013; Changet al, 2014; Neelakantan et al, 2015; Toutanova etal., 2015; Nickel et al, 2015; Verga et al, 2016;Verga and McCallum, 2016).
Although these mod-els are able to learn robust representations fromlarge amounts of data, they often lack common-sense knowledge.
Such knowledge is rarely explic-itly stated in texts but can be found in resourceslike PPDB (Ganitkevitch et al, 2013) or WordNet(Miller, 1995).Combining neural methods with symbolic com-monsense knowledge, for instance in the form ofimplication rules, is in the focus of current research(Rockta?schel et al, 2014; Wang et al, 2014; Bow-man et al, 2015; Wang et al, 2015; Vendrov et al,2016; Hu et al, 2016; Rockta?schel and Riedel, 2016;Cohen, 2016).
A recent approach (Rockta?schel etal., 2015) regularizes entity-tuple and relation em-beddings via first-order logic rules.
To this end, ev-ery first-order rule is propositionalized based on ob-served entity-tuples, and a differentiable loss term isadded for every propositional rule.
This approachdoes not scale beyond only a few entity-tuples andrules.
For example, propositionalizing the rule ?x :isMan(x)?
isMortal(x) would result in a verylarge number of loss terms on a large database.In this paper, we present a method to incorporatesimple rules while maintaining the computationalefficiency of only modeling training facts.
Thisis achieved by minimizing an upper bound of theloss that encourages the implication between rela-tions to hold, entirely independent from the num-ber of entity pairs.
It only involves representa-tions of the relations that are mentioned in rules, aswell as a general rule-independent constraint on theentity-tuple embedding space.
In the example givenabove, if we require that every component of the1389vector representation of isMan is smaller than thecorresponding component of relation isMortal,then we can show that the rule holds for any non-negative representation of an entity-tuple.
Hence ourmethod avoids the need for separate loss terms forevery ground atom resulting from propositionaliz-ing rules.
In statistical relational learning this typeof approach is often referred to as lifted inferenceor learning (Poole, 2003; Braz, 2007) because itdeals with groups of random variables at a first-orderlevel.
In this sense our approach is a lifted form ofrule injection.
This allows for imposing large num-bers of rules while learning distributed representa-tions of relations and entity-tuples.
Besides drasti-cally lower computation time, an important advan-tage of our method over Rockta?schel et al (2015) isthat when these constraints are satisfied, the injectedrules always hold, even for unseen but inferred facts.While the method presented here only deals with im-plications and not general first-order rules, it doesnot rely on the assumption of independence betweenrelations, and is hence more generally applicable.Our contributions are fourfold: (i) we develop avery efficient way of regularizing relation represen-tations to incorporate first-order logic implications(?3), (ii) we reveal that, against expectation, map-ping entity-tuple embeddings to non-negative spacedoes not hurt but instead improves the generaliza-tion ability of our model (?5.1) (iii) we show im-provements on a knowledge base completion task byinjecting mined commonsense rules from WordNet(?5.3), and finally (iv) we give a qualitative analysisof the results, demonstrating that implication con-straints are indeed satisfied in an asymmetric wayand result in a substantially increased structuring ofthe relation embedding space (?5.6).2 BackgroundIn this section we revisit the matrix factorization re-lation extraction model by Riedel et al (2013) andintroduce the notation used throughout the paper.We choose the matrix factorization model for itssimplicity as the base on which we develop impli-cation injection.Riedel et al (2013) represent every relation r ?R (selected from Freebase (Bollacker et al, 2008)or extracted as textual surface pattern) by a k-dimensional latent representation r ?
Rk.
A par-ticular relation instance or fact is the combinationof a relation r and a tuple t of entities that are en-gaged in that relation, and is written as ?r, t?.
Wewrite O as the set of all such input facts availablefor training.
Furthermore, every entity-tuple t ?
Tis represented by a latent vector t ?
Rk (with T theset of all entity-tuples in O).Model F by Riedel et al (2013) measures thecompatibility between a relation r and an entity-tuple t using the dot product r>t of their respec-tive vector representations.
During training, therepresentations are learned such that valid facts re-ceive high scores, whereas negative ones receive lowscores.
Typically no negative evidence is availableat training time, and therefore a Bayesian Personal-ized Ranking (BPR) objective (Rendle et al, 2009)is used.
Given a pair of facts fp := ?rp, tp?
6?
O andfq := ?rq, tq?
?
O, this objective requires thatr>p tp ?
r>q tq.
(1)The embeddings can be trained by minimizing aconvex loss function `R that penalizes violationsof that requirement when iterating over the trainingset.
In practice, each positive training fact ?r, tq?
iscompared with a randomly sampled unobserved fact?r, tp?
for the same relation.
The overall loss canhence be written asLR =??r,tq?
?Otp?T , ?r,tp?6?O`R(r>[tp ?
tq]).
(2)and measures how well observed valid facts areranked above unobserved facts, thus reconstructingthe ranking of the training data.
We will hence-forth call LR the reconstruction loss, to make a dis-tinction with the implication loss that we will intro-duce later.
Riedel et al (2013) use the logistic loss`R(s) := ?
log ?
(?s), where ?
(s) := (1 + e?x)?1denotes the sigmoid function.
In order to avoid over-fitting, an L2 regularization term on the r and t em-beddings is added to the reconstruction loss.
Theoverall objective to minimize hence isLF = LR + ?
(?r?r?22 +?t?t?22) (3)where ?
is the regularization strength.13903 Lifted Injection of ImplicationsIn this section, we show how an implication?t ?
T : ?rp, t?
?
?rq, t?, (4)can be imposed independently of the entity-tuples.For simplicity, we abbreviate such implications asrp ?
rq (e.g., professorAt?
employeeAt).3.1 Grounded Loss FormulationThe implication rule can be imposed by requiringthat every tuple t ?
T is at least as compatible withrelation rp as with rq.
Written in terms of the latentrepresentations, eq.
(4) therefore becomes?t ?
T : r>p t ?
r>q t (5)If ?rp, t?
is a true fact with a high score r>p t, andthe fact ?rq, t?
has an even higher score, it must alsobe true, but not vice versa.
We can therefore injectan implication rule by minimizing a loss term witha separate contribution from every t ?
T , addingup to the total loss if the corresponding inequalityis not satisfied.
In order to make the contribution ofevery tuple t to that loss independent of the magni-tude of the tuple embedding, we divide both sides ofthe above inequality by ?t?1.
With t?
:= t/?t?1, theimplication loss for the rule rp ?
rq can be writtenasLI =?
?t?T`I([rp ?
rq]>t?)
(6)for an appropriate convex loss function `I , similarlyto eq.
(2).
In practice, the summation can be reducedto those tuples that occur in combination with rp orrq in the training data.
Still, the propositionalizationin terms of training facts leads to a heavy computa-tional cost for imposing a single implication, simi-lar to the technique introduced in Rockta?schel et al(2015).
Moreover, with that simplification there isno guarantee that the implication between both re-lations would generalize towards inferred facts notseen during training.3.2 Lifted Loss FormulationThe problems mentioned above can be avoided ifinstead of LI , a tuple-independent upper bound isminimized.
Such a bound can be constructed, pro-vided all components of t are restricted to a non-negative embedding space, i.e., T ?
Rk,+.
If thisholds, Jensen?s inequality allows us to transformeq.
(6) as followsLI =?
?t?T`I( k?i=1t?i [rp ?
rq]>1i)(7)?k?i=1`I([rp ?
rq]>1i) ?
?t?Tt?i (8)where 1i is the unit vector along dimension i intuple-space.
This is allowed because the {t?i}ki=1form convex coefficients (t?i > 0, and ?i t?i = 1),and `I is a convex function.
If we defineLUI :=k?i=1`I([rp ?
rq]>1i) (9)we can writeLI ?
?LUI (10)in which ?
is an upper bound on ?t t?i.
One suchbound is |T |, but others are conceivable too.
In prac-tice we rescale ?
to a hyper-parameter ??
that we useto control the impact of the upper bound to the over-all loss.
We call LUI the lifted loss, as it no longerdepends on any of the entity-tuples; it is groundedover the unit tuples 1i instead.The implication rp ?
rq can thus be imposed byminimizing the lifted loss LUI .
Note that by mini-mizing LUI , the model is encouraged to satisfy theconstraint rp ?
rq on the relation embeddings,where ?
denotes the component-wise comparison.In fact, a sufficient condition for eq.
(5) to hold, isrp ?
rq and ?t ?
T : t ?
0 (11)with 0 the k-dimensional null vector.
This corre-sponds to a single relation-specific loss term, andthe general restriction T ?
Rk,+ on the tuple-embedding space.3.3 Approximately Boolean Entity TuplesIn order to impose implications by minimizing alifted loss LUI , the tuple-embedding space needs tobe restricted to Rk,+.
We have chosen to restrict thetuple space even more than required, namely to thehypercube t ?
[0, 1]k, as approximately Booleanembeddings (Kruszewski et al, 2015).
The tuple1391embeddings are constructed from real-valued vec-tors e, using the component-wise sigmoid functiont = ?
(e), e ?
Rk.
(12)For minimizing the loss, the gradients are hencecomputed with respect to e, and the L2 regulariza-tion is applied to the components of e instead of t.Other choices for ensuring the restriction t ?
0in eq.
(11) are possible, but we found that our ap-proach works better in practice than those (e.g., theexponential transformation proposed by Demeesteret al (2016)).
It can also be observed that the unittuples over which the implication loss is grounded,form a special case of approximately Boolean em-beddings.In order to investigate the impact of this restric-tion even when not injecting any rules, we introducemodel FS: the original model F, but with sigmoidalentity-tuples:LFS =??r,tq?
?Otp?T , ?r,tp?6?O`R(r>[?(ep)?
?
(eq)])+ ?
(?r?r?22 +?e?e?22) (13)Here, ep and eq are the real-valued representationsas in eq.
(12), for tuples tp and tq, respectively.With the above choice of a non-negative tuple-embedding space we can now state the full lifted ruleinjection model (FSL):LFSL = LFS + ??
?I?ILUI (14)LUI denotes a lifted loss term for every rule in a setI of implication rules that we want to inject.3.4 Convex Implication LossThe logistic loss `R (see ?2) is not suited for im-posing implications because once the inequality ineq.
(11) is satisfied, the components of rp and rq donot need to be separated any further.
However, with`R this would continue to happen due to the smallnon-zero gradient.
In the reconstruction loss LRthis is a desirable effect which further separates thescores for positive from negative examples.
How-ever, if an implication is imposed between two re-lations that are almost equivalent according to thetraining data, we still want to find almost equivalentembedding vectors.
Hence, we propose to use theloss`I(s) = max(0, s+ ?)
(15)with ?
a small positive margin to ensure that the gra-dient does not disappear before the inequality is ac-tually satisfied.
We use ?
= 0.01 in all experiments.The main advantage of the presented approachover earlier methods that impose the rules in agrounded way (Rockta?schel et al, 2015; Wang etal., 2015) is the computational efficiency of impos-ing the lifted loss.
Evaluating LUI or its gradient forone implication rule is comparable to evaluating thereconstruction loss for one pair of training facts.
Intypical applications there are much fewer rules thantraining facts and the extra computation time neededto inject these rules is therefore negligible.4 Related WorkRecent research on combining rules with learnedvector representations has been important for newdevelopments in the field of knowledge base com-pletion.
Rockta?schel et al (2014) and Rockta?schelet al (2015) provided a framework to jointly maxi-mize the probability of observed facts and proposi-tionalized first-order logic rules.
Wang et al (2015)demonstrated how different types of rules can beincorporated using an Integer Linear Programmingapproach.
Wang and Cohen (2016) learned em-beddings for facts and first-order logic rules usingmatrix factorization.
Yet, all of these approachesground the rules in the training data, limiting theirscalability towards large rule sets and KBs withmany entities.
As argued in the introduction, thisforms an important motivation for the lifted rule in-jection model put forward in this work, which byconstruction does not suffer from that limitation.Wei et al (2015) proposed an alternative strategy totackle the scalability problem by reasoning on a fil-tered subset of grounded facts.Wu et al (2015) proposed to use a path rankingapproach for capturing long-range interactions be-tween entities, and to add these as an extra loss term,besides the loss that models pairwise relations.
Ourmodel FSL differs substantially from their approach,in that we consider tuples instead of separate enti-ties, and we inject a given set of rules.
Yet, by cre-1392ating a partial ordering in the relation embeddingsas a result of injecting implication rules, model FSLcan also capture interactions beyond direct relations.This will be demonstrated in ?5.3 by injecting rulesbetween surface patterns only and still measuring animprovement on predictions for structured Freebaserelations.Combining logic and distributed representationsis also an active field of research outside of au-tomated knowledge base completion.
Recent ad-vances include the work by Faruqui et al (2014),who injected ontological knowledge from WordNetinto word representations.
Furthermore, Vendrov etal.
(2016) proposed to enforce a partial ordering inan embeddings space of images and phrases.
Ourmethod is related to such order embeddings sincewe define a partial ordering on relation embeddings.However, to ensure that implications hold for allentity-tuples we also need a restriction on the entity-tuple embedding space and derive bounds on theloss.
Another important contribution is the recentwork by Hu et al (2016), who proposed a frame-work for injecting rules into general neural networkarchitectures, by jointly training on the actual targetsand on the rule-regularized predictions provided bya teacher network.
Although quite different at firstsight, their work could offer a way to use our modelin various neural network architectures, by integrat-ing the proposed lifted loss into the teacher network.This paper builds upon our previous workshoppaper (Demeester et al, 2016).
In that work,we tested different tuple embedding transforma-tions in an ad-hoc manner.
We used approxi-mately Boolean representations of relations insteadof entity-tuples, strongly reducing the model?s de-grees of freedom.
We now derive the FSL modelfrom a carefully considered mathematical transfor-mation of the grounded loss.
The FSL model onlyrestricts the tuple embedding space, whereby rela-tion vectors remain real valued.
Furthermore, previ-ous experiments were performed on small-scale ar-tificial datasets, whereas we now test on a real-worldrelation extraction benchmark.Finally, we explicitly discuss the main differ-ences with respect to the strongly related work fromRockta?schel et al (2015).
Their method is more gen-eral, as they cover a wide range of first-order logicrules, whereas we only discuss implications.
Liftedrule injection beyond implications will be studied infuture research contributions.
However, albeit lessgeneral, our model has a number of clear advan-tages:Scalability ?
Our proposed model of lifted ruleinjection scales according to the number of implica-tion rules, instead of the number of rules times thenumber of observed facts for every relation presentin a rule.Generalizability ?
Injected implications willhold even for facts not seen during training, becausetheir validity only depends on the order relation im-posed on the relation representations.
This is notguaranteed when training on rules grounded in train-ing facts by Rockta?schel et al (2015).Training Flexibility ?
Our method can be trainedwith various loss functions, including the rank-basedloss as used in Riedel et al (2013).
This was notpossible for the model of Rockta?schel et al (2015)and already leads to an improved accuracy as seenfrom the zero-shot learning experiment in ?5.2.Independence Assumption ?
In Rockta?schel etal.
(2015) an implication of the form ap ?
aq fortwo ground atoms ap and aq is modeled by the log-ical equivalence ?
(ap ?
?aq), and its probabilityis approximated in terms of the elementary proba-bilities pi(ap) and pi(aq) as 1 ?
pi(ap)(1 ?
pi(aq)).This assumes the independence of the two atoms apand aq, which may not hold in practice.
Our ap-proach does not rely on that assumption and alsoworks for cases of statistical dependence.
For ex-ample, the independence assumption does not holdin the trivial case where the relations rp and rq inthe two atoms are equivalent, whereas in our model,the constraints rp ?
rq and rp ?
rq would simplyreduce to rp = rq.5 Experiments and ResultsWe now present our experimental results.
We startby describing the experimental setup and hyperpa-rameters.
Before turning to the injection of rules,we compare model F with model FS, and show thatrestricting the tuple embedding space has a regu-larization effect, rather than limiting the expressive-ness of the model (?5.1).
We then demonstrate thatmodel FSL is capable of zero-shot learning (?5.2),and show that injecting high-quality WordNet rules1393Test relation # R13-F F FS FSLperson/company 106 0.75 0.73 0.74 0.77location/containedby 73 0.69 0.62 0.70 0.71person/nationality 28 0.19 0.20 0.20 0.21author/works written 27 0.65 0.71 0.69 0.65person/place of birth 21 0.72 0.69 0.72 0.70parent/child 19 0.76 0.77 0.81 0.85person/place of death 19 0.83 0.85 0.83 0.85neighborhood/neighborhood of 11 0.70 0.67 0.63 0.62person/parents 6 0.61 0.53 0.66 0.66company/founders 4 0.77 0.73 0.64 0.67sports team/league 4 0.59 0.44 0.43 0.56team owner/teams owned 2 0.38 0.64 0.64 0.61team/arena stadium 2 0.13 0.13 0.13 0.12film/directed by 2 0.50 0.18 0.17 0.13broadcast/area served 2 0.58 0.83 0.83 1.00structure/architect 2 1.00 1.00 1.00 1.00composer/compositions 2 0.67 0.64 0.51 0.50person/religion 1 1.00 1.00 1.00 1.00film/produced by 1 0.50 1.00 1.00 0.33Weighted MAP 0.67 0.65 0.67 0.69Table 1: Weighted mean average precision for ourreimplementation of the matrix factorization model(F) compared to restricting the entity-pair space (FS)and injecting WordNet rules (FSL).
Model F resultsby Riedel et al (2013) are denoted as R13-F.leads to an improved precision (?5.3).
We proceedwith a visual illustration of the relation embeddingswith and without injected rules (?5.4), provide de-tails on time efficiency of the lifted rule injectionmethod (?5.5), and show that it correctly capturesthe asymmetry of implication rules (?5.6).All models were implemented in Tensor-Flow (Abadi et al, 2015).
We use the hyperparam-eters of Riedel et al (2013), with k = 100 hiddendimensions and a weight of ?
= 0.01 for the L2regularization loss.
We use ADAM (Kingma andBa, 2014) for optimization with an initial learningrate of 0.005 and a mini-batch size of 8192.
Theembeddings are initialized by sampling uniformlyfrom [?0.1, 0.1] and we use ??
= 0.1 for theimplication loss throughout our experiments.5.1 Restricted Embedding SpaceBefore incorporating external commonsense knowl-edge into relation representations, we were curioushow much we lose by restricting the entity-tuplespace to approximately Boolean embeddings.
Weevaluate our models on the New York Times datasetintroduced by Riedel et al (2013).
Surprisingly, wefind that the expressiveness of the model does notsuffer from this strong restriction.
From Table 1 wesee that restricting the tuple-embedding space seemsto perform slightly better (FS) as opposed to a real-valued tuple-embedding space (F), suggesting thatthis restriction has a regularization effect that im-proves generalization.
We also provide the originalresults for model F by Riedel et al (2013) (denotedas R13-F) for comparison.
Due to a different im-plementation and optimization procedure, the resultsfor our model F and R13-F are not identical.Inspecting the top relations for a sampled dimen-sion in the embedding space reveals that the rela-tion space of model FS more closely resembles clus-ters than that of model F (Table 2).
We hypothesizethat this might be caused by approximately Booleanentity-tuple representations in model FS, resulting inattribute-like entity-tuple vectors that capture whichrelation clusters they belong to.5.2 Zero-shot LearningThe zero-shot learning experiment performed inRockta?schel et al (2015) leads to an important find-ing: when injecting implications with right-handsides for Freebase relations for which no or very lim-ited training facts are available, the model should beable to infer the validity of Freebase facts for thoserelations based on rules and correlations betweentextual surface patterns.We inject the same hand-picked relations as usedby Rockta?schel et al (2015), after removing allFreebase training facts.
The lifted rule injection(model FSL) reaches a weighted MAP of 0.35,comparable with 0.38 by the Joint model fromRockta?schel et al (2015) (denoted R15-Joint).
Notethat for this experiment we initialized the Freebaserelations implied by the rules with negative randomvectors (sampled uniformly from [?7.9,?8.1]).
Thereason is that without any negative training facts forthese relations, their components can only go up dueto the implication loss, and we do not want to getvalues that are too high before optimization.Figure 1 shows how the relation extraction perfor-mance improves when more Freebase relation train-ing facts are added.
It effictively measures howwell the proposed models, matrix factorization (F),propositionalized rule injection (R15-Joint), and ourmodel (FSL), can make use of the provided rulesand correlations between textual surface form pat-1394Table 2: Top patterns for a randomly sampled dimension in non-restricted and restricted embedding space .Model F (non-restricted) Model FS (restricted)nsubj<-represent->dobj rcmod->return->prep->to->pobjappos->member->prep->of->pobj->team->nn nn<-return->prep->to->pobjnsubj<-die->dobj nsubj<-return->prep->to->pobjnsubj<-speak->prep->about->pobj rcmod->leave->dobjappos->champion->poss nsubj<-quit->dobj0.0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5Fraction of Freebase Training Facts0.00.10.20.30.40.50.60.7wMAPFigure 1: Weighted MAP for injecting hand-pickedrules as a function of the fraction of Freebase train-ing facts.
Comparison between model F (lowest, inblue), R15-Joint (middle, in green) and model FSL(highest, in red).terns and increased fractions of Freebase trainingfacts.
Although FSL starts at a lower performancethan R15-Joint when no Freebase training facts arepresent, it outperforms R15-Joint and a plain matrixfactorization model by a substantial margin whenprovided with more than 7.5% of Freebase train-ing facts.
This indicates that, in addition to beingmuch faster than R15-Joint, it can make better useof provided rules and few training facts.
We at-tribute this to the Bayesian personalized ranking lossinstead of the logistic loss used in Rockta?schel etal.
(2015).
The former is compatible with our rule-injection method, but not with the approach of max-imizing the expectation of propositional rules usedby R15-Joint.5.3 Injecting Knowledge from WordNetThe main purpose of this work is to be able toincorporate rules from external resources for aid-ing relation extraction.
We use WordNet hyper-nyms to generate rules for the NYT dataset.
Tothis end we iterate over all surface form patternsin the dataset and attempt to replace words inthe pattern by their hypernyms.
If the result-ing pattern is contained in the dataset, we gen-erate the corresponding rule.
For instance, wegenerate a rule appos->diplomat->amod ?appos->official->amod since both patternsare contained in the NYT dataset and we know fromWordNet that a diplomat is an official.
This leads to427 rules from WordNet that we subsequently anno-tate manually to obtain 36 high-quality rules.
Notethat none of these rules directly imply a Freebase re-lation.
Although the test relations all originate fromFreebase, we still hope to see improvements by tran-sitive effects, i.e., better surface form representationsthat in turn help to predict Freebase facts.We show results obtained by injecting theseWordNet rules in Table 1 (column FSL).
Theweighted MAP measure increases by 2% withrespect to model FS, and 4% compared to our reim-plementation of the matrix factorization model F.This demonstrates that imposing a partial orderingbased on implication rules can be used to incorpo-rate logical commonsense knowledge and increasethe quality of information extraction systems.
Notethat our evaluation setting guarantees that onlyindirect effects of the rules are measured, i.e., wedo not use any rules directly implying test relations.This shows that injecting such rules influencesthe relation embedding space beyond only therelations explicitly stated in the rules.
For example,injecting the rule appos<-father->appos?
poss<-parent->appos can contributeto improved predictions for the test relationparent/child.1395(a) (b)Figure 2: Visualization of embeddings (columns) forthe relations that appear in the high-quality Word-Net rules, (a) without and (b) with injection of theserules.
Values range from -1 (orange) via 0 (white) to1 (purple).
Best viewed in color.5.4 Visualizing Relation EmbeddingsWe provide a visual inspection of how the structureof the relation embedding space changes when rulesare imposed.
We select all relations involved in theWordNet rules, and gather them as columns in a sin-gle matrix, sorted by increasing `1 norm (values inthe 100 dimensions are similarly sorted).
Figures 2aand 2b show the difference between model F (with-out injected rules) and FSL (with rules).
The val-ues of the embeddings in model FSL are more po-larized, i.e., we observe stronger negative or posi-tive components than for model F. Furthermore, FSLalso reveals a clearer difference between the left-most (mostly negative, more specific) and right-most(predominantly positive, more general) embeddings(i.e., a clearer separation between positive and nega-tive values in the plot), which results from imposingthe order relation in eq.
(11) when injecting implica-tions.5.5 Efficiency of Lifted Injection of RulesIn order to get an idea of the time efficiency of in-jecting rules, we measure the time per epoch whenrestricting the program execution to a single 2.4GHzCPU core.
We measure on average 6.33s per epochwithout rules (model FS), against 6.76s and 6.97swhen injecting the 36 high-quality WordNet rulesand the unfiltered 427 rules (model FSL), respec-tively.
Increasing the amount of injected rules from36 to 427 leads to an increase of only 3% in compu-tation time, even though in our setup all rule lossesare used in every training batch.
This confirms thehigh efficiency of our lifted rule injection method.5.6 Asymmetric Character of ImplicationsIn order to demonstrate that injecting implicationsconserves their asymmetric nature, we perform thefollowing experiment.
After incorporating high-quality Wordnet rules rp ?
rq into model FSL weselect all of the tuples tp that occur with relation rpin a training fact ?rp, tp?.
Matching these with re-lation rq should result in high values for the scoresr>q tp, if the implication holds.
If however the tuplestq are selected from the training facts ?rq, tq?, andmatched with relation rp, the scores r>p tq shouldbe much lower if the inverse implication does nothold (in other words, if rq and rp are not equiva-lent).
Table 3 lists the averaged results for 5 examplerules, and the average over all relations in WordNetrules, both for the case with injected rules (modelFSL), and without rules (model FS).
For easier com-parison, the scores are mapped to the unit intervalvia the sigmoid function.
This quantity ?
(r>t) isoften interpreted as the probability that the corre-sponding fact holds (Riedel et al, 2013), but be-cause of the BPR-based training, only differencesbetween scores play a role here.
After injectingrules, the average scores of facts inferred by theserules (i.e., column ?
(r>q tp) for model FSL) are al-ways higher than for facts (incorrectly) inferred bythe inverse rules (column ?
(r>p tq) for model FSL).In the fourth example, the inverse rule leads to highscores as well (on average 0.79, vs. 0.98 for the ac-tual rule).
This is due to the fact that the daily andnewspaper relations are more or less equivalent,such that the components of rp are not much belowthose of rq.
For the last example (the ambassador?
diplomat rule), the asymmetry in the implica-tion is maintained, although the absolute scores arerather low for these two relations.The results for model FS reflect how strongly theimplications in either direction are latently presentin the training data.
We can only conclude thatmodel FS manages to capture the similarity be-1396rule model FSL model FSrp ?
rq ?
(r>q tp) ?
(r>p tq) ?
(r>q tp) ?
(r>p tq)appos->party->amod ?
appos->organization->amod 0.99 0.22 0.70 0.86poss<-father->appos ?
poss<-parent->appos 0.96 0.00 0.72 0.89appos->prosecutor->nn ?
appos->lawyer->nn 0.99 0.01 0.87 0.80appos->daily->amod ?
appos->newspaper->amod 0.98 0.79 0.90 0.86appos->ambassador->amod ?
appos->diplomat->amod 0.31 0.05 0.93 0.84average over 36 high-quality Wordnet rules 0.95 0.28 0.74 0.70Table 3: Average of ?
(r>q t) over all inferred facts ?rq, tp?
for tuples tp from training items for relation rp,and vice versa, for Wordnet implications rp ?
rq, and model FSL (injected rules) vs. model FS (no rules).tween relations, but not the asymmetric characterof implications.
For example, purely based on thetraining data, it appears to be more likely that theparent relation implies the father relation, thanvice versa.
This again demonstrates the importanceand added value of injecting external rules capturingcommonsense knowledge.6 ConclusionsWe presented a novel, fast approach for incorporat-ing first-order implication rules into distributed rep-resentations of relations.
We termed our approach?lifted rule injection?, as it avoids the costly ground-ing of first-order implication rules and is thus inde-pendent of the size of the domain of entities.
Byconstruction, these rules are satisfied for any ob-served or unobserved fact.
The presented approachrequires a restriction on the entity-tuple embeddingspace.
However, experiments on a real-world datasetshow that this does not impair the expressiveness ofthe learned representations.
On the contrary, it ap-pears to have a beneficial regularization effect.By incorporating rules generated from WordNethypernyms, our model improved over a matrix fac-torization baseline for knowledge base completion.Especially for domains where annotation is costlyand only small amounts of training facts are avail-able, our approach provides a way to leverage exter-nal knowledge sources for inferring facts.In future work, we want to extend the proposedideas beyond implications towards general first-order logic rules.
We believe that supporting con-junctions, disjunctions and negations would enableto debug and improve representation learning basedknowledge base completion.
Furthermore, we wantto integrate these ideas into neural methods beyondmatrix factorization approaches.AcknowledgmentsThis work was supported by the Research Founda-tion - Flanders (FWO), Ghent University - iMinds,Microsoft Research through its PhD ScholarshipProgramme, an Allen Distinguished InvestigatorAward, and a Marie Curie Career Integration Award.ReferencesMart?
?n Abadi, Ashish Agarwal, Paul Barham, EugeneBrevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,Andy Davis, Jeffrey Dean, Matthieu Devin, SanjayGhemawat, Ian Goodfellow, Andrew Harp, GeoffreyIrving, Michael Isard, Yangqing Jia, Rafal Jozefow-icz, Lukasz Kaiser, Manjunath Kudlur, Josh Leven-berg, Dan Mane?, Rajat Monga, Sherry Moore, DerekMurray, Chris Olah, Mike Schuster, Jonathon Shlens,Benoit Steiner, Ilya Sutskever, Kunal Talwar, PaulTucker, Vincent Vanhoucke, Vijay Vasudevan, Fer-nanda Vie?gas, Oriol Vinyals, Pete Warden, MartinWattenberg, Martin Wicke, Yuan Yu, and XiaoqiangZheng.
2015.
TensorFlow: Large-scale machinelearning on heterogeneous systems.
Software avail-able from tensorflow.org.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuring hu-man knowledge.
In Proceedings of the 2008 ACMSIGMOD international conference on Management ofdata, pages 1247?1250.
ACM.Samuel R Bowman, Christopher Potts, and Christopher DManning.
2015.
Recursive neural networks can learnlogical semantics.
In Proceedings of the 3rd Workshopon Continuous Vector Space Models and their Compo-sitionality (CVSC).Rodrigo De Salvo Braz.
2007.
Lifted First-order Proba-bilistic Inference.
Ph.D. thesis, Champaign, IL, USA.AAI3290183.Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and Christo-pher Meek.
2014.
Typed tensor decomposition of1397knowledge bases for relation extraction.
In EMNLP,pages 1568?1579.William.
W. Cohen.
2016.
TensorLog: A DifferentiableDeductive Database.
ArXiv e-prints, May.Thomas Demeester, Tim Rockta?schel, and SebastianRiedel.
2016.
Regularizing relation representationsby first-order implications.
In NAACL Workshop onAutomated Knowledge Base Construction (AKBC).Manaal Faruqui, Jesse Dodge, Sujay K Jauhar, ChrisDyer, Eduard Hovy, and Noah A Smith.
2014.Retrofitting word vectors to semantic lexicons.
arXivpreprint arXiv:1411.4166.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
Ppdb: The paraphrasedatabase.
In Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics (HLT-NAACL), pages 758?764.Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, EduardHovy, and Eric Xing.
2016.
Harnessing deepneural networks with logic rules.
arXiv preprintarXiv:1603.06318.Diederik Kingma and Jimmy Ba.
2014.
Adam: Amethod for stochastic optimization.
arXiv preprintarXiv:1412.6980.German Kruszewski, Denis Paperno, and Marco Baroni.2015.
Deriving boolean structures from distributionalvectors.
Transactions of the Association for Computa-tional Linguistics, 3:375?388.George A Miller.
1995.
Wordnet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41.Arvind Neelakantan, Benjamin Roth, and Andrew Mc-Callum.
2015.
Compositional vector space mod-els for knowledge base completion.
arXiv preprintarXiv:1504.06662.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2012.
Factorizing yago: scalable machinelearning for linked data.
In Proceedings of the 21stinternational conference on World Wide Web, pages271?280.
ACM.Maximilian Nickel, Kevin Murphy, Volker Tresp, andEvgeniy Gabrilovich.
2015.
A review of relationalmachine learning for knowledge graphs: From multi-relational link prediction to automated knowledgegraph construction.
arXiv preprint arXiv:1503.00759.David Poole.
2003.
First-order probabilistic inference.In Proceedings of the 18th International Joint Confer-ence on Artificial Intelligence (IJCAI), pages 985?991,San Francisco, CA, USA.
Morgan Kaufmann Publish-ers Inc.Steffen Rendle, Christoph Freudenthaler, Zeno Gantner,and Lars Schmidt-Thieme.
2009.
BPR: Bayesian per-sonalized ranking from implicit feedback.
In Proceed-ings of the Twenty-Fifth Conference on Uncertainty inArtificial Intelligence (UAI), pages 452?461, Arling-ton, Virginia, United States.
AUAI Press.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M Marlin.
2013.
Relation extraction withmatrix factorization and universal schemas.
In An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics (HLT-NAACL), pages 74?84.Tim Rockta?schel and Sebastian Riedel.
2016.
Learn-ing knowledge base inference with neural theoremprovers.
In NAACL Workshop on Automated Knowl-edge Base Construction (AKBC).Tim Rockta?schel, Matko Bosnjak, Sameer Singh, and Se-bastian Riedel.
2014.
Low-dimensional embeddingsof logic.
In ACL Workshop on Semantic Parsing.Tim Rockta?schel, Sameer Singh, and Sebastian Riedel.2015.
Injecting Logical Background Knowledge intoEmbeddings for Relation Extraction.
In Annual Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics (HLT-NAACL).Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Ng.
2013.
Reasoning with neural ten-sor networks for knowledge base completion.
InAdvances in Neural Information Processing Systems(NIPS).Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-fung Poon, Pallavi Choudhury, and Michael Gamon.2015.
Representing text for joint embedding of textand knowledge bases.
In EMNLP.Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Ur-tasun.
2016.
Order-embeddings of images and lan-guage.
arXiv preprint, abs/1511.06361.Patrick Verga and Andrew McCallum.
2016.
Row-lessuniversal schema.
In NAACL Workshop on AutomatedKnowledge Base Construction (AKBC).Patrick Verga, David Belanger, Emma Strubell, Ben-jamin Roth, and Andrew McCallum.
2016.
Multilin-gual relation extraction using compositional universalschema.
In Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics (HLT-NAACL), pages 886?896.
ACL.William Yang Wang and William W. Cohen.
2016.Learning first-order logic embeddings via matrix fac-torization.
In Proceedings of the 25th InternationalJoint Conference on Artificial Intelligence (IJCAI2015), New York, NY, July.
AAAI.William Yang Wang, Kathryn Mazaitis, and William WCohen.
2014.
Structure learning via parameter learn-ing.
In Proceedings of the 23rd ACM InternationalConference on Conference on Information and Knowl-edge Management, pages 1199?1208.
ACM.Quan Wang, Bin Wang, and Li Guo.
2015.
Knowledgebase completion using embeddings and rules.
In Pro-1398ceedings of the 24th International Conference on Ar-tificial Intelligence (IJCAI), pages 1859?1865.
AAAIPress.Zhuoyu Wei, Jun Zhao, Kang Liu, Zhenyu Qi, ZhengyaSun, and Guanhua Tian.
2015.
Large-scale knowl-edge base completion: Inferring via grounding net-work sampling over selected instances.
In Proceed-ings of the 24th ACM International on Conferenceon Information and Knowledge Management (CIKM),pages 1331?1340.
ACM.Fei Wu, Jun Song, Yi Yang, Xi Li, Zhongfei Zhang,and Yueting Zhuang.
2015.
Structured embeddingvia pairwise relations and long-range interactions inknowledge base.
In AAAI Conference on Artificial In-telligence, pages 1663?1670.1399
