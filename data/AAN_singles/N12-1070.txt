2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 582?586,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsMultimodal Grammar ImplementationKatya AlahverdzhievaUniversity of Edinburgh10 Crichton StreetEdinburgh EH8 9ABK.Alahverdzhieva@sms.ed.ac.ukDan FlickingerStanford UniversityStanford, CA 94305-2150danf@stanford.eduAlex LascaridesUniversity of Edinburgh10 Crichton StreetEdinburgh EH8 9ABalex@inf.ed.ac.ukAbstractThis paper reports on an implementation of amultimodal grammar of speech and co-speechgesture within the LKB/PET grammar engi-neering environment.
The implementation ex-tends the English Resource Grammar (ERG,Flickinger (2000)) with HPSG types and rulesthat capture the form of the linguistic signal,the form of the gestural signal and their rel-ative timing to constrain the meaning of themultimodal action.
The grammar yields a sin-gle parse tree that integrates the spoken andgestural modality thereby drawing on stan-dard semantic composition techniques to de-rive the multimodal meaning representation.Using the current machinery, the main chal-lenge for the grammar engineer is the non-linear input: the modalities can overlap tem-porally.
We capture this by identical speechand gesture token edges.
Further, the semanticcontribution of gestures is encoded by lexicalrules transforming a speech phrase into a mul-timodal entity of conjoined spoken and gestu-ral semantics.1 IntroductionOur aim is to regiment the form-meaning mappingof multimodal actions consisting of speech and co-speech gestures.
The language of study is English,and the gestures of interest are depicting?the handdepicts the referent?and deictic?the hand pointsat the referent?s spatial coordinates.Motivation for encoding the form-meaning map-ping in the grammar stems from the fact that formeffects judgments of multimodal grammaticality:e.g., in (1)1 the gesture performance along with1The speech item where the gesture is performed is markedby underlining, and the accented item is given in uppercase.the unaccented ?called?
in a single prosodic phraseseems ill-formed despite the gesture depicting an as-pect of the referent?the act of calling.
(1) * Your MOTHER called .
.
.Hand lifts to the ear to imitate holding a receiver.This intuitive judgment is in line with the em-pirical findings of Giorgolo and Verstraten (2008)who observed that prosody influences the perceptionof temporally misaligned speech-and-gesture sig-nals as ill-formed.
Further, Alahverdzhieva and Las-carides (2010) established empirically that the ges-ture performance can be predicted from the prosodicprominence in speech and that gestures not overlap-ping subject NPs cannot be semantically related withthat subject NP.
The fact that speech-and-gesture in-tegration is informed by the form of the linguisticsignal suggests formalising the integration withinthe grammar.
Alternatively, integrating the gestu-ral contribution by discourse update would involvepragmatic reasoning accessing information aboutlinguistic form, disrupting the transition betweensyntax/semantics and pragmatics.The work is set within HPSG ?
a constraint-basedgrammar framework with the different types andrules organised in a hierarchy.
The semantic infor-mation, derived in parallel with syntax, is expressedin Minimal Recursion Semantics (MRS) which sup-ports a high level of underspecifiability (Copestakeet al, 2005).
This is useful for computing gesturemeaning since even through discourse processingnot all semantic information resolves to a specificinterpretation.The rest of the paper is structured as follows: ?2provides theoretical background, ?3 details the im-plementation and ?4 discusses the evaluation.5822 Background2.1 Attachment AmbiguityWe view the integration of gesture and the syn-chronous, semantically related speech phrase as anattachment in a single parse tree constrained by theform of the speech signal?its prosodic prominence.With standard methods for semantic composition,we map this multimodal tree to an UnderspecifiedLogical Form (ULF) which supports the possible in-terpretations of the speech and gesture in their con-text.
The choices of attachment are not unique.
Sim-ilarly to ?John saw the man with the telescope?,there is ambiguity as to which linguistic phrase agesture is semantically related to, and hence likewiseambiguity as to which linguistic phrase it attaches toin syntax; e.g., in (2) the open vertical hand shapecan denote a container containing books or a con-tainee of books.
This interpretation is supported bya gesture attachment to the N ?books?.
A higherattachment to the root node of the tree supports an-other, metaphoric interpretation where the forwardmovement is the conduit metaphor of giving.
(2) I can give you other BOOKS .
.
.Hands are parallel with palms open vertical.
Theyperform a short forward move to the frontal centre.We address this ambiguity by grammar rulesthat allow for multiple attachments in the syntactictree constrained by the prosodic prominence of thespeech signal.
The two basic rules are as follows:1.
Prosodic Word Constraint.
Gesture can at-tach to a prosodically prominent spoken wordif there is an overlap between the timing of thegesture and the timing of the speech word.2.
Head-Argument Constraint.
Gesture can at-tach to a syntactic head partially or fully sat-urated with its arguments and/or modifiers ifthere is a temporal overlap between the syntac-tic constituent and the gesture.Applied to (2), these rules would attach the ges-ture to ?books?
(a prosodically prominent item),also to ?other books?, ?give you other books?, ?cangive you other books?
and even to ?I can give youother books?
(heads saturated with their arguments).However, nothing licenses attachments to ?I?
or?give?.
These distinct attachments would supportthe interpretations proposed above.2.2 Representing Gesture Form and MeaningIt is now commonplace to represent gesture formwith Typed Feature Structures (TFS) where each fea-ture captures an aspect of the gesture?s meaning;e.g., the gesture in (2) maps to the TFS in (3).
Notethat the TFS is typed as depicting so as to differen-tiate between, say, a hand shape of depicting ges-ture and a hand shape of deixis.
This distinction ef-fects the gestural interpretation: a depicting gestureprovides non-spatial aspects of the referent?s deno-tation, and so form bears resemblance to meaning.Conversely, deixis identifies the spatial coordinatesof the referent in the physical space.(3)??????
?depictingHAND-SHAPE open-flatPALM-ORIENT towards-centreFINGER-ORIENT away-bodyHAND-LOCATION centre-lowHAND-MOVEMENT away-body-straight??????
?Each feature introduces an underspecified ele-mentary predication (EP) into LF; e.g., the handshape introduces l1 : hand shape open flat(i1)where l1 is a unique label that underspecifies thescope of the EP relative to other EPs in the ges-ture?s LF, i1 is a unique metavariable that under-specifies the main argument?
sort (e.g., in (2) it canresolve to an individual if the gesture denotes thebooks or an event if it denotes the giving act) andhand shape open flat underspecifies reference toa property that the entity i1 has and that can be de-picted through the gesture?s open flat hand shape.In the grammar, we introduce underspecified se-mantic relations vis rel(s,g) between speech s anddepicting gesture g, and deictic rel(s,d) betweenspeech s and deixis d. The resolution of these un-derspecified predicates is a matter of commonsensereasoning (Lascarides and Stone, 2009) and it there-fore lies outside the scope of the grammar.3 ImplementationThe grammar was implemented in the LKB grammarengineering platform (Copestake, 2002) which wasdesigned for TFS grammars such as HPSG.
Sincethe LKB parser accepts as input linearly orderedstrings and we represent gesture form with TFSs,we used the PET engine (Callmeier, 2000) which al-lows for injecting an arbitrary XML-based FS into583the input tokens.
The input to our grammar is a lat-tice of FSs where the spoken tokens are augmentedwith prosodic information and the gesture tokens arefeature-value pairs such as (3).The main challenge for the multimodal grammarimplementation stems from the non-linear multi-modal input.
The HPSG-based parsing platforms?LKB, PET and TRALE?can parse linearly orderedstrings, and so they do not handle multimodal sig-nals whose input comes from separate channels con-nected through temporal relations.
Also, these pars-ing platforms do not support quantitative compari-son operations over the time stamps of the input to-kens.
This is essential for our grammar since themultimodal integration is constrained by temporaloverlap between speech and gesture (recall ?2.1).To solve this, we pre-processed the XML-basedFS input so that overlapping TIME START andTIME END values were ?translated?
into identicalstart and end edges of the speech token and the ges-ture token as follows:<edge source="v0" target="v1"><fs type="speech_token"><edge source="v0" target="v1"><fs type="gesture_token">This robust pre-processing step is sufficient sincethe only temporal relation required by the grammaris overlap, an abstraction over more fined-grainedrelations between speech (S) and gesture (G) suchas (precedence(start(S), start(G)) ?
identity (end(S),end(G))).The linking of gesture to its temporally over-lapping speech segment happens prior to parsingvia chart-mapping rules (Adolphs et al, 2008)which involve re-writing chart items into FSs.
Thegesture-unary-rule (see Fig.1) rewrites an in-put (I) speech token in the context (C) of a gesturetoken into a combined speech+gesture token wherethe +GEST and +PROS values of the speech and ges-ture tokens are copied onto the output (O).gesture-unary-rule := cm_rule &[+CONTEXT <gesture_token & [+GEST #gest]>,+INPUT <speech_token & [+PROS #pros]>,+OUTPUT <speech+gesture_token &[+GEST #gest, +PROS #pros]>,+POSITION "O1@I1, I1@C1" ].Figure 1: Definition of gesture-unary-ruleThe +PROS attribute contains prosodic informa-tion and the +GEST attribute is a feature-structurerepresentation as shown in (3).
The +POSITION con-straint restricts the position of the I, O and C items toan overlap (@), i.e., the edge markers of the gesturetoken should be identical to those of the speech to-ken, and also identical to the speech+gesture token.This chart-mapping rule recognises the gesture to-ken overlapping the speech token and it records thisby ?augmenting?
the speech token with the gesturefeature-values.In the grammar, we extended the ERG word andphrase rules with prosodic and gestural informationwhere the +PROS and +GEST features of the inputtoken are identified with the PROS and GEST of theword and/or lexical phrase in the grammar.
We thenadded a lexical rule (see Fig.
2) which projects a ges-ture daughter to a complex gesture-marked entity ofa single argument for which both the PROS and GESTfeatures are appropriate.gesture_lexrule := phrase_or_lexrule &[ ORTH [ PROS #pros ],ARGS <[ ORTH [ GEST gesture-form,PROS p-word & #pros ]]>].Figure 2: Definition of gesture lexruleThis rule constrains PROS to a prosodically promi-nent word of type p-word thereby preventing a ges-ture from plugging into a prosodically unmarkedword.
The gesture-form value is a supertype over thedistinct gesture types?depicting and deictic.
Thegesture lexrule is inherited by a lexical rulespecific to depicting gestures, and by a lexical rulespecific to deictic gestures.
In this way, we can en-code the semantic contribution of depicting gestureswhich is different from the semantic contribution ofdeixis.
For the sake of space, Fig.
3 presents only thedepicting lexrule.
The semantic informationcontributed by the rule is encoded within C-CONT.Following ?2.2, the rule introduces an underspec-ified vis rel between the main label #dltop of thespoken sign (via the HCONS constraints) and themain label #glbl of the gesture semantics (via theHCONS constraints).
Note that these two argumentsare in a geq (greater or equal) constraint.
This meansthat vis rel can operate over any projection of thespeech word; e.g., attaching the gesture to ?book?
in(2) means that the relation is not restricted to the EPscontributed by ?books?
but it can be also over theEPs of a higher projection.
The gesture?s semanticsis a bag of EPs (see ?2.2), all of which are outscoped584?gesture/12-04-02/pet?
Coverage Profiletotal positive word lexical distinct total overallAggregate items items string items analyses results coverage] ] ?
?
? ]
%90 ?
i-length < 95 126 92 93.00 26.46 1.67 92 100.070 ?
i-length < 75 78 54 71.00 12.00 1.00 54 100.060 ?
i-length < 65 249 179 60.00 9.42 1.00 179 100.045 ?
i-length < 50 18 14 49.00 7.00 1.00 14 100.0Total 471 339 70.25 14.35 1.18 339 100.0Table 1: Coverage Profile of Test Items generated by [incr tsdb()]depicting_lexrule := gesture_lexrule &[ARGS <[ SYNSEM.LOCAL.CONT.HOOK.LTOP#dltop,ORTH [ GEST depicting] >,C-CONT [ RELS <!
[ PRED vis_rel,S-ARG #arg1,G-ARG #arg2 ],[ PRED G_mod,LBL #glbl,ARG1 #harg ],[ LBL #larg1 ],...!>,HCONS <!geq&[ HARG #arg1,LARG #dltop ],qeq&[ HARG #arg2,LARG #glbl ],qeq&[ HARG #harg,LARG #larg1 ],...!>]].Figure 3: Definition of depicting lexruleby the gestural modality [G].
The rule therefore in-troduces in RELS a label (here #larg1) for an EPwhich is in qeq constraints with [G].
The instanti-ation of the particular EPs comes from the gesturallexical entry.
In the real implementation, the num-ber of these labels corresponds to the number of fea-tures.
They are designed in the same way and wethus forego any details about the rest.4 EvaluationThe evaluation was performed against a test suitedesigned in analogy to the traditional phenomenon-based test-suites (Lehmann et al, 1996): manually-crafted to ensure coverage of well-formed and ill-formed data, but inspired by an examination of natu-ral data.
We systematically tested syntactic phenom-ena (intransitivity, transitivity, complex NPs, coordi-nation, negation and modification) over well-formedand ill-formed examples where the ill-formed itemswere derived by means of the following operations:prosodic permutation (varying the prosodic marked-ness, e.g., from (4a) we derive (4b) to reflect in-tuitions of native speakers); gesture variation (test-ing distinct gesture types) and temporal permutation(moving the gestural performance over the distinctspeech items).
(4) a. ANNA ate .
.
.Depicting gesture along with ?Anna?.b.
*anna ATE .
.
.Depicting gesture along with ?Anna?.The test set contained 471 multimodal items (72%well-formed) covering the full range of prosodic(prosodic markedness and unmarkedness) and ges-ture (the span of depicting/deictic gesture and itstemporal relation to the prosodically marked ele-ments) permutations.
The gestural vocabulary waslimited since a larger gesture lexicon has no effectson the performance.
To test the grammar, we usedthe [incr tsdb()]2 competence and performance toolwhich enables batch processing of test items andwhich creates a coverage profile of the test set (seeTable 1).
The values are as follows: the left col-umn separates the items per aggregation criterion(the length of test items); the next column shows thenumber of test items per aggregate; then we havethe number of grammatical items; average length oftest item; average number of lexical items; averagenumber of distinct analyses and total coverage.5 Conclusions and Future WorkThis paper reported on an implementation of a mul-timodal grammar combining spoken and gestural in-put.
The main challenge for the current parsingplatforms was the non-linear input which we solvedby extending the spoken sign with the synchronousgestural sign semantics where synchrony was estab-lished by means of identical token edges.
In the fu-ture, we shall extend the lexical coverage so that thegrammar can handle various gestures and we alsointend to evaluate the grammar with naturally occur-ring examples in XML format.2http://www.delph-in.net/itsdb/585ReferencesPeter Adolphs, Stephan Oepen, Ulrich Callmeier,Berthold Crysmann, Daniel Flickinger, and BerndKiefer.
2008.
Some fine points of hybrid natural lan-guage parsing.
In Proceedings of the Sixth Interna-tional Language Resources and Evaluation.
ELRA.Katya Alahverdzhieva and Alex Lascarides.
2010.Analysing speech and co-speech gesture in constraint-based grammars.
In Stefan Mu?ller, editor, The Pro-ceedings of the 17th International Conference onHead-Driven Phrase Structure Grammar, pages 6?26,Stanford.
CSLI Publications.Ulrich Callmeier.
2000.
PET ?
A platform for experi-mentation with efficient HPSG processing techniques.Natural Language Engineering, 6 (1) (Special Issue onEfficient Processing with HPSG):99 ?
108.Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pol-lard.
2005.
Minimal recursion semantics: An intro-duction.
Journal of Research on Language and Com-putation, 3(2?3):281?332.Ann Copestake.
2002.
Implementing Typed FeatureStructure Grammars.
CSLI Publications, Stanford,CA.Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural Language En-gineering.Gianluca Giorgolo and Frans Verstraten.
2008.
Per-ception of speech-and-gesture integration.
In Pro-ceedings of the International Conference on Auditory-Visual Speech Processing 2008, pages 31?36.Alex Lascarides and Matthew Stone.
2009.
A formalsemantic analysis of gesture.
Journal of Semantics.Sabine Lehmann, Stephan Oepen, Sylvie Regnier-Prost,Klaus Netter, Veronika Lux, Judith Klein, KirstenFalkedal, Frederik Fouvry, Dominique Estival, EvaDauphin, Herve Compagnion, Judith Baur, LornaBalkan, and Doug Arnold.
1996.
Tsnlp - test suitesfor natural language processing.
In COLING, pages711?716.586
