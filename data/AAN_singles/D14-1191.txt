Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1804?1809,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsDetecting Latent Ideology in Expert Text: Evidence From AcademicPapers in EconomicsZubin Jelveh1, Bruce Kogut2, and Suresh Naidu31Dept.
of Computer Science & Engineering, New York University2Columbia Business School and Dept.
of Sociology, Columbia University3Dept.
of Economics and SIPA, Columbia Universityzj292@nyu.edu, bruce.kogut@columbia.edu, sn2430@columbia.eduAbstractPrevious work on extracting ideologyfrom text has focused on domains whereexpression of political views is expected,but it?s unclear if current technology canwork in domains where displays of ide-ology are considered inappropriate.
Wepresent a supervised ensemble n-grammodel for ideology extraction with topicadjustments and apply it to one such do-main: research papers written by academiceconomists.
We show economists?
polit-ical leanings can be correctly predicted,that our predictions generalize to new do-mains, and that they correlate with publicpolicy-relevant research findings.
We alsopresent evidence that unsupervised modelscan under-perform in domains where ide-ological expression is discouraged.1 IntroductionRecent advances in text mining demonstrate thatpolitical ideology can be predicted from text ?often with great accuracy.
Standard experimen-tal settings in this literature are ones where ide-ology is explicit, such as speeches by Americanpoliticians or editorials by Israeli and Palestinianauthors.
An open question is whether ideologycan be detected in arenas where it is strongly dis-couraged.
A further consideration for applied re-searchers is whether these tools can offer insightinto questions of import for policymakers.
To ad-dress both of these issues, we examine one suchdomain that is both policy-relevant and where ide-ology is not overtly expressed: research paperswritten by academic economists.Why economics?
Economic ideas are importantfor shaping policy by influencing the public debateand setting the range of expert opinion on variouspolicy options (Rodrik, 2014).
Economics alsoviews itself as a science (Chetty, 2013) carefullyapplying rigorous methodologies and using insti-tutionalized safe-guards such as peer review.
Thefield?s most prominent research organization ex-plicitly prohibits researchers from making policyrecommendations in papers that it releases (Na-tional Bureau of Economic Research, 2010).
De-spite these measures, economics?
close proximityto public policy decisions have led many to see itas being driven by ideology (A.S., 2010).
Doesthis view of partisan economics have any empiri-cal basis?To answer the question of whether economicsis politicized or neutral, we present a supervisedensemble n-gram model of ideology extractionwith topic adjustments.1Our methodology is mostclosely related to Taddy (2013) and Gentzkow andShapiro (2010), the latter of which used ?2teststo find phrases most associated with ideology asproxied by the language of U.S. Congresspersons.We improve on this methodology by accountingfor ideological word choice within topics and in-corporating an ensemble approach that increasespredictive accuracy.
We also motivate the need toadjust for topics even if doing so does not improveaccuracy (although it does in this case).
We furtherprovide evidence that fully unsupervised methods(Mei et al., 2007; Lin et al., 2008; Ahmed andXing, 2010; Paul and Girju, 2010; Eisenstein etal., 2011; Wang et al., 2012) may encounter dif-ficulties learning latent ideological aspects whenthose aspects are not first order in the data.Our algorithm is able to correctly predict theideology of 69.2% of economists in our datapurely from their academic output.
We also showthat our predictions generalize and are predictorsof responses by a panel of top economists on is-sues of economic importance.
In a companionpaper (Jelveh et al., 2014), we further show that1Grimmer and Stewart (2013) provide an overview ofmodels used for ideology detection.1804predicted ideologies are significantly correlatedto economists?
research findings.
The latter re-sult shows the relevance and applicability of thesetools beyond the task of ideology extraction.2 DataLinking Economists to Their Political Activity:We obtain the member directory of the Ameri-can Economics Association (AEA) and link it totwo datasets: economists?
political campaign con-tributions and petition signing activities.
We ob-tain campaign contribution data from the FederalElection Commission?s website and petition sign-ing data from Hedengren et al.
(2010).
From thisdata, we construct a binary variable to indicate theground-truth ideologies of economists.
See ourcompanion paper (Jelveh et al., 2014) for furtherdetails on the construction of this dataset.
Re-vealed ideology through contributions and peti-tions is largely consistent.
Of 441 economistsappearing in both datasets, 83.4% showed agree-ment between contributions and petitions.
Forthe final dataset of ground-truth authors we in-clude all economists with campaign contribu-tions and/or petition signatures, however, we dropthose economists whose ideologies where differ-ent across the contribution and petition datasets.Overall, 60% of 2,204 economists with imputedideologies in this final dataset are left-leaningwhile 40% lean rightwards.Economic Papers Corpus: To create our cor-pus of academic writings by economists, we col-lect 17,503 working papers from NBER?s websitecovering June 1973 to October 2011.
We also ob-tained from JSTOR the fulltext of 62,888 researcharticles published in 93 journals in economics forthe years 1991 to 2008.
Combining the set ofeconomists and papers leaves us with 2,171 au-thors with ground truth ideology and 17,870 pa-pers they wrote.
From the text of these papers wecreate n-grams of length two through eight.
Whilen-grams greater than three words in length are un-common, Margolin et al.
(2013) demonstrate thatideological word choice can be detected by longerphrases.
To capture other expressions of ideol-ogy not revealed in adjacent terms, we also in-clude skipgrams of length two by combining non-adjacent terms that are three to five words apart.We remove phrases used by fewer than five au-thors.Topic Adjustments: Table 1 presents the top20 most conservative and liberal bigrams rankedby ?2scores from a Pearson?s test of indepen-dence between phrase usage by left- and right-leaning economists.
It appears that top ideo-logical phrases are related to specific researchsubfields.
For example, right-leaning terms?free bank?, ?stock return?, and ?feder reserv?
arerelated to finance and left-leaning terms ?men-tal health?, ?child care?, and ?birth weight?
are re-lated to health care.
This observation leads us toask: Are apparently ideological phrases merelya by-product of an economist?s research interestrather than reflective of true ideology?To see why this is a critical question, considerthat ideology has both direct and indirect effectson word choice, the former of which is what wewish to capture.
The indirect pathway is throughtopic: ideology may influence the research areaan economist enters into, but not the word choicewithin that area.
In that case, if more conserva-tive economists choose macroeconomics, the ob-served correlation between macro-related phrasesand right-leaning ideology would be spurious.
Theimplication is that accounting for topics may notnecessarily improve performance but provide evi-dence to support an underlying model of how ide-ology affects word choice.
Therefore, to bettercapture the direct effect of ideology on phrase us-age we adjust our predictions by topic by creatingmappings from papers to topics.
For a topic map-ping, we predict economists?
ideologies from theirword choice within each topic and combine theseresults to form and overall prediction.
We com-pare different supervised and unsupervised topicmappings and assess their predictive ability.To create supervised topic mappings, we takeadvantage of the fact that economics papers aremanually categorized by the Journal of EconomicLiterature (JEL).
These codes are hierarchical in-dicators of an article?s subject area.
For exam-ple, the code C51 can be read, in increasing orderof specificity, as Mathematical and QuantitativeMethods (C), Econometric Modeling (C5), ModelConstruction and Estimation (C51).
We constructtwo sets of topic mappings: JEL1 derived fromthe 1st-level codes (e.g.
C) and JEL2 derived fromthe 2nd-level codes (e.g.
C5).
The former cov-ers broad areas (e.g.
macroeconomics, microeco-nomics, etc.)
while the latter contains more refinedones (e.g.
monetary policy, firm behavior, etc.
).For unsupervised mappings, we run Latent1805Left-Leaning Bigrams Right-Leaning Bigramsmental health public choicpost keynesian stock returnchild care feder reservlabor market yes yeshealth care market valuwork time journal financikeynesian econom bank notehigh school money supplipolici analys free bankanalys politiqu liquid effectpolitiqu vol journal financbirth weight median voterlabor forc law economjournal post vote sharelatin america war spendmental ill journal lawmedic care money demandlabour market gold reservsocial capit anna jsingl mother switch costTable 1: Top 20 bigrams and trigrams.Dirichilet Allocation (Blei et al., 2003) on our cor-pus.
We use 30, 50, and 100 topics to createLDA30, LDA50, and LDA100 topic mappings.We use the topic distributions estimated by LDAto assign articles to topics.
A paper p is assignedto a topic t if the probability that t appears in pis greater than 5%.
While 5% might seem to be alower threshold, the topic distributions estimatedby LDA tend to be sparse.
For example, even with50 topics to ?choose?
from in LDA50 and a thresh-old of 5%, 99.5% of the papers would be assignedto five or fewer topics.
This compares favorablywith JEL2 codings where 98.8% of papers havefive or fewer topics.3 AlgorithmThere are two components to our topic-adjustedalgorithm for ideology prediction.
First, we focuson n-grams and skipgrams that are most correlatedwith ideology in the training data.
For each topicwithin a topic mapping, we count the total num-ber of times each phrase is used by all left- andall right-leaning economists.
Then, we computePearson?s ?2statistic and associated p-values andkeep phrases with p ?
0.05.
As an additional fil-ter, we split the data into ten folds and perform the?2test within each fold.
For each topic, we keepphrases that are consistently ideological across allfolds.
This greatly reduces the number of ideo-logical phrases.
For LDA50, the mean number ofideological phrases per topic before the cross val-idation filter is 12,932 but falls to 963 afterwards.With the list of ideological phrases in hand, thesecond step is to iterate over each topic and predictthe ideologies of economists in our test set.
Tocompute the predictions we perform partial leastsquares (PLS): With our training data, we con-struct the standardized frequency matrix Ft,trainwhere the (e, p)-th entry is the number of timeseconomist e used partisan phrase p across all ofe?s papers in t. This number is divided by the totalnumber of phrases used by e in topic t. For paperswith multiple authors, each author gets same countof phrases.
About 5% of the papers in our datasetare written by authors with differing ideologies.We do not treat these differently.
Columns ofFt,trainare standardized to have unit variance.
Lety be the vector of ground-truth ideologies, test setideologies are predicted as follows:1) Compute w = Corr(Ft,train,y), the corre-lations between each phrase and ideology2) Project to one dimension: z = Ft,trainw3) Regress ideology, y, on the constructed vari-able z: y = b1z4) Predict ideology y?eof new economist byy?e= b1?fe?w, (?feis scaled frequency vector)To avoid over-fitting we introduce an ensembleelement: For each t, we sample from the list ofsignificant n-grams in t and sample with replace-ment from the authors who have written in t.2PLSis performed on this sample data 125 times.
EachPLS iteration can be viewed as a vote on whetheran author is left- or right-leaning.
We calculatethe vote as follows.
For each iteration, we pre-dict the ideologies of economists in the trainingdata.
We find the threshold f that minimizes thedistance between the true and false positive ratesfor the current iteration and the same rates for theperfect classifier: 1.0 and 0.0, respectively.
Then,an author in the test set is voted left-leaning ifyt,test?
f and right-leaning otherwise.For a given topic mapping, our algorithm re-turns a three-dimensional array with the (e, t, c)-thentry representing the number of votes economiste received in topic t for ideology c (left- or right-2The number of phrases sampled each iteration is thesquare root of the number of ideological phrases in the topic.1806leaning).
To produce a final prediction, we sumacross the second dimension and compute ideol-ogy as the percentage of right-leaning votes re-ceived across all topics within a topic-mapping.Therefore, ideology values closer to zero are as-sociated with a left-leaning ideology and valuescloser to one are associated with a rightward lean.To recap, we start with a topic mapping and thenfor each topic run an ensemble algorithm with PLSat its core.3The output for each topic is a set ofvotes.
We sum across topics to compute a finalprediction for ideology.4 Validation and ResultsWe split our ground-truth set of 2,171 authorsinto training (80%) and test sets (20%) and com-pute predictions as above.
As our data exhibitsskew with 1.5 left-leaning for every right-leaningeconomist, we report the area under the curve(AUC) which is robust to class skew (Fawcett,2006).
It?s worth noting that a classifier that ran-domly predicts a liberal economist 60% of thetime would have an AUC of 0.5.
To compareour model with fully unsupervised methods, wealso include results from running the Topic-AspectModel (TAM) (Paul and Girju, 2010) on our data.TAM decomposes documents into two compo-nents: one affecting topics and one affecting a la-tent aspect that influences all topics in a similarmanner.
We run TAM with 30 topics and 2 aspects(TAM2/30).
We follow Paul and Girju and use thelearned topic and aspect distributions as trainingdata for a SVM.4Columns 2 to 4 from Table 2 show that ourmodels?
predictions have a clear association withground-truth ideology.
The LDA topic mappingsoutperform the supervised mappings as well as amodel that does not adjust for topics (NoTopic).Perhaps not surprisingly, TAM does not performwell in our domain.
A drawback of unsupervisedmethods is that the learned aspects may not be re-lated to ideology but some other hidden factor.For further insight into how well our modelgeneralizes, we use data from Gordon and Dahl(2013) to compare our predictions to potentiallyideological responses of economists on a survey3Other predictions algorithms could be dropped in forPLS.
Logistic regression and SVM produced similar results.4Authors are treated as documents.
TAM is run for 1,000iterations with the following priors: ?
= 1.0, ?
= 1.0, ?0=1, ?1= 1, ?0= 20, ?1= 80.
(1) (2) (3) (4)TopicMapAccu-racy(%)Corr.
w/TruthAUCLDA50 69.2 0.381 0.719LDA100 66.3 0.364 0.707LDA30 65.0 0.313 0.674NoTopic 63.9 0.290 0.672JEL1 61.0 0.263 0.647JEL2 61.8 0.240 0.646TAM2/30 61.5 0.228 0.580Table 2: Model comparisons(1) (2) (3)LDA50 1.814???2.457???2.243???Log-Lik.
-1075.0 -758.7 -740.6JEL1 1.450???2.128???1.799???Log-Lik.
-1075.3 -757.4 -740.5No Topic 0.524???0.659???0.824???Log-Lik.
-1075.3 -760.5 -741.0Question No Yes YesDemog./Prof.
No No YesObservations 715 715 715Individuals 39 39 39?p < 0.10,?
?p < 0.05,??
?p < 0.01Table 3: IGM correlations.
Column (1) shows re-sults of regression of response on predicted ideol-ogy.
Column (2) adds question dummies.
Column(3) adds demographic and professional variables.conducted by the University of Chicago.5Eachsurvey question asks for an economists opinion onan issue of political relevance such as minimumwages or tax rates.
For further details on the datasee Gordon and Dahl.
Of importance here is thatGordon and Dahl categorize 22 questions whereagreement (disagreement) with the statement im-plies belief a conservative (liberal) viewpoint.To see if our predicted ideologies are corre-lated with survey responses, we run an ordered-logistic regression (McCullagh, 1980).
Surveyresponses are coded with the following order:Strongly Disagree, Disagree, Uncertain, Agree,Strongly Agree.
We regress survey responses ontopredicted ideologies.
We also include question-level dummies and explanatory variables for a re-5http://igmchicago.org1807spondent?s gender, year of Ph.D., Ph.D. univer-sity, NBER membership, and experience in federalgovernment.
Table 3 shows the results of these re-gressions for three topic mappings.
The correla-tion between our predictions and survey respon-dents are all strongly significant.One way to interpret these results is to com-pare the change in predicted probability of pro-viding an Agree or Strongly Agree answer (agree-ing with the conservative view point) if we changepredicted ideology from most liberal to most con-servative.
For NoTopic, this predicted probabil-ity is 35% when ideology is set to most liberaland jumps to 73.7% when set to most conserva-tive.
This difference increases for topic-adjustedmodels.
For LDA50, the probability of a conser-vative answer when ideology is set to most liberalis 14.5% and 93.8% for most conservative.Figure 1 compares the predicted probabilities ofchoosing different answers when ideology is setto most liberal and most conservative.
Our topic-adjusted models suggest that the most conserva-tive economists are much more likely to stronglyagree with a conservative response than for themost liberal economists to strongly agree with aliberal response.
It is worthwhile to note fromthe small increase in log-likelihood in Table 3when controls are added, suggesting that our ide-ology scores are much better predictors of IGMresponses than demographic and professional con-trols.5 Conclusions and Future WorkWe?ve presented a supervised methodology for ex-tracting political sentiment in a domain where it?sdiscouraged and shown how it even predicts thepartisanship calculated from completely unrelatedIGM survey data.
In a companion paper (Jelveh etal., 2014) we further demonstrate how this tool canbe used to aid policymakers in de-biasing researchfindings.
When compared to domains where ideo-logical language is expected, our predictive abilityis reduced.
Future work should disentangle howmuch this difference is due to modeling decisionsand limitations versus actual absence of ideology.Future works should also investigate how fully un-supervised methods can be extended to match ourperformance.0102030405060Str.
Dis.
Dis.
Uncert.
Agr.
Str.
Agr.JEL1 0102030405060LDA500102030405060NOTOPICconservativeliberalFigure 1: The predicted probability of agreeingwith a conservative response when ideology isset to most liberal (gray) and most conservative(black).AcknowledgementThis work was supported in part by the NSF (un-der grant 0966187).
The views and conclusionscontained in this document are those of the authorsand should not be interpreted as necessarily rep-resenting the official policies, either expressed orimplied, of any of the sponsors.1808ReferencesAmr Ahmed and Eric P. Xing.
2010.
Staying in-formed: supervised and semi-supervised multi-viewtopical analysis of ideological perspective.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, pages 1140?1150.
Association for Computational Linguistics.A.S.
2010.
Is economics a right-wing conspiracy?The Economist, August.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet allocation.
the Journal of ma-chine Learning research, 3:993?1022.Raj Chetty.
2013.
Yes, economics is a science.
TheNew York Times, October.Jacob Eisenstein, Amr Ahmed, and Eric P. Xing.
2011.Sparse additive generative models of text.
In Pro-ceedings of the 28th International Conference onMachine Learning (ICML-11), pages 1041?1048.T.
Fawcett.
2006.
An introduction to ROC analysis.Pattern recognition letters, 27(8):861?874.Matthew Gentzkow and Jesse M. Shapiro.
2010.
Whatdrives media slant?
evidence from U.S. daily news-papers.
Econometrica, 78(1):35?71.Roger Gordon and Gordon B Dahl.
2013.
Viewsamong economists: Professional consensus or point-counterpoint?
American Economic Review,103(3):629?635, May.J.
Grimmer and B. M. Stewart.
2013.
Text as data:The promise and pitfalls of automatic content anal-ysis methods for political texts.
Political Analysis,21(3):267?297, January.David Hedengren, Daniel B. Klein, and Carrie Mil-ton.
2010.
Economist petitions: Ideology revealed.Econ Journal Watch, 7(3):288?319.Zubin Jelveh, Bruce Kogut, and Suresh Naidu.
2014.Political language in economics.Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.2008.
A joint topic and perspective model forideological discourse.
In Machine Learning andKnowledge Discovery in Databases, pages 17?32.Springer.Drew Margolin, Yu-Ru Lin, and David Lazer.
2013.Why so similar?
: Identifying semantic organizingprocesses in large textual corpora.Peter McCullagh.
1980.
Regression models for ordinaldata.
Journal of the royal statistical society.
SeriesB (Methodological), pages 109?142.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,and ChengXiang Zhai.
2007.
Topic sentiment mix-ture: modeling facets and opinions in weblogs.
InProceedings of the 16th international conference onWorld Wide Web, pages 171?180.
ACM.National Bureau of Economic Research.
2010.Amended and restated by-laws of national bureau ofeconomic research, inc.Michael Paul and Roxana Girju.
2010.
A two-dimensional topic-aspect model for discoveringmulti-faceted topics.
Urbana, 51.Dani Rodrik.
2014.
When ideas trump interests:Preferences, worldviews, and policy innovations .Journal of Economic Perspectives, 28(1):189?208,February.Matt Taddy.
2013.
Multinomial inverse regression fortext analysis.
Journal of the American Statistical As-sociation, 108.William Yang Wang, Elijah Mayfield, Suresh Naidu,and Jeremiah Dittmar.
2012.
Historical analysisof legal opinions with a sparse mixed-effects latentvariable model.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics: Long Papers-Volume 1, pages 740?749.Association for Computational Linguistics.1809
