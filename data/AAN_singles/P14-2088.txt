Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 538?544,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsFast Easy Unsupervised Domain Adaptationwith Marginalized Structured DropoutYi Yang and Jacob EisensteinSchool of Interactive ComputingGeorgia Institute of Technology{yiyang, jacobe}@gatech.eduAbstractUnsupervised domain adaptation often re-lies on transforming the instance represen-tation.
However, most such approachesare designed for bag-of-words models, andignore the structured features present inmany problems in NLP.
We propose anew technique called marginalized struc-tured dropout, which exploits featurestructure to obtain a remarkably simpleand efficient feature projection.
Appliedto the task of fine-grained part-of-speechtagging on a dataset of historical Por-tuguese, marginalized structured dropoutyields state-of-the-art accuracy while in-creasing speed by more than an order-of-magnitude over previous work.1 IntroductionUnsupervised domain adaptation is a fundamen-tal problem for natural language processing, aswe hope to apply our systems to datasets unlikethose for which we have annotations.
This is par-ticularly relevant as labeled datasets become stalein comparison with rapidly evolving social mediawriting styles (Eisenstein, 2013), and as there isincreasing interest in natural language processingfor historical texts (Piotrowski, 2012).
While anumber of different approaches for domain adap-tation have been proposed (Pan and Yang, 2010;S?gaard, 2013), they tend to emphasize bag-of-words features for classification tasks such as sen-timent analysis.
Consequently, many approachesrely on each instance having a relatively largenumber of active features, and fail to exploit thestructured feature spaces that characterize syn-tactic tasks such as sequence labeling and pars-ing (Smith, 2011).As we will show, substantial efficiency im-provements can be obtained by designing domainadaptation methods for learning in structured fea-ture spaces.
We build on work from the deeplearning community, in which denoising autoen-coders are trained to remove synthetic noise fromthe observed instances (Glorot et al, 2011a).
Byusing the autoencoder to transform the originalfeature space, one may obtain a representationthat is less dependent on any individual feature,and therefore more robust across domains.
Chenet al (2012) showed that such autoencoders canbe learned even as the noising process is analyt-ically marginalized; the idea is similar in spiritto feature noising (Wang et al, 2013).
Whilethe marginalized denoising autoencoder (mDA) isconsiderably faster than the original denoising au-toencoder, it requires solving a system of equa-tions that can grow very large, as realistic NLPtasks can involve 105or more features.In this paper we investigate noising functionsthat are explicitly designed for structured featurespaces, which are common in NLP.
For example,in part-of-speech tagging, Toutanova et al (2003)define several feature ?templates?
: the currentword, the previous word, the suffix of the currentword, and so on.
For each feature template, thereare thousands of binary features.
To exploit thisstructure, we propose two alternative noising tech-niques: (1) feature scrambling, which randomlychooses a feature template and randomly selectsan alternative value within the template, and (2)structured dropout, which randomly eliminatesall but a single feature template.
We show how itis possible to marginalize over both types of noise,and find that the solution for structured dropout issubstantially simpler and more efficient than themDA approach of Chen et al (2012), which doesnot consider feature structure.We apply these ideas to fine-grained part-of-speech tagging on a dataset of Portuguese textsfrom the years 1502 to 1836 (Galves and Faria,2010), training on recent texts and evaluating538on older documents.
Both structure-aware do-main adaptation algorithms perform as well asstandard dropout ?
and better than the well-known structural correspondence learning (SCL)algorithm (Blitzer et al, 2007) ?
but structureddropout is more than an order-of-magnitude faster.As a secondary contribution of this paper, wedemonstrate the applicability of unsupervised do-main adaptation to the syntactic analysis of histor-ical texts.2 ModelIn this section we first briefly describe the de-noising autoencoder (Glorot et al, 2011b), its ap-plication to domain adaptation, and the analyticmarginalization of noise (Chen et al, 2012).
Thenwe present three versions of marginalized denois-ing autoencoders (mDA) by incorporating differ-ent types of noise, including two new noising pro-cesses that are designed for structured features.2.1 Denoising AutoencodersAssume instances x1, .
.
.
,xn, which are drawnfrom both the source and target domains.
We will?corrupt?
these instances by adding different typesof noise, and denote the corrupted version of xiby?xi.
Single-layer denoising autoencoders recon-struct the corrupted inputs with a projection matrixW : Rd?
Rd, which is estimated by minimizingthe squared reconstruction lossL =12n?i=1||xi?W?xi||2.
(1)If we write X = [x1, .
.
.
,xn] ?
Rd?n, and wewrite its corrupted version?X, then the loss in (1)can be written asL(W) =12ntr[(X?W?X)>(X?W?X)].
(2)In this case, we have the well-known closed-form solution for this ordinary least square prob-lem:W = PQ?1, (3)where Q =?X?X>and P = X?X>.
After ob-taining the weight matrix W, we can insert non-linearity into the output of the denoiser, such astanh(WX).
It is also possible to apply stack-ing, by passing this vector through another autoen-coder (Chen et al, 2012).
In pilot experiments,this slowed down estimation and had little effecton accuracy, so we did not include it.High-dimensional setting Structured predic-tion tasks often have much more features thansimple bag-of-words representation, and perfor-mance relies on the rare features.
In a naive im-plementation of the denoising approach, both Pand Q will be dense matrices with dimension-ality d ?
d, which would be roughly 1011ele-ments in our experiments.
To solve this problem,Chen et al (2012) propose to use a set of pivotfeatures, and train the autoencoder to reconstructthe pivots from the full set of features.
Specifi-cally, the corrupted input is divided to S subsets?xi=[(?x)1i>, .
.
.
, (?x)Si>]>.
We obtain a projec-tion matrix Wsfor each subset by reconstructingthe pivot features from the features in this subset;we can then use the sum of all reconstructions asthe new features, tanh(?Ss=1WsXs).Marginalized Denoising Autoencoders In thestandard denoising autoencoder, we need to gen-erate multiple versions of the corrupted data?Xto reduce the variance of the solution (Glorot etal., 2011b).
But Chen et al (2012) show that itis possible to marginalize over the noise, analyt-ically computing expectations of both P and Q,and computingW = E[P]E[Q]?1, (4)where E[P] =?ni=1E[xi?x>i] and E[Q] =?ni=1E[?xi?x>i].
This is equivalent to corruptingthe data m??
times.
The computation of theseexpectations depends on the type of noise.2.2 Noise distributionsChen et al (2012) used dropout noise for domainadaptation, which we briefly review.
We then de-scribe two novel types of noise that are designedfor structured feature spaces, and explain how theycan be marginalized to efficiently compute W.Dropout noise In dropout noise, each feature isset to zero with probability p > 0.
If we definethe scatter matrix of the uncorrupted input as S =XX>, the solutions under dropout noise areE[Q]?,?={(1?
p)2S?,?if ?
6= ?(1?
p)S?,?if ?
= ?, (5)andE[P]?,?= (1?
p)S?,?, (6)539where ?
and ?
index two features.
The form ofthese solutions means that computing W requiressolving a system of equations equal to the num-ber of features (in the naive implementation), orseveral smaller systems of equations (in the high-dimensional version).
Note also that p is a tunableparameter for this type of noise.Structured dropout noise In many NLP set-tings, we have several feature templates, such asprevious-word, middle-word, next-word, etc, withonly one feature per template firing on any token.We can exploit this structure by using an alterna-tive dropout scheme: for each token, choose ex-actly one feature template to keep, and zero out allother features that consider this token (transitionfeature templates such as ?yt, yt?1?
are not con-sidered for dropout).
Assuming we haveK featuretemplates, this noise leads to very simple solutionsfor the marginalized matrices E[P] and E[Q],E[Q]?,?={0 if ?
6= ?1KS?,?if ?
= ?
(7)E[P]?,?=1KS?,?, (8)ForE[P], we obtain a scaled version of the scat-ter matrix, because in each instance?x, there is ex-actly a 1/K chance that each individual featuresurvives dropout.
E[Q] is diagonal, because forany off-diagonal entry E[Q]?,?, at least one of ?and ?
will drop out for every instance.
We cantherefore view the projection matrix W as a row-normalized version of the scatter matrix S. Putanother way, the contribution of ?
to the recon-struction for ?
is equal to the co-occurence countof ?
and ?, divided by the count of ?.Unlike standard dropout, there are no freehyper-parameters to tune for structured dropout.Since E[Q] is a diagonal matrix, we eliminate thecost of matrix inversion (or of solving a system oflinear equations).
Moreover, to extend mDA forhigh dimensional data, we no longer need to di-vide the corrupted input?x to several subsets.1For intuition, consider standard feature dropoutwith p =K?1K.
This will look very similar tostructured dropout: the matrix E[P] is identical,and E[Q] has off-diagonal elements which arescaled by (1 ?
p)2, which goes to zero as K is1E[P] is an r by d matrix, where r is the number of pivots.large.
However, by including these elements, stan-dard dropout is considerably slower, as we show inour experiments.Scrambling noise A third alternative is to?scramble?
the features by randomly selecting al-ternative features within each template.
For a fea-ture ?
belonging to a template F , with probabilityp we will draw a noise feature ?
also belongingto F , according to some distribution q.
In thiswork, we use an uniform distribution, in whichq?=1|F |.
However, the below solutions will alsohold for other scrambling distributions, such asmean-preserving distributions.Again, it is possible to analytically marginal-ize over this noise.
Recall that E[Q] =?ni=1E[?xi?x>i].
An off-diagonal entry in the ma-trix?x?x>which involves features ?
and ?
belong-ing to different templates (F?6= F?)
can take fourdifferent values (xi,?denotes feature ?
in xi):?
xi,?xi,?if both features are unchanged,which happens with probability (1?
p)2.?
1 if both features are chosen as noise features,which happens with probability p2q?q?.?
xi,?or xi,?if one feature is unchanged andthe other one is chosen as the noise feature,which happens with probability p(1 ?
p)q?or p(1?
p)q?.The diagonal entries take the first two valuesabove, with probability 1 ?
p and pq?respec-tively.
Other entries will be all zero (only onefeature belonging to the same template will firein xi).
We can use similar reasoning to computethe expectation of P. With probability (1 ?
p),the original features are preserved, and we add theouter-product xix>i; with probability p, we add theouter-product xiq>.
Therefore E[P] can be com-puted as the sum of these terms.3 ExperimentsWe compare these methods on historical Por-tuguese part-of-speech tagging, creating domainsover historical epochs.3.1 Experiment setupDatasets We use the Tycho Brahe corpus toevaluate our methods.
The corpus contains a totalof 1,480,528 manually tagged words.
It uses a setof 383 tags and is composed of various texts from540historical Portuguese, from 1502 to 1836.
We di-vide the texts into fifty-year periods to create dif-ferent domains.
Table 1 presents some statistics ofthe datasets.
We hold out 5% of data as develop-ment data to tune parameters.
The two most recentdomains (1800-1849 and 1750-1849) are treatedas source domains, and the other domains are tar-get domains.
This scenario is motivated by train-ing a tagger on a modern newstext corpus and ap-plying it to historical documents.Dataset# of TokensTotal Narrative Letters Dissertation Theatre1800-1849 125719 91582 34137 0 01750-1799 202346 57477 84465 0 604041700-1749 278846 0 130327 148519 01650-1699 248194 83938 115062 49194 01600-1649 295154 117515 115252 62387 01550-1599 148061 148061 0 0 01500-1549 182208 126516 0 55692 0Overall 1480528 625089 479243 315792 60404Table 1: Statistics of the Tycho Brahe CorpusCRF tagger We use a conditional random fieldtagger, choosing CRFsuite because it supportsarbitrary real valued features (Okazaki, 2007),with SGD optimization.
Following the work ofNogueira Dos Santos et al (2008) on this dataset,we apply the feature set of Ratnaparkhi (1996).There are 16 feature templates and 372, 902 fea-tures in total.
Following Blitzer et al (2006), weconsider pivot features that appear more than 50times in all the domains.
This leads to a total of1572 pivot features in our experiments.Methods We compare mDA with three alterna-tive approaches.
We refer to baseline as traininga CRF tagger on the source domain and testing onthe target domain with only base features.
We alsoinclude PCA to project the entire dataset onto alow-dimensional sub-space (while still includingthe original features).
Finally, we compare againstStructural Correspondence Learning (SCL; Blitzeret al, 2006), another feature learning algorithm.In all cases, we include the entire dataset to com-pute the feature projections; we also conducted ex-periments using only the test and training data forfeature projections, with very similar results.Parameters All the hyper-parameters are de-cided with our development data on the trainingset.
We try different low dimension K from 10 to2000 for PCA.
Following Blitzer (2008) we per-form feature centering/normalization, as well asrescaling for SCL.
The best parameters for SCLare dimensionality K = 25 and rescale factor?
= 5, which are the same as in the original pa-per.
For mDA, the best corruption level is p = 0.9for dropout noise, and p = 0.1 for scramblingnoise.
Structured dropout noise has no free hyper-parameters.3.2 ResultsTable 2 presents results for different domain adap-tation tasks.
We also compute the transfer ra-tio, which is defined asadaptation accuracybaseline accuracy, shown inFigure 1.
The generally positive trend of thesegraphs indicates that adaptation becomes progres-sively more important as we select test sets that aremore temporally remote from the training data.In general, mDA outperforms SCL and PCA,the latter of which shows little improvement overthe base features.
The various noising approachesfor mDA give very similar results.
However, struc-tured dropout is orders of magnitude faster thanthe alternatives, as shown in Table 3.
The scram-bling noise is most time-consuming, with costdominated by a matrix multiplication.Method PCA SCLmDAdropout structured scamblingTime 7,779 38,849 8,939 339 327,075Table 3: Time, in seconds, to compute the featuretransformation4 Related WorkDomain adaptation Most previous work on do-main adaptation focused on the supervised setting,in which some labeled data is available in the tar-get domain (Jiang and Zhai, 2007; Daum?e III,2007; Finkel and Manning, 2009).
Our work fo-cuses on unsupervised domain adaptation, whereno labeled data is available in the target domain.Several representation learning methods have beenproposed to solve this problem.
In structural corre-spondence learning (SCL), the induced represen-tation is based on the task of predicting the pres-ence of pivot features.
Autoencoders apply a sim-ilar idea, but use the denoised instances as the la-tent representation (Vincent et al, 2008; Glorot etal., 2011b; Chen et al, 2012).
Within the con-text of denoising autoencoders, we have focused541Task baseline PCA SCLmDAdropout structured scramblingfrom 1800-1849?
1750 89.12 89.09 89.69 90.08 90.08 90.01?
1700 90.43 90.43 91.06 91.56 91.57 91.55?
1650 88.45 88.52 87.09 88.69 88.70 88.57?
1600 87.56 87.58 88.47 89.60 89.61 89.54?
1550 89.66 89.61 90.57 91.39 91.39 91.36?
1500 85.58 85.63 86.99 88.96 88.95 88.91from 1750-1849?
1700 94.64 94.62 94.81 95.08 95.08 95.02?
1650 91.98 90.97 90.37 90.83 90.84 90.80?
1600 92.95 92.91 93.17 93.78 93.78 93.71?
1550 93.27 93.21 93.75 94.06 94.05 94.02?
1500 89.80 89.75 90.59 91.71 91.71 91.68Table 2: Accuracy results for adaptation from labeled data in 1800-1849, and in 1750-1849.Figure 1: Transfer ratio for adaptation to historical texton dropout noise, which has also been applied asa general technique for improving the robustnessof machine learning, particularly in neural net-works (Hinton et al, 2012; Wang et al, 2013).On the specific problem of sequence labeling,Xiao and Guo (2013) proposed a supervised do-main adaptation method by using a log-bilinearlanguage adaptation model.
Dhillon et al (2011)presented a spectral method to estimate low di-mensional context-specific word representationsfor sequence labeling.
Huang and Yates (2009;2012) used an HMM model to learn latent rep-resentations, and then leverage the Posterior Reg-ularization framework to incorporate specific bi-ases.
Unlike these methods, our approach uses astandard CRF, but with transformed features.Historical text Our evaluation concerns syntac-tic analysis of historical text, which is a topic of in-creasing interest for NLP (Piotrowski, 2012).
Pen-nacchiotti and Zanzotto (2008) find that part-of-speech tagging degrades considerably when ap-plied to a corpus of historical Italian.
Moon andBaldridge (2007) tackle the challenging problemof tagging Middle English, using techniques forprojecting syntactic annotations across languages.Prior work on the Tycho Brahe corpus applied su-pervised learning to a random split of test andtraining data (Kepler and Finger, 2006; Dos San-tos et al, 2008); they did not consider the domainadaptation problem of training on recent data andtesting on older historical text.5 Conclusion and Future WorkDenoising autoencoders provide an intuitive so-lution for domain adaptation: transform the fea-tures into a representation that is resistant to thenoise that may characterize the domain adaptationprocess.
The original implementation of this ideaproduced this noise directly (Glorot et al, 2011b);later work showed that dropout noise could be an-alytically marginalized (Chen et al, 2012).
Wetake another step towards simplicity by showingthat structured dropout can make marginalizationeven easier, obtaining dramatic speedups withoutsacrificing accuracy.Acknowledgments : We thank the reviewers foruseful feedback.
This research was supported byNational Science Foundation award 1349837.542ReferencesJohn Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?06, pages 120?128, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In Association for Computational Linguis-tics, Prague, Czech Republic.John Blitzer.
2008.
Domain Adaptation of NaturalLanguage Processing Systems.
Ph.D. thesis, Uni-versity of Pennsylvania.Minmin Chen, Zhixiang Xu, Kilian Weinberger, andFei Sha.
2012.
Marginalized denoising autoen-coders for domain adaptation.
In John Langford andJoelle Pineau, editors, Proceedings of the 29th Inter-national Conference on Machine Learning (ICML-12), ICML ?12, pages 767?774.
ACM, New York,NY, USA, July.Hal Daum?e III.
2007.
Frustratingly easy domain adap-tation.
In ACL, volume 1785, page 1787.Paramveer S Dhillon, Dean P Foster, and Lyle H Ungar.2011.
Multi-view learning of word embeddings viacca.
In NIPS, volume 24, pages 199?207.C?
?cero Nogueira Dos Santos, Ruy L Milidi?u, andRa?ul P Renter??a.
2008.
Portuguese part-of-speechtagging using entropy guided transformation learn-ing.
In Computational Processing of the PortugueseLanguage, pages 143?152.
Springer.Jacob Eisenstein.
2013.
What to do about bad lan-guage on the internet.
In Proceedings of NAACL,Atlanta, GA.Jenny Rose Finkel and Christopher D Manning.
2009.Hierarchical bayesian domain adaptation.
In Pro-ceedings of Human Language Technologies: The2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 602?610.
Association for Computa-tional Linguistics.Charlotte Galves and Pablo Faria.
2010.
Ty-cho Brahe Parsed Corpus of Historical Por-tuguese.
http://www.tycho.iel.unicamp.br/ ty-cho/corpus/en/index.html.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011a.
Deep sparse rectifier networks.
In Proceed-ings of the 14th International Conference on Arti-ficial Intelligence and Statistics.
JMLR W&CP Vol-ume, volume 15, pages 315?323.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011b.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In Pro-ceedings of the 28th International Conference onMachine Learning (ICML-11), pages 513?520.Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,Ilya Sutskever, and Ruslan R Salakhutdinov.
2012.Improving neural networks by preventing co-adaptation of feature detectors.
arXiv preprintarXiv:1207.0580.Fei Huang and Alexander Yates.
2009.
Distribu-tional representations for handling sparsity in super-vised sequence-labeling.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP: Volume 1-Volume 1, pages 495?503.
Association for Compu-tational Linguistics.Fei Huang and Alexander Yates.
2012.
Biased rep-resentation learning for domain adaptation.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1313?1323.
Association for Computational Linguis-tics.Jing Jiang and ChengXiang Zhai.
2007.
Instanceweighting for domain adaptation in nlp.
In ACL,volume 2007, page 22.F?abio N Kepler and Marcelo Finger.
2006.
Comparingtwo markov methods for part-of-speech tagging ofportuguese.
In Advances in Artificial Intelligence-IBERAMIA-SBIA 2006, pages 482?491.
Springer.Taesun Moon and Jason Baldridge.
2007.
Part-of-speech tagging for middle english through align-ment and projection of parallel diachronic texts.
InEMNLP-CoNLL, pages 390?399.C?
?cero Nogueira Dos Santos, Ruy L. Milidi?u, andRa?ul P.
Renter??a.
2008.
Portuguese part-of-speechtagging using entropy guided transformation learn-ing.
In Proceedings of the 8th international con-ference on Computational Processing of the Por-tuguese Language, PROPOR ?08, pages 143?152,Berlin, Heidelberg.
Springer-Verlag.Naoaki Okazaki.
2007.
Crfsuite: a fast implementa-tion of conditional random fields (crfs).Sinno Jialin Pan and Qiang Yang.
2010.
A survey ontransfer learning.
Knowledge and Data Engineer-ing, IEEE Transactions on, 22(10):1345?1359.Marco Pennacchiotti and Fabio Massimo Zanzotto.2008.
Natural language processing across time:An empirical investigation on italian.
In Advancesin Natural Language Processing, pages 371?382.Springer.Michael Piotrowski.
2012.
Natural language process-ing for historical texts.
Synthesis Lectures on Hu-man Language Technologies, 5(2):1?157.543Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, April 16.Noah A Smith.
2011.
Linguistic structure prediction.Synthesis Lectures on Human Language Technolo-gies, 4(2):1?274.Anders S?gaard.
2013.
Semi-supervised learning anddomain adaptation in natural language processing.Synthesis Lectures on Human Language Technolo-gies, 6(2):1?103.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.
Association for Compu-tational Linguistics.Pascal Vincent, Hugo Larochelle, Yoshua Bengio, andPierre-Antoine Manzagol.
2008.
Extracting andcomposing robust features with denoising autoen-coders.
In Proceedings of the 25th internationalconference on Machine learning, pages 1096?1103.ACM.Sida I. Wang, Mengqiu Wang, Stefan Wager, PercyLiang, and Christopher D. Manning.
2013.
Fea-ture noising for log-linear structured prediction.
InEmpirical Methods in Natural Language Processing(EMNLP).Min Xiao and Yuhong Guo.
2013.
Domain adapta-tion for sequence labeling tasks with a probabilis-tic language adaptation model.
In Sanjoy Dasguptaand David Mcallester, editors, Proceedings of the30th International Conference on Machine Learn-ing (ICML-13), volume 28, pages 293?301.
JMLRWorkshop and Conference Proceedings.544
