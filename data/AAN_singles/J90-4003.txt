TECHNICAL CORRESPONDENCEPARSING DISCONTINUOUS CONSTITUENTS INDEPENDENCY GRAMMARDiscontinuous constituents--for example, a noun and itsmodifying adjective separated by words unrelated to them---arc common in variable-word-order languages; Figure Ishows examples.
But phrase structure grammars, includingID/LP grammars, require ach constituent to be a contigu-ous series of words.
Insofar as standard parsing algorithmsare based on phrase structure rules, they are inadequate forparsing such languagesJThe algorithm presented here, however, does not requireconstituents o be continuous, but merely prefers them so.It can therefore parse languages in which conventionalparsing techniques do not work.
At the same time, becauseof its preference for nearby attachments, it prefers to makeconstituents continuous when more than one analysis ispossible.
The new algorithm has been used successfully toparse Russian and Latin (Covington 1988, 1990).This algorithm uses dependency grammar.
That is, in-stead of breaking the sentence into phrases and subphrases,it establishes links between individual words.
Each linkconnects a word (the "head") with one of its "dependents"(an argument or modifier).
Figure 2 shows how this works.The arrows point from head to dependent; a head can havemany dependents, but each dependent can have only onehead.
Of course the same word can be the head in one linkand the dependent in another.
2Dependency grammar is equivalent to an X-bar theorywith only one phrasal bar level (Figure 3)--the dependentsof a word are the heads of its sisters.
Thus dependencygrammar captures the increasingly recognized importanceof headship in syntax.
At the same time, the absence ofphrasal nodes from the dependency representation stream-lines the search process during parsing.The parser presupposes a grammar that specifies whichwords can depend on which.
In the prototype, the grammarconsists of unification-based dependency rules (calledD-rules) such as:"category:noun \] r category:verb\] person:X Inumber."
Y I "~ |person:X |case:nominativeJ Lnumber:Y JThis rule sanctions a dependency relation between any twowords whose features unify with the structures shown--inthis case, the verb and its subject in a language such asRussian or Latin.
The arrow means "can depend on" andthe word order is not specified.
X and Y are variables.D-rules take the place of the phrase structure rules used byShieber (1986) and others; semantic information can easilybe added to them, and the whole power of unification-basedgrammar is available.The parser accepts words from the input string and keepstrack of whether or not each word is "independent" (not yetknown to depend on another word), indicated by + or - inFigure 4.
On accepting a word W, the parser does thefollowing:(1) Search the independent words (those marked +),mosl~ recent first, for words that can depend on W. If anyare tbund, establish the dependencies and change the mark-ing of the dependents from + to - .
(21) Search all words so far seen, most recent first, for aworct on which W can depend.
If one is found, establish thedependency and mark Was - .
Otherwise mark Was +.Figure 4 shows the process in action.
The first threewords, ultima Cumaei venit, are accepted without creatingany links.
Then the parser accepts iam and makes it dependon venit.
Next the parser accepts carminis, on which Cu-maei, already in the list, depends.
Finally it accepts aetas,which becomes a dependent ofvenit and the head of ultimaand carminis.The most-recent-first search order gives the parser itspreference for continuous constituents.
The search order issignificant because it is assumed that the parser can back-track, i.e., whenever there are alternatives it can back upand try them.
This is necessary to avoid "garden paths"such as taking animalia (ambiguously nominative or accu-sative) to be the subject of animalia vident pueri "boys seeanimals.
"With ordinary sentences, however, backtracking is rela-tively seldom necessary.
Further, there appear to be otherconstraints on variable word order.
Ades and Steedman(1982) propose that all discontinuities can be resolved by apushdown stack.
(For example, pick up ultima, then Cu-maei, then put down Cumaei next to carminis, then putdown ultima next to aetas.
Crossing movements are notpermitted.)
Moreover, there appears to be an absoluteconstraint against mixing clauses together?
If these hypoth-eses hold true, the parser can be modified to restrict thesearch process accordingly.Most dependency parsers have followed a "principle ofadjacency" that requires every word plus all its direct andindirect dependents o form a contiguous ubstring (Haysand Ziehe 1960; Starosta nd Nomura 1986; Fraser 1989;but not Hellwig 1986 and possibly not J/ippinen et al1986).
This is equivalent to requiring constituents to becontinuous.
This parser imposes no such requirement.
Toadd the adjacency requirement, one would modify it asfollows:(1) When looking for potential dependents of W, never234 Computational Linguistics Volume 16, Number 4, December 1990Technical CorrespondenceSVP NPA d ~ NAdj V Adv NI I I Iulll"nTa Cumael venil Jam corm\]his oelaslast Cumean has come now song age'The last era of the Cumean song has now arrived'(Latin; Vergil.
Eclogues IV.d)SNP Aux NP Vi ikurdu-ngku  ka mahdi  waj\])~lll"-nyi wi la-ngkuchild dog chase small'The small child is chasing the dog'(Warlpiri; Siewierska 1988:158. citing Nash)Figure I.
Examples of discontinuous constituents.
).he big dog chased Lhe catFigure 2.
Dependency representation f ascntence.
Arrows point from each word to itsdepcndents (modifiers or arguments).A B C DDA B C DFigure 3.
Equivalence of dependency network toX-bar tree.I.2.ulLllrn a+ul\[Zrna Curnael+ +3.
uH ima Cumaei  venil?
+ 4-4. u/lima Cumaei veni\[ iam + + + --5. ultZrna Cumael  venH ibm carmmis+ - -  + - -  4 -u lHma Cumaei  venH ibm carmimL~ aeLasFigure 4.
The parser accepts words one by one andtries to link them together; ' +' marks words thatdo not (yet) depend on other words.skip over an independent word.
That is, if an independentword is found that cannot depend on IV, then neither canany earlier independent word.
(2) When looking for the word on which W depends,consider only the previous word, that word's head, thehead's head if any, and so on.With these requirements added, the algorithm would bethe same as one implemented by Hudson (1989).Formal complexity analysis has not been carried out, butmy algorithm is simpler, at least conceptually, than thevariable-word-order parsers of Johnson (1985), Kashket(1986), and Abramson and Dahl (1989).
Johnson's parserand Abramson and Dahl's parser use constituency treeswith explicit discontinuity ("tangled trees"), with all theirinherent unwieldiness.
Kashket's parser, though based onGB theory, is effectively a dependency parser since it relieson case assignment and subcategorization rather than treestructure.Michae l  A.  Cov ingtonArtificial Intelligence ProgramsThe University of GeorgiaAthens, GA 30602REFERENCESAbramson, Harvey and Dahl, Veronica.
(1989).
Logic Grammars.Springer.Ades, Anthony E. and Steedman, Mark J.
(1982).
"On the order ofwords."
Linguistics and Philosophy, 4:517-558.Computational Linguistics Volume 16, Number 4, December 1990 235Technical CorrespondenceCovington, Michael A.
(1990).
"A dependency parser for variable-word-order languages."
In Computer Assisted Analysis and Modeling on theIBM 3090, edited by Hilton U.
Brown, MIT Press.Covington, Michael A.
(1988).
"Parsing variable-word-order languageswith unification-based dependency grammar."
Research report 01-.0022, Artificial Intelligence Programs, The University of Georgia.Fraser, Norman M. (1989).
"Parsing and dependency grammar."
UCLWorking Papers in Linguistics, 1: 296--319.Hays, David G. (1964).
"Dependency theory: a formalism and someobservations."
Language 40:511-525.Hays, David G. and Ziehe, T. W. (1960).
"Studies in machine translation,10---Russian sentence-structure determination."
Research memoran-dum RM-2358, The RAND Corporation, Santa Monica, CA.Hellwig, Peter.
(1986).
"Dependency unification grammar."
In Proceed-ings of the 1 I th International Conference on Computational Linguis-tics (COLING-86).
195-198.Hudson, Richard.
(1989).
"Towards a computer-testable word grammarof English."
UCL Working Papers in Linguistics, 1:321-339.Hudson, Richard.
(1984).
Word Grammar.
Blackwell.J\[ippinen, Harri; Lehtola, Aarno; and Valkonen, Karl (1986).
"Func-tional structures for parsing dependency onstraints."
In Proceedingsof the l lth International Conference on Computational Linguistics(COLING-86), 461463.Johnson, Mark.
(1985).
"Parsing with discontinuous constituents."
Pro-ceedings of the 23rd Annual Meeting of the Association for Computa-tional Linguistics, 127-132.Kashket, Michael B.
(1986).
"Parsing a free-word-order language:Warlpiri."
Proceedings of the 24th Annual Meeting of the Associationfor Computational Linguistics, 60-66.Mel'~uk, 1.
A.
(1988).
Dependency S ntax: Theory and Practice.
StateUniversity Press of New York.Rohinson, Jane J.
(1970).
"Dependency structures and transformationalrules."
Language 46:259-285.Schubert, Klaus.
(1987).
Metataxis: Contrastive Dependency S ntax forMr,~chine Translation.
Foris.Shieber, Stuart M. (1986).
An Introduction to Unification-Based Ap-prc,aches to Grammar.
(CSLI Lecture Notes, 4.)
Stanford: CSLI.Starosta, Stanley.
(1988).
The Case for Lexicase.
Pinter.Starosta, Stanley and Nomura, Hirosato.
(1986).
"Lexicase parsing: alex:icon-driven approach to syntactic analysis.
In Proceedings of thel lth International Conference on Computational Linguistics (COL-ING-86).Tesni~re, Lucien.
(1959).
Elkments de la Syntaxe Structurale.
Klinck-sleek.NOTES1.
The early stages of this work were supported by National ScienceF;oundation grant IST-85-02477.
I am grateful to Norman Fraser andPdchard Hudson for comments and encouragement.2.
On dependency grammar in general see Tesni6re 1959, Hays 1964,Robinson 1970, Hudson 1986, Schubert 1987, Mel'~uk 1988, andStarosta 1988.
In Hudson's ystem, a single word can have two headsprovided the grammatical relations connecting it to them are distinct.3.
As pointed out by an anonymous reviewer for Computational Linguis-tics.236 Computational Linguistics Volume 16, Number 4, December 1990
