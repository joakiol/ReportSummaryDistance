c?
2004 Association for Computational LinguisticsSquibs and DiscussionsThe Kappa Statistic: A Second LookBarbara Di Eugenio?
Michael Glass?University of Illinois at Chicago Valparaiso UniversityIn recent years, the kappa coefficient of agreement has become the de facto standard for evaluatingintercoder agreement for tagging tasks.
In this squib, we highlight issues that affect ?
and thatthe community has largely neglected.
First, we discuss the assumptions underlying differentcomputations of the expected agreement component of ?.
Second, we discuss how prevalence andbias affect the ?
measure.In the last few years, coded corpora have acquired an increasing importance in ev-ery aspect of human-language technology.
Tagging for many phenomena, such asdialogue acts (Carletta et al 1997; Di Eugenio et al 2000), requires coders to makesubtle distinctions among categories.
The objectivity of these decisions can be as-sessed by evaluating the reliability of the tagging, namely, whether the coders reacha satisfying level of agreement when they perform the same coding task.
Currently,the de facto standard for assessing intercoder agreement is the ?
coefficient, whichfactors out expected agreement (Cohen 1960; Krippendorff 1980).
?
had long beenused in content analysis and medicine (e.g., in psychiatry to assess how well stu-dents?
diagnoses on a set of test cases agree with expert answers) (Grove et al 1981).Carletta (1996) deserves the credit for bringing ?
to the attention of computationallinguists.?
is computed as P(A)?
P(E)1?
P(E) , where P(A) is the observed agreement among thecoders, and P(E) is the expected agreement, that is, P(E) represents the probabil-ity that the coders agree by chance.
The values of ?
are constrained to the inter-val [?1, 1].
A ?
value of one means perfect agreement, a ?
value of zero meansthat agreement is equal to chance, and a ?
value of negative one means ?perfect?disagreement.This squib addresses two issues that have been neglected in the computationallinguistics literature.
First, there are two main ways of computing P(E), the expectedagreement, according to whether the distribution of proportions over the categoriesis taken to be equal for the coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegeland Castellan 1988) or not (Cohen 1960).
Clearly, the two approaches reflect differentconceptualizations of the problem.
We believe the distinction between the two is oftenglossed over because in practice the two computations of P(E) produce very similaroutcomes in most cases, especially for the highest values of ?.
However, first, wewill show that they can indeed result in different values of ?, that we will call ?Co(Cohen 1960) and ?S&C (Siegel and Castellan 1988).
These different values can leadto contradictory conclusions on intercoder agreement.
Moreover, the assumption of?
Computer Science, 1120 SEO (M/C 152), 851 South Morgan Street, Chicago, IL 60607.
E-mail:bdieugen@uic.edu.?
Mathematics and Computer Science, 116 Gellerson Hall, Valparaiso, IN 46383.
E-mail: michael.glass@valpo.edu.96Computational Linguistics Volume 30, Number 1equal distributions over the categories masks the exact source of disagreement amongthe coders.
Thus, such an assumption is detrimental if such systematic disagreementsare to be used to improve the coding scheme (Wiebe, Bruce, and O?Hara 1999).Second, ?
is affected by skewed distributions of categories (the prevalence prob-lem) and by the degree to which the coders disagree (the bias problem).
That is, fora fixed P(A), the values of ?
vary substantially in the presence of prevalence, bias, orboth.We will conclude by suggesting that ?Co is a better choice than ?S&C in those studiesin which the assumption of equal distributions underlying ?S&C does not hold: the vastmajority, if not all, of discourse- and dialogue-tagging efforts.
However, as ?Co suffersfrom the bias problem but ?S&C does not, ?S&C should be reported too, as well as a thirdmeasure that corrects for prevalence, as suggested in Byrt, Bishop, and Carlin (1993).1.
The Computation of P(E)P(E) is the probability of agreement among coders due to chance.
The literature de-scribes two different methods for estimating a probability distribution for randomassignment of categories.
In the first, each coder has a personal distribution, basedon that coder?s distribution of categories (Cohen 1960).
In the second, there is onedistribution for all coders, derived from the total proportions of categories assignedby all coders (Scott 1955; Fleiss 1971; Krippendorff 1980; Siegel and Castellan 1988).1We now illustrate the computation of P(E) according to these two methods.
Wewill then show that the resulting ?Co and ?S&C may straddle one of the significantthresholds used to assess the raw ?
values.The assumptions underlying these two methods are made tangible in the waythe data are visualized, in a contingency table for Cohen, and in what we will callan agreement table for the others.
Consider the following situation.
Two coders2code 150 occurrences of Okay and assign to them one of the two labels Accept orAck(nowledgement) (Allen and Core 1997).
The two coders label 70 occurrences as Ac-cept, and another 55 as Ack.
They disagree on 25 occurrences, which one coder labelsas Ack, and the other as Accept.
In Figure 1, this example is encoded by the top contin-gency table on the left (labeled Example 1) and the agreement table on the right.
Thecontingency table directly mirrors our description.
The agreement table is an N ?
mmatrix, where N is the number of items in the data set and m is the number of labelsthat can be assigned to each object; in our example, N = 150 and m = 2.
Each entry nijis the number of codings of label j to item i.
The agreement table in Figure 1 showsthat occurrences 1 through 70 have been labeled as Accept by both coders, 71 through125 as Ack by both coders, and 126 to 150 differ in their labels.1 To be precise, Krippendorff uses a computation very similar to Siegel and Castellan?s to produce astatistic called alpha.
Krippendorff computes P(E) (called 1 ?
De in his terminology) with asampling-without-replacement methodology.
The computations of P(E) and of 1 ?
De show that thedifference is negligible:P(E) =?j(?inijNk)2(Siegel and Castellan)1 ?
De =?j(?inijNk)([?inij]?1Nk?1)(Krippendorff)2 Both ?S&C (Scott 1955) and ?Co (Cohen 1960) were originally devised for two coders.
Each has beenextended to more than two coders, for example, respectively Fleiss (1971) and Bartko and Carpenter(1976).
Thus, without loss of generality, our examples involve two coders.97Di Eugenio and Glass Kappa: A Second LookExample 1Coder 2Coder 1 Accept AckAccept 70 25 95Ack 0 55 5570 80 150Example 2Coder 2Coder 1 Accept AckAccept 70 15 85Ack 10 55 6580 70 150Accept AckOkay1 2 0...Okay70 2 0Okay71 0 2...Okay125 0 2Okay126 1 1...Okay150 1 1165 135Figure 1Cohen?s contingency tables (left) and Siegel and Castellan?s agreement table (right).Agreement tables lose information.
When the coders disagree, we cannot recon-struct which coder picked which category.
Consider Example 2 in Figure 1.
The twocoders still disagree on 25 occurrences of Okay.
However, one coder now labels 10of those as Accept and the remaining 15 as Ack, whereas the other labels the same10 as Ack and the same 15 as Accept.
The agreement table does not change, but thecontingency table does.Turning now to computing P(E), Figure 2 shows, for Example 1, Cohen?s com-putation of P(E) on the left, and Siegel and Castellan?s computation on the right.
Weinclude the computations of ?Co and ?S&C as the last step.
For both Cohen and Siegeland Castellan, P(A) = 125/150 = 0.8333.
The observed agreement P(A) is computedas the proportion of items the coders agree on to the total number of items; N is thenumber of items, and k the number of coders (N = 150 and k = 2 in our example).Both ?Co and ?S&C are highly significant at the p = 0.5 ?
10?5 level (significance iscomputed for ?Co and ?S&C according to the formulas in Cohen [1960] and Siegel andCastellan [1988], respectively).The difference between ?Co and ?S&C in Figure 2 is just under 1%, however, theresults of the two ?
computations straddle the value 0.67, which for better or worsehas been adopted as a cutoff in computational linguistics.
This cutoff is based on theassessment of ?
values in Krippendorff (1980), which discounts ?
< 0.67 and allowstentative conclusions when 0.67 ?
?
< 0.8 and definite conclusions when ?
?
0.8.Krippendorff?s scale has been adopted without question, even though Krippendorffhimself considers it only a plausible standard that has emerged from his and hiscolleagues?
work.
In fact, Carletta et al (1997) use words of caution against adoptingKrippendorff?s suggestion as a standard; the first author has also raised the issue ofhow to assess ?
values in Di Eugenio (2000).If Krippendorff?s scale is supposed to be our standard, the example just workedout shows that the different computations of P(E) do affect the assessment of inter-coder agreement.
If less-strict scales are adopted, the discrepancies between the two?
computations play a larger role, as they have a larger effect on smaller values of ?.For example, Rietveld and van Hout (1993) consider 0.20 < ?
?
0.40 as indicating fairagreement, and 0.40 < ?
?
0.60 as indicating moderate agreement.
Suppose that twocoders are coding 100 occurrences of Okay.
The two coders label 40 occurrences asAccept and 25 as Ack.
The remaining 35 are labeled as Ack by one coder and as Acceptby the other (as in Example 6 in Figure 4); ?Co = 0.418, but ?S&C = 0.27.
These twovalues are really at odds.98Computational Linguistics Volume 30, Number 1Assumption of different distributions amongcoders (Cohen)Step 1.
For each category j, compute the overallproportion pj,l of items assigned to j by each coderl.
In a contingency table, each row and columntotal divided by N corresponds to one such pro-portion for the corresponding coder.pAccept,1 = 95/150, pAck,1 = 55/150,pAccept,2 = 70/150, pAck,2 = 80/150Assumption of equal distributions among coders(Siegel and Castellan)Step 1.
For each category j, compute pj, the overallproportion of items assigned to j.
In an agreementtable, the column totals give the total counts foreach category j, hence:pj =1Nk ?
?i nijpAccept = 165/300 = 0.55, pAck = 135/300 = 0.45Step 2.
For a given item, the likelihood of bothcoders?
independently agreeing on category j bychance, is pj,1 ?
pj,2.pAccept,1 ?
pAccept,2 = 95/150 ?
70/150 = 0.2956pAck,1 ?
pAck,2 = 55/150 ?
80/150 = 0.1956Step 2.
For a given item, the likelihood of bothcoders?
independently agreeing on category j bychance is p2j .p2Accept = 0.3025p2Ack = 0.2025Step 3.
P(E), the likelihood of coders?
accidentallyassigning the same category to a given item, is?j pj,1 ?
pj,2 = 0.2956 + 0.1956 = 0.4912Step 3.
P(E), the likelihood of coders?
accidentallyassigning the same category to a given item, is?j p2j = 0.3025 + 0.2025 = 0.5050Step 4.?Co= (0.8333 ?
0.4912)/(1 ?
0.4912) =.3421/.5088=0.6724Step 4.?S&C= (0.8333 ?
0.5050)/(1 ?
0.5050) =.3283/.4950 = 0.6632Figure 2The computation of P(E) and ?
according to Cohen (left) and to Siegel and Castellan (right).2.
Unpleasant Behaviors of Kappa: Prevalence and BiasIn the computational linguistics literature, ?
has been used mostly to validate cod-ing schemes: Namely, a ?good?
value of ?
means that the coders agree on the cate-gories and therefore that those categories are ?real.?
We noted previously that assess-ing what constitutes a ?good?
value for ?
is problematic in itself and that differentscales have been proposed.
The problem is compounded by the following obviouseffect on ?
values: If P(A) is kept constant, varying values for P(E) yield vary-ing values of ?.
What can affect P(E) even if P(A) is constant are prevalence andbias.The prevalence problem arises because skewing the distribution of categories inthe data increases P(E).
The minimum value P(E) = 1/m occurs when the labels areequally distributed among the m categories (see Example 4 in Figure 3).
The maximumvalue P(E) = 1 occurs when the labels are all concentrated in a single category.
Butfor a given value of P(A), the larger the value of P(E), the lower the value of ?.Example 3 and Example 4 in Figure 3 show two coders agreeing on 90 out of 100occurrences of Okay, that is, P(A) = 0.9.
However, ?
ranges from ?0.048 to 0.80, andfrom not significant to significant (the values of ?S&C for Examples 3 and 4 are thesame as the values of ?Co).3 The differences in ?
are due to the difference in the relativeprevalence of the two categories Accept and Ack.
In Example 3, the distribution isskewed, as there are 190 Accepts but only 10 Acks across the two coders; in Example 4,the distribution is even, as there are 100 Accepts and 100 Acks, respectively.
Theseresults do not depend on the size of the sample; that is, they are not due to the fact3 We are not including agreement tables for the sake of brevity.99Di Eugenio and Glass Kappa: A Second LookExample 3Coder 2Coder 1 Accept AckAccept 90 5 95Ack 5 0 595 5 100P(A) = 0.90, P(E) = 0.905?Co = ?S&C = ?0.048, p = 1Example 4Coder 2Coder 1 Accept AckAccept 45 5 50Ack 5 45 5050 50 100P(A) = 0.90, P(E) = 0.5?Co = ?S&C = 0.80, p = 0.5 ?
10?5Figure 3Contingency tables illustrating the prevalence effect on ?.Example 5Coder 2Coder 1 Accept AckAccept 40 15 55Ack 20 25 4560 40 100P(A) = 0.65, P(E) = 0.52?Co = 0.27, p = 0.005Example 6Coder 2Coder 1 Accept AckAccept 40 35 75Ack 0 25 2540 60 100P(A) = 0.65, P(E) = 0.45?Co = 0.418, p = 0.5 ?
10?5Figure 4Contingency tables illustrating the bias effect on ?Co.Example 3 and Example 4 are small.
As the computations of P(A) and P(E) are basedon proportions, the same distributions of categories in a much larger sample, say,10,000 items, will result in exactly the same ?
values.
Although this behavior followssquarely from ?
?s definition, it is at odds with using ?
to assess a coding scheme.From both Example 3 and Example 4 we would like to conclude that the two codersare in substantial agreement, independent of the skewed prevalence of Accept withrespect to Ack in Example 3.
The role of prevalence in assessing ?
has been subjectto heated discussion in the medical literature (Grove et al 1981; Berry 1992; Goldman1992).The bias problem occurs in ?Co but not ?S&C.
For ?Co, P(E) is computed fromeach coder?s individual probabilities.
Thus, the less two coders agree in their overallbehavior, the fewer chance agreements are expected.
But for a given value of P(A),decreasing P(E) will increase ?Co, leading to the paradox that ?Co increases as thecoders become less similar, that is, as the marginal totals diverge in the contingencytable.
Consider two coders coding the usual 100 occurrences of Okay, according tothe two tables in Figure 4.
In Example 5, the proportions of each category are verysimilar among coders, at 55 versus 60 Accept, and 45 versus 40 Ack.
However, inExample 6 coder 1 favors Accept much more than coder 2 (75 versus 40 occurrences)and conversely chooses Ack much less frequently (25 versus 60 occurrences).
In bothcases, P(A) is 0.65 and ?S&C is stable at 0.27, but ?Co goes from 0.27 to 0.418.
Ourinitial example in Figure 1 is also affected by bias.
The distribution in Example 1yielded ?Co = 0.6724 but ?S&C = 0.6632.
If the bias decreases as in Example 2, ?Cobecomes 0.6632, the same as ?S&C.3.
DiscussionThe issue that remains open is which computation of ?
to choose.
Siegel andCastellan?s ?S&C is not affected by bias, whereas Cohen?s ?Co is.
However, it is100Computational Linguistics Volume 30, Number 1questionable whether the assumption of equal distributions underlying ?S&C is ap-propriate for coding in discourse and dialogue work.
In fact, it appears to us that itholds in few if any of the published discourse- or dialogue-tagging efforts for which?
has been computed.
It is, for example, appropriate in situations in which item i maybe tagged by different coders than item j (Fleiss 1971).
However, ?
assessments fordiscourse and dialogue tagging are most often performed on the same portion of thedata, which has been annotated by each of a small number of annotators (betweentwo and four).
In fact, in many cases the analysis of systematic disagreements amongannotators on the same portion of the data (i.e., of bias) can be used to improve thecoding scheme (Wiebe, Bruce, and O?Hara 1999).To use ?Co but to guard against bias, Cicchetti and Feinstein (1990) suggest that ?Cobe supplemented, for each coding category, by two measures of agreement, positiveand negative, between the coders.
This means a total of 2m additional measures, whichwe believe are too many to gain a general insight into the meaning of the specific ?Covalue.
Alternatively, Byrt, Bishop, and Carlin (1993) suggest that intercoder reliabilitybe reported as three numbers: ?Co and two adjustments of ?Co, one with bias removed,the other with prevalence removed.
The value of ?Co adjusted for bias turns out tobe .
.
.
?S&C.
Adjusted for prevalence, ?Co yields a measure that is equal to 2P(A) ?
1.The results for Example 1 should then be reported as ?Co = 0.6724, ?S&C = 0.6632,2P(A)?1 = 0.6666; those for Example 6 as ?Co = 0.418, ?S&C = 0.27, and 2P(A)?1 = 0.3.For both Examples 3 and 4, 2P(A)?
1 = 0.8.
Collectively, these three numbers appearto provide a means of better judging the meaning of ?
values.
Reporting both ?and 2P(A) ?
1 may seem contradictory, as 2P(A) ?
1 does not correct for expectedagreement.
However, when the distribution of categories is skewed, this highlightsthe effect of prevalence.
Reporting both ?Co and ?S&C does not invalidate our previousdiscussion, as we believe ?Co is more appropriate for discourse- and dialogue-taggingin the majority of cases, especially when exploiting bias to improve coding (Wiebe,Bruce, and O?Hara 1999).AcknowledgmentsThis work is supported by grantN00014-00-1-0640 from the Office of NavalResearch.
Thanks to Janet Cahn and to theanonymous reviewers for comments onearlier drafts.ReferencesAllen, James and Mark Core.
1997.
DAMSL:Dialog act markup in several layers;Coding scheme developed by theparticipants at two discourse taggingworkshops, University of Pennsylvania,March 1996, and Schlo?
Dagstuhl,February 1997.
Draft.Bartko, John J. and William T. Carpenter.1976.
On the methods and theory ofreliability.
Journal of Nervous and MentalDisease, 163(5):307?317.Berry, Charles C. 1992.
The ?
statistic [letterto the editor].
Journal of the AmericanMedical Association, 268(18):2513?2514.Byrt, Ted, Janet Bishop, and John B. Carlin.1993.
Bias, prevalence, and kappa.
Journalof Clinical Epidemiology, 46(5):423?429.Carletta, Jean.
1996.
Assessing agreement onclassification tasks: The Kappa statistic.Computational Linguistics, 22(2):249?254.Carletta, Jean, Amy Isard, Stephen Isard,Jacqueline C. Kowtko, GwynethDoherty-Sneddon, and Anne H.Anderson.
1997.
The reliability of adialogue structure coding scheme.Computational Lingustics, 23(1):13?31.Cicchetti, Domenic V. and Alvan R.Feinstein.
1990.
High agreement but lowkappa: II.
Resolving the paradoxes.Journal of Clinical Epidemiology,43(6):551?558.Cohen, Jacob.
1960.
A coefficient ofagreement for nominal scales.
Educationaland Psychological Measurement, 20:37?46.Di Eugenio, Barbara.
2000.
On the usage ofKappa to evaluate agreement on codingtasks.
In LREC2000: Proceedings of theSecond International Conference on LanguageResources and Evaluation, pages 441?444,Athens.Di Eugenio, Barbara, Pamela W. Jordan,Richmond H. Thomason, and Johanna D.Moore.
2000.
The agreement process: An101Di Eugenio and Glass Kappa: A Second Lookempirical investigation of human-humancomputer-mediated collaborativedialogues.
International Journal of HumanComputer Studies, 53(6):1017?1076.Fleiss, Joseph L. 1971.
Measuring nominalscale agreement among many raters.Psychological Bulletin, 76(5):378?382.Goldman, Ronald L. 1992.
The ?
statistic[letter to the editor (in reply)].
Journal ofthe American Medical Association,268(18):2513?2514.Grove, William M., Nancy C. Andreasen,Patricia McDonald-Scott, Martin B. Keller,and Robert W. Shapiro.
1981.
Reliabilitystudies of psychiatric diagnosis: Theoryand practice.
Archives of General Psychiatry,38:408?413.Krippendorff, Klaus.
1980.
Content Analysis:An Introduction to Its Methodology.
SagePublications, Beverly Hills, CA.Rietveld, Toni and Roeland van Hout.
1993.Statistical Techniques for the Study ofLanguage and Language Behaviour.
Moutonde Gruyter, Berlin.Scott, William A.
1955.
Reliability of contentanalysis: The case of nominal scalecoding.
Public Opinion Quarterly,19:127?141.Siegel, Sidney and N. John Castellan, Jr.1988.
Nonparametric statistics for thebehavioral sciences.
McGraw Hill, Boston.Wiebe, Janyce M., Rebecca F. Bruce, andThomas P. O?Hara.
1999.
Developmentand use of a gold-standard data set forsubjectivity classifications.
In ACL99:Proceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics,pages 246?253, College Park, MD.
