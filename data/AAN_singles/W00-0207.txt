Generation from Lexical Conceptual StructuresDavid Traum and Nizar HabashUMIACS, University of Maryland{traum, habash}@cs, umd.
edu1 IntroductionThis paper describes a system for generating naturallanguage sentences from an interlingual representa-tion, Lexical Conceptual Structure (LCS).
This sys-tem has been developed as part of a Chinese-EnglishMachine Translation system, however, it promisesto be useful for many other MT language pairs.The generation system has also been used in Cross-Language information retrieval research (Levow etal., 2000).One of the big challenges in Natural Languageprocessing efforts is to be able to make use of ex-isting resources, a big difficulty being the sometimeslarge differences in syntax, semantics, and ontolo-gies of such resources.
A case in point is the in-terlingua representations u ed for machine transla-tion and cross-language processing.
Such represen-tations are becoming fairly popular, yet there arewidely different views about what these languagesshould be composed of, varying from purely concep-tual knowledge-representations, having little to dowith the structure of language, to very syntactic rep-resentations, maintaining most of the idiosyncrasiesof the source languages.
In our generation system wemake use of resources associated with two different(kinds of) interlingua structures: Lexical ConceptualStructure (LCS), and the Abstract Meaning Repre-sentations used at USC/ISI (Langkilde and Knight,1998a).2 Lexical Conceptual StructureLexical Conceptual Structure is a compositionalabstraction with language-independent propertiesthat transcend structural idiosyncrasies (Jackendoff,1983; Jackendoff, 1990; Jackendoff, 1996).
This rep-resentation has been used as the interlingua of sev-eral projects uch as UNITRAN (Dorr et al, 1993)and MILT (Dorr, 1997).An LCS is adirected graph with a root.
Each nodeis associated with certain information, including atype, a primitive and a field.
The type of an LCSnode is one of Event, State, Path, Manner, Propertyor Thing, loosely correlated with verbs prepositions,adverbs, adjectives and nouns.
Within each of thesetypes, there are a number of conceptual primitivesof that type, which are the basic building blocks ofLCS structures.
There are two general classes ofprimitives: closed class or structural primitive (e.g.,CAUSE, GO, BE, TO) and CONSTANTS, correspond-ing to the primitives for open lexical classes (e.g.,reduce+ed, textile+, slash+ingly).
I. Exam-ples of fields include Locational, Possessional,Identificational.
Children are also designatedas to whether they are subject, argument, ormodifier position.An LCS captures the semantics of a lexical itemthrough a combination of semantic structure (spec-ified by the shape of the graph and its structuralprimitives and fields) and semantic ontent (speci-fied through constants).
The semantic structure ofa verb is the same for all members of a verb class(Levin and Rappaport Hovav, 1995) whereas thecontent is specific to the verb itself.
So, all the verbsin the "Cut Verbs - Change of State" class have thesame semantic structure but vary in their semanticcontent (for example, chip, cut, saw, scrape, slashand scratch).The lexicon entry or Root LCS (RLCS) of onesense of the Chinese verb xuel_jian3 is as follows:(1)(act_on loc(* thing 1)(* thing 2)((* \[on\] 23) loc (*head*) (thing 24))(cut+ingly 26)(down+/m))The top node in the.
RLCS has the structuralprimitive ACT_ON in the locational field.
Its sub-ject is a star-marked LCS, meaning a subordinateRLCS needs to be filled in here to form a completeevent.
It also has the restriction that the filler LCSbe of the type thing.
The number "1" in that nodespecifies the thematic role: in this case, agent.
Thesecond child node, in argument position, needs tot Suffixes uch as ?, ?ed, +ingly are markers of the openclass of primitives, indicating the type52be of type thing too.
The number "2" stands fortheme.
The last two children specify the manner ofthe locat iona l  act_on, that is "cutting in a down-ward manner".
The RLCS for nouns are generallymuch simpler since they usually include only oneroot node with a primitive?
For instance (US+) or(quota+).The meaning of complex phrases is represented asa composed LCS (CLCS).
This is constructed "com-posed" from several RLCSes corresponding to in-dividual words.
In the composition process, whichstarts with a parse tree of the input sentence, allthe obligatory positions in the root and subordinateRLCS corresponding to lexical items are filled withother RLCSes from appropriately placed items in theparse tree.
For example, the three RLCSes we haveseen already can compose to give the CLCS in (2),corresponding~o the English sentence: United statescut down (the) quota.
(2)(act_on loc(us+)(quota+)((* \[on\] 23) loc (*head*) (thing 24))(cut+ingly 26)(dowa+/m))CLCS structures can be composed of differentsorts of RLCS structures, corresponding to differ-ent words.
A CLCS can also be decomposed on thegeneration side in different ways depending on theRLCSes of the lexical items in the target language.For example, the CLCS above will match a singleverb and two arguments when generated in Chinese(regardless ofthe input language).
But it will matchfour lexical items in English: cut, US, quota, anddown, since the RLCS for the verb "cut" in the En-glish lexicon, as shown in (3), does not include themodifier down.
(3)(act_on ioc(* thing 1)(* thing 2)((* \[on\] 23) loc (*head*) (thing 24))(cut+ingly 26) )The rest of the examples in this paper will referto the slightly more complex CLCS shown in (4),corresponding to the English sentence The UnitedStates unilaterally reduced the China textile exportquota This LCS is presented without all the addi-tional features for sake of clarity.
Also, it is actuallyone of eight possible LCS compositions produced bythe analysis component from the input Chinese sen-tence.
(4)(cause (us+)(go ident (quota+ (china+)(textile+)(export+))(to ident (quota+ (china+)(textile+)(export+))(at ?dent (quota+ (china+)(textile+)(export+))(reduce+ed))))(with instr (*HEAD*) nil)(unilaterally+/m))3 The Generation SystemSince this generation system was developed in tan-dem with the most recent LCS composition system,and LCS-language and specific lexicon extensions,a premium was put on the ability for experimenta-tion along a number of parameters and rapid ad-justment on the basis of intermediate inputs and re-sults to the generation system.
This goal encour-aged a modular design, and made lisp a convenientlanguage for implementation.
We were also able tosuccessfully integrate components from the NitrogenGeneration System (Langkilde and Knight, 1998a;Langkilde and Knight, 1998b).CLCSI ~.Processing!Lexical Access" Alignmen~DecompositionsJ^M, I CreationLexical ChoiceLCS-Amr1,,J/Lineatizafionand MorphologylNITROGEN big'turn Iwel'em~nc~RealizationI EnglishfFigure 1: Generation System ArchitectureThe architecture of the generation system isshown in Figure 1, showing the main modules andsub-modules and flow of information between them.The first main component translates, with the use ofa language specific lexicon, from the LCS interlinguato a language-specific representation of the sentencein a modified form of the AMR-interlingua, usingwords and features specific to the target language,but also including syntactic and semantic informa-tion from the LCS representation.
The second maincomponent produces target language sentences from53this intermediate representation.
We will now de-scribe each of these components in more detail.The input to the generation component is a text-representation f a CLCS, the Lexical ConceptualStructure corresponding to a natural language sen-tence.
The particular format, known as long-handis equivalent o the form shown in (4), but mak-ing certain information more explicit and regular(at the price of increased verbosity).
The Long-hand CLCS can either be a fully language-neutralinterlingua representation, or one which still incor-porates some aspects of the source-language inter-pretation process.
This latter may include grammat-ical features on LCS nodes, but also nodes, known asfunctional nodes, which correspond to words in thesource language but are not LCS-nodes themselves,serving merely as place-holders for feature informa-tion.
Examples of these nodes include punctuationmarkers, coordinating conjunctions, grammatical s-pec t markers, and determiners.
An additional exten-sion of the LCS input language, beyond traditionalLCS is the in-place representation f an ambiguoussub-tree as a POSSIBLES node, which has the variouspossibilities represented as its own children.Thus, for example, the following structure (withsome aspects elided for brevity) represents a nodethat could be one of three possibilities.
In the secondone, the root of the sub-tree is a functional node,passing its features to its child, COUNTRY+:(5)(:POSSIBLES -2589104(MIDDLE+ (COUNTRY+ (DEVELOPING+/P)))(FUNCTIONAL (PDSTPOSITION AMONG)(COUNTRY+ (DEVELOPING+/P)))(CHINA+ (COUNTRY+ (DEVELOPING+/P))))3.1 Lexical ChoiceThe first major component, divided into fourpipelined sub-modules, as shown in Figure 1 trans-forms a CLCS structure to what we call an LCS-AMR structure, using the syntax of the abstractmeaning representation (AMR), used in the Nitro-gen generation system, but with words already cho-sen (rather than more abstract Sensus ontology con-cepts), and also augmented with information fromthe LCS that is useful for target language realiza-tion.3.1.1 Pre-ProcessingThe pre-processing phase converts the text input for-mat into internal graph representations, for efficientaccess of components (with links for parents as wellas children), also doing away with extraneous source-language features, converting, for example, (5) to re-move the functional node and promote COUNTRY+ tobe one of the possible sub-trees.
This involves a top-down ,reversal of the tree, including some complex-ities when functional nodes without children (whichthen assign features to their parents) are direct chil-dren of possibles nodes.3.1.2 Lexical AccessThe lexical access phase compares the internal CLCSform to the target language lexicon, decorating theCLCS tree with the RLCSes of target languagewords which are likely to match sub-structures ofthe CLCS.
In an off-line processing phase, the tar-get language lexicon is stored in a hash-table, witheach entry keyed on a designated primitive whichwould be a most distinguishing node in the RLCS.On-line decoration then proceeds in two step pro-cess, for each node in the CLCS:(6) a. look for RLCSes stored in the lexicon underthe CLCS node's primitivesb.
store retrieved RLCSes at the node in theCLCS that matches the root of this RLCSFigure 2 shows some of the English entries match-ing the CLCS in (4).
For most of these words, thedesignated primitive is the only node in the corre-sponding LCS for that entry.
For reduce, however,reduce+ed is the designated primitive.
While thiswill be retrieved in step (6) while examining thereduce+ed node from (4), in (6)b, the LCS for "re-duce" will be stored at the root node of (4) (cause).
( : DEF_WORD "reduce": CLASS "45.4. a":THETA_ROLES ( (I "_ag_th, instr (with)") ):LCS (cause (* thing I)(go ident (* Zhing 2)(toward ident (thing 2)(at ident (thing 2)(reduce+ed 9) ) ) )((* with 19) instr (*head*)(thin E 20) )):VAR_SPEC ((1 (animate +))))(:DEF_WORD .
"US" :LCS (US+ 0))(:DEF_WORD "China" :LCS (China+ 0))(:DEF_WORD "quota" :5CS (quota+ 0))(:DEF_WORD "WITH":LCS (with instr (thing 2) (* thing 20)))(: DEF_WORD "unilaterally":LCS (unilaterally+/m 0))Figure 2: Lexicon entriesB4The current English lexicon contains over 11000RLCS entries such as those in Figure 2, includingover 4000 verbs and 6200 unique primitive keys inthe hash-table.3.1.3 A l ignment /Decompos i t ionThe heart of the lexical access algorithm is the de-composition process.
This algorithm attempts toalign RLCSes selected by the lexical access portionwith parts of the CLCS, to find a complete cover-ing of the CLCS graph.
The main algorithm is verysimilar to that described in (Dorr, 1993), howeverwith some extensions to be able to also deal withthe in-place ambiguity represented by the possiblesnodes.The algorithm recursively checks a CLCS nodeagainst corresponding RLCS nodes coming from thelexical entries-retrieved and stored in the previousphase.
If significant incompatibilities are found, thelexical entry is discarded.
If all (obligatory) nodesin the RLCS match against nodes in the CLCS,then the rest of the CLCS is recursively checkedagainst other lexical entries stored at the remain-ing unmatched CLCS nodes.
Some nodes, indicatedwith a "*", as in Figure 2, require not just a matchagainst the corresponding CLCS node, but also amatch against another lexical entry.
Some CLCSnodes must thus match multiple RLCS nodes.
ACLCS node matches an RLCS node, if the followingconditions hold:(7) a.b.C.d.e.the primitives are the same (or primitive forone is a wild-card, represented as nil)the types (e.g., thing, event, state, etc.)
arethe samethe fields (e.g., identificational, possessive,locational, etc) are the samethe positions (e.g., subject, argument, ormodifier) are the sameall obligatory children of the RLCS nodehave corresponding matches to children ofthe CLCSSubject and argument children of an RLCS nodeare obligatory unless specified as optional, whereasmodifiers are optional unless specified as obliga-tory.
In the RLCS for " reduce"  in Figure 2,the nodes corresponding to agent and theme (num-bered 1 and 2, respectively) are obligatory, whilethe instrument (the node numbered 19) is optional.Thus, even though in (4) there is no matching lexicalentry for the node in Figure 2 numbered 20 ("*"-marked in the RLCS for "with"), the main RLCSfor ' ' reduce'   is allowed to match, though with-out any realization for the instrument.A complexity in the algorithm occurs when thereare multiple possibilities filling in a position in aCLCS.
in this case, only one of these possibilitiesis requirea to match all the corresponding RLCSnodes in order for a lexical entry to match.
In thecase where there are some of these possibilities thatdo not match any RLCS nodes (meaning there areno target-language realizations for these constructs),these possibilities can be pruned at this stage.
Onthe other hand, ambiguity can also be introduced atthe decomposition stage, if multiple lexical entriescan match a single structureThe result of the decomposition process is amatch-structure indicating the hierarchical relation-ship between all lexical entries, which, together coverthe input CLCS.3.1.4 LCS-AMR Creat ionThe match structure resulting from decompositionis then converted into the appropriate input formatused by the Nitrogen generation system.
Nitrogen'sinput, Abstract Meaning Representation (AMR), isa labeled directed graph written using the syntaxfor the PENMAN Sentence Plan Language (Penman1989).
the structure of an AMR is basically as in (8).
(8) AMR = <concept> I (<label> {<role><AMR>}+)Since the roles expected by Nitrogen's Englishgeneration grammar do not match well with the the-matic roles and features of a CLCS, we have ex-tended the AMR language with LCS-specific rela-tions, calling the result, an LCS-AMR.
To distin-guish the LCS relations from those used by Nitro-gen, we mark most of the new roles with the prefix: LCS-.
Figure 3 shows the LCS-AMR correspondingto the CLCS in (4).In the above example, the basic role / is usedto specify an instance.
So, the LCS-AMR can beread as an instance of the concept Ireduce I whosecategory is a verb and is in the active voice.
More-over, Ireducel has two thematic roles related to it, anagent and a theme; and it is modified by the conceptlunilaterally\].
The different roles modifying Ireduce Icome from different origins.
The :LCS-NODE valuecomes directly from the unique node number in theinput CLCS.
The category, voice and telicity are de-rived from features of the LCS entry for the verbIreduce\] in the English lexicon.
The specificationsof agent and theme come from the LCS represen-tation of the verb reduce in the English lexicon aswell, as can be seen by the node numbers 1 and 2, inthe lexicon entry in Figure 2.
The role :LCS-MOD-MANNER is derived by combining the fact that thecorresponding AMR had a modifier ole in the CLCSand because its type is a Manner.3.2 Real izationThe LCS-AMR representation is then passed to therealization module.
The strategy used by Nitrogen is55(a7537 / lreducel:LCS-NODE 6253520:LCS-V01CE ACTIVE:CAT V:TELIC +:LCS-AG (a7538 / \[United States\[:LCS-NODE 6278216:CAT N):LCS-TH (a7539 / ~quota\[:LCS-NODE 6278804:CAT N:LCS-MOD-THING (a7540 / \[china\[:LCS-NODE 6108872:CAT N):LCS-MOD-THING (a7541 / \[textile\[:LCS-NODE 6111224-- :CAT N):LCS-MOD-THING (a7542 / \[exportl:LCS-NODE 6112400:CAT N)):LCS-MOD-MANNER (a7543 / \[unilaterally\[:LCS-NODE 6279392:CAT ADV))Figure 3: LCS-AMRto over-generate possible sequences of English fromthe ambiguous or under-specified AMRs and thendecide amongst hem based on bigram frequency.The interface between the Linearization module andthe Statistical Extraction module is a word latticeof possible renderings.
The Nitrogen package of-fers support for both subtasks, Linearization andStatistical Extraction.
Initially, we used the Nitro-gen grammar to do Linearization.
But complexitiesin recasting the LCS-AMR roles as standard AMRroles as well as efficiency considerations compelledus to create our own English grammar implementedin Lisp to generate the word lattices.3.2.1 LinearizatlonIn this module, we force linear order on the un-ordered parts of an LCS-AMR.
This is done byrecursively calling subroutines that create variousphrase types (NP, PP, etc.)
from aspects of the LCS-AMR.
The result of the linearization phase is a wordlattice specifying the sequence of words that makeup the resulting sentence and the points of ambigu-ity where different generation paths are taken.
(9)shows the word lattice corresponding to the LCS-AMR in (8).
(9) (SEQ (WRD "*start-sentence*" BOS) (WRD"united states" NOUN) (WRD "unilaterally"ADJ) (WRD "reduced" VERB) (OR (WRD"the" ART) (WRD "a" ART) (WRD "an"ART)) (WRD "china" ADJ) (OR (SEQ (WRD"export" ADJ) (WRD "textile" ADJ)) (SEQ(WRD "textile" ADJ) (WRD "export" ADJ)))(WRD "quota" NOUN) (WRD "."
PUNC)(WRD "*end-sentence*" EOS))The keyword SEQ specifies that what follows it isa list of words in their correct linear order.
The key-word OR specifies the existence of different paths forgeneration.
In the above example, the word 'quota'gets all possible determiners since its definiteness isnot specified.
Also, the relative order of the words'textile' and 'export' is not resolved so both possi-bilities are generated.Sentences were realized according to the patternin (10).
That is, first subordinating conjunctions,if any, then modifiers in the temporal field (e.g.,"now", "in 1978"), then the first thematic role, thenmost other modifiers, the verb (with collocations ifany) then spatial modifiers ("up", "down"), then thesecond and third thematic roles, followed by prepo-sitional phrases and relative sentences.
Nitrogen'smorphology component was also used, e.g., to givetense to the head verb.
In the example above, sincethere was no tense specified in the input LCS, pasttense was used on the basis of the telicity of the verb.
(10) (Sconj ,) (temp-mod)* Whl (Mods)* V (coll)(stood)* (Th2)+ (Th3)+ (PP)* (RelS)*There is no one-to-one mapping between a partic-ular thematic role and an argument position.
Forexample, a theme can be the subject in some casesand it can be the object in others or even an oblique.Observe "cookie" in i l l) .i l l )  a. John ate a cookie (object)b. the cookie contains chocolate (subject)c. she nibbled at a cookie (oblique)Thematic roles are numbered for their correct re-alization order, according to the hierarchy for argu-ments hown in (12).
(12) agent > instrument > theme > perceived >( everythin gel se )So, in the case of the occurrence of theme alone,it is mapped to first argument position.
If a themeand an agent occur, the agent is mapped to first ar-gument position and the theme is mapped to secondargument position.
A more detailed discussion isavailable in (Doff et al, 1998).
For the LCS-AMR inFigure 3, the thematic hierarchy is what determinedthat the lunited statesl is the subject and Iquotal isthe object of the verb Ireducel.In our input CLCSs, in most cases little hierarchi-cal information was given about multiple modifiersof a noun.
Our initial, brute force, solution was to56generate all permutations and depend on statisti-cal extraction to decide.
This technique Worked fornoun phrases of about 6 words, but was too costlyfor larger phrases (of which there were several ex-amples in our test corpus).
This cost was alleviatedto some degree, also providing slightly better esultsthan pure bigram selection by labelling adjectives inthe English lexicon as belonging to one of severalordered classes, inspired by the adjective orderingscheme in (Quirk et al, 1985).
This is shown in(13).
(13) a. Determiner (all, few, several, some, etc.)b.
Most Adjectival (important, practical, eco-nomic, etc.)c.
Age (old, young, etc.)d.
Color (black, red, etc.)e.
Participle (confusing, adjusted, convincing,decided)f. Provenance (China, southern, etc.)g.
Noun (Bank_of_China, difference, memoran-dum, etc.)h.
Denominal (nouns made into adjectives byadding-al, e.g., individual, coastal, annual,etc.
)If multiple words fall within the same group, per-mutations are generated for them.
This situationcan be seen for the LCA-AMR in Figure 3 with theordering of the modifiers of the word I quota\]: I chinal,lexportl and Itextilel.
Ichinal fell within the Prove-nance class of modifiers which gives it precedenc eover the other two words.
They, on the other hand,fell in the Noun class and therefore both permuta-tions were passed on to the statistical component.3.2.2 Statistical PreferencesThe final step, extracting a preferred sentence fromthe word lattice of possibilities is done using Ni-trogen's Statistical Extractor without any changes.Sentences are scored using uni and bigram frequen-cies calculated based on two years of Wall StreetJournal (Langkilde and Knight, 1998b).4 Dealing with AmbiguityA major issue in sentence generation from an inter-lingua or conceptual structure, especially as part of amachine translation project, is how and when to dealwith ambiguity.
There are several different sourcesof ambiguity in the generation process outlined inthe previous ection.
Some of these include:?
ambiguity in source language analysis (as repre-sented by possibles nodes in the CLCS input tothe Generation system).
This can include am-biguity between multiple concepts, such as theexample in (5), LCS type/structure ( .g., thingor event, which field), or structural ambiguity(subject, argument or modifier).ambiguity introduced in lexical choice (whenmultiple match structures can cover a singleCLCS)ambiguity introduced in realization (when mul-tiple orderings are possible, also multiple mor-phological realizations)There are also several types of strategies for ad-dressing ambiguity at various phases, including:?
passing all possible structures down for furtherprocessing stages to deal with?
filtering based on "soft" preferences (only passthe highest set of candidates, according to somemetric)?
quota-based filtering, passing only the top Ncandidates?
threshold filtering, passing only candidates thatexceed a fixed threshold (either score or binarytest)The generation system uses a combination ofthesestrategies, at different phases in the processing.
Am-biguous CLCS sub-trees are sometimes annotatedwith scores based on preference ofattachment asanargument rather than a modifier.
The alignment al-gorithm can be run in either of two modes, one whichselects only the top scoring possibility for which amatching structure can be found, and one in whichall possible structures are passed on, regardless ofscore.
The former method is the only one feasiblewhen given very large (e.g., over 1 megabyte textfiles) CLCS inputs.
Also at the decomposition level,soft preferences are used in that missing lexical en-tries can be hypothesized tocover parts of the CLCS(essentially "making up" words in the target lan-guage).
This is done, however, only when no le-gitimate matches are found using only the availablelexical entries.
At the linearization phase, there areoften many choices for ordering of modifiers at thesame level.
As mentioned in the previous ection,we are experimenting with separating these into po-sitional classes, but our last resort is to pass alongall permutations of elements in each sub-class.
Theultimate arbiter is the statistical extractor, whichorders and presents the top scoring realizations.5 In ter l ingua l  representat ion  i ssuesOne issue that needs to be confronted in an Inter-lingua such as LCS is what to do when linguisticstructure of languages vary widely, and useful con-ceptual structure may also diverge from these.
A57case in point is the representation f numbers.
Lan-guages diverge widely as to which numbers are prim-itive terms, and how larger numbers are built com-positionaUy through modification (e.g., multiplica-tion and addition).
One question that immediatelycomes up is whether an interlingua such as LCSshould represent numbers according to the linguis-tic structure of the source language (or some partic-ular designated natural anguage) or as some otherinternal numerical form, (e.g.
decimal numerals).Likewise, on generation i to a target language, howmuch of the structure of the source language shouldbe kept, especially when this is not the most nat-ural way to group things in the target language.One might be tempted to always convert o a stan-dard interlingua representation f numbers, howeverthis does los_e some possible classification i to groupsthat might be present in the input (contrast in En-glish: "12 pair" with "2 dozen".In our Chinese-English efforts, such issues cameup, since the natural multiplication points in Chi-nese were 100, 10,000, and 100,000,000, rather than100, 1000, and 1,000,000, as in English.
Our provi-sional solution is to propogate the source languagemodification structure all the way through the LCS-AMR stage, and include special purpose rules look-ing for the "Chinese" numbers and multiplying themtogether to get numerals, and then divide and real-ize in the English fashion.
E.g., using the wordsthousand, million, and billion.6 EvaluationSo far most of the evaluation has been fairly small-scale and fairly subjective, generating English sen-tences from CLCSs produced from about 80 sen-tences.
Evaluation in this case is difficult, becausethe ultimate criteria is translation quality, whichcan, itself, be difficult to judge, but, moreover, itcan be hard to attribute specific deficits to the anal-ysis phase, the lexical resources, or the generationsystem proper.
So far results have been mostly ad-equate, even for large and fairly complex sentences,taking less than 1 minute for generation up to inputsof about 1 megabyte input CLCS files.
Ambiguityand complexity beyond that level tends to overtaxthe generation system.For the most part, the over-generation strategy ofNitrogen, coupled with the bigram preferences worksvery well.
There are still some difficulties, however.One major one is that, especially with its bias forshorter sentences, fluency is given preference overtranslation fidelity.
Thus, if there are options ofwhether or not to express ome optional informa-tion, this will tend to be left out.
Also, bigrams areobviously inadequate for capturing long-distance d -pendencies, and so, if things like agreement are notcarefully controlled in the symbolic omponent, theywill be incorrect in some cases.The generation component has also been used ona broader scale, generating thousands of simple sen-tences - at least one for each verb in the EnglishLCS lexicon, creating sentence templates to be usedin a Cross-Language information retrieval system(Levow et al, 2000).7 Future WorkThe biggest remaining step is a more careful evalu-ation of different sub-systems and preference strate-gies to more efficiently process very ambiguous andcomplex inputs, without substantially sacrificingtranslation quality.
Also a current research topicis how to combine other metrics coming from vari-ous points in the generation process with the bigramstatistics, to result in better overall outputs.Another topic of interest is developing other lan-guage outputs.
Most of the subcomponents arelanguage-independent.
The realization componentsbeing an obvious exception.
In particular, thepre-processing algorithm is completely language-independent.
The lexical access algorithm is lan-guage independent, although it requires a target-language lexicon, which of course is language de-pendent.
The alignment algorithm is also com-pletely language independent.
The lcs-amr creationlanguage is mostly language independent, howeverthere may not be sufficient features added to thelanguage and extracted from the LCS-AMR for fullgeneration of some other languages.
Some targetlanguages might require some extensions to the out-put language and new rules to extract his informa-tion from the LCS.
The realization process is mostlylanguage dependent.
The current linearizaton mod-ule is very dependent on the structure of English.We are, however working on a future version of thiscomponent splitting the linearization task into lan-guage independent processes and grammar compil-ers, and independent language-specific output gram-mars.
Nitrogen's realizer, also, is algorithmicallylanguage-independent, however one would need atarget language database for realization in anotherlanguage.AcknowledgementsThis work was supported by the US Department ofDefense through contract MDA904-96-I:t-0738.
TheNitrogen system used in the realization process wasprovided by USC/ISI, we would like to thank KevenKnight and Irene Langkilde for help and advice inusing it.
The adjective classifications described inSection 3 were devised by Carol Van Ess-Dykema.David Clark and Noah Smith worked on previousversions of the system, and we are indebted to Someof their ideas for the current implementation.
Wewould also like to thank the CLIP group at Uni-58veristy of Maryland, especially Ron Dolanl ~ BonnieDorr, Gina Levow, Mari Olsen, Wade sti~fi, AmyWeinberg, for helpful input and feedback on the gen-eration system.ReferencesBonnie J. Dorr, James Hendler, Scott Blanksteen,and Barrie Migdaloff.
1993.
Use of Lexical Con-ceptual Structure for Intelligent Tutoring.
Tech-nical Report UMIACS TR 93-108, CS TR 3161,University of Maryland.Bonnie J. Dorr, Nizar Habash, and David Traum.1998.
A Thematic Hierarchy for Efficient Gener-ation from Lexical-Conceptal Structure.
In Pro-ceedings of the Third Conference of the Associ-atio.n for Machine Translation in the Americas,AMTA-#8, in Lecture Notes in Artificial Intelli-gence, 15~9, pages 333-343, Langhorne, PA, Oc-tober 28-31.Bonnie J. Dorr.
1993.
Machine Translation: A Viewfrom the Lexicon.
The MIT Press.Bonnie J. Dorr.
1997.
Large-Scale Acquisition ofLCS-Based Lexicons for Foreign Language Tutor-ing.
In Proceedings of the A CL Fifth Conferenceon Applied Natural Language Processing (ANLP),pages 139-146, Washington, DC.Ray Jackendoff.
1983.
Semantics and Cognition.The MIT Press, Cambridge, MA.Ray Jackendoff.
1990.
Semantic Structures.
TheMIT Press, Cambridge, MA.Ray Jackendoff.
1996.
The Proper Treatment ofMeasuring Out, Telicity, and Perhaps Even Quan-tification in English.
Natural Language and Lin-guistic Theory, 14:305-354.Irene Langkilde and Kevin Knight.
1998a.
Gen-eration that Exploits Corpus-Based StatisticalKnowledge.
In Proceedings of COLING-A CL '98,pages 704-710.Irene Langkilde and Kevin Knight.
1998b.
ThePractical Value of N-Grams in Generation.
In In-ternational Natural Language Generation Work-shop.Beth Levin and Malka Rappaport Hovav.
1995.
Un-aecusativity: At the Syntaz-Lezical Semantics In-terface.
The MIT Press, Cambridge, MA.
LIMonograph 26.Gina Levow, Bonnie J. Dorr, and Dekang Lin.
2000.Construction of chinese-english emantic hierar-chy for cross-language retrieval, forthcoming.Randolph Quirk, Sidney Greenbaum, GeoffreyLeech, and Jan Svartvik.
1985.
A Comprehen-sive Grammar of the English Language.
Longman,London.59
