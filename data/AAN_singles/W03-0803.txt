OLLIE: On-Line Learning for Information ExtractionValentin Tablan, Kalina Bontcheva, Diana Maynard, Hamish CunninghamUniversity of SheffieldRegent Court, 211 Portobello St.Sheffield S1 4DP, UK{V.Tablan,diana,kalina,hamish}@dcs.shef.ac.ukAbstractThis paper reports work aimed at develop-ing an open, distributed learning environ-ment, OLLIE, where researchers can ex-periment with different Machine Learning(ML) methods for Information Extraction.Once the required level of performance isreached, the ML algorithms can be used tospeed up the manual annotation process.OLLIE uses a browser client while datastorage and ML training is performed onservers.
The different ML algorithms usea unified programming interface; the inte-gration of new ones is straightforward.1 IntroductionOLLIE is an on-line application for corpus annota-tion that harnesses the power of Machine Learning(ML) and Information Extraction (IE) in order tomake the annotator?s task easier and more efficient.A normal OLLIE working session starts with theuser uploading a set of documents, selecting whichML method to use from the several supplied by thesystem, choosing the parameters for the learningmodule and starting to annotate the texts.
Duringthe initial phase of the manual annotation process,the system learns in the background (i.e.
on theserver) from the user?s actions and, when a certaindegree of confidence is reached, it starts making sug-gestions by pre-annotating the documents.
Initially,some of these suggestions may be erroneous but, asthe user makes the necessary corrections, the systemwill learn from its mistakes and the performance willincrease leading to a reduction in the amount of hu-man input required.The implementation is based on a client-server ar-chitecture where the client is any Java-enabled webbrowser and the server is responsible for storingdata, training ML models and providing access ser-vices for the users.The client side of OLLIE is implemented as a setof Java Server Pages (JSPs) and a small number ofJava applets are used for tasks where the user inter-face capabilities provided by HTML are not enough.The server side comprises a JSP/servlet server,a relational database server and an instance of theGATE architecture for language engineering whichis used for driving all the language-related process-ing.
The general architecture is presented in Figure1.The next section describes the client side of theOLLIE system while Section 3 details the imple-mentation of the server with a subsection on the inte-gration of Machine Learning.
Section 4 talks aboutsecurity; Section 6 about future improvements andSection 7 concludes the paper.2 The OLLIE clientThe OLLIE client consists of several web pages,each of them giving the user access to a particularservice provided by the server.One such page provides facilities for uploadingdocuments in the system from a URL, a local file,or created from text pasted in a form.
A varietyof formats including XML, HTML, email and plaintext are supported.
When a document is created, theFigure 1: The general architecture of OLLIEoriginal markup ?if present?
is separated from tex-tual content to prevent it from interfering with thesubsequent language processing.Another page lets the user choose which machinelearning algorithm is to be used, the attributes thatcharacterise the instances (e.g., orthography, part-of-speech), and parameters for the chosen learningmethod (e.g., thresholds, smoothing values).
Theclasses to be learnt (e.g., Person, Organisation) areprovided as part of the user profile, which can beedited on a dedicated page.
All ML methods com-patible with OLLIE have a uniform way of describ-ing attributes and classes (see Section 3.1 for moredetails on the ML integration); this makes possiblethe use of a single user interface for all the ML al-gorithms available.
The fine-tuning parameters arespecific to each ML method and, although the MLmethods can be run with their default parameters, asestablished by (Daelemans and Hoste, 2002), sub-stantial variation in performance can be obtained bychanging algorithm options.Since OLLIE needs to support the users with theannotation process by learning in the backgroundand suggesting annotations, it offers control overthe accuracy threshold for these suggestions.
Thisavoids annoying the users with wrong suggestionswhile assuring that suggestions the system is confi-dent about are used to pre-annotate the data, reduc-ing the workload of the user.The document editor can then be used to annotatethe text (see Figure 2).
The right-hand side showsthe classes of annotations (as specified in the userprofile) and the user selects the text to be annotated(e.g., ?McCarthy?)
and clicks on the desired class(e.g., Person).
The new annotation is added to thedocument and the server is updated immediately (sothe new data becomes available to the ML algorithmtoo).
The document editor also provides facilitiesfor deleting wrong annotations, which are then prop-agated to the server, in a similar way.The graphical interface facilities provided by aweb browser could be used to design an interface forannotating documents but that would mean stretch-ing them beyond their intended use and it is hard tobelieve that such an interface would rate very highon a usability scale.
In order to provide a more er-gonomic interface, OLLIE uses a Java applet thatintegrates seamlessly with the page displayed by thebrowser.
Apart from better usability, this allows forgreater range of options for the user.The communication between the editor applet andthe server is established using Java Remote MethodInvocation (a protocol similar to the C++ RemoteProcedure Call ?
RPC) which allows the instant no-tification when updates are needed for the documentstored on the server.
The continuous communicationbetween the client and the server adds the benefit ofdata security in case of network failure.
The dataon the server always reflects the latest version of thedocument so no data loss can occur.
The sessiondata stored by the server expires automatically afteran idle time of one hour.
This releases the resourcesused on the server in case of persistent network fail-ures.The data structures used to store documents on theserver are relatively large because of the numerousindices stored to allow efficient access to the annota-tions.
The copy downloaded by the client when theannotation process is initiated is greatly reduced byfiltering out all the unnecessary information.
Mostof the data transferred during the client-server com-munication is also compressed, which reduces thelevel of network traffic ?
always a problem in clientserver architectures that run over the Internet.Figure 2: Annotating text in the OLLIE clientAnother utility provided by the client is a pagethat lets the user specify the access rights to the doc-ument/corpus, which determine whether it can beshared for viewing or collaborative annotation (seeSection 4 for details on security).3 Implementation of the OLLIE serverWhile the client side of the OLLIE application ispresented as set of web pages, the server part isbased on the open source GATE architecture.GATE is an infrastructure for developing and de-ploying software components that process humanlanguage (Cunningham et al, 2002).
It is writtenin Java and exploits component-based software de-velopment, object orientation and mobile code.
Onequality of GATE is that it uses Unicode through-out (Tablan et al, 2002).
Its Unicode capabilitieshave been tested on a variety of Slavic, Germanic,Romance, and Indic languages (Gamba?ck and Ols-son, 2000; Baker et al, 2002).
This allows OL-LIE to handle documents in languages other thanEnglish.
The back-end of OLLIE uses the GATEframework to provide language processing compo-nents, services for persistent storage of user data,security, and application management.When a document is loaded in the OLLIE clientand subsequently uploaded to the server, its formatis analysed and converted into a GATE documentwhich consists of textual content and one or morelayers of annotation.
The annotation format is amodified form of the TIPSTER format (Grishman,1997), is largely isomorphic with the Atlas format(Bird and Liberman, 1999) and successfully sup-ports I/O to/from XCES and TEI (Ide et al, 2000).1An annotation has a type, a pair of nodes pointingto positions inside the document content, and a setof attribute-values, encoding further linguistic infor-mation.
Attributes are strings; values can be anyJava object.
An annotation layer is organised as aDirected Acyclic Graph on which the nodes are par-ticular locations in the document content and thearcs are made out of annotations.
All the markupcontained in the original document is automaticallyextracted into a special annotation layer and can beused for processing or for exporting the documentback to its original format.1The American National Corpus is using GATE for a largeTEI-based project.Linguistic data (i.e., annotated documents andcorpora) is stored in a database on the server (seeFigure 1), in order to achieve optimal performance,concurrent data access, and persistence betweenworking sessions.One of the most important tasks for the OLLIEserver is the execution and control of ML algo-rithms.
In order to be able to use ML in OLLIE,a new processing resource was designed that addsML support to GATE.3.1 Machine Learning SupportOur implementation for ML uses classification al-gorithms for which annotations of a given type areinstances while the attributes for them are collectedfrom the context in which the instances occur in thedocuments.Three types of attributes are defined: nominal,boolean and numeric.
The nominal attributes cantake a value from a specified set of possible valueswhile the boolean and numeric ones have the usualdefinitions.When collecting training data, all the annotationsof the type specified as instances are listed, and foreach of them, the set of attribute values is deter-mined.
All attribute values for an instance refer tocharacteristics of a particular instance annotation,which may be either the current instance or one sit-uated at a specified relative position.Boolean attributes refer to the presence (or ab-sence) of a particular type of annotation overlappingat least partially with the required instance.
Nominaland numeric attributes refer to features on a partic-ular type of annotation that (partially) overlaps theinstance in scope.One of the boolean or nominal attributes ismarked as the class attribute, and the values whichthat attribute can take are the labels for the classesto be learnt by the algorithm.
Figure 3 depicts sometypes of attributes and the values they would takein a particular example.
The boxes represent an-notations, Token annotations are used as instances,the one in the centre being the current instance forwhich attribute values are being collected.Since linguistic information, such as part-of-speech and gazetteer class, is often used as at-tributes for ML, OLLIE provides support for iden-tifying a wide range of linguistic information - part-of-speech, sentence boundaries, gazetteer lists, andnamed entity class.
This information, together withtokenisation information (kind, orthography, and to-ken length) is obtained by using the language pro-cessing components available with GATE, as part ofthe ANNIE system (Cunningham et al, 2002).
SeeSection 5 for more details on the types of linguisticfeatures that can be used.
The user chooses which ofthis information is to be used as attributes.An ML implementation has two modes of func-tioning: training ?
when the model is being built,and application ?
when the built model is used toclassify new instances.
Our implementation consistsof a GATE processing resource that handles both thetraining and application phases.
It is responsible fordetecting all the instances in a document and col-lecting the attribute values for them.
The data thusobtained can then be forwarded to various externalimplementations of ML algorithms.Depending on the type of the attribute that ismarked as class, different actions will be performedwhen a classification occurs.
For boolean attributes,a new annotation of the type specified in the attributedefinition will be created.
Nominal attributes triggerthe addition of the feature specified in the attributedefinition on an annotation of the required type sit-uated at the position of the classified instance.
If nosuch annotation is present, it will be created.Once an ML model is built it can be stored as partof the user profile and reloaded for use at a later time.The execution of the ML processing resource iscontrolled through configuration data that selects thetype of annotation to be used as instances, defines allthe attributes and selects which ML algorithm willbe used and with what parameters.One good source of implementations for manywell-known ML algorithms is the WEKA library(Witten and Frank, 1999).2 It also provides a wealthof tools for performance evaluation, testing, and at-tribute selection, which were used during the devel-opment process.OLLIE uses the ML implementations provided byWEKA which is accessed through a simple wrap-per that translates the requests from GATE into APIcalls ?understood?
by WEKA.
The main types of re-quests dealt with by the wrapper are the setting of2WEKA homepage: http://www.cs.waikato.ac.nz/ml/weka/Figure 3: Example of attributes and their values.configuration data, the addition of a new training in-stance and the classification of an application-timeinstance.4 SecurityBecause OLLIE is deployed as a web application?
accessible by anyone with Internet access, secu-rity is an important issue.
Users store documents onthe server and the system also keeps some personaldata about the users for practical reasons.3 All usersneed to be provided with a mechanism to authen-ticate themselves to the system and they need to beable to select who else, apart from them, will be ableto see or modify the data they store on the server.Every user has a username and a password, usedto retrieve their profiles and determine which doc-uments they can access.
The profiles also containinformation specifying the types of annotations thatthey will be creating during the annotation process.For example, in the case of a basic named entity3Storing email addresses for instance is useful for sendingpassword reminders.recognition task, the profile will specify Person, Or-ganisation, and Location.
These tags will then beprovided in the document annotation pages.The access rights for the documents are handledby GATE which implements a security model simi-lar to that used in Unix file systems.
Table 1 showsthe combination of rights that are possible.
Theygive a good granularity for access rights, rangingfrom private to world readable.The set of known users is shared between GATEand OLLIE and, once a user is authenticated withthe system, the login details are kept in session datawhich is stored on the OLLIE server.
This allows forautomatic logins to the underlying GATE platformand transparent management of GATE sessions.5 ML Experiments and EvaluationTo the end of evaluating the suitability of the MLalgorithms provided by WEKA for use in OLLIEwe performed several experiments for named entityrecognition on the MUC-7 corpus (SAIC, 1998).
Weconcentrated on the recognition of ENAMEX enti-User User?s Group Other UsersMode Read Write Read Write Read Write?World Read/Group Write?
+ + + + + -?Group Read/Group Write?
+ + + + - -?Group Read/Owner Write?
+ + + - - -?Owner Read/Owner Write?
+ + - - - -Table 1: Security model ?
the access rightsties, i.e., Person, Organisation, and Location.
TheMUC-7 corpus contains 1880 Organisation (46%),1324 Location (32%), and 887 Person (22%) an-notations in 100 documents.
The task has two ele-ments: recognition of the entity boundaries and clas-sification of the entities in the three classes.
The re-sults are summarised below.We first tested the ability of the learners to iden-tify correctly the boundaries of named entities.
Us-ing 10-fold cross-validation on the MUC 7 corpusdescribed above, we experimented with differentmachine learning algorithms and parameters (usingWEKA), and using different attributes for training.5 different algorithms have been evaluated: ZeroR and OneR ?
as baselines, Naive Bayes, IBK (animplementation of K Nearest Neighbour) and J48(an implementation of a C4.5 decision tree).As expected, the baseline algorithms performedvery poorly (at around 1%).
For IBK small windowsgave low results, while large windows were very in-efficient.
The best results (f-measure of around 60%)were achieved using the J48 algorithm.The types of linguistic data used for the attributecollection included part of speech information, or-thography (upper case, lower case, initial upper caseletter, mixture of upper and lower case), token kind(word, symbol, punctuation or number), sentenceboundary, the presence of certain known names andkeywords from the gazetteer lists provided by theANNIE system.
Tokens were used as instance an-notations and, for each token, the window used forcollecting the attributes was of size 5 (itself plus twoother tokens in each direction).Additional information, such as features on awider window of tokens, tended to improve the re-call marginally, but decreased the precision substan-tially, resulting in a lower F-measure, and thereforethe trade off was not worthwhile.We also tested the algorithms on a smaller newscorpus (which contained around 68,000 instances asopposed to 300,000 for the MUC7 corpus).
Again,the J48 algorithm scored highest, with the decisiontable and the K nearest neighbour algorithms bothscoring approximately 1 percentage point lower thanthe J48.The second set of experiments was to classify thenamed entities identified into the three ENAMEXcategories: Organisations, Persons and Locations.Using 10-fold cross-validation on the MUC 7 corpusdescribed above, we experimented with the WEKAmachine learning algorithms and parameters, andusing attributes for training similar to those used forboundary detection.
The best results were achievedagain with the J48 algorithm, and, for this easiertask, they were situated at around 90%.
The at-tributes were chosen on the basis of their informa-tion gain, calculated using WEKA?s attribute selec-tion facilities.The named entity recognition experiments wereperformed mainly to evaluate the WEKA ML algo-rithms on datasets of different sizes, ranging fromsmall to fairly large ones (300,000 instances).
Thedifferent ML algorithms had different memory re-quirements and execution speed, tested on a PIII1.5GHz PC running Windows 2000 with 1GB RAM.From all algorithms tested, the decision table anddecision tree were the slowest (325 and 122 secondsrespectively on 68,000 instances) and required mostmemory - up to 800MB on the big datasets.
NaiveBayes was very fast (only 0.25 seconds) with 1R fol-lowing closely (0.28 seconds).6 Further WorkOLLIE is very much work-in-progress and there areseveral possible improvements we are considering.When dealing with a particular corpus, it is rea-sonable to assume that the documents may be quitesimilar in terms of subject, genre or style.
Becauseof that, it is possible that the quality of the user ex-perience can be improved by simply using a list ofpositive and negative examples.
This would allowthe system not to make the same mistakes by alwaysmissing a particular example or always annotating afalse positive ?
which can be very annoying for theuser.The big difference in execution time for differ-ent ML algorithms shows that there are practicaladvantages that can be gained from having morethan one ML algorithm integrated in OLLIE, whenit comes to supporting the user with the annotationtask.
Since the two experiments showed that NaiveBayes performs only slightly worse than the best,but slower algorithms, it may be feasible to trainboth a fast Naive Bayes classifier and a slower, butmore precise one.
In this way, while the slower MLalgorithm is being re-trained on the latest data, OL-LIE can choose between using the older model ofthis algorithm or the newly re-trained faster base-line, depending on which ones gives better results,and suggest annotations for the current document.As with other such environments, this performanceis measured with respect to the latest annotated doc-ument.We hope to be able to integrate more learningmethods, e.g., TiMBL (Daelemans et al, 1998) andwe will also provide support for other people willingto integrate theirs and make them available from ourOLLIE server or run their own server.We plan to experiment with other NLP tasks, e.g,relation extraction, coreference resolution and textplanning for language generation.Finally, we are working on a Web service imple-mentation of OLLIE for other distributed, Grid ande-science applications.7 ConclusionOLLIE is an advanced collaborative annotation en-vironment, which allows users to share and annotatedistributed corpora, supported by adaptive informa-tion extraction that trains in the background and pro-vides suggestions.The option of sharing access to documents withother users gives several users the possibility to en-gage in collaborative annotation of documents.
Forexample, one user can annotate a text with organi-sations, then another annotate it with locations.
Be-cause the documents reside on the shared server oneuser can see errors or questionable markup intro-duced by another user and initiate a discussion.
Suchcollaborative annotation is useful in the wider con-text of creating and sharing language resources (Maet al, 2002).A number of Machine Learning approaches forInformation Extraction have been developed re-cently, e.g., (Collins, 2002; Bikel et al, 1999), in-cluding some that use active learning, e.g., (Thomp-son et al, 1999) or offer automated support, e.g,(Ciravegna et al, 2002), in order to lower the over-head of annotating training data.
While there ex-ist corpora used for comparative evaluation, (e.g.,MUC or the CMU seminar corpus), there is no easyway to test those ML algorithms on other data, eval-uate their portability to new domains, or experimentwith different parameters of the models.
While someof the algorithms are available for experimentation,they are implemented in different languages, requiredifferent data formats, and run on different plat-forms.
All of this makes it hard to ensure experimen-tal repeatability and eliminate site-specific skew ef-fects.
Also, since not all systems are freely available,we propose an open, distributed environment whereresearchers can experiment with different learningmethods on their own data.Another advantage of OLLIE is that it definesa simple API (Application Programming Interface)which is used by the different ML algorithms to ac-cess the training data (see Section 3.1).
Therefore,the integration of a new machine learning algorithmin OLLIE amounts to providing a wrapper that im-plements this API (a straightforward process).
Wehave already provided a wrapper for the ML algo-rithms provided by the WEKA toolkit which can beused as an example.Although OLLIE shares features with other adap-tive IE environments (e.g., (Ciravegna et al, 2002))and collaborative annotation tools (e.g., (Ma et al,2002)), it combines them in a unique fashion.
In ad-dition, OLLIE is the only adaptive IE system that al-lows users to choose which ML approach they wantto use and to comparatively evaluate different ap-proaches.References[Baker et al2002] P. Baker, A. Hardie, T. McEnery,H.
Cunningham, and R. Gaizauskas.
2002.
EMILLE,A 67-Million Word Corpus of Indic Languages: DataCollection, Mark-up and Harmonisation.
In Proceed-ings of 3rd Language Resources and Evaluation Con-ference (LREC?2002), pages 819?825.
[Bikel et al1999] D. Bikel, R. Schwartz, and R.M.Weischedel.
1999.
An Algorithm that Learns What?sin a Name.
Machine Learning, Special Issue on Natu-ral Language Learning, 34(1-3), Feb.[Bird and Liberman1999] S. Bird and M. Liberman.1999.
A Formal Framework for Linguistic Anno-tation.
Technical Report MS-CIS-99-01, Depart-ment of Computer and Information Science, Uni-versity of Pennsylvania.
http://xxx.lanl.gov/-abs/cs.CL/9903003.
[Ciravegna et al2002] F. Ciravegna, A. Dingli, D. Pe-trelli, and Y. Wilks.
2002.
User-System Coop-eration in Document Annotation Based on Informa-tion Extraction.
In 13th International Conference onKnowledge Engineering and Knowledge Management(EKAW02), pages 122?137, Siguenza, Spain.
[Collins2002] M. Collins.
2002.
Ranking algorithms fornamed entity extraction: Boosting and the voted per-ceptron.
In Proceedings of the 40th Annual AnnualMeeting of the Association for Computational Linguis-tics (ACL?02), Philadelphia,PA.
[Cunningham et al2002] H. Cunningham, D. Maynard,K.
Bontcheva, and V. Tablan.
2002.
GATE: A Frame-work and Graphical Development Environment forRobust NLP Tools and Applications.
In Proceedingsof the 40th Anniversary Meeting of the Association forComputational Linguistics.
[Daelemans and Hoste2002] Walter Daelemans andVe?ronique Hoste.
2002.
Evaluation of MachineLearning Methods for Natural Language ProcessingTasks.
In LREC 2002 Third International Conferenceon Language Resources and Evaluation, pages 755?760, CNTS Language Technology Group, Universityof Antwerp, UIA, Universiteitsplein 1 (bldng A),B-2610 Antwerpen, Belgium.
[Daelemans et al1998] W. Daelemans, J. Zavrel,K.
van der Sloot, and A. van den Bosch.
1998.
Timbl:Tilburg memory based learner version 1.0.
TechnicalReport Technical Report 98-03, ILK.
[Gamba?ck and Olsson2000] B. Gamba?ck and F. Olsson.2000.
Experiences of Language Engineering Algo-rithm Reuse.
In Second International Conference onLanguage Resources and Evaluation (LREC), pages155?160, Athens, Greece.
[Grishman1997] R. Grishman.
1997.
TIPSTER Ar-chitecture Design Document Version 2.3.
Techni-cal report, DARPA.
http://www.itl.nist.gov/-div894/894.02/related projects/tipster/.
[Ide et al2000] N. Ide, P. Bonhomme, and L. Romary.2000.
XCES: An XML-based Standard for Linguis-tic Corpora.
In Proceedings of the Second Interna-tional Language Resources and Evaluation Confer-ence (LREC), pages 825?830, Athens, Greece.
[Ma et al2002] X. Ma, H. Lee, S. Bird, and K. Maeda.2002.
Models and tools for collaborative annotation.In Proceedings of 3rd Language Resources and Evalu-ation Conference (LREC?2002), Gran Canaria, Spain.
[SAIC1998] SAIC.
1998.
Proceedings of the Sev-enth Message Understanding Conference (MUC-7).
http://www.itl.nist.gov/iaui/894.02/-related projects/muc/index.html.
[Tablan et al2002] V. Tablan, C. Ursu, K. Bontcheva,H.
Cunningham, D. Maynard, O. Hamza, TonyMcEnery, Paul Baker, and Mark Leisher.
2002.
Aunicode-based environment for creation and use oflanguage resources.
In Proceedings of 3rd LanguageResources and Evaluation Conference.
[Thompson et al1999] C. A. Thompson, M. E. Califf, andR.
J. Mooney.
1999.
Active learning for natural lan-guage parsing and information extraction.
In Pro-ceedings of the International Conference on MachineLearning, pages 406?414.
[Witten and Frank1999] I. H. Witten and E. Frank.
1999.Data Mining: Practical Machine Learning Tools andTechniques with Java Implementations.
Morgan Kauf-mann.
