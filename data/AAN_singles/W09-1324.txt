Proceedings of the Workshop on BioNLP, pages 185?192,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsTEXT2TABLE:Medical Text Summarization System based on Named EntityRecognition and Modality IdentificationEiji ARAMAKI Yasuhide MIURA Masatsugu TONOIKEThe university of Tokyo Fuji Xerox Fuji Xeroxeiji.aramaki@gmail.com Yasuhide.Miura@fujixerox.co.jp masatsugu.tonoike@fujixerox.co.jpTomoko OHKUMAHiroshi MASHUICHIKazuhiko OHEFuji Xerox Fuji Xerox The university of Tokyo Hospitalohkuma.tomoko@fujixerox.co.jp hiroshi.masuichi@fujixerox.co.jp kohe@hcc.h.u-tokyo.ac.jpAbstractWith the rapidly growing use of electronichealth records, the possibility of large-scaleclinical information extraction has drawnmuch attention.
It is not, however, easy to ex-tract information because these reports arewritten in natural language.
To address thisproblem, this paper presents a system thatconverts a medical text into a table structure.This system?s core technologies are (1) medi-cal event recognition modules and (2) a nega-tive event identification module that judgeswhether an event actually occurred or not.Regarding the latter module, this paper alsoproposes an SVM-based classifier using syn-tactic information.
Experimental results dem-onstrate empirically that syntactic informationcan contribute to the method?s accuracy.1 IntroductionThe use of electronic texts in hospitals is increas-ing rapidly everywhere.
This study specificallyexamines discharge summaries, which are reportsgenerated by medical personnel at the end of a pa-tient?s hospital stay.
They include massive clinicalinformation about a patient?s health, such as thefrequency of drug usage, related side-effects, andcorrelation between a disease and a patient?s ac-tions (e.g., smoking, drinking), which enables un-precedented large-scale research, engenderingpromising findings.NA(1(2(3evertheless, it is not easy to extract clinical in-formation from the reports because these reportsare written in natural language.
An example of adischarge summary is presented in Table 1.
Thetable shows records that are full of medical jargon,acronyms, shorthand notation, misspellings, andsentence fragments (Tawanda et al, 2006).To address this problem, this paper presents aproposal of a system that extracts medical eventsand date times from a text.
It then converts theminto a table structure.
We designate this systemTEXT2TABLE, which is available from a website 1 .
The extraction method, which achieves ahigh accuracy extraction, is based on ConditionalRandom Fields (CRFs) (Lafferty et al, 2001).nother problem is posed by events that do notactually occur, i.e., future scheduled events, eventsthat are merely intended to take place, or hypo-thetical events.
As described herein, we call suchnon-actual events negative events.
Negativeevents are frequently mentioned in medical re-cords; actually, in our corpus, 12% of medicalevents are negative.
Several examples of negativeevents (in italic letters) are presented below:) no headache) keep appointment of radiotherapy) .. will have intravenous fluids1 http://lab0.com/185(4(4'(5th(6acABTTA) .. came for radiotherapy) .. came for headache) Every week radiation therapy and chemicalerapy are scheduled) Please call Dr. Smith with worsening head-he or back pain, or any other concern.Negative events have two characteristics.
First,various words and phrases indicate that an event isnegative.
For this study, such a word or phrase thatmakes an event negative is called a negative trig-ger.
For instance, a negation word ?no?
is a nega-tive trigger in (1).
A noun ?appointment?
in (2) is anegative trigger.
Similarly, the auxiliary ?will?
in(3) signals negation.
More complex phenomena arepresented in (4) and (4').
For instance, ?radiother-apy?
in (4) is a negative event because the therapywill be held in the future.
In contrast, ?headache?in (4') is not negative because a patient actually hasa ?headache?.
These indicate that a simple rule-based approach (such as a list of triggers) can onlyimply classification of whether an event is negativeor not, and that information of the event category(e.g., a therapy or symptom) is required.nother characteristic is a long scope of a nega-tive trigger.
Although negative triggers are near thedescriptive words of events in (1)?
(4), there couldalternatively be a great distance of separation, asportrayed in (5) and (6).
In (5), a noun coordina-tion separates a negative trigger from the event.
In(6), the trigger ?please?
renders all events in thatsentence negative.
These indicate that neighboringwords are insufficient to determine whether anevent is negative or not.
To deal with (5), syntacticinformation is helpful because the trigger and theevent are neighboring in the dependency structure,as portrayed in Fig.
2.
To deal with (6), bag-of-word (BOW) information is desired.ecause of the observation described above, thispaper presents a proposal of a classifier: whetheran event is negative or not.
The proposed classifieruses various information, the event category,neighboring words, BOW, and dependent phrases.he point of this paper is two-fold: (1) We pro-pose a new type of text-summarizing system(TEXT2TABLE) that requires a technique for anegative event identification.
(2) We investigatewhat kind of information is helpful for negativeevent identification.he experiment results revealed that, in spite ofthe risk of parsing error, syntactic information cancontribute to performance, demonstrating the fea-sibility of the proposed approach.lthough experiments described in this paper arerelated to Japanese medical reports, the proposedmethod does not depend on specific languages ordomains.Table 1: A Health Record Sample.BRIEF RESUME OF HOSPITAL COURSE : 57 yo withNSCLCa with back pain and headache .
Trans-ferred from neurosurgery for additional mgmtwith palliative XRT to head .
Pt initiallypresented with cough and hemoptysis to hisprimary MD .
On CXR he was found to have aupper left lobe mass .
He subsequently un-derwent bronchoscopy and bx revealed non-small cell adeno CA.
STaging revealed multi-ple bony mets including skull, spine withMRI revealing mild compression of vertebralbodies at T9, T11, T12 .
T9 with encroach-ment of spinal cord underwent urgent XRTwith no response so he was referred to neu-rosurgery for intervention .
MRI-rt.
fron-tal, left temporal, rt cerebellarhemorrhagic enhancing lesions- most likelyextensive intracranial mets?
T-spine surgeryconsidered second priority and plan to radi-ate cranially immediately with steroid andanticonvulsant .
He underwent simulation on3/28 to whole brain and T3-T7 fields withplan for rx to both sites over 2.5 weeks.Over the past 2 weeks he has noted frontaland occipital HA with left eyelid swelling,ptosis, and denies CP, SOB, no sig.
BM inpast 5 days, small amt of stool after sup-pository.
Neuro?He was Dilantin loaded and alevel should be checked on 3/31 .
He is tocontinue Decadron .
Onc?He is to receive XRTon 3/31 and daily during that week .
Paincontrol?Currently under control with MS con-tin and MSIR prn.
regimen .
Follow HA, LBP.ENDO?Glucose control monitored while on de-cadron with SSRI coverage .
Will checkHgbA1C prior to discharge .
GI?Aggressivebowel regimen to continue at home .
Pt isFull Code .
ADDITIONAL COMMENTS: Please callDr.
Xellcaugh with worsening headache orback pain, or any other concern .
Keep ap-pointment as scheduled with XRT .
Pleasecheck fingerstick once a day, and record,call MD if greater than 200 .186Figure 1: Visualization result (Left), magnified (Right).Figure 2: Negative Triggers and Events on a Depend-ency Structure.Table 2: Corpora and ModalitiesCORPUS MODALITYACE asserted, or otherTIMEML must, may, should, would, orcouldPrasad et al,2006assertion, belief, facts or eventu-alitiesSaur?
et al, 2007 certain, probable, possible, orotherInui et al, 2008 affirm, infer, doubt, hear, intend,ask, recommend, hypothesize, orotherTHIS STUDY S/O, necessity, hope, possible,recommend, intendTable 3: Markup Scheme (Tags and Definitions)Tag Definition (Examples)R Remedy, Medical operation(e.g.
radiotherapy)T Medical test, Medical examination(e.g., CT, MRI)D Deasese, Symptom(e.g., Endometrial cancer, headache)M Medication, administration of a drug(e.g., Levofloxacin, Flexeril)A patient action(e.g., admitted to a hospital)V Other verb(e.g., cancer spread to ...)2 Related Works2.1 Previous Markup SchemesIn the NLP field, fact identification has not beenstudied well to date.
Nevertheless, similar analysescan be found in studies of sentence modality.The Automatic Content Extraction (ACE)2 in-formation extraction program deals with event ex-traction, by which each event is annotated withtemporal and modal markers.ASATsimilar effort is made in the TimeML project(Pustejovsky et al, 2003).
This project specificallyexamines temporal expressions, but several modalexpressions are also covered.Prasad et al (2006) propose four factuality clas-sifications (certain, probable...etc.)
for the PennDiscourse TreeBank (PDTB) 3.aur?
et al (2007) propose three modal categoriesfor text entailment tasks.mong various markup schemes, the most recentone is Experience Mining (Inui et al, 2008), whichcollects personal experiences from the web.
Theyalso distinguish whether an experience is an actualone or not, which is a similar problem to that con-fronting us.able 2 portrays a markup scheme adopted byeach project.
Our purpose is similar to that of Ex-perience Mining.
Consequently, we fundamentallyadopt its markup scheme.
However, we modify thelabel to suit medical mannerisms.
For example,?doubt?
is modified into ?
(S/O) suspicion of?.
Raremodalities such as ?hear?
are removed.2.2 Previous AlgorithmsNegation is a traditional topic in medical fields.Therefore, we can find many previous studies ofthe topic in the relevant literature.An algorithm, NegEx4 was proposed by Chap-man et al (Chapman et al, 2001a; Chapman et al,2001b).
It outputs an inference of whether a term ispositive or negative.
The original algorithm isbased on a list of negation expressions.
Goldin et al(2003) incorporate machine learning techniques(Na?ve Bayes and decision trees) into the algorithm.The extended version (ConText) was also proposed(Chapman et al, 2007).Elkin et al (2005) use a list of negation wordsand a list of negation scope-ending words to iden-2 http://projects.ldc.upenn.edu/ace/3 http://www.seas.upenn.edu/~pdtb/4 http://www.dbmi.pitt.edu/chapman/NegEx.html187tify negated statements and their scope.
Their tech-nique was used in The MAYO Clinic VocabularyServer (MCVS)5, which encodes clinical expres-sions into medical ontology (SNOMED-CT) andidentifies whether the event is positive or negative.MHTAutalik et al (2001) earlier developed Negfinderto recognize negated patterns in medical texts.Their system uses regular expressions to identifywords indicating negation.
Then it passes them asspecial tokens to the parser, which makes use ofthe single-token look-ahead strategy.uang and Lowe (2007) implemented a hybridapproach to automated negation detection.
Theycombined regular expression matching withgrammatical parsing: negations are classified basedon syntactic categories.
In fact, they are located inparse trees.
Their hybrid approach can identify ne-gated concepts in radiology reports even when theyare located distantly from the negative term.he Medical Language Extraction and Encoding(MedLEE) system was developed as a generalnatural language processor to encode clinical doc-uments in a structured form (Friedman et al,1994).
Negated concepts and certainty modifiersare also encoded within the system.Veronika et al (2008) published a negationscope corpus6 in which both negation and uncer-tainty are addressed.lthough their motivations are identical to ours,two important differences are apparent.
(1) Previ-ous (except for Veronika et al, 2008) methods dealwith the two-way problem (positive or negative),whereas the analyses proposed herein tackle morefine-grained modalities.
(2) Previous studies (ex-cept for Huang et al, 2007) are based on BOWapproaches, whereas we use syntactic information.3 Medical Text Summarization System:TEXT2TABLEBecause the core problem of this paper is to iden-tify negative events, this section briefly presents adescription of the entire system, which consists offour steps.
The detailed algorithm of negative iden-tification is explained in Section 4.STEP 1: Event IdentificationFirst, we define the event discussed in this paper.We deal with events of six types, as presented in5 http://mayoclinproc.highwire.org/content/81/6/741.figures-only6 www.inf.u-szeged.hu/rgai/bioscopeTable 3.
Two of the four are Verb Phrases (baseVPs); the others are noun phrases (base-NPs).
Be-cause this task is similar to Named Entity Recogni-tion (NER), we use the state-of-the art NERmethod, which is based on the IOB2 representationand Conditional Random Fields (CRFs).
In learn-ing, we use standard features, as shown in Table 4.Table 4: Features for Event IdentificationLexiconandStemCurrent target word (and its stem) and itssurrounding words (and stem).
The win-dow size is five words (-2, -1, 0, 1, 2).POS Part of speech of current target word andits surrounding words (-2, -1, 0, 1, 2).
Thepart of speech is analyzed using a POStagger7.DIC A fragment for the target word appears inthe medical dictionary (Ito et al, 2003).STEP 2: NormalizationAs described in Section 1, a term in a record issometimes an acronym: shorthand notation.
Suchabbreviations are converted into standard notationthrough (1) date time normalization or (2) eventnormalization.
(1) Date Time NormalizationAs for date time expressions, relative date expres-sions are converted into YYYY/MM/DD as fol-lows.On Dec Last year ?
2007/12/XX10 Dec 2008        ?
2008/12/10These conversions are based on heuristic rules.
(2) Event NormalizationMedical terms are converted into standard notation(dictionary entry terms) using orthographic disam-biguation (Aramaki et al, 2008).STEP 3: TIME?EVENT Relation IdentificationThen, each event is tied with a date time.
The cur-rent system relies on a simple rule (i.e., an event istied with the latest date time).STEP 4: Negative IdentificationThe proposed SVM classifier distinguishes nega-tive events from other events.
The detailed algo-rithm is described in the next section.4 Modality Identification AlgorithmFirst, we define the negative.
We classify modalityevents into eight types (Table 5).
These classifica-tions are motivated by those used in previous stud-7 http://chasen-legacy.sourceforge.jp/188ies (Inui et al, 2008).
However, we simplify theirscheme because several categories are rare in thisdomain.TUhese classes are not exclusive.
For that reason,they sometimes lead to multiple class events.
Forexample, given ?No chemotherapy is planned?, anevent ?chemotherapy?
belongs to two classes,which are ?NEGATION?
and ?FUTURE?.Training Phasesing a corpus with modality annotation, we traina SVM classifier for each category.
The trainingfeatures come from four parts:(1) Current phrases: words included in a currentevent.
We also regard their STEMs, POSs, and thecurrent event category as features.
(2) Surrounding phrases: words included in thecurrent event phrase and its surrounding twophrases (p1, p2, n1, n2, as depicted in Fig.
3).
Theunit of the phrase is base-NP/VP, which is pro-duced by the Japanese parser (Kurohashi et al,1994).
Its window size is two in the neighboringphrase (p1, p2, c, n1, n2).
We also deal with theirSTEMs and POSs.
(3) Dependent phrases: words included in theparent phrase of the current phrase (d1 in Fig.
3),and grandparent phrases (d2 in Fig.
3).
We alsodeal with their STEMs and POSs.
(4) Previous Event: words (with STEMs andPOSs) included in the previous (left side) events.Additionally, we deal with the previous event cate-gory and the modality class.
(5) Bag-of-words: all words (with STEMs andPOSs) in the sentence.TEST PhraseDuring the test, each SVM classifier runs.Although this task is multiclass labeling, severalclass combinations are unnatural, such asFUTURE and S/O.
We list up possible label com-binations (that have at least one occurrence in thecorpora); if such a combination appears in a text,we adapt a high confidence label (using a marginaldistance).5 ExperimentsWe investigate what kind of information contrib-utes to the performance in various machine learn-ing algorithms.Table 5: Classification of ModalitiesNEGATION An event with negation wordssuch as ?not?
or ?no?.FUTURE An event that is scheduled forexecution in the future.PURPOSE An event that is planed by a doc-tor, but its time schedule is am-biguous (just a hope/intention).S/O An event (usually a disease) thatis suspected.
For example, given?suspected microscopic tumor in...?, ?microscopic tumor'' is anS/O event.
?NECESSITY An event (usually a remedy ormedical test) that is required.INTEND An event that is hoped for by apatient.Note that if the event is hoped bya doctor, we regard is aPURPOSE or FUTURE.
For ex-ample, given ?He hoped forchemical therapy?, ?chemicaltherapy?
is INTEND.POSSIBLE An event (usually remedy) that ispossible under the current situa-tion.RECOMMEND An event (usually remedy) that isrecommended by other doctor(s).5.1 Corpus and SettingWe collected 435 Japanese discharge summaries inwhich events and the modality are annotated.
Fortraining, we used the CRF toolkit8 with standardparameters.
In this experiment setting, the input isan event with its contexts.
The output is an eventmodality class (positive of negative in two-way)(or more detailed modality class in nine-way).The core problem addressed in this paper is mo-dality classification.
Therefore, this task settingassumes that all events are identified correctly.Table 6 presents the event identification accuracy.Except for the rare class V (the other verb), we gotmore than 80% F-scores.
It is true that the accu-racy is not perfect.
Nevertheless, most of the re-maining problems in this step will be solved usinga larger corpus.5.2 Comparable MethodsWe conducted experiments in the 10-fold crossvalidation manner.
We investigated the perform-8 http://crfpp.sourceforge.net/189ance in various feature combinations and the fol-lowing machine learning methods.Figure 3: FeaturesTable 6: Event Identification Result.
Tag precision re-call F-score.# P R FA (ACTION) 1,556 94.63 91.04 92.80V (VERB) 1,047 84.64 74.89 79.47D (DISEASE) 3,601 85.56 80.24 82.82M (MEDICINE) 1,045 86.99 81.34 84.07R (REMEDY) 1,699 84.50 76.36 80.22T (TEST) 2,077 84.74 76.68 80.51ALL 11,025 84.74 76.68 80.51Table 7: Various Machine Learning MethodSVM Support Vector Machine (Vapnik,1999).
We used TinySVM9 with apolynomial kernel (degree=2).AP Averaged Perceptron (Collins, 2002)PA1 Passive Aggressive I (Crammer etal., 2006)*PA2 Passive Aggressive II (Crammer etal., 2006)*CW Confidence Weighted (Dredze et al,2008)** The online learning library10 is used for AP PA1,2CW .5.3 Evaluation MetricsWe adopt evaluation of two types:(1) Two-way: positive or negative:(2) Nine-way: positive or one of eight modalitycategories.Recall and F-measure are investigated in both forevaluation precision.5.4 ResultsThe results are shown in Table 8 (Two-Way) andin Table 9 (Nine-Way).Current Event CategoryThe results in ID0?ID1 indicate that the currentevent category (CAT) is useful.
However, eventsare sometimes misestimated in real settings.
WeInRAAH9 http://chasen.org/ taku/software/TinySVM/10 http://code.google.com/p/ollmust check more practical performance in the fu-ture.Bag-of-words (BOW) InformationResults in ID1?ID2 indicate that BOW is impor-tant.Surrounding Phrase ContributionThe results appearing in ID2?ID9 represent thecontribution of each feature position.
From ID3,ID4, and ID7 results, next phrases (n1, n2) andparent phrases (d1) were able to boost the accuracy.Despite the risk of parsing errors, parent phrases(d1) are helpful, which is an insight of this study.contrast, we can say that the following featureshad little contribution: previous phrases (p1, p2from ID5 and ID6), grandparent phrases (d2 fromID8), and previous events (e from ID9).egarding p1 and p2, these modalities are rarelyexpressed in the previous parts in Japanese.s for d2, the grandparent phrases might be tooremoved from the target events.s for e, because texts in health records are frag-mented, each event might have little relation.owever, the above features are also helpful incases with a stronger learning algorithm.In fact, among ID10?ID14, the SVM-basedclassifier achieved the best accuracy with all fea-tures (ID14).Table 8: Two-way Results?
indicates the used feature.
c are features from the cur-rent phrase.
p1, p2, n1, n2 are features from surroundingphrases.
e are features from a previous event.
BOW is abag-of-words using features from an entire sentence.CAT is the category of the current event.190Learning MethodsRegarding the learning algorithms, all online learn-ing methods (ID7 and ID15?17) showed lower ac-curacies than SVM (ID11), indicating that this taskrequires heavy learning.Nine-way ResultsTable 9 presents the accuracies of each class.
Fun-damentally, we can obtain high performance in thefrequent classes (such as NEGATION, PURPOSE,and S/O).
In contrast, the classifier suffers fromlow frequent classes (such as FUTURE).
How tohandle such examples is a subject of future study.Table 9: Two-way Results# Preci-sionRe-callF-measureNEGATION 441 84.19 77.36 80.63PURPOSE 346 91.35 63.87 75.17S/O 242 90.74 72.39 80.53FUTURE 97 23.31 55.96 32.91POSSIBLE 36 83.33 40.55 54.55INTEND 32 76.66 29.35 42.44RECOMMEND 21 95.71 38.57 54.98NECESSITY 4 100 0 04.5 Future WorksIn this section, we will discuss several remainingproblems.
First, as described, the classifier suffersfrom low frequent modality classes.
To give moreexamples for such classes is an important problem.Our final goal is to realize precise information ex-traction from health records.
Our IE systems arealready available at the web site (http://lab0.com).Comprehensive evaluation of those systems is re-quired.6 ConclusionsThis paper presented a classifier that identifiedwhether an event has actually occurred or not.
Theproposed SVM-based classifier uses both BOWinformation and dependency parsing results.
Theexperimental results demonstrated 85.8 F-measure% accuracy and revealed that syntacticinformation can contribute to the method?s accu-racy.
In the future, a method of handling low-frequency events is strongly desired.AcknowledgmentsPart of this research is supported by Grant-in-Aidfor Scientific Research (A) of Japan Society for thePromotion of Science Project Number:?20680006F.Y.2008-20011 and the Research CollaborationProject with Fuji Xerox  Co. Ltd.ReferencesWendy Chapman, Will Bridewell, Paul Hanbury, Greg-ory F. Cooper, and Bruce Buchanan.
2001a.
Evalua-tion of negation phrases in narrative clinical reports.In Proceedings of AMIA Symp, pages 105-109.Wendy Chapman, Will Bridewell, Paul Hanbury, Greg-ory F. Cooper, and Bruce Buchanan.
2001b.
A sim-ple algorithm for identifying negated findings anddiseases in discharge summaries.
Journal of Bio-medical Informatics, 5:301-310.Wendy Chapman, John Dowling and David Chu.
2007.ConText: An algorithm for identifying contextualfeatures from clinical text.
Biological, translational,and clinical language processing (BioNLP2007), pp.81?88.Eiji Aramaki, Takeshi Imai, Kengo Miyo, and KazuhikoOhe: Orthographic Disambiguation IncorporatingTransliterated Probability International Joint Confer-ence on Natural Language Processing (IJCNLP2008),pp.48-55, 2008.Peter L. Elkin, Steven H. Brown, Brent A. Bauer, CaseyS.
Husser, William Carruth, Larry R. Bergstrom, andDietlind L. Wahner Roedler.
A controlled trial of au-tomated classification of negation from clinical notes.BMC Medical Informatics and Decision Making5:13.C.
Friedman, P.O.
Alderson, J.H.
Austin, J.J. Cimino,and S.B.
Johnson.
1994.
A general natural languagetext processor for clinical radiology.
Journal of theAmerican Medical Informatics Association,1(2):161-174.L.
Gillick and S.J.
Cox.
1989.
Some statistical issues inthe comparison of speech recognition algorithms.
InProceedings of IEEE International Conference onAcoustics, Speech, and Signal Processing, pages 532-535.Ilya M. Goldin and Wendy Chapman.
2003.
Learning todetect negation with not in medical texts.
In Work-shop at the 26th ACM SIGIR Conference.Yang Huang and Henry J. Lowe.
2007.
A novel hybridapproach to automated negation detection in clinicalradiology reports.
Journal of the American MedicalInformatics Association, 14(3):304-311.191Kentaro Inui, Shuya Abe, Hiraku Morita, Megumi Egu-chi, Asuka Sumida, Chitose Sao, Kazuo Hara, KojiMurakami, and Suguru Matsuyoshi.
2008.
Experi-ence mining: Building a large-scale database of per-sonal experiences and opinions from web documents.In Proceedings of the 2008 IEEE/WIC/ACM Interna-tional Conference on Web Intelligence, pages 314-321.M.
Ito, H. Imura, and H. Takahisa.
2003.
Igaku- Shoin?sMedical Dictionary.
Igakusyoin.Sadao Kurohashi and Makoto Nagao.
1994.
A syntacticanalysis method of long Japanese sentences based onthe detection of conjunctive structures.
Computa-tional Linguistics, 20(4).Pradeep G. Mutalik, Aniruddha Deshpande, and Pra-kash M. Nadkarni.
2001.
Use of general purpose ne-gation detection to augment concept indexing ofmedical documents: A quantitative study using theumls.
Journal of the American Medical InformaticsAssociation, 8(6):598-609.J.
Lafferty, A. McCallum, and F. Pereira: Conditionalrandom fields: Probabilistic models for segmentingand labeling sequence data, In Proceedings of the In-ternational Conference on Machine Learning(ICML2001), pp.282-289, 2001.R.
Prasad, N. Dinesh, A. Lee, A. Joshi and B. Webber:Annotating Attribution in the Penn Discourse Tree-Bank, In Proceedings of the International Conferenceon Computational Linguistics and the Annual Con-ference of the Association for Computational Lin-guistics (COLING/ACL2006) Workshop onSentiment and Subjectivity in Text, pp.31-38 (2006).R.
Saur?, and J. Pustejovsky: Determining Modality andFactuality for Text Entailment, Proceedings ofICSC2007, pp.
509-516 (2007).Gaizauskas, A. Setzer, G. Katz, and D.R.
Radev.
2003.New Directions in Question Answering: Timeml:Robust specification of event and temporal expres-sions in text.
AAAI Press.SNOMED-CT. 2002.
SNOMED Clinical Terms Guide.College of American Pathologists.Sibanda Tawanda, Tian He, Peter Szolovits, and UzunerOzlem.
2006.
Syntactically informed semantic cate-gory recognizer for discharge summaries.
In Proceed-ings of the Fall Symposium of the American MedicalInformatics Association (AMIA 2006), pages 11-15.Sibanda Tawanda and Uzuner Ozlem.
2006.
Role oflocal context in automatic deidentification of un-grammatical, fragmented text.
In Proceedings of theHuman Language Technology conference and theNorth American chapter of the Association for Com-putational Linguistics (HLT-NAACL2006), pages65-73.Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-orgy Mora, and Janos Csirik.
2008.
The bioscopecorpus: biomedical texts annotated for uncertainty,negation and their scopes.
BMC Bioinformatics,9(11).192
