Non-Native Users in the Let?s Go!!
Spoken Dialogue System:Dealing with Linguistic MismatchAntoine Raux and Maxine EskenaziLanguage Technologies InstituteCarnegie Mellon University5000 Forbes Avenue, Pittsburgh, PA 15232, USA{antoine+,max+}@cs.cmu.eduAbstractThis paper describes the CMU Let?s Go!!
businformation system, an experimental systemdesigned to study the use of spoken dialogueinterfaces by non-native speakers.
The differ-ences in performance of the speech recogni-tion and language understanding modules ofthe system when confronted with native andnon-native spontaneous speech are analyzed.Focus is placed on the linguistic mismatch be-tween the user input and the system?s expecta-tions, and on its implications in terms of lan-guage modeling and parsing performance.
Theeffect of including non-native data when build-ing the speech recognition and language under-standing modules is discussed.
In order to closethe gap between non-native and native input, amethod is proposed to automatically generateconfirmation prompts that are both close to theuser?s input and covered by the system?s lan-guage model and grammar, in order to help theuser acquire idiomatic expressions appropriateto the task.1 Introduction1.1 Spoken Dialogue Systems and Non-NativeSpeakersSpoken dialogue systems rely on models of human lan-guage to understand users?
spoken input.
Such modelscover the acoustic and linguistic space of the commonlanguage used by the system and the user.
In currentsystems, these models are learned from large corpora ofrecorded and transcribed conversations matching the do-main of the system.
In most of the cases, these cor-pora are gathered from native speakers of the languagebecause they are the main target of the system and be-cause developers and researchers are often native speak-ers themselves.
However, when the common languageis not the users?
native language, their utterances mightfall out of this ?standard?
native model, seriously degrad-ing the recognition accuracy and overall system perfor-mance.
As telephone-based information access systemsbecome more common and available to the general pub-lic, this inability to deal with non-native speakers (or withany ?non-standard?
subgroup such as the elderly) is aserious limitation since, at least for some applications,(e.g.
tourist information, legal/social advice) non-nativespeakers represent a significant portion of the everydayuser population.1.2 Previous Work on Non-Native SpeechRecognitionOver the past ten years, extensive work has been doneon non-native speech recognition.
Early research aimedat endowing Computer Assisted Language Learning soft-ware with speech recognition capabilities (e.g.
(Eske-nazi and Hansma, 1998), (Witt and Young, 1997)).
Usu-ally such systems are targeted at one specific popula-tion, that is, people who share the same native language(L1).
Thus, most research in non-native speech recog-nition uses knowledge of the L1, as well as databasesof accented speech specially recorded from speakers ofthe target population.
Ideally, by training acoustic mod-els on target non-native speech, one would capture itsspecific characteristics just as training on native speechdoes.
However collecting amounts of non-native speechthat are large enough to fully train speaker-independentmodels is a hard and often impractical task.
Therefore, re-searchers have resorted to using smaller amounts of non-native speech to retrain or adapt models that were orig-inally trained on large corpora of native speech.
As fornative speech, such methods were mostly applied to readspeech, with some success (e.g.
(Mayfield Tomokiyo andWaibel, 2001)).Unfortunately, we know from past research on na-tive speech recognition that read speech models performpoorly on conversational speech (Furui, 2001), which isthe style used when talking to spoken dialogue systems.A few studies have built and used databases of non-nativeconversational speech for evaluation (Byrne et al, 1998),and training (Wang and Schultz, 2003).In all those cases, the native language of the speaker isknown in advance.
One exception is (Fischer et al, 2001)who apply multilingual speech recognition methods tonon-native speech recognition.
The authors train acousticmodels on a database comprising native speech from fiveEuropean languages (English, Spanish, French, Germanand Italian) and use them to recognize non-native Englishfrom speakers of 10 European countries.
However, theirtask is the recognition of read digit strings, quite differentfrom conversational speech.Also, because of the difficulty researchers have torecord large amounts of spontaneous non-native speech,no thorough study of the impact of the linguistic differ-ences between native and non-native spontaneous speechhas been conducted to our knowledge.
The two spon-taneous non-native speech studies cited above, reportperplexity and out-of-vocabulary (OOV) word rate (for(Wang and Schultz, 2003)) but do not provide any analy-sis.In this paper, while acknowledging the importance ofacoustic mismatch between native models and non-nativeinput, we focus on linguistic mismatch in the context ofa task-based spoken dialogue system.
This includes dif-ferences in word choices which influences the number ofOOV words, and syntax which affects the performance ofthe speech recognizer?s language model and of the natu-ral language understanding (NLU) grammar.1.3 Non-Native Speakers as Language LearnersAll the research on non-native speech recognition de-scribed in the previous section sees non-native speakersas a population whose acoustic characteristics need tobe modeled specifically but in a static way, just like onewould model the acoustics of male and female voices dif-ferently.
A different approach to the problem is to seenon-native speakers as engaged in the process of acquir-ing the target language?s acoustic, phonetic and linguisticproperties.
In this paradigm, adapting dialogue systemsto non-native speakers does not only mean being able torecognize and understand their speech as it is, but alsoto help them acquire the vocabulary, grammar, and pho-netic knowledge necessary to fulfill the task the systemwas designed for.This idea follows decades of language teaching re-search that, since the mid sixties, has emphasized thevalue of learning language in realistic situations, in orderto perform specific tasks.
Immersion is widely consid-ered as the best way to learn to speak a language and mod-ern approaches to foreign language teaching try to mimicits characteristics.
If the student cannot be present in thecountry the language is spoken in, then the student shouldbe put into a series of situations imitating the linguisticexperience that he/she would have in the target country.Thus, most current language teaching methods, followingthe Communicative Approach (Littlewood, 1981) havefocused on creating exercises where the student is forcedto use language quickly in realistic situations and thus tolearn from the situation itself as well as from reactions tothe student?s actions.From a different viewpoint, (Bortfeld and Brennan,1997) showed in a psycholinguistic study that non-nativespeakers engaged in conversation-based tasks with nativespeakers do not only achieve the primary goal of the taskthrough collaborative effort but also acquire idiomatic ex-pressions about the task from the interaction.The research described in this paper, has the dual goalof improving the accessibility of spoken dialogue systemsto non-native speakers and of studying the usability of acomputer for task-based language learning that simulatesimmersion.The next section gives an overview of the CMU Let?sGo!!
bus information system that we built and use inour experiments.
Section 3 describes and analyzes the re-sults of experiments aimed at comparing the accuracy ofspeech recognition and the quality of language modelingon both native and non-native data.
Section 4 describesthe use of automatically generated confirmation promptsto help the user speak the language expected by the sys-tem.
Finally, section 5 draws conclusions and presentsfuture directions of research.2 Overview of the System2.1 The CMU Let?s Go!!
Bus Information SystemIn order to study the use of spoken dialogue systems bynon-native speakers in a realistic setting, we built Let?sGo!
!, a spoken dialogue system that provides bus sched-ule information for the Pittsburgh area(Raux et al, 2003).As shown in Figure 1, the system is composed of five ba-sic modules: the speech recognizer, the parser, the dia-log manager, the language generator, and the speech syn-thesizer.
Speech recognition is performed by the SphinxII speech recognizer (Huang et al, 1992).
The Phoenixparser (Ward and Issar, 1994) is in charge of natural lan-guage understanding.
The dialogue manager is basedon the RavenClaw framework (Bohus and Rudnicky,2003).
Natural language generation is done by a simpletemplate-based generation module, and speech synthe-sis by the Festival speech synthesis system (Black et al,1998).
The original system uses a high quality limited-domain voice recorded especially for the project but forsome experiments, lower quality, more flexible voicesFigure 1: General architecture of the Let?s Go!!
bus in-formation system.have been used.
All modules communicate through theGalaxy-II (Seneff et al, 1998) framework.2.2 Definition of the DomainThe Port Authority of Allegheny County, which man-ages the buses in Pittsburgh provided the full database ofbus routes and schedules.
Overall, this database containsmore than 10,000 bus stops but we restricted our systemto 5 routes and 559 bus stops in areas where internationalstudents are likely to travel since they are our main targetpopulation at present.In order to improve speech recognition accuracy, weconcatenated the words in the name of each bus stop(e.g.
?Fifth and Grant?)
and made them into a single en-try in the recognizer?s lexicon.
Because there are usu-ally several variant names for each bus stop and since weincluded other places such as landmarks and neighbor-hoods, the total size of the lexicon is 9914 words.2.3 Data Collection ExperimentsTo gather enough data to train and test acoustic and lan-guage models, we had the system running, advertisingit to international students at our university, as well asconducting several studies.
In those studies, we gave sce-narios to the participants in the form of a web page withmaps indicating the places of departure and destination,as well as additional time and/or route preferences.
Therewas as little written English as possible in the descrip-tion of the scenarios to prevent influencing the languagehabits of the participants.
Participants then called the sys-tem over the phone to get the required information.
Oneexperiment conducted in June 2003 netted 119 calls from11 different non-native speakers (5 of them were fromIndia and 6 from Japan), as well as 25 calls from 4 na-tive speakers of American English.
Another experimentin August 2003 allowed the collection of 47 calls from6 non-native speakers of various linguistic backgrounds.The rest of the non-native data comes from unsollicitedNative Non-NativeWord Error Rate 20.4 % 52.0 %Table 1: Word Error Rate of the speech recognizer with anative language model on native and non-native data.individual callers labelled as non-native by a human an-notator who transcribed their speech.
The total size of thespontaneous non-native corpus is 1757 utterances.3 Recognition and Understanding ofNon-Native Speech3.1 Recognition AccuracyWe used acoustic models trained on data consisting ofphone calls to the CMU Communicator system(Rudnickyet al, 2000).
The data was split into gender specificsets and corresponding models were built.
At recognitiontime, the system runs the two sets of models in paralleland for each utterance selects the result that has the high-est recognition score, as computed by Sphinx.
The lan-guage model is a class-based trigram model built on 3074utterances from past calls to the Let?s Go!!
system, inwhich place names, time expressions and bus route namesare each replaced by a generic class name to compensatefor the lack of training data.In order to evaluate the performance of these models onnative and non-native speakers, we used 449 utterancesfrom non-native users (from the August experiment andthe unsollicited calls) and 452 from native users of thesystem.
The results of recognition on the two data setsare given in Table 1.
Even for native speakers, perfor-mance was not very high with a word error rate of 20.4%.Yet, this is acceptable given the small amount of trainingdata for the language model and the conversational na-ture of the speech.
However, performance degrades sig-nificantly for non-native speakers, with a word error rateof 52.0%.
The two main potential reasons for this lossare acoustic mismatch and linguistic mismatch.
Acousticmismatch arises from the variations between the nativespeech on which the acoustic models were trained andnon-native speech, which often include different accentsand pronunciations.
On the other hand, linguistic mis-match stems from variations or errors in syntax and wordchoice, between the native corpus on which the languagemodel was trained and non-native speech.3.2 Impact of Linguistic Mismatch on thePerformance of the Language ModelTo analyze the effect of linguistic mismatch, we com-pared the number of out-of-vocabulary words (OOV) andthe perplexity of the model on the transcription of the testutterances.
Table 2 shows the results.
The percentage ofNative Non-Native Difference Significance% OOV words 1.2 % 3.09 % 157.5 % p < 10?4% utt.
w/ OOV words 5.9 % 14.0 % 174.5 % p < 10?5Perplexity 22.89 36.55 59.7 % ?% words parsed 63.3 % 56.0 % 56.0 % p < 10?9% utt.
fully parsed 56.4 % 49.7 % 49.7 % p < 0.05Table 2: The native language model and parsing grammar applied to native and non-native speech transcriptions.
Thestatistical significance of the difference between the native and non-native sets is computed using the chi-square testfor equality of distributions.OOVs is 3.09% for non-native speakers, more than 2.5times higher than it is for native speakers, which showsthe difference in word choices made by each population.Such differences include words that are correctly used butare not frequent in native speech.
For example, whenreferring to bus stops by street intersections, all nativespeakers in our training set simply used ?A and B?, hencethe word ?intersection?
was not in the language model.On the other hand, many non-native speakers used the fullexpression ?the intersection of A and B?.
Note that thedifferences inside the place name itself (e.g.
?A and B?
vs?A at B?)
are abstracted away by the class-based model,since all variants are replaced by the same class name(words like ?intersection?
and ?corner?
were kept out ofthe class to reduce the number of elements in the ?place?class).
In other cases non-native speakers used inappro-priate words, such as ?bus timing?
for ?bus schedule?,which were not in the language model.
Ultimately, OOVsaffect 14.0% of the utterances as opposed to 5.9% for na-tive utterances, which is significant, since an utterancecontaining an OOV is more likely to contain recognitionerrors even on its in-vocabulary words, since the OOVprevents the language model from accurately matchingthe utterance.
Differences between the native and non-native set in both OOV rate and the ratio of utterancescontaining OOVs were statistically significant.We computed the perplexity of the model on the utter-ances that did not contain any OOV.
The perplexity of themodel on this subset of the non-native test set is 36.55,59.7% higher than that on the native set.
This reflectsdifferences in syntax and selected constructions.
For ex-ample, although native speakers almost always used thesame expression to request a bus departure time (?Whendoes the bus leave ...??
), non-natives used a wider varietyof sentences (e.g.
?Which time I have to leave?
?, ?Whatthe next bus I have to take??).
Both the difference be-tween native and non-native and the larger variability ofnon-native language account for the larger perplexity ofthe model over the non-native set.
This results seems todisagree with what (Wang and Schultz, 2003) found intheir study, where the perplexity was larger on the nativeset.
Unfortunately, they do not describe the data used totrain the language model so it is hard to draw any conclu-sions.
But one main difference is that their experimentfocused only on German speakers of English, whereaswe collected data from a much more diverse population.3.3 Impact of the Linguistic Mismatch on LanguageUnderstandingThe Phoenix parser used in the natural language under-standing module of the system is a robust, context-freegrammar-based parser.
Grammar rules, including op-tional words, are compiled into a grammar network thatis used to parse user input.
When no complete parseis found, which is often the case with spoken language,Phoenix looks for partial parses and returns the parse for-est that it is most confident in.
Confidence is based oninternal measures such as the number of words coveredby the parses and the number of parse trees in the parseforest (for an equal number of covered words, a smallernumber of parse trees is preferred).The grammar rules were hand written by the devel-opers of the system.
Initially, since no data was avail-able, choices were made based on their intuition and ona small scale Wizard-of-Oz experiment.
Then, after thefirst version of the system was made available, the gram-mar was extended according to actual calls to the system.The grammar has thus undergone continuous change, asis often the case in spoken dialogue systems.The grammar used in this experiment (the ?native?grammar) was designed based for native speech withoutadaptation to non-native data.
It provides full parses ofsentences like ?When is the next bus going to the air-port?
?, but also, due to the robustness of the parser, partialparses to ungrammatical sentences like ?What time busleave airport??.
Once compiled, the grammar networkconsisted of 1537 states and 3076 arcs.
The two bot-tom rows of Table 2 show the performance of the parseron human-transcribed native and non-native utterances.Both the number of words that could be parsed and thenumber of sentences for which a full parse was obtainedare larger for native speakers (resp.
63.3% and 56.4%)than non-native (56% and 49.7%), although the relativedifferences are not as large as those observed for the lan-Figure 2: Comparison of the relative gain obtained byusing a language model and grammar that includes somenon-native data over the original purely native model, ontranscribed native and non-native speech.guage model.
This can be attributed to the original dif-ficulty of the task since even native speech contains alot of disfluencies that make it difficult to parse.
As aconsequence, robust parsers such as Phoenix, which aredesigned to be flexible enough to handle native disfluen-cies, can deal with some of the specificities of non-nativespeech.
Yet, the chi-square test shows that the differencebetween the native and non-native set is very significantfor the ratio of words parsed and mildly so for the ratioof fully parsed sentences.
The weak significance of thelatter can be partly explained by the small number of ut-terances in the corpora.3.4 Effect of Additional Non-Native Data onLanguage Modeling and ParsingIn order to study the improvement of performance pro-vided by mixing native and non-native data in the lan-guage model, we built a second language model (the?mixed?
model), using the 3074 sentences of the nativemodel to which were added 1308 sentences collectedfrom non-native calls to the system not included in thetest set.
Using this model, we were able to reduce theOOV rate by 56.6% and perplexity by 23.6% for our non-native test set.
While the additional data also improvedthe performance of the model on native utterances, theimprovement was relatively smaller than for non-nativespeakers (12.1%).
As can be seen by comparing Tables2 and 3, this observation is also true of OOV rate (56.6%improvement for non-native vs 50.0% for native) and theproportion of sentences with OOVs (43.1% vs 55.7%).Figure 2 shows the relative improvement due to the mixedLM over the native LM on the native and non-native set.We also evaluated the impact of additional non-nativedata on natural language understanding.
In this case,since we wrote the grammar manually and incrementallyover time, it is not possible to directly ?add the non-native data?
to the grammar.
Instead, we compared theJune 2003 version of the grammar, which is mostly basedon native speech, to its September 2003 version, whichcontains modifications based on the non-native data col-lected during the summer.
This part is therefore an eval-uation of the impact of the human grammar design doneby the authors based on additional non-native data.
Atthat point, the compiled grammar had grown to contain1719 states and 3424 arcs which represents an increaseof respectively 11.8% and 11.3% over the ?native?
gram-mar.
Modifications include the addition of new words(e.g.
?reach?
as a synonym of ?arrive?
), new constructs(e.g.
?What is the next bus??)
and the relaxation of somesyntactic constraints to accept ungrammatical sentences(e.g.
?I want to arrive the airport at five?
instead of ?Iwant to arrive at the airport at five?).
Using this newgrammar, the proportion of words parsed and sentencesfully parsed improved by respectively 10.4% and 11.3%for the native set and by 17.3% and 11.7% for the non-native set.
We believe that, as for the language model, thereduction in the number of OOVs is the main explana-tion behind the better improvement in word coverage ob-served for the non-native set compared to the native set.The reduction of the difference between the native andnon-native sets is also reflected in the weaker significancelevels for all ratios except that of fully parsed utterances,in 3, larger p-values meaning that there is a larger proba-bility that the differences between the ratios were due tospurious differences between the corpora rather than totheir (non-)nativeness.This confirms that even for populations with a widevariety of linguistic backgrounds, adding non-native datadoes reduce the linguistic mismatch between the modeland new, unseen, non-native speech.
Another explana-tion is that, on a narrow domain such as bus scheduleinformation, the linguistic variance of non-native speechis much larger than that of native speech.
Therefore,less data is required to accurately model native speechthan non-native speech.
It also appears from these resultsthat, in the context of task-based spoken dialogue sys-tems, higher-level modules, such as the natural languageunderstanding module, are less sensitive to explicit mod-eling of non-nativeness.
This can be explained by the factthat such modules were designed to be flexible in order tocompensate for speech recognition errors.
This flexibilitybenefits non-native speakers as well, regardless of addi-tional recognition errors.3.5 Effect of Additional Non-Native Data on SpeechRecognitionUnfortunately, the reduction of linguistic mismatch wasnot observed on recognition results.
While using the newlanguage model improved word error rate on both nativeNative Non-Native Difference Significance% OOV words 0.6 % 1.34 % 123.3 % p < 0.05% utt.
w/ OOV words 2.9 % 6.2 % 113.8 % p < 0.01Perplexity 20.12 27.92 38.8 % ?% words parsed 69.9 % 65.7 % 65.7 % p < 10?3% utt.
fully parsed 62.8 % 55.5 % 55.5 % p < 0.05Table 3: The mixed language model and parsing grammar applied to native and non-native speech transcriptions.Significance is computed using the chi-square test, except for perplexity where the relative difference is reported.Figure 3: Word Error Rate on Native and Non-NativeData using a Native and a Mixed Language Modeland non-native utterances (resp.
to 17.8% and 47.8%,see Figure 3 ), the impact was relatively larger for nativespeech.
This is an indication that acoustics play a promi-nent role in the loss of accuracy of speech recognition onnon-native speech.
Acoustic differences between nativeand non-native speakers are likely to be larger than thelinguistic ones, since, particularly on such a limited andcommon domain, it is easier for non-native speakers tomaster syntax and word choice than to improve their ac-cent and pronunciation habits.
Differences among non-native speakers of different origins are also very largein the acoustic domain, making it hard to create a singleacoustic model matching all non-native speakers.
Finally,the fact that additional non-native data improves perfor-mance on native speech is a sign that, generally speak-ing, the lack of training data for the language model is alimiting factor for recognition accuracy.
Indeed, if therewas enough data to model native speech, additional non-native data should increase the variance and therefore theperplexity on native speech.4 Adaptive Lexical Entrainment as aSolution to Linguistic Mismatch4.1 Gearing the User To the System?s LanguageThe previous section described the issue of recogniz-ing and understanding non-native speech and solutions toadapt traditional systems to non-native speakers.
Anotherapproach is to help non-native users adapt to the systemby learning appropriate words and expressions.
Lexicalentrainment is the phenomenon by which, in a conversa-tion, speakers negotiate a common ground of expressionsto refer to objects or topics.
Developers of spoken di-alogue systems frequently take advantage of lexical en-trainment to help users speak utterances that are withinthe language model of the system.
This is done by care-fully designing the system prompts to contain only wordsthat are recognized by the recognition and understandingmodules (Gustafson et al, 1997).
However, in the caseof non-native users, there is no guarantee that users actu-ally know the words the system wants them to use.
Also,even if they do, some non-native speakers might preferto use other words, which they pronounce better or thatthey better know how to use.
For those reasons, we be-lieve that to be optimal, the system must try to match theuser?s choice of words in its own prompts.
This idea ismotivated by the observations of (Bortfeld and Brennan,1997), who showed that this type of adaptation occursin human-human conversations between native and non-native speakers.The role of the system?s ?native?
prompts is to takethe users through the shortest path from their current lin-guistic state to the system?s expectations.
In fact, this isnot only true for non-native speakers and lexical entrain-ment is often described as a negotiation process betweenthe speakers (Clark and Wilkes-Gibbs, 1986).
However,while it is possible for limited-domain system design-ers to establish a set of words and constructions that arewidely used among native speakers, the variable natureof the expressions mastered by non-native speakers makeadaptation a desirable feature of the system.4.2 Automatic Generation of Corrective PromptsIn this study, not all prompts were modified to match theuser?s choice of words.
Instead, the focus was placed onconfirmation prompts that both ensure proper understand-ing between the user and the system and lexically entrainthe user towards the system?s expected input.
Two ques-tions arise: how to generate the prompts and when totrigger them.
Our approach has been to design a list oftarget prompts that fit the system?s language model andgrammar and find the closest target prompt to each userinput.
The distance between a user utterance as recog-nized by Sphinx and each of the target utterances is com-puted by the same dynamic programming algorithm thatis traditionally used to compute word error rate in speechrecognition evaluation.
It determines the number of wordinsertions, deletions and substitutions that lead from thetarget prompt to the user?s utterance.
The target promptthat is closest, i.e.
that requires the fewest operations tomatch the input, is selected.
In addition, words that rep-resent important concepts such as places, times or busroute numbers, are given additional weight.
This followsthe assumption that a target sentence is not appropriateif it has a missing or an extra concept compared to theutterance.
We also used this heuristic to answer the sec-ond question: when to trigger the confirmation prompts.The system asks for a confirmation whenever a target sen-tence is found that contains the same concepts as the userinput and differs from it by at least one word.
In this casea prompt like ?Did you mean ...?
followed by the tar-get sentence is generated.
Finally, the dynamic program-ming algorithm used to align the utterances also locatesthe words that actually differ between the input and thetarget.
This information is sent to the speech synthesizer,which puts particular emphasis on the words that differ.To provide natural emphasis, the intonation of all sen-tences is generated by the method described in (Raux andBlack, 2003) that concatenates portions of natural into-national contours from recorded utterances into a contourappropriate for each prompt.
Since the domain-limitedvoice recorded for the project does not allow us to eithergenerate non-recorded prompts or to modify the contourof the utterances, we used a different, generic voice forthis version of the system.4.3 Application and ExampleThe method described in the previous paragraph was im-plemented in the system and tested in a small pilot study.We manually wrote 35 different target prompts describingdeparture and destination places, times and route num-bers, based on our knowledge of the system?s languagemodel and grammar.
An example of a confirmation di-alogue obtained from one of these prompts is given inFigure 4.
In the first user utterance, the preposition ?to?
ismissing, either because it was not pronounced by the useror because it was not recognized by the speech recog-nition module.
As a consequence, the utterance cannotbe fully parsed by the language understanding module.In parallel, the confirmation module computes the dis-tance between the user?s input and each of the 35 targetprompts, and identifies the closest one as ?I want to go tothe airport?.
At the same time it finds that the user?s utter-ance is obtained from the target by deleting the word ?to?and therefore stresses it in the confirmation prompt.
OnceS: What can I do for you?U: I want to go the airport.S: Sorry, I didn?t get that.Did you mean:I want to go TO the airport?U: YesS: To the airport.Where are you leaving from?U: ...Figure 4: Example of an adaptive confirmation dialogue.The capital ?TO?
indicate that the word was emphasizedby the system.the user answers ?yes?
to the confirmation prompt, thetarget prompt is sent to the parser as if it had been utteredby the user and the state of the dialogue is updated ac-cordingly.
If the user answers ?no?, the prompt is simplydiscarded.
We found that this method works well whenspeech recognition is only slightly degraded and/or whenthe recognition errors mostly concern grammar and func-tion words.
In such cases, this approach is often able torepair utterances that would not be parsed correctly other-wise.
However, when too many recognition errors occur,or when they affect the values of the concepts (i.e.
thesystem recognizes one place name instead of another),the users receive too many confirmation prompts to whichthey must respond negatively.
Combined with the diffi-culty that non-native speakers have in understanding un-expected synthesized utterances, this results in cognitiveoverload on the user.
Yet, this method provides an easyway (since the designer only has to provide the list of tar-get prompts) to generate adaptive confirmation promptsthat are likely to help lexical entrainment.5 Conclusion and Future DirectionsIn this paper, we described the Let?s Go!!
bus informationsystem, a dialogue system targetted at non-native speak-ers of English.
In order to investigate ways to improve thecommunication between non-native users and the system,we recorded calls from both native and non-native speak-ers and analyzed their linguistic properties.
We foundthat besides the problem of acoustic mismatch that resultsfrom the differences in accent and pronunciation habits,linguistic mismatch is also significant and degrades theperformance of the language model and the natural lan-guage understanding module.
We are exploring two solu-tions to reduce the linguistic gap between native and non-native users.
First we studied the impact of taking intoaccount non-native data to model the user?s language andsecond we designed a mechanism to generate confirma-tion prompts that both match the user?s input and a set ofpredefined target utterances, so as to help the user acquireidiomatic expressions related to the task.Real-world systems like Let?s Go!!
are in constant evo-lution because the data that is collected from users call-ing the system is used to refine the acoustic and linguis-tic models of the system.
In the near future, our priorityis to collect more data to improve the acoustic modelsof the system and address the specific issues related toa general non-native population, which does not share acommon native language.
We will also work on integrat-ing the confirmation prompt generation method proposedin this work with state-of-the-art confidence annotationmethods.6 AcknowledgmentsThe authors would like to thank Alan W Black, Dan Bo-hus and Brian Langner for their help with this research.This material is based upon work supported by the U.S.National Science Foundation under Grant No.
0208835,?LET?S GO: improved speech interfaces for the generalpublic?.
Any opinions, findings, and conclusions or rec-ommendations expressed in this material are those of theauthors and do not necessarily reflect the views of the Na-tional Science Foundation.ReferencesA.
Black, P. Taylor, and R. Caley.
1998.
The Festivalspeech synthesis system.
http://festvox.org/festival.D.
Bohus and A. Rudnicky.
2003.
Ravenclaw: Dia-log management using hierarchical task decompositionand an expectation agenda.
In Proc.
Eurospeech 2003,pages 597?600, Geneva, Switzerland.H.
Bortfeld and S. Brennan.
1997.
Use and acquisitionof idiomatic expressions in referring by native and non-native speakers.
Discourse Processes, 23:119?147.W.
Byrne, E. Knodt, S. Khudanpur, and J. Bernstein.1998.
Is automatic speech recognition ready for non-native speech?
A data collection effort and initial ex-periments in modeling conversational hispanic english.In Proc.
ESCA Workshop on Speech Technology inLanguage Learning, pages 37?40, Marholmen, Swe-den.H.
Clark and D. Wilkes-Gibbs.
1986.
Referring as acollaborative process.
Cognition, 22:1?39.M.
Eskenazi and S. Hansma.
1998.
The Fluency pronun-ciation trainer.
In Proc.
ESCA Workshop on SpeechTechnology in Language Learning, pages 77?80.V.
Fischer, E. Janke, S. Kunzmann, and T. Ross.
2001.Multilingual acoustic models for the recognition ofnon-native speech.
In Proc.
ASRU ?01, Madonna diCampiglio, Italy.S.
Furui.
2001.
From read speech recognition to spon-taneous speech understanding.
In Proc.
6th NaturalLanguage Processing Pacific Rim Symposium, pages19?25, Tokyo, Japan.J Gustafson, A. Larsson, R. Carlson, and K. Hellman.1997.
How do system questions influence lexicalchoices in user answers?
In Proc.
Eurospeech ?97,pages 2275?2278, Rhodes, Greece.X.
Huang, F. Alleva, H.-W. Hon, K.-F. Hwang, M.-Y.Lee, and R. Rosenfeld.
1992.
The SPHINX-II speechrecognition system: an overview.
Computer Speechand Language, 7(2):137?148.W.
Littlewood.
1981.
Communicative Language Teach-ing.
Cambridge University Press.L.
Mayfield Tomokiyo and A. Waibel.
2001.
Adapta-tion methods for non-native speech.
In Proc.
Multilin-guality in Spoken Language Processing, Aalborg, Den-mark.A.
Raux and A.
Black.
2003.
A unit selection ap-proach to f0 modeling and its application to empha-sis.
In Proc.
IEEE Automatic Speech Recognition andUnderstanding Workshop 2003, pages 700?705, SaintThomas, US Virgin Islands.A.
Raux, B. Langner, A.
Black, and M. Eskenazi.
2003.Lets go: Improving spoken dialog systems for the el-derly and non-natives.
In Proc.
Eurospeech 2003,pages 753?756, Geneva, Switzerland.A.
Rudnicky, C. Bennett, A.
Black, A. Chotimongkol,K.
Lenzo, A. Oh, and R. Singh.
2000.
Task and do-main specific modelling in the carnegie mellon com-municator system.
In Proc.
ICSLP 2000, Beijing,China.S.
Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, andV.
Zue.
1998.
Galaxy-II: A reference architecture forconversational system development.
In Proc.
ICSLP?98, Sydney, Australia.Z.
Wang and T. Schultz.
2003.
Non-native spontaneousspeech recognition through polyphone decision treespecialization.
In Proc.
Eurospeech ?03, pages 1449?1452, Geneva, Switzerland.W.
Ward and S. Issar.
1994.
Recent improvements in theCMU spoken language understanding system.
In Proc.ARPA Human Language Technology Workshop, pages213?216, Plainsboro, NJ.S.
Witt and S. Young.
1997.
Language learning based onnon-native speech recognition.
In Proc.
Eurospeech?97, pages 633?636, Rhodes, Greece.
