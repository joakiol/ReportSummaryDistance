Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 996?1006, Dublin, Ireland, August 23-29 2014.Prior-informed Distant Supervisionfor Temporal Evidence ClassificationRidho ReinandaUniversity of AmsterdamAmsterdam, The Netherlandsr.reinanda@uva.nlMaarten de RijkeUniversity of AmsterdamAmsterdam, The Netherlandsderijke@uva.nlAbstractTemporal evidence classification, i.e., finding associations between temporal expressions and re-lations expressed in text, is an important part of temporal relation extraction.
To capture thevariations found in this setting, we employ a distant supervision approach, modeling the task asmulti-class text classification.
There are two main challenges with distant supervision: (1) noisegenerated by incorrect heuristic labeling, and (2) distribution mismatch between the target anddistant supervision examples.
We are particularly interested in addressing the second problemand propose a sampling approach to handle the distribution mismatch.
Our prior-informed distantsupervision approach improves over basic distant supervision and outperforms a purely super-vised approach when evaluated on TAC-KBP data, both on classification and end-to-end metrics.1 IntroductionTemporal relation extraction is the problem of extracting the temporal extent of relations between entities.A typical solution to the temporal relation extraction problem has three main components: (1) passageretrieval, (2) temporal evidence classification, and (3) temporal evidence aggregation.
A community-based effort to evaluate temporal relation extraction was introduced in 2011 as a TAC Knowledge BasePopulation task: Temporal Slot Filling, or TSF for short (Ji et al., 2011).An illustration of temporal slot filling is as follows.
Having identified a per:spouse relation be-tween two entities (Freeman Dyson, Imme Dyson), a system must establish the temporal boundariesfrom its supporting sentence.
In the case of the sentence ?In 1958, he married Imme Dyson?, the goalis to find that the relation lasts from 1958 until the present day.
Within the TSF setting, the boundariesare represented as beginning and ending intervals in a tuple (T1, T2, T3, T4) instead of an exact timeexpression, so as to allow uncertainty in the system output.
We investigate temporal relation extractionfollowing this setting.
We focus on the temporal evidence classification part.One of the challenges with relation extraction is the limited amount of training data available to capturethe variations in a target corpus: temporal relation extraction faces the same challenge.
Employing distantsupervision (Mintz et al., 2009) is a way to address the challenge.
But generating example training datain the temporal setting is not straightforward: we have to find not only the query and related entity, butalso the time expression, in a single text segment.Employing distant supervision for temporal evidence classification will introduce noise, in the formof labels and additional contexts (e.g., lexical features).
A lot of previous work in distant supervision hasbeen dedicated to reducing noise in distant supervision (Bunescu and Mooney, 2007; Riedel et al., 2010;Wei et al., 2012).
We are interested in another phenomenon: the class distributions found in trainingdata generated by a distant supervision approach.
These distributions become an issue if the distantsupervision corpus has a different structure and different characteristics compared to the target corpus,e.g., Wikipedia vs. news articles.
We observe that in the case of temporal evidence, news articles andWikipedia do indeed contain different class distributions.
Our working hypothesis is that incorporatingprior information about temporal class distribution helps improve our distant supervision approach.
WeThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/996test this hypothesis by comparing a distant supervision strategy with class priors to a distant supervisionwithout class priors.
We also demonstrate the effectiveness of our method by contrasting it with a purelysupervised approach.
In addition, we investigate how the difference in performance in temporal evidenceclassification affects the final score obtained in the overall end-to-end task.We discuss related work in Section 2.
In Section 3, we describe our distant supervision approach fortemporal evidence classification.
Our experimental setup is detailed in Section 4.
We follow with resultsin Section 5 and a conclusion in Section 6.2 Related WorkWe discuss two groups of related work: on temporal slot filling and on distant supervision.2.1 Temporal slot fillingSome previous work uses a pattern-based approach (Byrne and Dunnion, 2011); patterns are defined interms of query entity, temporal expression, and slot value.
For example, the word divorce should triggerthat the relation per:spouse is ending.
Other work uses temporal linking between time expressions andevents in an event-based approach (Burman et al., 2011), where the source documents are annotated withTimeML event annotations (Pustejovsky et al., 2003); the authors use intra-sentence event-time links,and inter-sentence event-event links, following a TempEval approach (UzZaman et al., 2012).
Garridoet al.
(2012) use a graph-based document representation; they convert document context to a graphrepresentation and use TARSQI to determine links between time expressions and events in documentsand later map the resulting links into five temporal classes.Li et al.
(2012) combine flat and structured approaches to perform temporal classification.
Theirapproach relies on a custom SVM kernel designed around flat (window and shallow dependency) featuresand structured (dependency path) features.
The structured approach is designed to overcome the longcontext problem.
They use a distant supervision approach for the temporal classification part, obtainedon Freebase relations.
They further extend their approach with self-training and relabeling (Ji et al.,2013).Finally, Surdeanu et al.
(2011) use n-grams around temporal expressions to train a distant supervisionsystem.
To be able to use Freebase facts, they find example sentences in Wikipedia, and use a windowof five words from the temporal expression, using Freebase facts as start and end trigger.
They useJaccard correlation between n-grams to determine the association to start and end.
Sil and Cucerzan(2014) performed distant supervision using facts obtained from Wikipedia infoboxes.
From Wikipediainfoboxes, they retrieve the relevant sentences and build n-gram language models of the relations.
Ina slightly different setting (exploratory search), Reinanda et al.
(2013) establish the temporal extent ofentity associations simply by looking at their co-occurrence within documents in the corpus.Our approach to temporal evidence classification differs from most existing approaches in its distantsupervision scheme.
We use distant supervision to directly perform a multi-class classification of tem-poral evidence against the five main temporal classes (including the before and after class), where mostof the previous systems train a model to detect the beginning and ending of relationships only.2.2 Reducing noise in distant supervisionWith distant supervision (Mintz et al., 2009), indirect examples in the form of relations from a knowledgebase such as Freebase and DBPedia are used.
From these relation tuples, instances of relations in theform of sentences in the corpus are searched.
Text features are later extracted from these sentences thatare then used to train classifiers that can identify relations in the text corpus.Reducing noise is an important ingredient when working with a distant supervision assumption.
Re-labeling is one such approach; Tamang and Ji (2012) perform relabeling based on semi-supervised lassoregression to reduce incorrect labeling.
Wei et al.
(2012) show that instances may be labeled incorrectlydue to the knowledge base being incomplete.
They propose to overcome the problem of incompleteknowledge bases for distant supervision through passage retrieval model with relation extraction.997Ritter et al.
(2003) focus on the issue of missing data for texts that contain rare entities that do not existin the original knowledge base.
Riedel et al.
(2010) work with a relaxed distant supervision assumption;they design a factor graph to explicitly model whether two entities are related, and later train this modelwith a semi-supervised constraint-driven algorithm; they achieve a 31 percent error reduction.Bunescu and Mooney (2007) introduce multiple instance learning to handle the weak confidence inthe assigned label.
They divide the instances into a positive bag (at least one positive example) anda negative bag (all negative examples).
They design a custom kernel to work with this weaker form ofsupervision.
Surdeanu et al.
(2012) operate on the same principle, but model the relation between entitiesand relation classes using graphical models.
Hoffmann et al.
(2011) also use multi-instance learning, butfocus on overlapping relations.What we add on top of existing work is the use of sampling techniques to correct for skewed distri-butions introduced through distant examples.
We propose prior sampling, correcting the distributions ofthe classes in the generated examples to fit the target corpora.3 MethodThe temporal slot filling task is defined as follows: given a relation R = (q, r, s), where q is a queryentity, r is a related entity, and s is a slot type, one must find TR, a tuple of four dates (T1, T2, T3, T4)where R holds, where T1and T2form the beginning interval of the relation, and T3and T4is the endinginterval.
A system first must retrieve all passages or sentences expressing the relation between q and r.Each sentences and any time information within them will serve as intermediate evidence.
This temporalevidence will later be aggregated and converted to tuple representation TR.In this paper, we focus on temporal evidence classification.
That is, assuming the passage retrievalcomponent has retrieved the relevant passages as intermediate evidence of temporal relations, we mustclassify whether the time expression t in the passage belongs to one these classes: BEGINNING, END-ING, BEFORE, AFTER, andWITHIN.
In the training and evaluation data available to us, only the offsetsof the time expression within the document are given for each intermediate evidence, therefore we firstextract the paragraph and find the context sentence mentioning t.Distant supervision for temporal classification The temporal slot filling task, as specified by TAC-KBP, defines 7 types of temporal-intensive relations.
In our distant supervision approach, we use aseparate knowledge base to find instances of the equivalent relations.
We use Freebase as our referenceknowledge base.
That is, we use the temporal information found in Freebase to generate training exam-ples.
We manually map the TAC-KBP?s 8 temporal relations into 6 Freebase mediator relations.
Thecomplete mapping of the relations can be found in Table 1.In an article, entities and time expressions are not always referred to using their full mentions within asingle sentence.
Sometimes information is scattered around several sentences: the query entity q in thefirst sentence, later referred to using a pronoun in the second sentence that contains a time expression,etc.
One common way to deal with this problem is to run full co-reference resolution, therefore ensuringall mentions are resolved.
We handle this problem by relaxing the distant supervision rule.
Rather thanretrieving sentences, we retrieve passages containing the query entity q, and related entity r instead.
Welater replace every pronoun found within the passage with q.
Based on our analysis of the Wikipediaarticles, this simple heuristic should work, because most Wikipedia articles are entity-centric, and a lotof the pronouns mentioned in the articles will refer to the query entity q.Each relation that we mapped from Freebase has temporal boundaries from and to.
Following Li et al.
(2012), we use Algorithm 1 to generate the training examples, but adapt it suit to our assumption.Sampling the DS examples We manually compared our main corpus (TAC document collection) andour distant supervision corpus (Wikipedia) and noticed some discrepancies.
The main corpus mainlyconsists of newswire articles; one of the main difference between Wikipedia articles and newswire ar-ticles is that Wikipedia articles mainly consist of milestone events.
In terms of class distribution, thismeans that most of the generated examples will be in the form of BEGINNING and ENDING class,followed by the BEFORE and AFTER class, with the smallest number of examples belonging to the998TAC Relations Freebase Relationsper:spouse marriageper:title employment-tenure,goverment-position-heldper:employee-of employment-tenureper:member-of political-party-tenureper:cities-of-residenceplaces-livedper:stateorprovinces-of-residenceplaces-livedper:countries-of-residenceplaces-livedorg:top-employees/membersorganization-leadershipTable 1: Relation mapping to Freebase.Data: Freebase temporal relation (q, r, from, to)Result: labeled training examplesRetrieve the Wikipedia article of the query entity q;Split article into passages;Retrieve the passages containing q, r;Extract all time expressions from the passages;for time-expression t doRetrieve the context sentence s containing t;If t is from : use s, t as BEGINNING example;If t is to : use s, t as ENDING example;If t before from : use s, t as BEFORE example;If t after to : use s, t as AFTER example;If t between from and to : use s, t as WITHIN example;endAlgorithm 1: Training data generation.WITHIN class.
In newswire, however, we tend to see something different; most of the time expressionswill belong to the WITHIN class.We argue that using the training data with a ?smarter?
prior is important.
More data not only meansmore information, but may also mean more noise.
This is particularly important with the relaxed dis-tant supervision assumption that we have.
Therefore, we choose to sample instead of using all of thegenerated training examples.We employ two sampling strategies: uniform, sampling from our generated training data and deliber-ately fitting them to a uniform distribution; and prior-sampling, where we deliberately construct trainingdata to fit a prior distribution.
One way to estimate such a prior is by looking at the distributions of classesin the gold-standard training data that we have.
In the case where gold-standard data is not available, wecan use a heuristic to estimate the distributions of temporal classes based on domain knowledge or onobservations of the target corpora.In summary, we generate the final training data according to the following steps.
First, generate train-ing data with the DS approach described before.
Next, estimate class distributions from the (supervised)training data.
Then, sample examples from the generated DS data with the probability estimated fromthe supervised training data (i.e., the empirical prior).
Keep sampling the training examples until we999reach the target percentage of the DS data.
Finally, use the sampled training data to train the multi-classclassifier.Feature representation Both for the training, evaluation, and DS data, we extract the context sentence,i..e, the sentence containing the relation and time expression t.We normalize the context sentence as follows.
First, we detect named entities within the sentence andreplace the mentions with their entity types (PERSON, ORGANIZATION, or LOCATION).
Second, wedetect other time expressions within the context and normalize them with regard to the main time expres-sion t, i.e., by normalizing them into TIME-LT and TIME-GT.
The idea is to capture the relationshipsbetween time expressions as features.We extract lexical features from the normalized sentence.
This comprises tokens surrounding thequery entity, related entity (slot filler), and time expression.
We consider the following four models asour feature representations:Model-1: bag-of-words All tokens within the normalized sentences are used as features.Model-2: context window All tokens within the proximity of 3 tokens from the query entity, relatedentity, and time expression are used as features.Model-3: context window with trigger words lexicon All tokens within the proximity of 3 token fromthe query entity, related entity, and time expression are used as features.
In addition, a list ofkeywords which might indicate the beginning and ending of relationships are used as gazetteerfeatures.
These list of keywords are expanded by using WordNet to extract related terms.Model-4: context window with position All tokens within the proximity of 3 tokens from the queryentity, related entity, and time expression are used as features.
Rather than considered as bag-of-words tokens, the positions of word occurrences are now taken into account as features.4 Experimental SetupWe introduce the dataset and the setup of our experiments.
Before that we formulate our research ques-tions as these dictate our further choices.Research questions We aim to answer the following research questions:RQ1 How does a purely supervised approach with different features and learning algorithms performon the task of temporal evidence classification?RQ2 How does the performance of a distant supervision approach compare to that of a supervisedlearning approach on the task of temporal evidence classification?RQ3 How does the performance of a prior-informed distant supervision approach compare to that of abasic distant supervision approach on the task of temporal evidence classification?RQ4 How do the approaches listed above compare in terms of their performance on the end-to-endtemporal relation extraction task?Corpora and knowledge base We use the TAC 2011 document collection, which contains 1.7M docu-ments, consisting of news wires, web texts, broadcast news, and broadcast conversation.
We use a recentversion of Freebase (October 2013) as our knowledge base and retrieve the latest version of Wikipediaas our distant supervision corpus.Ground truth We use the TAC-KBP 2011 Temporal Slot Filling Task dataset (Ji et al., 2011) as theground truth in our experiments.
The ground truth comes in two forms: intermediate evidence (withclassification labels) and tuples (boundaries of each relation).
We use the intermediate evidence to eval-uate our temporal evidence classification framework.
We later use the provided tuples to evaluate theend-to-end result.The dataset contains 173 examples in the training set and 757 examples in the evaluation set.
Thedistribution of the classes is shown in Table 2.1000Class Training Evaluation DS TrainingWITHIN 66 357 6,129BEGINNING 59 217 22,508ENDING 30 110 16,775BEFORE 9 45 24,932AFTER 9 28 12,499Table 2: Class distribution statistics.Evaluation metric We use F1 as the main evaluation metric for the temporal evidence classificationtask.
For the end-to-end temporal information extraction task, we use the evaluation metric proposed inTAC-KBP 2011, i.e., the Q score.
Given a relation r and the ground truth interval tuple Gr, Q(Tr), thequality score of a tuple Trreturned by system S is computed as follows:Q(Tr) =144Pi=111+di,where diis the absolute difference between Tiin system response and the ground truth tuple Gi(mea-sured in years).
To obtain an overall systemQ score, we average theQ scores obtained from each relationtuple returned.Experiments We run four contrastive experiments.
In Experiment 1, we contrast the performance onthe temporal evidence classification task of the different choices for our supervised methods (Model-1,-2, -3, -4), using either Support Vector Machine, Naive Bayes, Random Forest, or Gradient BoostedRegression Tree.
In Experiment 2 we examine our distant supervision method and contrast its perfor-mance with the supervised methods from Experiment 1.
In Experiment 3, we contrast different samplingmethods for our distant supervision method.In Experiment 4 we consider the overall performance on the temporal relation extraction task of ourmethods; in this experiment we use three ?oracle runs?
that we have not introduced yet: first, the Label-Oracle run uses the actual temporal classification label from the ground truth, use these ground truthlabel to aggregate the evidence and create the temporal tuples, and compute the end-to-end score; second,Within-Oracle assigns all temporal evidence to the WITHIN class; third, Nil-Baseline is a lower-boundrun that assigns NIL to every element of the temporal tuples.We use the implementations of the learning algorithms in the Scikit-learn machine learning package(Pedregosa et al., 2011).5 Results and DiscussionWe present the outcomes of the four experiments specified in the previous section.5.1 Preliminary experimentTo answer RQ1, How does the performance of the supervised learning approaches on the temporalevidence classification task vary with different representations and learning algorithms?, we start witha preliminary experiment.
The aim of this experiment is to get an idea of the classification performancewith a purely supervised approach.
The results are shown in Table 3.As shown in Table 3, Model-4 with the SVM and NB classifiers achieves the best overall performance.There seems to be a gradual increase in performance from the simpler to the more complex model withSVM and NB classifiers, with the exception of RF.
Interestingly, GBRT seems only slightly affected bythe different choice of model in this supervised setting.5.2 Distant supervision experimentsNext, we evaluate the distant supervision approach.
We aim to answer RQ2, How does the performanceof the distant supervision approach compare to that of the supervised learning approach?
We generate1001Model SVM NB RF GBRTModel-1 0.405 0.361 0.402 0.422Model-2 0.409 0.417 0.354 0.420Model-3 0.412 0.418 0.361 0.420Model-4 0.426 0.424 0.241 0.422Table 3: Experiment 1.
Supervised approaches to temporal evidence classification.training examples with the approach described in Section 3, and use the full generated training data totrain SVM and Naive Bayes classifiers with the same representation models that we use in the previousexperiments.
The results are shown in Table 4.Model Supervised DS DS-uniform DS-priorModel-1 SVM 0.405 0.212 0.379 0.408Model-2 SVM 0.409 0.185 0.389 0.450Model-3 SVM 0.412 0.183 0.384 0.452Model-4 SVM 0.426 0.200 0.400 0.463Model-1 NB 0.361 0.413 0.379 0.431Model-2 NB 0.417 0.299 0.372 0.451Model-3 NB 0.418 0.300 0.368 0.446Model-4 NB 0.424 0.270 0.400 0.486Model-1 RF 0.402 0.162 0.406 0.397Model-2 RF 0.354 0.177 0.399 0.418Model-3 RF 0.361 0.176 0.391 0.403Model-4 RF 0.241 0.171 0.399 0.446Model-1 GBRT 0.422 0.142 0.316 0.344Model-2 GBRT 0.420 0.137 0.343 0.418Model-3 GBRT 0.420 0.138 0.343 0.403Model-4 GBRT 0.422 0.140 0.399 0.433Table 4: Experiment 2 and 3.
Supervised, distant supervision, and distant supervision with samplingapproaches to temporal evidence classification.We observe that the distant supervision approach trained on the full set of generated examples (thecolumn labeled ?DS?)
performs poorly, well below the supervised approach.
We hypothesize that theaccuracy drops due to the amount of noise generated with our distant supervision assumption trainedfrom full data, and different class distribution statistics.In Section 3, we proposed our prior-sampling approach for distant supervision.
The next experimentis meant to answer RQ3, How does the performance of our prior-informed distant supervision approachcompare to that of the basic distant supervision approaches?
We sample 20 percent of the generatedexamples datasets with the following strategies: uniform and prior.
The results are also shown in Table 4,in the columns labeled ?DS-uniform?
and ?DS-prior,?
respectively.By observing the results in Table 4, we notice that distant supervision with prior sampling performsthe best, for every combination of model and classification method.
Uniform sampling already helpsin improving the performance, and prior sampling successfully boosts the performance of the basicdistant supervision (for all four models) further.
Distant supervision with prior sampling also performsconsistently better than the supervised approaches (Table 3) in many cases?interestingly, for GBRT,DS-prior only outperforms the supervised methods with sufficiently complex queries (Model-4 GBRT).10025.3 End-to-end experimentsNext, we answer RQ4.
That is, we consider how the classification performance on temporal evidenceclassification affects the end-to-end result.
We take the best performing models from the previous exper-iments and evaluate their end-to-end scores.
The results are shown in Table 5.1Model Avg-Q F1Label-Oracle 0.925 1.000Within-Oracle 0.676 0.302Nil-Baseline 0.393 N/ASupervisedModel-4 SVM 0.657 0.426Model-4 NB 0.648 0.424Model-4 RF 0.573 0.241Model-4 GBRT 0.649 0.422Distant supervisionModel-4 SVM 0.669 0.463Model-4 NB 0.679 0.486Model-4 RF 0.653 0.446Model-4 GBRT 0.669 0.433Table 5: Experiment 4.
End-to-end scores (Avg-Q) next to F1 scores for temporal evidence classification.From Table 5, we see that Model-4 RF (F1 on temporal evidence classification 0.446) and Model-4GBRT (F1 on temporal evidence classification 0.433) translate into 0.653 and 0.669, respectively, interms of Q-score.
This means that the misclassifications that Model-4 RF produces have a larger impactthan those of Model-4 GBRT.
However, the difference in performance is not large.The evaluation of this end-to-end task is important because not every misclassification has a similarcost.
Misclassification of class A into class B can result in a huge increase/decrease in performance.
First,the classification performance does not directly map to the end-to-end score.
Second, several relationshave more pieces of evidence than others; performing misclassifications on relations that have a lot ofsupporting evidence would probably have less effect on the final score.The state of the art performance, using distant supervision (Li et al., 2012), achieves an end-to-endAvg-Q score of 0.678 (on training data), where we achieve 0.679 (on evaluation data).
However, ourscores are not directly comparable since we reduce the number of classes (and the amount of evidence)in our evaluation.
It is important to note that Li et al.
(2012) use a complex combination of flat andstructured features as well as the web, where we use relatively simple features with Wikipedia and priorsampling.Furthermore, our approach manages to achieve the same level of end-to-end performance as theWithin-Oracle run, while achieving a significantly better F-score.
More pieces of evidence were ac-tually classified correctly, though this was not reflected directly in the end-to-end score due to issuesdescribed above.5.4 Error AnalysisWe proceed to analyse parts of our end-to-end results to see what is causing errors in the temporalevidence classification task.
We found several common problems.Semantic inference Some problems had to do with the fact that several snippets require semanticinference.
The fact that someone dies effectively ends any relationships that this person had.
Anotherexample is when someone marries someone (AmarriesC), and this beginning of relationships effectively1As the Nil-Baseline is applied directly to the final tuples rather than the classification labels, there are is no F1 score forthis run.1003means the end of relationships for previous relations (A and B).
A more complex method to deal withthis type of semantic inference is needed, simple classification does not work so well.
Here is an example:Angela Merkel is married to Joachim Sauer, a professor of chemistry at Berlin?s HumboldtUniversity, since 1998.
Divorced from Ulrich Merkel.
No children.For this example the fact is that the time expression 1998 happens after with regard to the spouse relationbetween Angela Merkel and Ulrich Merkel.Concise temporal representations Newspaper articles contain lots of temporal information in a con-cise way.
For example in the form (X?Y).
This implicit interval range is not expressed in a lexical contextbut rather with symbolic conventions.
In several articles, the information encoded is almost tabular ratherthan expressed in explicitly.
For example:Elected as german chancellor Nov. 22, 2005.
Chairwoman, christian democratic union,2000-present.
Chairwoman, christian democratic parliamentary group, 2002?2005.Complex time-inference BEFORE and AFTER are especially tricky to deal with because they requireadditional inference.
Even if a passage contains the word after, the time expression linked to it wouldprobably contain the before relation.He was called up by the Army in the spring of 1944, after marrying bea silverman in 1943,and was sent to The Philippines.For the above example, 1943 happens before the ?person joined the Army?
event.We observe quite a number of these cases on the evaluation data.
Furthermore, the lack of context onsome examples and evidence that is scattered around multiple sentences complicates the problem evenmore.
Because of semantic and implicit evidence, temporal evidence classification remains a challengingtask.
In order to achieve a better absolute performance, collective classification/inference of evidenceseems an interesting option.6 ConclusionWe have presented a distant-supervision approach to temporal evidence classification.
The main featureof our distant supervision approach is that we consider the prior distribution of classes in the target do-main in order to better model the task.
We show that our prior-informed distant supervision approachmanages to outperform a purely supervised approach.
Our method also achieves state-of-the-art perfor-mance on end-to-end temporal relation extraction with fewer and simpler features than previous work.Our error analysis on the temporal evidence classification task revealed several issues that inform ourfuture work aimed at further improving the performance on the subtask of temporal evidence classifica-tion, and the overall temporal relation extraction task.
In particular, we intend to deal with the challengingaspect of semantic inference over relations found in the evidence passage.
Another interesting directionthat we aim to tackle is dealing with evidence that is scattered across multiple sentences.AcknowledgementsThis research was supported by the European Communitys Seventh Framework Programme (FP7/2007-2013) under grant agreements nrs 288024 and 312827, the Netherlands Organisation for Scientific Re-search (NWO) under project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, the Center forCreation, Content and Technology (CCCT), the QuaMerdes project funded by the CLARIN-nl pro-gram, the TROVe project funded by the CLARIAH program, the Dutch national program COMMIT,the ESF Research Network Program ELIAS, the Elite Network Shifts project funded by the Royal DutchAcademy of Sciences (KNAW), the Netherlands eScience Center under project nr 027.012.105, the Ya-hoo!
Faculty Research and Engagement Program, the Microsoft Research PhD program, and the HPCFund.1004ReferencesRazvan C. Bunescu and Raymond J. Mooney.
2007.
Learning to extract relations from the web using minimalsupervision.
In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics(ACL?07), Prague, Czech Republic, June.Amev Burman, Arun Jayapal, Sathish Kannan, Ayman Kavilikatta, Madhu abd Alhelbawy, Leon Derczynski,and Robert Gauzauskas.
2011.
USFD at KBP 2011: Entity linking, slot filling and temporal bounding.
InProceedings of the TAC-KBP 2011 Workshop.
NIST.Lorna Byrne and John Dunnion.
2011.
UCD IIRG at tac 2011.
In Proceedings of the TAC-KBP 2011 Workshop.NIST.Guillermo Garrido, Anselmo Pe?nas, Bernardo Cabaleiro, and?Alvaro Rodrigo.
2012.
Temporally anchored relationextraction.
In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: LongPapers - Volume 1, ACL ?12, pages 107?116, Stroudsburg, PA, USA.
Association for Computational Linguistics.Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld.
2011.
Knowledge-basedweak supervision for information extraction of overlapping relations.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ?11,pages 541?550, Stroudsburg, PA, USA.
Association for Computational Linguistics.Heng Ji, Ralph Grishman, and Hoa Trang Dang.
2011.
Overview of the TAC 2011 knowledge base populationtask.
In Proceedings of the TAC-KBP 2011 Workshop.
NIST.Heng Ji, Taylor Cassidy, Qi Li, and Suzanne Tamang.
2013.
Tackling representation, annotation and classificationchallenges for temporal knowledge base population.
Knowledge and Information Systems, pages 1?36.Qi Li, Javier Artiles, Taylor Cassidy, and Heng Ji.
2012.
Combining flat and structured approaches for temporalslot filling or: how much to compress?
In Proceedings of the 13th international conference on ComputationalLinguistics and Intelligent Text Processing - Volume Part II, CICLing?12, pages 194?205, Berlin, Heidelberg.Springer-Verlag.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009.
Distant supervision for relation extraction withoutlabeled data.
In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies (ACL ?09), pages 1003?1011, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-napeau, Mattheiu Brucher, Matthieu Perrot, and?Edouard Duchesnay.
2011.
Scikit-learn: Machine learning inPython.
Journal of Machine Learning Research, 12:2825?2830.James Pustejovsky, Jos?e Casta?no, Robert Ingria, Roser Saur?
?, Robert Gaizauskas, Andrea Setzer, and GrahamKatz.
2003.
TimeML: Robust specification of event and temporal expressions in text.
In Fifth InternationalWorkshop on Computational Semantics (IWCS-5.Ridho Reinanda, Daan Odijk, and Maarten de Rijke.
2013.
Exploring entity associations over time.
In SIGIR2013 Workshop on Time-aware Information Access, August.Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010.
Modeling relations and their mentions withoutlabeled text.
In Proceedings of the 2010 European conference on Machine learning and knowledge discoveryin databases: Part III, ECML PKDD?10, pages 148?163, Berlin, Heidelberg.
Springer-Verlag.Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Etzioni.
2003.
Modeling missing data in distant supervisionfor information extraction.
In Transactions of the Association for Computational Linguistics, TACL?13, pages367?378, Stroudsburg, PA, USA.
Association for Computational Linguistics.Avirup Sil and Silviu Cucerzan.
2014.
Temporal scoping of relational facts based on Wikipedia data.
In CoNLL:Conference on Natural Language Learning.Mihai Surdeanu, Sonal Gupta, John Bauer, David McClosky, Angel X. Chang, Valentin I. Spitkovsky, and Christo-pher D. Manning.
2011.
Stanford?s distantly-supervised slot-filling system.
In Proceedings of the TAC-KBP2011 Workshop.
NIST.1005Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D. Manning.
2012.
Multi-instance multi-label learning for relation extraction.
In Proceedings of the 2012 Joint Conference on Empirical Methods inNatural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ?12, pages455?465, Stroudsburg, PA, USA.
Association for Computational Linguistics.Suzanne Tamang and Heng Ji.
2012.
Relabeling distantly supervised training data for temporal knowledge basepopulation.
In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scaleKnowledge Extraction, AKBC-WEKEX ?12, pages 25?30, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Naushad UzZaman, Hector Llorens, James F. Allen, Leon Derczynski, Marc Verhagen, and James Pustejovsky.2012.
TempEval-3: Evaluating events, time expressions, and temporal relations.
CoRR, abs/1206.5333.Xu Wei, Raphael Hoffmann, Le Zhao, and Ralph Grishman.
2012.
Filling knowledge base gaps for distant super-vision of relation extraction.
In Proceedings of the 50th Annual Meeting of the Association for ComputationalLinguistics: Long Papers - Volume 1, ACL ?12, pages 825?834, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.1006
