Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 731?741,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsNeural Network for Heterogeneous AnnotationsHongshen Chen???
Yue Zhang ?
Qun Liu??
?Key Laboratory of Intelligent Information ProcessingInstitute of Computing Technology, Chinese Academy of Sciences?University of Chinese Academy of Sciences?Singapore University of Technology and Design?
ADAPT centre, School of Computing, Dublin City Universitychenhongshen@ict.ac.cn, yue zhang@sutd.edu.sgAbstractMultiple treebanks annotated under heteroge-neous standards give rise to the research ques-tion of best utilizing multiple resources for im-proving statistical models.
Prior research hasfocused on discrete models, leveraging stack-ing and multi-view learning to address theproblem.
In this paper, we empirically inves-tigate heterogeneous annotations using neu-ral network models, building a neural networkcounterpart to discrete stacking and multi-view learning, respectively, finding that neuralmodels have their unique advantages thanks tothe freedom from manual feature engineering.Neural model achieves not only better accu-racy improvements, but also an order of mag-nitude faster speed compared to its discretebaseline, adding little time cost compared toa neural model trained on a single treebank.1 IntroductionFor many languages, multiple treebanks have beenannotated according to different guidelines.
For ex-ample, several linguistic theories have been usedfor defining English dependency treebanks, includ-ing Yamada and Matsumoto (2003), LTH (Johans-son and Nugues, 2007) and Stanford dependencies(De Marneffe et al, 2006).
For German, there existTIGER (Brants et al, 2002) and Tu?Ba-D/Z (Telljo-hann et al, 2006).
For Chinese, treebanks have beenmade available under various segmentation granu-larities (Sproat and Emerson, 2003; Emerson, 2005;Xue, 2003).
These give rise to the research problem?Work done when the first author was visiting SUTD.of effectively making use of multiple treebanks un-der heterogeneous annotations for improving outputaccuracies (Jiang et al, 2015; Johansson, 2013; Liet al, 2015).The task has been tackled using two typical ap-proaches.
The first is based on stacking (Wolpert,1992; Breiman, 1996; Wu et al, 2003).
As shown inFigure 1(a), the main idea is to have a model trainedusing a source treebank, which is then used to guidea target treebank model by offering source-style fea-tures.
This method has been used for leveraging twodifferent treebanks for word segmentation (Jiang etal., 2009; Sun and Wan, 2012) and dependency pars-ing (Nivre and McDonald, 2008; Johansson, 2013).The second approach is based on multi-viewlearning (Johansson, 2013; Li et al, 2015).
Theidea is to address both annotation styles simul-taneously by sharing common feature representa-tions.
In particular, Johansson (2013) trained depen-dency parsers using the domain adaptation methodof Daume?
III (2007), keeping a copy of shared fea-tures and a separate copy of features for each tree-bank.
Li et al (2015) trained POS taggers by cou-pling the labelsets from two different treebanks intoa single combined labelset.
A summary of suchmulti-view methods is shown in Figure 1(b), whichdemonstrates their main differences compared tostacking (Figure 1(a)).Recently, neural network has gained increasingresearch attention, with highly competitive resultsbeing reported for numerous NLP tasks, includingword segmentation (Zheng et al, 2013; Pei et al,2014; Chen et al, 2015), POS-tagging (Ma et al,2014; Plank et al, 2016), and parsing (Chen and731B modelA modelTrain A modelCorpus ATrain B modelCorpus B Corpus B & A labelsOutput BA modelB modelRaw sentence Output AMulti-view modelTrain multi-view modelCorpus A Corpus BOutput AMulti-view modelRaw sentenceTraining(a)  stacking(b)  multi-view learningTestingTrainingTesting Output BFigure 1: Two main approaches to utilizing hetero-geneous annotations.Manning, 2014; Dyer et al, 2015; Weiss et al, 2015;Zhou et al, 2015).
On the other hand, the aforemen-tioned methods on heterogeneous annotations are in-vestigated mainly for discrete models.
It remains aninteresting research question how effective multipletreebanks can be utilized by neural NLP models, andwe aim to investigate this empirically.We follow Li et al (2015), taking POS-taggingfor case study, using the methods of Jiang et al(2009) and Li et al (2015) as the discrete stackingand multi-view training baselines, respectively, andbuilding neural network counterparts to their mod-els for empirical comparison.
The base tagger isa neural CRF model (Huang et al, 2015; Lampleet al, 2016), which gives competitive accuracies todiscrete CRF taggers.Results show that neural stacking allows deeperintegration of the source model beyond one-best out-puts, and further the fine-tuning of the source modelduring the target model training.
In addition, the ad-vantage of neural multi-view learning over its dis-crete counterpart are many-fold.
First, it is freefrom the necessity of manual cross-labelset inter-active feature engineering, which is far from triv-ial for representing annotation correspondence (Liet al, 2015).
Second, compared to discrete model,parameter sharing in deep neural network eliminatesthe issue of exponential growth of search space, andallows separated training of each label type, in thesame way as multi-task learning (Collobert et al,2011).Our neural multi-view learning model achievesnot only better accuracy improvements, butalso an order of magnitude faster speed com-pared to its discrete baseline, adding little timecost compared to a neural model trained ona single treebank.
The C++ implementationsof our neural network stacking and multi-viewlearning models are available under GPL, athttps://github.com/chenhongshen/NNHetSeq.2 Baseline Neural Network TaggerWe adopt a neural CRF with a Long-Short-Term-Memory (LSTM) (Hochreiter and Schmidhuber,1997) feature layer for baseline POS tagger.
Asshown in Figure 2, the model consists of three mainneural layers: the input layer calculates dense rep-resentation of input words using attention model oncharacter embeddings; the feature layer employs abi-directional LSTM model to extract non-local fea-tures from input vectors; the output layer uses aCRF structure to infer the most likely label for eachinput word.2.1 Input LayerGiven a sentence w(1:n), the input layer builds a vec-tor representation r?iw for each wordwi based on bothword and character embeddings.
In particular, anembedding lookup table is used to convert vocabu-lary words into their embedding forms directly.
Toobtain a character based embedding of wi, we de-note the character sequence of wi with c(1:n), wherecj is the jth character in wi.A character lookup table is used to map each char-732Input layer ... ......... ... .........CRF layerFeaturelayer......... ........................ ... ......... ... ......Bi-LSTMTanhLinearOutput tagsw1 w2 wn-1 wnOutputlayer...t1 t2 tn-1 tnFigure 2: Baseline neural network tagger.acter cj into a character embedding e?jc.
The char-acter embeddings e?1c , e?2c , ..., e?mc are combined usingan attention model(Bahdanau et al, 2015): w?ic =?mj=1 ajc ?
e?jc, where ajc is the weight for e?jc, ?
isthe Hadamard product function, and?mj=1 ajc = 1.Each ajc is computed according to both the wordembedding vector and 5-character embedding win-dow with the current character e?jc in the middle:ajc =tjc?m1 tjctjc =exp(Wth?jc +Ute?iw + b?t)h?ic =tanh(Wc(e?j?2c ?
e?j?1c ?
e?jc ?
e?j+1c?
e?j+2c ) + b?c)Here ?
denotes vector concatenation and e?iw is theembedding of current word wi.Wt,Ut,Wc and b?t,b?c are model parameters.
Finally, w?ic is concatenatedwith word embedding to form final word represen-tation r?iw: r?iw = e?iw ?
w?ic2.2 Feature LayerRecently, bi-directional LSTM has been success-fully applied in various NLP tasks (Liu et al, 2015;Zhou and Xu, 2015; Klerke et al, 2016; Plank et al,2016).
The feature layer uses a bi-directional LSTMto extract a feature vector h?i for each word wi, re-spectively.
An input vector x?i = (r?i?2w ?r?i?1w ?r?iw?r?i+1w ?
r?i+2w ) is used to represent each word wi.We use a LSTM variation with peephole connec-tions (Graves and Schmidhuber, 2005) to extract fea-tures based on x?(1:n).
The model computes a hid-den vector h?i for each input x?i , passing informationfrom h?1, ..., h?i?1 to h?n via a sequence of cell statesc?1, c?2, ..., c?n.
Information flow is controlled using aninput gate g?i, a forget gate f?
i, and an output gate o?i:g?i =?
(W(g)x?i +U(g)h?i?1 +V(g)c?i?1 + b?(g))f?
i =?
(W(f)x?i +U(f)h?i?1 +V(f)c?i?1 + b?
(f))c?i =f?
i ?
c?i?1+g?i ?
tanh(W(u)x?i +U(u)h?i?1 + b?
(u))o?i =?
(W(o)x?i +U(o)h?i?1 +V(o)c?i + b?
(o))h?i =o?i ?
tanh(c?i),where ?
denotes the component-wise sigmoid func-tion.
W(g), W(f), W(u), W(o), U(g), U(f), U(u),U(o), V(g), V(f), V(o), b?
(g), b?
(f), b?
(u), b?
(o) aremodel parameters.Bi-directional extension of the above LSTMstructure is applied in both the left-to-right direc-tion and the right-to-left direction, resulting in twohidden vector sequences h(1:n)l , h(1:n)r , respectively.Each h?il is combined with its corresponding h?ir forfinal feature vector h?if :h?if = tanh(Wlh?il +Wrh?ir + b?
),whereWl,Wr and b?
are model parameters.2.3 Output LayerThe output layer employs a conditional random field(CRF) to infer the POS ti of each word wi based onthe feature layer outputs.
The conditional probabil-ity of a tag sequence given an input sentence is:p(y?|x?)
=?ni=1 ?
(x?, y?i)?ni=1 ?
(x?, y?i, y?i?1)Z(x?)
,where Z(x?)
is the partition function:Z(x?)
=?y?n?i=1?
(x?, y?i)n?i=1?
(x?, y?i, y?i?1)In particular, the output clique potential ?
(x?, y?i)shows the correlation between inputs and output la-bels: ?
(x?, y?i) = exp(s?i), with the emission vectors?i being defined as:s?i = ?
?0h?if , (1)where ?
?0 is the model parameter.The edge clique potential shows the correlationbetween consecutive output labels using a singletransition weight ?
(y?i, y?i?1).733A tagger...B tagger... ?
...w2ta2tb2... ?w1ta1...tb1... ?
...wn-1tan-1tbn-1wn... ?
...tantbnA tagger...B tagger... ?
...w2tb2... ?w1...tb1... ?
...wn-1tbn-1wn... ?
...tbn... ... ... ...(a)  one-best-output level stacking (b)  feature level stackingFigure 3: Neural stacking.3 Stacking3.1 Discrete StackingStacking integrates corpora A and B by first traininga tagger on corpus A, and then using the A taggerto provide additional features to a corpus B model.Figure 1(a) shows the training and testing of dis-crete stacking models, where the B tagger extractsfeatures from both the raw sentence and A taggeroutput.
This method achieves feature combinationat the one-best-output level.3.2 Neural StackingFigure 3(a) and (b) shows the two neural stackingmethods of this paper, respectively.Shallow Integration.
Figure 3(a) is a variation ofdiscrete stacking, with the output tags from tagger Abeing converted to a low-dimensional dense embed-ding features, and concatenated to the word embed-ding inputs to tagger B.
Formally, for each word wi,denote the tagger A output as tia, we concatenate theembedding form of tia, denoted as e?ia, to the wordrepresentation r?iw.r?i?w = r?iw ?
e?ia = e?iw ?
w?ic ?
e?ia (2)Deep Integration.
Figure 3(b) offers deeper inte-gration between the A and B models, which is fea-sible only with neural network features.
We call thismethod feature-level stacking.
For feature-level in-tegration, the emission vector s?i in Eq.
(1) is takenfor input to tagger B via a projection:w?ia = tanh(Wss?i)r?iw = e?iw ?
w?ic ?
w?ia,whereWs is a model parameter.Fine-tuning.
Feature-level stacking further al-lows tagger A to be fine-tuned during the trainingOutput layer AInput layerFeature layerOutput layer B...w1 w2 wn-1 wnFigure 4: Neural multi-view model.of tagger B, with the loss function being back prop-agated to tagger A via the w?ia layer (shown in the reddotted lines in Figure 3(b)).
This is a further benefitof neural stacking compared with discrete stacking.4 Multi-view Learning4.1 Discrete Label CouplingAs shown in Figure 1(b), multi-view learning (Li etal., 2015) utilizes corpus A and corpus B simultane-ously for training.
The coupled tagger directly learnsthe logistic correspondences between both corpora,therefore can lead a more comprehensive usage ofcorpus A compared with stacking.
In order to bettercapture such correlation, specifically designed fea-ture templates between two tag sets are essential.For each training instances, both A and B labelsare needed.
However, one type of tag is missing.Li et al (2015) used a mapping function to supple-ment the missing annotations with the help of theannotated tag.
The result is a set of sentence withbundled tags in both annotations, but with ambigu-ities on one side, due to one-to-many mappings.
Liet al (2015) showed that speed can be significantlyimproved by manually restricting possible mappingsbetween the labelsets, but a full mapping without re-striction yields the highest accuracies.4.2 Neural Multi-task LearningNeural multi-task learning is free from manual fea-ture engineering, and avoids manual mapping func-734tions between tag sets by establishing two separateoutput layers, one for each type of label, with sharedlow-level parameters.
The general structure of aneural multi-viewmodel is shown in Figure 4, whichcan be regarded as a variation of the parameter shar-ing model of Caruana (1993) and Collobert et al(2011).
Leveraging heterogeneous annotations forthe same task, compared to parameter sharing be-tween different NLP tasks (Collobert et al, 2011),can benefit from tighter integration of information,and hence allows deeper parameter sharing.
Theseare verified empirically in the experiments.In training and testing, sentences from both cor-pora go through the same input layer and featurelayer.
The outputs of each type of tag is then com-puted separately according to the shared parameters.The conditional probability of a tag sequence givenan input sentence and its corpus type is:p(y?|x?, T ) =?ni=1 ?T (x?, y?i)?ni=1 ?T (x?, y?i, y?i?1)ZT (x?
),where T is the corpus type, T ?
{A,B}.
?T (x?, y?i)and ?T (x?, y?i, y?i?1) are the corresponding outputclique potential and edge clique potential, respec-tively.
ZT (x?)
is the partition function:ZT (x?)
=?y?n?i=1?T (x?, y?i)n?i=1?T (x?, y?i, y?i?1)This indicates that each time only one output layeris activated according to the corpus type of inputsentences.5 TrainingA max-margin objective is used to train the full setof model parameters ?:L(?)
= 1DD?d=1l(x?d, y?d,?)
+?2 ??
?2 ,where x?d, y?d|Dd=1 are the training examples, ?
isa regularization parameter, and l(x?d, y?d,?)
is themax-margin loss function towards one example(x?d, y?d).The max-margin loss function is defined as:l(x?d, y?d,?)
=maxy(s(y?|x?d,?)
+ ?
(y?, y?d))?
s(y?d|x?d,?
),Algorithm 1 Neural multi-view trainingInput: Two training datasets: D(1) =(x(1)n , y(1)n )|Nn=1, D(2) = (x(2)m , y(2)m )|Mm=2;Parameters: E, A, ROutput: ?1: for i = 1 to E do2: Sample A instances from D(1) and A ?
R in-stances from D(2) to form a new dataset Di3: Shuffle Di.4: for each batch Dbi in Di do5: Forward: compute the cost6: Backward: compute the loss of each pa-rameter7: Update the parameters8: end for9: end forsentences tokensCTBtrain 16091 437991dev 803 20454test 1910 50319PDtrain 100749 5194829dev 18875 958026Table 1: Data statistics.where y?
is the model output, s(y?|x?)
= logP (y?|x?)
isthe log probability of y?
and ?
(y?, y?d) is the Hammingdistance between y?
and y?d.We adopt online learning, updating parametersusing AdaGrad (Duchi et al, 2011).
To train theneural stacking model, we first train a base taggerusing corpus A.
Then, we train the stacked taggerwith corpus B, where the parameters of the A taggerhas been pretrained from corpus A and the B taggeris randomly initialized.For neural multi-view model, we follow Li et al(2015) and take a the corpus-weighting strategy tosample a number of training instances from both cor-pora for each training iteration, as shown in Algo-rithm 1.
At each epoch, we randomly sample fromthe two datasets according to a corpus weights ratio,namely the ratio between the number of sentences ineach dataset used for training, to form a training setfor the epoch.7356 Experiments6.1 Experimental SettingsWe adopt the Penn Chinese Treebank version 5.0(CTB5) (Xue et al, 2005) as our main corpus,with the standard data split following previous work(Zhang and Clark, 2008; Li et al, 2015).
People?sDaily (PD) is used as second corpus with a differ-ent scheme.
We filter out PD sentences longer than200 words.
Details of the datasets are listed in Table1.
The standard token-wise POS tagging accuracyis used as the evaluation metric.
The systems areimplemented with LibN3L (Zhang et al, 2016).For all the neural models, we set the hidden layersize to 100, the initial learning rate for Adagrad to0.01 and the regularization parameter ?
to 10?8.word2vec1 is used to pretrain word embeddings.The Chinese Giga-word corpus version 5 (Graff andChen, 2005), segmented by zpar2 (Zhang and Clark,2011), is used for the training corpus for word em-beddings.
The size of word embedding is 50.6.2 Development ExperimentsWe use the development dataset for two main pur-poses.
First, under each setting, we tune the modelparameters, such as the number of training epochs.Second, we study the influence of several importanthyper-parameters using the development dataset.For example, for the NN multi-view learning model,the corpus weights ratio (section 5) plays an im-portant role for the performance.
We determine theparameters of the model by studying the accuracyalong with the increasing epochs.Effect of batch size and dropout.
The batch sizeaffects the speed of training convergence and the fi-nal accuracies of the neural models, and the dropoutrate has been shown to significantly influence theperformance (Chen et al, 2015).
We investigate theeffects of these two hyper-parameters by adoptinga corpus weight ratio of 1 : 1 (All the CTB train-ing data is used, while the same amount of PD issampled randomly), drawing the accuracies of theneural multi-view learning model against the num-ber of training epochs with various combinations ofthe dropout rate d and batch size b.
The results are1https://code.google.com/p/word2vec2https://github.com/SUTDNLP/ZPar878889909192939495960  5  10  15  20  25  30AccuracyonCTB-dev(\%)epochesb=1;d=0.2b=20;d=0.2b=1;d=0b=20;d=0b=1;d=0.5b=20;d=0.5Figure 5: Accuracy on CTB-dev with different batchsizes and dropout rates for a multi-view learningmodel.
b represents batch size, d denotes dropoutrate.92.59393.59494.59595.5960  5  10  15  20  25  30AccuracyonCTB-dev(\%)epoches1:0.21:11:4Figure 6: Accuracy on CTB-dev with different cor-pus weights ratio.shown for the multi-view learning model.
For thestacking model, we use b=100 for the PD sub model.The results are shown in Figure 5, where the twodashed lines on the top at epoch 30 represent thedropout rate of 20%, the two solid lines in the mid-dle represent zero dropout rate, and the two dottedlines in the bottom represent a dropout rate 50%.Without using dropout, the performance increasesin the beginning, but then decreases as the numberof training epochs increases beyond 10.
This indi-cates that the NNmodels can overfit the training datawithout dropout.
However, when a 50% dropoutrate is used, the initial performances are significantlyworse, which implies that the 50% dropout rate canbe too large and leads to underfitting.
As a result, wechoose a dropout rate of 20% for the remaining ex-periments, which strikes the balance between over-736System AccuracyCRF Baseline (Li et al, 2015) 94.10CRF Stacking (Li et al, 2015) 94.81CRF Multi-view (Li et al, 2015) 95.00NN Baseline 94.24NN Stacking 94.74NN Feature Stacking 95.01NN Feature Stacking & Fine-tuning 95.32NN Multi-view 95.40Integrated NN Multi-view & Stacking 95.53Table 2: Accuracies on CTB-test.fitting and underfitting.Figure 5 also shows that the batch size has a rela-tive small influence on the accuracies, which variesaccording to the dropout rate.
We simply choose abatch size of 1 for the remaining experiments ac-cording to the performance at the dropout rate 20%.Effect of corpus weights ratio.
Figure 6 showsthe effects of different corpus weights ratios.
In par-ticular, a corpus weights ratio of 1:0.2 yields relativelow accuracies.
This is likely because it makes useof the least amount of PD data.
The ratios of 1:1and 1:4 give comparable performances.
We choosethe former for our final tests because it is a muchfaster choice.6.3 Final ResultsTable 2 shows the final results on the CTB test data.We lists the results of stacking method of Jiang etal.
(2009) re-implemented by Li et al (2015), andCRF multi-view method reported by Li et al (2015).We adopt pair-wise significance test (Collins et al,2005) when comparing the results between two dif-ferent models.Stacking.
For baseline tagging using only CTB,NN model achieves a result of 94.24, slightly higherthan CRF baseline (94.10).
NN stacking model in-tegrating PD data achieves comparable performance(94.74) compared with CRF stacking model (94.81).Compared with NN baseline, NN stacking modelboosts the performance from 94.24 to 94.74, whichis significant at the confidence level p < 10?5.
Thisdemonstrates that neural network model can utilizeone-best prediction of the PD model for the CTBtask as effectively as the discrete stacking methodof Jiang et al (2009).One advantage of NN stacking as compared withdiscrete stacking method is that it can directly lever-age features of PD model for CTB tagging.
Com-parison between feature-level stacking and one-best-output level stacking of the NN stacking modelshows that the former gives significantly higher re-sults, namely 95.01 vs 94.74 at the confidence levelp < 10?3.One further advantage of NN stacking is that itallows the PD model to be fine-tuned as an integralsub-model during CTB training.
This is not possiblefor the discrete stacking model, because the outputof the PD model are used as atomic feature in thestacking CTB model rather than a gradient admis-sive neural layer.
By fine-tuning the PD sub-model,the performance is further improved from 95.01 to95.32 at the confidence level p < 10?3.
The finalNN stacking model improves over the NN baselinemodel from 94.24 to 95.32.
The improvement is sig-nificantly higher compared to that by using discretestacking which improves over the discrete baselinefrom 94.01 to 94.74.
The final accuracy of the NNstacking model is higher than that of the CRF stack-ing model, namely 94.81 vs 95.32 at the confidencelevel p < 10?3.
This shows that neural stacking is apreferred choice for stacking.Multi-view training.
With respect of the multi-view training method, the NN model improves overthe NN baseline from 94.24 to 95.40, by a marginof +1.16, which is higher than that of 0.90 broughtby discrete method of Li et al (2015) over its base-line, from 94.10 to 95.00.
NN multi-view trainingmethod gives relatively higher improvements com-pared with NN stacking method.
This is consis-tent with the observation of Li et al (2015), whoshowed that discrete label coupling training givesslightly better improvement compared with discretestacking.
The final accuracies of NN multi-viewtraining is also higher than that of its CRF counter-part, namely 95.00 vs 95.40 at the confidence levelp < 10?3.
The difference between the final NNmulti-view training result of 95.40 and the final NNstacking results is not significant.3Integration.
The flexibility of the NN modelsfurther allows both stacking (on the input) and multi-viewing (on the output) to be integrated.
When3Note, however, NN stacking method with one-best PD out-put gives significantly lower accuracies (94.74).
It is the fine-tuning strategy that allows stacking to give comparable resultscompared to multi-view training for the NN models.737System Time Cost(s)CRF Baseline 176.925CRF Multi-view (Li et al, 2015) 3992.27NN Baseline 416.338NN Multi-view 418.484Table 3: Time for testing CTB training data.NN multi-view training is combined with a fine-tuned NN feature stacking model, the performancefurther increases from 95.40 to 95.53.
This is thebest results we are aware of on this dataset.
Theimprovement is significant at the confidence levelp < 10?2 compared with fine-tuned NN stackingmodel (95.32).
This indicates that NN multi-viewtraining and stacking model each provide unique ad-vantages for heterogeneous annotations.6.4 Speed TestWe compare the efficiencies of neural and discretemulti-view training by running our models and themodel of Li et al (2015)4 with default configura-tions on the CTB5 training data.
The CRF baselineis adapted from Li et al (2015).
All the systems areimplemented in C++ running on an Intel E5-1620CPU.
The results are shown in Table 3.The NN baseline model is slower than the CRFbaseline model.
This is due to the higher computa-tion cost of a deep neural network on a CPU.
Com-pared with the CRF baseline, the CRF multi-viewmodel is significantly slower because of its large fea-ture set and the multi-label search space.
However,the NN multi-view model achieves almost the sametime cost with the NN baseline, and is much moreefficient than the CRF counterpart.
This shows theefficiency advantage of the NN multi-view model byparameter sharing and output splitting.7 Related WorkEarly research on heterogeneous annotations fo-cuses on annotation conversion.
For example,Gao et al (2004) proposed a transformation-basedmethod to convert the annotation style of a wordsegmentation corpus to that of another.
Manuallydesigned transformation templates are used, whichmakes it difficult to generalize the method to other4http://hlt.suda.edu.cn/zhli/resources/zhenghua-acl2015-resources.ziptasks and treebanks.Jiang et al (2009) described a stacking-basedmodel for heterogeneous annotations, using apipeline to integrate the knowledge from one cor-pus to another.
Sun and Wan (2012) proposed astructure-based stacking model, which makes useof structured features such as sub-words for modelcombination.
These feature integration is strongercompared to those of Jiang et al (2009).
Johansson(2013) introduced path-based feature templates inusing one parser to guide another.
In contrast to theabove discrete methods, our neural stacking methodoffers further feature integration by directly connect-ing the feature layer of the source tagger with the in-put layer of the target tagger.
It also allows the fine-tuning of the source tagger.
As one of the reviewersmentioned, two extensions of CRFs, dynamic CRFs(Sutton et al, 2004) and hidden-state CRFs (Quat-toni et al, 2004), can also perform similar deep in-tegration and fine-tuning.For multi-view training, Johansson (2013) useda shared feature representation along with separateindividual feature representation for each treebank.Qiu et al (2013) proposed a multi-task learningmodel to jointly predict two labelsets given an in-put sentences.
The joint model uses the union ofbaseline features for each labelset, without consid-ering additional features to capture the interactionbetween the two labelsets.
Li et al (2015) im-proves upon this method by using a tighter integra-tion between the two labelsets, treating the Carte-sian product of the base labels as a single combinedlabelset, and exploiting joint features from two la-belsets.
Though capturing label interaction, theirmethod suffers speed penalty from the sharply in-creased search space.
In contrast to their methods,our neural approach enables parameter sharing inthe hidden layers, thereby modeling label interactionwithout directly combining the two output labelsets.This leads to a lean model with almost the same timeefficiency as a single-label baseline.Recently, Zhang and Weiss (2016) proposeda stack-propagation model for learning a stackedpipeline of POS tagging and dependency parsing.Their method is similar to our neural stacking infine-tuning the stacked module which yields featuresfor the target model.
While their multi-task learningis on heterogenous tasks, our multi-task learning is738defined on heterogenous treebanks.8 ConclusionWe investigated two methods for utilizing heteroge-neous annotations for neural network models, show-ing that they have respective advantages comparedto their discrete counterparts.
In particular, neuralstacking allows tighter feature integration comparedto discrete stacking, and neural multi-view trainingis free from the feature and efficiency constraintsof discrete one.
On a standard CTB test, the neu-ral method gives the best integration effect, with amulti-view training model enjoying the same speedas its single treebank baseline.AcknowledgmentsThe corresponding author is Yue Zhang.
We thankZhenghua Li and Meishan Zhang for providing dataand the anonymous reviewers for their constructivecomments, which helped to improve the paper.
Thiswork is supported by Singapore Ministry of Educa-tion Tier 2 Grant T2MOE201301 and Natural Sci-ence Foundation of China (61379086).ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In Proceedings ofICLR.Sabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The tiger treebank.In Proceedings of the workshop on treebanks and lin-guistic theories, volume 168.Leo Breiman.
1996.
Stacked regressions.
Machinelearning, 24(1):49?64.Richard A Caruana.
1993.
Multitask learning: Aknowledge-based source of inductive bias1.
In Pro-ceedings of the Tenth International Conference on Ma-chine Learning, pages 41?48.
Citeseer.Danqi Chen and Christopher D Manning.
2014.
Afast and accurate dependency parser using neural net-works.
In EMNLP, pages 740?750.Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Pengfei Liu, andXuanjing Huang.
2015.
Long short-term memoryneural networks for chinese word segmentation.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd annual meet-ing on association for computational linguistics, pages531?540.
Association for Computational Linguistics.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.Hal Daume?
III.
2007.
Frustratingly easy domain adapta-tion.
pages 256?263.Marie-Catherine De Marneffe, Bill MacCartney, Christo-pher D Manning, et al 2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC, volume 6, pages 449?454.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
The Journal of MachineLearning Research, 12:2121?2159.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependeny parsing with stack long short-termmemory.
In Proceedings of the 53rd Annual Meet-ing of the Association of Computational Linguisticsand the 7th International Joint Conference on NaturalLanguage Processing of the Asian Federation of Natu-ral Language Processing (ACL-IJCNLP 2015).
ACL.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedings ofthe fourth SIGHAN workshop on Chinese languageProcessing, volume 133.Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,Hongqiao Li, Xinsong Xia, and Haowei Qin.
2004.Adaptive chinese word segmentation.
In Proceed-ings of the 42nd Annual Meeting on Association forComputational Linguistics, page 462.
Association forComputational Linguistics.David Graff and Ke Chen.
2005.
Chinese gigaword.LDC Catalog No.
: LDC2003T09, ISBN, 1:58563?58230.Alex Graves and Ju?rgen Schmidhuber.
2005.
Frame-wise phoneme classification with bidirectional lstmand other neural network architectures.
Neural Net-works, 18(5):602?610.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Zhiheng Huang, Wei Xu, and Kai Yu.
2015.
Bidirec-tional lstm-crf models for sequence tagging.
arXivpreprint arXiv:1508.01991.Wenbin Jiang, Liang Huang, and Qun Liu.
2009.
Au-tomatic adaptation of annotation standards: Chineseword segmentation and pos tagging: a case study.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th International739Joint Conference on Natural Language Processing ofthe AFNLP: Volume 1-Volume 1, pages 522?530.
As-sociation for Computational Linguistics.Wenbin Jiang, Yajuan , Liang Huang, and Qun Liu.2015.
Automatic adaptation of annotations.
Compu-tational Linguistics, 41(1):1?29.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProceedings of NODALIDA 2007, pages 105?112,Tartu, Estonia, May 25-26.Richard Johansson.
2013.
Training parsers on incompat-ible treebanks.
In HLT-NAACL, pages 127?137.Sigrid Klerke, Yoav Goldberg, and Anders S?gaard.2016.
Improving sentence compression by learning topredict gaze.
arXiv preprint arXiv:1604.03357.Guillaume Lample, Miguel Ballesteros, Sandeep Subra-manian, Kazuya Kawakami, and Chris Dyer.
2016.Neural architectures for named entity recognition.arXiv preprint arXiv:1603.01360.Zhenghua Li, Jiayuan Chao, Min Zhang, and WenliangChen.
2015.
Coupled sequence labeling on hetero-geneous annotations: pos tagging as a case study.
InProceedings of the 53rd Annual Meeting of the Associ-ation for Computational Linguistics and the 7th Inter-national Joint Conference on Natural Language Pro-cessing, volume 1, pages 1783?1792.Pengfei Liu, Shafiq Joty, and Helen Meng.
2015.
Fine-grained opinion mining with recurrent neural networksand word embeddings.
In Conference on EmpiricalMethods in Natural Language Processing (EMNLP2015).Ji Ma, Yue Zhang, and Jingbo Zhu.
2014.
Taggingthe web: Building a robust web tagger with neuralnetwork.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 144?154, Baltimore,Maryland, June.
Association for Computational Lin-guistics.Joakim Nivre and Ryan T McDonald.
2008.
Integratinggraph-based and transition-based dependency parsers.In ACL, pages 950?958.Wenzhe Pei, Tao Ge, and Baobao Chang.
2014.
Max-margin tensor neural network for chinese word seg-mentation.
In ACL, pages 293?303.Barbara Plank, Anders S?gaard, and Yoav Goldberg.2016.
Multilingual part-of-speech tagging with bidi-rectional long short-term memory models and auxil-iary loss.
arXiv preprint arXiv:1604.05529.Xipeng Qiu, Jiayi Zhao, and Xuanjing Huang.
2013.Joint chinese word segmentation and pos tagging onheterogeneous annotated corpora with multiple tasklearning.
In EMNLP, pages 658?668.Ariadna Quattoni, Michael Collins, and Trevor Darrell.2004.
Conditional random fields for object recogni-tion.
Advances in Neural Information Processing Sys-tems, pages 1097?1104.Richard Sproat and Thomas Emerson.
2003.
The firstinternational chinese word segmentation bakeoff.
InProceedings of the second SIGHAN workshop on Chi-nese language processing-Volume 17, pages 133?143.Association for Computational Linguistics.Weiwei Sun and Xiaojun Wan.
2012.
Reducing approx-imation and estimation errors for chinese lexical pro-cessing with heterogeneous annotations.
In Proceed-ings of the 50th Annual Meeting of the Associationfor Computational Linguistics: Long Papers-Volume1, pages 232?241.
Association for Computational Lin-guistics.Charles Sutton, Andrew Mccallum, and Khashayar Ro-hanimanesh.
2004.
Dynamic conditional randomfields: Factorized probabilistic models for labeling andsegmenting sequence data.
In ICML, pages 693?723.Heike Telljohann, Erhard W Hinrichs, Sandra Ku?bler,Heike Zinsmeister, and Kathrin Beck.
2006.
Style-book for the tu?bingen treebank of written german(tu?ba-d/z).
In Seminar fur Sprachwissenschaft, Uni-versitat Tubingen, Tubingen, Germany.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural networktransition-based parsing.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 1: Long Pa-pers), pages 323?333, Beijing, China, July.
Associa-tion for Computational Linguistics.David H Wolpert.
1992.
Stacked generalization.
Neuralnetworks, 5(2):241?259.Dekai Wu, Grace Ngai, and Marine Carpuat.
2003.A stacked, voted, stacked model for named entityrecognition.
In Proceedings of the seventh conferenceon Natural language learning at HLT-NAACL 2003-Volume 4, pages 200?203.
Association for Computa-tional Linguistics.Naiwen Xue, Fei Xia, Fu-Dong Chiou, andMarta Palmer.2005.
The penn chinese treebank: Phrase structure an-notation of a large corpus.
Natural language engineer-ing, 11(02):207?238.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, 8(1):29?48.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.
InProceedings of IWPT, volume 3, pages 195?206.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-based740and transition-based dependency parsing using beam-search.
In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing, pages562?571.
Association for Computational Linguistics.Yue Zhang and Stephen Clark.
2011.
Syntactic process-ing using the generalized perceptron and beam search.Computational linguistics, 37(1):105?151.Yuan Zhang and David Weiss.
2016.
Stack-propagation:Improved representation learning for syntax.
In Pro-ceedings of the 54th Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 1557?1566.
Association for Computa-tional Linguistics.Meishan Zhang, Jie Yang, Zhiyang Teng, and YueZhang.
2016.
Libn3l:a lightweight package for neu-ral nlp.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Thierry Declerck, Sara Goggi, MarkoGrobelnik, Bente Maegaard, Joseph Mariani, He-lene Mazo, Asuncion Moreno, Jan Odijk, and SteliosPiperidis, editors, Proceedings of the Tenth Interna-tional Conference on Language Resources and Evalu-ation (LREC 2016), Paris, France, may.
European Lan-guage Resources Association (ELRA).Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.
2013.Deep learning for chinese word segmentation and postagging.
In EMNLP, pages 647?657.Jie Zhou and Wei Xu.
2015.
End-to-end learning of se-mantic role labeling using recurrent neural networks.In Proceedings of the Annual Meeting of the Associa-tion for Computational Linguistics.Hao Zhou, Yue Zhang, Shujian Huang, and Jiajun Chen.2015.
A neural probabilistic structured-predictionmodel for transition-based dependency parsing.
InProceedings of the 53rd Annual Meeting of the Associ-ation for Computational Linguistics and the 7th Inter-national Joint Conference on Natural Language Pro-cessing (Volume 1: Long Papers), pages 1213?1222,Beijing, China, July.
Association for ComputationalLinguistics.741
