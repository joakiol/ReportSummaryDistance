Computational Generation of ReferringExpressions: A SurveyEmiel Krahmer?Tilburg UniversityKees van Deemter?
?University of AberdeenThis article offers a survey of computational research on referring expression generation (REG).It introduces the REG problem and describes early work in this area, discussing what basicassumptions lie behind it, and showing how its remit has widened in recent years.
We discusscomputational frameworks underlying REG, and demonstrate a recent trend that seeks to linkREG algorithms with well-established Knowledge Representation techniques.
Considerable at-tention is given to recent efforts at evaluating REG algorithms and the lessons that they allowus to learn.
The article concludes with a discussion of the way forward in REG, focusing onreferences in larger and more realistic settings.1.
IntroductionSuppose one wants to point out a person in Figure 1 to an addressee.
Most speakershave no difficulty in accomplishing this task, by producing a referring expressionsuch as ?the man in a suit,?
for example.
Now imagine a computer being confrontedwith the same task, aiming to point out individual d1.
Assuming it has access to adatabase containing all the relevant properties of the people in the scene, it needs tofind some combination of properties which applies to d1, and not to the other two.There is a choice though: There are many ways in which d1 can be set apart from therest (?the man on the left,?
?the man with the glasses,?
?the man with the tie?
), andthe computer has to decide which of these is optimal in the given context.
Moreover,optimality can mean different things.
It might be thought, for instance, that referencesare optimal when they are minimal in length, containing just enough information tosingle out the target.
But, as we shall see, finding minimal references is computationallyexpensive, and it is not necessarily what speakers do, nor what is most useful to hearers.So, what is Referring Expression Generation?
Referring expressions play a central rolein communication, and have been studied extensively in many branches of (com-putational) linguistics, including Natural Language Generation (NLG).
NLG is con-cerned with the process of automatically converting non-linguistic information (e.g.,?
Tilburg Center for Cognition and Communication (TiCC), Tilburg University, The Netherlands.E-mail: e.j.krahmer@uvt.nl.??
Computing Science Department, University of Aberdeen, Scotland, UK.
E-mail: k.vdeemter@abdn.ac.uk.Submission received: 16 December 2009; revised submission received: 27 April 2011; accepted for publication:15 June 2011.?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 1Figure 1A simple visual scene.from a database) into natural language text, which is useful for practical applicationsranging from generating weather forecasts to summarizing medical information (Reiterand Dale 2000).
Of all the subtasks of NLG, Referring Expression Generation (REG) isamong those that have received most scholarly attention.
A survey of implemented,practical NLG systems shows that virtually all of them, regardless of their purpose,contain an REG module of some sort (Mellish et al 2006).
This is hardly surprisingin view of the central role that reference plays in communication.
A system providingadvice about air travel (White, Clark, and Moore 2010) needs to refer to flights (?thecheapest flight,?
?the KLM direct flight?
), a pollen forecast system (Turner et al 2008)needs to generate spatial descriptions for areas with low or high pollen levels (?the cen-tral belt and further North?
), and a robot dialogue system that assembles constructiontoys together with a human user (Giuliani et al 2010) needs to refer to the components(?insert the green bolt through the end of this red cube?
).REG ?is concerned with how we produce a description of an entity that enablesthe hearer to identify that entity in a given context?
(Reiter and Dale 2000, page 55).Because this can often be done in many different ways, a REG algorithm needs to makea number of choices.
According to Reiter and Dale (2000), the first choice concerns whatform of referring expression is to be used; should the target be referred to, for instance,using its proper name, a pronoun (?he?
), or a description (?the man with the tie?
).Proper names have limited applicability because many domain objects do not havea name that is in common usage.
For pronoun generation, a simple but conservativerule is discussed by Reiter and Dale (2000), similar to one proposed by Dale (1989,pages 150?151): Use a pronoun if the target was mentioned in the previous sentence,and if this sentence contained no reference to any other entity of the same gender.Reiter and Dale (2000) concentrate mostly on the generation of descriptions.
If the NLGsystem decides to generate a description, two choices need to be made: Which set ofproperties distinguishes the target (content selection), and how the selected propertiesare to be turned into natural language (linguistic realization).
Content selection is acomplex balancing act: We need to say enough to enable identification of the intendedreferent, but not too much.
A selection of information needs to be made, and this needsto be done quickly.
Reiter and Dale discuss various strategies that try to manage this174Krahmer and van Deemter Computational Generation of Referring Expressionsbalancing act, based on Dale and Reiter (1995), an early survey article that summarizesand compares various influential algorithms for the generation of descriptions.Why a survey on REG, and how to read it?
REG, like NLG in general, has changedconsiderably since the overviews presented in Dale and Reiter (1995) and Reiter andDale (2000), owing largely to an increased use of empirical data, and a widening ofthe class of referring expressions studied.
Moreover, a gradual shift has taken placetowards extended application domains, different input and output formats, and moreflexible interactions with the user, and this shift is starting to necessitate the use of newREG techniques.
Examples include recent systems in areas such as weather forecasting(Turner, Sripada, and Reiter 2009) and medical care (Portet et al 2009), where complexreferences to spatial regions and time periods abound.
The results of recent REGresearch are scattered over proceedings, books, and journals.
The current survey offersa compact overview of the progress in this area and an assessment of the state of the art.The concept of reference is difficult to pin down exactly (Searle 1969; Abbott 2010).Searle therefore suggests that the proper approach is ?to examine those cases which con-stitute the center of variation of the concept of referring and then examine the borderlinecases in light of similarities and differences from the paradigms?
(Searle 1969, pages 26?27).
The ?paradigms?
of reference in Reiter and Dale (2000) are definite descriptions whoseprimary purpose it is to identify their referent.
The vast majority of recent REG researchsubscribes to this view as well.
Accordingly these paradigmatic cases will also be themain focus of this survey, although we shall often have occasion to discuss other types ofexpressions.
However, to do full justice to indefinite or attributive descriptions, propernames, and personal pronouns would, in our view, require a separate, additional survey.In Section 2 we offer a brief overview of REG research up to 2000, discussingsome classic algorithms.
Next, we zoom in on the new directions in which recentwork has taken REG research: extension of the coverage of algorithms, to include, forexample, vague, relational, and plural descriptions (Section 3), exploration of differentcomputational frameworks, such as Graph Theory and Description Logic (Section 4),and collection of data and evaluation of REG algorithms (Section 5).
Section 6 highlightsopen questions and avenues for future work.
Section 7 summarizes our findings.2.
A Very Short History of Pre-2000 REG ResearchThe current survey focuses primarily on the progress in REG research in the 21stcentury, but it is important to have a basic insight into pre-2000 REG research and howit laid the foundation for much of the current work.2.1 First BeginningsREG can be traced back to the earliest days of Natural Language Processing; Winograd(1972) (Section 8.3.3, Naming Objects and Events), for example, sketches a primitive?incremental?
REG algorithm, used in his SHRDLU program.
In the 1980s, researcherssuch as Appelt and Kronfeld set themselves the ambitious task of modeling the humancapacity for producing and understanding referring expressions in programs such asKAMP and BERTRAND (Appelt 1985; Appelt and Kronfeld 1987; Kronfeld 1990).
Theyargued that referring expressions should be studied as part of a larger speech act.
KAMP(Appelt 1985), for example, was conceived as a general utterance planning system,building on Cohen and Levesque?s (1985) formal speech act theory.
It used logical175Computational Linguistics Volume 38, Number 1axioms and a theorem prover to simulate an agent planning instructions such as ?usethe wheelpuller to remove the flywheel,?
which contains two referring expressions, aspart of a larger utterance.Like many of their contemporaries, Appelt and Kronfeld hoped to gain insight intothe complexities of human communication.
Doug Appelt (personal communication):?the research themes that originally motivated our work on generation were the out-growth of the methodology in both linguistics and computational linguistics at the timethat research progress was best made by investigating hard, anomalous cases that posedifficulties for conventional accounts.?
Their broad focus allowed these researchers torecognize that although referring expressions may have identification of the referentas their main goal, a referring expression can also add information about a target.
Bypointing to a tool on a table while saying ?the wheelpuller,?
the descriptive content ofthe referring expression may serve to inform the hearer about the function of the tool(Appelt and Kronfeld 1987).
They also observed that referring expressions need to besensitive to the communicative context in which they are used and that they should beconsistent with the Gricean maxims (see subsequent discussion), which militate againstoverly elaborate referring expressions (Appelt 1985).It is remarkably difficult, after 20 years, to find out how these programs actuallyworked, because code was lost and much of what was written about them is pitched ata high level of abstraction.
Appelt and Kronfeld were primarily interested in difficultquestions about human communication, but they were sometimes tantalizingly briefabout humbler matters.
Here, for instance, is how Appelt (1985, page 21) explains howKAMP would attempt to identify a referent:KAMP chooses a set of basic descriptors when planning a describe action to minimiseboth the number of descriptors chosen, and the amount of effort required to plan thedescription.
Choosing a provably minimal description requires an inordinate amountof effort and contributes nothing to the success of the action.
KAMP chooses a set ofdescriptors by first choosing a basic category descriptor (see [Rosch 1978]) for theintended concept, and then adding descriptors from those facts about the object that aremutually known by the speaker and the hearer, subject to the constraint that they are alllinguistically realizable in the current noun phrase, until the concept has been uniquelyidentified.
.
.
.
Some psychological evidence suggests the validity of the minimaldescription strategy; however, one does not have to examine very many dialoguesto find counter-examples to the hypothesis that people always produce minimaldescriptions.This quote contains the seeds of much later work in REG, given its skepticism aboutthe naturalness of minimal descriptions, its use of Rosch (1978)?style basic categories,and its acknowledgment of the role of computational complexity.
Broadly speaking, itsuggests an incremental generation strategy, compatible with the ones described subse-quently, although it is uncertain what exactly was implemented.
In recent years, theAppelt?Kronfeld line of research has largely given way to a new research traditionwhich focused away from the full complexity of human communication, with notableexceptions such as Heeman and Hirst (1995), Stone and Webber (1998), O?Donnell,Cheng and Hitzeman (1998), and Koller and Stone (2007).2.2 Generating Distinguishing DescriptionsIn the early 1990s a new approach to REG started gaining currency, when Dale andReiter re-focused on the problem of determining what properties a referring expression176Krahmer and van Deemter Computational Generation of Referring Expressionsshould use if identification of the referent is the central goal (Dale 1989, 1992; Reiter1990; Reiter and Dale 1992).
This line of work culminated in the seminal paper byDale and Reiter (1995).
Like Appelt (1985), Dale and Reiter are concerned with the linkbetween the Gricean maxims and the generation of referring expressions.
They discussthe following pair of examples:(1) Sit by the table.
(2) Sit by the brown wooden table.In a situation where there is only one table, which happens to be brown and wooden,both the descriptions in (1) and (2) would successfully refer to their target.
However,if you hear (2) you might make the additional inference that it is significant to knowthat the table is brown and wooden; why else would the speaker mention theseproperties?
If the speaker merely wanted to refer to the table, your inference would bean (incorrect) ?conversational implicature,?
caused by the speaker?s violation of Grice?s(1975, page 45) Maxim of Quantity (?Do not make your contribution more informativethan is required?).
Dale and Reiter (1995) ask how we can efficiently compute whichproperties to include in a description, such that it successfully identifies the target whilenot triggering false conversational implicatures.
For this, they zoom in on a relativelystraightforward problem definition, and compare a number of concise, well-definedalgorithms solving the problem.Problem Definition.
Dale and Reiter (1995) formulate the REG problem as follows.Assume we have a finite domain D of objects with attributes A.
In our example scene(Figure 1), D = {d1, d2, d3} and A = {type, clothing, position, .
.
.}.
The type attribute has aspecial status in Dale and Reiter (1995) because it represents the semantic content ofthe head noun.
Alternatively, we could have defined an attribute gender, stating that itshould be realized as the head noun of a description.
Typically, domains are representedin a knowledge base such as Table 1, where different values are clustered togetherbecause they are associated with the same attribute.
Left, right, and middle, for example,belong to the attribute position, and are said to be three values that this attribute cantake.
The objects of which a given attribute?value combination (or ?property?)
is trueare said to form its denotation.
Sometimes we will drop the attribute, writing man,rather than ?type, man?, for instance.The REG task is now defined by Dale and Reiter (1995) through what may becalled identification of the target: given a target (or referent) object r ?
D, find a setof attribute?value pairs L whose conjunction is true of the target but not of any ofthe distractors (i.e., D ?
{r}, the domain objects different from the target).
L is calleda distinguishing description of the target.
In our simple example, suppose that {d1}is the target (and hence {d2, d3} the set of distractors), then L could, for example, beTable 1Tabular representation of some information in our example scene.Object type clothing positiond1 man wearing suit leftd2 woman wearing t-shirt middled3 man wearing t-shirt right177Computational Linguistics Volume 38, Number 1either {?type, man?, ?clothing, wearing suit?}
or {?type, man?, ?position, left?
}, which could berealized as ?the man wearing a suit?
or ?the man to the left.?
If identification were allthat counted, a simple, fast, and fault-proof REG strategy would be to conjoin all theproperties of the referent: This conjunction will identify the referent if it can be identifiedat all.
In practice, Dale and Reiter, and others in their wake, include an additionalconstraint that is often left implicit: that the referring expressions generated should be assimilar to human-produced ones as possible.
In the Evaluation and Conclusion sections,we return to this ?human-likeness?
constraint (and to variations on the same theme).Full Brevity and Greedy Heuristic.
Dale and Reiter (1995) discuss various algorithmswhich solve the REG task.
One of these is the Full Brevity algorithm (Dale 1989) whichdeals with the problem of avoiding false conversational implicatures in a radical way,by always generating the shortest possible distinguishing description.
Originally, theFull Brevity algorithm was meant to generate both initial and subsequent descriptions,by relying on a previous step that determines the distractor set based on which objectsare currently salient.
Given this set, it first checks whether there is a single property ofthe target that rules out all distractors.
If this fails, it considers all possible combinationsof two properties, and so on:1.
Look for a description L that distinguishes target r using one property.If success then return L. Else go to 2.2.
Look for a description L that distinguishes target r using two properties.If success then return L. Else go to 3.3.
EtceteraUnfortunately, there are two problems with this approach.
First, the problem of findinga shortest distinguishing description has a high complexity?it is NP hard (see, e.g.,Garey and Johnson 1979)?and hence is computationally very expensive, making itprohibitively slow for large domains and descriptions.
Second, Dale and Reiter note thathuman speakers routinely produce descriptions that are not minimal.
This is confirmedby a substantial body of psycholinguistic research (Olson 1970; Sonnenschein 1984;Pechmann 1989; Engelhardt, Bailey, and Ferreira 2006).An approximation of Full Brevity is the Greedy Heuristic algorithm (Dale 1989,1992), which iteratively selects the property which rules out most of the distractorsnot previously ruled out, incrementally augmenting the description based on whatproperty has most discriminatory power at each stage (as a result, it does not alwaysgenerate descriptions of minimal size).
The Greedy Heuristic algorithm is a moreefficient algorithm than the Full Brevity one, but it was soon eclipsed by anotheralgorithm (Reiter and Dale 1992; Dale and Reiter 1995), which turned out to be themost influential algorithm of the pre-2000 era.
It is this later algorithm that came to beknown as ?the?
Incremental Algorithm (IA).The Incremental Algorithm.
The basic idea underlying the IA is that speakers ?prefer?certain properties over others when referring to objects, an intuition supported by theexperimental work of, for instance, Pechmann (1989).
Suppose you want to refer toa person 10 meters away from you.
You might mention the person?s gender.
If thisis insufficient to single out the referent, you might be more likely to make use of thecolor of the person?s coat than to the color of her eyes.
Less preferred attributes, suchas eye color, are only considered if other attributes do not suffice.
It is this intuitionof a preference order between attributes that the IA exploits.
By making this order178Krahmer and van Deemter Computational Generation of Referring Expressionsa parameter of the algorithm, a distinction can be made between domain/genredependent knowledge (the preferences), and a domain-independent search strategy.As in the Greedy Heuristic algorithm, descriptions are constructed incrementally;but unlike the Greedy Heuristic, the IA checks attributes in a fixed order.
By groupingproperties into attributes, Dale and Reiter predict that all values of a given attribute havethe same preference order.
Ordering attributes rather than values may be disadvanta-geous, however.
A simple shape (e.g., a circle), or a size that is unusual for its target (e.g.,a tiny whale) might be preferred over a subtle color (purplish gray).
Also, some values ofa given attribute might be difficult to express, and ?dispreferred?
for this reason (kind oflike a ufo shape with a christmas tree sticking out the side).Figure 2 contains a sketch of the IA in pseudo code.
It takes as input a targetobject r, a domain D consisting of a collection of domain objects, and a domain-specificlist of preferred attributes Pref (1).
Suppose we apply the IA to d1 of our examplescene, assuming that Pref = type > clothing > position.
The description L is initializedas the empty set (2), and the context set C of distractors (from which d1 needs to bedistinguished) is initialized as D ?
{d1} (3).
The algorithm then iterates through the listof preferred attributes (4), for each one looking up the target?s value (5), and checkingwhether this attribute?value pair rules out any of the distractors not ruled out so far (6).The function RulesOut (?Ai, V?)
returns the set of objects which have a different valuefor attribute Ai than the target object has.
If one or more distractors are ruled out, theattribute?value pair ?Ai, V?
is added to the description under construction (7) and a newset of distractors is computed (8).
The first attribute to be considered is type, for whichd1 has the value man.
This would rule out d2, the only woman in our domain, and hencethe attribute?value pair ?type, man?
is added to L. The new set of distractors is C = {d3},and the next attribute (clothing) is tried.
Our target is wearing suit, and the remainingdistractor is not, so the attribute?value pair ?clothing, wearing suit?
is included as well.
Atthis point all distractors are ruled out (10), a set of properties has been found whichuniquely characterize the target {?type, man?, ?clothing, wearing suit?}
(the man wearing asuit), and we are done (11).
If we had reached the end of the list without ruling out alldistractors, the algorithm would have failed (13): No distinguishing description for ourtarget was found.The sketch in Figure 2 simplifies the original algorithm in a number of respects.First, Dale and Reiter always include the type attribute, even if it does not rule out anyFigure 2Sketch of the core Incremental Algorithm.179Computational Linguistics Volume 38, Number 1distractors, because speakers use type information in virtually all their descriptions.Second, the original algorithm checks, via a function called UserKnows, whether a givenproperty is in the common ground, to prevent the selection of properties which theaddressee might not understand.
Unlike Appelt and Kronfeld, who discuss detailedexamples that hinge on differences in common ground, Dale and Reiter (1995) treatUserKnows as a function that returns ?true?
for each true proposition, assuming thatall relevant information is shared.
Third, the IA can take some ontological informationinto account via subsumption hierarchies.
For instance, in a dog-and-cat domain, a petmay be of the chihuahua type, but chihuahua is subsumed by dog, and dog in turn issubsumed by animal.
A special value in such a subsumption hierarchy is reserved for theso-called basic level values (Rosch 1978); dog in this example.
If an attribute comes witha subsumption hierarchy, the IA first computes the best value for that attribute, whichis defined as the value closest to the basic level value, such that there is no more specificvalue that rules out more distractors.
In other words, the IA favors dog over chihuahua,unless the latter rules out more distractors.The IA is conceptually straightforward and easy to implement.
In addition, itis computationally efficient, with polynomial complexity: Its worst-case run time is aconstant function of the total number of attribute?value combinations available.
Thiscomputational efficiency is due to the fact that the algorithm does not perform back-tracking: once a property has been selected, it is included in the final referring expres-sion, even if later additions render it superfluous.
As a result, the final description maycontain redundant properties.
Far from seeing this as a weakness, Dale & Reiter (1995,page 19) point out that this makes the IA less ?psycholinguistically implausible?
than itscompetitors.
It is interesting to observe that whereas Dale and Reiter (1995) discuss thetheoretical complexity of the various algorithms in detail, later research has tended toattach more importance to empirical evaluation of the generated expressions (Section 5).2.3 DiscussionAppelt and Kronfeld?s work, founded on the assumption that REG should be seen aspart of a comprehensive model of communication, started to lose some of its appeal inthe early 1990s because it was at odds with the emerging research ethos in computa-tional linguistics that stressed simple, well-defined problems allowing for measurableresults.
The way current REG systems are shaped is largely due to developmentssummarized in Dale and Reiter (1995), which focuses on a specific aspect of REG,namely, determining which properties serve to identify some target referent.
Dale andReiter?s work aimed for generating human-like descriptions, but was not yet coupledwith systematic investigation of data.REG as Search.
The algorithms discussed by Dale and Reiter (1995) can be seen asdifferent instantiations of a general search algorithm (Bohnet and Dale 2005; Gatt 2007).They all basically search through the same space of states, each consisting of three com-ponents: a description that is true of the target, a set of distractors, and a set of propertiesof the target that have not yet been considered.
The initial state can be formalized as thetriple ?
?, C, P?
(no description for the target has been constructed, no distractors havebeen ruled out, and all properties P of the target are still available), and the goal state as?L, ?, P?
?, for certain L and P?
: A description L has been found, which is distinguishing?the set of distractors is empty.
All other states in the search space are intermediate ones,through which an algorithm might move depending on its search strategy.
For instance,180Krahmer and van Deemter Computational Generation of Referring Expressionswhen searching for a distinguishing description for d1 in our example domain, an inter-mediate state could be s = ?
{?type, man?
}, {d3}, {?clothing, wearing suit?, ?position, left?
}?.The algorithms discussed earlier differ in terms of their so-called expand-method,determining how new states are created, and their queue-method, which determines theorder in which these states are visited (i.e., how states are inserted into the queue).
FullBrevity, for example, uses an expand-method that creates a new state for each attributeof the target not checked before (as long as it rules out at least one distractor).
Startingfrom the initial state and applied to our example domain, this expand-method wouldresult in three new states, creating descriptions including type, clothing, and positioninformation, respectively.
These states would be checked using a queue-method whichis breadth-first.
The IA, by contrast, uses a different expand-method, each time creatinga single new state in accordance with the pre-determined preference order.
Thus, inthe initial state, and assuming (as before) that type is the most preferred attribute, theexpand-method would create a single new state: s above.
Because there is always onlyone new state, the queue-method is trivial.Limitations of pre-2000 REG.
In the IA and related algorithms, the focus is on efficientlycomputing which properties to use in a distinguishing description.
These algorithmsrest on a number of implicit simplifications of the REG task, however.
(1) The target isalways just one object, not a larger set (hence, plural noun phrases are not generated).
(2)The algorithms all assume a very simple kind of knowledge representation, consistingof a set of atomic propositions.
Negated propositions are only represented indirectly,via the Closed World Assumption, so an atomic proposition that is not explicitly listed inthe database is false.
(3) Properties are always ?crisp,?
never vague.
Vague propertiessuch as small and large are treated as Boolean properties, which do not allow borderlinecases and which keep the same denotation, regardless of the context in which they areused.
(4) All objects in the domain are assumed to be equally salient, which implies thatall distractors have to be removed, even those having a very low salience.
(5) The fullREG task includes first determining which properties to include in a description, andthen providing a surface realization in natural language of the selected properties.
Thesecond stage is not discussed, nor is the relation with the first.
A substantial part of re-cent REG research is dedicated to lifting one or more of these simplifying assumptions,although other limitations are still firmly in place (as we shall discuss in Section 6).3.
Extending the Coverage3.1 Reference to SetsUntil recently, REG algorithms aimed to produce references to a single object.
Butreferences to sets are ubiquitous in most text genres.
In simple cases, it takes only aslight modification to allow classic REG algorithms to refer to sets.
The IA, for example,can be seen as referring to the singleton set {r} that contains the target r and nothingelse.
If in line 1 (Figure 2), {r} is replaced by an arbitrary set S, and line 3 is modifiedas saying C ?
D ?
S instead of C ?
D ?
{r}, then the algorithm produces a descriptionthat applies to all elements of S. Thus, it is easy enough to let these algorithms produceexpressions like ?the men?
or ?the t-shirt wearers,?
to identify {d1, d3} and {d2, d3},respectively.
Unfortunately, things are not always so simple.
What if we need to referto the set {d1, d2}?
Based on the properties in Table 1 alone this is not possible, becaused1 and d2 have no properties in common.
The natural solution is to treat the target set181Computational Linguistics Volume 38, Number 1as the union of two smaller sets, {d1} ?
{d2}, and refer to both sets separately (e.g., ?theman who wears a suit, and the woman?).
Once unions are used, it becomes natural toallow set complementation as well, as in ?the people who are not on the right.?
Notethat set complementation may also be useful for single referents.
Consider a situationwhere all cats except one are owned by Mary, and the owner of the remaining one isunknown or non-existent.
Complementation allows one to refer to ?the cat not ownedby Mary.?
We shall call the resulting descriptions Boolean.As part of a more general logical analysis of the IA, van Deemter (2002) made afirst stab at producing Boolean descriptions, using a two-stage algorithm whose firststage is a generalization of the IA, and whose second stage involves the optimization ofthe possibly lengthy expressions produced by the first phase.
The resulting algorithm islogically complete in the following sense: If a given set can be described at all using theproperties available then this algorithm will find such a description.
With intersectionas the only way to combine properties, REG cannot achieve logical completeness.The first stage of the algorithm starts by conjoining properties (man, left) (omittingattributes for the sake of readability) in the familiar manner of the IA; if this does notsuffice for singling out the target set then the same incremental process continues withunions of two properties (e.g., man ?
woman, middle ?
left; that is, properties expressingthat a referent is a man or a woman, in the middle or on the left), then with unions ofthree properties (e.g., man ?
wearing suit ?
woman), and so on.
The algorithm terminateswhen the referent (individual or set) is identified (success) or when all combinationsof properties have been considered (failure).
Figure 3 depicts this in schematic form,where n represents the total number of properties in the domain, and P+/?
denotes theset of all literals (atomic properties such as man, and their complements ?man).
Step (1)generalizes the original IA allowing for negated properties and target sets.
As before, Lis the description under construction.
It will consist of intersections of unions of literalssuch as (woman ?
man) ?
(woman ?
?wearing suit) (in other words, L is in ConjunctiveNormal Form, CNF).Note that this first stage is not only incremental at each of its n steps, but alsoas a whole: Once a property has been added to the description, later steps will notremove it.
This can lead to redundancies, even more than in the original IA.
Thesecond stage removes the most blatant of these, but only where the redundancy existsas a matter of logic, rather than world knowledge.
Suppose, for example, that Step 2selects the properties P ?
S and P ?
R, ruling out all distractors.
L now takes theform (P ?
S) ?
(P ?
R) (e.g., ?the things that are both (women or men) and (womenor wearing suits)?).
The second phase uses logic optimization techniques, originallydesigned for the minimization of digital circuits (McCluskey 1965), to simplify this toP ?
(S ?
R) (?the women, and the men wearing suits?
).Figure 3Outline of the first stage of van Deemter?s (2002) Boolean REG algorithm.182Krahmer and van Deemter Computational Generation of Referring ExpressionsVariations and Extensions.
Gardent (2002) drew attention to situations where this pro-posal produces unacceptably lengthy descriptions; suppose, for example, the algorithmaccumulates numerous properties during Steps 1 and 2, before finding one complexproperty (a union of three properties) during Step 3 which, on its own would havesufficed to identify the referent.
This will make the description generated much length-ier than necessary, because the properties from Steps 1 and 2 are now superfluous.Gardent?s take on this problem amounts to a reinstatement of Full Brevity embeddedin a reformulation of REG as a constraint satisfaction problem (see Section 4.2).
Theexistence of fast implementations for constraint satisfaction alleviates the problems withcomputational tractability to a considerable extent.
But by re-instating Full Brevity,algorithms like Gardent?s could run into the empirical problems noted by Dale andReiter, given that human speakers frequently produce non-minimal descriptions (seeGatt [2007] for evidence pertaining to plurals).Horacek (2004) makes a case for descriptions in Disjunctive Normal Form (DNF;unions of intersections of literals).
Horacek?s algorithm first generates descriptionsin CNF, then convert these into DNF, skipping superfluous disjuncts.
Consider ourexample domain (Table 1).
To refer to {d1, d2}, a CNF-oriented algorithm might gen-erate (man ?
woman) ?
(left ?
middle) (?the people who are on the left or middle?
).Horacek converts this, first, into DNF: (man ?
left) ?
(woman ?
middle) ?
(man ?
middle) ?
(woman ?
left), after which the last two disjuncts are dropped, because there are no menin the middle, and no women on the left.
The outcome could be worded as ?the man onthe left and the woman in the middle.?
Later work has tended to agree with Horacek inopting for DNF instead of CNF (Gatt 2007; Khan, van Deemter, and Ritchie 2008).Perspective and Coherence.
Recent studies have started to bring data-oriented methodsto the generation of references to sets (Gatt 2007; Gatt and van Deemter 2007; Khan,van Deemter, and Ritchie 2008).
One finding is that referring expressions benefit from a?coherent?
perspective.
For example, ?the Italian and the Greek?
is normally a betterway to refer to two people than ?the Italian and the cook,?
because the former isgenerated from one coherent perspective (i.e., nationalities).
Two questions need to beaddressed, however.
First, how should coherence be defined?
Gatt (2007) opted for adefinition that assesses the coherence of a combination of properties using corpus-basedfrequencies as defined by Kilgarriff (2003), which in turn is based on Lin (1998).
Thischoice was supported by a range of experiments (although the success of the approachis less well attested for descriptions that contain adjectives).
Secondly, what if fullcoherence can only be achieved at the expense of brevity?
Suppose a domain containsone Italian and two Greeks.
One of the Greeks is a cook, whereas the other Greek andthe Italian are both IT consultants.
If this is all that is known, the generator faces a choicebetween either generating a description that is fully coherent but unnecessarily lengthy(?the Italian IT consultant and the Greek cook?
), or brief but incoherent (?The Italianand the cook?).
Simply saying ?The Italian and the Greek?
would not be distinguishing.In such cases, coherence becomes a tricky, and computationally complex, optimizationproblem (Gatt 2007; Gatt and van Deemter 2007).Collective Plurals.
Reference to sets is a rich topic, where many issues on the border-line between theoretical, computational, and experimental linguistics are waiting to beexplored.
Most computational proposals, so far, use properties that apply to individualobjects.
To refer to a set, in this view, is to say things that are true of each member ofthe set.
Such references may be contrasted with collective ones (e.g., ?the lines that runparallel to each other,?
?the group of four people?)
which are more complicated from a183Computational Linguistics Volume 38, Number 1semantic point of view (Scha and Stallard 1988; L?nning 1997, among others).
For initialideas about the generation of collective plurals, we refer to Stone (2000).3.2 Relational DescriptionsAnother important limitation of most early REG algorithms is that they are restricted toone-place predicates (e.g., ?being a man?
), instead of relations involving two or morearguments.
Even a property like ?wearing a suit?
is modeled as if it were simply aone-place predicate without internal structure (instead of a relation between a personand a piece of clothing).
This means that the algorithms in question are unable toidentify one object via another, as when we say ?the woman next to the man who wearsa suit,?
and so on.One early paper does discuss relational descriptions, making a number of importantobservations about them (Dale and Haddock 1991).
First, it is possible to identify anobject through its relations to other objects without identifying each of these objectsseparately.
Consider a situation involving two cups and two tables, where one cup is onone of the tables.
In this situation, neither ?the cup?
nor ?the table?
is distinguishing,but ?the cup on the table?
succeeds in identifying one of the two cups.
Secondly,descriptions of this kind can have any level of ?depth?
: in a complex situation, one mightsay ?the white cup on the red table in the kitchen,?
and so on.
To be avoided, however,are the kinds of repetitions that can arise from descriptive loops, because these do notadd information.
It would, for example, be useless to describe a cup as ?the cup to theleft of the saucer to the right of the cup to the left of the saucer .
.
.
?
We shall return to thisissue in Section 4, where we shall ask how suitable each of a number of representationalframeworks is for the proper treatment of relational descriptions.Various researchers have attempted to extend the IA by allowing relational descrip-tions (Horacek 1996; Krahmer and Theune 2002; Kelleher and Kruijff 2006), often basedon the assumption that relational properties (like ?x is on y?)
are less preferred thannon-relational ones (like ?x is white?).
If a relation is required to distinguish the targetx, the basic algorithm is applied iteratively to y.
It seems, however, that these attemptswere only partly successful.
One of the basic problems is that relational descriptions?just like references to sets, but for different reasons?do not seem to fit in well with anincremental generation strategy.
In addition, it is far from clear that relational propertiesare always less preferred than non-relational ones (Viethen and Dale 2008).
Viethenand Dale suggest that even in simple scenes, where objects can easily be distinguishedwithout relations, participants still use relations frequently (in about one third of thetrials).
We return to this in Section 5.On balance, it appears that the place of relations in reference is only partly under-stood, with much of the iceberg still under water.
If two-place relations can play a rolein REG, then surely so can n-place relations for larger n, as when we say ?the city thatlies between the mountains and the sea?
(n = 3).
No existing proposal has addressed n-place relations in general, however.
Moreover, human speakers can identify a man as theman who ?kissed all women,?
?only women,?
or ?two women.?
The proposals discussedso far do not cover such quantified relations, but see Ren, van Deemter, and Pan (2010).3.3 Context Dependency, Vagueness, and GradabilitySo far we have assumed that properties have a crisply defined meaning that is fixed,regardless of the context in which they are used.
But many properties fail to fit thismold.
Consider the properties young and old, for example.
In Figure 1, it is the leftmost184Krahmer and van Deemter Computational Generation of Referring Expressionsmale who looks the older of the two.
But if we add an old-age pensioner to the scenethen suddenly he is the most obvious target of expressions like ?the older man?
or ?theold man.?
Whether a man counts as old or not, in other words, depends on what otherpeople he is compared to: being old is a context-dependent property.
The concept ofbeing ?on the left?
is context-dependent too: Suppose we add five people to the right ofthe young man in Figure 1; now all three characters originally depicted are suddenly onthe left, including the man in the t-shirt who started out on the right.Concepts like ?old?
and ?left?
involve comparisons between objects.
Therefore, ifthe knowledge base changes, the objects?
descriptions may change as well.
But evenif the knowledge base is kept constant, the referent may have to be compared againstdifferent objects, depending on the words in the expression.
The word ?short?
in ?Johnis a short basketball player,?
for example, compares John?s height with that of the otherbasketball players, whereas ?John is a short man?
compares its referent with all theother men, resulting in different standards for what it means to be short.?Old?
and ?short?
are not only context dependent but also gradable, meaning thatyou can be more or less of it (older, younger, shorter, taller) (Quirk et al 1980).
Gradablewords are extremely frequent, and in many NLG systems they are of great importance,particularly in those that have numerical input, for example, in weather forecasting(Goldberg, Driedger, and Kittredge 1994) or medical decision support (Portet et al2009).
In addition to being context-dependent, they are also vague, in the sense thatthey allow borderline cases.
Some people may be clearly young, others clearly not, butthere are borderline cases for whom it is not quite clear whether they were included.Context can help to diminish the problem, but it won?t go away: In the expression?short basketball player,?
the noun gives additional information about the intendedheight range, but borderline cases still exist.Generating Vague References.
REG, as we know it, lets generation start from a Knowl-edge Base (KB) whose facts do not change as a function of context.
This means thatcontext-dependent properties like a person?s height need to be stored in the KB in amanner that does not depend on other facts.
It is possible to deal with size adjectives ina principled way, by letting one?s KB contain a height attribute with numerical values.Our running example can be augmented by giving each of the three people a preciseheight, for example: height(d1) = 170 cm, height(d2) = 180 cm and height(d3) = 180 cm(here the height of the woman d2 has been increased for illustrative purposes).
Nowimagine we want to refer to d3.
This target can be distinguished by the set of twoproperties {man, height = 180 cm}.
Descriptions of this kind can be produced by meansof any of the classic REG algorithms.Given that type and height identify the referent uniquely, this set of properties can berealized simply as ?the man who is 180 cm tall.?
But other possibilities exist.
Given that180 cm is the greatest height of all men in this KB, the set of properties can be convertedinto {man, height = maximum}, where the exact height has been pruned away.
The newdescription can be realized as ?the tallest man?
or simply as ?the tall man?
(providedthe referent?s height exceeds a certain minimum value).
The algorithm becomes morecomplicated when sets are referred to (because the elements of the target set may not allhave the same heights), or when two or more gradable properties are combined (as in?the strong, tall man in the expensive car?)
(van Deemter 2006).Variations and Extensions.
Horacek (2005) integrates vagueness with other types ofuncertainty.
Horacek could be said to depict an REG algorithm as essentially a gamblerwho wants to maximize the chance of the referent being identified on the basis of185Computational Linguistics Volume 38, Number 1the generated expression.
Other things being equal, for example, it may be safer toidentify a dog as being ?owned by John,?
than as being ?tall,?
because the latter involvesborderline cases.
A similar approach can be applied to perceptual uncertainty (as whenit is uncertain whether the hearer will be able to observe a certain property), or to theuncertainty associated with little-known words (e.g., will the hearer know what a bassethound is?)
Quantifying all types of uncertainties could prove problematic in practice,yet by portraying a generator as a gambler, Horacek has highlighted an importantaspect of reference generation which had so far been ignored.
Crucially, his approachmakes the success of a description a matter of degrees.The idea that referential success is a matter of degrees appears to be confirmedby recent applications of REG to geo-spatial data.
Here there tend to arise situationsin which it is simply not feasible to produce a referring expression that identifies itstarget with absolute precision (though good approximations may exist).
Once again, thedegree of success of a referring expression becomes gradable.
Suppose you were askedto describe that area of Scotland where the temperature is expected to fall below zeroon a given night, based on some computer forecast of the weather.
Even if we assumethat this is a well-defined area with crisp boundaries, it is not feasible to identify thearea precisely, because listing all the thousands of data points that make up the areaseparately is hardly an option.
Various approximations are possible, including:(3) Roads above 500 meters will be icy.
(4) Roads in the Highlands will be icy.Descriptions of this kind are generated by a system for road gritting, where the decisionof which roads to treat with salt depends on the description generated by the system(Turner, Sripada, and Reiter 2009): Roads where temperatures are predicted to be icyshould be treated with salt; others should not.
These two descriptions are arguablyonly partially successful in singling out the target area.
Generally speaking, one candistinguish between false positives and false negatives: The former are roads that arecovered by the description but should not be (because the temperature there is notpredicted to fall below zero); the latter are icy roads that will be left un-gritted.
Turnerand colleagues decided that it would be unacceptable to have even one false negative.In other situations, safety (from accidents) and environmental damage (through salt)might be traded off in different ways, for example, by associating a finite cost witheach false positive and a possibly different cost with each false negative, and choos-ing the description that is associated with the lowest total cost (van Deemter 2010,pages 253?254).
Again, a crucial and difficult part is to come up with the right costfigures.3.4 Degrees of Salience and the Generation of PronounsWhen we speak about the world around us, we do not pay equal attention to all theobjects in it.
In a novel, for example, a sentence like ?Smiley saw the man approaching?does not mean that Smiley saw the only man: It simply means that Smiley saw the manwho is most salient at this stage of the novel.
Passonneau (1996) and Jordan (2000) haveshown how algorithms such as the IA may produce reasonable referring expressions?in context,?
by limiting the set of salient objects in some sensible way?for example,to those objects mentioned in the previous utterance.
Salience, in these works, wastreated as a two-valued, ?black-or-white?
concept.
But perhaps it is more natural to186Krahmer and van Deemter Computational Generation of Referring Expressionsthink of salience?just like height or age?as coming in degrees.
Existing theories oflinguistic salience do not merely separate what is salient from what is not.
They assignreferents to different salience bands, based on factors such as recency of mention andsyntactic structure (Gundel, Hedberg, and Zacharski 1993; Hajic?ova?
1993; Grosz, Joshi,and Weinstein 1995).Salience and Context-Sensitive REG.
Early REG algorithms (Kronfeld 1990; Dale andReiter 1995) assumed that salience could be modeled by means of a focus stack(Grosz and Sidner 1986): A referring expression is taken to refer to the highest elementon the stack that matches its description (see also DeVault, Rich, and Sidner 2004).Krahmer and Theune (2002) argue that the focus stack approach is not flexible enoughfor context-sensitive generation of descriptions.
They propose to assign individualsalience weights (sws) to the objects in the domain, and to reinterpret referring expres-sions like ?the man?
as referring to the currently most salient man.
Once such a gradablenotion of salience is adopted, we are back in the territory of Section 3.3.
One simple wayto generate context-sensitive referring expressions is to keep the algorithm of Figure 2exactly as it is, but to limit the set of distractors to only those domain elements whosesalience weight is at least as high as that of the target r. Line 3 (Figure 2) becomes:3?.
C ?
{x | sw(x) ?
sw(r)} ?
{r}To see how this works, consider the knowledge base of Table 1 once again, assumingthat sw(d1) = sw(d2) = 10, and sw(d3) = 0 (d1 and d2 are salient, for example, becausethey were just talked about, and d3 was not).
Suppose we keep the same domain andpreference order as before.
Now if d1 is the target, then, according to the new definition3?, C = {d1, d2} ?
{d1} = {d2} (i.e., d2 is the only distractor which is at least as salient asthe target, d1).
The algorithm will select ?type, man?, which rules out the sole distractord2, leading to a successful reference (?The man?).
If, however, d3 would be the targetthen C = {d1, d2, d3} ?
{d3} = {d1, d2}, and the algorithm would operate as normal,producing a description realizable as ?the man in the t-shirt.?
Krahmer and Theunechose to graft a variant of this idea onto the IA, but application to other algorithms isstraightforward.Krahmer and Theune (2002) compare two theories of computing linguisticsalience?one based on the hierarchical focus constraints of Hajic?ova?
(1993), the otheron the centering constraints of Grosz, Joshi, and Weinstein (1995).
They argue that thecentering constraints, combined with a gradual decrease in salience of non-mentionedobjects (as in the hierarchical focus approach) yields the most natural results.
Interest-ingly, the need to compute salience scores can affect the architecture of the REG module.In Centering Theory, for instance, the salience of a referent is co-determined by thesyntactic structure of the sentence in which the reference is realized; it matters whetherthe reference is in subject, object, or another position.
This suggests an architecture inwhich REG and syntactic realization should be interleaved, a point to which we returnsubsequently.Variations and Extensions.
Once salience of referring expressions is taken into accountand they are no longer viewed as de-contextualized descriptions of their referent, anumber of questions come up.
When, for example, is it appropriate to use a demon-strative (?this man,?
?that man?
), or a pronoun (?he,?
?she?)?
As for demonstratives, ithas proven remarkably difficult to decide when these should be used, and even harderto choose between the different types of demonstratives (Piwek 2008).
Concerningpronouns, Krahmer and Theune suggested that ?he?
abbreviates ?the (most salient)187Computational Linguistics Volume 38, Number 1man,?
and ?she?
?the (most salient) woman.?
In this way, algorithms for generatingdistinguishing descriptions might also become algorithms for pronoun generation.
Suchan approach to pronoun generation is too simple, however, because additional factorsare known to determine whether a pronoun is suitable or not (McCoy and Strube 1999;Henschel, Cheng, and Poesio 2000; Callaway and Lester 2002; Kibble and Power 2004).Based on analyses of naturally occurring texts, McCoy and Strube (1999), for example,emphasized the role of topics and discourse structure for pronoun generation, andpointed out that the changes in time scale are a reliable cue for this.
In particular,they found that in certain places a definite description was used where a pronounwould have been unambiguous.
This happened, for example, when the time frame ofthe current sentence differed from that of the sentence in which the previous mentionoccurred, as can be signaled, for example, by a change in tense or a cue-phrase such as?several months ago.?
Kibble and Power (2004), in an alternative approach, use Cen-tering Theory as their starting point in a constraint-based text generation framework,taking into account constraints such as salience, cohesion, and continuity for the choiceof referring expressions.Many studies on contextual reference take text as their starting point (Poesio andVieira 1998; Belz et al 2010, among others), unlike the majority of REG research dis-cussed so far, which uses standard knowledge representations of the kind exemplified inTable 1 (or some more sophisticated frameworks, see Section 4).
An interesting variantis presented by Siddharthan and Copestake (2004), who set themselves the task of gen-erating a referring expression at a specific point in a discourse, without assuming that aknowledge base (in the normal sense of the word) is available: All their algorithm hasto go by is text.
For example, a text might start saying ?The new president applaudedthe old president.?
From this alone, the algorithm has to figure out whether, in the nextsentence, it can talk about ?the old president?
(or some other suitable noun phrase)without risk of misinterpretation by the reader.
The authors argue that standard REGmethods can achieve reasonable results in such a setting, particularly (as we shall seenext) with respect to the handling of lexical ambiguities that arise when a word candenote more than one property.
Lexical issues such as these transcend the selection ofsemantic properties.
Clearly, it is time for us to consider matters that lie beyond ContentDetermination.Before we do this, however, we would like to mention that differences in saliencecan also be caused by nonlinguistic factors: Some domain objects, for example, may beless salient because they are further removed from the hearer than others.
Paraboni,van Deemter, and Masthoff (2007) demonstrated experimentally that such situationsmay require substantial deviations from existing algorithms to avoid causing unreason-able amounts of work to the reader.
To see the idea, consider the way we refer to a(non-salient) address on a map: we probably don?t say ?Go to house number 3012 inAberdeen,?
even if only one house in Aberdeen has that number and this descriptionis thus perfectly distinguishing.
It is more likely we say something like ?Go to housenumber 3012 So-and-so Road, in the West End of Aberdeen,?
adding logically redun-dant information specifically to aid the hearer?s search.3.5 Beyond Content DeterminationIn many early REG proposals, Lexical Choice and Surface Realization follow ContentDetermination, in the style of a pipeline, with most of the actual research focusingpredominantly on Content Determination.
One might have thought that good resultsare easy to achieve by sending the output of the Content Determination module to188Krahmer and van Deemter Computational Generation of Referring Expressionsa generic realizer (i.e., a program converting meaning representations into naturallanguage).
With hindsight, any such expectations must probably count as naive.Some REG studies have taken a different approach, interleaving Content Determi-nation and Surface Realization (Horacek 1997; Stone and Webber 1998; Krahmer andTheune 2002; Siddharthan and Copestake 2004), running counter to the pipeline archi-tecture (Mellish et al 2006).
In this type of approach, syntactic structures are built upin tandem with semantic descriptions: when ?type, man?
has been added to the semanticdescription, a partial syntactic tree is constructed for a noun phrase, whose head noun isman.
As more properties are added to the semantic description, appropriate modifiersare slotted into the syntax tree; finally, the noun phrase is completed by choosing anappropriate determiner.Even in these interleaved architectures, it is often assumed that there is a one-to-onecorrespondence between properties and words; but often a property can be expressedby different words, one of which may be more suitable than the other?for example,because it is unambiguous whereas the other is not (Siddharthan and Copestake 2004).One president may be ?old?
in the sense of former, whereas another is ?old?
in thesense of aged, in which case ?the old president?
can become ambiguous between thetwo people.
To deal with the choice between ?old?
and ?former,?
Siddharthan andCopestake propose to look at discriminatory power, the idea being that in this case?former?
rules out more distractors than ?old?
(both presidents are old).
One wonders,however, to what extent readers interpret ambiguous words ?charitably?
: Suppose twopresidents are aged, while only one is the former president.
In this situation, ?the oldpresident?
seems clear enough, because only one of its two interpretations justifies thedefinite article (namely the one where ?old?
is to be understood as ?former?).
Clearly,people?s processing of ambiguous expressions is an area where there is still much toexplore.If we turn away from Siddharthan and Copestake?s set-up, and return to the sit-uation where generation starts from a non-textual knowledge base, similar problemswith ambiguities may arise.
In fact, the problem is not confined to Lexical Choice:Ambiguities can arise during Surface Realization as well.
To see this, suppose ContentDetermination has selected the properties man and with telescope to refer to a person,and the result after Surface Realization and Lexical Choice is ?John saw the man with thetelescope?
; then, once again, the clarity of the semantic description can be compromisedby putting the description in a larger context, causing an attachment ambiguity, whichmay sometimes leave it unclear what man is the intended referent of the description.The generator can save the day by choosing a different realization, generating ?John sawthe man who holds the telescope?
instead.
Similar ambiguities occur in conjoined referencesto plurals, as in ?the old men and women,?
where ?old?
may or may not pertain to thewomen.
These issues have been studied in some detail as part of a systematic study ofthe ambiguities that arise in coordinated phrases of the form ?the Adjective Noun andNoun,?
asking when such phrases give rise to actual comprehension problems, andwhen they should be avoided by a generator (Chantree et al 2005; Khan, van Deemter,and Ritchie 2008).When the generated referring expressions are realized in a medium richer than plaintext, for instance, in the context of a virtual character (Gratch et al 2002), another set ofissues comes into play.
It needs to be decided, then, which words should be emphasizedin speech, possibly in combination with visual cues such as eyebrow movements andother gestures.
Doing full justice to the expanding literature on multimodal referenceis beyond the scope of this survey, but a few pointers may be useful.
Various earlystudies looked at multimodal reference (Lester et al 1999).
One account, where pointing189Computational Linguistics Volume 38, Number 1gestures directly enter the Content Determination module of REG, is presented by vander Sluis and Krahmer (2007), who focus on the trade-off between gestures and words.Kopp, Bergmann, and Wachsmuth (2008) are more ambitious, modeling different kindsof pointing gestures and integrating their approach with the generation strategy ofStone et al (2003).3.6 DiscussionEarly REG research made a number of simplifying assumptions, and as a result the earlyREG algorithms could only generate a limited variety of referring expressions.
Whenresearchers started lifting some of these assumptions, this resulted in REG algorithmswith an expanded repertoire, being able to generate, for instance, plural and relationaldescriptions.
This move created a number of new challenges, however.
For instance, thenumber of ways in which one can refer to a set of target objects increases, so choosing agood referring expression is more difficult as well.
Should we prefer, for example, ?themen not wearing an overcoat,?
?the young man and the old man,?
or ?the men left ofthe woman??
In addition, from a search perspective, the various proposals result in alarger search space, making computational issues more pressing.
For some of the exten-sions (e.g., where Boolean combinations of properties are concerned), the complexityof the resulting algorithm is substantially higher than that of the base IA.
Moreover,researchers have often zoomed in on one extension of the IA, developing a new versionwhich lifts one particular limitation.
Combining all the different extensions into onealgorithm which is capable of, say, generating references to salient sets of objects, usingnegations and relations and possibly vague properties, is a non-trivial enterprise.
Togive just one example, consider what happens when we combine salience with (other)gradable properties (cf.
Sections 3.4 and 3.3).
Should ?the old man?
be interpreted as?the oldest of the men that are sufficiently salient?
or ?the most salient of the men thatare sufficiently old??
Expressions that combine gradable properties can easily becomeunclear, and determining when such combinations are nevertheless acceptable is aninteresting challenge.Some simplifying assumptions have only just begun to be lifted, through extensionsthat are only in their infancy, particularly in terms of their empirical validation.
Othersimplifying assumptions are still in place.
For instance, there is a dearth of work that ad-dresses functions of referring expressions other than mere identification.
Similarly, evenrecent proposals tend to assume that it is unproblematic to determine what informationis shared between speaker and hearer.
We return to these issues in Section 6.4.
REG FrameworksMost early REG algorithms represent knowledge in a very basic way, specificallydesigned for REG.
This may have been justified at the time, but years of researchin Knowledge Representation (KR) suggest that such a carefree attitude towards themodeling of knowledge may not be wise in the long run.
For example, when well-established KR frameworks are used, it may become possible to re-use existing algo-rithms for these frameworks, which have often been optimized for speed, and whosecomputational properties are well understood.
Depending on the choice of framework,many other advantages can ensue.
Because research that couples REG with KR is rela-tively new, and technical properties of the frameworks themselves can be easily foundelsewhere, we shall be comparatively brief.
For each framework, we focus on three190Krahmer and van Deemter Computational Generation of Referring Expressionsquestions: (a) How is domain information represented?
(b) How is the semantic contentof a referring expression represented?
and (c) How can distinguishing descriptions befound?4.1 REG Using Graph SearchOne of the first attempts to link REG with a more generic mathematical formalismwas the proposal by Krahmer, van Erk, and Verleg (2003), who used labeled directedgraphs for this purpose.
In this approach, objects are represented as the nodes (vertices)in a graph, and the properties of and relations between these objects are representedas edges connecting the nodes.
Figure 4 shows a graph representation of our exampledomain.
One-place relations (i.e., properties) such as man are modeled as loops (edgesbeginning and ending in the same node), whereas two-place relations such as left of aremodeled as edges between different nodes.Two kinds of graphs play a role: a scene graph representing the knowledge base,and referring graphs representing the content of referring expressions.
The problemof finding a distinguishing referring expression can now be defined as a comparisonbetween graphs.
More specifically, it is a graph search problem: Given a target object(i.e., a node in the scene graph), look for a distinguishing referring graph that is asubgraph of the scene graph and uniquely characterizes the target.
Intuitively, such adistinguishing graph can be ?placed over?
the target node with its associated edges,and not over any other node in the scene graph.
The informal notion of one graph being?placed over?
another corresponds with a subgraph isomorphism (Read and Corneil1977).
Figure 5 shows a number of referring graphs which can be placed over our targetobject d1.
The leftmost, which could be realized as ?the man,?
fails to distinguish ourtarget, because it can be ?placed over?
the scene graph in two different ways (overnodes 1 and 3).Krahmer, van Erk, and Verleg (2003) use cost functions to guide the search processand to give preference to some solutions over others.
They assume that these costfunctions are monotonic, so extending a graph can never make it cheaper.
Graphs arecompatible with many different search algorithms, but Krahmer et al employ a simplebranch and bound algorithm for finding the cheapest distinguishing graph for a giventarget object.
The algorithm starts from the graph containing only the node representingthe target object and recursively tries to extend this graph by adding adjacent edges:edges starting from the target, or in any of the other vertices added later on to theFigure 4Representation of our example scene in Figure 1 as a labeled directed graph.191Computational Linguistics Volume 38, Number 1Figure 5Some referring graphs for target d1.referring graph under construction.
For each referring graph, the algorithm checkswhich objects in the scene graph it may refer to, other than the target; these are thedistractors.
As soon as this set is empty, a distinguishing referring graph has been found.At this point, only alternatives that are cheaper than this best solution found so far needto be inspected.
In the end, the algorithm returns the cheapest distinguishing graphwhich refers to the target, if one exists; otherwise it returns the empty graph.One way to define the cost function would be to assign each edge a cost of onepoint.
Then the algorithm will output the smallest graph that distinguishes a target(if one exists), just as the Full Brevity algorithm would.
Alternatively, one could assigncosts in accordance with the list of preferred attributes in the IA, making more preferredproperties cheaper than less preferred ones.
A third possibility is to compute the costsof an edge e in terms of the probability P(e) that e occurs in a distinguishing descrip-tion (which can be estimated by counting occurrences in a corpus), making frequentproperties cheap and rare ones expensive:cost(e) = ?log2(P(e))Experiments with stochastic cost functions have shown that these enable the graph-based algorithm to capture a lot of the flexibility of human references (Krahmer et al2008; Viethen et al 2008).In the graph-based perspective, relations are treated in the same way as individualproperties, and there is no risk of running into infinite loops (?the cup to the left ofthe saucer to the right of the cup .
.
.
?).
Unlike Dale and Haddock (1991) and Kelleherand Kruijff (2006), no special measures are required, because a relational edge is eitherincluded in a referring graph or not: including it twice is not possible.
Van Deemter andKrahmer (2007) show that many of the proposals discussed in Section 3 can be recast interms of graphs.
They argue, however, that the graph-based approach is ill-suited forrepresenting disjunctive information.
Here, the fact that directed graphs are not a fullyfledged KR formalism makes itself felt.
Whenever an REG algorithm needs to reasonwith complex information, heavier machinery is required.4.2 REG Using Constraint SatisfactionConstraint satisfaction is a computational paradigm that allows efficient solving of NP?hard combinatoric problems such as scheduling (van Hentenryck 1989).
It is among theearliest frameworks proposed for REG (Dale and Haddock 1991), but in later years, thisapproach has seldom been emphasized?with a few notable exceptions, such as Stoneand Webber (1998)?until Gardent (2002) showed how constraint programming can be192Krahmer and van Deemter Computational Generation of Referring Expressionsused to generate expressions that refer to sets.
She proposed to represent a descriptionL for a target set S as a pair of set variables:LS = ?P+S , P?S ?,where one variable (P+S ) ranges over sets of properties that are true of the elements inS and the other (P?S ) over properties that are false of the elements in S. The challenge?taken care of by existing constraint solving programs?is to find suitable values (i.e.,sets of properties) for these variables.
To be ?suitable,?
values need to fulfil a number ofREG-style constraints:1.
All the properties in P+S are true of all elements in S.2.
All the properties in P?S are false of all elements in S.3.
For each distractor d there is a property in P+S which is false of d, or there isa property in P?S which is true of d.The third clause says that every distractor is ruled out by either a positive property (i.e.,a property in P+S ) or a negative property (i.e., a property in P?S ), or both.
An example of adistinguishing description for the singleton target set {d1} in our example scene wouldbe ?
{man}, {right}?, because d1 is the only object in the domain who is both a man andnot on the right.
The approach can be adapted to accommodate disjunctive propertiesto enable reference to sets (Gardent 2002).Constraint satisfaction is compatible with a variety of search strategies (Kumar1992).
Gardent opts for a ?propagate-and-distribute?
strategy, which means that so-lutions are searched for in increasing size, first looking for single properties, next forcombinations of two properties, and so forth.
This amounts to the Full Brevity searchstrategy, of course.
Accordingly, Gardent?s algorithm yields a minimal distinguishingdescription for a target, provided one exists.
Given the empirical questions associatedwith Full Brevity, it may well be worthwhile to explore alternative search strategies.The constraint approach allows an elegant separation between the specificationof the REG problem and its implementation.
Moreover, the handling of relationsis straightforwardly applicable to relations with arbitrary numbers of arguments.Gardent?s approach does not run into the aforementioned problems with infinite loops,because a set of properties (being a set) cannot contain duplicates.
Yet, like the labeledgraphs, the approach proposed by Gardent has significant limitations, which stem fromthe fact that it does not rest on a fully developed KR system.
General axioms cannot beexpressed, and hence cannot play a role in logical deduction.
We are forced to re-visitthe question of what is the best way for REG to represent and reason with knowledge.4.3 REG Using Modern Knowledge RepresentationTo find out what is missing, let us see what happens when domains scale up.
Considera furniture domain, and suppose every chair is in a room, that every room is in anapartment, and every apartment in a house.
Listing all relevant relations betweenindividual objects separately (?chair a is in room b,?
?room b is in apartment c,?
?chaira is in apartment c,?
?apartment c is in house d?)
is onerous, error prone, space-consuming, and messy.
Modern KR systems solve this problem by employing axioms(e.g., expressing transitivity of the ?in?
relation; if x is in y, and y is in z, then x is in z).193Computational Linguistics Volume 38, Number 1Logical inference allows the KR system to derive implicit information.
For example,from ?chair a is in room b,?
?room b is in apartment c,?
and ?apartment c is in housed,?
the transitivity of ?in?
allows us to infer that ?chair a is in house d?.
This combina-tion of basic facts and general axioms allows a succinct and insightful representationof facts.Modern KR comes in different flavors.
Recently, two different KR frameworks havebeen linked with REG, one based on Conceptual Graphs (Croitoru and van Deemter2007), the other on Description Logics (Gardent and Striegnitz 2007; Areces, Koller, andStriegnitz 2008).
The first have their origin in Sowa (1984) and were greatly enhancedby Baget and Mugnier (2002).
The latter grew out of work on KL-ONE (Brachman andSchmolze 1985) and became even more prominent in the wider world of computingwhen they came to be linked with the ontology language OWL, which underpinscurrent work on the semantic Web (Baader et al 2003).
Both formalisms represent at-tempts to carve out computationally tractable fragments of First-Order Predicate Logicfor defining and reasoning about concepts, and are closely related (Kerdiles 2001).
Forreasons of space, we focus on Description Logic.The basic idea is that a referring expression can be modeled as a formula of De-scription Logic, and that REG can be viewed as the problem of finding a particular kindof formula, namely, one that denotes (i.e., refers to) the target set of individuals.
Let usrevisit our example domain, casting it as a logical model M, as follows: M = ?D, ?.?
?,where D (the domain) is a finite set {d1, d2, d3} and ?.?
is an interpretation functionwhich gives the denotation of the relevant predicates (thus: ?man?
= {d1, d3}, ?left-of?
={?d1, d2?, ?d2, d3?}
etc.).
Now the REG task can be formalized as: Given a model Mand a target set S ?
D, look for a Description Logic formula ?
such that ???
= S.The following three expressions are the Description Logic counterparts of the referringgraphs in Figure 5:(a) man(b) manwears suit(c) man?
left-of.
(womanwears t-shirt)The first, (a), would not be distinguishing for d1 (because its denotation includes d3),but (b) and (c) would.
Note thatrepresents the conjunction of properties, and ?represents existential restriction.
Negations can be added straightforwardly, as in man?
wears suit, which denotes d3.Areces, Koller, and Striegnitz (2008) search for referring expressions in a somewhatnon-standard way.
In particular, their algorithm does not start with one particular targetreferent: It simply attempts to find the different sets that can be referred to.
They startfrom the observation that REG can be reduced to computing the similarity set of eachdomain object.
The similarity set of an individual x is the set of those individuals thathave all the properties that x has.
Areces et al present an algorithm, based on a proposalby Hopcroft (1971), which computes the similarity sets, along with a Description Logicformula associated with each set.
The algorithm starts by partitioning the domain usingatomic concepts such as man and woman, which splits the domain in two subsets ({d1, d3}and {d2} respectively).
At the next stage, finer partitions are made by making use of con-cepts of the form ?R.AtomicConcept (e.g., men left of a woman), and so on, always usingconcepts established during one phase to construct more complex concepts during thenext.
All objects are considered in parallel, so there is no risk of infinite loops.
Control194Krahmer and van Deemter Computational Generation of Referring Expressionsover the output formulae is achieved by specifying an incremental preference order overpossible expressions, but alternative control strategies could have been chosen.4.4 DiscussionEven though the role of KR frameworks for REG has received a fair bit of attention inrecent years, one can argue that this constitutes just the first steps of a longer journey.The question of which KR framework suits REG best, for example, is still open; whichframework has the best coverage, which allows all useful descriptions to be expressed?Moreover, can referring expressions be found quickly in a given framework, and is itfeasible to convert these representations into adequate linguistic realizations?
Giventhe wealth of possibilities offered by these frameworks, it is remarkable that much oftheir potential is often left unused.
In Areces, Koller, and Striegnitz?s (2008) proposal,for example, generic axioms do not play a role, nor does logical inference.
Ren, vanDeemter, and Pan (2010) sketch how REG can benefit if the full power of KR is broughtto bear, using Description Logic as an example.
They show how generic axioms canbe exploited, as in the example of the furniture domain, where a simple transitivityaxiom allows a more succinct and insightful representation of knowledge.
Similarly,incomplete information can be used, as when we know that someone is either Dutch orBelgian, without knowing which of the two.
Finally, by making use of more expressivefragments of Description Logic, it becomes possible to identify objects that previousREG algorithms were unable to identify, as when we say ?the man who owns threedogs,?
or ?the man who only kisses women,?
referring expressions that were typicallynot considered by previous REG algorithms.Extensions of this kind raise new empirical questions, as well.
It is an open question,for instance, when human speakers would be inclined to use such complex descriptions.These problems existed even in the days of the classic REG algorithms (when it wasalready possible to generate lengthy descriptions) but they have become more acutenow that it is possible to generate structurally complex expressions as well.
There isa clear need for empirical work here, which might teach us how the power of theseformalisms ought to be constrained.5.
Evaluating REGPre-2000 REG research gave little or no attention to the empirical evaluation of algo-rithms.
More recently, however, REG evaluation studies have started to be carried outmore and more often.
It appears that most of these were predicated on the assumption(debated in Section 7) that REG algorithms should try to generate expressions that areoptimally similar to those produced by human speakers or writers, even though?importantly?this assumption was seldom made explicit.
The dominant method atthe moment is, accordingly, to measure the similarity between generated expressionsand those in a suitable corpus of referring expressions.
REG came late to corpus-basedevaluation (compared to other parts of computational linguistics) because suitable datasets are hard to come by.
In this section, we discuss what criteria a data set should meetto make it suitable for REG evaluation, and we survey which collections are currentlyavailable.
In addition, we discuss how one is to determine the performance of an REGalgorithm on a given data set.
We shall see that although much work has been done inrecent years, there are still significant open questions, particularly regarding the relationbetween automatic metrics and human judgments.195Computational Linguistics Volume 38, Number 15.1 Corpora for REG EvaluationText corpora are full of referring expressions.
For evaluating the realization of referringexpressions, such corpora are very suitable, and various researchers have used them,for instance, to evaluate algorithms for modifier orderings (Shaw and Hatzivassiloglou1999; Malouf 2000; Mitchell 2009).
Text corpora are also important for the study ofanaphoric links between referring expressions.
The texts that make up the GNOMEcorpus (Poesio et al 2004), for instance, contain descriptions of museum objects andmedical patient information leaflets, with each of the two subcorpora containing some6,000 NPs.
Much information is marked up, including anaphoric links.
Yet, text corporaof this kind are of limited value for evaluating the content selection part of REGalgorithms.
For that, one needs a corpus that is fully ?semantically transparent?
(vanDeemter, van der Sluis, and Gatt 2006): A corpus that contains the actual properties ofall domain objects as well as the properties that were selected for inclusion in a givenreference to the target.
Text corpora such as GNOME do not meet this requirement, andit is often difficult or impossible to add all necessary information, because of the sizeand complexity of the relevant domains.
For this reason, data sets for content selectionevaluation are typically collected via experiments with human participants in simpleand controlled settings.
Broadly speaking, two kinds of experimental corpora can bedistinguished: corpora specifically collected with reference in mind, and corpora col-lected wholly or partly for other purposes but which have nevertheless been analyzedfor the referring expressions in them.
We will briefly sketch some corpora of the latterkind, after which we shall discuss the former in more detail.General-Purpose Corpora.
One way to elicit ?natural?
references is to let participants per-form a task for which they need to refer to objects.
An example is the corpus of so-calledpear stories of Chafe (1980), in which people were asked to describe a movie about a manharvesting pears, in a fluent narrative.
The resulting narratives featured such sequencesas ?And he fills his thing with pears, and comes down and there?s a basket he puts themin.
.
.
.
And then a boy comes by, on a bicycle, the man is in the tree, and the boy gets offhis bicycle .
.
.
,?
where a limited set of individuals come up several times.
The referringexpressions in a subset of these stories were analyzed by Passonneau (1996), who askedhow the form of the re-descriptions (such as ?he,?
?them,?
and ?the man?)
in thesenarratives might best be predicted, comparing ?informational?
considerations (whichform the core of most algorithms in the tradition started by Dale and Reiter, as we haveseen) with considerations based on Centering Theory (Grosz, Joshi, and Weinstein 1995).Passonneau, who tested her rules on 319 noun phrases, found support for an integratedmodel, where centering constraints take precedence over informational considerations.The well-known Map Task corpus (Anderson et al 1991) is another example of acorpus in which reference plays an important role.
It consists of dialogues between twoparticipants; both have maps with landmarks indicated, but only one (the instructiongiver) has a route on the map and he or she instructs the other (the follower) about thisparticular route.
Referring expressions are routinely produced in this task to refer tothe landmarks on the maps (?the cliff?).
Participants use these not only for identifica-tion purposes but also, for instance, to verify whether they understood their dialoguepartner correctly.
In the original Map Task corpus, the landmarks were labeled withproper names (?cliff?
), making them less suitable for studying content determination.To facilitate the study of reference, the iMap corpus was created (Guhe and Bard 2008),a modified version of the Map Task corpus where landmarks are not labelled, andsystematically differ along a number of dimensions, including type (owl, penguin, etc.
),196Krahmer and van Deemter Computational Generation of Referring Expressionsnumber (singular, plural) and color; a target may thus be referred to as ?the two purpleowls.?
Because participants may refer to targets more than once, it becomes possible tostudy initial and subsequent reference (Viethen et al 2010).Yet another example is the Coconut corpus (Di Eugenio et al 2000), a set of task-oriented dialogues in which participants negotiate which furniture items they want tobuy on a fixed, shared budget.
Referring expressions in this corpus (?a yellow rug for150 dollars?)
do not only contain information to identify a particular piece of furniture,but also include properties which directly refer to the task at hand (e.g., how muchmoney is still available for a particular furniture item and what the state of agreementbetween the negotiators is).An attractive aspect of these corpora is that they represent fairly realisticcommunication, related to a more or less natural task.
However, in these corpora, theidentification of objects tends to be mixed with other communicative tasks (verification,negotiating).
This does not mean that the corpora in question are unsuitable for thestudy of reference, of course.
More specifically, they have been used for evaluating REGalgorithms, to compare the performance of traditional algorithms with special-purposealgorithms that take dialogue context into account (Jordan and Walker 2005; Gupta andStent 2005; Passonneau 1996).
For example, when the speaker attempts to persuadethe hearer to buy an item, Jordan?s Intentional Influences algorithm selects thoseproperties of the item that make it a better solution than a previously discussed item.In yet other situations?for example, when a summarization is offered?all mutuallyknown properties of the item are selected.
Jordan?s algorithm outperforms traditionalalgorithms, which is perhaps not surprising given that the latter were not designed todeal with references in interactive settings (Jordan 2000).Dedicated Corpora.
In recent years, a number of new corpora have been collected,specifically focusing on the types of referring expressions that we are focusing on inthis survey.
A number of such corpora are summarized in Table 2.
In some ways, thesecorpora are remarkably similar.
Reflecting the prevalent aims of research on REG, forexample, they focus on descriptions that aim to identify their referent ?in one shot,?disregarding the linguistic context of the expression (i.e., in the ?null context,?
as it issometimes called [Viethen and Dale 2007]).
In all these corpora, participants were askedto refer to targets in a visual scene also containing the distractors.
This set-up meansthat the properties of target objects and their distractors are known, which makes itcomparatively easy to make these corpora semantically transparent by annotatingthe references that were produced.
In addition, most corpora are ?pragmaticallyTable 2Overview of dedicated Referring Expression corpora (alphabetical), with for each corpus arepresentative reference, an indication of the domain, and the number of participants andcollected distinguishing descriptions.Corpus Reference Domain Participants DescriptionsNameBishop Gorniak & Roy (2004) Colored cones in 3D scene 9 447Drawer Viethen & Dale (2006) Drawers in filing cabinet 20 140GRE3D3 Viethen & Dale (2008) Spheres, Cubes in 3D scene 63 630iMap Guhe & Bard (2008) Various objects on a map 64 9,567TUNA van Deemter et al (in press) Furniture, People 60 2,280197Computational Linguistics Volume 38, Number 1transparent?
as well, meaning that the communicative goals of the participants wereknown (typically identification).An early example is the Bishop corpus (Gorniak and Roy 2004).
For this data set,participants were asked to describe objects in various computer generated scenes.
Eachof these scenes contained up to 30 objects (?cones?)
randomly positioned on a virtualsurface.
All objects had the same shape and size, and hence targets could only bedistinguished using their color (either green or purple) and their location on the surface(?the green cone at the left bottom?).
Each participant was asked to identify targets inone shot, and for the benefit of an addressee who was physically present but did notinteract with the participant.The Drawer corpus, collected by Viethen and Dale (2006), has a similar objective,but here targets are real, being one of 16 colored drawers in a filing cabinet.
On differentoccasions, participants were given a random number between 1 and 16 and asked torefer to the corresponding drawer for an onlooker.
Naturally, they were asked not touse the number; instead they could refer to the target drawers using color, row, andcolumn, or some combination of these.
In this corpus, referring expressions (?the pinkdrawer in the first row, third column?)
once again solely serve an identification purpose.Viethen and Dale (2008) also collected another corpus (GRE3D3) specifically lookingat when participants use spatial relations.
For this data collection, participants were pre-sented with 3D scenes (made with Google SketchUp) containing three simple geometricobjects (spheres and cubes of different colors and sizes, and in different configurations),of which one was the target.
Viethen and Dale (2008) found that spatial relations werefrequently used (?the ball in front of the cube?
), even though they were never requiredfor identification.
Whether this generalizes to other visual scenes (in which spatialrelations are less immediately ?available?)
is an interesting question for future research.The TUNA corpus (Gatt, van der Sluis, and van Deemter 2007; van Deemter et alin press) was collected via a Web-based experiment, in which singular and pluraldescriptions were gathered by showing participants one or two targets, where the pluraltargets could either be similar (same type) or dissimilar (different type).
Targets werealways displayed with six distractors, and the resulting domain objects were randomlypositioned in a 3 x 5 grid, with targets surrounded by a red border.
Example trials areshown in Figure 6.The corpus contains two different domains: a furniture domain and a people do-main.
The first domain is based on pictures of furniture and household items, takenfrom the Object Databank (see http://www.tarrlab.org/).
These were manipulated sothat besides type (chair, desk, fan) also color, orientation, and size could systematicallybe varied.
The number of possible attributes and values in the people domain is muchFigure 6Example trials from the TUNA corpus, a singular trial for the furniture domain (?the small bluefan,?
left) and a plural trial for the people domain (?the men with glasses,?
right).198Krahmer and van Deemter Computational Generation of Referring Expressionslarger (and more difficult to pin down); this domain consists of a set of black and whitephotographs of people (all famous mathematicians) used in an earlier study of vander Sluis and Krahmer (2007).
Properties of these photographs include gender, headorientation, age, beard, hair, glasses, suit, shirt, and tie.
It is interesting to note that theTUNA corpus was designed to have one shortest description for each target, whereasin other data sets, such as Viethen and Dale?s (2006) drawer corpus, a single shortestdescription does not always exist.
The TUNA corpus has formed the basis of threeshared REG challenges, to which we turn now.5.2 Evaluation MetricsHow should we compare human descriptions with those produced by a REG algorithm?When looking for measures that compute the content overlap, one source of inspirationmay come from biology and information retrieval (van Rijsbergen 1979).
One measureused in these fields is the Dice (1945) coefficient, which was originally proposed toquantify ecologic association between species, and was first applied to REG by Gatt,van der Sluis, and van Deemter (2007).
The Dice coefficient?which is not dissimilarto the ?match?
function used by Jordan (2000)?is computed by scaling the number ofelements that two sets have in common, by the size of the two sets combined:Dice(A, B) =2 ?
|A ?
B||A|+ |B| (1)The Dice measure ranges from 0 (no agreement; i.e., no elements shared between A andB) to 1 (complete agreement; A and B share all elements).
For REG, A and B can beunderstood as attributes (e.g., type) or as attribute?value pairs (properties; ?type, man?
).The former option tends to be used in earlier work, but has the somewhat counter-intuitive consequence that two descriptions which express different values of the sameattribute (?the man?
and ?the woman,?
say, or ?the dog?
and ?the chihuahua,?
in theearlier discussed cats-and-dogs example) have a Dice score of 1.
Hence, in the followingdiscussion we shall measure overlap in terms of properties.An alternative to Dice that is sometimes used is the MASI (Measuring Agreementon Set-valued Items) metric of Passonneau (2006):MASI(A, B) = ??
|A ?
B||A ?
B| (2)This is basically an extension of the well-known Jaccard (1901) metric with a weightingfunction ?
which biases the score in favor of similarity where one set is a sub- or asuperset of the other:?
=????????
?1, if A = B23 , if A ?
B or B ?
A0, if A ?
B = ?13 , otherwise(3)Dice and MASI are straightforward measures for overlap, but they do have theirdisadvantages.
For example, they assume that all properties are independent and thatall are equally different from each other.
Suppose a human participant referred to d1in our example domain as ?the man in the suit next to a woman,?
and consider the199Computational Linguistics Volume 38, Number 1following two references produced by a REG algorithm: ?the man in the suit?
and ?theman next to a woman.?
Both omit one property from the human reference and thus havethe same Dice and MASI scores.
But only the former reference is distinguishing; thelatter is not.
This problem could be solved, for example, by adopting a binary weightedversion of the metrics which multiply the resulting score with 1 for a distinguishingdescription and with 0 for a non-distinguishing one.A more general issue with these overlap metrics can be illustrated with an examplefrom Richard Power (personal communication).
Consider the two (roughly equivalent)expressions ?the palomino?
and ?the horse with the gold coat and white mane andtail.?
Straightforward counting of attribute?value pairs would result in an overlap scoreof zero, which would be misleading, because the two descriptions express essentiallythe same content, with the latter description combining, in one property, all propertiesexpressed in the former.
This problem clearly calls for a more principled approach torepresenting and counting properties.During evaluations, Dice or MASI scores are typically averaged over references fordifferent trials and produced by different human participants, making them fairly roughmeasures.
It could be that an algorithm?s predictions match the descriptions of someparticipants very well, but those of other participants not at all.
To partially compensatefor this, sometimes also the proportion of times an algorithm achieves a perfect matchwith a human reference is reported.
This measure is known, somewhat confusingly, asRecall (Viethen and Dale 2006), the Perfect Recall Percentage (PRP) (Gatt, van der Sluis,and van Deemter 2007), and Accuracy (Gatt, Belz, and Kow 2008).The measures discussed so far do not take the actual linguistic realization of thereferring expressions into account.
For these, string distance metrics are obvious candi-dates, because these have proven their worth in various other areas of computationallinguistics.
One well-known string distance metric, which has also been proposed forREG evaluation, is the Levenshtein (1966) distance: The minimal number of insertions,deletions, and substitutions needed to convert one string into another, possibly nor-malized with respect to length (Bangalore, Rambow, and Whittaker 2000).
The BLEU(Papineni et al 2002) and NIST (Doddington 2002) metrics, which have their originin machine translation evaluation, have also been proposed for REG evaluation.
BLEUmeasures n-gram overlap between strings; for machine translation n is often set to 4, butgiven that referring expressions tend to be short, n = 3 seems a better option for REGevaluation (Gatt, Belz, and Kow 2009).
NIST is a BLEU variant giving more importanceto less frequent (and hence more informative) n-grams.
Finally, Belz and Gatt (2008) alsouse the rouge-2 and rouge-su4 measures (Lin and Hovy 2003), originally proposed forevaluating automatically generated summaries.An obvious benefit of these string metrics is that they are easy to compute automat-ically, whereas property-based evaluation measures such as Dice require an extensivemanual annotation of selected properties.
However, the added value of string-basedmetrics for REG is relatively unclear.
It is not obvious, for instance, that a smallerLevenshtein distance is always to be preferred over a longer one; the expressions ?theman wearing a t-shirt?
and ?the woman wearing a t-shirt?
are at a mere Levenshteindistance of 2 from each other, but only the former would be a good description fortarget d3.
On the other hand, ?the male person on the right?
is at a Levenshtein distanceof 15 from ?the man wearing a t-shirt,?
and both are perfect descriptions of d3.Alternatively, referring expressions could also be evaluated by human judges, al-though this obviously is more time consuming than an automatic evaluation.
Gatt, Belz,and Kow (2009) collected judgments of Adequacy (?How clear is this description?
Tryto imagine someone who could see the same grid with the same pictures, but didn?t200Krahmer and van Deemter Computational Generation of Referring Expressionsknow which of the pictures was the target.
How easily would they be able to findit, based on the phrase given??)
and Fluency (?How fluent is this description?.
.
.
Is itgood, clear English??).
One may also be interested in the extent to which references areuseful for addressees.
This can be evaluated in a number of different ways.
Belz andGatt (2008), for example, first showed participants a generated description for a trial.After participants read this description, a scene appeared and participants were askedto click on the intended target.
This allowed them to compute three extrinsic evaluationmetrics: the reading time, the identification time, and the error rate, which they definedas the number of incorrectly identified targets.5.3 DiscussionThree lessons can be learned from the recent work on evaluation.
First, the emergence oftransparent corpora has greatly facilitated the empirical evaluation of REG algorithms,particularly for content selection.
Focusing on reference in simple situations, a numberof studies based on transparent corpora found that the IA outperformed the Full Brevityand Greedy Heuristic algorithms (Viethen and Dale 2006; van Deemter et al in press).There is an important catch, however: As demonstrated in van Deemter et al (in press),the performance of the IA crucially depends on the chosen preference order.
The bestpreference order outperforms the other two algorithms, but many other preferenceorders perform far worse.
This is a problem, because no procedure for finding a goodpreference order is known.
(For n attributes, there are n!
preference orders to consider,so trial and error is not an option except in extremely simple cases.)
Perhaps mostcontroversially, the authors argue that the evidence is starting to stack up in favor ofthe thesis that the Greedy algorithm?or variants of the Greedy algorithm that chooseproperties on the basis of more than just their discriminatory power?might be superiorto algorithms that use the same preference order all the time.Second, evaluations suggest that human-produced descriptions differ from auto-matically generated ones in a number of ways.
Human references often include re-dundant information, making the descriptions overspecified in ways that were notaccounted for by standard REG algorithms.
An additional problem is that there appearsto be considerable individual variation, both within and between speakers, which issomething that existing REG algorithms do not model (Dale and Viethen 2010).Third, it is still somewhat unclear what the best REG evaluation metrics are.
Thethree REG Challenges based on the TUNA set-up offer a wealth of information inthis respect (Gatt and Belz 2010).
In each of these challenges, a number of researchteams submitted one or more REG generation systems, allowing detailed statisticalanalyses over the various metrics.
It was found that Dice, MASI, and PRP are veryhighly correlated (all r > .95).
Interestingly, these metrics correlate negatively withthe proportion of references that are minimally specified (Gatt, Belz, and Kow 2008);in other words, systems that produce more overspecified references tend to do betterin terms of Dice and other overlap metrics.
Concerning the surface realization metrics, itwas found that?when comparing different realizations of a given set of attributes?theNIST and BLEU string metrics correlate strongly with each other (r = .9), as one mightexpect, but neither correlates well with Levenshtein distance (Gatt, Belz, and Kow 2008).As for the extrinsic measures, Gatt, Belz, and Kow (2008) only report a significantcorrelation between reading time and identification time, which suggests that slowreaders are also slow identifiers, or that referring expressions that are hard to readalso make it harder to identify the intended referent.
Gatt, Belz, and Kow (2009) letparticipants listen to expressions that were produced either automatically or by human201Computational Linguistics Volume 38, Number 1speakers, and found a strong correlation between identification accuracy and adequacy,suggesting that more adequate references also have more correct identifications.
Also,they found a negative correlation between fluency and identification time, implyingthat more fluent descriptions reduce the identification time.It is notable that essentially no correlations were found between these extrinsic taskperformance measures and the automatic metrics for human-likeness (Belz and Gatt2008; Gatt and Belz 2010).
Different explanations are possible for this lack of correlation.Gatt and Belz (2010) note that the nature of the TUNA data could be partly responsible.The TUNA data collection was carried out in a Web-based and relatively unrestrictedmanner, and idiosyncratic references do occur in it (?a red chair, if you sit on it, yourfeet would show the south east?).
It is possible that for another, more controlled corpus,a correlation between the two kinds of metrics would show up.
It could also be thatpeople are not always very good at designing their utterances in a way that is optimalfor hearers (Horton and Keysar 1996; see also Section 6), so producing descriptions thatresemble human-produced ones is not the same as producing descriptions that are ofoptimal use for hearers.
This suggests that the two sets of metrics measure differentthings, and that they correspond with two different aims that the designer of a REGalgorithm might have: One set of metrics could be used if the aim is to mimic speakers,another if the aim is to produce optimal benefits for hearers.So far, experimental evaluation has mostly been limited to the simplest of situations,focusing on algorithms that produce singular descriptions, expressing conjunctions ofbasic properties in small and artificial domains.
Most of the extensions discussed inSection 3 have not been evaluated systematically.
Moreover, tasks such as the one onwhich the TUNA corpus is based can be argued not to be ?ecologically valid?
: Humanparticipants produce typewritten expressions for an imaginary audience on the basis ofabstract visual scenes.
The effects of these limitations on the descriptions produced arepartly unknown, although some reassuring results have recently been obtained.
It hasbeen shown, for example, that, speakers who address an imaginary audience refer insimilar ways to those who address an audience that is physically present (van der Wege2009).
Similarly, Koolen et al (2009) show that speaking rather than typing has no effecton the kind and number of attributes in the referring expressions that are produced,although speakers tend to use more words than typists to convey the same amountof information.
It would nevertheless be valuable to evaluate REG algorithms in thecontext of a specific application, so the added value of different REG algorithms for areal-life application can be gauged (Gatt, Belz, and Kow 2009).Two recent evaluation challenges seem promising for these reasons.
GREC (Belzet al 2010) focuses on the task of deciding which form a referring expression should takein a textual context, which is important for generating coherent texts such as summaries(see also Section 6).
GIVE (Koller et al 2010) focuses on generating directions in a virtual3D environment, where reference is only one task among a number of others.
This newchallenge has so far not included a separate test of REG algorithms employed in thesystems submitted, but it seems likely that GIVE will cause REG research to focus onharder tasks, including reference in discourse context, reference to sets, and referencesthat are spread out over several utterances (Denis 2010).6.
Open IssuesIn the previous sections we have discussed three main dimensions in which REG re-search has moved beyond the state-of-the-art of 2000.
Along the way, various loose ends202Krahmer and van Deemter Computational Generation of Referring Expressionshave been identified.
For example, not all simplifying assumptions of early REG workhave been adequately addressed, and the enterprise of combining extensions is still inits infancy (Section 3).
It is unclear whether complex referring expressions can alwaysbe found quickly, particularly where the generation process relies on theorem-proving,and it is similarly unclear whether it is always feasible to turn the representations intofluent natural language expressions (Section 4).
Finally, empirical data has only beencollected for the simplest referring expressions, and it is still unclear what the properevaluation metrics are (Section 5).
In this section, we suggest six further questions forfuture research.6.1 How Do We Match an REG Algorithm to a Particular Domain and Application?Evaluation of classic REG algorithms has shown that with some preference orders, theIA outperformed the Full Brevity and Greedy Heuristic algorithms, but with othersit performed much worse than these (van Deemter et al in press).
The point is thatthe IA, as it stands, is under-determined, because it does not contain a procedure forfinding a preference order.
Sometimes psycholinguistic experiments come to our aid, forinstance Pechmann?s (1989) study showing that speakers have a preference for absoluteproperties (color) over relative ones (size).
Unfortunately, for most other attributes, nosuch experiments have been done.It seems reasonable to assume that frequency tells us something about preference:A property that is used frequently is also more likely to be high on the list of preferredproperties (Gatt and Belz 2010; van Deemter et al in press).
But suitable corpora to de-termine preferences are rare, as we have seen, and their construction is time-consuming.This raises the question of how much data would be needed to make reasonable guessesabout preferred properties; this could be studied, for instance, by drawing learningcurves where increasingly large proportions of a transparent corpus are used to estimatea preference order and the corresponding performance is measured.The IA is more drastically under-determined than most other algorithms: The FullBrevity and the Greedy Heuristic algorithm are specified completely up to situationswhere there is a tie?a tie between two equally lengthy descriptions in the first case, anda tie between two properties that have the same discriminatory power in the second.
Toresolve such ties frequency data would clearly be helpful.
Similar questions apply toother generation algorithms.
For instance, the graph-based algorithm as described byKrahmer et al (2008) assigns one of three different costs to properties (they can be free,cheap, or somewhat expensive), and frequency data is used to determine which costsshould be assigned to which properties (properties that are almost always used in aparticular domain can be for free, etc.).
A recent experiment (Theune et al 2011) suggeststhat training the graph-based algorithm on a corpus with a few dozen items mayalready lead to a good performance.
In general, knowing how much data is requiredfor a new domain to reach a good level of performance is an important open problemfor many REG algorithms.6.2 How Do We Move beyond the ?Paradigms?
of Reference?A substantial amount of REG research focuses on what we referred to in the Intro-duction as the ?paradigms?
of reference: ?first-mention?
distinguishing descriptionsconsisting of a noun phrase starting with ?the?
that serve to identify some target, andthat do so without any further context.
But how frequent are these ?paradigmatic?
kinds203Computational Linguistics Volume 38, Number 1of referring expressions?
Poesio and Vieira (1998), in one of the few systematic attemptsto quantify the frequency of different uses of definite descriptions in segments of theWall Street Journal corpus, reported that ?first mention definite descriptions?
are indeedthe most frequent in these texts.
These descriptions often do not refer to visual objectsin terms of perceptual properties but to more abstract entities.
One might think that itmatters little whether a description refers to a perceivable object or not; a descriptionlike ?the third quarter?
rules out three quarters much like ?the younger-looking man?in our example scene rules out the older-looking distractor.
It appears, however, that therepresentation of the relevant facts in such cases tends to be a more complicated affair,and it is here particularly that more advanced knowledge representation formalismsof the kind discussed in Section 4 come into their own (a point to which we returnsubsequently).Even though first-mention definite descriptions are the most frequent in Poesioand Vieira?s (1998) sample, other uses abound, including anaphoric descriptions andbridging descriptions, whose generation is studied by Gardent and Striegnitz (2007).Pronouns come to mind as well.
The content determination problem for these otherkinds of referring expressions may not be overly complex, but deciding where ina text or dialogue each kind of referring expression should be used is hard.
Still,this is an important issue for, for example, automatic summarization.
One of theproblems of extractive summaries is that co-reference chains may be broken, resultingin less coherent texts.
Regeneration of referring expressions is a potentially attractiveway of regaining some of the coherence of the source document (Nenkova andMcKeown 2003).
This issue is even more pressing in multi-document summarization,where different source documents may refer to a given person in different ways; seeSiddharthan, Nenkova, and McKeown (2011) for a machine-learning approach to thisproblem.REG research often works from the assumption that referents cannot be identifiedthrough proper names.
(If proper names were allowed, why bother inventing a de-scription?)
But in real text, proper names are highly frequent.
This does not only raisethe question when it?s best to use a proper name, or which version of a proper nameshould be used (is it ?Prince Andrei Nikolayevich Bolkonsky,?
?Andrei Bolkonsky,?
orjust ?Andrei??
), but also how proper names can occur as part of a larger description,as when we refer to a person using the description ?the author of Primary Colors,?
forexample, where the proper name Primary Colors refers to a well-known book (whoseauthor was long unknown).
Surely, it is time for REG to turn proper names intofirst-class citizens.Generation of referring expressions in a text is studied in the GREC (GeneratingReferring Expressions in Context) challenges (Belz et al 2008).
A corpus of Wikipediatexts (for persons, cities, countries, rivers, and mountains) was constructed, and in eachtext all elements of the coreference chain for the main subject were removed.
For eachof the resulting reference gaps, a list of alternative referring expressions, referring to thesubject, was given (including the ?correct?
reference, i.e., the one that was removed fromthe text).
One well-performing entry (Hendrickx et al 2008) predicted the correct typeof referring expression in 76% of the cases, using a memory-based learner.
These resultssuggest that it is feasible to learn which type of referring expression is best in whichinstance.
If so, REG in context could be conceived of as a two-stage procedure wherefirst the form of a reference is predicted, after which the content and realization aredetermined.
REG algorithms as described in the present survey would naturally fit intothis second phase.
It would be interesting to see if such a method could be developedfor a data collection such as that of Poesio and Vieira (1998).204Krahmer and van Deemter Computational Generation of Referring Expressions6.3 How Do We Handle Functions of Referring Expressions Otherthan Identification?Target identification is an important function of referring expressions, but it is not theonly one.
Consider the following example, which Dale and Reiter (1995) discuss toillustrate the limits of their approach:(5) Sit by the newly painted table.Here, ?the newly painted table?
allows the addressee to infer that it would be betternot to touch the table.
To account for examples such as this one, an REG algorithmshould be able to take into account different speaker goals (to identify, to warn, etc.
)and allow these goals to drive the generation process.
These issues were already studiedin the plan-based approach to REG of Appelt and Kronfeld (Section 2.1), and morerecent work addresses similar problems using new methods.
Heeman and Hirst (1995),for example, present a plan-based, computational approach to REG where referringis modeled as goal-directed behavior.
This approach accounts for the combination ofdifferent speaker goals, which may be realized in a single referring expression through?information overloading?
(Pollack 1991).
Context is crucial here: A variant such as?What do you think of the newly painted table??
does not trigger the intended ?don?ttouch?
inference.
In another extension of the plan-based approach to reference, Stoneand Webber (1998) use overloading to generate references that only become distin-guishing when the rest of the sentence is taken into account.
For example, we can say?Take the rabbit from the hat?
if there are two rabbits, as long as only one of them is ina hat.Plan-based approaches to natural language processing are not as popular as theywere in the 1980s and early 1990s, in part because they are difficult to develop and main-tain.
However, Jordan and Walker (2005) show that a natural language generator canbe trained automatically on features inspired by a plan-based model for REG (Jordan2002).
Jordan?s Intentional Influences model incorporates multiple communicative andtask-related problem solving goals, besides the traditional identification goal.
Jordansupports her model with data from the Coconut corpus (discussed previously) andshows that traditional algorithms such as the IA fail to capture which propertiesspeakers typically select for their references, not only because these algorithms focuson identification, but also because they ignore the interactive setting (see subsequentdiscussion).In short, it seems possible to incorporate different goals into a REG algorithm,even without invoking complex planning machinery.
This calls for a close coupling ofREG with the generation of the carrier utterance containing the generated expression,however.
What impact this has on the architecture of an NLG system, what the relevantgoals are, how combinations of different goals influence content selection and linguisticrealization, and how such expressions are best evaluated is still mostly unexplored.Answers might come from studying REG in the context of more complex applicationswhere the generator may need to refer to objects for different reasons.6.4 How Do We Generate Suitable Referring Expressions in Interactive Settings?Ultimately, referring expressions are generated for some addressee, yet most of thealgorithms we have discussed are essentially ?addressee-blind?
(Clark and Bangerter205Computational Linguistics Volume 38, Number 12004).
To be fair, some researchers have paid lip service to the importance of takingthe addressee into account (cf.
Dale and Reiter?s UserKnows function), but it is stilllargely an open question to what extent the classical approaches to REG can be used ininteractions.
In fact, there are good reasons to assume that most current REG algorithmscannot directly be applied in an interactive setting.
Psycholinguistic studies on referenceproduction, for example, show that human speakers do take the addressee into accountwhen referring?an instance of ?audience design?
(Clark and Murphy 1983).
Somepsycholinguists have argued that referring is an interactive and collaborative process,with speaker and addressee forming a ?conceptual pact?
on how to refer to some object(Heeman and Hirst 1995; Brennan and Clark 1996; Metzing and Brennan 2003).
Thisalso implies that referring is not necessarily a ?one shot?
affair; rather a speaker mayquickly produce a first approximation of a reference to some target, which may berefined following feedback from the addressee.Others have argued that conversation partners automatically ?align?
with eachother during interaction (Pickering and Garrod 2004).
For instance, Branigan et al (2010)report on a study showing that if a computer uses the word ?seat?
instead of the morecommon ?bench?
in a referring expression, the user is subsequently more likely to use?seat?
instead of ?bench?
as well.
This kind of lexical alignment takes place at the levelof linguistic realization, and there is at least one NLG realizer that can mimic this process(Buschmeier, Bergmann, and Kopp 2009).
Goudbeek and Krahmer (2010) found thatspeakers in an interactive setting also align at the level of content selection; they presentexperimental data showing that human speakers may opt for a ?dispreferred?
attribute(even when a preferred attribute would be distinguishing) when these were salient ina preceding interaction.
The reader may want to consult Arnold (2008) for an overviewof studies on reference choice in context, Clark and Bangerter (2004) for a discussion ofstudies on collaborative references, or Krahmer (2010) for a confrontation of some recentpsycholinguistic findings with REG algorithms.Psycholinguistic studies suggest that traditional REG algorithms which rely onsome predefined ranking of attributes cannot straightforwardly be applied in an inter-active setting.
This is confirmed by the findings of Jordan and Walker (2005) and Guptaand Stent (2005), who studied references in dialogue corpora discussed in Section 5.They found that in these data sets, traditional algorithms are outperformed by simplestrategies that pay attention to the referring expressions produced earlier in the dia-logue.
A more recent machine learning experiment on a larger scale, using data from theiMap corpus, confirmed the importance of features related to the process of alignment(Viethen, Dale, and Guhe 2011).
Other researchers have started exploring the generationof referring expressions in interactive settings as well.
Stoia et al (2006), for example,presented a system that generates references in situated dialogues, taking into accountboth dialogue history and spatial visual context, defined in terms of which distractorsare in the current field of vision of the speakers and how distant they are from thetarget.
Janarthanam and Lemon (2009) present a method which automatically adapts tothe expertise level of the intended addressee (using ?the router?
when communicatingwith an expert user, and ?the black block with the lights?
while interacting with anovice).
This line of research fits in well with another, more general, strand of researchconcentrating on choice optimization during planning based on user data (Walker et al2007; White, Clark, and Moore 2010).Interactive settings seem to call for sophisticated addressee modeling.
Detailedreasoning about the addressee can be computationally expensive, however, and somepsychologists have argued, based on clever experiments in which speakers andaddressees have slightly different information available, that speakers only have206Krahmer and van Deemter Computational Generation of Referring Expressionslimited capabilities for considering the addressee?s perspective (Horton and Keysar1996; Keysar, Lin, and Barr 2003; Lane, Groisman, and Ferreira 2006).
Some of the studiesmentioned herein, however, emphasize a level of cooperation that may not requireconscious planning: The balance of work on alignment, for example, suggests that itis predominantly an automatic process which does not take up much computationalresource.
Recently, Gatt, Krahmer, and Goudbeek (2011) proposed a new model forinteractive REG, consisting of a preference-based search process based on the IA, whichselects properties concurrently and in competition with a priming-based process, bothcontributing properties to a limited capacity working memory buffer.
This model offersa new way to think about interactive REG, and the role therein for REG algorithms ofthe kind discussed in this survey.6.5 What Is the Impact of Visual Information?In this survey, we have often discussed references to objects in shared visual scenes,partly because this offers a useful way to illustrate an algorithm.
So far, however,relatively few REG studies have taken visual information seriously.Most real-life scenes contain a multitude of potential referents.
Just look aroundyou: Every object in your field of vision could be referred to.
It is highly unlikelythat speakers would take all these objects into account when producing a referringexpression.
Indeed, there is growing evidence that the visual system and the speechproduction system are closely intertwined (Meyer, Sleiderink, and Levelt 1998; Spiveyet al 2001; Hanna and Brennan 2007), and that human speakers employ specific strate-gies when looking at real-world scenes (Itti and Koch 2000; Wooding et al 2002, amongothers).
Wooding and colleagues, for instance, found that certain properties of an image,such as changes in intensity and local contrasts, determine viewing patterns to a largeextent.
Top?down strategies also play a role: For instance, areas that are currently underdiscussion are looked at more frequently and for longer periods of time.
Little is knownabout how scene perception influences the human production of referring expressions,however, and how REG algorithms could mimic this.When discussing visual scenes, most REG researchers assume that some of therelevant visual information is stored in a database (compare our visual example scenein Figure 1 and its database representation in Table 1).
Still, the conversion from one tothe other is far from trivial.
Clearly, the visual scene is much more informative thanthe database; how do we decide which visual information we store in the databaseand which we ignore?
And how do we map visual information to symbolic labels?These are difficult questions that have received very little attention so far.
A partialanswer to the first question can be found in the work of John Kelleher and colleagues,who argue that visual and linguistic salience co-determine which aspects of a sceneare relevant for the understanding and generation of referring expressions (Kelleher,Costello, and van Genabith 2005; Kelleher and Kruijff 2006).
A partial answer to thesecond question is offered by Deb Roy and colleagues (Roy and Pentland 2002; Roy2005) who present a computational model for automatically grounding attributes basedon sensor data, and by Gorniak and Roy (2004) who apply such a model to referringexpressions.One impediment to progress in this area is the lack of relevant human data.
Most,if not all, of the dedicated data sets discussed in Section 5 were collected using artificialvisual scenes, either consisting of grids of unrelated objects not forming a coherentscene, or of coherent scenes of unrealistic simplicity.
Generally speaking, the situation in207Computational Linguistics Volume 38, Number 1psycholinguistics is not much better.
Recently, some studies started exploring the effectsof more realistic visual scenes on language production.
An example is Coco and Keller(2009), who Photoshopped a number of (more or less) realistic visual scenes, manipulat-ing the visual clutter and number of actors in each scene.
They found that more clutterand more actors resulted in longer delays before language production started, and thatthese factors influenced the syntactic constructions that were used as well.
A similarparadigm could be used to collect a new corpus of human-produced references, withtargets being an integral part of a visual scene (rather than being randomly positionedin a grid).
When participants are subsequently asked to refer to objects in these scenes,eye tracking can be used to monitor where they are looking before and during theproduction of particular references.
Such data would be instrumental for developingREG algorithms which take visual information seriously.6.6 What Knowledge Representation Framework Suits REG Best?Recent years have seen a strengthening of the link between REG and Knowledge Rep-resentation frameworks (see Section 4).
There is a new emphasis on questions involving(1) the expressive power of the formalism in which domain knowledge is expressed(e.g., does the formalism allow convenient representation of n-place predicates or quan-tification?
), (2) the expressive power of the formalism in which ontological informationis expressed (e.g., can it express more than just subsumption between concepts?
), (3) theamount of support available for logical inference, and (4) the mechanisms available forcontrolling the output of the generator.To illustrate the importance of expressive power and logical inference, consider thetype of examples discussed in Poesio and Vieira (1998).
What would it take to generatean expression like ?the report on the third quarter of 2009??
It would be cumbersome torepresent the relation between all entities separately, saying that 1950 has a first quarter,which has a report, and the same for all other years.
It would be more elegant andeconomical to spell out general rules, such as ?Every year has a unique first quarter,?
?Quarter 4 of a given year precedes Quarter 1 of any later year,?
?The relation ?precede?is transitive,?
and so on.
As NLG is starting to be applied in large-scale applications,the ability to capture generalizations of this kind is bound to become increasinglyimportant.It is remarkable that most REG research has distanced itself so drastically from otherareas of artificial intelligence, by limiting itself to atomic facts in the knowledge base.
IfREG came to be linked with modern knowledge representation formats?as opposed tothe simple attribute?value structures exemplified in Table 1?then atomic formulas areno longer the substance of the knowledge base but merely its seeds.
In many cases, re-sources developed for the semantic Web?ontology languages such as OWL, reasoningtools, and even the ontologies themselves?could be re-used in REG.
REG could evenlink up with ?real AI,?
by tapping into models of common-sense knowledge, such asLenat (1995) or Lieberman et al (2004).
The new possibilities raise interesting scientificand strategic questions.
For example, how do people generate referring expressions ofthe kind highlighted by the work of Poesio and colleagues?
Is this process best modeledusing a knowledge-rich approach using general axioms and deduction, or do otherapproaches offer a more accurate model?
Is it possible that, when REG starts to focus abit less on identification of the referent, the result might be a different, and possibly lesslogic-oriented problem?
What role could knowledge representation play in these cases?Here, as elsewhere in REG, we see ample space for future research.208Krahmer and van Deemter Computational Generation of Referring Expressions7.
General Conclusion and DiscussionAfter preparatory work in the 1980s by Appelt and Kronfeld, and the contributions sum-marized in Dale and Reiter (1995), the first decade of the new millennium has seen a newsurge of interest in referring expression generation.
Progress has been made in threerelated areas which have been discussed extensively in this survey.
First, researchershave lifted a number of simplifications present in the work of Dale and Reiter (1995)and others, thereby considerably extending coverage of REG algorithms to include, forinstance, relational, plural, and vague references (Section 3).
Second, proposals havebeen put forward to recast REG in terms of existing and well-understood computationalframeworks, such as labeled directed graphs and Description Logic, with various attrac-tive consequences (Section 4).
Third, there has been a shift towards data collection andempirical evaluation; this has made it possible to empirically evaluate REG algorithms,which is starting to give us an improved understanding of the strengths and weaknessesof existing work (Section 5).
As a result of these developments, REG is now one of thebest-developed subfields of NLG.How should the current state of the art in REG be assessed?
The good news is thatcurrent REG algorithms can produce natural descriptions, which may even be morehelpful than descriptions produced by people (Gatt, Belz, and Kow 2009).
This is onlytrue when certain simplifying assumptions are made, however, as in the early REGresearch typified by Dale and Reiter (1995).
When REG leaves this limited ?comfortzone,?
the picture changes drastically.
Although in recent years the research communityhas gained a much better understanding of the challenges that face REG in that widerarena, many of these challenges are still waiting to be met (Section 6).7.1 New ComplexitiesRecent REG research has revealed various new complexities.
Some of these pertainto the nature of the target.
Sets are difficult to refer to, for example, and algorithmsdesigned to deal with them achieve a lower human-likeness when referring to sets thanto individual objects (van Deemter et al in press).
Recent efforts to let REG algorithmsrefer to spatial regions suggest that in large, realistic domains, precise identification ofa target is a goal that can be approximated, but seldom achieved (Turner et al 2008;Turner, Sripada, and Reiter 2009).
Little work has been done so far on reference toevents, or to points and intervals in time (e.g., ?When Harry met Sally,?
?the momentafter the impact?
), and references to abstract and other uncountable entities (e.g., water,democracy) are beyond the horizon.
Where domain knowledge derives from sensordata?with unavoidably uncertain and noisy inputs?this is bound to cause problemsnot previously addressed by REG.
It is in such domains that salience (especially in thenon-linguistic sense) becomes a critical issue.
When reference takes place in real life?asopposed to a typical psycholinguistics experiment?it is often unclear what its saliencedepends on.
It might be that salience is partly in the eye of the beholder, and that this isone of the causes of the considerable individual variation that exists between differenthuman speakers (Dale and Viethen 2010).7.2 Human-Likeness and EvaluationIn early REG research, including Dale and Reiter (1995), it was often remarkably unclearwhat exactly the proposed algorithms aimed to achieve.
It was only when evaluationstudies were starting to be conducted that researchers had to ?show their cards?
and209Computational Linguistics Volume 38, Number 1say what they regarded as their criterion for success.
In most cases, they used a form ofhuman-likeness as their success criterion, by comparing the expressions generated byan algorithm with those in a corpus.The human-likeness criterion dictates that REG algorithms are to mimic humans?warts and all?
: If speakers produce unclear descriptions, then so should algorithms.But, of course, human-likeness is not the only yardstick that can be used.
In NLGsystems whose main aim is to be practically useful, for example, it may be moreimportant for referring expressions to be clear than to be human-like in all respects.
Thedifference is important because psycholinguists have shown that human speakers haveonly limited capabilities for taking the addressee into account, frequently producingexpressions that cannot be interpreted correctly by an addressee?for example, whenthey are under time pressure (Horton and Keysar 1996).
If usefulness or successfulness(Garoufi and Koller 2011), rather than human-likeness, is the yardstick for success thena different type of evaluation test needs to be used.
Possible tests include, for example,speed and accuracy of task completion (i.e., how often and how fast do readers findthe referent?).
A variety of hearer-oriented tests is starting to be used in recent REGresearch (Paraboni, van Deemter, and Masthoff 2007; Khan, van Deemter, and Ritchie2008), but evaluation of REG algorithms (and of NLG in general) remains difficult (see,e.g., Oberlander 1998; Belz 2009; and Gatt and Belz 2010).
Arguably, a central problemis that many different evaluations metrics are conceivable, and an REG algorithm (likean NLG system in general) may well score high on some and poorly on other metrics.Hearer-oriented experiments may also be useful for evaluating referring expres-sions that are logically complex (cf.
Section 4.4).
It is one thing for an REG algorithm touse logical quantification to generate a fairly simple description, such as ?the womanwho owns four cats,?
but quite another to generate a highly complex description (?thewoman who owns four cats that are chased by between three and six dogs each of whichis fed only by men?
), which can be generated using the same methods.
There are difficultmethodological questions to be answered here about whether the aim of the generator isto model human competence or human performance.
And if it is performance that is tobe modeled, then this raises the question of what types of complexities are exploited byhuman speakers, and what types of complexities are understandable to human hearers.Such questions can only be answered by new empirical studies.7.3 Widening the Scope of REG AlgorithmsMuch REG research has concentrated on the main ?paradigms?
of reference (Searle1969).
Early work on REG treated reference as emphatically part of communication,as we have seen (Section 2.1).
But after the refocusing that went on in the 1990s, manyREG algorithms operate as if describing objects were a goal unto itself, instead of apart of communication.
Still, when referring expressions occur in their natural habitat?in text or dialogue?then the reference game becomes subtly different, with factorssuch as salience and adaptation playing important (and partly unknown) roles.
In thesenatural contexts, it is also not always necessary to identify a referent ?in one shot?
: Indialogue, an identification of the referent can be seen as a joint responsibility of bothdialogue partners.
Even in monologue, an entire sequence of utterances may guide ahearer towards the referent.
In casual conversation, it is even unclear whether exactidentification of the referent is a requirement at all, in which case all existing algorithmsare wrong-footed.
Reference in real life is also characterized by domains that are muchlarger and complicated than the ones usually studied (at least until they have beennarrowed down by means of some salience metric): The set of people, for example, that210Krahmer and van Deemter Computational Generation of Referring Expressionswe are able to refer to in daily life is almost unlimited, and the properties that we can useto refer to them seem almost unbounded, including not only their physical appearanceand location, but their ideas, actions, and so on.
Evaluation challenges such as TUNAREG, GREC, and GIVE have helped to bring the research community together, focusingon small domains and, predominantly, on simple types of referring expressions.
Webelieve that it is time for evaluation studies to extend their remit and look at the types ofcomplex references that more recent REG research has drawn attention to.
Such studieswould do well, in our view, to pay considerable attention to the question of whichreferring expressions have the greatest benefit for readers or hearers.One day, perhaps, all these issues will have been resolved.
If there is anything that asurvey of the state of the art in REG makes clear it is that, for all the undeniable progressin this growing area of NLG, this holy grail is not yet within reach.AcknowledgmentsThe order of authors was determinedby chance; both contributed equally.Emiel Krahmer thanks The NetherlandsOrganisation for Scientific Research(NWO) for VICI grant ?Bridging the Gapbetween Computational Linguistics andPsycholinguistics: The Case of ReferringExpressions?
(277-70-007).
Kees vanDeemter thanks the EPSRC?s PlatformGrant ?Affecting People with NaturalLanguage.?
We both thank the anonymousreviewers for their constructive comments,and Doug Appelt, Johan van Benthem,Robert Dale, Martijn Goudbeek, HelmutHoracek, Ruud Koolen, Roman Kutlak,Chris Mellish, Margaret Mitchell, EhudReiter, Advaith Siddharthan, Matthew Stone,Marie?t Theune, Jette Viethen, and, especially,Albert Gatt for discussions and/orcomments on earlier versions of this text.Thanks to Jette Viethen for her extensiveREG bibliography, and to Harold Miesenfor producing the image in Figure 1 andfor allowing us to use it.ReferencesAbbott, Barbara.
2010.
Reference.
OxfordUniversity Press, Oxford, UK.Anderson, Anne A., Miles Bader,Ellen Gurman Bard, Elizabeth Boyle,Gwyneth Doherty, Simon Garrod,Stephen Isard, Jacqueline Kowtko,Jan McAllister, Jim Miller, CatherineSotillo, Henry Thompson, andRegina Weinert.
1991.
The HCRCmap task corpus.
Language and Speech,34:351?366.Appelt, Douglas.
1985.
Planning Englishreferring expressions.
Artificial Intelligence,26:1?33.Appelt, Douglas and Amichai Kronfeld.1987.
A computational model of referring.In Proceedings of the 10th International JointConference on Artificial Intelligence (IJCAI),pages 640?647, Milan.Areces, Carlos, Alexander Koller, andKristina Striegnitz.
2008.
Referringexpressions as formulas of DescriptionLogic.
In Proceedings of the 5th InternationalNatural Language Generation Conference(INLG), pages 42?49, Salt Fork, OH.Arnold, Jennifer E. 2008.
Referenceproduction: Production-internaland addressee-oriented processes.Language and Cognitive Processes,23:495?527.Baader, Franz, Diego Calvanese, DeborahMcGuinness, Daniele Nardi, and PeterPatel-Schneider.
2003.
The DescriptionLogic Handbook: Theory, Implementationand Applications.
Cambridge UniversityPress, Cambridge, UK.Baget, Jean-Franc?ois and Marie-LaureMugnier.
2002.
Extensions of simpleconceptual graphs: The complexityof rules and constraints.
Journalof Artificial Intelligence Research,16:425?465.Bangalore, Srinivas, Owen Rambow, andSteven Whittaker.
2000.
Evaluationmetrics for generation.
In Proceedingsof the 1st International Conference onNatural Language Generation (INLG),pages 1?8, Mitzpe Ramon.Belz, Anja.
2009.
That?s nice .
.
.
what can youdo with it?
[Last Words].
ComputationalLinguistics, 35:111?118.Belz, Anja and Albert Gatt.
2008.
Intrinsicvs.
extrinsic evaluation measuresfor referring expression generation.In Proceedings of the 46th Annual Meetingof the Association for ComputationalLinguistics (ACL), Columbus, OH.211Computational Linguistics Volume 38, Number 1Belz, Anja, Eric Kow, Jette Viethen, andAlbert Gatt.
2008.
The GREC challenge2008: Overview and evaluation results.In Proceedings of the 5th InternationalNatural Language Generation Conference(INLG), pages 183?191, Salt Fork, OH.Belz, Anja, Eric Kow, Jette Viethen, andAlbert Gatt.
2010.
Generating referringexpressions in context: The GREC taskevaluation challenges.
In Emiel Krahmerand Marie?t Theune, editors, EmpiricalMethods in Natural Language Generation.Springer Verlag, Berlin, pages 294?327.Bohnet, Bernd and Robert Dale.
2005.Viewing referring expression generationas search.
In Proceedings of the 19thInternational Joint Conference on ArtificialIntelligence (IJCAI), pages 1004?1009,Edinburgh.Brachman, Ronald J. and James G. Schmolze.1985.
An overview of the KL?ONEknowledge representation system.Cognitive Science, 9(2):171?216.Branigan, Holly P., Martin J. Pickering,Jamie Pearson, and Janet F. McLean.
2010.Linguistic alignment between peopleand computers.
Journal of Pragmatics,42:2355?2368.Brennan, Susan and Herbert H. Clark.
1996.Conceptual pacts and lexical choice inconversation.
Journal of ExperimentalPsychology, 22(6):1482?1493.Buschmeier, Hendrik, Kirsten Bergmann,and Stefan Kopp.
2009.
An alignment-capable microplanner for natural languagegeneration.
In Proceedings of the 12thEuropean Workshop on Natural LanguageGeneration (ENLG), pages 82?89, Athens.Callaway, Charles and James Lester.
2002.Pronominalization in generated discourseand dialogue.
In Proceedings of the 40thAnnual Meeting of the Association forComputational Linguistics (ACL),pages 88?95, Philadelphia, PA.Chafe, Wallace W. 1980.
The Pear Stories:Cognitive, Cultural, and Linguistic Aspects ofNarrative Production.
Ablex, Norwood, NJ.Chantree, Francis, Adam Kilgarriff, Annede Roeck, and Alistair Willis.
2005.Disambiguating coordinations using worddistribution information.
In Proceedingsof the International Conference on RecentAdvances in Natural Language Processing(RANLP), pages 144?151, Borovets.Clark, Herbert H. and Adrian Bangerter.2004.
Changing ideas about reference.In Ira A. Noveck and Dan Sperber,editors, Experimental Pragmatics.
PalgraveMacmillan, Basingstoke, UK, pages 25?49.Clark, Herbert H. and Gregory Murphy.1983.
Audience design in meaning andreference.
In Jean Francois Le Ny andWalter Kintsch, editors, Language andComprehension.
North Holland, TheNetherlands, pages 287?299.Coco, Moreno I. and Frank Keller.
2009.The impact of visual information onreference assignment in sentenceproduction.
In Proceedings of the 31stAnnual Conference of the CognitiveScience Society (CogSci), pages 274?279,Amsterdam.Cohen, Philip R. and Hector J. Levesque.1985.
Speech acts and rationality.
InProceedings of the 23rd Annual Meetingof the Association of Computational Linguists(ACL), pages 49?60, Chicago, IL.Croitoru, Madalina and Kees van Deemter.2007.
A conceptual graph approach tothe generation of referring expressions.In Proceedings of the 20th InternationalJoint Conference on Artificial Intelligence(IJCAI), pages 2456?2461, Hyderabad.Dale, Robert.
1989.
Cooking up referringexpressions.
In Proceedings of the 27thAnnual Meeting of the Association forComputational Linguistics (ACL),pages 68?75.Dale, Robert.
1992.
Generating ReferringExpressions: Constructing Descriptions in aDomain of Objects and Processes.
The MITPress, Cambridge, MA.Dale, Robert and Nicolas Haddock.
1991.Generating referring expressions involvingrelations.
In Proceedings of the 5th Conferenceof the European Chapter of the Associationof Computational Linguists (EACL),pages 161?166, Berlin.Dale, Robert and Ehud Reiter.
1995.Computational interpretations of theGricean maxims in the generation ofreferring expressions.
Cognitive Science,18:233?263.Dale, Robert and Jette Viethen.
2010.Attribute-centric referring expressiongeneration.
In Emiel Krahmer and Marie?tTheune, editors, Empirical Methods inNatural Language Generation.
SpringerVerlag, Berlin, pages 163?179.Denis, Alexandre.
2010.
Generating referringexpressions with reference domain theory.In Proceedings of the 6th InternationalNatural Language Generation Conference(INLG), pages 27?35, Trim.DeVault, David, Charles Rich, andCandace L. Sidner.
2004.
Naturallanguage generation and discoursecontext: Computing distractor sets from212Krahmer and van Deemter Computational Generation of Referring Expressionsthe focus stack.
In Proceedings of the 17thInternational Meeting of the Florida ArtificialIntelligence Research Society (FLAIRS),Miami Beach, FL.Di Eugenio, Barbara, Pamela W. Jordan,Richmond H. Thomason, and Johanna D.Moore.
2000.
The agreement process: anempirical investigation of human?humancomputer-mediated collaborative dialogs.International Journal of Human?ComputerStudies, 53:1017?1076.Dice, Lee R. 1945.
Measures of the amountof ecologic association between species.Ecology, 26:297?302.Doddington, George.
2002.
Automaticevaluation of machine translation qualityusing n-gram co-occurrence statistics.In Proceedings of the 2nd InternationalConference on Human Language TechnologyResearch (HLT), pages 138?145,San Diego, CA.Engelhardt, Paul E., Karl G.D Bailey, andFernanda Ferreira.
2006.
Do speakers andlisteners observe the Gricean Maxim ofQuantity?
Journal of Memory and Language,54:554?573.Gardent, Claire.
2002.
Generating minimaldefinite descriptions.
In Proceedings of the40th Annual Meeting of the Association forComputational Linguistics (ACL),pages 96?103, Philadelphia, PA.Gardent, Claire and Kristina Striegnitz.
2007.Generating bridging definite descriptions.In Harry Bunt and Reinhard Muskens,editors, Computing Meaning, Volume 3.Studies in Linguistics and Philosophy.Springer Publishers, pages 369?396,Berlin, DB.Garey, Michael R. and David S. Johnson.1979.
Computers and Intractability:A Guide to the Theory of NP?Completeness.W.
H. Freeman, New York.Garoufi, Konstantina and Alexander Koller.2011.
Combining symbolic andcorpus-based approaches for thegeneration of successful referringexpressions.
In Proceedings of the 13thEuropean Workshop on Natural LanguageGeneration (ENLG), Nancy.Gatt, Albert.
2007.
Generating CoherentReferences to Multiple Entities.
UnpublishedPh.D.
thesis, University of Aberdeen.Gatt, Albert and Anja Belz.
2010.
Introducingshared task evaluation to NLG: TheTUNA shared task evaluation challenges.In Emiel Krahmer and Marie?t Theune,editors, Empirical Methods in NaturalLanguage Generation.
Springer Verlag,Berlin, pages 264?293.Gatt, Albert, Anja Belz, and Eric Kow.
2008.The TUNA challenge 2008: Overview andevaluation results.
In Proceedings of the5th International Conference on NaturalLanguage Generation (INLG),pages 198?206, Salt Fork, OH.Gatt, Albert, Anja Belz, and Eric Kow.2009.
The TUNA?REG challenge 2009:Overview and evaluation results.In Proceedings of the 12th EuropeanWorkshop on Natural Language Generation(ENLG), pages 174?182, Athens.Gatt, Albert, Emiel Krahmer, and MartijnGoudbeek.
2011.
Attribute preferenceand priming in reference production:Experimental evidence and computationalmodeling.
In Proceedings of the 33rd AnnualMeeting of the Cognitive Science Society(CogSci), pages 2627?2632, Boston, MA.Gatt, Albert and Kees van Deemter.
2007.Lexical choice and conceptual perspectivein the generation of plural referringexpressions.
Journal of Logic, Languageand Information, 16:423?443.Gatt, Albert, Ielka van der Sluis, and Keesvan Deemter.
2007.
Evaluating algorithmsfor the generation of referring expressionsusing a balanced corpus.
In Proceedings ofthe 11th European Workshop on NaturalLanguage Generation (ENLG), pages 49?56,Schloss Dagstuhl.Giuliani, Manuel, Mary Ellen Foster, AmyIsard, Colin Matheson, Jon Oberlander,and Alois Knoll.
2010.
Situated reference ina hybrid human?robot interaction system.In Proceedings of the 6th InternationalNatural Language Generation Conference(INLG), pages 67?76, Dublin.Goldberg, Eli, Norbert Driedger, andRichard Kittredge.
1994.
Using naturallanguage processing to produce weatherforecasts.
IEEE Expert, 9(2):45?53.Gorniak, Peter and Deb Roy.
2004.
Groundedsemantic composition for visual scenes.Journal of Artificial Intelligence Research,21:429?470.Goudbeek, Martijn and Emiel Krahmer.2010.
Preferences versus adaptationduring referring expression generation.In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 55?59, Uppsala.Gratch, Jonathan, Jeff Rickel, ElisabethAndre?, Norman Badler, Justine Cassell,and Eric Petajan.
2002.
Creating interactivevirtual humans: Some assembly required.IEEE Intelligent Systems, 17:54?63.Grice, Paul.
1975.
Logic and conversation.
InPeter Cole and Jeffrey L. Morgan, editors,213Computational Linguistics Volume 38, Number 1Syntax and Semantics, Vol.
3: Speech Acts.Academic Press, New York, pages 43?58.Grosz, Barbara J., Aravind K. Joshi, and ScottWeinstein.
1995.
Centering: A frameworkfor modeling the local coherence ofdiscourse.
Computational Linguistics,21:203?225.Grosz, Barbara J. and Candace L. Sidner.1986.
Attention, intentions, and thestructure of discourse.
ComputationalLinguistics, 12:175?204.Guhe, Markus and Ellen Gurman Bard.2008.
Adapting referring expressionsto the task environment.
In Proceedings ofthe 30th Annual Conference of the CognitiveScience Society (CogSci), pages 2404?2409,Austin, TX.Gundel, Jeanette, Nancy Hedberg, and RonZacharski.
1993.
Cognitive status and formof referring expressions in discourse.Language, 69:247?307.Gupta, Surabhi and Amanda Stent.
2005.Automatic evaluation of referringexpression generation using corpora.In Proceedings of the 1st Workshop on UsingCorpora in Natural Language Generation(UCNLG), pages 1?6, Brighton.Hajic?ova?, Eva.
1993.
Issues of SentenceStructure and Discourse Patterns?Theoreticaland Computational Linguistics, Vol.
2.Charles University, Prague.Hanna, Joy E. and Susan E. Brennan.
2007.Speaker?s eye gaze disambiguatesreferring expressions early duringface-to-face conversation.
Journal ofMemory and Language, 57:596?615.Heeman, Peter A. and Graeme Hirst.
1995.Collaborating on referring expressions.Computational Linguistics, 21(3):351?382.Hendrickx, Iris, Walter Daelemans, KimLuyckx, Roser Morante, and VincentVan Asch.
2008.
CNTS: Memory-basedlearning of generating repeated references.In Proceedings of the 5th InternationalNatural Language Generation Conference(INLG), pages 194?195, Salt Fork, OH.Henschel, Renate, Hua Cheng, and MassimoPoesio.
2000.
Pronominalisation revisited.In Proceedings of the 18th InternationalConference on Computational Linguistics(COLING), pages 306?312, Saarbru?cken.Hopcroft, John.
1971.
An n log(n) algorithmfor minimizing states in a finiteautomaton.
In Zvi Kohave, editor, Theory ofMachines and Computations, pages 189?196.Academic Press, New York.Horacek, Helmut.
1996.
A new algorithmfor generating referring expressions.In Proceedings of the 12th EuropeanConference on Artificial Intelligence (ECAI),pages 577?581, Budapest.Horacek, Helmut.
1997.
An algorithm forgenerating referential descriptions withflexible interfaces.
In Proceedings of the35th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 206?213, Madrid.Horacek, Helmut.
2004.
On referring to setsof objects naturally.
In Proceedings of the 3rdInternational Conference on Natural LanguageGeneration (INLG), pages 70?79,Brockenhurst.Horacek, Helmut.
2005.
Generatingreferential descriptions under conditionsof uncertainty.
In Proceedings of the 10thEuropean Workshop on Natural LanguageGeneration (ENLG), pages 58?67,Aberdeen.Horton, William S. and Boaz Keysar.
1996.When do speakers take into accountcommon ground?
Cognition, 59:91?117.Itti, Laurent and Christof Koch.
2000.A saliency-based search mechanism forovert and covert shifts in visual attention.Vision Research, 40:1489?1506.Jaccard, Paul.
1901.
E?tude comparative de ladistribution florale dans une portion desalpes et des jura.
Bulletin de la Socie?te?Vaudoise des Sciences Naturelles, 37:547?579.Janarthanam, Srinivasan and Oliver Lemon.2009.
Learning lexical alignment policiesfor generating referring expressions forspoken dialogue systems.
In Proceedingsof the 12th European Workshop on NaturalLanguage Generation (ENLG), pages 74?81,Athens.Jordan, Pamela W. 2000.
Intentional Influenceson Object Redescriptions in Dialogue:Evidence from an Empirical Study.
Ph.D.thesis, University of Pittsburgh.Jordan, Pamela W. 2002.
Contextualinfluences on attribute selection forrepeated descriptions.
In Kees vanDeemter and Rodger Kibble, editors,Information Sharing: Reference andPresupposition in Language Generationand Interpretation, pages 295?328.
CSLIPublications, Stanford, CA.Jordan, Pamela W. and Marilyn Walker.2005.
Learning content selection rules forgenerating object descriptions in dialogue.Journal of Artificial Intelligence Research,24:157?194.Kelleher, John, Fintan Costello, and Josef vanGenabith.
2005.
Dynamically structuring,updating and interrelating representationsof visual and linguistics discourse context.Artificial Intelligence, 167:62?102.214Krahmer and van Deemter Computational Generation of Referring ExpressionsKelleher, John and Geert-Jan Kruijff.2006.
Incremental generation ofspatial referring expressions insituated dialog.
In Proceedings of the21st International Conference onComputational Linguistics (COLING)and 44th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 1041?1048, Sydney.Kerdiles, Gwen.
2001.
Saying It with Pictures:a Logical Landscape of Conceptual Graphs.Unpublished Ph.D. thesis, ILLC,Amsterdam.Keysar, Boaz, Shuhong Lin, and Dale J. Barr.2003.
Limits on theory of mind use inadults.
Cognition, 89:25?41.Khan, Imtiaz Hussain, Kees van Deemter,and Graeme Ritchie.
2008.
Generation ofreferring expressions: Managing structuralambiguities.
In Proceedings of the 22thInternational Conference on ComputationalLinguistics (COLING), pages 433?440,Manchester.Kibble, Rodger and Richard Power.
2004.Optimizing referential coherence in textgeneration.
Computational Linguistics,30:401?416.Kilgarriff, Adam.
2003.
Thesauruses fornatural language processing.
In Proceedingsof the International Conference on NaturalLanguage Processing and KnowledgeEngineering (NLP-KE), pages 5?13,Beijing.Koller, Alexander and Matthew Stone.2007.
Sentence generation as a planningproblem.
In Proceedings of the 45th AnnualMeeting of the Association for ComputationalLinguistics Conference Proceedings (ACL),pages 337?343, Prague.Koller, Alexander, Kristina Striegnitz,Donna Byron, Justine Cassell, Robert Dale,Johanna Moore, and Jon Oberlander.2010.
The first challenge on generatinginstructions in virtual environments.In Emiel Krahmer and Marie?t Theune,editors, Empirical Methods in NaturalLanguage Generation.
Springer Verlag,Berlin, pages 328?352.Koolen, Ruud, Albert Gatt, MartijnGoudbeek, and Emiel Krahmer.
2009.Need I say more?
On factors causingreferential overspecification.
In Proceedingsof the CogSci Workshop on the Production ofReferring Expressions (PRE-CogSci 2009),Amsterdam.Kopp, Stefan, Kirsten Bergmann, andIpke Wachsmuth.
2008.
Multimodalcommunication from multimodalthinking: Towards an integrated modelof speech and gesture production.Semantic Computing, 2:115?136.Krahmer, Emiel.
2010.
What computationallinguists can learn from psychologists(and vice versa).
Computational Linguistics,36:285?294.Krahmer, Emiel and Marie?t Theune.
2002.Efficient context-sensitive generation ofdescriptions in context.
In Kees vanDeemter and Rodger Kibble, editors,Information Sharing: Givenness and Newnessin Language Processing.
CSLI Publications,Stanford, CA, pages 223?264.Krahmer, Emiel, Marie?t Theune, JetteViethen, and Iris Hendrickx.
2008.Graph: The costs of redundancy inreferring expressions.
In Proceedingsof the International Conference onNatural Language Generation (INLG),pages 227?229, Salt Fork, OH.Krahmer, Emiel, Sebastiaan van Erk,and Andre?
Verleg.
2003.
Graph-basedgeneration of referring expressions.Computational Linguistics, 29(1):53?72.Kronfeld, Amichai.
1990.
Reference andComputation: An Essay in Applied Philosophyof Language.
Cambridge University Press,Cambridge.Kumar, Vipin.
1992.
Algorithms forconstraint satisfaction problems: a survey.Artificial Intelligence Magazine, 1:32?44.Lane, Liane Wardlow, Michelle Groisman,and Victor S. Ferreira.
2006.
Don?t talkabout pink elephants!
Speakers?
controlover leaking private information duringlanguage production.
Psychological Science,17 (4):273?277.Lenat, Douglas.
1995.
CYC: A large-scaleinvestment in knowledge infrastructure.Communication of the ACM, 38:33?38.Lester, James, Jennifer Voerman, StuartTowns, and Charles Callaway.
1999.Deictic believability: Coordinatinggesture, locomotion, and speech inlifelike pedagogical agents.
AppliedArtificial Intelligence, 13:383?414.Levenshtein, Vladimir I.
1966.
Binary codescapable of correcting deletions, insertions,and reversals.
Soviet Physics Doklady,10:707?710.Lieberman, Henry, Hugo Liu, Push Singh,and Barbara Barry.
2004.
Beating commonsense into interactive applications.AI Magazine, 25(4):63?76.Lin, Chin-Yew and Eduard Hovy.
2003.Automatic evaluation of summariesusing N-gram co?occurrence statistics.In Proceedings of the Human LanguageTechnology Conference of the North American215Computational Linguistics Volume 38, Number 1Chapter of the Association for ComputationalLinguistics (HLT?NAACL), pages 71?78,Edmonton.Lin, Dekang.
1998.
An information-theoreticdefinition of similarity.
In Proceedingsof the 15th International Conference onMachine Learning (ICML), pages 296?304,Madison, WI.L?nning, Jan Tore.
1997.
Plurals andcollectivity.
In Johan van Benthem andAlice ter Meulen, editors, Handbookof Logic and Language.
Elsevier,Amsterdam, pages 1009?1054.Malouf, Robert.
2000.
The order ofprenominal adjectives in naturallanguage generation.
In Proceedingsof the 38th Annual Meeting of theAssociation for ComputationalLinguistics (ACL), pages 85?92,Hong Kong.McCluskey, Edward J.
1965.
Introductionto the Theory of Switching Circuits.McGraw?Hill, New York.McCoy, Kathleen and Michael Strube.1999.
Generating anaphoric expressions:Pronoun or definite description?In Proceedings of ACL Workshop onDiscourse and Reference Structure,pages 63?71, College Park, MD.Mellish, Chris, Donia Scott, Lynn Cahill,Daniel Paiva, Roger Evans, and MikeReape.
2006.
A reference architecturefor natural language generationsystems.
Natural Language Engineering,12:1?34.Metzing, Charles A. and Susan E. Brennan.2003.
When conceptual pacts are broken:Partner effects on the comprehension ofreferring expressions.
Journal of Memoryand Language, 49:201?213.Meyer, Antje S., Astrid M. Sleiderink, andWillem J. M. Levelt.
1998.
Viewing andnaming objects: Eye movements duringnoun phrase production.
Cognition,66:B25?B33.Mitchell, Margaret.
2009.
Class-basedordering of prenominal modifiers.In Proceedings of the 12th European Workshopon Natural Language Generation (ENLG),pages 50?57, Athens.Nenkova, Ani and Kathleen R. McKeown.2003.
References to named entities:A corpus study.
In Proceedings of theHuman Language Technology (HLT)Conference, Companion Volume,pages 70?73, Edmonton.Oberlander, Jon.
1998.
Do the rightthing .
.
.
but expect the unexpected.Computational Linguistics, 24:501?507.O?Donnell, Michael, Hua Cheng, and JanetHitzeman.
1998.
Integrating referring andinforming in NP planning.
In Proceedingsof the ACL Workshop on The ComputationalTreatment of Nominals, pages 46?55,Montreal.Olson, David R. 1970.
Language andthought: Aspects of a cognitive theoryof semantics.
Psychological Review,77:257?273.Papineni, Kishore, Salim Roukos, ToddWard, and Wei-Jing Zhu.
2002.
BLEU:A method for automatic evaluation ofmachine translation.
In Proceedings ofthe 40th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 311?318, Philadelphia, PA.Paraboni, Ivandre?, Kees van Deemter,and Judith Masthoff.
2007.
Generatingreferring expressions: Making referentseasy to identity.
Computational Linguistics,33:229?254.Passonneau, Rebecca.
1996.
Using centeringto relax Gricean informational constraintson discourse anaphoric noun phrases.Language and Speech, 39:229?264.Passonneau, Rebecca.
2006.
Measuringagreement on set-valued items (MASI)for semantic and pragmatic annotation.In Proceedings of the 5th InternationalConference on Language Resources andEvaluation (LREC), pages 831?836,Genoa.Pechmann, Thomas.
1989.
Incrementalspeech production and referentialoverspecification.
Linguistics, 27:98?110.Pickering, Martin and Simon Garrod.
2004.Towards a mechanistic psychology ofdialogue.
Behavioural and Brain Sciences,27:169?226.Piwek, Paul.
2008.
Proximal and distal inlanguage and cognition: Evidence fromdeictic demonstratives in Dutch.
Journalof Pragmatics, 40:694?718.Poesio, Massimo, Rosemary Stevenson,Barbara di Eugenio, and Janet Hitzeman.2004.
Centering: A parametric theory andits instantiations.
Computational Linguistics,30:309?363.Poesio, Massimo and Renata Vieira.
1998.A corpus-based investigation of definitedescription use.
Computational Linguistics,24:183?216.Pollack, Martha.
1991.
Overloadingintentions for efficient practical reasoning.Nou?s, 25:513?536.Portet, Francois, Ehud Reiter, Albert Gatt,Jim Hunter, Somayajulu Sripada, YvonneFreer, and Cindy Sykes.
2009.
Automatic216Krahmer and van Deemter Computational Generation of Referring Expressionsgeneration of textual summaries fromneonatal intensive care data.
ArtificialIntelligence, 173:789?816.Quirk, Randolph, Sidney Greenbaum,Geoffrey Leech, and Jan Svartvik.
1980.A Grammar of Contemporary English(9th ed).
Longman, Burnt Mill,Harlow, UK.Read, Ronald C. and Derek G. Corneil.
1977.The graph isomorphism disease.
Journalof Graph Theory, 1(1):339?363.Reiter, Ehud.
1990.
The computationalcomplexity of avoiding conversationalimplicatures.
In Proceedings of the 28thAnnual Meeting of the Association forComputational Linguistics (ACL),pages 97?104, Pittsburgh, PA.Reiter, Ehud and Robert Dale.
1992.
A fastalgorithm for the generation of referringexpressions.
In Proceedings of the 14thInternational Conference on ComputationalLinguistics (COLING), pages 232?238,Nantes.Reiter, Ehud and Robert Dale.
2000.Building Natural Language GenerationSystems.
Cambridge University Press,Cambridge, UK.Ren, Yuan, Kees van Deemter, and Jeff Pan.2010.
Charting the potential of DescriptionLogic for the generation of referringexpressions.
In Proceedings of the 6thInternational Natural Language GenerationConference (INLG), pages 115?124, Dublin.Rosch, Eleanor.
1978.
Principles ofcategorization.
In Eleanor Rosch andBarbara L. Lloyd, editors, Cognition andCategorization.
Erlbaum, Hillsdale, NJ,pages 27?48.Roy, Deb.
2005.
Grounding words inperception and action: Computationalinsights.
Trends in Cognitive Sciences,9(8):389?396.Roy, Deb and Alex Pentland.
2002.Learning words from sights and sounds:A computational model.
Cognitive Science,26:113?146.Scha, Remko and David Stallard.
1988.Multi-level plurals and distributivity.In Proceedings of the 26th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 17?24, Buffalo, NY.Searle, John.
1969.
Speech Acts: An Essay inthe Philosophy of Language.
CambridgeUniversity Press, Cambridge, UK.Shaw, James and Vasileios Hatzivassiloglou.1999.
Ordering among premodifiers.In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 135?143, College Park, MA.Siddharthan, Advaith and Ann Copestake.2004.
Generating referring expressionsin open domains.
In Proceedings of the42nd Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 407?414, Barcelona.Siddharthan, Advaith, Ani Nenkova, andKathleen McKeown.
2011.
Informationstatus distinctions and referringexpressions: An empirical study ofreferences to people in news summaries.Computational Linguistics, 37(4):811?842.Sonnenschein, Susan.
1984.
The effect ofredundant communication on listeners:Why different types may have differenteffects.
Journal of Psycholinguistic Research,13:147?166.Sowa, John.
1984.
Conceptual Structures:Information Processing in Mind and Machine.Addison-Wesley, Boston, MA.Spivey, Michael, Melinda Tyler, KathleenEberhard, and Michael Tanenhaus.
2001.Linguistically mediated visual search.Psychological Science, 12:282?286.Stoia, Laura, Donna K. Byron,Darla Magdalene Shockley, andEric Fosler-Lussier.
2006.
Nounphrase generation for situated dialogs.In Proceedings of the 4th InternationalNatural Language Generation Conference(INLG), pages 81?88, Sydney.Stone, Matthew.
2000.
On identifying sets.In Proceedings of the 1st InternationalConference on Natural Language Generation(INLG), pages 116?123, Mitzpe Ramon.Stone, Matthew, Christine Doran,Bonnie Webber, Tonia Bleam, andMartha Palmer.
2003.
Microplanningwith communicative intentions:The SPUD system.
ComputationalIntelligence, 19:311?381.Stone, Matthew and Bonnie Webber.
1998.Textual economy through close couplingof syntax and semantics.
In Proceedingsof the 9th International Workshop onNatural Language Generation (INLG),pages 178?187, Niagara-on-the-Lake.Theune, Marie?t, Ruud Koolen, EmielKrahmer, and Sander Wubben.
2011.
Doessize matter: How much data is required totrain a REG algorithm?
In Proceedings of the49th Annual Meeting of the Association forComputational Linguistics: Human LanguageTechnologies (ACL-HLT), pages 660?664,Portland, OR.Turner, Ross, Somayajulu Sripada, andEhud Reiter.
2009.
Generatingapproximate geographic descriptions.In Proceedings of the 12th European217Computational Linguistics Volume 38, Number 1Workshop on Natural Language Generation(ENLG), pages 42?49, Athens.Turner, Ross, Somayajulu Sripada,Ehud Reiter, and Ian P. Davy.
2008.Using spatial reference frames togenerate grounded textual summaries ofgeoreferenced data.
In Proceedings ofthe 5th International Natural LanguageGeneration Conference (INLG), pages 16?24,Salt Fork, OH.van Deemter, Kees.
2002.
Generatingreferring expressions: Boolean extensionsof the Incremental Algorithm.Computational Linguistics, 28(1):37?52.van Deemter, Kees.
2006.
Generatingreferring expressions that involvegradable properties.
ComputationalLinguistics, 32(2):195?222.van Deemter, Kees.
2010.
Not Exactly: InPraise of Vagueness.
Oxford UniversityPress, Oxford, UK.van Deemter, Kees, Albert Gatt, Ielka van derSluis, and Richard Power (in press).Generation of referring expressions:Assessing the Incremental Algorithm.Cognitive Science, to appear.van Deemter, Kees and Emiel Krahmer.
2007.Graphs and Booleans: On the generation ofreferring expressions.
In Harry Bunt andReinhard Muskens, editors, ComputingMeaning, Volume 3.
Studies in Linguisticsand Philosophy.
Springer Publishers,Berlin, pages 397?422.van Deemter, Kees, Ielka van der Sluis, andAlbert Gatt.
2006.
Building a semanticallytransparent corpus for the generation ofreferring expressions.
In Proceedingsof the 4th International Conference onNatural Language Generation (INLG),pages 130?132, Sydney.van der Sluis, Ielka and Emiel Krahmer.2007.
Generating multimodal referringexpressions.
Discourse Processes,44(3):145?174.van der Wege, Mija.
2009.
Lexicalentrainment and lexical differentiationin reference phrase choice.
Journal ofMemory and Language, 60:448?463.van Hentenryck, Pascal.
1989.
ConstraintSatisfaction in Logic Programming.The MIT Press, Cambridge, MA.van Rijsbergen, C. J.
1979.
InformationRetrieval.
Butterworths, London,2nd edition.Viethen, Jette and Robert Dale.
2006.Algorithms for generating referringexpressions: Do they do what peopledo?
In Proceedings of the 4th InternationalConference on Natural Language Generation(INLG), pages 63?70, Sydney.Viethen, Jette and Robert Dale.
2007.Evaluation in natural languagegeneration: Lessons from referringexpression generation.
TraitementAutomatique des Langues, 48:141?160.Viethen, Jette and Robert Dale.
2008.The use of spatial relations in referringexpressions.
In Proceedings of the 5thInternational Natural Language GenerationConference (INLG), pages 59?67,Salt Fork, OH.Viethen, Jette, Robert Dale, and MarkusGuhe.
2011.
Generating subsequentreference in shared visual scenes:Computation vs re-use.
In Proceedingsof the Conference on Empirical Methods inNatural Language Processing (EMNLP),pages 1158?1167, Edinburgh.Viethen, Jette, Robert Dale, Emiel Krahmer,Marie?t Theune, and Pascal Touset.
2008.Controlling redundancy in referringexpressions.
In Proceedings of the 6thLanguage Resources and EvaluationConference (LREC), pages 239?246,Marrakech.Viethen, Jette, Simon Zwarts, Robert Dale,and Markus Guhe.
2010.
Dialoguereference in a visual domain.
In Proceedingsof the 7th Language Resources and EvaluationConference (LREC), Valetta.Walker, Marilyn, Amanda Stent, Franc?oisMairesse, and Rashmi Prasad.
2007.Individual and domain adaptationin sentence planning for dialogue.Journal of Artificial Intelligence Research,30:413?456.White, Michael, Robert Clark, andJohanna Moore.
2010.
Generatingtailored, comparative descriptions withcontextually appropriate intonation.Computational Linguistics, 36:159?201.Winograd, Terry.
1972.
Understanding NaturalLanguage.
Academic Press, New York.Wooding, David, Mark Muggelstone,Kevin Purdy, and Alastair Gale.
2002.Eye movements of large populations.Behavior Research Methods, Instrumentsand Computers, 34:509?517.218
