Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 13?18,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsDetecting Novelty in the context of Progressive SummarizationPraveen BysaniLanguage Technologies Research CenterIIIT Hyderabadlvsnpraveen@research.iiit.ac.inAbstractA Progressive summary helps a user to moni-tor changes in evolving news topics over a pe-riod of time.
Detecting novel information isthe essential part of progressive summariza-tion that differentiates it from normal multidocument summarization.
In this work, weexplore the possibility of detecting novelty atvarious stages of summarization.
New scoringfeatures, Re-ranking criterions and filteringstrategies are proposed to identify ?relevantnovel?
information.
We compare these tech-niques using an automated evaluation frame-work ROUGE, and determine the best.
Over-all, our summarizer is able to perform on parwith existing prime methods in progressivesummarization.1 IntroductionSummarization is the process of condensing text toits most essential facts.
Summarization is challeng-ing for its associated cognitive task and interestingbecause of its practical usage.
It has been success-fully applied for text content such as news articles 1,scientific papers (Teufel and Moens, 2002) that fol-low a discourse structure.
Update summarization isan emerging area with in summarization, acquiringsignificant research focus during recent times.
Thetask was introduced at DUC 20072 and continuedduring TAC 2008, 20093.
We refer to update sum-mariztion as ?Progressive Summarization?
in rest of1http://newsblaster.cs.columbia.edu/2http://duc.nist.gov/duc2007/tasks.html3http://www.nist.gov/tacthis paper, as summaries are produced periodicallyin a progressive manner and the latter title is moreapt to the task.
Progressive summaries contain infor-mation which is both relevant and novel, since theyare produced under the assumption that user has al-ready read some previous documents/articles on thetopic.
Such summaries are extremely useful in track-ing news stories, tracing new product reviews etc.Unlike dynamic summarization (Jatowt, 2004)where a single summary transforms periodically, re-flecting changes in source text, Progressive summa-rizer produce multiple summaries at specific timeintervals updating user knowledge.
Temporal Sum-marization (Allan et al, 2001) generate summaries,similar to progressive summaries by ranking sen-tences as combination of relevant and new scores.In this work, summaries are produced not just byreforming ranking scheme but also altering scoringand extraction stages of summarization.Progressive summarization requires differentiat-ing Relevant and Novel Vs Non-Relevant and NovelVs Relevant and Redundant information.
Such dis-crimination is feasible only with efficient Noveltydetection techniques.
We define Novelty detectionas identifying relevant sentences containing new in-formation.
This task shares similarity with TRECNovelty Track 4, that is designed to investigate sys-tems abilities to locate sentences containing relevantand/or new information given the topic and a set ofrelevant documents ordered by date.
A progressivesummarizer needs to identify, score and then finallyrank ?relevant novel?
sentences to produce a sum-mary.4http://trec.nist.gov/data/novelty.html13Previous approaches to Novelty detection atTREC (Soboroff, 2004) include cosine filter-ing (Abdul-Jaleel et al, 2004), where a sentencehaving maximum cosine similarity value with pre-vious set of sentences, lower than a preset thresh-old is considered novel.
Alternatively, (Schiffmanand McKeown, 2004) considered previously unseenwords as an evidence of Novelty.
(Eichmannac etal., 2004) expanded all noun phrases in a sentenceusing wordnet and used corresponding sysnsets fornovelty comparisions.Our work targets exploring the effect of detect-ing novelty at different stages of summarization onthe quality of progressive summaries.
Unlike mostof the previous work (Li et al, 2009) (Zhang etal., 2009) in progressive summarization, we em-ploy multiple novelty detection techniques at differ-ent stages and analyze them all to find the best.2 Document SummarizationThe Focus of this paper is only on extrac-tive summarization, henceforth term summariza-tion/summarizer implies sentence extractive multidocument summarization.
Our Summarizer has 4major stages as shown in Figure 1,Figure 1: Stages in a Multi Document SummarizerEvery news article/document is cleaned fromnews heads, HTML tags and split into sentences dur-ing Pre-processing stage.
At scoring, several sen-tence scoring features assign scores for each sen-tence, reflecting its topic relevance.
Feature scoresare combined to get a final rank for the sentencein ranking stage.
Rank of a sentence is predictedfrom regression model built on feature vectors ofsentences in the training data using support vectormachine as explained in (Schilder and Kondadandi,2008).
Finally during summary extraction, a sub-set of ranked sentences are selected to produce sum-mary after a redundancy check to filter duplicatesentences.2.1 Normal SummarizersTwo normal summarizers (DocSumm, TacBaseline)are developed in a similar fashion described inFigure 1.DocSumm produce summaries with two scoringfeatures, Document Frequency Score (DF) (Schilderand Kondadandi, 2008) and Sentence Position(SP).
DocSumm serves as a baseline to depict theeffect of novelty detection techniques describedin Section 3 on normal summarizers.
Documentfrequency (DF), of a word (w) in the document set(docs) is defined as ratio of number of documents inwhich it occured to the total number of documents.Normalized DF score of all content words in asentence is considered its feature score.DFdocs(w) ={|d| : w ?
d}|docs|Sentence Position (SP) assigns positional index (n)of a sentence (sn) in the document (d) it occurs asits feature score.
Training model will learn the opti-mum sentence position for the dataset.SP (snd) = nTacBaseline is a conventional baseline at TAC, thatcreates a n word length summary from first n wordsof the most recent article.
It provides a lower boundon what can be achieved with automatic multi docu-ment summarizers.3 Novelty DetectionProgressive summaries are generated at regular timeintervals to update user knowledge on a particularnews topic.
Imagine a set of articles published ona evolving news topic over time period T, with tdbeing publishing timestamp of article d. All the arti-cles published from time 0 to time t are assumed to14have been read previously, hence prior knowledge,pdocs.
Articles published in the interval t to T thatcontain new information are considered ndocs.ndocs = {d : td > t}pdocs = {d : td <= t}Progressive summarization needs a novelty detec-tion technique to identify sentences that contain rel-evant new information.
The task of detecting nov-elty can be carried out at 3 stages of summarizationshown in Figure 1.3.1 At ScoringNew Sentence scoring features are devised tocapture sentence novelty along with its relevance.Two features Novelty Factor (NF) (Varma et al,2009), and New Words (NW) are used at scoringlevel.Novelty Factor (NF)NF measures both topic relevancy of a sentenceand its novelty given prior knowledge of the userthrough pdocs.
NF score for a word w is calculatedas,NF (w) =|ndt||pdt|+ |ndocs|ndt = {d : w ?
d ?
d ?
ndocs}pdt = {d : w ?
d ?
d ?
pdocs}|ndt| captures the relevancy of w, and |pdt| elevatesthe novelty by penalizing words occurring fre-quently in pdocs.
Score of a sentence is the averageNF value of its content words.New Words (NW)Unlike NF, NW captures only novelty of a sentence.Novelty of a sentence is assessed by the amount ofnew words it contains.
Words that never occurredbefore in pdocs are considered new.
Normalizedterm frequency of a word (w) is used in calculatingfeature score of sentence.
Score of a sentence(s) isgiven by,Score(s) =?w?s NW (w)|s|NW (w) = 0 if w ?
pdocs= n/N elsen is frequency of w in ndocsN is total term frequency of ndocs3.2 At RankingRanked sentence set is re-ordered using MaximalMarginal relevance (Carbonell and Goldstein, 1998)criterion, such that prior knowledge is neglected andsentences with new information are promoted in theranked list.
Final rank (?Rank?)
of a sentence iscomputed as,Rank = relweight ?
rank ?(1?
relweight) ?
redundancy scoreWhere ?rank?
is the original sentence rank predictedby regression model as described in section 2, and?redundancy score?
is an estimate for the amountof prior information a sentence contains.
Parameter?relweight?
adjusts relevancy and novelty of asentence.
Two similarity measures ITSim, CoSimare used for calculating redundancy score.Information Theoretic Similarity (ITSim)According to information theory, Entropy quantifiesthe amount of information carried with a message.Extending this analogy to text content, EntropyI(w) of a word w is calculated as,I(w) = ?p(w) ?
log(p(w))p(w) = n/NMotivated by the information theoretic definition ofsimilarity by (Lin, 1998), we define similarity be-tween two sentences s1 and s2 as,ITSim(s1, s2) =2 ?
?w?s1?s2 I(w)?w?s1 I(w) +?w?s2 I(w)Numerator is proportional to the commonalitybetween s1 and s2 and denominator reflects differ-ences between them.Cosine Similarity (CoSim)Cosine similarity is a popular technique in TRECNovelty track to compute sentence similarity.Sentences are viewed as tf-idf vectors (Saltonand Buckley, 1987) of words they contain in a n-dimension space.
Similarity between two sentencesis measured as,CoSim(s1, s2) = cos(?)
=s1.s2|s1||s2|Average similarity value of a sentence with all sen-tences in pdocs is considered as its redundancyscore.153.3 At summary extractionNovelty Pool (NP)Sentences that possibly contain prior informationare filtered out from summary by creating NoveltyPool (NP), a pool of sentences containing one ormore novelwords.
Two sets of ?dominant?
wordsare generated one for each pdocs and ndocs.domndocs = {w : DFndocs(w) > threshold}dompdocs = {w : DFpdocs(w) > threshold}A word is considered dominant if it appears in morethan a predefined ?threshold ?
of articles, thus mea-suring its topic relevance.
Difference of the two domsets gives us a list of novelwords that are both rele-vant and new.novelwords = domndocs ?
dompdocs4 Experiments and ResultsWe conducted all the experiments on TAC 2009 Up-date Summarization dataset.
It consists of 48 topics,each having 20 documents divided into two clusters?A?
and ?B?
based on their chronological coverageof topic.
It serves as an ideal setting for evaluat-ing our progressive summaries.
Summary for clus-ter A (pdocs) is a normal multi document summarywhere as summary for cluster B (ndocs) is a Pro-gressive summary, both of length 100 words.
Eachtopic has associated 4 model summaries written byhuman assessors.
TAC 2008 Update summarizationdata that follow similar structure is used to buildtraining model for support vectors as mentioned inSection 2.
Thresholds for domndocs, dompdocs areset to 0.6, 0.3 respectively and relweight to 0.8 foroptimal results.Summaries are evaluated using ROUGE (Lin,2004), a recall oriented metric that automaticallyassess machine generated summaries based on theiroverlap with models.
ROUGE-2 and ROUGE-SU4are standard measures for automated summaryevaluation.
In Table 1 ROUGE scores of baselinesystems(Section 2.1) are presented.Five progressive runs are generated, each having anovelty detection scheme at either scoring, rankingor summary extraction stages.
ROUGE scores ofthese runs are presented in Table 2.ROUGE-2 ROUGE-SU4DocSumm 0.09346 0.13233TacBaseline 0.05865 0.09333Table 1: Average ROUGE-2, ROUGE-SU4 recall scoresof baselines for TAC 2009, cluster BNF+DocSumm : Sentence scoring is done with anadditional feature NF, along with default features ofDocSummNW+DocSumm : An additional feature NW isused to score sentences for DocSummITSim+DocSumm : ITSim is used for computingsimilarity between a sentence in ndocs and set of allsentences in pdocs.
Maximum similarity value isconsidered as redundancy score.
Re-ordered rankedlist is used for summary extractionCosim+DocSumm : CoSim is used as a similaritymeasure instead of ITSimNP+DocSumm : Only members of NP are consid-ered while extracting DocSumm summariesResults of top systems at TAC 2009, ICSI (Gillicket al, 2009) and THUSUM (Long et al, 2009) arealso provided for comparison.ROUGE-2 ROUGE-SU4ICSI 0.10417 0.13959NF+DocSumm 0.10273 0.13922NW+DocSumm 0.09645 0.13955NP+DocSumm 0.09873 0.13977THUSUM 0.09608 0.13499ITSim+DocSumm 0.09461 0.13306Cosim+DocSumm 0.08338 0.12607Table 2: Average ROUGE-2, ROUGE-SU4 recall scoresfor TAC 2009, cluster BNext level of experiments are carried out on combi-nation of these techniques.
Each run is produced bycombining two or more of the above(Section 3) de-scribed techniques in conjunction with DocSumm.Results of these runs are presented in table 3NF+NW : Both NF and NW are used for sentencescoring along with default features of DocSummNF+NW+ITSim : Sentences scored in NF+NW arere-ranked by their ITSim scoreNF+NW+NP : Only members of NP are selectedwhile extracting NF+NW summaries16NF+NW+ITSim+NP : Sentences are selected fromNP during extraction of NF+NW+ITSim summariesROUGE-2 ROUGE-SU4NF+NW 0.09807 0.14058NF+NW+ITSim 0.09704 0.13978NF+NW+NP 0.09875 0.14010{NP+NW+ITSim+NP} 0.09664 0.13812Table 3: Average ROUGE-2, ROUGE-SU4 recall scoresfor TAC 2009, cluster B5 Conclusion and DiscussionExperimental results prove that proposed NoveltyDetection techniques, particularly at scoring stageare very effective in the context of progressive sum-marization.
Both NF, a language modeling tech-nique and NW, a heuristic based feature are ableto capture relevant novelty successfully.
An ap-proximate 6% increase in ROUGE-2 and 3% in-crease in ROUGE-SU4 scores over DocSumm sup-port our argument.
Scores of NF+DocSumm andNW+DocSumm are comparable with existing bestapproaches.
Since CoSim is a word overlap mea-sure, and novel information is often embeddedwithin a sentence containing formerly known infor-mation, quality of progressive summaries declined.ITSim performs better than Cosim because it con-siders entropy of a word in similarity computations,which is a better estimate of information.
There is aneed for improved similarity measures that can cap-ture semantic relatedness between sentences.
Nov-elty pool (NP) is a simple filtering technique, thatimproved quality of progressive summaries by dis-carding probable redundant sentences into summary.From the results in Table 2, it can be hypothesizedthat Novelty is best captured at sentence scoringstage of summarization, rather than at ranking orsummary extraction.A slight improvement of ROUGE scores is ob-served in table 3, when novelty detection techniquesat scoring, ranking and extracting stages are com-bined together.
As Novel sentences are alreadyscored high through NF and NW, the effect of Re-Ranking and Filtering is not significant in the com-bination.The major contribution of this work is to iden-tify the possibility of novelty detection at differentstages of summarization.
Two new sentence scoringfeatures (NF and NW), a filtering strategy (NP), asentence similarity measure (ITSim) are introducedto capture relevant novelty.
Although proposed ap-proaches are simple, we hope that this novel treat-ment could inspire new methodologies in progres-sive summarization.
Nevertheless, the problem ofprogressive summarization is far from being solvedgiven the complexity involved in novelty detection.AcknowledgementsI would like to thank Dr. Vasudeva Varma at IIITHyderabad, for his support and guidance throughoutthis work.
I also thank Rahul Katragadda at YahooResearch and other anonymous reviewers, for theirvaluable suggestions and comments.ReferencesNasreen Abdul-Jaleel, James Allan, W. Bruce Croft, Fer-nando Diaz, Leah Larkey, and Xiaoyan Li.
2004.Umass at trec 2004: Novelty and hard.James Allan, Rahul Gupta, and Vikas Khandelwal.
2001.Temporal summaries of news topics.Jaime Carbonell and Jade Goldstein.
1998.
The use ofmmr, diversity-based reranking for reordering docu-ments and producing summaries.
In SIGIR ?98: Pro-ceedings of the 21st annual international ACM SIGIRconference on Research and development in informa-tion retrieval, pages 335?336, New York, NY, USA.ACM.David Eichmannac, Yi Zhangb, Shannon Bradshawbc,Xin Ying Qiub, Padmini Srinivasanabc, and AdityaKumar.
2004.
Novelty, question answering and ge-nomics: The university of iowa response.Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, BerndtBohnet, Yang Liu, and Shasha Xie.
2009.
The icsi/utdsummarization system at tac 2009.Adam Jatowt.
2004.
Web page summarization using dy-namic content.
In WWW Alt.
?04: Proceedings of the13th international World Wide Web conference on Al-ternate track papers and posters, pages 344?345, NewYork, NY, USA.
ACM.Sujian Li, Wei Wang, and Yongwei Zhang.
2009.
Tac2009 update summarization with unsupervised meth-ods.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In ICML ?98: Proceedings of the Fif-teenth International Conference on Machine Learn-17ing, pages 296?304, San Francisco, CA, USA.
MorganKaufmann Publishers Inc.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
pages 74?81, Barcelona,Spain, July.
Association for Computational Linguis-tics.Chong Long, Minlie Huang, and Xiaoyan Zhu.
2009.Tsinghua university at tac 2009: Summarizing multi-documents by information distance.Gerard Salton and Chris Buckley.
1987.
Term weight-ing approaches in automatic text retrieval.
Technicalreport, Ithaca, NY, USA.Barry Schiffman and Kathleen R. McKeown.
2004.Columbia university in the novelty track at trec 2004.Frank Schilder and Ravikumar Kondadandi.
2008.
Fast-sum: fast and accurate query-based multi-documentsummarization.
In Proceedings of the 46th AnnualMeeting of the Association for Computational Linguis-tics on Human Language Technologies.
Human Lan-guage Technology Conference.Ian Soboroff.
2004.
Overview of the trec 2004 noveltytrack.
National Institute of Standards and Technol-ogy,Gaithersburg, MD 20899.Simone Teufel and Marc Moens.
2002.
Summariz-ing scientific articles: experiments with relevance andrhetorical status.
Comput.
Linguist., 28(4):409?445.Vasudeva Varma, Praveen Bysani, Kranthi Reddy, VijayBharat, Santosh GSK, Karuna Kumar, Sudheer Kove-lamudi, Kiran Kumar N, and Nitin Maganti.
2009. iiithyderabad at tac 2009.
Technical report, Gaithersburg,Maryland USA.Jin Zhang, Pan Du, Hongbo Xu, and Xueqi Cheng.
2009.Ictgrasper at tac2009: Temporal preferred update sum-marization.18
