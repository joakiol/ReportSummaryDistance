Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 182?187,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsBayesian Word Alignment for Statistical Machine TranslationCos?kun Mermer1,21BILGEMTUBITAKGebze 41470 Kocaeli, Turkeycoskun@uekae.tubitak.gov.trMurat Sarac?lar22Electrical and Electronics Eng.
Dept.Bogazici UniversityBebek 34342 Istanbul, Turkeymurat.saraclar@boun.edu.trAbstractIn this work, we compare the translationperformance of word alignments obtainedvia Bayesian inference to those obtained viaexpectation-maximization (EM).
We proposea Gibbs sampler for fully Bayesian inferencein IBM Model 1, integrating over all possi-ble parameter values in finding the alignmentdistribution.
We show that Bayesian inferenceoutperforms EM in all of the tested languagepairs, domains and data set sizes, by up to 2.99BLEU points.
We also show that the proposedmethod effectively addresses the well-knownrare word problem in EM-estimated models;and at the same time induces a much smallerdictionary of bilingual word-pairs.1 IntroductionWord alignment is a crucial early step in the trainingof most statistical machine translation (SMT) sys-tems, in which the estimated alignments are used forconstraining the set of candidates in phrase/grammarextraction (Koehn et al, 2003; Chiang, 2007; Galleyet al, 2006).
State-of-the-art word alignment mod-els, such as IBM Models (Brown et al, 1993), HMM(Vogel et al, 1996), and the jointly-trained symmet-ric HMM (Liang et al, 2006), contain a large num-ber of parameters (e.g., word translation probabili-ties) that need to be estimated in addition to the de-sired hidden alignment variables.The most common method of inference in suchmodels is expectation-maximization (EM) (Demp-ster et al, 1977) or an approximation to EM whenexact EM is intractable.
However, being a maxi-mization (e.g., maximum likelihood (ML) or max-imum a posteriori (MAP)) technique, EM is gen-erally prone to local optima and overfitting.
Inessence, the alignment distribution obtained via EMtakes into account only the most likely point in theparameter space, but does not consider contributionsfrom other points.Problems with the standard EM estimation ofIBM Model 1 was pointed out by Moore (2004) anda number of heuristic changes to the estimation pro-cedure, such as smoothing the parameter estimates,were shown to reduce the alignment error rate, butthe effects on translation performance was not re-ported.
Zhao and Xing (2006) note that the param-eter estimation (for which they use variational EM)suffers from data sparsity and use symmetric Dirich-let priors, but they find the MAP solution.Bayesian inference, the approach in this paper,have recently been applied to several unsupervisedlearning problems in NLP (Goldwater and Griffiths,2007; Johnson et al, 2007) as well as to other tasksin SMT such as synchronous grammar induction(Blunsom et al, 2009) and learning phrase align-ments directly (DeNero et al, 2008).Word alignment learning problem was addressedjointly with segmentation learning in Xu et al(2008), Nguyen et al (2010), and Chung and Gildea(2009).
The former two works place nonparametricpriors (also known as cache models) on the param-eters and utilize Gibbs sampling.
However, align-ment inference in neither of these works is exactlyBayesian since the alignments are updated by run-ning GIZA++ (Xu et al, 2008) or by local maxi-mization (Nguyen et al, 2010).
On the other hand,182Chung and Gildea (2009) apply a sparse Dirichletprior on the multinomial parameters to prevent over-fitting.
They use variational Bayes for inference, butthey do not investigate the effect of Bayesian infer-ence to word alignment in isolation.
Recently, Zhaoand Gildea (2010) proposed fertility extensions toIBM Model 1 and HMM, but they do not place anyprior on the parameters and their inference method isactually stochastic EM (also known as Monte CarloEM), a ML technique in which sampling is used toapproximate the expected counts in the E-step.
Eventhough they report substantial reductions in align-ment error rate, the translation BLEU scores do notimprove.Our approach in this paper is fully Bayesian inwhich the alignment probabilities are inferred byintegrating over all possible parameter values as-suming an intuitive, sparse prior.
We develop aGibbs sampler for alignments under IBM Model 1,which is relevant for the state-of-the-art SMT sys-tems since: (1) Model 1 is used in bootstrappingthe parameter settings for EM training of higher-order alignment models, and (2) many state-of-the-art SMT systems use Model 1 translation probabil-ities as features in their log-linear model.
We eval-uate the inferred alignments in terms of the end-to-end translation performance, where we show the re-sults with a variety of input data to illustrate the gen-eral applicability of the proposed technique.
To ourknowledge, this is the first work to directly investi-gate the effects of Bayesian alignment inference ontranslation performance.2 Bayesian Inference with IBM Model 1Given a sentence-aligned parallel corpus (E,F), letei (fj) denote the i-th (j-th) source (target)1 wordin e (f ), which in turn consists of I (J) words anddenotes the s-th sentence in E (F).2 Each sourcesentence is also hypothesized to have an additionalimaginary ?null?
word e0.
Also let VE (VF ) denotethe size of the observed source (target) vocabulary.In Model 1 (Brown et al, 1993), each target word1We use the ?source?
and ?target?
labels following the gen-erative process, in which E generates F (cf.
Eq.
1).2Dependence of the sentence-level variables e, f , I , J (anda and n, which are introduced later) on the sentence index sshould be understood even though not explicitly indicated fornotational simplicity.fj is associated with a hidden alignment variable ajwhose value ranges over the word positions in thecorresponding source sentence.
The set of align-ments for a sentence (corpus) is denoted by a (A).The model parameters consist of a VE ?
VF ta-ble T of word translation probabilities such thatte,f = P (f |e).The joint distribution of the Model-1 variables isgiven by the following generative model3:P (E,F,A;T) =?sP (e)P (a|e)P (f |a, e;T) (1)=?sP (e)(I + 1)JJ?j=1teaj ,fj (2)In the proposed Bayesian setting, we treat T as arandom variable with a prior P (T).
To find a suit-able prior for T, we re-write (2) as:P (E,F,A|T) =?sP (e)(I + 1)JVE?e=1VF?f=1(te,f )ne,f (3)=VE?e=1VF?f=1(te,f )Ne,f?sP (e)(I + 1)J(4)where in (3) the count variable ne,f denotes thenumber of times the source word type e is alignedto the target word type f in the sentence-pair s, andin (4) Ne,f =?s ne,f .
Since the distribution over{te,f} in (4) is in the exponential family, specificallybeing a multinomial distribution, we choose the con-jugate prior, in this case the Dirichlet distribution,for computational convenience.For each source word type e, we assume the priordistribution for te = te,1 ?
?
?
te,VF , which is itselfa distribution over the target vocabulary, to be aDirichlet distribution (with its own set of hyperpa-rameters ?e = ?e,1 ?
?
?
?e,VF ) independent from thepriors of other source word types:te ?
Dirichlet(te;?e)fj |a, e,T ?
Multinomial(fj ; teaj )We choose symmetric Dirichlet priors identicallyfor all source words e with ?e,f = ?
= 0.0001 toobtain a sparse Dirichlet prior.
A sparse prior favors3We omit P (J |e) since both J and e are observed and sothis term does not affect the inference of hidden variables.183distributions that peak at a single target word andpenalizes flatter translation distributions, even forrare words.
This choice addresses the well-knownproblem in the IBM Models, and more severely inModel 1, in which rare words act as ?garbage col-lectors?
(Och and Ney, 2003) and get assigned ex-cessively large number of word alignments.Then we obtain the joint distribution of all (ob-served + hidden) variables as:P (E,F,A,T;?)
= P (T;?)
P (E,F,A|T) (5)where ?
= ?1 ?
?
?
?VE .To infer the posterior distribution of the align-ments, we use Gibbs sampling (Geman and Ge-man, 1984).
One possible method is to derive theGibbs sampler from P (E,F,A,T;?)
obtained in(5) and sample the unknowns A and T in turn, re-sulting in an explicit Gibbs sampler.
In this work,we marginalize out T by:P (E,F,A;?)
=?TP (E,F,A,T;?)
(6)and obtain a collapsed Gibbs sampler, which sam-ples only the alignment variables.Using P (E,F,A;?)
obtained in (6), the Gibbssampling formula for the individual alignments isderived as:4P (aj = i|E,F,A?j ;?
)=N?jei,fj + ?ei,fj?VFf=1N?jei,f+?VFf=1 ?ei,f(7)where the superscript ?j denotes the exclusion ofthe current value of aj .The algorithm is given in Table 1.
Initializationof A in Step 1 can be arbitrary, but for faster conver-gence special initializations have been used, e.g., us-ing the output of EM (Chiang et al, 2010).
Once theGibbs sampler is deemed to have converged after Bburn-in iterations, we collect M samples of A withL iterations in-between5 to estimate P (A|E,F).
Toobtain the Viterbi alignments, which are required forphrase extraction (Koehn et al, 2003), we select foreach aj the most frequent value in the M collectedsamples.4The derivation is quite standard and similar to otherDirichlet-multinomial Gibbs sampler derivations, e.g.
(Resnikand Hardisty, 2010).5A lag is introduced to reduce correlation between samples.Input: E, F; Output: K samples of A1 Initialize A2 for k = 1 to K do3 for each sentence-pair s in (E,F) do4 for j = 1 to J do5 for i = 0 to I do6 Calculate P (aj = i| ?
?
?
)according to (7)7 Sample a new value for ajTable 1: Gibbs sampling algorithm for IBM Model 1 (im-plemented in the accompanying software).3 Experimental SetupFor Turkish?English experiments, we used the20K-sentence travel domain BTEC dataset (Kikuiet al, 2006) from the yearly IWSLT evaluations6for training, the CSTAR 2003 test set for develop-ment, and the IWSLT 2004 test set for testing7.
ForCzech?English, we used the 95K-sentence newscommentary parallel corpus from the WMT sharedtask8 for training, news2008 set for development,news2009 set for testing, and the 438M-word En-glish and 81.7M-word Czech monolingual news cor-pora for additional language model (LM) training.For Arabic?English, we used the 65K-sentenceLDC2004T18 (news from 2001-2004) for training,the AFP portion of LDC2004T17 (news from 1998,single reference) for development and testing (about875 sentences each), and the 298M-word Englishand 215M-word Arabic AFP and Xinhua subsets ofthe respective Gigaword corpora (LDC2007T07 andLDC2007T40) for additional LM training.
All lan-guage models are 4-gram in the travel domain exper-iments and 5-gram in the news domain experiments.For each language pair, we trained standardphrase-based SMT systems in both directions (in-cluding alignment symmetrization and log-linearmodel tuning) using Moses (Koehn et al, 2007),SRILM (Stolcke, 2002), and ZMERT (Zaidan,2009) tools and evaluated using BLEU (Papineni etal., 2002).
To obtain word alignments, we used theaccompanying Perl code for Bayesian inference and6International Workshop on Spoken Language Translation.http://iwslt2010.fbk.eu7Using only the first English reference for symmetry.8Workshop on Machine Translation.http://www.statmt.org/wmt10/translation-task.html184Method TE ET CE EC AE EAEM-5 38.91 26.52 14.62 10.07 15.50 15.17EM-80 39.19 26.47 14.95 10.69 15.66 15.02GS-N 41.14 27.55 14.99 10.85 14.64 15.89GS-5 40.63 27.24 15.45 10.57 16.41 15.82GS-80 41.78 29.51 15.01 10.68 15.92 16.02M4 39.94 27.47 15.47 11.15 16.46 15.43Table 2: BLEU scores in translation experiments.
E: En-glish, T: Turkish, C: Czech, A: Arabic.GIZA++ (Och and Ney, 2003) for EM.For each translation task, we report two EM es-timates, obtained after 5 and 80 iterations (EM-5and EM-80), respectively; and three Gibbs samplingestimates, two of which were initialized with thosetwo EM Viterbi alignments (GS-5 and GS-80) and athird was initialized naively9 (GS-N).
Sampling set-tings were B = 400 for T?E, 4000 for C?E and8000 for A?E; M = 100, and L = 10.
For refer-ence, we also report the results with IBM Model 4alignments (M4) trained in the standard bootstrap-ping regimen of 15H53343.4 ResultsTable 2 compares the BLEU scores of Bayesian in-ference and EM estimation.
In all translation tasks,Bayesian inference outperforms EM.
The improve-ment range is from 2.59 (in Turkish-to-English)up to 2.99 (in English-to-Turkish) BLEU points intravel domain and from 0.16 (in English-to-Czech)up to 0.85 (in English-to-Arabic) BLEU points innews domain.
Compared to the state-of-the-art IBMModel 4, the Bayesian Model 1 is better in all traveldomain tasks and is comparable or better in the newsdomain.Fertility of a source word is defined as the num-ber of target words aligned to it.
Table 3 shows thedistribution of fertilities in alignments obtained fromdifferent methods.
Compared to EM estimation, in-cluding Model 4, the proposed Bayesian inferencedramatically reduces ?questionable?
high-fertility (4?
fertility?
7) alignments and almost entirely elim-9Each target word was aligned to the source candidate thatco-occured the most number of times with that target word inthe entire parallel corpus.Method TE ET CE EC AE EAAll 140K 183K 1.63M 1.78M 1.49M 1.82MEM-80 5.07K 2.91K 52.9K 45.0K 69.1K 29.4KM4 5.35K 3.10K 36.8K 36.6K 55.6K 36.5KGS-80 755 419 14.0K 10.9K 47.6K 18.7KEM-80 426 227 10.5K 18.6K 21.4K 24.2KM4 81 163 2.57K 10.6K 9.85K 21.8KGS-80 1 1 39 110 689 525EM-80 24 24 28 30 44 46M4 9 9 9 9 9 9GS-80 8 8 13 18 20 19Table 3: Distribution of inferred alignment fertilities.
Thefour blocks of rows from top to bottom correspond to (inorder) the total number of source tokens, source tokenswith fertilities in the range 4?7, source tokens with fertil-ities higher than 7, and the maximum observed fertility.The first language listed is the source in alignment (Sec-tion 2).Method TE ET CE EC AE EAEM-80 52.5K 38.5K 440K 461K 383K 388KM4 57.6K 40.5K 439K 441K 422K 405KGS-80 23.5K 25.4K 180K 209K 158K 176KTable 4: Sizes of bilingual dictionaries induced by differ-ent alignment methods.inates ?excessive?
alignments (fertility ?
8)10.The number of distinct word-pairs induced by analignment has been recently proposed as an objec-tive function for word alignment (Bodrumlu et al,2009).
Small dictionary sizes are preferred overlarge ones.
Table 4 shows that the proposed in-ference method substantially reduces the alignmentdictionary size, in most cases by more than 50%.5 ConclusionWe developed a Gibbs sampling-based Bayesian in-ference method for IBM Model 1 word alignmentsand showed that it outperforms EM estimation interms of translation BLEU scores across several lan-guage pairs, data sizes and domains.
As a resultof this increase, Bayesian Model 1 alignments per-form close to or better than the state-of-the-art IBM10The GIZA++ implementation of Model 4 artificially limitsfertility parameter values to at most nine.185Model 4.
The proposed method learns a compact,sparse translation distribution, overcoming the well-known ?garbage collection?
problem of rare wordsin EM-estimated current models.AcknowledgmentsMurat Sarac?lar is supported by the TU?BA-GEBI?Paward.ReferencesPhil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A Gibbs sampler for phrasal syn-chronous grammar induction.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages782?790, Suntec, Singapore, August.Tugba Bodrumlu, Kevin Knight, and Sujith Ravi.
2009.A new objective function for word alignment.
In Pro-ceedings of the NAACL HLT Workshop on Integer Lin-ear Programming for Natural Language Processing,pages 28?35, Boulder, Colorado, June.
Association forComputational Linguistics.Peter F. Brown, Vincent J. Della Pietra, Stephen A.Della Pietra, and Robert L. Mercer.
1993.
The mathe-matics of statistical machine translation: parameter es-timation.
Computational Linguistics, 19(2):263?311.David Chiang, Jonathan Graehl, Kevin Knight, AdamPauls, and Sujith Ravi.
2010.
Bayesian inferencefor finite-state transducers.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 447?455, Los Angeles, Cali-fornia, June.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Tagyoung Chung and Daniel Gildea.
2009.
Unsuper-vised tokenization for machine translation.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing, pages 718?726,Singapore, August.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.
Max-imum likelihood from incomplete data via the EM al-gorithm.
Journal of the Royal Statistical Society, Se-ries B, 39(1):1?38.John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.2008.
Sampling alignment structure under a Bayesiantranslation model.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 314?323, Honolulu, Hawaii, October.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of the As-sociation for Computational Linguistics, pages 961?968, Sydney, Australia, July.Stuart Geman and Donald Geman.
1984.
Stochastic re-laxation, Gibbs distributions, and the Bayesian restora-tion of images.
IEEE Transactions On Pattern Analy-sis And Machine Intelligence, 6(6):721?741, Novem-ber.Sharon Goldwater and Tom Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speech tag-ging.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 744?751, Prague, Czech Republic, June.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007.
Bayesian inference for PCFGs via Markovchain Monte Carlo.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 139?146, Rochester, New York, April.Genichiro Kikui, Seiichi Yamamoto, ToshiyukiTakezawa, and Eiichiro Sumita.
2006.
Com-parative study on corpora for speech translation.IEEE Transactions on Audio, Speech and LanguageProcessing, 14(5):1674?1682.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of HLT-NAACL 2003, Main Papers, pages 48?54,Edmonton, May-June.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics Companion Volume Pro-ceedings of the Demo and Poster Sessions, pages 177?180, Prague, Czech Republic, June.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the HumanLanguage Technology Conference of the NAACL, MainConference, pages 104?111, New York City, USA,June.Robert C. Moore.
2004.
Improving IBM word alignmentModel 1.
In Proceedings of the 42nd Meeting of theAssociation for Computational Linguistics (ACL?04),Main Volume, pages 518?525, Barcelona, Spain, July.ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.2010.
Nonparametric word segmentation for ma-186chine translation.
In Proceedings of the 23rd Interna-tional Conference on Computational Linguistics (Col-ing 2010), pages 815?823, Beijing, China, August.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.Philip Resnik and Eric Hardisty.
2010.
Gibbs samplingfor the uninitiated.
University of Maryland ComputerScience Department; CS-TR-4956, June.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Seventh International Conferenceon Spoken Language Processing, volume 3.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In COLING, pages 836?841.Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-mann Ney.
2008.
Bayesian semi-supervised Chineseword segmentation for statistical machine translation.In Proceedings of the 22nd International Conferenceon Computational Linguistics (Coling 2008), pages1017?10124, Manchester, UK, August.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91(1):79?88.Shaojun Zhao and Daniel Gildea.
2010.
A fast fertil-ity hidden Markov model for word alignment usingMCMC.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 596?605, Cambridge, MA, October.Bing Zhao and Eric P. Xing.
2006.
BiTAM: Bilingualtopic admixture models for word alignment.
In Pro-ceedings of the COLING/ACL 2006 Main ConferencePoster Sessions, pages 969?976, Sydney, Australia,July.
Association for Computational Linguistics.187
