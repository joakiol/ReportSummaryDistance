Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 59?68,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsOn The Applicability of Readability Models to Web TextsSowmya Vajjala Detmar MeurersSeminar fu?r SprachwissenschaftUniversita?t Tu?bingen{sowmya,dm}@sfs.uni-tuebingen.deAbstractAn increasing range of features is beingused for automatic readability classifica-tion.
The impact of the features typicallyis evaluated using reference corpora con-taining graded reading material.
But howdo the readability models and the featuresthey are based on perform on real-worldweb texts?
In this paper, we want to take astep towards understanding this aspect onthe basis of a broad range of lexical andsyntactic features and several web datasetswe collected.Applying our models to web search re-sults, we find that the average reading levelof the retrieved web documents is rela-tively high.
At the same time, documentsat a wide range of reading levels are iden-tified and even among the Top-10 searchresults one finds documents at the lowerlevels, supporting the potential usefulnessof readability ranking for the web.
Finally,we report on generalization experimentsshowing that the features we used gener-alize well across different web sources.1 IntroductionThe web is a vast source of information on a broadrange of topics.
While modern search enginesmake use of a range of features for identifying andranking search results, the question whether a webpage presents its information in a form that is ac-cessible to a given reader is only starting to receiveattention.
Researching the use of readability as-sessment as a ranking parameter for web searchcan be a relevant step in that direction.Readability assessment has a long history span-ning various fields of research from EducationalPsychology to Computer Science.
At the sametime, the question which features generalize to dif-ferent types of documents and whether the read-ability models are appropriate for real-life appli-cations has only received little attention.Against this backdrop, we want to see how wella state-of-the-art readability assessment approachusing a broad range of features performs when ap-plied to web data.
Based on the approach intro-duced in Vajjala and Meurers (2012), we thus setout to explore the following two questions in thispaper:?
Which reading levels can be identified in asystematic sample of web texts??
How well do the features used generalize todifferent web sources?The paper is organized as follows: Section 2surveys related work.
Section 3 introduces the cor-pus and the features we used.
Section 4 describesour readability models.
Section 5 discusses our ex-periments investigating the applicability of thesemodels to web texts.
Section 6 reports on a secondset of experiments conducted to test the generaliz-ability of the features used.
Section 7 concludesthe paper with a discussion of our results.2 Related Work2.1 Readability AssessmentThe need for assessing the readability of a pieceof text has been explored in the educational re-search community for over eight decades.
DuBay(2006) provides an overview of early readabilityformulae, which were based on relatively shallowfeatures and wordlists.
Some of the formulae arestill being used in practice, as exemplified by theFlesch-Kincaid Grade Level (Kincaid et al 1975)available in Microsoft Word.More recent computational linguistic ap-proaches view readability assessment as a59classification problem and explore differenttypes of features.
Statistical language modelinghas been a popular approach (Si and Callan,2001; Collins-Thompson and Callan, 2004),with the hypothesis that the word usage patternsacross grade levels are distinctive enough.
Heil-man et.
al.
(2007; 2008) extended this approachby combining language models with manuallyand automatically extracted grammatical features.The relation of text coherence and cohesionto readability is well explored in the CohMetrixproject (McNamara et al 2002).
Ma et al(2012a;2012b) approached readability assessment as aranking problem and also compared human versusautomatic feature extraction for the task of label-ing children?s literature.The WeeklyReader1, an American educationalnewspaper with graded readers has been a pop-ular source of data for readability classificationresearch in the recent past.
Petersen and Osten-dorf (2009), Feng et al(2009) and Feng (2010)used it to build readability models with a rangeof lexical, syntactic, language modeling and dis-course features.
In Vajjala and Meurers (2012)we created a larger corpus, WeeBit, by combiningWeeklyReader with graded reading material fromthe BBCBitesize website.2 We adapted measuresof lexical richness and syntactic complexity fromSecond Language Acquisition (SLA) research asfeatures for readability classification and showedthat such measures of proficiency can successfullybe used as features for readability assessment.2.2 Readability Assessment of Web TextsDespite the significant body of research on read-ability assessment, applying it to retrieve relevanttexts from the web has elicited interest only in therecent past.
While Benno?hr (2005) and Newboldet al(2010) created new readability formulae forthis purpose, Ott and Meurers (2010) and Tan etal.
(2012) used existing readability formulae to fil-ter search engine results.
The READ-X project(Miltsakaki and Troutt, 2008; Miltsakaki, 2009)combined standard readability formulae with topicclassification to retrieve relevant texts for users.The REAP Project3 supports the lexical acqui-sition of individual learners by retrieving texts thatsuit a given learner level.
Kidwell et al(2011) also1http://weeklyreader.com2http://www.bbc.co.uk/bitesize3http://reap.cs.cmu.eduused a word-acquisition model for readability pre-diction.
Collins-Thompson et al(2011) and Kimet al(2012) employed word distribution basedreadability models for personalized search and forcreating entity profiles respectively.
Nakatani etal.
(2010) followed a language modeling approachto rank search results to take user comprehensioninto account.
Google also has an option to filtersearch results based on reading level, apparentlyusing a language modeling approach.4 Kanungoand Orr (2009) used search result snippet basedfeatures to predict the readability of short web-summaries.All the above approaches primarily restrictthemselves to traditional formulae or statisticallanguage models encoding the distribution ofwords.
The effect of lexical and syntactic featuresas used in recent research on readability thus re-mains to be studied in a web context.
Furthermore,the generalizability of the features used to otherdata sets also remains to be explored.
These arethe primary issues we address in this paper.3 Corpus and FeaturesLet us turn to answering our first question: Whichreading levels can be identified in a systematicsample of web texts?
To address this question, wefirst need to introduce the features we used, thegraded corpus we used to train the model, and thenature of the readability model.Since the goal of this paper is not to presentnew features but to explore the application of areadability approach to the web, we here simplyadopt the feature and corpus setup introduced inVajjala and Meurers (2012).
The WeeBit corpusused is a corpus of texts belonging to five readinglevels, corresponding to children of age group 7?16 years.
It consists of 625 documents per readinglevel.
The articles cover a range of fiction and non-fiction topics.
Each article is labeled as belong-ing to one of five reading levels: Level 2, Level 3,Level 4, KS3 and GCSE.We adapted both the lexical and syntactic fea-tures of Vajjala and Meurers (2012) to build read-ability models on the basis of the WeeBit corpusand then studied their applicability to real-worlddocuments retrieved from the web as well as theapplicability of those features across different websources.4http://goo.gl/aVy9360Lexical features (LEXFEATURES) The lexicalfeatures are motivated by the lexical richness mea-sures used to estimate the quality of languagelearners?
oral narratives (Lu, 2012).
We includedseveral type-token ratio variants used in SLA re-search: generic type token ratio, root TTR, cor-rected TTR, bilogarithmic TTR and Uber Index.In addition, there are lexical variation measuresused to estimate the distribution of various partsof speech in the given text.
They include thenoun variation, adjective variation, modifier vari-ation, adverb variation and verb variation, whichrepresent the proportion of words of the respec-tive part of speech categories compared to all lex-ical words in the document.
Alternative measuresfor verb variation, namely, Squared Verb Variationand Corrected Verb Variation are also included.Apart from these, we also added the traditionallyused measures of average number of charactersper word, average number of syllables per word,and two readability formulae, the Flesch-Kincaidscore (Kincaid et al 1975) and the Coleman-Liauscore (Coleman and Liau, 1975).
Finally, we in-cluded the percentage of words from the Aca-demic Word List5.
It is a list created by Coxhead(2000) which consists of words that are more com-monly found in academic texts.Syntactic features (SYNFEATURES) Thesefeatures are adapted from the syntactic complexitymeasures used to analyze second language writing(Lu, 2010).
They are calculated based on theparser output of the BerkeleyParser (Petrov andKlein, 2007), using the Tregex (Levy and Andrew,2006) pattern matcher.
They include: meanlengths of various production units (sentence,clause and t-unit); clauses per sentence and t-unit;t-units per sentence; complex-t units per t-unitand per sentence; dependent clauses per clause,t-unit and sentence; co-ordinate phrases perclause, t-unit and sentence; complex nominals perclause and t-unit; noun phrases, verb phrases andpreposition phrases per sentence; average lengthof NP, VP and PP; verb phrases per t-unit; SBARsper sentence and average parse tree height.We refer to the feature subset containing allthe traditionally used features (# char.
per word,# syll.
per word and # words per sentence) asTRADFEATURES in this paper.5http://simple.wiktionary.org/wiki/Wiktionary:Academic_word_list4 The Readability ModelIn computational linguistics, readability assess-ment is generally approached as a classificationproblem.
To our knowledge, only Heilman et al(2008) and Ma et al(2012a) experimented withother kinds of statistical models.We approach readability assessment as a regres-sion problem.
This produces a model which pro-vides a continuous estimate of the reading level,enabling us to see if there are documents that fallbetween two levels or above the maximal levelfound in the training data.
We used the WEKAimplementation of linear regression for this pur-pose.
Since linear regression assumes that the datafalls on an interval scale with evenly spaced read-ing levels, we used numeric values from 1?5 asreading levels instead of the original class namesin the WeeBit corpus.
Table 1 shows the mappingfrom WeeBit classes to numeric values, along withthe age groups per class.WeeBit class Age (years) Reading levelLevel 2 7?8 1Level 3 8?9 2Level 4 9?10 3KS3 11?14 4GCSE 14?16 5Table 1: WeeBit Reading Levels for RegressionWe report Pearson?s correlation coefficient andRoot Mean Square Error (RMSE) as our evalua-tion metrics.
Correlation coefficient measures theextent of linear relationship between two randomvariables.
In readability assessment, a high corre-lation indicates that the texts at a higher difficultylevel are more likely to receive a higher level pre-diction from the model and those at lower diffi-culty level would more likely receive a lower pre-diction.
RMSE can be interpreted as the aver-age deviation in grade levels between the predictedand the actual values.We trained four regression models with the fea-ture subsets introduced in section 3: LEXFEA-TURES, SYNFEATURES, TRADFEATURES andALLFEATURES.
While the criterion used in cre-ating the graded texts in WeeBit is not known, itis likely that they were created with the traditionalmeasures in mind.
Indeed, the traditional featuresalso were among the most predictive features inVajjala and Meurers (2012).
Hence, apart from61training the above mentioned four regression mod-els, we also trained a fifth model excluding the tra-ditional features and formulae.
This experimentwas performed to verify if the traditional featuresare creating a skewed model that relies too heavilyon those well-known and thus easily manipulatedfeatures in making decisions on test data.
We referto this fifth feature group as NOTRAD.Table 2 shows the result of our regression ex-periments using 10-fold cross-validation on theWeeBit corpus, employing the different featuresubsets and the complete feature set.Feature Set # Features Corr.
RMSELEXFEATURES 17 0.84 0.78SYNFEATURES 25 0.88 0.64TRADFEATURES 3 0.66 1.06ALLFEATURES 42 0.92 0.54NOTRAD 37 0.89 0.63Table 2: Linear Regression Results for WeeBitThe best correlation of 0.92 was achieved withthe complete feature set.
0.92 is considered astrong correlation and coupled with an RMSE of0.54, we can conclude that our regression modelis a good model.
In comparison, in Vajjala andMeurers (2012), where we tackle readability as-sessment as a classification problem, we obtained93.3% accuracy on this dataset using all features.Looking at the feature subsets, there also is agood correlation between the model predictionsand the actual results in the other cases, exceptfor the model considering only traditional features.While traditional features often are among themost predictive features in readability research,we also found that a model which does not includethem can perform at a comparable level (0.89).Comparing these results with previous researchusing regression modeling for readability assess-ment is not particularly meaningful because of thedifferences in the corpus and the levels used.
Forexample, while Heilman et al(2008) used a cor-pus of 289 texts across 12 reading levels achievinga correlation of 0.77, we used the WeeBit corpuscontaining 3125 texts across 5 reading levels.6We took the two best models of Table 2,MODALL using ALLFEATURES and MODNO-TRAD using the NOTRAD feature set, and set outto answer our first guiding question, about the6Direct comparisons on the same data set would be mostindicative, but many datasets, such as the corpus used in Heil-man et al(2008), are not accessible due to copyright issues.reading levels which such models can identify in asystematic sample of web texts.5 Applying readability models to web textsTo investigate the effect of the two readabilitymodels for real-world web texts, we studied theirperformance on two types of web data:?
web documents we crawled from specificweb sites that offer the same type of materialfor two groups of readers differing in theirreading skills?
web documents identified by a web searchengine for a sample of web queries selectedfrom a public query log5.1 Readability of web data drawn fromcharacteristic web sites5.1.1 Web test sets usedFollowing the approach of Collins-Thompson andCallan (2005) and Sato et al(2008), who eval-uated readability models using independent web-based test sets, we compiled three sets of web doc-uments that given their origin can be classified intotwo classes each:Wiki ?
SimpleWiki: Wikipedia7, along with itsmanually simplified version Simple Wikipedia8 isincreasingly used in two-class readability classi-fication tasks and text simplification approaches(Napoles and Dredze, 2010; Zhu et al 2010;Coster and Kauchak, 2011).
We use a collectionof 2000 randomly selected parallel articles fromeach of the two websites, which in the followingis referred to as WIKI and SIMPLEWIKI.Time ?
Time for Kids: Time for Kids9 is a divi-sion of the TIME magazine10, which produces ar-ticles exclusively for children and is used widelyin classrooms.
We took a sample of 2000 docu-ments each from Time and from Time for Kids forour experiments and refer them TIME and TFK.NormalNews ?
ChildrensNews: We crawledwebsites that contain news articles written for chil-dren (e.g., http://www.firstnews.co.uk) andcategorized them as CHILDRENSNEWS.
We alsocrawled freely accessible articles from popularnews websites such as BBC or The Guardian and7http://en.wikipedia.org8http://simple.wikipedia.org9http://www.timeforkids.com10http://www.time.com62categorized them as NORMALNEWS.
We took10K documents from each of these two categoriesfor our experiments.These three corpus pairs collected as test casesdiffer in several aspects.
For example, Sim-pleWikipedia is not targeting children as such,whereas Time for Kids and ChildrensNews are.And SimpleWikipedia ?
Wikipedia covers paral-lel articles in two versions, whereas this is notthe case for the the two Time and the two Newscorpora.
However, as far as we see these differ-ences are orthogonal to the issue we are research-ing here, namely their use as real-life test cases tostudy the effect of the classification model learnedon the WeeBit data.We applied the two regression models whichhad performed best on the WeeBit corpus (cf.
Ta-ble 2 in section 4) to these web datasets.
The aver-age reading levels of the different datasets accord-ing to these two models are reported in Table 3.Data Set MODALL MODNOTRADSIMPLEWIKI 3.86 2.67TFK 4.15 2.72CHILDRENSNEWS 4.19 2.39WIKI 4.21 3.33TIME 5.04 4.07NORMALNEWS 5.58 4.42Table 3: Applying the WeeBit regression model tothe six web datasetsThe table shows that both MODALL and MOD-NOTRAD place the documents from the childrenwebsites (SIMPLEWIKI, TFK and CHILDREN-SNEWS) at lower reading levels than those from01020304050601 2 3 4 5 higher% of documents belongingto areadinglevelReading levelDistribution of reading levels across web texts with traditional featuresSimpleWikiTFKChildrensNewsWikiTimeNormalNewsFigure 1: Reading levels assigned by MODALLthe regular websites for adults (TIME, WIKI andNORMALNEWS).
However, there is an interestingdifference in the predictions made by the two mod-els.
The MODALL model including the traditionalfeatures consistently assigns a higher reading levelto all the documents, and it also fails to separateCHILDRENSNEWS (4.19) from WIKI (4.20).To be able to inspect this in detail, we plot-ted the class-wise reading level distribution of ourregression models.
Figure 1 shows the distribu-tion of reading levels for these web datasets usingMODALL.
As we already knew from the averages,the model assigns somewhat higher reading levelsto all documents, and the figure confirms that thetexts for children (SIMPLEWIKI, TFK and CHIL-DRENSNEWS) are only marginally distinguishedfrom the corresponding websites targeting adultreaders (TIME,WIKI and NORMALNEWS).
TheNORMALNEWS dataset al seems to be placedin a much higher distribution compared to all theother test sets, with more than 50% of the docu-ments getting a prediction of ?higher?
(the labelused for documents placed at level 6 or higher).Figure 2 shows the distribution of reading levelsacross the test sets according to MODNOTRAD,the model without traditional features.
The modelprovides a broader coverage across all reading lev-els, with documents from children web sites andSimpleWikipedia clearly being placed at the lowerend of the spectrum and web pages targeting adultsat the higher end.
NORMALNEWS documents areagain placed the highest, but less than 10% falloutside the range established by WeeBit.
TIMEshows the highest diversity, with around 20% foreach reading level above the lowest one.0510152025303540451 2 3 4 5 higher% of documents belongingto areadinglevelReading levelDistribution of reading levels across web texts without traditional featuresSimpleWikiTFKChildrensNewsWikiTimeNormalNewsFigure 2: Reading levels using MODNOTRAD63The first set of experiments shows that thereadability models which were successful on theWeeBit reference corpus seem to be able to iden-tify a corresponding broad range among web doc-uments that we selected top-down by relying onprototypical websites targeting ?adult?
and ?child?readers, which are likely to feature more difficultand easier web documents, respectively.
Whilewe cannot evaluate the difference between the twomodels quantitatively, given the lack of an externalgold standard classification of the crawled data,the MODNOTRAD conceptually seems to do a bet-ter job at distinguishing the two classes of web-sites in line with the top-down expectations.5.2 Readability of search resultsComplementing the first set of experiments, estab-lishing that the readability models are capable ofplacing web documents in line with the top-downclassification of the sites they originate from, inthe second set of experiments we want to investi-gate bottom-up whether for some random topics ofinterest, the web offers texts at different readabil-ity levels.
This also is of practical relevance, sinceranking web search results by readability is onlyuseful if there actually are documents at differentreading levels for a given query.For this investigation, we took the MOD-NOTRAD model and used it to estimate thereading level of web search results.
Forweb searching, we used the BING searchAPI (http://datamarket.azure.com/dataset/bing/search) and computed the reading levelsof the Top-100 search results for a sample of 50test queries, selected from a publicly accessibledatabase (Lu and Callan, 2003).Figure 3 characterizes the data obtained throughthe web searches in terms of the percentage of doc-Figure 3: Documents retrieved per reading leveluments belonging to a given reading level, accord-ing to the MODNOTRAD model.
In the Top-100search results obtained for each of the 50 queries,the model identifies documents at all reading lev-els, with a peak at reading level 4 (correspondingto KS3 in the original WeeBit dataset).To determine how much individual queries dif-fer in terms of the readability of the documentsthey retrieve, we also looked at the results for eachquery separately.
Figure 4 shows the mean read-ing level of the Top-100 results for each of the 50search queries.
From query to query, the aver-age readability of the documents retrieved seemsto differ relatively little, with most results fallinginto the higher reading levels (4 or above).Figure 4: Average reading level of search resultsReturning to the question whether there aredocuments of different reading levels for a givenquery, we need to check how much variation existsaround the observed, rather similar averages.
Ta-ble 4 provides the individual reading levels of theTop-10 search results for a sample of 10 queriesfrom our experiment, along with the average read-ing level of the Top-100 results for that query.
Theresults in Table 4 indicate that indeed there aredocuments at a broad range of reading levels evenamong the most relevant search results returned bythe BING web search engine.Looking at the individual query results, wefound that although a lot of news documentstended towards a higher reading level, it is in-deed possible to find some texts at lower read-ing levels even within Top-10 results (indicated inbold).
However, we found that even for queriesthat we would expect to result in hits from web-sites targeting child readers, those sites often didnot make it into the Top-10 results.
The same wastrue for sites offering ?simple?
language, such asSimple Wikipedia, which was not among the top64Result Rank?
1 2 3 4 5 6 7 8 9 10 AvgTop100Querylocal anaesthetic 3.18 4.57 5.35 3.09 4.24 4.6 3.95 4.74 2.72 4.73 3.78copyright copy law 1.77 4.59 1.43 2.67 4.63 6.2 2.69 1.1 3.87 5.61 4.57halley comet 1.69 4.47 4.54 4.24 2.37 4.1 4.86 3.56 4.21 3.56 4.04public offer 4.4 4.35 5.06 5.03 4.36 5.16 4.13 4.67 3.81 1.1 4.39optic sensor 2.67 3.38 4.5 3.17 2.54 4.19 4.84 1.47 2.2 3.31 3.83europe union politics 3.61 4.9 6.3 4.02 2.17 4.5 1.47 1.58 4.88 6.33 4.33presidential poll 4.98 5.38 1.77 6.1 4.76 3.82 1.05 5.11 3.92 4.25 3.95shakespeare 2.39 2.9 4.2 4.74 4.76 3.89 1.47 2.13 2.6 4.06 3.58air pollution 1.17 4.93 3.7 2.3 4.36 3.73 3.71 3.49 2.22 2.67 4.21euclidean geometry 3.88 4.71 4.7 4.3 4.45 4.63 4.04 4.1 3.48 2.58 3.18Table 4: Reading levels of individual search resultsresults even when it contained pages directly rel-evant to the query.
To provide access to thosepages, reranking the search results based on read-ability would thus be of value.While we do not want to jump to conclusionsbased on our sample of 50 queries, the resultsof our experiments seem to support the idea thatreadability-based re-ranking of web search resultscan help users in accessing web documents thatalso are at the right level for the given user.
Re-turning to the first overall question that lead ushere, our experiments support the answer that in-deed there are documents spread across differentreading levels on the web with a tendency towardshigher reading levels.6 Generalizability of the Feature SetWe can now turn to the second question raised inthe introduction: How well do the features gener-alize across different classes of web documents?We saw in section 5.1 that the predictions of thetwo models we used varied quite a bit, solelybased on whether the traditional readability fea-tures were included in the model or not.
This con-firms the need to investigate how generally appli-cable which types of features are across datasets.As far as we know, such an experiment vali-dating the generalizability of features was not yetperformed in this domain.
As there are no pub-licly available graded web datasets to build newreadability models with the same feature set, weused the datasets we introduced in section 5.1.1 forcreating two-class readability classification mod-els.
Since there are no clear age-group annota-tions with all these datasets, we decided to use abroad two-level classification instead of more finegrained grade levels.The difference between this experiment and theprevious one lies in the primary question it at-tempts to answer.
Here, the focus is on veri-fying if the features are capable of building ac-curate classification models on different trainingsets.
In the previous experiment, it was on check-ing if a given classification model (which in thatexperiment was trained on the WeeBit corpus) cansuccessfully discriminate reading levels for docu-ments from various real-world texts.We observed in Section 5.1 that with traditionalfeatures, the WeeBit based readability model as-signed higher reading levels to all the documentsfrom our web datasets.
So, it would perhaps bea natural step to train these binary classificationmodels excluding the traditional features.
How-ever, the traditional features may still be useful(with different weights) for constructing classifi-cation models with other training data.
So, wetrained two sets of models per training set ?
onewith ALLFEATURES and another excluding tradi-tional features (NOTRAD).We trained binary classification models usingthe following training sets:?
TIME ?
TFK texts?
WIKI ?
SIMPLEWIKI texts?
NORMALNEWS ?
KIDSNEWS texts?
TIME+WIKI ?
TFK+SIMPLEWIKI textsWe used the Sequential Minimal Optimization(SMO) algorithm implementation in the WEKAtool kit to train these classifiers.
The choice ofthe algorithm here was motivated by the fact thattraining is quick and that SMO has successfully65been used in previous research on readability as-sessment (Feng, 2010; Hancke et al 2012).Table 5 summarizes the classification accura-cies obtained with the four models using 10-foldcross validation for the four web corpora.Training Set Accuracy-All Accuracy-NoTradTIME ?
TFK 95.11% 89.52%WIKI ?
SIMPLEWIKI 92.32% 88.81%NORMALNEWS ?
KIDSNEWS 97.93% 92.54%TIME+WIKI ?
TFK+SIMPLEWIKI 93.38% 89.72%Table 5: Cross-validation accuracies for binaryclassification on different web corporaThe results in the table show that the same setof features consistently result in creating accu-rate classification models for all four web corpora.Each of the two-class classification models per-formed well, despite the fact that the documentswere created by different people and most likelywith different instructions on how to write sim-ple texts or simplify already existing texts.
It wasinteresting to note the role of traditional featuresin improving the accuracy of these binary classi-fication models.
But, in the previous experiment,the model with traditional features consistently putall the documents into higher reading levels.
It ispossible that the role of traditional features in theWeeBit corpus may be skewed as it is likely that itwas prepared with traditional readability measuresin mind.
Contrasting the results of these two ex-periments raises the question of what features holdmore weight in what dataset, which is an interest-ing issue to explore in the future.In sum, this experiment provides some clearevidence for affirmatively answering the secondquestion about the generalizability of the featureset we used.
The features seem to be sufficientlygeneral for them to be useful in performing read-ability assessment of real-world documents.7 Conclusion and DiscussionIn this paper, we set out to investigate the appli-cability and generalizability of readability modelsfor real-world web texts.
We started with build-ing readability models using linear regression, ona 5-level readability corpus with a range of lexi-cal and syntactic features (section 4).
We appliedthe two best models thus obtained to several webdatasets we compiled from websites targeting chil-dren and others designed for adults (section 5.1)and on the Top-100 results obtained using a stan-dard web search engine (section 5.2).We observed that the models identified textsacross a broad range of reading levels in the webcorpora.
Our pilot study of the reading levels ofthe search results confirmed that readability mod-els could be useful as re-ranking or filtering pa-rameters that prioritize relevant results which areat the right level for a given user.
At the sametime, we observed in both these experiments thatthe average reading level of general web articlesis relatively high according to our models.
Apartfrom result ranking, this also calls for the construc-tion of efficient text simplification systems whichpick up the difficult texts and attempt to simplifythem to a given reading level.We then proceeded to investigate how wellthe features used to build these readability mod-els generalize across different corpora.
For this,we reused the corpora with articles for childrenand adult readers from prototypical websites (sec-tion 5.1.1) and built four binary classificationmodels with all of the readability features (sec-tion 6).
Each of the models achieved good clas-sification accuracies, supporting that the broadfeature set used generalizes well across corpora.Whether or not to use traditional readability fea-tures is somewhat difficult to answer since thoseformulae are often taken into account when writ-ing materials, so high classification accuracy onsuch corpora may be superficial in that it is notnecessarily indicative of the spectrum of textsfound on the web (section 5.1).
This also raisesthe more general question which features workbest for which kind of dataset.
A systematic ex-ploration of the effect of the individual featuresalong with the impact of document topic and genreon readability would be interesting and relevant topursue in the future.In our future work, we also intend to explorefurther features for this task and improve our un-derstanding of the correlations between the differ-ent features.
Finally, we are considering reformu-lating readability assessment as ordinal regressionor preference ranking.AcknowledgementsWe would like to thank the anonymous reviewersfor their detailed, useful comments on the paper.This research was funded by the European Com-mission?s 7th Framework Program under grantagreement number 238405 (CLARA).66ReferencesJasmine Benno?hr.
2005.
A web-based personalisedtextfinder for language learners.
Master?s thesis,School of Informatics, University of Edinburgh.Meri Coleman and T. L. Liau.
1975.
A computer read-ability formula designed for machine scoring.
Jour-nal of Applied Psychology, 60:283?284.Kevyn Collins-Thompson and Jamie Callan.
2004.A language modeling approach to predicting read-ing difficulty.
In Proceedings of HLT/NAACL 2004,Boston, USA.Kevyn Collins-Thompson and Jamie Callan.
2005.Predicting reading difficulty with statistical lan-guage models.
Journal of the American Society forInformation Science and Technology, 56(13):1448?1462.K.
Collins-Thompson, P. N. Bennett, R. W. White,S.
de la Chica, and D. Sontag.
2011.
Personaliz-ing web search results by reading level.
In Proceed-ings of the Twentieth ACM International Conferenceon Information and Knowledge Management (CIKM2011).William Coster and David Kauchak.
2011.
Simple en-glish wikipedia: A new text simplification task.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 665?669, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Averil Coxhead.
2000.
A new academic word list.Teachers of English to Speakers of Other Languages,34(2):213?238.William H. DuBay.
2006.
The Classic ReadabilityStudies.
Impact Information, Costa Mesa, Califor-nia.Lijun Feng, Nomie Elhadad, and Matt Huenerfauth.2009.
Cognitively motivated features for readabilityassessment.
In Proceedings of the 12th Conferenceof the European Chapter of the ACL (EACL 2009),pages 229?237, Athens, Greece, March.
Associationfor Computational Linguistics.Lijun Feng.
2010.
Automatic Readability Assessment.Ph.D.
thesis, City University of New York (CUNY).Julia Hancke, Detmar Meurers, and Sowmya Vajjala.2012.
Readability classification for german usinglexical, syntactic, and morphological features.
InProceedings of the 24th International Conference onComputational Linguistics (COLING), pages 1063?1080, Mumbay, India.Michael Heilman, Kevyn Collins-Thompson, JamieCallan, and Maxine Eskenazi.
2007.
Combin-ing lexical and grammatical features to improvereadability measures for first and second languagetexts.
In Human Language Technologies 2007:The Conference of the North American Chapter ofthe Association for Computational Linguistics (HLT-NAACL-07), pages 460?467, Rochester, New York.Michael Heilman, Kevyn Collins-Thompson, andMaxine Eskenazi.
2008.
An analysis of statisticalmodels and features for reading difficulty prediction.In Proceedings of the 3rd Workshop on InnovativeUse of NLP for Building Educational Applicationsat ACL-08, Columbus, Ohio.Tapas Kanungo and David Orr.
2009.
Predicting thereadability of short web summaries.
In Proceed-ings of the Second ACM International Conferenceon Web Search and Data Mining, WSDM ?09, pages202?211, New York, NY, USA.
ACM.P.
Kidwell, G. Lebanon, and K. Collins-Thompson.2011.
Statistical estimation of word acquisition withapplication to readability prediction.
In Journal ofthe American Statistical Association.
106(493):21-30.Jin Young Kim, Kevyn Collins-Thompson, Paul N.Bennett, and Susan T. Dumais.
2012.
Characteriz-ing web content, user interests, and search behaviorby reading level and topic.
In Proceedings of thefifth ACM international conference on Web searchand data mining, WSDM ?12, pages 213?222, NewYork, NY, USA.
ACM.J.
P. Kincaid, R. P. Jr. Fishburne, R. L. Rogers, andB.
S Chissom.
1975.
Derivation of new readabilityformulas (Automated Readability Index, Fog Countand Flesch Reading Ease formula) for Navy enlistedpersonnel.
Research Branch Report 8-75, NavalTechnical Training Command, Millington, TN.Roger Levy and Galen Andrew.
2006.
Tregex and tsur-geon: tools for querying and manipulating tree datastructures.
In 5th International Conference on Lan-guage Resources and Evaluation, Genoa, Italy.Jie Lu and Jamie Callan.
2003.
Content-based retrievalin hybrid peer-to-peer networks.
In Proceedings ofthe Twelfth International Conference on Informationand Knowledge Management (CIKM?03).Xiaofei Lu.
2010.
Automatic analysis of syntac-tic complexity in second language writing.
Inter-national Journal of Corpus Linguistics, 15(4):474?496.Xiaofei Lu.
2012.
The relationship of lexical richnessto the quality of ESL learners?
oral narratives.
TheModern Languages Journal, pages 190?208.Yi Ma, Eric Fosler-Lussier, and Robert Lofthus.
2012a.Ranking-based readability assessment for early pri-mary children?s literature.
In Proceedings of the2012 Conference of the North American Chapterof the Association for Computational Linguistics:Human Language Technologies, NAACL HLT ?12,pages 548?552, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.67Yi Ma, Ritu Singh, Eric Fosler-Lussier, and RobertLofthus.
2012b.
Comparing human versus au-tomatic feature extraction for fine-grained elemen-tary readability assessment.
In Proceedings of theFirst Workshop on Predicting and Improving TextReadability for target reader populations, PITR ?12,pages 58?64, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Danielle S. McNamara, Max M. Louwerse, andArthur C. Graesser.
2002.
Coh-metrix: Auto-mated cohesion and coherence scores to predict textreadability and facilitate comprehension.
Proposalof Project funded by the Office of Educational Re-search and Improvement, Reading Program.Eleni Miltsakaki and Audrey Troutt.
2008.
Real timeweb text classification and analysis of reading dif-ficulty.
In Proceedings of the Third Workshop onInnovative Use of NLP for Building Educational Ap-plications (BEA-3) at ACL?08, pages 89?97, Colum-bus, Ohio.
Association for Computational Linguis-tics.Eleni Miltsakaki.
2009.
Matching readers?
prefer-ences and reading skills with appropriate web texts.In Proceedings of the 12th Conference of the Eu-ropean Chapter of the Association for Computa-tional Linguistics: Demonstrations Session, EACL?09, pages 49?52, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Makoto Nakatani, Adam Jatowt, and Katsumi Tanaka.2010.
Adaptive ranking of search results by consid-ering user?s comprehension.
In Proceedings of the4th International Conference on Ubiquitous Infor-mation Management and Communication (ICUIMC2010), pages 182?192.
ACM Press, Suwon, Korea.Courtney Napoles and Mark Dredze.
2010.
Learn-ing simple wikipedia: a cogitation in ascertainingabecedarian language.
In Proceedings of the NAACLHLT 2010 Workshop on Computational Linguisticsand Writing: Writing Processes and Authoring Aids,CL&W ?10, pages 42?50, Stroudsburg, PA, USA.Association for Computational Linguistics.Neil Newbold, Harry McLaughlin, and Lee Gillam.2010.
Rank by readability: Document weighting forinformation retrieval.
In Hamish Cunningham, Al-lan Hanbury, and Stefan Ru?ger, editors, Advances inMultidisciplinary Retrieval, volume 6107 of LectureNotes in Computer Science, pages 20?30.
SpringerBerlin / Heidelberg.Niels Ott and Detmar Meurers.
2010.
Information re-trieval for education: Making search engines lan-guage aware.
Themes in Science and TechnologyEducation.
Special issue on computer-aided lan-guage analysis, teaching and learning: Approaches,perspectives and applications, 3(1?2):9?30.Sarah E. Petersen and Mari Ostendorf.
2009.
A ma-chine learning approach to reading level assessment.Computer Speech and Language, 23:86?106.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 404?411, Rochester, New York, April.Satoshi Sato, Suguru Matsuyoshi, and Yohsuke Kon-doh.
2008.
Automatic assessment of japanese textreadability based on a textbook corpus.
In LREC?08.Luo Si and Jamie Callan.
2001.
A statistical model forscientific readability.
In Proceedings of the 10th In-ternational Conference on Information and Knowl-edge Management (CIKM), pages 574?576.
ACM.Chenhao Tan, Evgeniy Gabrilovich, and Bo Pang.2012.
To each his own: Personalized content se-lection based on text comprehensibility.
In In Pro-ceedings of WSDM.Sowmya Vajjala and Detmar Meurers.
2012.
On im-proving the accuracy of readability classification us-ing insights from second language acquisition.
InJoel Tetreault, Jill Burstein, and Claudial Leacock,editors, Proceedings of the 7th Workshop on Innova-tive Use of NLP for Building Educational Applica-tions (BEA7) at NAACL-HLT, pages 163?173, Mon-tral, Canada, June.
Association for ComputationalLinguistics.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proceedings of The23rd International Conference on ComputationalLinguistics (COLING), August 2010.
Beijing, China.68
