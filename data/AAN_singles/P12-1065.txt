Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 620?628,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsBootstrapping via Graph PropagationMax Whitney and Anoop Sarkar?Simon Fraser University, School of Computing ScienceBurnaby, BC V5A 1S6, Canada{mwhitney,anoop}@sfu.caAbstractBootstrapping a classifier from a small setof seed rules can be viewed as the propaga-tion of labels between examples via featuresshared between them.
This paper introduces anovel variant of the Yarowsky algorithm basedon this view.
It is a bootstrapping learningmethod which uses a graph propagation algo-rithm with a well defined objective function.The experimental results show that our pro-posed bootstrapping algorithm achieves stateof the art performance or better on several dif-ferent natural language data sets.1 IntroductionIn this paper, we are concerned with a case of semi-supervised learning that is close to unsupervisedlearning, in that the labelled and unlabelled datapoints are from the same domain and only a smallset of seed rules is used to derive the labelled points.We refer to this setting as bootstrapping.
In contrast,typical semi-supervised learning deals with a largenumber of labelled points, and a domain adaptationtask with unlabelled points from the new domain.The two dominant discriminative learning meth-ods for bootstrapping are self-training (Scud-der, 1965) and co-training (Blum and Mitchell,1998).
In this paper we focus on a self-trainingstyle bootstrapping algorithm, the Yarowsky algo-rithm (Yarowsky, 1995).
Variants of this algorithmhave been formalized as optimizing an objectivefunction in previous work by Abney (2004) and Haf-fari and Sarkar (2007), but it is not clear that anyperform as well as the Yarowsky algorithm itself.We take advantage of this formalization and in-troduce a novel algorithm called Yarowsky-propwhich builds on the algorithms of Yarowsky (1995)and Subramanya et al (2010).
It is theoretically?This research was partially supported by an NSERC,Canada (RGPIN: 264905) grant.
We would like to thank Gho-lamreza Haffari and the anonymous reviewers for their com-ments.
We particularly thank Michael Collins, Jason Eisner, andDamianos Karakos for the data we used in our experiments.x denotes an examplef , g denote featuresi, k denote labelsX set of training examplesFx set of features for example xY current labelling of XYx current label for example x?
value of Yx for unlabelled examplesL number of labels (not including ?)?
set of currently labelled examplesV set of currently unlabelled examplesXf set of examples with feature f?f set of currently labelled examples with fVf set of currently unlabelled examples with f?j set of examples currently labelled with j?fj set of examples with f currently labelled with jTable 1: Notation of Abney (2004).well-understood as minimizing an objective func-tion at each iteration, and it obtains state of the artperformance on several different NLP data sets.
Toour knowledge, this is the first theoretically mo-tivated self-training bootstrapping algorithm whichperforms as well as the Yarowsky algorithm.2 BootstrappingAbney (2004) defines useful notation for semi-supervised learning, shown in table 1.
Note that ?,V , etc.
are relative to the current labelling Y .
Weadditionally define F to be the set of all features,and use U to denote the uniform distribution.
In thebootstrapping setting the learner is given an initialpartial labelling Y (0) where only a few examples arelabelled (i.e.
Y (0)x = ?
for most x).Abney (2004) defines three probability distribu-tions in his analysis of bootstrapping: ?fj is the pa-rameter for feature f with label j, taken to be nor-malized so that ?f is a distribution over labels.
?x isthe labelling distribution representing the current Y ;it is a point distribution for labelled examples anduniform for unlabelled examples.
pix is the predic-tion distribution over labels for example x.The approach of Haghighi and Klein (2006b) andHaghighi and Klein (2006a) also uses a small set of620Algorithm 1: The basic Yarowsky algorithm.Require: training data X and a seed DL ?
(0)1: apply ?
(0) to X produce a labelling Y (0)2: for iteration t to maximum or convergence do3: train a new DL ?
on Y (t)4: apply ?
to X , to produce Y (t+1)5: end forseed rules but uses them to inject features into a jointmodel p(x, j) which they train using expectation-maximization for Markov random fields.
We focuson discriminative training which does not requirecomplex partition functions for normalization.
Blumand Chawla (2001) introduce an early use of trans-ductive learning using graph propagation.
X. Zhuand Z. Ghahramani and J. Lafferty (2003)?s methodof graph propagation is predominantly transductive,and the non-transductive version is closely related toAbney (2004) c.f.
Haffari and Sarkar (2007).13 Existing algorithms3.1 YarowskyA decision list (DL) is a (ordered) list of feature-label pairs (rules) which is produced by assigninga score to each rule and sorting on this score.
Itchooses a label for an example from the first rulewhose feature is a feature of the example.
For aDL the prediction distribution is defined by pix(j) ?maxf?Fx ?fj .
The basic Yarowsky algorithm isshown in algorithm 1.
Note that at any point sometraining examples may be left unlabelled by Y (t).We use Collins and Singer (1999) for our exactspecification of Yarowsky.2 It uses DL rule scores?fj ?|?fj |+ |?f |+ L(1)where  is a smoothing constant.
When constructinga DL it keeps only the rules with (pre-normalized)score over a threshold ?.
In our implementation weadd the seed rules to each subsequent DL.31Large-scale information extraction, e.g.
(Hearst, 1992),Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff andShepherd, 1997), and Junto (Talukdar, 2010) among others, alsohave similarities to our approach.
We focus on the formal anal-ysis of the Yarowsky algorithm by Abney (2004).2It is similar to that of Yarowsky (1995) but is better spec-ified and omits word sense disambiguation optimizations.
Thegeneral algorithm in Yarowsky (1995) is self-training with anykind of underlying supervised classifier, but we follow the con-vention of using Yarowsky to refer to the DL algorithm.3This is not clearly specified in Collins and Singer (1999),3.2 Yarowsky-cautiousCollins and Singer (1999) also introduce a variantalgorithm Yarowsky-cautious.
Here the DL trainingstep keeps only the top n rules (f, j) over the thresh-old for each label j, ordered by |?f |.
Additionallythe threshold ?
is checked against |?fj |/|?f | insteadof the smoothed score.
n begins at n0 and is incre-mented by ?n at each iteration.
We add the seed DLto the new DL after applying the cautious pruning.Cautiousness limits not only the size of the DL butalso the number of labelled examples, prioritizingdecisions which are believed to be of high accuracy.At the final iteration Yarowsky-cautious uses thecurrent labelling to train a DL without a thresholdor cautiousness, and this DL is used for testing.
Wecall this the retraining step.43.3 DL-CoTrainCollins and Singer (1999) also introduce the co-training algorithm DL-CoTrain.
This algorithm al-ternates between two DLs using disjoint views ofthe features in the data.
At each step it trains a DLand then produces a new labelling for the other DL.Each DL uses thresholding and cautiousness as wedescribe for Yarowsky-cautious.
At the end the DLsare combined, the result is used to label the data, anda retraining step is done from this single labelling.3.4 Y-1/DL-1-VSOne of the variant algorithms of Abney (2004) isY-1/DL-1-VS (referred to by Haffari and Sarkar(2007) as simply DL-1).
Besides various changesin the specifics of how the labelling is produced,this algorithm has two differences versus Yarowsky.Firstly, the smoothing constant  in (1) is replacedby 1/|Vf |.
Secondly, pi is redefined as pix(j) =1|Fx|?f?Fx ?fj , which we refer to as the sum def-inition of pi.
This definition does not match a literalDL but is easier to analyze.We are not concerned here with the details ofY-1/DL-1-VS, but we note that Haffari and Sarkarbut is used for DL-CoTrain in the same paper.4The details of Yarowsky-cautious are not clearly specifiedin Collins and Singer (1999).
Based on similar parts of DL-CoTrain we assume the that the top n selection is per labelrather in total, that the thresholding value is unsmoothed, andthat there is a retraining step.
We also assume their notationCount?
(x) to be equivalent to |?f |.621(2007) provide an objective function for this al-gorithm using a generalized definition of cross-entropy in terms of Bregman distance, which mo-tivates our objective in section 4.
The Breg-man distance between two discrete probability dis-tributions p and q is defined as B?
(p, q) =?i [?(pi)?
?(qi)?
??
(qi)(pi ?
qi)].
As a specificcase we have Bt2(p, q) =?i(pi?
qi)2 = ||p?
q||2.Then Bregman distance-based entropy is Ht2(p) =?
?i p2i , KL-Divergence is Bt2 , and cross-entropyfollows the standard definition in terms of Ht2 andBt2 .
The objective minimized by Y-1/DL-1-VS is:?x?Xf?FxHt2(?x||?f ) =?x?Xf?Fx[Bt2(?x||?f )??y?2x].
(2)3.5 Yarowsky-sumAs a baseline for the sum definition of pi, we intro-duce the Yarowsky-sum algorithm.
It is the sameas Yarowsky except that we use the sum definitionwhen labelling: for example x we choose the label jwith the highest (sum) pix(j), but set Yx = ?
if thesum is zero.
Note that this is a linear model similarto a conditional random field (CRF) (Lafferty et al,2001) for unstructured multiclass problems.3.6 Bipartite graph algorithmsHaffari and Sarkar (2007) suggest a bipartitegraph framework for semi-supervised learningbased on their analysis of Y-1/DL-1-VS and objec-tive (2).
The graph has vertices X ?
F and edges{(x, f) : x ?
X, f ?
Fx}, as in the graph shownin figure 1(a).
Each vertex represents a distributionover labels, and in this view Yarowsky can be seen asalternately updating the example distributions basedon the feature distributions and visa versa.Based on this they give algorithm 2, whichwe call HS-bipartite.
It is parametrized by twofunctions which are called features-to-example andexamples-to-feature here.
Each can be one oftwo choices: average(S) is the normalized aver-age of the distributions of S, while majority(S)is a uniform distribution if all labels are supportedby equal numbers of distributions of S, and other-wise a point distribution with mass on the best sup-ported label.
The average-majority form is similarAlgorithm 2: HS-bipartite.1: apply ?
(0) to X produce a labelling Y (0)2: for iteration t to maximum or convergence do3: for f ?
F do4: let p = examples-to-feature({?x : x ?
Xf})5: if p 6= U then let ?f = p6: end for7: for x ?
X do8: let p = features-to-example({?f : f ?
Fx})9: if p 6= U then let ?x = p10: end for11: end forto Y-1/DL-1-VS, and the majority-majority formminimizes a different objective similar to (2).In our implementation we label training data (forthe convergence check) with the ?
distributions fromthe graph.
We label test data by constructing new?x = examples-to-feature(Fx) for the unseen x.3.7 Semi-supervised learning algorithm of Sub-ramanya et al (2010)Subramanya et al (2010) give a semi-supervised al-gorithm for part of speech tagging.
Unlike the algo-rithms described above, it is for domain adaptationwith large amounts of labelled data rather than boot-strapping with a small number of seeds.This algorithm is structurally similar to Yarowskyin that it begins from an initial partial labelling andrepeatedly trains a classifier on the labelling andthen relabels the data.
It uses a CRF (Lafferty et al,2001) as the underlying supervised learner.
It dif-fers significantly from Yarowsky in two other ways:First, instead of only training a CRF it also uses astep of graph propagation between distributions overthe n-grams in the data.
Second, it does the propa-gation on distributions over n-gram types rather thanover n-gram tokens (instances in the data).They argue that using propagation over types al-lows the algorithm to enforce constraints and findsimilarities that self-training cannot.
We are not con-cerned here with the details of this algorithm, butit motivates our work firstly in providing the graphpropagation which we will describe in more detail insection 4, and secondly in providing an algorithmicstructure that we use for our algorithm in section 5.3.8 Collins and Singer (1999)?s EMWe implemented the EM algorithm of Collins andSinger (1999) as a baseline for the other algorithms.622Method V N (u) qu?-?
X ?
F Nx = Fx, Nf = Xf qx = ?x, qf = ?fpi-?
X ?
F Nx = Fx, Nf = Xf qx = pix, qf = ?f?-only F Nf =?x?XfFx \ f qf = ?f?T-only F Nf =?x?XfFx \ f qf = ?TfTable 2: Graph structures for propagation.They do not specify tuning details, but to get com-parable accuracy we found it was necessary to dosmoothing and to include weights ?1 and ?2 on theexpected counts of seed-labelled and initially unla-belled examples respectively (Nigam et al, 2000).4 Graph propagationThe graph propagation of Subramanya et al (2010)is a method for smoothing distributions attached tovertices of a graph.
Here we present it with an alter-nate notation using Bregman distances as describedin section 3.4.5 The objective is?
?u?Vv?N (i)wuvBt2(qu, qv) + ?
?u?VBt2(qu, U) (3)where V is a set of vertices, N (v) is the neighbour-hood of vertex v, and qv is an initial distribution foreach vertex v to be smoothed.
They give an iterativeupdate to minimize (3).
Note that (3) is independentof their specific graph structure, distributions, andsemi-supervised learning algorithm.We propose four methods for using this propaga-tion with Yarowsky.
These methods all use con-stant edge weights (wuv = 1).
The distributionsand graph structures are shown in table 2.
Figure 1shows example graphs for ?-?
and ?-only.
pi-?
and?T-only are similar, and are described below.The graph structure of ?-?
is the bipartite graphof Haffari and Sarkar (2007).
In fact, ?-?
the propa-gation objective (3) and Haffari and Sarkar (2007)?sY-1/DL-1-VS objective (2) are identical up to con-stant coefficients and an extra constant term.6 ?-?5We omit the option to hold some of the distributions at fixedvalues, which would add an extra term to the objective.6The differences are specifically: First, (3) adds the con-stant coefficients ?
and ?.
Second, (3) sums over each edgetwice (once in each direction), while (2) sums over each onlyonce.
Since wuv = wvu and Bt2(qu, qv) = Bt2(qv, qu), thiscan be folded into the constant ?.
Third, after expanding (2)there is a term |Fx| inside the sum for Ht2(?x) which is notpresent in (3).
This does not effect the direction of minimiza-tion.
Fourth, Bt2(qu, U) in (3) expands to Ht2(qu) plus a con-stant, adding an extra constant term to the total.
?f|F |?f4?f3?f2?f1 ?x1?x2?x3?x4?x|X|... ...(a) ?-?
method?f1?f|F |?f2?f4?f3...(b) ?-only methodFigure 1: Example graphs for ?-?
and ?-only propagation.therefore gives us a direct way to optimize (2).The other three methods do not correspond to theobjective of Haffari and Sarkar (2007).
The pi-?method is like ?-?
except for using pi as the distribu-tion for example vertices.The bipartite graph of the first two methods dif-fers from the structure used by Subramanya et al(2010) in that it does propagation between two dif-ferent kinds of distributions instead of only one kind.We also adopt a more comparable approach with agraph over only features.
Here we define adjacencyby co-occurrence in the same example.
The ?-onlymethod uses this graph and ?
as the distribution.Finally, we noted in section 3.7 that the algo-rithm of Subramanya et al (2010) does one addi-tional step in converting from token level distribu-tions to type level distributions.
The ?T-only methodtherefore uses the feature-only graph but for the dis-tribution uses a type level version of ?
defined by?Tfj =1|Xf |?x?Xfpix(j).5 Novel Yarowsky-prop algorithmWe call our graph propagation based algorithmYarowsky-prop.
It is shown with ?T-only propaga-tion in algorithm 3.
It is based on the Yarowsky al-gorithm, with the following changes: an added stepto calculate ?T (line 4), an added step to calculate ?P(line 5), the use of ?P rather than the DL to updatethe labelling (line 6), and the use of the sum defini-tion of pi.
Line 7 does DL training as we describe insections 3.1 and 3.2.
Propagation is done with theiterative update of Subramanya et al (2010).This algorithm is adapted to the other propagationmethods described in section 4 by changing the typeof propagation on line 5.
In ?-only, propagation is623Algorithm 3: Yarowsky-prop.1: let ?fj be the scores of the seed rules // crf train2: for iteration t to maximum or convergence do3: let pix(j) = 1|Fx|?f?Fx?fj // post.
decode4: let ?Tfj =Px?Xfpix(j)|Xf |// token to type5: propagate ?T to get ?P // graph propagate6: label the data with ?P // viterbi decode7: train a new DL ?fj // crf train8: end fordone on ?, using the graph of figure 1(b).
In ?-?
andpi-?
propagation is done on the respective bipartitegraph (figure 1(a) or the equivalent with pi).
Line4 is skipped for these methods, and ?
is as definedin section 2.
For the bipartite graph methods ?-?and pi-?
only the propagated ?
values on the featurenodes are used for ?P (the distributions on the exam-ple nodes are ignored after the propagation itself).The algorithm uses ?fj values rather than an ex-plicit DL for labelling.
The (pre-normalized) scorefor any (f, j) not in the DL is taken to be zero.
Be-sides using the sum definition of pi when calculating?T, we also use a sum in labelling.
When labellingan example x (at line 6 and also on testing data) weuse arg maxj?f?Fx: ?Pf 6=U?Pfj , but set Yx = ?
ifthe sum is zero.
Ignoring uniform ?Pf values is in-tended to provide an equivalent to the DL behaviourof using evidence only from rules that are in the list.We include the cautiousness of Yarowsky-cautious (section 3.2) in the DL training on line 7.
Atthe labelling step on line 6 we label only exampleswhich the pre-propagated ?
would also assign a label(using the same rules described above for ?P).
Thischoice is intended to provide an equivalent to theYarowsky-cautious behaviour of limiting the num-ber of labelled examples; most ?Pf are non-uniform,so without it most examples become labelled early.We observe further similarity between theYarowsky algorithm and the general approach ofSubramanya et al (2010) by comparing algorithm3 here with their algorithm 1.
The comments in al-gorithm 3 give the corresponding parts of their algo-rithm.
Note that each line has a similar purpose.6 Evaluation6.1 Tasks and dataFor evaluation we use the tasks of Collins and Singer(1999) and Eisner and Karakos (2005), with dataRank Score Feature Label1 0.999900 New-York loc.2 0.999900 California loc.3 0.999900 U.S. loc.4 0.999900 Microsoft org.5 0.999900 I.B.M.
org.6 0.999900 Incorporated org.7 0.999900 Mr. per.8 0.999976 U.S. loc.9 0.999957 New-York-Stock-Exchange loc.10 0.999952 California loc.11 0.999947 New-York loc.12 0.999946 court-in loc.13 0.975154 Company-of loc....Figure 2: A DL from iteration 5 of Yarowsky on the named en-tity task.
Scores are pre-normalized values from the expressionon the left side of (1), not ?fj values.
Context features are indi-cated by italics; all others are spelling features.
Specific featuretypes are omitted.
Seed rules are indicated by bold ranks.kindly provided by the respective authors.The task of Collins and Singer (1999) is namedentity classification on data from New York Timestext.7 The data set was pre-processed by a statisti-cal parser (Collins, 1997) and all noun phrases thatare potential named entities were extracted from theparse tree.
Each noun phrase is to be labelled asa person, organization, or location.
The parse treeprovides the surrounding context as context featuressuch as the words in prepositional phrase and rela-tive clause modifiers, etc., and the actual words inthe noun phrase provide the spelling features.
Thetest data additionally contains some noise exampleswhich are not in the three named entity categories.We use the seed rules the authors provide, which arethe first seven items in figure 2.
For DL-CoTrain,we use their two views: one view is the spelling fea-tures, and the other is the context features.
Figure 2shows a DL from Yarowsky training on this task.The tasks of Eisner and Karakos (2005) are wordsense disambiguation on several English wordswhich have two senses corresponding to two dif-ferent words in French.
Data was extracted fromthe Canadian Hansards, using the English side toproduce training and test data and the French sideto produce the gold labelling.
Features are theoriginal and lemmatized words immediately adja-7We removed weekday and month examples from the test setas they describe.
They note 88962 examples in their training set,but the file has 89305.
We did not find any filtering criteria thatproduced the expected size, and therefore used all examples.624cent to the word to be disambiguated, and origi-nal and lemmatized context words in the same sen-tence.
Their seeds are pairs of adjacent word fea-tures, with one feature for each label (sense).
Weuse the ?drug?, ?land?, and ?sentence?
tasks, andthe seed rules from their best seed selection: ?alco-hol?/?medical?, ?acres?/?court?, and ?reads?/?served?respectively (they do not provide seeds for theirother three tasks).
For DL-CoTrain we use adjacentwords for one view and context words for the other.6.2 Experimental set upWhere applicable we use smoothing  = 0.1, athreshold ?
= 0.95, and cautiousness parametersn0 = ?n = 5 as in Collins and Singer (1999)and propagation parameters ?
= 0.6, ?
= 0.01 asin Subramanya et al (2010).
Initial experimentswith different propagation parameters suggested thatas long as ?
was set at this value changing ?
hadrelatively little effect on the accuracy.
We did notfind any propagation parameter settings that outper-formed this choice.
For the Yarowsky-prop algo-rithms we perform a single iteration of the propa-gation update for each iteration of the algorithm.For EM we use weights ?1 = 0.98, and ?2 = 0.02(see section 3.8), which were found in initial experi-ments to be the best values, and results are averagedover 10 random initializations.The named entity test set contains some examplesthat are neither person, organization, nor location.Collins and Singer (1999) define noise accuracy asaccuracy that includes such instances, and clean ac-curacy as accuracy calculated across only the exam-ples which are one of the known labels.
We reportonly clean accuracy in this paper; noise accuracytracks clean accuracy but is a little lower.
There isno difference on the word sense data sets.
We alsoreport (clean) non-seeded accuracy, which we defineto be clean accuracy over only examples which arenot assigned a label by the seed rules.
This is in-tended to evaluate what the algorithm has learned,rather than what it can achieve by using the inputinformation directly (Daume, 2011).We test Yarowsky, Yarowsky-cautious,Yarowsky-sum, DL-CoTrain, HS-bipartite inall four forms, and Yarowsky-prop cautious andnon-cautious and in all four forms.
For each algo-rithm except EM we perform a final retraining stepGold Spelling features Context featuresloc.
Waukegan maker, LEFTloc.
Mexico, president, of president-of, RIGHTloc.
La-Jolla, La Jolla company, LEFTFigure 3: Named entity test set examples where Yarowsky-prop?-only is correct and no other tested algorithms are correct.
Thespecific feature types are omitted.as described for Yarowsky-cautious (section 3.2).Our programs and experiment scripts have beenmade available.86.3 AccuracyTable 3 shows the final test set accuracies for theall the algorithms.
The seed DL accuracy is alsoincluded for reference.The best performing form of our novel algo-rithm is Yarowsky-prop-cautious ?-only.
It numer-ically outperforms DL-CoTrain on the named entitytask, is not (statistically) significantly worse on thedrug and land tasks, and is significantly better onthe sentence task.
It also numerically outperformsYarowsky-cautious on the named entity task and issignificantly better on the drug task.
Is significantlyworse on the land task, where most algorithms con-verge at labelling all examples with the first sense.
Itis significantly worse on the sentence task, althoughit is the second best performing algorithm and sev-eral percent above DL-CoTrain on that task.Figure 3 shows (all) three examples from thenamed entity test set where Yarowsky-prop-cautious?-only is correct but none of the other Yarowskyvariants are.
Note that it succeeds despite mis-leading features; ?maker?
and ?company?
might betaken to indicate a company and ?president-of?
anorganization, but all three examples are locations.Yarowsky-prop-cautious ?-?
and pi-?
also per-form respectably, although not as well.
Yarowsky-prop-cautious ?T-only and the non-cautious versionsare significantly worse.
Although ?T-only was in-tended to incorporate Subramanya et al (2010)?sidea of type level distributions, it in fact performsworse than ?-only.
We believe that Collins andSinger (1999)?s definition (1) of ?
incorporates suf-ficient type level information that the creation of aseparate distribution is unnecessary in this case.Figure 4 shows the test set non-seeded accuraciesas a function of the iteration for many of the algo-8The software is included with the paper submission andwill be maintained at https://github.com/sfu-natlang/yarowsky.625AlgorithmTasknamed entity drug land sentenceEM81.05 78.64 55.96 54.85 32.86 31.07 67.88 65.42?0.31 ?0.34 ?0.41 ?0.43 ?0.00 ?0.00 ?3.35 ?3.57Seed DL 11.29 0.00 5.18 0.00 2.89 0.00 7.18 0.00DL-CoTrain (cautious) 91.56 90.49 59.59 58.17 78.36 77.72 68.16 65.69Yarowsky 81.19 78.79 55.70 54.02 79.03 78.41 62.91 60.04Yarowsky-cautious 91.11 89.97 54.40 52.63 79.10 78.48 78.64 76.99Yarowsky-cautious sum 91.56 90.49 54.40 52.63 78.36 77.72 78.64 76.99HS-bipartite avg-avg 45.84 45.89 52.33 50.42 78.36 77.72 54.56 51.05HS-bipartite avg-maj 81.98 79.69 52.07 50.14 78.36 77.72 55.15 51.67HS-bipartite maj-avg 73.55 70.18 52.07 50.14 78.36 77.72 55.15 51.67HS-bipartite maj-maj 73.66 70.31 52.07 50.14 78.36 77.72 55.15 51.67Yarowsky-prop ?-?
80.39 77.89 53.63 51.80 78.36 77.72 55.34 51.88Yarowsky-prop pi-?
78.34 75.58 54.15 52.35 78.36 77.72 54.56 51.05Yarowsky-prop ?-only 78.56 75.84 54.66 52.91 78.36 77.72 54.56 51.05Yarowsky-prop ?T-only 77.88 75.06 52.07 50.14 78.36 77.72 54.56 51.05Yarowsky-prop-cautious ?-?
90.19 88.95 56.99 55.40 78.36 77.72 74.17 72.18Yarowsky-prop-cautious pi-?
89.40 88.05 58.55 57.06 78.36 77.72 70.10 67.78Yarowsky-prop-cautious ?-only 92.47 91.52 58.55 57.06 78.36 77.72 75.15 73.22Yarowsky-prop-cautious ?T-only 78.45 75.71 58.29 56.79 78.36 77.72 54.56 51.05Num.
train/test examples 89305 / 962 134 / 386 1604 / 1488 303 / 515Table 3: Test set percent accuracy and non-seeded test set percent accuracy (respectively) for the algorithms on all tasks.
Bolditems are a maximum in their column.
Italic items have a statistically significant difference versus DL-CoTrain (p < 0.05 with aMcNemar test).
For EM, ?
indicates one standard deviation but statistical significance was not measured.rithms on the named entity task.
The Yarowsky-propnon-cautious algorithms quickly converge to the fi-nal accuracy and are not shown.
While the otheralgorithms (figure 4(a)) make a large accuracy im-provement in the final retraining step, the Yarowsky-prop (figure 4(b)) algorithms reach comparable ac-curacies earlier and gain much less from retraining.We did not implement Collins and Singer (1999)?sCoBoost; however, in their results it performs com-parably to DL-CoTrain and Yarowsky-cautious.
Aswith DL-CoTrain, CoBoost requires two views.6.4 CautiousnessCautiousness appears to be important in the perfor-mance of the algorithms we tested.
In table 3, onlythe cautious algorithms are able to reach the 90%accuracy range.To evaluate the effects of cautiousness we ex-amine the Yarowsky-prop ?-only algorithm on thenamed entity task in more detail.
This algorithm hastwo classifiers which are trained in conjunction: theDL and the propagated ?P.
Figure 5 shows the train-ing set coverage (of the labelling on line 6 of algo-rithm 3) and the test set accuracy of both classifiers,for the cautious and non-cautious versions.The non-cautious version immediately learns aDL over all feature-label pairs, and therefore has fullcoverage after the first iteration.
The DL and ?P con-verge to similar accuracies within a few more itera-tions, and the retraining step increases accuracy byless than one percent.
On the other hand, the cau-tious version gradually increases the coverage overthe iterations.
The DL accuracy follows the cover-age closely (similar to the behaviour of Yarowsky-cautious, not shown here), while the propagatedclassifier accuracy jumps quickly to near 90% andthen increases only gradually.Although the DL prior to retraining achieves aroughly similar accuracy in both versions, only thecautious version is able to reach the 90% accuracyrange in the propagated classifier and retraining.Presumably the non-cautious version makes an earlymistake, reaching a local minimum which it cannotescape.
The cautious version avoids this by makingonly safe rule selection and labelling choices.Figure 5(b) also helps to clarify the difference inretraining that we noted in section 6.3.
Like thenon-propagated DL algorithms, the DL componentof Yarowsky-prop has much lower accuracy than thepropagated classifier prior to the retraining step.
Butafter retraining, the DL and ?P reach very similar ac-curacies.6260.50.55 0.60.65 0.70.75 0.80.85 0.90.95  0100200300400500600Non-seeded test accuracyIterationDL-CoTrain (cautious) YarowskyYarowsky-cautiousYarowsky-cautioussum(a) Collins & Singer algorithms (plus sum form)0.50.55 0.60.65 0.70.75 0.80.85 0.90.95  0100200300400500600Non-seeded test accuracyIterationYarowsky-prop-cautiousphi-thetaYarowsky-prop-cautious pi-thetaYarowsky-prop-cautious theta-onlyYarowsky-prop-cautious thetatype-only(b) Yarowsky propagation cautiousFigure 4: Non-seeded test accuracy versus iteration for variousalgorithms on named entity.
The results for the Yarowsky-propalgorithms are for the propagated classifier ?P , except for thefinal DL retraining iteration.6.5 Objective functionThe propagation method ?-?
was motivated by opti-mizing the equivalent objectives (2) and (3) at eachiteration.
Figure 6 shows the graph propagation ob-jective (3) along with accuracy for Yarowsky-prop?-?
without cautiousness.
The objective value de-creases as expected, and converges along with accu-racy.
Conversely, the cautious version (not shownhere) does not clearly minimize the objective, sincecautiousness limits the effect of the propagation.7 ConclusionsOur novel algorithm achieves accuracy compara-ble to Yarowsky-cautious, but is better theoreticallymotivated by combining ideas from Haffari andSarkar (2007) and Subramanya et al (2010).
It alsoachieves accuracy comparable to DL-CoTrain, butdoes not require the features to be split into two in-dependent views.As future work, we would like to apply our al-0.40.50.60.70.80.9 1  0100200300400500600Non-seeded test accuracy | CoverageIterationmain dlcoverage(a) Non-cautious0.40.50.60.70.80.9 1  0100200300400500600Non-seeded test accuracy | CoverageIterationmain dlcoverage(b) CautiousFigure 5: Internal train set coverage and non-seeded test accu-racy (same scale) for Yarowsky-prop ?-only on named entity.0.40.50.60.70.80.9 1101001000 55000600006500070000750008000085000Non-seeded test accuracy | CoveragePropagation objective valueIterationmaincoverageobjectiveFigure 6: Non-seeded test accuracy (left axis), coverage (leftaxis, same scale), and objective value (right axis) for Yarowsky-prop ?-?.
Iterations are shown on a log scale.
We omit the firstiteration (where the DL contains only the seed rules) and startthe plot at iteration 2 where there is a complete DL.gorithm to a structured task such as part of speechtagging.
We also believe that our method for adapt-ing Collins and Singer (1999)?s cautiousness toYarowsky-prop can be applied to similar algorithmswith other underlying classifiers, even to structuredoutput models such as conditional random fields.627ReferencesS.
Abney.
2004.
Understanding the Yarowsky algorithm.Computational Linguistics, 30(3).Eugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting relations from large plain-text collections.In Proceedings of the Fifth ACM International Con-ference on Digital Libraries, DL ?00.A.
Blum and S. Chawla.
2001.
Learning from labeledand unlabeled data using graph mincuts.
In Proc.19th International Conference on Machine Learning(ICML-2001).A.
Blum and T. Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In Proceedingsof Computational Learning Theory.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In In EMNLP1999: Proceedings of the Joint SIGDAT Conference onEmpirical Methods in Natural Language Processingand Very Large Corpora, pages 100?110.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th Annual Meeting of the Association for Computa-tional Linguistics, pages 16?23, Madrid, Spain, July.Association for Computational Linguistics.Hal Daume.
2011.
Seeding, transduction, out-of-sample error and the Microsoft approach... Blogpost at http://nlpers.blogspot.com/2011/04/seeding-transduction-out-of-sample.html, April 6.Jason Eisner and Damianos Karakos.
2005.
Bootstrap-ping without the boot.
In Proceedings of HumanLanguage Technology Conference and Conference onEmpirical Methods in Natural Language Processing,pages 395?402, Vancouver, British Columbia, Canada,October.
Association for Computational Linguistics.Gholamreza Haffari and Anoop Sarkar.
2007.
Analysisof semi-supervised learning with the Yarowsky algo-rithm.
In UAI 2007, Proceedings of the Twenty-ThirdConference on Uncertainty in Artificial Intelligence,Vancouver, BC, Canada, pages 159?166.Aria Haghighi and Dan Klein.
2006a.
Prototype-drivengrammar induction.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 881?888, Sydney, Aus-tralia, July.
Association for Computational Linguistics.Aria Haghighi and Dan Klein.
2006b.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe Human Language Technology Conference of theNAACL, Main Conference, pages 320?327, New YorkCity, USA, June.
Association for Computational Lin-guistics.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings of the14th conference on Computational linguistics - Vol-ume 2, COLING ?92, pages 539?545, Stroudsburg,PA, USA.
Association for Computational Linguistics.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the Eighteenth InternationalConference on Machine Learning, ICML ?01, pages282?289, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.K.
Nigam, A. McCallum, S. Thrun, and T. Mitchell.2000.
Text classification from labeled and unlabeleddocuments using EM.
Machine Learning, 30(3).Ellen Riloff and Jessica Shepherd.
1997.
A corpus-based approach for building semantic lexicons.
In InProceedings of the Second Conference on EmpiricalMethods in Natural Language Processing, pages 117?124.H.
J. Scudder.
1965.
Probability of error of some adap-tive pattern-recognition machines.
IEEE Transactionson Information Theory, 11:363?371.Amarnag Subramanya, Slav Petrov, and FernandoPereira.
2010.
Efficient graph-based semi-supervisedlearning of structured tagging models.
In Proceedingsof the 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 167?176, Cambridge,MA, October.
Association for Computational Linguis-tics.Partha Pratim Talukdar.
2010.
Graph-based weakly-supervised methods for information extraction & in-tegration.
Ph.D. thesis, University of Pennsylvania.Software: https://github.com/parthatalukdar/junto.X.
Zhu and Z. Ghahramani and J. Lafferty.
2003.
Semi-supervised learning using Gaussian fields and har-monic functions.
In Proceedings of International Con-ference on Machine Learning.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Pro-ceedings of the 33rd Annual Meeting of the Associ-ation for Computational Linguistics, pages 189?196,Cambridge, Massachusetts, USA, June.
Associationfor Computational Linguistics.628
