Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1065?1073,Beijing, August 2010Semi-supervised dependency parsing using generalized tri-trainingAnders S?gaard and Christian Rish?jCenter for Language TechnologyUniversity of Copenhagen{soegaard|crjensen}@hum.ku.dkAbstractMartins et al (2008) presented what tothe best of our knowledge still ranks asthe best overall result on the CONLL-X Shared Task datasets.
The papershows how triads of stacked dependencyparsers described in Martins et al (2008)can label unlabeled data for each other ina way similar to co-training and produceend parsers that are significantly betterthan any of the stacked input parsers.We evaluate our system on five datasetsfrom the CONLL-X Shared Task and ob-tain 10?20% error reductions, incl.
thebest reported results on four of them.We compare our approach to other semi-supervised learning algorithms.1 IntroductionSemi-supervised learning of structured variablesis a difficult problem that has received consid-erable attention recently, but most results havebeen negative (Abney, 2008).
This paper usesstacked learning (Wolpert, 1992) to reduce struc-tured variables, i.e.
dependency graphs, to multi-nomial variables, i.e.
attachment and labelingdecisions, which are easier to manage in semi-supervised learning scenarios, and which canlater be combined into dependency trees usingparsing algorithms for arc-factored dependencyparsing.
Our approach thus combines ensemble-based methods and semi-supervised learning.Ensemble-based methods such as stackedlearning are used to reduce the instability of clas-sifiers, to average out their errors and to com-bine the strengths of diverse learning algorithms.Ensemble-based methods have attracted a lot ofattention in dependency parsing recently (Sagaeand Lavie, 2006; Hall et al, 2007; Nivre andMcDonald, 2008; Martins et al, 2008; Fisheland Nivre, 2009; Surdeanu and Manning, 2010).Nivre and McDonald (2008) were first to intro-duce stacking in the context of dependency pars-ing.Semi-supervised learning is typically moti-vated by data sparseness.
For many classifi-cation tasks in natural language processing, la-beled data can be in short supply but unla-beled data is more readily available.
Semi-supervised methods exploit unlabeled data in ad-dition to labeled data to improve performanceon classification tasks.
If the predictions of alearner l on unlabeled data are used to improvea learner l?
in semi-supervised learning, the ro-bustness of learning will depend on the stabil-ity of l. Combining ensemble-based and semi-supervised methods may thus lead to more ro-bust semi-supervised learning.Ensemble-based and semi-supervised meth-ods are some of the areas that receive most at-tention in machine learning today, but relativelylittle attention has been given to combining thesemethods (Zhou, 2009).
Semi-supervised learn-ing algorithms can be categorized with respectto the number of views, i.e.
the number of fea-ture sets, and the number of learners used to in-form each other (Hady and Schwenker, 2008).Self-training and expectation maximization areperhaps the best known semi-supervised learn-ing algorithms (Abney, 2008).
They are bothsingle-view and single-learner algorithms.
Sincethere is thus only a single perspective on data,1065selecting unlabeled data points with predictionsis a difficult task.
There is an imminent dangerthat the learner amplifies its previous mistakes,and while several techniques such as balancingand throttling have been developed to avoid suchcaveats, using single-view and single-learner al-gorithms often requires both caution and experi-ence with the modeling task at hand.Algorithms with multiple views on data areknown to be more robust.
This insight led to thedevelopment of co-training (Blum and Mitchell,1998), a two-view method where views informeach other, but it also paved the way for the inte-gration of ensemble-based and semi-supervisedmethods, i.e.
for methods with multiple learners.It was mentioned that relatively little work hasbeen devoted to this topic, but there are notableexceptions:Bennett et al (2003) generalized boosting tosemi-supervised learning in a seminal paper,where the idea of iterative or recursive ensembleswas also introduced.
Li and Zhou (2005) intro-duce tri-training, a form of co-training that trainsan ensemble of three learners on labeled data andruns them on unlabeled data.
If two learnersagree on their labeling of a data point, the datapoint is added to the labeled data of the thirdlearner with the prediction of the first two.
Di-daci and Roli (2006) extend self-training and co-training to multiple learners.
Li and Zhou (2007)generalize tri-training to larger ensembles of ran-dom trees.
The technique is also known as co-forests.
Hady and Schwenker (2008) general-ize existing ensemble-based methods for semi-supervised learning scenarios; in particular theyembed ensembles in a form of co-training that isshown to maintain the diversity of the ensembleover time.
Milidiu and Duarte (2009) generalizeboosting at start to semi-supervised learning.This paper applies a generalization of tri-training to two classification problems, attach-ment and labeling.
The attachment classifier?sweights are used for arc-factored dependencyparsing, and the labeling classifier?s weights arethen used to label the dependency tree deliveredby the parser.Semi-supervised dependency parsing has at-tracted a lot of attention recently (Koo et al,2008; Wang et al, 2008; Suzuki et al, 2009),but there has, to the best of our knowledge, beenno previous attempts to apply tri-training or re-lated combinations of ensemble-based and semi-supervised methods to any of these tasks, ex-cept for the work of Sagae and Tsujii (2007)discussed in Sect.
2.6.
However, tri-traininghas been applied to Chinese chunking (Chen etal., 2006), question classification (Nguyen et al,2008) and POS tagging (S?gaard, 2010).We compare generalized tri-training to othersemi-supervised learning algorithms, incl.
self-training, the original tri-training algorithm basedon bootstrap samples (Li and Zhou, 2005),co-forests (Li and Zhou, 2007) and semi-supervised support vector machines (Sindhwaniand Keerthi, 2006).Sect.
2 introduces dependency parsing andstacked learning.
Stacked learning is general-ized to dependency parsing, and previous work isbriefly surveyed.
We then describe how stackeddependency parsers can be further stacked as in-put for two end classifiers that can be combinedto produce dependency structures.
These twoclassifiers will learn multinomial variables (at-tachment and labeling) from a combination oflabeled data and unlabeled data using a gener-alization of tri-training.
Sect.
3 describes our ex-periments.
We describe the data sets, and howthe unlabeled data was prepared.
Sect.
4 presentsour results.
Sect.
5 presents an error analysis anddiscusses the results in light of other results inthe literature, and Sect.
6 concludes the paper.2 Background and related work2.1 Dependency parsingDependency parsing models a sentence as a treewhere words are vertices and grammatical func-tions are directed edges (dependencies).
Eachword thus has a single incoming edge, exceptone called the root of the tree.
Dependency pars-ing is thus a structured prediction problem withtrees as structured variables.
Each sentence hasexponentially many possible dependency trees.Our observed variables are sentences with wordslabeled with part-of-speech tags.
The task for1066each sentence is to find the dependency tree thatmaximizes an objective function which in ourcase is learned from a combination of labeledand unlabeled data.More formally, a dependency tree for asentence x = w1, .
.
.
, wn is a tree T =?
{0, 1, .
.
.
, n}, A?
with A ?
V ?
V the set ofdependency arcs.
Each vertex corresponds toa word in the sentence, except 0 which is theroot vertex, i.e.
for any i ?
n ?i, 0?
6?
A.Since a dependency tree is a tree it is acyclic.A tree is projective if every vertex has a continu-ous projection, i.e.
if and only if for every arc?i, j?
?
A and node k ?
V , if i < k < jor j < k < i then there is a subset of arcs{?i, i1?, ?i1, i2?, .
.
.
, ?ik?1, ik?}
?
A such thatik = k.In this paper we use a maximum spanning treealgorithm, the so-called Chu-Liu-Edmonds algo-rithm (CLE) (Edmonds, 1967) to turn the pre-dictions of our semi-supervised classifiers into adependency tree.2.2 Stacked learningStacked generalization, or simply stacking, wasfirst proposed by Wolpert (1992).
Stacking is anensemble-based learning method where multipleweak classifiers are combined in a strong endclassifier.
The idea is to train the end classifierdirectly on the predictions of the input classifiers.Say each input classifier ci with 1 ?
i ?n receives an input x and outputs a predictionci(x).
The end classifier then takes as input?x, c1(x), .
.
.
, cn(x)?
and outputs a final predic-tion c0(?x, c1(x), .
.
.
, cn(x)?).
Training is doneby cross-validation.
In sum, stacking is traininga classifier on the output of classifiers.2.3 Stacked dependency parsingStacked learning can be generalized to structuredprediction tasks such as dependency parsing.
Ar-chitectures for stacking dependency parsers typi-cally only use one input parser, but otherwise theintuition is the same: the input parser is used toaugment the dependency structures that the endparser is trained and evaluated on.Nivre and McDonald (2008) first showed howthe MSTParser (McDonald et al, 2005) and theMaltParser (Nivre et al, 2007) could be im-proved by stacking each parser on the predic-tions of the other.
Martins et al (2008) general-ized their work, considering more combinationsof parsers, and stacking the end parsers on non-local features from the predictions of the inputparser, e.g.
siblings and grand-parents.
In thiswork we use three stacked dependency parsersfor each language: mst2 (p1), malt/mst2 (p2) andmalt/mst1 (p3).The notation ?malt/mst2?
means that thesecond-order MSTParser has been stacked on theMaltParser.
The capital letters refer to featureconfigurations.
Configuration D stacks a level 1parser on several (non-local) features of the pre-dictions of the level 0 parser (along with the in-put features): the predicted edge, siblings, grandparents and predicted head of candidate modifierif predicted edge is 0.
Configuration E stacksa level 1 parser on the features in configurationD and all the predicted children of the candi-date head.
The chosen parser configurations arethose that performed best in Martins et al (2008)across the different datasets.2.4 Stacking stacked dependency parsingThe input features of the input classifiers instacked learning x can of course be removedfrom the input of the end classifier.
It is alsopossible to stack stacked classifiers.
This leavesus with four strategies for recursive stacking;namely to constantly augment the feature set,with level n classifiers trained on the predictionsof the classifiers at all n?
1 lower levels with orwithout the input features x, or simply to train alevel n classifier on the predictions of the leveln?
1 classifiers with or without x.In this work we stack stacked dependencyparsers by training classifiers on the output ofthree stacked dependency parsers and POS tags.Consequently, we use one of the features from x.Note that we train classifiers and not parsers onthis new level 2.The reduction is done the following way: Firstwe train a classifier on the relative distance froma word to its head to induce attachments.
Forexample, we may obtain the following featuresfrom the predictions of our level 1 parsers:1067label p1 p2 p3 POS1 1 -1 1 NNP0 0 0 0 VBDIn the second row all input parsers, p1?3 incolumnsaa 2?4, agree that the verb is the root ofthe sentence.
Column 1 tells us that this is cor-rect.
In the first row, two out of three parsersagree on attaching the noun to the verb, whichagain is correct.
We train level 2 classifiers onfeature vectors produced this way.
Note that or-acle performance of the ensemble is no upperbound on the accuracy of a classifier trained onlevel 1 predictions this way, since a classifiermay learn the right decision from three wrongpredictions and a POS tag.Second we train a classifier to predict depen-dency relations.
Our feature vectors are similarto the ones just described, but now contain de-pendency label predictions, e.g.
:label p1 p2 p3 POSSBJ SBJ SBJ SBJ NNROOT ROOT ROOT COORD VBN2.5 Generalized tri-trainingTri-training was originally introduced in Li andZhou (2005).
The method involves three learnersthat inform each other.Let L denote the labeled data and U theunlabeled data.
Assume that three classifiersc1, c2, c3 have been trained on L. In the origi-nal algorithm, the three classifiers are obtainedby applying the same learning algorithm to threebootstrap samples of the labeled data; but in gen-eralized algorithms, three different learning al-gorithms are used.
An unlabeled datapoint inU is labeled for a classifier, say c1, if the othertwo classifiers agree on its label, i.e.
c2 and c3.Two classifiers inform the third.
If the two clas-sifiers agree on a labeling, we assume there is agood chance that they are right.
In the originalalgorithm, learning stops when the classifiers nolonger change; in generalized tri-training, a fixedstopping criterion is used.
The three classifiersare combined by voting.
Li and Zhou (2005)show that under certain conditions the increasein classification noise rate is compensated by theamount of newly labeled data points.The most important condition is that thethree classifiers are diverse.
If the three clas-1: for i ?
{1..3} do2: ci ?
train classifier (li, L)3: end for4: repeat5: for i ?
{1..3} do6: for x ?
U do7: Li ?
?8: if cj(x) = ck(x)(j, k 6= i) then9: Li ?
Li ?
{(x, cj(x)}10: end if11: end for12: ci ?
train classifier(li, L ?
Li)13: end for14: until stopping criterion is met15: apply c1Figure 1: Generalized tri-training.sifiers are identical, tri-training degenerates toself-training.
As already mentioned, Li andZhou (2005) obtain this diversity by trainingclassifiers on bootstrap samples.
In their exper-iments, they consider classifiers based on deci-sion trees, BP neural networks and na?
?ve Bayesinference.In this paper we generalize the tri-training al-gorithm and use three different learning algo-rithms rather than bootstrap samples to creatediversity: a na?
?ve Bayes algorithm (no smooth-ing), random forests (Breiman, 2001) (with 100unpruned decision trees) and an algorithm thatinduces unpruned decision trees.
The overall al-gorithm is sketched in Figure 1 with li a learningalgorithm.Our weights are those of the random forestclassifier after a fixed number of rounds.
Theattachment classifier iterates once over the unla-beled data, while the dependency relations clas-sifier uses three iterations.
The optimal numberof iterations could of course be estimated on de-velopment data instead.
Given the weights for aninput sentence we use CLE to find its most likelydependency tree.2.6 Related workThis paper uses stacking rather than voting toconstruct ensembles, but voting has been more1068widely used in dependency parsing than stack-ing.
Voting was first introduced in dependencyparsing in Zeman and Zabokrtsky (2005).
Sagaeand Lavie (2006) later used weighted voting andreparsing, i.e.
using CLE to find the dependencytree that reflects the maximum number of votes.They also showed that binning the vote overpart-of-speech tags led to further improvements.This set-up was adopted by Hall et al (2007) inthe best performing system in the CONLL 2007Shared Task.
Fishel and Nivre (2009) later ex-perimented with binning the vote on other fea-tures with modest improvements.Semi-supervised dependency parsing has onlyrecently been explored, and failures have beenmore frequent than successes.
There are,however, noteable exceptions such as Koo etal.
(2008), Wang et al (2008), Suzuki etal.
(2009) and Sagae and Gordon (2009).The semi-supervised methods employed inthese experiments are very different from moretraditional scenarios such as self-training and co-training.
Two approaches (Koo et al, 2008;Sagae and Gordon, 2009) use clusters obtainedfrom large amounts of unlabeled data to augmenttheir labeled data by introducing new features,and two approaches (Wang et al, 2008; Suzuki etal., 2009) combine probability distributions ob-tained from labeled data with probability distri-butions obtained from unlabeled data.Successes with self-training and co-trainingare rare, and several authors report negative re-sults, e.g.
Spreyer and Kuhn (2009).
A note-able exception in constituent-based parsing is thework of McClosky et al (2006) who show thatself-training is possible if a reranker is used toinform the underlying parser.Sagae and Tsujii (2007) participated in (andwon) the CONLL 2007 Shared Task on do-main adaptation.
They first trained a max-imum entropy-based transition-based depen-dency parser on the out-of-domain labeled dataand an SVM-based transition-based dependencyparser on the reversed out-of-domain labeleddata.
The two parsers parse the in-domain la-beled data (reversed, in the case of the SVM-based parser).
Identical analyses are added to theoriginal training set.
The first parser is retrainedand used to parse the test data.
In sum, the au-thors do one round of co-training with the fol-lowing selection criterion: If the two parsers pro-duce the same dependency structures for a sen-tence, the dependency structure is added to thelabeled data.
This criterion is also the selectioncriterion in tri-training.3 Experiments3.1 DataWe use five datasets from the CONLL-X SharedTask (Buchholz and Marsi, 2006).1 Lemmas andmorphological features (FEATS) are ignored,since we only add POS and CPOS tags to un-labeled data.
For German and Swedish, weuse 100,000 sentences from the Leipzig CorporaCollection (Biemann et al, 2007) as unlabeleddata.
For Danish, Dutch, and Portuguese weuse 100,000 sentences from the Europarl cor-pus (Koehn, 2005).
The data characteristics areprovided in Figure 2.
The unlabeled data werePOS tagged using the freely available SVMTool(Gimenez and Marquez, 2004) (model 4, left-right-left).3.2 AlgorithmOnce our data has been prepared, we train thestacked dependency parsers and use them to la-bel training data for our classifiers (?4,000 to-kens), our test data and our unlabeled data.
Thisgives us three sets of predictions for each of thethree data sets.
Using the features described inSect.
2.4 we then construct data for training ourtwo triads of classifiers (for attachment and de-pendency relations).
The entire architecture canbe depicted as in Figure 3.We first stack three dependency parsers asdescribed in Martins et al (2008).
We thenstack three classifiers on top of these dependencyparsers (and POS tags): a na?
?ve Bayes classifier,a random forest, and a decision tree.
Finally,1The CONLL-X Shared Task consists of 12 datasets,but we did not have consistently tokenized unlabeled datafor Arabic, Chinese, Japanese, Slovene and Turkish.
Mar-tins et al (2008) ignore Czech.
Our experiment with theSpanish dataset crashed unexpectedly.
We will post resultson the website as soon as possible.1069tokens sents tokens/sents POSs DEPRELsDanish train 94,386 5,190 18.2 24 52unl (Europarl) 2,422,144 100,000 24.2 - -test 5,852 322 18.2 - -Dutch train 195,069 13,349 14.6 13 26unl (Europarl) 2,336,176 100,000 23.4 - -test 5,585 386 14.5 - -German train 699,610 39,216 17.8 52 46unl (LCC) 1,763,281 100,000 17.6 - -test 5,694 357 15.9 - -Portuguese train 206,678 9,071 22.3 21 55unl (Europarl) 2,882,967 100,000 28.8 - -test 5,867 288 22.8 - -Swedish train 191,467 11,042 17.4 37 56unl (LCC) 1,727,068 100,000 17.3 - -test 5,656 389 14.5 - -Figure 2: Characteristics of the data sets.tri-training...nb forests treestackingmst2/mst2 malt/mst2 malt/mst1stackingmst2 malt mst1Figure 3: Tri-training stacked classifiers.we tri-train these three stacked classifiers and foreach test sentence output the weights providedby the random forest classifier.
These weightsare used to find the best possible dependency treeusing CLE.3.3 BaselinesThe best of the stacked input parsers is of courseour natural baseline.Since we have generalized tri-training, wealso compare generalized tri-training to the orig-inal tri-training algorithm based on bootstrapsamples.
The original tri-training algorithmis run with the same decomposition and thesame features as our generalized tri-training al-gorithm.
We use the learning algorithm orig-inally used in Li and Zhou (2005), namelyC4.5.
We also compare our results to self-training (no pool, no growth rate) and co-forests(Li and Zhou, 2007).
Finally, we compare ourresults to semi-supervised support vector ma-chines (S3VMs) (Sindhwani and Keerthi, 2006).Since S3VMs produce binary classifiers, andone-vs.-many combination would be very time-consuming, we train a binary classifier that pro-duces a probability that any candidate arc is cor-rect and do greedy head selection.
We optimizedthe feature set and included a total of seven fea-tures (head POS, dependent POS, dependent leftneighbor POS, distance+direction, predictions ofthe three classifiers).4 ResultsOur results are presented in Figure 4.
Labeled(LAS) and unlabeled attachment scores (UAS)and labeling accuracy (LA) are defined as usualand include punctuation signs unless otherwisenoted.
Difference (?)
in LAS, error reductionand p-value compare our results to the best inputstacked parser (malt/mst2, excerpt for Swedish).Generalized tri-training (tri-training-CLE),i.e.
using CLE to find the best well-formed de-pendency trees given the weights provided byour tri-trained random forest classifier, leads tohighly significant improvements on all data sets(p < 0.001) with an average error reduction of14,9%.
The results for the other semi-supervisedlearning algorithms are presented in Figure 5.We only used 10% of the unlabeled data (10ksentences) in this experiment and only did un-labeled parsing, but it is quite evident that theselearning strategies seem less promising than gen-1070Danish LAS(%) UAS(%) LA(%) EM(%) ?
LAS err.red(%) p-valuemst2 84.64 89.11 91.35 24.84malt/mst2 86.36 90.50 92.09 27.64malt/mst1 86.11 90.23 91.87 25.78tri-training-CLE 87.76 92.11 92.87 27.95 1.40 10.26 <0.0001tri-training-CLE (excl.
pnc.)
87.54 92.61 91.68CONLL-X best (excl.
pnc.)
84.79 90.58 89.22Martins et al (excl.
pnc.)
86.79 - -Dutchmst2 80.27 84.32 84.96 23.32malt/mst2 81.00 84.58 85.46 24.35malt/mst1 80.72 84.17 85.34 26.17tri-training-CLE 83.42 88.18 87.82 28.00 2.42 12.74 <0.0001tri-training-CLE (excl.
pnc.)
81.73 86.97 86.61CONLL-X best (excl.
pnc.)
79.19 83.57 83.89Martins et al (excl.
pnc.)
81.61 - -Germanmst2 87.32 89.88 93.05 35.85malt/mst2 88.06 90.53 93.52 40.06malt/mst1 88.04 90.50 93.48 38.10tri-training-CLE 90.41 93.22 94.61 43.14 2.35 19.68 <0.0001tri-training-CLE (excl.
pnc.)
90.30 93.49 93.87CONLL-X best (excl.
pnc.)
87.34 90.38 92.11Martins et al (excl.
pnc.)
88.66 - -Portuguesemst2 84.83 88.44 92.04 25.69malt/mst2 85.39 88.80 92.59 28.13malt/mst1 85.00 88.39 92.23 25.69tri-training-CLE 88.03 91.89 93.54 29.86 2.64 18.07 <0.0001tri-training-CLE (excl.
pnc.)
89.18 93.69 92.43CONLL-X best (excl.
pnc.)
87.60 91.36 91.54Martins et al (excl.
pnc.)
88.46 - -Swedishmst2 81.82 87.36 87.29 27.76malt/mst2 84.42 89.57 88.68 31.62malt/mst1 84.74 89.83 89.07 31.11tri-training-CLE 86.83 92.04 90.65 32.65 2.09 13.70 <0.0001tri-training-CLE (excl.
pnc.)
86.66 92.45 89.58CONLL-X best (excl.
pnc.)
84.58 89.50 87.39Martins et al (excl.
pnc.)
85.16 - -AV 2.18 14.89Figure 4: Results on CONLL-X datasets.
Scores are including punctuation unless otherwise noted.?
and p-value is difference with respect to best input parser.UAS malt-mst2 S3VMs self-training orig-tri-training co-forests tri-training tri-training[full]Danish 90.50 90.47 89.68 89.66 88.79 90.60 92.21Dutch 84.58 85.34 84.06 83.83 83.97 86.07 88.06German 90.53 90.15 89.83 89.92 88.47 90.81 93.20Portuguese 88.80 65.64 87.60 87.62 87.06 89.16 91.87Swedish 89.83 81.46 89.09 89.20 88.65 90.22 92.24AV 88.80 82.61 88.05 88.05 87.44 89.37 91.52Figure 5: Comparison of different semi-supervised learning algorithms (10% of unlabeled data)using 2-fold CV and no reparsing, UAS including punctuation.1071eralized tri-training.5 Error analysis and discussionError reductions are higher with dependenciesto the root node and long distance dependenciesthan with local dependencies.
The table belowlists the labeled attachment F1-scores for the fivedatasets binned on dependency length.
The av-erage error reduction is the same for root depen-dencies and long distance dependencies (length>7), but significantly lower for local dependen-cies.
This seems to indicate that large amounts ofdata are necessary for the parser to recover longdistance dependencies.root 1 2 4?7 >7Da(F1) 98.45 96.21 92.09 88.17 90.93?
err.red 41.34 10.69 13.92 15.75 21.92Du(F1) 83.65 94.47 88.60 82.40 81.54?
err.red 28.39 16.74 20.72 17.00 31.88Ge(F1) 97.33 96.47 94.28 92.42 93.94?
err.red 26.65 19.77 17.46 25.25 38.97Po(F1) 96.23 97.05 95.17 84.80 87.11?
err.red 22.47 19.56 24.86 22.56 26.97Sw(F1) 96.37 95.67 93.46 88.42 89.57?
err.red 32.85 14.10 15.04 25.97 31.50AV err.red 30.34 16.17 18.40 21.31 30.25Our results for Danish, Dutch, German andPortuguese are to the best of our knowledge thebest reported results in the literature.
Zhang andChan (2009) obtain a LAS of 87.20 for Swedishwith transition-based parsing based on reinforce-ment learning.
They evaluate their system ona subset of the CONLL-X datasets and obtaintheir (by far) best improvement on the Swedishdataset.
They speculate that ?the reason mightbe that [long distance dependencies] are not pop-ular in Swedish?.
Since our parser is particu-larly good at long distance dependencies, thismay also explain why a supervised parser outper-forms our system on this dataset.
Interestingly,our unlabeled attachment score is a lot betterthan the one reported by Zhang and Chan (2009),namely 92.45 compared to 91.84.Generally, our UASs are better than our LASs.Since we separate attachment and labeling outin two independent steps, improvements in UASand improvements in LA do not necessarily leadto improvements in LAS.
While our average er-ror reduction in LAS is 14.9%, our average errorreductions in UAS is 23.6%.
The average errorreduction in LA is 14.0%.
In two-stage depen-dency parsers or dependency parsers with jointmodels, improvements in UAS are typically fol-lowed by comparable improvements in LAS.6 ConclusionThis paper showed how the stacked depen-dency parsers introduced in Martins et al (2008)can be improved by inference from unlabeleddata.
Briefly put, we stack three diverse clas-sifiers on triads of stacked dependency parsersand let them label unlabeled data for eachother in a co-training-like architecture.
Ouraverage error reductions in LAS over the bestof our stacked input parsers is 14.9%; inUAS, it is 23.6%.
The code is available athttp://cst.dk/anders/tridep.html.ReferencesAbney, Steven.
2008.
Semi-supervised learning forcomputational linguistics.
Chapman & Hall.Bennett, Kristin, Ayhan Demiriz, and RichardMaclin.
2003.
Exploiting unlabeled data in en-semble methods.
In KDD.Biemann, Chris, G. Heyer, U. Quasthoff, andM.
Richter.
2007.
The Leipzig corpora collection.In Corpus Linguistics.Blum, Avrim and Tom Mitchell.
1998.
Combininglabeled and unlabeled with-co-training.
In COLT.Breiman, Leo.
2001.
Random forests.
MachineLearning, 45:5?32.Buchholz, Sabine and Erwin Marsi.
2006.
CONLL-X shared task on multilingual dependency parsing.In CONLL.Chen, Wenliang, Yujie Zhang, and Hitoshi Isahara.2006.
Chinese chunking with tri-training learning.In Computer processing of oriental languages,pages 466?473.
Springer, Berlin, Germany.Didaci, Luca and Fabio Roli.
2006.
Using co-training and self-training in semi-supervised mul-tiple classifier systems.
In SSPR& SPR, pages522?530.
Springer, Berlin, Germany.Edmonds, J.
1967.
Optimum branchings.
Journalof Research of the National Bureau of Standards,71:233?240.1072Fishel, Mark and Joakim Nivre.
2009.
Voting andstacking in data-driven dependency parsing.
InNODALIDA.Gimenez, Jesus and Lluis Marquez.
2004.
SVM-Tool: a general POS tagger generator based onsupport vector machines.
In LREC.Hady, Mohamed and Friedhelm Schwenker.
2008.Co-training by committee.
International Journalof Software and Informatics, 2:95?124.Hall, Johan, Jens Nilsson, Joakim Nivre, GulsenEryigit, Beata Megyesi, Mattias Nilsson, andMarkus Saers.
2007.
Single malt or blended?
InCONLL.Koehn, Philipp.
2005.
Europarl: a parallel corpus forstatistical machine translation.
In MT-Summit.Koo, Terry, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency pars-ing.
In ACL.Li, Ming and Zhi-Hua Zhou.
2005.
Tri-training:exploiting unlabeled data using three classifiers.IEEE Transactions on Knowledge and Data En-gineering, 17(11):1529?1541.Li, Ming and Zhi-Hua Zhou.
2007.
Improvecomputer-aided diagnosis with machine learningtechniques using undiagnosed samples.
IEEETransactions on Systems, Man and Cybernetics,37(6):1088?1098.Martins, Andre?, Dipanjan Das, Noah Smith, and EricXing.
2008.
Stacking dependency parsers.
InEMNLP.McClosky, David, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InHLT-NAACL.McDonald, Ryan, Fernando Pereira, Kiril Ribarov,and Jan Hajic?.
2005.
Non-projective dependencyparsing using spanning tree algorithms.
In HLT-EMNLP.Milidiu, Ruy and Julio Duarte.
2009.
Improv-ing BAS committee performance with a semi-supervised approach.
In European Symposium onArtificial Neural Networks.Nguyen, Tri, Le Nguyen, and Akira Shimazu.
2008.Using semi-supervised learning for question clas-sification.
Journal of Natural Language Process-ing, 15:3?21.Nivre, Joakim and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In ACL-HLT.Nivre, Joakim, Johan Hall, Jens Nilsson, AtanasChanev, Gu?lsen Eryigit, Sandra Ku?bler, SvetoslavMarinov, and Erwin Marsi.
2007.
MaltParser.Natural Language Engineering, 13(2):95?135.Sagae, Kenji and Andrew Gordon.
2009.
Cluster-ing words by syntactic similarity improves depen-dency parsing of predicate-argument structures.
InIWPT.Sagae, Kenji and Alon Lavie.
2006.
Parser combina-tion by reparsing.
In HLT-NAACL.Sagae, Kenji and Jun?ichi Tsujii.
2007.
Dependencyparsing and domain adaptation with lr models andparser ensembles.
In EMNLP-CONLL.Sindhwani, Vikas and Sathiya Keerthi.
2006.
Largescale semi-supervised linear SVMs.
In ACM SI-GIR.S?gaard, Anders.
2010.
Simple semi-supervisedtraining of part-of-speech taggers.
In ACL.Spreyer, Kathrin and Jonas Kuhn.
2009.
Data-drivendependency parsing of new languages using in-complete and noisy training data.
In CONLL.Surdeanu, Mihai and Christopher Manning.
2010.Ensemble models for dependency parsing: cheapand good?
In NAACL.Suzuki, Jun, Hideki Isozaki, Xavier Carreras, andMichael Collins.
2009.
Semi-supervised convextraining for dependency parsing.
In EMNLP.Wang, Qin, Dekang Lin, and Dale Schuurmans.2008.
Semi-supervised convex training for depen-dency parsing.
In ACL.Wolpert, David.
1992.
Stacked generalization.
Neu-ral Networks, 5:241?259.Zeman, Daniel and Zdene?k ?Zabokrtsky?.
2005.
Im-proving parsing accuracy by combining diversedependency parsers.
In IWPT.Zhang, Lidan and Kwok Chan.
2009.
Dependencyparsing with energy-based reinforcement learning.In IWPT.Zhou, Zhi-Hua.
2009.
When semi-supervised learn-ing meets ensemble learning.
In MCS.1073
