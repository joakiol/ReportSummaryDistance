Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 715?725,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsAutomatic Text Scoring Using Neural NetworksDimitrios AlikaniotisDepartment of Theoreticaland Applied LinguisticsUniversity of CambridgeCambridge, UKda352@cam.ac.ukHelen YannakoudakisThe ALTA InstituteComputer LaboratoryUniversity of CambridgeCambridge, UKhy260@cl.cam.ac.ukMarek ReiThe ALTA InstituteComputer LaboratoryUniversity of CambridgeCambridge, UKmr472@cl.cam.ac.ukAbstractAutomated Text Scoring (ATS) providesa cost-effective and consistent alternativeto human marking.
However, in orderto achieve good performance, the pre-dictive features of the system need tobe manually engineered by human ex-perts.
We introduce a model that formsword representations by learning the ex-tent to which specific words contribute tothe text?s score.
Using Long-Short TermMemory networks to represent the mean-ing of texts, we demonstrate that a fullyautomated framework is able to achieveexcellent results over similar approaches.In an attempt to make our results moreinterpretable, and inspired by recent ad-vances in visualizing neural networks, weintroduce a novel method for identifyingthe regions of the text that the model hasfound more discriminative.1 IntroductionAutomated Text Scoring (ATS) refers to the set ofstatistical and natural language processing tech-niques used to automatically score a text on amarking scale.
The advantages of ATS systemshave been established since Project Essay Grade(PEG) (Page, 1967; Page, 1968), one of the earli-est systems whose development was largely moti-vated by the prospect of reducing labour-intensivemarking activities.
In addition to providing acost-effective and efficient approach to large-scalegrading of (extended) text, such systems ensure aconsistent application of marking criteria, there-fore facilitating equity in scoring.There is a large body of literature with re-gards to ATS systems of text produced by non-native English-language learners (Page, 1968; At-tali and Burstein, 2006; Rudner and Liang, 2002;Elliot, 2003; Landauer et al, 2003; Briscoe et al,2010; Yannakoudakis et al, 2011; Sakaguchi etal., 2015, among others), overviews of which canbe found in various studies (Williamson, 2009;Dikli, 2006; Shermis and Hammer, 2012).
Im-plicitly or explicitly, previous work has primarilytreated text scoring as a supervised text classifica-tion task, and has utilized a large selection of tech-niques, ranging from the use of syntactic parsers,via vectorial semantics combined with dimension-ality reduction, to generative and discriminativemachine learning.As multiple factors influence the quality oftexts, ATS systems typically exploit a large rangeof textual features that correspond to differentproperties of text, such as grammar, vocabulary,style, topic relevance, and discourse coherenceand cohesion.
In addition to lexical and part-of-speech (POS) ngrams, linguistically deeper fea-tures such as types of syntactic constructions,grammatical relations and measures of sentencecomplexity are among some of the properties thatform an ATS system?s internal marking criteria.The final representation of a text typically consistsof a vector of features that have been manually se-lected and tuned to predict a score on a markingscale.Although current approaches to scoring, suchas regression and ranking, have been shown toachieve performance that is indistinguishable fromthat of human examiners, there is substantial man-ual effort involved in reaching these results on dif-ferent domains, genres, prompts and so forth.
Lin-guistic features intended to capture the aspects ofwriting to be assessed are hand-selected and tunedfor specific domains.
In order to perform well ondifferent data, separate models with distinct fea-ture sets are typically tuned.715Prompted by recent advances in deep learningand the ability of such systems to surpass state-of-the-art models in similar areas (Tang, 2015; Tai etal., 2015), we propose the use of recurrent neuralnetwork models for ATS.
Multi-layer neural net-works are known for automatically learning use-ful features from data, with lower layers learn-ing basic feature detectors and upper levels learn-ing more high-level abstract features (Lee et al,2009).
Additionally, recurrent neural networks arewell-suited for modeling the compositionality oflanguage and have been shown to perform verywell on the task of language modeling (Mikolovet al, 2011; Chelba et al, 2013).
We thereforepropose to apply these network structures to thetask of scoring, in order to both improve the per-formance of ATS systems and learn the requiredfeature representations for each dataset automat-ically, without the need for manual tuning.
Morespecifically, we focus on predicting a holistic scorefor extended-response writing items.1However, automated models are not a panacea,and their deployment depends largely on the abil-ity to examine their characteristics, whether theymeasure what is intended to be measured, andwhether their internal marking criteria can be in-terpreted in a meaningful and useful way.
Thedeep architecture of neural network models, how-ever, makes it rather difficult to identify and ex-tract those properties of text that the network hasidentified as discriminative.
Therefore, we alsodescribe a preliminary method for visualizing theinformation the model is exploiting when assign-ing a specific score to an input text.2 Related WorkIn this section, we describe a number of the moreinfluential and/or recent approaches in automatedtext scoring of non-native English-learner writing.Project Essay Grade (Page, 1967; Page, 1968;Page, 2003) is one of the earliest automated scor-ing systems, predicting a score using linear regres-sion over vectors of textual features considered tobe proxies of writing quality.
Intelligent EssayAssessor (Landauer et al, 2003) uses Latent Se-mantic Analysis to compute the semantic similar-ity between texts at specific grade points and a testtext, which is assigned a score based on the ones in1The task is also referred to as Automated Essay Scoring.Throughout this paper, we use the terms text and essay (scor-ing) interchangeably.the training set to which it is most similar.
Lons-dale and Strong-Krause (2003) use the Link Gram-mar parser (Sleator and Templerley, 1995) to anal-yse and score texts based on the average sentence-level scores calculated from the parser?s cost vec-tor.The Bayesian Essay Test Scoring sYstem (Rud-ner and Liang, 2002) investigates multinomial andBernoulli Naive Bayes models to classify textsbased on shallow content and style features.
e-Rater (Attali and Burstein, 2006), developed bythe Educational Testing Service, was one of thefirst systems to be deployed for operational scor-ing in high-stakes assessments.
The model usesa number of different features, including aspectsof grammar, vocabulary and style (among others),whose weights are fitted to a marking scheme byregression.Chen et al (2010) use a voting algorithm andaddress text scoring within a weakly supervisedbag-of-words framework.
Yannakoudakis et al(2011) extract deep linguistic features and employa discriminative learning-to-rank model that out-performs regression.Recently, McNamara et al (2015) used a hier-achical classification approach to scoring, utilizinglinguistic, semantic and rhetorical features, amongothers.
Farra et al (2015) utilize variants of lo-gistic and linear regression and develop modelsthat score persuasive essays based on features ex-tracted from opinion expressions and topical ele-ments.There have also been attempts to incorporatemore diverse features to text scoring models.
Kle-banov and Flor (2013) demonstrate that essayscoring performance is improved by adding to themodel information about percentages of highlyassociated, mildly associated and dis-associatedpairs of words that co-exist in a given text.
So-masundaran et al (2014) exploit lexical chains andtheir interaction with discourse elements for evalu-ating the quality of persuasive essays with respectto discourse coherence.
Crossley et al (2015)identify student attributes, such as standardizedtest scores, as predictive of writing success anduse them in conjunction with textual features todevelop essay scoring models.In 2012, Kaggle,2sponsored by the HewlettFoundation, hosted the Automated Student As-sessment Prize (ASAP) contest, aiming to demon-2http://www.kaggle.com/c/asap-aes/716strate the capabilities of automated text scoringsystems (Shermis, 2015).
The dataset releasedconsists of around twenty thousand texts (60% ofwhich are marked), produced by middle-schoolEnglish-speaking students, which we use as partof our experiments to develop our models.3 Models3.1 C&W EmbeddingsCollobert and Weston (2008) and Collobert et al(2011) introduce a neural network architecture(Fig.
1a) that learns a distributed representation foreach word w in a corpus based on its local context.Concretely, suppose we want to learn a represen-tation for some target word wtfound in an n-sizedsequence of words S = (w1, .
.
.
, wt, .
.
.
, wn)based on the other words which exist in the samesequence (?wi?
S |wi6= wt).
In order to derivethis representation, the model learns to discrimi-nate between S and some ?noisy?
counterpart S?in which the target word wthas been substitutedfor a randomly sampled word from the vocabu-lary: S?= (w1, .
.
.
, wc, .
.
.
, wn|wc?
V).
In thisway, every word w is more predictive of its localcontext than any other random word in the corpus.Every word in V is mapped to a real-valuedvector in ?
via a mapping function C(?)
suchthat C(wi) = ?M?i?, where M ?
RD?|V|isthe embedding matrix and ?M?i?
is the ith col-umn of M. The network takes S as input byconcatenating the vectors of the words found init; st= ?C(w1)??
.
.
.
?C(wt)??
.
.
.
?C(wn)??
?RnD.
Similarly, S?is formed by substitutingC(wt) for C(wc) ?M |wc6= wt.The input vector is then passed through ahard tanh layer defined as,htanh(x) =?????
?1 x < ?1x ?1 6 x 6 11 x > 1(1)which feeds a single linear unit in the output layer.The function that is computed by the network isultimately given by (4):st= ?M??1?
.
.
.
?M??t?
.
.
.
?M??n??
(2)i = ?
(Whist+ bh) (3)f(st) = Wohi + bo(4)f(s),bo?
R1Woh?
RH?1Whi?
RD?Hs ?
RDbo?
RHwhere M,Woh,Whi,bo,bhare learnable param-eters, D,H are hyperparameters controlling thesize of the input and the hidden layer, respectively;?
is the application of an element-wise non-linearfunction (htanh in this case).The model learns word embeddings by rankingthe activation of the true sequence S higher thanthe activation of its ?noisy?
counterpart S?.
Theobjective of the model then becomes to minimizethe hinge loss which ensures that the activationsof the original and ?noisy?
ngrams will differ byat least 1:losscontext(target, corrupt) =[1?
f(st) + f(sck)]+, ?k ?
ZE(5)whereE is another hyperparameter controlling thenumber of ?noisy?
sequences we give along withthe correct sequence (Mikolov et al, 2013; Gut-mann and Hyv?arinen, 2012).3.2 Augmented C&W modelFollowing Tang (2015), we extend the previousmodel to capture not only the local linguistic en-vironment of each word, but also how each wordcontributes to the overall score of the essay.
Theaim here is to construct representations which,along with the linguistic information given by thelinear order of the words in each sentence, are ableto capture usage information.
Words such as is,are, to, at which appear with any essay score areconsidered to be under-informative in the sensethat they will activate equally both on high and lowscoring essays.
Informative words, on the otherhand, are the ones which would have an impact onthe essay score (e.g., spelling mistakes).In order to capture those score-specific wordembeddings (SSWEs), we extend (4) by adding afurther linear unit in the output layer that performslinear regression, predicting the essay score.
Us-ing (2), the activations of the network (presentedin Fig.
1b) are given by:717. .
.
.
.
.
.
.
.therecentadvances(a).
.
.
.
.
.
.
.
.therecentadvances(b)Figure 1: Architecture of the original C&W model (left) and of our extended version (right).fss(s) = Woh1i + bo1(6)fcontext(s) = Woh2i + bo2(7)fss(s) ?
[min(score), max(score)]bo1?
R1Woh1?
R1?HThe error we minimize for fss(where ss stands forscore specific) is the mean squared error betweenthe predicted y?
and the actual essay score y:lossscore(s) =1NN?i=1(y?i?
yi)2(8)From (5) and (8) we compute the overall lossfunction as a weighted linear combination of thetwo loss functions (9), back-propagating the errorgradients to the embedding matrix M:lossoverall(s) =?
?
losscontext(s, s?
)+ (1?
?)
?
lossscore(s)(9)where ?
is the hyper-parameter determining howthe two error functions should be weighted.
?
val-ues closer to 0 will place more weight on the score-specific aspect of the embeddings, whereas valuescloser to 1 will favour the contextual information.Fig.
2 shows the advantage of using SSWEs inthe present setting.
Based solely on the informa-tion provided by the linguistic environment, wordssuch as computer and laptop are going to be placedtogether with their mis-spelled counterparts cop-muter and labtop (Fig.
2a).
This, however, doesnot reflect the fact that the mis-spelled words tendto appear in lower scoring essays.
Using SSWEs,the correctly spelled words are pulled apart in thevector space from the incorrectly spelled ones, re-taining, however, the information that labtop andcopmuter are still contextually related (Fig.
2b).3.3 Long-Short Term Memory NetworkWe use the SSWEs obtained by our model toderive continuous representations for each essay.We treat each essay as a sequence of tokensand explore the use of uni- and bi-directional(Graves, 2012) Long-Short Term Memory net-works (LSTMs) (Hochreiter and Schmidhuber,1997) in order to embed these sequences in a vec-tor of fixed size.
Both uni- and bi-directionalLSTMs have been effectively used for embeddinglong sequences (Hermann et al, 2015).
LSTMsare a kind of recurrent neural network (RNN) ar-chitecture in which the output at time t is condi-tioned on the input s both at time t and at timet?
1:yt= Wyhht+ by(10)ht= H(Whsst+ Whhht?1+ bh) (11)where stis the input at time t, and H is usuallyan element-wise application of a non-linear func-tion.
In LSTMs, H is substituted for a compositefunction defining htas:it=?
(Wisst+ Wihht?1+Wicct?1+ bi)(12)ft=?
(Wfsst+ Wfhht?1+Wfcct?1+ bf)(13)ct=itg(Wcsst+ Wchht?1+ bc)+ftct?1(14)7181 2 3 4123COPMUTARCOMPUTERLAPTOPLABTOP(a)Standard neural embeddings1 2 3 4123COPMUTARCOMPUTERLAPTOPLABTOP(b)Score-specific word embeddingsFigure 2: Comparison between standard and score-specific word embeddings.
By virtue of appearing insimilar environments, standard neural embeddings will place the correct and the incorrect spelling closerin the vector space.
However, since the mistakes are found in lower scoring essays, SSWEs are able todiscriminate between the correct and the incorrect versions without loss in contextual meaning.thewtherecentwrecentadvanceswadvances.
.
.w...??h?
?hyFigure 3: A single-layer Long Short Term Mem-ory (LSTM) network.
The word vectors wienterthe input layer one at a time.
The hidden layerthat has been formed at the last timestep is usedto predict the essay score using linear regression.We also explore the use of bi-directional LSTMs(dashed arrows).
For ?deeper?
representations, wecan stack more LSTM layers after the hidden layershown here.ot=?
(Wosst+ Wohht?1+Wocct+ bo)(15)ht= oth(ct) (16)where g, ?
and h are element-wise non-linearfunctions such as the logistic sigmoid (11+e?x) andthe hyperbolic tangent (e2z?1e2z+1); is the Hadamardproduct; W,b are the learned weights and biasesrespectively; and i, f, o and c are the input, forget,output gates and the cell activation vectors respec-tively.Training the LSTM in a uni-directional manner(i.e., from left to right) might leave out importantinformation about the sentence.
For example, ourinterpretation of a word at some point timight bedifferent once we know the word at ti+5.
An ef-fective way to get around this issue has been totrain the LSTM in a bidirectional manner.
This re-quires doing both a forward and a backward passof the sequence (i.e., feeding the words from leftto right and from right to left).
The hidden layerelement in (10) can therefore be re-written as theconcatenation of the forward and backward hiddenvectors:yt= Wyh(??h?t?
?h?t)+ by(17)We feed the embedding of each word foundin each essay to the LSTM one at a time,zero-padding shorter sequences.
We form D-dimensional essay embeddings by taking the ac-tivation of the LSTM layer at the timestep wherethe last word of the essay was presented to the net-work.
In the case of bi-directional LSTMs, the twoindependent passes of the essay (from left to rightand from right to left) are concatenated together topredict the essay score.
These essay embeddingsare then fed to a linear unit in the output layerwhich predicts the essay score (Fig.
3).
We use themean square error between the predicted and thegold score as our loss function, and optimize withRMSprop (Dauphin et al, 2015), propagating theerrors back to the word embeddings.33The maximum time for jointly training a particular SSWE+ LSTM combination took about 55?60 hours on an Ama-zon EC2 g2.2xlarge instance (average time was 27?30hours).7193.4 Other BaselinesWe train a Support Vector Regression model (seeSection 4), which is one of the most widely usedapproaches in text scoring.
We parse the data us-ing the RASP parser (Briscoe et al, 2006) andextract a number of different features for assess-ing the quality of the essays.
More specifically,we use character and part-of-speech unigrams, bi-grams and trigrams; word unigrams, bigrams andtrigrams where we replace open-class words withtheir POS; and the distribution of common nouns,prepositions, and coordinators.
Additionally, weextract and use as features the rules from thephrase-structure tree based on the top parse foreach sentence, as well as an estimate of the errorrate based on manually-derived error rules.Ngrams are weighted using tf?idf, while the restare count-based and scaled so that all features haveapproximately the same order of magnitude.
Thefinal input vectors are unit-normalized to accountfor varying text-length biases.Further to the above, we also explore the useof the Distributed Memory Model of ParagraphVectors (PV-DM) proposed by Le and Mikolov(2014), as a means to directly obtain essay embed-dings.
PV-DM takes as input word vectors whichmake up ngram sequences and uses those to pre-dict the next word in the sequence.
A feature ofPV-DM, however, is that each ?paragraph?
is as-signed a unique vector which is used in the predic-tion.
This vector, therefore, acts as a ?memory?,retaining information from all contexts that haveappeared in this paragraph.
Paragraph vectors arethen fed to a linear regression model to obtain es-say scores (we refer to this model as doc2vec).Additionally, we explore the effect of our score-specific method for learning word embeddings,when compared against three different kinds ofword embeddings:?
word2vec embeddings (Mikolov et al,2013) trained on our training set (see Sec-tion 4).?
Publicly available word2vec embeddings(Mikolov et al, 2013) pre-trained on theGoogle News corpus (ca.
100 billion words),which have been very effective in capturingsolely contextual information.?
Embeddings that are constructed on the fly bythe LSTM, by propagating the errors from itshidden layer back to the embedding matrix(i.e., we do not provide any pre-trained wordembeddings).44 DatasetThe Kaggle dataset contains 12.976 essays rang-ing from 150 to 550 words each, marked by tworaters (Cohen?s ?
= 0.86).
The essays were writ-ten by students ranging from Grade 7 to Grade10, comprising eight distinct sets elicited by eightdifferent prompts, each with distinct marking cri-teria and score range.5For our experiments, weuse the resolved combined score between the tworaters, which is calculated as the average betweenthe two raters?
scores (if the scores are close), oris determined by a third expert (if the scores arefar apart).
Currently, the state-of-the-art on thisdataset has achieved a Cohen?s ?
= 0.81 (usingquadratic weights).
However, the test set was re-leased without the gold score annotations, render-ing any comparisons futile, and we are thereforerestricted in splitting the given training set to cre-ate a new test set.The sets where divided as follows: 80%of the entire dataset was reserved for train-ing/validation, and 20% for testing.
80% ofthe training/validation subset was used for actualtraining, while the remaining 20% for validation(in absolute terms for the entire dataset: 64% train-ing, 16% validation, 20% testing).
To facilitatefuture work, we release the ids of the validationand test set essays we used in our experiments, inaddition to our source code and various hyperpa-rameter values.65 Experiments5.1 ResultsThe hyperparameters for our model were as fol-lows: sizes of the layers H , D, the learning rate?, the window size n, the number of ?noisy?
se-quences E and the weighting factor ?.
Also thehyperparameters of the LSTM were the size of theLSTM layer DLSTMas well as the dropout rate r.4Another option would be to use standard C&W embed-dings; however, this is equivalent to using SSWEs with?
= 1,which we found to produce low results.5Five prompts employed a holistic scoring rubric, one wasscored with a two-trait rubric, and two were scored with amulti-trait rubric, but reported as a holistic score (Shermisand Hammer, 2012).6The code, by-model hyperparameter configurations andthe IDs of the testing set are available at https://github.com/dimalik/ats/.720Model Spearman?s ?
Pearson r RMSE Cohen?s ?doc2vec 0.62 0.63 4.43 0.85SVM 0.78 0.77 8.85 0.75LSTM 0.59 0.60 6.8 0.54BLSTM 0.7 0.5 7.32 0.36Two-layer LSTM 0.58 0.55 7.16 0.46Two-layer BLSTM 0.68 0.52 7.31 0.48word2vec + LSTM 0.68 0.77 5.39 0.76word2vec + BLSTM 0.75 0.86 4.34 0.85word2vec + Two-layer LSTM 0.76 0.71 6.02 0.69word2vec + Two-layer BLSTM 0.78 0.83 4.79 0.82word2vecpre-trained+ Two-layer BLSTM 0.79 0.91 3.2 0.92SSWE + LSTM 0.8 0.94 2.9 0.94SSWE + BLSTM 0.8 0.92 3.21 0.95SSWE + Two-layer LSTM 0.82 0.93 3 0.94SSWE + Two-layer BLSTM 0.91 0.96 2.4 0.96Table 1: Results of the different models on the Kaggle dataset.
All resulting vectors were trainedusing linear regression.
We optimized the parameters using a separate validation set (see text)and report the results on the test set.Since the search space would be massive for gridsearch, the best hyperparameters were determinedusing Bayesian Optimization (Snoek et al, 2012).In this context, the performance of our models inthe validation set is modeled as a sample from aGaussian process (GP) by constructing a proba-bilistic model for the error function and then ex-ploiting this model to make decisions about whereto next evaluate the function.
The hyperparame-ters for our baselines were also determined usingthe same methodology.All models are trained on our trainingset (see Section 4), except the one prefixed?word2vecpre-trained?
which uses pre-trained em-beddings on the Google News Corpus.
We re-port the Spearman?s rank correlation coefficient ?,Pearson?s product-moment correlation coefficientr, and the root mean square error (RMSE) be-tween the predicted scores and the gold standardon our test set, which are considered more appro-priate metrics for evaluating essay scoring systems(Yannakoudakis and Cummins, 2015).
However,we also report Cohen?s ?
with quadratic weights,which was the evaluation metric used in the Kag-gle competition.
Performance of the models isshown in Table 1.In terms of correlation, SVMs produce com-petitive results (?
= 0.78 and r = 0.77), out-performing doc2vec, LSTM and BLSTM, aswell as their deep counterparts.
As describedabove, the SVM model has rich linguistic knowl-edge and consists of hand-picked features whichhave achieved excellent performance in similartasks (Yannakoudakis et al, 2011).
However, interms of RMSE, it is among the lowest performingmodels (8.85), together with ?BLSTM?
and ?Two-layer BLSTM?.
Deep models in combinationwith word2vec (i.e., ?word2vec + Two-layerLSTM?
and ?word2vec + Two-layer BLSTM?
)and SVMs are comparable in terms of r and ?,though not in terms of RMSE, where the formerproduce better results, with RMSE improving byhalf (4.79).
doc2vec also produces competitiveRMSE results (4.43), though correlation is muchlower (?
= 0.62 and r = 0.63).The two BLSTMs trained with word2vec em-beddings are among the most competitive modelsin terms of correlation and outperform all the mod-els, except the ones using pre-trained embeddingsand SSWEs.
Increasing the number of hidden lay-ers and/or adding bi-directionality does not alwaysimprove performance, but it clearly helps in thiscase and performance improves compared to theiruni-directional counterparts.Using pre-trained word embeddings improvesthe results further.
More specifically, we found?word2vecpre-trained+ Two-layer BLSTM?
to bethe best configuration, increasing correlation to0.79 ?
and 0.91 r, and reducing RMSE to 3.2.We note however that this is not an entirely721fair comparison as these are trained on a muchlarger corpus than our training set (which we useto train our models).
Nevertheless, when weuse our SSWEs models we are able to outper-form ?word2vecpre-trained+ Two-layer BLSTM?,even though our embeddings are trained on fewerdata points.
More specifically, our best model(?SSWE + Two-layer BLSTM?)
improves correla-tion to ?
= 0.91 and r = 0.96, as well as RMSEto 2.4, giving a maximum increase of around 10%in correlation.
Given the results of the pre-trainedmodel, we believe that the performance of our bestSSWE model will further improve should moretraining data be given to it.75.2 DiscussionOur SSWE + LSTM approach having no priorknowledge of the grammar of the language or thedomain of the text, is able to score the essays ina very human-like way, outperforming other state-of-the-art systems.
Furthermore, while we tunedthe models?
hyperparameters on a separate vali-dation set, we did not perform any further pre-processing of the text other than simple tokeniza-tion.In the essay scoring literature, text length tendsto be a strong predictor of the overall score.
Inorder to investigate any possible effects of essaylength, we also calculate the correlation betweenthe gold scores and the length of the essays.
Wefind that the correlations on the test set are rela-tively low (r = 0.3, ?
= 0.44), and therefore con-clude that there are no such strong effects.As described above, we used Bayesian Op-timization to find optimal hyperparameter con-figurations in fewer steps than in regular gridsearch.
Using this approach, the optimizationmodel showed some clear preferences for someparameters which were associated with betterscoring models:8the number of ?noisy?
sequencesE, the weighting factor ?
and the size of theLSTM layer DLSTM.
The optimal ?
value wasconsistently set to 0.1, which shows that our SSWEapproach was necessary to capture the usage ofthe words.
Performance dropped considerably as?
increased (less weight on SSWEs and more onthe contextual aspect).
When using ?
= 1, which7Our approach outperforms all the other models in termsof Cohen?s ?
too.8For the best scoring model the hyperparameters were asfollows: D = 200, H = 100, ?
= 1e ?
7, n = 9, E =200, ?
= 0.1, DLSTM= 10, r = 0.5.is equivalent to using the basic C&W model, wefound that performance was considerably lower(e.g., correlation dropped to ?
= 0.15).The number of ?noisy?
sequences was set to200, which was the highest possible setting weconsidered, although this might be related more tothe size of the corpus (see Mikolov et al (2013) fora similar discussion) rather than to our approach.Finally, the optimal value for DLSTMwas 10 (thelowest value investigated), which again may becorpus-dependent.6 Visualizing the black boxIn this section, inspired by recent advances in(de-) convolutional neural networks in computervision (Simonyan et al, 2013) and text summa-rization (Denil et al, 2014), we introduce a novelmethod of generating interpretable visualizationsof the network?s performance.
In the present con-text, this is particularly important as one advantageof the manual methods discussed in ?
2 is that weare able to know on what grounds the model madeits decisions and which features are most discrim-inative.At the outset, our goal is to assess the ?qual-ity?
of our word vectors.
By ?quality?
we meanthe level to which a word appearing in a particu-lar context would prove to be problematic for thenetwork?s prediction.
In order to identify ?high?and ?low?
quality vectors, we perform a single passof an essay from left to right and let the LSTMmake its score prediction.
Normally, we wouldprovide the gold scores and adjust the networkweights based on the error gradients.
Instead, weprovide the network with a pseudo-score by takingthe maximum score this specific essay can take9and provide this as the ?gold?
score.
If the wordvector is of ?high?
quality (i.e., associated withhigher scoring texts), then there is going to be lit-tle adjustment to the weights in order to predict thehighest score possible.
Conversely, providing theminimum possible score (here 0), we can assesshow ?bad?
our word vectors are.
Vectors which re-quire minimal adjustment to reach the lowest scoreare considered of ?lower?
quality.
Note that sincewe do a complete pass over the network (withoutdoing any weight updates), the vector quality isgoing to be essay dependent.9Note the in the Kaggle dataset essays from different es-say sets have different maximum scores.
Here we take asy?maxthe essay set maximum rather than the global maxi-mum.722.
.
.
way to show that Saeng is a determined .
.
.
.. .
.
sometimes I do .
Being patience is being .
.
.. .
.
which leaves the reader satisfied .
.
.. .
.
is in this picture the cyclist is riding a dry and area which could mean that it is veryand the looks to be going down hill there looks to be a lot of turns .
.
.
.. .
.
The only reason im putting this in my own way is because know one ispatient in my family .
.
.
.. .
.
Whether they are building hand-eye coordination , researching a country , or family andfriends through @CAPS3 , @CAPS2 , @CAPS6 the internet is highly andI hope you feel the same way .Table 2: Several example visualizations created by our LSTM.
The full text of the essay is shown inblack and the ?quality?
of the word vectors appears in color on a range from dark red (low quality) todark green (high quality).Concretely, using the network function f(x) ascomputed by Eq.
(12) ?
(17), we can approximatethe loss induced by feeding the pseudo-scores bytaking the magnitude of each error vector (18) ?(19).
Since lim?w?2?0y?
= y, this magnitudeshould tell us how much an embedding needs tochange in order to achieve the gold score (herepseudo-score).
In the case where we provide theminimum as a pseudo-score, a ?w?2value closerto zero would indicate an incorrectly used word.For the results reported here, we combine the mag-nitudes produced from giving the maximum andminimum pseudo-scores into a single score, com-puted as L(y?max, f(x))?
L(y?min, f(x)), where:L(y?, f(x)) ?
?w?2(18)w = ?L(x) ,?L?x????
(y?,f(x))(19)where ?w?2is the vector Euclidean norm w =?
?Ni=1w2i; L(?)
is the mean squared error as inEq.
(8); and y?
is the essay pseudo-score.We show some examples of this visualizationprocedure in Table 2.
The model is capable ofproviding positive feedback.
Correctly placedpunctuation or long-distance dependencies (as inSentence 6 are .
.
.
researching) are particularlyfavoured by the model.
Conversely, the modeldoes not deal well with proper names, but is ableto cope with POS mistakes (e.g., Being patience orthe internet is highly and .
.
.
).
However, as seenin Sentence 3 the model is not perfect and returnsa false negative in the case of satisfied.One potential drawback of this approach is thatthe gradients are calculated only after the end ofthe essay.
This means that if a word appears mul-tiple times within an essay, sometimes correctlyand sometimes incorrectly, the model would notbe able to distinguish between them.
Two possi-ble solutions to this problem are to either providethe gold score at each timestep which results intoa very computationally expensive endeavour, or tofeed sentences or phrases of smaller size for whichthe scoring would be more consistent.107 ConclusionIn this paper, we introduced a deep neural networkmodel capable of representing both local contex-tual and usage information as encapsulated by es-say scoring.
This model yields score-specific wordembeddings used later by a recurrent neural net-work in order to form essay representations.We have shown that this kind of architecture isable to surpass similar state-of-the-art systems, aswell as systems based on manual feature engineer-ing which have achieved results close to the upperbound in past work.
We also introduced a novelway of exploring the basis of the network?s inter-nal scoring criteria, and showed that such modelsare interpretable and can be further exploited toprovide useful feedback to the author.AcknowledgmentsThe first author is supported by the Onassis Foun-dation.
We would like to thank the three anony-mous reviewers for their valuable feedback.10We note that the same visualization technique can beused to show the ?goodness?
of phrases/sentences.
Withinthe phrase setting, after feeding the last word of the phrase tothe network, the LSTM layer will contain the phrase embed-ding.
Then, we can assess the ?goodness?
of this embeddingby evaluating the error gradients after predicting the high-est/lowest score.723ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-Rater v.2.0.
Journal of Technology,Learning, and Assessment, 4(3):1?30.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Pro-ceedings of the COLING/ACL, volume 6.Ted Briscoe, Ben Medlock, and ?istein E. Andersen.2010.
Automated assessment of ESOL free textexaminations.
Technical Report UCAM-CL-TR-790, University of Cambridge, Computer Labora-tory, nov.Ciprian Chelba, Tom?a?s Mikolov, Mike Schuster, Qi Ge,Thorsten Brants, Phillipp Koehn, and Tony Robin-son.
2013.
One Billion Word Benchmark for Mea-suring Progress in Statistical Language Modeling.In arXiv preprint.YY Chen, CL Liu, TH Chang, and CH Lee.
2010.An Unsupervised Automated Essay Scoring System.IEEE Intelligent Systems, pages 61?67.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: deepneural networks with multitask learning.
Proceed-ings of the Twenty-Fifth international conference onMachine Learning, pages 160?167, July.Ronan Collobert, Jason Weston, Leon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
Mar.Scott Crossley, Laura K Allen, Erica L Snow, andDanielle S McNamara.
2015.
Pssst... textual fea-tures... there is more to automatic essay scoring thanjust you!
In Proceedings of the Fifth InternationalConference on Learning Analytics And Knowledge,pages 203?207.
ACM.Yann N. Dauphin, Harm de Vries, and Yoshua Bengio.2015.
Equilibrated adaptive learning rates for non-convex optimization.
Feb.Misha Denil, Alban Demiraj, Nal Kalchbrenner, PhilBlunsom, and Nando de Freitas.
2014.
Modelling,visualising and summarising documents with a sin-gle convolutional neural network.
Jun.Semire Dikli.
2006.
An overview of automated scor-ing of essays.
Journal of Technology, Learning, andAssessment, 5(1).S.
Elliot.
2003.
IntellimetricTM: From here to valid-ity.
In M. D. Shermis and J. Burnstein, editors, Au-tomated Essay Scoring: A Cross-Disciplinary Per-spective, pages 71?86.
Lawrence Erlbaum Asso-ciates.Noura Farra, Swapna Somasundaran, and Jill Burstein.2015.
Scoring persuasive essays using opinions andtheir targets.
In Proceedings of the Tenth Workshopon Innovative Use of NLP for Building EducationalApplications, pages 64?74.Alex Graves.
2012.
Supervised Sequence Labellingwith Recurrent Neural Networks.
Springer BerlinHeidelberg.Michael U. Gutmann and Aapo Hyv?arinen.
2012.Noise-contrastive estimation of unnormalized sta-tistical models, with applications to natural imagestatistics.
J. Mach.
Learn.
Res., 13:307?361, Febru-ary.Karl Moritz Hermann, Tom Koisk, Edward Grefen-stette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom.
2015.
Teaching machines to readand comprehend.
Jun.S Hochreiter and J Schmidhuber.
1997.
Long short-term memory.
Neural computation, 9(8):1735?1780.Beata Beigman Klebanov and Michael Flor.
2013.Word association profiles and their use for auto-mated scoring of essays.
In Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguistics, pages 1148?1158.Thomas K. Landauer, Darrell Laham, and Peter W.Foltz.
2003.
Automated scoring and annotation ofessays with the Intelligent Essay Assessor.
In M.D.Shermis and J.C. Burstein, editors, Automated essayscoring: A cross-disciplinary perspective, pages 87?112.Quoc V. Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
May.Honglak Lee, Roger Grosse, Rajesh Ranganath, andAndrew Y. Ng.
2009.
Convolutional deep be-lief networks for scalable unsupervised learning ofhierarchical representations.
Proceedings of the26th Annual International Conference on MachineLearning ICML 09.Deryle Lonsdale and D. Strong-Krause.
2003.
Auto-mated rating of ESL essays.
In Proceedings of theHLT-NAACL 2003 Workshop: Building EducationalApplications Using Natural Language Processing.Danielle S McNamara, Scott A Crossley, Rod DRoscoe, Laura K Allen, and Jianmin Dai.
2015.
Ahierarchical classification approach to automated es-say scoring.
Assessing Writing, 23:35?59.Tom?a?s Mikolov, Stefan Kombrink, Anoop Deo-ras, Luk?a?s Burget, and Jan?Cernock?y.
2011.RNNLM-Recurrent neural network language mod-eling toolkit.
In ASRU 2011 Demo Session.Tomas Mikolov, I Sutskever, K Chen, G S Corrado,and Jeffrey Dean.
2013.
Distributed representa-tions of words and phrases and their composition-ality.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Ellis B.
Page.
1967.
Grading essays by computer:progress report.
In Proceedings of the InvitationalConference on Testing Problems, pages 87?100.724Ellis B.
Page.
1968.
The use of the computer in ana-lyzing student essays.
International Review of Edu-cation, 14(2):210?225, June.E.B.
Page.
2003.
Project essay grade: PEG.
In M.D.Shermis and J.C. Burstein, editors, Automated essayscoring: A cross-disciplinary perspective, pages 43?54.L.M.
Rudner and Tahung Liang.
2002.
Automatedessay scoring using Bayes?
theorem.
The Journal ofTechnology, Learning and Assessment, 1(2):3?21.Keisuke Sakaguchi, Michael Heilman, and Nitin Mad-nani.
2015.
Effective feature integration for auto-mated short answer scoring.
In Proceedings of theTenth Workshop on Innovative Use of NLP for Build-ing Educational Applications.M Shermis and B Hammer.
2012.
Contrasting state-of-the-art automated scoring of essays: analysis.Technical report, The University of Akron and Kag-gle.Mark D Shermis.
2015.
Contrasting state-of-the-artin the machine scoring of short-form constructed re-sponses.
Educational Assessment, 20(1):46?65.Karen Simonyan, Andrea Vedaldi, and Andrew Zisser-man.
2013.
Deep inside convolutional networks:Visualising image classification models and saliencymaps.
12.D.D.K.
Sleator and D. Templerley.
1995.
ParsingEnglish with a link grammar.
Proceedings of the3rd International Workshop on Parsing Technolo-gies, ACL.Jasper Snoek, Hugo Larochelle, and Ryan P. Adams.2012.
Practical bayesian optimization of machinelearning algorithms.
Jun.Swapna Somasundaran, Jill Burstein, and MartinChodorow.
2014.
Lexical chaining for measuringdiscourse coherence quality in test-taker essays.
InCOLING, pages 950?961.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural net-works.
Sep.Kai Sheng Tai, Richard Socher, and Christopher D.Manning.
2015.
Improved semantic representa-tions from tree-structured long short-term memorynetworks.
Feb.Duyu Tang.
2015.
Sentiment-specific representationlearning for document-level sentiment analysis.
InProceedings of the Eighth ACM International Con-ference on Web Search and Data Mining - WSDM'15.
Association for Computing Machinery (ACM).D.
M. Williamson.
2009.
A framework for imple-menting automated scoring.
Technical report, Ed-ucational Testing Service.Helen Yannakoudakis and Ronan Cummins.
2015.Evaluating the performance of automated text scor-ing systems.
In Proceedings of the Tenth Work-shop on Innovative Use of NLP for Building Educa-tional Applications.
Association for ComputationalLinguistics (ACL).Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading ESOL texts.
In The 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, Proceedingsof the Conference, 19-24 June, 2011, Portland, Ore-gon, USA, pages 180?189.725
