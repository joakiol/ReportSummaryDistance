Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 18?27,Sydney, July 2006. c?2006 Association for Computational LinguisticsBuilding Effective Question Answering CharactersAnton Leuski and Ronakkumar Patel and David TraumInstitute for Creative TechnologiesUniversity of Southern CaliforniaMarina del Rey, CA, 90292, USAleuski,ronakkup,traum@ict.usc.eduBrandon KennedyBrandon.Kennedy@usma.eduAbstractIn this paper, we describe methods forbuilding and evaluation of limited do-main question-answering characters.
Sev-eral classification techniques are tested, in-cluding text classification using supportvector machines, language-model basedretrieval, and cross-language informationretrieval techniques, with the latter havingthe highest success rate.
We also evalu-ated the effect of speech recognition errorson performance with users, finding that re-trieval is robust until recognition reachesover 50% WER.1 IntroductionIn the recent Hollywood movie ?iRobot?
set in2035 the main character played by Will Smith isrunning an investigation into the death of an oldfriend.
The detective finds a small device thatprojects a holographic image of the deceased.
Thedevice delivers a recorded message and respondsto questions by playing back prerecorded answers.We are developing virtual characters with similarcapabilities.Our target applications for these virtual charac-ters are training, education, and entertainment.
Foruse in education, such a character should be ableto deliver a message to the student on a specifictopic.
It also should be able to support a basic spo-ken dialog on the subject of the message, e.g., an-swer questions about the message topic and giveadditional explanations.
For example, consider astudent learning about an event in a virtual world.Lets say there is a small circus in a small town andsomeone has released all the animals from circus.A young student plays a role of a reporter to findout who caused this local havoc.
She is out to in-terrogate a number of witnesses represented by thevirtual characters.
It is reasonable to expect thateach conversation is going to be focused solely onthe event of interest and the characters may refuseto talk about anything else.
Each witness may havea particular and very narrow view into an aspect ofthe event, and the student?s success would dependon what sort of questions she asks and to whichcharacter she addresses them.Automatic question answering (QA) has beenstudied extensively in recent years.
For example,there is a significant body of research done in thecontext of the QA track at the Text REtrieval Con-ference (TREC) (Voorhees, 2003).
In contrast tothe TREC scenario where both questions and an-swers are based on facts and the goal is to providethe most relevant answer, we focus the answer?sappropriateness.
In our example about an inves-tigation, an evasive, misleading, or an ?honestly?wrong answer from a witness character would beappropriate but might not be relevant.
We tryto highlight that distinction by talking about QAcharacters as opposed to QA systems or agents.We expect that a typical simulation would con-tain quite a few QA characters.
We also expectthose characters to have a natural spoken languageinteraction with the student.
Our technical require-ments for such a QA character is that it should beable to understand spoken language.
It should berobust to disfluencies in conversational English.
Itshould be relatively fast, easy, and inexpensive toconstruct without the need for extensive domainknowledge and dialog management design exper-tise.In this paper we describe a QA character by thename of SGT Blackwell who was originally de-signed to serve as an information kiosk at an army18conference (see Appendix C for a photograph ofthe system) (?).
We have used SGT Blackwell todevelop our technology for automatic answer se-lection, conversation management, and system in-tegration.
We are presently using this technologyto create other QA characters.In the next section we outline the SGT Black-well system setup.
In Section 3 we discuss theanswer selection problem and consider three dif-ferent algorithms: Support Vector Machines clas-sifier (SVM), Language Model retrieval (LM), andCross-lingual Language Model (CLM) retrieval.We present the results of off-line experimentsshowing that the CLM method performs signifi-cantly better than the other two techniques in Sec-tion 4.
Section 5 describes a user study of the sys-tem that uses the CLM approach for answer selec-tion.
Our results show that the approach is veryrobust to deviations in wording from expected an-swers, and speech recognition errors.
Finally, wesummarize our results and outline some directionsfor future work in Section 6.2 SGT BlackwellA user talks to SGT Blackwell using a head-mounted close capture USB microphone.
Theuser?s speech is converted into text using an au-tomatic speech recognition (ASR) system.
Weused the Sonic statistical speech recognition en-gine from the University of Colorado (Pellom,2001) with acoustic and language models pro-vided to us by our colleagues at the University ofSouthern California (Sethy et al, 2005).
The an-swer selection module analyzes the speech recog-nition output and selects the appropriate response.The character can deliver 83 spoken lines rang-ing from one word to a couple paragraphs longmonologues.
There are three kinds of lines SGTBlackwell can deliver: content, off-topic, andprompts.
The 57 content-focused lines cover theidentity of the character, its origin, its languageand animation technology, its design goals, ouruniversity, the conference setup, and some mis-cellaneous topics, such as ?what time is it??
and?where can I get my coffee?
?When SGT Blackwell detects a question thatcannot be answered with one of the content-focused lines, it selects one out of 13 off-topic re-sponses, (e.g., ?I am not authorized to commenton that,?)
indicating that the user has ventured outof the allowed conversation domain.
In the eventthat the user persists in asking the questions forwhich the character has no informative response,the system tries to nudge the user back into theconversation domain by suggesting a question forthe user to ask: ?You should ask me instead aboutmy technology.?
There are 7 different prompts inthe system.One topic can be covered by multiple answers,so asking the same question again often results ina different response, introducing variety into theconversation.
The user can specifically requestalternative answers by asking something alongthe lines of ?do you have anything to add??
or?anything else??
This is the first of two typescommand-like expressions SGT Blackwell under-stands.
The second type is a direct request to re-peat the previous response, e.g., ?come again??
or?what was that?
?If the user persists on asking the same questionover and over, the character might be forced to re-peat its answer.
It indicates that by preceding theanswer with one of the four ?pre-repeat?
lines in-dicating that incoming response has been heard re-cently, e.g., ?Let me say this again...?3 Answer SelectionThe main problem with answer selection is uncer-tainty.
There are two sources of uncertainty ina spoken dialog system: the first is the complexnature of natural language (including ambigu-ity, vagueness, underspecification, indirect speechacts, etc.
), making it difficult to compactly char-acterize the mapping from the text surface form tothe meaning; and the second is the error-prone out-put from the speech recognition module.
One pos-sible approach to creating a language understand-ing system is to design a set of rules that select aresponse given an input text string (Weizenbaum,1966).
Because of uncertainty this approach canquickly become intractable for anything more thanthe most trivial tasks.
An alternative is to cre-ate an automatic system that uses a set of train-ing question-answer pairs to learn the appropriatequestion-answer matching algorithm (Chu-Carrolland Carpenter, 1999).
We have tried three differ-ent methods for the latter approach, described inthe rest of this section.3.1 Text ClassificationThe answer selection problem can be viewed as atext classification task.
We have a question text19as input and a finite set of answers, ?
classes, ?we build a system that selects the most appropriateclass or set of classes for the question.
Text classi-fication has been studied in Information Retrieval(IR) for several decades (Lewis et al, 1996).
Thedistinct properties of our setup are (1) a very smallsize of the text, ?
the questions are very short, and(2) the large number of classes, e.g, 60 responsesfor SGT Blackwell.An answer defines a class.
The questions corre-sponding to the answer are represented as vectorsof term features.
We tokenized the questions andstemmed using the KStem algorithm (Krovetz,1993).
We used a tf ?
idf weighting scheme toassign values to the individual term features (Al-lan et al, 1998).
Finally, we trained a multi-classSupport Vector Machines (SVM struct) classifierwith an exponential kernel (Tsochantaridis et al,2004).
We have also experimented with linearkernel function, various parameter values for theexponential kernel, and different term weightingschemes.
The reported combination of the ker-nel and weighting scheme showed the best clas-sification performance.
Such an approach is well-known in the community and has been shown towork very well in numerous applications (Leuski,2004).
In fact, SVM is generally considered to beone of the best performing methods for text clas-sification.
We believe it provides us with a verystrong baseline.3.2 Answer RetrievalThe answer selection problem can also be viewedas an information retrieval problem.
We have aset of answers which we can call documents in ac-cordance with the information retrieval terminol-ogy.
Let the question be the query, we comparethe query to each document in the collection andreturn the most appropriate set of documents.Presently the best performing IR techniquesare based on the concept of Language Model-ing (Ponte and Croft, 1997).
The main strategyis to view both a query and a document as samplesfrom some probability distributions over the wordsin the vocabulary (i.e., language models) and com-pare those distributions.
These probability distri-butions rarely can be computed directly.
The ?art?of the field is to estimate the language models asaccurately as possible given observed queries anddocuments.Let Q = q1...qm be the question that is re-ceived by the system, RQ is the set of all the an-swers appropriate to that question, and P (w|RQ)is the probability that a word randomly sampledfrom an appropriate answer would be the word w.The language model of Q is the set of probabili-ties P (w|RQ) for every word in the vocabulary.
Ifwe knew the answer set for that question, we caneasily estimate the model.
Unfortunately, we onlyknow the question and not the answer set RQ.
Weapproximate the language model with the condi-tional distribution:P (w|RQ) ?
P (w|Q) =P (w, q1, ..., qm)P (q1, ..., qm)(1)The next step is to calculate the joint probabil-ity of observing a string: P (W ) = P (w1, ..., wn).Different methods for estimating P (W ) have beensuggested starting with simple unigram approachwhere the occurrences of individual words are as-sumed independent from each other: P (W ) =?ni=1 P (wi).
Other approaches include Proba-bilistic Latent Semantic Indexing (PLSI) (Hoff-man, 1999) and Latent Dirichlet Allocation(LDA) (Blei et al, 2003).
The main goal of thesedifferent estimations is to model the interdepen-dencies that exist in the text and make the esti-mation feasible given the finite amount of trainingdata.In this paper we adapt an approach suggestedby Lavrenko (Lavrenko, 2004).
He assumed thatall the word dependencies are defined by a vectorof possibly unknown parameters on the languagemodel.
Using the de Finetti?s representation the-orem and kernel-based probability estimations, hederived the following estimate for the query lan-guage model:P (w|Q) =?s?S pis(w)?mi=1 pis(qi)?s?mi=1 pis(qi)(2)Here we sum over all training strings s ?
S,where S is the set of training strings.
pis(w) is theprobability of observing word w in the string s,which can be estimated directly from the trainingdata.
Generally the unigram maximum likelihoodestimator is used with some smoothing factor:pis(w) = ?pi ?#(w, s)|s|+ (1?
?pi) ?
?s #(w, s)?s |s| (3)20where #(w, s) is the number of times word w ap-pears in string s, |s| is the length of the string s,we sum over all training strings s ?
S, and theconstant ?pi is the tunable parameter that can bedetermined from training data.We know all the possible answers, so the answerlanguage model P (w|A) can be estimated fromthe data:P (w|A) = piA(w) (4)3.3 Ranking criteriaTo compare two language models we use theKullback-Leibler divergence D(pq||pa) defined asD(pq||pa) =?w?VP (w|Q) logP (w|Q)P (w|A)(5)which can be interpreted as the relative entropy be-tween two distributions.
Note that the Kullback-Leibler divergence is a dissimilarity measure, weuse ?D(pq||pa) to rank the answers.So far we have assumed that both questionsand answers use the same vocabulary and havethe same a priori language models.
Clearly, it isnot the case.
For example, consider the follow-ing exchange: ?what happened here??
?
?well,maam, someone released the animals this morn-ing.?
While the answer is likely to be very appro-priate to the question, there is no word overlap be-tween these sentences.
This is an example of whatis known in information retrieval as vocabularymismatch between the query and the documents.In a typical retrieval scenario a query is assumedto look like a part of a document.
We cannot makethe same assumption about the questions becauseof the language rules: e.g., ?what?, ?where?, and?why?
are likely to appear much more often inquestions than in answers.
Additionally, a typi-cal document is much larger than any of our an-swers and has a higher probability to have wordsin common with the query.
Finally, a typical re-trieval scenario is totally context-free and a user isencouraged to specify her information need as ac-curately as possible.
In a dialog, a portion of theinformation is assumed to be well-known to theparticipants and remains un-verbalized leading tosometimes brief questions and answers.We believe this vocabulary mismatch to be sosignificant that we view the participants as speak-ing two different ?languages?
: a language of ques-tions and a language of answers.
We will modelthe problem as a cross-lingual information task,where one has a query in one language and wishesto retrieve documents in another language.
Thereare two ways we can solve it: we can translate theanswers into the question language by building arepresentation for each answer using the questionvocabulary or we can build question representa-tions in the answer language.3.4 Question domainWe create an answer representation in the ques-tion vocabulary by merging together all the train-ing questions that are associated with the answerinto one string: a pseudo-answer.
We use equa-tions 5, 2, 3, and 4 to compare and rank thepseudo-answers.
Note that in equation 2 s iteratesover the set of all pseudo-answers.3.5 Answer domainLet us look at the question language modelP (w|Q) again, but now we will take into accountthat w and Q are from different vocabularies andhave potentially different distributions:P (w|Q) =?s ?As(w)?mi=1 piQs(qi)?s?mi=1 piQs(qi)(6)Here s iterates over the training set of question-answer pairs {Qs, As} and ?x(w) is the experi-mental probability distribution on the answer vo-cabulary given by the expression similar to equa-tion 3:?x(w) = ?
?#(w, x)|x|+ (1?
??
)?s #(w, x)?s |x|and the answer language model P (w|A) can beestimated from the data asP (w|A) = ?A(w)4 Algorithm comparisonWe have a collection of questions for SGT Black-well each linked to a set of appropriate responses.Our script writer defined the first question or twofor each answer.
We expanded the set by a) para-phrasing the initial questions and b) collectingquestions from users by simulating the final sys-tem in a Wizard of Oz study (WOZ).
There are1,261 questions in the collection linked to 72 an-swers (57 content answers, 13 off-topic responses,and 2 command classes, see Section 2).
For this21study we considered all our off-topic responsesequally appropriate to an off-topic question andwe collapsed all the corresponding responses intoone class.
Thus we have 60 response classes.We divided our collection of questions intotraining and testing subsets following the 10-foldcross-validation schema.
The SVM system wastrained to classify test questions into one of the 60classes.Both retrieval techniques produce a ranked listof candidate answers ordered by the ?D(pq||pa)score.
We only select the answers with scores thatexceed a given threshold ?D(pq||pa) > ?
.
If theresulting answer set is empty we classify the ques-tion as off-topic, i.e., set the candidate answer setcontains to an off-topic response.
We determinethe language model smoothing parameters ?s andthe threshold ?
on the training data.We consider two statistics when measuring theperformance of the classification.
First, we mea-sure its accuracy.
For each test question the firstresponse returned by the system, ?
the class fromthe SVM system or the top ranked candidate an-swer returned by either LM or CLM methods, ?is considered to be correct if there is link betweenthe question and the response.
The accuracy is theproportion of correctly answered questions amongall test questions.The second statistic is precision.
Both LM andCLM methods may return several candidate an-swers ranked by their scores.
That way a user willget a different response if she repeats the question.For example, consider a scenario where the firstresponse is incorrect.
The user repeats her ques-tion and the system returns a correct response cre-ating the impression that the QA character simplydid not hear the user correctly the first time.
Wewant to measure the quality of the ranked list ofcandidate answers or the proportion of appropri-ate answers among all the candidate answers, butwe should also prefer the candidate sets that list allthe correct answers before all the incorrect ones.A well-known IR technique is to compute aver-age precision ?
for each position in the ranked listcompute the proportion of correct answers amongall preceding answers and average those values.Table 1 shows the accuracy and average preci-sion numbers for three answer selection methodson the SGT Blackwell data set.
We observe a sig-nificant improvement in accuracy in the retrievalmethods over the SVM technique.
The differencesshown are statistical significant by t-test with thecutoff set to 5% (p < 0.05).We repeated out experiments on QA charac-ters we are developing for another project.
Therewe have 7 different characters with various num-ber of responses.
The primary difference withthe SGT Blackwell data is that in the new sce-nario each question is assigned to one and onlyone answer.
Table 2 shows the accuracy numbersfor the answer selection techniques on those datasets.
These performance numbers are generallylower than the corresponding numbers on the SGTBlackwell collection.
We have not yet collectedas many training questions as for SGT Blackwell.We observe that the retrieval approaches are moresuccessful for problems with more answer classesand more training data.
The table shows the per-cent improvement in classification accuracy foreach LM-based approach over the SVM baseline.The asterisks indicate statistical significance usinga t-test with the cutoff set to 5% (p < 0.05).5 Effect of ASRIn the second set of experiments for this paperwe studied the question of how robust the CLManswer selection technique in the SGT Blackwellsystem is to the disfluencies of normal conversa-tional speech and errors of the speech recogni-tion.
We conducted a user study with people in-terviewing SGT Blackwell and analyzed the re-sults.
Because the original system was meant forone of three demo ?reporters?
to ask SGT Black-well questions, specialized acoustic models wereused to ensure the highest accuracy for these three(male) speakers.
Consequently, for other speak-ers (especially female speakers), the error rate wasmuch higher than for a standard recognizer.
Thisallowed us to calculate the role of a variety ofspeech error rates on classifier performance.For this experiment, we recruited 20 partici-pants (14 male, 6 female, ages from 20 to 62)from our organization who were not members ofthis project.
All participants spoke English flu-ently, however the range of their birth languagesincluded English, Hindi, and Chinese.After filling out a consent form, participantswere ?introduced?
to SGT Blackwell, and demon-strated the proper technique for asking him ques-tions (i.e., when and how to activate the micro-phone and how to adjust the microphone posi-tion.)
Next, the participants were given a scenario22SVM LM CLMaccuracy accuracy impr.
SVM avg.
prec.
accuracy impr.
SVM avg.
prec.53.13 57.80 8.78 63.88 61.99 16.67 65.24Table 1: Comparison of three different algorithms for answer selection on SGT Blackwell data.
Eachperformance number is given in percentages.number of number of SVM LM CLMquestions answers accuracy accuracy impr.
SVM accuracy impr.
SVM1 238 22 44.12 47.06 6.67* 47.90 8.57*2 120 15 63.33 62.50 -1.32 64.17 1.323 150 23 42.67 44.00 3.12* 50.00 17.19*4 108 18 42.59 44.44 4.35* 50.00 17.39*5 149 33 32.21 41.35 28.37* 42.86 33.04*6 39 8 69.23 58.97 -14.81* 66.67 -3.707 135 31 42.96 44.19 2.85 50.39 17.28*average 134 21 48.16 48.93 1.60* 53.14 10.34*Table 2: Comparison of three different algorithms for answer selection on 7 additional QA characters.The table shows the number of answers and the number of questions collected for each character.
Theaccuracy and the improvement over the baseline numbers are given in percentages.wherein the participant would act as a reporterabout to interview SGT Blackwell.
The partici-pants were then given a list of 10 pre-designatedquestions to ask of SGT Blackwell.
These ques-tions were selected from the training data.
Theywere then instructed to take a few minutes towrite down an additional five questions to ask SGTBlackwell.
Finally they were informed that af-ter asking the fifteen written down questions, theywould have to spontaneously generate and ask fiveadditional questions for a total of 20 questionsasked all together.
Once the participants had writ-ten down their fifteen questions, they began theinterview with SGT Blackwell.
Upon the com-pletion of the interview the participants were thenasked a short series of survey questions by theexperimenter about SGT Blackwell and the inter-view.
Finally, participants were given an explana-tion of the study and then released.
Voice record-ings were made for each interview, as well as theraw data collected from the answer selection mod-ule and ASR.
This is our first set of question an-swer pairs, we call it the ASR-QA set.The voice recordings were later transcribed.
Weran the transcriptions through the CLM answer se-lection module to generate answers for each ques-tion.
This generated question and answer pairsbased on how the system would have respondedto the participant questions if the speech recogni-tion was perfect.
This is our second set of ques-tion answer pairs ?
the TRS-QA set.
Appendix Bshows a sample dialog between a participant andSGT Blackwell.Next we used three human raters to judge theappropriateness of both sets.
Using a scale of1-6 (see Appendix A) each rater judged the ap-propriateness of SGT Blackwell?s answers to thequestions posed by the participants.
We evaluatedthe agreement between raters by computing Cron-bach?s alpha score, which measures consistency inthe data.
The alpha score is 0.929 for TRS-QAand 0.916 for ASR-QA, which indicate high con-sistency among the raters.The average appropriateness score for TRS-QAis 4.83 and 4.56 for ASR-QA.
The difference inthe scores is statistically significant according to t-test with the cutoff set to 5%.
It may indicate thatASR quality has a significant impact on answerselection.We computed the Word Error Rate (WER) be-tween the transcribed question text and the ASRoutput.
Thus each question-answer pair in theASR-QA and TRS-QA data set has a WER scoreassigned to it.
The average WER score is 37.33%.We analyzed sensitivity of the appropriatenessscore to input errors.
Figure 1a and 1b showplots of the cumulative average appropriatenessscore (CAA) as function of WER: for each WERvalue t we average appropriateness scores for allquestions-answer pairs with WER score less than23(a) pre-designated (b) user-designatedFigure 1: Shows the cumulative average appropriateness score (CAA) of (a) pre-designated and (b)user-designated question-answer pairs as function of the ASR?s output word error rate.
We show thescores for TRS-QA (dotted black line) and ASR-QA (solid black line).
We also show the percentage ofthe question-answer pairs with the WER score below a given value (?# ofQA?)
as a gray line with thecorresponding values on the right Y axis.or equal to t.CAA(t) =1|S|?p?SA(p), S = {p|WER(p) ?
t}where p is a question-answer pair, A(p) is theappropriateness score for p, and WER(p) is theWER score for p. It is the expected value of the ap-propriateness score if the ASR WER was at mostt.Both figures show the CAA values for TRS-QA (dotted black line) and ASR-QA (solid blackline).
Both figures also show the percentage ofthe question-answer pairs with the WER score be-low a given value, i.e., the cumulative distributionfunction (CDF) for the WER as a gray line withthe corresponding values depicted on the right Yaxis.Figure 1a shows these plots for the pre-designated questions.
The values of CAA forTRS-QA and ASR-QA are approximately thesame between 0 and 60% WER.
CAA for ASR-QA decreases for WER above 60% ?
as the inputbecomes more and more garbled, it becomes moredifficult for the CLM module to select an appropri-ate answer.
We confirmed this observation by cal-culating t-test scores at each WER value: the dif-ferences between CAA(t) scores are statisticallysignificant for t > 60%.
It indicates that untilWER exceeds 60% there is no noticeable effect onthe quality of answer selection, which means thatour answer selection technique is robust relative tothe quality of the input.Figure 1b shows the same plots for the user-designated questions.
Here the system has to dealwith questions it has never seen before.
CAA val-ues decrease for both TRS-QA and ASR-QA asWER increases.
Both ASR and CLM were trainedon the same data set and out of vocabulary wordsthat affect ASR performance, affect CLM perfor-mance as well.6 Conclusions and future workIn this paper we presented a method for efficientconstruction of conversational virtual characters.These characters accept spoken input from a user,convert it to text, and select the appropriate re-sponse using statistical language modeling tech-niques from cross-lingual information retrieval.We showed that in this domain the performanceof our answer selection approach significantly ex-ceeds the performance of a state of the art text clas-sification method.
We also showed that our tech-nique is very robust to the quality of the input andcan be effectively used with existing speech recog-nition technology.Preliminary failure analysis indicates a few di-rections for improving the system?s quality.
First,we should continue collecting more training dataand extending the question sets.Second, we could have the system generate aconfidence score for its classification decisions.Then the answers with a low confidence score canbe replaced with an answer that prompts the userto rephrase her question.
The system would then24use the original and the rephrased version to repeatthe answer selection process.Finally, we observed that a notable percent ofmisclassifications results from the user asking aquestion that has a strong context dependency onthe previous answer or question.
We are presentlylooking into incorporating this context informa-tion into the answer selection process.AcknowledgmentsThe project or effort described here has been spon-sored by the U.S. Army Research, Development,and Engineering Command (RDECOM).
State-ments and opinions expressed do not necessarilyreflect the position or the policy of the UnitedStates Government, and no official endorsementshould be inferred.ReferencesJames Allan, Jamie Callan, W. Bruce Croft, LisaBallesteros, Donald Byrd, Russell Swan, and JinxiXu.
1998.
Inquery does battle with TREC-6.
InSixth Text REtrieval Conference (TREC-6), pages169?206, Gaithersburg, Maryland, USA.D.
M. Blei, A. Y. Ng, and M. I. Jordan.
2003.
Latentdirichlet alocation.
Journal of Machine LearningResearch, 3:993?1022.Jennifer Chu-Carroll and Bob Carpenter.
1999.Vector-based natural language call routing.
Journalof Computational Linguistics, 25(30):361?388.Sudeep Gandhe, Andrew S. Gordon, and David Traum.2006.
Improving question-answering with linkingdialogues.
In Proceedings of the 11th internationalconference on Intelligent user interfaces (IUI?06),pages 369?371, New York, NY, USA.
ACM Press.T.
Hoffman.
1999.
Probabilistic latent semantic index-ing.
In Proceedings of the 22nd International ACMSIGIR Conference, pages 50?57.Robert Krovetz.
1993.
Viewing morphology as an in-ference process.
In Proceedings of the 16th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages 191?202.Victor Lavrenko.
2004.
A Generative Theory of Rele-vance.
Ph.D. thesis, University of Massachusetts atAmherst.Anton Leuski.
2004.
Email is a stage: discover-ing people roles from email archives.
In Proceed-ings of 27th annual international ACM SIGIR Con-ference on Research and Development in Informa-tion Retrieval (SIGIR?04), pages 502?503, Sheffield,United Kingdom.
ACM Press.
NY, USA.David D. Lewis, Robert E. Schapire, James P. Callan,and Ron Papka.
1996.
Training algorithms for lin-ear text classifiers.
In Proceedings of the 19th Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages 298?306, Zurich, Switzerland.Bryan Pellom.
2001.
Sonic: The university of col-orado continuous speech recognizer.
Technical Re-port TR-CSLR-2001-01, University of Colorado,Boulder, CO.Jay M. Ponte and W. Bruce Croft.
1997.
Text segmen-tation by topic.
In Proceedings of the First EuropeanConference on Research and Advanced Technologyfor Digital Libraries, pages 120?129.Abhinav Sethy, Panayiotis Georgiou, and ShrikanthNarayanan.
2005.
Building topic specific languagemodels from webdata using competitive models.
InProceedings of EUROSPEECH, Lisbon, Portugal.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2004.
Support vectormachine learning for interdependent and structuredoutput spaces.
In Proceedings of the Twenty-first in-ternational conference on Machine learning, Banff,Alberta, Canada.Ellen M. Voorhees.
2003.
Overview of the trec 2003question answering track.
In Proceedings of TheTwelfth Text Retrieval Conference, pages 54?69.Joseph Weizenbaum.
1966.
Eliza?a computer programfor the study of natural language communication be-tween man and machine.
Communications of theACM, 9(1):36?45.25A Appropriateness gradingGandhe and his colleagues (Gandhe et al, 2006)suggested the following grading scheme that weused in our user evaluation.Grade Description1 Response is not related in any way thequestion2 Response contains some discussionof people or objects mentioned in thequestion, but does not really addressthe question itself.3 Response partially addresses thequestion, but little or no coherencebetween the question and response.4 Response does mostly address thequestion, but with major problems inthe coherence between question andresponse; seems like the response isreally addressing a different questionthan the one asked.5 Response does address the question,but the transition is somewhat awk-ward.6 Response answers the question in aperfectly fluent manner.Table 3: The question-answer appropriatenessscale.B Sample dialogThis section presents an example of a participant?sdialog with SGT Blackwell.
We only show spon-taneous questions asked by the participant.
Eachof the participant?s questions (called ?Reporter?
inthis transcript) is followed by the ASR output ofthat line in brackets.
That is what the answer se-lection module ?heard.?
Note that ASR does makean occasional mistake and the answer selection isable to recover gracefully until the next to the lastquestion, where the input was so garbled that onlyone correct word got through.Reporter: Who created you (Who created you)Blackwell: Tell you the truth, I have spent a lotof time with the smart folks at ICT, testingand tweaking.
I am about ready to get outthere and train with real soldiers.
I am think-ing first stop is going to be Fort Sill.
But myultimate goal is to get forward deployed...Reporter: Why should we spend money on you(What should we spend money on you)Blackwell: Why do you need me?!
What youshould be saying is ?How did you get alngwithout me??
I will show you how to be aleader, how to make critical decisions understress... And I am glad to be here, because Ican help.
I know I can.
Look...
I have onemission: to make better soldiers, make bet-ter warfighters.
And if I can ... can save onelife... bring one loved one back to his fam-ily... Well, then, I am definitely worth it.
Yoube the judge.Reporter: What?s your value (What?s the valuein)Blackwell: I can do ?Virtually?
anything.
I domission rehearsals, Urban Combat training,language training - I speak Farsi, Pashtun andArabic.
?Mar Haba shabab!
That means howare you in Arabic.Reporter: Do you really work or is this just ademo (G. really worker is this just a demo)Blackwell: Sorry.
That?s outside my AO.Reporter: How could you be better (How couldyou be better)Blackwell: Stop mumbling.
Just kidding.
Ididn?t get that.Reporter: What do you wish you could do(When you wish you could do)Blackwell: Career Army.
That is what I amthinking.
Just call me a lifer.Reporter: What kind of people have you trained(What other people in the turn in)Blackwell: I teach cultural awareness, functionas a training mentor and can call in the fullrange of Joint fires and effects.Reporter: What?s the hardest part of your job(He was the hardest are you job)Blackwell: I am not authorized to comment onthatReporter: You have any virtual friends (The uni-versal friends)26Blackwell: Sorry, I can?t hear youReporter: Are you hard of hearing (Are you hada hearing)Blackwell: I can?t understand you.C SGT BlackwellFigure 2: A photograph of the SGT Blackwell sys-tem setup.27
