Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 180?189,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsHigh-Order Sequence Modeling for Language Learner Error DetectionMichael GamonMicrosoft ResearchOne Microsoft WayRedmond, WA 98052mgamon@microsoft.comAbstractWe address the problem of detecting Eng-lish language learner errors by using a dis-criminative high-order sequence model.Unlike most work in error-detection, thismethod is agnostic as to specific errortypes, thus potentially allowing for higherrecall across different error types.
The ap-proach integrates features from manysources into the error-detection model,ranging from language model-based fea-tures to linguistic analysis features.
Evalua-tion results on a large annotated corpus oflearner writing indicate the feasibility ofour approach on a realistic, noisy and in-herently skewed set of data.
High-ordermodels consistently outperform low-ordermodels in our experiments.
Error analysison the output shows that the calculation ofprecision on the test set represents a lowerbound on the real system performance.1.
IntroductionSystems for automatic detection and correction oferrors in native writing have been developed formany decades.
Early in the development of thesesystems, the approach was exclusively based onknowledge engineering.
Hand-crafted grammarswould analyze a sentence and would contain spe-cial mechanisms for rule or constraint relaxationthat allow ungrammatical sentences to produce aparse, while at the same time indicating that agrammatical error is present.
More recently, data-driven methods have assumed prominence andthere has been an emerging area of research intothe challenge of detecting and correcting errors inlearner language (for an overview see Leacock etal.
2010).
Data-driven methods offer the familiarset of advantages: they can be more flexible than amanually maintained set of rules and they tend tocope better with noisy input.
Drawbacks includethe inability to handle linguistically more complexerrors that involve long distance dependencies suchas subject-verb agreement.
Learner errors as a tar-get for error detection and correction pose a partic-ular challenge but also offer some uniqueopportunities.
The challenge lies in the density oferrors (much higher than in native writing), thevariety of errors (a superset of typical native er-rors) and the generally more non-idiomatic writing.On the other hand, the availability of annotatedcorpora, often comprised of manually correctedlearner essays or scripts, provides a big advantagefor the evaluation and training of data-driven sys-tems.Data-driven systems for English learner errordetection and correction typically target a specificset of error types and contain a machine learnedcomponent for each error type.
For example, sucha system may have a classifier that determines thecorrect choice of preposition given the lexical andsyntactic part-of-speech (POS) context and hencecan aid the learner with the notoriously difficultproblem of identifying an appropriate preposition.Similarly, a classifier can be used to predict thecorrect choice of article in a given context.
Suchtargeted systems have the advantage that they oftenachieve relatively high precision at, of course, thecost of recall.
However, while there are a few ma-jor learner error categories, such as prepositionsand articles, there is also a long tail of contentword and other errors that is not amenable to a tar-geted approach.In this paper, we depart from the error-specificparadigm and explore a sequence modeling ap-proach to general error detection in learner writing.This approach is completely agnostic as to the er-ror type.
It attempts to predict the location of an180error in a sentence based on observations gatheredfrom a supervised training phase on an error-annotated learner corpus.
Features used here arebased on an n-gram language model, POS tags,simple string features that indicate token lengthand capitalization, and linguistic analysis by a con-stituency parser.
We train and evaluate the methodon a sizeable subset of the corpus.
We show thecontribution of the different feature types and per-form a manual error analysis to pinpoint shortcom-ings of the system and to get a more accurate ideaof the system?s precision.2.
Related workError-specific approaches comprise the majority ofrecent work in learner error detection.
Two of themost studied error types in learner English arepreposition and article errors since they make up alarge percentage of errors in learner writing (16%and 13% respectively in the Cambridge LearnerCorpus, without considering spelling and punctua-tion errors).
The most widely used approach fordetecting and correcting these errors is classifica-tion, with lexical and POS features gleaned from awindow around the potential preposition/articlesite in a sentence.
Some recent work includes Cho-dorow et al (2007), De Felice and Pulman (2008),Gamon (2010), Han et al (2010), Izumi et al(2004), Tetreault and Chodorow (2008), Ro-zovskaya and Roth (2010a, 2010b).
Gamon et al(2008) and Gamon (2010) used a language modelin addition to a classifier and combined the classi-fier output and language model scores in a meta-classifier.
These error-specific methods achievehigh precision (up to 80-90% on some corpora) butonly capture highly constrained error types such aspreposition and determiner errors.There has also been research on error-detectionmethods that are not designed to identify a specificerror type.
The basic idea behind these error-agnostic approaches is to identify an error wherethere is a particularly unlikely sequence comparedto the patterns found in a large well-formed corpus.Atwell (1986) used low-likelihood sequences ofPOS tags as indicators for the presence of an error.Sj?bergh (2005) used a chunker to detect unlikelychunks in native Swedish writing compared to thechunks derived from a large corpus of well-formedSwedish writing.
Bigert and Knutsson (2002) em-ployed a statistical method to identify a variety oferrors in Swedish writing as rare sequences ofmorpho-syntactic tags.
They significantly reducedfalse positives by using additional methods to de-termine whether the unexpected sequence is due tophrase or sentence boundaries or due to rare singletags.
Chodorow and Leacock (2000) utilized mutu-al information and chi-square statistics to identifytypical contexts for a small set of targeted wordsfrom a large well-formed corpus.
Comparing thesestatistics to the ones found in a novel sentence,they could identify unlikely contexts for the target-ed words that were often good indicators of thepresence of an error.
Sun et al (2007) mined forpatterns that consist of POS tags and functionwords.
The patterns are of variable length and canalso contain gaps.
Patterns were then combined ina classifier to distinguish correct from erroneoussentences.
Wagner et al (2007) combined parseprobabilities from a set of statistical parsers andPOS tag n-gram probabilities in a classifier to de-tect ungrammatical sentences.
Okanohara and Tsu-jii (2007) differed from the previous approaches inthat they directly used discriminative languagemodels to distinguish correct from incorrect sen-tences, without the direct modeling of error-indicating patterns.
Park and Levy (2011) use anoisy channel model with a base language modeland a set of error-specific noise models for errordetection and correction.In contrast to previous work, we cast the task asa sequence modeling problem.
This provides aflexible framework in which multiple statisticaland linguistic signals can be combined and cali-brated by supervised learning.
The approach is er-ror-agnostic and can easily be extended withadditional statistical or linguistic features.3.
Error detection by sequence modelingErrors consist of a sub-sequence of tokens in alonger token sequence.
They can be identified by acombination of internal and contextual features,the latter requiring a notion of Markov window (awindow around a token in which relevant infor-mation is likely to be found).
This is similar totasks such as named entity recognition (NER) orpart-of-speech tagging, where sequence modelinghas proven to be successful.We choose a Maximum Entropy Markov Model(MEMM, McCallum et al 2000) as the modelingtechnique.
In NER, the annotation convention uses181three labels for a token ?O?
(outside of NE), ?B?
(beginning of NE), and ?I?
(inside of NE).
For ourpurpose we reduced the set of labels to just ?O?and ?I?
since most of the errors are relatively short.Conditional Random Fields (Lafferty et al2001) are considered to be superior to MEMMs inlearning problems affected by label bias (Bottou1991).
In our scheme, however, there are only twostates ?O?
and ?I?, and both states can transition toeach other.
Since there are no states with asymmet-ric transition properties that would introduce a biastowards states with fewer transitions, label bias isnot a problem for us.Figure 1 shows the structure of our MEMM witha Markov order of five (the diagram only showsthe complete set of arcs for the last state).
The in-put sentence contains the token sequence the pastyear I was stayed ?
with the error was stayed.
In-stead of using the tokens themselves as observa-tions, we chose to use POS tags assigned by anautomatic tagger (Toutanova et al 2003).
Thischoice was motivated by data sparseness.
Learninga model that observes individual lexical items andpredicts a sequence of error/non-error tags wouldbe ideal, but given the many different error typesand triggering contexts for an error, such a modelwould require much more training data.
A large setof features that serve as constraints on the statetransition models are extracted for each state.
The-se features are described in Section 5.Note that the model structure would lend itselfto a factorial conditional random field (McCallumet al 2003) which allows the joint labeling of POStags and state labels.
This would, however, requiretraining data that is labeled for both errors andPOS tags.Figure 1: MEMM model for error detection, thefull set of dependencies is only shown for the laststate.4.
Detecting errors in the CambridgeLearner CorpusThe learner corpus used to train and evaluate thesystem is the Cambridge Learner Corpus (CLC).
Itconsists of essays (scripts) written as part of theUniversity of Cambridge English for Speakers ofOther Languages (ESOL) examinations.
The cor-pus contains about 30 million words of learnerEnglish.
All errors are annotated and include, whenpossible, a single suggested correction.
Errors arecategorized into 87 error types.We performed a number of preprocessing stepson the data.
On the assumption that learners haveaccess to a spell checker, errors that were markedas spelling errors were corrected based on the an-notations.
Confused words (their/there) were treat-ed in the same way, given that they are correctedby a modern proofing tool such as the one in Mi-crosoft Word.
In addition, British English spellingconventions were changed to those of AmericanEnglish.
Sentences containing errors that had nosuggested rewrite were eliminated.
Finally, onlylexical errors are covered in this work.
For punctu-ation and capitalization we removed the error an-notations, retaining the original (erroneous)punctuation and capitalization.We grouped the remaining 60 error classifica-tions into eight categories: Content word, Inflec-tional morphology, Noun phrase errors,Preposition errors, Multiple errors, Other errorsinvolving content words, Other errors involvingfunction words and Derivational morphology.
Thedistribution of error categories is shown in Table 1.Error Class Freq PctContent word insertion, dele-tion or choice185,201 21%Inflectional morphology andagreement of content words157,660 18%Noun phrase formation: De-terminers and quantifiers130,829 15%Preposition error 124,902 14%Multiple: Adjacent and nestedannotations113,615 13%Other content word errors 79,596 9%Other function word errors:anaphors and conjunctions65,034 7%Derivational morphology ofcontent words39,213 4%Table 1: Error types in the CLC.182The multiple error class includes any combinationof error types where the error annotations are eithernested or adjacent.
The other categories are morefocused: the errors are of a particular class andtheir adjacent context is correct, although theremay be another error annotation a single tokenaway.
Content word errors involve the insertion,deletion and substitution of nouns, verbs, adjec-tives and adverbs.
Further analysis of this errorcategory on a random sample of 200 instances re-veals that the majority (72%) of content word er-rors involve substitutions, while deletions accountfor 10% of the errors and insertions for 18%.
Mostsubstitutions (63%) involve the wrong choice of aword that is somewhat semantically related to thecorrect choice.
Inflectional morphology includesall inflection errors for content words as well assubject-verb agreement errors.
The inflectionalerrors include many cases of what might be con-sidered spelling errors, for example *dieing/dying.Similarly, the derivational morphology errors in-clude all derivational errors for content words ?and also include many errors that may be consid-ered as spelling errors.
Noun formation errors in-clude all annotations involving determiners andquantifiers: inflection, derivation, countability,word form and noun-phrase-internal agreement.Preposition errors include all annotations that in-volve prepositions: insertion, deletion, substitutionand a non-preposition being used in place of apreposition.
There are two other categories: thoseinvolving the remaining function words (anaphorsand conjunctions) and those involving remainingcontent words (collocation, idiom, negative for-mation, argument structure, word order, etc.
).It is important to highlight the challenges inher-ent in this data set.
First of all, the problem is high-ly skewed since only 7.3% of tokens in the test setare involved in an error.
Second, since we includedcorrect learner sentences in the development andtest sets in the proportion they occur in the overallcorpus, only 47% of sentences in the test set con-tain error annotations, greatly increasing the likeli-hood of false positives.5.
Features5.1 Language model featuresThe language model (LM) features comprise atotal of 29 features.
Each of these features is calcu-lated from n-gram probabilities observed at andaround the current token.
All LM features arebased on scores from a 7-gram language modelwith absolute discount smoothing built from theGigaword corpus (Gao et al 2001, Nguyen et al2007).We group the language model features concep-tually into five categories: basic features, ratio fea-tures, drop features, entropy delta features andmiscellaneous.
All probabilities are log probabili-ties, and n in the n-grams ranges from 1 to 5.
Allfeatures are calculated for each token w of the to-kens w0?wi in a sentence.Basic LM features consist of two features: theunigram probability of w and the average n-gramprobability of all n-grams in the sentence that con-tain w.Ratio features are based on the intuition that er-rors can be characterized as involving tokens thathave a very low ratio of higher order n-gram prob-abilities to lower order n-gram probabilities.
Inother words, these are tokens that are part of anunlikely combination of otherwise likely smaller n-grams.
These features are calculated as the ratio ofthe average x-gram probability of all x-grams con-taining w to the average y-gram probability of ally-grams containing w. The values for x and y are: 5and 1, 4 and 1, 3 and 1, 2 and 1, 5 and 4, 4 and 3, 3and 2.Drop features measure either the drop or in-crease in n-gram probability across token w. Forexample, the bigram drop at wi is the delta betweenthe bigram probability of the bigram starting at i-1to the bigram probability of the bigram starting at i.Drop features are calculated for n-grams with 2 ?
n?
5.Entropy delta features offer another way to lookat the changes of n-gram probability across a tokenw.
Forward entropy for wi is defined as the entropyof the string wi?wn where n is the index of the lasttoken in the sentence.
We calculate the entropy ofan n-gram as the language model probability ofstring wi?wn divided by the number of tokens inthat string.
Backward entropy is calculated analo-gously for w0?wi.
For n-grams with 1 ?
n ?
5, wealso calculate, at each index i into the token array,the delta between the n-gram entropy of the n-gramstarting at i and the n-gram starting at i+1 (forwardsliding entropy).
Similarly the delta between the n-gram entropy of the n-gram starting at i and the n-gram starting at i-1 (backward sliding entropy) iscalculated.183There are four miscellaneous language modelfeatures.
Three of them, minimum ratio to random,average ratio to random, and overall ratio to ran-dom address the fact that a ?good?
n-gram is likelyto have a much higher probability than an n-gramwith the same tokens in random order.
For all n-grams where 2 ?
n ?
5 we calculate the ratio be-tween the n-gram probability and the sum of theunigram probabilities.
For a token wi we producethe minimum ratio to random (the minimum ratioof all n-grams including w) and the average ratioto random (the average of all ratios of the n-gramsincluding w).
Overall ratio to random is obtainedby looping through each n-gram where 2 ?
n ?
5that includes wi and summing the n-gram proba-bilities (sum1) as well as the unigram probabilitiesof all unigrams in these n-grams (sum2).
The ratiofeature is then sum1/sum2.
The final feature ad-dresses the intuition that an erroneous word maycause n-grams that contain the word to be less like-ly than adjacent but non-overlapping n-grams.Overlap to adjacent ratio is the sum of probabili-ties of n-grams including wi, divided by the sum ofprobabilities of n-grams that are adjacent to wi butdo not include it.Note that this use of a host of language modelfeatures is substantially different from using a sin-gle language model score on hypothesized errorand potential correction to filter out unlikely cor-rection candidates as in Gamon et al (2008) andGamon (2010).5.2 String featuresString features capture information about the char-acters in a token and the tokens in a sentence.
Twobinary features indicate whether a token is capital-ized (initial capitalization or all capitalized), onefeature indicates the token length in characters andone feature measures the number of tokens in thesentence.5.3 Linguistic Analysis featuresEach sentence is linguistically analyzed by aPCFG-LA parser (Petrov et al, 2006) trained onthe Penn Treebank (Marcus et al, 1993).
A num-ber of features are extracted from the constituencytree to assess the syntactic complexity of the wholesentence, the syntactic complexity of the local en-vironment of a token, and simple constituency in-formation for each token.
These features are: labelof the parent and grandparent node, number of sib-ling nodes, number of siblings of the parent, pres-ence of a governing head node, label of thegoverning head node, and length of path to theroot.
An additional feature indicates whether thePOS tag assigned by the parser does not match thetag assigned by the POS tagger, which may indi-cate a tagging error.6.
Experiments6.1 DesignFor our experiments we use three different mutual-ly exclusive random subsets of CLC.
50K sentenc-es are used for training of the models (larger datasets exceeded the capabilities of our MEMM train-er).
In this set, we only include sentences that con-tain at least one annotated error.
We alsoexperimented using a mix of error-free and errone-ous sentences, but the resulting models turned outto be extremely skewed towards always predictingthe majority state ?O?
(no error).
20K sentences(including both erroneous and correct sentences)are used for parameter tuning and testing, respec-tively.Each token in the data is annotated with one ofthe states ?O?
or ?I?.
Performance is measured ona per token basis, i.e.
each mismatch between thepredicted state and the annotated state is counted asan error, each match is counted as a correct predic-tion.We use the development set to tune two parame-ters: the size of the Markov window and a prior toprevent overfitting.
The latter is a Gaussian prior(or quadratic regularizer) where the mean is fixedto zero and the variance is left as a free parameter.We perform a grid search to find values for theparameters that optimize the model?s F1 score onthe development data.In order to be able to report precision and recallcurves, we use a technique similar to the one de-scribed in Minkov et al (2010): we introduce anartificial feature with a constant value at trainingtime.
At test time we perform multiple runs, modi-fying the weight on the artificial feature.
Thisweight variation influences the model?s prior pro-pensity to assign each of the two states, allowingus to measure a precision/recall tradeoff.1846.2 Performance of feature setsFigure 2 illustrates the performance of three differ-ent feature sets and combinations.
The baseline isusing only language model features and standardPOS tags, which tops out at about 20% precision.Adding the string features discussed in the previ-ous section, and partially lexicalized (PL) POStags, where we used POS tags for content wordtokens and the lexicalized token for functionwords, we get a small but consistent improvement.We obtain the best performance when all featuresare used, including the linguistic analysis features(DepParse).
We found that a high-order modelwith a Markov window size of 14 performed bestfor all experiments with a top F1 score.
F1 at low-er orders was significantly worse.
Training time forthe best models was less than one hour.6.3 Predicting error typesIn our next experiment, we tried to determine howthe sequence modeling approach performs for in-dividual error types.
Here we trained eight differ-ent models, one for each of the error types in Table1.
As in the previous experiments, the developmentand test files contained error-free sentences.
Theoptimal Markov window size ranged from 8 to 15.Note that our general sequence model described inthe previous sections does not recognize differenterror types, so it was necessary to train one modelper error type for the experiments in this section.Figure 3 shows the results from this series ofexperiments.
We omit the results for other contentword error, other function word and multiple er-rors in this graph since these relatively ill-definederror classes performed rather poorly.
As Figure 3illustrates, derivational errors and preposition er-rors achieve by far the best results.
The fact thatthe individual precision never reaches the level ofthe general sequence model (Figure 2) can be at-tributed to the much smaller overall set of errors ineach of the eight training sets.
In Figure 4 we com-pare the sequence modeling results for prepositionswith results from the preposition component of thecurrent version of the system described in Gamon(2010) on the same test set.
That system consists ofa preposition-specific classifier, a language modeland a meta-classifier that combines evidence fromthe classifier and the language model.
The se-quence model approach outperforms the classifierof that system, but the full system including lan-guage model and meta-classifier achieves muchhigher precision than the sequence modeling ap-proach.6.4 Learning curve experimentsAn obvious question that arises is how much train-ing data we need for an error detection sequencemodel, i.e.
how does performance degrade as wedecrease the amount of training data from the 50Kerror-annotated sentences that were used in theprevious experiments.
To this end we producedrandom subsets of the training data in 20% incre-ments.
For each of these training sets, we deter-mined the resulting F1 score by first performingparameter tuning on the development set and thenmeasuring precision and recall of the best modelon the test set.
Results are shown in Figure 5: at20% of training data, precision starts to increase atthe cost of recall.
At 80% of the training data, re-call starts to trend up as well.
This upward trend ofboth precision and recall indicates that increasingthe amount of training data is likely to further im-prove results.6.5 Error  analysisThe precision values obtained in our experi-ments are low, but they are also based on thestrictest possible measure of accuracy: an errorprediction is only counted as correct if it exactlymatches a location and annotation in the CLC.
Amanual analysis of 400 randomly selected sentenc-es containing ?false positives?, where the systemhad 29% precision and 10% recall, by the strictestcalculation, showed that 14% of the ?false posi-tives?
identified an error that was either not anno-tated in CLC or was an error type not covered bythe system such as punctuation or case (recall fromSection 4 that for these errors we removed the er-ror annotations but retained the original string).
Anadditional 16% were adjacent to an error annota-tion.
12% had error annotations within 2-4 tokensfrom the predicted error.
Foreign language andother unknown proper names comprised an addi-tional 6%.
Finally, 9% were due to tokenizationproblems or all-upper case input that throws off thePOS tagger.
Thus the precision reported in Figure2 through Figure 6 is really a lower bound.
30% ofthe ?false positives?
either identify, or are adjacentto, an error.185Sentence length has a strong influence on theaccuracy of the sequence model.
For sentences lessthan 7 tokens long, average precision is approxi-mately 7%, whereas longer sentences average at29% precision.
This observation fits with the factthat high-order models perform best in the task, i.e.the more context a model can access, the more re-liable its predictions are.
Shorter sentences are alsoless likely to contain an error: only 12% of shortsentences contain an error, as opposed to 46% ofsentences of seven tokens or longer.For sentences that are at least 7 tokens long, er-ror predictions on the first and last two tokens (thelast token typically being punctuation) have an av-erage precision of 22% as compared to an averageof 30% at all other positions.
Other unreliable errorpredictions include those involving non-alphabeticcharacters (quotes, parentheses, symbols, numbers)with 1% precision and proper name tags with 10%precision.
Many of the predictions on NNP tagsidentify, by and large, unknown or foreign names(Cricklewood, Cajamarca).
Ignoring system flagson short sentences, symbols and NNP tags wouldimprove precision with little cost to recall.We also experimented with a precision/recallmetric that is less harsh but at the same time realis-tic for error detection.
For this ?soft metric?
wecount correct and incorrect predictions at the errorlevel instead of the token level.
An error is definedas a consecutive sequence of n error tags, where n?
1.Figure 2: Precision and recall of different feature sets.Figure 3: Precision and recall of different error models.00.10.20.30.40.50.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0precisionrecallPrecision and recallLM LM+ String + PL LM + String + PL + DepParse00.050.10.150.20.250.30.350.40.450 0.1 0.2 0.3 0.4 0.5 0.6 0.7precisionrecallPrecision and Recall per Error Typecontent deriv inflect nounphrase preposition186Figure 4: Preposition precision and recall.Figure 5: Learning curve.Figure 6: Precision and recall for adjacent annotated error00.10.20.30.40.50.60.70.80 0.1 0.2 0.3 0.4 0.5 0.6 0.7precisionrecallPrecision and Recall Prepositionssequence model full system Gamon (2010) classifier only Gamon (2010)00.050.10.150.20.250.30.350 10 20 30 40 50 60 70 80 90 100percent of training dataPrecision, recall and amount of training dataPrecision Recall00.20.40.60.810 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1precisionrecallPrecision and recall: soft metric and per sentence accuracyexact match soft metricper sentence soft metric, short sentences excludedper sentence, short sentences excluded exact match, short sentences excluded187A predicted error counts as being correct with re-spect to an annotated error if the following twocriteria are met:a) At least one predicted error token is part ofan annotated error or is directly adjacent toan annotated errorb) No more than two predicted error tokensfall outside the annotated error.Criterion (a) establishes that predicted and annotat-ed error are overlapping or at least directly adja-cent.
Criterion (b) ensures that the predicted erroris ?local?
enough to the annotated error and doesnot include too much irrelevant context, but it stillallows an annotated error to be flanked by predict-ed error tokens.
Figure 6 illustrates the preci-sion/recall characteristics of the best model whenusing this soft metric as compared to the strict met-ric.
We also included a ?per sentence?
metric inFigure 6, where we measure precision and recall atthe level of identifying a sentence as containing anerror or not, in other words when using the modelas a detector for ungrammatical sentences.
In addi-tion we show for each of the three metrics how theresults change if short sentences (shorter than 7tokens) are excluded from the evaluation.7.
Conclusion and future workWe have shown that a discriminative high ordersequence model can be used to detect errors inEnglish learner writing.
This enables a general ap-proach to error detection, at the cost of requiringannotated data.
High-order models outperformlower order models significantly for this problem.It is obvious that there are several avenues topursue in order to improve upon these initial re-sults.
Two possibilities that we would like to high-light are the model structure and the feature set.
Asmentioned in Section 3, instead of using a separatePOS tagger we could follow McCallum et al(2003) and design a model that jointly predicts twosequences: POS tags and error tags.
As for featuresets, we conducted some preliminary additionalexperiments where we added a second set of lan-guage model features, based on a different lan-guage model, namely the Microsoft web n-grammodel (Wang et al 2010).
The addition of thesefeatures raised both precision and recall.Finally, an error detection system is only ofpractical use if it is combined with a componentthat suggests possible corrections.
For future work,we envision a combination of generic error detec-tion with a corpus-based lookup system that findsalternative strings that have been observed in simi-lar contexts.
All these alternatives can then bescored by a language model in the original contextof the user input, allowing only those suggestionsto be shown to the user that achieve a better lan-guage model score than the original input.
Thiscombination of error detection and error correctionhas the advantage that the error detection compo-nent can be used to provide recall, i.e.
it can beallowed to operate at a lower precision level.
Theerror correction component, on the other hand,then reduces the number of false flags by vettingpotential corrections by language model scores.AcknowledgmentsWe would like to thank Claudia Leacock for themanual error analysis, Michel Galley for detailedcomments on an earlier draft and Chris Quirk fordiscussions and help around the MEMM modelimplementation.
The idea of the ratio to randomlanguage model features is Yizheng Cai?s.
We alsogreatly benefited from the comments of the anon-ymous reviewers.ReferencesEric Steven Atwell.
1986.
How to detect grammaticalerrors in a text without parsing it.
In Proceedings ofEACL, pp.
38-45.L?on Bottou.
1991.
Une approche th?orique del?apprentissage connexionniste: Applications ?
la re-connaissance de la parole.
Doctoral dissertation,Universit?
de Paris XI.Johnny Bigert and Ola Knutsson.
2002.
Robust errordetection: a hybrid approach combining unsupervisederror detection and linguistic knowledge.
In Proceed-ings of the Second Workshop on Robust Methods inAnalysis of Natural Language Data, pp.
10-19.Martin Chodorow and Claudia Leacock.
2000.
An un-supervised method for detecting grammatical errors.In Proceedings of NAACL, pp.
140-147.Martin Chodorow, Joel Tetreault and Na-Rae Han.2007.
Detection of grammatical errors involvingprepositions.
In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions, pp.
25-30.Rachele De Felice and Stephen G. Pulman.
2008.
Aclassifier-based approach to preposition and deter-188miner error correction in L2 English.
In Proceedingsof COLING, pp.
169-176.Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-dre Klementiev, William Dolan, Dmitriy Belenkoand Lucy Vanderwende.
2008.
Using ContextualSpeller Techniques and Language Modeling for ESLError Correction.
In Proceedings of IJCNLP.Michael Gamon.
2010.
Using mostly native data to cor-rect errors in learners?
writing.
In Proceedings ofNAACL.Jianfeng Gao, Joshua Goodman, and Jiangbo Miao.2001.
The use of clustering techniques for languagemodeling--Application to Asian languages.
Computa-tional Linguistics and Chinese Language Processing,6(1), 27-60.Na-Rae Han, Joel Tetreault, Soo-Hwa Lee and Jin-Young Ha.
2010.
Using error-annotated ESL data todevelop an ESL error correction system.
In Proceed-ings of LREC.Emi Izumi, Kiyotaka Uchimoto and Hitoshi Isahara.2004.
SST speech corpus of Japanese learners?
Eng-lish and automatic detection of learners?
errors.
In-ternational Computer Archive of Modern EnglishJournal, 28:31-48.John Lafferty, Andrew McCallum and Fernando Perei-ra.
2001.
Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.
InProceedings of ICWSM, pp.
282-289.Claudia Leacock, Martin Chodorow, Michael Gamonand Joel Tetreault.
2010.
Automated GrammaticalError Detection for Language Learners.
Morgan andClaypool.Mitchell P. Marcus, Beatrice Santorini and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics 19:313-330.Andrew McCallum, Dayne Freitag and Fernando Perei-ra.
2000.
Maximum entropy Markov models for in-formation extraction and segmentation.
InProceedings of ICML, pp.
591-598.Andrew McCallum, Khashayar Rohanimanesh andCharles Sutton.
2003.
Dynamic Conditional RandomFields for jointly labeling multiple sequences.
InProceedings of NIPS Workshop on Syntax, Semanticsand Statistics.Einat Minkov, Richard C. Wang, Anthony Tomsaic andWilliam C. Cohen.
2010.
NER systems that suit us-er?s preferences: Adjusting the Recall-Precisiontrade-off for entity extraction.
In Proceedings ofNAACL, pp.
93-96.Patrick Nguyen, Jianfeng Gao, and Milind Mahajan.2007.
MSRLM: A scalable language modelingtoolkit (MSR-TR-2007-144).
Redmond, WA: Mi-crosoft.Daisuke Okanohara and Jun?ichi Tsujii.
2007.
A dis-criminative language model with pseudo-negativesamples.
In Proceedings of ACL, pp.
73-80.Y.
Albert Park and Roger Levy.
2011.
Automatedwhole sentence grammar correction using a NoisyChannel Model.
In Proceedings of ACL 2011.Slav Petrov, Leon Barrett, Romain Thibaux and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings ofCOLING/ACL, pp.
443-440.Alla Rozovskaya and Dan Roth.
2010a.
Training Para-digms for correcting errors in grammar and usage.
InProceedings of NAACL-HLT.Alla Rozovskaya and Dan Roth.
2010b.
Generating con-fusion sets for context-sensitive error correction.
InProceedings of EMNLP.Jonas Sj?bergh.
2005.
Chunking: An unsupervisedmethod to find errors in text.
In Proceedings of the15th NODALIDA conference.Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,Zhongyang Xiong, John Lee and Chin-Yew Lin.2007.
Detecting erroneous sentences using automati-cally mined sequential patterns.
In Proceedings ofACL, pp.
81-88.Joel Tetreault and Martin Chodorow.
2008.
The ups anddowns of preposition error detection in ESL writing.In Proceedings of COLING, pp.
865-872.Kristina Toutanova, Dan Klein, Chris Manning, andYoram Singer.
2003.
Feature-rich part-of-speech tag-ging with a cyclic dependency network.
In Proceed-ings of NAACL, pp.
252-259.Joachim Wagner, Jennifer Foster, and Josef vanGenabith.
2007.
Judging grammaticality: Experi-ments in sentence classification.
In Proceedings ofEMNLP & CONLL, pp 112-121.Kuansan Wang, Christopher Thrasher, Evelyne Viegas,Xialong Li, and Paul Hsu.
2010.
An Overview ofMicrosoft web n-gram corpus and applications.
In:Proceedings of NAACL 2010.189
