|IiImmmmmThe Present  Use  of  Statistics in the Evaluat ion of  NLP ParsersJ .Entw is le  D .M.W.PowersF l inders  Un ivers i ty  of South Austra l ia  F l inders Univers i ty of South Austral iaj im@cs ,  f l i nders ,  edu.
au powers~cs ,  f l inders ,  edu.
auAbst rac tWe are concerned that the quality of resultsproduced by an NLP parser bears little, ifany, relation to the percentage-results c aimedby the various NLP parser-systems presentlyavailable for use.
To illustrate this problem,we examine one readily available NLP taggingand parsing system, the ENGCG parser; andone tagger, the Brin tagger.
We note responsesto both artificially generated and naturally oc-curring text.
The percentage assessments aremethodologically f awed, and should be takenwith a grain of salt; instead, assessment of theperformance of an NLP parser should be ef-fected by a user, and solely from a considera-tion of the resulting parses of exactly the in-put which an NLP user decides to contributefor such an assessment.
Careful attention toinput of whatever corpus the user decides on,is presently the only suitable qualifying test ofparsing ability.
The parsers available are noneof them perfectible yet, despite apparent yieldsnow quoted at 99%+.
We consider the impactof Zipf's argument of 'least effort' on percent-age assessment; and we open a discussion onestimating the relative complexities of corpora.1 IntroductionStatistics are frequently bandied around in NLP, andwould seem to be the obvious way to compare com-peting systems and methodologies.
For example:As a rule, data-driven systems rely onstatistical generalisations about short se-quences of words or tags....\[T\]hey t nd toreach a 95-97% accuracy \[and 12 parsersare referred to.\] Interestingly, no signifi-cant improvement beyond the 97% "bar-rier" by means of purely data-driven sys-tems has been reported so far...\[Then thereis a report of three hybrids - systems thatemploy linguistic rules for solving some am-biguities - with various additions; and thesehybrids\] seem capable of exceeding the97% barrier .... Next, a new system..usesonly linguistic distributional rules.
Testedagainst a 38,000-word corpus of previouslyunseen text, the tagger eaches a better ac-curacy than previous ystems (over 99%).
(Voutilainen, 1995)We are concerned about the misleading nature ofsuch published statistics, although researchers work-ing on NLP systems are of course well aware thatthe figures must be interpreted carefully.
To be ableto show our cause for concern we must look at thestatistics of a well known parser, and also at its ac-tions.
Unfortunately there are relatively few systemswhich are freely available for consultation or exami-nation, and for this reason we are forced to pick onsome of the few systems that are.
The criticisms wemake here are not directed against he parsers weuse as examples, but against he way in which ourfield is treating its statistics.Our main example is the ENGCG parser, in onespecific form which" has been available for severalyears now, and frorc hwhich many influential variantshave been spawned,~including theone being used totag the Bank of English (Voutilainen and Silvonen,1996).
Perhaps unfortunately for it, the ENGCGparser is very conveniently consulted on the Net, andstatistics on it have been published.
Another goodreason for our examination of this parser-group isthe comment made by them, above, that "the tag-ger reaches a better accuracy than previous ystems(over 99%)."
And the fact is that it does seem torepresent the best approach currently.2 Assessment  o f  the  ENGCG parserThe ENGCG configuration we are focussing on isactually a collection of tools or layers: morphologi-cal analyser, morphological disambiguator, POS tag-ger, finite-state parser and heuristic enhancementprograms, etc.
(Karlssen, 1990), (Tapanainen andEntwisle and Powers 215 Use of Statistics in the Evaluation of NLP ParsersJim Entwisle and David Powers (1998) The Present Use of Stalisties in the Evaluation of NLP Parsers.
In D.M.W.
Powers(ed.)
NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 215-224.Jarvinen, 1993), (Voutilainen, 1995), (Voutilainen,1997), (Voutilainen and Silvonen, 1996).In this paper, we will use the name "the ENGCGparser" as referring to a specific ombination ofthesecomponents in the manner that they are reported inVoutilainen (1995); in particular we are using theversion which augments the underlying tagger witha finite-state parser, with heuristics witched on.In considering and assessing the statistics on thisparser, and also the parser itself, we record that weshould not like it thought we are only trying to crit-icise that program.
So let us repeat hat in our viewthe ENGCG parser appears to be at least as effectiveas any other parser presently generally consultableor available, and that it may well be the most effec-tive member of that class today.
Indeed, we see thatparser as defining the standards for a corpus-basedparser.
But, as will be noted, we nonetheless thinkthe ENGCG parser parses poorly, when it is com-pared with human parsers - a conclusion we havereached partly because of matters discussed in ?3.2.1 The  ENGCG parser  - a stat ist icalassessment by its creatorsThe creators of the ENGCG parser make it clearthat their parser is not expected to parse contrivedsentences, and they acknowledge that sentences canbe created that will result in a less favourable re-sponse than they report; they also mention thattheir parser is intended to work on such things asmanuals and they acknowledge that it works lesswell on fictional literature.
Their claim and spec-ifications are:1,200 "grammar-based" constraints99.7-100% of all words retain the appropri-ate morphological reading3-7% of all words remain (partly) ambigu-ous200 "heuristic" constraintsresolves ome 50% of remaining ambigui-ties after heuristic disambiguation, 99.5%or more retain the appropriate morpholog-ical reading...The performance figures are measuredagainst fairly neutral running English ofthe written variety.
Similar performanceon e.g.
invented laboratory sentences i notguaranteed.
(Voutilainen and Silvonen, 1996).As to corpus, Voutilainen and Tapanainen (1993)refer to their tagging scheme as having "been man-ually applied on some 20,000 words of running textfrom various genres as well as on some 2,000 testEntwisle and Powers 216sentences from a large grammar (Quirk et al 1993),as a first approximation f the inventory of syntacticstructures in written English..." This allowed themto validate their approach informally and to "ascer-tain the generality of the proposed rules" (Vouti-lainen and Tapanainen, 1996, p4).
Voutilainen(1995, pg) later used "a 38,202-word test corpusconsisting of previously unseen journalistic, scientificand manual texts" to test the ENGCG implementa-tion of this scheme.2.2 The ENGCG parser -  our statisticalassessmentOur own assessment of the operation of the ENGCGparser, over the first 1000 words of Chapter 3 of "Al-ice in Wonderland" (Carroll, 1975) (which might notbe described as 'ffairly neutral"; but it is our owndomain of study), is that the parser reported 108(10.8%) words with extra wrong roles alongside thecorrect role, and 33 (3.3%) words with wrong rolesand no accompanying correct role.
27 of the wrongroles that had no accompanying correct role werenouns that were mis-designated as to case, but werecorrectly designated as nouns.
We did notice thatthere is no special tag specifically for the objects ofnon-finite verbs, and that these words were desig-nated as having the label nora (i.e.
"nominal") asalso were the objects of prepositional phrases; thiscaused an extra 10 responses which we regarded asunusual, but which were not counted as incorrectbecause they seemed to be correctly designated ac-cording to the designers' pecifications.Further, there were a number of words with morethan one role in positions where the words wereclearly ambiguous; and these too were accepted asbeing correct.
Thus, our results for the ENGCGparser on the first 1000 words of chapter three of"Alice in Wonderland" is that 89.2% of the wordshad no extra wrong roles and 96.7% of the wordshad a correct ag amongst their final list of roles.We have conducted a further assessment on theoperation of the ENGCG parser, over the first 1000words of Chapter four of Alice in Wonderland {Car-roll, 1975); the results there were less impressive-82.0% of words had no extra wrong roles (18.0%did), and 94.8% of words had a correct ag amongsttheir final list of roles (5.2% did not).Our conclusions regarding the ENGCG parser,and these figures on it, are that:1 for what it does, it works well, and speedily too.2 the difficulty that this parser faces is due mainlyto the limitations on the starting roles (tags inthe lexicon) on each word at the commencementUse of Statistics in the Evaluation of NLP ParsersIIIIIIIIh,h,|/mmmof parsing; the limitations are those suppliedfrom a restrictive lexicon.3 The ENGCG parser can only throw roles away,never gain any; so there is no way of redeemingany limitations inherited from the lexicon.We see point 2 as the critical one.
We suggest hata tagger or parser should not arbitrarily restrict hestarting roles of a word, lest that restriction happento exclude some legitimate parse.
Probably the as-sumption in the ENGCG parser regarding starting-roles for words is not a feature that can easily beremoved from this parser, but we will not speculatefurther on this matter except to note that relianceon this restricted set of roles pre-empts some parsingdecisions where valid possibilities are not acknowl-edged.
These roles are never added back in later,so their removal decreases the number of roles inthe final results.
An alternative approach to pars-ing, which makes no such lexical restriction on roles,is presented by Entwisle and Groves (1994), but ex-ploring this further is beyond the scope of this paper.3 The  l im i ta t ions  o f  the  ENGCGs tar t ing  ro le  l i s tsWe now proceed to demonstrate ENCG's depen-dency on the starting-roles, which we note are de-cided upon partly from semantic onsiderations; forthis purpose we create and submit to the ENGCGparser some artificially generated sentences.Our sentences themselves might be considered tobe contrived.
Thus the parser's creators may saythat our test sentences were not within the domain oftheir device.
But keep in mind what we are demon-strating here: we are not arguing that the parser justfails, nor that it fails because it does not parse thesereasonably average sentences.
We are demonstrat-ing that such failures reflect the high dependency ofthis parser on starting from this more presumptu-ous position; that such parses effected by this parserthemselves demonstrate hat that position is not anacceptable starting point for NLP.We report parses ubmitted, below.
Sentence (1):has four words that have each two starting roles(noun, verb); each role being fairly equally balancedin three of those words in English corpus.
Sentence(2): has words similar to sentence (1) but one wordinstead is disproportionately balanced, so one roleis much more likely.
Sentences (1) and (2) are syn-tactically identical.
Sentence (3) shows a parsingfailure consequent on the limited starting-roles givento the word changed for sentence (2).
Useful wordsfor demonstrating these limitations are words likewatches and sails and fish, all readily noun or verb.In these parses, each indented line following a wordin angle brackets has a separate role (or roles) forthat word: e.g.man is given roles of subject and ob-ject; fish has three roles.Our first test sentence is:(1) The man who sails boats watches fish.The ENGCG parser could say little about this; itdid say:"<*the>""the" <*> DET CENTRAL ART SG/PL ~DN>"<mall>""man" N NOM SG @SUBJ @OBJ"<who>""who" <**CLB> PRON WH NOMSG/PL @SUBJ"<sails>""sail" V PRES SG3 VFIN ~+FMAINV"<boats>""boat" N NOM PL @OBJ"<watches>""watch" N NOM PL @SUBJ ~OBJ"<fish>""fish" <P/for> V 1NF @-FMAINV"fish" <P/for> V PRES -SG3 VFIN @+FMAINV"fish" N NOM SG/PL @OBJ"<$.>"Somehow watches has lost the role of main verb,which a native speaker would say was its only avail-able role.
And there is a second syntactic parsein that sentence, not reported by ENGCG.
Thatparse is: the:article man:subject \[who:relative-pronoun sails:verb\] boats:verb watches:indirect.object fish:object.
Such a parse is, of course, seman-tically most unlikely.
Note that boats here is workingas a ditransitive verb (Entwisle and Groves, 1994).The verb ships has this subcategorisation (as in Heships them supplies each week), more commonly thanboats.The ENGCG parser's reliance on starting-roles iclear, because of the different result when a sentenceof the same inflection pattern, but with the wordwater, a word more commonly a noun, substitutedfor fish, is submitted:(2) The man who sails boats watches water.which gave:"<*the>""the" <*> DET CENTRAL ART SG/PL @DN>"<man>""man" N NOM SG @SUBJ @OBJ"<who>""who" <**CLB> PRON WH NOM SG/PL @SUBJ"<sails>""sail" V PRES SG3 VFIN @+FMAINVEntwisle and Powers 217 \ Use of Statistics in the Evaluation of NLP Parsers"<boats>""boat" N NOM PL ~OBJ"<watches>""watch" <InfComp> V PRES SG3VFIN @+FMAINV<water>""water" N NOM SG @OBJ,,<$.>"a better esult; ENGCG provided a parse that tradi-tional grammar requires, as well as a spurious parse;the reason for the improvement is clear when we ex-amine that parser's response to a sentence wherewater has the less usual, but not unusual, role ofverb, in:(3) I water the plants.
"<*i>""i" <*> PRON PERS NOM SG1 SUB3 ~SUBJ"<water>""water" N NOM SG ~NPHR"<the>""the" DET CENTRAL ART SG/PL ~DN>"<plants>""plant" N NOM PL @NPHR,,<$.>"The symbol nphr designates a "stray NP".
The pos-sibility that watercould be a verb is not entertainedby this parser; and a single, completely incorrectparse is the only result: noun noun determiner noun.An impossible answer.
Note too: ENGCG signals animproper parse: but fails to signal the failure 1.We tested the ENGCG parser further: with otherwords of a similar nature, to show that this error wasnot a feature of just one word, water.
We created alist of sentences to test starting-roles specifically:(1) The man who sails boats watches fish.
(2) The man who sails boats watches water.
(3) I water the plants.
(4) Let him water the plants.
(5) The man sands wood.
(6) He fords streams.
(7) They" dog his life.
(8) The past fades.
(9) He watches watches.
(10) They dynamite bridges.
(11) He compliments hem.
(12) He ferrets out answers.l In this connection, the separate matter of permittingan "I don't know" response is an important feature inthe correct approach to NLP but is not relevant in thispaper.
(13) He stopped hunting rabbits.
(14) He sights along the rifle.The italicised word in each sentence is the test-word, the word of interest.
Table 1 reports results.The ENGCG parser is relying to some degree onlimiting the starting roles of words to only the morelikely ones, then it starts at that point to parse -but that restriction still allow multiplication of theparses that ENGCG offers - out to twelve, for thecase of sentence (1).
This parser then attempts onlyto give the more likely reading(s), but it does notnecessarily offer a legal parse.
Because of starting-role restrictions, this parsing program is not alwaysproducing an acceptable parse, which is unsatisfac-tory.
Indeed, we view the two clear parsing failuresregarding sentences (1) and (3), in the conditions wehave established, as further evidence of a somewhatweak parsing action, one which fails to use all of thesyntactic onstraints in English, properly.We ran these sentences in the more recentENGCG-2 Tagger(Voutilainen 1995) and receivedparses that were improved, but the substantial pointremained, although to a lesser extent: that taggerhad not received the instruction to accept he verbto water as a fully equipped verb, and so a variationin sentence (3) - to(4)Let him water the plants.returned water as noun only.
Nor did that taggerreport the second parse in either of sentences (1) or(2).Because the above sentences are artificially gen-erated, the ENGCG parser has been working in adomain beyond its design; but this, the full domainof all written English, is our interest.
So, the aimof the ENGCG parser is not that of unequivocallyparsing English, in NLP.The limited set of roles provided by the lexiconprobably arises from omission, but we note-two con-sequences arising from reduction of the set.
of pos-sible tags through omission of valid, but rare roles.The first is obvious: if there is less disambiguationto perform, the tagger and parser will be faster.
Thesecond is perhaps less obvious: if rarer roles areomitted from a parser then it is incapable of cor-rectly resolving them.
It is possible to manipulatespeed and error rate by judicious omission of roles.We learn from the above demonstration that whena word of a sentence submitted to the ENGCGparser needs a lower-probability word-role, it maynot find it; in fact it returned astrange "four nounsmake a sentence" type of parse.
It was not con-strained by a grammar rule of English to say thatwater cannot be a verb - but by an arbitrary re-Entwisle and Powers 218 Use of Statistics in the Evaluation of NLP ParsersIU||mBIIIIIIIIi iIIIIIIIIIIII System Alice Chapter 3 - 1000 wordswrong extra missing wantedroles rolesI ENGCG parser 10.8% 3.3%I Brill tagger 4.9% 4.9%\[ Obvious tagger 13% 18.4%Alice Chapter 4 - 1000 words Our test sentenceswrong extra missing wanted test-word r0ieroles roles missed18% 5.2% 11/145.1% 5.1% 12/1415.5% 18.6% 10/14Table 1: A comparison of three NLP programsstraint, taken from a starting-role list.
In parsingEnglish, the proper constraints must be used; nogenuine rule of English arbitrarily limits a word'sstarting-role in this manner.3.1 Consideration of  the resultThe foregoing may appear to have been very criti-cal on one parser, and on just one point.
So let ushere remind you what was the aim of ?3: to showthat a parser can be covering over a lesser "parsing-action"; here, by a lexicon's restrictions on starting-roles for words.
We had to document fully that pre-cise claim to make clear our complaint.
After wemake a comparison with results from another tag-ger, we will make use of those findings in the subjectof this paper - parser statistics.4 The  Br i l l  taggerThe second NLP system that we will examine is theolder Brill tagger; we will first report on it, and thencompare results from both ENGCG parser and Brilltagger.
The problem of limited starting-roles, indi-cated in ?3 as causing parsing failures for the EN-GCG parser, is seen in the Brill tagger also.We found a slight complication i comparing thetwo devices; the Brill tagger offered only one tagper word.
So, in applying the scoring system above,each incorrectly tagged word is firstly a wrong extratag and then secondly a word without a correct ag- ?5.1.
This counting method might appear a littleunfair to Brilh the error counts doubly.
However,consider firstly that that scoring system follows thesame scheme that was applied to the ENGCG parserpreviously (where we had no choice but to score itth~at way, because that program could, and did, haveone or more correct ags, or a combination ofcorrectand wrong tags, or wrong tags only - on a word.
)Secondly, Brill said it was offering the alternativeof more than one tag (but it just did not for ourtest text), thereby conceding the propriety of thatscoring.
Thirdly, we assess that scoring as proper,because English sometimes offers ambiguous syntac-tic roles on words; and an NLP parser must allowfor actual English usages.In Table 1 are the results of tests o.n both theENGCG parser and the Brill tagger - in respect ofour above list of 14 test sentences, and also of the twosections of Alice in Wonderland detailed in ?2.2.
Thetwo separate results relating to each Alice chapterare "extra wrong roles" and "missed correct roles".The last column, titled "our test sentences", recordsthe count of the number of mis-calculated roles, oneach italicised word of our fourteen test sentences of?3 (scored at maximum of one error per sentence).In particular, we note that those two parsing pro-grams, ENGCG and Brill, failed to parse eleven andtwelve words (respectively), out of those fourteentest-words contained in our testing sentences.
Webelieve that our test sentences are not unnatural inany way.
2 Those results indicate to us that, in thoseprograms, the starting-roles of most of those four-teen italicised words have been unduly limited.
In-deed, the figures in the last column (titled "our testsentences") may have found a problem affecting allcorpus-based parsers.
Such a matter is not be rel-evant to this paper, and anyway we ourselves havenot yet further assessed those figures, either.As noted earlier above, we have found the EN-GCG parser to be the most effective and informativeof all readily-available current NLP parsers, and thestrongest parsing program {though the results tabu-lated above do not completely vindicate our choice.
)On this account, we have given above the detail thatthe ENGCG passer displays in its parses - for thatinformation; and we will restrict comment and ex-ample hereafter, to the ENGCG parser.5 S ta t i s t i ca l  measurement  o f  parsersWe are now in a position to consider the matter ofstatistics and parsers.
We discuss the current at-tempted statistical measurement of the quality ofparsing programs.
We find these measurements un-convincing, and indicative of a flawed methodology -2We do consider sentence (9) to be a little unnatural:that sentence was submitted so as to test further on theword watches: and in the event, both programs foundthe correct parse there, so we feel bound to include it.We have reported all the sentences we tried.Entwisle and Powers 219 Use of Statistics inthe Evaluation of NLP Parsersfor the reason that the measurements that have beenadopted are not properly indicative of any gain-or-fall index in the ability of a parser to handle thesyntax of the natural language of English.We find that there are difficulties over the use ofthese statistics in the evaluation of the qualities ofa parsing program.
Take a concrete case for whichin the foregoing section we have established the fact:that the ENGCG parser failed to have the starting-role of verb included for the word water.
This fail-ure, this omission, would almost certainly reduce thenumber of wrong roles that this parser would offeron almost any given corpus - unless the corpus justhappened to include a substantial number of refer-ences to something about watering ardens, or sim-ilar expressions: that latter usage, of the verb "towater", whilst not unusual, is probably not going tobe the usage met with by a parser using the typeof corpus that the ENGCG parser advertises.
Evenwith a number of occurrences of applying water, (i.e.using water as a verb), the parser will still probablyshow better results by omitting the possibility of itbeing a verb: for the most part the parser will oper-ate "more accurately" from the omission, so long asthe number of verb-water is fairly small.There is no verb-role given to the word water by"the ENGCG parser in its response to our example-sentence (2} above (that "non-response" was ofcourse correct); and the noun role given by it is cor-rect not only for sentence (2) but perhaps for about99% of the appearances of the word water.
Wa-ter, the noun, is a word very commonly met.
So,the omission of the consideration of a correct rolefor the case of sentence (3) above, happens to haveimproved the statistics for the parse of sentence (2);and that is so, even though another esult is that an-other ordinary sentence - sentence (3) - is wronglyparsed.How can that deterioration in the ability of aparser to handle English generally, be considered tobe an improvement in parsing quality?
We see thisas evidence 'of a methodology that demands impleimprovement in some percentage-statistics for a rea-son which logically might have been considered in-stead as causing a decay in that percentage: thereasoning behind such a variation of those statis-tics appears to us as flawed.
These particular statis-tics for parsers, then, are meaningless: the figurescould, if one really wished it, easily be increasedright up to 99% (or even higher) even if water ap-peared once or twice as a verb.
That enhancementcould be done by omitting every alternative word-role from the starting-role list if that alternative isonly seldom ever needed in the given corpus: thatwould cause substantial statistical enhancement of aparser's reaction-measurements on even a very large,genuinely naturally occurring, corpus.It could be argued that, because the parser thenfollows the main-chance reading of English, it is nowactually the better parser for that.
It is probably thefaster parser, and it will respond with less spuriousroles to many sentences, but it lacks depth in itsdeductions as a result; crucially in our view, thatparser would have failed to tap any deep regularitiesof the patterns of English.
Our own view is that untila parser is seen to be successfully handling sentenceslike sentence (3) without compromising its results onsentence (2), we do not care how good its statisticsare on even randomly chosen corpora: such figuresmeasure little, in our view.
We do not suggest hatstarting-roles here have been tampered with at all,let alne deliberately sieved for statistical improve-ments: we say that absurdities such as increasing thepositive statistics of a parser by reducing its parsingability, and such opportunities for sieving as above,mean that those statistics are valueless for evidenceof parser ability, let alne for comparison betweenparsers.
Once we see that a parser is consideringEnglish with a fair degree of depth in its appraisal,we are prepared to consider that its response to ran-domly chosen corpora has meaning; but not untilthen.
Such a use of statistics not only impairs theway parsers are evaluated but undermines the wholeidea of statistical NLP.5.1 Types  of  e r ro r  in an NLP  parserThere are two separate rrors that we are looking atin relation to any computer-based parsing system;they both relate to the grammatical role or roles thata parser is offering in respect of a word.
The first ofthese is: the failure to offer a correct role (i.e.
a roleis missing.)
The second is: the offering of an extra,incorrect role.
These two errors are essentially whatis known in statistics (Speigel 1972) as, respectively,Type I and Type II errors - names which we will usehereafter.
The two errors are also analogous to theerrors of under- and over-generation respectively.We say that, if a parser is both losing wanted roles(Type I error) and also gaining extra roles (TypeII error), each error to a non-negligible degree, thequality of the parser cannot be measured; and so thefigures for ENGCG parser, of 10.8% and 3.3% (and18% and 5.2%) - ?2.2 - are of further concern to uson this count.If one were to make small changes to a parser,changes that did not fundamentally alter the pars-ing procedure but did alter the Type I - Type II bal-ance, then the consequent reduction in one of thoseEntwisle and Powers 220 Use of Statistics in the Evaluation of NLP ParsersIIIIII!1IIIIII!iIIIIIIIIIIIIIIilIIIIIIIII!I!IIerrors would result in an increase in the other.
Thatmuch is clear.
It is, however, unlikely that the twoerrors will move proportionately.
On the contrary,for many systems, a change that causes a slight re-duction in either one, causes a large increase in theother, since parsers are generally balanced at a pointthat sets both errors to a reasonable compromise.But this point is often a finely-balanced and sensi-tive, one.
We point out this problem to make it clearthat an error-pair of 18% and 5.2% respectively doesnot imply, say, a "total error" of 23.2% or an "av-erage error" of 11.6% or any other such simplisticformulation.
In fact, Samuelsson and Voutilainen(1997) show a hyperbolic relationship between thetwo errors (for the case of a 1988-style HMM-basedparser).
They name that relationship "the Error-Rate-Ambiguity Trade-Off".Thus, we claim that if both kinds of error in aparser are non-negligible, neither figure means much.So, while "watches" and "water" (and other words)are being denied verb-roles (i.e.
error Type I is non-negligible), the power of the parser in the other di-rection (error Type II) has little meaning.
How badwill the "extra wrong roles error" (Type II) need tobecome before the "roles lost error" (Type I) van-ishes?
We cannot tell, from the information whichthose two last-mentioned writers provide.
For our-selves then, we comment that only when we see thatsentences like He dogs my life and They dynamitebridges are getting ood consideration (so error TypeI has become minimal), can an "extra wrong roleserror" (error Type II) percentage mean something.This is an alternative, and more general way, ofviewing the concern that is the central point of thispaper.
By not permitting a parsing program to con-sider fully all the possible syntactic-roles on a word(by the use of limiting starting roles, say) error TypeI is artificially raised somewhat, which lowers errorType II, possibly by a large amount; that forms ourmajor concern.We refer above to a suggestion of keeping one ofthe errors minimal.
If possible, of course, that er-ror should be brought to 0%, but the question ofwhether that is feasible is not going to be consid-ered here.E r ror -  ambiguityVoutilainen (1995) uses the term error for "errorType I" and ambiguity for any extra syntactic rolesreported by his parser.
We are concerned over suchnomenclature because we cannot tell if these extrasyntactic roles are wanted (i.e.
genuine xtra read-ings) or unwanted (i.e.
errors of Type II).
Ambiguityis not necessarily an error; several genuine readingsmay actually exist in the original text.Genuine syntactic ambiguity is a completely dif-ferent matter from any part of the subject of thispaper; and this is so, even though there are two syn-tactic readings of each of sentences (1) and (2): wedo acknowledge that this means that the two sen-tences are each syntactically ambiguous (but surelynot semantically so!
).6 The  cho ice  o f  a corpus  in an  NLPparserWe find that some kinds of naturally occurring cor-pora never give a convincing display of the power ofa parser, when the parser is tested on them alone - orindeed measured by reference to them alone.
Someof what occurs in naturally occurring text is verystandard and repetitive in structure: technical writ-ing and newspaper reports use language that is veryregular and repetitious in its patterning.
Reams ofthis kind of text may have to be fed into a parserbefore that parser has been exposed to even a fewof the many different syntactic phenomena that thelanguage of English presents.
And yet, all the time,these reams of text are prejudicing the statistics; the"percentage correct" count is nearing 100%, for areason that is completely irrelevant o the parsingexpertise of a parser - yet that, and only that, shouldbe all that is being measured.6.1 The corpus approach to parsingWe do accept hat the use of large collections of nat-urally occurring text is at least partly a resolution ofthe problems inherent in small parsers of the past,in parsers that were shown to work only on one ortwo special phenomena.
To that extent, we welcomethe corpus-based approach as a response to such anoffering.
But as we have shown, the corpus-based ap-proach is not without its own quirks.
In summary:it is not the number of words that a parser can suc-cessfully parse (or even sentence statistics, which aredramaticallylower).
The number of different linguis-tic phenomena and the amount of syntactic diversitythat the parser can successfully handle are the onlyproper measures of a parser's power to parse; that iswhat should be being measured by parser statistics.We postulate that the more syntactically complexthe corpus, the more a trial of the parser has been ef-fected.
In this regard researchers should welcome theappraisal of their parsers by others' use of inventedsentences.
But we would never defend the use ofsome absurd 'trap' of a sentence, contrived not forenquiry, clarification or proper test, but merely toscore points against a parser.
A charge of 'unnat-uralness' in a sentence can be easily resolvable bynative speakers or even by formal experiment, butEntwisle and Powers 221 Use of Statistics in the Evaluation of NLP Parsersonly once the sentence has been placed in the ap-propriate context.
A better option would be to findthe required construct (or the actual sentence) in acorpus.We believe that artificially generated natural sen-tences should be used freely by people other thanthe authors of the target in order to decide on thequality of the parsing program, but, as is commonpractice currently, a report by the creators houlduse corpus-generated measurements only.We are unhappy with riders like - "Similar per-formance on e.g.
invented laboratory sentences isnot guaranteed" (Voutilainen and Silvonen, 1996):as being almost intimidatory to critics.7 Es t imat ing  syntact i c  complex i ty  o fcorporaWe consider some of the potential to variety in lin-guistic corpora, and we have also suggested somepossible valuations of that variety.7.1 Appl icat ion of  Z ipf 's  workZipf's concept of 'least effort' may be relevant here(Zipf, 1947; Powers, 1998): Zipf argues that the sim-pler a construct is, the more often humans will wantto use it - or the more often they need it, the sim-pler they will make it.
Zipf's concept applies tocomplex syntactic onstructions too, and Zipf him-self has demonstrated that his law applies even tochunks of text as large as newspaper articles andbooks.
Thus, by application of Zipf's concept, com-plex constructions will be correspondingly rarer inappearance in a corpus.
If this extension of Zipf'sargument is valid and applicable here, as we claim,then the extension indicates a further dilut_ion ofcomplex syntactic structures in corpus, with a cor-responding further skewing of percentage r sults.Identifying and counting the number of differentconstructs begs the question in some ways, as thereis needed an extremely large parsed corpus in whichthe rare constructs occur and are recognized.
Thiswould be an interesting project to perform in theBank of English once the tagging (and preferablyparsing) project is complete.There is, however, another way we can approachthis: by identifying the obvious easy constructs andsimply counting those which are not handled.A means by which the effects of repetitive or ob-vious constructs in corpora are removed from thescored percentages of parsers, appears appropriate,then.
In order to do this, we wrote an extremelyelementary (simple) grammar s (the rules for it are3Thought-time for creation of the '2"ules" for thisnoted in Appendix A).
We then counted the num-ber of correct roles that this grammar gained, ona section of Alice in Wonderland.
This simplisticparser scored a figure of 80.7% correct on the pieceof text selected - which figure, in a sense, createsa baseline or zero-level for that piece of text.
Thisis really the way that one traditionally develops agrammar - the initial version would normally be en-hanced as further iterations are made, but whilst wewere tempted to rectify the obvious problems, thestatistics in Table 1 for this "obvious tagger" hasnot had the benefit of any refinement.This "grammar" is designed to be used only forbenchmarking, not as a real grammar or for a pro-duction parser.
It can be employed in either of twoways: On the one hand, a comparison can be madebetween different corpora, by using the figures as-sessed by the grammar to characterize the difficultyof a corpus.
Alternatively, and perhaps more inter-estingly, the figure calculated by the grammar on acorpus can be used to bring percentages of correctroles on words into a proportion which does havereal relevance to a standard of parsing.
For an ex-ample for this test, the statistics of say 97% of thetotal number of words correctly tagged by TaggerX on a corpus which has been assessed as having azero-level of 80.7%, is re-balanced proportionately -in that case down to 84.45%: by which we mean that.84.45% of the words that are a test to a substantialextent, of a parsing program, are being handled cor-rectly.
It will be noted then that the grammar's rulesleave plenty of scope to reward the parser which isoperating even moderately well, and that the tech-nique can be applied to both type I and type II er-rors.The obvious parser is somewhat rough, but itmakes the statistics offered by the measurement ofcorrect word-roles on corpora far more meaningfulas long as the baseline is set for each corpus - oth-erwise we are simply multiplying the error rate by5 if we set the benchmark ceiling to (roughly) 20%rather than the more commonly assumed 100%.Certainly, this re-balancing test means that aparser is no longer given credit for just correctly ap-plying a tag which is completely obvious anyway (forexample, the tag article for the word the), but thatis currently what is occurring.
And since the is usu-ally the most common word in English text, that isusually occurring rather too often.We conclude this section by noting that the base-line grammar's surprising success on the sentences"grammar" - about an hour, much of that hour typingand refining those rules.
This "grammar" has nothingwhatever to do with our own parser noted above.Entwisle and Powers 222 Use of Statistics in the Evaluation of  NLP ParsersIII!II!1IIIIIIIDBIIl///II/////////for which the others failed, probably results bothfrom rarer more complicated constructs not beingrecognized (which is the main point of it as a base-line) and from the fact that the grammar is the soleinfluence on the result (there is no probabilistic biasor arbitrary selection or omission of roles on openclass words).7.2 A measurement  o f  the  diff iculty- level o fa corpusRather than one attempting to locate a base-line formeasurement and comparison, it should be possi-ble to measure the comparative difficulty of corporain terms of problematic onstructs.
For this pur-pose, we chose one phenomenon as representative:the number of syntactic usages or subcategorizationsof verbs.
To do this, we first selected, at random, apoint of the text, and then we counted off the nexteighty verbs.
We then counted the number of timesa verb out of those eighty verbs was presented to usin a different form: that is, either the word had notappeared as a verb at all before or, as a verb, it hadbeen in some manner, used differently syntactically.Thus the same verb could be counted twice if it hada different subcategorisation r syntactic variation ineach of two uses in the extract; our suggestion be-ing that probably any syntactic phenomenon couldbe examined for variation, and verbs, being so cen-tral to a sentence, would probably indicate varietyas successfully as any other syntactic feature could.We believe that a variety is indicated in the re-sults that we recorded, on eighty verbs: see table 2.However, we caution that the base line for the verbsneeds to be determined from a comprehensive andrepresentative corpus of English, and this has notyet been done.On examination following that analysis, we de-cided that the scientific paper was unusually variedin its style at the particular point that we attemptedassessment, which may account for that high rating.Our own opinion is that these figures correspondedwell with the amount of variety in each piece of writ-ing.
In particular, extracts four and five were, in ouropinion, of a particularly staid and dry, repetitivestyle.From this brief study, we suggest that it maynot be difficult to assess the syntactic complexityof pieces of writing, and we intend to try each pieceof writing on the parsers themselves to see if a cor-relation can be observed between our estimate ofthe complexity of the work, the variety amongstthe verb-forms presented, and the responses of theparsers themselves.Append icesA The  s imp le  grammar  ( fo r  theObv ious  Parser )?
The text is examined for all capitalised wordsthat do not start sentences.
These words be-come the proper nouns, and are all classified asnouns.?
All words with one, single, obvious role weremerely given that role.
Thus, the articles, theunequivocal conjunctions (e.g.
because but notso), and most of the adverbs, were all just giventheir obvious role.?
Some words with an obvious class and a sec-ondary, less likely class, were given their obviousclass only.
The equivocal modals (will, might,can etc.)
were all given the role of modal, alongwith the unequivocal modals (would, shouldetc.)?
Demonstratives, equivocal possessive pronouns:were not pronouns - they were determiners only,(unless the word already had a class, otherwisegiven herein).?
The words how and so and what were conjunc-tions.?
The word that was always a relative pronoun.?
All other words which may be relative pronounswere relative pronouns.?
The word following an article was classified asa noun (unless it already had a class, otherwisegiven herein).?
The word following an adjective was classified'asa noun (unless it already had a class, otherwisegiven herein).?
The word following a noun was classified as averb (unless it already had a class, otherwisegiven herein).?
The word following a verb was classified as anoun (unless it already had a class, otherwisegiven herein).?
All -/y-inflected words were classified as ad-verbs.?
The words once, twice, etc., and either and hereand just and there were adverbs.?
very was always to the left of an adjective.?
A preposition (equivocal or not) always be-gan a prepositional phrase and the prepo-sitional phrases were only ever of the formEntwisle and Powers 223 Use of Statistics in the Evaluation of NLP ParsersBook Book genre Number of different verb-formsnumberFiction bookFiction bookResearch Scientific paperScientific bookTextbook6455594448Table 2: Count of different verb-forms for various extractspreposition article/determiner object-of-prepositional-phrase or preposition object-of-prepositional-phrase.?
The word too was an intensifier.?
The word thought was always an ed-inflectedword.?
The word to was always an infinitive, coupledwith the word following it.?
All parts of the verbs to be and to have (otherthan the infinite itself, dealt with in the rulelast above) were treated as auxiliaries.?
All ing-inflected words were classified as partici-ples.?
All ed-inflected words were classified:?
in all cases, all part-tense forms of the En-glish "strong-verbs" being treated as ed-inflected words (with, of course, the furtheradvantages to parsing given by the diffen-tiated forms of preterite and past-particpleforms of "strong-verbs".)?
if an auxiliary was to its left, then as a par-ticiple (so forming a verb, with the auxil-iary).?
if otherwise than the last, then as a verb.?
And anything not classified (as well as, ofcourse, anything improperly classified) by theabove rules, was wrong.This grammar and its SCHEME LISP implemen-tation is available from the first author on request.Re ferencesLewis Carroll.
Alice's Adventures in Wonderland;and Through the looking glass and what Alicefound there.
Oxford University Press, London,1975.Jim Entwisle and Michael Groves.
A method of pars-ing English based on sentence form.
In Interna-tional Conference on New Methods in LanguageProcessing.
Centre for Computational Linguistics,1994.Karlssen.
Constraint grammar as a framework forparsing unrestricted text.
In Proceedings of the13th International Conference of ComputationalLinguistics,, volume 3, pages pp.168-173, 1990.David Powers.
Applications and explanations ofzipf's law.
In International Conference on NewMethods in Language Processing, 1998.Randolph Quirk, Sidney Greenbaum, GeoffreyLeech, and Jan Svartvik.
A ComprehensiveGrammar of the English Language.
Longman,Great Britain, 1993.Christer Samuelsson and Atro Voutilainen.
Compar-ing a linguistic and a stochastic tagger.
In Pro-ceedings of the 17th International Conference ofComputational Linguistics, 1997.M.
R. Speigel.
Theory and Problems of Statistics.McGraw Hill, 1972.Pasi Tapanainen and Timo Jarvinen.
Syntacticanalysis of natual language using linguistic rulesand corpus-based patterns.
In Proceedings ofCOLING-9J, Kyoto, Japan, 1994.Atro Voutilainen.
A sytax-based part-of-speechanalyser.
In Proceedings of the Seventh Confer-ence of the European Chapter of the Associationfor Computational Linguistics, Dublin, 1995.Atro Voutilainen and Mikko Silvonen.
A short in-troduction to engcg, http://www.lingsoft.fi/doc/engcg/intro/components.html, 1996.Atro Voutilainen and Pasi Tapanainen.
Ambiguityresolution in a reductionistic parser.
In Proceed-ings of EACL '93, 1993.George K. Zip/.
Human behaviour and the principleof least effort.
A.W., 1947.Entwisle and Powers 224 Use of Statistics in the Evaluation of NLP ParsersIIIIIIIIIgIIIIIIIIIgIImm
