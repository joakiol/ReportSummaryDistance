Proceedings of the 12th European Workshop on Natural Language Generation, pages 94?97,Athens, Greece, 30 ?
31 March 2009. c?2009 Association for Computational LinguisticsA Wizard-of-Oz environment to study Referring Expression Generationin a Situated Spoken Dialogue TaskSrinivasan JanarthanamSchool of InformaticsUniversity of EdinburghEdinburgh EH8 9ABs.janarthanam@ed.ac.ukOliver LemonSchool of InformaticsUniversity of EdinburghEdinburgh EH8 9ABolemon@inf.ed.ac.ukAbstractWe present a Wizard-of-Oz environmentfor data collection on Referring Expres-sion Generation (REG) in a real situatedspoken dialogue task.
The collected datawill be used to build user simulation mod-els for reinforcement learning of referringexpression generation strategies.1 IntroductionIn this paper, we present a Wizard-of-Oz (WoZ)environment for data collection in a real situatedspoken dialogue task for referring expression gen-eration (REG).
Our primary objective is to studyhow participants (hereafter called users) with dif-ferent domain knowledge and expertise interpretand resolve different types of referring expressions(RE) in a situated dialogue context.
We also studythe effect of the system?s lexical alignment dueto priming (Pickering and Garrod, 2004) by theuser?s choice of REs.
The users follow instruc-tions from an implemented dialogue manager andrealiser to perform a technical but realistic task ?setting up a home Internet connection.
The dia-logue system?s utterances are manipulated to con-tain different types of REs - descriptive, technical,tutorial or lexically aligned REs, to refer to variousdomain objects in the task.
The users?
responsesto different REs are then logged and studied.
(Janarthanam and Lemon, 2009) presented aframework for reinforcement learning of optimalnatural language generation strategies to chooseappropriate REs to users with different domainknowledge expertise.
For this, we need user sim-ulations with different domain knowledge profilesthat are sensitive to the system?s choice of REs.
AWoZ environment is an ideal tool for data collec-tion to build data-driven user simulations.
How-ever, our study requires a novel WoZ environment.In section 2, we present prior related work.
Sec-tion 3 describes the task performed by partici-pants.
In section 4, we describe the WoZ envi-ronment in detail.
Section 5 describes the datacollected in this experiment and section 6 presentssome preliminary results from pilot studies.2 Related Work(Whittaker et al, 2002) present a WoZ environ-ment to collect data concerning dialogue strate-gies for presenting restaurant information to users.This study collects data on strategies used by usersand human expert wizards to obtain and present in-formation respectively.
(van Deemter et al, 2006)present methods to collect data (the TUNA cor-pus) for REG using artificially constructed pic-tures of furniture and photographs of real people.
(Arts, 2004) presents a study choosing betweentechnical and descriptive expressions for instruc-tion writing.In contrast to the above studies, our study isnovel in that it collects data from users having dif-ferent levels of expertise in a real situated task do-main, and for spontaneous spoken dialogue.
Ourfocus is on choosing between technical, descrip-tive, tutorial, and lexically aligned expressionsrather than selecting different attributes for gen-erating descriptions.3 The Domain TaskIn this experiment, the task for each user is to lis-ten to and follow the instructions from the WoZsystem and set up their home broadband Internetconnection.
We provide the users with a home-like environment with a desktop computer, phonesocket and a Livebox package from Orange con-taining cables and components such as the mo-dem, broadband filters and a power adaptor.
Dur-ing the experiment, they set up the Internet con-nection by connecting these components to eachother.
Prior to the task, the users are informed thatthey are interacting with a spoken dialogue system94that will give them instructions to set up the con-nection.
However, their utterances are interceptedby a human wizard.
The users are requested tohave a conversation as if they were talking to a hu-man operator, asking for clarifications if they areconfused or fail to understand the system?s utter-ances.
The system?s utterances are converted au-tomatically to speech using the Cereproc SpeechSynthesiser and played back to the user.
The userfollows the instructions and assembles the compo-nents.
The setup is examined by the wizard at theend of the experiment to measure the percentage oftask success.
The user also fills in questionnairesprior to and after the task answering questions onhis background, quality of the system during thetask and the knowledge gained during the task.4 The Wizard-of-Oz environmentThe Wizard-of-Oz environment facilitates the en-tire experiment as described in the section above.The environment consists of the Wizard Interac-tion Tool, the dialogue system and the wizard.
Theusers wear a headset with a microphone.
Their ut-terances are relayed to the wizard who then anno-tates it using the Wizard Interaction Tool (shownin figure 1) and sends it to the dialogue system.The system responds with a natural language ut-terance which is automatically converted to speechand is played back to the user and the wizard.4.1 Wizard Interaction Tool (WIT)The Wizard Interaction Tool (WIT) (shown in fig-ure 1) allows the wizard to interact with the dia-logue system and the user.
The GUI is divided into several panels.a.
System Response Panel - This panel displaysthe dialogue system?s utterances and RE choicesfor the domain objects in the utterance.
It also dis-plays the strategy adopted by the system currentlyand a visual indicator of whether the system?s ut-terance is being played to the user.b.
Confirmation Request Panel - This panel letsthe wizard handle issues in communication (fore.g.
noise).
The wizard can ask the user to repeat,speak louder, confirm his responses, etc using ap-propriate pre-recorded messages or build his owncustom messages.c.
Confirmation Panel - This panel lets the wiz-ard handle confirmation questions from the user.The wizard can choose ?yes?
or ?no?
or build a cus-tom message.yes ?Yes it is on?no ?No, its not flashing?ok ?Ok.
I did that?req description ?Whats an ethernet cable?
?req location ?Where is the filter?
?req verify jargon ?Is it the ethernet cable?
?req verify desc ?Is it the white cable?
?req repeat ?Please repeat?req rephrase ?What do you mean?
?req wait ?Give me a minute?
?Table 1: User Dialogue Acts.d.
Annotation panel - This panel lets the wizardannotate the content of participant?s utterances.User responses (dialogue acts and example utter-ances) that can be annotated using this panel aregiven in Table 1.
In addition to these, other be-haviours, like remaining silent or saying irrelevantthings are also accommodated.e.
User?s RE Choice panel - The user?s choiceof REs to refer to the domain objects are annotatedby the wizard using this panel.4.2 The Instructional Dialogue ManagerThe dialogue manager drives the conversation bygiving instructions to the users.
It follows a deter-ministic dialogue management policy so that weonly study variation in the decisions concerningthe choice of REs.
It should be noted that typi-cal WoZ environments (Whittaker et al, 2002) donot have dialogue managers and the strategic de-cisions will be taken by the wizard.
Our dialoguesystem has three main responsibilities - choosingthe RE strategy, giving instructions and handlingclarification requests.The dialogue system, initially randomlychooses the RE strategy at the start of thedialogue.
The list of strategies are as follows.1.
Jargon: Choose technical terms for every ref-erence to the domain objects.2.
Descriptive: Choose descriptive terms for ev-ery reference to the domain objects.3.
Tutorial: Use technical terms, but also aug-ment the description for every reference.The above three strategies are also augmentedwith an alignment feature, so that the system caneither align or not align with the user?s prior choiceof REs.
In aligned strategies, the system abandonsthe existing strategy (jargon, descriptive or tuto-rial) for a domain object reference when the user95Figure 1: Wizard Interaction Tooluses a different expression from that of the systemto refer to the domain object.
For instance, underthe descriptive strategy, the ethernet cable is re-ferred to as ?the thick cable with red ends?.
Butif the user refers to it as ?ethernet cable?, then thesystem uses ?ethernet cable?
in subsequent turnsinstead of the descriptive expression.
In case ofnon-aligned strategies, the system simply ignoresuser?s use of novel REs and continues to use itsown strategy.The step-by-step instructions to set up thebroadband connection are hand-coded as a dia-logue script.
The script is a simple determinis-tic finite state automaton, which contains execu-tion instruction acts(e.g.
Plug in the cable in tothe socket) and observation instruction acts(e.g.
Isthe ethernet light flashing?)
for the user.
Basedon the user?s response, the system identifies thenext instruction.
However, the script only con-tains the dialogue acts.
The dialogue acts are thenprocessed by a built-in realiser component to cre-ate the system utterances.
The realiser uses tem-plates in which references to domain objects arechanged based on the selected strategy to createfinal utterances.
By using a fixed dialogue man-agement policy and by changing the REs, we onlyexplore users?
reactions to various RE strategies.The utterances are finally converted to speech andare played back to the user.The dialogue system handles two kinds of clar-ification requests - open requests and closed re-quests.
With open CRs, users request the sys-tem for location of various domain objects (e.g.
?where is the ethernet cable??)
or to describethem.
With closed CRs, users verify the intendedreference, in case of ambiguity (e.g.
?Do youmean the thin white cable with grey ends?
?, ?Isit the broadband filter?
?, etc.).
The system han-dles these requests using a knowledge base of thedomain objects.4.3 Wizard ActivitiesThe primary responsibility of the wizard is to un-derstand the participant?s utterance and annotateit as one of the dialogue acts in the Annotationpanel, and send the dialogue act to the dialoguesystem for response.
In addition to the primaryresponsibility, the wizard also requests confirma-tion from the user (if needed) and also responds toconfirmation requests from the user.
The wizardalso observes the user?s usage of novel REs andrecords them in the User?s RE Choice panel.
Asmentioned earlier, our wizard neither decides onwhich strategy to use to choose REs nor chooses96the next task instruction to give the user.5 Data collectedSeveral different kinds of data are collected before,during and after the experiment.
This data will beused to build user simulations and reward func-tions for learning REG strategies and languagemodels for speech recognition.1.
WIT log - The WIT logs the whole conversa-tion as an XML file.
The log contains system anduser dialogue acts, time of system utterance, sys-tem?s choice of REs and its utterance at every turn.It also contains the dialogue start time, total timeelapsed, total number of turns, number of wordsin system utterances, number of clarification re-quests, number of technical, descriptive and tuto-rial REs, number of confirmations etc.2.
Background of the user - The user is asked to fillin a pre-task background questionnaire containingqueries on their experience with computers, Inter-net and dialogue systems.3.
User satisfaction survey - The user is re-quested to fill in a post-task questionnaire contain-ing queries on the performance of the system dur-ing the task.
Each question is answered in a fourpoint Likert scale on how strongly the user agreesor disagrees with the given statement.
Statementslike, ?Conversation with the system was easy?,?I would use such a system in future?, etc arejudged by the user which will be used to build re-ward functions for reinforcement learning of REGstrategies.4.
Knowledge pre-test - Users?
initial domainknowledge is tested by asking them to match a listof technical terms to their respective descriptiveexpressions.5.
Knowledge gain post-test - Users?
knowledgegain during the dialogue task is measured by ask-ing them to redo the matching task.6.
Percentage of task completion - The wizardexamines the final set up on the user?s table todetermine the percentage of task success using aform containing declarative statements describingthe ideal broadband set up (for e.g.
?the broad-band filter is plugged in to the phone socket onthe wall?).
The wizard awards one point to everystatement that is true of the user?s set up.7.
User?s utterances WAV file - The user?s ut-terances are recorded in WAV format for build-ing language models for automatic speech recog-nition.6 Results from pilot studiesWe are currently running pilot studies (with 6 par-ticipants so far) and have collected around 60 min-utes of spoken dialogue data.
We found that inthe jargon strategy, some users take a lot longer tofinish the task than others (max 59 turns, min 26turns).
We found that besides requesting clarifi-cations, sometimes novice users assume incorrectreferences to some domain objects, affecting theirtask completion rates.7 ConclusionWe have presented a novel Wizard-of-Oz environ-ment to collect spoken data in a real situated taskenvironment, and to study user reactions to a va-riety of REG strategies, including system align-ment.
The data will be used for training user sim-ulations for reinforcement learning of REG strate-gies to choose between technical, descriptive, tu-torial, and aligned REs based on a user?s expertisein the task domain.AcknowledgementsThe research leading to these results has re-ceived funding from the European Community?sSeventh Framework (FP7) under grant agree-ment no.
216594 (CLASSiC Project www.classic-project.org), EPSRC project no.
EP/E019501/1,and the British Council (UKIERI PhD Scholar-ships 2007-08).ReferencesA.
Arts.
2004.
Overspecification in Instructive Text.Ph.D.
thesis, Tilburg University, The Netherlands.S.
Janarthanam and O.
Lemon.
2009.
Learning Lexi-cal Alignment Policies for Generating Referring Ex-pressions for Spoken Dialogue Systems.
In Proc.ENLG?09.M.
J. Pickering and S. Garrod.
2004.
Toward a mech-anistic psychology of dialogue.
Behavioral andBrain Sciences, 27:169?225.K.
van Deemter, I. van der Sluis, and A. Gatt.2006.
Building a semantically transparent corpusfor the generation of referring expressions.
In Proc.INLG?06.S.
Whittaker, M. Walker, and J. Moore.
2002.
Fishor Fowl: A Wizard of Oz Evaluation of DialogueStrategies in the Restaurant Domain.
In LanguageResources and Evaluation Conference.97
