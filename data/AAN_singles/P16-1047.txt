Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 495?504,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsNeural Networks For Negation Scope DetectionFederico Fancellu and Adam Lopez and Bonnie WebberSchool of InformaticsUniversity of Edinburgh11 Crichton Street, Edinburghf.fancellu[at]sms.ed.ac.uk, {alopez,bonnie}[at]inf.ed.ac.ukAbstractAutomatic negation scope detection is atask that has been tackled using differ-ent classifiers and heuristics.
Most sys-tems are however 1) highly-engineered, 2)English-specific, and 3) only tested on thesame genre they were trained on.
We startby addressing 1) and 2) using a neuralnetwork architecture.
Results obtained ondata from the *SEM2012 shared task onnegation scope detection show that evena simple feed-forward neural network us-ing word-embedding features alone, per-forms on par with earlier classifiers, witha bi-directional LSTM outperforming allof them.
We then address 3) by means ofa specially-designed synthetic test set; indoing so, we explore the problem of de-tecting the negation scope more in depthand show that performance suffers fromgenre effects and differs with the type ofnegation considered.1 IntroductionAmongst different extra-propositional aspects ofmeaning, negation is one that has received a lotof attention in the NLP community.
Previous workhave focused in particular on automatically detect-ing the scope of negation, that is, given a nega-tive instance, to identify which tokens are affectedby negation (?2).
As shown in (1), only the firstclause is negated and therefore we mark he and thecar, along with the predicate was driving as insidethe scope, while leaving the other tokens outside.
(1) He was not driving the car and she left togo home.In the BioMedical domain there is a long lineof research around the topic (e.g.
Velldal et al(2012) and Prabhakaran and Boguraev (2015)),given the importance of recognizing negation forinformation extraction from medical records.
Inmore general domains, efforts have been morelimited and most of the work centered around the*SEM2012 shared task on automatically detectingnegation (?3), despite the recent interest (e.g.machine translation (Wetzel and Bond, 2012;Fancellu and Webber, 2014; Fancellu and Webber,2015)).The systems submitted for this shared task,although reaching good overall performance arehighly feature-engineered, with some relying onheuristics based on English (Read et al (2012)) oron tools that are available for a limited number oflanguages (e.g.
Basile et al (2012), Packard et al(2014)), which do not make them easily portableacross languages.
Moreover, the performance ofthese systems was only assessed on data of thesame genre (stories from Conan Doyle?s SherlockHolmes) but there was no attempt to test theapproach on data of different genre.Given these shortcomings, we investigatewhether neural network based sequence-to-sequence models (?
4) are a valid alternative.
Thefirst advantage of neural networks-based methodsfor NLP is that we could perform classificationby means of unsupervised word-embeddingsfeatures only, under the assumption that they alsoencode structural information previous systemhad to explicitly represent as features.
If thisassumption holds, another advantage of contin-uous representations is that, by using a bilingualword-embedding space, we would be able totransfer the model cross-lingually, obviating theproblem of the lack of annotated data in otherlanguages.The paper makes the following contributions:1.
Comparable or better performance: Weshow that neural networks perform on parwith previously developed classifiers, witha bi-directional LSTM outperforming them495when tested on data from the same genre.2.
Better understanding of the problem: We an-alyze in more detail the difficulty of detectingnegation scope by testing on data of differentgenre and find that the performance of word-embedding features is comparable to that ofmore fine-grained syntactic features.3.
Creation of additional resources: We cre-ate a synthetic test set of negative sentencesextracted from Simple English Wikipedia (?5) and annotated according to the guidelinesreleased during the *SEM2012 shared task(Morante et al, 2011), that we hope willguide future work in the field.2 The taskBefore formalizing the task, we begin by givingsome definitions.
A negative sentence n is definedas a vector of words ?w1, w2...wn?
containing oneor more negation cues, where the latter can be aword (e.g.
not), a morpheme (e.g.
im-patient) ora multi-word expression (e.g.
by no means, nolonger) inherently expressing negation.A word is a scope token if included in thescope of a negation cue.
Following Blancoand Moldovan (2011), in the *SEM2012 sharedtask the negation scope is understood as partof a knowledge representation focused around anegated event along with its related semantic rolesand adjuncts (or its head in the case of a nominalevent).
This is exemplified in (2) (from Blanco andMoldovan (2011)) where the scope includes boththe negated event eat along the subject the cow,the object grass and the PP with a fork.
(2) The cow did n?t eat grass with a fork.1Each cue defines its own negation instance, heredefined as a tuple I(n,c) where c ?
{1,0}|n|is avector of length n s.t.
ci= 1 if wiis part of the cueand 0 otherwise.
Given I the goal of automaticscope detection is to predict a vector s ?
{O,I}|n|s.t.
si= I (inside of the scope) if wiis in the scopeof the cue or O (outside) otherwise.In (3) for instance, there are two cues, not andno longer, each one defining a separate negationinstance, I1(n,c1) and I2(n,c2), and each with itsown scope, s1 and s2.
In both (3a) and (3b), n =1In the *SEM2012 shared task, negation is not consideredas a downward monotone function and definite expressionsare included in its scope.
[I, do, not, love, you, and, you, are, no, longer,invited]; in (3a), the vector c1 is 1 only at index 3(w2=?not?
), while in (3b) c2 is 1 at position 9, 10(where w9w10= ?no longer?
); finally the vectorss1 and s2 are I only at the indices of the wordsunderlined and O anywhere else.
(3) a. I do not love you and you are nolonger invitedb.
I do not love you and you are nolonger invitedThere are the two main challenges involved in de-tecting the scope of negation: 1) a sentence cancontain multiple instances of negation, sometimesnested and 2) scope can be discontinuous.
Asfor 1), the classifier must correctly classify eachword as being inside or outside the scope and as-sign each word to the correct scope; in (4) for in-stance, there are two negation cues and thereforetwo scopes, one spanning the entire sentence (3a.
)and the other the subordinate only (3b.
), with thelatter being nested in the former (given that, ac-cording to the guidelines, if we negate the event inthe main, we also negate its cause).
(4) a. I did not drive to school because mywife was not feeling well .2b.
I did not drive to school becausemy wife was not feeling well .In (5), the classifier should instead be able to cap-ture the long range dependency between the sub-ject and its negated predicate, while excluding thepositive VP in the middle.
(5) Naomi went to visit her parents to givethem a special gift for their anniversary butnever came back .In the original task, the performance of the classi-fier is assessed in terms of precision, recall andF1measure over the number of words correctlyclassified as part of the scope (scope tokens) andover the number of scopes predicted that exactly2One might object that the scope only spans over the sub-ordinate given that it is the part of the scope most likely to beinterpreted as false (It is not the case that I drove to schoolbecause my wife was not at home, but for other reasons).
Inthe *SEM2012 shared task however this is defined separatelyas the focus of negation and considered as part of the scope.One reason to distinguish the two is the high ambiguity ofthe focus: one can imagine for instance that if the speakerstresses the words to school this will be most likely consid-ered the focus and the statement interpreted as It is not thecase that I drive to school because my wife was not feelingwell (but I drove to the hospital instead).496match the gold scopes (exact scope match).
Asfor latter, recall is a measure of accuracy since wescore how many scopes we fully predict (true posi-tives) over the total number of scopes in our test set(true positives and false negatives); precision takesinstead into consideration false positives, that isthose negation instances that are predicted as hav-ing a scope but in reality don?t have any.
This isthe case of the interjection No (e.g.
?No, leave heralone?)
that never take scope.3 Previous workTable 1 summarizes the performance of systemspreviously developed to resolve the scope of nega-tion in non-Biomedical texts.In general, supervised classifiers perform betterthan rule-based systems, although it is a combina-tion of hand-crafted heuristics and SVM rankersto achieve the best performance.
Regardless of theapproach used, the syntactic structure (either con-stituent or dependency-based) of the sentence isoften used to detect the scope of negation.
Thisis because the position of the cue in the treealong with the projection of its parent/governor arestrong indicators of scope boundaries.
Moreover,given that during training we basically learn whichsyntactic patterns the scope are likely to span, itis also possible to hypothesize that this systemshould scale well to other genre/domain, as longas we can have a parse for the sentence; this how-ever was never confirmed empirically.
Althoughinformative, these systems suffers form three mainshortcomings: 1) they are highly-engineered (as inthe case of Read et al (2012)) and syntactic fea-tures add up to other PoS, word and lemma n-gramfeatures, 2) they rely on the parser producing a cor-rect parse and 3) they are English specific.Other systems (Basile et al, 2012; Packard etal., 2014) tried to traverse a semantic representa-tion instead.
Packard et al (2014) achieves thebest results so far, using hand-crafted heuristics totraverse the MRS (Minimal Recursion Semantics)structures of negative sentences.
If the semanticparser cannot create a reliable representation fora sentence, the system ?backs-off?
to the hybridmodel of Read et al (2012), which uses syntacticinformation instead.
This system suffers howeverfrom the same shortcomings mentioned above, inparticular, given that MRS representation can onlybe built for a small set of languages.4 Scope detection using Neural NetworksIn this paper, we experiment with two differ-ent neural networks architecture: a one hiddenlayer feed-forward neural network and a bi-directional LSTM (Long Short Term Memory,BiLSTM below) model.
We chose to ?start sim-ple?
from a feed-forward network to investigatewhether even a simple model can reach good per-formance using word-embedding features only.We then turned to a BiLSTM because a betterfit for the task.
BiLSTM are sequential modelsthat operate both in forward and backwards fash-ion; the backward pass is especially important inthe case of negation scope detection, given thata scope token can appear in a string before thecue and it is therefore important that we see thelatter first to classify the former.
We opted inthis case for LSTM over RNN cells given thattheir inner composition is able to better retain use-ful information when backpropagating the error.4Both networks take as input a single negativeinstance I(n,c).
We represent each word wi?
nas a d-dimensional word-embedding vector x ?Rd(d=50).
In order to encode information aboutthe cue, each word is also represented by a cue-embedding vector c ?
Rdof the same dimension-ality of x. c can only take two representations, cue,if ci=1, or notcue otherwise.
We also define Evxdwas the word-embedding matrix, where v is the vo-cabulary size, and E2xdcas the cue-embedding ma-trix.In the case of a feed-forward neural network,the input for each word wi?
n is the concate-nation of its representation with the ones of itsneighboring words in a context window of lengthl.
This is because feed-forward networks treat theinput units as separate and information about howwords are arranged as sequences must be explic-itly encoded in the input.
We define these con-catenations xconcand cconcas xwi?l...xwi?1; xwi;xwi+1...xwi+land cwi?l...cwi?1; cwi; cwi+1...cwi+lrespectively.
We chose the value of l after analyz-ing the negation scopes in the dev set.
We foundthat although the furthest scope tokens are 23 and31 positions away from the cue on the left and theright respectively, 95% of the scope tokens fall ina window of 9 tokens to the left and 15 to the right,these two values being the window sizes we con-4For more details on LSTM and related mathematical for-mulations, we refer to reader to Hochreiter and Schmidhuber(1997)497Scope tokens3Exact scope matchMethod Prec.
Rec.
F1Prec.
Rec.
F1*SEM2012ClosedtrackUiO1 (Read et al, 2012) heuristics + SVM 81.99 88.81 85.26 87.43 61.45 72.17UiO2 (Lapponi et al, 2012) CRF 86.03 81.55 83.73 85.71 62.65 72.39FBK (Chowdhury and Mahbub, 2012) CRF 81.53 82.44 81.89 88.96 58.23 70.39UWashington (White, 2012) CRF 83.26 83.77 83.51 82.72 63.45 71.81UMichigan (Abu-Jbara and Radev, 2012) CRF 84.85 80.66 82.70 90.00 50.60 64.78UABCoRAL (Gyawali and Solorio, 2012) SVM 85.37 68.86 76.23 79.04 53.01 63.46OpentrackUiO2 (Lapponi et al, 2012) CRF 82.25 82.16 82.20 85.71 62.65 72.39UGroningen (Basile et al, 2012) rule-based 69.20 82.27 75.15 76.12 40.96 53.26UCM-1 (de Albornoz et al, 2012) rule-based 85.37 68.53 76.03 82.86 46.59 59.64UCM-2 (Ballesteros et al, 2012) rule-based 58.30 67.70 62.65 67.13 38.55 48.98Packard et al (2014) heuristics + SVM 86.1 90.4 88.2 98.8 65.5 78.7Table 1: Summary of previous work on automatic detection of negation scope.sider for our input.
The probability of a given in-put is then computed as follows:h = ?
(Wxxconc+ Wccconc+ b)y = g(Wyh + by)(1)where W and b the weight and biases matrices,h the hidden layer representation, ?
the sigmoidactivation function and g the softmax operation(g(zm)= ezm/?kezk) to assign a probability tothe input of belonging to either the inside (I) oroutside (O) of the scope classes.In the biLSTM, no concatenation is performed,given that the structure of the network is alreadysequential.
The input to the network for each wordwiare the word-embedding vector xwiand thecue-embedding vector cwi, where wiconstitutes atime step.
The computation of the hidden layerat time t and the output can be represented as fol-lows:it= ?
(W(i)xx + W(i)cc + W(i)hht?1+ b(i))ft= ?
(W(f)xx + W(f)cc + W(f)hht?1+ b(f))ot= ?
(W(o)xx + W(o)cc + W(o)hht?1+ b(o))c?t= tanh(W(c)xx + W(c)cc + W(c)hht?1+ b(c))ct= ft?
c?t?1+ it?
c?thback/forw= ot?
tanh(ct)yt= g(Wy(hback;hforw) + by)(2)where the Ws are the weight matrices, ht?1thehidden layer state a time t-1, it, ft, otthe input,forget and the output gate at the time t and hback;hforwthe concatenation of the backward and for-ward hidden layers.Finally, in both networks our training objectiveis to minimise, for each negative instance, the neg-ative log likelihood J(W,b) of the correct predic-tions over gold labels:J(W, b) = ?1ll?i=1y(wi)log h?
(x(wi))+ (1?
y(wi)) log(1?
h?
(x(wi)))(3)where l is the length of the sentence n ?
I, x(wi)the probability for the word wito belong to eitherthe I or O class and y(wi)its gold label.An overview of both architectures is shown inFigure 1.4.1 ExperimentsTraining, development and test set are a col-lection of stories from Conan Doyle?s SherlockHolmes annotated for cue and scope of negationand released in concomitance with the *SEM2012shared task.5For each word, the correspondentlemma, POS tag and the constituent subtree it be-longs to are also annotated.
If a sentence containsmultiple instances of negation, each is annotatedseparately.Both training and testing is done on negativesentences only, i.e.
those sentences with at leastone cue annotated.
Training and test size are of848 and 235 sentences respectively.
If a sentencecontains multiple negation instances, we create asmany copies as the number of instances.
If thesentence contains a morphological cue (e.g.
im-patient) we split it into affix (im-) and root (pa-tient), and consider the former as cue and the latteras part of the scope.Both neural network architectures are imple-mented using TensorFlow (Abadi et al, 2015)with a 200-units hidden layer (400 in total for twoconcatenated hidden layers in the BiLSTM), theAdam optimizer (Kingma and Ba, 2014) with a5For the statistics regarding the data, we refer the readerto Morante and Blanco (2012).498Figure 1: An example of scope detection usingfeed-forward and BiLSTM for the tokens ?you areno longer invited?
in the instance in ex.
(3b).starting learning rate of 0.0001, learning rate de-cay after 10 iterations without improvement andearly stopping.
In both cases we experimentedwith different settings:1.
Simple baseline: In order to understand howhard the task of negation scope detection is,we created a simple baseline by tagging aspart of the scope all the tokens 4 words tothe left and 6 to the right of the cue; thesevalues were found to be the average span ofthe scope in either direction in the trainingdata.2.
Cue info (C): The word-embedding matrix israndomly initialised and updated relying onthe training data only.
Information about thecue is fed through another set of embeddingvectors, as shown in 4.
This resembles the?Closed track?
of the *SEM2012 shared tasksince no external resource is used.3.
Cue info + external embeddings (E): This isthe same as setting (2) except that the embed-dings are pre-trained using external data.
Weexperimented with both keeping the word-embedding matrix fixed and updating it dur-ing training but we found small or no dif-ference between the two settings.
To dothis, we train a word-embedding matrix us-ing Word2Vec (Mikolov et al, 2013) on 770million tokens (for a total of 30 million sen-tences and 791028 types) from the ?One Bil-lion Words Language Modelling?
dataset6and the Sherlock Holmes data (5520 sen-tences) combined.
The dataset was tokenizedand morphological cues split into negationaffix and root to match the Conan Doyle?sdata.
In order to perform this split, wematched each word against an hand-craftedlist of words containing affixal negation7; thismethod have an accuracy of 0.93 on the Co-nan Doyle test data.4.
Adding PoS / Universal PoS information(PoS/uni PoS): This was mainly to assesswhether we could get further improvement byadding additional information.
For all the set-ting above, we also add an extra embeddinginput vector for the POS or Universal POSof each word wi.
As for the word and the cueembeddings, PoS-embedding information arefed to the hidden layer through a separateweight matrix.
When pre-trained, the train-ing data for the external PoS-embedding ma-trix is the same used for building the wordembedding representation, except that in thiscase we feed the PoS / Universal PoS tag foreach word.
As in (3), we experimented withboth updating the tag-embedding matrix andkeeping it fixed but found again small or nodifference between the two settings.
In or-der to maintain consistency with the originaldata, we perform PoS tagging using the GE-NIA tagger (Tsuruoka et al, 2005)8and thenmap the resulting tags to universal POS tags.94.2 ResultsThe results for the scope detection task are shownin Table 2.6Available at https://code.google.com/archive/p/word2vec/7The list was courtesy of Ulf Hermjakob and NathanSchneider.8https://github.com/saffsd/geniatagger9Mapping available at https://github.com/slavpetrov/universal-pos-tags499Results for both architecture when word-embedding features only are used (C and C + E)show that neural networks are a valid alternativefor scope detection, with bi-directional LSTM be-ing able to outperform all previously developedclassifiers on both scope token recognition and ex-act scope matching.
Moreover, a bi-directionalLSTM shows similar performance to the hybridsystem of Packard et al (2014) (rule-based +SVM as a back-off) in absence of any hand-craftedheuristics.It is also worth noticing that although pre-training the word-embedding and PoS-embeddingmatrices on external data leads to a slight improve-ment in performance, the performance of the sys-tems using internal data only is already competi-tive; this is a particularly positive result consider-ing that the training data is relatively small.Finally, adding universal POS related infor-mation leads to a better performance in mostcases.
The fact that the best system is built usinglanguage-independent features only is an impor-tant result when considering the portability of themodel across different languages.4.3 Error analysisIn order to understand the kind of errors our bestclassifier makes, we performed an error analysison the held-out set.First, we investigate whether the per-instanceprediction accuracy correlates with scope-related(length of the scope to the left, to the right andcombined; maximum length of the gap in a discon-tinuous scope) and cue-related (type of cue -one-word, prefixal, suffixal, multiword-) variables.
Wealso checked whether the neural network is biasedtowards the words it has seen in the training(forinstance, if it has seen the same token always la-beled as O it will then classify it as O).
For our bestbiLSTM system, we found only weak to moderatenegative correlations with the following variables:?
length of the gap, if the scope is discontinu-ous (r=-0.1783, p = 0.004);?
overall scope length (r=-0.3529, p < 0.001);?
scope length to the left and to the right (r=-0.3251 and -0.2659 respectively with p <0.001)?
presence of a prefixal cue (r=-0.1781, p =0.004)?
presence of a multiword cue (r=-0.1868, p =0.0023)meaning that the variables considered are notstrong enough to be considered as error patterns.For this reason we also manually analyzed the96 negation scopes that the best biLSTM systempredicted incorrectly and noticed several error pat-terns:?
in 5 cases, the scope should only span on thesubordinate but end up including elementsfrom the main.
In (6) for instance, where thesystem prediction is reported in curly brack-ets, the BiLSTM ends up including the mainpredicate with its subject in the scope.
(6) You felt so strongly about itthat {I knew you could} not{think of Beecher without thinking ofthat also} .?
in 5 cases, the system makes an incorrect pre-diction in presence of the syntactic inversion,where a subordinate appears before the mainclause; in (7) for instance, the system ex-tends the prediction to the main clause whenthe scope should instead span the subordinateonly.
(7) But {if she does} not {wish to shieldhim she would give his name}?
in 8 cases, where two VPs, one positive andone negative, are coordinated, the systemends up including in the scope the positiveVP as well, as shown in (8).
We hypothe-sized this is due to the lack of such examplesin the training set.
(8) Ah, {you do} n?t {know Sarah ?stemper or you would wonder nomore} .As in Packard et al (2014), we also noticed thatin 15 cases, the gold annotations do not follow theguidelines; in the case of a negated adverb in par-ticular, as shown in (9a) and (9b) the annotationsdo not seem to agree on whether consider as scopeonly the adverb or the entire clause around it.500Scope tokens Exact scope matchSystem gold tp fp fn Prec.
Rec.
F1Prec.
Rec.
F1Baseline 1830 472 3031 1358 13.47 25.79 17.70 0.0 0.0 0.0Best closed track: UiO1 N/A N/A N/A N/A 81.99 88.81 85.26 87.43 61.45 72.17Packard et al (2014) N/A N/A N/A N/A 86.1 90.4 88.2 98.8 65.5 78.7FF - C 1830 1371 273 459 83.39 74.91 78.92 93.61 34.10 50.00FF - C + PoS 1830 1413 235 417 85.74 77.21 81.25 92.51 37.50 53.33FF - C + Uni PoS 1830 1435 276 395 83.86 78.41 81.05 93.06 36.57 52.51FF - C + E 1830 1455 398 375 78.52 79.50 79.01 89.53 30.19 45.16FF - C + PoS + E 1830 1413 179 417 88.75 77.21 82.58 96.63 44.23 60.68FF - C + Uni PoS + E 1830 1412 158 418 89.93 77.15 83.05 96.58 43.46 59.94BiLSTM - C 1830 1583 175 247 90.04 86.50 88.23 98.71 58.77 73.68BiLSTM - C + PoS 1830 1591 203 239 88.68 86.93 87.80 98.70 58.01 73.07BiLSTM - C + Uni Pos 1830 1592 193 238 89.18 86.95 88.07 98.96 57.63 72.77BiLSTM - C + E 1830 1570 157 260 90.90 85.79 88.27 99.37 60.83 75.47BiLSTM - C + PoS + E 1830 1546 148 284 91.26 84.48 87.74 98.75 60.30 74.88BiLSTM - C + Uni PoS + E 1830 1552 124 272 92.62 85.13 88.72 99.40 63.87 77.77Table 2: Results for the scope detection task on the held-out set.
Results are plotted against the simple baseline, the best systemso far (Packard et al, 2014) and the system with the highest F1for scope tokens classification amongst the ones submitted forthe *SEM2012 shared task.
We also report the number of gold scope tokens, true positive (tp), false positives(fp) and falsenegatives(fn).
(9) a.
[...] tossing restlessly from side to side[..]b.
[...] glaring helplessly at the frightfulthing which was hunting him down.5 Evaluation on synthetic data set5.1 MethodologyOne question left unanswered by previous work iswhether the performance of scope detection classi-fiers is robust against data of a different genre andwhether different types of negation lead to differ-ence in performance.
To answer this, we comparetwo of our systems with the only original submis-sion to the *SEM2012 we found available (White,2012)10.
We decided to use both our best sys-tem, BiLSTM+C+UniPoS+E and a sub-optimalsystems, BiLSTM+C+E to also assess the robust-ness of non-English specific features.The synthetic test set here used is built on sen-tences extracted from Simple Wikipedia and man-ually annotated for cue and scope according to theannotation guidelines released in concomitancewith the *SEM2012 shared task (Morante et al,2011).
We created 7 different subsets to test dif-ferent types of negative sentences:Simple: we randomly picked 50 positive sen-tences, containing only one predicate, no dates andno named entities, and we made them negative by10In order for the results to be comparable, we feed White?ssystem with the cues from the gold-standard instead of auto-matically detecting them.adding a negation cue (do support or minor mor-phological changes were added when required).
Ifmore than a lexical negation cue fit in the context,we used them all by creating more than one nega-tive counterpart, as shown in (10).
The sentenceswere picked to contain different kind of predicates(verbal, existential, nominal, adjectival).
(10) a.
Many people disagree on the topicb.
Many people do not disagree on thetopicc.
Many people never disagree on thetopicLexical: we randomly picked 10 sentences11foreach lexical (i.e.
one-word) cue in training data(these are not, no, none, nobody, never, without)Prefixal: we randomly picked 10 sentences foreach prefixal cue in the training data (un-, im-, in-,dis-, ir-)Suffixal: we randomly picked 10 sentences forthe suffixal cue -less.Multi-word: we randomly picked 10 sen-tences for each multi-word cue (neither...nor,nolonger,by no means).Unseen: we include 10 sentences for each ofthe negative prefixes a- (e.g.
a-cyclic), ab- (e.g.ab-normal) non- (e.g.
non-Communist) that arenot annotated as cue in the Conan Doyle corpus,11In some cases, we ended up with more than 10 examplesfor some cues given that some of the sentences we pickedcontained more than a negation instance.501Scope tokens Exact scope matchData gold tp fp fn Prec.
Rec.
F1Prec.
Rec.
F1White (2012)simple 850 830 0 20 100.00 97.65 98.81 100.00 93.98 96.90lexical 814 652 101 162 86.59 80.10 83.22 100.00 58.41 73.75prefixal 316 232 103 83 68.98 73.40 71.12 100.00 32.76 49.35suffixal 100 78 7 22 91.76 78.00 84.32 100.00 69.23 81.82multi-word 269 190 12 49 89.62 70.63 79.00 100.00 9.00 16.67unseen 220 138 40 82 77.53 62.73 69.35 100.00 38.89 56.00avg.
2569 2120 263 418 85.74 77.08 80.97 100.00 50.37 62.41BiLSTM - C+ Esimple 850 827 0 23 100.00 97.29 98.62 100.00 88.72 94.02lexical 814 618 120 133 85.01 83.66 84.33 100.00 40.35 57.50prefixal 316 235 156 81 60.10 74.36 66.47 100.00 10.34 18.75suffixal 100 53 5 47 91.52 53.46 67.50 100.00 15.28 26.66multi-word 269 192 22 79 93.65 71.37 81.01 100.00 36.36 53.00unseen 220 151 79 69 66.09 69.05 67.54 100.00 22.22 36.36avg.
2569 2076 382 432 82.72 74.86 77.57 100.00 35.54 47.76BiLSTM - C+ UniPos + Esimple 850 816 0 34 100.00 96 97.95 100.00 82.70 90.05lexical 814 668 97 146 87.32 82.06 84.61 100.00 42.10 59.25prefixal 316 231 128 85 64.34 73.10 68.44 100.00 20.68 34.28suffixal 100 54 3 47 94.73 53.46 68.35 100.00 38.46 55.55multi-word 269 202 19 67 91.40 75.09 82.44 100.00 27.27 42.85unseen 220 152 56 71 73.07 68.16 70.53 100.00 25.00 40.00avg.
2569 2123 303 449 85.14 74.64 78.72 100.00 39.36 53.66Table 3: Results for the scope detection task on the synthetic test set.to test whether the system can generalise the clas-sification to unseen cues.125.2 ResultsTable 3. shows the results for the comparison onthe synthetic test set.
The first thing worth noticingis that by using word-embedding features only itis possible to reach comparable performance witha classifier using syntactic features, with univer-sal PoS generally contributing to a better perfor-mance; this is particularly evident in the multi-word and lexical sub-sets.
In general, genre ef-fects hinder both systems; however, consideringthat the training data is less than 1000 sentences,results are relatively good.Performance gets worse when dealing withmorphological cues and in particular in the case ofour classifier, with suffixal cues; at a closer inspec-tion however, the cause of such poor performanceis attributable to a discrepancy between the an-notation guidelines and the training data, alreadynoted in ?4.4.
The guidelines state in fact that ?Ifthe negated affix is attached to an adverb that isa complement of a verb, the negation scopes overthe entire clause?
(Morante et al, 2011, p. 21) andwe annotated suffixal negation in this way.
How-ever, 3 out of 4 examples of suffixal negation inadverbs in the training data (e.g.
9a.)
mark the12The data, along with the code, is freely available athttps://github.com/ffancellu/NegNNscope on the adverbial root only and that?s whatour classifiers learn to do.Finally, it can be noticed that our system doesworse at exact scope matching than the CRF clas-sifier.
This is because White (2012)?s CRF modelis build on constituency-based features that willthen predict scope tokens based on constituentboundaries (which, as we said, are good indica-tor of scope boundaries), while neural networks,basing the prediction only on word-embedding in-formation, might extend the prediction over theseboundaries or leave ?gaps?
within.6 Conclusion and Future WorkIn this work, we investigated and confirmed thatneural networks sequence-to-sequence models area valid alternative for the task of detecting thescope of negation.
In doing so we offer a detailedanalysis of its performance on data of differentgenre and containing different types of negation,also in comparison with previous classifiers, andfound that non-English specific continuous repre-sentation can perform batter than or on par withmore fine-grained structural features.Future work can be directed towards answeringtwo main questions:Can we improve the performance of our classi-fier?
To do this, we are going to explore whetheradding language-independent structural informa-502tion (e.g.
universal dependency information) canhelp the performance on exact scope matching.Can we transfer our model to other languages?Most importantly, we are going to test the modelusing word-embedding features extracted from abilingual embedding space.AcknowledgmentsThis project was also founded by the EuropeanUnions Horizon 2020 research and innovationprogramme under grant agreement No 644402(HimL).The authors would like to thank Naomi Saphra,Nathan Schneider and Claria Vania for the valu-able suggestions and the three anonymous review-ers for their comments.ReferencesM Abadi, A Agarwal, P Barham, E Brevdo, Z Chen,C Citro, GS Corrado, A Davis, J Dean, M Devin,et al 2015.
Tensorflow: Large-scale machine learn-ing on heterogeneous systems.
White paper, GoogleResearch.Amjad Abu-Jbara and Dragomir Radev.
2012.Umichigan: A conditional random field model forresolving the scope of negation.
In Proceedings ofthe First Joint Conference on Lexical and Computa-tional Semantics-Volume 1: Proceedings of the mainconference and the shared task, and Volume 2: Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation, pages 328?334.
Association forComputational Linguistics.Miguel Ballesteros, Alberto D?
?az, Virginia Francisco,Pablo Gerv?as, Jorge Carrillo De Albornoz, andLaura Plaza.
2012.
Ucm-2: a rule-based approachto infer the scope of negation via dependency pars-ing.
In Proceedings of the First Joint Conferenceon Lexical and Computational Semantics-Volume 1:Proceedings of the main conference and the sharedtask, and Volume 2: Proceedings of the Sixth Inter-national Workshop on Semantic Evaluation, pages288?293.
Association for Computational Linguis-tics.Valerio Basile, Johan Bos, Kilian Evang, and NoortjeVenhuizen.
2012.
Ugroningen: Negation detectionwith discourse representation structures.
In Pro-ceedings of the First Joint Conference on Lexicaland Computational Semantics-Volume 1: Proceed-ings of the main conference and the shared task, andVolume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation, pages 301?309.Association for Computational Linguistics.Eduardo Blanco and Dan I Moldovan.
2011.
Someissues on detecting negation from text.
In FLAIRSConference, pages 228?233.
Citeseer.Md Chowdhury and Faisal Mahbub.
2012.
Fbk:Exploiting phrasal and contextual clues for nega-tion scope detection.
In Proceedings of the FirstJoint Conference on Lexical and ComputationalSemantics-Volume 1: Proceedings of the main con-ference and the shared task, and Volume 2: Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation, pages 340?346.
Association forComputational Linguistics.Jorge Carrillo de Albornoz, Laura Plaza, Alberto D?
?az,and Miguel Ballesteros.
2012.
Ucm-i: A rule-basedsyntactic approach for resolving the scope of nega-tion.
In Proceedings of the First Joint Conferenceon Lexical and Computational Semantics-Volume 1:Proceedings of the main conference and the sharedtask, and Volume 2: Proceedings of the Sixth Inter-national Workshop on Semantic Evaluation, pages282?287.
Association for Computational Linguis-tics.Federico Fancellu and Bonnie L Webber.
2014.
Ap-plying the semantics of negation to smt through n-best list re-ranking.
In EACL, pages 598?606.Federico Fancellu and Bonnie Webber.
2015.
Trans-lating negation: A manual error analysis.
ExProM2015, page 1.Binod Gyawali and Thamar Solorio.
2012.
Uabco-ral: a preliminary study for resolving the scope ofnegation.
In Proceedings of the First Joint Con-ference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference andthe shared task, and Volume 2: Proceedings of theSixth International Workshop on Semantic Evalua-tion, pages 275?281.
Association for ComputationalLinguistics.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Diederik Kingma and Jimmy Ba.
2014.
Adam: Amethod for stochastic optimization.
arXiv preprintarXiv:1412.6980.Emanuele Lapponi, Erik Velldal, Lilja ?vrelid, andJonathon Read.
2012.
Uio 2: sequence-labelingnegation using dependency features.
In Proceedingsof the First Joint Conference on Lexical and Com-putational Semantics-Volume 1: Proceedings of themain conference and the shared task, and Volume2: Proceedings of the Sixth International Workshopon Semantic Evaluation, pages 319?327.
Associa-tion for Computational Linguistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in neural information processingsystems, pages 3111?3119.Roser Morante and Eduardo Blanco.
2012.
* sem 2012shared task: Resolving the scope and focus of nega-tion.
In Proceedings of the First Joint Conference503on Lexical and Computational Semantics-Volume 1:Proceedings of the main conference and the sharedtask, and Volume 2: Proceedings of the Sixth Inter-national Workshop on Semantic Evaluation, pages265?274.
Association for Computational Linguis-tics.Roser Morante, Sarah Schrauwen, and Walter Daele-mans.
2011.
Annotation of negation cues and theirscope: Guidelines v1.
Computational linguisticsand psycholinguistics technical report series, CTRS-003.Woodley Packard, Emily M Bender, Jonathon Read,Stephan Oepen, and Rebecca Dridan.
2014.
Sim-ple negation scope resolution through deep parsing:A semantic solution to a semantic problem.
In ACL(1), pages 69?78.Vinodkumar Prabhakaran and Branimir Boguraev.2015.
Learning structures of negations from flat an-notations.
Lexical and Computational Semantics (*SEM 2015), page 71.Jonathon Read, Erik Velldal, Lilja ?vrelid, andStephan Oepen.
2012.
Uio 1: Constituent-baseddiscriminative ranking for negation resolution.
InProceedings of the First Joint Conference on Lexicaland Computational Semantics-Volume 1: Proceed-ings of the main conference and the shared task, andVolume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation, pages 310?318.Association for Computational Linguistics.Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,Tomoko Ohta, John McNaught, Sophia Ananiadou,and Jun?ichi Tsujii.
2005.
Developing a robust part-of-speech tagger for biomedical text.
Advances ininformatics, pages 382?392.Erik Velldal, Lilja ?vrelid, Jonathon Read, andStephan Oepen.
2012.
Speculation and negation:Rules, rankers, and the role of syntax.
Computa-tional linguistics, 38(2):369?410.Dominikus Wetzel and Francis Bond.
2012.
Enrich-ing parallel corpora for statistical machine transla-tion with semantic negation rephrasing.
In Proceed-ings of the Sixth Workshop on Syntax, Semanticsand Structure in Statistical Translation, pages 20?29.
Association for Computational Linguistics.James Paul White.
2012.
Uwashington: Negation res-olution using machine learning methods.
In Pro-ceedings of the First Joint Conference on Lexicaland Computational Semantics-Volume 1: Proceed-ings of the main conference and the shared task, andVolume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation, pages 335?339.Association for Computational Linguistics.504
