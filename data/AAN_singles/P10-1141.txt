Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1386?1395,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsA study of Information Retrieval weighting schemes for sentiment analysisGeorgios PaltoglouUniversity of WolverhamptonWolverhampton, United Kingdomg.paltoglou@wlv.ac.ukMike ThelwallUniversity of WolverhamptonWolverhampton, United Kingdomm.thelwall@wlv.ac.ukAbstractMost sentiment analysis approaches use asbaseline a support vector machines (SVM)classifier with binary unigram weights.In this paper, we explore whether moresophisticated feature weighting schemesfrom Information Retrieval can enhanceclassification accuracy.
We show that vari-ants of the classic tf.idf scheme adaptedto sentiment analysis provide significantincreases in accuracy, especially when us-ing a sublinear function for term frequencyweights and document frequency smooth-ing.
The techniques are tested on a wideselection of data sets and produce the bestaccuracy to our knowledge.1 IntroductionThe increase of user-generated content on the webin the form of reviews, blogs, social networks,tweets, fora, etc.
has resulted in an environ-ment where everyone can publicly express theiropinion about events, products or people.
Thiswealth of information is potentially of vital im-portance to institutions and companies, providingthem with ways to research their consumers, man-age their reputations and identify new opportuni-ties.
Wright (2009) claims that ?for many busi-nesses, online opinion has turned into a kind ofvirtual currency that can make or break a productin the marketplace?.Sentiment analysis, also known as opinion min-ing, provides mechanisms and techniques throughwhich this vast amount of information can be pro-cessed and harnessed.
Research in the field hasmainly, but not exclusively, focused in two sub-problems: detecting whether a segment of text, ei-ther a whole document or a sentence, is subjectiveor objective, i.e.
contains an expression of opin-ion, and detecting the overall polarity of the text,i.e.
positive or negative.Most of the work in sentiment analysis has fo-cused on supervised learning techniques (Sebas-tiani, 2002), although there are some notable ex-ceptions (Turney, 2002; Lin and He, 2009).
Pre-vious research has shown that in general the per-formance of the former tend to be superior to thatof the latter (Mullen and Collier, 2004; Lin andHe, 2009).
One of the main issues for supervisedapproaches has been the representation of docu-ments.
Usually a bag of words representation isadopted, according to which a document is mod-eled as an unordered collection of the words thatit contains.
Early research by Pang et al (2002) insentiment analysis showed that a binary unigram-based representation of documents, according towhich a document is modeled only by the pres-ence or absence of words, provides the best base-line classification accuracy in sentiment analysisin comparison to other more intricate representa-tions using bigrams, adjectives, etc.Later research has focused on extending thedocument representation with more complex fea-tures such as structural or syntactic informa-tion (Wilson et al, 2005), favorability mea-sures from diverse sources (Mullen and Collier,2004), implicit syntactic indicators (Greene andResnik, 2009), stylistic and syntactic feature selec-tion (Abbasi et al, 2008), ?annotator rationales?
(Zaidan et al, 2007) and others, but no systematicstudy has been presented exploring the benefits ofemploying more sophisticated models for assign-ing weights to word features.In this paper, we examine whether term weight-ing functions adopted from Information Retrieval(IR) based on the standard tf.idf formula andadapted to the particular setting of sentiment anal-ysis can help classification accuracy.
We demon-strate that variants of the original tf.idf weightingscheme provide significant increases in classifica-tion performance.
The advantages of the approachare that it is intuitive, computationally efficient1386and doesn?t require additional human annotationor external sources.
Experiments conducted on anumber of publicly available data sets improve onthe previous state-of-the art.The next section provides an overview of rel-evant work in sentiment analysis.
In section 3we provide a brief overview of the original tf.idfweighting scheme along with a number of variantsand show how they can be applied to a classifica-tion scenario.
Section 4 describes the corpora thatwere used to test the proposed weighting schemesand section 5 discusses the results.
Finally, weconclude and propose future work in section 6.2 Prior WorkSentiment analysis has been a popular researchtopic in recent years.
Most of the work has fo-cused on analyzing the content of movie or gen-eral product reviews, but there are also applica-tions to other domains such as debates (Thomas etal., 2006; Lin et al, 2006), news (Devitt and Ah-mad, 2007) and blogs (Ounis et al, 2008; Mishne,2005).
The book of Pang and Lee (2008) presentsa thorough overview of the research in the field.This section presents the most relevant work.Pang et al (2002) conducted early polarityclassification of reviews using supervised ap-proaches.
They employed Support Vector Ma-chines (SVMs), Naive Bayes and Maximum En-tropy classifiers using a diverse set of features,such as unigrams, bigrams, binary and term fre-quency feature weights and others.
They con-cluded that sentiment classification is more dif-ficult that standard topic-based classification andthat using a SVM classifier with binary unigram-based features produces the best results.A subsequent innovation was the detection andremoval of the objective parts of documents andthe application of a polarity classifier on the rest(Pang and Lee, 2004).
This exploited text coher-ence with adjacent text spans which were assumedto belong to the same subjectivity or objectivityclass.
Documents were represented as graphs withsentences as nodes and association scores betweenthem as edges.
Two additional nodes representedthe subjective and objective poles.
The weightsbetween the nodes were calculated using three dif-ferent, heuristic decaying functions.
Finding a par-tition that minimized a cost function separated theobjective from the subjective sentences.
They re-ported a statistically significant improvement overa Naive Bayes baseline using the whole text butonly slight increase compared to using a SVMclassifier on the entire document.Mullen and Collier (2004) used SVMs and ex-panded the feature set for representing documentswith favorability measures from a variety of di-verse sources.
They introduced features based onOsgood?s Theory of Semantic Differentiation (Os-good, 1967) using WordNet to derive the valuesof potency, activity and evaluative of adjectivesand Turney?s semantic orientation (Turney, 2002).Their results showed that using a hybrid SVMclassifier, that uses as features the distance of doc-uments from the separating hyperplane, with allthe above features produces the best results.Whitelaw et al (2005) added fine-grained se-mantic distinctions in the feature set.
Their ap-proach was based on a lexicon created in a semi-supervised fashion and then manually refined Itconsists of 1329 adjectives and their modifiers cat-egorized under several taxonomies of appraisal at-tributes based on Martin and White?s AppraisalTheory (2005).
They combined the produced ap-praisal groups with unigram-based document rep-resentations as features to a Support Vector Ma-chine classifier (Witten and Frank, 1999), result-ing in significant increases in accuracy.Zaidan et al (2007) introduced ?annotator ra-tionales?, i.e.
words or phrases that explain thepolarity of the document according to human an-notators.
By deleting rationale text spans from theoriginal documents they created several contrastdocuments and constrained the SVM classifier toclassify them less confidently than the originals.Using the largest training set size, their approachsignificantly increased the accuracy on a standarddata set (see section 4).Prabowo and Thelwall (2009) proposed a hy-brid classification process by combining in se-quence several ruled-based classifiers with a SVMclassifier.
The former were based on the Gen-eral Inquirer lexicon (Wilson et al, 2005), theMontyLingua part-of-speech tagger (Liu, 2004)and co-occurrence statistics of words with a setof predefined reference words.
Their experimentsshowed that combining multiple classifiers canresult in better effectiveness than any individualclassifier, especially when sufficient training dataisn?t available.In contrast to machine learning approachesthat require labeled corpora for training, Lin and1387He (2009) proposed an unsupervised probabilis-tic modeling framework, based on Latent Dirich-let Allocation (LDA).
The approach assumes thatdocuments are a mixture of topics, i.e.
proba-bility distribution of words, according to whicheach document is generated through an hierarchi-cal process and adds an extra sentiment layer toaccommodate the opinionated nature (positive ornegative) of the document.
Their best attained per-formance, using a filtered subjectivity lexicon andremoving objective sentences in a manner similarto Pang and Lee (2004), is only slightly lower thanthat of a fully-supervised approach.3 A study of non-binary weightsWe use the terms ?features?, ?words?
and ?terms?interchangeably in this paper, since we mainly fo-cus on unigrams.
The approach nonetheless caneasily be extended to higher order n-grams.
Eachdocument D therefore is represented as a bag-of-words feature vector: D ={w1, w2, ..., w|V |}where |V | is the size of the vocabulary (i.e.
thenumber of unique words) and wi, i = 1, .
.
.
, |V |is the weight of term i in document D.Despite the significant attention that sentimentanalysis has received in recent years, the best ac-curacy without using complex features (Mullenand Collier, 2004; Whitelaw et al, 2005) or ad-ditional human annotations (Zaidan et al, 2007) isachieved by employing a binary weighting scheme(Pang et al, 2002), where wi = 1, if tfi > 0 andwi = 0, if tfi = 0, where tfi is the number oftimes that term i appears in document D (hence-forth raw term frequency) and utilizing a SVMclassifier.
It is of particular interest that using tfiin the document representation usually results indecreased accuracy, a result that appears to be incontrast with topic classification (Mccallum andNigam, 1998; Pang et al, 2002).In this paper, we also utilize SVMs but ourstudy is centered on whether more sophisticatedthan binary or raw term frequency weighting func-tions can improve classification accuracy.
Webase our approach on the classic tf.idf weightingscheme from Information Retrieval (IR) and adaptit to the domain of sentiment classification.3.1 The classic tf.idf weighting schemesThe classic tf.idf formula assigns weight wi toterm i in document D as:wi = tfi ?
idfi = tfi ?
logNdfi(1)where tfi is the number of times term i occurs inD, idfi is the inverse document frequency of termi, N is the total number of documents and dfi isthe number of documents that contain term i.The utilization of tfi in classification is ratherstraightforward and intuitive but, as previouslydiscussed, usually results in decreased accuracyin sentiment analysis.
On the other hand, usingidf to assign weights to features is less intuitive,since it only provides information about the gen-eral distribution of term i amongst documents ofall classes, without providing any additional evi-dence of class preference.
The utilization of idfin information retrieval is based on its ability todistinguish between content-bearing words (wordswith some semantical meaning) and simple func-tion words, but this behavior is at least ambiguousin classification.Table 1: SMART notation for term frequency vari-ants.
maxt(tf) is the maximum frequency of anyterm in the document and avg dl is the averagenumber of terms in all the documents.
For ease ofreference, we also include the BM25 tf scheme.The k1 and b parameters of BM25 are set to theirdefault values of 1.2 and 0.95 respectively (Joneset al, 2000).Notation Term frequencyn (natural) tfl (logarithm) 1 + log(tf)a (augmented) 0.5 + 0.5?tfmaxt(tf)b (boolean){1, tf > 00, otherwiseL (log ave) 1+log(tf)1+log(avg dl)o (BM25) (k1+1)?tfk1((1?b)+b?
dlavg dl)+tf3.2 Delta tf.idfMartineau and Finin (2009) provide a solution tothe above issue of idf utilization in a classificationscenario by localizing the estimation of idf to thedocuments of one or the other class and subtract-ing the two values.
Therefore, the weight of term1388Table 2: SMART notation for inverse documentfrequency variants.
For ease of reference we alsoinclude the BM25 idf factor and also present theextensions of the original formulations with their?
variants.Notation Inverse Document Fre-quencyn (no) 1t (idf) logNdfp (prob idf) logN?dfdfk (BM25 idf) logN?df+0.5df+0.5?
(t) (Delta idf) logN1?df2N2?df1?(t?)
(Delta smoothedidf)logN1?df2+0.5N2?df1+0.5?
(p) (Delta prob idf) log (N1?df1)?df2df1?(N2?df2)?(p?)
(Delta smoothedprob idf)log (N1?df1)?df2+0.5(N2?df2)?df1+0.5?
(k) (Delta BM25 idf) log (N1?df1+0.5)?df2+0.5(N2?df2+0.5)?df1+0.5i in document D is estimated as:wi = tfi ?
log2(N1dfi,1)?
tfi ?
log2(N2dfi,2)= tfi ?
log2(N1 ?
dfi,2dfi,1 ?N2) (2)where Nj is the total number of training docu-ments in class cj and dfi,j is the number of train-ing documents in class cj that contain term i. Theabove weighting scheme was appropriately namedDelta tf.idf .The produced results (Martineau and Finin,2009) show that the approach produces betterresults than the simple tf or binary weightingscheme.
Nonetheless, the approach doesn?t takeinto consideration a number of tested notions fromIR, such as the non-linearity of term frequency todocument relevancy (e.g.
Robertson et al (2004))according to which, the probability of a documentbeing relevant to a query term is typically sub-linear in relation to the number of times a queryterm appears in the document.
Additionally, theirapproach doesn?t provide any sort of smoothingfor the dfi,j factor and is therefore susceptible toerrors in corpora where a term occurs in docu-ments of only one or the other class and thereforedfi,j = 0 .3.3 SMART and BM25 tf.idf variantsThe SMART retrieval system by Salton (1971) isa retrieval system based on the vector space model(Salton and McGill, 1986).
Salton and Buckley(1987) provide a number of variants of the tf.idfweighting approach and present the SMART nota-tion scheme, according to which each weightingfunction is defined by triples of letters; the firstone denotes the term frequency factor, the sec-ond one corresponds to the inverse document fre-quency function and the last one declares the nor-malization that is being applied.
The upper rowsof tables 1, 2 and 3 present the three most com-monly used weighting functions for each factor re-spectively.
For example, a binary document repre-sentation would be equivalent to SMART.bnn1or more simply bnn, while a simple raw term fre-quency based would be notated as nnn or nncwith cosine normalization.Table 3: SMART normalization.Notation Normalizationn (none) 1c (cosine) 1?w21+w22+...+w2nSignificant research has been done in IR on di-verse weighting functions and not all versions ofSMART notations are consistent (Manning et al,2008).
Zobel and Moffat (1998) provide an ex-haustive study but in this paper, due to space con-straints, we will follow the concise notation pre-sented by Singhal et al (1995).The BM25 weighting scheme (Robertson et al,1994; Robertson et al, 1996) is a probabilisticmodel for information retrieval and is one of themost popular and effective algorithms used in in-formation retrieval.
For ease of reference, we in-corporate the BM25 tf and idf factors into theSMART annotation scheme (last row of table 1and 4th row of table 2), therefore the weight wiof term i in document D according to the BM25scheme is notated as SMART.okn or okn.Most of the tf weighting functions in SMARTand the BM25 model take into consideration thenon-linearity of document relevance to term fre-1Typically, a weighting function in the SMART system isdefined as a pair of triples, i.e.
ddd.qqq where the first triplecorresponds to the document representation and the secondto the query representation.
In the context that the SMARTannotation is used here, we will use the prefix SMART forthe first part and a triple for the document representation inthe second part, i.e.
SMART.ddd, or more simply ddd.1389quency and thus employ tf factors that scale sub-linearly in relation to term frequency.
Addition-ally, the BM25 tf variant also incorporates a scal-ing for the length of the document, taking into con-sideration that longer documents will by definitionhave more term occurences2 .
Effective weightingfunctions is a very active research area in infor-mation retrieval and it is outside the scope of thispaper to provide an in-depth analysis but signifi-cant research can be found in Salton and McGill(1986), Robertson et al (2004), Manning et al(2008) or Armstrong et al (2009) for a more re-cent study.3.4 Introducing SMART and BM25 Deltatf.idf variantsWe apply the idea of localizing the estimationof idf values to documents of one class but em-ploy more sophisticated term weighting functionsadapted from the SMART retrieval system andthe BM25 probabilistic model.
The resulting idfweighting functions are presented in the lower partof table 2.
We extend the original SMART anno-tation scheme by adding Delta (?)
variants of theoriginal idf functions and additionally introducesmoothed Delta variants of the idf and the probidf factors for completeness and comparative rea-sons, noted by their accented counterparts.
Forexample, the weight of term i in document D ac-cording to the o?
(k)n weighting scheme wherewe employ the BM25 tf weighting function andutilize the difference of class-based BM25 idf val-ues would be calculated as:wi =(k1 + 1) ?
tfiK + tfi?
log(N1 ?
dfi,1 + 0.5dfi,1 + 0.5)?
(k1 + 1) ?
tfiK + tfi?
log(N2 ?
dfi,2 + 0.5dfi,2 + 0.5)= (k1 + 1) ?
tfiK + tfi?
log((N1 ?
dfi,1 + 0.5) ?
(dfi,2 + 0.5)(N2 ?
dfi,2 + 0.5) ?
(dfi,1 + 0.5))where K is defined as k1((1 ?
b) + b ?
dlavg dl).However, we used a minor variation of the aboveformulation for all the final accented weightingfunctions in which the smoothing factor is addedto the product of dfi with Ni (or its variation for?(p?)
and ?
(k)), rather than to the dfi alone as the2We deliberately didn?t extract the normalization compo-nent from the BM25 tf variant, as that would unnecessarilycomplicate the notation.above formulation would imply (see table 2).
Theabove variation was made for two reasons: firstly,when the dfi?s are larger than 1 then the smooth-ing factor influences the final idf value only in aminor way in the revised formulation, since it isadded only after the multiplication of the dfi withNi (or its variation).
Secondly, when dfi = 0, thenthe smoothing factor correctly adds only a smallmass, avoiding a potential division by zero, whereotherwise it would add a much greater mass, be-cause it would be multiplied by Ni.According to this annotation scheme therefore,the original approach by Martineau and Finin(2009) can be represented as n?
(t)n.We hypothesize that the utilization of sophisti-cated term weighting functions that have provedeffective in information retrieval, thus providingan indication that they appropriately model thedistinctive power of terms to documents and thesmoothed, localized estimation of idf values willprove beneficial in sentiment classification.Table 4: Reported accuracies on the Movie Re-view data set.
Only the best reported accuracy foreach approach is presented, measured by 10-foldcross validation.
The list is not exhaustive and be-cause of differences in training/testing data splitsthe results are not directly comparable.
It is pro-duced here only for reference.Approach Acc.SVM with unigrams & binaryweights (Pang et al, 2002), reportedat (Pang and Lee, 2004)87.15%Hybrid SVM with Turney/OsgoodLemmas (Mullen and Collier, 2004)86%SVM with min-cuts (Pang and Lee,2004)87.2%SVM with appraisal groups 90.2%(Whitelaw et al, 2005)SVM with log likehood ratio featureselection (Aue and Gamon, 2005)90.45%SVM with annotator rationales 92.2%(Zaidan et al, 2007)LDA with filtered lexicon, subjectiv-ity detection (Lin and He, 2009)84.6%The approach is straightforward, intuitive, com-putationally efficient, doesn?t require additionalhuman effort and takes into consideration stan-dardized and tested notions from IR.
The re-sults presented in section 5 show that a number1390of weighting functions solidly outperform otherstate-of-the-art approaches.
In the next section, wepresent the corpora that were used to study the ef-fectiveness of different weighting schemes.4 Experimental setupWe have experimented with a number of publiclyavailable data sets.The movie review dataset by Pang et al (2002)has been used extensively in the past by a numberof researchers (see Table 4), presenting the oppor-tunity to compare the produced results with pre-vious approaches.
The dataset comprises 2,000movie reviews, equally divided between positiveand negative, extracted from the Internet MovieDatabase3 archive of the rec.arts.movies.reviewsnewsgroup.
In order to avoid reviewer bias, only20 reviews per author were kept, resulting in a to-tal of 312 reviewers4.
The best attained accuraciesby previous research on the specific data are pre-sented in table 4.
We do not claim that those re-sults are directly comparable to ours, because ofpotential subtle differences in tokenization, classi-fier implementations etc, but we present them herefor reference.The Multi-Domain Sentiment data set (MDSD)by Blitzer et al (2007) contains Amazon reviewsfor four different product types: books, electron-ics, DVDs and kitchen appliances.
Reviews withratings of 3 or higher, on a 5-scale system, werelabeled as positive and reviews with a rating lessthan 3 as negative.
The data set contains 1,000positive and 1,000 negative reviews for each prod-uct category for a total of 8,000 reviews.
Typically,the data set is used for domain adaptation applica-tions but in our setting we only split the reviewsbetween positive and negative5.Lastly, we present results from the BLOGS06(Macdonald and Ounis, 2006) collection that iscomprised of an uncompressed 148GB crawl ofapproximately 100,000 blogs and their respectiveRSS feeds.
The collection has been used for 3 con-secutive years by the Text REtrieval Conferences(TREC)6.
Participants of the conference are pro-vided with the task of finding documents (i.e.
webpages) expressing an opinion about specific enti-3http://www.imdb.com4The dataset can be found at: http://www.cs.cornell.edu/People/pabo/movie-review-data/review polarity.tar.gz.5The data set can be found at http://www.cs.jhu.edu/mdredze/datasets/sentiment/6http://www.trec.nist.govties X, which may be people, companies, filmsetc.
The results are given to human assessors whothen judge the content of the webpages (i.e.
blogpost and comments) and assign each webpage ascore: ?1?
if the document contains relevant, fac-tual information about the entity but no expressionof opinion, ?2?
if the document contains an ex-plicit negative opinion towards the entity and ?4?is the document contains an explicit positive opin-ion towards the entity.
We used the produced as-sessments from all 3 years of the conference in ourdata set, resulting in 150 different entity searchesand, after duplicate removal, 7,930 negative docu-ments (i.e.
having an assessment of ?2?)
and 9,968positive documents (i.e.
having an assessment of?4?
), which were used as the ?gold standard?
7.Documents are annotated at the document-level,rather than at the post level, making this data setsomewhat noisy.
Additionally, the data set is par-ticularly large compared to the other ones, makingclassification especially challenging and interest-ing.
More information about all data sets can befound at table 5.We have kept the pre-processing of the docu-ments to a minimum.
Thus, we have lower-casedall words and removed all punctuation but we havenot removed stop words or applied stemming.
Wehave also refrained from removing words withlow or high occurrence.
Additionally, for theBLOGS06 data set, we have removed all html for-matting.We utilize the implementation of a support vec-tor classifier from the LIBLINEAR library (Fan etal., 2008).
We use a linear kernel and defaultparameters.
All results are based on leave-oneout cross validation accuracy.
The reason for thischoice of cross-validation setting, instead of themost standard ten-fold, is that all of the proposedapproaches that use some form of idf utilize thetraining documents for extracting document fre-quency statistics, therefore more information isavailable to them in this experimental setting.Because of the high number of possible combi-nations between tf and idf variants (6?9?2 = 108)and due to space constraints we only present re-sults from a subset of the most representative com-binations.
Generally, we?ll use the cosine nor-malized variants of unsmoothed delta weightingschemes, since they perform better than their un-7More information about the data set, as well as in-formation on how it can be obtained can be found at:http://ir.dcs.gla.ac.uk/test collections/blogs06info.html1391Table 5: Statistics about the data sets used.Data set #Documents #Terms #UniqueTermsAverage #Termsper DocumentMovie Reviews 2,000 1,336,883 39,399 668Multi-Domain SentimentDataset (MDSD)8,000 1,741,085 455,943 217BLOGS06 17,898 51,252,850 367,899 2,832Figure 1: Reported accuracy on the Movie Review data set.normalized counterparts.
We?ll avoid using nor-malization for the smoothed versions, in order tofocus our attention on the results of smoothing,rather than normalization.5 ResultsResults for the Movie Reviews, Multi-DomainSentiment Dataset and BLOGS06 corpora are re-ported in figures 1, 2 and 3 respectively.On the Movie Review data set, the results re-confirm that using binary features (bnc) is bet-ter than raw term frequency (nnc) (83.40%) fea-tures.
For reference, in this setting the unnor-malized vector using the raw tf approach (nnn)performs similar to the normalized (nnc) (83.40%vs.
83.60%), the former not present in the graph.Nonetheless, using any scaled tf weighting func-tion (anc or onc) performs as well as the binaryapproach (87.90% and 87.50% respectively).
Ofinterest is the fact that although the BM25 tf algo-rithm has proved much more successful in IR, thesame doesn?t apply in this setting and its accuracyis similar to the simpler augmented tf approach.Incorporating un-localized variants of idf (mid-dle graph section) produces only small increasesin accuracy.
Smoothing also doesn?t provide anyparticular advantage, e.g.
btc (88.20%) vs. bt?c(88.45%), since no zero idf values are present.Again, using more sophisticated tf functions pro-vides an advantage over raw tf , e.g.
nt?c at-tains an accuracy of 86.6% in comparison to at?c?s88.25%, although the simpler at?c is again as ef-fective than the BM25 tf (ot?c), which performs at88%.
The actual idf weighting function is of someimportance, e.g.
ot?c (88%) vs. okc (87.65%) andakc (88%) vs. at?c (88.25%), with simpler idf fac-tors performing similarly, although slightly betterthan BM25.Introducing smoothed, localized variants of idfand scaled or binary tf weighting schemes pro-duces significant advantages.
In this setting,smoothing plays a role, e.g.
n?
(t)c8 (91.60%)vs.
n?(t?
)n (95.80%) and a?
(p)c (92.80%)vs.
a?(p?
)n (96.55%), since we can expect zeroclass-based estimations of idf values, supportingour initial hypothesis on its importance.
Addition-ally, using augmented, BM25 or binary tf weightsis always better than raw term frequency, pro-viding further support on the advantages of us-ing sublinear tf weighting functions9 .
In this set-ting, the best accuracy of 96.90% is attained usingBM25 tf weights with the BM25 delta idf variant,although binary or augmented tf weights using8The original Delta tf.idf by Martineau and Finin (2009)has a limitation of utilizing features with df > 2.
In ourexperiments it performed similarly to n?
(t)n (90.60%) butstill lower than the cosine normalized variant n?
(t)c in-cluded in the graph (91.60%).9Although not present in the graph, for completeness rea-sons it should be noted that l?
(s)n and L?
(s)n also per-form very well, both reaching accuracies of approx.
96%.1392Figure 2: Reported accuracy on the Multi-Domain Sentiment data set.delta idf perform similarly (96.50% and 96.60%respectively).
The results indicate that the tf andthe idf factor themselves aren?t of significant im-portance, as long as the former are scaled and thelatter smoothed in some manner.
For example,a?(p?
)n vs.
a?(t?
)n perform quite similarly.The results from the Multi-Domain Sentimentdata set (figure 2) largely agree with the find-ings on the Movie Review data set, providing astrong indication that the approach isn?t limitedto a specific domain.
Binary weights outperformraw term frequency weights and perform similarlywith scaled tf ?s.
Non-localized variants of idfweights do provide a small advantage in this dataset alhough the actual idf variant isn?t important,e.g.
btc, bt?c, and okc all perform similarly.
Theutilized tf variant also isn?t important, e.g.
at?c(88.39%) vs. bt?c (88.25%).We focus our attention on the delta idf vari-ants which provide the more interesting results.The importance of smoothing becomes apparentwhen comparing the accuracy of a?
(p)c and itssmoothed variant a?(p?
)n (92.56% vs. 95.6%).Apart from that, all smoothed delta idf variantsperform very well in this data set, including some-what surprisingly, n?(t?
)n which uses raw tf(94.54%).
Considering that the average tf perdocument is approx.
1.9 in the Movie Reviewdata set and 1.1 in the MDSD, the results can beattributed to the fact that words tend to typicallyappear only once per document in the latter, there-fore minimizing the difference of the weights at-tributed by different tf functions10 .
The best at-tained accuracy is 96.40% but as the MDSD hasmainly been used for domain adaptation applica-tions, there is no clear baseline to compare it with.10For reference, the average tf per document in theBLOGS06 data set is 2.4.Lastly, we present results on the BLOGS06dataset in figure 3.
As previously noted, this dataset is particularly noisy, because it has been an-notated at the document-level rather than the post-level and as a result, the differences aren?t as pro-found as in the previous corpora, although theydo follow the same patterns.
Focusing on thedelta idf variants, the importance of smoothingbecomes apparent, e.g.
a?
(p)c vs.
a?(p?
)n andn?
(t)c vs.
n?(t?)n.
Additionally, because of thefact that documents tend to be more verbose inthis data set, the scaled tf variants also performbetter than the simple raw tf ones, n?(t?
)n vs.a?(t?)n.
Lastly, as previously, the smoothed lo-calized idf variants perform better than their un-smoothed counterparts, e.g.
n?
(t)n vs.
n?(t?
)nand a?
(p)c vs.
a?(p?
)n.6 ConclusionsIn this paper, we presented a study of documentrepresentations for sentiment analysis using termweighting functions adopted from information re-trieval and adapted to classification.
The pro-posed weighting schemes were tested on a num-ber of publicly available datasets and a numberof them repeatedly demonstrated significant in-creases in accuracy compared to other state-of-the-art approaches.
We demonstrated that for accurateclassification it is important to use term weight-ing functions that scale sublinearly in relation tothe number of times a term occurs in a documentand that document frequency smoothing is a sig-nificant factor.In the future we plan to test the proposedweighting functions in other domains such as topicclassification and additionally extend the approachto accommodate multi-class classification.1393Figure 3: Reported accuracy on the BLOGS06 data set.AcknowledgmentsThis work was supported by a European Uniongrant by the 7th Framework Programme, Theme3: Science of complex systems for socially intelli-gent ICT.
It is part of the CyberEmotions Project(Contract 231323).ReferencesAhmed Abbasi, Hsinchun Chen, and Arab Salem.2008.
Sentiment analysis in multiple languages:Feature selection for opinion classification in webforums.
ACM Trans.
Inf.
Syst., 26(3):1?34.Timothy G. Armstrong, Alistair Moffat, William Web-ber, and Justin Zobel.
2009.
Improvements thatdon?t add up: ad-hoc retrieval results since 1998.In David Wai Lok Cheung, Il Y.
Song, Wesley W.Chu, Xiaohua Hu, Jimmy J. Lin, David Wai LokCheung, Il Y.
Song, Wesley W. Chu, Xiaohua Hu,and Jimmy J. Lin, editors, CIKM, pages 601?610,New York, NY, USA.
ACM.Anthony Aue and Michael Gamon.
2005.
Customiz-ing sentiment classifiers to new domains: A casestudy.
In Proceedings of Recent Advances in Nat-ural Language Processing (RANLP).John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 440?447, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Ann Devitt and Khurshid Ahmad.
2007.
Sentimentpolarity identification in financial news: A cohesion-based approach.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Lin-guistics, pages 984?991, Prague, Czech Republic,June.
Association for Computational Linguistics.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Stephan Greene and Philip Resnik.
2009.
More thanwords: Syntactic packaging and implicit sentiment.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 503?511, Boulder, Colorado, June.Association for Computational Linguistics.K.
Sparck Jones, S. Walker, and S. E. Robertson.
2000.A probabilistic model of information retrieval: de-velopment and comparative experiments.
Inf.
Pro-cess.
Manage., 36(6):779?808.Chenghua Lin and Yulan He.
2009.
Joint senti-ment/topic model for sentiment analysis.
In CIKM?09: Proceeding of the 18th ACM conference on In-formation and knowledge management, pages 375?384, New York, NY, USA.
ACM.Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, andAlexander Hauptmann.
2006.
Which side are youon?
identifying perspectives at the document andsentence levels.
In Proceedings of the Conferenceon Natural Language Learning (CoNLL).Hugo Liu.
2004.
MontyLingua: An end-to-end naturallanguage processor with common sense.
Technicalreport, MIT.C.
Macdonald and I. Ounis.
2006.
The trec blogs06collection : Creating and analysing a blog test col-lection.
DCS Technical Report Series.Christopher D. Manning, Prabhakar Raghavan, andHinrich Schu?tze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press, 1 edition,July.J.
R. Martin and P. R. R. White.
2005.
The language ofevaluation : appraisal in English / J.R. Martin andP.R.R.
White.
Palgrave Macmillan, Basingstoke :.Justin Martineau and Tim Finin.
2009.
Delta TFIDF:An Improved Feature Space for Sentiment Analysis.In Proceedings of the Third AAAI Internatonal Con-ference on Weblogs and Social Media, San Jose, CA,May.
AAAI Press.
(poster paper).A.
Mccallum and K. Nigam.
1998.
A comparison ofevent models for naive bayes text classification.1394G.
Mishne.
2005.
Experiments with mood classifi-cation in blog posts.
In 1st Workshop on StylisticAnalysis Of Text For Information Access.Tony Mullen and Nigel Collier.
2004.
Sentiment anal-ysis using support vector machines with diverse in-formation sources.
In Dekang Lin and Dekai Wu,editors, Proceedings of EMNLP 2004, pages 412?418, Barcelona, Spain, July.
Association for Com-putational Linguistics.Charles E. Osgood.
1967.
The measurement of mean-ing / [by] [Charles E. Osgood, George J. Suci [and]Percy H. Tannenbaum].
University of Illinois Press,Urbana :, 2nd ed.
edition.Iadh Ounis, Craig Macdonald, and Ian Soboroff.
2008.Overview of the trec-2008 blog trac.
In The Seven-teenth Text REtrieval Conference (TREC 2008) Pro-ceedings.
NIST.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In In Proceedingsof the ACL, pages 271?278.B.
Pang and L. Lee.
2008.
Opinion Mining and Senti-ment Analysis.
Now Publishers Inc.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification usingmachine learning techniques.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).Rudy Prabowo and Mike Thelwall.
2009.
Sentimentanalysis: A combined approach.
Journal of Infor-metrics, 3(2):143?157, April.Stephen E. Robertson, Steve Walker, Susan Jones,Micheline Hancock-Beaulieu, and Mike Gatford.1994.
Okapi at trec-3.
In TREC, pages 0?.S E Robertson, S Walker, S Jones, M M Hancock-Beaulieu, and M Gatford.
1996.
Okapi at trec-2.In In The Second Text REtrieval Conference (TREC-2), NIST Special Special Publication 500-215, pages21?34.Stephen Robertson, Hugo Zaragoza, and Michael Tay-lor.
2004.
Simple bm25 extension to multipleweighted fields.
In CIKM ?04: Proceedings of thethirteenth ACM international conference on Infor-mation and knowledge management, pages 42?49,New York, NY, USA.
ACM.Gerard Salton and Chris Buckley.
1987.
Term weight-ing approaches in automatic text retrieval.
Technicalreport, Ithaca, NY, USA.Gerard Salton and Michael J. McGill.
1986.
Intro-duction to Modern Information Retrieval.
McGraw-Hill, Inc., New York, NY, USA.G.
Salton.
1971.
The SMART Retrieval System?Experiments in Automatic Document Processing.Prentice-Hall, Inc., Upper Saddle River, NJ, USA.Fabrizio Sebastiani.
2002.
Machine learning in au-tomated text categorization.
ACM Computing Sur-veys, 34(1):1n?47.Amit Singhal, Gerard Salton, and Chris Buckley.
1995.Length normalization in degraded text collections.Technical report, Ithaca, NY, USA.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Getout the vote: Determining support or oppositionfrom congressional floor-debate transcripts.
CoRR,abs/cs/0607062.Peter D. Turney.
2002.
Thumbs up or thumbs down?semantic orientation applied to unsupervised classi-fication of reviews.
In ACL, pages 417?424.Casey Whitelaw, Navendu Garg, and Shlomo Arga-mon.
2005.
Using appraisal groups for sentimentanalysis.
In CIKM ?05: Proceedings of the 14thACM international conference on Information andknowledge management, pages 625?631, New York,NY, USA.
ACM.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of HumanLanguage Technologies Conference/Conference onEmpirical Methods in Natural Language Processing(HLT/EMNLP 2005), Vancouver, CA.Ian H. Witten and Eibe Frank.
1999.
Data Mining:Practical Machine Learning Tools and Techniqueswith Java Implementations (The Morgan KaufmannSeries in Data Management Systems).
MorganKaufmann, 1st edition, October.Alex Wright.
2009.
Mining the web for feelings, notfacts.
August 23, NY Times, last accessed October2, 2009, http://http://www.nytimes.com/2009/08/24/technology/internet/ 24emotion.html?
r=1.O.F.
Zaidan, J. Eisner, and C.D.
Piatko.
2007.
UsingAnnotator Rationales to Improve Machine Learn-ing for Text Categorization.
Proceedings of NAACLHLT, pages 260?267.Justin Zobel and Alistair Moffat.
1998.
Exploring thesimilarity space.
SIGIR Forum, 32(1):18?34.1395
