Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 245?253,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUsing a Dependency Parser to Improve SMT for Subject-Object-VerbLanguagesPeng Xu, Jaeho Kang, Michael Ringgaard and Franz OchGoogle Inc.1600 Amphitheatre ParkwayMountain View, CA 94043, USA{xp,jhkang,ringgaard,och}@google.comAbstractWe introduce a novel precedence reorderingapproach based on a dependency parser to sta-tistical machine translation systems.
Similarto other preprocessing reordering approaches,our method can efficiently incorporate linguis-tic knowledge into SMT systems without in-creasing the complexity of decoding.
For a setof five subject-object-verb (SOV) order lan-guages, we show significant improvements inBLEU scores when translating from English,compared to other reordering approaches, instate-of-the-art phrase-based SMT systems.1 IntroductionOver the past ten years, statistical machine transla-tion has seen many exciting developments.
Phrase-based systems (Och, 2002; Koehn et.al., 2003;Och and Ney, 2004) advanced the machine transla-tion field by allowing translations of word sequences(a.k.a., phrases) instead of single words.
This ap-proach has since been the state-of-the-art because ofits robustness in modeling local word reordering andthe existence of an efficient dynamic programmingdecoding algorithm.However, when phrase-based systems are usedbetween languages with very different word or-ders, such as between subject-verb-object (SVO)and subject-object-verb (SOV) languages, long dis-tance reordering becomes one of the key weak-nesses.
Many reordering methods have been pro-posed in recent years to address this problem in dif-ferent aspects.The first class of approaches tries to explicitlymodel phrase reordering distances.
Distance baseddistortion model (Och, 2002; Koehn et.al., 2003) isa simple way of modeling phrase level reordering.It penalizes non-monotonicity by applying a weightto the number of words between two source phrasescorresponding to two consecutive target phrases.Later on, this model was extended to lexicalizedphrase reordering (Tillmann, 2004; Koehn, et.al.,2005; Al-Onaizan and Papineni, 2006) by applyingdifferent weights to different phrases.
Most recently,a hierarchical phrase reordering model (Galley andManning, 2008) was proposed to dynamically deter-mine phrase boundaries using efficient shift-reduceparsing.
Along this line of research, discrimina-tive reordering models based on a maximum entropyclassifier (Zens and Ney, 2006; Xiong, et.al., 2006)also showed improvements over the distance baseddistortion model.
None of these reordering modelschanges the word alignment step in SMT systems,therefore, they can not recover from the word align-ment errors.
These models are also limited by amaximum allowed reordering distance often used indecoding.The second class of approaches puts syntacticanalysis of the target language into both modelingand decoding.
It has been shown that direct model-ing of target language constituents movement in ei-ther constituency trees (Yamada and Knight, 2001;Galley et.al., 2006; Zollmann et.al., 2008) or depen-dency trees (Quirk, et.al., 2005) can result in signifi-cant improvements in translation quality for translat-ing languages like Chinese and Arabic into English.A simpler alternative, the hierarchical phrase-based245approach (Chiang, 2005; Wu, 1997) also showedpromising results for translating Chinese to English.Similar to the distance based reordering models, thesyntactical or hierarchical approaches also rely onother models to get word alignments.
These mod-els typically combine machine translation decodingwith chart parsing, therefore significantly increasethe decoding complexity.
Even though some re-cent work has shown great improvements in decod-ing efficiency for syntactical and hierarchical ap-proaches (Huang and Chiang, 2007), they are stillnot as efficient as phrase-based systems, especiallywhen higher order language models are used.Finally, researchers have also tried to put sourcelanguage syntax into reordering in machine trans-lation.
Syntactical analysis of source languagecan be used to deterministically reorder input sen-tences (Xia and McCord, 2004; Collins et.al., 2005;Wang et.al., 2007; Habash, 2007), or to provide mul-tiple orderings as weighted options (Zhang et.al.,2007; Li et.al., 2007; Elming, 2008).
In theseapproaches, input source sentences are reorderedbased on syntactic analysis and some reorderingrules at preprocessing step.
The reordering rulescan be either manually written or automatically ex-tracted from data.
Deterministic reordering based onsyntactic analysis for the input sentences providesa good way of resolving long distance reordering,without introducing complexity to the decoding pro-cess.
Therefore, it can be efficiently incorporatedinto phrase-based systems.
Furthermore, when thesame preprocessing reordering is performed for thetraining data, we can still apply other reordering ap-proaches, such as distance based reordering and hi-erarchical phrase reordering, to capture additionallocal reordering phenomena that are not captured bythe preprocessing reordering.
The work presented inthis paper is largely motivated by the preprocessingreordering approaches.In the rest of the paper, we first introduce our de-pendency parser based reordering approach based onthe analysis of the key issues when translating SVOlanguages to SOV languages.
Then, we show exper-imental results of applying this approach to phrase-based SMT systems for translating from English tofive SOV languages (Korean, Japanese, Hindi, Urduand Turkish).
After showing that this approach canalso be beneficial for hierarchical phrase-based sys-John can hit ballthe??
?
???
????
?..Figure 1: Example Alignment Between an English and aKorean Sentencetems, we will conclude the paper with future re-search directions.2 Translation between SVO and SOVLanguagesIn linguistics, it is possible to define a basic wordorder in terms of the verb (V) and its arguments,subject (S) and object (O).
Among all six possiblepermutations, SVO and SOV are the most common.Therefore, translating between SVO and SOV lan-guages is a very important area to study.
We useEnglish as a representative of SVO languages andKorean as a representative for SOV languages in ourdiscussion about the word orders.Figure 1 gives an example sentence in English andits corresponding translation in Korean, along withthe alignments between the words.
Assume that wesplit the sentences into four phrases: (John , t@),(can hit , `  ????
), (the ball , ?
?D)and (.
, .).
Since a phrase-based decoder generatesthe translation from left to right, the following stepsneed to happen when we translate from English toKorean:?
Starts from the beginning of the sentence,translates ?John?
to ?t@?;?
Jumps to the right by two words, translates ?theball?
to ???D?;?
Jumps to the left by four words, translates ?canhit?
to ?`?????;?
Finally, jumps to the right by two words, trans-lates ?.?
to ?.
?.It is clear that in order for the phrase-based decoderto successfully carry out all of the reordering steps, avery strong reordering model is required.
When thesentence gets longer with more complex structure,the number of words to move over during decod-ing can be quite high.
Imagine when we translate246Figure 2: Dependency Parse Tree of an Example EnglishSentencethe sentence ?English is used as the first or secondlanguage in many countries around the world .
?.The decoder needs to make a jump of 13 words inorder to put the translation of ?is used?
at the endof the translation.
Normally in a phrase-based de-coder, very long distance reordering is not allowedbecause of efficiency considerations.
Therefore, itis very difficult in general to translate English intoKorean with proper word order.However, knowing the dependency parse trees ofthe English sentences may simplify the reorderingproblem significantly.
In the simple example in Fig-ure 1, if we analyze the English sentence and knowthat ?John?
is the subject, ?can hit?
is the verb and?the ball?
is the object, we can reorder the Englishinto SOV order.
The resulting sentence ?John theball can hit .?
will only need monotonic translation.This motivates us to use a dependency parser for En-glish to perform the reordering.3 Precedence Reordering Based on aDependency ParserFigure 2 shows the dependency tree for the examplesentence in the previous section.
In this parse, theverb ?hit?
has four children: a subject noun ?John?,an auxiliary verb ?can?, an object noun ?ball?
and apunctuation ?.?.
When transforming the sentence toSOV order, we need to move the object noun and thesubtree rooted at it to the front of the head verb, butafter the subject noun.
We can have a simple rule toachieve this.However, in reality, there are many possible chil-dren for a verb.
These children have some relativeordering that is typically fixed for SOV languages.In order to describe this kind of ordering, we pro-pose precedence reordering rules based on a depen-dency parse tree.
All rules here are based Englishand Korean examples, but they also apply to otherSOV languages, as we will show later empirically.A precedence reordering rule is a mapping fromT to a set of tuples {(L,W,O)}, where T is thepart-of-speech (POS) tag of the head in a depen-dency parse tree node, L is a dependency label fora child node, W is a weight indicating the order ofthat child node and O is the type of order (eitherNORMAL or REVERSE).
The type of order is onlyused when we have multiple children with the sameweight, while the weight is used to determine therelative order of the children, going from largest tosmallest.
The weight can be any real valued num-ber.
The order type NORMAL means we preservethe original order of the children, while REVERSEmeans we flip the order.
We reserve a special labelself to refer to the head node itself so that we canapply a weight to the head, too.
We will call thistuple a precedence tuple in later discussions.
In thisstudy, we use manually created rules only.Suppose we have a precedence rule: VB ?
(nsubj, 2, NORMAL), (dobj, 1, NORMAL), (self,0, NORMAL).
For the example shown in Figure 2,we would apply it to the ROOT node and result in?John the ball can hit .
?.Given a set of rules, we apply them in a depen-dency tree recursively starting from the root node.
Ifthe POS tag of a node matches the left-hand-side ofa rule, the rule is applied and the order of the sen-tence is changed.
We go through all children of thenode and get the precedence weights for them fromthe set of precedence tuples.
If we encounter a childnode that has a dependency label not listed in the setof tuples, we give it a default weight of 0 and de-fault order type of NORMAL.
The children nodesare sorted according to their weights from highest tolowest, and nodes with the same weights are orderedaccording to the type of order defined in the rule.3.1 Verb Precedence RulesVerb movement is the most important movementwhen translating from English (SVO) to Korean(SOV).
In a dependency parse tree, a verb node canpotentially have many children.
For example, aux-iliary and passive auxiliary verbs are often groupedtogether with the main verb and moved together withit.
The order, however, is reversed after the move-ment.
In the example of Figure 2, the correct Korean247???
???
????????
?.Figure 3: Dependency Parse Tree with Alignment for aSentence with Preposition Modifierword order is ?` (hit)  ????
(can) .
Othercategories that are in the same group are phrasal verbparticle and negation.If the verb in an English sentence has a preposi-tional phrase as a child, the prepositional phrase isoften placed before the direct object in the Koreancounterpart.
As shown in Figure 3, ?
)?t \?
(?with a bat?)
is actually between ?t@?
(?John?
)and ???D?
(?the ball?
).Another common reordering phenomenon iswhen a verb has an adverbial clause modifier.
In thatcase, the whole adverbial clause is moved together tobe in front of the subject of the main sentence.
Insidethe adverbial clause, the ordering follows the sameverb reordering rules, so we recursively reorder theclause.Our verb precedence rule, as in Table 1, can coverall of the above reordering phenomena.
One wayto interpret this rule set is as follows: for any nodewhose POS tag is matches VB* (VB, VBZ, VBD,VBP, VBN, VBG), we group the children node thatare phrasal verb particle (prt), auxiliary verb (aux),passive auxiliary verb (auxpass), negation (neg) andthe verb itself (self) together and reverse them.
Thisverb group is moved to the end of the sentence.
Wemove adverbial clause modifier to the beginning ofthe sentence, followed by a group of noun subject(nsubj), preposition modifier and anything else notlisted in the table, in their original order.
Right be-fore the verb group, we put the direct object (dobj).Note that all of the children are optional.3.2 Adjective Precedence RulesSimilar to the verbs, adjectives can also take an aux-iliary verb, a passive auxiliary verb and a negationT (L, W, O)VB*(advcl, 1, NORMAL)(nsubj, 0, NORMAL)(prep, 0, NORMAL)(dobj, -1, NORMAL)(prt, -2, REVERSE)(aux, -2, REVERSE)(auxpass, -2, REVERSE)(neg, -2, REVERSE)(self, -2, REVERSE)JJ or JJS or JJR(advcl, 1, NORMAL)(self, -1, NORMAL)(aux, -2, REVERSE)(auxpass, -2, REVERSE)(neg, -2, REVERSE)(cop, -2, REVERSE)NN or NNS(prep, 2, NORMAL)(rcmod, 1, NORMAL)(self, 0, NORMAL)IN or TO (pobj, 1, NORMAL)(self, -1, NORMAL)Table 1: Precedence Rules to Reorder English to SOVLanguage Order (These rules were extracted manually bya bilingual speaker after looking at some text book exam-ples in English and Korean, and the dependency parsetrees of the English examples.
)as modifiers.
In such cases, the change in order fromEnglish to Korean is similar to the verb rule, exceptthat the head adjective itself should be in front of theverbs.
Therefore, in our adjective precedence rule inthe second panel of Table 1, we group the auxiliaryverb, the passive auxiliary verb and the negation andmove them together after reversing their order.
Theyare moved to right after the head adjective, which isput after any other modifiers.For both verb and adjective precedence rules,we also apply some heuristics to prevent exces-sive movements.
In order to do this, we disallowany movement across punctuation and conjunctions.Therefore, for sentences like ?John hit the ball butSam threw the ball?, the reordering result would be?John the ball hit but Sam the ball threw?, insteadof ?John the ball but Sam the ball threw hit?.3.3 Noun and Preposition Precedence RulesIn Korean, when a noun is modified by a preposi-tional phrase, such as in ?the way to happiness?,the prepositional phrase is usually moved in front ofthe noun, resulting in ???
(happiness)<\ ?8 (to the way)?
.
Similarly for relative clause mod-ifier, it is also reordered to the front of the head noun.For preposition head node with an object modifier,248the order is the object first and the preposition last.One example is ?with a bat?
in Figure 3.
It corre-sponds to ?
)?t (a bat) \(with)?.
We handlethese types of reordering by the noun and preposi-tion precedence rules in the third and fourth panel ofTable 1.With the rules defined in Table 1, we now show amore complex example in Figure 4.
First, the ROOTnode matches an adjective rule, with four childrennodes labeled as (csubj, cop, advcl, p), and withprecedence weights of (0, -2, 1, 0).
The ROOT nodeitself has a weight of -1.
After reordering, the sen-tence becomes: ?because we do n?t know what thefuture has Living exciting is .?.
Note that the wholeadverbial phrase rooted at ?know?
is moved to thebeginning of the sentence.
After that, we see thatthe child node rooted at ?know?
matches a verb rule,with five children nodes labeled as (mark, nsubj,aux, neg, ccomp), with weights (0, 0, -2, -2, 0).
Inthis case, the verb itself also has weight -2.
Nowwe have two groups of nodes, with weight 0 and -2,respectively.
The first group has a NORMAL orderand the second group has a REVERSE order.
Af-ter reordering, the sentence becomes: ?because wewhat the future has know n?t do Living excitingis .?.
Finally, we have another node rooted at ?has?that matches the verb rule again.
After the final re-ordering, we end up with the sentence: ?because wethe future what has know n?t do Living excitingis .?.
We can see in Figure 4 that this sentence has analmost monotonic alignment with a reasonable Ko-rean translation shown in the figure1.4 Related WorkAs we mentioned in our introduction, there havebeen several studies in applying source sentence re-ordering using syntactical analysis for statistical ma-chine translation.
Our precedence reordering ap-proach based on a dependency parser is motivated bythose previous works, but we also distinguish fromtheir studies in various ways.Several approaches use syntactical analysis toprovide multiple source sentence reordering optionsthrough word lattices (Zhang et.al., 2007; Li et.al.,2007; Elming, 2008).
A key difference between1We could have improved the rules by using a weight of -3for the label ?mark?, but it was not in our original set of rules.their approaches and ours is that they do not performreordering during training.
Therefore, they wouldneed to rely on reorder units that are likely not vio-lating ?phrase?
boundaries.
However, since we re-order both training and test data, our system oper-ates in a matched condition.
They also focus on ei-ther Chinese to English (Zhang et.al., 2007; Li et.al.,2007) or English to Danish (Elming, 2008), whicharguably have less long distance reordering than be-tween English and SOV languages.Studies most similar to ours are those preprocess-ing reordering approaches (Xia and McCord, 2004;Collins et.al., 2005; Wang et.al., 2007; Habash,2007).
They all perform reordering during prepro-cessing based on either automatically extracted syn-tactic rules (Xia and McCord, 2004; Habash, 2007)or manually written rules (Collins et.al., 2005; Wanget.al., 2007).
Compared to these approaches, ourwork has a few differences.
First of all, we studya wide range of SOV languages using manually ex-tracted precedence rules, not just for one languagelike in these studies.
Second, as we will show inthe next section, we compare our approach to avery strong baseline with more advanced distancebased reordering model, not just the simplest distor-tion model.
Third, our precedence reordering rules,like those in Habash, 2007, are more flexible thanthose other rules.
Using just one verb rule, we canperform the reordering of subject, object, preposi-tion modifier, auxiliary verb, negation and the headverb.
Although we use manually written rules inthis study, it is possible to learn our rules automat-ically from alignments, similarly to Habash, 2007.However, unlike Habash, 2007, our manually writ-ten rules handle unseen children and their order nat-urally because we have a default precedence weightand order type, and we do not need to match an oftentoo specific condition, but rather just treat all chil-dren independently.
Therefore, we do not need touse any backoff scheme in order to have a broad cov-erage.
Fourth, we use dependency parse trees ratherthan constituency trees.There has been some work on syntactic word or-der model for English to Japanese machine transla-tion (Chang and Toutanova, 2007).
In this work, aglobal word order model is proposed based on fea-tures including word bigram of the target sentence,displacements and POS tags on both source and tar-249???
?????????????
?
???
?????
??
?.we the Livingwhatfuture knowhas n't do excitingbecause is .csubj cop detmarkROOT auxnsubj neg advcl nsubjdobj ccomp pLiving is thebecauseexciting dowe n't know futurewhat has.VBG VBZ DTINJJ VBPPRP RB VB NNWP VBZ.LabelTokenPOSFigure 4: A Complex Reordering Example (Reordered English sentence and alignments are at the bottom.
)get sides.
They build a log-linear model using thesefeatures and apply the model to re-rank N -best listsfrom a baseline decoder.
Although we also study thereordering problem in English to Japanese transla-tion, our approach is to incorporate the linguisticallymotivated reordering directly into modeling and de-coding.5 ExperimentsWe carried out all our experiments based on a state-of-the-art phrase-based statistical machine transla-tion system.
When training a system for Englishto any of the 5 SOV languages, the word alignmentstep includes 3 iterations of IBM Model-1 trainingand 2 iterations of HMM training.
We do not useModel-4 because it is slow and it does not add muchvalue to our systems in a pilot study.
We use thestandard phrase extraction algorithm (Koehn et.al.,2003) to get al phrases up to length 5.
In additionto the regular distance distortion model, we incor-porate a maximum entropy based lexicalized phrasereordering model (Zens and Ney, 2006) as a fea-ture used in decoding.
In this model, we use 4 re-ordering classes (+1, > 1, ?1, < ?1) and wordsfrom both source and target as features.
For sourcewords, we use the current aligned word, the wordbefore the current aligned word and the next alignedword; for target words, we use the previous twowords in the immediate history.
Using this type offeatures makes it possible to directly use the maxi-mum entropy model in the decoding process (Zensand Ney, 2006).
The maximum entropy models aretrained on all events extracted from training dataword alignments using the LBFGS algorithm (Mal-ouf, 2002).
Overall for decoding, we use between 20System Source TargetEnglish?Korean 303M 267MEnglish?Japanese 316M 350MEnglish?Hindi 16M 17MEnglish?Urdu 17M 19MEnglish?Turkish 83M 76MTable 2: Training Corpus Statistics (#words) of Systemsfor 5 SOV Languagesto 30 features, whose weights are optimized usingMERT (Och, 2003), with an implementation basedon the lattice MERT (Macherey et.al., 2008).For parallel training data, we use an in-house col-lection of parallel documents.
They come from var-ious sources with a substantial portion coming fromthe web after using simple heuristics to identify po-tential document pairs.
Therefore, for some doc-uments in the training data, we do not necessarilyhave the exact clean translations.
Table 2 shows theactual statistics about the training data for all fivelanguages we study.
For all 5 SOV languages, weuse the target side of the parallel data and some moremonolingual text from crawling the web to build 4-gram language models.We also collected about 10K English sentencesfrom the web randomly.
Among them, 9.5K are usedas evaluation data.
Those sentences were translatedby humans to all 5 SOV languages studied in thispaper.
Each sentence has only one reference trans-lation.
We split them into 3 subsets: dev contains3,500 sentences, test contains 1,000 sentences andthe rest of 5,000 sentences are used in a blindtestset.
The dev set is used to perform MERT training,while the test set is used to select trained weightsdue to some nondeterminism of MERT training.
Weuse IBM BLEU (Papineni et al, 2002) to evaluate250our translations and use character level BLEU forKorean and Japanese.5.1 Preprocessing Reordering and ReorderingModelsWe first compare our precedence rules based prepro-cessing reordering with the maximum entropy basedlexicalized reordering model.
In Table 3, Baselineis our system with both a distance distortion modeland the maximum entropy based lexicalized reorder-ing model.
For all results reported in this section,we used a maximum allowed reordering distance of10.
In order to see how the lexicalized reorderingmodel performs, we also included systems with andwithout it (-LR means without it).
PR is our pro-posed approach in this paper.
Note that since we ap-ply precedence reordering rules during preprocess-ing, we can combine this approach with any otherreordering models used during decoding.
The onlydifference is that with the precedence reordering, wewould have a different phrase table and in the caseof LR, different maximum entropy models.In order to implement the precedence rules, weneed a dependency parser.
We choose to use adeterministic inductive dependency parser (Nivreand Scholz, 2004) for its efficiency and good ac-curacy.
Our implementation of the deterministicdependency parser using maximum entropy modelsas the underlying classifiers achieves 87.8% labeledattachment score and 88.8% unlabeled attachmentscore on standard Penn Treebank evaluation.As our results in Table 3 show, for all 5 lan-guages, by using the precedence reordering rules asdescribed in Table 1, we achieve significantly bet-ter BLEU scores compared to the baseline system.In the table, We use two stars (??)
to mean thatthe statistical significance test using the bootstrapmethod (Koehn, 2004) gives an above 95% signif-icance level when compared to the baselie.
We mea-sured the statistical significance level only for theblindtest data.Note that for Korean and Japanese, our prece-dence reordering rules achieve better absoluteBLEU score improvements than for Hindi, Urdu andTurkish.
Since we only analyzed English and Ko-rean sentences, it is possible that our rules are moregeared toward Korean.
Japanese has almost exactlythe same word order as Korean, so we could assumeLanguage System dev test blindKoreanBL 25.8 27.0 26.2-LR 24.7 25.6 25.1-LR+PR 27.3 28.3 27.5**+PR 27.8 28.7 27.9**JapaneseBL 29.5 29.3 29.3-LR 29.2 29.0 29.0-LR+PR 30.3 31.0 30.6**+PR 30.7 31.2 31.1**HindiBL 19.1 18.9 18.3-LR 17.4 17.1 16.4-LR+PR 19.6 18.8 18.7**+PR 19.9 18.9 18.8**UrduBL 9.7 9.5 8.9-LR 9.1 8.6 8.2-LR+PR 10.0 9.6 9.6**+PR 10.0 9.8 9.6**TurkishBL 10.0 10.5 9.8-LR 9.1 10.0 9.0-LR+PR 10.5 11.0 10.3**+PR 10.5 10.9 10.4**Table 3: BLEU Scores on Dev, Test and Blindtest for En-glish to 5 SOV Languages with Various Reordering Op-tions (BL means baseline, LR means maximum entropybased lexialized phrase reordering model, PR meansprecedence rules based preprocessing reordering.
)the benefits can carry over to Japanese.5.2 Reordering ConstraintsOne of our motivations of using the precedence re-ordering rules is that English will look like SOV lan-guages in word order after reordering.
Therefore,even monotone decoding should be able to producebetter translations.
To see this, we carried out a con-trolled experiment, using Korean as an example.Clearly, after applying the precedence reorderingrules, our English to Korean system is not sensitiveto the maximum allowed reordering distance any-more.
As shown in Figure 5, without the rules, theblindtest BLEU scores improve monotonically asthe allowed reordering distance increases.
This indi-cates that the order difference between English andKorean is very significant.
Since smaller allowedreordering distance directly corresponds to decod-ing time, we can see that with the same decodingspeed, our proposed approach can achieve almost5% BLEU score improvements on blindtest set.5.3 Preprocessing Reordering andHierarchical ModelThe hierarchical phrase-based approach has beensuccessfully applied to several systems (Chiang,2511 2 4 6 8 10Maximum Allowed Reordering Distance0.230.240.250.260.270.28Blindtest BLEU Score No LexReorderBaselineNo LexReorder, with ParserReorderWith ParserReorderFigure 5: Blindtest BLEU Score for Different MaximumAllowed Reordering Distance for English to Korean Sys-tems with Different Reordering Options2005; Zollmann et.al., 2008).
Since hierarchicalphrase-based systems can capture long distance re-ordering by using a PSCFG model, we expect it toperform well in English to SOV language systems.We use the same training data as described in theprevious sections for building hierarchical systems.The same 4-gram language models are also used forthe 5 SOV languages.
We adopt the SAMT pack-age (Zollmann and Venugopal, 2006) and followsimilar settings as Zollmann et.al., 2008.
We alloweach rule to have at most 6 items on the source side,including nonterminals and extract rules from initialphrases of maximum length 12.
During decoding,we allow application of all rules of the grammar forchart items spanning up to 12 source words.Since our precedence reordering applies at pre-processing step, we can train a hierarchical systemafter applying the reordering rules.
When doing so,we use exactly the same settings as a regular hier-archical system.
The results for both hierarchicalsystems and those combined with the precedence re-ordering are shown in Table 4, together with the bestnormal phrase-based systems we copy from Table 3.Here again, we mark any blindtest BLEU score thatis better than the corresponding hierarchical systemwith confidence level above 95%.
Note that the hier-archical systems can not use the maximum entropybased lexicalized phrase reordering models.Except for Hindi, applying the precedence re-ordering rules in a hierarchical system can achievestatistically significant improvements over a normalhierarchical system.
We conjecture that this may bebecause of the simplicity of our reordering rules.Language System dev test blindKoreanPR 27.8 28.7 27.9Hier 27.4 27.7 27.9PR+Hier 28.5 29.1 28.8**JapanesePR 30.7 31.2 31.1**Hier 30.5 30.6 30.5PR+Hier 31.0 31.3 31.1**HindiPR 19.9 18.9 18.8Hier 20.3 20.3 19.3PR+Hier 20.0 19.7 19.3UrduPR 10.0 9.8 9.6Hier 10.4 10.3 10.0PR+Hier 11.2 10.7 10.7**TurkishPR 10.5 10.9 10.4Hier 11.0 11.8 10.5PR+Hier 11.1 11.6 10.9**Table 4: BLEU Scores on Dev, Test and Blindtest for En-glish to 5 SOV Languages in Hierarchical Phrase-basedSystems (PR is precedence rules based preprocessing re-ordering, same as in Table 3, while Hier is the hierarchi-cal system.
)Other than the reordering phenomena covered byour rules in Table 1, there could be still some local orlong distance reordering.
Therefore, using a hierar-chical phrase-based system can improve those cases.Another possible reason is that after the reorderingrules apply in preprocessing, English sentences inthe training data are very close to the SOV order.
Asa result, EM training becomes much easier and wordalignment quality becomes better.
Therefore, a hier-archical phrase-based system can extract better rulesand hence achievesbetter translation quality.We also point out that hierarchical phrase-basedsystems require a chart parsing algorithm during de-coding.
Compared to the efficient dynamic pro-gramming in phrase-based systems, it is muchslower.
This makes our approach more appealingin a realtime statistical machine translation system.6 ConclusionIn this paper, we present a novel precedence re-ordering approach based on a dependency parser.We successfully applied this approach to systemstranslating English to 5 SOV languages: Korean,Japanese, Hindi, Urdu and Turkish.
For all 5 lan-guages, we achieve statistically significant improve-ments in BLEU scores over a state-of-the-art phrase-based baseline system.
The amount of training datafor the 5 languages varies from around 17M to morethan 350M words, including some noisy data from252the web.
Our proposed approach has shown to berobust and versatile.
For 4 out of the 5 languages,our approach can even significantly improve over ahierarchical phrase-based baseline system.
As far aswe know, we are the first to show that such reorder-ing rules benefit several SOV languages.We believe our rules are flexible and can covermany linguistic reordering phenomena.
The formatof our rules also makes it possible to automaticallyextract rules from word aligned corpora.
In the fu-ture, we plan to investigate along this direction andextend the rules to languages other than SOV.The preprocessing reordering like ours is knownto be sensitive to parser errors.
Some preliminaryerror analysis already show that indeed some sen-tences suffer from parser errors.
In the recent years,several studies have tried to address this issue by us-ing a word lattice instead of one reordering as in-put (Zhang et.al., 2007; Li et.al., 2007; Elming,2008).
Although there is clearly room for improve-ments, we also feel that using one reordering duringtraining may not be good enough either.
It would bevery interesting to investigate ways to have efficientprocedure for training EM models and getting wordalignments using word lattices on the source side ofthe parallel data.
Along this line of research, wethink some kind of tree-to-string model (Liu et.al.,2006) could be interesting directions to pursue.ReferencesYaser Al-Onaizan and Kishore Papineni 2006.
Distortion Models forStatistical Machine Translation In Proceedings of ACLPi-Chuan Chang and Kristina Toutanova 2007.
A Discriminative Syn-tactic Word Order Model for Machine Translation In Proceedingsof ACLDavid Chiang 2005.
A Hierarchical Phrase-based Model for StatisticalMachine Translation In Proceedings of ACLMichael Collins, Philipp Koehn and Ivona Kucerova 2005.
ClauseRestructuring for Statistical Machine Translation In Proceedings ofACLJakob Elming 2008.
Syntactic Reordering Integrated with Phrase-based SMT In Proceedings of COLINGMichel Galley and Christopher D. Manning 2008.
A Simple and Ef-fective Hierarchical Phrase Reordering Model In Proceedings ofEMNLPMichel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, SteveDeNeefe, Wei Wang and Ignacio Thayer 2006.
Scalable Inferenceand Training of Context-Rich Syntactic Translation Models In Pro-ceedings of COLING-ACLNizar Habash 2007.
Syntactic Preprocessing for Statistical MachineTranslation In Proceedings of 11th MT SummitLiang Huang and David Chiang 2007.
Forest Rescoring: Faster De-coding with Integrated Language Models, In Proceedings of ACLPhilipp Koehn 2004.
Statistical Significance Tests for Machine Trans-lation Evaluation In Proceedings of EMNLPPhilipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, ChrisCallison-Burch, Miles Osborne and David Talbot 2005.
EdinborghSystem Description for the 2005 IWSLT Speech Translation Evalu-ation In International Workshop on Spoken Language TranslationPhilipp Koehn, Franz J. Och and Daniel Marcu 2003.
StatisticalPhrase-based Translation, In Proceedings of HLT-NAACLChi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou, Minghui Li and YiGuan 2007.
A Probabilistic Approach to Syntax-based Reorderingfor Statistical Machine Translation, In Proceedings of ACLYang Liu, Qun Liu and Shouxun Lin 2006.
Tree-to-string AlignmentTemplate for Statistical Machine Translation, In Proceedings ofCOLING-ACLWolfgang Macherey, Franz J. Och, Ignacio Thayer and Jakob Uszkoreit2008.
Lattice-based Minimum Error Rate Training for StatisticalMachine Translation In Proceedings of EMNLPRobert Malouf 2002.
A comparison of algorithms for maximum en-tropy parameter estimation In Proceedings of the Sixth Workshopon Computational Language Learning (CoNLL-2002)Joakim Nivre and Mario Scholz 2004.
Deterministic Dependency Pars-ing for English Text.
In Proceedings of COLINGFranz J. Och 2002.
Statistical Machine Translation: From Single WordModels to Alignment Template Ph.D. Thesis, RWTH Aachen, Ger-manyFranz J. Och.
2003.
Minimum Error Rate Training in Statistical Ma-chine Translation.
In Proceedings of ACLFranz J. Och and Hermann Ney 2004.
The Alignment Template Ap-proach to Statistical Machine Translation.
Computational Linguis-tics, 30:417-449Kishore Papineni, Roukos, Salim et al 2002.
BLEU: A Method forAutomatic Evaluation of Machine Translation.
In Proceedings ofACLChris Quirk, Arul Menezes and Colin Cherry 2005.
Dependency TreeTranslation: Syntactically Informed Phrasal SMT In Proceedings ofACLChristoph Tillmann 2004.
A Block Orientation Model for StatisticalMachine Translation In Proceedings of HLT-NAACLChao Wang, Michael Collins and Philipp Koehn 2007.
Chinese Syntac-tic Reordering for Statistical Machine Translation In Proceedings ofEMNLP-CoNLLDekai Wu 1997.
Stochastic Inversion Transduction Grammars andBilingual Parsing of Parallel Corpus In Computational Linguistics23(3):377-403Fei Xia and Michael McCord 2004.
Improving a Statistical MT Sys-tem with Automatically Learned Rewrite Patterns In Proceedings ofCOLINGDeyi Xiong, Qun Liu and Shouxun Lin 2006.
Maximum EntropyBased Phrase Reordering Model for Statistical Machine TranslationIn Proceedings of COLING-ACLKenji Yamada and Kevin Knight 2001.
A Syntax-based StatisticalTranslation Model In Proceedings of ACLYuqi Zhang, Richard Zens and Hermann Ney 2007.
Improve Chunk-level Reordering for Statistical Machine Translation In Proceedingsof IWSLTRichard Zens and Hermann Ney 2006.
Discriminative ReorderingModels for Statistical Machine Translation In Proceedings of theWorkshop on Statistical Machine Translation, HLT-NAACL pages55-63Andreas Zollmann and Ashish Venugopal 2006.
Syntax AugmentedMachine Translation via Chart Parsing In Proceedings of NAACL2006 - Workshop on Statistical Machine TranslationAndreas Zollmann, Ashish Venugopal, Franz Och and Jay Ponte2008.
A Systematic Comparison of Phrase-Based, Hierarchical andSyntax-Augmented Statistical MT In Proceedings of COLING253
