Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 1011?1018, Vancouver, October 2005. c?2005 Association for Computational LinguisticsLearning Mixed Initiative Dialog StrategiesBy Using Reinforcement Learning On Both ConversantsMichael S. English and Peter A. HeemanCenter for Spoken Language UnderstandingOGI School of Science & EngineeringOregon Health & Science UniversityBeaverton OR, 97006, USAmenglish6@gmail.com and heeman@cslu.ogi.eduAbstractThis paper describes an application of re-inforcement learning to determine a dia-log policy for a complex collaborative taskwhere policies for both the system and aproxy for a user of the system are learnedsimultaneously.
With this approach a use-ful dialog policy is learned without thedrawbacks of other approaches that re-quire significant human interaction.
Thespecific task that the agents were trainedon was chosen for its complexity and re-quirement that both conversants bring taskknowledge to the interaction, thus ensur-ing its collaborative nature.
The results ofour experiment show that you can use re-inforcement learning to create an effectivedialog policy, which employs a mixed ini-tiative strategy, without the drawbacks oflarge amounts of data or significant humaninput.1 IntroductionThe problem of developing a dialog manager can beexpressed as the task of building a specific dialogpolicy for the dialog system to follow as it interactswith the user.
A dialog policy can be thought of as anenumeration of all of the states a dialog system canbe in, and the corresponding action to take from eachof those states.
Thus a policy completely specifiesthe behavior of a dialog manager.Most conventional approaches to accomplishingthis task seek to directly model human interactionsin some manner.
These techniques include hand-crafting a policy, using a Wizard-of-Oz approach inan iterative manner and inducing a policy from ahuman-human dialog corpus.
All three approacheshave shortcomings that make them less than ideal fordeveloping dialog systems.
The approach of hand-crafting of a dialog policy is problematic as it isdifficult to predict how a user with interact with it,making it difficult to craft an optimal policy.
To getaround this, an iterative approach can be used, witha Wizard taking the place of the system.
However, itis still difficult to train a wizard, and it is difficult toexplore many different strategies in order to find theoptimal one.
Human-human dialog can be used forpolicy generation, as this should represent optimalbehavior to accomplish a task.
However, computersare not capable of behaving exactly as a human.
Inaddition, humans might not interact with a computeras they would another person.Recently a number of researchers have proposedusing reinforcement learning to alleviating the prob-lems encountered with more conventional methodsof developing dialog policies.
With the developmentof a good policy evaluation function, reinforcementlearning can effectively and quickly explore a largepolicy space.
There is the additional benefit that itwill learn a policy that is optimal for the capabilitiesof the system.The main drawback of reinforcement learning ap-proaches is that they require some form of conver-sational partner to train the system against.
Con-ventionally, these partners have taken the form of ahuman (Walker, 2000; Singh et al, 2002) or a simu-lated user (Levin et al, 2000; Scheffler and Young,2002; Georgila et al, 2005).
These two types of con-versational partners limit the complexity and diver-sity of policies that can be generated by reinforce-ment learning.
These two approaches to trainingpartners limit the whole system to the abilities ofthe partners themselves.
For a human partner we1011run into the significant time and effort problems thatwere present in Wizard-of-Oz and handcrafting pol-icy development.
With a simulated user the systemis limited by the complexity and flexibility of thesimulated user, which itself can require a large de-gree of handcrafting by its creator.In this paper, we propose a solution to the con-versational partner problem of generating a dialogpolicy with reinforcement learning.
We have taken acomplex collaborative task and used reinforcementlearning, applied to both participants, to develop adialog policy for the task.
By training both agentssimultaneously we are able to avoid the uncertain-ties of creating a user to train against, as well as thetime and data limitations of training directly againsthumans.
Our training approach allows us to avoidthese conventional drawbacks even while applyingreinforcement learning to complex tasks.Section 2 provides a brief overview of previouswork in using reinforcement learning for dialog sys-tems.
Sections 3 and 4 describe the dialog task andits specification as a reinforcement-learning prob-lem.
Section 5 and 6 present the results of this ex-periment and a discussion of them.2 Related WorkA number of researchers have explored using re-inforcement learning to create a policy for a dia-log system.
Walker (2000) trained a dialog system,ELVIS, to learn a dialog strategy for supporting spo-ken language access to a user?s email.
The mainfunction of ELVIS is to provide verbal summaries ofemail folders.
This summary could consist of simplestatements about the number of messages or a moredetailed description of current emails.Reinforcement learning is used to determine thebest settings for a variety of properties of the sys-tem.
For example, the system must learn to choosebetween email reading styles of reading back the fullemail first, reading a summary of the email first, orprompting the user with the two choices of readingstyles.
The system also learns whether it is better totake a mixed initiative or a system initiative strategywhen interacting with the user.To enable the learning process, ELVIS utilizedhuman users as its conversational partner.
Users per-formed a set of tasks with ELVIS, with each run us-ing different state-property values, which were ran-domly chosen for that dialog.
In order to support hu-mans as a training partner Walker restricted the pol-icy space so that it would only contain policies thatwere capable of accomplishing the available systemtasks.
Thus, during training the users would not befaced with a system that simply could not performthe tasks asked of it.ELVIS was trained with a Q-learning approachthat sought to determine the expected utility at eachstate, where utility was a subjective function involv-ing such variables as task completion and user sat-isfaction.
The state variables utilized in the trainingprocess were (a) whether the user?s name is known,(b) what the initiative style is, (c) the task progress,and (d) what the user?s current goal is.
Given thesestate variables, ELVIS was able to learn the beststyle to adopt in responding to the user?s requests atvarious points in the dialog.
One major shortcomingof the conversational partner used with ELVIS is itsreliance upon human interaction for training.
Thisshortcoming is somewhat mitigated by the fact thatthe learning problem was one of fitting together pre-existing policy components, but would be severelylimiting if the goal was to learn a complete dialogpolicy.
The amount of data necessary for learning acomplete policy makes direct human interaction inthe learning process unrealistic.Levin et al (2000) tackles a slightly differentreinforcement-learning task.
She is learning a pol-icy to use in a dialog system built from a small setof atomic actions.
This system is trained to providea verbal interface to an airline flight database.
Thissystem is able to provide users with a way to findflights that meet a dynamic set of criteria.
The di-alog agent?s state consists of information regardingthe departure city, destination city, flight date, etc.Levin takes a useful approach in reducing the sizeof true state space by simply tracking when a partic-ular state variable has a value rather than includingthe specific value in the state.
For instance duringa dialog when the system determines that the de-parture city is New York it does not distinguish thisfrom when it has determined that the departure cityis Chicago.To converse with the dialog agent during rein-forcement learning, Levin uses a ?simulated user.
?The simulated user is created from a corpus of hu-man dialogs with a prior airline system.
In de-1012veloping this user Levin makes the simplifying as-sumption that a user?s response is based solely onthe previous prompt.
Then the specific probabilitiesfor each user response are determined by examin-ing the corpus for exchanges that match the possibleprompts for the new dialog agent as well as handcrafting some of the probabilities.
During the actuallearning the agent used Monte Carlo training withexploring starts in order to fully explore the statespace.The ?simulated user?
method of supplying theconversational partner seems difficult and not partic-ularly applicable to tasks where a dialog corpus doesnot already exist, but Kearns and Singh (1998) indi-cates that the accuracy of the transition probabilitiesfor the probabilistic user is not critical for the dialogagent to learn an optimal strategy.
While this experi-ment does allow for the dialog agent to learn a com-plex strategy, the notion of learning against a sim-ulated user limits the space of policies that will beconsidered during training.
Training against a con-versational partner that is a model of a human au-tomatically prejudices the system towards policiesthat we would be inclined towards building by handand precludes the sincere exploration of all possiblepolicies.3 Task SpecificationFor our experiment we use the task presented inYang and Heeman (2004), which is a modificationof the DesignWorld task of Walker (1995).
The taskrequires 2 conversants to agree on 5 pieces of furni-ture to place in a room.
Both conversants know allof the furniture items that can be chosen, which dif-fer by color, type and point value.
Each conversantalso has private preferences about which furnitureitems it wants in the room; such as ?if there is a redcouch in the room, I also want a lamp in the room?.Each preference has a score.
As this is a collabora-tive task, the conversants have the goal of finding the5 furniture items that have the highest score, wherethe score is the sum of the point value of each ofthe 5 chosen furniture items less the scores for anyviolated preferences of either conversant.The conversational agents work to achieve theirgoal by performing the following actions: propose,accept, reject, inform, and release turn.
If thereis not a current proposal, either agent can proposean item, which makes that item into the current pro-posal.
If there is a current proposal, the other conver-sant can accept it or reject it.
Accepting an item re-sults in that item being included in the task solutionand removes it as the current proposal.
Rejectinga proposed item removes it as the current proposal.When an item has been rejected it remains a validchoice for future proposals.
In addition to accept-ing or rejecting a proposal, either conversant mayinform the other conversant of preferences that areviolated by the current proposal.
A preference is vi-olated by the current proposal if the addition of thatproposed item to the solution set would cause thesolution set to violate the preference.
When a con-versant informs of a violated preference, that prefer-ence becomes mutually known and so affects futuredecisions by both participants.
Only preferences thatare not known by the other conversant are commu-nicated.
For turn taking, we include the action re-lease turn, which the conversant that currently hasthe turn can perform to signal that it is relinquishingthe turn (cf.
Traum and Hinkelman, 1992).
Note thatafter a release turn, the other agent must make thenext move, which could itself be a release turn.
Theinclusion of this action allows conversants to per-form multiple actions in a row, such as a reject, aninform, and a propose.
Our approach to turn tak-ing differs slightly from Yang and Heeman, as theymake it an implicit part of other actions.In order to successfully utilize these actions in adialog, some reasoning effort is required of the con-versants.
Conversants must be able to determinewhat preferences are violated by a pending proposaland which of the remaining items makes the bestproposal.
In order to keep the reasoning effort man-ageable, we follow Yang and Heeman and use agreedy algorithm to pick the item that results in thebest score for the item plus the set of items alreadyaccepted.
The conversants do not consider interac-tions with the items that will be subsequently addedto the plan.
Conversants using this greedy approachcan construct a plan that is very close to optimal.4 Learning Specification4.1 Agent SpecificationIn order to apply reinforcement learning to this taskwe must formalize the conversants as reinforcement1013learning agents, specifying their state and actions,as well as the environment they will interact in.
Inorder to reduce the size of the state space for thistask we simplified the representation of the state ina manner similar to that done by Levin (2004).
Weformulated the state of the dialog agents with manyof the more specific details of the actual state of thetask removed.
For instance the agent state does notinclude specific information about the furniture itemthat is the pending proposal, rather the agent?s stateonly indicates that there is a pending proposal.The state specification for each agent includesthe following binary variables: Pending-Proposal,I-Proposed, Violated-Preference, Prior-Violated-Preferences, and Better-Alternative.
Pending-Proposal indicates whether an item has been pro-posed but not accepted or rejected.
I-Proposed in-dicates if the agent made the most recent proposal.Violated-Preference indicates that the pending pro-posal has caused one or more violations of theconversant?s private preferences.
Prior-Violated-Preferences indicates whether the conversant hadone or more violated preferences when the pendingproposal was made.
This variable allows the agentto remember what its original response to a proposalwas, even after it may have shared all of its prefer-ences that were violated (thus creating a state whereit no longer has any violated personal preferences).Better-Alternative indicates that the agent thinks itknows an item that would achieve a better score thanthe item currently proposed.The actions from Section 3 can be sequenced ina number of different orders, leading to differentpolicies.
Unlike Yang and Heeman, who comparedhandcrafted policies, we use reinforcement learningto learn policy pairs, one part of the pair for the sys-tem, and the other for the simulated user.
We haverestricted the space of policies that can be learned.First, we reduce the space by only considering le-gal sequences of actions.
For example, if there is apending proposal, another item cannot be proposed.Second, after 5 items have been accepted, the dialogis automatically ended.
Third, to keep the space ofdialog policies small, we force an inform to informof all violated preferences at once.The Reinforcement Learning states and actions ofour dialog agents capture a subset of the true stateof the dialog.
Our agents do not have the ability todistinguish between, or develop distinct policies inresponse to, the proposal of a blue chair versus a reddesk.
Since our formulation of the dialog agents donot encode specific information about items or pref-erences, the dialog environment must maintain thesedetails.
This extra information that must include thecurrently proposed item, what each agent?s privateand currently violated preferences are, what pref-erences are shared between each agent, what itemshave been accepted as part of the task solution, andwhat items are still available for selection.
This tech-nique of generalizing the state space is the same asthe one used by Levin (2000), and allows us to keepthe state space at a manageable size for our task.4.2 Reinforcement LearningFor our Reinforcement Learning algorithm we choseto use an on-policy Monte Carlo method (Sutton andBarto, 1998).
Our chosen task is naturally episodicsince the two agents agreeing upon five items indi-cates task completion and thus the end of the dialog,which constitutes one learning episode.
We also im-posed a limit of 500 interactions per dialog in orderto ensure that each learning episode was finite evenif the task was not successfully completed.
Forsome state-action pairs our task does not allow theaccurate specification of the resulting state.
In fact,due to the way that our state representation simpli-fies the true task environment an action choice formany states will necessarily lead to different statesdepending upon the task environment.
For instance,proposing an item will sometimes lead to that itemsacceptance and sometimes it will be rejected.
Giventhis uncertainty our learning approach necessarilyhad to learn the expected rewards of actions insteadof states.At the end of each dialog the interaction is givena score based on the evaluation function and thatscore is used to update the dialog policy of bothagents.
The state-action history for each agent isiterated over separately and the score from the re-cent dialog is averaged in with the expected returnfrom the existing policy.
We chose not to includeany discounting factor to the dialog score as we pro-gressed back through the dialog history.
The deci-sion to equally weight each state-action pair in thedialog history was made because an action?s contri-bution to the dialog score is not dependent upon its1014proximity to the end of the task.
An action that ac-cepts a proposed item at the beginning of the dialogshould be rewarded as much as an action that acceptsa proposed item later in the same dialog.In order for the learning agents to obtain a largeenough variety of experiences to fully explore thestate space some exploration technique must beused.
We chose to use e-greedy action selection inorder to achieve this goal.
With this approach thedialog agent makes an on policy action choice withprobability 1-e and a random valid action choice therest of the time.Training both agents simultaneously causes eachagent to learn its policy as an optimal response to theopposing agent.
This can create problems in the ini-tial stages of training as each agent has an immaturepolicy that is based on little experience.
In this situ-ation each of the agents will associate weights withstate action pairs based on action choices of the op-posing agent that are themselves not well developed.As training progresses the eccentricities of the ini-tial immature policies are perpetuated and the learn-ing process does not converge on an effective dialogpolicy for either agent.In order to combat the problem of converging toan effective policy we divided up the agent trainingprocess into multiple epochs.
Each epoch is com-posed of a number of training episodes.
The initialepsilon value is set to a large value and for each suc-cessive epoch the epsilon value for action selectionis decreased.
With an initially high epsilon valuethe agents are able to develop a policy that is ini-tially weighted more heavily towards a response torandom action selection than the immature policy ofthe other agent.
As the epsilon value decreases, eachagent slowly adjusts its learning to be weighted moreheavily towards a response to the other agent?s pol-icy.
This approach allows the agents to develop aminimally coherent dialog policy before beginningto rely too heavily upon the response of the oppos-ing agent.Utilizing this strategy of continuously decreasingepsilon values we were able to get both agents toconverge to an effective and coherent dialog policy.The initial epsilon value was set to 804.3 Objective FunctionIn the reinforcement learning process the objectivefunction provides the dialog agents with feed-backon the success of each dialog.
The specification ofthis function requires input from a human.
For ourlearning specification we crafted a simple functionthat attempted to model a human perception of a di-alog?s quality.
Our objective function is linear com-bination of the solution quality (S) and the dialoglength (L), taking the form:o(S, I) = w1S ?
w2Lwhere w1 and w2 are positive constants.
As highervalues for S and lower values for L indicate betterdialogs, we subtract w2L from w1S.
Instead of at-tempting to hand pick the constants in the objectivefunction, we explored the effects of different values,which we report in Section 5.2.For our experiment we trained the dialog agentsfor 200 epochs, where each epoch consisted of 200training episodes.
After the training the agents, wethen had them perform 5000 dialogs with 100% on-policy action selection (i.e.
strictly following thelearned policy).
The results of these 5000 dialogswere then combined to obtain an average plan scoreand average number of interactions for the policy ofthe agents.
These two values are then combined ac-cording to the objective function to obtain a numericscore for the learned policy.5 ResultsIn this section, we present the results of the dialogpolicies that we learned.
We first present 3 baselinepolicies to which we will compare the performanceof our learned policies.
We will then present resultsvarying the weights in the objective function in com-parison to the baseline policies.
As we are learninga pair of policies?one for the system and one rep-resenting the user?we explore how well the systempolicy does against handcrafted ones, that will repre-sent what a user might do, rather than test it againstits learned counter-part.5.1 Baseline PoliciesIn order to provide comparative data to evaluate theeffectiveness of our approach, we will compare theperformance of the policies learned for the systemand user against several pairs of handcrafted poli-1015cies.
The first pair implement the unrestricted ini-tiative strategy of Yang and Heeman.
Here, one con-versant, A, proposes an item and then the other, B,informs A of any violated preferences.
B then pro-poses an alternative and A informs B of any violatedpreferences.
The process repeats until an item is pro-posed that does not violate any of the other agent?spreferences.
The second pair of policies implementthe restricted initiative policy of Yang and Heeman,in which A proposes an item and B informs A ofany violated preferences.
However, the conversantsdo not switch roles: it is always A who proposesitems and B that informs of preferences and accepts.These two policies represent successful handcraftedpairs of dialog policies.
The third pair represents aminimum performance: A proposes an item and Bsimply accepts it.
This is repeated for all 5 items,with A making all of the proposals.
This policyis an un-collaborative approach, which representshow well A can do on its own.5.2 Impact of Weights on Learned PolicyWe first explore the ability of the reinforcementlearning algorithm to learn a dialog policy pair thatis optimal with respect to the objective function.
Theonly important aspect of the weights is the ratio be-tween the two: w2/w1.
We varied the ratio from0.1 to 0.5 in increments of 0.02.
For each weightsetting, we learned 66 policy pairs, and tested eachpolicy pair on 1000 different task configurations.
Wecompared the average objective function score of thelearned policy pairs with the baseline restricted pol-icy pair (cf.
Scheffler and Young, 2002).
Figure 1shows the percentage of the learned policies that per-form at least as well as the unrestricted policy pair00.10.20.30.40.50.60.70.80.020.06 0.1 0.140.180.220.26 0.3 0.340.380.420.46w2/w1PercentFigure 1: Percentage of learned policies performingbetter than unrestricted baseline pair.at each weight setting.
Interestingly, it is clear thatthere is a lack of convergence in the learning pro-cess, no weight ratio learns a good policy 100% ofthe time.
Additionally, we see that as the weightratio increases (putting more emphasis on shorterdialogs), the ability of the algorithm to learn goodpolicies decreases.
As the objective function givesthis aspect more weight, it is more difficult for theobjective function to learn the importance of solu-tion quality.
We think this lack of convergence isdue to learning both the system and a simulated userat the same time, which is a more difficult reinforce-ment learning problem than just learning the policyfor the system against a fixed user.5.3 Lack of ConvergenceTo better understand the lack of convergence, we ex-plore when a single weight is chosen for the objec-tive function.
For this analysis, we restricted our-selves to the objective function having a ratio forw2/w1 of 0.1, one of the best performing weightsfrom section 5.2.
For this setting, we learned a num-ber of policy pairs, each learned from a different se-quence of task configurations.
We then tested eachpolicy pair on 1000 task configurations, in which ac-tions are selected strictly according to the learnedpolicy.
This gives us 1000 dialogs for each policypair.
We then computed the average objective func-tion score for each policy pair and plotted them as ahistogram in Figure 2.
As can be seen, at this weightsetting, 63% of the learned policies achieved an ob-jective function score around 44.8.
However, therest achieved a performance substantially less thanthis.
Hence, the reinforcement learning proceduredoes not always converge on an optimal solution.To better understand why reinforcement learningis not always converging, we examined the compo-nents of the objective function score: solution qual-ity and dialog length.
Figure 3 uses the same x-axisas Figure 2: average objective function score.
They-axis plots the average solution quality and averagedialog length.
We see that at this weight ratio, alllearned dialogue pairs are very consistent in solutionquality, but that the difference in objective functionscores is mainly due to differences in dialog length.This is consistent with our earlier observation thatthe reinforcement learning strategy sometimes dis-proportionately favors shorter dialog length.101605101520253035404543.82 43.93 44.05 44.16 44.27 44.39 44.50 44.61 44.73 44.84Objective Function ScoreInstancesFigure 2: Average objective function scores for poli-cies learned with w2/w1 = 0.1.0510152025303540455043.82 43.93 44.05 44.16 44.27 44.39 44.50 44.61 44.73 44.84Objective Function ScoreSolution Quality Dialog LengthFigure 3: Variation of solution quality and dialoguelength versus objective function score for policieslearned with w2/w1 = 0.1.5.4 Consistency of PoliciesFor the weight ratio of 0.1, the reinforcement learn-ing algorithm usually finds a good policy pair.
Tofurther improve the likelihood of this happening, wecould learn multiple policy pairs, and then pick thebest performing one.
In this section, we comparelearned policies chosen in this way against the re-stricted baseline pairs.
We learned 10 sets of 10 dia-logue pairs.
We then ran each on 1000 task configu-rations and chose the best performing policy pair ineach set.
We then ran the resulting 10 policy pairson another set of 1000 task configurations.
Table 1gives the average objective function score for each ofthe 10 learned policy pairs and the 3 baseline pairs.From the table, we see that the learned policy pairperforms almost as well as the restricted policy pair,for both solution quality and dialog length.5.5 Robustness of Learned PoliciesAll of the results so far have used the learned pol-icy for the system interacting with the correspondingpolicy that was learned for the user.
However, thereObjective Solution DialogFunction Quality LengthLearned Policies 44.90 46.71 18.17Restricted 45.04 46.89 18.44Unrestricted 44.40 46.80 24.07Uncollaborative 32.52 33.62 11.00Table 1: Comparison of Learned Policiesis no guarantee that a real user will behave like thelearned policy.
Thus, the true test of our approachis to run the learned system policy against actualusers.
The problem with testing our policies againstactual users is that there are a number of aspectsof dialog that we have not modeled, such as non-understandings, misunderstandings, and even pars-ing sentences into the action specification and gener-ating sentences from the action specification.
Thus,as a simplification we tested our learned system pol-icy on the handcrafted baseline policies.For the weight ratio of 0.1, we learned 10 sets of10 pairs of policies and choose the best policy pairfrom each set.
For each of the 10 policy pairs, we ranthe system policy against the 6 individual policiesfrom the 3 baseline policy pairs.
We changed thehand-crafted policies slightly from Yang and Hee-man so that the policies would not fail if they en-countered unexpected input.
For example, for therestricted policy for A (the conversant who proposesbut never informs), if the learned policy proposes anitem, A always rejects it.
For the restricted policyfor B (the conversant who informs but never pro-poses), if the learned policy releases the turn whenthere is not an item proposed, B simply releases theturn back to the learned policy.Figure 4 shows the resulting average objectivefunction scores on 1000 dialog runs.
For each base-line policy, we show the performance with the pol-icy pair, and then with each side of the baseline pol-icy interacting with the learned policy.
We see thatalthough the performance of the learned policy isnot as good as with the handcrafted pair, the perfor-mance is close, with the major shortcoming beinga general increase in dialog length.
Thus, the poli-cies that we have learned our robust against differentstrategies a user might want to use.1017Solution Quality05101520253035404550Uncollaborative Unrestricted RestrictedLearned with Baseline A Learned with Baseline B Baseline PairDialog Length05101520253035404550Uncollaborative Unrestricted RestrictedLearned with Baseline A Learned with Baseline BBaseline PairFigure 4: Learned dialogue policies interacting withbaseline policies.6 ConclusionIn this paper, we proposed using reinforcement forlearning a dialog strategy for the system.
Our ap-proach differs from past research in that we learnthe system policy in conjunction with learning a userpolicy.
This approach of learning the user policy al-lows us to minimize human involvement, as neithera training corpus must be collected nor a simulateduser built.
Thus, the only human input required forthis approach was to define the domain task and todefine success in that domain.
While our trainingapproach did not always find an effective policy, weovercame this obstacle by carefully choosing a ra-tio for the weights in the objective function and byrunning the learning algorithm multiple times.
Ourapproach resulted in learned system and user dia-log policies that achieved comparable performancewith handcrafted system and user policy pairs.
Fur-thermore, the learned system policies were robust.When the learned system policies ?conversed?
withthe handcrafted user policies, the resulting dialogshad comparable solution quality to what the hand-crafted system and user policies achieved together.Even with the lack of convergence our approachcould be applied to more complicated domains in or-der to learn an effective dialog policy.
Our approachwould be especially useful in situations where thereare no existing corpora of human-human interac-tions for the domain or as a way to provide a checkagainst a policy based on human intuition.
In mostsituations where the domain requires significant col-laboration between the dialog system and the user,training both the system and a user simultaneouslywill prove to be much less costly and labor intensiveapproach.7 AcknowledgmentsThe authors thank John Moody and Fan Yang forhelpful discussions.
Partial funding for this researchwas provided by the National Science Foundationunder grant IIS-0326496.
The first author is now atGoogle.ReferencesK.
Georgila, J. Henderson, and O.
Lemon.
2005.
Learn-ing user simulations for information state update dia-logue systems.
In Eurospeech, Lisbon Portugal.M.
Kearns and S. Singh.
1998.
Finite-sample conver-gence rates for q-learning and indirect algorithms.
InNIPS, Denver CO.E.
Levin, R. Pieraccini, and W. Eckert.
2000.
A stochas-tic model of human-machine interaction for learningdialog strategies.
IEEE Transactions on Speech andAudio Processing, 8(1):11?23.K.
Scheffler and S. J.
Young.
2002.
Automatic learn-ing of dialogue strategy using dialogue simulation andreinforcement learning.
In HLT, pages 12?18.S.
Singh, D. Litman, M. Kearns, and M. Walker.
2002.Optimizing dialogue managment with reinforcementlearning: Experiments with the njfun system.
Journalof Artificial Intelligence Research, 16:105?133.R.
Sutton and A. Barto.
1998.
Reinforcement Learning.MIT Press, Cambridge MA.D.
Traum and E. Hinkelman.
1992.
Conversation acts intask-oriented spoken dialogue.
Computational Intelli-gence, 8(3):575?599.M.
Walker.
1995.
Testing collaborative strategies bycomputational simulation: Cognitive and task effects.Knowledge-Based Systems, 8:105?116.M.
Walker.
2000.
An application of reinforcement learn-ing to dialog strategy selection in a spoken dialoguesystem.
Journal of Artificial Intelligence Research.F.
Yang and P. Heeman.
2004.
Using computer simu-lation to compare two models of mixed-initiative.
InICSLP.1018
