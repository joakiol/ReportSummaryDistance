Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 9?16,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsReliable Lexical Simplification for Non-Native SpeakersGustavo Henrique PaetzoldDepartment of Computer ScienceUniversity of SheffieldSheffield, United Kingdomghpaetzold1@sheffield.ac.ukAbstractLexical Simplification is the task of modifyingthe lexical content of complex sentences in or-der to make them simpler.
Due to the lack ofreliable resources available for the task, mostexisting approaches have difficulties produc-ing simplifications which are grammatical andthat preserve the meaning of the original text.In order to improve on the state-of-the-art ofthis task, we propose user studies with non-native speakers, which will result in new, size-able datasets, as well as novel ways of per-forming Lexical Simplification.
The resultsof our first experiments show that new typesof classifiers, along with the use of additionalresources such as spoken text language mod-els, produce the state-of-the-art results for theLexical Simplification task of SemEval-2012.1 IntroductionLexical Simplification (LS) is often perceived as thesimplest of all Text Simplification sub-tasks.
Its goalis to replace the complex words and expressions ofa given sentence with simpler alternatives of equiv-alent meaning.
However, this is a very challengingtask as the substitution must preserve both originalmeaning and grammaticality of the sentence beingsimplified.However, this is a very challenging task as thesubstitution needs to ensure grammaticality andmeaning preservation.
Most LS strategies in the lit-erature are structured according to the pipeline illus-trated in Figure 1, which is an adaptation of the oneproposed by (Shardlow, 2014).Figure 1: Lexical Simplification pipelineIn this thesis, we intend to identify and addressthe major limitations of the approaches in the liter-ature with respect to each step of the LS pipeline ofFigure 1.
In an effort to create new reliable datasetsfor LS and to unveil information about the needs ofthose who can most benefit from Text Simplifica-tion, we propose new user studies with non-nativespeakers.
We also present novel modelling strate-gies for each step of the LS pipeline with respect tothe limitations of the approaches in the literature.2 Lexical Simplification: A SurveyTo our knowledge, there are no examples of studieswhich compare the performance of LS approaches intheir entirety.
For this reason, we choose instead todiscuss the merits and limitations of strategies usedby authors to address each step of the LS pipeline.2.1 Complex Word IdentificationThe goal of Complex Word Identification (CWI) isto identify which words in a given sentence need tobe simplified.
Some authors, such as (Devlin andTait, 1998), (Carroll et al, 1998) and (Carroll et al,91999) choose to not address this task, but as shownin (Paetzold and Specia, 2013), this can lead to theproduction of incoherent and/or ungrammatical sen-tences.
Several categories of CWI strategies can befound in literature:Lexicon-Based Explore the hypothesis that, if aword w is part of a lexicon L of complex/simplewords, then it does/does not need to be simplified.While (Watanabe and Junior, 2009) and (Aluisio andGasperin, 2010) use as lexicons books for children,(Elhadad and Sutaria, 2007), (Del?eger and Zweigen-baum, 2009) and (Elhadad, 2006) use a database ofcomplex medical terms.
Acquiring lexicons can beeasy, but they must correlate with the needs of thetarget audience in question.Threshold-Based Explore the hypothesis that athreshold t over a word metric M(w) can separatecomplex from simple words.
The most frequentlyused metrics are word frequency (Bott et al, 2012),(Leroy et al, 2013) and word length (Keskis?arkk?a,2012).
However, the corpus evaluation of (Bottet al, 2012) shows that determining such thresholdt is impractical.User-Driven Such approaches allow the usersthemselves to select which words are complex, andsimplify them on demand.
Although the results ob-tained by (Devlin and Unthank, 2006) and (Relloet al, 2013) show that this is a very effective strat-egy, it might be difficult for it to be used in smallerdevices, such as phones.Classification Methods Train classifiers whichdiscriminate between complex and simple words.For English, the SVM approach of (Shardlow,2013a) is the only example in literature.
Althoughtheir study shows that their SVM is not able tooutperform neither a threshold-based approach or a?simplify everything?
method, we believe the resultsobtained are controversial.In another study conducted by the same author(Shardlow, 2014) it was found that replacing wordswhich do not need simplification is one of the mostfrequent mistakes made by naive LS approaches,and hence we believe the results obtained by (Shard-low, 2013a) do not reveal the potential of classifi-cation methods in CWI.
Also, the dataset used theexperiments of (Shardlow, 2013a) was created au-tomatically and did not attempt to model the needsof any particular target audience.
A more substan-tial comparative study between multiple distinct ma-chine learning methods over a more carefully craftedcorpus could be a major milestone in the develop-ment of more efficient CWI approaches.2.2 Substitution GenerationThe Substitution Generation (SG) task consists inacquiring candidate substitutions for the complexwords in a sentence.
This task have been approachedby authors in two different ways:Querying Linguistic Databases Resources suchas WordNet (Fellbaum, 1998) and UMLS (Boden-reider, 2004) provide large word ontologies, andhave been largely used even in modern contribu-tions.
The approaches of (Devlin and Tait, 1998),(Sinha, 2012), (Leroy et al, 2013), (Chen et al,2012), (Elhadad, 2006) and (Nunes et al, 2013) aresome examples.
The study of (Shardlow, 2014),however, shows that over 42% of the mistakesmade by the approach of (Carroll et al, 1998) arecaused by WordNet not having simpler synonymsfor complex words.
Using such resources also limitsthe cross-lingual capabilities of the approach, sincemost of those resources are restricted to one or veryfew languages.Automatic Generation Consists in automaticallygenerating pairs of related words and paraphrases.The works of (Elhadad and Sutaria, 2007), (Kauchakand Barzilay, 2006) and (Del?eger and Zweigen-baum, 2009) focus on extracting paraphrases fromcomparable documents.
The methods of (Paetzoldand Specia, 2013), (Feblowitz and Kauchak, 2013),and(Horn et al, 2014) extract pairs of similar ex-pressions from a aligned sentences from Wikipediaand Simple Wikipedia.
But although such ap-proaches do not need linguistic databases, they re-quire for other resources, such as parallel corpora,which are also scarce.
They can also suffer for ex-tracting too many meaningless substitutions, such asobserved in (Paetzold and Specia, 2013).In order to solve the cross-lingual problem, an SGapproach would have to be able to find substitutionsby exploiting only resources which are either abun-dant in most languages or easy to produce.
In Sec-10tion 3 we discuss how we attempt to address thisproblem.2.3 Substitution SelectionSubstitution Selection (SS) is the task of determin-ing which substitutions fit the context in which acomplex word appears, and hence ensuring meaningpreservation.
SS have been addressed by authors inthree ways:Word Sense Disambiguation Determine thesense of a complex word in a target sentence, andthen filter substitutions which do not share suchsense.
The approaches of (Sedding and Kazakov,2004) and (Nunes et al, 2013) have proven to besuccessful in SS alone, but have not been evaluatedin practice.
The main limitation of this strategyis that it relies on manually constructed sensedatabases, which are scarce.Adapted Disambiguation Use surrogate classesto discriminate between the meanings of an ambigu-ous word.
The words?
POS tags are used in theworks of (Aluisio and Gasperin, 2010), (Yamamoto,2013) and (Paetzold and Specia, 2013).
While us-ing POS tags may help with words of more than onegrammatical type, it does not solve the problem ofhighly ambiguous words.Semantic Similarity Estimate the semantic simi-larity between words and verify if they are replace-able.
In (Keskis?arkk?a, 2012) is employed a simpleapproach: if a pair of words has a synonymy coef-ficient higher than a threshold, they are replaceable.This approach, however, requires for a database ofsynonymy levels.
The approach of (Biran et al,2011) solves that by representing the semantic con-text of words with word vectors estimated over largecorpora, then using the cosine distance between vec-tors as its semantic dissimilarity.We did not find mentions of Machine Learningmethods being applied to SS.
Such methods havebeen used to produce state-of-the-art results in manyclassification tasks, and hence modelling SS as aclassification problem can be a promising strategy.2.4 Substitution RankingConsists in deciding which substitution is the sim-plest of the ones available.
The LS task of SemEval2012 brought a lot of visibility to the task, and manyauthors still visit this subject to this day.
The threemost efficient strategies found in literature are:Frequency-based Explore the intuition that themore frequently a word is used, the simpler it is.Most authors use raw frequencies from large corpora(Keskis?arkk?a, 2012), (Leroy et al, 2013), (Aluisioand Gasperin, 2010), (Nunes et al, 2013) or theKucera-Francis coefficient (Rudell, 1993), (Devlinand Tait, 1998), (Carroll et al, 1998).
Although(Brysbaert and New, 2009) points out several issueswith the Kucera-Francis coefficient, the results ofSemEval 2012 (Specia et al, 2012) show that rawfrequencies from the Google 1T corpus outperformalmost all other approaches.Measuring Simplicity Elaborate metrics to repre-sent the simplicity of a word.
The metric of (Sinha,2012) considers the word?s length, number of sensesand frequency, and have tied in 2nd place in Se-mEval 2012 with the Google 1T baseline.
The otherexamples in literature, (Biran et al, 2011) and (Bottet al, 2012), were published before SemEval 2012,and hence have not yet been compared to other ap-proaches.Linear Scoring Functions Rank candidates basedon a linear scoring function over various metrics,such as frequency and word length.
This strategyis used by the approach that placed 1st in SemEval2012 (Jauhar and Specia, 2012).In (Shardlow, 2014) it is shown that word frequen-cies from spoken text corpora have great potential inSR.
In Section 3.4 we describe an experiment whichreveals the potential of such resources.3 Planning and Preliminary ResultsIn the following Sections, we discuss which chal-lenges we aim to address in the near future, andbriefly describe the solutions we intend explore.3.1 User Studies and DatasetsAs pointed out in Section 2, the scarcity of user stud-ies about audiences that may benefit from LS com-pel authors to treat simplification as a generalisedprocess, forcing them to use datasets such as theSimple Wikipedia, which can be edited by anyone.11Since we do not believe this ideal, we intend to con-duct an array of user studies with non-native speak-ers.
We chose such audience because of three mainreasons:Demand Unfamiliarity with a language is not amedical condition that can be cured, and hence suchaudience is not likely to disappear in the near future.Convenience Conducting studies with ill or youngsubjects needs to be done within various ethical con-straints, and can be both expensive and time con-suming.
Although the needs of these audiencesshould also be addressed, hiring non-native speak-ers is much easier, and we believe they fit best ourtime and resource constraints.Diversity Statistics show that there is a lot of age,nationality and education level diversity among thenon-native speakers (Austin et al, 2006).
Such di-versity allows for us to investigate several interestinghypothesis regarding possible correlations betweenthe subjects?
characteristics and difficulty with cer-tain types of words.We propose two initial user studies:Identifying Complex Words In this user study,subjects select which words from a given sentencethey do not understand the meaning of.
From thisstudy we hope to better understand what types ofwords are challenging for non-native speakers.It is very important for a reliable Complex WordIdentification dataset to be made available in liter-ature.
To our knowledge, there is only one contri-bution in literature that compares different CWI ap-proaches (Shardlow, 2013a), and since the datasetused was not created with respect to the needs of aspecific target audience, the results obtained are notvery informative.This study is already being conducted.
Severalvolunteers of various nationalities were asked to se-lect which words they find complex in 40 Englishsentences each, of which 10 are part of a set whichoverlaps between 5 volunteers and 30 are unique.The sentences vary between 20 and 40 words inlength, and were extracted from 3 distinct sources:the CW corpus (Shardlow, 2013b), the LexMturkcorpus (Horn et al, 2014) and Wikipedia (Kauchak,2013).
From the CW and LexMturk corpora wereextracted 231 and 269 non-spurious sentences, re-spectively, of which exactly 1 word is deemed com-plex by an anonymous annotator (more specifically,a Wikipedia editor).
From Wikipedia were extracted11945 sentences which were aligned to an identicalsentence from Simple Wikipedia.
By selecting suchsentences, we hope to be able to judge whether ornot those resources can be reliably used for the train-ing of Lexical Simplification approaches for non-native speakers.So far, 51 volunteers participated, who annotateda total of 2, 040 sentences.
A total of 1, 261 dis-tinct complex words (1, 597 total) were identified,12% of 10, 650 distinct words (53, 125 total).
Thevolunteers have distinct education levels (8% HighSchool, 57% Undergraduate and 35% Postgraduate),English proficiency levels (11% Advanced, 18%Pre-Advanced, 18% Upper-Intermediate, 37% In-termediate, 14% Pre-Intermediate, 2% Elementary),and have ages varying between 17 and 38 years old(averaging 24 years old).Selecting the Simplest Candidate We intend tofind out what are the key features taken into con-sideration by non-native speakers on determiningwhich is the simplest word that fits a given context.Just like in the case of Complex Word Identification,we believe that the creation of a reliable dataset forSubstitution Ranking is very important.The only dataset developed specifically for thispurpose is the one presented in SemEval 2012.
Butsince the rankings were produced by only 5 non-native annotators, there are a various examples ofties between two candidate substitutions.
Also, allsubjects were skilled speakers of the English lan-guage, which means that, at best, the dataset cap-tures the LS needs of an audience which may notneed LS at all.
With a larger dataset annotated bymore subjects of the same target audience, we willbe able to have a more reliable resource to createnovel Substitution Ranking approaches.3.2 Complex Word Identification MethodsWe intend to, based on the new datasets producedin our user studies, propose and evaluate the ef-ficiency of multiple different methods of ComplexWord Identification.
The methods we intend to eval-uate are:12Lexicon-Based Approaches We will compile aselection of corpora and see whether or not we canbuild lexicons from them which separate complexfrom simple words.
The Simple Wikipedia (Hornet al, 2014) and the SUBTLEX corpus (Brysbaertand New, 2009) are some examples.Threshold-Based Approaches There are multiplemetrics which we plan to use in order to train athreshold-based complex word identifier, some ofthem are: word frequency in a given corpus, wordlength, number of syllables, familiarity and age ofacquisition.Machine Learning Assisted By combining met-rics and lexicons, we can train many different clas-sification systems by using Machine Learning meth-ods.
Support Vector Machines, Gaussian Processesand Decision Trees are some Machine Learningmethods which we intend to test on Complex WordIdentification.3.3 Substitution Generation and SelectionWe propose an entirely new setup for joint mod-elling Substitution Generation and Selection.
Ourapproach consists in training classifiers capable ofdeciding which words wsof a vocabulary V can re-place a target word wcin a sentence s.Although this seems like a very challenging task,such an approach could be a very powerful tool forLS.
It could possibly dismiss entirely the need of us-ing parallel corpora or linguistic databases for suchtasks, and hence provide a cost-effective strategy forLS approaches to be ported to multiple languages.We suggest a two-step solution for this task:1.
Define a set G ?
V composed by all words wsfrom vocabulary V that can replace a word wcin sentence s without compromising its gram-maticality.2.
Define a set M ?
V composed by all wordswsfrom set G that express the same meaningof wcin sentence s.Once set M is determined, one can then use aSubstitution Ranking method to select which one ofthem is the simplest.
To create a dataset for this task,we plan to hire volunteer native speakers of the En-glish language to manually judge which words canbe part of G and M for a large array of different con-texts.
The user study data will be composed by sev-eral automatically generated substitutions for a set of50 complex words manually selected from the onesproduced in the Complex Word Identification study.3.4 Substitution RankingThe findings of the Lexical Simplification Task ofSemEval 2012 (Specia et al, 2012) have shown thatranking substitution candidates with respect to theirsimplicity is not an easy task.
In order to improve onthe state-of-the-art of Substitution Ranking, we in-tend to explore the usage of spoken textual content.As discussed in (Brysbaert and New, 2009), frequen-cies extracted from corpora of spoken text, such assubtitles, tend to correlate better with word familiar-ity than frequencies of other sources, given that thetext in subtitles is mostly composed of speech ex-cerpts from character interactions similar to the onesthat frequently occur in real life.
In order to evaluatetheir potential, we conducted a preliminary experi-ment.Goal In this experiment, we aim to answer the fol-lowing question: Can a language model of spokentext be used to outperform state-of-the-art Substitu-tion Ranking approaches?Datasets To build a corpus of spoken text, we haveparsed 13 HTML lists of movies and series for chil-dren created by IMDB1users.
A total of 1, 793IMDB IDs of distinct movies and series were gath-ered.
We then used such IDs to query the Open-Subtitles2API in search of subtitles for them.
Sincetheir API imposes a limit of 100 downloads per day,so far we were only able to collect subtitles of 163movies and series.
By removing the annotationsfrom the files downloaded, we compiled a corpusof 2, 103, 237 sentences.
For testing, we chose theSemEval 2, 012 corpus, which contains 300 traininginstances and 1, 710 test instances.
Each instance iscomposed of a sentence, a target word to be simpli-fied, and a list of candidate substitutions.Approach To rank the candidate substitutions, wepropose a novel binary classification setup for thetask.
For each training instance, we assign the label1http://www.imdb.com2http://www.opensubtitles.org131 to the highest ranked candidate, and 0 to the re-maining ones.
We then train a linear classifier overthe data to learn ranking weights for the selected fea-tures.
In testing, we rank substitution candidates ac-cording to their distance to the decision boundary:the furthest they are from the ?negative?
region, thesimpler they are.Our feature set is composed by 9 different col-locational features.
Each collocational feature of acandidate substitution c in context s is the log prob-ability produced by KenLM (Heafield et al, 2013),given the language model of a certain corpus, of ann-gram si?1i?lc si+ri+1, where i is the position of thetarget complex word in s, and both l and r are to-ken windows in the interval [0 : 2].
If l and r are0, then the collocational feature says respect to theprobability of candidate c independent of context s.Evaluation Metrics We have chosen the TRnkand recall-at-n measures proposed by (Specia et al,2012) to estimate the performance of our approach.The TRnk calculates the ratio with which a givenapproach has correctly ranked at least one of thehighest ranked substitutions on the gold-standard,while recall-at-n measures the coverage of correctlyranked candidates until position 1 ?
n ?
3.
Thereason for using such metrics instead of a rankingscore is that we believe they best represent the goalof the task in practice, which is selecting the sim-plest substitution possible for a complex word.Results Table 1 shows a performance comparisonbetween the highest ranking approach of SemEval2012 and our novel strategy trained with 10-foldcross validation over the training set.
We extractcollocational features from 4 distinct corpora: ourcorpus of IMDB subtitles (SubIMDB), the SimpleWikipedia corpus (Horn et al, 2014), composed of505, 254 sentences, the SUBTLEX corpus (Brys-baert and New, 2009), composed of 6, 043, 188 sen-tences taken from assorted subtitles, and the con-catenation of SubIMDB and SUBTLEX.The results show that our strategy outperforms theformer state-of-the-art approach of SemEval 2012by around 5% in TRank and 3% in recall-at-1.
Therecall-at-2 and 3 results, although lower than Se-mEval?s best, showcase not a limitation, but ratheran advantage of our binary classification setup: byfocusing on the task?s goal in practice, we are ableTable 1: TRank and recall-at-n results obtainedCorpus TRnk n=1 n=2 n=3Best SemEval 0.602 0.575 0.689 0.769IMDB+LEX 0.654 0.607 0.594 0.658SUBTLEX 0.638 0.592 0.584 0.658SubIMDB 0.628 0.583 0.578 0.637Simple Wiki 0.601 0.558 0.571 0.645to optimize not the correlation between the learnedrankings and the gold-standard, but instead the like-lihood of the best candidate substitution to be rankedfirst.
We can also notice from the results that, whentrained with features extracted from the SubIMDBcorpus, our approach performs similarly than whentrained with the SUBTLEX corpus, which is 3 timeslarger.
This phenomena suggests that restricting thedomain of the subtitles selected to that of moviestargeting younger audiences may help ranking ap-proaches in capturing word simplicity.In the future, we want to experiment with othertypes of language models, and also explore the po-tential of other types of spoken content, such as songlyrics and online conversations.4 Final Remarks and Future workIn this paper we described a thesis proposal whichfocuses in providing studies on the needs of non-native speakers in terms of LS, producing more reli-able datasets for various tasks of the LS pipeline, anddevising novel solutions to the limitations of mod-ern LS approaches.
We have provided a thoroughdiscussion on the state-of-the-art of LS, a detailedplan of the activities to be conducted throughout thedoctorate program and the results of our first experi-ment, in which we managed to achieve state-of-the-art results for the task of Substitution Ranking.In the future, we intend to study the simplifica-tion needs of other target audiences and explore LSstrategies that go beyond replacing complex wordsand expressions for simpler equivalents, such asby removing unimportant information and learningdeep simplification rules from parallel corpora bycombining constituency and dependency parses.14ReferencesAluisio, S. and Gasperin, C. (2010).
Proceedings of theNAACL HLT 2010 Young Investigators Workshop onComputational Approaches to Languages of the Amer-icas, chapter Fostering Digital Inclusion and Acces-sibility: The PorSimples project for Simplification ofPortuguese Texts, pages 46?53.
Association for Com-putational Linguistics.Austin, M., Paul, B., and Phil, B.
(2006).
Current stateof english-language learners in the u.s. k-12 studentpopulation.Biran, O., Brody, S., and Elhadad, N. (2011).
Putting itSimply: a Context-Aware Approach to Lexical Sim-plification.
Proceedings of the 49th Annual Meeting ofthe Association for Computaional Linguistics, pages496?501.Bodenreider, O.
(2004).
The unified medical languagesystem (umls): integrating biomedical terminology.Nucleic acids research.Bott, S., Rello, L., Drndarevic, B., and Saggion, H.(2012).
Can Spanish Be Simpler ?
LexSiS : LexicalSimplification for Spanish Puede ser el Espa?nol m?assimple ?
LexSiS : Simplificaci?on L?exica en Espa?nol.Brysbaert, M. and New, B.
(2009).
Moving beyondKucera and Francis: a critical evaluation of currentword frequency norms and the introduction of a newand improved word frequency measure for AmericanEnglish.
Behavior research methods.Carroll, J., Minnen, G., Canning, Y., Devlin, S., and Tait,J.
(1998).
Practical simplification of english newspa-per text to assist aphasic readers.
In In Proc.
of AAAI-98 Workshop on Integrating Artificial Intelligence andAssistive Technology, pages 7?10.Carroll, J., Minnen, G., Pearce, D., Canning, Y., Devlin,S., and Tait, J.
(1999).
Simplifying Text for LanguageImpaired Readers.
Proceedings of the 9th Conferenceof the European Chapter of the ACL (EACL ?99).Chen, H.-b., Huang, H.-h., Chen, H.-h., and Tan, C.-t. (2012).
A Simplification-Translation-RestorationFramework for Cross-Domain SMT Applications.Del?eger, L. and Zweigenbaum, P. (2009).
Extractinglay paraphrases of specialized expressions from mono-lingual comparable medical corpora.
Proceedings ofthe 2nd Workshop on Building and Using Compara-ble Corpora: from Parallel to Non-parallel Corpora(BUCC).Devlin, S. and Tait, J.
(1998).
The use of a psycholin-guistic database in the simplification of text for aphasicreaders.
Linguistic Databases, pages 161?173.Devlin, S. and Unthank, G. (2006).
Helping aphasic peo-ple process online information.
Proceedings of the 8thinternational ACM SIGACCESS conference on Com-puters and accessibility.Elhadad, N. (2006).
Comprehending Technical Texts: Predicting and Defining Unfamiliar Terms.
pages239?243.Elhadad, N. and Sutaria, K. (2007).
Mining a lexicon oftechnical terms and lay equivalents.
Proceedings ofthe Workshop on BioNLP 2007: Biological, Transla-tional, and Clinical Language Processing.Feblowitz, D. and Kauchak, D. (2013).
Sentence sim-plification as tree transduction.
Proc.
of the SecondWorkshop on Predicting and Improving Text Readabil-ity for Target Reader Populations, pages 1?10.Fellbaum, C. (1998).
WordNet: An Electronic LexicalDatabase.
Bradford Books.Heafield, K., Pouzyrevsky, I., Clark, J. H., and Koehn,P.
(2013).
Scalable modified Kneser-Ney languagemodel estimation.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Linguis-tics, pages 690?696, Sofia, Bulgaria.Horn, C., Manduca, C., and Kauchak, D. (2014).
Learn-ing a Lexical Simplifier Using Wikipedia.
Proceed-ings of the 52nd Annual Meeting of the Association forComputational Linguistics, pages 458?463.Jauhar, S. and Specia, L. (2012).
UOW-SHEF:SimpLex?lexical simplicity ranking based on contex-tual and psycholinguistic features.
First Joint Confer-ence on Lexical and Computational Semantics, pages477?481.Kauchak, D. (2013).
Improving text simplification lan-guage modeling using unsimplified text data.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics, pages 1537?1546,Sofia, Bulgaria.
Association for Computational Lin-guistics.Kauchak, D. and Barzilay, R. (2006).
Paraphrasing forautomatic evaluation.
Proceedings of the Main Con-ference on Human Language Technology Conferenceof the North American Chapter of the Association ofComputational Linguistics.Keskis?arkk?a, R. (2012).
Automatic text simplification viasynonym replacement.Leroy, G., Endicott, E. J., Kauchak, D., Mouradi, O., andJust, M. (2013).
User evaluation of the effects of a textsimplification algorithm using term familiarity on per-ception, understanding, learning, and information re-tention.
Journal of Medical Internet Research (JMIR).Nunes, B. P., Kawase, R., Siehndel, P., Casanova, M. a.,and Dietze, S. (2013).
As Simple as It Gets - ASentence Simplifier for Different Learning Levels andContexts.
2013 IEEE 13th International Conferenceon Advanced Learning Technologies.Paetzold, G. H. and Specia, L. (2013).
Text simplificationas tree transduction.
In Proceedings of the 9th Brazil-ian Symposium in Information and Human LanguageTechnology.15Rello, L., Baeza-Yates, R., Bott, S., and Saggion, H.(2013).
Simplify or help?
: text simplification strate-gies for people with dyslexia.
Proceedings of the 10thW4A.Rudell, A. P. (1993).
Frequency of word usage and per-ceived word difficulty: Ratings of Kuera and Franciswords.
Behavior Research Methods.Sedding, J. and Kazakov, D. (2004).
Wordnet-based textdocument clustering.
In Proceedings of the 3rd Work-shop on Robust Methods in Analysis of Natural Lan-guage Data.
Association for Computational Linguis-tics.Shardlow, M. (2013a).
A Comparison of Techniques toAutomatically Identify Complex Words.
ACL (StudentResearch Workshop), pages 103?109.Shardlow, M. (2013b).
Proceedings of the Second Work-shop on Predicting and Improving Text Readability forTarget Reader Populations, chapter The CW Corpus:A New Resource for Evaluating the Identification ofComplex Words, pages 69?77.
Association for Com-putational Linguistics.Shardlow, M. (2014).
A Survey of Automated Text Sim-plification.
International Journal of Advanced Com-puter Science and Applications 2014, pages 58?70.Sinha, R. (2012).
UNT-S IMPRANK : Systems for Lex-ical Simplification Ranking.
pages 493?496.Specia, L., Jauhar, S. K., and Mihalcea, R. (2012).Semeval-2012 task 1: English lexical simplification.In Proceedings of the First Joint Conference on Lexicaland Computational Semantics.
Association for Com-putational Linguistics.Watanabe, W. and Junior, A.
(2009).
Facilita: readingassistance for low-literacy readers.
Proceedings ofthe 2010 international cross-disciplinary workshop onWeb accessibility.Yamamoto, T. (2013).
Selecting Proper Lexical Para-phrase for Children.
Proceedings of the Twenty-FifthConference on Computational Linguistics and SpeechProcessing (ROCLING 2013).16
