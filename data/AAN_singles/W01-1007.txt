AbstractCategorization of text in IR hastraditionally focused on topic.
As useof the Internet and e?mail increases,categorization has become a key areaof research as users demand methodsof prioritizing documents.
This workinvestigates text classification byformat style, i.e.
"genre", anddemonstrates, by complementing topicclassification, that it can significantlyimprove retrieval of information.
Thepaper compares use of presentationfeatures to word features, and thecombination thereof, using Na?veBayes, C4.5 and SVM classifiers.Results show use of combined featuresets with SVM yields 92%classification accuracy in sorting sevengenres.1 IntroductionThis paper firstly defines genre, explains therationale for automatic genre classification, andreviews some previously published workrelevant to this problem.
It describes thefeatures chosen to be extracted from documentsfor input to a classification system.
The papernext describes data used, experiments carriedout, and the results obtained.
Finally the paperdiscusses the results and suggests ways for theresearch to progress.1.1 Defining GenreThe genre of a document is defined here as alabel which denotes a set of conventions in theway in which information is presented.
Theseconventions cover both formatting and style oflanguage used.
Examples of genres include"Newswire", "Classified Advertisements", and"Radio Broadcast News Transcript".
Theformat of the text and the style of language usedwithin a genre is usually consistent even thoughthe topics of different documents may varygreatly.
Note that text classifications such as"Sport" or "Politics" are not considered asgenres here since these are broad topic areas.1.2 Why Genre?Many people are experiencing the growth in thevolume of electronic text: Sources include newsservices, online journals, and e?mail.
Fewpeople have time to scan every text source ofpotential interest to them and not all sources areof equal interest to everyone.The continuing expansion of the Internetmakes it increasingly hard to find informationrelevant to the user?s needs.
Search engines gosome way to solving this problem, but often theresults are dominated by hits that do not matchthe user?s requirements.
Many search engines,such as Yahoo, provide a hierarchicalclassification of sites which organize web sitesby the type of information and/or services theyprovide.
However, the hierarchies only cover afraction of the Web and are largely hand built.An automatic method of building site categories,in conjunction with topic identification, wouldspeed the hierarchy construction and allow morefrequent updates.The authors believe that a classifier can betrained to distinguish different documentclasses, or genres, such as advertisements orjokes from news stories, for example.
It can betrained to help identify the proportion of user?relevant texts, which can often be very small.
Ifa user is searching for "stories of the godJupiter" then news articles and scientific paperswould less likely be of interest than classicalfiction.
Note that sorting by genre differs from"Information filtering" as the latter carries outtext selection based on content (Oard, 1994).The belief is that for users who often findirrelevant texts classified as relevant, and thusThe Form is the Substance: Classification of Genres in TextNigel Dewdney Carol VanEss?Dykema Richard MacMillanU.S.
Department of Defense U.S. Department of Defense MITRE Corp.njdewdn@afterlife.ncsc.mil cjvanes@afterlife.ncsc.mil macmilla@mitre.orgmaking finding the relevant texts timeconsuming, a genre filter can function with ahigh degree of accuracy and lessenmisclassification of text.The junk e?mail problem is also well known.The variety in types of e?mail is just as large asthat found in regular mail: from personalcommunications through to unsolicited junkmail.
The user must wade through the e?mailqueue or risk insufficient storage space.
Userscould hand?craft rules to filter junk mail butthese require constant refinement (Sahami,1997).
"Spam" filters are becoming popularthough these just seek to separate unsolicitedand solicited mail.
A genre sorter could serve asa prioritization tool for helping the user toorganize the e?mail in his or her queue beyondjust "solicited" and "unsolicited" with a higherdegree of accuracy.The following example illustrates thepotential of genre identification by showingimprovement in topic based informationretrieval.
Note: This is a single example, sofigures have no significance.Consider looking for information abouthouses currently for sale.
In a 9,705 documentcorpus, detailed later, there are 20 suchdocuments.
(Each document is tagged with oneof seven genres.)
If one attempts to retrievethese documents with the na?ve query "house"one finds 693 documents, 16 of which containthe information required.
If one runs the samequery but specifies the genre "ads", then 35documents are retrieved, 15 of which containrelevant information.
A more specific booleanquery works better than the na?ve keyword(unsurprisingly), but adding the genrespecification still improves the result.Four documents offered houses for sale butdid not mention "house", one document offereda house for sale but was not tagged "ads".The goal, then, is to develop a system thatcan automatically categorize text into genres,where genres are defined as classes whosemembers follow certain conventions of form.1.3 Related WorkThe idea of genre classification is not new.Kessler et.
al.
(?97), for example, proposed genre"facets".
They note that genre does not becomesalient until faced with large hetergoneoussearch spaces, but only report an experimentusing insignificant quantities of data.
Stamatatoset.
al.
(2000) used discriminant analysis on thefrequencies of commonly occurring words.
Theyalso improved results by including frequenciesof eight punctuation marks.
Four identifiedgenres from the Wall Street Journal formed thecorpus, but only 40 samples per genre wereused.
Both works dispensed with the morecomplex features proposed by Karlgren andCutting (1994) which showed promising results.They report 27% error in distinguishing fourgenres in 500 samples from the Brown corpus.Illouz et.
al.
(2000) report successful use ofcoarse level Part?of?Speech features indistinguishing section types from Le Monde.Their work also showed that fine grain Part?of?Speech distinctions did not make for goodfeatures in genre classification.Sahami et.
al.
(1997) report on methods forautomating the construction of e?mail filters,sorting into two categories: "junk" and "non?junk".
Using probabilistic learning methods inconjunction with a differential misclassificationcost to produce filters, they used selected wordsas their feature set.
They also augmented theseby some domain specific features, such asnumber of recipients and time of posting.
Thisyielded improved results, but no results on useof domain specific features alone are presented.Pannu and Sycara (1996) describe a reusableagent that learns a model of the user?s researchinterests for filtering ConferenceAnnouncements and Requests for Proposalsfrom the Web.
They used Information Retrieval(IR) and Artificial Neural Network (ANN)techniques.
The features used were a fixednumber of keywords selected from terms sortedby TF*IDF (term frequency x inverse documentQuery "house" "house" &("sale" | "rent")"house"+ genre "ads"G(ads) "house" &("sale" | "rent")Recall 80% 53% 75% 47%Precision 2% 19% 43% 35%F1 Metric 4 28 55 40Table 1: Comparison of example queries with and without use of genre tag in a marked corpusfrequency).
They report results of 85% and 70%accuracy in classifying a document?s relevanceusing IR techniques and ANN techniquesrespectively.May (1997) describes a system thatautomatically classifies e?mail messages in theHUMANIST electronic discussion group intoone of four categories.
He based theclassification of a message on string matchingusing predefined phrase sets, selected manually,for each message type.
Results withincategories were mixed but overall May reports a45.9% accuracy in tagging.Cohen (1996) reports on two methods forclassification in sorting personal e?mailmessages.
He uses a TF?IDF weightingmethod, and a new method for learning sets of"keyword?spotting rules" based on the RIPPERrule learning algorithm.
Both methods obtainederror rates of below 5%.
However, onlyextraction of one category from the stream isconsidered.2 Document featuresThis work investigates the use of two differentfeature sets: a set based on words (traditional),and a set of features that reflect the way inwhich the text is presented.2.1 Word featuresTraditionally the document features used havebeen the words therein.
Text classification andclustering experiments have focussed on "bag?of?words" techniques: the features used areindividual words usually weighted by somescheme such as TF*IDF.
Feature selectiontechniques, such as thresholding term weights,are used to reduce feature vector size.
TheInformation Gain algorithm is employed for thiswork.Information Gain is frequently employed inmachine learning to reduce computationalcomplexity.
For document classification, itmeasures the number of bits of informationgained, with respect to deciding the class towhich a document belongs, by each word?sfrequency of occurrence in the document(Mitchell, 1997.
Yang, 1997).
Maximum?entropy characterization of information gain [2]is used for this paper as formulated fordocument classification by Yang et.
al.
(1997).Yang?s formulation is appropriate because ittreats features as objects whose values can bemeasured numerically, rather than as a finite setof predefined discrete values:Let {gi}{i=1..m} be the set of target genres, let wbe a word, and let k be a token (i.e., anoccurrence of a word) in the document corpus.Denote by P(w) the probability that a randomlychosen token k is an occurrence of the word w.Let g(k) be the genre of the document in whichk occurs, P(gi) be the probability that a randomlychosen k has g(k) = gi, and P(gi | w) be theconditional probability that a randomly chosen khas g(k) = gi, given that k is an occurrence of theword w. Denote by |S| the number of membersin any set S. An overstrike denotes the negationof a condition.
The information gain of word wis defined to be:IG(w)  ?i=1..m   ( P(gi) log P(gi)+P(?gi) logP(?gi))+ P(w) P(gi | w) log P(gi | w)+ P(?w) P(gi |?w) log P(gi |?w+ P(w) P(?gi | w) log P(?gi | w)+ P(?w) P(?gi |?w) log P(?gi |?w)   eqn 1where:P(w) = (no.
of occurrences of w in corpus) /(total no.
of all word?occurrences in corpus)P(?w) = (no.
occurrences of words other thanw in corpus) / (total no.
of all word?occurrences in corpus)P(gi) = (|{x | g(x) = gi}|) / (total no.
of allword?occurrences in corpus)P(?gi)=(|{x | g(x)  gi}|) / (total no.
of word?occurrences in corpus)Note: The experiments, here, employ theinformation gain algorithm just once over thewhole corpus and apply a threshold so that nodocument would result in a zero vector.
Thisgives an "ideal" feature vector but does notinvestigate the performance of the featureselection algorithm.2.2 Presentation featuresThere are many more features present indocuments than words.
These vary fromlinguistic features such as prevalence ofadjectives, use of tense, and sentencecomplexity, to layout features such as line?spacing, tabulation, and non alpha?numericcharacters.
Space limitations preclude adetailed description of all features used in the setwhich comprises eighty nine such features, butan outline description is given below.The feature extractor developed for this workemploys a ?rough and ready?
Part?of?Speechtagger based on the Porter Stemming Algorithm(Porter, 1980).
The algorithm analyzes wordmorphology and decides, where possible, if theword is a noun, verb, adjective, or adverb.However, it does not reduce the original word toits root.
It is augmented by tables of closed?class words which include words normallyconsidered stop?words.
These tables allow theidentification of pronouns, conjunctions, andarticles.
Simple hand?crafted rules combine thetables and the morphological analysis to aidverb tense identification.
The program,therefore, would not compare well with anymodern Part?of?Speech tagger, but accuracyshould not be too important provided wordtagging is consistent.The tenses identified for verbs are restrictedto past, present and future.
However, theprogram also calculates the proportion oftransitions in verb tense from one identified verbto the next.
For example, if a verb was idenfiedas past tense and the next one idenfied as beingpresent tense, a "past?to?present" change wouldbe recorded.Frequencies of different closed?class wordsets are calculated during the analysis.
By wordsets, here, we mean such things as days of theweek, months of the year, signs of the zodiacetc.
Some of these sets are general, others arespecific to genre.
While a term such as ?leo?might not, by itself, be a particularly gooddiscriminator for a horoscope genre, the fact thatit is an astrological sign and appears with otherterms deemed astrological may well be.The mean and variance of sentence length,and the mean and variance of word length aremeasured.
Sentence length, word length andsyllable estimates are combined to givemeasures of sentence complexity.
Mean wordlength divided by the mean sentence length, andthe Flesch metric (Flesch, 1974) are alsocalculated.The remainder of the presentation feature setcomprise punctuation character usage, the use ofupper and lower case characters, the amount ofwhitespace, and combinations of characters suchas ":?)"
often referred to as "smilies".
Use ofstreams of punctuation marks to act as a sectionbreak in the text are also identified.
Indentation,line?spacing, and tabulation are also measured.All features are normalized over thedocument length and scaled to lie in the range[0,1] as this range is suitable for the SVM?lightand C4.5 classifiers.
If the feature is a countrather than a proportion the inverse of the count(minimum value one) is taken.
Featureextraction results in a vector which is used by aclassifier either in training or in the testing of amodel.3 Evaluation systemBoth word based features and presentationfeatures could be calculated from samples andtheir use compared in classification experiments.Experiments used these feature values with threedifferent classifiers as it was thought thatdifferent classifiers might work better with oneor other of the feature sets.3.1 Data SetJaime Carbonell and Fang Liu of CarnegieMellon University (CMU) supplied the dataused in the experiments.
The corpus iscomprised of seven genres and is summarizedin Table 2.
The genres "Television News" and"Radio News" were predominantly produced bytranscription systems and contain errors.
(Whether these two classes are truly distinctgenres is, perhaps, debatable.
)Genre No.
of SamplesAdvertisement 1091Bulletin Board 998Frequently Asked Questions 1062Message Board 1106Radio News 2000Reuters Newswire 2000Television News 1448TOTAL 9705Table 2: Breakdown of CMU genre corpus3.2 The classifiersThe three different classifier types used were:Na?ve Bayes, C4.5 decision tree, and a SupportVector Machine.
There are several methods foremploying Bayes?
equation; the formulationused here is outlined below.Bayes formula yields the conditionalprobability of a random variable X having valuex, given that another random variable Y hasvalue y.
In adapting Bayes conditionalprobability formula to document classification,this work followed the treatment of Mitchell(1997) pp.174?184.
A document is a series oftokens denoted K(d).
G denotes a set of genres.The probability that the genre of document d,g(d), is gi ?
G, given that d is an arbitrary tokenseries KS, is written:P(g(d)=gi|K(d)=KS) =P(g(d)=gi)P(K(d)=KS|g(d)=gi) /?gi?G[P(g(d)=gi)P(K(d)=KS | g(d)=gi)]   (eqn.2)Classification only requires the most likelygenre.
The denominator in the above equationis constant so only the numerator needs to beconsidered.
Using only those words with highInformation Gain, WS, according to eqn.
1, adocument d?s words W(d)  WS is often null,i.e.
zero probability for all cases.
FollowingMitchell?s smoothing method to prevent this,the most likely genre for a document d is thegenre gm such that m=argmaxi of eqn.
2.
Thenumerator value is calculated from:P(g(d)=gi) = |D(gi)| / |C|                           (eqn.3)P(K(d)=KS | g(d)=gi) = ?w ?
W(d)(|{k ?
K(gi) |W(k)=w ?
WS}| / |K(gi)|) |(w,K(d))|)                (eqn.4)where:C is the document corpus and D(gi) aredocuments tagged as being of genre  gi.The second classifier used was C4.5(Quinlan 1993).
This decision tree classifieruses the Information Gain algorithm to rankfeatures.
A tree is constructed where at eachnode is a decision by which the data are splitinto two groups using the feature with the mostinformation gain among features not yetconsidered.
Leaves are points at which aclassification is made.
The tree is then prunedby replacing a sub?tree with a leaf if theexpected error is reduced.
This alleviates over?fitting and reduces the complexity of the tree.The pruned tree is the resultant classifier for useon new data.
During the classification phase fordocuments under test the rules at each node areapplied to the corresponding document featurevalue to select the next node rule to apply.
Thedocument is classified when a leaf is reached.The heuristics used in the process are tunable,but we chose to use C4.5 with default settings.The third classifier used was the SupportVector Machine (SVM) (Burges, 1998.Christianini, 2000) classifier which has receivedsignificant interest in recent years (Osuna, 1997Joachims, 1998).
The version of SVM used wasSVM?light by Thorsten Joachims (UniversitatDortmund) (SVM?light webpages).
Thisclassifier has many tunable parameters and thevector space may have a function applied to it.Initial experiments had some trouble in gettingmodels using linear vector space to converge inreasonable time.
Using a radial basis functionseemed to alleviate this problem.
In all otherrespects experiments used SVM with defaultsettings.
SVM?light builds binary models.
Inthe case where multiple classes are present, as inour experiments, a model must be produced foreach class.
The classifier outputs real valuesrather than binary decisions.
Each item iscompared against each model and classifiedaccording to a winner?takes?all rule.4 ExperimentsThe experiments detailed here were run underthe ten?fold cross?validation method.
Thissplits the data up into training and test sets in a90%/10% proportion.
Experiments are repeatedten times with the split being made in a round?robin fashion.
In this way all of the data is usedboth in training and testing but not within thesame cycle.
Recorded here are Recall, Precisionand F1 where recall is the number of correctclassifications divided by number of documents,precision is the number of correct classificationsdivided by the number of classifications made,and F1=2 * (Precision * Recall) / (Precision +Recall).
Note that in experiments where aclassification is required (i.e.
No "unknown"class) Recall, Precision and F1 are all equal.323 word features were selected by analysingthe Information Gain over the whole corpus.
Theselected words, therefore, are in some sense an"ideal" feature set.
The length of the vector waschosen such that no zero vector would result.Word counts were measured in each document.When using Na?ve Bayes these counts weremultiplied by the log probabilities calculated.1 Duplicate sub?vectors across class caused problems using SVM.
Doubling the sub?vector length for the optimisationphase (using the ?q parameter in SMV?light) increased training time but avoided the problem.2 The version of SVM light used consistently failed to converge when encountering duplicate vectors across classes.This situation easily arises if documents containing no words present in the feature set are present.For SVM and C4.5 logs of the counts weretaken and divided by logs of the total wordcount in the corresponding document.
(Smoothing carried out by adding 1 to bothnumerator and denominator prior to takinglogs.)
A misclassification matrix example isshown in Table 3.
The true genre tag isindicated by row and the classifier?s decision islisted by column.
The number of correctclassifications appear on the diagonal of thetable, and the numbers of misclassifications areshown in the remaining column cells.SVM and C4.5 experimnets usedpresentation feature values directly.
Using theNa?ve Bayes classifier requires that feature valueranges be defined because Bayesian classifiersusually work with features that are either presentor absent.
However, Mitchell?s formulationgeneralizes to real?valued functions: Theconditional probability that the genre ofdocument d is gi, given that feature f is in d,v(f,d), is a real value V, given by:V = v(f,d)[?d ?
D(gi) v(f,d)] / |D(gi)|              (eqn.5)where:D(gi) is the subset of the training corpus taggedas being of genre gi.Table 4 shows an example misclassificationmatrix resulting from using presentation featurevectors.Misclassifications made using wordfrequency features were different from thosemade using presentation features.
The questionarises as to whether combining the two featuresets would show improvement.
To test this, eachdocument had its word frequency andpresentation feature vectors combined.
AnGenre Ads Bulletin F.A.Q.
Message Radio Reuters TV UnknownAds 823 47 23 6 6 5 1 179Bulletin Board 85 715 41 1 10 1 137F.A.Q.
13 36 756 14 4 3 234Message Board 7 1 6 1045 2 4 1 34Radio 4 4 3 1640 5 277 67Reuters 4 1 1 1 5 1966 22TV 272 1147 21Table 4: Misclassification  Matrix using SVM with presentation features from CMU datasetGenre Ads Bulletin F.A.Q.
Message Radio Reuters TV UnknownAds 830 18 7 7 1 ?
1 226Bulletin Board 49 608 8 ?
2 ?
?
323F.A.Q.
5 3 836 7 6 ?
?
203Message Board 2 1 3 996 4 2 ?
92Radio ?
2 ?
?
1624 ?
219 155Reuters 1 ?
?
?
?
1952 ?
47TV ?
?
?
?
87 ?
1294 59Table 5: Misclassification Matrix using SVM with presentation + word features from CMU datasetGenre Ads Bulletin F.A.Q.
Message Radio Reuters TV UnknownAds 911 50 13 32 6 ?
5 73Bulletin Board 78 804 6 24 4 1 14 59F.A.Q.
13 9 907 34 7 ?
14 76Message Board 20 17 27 875 25 3 25 108Radio 10 7 8 22 1543 19 205 186Reuters ?
2 ?
3 22 1934 18 31TV 6 10 12 23 234 10 952 193Table 3: Misclassification Matrix for SVM using 323 word frequencies from CMU datasetexample misclassification matrix resulting fromusing combined feature set vectors is shown inTable 5.4.1 ResultsIn the first set of experiments each classifier?s"best guess" at a document?s genre was taken asthe class.
The results are shown in Table 6.Each cell shows average recall with an errormargin quoted at one standard deviation.The C4.5 classifier is not able to beconfigured to allow an unknown classification.It is possible for Na?ve Bayes and SVM to be soconfigured, since their results give a numericvalue for the most likely classification.
Adocument is deemed "unknown" if it scores lessthan 0.5 using Na?ve Bayes, or negatively usingSVM.
Recall and precision figures can becalculated in using this scheme.
The results areshown in Table 7.
(Mean value with onestandard deviation error margin.
)4.2 DiscussionUse of the presentation feature set yields asignificant advatage over use of wordfrequencies except when using Na?ve Bayes.This shows that presentations features alone,when used with a suitable classifier, arepertinent to classifying by genre without theneed for word features often used.
Theadvantage in combining feature values is seenconsistently over the three classifiers and is trueeven for the Na?ve Bayes classifier which doespoorly with presentation features alone.Applying a threshold to the classifier outputscore, to allow an "unknown" class, increasesprecision at some expense in recall as should beexpected.When the Na?ve Bayes classifier used the 89presentation features, the results wereconsiderably less accurate than when it used the323 word features.
It seems likely that at leastthree factors were involved.
Firstly, Na?ve Bayesassumes feature independence.
While this is nottrue for words, it has been the experience of theIR community that word dependencies are smallenough for documents to be treated as "bags?of?words".
Some features in the presentationfeature set, however, are far from independent;e.g., the proportions of parts of speech identifiedare explicitly linked.
Secondly, the Bayesianformulation in Section 3.2 has an implicitassumption of monotonicity.
If word w occursin document d a total of n=|(w,K(d))| times, thequantity |(w,K(d))|, being an exponent in theformula, gives word w more weight when itoccurs more times.
Let genre gj be the genremost strongly associated with w in the trainingcorpus.
The more times w occurs in d, the morelikely the Na?ve Bayes classifier is to classify das genre gj; and this effect is monotonic.
Butmonotonicity does not always hold, even withwords; and it fails to hold even more often in thepresentation features.
For example, in ourtraining data, the word "said" occurs seldom inthe genres: Message?board, Bulletin?board, andClassifier  Word frequency features Presentation features Combined featuresNa?ve Bayes 77.8% +/?
1.6% 64.0% +/?
1.2% 83.1% +/?
1.5%C4.5 Decision Tree 79.8% +/?
1.1% 85.3% +/?
1.0% 87.8% +/?
1.1%SVM 85.4% +/?
0.9% 87.1% +/?
1.0% 92.1% +/?
0.8%Table 6: Average recall in 10?fold cross validation genre identification experiment; forced decisionClassifier  Word frequency ftrs.
Presentation ftrs.
Combined ftrs.Recall Na?ve Bayes 76.7% +/?
1.4% 33.9% +/?
1.4% 82.4% +/?
1.5%SVM 81.8% +/?
0.6% 83.6% +/?
0.9% 84.0% +/?
2.5%Precision Na?ve Bayes 80.4% +/?
1.3% 78.8% +/?
1.4% 84.1% +/?
1.4%SVM 88.4% +/?
1.3% 90.1% +/?
2.4% 94.9% +/?
0.7%F1 Metric Na?ve Bayes 78.5% +/?
1.4% 47.4% +/?
1.5% 83.2% +/?
1.4%SVM 85.0% +/?
0.4% 86.7% +/?
0.9% 89.1% +/?
1.5%Table 7: Mean recall in 10?fold cross validation experiment; positive identification threshold appliedFAQ.
It occurs very frequently in Reuters?newswire, and with intermediate frequency inRadio and TV transcripts.
Thirdly, the word?feature?counts are integer?valued, while thepresentation features have continuous values.The effect of assigning ranges of continuousvalues to discrete value bins, in the use ofpresentation features with Na?ve Bayes, is notyet clear.
There has yet to be performed an erroranalysis to quantify the reason(s) for theobserved accuracy decrease.The large increase in precision with only asmall decrease in recall when allowing an"unknown" class suggests that a more relaxedthreshold could be applied to SVM or Na?veBayes output.5 ConclusionExperiments have demonstrated success increating genre models for automatic recognitionof multiple genres using a corpus of sufficientsize to draw some conclusions.
Theseexperiments have shown that linguistic andformat features alone can be used successfully insorting documents into different genres, and thatperformance is at least as good as use of wordbased features providing a suitable classifier isused.
Rather than presentation features aidingdiscimination by words it seems selected wordfeatures assist discrimination by presentationfeatures as the best results are obtained using acombination.
The results suggest that automaticgenre identification could be used inapplications, such as Information Retrieval, forbetter performance.
For example, this techniquecould improve IR performance against taskssuch as the TREC web track (TREC webpages).Future work will investigate the effects ofhaving an increased number of genres andincreased corpus size.6 ReferencesBiber, D. (1995) "Variation Across Speech and Writing",Cambridge University Press, New York.Berger, A., S.Della Pietra, V.Della Pietra, ?
(1996) "AMaximum Entropy Approach to Natural LanguageProcessing", Computational Linguistics, vol.
22 No.1, pp39?71.Burges, C. (1998) "A Tutorial on Support VectorMachines for Pattern Recognition", Data Mining andKnowledge Discovery, No.
2.Cherney, L.  (1999) "Conversation and Community:  Chatin a Virtual World",  CSLI Publications, StanfordUniversity.Cohen, W.W.  (1996) "Learning Rules that Classify E?Mail", AAAI Spring  Symposium  on MachineLearning in Information Access.Cristianini, N., J.Shaw?Taylor, (2000) "Introduction toSupport Vector Machines", Cambridge UniversityPress, .Flesch, R. (1974) "The Art of Readable Writing", Harperand Row, New York.Illouz, G., B.Habert, H.Folch, S.Heiden, S.Fleury,P.Lafon, S.Prevost (2000) "TyPTex: Genric featuresfor Text Profiler", Proc.
RIAO 2000, Paris, France.Joachims, T. (1998) "Text Categorization with SupportVector Machines: Learning with Many RelevantFeatures", European Conference on MachineLearning.Karlgren, J., and D Cutting (1994) "Recognizing TextGenres with Simple Metrics Using DiscriminantAnalysis".
In Proc.
of the 15 th International conferenceon Computational Linguistings (COLING?94)Kessler, B., G Nunberg, H. Schutze (1997) "AutomaticDetection of Text Genre".
In Proc.
if 35 th AnnualMeeting of the ACL and 8 th Conference of ECACL.May, A.
(1997) "Automatic Classification of E?MailMessages by Message Type", JASIS, vol 48 No.1,pp32?39.Mitchell, T.M.
(1997)  "Machine Learning",  McGraw?Hill, Boston, Massachusetts.Oard, D.W., N. De Claris, B.J.Dorr, C.Faloutsos,  (1994)"On Automatic Filtering of Multilingual Texts",Proceedings of IEEE International Conference onSytems, Man, and Cybernetics, pp1645?1650, VsanAntonio Texas.Osuna, E. R.Freund, and F.Girosi, (1997) "Trainingsupport vector machines: an application to facerecognition", IEEE Conference on Computer Visionand Pattern Recognition.Pannu, Anandeep, Sycara.
(1996) "Learning TextFiltering Preferences", Symposium on MachineLearning and Information Processing, AAAISymposium Series, Stanford, Ca., March?96.Porter, M.E.
(1980) "An Algorithm for Suffix Stripping",Program Vol.
14, No.
3, pp318?327, July?80.Quinlan, J.R. (1993) "C4.5: Programs for MachineLearning", Morgan Kaufmann, California.Sahami, M., S.Dumais, D.Heckerman, E.Horvitz, (1998)"A Bayesian Approach to Filtering Junk E?Mail"AAAI Workshop Technical Report WS?98?05.Stamatatos, E., N. Fakotakis, and G. Kokkinakis (2000)"Text Genre Detection Using Common WordFrequencies".
In the Proc.
of the 18 th InternationalConfernece on Computational Linguistics(COLING2000)SVM?light webpages URL:http://ais.gmd.de/~thorsten/svm_light/TREC webpages.
URL: http://trec.nist.govYang, Y., J.Pedersen, (1997) "A Comparative Study onFeature Selection in Text Categorization",Proceedings of the 1997 International Conference onMachine Learning (ICML), pp.412?420.
