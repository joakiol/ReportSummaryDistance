Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1264?1274,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsHUME: Human UCCA-Based Evaluation of Machine TranslationAlexandra Birch1?, Omri Abend2*, Ondr?ej Bojar3*, Barry Haddow1*1School of Informatics, University of Edinburgh2School of Computer Science and Engineering, Hebrew University of Jerusalem3 Charles University in Prague, Faculty of Mathematics and Physicsa.birch@ed.ac.uk, oabend@cs.huji.ac.ilbojar@ufal.mff.cuni.cz, bhaddow@inf.ed.ac.ukAbstractHuman evaluation of machine translation nor-mally uses sentence-level measures such asrelative ranking or adequacy scales.
However,these provide no insight into possible errors,and do not scale well with sentence length.We argue for a semantics-based evaluation,which captures what meaning components areretained in the MT output, thus providing amore fine-grained analysis of translation qual-ity, and enabling the construction and tuningof semantics-based MT.
We present a novelhuman semantic evaluation measure, HumanUCCA-based MT Evaluation (HUME), build-ing on the UCCA semantic representationscheme.
HUME covers a wider range of se-mantic phenomena than previous methods anddoes not rely on semantic annotation of thepotentially garbled MT output.
We experi-ment with four language pairs, demonstratingHUME?s broad applicability, and report goodinter-annotator agreement rates and correla-tion with human adequacy scores.1 IntroductionHuman judgement should be the ultimate test of thequality of an MT system.
Nevertheless, commonmeasures for human MT evaluation, such as ade-quacy and fluency judgements or the relative rank-ing of possible translations, are problematic in twoways.
First, as the quality of translation is multi-faceted, it is difficult to quantify the quality of theentire sentence in a single number.
This is indeed?
All authors contributed equally to this work.reflected in the diminishing inter-annotator agree-ment (IAA) rates of human ranking measures withthe sentence length (Bojar et al, 2011).
Second, asentence-level quality score does not indicate whatparts of the sentence are badly translated, and socannot inform developers in repairing these errors.These problems are partially addressed by mea-sures that decompose over parts of the evaluatedtranslation, often words or n-grams (see ?2 for abrief survey of previous work).
A promising lineof research decomposes metrics over semanticallydefined units, quantifying the similarity of the out-put and the reference in terms of their verb argu-ment structure; the most notable of these measuresis HMEANT (Lo and Wu, 2011).We propose the HUME metric, a human evalua-tion measure that decomposes over UCCA semanticunits.
UCCA (Abend and Rappoport, 2013) is anappealing candidate for semantic analysis, due to itscross-linguistic applicability, support for rapid anno-tation, and coverage of many fundamental semanticphenomena, such as verbal, nominal and adjectivalargument structures and their inter-relations.HUME operates by aggregating human assess-ments of the translation quality of individual seman-tic units in the source sentence.
We are thus avoidingthe semantic annotation of machine-generated text,which is often garbled or semantically unclear.
Thisalso allows the re-use of the source semantic anno-tation for measuring the quality of different transla-tions of the same source sentence and avoids relyingon reference translations, which have been shown tobias annotators (Fomicheva and Specia, 2016).After a brief review (?2), we describe HUME in1264detail (?3).
Our experiments with four languagepairs: English to Czech, German, Polish and Roma-nian (?4) document HUME?s inter-annotator agree-ment and efficiency (time of annotation).
We furtherempirically compare HUME with direct assessmentof human adequacy ratings (?5), and conclude bydiscussing the differences with HMEANT (?6).2 BackgroundMT Evaluation.
Human evaluation is generallydone by ranking the outputs of multiple systemse.g., in the WMT tasks (Bojar et al, 2015), or byassigning adequacy/fluency scores to each transla-tion, a procedure recently improved by Graham etal.
(2015b) under the title Direct Assessment.
Weuse this latter method to compare and contrast withHUME later in the paper.
HTER (Snover et al,2006) is another widely used human evaluation met-ric which uses edit distance metrics to compare atranslation and its human post-edition.
HTER suf-fers from the problem that small edits in the transla-tion could in fact be serious flaws in accuracy, e.g.,deleting a negation.
Some manual measures ask an-notators to explicitly mark errors, but this has beenfound to have even lower agreement than ranking(Lommel et al, 2014).However, while providing the gold standard forMT evaluation, human evaluation is not a scalablesolution.
Scalability is addressed by employing au-tomatic and semi-automatic approximations of hu-man judgements.
Commonly, such scores decom-pose over the sub-parts of the translation, and quan-tify how many of these sub-parts appear in a manu-ally created reference translation.
This decomposi-tion allows system developers to localize the errors.The most commonly used measures decompose overn-grams or individual words, e.g., BLEU (Papineniet al, 2002), NIST (Doddington, 2002) and ME-TEOR (Banerjee and Lavie, 2005).
Another com-mon approach is to determine the similarity betweenthe reference and translation in terms of string edits(Snover et al, 2006).
While these measures stimu-lated much progress in MT research by allowing theevaluation of massive-scale experiments, the focuson words and n-grams does not provide a good esti-mate of semantic correctness, and may favour shal-low string-based MT models.L Linker A ParticipantH Parallel Scene R RelaterP Process C CentreFigure 1: Sample UCCA annotation.
Leaves correspondto words and nodes to units.
The dashed edge indicatesthat ?Tom?
is also a participant in the ?moved to Amer-ica?
Scene.
Edge labels mark UCCA categories.In order to address this shortcoming, more recentwork quantified the similarity of the reference andtranslation in terms of their structure.
Liu and Gildea(2005) took a syntactic approach, using dependencygrammar, and Owczarzak et al (2007) took a sim-ilar approach using Lexical Functional Grammarstructures.
Gim?nez and M?rquez (2007) proposedto combine multiple types of information, captur-ing the overlap between the translation and refer-ence in terms of their semantic (predicate-argumentstructures), lexical and morphosyntactic features.Mach?c?ek and Bojar (2015) divided the source sen-tences into shorter segments, defined using a phrasestructure parse, and applied human ranking to theresulting segments.Perhaps the most notable attempt at semanticMT evaluation is MEANT and its human variantHMEANT (Lo and Wu, 2011), which quantifies thesimilarity between the reference and translation interms of the overlap in their verbal argument struc-tures and associated semantic roles.
We discuss thedifferences between HMEANT and HUME in ?6.Semantic Representation.
UCCA (UniversalConceptual Cognitive Annotation) (Abend andRappoport, 2013) is a cross-linguistically applicablescheme for semantic annotation.
Formally, anUCCA structure is a directed acyclic graph (DAG),whose leaves correspond to the words of the text.The graph?s nodes, called units, are either terminalsor several elements jointly viewed as a single entityaccording to some semantic or cognitive considera-tion.
Edges bear a category, indicating the role ofthe sub-unit in the structure the unit represents.UCCA?s basic inventory of distinctions (its foun-dational layer) focuses on argument structures (ad-1265jectival, nominal, verbal and others) and relationsbetween them.
The most basic notion is the Scene,which describes a movement, an action or a statewhich persists in time.
Each Scene contains onemain relation and zero or more participants.
For ex-ample, the sentence ?After graduation, Tom movedto America?
contains two Scenes, whose main rela-tions are ?graduation?
and ?moved?.
The participant?Tom?
is a part of both Scenes, while ?America?only of the latter (Figure 1).
Further categories ac-count for inter-scene relations and the sub-structuresof participants and relations.The use of UCCA for semantic MT evaluationhas several motivations.
First, UCCA?s foundationallayer can be annotated by non-experts after a shorttraining (Abend and Rappoport, 2013; Marinotti,2014).
Second, UCCA is cross-linguistically appli-cable, seeking to represent what is shared betweenlanguages by building on linguistic typological the-ory (Dixon, 2010b; Dixon, 2010a; Dixon, 2012).
Itscross-linguistic applicability has so far been tested inannotations of English, French, German and Czech.Third, the scheme has been shown to be stable acrosstranslations: UCCA annotations of translated textusually contain the same set of relations (Sulem etal., 2015), indicating that UCCA reflects a layer ofrepresentation that in a correct translation is mostlyshared between the translation and the source.The Abstract Meaning Representation (AMR)(Banarescu et al, 2013) shares UCCA?s motivationfor defining a more complete semantic annotation.However, using AMR is not optimal for defining adecomposition of a sentence into semantic units asit does not anchor its semantic symbols in the text,and thus does not provide clear decomposition of thesentence into sub-spans.
Also, AMR is more fine-grained than UCCA and consequently harder to an-notate.
Other approaches represent semantic struc-tures as bi-lexical dependencies (Sgall et al, 1986;Hajic?
et al, 2012; Oepen and L?nning, 2006), whichare indeed anchored in the text, but are less suitablefor MT evaluation as they require linguistic exper-tise for their annotation.3 The HUME Measure3.1 Annotation ProcedureThis section summarises the manual annotationprocedure used to compute the HUME measure.
Wedenote the source sentence as s and the translationas t. The procedure involves two manual steps: (1)UCCA-annotating s, (2) HUME-annotation: humanjudgements as to the translation quality of each se-mantic unit of s relative to t, where units are definedaccording to the UCCA annotation.
UCCA annota-tion is performed once for every source sentence, ir-respective of the number of its translations we wishto evaluate, and requires proficiency in the sourcelanguage only.
HUME annotation requires the em-ployment of bilingual annotators.1UCCA Annotation.
We begin by creating UCCAannotations for the source sentence, following theUCCA guidelines.2 A UCCA annotation for a sen-tence s is a labeled DAG G, whose leaves are thewords of s. For every node in G, we define its yieldto be its leaf descendants.
The semantic units for saccording to G are the yields of nodes in G.Translation Evaluation.
HUME annotation isdone by traversing the semantic units of the sourcesentence, which correspond to the arguments and re-lations expressed in the text, and marking the ex-tent to which they have been correctly translated.HUME aggregates the judgements of the users intoa composite score, which reflects the overall extentto which the semantic content of s is preserved in t.Annotation of the semantic units requires first de-ciding whether a unit is structural, i.e., has meaning-bearing sub-units in the target language, or atomic.In most cases, atomic units correspond to individualwords, but they may also correspond to multi-wordexpressions that translate as one unit.
For instance,the expression ?took a shower?
is translated into theGerman ?duschte?, while its individual words do notcorrespond to any sub-part of the German transla-tion, motivating the labeling the entire expression asan atomic node.
When a multi-word unit is labeled1Where bilingual annotators are not available, the evaluationcould be based on the UCCA structure for the reference trans-lation.
See discussion in ?6.2All UCCA-related resources can be found here: http://www.cs.huji.ac.il/~oabend/ucca.html1266Figure 2: HUME annotation of an UCCA tree with aword-aligned example translation shown below.
Atomicunits are labelled using traffic lights (Red, Orange,Green) and structural units are marked A or B.as atomic, its sub-units?
annotations are ignored inthe evaluation.Atomic units can be labelled as ?Green?
(G, cor-rect), ?Orange?
(O, partially correct) and ?Red?
(R,incorrect).
Green means that the meaning of theword or phrase has been largely preserved.
Orangemeans that the essential meaning of the unit has beenpreserved, but some part of the translation is wrong.This is often be due to the translated word having thewrong inflection, in a way that impacts little on theunderstandability of the sentence.
Red means thatthe essential meaning of the unit has not been cap-tured.Structural units have sub-units (children in theUCCA graph), which are themselves atomic orstructural.
Structural units are labeled as ?Adequate?
(A) or ?Bad?
(B), meaning that the relation betweenthe sub-units went wrong3.
We will use the exam-ple ?man bites dog?
to illustrate typical examples ofwhy a structural node should be labelled as ?Bad?
:incorrect ordering (?dog bites man?
), deletion (?manbites?)
and insertion (?man bites biscuit dog?
).HUME labels reflect adequacy, rather than flu-ency judgements.
Specifically, annotators are in-structed to label a unit as Adequate if its translationis understandable and preserves the meaning of thesource unit, even if its fluency is impaired.Figure 2 presents an example of a HUME annota-tion, where the translation is in English for ease ofcomprehension.
When evaluating ?to America?
theannotator looks at the translation and sees the word?stateside?.
This word captures the whole phrase3Three labels are used with atomic units, as opposed to twolabels with structural units, as atomic units are more susceptibleto slight errors.and so we mark this non-leaf node with an atomic la-bel.
Here we choose Orange since it approximatelycaptures the meaning in this context.
The ability tomark non-leaves with atomic labels allows the an-notator to account for translations which only cor-respond at the phrase level.
Another feature high-lighted in this example is that by separating struc-tural and atomic units, we are able to define wherean error occurs, and localise the error to its point oforigin.
The linker ?After?
is translated incorrectly as?by?
which changes the meaning of the entire sen-tence.
This error is captured at the atomic level, andit is labelled Red.
The sentence still contains twoScenes and a Linker and therefore we mark the rootnode as structurally correct, Adequate.3.2 Composite ScoreWe proceed to detailing how judgements on thesemantic units of the source are aggregated into acomposite score.
We start by taking a very sim-ple approach and compute an accuracy score.
LetGreen(s, t), Adequate(s, t) and Orange(s, t) be thenumber of Green, Adequate and Orange units, re-spectively.
Let Units(s) be the number of unitsmarked with any of the labels.
Then HUME?s com-posite score is:HUME(s, t) = Green(s, t) + Adequate(s, t) + 0.5 ?
Orange(s, t)Units(s)3.3 Annotation InterfaceFigure 3 shows the HUME annotation interface4.One source sentence and one translation are pre-sented at a time.
The user is asked to select a labelfor each source semantic unit, by clicking the ?A?,?B?, Green, Orange, or Red buttons to the right ofthe unit?s box.
Units with multiple parents (as with?Tom?
in Figure 2) are displayed twice, once undereach of their parents, but are only annotatable in oneof their instances, to avoid double counting.The interface presents, for each unit, the transla-tion segment aligned with it.
This allows the user,especially in long sentences, to focus her attentionon the parts that are most likely to be relevant for herjudgement.
As the alignments are automatically de-rived, and therefore noisy, the annotator is instructedto treat the aligned text is a cue, but to ignore thealignment if it is misleading, and instead make a4A demo of HUME can be found in www.cs.huji.ac.il/~oabend/hume_demo.html1267Figure 3: The HUME annotation tool.
The top orange box contains the translation.
The source sentence is directlybelow it, followed by the tree of the source semantic units.
Alignments between the source and translation are in italicsand unaligned intervening words are in red (see text).judgement according to the full translation.
Con-cretely, let s be a source sentence, t a translation, andA ?
2s?2t a many-to-many word alignment.
If u isa semantic unit in s, whose yield is yld(u), we definethe aligned text in t to be ?(xs,xt)?A?xs?yld(u)6=?
xt.Where the aligned text is discontinuous in t,words between the left and right boundaries whichare not contained in it (intervening words) are pre-sented in a smaller red font.
Intervening words arelikely to change the meaning of the translation ofu, and thus should be attended to when consideringwhether the translation is correct or not.For example, in Figure 3, ?ongoing pregnancy?is translated to ?Schwangerschaft ... laufenden?
(lit.
?pregnancy ...
ongoing?).
This alone seems accept-able but the interleaving words in red notify the an-notator to check the whole translation, in which themeaning of the expression is not preserved5.
Theannotator should thus mark this structural node asBad.4 ExperimentsIn order to validate the HUME metric, we ran an an-notation experiment with one source language (En-glish), and four target languages (Czech, German,Polish and Romanian), using text from the publichealth domain.
Semantically accurate translation isparamount in this domain, which makes it particu-larly suitable for semantic MT evaluation.
HUME isevaluated in terms of its consistency (inter-annotator5The interleaving words are ?...
und beide berichtetberichteten ...?
(lit.
?...
and both report reported ...?
), whichdoesn?t form any coherent relation with the rest of the sentence.agreement), efficiency (time of annotation) and va-lidity (by comparing it with crowd-sourced ade-quacy judgements).4.1 Datasets and Translation SystemsFor each of the four language pairs under con-sideration we built phrase-based MT systems usingMoses (Koehn et al, 2007).
These were trainedon large parallel data sets extracted from OPUS(Tiedemann, 2009), and the data sets released forthe WMT14 medical translation task (Bojar et al,2014), giving between 45 and 85 million sentencesof training data, depending on the language pair.These translation systems were used to translatetexts derived from both NHS 246 and Cochrane7 intothe four languages.
NHS 24 is a public body provid-ing healthcare and health-service related informa-tion in Scotland; Cochrane is an international NGOwhich provides independent systematic reviews onhealth-related research.
NHS 24 texts come from the?Health A-Z?
section in the NHS Inform website,and Cochrane texts come from their plain languagesummaries and abstracts.4.2 HUME Annotation StatisticsThe source sentences are all in English, and theirUCCA annotation was performed by four computa-tional linguists and one linguist.
For the annotationof the MT output, we recruited two annotators foreach of German, Romanian and Polish and one mainannotator for Czech.
For computing Czech IAA,several further annotators worked on a small numberof sentences each.
We treat these further annotators6http://www.nhs24.com/7http://www.cochrane.org/1268cs de pl ro#Sentences Annot.
1 324 339 351 230Annot.
2 205 104 340 337#Units Annot.
1 8794 9253 9557 6152Annot.
2 5553 2906 9303 9228Table 1: HUME-annotated #sentences and #units.cs de pl roAnnot.
1 255 140 138 96Annot.
2 ?
162 229 207Table 2: Median annotation times per sentence, in sec-onds.
?
: no timing information is available, as this was acollection of annotators, working in parallel.as one annotator, resulting in two annotators for eachlanguage pair.
The annotators were all native speak-ers of the respective target languages and fluent inEnglish.
They completed a three hour on-line train-ing session which included a description of UCCAand the HUME task, followed by walking through afew examples.Table 1 shows the total number of sentences andunits annotated by each annotator.
Not all units in allsentences were annotated, often due to the annotatoraccidentally missing a node.Efficiency.
We estimate the annotation time us-ing the timestamps provided by the annotation tool,which are recorded whenever an annotated sentenceis submitted.
Annotators are not able to re-open asentence once submitted.
To estimate the annota-tion time, we compute the time difference betweensuccessive sentences, and discard outlying times,assuming annotation was not continuous in thesecases.
From inspection of histograms of annotationtimes, we set the upper threshold at 500 seconds.Median annotation times are presented in Table 2,indicating that the annotation of a sentence takesaround 2?4 minutes, with some variation betweenannotators.Inter-Annotator Agreement.
In order to assessthe consistency of the annotation, we measure theInter-Annotator Agreement (IAA) using Cohen?sKappa on the multiply-annotated units.
Table 3 re-ports the number of units which have two annota-tions from different annotators and the correspond-ing Kappas.
We report the overall Kappa, as well asseparate Kappas on atomic units (annotated as Red,Orange or Green) and structural units (annotated ascs de pl roSentences 181 102 334 217All units 4686 2793 8384 5604Kappa 0.64 0.61 0.58 0.69Atomic units 2982 1724 5386 3570Kappa 0.54 0.29 0.54 0.50Structural units 1602 1040 2655 1989Kappa 0.31 0.44 0.33 0.58Table 3: IAA for the multiply-annotated units, measuredby Cohen?s Kappa.
(a) English-Czech (b) English-German(c) English-Polish (d) English-RomanianFigure 4: Confusion matrices for each language pair.Adequate or Bad).
As expected and confirmed byconfusion matrices in Figure 4, there is generally lit-tle confusion between the two types of units.
Thisresults in the Kappa for all units being considerablyhigher than the Kappa over the atomic units or struc-tural units, where there is more internal confusion.To assess HUME reliability for long sentences,we binned the sentences according to length andmeasured Kappa on each bin (Figure 5).
We see nodiscernible reduction of IAA with sentence length.Table 3 also shows that the overall IAA is similarfor all languages, presenting good agreement (0.6?0.7).
However, there are differences observed whenwe break down by node type.
Specifically, we see acontrast between Czech and Polish, where the IAAis higher for atomic than for structural units, andGerman and Romanian, where the reverse is true.We also observe low IAA (around 0.3) in the casesof German atomic units, and Polish and Czech struc-tural units.Looking more closely at the areas of disagree-1269(a) English-Czech (b) English-German(c) English-Polish (d) English-RomanianFigure 5: Kappa versus sentence length for structural andatomic units.
(Node counts in bins on top of each bar.
)ment, we see that for the Polish structural units, theproportion of As was quite different between the twoannotators (53% vs. 71%), whereas for other lan-guages the annotators agree in the proportions.
Webelieve that this was because one of the Polish an-notators did not fully understand the guidelines forstructural units, and percolated errors up the tree,creating more Bs.
For German atomic and Czechstructural units, where Kappa is also around 0.3, theproportion of such units being marked as ?correct?
isrelatively high, meaning that the class distribution ismore skewed, so the expected agreement used in theKappa calculation is high, lowering Kappa.
Finallywe note some evidence of domain-specific disagree-ments, for instance the German MT system normallytranslated ?review?
(as in ?systematic review?
?
afrequent term in the Cochrane texts) as ?
?berpr?-fung?, which one annotator marked correct, and theother (a Cochrane employee) as incorrect.5 Comparison with Direct AssessmentRecent research (Graham et al, 2015b; Graham etal., 2015a; Graham, 2015) has proposed a new ap-proach for collecting accuracy ratings, direct assess-ment (DA).
Statistical interpretation of a large num-ber of crowd-sourced adequacy judgements for eachcandidate translation on a fine-grained scale of 0 to100 results in reliable aggregate scores, that corre-late very strongly with one another.0.0 0.2 0.4 0.6 0.8 1.0HUME scores2.52.01.51.00.50.00.51.01.5DAscores(a) English-German0.0 0.2 0.4 0.6 0.8 1.0HUME scores2.01.51.00.50.00.51.01.52.0DAscores(b) English-RomanianFigure 6: HUME vs DA scores.
DA score have been stan-dardised for each crowdsourcing annotator and averagedacross exactly 10 annotators.
HUME scores are averagedwhere there were two annotations.We attempted to follow Graham et al (2015b) butstruggled to get enough crowd-sourced judgementsfor our target languages.
We ended up with 10 ade-quacy judgements on most of the HUME annotatedtranslations for German and Romanian but insuffi-cient data for Czech and Polish.
We see this as asevere practical limitation of DA.Figure 6 plots the HUME score for each sentenceagainst its DA score.
HUME and Direct Assessmentscores correlate reasonably well.
The Pearson corre-lation for en-ro (en-de) is 0.70 (0.58), or 0.78 (0.74)if only doubly HUME-annotated points are consid-ered.
This confirms that HUME is consistent withan accepted human evaluation method, despite theirconceptual differences.
While DA is a valuable tool,HUME has two advantages: it returns fine-grained1270allatomicstructP and S H A C E L 0.10.00.10.20.30.40.50.60.70.8CorrelationGermanRomanianFigure 7: Pearson correlation of HUME vs. DA scoresfor en-ro and en-de.
Each bar represents a correlationbetween DA and an aggregate HUME score based ona sub-set of the units (#nodes for the en-de/en-ro set-ting in brackets): all units (?all?, 8624/10885), atomic(?atomic?, 5417/6888) and structural units (?struct?,3207/3997), and units by UCCA categories: Scenemain relations (i.e, Process and State units; ?P and S?,954/1178), Parallel Scenes (?H?, 656/784), Participants(?A?, 1348/1746), Centres (?C?, 1904/2474), elaborators(?E?, 1608/2031) and linkers (?L?, 261/315).semantic information about the quality of transla-tions and it only requires very few annotators.
Di-rect assessment returns a single opaque score, and(as also noted by Graham et al) requires a largecrowd which may not be available or reliable.Figure 7 presents an analysis of HUME?s corre-lations with DA by HUME unit type, an analysisenabled by HUME?s semantic decomposition.
Forboth target languages, correlation is highest in the?all?
case, supporting our claim for the value of ag-gregating over a wide range of semantic phenom-ena.
Some types of nodes predict the DA scores bet-ter than others.
HUME scores on As correlate morestrongly with DA than scores on Scene Main Rela-tions (P+S).
Center nodes (C) are also more corre-lated than elaborator nodes (E), which is expectedgiven that Centers are defined to be more semanti-cally dominant.
Future work will construct an aggre-gate HUME score which weights the different nodetypes according to their semantic prominence.HUME and DA are conceputally very differentmetrics: while DA standardises and averages scoresacross annotators to denoise the crowd-sourced rawdata, thus obtaining a single aggregate score, HUMEdecomposes over a combinatorial structure, thus al-lowing to localize the translation errors.
We nowturn to comparing HUME to a more conceptually-related measure, namely HMEANT.6 Comparison with HMEANTHMEANT is a human MT evaluation metric thatmeasures the overlap between the translation a ref-erence in terms of their SRL annotations.
In thissection we present a qualitative comparison betweenHUME and HMEANT, using examples from our ex-perimental data.Verbal Structures Only?
HMEANT focuses onverbal argument structures, ignoring other pervasivephenomena such as non-verbal predicates and inter-clausal relations.
Consider the following example:Source a coronary angioplasty may not betechnically possibleTransl.
eine koronare Angioplastie kann nichttechnisch m?glichGloss a coronary angioplasty can not techni-cally possibleThe German translation is largely correct, exceptthat the main verb ?sein?
(?be?)
is omitted.
Whilethis may be interpreted as a minor error, HMEANTwill assign the sentence a very low score, as it failedto translate the main verb.It is also relatively common that verbal construc-tions are translated as non-verbal ones or vice versa.Consider the following example:Source ... tend to be higher in saturated fatsTransl.
... in der Regel h?her in ges?ttigteFetteGloss ... as a rule higher in saturated fatsThe German translation is largely correct, despitethe grammatical divergence, namely that the Englishverb ?tend?
is translated into the German preposi-tional phrase ?in der Regel?
(?as a rule?).
HMEANTwill consider the translation to be of poor quality asthere is no German verb to align with the Englishone.We conducted an analysis of the English UCCAWikipedia corpus (5324 sentences) in order to assessthe pervasiveness of three phenomena that are notwell supported by HMEANT.8 First, copula clauses8Argument structures and linkers are explicitly marked inUCCA.
Non-auxiliary instances of ?be?
and nouns are identi-1271are treated in HMEANT simply as instances of themain verb ?be?, which generally does not convey themeaning of these clauses.
They appear in 21.7% ofthe sentences, according to conservative estimatesthat only consider non-auxiliary instances of ?be?.Second, nominal argument structures, ignored byHMEANT, are in fact highly pervasive, appearingin 48.7% of the sentences.
Third, linkers that ex-press inter-relations between clauses (mainly dis-course markers and conjunctions) appear in 56% ofthe sentences, but are again ignored by HMEANT.For instance, linkers are sometimes omitted in trans-lation, but these omissions are not penalized byHMEANT.
The following is such an example fromour experimental dataset:Source However, this review was restricted to...Transl.
Diese ?berpr?fung bescr?nkte sichauf ...Gloss This review was restricted to ...We note that some of these issues were alreadyobserved in previous applications of HMEANT tolanguages other than English.
See Birch et al (2013)for German, Bojar and Wu (2012) for Czech andChuchunkov et al (2014) for Russian.One Structure or Two.
HUME only annotatesthe source, while HMEANT relies on two indepen-dently constructed structural annotations, one for thereference and one for the translation.
Not annotat-ing the translation is appealing as it is often impos-sible to assign a semantic structure to a low qualitytranslation.
On the other hand, HUME may be ar-tificially boosting the perceived understandability ofthe translation by allowing access to the source.Alignment.
In HMEANT, the alignment betweenthe reference and translation structures is a key partof the manual annotation.
If the alignment cannotbe created, the translation is heavily penalized.
Bo-jar and Wu (2012) and Chuchunkov et al (2014)argue that the structures of the reference and of anaccurate translation may still diverge, for instancedue to a different interpretation of a PP-attachment,or the verb having an additional modifier in one ofthe structures.
It would be desirable to allow mod-ifications to the SRL annotations at the alignmentfied using the NLTK standard tagger.
Nominal argument struc-tures are here Scenes whose Main Relation is headed by a noun.stage, to avoid unduly penalizing such spurious di-vergences.The same issue is noted by Lo and Wu (2014): theIAA on SRL dropped from 90% to 61% when thetwo aligned structures were from two different an-notators.
HUME uses automatic (word-level) align-ment, which only serves as a cue for directing theattention of the annotators.
The user is expected tomentally correct the alignment as needed, thus cir-cumventing this difficulty.Monolingual vs. Bilingual Evaluation.
HUMEdiverges from HMEANT and from shallower mea-sures like BLEU, in not requiring a reference.
In-stead, it directly compares the source and the trans-lation.
This requires the employment of bilingualannotators, but has the benefit of avoiding using areference, which is never uniquely defined, and maythus lead to unjustly low scores where the transla-tion is a paraphrase of the reference.
If only mono-lingual annotators are available, the HUME evalua-tion could be performed with a reference sentenceinstead of with the source.
This, however, wouldrisk inaccurate judgements due to the naturally oc-curring differences between the source and its refer-ence translations.7 ConclusionWe have introduced HUME, a human semantic MTevaluation measure which addresses a wide rangeof semantic phenomena.
We have shown that itcan be reliably and efficiently annotated in multi-ple languages, and that annotation quality is robustto sentence length.
Comparison to direct assess-ments further support HUME?s validity.
We be-lieve that HUME, and a future automated version ofHUME, allows for a finer-grained analysis of trans-lation quality, and will be useful in informing the de-velopment of a more semantically aware approach toMT.All annotation data gathered in this project, to-gether with analysis scripts, is available online9.AcknowledgmentsThis project has received funding from the EuropeanUnion?s Horizon 2020 research and innovation pro-gramme under grant agreement 644402 (HimL).9https://github.com/bhaddow/hume-emnlp161272ReferencesOmri Abend and Ari Rappoport.
2013.
Universal con-ceptual cognitive annotation (ucca).
In Proceedingsof the 51st Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers),pages 228?238, Sofia, Bulgaria, August.
Associationfor Computational Linguistics.Laura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
2013.
Abstract Meaning Representationfor sembanking.
In Proceedings of Linguistic Anno-tation Workshop and Interoperability with Discourse,pages 178?186.Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization, pages 65?72, Ann Arbor, MI, USA.
Asso-ciation for Computational Linguistics.Alexandra Birch, Barry Haddow, Ulrich Germann, MariaNadejde, Christian Buck, and Philipp Koehn.
2013.The feasibility of HMEANT as a human MT evalua-tion metric.
In Proceedings of the Eighth Workshop onStatistical Machine Translation, pages 52?61, Sofia,Bulgaria, August.
Association for Computational Lin-guistics.Ondr?ej Bojar and Dekai Wu.
2012.
Towards a Predicate-Argument Evaluation for MT.
In Proceedings of theSixth Workshop on Syntax, Semantics and Structure inStatistical Translation, pages 30?38, Jeju, Republic ofKorea, July.
Association for Computational Linguis-tics.Ondr?ej Bojar, Milo?
Ercegovc?evic?, Martin Popel, andOmar F. Zaidan.
2011.
A grain of salt for the WMTmanual evaluation.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation, pages 1?11,Edinburgh, Scotland.Ondrej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Johannes Leveling,Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Ale?
Tam-chyna.
2014.
Findings of the 2014 workshop on statis-tical machine translation.
In Proceedings of the NinthWorkshop on Statistical Machine Translation, pages12?58, Baltimore, Maryland, USA, June.
Associationfor Computational Linguistics.Ondr?ej Bojar, Rajen Chatterjee, Christian Federmann,Barry Haddow, Matthias Huck, Chris Hokamp, PhilippKoehn, Varvara Logacheva, Christof Monz, MatteoNegri, Matt Post, Carolina Scarton, Lucia Specia, andMarco Turchi.
2015.
Findings of the 2015 workshopon statistical machine translation.
In Proceedings ofthe Tenth Workshop on Statistical Machine Transla-tion, pages 1?46, Lisbon, Portugal, September.
Asso-ciation for Computational Linguistics.Alexander Chuchunkov, Alexander Tarelkin, and IrinaGalinskaya.
2014.
Applying HMEANT to English-Russian Translations.
In Proceedings of SSST-8,Eighth Workshop on Syntax, Semantics and Structurein Statistical Translation, pages 43?50, Doha, Qatar,October.
Association for Computational Linguistics.Robert M.W.
Dixon.
2010a.
Basic Linguistic Theory:Grammatical Topics, volume 2.
Oxford UniversityPress.Robert M.W.
Dixon.
2010b.
Basic Linguistic Theory:Methodology, volume 1.
Oxford University Press.Robert M.W.
Dixon.
2012.
Basic Linguistic Theory:Further Grammatical Topics, volume 3.
Oxford Uni-versity Press.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proceedings of the second interna-tional conference on Human Language TechnologyResearch, pages 138?145, San Diego, CA, USA.
Mor-gan Kaufmann Publishers Inc.Marina Fomicheva and Lucia Specia.
2016.
Referencebias in monolingual machine translation evaluation.
In54th Annual Meeting of the Association for Computa-tional Linguistics, ACL, Berlin, Germany.Jes?s Gim?nez and Llu?s M?rquez.
2007.
Linguisticfeatures for automatic evaluation of heterogenous mtsystems.
In Proceedings of the Second Workshop onStatistical Machine Translation, pages 256?264.Yvette Graham, Timothy Baldwin, Alistair Moffat, andJustin Zobel.
2015a.
Can machine translation systemsbe evaluated by the crowd alone?
Natural LanguageEngineering, pages 1?28.Yvette Graham, Nitika Mathur, and Timothy Baldwin.2015b.
Accurate evaluation of segment-level machinetranslation metrics.
In Proc.
of NAACL-HLT, pages1183?1191.Yvette Graham.
2015.
Improving evaluation of machinetranslation quality estimation.
In Proceedings of the53rd Annual Meeting of the Association for Computa-tional Linguistics and the 7th International Joint Con-ference on Natural Language Processing (Volume 1:Long Papers), pages 1804?1813, Beijing, China, July.Association for Computational Linguistics.Jan Hajic?, Eva Hajic?ov?, Jarmila Panevov?, Petr Sgall,Ondr?ej Bojar, Silvie Cinkov?, Eva Fuc?
?kov?, MarieMikulov?, Petr Pajas, Jan Popelka, Jir??
Semeck?, Jana?indlerov?, Jan ?te?p?nek, Josef Toman, Zden?ka Ure-?ov?, and Zdene?k ?abokrtsk?.
2012.
AnnouncingPrague Czech-English Dependency Treebank 2.0.
In1273Proceedings of the Eighth International Language Re-sources and Evaluation Conference (LREC?12), pages3153?3160, Istanbul, Turkey, May.
ELRA, EuropeanLanguage Resources Association.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,et al 2007.
Moses: Open source toolkit for statis-tical machine translation.
In Proceedings of the 45thAnnual Meeting of the Association for ComputationalLinguistics, Companion Volume Proceedings of theDemo and Poster Sessions, pages 177?180, Prague,Czech Republic.
Association for Computational Lin-guistics.Ding Liu and Daniel Gildea.
2005.
Syntactic features forevaluation of machine translation.
In ACL 2005 Work-shop on Intrinsic and Extrinsic Evaluation Measuresfor Machine Translation and/or Summarization, pages25?32.Chi-kiu Lo and Dekai Wu.
2011.
Structured vs. flatsemantic role representations for machine translationevaluation.
In Proceedings of the Fifth Workshop onSyntax, Semantics and Structure in Statistical Transla-tion, pages 10?20.
Association for Computational Lin-guistics.Chi-Kiu Lo and Dekai Wu.
2014.
On the Reliabil-ity and Inter-Annotator Agreement of Human Seman-tic MT Evaluation via HMEANT.
In Nicoletta Cal-zolari (Conference Chair), Khalid Choukri, ThierryDeclerck, Hrafn Loftsson, Bente Maegaard, JosephMariani, Asuncion Moreno, Jan Odijk, and SteliosPiperidis, editors, Proceedings of the Ninth Interna-tional Conference on Language Resources and Evalu-ation (LREC?14), Reykjavik, Iceland.
European Lan-guage Resources Association (ELRA).Arle Richard Lommel, Maja Popovic, and Aljoscha Bur-chardt.
2014.
Assessing Inter-Annotator Agreementfor Translation Error Annotation.
In MTE: Workshopon Automatic and Manual Metrics for OperationalTranslation Evaluation.
LREC.Matou?
Mach?c?ek and Ondr?ej Bojar.
2015.
EvaluatingMachine Translation Quality Using Short SegmentsAnnotations.
The Prague Bulletin of MathematicalLinguistics, 103:85?110, April.Pedro Marinotti.
2014.
Measuring semantic preservationin machine translation with HCOMET: human cogni-tive metric for evaluating translation.
Master?s thesis,University of Edinburgh.Stephan Oepen and Jan Tore L?nning.
2006.Discriminant-based MRS banking.
In Proceedings ofLREC, pages 1250?1255.Karolina Owczarzak, Josef van Genabith, and Andy Way.2007.
Evaluating machine translation with LFG de-pendencies.
Machine Translation, 21(2):95?119.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics, pages 311?318, Philadelphia, PA,USA.
Association for Computational Linguistics.Petr Sgall, Eva Hajic?ov?, and Jarmila Panevov?.
1986.The Meaning of the Sentence and Its Semantic andPragmatic Aspects.
Academia/Reidel PublishingCompany, Prague, Czech Republic/Dordrecht, Nether-lands.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of association for machine translationin the Americas, pages 223?231.Elior Sulem, Omri Abend, and Ari Rappoport.
2015.Conceptual annotations preserve structure acrosstranslations: A French-English case study.
In ACL2015 Workshop on Semantics-Driven Statistical Ma-chine Translation (S2MT), pages 11?22.J?rg Tiedemann.
2009.
News from OPUS ?
a collectionof multilingual parallel corpora with tools and inter-faces.
In Recent Advances in Natural Language Pro-cessing, volume 5, pages 237?248, Borovets, Bulgaria.John Benjamins.1274
