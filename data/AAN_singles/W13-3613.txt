Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 96?101,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsRule-based System for Automatic Grammar CorrectionUsing Syntactic N-grams for English Language Learning (L2)Grigori Sidorov?, Anubhav Gupta?, Martin Tozer?, Dolors Catala?, Angels Catena?
and Sandrine Fuentes?
?Centro de Investigacio?n en Computacio?n, Instituto Polite?cnico Nacional (IPN), Mexico?
Departament de Filologia Francesa i Roma`nica, Universitat Auto`noma de Barcelona, Spainwww.cic.ipn.mx\?sidorov,{anubhav.gupta, tozer.martin}@e-campus.uab.cat,{dolors.catala, angels.catena, sandrine.fuentes}@uab.catAbstractWe describe the system developed for theCoNLL-2013 shared task?automatic En-glish L2 grammar error correction.
Thesystem is based on the rule-based ap-proach.
It uses very few additional re-sources: a morphological analyzer and alist of 250 common uncountable nouns,along with the training data provided bythe organizers.
The system uses the syn-tactic information available in the train-ing data: this information is representedas syntactic n-grams, i.e.
n-grams ex-tracted by following the paths in depen-dency trees.
The system is simple andwas developed in a short period of time(1 month).
Since it does not employany additional resources or any sophisti-cated machine learning methods, it doesnot achieve high scores (specifically, it haslow recall) but could be considered as abaseline system for the task.
On the otherhand, it shows what can be obtained usinga simple rule-based approach and presentsa few situations where the rule-based ap-proach can perform better than ML ap-proach.1 IntroductionThere are two main approaches in the design of themodern linguistic experiments and the develop-ment of the natural language processing applica-tions: rule-based and machine learning-based.
Inpractical applications of machine learning (ML),the best results are achieved by the methods thatuse supervised learning, i.e., that are based onmanually prepared training data for learning.
Itis also worth mentioning what can be considereda general rule for the combination of these twoapproaches: a system based on the mixed ap-proach should obtain better results if each partof the system is applied according to its ?com-petence?.
Specifically, some problems are bettersolved by the application of the rules?like therules for choosing the correct allomorph of the ar-ticle ?a?
vs.
?an?, while other problems are bettersolved by the usage of ML methods?such as de-ciding the presence or absence of a definite or anindefinite determiner.This paper describes the system developed forthe CoNLL-2013 shared task.
The task consistsof grammar correction in texts written by peoplelearning English as a second language (L2).
Thereare five types of errors considered in the task: nounnumber, subject-verb agreement, verb form, ar-ticle/determiner and choice of preposition.
Thetraining data processed by the Stanford parser (deMarneffe et al 2006) is provided.
This data is partof the NUCLE corpus (Dahlmeier et al 2013).The data also contains the error types and the cor-rected version.Development of the systemwas started only twomonths before the deadline, so it is also an inter-esting example of what can be done in a rathershort period of time and with relatively little ef-fort: only one person-month joint effort in total.In our system, we considered mainly the rule-based approach.
Note that we used the ConLLdata to extract preposition patterns, which can beconsidered as a very reduced form of machinelearning with yes/no classifier, as well as to con-struct rules directly from the data.Another feature of our system is the widespreaduse of the syntactic information present in the pro-vided data.
In our previous works, we general-ized the use of syntactic information in NLP byintroducing the concept of syntactic n-grams, i.e.n-grams constructed by following the dependencypaths in a syntactic tree (Sidorov et al 2012;Sidorov et al 2013).
Note that they are not n-grams of POS tags, as could be assumed from thename; the name refers to the manner in which they96Figure 1: Example of syntactic tree (for extractionof syntactic n-grams).are constructed.
That is to say, in a dependencyrelation, there is always a head word and a depen-dent word.
In the syntactic tree, this relation isgraphically represented by an arrow: head?
de-pendent.
As it can be observed in Fig.
1, we canalso use the tree hierarchy?the head word is al-ways ?higher?
in the syntactic tree.The algorithm for the construction of syntacticn-grams is as follows: we start from the root wordand move to each dependent word following thedependency relations.
At each step, the sequenceof previous elements in the route taken are takeninto account.
The last n words in the sequencecorrespond to the syntactic n-gram.
This could bereformulated as: we should take the last n wordsof the (unique) path from the root to the currentword.In other words, we start from the root and reachone of the dependent words.
If we want to con-struct bigrams, then we have a bigram already.
Ifwe need other elements of the n-gram, then wemove to the word that is dependent and continue tothe words that are dependent on it.
If a word hasseveral dependent words, we consider them oneafter another and thus, obtain several syntactic n-grams.
Note that the head word always appearsbefore the dependent word in the syntactic n-gramduring the construction process.For example, from the tree presented in Fig.
1,the following syntactic bigrams can be extracted:likes-also, likes-dog, dog-my, likes-eating, eating-sausage.
Note that only two syntactic 3-gramscan be constructed: likes-dog-my, likes-eating-sausage.
The construction process is the follow-ing: we start with the root word like.
It has severaldependent words: dog, also, eating.
Consideringthem one after another, we obtain three syntacticbigrams.
Then we move on to the word dog.
Ithas only one dependent word: my.
This is anotherbigram dog-my.
However, the path from like alsogoes through it, so this is also the 3-gram like-dog-my, etc.The reader can compare these syntactic n-gramswith traditional n-grams and consider their advan-tages: there are a lot less syntactic n-grams, theyare less arbitrary, they have linguistic interpreta-tion, etc.Note that syntactic n-grams can be formed bywords (lemmas, stems), POS tags, names of de-pendency relations, or they can be mixed, i.e., acombination of the mentioned types.
Being n-grams, they can be applied in any machine learn-ing task where traditional n-grams are applied.However, unlike traditional n-grams, they have aclear linguistic interpretation and can be consid-ered as an introduction of linguistic (syntactic) in-formation into machine learning methods.
Previ-ously, we obtained better results by applying thesyntactic n-grams to opinion mining and author-ship attribution tasks compared to the traditionaln-grams.
Further in this paper, it is described howwe use syntactic n-grams for the formulation ofrules in our system and for the extraction of pat-terns.The system described in this paper does not ob-tain high scores.
In our opinion, it could be con-sidered a baseline system for the grammar correc-tion task due to its simplicity, its use of very fewadditional resources and the speed of its develop-ment.
Concretely, if a more sophisticated systemoutperforms ours, it reflects well upon that system.If it performs more poorly, its design should berevised.
On the other hand, this paper also dis-cusses the few situations where the rule-based sys-tem can outperform an ML approach.
As we men-tioned earlier, the ideal system would combineboth these approaches.
To quote Tapanainen andVoutilainen (1994), ?don?t guess if you know?.Further below, we describe the lexical resourcesthat we used, the processing of each type of errorand the evaluation of the system.2 The System?s Linguistic ResourcesThe system consists of several program moduleswritten in the Python programming language.
Weused only three types of linguistic resources:?
The provided corpus NUCLE data was pro-cessed with the Stanford parser.
It wasused for the extraction of patterns to identify97preposition errors and for the formulation ofrules.?
A list of the 250 most common uncountablenouns1.
This list was used for processing thepossibility of using the nouns in plural form.?
A morphological analysis system for Englishthat in our case was based on the FreeL-ing morphological dictionary (Padro?
et al2010).The FreeLing dictionary is a freely availabletext file which contains more than 71,000 wordforms with standard POS tags.
It has the follow-ing data: for each word form, it contains a list oflemmas and POS tags.
An example of the entries:...abandon abandon VB abandon VBPabandoned abandon VBD abandon VBNabandoning abandon VBGabandonment abandonment NNabandons abandon VBZ...This list can also be easily reordered by lemmas.It is therefore very easy to apply this word list toboth morphological analysis and generation.
Themorphological analysis simply consists of search-ing for a word form in the list, while the mor-phological generation involves searching the listof lemmas and then finding the word form withthe necessary POS tag, i.e., for the generation, theinput consists in the lemma and the POS tag.
Forexample, if we want to generate the VBZ form ofthe verb take, then we search in the list ordered ac-cording to the lemma take; there are several forms:take took VBP, take taken VBN, take takes VBZ andchoose the form that has the POS tag VBZ.3 Error ProcessingIn accordance with the rules of the ConLL sharedtask, only five types of errors were considered:noun number, incorrect preposition, choice of de-terminer or article, subject-verb agreement andverb form.
More error types are marked in thecorpus, but they are much more complex, beingrelated to the meaning and content.Let us see examples of the errors:?
Preposition error: ?...the need of habitableenvironment...?, where ?for?
should be used.1List of 250 most common uncountable nouns.www.englishclub.com>Learn English>Vocabulary>Nouns.?
Nn error: ?...people are getting more con-scious of the damages...?, the word ?damage?in singular should be used.?
SVA error: ?...relevant information are read-ily available...?, where ?is?
should be used in-stead.?
Vform error: ?The solution can be obtainby using technology...?, where ?obtained?should appear.?
ArtOrDet error: ?...It is also important to cre-ate a better material...?, where ?a?
should notbe used.The total number of errors marked in the train-ing and the test data for ConLL 2013 are presentedby type in Table 1.Table 1: Numbers of errors in training and test datalisted by type.Error type Training TestVform (Verb form) 1,451 122SVA (Subject-verb agreement) 1,529 124ArtOrDet 6,654 690Nn (Noun number) 3,773 396Prep (Preposition) 2,402 311Note that the errors related to the noun num-ber should be processed first since later, an agree-ment error could be produced if the noun numberis changed.
If the agreement error is introduced bythe modification of the noun number, it is not theerror committed by the student, however it is con-sidered as such in the current version of the task.Probably, it can be considered as some sort of sec-ondary error.
The order in which other errors areprocessed is irrelevant.3.1 Noun Number Error ProcessingThe only rule we implemented in this case was thatuncountable nouns do not have a plural.
We useda list of the 250 most common uncountable nouns(as mentioned in the Section 2) to determine thepossibility of a plural form for a noun.
For ex-ample: ...ethics, evidence, evolution, failure, faith,fame, fiction, flour, flu, food, freedom...We made an exception for the noun ?time?
anddo not consider it as uncountable, because its usein the common expressions such as ?many times?98is much more frequent than its use as an uncount-able noun as in ?theory of time?
or ?what time isit now??.
More sophisticated systems should ana-lyze the contexts obtained from vast data sets (cor-pora), i.e.
consider n-grams or syntactic n-grams.Note that word sense disambiguation would behelpful in the resolution of the mentioned ambigu-ities.
Also, the rule that considers the presence ofthe dependent words like ?many, a lot of, amountof?
could be added.3.2 Subject-Verb Agreement and Verb FormError ProcessingWe consider these two types of errors together be-cause they are related to a similar and a rather sim-ple grammatical phenomenon.
To correct theseerrors we used syntactic information to formulatethe rules.
This is logical because we cannot relyon the context words (neighbours) as they appearin texts (traditional n-grams).
Note that the rulesare also related to the modal verbs and the passiveconstructions.The rules for the agreement are very simple: 1)if the noun is in plural and the VBZ tag is present,then change the tag to VB, 2) if the noun is insingular and the VB tag is present, then changethe tag to VBZ.
The corresponding morphologicalgeneration is also performed.The rules for verb form correction are as fol-lows: 1) if we have a modal verb, then the depend-ing verb should have a VB tag, 2) if we have anauxiliary verb ?have?, then the main verb shouldhave a VB tag (perfect tense), etc.
Moreover, theFreeLing morphological dictionary is utilized toidentify the correct verb form.
Note that there aresome assumptions here about what drives the verbform, e.g., that a noun or a modal verb are correctand the verb needs to change.
This appears to bea reasonable assumption, but may not always becorrect.3.3 Preposition Error ProcessingIt is well-known that prepositions depend on lex-ical units that are their heads, see (Eeg-Olofssonand Knutsson, 2003).
But what should be doneif we want to consider the dependent word?
Say,that in the PP attachment task, the lexical unit isthe preferred solution as well.
In general, it wouldbe an ideal solution in grammar correction, but inthe case of our system, very little training data wasused.
If we consider that the dependent word is alexical unit, we will have less recall.
We are there-fore practically obliged to consider that it is a POStag.To process the prepositions, we used the train-ing data provided by the organizers.
Specifically,we extracted preposition patterns.
We apply theconcept of syntactic n-grams to include both thehead word of the preposition and the dependentword into the pattern.
The pattern data corre-sponds to syntactic n-grams because they are con-structed using syntactic dependencies.
As wementioned previously, syntactic n-grams can con-sist of words, POS tags or a combination.
In ourcase, we used mixed syntactic n-grams: the headword is the lexical unit, while the dependent wordis the POS tag, as shown in Table 2.For example, the first line corresponds to the er-roneous phrase ?...unwelcomed among public...?,where ?among?
should be substituted by ?by?.Note that there can be other words between thesethree words in the surface representation of thesentence, but the parser allows the extraction ofthe syntactic n-gram, which represents the ?pure?pattern.In order to choose the syntactic n-gram type, ourfirst consideration was that the head word shouldbe a lexical unit (word), because this determinesthe choice of the preposition.
We used a POStag for the dependent element, because we consid-ered that using a word there would be too specific.Thus, our final syntactic n-gram for the first linewas ?...unwelcomed among NN...?, which shouldbe changed to ?...unwelcomed by NN...?.
The syn-tactic n-gram for the second line was ?...trouble forNN...?, which should be changed to ?...trouble inNN...?, etc.
Note that insertion of prepositions isnot considered, but deletion can be performed, i.e.,changing the preposition to nothing.The rule for the system is formulated in the fol-lowing way: if we find a relation ?preposition?
inthe dependency tree, then for the preposition thatcorresponds to this relation, we search the list ofthe extracted patterns.
If we find the pattern, thenwe change the preposition.
It is quite clear thatthe training data is too limited to obtain patternsfor a great majority of words.
Our list containedonly 1,896 elements.
These patterns should be ex-tracted from a very large corpus or a dictionary.3.4 Article or Determiner Error ProcessingIn this case, we found only two clear rules, bothrelated to the article ?a?
: 1) choice of the allo-99Table 2: Examples of patterns for prepositions.Preposition Preposition Head word Head word Dependent word Dependent word(error) (correction) (lemma) (POS) (lemma) (POS)among by unwelcomed VBN public NNfor in trouble NN development NNon in practice NN October NNPon in face VBG field NNmorph ?a/an?, and 2) the fact that the article ?a?cannot be used with nouns in plural.
Other ruleswould be too complex for a manually created rule-based system.
The first rule takes into the accountthe immediate neighbor: the choice depends on itsphonetic properties.
The second rule considers thesyntactically related head word, which cannot bein plural if we use the indefinite article.4 Evaluation of the SystemFor the evaluation, the organizers provided datasimilar to the training data from the same NU-CLE corpus, which also contained syntactic in-formation.
The evaluation results were providedby the organizers using their evaluation script inPython (Dahlmeier and Ng, 2012).
The results ob-tained with this script for our system are: precision17.4 %, recall 1.8%, and F1 measure 3.3% (thepreliminary scores were: 12.4%, 1.2% and 2.2%correspondingly).
See the final remarks in thissection, where we argue that the real values shouldbe: precision 25%, recall 2.6%, and F1 measure4.7%.The results are low, but as we mentioned previ-ously, our system uses a rule-based approach withvery few additional resources, so it cannot com-pete with ML based approaches that additionallyrely on vast lexical resources and the Internet.
Dueto its simplicity, low use of additional resources,and very short development time, we consider oursystem a possible baseline system for the task.
Onthe other hand, we showed that in some cases therules should be used as a complementary tech-nique for ML learning methods: don?t guess if youknow.The low recall of the system is to be expectedas we process only clearly defined errors, ignoringmore complex cases.It is always interesting to perform an analysis ofthe errors committed by a system.
Let us analyzethe supposed errors committed by our systemfor the noun number error type.
It performed 18corrections, 3 of which coincide with the marksin the corpus data.
Two of them are clear errorsof the system: ?traffic jam?, where the word?jam?
is used in a sense other than that of the?substance?, and ?many respects?, where againthe word ?respect?
has a different meaning to thatof the uncountable noun.
There are 13 cases listedbelow, that our system marked as errors, becausethey are uncountable nouns in plural, but theyare not marked in the corpus.
Let us consider thenouns in capital letters:...peaceful(JJ) LIVINGS(NNS)2...,...life(NN) QUALITIES(NNS)...,...Many(JJ) science(NN) FICTIONS(NNS)...,...does(VBZ) not(RB) have(VB) enough(JJ)LANDS(NNS)...,...indicates(VBZ) that(IN) the(DT) FOODS(NNS)the(DT) people(NNS) eat(VBP)...,...problem(NN) of(IN) public(JJ) TRANSPORTA-TIONS(NNS)...,...healthcare(NN) consume(VBP) large(JJ)QUANTITIES(NNS) of(IN) energy...,...this(DT) society(NN) may(MD) lack(VB) of(IN)LABOURS(NNS)...Note that the words ?equipment?
and ?usage?in plural were marked as errors in the corpus.
Inour opinion, it is inconsistent to mark these two aserrors, and not to mark the words from this list assuch.
While it is true that their use in plural is pos-sible, it is clearly forced and is much less probable.At least, students of English should learn to usethese words in singular only.
Some of these mis-takes (but not all) were corrected by the organizersfor the final scoring data.
If we consider all thesecases as correctly marked errors, then the preci-sion of our system is around 25%, recall 2.6%, andF1 measure 4.7%.2?LIVINGS?
is encountered 5 times and ?QUANTITIES?is encountered 2 times1005 ConclusionsIn this paper we have described the system pre-sented for the CoNLL-2013 shared task for gram-mar correction in English (L2).
The system usesa rule-based approach and relies on very few addi-tional resources: a list of 250 uncountable nouns, amorphological analyzer and the training data fromthe NUCLE corpus provided by the organizers.The system uses syntactic n-grams for rule formu-lation, i.e., n-grams that are constructed by follow-ing the dependency paths in a parsed tree.We analyzed various situations in which a rulebased technique can give better results than MLtechniques: don?t guess if you know.
These casesare: 1) two rules for the article ?a?, and 2) therules for uncountable nouns (in this case, wordsense disambiguation would help to determine ifthe sense in the text is an uncountable noun orhas some other use), and 3) the subject-verb agree-ment rule.
In the case of prepositions, ML learn-ing is definitely better.
Otherwise, vast resourceswould need to be used, which in any case, wouldresemble machine learning.
We are not sure aboutverb form errors: the rules which we formulatedare rather simple, but the performance of variousML methods should be analysed in order to decidewhich technique is better.The system is simple and was developed in avery short time.
It does not obtain high scores andcould be considered as a baseline system for thetask.AcknowledgementsThis work was done under partial support of theMexican Government (CONACYT, SNI, COFAA-IPN, SIP-IPN 20120418, 20121823), CONACYT-DST India (?Answer Validation through TextualEntailment?
), Mexico City Government (ICYTPICCO10-120), and FP7-PEOPLE-2010- IRSES:Web Information Quality - Evaluation Initiative(WIQ-EI) European Commission project 269180.ReferencesDaniel Dahlmeier and Hwee Tou Ng.
2012.
Betterevaluation for grammatical error correction.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics (NAACL 2012), pages 568?572.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerEnglish: The NUS corpus of learner English.M.C.
de Marneffe, B. MacCartney, and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In LREC 2006.Jens Eeg-Olofsson and Ola Knutsson.
2003.
Auto-matic grammar checking for second language learn-ers ?
the use of prepositions.
In Proceedings ofNodalida?03.Llus Padro?, Miquel Collado, Samuel Reese, MarinaLloberes, and Irene Castello?n.
2010.
Freeling2.1: Five years of open-source language processingtools.
In Proceedings of 7th Language Resourcesand Evaluation Conference (LREC 2010), ELRA.G.
Sidorov, F. Velasquez, E. Stamatatos, A. Gel-bukh, and L. Chanona-Hernandez.
2012.
Syntac-tic dependency-based n-grams as classification fea-tures.
LNAI 7630, pages 1?11.G.
Sidorov, F. Velasquez, E. Stamatatos, A. Gel-bukh, and L. Chanona-Hernandez.
2013.
Syntacticdependency-based n-grams: More evidence of use-fulness in classification.
LNCS 7816 (Proc.
of CI-CLing), pages 13?24.Pasi Tapanainen and Atro Voutilainen.
1994.
Taggingaccurately - don?t guess if you know.
In Proceedingsof ANLP ?94.101
