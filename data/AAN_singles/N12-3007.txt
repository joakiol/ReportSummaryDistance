Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 25?28,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsIncremental Speech Understanding in a Multi-Party Virtual HumanDialogue SystemDavid DeVault and David TraumInstitute for Creative TechnologiesUniversity of Southern California12015 Waterfront Drive, Playa Vista, CA 90094{devault,traum}@ict.usc.edu1 Extended AbstractThis demonstration highlights some emerging ca-pabilities for incremental speech understanding andprocessing in virtual human dialogue systems.
Thiswork is part of an ongoing effort that aims to en-able realistic spoken dialogue with virtual humans inmulti-party negotiation scenarios (Plu?ss et al, 2011;Traum et al, 2008b).
These scenarios are designedto allow trainees to practice their negotiation skillsby engaging in face-to-face spoken negotiation withone or more virtual humans.An important component in achieving naturalisticbehavior in these negotiation scenarios, which ide-ally should have the virtual humans demonstratingfluid turn-taking, complex reasoning, and respond-ing to factors like trust and emotions, is for the vir-tual humans to begin to understand and in somecases respond in real time to users?
speech, as theusers are speaking (DeVault et al, 2011b).
These re-sponses could range from relatively straightforwardturn management behaviors, like having a virtual hu-man recognize when it is being addressed by a userutterance, and possibly turn to look at the user whohas started speaking, to more complex responsessuch as emotional reactions to the content of whatusers are saying.The current demonstration extends our previousdemonstration of incremental processing (Sagae etal., 2010) in several important respects.
First, itincludes additional indicators, as described in (De-Vault et al, 2011a).
Second, it is applied to a newdomain, an extension of that presented in (Plu?ss etal., 2011).
Finally, it is integrated with the dialogueFigure 1: SASO negotiation in the saloon: Utah (left)looking at Harmony (right).models (Traum et al, 2008a), such that each par-tial interpretation is given a full pragmatic interpre-tation by each virtual character, which can be usedto generate real-time incremental non-verbal feed-back (Wang et al, 2011).Our demonstration is set in an implemented multi-party negotiation domain (Plu?ss et al, 2011) inwhich two virtual humans, Utah and Harmony (pic-tured in Figure 1), talk with two human negotiationtrainees, who play the roles of Ranger and Deputy.The dialogue takes place inside a saloon in an Amer-ican town in the Old West.
In this negotiation sce-nario, the goal of the two human role players is toconvince Utah and Harmony that Utah, who is cur-rently employed as the local bartender, should takeon the job of town sheriff.One of the research aims for this work is tosupport natural dialogue interaction, an example ofwhich is the excerpt of human role play dialogueshown in Figure 2.
One of the key features of immer-sive role plays is that people often react in multipleways to the utterances of others as they are speaking.For example, in this excerpt, the beginning of the25Ranger We can?t leave this place and have it overrun by outlaws.Uh there?s no way that?s gonna happen so we?re gonnamake sure we?ve got a properly deputized and equippedsheriff ready to maintain order in this area.00:03:56.660 - 00:04:08.830Deputy Yeah and you know and and we?re willing to00:04:06.370 - 00:04:09.850Utah And I don?t have to leave the bar completely.
I can stilluh be here part time and I can um we can hire someone todo the like day to day work and I?ll do the I?ll supervisethem and I?ll teach them.00:04:09.090 - 00:04:22.880Figure 2: Dialogue excerpt from one of the role plays.Timestamps indicate the start and end of each utterance.Deputy?s utterance overlaps the end of the Ranger?s,and then Utah interrupts the Deputy and takes thefloor a few seconds later.Our prediction approach to incremental speechunderstanding utilizes a corpus of in-domain spo-ken utterances, including both paraphrases selectedand spoken by system developers, as well as spo-ken utterances from user testing sessions (DeVaultet al, 2011b).
An example of a corpus element isshown in Figure 3.
In previous negotiation domains,we have found a fairly high word error rate in au-tomatic speech recognition results for such sponta-neous multi-party dialogue data; for example, ouraverage word error rate was 0.39 in the SASO-ENnegotiation domain (Traum et al, 2008b) with many(15%) out of domain utterances.
Our speech un-derstanding framework is robust to these kinds ofproblems (DeVault et al, 2011b), partly throughapproximating the meaning of utterances.
Utter-ance meanings are represented using an attribute-value matrix (AVM), where the attributes and val-ues represent semantic information that is linked toa domain-specific ontology and task model (Traum,2003; Hartholt et al, 2008; Plu?ss et al, 2011).
TheAVMs are linearized, using a path-value notation, asseen in Figure 3.
In our framework, we use this datato train two data-driven models, one for incremen-tal natural language understanding, and a second forincremental confidence modeling.The first step is to train a predictive incrementalunderstanding model.
This model is based on maxi-mum entropy classification, and treats entire individ-ual frames as output classes, with input features ex-tracted from partial ASR results, calculated in incre-ments of 200 milliseconds (DeVault et al, 2011b).?
Utterance (speech): i?ve come here today to talk to youabout whether you?d like to become the sheriff of this town?
ASR (NLU input): have come here today to talk to youabout would the like to become the sheriff of this town?
Frame (NLU output):<S>.mood interrogative<S>.sem.modal.desire want<S>.sem.prop.agent utah<S>.sem.prop.event providePublicServices<S>.sem.prop.location town<S>.sem.prop.theme sheriff-job<S>.sem.prop.type event<S>.sem.q-slot polarity<S>.sem.speechact.type info-req<S>.sem.type questionFigure 3: Example of a corpus training example.Each partial ASR result then serves as an incremen-tal input to NLU, which is specially trained for par-tial input as discussed in (Sagae et al, 2009).
NLUis predictive in the sense that, for each partial ASRresult, the NLU module produces as output the com-plete frame that has been associated by a human an-notator with the user?s complete utterance, even ifthat utterance has not yet been fully processed bythe ASR.
For a detailed analysis of the performanceof the predictive NLU, see (DeVault et al, 2011b).The second step in our framework is to train a setof incremental confidence models (DeVault et al,2011a), which allow the agents to assess in real time,while a user is speaking, how well the understand-ing process is proceeding.
The incremental confi-dence models build on the notion of NLU F-score,which we use to quantify the quality of a predictedNLU frame in relation to the hand-annotated correctframe.
The NLU F-score is the harmonic mean ofthe precision and recall of the attribute-value pairs(or frame elements) that compose the predicted andcorrect frames for each partial ASR result.
By usingprecision and recall of frame elements, rather thansimply looking at frame accuracy, we take into ac-count that certain frames are more similar than oth-ers, and allow for cases when the correct frame isnot in the training set.Each of our incremental confidence modelsmakes a binary prediction for each partial NLU re-sult as an utterance proceeds.
At each time t dur-26Figure 4: Visualization of Incremental Speech Processing.ing an utterance, we consider the current NLU F-Score Ft as well as the final NLU F-Score Ffinalthat will be achieved at the conclusion of the ut-terance.
In (DeVault et al, 2009) and (DeVaultet al, 2011a), we explored the use of data-drivendecision tree classifiers to make predictions aboutthese values, for example whether Ft ?
12 (cur-rent level of understanding is ?high?
), Ft ?
Ffinal(current level of understanding will not improve),or Ffinal ?
12 (final level of understanding will be?high?).
In this demonstration, we focus on thefirst and third of these incremental confidence met-rics, which we summarize as ?Now Understanding?and ?Will Understand?, respectively.
In an evalua-tion over all partial ASR results for 990 utterancesin this new scenario, we found the Now Under-standing model to have precision/recall/F-Score of.92/.75/.82, and the Will Understand model to haveprecision/recall/F-Score of .93/.85/.89.
These incre-mental confidence models therefore provide poten-tially useful real-time information to Utah and Har-mony about whether they are currently understand-ing a user utterance, and whether they will ever un-derstand a user utterance.The incremental ASR, NLU, and confidencemodels are passed to the dialogue managers for eachof the agents, Harmony and Utah.
These agents thenrelate these inputs to their own models of dialoguecontext, plans, and emotions, to calculate pragmaticinterpretations, including speech acts, reference res-olution, participant status, and how they feel aboutwhat is being discussed.
A subset of this informa-tion is passed to the non-verbal behavior generationmodule to produce incremental non-verbal listeningbehaviors (Wang et al, 2011).In support of this demonstration, we have ex-tended the implementation to include a real-time vi-sualization of incremental speech processing results,which will allow attendees to track the virtual hu-mans?
understanding as an utterance progresses.
Anexample of this visualization is shown in Figure 4.2 Demo scriptThe demonstration begins with the demo operatorproviding a brief overview of the system design, ne-gotiation scenario, and incremental processing capa-bilities.
The virtual humans Utah and Harmony (seeFigure 1) are running and ready to begin a dialoguewith the user, who will play the role of the Ranger.As the user speaks to Utah or Harmony, attendeescan observe the real time visualization of speech27processing to observe changes in the incrementalprocessing results as the utterance progresses.
Fur-ther, the visualization interface enables the demo op-erator to ?rewind?
an utterance and step through theincremental processing results that arrived each 200milliseconds, highlighting how specific partial ASRresults can change the virtual humans?
understand-ing or confidence.For example, Figure 4 shows the incrementalspeech processing state at a moment 4.8 seconds intoa user?s 7.4 second long utterance, i?ve come heretoday to talk to you about whether you?d like to be-come the sheriff of this town.
At this point in time,the visualization shows (at top left) that the virtualhumans are confident that they are both Now Under-standing and Will Understand this utterance.
Next,the graph (in white) shows the history of the agents?expected NLU F-Score for this utterance (rangingfrom 0 to 1).
Beneath the graph, the partial ASR re-sult (HAVE COME HERE TODAY TO TALK TOYOU ABOUT...) is displayed (in white), alongwith the currently predicted NLU frame (in blue).For ease of comprehension, an English gloss (utahdo you want to be the sheriff?)
for the NLU frame isalso shown (in blue) above the frame.To the right, in pink, we show some of Utah andHarmony?s agent state that is based on the current in-cremental NLU results.
The display shows that bothof the virtual humans believe that Utah is being ad-dressed by this utterance, that utah has a positive at-titude toward the content of the utterance while har-mony does not, and that both have comprehensionand participation goals.
Further, Harmony believesshe is a side participant at this moment.
The demooperator will explain and discuss this agent state in-formation, including possible uses for this informa-tion in response policies.AcknowledgmentsWe thank all the members of the ICT Virtual Hu-mans team.
The project or effort described herehas been sponsored by the U.S. Army Research,Development, and Engineering Command (RDE-COM).
Statements and opinions expressed do notnecessarily reflect the position or the policy of theUnited States Government, and no official endorse-ment should be inferred.ReferencesDavid DeVault, Kenji Sagae, and David Traum.
2009.Can I finish?
Learning when to respond to incrementalinterpretation results in interactive dialogue.
In Pro-ceedings of SIGDIAL.David DeVault, Kenji Sagae, and David Traum.
2011a.Detecting the status of a predictive incremental speechunderstanding model for real-time decision-making ina spoken dialogue system.
In Proceedings of Inter-Speech.David DeVault, Kenji Sagae, and David Traum.
2011b.Incremental interpretation and prediction of utterancemeaning for interactive dialogue.
Dialogue & Dis-course, 2(1).Arno Hartholt, Thomas Russ, David Traum, EduardHovy, and Susan Robinson.
2008.
A common groundfor virtual humans: Using an ontology in a naturallanguage oriented virtual human architecture.
In Pro-ceedings of LREC, Marrakech, Morocco, may.Brian Plu?ss, David DeVault, and David Traum.
2011.Toward rapid development of multi-party virtual hu-man negotiation scenarios.
In Proceedings of SemDial2011, the 15th Workshop on the Semantics and Prag-matics of Dialogue.Kenji Sagae, Gwen Christian, David DeVault, andDavid R. Traum.
2009.
Towards natural language un-derstanding of partial speech recognition results in dia-logue systems.
In Short Paper Proceedings of NAACLHLT.Kenji Sagae, David DeVault, and David R. Traum.
2010.Interpretation of partial utterances in virtual humandialogue systems.
In Demonstration Proceedings ofNAACL-HLT.D.
Traum, W. Swartout, J. Gratch, and S. Marsella.2008a.
A virtual human dialogue model for non-teaminteraction.
In L. Dybkjaer and W. Minker, editors,Recent Trends in Discourse and Dialogue.
Springer.David Traum, Stacy Marsella, Jonathan Gratch, JinaLee, and Arno Hartholt.
2008b.
Multi-party, multi-issue, multi-strategy negotiation for multi-modal vir-tual agents.
In Proceedings of IVA.David Traum.
2003.
Semantics and pragmatics of ques-tions and answers for dialogue agents.
In Proc.
of theInternational Workshop on Computational Semantics,pages 380?394, January.Zhiyang Wang, Jina Lee, and Stacy Marsella.
2011.Towards more comprehensive listening behavior: Be-yond the bobble head.
In Hannes Vilhjlmsson, StefanKopp, Stacy Marsella, and Kristinn Thrisson, editors,Intelligent Virtual Agents, volume 6895 of LectureNotes in Computer Science, pages 216?227.
SpringerBerlin / Heidelberg.28
