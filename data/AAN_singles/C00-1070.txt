Lex ica l i zed  H idden Markov  Mode ls  for  Par t -o f -Speech  Tagg ingSang-Zoo  Lee  and Jun - i ch i  Tsu j i iDel)artInent of Infor lnation ScienceGraduate  School of Scien(:eUniversity of Tokyo, Hongo 7-3-1Bunkyo-ku, Tokyo 113, Ja, l)3,iI{lee,tsujii} ((~is.s.u-tokyo.ac.jpHae-Chang R imDet)artment of Computer  ScienceKorea Ulfiversityi 5-Ca Anam-Dong,  Seongbuk-C~uSeoul 136-701, Korearim~)nll).korea.ac.krAbst rac tSince most previous works tbr HMM-1)ased tag-ging consider only part-ofsl)eech intbrmation incontexts, their models (:minor utilize lexical in-forlnatiol~ which is crucial tbr resolving somemorphological tmfl)iguity.
In this paper we in-troduce mliformly lexicalized HMMs fin: i)art -ofst)eech tagging in 1)oth English and \](ore, an.The lexicalized models use a simplified back-offsmoothing technique to overcome data Sl)arse-hess.
In experiment;s, lexi(:alized models a(:hievehigher accuracy than non-lexicifliz(~d modelsand the l)ack-off smoothing metho(l mitigatesdata sparseness 1)etter (;ban simple smoothingmethods.1 I n t roduct ion1)arl;-Ofsl)e(:('h(POS) tagging is a l)ro(:ess illwhich a l)rOl)('.r \])()S l;ag is assigned to ea(:h wor(lin raw tex(;s. Ev('n though morl)h()logi(:ally am-l)iguous words have more thnn one P()S tag,they l)elong to just one tag in a colll;ex(;.
'J~oresolve such ambiguity, taggers lmve to consultvarious som'ces of inibrmation such as lexica\]i)retbrences (e.g.
without consulting context,table is more probably a n(mn than a. ver}) oran adje(:t;ive), tag n-gram context;s (e.g.
after anon-1)ossessiv(: pronoun, table is more l)robal)lya verb than a. nmm or an adjective., as in th, ey ta-ble an amendment), word n-grain conl;e.xl;s (e.g.betbre lamp, table is more probal)ly an adjectivethan ~ noun or ~ verb, as in I need a table lamp),and so on(Lee et al, 1.999).However, most previous HMM-1)ased tag-gers consider only POS intbrmation in con-texts, and so they C~I~IlII()t capture lexical infi)r-nmtion which is necessary for resolving somemort)hological alnbiguity.
Some recent, workslmve rel)orted thai; tagging a('curacy couldl)e iml)roved 1)y using lexicM intbrnml;ion intheir models such as the transtbrmation-basedpatch rules(Brill, 1994), the ln~txinnun entropymodel(lIatn~q)arkhi, 1996), the statistical ex-ical ruh:s(Lee et al, 1999), the IIMM consid-ering multi-words(Kim, 1996), the selectivelylexicalized HMM(Kim et al, 1999), and so on.In the l)revious works(Kim, 1996)(Kim et al,1999), however, their ItMMs were lexicalized se-h:ctively and resl;rictively.\]n this l>al)er w('.
prol)ose a method of uni-formly lcxicalizing the standard IIMM for part-o f  speech tagging in both English and Korean.Because the slmrse-da.ta problem is more seri-ous in lexicMized models ttl~ll ill the standardmodel, a simplified version of the well-knownback-oil' smoothing nml;hod is used to overcomethe.
1)rol)lem.
For experiments, the Brown cor-pus(Francis, 1982) is used lbr English taggingand the KUNLP (:orlms(Lee ('t al., 1999) isused for Kore, an tagging.
Tim eXl)criln(;nl;~t\] re-sults show that lexicalized models l)erform bet-ter than non-lexicalized models and the simpli-fied back-off smoothing technique can mitigatedata sparseness betl;er than silnple smoothingtechniques.2 Ti le "s tandard"  HMMWe basically follow the not~ti(m of (Charniaket al, 1993) to describe Bayesian models.
Inthis paper, we assume that {w I , 'w~,..., w ~0 } isa set of words, {t t , t '2 , .
.
.
, t ;}  is a set of POStags, a sequence of random variables l'lq,,~ =l~q lazy...
I'E~ is a sentence of n words, and asequence of random w~riables T1,,, = 7~T,2... TT~is a sequence of n POS tags.
Because each ofrandom wtrbflfles W can take as its value anyof the words in the vocabulary, we denote thevalue of l'l(i by wi mM a lmrticular sequence ofwflues tbr H~,j (i < j) by wi, j.
In a similar wl.ty,we denote the value of Ti by l,i and a particular481sequence of values for T/,j (i _< j) t)y ti,j.
Forgenerality, terms wi,j and ti,j (i > j) are definedas being empty.Tile purpose of Bayesian models for POS tag-ging is to find the most likely sequence of POStags for a given sequence of' words, as follows:= arg lnaxPr (T , ,n  =- I W,,,, = w,, ,dtl,nBecause l'efhrence to the random variablesthelnselves can 1)e oulitted, the above equationb eco lnes :T('wl,n) = argmax Pr(tl,n \[ wl,,z) (1)~'l,~tNow, Eqn.
1 is transtbrnled into Eqn.
2 sincePr(wl,n) is constant for all tq,~,Pr (l.j ,n, wl,n)T(*/q,n) -- argmaxt ,  .... Pr('wl,n)= arDnaxP,'(tj,,~,w,,,,) (2)tl ,nThen, tile prolmbility Pr(tL,z, wl,n ) is brokendown into Eqn.
3 by using tile chain rule.fl(Pr(ti,t\],i-l,Wl,i-1) )Pr(tl,n,~q,r,,) = x Pr(/~i \[tl,i,~Vl,i-l) (3)i= lBecause it is difficult to compute Eqn.
3, thestandard ItMM simplified it t)3; making a strictMarkov assumption to get a more tract~d)letbrm.Pr(tl,,,, Wl,n) ~ x Pr(wi I td (4)i= lI51 the standard HMM, the probability of thecurrent tag ti depends oi5 only the previous Ktags ti-K,i-1 and the t)robability of' the cur-rent word wi depends on only the current ag 1.Thereibre, this model cannot consider lexical in-formation in contexts.3 Lex ica l i zed  HMMsIn English POS tagging, the tagging unit is aword.
On the contrary, Korean POS taggingprefers a morpheme 2.1Usually, K is determined as1 (bigram as in (Char-niak et al, 1993)) or 2 (trigram as in (Merialdo, 1991)).2The main reason is that the mtmber of word-unittags is not finite because I(orean words can be ti'eelyand newly formed l)y agglutinating morphemes(Lee tal., 1999)., / ,Flies/NNS Flies/VBZlike/CS like/IN like/JJ like/VBa/A~ a/IN a/NNttower/NN flower/VB.
/ .$/$Figure 1: A word-unit lattice ot' "Flies like a\ [ l ower  .
"Figure 1 shows a word-unit lattice of an Eil-glish sentence, "Flies like a flowc'r.
", where eachnode has a word and its word-unit tag.
Fig-ure 2 shows a morpheme-unit lattice of a Ko-rean sentence, "NcoNeun tIal Su issDa.
", whereeach node has a morphenm and its morI)heme-unit tag.
In case of Korean, transitions acrossa word boundary, which are depicted by a solidline, are distinguished fl'om transitions within aword, which are depicted by a dotted line.
illboth cases, sequences connected by bold linesindicate the most likely sequences.3.1 Word-un i t  mode lsLexicalized HMMs fbr word-unit agging are de-fined 1)y making a less strict Markov assmnp-tion, as tbllows:A(T(K,j), W( I ; j ) )~  Pr(tl,,~,wl,n)i=\] x Pr(wi I ti-L,i, wi-I , i -1)Ill models A(T(K,j), 14/(L j)) ,  the probability ofthe current tag ti depends on both tile previ-ous I f  tags t i -K, i - i  and the previous d wordswi- j , i - i  and the probability of the current word'wi depends on the current ag and the previousL tags ti_L, i and the previous I words wi-l , i -~.So, they can consider lexieal inforination.
In ex-periments, we set I f  as 1 or 2, J as 0 or K, L as1 or 2, and 1 as 0 or L. If J and I are zero, theabove models are non-lexicalized models.
Oth-erwise, they are lexicalized models.482$/,Neo/N NI" Ncol/VV?.
4No'an~ P X Ncun/EFDH~d/NNCC Hd/NNBU H~(VV \ ] Ia /VXS'a/NNCG Su/NNBGiss/\zJ iss/VXDa/EFF Da/EFC?
"'OOoo,,,j~g_._.--"-./ss.$/$Figure 2: A morl)heme-unit latti(:(; of "N,oN,'unllal S'u i.ssl)a."
(= You (:an do it.
)r l  f in a lexicalized model A(~/(9,2), lI ('J,2)), fin" ex-mnl)lc , the t)robal)ility of a node "a/AT" of tlmmost likely sequen(:e in Figure 1 is calculate(t astbllows:l'r(AT' I NM& vIL Fli(:,~, lit,:c)?
tq  ?
x Pr(a t :'1~, NNS,  VH, 1 l'~,c.s, lil,:c)3.2  Morphe lne-un i t  mode lsl);~yesian models for lnOrl)heme-unit taggingtin(t the most likely se(lueame of mor\])h(mmsand corresponding tags fi)r ;~ given sequence ofwords, as follows:~'(11) ,1,,) = al'glll;XX Pr (c  l,v,, ?/~,,,u I '1,,,,~) (6)Cl~u flltl,,t, ra-ax Pr(c,,,,, m,,. '
,,,,, ,,,) (7)Cl,~tllt~l,uIn the above equations, u(_> 'n) denotes thellllIlll)cr of morph(mms in a Se(ltlell(;e ('orre-spending the given word sequ('ncc, c denotesa morl)heme-mfit tag, 'm.
denotes a morl)heme ,aim p denotes a type of transition froln the pre-v ious  tag to the current ag.
p can have one oftwo values, "#" denoting a transition across aword bomldary and "+" denoting a transitionwithin a word.
Be(-ause it is difficult to calculateEqn.
6, the word sequence term 'w~,,, is usuallyignored as ill Eqn.
7.
Instead, we introduce p inEqn.
7 to consider word-spacing 3.Tile probability Pr(cj ,~L, P2,u, 'm,~ ,u) is also bro-ken down into Eqn.
8 t)3r using the chain rule.Pr(c~ ,,,, P2,,, , 'm, , ,,,,)f l  ( \])r(ci,Pi \[ cl,i-l,P2,i-l,'lnl,i-l) )~- X P1"(1~'1,i \[('d,i,I,2,i,17tl,i_\]) (8) i=1\]3('caus(' Eqn.
8 is not easy to (;omlmte ~it issinll)lified by making a Marker assmnt)tion toget; a more tractal)le forlll.In a similar way to the case of word-unit; tag-ging, lexicalize(t HMMs for morl)heme-mfit tag-ging are defined by making a less strict Markovassunq)tion, as tblh)ws:A(C\[,q(K,.\]), AJ\[sI(L,1)) 1= Pr(c\],,,,p2,,,, 'mq,~,)I'r(c \[,pd I ,,I,i-,Uc/--lC/-' (!
))~=~, x l ' r (mi l c i  l,,i\[,>-L+l,,i\],'mi-l,i--I)In models A(C\[.q(tc,,I),M\[q(L,Q), the 1)robal)il-ity of the (:urrent mori)heme tag ci dependson l)oth the 1)revious K |:ags Ci_K,i_ 1 (oi)tion-ally, th(' tyl)eS of their transition Pi-K~ 1,i-~)a.n(l the 1)revious ,\] morl)hemes H~,i_.l,i_ 1 all(1the probability of the current mort)heine 'm,i (t(>1)en(ls on the current, tag and I:he previous Ltags % l,,i (optional\]y, the typ('~s of their tran-sition Pi -L-t-I,i) and the 1)revious I morl)hemes?lti--l,i-1.
~()~ t\]l(ly ('&ll &lSO (-onsid(,r h;xi(-al in-formation.In a lexicalized model A(C,.
(~#), M(~,2)) whea:eword-spa(:ing is considered only in the tag prob-al)ilities, for example, the 1)rol)al)ility of a nod(;"S'u/NNBG" of the most likely sequence in Fig-urc 2 is calculated as follows:Pr(NNBG, # \[ Vl4 EFD, +, Ha, l)x Pr(gu \[ VV, EFD, NNBG,  Ha, l)3.3  Parameter  es t imat ionIn supervised lcarning~ the simpliest parameterestimation is the maximum likelihood(ML) cs-t imation(Duda et al, 1973) which lnaximizesthe i)robal)ility ot!
a training set.
The ML esti-mate of tag (K+l ) -gram i)robal)ility, PrML (f;i \[t,i-K,i-i), is calculated as follows:P Pr(ti l ti_ir,i_j) __ \]: q ( t i - i ( , i )  (10)ML Fq(ti-lGi-l)aMost 1)rcvious HMM-bascd Korean taggcrs except(Kim et al, 1998) did not consider word-spacing.483where the flmction Fq(x) returns the fl:equencyof x in the training set.
When using the max-imum likelihood estimation, data sparseness imore serious in lexicalized models than in non-lexicalized models because the former has evenmore parameters than the latter.In (Chen, 1996), where various smoothingtechniques was tested for a language modelby using the perplexity measure, a back-offsmoothing(Katz, 1987) is said to perform bet-ter on a small traning set than other methods.In the back-off smoothing, the smoothed prob-ability of tag (K+l ) -gram PrsBo(ti \[ ti-l~,i-l)is calculated as tbllows:Pr (ti \[ ti-I(,i-~) =,5'1~20drPrML(ti \[ti-I(,i-1) " if r>0 (11)c~(ti-K,i-1) Prsso(ti \[ ti-K+l,i-l)if r = 0where r = Fq(ti_t(,i), r* = ( r+ 1)'nr+l7~, rr* (r+l.)
x~%.+ldr  ~ F l t l1-  (r+l)xm.+ln ln,.
denotes the nmnber of (K+l ) -gram whosefrequency is r, and the coefficient dr is calledthe discount ratio, which reflects the Good-~lhtring estimate(Good, 1953) 4.
Eqn.
11 meansthat Prxgo(ti \[ ti-K,i-l) is under-etimated bydr than its maximum likelihood estimate, ifr > 0, or is backed off by its smoothing termPrsuo(ti \[ ti-K+j,i-l) in proportion to thevalue of the flmction (~(ti-K,i-t) of its condi-tional term ti-K,i-1, if r = 0.However, because Eqn.
11 requires compli-cated computation in ~(ti-l(,i-1), we simI)lifyit to get a flmction of the frequency of a condi-tional term, as tbllows:ct(Fq(ti-K,i-1) = f) =E\[Fq(ti-I(,i-1) = f\] Ax E7-o E\[Fq(ti-K,i-1) -= f\]where A = 1 - ~ Pr (tglti-/c,i-,),SBO ti--K,i~r>OE\[Fq(ti-g,i-1) = f\] =SP\]to ( ti \[ti-K + l,i-1)t i -  K + L i,r=O,F q( t i -  K, i -1)= f '(12)In Eqn.
12, the range of .f is bucketed into 74Katz  said that  d,.
= i if r > 5.regions such as f = 0, 1, 2, 3, 4, 5 and f > 6 sinceit is also difficult to compute this equation tbrall possible values of f .Using the formalism of our simplified back-offsmoothing, each of probabilities whose ML es-timate is zero is backed off by its correspondingsmoothing term.
In experiments, the smooth-ing terms of Prsl~o(ti \[ ti-K,i-l,~t)i-,l,i-l) aredetermined as follows:PI'sBo(ti\[ ti-Ii+l,i-h )if K> 1,d> 1wi_j+~,i_~Prsuo(ti i fK  >_ 1, d = 1Prs13o(ti \[ ti-K+Li-l) if K > 1, J = 0PrAD(ti) if K = 0, J = 0Also, the snloothing terms of' Pl's\]~o(witi_L,i, Wi_l,i_ 1 ) are determined as follows:\[ Prst~o(wiPrsuo  (wiPrs,o (wiPrsBO(Wi)PrA.O i)ti-L+~,i, ) if L _> 1, I>  1i l ) i - I+ l  , i - Iti-L,i) if L _> 1, I = 1ti-L+Li) if L >_ 1, I = 0i f L  = 0, I --= 0i l L  = -1 ,  I = 0In Eqn.
13 and 14, the smoothing term of aunigram probability is calculated by using anadditive smoothing with 5 = 10 .2 which is cho-sen through experiments.
The equation for theadditive smoothing(Chen, 1996) is as tbllows:Fq(ti-t(,i) + 5AD ~tl (Fq(ti-lf,i) + 5)In a similar way, the smoothing terms of param-eters in Eqn.
9 ~re determined.3.4 Model  decodingh'om the viewpoint of the lattice structure, thet)roblem of POS tagging can be regarded as theproblem of finding the most likely path ti'om thestart node ($/$) to the end node ($/$).
TheViterbi search algorithm(Forney, 1973), whichhas been used for HMM decoding, can be effec-tively applied to this task just with slight mod-ification 5.4 Exper iments4.1 EnvironmentIn experiments, the Brown corpus is used tbrEnglish POS tagging and the KUNLP corpus'%uch modification is explained in detail in (Lee,1999).
(13)(14)484NW 1,113,189NSNTDARUABrown KUNLP167,11553,885 15,21182 651.64 3.4:161.54% 26.72%NW Number of words.
NS Number of sen-tcnccs.
NT Numl){'.r of tags (nlorpheme-unittag for KUNLP).
DA Degree of mnbiguity(i.e.
the number of tags per word).
RUA1\].atio f mlanlbiguous words.Table 1: Intbrmat ion al)out the Brown eortmsand the KUNLP tort}usInside-test ()utside-|;(;stML 95.57 94 .97= 1)-AD(a - \](}- \])AD(~ = 1{}2T)  -ADO; - -  =a)A\]) ( ( ;  =AD(5  =AD(5 =AD(5 = \]\]}-~7)-AD(5 =93.92 93.0295.02 94.7995.42 95.0895.55 95.0595.57 94.9895.57 94.94 :95.57 94.9195.57 94.8995.57 94.87SBO 95.55 95.25ML Maximum likelihood estimate (with sim-ple smoothing).
A\]) Additiv(~ smoothing.SBO Sinll}liticd 1)ack-off smootlfing.lal)l(, 2: lagging accura(:y (}f A(C(\]:o), M0}:0 ))for Kore~m POS tagging.
Table 1 shows someintbrmation M)out 1}oth (:ori)ora {~.
Each of themwas segmented into two parts, the training setof 90% and the test; set of 10%, ill. the way thateach sentence in the test set was extra{'tc, d \]i'()lnevery 1(} senl;ellce.
A(:cording to Tabl(!
1, Ko-reml is said to 1)e lllOre (litli(:ult to disambiguat(;tl\]ml English.We assmne "closed" wmabulary for Englishand "open" vocabulary for Korean since we donot h~ve any Engl ish morphological  mmlyzerconsistent with the Brown corlms.
Therefore,for morphological mmlysis of English, we justaNote that some sentcnc('.s, which have coml}ositetags(such as "HV+TO" in "hafta") ,  "ILLEGAL" tag,or "NIL" tag~ were remov(M fronl the Brown corl)us andtags with "*" (not) such as "BEZ*" were r(',l)la(:(~(t 1)y (:ofr{~st}o\]ttling ta s without "*" such as "BEZ".2M1.5MIM(}.5MI I I I I- MLAD .x .
-SBO1,02 ,01 ,02 ,01 ,02 ,0{},(} 0 ,01  ,(} 1 ,02 , (}  2 ,0\ ] '  - - I  I \[ I I.99.98.97 _ I~L~ ~_?1,02 ,01 ,023} 13} 2,{1(},0 {},{} 1 ,01 ,1}  2 ,02 ,0.98.972)6.
(,):, ?vii, -r J--AD '?- -.
:)4 SBO -~---1,02,01,02,(11,02,0o,00,01,01,02,02,01..99.98\[(.97.96I I I I I i t ~I I I I I t t } I I I I I1,11,11,01,12,01,12,22,22,22,21,02,01,12,20,01,01,12,0 1,1 1,1 0,01,0 1,12,0 2,2 2,2 2,2 2,2(a) # of paraln{;tersM\], -D-AD -?- - -SB( )I I I I I I I I I I I I I1 , 1 1, l 1,0 1,1 2,01,1 2,22,22,22,2 1 ,(} 2,01,12,20,01 0 1,12,01,11,10,01,01 l 2,02,22,22,22,2(1)} Inside-test1,11,I 1,01,12,01,I 2,22,22,22,21,02,01,12,20 01,01,12,01,11,10,01,01,12,02,22,22,22,2(c) Ouiside-test1,02,01,02,01,02,0 1,11,11,01,12,01,12,22,22,22,21,02,01,12,20,00,01,0 1,02,02,0 0,01,01,l  2,01,11,10,(11,{11,I 2,02,22,22,22,2(d) inside vs. outside-test in SBOFigure 3: Results of English tagging485looked up the dictionary tailored to the Browncorpus.
In case of Korean, we have used a Ko-rean morphological analyzer(Lee, 1999) whichis consistent with the KUNLP corpus.4.2  Resu l t s  and  eva luat ionTable 2 shows the tagging accuracy of the sim-plest HMM, A(C(l:0),M(0:0)), for Korean tag-ging, according to various smoothing meth-ods 7.
Note that ML denotes a simple smooth-ing method where ML estimates with prob-ability less than 10 -9  a re  smoothed and re-placed by 10-9?
Because, in the outside-test,AD(d = 10 -2) performs better than ML andkD(a ?
10-2), we use 5 = 10 -2 in our ad-ditive smoothing.
According to Table 2, SBOI)ertbrms well even in the simplest HMM.Figure 3 illustrates 4 graphs'about the resultsof English tagging: (a) the number of param-eters in each model, (b) the accuracy of eachmodel tbr the training set, (c) the accuracy ofeach model for the test set, and (d) the accuracyof each model with SBO tbr both training andtest set.
Here, labels in x-axis sI)ecify modelsK,  ,1 in the way that ~ denotes A(T(\];,j) , W(Lj)).Therefore, the first 6 models are non-lexicalizedmodels and tile others are lexicalized models.Actually, SBO uses more parameters thanothers.
The three smoothing methods, ML,AD, SBO, perform well for the training set;since the inside-tests usually have little datasparseness.
On the other hand, tbr the un-seen test set, the simple methods, ML andAD, cannot mitigate the data sparseness prob-lem, especially in sophisticated models.
How-ever, our method SBO can overcome the prob-lem, as shown in Figure 3(c).
Also, we cansee in Figure 3(d) that some lexicalized mod-els achieve higher accuracy than non-lexicalizedmodels.
We can say that the best lexicalizedmodel, A(T(1,~),W(1,1)) using SBO, improvedthe simple bigram model, A(T(L0),W(0,0)) us-?
~ 0 mg SBO, from 97.19>/o to 97.87~ (the error re-duction ratio of 24.20%).
Interestingly, somelexicalized models (such as A(T(1,1), W-(0,0)) andA(T(1,1), W(1,o))), which have a relatively smallnumber of paranmters, perform better thannon-lexicalized models in the case of outside-tests using SBO.
Untbrtunately, we cannot ex-r Ins ide - tes t  means  an  exper iment  on  the  t ra in ing  seti t se l f  and  outs ide - tes t  an  exper iment  on  the  tes t  se t ..96.94 ~ ?
.
.~  uu .
X ? ""
" ~ '  " .~1%~ ~.92.90ML ~ k.88 AD .x.
-SBO.86 I I I I I I I I I I f I I I I I I I1,02,01,02,01,02,0 1,11,11,01,12,01,12,22,22,22,21,02,01,12,20,00,01,01,02,02~0 0,01,01,12,01,11,10,01,01,12,02,22,22,22,2(a) Outside-test?
97 I I I I I I ~ I I I d~ I I I t I I -~C,M + ?.966 C~,/l~/ +.9(;2 ~.~, -~I~ X \[\]+1,02,01,02,01102,0 1,11,11,01,12,01,12,22,22,22,21,02,01,12,20,00,01,01,02,02,0 0,01,01,12,01,11,10,01,01,12,02,22,22,22,2(b) Considering word-spacing+x?x\ [ \ ]  ?
?I l l l l l l lFigure 4: Results of Korean taggingpect the result of outside-tests from that ofinside-tests because there is no direct relationt)etween themFigm:e 4 includes 2 graphs about the re-sults of Korean tagging: (a) the outside ac-curacy of each model A(C(K,j),MiL,I)) and(b) the outside accnracy of each modelA(C\[s\](~-g),M\[s\](L,0) with/without consideringword-spacing when using SBO.
Here, labels inK,J de-  x-axis specify models in the way that ,7,,notes A(C\[s\](K,j),i~/I\[.~\](Lj)) and, tbr example,C , ,M in (b) denotes k(C~(,r,j), M(L,r)).As shown in Figure 4, the simple meth-ods, ML and AD, cannot mitigate that sparse-data problem, t)ut our method SBO can over-come it.
Also, some lexicalized models per-tbrm better than non-lexicalized models.
Onthe other hand, considering word-spacing ivesgood clues to the models sometimes, but yetwe cannot sw what is the best ww.
Fromthe experimental results, we can say that thebest model, A(C(9,2),M(2,2)) using SBO, im-proved the previous models, A(C(1,0), M(o,o)) us-486ing ML(Lee, 1995), and A(G(,,0), M(0,0))usingML(Kim et al, 1998), t'ronl 94.97% and 95.05%to 96.98% (the error reduction ratio of 39.95%mid 38.99%) respectively.5 Conc lus ionWe have 1)resented unitbrmly lexicalized HMMsfor POS tagging of English and Korean.
Inthe models, data sparseness was etlix:tively mit-igated by using our simplified ba(-k-ofl" smooth-ing.
From the ext)eriments, we have ol)servedthat lexical intbrmation is usefifl fi)r POS tag-ging in HMMs, as is in other models, andore" lexicalized models improved non-lexicalizedmodels by the error reduction ratio of 24.20%(in English tagging) and 39.95% (in Korean tag-ging).G('.nerally, the mfiform extension of modelsrequires ral)id increase of parameters, and hencesuffers fl'om large storage a.nd sparse data.
l~.e-cently in many areas where HMMs are used,many eflbrts to extend models non-mfitbrmlyhave been made, sometimes resulting in notice-able improvement.
For this reason~ we are try-ing to transfbnn our uniform models into non-mliform models, which may 1)e more effectivein terms of both st)ace (:omt)h'~xity and relial)leestimation of I)areme|;ers, without loss of accu-racy.Re ferences12.
Brill.
1994.
Some Advances in~l?ansformation-B ased Part of St)eech~Dtgging.
In P~ve.
of the 12th, Nat'l Cm?.
onArt'tficial hdelligencc(AAAI-.9~), 722-727.E.
Charniak, C. Hendrickson, N. Jacobson, andM.
Perkowitz.
1993. l~3quations for Part-o f  Speech %~gging.
In Proc, of the 11th,Nat'l CoT~:f. on Artificial Intclligence(AAAL93), 784-789.S.
F. Chen.
1996.
Building Probabilistic Modelsfor Natural Language.
Doctoral Dissert~tion,Harvard University, USA.R.
O. Duda and R. E. Hart.
1973.
Pattern CIas-s'~fication and Scene Analysis.
John Wiley.G.
D. Forney.
1973.
The Viterbi Algorithm.
IllProc.
of the IEEE, 61:268-278.W.
N. Francis and H. Ku~era.
1982.
Fre-quency Analysis of English Usage: Lcziconand GTnmmar.
Houghton Mitltin Coral)any ,Boston, Massachusetts.I.
J.
Good.
1953.
"The Population Frequen-cies of Species and the Estimation of Pop-ulation Parameters," Ill Biometrika, 40(3-4):237-264.S.
M. Katz.
1987.
Estimation of Probabilitiesfronl Sparse Data for the Language ModelComponent of a Speech Recognizer.
In IEEETransactions on Acoustics, Speech, and Signali'rocessing(ASSl'), 35(3):400-401.J.-\]).
Kim, S.-Z.
Lee, and H.-C. Rim.
1998.A Morpheme-Unit POS Tagging Model Con-sidering Word-Spacing.
Ill Pwc.
of th.e I0 thNational CoT~:fercnce on Korean h~:formationPTveessing, 3-8.J.-D. Kim, S.-Z.
Lee, and H.-C. Rim.
1999.HMM Specialization with Selective Lexi-calization.
In Pwe.
of the joint SIGDATCo~l:h':rence on Empirical Methods in Nat-'aral Language Processing and Very La'qtcCo'rpora(EMNLP- VL C-99), ld4-148.J.-H. Kim.
1996.
Lcxieal Disambig'aation withError-Driven Learning.
Doctoral Disserta-tion, Korea Advanced Institute of Science andTe.clmology(KAIST), Korea.S.-H. Lee.
1995.
Korean POS Tagging SystemConsidering Unknown Words.
Master The-sis, Korea Advanced Institute of Science andTeclmology(KAIST), Korea.S.-Z.
Lee, .I.-D. Kim, W.-H. Ryu, and H.-C. Rim.
1999.
A Part-of Speech TaggingModel Using Lexical l/.ules Based on CorlmsStatistics.
In Pwc.
of the International Con-ference on Computer \])'lvcessin 9 of OrientalLanguages(lCCPOL-99), 385-390.S.-Z.
Lee.
1999.
New Statistical Models for Au-tomatic POS Tagging.
Doctoral Dissertation,l(orea University, Korea.B.
Merialdo.
1991.
Tagging Text with a Prol)-abilisl;ic Model.
In P~vc.
of the InternationalConference on Acoustic, Spccch and SignalProcessing(ICASSP-91), 809-812.A.
Ratnap~rkhi.
1996.
A Maximum Entrol)yModel tbr Part-of-Speech Tagging.
In Proe.of the Empirical Methods in Natural Lan-guage P~vcessi'ng Co'a:fercnce(EMNLP-9b'),133-142.487
