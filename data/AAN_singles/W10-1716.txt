Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 121?126,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsLIUM SMT Machine Translation System for WMT 2010Patrik Lambert, Sadaf Abdul-Rauf and Holger SchwenkLIUM, University of Le Mans72085 Le Mans cedex 9, FRANCEFirstName.LastName@lium.univ-lemans.frAbstractThis paper describes the development ofFrench?English and English?French ma-chine translation systems for the 2010WMT shared task evaluation.
These sys-tems were standard phrase-based statisti-cal systems based on the Moses decoder,trained on the provided data only.
Mostof our efforts were devoted to the choiceand extraction of bilingual data used fortraining.
We filtered out some bilingualcorpora and pruned the phrase table.
Wealso investigated the impact of adding twotypes of additional bilingual texts, ex-tracted automatically from the availablemonolingual data.
We first collected bilin-gual data by performing automatic trans-lations of monolingual texts.
The secondtype of bilingual text was harvested fromcomparable corpora with Information Re-trieval techniques.1 IntroductionThis paper describes the machine translation sys-tems developed by the Computer Science labora-tory at the University of Le Mans (LIUM) for the2010 WMT shared task evaluation.
We only con-sidered the translation between French and En-glish (in both directions).
The main differenceswith respect to previous year?s system (Schwenket al, 2009) are as follows: restriction to the datarecommended for the workshop, usage of the (fil-tered) French?English gigaword bitext, pruning ofthe phrase table, and usage of automatic trans-lations of the monolingual news corpus to im-prove the translation model.
We also used a largeramount of bilingual data extracted from compara-ble corpora than was done in 2009.
These differentpoints are described in the rest of the paper, to-gether with a summary of the experimental resultsshowing the impact of each component.2 Resources UsedThe following sections describe how the resourcesprovided or allowed in the shared task were usedto train the translation and language models of thesystem.2.1 Bilingual dataOur system was developed in two stages.
First,a baseline system was built to generate automatictranslations of some of the monolingual data avail-able.
These automatic translations may be useddirectly with the source texts to build additionalbitexts, or as queries of an Information Retrieval(IR) system to extract new bitexts from compara-ble corpora.
In a second stage, these additionalbilingual data were incorporated to the system (seeSection 4 and Tables 1 and 2).The latest version of the News-Commentary(NC) corpus, of the Europarl (Eparl) corpus (ver-sion 5), and of the United Nations (UN) corpuswere used.
We also took as training data a sub-set of the French?English Gigaword (109) cor-pus.
Since a significant part of the data wascrawled from the web, we thought that many sen-tence pairs may be only approximate translationsof each other.
We applied a lexical filter to dis-card them.
Furthermore, some sentences of thiscorpus were extracted from web page menus andare not grammatical.
Although we could haveused a part of the menu items as a dictionary, forsimplicity we applied an n-gram language model(LM) filter to remove all non-grammatical sen-tences.
Thanks to this filter, sentences out of thelanguage model domain (in this case, mainly thenews domain), may also have been discarded be-cause they contain many unknown or unfrequentn-grams.
The lexical filter was based on the IBMmodel 1 cost (Brown et al, 1993) of each side ofa sentence pair given the other side, normalisedwith respect to both sentence lengths.
This filter121was trained on a corpus composed of Eparl, NC,and UN data.
The language model filter was ann-gram LM cost of the target sentence (see Sec-tion 3), normalised with respect to its length.
Thisfilter was trained with all monolingual resourcesavailable except the 109 data.
We generated a firstsubset, 1091, selecting sentence pairs with a lexi-cal cost inferior to 4 and an LM cost inferior to2.3.
The corpus selected in this way contains 115million words in the English side (out of 580 mil-lion in the original corpus).
Close to the evaluationdeadline we decided to generate a second corpus(1092) by raising the LM cost threshold to 2.6.
The1092 corpus contains 232 million words on the En-glish side (twice as much as in the 1091 corpus).In the French side of the bilingual corpora, forthe French?English direction only, the contrac-tions ?du?
(?of the?
), ?au?
and ?aux?
(?to the?
singu-lar and plural) were substituted by their expandedforms (?de le?, ?a` le?
and ?a` les?
).2.2 Use of Automatic Translations andComparable corporaAvailable human translated bitexts such as the UNcorpus seem to be out-of domain for this task.We used two types of automatically extracted re-sources to adapt our system to the task domain.First, we generated automatic translations of theFrench News corpus provided (231M words), andselected the sentences with a normalised transla-tion cost (returned by the decoder) inferior to athreshold.
The resulting bitext has no new wordsin the English side, since all words of the transla-tion output come from the translation model, butit contains new combinations (phrases) of knownwords, and reinforces the probability of somephrase pairs (Schwenk, 2008).Second, as in last year?s evaluation, we auto-matically extracted and aligned parallel sentencesfrom comparable in-domain corpora.
This yearwe used the AFP and APW news texts since thereare available in the French and English LDC Gi-gaword corpora.
The general architecture of ourparallel sentence extraction system is described indetail by Abdul-Rauf and Schwenk (2009).
Wefirst translated 91M words from French into En-glish using our first stage SMT system.
These En-glish sentences were then used to search for trans-lations in the English AFP and APW texts of theGigaword corpus using information retrieval tech-niques.
The Lemur toolkit (Ogilvie and Callan,2001) was used for this purpose.
Search was lim-ited to a window of ?5 days of the date of theFrench news text.
The retrieved candidate sen-tences were then filtered using the Translation Er-ror Rate (TER) with respect to the automatic trans-lations.
In this study, sentences with a TER be-low 65% for the French?English system and 75%for the English?French system were kept.
Sen-tences with a large length difference (French ver-sus English) or containing a large fraction of num-bers were also discarded.
By these means, about15M words of additional bitexts were obtained toinclude in the French?English system, and 21Mwords to include in the English?French system.Note that these additional bitexts do not dependon the translation direction.
The most suitableamount of additional data was just different inthe French?English and English?French transla-tion directions.2.3 Monolingual dataThe French and English target language modelswere trained on all provided monolingual data.
Inaddition, LDC?s Gigaword collection was used forboth languages.
Data corresponding to the devel-opment and test periods were removed from theGigaword collections.2.4 Development dataAll development was done on news-test2008, andnewstest2009 was used as internal test set.
For allcorpora except the French side of the bitexts usedto train the French?English system (see above),the default Moses tokenization was used.
How-ever, we added abbreviations for the French tok-enizer.
All our models are case sensitive and in-clude punctuation.
The BLEU scores reported inthis paper were calculated with the multi-bleu.perltool and are case sensitive.
The BLEU scorewas one of metrics with the best correlation withhuman ratings in last year evaluation (Callison-Burch et al, 2009) for the French?English andEnglish?French directions.3 Architecture of the SMT systemThe goal of statistical machine translation (SMT)is to produce a target sentence e from a sourcesentence f .
It is today common practice to usephrases as translation units (Koehn et al, 2003;Och and Ney, 2003) and a log linear framework inorder to introduce several models explaining the122translation process:e?
= argmaxep(e|f)= argmaxe{exp(?i?ihi(e, f))} (1)The feature functions hi are the system mod-els and the ?i weights are typically optimized tomaximize a scoring function on a developmentset (Och and Ney, 2002).
In our system fourteenfeatures functions were used, namely phrase andlexical translation probabilities in both directions,seven features for the lexicalized distortion model,a word and a phrase penalty and a target languagemodel (LM).The system is based on the Moses SMT toolkit(Koehn et al, 2007) and constructed as follows.First, word alignments in both directions are cal-culated.
We used a multi-threaded version of theGIZA++ tool (Gao and Vogel, 2008).1 This speedsup the process and corrects an error of GIZA++that can appear with rare words.Phrases and lexical reorderings are extractedusing the default settings of the Moses toolkit.The parameters of Moses were tuned on news-test2008, using the ?new?
MERT tool.
We repeatedthe training process three times, each with a differ-ent seed value for the optimisation algorithm.
Inthis way we have an rough idea of the error intro-duced by the tuning process.4-gram back-off LMs were used.
The wordlist contains all the words of the bitext used totrain the translation model and all words that ap-pear at least ten times in the monolingual corpora.Words of the monolingual corpora containing spe-cial characters or sequences of uppercase charac-ters were not included in the word list.
SeparateLMs were build on each data source with the SRILM toolkit (Stolcke, 2002) and then linearly in-terpolated, optimizing the coefficients with an EMprocedure.
The perplexities of these LMs were103.4 for French and 149.2 for English.4 Results and DiscussionThe results of our SMT system for the French?English and English?French tasks are summarizedin Tables 1 and 2, respectively.
The MT metricscores are the average of three optimisations per-formed with different seeds (see Section 3).
The1The source is available at http://www.cs.cmu.edu/?qing/numbers in parentheses are the standard deviationof these three values.
The standard deviation givesa lower bound of the significance of the differencebetween two systems.
If the difference betweentwo average scores is less than the sum of the stan-dard deviations, we can say that this difference isnot significant.
The reverse is not true.
Note thatmost of the improvements shown in the tables aresmall and not significant.
However many of thegains are cumulative and the sum of several smallgains makes a significant difference.Phrase-table PruningWe tried to prune the phrase-table as proposed byJohnson et.
al.
(2007), and available in moses(?sigtest-filter?).
We used the ?
?
 filter2.
Aslines 3 and 4 of Table 1, and lines 3 and 4 of Ta-ble 2 reveal, in addition to the reduction 43% ofthe phrase-table, a small gain in BLEU score (0.15and 0.11 respectively) was obtained with the prun-ing.Baseline French?English SystemThe first section of Table 1 (lines 1 to 5) shows re-sults of the development of the baseline SMT sys-tem, used to generate automatic translations.
Al-though being out-of-domain data, the introductionof the UN corpus yields an improvement of oneBLEU point with respect to Eparl+NC.
Adding the1091 corpus, we gain 0.7 BLEU point more.
Ac-tually, we obtained the same score with the 1091added directly to Eparl+NC (line 5).
However, wechoose to include the UN corpus to generate trans-lations to have a larger vocabulary.
The systemhighlighted in bold (line 4) is the one we chooseto generate our English translations.Although no French translations were gener-ated, we did similar experiments in the English?French direction (lines 1 to 4 of Table 2).
In thisdirection, the 1091 corpus is still more valuable thanthe UN corpus when added to Eparl+NC, but withless difference in terms of BLEU score.
In this di-2The p-value of two-by-two contingency tables (describ-ing the degree of association between a source and a targetphrase) is calculated with Fisher exact test.
This probabilityis interpreted as the probability of observing by chance an as-sociation that is at least as strong as the given one, and henceas its significance.
An important special case of a table oc-curs when a phrase pair occurs exactly once in the corpus,and each of the component phrases occurs exactly once in itsside of the parallel corpus (1-1-1 phrase pairs).
In this casethe negative log of the p-value is ?
= logN (N is number ofsentence pairs in the corpus).
?
?
 is the largest thresholdthat results in all of the 1-1-1 phrase pairs being included.123rection, we obtain a gain by adding the UN corpusto Eparl+NC+1091.Filtering the 109 CorpusLines 5 to 7 of Table 1 show the impact of filteringthe 109 corpus.
The system trained on the full 109corpus added to Eparl+NC achieves a BLEU scoreof 26.83.
Substituting the full 109 corpus by 1091 (5times smaller), i.e.
using the first filtering settings,we gain 0.13 BLEU point.
Using 1092 instead of1091, we gain another 0.16 BLEU point, that is 0.3in total.
With respect to not using the 109 data atall (as we did last year), we gain 0.8 BLEU point.Impact of the Additional BitextsWith the baseline French?English SMT system(see above), we translated the French News cor-pus to generated an additional bitext (News).
Wealso translated some parts of the French LDC Gi-gaword corpus, to serve as queries to our IR sys-tem (see section 2.2).
The resulting additional bi-text is referred to as IR.
Lines 8 to 13 of Table 1and lines 6 to 12 of Table 2 summarize the systemdevelopment including the additional bitexts.With the News additional bitext added toEparl+NC, we obtain a system of similar perfor-mance as the baseline system used to generatethe automatic translations, but with less than 30%of the data.
This holds in both translation direc-tions.
Adding the News corpus to a larger corpus,such as Eparl+NC+1091, has less impact but stillyields some improvement: 0.15 BLEU point inFrench?English and 0.3 in English?French.
Thus,the News bitext translated from French to Englishmay have more impact when translating from En-glish to French than in the opposite direction.
Notethat the number of additional phrase-table entriesper additional running word is twice as high forthe News bitext than for the other corpora.
Forexample, with respect to Eparl+NC+UN+1091 (Ta-ble 2), Eparl+NC+UN+1091+News has 56M morewords and 116M more entries in the phrase-table,thus the ratio is more than 2.
For all other cor-pora, the ratio is equal to 1 or less.
This is un-expected, particularly in this case where the Newsbitext has no new English vocabulary with respectto the Eparl+NC+UN+1091 corpus, from which itsEnglish side was generated.With the IR additional bitext added toEparl+NC, we obtain a system of similar perfor-mance as the system trained on Eparl+NC+UN,while the IR bitext is 10 times smaller than theUN corpus.
Added to Eparl+NC+1091+News, theIR bitext allows gains of 0.13 and 0.2 BLEU pointrespectively in the French?English and English?French directions.Comparing the systems trained onEparl+NC+1091 or Eparl+NC+1092 to the sys-tems trained on the same corpora plus News+IR,we can estimate the cumulative impact of theadditional bitexts.
The gain is around 0.3 BLEUpoint for French?English and around 0.5 BLEUpoint for English?French.Final SystemIn both translation directions our best system wasthe one trained on Eparl+NC+1092+News+IR.
Wefurther achieved small improvements (0.3 BLEUpoint) by pruning the phrase-table (as above) andby using a language model with no cut-off togetherwith increasing the beam size and/or the maxi-mum number of translation table entries per inputphrase.
Note that the English LM with cut-off hada size of 6G, and the one with no cut-off had asize of 29G.
It was too much to fit in our 72Gmachines so we pruned it with the SRILM prun-ing tool down to a size of 19G.
The French LMwith cut-off had a size of 2G and the one withno cut-off had a size of 9G.
These sizes corre-spond to the binary format.
Taking as example theFrench?English direction, the running time wentfrom 8600 seconds for the system of line 14 (witha threshold pruning coefficient of 0.4 and a LMwith cut-off) to 28200 seconds for the system sub-mitted (with the LM without cut-off pruned by theSRILM tool and a threshold pruning coefficient of0.00001).5 Conclusions and Further WorkWe presented the development of our machinetranslation system for the French?English andEnglish?French 2010 WMT shared task.
Our sys-tem was actually a standard phrase-based SMTsystem based on the Moses decoder.
Its original-ity mostly lied in the choice and extraction of thetraining data used.We decided to use a part of the 109 French?English corpus.
We found this resource useful,even without filtering.
We nevertheless gained 0.3BLEU point by selecting sentences based on anIBM Model 1 filter and a language model filter.We pruned the phrase table with the ?sigtest-filter?
distributed in Moses, yielding improve-124Bitext #Fr Words P-table Mem news-test2008 newstest2009(M) size (M) (G) BLEU BLEU1 Eparl+NC 52 66 19.3 22.80 (0.03) 25.31 (0.2)2 Eparl+NC+UN 275 250 22.8 23.38 (0.1) 26.30 (0.2)3 Eparl+NC+UN+1091 406 376 25.1 23.81 (0.05) 27.0 (0.2)4 Eparl+NC+UN+1091 pruned 406 215 21.4 23.96 (0.1) 27.15 (0.18)5 Eparl+NC+1091 183 198 22.1 23.83 (0.07) 26.96 (0.04)6 Eparl+NC+1092 320 319 24.1 23.95 (0.03) 27.12 (0.1)7 Eparl+NC+109 733 580 29.5 23.65 (0.09) 26.83 (0.2)8 Eparl+NC+News 111 188 19.5 23.46 (0.1) 26.95 (0.2)9 Eparl+NC+1091+News 242 317 22.5 23.77 (0.04) 27.11 (0.04)10 Eparl+NC+IR 68 78 19.5 22.97 (0.03) 26.20 (0.1)11 Eparl+NC+News+IR 127 198 20.1 23.62 (0.01) 27.04 (0.06)12 Eparl+NC+1091+News+IR 258 327 22.8 23.75 (0.05) 27.24 (0.05)13 Eparl+NC+1092+News+IR 395 441 24.4 23.87 (0.03) 27.43 (0.08)14 Eparl+NC+1092+News+IR pruned 395 285 62.5 24.04 27.72(+larger beam, +no-cutoff LM)Table 1: French?English results: number of French words (in million), number of entries in the phrase-table (in million), memory needed during decoding (in gigabytes) and BLEU scores in the development(news-test2008) and internal test (newstest2009) sets for the different systems developped.
The BLEUscores and the number in parentheses are the average and standard deviation over 3 values (see Section 3.
)ments of 0.1 to 0.2 BLEU point for a 43% reduc-tion of the phrase-table size.We used additional bitexts extracted automati-cally from the available monolingual corpora.
Thefirst type of additional bitext is generated with au-tomatic translations of the monolingual data witha baseline SMT system.
The second one is ex-tracted from comparable corpora, with Informa-tion Retrieval techniques.
With the additional bi-texts we gained 0.3 and 0.5 BLEU point for theFrench?English and English?French systems, re-spectively.Next year we want to perform an improved se-lection of parallel training data with re-samplingtechniques.
We also want to use a continuousspace language model (Schwenk, 2007) in an n-best list rescoring step after decoding.
Finally, weplan to train different types of systems (such asa hierarchical SMT system and a Statistical Post-Editing system) and combine their outputs withthe MANY open source system combination soft-ware (Barrault, 2010).AcknowledgmentsThis work has been partially funded bythe European Union under the EuroMatrixPlus project ?
Bringing Machine Transla-tion for European Languages to the User ?
(http://www.euromatrixplus.net, IST-2007.2.2-FP7-231720).ReferencesSadaf Abdul-Rauf and Holger Schwenk.
2009.
On theuse of comparable corpora to improve SMT perfor-mance.
In Proceedings of the 12th Conference of theEuropean Chapter of the ACL (EACL 2009), pages16?23, Athens, Greece.Lo?
?c Barrault.
2010.
MANY : Open source machinetranslation system combination.
Prague Bulletinof Mathematical Linguistics, Special Issue on OpenSource Tools for Machine Translation, 93:147?155.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?311.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the ACL Fourth Workshop on Sta-tistical Machine Translation, pages 1?28, Athens,Greece.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance for Natu-ral Language Processing, pages 49?57, Columbus,Ohio, June.
Association for Computational Linguis-tics.125Bitext #En Words Phrase-table news-test2008 newstest2009(M) size (M) BLEU BLEU1 Eparl+NC+UN 242 258 24.21 (0.01) 25.29 (0.12)2 Eparl+NC+1091 163 203 24.24 (0.06) 25.51 (0.13)3 Eparl+NC+UN+1091 357 385 24.46 (0.08) 25.73 (0.20)4 Eparl+NC+UN+1091 pruned 357 221 24.42 (0.1) 25.84 (0.05)5 Eparl+NC+1092 280 330 24.43 (0.04) 25.68 (0.12)6 Eparl+NC+News 103 188 24.27 (0.2) 25.70 (0.15)7 Eparl+NC+1091+News 218 321 24.51 (0.05) 25.83 (0.05)8 Eparl+NC+UN+1091+News 413 501 24.70 (0.1) 25.86 (0.14)9 Eparl+NC+IR 69 81 24.14 (0.05) 25.17 (0.2)10 Eparl+NC+News+IR 124 201 24.32 (0.12) 25.84 (0.17)11 Eparl+NC+1091+News+IR 239 333 24.54 (0.1) 26.03 (0.15)12 Eparl+NC+1092+News+IR 356 453 24.68 (0.04) 26.19 (0.05)13 Eparl+NC+1092+News+IR pruned 356 293 25.06 26.53(+larger beam, +no-cutoff LM)Table 2: English?French results: number of English words (in million), number of entries in the phrase-table (in million) and BLEU scores in the development (news-test2008) and internal test (newstest2009)sets for the different systems developped.
The BLEU scores and the number in parentheses are theaverage and standard deviation over 3 values (see Section 3.
)Howard Johnson, Joel Martin, George Foster, andRoland Kuhn.
2007.
Improving translation qual-ity by discarding most of the phrasetable.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 967?975, Prague, Czech Republic.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrased-based machine translation.In HLT/NACL, pages 127?133.Philipp Koehn et al 2007.
Moses: Open source toolkitfor statistical machine translation.
In ACL, demon-stration session.Franz Josef Och and Hermann Ney.
2002.
Discrim-inative training and maximum entropy models forstatistical machine translation.
In Proc.
of the An-nual Meeting of the Association for ComputationalLinguistics, pages 295?302.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignementmodels.
Computational Linguistics, 29(1):19?51.Paul Ogilvie and Jamie Callan.
2001.
Experimentsusing the Lemur toolkit.
In In Proceedings of theTenth Text Retrieval Conference (TREC-10), pages103?108.Holger Schwenk, Sadaf Abdul Rauf, Lo?
?c Barrault,and Jean Senellart.
2009.
SMT and SPE machinetranslation systems for WMT?09.
In Proceedings ofthe Fourth Workshop on Statistical Machine Trans-lation, pages 130?134, Athens, Greece.
Associationfor Computational Linguistics.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech and Language, 21:492?518.Holger Schwenk.
2008.
Investigations on large-scale lightly-supervised training for statistical ma-chine translation.
In IWSLT, pages 182?189.A.
Stolcke.
2002.
SRILM: an extensible languagemodeling toolkit.
In Proc.
of the Int.
Conf.
on Spo-ken Language Processing, pages 901?904, Denver,CO.126
