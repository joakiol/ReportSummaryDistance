The Effect of Topological  Structure on HierarchicalText CategorizationStephen D'Aless io  Ke i tha  MurrayRober t  SchiaffinoDepar tment  of  Computer  and In format ion ScienceIona Col legeNew Rochel le,  N.Y. 10801, USAsdalessio@iona.edu, kmurray@iona.edu,  rschiaff ino@iona.eduAaron  KershenbaumDepar tment  of  Computer  SciencePo ly techn ic  Univers i tyHawthorne ,  N.Y. 10532, USAakershen@duke.po ly .eduAbst rac tThe problem of assigning documents to categoriesin a hierarchically organized taxonomy and the ef-fect of modifying the topology of the hierarchyare considered.
Given a training corpus of doc-uments already placed in categories, vocabulary isextracted.
The vocabulary, words that appear withhigh relative frequency within a given category,characterize each subject area by being associatedwith nodes in the hierarchy.
Each node's vocabu-lary is filtered and its words assigned weights withrespect to the specific category.
Test documentsare scanned for this vocabulary and categories areranked with respect o the document based on thepresence of terms from this vocabulary.
Documentsare assigned to categories based on these rankings.Precision and recall are measured.We present an algorithm for associating wordswith individual categories within the hierarchy anddemonstrate that precision and recall can be sig-nificantly improved by solving the categorizationproblem taking the topology of the hierarchy intoaccount.
We also show that these results can beimproved even further by inteUigent'y selecting in-termediate categories in the hierarchy.
Solving theproblem iteratively, moving downward from theroot of the taxonomy to the leaf nodes, we improveprecision from 82% to 89% and recall from 82%to 87% on the much-studied Reuters-21578 corpuswith 135 categories organized in a three-level hier-archy of categories.1 In t roduct ion  and  BackgroundThe proliferation of available online information at-tributable to the explosive use of the Internet hasbrought about the necessity for text retrieval sys-tems that can assist the user in accessing this in-formation in an effective, efficient and timely man-ner.
Today's search engines have had difificultykeeping pace with the increasing amount of infor-mation that continuously needs to be indexed andsearched.
Categorization of the original text is ameans by which the information can be arrangedarid organized to facilitate the retrieval task.
Nat-ural language processing systems can be used toquery against these pre-specified categories yield-ing retrieval results more acceptable and beneficialto the user.The document categorization problem is one ofassigning newly arriving documents to categorieswithin a given hierarchy of categories.
In general,lower level categories may be part of more thanone higher level category.
Moreover, a documentmay belong to more than one low-level category.While the techniques described here can be appliedto this more general problem, the experiments wehave conducted, to date, have been carried out on acorpus where each document is a member of a sin-gle category and the categories form a tree ratherthan a more general directed acyclic graph.
Vv~ lim-ited the investigation to this more specific problemin order to focus the investigation on the effect ofmaking use of the hierarchy, specifically on changes66in the topology" of the hierarchy.Most computational experience discussed inthe literature deaJs with hierarchies that aretrees.
Indeed, until recently, most problems dis-cussed dealt with categorization within a sim-ple (non-hierarchical) set of categories (Frakesand Baeza-Yates, 1992).
The Reuters-21578corpus (available at David Lewis's home page:http://www.research.att.com/ lewis) has beenstudied extensively.
"~.ng ('~hag, 1997) compares14 categorization algorithms applied to this Reuterscorpus as a flat categorization problem on 135 cat-egories.
This same corpus has been more recentlystudied by others treating the categories as a hierar-chy" (Chakrabarti et al, 1997)(Koller and Sahami,1997)(Ng et al, 1997)(Yang, 1996).
"~.ng examinesa portion of the OHSUMED (Hersh et al, 1994)corpus of medical abstracts, a part of the NationalLibrary of Medicine corpus that has over 9 millionabstracts organized into over 10,000 categories in ataxonomy (called MESH) which is seven levels deepin some places.We describe an algorithm for hierarchical docu-ment categorization where the vocabulary and termweights are associated with categories at each levelin the taxonomy and where the categorization prro-cess itself is iterated over levels in the hierarchy.Thus a given term may be a discriminator at onelevel in the taxonomy receiving a large weight andthen become a stopword at another level in the hi-erarchy.
We also consider making modifications tothe hierarchy itself as a means of increasing the ac-curacy and speed of the categorization process.There are two strong motivations for taking thehierarchy into account.
First, experience to datehas demonstrated that both precision and recall de-crease as the number of categories increases (Apteet al, 1994) (Yang, 1996).
One of the reasons forthis is that as the scope of the corpus increases,terms become increasingly polysemous.
This is par-ticularly evident for acronyms, which are often lim-ited by the number of 3- and 4-1etter combinations,and which are reused from one domain to another.The second motivation for doing categorizationwithin a hierarchical setting is it affords the abilityto deal with very large problems.
As the numberof categories grows, the need for domain-specificvocabulary grows as well.
Thus, we quickly reachthe point where the index no longer fits in mem-ory and we are trading accuracy against speed andsoftware complexity.
On the other hand, by treat-ing the problem hierarchically, we can decomposeit into several problems each involving a smallernumber of categories and smaller domain-specificvocabularies and perhaps yield savings of severalorders of magnitude-Feature selection, deciding which terms to actu-ally include ha the indexing and categorization pro-cess, is another aspect affected by size of the corpus.Some methods remove words with low frequenciesboth in order to reduce the number of features andbecause such words are often unreliable due to thelow confidence in their distribution of occurrenceacross categories.
Depending on the size of the cor-pus, this may still leave over 10,000 features, whichrenders even the simplest categorization methodstoo slow to be of use on very large corpora andrenders the more complex ones entirely infeasible..Methods that incorporate additional feature se-lection have been studied (Apte et al, 1994)(Chakrabarti et al, 199T) (Deerwester t al.
1990)(Koller and Sahami, 1996) (Lewis, 1992) (Ng et al,1997) (~h.ng and Pederson 1997).
The effectivenessoff these feature selection methods varies.
Most re-duce the size of the feature set by one to two ordersof magnitude without significantly reducing preci-sion and recall from what is obtained with largerfeature sets.
Some approaches assign weights tothe features and then assign category ranks basedon a sum of the weights of features present.
Someweigh the features further by their frequency in thetest documents.
These methods are all known aslinear cl~sifiers and are computationally simplestand most efficient, but they sometimes lose accu-racy because of the assumption they make that thefeaaures appea~'independently i  documents.
Moresophisticated categorization methods base the cat-egory ranks on groups of terms (Chakrabarti etal., 1997) (Heckerman, 1996) (Koller and Saharni,1997) (Sahami, 1996) (Yang, 1997).
The methodsthat approach the problem hierarchically computeprobabilities and make the categorization decisionone level in the taxonomy at a time.Precision and recall are used by most authors as ameasure of the effectiveness of the algorithms.
Mostof the simpler methods achieved values for thesenear 80% for the Reuters corpus (Apte et al, 1994)(Cohen and Singer, 1996).
More computationallyexpensive methods using the same corpus achievedresults near 90% (Koller and Sahami, 1997) whilethe methods that used hierarchy obtained small inocreases in precision and large increases in speed (Nget al, 1997).
As the number of categories increasedin a corpus (OSHUMED), precision and recall de-cline to 60% (Yang 1996).67'2 Prob lem Def in i t ion2.1 Definition of CategoriesWe are given a set of categories where sets of cat-egories can be further organized into supercate-gories.
We are given a training corpus and, for eachdocument, the category to which it belongs.
Doc-uments can, in general, be members of more thanone category-.
In that case, it is possible to considera binary categorization problem where a decision ismade whether each document is or is not in eachcategory.
Here, we examine the M-ary categoriza-tion problem where we choose a single category foreach document.2.2 Document  Corpus and TaxonomyWe use the Reuters-21578 corpus, Distribution 1.0,which is comprised of 21578 documents, repr~ent-ing what remains of the original Reuters-22173 cor-pus after the elimination of 595 duplicates by SteveLynch and David Lewis in 1996.
The size of thecorpus is 28,329,337 bytes, yielding an average doc-ument size of 1,313 bytes per document.
The doc-uments are "categorized" along five axes - topics,people, places, organizations, and exchanges.
Weconsider only the categorization along the topicsaxis.
Close to half of the documents (10,211):haveno topic and as Yang (~hng, 1996) and others ug-gest, we do not include these documents in eitherour training or test sets.
Note, that unlike Lewis(acting for consistency with earlier studies), thedocuments that we consider no-category are thosethat have no categories listed between the topictags in the Reuters-21578 corpus' documents.
Thisleaves 11,367 documents with one or more topics.Most of these documents (9,49.5) have only a singletopic.
The average number of topics per documentis 1.26.The Reuters collection uses a set of 135 categoriesorganized as a flat taxonomy.
Although the collec-tion does not have a pre-defined hierarchical clas-sification structure, additional information on thecategory sets available at Lewis's site describes anorganization that has 5 additional categories thatbecome supercategories of all but 3 of the originaltopics categories.
Adding a root forms a 3-1evel hi-erarchy (see Figure 1).
Figure 1 includes countsby selected individual leaf categories and summa-rized by upper level supercategories.
The numberof categories per supercategory varies widely, froma minimum of 2 to a maximum of 78.
The numberof test documents per category also varies widely,from a minimum of 0 (for 76 such categories) to amaximum of 1,156 (earn).
On the other hand, doc-ument size does not vary greatly across categories.In the same way that a wide variation in docu-ment size makes ranking documents with respect toa query in information retrieval difficult, it is difl~-cult to accurately rank categories with respect o adocument when the number of documents per cate-gory varies greatly across categories.
Of course, wecannot control the number of documents actually ineach category.
We can reduce this variation to someextent by altering the hierarchy, as least temporar-ily, during the categorization process.
Thus, forexample, the hierarchy described in Figure 1 abovegroup the "acq" and "earn" categories into a com-mon supercategory "corporate".
Each of these cat-egories separately contains more documents thanall of the other supercategories.
Thus, we mightimprove the precision of the categorization processby "promoting" these categories tc 3upercateguries.This idea is explored in Section 4.It might also help to temporarily move a categoryto a different part of the hierarchy when it sharesimportant features with other categories there.
Inthis case, by moving the categories under a com-mon parent we can reliably get the document tothat parent and then, using features that specifi-cally separate these categories from one another, wecan accurately complete the categorization.
Mov-ing categories i also explored in Section 4.2.3 Performance Metr icsWe measure the-effectiveness of our algorithm byusing the standard measures of microaveraged pre-cision and recall; i.e., the ratio of correct decisionsto the total number of decisions and the ratio ofcorrect decisions to the total number of documents-We do, however, sometimes leave documents innon-leaf categories and then, in measuring precisionand recall, count these as "no-category", reducingrecall but not precision.3 A lgor i thm Descr ip t ion3.1 OverviewWe begin by creating training and test files us-ing the 9,495 single-category documents from theReuters-21578 corpus.
While this led to somewhathigher precision and recall than would have beenobtained by including multicategory documents,our 89% precision and 87% recall is also higher thanthe roughly 80% typically reported for categoriza-tion methods of comparable speed and complexity.68Thus, our approach is comparable to those methodsand serves as a reasonable baseline against whichto study the effects of the hierarchy.The corpus is divided randomly, using a70%/30% split, into a training corpus of 6,753training documents and 2,742 test documents.Documents in both the training and test corporaare then divided into words using the same proce-dure.
Non-alphabetic characters (with the excep-tion of "-") are removed and all characters are Iow-ercased.
Stopwords are removed.
The documentis then parsed into "words"; i.e., character stringsdelimited by whitespace, and these words are thenused as features.Next, we count the number of times each featureappears in each document and, from that, we com-pute the total number of times each feature appearsin training documents in each category.
We retainonly features appearing 2 or more times in a singletraining document or 10 or more times across thetraining corpus.
All other features are discarded asbeing insufficiently reliable.Next we use a variant of the ACTION Algorithm(Wong et al 1996), described in detail in Section3.2 below, to associate features with nodes in thetaxonomy.
This is one of the aspects that makeour approach novel.
This algorithm is particularlyuseful because it allows us to compare the frequencyof a feature within a category with its frequency insibling categories in the same subtree.
This is moreeffective than just comparing the frequency withina category with global frequency as it focuses onthe decision actually being made at that node inthe hierarchy.By eliminating most features from most cate-gories, we gain several advantages.
First, by limit-ing the appearance of a feature to a small numberof categories (usually, just one) where it is an un-ambiguous discriminator, we improve the precisionof the categorization process.
Second, by workingwith a small number of features, we avoid optimiza-tion over a large number of features, and have aprocedure with low computational complexity thatcan be applied to large problems with many cate-gories.
(Currently the number of features is set to50).
Our feature selection procedure most closelyresembles rule induction (Apte et al, 1994) but itdiffers from that approach in that it considers theinteractions among a larger number of features fora given amount of computational effort.Weights are now assigned to the surviving fea-tures in each category.
We associate a weight, Wlc, with each surviving feature, f ,  in category c. Wedefine W/?
by:= + (1 - (1)where NI?
is the number of times f appears in c,Mc is the maximum frequency of any feature in c,and is a parameter (currently set to 0.4).where N(fc) is the number of times f appears inc, Mc is the maximum frequency of any feature inc, and is a parameter (currently set to 0.4).We also assign a negative weight to features asso-ciated with siblings (successors of the same parentnode) of each category.
A feature appearing in oneor more siblings of c but not in c itself, is assigneda negative weight~)~ = -(~, + (1 - A)-~7~- ) (2)where p is the parent of c in the hierarchy.
ThusNip is the number of times f appears In the parentof c, which is In turn the number of times f appearsin all siblings of c since it does not appear in c itselfat all.
Mp is the maximum frequency of any featurein c's parent.Finally, we filter the set of positive and negativewords associated with each category, retaining, atmost, 50 positive and 50 negative words with high-est weights for each category, both leaf and interior.We now have an index suitable for use in the cat-egory ranking process.
The index contains featuresand a weight, WI?, associated with each feature ineach category.
Given a document, d, a rank cannow be associated with each category with respectto d. Let F be the set of features, f, in D. Theranking of category c with respect o document d,R(cd), is then defined to be:nee = ,vI wI, (3)!where the sum is over all positive and negative fea-tures associated with c and IVI,~ is the number oftimes f appears in d. Note that, in practice, thesum is taken only over features that are in the in-tersection of the sets of features actually appearingin d and actually associated with c. Note that R?4may be positive, negative or zero.Test document d is now placed in a category.Starting at r, the root of the hierarchy, we com-pute Red for all c which are successors of r. If allR?,l are zero or negative, d is left at r. If any R.c,~is positive, let c' be the category with the highestrank.
If c' is a leaf node, d is placed in c'.
If c'is an interior node, the contest is repeated at nodec'.
Thus, d is eventually placed either in a leaf cat-egory which wins a contest among its siblings or69in an interior node none of whose children have apositive rank with respect o d. In this latter case,we may say that d is actually placed in the interiorcategory, partially categorized or not categorized atall.
Which of these we choose is dependent uponthe application and on how much we value precisionversus recall.3.2 The  ACT ION A lgor i thmThe ACTION Algorithm was first described in(~Vong et al, 1996) ~ a method of associating doc-uments with categories within a hierarchy.
Here,we use it to associate vocabulary with nodes in ahierarchy and associate documents with the nodesusing the procedure described in Section 3.1 above.The original algorithm applied to problems withdocuments at interior and leaf nodes.
Although ouradaptations apply to the more general case also, wedescribe the algorithm with respect o that simplercase since the corpus we are using has documentsonly at leaf nodes.The algorithm begins by counting N i t ,  the num-ber of times feature f appears in documents associ-ated with category c in the training set, for all f andc.
There is a level,, associated with each category,c, in the hierarchy'.
By convention, the root is'atlevel 1; its immediate successors are at level 2, etc.We then define EFtc, the effective frequency ofsubtree rooted at node c with respect o featurefasEF/c = E (4)jcS,Thus, EFIc is the total number of occurrences of fin c and all subcategories, S?
of node c.Finally, we define i'~,c, the significance value of cwith respect o f, as= ?
(5 )Thus, a node gets credit, in proportion to its level,for occurrences of f in itself and in its successors.The farther down the tree a node is, the more creditit is given for its level, but the higher up the treea node is, the larger the subtree rooted at c andthe larger the credit it gets for effective frequency.A competition thus takes place between each nodeand its parent (immediate predece.~or).
For eachfeature, f, EFIc is compared with, EFIp , where pis the parent of c and if EFIc is smaller then f isremoved from node c. Thus a parent can remove afeature from a child but not vice versa.
In the caseof a tie, the child loses the feature.
All this compe-tition proceeds from the leaves upward towards theroot.The net effect of this is that if a feature occurs inonly a single child of a given parent, then the childretains the feature (as does the parent), but if thefeature occurs significantly in more than one childof the same parent, then only the parent retains thefeature.Several advantages accrue from all this.
First,common features, including stopwords, will natu-rally rise to the root, where they will not participatein any rankings.
Thus, this algorithm is a gener-alized version of removing stopwords.
If a featureis prominent in several children of the same node,the parent will remove it from all of them.
Ideally,words that are important for making fine distinc-tions among categories farther down in the categoryhierarchy, but are ambiguous at higher levels, willparticipate only in places where they can help.Note that we never directly remove a feature fromthe parent even when the child retains it.
The rea-son for this is that we may need the feature to getthe document to the parent; if it doesn't reach theparent it can never reach the child.
In the casewhere a feature strongly represents only one cate-gory, there is no harm in the parent retaining it.
Inthe cases where it is ambiguous at the level of theparent, the grandparent removes it from the parent(its child).Thus, at the end of the algorithm when we filterthe feature set for each category (leaf and non-leaf)retaining only the 50 most highly ranked positiveand negative words, at non-leaf categories we alsoretain any words retained by their children.4 Comput~rt iona l  Exper ienceThere are a number of ways that the performance ofa hierarchical categorization system can be tuned.Two alternatives that we are exploring are modi-fying the topology of the hierarchy and adjustingthe weighting functions.
This paper describes theexperiments that we performed in order to under-stand the effects of modifying the topology.
In an-other paper, we describe the effect of adjusting thelevel numbers (weights) of the categories within thehierarchy (D'Alessio et al, 1998).
Our ult imate ob-jective is to find a set of transformations that canbe applied to a hierarchy as a part of the trainingprocess.In the first experiment no hierarchy was used,that is, none of the 5 Reuters supercategories wereused.
We applied our feature selection algorithmand our categorization algorithm in the normalmanner, however we assigned the root a level of0.
The effect of this is to prevent any features from70being associated with the root.
~Ve refer to thisorganization as Flat-0.
The remaining categoriesthen keep their 50 most significant positive and neg-ative features.
The results for overall precision andrecall, the number of unclassified ocuments (doc-uments left at the root), and a selected example arereported in Table 1.
Examining the results of thisexperiment shows that our algorithm does poorlyin the case of several small categories.
For exam-ple, there are only 4 petrochemical test documents,however our algorithm assigned 124 documents tothe petrochemicals category of which only I was ac-tuaUy a petrochemicals document.
Other small cat-egories uch as lumber, strategic metals, and moneysupply exhibit similar behavior.
An examination ofthese categories shows that in each case they sharea few key features ~ith a larger category.
Whenthese features appear in a test document they aregiven disproportionate weight in the smaller cat-egories.
Of the incorrect documents assigned topetrochemicals, nearly all (118) were either acqui-sitions or earnings documents.
The vocabulary as-sociated with the petrochemicals category in Flat-0includes words such as "rain" and "dlrs" that arealso earnings and acquisitions words.
The formulaused to assign weights to words found in test doc-uments uses a normalization factor to account forthe differences in the sizes of the categories.
In thiscase the net effect is to bias the decision towardspetrochemicals whenever these words appear in atest document.One advantage of using a hierarchy is that itshould provide a mechanism for moving featuresto positions where they aid in categorization andremove features from positions where they are am-biguous.
We tested this hypothesis by introducinga simple hierarchical organization.
We changed thelevel of the root node from 0 to 1, and gave thesubcategories of the root a level of 2.
We refer tothis organization as Fla~-l. Again, each categorykept its 50 most significant positive and negativefeatures and the categorization algorithm was ap-plied to the same test data as above.
The compari-son between Flat-0 and Flat-l, for this case, is alsogiven in Table 1.
Note the significant improvementin precision and recall.
Examination of the vocabu-lary associated with the petrochemicals category inFlat-1 no longer includes "mln" and "dlrs" as theACTION algorithm has removed them preventingthis small category from stealing documents fromlarger categories with some similar features.
Ad-ditionally, the time required for the categorizationwas reduced by a factor of one third.
This experi-ment demonstrated the beneficial effect of using theACTION algorithm with the hierarchy by allowingus to efficiently compare the relative frequency offeatures within a category and outside a category.The ambiguous words that were previously associ-ated with petrochemicals were either moved to theroot where they" became stop words, or were movedto other categories.We then conducted a number of experiments oexplore how modifying the topology of the hier-archy affects the categorization.
As a baseline,we used the hierarchy of topics supplied with theReuters corpus (see Figure 1) referred to as the Ba-sic hierarchy.
This organization is significantly dif-ferent from Flat-1 in that it is a three-level hierar-chy with 5 supercategories.
We applied our featureselection and categorization algorithms using thesame test data as above.
The results for overallprecision and recall, the precisions and recalls asso-ciated with the acquisitions and earnings categoriesthemselves, and document placement counts axe re-ported in Table 2 below.
The time required for thecategorization for the Basic hierarchy was approx-imately one half the time required for the Flat-1case.
An examination of the results hows that thishierarchy also avoids the small category problemexperienced in Flat-0.
However the overall perfor-mance was not as good as in Flat-1.
We identifiedand analyzed situations where the use of the deeperhierarchy caused problems and attempted to studythe problems by modifying the hierarchy.An error analysis using the dispersion matrixidentified the first problem as occurring when sib-ling leaf categories teal documents from eachother.
An exarfiple is the case of the earnings andacquisitions categories.
In the Basic hierarchy bothearnings and acquisitions are subcategories of thecorporate category while in Flat-1 both are sub-categories of the root.
A comparison of the pre-cision and recall for acquisitions and earnings us-ing Flat-1 versus Basic shows that acquisitions' re-Call drops from 92% to 77% with the other val-ues remaining somewhat comparable.
In this casethe deeper hierarchy" impedes performance.
An ex-amination of the dispersion matrix (Table 2) forthe Basic hierarchy" shows that 91 acquisition doc-uments are classified as earnings documents and15 earning documents are classified as acquisitionsdocuments with another 19 acquisition documentsbeing left at the corporate node.
This indicatesthat most of the earnings and acquisitions docu-ments are being correctly classified as corporatedocuments, however, in many cases there is insuffi-cient information to make the distinction betweenearning and acquisitions.
We hypothesize that in71OverallPrec/Rec82.85/82.7989.36/85.74U nclass \ [ '~t to~Docs I Corr I Incorr UL_it_N l l lTable h Comparison Between Flat-0 and Flat-1this case, our vocabulary selection algorithm hasremoved too many terms from earnings and acqui-sitions and given them to corporate.
Removing thecorporate category from the hierarchy would allowearnings and acquisitions to become subcategoriesof the root and retain more of their significant fea-tures.We tested this hypothesis by constructing a newhierarchy, ~.r-1, by removing corporate from theBasic hierarchy.
Table 3 summarizes the compari-son between these two topologies.
The table illus-trates that in the case where acquisitions and earn-ings are both children of the root (W~.r-1) there isless stealing of documents occurring between thesetwo siblings resulting in an overall improvementover the Basic hierarchy case.A second problem we identified is that in :somecases the vocabulary selection process removes toomany features from a leaf category with the resultthat it becomes difficult to properly categorize doc-uments belonging to that category.
An exampleof this can be seen with the category interest.
Asshown in the dispersion matrix for the Basic hierar-chy above, there are 104 test documents belongingto the interest category, however only 24 interestdocuments are correctly classified.
In this case in-terest is a subcategory of the root and most of itsincorrectly classified test documents (68) are clas-sifted in the economic indicators ubtree.
Here wehave a slightly different problem.
We do not havesibling leaves stealing documents from each other.Only one economic indicators document, a tradedocument, is placed in interest.
We have a leaf cat-egory competing directly with a larger, similar sub-tree.
As a result many of its documents are placedin the subtree.
%~ hypothesize that in this casethe leaf category should be moved into the subtree.This would allow the smaller category to competefor the documents that are assigned to the subtree.We tested this hypothesis by constructing a newhierarchy, ~ur-2, by making interest a subcategoryof economic indicators.
Table 4 summarizes thecomparison of the overall precision and recall, andselected document placement counts for the Basichierarchy, Var-2, and a third hierarchy, called Vat-3, that is a variation combining variations one andtwo.
Again, we see an improvement in overall pre-cision and recall but this time it was a result ofmaking a category that was weak and losing its doc-uments stronger by moving it to a position whereit could directly compete for features and thus doc-uments.A third type of problem was also identified.
Attimes a leaf category" will have poor precision be-.cause it is assigned many documents not belongingto the category.
In some cases this occurs becausedocuments were incorrectly classified at a highernode in the hierarchy.
These documents are thenexamined along the wrong path and are placed inan incorrect leaf.
An example of this occurs in thecategory trade, which is the largest subcategory ofeconomic indicators in the Basic hierarchy.
Thedispersion matrLx shows that there are 104 tradetest documents, 94 of which axe correctly classi-fied; 101 other documents are incorrectly classifiedas trade documents.
This is not a case of a cate-gory stealing documents from its sibling categories,rather documents belonging to a variety of non-economic indicator categories are incorrectly clas-sifted as economic indicators documents.
When wehave to decide which subcategory of economic in-dicators to plaice the documents into, trade beingtl~e largest subcategory attracts the majority of thedocuments.
We hypothesize that we can correctthis problem by" moving trade and making it a sub.-category of the root.
This has two effects.
First,it weakens economic indicators by removing one ofits largest categories.
Second, it weakens trade be-cause it lowers its level number and therefore re-duces the significance of its features.
This is ex-actly the reverse of the actions that we took withinterest, a category" that was too weak to attractthe documents it needed.To test our hypothesis we constructed a new hier-archy, ~.r-4, by making trade a subcategory of theroot.
%~ also incorporated our other variations,so that earnings and acquisitions are also subcate-gories of the root and interest is a subcategory ofeconomic indicators.
Table 5 reports the compari-son of the Basic and Var-4 hierarchies.
The overallprecision and recall improve again, this time, bytaking a category that is stealing because it was72-,-..zj.../ ~ " .
.
.
j .~_L .
/~ST"trade104"?
number of test documents1" number of subcategoriesFigure 1 Reuters basic hierarchyroot2742'acquisitions earnings688" 1156"RootCorpAcqEarnInterestTradeEci*OtherRoot Corp Acq Earn Interst !
Trade Eci*o o o o oi o o0 0 0 0 0 0 025 19 529 91 0 8 12 1 15 1121 0 4 90 3 4 2 24 24 440 0 1 2 1" 94 02 1 0 7 0 211 122I 4 30 15 2 44 18Other00154369388Table 2: Dispersion Table for Basic HierarchyThe columns list the categories where documents were placed by the algorithmthe rows list the categories the documents were actually in.OverallPrec/RecBasic 85.71/82.06v -1 87.55/84.61AcqPrec/Rec91/7792/90EarnPrec/Rec91/9797/94Acq docsat Corp19Earn as Acq asAcq Earn15 9138 16Table 3: Comparison Between Basid and Var-IOverallPrec/RecBasic 85.71/82.06Vat-2 86.46/82.93Vat-3 88.72/85.78Interest docsas Interest247476Non-Interestdocs as Interest32830Interest docs placedincorrectlyin eci subtree682627Table 4: Comparison Among Basic, Var-2 and Var-373OverallPrec/RecBasic 85.71/82.06~r -4  89.49/86.91Trade docsas Trade9487Non-Tradedocs as 'I~'acle10124Table 5: Comparison Between Basic and Ya.r-4too strong and moving it to a position where it hadto compete with equally strong siblings.Table 6 is a summary of the results for the a J1 thehierarchies discussed above.5 SummaryWe have demonstrated that using a hierarchy canhave a positive impact on the categorization task.Precision and recall are increased and the process-ing time is substantially reduced.
In addition wehave shown that the topology of the hierarchy canbe modified to produce improvements in precisionand recall.
Our ultimate goal is to identify a setof transformations, category level settings, asld theconditions under which each should be applied sothat we can automatically train the hierarchy.
Thiswould allow us to begin with a minimal hierarchysuch as Flat-l, and, using training data, automati-cally evolve an optimal hierarchy.
We are continu-ing to do research in this area.An obvious danger when using a hierarchy is thatplacing a document into its correct category in-volves multiple decision points.
If an error is madeat an upper level in the hierarchy, the documentwill be placed incorrectly.
Therefore it is criticalthat these early decisions be extremely accurate.Our experiments demonstrate hat it is possible toachieve this accuracy.
In the case of Flat-1 only onedecision point is used and 2351 of 2742 (85.7%) testdocuments are placed in correct categories.
In thecase of Var-4 if we look at only the first level, 2467of the 2742 (89.7%) test documents are placed intothe correct subcategory.
In addition in Flat-l, theroot is unable to make any decision for l l I  (4%)documents while in Var-4 there are only 23 (0.8%)such documents.
On a supercategory basis, the rootperformed better for some than others.
For com-modities, it had precision and reca', t.around 82%.For energy, it had about 93% precision and recall.Likewise, the performance of the interior nodes inthe hierarchy varied.
Economic indicators had a88% precision and a 76% recall, while commoditieshad a 96% precision and a 93% recall.
Thus wesee that there is room for further improvement viamoving categories from one part of the hierarchyto another and this investigation is the focus of ourcurrent research.Re ferencesApte C., Damerau F. and Weiss S.M.
(1994) Auto-mated Learning of Decision Rules for Text Catego-rization.
ACM Transactions on Information Sys-tems, 233.-251.Chakrabarti S., Dom B., Agarawal R., and RaghavanP.
(1997) Using Taxonomy, Discriminanta nd Sig-naturea for Navigating in Te.z~ Databases.
Proceed-ings of the 23rd VLDB Conference; Athens, Greece.Cohen W.W. and Singer Y.
(1996) Context-SensitiveLearning Methods for Text Categorization.
Pro-ceedings of the 19th Annual ACM/SIGIR Confer-ence .D'Alessio S., Kershenbaum A., Murray K., SchiaffinoR.
(1998) Category Levels in Hierarchical Te~ Cat-egorization.
Proceedings of the Third Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP-3).Deerwester S., Dumais S., Furnas G., Landauer T., andHarshman R. (1990) Indexing by Latent SemanticAnalysis.
Journal of the American Society for In-formation Science, 41(6), pp.
391-407.Frakes W.B.
and Baeza-Yates R. (1992) Informa-tion Retrieval: Data Structures and Algorithms.Prentice-Hall.Heckerman D. (1996) Bayesian Networks for Knowl-edge Discovery.
Advances in Knowledge Discov-ery and Data Mining.
Fayyad, Piatetsky-Shapiro,Smyth and Utlaurusamy eds., MIT Press.Hersh W., Buckley C., Leone T. and Hickrnan D.(1994) OHSUMED: An Interactive Retrieval Evabuation and a New Large Text Collection \]or Re-search.
Proceedings of the 17th Annual Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, Philadelphia.Koller D. and Sahami M. (1996) Towards Optimal Fea-ture Selection.
International Conference on Ma-chine Learning, "volume 13, Morgan-Kauffman.Koller D. and Sahami M. (1997) Hierarchically Clas-sifying Documents using Very Few Words.
Inter-national Conference on Machine Learning, Volume14, Morgan-Kauffman.Larkey L. and Croft W.B.
(1996) Combining Classi-tiers in Text Categorization.
Proceedings of the19th Annual ACM/SIGIR Conference.Lewis.
D (1992) Text Representation for Intel-ligent Text Retrieval: A Classification-OrientedView.
Text-Based Intelligent Systems, P.S.
Jacobs,Lawrence-Erlbaum.74/I Hier= YPrecisionRecall82.85 \[ 89.36 85.71 87.5,5 86.46 88.72 \[ 89.4982.79 I 85.74 82.06 84.61 82.93 85.78 l SO.9xTable 6: Precision (%) and Recall (%) for Seven TaxonomiesLewis D. and Ringuette.
M. (1994) A Comparison ofTwo Learning Algorithms for text Categorization.Third Annual Symposium on Document Analysisand In.formation Retrieval, Las Vegas, pp.
81-93.Ng H.-T., Gob W.-B.
and Low K.-L. (1997) Feature Se-lection, Perception Learning and a Usability CaseStudy.
Proceedings of the 20th Ann'aal Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, Philadelphia,July 27-31, pp.
67-73Sahami M. (1996).
Learning Limited DependenceBayesian Classifiers.
Proc.
KDD-96, pp.33~33&Salton G. (1989) Automatic Text Processing: TheTransformation, Analysis and Retrieval of Infor-mation by Computer., Addison-~,Vesley.van Rijsbergen.
C.J.
(1979) Information Retrieval.Buttersworth, London, second edition.Witten I.H., Moffat A. and Bell T. (1994) ManagingGigabytes.
Van Nostrand Reinhold.Wong J.W.T., Wan W.K.
and Young G. (1996) AC-TION: Automatic Classification \[or Full- Text Doc-uments.
SIGIR Forum 30(1), pp.
11-25.Yang Y.
(1997) An Evaluation of Statistical Ap-proaches to Text Categorization.
Technical ReportCMU-CS-97-127, Computer Science Department,Carnegie Mellon University.Yang Y.(1996).
An Evaluation of Statistical Ap-proaches to MEDLINE Indexing.
Proceedings ofthe AMIA, pp.
358.-362.
"~ng "k'.
and Chute.C.G.
(1992) A Linear Leant SquareJFit Mapping Method/or Information Retrieval fi,amNatural Language Tee.yrs.
Proceedings of COLIiNG'92, pp.
447-453.Yang Y. and Pederson.J.P.
(1997) Feature Selection inStatistical Learning of Text Categorization.
Inter-national Conference on Machine Learning, Volume14, Morgan-Kauffman.
(1997) UMLS Knowledge Sources 8th Edition NationalLibrary of Medicine75
