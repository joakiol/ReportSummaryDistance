Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 75?84,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLearning to Recognize Discontiguous EntitiesAldrian Obaja Muis Wei LuSingapore University of Technology and Design{aldrian muis,luwei}@sutd.edu.sgAbstractThis paper focuses on the study of recognizingdiscontiguous entities.
Motivated by a previ-ous work, we propose to use a novel hyper-graph representation to jointly encode discon-tiguous entities of unbounded length, whichcan overlap with one another.
To comparewith existing approaches, we first formally in-troduce the notion of model ambiguity, whichdefines the difficulty level of interpreting theoutputs of a model, and then formally analyzethe theoretical advantages of our model overprevious existing approaches based on linear-chain CRFs.
Our empirical results also showthat our model is able to achieve significantlybetter results when evaluated on standard datawith many discontiguous entities.1 IntroductionBuilding effective automatic named entity recogni-tion (NER) systems that is capable of extractinguseful semantic shallow information from texts hasbeen one of the most important tasks in the field ofnatural language processing.
An effective NER sys-tem can typically play an important role in certaindownstream NLP tasks such as relation extraction,event extraction, and knowledge base construction(Hasegawa et al, 2004; Al-Rfou and Skiena, 2012).Most traditional NER systems are capable of ex-tracting entities1 as short spans of texts.
Two ba-sic assumptions are typically made when extract-1Or sometimes mentions are considered, which can benamed, nominal or pronominal references to entities (Florianet al, 2004).
In this paper we use ?mentions?
and ?entities?interchangeably.EGD showed [hiatal hernia]1 and vertical [laceration]2in distal [esophagus]2 with [blood in [stomach]4]3 andoverlying [lac]4.Figure 1: Discontiguous entities in a medical domain.
Wordsannotated with the same index are part of the same entity.
Notethat entity 3 and entity 4 overlap with one another.ing entities: 1) entities do not overlap with one an-other, and 2) each entity consists of a contiguous se-quence of words.
These assumptions allow the taskto be modeled as a sequence labeling task, for whichmany existing models are readily available, such aslinear-chain CRFs (McCallum and Li, 2003).While the above two assumptions are valid formost cases, they are not always true.
For example,in the entity University of New Hampshire of typeORG there exists another entity New Hampshire oftype LOC.
This violates the first assumption above,yet it is crucial to extract both entities for subsequenttasks such as relation extraction and knowledge baseconstruction.
Researchers therefore have proposedto tackle the above issues in NER using more so-phisticated models (Finkel and Manning, 2009; Luand Roth, 2015).
Such efforts still largely rely onthe second assumption.Unfortunately, the second assumption is also notalways true in practice.
There are also cases wherethe entities are composed of multiple discontiguoussequences of words, such as in disorder mentionrecognition in clinical texts (Pradhan et al, 2014b),where the entities (disorder mentions in this case)may be discontiguous.
Consider the example shownin Figure 1.
In this example there are four enti-75ties, the first one, hiatal hernia, is a conventionalcontiguous entity.
The second one, laceration ...esophagus, is a discontiguous entity, consisting oftwo parts.
The third and fourth ones, blood in stom-ach and stomach ... lac (for stomach laceration),are overlapping with each other, with the fourth be-ing discontiguous at the same time.For such discontiguous entities which can poten-tially overlap with other entities in complex man-ners, existing approaches such as those based onsimple sequence tagging models have difficultieshandling them accurately.
This stems from the factthat there is a very large number of possible entitycombinations in a sentence when the entities can bediscontiguous and overlapping.Motivated by this, in this paper we propose anovel model that can better represent both contigu-ous and discontiguous entities which can overlapwith one another.
Our major contributions can besummarized as follows:?
We propose a novel model that is able to repre-sent both contiguous and discontiguous entities.?
Theoretically, we introduce the notion of modelambiguity for quantifying the ambiguity of dif-ferent NER models that can handle discontigu-ous entities.
We present a study and make com-parisons about different models?
ambiguity un-der this theoretical framework.?
Empirically, we demonstrate that our modelcan significantly outperform conventional ap-proaches designed for handling discontiguousentities on data which contains many discontigu-ous entities.2 Related WorkLearning to recognize named entities is a populartask in the field of natural language processing.
Asurvey by Nadeau (2007) lists several approachesin NER, including Hidden Markov Models (HMM)(Bikel et al, 1997), Decision Trees (Sekine, 1998),Maximum Entropy Models (Borthwick and Sterling,1998), Support Vector Machines (SVM) (Asaharaand Matsumoto, 2003), and also semi-supervisedand unsupervised approaches.
Ratinov (2009) uti-lizes averaged perceptron to solve this problem andalso focused on four key design decisions, achiev-ing state-of-the-art in MUC-7 dataset.
These ap-proaches work on standard texts, such as news ar-ticles, and the entities to be recognized are definedto be contiguous and non-overlapping.Noticing that many named entities contain othernamed entities inside them, Finkel and Manning(2009) proposed a model that is capable of extract-ing nested named entities by representing the sen-tence as a constituency parse tree, with named enti-ties as phrases.
As a parsing-based model, the ap-proach has a time complexity that is cubic in thenumber of words in the sentence.Recently, Lu and Roth (2015) proposed a modelthat can represent overlapping entities.
In addition tosupporting nested entities, theoretically this modelcan also represent overlapping entities where nei-ther is nested in another.
The model represents eachsentence as a hypergraph with nodes indicating en-tity types and boundaries.
Compared to the previ-ous model, this model has a lower time complexity,which is linear in the number of words in the sen-tence.All the above models focus on NER in conven-tional texts, where the assumption of contiguous en-tities is valid.
In the past few years, there is a grow-ing body of works on recognizing disorder mentionsin clinical text.
These disorder mentions may bediscontiguous and also overlapping.
To tackle suchan issue, a research group from University of TexasHealth Science Center at Houston (Tang et al, 2013;Zhang et al, 2014; Xu et al, 2015) first utilized aconventional linear-chain CRF to recognize disordermention parts by extending the standard BIO (Begin,Inside, Outside) format, and next did some postpro-cessing to combine different components.
Thougheffective, as we will see later, such a model comeswith some drawbacks.
Nevertheless, their work mo-tivated us to perform further analysis on this issueand propose a novel model specifically designed fordiscontiguous entity extraction.3 Models3.1 Linear-chain CRF ModelBefore we present our approach, we would like tospend some time to discuss a simple approach basedon linear-chain CRFs (Lafferty et al, 2001).
Thisapproach is primarily based on the system by Tanget al (2013), and this will be the baseline system76EGD showed hiatal[B] hernia[I] and vertical laceration[BD]in distal esophagus[BD] with blood[B] in[I] stomach[BH] andoverlying lac[BD].Infarctions[BH] either water[BD] shed[ID] or embolic[BD]Figure 2: Entity encoding in the linear-chain model.
Top: forthe example in Fig 1.
Bottom: for the second example in Fig 4.The O labels are not shown.that we will make comparison with in later sections.The problem is regarded as a sequence predictiontask, where each word is assigned a label similar toBIO format often used for NER.
We used the en-coding used by Tang et al (2013), which uses 7tags to handle entities that can be discontiguous andoverlapping.
Specifically, we used B, I, O, BD, ID,BH, and IH to denote Beginning of entity, Inside en-tity, Outside of entity, Beginning of Discontiguousentity, Inside of Discontiguous entity, Beginning ofHead, and Inside of Head.
To encode a sentencein this format, first we identify the contiguous wordsequences which are parts of multiple entities.
Wecall these head components and we label each wordinside each component with BH (for the first wordin each component) or IH.
Then we find contiguousword sequences which are parts of a discontiguousentity, which we call the body components.
Wordsinside those components which have not been la-beled are labeled with BD (for the first word in eachcomponent) or ID.
Finally, words that are parts of acontiguous entity are called contiguous component,and, if they have not been labeled, are labeled as B(for the first word in each component) or I.This encoding is lossy, since the informationon which parts constitute the same entity is lost.The top example in Figure 2 is the encoding ofthe example shown in Figure 1.
During decod-ing, based on the labels only it is not entirelyclear whether ?laceration?
should be combined with?esophagus?
or with ?stomach?
to form a singlemention.
For the bottom example, we cannot deducethat ?Infarctions?
alone is a mention, since thereis no difference in the encoding of a sentence withonly two mentions {?Infarctions .
.
.
water shed?,?Infarctions .
.
.
embolic?}
or having three mentionswith ?Infarctions?
as another mention, since in bothcases, the word ?Infarctions?
is labeled with BH.Also, it should be noted that some of the label se-quences are not valid.
For example, a sentence inwhich there is only one word labeled as BD is in-valid, since a discontiguous entity requires at leasttwo words to be labeled as BD or BH.
This is, how-ever, a possible output from the linear CRF model,due to the Markov assumption inherent in linearCRF models.
Later we see that our models do nothave this problem.3.2 Our ModelLinear-chain CRF models are limited in their repre-sentational power when handling complex entities,especially when they can be discontiguous and canoverlap with one another.
While recent models havebeen proposed to effectively handle overlapping en-tities, how to effectively handle discontiguous en-tities remains a research question to be answered.Motivated by previous efforts on handling overlap-ping entities (Lu and Roth, 2015), in this work wepropose a model based on hypergraphs that can bet-ter represent entities that can be discontiguous andat the same time be overlapping with each other.Unlike the previous work (Lu and Roth, 2015), weestablish a novel theoretical framework to formallyquantify the ambiguity of our hypergraph-basedmodels and justify their effectiveness by makingcomparisons with the linear-chain CRF approach.Now let us introduce our novel hypergraph rep-resentation.
A hypergraph can be used to represententities of different types and their combinations ina given sentence.
Specifically, a hypergraph is con-structed as follows.
For the word at position k, wehave the following nodes:?
Ak: this node represents all entities that beginwith the current or a future word (to the right ofthe current word).?
Ek: this node represents all entities that beginwith the current word.?
Tkt : this node represents entities of certain spe-cific type t that begin with the current word.There is one Tkt for each different type.?
Bkt,i: this node indicates that the current word ispart of the i-th component of an entity of type t.?
Okt,i: this node indicates that the current wordappears in between (i-1)-th and i-th componentsof an entity of type t.77There is also a special leaf node, X-node, whichindicates the end (i.e., right boundary) of an entity.The nodes are connected by directed hyperedges,which for the purpose of explaining our models aredefined as those edges that connect one node, calledthe parent node, to one or more child nodes.
For easeof notation, in the rest of this paper we use edge torefer to directed hyperedge.The edges Each Ak is a parent to Ek and Ak+1,encoding the fact that the set of all entities at positionk is the union of the set of entities starting exactly atcurrent position (Ek) with the set of entities startingat or after position k + 1 (Ak+1).Each Ek is a parent to Tk1 , .
.
.
, TkT , where T isthe total number of possible types that we consider.Each Tkt has two edges where it serves as a parent,within one it is parent to Bkt,0 and within another it isto X.
These edges encode the fact that at position k,either there is an entity of type t that begins with thecurrent word (to Bkt,0), or there is no entity of type tthat begins with the current word (to X).In the full hypergraph, each Bkt,i is a parent toBk+1t,i (encoding the fact that the next word also be-longs to the same component of the same entity),to Ok+1t,i+1 (encoding the fact that this word is partof a discontiguous entity, and the next word is thefirst word separating current component and the nextcomponent), and to X (representing that the entityends at this word).
Also there are edges with all pos-sible combinations of Bk+1t,i , Ok+1t,i+1, and X as thechild nodes, representing overlapping entities.
Forexample, the edge Bkt,i ?
(Bk+1t,i ,X ) denotes thatthere is an entity which continues to the next word(the edge toBk+1t,i ), while there is another entity end-ing at k-th word (the edge to X).
In total there are 7edges in which Bkt,i is a parent, which are:?
Bkt,i?
(X)?
Bkt,i?
(Ok+1t,i+1 )?
Bkt,i?
(Ok+1t,i+1 ,X)?
Bkt,i?
(Bk+1t,i )?
Bkt,i?
(Bk+1t,i ,X)?
Bkt,i?
(Bk+1t,i ,Ok+1t,i+1 )?
Bkt,i?
(Bk+1t,i ,Ok+1t,i+1 ,X)Analogously, Okt,i has three edges that connect toA A A A A AE E E E E ET T T T T TB0O1 O1 O1 O1B1 B1 B1X X X X XXX X[[[Infarctions]1]2]3 either [water shed]2 or [embolic]3Figure 3: The hypergraph for SHARED model for the secondexample in Figure 4.
The type information in T, B, and O-nodes is not shown.
The X-node is drawn multiple times forbetter visualization.Ok+1t,i , Bk+1t,i+1, and both.
Note that Okt,i is not a par-ent to X by definition.During testing, the model will predict a subgraphwhich will result in the predicted entities after de-coding.
We call this subgraph representing certainentity combination entity-encoded hypergraph.For example, Figure 3 shows the entity-encodedhypergraph of our model encoding the three men-tions in the second example in Figure 4.
The edgefrom the T-node for the first word to the B-node forthe first word shows that there is at least one entitystarting with this word.
The three places where anX-node is connected to a B-node show the end ofthe three entities.
Note that this hypergraph clearlyshows the presence of the three mentions withoutambiguity, unlike a linear-chain encoding of this ex-ample where it cannot be inferred that ?Infarctions?alone is a mention, as discussed previously.
In thispaper, we set the maximum number of componentsto be 3 since the dataset does not contain any men-tion with more than 3 components.Also note that this model supports discontiguousand overlapping mentions of different types sinceeach type has its own set of O-nodes and B-nodes,unlike the linear-chain model, which supports onlyoverlapping mentions of the same type.We also experimented with a variant of thismodel, where we split the T-nodes, B-nodes, andO-nodes further according to the number of com-ponents.
We split Bkt,i into Bkt,i,j , i = 1 .
.
.
j, j =781 .
.
.
3 which represents that the word is part of thei-th component of a mention with total j compo-nents.
Similarly we split Okt,i into Okt,i,j and Tkt intoTkt,j .
We call the original version SHARED model,and this variant SPLIT model.
The motivation forthis variant is that the majority of overlaps in thedata are between discontiguous and contiguous enti-ties, and so splitting the two cases ?
one component(contiguous) and more (discontiguous) ?
will reduceambiguity for those cases.These models are still ambiguous to some degree,for example when an O-node has two child nodesand two parents, we cannot decide which of the par-ent node is paired with which child node.
However,in this paper we argue that:?
This model is less ambiguous compared to thelinear-chain model, as we will show later theo-retically and empirically.?
Every output of our model is a valid prediction,unlike the linear-chain model since this modelwill always produce a valid path from T-nodesto the X-nodes representing some entities.We will also show through experiments that ourmodels can encode the entities more accurately.3.3 Interpreting Output StructuresBoth the linear-chain CRF model and our models arestill ambiguous to some degree, so we need to handlethe ambiguity in interpreting the output structuresinto entities.
For all models, we define two gen-eral heuristics: ENOUGH and ALL.
The ENOUGHheuristic handles ambiguity by trying to produce aminimal set of entities which encodes to the one pro-duced by the model, while ALL heuristic handlesambiguity by producing the union of all possible en-tity combinations that encode to the one producedby the model.
For more details on how these heuris-tics are implemented for each model, please refer tothe supplementary material.3.4 TrainingFor both models, the training follows a log-linearformulation, by maximizing the loglikelihood of thetraining data D:L(D) =?(x,y)?D??
?e?E(x,y)[wT f(e)]?
logZw(x)???
?||w||2Here (x,y) is a training instance consisting ofthe sentence x and the entity-encoded hypergraphy ?
Y where Y is the set of all possible mention-encoded hypergraphs.
The vector w consists of fea-ture weights, which are the model parameters to belearned.
The set E(x,y) consists of all edges presentin the entity-encoded hypergraph y for input x. Thefunction f(e) returns the features defined over theedge e, Zw(x) is the normalization term which givesthe sum of scores over all possible entity-encodedhypergraphs in Y that is relevant to the input x, andfinally ?
is the `2-regularization parameter.4 Model AmbiguityThe main aim of this paper is to assess how welleach model can represent the discontiguous entities,even in the presence of overlapping entities.In this section, we will theoretically compare themodels?
ambiguity, which is defined as the aver-age number of mention combinations that map tothe same encoding in a model.
Now, to comparetwo models, instead of calculating the ambiguity di-rectly, we can calculate the relative ambiguity be-tween the two models directly by comparing thenumber of canonical encodings in the two models.A canonical encoding is a fixed, selected repre-sentation of a particular set of mentions in a sen-tence, among (possibly) several alternative represen-tations.
Several alternatives may be present due tothe ambiguity of the encoding-decoding process andalso since the output of the model is not restrictedto a specific rule.
For example, for the text ?JohnSmith?, a model trained in BIO format might output?B-PER I-PER?
or ?I-PER I-PER?, and both willstill mean that ?John Smith?
is a person, althoughthe ?correct?
encoding would of course be ?B-PERI-PER?, which is selected as the canonical encoding.Intuitively, a canonical encoding is a formal way tosay that we only consider the ?correct?
encodings.A model with larger number of canonical encod-ings will, on average, have less ambiguity comparedto the one with smaller number of canonical encod-ings.
Subsequently, a model with less ambiguity willbe more precise in predicting entities.Let MLI(n),MSH(n),MSP(n) denote the num-ber of canonical encodings of the linear-chain,SHARED, and SPLIT model, respectively, for a sen-79tence with n words.
Then we formally define therelative ambiguity of model M1 over model M2,Ar(M1,M2), as follows:Ar(M1,M2) = limn?
?log?ni=1MM2(i)log?ni=1MM1(i)(1)Ar(M1,M2) > 1 means model M1 is more am-biguous than M2.
Now, we claim the following:Theorem 4.1.
Ar(LI, SH) > 1We provide a proof sketch below.
Due to spacelimitation, we cannot provide the full dynamic pro-gramming calculation.
We refer the reader to thesupplementary material for the details.Proof Sketch The number of canonical encodingsin the linear-chain model is less than 7n since thereare 7 possible tags for each of the n words and notall of the 7n tag sequences are canonical encodings.So we have MLI(n) < 7n and thus we can derivelog?ni=1MLI(i) < 3n log 2.For our models, by employing some dynamic pro-gramming adapted from the inside algorithm (Baker,1979), we can calculate the growth order of the num-ber of canonical encodings for SHARED model to ar-rive at a conclusion that ?n > n0, ?ni=1MSH(i) >C ?
210n for some constants n0, C. Then we have:Ar(LI, SH)?
limn?
?logC+10n log 23n log 2 =103 >1 (2)Theorem 4.1 says that the linear-chain model ismore ambiguous compared to our SHARED model.Similarly, we can also establish Ar(SH, SP) > 1.Later we also see this empirically from experiments.5 Experiments5.1 DataTo allow us to conduct experiments to empiricallyassess different models?
capability in handling en-tities that can be discontiguous and can potentiallyoverlap with one another, we need a text corpus an-notated with entities which can be discontiguous andoverlapping with other entities.
We found the largestof such corpus to be the dataset from the task torecognize disorder mentions in clinical text, initiallyorganized by ShARe/CLEF eHealth Evaluation Lab(SHEL) in 2013 (Suominen et al, 2013) and contin-ued in SemEval-2014 (Pradhan et al, 2014a).The definition of the task is to recognize men-tions of concepts that belong to the Unified Medi-cal Language System (UMLS) semantic group dis-orders from a set of clinical texts.
Each text has beenannotated with a list of disorder mentions by twoprofessional coders trained for this task, followed byan open adjudication step (Suominen et al, 2013).Unfortunately, even in this dataset, only 8.95% ofthe mentions are discontiguous.
Working directlyon such data would prevent us from understandingthe true effectiveness of different models when han-dling entities which can be discontiguous and over-lapping.
In order to truly understand how differentmodels behave on data with discontiguous entities,we consider a subset of the data where we considerthose sentences which contain at least one discon-tiguous entity.
We call the resulting subset the ?Dis-contiguous?
subset of the ?Original?
dataset.
Laterwe will also still use the training data of the ?Origi-nal?
dataset in the experiments.Note that this ?Discontiguous?
subset still con-tains contiguous entities since a sentence usuallycontains more than one entity.
The subset is a bal-anced dataset with 53.61% of the entities being dis-contiguous and the rest contiguous.
We then splitthis dataset into training, development, and test set,according to the split given in SemEval 2014 setting(henceforth LARGE dataset).
To see the impact ofdataset size, we also experiment on a subset of theLARGE dataset, following the SHEL 2013 setting,with the development set in the LARGE dataset usedas test set (henceforth SMALL dataset).
The trainingand development set of the SMALL dataset comesfrom a random 80% (Tr80) and 20% (Tr20) split ofthe training set in LARGE dataset.The statistics of the datasets, including the num-ber of overlaps between the entities in the ?All?
col-umn, are shown in Table 1.We note that this dataset only contains one type ofentity.
In later experiments, in order to evaluate themodels on multiple types, we create another datasetwhere we split the entities based on the entity-levelsemantic category.
This information is available forsome entities through the Concept Unique Identifier(CUI) annotation in the data.
In total we have threetypes: two types (type A and B) based on the seman-tic category, and one type (type N) for those entities80Split #Sentences Number of mentions #Overlaps1 part 2 parts 3 parts Total All DiffTrain 534 544 607 44 1,195 205 58- Tr80 416 448 476 33 957 164 48- Tr20 118 96 131 11 238 41 10Dev 303 357 421 18 796 240 28Test 430 584 610 16 1,210 327 61Table 1: The statistics of the data.
Tr80 and Tr20 refers to the80% and 20% partitions of the full training data.having no semantic category information2.
See thesupplementary material for more details.
The num-ber of overlaps between different types is shown inthe ?Diff?
column in Table 1.
Except for a handfuloverlaps in development set, all overlaps involve atleast one discontiguous entity.
Our main result willstill be based on the dataset with one type of entity.The patient had blood in his mouth and on his tongue,pupils were pinpoint and reactive.- blood in his mouth- blood .
.
.
on his tongue- pupils .
.
.
pinpointInfarctions either water shed or embolic- Infarctions- Infarctions .
.
.
water shed- Infarctions .
.
.
embolicYou see blood or dark/black material when you vomit orhave a bowel movement.- blood .
.
.
vomit- blood .
.
.
bowel movement- dark .
.
.material .
.
.
vomit- dark .
.
.
bowel movement- black material .
.
.
vomit- black material .
.
.
bowel movementFigure 4: Examples of discontiguous and overlapping men-tions, taken from the dataset.Figure 4 shows some examples of the mentions.The first example shows two discontiguous men-tions that do not overlap.
The second example showsa typical discontiguous and overlapping case.
Thelast example shows a very hard case of overlapping2It is tempting to just ignore these entities since the N typedoes not convey any specific information about the entities init.
However, due to the dataset size, excluding this type willlead to very small number of interactions between types.
So wedecided to keep this typeand discontiguous mentions, as each of the compo-nents in {blood, dark, black material} is paired witheach of the word in {vomit, bowel movement}, re-sulting in six mentions in total, with one having threecomponents (dark .
.
.material .
.
.
vomit).5.2 FeaturesMotivated by the features used by Zhang etal.
(2014), for both the linear-chain CRF model andour models we use the following features: neigh-bouring words with relative position information(we consider previous and next k words, wherek=1, 2, 3), neighbouring words with relative posi-tion information paired with the current word, wordn-grams containing the current word (n=2,3), POStag for the current word, POS tag n-grams con-taining the current word (n=2,3), orthographic fea-tures (prefix, suffix, capitalization, lemma), notetype (discharge summary, echo report, radiology,and ECG report), section name (e.g.
Medications,Past Medical History)3, Brown cluster, and word-level semantic category information4.
We used Stan-ford POS tagger (Toutanova et al, 2003) for POStagging, and NLP4J package5 for lemmatization.For Brown cluster features, following Tang et al(2013), we used 1,000 clusters from the combina-tion of training, development, and test set, and usedall the subpaths of the cluster IDs as features.5.3 Experimental SetupWe evaluated the three models on the SMALL datasetand the LARGE dataset.Note that in both the SMALL and LARGE dataset,about half of all mentions are discontiguous, both intraining and test set.
We also want to see whethertraining on a set where the majority of the mentionsare contiguous will affect the performance on rec-ognizing discontiguous mentions.
So we also per-formed another experiment where we trained eachmodel on the original training set where the major-ity of the entities are contiguous.
We refer to thisoriginal dataset as ?Train-Orig?
(it contains 10,405sentences, including those with no entities) and the3Section names were determined by some heuristics, referto the supplementary material for more information4This is standard information that can be extracted fromUMLS.
See (Zhang et al, 2014) for more details.5http://www.github.com/emorynlp/nlp4j/81SMALL LARGETrain-Disc Train-Orig Train-Disc Train-OrigP R F1 P R F1 P R F1 P R F1LI-ENH 59.7 39.8 47.8 71.0 45.8 55.7 54.7 41.2 47.0 64.1 46.5 53.9LI-ALL 16.6 43.5 24.1 55.5 49.2 52.2 15.2 44.9 22.7 52.8 49.4 51.1SH-ENH 85.9 39.7 54.3 82.2 48.0 60.6 76.9 40.1 52.7 73.9 49.1 59.0SH-ALL 85.9 39.7 54.3 82.2 48.0 60.6 76.0 40.5 52.8 73.4 49.5 59.1SP-ENH 86.7 37.8 52.7 82.5 48.0 60.7 79.4 38.6 52.0 75.3 48.8 59.2SP-ALL 86.7 37.8 52.7 82.5 48.0 60.7 79.4 38.6 52.0 75.3 48.8 59.2Table 2: Results on the two datasets and two different training data after optimizing regularization hyperparameter ?
in developmentset.
The -ENH and -ALL suffixes refer to the ENOUGH and ALL heuristics.
The best result in each column is put in boldface.earlier one as ?Train-Disc?.First we trained each model on the training set,varying the regularization hyperparameter ?,6 thenthe ?
with best result in the development set usingthe respective ENOUGH heuristic for each model ischosen for final result in the test set.For each experiment setting, we show precision(P), recall (R) and F1 measure.
Precision is thepercentage of the mentions predicted by the modelwhich are correct, recall is the percentage of men-tions in the dataset correctly discovered by themodel, and F1 measure is the harmonic mean of pre-cision and recall.5.4 Results and DiscussionsThe full results are recorded in Table 2.We see that in general our models have higher pre-cision compared to the linear-chain baseline.
Thisis expected, since our models have less ambiguity,which means that from a given output structure it iseasier in our model to get the correct interpretation.We will explore this more in Section 5.5.The ALL heuristic, as expected, results in higherrecall, and this is more pronounced in the linear-chain model, with up to 4% increase from theENOUGH heuristic, achieving the highest recall inthree out of four settings.
The high recall of the ALLheuristic in the linear-chain model can be explainedby the high level of ambiguity the model has.
Sinceit has more ambiguity compared to our models, onelabel sequence predicted by the model produces a lotof entities, and so it is more likely to overlap with thegold entities.
But this has the drawback of very lowprecision as we can see in the result.We see switching from one heuristic to the other6Taken from the set {0.125, 0.25, 0.5, 1.0, 2.0}does not affect the results of our models much.Looking at the output of our models, they tend toproduce output structures with less ambiguity, whichcauses little difference in the two heuristics.One example where the baseline made a mis-take is the sentence: ?Ethanol Intoxication andwithdrawal?.
The gold mentions are ?EthanolIntoxication?
and ?Ethanol withdrawal?.
Butthe linear-chain model labeled it as ?
[Ethanol][B][Intoxication][I] and [withdrawal][BD]?, which is in-consistent since there is only one discontiguouscomponent.
Our models do not have this issue be-cause in our models every subgraph that may be pre-dicted translates to valid mention combinations, asdiscussed in Section 3.2.In the ?Train-Orig?
column, we see that all mod-els can recognize discontiguous entities better whengiven more data, even though the majority of the en-tities in ?Train-Orig?
are contiguous.5.5 Experiments on AmbiguityTo see the ambiguity of each model empirically,we run the decoding process for each model giventhe gold output structure, which is the true labelsequence for the linear-chain model and the truemention-encoded hypergraph for our models.We used the entities from the training and devel-opment sets for this experiment, and we compare the?Original?
datasets with the ?Discontiguous?
subsetto see that the ambiguity is more pronounced whenthere are more discontiguous entities.
Then we showthe precision and recall errors (defined as 1?P and1?R, respectively) in Table 3.Since the ALL heuristics generates all possiblementions from the given encoding, theoretically itshould give perfect recall.
However, due to errorsin the training data, there are mentions which can-82Discontiguous OriginalPrec Err Rec Err Prec Err Rec ErrLI-ALL 63.66% 0.30% 23.81% 0.17%SH-ALL 1.73% 0.30% 0.35% 0.17%SP-ALL 1.05% 0.30% 0.22% 0.17%LI-ENH 2.74% 3.82% 0.52% 0.90%SH-ENH 1.21% 1.46% 0.25% 0.38%SP-ENH 0.75% 0.90% 0.17% 0.28%Table 3: Precision and recall errors (%) of each model in the?Discontiguous?
and ?Original?
datasets when given the goldoutput structure (label sequence in linear-chain model, hyper-graph in our models).
Lower numbers are better.Type #Ent Linear-chain SHARED SPLITP R F P R F P R FA 289 69.8 59.9 64.4 79.4 56.1 65.7 81.0 56.1 66.3B 418 50.0 34.0 40.5 56.8 29.0 38.4 58.2 28.0 37.8N 503 62.1 37.8 47.0 84.8 43.3 57.4 84.9 42.4 56.5Total 1210 60.3 41.7 49.3 74.3 41.4 53.2 75.5 40.7 52.9Table 4: Results on the LARGE dataset when entities are splitinto three types: A, B, and N. #Ent is the number of entitiesnot be properly encoded in the models7.
Removingthese errors results in perfect recall (0% recall er-ror).
This means that all models are complete: theycan encode any mention combinations.We see however, a very huge difference on theprecision error between the linear-chain model andour models, even more when most of the entitiesare discontiguous.
For the discontiguous subset withthe ALL heuristic, the linear-chain model produced5,463 entities, while the SHARED and SPLIT modelproduced 2,020 and 2,006 entities, respectively.
Thetotal number of gold entities is 1,991.
This meansone encoding in the linear-chain model producesmuch more distinct mention combinations comparedto our model, which again shows that the linear-chain model has more ambiguity.
Similarly, we candeduce that the SHARED model has slightly moreambiguity compared to the SPLIT model.
This con-firms our theoretical result presented previously.It is also worth noting that in the ENOUGH heuris-tic our models have smaller errors compared to thelinear-chain model, showing that when both mod-els can predict the true output structure (the correct7There are 19 errors in the original dataset, and 6 in the dis-contiguous subset, which include duplicate mentions and men-tions with incorrect boundarieslabel sequence for the baseline model and mention-encoded hypergraph for our models), it is easier inour models to get the desired mention combinations.5.6 Experiments on Multiple Entity TypesWe used the LARGE dataset with the multiple-typeentities for this experiment.
We ran our two modelsand the linear-chain CRF model with the ENOUGHheuristic on this multi-type dataset, in the same set-ting as Train-Orig in previous experiments, and theresult is shown in Table 4.
We used the best lambdafrom the main experiment for this experiment.There is a performance drop compared to theLARGE-Train-Orig results in Table 2, which is ex-pected since the presence of multiple types make thetask harder.
But in general we still see that our mod-els are still better than the baseline, especially theSPLIT model, which shows that in the presence ofmultiple types, our models can still work better thanthe baseline model.6 Conclusions and Future WorkIn this paper we proposed new models that can bet-ter represent discontiguous entities that can be over-lapping at the same time.
We validated our claimsthrough theoretical analysis and empirical analysison the models?
ambiguity, as well as their perfor-mances on the task of recognizing disorder men-tions on datasets with a substantial number of dis-contiguous entities.
When the true output structureis given, which is still ambiguous in all models, ourmodels show that it is easier to produce the desiredmention combinations compared to the linear-chainCRF model with reasonable heuristics.
We note thatan extension similar to semi-Markov or weak semi-Markov (Muis and Lu, 2016) is possible for ourmodels.
We leave this for future investigations.The supplementary material and our implementa-tions for the models are available at:http://statnlp.org/research/ieAcknowledgmentsWe would like to thank the anonymous reviewers fortheir helpful feedback, and also the ShARe/CLEFeHealth Evaluation Lab for providing us the dataset.This work is supported by MOE Tier 1 grantSUTDT12015008.83ReferencesRami Al-Rfou and Steven Skiena.
2012.
SpeedRead: AFast Named Entity Recognition Pipeline.
Proceedingsof COLING 2012, pages 51?66.Masayuki Asahara and Yuji Matsumoto.
2003.
JapaneseNamed Entity Extraction with Redundant Morpholog-ical Analysis.
In Proceedings of HLT-NAACL ?03, vol-ume 1, pages 8?15.James K Baker.
1979.
Trainable Grammars for SpeechRecognition.
Journal of the Acoustical Society ofAmerica, 65(S1):S132.Daniel M. Bikel, Scott Miller, Richard M. Schwartz,and Ralph Weischedel.
1997.
Nymble: a high-performance learning name-finder.
Proceedings of thefifth conference on Applied Natural Language Pro-cessing (ANLP ?97), pages 194?201.Andrew Borthwick and John Sterling.
1998.
NYU: De-scription of the MENE named entity system as usedin MUC-7.
In Proceedings of the 7th Message Under-standing Conference (MUC-7).Jenny Rose Finkel and Christopher D. Manning.
2009.Nested named entity recognition.
In Proceedings ofthe 2009 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP ?09), volume 1, pages141?150.Radu Florian, Hany Hassan, Abraham Ittycheriah,Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,H Nicolov, and Salim Roukos.
2004.
A statisticalmodel for multilingual entity detection and tracking.In Proceedings of HLT-NAACL ?04, pages 1?8.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.2004.
Discovering Relations Among Named Entitiesfrom Large Corpora.
Proceedings of the 42nd An-nual Meeting on Association for Computational Lin-guistics, pages 415?422.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of International Conference onMachine Learning (ICML ?01), pages 282?289.Wei Lu and Dan Roth.
2015.
Joint Mention Extractionand Classification with Mention Hypergraphs.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP ?15),pages 857?867.Andrew McCallum and Wei Li.
2003.
Early resultsfor named entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In Proceedings of HLT-NAACL ?03, volume 4, pages188?191.Aldrian Obaja Muis and Wei Lu.
2016.
Weak Semi-Markov CRFs for Noun Phrase Chunking in InformalText.
In Proceedings of HLT-NAACL ?16, pages 714?719.David Nadeau and Satoshi Sekine.
2007.
A survey ofnamed entity recognition and classification.
Lingvisti-cae Investigationes, 30(1):3?26.Sameer Pradhan, Noe?mie Elhadad, Wendy W. Chapman,Suresh Manandhar, and Guergana Savova.
2014a.SemEval-2014 Task 7: Analysis of Clinical Text.
InProceedings of the 8th International Workshop on Se-mantic Evaluation (SemEval 2014), pages 54?62.Sameer Pradhan, Noe?mie Elhadad, Brett R. South, DavidMartinez, Lee Christensen, Amy Vogel, Hanna Suomi-nen, Wendy W. Chapman, and Guergana Savova.2014b.
Evaluating the state of the art in disorderrecognition and normalization of the clinical narrative.Journal of the American Medical Informatics Associa-tion : JAMIA, 22(1):143?54.Lev Ratinov and Dan Roth.
2009.
Design Challengesand Misconceptions in Named Entity Recognition.
InProceedings of the Thirteenth Conference on Com-putational Natural Language Learning (CoNLL ?09),pages 147?155.Satoshi Sekine.
1998.
NYU: Description of the JapaneseNE system used for MET-2.
In Proceedings of the 7thMessage Understanding Conference (MUC-7).Hanna Suominen, Sanna Salantera?, Sumithra Velupillai,Wendy W. Chapman, Guergana Savova, Noemie El-hadad, Sameer Pradhan, Brett R. South, Danielle L.Mowery, Gareth J. F. Jones, Johannes Leveling, LiadhKelly, Lorraine Goeuriot, David Martinez, and GuidoZuccon, 2013.
Overview of the ShARe/CLEF eHealthEvaluation Lab 2013, pages 212?231.
Springer BerlinHeidelberg, Berlin, Heidelberg.Buzhou Tang, Hongxin Cao, Yonghui Wu, Min Jiang,and Hua Xu.
2013.
Recognizing clinical entitiesin hospital discharge summaries using Structural Sup-port Vector Machines with word representation fea-tures.
BMC medical informatics and decision making,13 Suppl 1(Suppl 1):S1.Kristina Toutanova, Dan Klein, and Christopher D Man-ning.
2003.
Feature-rich part-of-speech tagging witha cyclic dependency network.
In Proceedings of HLT-NAACL ?03, volume 1, pages 252?259.Jun Xu, Yaoyun Zhang, Jingqi Wang, Yonghui Wu, andMin Jiang.
2015.
UTH-CCB : The Participation of theSemEval 2015 Challenge Task 14.
In Proceedings ofthe 9th International Workshop on Semantic Evalua-tion (SemEval 2015), pages 311?314.Yaoyun Zhang, Jingqi Wang, Buzhou Tang, YonghuiWu, Min Jiang, Yukun Chen, and Hua Xu.
2014.UTH CCB: A report for SemEval 2014 ?
Task 7 Anal-ysis of Clinical Text.
In Proceedings of the 8th Inter-national Workshop on Semantic Evaluation (SemEval2014), pages 802?806.84
