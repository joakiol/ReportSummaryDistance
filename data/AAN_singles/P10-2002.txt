Proceedings of the ACL 2010 Conference Short Papers, pages 6?11,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsA Joint Rule Selection Model for Hierarchical Phrase-based Translation?Lei Cui?, Dongdong Zhang?, Mu Li?, Ming Zhou?, and Tiejun Zhao?
?School of Computer Science and TechnologyHarbin Institute of Technology, Harbin, China{cuilei,tjzhao}@mtlab.hit.edu.cn?Microsoft Research Asia, Beijing, China{dozhang,muli,mingzhou}@microsoft.comAbstractIn hierarchical phrase-based SMT sys-tems, statistical models are integrated toguide the hierarchical rule selection forbetter translation performance.
Previouswork mainly focused on the selection ofeither the source side of a hierarchical ruleor the target side of a hierarchical rulerather than considering both of them si-multaneously.
This paper presents a jointmodel to predict the selection of hierar-chical rules.
The proposed model is esti-mated based on four sub-models where therich context knowledge from both sourceand target sides is leveraged.
Our methodcan be easily incorporated into the prac-tical SMT systems with the log-linearmodel framework.
The experimental re-sults show that our method can yield sig-nificant improvements in performance.1 IntroductionHierarchical phrase-based model has strong ex-pression capabilities of translation knowledge.
Itcan not only maintain the strength of phrase trans-lation in traditional phrase-based models (Koehnet al, 2003; Xiong et al, 2006), but also char-acterize the complicated long distance reorderingsimilar to syntactic based statistical machine trans-lation (SMT) models (Yamada and Knight, 2001;Quirk et al, 2005; Galley et al, 2006; Liu et al,2006; Marcu et al, 2006; Mi et al, 2008; Shen etal., 2008).In hierarchical phrase-based SMT systems, dueto the flexibility of rule matching, a huge numberof hierarchical rules could be automatically learntfrom bilingual training corpus (Chiang, 2005).SMT decoders are forced to face the challenge of?This work was finished while the first author visited Mi-crosoft Research Asia as an intern.proper rule selection for hypothesis generation, in-cluding both source-side rule selection and target-side rule selection where the source-side rule de-termines what part of source words to be translatedand the target-side rule provides one of the candi-date translations of the source-side rule.
Improperrule selections may result in poor translations.There is some related work about the hierarchi-cal rule selection.
In the original work (Chiang,2005), the target-side rule selection is analogous tothe model in traditional phrase-based SMT systemsuch as Pharaoh (Koehn et al, 2003).
Extendingthis work, (He et al, 2008; Liu et al, 2008) in-tegrate rich context information of non-terminalsto predict the target-side rule selection.
Differentfrom the above work where the probability dis-tribution of source-side rule selection is uniform,(Setiawan et al, 2009) proposes to select source-side rules based on the captured function wordswhich often play an important role in word re-ordering.
There is also some work considering toinvolve more rich contexts to guide the source-siderule selection.
(Marton and Resnik, 2008; Xionget al, 2009) explore the source syntactic informa-tion to reward exact matching structure rules orpunish crossing structure rules.All the previous work mainly focused on eithersource-side rule selection task or target-side ruleselection task rather than both of them together.The separation of these two tasks, however, weak-ens the high interrelation between them.
In this pa-per, we propose to integrate both source-side andtarget-side rule selection in a unified model.
Theintuition is that the joint selection of source-sideand target-side rules is more reliable as it conductsthe search in a larger space than the single selec-tion task does.
It is expected that these two kindsof selection can help and affect each other, whichmay potentially lead to better hierarchical rule se-lections with a relative global optimum instead ofa local optimum that might be reached in the pre-6vious methods.
Our proposed joint probabilitymodel is factored into four sub-models that canbe further classified into source-side and target-side rule selection models or context-based andcontext-free selection models.
The context-basedmodels explore rich context features from bothsource and target sides, including function words,part-of-speech (POS) tags, syntactic structure in-formation and so on.
Our model can be easily in-corporated as an independent feature into the prac-tical hierarchical phrase-based systems with thelog-linear model framework.
The experimental re-sults indicate our method can improve the systemperformance significantly.2 Hierarchical Rule Selection ModelFollowing (Chiang, 2005), ?
?, ??
is used to repre-sent a synchronous context free grammar (SCFG)rule extracted from the training corpus, where ?and ?
are the source-side and target-side rule re-spectively.
Let C be the context of ?
?, ??.
For-mally, our joint probability model of hierarchicalrule selection is described as follows:P (?, ?|C) = P (?|C)P (?|?,C) (1)We decompose the joint probability model intotwo sub-models based on the Bayes formulation,where the first sub-model is source-side rule se-lection model and the second one is the target-siderule selection model.For the source-side rule selection model, we fur-ther compute it by the interpolation of two sub-models:?Ps(?)
+ (1?
?
)Ps(?|C) (2)where Ps(?)
is the context-free source model(CFSM) and Ps(?|C) is the context-based sourcemodel (CBSM), ?
is the interpolation weight thatcan be optimized over the development data.CFSM is the probability of source-side rule se-lection that can be estimated based on maximumlikelihood estimation (MLE) method:Ps(?)
=??
Count(?
?, ??)Count(?
)(3)where the numerator is the total count of bilin-gual rule pairs with the same source-side rule thatare extracted based on the extraction algorithm in(Chiang, 2005), and the denominator is the totalamount of source-side rule patterns contained inthe monolingual source side of the training corpus.CFSM is used to capture how likely the source-side rule is linguistically motivated or has the cor-responding target-side counterpart.For CBSM, it can be naturally viewed as a clas-sification problem where each distinct source-siderule is a single class.
However, considering thehuge number of classes may cause serious datasparseness problem and thereby degrade the clas-sification accuracy, we approximate CBSM by abinary classification problem which can be solvedby the maximum entropy (ME) approach (Bergeret al, 1996) as follows:Ps(?|C) ?
Ps(?|?,C)=exp[?i ?ihi(?, ?,C)]???
exp[?i ?ihi(??
, ?, C)](4)where ?
?
{0, 1} is the indicator whether thesource-side rule is applied during decoding, ?
= 1when the source-side rule is applied, otherwise?
= 0; hi is a feature function, ?i is the weightof hi.
CBSM estimates the probability of thesource-side rule being selected according to therich context information coming from the surfacestrings and sub-phrases that will be reduced tonon-terminals during decoding.Analogously, we decompose the target-side ruleselection model by the interpolation approach aswell:?Pt(?)
+ (1?
?
)Pt(?|?,C) (5)where Pt(?)
is the context-free target model(CFTM) and Pt(?|?,C) is the context-based tar-get model (CBTM), ?
is the interpolation weightthat can be optimized over the development data.In the similar way, we compute CFTM by theMLE approach and estimate CBTM by the MEapproach.
CFTM computes how likely the target-side rule is linguistically motivated, while CBTMpredicts how likely the target-side rule is appliedaccording to the clues from the rich context infor-mation.3 Model Training of CBSM and CBTM3.1 The acquisition of training instancesCBSM and CBTM are trained by ME approach forthe binary classification, where a training instanceconsists of a label and the context related to SCFGrules.
The context is divided into source context7Figure 1: Example of training instances in CBSM and CBTM.and target context.
CBSM is trained only basedon the source context while CBTM is trained overboth the source and the target context.
All thetraining instances are automatically constructedfrom the bilingual training corpus, which have la-bels of either positive (i.e., ?
= 1) or negative (i.e.,?
= 0).
This section explains how the training in-stances are constructed for the training of CBSMand CBTM.Let s and t be the source sentence and targetsentence,W be the word alignment between them,rs be a source-side rule that pattern-matches asub-phrase of s, rt be the target-side rule pattern-matching a sub-phrase of t and being aligned to rsbased on W , and C(r) be the context features re-lated to the rule r which will be explained in thefollowing section.For the training of CBSM, if the SCFG rule?rs, rt?
can be extracted based on the rule extrac-tion algorithm in (Chiang, 2005), ??
= 1, C(rs)?is constructed as a positive instance, otherwise??
= 0, C(rs)?
is constructed as a negative in-stance.
For example in Figure 1(a), the context ofsource-side rule ?X1 hezuo?
that pattern-matchesthe phrase ?youhao hezuo?
produces a positiveinstance, while the context of ?X1 youhao?
thatpattern-matches the source phrase ?de youhao?
or?shuangfang de youhao?
will produce a negativeinstance as there are no corresponding plausibletarget-side rules that can be extracted legally1.For the training of CBTM, given rs, supposethere is a SCFG rule set {?rs, rkt ?|1 ?
k ?
n}extracted from multiple distinct sentence pairs inthe bilingual training corpus, among which we as-sume ?rs, rit?
is extracted from the sentence pair?s, t?.
Then, we construct ??
= 1, C(rs), C(rit)?1Because the aligned target words are not contiguous and?cooperation?
is aligned to the word outside the source-siderule.as a positive instance, while the elements in {??
=0, C(rs), C(rjt )?|j 6= i ?
1 ?
j ?
n} are viewedas negative instances since they fail to be appliedto the translation from s to t. For example in Fig-ure 1(c), Rule (1) and Rule (2) are two differentSCFG rules extracted from Figure 1(a) and Figure1(b) respectively, where their source-side rules arethe same.
As Rule (1) cannot be applied to Fig-ure 1(b) for the translation and Rule (2) cannotbe applied to Figure 1(a) for the translation either,??
= 1, C(ras ), C(rat )?
and ??
= 1, C(rbs), C(rbt )?are constructed as positive instances while ??
=0, C(ras ), C(rbt )?
and ??
= 0, C(rbs), C(rat )?
areviewed as negative instances.
It is noticed thatthis instance construction method may lead to alarge quantity of negative instances and choke thetraining procedure.
In practice, to limit the sizeof the training set, the negative instances con-structed based on low-frequency target-side rulesare pruned.3.2 Context-based features for ME trainingME approach has the merit of easily combiningdifferent features to predict the probability of eachclass.
We incorporate into the ME based modelthe following informative context-based featuresto train CBSM and CBTM.
These features arecarefully designed to reduce the data sparsenessproblem and some of them are inspired by pre-vious work (He et al, 2008; Gimpel and Smith,2008; Marton and Resnik, 2008; Chiang et al,2009; Setiawan et al, 2009; Shen et al, 2009;Xiong et al, 2009):1.
Function word features, which indicatewhether the hierarchical source-side/target-side rule strings and sub-phrases covered bynon-terminals contain function words that areoften important clues of predicting syntacticstructures.82.
POS features, which are POS tags of theboundary source words covered by non-terminals.3.
Syntactic features, which are the constituentconstraints of hierarchical source-side rulesexactly matching or crossing syntactic sub-trees.4.
Rule format features, which are non-terminal positions and orders in source-side/target-side rules.
This feature interactsbetween source and target components sinceit shows whether the translation ordering isaffected.5.
Length features, which are the lengthof sub-phrases covered by source non-terminals.4 Experiments4.1 Experiment settingWe implement a hierarchical phrase-based systemsimilar to the Hiero (Chiang, 2005) and evaluateour method on the Chinese-to-English translationtask.
Our bilingual training data comes from FBIScorpus, which consists of around 160K sentencepairs where the source data is parsed by the Berke-ley parser (Petrov and Klein, 2007).
The ME train-ing toolkit, developed by (Zhang, 2006), is used totrain our CBSM and CBTM.
The training size ofconstructed positive instances for both CBSM andCBTM is 4.68M, while the training size of con-structed negative instances is 3.74M and 3.03M re-spectively.
Following (Setiawan et al, 2009), weidentify function words as the 128 most frequentwords in the corpus.
The interpolation weights areset to ?
= 0.75 and ?
= 0.70.
The 5-gram lan-guage model is trained over the English portionof FBIS corpus plus Xinhua portion of the Giga-word corpus.
The development data is from NIST2005 evaluation data and the test data is fromNIST 2006 and NIST 2008 evaluation data.
Theevaluation metric is the case-insensitive BLEU4(Papineni et al, 2002).
Statistical significance inBLEU score differences is tested by paired boot-strap re-sampling (Koehn, 2004).4.2 Comparison with related workOur baseline is the implemented Hiero-like SMTsystem where only the standard features are em-ployed and the performance is state-of-the-art.We compare our method with the baseline andsome typical approaches listed in Table 1 whereXP+ denotes the approach in (Marton and Resnik,2008) and TOFW (topological ordering of func-tion words) stands for the method in (Setiawan etal., 2009).
As (Xiong et al, 2009)?s work is basedon phrasal SMT system with bracketing transduc-tion grammar rules (Wu, 1997) and (Shen et al,2009)?s work is based on the string-to-dependencySMT model, we do not implement these two re-lated work due to their different models from ours.We also do not compare with (He et al, 2008)?swork due to its less practicability of integratingnumerous sub-models.Methods NIST 2006 NIST 2008Baseline 0.3025 0.2200XP+ 0.3061 0.2254TOFW 0.3089 0.2253Our method 0.3141 0.2318Table 1: Comparison results, our method is signif-icantly better than the baseline, as well as the othertwo approaches (p < 0.01)As shown in Table 1, all the methods outper-form the baseline because they have extra mod-els to guide the hierarchical rule selection in someways which might lead to better translation.
Ap-parently, our method also performs better than theother two approaches, indicating that our methodis more effective in the hierarchical rule selectionas both source-side and target-side rules are se-lected together.4.3 Effect of sub-modelsDue to the space limitation, we analyze the ef-fect of sub-models upon the system performance,rather than that of ME features, part of which havebeen investigated in previous related work.Settings NIST 2006 NIST 2008Baseline 0.3025 0.2200Baseline+CFSM 0.3092?
0.2266?Baseline+CBSM 0.3077?
0.2247?Baseline+CFTM 0.3076?
0.2286?Baseline+CBTM 0.3060 0.2255?Baseline+CFSM+CFTM 0.3109?
0.2289?Baseline+CFSM+CBSM 0.3104?
0.2282?Baseline+CFTM+CBTM 0.3099?
0.2299?Baseline+all sub-models 0.3141?
0.2318?Table 2: Sub-model effect upon the performance,*: significantly better than baseline (p < 0.01)As shown in Table 2, when sub-models are inte-9grated as independent features, the performance isimproved compared to the baseline, which showsthat each of the sub-models can improve the hier-archical rule selection.
It is noticeable that the per-formance of the source-side rule selection modelis comparable with that of the target-side rule se-lection model.
Although CFSM and CFTM per-form only slightly better than the others amongthe individual sub-models, the best performance isachieved when all the sub-models are integrated.5 ConclusionHierarchical rule selection is an important andcomplicated task for hierarchical phrase-basedSMT system.
We propose a joint probabilitymodel for the hierarchical rule selection and theexperimental results prove the effectiveness of ourapproach.In the future work, we will explore more usefulfeatures and test our method over the large scaletraining corpus.
A challenge might exist whenrunning the ME training toolkit over a big sizeof training instances from the large scale trainingdata.AcknowledgmentsWe are especially grateful to the anonymous re-viewers for their insightful comments.
We alsothank Hendra Setiawan, Yuval Marton, Chi-Ho Li,Shujie Liu and Nan Duan for helpful discussions.ReferencesAdam L. Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A Maximum Entropy Ap-proach to Natural Language Processing.
Computa-tional Linguistics, 22(1): pages 39-72.David Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Proc.ACL, pages 263-270.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 New Features for Statistical Machine Trans-lation.
In Proc.
HLT-NAACL, pages 218-226.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable Inference and Training ofContext-Rich Syntactic Translation Models.
In Proc.ACL-Coling, pages 961-968.Kevin Gimpel and Noah A. Smith.
2008.
Rich Source-Side Context for Statistical Machine Translation.
InProc.
the Third Workshop on Statistical MachineTranslation, pages 9-17.Zhongjun He, Qun Liu, and Shouxun Lin.
2008.
Im-proving Statistical Machine Translation using Lexi-calized Rule Selection.
In Proc.
Coling, pages 321-328.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proc.
EMNLP.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proc.
HLT-NAACL, pages 127-133.Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.2008.
Maximum Entropy based Rule SelectionModel for Syntax-based Statistical Machine Trans-lation.
In Proc.
EMNLP, pages 89-97.Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.2007.
Forest-to-String Statistical Translation Rules.In Proc.
ACL, pages 704-711.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical MachineTranslation.
In Proc.
ACL-Coling, pages 609-616.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical Ma-chine Translation with Syntactified Target LanguagePhrases.
In Proc.
EMNLP, pages 44-52.Yuval Marton and Philip Resnik.
2008.
Soft SyntacticConstraints for Hierarchical Phrased-Based Trans-lation.
In Proc.
ACL, pages 1003-1011.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-Based Translation.
In Proc.
ACL, pages 192-199.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
In Proc.
ACL,pages 311-318.Slav Petrov and Dan Klein.
2007.
Improved Inferencefor Unlexicalized Parsing.
In Proc.
HLT-NAACL,pages 404-411.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency Treelet Translation: Syntactically In-formed Phrasal SMT.
In Proc.
ACL, pages 271-279.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
ANew String-to-Dependency Machine Translation Al-gorithm with a Target Dependency Language Model.In Proc.
ACL, pages 577-585.Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,and Ralph Weischedel.
2009.
Effective Use of Lin-guistic and Contextual Information for StatisticalMachine Translation.
In Proc.
EMNLP, pages 72-80.Hendra Setiawan, Min Yen Kan, Haizhou Li, and PhilipResnik.
2009.
Topological Ordering of FunctionWords in Hierarchical Phrase-based Translation.
InProc.
ACL, pages 324-332.10Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Cor-pora.
Computational Linguistics, 23(3): pages 377-403.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum Entropy Based Phrase Reordering Model forStatistical Machine Translation.
In Proc.
ACL-Coling, pages 521-528.Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.2009.
A Syntax-Driven Bracketing Model forPhrase-Based Translation.
In Proc.
ACL, pages315-323.Kenji Yamada and Kevin Knight.
2001.
A Syntax-based Statistical Translation Model.
In Proc.
ACL,pages 523-530.Le Zhang.
2006.
Maximum entropy mod-eling toolkit for python and c++.
avail-able at http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html.11
