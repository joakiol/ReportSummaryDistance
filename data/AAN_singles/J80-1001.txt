Cascaded ATN GrammarsWill iam A. WoodsBolt  Beranek  and Newman,  Inc.50 Mou l ton  St reetCambr idge ,  Massachuset ts  02138A generalization of the notion of ATN grammar, called a cascaded ATN (CATN),is presented.
CATN's permit a decomposition of complex language understandingbehavior into a sequence of cooperating ATN's with separate domains of responsibility,where each stage (called an ATN transducer) takes its input from the output of theprevious stage.
The paper includes an extensive discussion of the principle of factoring-- conceptual factoring reduces the number of places that a given fact needs to berepresented in a grammar, and hypothesis factoring reduces the number of distincthypotheses that have to be considered during parsing.1.
IntroductionATN grammars, as presented in Woods (1970),are a form of augmented pushdown store automata,augmented to carry a set of register contents in ad-dition to state and stack information and to permitarbitrary computational tests and actions associatedwith the state transitions.
Conceptually, an ATNconsists of a network of states with connecting arcsbetween them.
Each arc indicates a kind of con-stituent that can cause a transition between thestates it connects.
The states in the network can beconceptually divided into "levels" corresponding tothe different constituents that can be recognized.Each such level has a start state and one or morefinal states.
Transitions are of three basic types, asindicated by three different types of arc.
A WRD(or CAT) transition corresponds to the consumptionof a single word from the input string, a JUMPtransition corresponds to a transition from one stateto another without consuming any of the in.putstring, and a PUSH transition corresponds to theconsumption of a phrase parsed by a subordinateinvocation of some level of the network to recognizea constituent.ATN's  have the advantage of being a class ofautomata into which ordinary context-free phrasestructure and "augmented" phrase structure gram-mars have a straightforward embedding, but whichpermit various transformations to be performed toproduce grammars that can be more efficient thanthe original.
Such transformations can reduce thenumber of states or arcs in the grammar or can re-duce the number of alternative hypotheses that needto be explicitly considered during parsing.
(Sometransformations tend to reduce both, but in generalthere is a tradeoff between the two).
Both kinds ofefficiency result from a principle that I have called"factor ing",  which amounts to merging commonparts of alternative paths in order to reduce thenumber of alternative combinations explicitly enu-merated.
The former ("conceptual factoring") re-suits from factoring common parts of the grammarto make the grammar as compact as possible, whilethe latter ("hypothesis factoring") results from ar-ranging the grammar so as to  factor common partsof the hypotheses that will be enumerated at parsetime.Conceptual  factoring promotes ease of humancomprehension of the grammar and should facilitatelearning of grammars by machine.
Hypothesis fac-toring promotes efficiency of run time execution.
Inthis paper, I will present a generalization of the no-tion of ATN grammar, called a cascaded ATN orCATN, that capitalizes further on the principle offactoring in a manner similar to serial decompositionof finite state machines.
A CATN consists of asequence of ATN transducers each of which takes itsinput from the output of the previous.
An ATNtransducer is an ATN that includes among its ac-tions an output operation ( "TRANSMIT" )  whichcan be executed on arcs to generate elements of anoutput sequence.
Such an ATN cascade gains afactoring advantage from merging common compu-tations at early stages of the cascade.Copyright 1980 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material isgranted provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright?
notice are included on the first page.
To copy otherwise, or to republish, requires a fee and/or specific permission.0362-613X/80/010001-12501.00American Journal of Computational Linguistics, Volume 6, Number 1, January-March 1980 1William A.
Woods Cascaded ATN GrammarsCascaded ATN's are analogous to serial decom-position of finite state machines and carry many ofthe advantages of such decomposition into the do-main of more general recognition automata.
Thenormal decomposition of natural language descrip-tion into levels of phonology, lexicon, syntax, se-mantics, and pragmatics can be viewed as a cascadeof ATN transducers - one for each of the individuallevels.
Viewing natural language understanding asparsing with such a cascade has computational ad-vantages and also provides an efficient, systematicframework for characterizing the relationships be-tween different levels of analysis due to conceptualfactoring.
The factoring advantages of cascade de-compositions can thus serve as a partial explanationof why such a componential description of naturallanguage understanding has arisen.2.
Factoring in ATN's  and Phrase StructureGrammarsAs discussed above, the principle of factoringinvolves the merging of common parts of alternativepaths through an ATN or similar structure in orderto minimize the number of combinations.
This canbe done either to reduce the size of the grammar orto reduce the number of alternative hypotheses con-sidered at parse time.
Conceptual factoring attemptsto reduce the size of the grammar by minimizing thenumber of places in the grammar where the same orsimilar constituents are recognized.
Frequently suchfactoring results from "hiding" some of the differ-ences between two paths in registers so that thepaths are otherwise the same and can be merged.For example, in order to represent number agree-ment between a subject and a verb, one could havetwo distinct paths through the grammar - one topick up a singular subject and correspondingly in-fleeted verb, and one to pick up a plural subject andits verb.
By keeping the number of the subject in aregister, however, one can merge these two paths sothere is only one push to pick up the subject nounphrase and one push to pick up the main verb.In other cases, conceptual factoring results frommerging common initial, final, and/or  medial se-quences of paths across a constituent hat are notthe same, but which share subsequences.
For exam-ple, an interrogative sentence can start with an aux-iliary verb followed by the subject noun phrase,while a declarative can start with a noun phrasefollowed by the auxiliary.
In either case, however,the subsequent constituents that can make up thesentence are the same and the grammar paths torecognize them can be merged.
Moreover, in eithercase there can be initial prepositional phrases beforeeither the subject or the auxiliary and again thesecan be merged.
When one begins to represent hedetails of supporting auxiliaries that are present ininterrogatives but not in declaratives, the commonal-ities these modalities have with imperatives, and theinteraction of all three with the various possibilitiesfollowing the verb (depending on whether it istransitive or intransitive, takes an indirect object orcomplement, etc.
), this kind of factoring becomesincreasingly more important.In ordinary phrase structure grammars (PSG's),the only mechanism for capturing the kinds of merg-ing discussed above is the mechanism of recursion or"pushing" for constituent phrases.
In order to cap-ture the equivalent of the above merging of com-monality between declaratives and interrogatives,one would have to treat the subject-auxiliary pair asa constituent of some kind (an organization that islinguistically counter-intuitive).
Alternatively, onecan capture such factoring in a PSG by emulating anATN - e.g., by constructing a phrase structure rulefor every arc in the ATN and treating the states atthe ends of the arc as constituents.
Specifically, anarc from s l to s2 that picks up a phrase p can berepresented by a phrase structure rule sl - -> p s2,and a final state s3 can be expressed by an "e rule"s3 --> e (where e represents the "empty string").In either case, one is forced to introduce a "push"to a lower level of recursion where it is not neces-sary for an ATN, and to introduce a kind of"constituent" that is motivated solely by principlesof factoring and not necessarily by any linguisticcriteria of constituenthood.A phrase structure grammar emulating an ATNas in the above construction will contain all of thefactoring that the ATN contains, but will not make adistinction between the state name and the phrasename.
Failure to make this distinction masks theintuitions of state transition that lead to some of theATN optimization transformations and the concep-tual understanding of the operation of ATN's asparsing automata.
The difference here is a lot likethe difference between the way that LISP imple-ments list structure in terms of an underlying binarybranching "cons" cell and the way that it is appro-priate to view lists for conceptual reasons.
For ex-actly the same kinds of reasons, it is appropriate tothink of certain sequences of constituents that makeup a phrase as sequences of immediate constituentsrather than as a right-recursive nest of binarybranching phrases.From the perspective of hypothesis factoring, thedistinction made in an ATN between states that canbe recursively pushed to and states that merely markintermediate stages in the recognition of a constitu-ent sequence permits a distinction between that partof a grammar that is essentially finite state (andhence amenable to certain kinds of optimization)and that which is inherently recursive.
This permitssuch operations as mechanically eliminating unneces-2 American Journal of Computational Linguistics, Volume 6, Number 1, January-March 1980Wil l iam A.
Woods Cascaded ATN Grammarssary recursion and performing finite-state optimiza-tions procedures on what remains - see Woods(1969).
These transformations can result in signifi-cant gains in parsing efficiency by trading recursionfor iteration wherever possible and by minimizingthe non-determinism (by hypothesis factoring) in theresulting networks.The construction given above for emulating anATN with a PSG can, of course, emulate the samehypothesis factoring optimization that an ATN per-mits, but its ability to do so depends critically on theuse of e-rules for the final states.
Most parsers forPSG's, on the other hand, do not permit e-rules,probably because they are highly non-deterministicwhen applied bottom-up.
Unfortunately, the con-struction that transforms a PSG with e-rules into anequivalent PSG with no e-rules would give up someof the factoring achieved in the ATN emulationwhen applied to final states that are not obligatorilyfinal (a common occurrence in natural languagegrammars).
Every transition coming into such astate, would effectively be duplicated - once leadingto an unambiguously final state (sl --> p), andonce forcing subsequent consumption of additionalinput (sl --> p s2).
It thus appears that as a classof formal automata, ATN's permit a greater flexibili-ty in capturing hypothesis factoring advantages thando conventional PSG's.As we have discussed them, the principles ofconceptual factoring and hypothesis factoring havebeen motivated by different measures of cost.
Nev-ertheless, many of the factoring transformations thatcan be applied to ATN's gain a simultaneous effi-ciency in both dimensions.
This is not always thecase however.
In particular, the transformationsthat optimally minimize nondeterminism for left-to-right parsing tend to cause an increase in the num-ber of states and arcs in a grammar (unless fortui-tous regularity causes a collapsing).
Since a majorcharacteristic of the ATN grammar formalism is thatit permits the expression of mechanical algorithmsfor performing hypothesis factoring transformations,it is probably appropriate for grammar writers todevote their attention to conceptual factoring as agrammar writing style, while leaving to variousgrammar compilation algorithms the task of trans-forming the grammar into an efficient parsing en-gine.
However, in absence of such compilers, it isalways possible within the ATN formalism for agrammar writer to incorporate xplicit hypothesisfactoring structure into his grammar and to maketradeoffs between the two factoring principles.3.
Notat ionATN's are characterized as automata by specify-ing their computations in terms of instantaneousconfigurations and a transition function that com-putes possible successor configurations.
As such,they can admit a variety of superficial syntaxes,without changing the essential nature of the automa-ton.
In this paper, I will use a notation that issomewhat more concise and slightly more conven-ient than the original ATN syntax specified inWoods (1970).
The major change will be a formaldistinction between a phrase type and an initial statefor recognizing a phrase.
(The original ATN speci-fication used the initial state to serve double duty.
)Moreover, I will permit a given phrase type to haveseveral distinct initial states and several phrase typesto share some initial states.
This permits somewhatgreater flexibility in factoring and sharing commonparts of different phrase types.
The pop arcs ofthese ATN's will indicate the phrase type being pop-ped, and a given state can be a final state for severalphrase types.
A BNF specification of the syntax Iwill use is given, in Figure 1 on the next page.A simple example, using the conventions given inthe figure, is the following grammar:(m (accepts q)(sl (initial q)('a s2 (setr n 1)))(s2(q s3 (setr n !
(1 + !c)))(J s3))(s3('b s4))(s4(pop q !n)))This grammar is equivalent (minus augmentation) tothe phrase structure grammar: q-->'a'b, q-->'aq'b.It parses a string of n a's followed by nb 's  and(through its augments) pops the number n.4.
Cascaded ATN'sThe advantages of having semantic and pragmaticinformation available at early stages of parsing natu-ral language sentences have been demonstrated in avariety of systems, i Ways of achieving such closeinteraction between syntax and semantics have tra-ditionally involved writing semantic interpretationrules in 1-1 correspondence with phrase structurerules (e.g., Thompson, 1963), writing "semanticgrammars" that integrate syntactic and semanticconstraints in a single grammar (e.g., Burton, 1976),or writing ad hoe programs that combine such infor-mation in unformalized ways.
The first approachrequires as many syntactic rules as semantic rules,and hence is not really much different from the se-1 There are some compensating disadvantages if thesemantic domain is more complex than the syntactic one, butwe will assume here that immediate semantic feedback isdesired.American Journal of Computat ional  Linguistics, Volume 6, Number 1, January-March 1980 3Will iam A.
Woods Cascaded ATN Grammars<ATN> -> (<machinename> (accepts <phrasetype>*) <statespec>*);an ATN is a list consisting of a machine name, a;specification of the phrasetypes which it will;accept, and a list of sta?e specifications.<statespec> -> (<statename> {optional <initialspec>} <arc>*)<initialspec> -> (initial <phrasetype>*) ;indicates that this state;is an initial state for the indicated phrgsetypes.<arc> -> (<phrasetype> <nextstate> <act>*) ;a transition that;consumes a phrase of indicated type.-> (<pattern> <nextstate> <act>*) ;a transition that consumes;an input element that matches a pattern.-> (J <nextstate> <act>*) ;a transition that jumps to a new;state without consuming any input.-> (POP <phrasetype> <form>) ;indicates a final state;for the indicated phrase type and specifies;a form to be returned as its structure.<nextstate> -> <statename><pattern> -> ( <pattern>* )-> <wordlist>-> &-> <form>-> <<classname>>;specifies next state for a transition.
;matches a list whose elements match;the successive specified patterns.
;matches any word in the list.
;matches any element.
;matches any subsequence.
;matches value of <form>.
;matches anything that has or inherits;the class name as a feature.<wordlist> -> {'<word> I '<word>, <wordlist>}<act> -> (transmit <form>) ;transmit value of form as an output.-> (setr <registername> <form>) ;set register to value of form.-> (addr <registername> <form>) ;add the value of form to the;end of the list in the indicated register (assumed;initially NIL when the register has not been set).-> (require <proposition>) ;abort path if proposition is false.-> (dec <flaglist>) ;set indicated flags.-> (req <flagproposition>) ;abort path if proposition is false.-> (once <flag>) ;equivalent to (req (not <flag>)) (dec <flag>).<flagproposit ion> -> <boolean combination of flag registers><proposition> -> <form> ;the proposition is false if the value;of the form is NIL.<form> -> !<registername> ;returns contents of the register.-> '<liststructure> ;returns a copy of a list structure;except that any expressions preceded by !
are;replaced by their value and any preceded;by @ have their value inserted as a sublist.-> !c ;contents of the current constituent register.-> !<liststructure> ;returns value of list structure;interpreted as a functional expression.Figure 1.
BNF specif ication of ATN syntax,mantic grammar approach (this is the conventionalway of defining semantics of programming lan-guages).
The second approach has the tendency tomiss generalities and its results do not automaticallyextend to new domains.
It misses syntactic generali-ties, for example, by having to duplicate the syntac-tic information necessary to characterize the deter-miner structures of noun phrases for each of thedifferent semantic kinds of noun phrase that can beaccepted.
Likewise, it tends to miss semantic gener-al izations by repeating the same semantic tests invarious places in the grammar when a given seman-tic constituent can occur in various places in a sen-tence.
The third approach, of course, may yieldsome level of  operational system, but does not usu-ally shed any light on how such interaction shouldbe organized, and is difficult to extend.4 American Journal of Computational Linguistics, Volume 6, Number 1, January-March 1980William A.
Woods Cascaded ATN GrammarsRusty Bobrow's RUS parser (Bobrow, 1978) isthe first parser to my knowledge to make a cleanseparation between syntactic and semantic specifica-tion while gaining the benefit of early and incremen-tal semantic filtering and maintaining the factoringadvantages of an ATN.
It can be characterized as acascade of two ATN's  - one doing syntactic analysisand one doing semantic interpretation.
Such a cas-cade of ATN's  provides a way to reduce having tosay the same thing multiple times or in multipleplaces, while providing efficiency comparable to a"semantic" grammar and at the same time maintain-ing a clean separation between syntactic and seman-tic levels of description.
It is essentially a mecha-nism for permitting decomposition of an ATN gram-mar into an assembly of cooperating ATN's,  eachwith its own characteristic domain of responsibility.As mentioned previously, a CATN is a sequenceof ordinary ATN's  that include among the actionson their arcs an operation TRANSMIT,  which trans-mits an element to the next machine in the se-quence.
The first machine in the cascade takes itsinput from the input sequence, and subsequent ma-chines take their input from the TRANSMIT  com-mands of the previous ones.
The output of the finalmachine in the cascade is the output of the machineas a whole.
The only feedback from later stages toearlier ones is a filtering function that causes pathsof the nondeterministic computation to die if a laterstage cannot accept the output of an earlier one.The conception of cascaded ATN's  arose fromobserving the interaction between the lexical retriev-al component and the "pragmatic" grammar of theHWIM speech understanding system (Woods et al,1976).
The lexical retrieval component made use ofa network that consumed successive phonemes fromthe output of an acoustic phonetic recognizer andgrouped them into words.
Because of phonologicaleffects across word boundaries, this network couldconsume several phonemes that were part of thetransition into the next word before determiningthat a given word was possibly present.
At certainpoints, it would return a found word together with anode in the network at which matching should beginto find the next word (essentially a state remember-ing how much of the next word has already beenconsumed due to the phonological word boundaryeffect).
This can be viewed as an ATN that con-sumes phonemes and transmits words as soon as itshas enough evidence that the word is there.The lexical retrieval component of HWIM canthus be viewed as an ATN whose output drives an-other ATN.
This led to the conception of a com-plete speech understanding system as a cascade ofATN's,  one for acoustic phonetic recognition, onefor lexical retrieval (word recognition), one for syn-tax, one for semantics, and one for subsequent dis-course tracking.
A predecessor of the RUS parser(Bobrow, 1978) was subsequently perceived to bean instance of a syntax/semantics ascade, since thesemantic structures that it was obtaining from thelexicon to filter the paths through the grammarcould be viewed as ATN's.
Hence, practical solu-tions to problems of combinatorics in two differentproblem areas have independently motivated compu-tation structures that can be viewed as cascadedATN's.
It remains to be seen how effectively cas-cades can be used to model acoustic phonetic recog-nition or to track discourse structure, but the possi-bilities are intriguing.4.1 Spec i f icat ion of a CATN Computat ionAs with ordinary ATN's  and other formal auto-mata, the specification of the computat ion of aCATN will consist of the specification of an instan-taneous "configuration" of the automaton and thespecification of a transition function that computespossible successor configurations for any given con-figuration.
Since CATN's  are nondeterministic, agiven configuration can in general have more thanone successor configuration and may occasionallyhave no successor.
One way to implement a parserfor CATN's  would be to explicitly mimic this formalspecification by implementing the configurations asdata structures and writing a program to implementthe transition function.
Just as for ordinary ATN's,however, there are also many other ways to organizea parser, with various efficiency tradeoffs.A configuration of a CATN consists of a vectorof state configurations of the successive machines,plus a pointer to the input string where the firstmachine is about to take input.
The transition func-tion (nondeterministic) operates as follows:I.
A distinguished register C is set (possiblynondeterministically) to the next input ele-ment to be consumed and the pointer in theinput string is advanced.
Then a stagecounter k is set to 1.2.
The state of the kth machine in the se-quence is used to determine a set of arcsthat may consume the current input(possibly following a sequence of JUMPs,PUSHes, and POPs to reach a consumingtransition).3.
Whenever a transmission operation TRANS-MIT is executed by the stage k machine, thestage k+ 1 configuration is activated to proc-ess that input, and the stage k+ 1 componentof the configuration vector is updated ac-cordingly.
If the k+l  stage cannot acceptthe transmitted structure, the configurationis aborted.American Journal of Computational Linguistics, Volume 6, Number 1, January-March 1980 5William A.
Woods Cascaded ATN GrammarsAs for a conventional ATN, the format of thestate configurations of the individual machines con-sist of a state name, a set of registers and contents,and a stack pointer (or its equivalent).
2 Each ele-ment of a stack is a pair consisting of a PUSH arcand a set of register contents.
Transitions within asingle stage are the same as for ordinary ATN's.4.2 Uses of CATN'sA good illustrative example of the use of cascad-ed ATN's  for natural language understanding wouldbe a three stage machine consisting of a first stagethat performs lexical analysis, a second stage forsyntactic analysis, and a third stage for semanticanalysis.
The lexical stage ATN would consumeletters from an input sequence and perform wordidentification, including inflectional analysis, tenseextraction (e.g., BEEN => PASTPART BE), de-composit ion of contractions, and aggregation ofcompound phrases, producing as its output a se-quence of words with syntactic categories and fea-ture values.
This machine could also perform cer-tain standard bottom-up, locally determined parsingssuch as constructing noun phrase structures forproper nouns and pronouns.
Ambiguity in syntacticclass, in word grouping, and in homographs within asyntactic class can all be taken care of by the non-determinism of this first stage machine (e.g., "saw"as a past tense of "see" vs present tense of "saw"can be treated by two different alternative outputsof the lexical stage).This lexical stage machine is not likely to involveany recursion, unlike other stages of the cascade,but does use its registers to perform a certainamount of buffering before deciding what to trans-mit to the next stage.
Because stages such as thisone will reach states where they have essentiallyfinished with a particular construction and are readyto begin a new one, a convenient action to haveavailable on their arcs is one to reset al or a speci-fied set of registers to their initial empty valuesagain.
Such register clearing is similar to that whichhappens on a push to a lower level, except that herethe previous values need not be saved.
The use of aregister clearing action thus has the desired effectwithout the expense of a push.The second stage machine in our example willperform the normal phrase grouping functions of asyntactic grammar and produce TRANSMIT  com-mands when it has identified constituents that areserving specific syntactic roles.
The third stage ma-chine will consume such constituents and incorpo-2 For example, Earley's algorithm for context freegrammars (Earley, 1968) replaces the stack pointer with apointer to a place where the configuration(s) that caused thepush can be found.
A similar technique can be used withATN grammars.rate them into an incremental interpretation of theutterance (and may also produce differential ikeli-hoods for alternative interpretations depending onthe semantic and pragmatic onsistency and plausi-bility of the partial interpretation).The advantage of having a separate stage for thesemantic interpretation, in addition to providing aclean separation between syntactic and semanticlevels of description and a more domain- independentsyntactic level, is that during the computation, dif-ferent partial semantic interpretations that have thesame initial syntactic structure share the same syn-tactic processing.
In a single "semantic" ATN, suchdifferent semantic interpretation possibilities wouldhave to make their own separate syntact ic/semanticpredictions with no sharing of the syntactic com-monal ity between those predictions.
CascadedATN'S  avoid this while retaining the benefit  ofstrong semantic onstraint.4.3 Benef i ts  of CATN'sThe decomposit ion of a natural language analyzerinto a cascade of ATN's  gains a "factoring" advan-tage similar to that which ATN's  themselves providewith respect to ordinary phrase structure grammars.Specifically, the cascading allows alternative config-urations in the later stages of the cascade to sharecommon processing in the earlier stages that wouldotherwise have to be done independently.
That is, ifseveral semantic hypotheses can use a certain kindof constituent at a given place, there need be onlyone syntactic process to recognize it.
3Cascades also provide a simpler overall descrip-tion of the acceptable input sequences than a singlemonolithic ATN combining all of the informationinto a single network would give.
That is, if anysemantic level process can use a certain kind of con-stituent at a given place, then there need be onlyone place in the syntactic stage ATN that will recog-nize it.
Conversely, if there are several syntacticcontexts in which a constituent filling a given se-mantic role can be found, there need be only oneplace in the semantic ATN to receive that role.
(Asingle network covering the same facts would beexpected to have a number of states on the order ofthe product, rather than the sum, of the numbers ofstates in the individual stages of the cascade.
)3 One might ask at this point whether there are situa-tions in which one cannot ell what is present locally without"top-down" guidance from later stages.
In fact, any suchlater stage guidance can be implemented by semantic filter-ing of syntactic possibilities.
For example, if there is a givensemantic ontext hat permits a constituent construction thatis otherwise not legal, one can still put the recognition trans-itions for that construction into the syntactic ATN with anaction on the first transition to check compatibility with laterstage expectations (e.g., by transmitting a flag indicating thatit is about o try to recognize this special construction).6 American Journal of Computational Linguistics, Volume 6, Number 1, January-March 1980William A.
Woods Cascaded ATN GrammarsAn additional advantage provided by the factor-ing commonality introduced by the cascade is thatthe resulting localization of early stage activities in asingle place provides a single place for a given lin-guistic fact to be learned, rather than independentversions of essentially the same fact having to belearned in different semantic contexts.
Moreover,the separation of the stages of the cascade providesa decomposition of the overall problem into individ-ually learnable skills.
These facts may be significantnot only for theories of human language develop-ment and use, but also for computer systems thatcan be easily debugged and can contribute to theirown acquisition of improved language skill.The above facts suggest hat the traditional char-acterization of natural language in terms of pho-nemes, syllables, words, phrases, sentences, andhigher level pragmatic onstructs may be more deep-ly significant than just a convenience for scientificmanipulation.4.4 Parsing w i th  CATN'sConceptually, each ATN in a cascade produces(nondeterministically) a sequence of inputs for thenext stage, which the next stage then parses.
Onecould implement a computer parsing algorithm for acascade in several ways.
For example, the individualcomponents of a configuration could be incrementedas described above, with the later stages advancedas soon as the earlier stages transmit something.Alternatively, the later stages could wait until theearlier stages have completed a path through theinput sequence before they begin to process theoutput of the earlier stages.
The latter approach hasthe advantage of not performing second stage analy-sis on a path that will eventually, fail at the firststage.
On the other hand, it will result in the firststage occasionally continuing to extend partial pathsthat could already be rejected at the second stage.In general, one can envisage an implementationin which the second stage can wait until the firststage has proceeded some distance past the currentpoint before commencing its operations.
This couldeither be done by having a fixed " lookahead" par-ameter which would always run the first stage somenumber of transmissions ahead of the second stage,or one could have a command that the first stagecould execute when it considered its current pathsufficiently likely to make it worthwhile for the sec-ond stage to operate on it.
In fact, to handle bothof these cases, one could simply have the first stagebuffer its information in registers until it is ready forthe next stage to work on it and only then performthe transmissions.
For the remainder of this paper, Iwill assume that this is done and that the next stagebegins to operate as soon as its input is transmitted.As presented above, an instantaneous configura-tion of a CATN is essentially a vector of configura-tions for the individual stages of the cascade.
Let uscall the individual configurations IC's  and the vectoras a whole a configuration vector.
Since any twoconfiguration vectors having the same IC in somecomponent will perform the same computation forthat component  and will only differ when theytransmit to a subsequent stage, a parsing implemen-tation should merge such common components andonly perform their processing once.
This can beachieved by representing the set of instantaneousconfigurations of the CATN not simply as a set ofIC vectors, but as a tree structure (TC) that mergesthe common initial parts of those vectors.
That is,each vector representing an instantaneous configura-tion of the CATN will be represented by a paththrough the TC from root to leaf, with the succes-sive nodes in the path being the successive IC's ofthe vector.
It is straightforward to transform thetransition function that computes uccessor configu-ration vectors into a transition function that com-putes successor TC's  from a given TC.The TC representation has the characteristic thatas long as the common left parts of configurationvectors are merged, the computation of a given ICat some level k will be done only once.
To fullycapitalize on the factoring advantages of this repre-sentation, one would like to assure that the commoninitial parts of alternative configuration vectors re-main merged.
This happens automatically for alter-native stage k+l  computations that stem from acommon stage k configuration.
However, it is possi-ble for two distinct k stage configurations, whichhave gone their separate ways and accumulated theirown trees of higher level configurations, to comeagain to essentially the same k-stage configurationvia different paths.
This can happen especially withlexical stage computations when one word is recog-nized and the parsing of the next word begins.
Toprovide maximum factoring, it is thus necessary tocheck for such cases and merge subtrees when theIC's at their heads are found to be equivalent.When the k-stage network happens to be a finitestate machine (i.e., makes no use of registers orrecursion) the detection of a duplicate configurationis easy due to the simple equivalence test (i.e.,sameness of s tate) .
When it is a general ATN, thedetection of the conditions for merging are some-what more involved (due to the register contents),and the likelihood of such merging being possibletends to be less.
Hence for such stages the cost ofchecking for duplication may not be worth the bene-fit.
Interestingly, it appears that the early stages ofphonetic, lexical, and simple phrase recognition dohave essentially finite state transition networks,while those of the later stages, where such sharing isAmerican Journal of Computational Linguistics, Volume 6, Number 1, January-March 1980 7William A.
Woods Cascaded ATN Grammarsnot as important or as likely, is more apt to requirenon-finite-state r gister activities.4.5 Comparison of Cascading wi th  RecursionSome interesting questions arise when consider-ing the nature of cascaded ATN's  as automata.
Forexample, since a number of activities that are nor-mally done with recursion in ATN's  and otherphrase structure grammars can be done by separatestages of a cascade, one is led to wonder about therelationship between cascading and recursion.
Thatis, instead of arcs of an ATN pushing for a constitu-ent of a certain kind, occasionally a cascade can beset up to find constituents of that kind and transmitthem to a later stage of the cascade as units.
Aparticular example, which has occasionally beenproposed informally, would be for an early stageprocessor to group the input words into basic nounphrases, verb groups, etc.
and for a later stage totake such units as input.
Clearly this is a task nor-mally performed by recursion.
One might then won-der whether cascading was just another form of re-cursion, or somehow equivalent o it.It turns out that cascading is in some respectsweaker than recursion, and in other respects it ismore powerful.
In the next section, I will give anexample of a context free cascade that can recognizea language that cannot be recognized by a singlecontext free ATN.
Hence, cascading clearly increas-es the power of a basic ATN beyond that providedby recursion alone.
On the other hand, one is con-siderably more constrained in the way he can usecascading when writing a grammar than he is in theuse of recursion.
For example, indefinitely deeprecursion can be used to recognize noun phrasesinside prepositional phrases inside noun phrases, etc.When setting up a cascade of two ATN's  to performsuch grouping, the earlier cascade cannot model thisdirectly, but instead would have to recognize"elementary" noun phrases consisting of, say, deter-miner, adjectives, and head noun, and would uselooping transitions to accept subsequent preposition-al phrases and relative clauses.
Moreover, this stageof the cascade could not content itself solely withthe noun phrases,  but would also have to transmitthe other elements of the sentence (auxiliaries,verbs, adverbs, particles, etc.)
so that the laterstages of the cascade will have a chance to see them.That is, a stage of a cascade provides a level of de-scription of the entire input sequence in terms of asequence of units to be transmitted to a later stageof analysis.
Hence it appears that cascading is afundamental ly different operation that interacts withrecursion and overlaps some of its functions in inter-esting ways.Another interesting comparison arises betweencascaded ATN's  and the kinds of transformationsused in a transformational grammar.
If one attemptsto use a transformational grammar by successivelyapplying its transformations in reverse to the surfacestring, one repeatedly performs a partitioning of theinput into a sequence of units as described above.That is, in applying a reverse transformation to asyntax tree in the course of a reverse transforma-tional analysis, the operation of matching the pat-tern description of the transformation to the syntaxtree amounts to finding a level at which the syntaxtree can be "cut" yielding a sequence of unitsmatching the sequence of elements in the pattern ofthe rule.
This is exactly the kind of partitioning ofthe input into units that is done by a stage of a cas-caded ATN.
Moreover, the result of the transfor-mation is expressed by a "r ight-hand-side" of thetransformational rule, which may reorder the inputsequence into a slightly modified sequence, and maycopy an element several times, modify it in certainrestricted ways, or even delete it (under suitablerestrictions).
In exactly the same way, a stage of acascade can transmit the units that it has picked upin a different order than it found them, can dupli-cate a unit, drop a unit, insert a constant, and trans-mit units that are modified from the form in whichthey were recognized.
In short, a stage of an ATNcascade can mirror the activity of any given trans-formational rule.However,  transformational  rules arc normallyconsidered to apply in a cycle governed by the num-ber of levels of embedding of clauses in the sen-tence, so that the number of successive transforma-tions applied can be unbounded.
By contrast, in anATN cascade, there are only a finite number ofstages in the cascade.
Moreover: successive trans-formations in a transformational grammar are free todiscard everything that was learned about the struc-ture of the input in the matching of the previoustransformation and there is no constraint that themanner in which a subsequent transformation ana-lyzes the result of the previous transformation bearany relationship to the level of description imposedon the input by that previous transformation.
In anATN cascade, there is an assumed sequence of prog-ressive aggregation and higher level of  descriptionimplied by the transduction of information to suc-cessive stages of the cascade, with each stage per-ceiving the input in the terms that it was describedby the previous.
Thus, the ATN cascade seems toimpose additional constraints on the process of lan-guage recognition that are not imposed by an ordi-nary transformational grammar.44 These constraints tend to promote the efficiency ofthe processing.
See Woods (1970) for a discussion of someof the inherent inefficiencies of an ordinary transformationalanalysis.8 American Journal of Computational Linguistics, Volume 6, Number 1, January-March 1980Wil l iam A.
Woods Cascaded ATN GrammarsbIf l  2 :~rans  ~ ~  ?~9b trans b ~ ",,~,r o .~  ~ c trans cc?j Jpop rFigure 2.
ATN cascade for {anbncn: n_> 1 }Experience with ATN grammars for natural lan-guage indicates that everything that a transforma-tional grammar of natural language does can bedone with even a single ATN, so there does notappear to be any need for more than a finite numberof stages of a cascade.
On the other hand, the argu-ments presented here indicate that one may be ableto obtain a simpler description of an overall set offacts with a cascade than with a single monolithicATN.
It is possible, therefore, that a cascade ofATN's corresponds to a more appropriate formaliza-tion of the underlying facts of language that gaverise to the original model of transformational gram-mar than does the conventional conception.4.6 A S imple  Formal ExampleAs a simple example of what a cascade of ATN'scan do, I will give here a simple ATN cascade thatwithout the use of registers can recognize the set ofstrings of the form n a's followed by n b's followedby n c's, for arbitrary n. This language is a tradi-tional example of a language that is not context freebut is context sensitive.
However, it does happen tobe specifiable as the intersection of two context freelanguages.
Capitalizing on this fact, it is possible torepresent it by a cascade of two "context free"ATN's (i.e., ATN's which do not use registers tocheck constraints between different constituents).This cascade effectively computes the intersection oftwo ways of viewing the input.
The two ATN's,whose structure is illustrated in Figure 2 above(where "trans" in the figure is short for "transmit"),can be written as follows:(ml (accepts q)(sl (initial p q)('a s2))(s2(p s3)('b s4 (transmit 'b)))(s3'('b s4 (transmit 'b)))(s4 (pop p)('c s5 (transmit 'c)))(s5 (pop q)('c s5 (transmit 'c))))(m2 (accepts r)(sl (initial r)('b s2))(s2(r s3)('c s4))(s3('c s4))(s4 (pop r)))These two machines correspond to the grammars:q-->pc*, p-->ab, p- ->apbandr-->bc, r - ->brcwith augmentation such that the b's and c's acceptedby the first grammar are passed through to be ac-cepted by the second.
The first stage checks thatthe number of a's and b's agree and accepts anynumber of o's, while the second stage requires thatthe b's and c's agree.4.7 Another  Example  - Syntax  and Semant icsAnother, less trivial example is the use of anATN cascade to represent syntactic and semanticknowledge sources of a language understanding sys-tem.
We will give here a brief example illustrating akind of cascading of syntactic and semantic knowl-edge similar to that done by R, Bobrow in his RUSparser (Bobrow, 1978).
A rough characterization fthis parser is that as the syntactic omponent worksits way through a noun phrase, it accumulates infor-mation about the determiner structure and initialpremodifiers of the head noun until it encounters theAmerican Journal of Computational Linguistics, Volume 6, Number 1, January-March 1980 9Will iam A.
Woods Cascaded ATN Grammarshead noun (i.e., takes a path corresponding to ahypothesis that it has found the head noun).
Atthat point, it begins to transmit information to thesemantic stage, starting with the head noun, andfollowed by the premodifiers of that noun.
Then itcontinues to pick up post modifiers of the nounphrase, transmitting them to the semantic stage as itencounters them, and finally, when it hypothesizesthat the noun phrase is completed, it transmits thedeterminer information.In a similar way, in the parsing of a clause, thesyntactic ATN can wait until it has encountered themain verb before transmitting that verb followed byits subject and any fronted adverbial modifiers.
Af-ter that it can transmit subsequent post verbal ele-ments as they are encountered, and finally transmitany governing modality information such as tense,aspect, and any governing negations.The example presented here, is a constructed oneto illustrate the principle, and does not directly rep-resent the analyses by the RUS grammar.
The ex-ample implements a subset of the semantic rules ofthe airline flight schedules ystem of Woods (1967),a predecessor of the LUNAR system (Woods eta1.,1972).
I will give here only a fragment of thesemantic stage ATN that understands designators(i.e., noun phrases).
It assumes that the syntacticstage operates as outlined above and, in particular,that it transmits prepositional phrases by transmit-ting the preposition and then transmitting its object.It also assumes that the syntax stage transmits asignal QUANT when it has hypothesized the end ofa noun phrase and is about to transmit he determin-er and number information.
One could alternativelytransmit prepositional phrases as single units to betested for syntactic and semantic features.
I willassume that a pattern such as <flight> on a con-suming arc is matched by a constituent that receivesthe indicated semantic marker (e.g., FLIGHT).
(m2 (accepts designators)(dl (initial designator)(J d2 (setr vbl (getnewvar)))(d2('flight,'plane d/flight (setr head 'FLIGHT))('jet d/flight (setr head 'FLIGHT)(addr mods '(JET !vbl)))('airline d/head (setr head 'AIRLINE))('city,'town d/head (setr head 'CITY))('airport,'place d/head (setr head 'AIRPORT))('time d/time)('fare d/fare)('owner,'operator d/owner))(d/owner('of d/owner-of))(d/owner-of(<flight> d/head (addr quants (getquant lc))(setr head '(OWNER !c))))(d/fare('(rood first-class),'(mod coach),'(mod stand by) d/fare(require (not class))(setr class It))('(mod one-way),'(mod round-trip) d/fare(require (not type))(setr type !c)))('from d/fare-from (require (not from)))('to d/fare-to (require (not to)))(J d/head (require class type from to)(setr head '(FARE !from lto !type !class)))(d/fare-from(<place> d/fare (addr quants (getquant !c))(setr from !c)))(d/fare-to(<place> d/fare (addr quants (getquant lc))(setr to !c)))(d/time('(mod departure) d/time (require (not op))(setr op 'DTIME))('(mod arrival) d/time (require (not op))(setr op 'ATIME))('of d/time-gf (require (not flight)))('in,'at d/time-prep (require (eq op 'ATIME)))('from d/time-prep (require (eq op 'DTIME)))(J d/head (require op flight place)(setr head '(!op !flight !c))(* e.g., (setr head'(ATIME AA-57 CHICAGO)))))(d/time-of(<flight> d/time (addr quants (getquant !c))(setr flight !c)))(d/time-prep(<place> d/time (addr quants (getquant lc))(setr place !c)))(d/head('QUANT d/quant (setr mod \[(packmods))))(d/flight('from d/flight-from (require (not from)))('to d/flight-to (require (not to)))('(mod first-class),'(mod coach),'(mod jet-coach) d/flight (once class)(addr mods '(SERVCLASS !vbl It)))('(mod jet) d/flight (addr mods '(JET !vbl)))('(mod propeller) d/flight (once equip)(addr mods '(NOT (JET !vbl))))(J d/flight (once connect) (require from to)(addr mods '(CONNECT !vbl!
(sem from) l(sem to))))('QUANT d/quant (setr mod l(packmods))))10 American Journal of Computational Linguistics, Volume 6, Number 1, January-March 1980Will iam A.
Woods Cascaded ATN Grammars(d/flight-from(<place> d/flight(addr quants (getquant !c))(setr from !c))(d/flight-to(<place> d/flight(addr quants (getquant !c))(setr to !c)))(d/quant('some,'a,'any,'NIL d/some)('each,'every d/each)('all d/all)('not d/not)('the d/the)('this,'that d/this)('which,'what d/what)(<integer> d/integer))(d/some('sg,'pl d/end(setr quant '(FOR SOME !vbl /!head : !mod ; DLT))))(d/each('sg d/universal))(d/all('pl d/universal))(d/universal(J d/end (setr quant '(FOR EVERY !vbl /!head : !mod ; DLT))))(d/not('some d/not-some)('every d/not-every)('all d/not-all))(d/not-some('sg,'pl d/end(setr quant '(NOT (FOR SOME !vbl /!head : !mod ; DLT)))))(d/not-every('sg d/not-universal))(d/not-aU('pl d/not-universal))(d/not-universal(J d/end(etr quant '(NOT (FOR EVERY !vbl /!head : !mod ; DLT)))))(d/the('sg d/end (setr quant '(FOR THE !vbl /!head : !mod ; DLT)))('pl d/end (setr quant '(FOR EVERY !vbl /\[head : !mod ; DLT))))(d/this('sg d/end (setr quant '(FOR THE !vbl /!head : !mod ; DLT))))(d/what('sg d/end (setr quant'(FOR THE !vbl / !head :(AND !mod DLT) ;(PRINTOUT !vbl))))('pl d/end (setr quant'(FOR EVERY !vbl / !head :(AND !mod DLT) ;(PRINTOUT !vbl)))))(d/integer('sg,'pl d/end (setr quant'(FOR !integer MANY !vbl /!head : !mod ; DLT))))(d/end(pop <designator> (sem-quant!quants !quant !vbl))))In the above fragment grammar, the state dl getsa variable name to use for the recognized esigna-tor, the state d2 dispatches on the head noun of thedesignator phrase to various states that recognizemodifiers that are particular to the head.
Eventuallythe path for each such head will lead to the stated/quant, where the determiner and number informa-tion is picked up to build the quantifier that governsthis designator.
This transition is triggered by thetransmission of the flag QUANT from the syntaxstage, signaling that the noun phrase is complete andthe determiner information is coming.
Notice howthe quantification information that is common tomost designators i shared.The transitions that follow d/quant implementmost of the d-rules in Woods (1967), which is itselfa subset of the d-rules of the LUNAR system(Woods, et al.
1972; Woods, 1978b).
The functionsem-quant is a function that performs the sem-quantpair manipulations described in Woods (1978b).These manipulations usually embed the quantifierjust constructed (lquant) into the quantifier nestaccumulated from below (!quants) to form a quanti-fier nest to be passed up to a higher clause.
Theythen return the variable name (!vbl) as the "sem" tobe inserted into an argument position in the higherstructure.
The function getquant, here, is a functionthat extracts the quant from a structure that hasbeen passed up from below and is used to accumu-late the quantifier nest (quants) from subordinatedesignators that should dominate the quantifier ofthe designator being interpreted.
The functionpackmods examines the contents of the registermods and returns an AND of the roods if there areseveral, a single mod if there is only one, and T ifthere are none.American Journal of Computational Linguistics, Volume 6, Number 1, January-March 1980 11Wil l iam A.
Woods Cascaded ATN Grammars5.
Conclus ionsIn Woods (1977, 1978a; Woods & Brachman,1978), I discussed the general principle of hypothe-sis "factoring" - i.e., the coalescing of common partsof alternative hypotheses in such a way that an in-cremental hypothesis development and search algor-ithm does not need to individuate and consider sepa-rate hypotheses until sufficient information is pres-ent to make different predictions in the differentcases.
The most common example of factoring isthe well-known device called "decision trees" inwhich a cascade of questions at nodes of a treeleads eventually to selection of a particular "leaf" ofthe tree without explicit comparison to each of theindividual leaves.
If the tree is balanced, then thisleads to the selection of the desired individual eaf inlog(n) tests rather than n tests, where n is the num-ber of leaves of the tree.
Another example of fac-toring is the mechanism in ATN grammars wherebycommon parts of different phrase structure rules aremerged, thereby saving the redundant processing ofcommon parts of alternative hypotheses.One can think of an ATN as a generalization ofthe notion of decision tree to permit recursion, loop-ing, register augmentation, and recombination ofpaths.
In this paper, I have discussed a generaliza-tion of ATN's,  called cascaded ATN's  (CATN's) ,which provides additional factoring capabilities.
ACATN consists of a sequence of ATN transducersthe later stages of which take input from the outputof the previous stage.
ATN cascades permit a de-.composition of complex language understandingbehavior into a sequence of cooperating ATN's withseparate domains of responsibility.Of specific interest are two distinct notions ofthe concept of factoring that are beginning toemerge from such considerations.
One, which Ihave called hypothesis factoring, provides a reductionthrough sharing in the number of distinct hypothesesthat have to be explicitly considered uring parsing.The other, which I will call conceptual factoring,provides a reduction through sharing in the numberof times or places that a given fact or rule needs tobe represented in a long-term conceptual structure(e.g., the grammar).
The former promotes efficien-cy of "run-time" parsing, while the latter promotesefficiency of grammar maintenance and learning.
Inmany cases conceptual factoring promotes hypothe-sis factoring, but this is not necessarily always thecase.ReferencesBobrow, R.J. (1978).
"The RUS System", in B.L.
Webberand R.J. Bobrow, Research in Natural Language Under-standing, Quarterly Technical Progress Report No.
3.BBN Report No.
3878, Bolt Beranek and Newman Inc.,Cambridge, MA.
July.Burton, R.R.
(1976).
"Semantic Grammar: An EngineeringTechnique for Constructing Natural Language Under-standing Systems."
BBN Report No.
3453, Bolt Beranekand Newman Inc., Cambridge, MA, December.Earley, J.
(1968).
"An Efficient Context-free Parsing Algor-ithm."
Ph.D. thesis, Dept.
of Computer Science,Carnegie-Mellon University, Pittsburgh, PA.Thompson, F.B.
(1963).
"The Semantic Interface in Man-Machine Communication," Report No.
RM 63TMP-35,Tempo, General Electric Co., Santa Barbara, CA, Sep-tember.Woods, W.A.
(1967).
"Semantics for a Question-AnsweringSystem," Ph.D. thesis, Division of Engineering and Ap-plied Physics, Harvard University.
Also Report NSF-19,Harvard Computation Laboratory, September.
(Available from NTIS as PB-176-548, and from GarlandPublishing, Inc. as a volume in a new series: OutstandingDissertations in the Computer Sciences.
)Woods, W.A.
(1969).
"Augmented Transition Networks forNatural Language Analysis," Report No.
CS-I, AikenComputation Laboratory, Harvard University, Decem-ber.
(Available from ERIC as ED-037-733; also fromNTIS as Microfiche PB-203-527.
)Woods, W.A.
(1970).
"Transition Network Grammars forNatural Language Analysis," CACM, Vol.
13, No.
10,October.Woods, W.A.
(1977).
"Spinoffs From Speech UnderstandingResearch," in Panel Session on Speech Understandingand AI, Proceedings of the 5th Int.
Joint Conference onArtificial Intelligence, August 22-25, p. 972.Woods, W.A.
(1978a).
"Taxonomic Lattice Structures forSituation Recognition," in TINLAP-2, Conference onTheoretical Issues in Natural Language Processing-2,University of Illinois at Urbana-Champaign, July 25-27.
(Also in AJCL, Mf.
78, 1978:3).Woods, W.A.
(1978b).
"Semantics and Quantification inNatural Language Question Answering," in Advances inComputers, Vol.
17.
New York: Academic Press.
(AlsoReport No.
3687, Bolt Beranek and Newman Inc.)Woods, W.A., R.M.
Kaplan, and B.L.
Nash-Webber (1972).
"The Lunar Sciences Natural Language InformationSystem: Final Report," BBN Report No.
2378, Bolt Bera-nek and Newman Inc., Cambridge, MA, June.
(Available from NTIS as N72-28984.
)Woods, W.A., M. Bates, G. Brown, B. Bruce, C. Cook,J.
Klovstad, J. Makhoul, B. Nash-Webber, R. Schwartz,J.
Wolf, V. Zue (1976).
Speech Understanding Systems- Final Report, 30 October 1974 to 29 October 1976, BBNReport No.
3438, Vols.
I-V, Bolt Beranek and NewmanInc., Cambridge, MA.Woods, W.A.
and Brachman, R,J.
(1978).
Research in Natu-ral Language Understanding, Quarterly Technical Prog-ress Report No.
1, 1 September 1977 to 30 November1977, BBN Report No.
3742, Bolt Beranek and NewmanInc., Cambridge, MA, January.
(Now available fromNTIS as AD No.
AO53958).12 American Journal of Computat ional  Linguistics, Volume 6, Number 1, January-March 1980
