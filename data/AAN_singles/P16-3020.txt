Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ?
Student Research Workshop, pages 132?137,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsQA-It: Classifying Non-Referential It for Question Answer PairsTimothy LeeMath & Computer ScienceEmory UniversityAtlanta, GA 30322, USAtlee54@emory.eduAlex LutzMath & Computer ScienceEmory UniversityAtlanta, GA 30322, USAajlutz@emory.eduJinho D. ChoiMath & Computer ScienceEmory UniversityAtlanta, GA 30322, USAjinho.choi@emory.eduAbstractThis paper introduces a new corpus, QA-It,for the classification of non-referential it.Our dataset is unique in a sense that it is an-notated on question answer pairs collectedfrom multiple genres, useful for develop-ing advanced QA systems.
Our annotationscheme makes clear distinctions between 4types of it, providing guidelines for manyerroneous cases.
Several statistical modelsare built for the classification of it, show-ing encouraging results.
To the best of ourknowledge, this is the first time that such acorpus is created for question answering.1 IntroductionOne important factor in processing document-leveltext is to resolve coreference resolution; one of theleast developed tasks left in natural language pro-cessing.
Coreference resolution can be processed intwo steps, mention detection and antecedent resolu-tion.
For mention detection, the classification of thepronoun it as either referential or non-referential isof critical importance because the identification ofnon-referential instances of it is essential to removefrom the total list of possible mentions (Branco etal., 2005; Wiseman et al, 2015).Although previous work has demonstrated a lotof promise for classifying all instances of it (Boydet al, 2005; M?uller, 2006; Bergsma et al, 2008;Li et al, 2009), it is still a difficult task, especiallywhen performed on social networks data containinggrammatical errors, ambiguity, and colloquial lan-guage.
In specific, we found that the incorrect clas-sification of non-referential it was one of the majorreasons for the failure of a question answering sys-tem handling social networks data.
In this paper,we first introduce our new corpus, QA-It, sampledfrom the Yahoo!
Answers corpus and manually an-notated with 4 categories of it, referential-nominal,referential-others, non-referential, and errors.
Wealso present statistical models for the classificationof these four categories, each showing incrementalimprovements from one another.The manual annotation of this corpus is challeng-ing because the rhetoric used in this dataset is oftenambiguous; consequently, the automatic classifica-tion becomes undoubtedly more challenging.
Ourbest model shows an accuracy of ?78%, which islower than some of the results achieved by previ-ous work, but expected because our dataset is muchharder to comprehend even for humans, showingan inter-annotation agreement of ?65%.
However,we believe that this corpus provides an initiative todevelopment a better coreference resolution systemfor the setting of question answering.2 Related WorkThe identification of non-referential it, also knownas pleonastic it, has been studied for many years,starting with Hobbs (1978).
Although most of theseearlier approaches are not used any more, the rulesthey discovered have helped for finding useful fea-tures for later machine learning approaches.
Evans(2001) used 35 features and memory-based learn-ing to classify 7 categories of it using data sampledfrom the SUSANNE and BNC corpora.
Boyd etal.
(2005) took this approach and added 25 morefeatures to identify 5 categories of it.M?uller (2006) classified 6 categories of it usingspoken dialogues from the ICSI Meeting corpus.Bergsma et al (2008) used n-gram models to iden-tify it as either referential or non-referential.
Liet al (2009) used search queries to help classify 7categories of it.
Figure 2 shows how the annotationscheme for non-referential it has changed over time.Our approach differs from the recent work becausewe not only identify instances of it as either refer-132Nominal AnaphoricClause AnaphoricPleonasticCataphoricIdiomatic / StereotypicProactionDiscourse TopicNon-Referential 1.
Extra positional 2.
Cleft 3.
Weather/Time?
Condition/Place 4.
IdiomaticReferential Non-Referential1.
Cataphoric 2.
Proaction 3.
Discourse Topic 4.
Clause Anaphoric 5.
Pleonastic 6.
Idiomatic/?
Stereotypic1.
Referential - Nominal1.
Nominal Anaphoric2.
Referential - Others1.
Proaction 2.
Discourse Topic 3.
Clause Anaphoric3.
Non-Referential1.
Cataphoric?2.
Pleonastic 3.
Idiomatic/Stereotypic4.
ErrorsEvans (2001) Boyd (2005) Bergsma (2008) This Work (2016)Referential - Noun1.
Nominal Anaphoric Referential - Nominal1.
Nominal AnaphoricNon-Referential1.
Cataphoric 2.
Proaction 3.
Discourse Topic 4.
Pleonastic 5.
Idiomatic/?
StereotypicReferential - Clause1.
Clause AnaphoricLi (2009)Figure 1: The chronicles of non-referential it annotation schemes.ential or not, but also categorize whether referentialit refers to a nominal or others, providing corefer-ence resolution systems more valuable information.Furthermore, our corpus includes more colloquiallanguage, which makes it harder to disambiguatedifferent categories of it.3 Data CollectionWe inspected several corpora (e.g., Amazon prod-uct reviews, Wikipedia, New York Times, Yahoo!Answers), and estimated the maximum likelihoodof non-referential it in each corpus.
After thoroughinspection, the Yahoo!
Answers and the Amazonproduct reviews were found to contain the highestnumbers of it; however, an overwhelming percent-age of it in the Amazon product reviews was ref-erential.
On the other hand, the Yahoo!
Answersshowed great promise with over 35% instances ofnon-referential and referential-others it.
Thus, ques-tion answer pairs were uniformly sampled from 9genres in the Yahoo!
Answers corpus:1Computers and Internet,2Science and Mathematics,3Yahoo!
Products,4Education and Reference,5Business and Finance,6Entertainment and Music,7Society and Culture,8Health,9Politics and GovernmentThese genres contained the highest numbers of it.Each question answer pair was then ranked by thenumber of tokens it contained, ranging from 0 to20, 20 to 40, all the way from 200 to 220, to see theimpact of the document size on the classification ofit.
It is worth mentioning that our annotation wasdone on the document-level whereas annotationsfrom most of the previous work were done on thesentence-level.
While training our annotators, weconfirmed that the contextual information was vitalin classifying different categories of it.4 Annotation SchemeInstances of it are grouped into 4 categories in ourannotation scheme; referential-nominal, referential-others, non-referential, and errors (Figure 2).
Someof these categories are adapted from Evans (2001)who classified it into 7 categories; their categoriescaptured almost every form of it, thus linguisticallyvaluable, but a simpler scheme could enhance theannotation quality, potentially leading to more ro-bust coreference resolution.Boyd et al (2005) focused on the detection ofnon-referential it, and although their scheme was ef-fective, they did not distinguish referents that werenominals from the others (e.g., proaction, clause,discourse topic), which was not as suited for coref-erence resolution.
Bergsma et al (2008) attemptedto solve this issue by defining that only instancesof it referent to nominals were referential.
Li etal.
(2009) further elaborated above rules by addingreferential-clause; their annotation scheme is simi-lar to ours such that we both make the distinctionbetween whether it refers to a nominal or a clause;however, we include proaction and discourse topicto referential-others as well as cataphoric instancesto non-referential.Our aim is to generate a dataset that is useful fora coreference system to handle both nominal andnon-nominal referents.
With our proposed scheme,it is up to a coreference resolution system whetheror not to handle the referential-others category, in-cluding clause, proaction, and discourse topic, dur-ing the process of mention detection.
Furthermore,the errors category is added to handle non-pronouncases of it.
Note that we only consider referentialas those that do have antecedents.
If the pronoun iscataphoric, it is categorized as non-referential.133Genre Doc Sen Tok C1C2C3C4C?1.
Computers and Internet 100 918 11,586 222 31 24 3 2802.
Science and Mathematics 100 801 11,589 164 35 18 3 2203.
Yahoo!
Products 100 1,027 11,803 176 36 25 3 2404.
Education and Reference 100 831 11,520 148 55 36 2 2415. Business and Finance 100 817 11,267 139 57 37 0 2336.
Entertainment and Music 100 946 11,656 138 68 30 5 2417.
Society and Culture 100 864 11,589 120 57 47 2 2268.
Health 100 906 11,305 142 97 32 0 2719.
Politics and Government 100 876 11,482 99 81 51 0 231Total 900 7,986 103,797 1,348 517 300 18 2,183Table 1: Distributions of our corpus.
Doc/Sen/Tok: number of documents/sentences/tokens.
C1..4: numberof it-instances in categories described in Sections 4.1, 4.2, 4.3, and 4.4.4.1 Referential - NominalThis category is for anaphoric instances of it thatclearly refer to nouns, noun phrases, or gerunds.This is the standard use of it that is already beingreferenced by coreference resolution models today.4.2 Referential - OthersThis category is for any anaphoric instances ofit that do not refer to nominals.
Some anaphorareferents could be in the form of proaction, clauseanaphoras, or discourse topic (Evans, 2001).
Mostcoreference resolution models do not handle thesecases, but as they still have anaphora referents, itwould be valuable to indicate such category for thefuture advance of a coreference resolution system.4.3 Non-ReferentialThis category is for any extraposition, clefts, andpronouns that do not have referent.
This also in-cludes cataphora (Evans, 2001).
Our distinctionof non-referential it is similar to the one made byBoyd et al (2005), except that we do not includeweather, condition, time, or place in this categorybecause it would often be helpful to have thoseinstances of it be referential:What time is it now in Delaware US?It would be approximately 9:00 am.Many could argue that the second instance of it isnon-referential for the above example.
But whencontext is provided, it would be more informativeto have it refer to ?the time now in Delaware US?for coreference resolution.
If it is simply markedas non-referential, we would essentially be los-ing the context that the time in Delaware is 9:00am.
Although this does not appear many times inour corpus, it is important to make this distinctionbased on the context because without the context,this instance of it would be simply marked as non-referential.4.4 ErrorsThis category includes any uses of a non-pronounform of it including IT (Information Technology),disfluencies, and ambiguous it in book/song titles.When you leave a glass of water sitting aroundfor a couple hours or so , do bubbles form it itIn the example above, the two instances of it servesno purpose and cannot be identified as a potentialmisspelling of another word.
This category is notpresent in any of the previous work, but due to thenature of our corpus as mentioned in difficulties, itis included in our annotation scheme.5 Corpus Analytics5.1 Annotation DifficultiesThe Yahoo!
Answers contains numerous grammat-ical errors, ambiguous references, disfluency, frag-ments, and unintelligible question and answer pairs,all of which contributes to difficulties in annota-tion.
Ambiguous referencing had been problematicthroughout the annotation and sometimes an agree-ment was hard to reach between annotators:After selling mobile phones, I got post datedcheques ($170,000).
But he closed office andbank account.
help me?...
That?s a lot ofmoney to just let go.
If it were $1,700.00 thenI might just whoop his a** and let it go butfor $170,000... are you kidding?...134Here, it can be either idiomatic, or refer to the ?postdated cheque?
or the ?process of receiving the postdated cheque?
such that disambiguating its cate-gory is difficult even with the context.
There weremore of such cases where we were not certain if thereferent was referential-nominal, referential-others,or idiomatic; in which case, the annotators wereinstructed to use their best intuition to categorize.5.2 Inter-Annotation AgreementAll instances of it were double annotated by stu-dents trained in both linguistics and computer sci-ence.
Adjudication was performed by the authorsof this paper.
For the inter-annotator agreement,our annotation gave the Cohans Kappa score of65.25% and the observed proportionate agreementscore of 81.81%.5.3 Analysis By GenreThe genre has a noticeable influence on the rela-tive number of either referential or non-referentialinstances of it.
The genres with the lowest percent-age of referential-nominal are ?Society and Culture?and ?Politics and Government?.
These genres alsocontain the most abstract ideas and thoughts withinthe question and answer pairs.
The genres whichcontain the most number of referential-nominal are?Computers and Internet?, ?Science and Mathemat-ics?, and ?Yahoo!
Products?.
This makes sensebecause in each of these categories, the questionsand answers deal with specific, tangible objectssuch as ?pressing a button on the computer to unin-stall software?.
Overall, the more abstract the ques-tions and answers get, the more likely it is to usenon-referential it or referential-others.Figure 2: The proportion of referential-nominal foreach genre.
C1..3: the first 3 categories in Section 4,G1..9: the 9 genres in Table 1.5.4 Analysis By Document SizeThe document size shows a small influence on thecategorization of it.
The document group with themost instances of non-referential it is the smallestin size with a total number of tokens between 0and 20.
The rest of the document groups containsfewer instances of non-referential it although thedifferences are not as large as expected.Document Size C1C2C3C4C?0-20 21 60 20 0 10120-40 14 84 33 0 13140-60 27 100 33 1 16160-80 24 129 42 2 197100-120 29 132 56 2 219120-140 28 148 53 3 232140-160 32 163 68 2 265160-180 28 158 74 6 266180-200 43 190 70 0 303200-220 54 184 68 2 308Table 2: Distributions of our data for each docu-ment size.5.5 Importance of Contextual InformationIn certain cases, context is mandatory in determin-ing the category of it:Q: Regarding IT, what are the fastest ways ofgetting superich?A: Find something everyone will need andthen patent it.
It could be anything that woulddo with or about computers.
Look at RIM andthe struggle it is now facing.
With good maket-ing ANY enhancement or a new design couldbe worth millions.
However, the biggest pathto being rich is with maintenece or service ofsystems or with old programming languages.For the first instance of it, if the annotators are onlygiven the question, they possibly categorize it asreferential-nominal or referential-others.
However,we can confirm from further reading the contextthat it refers to the IT, ?Information Technology?.6 Experiments6.1 CorpusTable 4 shows the distributions of our corpus, splitinto training (70%), development (10%), and eval-uation (20%) sets.
A total of 1,500, 209, and 474instances of it is found in each set, respectively.135ModelDevelopment Set Evaluation SetACC C1C2C3C4ACC C1C2C3C4M072.73 82.43 35.48 57.14 0.00 74.05 82.65 49.20 71.07 0.00M173.21 82.56 50.00 62.50 0.00 74.68 82.93 53.14 73.33 0.00M273.08 82.56 49.41 60.00 - 75.21 83.39 51.23 73.95 -M376.44 82.31 64.75 - 77.14 82.26 67.87 -M476.92 83.45 61.90 - 78.21 83.39 68.32 -Table 3: Accuracies achieved by each model (in %).
ACC: overall accuracy, C1..4: F1 scores for 4categories in Section 4.
The highest accuracies are highlighted in bold.All data are tokenized, sentence segmented, part-of-speech tagged, lemmatized, and dependency parsedby the open-source NLP toolkit, NLP4J (Choi andPalmer, 2012; Choi and McCallum, 2013).1Set Doc Sen Tok C1C2C3C4TRN 630 5,650 72,824 927 353 209 11DEV 90 787 10,348 139 42 27 1TST 180 1,549 20,625 282 122 64 6Table 4: Distributions of our data splits.6.2 Feature TemplateFor each token wiwhose lemma is either it or its,features are extracted from the template in Table 5.wi?kand wi+kare the k?th preceding and succeed-ing tokens of wi, respectively.
h(wi) is the depen-dency head of wi.
The joint features in line 2 aremotivated by the rules in Boyd et al (2005).
Forinstance, with a sufficient amount of training data,features extracted from [wi+1.p + wi+2.m] shouldcover all rules such as [it + verb + to/that/what/etc].Three additional features are used, the relative posi-tion of wiwithin the sentence Sk(rpw; wi?
Sk),the relative distance of wifrom the nearest preced-ing noun wj(rdw; wj?
Sk), and the relative posi-tion of Skwithin the document D (rps; Sk?
D):rpw =i/t , t = # of tokens in Sk.rdw =|i?j|/t , t = # of tokens in Sk.rps =k/d , d = # of sentences in D.wi.p, wi?1.p, wi?2.p, h(wi).p, wi?1.m, h(wi).mwi+1.p + wi+2.m, wi+1.p + wi+2.p + wi+3.mwi.d , h(wi).dmTable 5: Feature template used for our experiments.p: part-of-speech tag, m: lemma, d: dependencylabel, dm: set of dependents?
lemmas.1https://github.com/emorynlp/nlp4jIt is worth mentioning that we experimented withfeatures extracted from brown clusters (Brown etal., 1992) and word embeddings (Mikolov et al,2013) trained on the Wikipedia articles, which didnot lead to a more accurate result.
It may be due tothe different nature of our source data, Yahoo!
An-swers.
We will explore the possibility of improvingour model by facilitating distributional semanticstrained on the social networks data.6.3 Machine LearningA stochastic adaptive gradient algorithm is usedfor statistical learning, which adapts per-coordinatelearning rates to exploit rarely seen features whileremaining scalable (Duchi et al, 2011).
Regular-ized dual averaging is applied for `1regularization,shown to work well with ADAGRAD (Xiao, 2010).In addition, mini-batch is applied, where each batchconsists of instances from k-number of documents.The following hyperparameters are found duringthe development and used for all our experiments:the learning rate ?
= 0.1, the mini-batch boundaryk = 5, the regularization parameter ?
= 0.001.6.4 EvaluationTable 3 shows the accuracies achieved by our mod-els.
M0is the baseline model using only the fea-tures extracted from Table.
M1uses the additionalfeatures of rpw, rdw, and rps in Section 6.2.
Theadditional features show robust improvements onboth the development and the evaluation sets.
No-tice that the F1 score for C4(errors) is consistently0; this is not surprising given the tiny amount oftraining instances C4has.
M2is experimented ondatasets where annotations for C4are discarded.
Asmall improvement is shown for M2on the evalua-tion set but not on the development set, where only1 instance of C4is found.M3and M4aim to classify instances of it into 2classes by merging C2and C3during either train-136ing (M3) or evaluation (M4).
Training with 3 cat-egories and merging the predicted output into 2categories during evaluation (M4) gives higher ac-curacies than merging the gold labels and trainingwith 2 categories (M3) in our experiments.7 ConclusionThis paper introduces a new corpus called, QA-It,sampled from nine different genres in the Yahoo!Answers corpus and manually annotated with fourcategories of it.2Unlike many previous work, ourannotation is done on the document-level, whichis useful for both human annotators and machinelearning algorithms to disambiguate different typesof it.
Our dataset is challenging because it includesmany grammatical errors, ambiguous references,disfluency, and fragments.
Thorough corpus ana-lysts are provided for a better understanding of ourcorpus.
Our corpus is experimented with severalstatistical models.
Our best model shows an accu-racy of 78%; considering the challenging natureof our corpus, this is quite encouraging.
Our workcan be useful for those who need to perform coref-erence resolution for question answering systems.In the future, we will double the size of our an-notation so we can train a better model and have amore meaningful evaluation.
We are also planningon developing a recurrent neural network modelfor the classification of it.ReferencesShane Bergsma, Dekang Lin, and Randy Goebel.
2008.Distributional Identification of Non-Referential Pro-nouns.
In Proceedings of the Annual Conferenceof the Association for Computational Linguistics,ACL?08, pages 10?18.Adriane Boyd, Whitney Gegg-Harrison, and DonnaByron.
2005.
Identifying Non-Referential it: AMachine Learning Approach Incorporating Linguis-tically Motivated Patterns.
In Proceedings of theACL Workshop on Feature Engineering for MachineLearning in Natural Language Processing, pages40?47.Ant?onio Branco, Tony McEnery, and Ruslan Mitkov,editors.
2005.
Anaphora Processing: Linguistic,Cognitive and Computational Modelling.
John Ben-jamins Publishing Company.Peter F. Brown, Peter V. deSouza, Robert L. Mercer,Vincent J. Della Pietra, and Jenifer C. Lai.
1992.Class-based n-gram Models of Natural Language.Computational Linguistics, 18(4):467?480.2https://github.com/emorynlp/qa-itJinho D. Choi and Andrew McCallum.
2013.Transition-based Dependency Parsing with Selec-tional Branching.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics, ACL?13, pages 1052?1062.Jinho D. Choi and Martha Palmer.
2012.
Fast and Ro-bust Part-of-Speech Tagging Using Dynamic ModelSelection.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguistics,ACL?12, pages 363?367.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive Subgradient Methods for Online Learningand Stochastic Optimization.
The Journal of Ma-chine Learning Research, 12(39):2121?2159.Richard Evans.
2001.
Applying Machine Learning To-ward an Automatic Classification of It.
Literary andLinguistic Computing, 16(1):45?57.Jerry R. Hobbs.
1978.
Resolving Pronoun References.Lingua, 44:331?338.Yifan Li, Petr Mus?
?lek, Marek Reformat, and LorenWyard-Scott.
2009.
Identification of Pleonastic ItUsing the Web.
Journal Of Artificial IntelligenceResearch, 34:339?389.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed Representa-tions of Words and Phrases and their Compositional-ity.
In Proceedings of Advances in Neural Informa-tion Processing Systems 26, NIPS?13, pages 3111?3119.Christoph M?uller.
2006.
Automatic detection of non-referential it in spoken multi-party dialog.
In 11thConference of the European Chapter of the Associa-tion for Computational Linguistics, EACL?06, pages49?56.Sam Wiseman, Alexander M. Rush, Stuart Shieber, andJason Weston.
2015.
Learning Anaphoricity andAntecedent Ranking Features for Coreference Res-olution.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguisticsand the 7th International Joint Conference on Natu-ral Language Processing (Volume 1: Long Papers),ACL?15, pages 1416?1426.Lin Xiao.
2010.
Dual Averaging Methods for Regular-ized Stochastic Learning and Online Optimization.Journal of Machine Learning Research, 11:2543?2596.137
