Estimating Upper and Lower Boundson the Performance of Word-Sense Disambiguation ProgramsWilliam GaleKenneth Ward ChurchDavid YarowskyAT&T Bell Laboratories600 Mountain Ave.Murray Hill, NJ 07974kwc@research.att.comAbstractWe have recently reported on two new word-sensedisambiguation systems, one trained on bilingualmaterial (the Canadian Hansards) and the other trainedon monolingual material (Roget's Thesaurus andGrolier's Encyclopedia).
After using both themonolingual and bilingual classifiers for a few months,we have convinced ourselves that the performance isremarkably good.
Nevertheless, we would really like tobe able to make a stronger statement, and therefore, wedecided to try to develop some more objectiveevaluation measures.
Although there has been a fairamount of literature on sense-disambiguation, theliterature does not offer much guidance in how we mightestablish the success or failure of a proposed solutionsuch as the two systems mentioned in the previousparagraph.
Many papers avoid quantitative evaluationsaltogether, because it is so difficult to come up withcredible stimates of performance.This paper will attempt to establish upper and lowerbounds on the level of performance that can be expectedin an evaluation.
An estimate of the lower bound of75% (averaged over ambiguous types) is obtained bymeasuring the performance produced by a baselinesystem that ignores context and simply assigns the mostlikely sense in all cases.
An estimate of the upper boundis obtained by assuming that our ability to measureperformance is largely limited by our ability obtainreliable judgments from human informants.
Notsurprisingly, the upper bound is very dependent on theinstructions given to the judges.
Jorgensen, for example,suspected that lexicographers tend to depend too muchon judgments by a single informant and foundconsiderable variation over judgments (only 68%agreement), as she had suspected.
In our ownexperiments, we have set out to find word-sensedisambiguation tasks where the judges can agree oftenenough so that we could show that they wereoutperforming the baseline system.
Under quitedifferent conditions, we have found 96.8% agreementover judges.1.
Introduction: Using Massive Lexicographic ResourcesWord-sense disambiguation is a long-standing problemin computational linguistics (e.g., Kaplan (1950), Yngve(1955), Bar-I-Iillel (1960), Masterson (1967)), withimportant implications for a number of practicalapplications including text-to-speech (TI'S), machinetranslation (MT), information retrieval (IR), and manyothers.
The recent interest in computationallexicography has fueled a large body of recent work onthis 40-year-old problem, e.g., Black (1988), Brown etal.
(1991), Choueka nd Lusignan (1985), Clear (1989),Dagan et al (1991), Gale et al (to appear), Hearst(1991), Lesk (1986), Smadja and McKeown (1990),Walker (1987), Veronis and Ide (1990), Yarowsky(1992), Zemik (1990, 1991).
Much of this work offersthe prospect that a disambiguation system might be ableto input unrestricted text and tag each word with themost likely sense with fairly reasonable accuracy andefficiency, just as part of speech taggers (e.g., Church(1988)) can now input unrestricted text and assign eachword with the most likely part of speech with fairlyreasonable accuracy and efficiency.The availability of massive lexicographic databasesoffers a promising route to overcoming the knowledgeacquisition bottleneck.
More than thirty years ago, Bar-I-Iillel (1960) predicted that it would be "futile" to writeexpert-system-like rules by-hand (as they had been doingat Georgetown at the time) because there would be noway to scale up such rules to cope with unrestrictedinput.
Indeed, it is now well-known that expert-system-like rules can be notoriously difficult to scale up, asSmall and Reiger (1982) and many others haveobserved:"The expert for THROW is currently six pages long.., butit should be 10 times that size.
"Bar-Hillel was very early in realizing the scope of theproblem; he observed that people have a large set offacts at their disposal, and it is not obvious how acomputer could ever hope to gain access to this wealthof knowledge.249" 'But why not envisage a system which will put thisknowledge at the disposal of the translation machine?
'Understandable as this reaction is, it is very easy to showits futility.
What such a suggestion amounts o, if takenseriously, is the requirement that a translation machineshould not only be supplied with a dictionary but also witha universal encyclopedia.
This is surely utterly chimericaland hardly deserves any further discussion.
Since,however, the idea of a machine with encyclopedicknowledge has popped up also on other occasions, let meadd a few words on this topic.
The number of facts wehuman beings know is, in a ceaain very pregnant sense,infinite."
(Bar-Hillel, 1960)Ironically, much of the research cited above is takingexactly the approach that Bar-Hillel ridiculed as utterlychimerical and hardly deserving of any furtherdiscussion.
Back in 1960, it may have been hard toimagine how it would be possible to supply a machinewith both a dictionary and an encyclopedia.
But muchof the recent work cited above goes much further; notonly does it supply a machine with a dictionary and anencyclopedia, but many other extensive references worksas well, including Roget's Thesaurus and numerouslarge corpora.
Of course, we are using these referenceworks in a very superficial way; we are certainly notsuggesting that the machine should attempt to solve the"AI  Complete" problem of "understanding" thesereference works.2.
A Brief Summary of Our Previous WorkOur own work has made use of many of these lexicalresources.
In particular, (Gale et al, to appear) achies'edconsiderable progress by using well-understoodstatistical methods and very large datasets of tens ofmillions of words of parallel English and French text(e.g., the Canadian Hansards).
By aligning the text aswe have, we were able to collect a large set of examplesof polysemous words (e.g., sentence) in each sense (e.g.,judicial sentence vs. syntactic sentence), by extractinginstances from the corpus that were translated one wayor the other (e.g, peine or phrase).
These data sets werethen analyzed using well-understood Bayesiandiscrimination methods, which have been used verysuccessfully in many other applications, especiallyauthor identification (Mosteller and Wallace, 1964,section 3.1) and information retrieval (IR) (vanRijsbergen, 1979, chapter 6; Salton, 1989, section 10.3),though their application to word-sense disambiguation isnovel.In author identification and information retrieval, it iscustomary to split the discrimination process up into atesting phase and a training phase.
During the trainingphase, we are given two (or more) sets of documents andare asked to construct a discriminator which candistinguish between the two (or more) classes of250documents.
These discriminators are then applied tonew documents during the testing phase.
In the authoridentification task, for example, the training set consistsof several documents written by each of the two (ormore) authors.
The resulting discriminator is then testedon documents whose authorship is disputed.
In theinformation retrieval application, the training set consistsof a set of one or more relevant documents and a set ofzero or more irrelevant documents.
The resultingdiscriminator is then applied to all documents in thelibrary in order to separate the more relevant ones fromthe less relevant ones.There is an embarrassing wealth of information in thecollection of documents that could be used as the basisfor discrimination.
It is common practice to treatdocuments as "merely" a bag of words, and to ignoremuch of the linguistic structure, especially dependencieson word order and correlations between pairs of words.In other words, one assumes that there are two (or more)sources of word probabilities, rel and irrel, in the IRapplication, and author t and author 2 in the authoridentification application.
During the training phase, weattempt to estimate Pr(wlsource) for all words w in thevocabulary and all sources.
Then during the testingphase, we score all documents as follows and select highscoring documents as being relatively likely to havebeen generated by the source of interest.Pr(wl rel) Information Retreival (IR)w ~ Pr(wl irrel)Pr( w l author l )w Eoe Pr(wlauthor2) Author IdentificationIn the sense disambiguation application, the 100-wordcontext surrounding instances of a polysemous word(e.g., sentence) are treated very much like a document.
1Pr( w l sense t )w in el~Iontext Pr(wlsensez) sense DisambiguationThat is, during the testing phase, we are given a newinstance of a polysemous word, e.g., sentence, and askedto assign it to one or more senses.
We score the wordsin the 100-word context using the formula given above,and assign the instance to sense t if the score is large.I.
It is common to use very small contexts (e.g., 5-words) based onthe observation that people seem to be able to disambiguate word-senses based on very little context.
We have taken a differentapproach.
Since we have been able to find useful information outto 100 words (and measurable information out to 10,000 words),we feel we might as well make use of the the larger contexts.
Thistask is very difficult for the machine; it needs all the help it can get.The conditional probabilities, Pr (w lsense) ,  aredetermined uring the training phase by counting thenumber of times that each word in the vocabulary wasfound near each sense of the polysemous word (and thensmoothing these estimates in order to deal with thesparse-data problems).
See Gale et al (to appear) forfurther details.At first, we thought that the method was completelydependent on the availability of parallel corpora fortraining.
This has been a problem since parallel textremains somewhat difficult to obtain in large quantity,and what little is available is often fairly unbalanced andunrepresentative of general language.
Moreover, theassumption that differences in translation correspond todifferences in word-sense has always been somewhatsuspect.
Recently, Yarowsky (1992) has found a way toextend our use of the Bayesian techniques by training onthe Roget's Thesaurus (Chapman, 1977) 2 and G-rolier'sEncyclopedia (1991) instead of the Canadian Hansards,thus circumventing many of the objections to our use ofthe Hansards.
Yarowsky (1992) inputs a 100-wordcontext surrounding a polysemous word and scores eachof the 1042 Roget Categories by:1-\[ P r (w lRoget  Categoryi )w in contextThe program can also be run in a mode where it takesunrestricted text as input and tags each word with itsmost likely Roget Category.
Some results for the wordcrane are presented below, showing that the program canbe used to sort a concordance by sense.I nput  OutputTreadmills attached to cranes were used to lift heavy TOOLSfor supplying power for cranes, hoists, and lifts rOOl.SAbove this height, atower crane is often used .SB This TOO~elaborate courtship rituals cranes build a nest of vegetation A~aALare more closely related to cranes and rails .SB They range ANIMALlow trees .PP At least five crane species are in danger of !
AN~t~After using both the monolingual and bilingualclassifiers for a few months, we have convincedourselves that the performance is remarkably good.Nevertheless, we would really like to be able to make astronger statement, and therefore, we decided to try todevelop some more objective valuation measures.2.
Note that his edition of the Roger's Thesaurus i much moreextensive than the 1911 version, though somewhat more difficult oobtain in eleclxonie form.3.
The Literature on EvaluationAlthough there has been a fair amount of literature onsense-disambiguation, the literature does not offer muchguidance in how we might establish the success orfailure of a proposed solution such as the two describedabove.
Most papers tend to avoid quantitativeevaluations.
Lesk (1986), an extremely innovative andcommonly cited reference on the subject, provides ashort discussion of evaluation, but fails to offer any verysatisfying solutions that we might adopt to quantify theperformance of our two disambiguation algorithms.
3Perhaps the most common evaluation technique is toselect a small sample of words and compare the resultsof the machine with those of a human judge.
Thismethod has been used very effectively by Kelly andStone (1975), Black (1988), Hearst (1991), and manyothers.
Nevertheless, this technique is not without itsproblems, perhaps the worst of which is that the samplemay not be very representative of the generalvocabulary.
Zernik (1990, p. 27), for example, reports70% performance for the word interest, and thenacknowledges that this level of performance may notgeneralize very well to other words.
4Although we agree with Zernik's prediction that interestis not very representative of other words, we suspect thatinterest is actually more difficult than most other words,not less difficult.
Table 1 shows the performance ofYarowsky (1992) on twelve words which have beenpreviously discussed in the literature.
Note that interestis at the bottom of the list.The reader should exercise some caution in interpretingthe numbers in Table 1.
It is natural to try to use thesenumbers to predict performance on new words, but thestudy was not designed for that purpose.
The test wordswere selected from the literature in order to makecomparisons over systems.
If  the study had beenintended to support predictions on new words, then thestudy should have used a random sample of such words,rather than a sample of words from the literature.3.
"What is the current performance of this program?
Some verybrief experimentation with my program has yielded accuracies of50-70% on short samples of Pride and Prejudice and an AssociatedPress news story.
Considerably more work is needed both toimprove the program and to do more thorough evaluation... Thereis too much subjectivity in these measurements."
(Lesk, 1986, p. 6)4.
"For all 4 senses of INTEREST, both recall and precision are over70%...
However, not for all words are the obtained results thatpositive...
The fact is that almost any English word possessesmultiple senses.
(Zernik, 1990, p. 27)251Table 1: Comparison over SystemsWord Yarowsky (1992) Previous Systemsbow 91% < 67% (Clear, 1989)bass 99% 100% (Hearst, 1991)galley 99% 50-70% (Lesk, 1986)mole 99% N/A (Hirst, 1987)sentence 98% 90% (Gale et al)slug 97% N/A (Hirst, 1987)star 96% N/A (Hirst, 1987)duty 96% 96% (Gale et al)issue 94% < 70% (Zernik, 1990)taste 93% < 65% (Clear, 1989)cone 77% 50-70% (Lesk, 1986)interest 72% 72% (Black, 1988);70% (Zernik, 1990)AVERAGE 92% N/AIn addition to the sampling questions, one feelsuncomfortable about comparing results acrossexperiments, ince there are many potentially importantdifferences including different corpora, different words,different judges, differences in treatment of  precisionand recall, and differences in the use of  tools such asparsers and part of  speech taggers, etc.
In short, thereseem to be a number of  serious questions regarding thecommonly used technique of reporting percent correcton a few words chosen by hand.
Apparently, theliterature on evaluation of  word-sense disambiguationalgorithms fails to offer a clear role model that we mightfollow in order to quantify the performance of ourdisambiguation algorithms.4.
What is the State-of-the-Art, and How Good Does ItNeed To Be?Moreover, there doesn't seem to be a very clear sense ofwhat is possible.
Is interest a relatively easy word or isit a relatively hard word?
Zernik says it is relativelyeasy; we say it is relatively hard.
5 Should we expect henext word to be easier than interest or harder thaninterest?One might ask if 70% is good or bad.
In fact, bothBlack (1988) and Yarowsky (1992) report 72%performance on this very same word.
Although it isdangerous to compare such results since there are manypotentially important differences (e.g., corpora, judges,5.
As evidence that interest is relatively difficult, we note that both theOxford Advanced Learner's Dictionary (OALD) (Crowie et al,1989, p. 654) and COBUILD (Sinclair et al, 1987), for example,devote more than a full column to this word, indicating that it is anextremely complex word, at least by their standards.etc.
), it appears that Zernik's 70% figure is fairlyrepresentative of  the state of the art.
6Should we be happy with 70% performance?
In fact,70% really isn't very good.
Recall that Bar-Hillel (1960,p.
159) abandoned the machine translation field when hecouldn't see how a machine could possibly do a decentjob in translating text if it couldn't do better than this indisambiguating word senses.
Bar-Hillel's real objectionwas an empirical one.
Using his numbers, 7 it appearsthat programs, at the time, could disambiguate onlyabout 75% of the words in a sentence (e.g., 15 out of20).
If interest is a relatively easy word, as Zernik(1990) suggests, then it would seem that Bar-Hillel'sargument remains as true today as it was in 1960, and weought to follow his lead and find something moreproductive to do with our time.
On the other hand, if weare correct and interest is a relatively difficult word, thenit is possible that we have made some progress over thepast thirty years...5.
Upper and Lower Bounds5.1 Lower BoundsWe could be in a better position to address the questionof  the relative difficulty of interest if we could establisha rough estimate of the upper and lower bounds on thelevel of performance that can be expected.
We willestimate the lower bound by evaluating the performanceof a straw man system, which ignores context andsimply assigns the most likely sense in all cases.
Onemight hope that reasonable systems should generally7.In fact, Zemik's 70% figure is probably significantly inferior to the72% reported by Black and Yarowsky, because Zernik reportsprecision and recall separately, whereas the others report a singlefigure of merit which combines both Type I (false rejection) andType II (false acceptance) errors by reporting precision at 100%recall.
Gale et al show that error ates for 70% recall were half ofthose for 100% recall, on their test sample.
"Let me state rather dogmatically that here xists at this momentno method of reducing the polysemy of the, say, twenty words ofan average Russian sentence in a scientific article below aremainder of, I would estimate, at least five or six words withmultiple English renderings, which would not seriously endangerthe quality of the machine output.
Many tend to believe that byreducing the number of initially possible renderings of a twentyword Russian sentence from a few tens of thousands (which is theapproximate number resulting from the assumption that each of thetwenty Russian words has two renderings on the average, whileseven or eight of them have only one rendering) to some ighty(which would be the number of renderings on the assumption thatsixteen words are uniquely rendered and four have three renderingsapiece, forgetting now about all the other aspects such as change ofword order, etc.)
the main bulk of this kind of work has beenachieved, the remainder requiring only some slight additionaleffort."
(Bar-Hillel, 1960, p. 163)252outperform this baseline system, though not all suchsystems actually do.
In fact, Yarowsky (1992) fallsbelow the baseline for one of the twelve words (issue),although perhaps, we needn't be too concerned aboutthis one deviation.
8There are, of course, a number of problems with thisestimate of the baseline.
First, the baseline system is notoperational, at least as we have defined it.
Ideally, thebaseline system ought o try to estimate the most likelysense for each word in the vocabulary and then assignthat sense to each instance of the word in the test set.Unfortunately, since it isn't clear just how thisestimation should be accomplished, we decided to"cheat" and let the baseline system peek at the test setand "estimate" the most likely sense for each word asthe more frequent sense in the test set.
Consequently,the performance ofthe baseline cannot fall below chance(100/k% for a particular word with k senses).
9In addition, the baseline system assumes that Type I(false rejection) errors are just as bad as Type II (falseacceptance) rrors.
If one desires extremely high recalland is willing to sacrifice precision in order to obtain thislevel of recall, then it might be sensible to tune a systemto produce behavior which might appear to fall belowthe baseline.
We have run into such situations when wehave attempted to help lexicographers find extremelyunusual events.
In such a case, a lexicographer might bequite happy receiving a long list of potential candidates,only a small fraction of which are actually the case ofinterest.
One can come up with quite a number of otherscenarios where the baseline performance could besomewhat misleading, especially when there is anunusual trade-off between the cost of a Type I error andthe cost of a Type II error.Nevertheless, the proposed baseline does seem toprovide a usable rough estimate of the lower bound onperformance.
Table 2 shows the baseline performancefor each of the twelve words in Table 1.
Note thatperformance is generally above the baseline as we would8.
Many of the systems mentioned in Table 2 including Yarowsky(1992) do not currently take advantage of the prior probabilities ofthe senses, so they would be at a disadvantage r lative to thebaseline if one of the senses had a very high prior, as is the case forthe test word issue.9.
In addition, the baseline doesn't deal as well as it could withskewed distributions.
One could almost certainly improve themodel of the baseline by making use of a notion like entropy thatcould deal more effectively with skewed distributions.Nevertheless, we will stick with our simpler notion of the baselinefor expository convenience.hope.Table 2: The BaselineWord Baseline Yarowsky (1992)issue 96% 94%duty 87% 96%galley 83% 99%star 83% 96%taste 74% 93%bass 70% 99%slug 62% 97%sentence 62% 98%interest 60% 72%mole 59% 99%cone 51% 77%bow 48% 91%AVERAGE 70% 92%As mentioned previously, the test words in Tables 1 and2 were selected from the literature on polysemy, andtherefore, tend to focus on the more difficult cases.
Inanother experiment, we selected a random sample of 97words; 67 of them were unambiguous and therefore hada baseline performance of 100%) 0 The remaining thirtywords are listed along with the number of senses andbaseline performance: virus (2, 98%), device (3, 97%),direction (2, 96%), reader (2, 96%), core (3, 94%), hull(2, 94%), right (5, 94%), proposition (2, 89%), deposit(2, 88%), hour (4, 87%), path (2, 86%), view (3, 86%),pyramid (3, 82%), antenna (2, 81%), trough (3, 77%),tyranny (2, 75%), figure (6, 73%), institution (4, 71%),crown (4, 64%), drum (2, 63%), pipe (4, 60%),processing (2, 59%), coverage (2, 58%), execution (2,57%), rain (2, 57%), interior (4, 56%), campaign (2,51%), output (2, 51%), gin (3, 50%), drive (3, 49%).
Instudying these 97 words, we found that the averagebaseline performance is much higher than we might haveguessed (93% averaged over tokens, 92% averaged overtypes).
In particular, note that his baseline is well abovethe 75% figure that we associated with Bar-Hillel above.Of course, the large number of unambiguous wordscontributes greatly to the baseline.
If we exclude theunambiguous words, then the average baseline10.
The 67 unambiguous words were: acid, annexation, benzene, berry,capacity, cereal clock, coke, colon, commander, consort, contract,cruise, cultivation, delegate, designation, dialogue, disaster,equation, esophagus, fact, fear;, fertility, flesh, fox, gold, interface,interruption, intrigue, journey, knife, label landscape, laurel Ib,liberty, lily, locomotion, lynx, marine, memorial menstruation,miracle, monasticism, ountain, itrate, orthodoxy, pest, planning,possibility, pottery, projector, regiment, relaxation, reunification,shore, sodium, specialty, stretch, summer, testing, tungsten,universe, variant, vigor, wire, worship.253performance falls to 81% averaged over tokens and 75%averaged over types.5.2 Upper BoundsWe will attempt to estimate an upper bound onperformance by estimating the ability for human judgesto agree with one another (or themselves).
We will find,not surprisingly, that the estimate varies widelydepending on a number of factors, especially thedefinition of the task.
Jorgensen (1990) has collectedsome interesting data that may be relevant for estimatingthe agreement among judges.
As part of her dissertationunder George Miller at Princeton, she was interested inassessing "the extent of psychologically real polysemyin the mental exicon for nouns."
Her experiment wasdesigned to study one of the more commonly employedmethods in lexicography for writing dictionarydefinitions, namely the use of citation indexes.
She wasconcerned that lexicographers and computationallinguists have tended to depend too much on theintuitions of a single informant.
Not surprisingly, shefound considerable variation across judgements, just asshe had suspected.
This finding could have seriousimplications for evaluation.
How do we measureperformance if we can't depend on the judges?Jorgensen selected twelve high frequency nouns atrandom from the Brown Corpus, six were highlypolysemous (head, life, world, way, side, hand) and sixwere less so (fact, group, night, development, something,war).
Sentences containing each of these words weredrawn from the Brown Corpus and typed on filing cards.Nine subjects where then asked to cluster a packet ofthese filing cards by sense.
A week or two later, thesame nine subjects were asked to repeat he experiment,but this time they were given access to the dictionarydefinitions.Jorgensen reported performance in terms of the"Agreement-Disagreement" (A-D) ratio (Shipstone,1960) for each subject and each of the twelve test words.We have found it convenient to transform the A-D ratiointo a quantity which we call the percent agreement, thenumber of observed agreements over the total number ofpossible agreements.
The grand mean percentagreement over all subjects and words is only 68%.
Inother words, at least under these conditions, there isconsiderable variation across judgements, perhaps somuch so that it would be hard to show that a proposedsystem was outperforming the baseline system (75%,averaged over ambiguous types).
Moreover, if weaccept Bar-Hillel's argument hat 75% is not-good-enough, then it would be hard to show that a system wasdoing well-enough.2546.
A Discrimination ExperimentFor evaluation purposes, it is important to find a task thatis somewhat easier for the judges.
If the task is too hard(as Jorgensen's classification task may he), then therewill be almost no room between the limits of themeasurement and the baseline.
In other words, therewon't be enough dynamic range to measure differencesbetween better systems and worse systems.
In contrast,if we focus on easier tasks, then we might have enoughdynamic range to show some interesting differences.Therefore, unlike Jorgensen who was interested inhighlighting differences among judgments, we are muchmore interested in highlighting agreements.
Fortunately,we have found in (Gale et al, 1992) that the agreementrate can be very high (96.8%), which is well above thebaseline, under very different experimental conditions.Of course, it is a fairly major step to redefine theproblem from a classification task to a discriminationone, as we are proposing.
One might have preferred notto do so, but we simply don't know how one couldestablish enough dynamic range in that case to show anyinteresting differences.
It has been our experience that itis very hard to design an experiment of any kind whichwill produce the desired agreement among judges.
Weare very happy with the 96.8% agreement that we wereable to show, even if it is limited to a much easier taskthan the one that Jorgensen was interested in.We originally designed the experiment in Gale et al(1992) to test the hypothesis that multiple uses of apolysemous word tend to have the same sense within acommon discourse.
A simple (but non-blind) pilotexperiment provided some suggestive evidenceconfirming the hypothesis.
A random sample of 108nouns (which included the 97 words previouslymentioned) was extracted for further study.
A panel ofthree judges (the three authors of this paper) were given100 sets of concordance lines containing one of the testwords selected from a single article in Grolier's.
Thejudges were asked to indicate if the set of concordancelines used the same sense or not.
Only 6 of 300 article-judgements were judged to contain multiple senses ofone of the test words.
All three judges were convincedafter grading 100 articles that there was considerablevalidity to the hypothesis.With this promising preliminary verification, thefollowing blind test was devised.
Five subjects (thethree authors and two of their colleagues) were given aquestionnaire starting with a set of definitions electedfrom OALD (Crowie et al, 1989) and followed by anumber of pairs of concordance lines, randomly selectedfrom Grolier's Encyclopedia (1991).
The subjects wereasked to decide for each pair, whether the twoconcordance lines corresponded to the same sense or not.antenna1.
jointed organ found in pairs on the heads ofinsects and crustaceans, used for feeling, etc.
---> theillus at insect.2.
radio or TV aerial.lack eyes, legs, wings, antennae,  and distinct mouthparts andThe Brachycera have short antennae and include the more evolvedsilk moths passes over the antennae .SB Only males that detectrelatively simple form of antenna is the dipole, or doubletThe questionnaire contained a total of 82 pairs ofconcordance lines for 9 polysemous words: antenna,campaign, deposit, drum, hull, interior, knife, landscape,and marine.
The results of the experiment are shownbelow in Table 3.
With the exception of judge 2, all ofthe judges agreed with the majority opinion in all butone or two of the 82 cases.
The agreement rate was96.8%, averaged over all judges, or 99.1%, averagedover the four best judges.
In either case, the agreementrate is well above the previously described ceiling.Table 3Judge n %1 82 100.0%2 72 87.8%3 81 98.7%4 82 100.0%5 80 97.6%Average 96.8%Average (without Judge 2) 99.1%Incidentally, the experiment did, in fact, confirm thehypothesis that multiple uses of a polysemous word willgenerally take on the same sense within a discourse.
Ofthe 82 judgments, 54 were selected from the samediscourse and were judged to have the same sense by themajority in 96.9% of the cases.
(The remaining 28 ofthe 82 judgments were used as a control to force thejudges to say that some pairs were different.
)Note that the tendency for multiple uses of a polysemousword to have the same sense is extremely strong; 96.9%is much greater than the baseline, and indeed, it isconsiderably above the level of performance that mightbe expected from state-of-the-art word-sensedisambiguation systems.
Since it is so reliable and soeasy to compute, it might be used as a quick-and-dirtymeasure for testing such systems.
Unfortunately, wealso need a complementary measure that would penalizea system like the baseline system that simply assignedall instances of a polysemous word to the same sense.255At present, we have yet to identify a quick-and-dirtymeasure that accomplishes this control, andconsequently, we are forced to continue to depend on therelatively expensive panel of judges.
But, at least, wehave been able to establish that it is possible to design adiscrimination experiment such that the panel of judgescan agree with themselves often enough to be useful.
Inaddition, we have established that  the discourseconstraint on polysemy is extremely strong, muchstronger than our ability to tag word-sensesautomatically.
Consequently, it ought to be possible touse this constraint in our next word-sense taggingalgorithm to produce ven better performance.7.
ConclusionsWe began this discussion with a review of our recentwork on word-sense disambiguation, which extends theapproach of using massive lexicographic resources (e.g.,parallel corpora, dictionaries, thesauruses andencyclopedia) in order to attack the knowledge-acquisition bottleneck that Bar-Hillel identified overthirty years ago.
After using both the monolingual andbilingual classifiers for a few months, we haveconvinced ourselves that the performance is remarkablygood.
Nevertheless, we would really like to be able tomake a stronger statement, and therefore, we decided totry to develop some more objective valuation measures.A survey of the literature on evaluation failed to identifyan attractive role model.
In addition, we found itparticularly difficult to obtain a clear estimate of thestate-of-the-art.In order to address this state of affairs, we decided to tryto establish upper and lower bounds on the level ofperformance that we could expect to obtain.
Weestimated the lower bound by positing a simple baselinesystem which ignored context and simply assigned themost likely sense in all cases.
Hopefully, mostreasonable systems would outperform this system.
Theupper bound was approximated by trying to estimate thelimit of our ability to measure performance.
Weassumed that this limit was largely dominated by theability for the human judges to agree with one another.The estimate depends very much, not surprisingly, onthe particular experimental design.
Jorgensen, who wasinterested in highlighting differences among informants,found a very low estimate (68%), well below thebaseline (75%), and also well below the level that Bar-Hillel asserted as not-good-enough.
In our own work,we have attempted to highlight agreements, sothat therewould more dynamic range between the baseline and thelimit of our ability to measure performance.
In so doing,we were able to obtain a much more usable estimate of(96.8%) by redefining the task from a classification tasktO a discrimination task.
In addition, we also made useof  the constraint that multiple instances of a polysemousword in the same discourse have a very strong tendencyto take on the same sense.
This constraint will probablyprove useful for improving the performance of futureword-sense disambiguation algorithms.Similar attempts to establish upper and lower bounds onperformance have been made in other areas ofcomputational linguistics, specifically part of speechtagging.
For that application, it is generally acceptedthat the baseline part-of-speech tagging performance isabout 90% (as estimated by a similar baseline systemthat ignores context and simply assigns the most likelypart of speech to all instances of  a word) and that theupper bound (imposed by the limit for judges to agreewith one another) is about 95%.
Incidentally, most partof speech algorithms are currently performing at or nearthe limit of our abil ity to measure performance,indicating that there may be room for refining theexperimental conditions along similar lines to what wehave done here, in order to improve the dynamic rangeof  the evaluation.ReferencesBar-Hillel (1960), "Automatic Translation of Languages," inAdvances in Computers, Donald Booth and R. E. Meagher, eds.,Academic, NY.Black, Ezra (1988), "An Experiment inComputational Discriminationof English Word Senses," IBM Journal of Research and Development,v 32, pp 185-194.Brown, Peter, Stephen Della Pietra, Vincent Delia Pietra, and RobertMercer (1991), "Word Sense Disambiguation using StatisticalMethods," ACL, pp.
264-270.Chapman, Robert (1977).
Roger's International Thesaurus (FourthEdition), Harper and Row, NY.Choueka, Yaacov, and Serge Lusignan (1985), "Disambiguation byShort Contexts," Computers and the Humanities, v 19. pp.
147-158.Church, Kenneth (1988), "A Stochastic Parts Program an Noun PhraseParser for Unrestricted Text," Applied ACL Conference, Austin, Texas.Clear, Jeremy (1989).
"An Experiment in Automatic Word SenseIdentification," Internal Document, Oxford University Press, Oxford.Crowie, Anthony et al (eds.)
(1989), "Oxford Advanced Learner'sDictionary," Fourth Edition, Oxford University Press.Dagan, Ido, Alon Itai, and Ulrike Schwall (1991), "Two Languages aremore Informative than One," ACL, pp.
130-137.Gale, William, Kenneth Church, and David Yarowsky (to appear) "AMethod for Disambiguating Word Senses in a Large Corpus,"Computers and Humanities.Gale, William, Kenneth Church, and David Yarowsky (1992) "OneSense Per Discourse," Darpa Speech and Natural Language Workshop.Gove, Philip et al (eds.)
(1975) "Webster's Seventh New CollegiateDictionary," G. & C. Merriam Company, Springfield, MA.Grolier's Inc. ( 1991 ) New Grolier's Electronic Encyclopedia.Hanks, Patrick (ed.)
(1979), Collins English Dictionary, Collins,London and Glasgow.256Hearst, Marti (1991), "Noun Homograph Disambiguation Using LocalContext in Large Text Corpora," Using Corpora, University ofWaterloo, Waterloo, Ontario.Hirst, Graerae.
(1987), Semantic Interpretation and the Resolution ofAmbiguity, Cambridge University Press, Cambridge.Jorgensen, Julia (1990) "The Psychological Reality of Word Senses,"Journal of Psychalinguistic Research, v. 19, pp 167-190.Kaplan, Abraham (1950), "An Experimental Study of Ambiguity inContext," cited in Mechanical Translation, v. I, nos.
I-3.Kelly, Edward, and Phillip Stone (1975), Computer Recognition ofEnglish Word Senses, North-Holland, Amsterdam.Lesk, Michael (1986), "Automatic Sense Disambiguation: How to tella Pine Cone from an Ice Cream Cone," Proceeding of the 1986SIGDOC Conference, ACM,  NY.Masterson, Margaret (1967), "Mechanical Pidgin Translation," inMachine Translation, Donald Booth, ed., Wiley, 1967.Mosteller, Fredrick, and David Wallace (1964) Inference and DisputedAuthorship: The Federalist, Addison-Wesley, Reading, Massachusetts.Procter, P., R. Ilson, J. Ayto, et al (1978), Longman Dictionary ofContemporary English, Longman, Harlow and London.Salton, G. (1989) Automatic Text Processing, Addison-Wesley.Shipstone, E. (1960) "Some Variables Affecting Pattern Conception,"Psychological Monographs, General and Applied, v. 74, pp.
1-4 I.Sinclair, I., Hanks, P., Fox, G., Moon, R., Stock, P. et al (eds.)
(1987)Collins Cobuild English Language Dictionary, Collins, London andGlasgow.Smadja, F. and K. McKoown (1990), "Automatically Extracting andRepresenting Collocations for Language Generation," ACL, pp.
252-259.Small, S. and C. Rieger (1982), "Parsing and Comprehending withWord Experts (A Theory and its Realization)," in Strategies forNatural Language Processing, W. Lehnert and M. Ringle, eds.,Lawrence Erlbanm Associates, Hillsdale, NJ.van Rijsbergen, C. (1979) Information Retrieval, Second Editional,Butterworths, London.Veronis, Jean and Nancy Ide (1990), "Word Sense Disambiguationwith Very Large Neural Networks Extracted from Machine ReadableDictionaries," inProceedings COLING-90, pp 389-394.Walker, Donald (1987), "Knowledge Resource Tools for AccessingLarge Text Files," in Machine Translation: Theoretical andMethodological Issues, Sergei Nirenberg, ed., Cambridge UniversityPress, Cambridge, England.Weiss, Stephen (1973), "Learning to Disambiguate," InformationStorage and Retrieval, v. 9, pp 33-41.Yarowsky, David (1992), "Word-Sense Disambiguation UsingStatistical Models of Roget's Categories Trained on Large-Corpora,"Proceedings COLING-92.Yngve, Victor (1955), "Syntax and the Problem of MultipleMeaning," in Machine Translation of languages, William Locke andDonald Booth, eds., Wiley, NY.Zernik, Uri (1990) "Tagging Word Senses in Corpus: The Needle inthe Haystack Revisited," in Text-Based Intelligent Systems: CurrentResearch in Text Analysis, Information Extraction, and Retrieval, P.S.Jacobs, ed., GE Research & Development Center, Schenectady, NY.Zernik, Uri (1991) "Trainl vs. Train2: Tagging Word Senses inCorpus," in Zemik (ed.)
Lexical Acquisition: Exploiting On-LineResources to Build a Lexicon, Lawrence Erlbaum, Hillsdale, NJ.
