Proceedings of the NAACL HLT 2010: Tutorial Abstracts, pages 7?8,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsRecent Advances in Dependency ParsingQin Iris Wang, AT&T InteractiveYue Zhang, OxfordData-driven (statistical) approaches have been playing anincreasingly prominent role in parsing since the 1990s.
In recentyears, there has been a growing interest in dependency-based asopposed to constituency-based approaches to syntactic parsing, withapplication to a wide range of research areas and different languages.Graph-based and transition-based methods are the two dominantdata-driven approaches to dependency parsing.
In a graph-based model,it defines a space of candidate dependency trees for a given sentence.Each candidate tree is scored via a local or global scoring function.The parser (usually uses dynamic programming) outputs thehighest-scored tree.
In contrast, in a transition-based model, itdefines a transition system for mapping a sentence to its dependencytree.
It induces a model for predicting the next state transition,given the transition history.
Given the induced model, the outputparse tree is built deterministically upon the construction of theoptimal transition sequence.Both Graph-based and transition-based approaches have been used toachieve state-of-the-art dependency parsing results for a wide rangeof languages.
Some researchers have used the combination of the twomodels and it shows the performance of the combined model issignificantly better than the individual models.
Another recent trendis to apply online training to shift-reduce parsing in thetransition-based models.
In this tutorial, we first introduce the twomain-stream data-driven dependency parsing models--- graph-based andtransition-based models.
After comparing the differences between them,we show how these two models can be combined in various ways toachieve better results.OutlinePart A: Introduction to Dependency ParsingPart B: Graph-based Dependency Parsing Models- Learning Algorithms (Local Learning vs.
Global Learning)- Parsing Algorithms (Dynamic Programming)- Features (Static Features vs.
Dynamic Features)7Part C: Transition-based Dependency Parsing Models- Learning Algorithms (Local Learning vs. online Learning)- Parsing Algorithms (Shift-reduce Parsing)- FeaturesPart D: The Combined Models- The stacking Method- The ensemble Method- Single-model CombinationPart E: Other Recent Trends in Dependency Parsing- Integer Linear Programming- Fast Non-Projective ParsingPresentersQin Iris WangEmail: qiniriswang@gmail.comQin Iris Wang is currently a Research Scientist at AT&T Interactive(San Francisco).
Qin obtained her PhD in 2008 from the University ofAlberta under Dekang Lin and Dale Schuurmans.
Qin's research interestsinclude NLP (in particular dependency parsing), machine learning,information retrieval, text mining and large scale data processing.Qin's PhD studies was focused on Learning Structured Classifiers forStatistical Dependency Parsing.
Before joined AT&T, she was a researchscientist at Yahoo Labs.
Qin was a teaching assistant for two yearsduring her PhD studies.
In 2009, Qin organized a workshop on "Semi-supervised Learning for Natural Language Processing" atNAACL-HLT.Yue ZhangEmail: yue.zhang@comlab.ox.ac.ukYue Zhang just defended his PhD thesis at the University of Oxford.Yue's research interests include natural language processing (wordsegmentation, parsing, machine translation), machine learning, etc.More specifically, his research area is the syntactic analysis of theChinese language, using discriminative machine-learning approaches.
Hehas worked on word segmentation, joint word segmentation andPOS-tagging, phrase-structure parsing and dependency parsing.
Yueworked on Chinese-English machine-translation during MSc studies inOxford, and parallel computing during undergrad studies in TsinghuaUniversity.8
