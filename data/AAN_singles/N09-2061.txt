Proceedings of NAACL HLT 2009: Short Papers, pages 241?244,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSentence Boundary Detection and the Problem with the U.S.Dan GillickComputer Science DivisionUniversity of California, Berkeleydgillick@cs.berkeley.eduAbstractSentence Boundary Detection is widely usedbut often with outdated tools.
We discuss whatmakes it difficult, which features are relevant,and present a fully statistical system, now pub-licly available, that gives the best known er-ror rate on a standard news corpus: Of some27,000 examples, our system makes 67 errors,23 involving the word ?U.S.
?1 IntroductionMany natural language processing tasks begin byidentifying sentences, but due to the semantic am-biguity of the period, the sentence boundary detec-tion (SBD) problem is non-trivial.
While reportederror rates are low, significant improvement is pos-sible and potentially valuable.
For example, sincea single error can ruin an automatically generatedsummary, reducing the error rate from 1% to 0.25%reduces the rate of damaged 10-sentence summariesfrom 1 in 10 to 1 in 40.
Better SBD may improvelanguage models and sentence alignment as well.SBD has been addressed only a few times in theliterature, and each result points to the importance ofdeveloping lists of common abbreviations and sen-tence starters.
Further, most practical implementa-tions are not readily available (with one notable ex-ception).
Here, we present a fully statistical systemthat we argue benefits from avoiding manually con-structed or tuned lists.
We provide a detailed anal-ysis of features, training variations, and errors, allof which are under-explicated in the literature, anddiscuss the possibility of a more structured classifi-cation approach.
Our implementation gives the bestperformance, to our knowledge, reported on a stan-dard Wall Street Journal task; it is open-source andavailable to the public.2 Previous WorkWe briefly outline the most important existing meth-ods and cite error rates on a standard English dataset, sections 03-06 of the Wall Street Journal (WSJ)corpus (Marcus et al, 1993), containing nearly27,000 examples.
Error rates are computed as(number incorrect/total ambiguous periods).
Am-biguous periods are assumed to be those followedby white space or punctuation.
Guessing the major-ity class gives a 26% baseline error rate.A variety of systems use lists of hand-crafted reg-ular expressions and abbreviations, notably Alem-bic (Aberdeen et al, 1995), which gives a 0.9% er-ror rate.
Such systems are highly specialized to lan-guage and genre.The Satz system (Palmer and Hearst, 1997)achieves a 1.0% error rate using part-of-speech(POS) features as input to a neural net classifier (adecision tree gives similar results), trained on held-out WSJ data.
Features were generated using a5000-word lexicon and a list of 206 abbreviations.Another statistical system, mxTerminator (Reynarand Ratnaparkhi, 1997) employs simpler lexical fea-tures of the words to the left and right of the can-didate period.
Using a maximum entropy classifiertrained on nearly 1 million words of additional WSJdata, they report a 1.2% error rate with an automati-cally generated abbreviation list and special corpus-specific abbreviation features.There are two notable unsupervised systems.Punkt (Kiss and Strunk, 2006) uses a set of log-likelihood-based heuristics to infer abbreviationsand common sentence starters from a large textcorpus.
Deriving these lists from the WSJ testdata gives an error rate of 1.65%.
Punkt is eas-ily adaptable but requires a large (unlabeled) in-domain corpus for assembling statistics.
An imple-mentation is bundled with NLTK (Loper and Bird,2002).
(Mikheev, 2002) describes a ?document-241centered?
approach to SBD, using a set of heuris-tics to guess which words correspond to abbrevia-tions and names.
Adding carefully tuned lists froman extra news corpus gives an error rate of 0.45%,though this increases to 1.41% without the abbrevi-ation list.
Combining with a supervised POS-basedsystem gives the best reported error rate on this task:0.31%.Our system is closest in spirit to mxTerminator,and we use the same training and test data in ourexperiments to aid comparison.3 Our ApproachEach example takes the general form ?L.
R?, whereL is the context on the left side of the period inquestion, and R is the context on the right (we useonly one word token of context on each side).
Weare interested in the probability of the binary sen-tence boundary class s, conditional on its context:P (s|?L.
R?).
We take a supervised learning ap-proach, extracting features from ?L.
R?.Table 1 lists our features and their performance,using a Support Vector Machine (SVM) with a lin-ear kernel1.
Feature 1 by itself, the token endingwith the candidate period, gives surprisingly goodperformance, and the combination of 1 and 2 out-performs nearly all documented systems.
While nopublished result uses an SVM, we note that a simpleNaive Bayes classifier gives an error rate of 1.05%(also considerably better than mxTerminator), sug-gesting that the choice of classifier alone does notexplain the performance gap.There are a few possible explanations.
First,proper tokenization is key.
While there is not roomto catalog our tokenizer rules, we note that both un-tokenized text and mismatched train-test tokeniza-tion can increase the error rate by a factor of 2.Second, poor feature choices can hurt classifica-tion.
In particular, adding a feature that matches alist of abbreviations can increase the error rate; us-ing the list (?Mr.
?, ?Co.?)
increases the number oferrors by up to 25% in our experiments.
This is be-cause some abbreviations end sentences often, andothers do not.
In the test data, 0 of 1866 instancesof ?Mr.?
end a sentence, compared to 24 of 86 in-stances of ?Calif.?
(see Table 2).
While there may1We use SVM Light, with c = 1 (Joachims, 1999).
Non-linear kernels did not improve performance in our experiments.# Feature Description Error1 L = wi 1.88%2 R = wj 9.36%3 len(L) = l 9.12%4 is cap(R) 12.56%5 int(log(count(L; no period))) = ci 12.14%6 int(log(count(R; is lower)) = cj 18.79%7 (L = wi, R = wj) 10.01%8 (L = wi, is cap(R)) 7.54%1+2 0.77%1+2+3+4 0.36%1+2+3+4+5+6 0.32%1+2+3+4+5+6+7+8 0.25%Table 1: All features are binary.
SVM classification re-sults shown; Naive Bayes gives 0.35% error rate with allfeatures.be meaningful abbreviation subclasses, a feature in-dicating mere presence is too coarse.Abbr.
Ends Sentence Total RatioInc.
109 683 0.16Co.
80 566 0.14Corp.
67 699 0.10U.S.
45 800 0.06Calif.
24 86 0.28Ltd.
23 112 0.21Table 2: The abbreviations appearing most often as sen-tence boundaries.
These top 6 account for 80% ofsentence-ending abbreviations in the test set, though only5% of all abbreviations.Adding features 3 and 4 better than cuts the re-maining errors in half.
These can be seen as a kindof smoothing for sparser token features 1 and 2.
Fea-ture 3, the length of the left token, is a reasonableproxy for the abbreviation class (mean abbreviationlength is 2.6, compared to 6.1 for non-abbreviationsentence enders).
The capitalization of the right to-ken, feature 4, is a proxy for a sentence starter.
Ev-ery new sentence that starts with a word (as opposedto a number or punctuation) is capitalized, but 70%of words following abbreviations are also, so thisfeature is mostly valuable in combination.While we train on nearly 1 million words, most ofthese are ignored because our features are extractedonly near possible sentence boundaries.
Considerthe fragment ?...
the U.S.
Apparently some ...?,242which our system fails to split after ?U.S.?
The word?Apparently?
starts only 8 sentences in the train-ing data, but since it usually appears lowercased (89times in training), its capitalization here is meaning-ful.
Feature 6 encodes this idea, indicating the logcount of lowercased appearances of the word rightof the candidate period.
Similarly, feature 5 givesthe log count of occurrences of the token left of thecandidate appearing without a final period.Another way to incorporate all of the trainingdata is to build a model of P (s|?L R?
), as is of-ten used in sentence segmentation for speech recog-nition.
Without a period in the conditional, manymore negative examples are included.
The resultingSVM model is very good at placing periods giveninput text without them (0.31% error rate), but whenlimiting the input to examples with ambiguous peri-ods, the error rate is not competitive with our origi-nal model (1.45%).Features 7 and 8 are added to model the nuancesof abbreviations at sentence boundaries, helping toreduce errors involving the examples in Table 2.4 Two Classes or Three?SBD has always been treated as a binary classifica-tion problem, but there are really three classes: sen-tence boundary only (S); abbreviation only (A); ab-breviation at sentence boundary (A + S).
The labelspace of the test data, which has all periods anno-tated, is shown in Figure 1.Sentence Boundaries (S)Abbreviations (A)(A+S)(A)(A+S)All DataErrorsFigure 1: The overlapping label space of the test data:sentence boundaries 74%; abbreviations 26%; intersec-tion 2%.
The distribution of errors given by our classifieris shown as well (not to scale with all data).Relative to the size of the classes, A + S exam-ples are responsible for a disproportionate numberof errors, pointing towards the problem with a bi-nary classifier: In the absence of A + S examples,the left context L and the right context R both helpdistinguish S from A.
But A + S cases have L re-sembling the A class and R resembling the S class.One possibility is to add a third class, but this doesnot improve results, probably because we have sofew A + S examples.
We also tried taking a morestructured approach, depicted in Figure 2, but thistoo fails to improve performance, mostly becausethe first step, identifying abbreviations without theright context, is too hard.
Certainly, the A+S casesare more difficult to identify, but perhaps some bet-ter structured approach could reduce the error ratefurther.?
?P(A?|??L.?)?>?0.5P(S?|??R?
)?>?0.5SA+SAnoyesnoyesFigure 2: A structured classification approach.
The leftcontext is used to separate S examples first, then thoseremaining are classified as either A or A + S using theright context.5 Training DataOne common objection to supervised SBD systemsis an observation in (Reynar and Ratnaparkhi, 1997),that training data and test data must be a good match,limiting the applicability of a model trained from aspecific genre.
Table 3 shows respectable error ratesfor two quite different test sets: The Brown corpusincludes 500 documents, distributed across 15 gen-res roughly representative of all published English;The Complete Works of Edgar Allen Poe includesan introduction, prose, and poetry.A second issue is a lack of labeled data, espe-cially in languages besides English.
Table 4 showsthat results can be quite good without extensive la-beled resources, and they are likely to continue toimprove if additional resources were available.
Atthe least, (Kiss and Strunk, 2006) have labeled over243Corpus Examples in S SVM Err NB ErrWSJ 26977 74% 0.25% 0.35%Brown 53688 91% 0.36% 0.45%Poe 11249 95% 0.52% 0.44%Table 3: SVM and Naive Bayes classification error rateson different corpora using a model trained from a disjointWSJ data set.10000 sentences in each of 11 languages, though wehave not experimented with this data.Corpus 5 50 500 5000 42317WSJ 7.26% 3.57% 1.36% 0.52% 0.25%Brown 5.65% 4.46% 1.65% 0.74% 0.36%Poe 4.01% 2.68% 2.22% 0.98% 0.52%Table 4: SVM error rates on the test corpora, using mod-els built from different numbers of training sentences.We also tried to improve results using a standardbootstrapping method.
Our WSJ-trained model wasused to annotate 100 million words of New YorkTimes data from the AQUAINT corpus, and we in-cluded high-confidence examples in a new trainingset.
This did not degrade test error, nor did it im-prove it.6 ErrorsOur system makes 67 errors out of 26977 exampleson the WSJ test set; a representative few are shownin Table 5.
34% of the errors involve the word ?U.S.
?which distinguishes itself as the most difficult of to-kens to classify: Not only does it appear frequentlyas a sentence boundary, but even when it does not,the next word is often capitalized (?U.S.
Govern-ment?
; ?U.S.
Commission?
), further confusing theclassifier.
In fact, abbreviations for places, includ-ing ?U.K.
?, ?N.Y.
?, ?Pa.?
constitute 46% of all er-rors for the same reason.
Most of the remaining er-rors involve abbreviations like those in Table 2, andall are quite difficult for a human to resolve withoutmore context.
Designing features to exploit addi-tional context might help, but could require parsing.7 ConclusionWe have described a simple yet powerful method forSBD.
While we have not tested models in languagesother than English, we are providing the code andour models, complete with tokenization, availableContext Label P (S)... the U.S. Amoco already ... A + S 0.45... the U.K. Panel on ... A 0.57... the U.S. Prudential Insurance ... A + S 0.44... Telephone Corp. President Haruo ... A 0.73... Wright Jr. Room ... A 0.67... 6 p.m. Travelers who ... A + S 0.44Table 5: Sample errors with the probability of being inthe S class assigned by the SVM.at http://code.google.com/p/splitta.
Future work in-cludes further experiments with structured classifi-cation to treat the three classes appropriately.AcknowledgmentsThanks to Benoit Favre, Dilek Hakkani-Tu?r, KofiBoakye, Marcel Paret, James Jacobus, and LarryGillick for helpful discussions.ReferencesJ.
Aberdeen, J. Burger, D. Day, L. Hirschman, P. Robin-son, and M. Vilain.
1995.
MITRE: description of theAlembic system used for MUC-6.
In Proceedings ofthe 6th conference on Message understanding, pages141?155.
Association for Computational LinguisticsMorristown, NJ, USA.T.
Joachims.
1999.
Making large-scale support vectormachine learning practical, Advances in kernel meth-ods: support vector learning.T.
Kiss and J. Strunk.
2006.
Unsupervised MultilingualSentence Boundary Detection.
Computational Lin-guistics, 32(4):485?525.E.
Loper and S. Bird.
2002.
NLTK: The Natural Lan-guage Toolkit.
In Proceedings of the ACL Workshopon Effective Tools and Methodologies for TeachingNatural Language Processing and Computational Lin-guistics, pages 62?69.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: the penn treebank.
Computational Linguistics,19(2):313?330.A.
Mikheev.
2002.
Periods, Capitalized Words, etc.Computational Linguistics, 28(3):289?318.D.D.
Palmer and M.A.
Hearst.
1997.
Adaptive Multilin-gual Sentence Boundary Disambiguation.
Computa-tional Linguistics, 23(2):241?267.J.C.
Reynar and A. Ratnaparkhi.
1997.
A maximum en-tropy approach to identifying sentence boundaries.
InProceedings of the Fifth Conference on Applied Natu-ral Language Processing, pages 16?19.244
