Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 21?24,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsMSR SPLAT, a language analysis toolkitChris Quirk, Pallavi Choudhury, JianfengGao, Hisami Suzuki, Kristina Toutanova,Michael Gamon, Wen-tau Yih, LucyVanderwendeColin CherryMicrosoft Research National Research Council CanadaRedmond, WA 98052 USA 1200 Montreal RoadOttawa, Ontario K1A 0R6{chrisq, pallavic, jfgao,hisamis, kristout,mgamon,scottyih,lucyv@microsoft.com}colin.cherry@nrccnrc.gc.caAbstractWe describe MSR SPLAT, a toolkit for lan-guage analysis that allows easy access to thelinguistic analysis tools produced by the NLPgroup at Microsoft Research.
The tools in-clude both traditional linguistic analysis toolssuch as part-of-speech taggers, constituencyand dependency parsers, and more recent de-velopments such as sentiment detection andlinguistically valid morphology.
As we ex-pand the tools we develop for our own re-search, the set of tools available in MSRSPLAT will be extended.
The toolkit is acces-sible as a web service, which can be usedfrom a broad set of programming languages.1 IntroductionThe availability of annotated data sets that havebecome community standards, such as the PennTreeBank (Marcus et al, 1993) and PropBank(Palmer et al, 2005), has enabled many researchinstitutions to build core natural language pro-cessing components, including part-of-speech tag-gers, chunkers, and parsers.
There remain manydifferences in how these components are built, re-sulting in slight but noticeable variation in thecomponent output.
In experimental settings, it hasproved sometimes difficult to distinguish betweenimprovements contributed by a specific componentfeature from improvements due to using a differ-ently-trained linguistic component, such as tokeni-zation.
The community recognizes this difficulty,and shared task organizers are now providing ac-companying parses and other analyses of theshared task data.
For instance, the BioNLP sharedtask organizers have provided output from a num-ber of parsers1, alleviating the need for participat-ing systems to download and run unfamiliar tools.On the other hand, many community membersprovide downloads of NLP tools2 to increase ac-cessibility and replicability of core components.Our toolkit is offered in this same spirit.
Wehave created well-tested, efficient linguistic toolsin the course of our research, using commonlyavailable resources such as the PTB and PropBank.We also have created some tools that are lesscommonly available in the community, for exam-ple linguistically valid base forms and semanticrole analyzers.
These components are on par withother state of the art systems.We hope that sharing these tools will enablesome researchers to carry out their projects withouthaving to re-create or download commonly usedNLP components, or potentially allow researchersto compare our results with those of their owntools.
The further advantage of designing MSRSPLAT as a web service is that we can share newcomponents on an on-going basis.2 Parsing Functionality2.1 Constituency Parsing1 See www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask  forthe description of other resources made available in addition tothe shared task data.2 See, for example, http://nlp.stanford.edu/software;http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp;http://incubator.apache.org/opennlp21The syntactic parser in MSR SPLAT attempts toreconstruct a parse tree according the Penn Tree-Bank specification (Marcus et al, 1993).
This rep-resentation captures the notion of labeled syntacticconstituents using a parenthesized representation.For instance, the sentence ?Colorless green ideassleep furiously.?
could be assigned the followingparse tree, written in the form of an S expression:(TOP (S(NP (JJ Colorless) (JJ green) (NNS ideas))(VP (VB sleep) (ADVP (RB furiously)))(.
.
)))For instance, this parse tree indicates that ?Color-less green ideas?
is a noun phrase (NP), and ?sleepfuriously?
is a verb phrase (VP).Using the Wall Street Journal portion of thePenn TreeBank, we estimate a coarse grammarover the given grammar symbols.
Next, we per-form a series of refinements to automatically learnfine-grained categories that better capture the im-plicit correlations in the tree using the split-mergemethod of Petrov et al (2006).
Each input symbolis split into two new symbols, both with a newunique symbol label, and the grammar is updatedto include a copy of each original rule for eachsuch refinement, with a small amount of randomnoise added to the probability of each productionto break ties.
We estimate new grammar parame-ters using an accelerated form of the EM algorithm(Salakhutdinov and Roweis, 2003).
Then the low-est 50% of the split symbols (according to theirestimated contribution to the likelihood of the data)are merged back into their original form and theparameters are again re-estimated using AEM.
Wefound six split-merge iterations produced optimalaccuracy on the standard development set.The best tree for a given input is selected ac-cording to the max-rule approach (cf.
Petrov et al2006).
Coarse-to-fine parsing with pruning at eachlevel helps increase speed; pruning thresholds arepicked for each level to have minimal impact ondevelopment set accuracy.
However, the initialcoarse pass still has runtime cubic in the length ofthe sentence.
Thus, we limit the search space of thecoarse parse by closing selected chart cells beforethe parse begins (Roark and Hollingshead, 2008).We train a classifier to determine if constituentsmay start or end at each position in the sentence.For instance, constituents seldom end at the word?the?
or begin at a comma.
Closing a number ofchart cells can substantially improve runtime withminimal impact on accuracy.2.2 Dependency ParsingThe dependency parses produced by MSR SPLATare unlabeled, directed arcs indicating the syntacticgovernor of each word.These dependency trees are computed from theoutput of the constituency parser.
First, the head ofeach non-terminal is computed according to a setof rules (Collins, 1999).
Then, the tree is flattenedinto maximal projections of heads.
Finally, we in-troduce an arc from a parent word p to a childword c if the non-terminal headed by p is a parentof the non-terminal headed by c.2.3 Semantic Role LabelingThe Semantic Role Labeling component of MSRSPLAT labels the semantic roles of verbs accord-ing to the PropBank specification (Palmer et al,2005).
The semantic roles represent a level ofbroad-coverage shallow semantic analysis whichgoes beyond syntax, but does not handle phenome-na like co-reference and quantification.For example, in the two sentences ?John brokethe window?
and ?The window broke?, the phrasethe window will be marked with a THEME label.Note that the syntactic role of the phrase in the twosentences is different but the semantic role is thesame.
The actual labeling scheme makes use ofnumbered argument labels, like ARG0, ARG1, ?,ARG5 for core arguments, and labels like ARGM-TMP,ARGM-LOC, etc.
for adjunct-like argu-ments.
The meaning of the numbered arguments isverb-specific, with ARG0 typically representing anagent-like role, and ARG1 a patient-like role.This implementation of an SRL system followsthe approach described in (Xue and Palmer, 04),and includes two log-linear models for argumentidentification and classification.
A single syntaxtree generated by the MSR SPLAT split-mergeparser is used as input.
Non-overlapping argumentsare derived using the dynamic programming algo-rithm by Toutanova et al (2008).3 Other Language Analysis Functionality3.1 Sentence Boundary / Tokenization22This analyzer identifies sentence boundaries andbreaks the input into tokens.
Both are representedas offsets of character ranges.
Each token has botha raw form from the string and a normalized formin the PTB specification, e.g., open and close pa-rentheses are replaced by -LRB- and -RRB-, re-spectively, to remove ambiguity with parenthesesindicating syntactic structure.
A finite state ma-chine using simple rules and abbreviations detectssentence boundaries with high accuracy, and a setof regular expressions tokenize the input.3.2 Stemming / LemmatizationWe provide three types of stemming: Porter stem-ming, inflectional morphology and derivationalmorphology.3.2.1 StemsThe stemmer analyzer indicates a stem form foreach input token, using the standard Porter stem-ming algorithm (Porter, 1980).
These forms areknown to be useful in applications such as cluster-ing, as the algorithm assigns the same form ?dai?to ?daily?
and ?day?, but as these forms are notcitation forms of these words, presentation to endusers is known to be problematic.3.2.2 LemmasThe lemma analyzer uses inflectional morphologyto indicate the dictionary lookup form of the word.For example, the lemma of ?daily?
will be ?daily?,while the lemma of ?children?
will be ?child?.
Wehave mined the lemma form of input tokens usinga broad-coverage grammar NLPwin (Heidorn,2000) over very large corpora.3.2.3 BasesThe base analyzer uses derivational morphology toindicate the dictionary lookup form of the word; asthere can be more than one derivation for a givenword, the base type returns a list of forms.
For ex-ample, the base form of ?daily?
will be ?day?,while the base form of ?additional?
will be ?addi-tion?
and ?add?.
We have generated a static list ofbase forms of tokens using a broad-coveragegrammar NLPwin (Heidorn, 2000) over very largecorpora.
If the token form has not been observed inthose corpora, we will not return a base form.3.3 POS taggingWe train a maximum entropy Markov Model onpart-of-speech tags from the Penn TreeBank.
Thisoptimized implementation has very high accuracy(over 96% on the test set) and yet can tag tens ofthousands of words per second.3.4 ChunkingThe chunker (Gao et al, 2001) is based on a Cas-caded Markov Model, and is trained on the PennTreeBank.
With state-of-the-art chunking accuracyas evaluated on the benchmark dataset, the chunkeris also robust and efficient, and has been used toprocess very large corpora of web documents.4 The Flexibility of a Web ServiceBy making the MSR SPLAT toolkit available as aweb service, we can provide access to new tools,e.g.
sentiment analysis.
We are in the process ofbuilding out the tools to provide language analysisfor languages other than English.
One step in thisdirection is a tool for transliterating between Eng-lish and Katakana words.
Following Cherry andSuzuki (2009), the toolkit currently outputs the 10-best transliteration candidates with probabilities forboth directions.Another included service is the Triples analyz-er, which returns the head of the subject, the verb,and the head of the object, whenever such a tripleis encountered.
We found this functionality to beuseful as we were exploring features for our sys-tem submitted to the BioNLP shared task.5 Programmatic Access5.1 Web service referenceWe have designed a web service that accepts abatch of text and applies a series of analysis toolsto that text, returning a bag of analyses.
This mainweb service call, named ?Analyze?, requires fourparameters: the language of the text (such as ?en?for English), the raw text to be analyzed, the set ofanalyzers to apply, and an access key to monitorand, if necessary, constrain usage.
It returns a listof analyses, one from each requested analyzer, in a23simple JSON (JavaScript Object Notation) formateasy to parse in many programming languages.In addition, there is a web service call ?Lan-guages?
that enumerates the list of available lan-guages, and ?Analyzers?
to discover the set ofanalyzers available in a given language.5.2 Data FormatsWe use a relatively standard set of data representa-tions for each component.
Parse trees are returnedas S expressions, part-of-speech tags are returnedas lists, dependency trees are returned as lists ofparent indices, and so on.
The website contains anauthoritative description of each analysis format.5.3 SpeedSpeed of analysis is heavily dependent on thecomponent involved.
Analyzers for sentence sepa-ration, tokenization, and part-of-speech taggingprocess thousands of sentences per second; ourfastest constituency parser handles tens of sentenc-es per second.
Where possible, the user is encour-aged to send moderate sized requests (perhaps aparagraph at a time) to minimize the impact ofnetwork latency.6 ConclusionWe hope that others will find the tools that wehave made available as useful as we have.
We en-courage people to send us their feedback so that wecan improve our tools and increase collaboration inthe community.7 Script OutlineThe interactive UI (Figure 1) allows an arbitrarysentence to be entered and the desired levels ofanalysis to be selected as output.
As there existother such toolkits, the demonstration is primarilyaimed at allowing participants to assess the quality,utility and speed of the MSR SPLAT tools.http://research.microsoft.com/en-us/projects/msrsplat/ReferencesColin Cherry and Hisami Suzuki.
2009.
Discriminative sub-string decoding for transliteration.
In Proceedings ofEMNLP.Michael Collins.
1999.
Head-driven statistical models fornatural language parsing.
PhD Dissertation, University ofPennsylvania.Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun, MingZhou and Chang-Ning Huang.
2001.
Improving querytranslation for CLIR using statistical Models.
In Proceed-ings of SIGIR.George Heidorn.
2000.
Intelligent writing assistance.
In R.Dale, H. Moisl and H. Somers (eds.
), A Handbook of Natu-ral Language Processing: Techniques and Applications forthe Processing of Text.
New York: Marcel Dekker.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a Large Annotated Corpusof English: The Penn Treebank.
Computational Linguistics19(2): 313-330.Martha Palmer, Dan Gildea, Paul Kingsbury.
2005.
The Prop-osition Bank: An Annotated Corpus of Semantic Roles.Computational Linguistics, 31(1): 71-105Martin Porter.
1980.
An algorithm for suffix stripping.
Pro-gram, 14(3): 130-137.Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.2006.
Learning Accurate, Compact, and Interpretable TreeAnnotation.
In Proceedings of ACL.Brian Roark and Kristy Hollingshead.
2008.
Classifying chartcells for quadratic complexity context-free inference.
InProceedings of COLING.Ruslan Salakhutdinov and Sam Roweis.
2003.
Adaptive Over-relaxed Bound Optimization Methods.
In Proceedings ofICML.Kristina Toutanova, Aria Haghighi, and Christopher D. Man-ning.
2008.
A global joint model for semantic role labeling,Computational Linguistics, 34(2): 161-191.Nianwen Xue and Martha Palmer.
2004.
Calibrating Featuresfor Semantic Role Labeling.
In Proceedings of EMNLP.Munmun de Choudhury, Scott Counts, Michael Gamon.
NotAll Moods are Created Equal!
Exploring Human EmotionalStates in Social Media.
Accepted for presentation inICWSM 2012Munmun de Choudhury, Scott Counts, Michael Gamon.
Hap-py, Nervous, Surprised?
Classification of Human AffectiveStates in Social Media.
Accepted for presentation (shortpaper) in ICWSM 2012Figure 1.
Screenshot of the MSR SPLAT interactive UIshowing selected functionalities which can be toggledon and off.
This is the interface that we propose todemo at NAACL.24
