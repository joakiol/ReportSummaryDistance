Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 79?83,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsDeterministic Word SegmentationUsing Maximum Matching with Fully Lexicalized RulesManabu SassanoYahoo Japan CorporationMidtown Tower, 9-7-1 Akasaka, Minato-ku, Tokyo 107-6211, Japanmsassano@yahoo-corp.jpAbstractWe present a fast algorithm of word seg-mentation that scans an input sentencein a deterministic manner just one time.The algorithm is based on simple max-imum matching which includes execu-tion of fully lexicalized transformationalrules.
Since the process of rule match-ing is incorporated into dictionary lookup,fast segmentation is achieved.
We eval-uated the proposed method on word seg-mentation of Japanese.
Experimental re-sults show that our segmenter runs consid-erably faster than the state-of-the-art sys-tems and yields a practical accuracy whena more accurate segmenter or an annotatedcorpus is available.1 IntroductionThe aim of this study is to improve the speedof word segmentation.
Applications for manyAsian languages including Chinese and Japaneserequire word segmentation.
Such languages do nothave explicit word delimiters such as white spaces.Word segmentation is often needed before everytask of fundamental text processing such as count-ing words, searching for words, indexing docu-ments, and extracting words.
Therefore, the per-formance of word segmentation is crucial for theselanguages.
Take for instance, information retrieval(IR) systems for documents in Japanese.
It typi-cally uses a morphological analyzer1 to tokenizethe content of the documents.
One of the mosttime consuming tasks in IR systems is indexing,which uses morphological analysis intensively.Major approaches to Japanese morphologicalanalysis (MA) are based on methods of finding1 Japanese has a conjugation system in morphology anddoes not put white spaces between words.
Therefore, wehave to do morphological analysis in order to segment a givensentence into words and give an associated part-of-speech(POS) tag to each word.
In the main stream of the researchof Japanese language processing, morphological analysis hasmeant to be a joint task of segmentation and POS tagging.the best sequence of words along with their part-of-speech tags using a dictionary where they usethe Viterbi search (e.g., (Nagata, 1994), (Kudo etal., 2004)).
However, computation cost of mod-ern MA systems is mainly attributed to the Viterbisearch as Kaji et al.
(2010) point out.One of methods of improving the speed of MAor word segmentation will be to avoid or reducethe Viterbi search.
We can avoid this by usingmaximum matching in the case of word segmenta-tion.
Since there are many applications such as IRand text classification, where part-of-speech tagsare not mandatory, in this paper we focus on wordsegmentation and adopt maximum matching for it.However, maximum matching for Japanese wordsegmentation is rarely used these days because thesegmentation accuracy is not good enough and theaccuracy of MA is much higher.
In this paper weinvestigate to improve the accuracy of maximum-matching based word segmentation while keepingspeedy processing.2 Segmentation AlgorithmOur algorithm is basically based on maximummatching, or longest matching (Nagata, 1997).
Al-though maximum matching is very simple andeasy to implement, a segmenter with this algo-rithm is not sufficiently accurate.
For the pur-pose of improving the segmentation accuracy, sev-eral methods that can be combined with maximummatching have been examined.
In previous studies(Palmer, 1997; Hockenmaier and Brew, 1998), thecombination of maximum matching and character-based transformational rules has been investigatedfor Chinese.
They have reported promising resultsin terms of accuracy and have not mentioned therunning time of their methods, which might sup-posedly be very slow because we have to scan aninput sentence many times to apply learned trans-formational rules.In order to avoid such heavy post processing,we simplify the type of rules and incorporate theprocess of applying rules into a single process ofmaximum matching for dictionary lookup.
We79Input: ci: sentence which is represented as a char-acter sequence.
N : the number of characters in agiven sentence.
t: dictionary, the data structure ofwhich should be a trie.
oj: map an ID j to a singleword or a sequence of words.
hj: total length ofoj.Function: Lookup(t, c, i, N ): search the dic-tionary t for the substring c starting at the positioni up to N by using maximum matching.
This re-turns the ID of the entry in the dictionary t when itmatches and otherwise returns ?1.procedure Segment(c, N , t)var i: index of the character sequence cvar j: ID of the entry in the trie dictionary tbegini ?
1while (i ?
N ) do beginj = Lookup(t, c, i, N )if (j = ?1) then{ unknown word as a single character }print ci; i = i+ 1elseprint oj; i = i+ hj{ if ojis a sequence of words, print eachword in the sequence with a delimiter.Otherwise print ojas a single token.
}endifprint delimiter{ delimiter will be a space or something.
}endendFigure 1: Algorithm of word segmentation withmaximum matching that incorporates execution oftransformational rules.show in Figure 1 the pseudo code of the algorithmof word segmentation using maximum matching,where the combination of maximum matching andexecution of simplified transformational rules isrealized.
If each of the data ojin Figure 1 is a sin-gle token, the algorithm which is presented here isidentical with segmentation by maximum match-ing.We use the following types of transformationalrules: c0c1...cl?1cl?
w0...wmwhere ciis a char-acter and wjis a word (or morpheme).
Below aresample rules for Japanese word segmentation:?
????
(ha-na-i-ka) ?
?
(ha; topic-marker)??
(na-i; ?does not exist?)?
(ka;?or?)
2?
????
(dai-ko-gaku-bu) ?
?
(dai; ?uni-versity?)???
(ko-gaku-bu; ?the faculty ofengineering?
)Note that the form of the left hand side of the ruleis the sequence of characters, not the sequence ofwords.
Due to this simplification, we can combinedictionary lookup by maximum matching with ex-ecution of transformational rules and make theminto a single process.
In other words, if we finda sequence of characters of the left hand side of acertain rule, then we write out the right hand sideof the rule immediately.
This construction enablesus to naturally incorporate execution (or appli-cation) of transformational rules into dictionary-lookup, i.e., maximum matching.Although the algorithm in Figure 1 does notspecify the algorithm or the implementation offunction Lookup(), a trie is suitable for the struc-ture of the dictionary.
It is known that an effi-cient implementation of a trie is realized by usinga double-array structure (Aoe, 1989), which en-ables us to look up a given key at the O(n) cost,where n is the length of the key.
In this case thecomputation cost of the algorithm of Figure 1 isO(n).We can see in Figure 1 that the Viterbi searchis not executed and the average number of dictio-nary lookups is fewer than the number of char-acters of an input sentence because the averagelength of words is longer than one.
This contrastswith Viterbi-based algorithms of word segmenta-tion or morphological analysis that always requiredictionary lookup at each character position in asentence.3 Learning Transformational Rules3.1 Framework of LearningThe algorithm in Figure 1 can be combined withrules learned from a reference corpus as well ashand-crafted rules.
We used here a modified ver-sion of Brill?s error-driven transformation-basedlearning (TBL) (Brill, 1995) for rule learning.In our system, an initial system is a word seg-menter that uses maximum matching with a given2 If we use simple maximum matching, i.e., with no trans-formational rules, to segment the samples here, we will getwrong segmentations as follows: ????
?
??
(ha-na;?flower?)
??
(i-ka; ?squid?
), ????
?
??
(dai-ku;?carpenter?)??
(gaku-bu; ?faculty?).80w?0w?1?
?
?w?n?
w0w1?
?
?wmLw?0w?1?
?
?w?n?
Lw0w1?
?
?wmw?0w?1?
?
?w?nR ?
w0w1?
?
?wmRLw?0w?1?
?
?w?nR ?
Lw0w1?
?
?wmRTable 1: Rule templates for error-driven learningword list and words which occur in a given ref-erence corpus (a training corpus).
Our segmentertreats an unknown word, which is not in the dictio-nary, as a one-character word as shown in Figure 1.3.2 Generating Candidate RulesIn order to generate candidate rules, first wecompare the output of the current system withthe reference corpus and extract the differences(Tashiro et al., 1994) as rules that have the follow-ing form: Lw?0w?1?
?
?w?nR ?
Lw0w1?
?
?wmRwhere w?0w?1?
?
?w?nis a word sequence in the sys-tem output and w0w1?
?
?wmis a word sequencein the reference corpus and L is a word in the leftcontext and R is a word in the right context.
Afterthis extraction process, we generate four lexical-ized rules from each extracted rule by using thetemplates defined in Table 1.3.3 Learning RulesIn order to reduce huge computation when learn-ing a rule at each iteration of TBL, we use someheuristic strategy.
The heuristic score h is definedas: h = f ?
(n + m) where f is a frequency ofthe rule in question and n is the number of wordsin w?0w?1?
?
?w?nand m is the number of words inw0w1?
?
?wm.
After sorting the generated rules as-sociated with the score h, we apply each candidaterule in decreasing order of h and compute the errorreduction.
If we get positive reduction, we obtainthis rule and incorporate it into the current dictio-nary and then proceed to the next iteration.
If wedo not find any rules that reduce errors, we termi-nate the learning process.4 Experiments and Discussion4.1 Corpora and an Initial Word ListIn our experiments for Japanese we used the KyotoUniversity Text Corpus Version 4 (we call it KC4)(Kurohashi and Nagao, 2003), which includesnewspaper articles, and 470M Japanese sentences(Kawahara and Kurohashi, 2006), which is com-piled from the Web.
For training, we used twosets of the corpus.
The first set is the articles onJanuary 1st through 8th (7,635 sentences) of KC4.The second one is 320,000 sentences that are se-lected from the 470M Web corpus.
Note that theWeb corpus is not annotated and we use it afterword segmentation is given by JUMAN 6.0 (Kuro-hashi and Kawahara, 2007).
The test data is a setof sentences in the articles on January 9th (1,220sentences).
The articles on January 10th were usedfor development.We used all the words in the dictionary of JU-MAN 6.0 as an initial word list.
The number ofthe words in the dictionary is 542,061.
They aregenerated by removing the grammatical informa-tion such as part-of-speech tags from the entries inthe original dictionary of JUMAN 6.0.4.2 Results and DiscussionSegmentation Performance We used wordbased F-measure and character-wise accuracy toevaluate the segmentation performance.Table 2 shows comparison of various systemsincluding ours.
It is natural that since our sys-tem uses only fully lexicalized rules and does notuse any generalized rules, it achieves a moderateperformance.
However, by using the Web cor-pus that contains 320,000 sentences, it yields anF-measure of near 0.96, which is at the same levelas the F-measure of HMMs (baseline) in (Kudo etal., 2004, Table 3).
We will discuss how we canimprove it in a later section.Segmentation Speed Table 3 shows comparisonof the segmentation speed of various systems for320,000 sentences of the Web corpus.
Since, ingeneral, such comparison is heavily dependent onthe implementation of the systems, we have to becareful for drawing any conclusion.
However, wecan see that our system, which does not use theViterbi search, achieved considerably higher pro-cessing speed than other systems.Further Improvement The method that wehave presented so far is based on lexicalized rules.That is, we do not have any generalized rules.
Thesystem does not recognize an unknown Englishword as a single token because most of such wordsare not in the dictionary and then are split into sin-gle letters.
Similarly, a number that does not ap-pear in the training corpus is split into digits.It is possible to improve the presented methodby incorporating relatively simple post-processingthat concatenates Arabic numerals, numerals in81System # of Sent.
F-measure Char.
Acc.
# of RulesJUMAN 6.0 NA 0.9821 0.9920 NAMeCab 0.98 w/ jumandic 7,958 0.9861 0.9939 NAOurs w/o training corpus 0 0.8474 0.9123 0Ours w/ KC4 7,635 0.9470 0.9693 2228w/ Web320K 320,000 0.9555 0.9769 24267Table 2: Performance summary of various systems and configurations.
Jumandic for MeCab (Kudo etal., 2004) is stemmed from the dictionary of JUMAN.System (Charset Encoding) Model/Algorithm Time (sec.
)JUMAN 6.0 (EUC-JP) Markov model w/ hand-tuned costs 161.09MeCab 0.98 (UTF-8) w/ jumandic CRFs 13.71KyTea 0.3.3 (UTF-8) w/ jumandic Pointwise prediction w/ SVM 188.01Ours (UTF-8) Maximum matching w/ rules 3.22Table 3: Running time on the Web320K corpus.
We used a PC (Intel Xeon 2.33 GHz with 8GB memoryon FreeBSD 6.3).
The model for segmentation of KyTea (Neubig et al., 2011) in our experiments istrained with the word list of JUMAN on KC4 (see in Section 4.1).System F-measureJUMAN 6.0 0.9821MeCab 0.98 w/ jumandic 0.9861KyTea 0.3.3 w/ jumandic 0.9789MEMMs (Uchimoto et al., 2001) 0.9644HMMs (Kudo et al., 2004, Table 3) 0.9622Ours w/ KC4 0.9470Ours w/ KC4 + post-proc.
0.9680Ours w/ Web320K 0.9555Ours w/ Web320K + post-proc.
0.9719Table 4: Performance comparison to other sys-tems.kanji3, Latin characters, and katakana4 ones.
Thistype of post processing is commonly used inJapanese morphological analysis.
JUMAN andMeCab have a similar mechanism and use it.As an additional experiment, we incorporatedthis post processing into our segmenter and mea-sured the performance.
The result is shown in Ta-ble 4.
The segmenter with the post processingyields an F-measure of 0.9719 when it is trainedon the 320k Web corpus.
We observed that theperformance gap between state-of-the-art systemssuch as JUMAN and MeCab and ours becomessmaller.
Additional computation time was +10%3 Kanji in Japanese, or hanzi in Chinese, is a ideographicscript.
Kanji means Chinese characters.4 Katakana is one of the phonetic scripts used in Japanese.It is mainly used to denote loan words and onomatopoeias.Such type of words are very productive and are often un-known words in Japanese language processing.for the post processing and this means the seg-menter with the post processing is still much fasterthan other sophisticated MA systems.
Many ap-plications which have to process a huge amount ofdocuments would gain the benefits from our pro-posed methods.5 Related WorkThe use of transformational rules for improvingword segmentation as well as morphological anal-ysis is not new.
It is found in previous work (Papa-georgiou, 1994; Palmer, 1997; Hockenmaier andBrew, 1998; Gao et al., 2004).
However, their ap-proaches require the Viterbi search and/or a heavypost process such as cascaded transformation inorder to rewrite the output of the base segmenter.This leads to slow execution and systems that in-corporate such approaches have much higher costof computation than ours.6 ConclusionWe have proposed a new combination of maxi-mum matching and fully lexicalized transforma-tional rules.
The proposed method allows us tocarry out considerably faster word segmentationwith a practically reasonable accuracy.
We haveevaluated the effectiveness of our method on cor-pora in Japanese.
The experimental results showthat we can combine our methods with eitheran existing morphological analyzer or a human-edited training corpus.82ReferencesJun-Ichi Aoe.
1989.
An efficient digital search al-gorithm by using a double-array structure.
IEEETransactions on Software Engineering, 15(9):1066?1077.Eric Brill.
1995.
Transformation-based error drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
Computational Lin-guistics, 21(4):543?565.Jianfeng Gao, Andi Wu, Cheng-Ning Huang,Hong qiao Li, Xinsong Xia, and Hauwei Qin.2004.
Adaptive Chinese word segmentation.
InProc.
of ACL-2004, pages 462?469.Julia Hockenmaier and Chris Brew.
1998.
Error-drivenlearning of Chinese word segmentation.
In Proc.
ofPACLIC 12, pages 218?229.Nobuhiro Kaji, Yasuhiro Fujiwara, Naoki Yoshinaga,and Masaru Kitsuregawa.
2010.
Efficient staggereddecoding for sequence labeling.
In Proc.
of ACL2010, pages 485?494.Daisuke Kawahara and Sadao Kurohashi.
2006.Case frame compilation from the web using high-performance computing.
In Proc.
of LREC 2006,pages 1344?1347.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Appliying conditional random fields toJapanese morphological analysis.
In Proc.
ofEMNLP 2004, pages 230?237.Sadao Kurohashi and Daisuke Kawahara.
2007.JUMAN (a User-Extensible Morphological An-alyzer for Japanese).
http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN,http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN.Sadao Kurohashi and Makoto Nagao.
2003.
Build-ing a Japanese parsed corpus.
In Anne Abeille, edi-tor, Treebanks: Building and Using Parsed Corpora,pages 249?260.
Kluwer Academic Publishers.Masaaki Nagata.
1994.
A stochastic Japanese morpho-logical analyzer using a forward-DP backward-A*n-best search algorithm.
In Proc.
of COLING-94,pages 201?207.Masaaki Nagata.
1997.
A self-organizing Japaneseword segmenter using heuristic word identificationand re-estimation.
In Proc.
of WVLC-5, pages 203?215.Graham Neubig, Yosuke Nagata, and Shinsuke Mori.2011.
Pointwise prediction for robust, adaptableJapanese morphological analysis.
In Proc.
of ACL-2011.David D. Palmer.
1997.
A trainable rule-based algo-rithm for word segmentation.
In Proc.
of ACL-1997,pages 321?328.Constantine P. Papageorgiou.
1994.
Japanese wordsegmentation by hidden Markov model.
In Proc.
ofHLT-1994, pages 283?288.Toshihisa Tashiro, Noriyoshi Uratani, and TsuyoshiMorimoto.
1994.
Restructuring tagged corpora withmorpheme adjustment rules.
In Proc.
of COLING-1994, pages 569?573.Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-hara.
2001.
The unknown word problem: a morpho-logical analysis of Japanese using maximum entropyaided by a dictionary.
In Proc.
of EMNLP 2001,pages 91?99.83
