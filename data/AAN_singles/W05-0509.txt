Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 72?81,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsClimbing the path to grammar: a maximum entropy model ofsubject/object learningFelice Dell?Orletta Alessandro Lenci Simonetta Montemagni Vito PirrelliDept.
of Computer Science Dept.
of Linguistics ILC-CNR ILC-CNRUniversity of Pisa University of Pis a Area della Ricerca Area della RicercaLargo Pontecorvo 356100 Pisa (Italy)Via Santa Maria 3656100 Pisa (Italy)Via Moruzzi 156100 Pisa (Italy)Via Moruzzi 156100 Pisa (Italy){felice.dellorletta, alessandro.lenci, simonetta.montemagni, vito.pirrelli}@ilc.cnr.itAbstractIn this paper, we discuss an applic ation ofMaximum Entropy to modeling the acqui-sition of subject and object processing inItalian.
The model is able to learn fromcorpus data a set of experimentally andtheoretically well-motivated linguisticconstraints, as well as their relative sali-ence in Italian grammar development andprocessing.
The model is also shown toacquire robust syntactic generalizationsby relying on the evidence provided by asmall number of high token frequencyverbs only.
These results are consistentwith current research focusing on the roleof high frequency verbs in allowing chil-dren to converge on the most salient con-straints in the grammar.1 IntroductionCurrent research in language learning supports theview that developing grammatical competence in-volve mastering and integrating multiple, parallel,probabilistic constraints defined over differenttypes of linguistic (and non linguistic) information(Seidenberg and MacDonald 1999, MacWhinney2004).
This is particularly clear when we focus onthe core of grammatical deve lopment, namely theability to properly identify syntactic relations.
Psy-cholinguistic evidence shows that children learn toidentify sentence subjects and direct objects bycombining various types of probabilistic cues, suchas word order, noun animacy, definiteness, agree-ment, etc.
The relative prominence of each of thesecues during the development of a child?s syntacticcompetence can considerably vary cross-linguistically, mirroring their relative salience inthe adult grammar system (cf.
Bates et al 1984).If grammatical constraints are inherently prob-abilistic (Manning 2003), the path through whichthe child acquires adult grammar competence canbe viewed as the process of building a stochasticmodel out of the linguistic input.
Consistently with?usage-based?
approaches to language acquisition(cf.
Tomasello, 2000) grammatical constraintswould thus emerge from language use thanks to thechild?s ability to keep track of statistical regulari-ties in linguistic cues.
In turn, this raises the issueof how children are able to exploit the statisticaldistribution of cues in the linguistic input.
Varioustypes of cross-linguistic evidence converge on thehypothesis that children are actually able to takegreat advantage of the highly skewed distributionof naturalistic language data.
Goldberg et al(2004), Matthews et al (2003), Ninio (1999)among the others argue that verbs with high tokenfrequency in the input have a facilitatory effect inallowing children to derive robust syntactic gener-alizations even from surprisingly minimal input.According to this model, syntactic learning isdriven by a small pool of verbs occurring with thehighest token frequency: they approximately corre-spond to so-called ?light verbs?
such as Englishgo, give , want etc.
These verbs would act as ?cata-72lysts?
in allowing children to converge on the mostsalient grammar constraints of the language theyare acquiring.In computational linguistics, Maximum Entropymodels have proven to be robust statistical learningalgorithms that perform well in a number of proc-essing tasks (cf.
Ratnaparkhi 1998).
In this paper,we discuss successful application of a MaximumEntropy (ME) model to the processing of Italiansyntactic relations.
We believe that this discussionis of general interest for two basic reasons.
First,the model is able to learn, from corpus data, a setof experimentally and theoretically well-motivatedlinguistic constraints, as well as their relative sali-ence in the processing of Italian.
This suggests thatit is possible for a child to bootstrap and use thistype of knowledge on the basis of a specific distri-bution of real language data, a conclusion thatbears on the question of the role and type of innateinductive biases.
Secondly, the model is alsoshown to acquire robust syntactic generalizationsby relying on the evidence provided by a smallnumber of high token frequency verbs only.
Withsome qualifications, this evidence sheds light onthe interaction between highly skewed languagedata distributions and language maturation.
Robustgrammar generalizations emerge on the basis ofexposure to early, statist ically stable and lexicallyunderspecified evidence, thus providing a reliablebackbone to children?s syntactic development andlater lexical organization.In the following section we first broach thegeneral problem of parsing subjects and objects inItalian.
Section 3 describes an ME model of theproblem.
Section 4 and 5 are devoted to a detailedempirical analysis of the interaction of differentfeature configurations and of the interplay betweenverb token frequency and relevant generalizations.Conclusions are drawn in the final discussion.2 Subjects and Objects in ItalianChildren that learn how to process subjects andobjects in Italian are confronted with a twofoldchallenge: i) the relatively free order of Italian sen-tence constituents and ii) the possible absence ofan overt subject.
The existence of a preferred Sub-ject Verb Object (SVO) order in Italian mainclauses does not rule out all other possible permu-tations of these units: in fact, they are all attested,albeit with considerable differences in distributionand degree of markedness (Bartolini et al 2004).1Moreover, because of pro-drop, an Italian VerbNoun (VN) sequence can either be interpreted as aVO construction with subject omission (e.g.
hadichiarato guerra ?
(he) declared war?)
or as aninstance of postverbal subject (VS, e.g.
ha di-chiarato Giovanni ?John declared?).
Symmetri-cally, an NV sequence is potentially ambiguousbetween SV and OV: compare il bambino ha man-giato  ?the child ate?
with il gelato ha mangiato ?theice-cream, (he) ate?.These grammatical facts are in keeping withwhat we know about Italian children?s parsingstrategies.
Bates et al (1984) show that while, inEnglish, word order is by and large the most effec-tive cue for subject-object identification (hence-forth SOI) both in syntactic processing and duringthe child?s syntactic development, the same cueplays second fiddle in Italian.
Bates and colleaguesbring empirical evidence supporting the hypothesisthat Italian children show extreme reliance on NVagreement and, secondly, on noun animacy, ratherthan word order.
They conclude that the followingsyntactic constraints dominance hierarchy is opera-tive in Italian: agreement > animacy > word order.The fact that animacy can reliably be resortedto in Italian SOI receives indirect confirmationfrom corpus data.
We looked at the distribution ofanimate subjects and objects in the Italian Syntac-tic Semantic Treebank (ISST, Montemagni et al,2003), a 300,000 tokens syntactically annotatedcorpus, including articles from contemporary Ita l-ian newspapers and periodicals covering a broadvariety of topics.
Subjects and objects in ISSTwere automatically annotated for animacy usingthe SIMPLE Italian computational lexicon (Lenciet al 2000) as a background semantic resource.The annotation was then checked manually.
Cor-pus analysis highlights a strong asymmetry in thedistribution of animate nouns in subject and objectroles: over 56.6% of ISST subjects are animate(out of a total number of 12,646), while only the11.1% of objects are animate (out of a total numberof 5,559).
Such an overwhelming preference forinanimate ob jects in adult language data makesanimacy play a very important role in SOI, both asa key developmental factor in the bootstrapping ofthe syntax-semantics mapping and as a reliable1 In the present paper we restrict ourselves to the case of de-clarative main clauses.73processing cue, consistently with psycholinguisticdata.On the other hand, the distribution of word or-der configurations in the same corpus shows an-other interesting asymmetry.
NV sequences receivean SV interpretation in 95.6% of the cases, and anobject interpretation in the remaining 4.4% (mostof which are clitic and relative pronouns, whosepreverbal pos ition is grammatically constrained).The situation is quite different when we turn to VNsequences, where verb-object pairs represent73.4% of the cases, with verb-subject pairs repre-senting the remaining 26.6%.
We infer that ?
atleast in standard written Italian ?
VS is a muchmore consistently used construction than OV, andthat the role of word order in Italian parsing is nota marginal one across the board, but rather relativeto VN contexts only.
In NV constructions there is astrong preference for a subject interpretation, andthis suggests a more dynamic dominance hierarchyof Italian syntactic constraints than the one pro-vided above.As for agreement, it represents conclusive evi-dence for SOI only when a nominal constituent anda verb do not agree in number and/or person (as inleggono il libro ?
(they) read the book?).
On thecontrary, when noun and verb share the same per-son and number the impact of agreement on SOI isneutralised, as in il bambino legge il libro ?thechild reads the book?
or in ha dichiarato il presi-dente ?the president declared?.
Although this ambi-guity arises in specific contexts (i.e.
when the verbis used in the third person singular or plural and thesubject/object candidate agrees with it), it is inter-esting to note that in ISST: third person verb formscover 95.6% of all finite verb forms; and, moreinterestingly for our present concerns, 87.9% of allVN and NV pairs involving a third person verbform contains an agreeing noun.
From this we con-clude that the contribution of agreement to ourproblem is fairly limited, as lack  of agreementshows up only in a limited number of contexts.All in all, corpus data lend support to the ideathat in Italian SOI is governed by a complex inter-play of probabilistic constraints of a different na-ture (morpho-syntactic, semantic, word order etc.
).Moreover, distributional asymmetries in languagedata seem to provide a fairly reliable statistical ba-sis upon which relevant probabilistic constraintscan be bootstrapped and combined consistently.
Inthe following section we shall present a ME modelof how constraints and their interaction can bebootstrapped from language data.3 A Maximum Entropy model of SOIThe Maximum Entropy (ME) framework offers amathematically  sound way to build a probabilisticmodel for SOI, which combines different linguisticcues.
Given a linguistic context c and an outcomea?A that depends on c, in the ME framework theconditional probability distribution p(a|c) is esti-mated on the basis of the assumption that no a pri-ori constraints must be met other than those relatedto a set of features f j(a,c) of c, whose distribution isderived from the training data.
It can be proventhat the probability distribution p satisfying theabove assumption is the one with the highest en-tropy, is unique and has the following exponentialform (Berger et al 1996):(1) ?==kjcajfjcZcap1),()(1)|( awhere Z(c) is a normalization factor, f j(a,c) are thevalues of k features of the pair (a,c) and correspondto the linguistic cues of c that are relevant to pre-dict the outcome a.
Features are extracted from thetraining data and define the constraints that theprobabilistic model p must satisfy.
The parametersof the distribution a1, ?, ak correspond to weightsassociated with the features, and determine therelevance of each feature in the overall model.
Inthe experiments reported below feature weightshave been estimated with the Generative IterativeScaling (GIS) algorithm implemented in the AMISsoftware (Miyao and Tsujii 2002).We model SOI as the task of predicting the cor-rect syntactic function f  ?
{subject, object} of anoun occurring in a given syntactic context s. Thisis equivalent to build the conditional probabilitydistribution p(f |s) of having a syntactic function fin a syntactic context s .
Adopting the ME ap-proach, the distribution p can be rewritten in theparametric form of (1), with features correspond-ing to the linguistic contextual cues relevant toSOI.
The context s  is a pair <vs , ns>, where vs isthe verbal head and ns its nominal dependent in s.This notion of s departs from more traditionalways of describing an SOI context as a triple ofone verb and two nouns in a certain syntactic con-figuration (e.g, SOV or VOS, etc.).
In fact, we as-sume that SOI can be stated in terms of the more74local task of establishing the grammatical functionof a noun n observed in a verb-noun pair.
Thissimplifying assumption is consistent with the claimin MacWhinney et al (1984) that SVO word orderis actually derivative from SV and VO local pat-terns and downplays the role of the transitive com-plex construction in sentence processing.
Evidencein favour of this hypothesis also comes from cor-pus data: in ISST, there are 4,072 complete sub-ject-verb-object-configurations, a small number ifcompared to the 11,584 verb tokens appearing witheither a subject or an object only.
Due to the com-parative sparseness of canonical SVO constructionsin Italian, it seems more reasonable to assume thatchildren should pay a great deal of attention toboth SV and VO units as cues in sentence percep-tion (Matthews et al 2004).
Reconstruction of thewhole lexical SVO pattern can accordingly be seenas the end point of an acquisition process wherebysmaller units are re-analyzed as being part of morecomprehensive constructions.
This hypothesis ismore in line with a distributed view of canonicalconstructions as derivative of more basic local po-sitional patterns, working together to yield morecomplex and abstract constructions.
Last but notleast, assuming verb-noun pairs as the relevantcontext for SOI allows us to simultaneously modelthe interaction of word order variation with pro-drop in Italian.4 Feature selectionThe most important part of any ME model is theselection of the context features whose weights areto be estimated from data distributions.
Our featureselection strategy is grounded on the main assump-tion that features should correspond to linguisti-cally and psycholinguistically well-motivatedcontextual cues.
This allows us to evaluate theprobabilistic model also with respect to its abilityto replicate psycholinguistic experimental resultsand to be consistent with linguistic generalizations.Features are binary functions fki,f  (f ,s), whichtest whether a certain cue ki for the function f  oc-curs in the context s .
For our ME model of SOI,we have selected the following types of features:Word order tests the position of the noun wrt theverb, for instance:(2)???
==otherwisepostposnounifsubjf subjpost 0.1),(,ssAnimacy  tests whether the noun in s  is animate orinanimate (cf.
?.2).
The centrality of this cue inItalian is widely supported by psycholinguisticevidence.
Another source of converging evidencecomes from functional and typological linguisticresearch.
For instance, Aissen (2003) argues forthe universal value of the following hierarchy rep-resenting the relative markedness of the associa-tions between grammatical functions and animacydegrees (with each item in these scale been lessmarked than the elements to its right):Animacy Markedness HierarchySubj/Human > Subj/Animate > Subj/InanimateObj/Inanimate > Obj/Animate > Obj/HumanMarkedness hierarchies have also been interpretedas probabilistic constraints estimated form corpusdata (Bresnan et al 2001, ?vrelid 2004).
In ourME model we have used a reduced version of theanimacy markedness hierarchy in which humanand animate nouns have been both subsumed underthe general class animate.Definiteness tests the degree of ?referentiality?
ofthe noun in a context pair s .
Like for animacy,definiteness has been claimed to be associated withgrammatical functions, giving rise to the followinguniversal markedness hierarchy Aissen (2003):Definiteness Markedness HierarchySubj/Pro > Subj/Name > Subj/Def > Subj/IndefObj/Indef > Obj/Def > Obj/Name > Obj/ProAccording to this hierarchy, subjects with a lowdegree of definiteness are more marked than sub-jects with a high degree of definiteness (for objectsthe reverse pattern holds).
Given the importanceassigned to the definiteness markedness hierarchyin current linguistic research, we have included thedefiniteness cue in the ME model.
It is worth re-marking that, unlike animacy, in psycholinguisticexperiments definiteness has not been assigned anyeffective role in SOI.
This makes testing this cue ina computational model even more interesting, as away to evaluate its effective contribution to ItalianSOI.
In our experiments, we have used a ?com-pact?
version of the definiteness scale: the defi-niteness cue tests whether the noun in the context75pair i) is a name or a pronoun ii) has a definite arti-cle iii), has an indefinite article or iv) is a ?bare?noun (i.e.
with no article).
It is worth saying that?bare?
nouns are usually placed at the bottom endof the definiteness scale.The three types of features above only refer tonominal cues in the context pairs.
Nevertheless,specific lexical properties of the verb can also beresorted to in SOI.
The probability for ns to be sub-ject or object may also depend on the specific lexi-cal preferences of vs. To take this lexical factorinto account, we add a set of lexical cues to thethree general feature types above.
Lexical cues testanimacy with respect to a specific verb vk:(3)????
?=?==otherwiseanimnvvifsubjf ksubjkvanim01),(,,sssLexical features provide evidence of the propensityof a given verb to have an animate (inanimate)subject or object.
In fact, the verb argument struc-ture and thematic properties may well influence thepossible distribution of animate (inanimate) sub-jects and objects, thus overriding more generaltendencies.
By including lexical cues, we are thusable to test the interplay of lexical constraints withgeneral grammatical ones.Note that in our ME model we have not in-cluded agreement as a feature, in spite of itsprominent role in Italian.
The fact that agreementis often inconclusive for SOI (?.2) suggests thatchildren must also acquire the ability to deal withthe interplay of various concurrent constraints,none of which is singularly sufficient for the taskcompletion this type of competence.
It is exactlythis area of syntactic competence that we wanted toexplore with the experiments reported below (cf.MacWhinney et al 1984, who similarly abstractfrom the dominant role of case in German SOI).5 Testing feature configurations for SOIThe ME model for Italian SOI has been trained on18,205 verb-subject/object pairs extracted fromISST.
The training set was obtained by extractingall verb-subject and verb-object dependenciesheaded by an active verb occurring in a finite ver-bal construction and by excluding all cases wherethe position of the nominal constituent was gram-matically constrained (e.g.
clitic objects, relativeclauses).
Two different feature configurations havebeen used for training:-  non-lexical feature configuration (NLC), in-cluding only general features acting as globalconstraints: namely word order, noun animacyand noun definiteness;- lexical feature configuration (LC), includingword order, noun animacy and definiteness,and information about the verb head.The test corpus consists of 645 verb-noun pairsextracted from contexts where agreement happensto be neutralized.
Of them, 446 contained a subject(either pre- or post-verbal) and 199 contained anobject (either pre- or post-verbal).
The two featureconfigurations were evaluated by calculating thepercentage of correctly assigned relations over thetotal number of test pairs (accuracy).
As our modelalways assigns one syntactic relation to each testpair, accuracy equals both standard precision andrecall.
Finally, we have assumed a baseline scoreof 69%, corresponding to the result yielded by adumb model assigning to each test pair the mostfrequent relation in the training corpus, i.e.
subject.5.1 Non-lexical feature configurationOur first experiment was carried out with NLC.The accuracy on the test corpus is 91.5%; mosterrors (i.e.
96.4%) relate to the postverbal position,with 44 mistaken subjects (42 inanimate) and 9mistaken objects (all animate).
The score was con-firmed by a 10-fold cross-validation on the wholetraining set (89.3% accuracy).A further way to evaluate the goodness of themodel is by inspecting the weights associated withfeature values (Table 1).Subj ObjPreverbal 1,34E+00 2,10E-02Postverbal 5,21E-01 1,47E+00Anim 1,28E+00 3,34E-01Inanim 8,60E-01 1,21E+00PronName  1,22E+00 5,75E-01DefArt 1,05E+00 1,00E+00IndefArt 8,33E-01 1,16E+00NoArticle 9,46E-01 1,07E+00Table 1 ?
Feature value weights in NLCThe grey cells in Table 1 highlight the preferenceof each feature value for either subject or objectidentif ication: e.g.
preverbal subjects are stronglypreferred over preverbal objects; animate subjects76are preferred over animate objects, etc.
Interest-ingly, if we rank the Anim and Inanim values forsubjects and objects, we can observe tha t they dis-tribute consistently with the Animacy MarkednessHierarchy reported in ?.4: Subj /Anim >Subj/Inanim and Obj/Inanim > Obj/Anim.
Simi-larly, by ranking the values of the definiteness fea-tures in the Subj column by decreasing weightvalues we obtain the following ordering: Pron-Name > DefArt > IndefArt > NoArt, which nicelyfits in with the Definiteness Markedness Hierarchyin ?.4.
The so-called ?markedness reversal?
is ob-served if we focus on the values for the same fea-tures in the Obj column: the PronName featurerepresents the most marked option, followed byDefArt.
The only exception is represented by therelative ordering of IndefArt and NoArt whichhowever show very close values.Evaluating feature salienceIn order to evaluate the most reliable cues in ItalianSOI, we have analysed the model predictions fordifferent bundles of feature values.
For each of the16 different bundles (b) attested in the data, wehave estimated p(subj|b) and p(obj|b):b p(subj|b) p(obj|b)Pre Anim IndefArt 0,994 0,006Pre Anim DefArt 0,996 0,004Pre Anim NoArt 0,995 0,005Pre Anim PronName 0,998 0,002Pre Inanim IndefArt 0,970 0,030Pre Inanim DefArt 0,979 0,021Pre Inanim NoArt 0,976 0,024Pre Inanim PronName 0,990 0,010Post Anim IndefArt 0,495 0,505Post Anim DefArt 0,589 0,411Post Anim NoArt 0,546 0,454Post Anim PronName  0,743 0,257Post Inanim IndefArt 0,153 0,847Post Inanim DefArt 0,209 0,791Post Inanim NoArt 0,182 0,818Post Inanim PronName 0,348 0,652Table 2 ?
Subj/obj probabilities by different bundlesThe model shows a neat preference for subjectwhen the noun is preverbal.
Instead, when the nounis postverbal, function assignment is de facto de-cided by the noun animacy.
Conversely, definite-ness features have a much more secondary role:they can re-enforce (or weaken) the preference ex-pressed by animacy, but they do not have thestrength to determine SOI.The relative salience of the different constraintsacting on SOI can also be inferred by comparingthe weights associated with individual feature val-ues.
For instance, Goldwater and Johnson (2003)show that ME can be successfully applied to learnconstraint rankings in Optimality Theory, by as-suming the parameter weights a1, ?, ak as theranking values of the constraints.
The followingtable lists the 16 general constraints of the modelby increasing weight values:Feature WeightPreverbal_Obj 2,10E-02Anim_Obj 3,34E-01Postverbal_Subj 5,21E-01ProName_Obj 5,75E-01IndefArt_Subj 8,33E-01Inanim_Subj 8,60E-01NoArticle_Subj 9,46E-01ArtDef_Obj 1,00E+00DefArt_Subj 1,05E+00NoArticle_Obj 1,07E+00IndefArt_Obj 1,16E+00Inanim_Obj 1,21E+00PronName_Subj 1,22E+00Anim_Subj 1,28E+00Preverbal_Subj 1,34E+00Postverbal_Obj 1,47E+00Table 3 ?
Constraint weights rankingThe rankings in Table 3 can be used to derive therelative salience of each constraint.
Lower rankedconstraints correspond to more marked syntacticconfigurations that are then disfavoured in SOI.Notice that the two animacy constraints Anim_Objand Anim_Subj are respectively placed near thebottom and the top end of the scale.
Notwithstand-ing the low position of Postverbal_Subj, animacyis thus able to override the word order constraintand to produce a strong tendency to identify ani-mate nouns as subjects, even when they appear inpostverbal position (cf.
Table 2 above).
The con-straint ranking thus confirms the interplay betweenanimacy and word order in Italian, with the formerplaying a decisive role in assigning the syntacticfunction of postverbal nouns.
On the other hand,77the constraints involving noun definiteness occupya more intermediate position in the general rank-ing, with very close values.
This is again consistentwith the less decisive role of this feature type inSOI, as shown above.5.2 Lexical feature configurationIn this experiment the general features reported inTable 1 have been integrated with 4,316 verb-specific features as the ones exemplified below forthe verb dire ?say?
:dire_animSog 1.228213e+00dire_noanimSog 7.028484e-01dire_animOgg 3.645964e-01dire_noanimOgg 1.321887e+00whose associated weights show the strong prefer-ence of this verb to take animate subjects as op-posed to inanimate ones as well as a preference forinanimate objects with respect to animate ones.The results achieved with LC on the test corpusshow a significant improvement with respect tothose obtained with NLC: the accuracy is now95.5%, with a  4% improvement, confirmed by a10-fold cross-validation (94.9%).
Also in this case,most of the errors relate to the pos tverbal position(i.e.
27 out of 29), partitioned into 26 mistakensubjects and 1 mistaken object.
Lexical featureshave been resorted to to solve most of the NLCerrors (i.e.
34 out of 55).
It is interesting to notehowever that lexical features can also be mislead-ing.
The LC results include 8 new errors, suggest-ing that lexical features do not always provideconclusive evidence: in fact, in 185 cases out of645 test VN pairs (i.e.
28.7% of the cases) generalfeatures are preferred over lexical ones.
It is alsoworth mentioning that the ranking of general ani-macy and definiteness features in LC actually fitsin with the respective markedness hierarchies evenwith a better approximation than the one producedby NLC.
Finally, the relative prominence of thedifferent global features confirms the trend in Ta-ble 2, with word order being predominant in pre-verbal pos ition and animacy playing a major rolewith postverbal nouns.Both feature configurations of the ME modelthus appear to comply with linguistic and psycho-linguistic generalizations on SOI.
On the linguisticside, the constraints learnt by the model are consis-tent with universal markedness hierarchies forgrammatical relations.
Secondly, the prominenceof the various constraints in the model fits in wellwith psycholinguistic data.
Consistently with theresults in Bates et al (1984), the model confirmsthe great impact of noun animacy in Italian, al-though in this case its key role seems to be moredirectly limited to the postverbal position.
Con-versely, the preverbal position is by itself a verystrong cue for subject interpretation.6 High frequency verbs and SOIFrequency is known to play a major influence inlanguage learning.
In morphology, for example,highly frequent lexical items tend to be shorterforms, more readily accessible in the mental lexi-con, independently stored as whole items (Stem-berger and MacWhinney 1986) and fairly resistantto morphological overgeneralization through time,thus establishing a correlation between irregularinflected forms and frequency.
Frequency has alsobeen assigned a key role in the acquisition of syn-tactic constructions.
In fact, Goldberg (1998) andNinio (1999) have independently argued for theexistence of a causal relation between early expo-sure to highly frequent light verbs and acquisitionof abstract syntax-semantics mappings (construc-tions).
Light verbs such as want, put and go tend tobe very frequent, because they are applicable in awider range of contexts and are learned and used atan early language maturation stage The main ideais that children?s early use of these high frequencyverbs is conducive to the acquisition of abstractconstructional properties generalizing over particu-lar instances.Goldberg et al (2004) motivate this hypothesisby observing that light verbs have high input fre-quency in the child?s developmental environmentand, at the same time, exhibit a low degree of se-mantic specialization.
Hence, she argues, it takes alittle abstraction step for a child to jump from ac-tual instances of use of light verbs to the syntax-semantics association of their underlying construc-tion.
On the other hand, Ninio (1999) grounds thefacilitatory role of highly frequent verbs on theirbeing ?pathbreaking?
prototypes of the construc-tion they instantiate, since they are the best modelsof the relevant combinatorial and semantic proper-ties of their construction in a relatively undilutedfashion.
However, in the case of light verb con-structions, the correlation between high frequency78and construction prototypicality and extension istenuous.
In fact, it is difficult to argue that frequentlight verbs such as see, want or do exhibit a highdegree of both semantic and constructional trans i-tivity (Goldberg et al 2004).
This is reminiscent ofthe morphological behaviour of very frequent wordforms in inflectional languages, as most of theseforms are highly fused and show a general ten-dency towards irregular inflection and low mor-phological prototypicality.
Furthermore, it isdifficult to reconcile the ?pathbreaking?
view withthe observation that frequently observed linguisticunits are memorized in full, as unanalyzed wholes.6.1 Testing the role of frequencyTo address these open issues and put the alleged?pathbreaking?
role of light verbs to the challeng-ing test of a probabilistic model, we carried out asecond battery of experiments to learn the general,non-lexical constraints from two training corporaof roughly equivalent size where overall type andtoken verb frequencies were controlled for.
Bothcorpora are a subset of the original training set:1. skewed frequency corpus (SF) ?
it includes5,261 context pairs, obtained by selecting 15 verbsoccurring more than 100 times in ISST (figures inparentheses give their token frequency): essere?be?
(2406), avere ?have?
(708), fare  ?do, make?
(527), dire ?say, tell?
(275), dare ?give?
(173), ve-dere ?see?
(134), andare ?go?
(126), sembrare?seem?
(124), cercare ?try?
(122), mettere ?put?
(122), portare ?take?
(121), trovare ?find?
(112),volere ?want?
(105), lasciare ?leave?
(105), riuscire?manage?
(101).
It is worth noticing that this setincludes typical ?pathbreaking?
verbs;2. balanced frequency corpus (BF) ?
this corpusincludes 5,373 context pairs selected in such a wayto ensure that every verb type in the original train-ing set is attested in BF and occurs at most 6 times.For verbs occurring with a higher frequency, thepairs to be included in BF have been randomly se-lected.Thus SF and BF represent two opposite trainingsituations: SF contains few types with very hightoken frequencies, while BF contains a high num-ber of verb types (i.e.
1457), with very low anduniform token frequency.
These training sets re-semble the structure of linguistic input used byGoldberg et al (2004) for their experiments.
Inthat case, one group of subjects was exposed tolinguistic inputs in which some verbs occurredwith a much higher frequency than the others; asecond group of subjects was instead exposed tolinguistic stimuli in which every verb occurredwith roughly equal frequency.
Therefore, by train-ing our ME model on SF and BF we are able  toevaluate the effective role of high token frequencyverbs in driving syntactic learning.The ME model with the general features only(i.e.
NLC) was first trained on SF, and then testedon the 645-pair corpus in ?.5, showing a 90% ac-curacy.
The same ME model was then trained onBF, and then tested on the 645-pair corpus, scoringa 87% accuracy.
The ME model trained on theskewed frequency data thus outperforms the modeltrained on BF in a statistically significant way (?2 =4.97; a=0.05; p-value = 0.025).By using a training set formed only by the verbswith the highest token frequency, the model hasthus been able to acquire robust syntactic con-straints for SOI.
Once these constraints have beenapplied to unseen events, the model has achieved aperformance comparable to the one of the generalmodels in ?.5.
This is somehow even more signif i-cant if we consider that the training set was nowformed by less than one-third of the pairs on whichthe models in ?.5 were trained.
Data quantity aside,the most relevant fact is that it is the way verb fre-quencies are distributed to determine the learningpath, with a significant positive effect produced byhigh token frequency verbs.
In the model trainedon SF, feature ranking is also governed by mark-edness relations, and the relative prominence of thevarious constraints is utterly similar to the one dis-cussed in ?.5.
In other terms, the results of this ex-periment prove that frequent verbs are actuallyable to act as ?catalysts?
of the syntactic acquis i-tion process.
It is possible for children to convergeon the correct generalizations governing SOI inItalian, just by relying on the linguistic evidenceprovided by the most frequent verbs.This view suggests a way out of the apparentparadox of the ?pathbreaking?
hypothesis: highlyfrequent verbs can be assumed to provide stableand consistent multiple probabilistic cues for theassignment of subject/object relations.
The exis-tence of pos itional patterns that occur with hightoken frequency may well provide a deeply en-trenched and highly salient set of distributionalcues that act as probabilistic constraints on con-structional generalizations.
We hypothesize thatsimilar constructions of other less frequent verbs79are processed, for lack of more specific overridinginformation, in the light of these constraints.
Sinceprocessing is the result of a ?conspiracy?
of dis-tributed constraints, ?pathbreaking?
prototypesneed not be real construction exemplars but highlyschematic patterns.
We proved that highly frequentlocal positional patterns offer the right sort of con-straint conspiracy.7 General discussionIt appears that the distributional evidence of highfrequency light verbs may well provide a solidcognitive anchor for sweeping perceptual generali-zations on the syntax-semantics mapping.
Thesegeneralizations are local, in that they involve pos i-tional NV and VN pairs only, and are perceptual asthey address the issue of identifying appropriatesyntactic relations by relying on perceptual fea-tures of linguistic contexts, such as position, ani-macy, etc.
On the basis of these findings, one canreasonably argue that complex lexical construc-tions (in the sense of Goldberg 1998) are builtupon these local patterns, by combining them inthose contexts where the presence of a particularverb licenses such a combination.The two feature configurations discussed in ?.5(i.e.
NLC and LC) can thus be viewed as two suc-cessive steps along the path that leads towards theemergence of complex, lexically-driven construc-tions.
This can actually be modeled as the incre-mental process of adding more and more lexicalconstraints to early lexicon-free generalizations(based on word order, animacy, definiteness etc.
).As a result of such additional constraints, the pres-ence of an intransitive verb may completely ruleout the object interpretation of a VN pattern, flyingin the face of a general bias towards viewing VN asa transitive pattern.
This picture is compatible withthe well-known observation that constructions areused rather conservatively by children at earlystages of language maturation (Tomasello 2000).In fact, if early generalizations are mainly percep-tual and local, we do not expect them to be used inproduction, at least until the child reaches a stagewhere they are combined into bigger lexically-driven constructions.ME has proven to be a sound computationallearning framework to simulate the interplay ofcomplex probabilistic constraints in language.
Ourexperiments confirm linguistic generalizations andpsycholinguistic data for subjects and objects inItalian, while raising new interesting issues at thesame time.
This is the case of the role of definite-ness in SOI.
In fact, the model features neatly re-produce the definiteness markedness hierarchy, butdefiniteness does not appear to be really influentialfor subject and object processing.
Various hy-potheses are compatible with such results, inclu d-ing that definiteness is not a cue on which speakersrely for SOI in Italian.
Another more interestingpossibility is that definiteness constraints may in-deed play a decisive role when the learner is askedto assign subject and object relations in the contextof a more complex construction than a simple NVpair.
Suppose that both nouns of a noun-noun-verbtriple are amenable to a subject interpretation, butthat one of them is a more likely subject than theother due to its being part of a definite nounphrase.
Then, it is reasonable to expect that themodel would select the definite noun phrase as thesubject in the triple and opt for an object interpre-tation of the other candidate noun phrase.As part of our future work, we plan to train theME model on a more realistic corpus of parentalinput to Italian children, available in the CHILDESdatabase (MacWhinney, 2000: http://childes.psy.cmu.edu/data/Romance/Italian).
In fact, there isconverging evidence that the use of particular con-structions in parental speech is largely dominatedby the use of each construction with one specific,highly frequent verb (e.g.
go for the intransitiveconstruction).
The same trends noted in mother?sspeech to children are mirrored in children?s earlyspeech (Goldberg et al, 2004).
Quochi (in prepara-tion) reports a similar distributional pattern for thecaused motion and intransitive motion verbs in twoItalian CHILDES corpora (named ?Italian-Antelmi?
and ?Italian-Calambrone?).
If these find-ings are confirmed, the high accuracy of our MEmodel trained on the skewed frequency corpus(SF) allows us to expect an equally high accuracywhen training the model on evidence from Italianparental speech.This brings us to another related point: lack ofcorrection/supervision in parental input.
Since ourME model heavily relies on previously classifiednoun-verb pairs, we can legitimately wonder howeasily it can be extended to simulate child languagelearning in an unsupervised mode.
In fact, it shouldbe appreciated that, in our experiments, compar-tively little rests on supervised classification.
Iden-80tification of the contextually-relevant subject is, forlack of explicit morphosyntactic clues such asagreement and diathesis, simply a matter of guess-ing the more likely agent of the action expressedby the verb on the basis of semantic and pragmaticfeatures such as animacy, definiteness and nounposition to the verb.
Mutatis mutandis, the sameholds for object identification.
It is then highlylikely that salient evidence for the correct sub-ject/object classific ation comes to the child fromdirect observation of the situation described by asentence.
It is such systematic coupling of linguis-tic evidence from the sentence with perceptual evi-dence of the situation described by the sentencethat can assist the child in developing interfacenotions such as subject, object and the like.ReferencesAissen J., 2003.
Differential object marking: iconicityvs.
economy.
Natural Language and Linguistic The-ory, 21: 435-483.Bartolini R., Lenci A., Montemagni S., Pirrelli V., 2004.Hybrid constraints for robust parsing: First experi-ments and evaluation.
LREC2004: 859-862.Bates E., MacWhinney B., Caselli C., Devescovi A.,Natale F., Venza V., 1984.
A crosslinguistic study ofthe development of sentence interpretation strategies.Child Development, 55: 341-354.Berger A., Della Pietra S., Della Pietra V., 1996.
Amaximum entropy approach to natural languageprocessing.
Computational Linguistics 22(1): 39-71Bresnan J., Dingare D., Manning C. D., 2001.
Soft con-straints mirror hard constraints: voice and person inEnglish and Lummi.
Proceedings of the LFG01 Con-ference , Hong Kong: 13-32.Goldberg A. E., 1998.
The emergence of the semanticsof argument structure constructions.
In B. MacWhin-ney (e d.), The Emergence of Language .
LawrenceErlbaum Associates, Hillsdale, N. J.: 197-212.Goldberg A. E., Casenhiser D., Sethuraman N., 2004.Learning argument structure generalizations, Cogni-tive Linguistics.Goldwater S., Johnson M. 2003.
Learning OT Con-straint Rankings Using a Maximum Entropy Model.In Spenader J., Eriksson A., Dahl ?.
(eds.
), Proceed-ings of the Stockholm Workshop on Variation withinOptimality Theory.
April 26-27, 2003, StockholmUniversity: 111-120.Lenci A. et al, 2000.
SIMPLE: A Ge neral Frameworkfor the Development of Multilingual Lexicons.
Inter-national Journal of Lexicography, 13 (4): 249-263.Manning C. D., 2003.
Probabilistic syntax.
In R. Bod, J.Hay, S. Jannedy (eds), Probabilistic Linguistics,MIT Press, Cambridge MA: 289-341.MacWhinney, B., 2000.
The CHILDES project: Toolsfor analyzing talk.
Third Edition.
Mahwah, NJ: La w-rence Erlbaum AssociatesMacWhinney B., Bates E., Kliegl R., 1984.
Cue validityand sentence interpretation in English, German, andItalian.
Journal of Verbal Learning and Verbal Be-havior, 23: 127-150.MacWhinney B., 2004.
A unified model of languageacquisition.
In J. Kroll & A.
De Groot (eds.
), Hand-book of bilingualism: Psycholinguistic approaches,Oxford University Press, Oxford.Matthews D., Lieven E., Theakston A., Tomasello M.,in press, The role of frequency in the acquisition ofEnglish word order, Cognitive Development.Miyao Y., Tsujii J., 2002.
Maximum entropy estimationfor feature forests.
Proc.
HLT2002.Montemagni S. et al 2003.
Building the Italian syntac-tic-semantic treebank.
In Abeill?
A.
(ed.)
Treebanks.Building and Using Parsed Corpora , Kluwer,Dordrecht: 189-210.Ninio, A.
1999.
Pathbreaking verbs in syntactic devel-opment and the question of prototypical transitivity.Journal of Child Language, 26: 619- 653.?vrelid L., 2004.
Disambiguation of syntactic functionsin Norwegian: modeling variation in word order in-terpretations conditioned by animacy and definite-ness.
Proceedings of the 20th ScandinavianConference of Linguistics, Helsinki.Quochi, V., (in preparation).
A constructional analysisof parental speech: The role of frequency and predic-tion in language acquisition, evidence from Italian.Ratnaparkhi A., 1998.
Maximum Entropy Models forNatural Language Ambiguity Resolution.
Ph.D. Dis-sertation, University of Pennsylvania.Seidenberg M. S., MacDonald M. C. 1999.
A probabil-istic constraints approach to language acquisition andprocessing.
Cognitive Science  23(4): 569-588.Stemberger, J., MacWhinney, B.
1986.
Frequency andthe lexical storage of regularly inflected forms.Memory and Cognition, 14:17-26.Tomasello M., 2000.
Do young children have adult syn-tactic competence?
Cognition , 74: 209-253.81
