Word Order  Acqu is i t ion  f rom CorporaKiyotaka Uchimoto I, Masaki Murata t, Qing Ma*,Satoshi Sekine*, and Hitoshi Isahara ttCommunications Research LaboratoryMinistry of Posts and Telecommunications588-2, Iwaoka, Iwaoka-cho, Nishi-kuKobe, Hyogo, 651-2492, Japan\[uchimoto ,murata, qma, isahara\] @crl.
go.
jp*New York University715 Broadway, 7th floorNew York, NY 10003, USAsekYne~cs, nyu.
eduAbstractIn this paper we describe a method of acquiring wordorder fl'om corpora.
Word order is defined as the or-der of modifiers, or the order of phrasal milts called'bunsetsu' which depend on the stone modifiee.
Themethod uses a model which automatically discoverswhat the tendency of the word order in Japanese isby using various kinds of information in and aroundthe target bunsetsus.
This model shows us to whatextent each piece of information contributes to de-ciding the word order mid which word order tends tobe selected when several kinds of information con-flict.
The contribution rate of each piece of informa-tion in deciding word order is eiIiciently learned by amodel within a maximum entropy framework.
Theperformance of this traiimd model can be ewfluatedby checking how many instances of word order st-letted by the model agree with those in the originaltext.
In this paper, we show t, hat even a raw cor-pits that has not been tagged can be used to trainthe model, if it is first analyzed by a parser.
Thisis possible because the word order of the text in thecorpus is correct.1 IntroductionAlthough it is said tha~ word order is free inJapanese, linguistic research shows that there artcertain word order tendencies - -  adverbs of time, forexample, tend to t)recede subjects, mM bunsetsus ina sentence that are modified by a long modifier tendto precede other bunsetsus in the sentence.
Knowl-edge of these word order tendencies would be usefulin analyzing and generating sentences.Ii1 this paper we define word order as the order ofnrodifiers, or the order of bunsetsns wlfich depend onthe same modifiee.
There arc several elements whichcontribute to deciding the word order, and they aresummarized by Saeki (Saeki, 1.998) as basic condi-tions that govern word order.
When interpretingthese conditions according to our definition, we era:summarize them ,~ tbllows.Component la l  eondit lons?
A bunsetsu having a deep dependency tendsto precede a bunsetsu having a shallow depen-dency.When there is a long distance between amodifierand its modifiee, the modifier is defined as a bun-setsu having a deep dependency.
For example,the usual word order of modifiers in Japaneseis tlm following: a bunsetsu which contains aninterjection, a bunsetsu which contains an ad-verb of time, a bunsetsu which contains a sub-ject, and a bunsetsu which contains an object.Here, the bunsetsu containing an adverb of timeis defined as a bunsetsu having deeper depen-dency than the one containing a subject.
Wecall the concept representing the distance be-tween a modifier and its modifiee the depth ofdependency.A bunsetsu having wide dependency tends toprecede a bunsetsu having narrow dependency.A bunsetsu having wide dependency is definedas a bunsetsu which does not rigidly restrict itsmodifiee.
For example, the bunsetsu "~btqlo_c(to Tokyo)" often depends on a bunsetsu whicllcontains a verb of motion such as "ihu (go)"while the bunsetsu "watashi_.qa (I)" can dependon a bunsetsu which contains any kind of verb.Here, the bunsetsu "watashi_ga (I)" is defined asa bunsetsu having wider dependency than 1;11otmnsetsu ':Tok~./o_c (to Tokyo)."
We call theconcept of how rigidly a modifier restricts itsmodifiee the width of dependency.Syntact i c  condit ions?
A bunsetsu modified by a long inodifier ton(Is toprecede a bunsetsu modified by a short lnodifier.A long modifier is a long clause, or a clause thatcontains many bunsetsus.?
A bunsel, su containing a reference pronoun tendsto precede other bunsetsus in the sentence.?
A bunsetsu containing a repetition word tendsto precede other bunsetsus in the sentence.A repetition word is a word referring to a wordin a preceding sentence.
For example, Taromid Hanako in the following text are repetitionwords.
"Taro and Hanako love each other.
Tarois a civil servant and Hanako is a doctor."?
A bunsetsu containing the case marker "wa"tends to precede other bunsetsus in the sentence.A mnnber of studies have tried to discover the rela-tionship between these conditions and word order in871Japanese.
Tokunaga and Tanalca proposed a modelfor estimating JaI)anese word order based on a dic-tionary.
They focused on the width of dependency(Tokunaga and Tanal~a, 1991).
Under their model,however, word order is restricted to the order of caseelements of verbs, and it is pointed out that themodel can deal with only the obligatory case andit cmmot deal with contextual information (Saeki,1998).
An N-gram model fbr detecting word orderhas also been proposed by Maruyama (Maruyama,1994), but under this model word order is defined asthe order of morpheines in a sentence.
The problemsetting of Maruyama's study thus differed fl'om ours,and the conditions listed above were not taken intoaccount in that study.
As for estimating word or-der in English, a statistical model has been proposedby Shaw and Hatzivassiloglou (Shaw and Hatzivas-siloglou, 1999).
Under their model, however, wordorder is restricted to the order of premodifiers ormodifiers depending on nouns, and the model doesnot simultaneously take into account many elementsthat contribute to determining word order.
It wouldbe difficult to apply the model to estimating wordorder in Japanese when considering the many condi-tions as listed above.In this paper, we propose a method for acquiringfrom corpora the relationship between the conditionsitemized above and word order in Japanese.
Themethod uses a model which automatically discoverswhat the tendency of the word order in Japanese isby using various kinds of information in and aroundthe target bunsetsus.
This model shows us to whatextent each piece of information contributes to decid-ing the word order and which word order tends to beselected when several kinds of information conflict.The contribution rate of each piece of information indeciding word order is efficiently learned by a modelwithin a maximum entrot)y (M.E.)
framework.
Theperformance of the trained model can be evaluatedaccording to how many instances of word order se-lected by the model agree with those in the originaltext.
Because the word order of the text in the corpusis correct, the model can be trained using a raw co>pus instead of a tagged corpus, if it is first analyzedby a parser.
In this paper, we show experimental re-sults demonstrating that this is indeed possible evenwhen the parser is only 90% accurate.This work is a part of the corpus based text gen-eration.
A whole sentence can be generated in thenatural order by using the trained model, given de-pendencies between bunsetsus.
It could be helpfulfor several applications uch as refinement supportand text generation in machine translation.2 Word  Order  Acqu is i t ion  andEs t imat ion2.1 Word  Order  Mode lThis section describes a model which estimates thelikelihood of the appropriate word order.
We callthis model a word order model, and we implementedit within an M.E.
framework.Given tokenization of a test corpus, the problemof word order estimation in Japanese can be reducedto the problem of assigning one of two tags to eachrelationship between two modifiers.
A relationshipcould be tagged with "1" to indicate that the orderof the two modifiers is appropriate, or with "0" to in-dicate that it is not.
Ordering all modifiers so as toassign the tag "1" to all relationshit)s indicates thatall modifiers art  in the appropriate word order.
Thetwo tags form the space of "futures" in the M.E.formulation of our estimation problem of word or-der between two modifiers.
The M.E.
model, as wellas other similar models allows the computation ofP(flh) for any f in the space of possible futures, F,and for every h in the space of possible histories, H.A "history" in maximum entropy is all of the condi-tioning data that enable us to make a decision in thespace of futures.
In the estimation problem of wordorder, we could reformulate this in terms of findingthe probability of f associated with the relationshipat index t in the test cortms as:P(flht) = P( f l  hfformation derivablefrom the test corpusrelated to relationship t)The computation of P(flh) in any M.E.
models isdependent on a set of "features" which should behdpful in making a prediction about the flmlre.
Likemost current M.E.
models in computational linguis-tics, our model is restricted to features which arebinary functions of the history and future.
For in-stance, one of our features is1.
: if has(h,x) = true,x = "Mdfl'l - Head-g(h,f) = POS(Major) : verb" (1)&f= l0 : otherwise.Here "has(h,x)" is a binary flmction which returnstrue if the history h has feature z.
We focus on theattributes of a bunsetsu itself and on the featuresoccurring between bunsetsus.Given a set of features and some training data,the maximum entropy estimation process produces amodel ill which every feature .qi has associated with ita parameter ai.
This allows us to compute the con-ditional probability as follows (Berger et al, 1996):ag~ (h .f)P( / Ih ) -  1L ' (2)Z (h)ct i .
(3)Y iThe maximum entropy estimation technique guaran-tees that for every feature gi, the expected value ofgi according to the M.E.
model will equal the empir-ical expectation of gi in the training corpus.
In otherwords:P(h,/).
Mh, f)h,f= (4)h /Here /5 is an empirical probability and l~ Ie  is the872Table l: Example of estimating the probabilities of word orders.? )
I"I~{H (yesterday) / ?=x~ (tennis) / ~(f~l~l$ (Taro) / bLo ( l ) I t~yed. )
"  /I~,,'~<~,~ x llail.-)'nxt, x 1.7:-.x,~j~m ::: 0.6 x 0.8 x 0.3 0.144 ) \["NI!Iii:k (Taro) )  I~1!11 (yesterday) / ?
:LX ~ (tennis) / bt:?
(played.)"
IP:mi~ ~H x I~*H,)-: x,,< x t?r,l~.?
::x,: := 0.4 x 0.8 ?
0.7 0.224\[")kl'!l~l:~ (Taro) / -Y : :x '~ (tennis) / 'a~H (yesterday) / b?
:o (plowed.)"
I/)~:,,~ll x tg.=.x'~.,~H x P,k~a*~L-)::?~ =: 0.4 x 0.2 x 0 .7  0.05(i1"9:-':;?
~ (tennis) / II~{H (yesl.erd;ty) / >kfl\[~l~: ( l'&l'O) / \[,~:.?
(played.)"
\[13*lll,2<I,u:l. x l: ':.xt, jall x l!).
:.x.~,.j<r~m :: 0.6 x 0.2 x 0.3 0.036\["5:=x ~ (tennis) / )kf~lll:t (Taro) / I~I~Ft (yesterday) / blz0 (played.)"
~ ?
H  x l~;.
:.xr, ~,~ZI?
x l?y:.x~l~,~ =: 0.4 x 0.2 x 0.3 0.024prol)ability assigned by the M.E.
model.We detine a word order model as a model whichlearns the at)l)ropriate order of each pair of nlodifierswhich depend on the same modifiee.
'l'his model isderived from Eq.
(2) as follows?
Assmne that thereare two bunsetsus 231 and 23~ which depend on thebuusetsu B and that  It is the information derivablefrom the test corpus.
\]?lie probability that "B\] B2" isthe at)propriate order is given by the following equa-tion:\]iik: :1 ffi( l  ,b) (~'i ,iwhere .qi(1 < i < k) is a fl,atm'e and "1" indicatesthat the order is at)propriate.
The terms cq,i and(~0,i are estimated fl'oln a eorl)us which is nlorpho-logically and syntactically analyzed.
When there arethree or more b\]msetsus that det)end on tit('.
S}tlne?
moditiee, the probability is estimated as follows: I or~'t bunsetsus 231, 232, .
.
.
,  23n which depend on thebmtsetsu B and for the information h derivaMe fromthe test corpus, the prot)ability t;hat "23\] 23~ .
.
.
23,"is the at)propriate order, or P( l lh) ,  is represented assthe probability that every two bunsetsus "Bi ~i-Fj(1 _< i < n - 1,1 < j < 'n - i)" are the appropri~ate.
order, or  P({14~i, i_ l .
j  = l \ [ l<  i <.
n - -  1, l :< j <n - - i} lh ) ,  ilere "I4Li+j -- l" represents that ".l~i23i-Fj" is the appropriate order.
Let us assume thatevery 14Q,~:+j is independent each other.
Then 1)(1 Ih,)is derived as follows:2 ' ( l ib )  = \]1\].
i , .
-  \ ] ,1 < .i _< , , , -  i}lh)n- - |  n - ii -- I  j --1= l l  1I j),i= l  j= lwhere \[Si,i+ j is the information derivable when fb-cusing on the bunsetsu 13 m~d its modifiers 13i andBi+j.For example, in the sentence "I~ U (kinou, yester-day) / :kflll ~ (Taro_wa, Taro) / -P ~ x ~ (tcnnis_wo,tennis) / b t:o (sita., l)layed.
)," where a "/" repre-sents a bunsetsu boundary, there are three bunset-sus that depend on the verb "b  ~: (sita)."
We traina word order inodel under the assmnl)tion that theorders of three t)airs of modifiers -"I~l U" and "~f$1.~," "Net\] " and "?
7-:7, ~ ," and ":kl~l*l~" and "5:m :7, ~"  .... are al)ttropriate.
We use various ldnds ofintormation in and around the target bunsetsus asfeatures.
For example, the information or the featurethat a noun of time i)recedes a t)rot)er noun is deriv-able fl'om the order "IP} H (yesterday) / Y;fll~ I~ (Taro)/ b 1=o (pl~\yed.
)," and the feature that a case fol-lowed by a case marker "w?'
precedes a case followedby a caqe marker "wo" is derivable from the order ":~fll~ It.
( Taro_wa, Taro) / ?
~ 7.
?k (tennis_wo, tennis) /b 2C_o (sita., t)layed.).
"2.2 Word Order  Es t imat ionThis section describes the algorithm of estimatingthe word order by using a trained word order model.The word order estimation is defined as decidingthe order of ntoditiers or bunsetsus which dependon the same modifiee.
The input of this task con-sists of modifiers and informat, ion necessary to knowwhether or not features are found.
The output isthe order of the inodifiers.
We assume that lexicalselection in each bunsetsu is already done and alldel)endencies in a sentence are found.
The informa-tion necessary to know whether or not features arefound is morphological, syntactic, semmltic, and ('on-textual information, and the locations of bunsetsubonndaries.
The features used in our ext)erimentsare described in Section 3.Word order is estimated in the following steps.Procedures1.
All possible orders of modifiers are found.2.
For each, the probability that it is apt)ropriateis estimated by a word order model, or Eq.
(6).3.
The order with the highest probability of 1)eingapprol)riate is selected.l)br example, given the sentence "1~ U (kinou,yesterday) /:kfilIl~ (Taro_wa, Taro) /?
: :x  ~ (tcn-nis_wo, temfis) / b t:o (sita., played.
)," tim modi-tiers of a verb "b  ?
:_ (played)" are three tmnsetsus,"l~ U (yesterday)," :k~/i ~ (Taro)," "?
= x ~ (ten-nis)."
Their apt)ropriate order is estimated in thefollowing steps.1.
The probabilities that the orders of the threepairs of modifiers "N- LI " and ":is: BII l~ ," "I~U" and "?~:7 ,~,"  and "~fllIl?"
and "?c .x  ~" are appropriate are estimated.
As-sume, for example, ~-H ,;k~l~ta, PrI~ It ,?
:-~.
~, andP;kfzlIla,~ ca  ~ are respectively 0.6, 0.8, and 0.7.2.
As shown in Table 1, probabilities are estimatedfor all six possible orders.
The order "I~ U / :kfill IS / -7- ~- y.
~ / b \]Co ," which has the highestprobability, is selected as the most apt)ropriateorder.2.3 Per fo rmance  Eva luat ionThe pcrformancc of a word order model can be eval-uated in the following way.
First, extract from atest corpus bunsetsus having two or more modifiers.Then, using those 1)unsetsus and their modifiers as873Data~Jtlnsetsll lJtlnsetstl nHIYiber Of babelnlllllber modifier0 1 P1 52 33 44 5 P5Table 2: Example of modifiers extracted fl'om a corpus.Modifiers (Bunsetsu number)Strings in a bunsetsu>kflli ~ ( Taro_to, Taro and)~Y'{a ( lIanako_to, llanako)-Y- = x q~ (tennis_no, tennis)~lc .
(sial_hi, tournament)lll'C, (dete,, participate,)~@ b t=, (yusyo_sita., won.
)Moditiers whose modiliee is the bunsetsuin the left column.~,~ a (0)?=x0~ (2)~a  (0) ~-r-~a 0) '~ :  (a)~e  (o) ~?-~* (1) If'~ (4)input, estimate the orders of the modifiers as de-scribed in Section 2.2.
The percentage of the modi-flees whose modifiers' word order agrees with that inthe original text then gives what we call the agree-ment rate.
It is a measure of how close the wordorder estimated by the model is to the actual wordorder in the training corpus.We use the following two measurements to calcu-late the agreement rate.Pa i r  of  modi f ie rs  The first measurement is thepercentage of the pairs of modifiers whose wordorder agrees with that in the test corpus.
Forexmnple, given the sentence in a test corpus "Nkl (kinou, yesterday) / ~t I la  (Taro_wa, Taro)/ -7- = 2` ~2 (tennis_wo, tennis) / t. ~:o (sita.,played.
)," if the word order estimated by themodel is "~ H (yesterday) / -7" -- 2.
~ (tennis) /~1~ ~:~ (Taro) / b too (played.
)," then the or-ders of the pairs of modifiers in the original sen-tence are "N H / ;k~l~  ," "15 H / -7- =- :7, ~ ," and"~lit:~ / ~--2` ~ ," and those in the estimatedword order are "~H / -~---2`~," "~H /1~1~ la~ ," and "Y- = 2` ~ / %:t~ll lak ."
The agreementrate is 67% (2/3) because two of the three ordersare the same as those in the original sentence.Complete  agreement  The second measurement isthe percentage of the modifiees whose modifiers'word order agrees with that in the test corpus.3 Exper iments  and  D iscuss ionIn our experiment, we used the Kyoto University textcorpus (Version 2) (Kurohashi mid Nagao, 1997), atagged corpus of the Mainichi newspaper.
For train-ing, we used 17,562 sentences from newspaper arti-cles appearing in 1995, from January 1st to Jmmary8th and from Jmmary 10th to June 9th.
For testing,we used 2,394 sentences fl'om articles appearing onJanuary 9th and from June 10th to June 30th.3.1 Def in i t ion of  Word  Order  in  a CorpusIn the Kyoto University corpus, each bunsetsu hasonly one modifiee.
When a bunsetsu Bm depends ona bunsetsu Bd and there is a bunsetsu /3p that de-pends on and is coordinate with \])d, Bp has not onlythe information that its modifiee is \]~d but also a la-bel indicating a coordination or the information thatit is coordinate with B d. This information indirectlyshows that the bunsetsu Bm can depend on both \]3pand Bd.
In this case, we consider Bm a modifier ofboth Bv and B d.Under this condition, modifiers of a bunsetsu Bare identified in the following steps.1.
Bunsetsus that depend on a bunsetsu B are clas-sifted as modifiers of B.2.
When B has a label indicating a coordination,bunsetsus that are to tile left of 13 and depend onthe same modifiee as B are classified as modifiersof B.3.
Bunsetsus that depend on a modifier of B andhave a label indicating a coordination are clas-sifted as modifiers of B.
The third step is re-peated.When the above procedure is completed, all bunset-sus that coordinate with each other are identified asmodifiers which depend oi1 the same nmdifiee.
Forexample, from the data listed on the left side of To-ble 2, the modifiers listed in the right-hand columnare identified for each bunsetsu.
"Nt~I; ~ (Taro_to,Taro and)," "?~g-~ IS (Hanako_to, Hanako)," "ql "(,(dete,, participate,)" are all identified as modifierswhich depend on the same modifiee "~ b 7=?
(yusyo_sita., won.).
"3.2 Exper imenta l  Resu l tsThe features used in our experiment are listed in Ta-bles 3 and 4.
Each feature consists of a type anda value.
The features consist basically of some at-tributes of the bunsetsu itself, and syntactic and con-textual information.
We call the features listed inTables 3 'basic features.'
We selected them man-ually so that they reflect the basic conditions gov-erning word order that were sunmmrized by Saeki(Saeki, 1998).
The features in Table 4 are combina-tions of basic features ('combined features') and werealso selected manually.
They are represented by thenmne of the target bunsetsu plus the feature type ofthe basic features.
The total number of features wasabout 190,000, and 51,590 of them were observed inthe training cortms three or more times.
These werethe ones we used in our experiment.The following terms are used in these tables:Mdf r l ,  Mdf r2 ,  Mdfe:  The word order model de-scribed in Section 2.1 estimates the probabilitythat modifiers are in the appropriate order asthe product of the probabilities of all pairs ofmodifiers.
When estimating the probability tbreach pair of modifiers, the model assmnes thatthe two modifiers are in the appropriate order.Here we call the left modifier Mdfrl, the rightmodifier Mdfr2, and their modifiee Mdfe.Head:  the rightmost word in a bunsetsu other thanthose whose major pro't-of-speech I category is1Part-of-speech categories follow those of JUMAN (Kuro-hashi and Nagao, 1998).874Table 3: Basic features.Bas ic  featuresFeature values (Number of type) Feature type tegors\] Target1)tmsetsus1 Mdfrl, Mdfr2,Mdfe2 Mdfrl, Mdfr2,Mdfe3 Mdfrl, Mdfr2,Mdfe4 Mdfrl, Mdh'2,Mdfem m\[\]ead-POS(Major)\[tead-POS(Minor)\[lead-hff(Major)\[Iead-Inf(Minor)\[Iead-SemFeat(110)\[1ead-SemFeat(111 )\[Iead-SemFeat(433)5 Mdfrl, Mdff2, rype(String)Mdfe rype(Major)type(Minor)6 Mdfrl, Mdfr2, lOSIll l(String)Mdfe lOSIIIl(Minor)lOSII12(String)lOSUI2(Minor)7 Mdfrl, Mdfr2, PeriodMdfe8 Mdfrl, Mdfr2 Numl)erOfMdfrsMdfe NumberOfMdfrs9 Mdfrl, Mdfr2,Mdfe10 Mdfl'l, Mdfr2Coordination(Total : 90)Mdfrl-MdfrType-ll )to-Mdfr2-TypcMdfl'2-MdfrTypeql )to-Mdfl-I -q'ypeMdfi'l -Md frType-lDto- Md fr2-MdfrType11 Mdfrl, Mdfr2, Rel)etition-llead-l,exMdfe Repetition-Md fr-lleadq,ex12 Mdfrl, Mdfr2 ReferencePronouni~efereneePronou  (String)',.%066)~u (verb), s~u (adjective), ~,'~ (noun) .
.
.
.
(11)~'~:~ (common oun), m'~ (quantifier) .
.
.
.
(24)~1~ (vowel verb) .
.
.
.
(30)'.
'~ (stem), t~*~ (fandamental form) .
.
.
.
(60)rrue (1)true (1)true (1):~, :a ,  <-u<, t:u, ~, ~=, t .
.
.
.
(7:3)uJ:~ (post-positional particle), .
.
.
(43)?,~JJ'~ (e~use marker), ~*$ (imperative form) .
.
.
(102)'~',5, ~<', a~, ,., l,~.
.
.
.
.
(63))ill\], ~$m (ease marker), .
.
.
(5)~, ~, *, ~,*,, ... ((~3)~,~;~1 (ease marker) .
.
.
.
(41\[nil\], \[exist\] (2)A(0), B(1), C(2), 1)(3 or more) (4)A(2), B(:3), C(4 or more) (3)P(Coordim~te), A(Apposition), I)(otherwise) (3)\]'rue, False (2)rrue, False (2)true, False (2)86.65% 73.87%\[--0.79%) (--1.54%)87.07% 75.03%',--O.37%) (--0.38%)87.39% 75.20%',-0.05%) (-0.m%)87.21% 75.20%\[--0.23%) (--0.21%)84.78% 70.03%(-2.66%) (-5.38%)87.32% 75.14%(-0.12% (--0.27%)87.39% 7 ~(-0.05%) (+0.13%)87.14% 74.86%(-0.30%) (-0.55%)87.40% 70.30?./o(-0.04%) (-0.0~%)8~.2~% 73.61%(--1.18% (--1.80%)87.34% 75.09%(--0.10%) (--0.
'32%)\[nil\],\[exist\] (2) 87.31% 75.id%\[nil\], \[exist\] (2) (--0.13%) (--0.27%)\[nil\], \[exist\] (2) 87.27% 75.12%:~, :a,  :~ .~,~=.~, ,  ~.
.
.
.
.
(42) (--0.17%) (--0.29%)'%} @ (special marks)," "112 N (1)ost-posi~ioualparticles)," or "}~N~?
(suffixes).
"Head-Lex :  the fllndalnental forth (unintlectedforln) of the head word.
Only words with a fre-quency of tlve or more are used.Head- In f :  the inflection type of a head.SemFeat :  We use the upper third layers of bunrui.qoihyou (NLl/I(National Language Research In-stitute), 19641 as semantic features.
Bunrui goi-hyou is a Japanese thesaurus that has a treestructure and consists of seven layers.
The treehas words in its leaves, and each word has a fig-ure indicating its category number.
For exam-ple, the figure in parenthesis of a feature "Head-SemFeat( l l0)" in Table 3 shows the upper threedigits of the category number of the head wordor the ancestor node of the head word in thethird layer in the tree.Type:  the rightmost word other than those whosemajor part-of-speech category is "~@ (specialmarks)."
If the major category of the wordis neither "NJN (post-positional particles)" nor"}~/~'~ (suffixes)," and the word is inflectable, 2then the type is represented by the inflectiontype.JOSHI1 ,  JOSHI2 : JOSHI1  is the rightmost post-positional particle in the bunsetsu.
And if thereare two or more post-positional particles in thebunsetsu, JOSHI2 is the second-rightmost post-positiolml particle.NmnberOfMdf rs :  number of modifiers.2The inflection types follow those of J UMAN.Mdfr l -Mdf rType ,  Mdf r2 -Mdf rType:  Types oftile modifiers of Mdfi'l and Mdfr2.X- IDto -Y :  X is identical to Y.Repet i t ion -Head-Lex :  a ret)etition word allpear-ing ill a preceding senteuce.Re ferencePronour l :  a reference pronoun appear-ing in the target bunsetsu or ill its modifiers.Categories 1 to 6 ill Table 3 reI)resent attributesin a bunsetsu, categories 7 to 10 represent syntac-tic information, and categories 11 and 12 representcontextual information.The results of our experiment are listed in Table 5.The first line shows tlle agreement rate when we esti-mated word order for 5,278 bunsetsus that have twoor more modifiers and were extracted from 2,394 sen-tences al)pearing on Jmmary 9th and from June 10thto June 301tl.
\Ve used bunsetsu boundary informa-tion and syntactic and contextual information whichwere derivable froln the test corpus and related tothe input bunsetsus.
As syntactic ilffOrlnation weused dependency inforlnation, coordinate structure,and information on whether the target bunsetsu is atthe eM of a sentence.
As contextual information weused the preceding sentence.
The values in the rowlabeled Baseline1 in Table 5 are the agreement ratesobtained when every order of all pairs of modifierswas selected randolnly.
And values in the B&seline2row are the agreement rates obtained when we usedthe following equation instead of Eq.
(5):freq(w12)PMu'(llh) = freq(w12) + frcq(w21)" (7)875Table 4: Combined features.Accuracy withoutthe featurePair of Completemodifiers :tgreement87.23% 74.65%(-0.21%) (-0.76%)Combined  features- -  Twin tbatures(Mdfr 1-Type, Mdfr2-Type) ,(Mdfr 1-Type,  Mdfe- I Iead-Lex) ,(Mdfr  1-Type,  Md fe- t tead-POS) ,(Mdfr  1-Type,  Mdfr  1-Coordlnrtt ion),(Mdfr  1-Type, Mdf r2 -Mdf rType- IDto -Md fr1-2"ypc),(MdfrE-Type,  Mdfe-Head-Lex) ,(Mdfrg-Type,  Mdfe-Head-POS) ,(Mdfr2-Type,  Mdfr2-Ooord inat ion) ,(Mdfr2-Type,  Md fr 1-MdfrType- lDto-Mdfr2-Type) ,Mdfr  1-Head-Lex,  Mdfe-Per iod) ,Mdfr  1 -nead-POS,  Mdfe-Per lod) ,Mdfr  1-1tead-POS, Mdfr  1-Repet l t lon-Head- I ,ex) ,Mdfr2- I tead-Lex,  Mdfe-Pet lod) ,Mdfr2-Ite,~d-POS, Mdfe-Per lod) ,Mdf r2 - I Iead-POS,  Mdfr2- I tepet l t lon- I lead-Lex)' t~iplet  tca tures  87.22% i 74.86%Mdfr l -Wype,  Mdfr2-Type,  Mdfe- l lead-Lex) ,  (--0.220./0) (--0.55%)Mdfr l -Type ,  Mdf r2 -Type,  Mdfe-Head-POS) ,Mdf r l -Type ,  Mdf r l -Coord inat lon ,  Mdfe-Type) ,MdfrE-Type,  Mdfr2-Coord lnat lon ,  Mdfe-Type) ,Mdf r l - JOSHI1 ,  Mdf r l - JOSHI2 ,  Mdfe- I Iead-Lex),Mdf r l -aOSHl l ,  Mdf r l - JOSHI2 ,  Mdfe- I Iead-POS) ,Mdf r2 - JOSHI1 ,  Mdfr2- JOSI I I2 ,  Mdfe-Head-Lex),Mdf r2-aOSH\[1 ,  Mdfr2-JOSl~12, Mdfo- l lead-POS)All of  above  combined  features  85.79% 77.67%~--1.65%) (--3.74%)Table 5: Results of agreement rates.Agreement  ra tePair of modifiers Coml)lete agreementOur method 87.44%(72,367/14,137) 75.41% (3,980/5,278)Baseline1 48.96% (6,921/14,137) 35.10% (7,747/5,278)Baseline2 49.20% (6,956/14,137) 53.84% (1,786/5,278)IIere we assume that B1 and \]32 are modifiers, theirmodifiee is B, the word types of B1 and \]32 are re-spectively Wl and we.
The values frcq(wr2) andfrcq(w.27 ) then respectively represent the fl'equencieswith which w7 and w,2 appeared in the order "WT, we,mid w" and "w2, WT, and w" in Malnichi newspaperarticles fl'om 1991 to 1997. a Equation (7) meansthat given the sentence "~t~lt I:t (Taro_wa) / ~- -- ~,(tennis_wo) / b ~-:o (sita.
)," one of two possibili-ties, "1$ (wa) / ~ (wo) / t, ~:o (sita.)"
and "#c (wo)/ tS (wa) / b ~:o (sita.
)," which has the higher fre-quency, is selected.3.3 Features  and Agreement  RateThis section describes how much each feature set con-tributes to improving the agreement rate.The values listed in the rightmost columns in Ta-bles 3 and 4 shows the performance of the word or-der estimation without each feature set.
The valuesin parentheses are the percentage of improvement ordegradation to the formal experiment.
In the exper-iments, when a basic feature was deleted, the com-bined features that included the basic feature werealso deleted.
The most useful feature is the type of3When wl and w2 were the same word, we used the headwords in Bt  and 132 as Wl and w2.
When one offreq(wt2) andfreq(w21) was zero and the other was five or more, we usedthe f lequencies when they appeared in the order "Wl ws" and"w2 wt,"  respectively~ instead of frcq(wi2) al,d freq(wsl).When both freq(wl.2) and freq(w27) were zero, we insteadused random figures between 0 and t.bunsetsu, which basically signifies the case marker orinflection type.
This result is close to our expecta-tions.We selected features that, according to linguisticstudies, as mudl  as possible reflect the basic condi-tions governing word order.
The rightmost columnin Tables 3 and 4 shows the extent o which each con-dition contributes to improving the agreement rate.However, each category of features might be rougherthan that which is linguistically interesting.
For ex-ample, all case markers uch as "wa" and "wo" wereclassified into the same category, and were deletedtogether in the experiment when single categorieswere removed.
An experiment that considers eachof these markers eparately would help us verify theimportance of these markers separately.
If we findnew features in future linguistic research on word or-der, the experiments lacking each feature separatelywould help us verify their importance in the samemanner .3.4 Tra in ing  Corpus  and Agreement  RateThe agreement rates for the training corpus and thetest corpus are shown in Figure 1 as a function ofthe amount of training data (ntunber of sentences).The agreement rates in the "pair of modifiers" and'?
!19~ .
.
.
.
.
%:I:: 'i, ~ .
.
.
.
.
.
.
.
.
~':z~-,~',:: : : i=?I 90 90!
'S5 ~5 i ~ -E75 ~ 757O 7O65 .
.
.
.
.
.
.
.
(,5 0 .
.
.
.0 2000 400{) 6000 80O0 Io(mO 120{}0 14O00 16000 18000 2000 4000 6,300 ~000 IODO0 12(;00 14000 16O00 la00{1\] 11o NtllObor o~ Snnloncos \]o 111o Traif l in9 Data l lm Number ol Sentences in l lm T f 4dlli11U \[)alaFigure 1: Relationship between tile amount of trainingdata and the agreement rate.
"Complete agreement" measurements were respec-tiw~ly 82.54% and 68.40%.
These values were ob-tained with very small training sets (250 sentences).These rates m'e considerably higher than those ofthe baselines, indicating that word order in Japanesecan be acquired fl'om newspaper articles even with asmall training set.With 17,562 training sentences, the agreemenl,rate in the "Complete agreement" measurement was75.41%.
We randomly selected and analyzed 100modifiees from 1,298 modifiees whose modifiers' wordorder did not agree with those in the original text.We found that 48 of them were in a natural orderand 52 of them were in an unnatural order.
Theformer result shows that the word order was rela-tively fl'ee and several orders were acceptable.
Thelatter result shows that the word order acquisitionwas not sufficient.
To complete the acquisition weneed more training corpora and features which takeinto account different information than that m Ta-bles 3 mid 4.
We found many idiomatic expres-876sions in the uimatural word order results, such as "~ffl\[~il~:5~ (houchi-kokka_ga,  country under the ruleof law) / \ [ l} l~  (kiitc, to listen) /~#t~ (alcireru,to disgust), ~rj ~ b ?= a ~ (souan-s~,ta-no_ga, orlgl-,ration) / ~ *o ~- *o co (somosomo-no, at all) / ~t~  U(hg~'ima~'4 the beginning)," and ""~ l~ (g#-~4 taste) /~'~B (seikon, one's heart and soul) /g~ 6 (homcru,to trot somethil,g into soinething)."
We think thatthe apt)ropriate word order for these idiomatic ex-pressions could be acquired if we had more trainingdata.
We also found several coordinate structures inthe Ulnlatural word order results, suggesting that weshould survey linguistic studies on coordinate struc-tures and try to find efllcient features for acquiringword order from coordinate structures.We (lid not use the results of semantic and con-textual analyses as input because corpora with se-mantic and contextuM tags were not available.
Ifsuch corpora were available, we could more et\[icientlyuse features dealing with seinantic features, referencepronouns, and repetition words.
We plan to makecorpora with semantic and contextual tags and usethese tags as input.3.5 Acqu is i t ion  f rom a Raw CorpusIn this section, we show that a raw cortms instead ofa tagged corpus can be used to train the lnodel, if itis first analyzed by a parser.
We used the lnorl)holog-ical analyzer JUMAN and a tmrser KNP (Kurohashi,11198) which is based on a det)endency grainlnar,it, order to extract iuforumtion from a raw corpusfor detecting whether or not each feature is found.
'l?tm accuracy of JUMAN for detecting inorphologi-cal boundaries and part-of-speech tags is about 98%,and the parsecs dependency accuracy is about 90%.These results were obtained from analyzing Mainichinewspaper articles.We used 217,562 sentences for training.
Whenthese sel~t, ences were all extracted from a raw corlms ,the agreement rate was 87.64% for "pair of modifiers"and was 75.77% for "Colnplete agreement."
Whenthe 217,562 training sentences were sentences fl'olnthe tagged cortms (17,562 sentences) used in our for-real exl)eriment aInl froln a raw cortms, the agree-" e S :~ ment rate for "pair of lno(hfi.r, was 87.66% andfor "Complete agreement" was 75.88%.
These rateswere about 0.5% higher than those obtained when weused only sentences from a tagged corlms.
Thus, wecan acquire word order by adding inforlnation frolna rmv corpus even if we do not have a large taggedcorpus.
The results also indicate that the parser ac-curacy is not so significant for word order acquisitionand that an accuracy of about 90% is sufficient.4 Conc lus ionThis paper described a method of acquiring word or-der froln corpora.
We defined word order as the orderof lnodifiers which depend on tile same lnodifiee.
Thelnethod uses a model which estimates the likelihoodof the apt)ropriate word order.
The lnodel automat-ically discovers what the tendency of the word orderin Japanese is by nsing various ldnds of informationin and arouud the target bunsetsus plus syntacticand contextual inforlnation.
The contribution rateof each piece of inforination in deciding word orderis efficiently learned by a model implemented withinan ),,I.E.
framework.
Comparing results of experi-ments controlling for each piece of information, wefound that the type of inforinatiou having the great~est influence was the case marker or inflection type ina bunsetsu.
Analyzing the relationship between theamount of training data and the agreement rate, wefimnd that word order could be acquired even witha small set of training data.
We also folmd that araw cortms as well as a tagged cortms can be used totrain the model, if it is first, analyzed by a parser.
Theagreement rate was 75.41% for the Kyoto Universitycorpus.
We analyzed the lnodifiees whose modifiers'word order did not agree with that in the originaltext, and folmd that 48% of theln were in a naturalorder.
This shows that, in umny cases, word orderin Japanese is relatively free and several orders areacceptable.The text we used were lmwspaper articles, whichtend to have a standard word order, but we thinkthat word orders tend to differ between ditferentstyles of writing.
We would therefore like to carryout experiments with other types of texts, such asnovels, having styles different froln that of newspa-pers.it has been (lift\]cult o evaluate tile reslflts of textgeneration objectively becmlse there have been nogood stmldards for ewlllmtion.
By using the stan-(lard we describe in this paper, however, we can evN-uate results objectively, at least for word order esti-mation in text, generation.We expect hat our lnodel can be used for severalapplications as well as linguistic veritication, such astext; refinement silt)port and text generation in nla-chine translation.ReferencesAdam L. Better, Stephen A. I)ella Pietra, and Vincent J. DellaPietra.
199(L A Maximum t'\]ntropy Approach to N~ttural ,~tn-gmtge Processing.
Computational Linguistics, 22(11:39-71.Sadao Kurohashi and Makol.o Nagao.
1997.
Kyoto UniversityText Corl)uS Project.
In Proceedings of The Third AnnualMectin9 of The Association for Natural Language Process-ing, pages 115-118.
(in Japanese).Sadao Kurohashi and Makoto Nagao, 1998.
Japanese Morpho-logical Analysis System JUMAN Version 3.6. l)epartment ofInformatics, Kyoto University.Sadao Kurohashi, 11198.
Japanese Dependency/Case StructureAnalyzer KNP Version 2.0b6.
Department of Inforln~tties, Ky-ore University.IIiroshl Maruyama.
1994.
Experhnents on V~rord-Order RecoveryUsing N-Cram Models.
In YTte \]~9th Annual (;onvention IPSJapan.
(in Japanese).NLRl(National Language Research Institute).
1964.
Word Listby Semantic Pri~ciples.
Syuei Syuppan.
(in Japmlese).Tetsuo Saekl.
1998.
Yousetsu nihongo no 9ojun (Survey: WordOrder in Japanese).
Kuroshio Syupl)an.
(in Japanese).James Shaw and Vasileios Ilatzivassiloglou.
1999.
OrderingAmong l'remodifiers.
In Proceedings of the 37th Annual Meet-ing of the Association for Computational Linguistics (,4 CL),pages 135-143.Takenobu Tokunaga and IIozumi Tanaka.
1991.
On Estimat-ing Japanese Word Order B~used on Valency Information.Keiryo Kokugogaku (Mathematical Linguistics), 18(21:53-(;5.
(in Japanese).877
