Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 33?37,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsA Regression Model of Adjective-Noun Compositionalityin Distributional SemanticsEmiliano GuevaraTekstlab, ILN, University of OsloOslo, Norwaye.r.guevara@iln.uio.noAbstractIn this paper we explore the computationalmodelling of compositionality in distri-butional models of semantics.
In par-ticular, we model the semantic composi-tion of pairs of adjacent English Adjec-tives and Nouns from the British NationalCorpus.
We build a vector-based seman-tic space from a lemmatised version ofthe BNC, where the most frequent A-Nlemma pairs are treated as single tokens.We then extrapolate three different mod-els of compositionality: a simple additivemodel, a pointwise-multiplicative modeland a Partial Least Squares Regression(PLSR) model.
We propose two evalu-ation methods for the implemented mod-els.
Our study leads to the conclusion thatregression-based models of composition-ality generally out-perform additive andmultiplicative approaches, and also show anumber of advantages that make them verypromising for future research.1 IntroductionWord-space vector models or distributional mod-els of semantics (henceforth DSMs), are com-putational models that build contextual seman-tic representations for lexical items from corpusdata.
DSMs have been successfully used in therecent years for a number of different computa-tional tasks involving semantic relations betweenwords (e.g.
synonym identification, computationof semantic similarity, modelling selectional pref-erences, etc., for a thorough discussion of the field,cf.
Sahlgren, 2006).
The theoretical foundation ofDSMs is to be found in the ?distributional hypoth-esis of meaning?, attributed to Z. Harris, whichmaintains that meaning is susceptible to distribu-tional analysis and, in particular, that differencesin meaning between words or morphemes in alanguage correlate with differences in their distri-bution (Harris 1970, pp.
784?787).While the vector-based representation of wordmeaning has been used for a long time in com-putational linguistics, the techniques that are cur-rently used have not seen much development withregards to one of the main aspects of semantics innatural language: compositionality.To be fair, the study of semantic composition-ality in DSMs has seen a slight revival in the re-cent times, cf.
Widdows (2008), Mitchell & La-pata (2008), Giesbrecht (2009), Baroni & Lenci(2009), who propose various DSM approachesto represent argument structure, subject-verb andverb-object co-selection.
Current approaches tocompositionality in DSMs are based on the appli-cation of a simple geometric operation on the basisof individual vectors (vector addition, pointwise-multiplication of corresponding dimensions, ten-sor product) which should in principle approxi-mate the composition of any two given vectors.On the contrary, since the the very nature ofcompositionality depends on the semantic rela-tion being instantiated in a syntactic structure, wepropose that the composition of vector representa-tions must be modelled as a relation-specific phe-nomenon.
In particular, we propose that the usualprocedures from machine learning tasks must beimplemented also in the search for semantic com-positionality in DSM.In this paper we present work in progress onthe computational modelling of compositionalityin a data-set of English Adjective-Noun pairs ex-tracted from the BNC.
We extrapolate three differ-ent models of compositionality: a simple additivemodel, a pointwise-multiplicative model and, fi-nally, a multinomial multiple regression model byPartial Least Squares Regression (PLSR).332 Compositionality of meaning in DSMsPrevious work in the field has produced a smallnumber of operations to represent the composi-tion of vectorial representations of word meaning.In particular, given two independent vectors v1and v2, the semantically compositional result v3is modelled by:?
vector addition, the compositional meaningof v3 consists of the sum of the independentvectors for the constituent words:v1i + v2i = v3i?
pointwise-multiplication (Mitchell and La-pata 2008), each corresponding pair of com-ponents of v1 and v2 are multiplied to obtainthe corresponding component of v3:v1i ?
v2i = v3i?
tensor product, v1 ?
v2 = v3, where v3 isa matrix whose ij-th entry is equal to v1iv2j(cf.
Widdows 2008, who also proposes therelated method of convolution product, bothimported from the field of quantum mechan-ics)In the DSM literature, the additive model has be-come a de facto standard approach to approximatethe composed meaning of a group of words (or adocument) as the sum of their vectors (which re-sults in the centroid of the starting vectors).
Thishas been successfully applied to document-basedapplications such as the computation of documentsimilarity in information retrieval.Mitchell & Lapata (2008) indicate that the var-ious variations of the pointwise-multiplicationmodel perform better than simple additive mod-els in term similarity tasks (variations includedcombination with simple addition and addingweights to individual vector components).
Wid-dows (2008) Obtain results indicating that both thetensor product and the convolution product per-form better than the simple additive model.For the sake of simplifying the implementa-tion of evaluation methods, in this paper we willcompare the first two approaches, vector additionand vector pointwise-multiplication, with regres-sion modelling by partial least squares.3 Partial least squares regression ofcompositionalityWe assume that the composition of meaning inDSMs is a function mapping two or more inde-pendent vectors in a multidimensional space to anewly composed vector the same space and, fur-ther, we assume that semantic composition is de-pendent on the syntactic structure being instanti-ated in natural language.1Assuming that each dimension in the startingvectors v1 and v2 is a candidate predictor, and thateach dimension in the composed vector v3 is a de-pendent variable, vector-based semantic composi-tionality can be formulated as a problem of multi-variate multiple regression.
This is, in principle,a tractable problem that can be solved by stan-dard machine learning techniques such as multi-layer perceptrons or support vector machines.However, given that sequences of words tend tobe of very low frequency (and thus difficult to rep-resent in a DSM), suitable data sets will inevitablysuffer the curse of dimensionality: we will oftenhave many more variables (dimensions) than ob-servations.Partial Least Squares Regression (PLSR) is amultivariate regression technique that has been de-signed specifically to tackle such situations withhigh dimensionality and limited data.
PLSR iswidely used in in unrelated fields such as spec-troscopy, medical chemistry, brain-imaging andmarketing (Mevik & Wehrens, 2007).4 Materials and toolsWe use a general-purpose vector space extractedfrom the British National Corpus.
We used theInfomap software to collect co-occurrence statis-tics for lemmas within a rectangular 5L?5R win-dow.
The corpus was pre-processed to representfrequent Adjective-Noun lemma pairs as a sin-gle token (e.g.
while in the original corpus theA-N phrase nice house consists in two separatelemmas (nice and house), in the processed cor-pus it appears as a single entry nice_house).
Thecorpus was also processed by stop-word removal.We extracted a list of A-N candidate pairs withsimple regex-based queries targeting adjacent se-quences composed of [Det/Art?A?N] (e.g.
that lit-tle house).
We filtered the candidate list by fre-quency (> 400) obtaining 1,380 different A-Npairs.The vector space was built with the 40,000 mostfrequent tokens in the corpus (a cut-off point thatincluded all the extracted A-N pairs).
The origi-nal dimensions were the 3,000 most frequent con-1Mitchell & Lapata (2008) make very similar assumptionsto the ones adopted here.34tent words in the BNC.
The vector space wasreduced to the first 500 ?latent?
dimensions bySVD as implemented by the Infomap software.Thus, the resulting space consists in a matrix with40, 000?
500 dimensions.We then extracted the vector representation foreach A-N candidate as well as for each indepen-dent constituent, e.g.
vectors for nice_house (v3),as well as for nice (v1) and house (v2) were saved.The resulting vector subspace was imported intothe R statistical computing environment for thesubsequent model building and evaluation.
Inparticular, we produced our regression analysiswith the pls package (Mevik & Wehrens, 2007),which implements PLSR and a number of veryuseful functions for cross-validation, prediction,error analysis, etc.By simply combining the vector representationsof the independent Adjectives and Nouns in ourdata-set (v1 and v2) we built an additive predic-tion model (v1 + v2) and a simplified pointwisemultiplicative prediction model (v1?
v2) for eachcandidate pair.We also fitted a PLSR model using v1 and v2as predictors and the corresponding observed pairv3 as dependent variable.
The data were dividedinto a training set (1,000 A-N pairs) and a testingset (the remaining 380 A-N pairs).
The model?sparameters were estimated by performing 10-foldcross-validation during the training phase.In what follows we briefly evaluate the three re-sulting models of compositionality.5 EvaluationIn order to evaluate the three models of composi-tionality that were built, we devised two differentprocedures based on the Euclidean measure of ge-ometric distance.The first method draws a direct comparison ofthe different predicted vectors for each candidateA-N pair by computing the Euclidean distance be-tween the observed vector and the modelled pre-dictions.
We also inspect a general distance matrixfor the whole compositionality subspace, i.e.
allthe observed vectors and all the predicted vectors.We extract the 10 nearest neighbours for the 380Adjective-Noun pairs in the test set and look forthe intended predicted vectors in each case.
Theidea here is that the best models should producepredictions that are as close as possible to the orig-inally observed A-N vector.Our second evaluation method uses the 10 near-est neighbours of each of the observed A-N pairsin the test set as gold-standard (excluding anymodelled predictions), and compares them withthe 10 nearest neighbours of each of the corre-sponding predictions as generated by the models.The aim is to assess if the predictions made byeach model share any top-10 neighbours with theircorresponding gold-standard.
We award 1 pointfor every shared neighbour.5.1 The distance of predictionsWe calculated the Euclidean distance betweeneach observed A-N pair and the correspondingprediction made by each model.
On general in-spection, it is clear that the approximation of A-Ncompositional vectors made by PLSR is consid-erably closer than those produced by the additiveand multiplicative models, cf.
Table 1.Min.
1st Q.
Median Mean 3rd Q. Max.ADD 0.877 1.402 1.483 1.485 1.570 1.814MUL 0.973 0.998 1.002 1.002 1.005 1.019PLSR 0.624 0.805 0.856 0.866 0.919 1.135Table 1: Summary of distance values between the 380observed A-N pairs and the predictions from each model(ADD=additive, MUL=multiplicative, PLSR=Partial LeastSquares Regression).We also computed in detail which of the three pre-dicted composed vectors was closest to the corre-sponding observation.
To this effect we extractedthe 10 nearest neighbours for each A-N pair in thetest set using the whole compositionality subspace(all the predicted and the original vectors).
In 94cases out of 380, the PLSR intended predictionwas the nearest neighbour.
Cumulatively, PLSR?spredictions were in the top-10 nearest neighbourlist in 219 out of 380 cases (57.6%).
The othermodels?
performance in this test was negligible,cf.
Table 2.
Overall, 223 items in the test set hadat least one predicted vector in the top-10 list; ofthese, 219 (98%) were generated by PLSR and theremaining 4 (1%) by the multiplicative model.1 2 3 4 5 6 7 8 9 10 Tot.ADD 0 0 0 0 0 0 0 0 0 0 0MUL 0 1 0 2 1 0 0 0 0 0 4PLSR 94 51 24 18 10 7 7 5 2 1 219Table 2: Nearest predicted neighbours and their positions inthe top-10 list.355.2 Comparing prediction neighbours to thegold standardSince the main use of DSMs is to extract similarvectors from a multidimensional space (represent-ing related documents, distributional synonyms,etc.
), we would like to test if the modelling of se-mantic compositionality is able to produce predic-tions that are as similar as possible to the originallyobserved data.
A very desirable result would beif any predicted compositional A-N vector couldbe reliably used instead of the extracted bigram.This could only be achieved if a model?s predic-tions show a similar distributional behaviour withrespect to the observed vector.To test this idea using our data, we took the10 nearest neighbours of each of the observed A-N pairs in the test set as gold standard.
Thesegold neighbours were extracted from the obser-vation testing subspace, thus excluding any mod-elled predictions.
This is a very restrictive set-ting: it means that the gold standard for each ofthe 380 test items is composed of the 10 nearestneighbours from the same 380 items (which mayturn out to be not very close at all).
We then ex-tracted the 10 nearest neighbours for each of thethree modelled predictions, but this time the sub-space included all predictions, as well as all theoriginal observations (380?
4 = 1520 items).
Fi-nally, we tested if the predictions made by eachmodel shared any top-10 neighbours with theircorresponding gold-standard.
We awarded 1 pointfor every shared neighbour.The results obtained with these evaluation set-tings were very poor.
Only the additive modelscored points (48), although the performance wasrather disappointing (maximum potential score forthe test was 3,800 points).
Both the pointwise mul-tiplicative model and the PLSR model failed to re-trieve any of the gold standard neighbours.
Thispoor results can be attributed to the very restric-tive nature of our gold standard and, also, to theasymmetrical composition of the compared data(gold standard: 3,800 neighbours from a pool ofjust 380 different items; prediction space: 11,400neighbours from a pool of 1,520 items).However, given the that DSMs are knownfor their ability to extract similar items fromthe same space, we decided to relax our testsettings by awarding points not only to sharedneighbours, but also to the same model?s predic-tions of those neighbours.
Thus, given a tar-get neighbour such as good_deal, in our sec-ond setting we awarded points not only to thegold standard good_deal, but also to the pre-dictions good_deal_ADD, good_deal_MUL andgood_deal_PLSR when evaluating each corre-sponding model.
With these settings the comparedspaces become less asymmetrical (gold standard:7,600 neighbours from a pool of just 380 differentitems plus predictions; prediction space: 11,400neighbours from a pool of 1,520 items).
The ob-tained results show a great improvement (max.
po-tential score 7,600 points):Shared Neigh.
Predicted Neigh.
TotalADD 48 577 625MUL 0 37 37PLSR 0 263 263Not shared: 6,675Table 3: Shared neighbours with respect to the gold standardand shared predicted neighbours.Once again, the additive model showed the bestperformance, followed by PLSR.
The multiplica-tive model?s performance was negligible.While carrying out these experiments, an unex-pected fact became evident.
Each of the models inturn produces predictions that are relatively closeto each other, regardless of the independent wordsthat were used to calculate the compositional vec-tors.
This has the consequence that the nearestneighbour lists for each model?s predictions are,by and large, populated by items generated in thesame model, as shown in Table 4.ADD MUL PLSR OBSADD 2,144 (56%) ?
?
?MUL 59 (1%) 3,800 (100%) 998 (26%) 1,555 (40%)PLSR 1,472 (38%) ?
2,802 (73%) 2,190 (57%)OBS 125 (3%) ?
?
55 (1%)Table 4: Origins of neighbours in each models?
top-10 listof neighbours extracted from the full space composed ofobservations and predictions (380 ?
4 = 1, 440 items)(ADD=additive, MUL=multiplicative, PLSR=Partial LeastSquares Regression, OBS=observed vectors) .Neighbours of predictions from the multiplicativemodel are all multiplicative.
The additive modelhas the most varied set of neighbours, but themajority of them are additive-neighbours.
PLSRshows a mixed behaviour.
However, PLSR pro-duced neighbours that find their way into theneighbour sets of both the additive model and theobservations.These remarks point in the same direction: ev-36ery model is a simplified and specialised versionof the original space, somewhat more orderly thanthe observed data, and may give different resultsdepending on the task at stake.
PLSR (and to alesser extent also the multiplicative model) is par-ticularly efficient as generator of neighbours forreal vectors, a characteristic that could be appliedto guess distributional synonyms of unseen A-Npairs.
On the other hand, the additive model (andto a lesser extent PLSR) is especially successfulin attracting gold standard neighbours.
Overall,even at this experimental stage, PLSR is clearlythe model that produces the most consistent re-sults.6 Concluding remarksThis paper proposed a novel method to modelthe compositionality of meaning in distributionalmodels of semantics.
The method, Partial LeastSquares Regression, is well known in other data-intensive fields of research, but to our knowledgehad never been put to work in computational dis-tributional semantics.
Its main advantage is thefact that it is designed to approximate functionsin problems of multivariate multiple regressionwhere the number of observations is relativelysmall if compared to the number of variables (di-mensions).We built a DSM targeting a type of semanticcomposition that has not been treated extensivelyin the literature before, adjacent A-N pairs.The model built by PLSR performed better thanboth a simple additive model and a multiplicativemodel in the first proposed evaluation method.Our second evaluation test (using comparisonto a gold standard) gave mixed results: the bestperformance was obtained by the simple additivemodel, with PLSR coming in second place.This is work in progress, but the results lookvery promising.
Future developments will cer-tainly focus on the creation of better evaluationmethods, as well as on extending the experi-ments to other techniques (e.g.
convolution prod-uct as discussed by Widdows, 2008 and Gies-brecht, 2009).
Another important issue that westill have not touched is the role played by lex-ical association (collocations) in the predictionmodels.
We would like to make sure that weare not modelling the compositionality of non-compositional examples.A last word on the view of semantic composi-tionality suggested by our approach.
Modellingcompositionality as a machine learning task im-plies that a great number of different ?types?
ofcomposition (functions combining vectors) maybe learned from natural language samples.
In prin-ciple, any semantic relation instantiated by anysyntactic structure could be learned if sufficientdata is provided.
This approach must be con-fronted with other linguistic phenomena, also ofgreater complexity than just a set of bigrams.
Fi-nally, we might wonder if there is an upper limit tothe number of compositionality functions that weneed to learn in natural language, or if there aretypes of functions that are more difficult, or evenimpossible, to learn.AcknowledgementsThanks are due to Marco Baroni, Stefan Evert,Roberto Zamparelli and the three anonymous re-viewers for their assistance and helpful comments.ReferencesMarco Baroni and Alessandro Lenci.
2009.
Onesemantic memory, many semantic tasks.
In Pro-ceedings GEMS 2009, 3?11.
Athens: Associa-tion for Computational Linguistics.Eugenie Giesbrecht.
2009.
In Search of Seman-tic Compositionality in Vector Spaces.
In Pro-ceedings of the 17th International Conferenceon Conceptual Structures, ICCS 2009, Moscow,Russia, pp.
173?184.
Berlin: Springer.Zellig Harris.
1970 [1954].
Distributional struc-ture.
In Papers in structural and transforma-tional linguistics, 775?794.
Dordrecht: Reidel.Bj?rn-Helge Mevik and Ron Wehrens.
2007.
Thepls package: principal component and partialleast squares regression in R. Journal of Statis-tical Software, 18(2): 1?24.Jeff Mitchell and Mirella Lapata.
2008.
Vector-based Models of Semantic Composition.
InProceedings of ACL-08: HLT, 236?244.Columbus, OH.Dominic Widdows.
2008.
Semantic Vector Prod-ucts: Some Initial Investigations.
Second AAAISymposium on Quantum Interaction, Oxford,26th?28th March 2008.
URL: http://www.puttypeg.com/papers/Magnus Sahlgren.
2006.
The Word Space Model.Ph.D.
dissertation, Stockholm University.37
