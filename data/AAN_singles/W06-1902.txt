Dutch 1.90% Arabic,1.70%English,39.6%Chinses,14.7%Spanish,9.6%Japanese9%German7.4%French 4.5%Italian, 4.1%Korean 4.2%Portuegese3.3%The Affect of Machine Translation on the Performance of Arabic-English QA SystemAzzah Al-MaskariDept.
of Information Studies,University of Sheffield,Sheffield, S10 2TN, UKlip05aaa@shef.ac.ukMark SandersonDept.
of Information Studies,University of Sheffield,Sheffield, S10 2TN, UKm.sanderson@shef.ac.ukAbstractThe aim of this paper is to investigatehow much the effectiveness of a Ques-tion Answering (QA) system was af-fected by the performance of MachineTranslation (MT) based question transla-tion.
Nearly 200 questions were selectedfrom TREC QA tracks and ran through aquestion answering system.
It was able toanswer 42.6% of the questions correctlyin a monolingual run.
These questionswere then translated manually from Eng-lish into Arabic and back into English us-ing an MT system, and then re-applied tothe QA system.
The system was able toanswer 10.2% of the translated questions.An analysis of what sort of translation er-ror affected which questions was con-ducted, concluding that factoid typequestions are less prone to translation er-ror than others.1 IntroductionIncreased availability of on-line text in languagesother than English and increased multi-nationalcollaboration have motivated research in Cross-Language Information Retrieval (CLIR).
Thegoal of CLIR is to help searchers find relevantdocuments when their query terms are chosenfrom a language different from the language inwhich the documents are written.
Multilingualityhas been recognized as an important issue for thefuture of QA (Burger et al 2001).
The multilin-gual QA task was introduced for the first time inthe Cross-Language Evaluation Forum CLEF-2003.According to the Global Reach web site(2004), shown in Figure 1, it could be estimatedthat an English speaker has access to around 23times more digital documents than an Arabicspeaker.
One can conclude from the given infor-mation shown in the Figure that cross-languageis potentially very useful when the required in-formation is not available in the users?
language.Figure 1: Online language Population (March, 2004)The goal of a QA system is to find answers toquestions in a large collection of documents.
Theoverall accuracy of a QA system is directly af-fected by its ability to correctly analyze the ques-tions it receives as input, a Cross LanguageQuestion Answering (CLQA) system will besensitive to any errors introduced during questiontranslation.
Many researchers criticize the MT-based CLIR approach.
The reason for their criti-cism mostly stem from the fact that the currenttranslation quality of MT is poor, in addition MTsystem are expensive to develop and their appli-cation degrades the retrieval efficiency due to thecost of the linguistic analysis.EACL 2006 Workshop on Multilingual Question Answering - MLQA069This paper investigates the extent to whichMT error affects QA accuracy.
It is divided asfollows: in section 2 relevant previous work oncross-language retrieval is described, section 3explains the experimental approach which in-cludes the procedure and systems employed, italso discuss the results obtained, section 4 drawsconclusions and future research on what im-provements need to be done for MT systems.2 Related ResearchCLIR is an active area, extensive research onCLIR and the effects of MT on QA systems?
re-trieval effectiveness has been conducted.
Lin andMitamua (2004) point out that the quality oftranslation is fully dependent upon the MT sys-tem employed.Perret (2004) proposed a question answeringsystem designed to search French documents inresponse to French queries.
He used automatictranslation resources to translate the original que-ries from (Dutch, German, Italian, Portuguese,Spanish, English and Bulgarian) and reports theperformance level in the monolingual task was24.5% dropping to 17% in the bilingual task.
Asimilar experiment was conducted by Plamondonand Foster (2003) on TREC questions and meas-ured a drop of 44%, and in another experimentusing Babelfish, the performance dropped evenmore, 53%.
They believe that CLEF questionswere easier to process because they did not in-clude definition questions, which are harder totranslate.
Furthermore, Plamondon and Foster(2004) compare the cross language version oftheir Quantum QA system with the monolingualEnglish version on CLEF questions and note theperformance of a cross-language system (Frenchquestions and English documents) was 28%lower than the monolingual system using IBM1translation.Tanev et al (2004) note that DIOGENE sys-tem, which relies on the Multi-WordNet, per-forms 15% better in the monolingual (Italian-Italian) than cross-language task (Italian-English).
In Magnini et al?s (2004) report for theyear 2003, the average performance of correctanswers on monolingual tasks was 41% and 25%in the bilingual tasks.
In addition in the year2004, the average accuracy in the monolingualtasks was 23.7% and 14.7% in bilingual tasks.As elucidated above, much research has beenconducted to evaluate the effectiveness of QAsystems in a cross language platform by employ-ing MT systems to translate the queries from thesource language to the target language.
How-ever, most of them are focused on European lan-guage pairs.
To our knowledge, only one pastexample of research has investigated the per-formance of a cross-language Arabic-EnglishQA system Rosso et al(2005).
The QA systemused by Rosso et al(2005) is based on a systemreported in Del Castillo (2004).
Their experimentwas carried out using the question corpus of theCLEF-2003 competition.
They used questions inEnglish and compared the answers with thoseobtained after the translation back into Englishfrom an Arabic question corpus which wasmanually translated.
For the Arabic-Englishtranslation process, an automatic machine trans-lator, the TARJIM Arabic-English machinetranslation system, was used.
Rosso el al re-ported a decrease of QA accuracy by more than30% which was caused by the translation proc-ess.Work in the Rosso paper was limited to a sin-gle QA and MT system and also did not analyzetypes of errors or how those errors affected dif-ferent types of QA questions.
Therefore, it wasdecided to conduct further research on MT sys-tems and its affect on the performance in QAsystems.
This paper presents an extension on theprevious mentioned study, but with more diverseranges of TREC data set using different QA sys-tem and different MT system.3 Experimental ApproachTo run this experiment, 199 questions were ran-domly compiled from the TREC QA track,namely from TREC-8, TREC-9, TREC-11,TREC-2003 and TREC-2004, to be run throughAnswerFinder, the results of which are discussedin section 3.1.
The selected 199 English TRECquestions were translated into Arabic by one ofthe authors (who is an Arabic speaker), and thenfed into Systran to translate them into English.The analysis of translation is discussed in detailin section 3.2.3.1 Performance of AnswerFinderThe 199 questions were run over AnswerFinder;divided as follows: 92 factoid questions, 51 defi-nition questions and 56 list questions.
The an-swers were manually assessed following an as-sessment scheme similar to the answer categoriesin iCLEF 2004:?
Correct: if the answer string is valid andsupported by the snippetsEACL 2006 Workshop on Multilingual Question Answering - MLQA0610?
Non-exact: if the answer string is miss-ing some information, but the full answeris found in the snippets.?
Wrong: if the answer string and thesnippets are missing important informa-tion or both the answer string and thesnippets are wrong compared with the an-swer key.?
No answer: if the system does not returnany answer at all.Table 1 provides an overall view, the system cor-rectly answered 42.6% of these questions,whereas 25.8% wrongly, 23.9% no answer and8.1% non-exactly.
Table 2 illustrates Answer-Finder?
abilities to answer each type of thesequestions separately.Table 1.
Overall view of AnswerFinder Mono-lingual AccuracyFactoid Definition ListCorrect 63 6 15Not exact 1 6 9Wrong 22 15 13No answer 6 23 18Table 2.
Detail analysis of AnswerFinder Per-formance-monolingual runTo measure the performance of Answer-Finder, recall (ratio of relevant items retrieved toall relevant items in a collection) and precision(the ratio of relevant items retrieved to all re-trieved items) were calculated.
Thus, recall andprecision and F-measure for AnswerFinder are,0.51 and 0.76, 0.6 respectively.3.2 Systran TranslationMost of the errors noticed during the transla-tion process were of the following types: wrongtransliteration, wrong word sense, wrong wordorder, and wrong pronoun translations.
Table 3lists Systran?s translation errors to provide cor-rect transliteration 45.7%, wrong word senses(key word) 31%, wrong word order 25%, andwrong translation of pronoun 13.5%.Below is a discussion of Systran?
translationaccuracy and the problems that occurred duringtranslation of the TREC QA track questions.Table 3.
Types of Translation ErrorsWrong TransliterationWrong transliteration is the most common errorthat encountered during translation.
Translitera-tion is the process of replacing words in thesource language with their phonetic equivalent inthe target language.
Al-Onaizan and Knight(2002) state that transliterating names from Ara-bic into English is a non-trivial task due to thedifferences in their sound and writing system.Also, there is no one-to-one correspondence be-tween Arabic sounds and English sounds.
Forexample P and B are both mapped to the singleArabic letter ???
; Arabic ???
and ??
?are mappedinto English H.Table 4.
Incorrect use of translation when trans-literation should have been usedTable 5.
Wrong translation of person?s nameTransliteration mainly deals with propernames errors when MT doesn?t recognize themas a proper name and translates them instead oftransliterating them.
Systran produced 91 ques-tions (45.7%) with wrong transliteration.
It trans-lated some names literally, especially those witha descriptive meaning.
Table 4 provides an ex-ample of such cases where ?Aga?
was wronglytransliterated; and ?khan?
was translated to ?be-tray?
where it should have been transliterated.This can also be seen in table 5; ?Hassan Ro-hani?
was translated literally as ?Spiritual good-ness?.Answer TypeCorrect 42.6%Non exact 8.1%Wrong 25.8%No Answer 23.9%Type of Translation Error PercentageWrong Transliteration 45.7%Wrong Sense 31%Wrong Word Order 25%Wrong Pronoun 13.5%Original text Who is Aga1 Khan2?Arabic version ???
?2 ???
1??
???
?Translation(wrong)From [EEjaa] 1 be-trayed2?Original text Who is Hasan1 Rohani2Arabic version  ???????2???
1??
???
?Translation(wrong)From spiritual2 goodness1is?EACL 2006 Workshop on Multilingual Question Answering - MLQA0611Wrong Word SenseWrong translation of words can occur when asingle word can have different senses accordingto the context in which the word is used.
Wordsense problems are commoner in Arabic than in alanguage like English as Arabic vowels (writtenas diacritics) are largely unwritten in most texts.Systran translated 63 questions (31.2%) withat least one wrong word sense, 25% of themcould have been resolved by adding diacritics.Table 6 illustrates an error resulting fromSystran?s failure to translate words correctlyeven after diacritics have been added; the term?????
???????
(psychology) was wrongly translatedas ?flag of breath?.
The Arabic form is a com-pound phrase; however Systran translated eachword individually even after diacritics wereadded.Original text Who was the father of psy-chology?Arabic ver-sion???
??
????
?????
?Translation(wrong)From father flag of thebreath?Table 6.
Example of incorrect translation due toincorrect sense choiceThese errors can occur when a single word canhave different senses according to the con-text inwhich the word is used.
They also occur due tothe diacritization in Arabic language.
Arabicwriting involves diacritization (vowel), which islargely ignored in modern texts.
Ali (2003) givesa good example that can make an Englishspeaker grasp the complexity caused by droppingArabic diacritization.
Suppose that vowels aredropped from an English word and the result is?sm?.
The possibilities of the original word are:some, same, sum, and semi.Systran translated 63 questions out of 199(31.2%) with wrong word sense, 25% of themcan be resolved by adding diacritization.Wrong Word OrderWord order errors occurred when the translatedwords were in order that made no sense and thisproblem produced grammatically ill formed sen-tences that the QA system was unable to process.Systran translated 25% of the questions withwrong word orders which lead to the construc-tion of ungrammatical questions.
Table 7 showsan example of wrong word order.Table 7.
Wrong word orderWrong PronounSystran frequently translated the pronoun ????
to?air?
in place of ?him?
or ?it?
as shown in table8.
Table 9 shows an example pronoun problems;the pronoun ????
is translated into ?her?, insteadof ?it?, which refers to ?building?
in this case.Original text Who is Colin Powell?Arabic version ???
???
????
????
?Translation(wrong)From Collin Powell air?Table 8.
Wrong translation of ?who?Table 9. wrong pro-drop of translation of ?it?It has been observed that Systran translationerrors exhibited some clear regularities for cer-tain questions as might be expected from a rule-based system.
As shown in tables 2,3,4,7 theterm ??????
was translated to ?from?
instead of?who?.
This problem is related to the recognitionof diacritization.English Translation ArabicWord Returned Expected???
Big size????
Tall, big long?????
Ground earth????
Create locate???
Nation country????
Far distance????
Many most, muchTable 10.
List of wrong key word returned bySystranThe evaluator observed that Systran?
propen-sity to produce some common wrong sense trans-lations which lead to change the meaning of thequestions, table 10 shows some of these commonsense translation.Original text What are the names of thespace shuttles?Arabic version ??
?????
??????
???????
?Translation(wrong)What space name of theshuttle?Original text What buildings had Frank de-signed?Arabic version ??
???????
????
?????
?????
?Translation(wrong)What the buildings which de-signed her Frank?EACL 2006 Workshop on Multilingual Question Answering - MLQA06123.3 The Effectiveness of AnswerFindercombined with Systran?
TranslationAfter translating the 199 questions using Systran,they were passed to AnswerFinder.
Figure 2 il-lustrates the system?s abilities to answer theoriginal and the translated questions; Answer-Finder was initially able to answer 42.6% of thequestions while after translation, its accuracy toreturn correct answers dropped to 10.2%.
Itsfailure to return any answer increased by 35.5%(from 23.9% to 59.4%); in addition, non-exactanswers decreased by 6.6% while wrong answersincreased by 3.6% (from 25.4% to 28.9%).42.68.11.528.959.425.4 23.910.2correct not exact wrong no answerOriginal/TranslatedSuccess(%originaltranslatedFigure 2.
Answering Original/Translated Ques-tions (%)AnswerFinder was able to answer 23 trans-lated questions out of 199.
Out of these 23 ques-tions, 12 were correctly translated and 11 exhib-ited some translation errors.
Looking closely atthe 12 corrected translated question (shown intable 11), 9 of them are of the factoid type, oneof definition type and two of the list type.
Andby investigating the other 11 questions that ex-hibited some translation errors (shown in table12), it can be noticed that 9 of them are factoidand 2 are list types.
Our assumption for Systran?ability to translate factoid questions more thandefinition and list questions is that they exhibitedless pronouns, in contrast to definition and listquestions where many proper names were in-cluded.In total, Systran has significantly reduced An-swerFinder?s ability to return correct answers by32.4%.
Table 13 shows recall, precision and F-measure before and after translation, the value ofrecall before translation is 0.51 and after transla-tion has dropped down to 0.12.
Similarly, theprecision value has gone down from 0.76 to 0.41,accordingly the F-measure also dropped downfrom 0.6 to 0.2.
Altogether, in multilingual re-trieval task precision and recall are 40.6% and30%, respectively, below the monolingual re-trieval task.QuestionTypeCorrectlytranslatedCorrectly answeredafter translationFactoid (9/92) (19/92)Defn.
(1/51) (0/51)List (2/56) (3/56)Total 12 13Table 11.
Types of questions translated and an-swered correctly in the bilingual runTranslation Error Type Question TypeWord Sense 4 factoidWord Order 2 factoid, 2 listDiacritics 2 factoidTransliteration 1 factoidTotal 11 questionsTable 12.
Classification of wrongly translatedquestions but correctly answeredEffectivenessmeasureBefore Trans-lationAfter Trans-lationRecall 0.51 0.12Precision 0.76 0.41F-measure 0.6 0.2Table 13.
Effectiveness measures of Answer-Finder4 ConclusionsSystran was used to translate 199 TREC ques-tions from Arabic into English.
We have scruti-nized the quality of Systran?s translation throughout this paper.
Several translation errors ap-peared during translations which are of the type:wrong transliteration, wrong word sense, wrongword order and wrong pronoun.
The translatedquestions were fed into AnswerFinder, whichhad a big impact on its accuracy in returning cor-rect answers.
AnswerFinder was seriously af-fected by the relatively poor output of Systran;its effectiveness was degraded by 32.4%.
Thisconclusion confirms Rosso et al(2005) findingsusing different QA system, different test sets anddifferent machine translation system.
Our resultsvalidate their results which they concluded thattranslation of the queries from Arabic into Eng-lish has reduced the accuracy of QA system bymore than 30%.We recommend using multiple MT to give awider range of translation to choose from, hence,correct translation is more likely to appear inEACL 2006 Workshop on Multilingual Question Answering - MLQA0613multiple MT systems than in a single MT sys-tem.
However, it is essential to note that in somecases MT systems may all disagree with one an-other in providing correct translation or they mayagree on the wrong translation.It should also be borne in mind that somekeywords are naturally more important than oth-ers, so in a question-answering setting it is moreimportant to translate them correctly.
Some key-words may not be as important, and some key-words due to the incorrect analysis of the Englishquestion sentence by the Question Analysismodule, may even degrade the translation andquestion-answering performance.We believe there are ways to avoid the MT er-rors that discussed previously (i.e.
wrong trans-literation, wrong word senses, wrong word order,and wrong translation of pronoun).
Below aresome suggestions to overcome such problems:?
One solution is to make some adjust-ments (Pre or Post- processing) to thequestion translation process to minimizethe effects of translation by automaticallycorrecting some regular errors using aregular written expression.?
Another possible solution is to try build-ing an interactive MT system by providingusers more than one translation to pick themost accurate one, we believe this will of-fer a great help in resolving word senseproblem.
This is more suit-able for expertusers of a language.In this paper, we have presented the errors as-sociated with machine translation which indi-cates that the current state of MT is not very reli-able for cross language QA.
Much work has beendone in the area of machine translation for CLIR;however, the evaluation often focuses on re-trieval effectiveness rather than translation cor-rectness.AcknowledgementWe would like to thank EU FP6 project BRICKS(IST-2002-2.3.1.12) and Ministry of Manpower,Oman, for partly funding this study.
Thanks arealso due to Mark Greenwood for helping us withaccess to his AnswerFinder system.ReferencesBurger, J., Cardie, C., Chaudhri, V., Gaizauskas, R.,Harabagiu, S., Israel, D., Jacquemin, C., Lin,C.,Maiorano, S., Miller, G.,  Moldovan, D., Ogden,B., Prager, J., Riloff, E., Singhal, A., Shrihari, R.,Strzalkowski, T., Voorhees, E., Weishedel, R.,(2001) Issues, Tasks and Program Structures toRoadmap Research in Question & Answering(Q&A).
Technical report, National Institute ofStandards and Technology (NIST)Del Castillo, M., Montes y G?mez, M., and Vil-lase?or, L. (2004) QA on the web: A preliminarystudy for Spanish language.
Proceedings of the 5thMexican International Conference on ComputerScience (ENC04), Colima, Mexico.Hull, D. A., and Grefenstette, G. (1996) Queryingacross languages: a dictionary-based approach tomultilingual information retrieval.
Research andDevelopment in Information Retrieval, pp46-57.Lin, F., & Mitamura, T. (2004) Keyword Translationfrom English to Chinese for Multilingual QA.
In:The 6th Conference of the Association for MachineTranslation in the Americas (AMTA-2004)Magnini, B., Vallin, A., Ayache C., Erbach, G.,Pe?as, A., Rijke, M. Rocha, P., Simov, K., Sut-cliffe, R. (2004) Overview of the CLEF 2004Multi-lingual Question Answering Track.
In: Pro-ceedings of CLEF 2004Perrot, L. (2004).
Question Answering system for theFrench Language.
In: Proceedings of CLEF 2004Plamondon, L. and Foster, G. (2003) Quantum, aFrench/English Cross-Language Question Answer-ing System.
Proceedings of CLEF 2003Tanev, H., Negri, M., Magnini, B., Kouylekov, M.(2004) The Diogenes Question Answering Systemat CLEF-2004.
Proceedings of CLEF 2004Yaser Al-Onaizan and Kevin Knight.
(2002) Translat-ing Named Entities Using Monolingual and Bi-lingual Resources.
In: Proceedings of the ACLWorkshop on Computational Approaches to Se-mitic LanguagesEACL 2006 Workshop on Multilingual Question Answering - MLQA0614
