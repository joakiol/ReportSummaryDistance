Towards Measuring Scalability in Natural Language Understanding TasksRobert Porzel Rainer MalakaEuropean Media LaboratorySchloss-Wolfsbrunnenweg 33D-69118 Heidelberg, Germany{robert.porzel,rainer.malaka@eml-d.villa-bosch.de}AbstractIn this paper we present a discussion of existingmetrics for evaluation the performance of indi-vidual natural language understanding systemsand components as well as the commonly em-ployed metrics for measuring the specific taskdifficulties.
We extend and generalize the com-mon majority class baseline metric and intro-duce an general entropy-based metric for mea-suring the task difficulty of arbitrary languageunderstanding tasks.
Finally, we show an em-pirical study evaluating this metric followed bya discussion of its role in measuring the scal-ability of language understanding systems andcomponents.1 IntroductionCurrent evaluation frameworks for uni- or multi-modaldialogue systems (Walker et al, 2000; Beringer et al,2002) that allow for spoken language input do not includemetrics for measuring the accuracy of the involved inten-tion recognition systems, simply because such informa-tion is hard to extract automatically from log files.
Fur-thermore no general computational method or frameworkfor measuring the difficulty of natural language under-standing tasks have been proposed so far.
We are, there-fore, faced with a lack of methods for measuring the dif-ficulties of the individual tasks involved in the languageunderstanding process.
Such generally applicable meth-ods, however, are needed for measuring the scalability ofnatural language understanding systems and components.In this paper we first discuss existing metrics for mea-suring task-specific performances and the correspond-ing baseline metrics in natural language understandingin Section 2.
We, then, propose a generalized baseline-based metric in Section 4.1 as well as a general entropy-based metric in Section 4.2.
Both methods can be em-ployed for measuring the difficulties of various under-standing tasks and, consequently, for evaluating naturallanguage understanding components involved in the in-tention recognition process.
Section 5 provides a casestudy evaluation of the proposed methods.
In Section 6we discuss how an analysis of a specific system on tasksdiffering in their difficulty can yield a first approach formeasuring the scalability of a natural language under-standing systems and its components.2 Evaluating Dialogue-, Speech- andDiscourse Understanding SystemsIn this section we will briefly sketch out the most fre-quently used metrics for evaluating the performances ofthe relevant components and systems at hand.Evaluation of the Dialogue Systems Performance:For evaluation of the overall performance of a dia-logue system as a whole frameworks such as PAR-ADISE (Walker et al, 2000) for unimodal and PROMISE(Beringer et al, 2002) for multimodal systems have seta de facto standard.
These frameworks differentiate be-tween:?
dialogue efficiency metrics, i.e.
elapsed time,system- and user turns?
dialogue quality metrics, mean recognition scoreand absolute number as well as percentages of time-outs, rejections, helps, cancels, and barge-ins,?
task success metrics, task completion (per survey)?
user satisfaction metrics (per survey)These metrics are crucial for evaluating the aggregateperformance of the individual components, they cannot,however, determine the amount of understanding versusmisunderstanding or the system-specific a priori diffi-culty of the understanding task.
Their importance, how-ever, will remain undiminished, as ways of determiningsuch global parameters are vital to determining the aggre-gate usefulness and felicity of a system as a whole.
At thesame time individual components and ensembles thereof- such as the performance of the uni- or multi-modal in-put understanding system - need to be evaluated as wellto determine bottlenecks and weak links in the discourseunderstanding processing chain.Evaluation of the Automatic Speech Recognition Per-formance: The commonly used word error rate (WER)can be calculated by aligning any two sets word se-quences and adding the number of substitutions S, dele-tions D and insertions I .
The WER is then given by thefollowing formula where N is the total number of wordsin the test set.WER = S + D + IN ?
100Another measure of accuracy that is frequently used isthe so called Out Of Vocabulary (OOV) measure, whichrepresents the percentage of words that was not recog-nized despite their lexical coverage.
WER and OOVare commonly intertwined together with the combinedacoustic- and language-model confidence scores, whichare constituted by the posterior probabilities of the hiddenMarkov chains and n-gram frequencies.
Together thesescores enable evaluators to measure the absolute perfor-mance of a given speech recognition system.
In orderto arrive at a measure that is relative to the given task-difficulty, this difficulty must also be calculated, whichcan be done by means of measuring the perplexity of thetask (see Section 3).Evaluation of the Natural Language UnderstandingPerformance: A measure for understanding rates -called concept error rate has been proposed for exampleby Chotimongcol and Rudnicky (2001) and is designedin analogy to word error rates employed in automaticspeech recognition that are combined with keyword spot-ting systems.
Chotimongcol and Rudnicky (2001) pro-pose to differentiate whether the erroneous concept oc-curs in a non-concept slot that contains information thatis captured in the grammar but not considered relevant forselecting a system action (e.g., politeness markers, suchas please), in a value-insensitive slot whose identity, suf-fices to produce a system action (e.g., affirmatives suchas yes), or in a value-sensitive slot for which both theoccurrence and the value of the slot are important (e.g.,a goal object, such as Heidelberg).
An alternative pro-posal for concept error rates is embedded into the speechrecognition and intention spotting system by Lumenvox1,wherein two types of errors and two types of non-errorsfor concept transcriptions are proposed:?
A match when the application returned the correctconcept and an out of grammar match when the ap-1www.lomunevox.com/support/tunerhelp/Tuning/ConceptTranscription.htmplication returned no concepts, or discarded the re-turned concepts because the user failed to say anyconcept covered by the grammar.?
A grammar mismatch when the application returnedthe incorrect concept, but the user said a conceptcovered by the grammar and an out of grammar mis-match when the application returned a concept, andchose that concept as a correct interpretation, but theuser did not say a concept covered by the grammar.Neither of these measures are suitable for our pur-poses as they are known to be feasible only for context-insensitive applications that do not include discoursemodels, implicit domain-specific information and othercontextual knowledge as discussed in (Porzel et al,2004).
Therefore this measure has also been called key-word recognition rate for single utterance systems.
In ourminds another crucial shortcoming is the lack of compa-rability, as these measures do not take the general dif-ficulty of the understanding tasks into account.
Again,this has been realized in the automatic speech recogni-tion community and led to the so called perplexity mea-surements for a given speech recognition task.
We will,therefore, sketch out the commonly employed perplexitymeasurements in Section 3.The most detailed evaluation scheme for discoursecomprehension, introduced by Higashinaka et al (2002)and also extended by Higashinaka et al (2003), featuresthe metrics given in Table 2.1. slot accuracy2.
insertion error rate3.
deletion error rate4.
substitution error rate5.
slot error rate6.
update precision7.
update insertion error rate8.
update deletion error rate9.
update substitution error rate10.
speech understanding rate11.
slot accuracy for filled slots12.
deletion error rate for filled slots13.
substitution error rate for filled slotsTable 1: Discourse Comprehension MeasurementsThese metrics are combined by means of combin-ing the results of an m5 multiple linear regression al-gorithm and a support vector regression approach.
Theresulting weighted sum is compared to human intuitionsand PARADISE-like metrics concerning task completionrates and -times.
While this promising approach managesto combine factors related to speech recognition, interpre-tation and discourse modeling, there are some shortcom-ings that stem from the fact that this schema was devel-oped for single-domain systems that employ frame-basedattribute value pairs for representing the user?s intent.Recent advances in dialogue management and multi-domain systems enable approaches that are more flexi-ble than slot-filling, e.g.
using discourse pegs, dialoguegames and overlay operations for handling multiple tasksand cross-modal references (LuperFoy, 1992; Lo?ckelt etal., 2002; Pfleger et al, 2002; Alexandersson and Becker,2003).
More importantly - for the topic of this paper - nomeans of measuring the a priori discourse understandingdifficulty is given.Measuring Precision, Recall and F-Measures: In therealm of semantic analyses the task of word sense dis-ambiguation is usually regarded to be among the difficultones.
This means it can only be solved after all otherproblems involved in language understanding have beenresolved as well.
The hierarchical nature and interdepen-dencies of the various tasks are mirrored in the resultsof the corresponding competitive evaluation tracts - e.g.the message understanding conference (MUC) or SEN-SEVAL competition.
It becomes obvious that the un-graceful degradation of f-measure scores (shown in Ta-ble 2 is due to the fact that each higher-level task inheritsthe imprecisions and omissions of the previous ones, e.g.errors in the named entity recognition (NE) task cause re-call and precision declines in the template element task(TE), which, in turn, thwart successful template relationtask performance (TR) as well as the most difficult sce-nario template (ST) and co-reference task (CO).
This de-cline can be seen in Table 2 (Marsh and Perzanowski,1999).NE CO TE TR STf ?
.94 f ?
.62 f ?
.87 f ?
.76 f ?
.51Table 2: F-measure-based (?
= 0.5) evaluation results ofthe best performing systems of the 7th Message Under-standing ConferenceDespite several problems stemming from the prerequi-site to craft costly gold standards, e.g.
tree banks or an-notated test corpora, precision and recall and their weigh-able combinations in the corresponding f-measures (suchas given in Table 2), have become a de facto standard formeasuring the performance of classification and retrievaltasks (Van Rijsbergen, 1979).
Precision p states the per-centage of correctly tagged (or classified) entities of alltagged/classified entities, whereas recallr states the pos-itive percentage of entities tagged/classified as comparedto the normative amount, i.e.
those that ought to havebeen tagged or classified.
Together these are combinableto an overall f-measure score, defined as:F = 1?
1p + (1 ?
?)
1rHerein ?
can be set to reflect the respective importanceof p versus r, if ?
= 0.5 then both are weighted equally.These measures are commonly employed for evaluatingpart-of-speech tagging, shallow parsing, reference reso-lution tasks and information retrieval tasks and sub-tasks.An additional problem with this method is that mostnatural language understanding systems that performdeeper semantic analyses produce representations oftenbased on individual grammar formalisms and mark-uplanguages for which no gold standards exist.
For evaluat-ing discourse understanding systems, however, such goldstandards and annotated training corpora will continue tobe needed.3 Measuring Perplexity and BaselinesIn this section we will describe the most frequently usedmetrics for estimating the complexity of the tasks per-formed by the relevant components and systems at hand.Measuring Perplexity in Automatic Speech Recog-nition: Perplexity is a measure of the probabilityweighted average number of words that may follow af-ter a given word (Hirschman and Thompson, 1997).
Inorder to calculate the perplexity B, the entropy H needsto be given - i.e., the probability of the word sequences inthe specific language of the systemW .
The perplexity isthen defined as:H = ??
?WP (W )log2P (W )B = 2HImprovements of specific ASR systems can then con-sequently be measured by keeping the perplexity constantand measuring WER and OOV performance for recogni-tion quality and confidence scores for hypothesis verifi-cation and selection.Measuring Task-specific Baselines: Baselines forclassification or tagging tasks are commonly definedbased on chance performance, on an a posteriori com-puted majority class performance or on the performanceof an established baseline classification method such asnaive bayes, tf*idf or k-means .
That means:?
what is the corresponding f-measure, if the evalu-ated component guesses randomly - for chance per-formance metrics,?
what is the corresponding f-measure if the evaluatedcomponent always chooses the most frequent solu-tion - for majority class performance metrics,?
what is the corresponding f-measure of the estab-lisched baseline classification method.Much like kappa statistics proposed byCarletta (1996), existing employments of majorityclass baselines assume an equal set of identical poten-tial mark-ups, i.e.
attributes and their values, for allmarkables.
Therefore, they cannot be used in a straightforward manner for many tasks that involve disjunctsets of attributes and values in terms of the type andnumber of attributes and their values involved in theclassification task.
This, however, is exactly what wefind in natural language understanding tasks, such assemantic tagging or word sense disambiguation tasks(Stevenson, 2003).
Additionally, baseline computed onother methods cannot serve as a means for measuringscalability, because of the circularity involved: as onewould need a way of measuring the baseline method?sscalability factor in the first place.
Table 3 provides anoverview of the existing ways of measuring performanceand task difficulty in automatic speech recognition andunderstanding.Domain Performance Complexityautomaticspeech WER/OVV Perplexityrecognitionnaturallanguage CER noneunderstandingMUC tasks(NE, TE, TR, f-measure baselinesST, CO)unimodaldialogue PARADISE nonesystemmulitmodaldialogue PARADISE nonesystemTable 3: Summary of Measurements4 Measuring Task Difficulty4.1 Proportional Baseline RatesAs a precursor step before this we need a clear defini-tion of a natural language understanding task.
For thiswe propose to assume a MATE-like annotation point ofview, which provides a set of disjunct levels of annota-tions for the individual discriminatory decisions that canbe performed on spoken dialogue data, ranging from an-notating referring expressions, e.g.
named entities andtheir relations, anaphora and their antecedents, to wordsenses and dialogue acts.
Each task must, therefore, havea clearly defined set of markables, attributes and valuesfor each corpus of spoken dialogue data.As a first step we will propose a uniform and genericmethod for computing task-specific majority class base-lines for a given task Tw from the entire set of task, i.e.T = {T1, .
.
.
, Tz} and Tw ?
T .A gold standard annotation of a task features a finite setof markable tokens C = {c1, .
.
.
, cn} for task Tw, e.g.if n = 2 in a corpus containing only the two ambiguouslexemes bank and run as markables, i.e.
c1 and c2 respec-tively.
For a member ci of the set C we can now definethe number of values for the tagging attribute of senseas: Ai = {bi1, .
.
.
, bini}.
For example, for three senses ofthe markable bank as c1 we get the corresponding valueset A1 = {building, institution, shore} andfor run as c2 the value set A2 = {motion, storm}.Note that the value sets have markable-dependent sizes.For our toy example containing the two markables c1 forbank and c2 for run they are:bij bi1 bi2 bi3A1 building institution shoreA2 motion stormFor computing the proportional majority classes weneed to compute the occurrences of a value j for a mark-able i in a given gold standard test data set.
We call thisVij .
Now we can determine the most frequently givenvalue and its number for each markable ci as:V maxi = maxij?
{1,...,bi}VijFor example, given a marked-up toy corpus containingour ambiguous lexemes as shown below as task T1:The runstorm on the bankbuilding on Mon-day caused the bankinstitution to collapse earlythis week.
It employees can therefore nowenjoy a leisurely runmotion on the bankshoreof the Hudson river.
It is uncertain if thebankinstitution can be saved so that they canrunmotion back to their desks and resume theirwork.This results in the list of the value occurrences shownbelow with V maxi set in bold face:Vij bi1 bi2 bi3c1 1 2 1c2 2 1We define the total number of values for a markableci as:V Si =n1?j=1VijWith V maxi we define the majority classe baseline as:Bi =V maxiV SiIf we always choose the most frequent attribute for mark-able ci, the percentage of correct guesses correspomds toBi.
We can now calculate the total number of values as:V S =n?i=1V SiBased on this we can compute the task-specific propor-tional baseline for task Tw, i.e., BTw , over the entire testset as:BTw =1V S ?n?i=1V Si Bi =1V S ?n?i=1V maxiThus, BTw calculates the average of correct guessesfor the majority baseline.
Returning to our toy examplefor c1 we get V S1 = 4 and for c2 we get V S2 = 3.
Ad-ditionally, we also get different individual majority classbaselines for each markable, i.e., for c1 we get B1 = 12 ,and for c2 we get B2 = 23 .
We also get a total number ofvalues given for C (c1 and c2), i.e., V S = 7.
Now we cancompute the overall baseline BT1 as:17 ?
((4 ?12) + (3 ?23)) =47 ?
0.57If we extend the corpus by an additional ambiguity, i.e.that of the spatial and temporal readings of the lexeme on,to yield an annotated corpus such as given below as taskT2:The runstorm on the bankbuilding ontemporalMonday caused the bankinstitution to collapseearly this week.
It employees can thereforenow enjoy a leisurely runmotion onspatial thebankshore of the Hudson river.
It is uncertainif the bankinstitution can be saved so that theycan runmotion back to their desks and resumetheir work.We get the list of the value occurrences shown below:C bi1 bi2 bi3c1 1 2 1c2 2 1c3 1 1Now we can compute the overall baseline BT2 againas:19 ?
((4 ?12) + (3 ?23) + (2 ?12)) =59 ?
0.55The reduction by .02 points, in this case, indicates thata method that for each markable always chooses the mostfrequently occurring one would perform slightly worseon the second corpus as compared to the first.
Note thatthis proportional baseline measure is able to compute theperformance of such a majority class-based method onany data set for any task.
It does as such provide a pic-ture depicting a problem?s or task?s inherent difficulty, butonly if the distribution of values for the markables at handis fairly homogeneous.
However, if we assume distribu-tions of markable values such as shown below, we getidentical values for BT3 and BT4 .T3 bi1 bi2 bi3 bi4 bi5c3 16 16 0 0 0T4 bi1 bi2 bi3 bi4 bi5c4 16 4 4 4 4That is, we get:132 ?
(32 ?12) =12 = 0.5for both task baselines BT3 and BT4 with T3 featuring thetask distribution depicted as c3 and T4 that of c4, despitethe fact task T2 was undoubtedly the more difficult one.To create a more applicable measure for task difficulty -i.e.
one that also applies for cases of heterogeneous valuedistributions - we need do need to calculate an entropymetric that takes the individual value distributions intoaccount.4.2 Measuring Markable-specific EntropyAs a means of illustrating such a markable-specific en-tropy metric we can look at the value space for eachmarkable and define a minimal amount of binary deci-sions that are on average necessary for solving the prob-lem and compute what part of the problem is solved bythem.
For example, looking at the markable c4 fromabove we find that the problem can be solved by means ofthe following decisions: With one decision we can parti-tion the space between b41 and the rest (b42 through b45)thereby assigning 16 times the value b41 to c4.
This deci-sion already solves 50% of the problem.
Next we need asecond decision for partitioning the value space betweenb42 ?
b43 and b44 ?
b45 and a third for cutting between b42and b43 as well as b44 and b45 respectively.
Therefore, threedecisions are needed for assigning the value 4 to b42 andsolving 12.5% of the problem.
In the case of c4 the sameholds for b43, b44 and b45, giving us the following decisionand solution table with d(bji ) standing for the averageamount of binary decisions necessary for solving bji :T4 b41 b42 b43 b44 b45c4 16 4 4 4 4d(b4i ) 1 3 3 3 3solved 50% 12.5% 12.5% 12.5% 12.5%Looking at the markable c3 we find that the problemcan be solved as shown below:T3 b31 b32 b33 b34 b35c4 16 16 0 0 0d(b3i ) 1 1 0 0 0solved 50% 50% 0% 0% 0%As an illustrative approximation of a task?s entropywe can now compute the aggregate amount of decisionsweighted by their contribution to the overall solution(given as its probability - i.e.
50% = .5).
For c4 thisyields:2 = (1 ?
.5)+(3 ?
.125)+(3 ?
.125)+(3 ?
.125)+(3 ?
.125)And for T3 we get:1 = (1 ?
.5) + (1 ?
.5)In these cases we can now say that solving the markable-specific value distribution of c4 is more difficult thansolving that of c3, indicated by the increase of 1 pointin this quasi-entropy measure.
Note that if we had abinary decision procedure that solves f% of the casescorrectly than we get an average error rate for T4f2 of0.9 ?
0.9 = 0, 81 whereas for T3 only 0.9.After this approximate illustration of measuring taskdifficulty via the notion of its entropy, we can now com-pute a corresponding markable-specific entropy measureHci based on the standard formula:Hci = ?ni?j=1P (Vij)log2P (bij)This computation yields Hc3 = 1 and Hc4 = 2, whichalso reflects the difference in difficulty of T3 (consistingof the sole markable c3) versus T4 (consisting of the solemarkable c4).4.3 Combing Markable-specific EntropiesWe propose to apply an analogous way of combining theindividual markable-specific entropies, by a weighted av-erage, whereby the markable-specific weights are deter-mined by V Si and the averaging based on V S .
As anexample we return to our sample tasks T1 and T2.Vij bi1 bi2 bi3 Hci V Si Hci ?
V Sic1 1 2 1 1.5 4 6c2 2 1 ?
0.92 3 ?
2.76c3 1 1 1 2 2Based on this we can defineHTw as follows:HTw =?ni=1(Hci ?
V Si )V SCorrespondingly, we get for task T1 consisting ofmarkables c1 and c2 a value HT1 ?
1.25 and for task T2consisting of markables c1, c2 and c3 a value HT2 ?
1.35.In much the same way as the proportional baseline rate- only more generally applicable - this increase of 0.1points in task entropy reflects the increase in task diffi-culty from T1 to T2.
Now, that we have a clearly de-fined way of measuring task-specific difficulties - basedon their markable-specific entropies - we can evaluate ourapproach by means of a larger experiment described be-low.5 Evaluating the MetricsIn the following we will report on the results of a corpusstudy to evaluate the task-specific entropy measurementproposed above.
In our mind such a study can be per-formed in the following way: Given a marked up corpusas an evaluation gold standard we can alternate the cor-pus?
difficulty in three potential ways:?
eliminate parts of the corpus so that the number ofvalues of the individual markables is decreased, wewill call this vertical pruning;?
eliminate parts of the corpus so that the number ofthe individual values is decreased, we will call thishorizontal pruning;?
eliminate parts of the corpus so that both the numberof markables and their respective values is reduced,we will call this diagonal pruning.Since each of these procedures can increase and reducethe overall task difficulty, we can use them to test if ourproposed task entropy measure is able to reflect that ina non toy-world example.
For our study we employ theSMARTKOM (Wahlster, 2003) sense-tagged corpus em-ployed in the word sense disambiguation study reportedby (Loos and Porzel, 2004).
An overview of the mark-ables and their value distributions is given in Appendix1.We can now compute the entropy for the whole task as:HTwhole =1966.180832100 ?
0.94For the horizontal pruning we removed all markableswere a single value assumed more than 90% of the entireset.
Intuitively that makes the task harder because wetook out the easy cases which amounted to about 20% ofthe entire corpus.We can now compute the entropy for the horizontallyeased task as:HThorizontal =1718.079591548 ?
1.11For the vertical pruning we removed all values of bi3of the entire set.
Intuitively that makes the task easierbecause less decisions are necessary to solve those mark-ables that had values in bi3.HTvertical =1894.893862073 ?
0.91For the diagonal pruning we again removed all valuesof bi3 of the entire set making the task easier and removedhorizontally all markable where the majority class wasunder 60%, i.e.
the hardest cases.HTdiagonal =1729.42541936 ?
0.896 Conclusion and Future WorkWe have discussed various measures for evaluating per-formance of individual components and systems and forestimating the corresponding task complexities.
Addi-tionally, we demonstrated the feasibility to employ anentropy-based metric for tasks that are heterogeneouslystructured in terms of their markable/attribute setup aswell as attribute/value distribution.
That means it can beapplied to any corpora even if they feature disjunct at-tributes with different values of their set sizes.
In a firststudy, on such a heterogeneous task, we have shown thatthe results of this generally applicable entropy-based met-ric line up correspondingly to increases and decreases intask difficulty.In our minds this metric for measuring task difficultycan now be employed to approach the question of measur-ing scalability.
Since it is now feasible to manipulate tasksizes and difficulties in a controlled and measurable fash-ion, future experiments and studies can be performed thatdo almost exactly the opposite from current evaluationsof systems and components.
That is, instead of keep-ing the task - test corpus - identical and measuring theperformance of different methods, we can now keep themethod identical and measure its performance on tasksdiffering in their difficulty.
Hereby, some open questionstill have to be solved, such as evaluating and determiningsuitable performance measures and formalizing the spe-cific dimensions of scalability that can be measured usingthis approach, e.g.
scalability in terms of performance onproblems that are equally difficult but vary in sizeversusproblems that vary in size and difficulty to name a few.ReferencesJan Alexandersson and Tilman Becker.
2003.
The For-mal Foundations Underlying Overlay.
In Proceedingsof the Fifth International Workshop on ComputationalSemantics (IWCS-5), Tilburg, The Netherlands, Febru-ary.Nicole Beringer, Ute Kartal, Katerina Louka, FlorianSchiel, and Uli Tu?rk.
2002.
PROMISE: A Procedurefor Multimodal Interactive System Evaluation.
In Pro-ceedings of the Workshop ?Multimodal Resources andMultimodal Systems Evaluation, Las Palmas, Spain.Jean Carletta.
1996.
Assessing agreement on classifi-cation tasks: The kappa statistic.
Computational Lin-guistics, 22(2):249?254.Ananlada Chotimongcol and Alexander Rudnicky.
2001.N-best speech hypotheses reordering using linear re-gression.
In Proceedings of Eurospeech, pages 1829?1832, Aalborg, Denmark.Ryuichiro Higashinaka, Noboru Miyazaki, MikioNakano, and Kiyoaki Aikawa.
2002.
A methodfor evaluating incremental utterance understandingin spoken dialogue systems.
In Proceedings of theInternational Conference on Speech and LanguageProcessing 2002, pages 829?833, Denver, USA.Ryuichiro Higashinaka, Noboru Miyazaki, MikioNakano, and Kiyoaki Aikawa.
2003.
Evaluatingdiscourse understanding in spoken dialogue systems.In Proceedings of Eurospeech, pages 1941?1944,Geneva, Switzerland.Lynette Hirschman and Henry Thompson.
1997.Overview of evaluation in speech and natural language.In R Cole, editor, Survey of the State of the Art inHuman Language Technology.
Cambridge UniversityPress, Cambridge.Markus Lo?ckelt, Tilman Becker, Norbert Pfleger, and JanAlexandersson.
2002.
Making sense of partial.
InProceedings of the sixth workshop on the semanticsand pragmatics of dialogue (EDILOG 2002), pages101?107, Edinburgh, UK, September.Berenike Loos and Robert Porzel.
2004.
Resolutionof lexical ambiguities in spoken dialogue systems.
InProceedings of the 5th SIGdial Workshop on Discourseand Dialogue, Boston, USA.
Sumitted.Susann LuperFoy.
1992.
The representation of multi-modal user interface dialogues using discourse pegs.In Proceedings of the 30th Annual Meeting of the Asso-ciation for Computational Linguistics, Newark, Del.,28 June ?
2 July 1992, pages 22?31.Elaine Marsh and Dennis Perzanowski.
1999.
MUC-7evaluation of IE technology: Overview of results.
InProceedings of the 7th Message Understanding Con-ference.
Morgan Kaufman Publishers.Norbert Pfleger, Jan Alexandersson, and Tilman Becker.2002.
Scoring functions for overlay and their ap-plication in discourse processing.
In KONVENS-02,Saarbru?cken, September ?
October.Robert Porzel, Iryna Gurevych, and Rainer Malaka.2004.
In context: Integrating domain- and situation-specific knowledge.
In W. Wahlster, editor,SmartKom- Foundations of Multimodal Dialogue Systems.Sprigner, Berlin.Mark Stevenson.
2003.
Word Sense Disambiguation:The Case for Combining Knowldge Sources.
CSLI.C.
J.
Van Rijsbergen.
1979.
Information Retrieval, 2ndedition.
Dept.
of Computer Science, University ofGlasgow.Wolfgang Wahlster.
2003.
SmartKom: Symmetric mul-timodality in an adaptive an reusable dialog shell.
InProceedings of the Human Computer Interaction Sta-tus Conference, Berlin, Germany.Marilyn A. Walker, Candace A. Kamm, and Diane J. Lit-man.
2000.
Towards developing general model of us-ability with PARADISE.
Natural Language Engeneer-ing, 6.Appendix 1C V Si bi1 bi2 bi3 bi4altstadt 7 42.86 0 0 57.14am 29 24.14 24.14 0 51.72an 27 0 7.41 0 92.59auf 43 0 6.98 9.30 83.72aus 27 0 14.81 0 85.19bin 37 56.76 10.81 0 32.43bis 10 40.00 50.00 0 10.00ein 87 0 1.15 0 98.85ersten 7 0 14.29 0 85.71geben 15 0 93.33 0 6.67gibt 111 91.89 0 0 8.11kirche 8 12.50 0 62.50 25.00htte 25 0 92.00 0 8.00in 214 70.56 12.15 0 17.29ins 74 94.59 0 0 5.41is 177 22.03 0.56 0 77.40ist 55 18.18 1.81 0 80.00kann 98 0 69.39 0 30.61kino 226 39.82 44.69 0 15.49kirche 6 16.67 66.67 0 16.67kommen 6 50.00 16.67 0 33.33kommt 31 0 74.19 0 25.81laufen 16 6.25 81.25 0 12.50luft 49 0 95.92 0 4.08mchte 149 97.99 0 0 2.01nach 38 36.84 23.68 0 39.47nehmen 12 0 41.67 0 58.33schlo 61 21.31 27.87 26.23 24.59schlsser 2 0 50 0 50sind 28 17.86 0 0 82.14um 50 76.00 0 0 24.00vom 26 26.92 0 0 73.08von 82 2.44 30.49 0 67.07vor 4 0 50.00 0 50.00war 14 21.43 0 0 78.57welch 3 0 33.33 0 66.67welche 25 0 88.00 0 12.00will 21 90.48 0 0 9.52zeig 26 73.08 0 0 26.92zeige 7 85.71 0 0 14.29zeigen 27 59.26 14.81 0 25.93zu 85 0 10.59 0 89.41zum 55 47.27 0 0 52.7222Horizontal single lines indicate removed markables in hor-izontal pruning, vertical single lines indicate removed values invertical and diagonal pruning and horizontal double lines indi-cate removed markables in diagonal pruning.
