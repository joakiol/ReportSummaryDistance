Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 1?9,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Multimodal Home Entertainment Interface via a Mobile DeviceAlexander Gruenstein Bo-June (Paul) Hsu James Glass Stephanie SeneffLee Hetherington Scott Cyphers Ibrahim Badr Chao Wang Sean LiuMIT Computer Science and Artificial Intelligence Laboratory32 Vassar St, Cambridge, MA 02139 USAhttp://www.sls.csail.mit.edu/AbstractWe describe a multimodal dialogue system forinteracting with a home entertainment centervia a mobile device.
In our working proto-type, users may utilize both a graphical andspeech user interface to search TV listings,record and play television programs, and listento music.
The developed framework is quitegeneric, potentially supporting a wide varietyof applications, as we demonstrate by integrat-ing a weather forecast application.
In the pro-totype, the mobile device serves as the locusof interaction, providing both a small touch-screen display, and speech input and output;while the TV screen features a larger, richerGUI.
The system architecture is agnostic tothe location of the natural language process-ing components: a consistent user experienceis maintained regardless of whether they runon a remote server or on the device itself.1 IntroductionPeople have access to large libraries of digital con-tent both in their living rooms and on their mobiledevices.
Digital video recorders (DVRs) allow peo-ple to record TV programs from hundreds of chan-nels for subsequent viewing at home?or, increas-ingly, on their mobile devices.
Similarly, havingaccumulated vast libraries of digital music, peopleyearn for an easy way to sift through them from thecomfort of their couches, in their cars, and on the go.Mobile devices are already central to accessingdigital media libraries while users are away fromhome: people listen to music or watch video record-ings.
Mobile devices also play an increasingly im-portant role in managing digital media libraries.
Forinstance, a web-enabled mobile phone can be used toremotely schedule TV recordings through a web siteor via a custom application.
Such management tasksoften prove cumbersome, however, as it is challeng-ing to browse through listings for hundreds of TVchannels on a small display.
Indeed, even on a largescreen in the living room, browsing alphabetically,or by time and channel, for a particular show usingthe remote control quickly becomes unwieldy.Speech and multimodal interfaces provide a nat-ural means of addressing many of these challenges.It is effortless for people to say the name of a pro-gram, for instance, in order to search for existingrecordings.
Moreover, such a speech browsing ca-pability is useful both in the living room and awayfrom home.
Thus, a natural way to provide speech-based control of a media library is through the user?smobile device itself.In this paper we describe just such a prototypesystem.
A mobile phone plays a central role in pro-viding a multimodal, natural language interface toboth a digital video recorder and a music library.Users can interact with the system?presented as adynamic web page on the mobile browser?usingthe navigation keys, the stylus, or spoken naturallanguage.
In front of the TV, a much richer GUI isalso available, along with support for playing videorecordings and music.In the prototype described herein, the mobile de-vice serves as the locus of natural language in-teraction, whether a user is in the living room orwalking down the street.
Since these environmentsmay be very different in terms of computational re-1sources and network bandwidth, it is important thatthe architecture allows for multiple configurations interms of the location of the natural language pro-cessing components.
For instance, when a deviceis connected to a Wi-Fi network at home, recogni-tion latency may be reduced by performing speechand natural language processing on the home me-dia server.
Moreover, a powerful server may enablemore sophisticated processing techniques, such asmultipass speech recognition (Hetherington, 2005;Chung et al, 2004), for improved accuracy.
In sit-uations with reduced network connectivity, latencymay be improved by performing speech recognitionand natural language processing tasks on the mobiledevice itself.
Given resource constraints, however,less detailed acoustic and language models may berequired.
We have developed just such a flexible ar-chitecture, with many of the natural language pro-cessing components able to run on either a server orthe mobile device itself.
Regardless of the configu-ration, a consistent user experience is maintained.2 Related WorkVarious academic researchers and commercial busi-nesses have demonstrated speech-enabled interfacesto entertainment centers.
A good deal of the workfocuses on adding a microphone to a remote con-trol, so that speech input may be used in additionto a traditional remote control.
Much commercialwork, for example (Fujita et al, 2003), tends to fo-cus on constrained grammar systems, where speechinput is limited to a small set of templates corre-sponding to menu choices.
(Berglund and Johans-son, 2004) present a remote-control based speechinterface for navigating an existing interactive tele-vision on-screen menu, though experimenters man-ually transcribed user utterances as they spoke in-stead of using a speech recognizer.
(Oh et al, 2007)present a dialogue system for TV control that makesuse of concept spotting and statistical dialogue man-agement to understand queries.
A version of theirsystem can run independently on low-resource de-vices such as PDAs; however, it has a smaller vo-cabulary and supports a limited set of user utterancetemplates.
Finally, (Wittenburg et al, 2006) lookmainly at the problem of searching for televisionprograms using speech, an on-screen display, and aremote control.
They explore a Speech-In List-Outinterface to searching for episodes of television pro-grams.
(Portele et al, 2003) depart from the model ofadding a speech interface component to an exist-ing on-screen menu.
Instead, they a create a tabletPC interface to an electronic program guide, thoughthey do not use the television display as well.
Usersmay search an electronic program guide using con-straints such as date, time, and genre; however, theycan?t search by title.
Users can also perform typi-cal remote-control tasks like turning the televisionon and off, and changing the channel.
(Johnston etal., 2007) also use a tablet PC to provide an inter-face to television content?in this case a database ofmovies.
The search can be constrained by attributessuch as title, director, or starring actors.
The tabletPC pen can be used to handwrite queries and to pointat items (such as actor names) while the user speaks.We were also inspired by previous prototypes inwhich mobile devices have been used in conjunc-tion with larger, shared displays.
For instance, (Paeket al, 2004) demonstrate a framework for buildingsuch applications.
The prototype we demonstratehere fits into their ?Jukebox?
model of interaction.Interactive workspaces, such as the one described in(Johanson et al, 2002), also demonstrate the utilityof integrating mobile and large screen displays.
Ourprototype is a departure from these systems, how-ever, in that it provides for spoken interactions.Finally, there is related work in the use of mobiledevices for various kinds of search.
For instance, of-ferings from Microsoft (Acero et al, 2008), Vlingo,1and Promptu2 allow users to search for items likebusinesses and songs using their mobile phones.These applications differ from ours in that speechis used only for search, without any accompanyingcommand and control capabilities.
Also, these ser-vices do not allow interaction with your own de-vices at home.
Efforts have been made to use mo-bile devices for control of devices in the home, suchas in (Nichols and Myers, 2006), however these ef-forts have not involved the use of speech as an inputmodality.1http://www.vlingo.com2http://www.promptu.com2Navigationpad or styluscanbeusedtonavigateGUIStatusiconindicates connectiontoserver,microphone icon when listeningHold-to-talkbuttonOpera webbrowserSynchronizedselection on mobileandtelevisiondisplaysFirefoxweb browserTabsalloweasyswitchingamong domainsMediaplaybackcontrolsFigure 1: User interface overview.
(a) Program Guide(b) Scheduled Recordings(c) Music LibraryU: What?s on tonight at seven o?clock?S: I found seventy entries.
[Displays program guide.
]U: Are there any action movies on HBO tomorrow?S: I found two entries.
[Displays results.
]U: When is American Idol on?S: I found three entries.
[Displays results.
]U: Record the first one.S: I have added American Idol to the list of programs to berecorded.
[Displays recording settings.
]U: Please record Nova on Wednesday night.S: I have added Nova to the list of programs to be recorded.
[Displays recording settings.
]U: Show all my scheduled recordings.S: There are seven scheduled recordings.
[Shows list.
]U: Delete BBC World News.S: I have deleted the scheduled recording: BBC WorldNews.
[Updates List.
]U: Show my videos.S: [Displays list of videos.
]U: Delete Frasier.S: I have deleted the recording Frasier.
[Updates list.
]U: Play Grey?s Anatomy.S: Playing Grey?s Anatomy.
[Plays recording.
]U: Stop Playing.S: Stopping.
[Stops playback.
]U: [Clicks on Music tab.
]S: [Displays artist list.
]U: Show albums by Billie Holiday.S: I found three albums by Billie Holiday.
[Shows albums.
]U: Please play A Hard Rain?s A-Gonna Fall by Bob Dylan.S: Playing A Hard Rain?s A-Gonna Fall.
[Plays song.
]Figure 2: Screenshots and an example interaction.
Utterances are labeled with U for user and S for system.33 User ExperienceOur current prototype system implements the basicfunctionalities that one expects from a home enter-tainment center.
Users can navigate through andrecord programs from the television?s electronic pro-gram guide, manage recording settings, and playrecorded videos.
They can also browse and listen toselections from their music libraries.
However, un-like existing prototypes, ours employs a smartphonewith a navigation pad, touch-sensitive screen, andbuilt-in microphone as the remote control.
Figure 1provides an overview of the graphical user interfaceon both the TV and mobile device.Mirroring the TV?s on-screen display, the proto-type system presents a reduced view on the mobiledevice with synchronized cursors.
Users can navi-gate the hierarchical menu structure using the arrowkeys or directly click on the target item with the sty-lus.
While away from the living room, or when arecording is playing full screen, users can browseand manage their media libraries using only the mo-bile device.While the navigation pad and stylus are great forbasic navigation and control, searching for mediawith specific attributes, such as title, remains cum-bersome.
To facilitate such interactions, the cur-rent system supports spoken natural language inter-actions.
For example, the user can press the hold-to-talk button located on the side of the mobile deviceand ask ?What?s on the National Geographic Chan-nel this afternoon??
to retrieve a list of shows withthe specified channel and time.
The system respondswith a short verbal summary ?I found six entries onJanuary seventh?
and presents the resulting list onboth the TV and mobile displays.
The user can thenbrowse the list using the navigation pad or press thehold-to-talk button to barge in with another com-mand, e.g.
?Please record the second one.?
Depress-ing the hold-to-talk button not only terminates anycurrent spoken response, but also mutes the TV tominimize interference with speech recognition.
Asthe previous example demonstrates, contextual in-formation is used to resolve list position referencesand disambiguate commands.The speech interface to the user?s music libraryworks in a similar fashion.
Users can search byartist, album, and song name, and then play thesongs found.
To demonstrate the extensibility ofthe architecture, we have also integrated an exist-ing weather information system (Zue et al, 2000),which has been previously deployed as a telephonyapplication.
Users simply click on the Weather tabto switch to this domain, allowing them to ask a widerange of weather queries.
The system responds ver-bally and with a simple graphical forecast.To create a natural user experience, we designedthe multimodal interface to allow users to seam-lessly switch among the different input modalitiesavailable on the mobile device.
Figure 2 demon-strates an example interaction with the prototype, aswell as several screenshots of the user interface.4 System ArchitectureThe system architecture is quite flexible with re-gards to the placement of the natural language pro-cessing components.
Figure 3 presents two possibleconfigurations of the system components distributedacross the mobile device, home media server, andTV display.
In 3(a), all speech recognition and nat-ural language processing components reside on theserver, with the mobile device acting as the micro-phone, speaker, display, and remote control.
In 3(b),the speech recognizer, language understanding com-ponent, language generation component, and text-to-speech (TTS) synthesizer run on the mobile de-vice.
Depending on the capabilities of the mobiledevice and network connection, different configu-rations may be optimal.
For instance, on a power-ful device with slow network connection, recogni-tion latency may be reduced by performing speechrecognition and natural language processing on thedevice.
On the other hand, streaming audio via a fastwireless network to the server for processing mayresult in improved accuracy.In the prototype system, flexible and reusablespeech recognition and natural language processingcapabilities are provided via generic components de-veloped and deployed in numerous spoken dialoguesystems by our group, with the exception of an off-the-shelf speech synthesizer.
Speech input from themobile device is recognized using the landmark-based SUMMIT system (Glass, 2003).
The result-ing N-best hypotheses are processed by the TINAlanguage understanding component (Seneff, 1992).4GalaxySpeech?RecognizerLanguage?UnderstandingDialogue?ManagerLanguage?GenerationText?To?SpeechWeb?ServerTelevision Web?BrowserMedia?PlayerTV?Guide?MediaMobile?DeviceWeb?BrowserHome?Media?ServerWeatherMobile?ManagerAudio?Input?/?OutputGalaxyDialogue?ManagerWeb?Server Television Web?BrowserMedia?PlayerTV?Guide?MediaMobile?DeviceWeb?BrowserHome?Media?ServerWeatherMobile?ManagerAudio?Input?/?OutputSpeech?RecognizerLanguage?GenerationText?To?SpeechLanguage?Understanding(a) (b)Figure 3: Two architecture diagrams.
In (a) speech recognition and natural language processing occur on the server,while in (b) processing is primarily performed on the device.Based on the resulting meaning representation, thedialogue manager (Polifroni et al, 2003) incorpo-rates contextual information (Filisko and Seneff,2003), and then determines an appropriate response.The response consists of an update to the graph-ical display, and a spoken system response whichis realized via the GENESIS (Baptist and Seneff,2000) language generation module.
To support on-device processing, all the components are linked viathe GALAXY framework (Seneff et al, 1998) withan additional Mobile Manager component responsi-ble for coordinating the communication between themobile device and the home media server.In the currently deployed system, we use a mo-bile phone with a 624 MHz ARM processor run-ning the Windows Mobile operating system andOpera Mobile web browser.
The TV program andmusic databases reside on the home media serverrunning GNU/Linux.
The TV program guide dataand recording capabilities are provided via MythTV,a full-featured, open-source digital video recordersoftware package.3 Daily updates to the programguide information typically contain hundreds ofunique channel names and thousands of unique pro-gram names.
The music library is comprised of5,000 songs from over 80 artists and 13 major gen-res, indexed using the open-source text search en-gine Lucene.4 Lastly, the TV display can be drivenby a web browser on either the home media server ora separate computer connected to the server via a fastEthernet connection, for high quality video stream-ing.3http://www.mythtv.org/4http://lucene.apache.org/While the focus of this paper is on the natural lan-guage processing and user interface aspects of thesystem, our work is actually situated within a largercollaborative project at MIT that also includes sim-plified device configuration (Mazzola Paluska et al,2008; Mazzola Paluska et al, 2006), transparent ac-cess to remote servers (Ford et al, 2006), and im-proved security.5 Mobile Natural Language ComponentsPorting the implementation of the various speechrecognizer and natural language processing com-ponents to mobile devices with limited computa-tion and memory presents both a research and en-gineering challenge.
Instead of creating a small vo-cabulary, fixed phrase dialogue system, we aim tosupport?on the mobile device?the same flexibleand natural language interactions currently availableon our desktop, tablet, and telephony systems; seee.g., (Gruenstein et al, 2006; Seneff, 2002; Zue etal., 2000).
In this section, we summarize our ef-forts thus far in implementing the SUMMIT speechrecognizer and TINA natural language parser.
Portsof the GENESIS language generation system and ofour dialogue manager are well underway, and we ex-pect to have these components working on the mo-bile device in the near future.5.1 PocketSUMMITTo significantly reduce the memory footprint andoverall computation, we chose to reimplement oursegment-based speech recognizer from scratch, uti-lizing fixed-point arithmetic, parameter quantiza-tion, and bit-packing in the binary model files.The resulting PocketSUMMIT recognizer (Hether-5ington, 2007) utilizes only the landmark features,initially forgoing segment features such as phoneticduration, as they introduce algorithmic complexitiesfor relatively small word error rate (WER) improve-ments.In the current system, we quantize the mean andvariance of each Gaussian mixture model dimensionto 5 and 3 bits, respectively.
Such quantization notonly results in an 8-fold reduction in model size, butalso yields about a 50% speedup by enabling tablelookups for Gaussian evaluations.
Likewise, in thefinite-state transducers (FSTs) used to represent thelanguage model, lexical, phonological, and class di-phone constraints, quantizing the FST weights andbit-packing not only compress the resulting binarymodel files, but also reduce the processing time withimproved processor cache locality.In the aforementioned TV, music, and weather do-mains with a moderate vocabulary of a few thou-sand words, the resulting PocketSUMMIT recog-nizer performs in approximately real-time on 400-600 MHz ARM processors, using a total of 2-4MB of memory, including 1-2 MB for memory-mapped model files.
Compared with equivalent non-quantized models, PocketSUMMIT achieves dra-matic improvements in speed and memory whilemaintaining comparable WER performance.5.2 PocketTINAPorting the TINA natural language parser to mobiledevices involved significant software engineering toreduce the memory and computational requirementsof the core data structures and algorithms.
TINAutilizes a best-first search that explores thousands ofpartial parses when processing an input utterance.To efficiently manage memory allocation given theunpredictability of pruning invalid parses (e.g.
dueto subject-verb agreement), we implemented a markand sweep garbage collection mechanism.
Com-bined with a more efficient implementation of thepriority queue and the use of aggressive ?beam?pruning, the resulting PocketTINA system providesidentical output as server-side TINA, but can parsea 10-best recognition hypothesis list into the corre-sponding meaning representation in under 0.1 sec-onds, using about 2 MB of memory.6 Rapid Dialogue System DevelopmentOver the course of developing dialogue systems formany domains, we have built generic natural lan-guage understanding components that enable therapid development of flexible and natural spoken di-alogue systems for novel domains.
Creating suchprototype systems typically involves customizingthe following to the target domain: recognizer lan-guage model, language understanding parser gram-mar, context resolution rules, dialogue managementcontrol script, and language generation rules.Recognizer Language Model Given a new do-main, we first identify a set of semantic classeswhich correspond to the back-end application?sdatabase, such as artist, album, and genre.
Ideally,we would have a corpus of tagged utterances col-lected from real users.
However, when building pro-totypes such as the one described here, little or notraining data is usually available.
Thus, we createa domain-specific context-free grammar to generatea supplemental corpus of synthetic utterances.
Thecorpus is used to train probabilities for the naturallanguage parsing grammar (described immediatelybelow), which in turn is used to derive a class n-gram language model (Seneff et al, 2003).Classes in the language model which corre-spond to contents of the database are marked asdynamic, and are populated at runtime from thedatabase (Chung et al, 2004; Hetherington, 2005).Database entries are heuristically normalized intospoken forms.
Pronunciations not in our 150,000word lexicon are automatically generated (Seneff,2007).Parser Grammar The TINA parser uses a prob-abilistic context-free grammar enhanced with sup-port for wh-movement and grammatical agreementconstraints.
We have developed a generic syntac-tic grammar by examining hundreds of thousandsof utterances collected from real user interactionswith various existing dialogue systems.
In addition,we have developed libraries which parse and inter-pret common semantic classes like dates, times, andnumbers.
The grammar and semantic libraries pro-vide good coverage for spoken dialogue systems indatabase-query domains.6To build a grammar for a new domain, a devel-oper extends the generic syntactic grammar by aug-menting it with domain-specific semantic categoriesand their lexical entries.
A probability model whichconditions each node category on its left sibling andparent is then estimated from a training corpus ofutterances (Seneff et al, 2003).At runtime, the recognizer tags the hypothesizeddynamic class expansions with their class names,allowing the parser grammar to be independent ofthe database contents.
Furthermore, each semanticclass is designated either as a semantic entity, or asan attribute associated with a particular entity.
Thisenables the generation of a semantic representationfrom the parse tree.Dialogue Management & Language GenerationOnce an utterance is recognized and parsed, themeaning representation is passed to the context res-olution and dialogue manager component.
The con-text resolution module (Filisko and Seneff, 2003)applies generic and domain-specific rules to re-solve anaphora and deixis, and to interpret frag-ments and ellipsis in context.
The dialogue man-ager then interacts with the application back-endand database, controlled by a script customized forthe domain (Polifroni et al, 2003).
Finally, theGENESIS module (Baptist and Seneff, 2000) ap-plies domain-specific rules to generate a natural lan-guage representation of the dialogue manager?s re-sponse, which is sent to a speech synthesizer.
Thedialogue manager also sends an update to the GUI,so that, for example, the appropriate database searchresults are displayed.7 Mobile Design ChallengesDialogue systems for mobile devices present aunique set of design challenges not found in tele-phony and desktop applications.
Here we describesome of the design choices made while developingthis prototype, and discuss their tradeoffs.7.1 Client/Server TradeoffsTowards supporting network-less scenarios, we havebegun porting various natural language processingcomponents to mobile platforms, as discussed inSection 5.
Having efficient mobile implementationsfurther allows the natural language processing tasksto be performed on either the mobile device or theserver.
While building the prototype, we observedthat the Wi-Fi network performance can often be un-predictable, resulting in erratic recognition latencythat occasionally exceeds on-device recognition la-tency.
However, utilizing the mobile processor forcomputationally intensive tasks rapidly drains thebattery.
Currently, the component architecture in theprototype system is pre-configured.
A more robustimplementation would dynamically adjust the con-figuration to optimize the tradeoffs among networkuse, CPU utilization, power consumption, and user-perceived latency/accuracy.7.2 Speech User InterfaceAs neither open-mic nor push-to-talk with automaticendpoint detection is practical on mobile deviceswith limited battery life, our prototype system em-ploys a hold-to-talk hardware button for microphonecontrol.
To guide users to speak commands onlywhile the button is depressed, a short beep is playedas an earcon both when the button is pushed andreleased.
Since users are less likely to talk overshort audio clips, the use of earcons mitigates thetendency for users to start speaking before pushingdown the microphone button.In the current system, media audio is played overthe TV speakers, whereas TTS output is sent tothe mobile device speakers.
To reduce backgroundnoise captured from the mobile device?s far-field mi-crophone, the TV is muted while the microphonebutton is depressed.
Unlike telephony spoken di-alogue systems where the recognizer has to con-stantly monitor for barge-in, the use of a hold-to-talk button significantly simplifies barge-in support,while reducing power consumption.7.3 Graphical User InterfaceIn addition to supporting interactive natural lan-guage dialogues via the spoken user interface, theprototype system implements a graphical user in-terface (GUI) on the mobile device to supplementthe TV?s on-screen interface.
To faciliate rapid pro-totyping, we chose to implement both the mobileand TV GUI using web pages with AJAX (Asyn-chronous Javascript and XML) techniques, an ap-proach we have leveraged in several existing mul-timodal dialogue systems, e.g.
(Gruenstein et al,72006; McGraw and Seneff, 2007).
The resulting in-terface is largely platform-independent and allowsdisplay updates to be ?pushed?
to the client browser.As many users are already familiar with the TV?son-screen interface, we chose to mirror the same in-terface on the mobile device and synchronize theselection cursor.
However, unlike desktop GUIs,mobile devices are constrained by a small display,limited computational power, and reduced networkbandwidth.
Thus, both the page layout and infor-mation detail were adjusted for the mobile browser.Although AJAX is more responsive than traditionalweb technology, rendering large formatted pages?such as the program guide grid?is often still un-acceptably slow.
In the current implementation, weaddressed this problem by displaying only the firstsection of the content and providing a ?Show More?button that downloads and renders the full content.While browser-based GUIs expedite rapid prototyp-ing, deployed systems may want to take advantageof native interfaces specific to the device for moreresponsive user interactions.
Instead of limiting themobile interface to reflect the TV GUI, improved us-ability may be obtained by designing the interfacefor the mobile device first and then expanding thevisual content to the TV display.7.4 Client/Server CommunicationIn the current prototype, communication betweenthe mobile device and the media server consists ofAJAX HTTP and XML-RPC requests.
To enableserver-side ?push?
updates, the client periodicallypings the server for messages.
While such an im-plementation provides a responsive user interface, itquickly drains the battery and is not robust to net-work outages resulting from the device being movedor switching to power-saving mode.
Reestablish-ing connection with the server further introduces la-tency.
In future implementations, we would like toexamine the use of Bluetooth for lower power con-sumption, and infrared for immediate response tocommon controls and basic navigation.8 Conclusions & Future WorkWe have presented a prototype system that demon-strates the feasibility of deploying a multimodal,natural language interface on a mobile device forbrowsing and managing one?s home media library.In developing the prototype, we have experimentedwith a novel role for a mobile device?that of aspeech-enabled remote control.
We have demon-strated a flexible natural language understanding ar-chitecture, in which various processing stages maybe performed on either the server or mobile device,as networking and processing power considerationsrequire.While the mobile platform presents many chal-lenges, it also provides unique opportunities.Whereas desktop computers and TV remote controlstend to be shared by multiple users, a mobile deviceis typically used by a single individual.
By collect-ing and adapting to the usage data, the system canpersonalize the recognition and understanding mod-els to improve the system accuracy.
In future sys-tems, we hope to not only explore such adaptationpossibilities, but also study how real users interactwith the system to further improve the user interface.AcknowledgmentsThis research is sponsored by the TParty Project,a joint research program between MIT and QuantaComputer, Inc.; and by Nokia, as part of a joint MIT-Nokia collaboration.
We are also thankful to threeanonymous reviewers for their constructive feed-back.ReferencesA.
Acero, N. Bernstein, R. Chambers, Y. C. Jui, X. Li,J.
Odell, P. Nguyen, O. Scholz, and G. Zweig.
2008.Live search for mobile: Web services by voice on thecellphone.
In Proc.
of ICASSP.L.
Baptist and S. Seneff.
2000.
Genesis-II: A versatilesystem for language generation in conversational sys-tem applications.
In Proc.
of ICSLP.A.
Berglund and P. Johansson.
2004.
Using speech anddialogue for interactive TV navigation.
Universal Ac-cess in the Information Society, 3(3-4):224?238.G.
Chung, S. Seneff, C. Wang, and L. Hetherington.2004.
A dynamic vocabulary spoken dialogue inter-face.
In Proc.
of INTERSPEECH, pages 327?330.E.
Filisko and S. Seneff.
2003.
A context resolutionserver for the GALAXY conversational systems.
InProc.
of EUROSPEECH.B.
Ford, J. Strauss, C. Lesniewski-Laas, S. Rhea,F.
Kaashoek, and R. Morris.
2006.
Persistent personalnames for globally connected mobile devices.
In Pro-ceedings of the 7th USENIX Symposium on OperatingSystems Design and Implementation (OSDI ?06).8K.
Fujita, H. Kuwano, T. Tsuzuki, and Y. Ono.
2003.A new digital TV interface employing speech recog-nition.
IEEE Transactions on Consumer Electronics,49(3):765?769.J.
Glass.
2003.
A probabilistic framework for segment-based speech recognition.
Computer Speech and Lan-guage, 17:137?152.A.
Gruenstein, S. Seneff, and C. Wang.
2006.
Scalableand portable web-based multimodal dialogue interac-tion with geographical databases.
In Proc.
of INTER-SPEECH.I.
L. Hetherington.
2005.
A multi-pass, dynamic-vocabulary approach to real-time, large-vocabularyspeech recognition.
In Proc.
of INTERSPEECH.I.
L. Hetherington.
2007.
PocketSUMMIT: Small-footprint continuous speech recognition.
In Proc.
ofINTERSPEECH, pages 1465?1468.B.
Johanson, A.
Fox, and T. Winograd.
2002.
The in-teractive workspaces project: Experiences with ubiq-uitous computing rooms.
IEEE Pervasive Computing,1(2):67?74.M.
Johnston, L. F. D?Haro, M. Levine, and B. Renger.2007.
A multimodal interface for access to content inthe home.
In Proc.
of ACL, pages 376?383.J.
Mazzola Paluska, H. Pham, U. Saif, C. Terman, andS.
Ward.
2006.
Reducing configuration overhead withgoal-oriented programming.
In PerCom Workshops,pages 596?599.
IEEE Computer Society.J.
Mazzola Paluska, H. Pham, U. Saif, G. Chau, C. Ter-man, and S. Ward.
2008.
Structured decomposition ofadapative applications.
In Proc.
of 6th IEEE Confer-ence on Pervasive Computing and Communications.I.
McGraw and S. Seneff.
2007.
Immersive second lan-guage acquisition in narrow domains: A prototype IS-LAND dialogue system.
In Proc.
of the Speech andLanguage Technology in Education Workshop.J.
Nichols and B.
A. Myers.
2006.
Controlling home andoffice appliances with smartphones.
IEEE PervasiveComputing, special issue on SmartPhones, 5(3):60?67, July-Sept.H.-J.
Oh, C.-H. Lee, M.-G. Jang, and Y. K. Lee.
2007.An intelligent TV interface based on statistical dia-logue management.
IEEE Transactions on ConsumerElectronics, 53(4).T.
Paek, M. Agrawala, S. Basu, S. Drucker, T. Kristjans-son, R. Logan, K. Toyama, and A. Wilson.
2004.
To-ward universal mobile interaction for shared displays.In Proc.
of Computer Supported Cooperative Work.J.
Polifroni, G. Chung, and S. Seneff.
2003.
Towardsthe automatic generation of mixed-initiative dialoguesystems from web content.
In Proc.
EUROSPEECH,pages 193?196.T.
Portele, S. Goronzy, M. Emele, A. Kellner, S. Torge,and J. te Vrugt.
2003.
SmartKom-Home - an ad-vanced multi-modal interface to home entertainment.In Proc.
of INTERSPEECH.S.
Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, andV.
Zue.
1998.
GALAXY-II: A reference architecturefor conversational system development.
In Proc.
IC-SLP.S.
Seneff, C. Wang, and T. J. Hazen.
2003.
Automatic in-duction of n-gram language models from a natural lan-guage grammar.
In Proceedings of EUROSPEECH.S.
Seneff.
1992.
TINA: A natural language systemfor spoken language applications.
Computational Lin-guistics, 18(1):61?86.S.
Seneff.
2002.
Response planning and generation inthe MERCURY flight reservation system.
ComputerSpeech and Language, 16:283?312.S.
Seneff.
2007.
Reversible sound-to-letter/letter-to-sound modeling based on syllable structure.
In Proc.of HLT-NAACL.K.
Wittenburg, T. Lanning, D. Schwenke, H. Shubin, andA.
Vetro.
2006.
The prospects for unrestricted speechinput for TV content search.
In Proc.
of AVI?06.V.
Zue, S. Seneff, J.
Glass, J. Polifroni, C. Pao, T. J.Hazen, and L. Hetherington.
2000.
JUPITER: Atelephone-based conversational interface for weatherinformation.
IEEE Transactions on Speech and AudioProcessing, 8(1), January.9
