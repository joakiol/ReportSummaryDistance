Prosodic Cues to Discourse Segment Boundaries in Human-ComputerDialogueGina-Anne LevowUniversity of Chicagolevow@cs.uchicago.eduAbstractTheories of discourse structure hypothesize ahierarchical structure of discourse segments,typically tree-structured.
While substantialwork has been done on identifying and auto-matically recognizing the textual and prosodiccorrelates of discourse structure in mono-logue, comparable cues for dialogue or multi-party conversation, and in particular human-computer dialogue remain relatively less stud-ied.
In this paper, we explore prosodiccues to discourse segmentation in human-computer dialogue.
Using data drawn from60 hours of interactions with a voice-onlyconversational spoken language system, weidentify pitch and intensity features that sig-nal segment boundaries.
Specifically, basedon 473 pairs of segment-final and segment-initiating utterances, we find significant in-creases for segment-initial utterances in max-imum pitch, average pitch, and average inten-sity, while segment-final utterances show sig-nificantly lower minimum pitch.
These resultssuggest that even in the artificial environmentof human-computer dialogue, prosodic cues ro-bustly signal discourse segment structure, com-parably to the contrastive uses of pitch and am-plitude identified in natural monologues.Keywords Dialogue Systems, Discourse structure,Prosody in understanding1 IntroductionContemporary theories of discourse, both computationaland descriptive, postulate a tree-structured hierarchicalmodel of discourse.
These structures may be viewed ascorresponding to?intentional?
structure of discourse seg-ment purposes in the view of (Grosz and Sidner, 1986),to plan and subplan structure directly in the view of(Allen and Litman, 1990) , to nuclei and satellite rhetori-cal relations in the Rhetorical Structure Theory of (Mannand Thompson, 1987), or to information structures as in(Traum and Hinkelman, 1992).
Despite this diversity ofviews on the sources of structural organization, these the-ories agree on the decomposition of discourse into seg-ments and subsegments in a hierarchical structure.Discourse segments help to establish the domain of in-terpretation for referents or anaphors.
(Grosz, 1977) Dis-course segmentation can also provide guidance for sum-marization or retrieval by identifying the topical structureof extended text spans.
As a result, an understanding ofthe mechanisms that signal discourse structure is highlydesirable.While substantial work has been done on identifyingand automatically recognizing the textual and prosodiccorrelates of discourse structure in monologue, compa-rable cues for dialogue or multi-party conversation, andin particular human-computer dialogue remain relativelyless studied.
In this paper, we explore prosodic cues todiscourse segmentation in human-computer dialogue.Using data from 60 hours of interactions with a voice-only conversational spoken language system, we identifypitch and intensity features that signal segment bound-aries.
Specifically, based on 473 pairs of segment-finaland segment-initiating utterances, we find significant in-creases for segment-initial utterances in maximum andaverage pitch and average intensity, with significantlylower minimum pitch for segment-final utterances.
Theseresults suggest that even in the artificial environment ofhuman-computer dialogue, prosodic cues robustly sig-nal discourse segment structure, comparably to the con-trastive uses of pitch and amplitude identified in naturalmonologues.1.1 OverviewWe begin with a discussion of related work on discoursesegmentation and dialogue act identification in mono-logue and dialogue, primarily in the human-human case.Then we introduce the system and data collection pro-cess that produced the human-computer discourse seg-ment change materials for the current analysis.
We de-scribe the acoustic analyses performed and the featureschosen for comparison.
Then we identify the prosodiccues that distinguish discourse segment boundaries anddiscuss the relation to previously identified cues for otherdiscourse types.
Finally we conclude and present somefuture work.2 Related WorkCues for and automatic segmentation of discourse struc-ture have been most extensively studied for written andspoken monologue.
For written narrative, discourse seg-ment boundaries have been identified based on textualtopic similarity with a variety of approaches based onHearst?s Textiling(Hearst, 1994).
More complex rheto-rial structure theory trees have also been extracted basedheavily on cue phrases and discourse markers by (Marcu,2000).In spoken monologue, prosodic cues to discoursestructure and segmentation have been explored by(Nakatani et al, 1995; Swerts, 1997).
Increases in pitchrange, amplitude, and silence duration appear to signaldiscourse segment boundaries across different domains- voicemail, broadcast news, descriptive narrative - andacross different languages, such as English and Dutch.Comparable prosodic cues have been applied to the re-lated task of news story segmentation, in conjunctionwith textual cues to topicality, by (Tur et al, 2001), wherelarge pitch differences between pre- and post- boundarypositions play the most significant role among prosodiccues.In spoken dialogue, research has focused on the identi-fication of dialogue acts and dialogue games.
Integrationof textual and prosodic cues, such as particular pitch ac-cent or contour types, have been found useful for identi-fying act type(Shriberg et al, 1998; Taylor et al, 1998).Specific classes of dialogue act, such as corrections (re-quest repair), have received particular interest in workby (Levow, 1998; Swerts et al, 2000) in the context ofhuman-computer error resolution.
Recent work on theICSI multi-party meeting recorder data has demonstratedsome very preliminary results on multi-party segmenta-tion (Galley et al, 2003); prosodic information in thiscase was limited to silence duration.With the exception of work on error resolution, mostwork on dialogue has focused human-human interactionand on identification of particular act or game types.
Herewe concentrate on the general question of discourse seg-mentation in voice-only human-computer interaction.
Weask whether the cues to segment structure identified formonologue are robust to the change in number and typeof conversational participant.3 Data Collection3.1 Speech System DescriptionThe utterances used in the analysis of discourse seg-mentation for human-computer interaction were drawnfrom approximately 60 hours of user interactions in afield trial of the Sun Microsystems SpeechActs system(Yankelovich et al, 1995).
SpeechActs provides a voice-only interface to common desktop applications such ase-mail, calendar, weather reports, stock quotes, and timeand currency conversions.
The system allows conversa-tional interactions with speech recognition provided byBBN?s HARK recognizer and speech synthesis throughCentigram?s TruVoice system.
A sample interaction ap-pears below.
1SpeechActs: You have 14 new messages and47 old messages.
The first newmessage is from Eric Baatzregarding ?report completed.
?User: Let me hear it.SpeechActs: ?The first draft is ready foryour comments.
Eric.
?User: Reply and include current message.SpeechActs: Preparing message to Eric Baatz.Begin recording after the tone.When finished, pause several seconds.User: Eric, I?ll get those to youthis afternoon.SpeechActs: Say cancel, send, or review.User: Send.SpeechActs: Message sent.
What now?User: Next message.SpeechActs: New message two from StuartAdams, subject ?Can we meet at3:00 today?
?User: Switch to calendar...The field trial involved a group of nineteen subjects.Four of the participants were members of the system de-velopment staff, fourteen were volunteers drawn fromSun Microsystems?
staff, and a final class of subjects con-sisted of one-time guest users.
There were three femaleand fifteen male regular user subjects.All interactions with the system were recorded and dig-itized in standard telephone audio quality format at 8kHzsampling in 8-bit mu-law encoding during the conversa-tion.
In addition, speech recognition results, parser re-sults, and synthesized responses were logged.
A paid as-sistant then produced a correct verbatim transcript of alluser utterances.
Overall there were 7752 user utterancesrecorded.1Designing SpeechActs: Issues in Speech User InterfaceDesign (Yankelovich et al, 1995) p. 23.2 Data Coding and ExtractionConsistent discourse segmentation can be difficult evenfor trained experts (Nakatani et al, 1995; Swerts, 1997;Passoneau and Litman, 1997), and differences in depthof nesting for discourse structure appear to be the mostproblematic.
As a result, we chose to examine utteranceswhose segment and topic initiating status would be rela-tively unambiguous.
As the SpeechActs system consistsof 6 different applications, we chose to focus on changesfrom application to application as reliable indicators oftopic initiation.
These commands are either simply thename of the desirable application, as in ?Mail?
or ?Cal-endar?, possibly with an optional politeness term, or aswitch command, such as ?Switch to?
and the name ofthe application.
Approximately 1400 such utterances oc-cured during the field trial data collection.We performed an automatic forced alignment in orderto identify and extract the relevant utterances from thedigitized audio.
Using the full sequence of synthesizedcomputer utterances and manually transcribed user utter-ances, we applied the align function of the Sonic speechrecognizer provided as part of the University of Colorado(CU) Communicator system to a 16-bit linear version ofthe original audio recording.
473 utterances that werecorrectly aligned by this automatic process were used forthe current analysis.4 Acoustic Feature ExtractionBased on prior results for monologue, we selected pitchand amplitude features for consideration.
Although si-lence duration is often a good cue to discourse segmentboundary position in narrative, we excluded it from con-sideration in the current study due to the awkward pace ofthe SpeechActs human-computer interactions.
Users hadto wait for a tone to speak, and interturn silences were aslong as six seconds.We used the ?To Pitch...?
and ?To intensity?
functionsin Praat(Boersma, 2001), a freely available acoustic-phonetic analysis package, to automatically extract thepitch (in Hertz) and amplitude (in decibels) for the in-teraction.
To smooth out local jitter and noise in thepitch and amplitude contours, we applied a 5-point me-dian filter.
Finally, in order to provide overall compara-bility across male and female subjects and across differ-ent channel characteristics for different sessions2, we per-formed per-speaker, per-session normalization of pitchand amplitude values, computed as .
The re-sulting pitch and amplitude values within the time re-gions identified for each utterance by forced alignment2Since the interface was accessed over a regular analog tele-phone line from a wide variety of locations - including noisyinternational airports, the recording quality and level variedwidely.Figure 1: Significant differences in normalized pitchand intensity.
Light grey: Segment-initial; Dark grey:segment-finalwere used for subsequent analysis.5 Prosodic AnalysisFor both pitch and amplitude we computed summaryscalar measures for each utterance.
Mean pitch and inten-sity are intended to capture overall increases or decreases.Maximum and minimum pitch and maximum amplitudeserved to describe increases in range that might not af-fect overall average.
We compared the segment-initial?application change?
utterances with their immediatelypreceding segment-final utterances.3 We find significantincreases in maximum pitch (fffiflffi !#"$!%'&(*) (,+ ), mean pitch (%-& (*) (*.
), and mean amplitude (%/&(*) (0(1. )
of segment-initial utterances relative to segment-final cases.
We also find highly significant decreases inminimum pitch (%2& (*) (0(,(*. )
for segment-final utter-ances relative to segment-initial utterances.
Changes inmaximum amplitude did not reach significance.
Figure 5illustrates these changes.6 DiscussionThe significant increases in maximum and mean pitchfor segment-initial utterances, coupled with a decreasein pitch minimum for segment-final utterances, suggest acontrastive use of pitch range across the segment bound-ary.
For amplitude, there is a global increase in intensity.These basic features of discourse segment-initial versusdiscourse segment-final utterances are consistent with the3For consistency, we excluded utterances that participatedin error spirals, and segment-final utterances which were alsosegment-initial.prior findings for monologue.
It is interesting to note thatin spite of the less than fluent style of interaction imposedon users by the prototype system, cues to discourse seg-ment structure remain robust and consistent.
We also ob-serve that the contrasts across discourse segment bound-aries are based on the speaker?s own baseline prosodicbehavior, rather than the conversational partner?s, at leastin this largely user-initiative system.7 Conclusions and Future WorkBased on analysis of more than 450 discourse segmentboundary pairs, we found significant increases in max-imum pitch, average pitch, and average intensity forsegment-initial utterances, with a significant decrease inminimum pitch for segment-final utterances.
Consistentwith prior work on human monologue, new discoursesegments in human-computer dialogue are signaled by in-creases in pitch, contrastive use of pitch range, and loud-ness, cues which could serve to attract the attention of theother conversational participants.In future work, we plan to apply these features to auto-matic extraction of discourse boundaries and global dis-course structure.
These features could also be used inconjunction with phonetic recognition results to enhanceconfidence scoring for utterances that would cause a topicshift.
In systems such as SpeechActs where topic shiftoften signals an application change, a somewhat time-consuming activity as a new recognizer is swapped inand new data loaded, it is desirable to have additionalimplicit confirmation that such an action has in fact beenrequested.
Finally we hope to explore cues to more fine-grained hierarchical discourse structure to distinguish fulltopic shifts from initiation or completion of subdialogues.Acknowledgments We thank Nicole Yankelovich andSun Microsystems for access to the field trial data andtheir assistance in transcription of these materials.ReferencesJ.
F. Allen and D.J.
Litman, 1990.
Discourse Processingand Common sense Plans.
MIT Press.P.
Boersma.
2001.
Praat, a system for doing phoneticsby computer.
Glot International, 5(9?10):341?345.Michel Galley, Kathleen R. McKeown, Eric Fosler-Lussier, and Hongyan Jing.
2003.
Discourse segmen-tation of multi-party conversation.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics (ACL-03), pages 562?569.B.
Grosz and C. Sidner.
1986.
Attention, intention, andthe structure of discourse.
Computational Linguistics,12(3):175?204.Barbara Grosz.
1977.
The representation and use of fo-cus in a system for understanding dialogs.
In Proceed-ings of the International Joint Conference on ArtificialIntelligence (IJCAI?77), page 67=76.M.
Hearst.
1994.
Multi-paragraph segmentation of ex-pository text.
In Proceedings of the 32nd Annual Meet-ing of the Association for Computational Linguistics.G.-A.
Levow.
1998.
Characterizing and recognizing spo-ken corrections in human-computer dialogue.
In Pro-ceedings of COLING-ACL ?98.W.
C. Mann and S. A. Thompson.
1987.
Rhetori-cal structure theory: Description and construction oftext structures.
In G. Kempen, editor, Natural Lan-guage Generation: New Results in Artificial Intelli-gence, Psychology, and Linguistics, pages 85?95.
Ni-jhoff, Dordrecht.D.
Marcu.
2000.
The Theory and Practice of DiscourseParsing and Summarization.
MIT Press.C.
H. Nakatani, J. Hirschberg, and B. J. Grosz.
1995.Discourse structure in spoken language: Studies onspeech corpora.
In Working Notes of the AAAI SpringSymposium on Empirical Methods in Discourse Inter-pretation and Generation, pages 106?112.Rebecca Passoneau and Diane Litman.
1997.
Discoursesegmentation by human and automated means.
Com-putational Linguistics, 23(1):103?139.E.
Shriberg, R. Bates, A. Stolcke, P. Taylor, D. Jurafsky,K.
Ries, N. Coccaro, R. Martin, M. Meteer, and C. VanEss-Dykema.
1998.
Can prosody aid the automaticclassification of dialog acts in conversational speech?Language and Speech, 41(3?4):439?487.M.
Swerts, J. Hirschberg, and D. Litman.
2000.
Cor-rections in spoken dialogue systems.
In Proceedingsof the International Conference on Spoken LanguageProcessing (ICSLP?00), pages 615?619.Marc Swerts.
1997.
Prosodic features at discourseboundaries of different strength.
Journal of the Acous-tical Society of America, 101(1):514?521.P.
Taylor, S. King, and S. Isard ans H.Wright.
1998.
In-tonation and dialogue context as constraints for speechrecognition.
Language and Speech, 41(3?4).D.
R. Traum and E. A. Hinkelman.
1992.
Conversationacts in task-oriented spoken dialogue.
ComputationalIntelligence, 8(3):575?599.G.
Tur, D. Hakkani-Tur, A. Stolcke, and E. Shriberg.2001.
Integrating prosodic and lexical cues for auto-matic topic segmentation.
Computational Linguistics,27(1):31?57.N.
Yankelovich, G. Levow, and M. Marx.
1995.
Design-ing SpeechActs: Issues in speech user interfaces.
InCHI ?95 Conference on Human Factors in ComputingSystems, Denver, CO, May.
