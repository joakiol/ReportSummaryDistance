eeeOeOOeOOOOOOOO0OeOOOOOOOOOOOeOOOeOOOOAnaphora Resolution using an Extended Centering Algorithmin a Multi-modal Dialogue SystemHarksoo IGm, Jeong-Mi Cho, Jungyun SCODepartment ofComputer Science, Sogang UniversitySeoul, 121-742, Koreahskim@nlpzodiac.sogang.ac.kr, jmcho@nlprep.sogang.ac.kr, seojy@ccs.sogang.ac:krAbstractAnaphora in multi-modal dialogues havedifferent aspects compared to theanaphora in language-only dialogues.They often refer to the items signified bya gesture or by visual means.
In this paper,we define two kinds of anaphora: screenanaphora and referring anaphora, andpropose two general methods to resolvethese anaphora.
One is a simple mappingalgorithm that can find items refetxedwith/without pointing estures on a screen.The other is the centering algorithm with adual cache model, which Walker'scentering algorithm is extended to for amulti-modal dialogue system.
Theextended algorithm is appropriate toresolve various anaphora in a multi-modaldialogue .because it keeps utterances,visual information and screen switching-,.tim.e.
In the experiments, the system.Correctly resolved 384 anaphora out of402 anaphom in 40 dialogues (0..54anaphom per utterance) showing 95.5%correctness.IntroductionHuman face<o-face ommunication is an idealmodel for humm-computa- interface.
One of themajor features of face-to-face communication isits multiplicity of communication channels thatacts on multiple modalities.
By providing anumber of channels through which informationmay pass between a user and a computer, amulti-modal dialogue system gives the user a?
more convenient and natural interface than alanguage-only dialogue system.
In the system, auser often uses a variety of anaphodcexpressions like this, the red item, it, etc.
User'sintention is passed to the system throughmultiple channels, e.g., the auditory channel(carrying speech) and the visual channel(c~nrying gestures and/or facial expressions).
Forexample, a user can say utterance (4) in Figure!
!
while touching an item on the screen.
Theuser may also say utterance (8) without ouchingthe screen when there is only one red itemdisplayed on the screen.
Moreover, the user canuse anaphoric expression to refer to an entity inprevious utterances a in utterance (10).
(1) S: May I help you7(2) U: I want o see some desks.
(3) S: (displaying mode/200 and mode/250)We have these modeb.
(4) U: (pointing to the model200)How much is thLv?
(5) S: It is 150,000 Won.
(6) U: I'd like to see some chairs, too.
(7) S: (displaying model 100 and model 150)We have t/w~re medeb.
(8) U: How much is the red item?
(9) S: It is 80,000 Won.
(10) U: (pointing to the model 100)l'd like to buy thb and tke prev/ous se/ect/on.Figure 1: Motivational example IPrevious rescaw, h on a multi-modal dialoguesystem was focused on finding the relationshipbetween a pointing gesture and a deicticexpression (Bolt (1980), Neal et al (1988),Salisbury et al (1990.
), Shimazu et al (1994),Shimazu and Takmhima (1996))and onmapping a predefined symbol to a simplet S means a multi-modal dialogue system and Umeans a user.
Our goal is developing a multi-modaldialogue system (Kim and Son (1997)).
of whichdomain is home shopping and in which a userpurchases furniture using Korean utC~',mccs withpointing gestures on a touch screw.21command (Johnston et al (1997)).
None of them,however, suggest methods of resolving deicticexpressions with which pointing gestures areomitted: e.g.. the red item in utterance (8).
Theseapproaches do not consider resolving ananaphoric expression that refers an objectmentioned in previous utterances or displayedon previous creens.
It, however, is importantalso for a multi-modal dialogue system toresolve all of these anaphora so that the systemshould correctly catch his/her intention.
In thispaper, we propose general methods to resolve avariety of anaphoric expressions that are foundin a multi-modal dialogue.
We classify anaphorainto two types: deictic expression with/without apointing gesture and referring expression, andpropose methods to resolve them.To resolve deictic expression like this inutterance (4) which c~rs  with a pointinggesture and the red item in utterance (8) which isuttered with no  pointing gestures, the systemcounts the "number of pointing estures and thenumber of anaphoric noun phrases included in auser's utterance, and compares them.
Then, thesystem aps the noun phrases to pointed items.To resolve referring expression, one of thewe l l  known methods is centering theorydeveloped by Grosz, Jo~hi, and Weinstein(Grosz et al (1983)).
The centering algorithmwas further developed by Brennan, Friedmanand Pollard for pronoun resolution (Brennan etal.
(1987)) and was improved by Walker(Walker (1998)).
However,' those centeringalgorithms are not applicable to resolveanaphora in a multi-medal dialogue because thealgurithm excludes the gestures and facial?
expression of a dialogue partner, which areimportant clues to mgierstand his/her uttexances.And.
the algorithm cannot resolve complexanaphora like the previous selection in(10)beeanse it does not keep the time When theprevious creen is switched to the current screen.To resolve inch anaphom, we extend Walker'scentedng algorithm to the one with a dual cachemodel, which keeps the information displayedon a ~ With screen switching-time.The rest of this paper begins with describingour approach in section !.
After showing twomethods to resolve anaphora in a rrmlti-modaldialogue system in section 2, we reportexperimental results on these methods in section3.
Finally.
we draw some conclusions.221 Our  approachIn this paper, we define two types of anaphora:screen anaphora nd referring anaphora.
Screenanaphora isan anaphoric noun phrase that refersto an entity on the present screen by a pointinggesture or through a visual channel.
For example,th/s in uuerance (4) in Figure !
is the screenanaphora referred by a pointing esture, and thered item in utterance (8) is the one referredthrough a visual channel.
Referring anaphora isan anaphoric noun phrase that refers to an entityin previous utterances or on pre~,ions screens,For example, we call it in utterance (9) referringanaphora because the referred entity is the reditem in the previous utterance (8).
We also callthe previous election in utterance (10) referringanaphora because the refe~nt is the model 200shown on the previous creen.The screen anaphora resolution algorithmcounts the number of pointing gestures and thenumber of anaphoric noun phrases included inthe user's utterance and compares them.
If thenumbers are equal, the system maps the gesturesto the phrases.
Otherwise.
the system uses someheuristics to map the gestures according to thepriority of the phrases.The referring anaphora resolution algorithmis based on the Walker's centering algorithmwith a cache model.
Centering is formulated as atheory that relates focus of attention, choice ofreferring expressions and perceived coherence ofutterances; within a discourse segment.
Thecentering algorithm (Ccnsz et al (1983),Brennan et al (1987), Walker et aL (1990))consists of three main structures.
Forward-looking Centers are entities which form a set ofentities associated with each.
utterance.Forward-looking Centers are ranked accordingto their relative salience.
The highest rangedentity is called the Preferred Center.
Backward-looking Center is a special member of this set.
Itis the highest ranked member of Forward-looking Centers of the previous utterance, whichis also realized in the current utterance.
Thealgorithm defines a set of constraints, rules, andtransition states between a pair of utterances bythe use of these structures.
It incorlgntes theserules and the other linguistic constraints toresolve anaphoric expressions.
In the past, it wasintegrated with a stack model because mostresearchers believed that the centers hould existwithin a discourse segment (Grosz and Sidner0000O0000O00000000O0000000O0000OO(9} NtS 80.000 WOn.
-{8} HOw much nt ff~' *~ steoO(~') We have s.,~se moOel$c i ter* ,  ttmfeuee?
..>S 'wf fch i t~ l - f imeCache MemoryFigure 2: Walker's cache model?
~ m~ra~ sk~ ~maml s lo llwmbll~m ~m ~ t  r ,e~m m~ JG~m~Id~ ~a~ ~iw um~qW mb t~.~~.
/ leh lno - l im~Iffo.fg f t~  ~k~|{2} ~111, e so me ~mlO) Mwem,~u?Dual CacheFigure 3: Dual cache modelOual Memory(1986), Brennan et al (1987), Walker (1989)).Walker replaced the stack model with a cachemodel because some objects were often referredin other discourse segments (Walker (1998)).The fundamental idea of the cache model is thatthe function of the cache when processingdiscourse is analogous to that of a cache whenexecuting a program on a computer.
In the cachemodel, the centers of an utterance are stored inthe cache fill the cache is full.
When it is full, theleast ~ently accessed centet~ in the cache arereplaced to main memory.
Figure 2 shows a stateof the cache and memory when we apply thecache model to Figure 1.
In Figure 2, we havereplaced the centers by the user's utterances forsimpfifying the illustration, In Figme 2 and 3,the utterance with the highly ranked entitiesplaced above the utterance with low rankedentities.To resolve the amphora h'ke the previousselection Shown in utterance (I0) in Figure I, thesystem should refer to previous utterances.
If weemploy Walker's cache model to find its referent,the system cannot get the correct result which ismodel 200 in utterance (4),but the red item inutterance (8).
The reason is that the model doesnot have the time base of the modifier, previous.In other words, it cannot decide the exact ime ofprevious.
In this paper, we propose an extendedcentering algorithm with a dual cache model fora multi-modal dialogue system.
To referprevious utterances and screens, the extendedmodel keeps the centers of user's utterances,visual information, and screen switching-time,The visual information means the characteristicsof the items on a screen, e.g.. model number,color, size, shape and so on.
The screenswitching-time means the time when the systemchanges the previous creen to the current screento show new items.
Figure 3 shows a state of thedual cache and memory when we apply the newmodel to Figure 1.
To decide the priority of theutterances, we present a priority rule; the morekinds of information an utterance carries, thehigher priority the utterance has.
One of theheuristic roles is that utterances occurring withpointing gestures have the higher possibility tobe focused in a future dialogue than thosewithout pointing estures.
If uttemnc~ carry thesame kinds of information, the recent utterancehave higher priority than the earlier utterancein the stack model.
For example, utterance (8) inFigure !
were spoken earlier than utterance (9).It has, however, higher ank than utterance (9) asshown in Figure 3 because it occurred withvisual information.
Utterance (7) haS higher ank23than utterance (8) because it occurred Withgesture information such as the user's pointinggesture or the system's blinking gesture for thehighlighted items as well as visual information.Now, if the dual cache model is applied to thesystem to find the referred entity of the previousselection shown in utterance (10) in Figure l. thesystem can get the correct referent, model 200 inutterance (4), because the model recognizes thetime base of previous and searches centers in theutterance slot associated with the previous visualslot which includes model 200 and model 250 asshown in Figure 3.2 Anaphora resolution algorithms2.1 The screen anaphora resolution algorithmThe screen anaphora resolution algorithmreplaces screen anaphora inan utterance with theitems referred with/without pointing gestures.For example, if a user says "I'd like to buy thisand the red cha/r."
when pointingto an item onthe screen, this in the utterance means the itemthat he/she points to and the red chair means thechair on the screen that is red.
According to thenumber of anaphora nd the number of gesturesco-occun-ed with an utterance, we divide thealgorithm into three cases.Case 1: The number of gestures and thenumber of anaphora re equal.Case 1 occurs most frequently in a multi-modaldialogue.
In this case, the algorithm replaces the?
anaphora with the pointed items according.to heorder of occunence.
For example; "th/s in theutterance (4) in Figure I can bz resolved by thissimple mapping.Case 2: The number of gestures is less thanthe number of anaphora.Case 2. occurs when a user omits a pointinggesture because heJshe can uniquely select anitem on the screen with the anaphoric expressionor when the~ are referring anaphom as well asscreen amphora s in utterance (I0) in Figure I.In the former case, the algorithm can easilyresolve the anaphora because it can uniquelydecide the referred entity on the screen by visualinformation of the item.
However, the algorithmcam~ot resolve the referring anaphora in this stepbecause it needs to look at the previousutterances.
The algorithm passes them to thereferring anaphor a resolution algorithm.
We willshow the referring anaphora resolution algorithmin section 3.2.
.Case 3: The number of gestures is greaterthan the number of anaphora.Case 3 occurs when a user omits all or part ofthe utterance.
The omission consists of twotypes: partial omission and total omission.
Toprocess the former, the algorithm first checks formissing essential cases in the case-frame.
I f  thealgorithm finds the omitted one, it fills theomitted one up and generates the supplementedresult of the semantic analysis.
For example, i f  auser says "How much.'?"
when he/she pointstomodel i00 on the screen, the algorithm assumesthat he/she omitted the theme it and generatesthe new semantic result like "How much is it?
".Then, it processes the result according to thesame method as Case 1.
In the latter case, thealgorithm assumes that the user uttered either?
'o l~o l~.
f fh is ,  please.)"
or "ol~-~ol~.
(These, please.)"
because he/she just pointed toan item/ite.ms without an utterance.
Afterrestoring the omitted utteranc.e, it can easilyresolve the anaphora as Case 1.2.2 The referring anaphora resolut ionalgorithmThe referring anaphora resolution algorithmfinds referents by using previous utterances andvisual information.
In utterance (10) in Figure I,the user says, " I'd like to buy this and theprevious selection."
while he/she points tomodel 100.
The system can resolve th/s usingthe screen armphora resolution algorithm.However, the system cannot find the referredentity of the previous election.
The referringanaphora resolution algorithm resolves thesekinds of anaphom.
The algorithm is based on anextended centering algorithm with the dualcache model.
The dual cache model consists o ftwo slots and time points: visual slot, utteranceslot, and screen switching-times as shown inFigure 4.
The visual slot contains the.
visualinfommtion, Le.., items displayed on a screen.The utterance slot contains the centers of theuser's ul~.amtces.
The screen switching-time,which is illustrated by an arrow, keeps the timewhen the previous screen is switched to thecurrent screen.
In Figure 4, Vt is the kth visualslot, which includes visual information of the kthscreen././eta means the utterance that has the jthpriority at the kth visual slot.
Cf is a list of24000OO000000000e0000000000000000OVi~t JA!
q l lO l  l | l l{e ,q l J} t ' f  ?
.t11~1t~*  ?
ICb  GelUf '*  * |Cb Ct lW*t in ,  , .
,  |Clt .Gg|t , '~.
.
,~, ICb .C l l"4* ,Oual  CacheT, cm~n -4 'ewi t  r ' .h i t ,~n - f l rm~~it  t '~h l t ' t tq  - | | t~u*'.
, .
IC~C,!Oual  Memory-Figure 4: The structure of the dual cache model.
(~) Priority RuleThe more kinds of information an utterance carries, the higher priority the utterance has.
So,.
the prioritybetween anchors i :utterance +a pointing esture >utterance + visual information > utterance with no other informationFigure 5: The priority of utterancesFmward-lookin 8 Centers, and Cb is theBackward.looking Center.
A pair of Cb and Cfis called an anchor.In the case of a language-only dialogue?
system, anchors stack up according to LIFOmechanism because the system processes onlyutterance without co-occurring information likea gesture.
However, anchors in the dual cachemodel should not follow the mechanism becauseeach utterance contains different kinds ofinformation occurring with it.
For example,utterance (4) in Figure 1 occurs with a gesture.and utterance (9) contains visual information.
Todecide the priorities among anchors, we proposea priority rule as shown in Figure 5.
The rulemeans that utterances eccmring with pointinggestures have the higher possibility to befocused in a future dialogue than those withoutpointing gestures.
I f  anchors occur with thesame kinds of informadon, they follow themechanism.
The reason why the algorithmshould keep Vk in the dual cache model is thatthere are some anaphora eferring to items whicha user does not utter on the previous creen.
Asshown in utterance (5) in Figure 6, if the systemdoes not keep.
the visual information, it cannotresolve the previous red item because the usersaw the color of model 200 through the visualchannel but never uttered about the red itemuntil utterance (5).
(1) U: I'd like to see some desks.
(2) $: (displaying model 200 and model 250)We have these.
(3) U: I'd like to see some chairs, too.
(4) S: (displaying mode/100 and model 150)We have these.?
(5) u: (poindng to model/00)I'd like to buy this and the pr~ioas red item.Figure 6: Motivational example 2In this paper, the ranking of the items in Cf alsofollows Figure 5.
If the items have the samepriority, the algorithm ranks them by theobliqueness of grammatical relation of thesubcategorized functions of the main verb: thatis, first the subject, object, and objects2,followed by other subcategorized fen~ons, andfinally, adjuncts (Grosz and Sidner (1986),Brennan et ul.
(1987)).The centering algorithm is based onconstraints and rules as well as Cbs and C~.
Inthis paper, we propose xtended constraints andrules as shown in Figure 7 because the structureof the cache model and the priority of theutterances are changed according to Figure 4 and5.25?
- .
.
?
(~) ConstraintsI.
There is exactly one Cb for each uttcrance Ui.2.
Every element of CflUi) must bc realized in UL3.
Cb(Ui) is highest-ranked element of Cj~UPk.j)that is realized in UL4.
The visual information and the gesturesoccurring with an utterance arc not related tothe previous utterances.
(~) Rules!.
If some elements of C.KUPkj) are realized as apronoun in Ui, then so is tb(Ui).2.
The priority of transitions from one utterance tothe next is:COITI'INUE > RETAIN > SMOOTH SHIFT >ROUGH SHIFTFigure 7: Extended constraints and rulesFigure 7 is similar to those of (Brennan et al(1987)) except that C.KUz-z) is replaced to?~Ue~), and the 4th constraint is added.
Thereplacement means that tb(UO may not berealized in the previous utterance., .Ui-z.
Forexample, the referent of the previous selection inutterance (I0) in Figure I should be focusedamong the entities displayed in the previousscreen.
In.such case.
Cb(UO should be realizedin the utterance Ue~j, where Ut is the currentvisual slot, and j is decided by the priority rulein Figure 5.
In other words, when the systemdetects a time base modifier such as the previous,first, it must decide the correct visual slot andthen apply centering heuristics to decide Cb(UO.The 4th constraint shows that the user's currentgesture must not be related to the previousutterances.
We already adopted this constraint inthe screen anaphora resolution algoritlun insection 3.1.
This constraint should also beapplied to filter out unlike candidates.
Thetransition types from one utterance to the nextaxe extended as shown in Pig~rc 8 by the samereason for the constraints and rules in Figure 7.C~u,) = C~U,)ict~u,), cr~u,~ Rm'^n?Figure 8: Extended Transition statesC/,(U,)- Ct<O~.j) ?
::~U,),* Ct<Up~)CONTINUE SMOOTH SHIFTROUGH SHIFT26The referring anaphom resolution a lgor i thmthat is based on theses changes is the fol lowing:First, selects an anchor, Cb(UPL,) and Cf(Ue~),and constructs all potential anchors in thepresent utterance Ui.
In order to find the anchorin UPs, we choose the kth visual informationslot in the dual cache and memory and searchanchors in utterance slots associated with thevisual slot.
That is, if the modifier expresses thetime base like previous in Figure l, thealgorithm will search utterance slots associatedwith the previous visual information slot.
Duringthis process, it chocks whether Cj~Uij andCJ~UP~) are satisfied with the agreements, thegrammatical functions and selectionaIrestrictions (Brennan et al (1987), Walker(1998)).
In other words, the potential anchors aregenerated for each referring expression in anutterance and are specified for the agreements,the grammatical functions and the seloctionalrestrictions.
Then, it filters off unsuitableanchors using the extended filters in Figure 9,which are based on the constraints and rules inFigure 7.
If an anchor emains, it is regarded as apair of Cb and Cfin the present utterance.For each anchor in the current list of anchors, applythe following filters derived from the centeringconstraints and rules.
The first anchor that passes eachfilter is used to update the contcxL If more than oneanchor at the same ranking passes all the fiitc~ thenthe algorithm predicts that he utterance is ambiguous.
(D FILTER h If the proposed Cb of the anchor doesnot equal the first element of this constructed list.then eliminate this anchor.
This corresponds toconstraint 3.~) FILTER 2: If none of the entities realized asanaphota in the proposed Cf equals the proposedCb.
then eliminate this anchor.
If there are noanaphora inthe proposed qthcn+the anchor passesthis filter.
This conmpoads to rule 1.
Howcvor.anaphora that are resolved by a gesture and visualinformation must not be filtered.
This correspondsto constraint 4.Figure 9: Extended Idlers0000eooooooooeooooeooooooooooooo6Tabl e 1: The component ratio of the experimental data and the experimental resultScreen Anapbora Referring Anaphora TotalFrequency 328 74 402(the component ratio) (8 !
.6%) (I 8.4%) (100%)# of detected anaphora 317 71 388(recall) (96.6%) (95.9%) (96.5%)# of resolved anaphora 316 68 384(precision) (99.7%) (94.4%) (98.7%)(# of resolved anaphora / 96.3% 91.9% 95.5%Frequency)* 1003 Evaluation and analysis of theexperiments3.1 The experimental dataIn order to experiment the proposed alg0rithms,we collected multi-modal dialogues which weresimulated by 10 graduate students.
They consistof 40 dialogues with 754 utterances (18.85utterances per dialogue).
The subject of thedialogue is furniture home shopping using atouch screen monitor.
The data contains theuser's utterances, pointing gestures and variousvisual information.
In the data, we found 402anaphoric noun phrases (10.5 anaphora perdialogue, and 0.54 anaphora per utterance).
Itmeans that anaphora resolution is very importantfor the multi-modal dialogue system.
The screenanaphora ppeared 4 times (81.6%) as much asthe referring anaphora s shown in Table I. Itshows that a user usually points to the itemwhen he/she wants to select an item on thescreen.3.2 The analysis of experimental resultThe two proposed aigndthlm detected 388anaphora correctly from 402 amphora anddetected i mmphora incorrectly.
The recall rateis %..5% as shown in Table !.
The algorithmsresolved 384 mmphora from the detectedanaphora (389 anaphora), and the pt~sion is98.7% as shown in Table !.
Most of the failureswere caused by preprocessing modules in ourmulti-modal dialogue system that generate aninput for proposed anaphora resolutionalgorithms.
The failure patterns are thefollowing.?
The system failed to detect the anaphoramodified by a subordinate clause like "ot .~buy the red chair that I selected before.
)"because we restricted anaphora as nounphrases modified by noun or adjective.?
The screen anaphora resolution algorithmfailed to know whether -~ g:'~(twomode/s) in "-~ .E W~ ;'I'll ~\]'o17~\]~o\] o~q..8.?
(What is the price differenceof two models?)"
was anaphori?
nounphrase because a user did not explicitly usea definite article, the, in Korean.
In English,we can easily know that -~ .H~'(twomode/s) is a anaphoric noun phrase becausea user normally uses the.
In Korean,however, it is difficult to find this kind ofanaphora because the use of a definitearticle is a weak grammar role.?
.The referring armphora resolution algorithmfailed to resolve the anaphora i i keO~o q\]~ ~all of the previous chair) becausethe anaphom is expressed as a singularexpression i  Korean.
Usually Koreans arenot strict in number agreement.
If thepreprocessing modules can recognize thesingular exp/ession as the plural ~pressionby looking at the meaning of Cl~all), thealgoritlnn can resolve these kinds of failurepauems.A~ we can see in the=~ failure cases, mostfailures are due to some special characteristics ofKorean dialogues.
We believe the proposedalgorithms work much better in English multi-modal dialogues.ConclusionUnlike a language.only dialogue system, themulti-modal dialogue system has to resolve a27variety o f  anaphora because the system hasvarious input channels, We proposed generalalgorithms to resolve such various anaphora inthe multi-modal dialogue system, We definedtwo kinds of multi-modal anaphora, screenanaphora and referring anaphora.
To resolve thescreen anaphora, we proposed simple mappingalgorithm.
We proposed an extended centeringalgorithm integrated with the dual cache modelto resolve referring anaphora.
In the experiments,among 402 anaphora in 40 dialogues (0.54anaphora per utterance), 384 anaphora wereresolved.
The result reflects the fact that theproposed algorithms work fairly well inresolving a variety of anaphora in multi-modaldialogues.In a furore work, we will test the system byusing the Cf ranking method (Walker et at.
(1990), Walker et at.
(1994)) that Walker usesfor Japanese.
Since Korean is similar in structureto Japanese (i.e.
it is a free word order, head-final language, with morphemic marking forgrammatical function and topic), it would beinteresting to see if the Cf ranking method canenhance our system's performance in multi-modal dialogue nvironment.AcknowledgementsAuthors arc grateful to the anonymous reviewersfor their, valuable comments on this paper.
Thiswork was supported in pa~ by the Ministry ofInformation and Communication U der the titleof "A Research on Multimodal DialogueInterlace".ReferencesBolt It.
0980) Put-That-Them: Voice and gesture atthe graphics interface.
Compnter Graplu'cs,14(3)'.262-270.Brennan $.. Friedman M. end Pollant C (1987) A?
Ccn~iaS Approach to PronouL In Proceedin&s of25~ AC/., pp.
155-162.Gfo~ B. J., Jofhi A.
~ and Weinstein S. (1983)Providing a unified account of definite nounphrases in discourse.
In Proceedings of the 21stAnmml Meetlng of the ACL, pp.
44-50.Grosz B. J. and Sidncr C. L. (1986) Attentions,intentions, and the structure of discourse.Computational Linguistics, i 2(3): 175-204.Johnston M., Cohen P., McGee D., Oviatt S., PittmanJ.
and Smith 1.
(1997) Unification-basedMullimodal Integration.
In Proreedi.g.~ .
f  81hEACL.
pp.
281-28X.Kim H. and Sco J.
(1997) A Mulli-Modal Userinlcrface System in Home Shopping Domain.
(inKorean) In '97 Spring Proceedings ofConferenceon Korea Cognitive SCience Society.
pp.
74-80.Neal J., Dobes Z., Bcttinger K. and Byoun J.
(1988)Multi-Modal References in Human-ComputerDialogue.
In Proceedings ofAAAI-88, pp.
819-823.Salisbury M. W., Hendrickson J. H. and Lammers T.L., Fu C. and Moody S. A.
(1990) Talk and draw:bundling speech and graphics.
IEEE Computer,23(8)'.59-65.Shimazu H., Arita S. and Takashima Y.
(1994)Multi-Modal Definite Clause Grammar.
InProceedings ofCOUNG'94, pp.
832-836.Shimazu H. and Takashima Y.
(1996) Multi-Modal-Method: A Design Method for Building Multi-Modal Systems.
InProceedings OfCOUNG'96, pp.925-930.Walker M. (1989) Evaluating discourse processingalgorithms.
In Proceedings of the 27th AnnualMeeting of the ACL, pp.
251-26 i. :':~Walker M., lida M. and Cote S. (1990) Centering inJapanese discourse.
In Proceedings of the 13thInternational Conference on ComputationalLinguistics.
pp.
1.7.Walker M., lida M. and Cote S. (1994) Japanesediscourse and the process of centering.Computational Linguistics, 20(2): 193-233.Walker M. (1998) Centering Anaphora Resolution,and Discourse Structure.
In: M. A. Walker, A.
If..Joshi & E F.
Prince(Eds.
), Centering Theory inDiscour=e.
Oxford/UK: Oxford U.P., pp.
401-435.28@00O00OOO0O00@0OO0O00O000@@0@0O@@QI
