Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 43?51,Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational LinguisticsEncoding syntactic dependencies by vector permutationPierpaolo BasileDept.
of Computer ScienceUniversity of BariVia Orabona, 4I-70125, Bari (ITALY)basilepp@di.uniba.itAnnalina CaputoDept.
of Computer ScienceUniversity of BariVia Orabona, 4I-70125, Bari (ITALY)acaputo@di.uniba.itGiovanni SemeraroDept.
of Computer ScienceUniversity of BariVia Orabona, 4I-70125, Bari (ITALY)semeraro@di.uniba.itAbstractDistributional approaches are based on a sim-ple hypothesis: the meaning of a word can beinferred from its usage.
The application of thatidea to the vector space model makes possi-ble the construction of a WordSpace in whichwords are represented by mathematical pointsin a geometric space.
Similar words are rep-resented close in this space and the definitionof ?word usage?
depends on the definition ofthe context used to build the space, which canbe the whole document, the sentence in whichthe word occurs, a fixed window of words,or a specific syntactic context.
However, inits original formulation WordSpace can takeinto account only one definition of context ata time.
We propose an approach based onvector permutation and Random Indexing toencode several syntactic contexts in a singleWordSpace.
Moreover, we propose some op-erations in this space and report the resultsof an evaluation performed using the GEMS2011 Shared Evaluation data.1 Background and motivationDistributional approaches usually rely on theWordSpace model (Schu?tze, 1993).
An overviewcan be found in (Sahlgren, 2006).
This model isbased on a vector space in which points are used torepresent semantic concepts, such as words.The core idea behind WordSpace is that wordsand concepts are represented by points in a math-ematical space, and this representation is learnedfrom text in such a way that concepts with sim-ilar or related meanings are near to one an-other in that space (geometric metaphor of mean-ing).
The semantic similarity between concepts canbe represented as proximity in an n-dimensionalspace.
Therefore, the main feature of the geomet-ric metaphor of meaning is not that meanings canbe represented as locations in a semantic space, butrather that similarity between word meanings can beexpressed in spatial terms, as proximity in a high-dimensional space.One of the great virtues of WordSpaces is thatthey make very few language-specific assumptions,since just tokenized text is needed to build semanticspaces.
Even more important is their independencyof the quality (and the quantity) of available train-ing material, since they can be built by exploiting anentirely unsupervised distributional analysis of freetext.
Indeed, the basis of the WordSpace model isthe distributional hypothesis (Harris, 1968), accord-ing to which the meaning of a word is determined bythe set of textual contexts in which it appears.
As aconsequence, in distributional models words can berepresented as vectors built over the observable con-texts.
This means that words are semantically relatedas much as they are represented by similar vectors.For example, if ?basketball?
and ?tennis?
occur fre-quently in the same context, say after ?play?, theyare semantically related or similar according to thedistributional hypothesis.Since co-occurrence is defined with respect to acontext, co-occurring words can be stored into ma-trices whose rows represent the terms and columnsrepresent contexts.
More specifically, each row cor-responds to a vector representation of a word.
Thestrength of the semantic association between words43can be computed by using cosine similarity.A weak point of distributional approaches is thatthey are able to encode only one definition of con-text at a time.
The type of semantics represented inWordSpace depends on the context.
If we choosedocuments as context we obtain a semantics differ-ent from the one we would obtain by selecting sen-tences as context.
Several approaches have inves-tigated the above mentioned problem: (Baroni andLenci, 2010) use a representation based on third-order tensors and provide a general framework fordistributional semantics in which it is possible torepresent several aspects of meaning using a sin-gle data structure.
(Sahlgren et al, 2008) adoptvector permutations as a means to encode order inWordSpace, as described in Section 2.
BEAGLE(Jones and Mewhort, 2007) is a very well-knownmethod to encode word order and context informa-tion in WordSpace.
The drawback of the BEAGLEmodel is that it relies on a complex model to buildvectors which is computational expensive.
Thisproblem is solved by (De Vine and Bruza, 2010)in which the authors propose an approach similarto BEAGLE, but using a method based on Circu-lar Holographic Reduced Representations to com-pute vectors.All these methods tackle the problem of repre-senting word order in WordSpace, but they do nottake into account syntactic context.
A valuable at-tempt in this direction is described in (Pado?
and La-pata, 2007).
In this work, the authors propose amethod to build WordSpace using information aboutsyntactic dependencies.
In particular, they considersyntactic dependencies as context and assign dif-ferent weights to each kind of dependency.
More-over, they take into account the distance betweentwo words into the graph of dependencies.
The re-sults obtained by the authors support our hypothesisthat syntactic information can be useful to produceeffective WordSpace.
Nonetheless, their methodsare not able to directly encode syntactic dependen-cies into the space.This work aims to provide a simple approach toencode syntactic relations dependencies directly intothe WordSpace, dealing with both the scalabilityproblem and the possibility to encode several con-text information.
To achieve that goal, we devel-oped a strategy based on Random Indexing and vec-tor permutations.
Moreover, this strategy opens newpossibilities in the area of semantic composition asa result of the inherent capability of encoding rela-tions between words.The paper is structured as follows.
Section 2describes Random Indexing, the strategy for build-ing our WordSpace, while details about the methodused to encode syntactic dependencies are reportedin Section 3.
Section 4 describes the formal defi-nition of some operations over the WordSpace andshows a first attempt to define a model for semanticcomposition.
Finally, the results of the evaluationperformed using the GEMS 2011 Shared Evaluationdata1 is presented in Section 5, while conclusionsare reported in Section 6.2 Random IndexingWe exploit Random Indexing (RI), introduced byKanerva (Kanerva, 1988), for creating a WordSpace.This technique allows us to build a WordSpace withno need for (either term-document or term-term)matrix factorization, because vectors are inferred byusing an incremental strategy.
Moreover, it allowsto solve efficiently the problem of reducing dimen-sions, which is one of the key features used to un-cover the ?latent semantic dimensions?
of a worddistribution.RI is based on the concept of Random Projectionaccording to which high dimensional vectors chosenrandomly are ?nearly orthogonal?.Formally, given an n ?m matrix A and an m ?k matrix R made up of k m-dimensional randomvectors, we define a new n?
k matrix B as follows:Bn,k = An,m?Rm,k k << m (1)The new matrix B has the property to preservethe distance between points.
This property is knownas Johnson-Lindenstrauss lemma: if the distance be-tween two any points of A is d, then the distance drbetween the corresponding points in B will satisfythe property that dr = c ?
d. A proof of that propertyis reported in (Dasgupta and Gupta, 1999).Specifically, RI creates a WordSpace in two steps(in this case we consider the document as context):1Available on line:http://sites.google.com/site/geometricalmodels/shared-evaluation441.
a context vector is assigned to each document.This vector is sparse, high-dimensional andternary, which means that its elements can takevalues in {-1, 0, 1}.
A context vector contains asmall number of randomly distributed non-zeroelements, and the structure of this vector fol-lows the hypothesis behind the concept of Ran-dom Projection;2. context vectors are accumulated by analyzingterms and documents in which terms occur.
Inparticular, the semantic vector for a term iscomputed as the sum of the context vectors forthe documents which contain that term.
Con-text vectors are multiplied by term occurrences.Formally, given a collection of documents Dwhose vocabulary of terms is V (we denote withdim(D) and dim(V ) the dimension of D and V ,respectively) the above steps can be formalized asfollows:1.
?di ?
D, i = 0, .., dim(D) we built the cor-respondent randomly generated context vectoras:?
?rj = (ri1, ..., rin) (2)where n  dim(D), ri?
?
{?1, 0, 1} and?
?rjcontains only a small number of elements dif-ferent from zero;2. the WordSpace is made up of all term vectors?
?tj where:?
?tj = tfj?di?Dtj?di?
?ri (3)and tfj is the number of occurrences of tj indi;By considering a fixed window W of terms ascontext, the WordSpace is built as follows:1. a context vector is assigned to each term;2. context vectors are accumulated by analyzingterms in which terms co-occur in a window W .In particular, the semantic vector for each termis computed as the sum of the context vectorsfor terms which co-occur in W .It is important to point out that the classical RIapproach can handle only one context at a time, suchas the whole document or the window W .A method to add information about context in RIis proposed in (Sahlgren et al, 2008).
The authorsdescribe a strategy to encode word order in RI by thepermutation of coordinates in random vector.
Whenthe coordinates are shuffled using a random permu-tation, the resulting vector is nearly orthogonal to theoriginal one.
That operation corresponds to the gen-eration of a new random vector.
Moreover, by apply-ing a predetermined mechanism to obtain randompermutations, such as elements rotation, it is alwayspossible to reconstruct the original vector using thereverse permutations.
By exploiting this strategy it ispossible to obtain different random vectors for eachcontext2 in which the term occurs.
Let us considerthe following example ?The cat eats the mouse?.
Toencode the word order for the word ?cat?
using acontext window W = 3, we obtain:< cat >= (?
?1the) + (?+1eat)++(?+2the) + (?+3mouse)(4)where ?nx indicates a rotation by n places of theelements in the vector x.
Indeed, the rotation is per-formed by n right-shifting steps.3 Encoding syntactic dependenciesOur idea is to encode syntactic dependencies, in-stead of words order, in the WordSpace using vectorpermutations.A syntactic dependency between two words is de-fined as:dep(head, dependent) (5)where dep is the syntactic link which connectsthe dependent word to the head word.
Gener-ally speaking, dependent is the modifier, object orcomplement, while head plays a key role in de-termining the behavior of the link.
For example,subj(eat, cat) means that ?cat?
is the subject of?eat?.
In that case the head word is ?eat?, whichplays the role of verb.The key idea is to assign a permutation functionto each kind of syntactic dependencies.
Formally,2In the case in point the context corresponds to the wordorder45let D be the set of all dependencies that we take intoaccount.
The function f : D ?
?
returns a schemaof vector permutation for each dep ?
D. Then, themethod adopted to construct a semantic space thattakes into account both syntactic dependencies andRandom Indexing can be defined as follows:1. a context vector is assigned to each term, as de-scribed in Section 2 (Random Indexing);2. context vectors are accumulated by analyzingterms which are linked by a dependency.
Inparticular the semantic vector for each term tiis computed as the sum of the permuted con-text vectors for the terms tj which are depen-dents of ti and the inverse-permuted vectorsfor the terms tj which are heads of ti.
Thepermutation is computed according to f .
Iff(d) = ?n the inverse-permutation is definedas f?1(d) = ?
?n: the elements rotation is per-formed by n left-shifting steps.Adding permuted vectors to the head word andinverse-permuted vectors to the corresponding de-pendent word allows to encode the informationabout both heads and dependents into the space.This approach is similar to the one investigated by(Cohen et al, 2010) to encode relations betweenmedical terms.To clarify, we provide an example.
Given the fol-lowing definition of f :f(subj) = ?+3 f(obj) = ?+7 (6)and the sentence ?The cat eats the mouse?, we obtainthe following dependencies:det(the, cat) subj(eat, cat)obj(eat,mouse) det(the,mouse)(7)The semantic vector for each word is computed as:?
eat:< eat >= (?+3cat) + (?+7mouse) (8)?
cat:< cat >= (?
?3eat) (9)?
mouse:< mouse >= (?
?7eat) (10)In the above examples, the function f does notconsider the dependency det.4 Query and vector operationsIn this section, we propose two types of queriesthat allow us to compute semantic similarity be-tween two words exploiting syntactic dependenciesencoded in our space.
Before defining query andvector operations, we introduce a small set of nota-tions:?
R denotes the original space of random vectorsgenerated during the WordSpace construction;?
S is the space of terms built using our strategy;?
rti ?
R denotes the random vector of the termti;?
sti ?
S denotes the semantic vector of the termti;?
sim(v1, v2) denotes the similarity between twovectors; in our approach we adopt cosine simi-larity;?
?dep is the permutation returned from f(dep).?
?dep is the inverse-permutation.The first family of queries is dep(ti, ?).
The ideais to find all the dependents which are in relationwith the head ti, given the dependency dep.
Thequery can be computed as follows:1. retrieve the vector sti from S;2. for each rtj ?
R compute the similarity be-tween sti and < ?deprtj >:sim(sti , < ?deprtj >);3. rank in descending order all tj according to thesimilarity computed in step 2.The idea behind this operation is to compute howeach possible dependent tj contributes to the vectorti, which is the sum of all the dependents related toti.
It is important to note that we must first apply thepermutation to each rtj in order to take into accountthe dependency relation (context).
This operationhas a semantics different from performing the queryby applying first the inverse permutation to ti in Rand then computing the similarity with respect to allthe vectors tj in S. Indeed, the last approach would46compute how the head ti contributes to the vector tj ,which differs from the goal of our query.Using the same approach it is possible to computethe query dep(?, tj), in which we want to search allthe heads related to the dependent tj fixed the de-pendency dep.
In detail:1. retrieve the vector stj from S;2. for each rti ?
R compute the similarity be-tween stj and the inverse-permutation of rti ,< ?
?deprti >: sim(stj , < ?
?deprti >);3. rank in descending order all ti according to thesimilarity computed in step 2.In this second query, we compute how the inverse-permutation of each ti (head) affects the vector stj ?S.
In the following sub-section we provide someinitial idea about semantic composition.4.1 Compositional semanticsDistributional approaches represent words in isola-tion and they are typically used to compute similar-ities between words.
They are not able to representcomplex structures such as phrases or sentences.
Insome applications, such as Question Answering andText Entailment, representing text by single words isnot enough.
These applications would benefit fromthe composition of words in more complex struc-tures.
The strength of our approach lies on the ca-pability of codify syntactic relations between wordsovercoming the ?word isolation?
issue.A lot of recent work argue that tensor product (?
)could be useful to combine word vectors.
In (Wid-dows, 2008) some preliminary investigations aboutproduct and tensor product are provided, while aninteresting work by Clark and Pulman (Clark andPulman, 2007) proposes an approach to combinesymbolic and distributional models.
The main ideais to use tensor product to combine these two as-pects, but the authors do not describe a method torepresent symbolic features, such as syntactic de-pendencies.
Conversely, our approach is able to en-code syntactic information directly into the distri-butional model.
The authors in (Clark and Pulman,2007) propose a strategy to represent a sentence like?man reads magazine?
by tensor product:man?
subj ?
read?
obj ?magazine (11)They also propose a solid model for composition-ality, but they do not provide a strategy to repre-sent symbolic relations, such as subj and obj.
Theywrote: ?How to obtain vectors for the dependencyrelations - subj, obj, etc.
- is an open question?.
Webelieve that our approach can tackle this problem byencoding the dependency directly in the space, be-cause each semantic vector in our space contains in-formation about syntactic roles.The representation based on tensor product isuseful to compute sentence similarity.
Given theprevious sentence and the following one ?womanbrowses newspaper?, we want to compute the sim-ilarity between the two sentences.
The sentence?woman browses newspaper?, using the composi-tional model, is represented by:woman?subj?browse?obj?newspaper (12)Computing the similarity of two representationsby inner product is a complex task, but exploitingthe following property of the tensor product:(w1?w2) ?
(w3?w4) = (w1 ?w3)?
(w2 ?w4) (13)the similarity between two sentences can be com-puted by taking into account the pairs in each depen-dency and multiplying the inner products as follows:man ?
woman?
read ?
browse?
?magazine ?
newspaper(14)According to the property above mentioned, wecan compute the similarity between sentences with-out using the tensor product.
However, some openquestions arise.
This simple compositional strategyallows to compare sentences which have similar de-pendency trees.
For example, the sentence ?the dogbit the man?
cannot can be compared to ?the manwas bitten by the dog?.
This problem can be easilysolved by identifying active and passive forms of averb.
When two sentences have different trees, Clarkand Pulman (Clark and Pulman, 2007) propose toadopt the convolution kernel (Haussler, 1999).
Thisstrategy identifies all the possible ways of decom-posing the two trees, and sums up the similarities be-tween all the pairwise decompositions.
It is impor-tant to point out that, in a more recent work, Clark47et al (Clark et al, 2008) propose a model basedon (Clark and Pulman, 2007) combined with a com-positional theory for grammatical types, known asLambek?s pregroup semantics, which is able to takeinto account grammar structures.
It is important tonote that this strategy is not able to encode gram-matical roles into the WordSpace.
This peculiaritymakes our approach completely different.
In the fol-lowing section we provide some examples of com-positionality.5 EvaluationThe goal of the evaluation is twofold: proving thecapability of our approach by means of some exam-ples and providing results of the evaluation exploit-ing the ?GEMS 2011 Shared Evaluation?, in particu-lar the compositional semantics dataset.
We proposetwo semantic spaces built from two separate corporausing our strategy.
To achieve the first goal we pro-vide several examples for each family of queries de-scribed in Section 4.
Concerning the second goal,we evaluate our approach to compositional seman-tics using the dataset proposed by Mitchell and Lap-ata (Mitchell and Lapata, 2010), which is part of the?GEMS 2011 Shared Evaluation?.
The dataset is alist of two pairs of adjective-noun combinations orverb-object combinations or compound nouns.
Hu-mans rated pairs of combinations according to simi-larity.
The dataset contains 5,833 rates which rangefrom 1 to 7.
Examples of pairs follow:support offer help provide 7old person right hand 1where the similarity between offer-support andprovide-help (verb-object) is higher than the one be-tween old-person and right-hand (adjective-noun).As suggested by the authors, the goal of the eval-uation is to compare the system performace againsthumans scores by means of Spearman correlation.5.1 System setupThe system is implemented in Java and relies onsome portions of code publicly available in theSemantic Vectors package (Widdows and Ferraro,2008).
For the evaluation of the system, we buildtwo separate WordSpaces using the following cor-pora: ukWaC (Baroni et al, 2009) and TASA.ukWaC contains 2 billion words and it is constructedfrom the Web by limiting the crawling to the .ukdomain and using medium-frequency words fromthe BNC corpus as seeds.
We use only a por-tion of ukWaC corpus consisting of 7,025,587 sen-tences (about 220,000 documents).
The TASA cor-pus (compiled by Touchstone Applied Science As-sociates) was kindly made available to us by Prof.Thomas Landauer from the University of Colorado.The TASA corpus contains a collection of Englishtexts that is approximately equivalent to what the av-erage college-level student has read in his/her life-time.
The TASA corpus consists of about 800,000sentences.To extract syntactic dependencies, we adoptMINIPAR3 (Lin, 2003).
MINIPAR is an efficientEnglish parser, which is suitable for parsing a largeamount of data.
The total amount of extracted de-pendencies is about 112,500,000 for ukWaC and8,850,000 for TASA.Our approach involves some parameters.
We setthe random vector dimension to 4,000 and the num-ber of non-zero elements in the random vector equalto 10.
We restrict the WordSpace to the 40,000 mostfrequent words4.
Another parameter is the set of de-pendencies that we take into account.
In this prelim-inary investigation we consider the four dependen-cies described in Table 1, that reports also the kindof permutation5 applied to vectors.5.2 ResultsIn this section we report some results of queries per-formed in ukWaC and TASA corpus.Table 2 and Table 3 report the results respectivelyfor the queries dep(ti, ?)
and dep(?, tj).
The effectsof encoding syntactic information is clearly visible,as can be inferred by results in the tables.
Moreover,the results with the two corpora are different, as ex-pected, but in many cases the first result of the queryis the same.Our space can be also exploited to perform classi-cal queries in which we want to find ?similar?
words.Tables 4 and 5 report results for TASA and ukWaC3MINIPAR is available athttp://webdocs.cs.ualberta.ca/?lindek/minipar.htm4Word frequency is computed taking into account the se-lected dependencies.5The number of rotations is randomly chosen.48Dependency Description Permutationobj object of verbs ?+7subj subject of verbs ?+3mod the relationship between a word and its adjunct modifier ?+11comp complement ?+23Table 1: The set of dependencies used in the evaluation.corpus, respectively.
The results obtained by similartest are not the typical results expected by classicalWordSpace.
In fact, in Table 5 the word most simi-lar to ?good?
is ?bad?, because they are used in thesame syntactic context, but have opposite meaning.The similarity between words in our space stronglydepends on their syntactic role.
For example, thewords similar to ?food?
are all the nouns which areobject/subject of the same verbs in syntactic relationwith ?food?.Finally, we provide the results of semantic com-position.
Table 6 reports the Spearman correlationbetween the output of our system and the meansimilarity scores given by the humans.
The tableshows results for each types of combination: verb-object, adjective-noun and compound nouns.
To per-form the experiment on compound nouns, we re-build the spaces encoding the ?nn?
relation providedby MINIPAR which refers to compound nouns de-pendency.
Table 6 shows the best result obtainedby Mitchell and Lapata (Mitchell and Lapata, 2008)using the same dataset.
Our method is able to out-perform MLbest and obtains very high results whenadjective-noun combination is involved.Corpus Combination ?TASAverb-object 0.260adjective-noun 0.637compound nouns 0.341overall 0.275ukWaCverb-object 0.292adjective-noun 0.445compound nouns 0.227overall 0.261- MLbest 0.190Table 6: GEMS 2011 Shared Evaluation results.The experiments reported in this preliminary eval-uation are only a small fraction of the experimentsthat are required to make a proper evaluation of theeffectiveness of our semantic space and to compareit with other approaches.
This will be the main fo-cus of our future research.
The obtained results seemto be encouraging and the strength of our approach,capturing syntactic relations, allows to implementseveral kind of queries using only one WordSpace.We believe that the real advantage of our approach,that is the possibility to represent several syntacticrelations, has much room for exploration.6 ConclusionsIn this work, we propose an approach to encode syn-tactic dependencies in WordSpace using vector per-mutations and Random Indexing.
In that space, a setof operations is defined, which relies on the possibil-ity of exploiting syntactic dependencies to performsome particular queries, such as the one for retriev-ing all similar objects of a verb.
We propose an earlyattempt to use that space for semantic compositionof short sentences.
The evaluation using the GEMS2011 shared dataset provides encouraging results,but we believe that there are open points which de-serve more investigation.
We planned a deeper eval-uation of our WordSpace and a more formal studyabout semantic composition.AcknowledgementsThis research was partially funded by MIUR (Min-istero dell?Universita` e della Ricerca) under thecontract Fondo per le Agevolazioni alla Ricerca,DM19410 ?Laboratorio?
di Bioinformatica per laBiodiversita` Molecolare (2007-2011).49obj(provide, ?)
mod(people, ?
)TASA ukWaC TASA ukWaCinformation 0.344 information 0.351 young 0.288 young 0.736food 0.208 service 0.260 black 0.118 with 0.360support 0.143 you 0.176 old 0.089 other 0.223energy 0.143 opportunity 0.141 conquered 0.086 handling 0.164job 0.142 support 0.127 deaf 0.086 impressive 0.162Table 2: Examples of query dep(ti, ?
).obj(?, food) mod(?, good)TASA ukWaC TASA ukWaCeat 0.604 eat 0.429 idea 0.350 practice 0.510make 0.389 serve 0.256 place 0.320 idea 0.363grow 0.311 provide 0.230 way 0.269 news 0.274need 0.272 have 0.177 friend 0.246 for 0.269store 0.161 buy 0.169 time 0.234 very 0.228Table 3: Examples of query dep(?, tj).food provide goodfood 1.000 provide 1.000 good 0.999foods 0.698 make 0.702 best 0.498meat 0.654 restructure 0.693 excellent 0.471meal 0.651 ready 0.680 wrong 0.453bread 0.606 leave 0.673 main 0.430wheato 0.604 mean 0.672 nice 0.428thirty percent 0.604 work 0.672 safe 0.428mezas 0.604 offer 0.671 new 0.428orgy 0.604 relate 0.667 proper 0.400chocolatebar 0.604 gather 0.667 surrounded 0.400Table 4: Find similar words, TASA corpus.food provide goodfood 1.000 provide 0.999 good 1.000meal 0.724 offer 0.855 bad 0.603meat 0.656 supply 0.819 best 0.545pie 0.578 deliver 0.801 anti-discriminatory 0.507tea 0.576 give 0.787 nice 0.478fresh food 0.576 contain 0.786 reflective 0.470supper 0.556 require 0.784 brilliant 0.464porridge 0.553 present 0.782 great 0.462entertainment 0.533 gather 0.778 evidence-based 0.453soup 0.532 work 0.777 unsafe 0.444Table 5: Find similar words, ukWaC corpus.50ReferencesM.
Baroni and A. Lenci.
2010.
Distributional mem-ory: A general framework for corpus-based semantics.Computational Linguistics, 36(4):673?721.M.
Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.2009.
The WaCky Wide Web: A collection of verylarge linguistically processed Web-crawled corpora.Language Resources and Evaluation, 43(3):209?226.S.
Clark and S. Pulman.
2007.
Combining symbolic anddistributional models of meaning.
In Proceedings ofthe AAAI Spring Symposium on Quantum Interaction,pages 52?55.S.
Clark, B. Coecke, and M. Sadrzadeh.
2008.
A com-positional distributional model of meaning.
In Pro-ceedings of the Second Quantum Interaction Sympo-sium (QI-2008), pages 133?140.T.
Cohen, D. Widdows, R.W.
Schvaneveldt, and T.C.Rindflesch.
2010.
Logical leaps and quantum con-nectives: Forging paths through predication space.
InAAAI-Fall 2010 Symposium on Quantum Informaticsfor Cognitive, Social, and Semantic Processes, pages11?13.S.
Dasgupta and A. Gupta.
1999.
An elementary proof ofthe Johnson-Lindenstrauss lemma.
Technical report,Technical Report TR-99-006, International ComputerScience Institute, Berkeley, California, USA.L.
De Vine and P. Bruza.
2010.
Semantic Oscillations:Encoding Context and Structure in Complex ValuedHolographic Vectors.
Quantum Informatics for Cog-nitive, Social, and Semantic Processes (QI 2010).Z.
Harris.
1968.
Mathematical Structures of Language.New York: Interscience.D.
Haussler.
1999.
Convolution kernels on discretestructures.
Technical Report UCSC-CRL-99-10.M.N.
Jones and D.J.K.
Mewhort.
2007.
Representingword meaning and order information in a compositeholographic lexicon.
Psychological review, 114(1):1?37.P.
Kanerva.
1988.
Sparse Distributed Memory.
MITPress.D.
Lin.
2003.
Dependency-based evaluation of MINI-PAR.
Treebanks: building and using parsed corpora.J.
Mitchell and M. Lapata.
2008.
Vector-based modelsof semantic composition.
In Proceedings of ACL-08:HLT, pages 236?244, Columbus, Ohio, June.
Associa-tion for Computational Linguistics.J.
Mitchell and M. Lapata.
2010.
Composition in distri-butional models of semantics.
Cognitive Science.
Toappear.S.
Pado?
and M. Lapata.
2007.
Dependency-based con-struction of semantic space models.
ComputationalLinguistics, 33(2):161?199.M.
Sahlgren, A. Holst, and P. Kanerva.
2008.
Permu-tations as a means to encode order in word space.
InProceedings of the 30th Annual Meeting of the Cogni-tive Science Society (CogSci?08).M.
Sahlgren.
2006.
The Word-Space Model: Us-ing distributional analysis to represent syntagmaticand paradigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, Stockholm:Stockholm University, Faculty of Humanities, Depart-ment of Linguistics.H.
Schu?tze.
1993.
Word space.
In Stephen Jose?
Hanson,Jack D. Cowan, and C. Lee Giles, editors, Advances inNeural Information Processing Systems, pages 895?902.
Morgan Kaufmann Publishers.D.
Widdows and K. Ferraro.
2008.
Semantic Vectors: AScalable Open Source Package and Online TechnologyManagement Application.
In Proceedings of the 6thInternational Conference on Language Resources andEvaluation (LREC 2008).D.
Widdows.
2008.
Semantic vector products: Someinitial investigations.
In The Second AAAI Symposiumon Quantum Interaction.51
