ON THE USE OF T IED-MIXTURE D ISTR IBUT IONSOwen Kimball, Mari OstendorfElectr ical ,  Computer  and Systems Eng ineer ingBoston  Univers i ty,  Boston,  MA 02215ABSTRACTTied-mixture (or semi-continuous) distributions are an im-portant tool for acoustic modeling, used in many high-performance speech recognition systems today.
This paperprovides a survey of the work in this area, outlining thedifferent options available for tied mixture modeling, intro-ducing algorithms for reducing training time, and provid-ing experimental results assessing the trade-offs for speaker-independent recognition on the Resource Management ask.Additionally, we describe an extension of tied mixtures tosegment-level distributions.1.
INTRODUCTIONTied-mixture (or semi-continuous) distributions haverapidly become an important ool for acoustic model-ing in speech recognition since their introduction byHuang and Jack \[1\] and nellegarda nd iahamoo \[2\],finding widespread use in a number of high-performancerecognition systems.
Tied mixtures have a number ofadvantageous properties that have contributed to theirsuccess.
Like discrete, "non-parametric" distributions,tied mixtures can model a wide range of distributionsincluding those with an "irregular shape," while retain-ing the smoothed form characteristic of simpler para-metric models.
Additionally, because the componentdistributions of the mixtures are shared, the number offree parameters i  reduced, and tied-mixtures have beenfound to produce robust estimates with relatively smallamounts of training data.
Under the general headingof tied mixtures, there are a number of possible choicesof parameterization that lead to systems with differentcharacteristics.
This paper outlines these choices andprovides a set of controlled experiments assessing trade-otis in speaker-independent recognition on the ResourceManagement corpus in the context of the stochastic seg-ment model (SSM).
In addition, we introduce new vari-ations on training algorithms that reduce computationalrequirements and generalize the tied mixture formalismto include segment-level mixtures.2.
PREVIOUS WORKA central problem in the statistical approach to speechrecognition is finding a good model for the probabil-ity of acoustic observations conditioned on the state inhidden-Markov models (HMM), or for the case of theSSM, conditioned on a region of the model.
Some of theoptions that have been investigated include discrete dis-tributions based on vector quantization, as well as Gaus-sian, Gaussian mixture and tied-Gaussian mixture dis-tributions.
In tied-mixture modeling, distributions aremodeled as a mixture of continuous densities, but unlikeordinary, non-tied mixtures, rather than estimating thecomponent Gaussian densities eparately, each mixtureis constrained to share the same component densitieswith only the weights differing.
The probability densityof observation vector x conditioned on being in state iis thusp(x Is = i) = Z wikpk(x).
(1)kNote that the component Gaussian densities, Pk(x) -'~N(t~k, ~k), are not indexed by the state, i.
In this light,tied mixtures can be seen as a particular example of thegeneral technique of tying to reduce the number of modelparameters that must be trained \[3\].
"Tied mixtures" and "semi-continuous HMMs" are usedin the literature to refer to HMM distributions of theform given in Equation (1).
The term "semi-continuousHMMs" was coined by Huang and Jack, who first pro-posed their use in continuous speech recognition \[1\].
The"semi-continuous" terminology highlights the relation-ship of this method to discrete and continuous densityHMMs, where the mixture component means are analo-gous to the vector quantization codewords of a discreteHMM and the weights to the discrete observation prob-abilities, but, as in continuous density HMMs, actualquantization with its attendant distortion is avoided.Bellegarda nd Nahamoo independently developed thesame technique which they termed "tied mixtures" \[2\].For simplicity, we use only one name in this paper, andchoose the term tied mixtures, to highlight he relation-ship to other types of mixture distributions and becauseour work is based on the SSM, not the HMM.Since its introduction, a number of variants of the tiedmixture model have been explored.
First, different as-sumptions can be made about feature correlation within102individual mixture components.
Separate sets of tiedmixtures have been used for various input features in-cluding cepstra, derivatives of cepstra, and power andits derivative, where each of these feature sets have beentreated as independent observation streams.
Within anobservation stream, different assumptions about featurecorrelation have been explored, with some researcherscurrently favoring diagonal covariance matrices \[4, 5\] andothers adopting full covariance matrices \[6, 7\].Second, the issue of parameter initialization can be im-portant, since the training algorithm is an iterative hill-climbing technique that guarantees convergence only to alocal optimum.
Many researchers initialize their systemswith parameters e timated from data subsets determinedby K-means clustering, e.g.
\[6\], although Paul describesa different, bootstrapping initialization \[4\].
Often a largenumber of mixture components are used and, since theparameters can be overtrained, contradictory esults arereported on the benefits of parameter re-estimation.
Forexample, while many researchers find it useful to reesti-mate all parameters of the mixture models in training,BBN reports no benefit for updating means and covari-ances after the initialization from clustered ata \[7\].Another variation, embodied in the CMU senone mod-els \[8\], involves tying mixture weights over classes ofcontext-dependent models.
Their approach to finding re-gions of mixture weight ying involves clustering discreteobservation distributions and mapping these clustereddistributions to the mixture weights for the associatedtriphone contexts.In addition to the work described above, there are re-lated methods that have informed the research concern-ing tied mixtures.
First, mixture modeling does not re-quire the use of Gaussian distributions.
Good resultshave also been obtained using mixtures of Laplacian dis-tributions \[9, 10\], and presumably other component den-sities would perform well too.
Ney \[11\] has found strongsimilarities between radial basis functions and mixturedensities using Gaussians with diagonal covariances.
Re-cent work at BBN has explored the use of elliptical basisfunctions which share many properties with tied mix-tures of full-covariance Gaussians \[12\].
Second, the posi-tive results achieved by several researchers u ing non-tiedmixture systems \[13\] raise the question of whether tied-mixtures have significant performance advantages overuntied mixtures when there is adequate training data.It is possible to strike a compromise and use limited ty-ing: for instance the context models of a phone can alluse the same tied distributions (e.g.
\[14, 15\]).Of course, the best choice of model depends on the na-ture of the observation vectors and the amount of train-ing data.
In addition, it is likely that the amount oftying in a system can be adjusted across a continuum tofit the particular task and amount of training data.
flow-ever, an assessment of modeling trade-offs for speaker-independent recognition is useful for providing insightinto the various choices, and also because the variousresults in the literature are difficult to compare due todiffering experimental paradigms.3.
TRAIN ING ALGORITHMSIn this section we first review properties of the SSMand then describe the training algorithm used for tiedmixtures with the SSM.
Next, we describe an effi-cient method for training context-dependent models,and lastly we describe a parallel implementation f thetrainer that greatly reduces experimentation time.3.1.
The SSM and "Viterbi" Trainingwith Tied MixturesThe SSM is characterized by two components: a fam-ily of length-dependent distribution functions and a de-terministic mapping function that determines the dis-tribution for a variable-length observed segment.
Morespecifically, in the work presented here, a linear timewarping function maps each observed frame to one ofm regions of the segment model.
Each region is de-scribed by a tied Gaussian mixture distribution, andthe frames are assumed conditionally independent giventhe length-dependent warping.
The conditional inde-pendence assumption allows robust estimation of themodel's tatistics and reduces the computation ofdeter-mining a segment's probability, but the potential of thesegment model is not fully utilized.
Under this formu-lation, the SSM is similar to a tied-mixture tIMM witha phone-length-dependent, constrained state trajectory.Thus, many of the experiments reported here translateto HMM systems.The SSM training algorithm \[16\] iterates between seg-mentation and maximum likelihood parameter estima-tion, so that during the parameter estimation phase ofeach iteration, the segmentation f that pass gives a setof known phonetic boundaries.
Additionally, for a givenphonetic segmentation, the assignment of observationsto regions of the model is uniquely determined.
SSMtraining is similar to IIMM "Viterbi training", in whichtraining data is segmented using the most likely statesequence and model parameters are updated using thissegmentation.
Although it is possible to define an SSMtraining algorithm equivalent to the Baum-Welch algo-rithm for HMMs, the computation is prohibitive for theSSM because of the large effective state space.103The use of a constrained segmentation greatly simpli-fies parameter estimation in the tied mixture case, sincethere is only one unobserved component, the mixturemode.
In this case, the parameter estimation step of theiterative segmentation/estimation algorithm involves thestandard iterative expectation-maximization (EM) ap-proach to estimating the parameters of a mixture distri-bution \[17\].
In contrast, the full EM algorithm for tiedmixtures in an HMM handles both the unobserved statein the Markov chain and the unobserved mixture mode\[21.3.2.
Tied-Mixture Context ModelingWe have investigated two methods for training context-dependent models.
In the first, weights are used to com-bine the probability of different ypes of context.
Theseweights can be chosen by hand \[18\] or derived automat-ically using a deleted-interpolation algorithm \[3\].
Paulevaluated both types of weighting for tied-mixture con-text modeling and reported no significant performancedifference between the two \[4\].
In our experiments, weevaluated just the use of hand-picked weights.In the second method, only models of the most de-tailed context (in our case triphones) are estimated i-rectly from the data and simpler context models (left,right, and context-independent models) are computedas marginals of the triphone distributions.
The com-putation of marginals is negligible since it involves justthe summing and normalization of mixture weights atthe end of training.
This method reduces the number ofmodel updates in training in proportion to the numberof context ypes used, although the computation of ob-servation probabilities conditioned on the mixture com-ponent densities, remains the same.
In recognition withmarginal models, it is still necessary to combine the dif-ferent context ypes, and we use the same hand-pickedweights as before for this purpose.
We compared thetwo training methods and found that performance on anindependent test set was essentially the same for bothmethods (marginal training produced 2 fewer errors onthe Feb89 test set) and the marginal trainer required20 to 35% less time, depending on the model size andmachine memory.3.3.
Parallel TrainingTo reduce computation, our system prunes low probabil-ity observations, as in \[4\], and uses the marginal trainingalgorithm described above.
However, even with thesesavings, tied-mixture training involves a large computa-tion, making experimentation potentially cumbersome.When the available computing resources consist of a net-work of moderately powerful workstations, as is the caseat BU, we would like to make use of many machinesat once to speed training.
At the highest level, tiedmixture training is inherently a sequential process, sinceeach pass requires the parameter estimates from the pre-vious pass.
However, the bulk of the training compu-tation involves estimating counts over a database, andthese counts are all independent of each other.
We cantherefore speed training by letting machines estimate thecounts for different parts of the database in parallel andcombine and normalize their results at the end of eachpass.To implement this approach we use a simple "bakery" al-gorithm to assign tasks: as each machine becomes free, itreads and increments the value of a counter from a com-mon location indicating the sentences in the databaseit should work on next.
This approach provides loadbalancing, allowing us to make efficient use of machinesthat may differ in speed.
Because of the coarse grain ofparallelism (one task typically consists of processing 10sentences), we can use the relatively simple mechanismof file locking for synchronization a d mutual exclusion,with no noticeable fficiency penalty.
Finally, one pro-cessor is distinguished as the "master" processor and isassigned to perform the collation and normalization ofcounts at the end of each pass.
With this approach, weobtain a speedup in training linear with the number ofmachines used, providing a much faster environment forexperimentation.4.
MODELING & ESTIMATIONTRADE-OFFSWithin the framework of tied Gaussian mixtures, thereare a number of modeling and training variations thathave been proposed.
In this section, we will describe sev-eral experiments hat investigate the performance impli-cations of some of these choices.4.1.
Experimental ParadigmThe experiments described below were run on theResource Management (RM) corpus using speaker-independent, gender-dependent models trained on thestandard SI-109 data set.
The feature vectors used asinput to the system are computed at 10 millisecond in-tervals and consist of 14 cepstral parameters, their firstdifferences, and differenced energy (second cepstral dif-ferences are not currently used).
In recognition, the SSMuses an N-best rescoring formalism to reduce computa-tion: the BBN BYBLOS system \[7\] is used to generate20 hypotheses per sentence, which are rescored by theSSM and combined with the number of phones, num-ber of words, and (optionally) the BBN HMM score, torerank the hypotheses.
The weights for recombination104are estimated on one test set and held fixed for all othertest sets.
Since our previous work has indicated prob-lems in weight estimation due to test-set mismatch, wehave recently introduced a simple time normalization ofthe scores that effectively reduces the variability of scoresdue to utterance length and leads to more robust perfor-mance across test sets.Although the weight estimation test set is strictly speak-ing part of the training data, we find that for most ex-periments, the bias in this type of testing is small enoughto allow us to make comparisons between systems whenboth are run on the weight-training set.
Accordinglysome of the experiments reported below are only run onthe weight training test set.
Of course, final evaluationof a system must be on an independent test set.4 .2 .
Exper imentsWe conducted several series of experiments to exploreissues associated with parameter allocation and train-ing.
The results are compared to a baseline, non-mixtureSSM that uses full covariance Gaussian distributions.The first set of experiments examined the number ofcomponent densities in the mixture, together with thechoice of full- or diagonal-covariance matrices for themixture component densities.
Although the full covari-ance assumption provides a more detailed descriptionof the correlation between features, diagonal covariancemodels require substantially ess computation and it maybe possible to obtain very detailed models using a largernumber of diagonal models.In initial experiments with just female speakers, we useddiagonal covariance Gaussians and compared 200- ver-sus 300-density mixture models, exploring the rangetypically reported by other researchers.
With context-independent models, after several training passes, bothsystems got 6.5% word error on the Feb89 test set.
Forcontext-dependent models, the 300-density system per-formed substantially better, with a 2.8% error rate, com-pared with 4.2% for the 200 density system.
These re-sults compare favorably with the baseline SSM whichhas an error rate on the Feb89 female speakers of 7.7%for context-independent models and 4.8% for context-dependent models.For male speakers, we again tried systems of 200 and300 diagonal covariance density systems, obtaining errorrates of 10.9% and 9.1% for each, respectively.
Unlikethe females, however, this was only slightly better thanthe result for the baseline SSM, which achieves 9.5%.We tried a system of 500 diagonal covariance densities,which gave only a small improvement in performance to8.8% error.
Finally, we tried using full-covariance Gaus-sians for the 300 component system and obtained an8.0% error rate.
The context-dependent performance formales using this configuration showed similar improve-ment over the non-mixture SSM, with an error rate of3.8% for the mixture system compared with 4.7% for thebaseline.
Returning to the females, we found that us-ing full-covariance densities gave the same performanceas diagonal.
We have adopted the use of full-covariancemodels for both genders for uniformity, obtaining a com-bined word error rate of 3.3% on the Feb89 test set.In the RM SI-109 training corpus, the training data formales is roughly 2.5 times that for females, so it is notunexpected that the optimal parameter allocation foreach may differ slightly.Unlike other reported systems which treat cepstral pa-rameters and their derivatives as independent observa-tion streams, the BU system models them jointly usinga single output stream, which gives better performancethan independent streams with a single Gaussian dis-tribution (non-mixture system).
Presumably, the resultwould also hold for mixtures.Since the training is an iterative hill climbing tech-nique, initialization can be important o avoid converg-ing to a poor solution.
In our system, we choose ini-tial models, using one of the two methods described be-low.
These models are used as input to several iterationsof context-independent training followed by context-dependent training.
We add a small padding value tothe weight estimates in the early training passes to de-lay premature parameter convergence.We have investigated two methods for choosing the ini-tial models.
In the first, we cluster the training datausing the K-means algorithm and then estimate a meanand covariance from the data corresponding to each clus-ter.
These are then used as the parameters of the compo-nent Gaussian densities of the initial mixture.
In the sec-ond method, we initialize from models trained in a non-mixture version of the SSM.
The initial densities are cho-sen as means of triphone models, with covariances chosenfrom the corresponding context-independent model.
Foreach phone in our phone alphabet we iteratively choosethe triphone model of that phone with the highest fre-quency of occurrence in training.
The object of this pro-cedure is to attempt o cover the space of phones whileusing robustly estimated models.We found that the K-means initialized models convergedslower and had significantly worse performance on inde-pendent est data than that of the second method.
Al-though it is possible that with a larger padding valueadded to the weight estimates and more training passes,the K-means models might have "caught up" with the105SystemBaseline SSMT.M.
SSMT.M.
SSM + HMMTest setOct 89 Sep 924.8 8.53.6 7.33.2 6.1Table 1: Word error rate on the Oct89 and Sep92 testsets for the baseline non-mixture SSM, the tied-mixtureSSM alone and the SSM in combination with the BYB-LOS HMM system.other models, we did not investigate his further.The various elements of the mixtures (means, covari-ances, and weights) can each be either updated in train-ing, or assumed to have fixed values.
In our experiments,we have consistently found better performance when allparameters of the models are updated.Table 1 gives the performance on the RM Oct89 andSept92 test set for the baseline SSM, the tied-mixtureSSM system, and the tied-mixture system combined inN-best rescoring with the BBN BYBLOS HMM system.The mixture SSM's performance is comparable to resultsreported for many other systems on these sets.
We notethat it may be possible to improve SSM performance byincorporating second difference cepstral parameters asmost HMM systems do.5.
SEGMENTAL  MIXTUREMODEL INGIn the version of the SSM described in this paper, inwhich observations are assumed conditionally indepen-dent given model regions, the dependence of observationsover time is modeled implicitly by the assumption oftime-dependent stationary regions in combination withthe constrained warping of observations to regions.
Be-cause segmentation is explicit in this model, in principleit is straightforward to model distinct segmental trajec-tories over time by using a mixture of such segment-levelmodels, and thus take better advantage of the segmentformalism.
The probability of the complete segment ofobservations, Y, given phonetic unit c~ is thenP(Y I a) = E wk P(Y I ak),kwhere each of the densities P(Y\]trk) is an SSM.
Thecomponent models could use single Gaussians insteadof tied mixtures for the region dependent distributionsand they would remain independent frame models, butin training all the observations for a phone would beupdated jointly, so that the mixture components capturedistinct rajectories ofthe observations across a completesegment.
In practice, each such trajectory isa point in avery high-dimensional feature space, and it is necessaryto reduce the parameter dimension i  order to train suchmodels.
There are several ways to do this.
First, wecan model the trajectories within smaller, subphoneticunits, as in the microsegment model described in \[19, 20\].Taking this approach and assuming microsegments areindependent, the probability for a segment isP(Y I?t) = H E wjk P(Yj I oqk), (2)j kwhere aik is the k th mixture component of microseg-ment j and Yj is the subset of frames in Y that map tomicrosegment j.
Given the SSM's deterministic warp-ing and assuming the same number of distributions forall mixture components of a given microsegment, theextension of the EM algorithm for training mixtures ofthis type is straightforward.
The tied-mixture SSM dis-cussed in previous ections is a special case of this model,in which we restrict each microsegment to have just onestationary region and a corresponding mixture distribu-tion.A different way to reduce the parameter dimension is tocontinue to model the complete trajectory across a seg-ment, but assume independence b tween subsets of thefeatures of a frame.
This case can be expressed in thegeneral form of (2) if we reinterpret the Yj as vectorswith the same number of frames as the complete seg-ment, but for each frame, only a specific subset of theoriginal frame's features are used.
We can of course com-bine these two approaches, and assume independencebetween observations representing feature subsets of dif-ferent microsegmental units.
There are clearly a largenumber of possible decompositions of the complete seg-ment into time and feature subsets, and the correspond-ing models for each may have different properties.
Ingeneral, because of constraints of model dimensionalityand finite training data, we expect a trade-off betweenthe ability to model trajectories across time and to modelthe correlation of features within a local time region.Although no single model of this form may have all theproperties we desire, we do not necessarily have to chooseone to the exclusion of all others.
All the models dis-cussed here compute probabilities over the same obser-vation space, allowing for a straightforward combinationof different models, once again using the simple mecha-nism of non-tied mixtures:P(Y I oc) = ~I~wi jkP(Y j la i j k ) .i j kIn this case, each of the i components of the leftmostsummation is some particular ealization of the general106model expressed in Equation (2).
Such a mixture cancombine component models that individually have ben-eficial properties for modeling either time or frequencycorrelation, and the combined model may be able tomodel both aspects well.
We note that, in principle,this model can also be extended to larger units, such assyllables or words.6.
SUMMARYThis paper provided an overview of work using tied-mixture models for speech recognition.
We described theuse of tied mixtures in the SSM as well as several innova-tions in the training algorithm.
Experiments comparingperformance for different parameter allocation choicesusing tied-mixtures were presented.
The performanceof the best tied-mixture SSM is comparable to HMMsystems that use similar input features.
Finally, we pre-sented a general method we are investigating for model-ing segmental dependence with the SSM.ACKNOWLEDGMENTSThe authors gratefully acknowledge BBN Inc. for theirhelp in providing the N-best sentence hypotheses.
Wethank J. Robin Rohlicek of BBN for many useful dis-cussions.
This research was jointly funded by NSF andDARPA under NSF grant number IRI-8902124, and byDARPA and ONft under ONR grant number N00014-92-J-1778.References1.
Huang, X. D. and Jack, M. A., "Performance com-parison between semi-continuous and discrete hiddenMarkov models," IEE Electronics Letters, Vol.
24 no.3, pp.
149-150.2.
Bellegarda, J. R. and Nahamoo, D., "Tied Mixture Con-tinuous Parameter Modeling for Speech Recognition,"1EEE Trans.
on Acoust., Speech and Signal Processing,Dec 1990, pp.
2033-2045.3.
Jelinek, F. and Mercer, R.L., "Interpolated Estimationof Markov Source Parameters from Sparse Data," inProc.
Workshop Pattern Recognition in Practice, May1980, pp.
381-397.4.
Paul, D.B., "The Lincoln Tied-Mixture HMM Continu-ous Speech Recognizer," Proc.
1EEE Int.
Conf.
Acoust.,Speech, Signal Processing, May 1991, pp.
329-332.5.
Murveit, H., Butzberger, J., Weintraub, M., "SpeechRecognition in SRI's Resource Management and ATISSystems," Proc.
of the DARPA Workshop on Speech andNatural Language, June 1990, pp.
94-100.6.
Huang, X.D., Lee, K.F., I-Ion, H.W., and Hwang, M.-Y.,"Improved Acoustic Modeling with the SPHINX SpeechRecognition System," Proc.
1EEE Int.
Conf.
Acoust.,Speech, Signal Processing, May 1991, pp.
345-348.7.
Kubaia, F., Austin, S., Barry, C., Makhoul, J. Place-way, P., and Schwartz, R., "BYBLOS Speech Recogni-tion Benchmark Results," Proc.
of the DARPA Work-shop on Speech and Natural Language, Asilomar, CA,Feb.
1991, pp.
77-82.8.
Hwang, M.-Y., I-Iuang, X. D., "Subphonetic Modelingwith Markov States - Senone," Proc.
IEEE Int.
Conf.Acoust., Speech, Signal Processing, March 1992, pp.
1-33-36.9.
Ney, H., Hacb-Umbach, R., Tran, B.-H., Oerder, M.,"Improvements in Beam Search for 10000-Word Con-tinuous Speech Recognition," Proc.
IEEE Int.
Conf.Acoust., Speech, Signal Processing, April 1992, pp.
I-9-12.10.
Baker, J. K., Baker, J. M., Bamberg, P., Bishop, K.,Gillick, L., Helman, V., Huang, Z., Ito, Y., Lowe, S.,Peskin, B., Roth, R., Scattone, F., "Large VocabularyRecognition of Wail Street Journal Sentences at DragonSystems," Proc.
of the DARPA Workshop on Speech andNatural Language, February 1992, pp.
387-392.11.
H. Ney, "Speech Recognition in a Neural NetworkFramework: Discriminative Training of Gaussian Mod-els and Mixture Densities as Radial Basis Functions,"Proc.
IEEE Int.
Conf.
Acoust., Speech, Signal Process-ing, May 1991, pp.
573-576.12.
Zavaliagkos, G., Zhao, Y., Schwartz, R., andMakhoul, J., to appear in Proc.
of the DARPA Workshopon Artificial Neural Networks and CSR, Sept. 1992.13.
Pailett, D., Results for the Sept. 1992 Resource Manage-ment Benchmark, presented at the DARPA Workshopon Artificial Neural Networks and CSR, Sept. 1992.14.
Lee, C., Rabiner, L., Pieraccini, R., and Wilpon, J.,"Acoustic Modeling for Large Vocabulary Speech Recog-nition," Computer Speech and Language, April.
1990,pp.
127-165.15.
Paul, D. B., "The Lincoln Robust Continuous SpeechRecognizer," Proc.
IEEE Int.
Conf.
Acoust., Speech,Signal Processing, May 1989, pp.
449-452.16.
Ostendorf, M. and Roukos, S. , "A Stochastic SegmentModel for Phoneme-Based Continuous Speech Recogni-tion," IEEE Trans.
on Acoust., Speech and Signal Pro-cessing, Dec. 1989, pp.
1857-1869.17.
Dempster, A., Laird, N. and Rubin, D., "MaximumLikelihood from Incomplete Data via the EM Algo-rithm," J. Royal Statist.
Sac.
Set.
B, Vol.
39 No.
1, pp.1-22, 1977.18.
Schwartz, R., Chow, Y. L., Kimball, O., Roucos, S.,Krasner, M. and Makhoul, J., "Context-DependentModeling for Acoustic-Phonetic Recognition of Contin-uous Speech," Proc.
IEEE Int.
Conf.
Acoust., Speech,Signal Processing, March 1985, pp.
1205-1208.19.
Digaiakis, V. Segment-Based Stochastic Models ofSpectral Dynamics for Continuous Speech Recognition,Boston University Ph.D. Dissertation, 1992.20.
Kannan, A., and Ostendorf, M., "A Comparison of Tra-jectory and Mixture Modeling in Segment-Based WordRecognition," Proc.
IEEE Int.
Conf.
Acoust., Speech,Signal Processing, April 1993.107
