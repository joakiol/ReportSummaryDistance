COLING 82, J. Horeck~ {ed.
)North-Holland Publishing Company?
Academ~ 1982INCREMENTAL SENTENCE GENERATION: IMPLICATIONS FORTHE STRUCTURE OF A SYNTACTIC PROCESSORGerard Kempen & Edward HoenkampDepartment of PsychologyUniversity of Nijmegen, The NetherlandsitHuman speakers often produce sentences incremen-tally.
They can start speaking having in mind onlya fragmentary idea of what they want to say, andwhile saying this they refine the contents underly-ing subsequent parts of the utterance.
This capa-b i l i ty  imposes a number of constraints on thedesign of a syntactic processor.
This paperexplores these constraints and evaluates somerecent computational sentence generators from theperspective of incremental production.An important characteristic of spontaneous speech is that overtpronunciation of a sentence can be initiated before the speaker hascompletely worked out the conceptual content he is going to expressin that sentence.
Apparently, the speaker is able to build up a syn-tactically coherent utterance out of a series of syntactic fragmentseach rendering a new part of the conceptual content.
This inc remen-tal, piecemeal mode of sentence generation imposes some importantconstraints on the design of possible mechanisms for building syntac-tic structures.CONSTRAINTS ON INCREMENTAL SYNTACTIC PROCESSORS1.
Lexically driven syntactic processing.
The first constraintderives from the fact that it is conceptual structures which serve asinput to the tree formation process.
A good strategy for translatingthese meanings into language begins by looking up words covering them("lexicalization").
Subsequently, the processor attemps to build asyntactic framework which accommodates all words while respectingtheir syntactic properties (e.g.
word class).
In case of success,the result is a syntactic tree with lexical items as terminal nodes.In case of failure, one or more words are replaced by other lexicalmaterial which expresses the same meaning but whose syntactic proper-ties are more favorable.
The point we want to make here is that it isthe syntactic properties and pecu l ia r i t ies  of lexical items whichguide the tree formation process.In short, syntactic processing is lexically driven.
This featurerequires special rules not found in current linguistic grammars whereit is common practice to set up a linguistic framework (e.g., byapplying phrase-structure rules) without reference to syntactic pro-perties of lexical items \[I\].
Adopting this practice would presupposethat syntactic trees are directly computable from the shape of con-ceptual structures, that is, without the intermediation of lexicalitems.
This supposition is valid only for conceptual structures whichare virtually isomorphic with syntactic trees.
Most probably, such anisomorphism does not hold for the structures delivered by the concep-tualization system in human speakers.151152 G. KEMPEN and E. HOENKAMP2.
Hierarchy and order of constituents computed by differentcomponents.
The second constraint hinges upon the-~ndependencebetween the order of conceptual fragments coming in and the order ofthe corresponding syntactic fragments.
With the possible exception oflanguages with extremely flexible word order, grammar rules do notalways permit a new syntactic fragment to be simply appended to theright-hand side of the current tree.
Other spatial arrangements ofthe new fragment with respect to the current syntactic tree are pos-sible, depending on the word order rules in the grammar.
Sometimesthese rules even ask for the presence of other elements between thecurrent tree and a newly computed syntactic fragment.
A clear exam-ple is provided by the position of verbs in main clauses of Dutch andGerman.
Subject noun phrases and adverbial phrases cannot follow eachother at the beginning of a main clause.
The finite main verb or aux-iliary is always in between: either NP-V-AP or AP-V-NP but not NP-AP-V or AP-NP-V. Grammars which use some version of the traditionalphrase-structure rules do not keep word order apart from phrasemembership (more precisely, constituent hierarchy from constituentorder).
For example, consider the following rules which express theabove word order contingencies:S .... > NP+V+APS .... > AP+V+NPNow suppose that the syntactic processor is working on a conceptualfragment which lexicalizes into a verb, and applies the first rulewhich says, among other things, that the verb needs an NP at itsleft-hand side.
In the meantime a new conceptual fragment has come inwhich receives the syntactic shape of an AP.
The first rule doeshave an AP slot, but not to the left of the verb.
This implies thesyntactic processor has to wait for a third conceptual fragment whichcan be worded in the form of an NP.
At that point the syntactic pro-cessor can deliver its first output: an NP-V-AP utterance.
The wait-ing time, that is, the period between onset of (conceptual) input andonset of (syntactic) output, would have beenshor ter ,  had the syntac-tic processor picked the second phrase-structure rule.
Then, outputcould already have begun after the second conceptual fragment ("AP-V...") and closed off grammatically with "...NP".
Because the orderof conceptual fragments is unknown~in advance, the syntactic proces-sor can never be sure of having ~ade the best choice between rules.This problem does not arise in a rule system which allows word orderto be computed independently of phrase membership.
We conclude,therefore, that in an incremental syntactic processor it is desirableto have seperate components for tree (or rather "mobile") formationand for word order.3.
Explicit computation of grammatical .
(functional) relationships.Traditional phrase-struct-ure rules allow grammatical relationships(subject, direct object, nominal modifier, etc.)
to be inferred fromconfigurations of categorial nodes in the syntactic tree.
This isnot true of tree formation rules which leave left-to-right order ofconstituents undefined.
If such a system contained a ruleVP .
.
.
.
> V-NP-NPit would be impossible to determine which of the NPs served the func-tion of direct object.
Functional information of this kind is neededby the word order component (or, in languages with free word order,by the morphological case component).INCREMENIAL SENTENCE GENERATION 153An addit ional  mot ivat ion for d irect  computat ion of funct ional  syntac-tic re lat ionships is provided by the lex ica l izat ion process.
Con-straint #I makes the prescr ipt ion that it is choice of lexical  itemswhich guide the formation of syntact ic  trees rather than vice versa.Many lexical  items require a speci f ic  syntact ic  environment,  a typi-cal example being verbs l ike give which cause the format ion a VP withtwo NPs, for direct and indirect  object respect ively.
In conjunct ionwith customary phrase-structure rules, this property of g ive could beexpressed in terms of a desired conf igurat ion of categor ia l  nodes,e.g.
(VP(NP )(NP )).
This opt ion is not avai lab le  in a tree for-mat ion system which generates "mobiles".
Here, the lexical  entry forgive should expl ic i t ly  reference direct  and indirect  objects asdesired const i tuents.4.
S imultaneous construct ion of paral le l  branches of syntact ictrees.
Constra int  #2 entai ls  a work schedul ing problem: if more thanone branch is to descend from a given node, in what order should theybe constructed by the syntact ic processor?
The standard solution,i.e.
to develop const i tuents  in their order of appearance in surfacestructure, is no longer appl icable  since le f t - to- r ight  order is unde-fined at this stage \[2\].
In a lexical ly  dr iven syntact ic processor,the most ef f ic ient  so lut ion is a pr ior i ty  scheme based on the orderof arr ival  of lexical  items (cf.
Constra int  #I).
This order, in turn,is the combined result of the order in which conceptual  fragmentsbecome avai lab le  and the manner of operat ion of the lex ica l izat ionprocess, and need not corrrespond at all to their surface structureorder.
For example, the verb-second rule of German and Dutch appl iesi r respect ive of whether the verb comes into play ear l ier  or la terthan other lexical mater ia ls  of the main clause.When comput ing a branch connect ing a lexical  item to the current  syn-tactic tree, the processor has to take into account the funct ionalrelat ions this item mainta ins  with other lexical  items put forward bythe lex ica l izat ion process (cf.
Constra int  #3)- For example, thenoun designated as subject of a verb wil l  receive a d i f ferent  placein the syntact ic tree than the object noun.
For the rest, there areno cross-branch computat ional  dependencies  forcing a systematic  orderupon the construct ion of branches of  syntact ic  trees.
This statementis supported by the success most grammar types obta in by havingcontext- f ree rules generate deep structure trees (and sometimes evensurface strUcture trees).
This impl ies we can trust the above prior-ity scheme (simply fo l low order of arr ival  of lexical  items) eventhough it does not br ing any syntact ic  expert ise to bear.
Now supposethe syntact ic processor is capable of a certa in amount of paral le lprocessing.
We are then permitted to assume that lexical  items areattached to the tree s imul taneous ly  (again respect ing their order ofarr ival)  rather than sequential ly.
As a matter  of fact, this is whathuman speakers seem to do, as wi tnessed by certa in speech errorphenomena (Garrett, 1975) and by react ion times to init iate sentences(Kempen & Huijbers, in press).5.
Operat ions on syntact ic  trees su__~ect to local i ty  constraints.In an incremental  syntact ic processor,  the appl icat ion of tree forma-tion and word order rules wil l  often yield "narrow" trees dominat ingsmall sentence fragments?
Now suppose that some trees have to undergocertain obl igatory operat ions (e.g., t ransformat ions  to be executedor anaphor ic  re lat ionships to be establ ished) and that such opera-tions are tr iggered as soon as the tree matches a speci f ic  pattern.One can imagine "hor izontal"  ( left - to-r ight)  tr igger patterns span-ning a number paral le l  branches, "vert ical"  (top-down) patterns154 G. KEMPEN and E. HOENKAMPspecifying some configuration of dominating and dominated nodes onone branch, or "mixed" patterns.
The pattern that triggers passivi-zation is an example of a horizontal one involving several parallelbranches (object NP, passive marker, main verb, and optional subjectNP).
Wh-fronting and Raising transformations are triggered by verti-cal or nearly vertical patterns.
Incremental production favors vert-ical trigger patterns because they are more easily satisfied by nar-row (partial) syntactic trees corresponding to fragmentary conceptualinputs.
Horizontal patterns can only be matched by "wider" syntactictrees which correspond to more elaborate conceptual structures.
Thislatter requirement, however, runs counter to the very idea of incre-mental sentence production.The interesting point is that conditions on transformations and otherlinguistic rules can usually be expressed in terms of (nearly) verti-cal node configurations.
A clear example is provided by Koster's(1978) Local ity Principles where a central role is played by domi-nance and command relationships between nodes.
In the context of acomputer implementation of our Procedural Grammar (Kempen & Hoenkamp,1981), which was specially designed for the purpose of incrementalproduction, we have attempted to make more precise the parallelismbetween vertical trigger patterns and locality constraints ontransformations (see Hoenkamp, 1982, for a more formal approach).6.
Lexical transformations.
By constructing a partial syntactictree the syntactic processor commits itself to a limited range ofcontinuation alternatives.
The lexiealization process should be sen-sitive to such syntactic commitments by making available lexiealmaterials which are compatible with what has been said in earlierparts of the sentence.
Take the example of a concept which, afterhaving been expressed as a subject NP, turns out to be the patient ofan action, as specified in a subsequent conceptual fragment.
A typl-eal lexlcal realization of the action might be an active verb whichprescribes the patient to be rendered as the object NP.
However,this would entail incompatibi l ity with the current syntactic tree.The solution is provided by a lexical passive transformation that,among other things, alters the pairings of conceptual cases with syn-tactic functions (causing patient to go with subject).
Thetransformed lexical item is then successfully attached to the currenttree.
By attuning lexical items to the exigencies of incomplete syn-tactic trees, the lexicalization component greatly enhances theleft-to-right planning capabil it ies of the syntactic processor \[3\].7.
Sentence production in two stages.
In the foregoing we have notyet touched upon the issue--of where and when inflectional computa-tions are carried out.
The obvious placement of an inflectional com-ponent -- somewhere at the end of the tree formation process -- leadsto an interesting problem.
In many languages, including English,German and Dutch, clitics (monosyllabic function words) are option-ally contracted with preceding lexlcal items.
Some examples are Johnis Ill --> John's ill; will not --> won't; (Get.)
unter dem Turm -->unterm Turm ("under the to-w-6r"--~ yon dem --> vom ("o--f-~e'-~..'~.Cli-tie contraction implies merging the lexical items of two adjacentbranches of a syntactic tree into a single word.In the context of Constraint #4 we have seen that the most efficientorder of constructing branches of a syntactic tree simply copies theorder of arrival of lexical items.
This, in turn, implies that cliticcontraction cannot be performed by the tree formation components(including the word order component): there is always a chance that aINCREMENTAL SENTENCE GENERATION 155later lexical item gets hold of a place inbetween the clitic and itspredecessor.
For instance, John be ill might be expanded into Johnwill be ill, or yon ~em into yon all dem ("of all the ...").
There-fore, clitic contraction must take place after tree formation, thatis, after the moment the syntactic processor decides that the currenttree (possibly an incomplete one) is a suitable l inguistic formula-tion of the conceptual input received so far.
It follows there is asubsequent stage of processing which takes care of clitic contrac-tion, and maybe of other aspects of the morphological shape of words.This latter addition is plausible from the point of view of effi-ciency.
It does not make sense to have the tree formation componentsengage in detailed inflectional computations if some of these areundone later (namely, the computations that are superseded by cliticcontraction).It is a remarkable fact that speech error data have given rise to atwo-stage sentence production model with a similar division of laborbetween stages: roughly, syntactic tree formation versus inflectional.morphology (Garrett, 1975; Kempen & Huijbers, in press).
These dataalso suggest that the second processing stage deals with the terminalnodes of a (possibly incomplete) syntactic tree in their left-to-right order.INCREMENTAL SENTENCE PRODUCTION IN MODELS OF THE SPEAKERIt will come as no surprise to the reader that the only computationalmodel of sentence production which, in the authors opinion, satis-fies all or most of the above constaints, is the one developed by theauthors themselves (Kempen & Hoenkamp, 1981).
We know of one othercomputational sentence generator whose design was explicit ly con-cerned with incremental production.
It was written by McDonald(1980, 1982) and embodies a broad range of syntactic constructions.However, this model fails to distinguish hierarchical from word orderrules and, consequently, violates Constraint #3.
We cannot judgewhether removal of this shortcoming will necessitate drastic changesto the rest of the program.The type of grammar embodied by the Kempen & Hoenkamp model (Pro-cedural Grammar) is similar to Lexical Functional Grammar (Kaplan &Bresnan, 1982; see also Bresnan, 1981).
The main difference concernsthe attitude towards transformations.
In Lexical Functional Grammar,surface trees are base-generated and no transformational component isneeded.
If Kaplan & Bresnan motivate their rejection of a transforma-tional component on psychological grounds \[4\], we disagree.
Neitherincremental product ion nor any other known fact about human sentenceproduction processes argues for complete banishment of transforma-tional operations on syntactic trees.Procedural Grammar is unique in its abil ity to deal effectively withconceptual inputs which may change on line.
A conceptual structurewhich is altered after it has been expressed l inguistical ly causesthe processor to backtrack and to make "repairs".Acknowledgements.
The work reported in this paper was supported by agrant from ZWO, the Netherlands Organization for Pure ScientificResearch.
We are indebted to Patrick Hudson for his valuable com-ments.156 G. KEMPEN and E. HOENKAMPNOTES\ [ I \ ]  Categcrial grammars form an exception here.
However, a processorbased on this grammar type violates Constraints #2 and #3:categorial rules presuppose left-to-right order of lexical items,and make no use of functional syntactic relations.\[2\] This solution is the one that has been adopted of old, from Yngve(1960), via ATN-based generators (e.g.
Simmons & Slocum, 1972;Anderson, 1976) to McDonald (1980).\[3\] Lexical transformations may involve other types of alterations aswell, e.g., derivational morphological operations and insertion'of function words.
Actually, the addition of function words andinflections (or, rather, inflectional presciptions) is anothergeneral possibility for the lexiealization component to accommo-date a lexieal item to properties of the current syntactic tree.
(Inflectional prescriptions are executed during a subsequent pro-cessing stage; see Constraint #7.
)\[4\] Book (1982, p. 28) opts for Gazdar's (1981) context-free grammarsbecause they are "much mope compatible with on-line processingmodels than transformational grammars".REFERENCESAnderson, J.
Language, memory, and thought.
Hillsdale, N.J.: Erl-baum, 1976.Book, J.K. Toward a cognitive psychology of syntax: information pro-cessing contributions to sentence formulation.
PsychologicalReview, 1982, I, 1-47.Bresnan, J.
An approach to Universal Grammar and the mentalrepresentation of language.
Cognition, 1981, I0, 39-52.Garrett, M. The analysis of sentence production.
In: G.
Bower(ed.
), The psychology of learning and motivation, Vol.
9.
NewYork: Academic Press, 1975.Oazdar, G. Unbounded dependencies and coordinate structure.Linguistic Inquiry, 1981, 12, 155-184.Hoenkamp, E. Aspecten van een computermodel van de spreker.
Ph.D.Dissertation, University of Nijmegen, 1982 (in prep.
).Kaplan, R.M.
& Bresnan, J. Lexical-Functional Grammar: a formal sys-tem for grammatical representation.
To appear in: Bresnan, J.(ed.
), The mental representation of grammatical relations.
Cam-bridge, Mass.
: MIT Press, 1982.Kempen, G. & Hoenkamp, E. A procedural grammar fop sentence produc-tion.
Report 81FU 03.
Department of Psychology, University ofNijmegen, 1981.Kempen, G. & Huijbers, P. The lexicalization process in sentenceproduction and naming: indirect election of words.
Cognition, inpress.Koster, J.
Locality principles in syntax.
Dordrecht: Foris, 1978.McDonald, D. Natural language production as a process of decision-making under constraints.
Ph.D. Dissertation, MIT, 1980.McDonald, D. Natural language generation as a computational problem:an introduction.
To appear in: Brady (ed.
), Computationaltheories of discourse.
Cambridge, Mass.
: MIT, 1982.Simmons, R. & Slooum, J.
Generating English discourse from semanticnetworks.
Communications of the ACM, 1972, 15, 891-905.Yngve, V. A model and a hypothesis for language structure.
Proo.Amer.
Phil.
Sot., 1960, 104, 444-466.
