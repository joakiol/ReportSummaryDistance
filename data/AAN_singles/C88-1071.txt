SPEECH RECOGNIT ION AND THE FREQUENCY OF RECENTLY  USED WORDSA MODIF IED MARKOV MODEL FOR NATURAL LANGUAGERoland KuhnSchool of Computer Science, MeGill University805 Sherbrooke St. West, MontrealAbst rac tSpeech recognition systems incorporate a language modelwhich, at each stage of the recognition task, assigns a probabil-ity of occurrence to each word in the vocabulary.
A class of Mar-kov langnage models identified by Jclinek has achieved consider-.able success in this domain.
A modification of the Markovapproach, wblch assigns higher probabilities to recently usedwords, is proposed and tested against a pure Markov model.Parameter calculation and comparison of the two models bothinvolve use of the LOB CorPus of tagged modern English.1 In t roduct ionSpeech recognition systems consist of two components.
Anacoustic component matches the most recent acoustic input towords in its vocabulary, producing a list of the most plausibleword candidates together with a probability for each.
The secondcomponent, which incorporates a language model, utilizes thestring of previously identified words to estimate for each word inthe vocabulary the probability that it will occur next.
Each wordcandidate originally selected by the acoustic omponent is thusassociated with two probabilities, the first based on itsresemblance to the observed signal and the second based on thelinguistic plausibility of that word occurring immediately afterthe previously recognized words.
Multiplication of these twoprobabilities produces an overall probability for each wordcandidate.Our work focuses on the language model incorporated inthe second component.
The language model we use is based on aclass of Markov models identified by Jelinek, the "n-gram" and"Mg-gram" models \[Jelinek 1985, 1983\].
These models, whoseparmneters are calculated from a large training text, produce areasonable non-zero probability for every word in the vocabularyduring every stage of the speech recognition task.
Our modelincorporates both a Markov 3g-gram component and an added"cache" component which tracks short-term fluctuations in wordfrequency.We adopted the hypothesis that a word used in the recentpast is much more likely to be used soon than either its overallfrequency in the language or a Markov model would suggest.The cache component of our model estimates the probability of aword from its recent frequency of use.
The overall model uses aweighted average of the Markov and cache components incalculating word probabilities, where the relative weightsassigned to each component depend on the part of speech (POS).For each POS, the overall model may therefore place morereliance on the cache component than on the Markovcomponent, or vice veins; the relative weights arc obtainedempirically for each POS from a training text.
This dependanceon POS arises from the hypothesis that a content word, such asa particular noun or verb, will occur in bursts.
Function wm'ds,on the other hand, would be spread more evenly across a text ora conversation; their short-term frequencies of use would varyless dramatically from their long-term frequencies.
One of theaims of our research was to assess this hypothesis empirically.
Ifit is correct, the relative weight calculated from the training textfor the cache component for most content POSs will be higherthan the cache weighting for most flmction POSs.We intend to compare the pcrfor.mance of a standard 3g-gram Markov model with that of our model \[containing thesame Markov model along with a cache component) incalculating the probability of 100 texts, each approximately 2000words long.
The texts are taken from the Lancaster-Oats/Bergen(LOB) Corpus of modern English \[Johansson et al1988, 1982\];the rest of the corpus is utilized as a training text whichdetermines the parameters of both models.
Comparison of thetwo sets of probabilities will allow one to assess the extent ofimprovement over the pure Ma,kov model acifieved by adding a348cache component.
Furthermore, the relative weigbts calculatedfrom the training text for the two components of the combinedmodel indicate tlmse POSs for which short-term frequencies ofword use differ drastically from long-term frequencies, and thosefor which word frequencies Stay nearly constant over time.2 A Natura l  Language Model w i th  Markov  ~.ndCache ComponentsThe "trigram " Markov language model for speechrecognition developed by F. Jelinek and his colleagues uses thecontext provided by the two preceding words to estimate theprobability that the word W i occurring at time i is a givenvocabulary item W. Assume rccursivcly that at time i we havejust recognized the word sequence W 0 ."
" ,Wi_ 2 Wi__ 1.
Thetrigram model approximatss P (Wi : W \] Wo, ? "
?
, Wi_2, W~_I)by f (W~= W \[ W~_2, W~-I) "whets the frequencies f arecalculated from a huge "training text" before the recogaitiontask takes place.One adaptation of the trigram model employs trigrams ofPOSs to predict the POS of W i , and frequency of words withineach POS to predict W i itself.
Thus, this "3g-gram" model givesp(  w~=w \] wo ?
.
.
, W,_2, Wi_l) ~-P(w~=w Ig(w~)= g;)p(g(w~) =gj Ig(w~_:),g(w._0)~EGwhere we let P(WI=W I#(w~) =g~.)
=f(w,=w Ig(w~) =g~),P(g(Wi )  =gi Ig(Wl-2),  g (Wi -O)  ~-f (g(w~) =g~ Ig(W~_2), g(w~_0).Here G denotes the set of all parts of speech, gj denotes aparticular part of speech, and g (Wi) denotes the part of speechcatego~7 t6 which word W i belongs (abbreviated to gi from nowon); f denotes a frequency calculated from the training text.This "Sg-gram" model was used by Derouault and Merialdo forFrench language modeling \[Derouault and Merialdo 1986, 19841,and forms the Markov component of our own model.
In practicemany POS triplets will never appear in the training text but willappear during the recognition task, so Derouanlt and Merialdouse a weighted average of triplet and doublet POS frequenciesplus a low arbitrary constant o prevent zero estimates for theprobability of occurrence of a given POS :P(g~ =gj \]gl-2 gi-x) :~q *f (g~ =gj I g~-~,g~-~)+12*f (g~ =gi I g~-0+ 10-4.The parameters ILl 2 are not constant but can be made todepend on the count of occurrences of the sequence gi~.2,yl_lloron the POS of the preceding word, gi-1.
In either Case thgseparameters most sum to 0.9999 and can be optimized iteratively;Deronault and Meriatdo .~'ound that the two weighting methodsperformed equMly well.The 3g-gram component of our model is almost identicalto that of Derouault.and Merialdo, although the 153 POSs weuse are those of the LOB Corpus.
We let l 1 and 12 depend on thepreceding POS gi-1.
The cache component keeps track of ~herecent frequencies of words within each POS; it assigns highprobabilities to recently used words.
Now, le t  Cj (W, i )  denotethe caehc-based probability of word W at time i for POS gj ffg (W) ~ gY then Gj (W, i )  -=0 at all times i, i.e.
if W does notbelong to POS gi, its cache-based probability for that POS isalways 0.
Similarly, let My(W)  denote the Markovprobabi l i ty  due to the rest of the pure 3g-gram Mackov ~a-mdeLThis is approximated by i i (W)  ~ f (Wi~W \]g(Wi) =gj),i.e, the frequency of word W among all words with POS ~ gjin the trainin~ text.The final, combined model is then P( W i --=W) =P(.q~' Ig(Wi-~), g(W~_l)) X \[kU, 1 ?
Mi(W ) + ks, 1 ?
(w,~) \]\]\]ere k M \] "4- k~ j ~1; k M 1 denotes the weighting iven to the"frequen'~y within POS ~' component and kc, i the weighting ofthe "eaebe~based probability" component of ~OS gj.
One would~peet relatively ,insensitive" POSs, whose constituent words donot vary much in frequency over time, to have high values ofkM, j and low values of k v j ;  the reverse should be true for"sensitive" POSs.
As is 'described in the next section,approximate values k6.
J aud kMj were determined empiricallyfor two POSs gj to see if these expectations were correct.Th~e cache-bnsed probabilities C\](W,i) were calculatedas followt~.
For each POS, a "cache" (just a buffer) with.
roomfor 200 words is maintained.
Each new word is assigned to asingle POS'gj and pushed into the corresponding buffer.
Assoon as there are 5 words in a cache, it begins to outputprobabilities which correspond to the relative proportions ofwords it contains.
The lower limit of 5 on the size of the cachebefore it t~tarts producing probabilities, and the upper size limitof 200, are arbitrary; there are many possible heuristics forproducing cache-based probabilities.3 hnplementatto~a and Test ing of the CombinedModel3.1 The LOB CorpusThe Laneaster-Oslo/Bergen Corpus of British Englishconsists of 500 samples of about 2000 words each; each word intile corpus is tagged with exactly one of 153 POSs.
The sampleswere extracted from texts published in Britain in 1981, and havebeen grouped by the LOB researchers into 15 categories spanninga wilde range of English prose \[Joban~son et al1086, 1982\].
Wesplit the i;agged LOB Corpus into two unequal parts, one ofwhich aslTed as a training text for our models and the other ofwhich was used to test and compare them.
Thecomprehensiveness of the LOB Corpus made it an ideal trainingtext and a tough test of the robustness of the language model.Fnrthermore, the fact that it has been tagged by an expert eamof gramm:~rians and lexicographers freed us from having todevise onr own tagging procedure.3.2 t)arameter Calculation400 sample texts form the training text used for parametercalculation; the remaining 100 samples form a testing text usedfor testing and comparison of the pure 3g-gram model with thecombined lnodel.
Samples were allocated to the training text andthe testing text in a rammer that ensured that each had similarproportions of samples belonging to the 15 categories identifiedby the LOB researchers.
All parameters for both tile pure 3g~gram model and the combined model were calculated from the400-sample training text.The two models hare a POS prediction component wlfichis estimated by the Derouanlb-Merialdo method.
Triplet anddoublet POS frequencies were obtained from 75% (300 of the 400samples) of the training text; the remaining 25% (100 samples)gave the weights, ll(gi_l) and 12(gi_l) , needed for smoothingbetween th~se two frequencies.
These were computed iterativelyusing the Forward-Backward algorithm ( Derouault andMerialdo \[1~i88\], Rabiner and Juang \[1986\]).Now ~,he pure 3g-gram model is complete - it remains tofind kg,.i and k,jd for the combined model.
This can becalculated by means of the Forward-Backward method from the400 samples.8.3 Testing the Combined ModelAs dc.~cribed in 4.2, 80% of the LOB Corpus is used to findtile best-fit parameters for a. the pure 3g-gram model b. thecombined model, made up of the 3g-gram model plus a cachecomponent.
These two models will then be tested on theremaining ~l% of the LOB Corpus as follows.
Each is given thisportion of the LOB Corpus word by word, calculating theprobability .
f  each word as it goes along.
The probability of thissequence of ~Lbout 200,{D0 words as estimated by either model issimply the product of the~,iudividnal w0rd i probabilities asincrease achieved by the latter over the former is the measure ofthe improvemen t due to !addition of ~he Cache'component.Note that in order to calculate word probabilitir~, bothmodels must have guessed the POSs of the two preceding words.Thus every word encountered must be assigned a POS.
There arethree cases :a).
the word did not occur in the tagge d training text andtherefore is not in the vocabulary;b).
the word was in the training text, and had tim sanietag wherever it occurred;c).
the word was in the training text, and had more thanone tag (e.g.
the word "light" migbt have been tagged as a norm,verb, and adjective).The heuristics employed to assign tags were ns follows :a).
in this ease, the two previous POSs are substituted "intile Derouault-Merialdo wcighted-average formula and theprogram tries all 153 possible tags to find the one thatmaximizes the probability given by the formula.b).
in this ease, there is no choice; the tag chosen is theunique tag associated with the word in the training text.c).
when the word has two or more po~ible tags, the tagchoasn is the one which makes the largest contribution to ~heword's probability (i.e.
which gives rise to the largczt componentin the summation on pg.
1).Thus, although the portion of the LOB Corpus used fortesting is tagged, these tags were not employed in theimplementation of either model; in both eases the heuristicsgiven above guessed POSs.
A separate part of the programcmnpared actual tags with guessed ones in order to collectstatistics on the performance of these heuristics.4 Pre l iminary  Resu l t s1.
The first results of our calculations are tile values\[l(gi-1) and 12(gi_l) obtained iterativcly to optimize theweighting between the 19OS triplet frequency f (gl I gi-2,gi-1)and the POS doublet frequency f (gl \[ g/-1) in the estimation ofP(m=gj \[m-2,m-~).
A~ one might expect, l l (m-l)  tends to behigh relative to 12(gi-1) when gi-1 occurs often, because the~ triplet frequency is quite reliable in this ease.
For instance, themost frequent ag in the LOB Corpus is "NN", singular commonnoun; we have II(NN ) ~ 0.61 .
The tag "HVG", attached onlyto the word "having", is fairly rare; we have II(HVG ) =-: 0.13 .However, there are other factors to consider.
Derouanltand Merialdo state that for gi-I equal to an article, l I wasrelatively low because we need not know the POS gi-2 to predictthat gl is a noun or adjective.
Thus doublet frequencies alonewere quite reliable in this case.
On the other hand, when gi-I isa negation, knowing gl-2 was very important in making aprediction of gl, because of French phrases like "il ne veut" andUje ne veux".Our results from English texts show somewhat differentpatterns.
The tag "AT" for singulm" articles bml an l 1 that wasneither high nor low, 0.47 .
The tag "CC" for coordinatingconjunctions, including "imt", had a high l I value, 0.80 .Adjectives ("JJ") and adverbs ("RB") had 11 values even higherttmn one wouhl expect on the basis of their high frequencies ofoccurrence : 0.O0 and 0.86 respectively.2.
We collected statistics on the success rate of the pureMarker component in guessing the POS of the latest word(using the tag actually assigned the word in the LOB Corpus asthe criterion).
This rate has a powerful impact on theperformance of both models, especially the one with a cachecomponent; each incorrectly guessed POS leads to looking in thewrong cache and thus to a cache-bused probability of 0.
We areparticularly interested in forming an idea of how fast this successrate will increase as we increase the size of the training text.Of the words that had occurred at least once in thetraining text, 83.9 o~ had tags that were gue~ed correctly (ltL1o~ incorrectly).
Words that never occurred in the training textwere assigned the correct tag only 22 o~ of the time (78 %incorrect).
Apparently the informatiofi contained in the counts ofPOS triplets, doublets, and singlets is a good POS predictorwhen combined with some knowledge of the possible tags a wordmay have, but not nearly as good on its own.Among the words that appeared at least once in thetraining text, a surprisingly high proportion - 42.8 ~ - had morethan one possible POS.
Of these, 66.7 % had POSs that wereguessed correctly, Thus it might appear that performance isdegraded when the program ..must make a choice betweenpbssiblc tags.
This analYSiS is faulty i a given word might have349many POSs, and perhaps the correct one was not found in thetraining text at all.
The most important statistic , therefore, isthe proportion of words in the testing text who~e tag wasguessed correctly among the words that had also appeared withthe correct ag in the training text.
This proportion is 94.0 %.It seems reasonable to regard this as being an indication of theupper limit for the success rate of POS prediction with trainingtexts of manageable size; it provides an estimate of the successrate when the two main sources of error ( words found in thetesting text but not the training text, words found in both textswhich are tagged in the testing text with a POS not attached tothem anywhere in the training text ) are eliminated.3.
We have not yet tested the full combined model ( with acache component and a Markov component ) against the 3g-gram Marker model.
However, we have examined the effect onthe predictive power of the Marker model of including cachecomponents for two POSs : singular common oun ( label "NN"in the LOB Corpus ) and preposition ( label "IN" in the LOBCorpus ).
These two were chosen because they occur with highfrequency in the Corpus, in which tllere are 148,759 occurrencesof "NN`' and 123,440 occurrences of "IN", and because "NN`' is acontent word category and "IN" a fnnction word category.
Thusthey provide a means of testing the hypothesis outlined in theIntroduction, that a cache component will increase predictivepower for content POSs but not make much difference forfunction POSs.For both POSs, the expectation that the 200-word cachewill often contain the current word was abundantly fulfilled.
Onaverage, if the current word was an NN-word, it was stored inthe NN cache 25.8 % of the time; if it was an IN-word, it wasstored in the IN cache 64.7 % of the time.
The latter is nosurprise - there are relatively few different prepositions - but timformer figure is remarkably high, given the large nmnher ofdifferent nouns.
Note that the figure would be higher if wecounted plurals as variants of the singular word ( as we may doin future implementations ).We have not yet obtained the best-fit weighting for thecombined model.
However, we tried 3 different combinations forthe NN-words and the IN-words.
If "a" is the weight for thecache component and "b" the weight for the Marker component,tile a combinations (a, b) are (0.2, 0.8), (0.5, 0.5), and (0.9, 0.1);the pure Marker model corresponds to the weighting (0.0, 1.0).To assess the performance of each combination for NN-wordsand IN-words, we calculated i), the log product of the estimatedprobabilities for NN-words only under each of the 4 formulas ii).the log product of the estimated probabilities for IN- words onlyunder each of the 4 formulas.
It is then straightforward tocalculate the improvement per word obtained by using a cacheinstead of the pure Marker model.For N'N-words, the (0.2, 0.8) weighting yielded an averagemultiple of 2.3 in the estimated probability of a word in thetesting text over the probability as calculated by the pureMarker model ; the (0.5, 0.5) weighting yielded a multiple of 2.0per word, and the (0.0, 0.1} actually decreased the probability bya factor of 1.5 per word.For IN-words, the (0.2, 0.8) weighting gave an averagemultiple of 5.1, the (0.5, 0.5) weighting a multiple of 7.5 and the(0.9, 0.1) weighting a multiple of 6.2 .ConclusionsThe preliminmT results listed above seem to confirm ourhypothesis that reeently-uasd words have a higher probability ofoccurrence titan the 3g-gram model would predict.
Surprisingly ,if the above comparison of the POS categories "NN" and "IN" isa reliable guide, this increased probability is more dramatic inthe case of content-word categories.
Perhaps the smaller numberof different prepositions makes the cache-based probabilitiesmore reliable in this ease.Since the cost of maintaining a 200-word cache, in terms ofmemory and time, is modest, and the increase in predictivepower can be great, the approach outlined above should heconsidered as a simple way of intproving on the performance of a3g-gram language model for speech recognition.
If memory islimited, one would he wise to create caches only for POSs thatoccur with high frequency and ignore other POSs.Our immediate goal is to build caches for a larger numberof POSs, and to obtain the best-fit weighting for each of them,in order to test the full power of the combined model.Eventually, we may explore the possibility of ignoring variationsin the exact form of a word, merging the singular form of a nounwith its plural, and different tenses and persons of a verb.350This line of research as more general implications.
Theresults above seem to suggest hat at a given time, a humanbeing works with only a small fraction of his vocabulary.Perhaps if we followed an individual's written or spoken use oflanguage through the eoume of a day, it would consist largely oftime spent in language "islands" or sublanguages, with briefperiods of time during which he is in transition between islands.One might attempt to chart these "islands" by identifying roupsof words which often occur together in the language.
If thiswork is ever carried out on a large scale, it could lead topseudo-semantic language models for speech recognition, sincetbe occurrence of several words characteris$ic of an.
"island"makes the appearance of all words in that island more probable.Bibl iography1.
R. Camps, L. Fissore, A. Martelli, G. Micea, and G.Volpi, "Probabilistic Models of the Italian Language forSpeech Recognition".
Recent Advances and Applications ofSpeech Recognition (international workshop), pp.
49-56,Rome, May 1986.2.
A.M. Derouault and B.
Mdrialdo., "Natural LanguageModeling for Phoneme-to-Text Transcription", IEEETrans.
Pattern Anal.
Machine Intell., Vol.
PAMI-8, pp.742-749, No.
1986.3.
A.M. Derouault and B. Mdrialdo~ "Language Modelingat the Syntactic Level", 7th Int.
Conf.
PatternRecognition, Vol.
II, pp.
1373-1375, Montreal, Aug. 1984.4.
W.N.
Francis, "A Tagged Corpus - Problems andProspects", in Studies in English Linguistics for RandolphQuirk, S. Greenbaum, G. Leech, and J. Svartvik, Eds.London: Longman, 1980, pp.
193-209,5.
F. Jelinek, "The Development of an ExperimentalDiscrete Dictation Recognizer", Prec.
IEEE, Vol.
73,No.ll, pp 1616-1624, Nov. 1985.6.
F. Jelinek, R.L.
Mercer, and L.R.
Bahl, "A MaximumLikehood Approach to Continuous Speech Recognition",IEEE Trans.
Pattern Anal.
Machine lntell., Vol.
PAMI-5,pp.
179-90, Mar.
1983.7.
F. Jelinek, "Marker Source Modeling of TextGeneration", personM communication.8.
F. Jelinek, "Self-Organized Language Modeling forSpeech Recognition", personal communication.9.
S. Johansson, E. Atwell, R. Garside, and G. Leech, TheTagged LOB Corpus Users Manual.
Norwegian ComputingCentre for the Humanities, Bergen, 1986.10.
S. Johansson, ed, Computer Corpora in EnglishLanguage Research.
Norwegian Computing Centre for theHumanities, Bergen 1982.11.
S.E.
Levinson, L.R.
Rabiner, and M.M.
Sondhi, "AnIntroduction to the Application of Probabilistic Functionsof a Marker Process to Automatic Speech Recognition",The Bell System Technical Journal, Vol.
62, No.
4, pp.1035-1074, Apr.
1983.12.
I. Marshall, "Choice of Grammatical Word-ClassWithout Global Syntactic Analysis: Tagging Words in theLOB Corpus", Computers and the Humanities, Vol.
17,No.
3, pp.
139-150, Sept. 1983.13.
A. Martelli, "Probability Estimation of Unseen Eventsfor Language Modeling", personal communication.14.
E.M. Mucks~eial "A Natural Language Parser withStatistical Applications", IBM Research Report RC751fi(~38450), Mar.
1981.15.
A. Nadas, "Estimation of Probabilities in theLanguage Model of the IBM Speech Recognition System",IEEE Trans.
Acoust., Speech, Signal Processing, Vol.
32,pp.
859-861, Aug. 1984.
