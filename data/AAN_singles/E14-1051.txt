Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 482?490,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsWord Embeddings through Hellinger PCAR?emi LebretIdiap Research InstituteRue Marconi 19, CP 5921920 Martigny, Switzerlandremi@lebret.chRonan CollobertIdiap Research InstituteRue Marconi 19, CP 5921920 Martigny, Switzerlandronan@collobert.comAbstractWord embeddings resulting from neurallanguage models have been shown to bea great asset for a large variety of NLPtasks.
However, such architecture mightbe difficult and time-consuming to train.Instead, we propose to drastically sim-plify the word embeddings computationthrough a Hellinger PCA of the word co-occurence matrix.
We compare those newword embeddings with some well-knownembeddings on named entity recognitionand movie review tasks and show that wecan reach similar or even better perfor-mance.
Although deep learning is not re-ally necessary for generating good wordembeddings, we show that it can providean easy way to adapt embeddings to spe-cific tasks.1 IntroductionBuilding word embeddings has always generatedmuch interest for linguists.
Popular approachessuch as Brown clustering algorithm (Brown et al.,1992) have been used with success in a wide vari-ety of NLP tasks (Sch?utze, 1995; Koo et al., 2008;Ratinov and Roth, 2009).
Those word embed-dings are often seen as a low dimensional-vectorspace where the dimensions are features poten-tially describing syntactic or semantic properties.Recently, distributed approaches based on neuralnetwork language models (NNLM) have revivedthe field of learning word embeddings (Collobertand Weston, 2008; Huang and Yates, 2009; Turianet al., 2010; Collobert et al., 2011).
However, aneural network architecture can be hard to train.Finding the right parameters to tune the model isoften a challenging task and the training phase isin general computationally expensive.This paper aims to show that such good wordembeddings can be obtained using simple (mostlylinear) operations.
We show that similar wordembeddings can be computed using the word co-occurrence statistics and a well-known dimension-ality reduction operation such as Principal Com-ponent Analysis (PCA).
We then compare our em-beddings with the CW (Collobert and Weston,2008), Turian (Turian et al., 2010), HLBL (Mnihand Hinton, 2008) embeddings, which come fromdeep architectures and the LR-MVL (Dhillon etal., 2011) embeddings, which also come from aspectral method on several NLP tasks.We claim that, assuming an appropriate met-ric, a simple spectral method as PCA can generateword embeddings as good as with deep-learningarchitectures.
On the other hand, deep-learningarchitectures have shown their potential in sev-eral supervised NLP tasks, by using these wordembeddings.
As they are usually generated overlarge corpora of unlabeled data, words are repre-sented in a generic manner.
Having generic em-beddings, good performance can be achieved onNLP tasks where the syntactic aspect is domi-nant such as Part-Of-Speech, chunking and NER(Turian et al., 2010; Collobert et al., 2011; Dhillonet al., 2011).
For supervised tasks relying moreon the semantic aspect as sentiment classification,it is usually helpful to adapt the existing embed-dings to improve performance (Labutov and Lip-son, 2013).
We show in this paper that such em-bedding specialization can be easily done via neu-ral network architectures and that helps to increasegeneral performance.2 Related WorkAs 80% of the meaning of English text comesfrom word choice and the remaining 20% comesfrom word order (Landauer, 2002), it seems quiteimportant to leverage word order to capture all thesemantic information.
Connectionist approacheshave therefore been proposed to develop dis-tributed representations which encode the struc-482tural relationships between words (Hinton, 1986;Pollack, 1990; Elman, 1991).
More recently, aneural network language model was proposed inBengio et al.
(2003) where word vector representa-tions are simultaneously learned along with a sta-tistical language model.
This architecture inspiredother authors: Collobert and Weston (2008) de-signed a neural language model which eliminatesthe linear dependency on vocabulary size, Mnihand Hinton (2008) proposed a hierarchical linearneural model, Mikolov et al.
(2010) investigateda recurrent neural network architecture for lan-guage modeling.
Such architectures being trainedover large corpora of unlabeled text with the aimto predict correct scores end up learning the co-occurence statistics.Linguists assumed long ago that words occur-ring in similar contexts tend to have similar mean-ings (Wittgenstein, 1953).
Using the word co-occurrence statistics is thus a natural choice to em-bed similar words into a common vector space(Turney and Pantel, 2010).
Common approachescalculate the frequencies, apply some transforma-tions (tf-idf, PPMI), reduce the dimensionality andcalculate the similarities (Lowe, 2001).
Consid-ering a fixed-sized word vocabulary D and a setof wordsW to embed, the co-occurence matrix Cis of size |W|?|D|.
C is then vocabulary size-dependent.
One can apply a dimensionality reduc-tion operation to C leading to?C ?
R|W|?d, whered  |D|.
Dimensionality reduction techniquessuch as Singular Valued Decomposition (SVD)are widely used (e.g.
LSA (Landauer and Du-mais, 1997), ICA (V?ayrynen and Honkela, 2004)).However, word co-occurence statistics are dis-crete distributions.
An information theory mea-sure such as the Hellinger distance seems to bemore appropriate than the Euclidean distance overa discrete distribution space.
In this paper wewill compare the Hellinger PCA against the clas-sical Euclidean PCA and the Low Rank Multi-View Learning (LR-MVL) method, which is an-other spectral method based on Canonical Corre-lation Analysis (CCA) to learn word embeddings(Dhillon et al., 2011).It has been shown that using word embeddingsas features helps to improve general performanceon many NLP tasks (Turian et al., 2010).
How-ever these embeddings can be too generic to per-form well on other tasks such as sentiment clas-sification.
For such task, word embeddings mustcapture the sentiment information.
Maas et al.
(2011) proposed a model for jointly capturing se-mantic and sentiment components of words intovector spaces.
More recently, Labutov and Lip-son (2013) presented a method which takes exist-ing embeddings and, by using some labeled data,re-embed them in the same space.
They showedthat these new embeddings can be better predic-tors in a supervised task.
In this paper, we con-sider word embedding-based linear and non-linearmodels for two NLP supervised tasks: Named En-tity Recognition and IMDB movie review.
We an-alyze the effect of fine-tuning existing embeddingsover each task of interest.3 Spectral Method for WordEmbeddingsA NNLM learns which words among the vocab-ulary are likely to appear after a given sequenceof words.
More formally, it learns the next wordprobability distribution.
Instead, simply countingwords on a large corpus of unlabeled text can beperformed to retrieve those word distributions andto represent words (Turney and Pantel, 2010).3.1 Word co-occurence statistics?You shall know a word by the company it keeps?
(Firth, 1957).
It is a natural choice to use the wordco-occurence statistics to acquire representationsof word meanings.
Raw word co-occurence fre-quencies are computed by counting the number oftimes each context word w ?
D occurs after a se-quence of words T :p(w|T ) =p(w, T )p(T )=n(w, T )?wn(w, T ), (1)where n(w, T ) is the number of times each contextword w occurs after the sequence T .
The size ofT can go from 1 to t words.
The next word prob-ability distribution p for each word or sequence ofwords is thus obtained.
It is a multinomial dis-tribution of |D| classes (words).
A co-occurencematrix of size N ?
|D| is finally built by com-puting those frequencies over all the N possiblesequences of words.3.2 Hellinger distanceSimilarities between words can be derived bycomputing a distance between their correspond-ing word distributions.
Several distances (or met-rics) over discrete distributions exist, such as the483Bhattacharyya distance, the Hellinger distance orKullback-Leibler divergence.
We chose here theHellinger distance for its simplicity and symme-try property (as it is a true distance).
Consid-ering two discrete probability distributions P =(p1, .
.
.
, pk) and Q = (q1, .
.
.
, qk), the Hellingerdistance is formally defined as:H(P,Q) = ?1?2????k?i=1(?pi?
?qi)2, (2)which is directly related to the Euclidean norm ofthe difference of the square root vectors:H(P,Q) =1?2?
?P ??Q?2.
(3)Note that it makes more sense to take the Hellingerdistance rather than the Euclidean distance forcomparing discrete distributions, as P and Q areunit vectors according to the Hellinger distance(?P and?Q are units vector according to the `2norm).3.3 Dimensionality ReductionAs discrete distributions are vocabulary size-dependent, using directly the distribution as aword embedding is not really tractable for largevocabulary.
We propose to perform a princi-pal component analysis (PCA) of the word co-occurence probability matrix to represent wordsin a lower dimensional space while minimizingthe reconstruction error according to the Hellingerdistance.4 Architectures for NLP tasksTraditional NLP approaches extract from docu-ments a rich set of hand-designed features whichare then fed to a standard classification algorithm.The choice of features is a task-specific empiricalprocess.
In contrast, we want to pre-process ourfeatures as little as possible.
In that respect, a mul-tilayer neural network architecture seems appro-priate as it can be trained in an end-to-end fashionon the task of interest.4.1 Sentence-level ApproachThe sentence-level approach aims at tagging witha label each word in a given sentence.
Embed-dings of each word in a sentence are fed to linearand non-linear classification models followed by aCRF-type sentence tag inference.
We chose hereneural networks as classifiers.Sliding window Context is crucial to character-ize word meanings.
We thus consider n contextwords around each word xtto be tagged, lead-ing to a window of N = (2n + 1) words [x]t=(xt?n, .
.
.
, xt, .
.
.
, xt+n).
As each word is em-bedded into a dwrd-dimensional vector, it resultsa dwrd?
N vector representing a window of Nwords, which aims at characterizing the middleword xtin this window.
Given a complete sen-tence of T words, we can obtain for each word acontext-dependent representation by sliding overall the possible windows in the sentence.
A samelinear transformation is then applied on each win-dow for each word to tag:g([x]t) = W [x]t+ b , (4)where W ?
RM?dwrdNand b ?
RMare the pa-rameters, with M the number of classes.
Alterna-tively, a one hidden layer non-linear network canbe considered:g([x]t) = Wh(U [x]t) + b , (5)where U ?
Rnhu?dwrdN, with nhuthe number ofhidden units and h(.)
a transfer function.CRF-type inference There exists strong depen-dencies between tags in a sentence: some tagscannot follow other tags.
To take the sentencestructure into account, we want to encourage validpaths of tags during training, while discourag-ing all other paths.
Considering the matrix ofscores outputs by the network, we train a sim-ple conditional random field (CRF).
At inferencetime, given a sentence to tag, the best path whichminimizes the sentence score is inferred with theViterbi algorithm.
More formally, we denote ?all the trainable parameters of the network andf?
([x]T1) the matrix of scores.
The element [f?
]i,tof the matrix is the score output by the network forthe sentence [x]T1and the ithtag, at the tthword.We introduce a transition score [A]i,jfor jumpingfrom i to j tags in successive words, and an initialscore [A]i,0for starting from the ithtag.
As thetransition scores are going to be trained, we define??
= ??
{[A]i,j?i, j}.
The score of a sentence [x]T1along a path of tags [i]T1is then given by the sumof transition scores and networks scores:s([x]T1, [i]T1,??)
=T?t=1(A[i]t?1,[i]t+ [f?
][i]t,t) .
(6)484We normalize this score over all possible tag paths[j]T1using a softmax, and we interpret the resultingratio as a conditional tag path probability.
Takingthe log, the conditional probability of the true path[y]T1is therefore given by:log p([y]T1, [x]T1,??)
= s([x]T1, [y]T1,??)?
logadd?
[j]T1s([x]T1, [j]T1,??)
,(7)where we adopt the notationlogaddizi= log (?iezi) .
(8)Computing the log-likelihood efficiently is notstraightforward, as the number of terms in thelogadd grows exponentially with the length ofthe sentence.
It can be computed in linear timewith the Forward algorithm, which derives a recur-sion similar to the Viterbi algorithm (see Rabiner(1989)).
We can thus maximize the log-likelihoodover all the training pairs ([x]T1, [y]T1) to find, givena sentence [x]T1, the best tag path which minimizesthe sentence score (6):argmax[j]T1s([x]T1, [j]T1,??)
.
(9)In contrast to classical CRF, all parameters ?
aretrained in a end-to-end manner, by backpropa-gation through the Forward recursion, followingCollobert et al.
(2011).4.2 Document-level ApproachThe document-level approach is a document bi-nary classifier, with classes y ?
{?1, 1}.
For eachdocument, a set of (trained) filters is applied tothe sliding window described in section 4.1.
Themaximum value obtained by the ithfilter over thewhole document is:maxt[wi[x]t+ bi]i,t1 ?
i ?
nfilter.
(10)It can be seen as a way to measure if the infor-mation represented by the filter has been capturedin the document or not.
We feed all these inter-mediate scores to a linear classifier, leading to thefollowing simple model:f?
(x) = ?maxt[W [x]t+ b].
(11)In the case of movie reviews, the ithfilter mightcapture positive or negative sentiment dependingon the sign of ?i.
As in section 4.1, we will alsoconsider a non-linear classifier in the experiments.Training The neural network is trained usingstochastic gradient ascent.
We denote ?
all thetrainable parameters of the network.
Using a train-ing set T , we minimize the following soft marginloss function with respect to ?:?
??
(x,y)?Tlog(1 + e?yf?(x)).
(12)4.3 Embedding Fine-TuningAs seen in section 3, the process to computegeneric word embedding is quite straightforward.These embeddings can then be used as featuresfor supervised NLP systems and help to improvethe general performance (Turian et al., 2010; Col-lobert et al., 2011; Chen et al., 2013).
However,most of these systems cannot tune these embed-dings as they are not structurally able to.
By lever-aging the deep architecture of our system, we candefine a lookup-table layer initialized with exist-ing embeddings as the first layer of the network.Lookup-Table Layer We consider a fixed-sizedword dictionary D. Given a sequence of N wordsw1, w2, .
.
.
, wN, each word wn?
W is first em-bedded into a dwrd-dimensional vector space, byapplying a lookup-table operation:LTW(wn) =W(0, .
.
.
, 1 , .
.
.
, 0at index wn)= ?W ?wn,(13)where the matrix W ?
Rdwrd?|D|representsthe embeddings to be tuned in this lookup layer.
?W ?wn?
Rdwrdis the wthcolumn ofW and dwrdis the word vector size.
Given any sequence of Nwords [w]N1in D, the lookup table layer appliesthe same operation for each word in the sequence,producing the following output matrix:LTW([w]N1) =(?W ?1[w]1. .
.
?W ?1[w]N).
(14)Training Given a task of interest, a relevant rep-resentation of each word is then given by the cor-responding lookup table feature vector, which istrained by backpropagation.
Word representationsare initialized with existing embeddings.4855 Experimental SetupWe evaluate the quality of our embeddings ob-tained on a large corpora of unlabeled text by com-paring their performance against the CW (Col-lobert and Weston, 2008), Turian (Turian et al.,2010), HLBL (Mnih and Hinton, 2008), and LR-MVL (Dhillon et al., 2011) embeddings on NERand movie review tasks.
We also show that thegeneral performance can be improved for thesetasks by fine-tuning the word embeddings.5.1 Building Word Representation overLarge CorporaOur English corpus is composed of the entire En-glish Wikipedia1(where all MediaWiki markupshave been removed), the Reuters corpus and theWall Street Journal (WSJ) corpus.
We considerlower case words to limit the number of wordsin the vocabulary.
Additionally, all occurrencesof sequences of numbers within a word are re-placed with the string ?NUMBER?.
The result-ing text was tokenized using the Stanford tok-enizer2.
The data set contains about 1,652 millionwords.
As vocabulary, we considered all the wordswithin our corpus which appear at least one hun-dred times.
This results in a 178,080 words vocab-ulary.
To build the co-occurence matrix, we usedonly the 10,000 most frequent words within ourvocabulary as context words.
To get embeddingsfor words, we needed to only consider sequencesT of t = 1 word.
After PCA, each word canbe represented in any n-dimensional vector (withn ?
{1, .
.
.
, 10000}).
We chose to embed wordsin a 50-dimensional vector, which is the commondimension among the other embeddings in the lit-erature.
The resulting embeddings will be referredas H-PCA in the following sections.
To highlightthe importance of the Hellinger distance, we alsocomputed the PCA of the co-occurence probabil-ity matrix with respect to the Euclidean metric.The resulting embeddings are denoted E-PCA.Computational cost The Hellinger PCA is veryfast to compute.
We report in Table 1 the timeneeded to compute the embeddings describedabove.
For this benchmark we used Intel i7 3770K3.5GHz CPUs.
As the computation of the covari-ance matrix is highly parallelizable, we report re-sults with 1, 100 and 500 CPUs.
The Eigende-1Available at http://download.wikimedia.org.
We took theMay 2012 version.2Available at http://nlp.stanford.edu/software/tokenizer.shtmlcomposition of the C matrix has been computedwith the SSYEVR LAPACK subroutine on oneCPU.
We compare completion times for 1,000 and10,000 eigenvectors.
Finally, we report comple-tion times to generate the emdeddings by linearprojection using 50, 100 and 200 eigenvectors.
Al-though the linear projection is already quite faston only one CPU, this operation can also be com-puted in parallel.
Those results show that theHellinger PCA can generate about 200,000 em-beddings in about three minutes with a cluster of100 CPUs.time (s)# of CPUs 1 100 500Covariance matrix 9930 99 201,000 Eigenvectors 72 - -10,000 Eigenvectors 110 - -50D Embeddings 20 0.2 0.04100D Embeddings 29 0.29 0.058200D Embeddings 67 0.67 0.134Total for 50D 10,022 171.2 92.04Table 1: Benchmark of the experiment.
Times arereported in seconds.5.2 Existing Available Word EmbeddingsWe compare our H-PCA?s embeddings with thefollowing publicly available embeddings:?
LR-MVL3: it covers 300,000 words with 50dimensions for each word.
They were trainedon the RCV1 corpus using the Low RankMulti-View Learning method.
We only usedtheir context oblivious embeddings comingfrom the eigenfeature dictionary.?
CW4: it covers 130,000 words with 50 di-mensions for each word.
They were trainedfor about two months, over Wikipedia, usinga neural network language model approach.?
Turian5: it covers 268,810 words with 25,50, 100 or 200 dimensions for each word.They were trained on the RCV1 corpus us-ing the same system as the CW embeddingsbut with different parameters.
We used onlythe 50 dimensions.3Available at http://www.cis.upenn.edu/ un-gar/eigenwords/4From SENNA: http://ml.nec-labs.com/senna/5Available at http://metaoptimize.com/projects/wordreprs/486?
HLBL5: it covers 246,122 words with 50 or100 dimensions for each word.
They weretrained on the RCV1 corpus using a Hierar-chical Log-Bilinear Model.
We used only the50 dimensions.5.3 Supervised Evaluation TasksUsing word embeddings as feature proved that itcan improve the generalization performance onseveral NLP tasks (Turian et al., 2010; Collobertet al., 2011; Chen et al., 2013).
Using our wordembeddings, we thus trained the sentence-level ar-chitecture described in section 4.1 on a NER task.Named Entity Recognition (NER) It labelsatomic elements in the sentence into categoriessuch as ?PERSON?
or ?LOCATION?.
TheCoNLL 2003 setup6is a NER benchmark dataset based on Reuters data.
The contest providestraining, validation and testing sets.
The networksare fed with two raw features: word embeddingsand a capital letter feature.
The ?caps?
featuretells if each word was in lowercase, was all up-percase, had first letter capital, or had at leastone non-initial capital letter.
No other feature hasbeen used to tune the models.
This is a maindifference with other systems which usually usemore features as POS tags, prefixes and suffixesor gazetteers.
Hyper-parameters were tuned onthe validation set.
We selected n = 2 contextwords leading to a window of 5 words.
We used aspecial ?PADDING?
word for context at the be-ginning and the end of each sentence.
For thenon-linear model, the number of hidden units was300.
As benchmark system, we report the systemof Ando et al.
(2005), which reached 89.31% F1with a semi-supervised approach and less special-ized features than CoNLL 2003 challengers.The NER evaluation task is mainly syntactic.As we wish to evaluate whether our word embed-dings can also capture semantic, we trained thedocument-level architecture described in section4.2 over a movie review task.IMDB Review Dataset We used a collection of50,000 reviews from IMDB7.
It allows no morethan 30 reviews per movie.
It contains an evennumber of positive and negative reviews, so ran-domly guessing yields 50% accuracy.
Only highlypolarized reviews have been considered.
A nega-6http://www.cnts.ua.ac.be/conll2003/ner/7Available at http://www.andrew-maas.net/data/sentimenttive review has a score ?
4 out of 10, and a posi-tive review has a score ?
7 out of 10.
It has beenevenly divided into training and test sets (25,000reviews each).
For this task, we only used theword embeddings as features.
We perform a sim-ple cross-validation on the training set to choosethe optimal hyper-parameters.
The network had awindow of 5 words and nfilter= 1000 filters.
Asbenchmark system, we report the system of Maaset al.
(2011), which reached 88.90% accuracy witha mix of unsupervised and supervised techniquesto learn word vectors capturing semantic term-document information, as well as rich sentimentcontent.87.58888.58989.59090.59191.59292.5930.001  0.01  0.1  1F1scorelambdaLRMVLTurianCWH-PCAHLBL(a) NER validation set.8384858687888990910.001  0.01  0.1  1accuracylambdaLRMVLTurianCWH-PCAHLBL(b) IMDB review dataset.Figure 1: Effect of varying the normalization fac-tor ?
with a non-linear approach and fine-tuning.5.4 Embeddings NormalizationWord embeddings are continuous vector spacesthat are not necessarily in a bounded range.
Toavoid saturation issues in the network architec-tures, embeddings need to be properly normalized.Considering the matrix of word embeddings E,the normalized embeddings are:?E =?
(E ??E)?
(E)(15)487where?E is the mean of the embeddings, ?
(E) isthe standard deviation of the embeddings and ?
isa normalization factor.
Figure 1 shows the effectof ?
on both supervised tasks.
The embeddingsnormalization depends on the type of the networkarchitecture.
In the document-level approach, bestresults are obtained with ?
= 0.1 for all embed-dings, while a normalization factor set to 1 is bet-ter for H-PCA?s embeddings in the sentence-levelapproach.
These results show the importance ofapplying the right normalization for word embed-dings.5.5 ResultsH-PCA?s embeddings Results summarized inTable 2 reveal that performance on NER task canbe as good with word embeddings from a word co-occurence matrix decomposition as with a neuralnetwork language model trained for weeks.
Thebest F1 scores are indeed obtained using the H-PCA tuned embeddings.
Results for the movie re-view task in Table 3 show that H-PCA?s embed-dings also perform as well as all the other embed-dings on the movie review task.
It is worth men-tioning that on both tasks, H-PCA?s embeddingsoutperform the E-PCA?s embeddings, demonstrat-ing the value of the Hellinger distance.
When theembeddings are not tuned, the CW?s embeddingsslightly outperform the H-PCA?s embeddings onNER task.
The performance difference betweenboth fixed embeddings on the movie review task isabout 3%.
Embeddings from the CW neural lan-guage model seems to capture more semantic in-formation but we showed that this lack of semanticinformation can be offset by fine-tuning.Embeddings fine-tuning We note that tuningthe embeddings by backpropagation increases thegeneral performance on both NER and movie re-view tasks.
The increase is, in general, higher forthe movie review task, which reveals the impor-tance of embedding fine-tuning for NLP tasks witha high semantic component.
We show in Table 4that the embeddings after fine-tuning give a higherrank to words that are related to the task of interestwhich is movie-sentiment-based relations in thiscase.Linear vs nonlinear model We also report re-sults with a linear version of our neural networks.Having non-linearity helps for NER.
It seems im-portant to extract non-linear features for such atask.
However, we note that the linear approachApproach Fixed TunedBenchmark 89.31Non-Linear ApproachH-PCA 87.91 ?
0.17 89.16 ?
0.09E-PCA 84.28 ?
0.15 87.09 ?
0.12LR-MVL 86.83 ?
0.20 87.38 ?
0.07CW 88.14 ?
0.21 88.69 ?
0.16Turian 86.26 ?
0.13 87.35 ?
0.12HLBL 83.87 ?
0.25 85.91 ?
0.17Linear ApproachH-PCA 84.64 ?
0.11 87.97 ?
0.09E-PCA 78.15 ?
0.15 85.99 ?
0.09LR-MVL 82.27 ?
0.14 86.83 ?
0.17CW 84.50 ?
0.19 86.84 ?
0.08Turian 83.33 ?
0.07 86.79 ?
0.11HLBL 80.31?
0.11 85.06 ?
0.13Table 2: Performance comparison on NER taskwith different embeddings.
The first column isresults with the original embeddings.
The sec-ond column is results with embeddings after fine-tuning for this task.
Results are reported in F1score (mean ?
standard deviation of ten trainingruns with different initialization).Approach Fixed TunedBenchmark 88.90Non-Linear ApproachH-PCA 84.20 ?
0.16 89.89 ?
0.09E-PCA 74.85 ?
0.12 89.70 ?
0.06LR-MVL 85.33 ?
0.14 90.06 ?
0.09CW 87.54 ?
0.27 89.77 ?
0.05Turian 85.33 ?
0.10 89.99 ?
0.05HLBL 85.51 ?
0.14 89.58 ?
0.06Linear ApproachH-PCA 84.11 ?
0.05 89.90 ?
0.10E-PCA 73.27 ?
0.16 89.62 ?
0.05LR-MVL 84.37 ?
0.16 89.77 ?
0.09CW 87.62 ?
0.24 89.92 ?
0.07Turian 84.44 ?
0.13 89.66 ?
0.10HLBL 85.34 ?
0.10 89.64 ?
0.05Table 3: Performance comparison on movie re-view task with different embeddings.
The firstcolumn is results with the original embeddings.The second column is results with embeddings af-ter fine-tuning for this task.
Results are reportedin classification accuracy (mean ?
standard devi-ation of ten training runs with different initializa-tion).488BORING BAD AWESOMEbefore after before after before afterSAD CRAP HORRIBLE TERRIBLE SPOOKY TERRIFICSILLY LAME TERRIBLE STUPID AWFUL TIMELESSSUBLIME MESS DREADFUL BORING SILLY FANTASTICFANCY STUPID UNFORTUNATE DULL SUMMERTIME LOVELYSOBER DULL AMAZING CRAP NASTY FLAWLESSTRASH HORRIBLE AWFUL WRONG MACABRE MARVELOUSLOUD RUBBISH MARVELOUS TRASH CRAZY EERIERIDICULOUS SHAME WONDERFUL SHAME ROTTEN LIVELYRUDE AWFUL GOOD KINDA OUTRAGEOUS FANTASYMAGIC ANNOYING FANTASTIC JOKE SCARY SURREALTable 4: Set of words with their 10 nearest neighbors before and after fine-tuning for the movie reviewtask (using the Euclidean metric in the embedding space).
H-PCA?s embeddings are used here.performs as well as the non-linear approach forthe movie review task.
Our linear approach cap-tures all the necessary sentiment features to pre-dict whether a review is positive or negative.
Itis thus not surprising that a bag-of-words basedmethod can perform well on this task (Wang andManning, 2012).
However, as our method takesthe whole review as input, we can extract windowsof words having the most discriminative power:it is a major advantage of our method comparedto conventional bag-of-words based methods.
Wereport in Table 5 some examples of windows ofwords extracted from the most discriminative fil-ters ?i(positive and negative).
Note that there isabout the same number of positive and negativefilters after learning.6 ConclusionWe have demonstrated that appealing wordembeddings can be obtained by computing aHellinger PCA of the word co-occurence ma-trix.
While a neural network language modelcan be painful and long to train, we can geta word co-occurence matrix by simply countingwords over a large corpus.
The resulting em-beddings give similar results on NLP tasks, evenfrom a N ?
10, 000 word co-occurence matrixcomputed with only one word of context.
It re-veals that having a significant, but not too largeset of common words, seems sufficient for cap-turing most of the syntactic and semantic char-acteristics of words.
As PCA of a N ?
10, 000matrix is really fast and not memory consuming,our method gives an interesting and practical al-ternative to neural language models for generat-?i[x]t-the worst film this yearvery worst film i ?vevery worst movie i ?ve-watch this unfunny stinker ., extremely unfunny drivel come, this ludicrous script gets-it was pointless and boringit is unfunny .
unfunnyfilm are awful and embarrassing+both really just wonderful .. a truly excellent film.
a really great film+excellent film with great performancesexcellent film with a greatexcellent movie with a stellar+incredible .
just incredible .performances and just amazing .one was really great .Table 5: The top 3 positive and negative filters?iwiand their respective top 3 windows of words[x]twithin the whole IMDB review dataset.ing word embeddings.
However, we showed thatdeep-learning is an interesting framework to fine-tune embeddings over specific NLP tasks.
OurH-PCA?s embeddings are available online, here:http://www.lebret.ch/words/.AcknowledgmentsThis work was supported by the HASLER foun-dation through the grant ?Information and Com-munication Technology for a Better World 2020?(SmartWorld).489ReferencesR.
K. Ando, T. Zhang, and P. Bartlett.
2005.
A frame-work for learning predictive structures from multi-ple tasks and unlabeled data.
Journal of MachineLearning Research, 6:1817?1853.Y.
Bengio, R. Ducharme, P. Vincent, and C. Janvin.2003.
A neural probabilistic language model.
J.Mach.
Learn.
Res., 3:1137?1155, March.P.
F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra,and J C. Lai.
1992.
Class-based n-gram mod-els of natural language.
Computational Linguistics,18(4):467?479.Y.
Chen, B. Perozzi, R.
Al-Rfou?, and S. Skiena.
2013.The expressive power of word embeddings.
CoRR,abs/1301.3226.R.
Collobert and J. Weston.
2008.
A unified architec-ture for natural language processing: Deep neuralnetworks with multitask learning.
In InternationalConference on Machine Learning, ICML.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2493?2537.P.
S. Dhillon, D. Foster, and L. Ungar.
2011.
Multi-view learning of word embeddings via CCA.
InAdvances in Neural Information Processing Systems(NIPS), volume 24.J.
L. Elman.
1991.
Distributed representations, sim-ple recurrent networks, and grammatical structure.Machine Learning, 7:195?225.J.
R. Firth.
1957.
A synopsis of linguistic theory 1930-55.
1952-59:1?32.G.
E. Hinton.
1986.
Learning distributed representa-tions of concepts.
In Proceedings of the Eighth An-nual Conference of the Cognitive Science Society,pages 1?12.
Hillsdale, NJ: Erlbaum.F.
Huang and A. Yates.
2009.
Distributional represen-tations for handling sparsity in supervised sequence-labeling.
In Proceedings of the Association forComputational Linguistics (ACL), pages 495?503.Association for Computational Linguistics.T.
Koo, X. Carreras, and M. Collins.
2008.
Simplesemi-supervised dependency parsing.
In Proceed-ings of the Association for Computational Linguis-tics (ACL), pages 595?603.I.
Labutov and H. Lipson.
2013.
Re-embedding words.In ACL.T.
K. Landauer and S. T. Dumais.
1997.
A solution toPlato?s problem: The Latent Semantic Analysis the-ory of the acquisition, induction, and representationof knowledge.
Psychological Review.T.
K. Landauer.
2002.
On the computational basisof learning and cognition: Arguments from lsa.
InN.
Ross, editor, The psychology of learning and mo-tivation, volume 41, pages 43?84.
Academic Press,San Francisco, CA.W.
Lowe, 2001.
Towards a theory of semantic space,pages 576?581.A.
L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y.Ng, and C. Potts.
2011.
Learning word vectors forsentiment analysis.
In ACL, pages 142?150.T.
Mikolov, M. Karafiat, L. Burget, J. Cernocky, andSanjeev Khudanpur.
2010.
Recurrent neural net-work based language model.A.
Mnih and G. Hinton.
2008.
A Scalable HierarchicalDistributed Language Model.
In Advances in NeuralInformation Processing Systems, volume 21.J.
B. Pollack.
1990.
Recursive distributed representa-tions.
Artificial Intelligence, 46:77?105.L.
R. Rabiner.
1989.
A tutorial on hidden markovmodels and selected applications in speech recogni-tion.
In Proceedings of the IEEE, pages 257?286.L.
Ratinov and D. Roth.
2009.
Design challenges andmisconceptions in named entity recognition.
In Pro-ceedings of the Thirteenth Conference on Computa-tional Natural Language Learning (CoNLL), pages147?155.
Association for Computational Linguis-tics.H.
Sch?utze.
1995.
Distributional part-of-speech tag-ging.
In Proceedings of the Association for Compu-tational Linguistics (ACL), pages 141?148.
MorganKaufmann Publishers Inc.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: A simple and general method for semi-supervised learning.
In ACL.P.
D. Turney and P. Pantel.
2010.
From frequency tomeaning: Vector space models of semantics.
J. Artif.Int.
Res., 37(1):141?188, January.J.
J. V?ayrynen and T. Honkela.
2004.
Word categorymaps based on emergent features created by ICA.In Proceedings of the STeP?2004 Cognition + Cy-bernetics Symposium.S Wang and C. D. Manning.
2012.
Baselines and bi-grams: Simple, good sentiment and topic classifica-tion.
ACL ?12.L.
Wittgenstein.
1953.
Philosophical Investigations.Blackwell.490
