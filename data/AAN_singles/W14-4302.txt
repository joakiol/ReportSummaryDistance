Proceedings of the SIGDIAL 2014 Conference, pages 2?11,Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational LinguisticsCrowdsourcing Street-level Geographic Information Using aSpoken Dialogue SystemRaveesh Meena Johan Boye Gabriel Skantze Joakim GustafsonKTH Royal Institute of TechnologySchool of Computer Science and CommunicationStockholm, Sweden{raveesh, jboye}@csc.kth.se, {gabriel, jocke}@speech.kth.seAbstractWe present a technique for crowd-sourcing street-level geographic infor-mation using spoken natural language.
Inparticular, we are interested in obtainingfirst-person-view information about whatcan be seen from different positions inthe city.
This information can then forexample be used for pedestrian routingservices.
The approach has been tested inthe lab using a fully implemented spokendialogue system, and has shown promis-ing results.1 IntroductionCrowdsourcing is increasingly being used inspeech processing for tasks such as speech dataacquisition, transcription/labeling, and assess-ment of speech technology, e.g.
spoken dialoguesystems (Parent & Eskenazi, 2011).
However,we are not aware of any attempts where a dia-logue system is the vehicle for crowdsourcingrather than the object of study, that is, where aspoken dialogue system is used to collect infor-mation from a large body of users.
A task wheresuch crowdsourcing dialogue systems would beuseful is to populate geographic databases.
Whilethere are now open databases with geographicinformation, such as OpenStreetMap (Haklay &Weber, 2008), these are typically intended formap drawing, and therefore lack detailed street-level information about city landmarks, such ascolors and height of buildings, ornamentations,facade materials, balconies, conspicuous signs,etc.
Such information could for example be veryuseful for pedestrian navigation (Tom & Denis,2003; Ross et al., 2004).
With the current grow-ing usage of smartphones, we might envisage acommunity of users using their phones to con-tribute information to geographic databases, an-notating cities to a great level of detail, usingmulti-modal method including speech.
The keyreason for using speech for map annotation isconvenience; it is easy to talk into a mobilephone while walking down the street, so a userwith a little experience will not be slowed downby the activity of interacting with a database.This way, useful information could be obtainedthat is really hard to add offline, sitting in frontof one?s PC using a map interface, things like:Can you see X from this point?
Is there a bigsign over the entrance of the restaurant?
Whatcolor is the building on your right?Another advantage of using a spoken dialoguesystem is that the users could be asked to freelydescribe objects they consider important in theircurrent view.
In this way, the system could learnnew objects not anticipated by the system de-signers, and their associated properties.In this paper we present a proof-of-conceptstudy of how a spoken dialogue system could beused to enrich geographic databases bycrowdsourcing.
To our knowledge, this is thefirst attempt at using spoken dialogue systemsfor crowdsourcing in this way.
In Section 2, weelaborate on the need of spoken dialogue systemsfor crowdsourcing geographic information.
InSection 3 we describe the dialogue system im-plementation.
Section 4 presents our in-labcrowdsourcing experiment.
We present an analy-sis of crowd-sourced data in Section 5, and dis-cuss directions for future work in Section 6.2 The pedestrian routing domainRouting systems have been around quite sometime for car navigation, but systems for pedestri-2an routing are relatively new and are still in theirnascent stage (Bartie & Mackaness, 2006; Kruget al., 2003; Janarthanam et al., 2012; Boye et al.,2014).
In the case of pedestrian navigation, it ispreferable for way-finding systems to base theirinstructions on landmarks, by which we under-stand distinctive objects in the city environment.Studies have shown that the inclusion of land-marks into system-generated instructions for apedestrian raises the user?s confidence in the sys-tem, compared to only left-right instructions(Tom & Denis, 2003; Ross et al., 2004).Basing routing instructions on landmarksmeans that the routing system would, for exam-ple, generate an instruction ?Go towards the redbrick building?
(where, in this case, ?the redbrick building?
is the landmark), rather than?Turn slightly left here?
or ?Go north 200 me-ters?.
This strategy for providing instructionsplaces certain requirements on the geographicdatabase: It has to include many landmarks andmany details about them as well, so that the sys-tem can generate clear and un-ambiguous in-structions.
However, the information containedin current databases is still both sparse andcoarse-grained in many cases.Our starting point is a pedestrian routing sys-tem we designed and implemented, using thelandmark-based approach to instruction-giving(Boye et al., 2014).
The system performs visibil-ity calculations whenever the pedestrian ap-proaches a waypoint, in order to compute the setof landmarks that are visible for the user from hiscurrent position.
OpenStreetMap (Haklay & We-ber, 2008) is used as the data source.
Figure 1shows a typical situation in pedestrian routingsession.
The blue dot indicates the user?s positionand the blue arrow her direction.
Figure 2 showsthe same situation in a first-person perspective.The system can now compute the set of visiblelandmarks, such as buildings and traffic lights,along with distances and angles to those land-marks.
The angle to a building is given as an in-terval in degrees relative to the direction of theuser (e.g.
90?
left to 30?
left).
This is exemplifiedin Figure 1, where four different buildings are inview (with field of view marked with numbers1?4).
Landmarks that are not buildings are con-sidered to be a single point, and hence the rela-tive angle can be given as a single number.When comparing the map with the street viewpicture, it becomes obvious that the ?SEB?
bankoffice is very hard to see and probably not verysuitable to use as a landmark in route descrip-tions.
On the other hand, the database does notcontain the fact that the building has six storiesand a fa?ade made of yellow bricks, somethingthat would be easily recognizable for the pedes-trian.
This is not due to any shortcoming of theOpenStreetMap database; it just goes to showthat the database has been constructed with mapdrawing in mind, rather than pedestrian routing.There are also some other notable omissions inthe database; e.g.
the shop on the corner, visibleright in front of the user, is not present in the da-tabase.
Since OpenStreetMap is crowd-sourced,there is no guarantee as to which informationwill be present in the database, and which willnot.
This also highlights the limitation of existingapproaches to crowd-sourcing geographic infor-mation: Some useful information is difficult toadd off-line, using a map interface on a PC.
Onthe other hand, it would be a straightforwardmatter given the kind of crowd-sourcing spokendialogue system we present next.Figure 1: A pedestrian routing scenarioFigure 2: The visual scene corresponding to thepedestrian routing scenario in Figure 13 A dialogue system for crowd-sourcingTo verify the potential of the ideas discussedabove, we implemented a spoken dialogue sys-tem that can engage in spoken conversation with3users and learn details about landmarks in visualscenes (such as Figure 2).
To identify the kind ofdetails in a visual scene that the system couldpotentially ask the users, we first conducted apreliminary informal crowd-sourcing dialogue:one person (the receiver), was instructed to seekinformation that could be useful for pedestriannavigation from the other person (the giver).The receiver only had access to informationavailable in the maps from OpenStreetMap, as inFigure 1, but without any marking of field ofviews, whereas the giver only had access to thecorresponding visual scene (as in Figure 2).
In-teraction data from eight such dialogues (fromfour participants, and four different visualscenes) suggested that in a city environment,buildings are prominent landmarks and much ofthe interaction involves their properties such ascolor, number of stories, color of roof, signs orornamentations on buildings, whether it hasshops, etc.
Seeking further details on mentionedsigns, shops, and entities (whether mapped orunmapped) proved to be a useful strategy to ob-tain information.
We also noted that asking foropen-ended questions, such as ?Is there anythingelse in this scene that I should be aware of?
?towards the end has the potential of revealingunknown landmarks and details in the map.Obtaining specific details about known objectsfrom the user corresponds to slot-filling in a dia-logue system, where the dialogue system seeks avalue for a certain slot (= attribute).
By engagingin an open-ended interaction the system couldalso obtain general details to identify new slot-value pairs.
Although slots could be in some cas-es be multi-valued (e.g., a building could haveboth color red and yellow), we have here madethe simplifying assumption that they are singlevalued.
Since users may not always be able tospecify values for slots we treat no-value as avalid slot-value for all type of slots.We also wanted the system to automaticallylearn the most reliable values for the slots, overseveral interactions.
As the system interacts withnew users, it is likely that the system will obtaina range of values for certain slots.
The variabilityof the answers could appear for various reasons:users may have differences in perception aboutslot-values such as colors, some users mightmisunderstand what building is being talkedabout, and errors in speech recognition mightresult in the wrong slot values.
Some of thesevalues may therefore be in agreement with thosegiven by other users, while some may differslightly or be in complete contradiction.
Thus thesystem should be able to keep a record of all thevarious slot-values obtained (including the dis-puted ones), identify slot-values that need to beclarified, and engage in a dialogue with users forclarification.In view of these requirements, we have de-signed our crowd-sourcing dialogue system to beable to (1) take and retain initiative during theinteractions for slot-filling, (2) behave as a re-sponsive listener when engaging in open-endeddialogue, and (3) ask wh?
and yes?no questionsfor seeking and clarifying slot-values, respective-ly.
Thus when performing the slot-filling task,the system mainly asks questions, acknowledges,or clarifies the concepts learned for the slot-values.
Apart from requesting repetitions, theuser cannot ask any questions or by other meanstake the initiative.
A summary of all the attrib-utes and corresponding system prompts is pre-sented in Appendix A.The top half of Figure 3 illustrates the keycomponents of the dialogue system.
The Dia-logue Manager queries the Scene Manager (SM)for slots to be filled or slot-values to be clarified,engages in dialogue with users to learn/clarifyslot-values, and informs the SM about the valuesobtained for these slots.
The SM manages a listof scenes and the predefined slots ?
for each typeof landmark in visual scenes ?
that need to befilled, maintains a record of slot-values obtainedfrom all the users, and identifies slot-values withmajority vote as the current reliable slot-value.To achieve these objectives, the scene manageruses an XML representation of visual scenes.
Inthis representation, landmarks (e.g., buildings,junctions, etc.)
?
automatically acquired throughthe OpenStreetMap database and the visibilitycomputations mentioned in Section 2  ?
arestored as scene-objects (cf.
Figure 4).Figure 3: Dialogue system architectureThe Dialogue Manager (DM) uses scene-object attributes, such as type, angle or intervalof a building, to generate referential expressions,such as ?Do you see a building on the far left?
?4or ?Do you see a shop on the left??
to draw theusers?
attention to the intended landmark in thescene.
During the course of interaction, the Sce-ne Manager (SM) extends scene-objects with aset of predefined attributes (= slots) that we iden-tified in the preliminary study, along with theirvarious slot-values (cf.
Figure 5).
For each slot,the SM keeps a record of slot-values obtainedthrough wh?
questions as well as the ones dis-puted by the users in yes?no questions (cf.
ob-tained and disputed tags in the XML), anduses their tally to identify the slot-value in major-ity.
The system assumes this slot-value (or one ofthem in case of a tie) as its best estimate of aslot-value pair, which it could clarify with anoth-er user using a yes?no query.
During the slot-filling mode the DM switches to open-ended in-teraction mode to seek general details (usingprompts such as ?Could you describe it/them??
),if the user suggests/agrees that there are signson/at a scene-object, or a building has shops orrestaurants.
Once all the slots for all the scene-objects in a visual scene have been queried, theDM once again switches to the open-ended inter-action mode and queries the users whether thereare any other relevant signs or landmarks that thesystem may have missed and should be aware of.On completion of the open-ended queries the SMselects the next visual scene, and the DM engag-es in a new dialogue.<scene xmlns="cityCS.scene" name=" view7.jpg" lat="59.34501"lon="18.0614" fovl="-60" fovr="60" bearing="320" dist="100"><scene-object><id>35274588</id> <type>building</type><from>-60</from> <end>-39</end></scene-object><scene-object><id>538907080</id> <type>shop</type><distance>34.82</distance><angle>-39</angle> <bearing>281</bearing></scene-object><scene-object><id>280604</id> <type>building</type><from>-38</from> <end>6</end></scene-object><scene-object><id>193906</id> <type>traffic_signals</type><distance>40.77</distance><angle>-14</angle> <bearing>306</bearing></scene-object>...</scene>Figure 4: XML representation of visual scenesFor speech recognition and semantic interpre-tation the system uses a context-free grammarwith semantic tags (SRGS1), tailored for the do-main.
The output of semantic interpretation is aconcept.
If the concept type matches the type ofthe slot, the dialogue manager informs the scenemanager about the obtained slot-value.
If the1 http://www.w3.org/TR/speech-grammar/concept type is inappropriate the DM queries theuser once more (albeit using different utteranceforms).
If still no appropriate concept is learnedthe DM requests the SM for the next slot andproceeds with the dialogue.
For speech synthesis,we use the CereVoice system developed byCereProc2.
The dialogue system has been imple-mented using the IrisTK framework (Skantze &Al Moubayed, 2012).<scene-object><id>35274588</id> <type>building</type><from>-60</from> <end>-39</end><slot slotName="VISIBLE">?
</slot><slot slotName="COLOR"><obtained><value slotValue="Green"><userlist><usrDtls uid="u01" asrCnf="0.06" qType="WH"/></userlist></value><value slotValue="no-value"><userlist><usrDtls uid="u02" asrCnf="0.46" qType ="WH"/></userlist></value><value slotValue="Gray"><userlist><usrDtls uid="u03" asrCnf="0.19" qType ="WH"/></userlist></value></obtained><disputed><value slotValue="Green"><userlist><usrDtls uid="u02" asrCnf="0.92" qType ="YN"/></userlist></value></disputed></slot><slot slotName="STORIES">?
</slot><slot slotName="ROOF_COLOR">?
</slot>?</scene-object>Figure 5: Every slot-value is recordedIn contrast to the slot-filling mode, when en-gaging in an open-ended interaction, the systemleaves the initiative to the user and behaves as aresponsive listener.
That is, the system only pro-duces feedback responses, such as backchannels(e.g., okay, mh-hmm, uh-huh), repetition requestsfor longer speaker turns (e.g., could you repeatthat?
), or continuation prompts such as ?any-thing else??
until the user is finished speaking.Unless the system recognized an explicit closingstatement from the user (e.g., ?I can?t?
), the sys-tem encourages the user to continue the descrip-tions for 2 to 4 turns (chosen randomly).To detect appropriate locations in users?speech where the system should give feedbackresponse, the system uses a trained data-drivenmodel (Meena et al., 2013).
When the voice ac-tivity detector detects a silence of 200 ms in us-ers?
speech, the model uses prosodic, contextualand lexico-syntactic features from the precedingspeech segment to decide whether the system2 https://www.cereproc.com/5should produce a feedback response.
The lowerhalf of Figure 3 shows the additional componentsof the dialogue system used in open-ended inter-action mode.
In this mode, the ASR system usesa language model that is trained on interactionsfrom a related domain (verbal route descrip-tions), in parallel to the SRGS grammar.4 In-lab crowd-sourcing experimentNine visual scenes (wide-angle pictures in first-person perspective and taken in Stockholm city,cf.
Figure 2) were used for the task ofcrowdsourcing.
Fifteen human participants (4females and 11 males) participated in thecrowdsourcing exercise.
All participants eitherstudied or worked at the School of ComputerScience and Communication, KTH, Stockholm.Participants were placed in front of a computerdisplay and were told that the system will engagethem in a spoken conversation to seek or clarifydetails about landmarks and other objects in vis-ual scenes.
They were told that the details wouldbe used for pedestrian routing and therefore theyare free to choose and specify details (in open-ended questions) that they thought would be use-ful when giving route instructions to another per-son.Each participant did the nine visual scenes inthe same order, with a 1 minute pause betweeneach of them.
The first visual scene was used asa trial in order to familiarize participants with theinteraction scenario.
For this reason, the trial in-teraction was specifically designed to engage theparticipants in both wh?
and yes?no type ques-tions.
We also wanted to investigate whether theuse of wh?
or yes?no questions alone has anyimpact on the interactions.
For the remainingeight interactions, the dialogue system thereforealternatively used wh?
and yes?no questionsthroughout the whole dialogue.
The order of que-ry types was switched for each successive partic-ipant.
Thus visual scenes presented with wh?queries to a participant pi were presented to par-ticipant pi+1 with only yes?no queries, and vice-versa.
After each interaction participants wereasked (1) whether the system appeared to be fa-miliar with the visual scene; and (2) whether theinteractions went smoothly.An example of a conversation with wh?typequeries is presented in Table 1.
The second col-umn indicates the various types of utterances(dialogue acts) used by the system during thisinteraction.
Note that during the slot-fillingmode, the system has the initiative, whereas inthe open-ended queries (cf.
rows 15?22 in Table1) the system gives the initiative to the partici-pant and only produces feedback responses.
Ta-ble 2 presents a conversation with yes?no queriesfollowing the conversation in Table 1.
In a con-versation with only yes?no  queries, a wh?
querymay be presented if a slot has been observedwith no-value (cf.
rows 9?10 in Table 1) or whenthe participant disputes the system?s current bestestimate (cf.
rows 6?9 in Table 2).S Introduction Could you help me with some de-tails about this view?P  YesS wh?
query Do you see a building on the farleft?P  YesS Backchannel Mh-hmmS wh?
query What color does the building have?P  Cream color [Green color]S Reprise GreenS wh?
query How many floors is this building?P  I don?t know [I do no]?S wh?
query Are there any important signs inthis scene that I may have missed?P  NoS Backchannel Uh-huhS wh?
query Is there anything peculiar in thisscene that I should be familiar withP  There is a bike painted on streetjust in front of me[ is the right pink university in frontof may ]S Backchannel Anything else?P  Yeah its white paint [Yes light pink]S Backchannel Mh-hmmP  That?s it [that that]S Backchannel EhmP  I am done [okay]S Ending Okay, thanksTable 1: An example interaction with wh?
que-ries for the visual scene in Figure 2.
S: system, P:participant (here, u01 from Figure 5).
ASR re-sults are shown within square brackets.S Introduction Could you help me with some detailsabout this view?P  YesS yes?noqueryIs the building on the far left visible toyou?P  YesS Backchannel Mh-hmmS yes?noqueryDoes this building have green color?P  NoS Backchannel Uh-huhS wh?
query But than what color is the building?6P  white [light]S Repeat Sorry, what color did you say?P  white [light]S wh?
query How many stories is this building?P  I don?t know [I do rental]S Backchannel Ehm?S wh?
query Are there any important signs in thisscene that I may have missed?P  Ehm no [nine no]S Backchannel EhmS yes?noqueryI am informed that there are otherimportant landmarks or structures inthis scene that I should be aware of,is it true?P  NoS Backchannel uh-huhS Ending Okay, thanksTable 2: An example interaction with yes?noqueries corresponding to the visual scene in Fig-ure 2.
S: system, P: participant (here u02 fromFigure 5).
ASR results are shown within squarebrackets.5 Data analysisWe analyzed the data (15 8 interactions) col-lected from the experiment along the followingtracks: first, we compare the majority value ofthe slots to the ground truth as given by a humanannotator; second, we explore how the groundtruth of slot-values could be estimated automati-cally; third, we also analyzed the instances wherethe participants disputed the system?s currentestimate of slot-values; and fourth, we examinedthe post-experimental questionnaires.5.1 Rate of learning slot-valuesA total of 197 slots were learned in the exper-iment.
We analyzed how many slot-values hadbeen correctly retrieved after 1, 2?
15 users.
InFigure 6, the curve ?Majority?
illustrates thefraction of slot-values correctly learned witheach new user, under the assumption that theslot-values with majority votes ?
from all the 15users ?
constitute the ground truth.
Thus afterinteracting with the first user the system had ob-tained 67.0% of slot-values correctly (accordingto the majority) and 96.4% of slot-values afterinteracting with the first six users.
Another eightusers, or fourteen in total, were required to learnall the slot-values correctly.
The progressioncurve thus provides an estimate of how manyusers are required to achieve a specific percent-age of slot-values correctly if majority is to beconsidered the ground truth.
The curve ?Not-in-Majority?
indicates the number of slot with val-ues that were not in the majority.
Thus after in-teracting with the first user 20.8% of slot-valuesthe system had obtained were not in majority andcould be treated as incorrect.
Note that the curvesMajority and Not-in-Majority do not sum up to100%, this is because we consider no-value as avalid slot-value, and treat the slot as unfilled.
Forexample, 12.2% of the slots remained unfilledafter interacting with the first user.Figure 6: Rate of learning slot-values with two differ-ent estimates of ground truthWe also investigated how close the majority isto the actual truth.
A human annotator (one of thecoauthors) labeled all the obtained slot-values aseither sensible or insensible, based on the com-bined knowledge from the corresponding maps,the visual scenes, and the set of obtained values.Thus a slot could have many sensible values.
Forexample, various parts of a building could bepainted in different colors.
The progressioncurves ?Sensible?
and ?Insensible?
in Figure 6illustrate the fraction of total slots for which thelearned values were actually correct and incor-rect, respectively.
While the curve for sensiblevalues follows the same pattern as the progres-sion curve for majority as the estimate of groundtruth, the percent of slot-values that were actuallycorrect is always lower than the majority asground truth, and it never reached 100%.
Theconstant gap between the two curves suggeststhat some slot-values learned by the majoritywere not actually the ground truth.
What led themajority into giving incorrect slot-values is leftas a topic for future work.As mentioned earlier, much of the slot-fillinginteraction involved buildings and their proper-ties.
Figure 7 illustrates that sensible values formost slots, pertaining to whether a building isvisible, whether it is residential, whether it hasshops, and the color of roof were obtained byinteracting with only few participants.
In con-trast, properties such as color of the building and7number of stories required many more partici-pants.
This could be attributed to the fact thatparticipants may have differences in perceptionabout slot-values.
As regards to whether there aresigns on buildings, we observed that the recall isrelatively low.
This is largely due to lack ofcommon ground among participants about whatcould be considered a sign.
Our intentions withdesigning this prompt was to retrieve any peculi-ar detail on the building that is easy to locate: forus a sign suggesting a name of restaurant is asuseful as the knowledge that the building hasblue sunshade on the windows.
Some partici-pants understood this while other didn?t.Figure 7: Learning rate of various slots for land-mark type building5.2 Estimated ground truth of slot-valuesThe 15 subjects in the in-lab experiment were allasked for the same information.
In a real applica-tion, however, we want the system to only askfor slots for which it has insufficient or conflict-ing information.
If the ground truth of a certainslot-value pair can be estimated with a certaintyexceeding some threshold (given the quality re-quirements of the database, say 0.8), the systemcan consider the matter settled, and need not askabout that slot again.
We therefore want to esti-mate the ground truth of slot-values along with acertainty measure.
To this end, we use theCityCrowdSource Trust software package(Dickens & Lupu, 2014), which is based on theprobabilistic approach for supervised learningwhen we have multiple annotators providing la-bels (possibly noisy) but no absolute gold stand-ard, presented in Raykar et al.
(2009).Using this approach, a question concerning thecolor of a building, say with ID 24, (e.g.
?Whatcolor is the building??)
would be translated intoseveral binary predicates COLOR_Red(24),COLOR_Brown(24), COLOR_Orange(24), etc.The justification for this binary encoding is thatthe different color values are not mutually exclu-sive: A building might of course have more thanone color, and in many cases more than one colorname might be appropriate even though thebuilding has only one dominating color (e.g.
todescribe the color either as ?brown?
and ?red?might be acceptable to most people).
Figure 8shows the incremental estimates for differentcolors for a certain building (OpenStreetMap ID163966736) after 1, 2?
15 subjects had beenasked.
The answer from the first subject was er-roneously recognized as ?pink?.
The next 9 sub-jects all referred to the building as ?brown?.Among the final subjects, 3 subjects referred tobuilding as ?red?, and 2 subjects as ?brown?.
Thefinal truth estimates are 0.98 for ?brown?, 0.002for ?red?, and 0.00005 for ?pink?.
The diagramshows that if the certainty threshold is set to 0.8,the value ?brown?
would have been establishedalready after 4 subjects.Figure 8: Probabilities of different estimated groundtruth values for the color of a certain building5.3 Disputed slot-valuesWe also examined all system questions ofyes?no type that received negative answers, i.e.instances where the participants disputed the sys-tem?s current best estimate (based on majorityvote) of a slot-value.
Among the 95 such in-stances, the system?s current best estimate wasactually insensible only on 43 occasions.
In 30 ofthese instances the participants provided a recti-fied slot-value that was sensible.
For the remain-ing 13 instances the new slot-values proposed bythe participant were actually insensible.
Therewere 52 instances of false disputations, i.e.
thesystem?s current estimate of a slot-value wassensible, but the participants disputed it.
6 of the-se occurrences were due to errors in speechrecognition, but for the remaining 46 occasions,error in grounding the intended landmark (15),users?
perception of slot-values (3), and ambigui-ty in what the annotator terms as sensible slot-values (28), (e.g.
whether there are signs on abuilding (as discussed in Section 5.1)) were iden-8tified as the main reasons.
This suggests thatslots (i.e.
attributes) that are often disputed maynot be easily understood by users.5.4 Post-experimental questionnaireAs described above, the participants filled in aquestionnaire after each interaction.
They wereasked to rate the system?s familiarity with thevisual scene based on the questions asked.
AMann?Whitney U test suggests that participants?perception of the system?s familiarity with thevisual scene was significantly higher for interac-tions with yes?no queries than interactions withwh?
queries (U=1769.5, p= 0.007).
This resulthas implications for the design choice for sys-tems that provide as well as ask for informationfrom users.
For example, a pedestrian routingsystem can already be used to offer routing in-structions as well as crowdsourcing information.The system is more likely to give an impressionof familiarity with the surrounding, to the user,by asking yes?no type questions than wh?questions.
This may influence a user?s confi-dence or trust in using the routing system.Since yes?no questions expect a ?yes?
or?no?
in response, we therefore hypothesized thatinteractions with yes?no questions would be per-ceived smoother in comparison to interactionswith wh?
questions.
However, a Mann?WhitneyU test suggests that the participants perceived nosignificant difference between the two interac-tion types (U=1529.0, p= 0.248).
Feedbackcomments from participants suggest that abruptending of open-ended interactions by the system(due to the simplistic model of detecting whetherthe user has anything more to say) gave users animpression that the system is not allowing themto speak.6 Discussion and future workWe have presented a proof-of-concept study onusing a spoken dialogue system for crowd-sourcing street-level geographic information.
Toour knowledge, this is the first attempt at usingspoken dialogue systems for crowdsourcing inthis way.
The system is fully automatic, in thesense that it (i) starts with minimal details ?
ob-tained from OpenStreetMap ?
about a visual sce-ne, (ii) prompts users with wh?
questions to ob-tain values for a predefined set of attributes; and(iii) assumes attribute-values with majority voteas its beliefs, and engages in yes?no questionswith new participants to confirm them.
In a datacollection experiment, we have observed thatafter interacting with only 6 human participantsthe system acquires more than 80% of the slotswith actually sensible values.We have also shown that the majority vote (asperceived by the system) could also be incorrect.To mitigate this, we have explored the use of theCityCrowdSource Trust software package(Dickens & Lupu, 2014) for obtaining the proba-bilistic estimate of the ground truth of slot-valuesin a real crowd-sourcing system.
However, it isimportant not only to consider the ground truthprobabilities per se, but also on how many con-tributing users the estimate is based and the qual-ity of information obtained.
We will explore the-se two issues in future work.We have observed that through open-endedprompts, the system could potentially collect alarge amount of details about the visual scenes.Since we did not use any automatic interpretationof these answers, we transcribed key concepts inparticipants?
speech in order to obtain an esti-mate of this.
However, it is not obvious how toquantify the number of concepts.
For example,we have learned that in Figure 2, at the junctionahead, there is: a traffic-sign, a speed-limit sign,a sign with yellow color, a sign with red color, asign with red boarder, a sign that is round, a signwith some text, the text says 50.
These are detailsobtained in pieces from various participants.Looking at Figure 2 one can see that these pieceswhen put together refer to the speed-limit signmounted on the traffic-signal at the junction.How to assimilate these pieces together into aunified concept is a task that we have left for fu-ture work.AcknowledgementWe would like to thank the participants of the in-lab crowd-sourcing experiment.
This work issupported by the EIT KIC project?CityCrowdSource?, and the Swedish researchcouncil (VR) project Incremental processing inmultimodal conversational systems (2011-6237).ReferenceBartie, P. J., & Mackaness, W. A.
(2006).
Develop-ment of a Speech-Based Augmented Reality Sys-tem to Support Exploration of Cityscape.
Transac-tions in GIS, 10(1), 63-86.Boye, J., Fredriksson, M., G?tze, J., Gustafson, J., &K?nigsmann, J.
(2014).
Walk This Way: SpatialGrounding for City Exploration.
In Mariani, J.,Rosset, S., Garnier-Rizet, M., & Devillers, L.9(Eds.
), Natural Interaction with Robots, Knowbotsand Smartphones (pp.
59-67).
Springer New York.Dickens, L., & Lupu, E. (2014).
Trust service finaldeliverable report.
Technical Report, Imperial Col-lege, UK.Haklay, M., & Weber, P. (2008).
OpenStreetMap:User-Generated Street Maps.
IEEE PervasiveComputing, 7(4), 12-18.Janarthanam, S., Lemon, O., Liu, X., Bartie, P.,Mackaness, W., Dalmas, T., & Goetze, J.
(2012).Integrating Location, Visibility, and Question-Answering in a Spoken Dialogue System for Pe-destrian City Exploration.
In Proceedings of the13th Annual Meeting of the Special Interest Groupon Discourse and Dialogue (pp.
134-136).
Seoul,South Korea: Association for Computational Lin-guistics.Krug, K., Mountain, D., & Phan, D. (2003).
Webpark:Location-based services for mobile users in pro-tected areas.. GeoInformatics, 26-29.Parent, G., & Eskenazi, M. (2011).
Speaking to theCrowd: Looking at Past Achievements in UsingCrowdsourcing for Speech and Predicting FutureChallenges.
In INTERSPEECH (pp.
3037-3040).ISCA.Raykar, V. C., Yu, S., Zhao, L. H., Jerebko, A., Flor-in, C., Valadez, G. H., Bogoni, L., & Moy, L.(2009).
Supervised Learning from Multiple Ex-perts: Whom to Trust when Everyone Lies a Bit.
InProceedings of the 26th Annual International Con-ference on Machine Learning (pp.
889-896).
NewYork, NY, USA: ACM.Ross, T., May, A., & Thompson, S. (2004).
The Useof Landmarks in Pedestrian Navigation Instructionsand the Effects of Context.
In Brewster, S., & Dun-lop, M.
(Eds.
), Mobile Human-Computer Interac-tion - MobileHCI 2004 (pp.
300-304).
SpringerBerlin Heidelberg.Skantze, G., & Al Moubayed, S. (2012).
IrisTK: astatechart-based toolkit for multi-party face-to-faceinteraction.
In Proceedings of ICMI.
Santa Monica,CA.Tom, A., & Denis, M. (2003).
Referring to Landmarkor Street Information in Route Directions: WhatDifference Does It Make?.
In Kuhn, W., Worboys,M., & Timpf, S.
(Eds.
), Spatial Information Theo-ry.
Foundations of Geographic Information Sci-ence (pp.
362-374).
Springer Berlin Heidelberg.10Appendix AThe table below lists slots (= landmark attributes) and the corresponding wh?
and yes?no system questions.
Forattributes marked with * the dialogue manager switches to open-ended interaction mode.Slot (=attribute ) System wh?
questions System yes?no questionsVisible: whether a particularlandmark is visible from thisview.?
Do you see a building on the far left??
Do you see another building in front ofyou??
Is there a junction on the right??
Do you see a traffic-signal ahead??
Is the building on the far right visible toyou??
I think there is another building in front ofyou, do you see it??
Can you see the junction on the right??
Are you able to see the traffic-signalahead?Color of the building?
What color does the building have??
What color is the building??
I think this building is red in color, what doyou think??
Does this building have red color?Size of the building (in num-ber of stories)?
How many floors do you think arethere in this building?
How many stories is this building?
I think there are six floors in this building,what do you think??
Is this building six storied?Color of the building?s roof?
What color does the roof of this build-ing have??
What color is the roof of this building??
I think the roof of this building is orange incolor, what do you think??
Do you think that the roof of this buildingis orange?Signs or ornamentation on thebuilding?
Do you see any signs or decorationson this building??
I think there is a sign or some decorationon this building, do you see it??
There may be a sign or a name on thisbuilding, do you see it?Shops or restaurants in thebuilding?
Are there any shops or restaurants inthis building??
I am informed that there are some shops orrestaurants in this building, is it true??
I think there are some shops or restaurantsin this building, what do you think?Signs at landmarks?
Are there any important signs at thejunction/crossing??
I believe there is a sign at this junc-tion/crossing, do you see it??
Do you see the sign at this junc-tion/crossing?
*Description of sign?
Could you describe this sign??
What does this sign look like??
Does the sign say something??
Could you describe this sign??
What does this sign look like??
Does the sign say something?
*Signs in the visual scene?
Are there any important signs in thisscene that I may have missed??
Have I missed any relevant signs inthis scene??
There are some important signs in thisscene that could be useful for myknowledge, am I right??
I am informed that there are some signs inthis scene that are relevant for me, is ittrue?
*Landmarks in the visual sce-ne?
Are there any other important build-ings or relevant structures in this scenethat I should be aware of??
Is there anything particular in thisscene that I should be familiar with??
Have I missed any relevant buildingsor landmarks in this scene??
I am informed that there are some im-portant landmarks or structures in this sce-ne that I should be aware of, is it true??
I have been told that there are some otherthings in this scene that I are relevant forme, is it true??
I believe I have missed some relevantlandmarks in this scene, am I right?
*Description of unknownlandmarks e.g.
shop, restau-rant, building, etc.?
Could you describe it??
Could you describe them??
How do they look like??
Could you describe it??
Could you describe them??
How do they look like?11
