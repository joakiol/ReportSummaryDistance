Treatment of Epsilon Moves in SubsetConstructionGert jan  van  Noord"Rijksuniversiteit GroningenThe paper discusses the problem of determinizing finite-state automata containing large numbersof c-moves.
Experiments with finite-state approximations ofnatural anguage grammars oftengive rise to very large automata with a very large number of c-moves.
The paper identifies andcompares a number of subset construction algorithms that treat c-moves.
Experiments have beenperformed which indicate that the algorithms differ considerably in practice, both with respectto the size of the resulting deterministic automaton, and with respect o practical efficiency.Furthermore, the experiments suggest that the average number of ~-moves per state can be usedto predict which algorithm is likely to be the fastest for a given input automaton.1.
Introduction1.1 Finite-State Language ProcessingAn important problem in computational linguistics is posed by the fact that the gram-mars typically hypothesized by linguists are unattractive from the point of view ofcomputation.
For instance, the number of steps required to analyze a sentence of nwords is n 3 for context-free grammars.
For certain linguistically more attractive gram-matical formalisms it can be shown that no upper bound to the number of stepsrequired to find an analysis can be given.
The human language user, however, seemsto process in linear time; humans understand longer sentences with no noticeabledelay.
This implies that neither context-free grammars nor more powerful grammati-cal formalisms are likely models for human language processing.
An important issuetherefore is how the linearity of processing by humans can be accounted for.A potential solution to this problem concerns the possibility of approximatingan underlying general and abstract grammar by techniques of a much simpler sort.The idea that a competence grammar might be approximated by finite-state meansgoes back to early work by Chomsky (Chomsky 1963, 1964).
There are essentiallythree observations that motivate the view that the processing of natural anguage isfinite-state:1.2..humans have a finite (small, limited, fixed) amount of memory availablefor language processinghumans have problems with certain grammatical constructions, uch ascenter-embedding, which are impossible to describe by finite-state means(Miller and Chomsky 1963)humans process natural anguage very efficiently (in linear time)* Alfa-informatica & BCN.
E-mail: vannoord@let.rug.nl(~ 2000 Association for Computational LinguisticsComputational Linguistics Volume 26, Number 11.2 Finite-State Approximation and c-MovesIn experimenting with finite-state approximation techniques for context-free and morepowerful grammatical formalisms (such as the techniques presented in Black \[1989\],Pereira and Wright \[1991, 1997\], Rood \[1996\], Grimley-Evans \[1997\], Nederhof \[1997,1998\], and Johnson \[1998\]), we have found that the resulting automata often are ex-tremely large.
Moreover, the automata contain many e-moves (jumps).
And finally, ifsuch automata re determinized then the resulting automata re often smaller.
It turnsout that a straightforward implementation f the subset construction determinizationalgorithm performs badly for such inputs.
In this paper we consider a number ofvariants of the subset construction algorithm that differ in their treatment of c-moves.Although we have observed that finite-state approximation techniques typicallyyield automata with large numbers of c-moves, this is obviously not a necessity.
Insteadof trying to improve upon determinization techniques for such automata, it might bemore fruitful to try to improve these approximation techniques in such a way thatmore compact automata re produced.
1 However, because research into finite-stateapproximation is still of an exploratory and experimental nature, it can be arguedthat more robust determinization algorithms do still have a role to play: it can beexpected that approximation techniques are much easier to define and implement ifthe resulting automaton is allowed to be nondeterministic and to contain c-moves.Note furthermore that even if our primary motivation is in finite-state approxima-tion, the problem of determinizing finite-state automata with c-moves may be relevantin other areas of language research as well.1.3 Subset Construction and c-MovesThe experiments were performed using the FSA Utilities.
The FSA Utilities toolbox(van Noord 1997, 1999; Gerdemann and van Noord 1999; van Noord and Gerde-mann 1999) is a collection of tools to manipulate regular expressions, finite-stateautomata, and finite-state transducers.
Manipulations include determinization, min-imization, composition, complementation, i tersection, Kleene closure, etc.
Variousvisualization tools are available to browse finite-state automata.
The toolbox is imple-mented in SICStus Prolog, and is available free of charge under Gnu General PublicLicense via anonymous ftp at ftp://ftp.let.rug.nl/pub/vannoord/Fsa/, and via theweb at http://www.let.rug.nl/~vannoord/Fsa/.
At the time of our initial experimentswith finite-state approximation, an old version of the toolbox was used, which raninto memory problems for some of these automata.
For this reason, the subset con-struction algorithm has been reimplemented, paying special attention to the treatmentof E-moves.
Three variants of the subset construction algorithm are identified, whichdiffer in the way c-moves are treated:per graph The most obvious and straightforward approach is sequential in thefollowing sense: Firstly, an equivalent automaton without c-moves is con-structed for the input.
To do this, the transitive closure of the graph consist-ing of all c-moves is computed.
Secondly, the resulting automaton is thentreated by a subset construction algorithm for c-free automata.
Differentvariants of per graph can be identified, depending on the implementationof the c-removal step.1 Indeed, a later implementation by Nederhof avoids construction f the complete nondetermisticautomaton byminimizing subautomata before they are embedded into larger subautomata.62van Noord Epsilon Moves in Subset Constructionper state For each state that occurs in a subset produced uring subset construc-tion, compute the states that are reachable using e-moves.
The results ofthis computation can be memorized, or computed for each state in a pre-processing step.
This is the approach mentioned briefly in Johnson andWood (1997).
2per subset For each subset Q of states that arises during subset construction, com-pute Q~ 2 Q, which extends Q with all states that are reachable from anymember of Q using e-moves.
Such an algorithm is described in Aho, Sethi,and Ullman (1986).The motivation for this paper is the knowledge gleaned from experience, that thefirst approach turns out to be impractical for automata with very large numbers ofe-moves.
An integration of the subset construction algorithm with the computation ofe-reachable states performs much better in practice for such automata.Section 2 presents a short statement of the problem (how to determinize a givenfinite-state automaton), and a subset construction algorithm that solves this problem inthe absence of e-moves.
Section 3 defines a number of subset construction algorithmsthat differ with respect to the treatment of e-moves.
Most aspects of the algorithms arenot new and have been described elsewhere, and/or were incorporated in previousimplementations; a comparison of the different algorithms had not been performedpreviously.
We provide a comparison with respect to the size of the resulting determin-istic automaton (in Section 3) and practical efficiency (in Section 4).
Section 4 providesexperimental results both for randomly generated automata nd for automata gen-erated by approximation algorithms.
Our implementations of the various algorithmsare also compared with AT&T's FSM utilities (Mohri, Pereira, and Riley 1998), to es-tablish that the experimental differences we find between the algorithms are trulycaused by differences in the algorithm (as opposed to accidental implementation de-tails).2.
Subset Construction2.1 Problem StatementLet a finite-state machine M be specified by a tuple (Q, G, 6, S, F) where Q is a finiteset of states, G is a finite alphabet, and ~ is a function from Q x (G u {?})
--* 2 Q.Furthermore, S c_ Q is a set of start states and F _C Q is a set of final states.
3Let e-move be the relation {(qi, qj)lqj E ~(qi, e)}.
c-reachable is the reflexive andtransitive closure of e-move.
Let e-CLOSURE: 2 Q ~ 2 Q be a function defined as:e-CLOSURE(Q') = {qlq' E Q', (q',q) E e-reachable}Furthermore, we write e-CLOSURE-I(Q ') for the set {qlq' E Q', (q,q') E e-reachable}.2 According to Derick Wood (p. c.), this approach as been implemented in several systems, includingHoward Johnson's INR system.3 Note that a set of start states is required, rather than a single start state.
Many operations on automatacan be defined somewhat more elegantly in this way (including per graph t discussed below).
Obviously,for deterministic automata this set should be a singleton set.63Computational Linguistics Volume 26, Number 1funct subset_eonstruction( ( Q, ~, ~, S, F) )index_transitions(); Trans := O; Finals := O; States := O;Start := epsilon_closure( S)add(Start)whi le there is an unmarked subset T E States d__qomark(T)foreach (a, U) C instructions(T) doU := epsilon_closure(U)Trans\[T,a\] := {U}add(U)ododreturn (States, ~, Trans, {Start}, Finals)endproc add(U) Reachable-state-set Maintenanceif U ~ Statesthen add U unmarked to Statesif U M F then Finals := Finals U {U} fifiendfunct instructions(P) Instruction Computationreturn merge(Up~ P transitions(p))endfunct epsilon_closure( U)return Uendvariant 1: No c-movesFigure 1Subset construction algorithm.For any given finite-state automaton M = (Q, G, 6, S, F), there is an equivalent de-terministic automaton M I = (2 Q, G, 6', {Q0}, FI) ?
F ~ is the set of all states in 2 Q containinga final state of M, i.e., the set of subsets {Qi E 2Qiq E Qi, q E F}.
M'  has a single startstate Q0, which is the epsilon closure of the start states of M, i.e., Q0 = c-CLOSURE(S).Finally,6'({ql, q2 .
.
.
.
.
qi},a) = E-CLOSURE(6(ql, a) U 6(q2,a) U .
.
.
U 6(qi, a))An algorithm that computes M / for a given M will only need to take into accountstates in 2 Q that are reachable from the start state Q0.
This is the reason that for manyinput automata the algorithm does not need to treat all subsets of states (but note thatthere are automata for which all subsets are relevant, and hence exponential behaviorcannot be avoided in general).Consider the subset construction algorithm in Figure 1.
The algorithm maintainsa set of subsets States.
Each subset can be either marked or unmarked (to indicatewhether the subset has been treated by the algorithm); the set of unmarked sub-sets is sometimes referred to as the agenda.
The algorithm takes such an unmarkedsubset T and computes all transitions leaving T. This computat ion is per formed bythe function instructions and is called instruction computation by Johnson and Wood(1997).64van Noord Epsilon Moves in Subset ConstructionThe function index_transitions constructs the function transitions: Q --, ~.
x 2 Q, whichreturns for a given state p the set of pairs (s, T) representing the transitions leaving p.Furthermore, the function merge takes such a set of pairs and merges all pairs with thesame first element (by taking the union of the corresponding second elements).
Forexample:merge({(a, {1,2,4}), (b, {2,4}), (a, {3,4}), (b, {5,6})})= {(a, {1,2,3,4}), (b, {2,4,5,6})}The procedure add is responsible for "reachable-state-set maintenance," by en-suring that target subsets are added to the set of subsets if these subsets were notencountered before.
Moreover, if such a new subset contains a final state, then thissubset is added to the set of final states.3.
Variants for E-MovesThe algorithm presented in the previous ection does not treat c-moves.
In this section,possible extensions of the algorithm are identified to treat c-moves.3.1 Per GraphIn the per graph variant, two steps can be identified.
In the first step, efree, an equiva-lent c-free automaton is constructed.
In the second step this c-free automaton is deter-minized using the subset construction algorithm.
The advantage of this approach isthat the subset construction algorithm can remain simple because the input automatonis c-free.An algorithm for efree is described for instance in Hopcroft and Ullman (1979, 26-27).
The main ingredient of efree is the construction of the function c-CLOSURE, whichcan be computed using a standard transitive closure algorithm for directed graphs:this algorithm is applied to the directed graph consisting of all c-moves of M. Suchan algorithm can be found in several textbooks (see, for instance, Cormen, Leiserson,and Rivest \[1990\]).For a given finite-state automaton M = (Q, G,6,S,F), efree computes M' =(Q, ~, 6', S', F'), where S' = c-CLOSURE(S), F' = c-CLOSURE -1 (F), and 6'(p,a) ={qiq' E 6(p', a), p' c c-CLOSURE -1 (p), q E c-CLOSURE(q')}.
Instead of using c-CLOSUREon both the source and target side of a transition, efree can be optimized in two differentways by using c-CLOSURE only on one side:efreet: M' = (Q, ~, 6', S',F), where S' = c-CLOSURE(S), and6'(p,a) = {qiq' E 6(p,a),q E c-CLOSURE(q')}.efreeS: M' = (Q, ~, 6', S,F'), where F' = ?-CLOSURE-I(F), and6'(p,a) = {qlq E 6(p',a),p' E c-CLOSURE-I(p)}.Although the variants appear very similar, there are some differences.
Firstly, efree tmight introduce states that are not coaccessible: states from which no path exists to afinal state; in contrast, efree s might introduce states that are not accessible: states fromwhich no path exists from the start state.
A straightforward modification of both algo-rithms is possible to ensure that these states are not present in the output.
Thus efree t,c65Computational Linguistics Volume 26, Number 1ca a(1)(2) (3)a2)a a a(4 (5)Figure 2Illustration of the difference in size between two variants of efree.
(1) is the input automaton.The result of efree t is given in (2); (3) is the result of erred.
(4) and (5) are the result of applyingthe subset construction to the result of efree t and efred, respectively.ensures that all states in the resulting automaton are co-accessible; free s,a ensures thatall states in the resulting automaton are accessible.
As a consequence, the size of thedeterminized machine is in general smaller if efree t,c is employed, because states thatwere not co-accessible (in the input) are removed (this is therefore an additional ben-efit of efreet,C; the fact that efree s,a removes accessible states has no effect on the size ofthe determinized machine because the subset construction algorithm already ensuresaccessibility anyway).Secondly, it turns out that applying eSree t in combination with the subset construc-tion algorithm generally produces maller automata than efree s (even if we ignore thebenefit of ensuring co-accessibility).
An example is presented in Figure 2.
The differ-ences can be quite significant, as illustrated in Figure 3.Below we will write per graph x to indicate the nonintegrated algorithm based onefree x .3.2 Per Subset and Per StateNext, we discuss two variants (per subset and per state) in which the treatment of c-moves is integrated with the subset construction algorithm.
We will show later thatsuch an integrated approach is in practice often more efficient han the per graph ap-proach if there are many C-moves.
The per subset and per state approaches are alsomore suitable for a lazy implementation of the subset construction algorithm (in sucha lazy implementation, subsets are only computed with respect to a given inputstring).The per subset and the per state algorithms use a simplified variant of the transitiveclosure algorithm for graphs.
Instead of computing the transitive closure of a given66van Noord Epsilon Moves in Subset Construction2000018000160001400012000100008000Z6000400020000I I Iefree-source oef ree- target  ,0.2 0.4 0.6 0.8 1 1.2 1.4Deterministic Jump Density (mean)1.6 1.8 2Figure 3Difference in sizes of deterministic automata constructed with either efree s or  efree t, forrandomly generated input automata consisting of 100 states, 15 symbols, and various numbersof transitions and jumps (cf.
Section 4).
Note that all states in the input are co-accessible; thedifference in size is due solely to the effect illustrated in Figure 2.funct closure(T)D:=0foreach t E T do add t unmarked to D odwhile there is an unmarked state t C D domark(t)foreach q E ~5(t, e) doif q ~ D then add q unmarked to D fiododretum DendFigure 4Epsilon closure algorithm.graph, this algorithm only computes the closure for a given set of states.
Such analgorithm is given in Figure 4.In both of the two integrated approaches, the subset construction algorithm is ini-tialized with an agenda containing a single subset hat is the e-CLOSURE of the set ofstart states of the input; furthermore, the way in which new transitions are computedalso takes the effect of c-moves into account.
Both differences are accounted for by analternative definition of the epsilon_closure function.The approach in which the transitive closure is computed for one state at a t imeis defined by the following definition of the epsilon_closure function.
Note that wemake sure that the transitive closure computat ion is only performed once for each67Computational Linguistics Volume 26, Number 1input state, by memorizing the closure function; the full computation is memorizedas well.
4funct epsilon_closure( U)return memo(Uu~u memo(closure( {u} ) ) )endvariant 2: per stateIn the case of the per subset approach, the closure algorithm is applied to eachsubset.
We also memorize the closure function, in order to ensure that the closurecomputation is performed only once for each subset.
This can be useful, since thesame subset can be generated many times during subset construction.
The definitionsimply is:funct epsilon_closure( U)return memo(closure(U))endvariant 3: per subsetThe motivation for the per state variant is the insight that in this case the closurealgorithm is called at most IQ\] times.
In contrast, in the per subset approach the transi-tive closure algorithm may need to be called 2 IQI times.
On the other hand, in the perstate approach some overhead must be accepted for computing the union of the resultsfor each state.
Moreover, in practice, the number of subsets is often much smaller than21QI.
In some cases, the number of reachable subsets is smaller than the number ofstates encountered in those subsets.3.3 ImplementationIn order to implement the algorithms efficiently in Prolog, it is important to use ef-ficient data structures.
In particular, we use an implementation f (non-updatable)arrays based on the N+K trees of O'Keefe (1990, 142-145) with N = 95 and K = 32.On top of this data structure, a hash array is implemented using the SICStus librarypredicate term_hash/4, which constructs a key for a given term.
In such hashes, avalue in the underlying array is a partial list of key-value pairs; thus collisions areresolved by chaining.
This provides efficient access in practice, although such ar-rays are quite memory-intensive: care must be taken to ensure that the deterministicalgorithms indeed are implemented without introducing choice-points during run-time.4.
ExperimentsTwo sets of experiments have been performed.
In the first set of experiments, randomautomata re generated according to a number of criteria based on Leslie (1995).
Inthe second set of experiments, results are provided for a number of (much larger)automata that surfaced uring actual development work on finite-state approximationtechniques.
5Random Automata.
Here, we report on a number of experiments for randomly gener-ated automata.
Following Leslie (1995), the absolute transition density of an automaton4 This is an improvement over the algorithm given in a preliminary version of this paper (van Noord1998).5 All the automata used in the experiments are freely available fromhttp://www.let.rug.nl/-vannoord / Fsa/.68van Noord Epsilon Moves in Subset Constructionis defined as the number of transitions divided by the square of the number of statesmultiplied by the number of symbols (i.e., the number of transitions divided by themaximum number of "possible" transitions, or, in other words, the probability that apossible transition in fact exists).
Deterministic transition density is the number of tran-sitions divided by the number of states multiplied by the number of symbols (i.e., theratio of the number of transitions and the maximum number of "possible" transitionsin a deterministic machine).In both of these definitions, the number of transitions hould be understood asthe number of nonduplicate ransitions that do not lead to a sink state.
A sink stateis a state from which there exists no sequence of transitions to a final state.
In therandomly generated automata, states are accessible and co-accessible by construction;sink states and associated transitions are not represented.Leslie (1995) shows that deterministic transition density is a reliable measure forthe difficulty of subset construction.
Exponential blow-up can be expected for inputautomata with deterministic transition density of around 2.
6 He concludes (page 66):randomly generated automata exhibit the maximum execution time,and the maximum number of states, at an approximate deterministicdensity of 2.
Most of the area under the curve occurs within 0.5 and2.5 deterministic density--this  the area in which subset constructionis expensive.Conjecture.
For a given NFA, we can compute the expected num-bers of states and transitions in the corresponding DFA, produced bysubset construction, from the deterministic density of the NFA.
In ad-dition, this functional relationship gives rise to a Poisson-like curvewith its peak approximately ata deterministic density of 2.A number of automata were generated randomly, according to the number ofstates, symbols, and transitions.
For the first experiment, automata were generatedconsisting of 15 symbols, 25 states, and various densities (and no c-moves).
The resultsare summarized in Figure 5.
CPU-time was measured on a HP 9000/785 machinerunning HP-UX 10.20.
Note that our timings do not include the start-up of the Prologengine, nor the time required for garbage collection.In order to establish that the differences we obtain later are genuinely due todifferences in the underlying algorithm, and not due to "accidental" implementationdetails, we have compared our implementation with the determinizer ofAT&T's FSMutilities (Mohri, Pereira, and Riley 1998).
For automata without e-moves, we establishthat FSM normally is faster: for automata with very small transition densities, FSM isup to four times as fast; for automata with larger densities, the results are similar.A new concept called absolute jump density is introduced to specify the numberof c-moves.
It is defined as the number of e-moves divided by the square of thenumber of states (i.e., the probability that an c-move exists for a given pair of states).Furthermore, deterministic jump density is the number of e-moves divided by thenumber of states (i.e., the average number of e-moves that leave a given state).
Inorder to measure the differences between the three implementations, a number ofautomata have been generated consisting of 15 states and 15 symbols, using various6 Leslie uses the terms absolute density and deterministic density.69Computational Linguistics Volume 26, Number 1le+06L~o 100000 +100001000Z%- 100g~ 101Figure 5++\[\]+\[\]~+ o++\[\]++fsafsm +states \[\]1 10Deterministic DensityDeterministic transition density versus CPU-time in msec.
The input automata have 25 states,15 symbols, and no C-moves.
fsa represents he CPU-time required by our FSA6implementation; fsm represents he CPU-time required by AT&T's FSM library; statesrepresents he sum of the number of states of the input and output automata.transition densities between 0.01 and 0.3 (for larger densities, the automata tend tocollapse to an automaton for ~.*).
For each of these transition densities, deterministicjump densities were chosen in the range 0 to 2.5 (again, for larger values, the automatatend to collapse).
In Figures 6 to 9, the outcomes of these experiments are summarizedby listing the average amount of CPU-time required per deterministic jump density(for each of the algorithms), using automata with 15, 20, 25, and 100 states, respectively.Thus, every dot represents he average for determinizing a number of different inputautomata with various absolute transition densities and the same deterministic jumpdensity.The striking aspect of these experiments i  that the integrated per subset and perstate variants are much more efficient for larger deterministic jump densities.
The pergraph t is typically the fastest algorithm of the nonintegrated versions.
However, in theseexperiments all states in the input are co-accessible by construction; and moreover, allstates in the input are final states.
Therefore, the advantages of the pergraph t'c algorithmcould not be observed here.The turning point is a deterministic jump density of around 0.8: for smaller densi-ties the per graph t is typically slightly faster; for larger densities the per state algorithmis much faster.
For densities beyond 1.5, the per subset alorithm tends to perform bet-ter than the per state algorithm.
Interestingly, this generalization is supported by theexperiments on automata generated by approximation techniques (although the re-sults for randomly generated automata re more consistent than the results for "real"examples).701000010000i i i i iper_graph(t) oper_graph(s) ,per_graph(s,a) \[\]per_graph(t,c) xper subset -~per_state xfsm -1000-y;~ \]oo10 ' ' ' ' '0 0.5 1 1.5 2 2.5#Jumps/#StatesFigure 6Average amount of CPU-time versus jump density for each of the algorithms, and FSM.
Inputautomata have 15 states.
Absolute transition densities: 0.01-0.3.i i t i 4per_graph(t) 0~ per_graph(s ) ,~"~,  per_graph(s,a) \[\]\ ~  per_graph(t,c) ><~'~x per subset -~"N, \ \  per_state%-1000100van Noord Epsilon Moves in Subset Construction10 I I I I I0 0.5 1 1.5 2 2.5#Jumps/#StatesFigure 7Average amount of CPU-time versus jump density for each of the algorithms, and FSM.
Inputautomata have 20 states.
Absolute transition densities: 0.01-0.3.71Computational Linguistics Volume 26, Number 110000010000%`"~ 1000D p~U100per_graph(t) oper graph(s) ,per_graph(s,a) \[\]per_graph(t,c) xper_subset -~per_statefsm -10 i i 1 i i0 0.5 1 1.5 2 2.5#Jumps/#StatesFigure 8Average amount of CPU-time versus deterministic jump density for each of the algorithms,and FSM.
Input automata have 25 states.
Absolute transition densities: 0.01-0.3.100000%-100001000i i i i iper_graph(t) oper__graph(s) ,per_graph(s,a) \[\]per_graph(t,c) ){per_subsetper_state _*100 i i i i i0 0.5 1 1.5 2 2.5#Jumps/#StatesFigure 9Average amount of CPU-time versus deterministic jump density for each of the algorithms,and FSM.
Input automata have 100 states.
Absolute transition densities: 0.001-0.0035.72van Noord Epsilon Moves in Subset ConstructionComparison with the FSM Library.
We also provide the results for AT&T's FSM library.FSM is designed to treat weighted automata for very general weight sets.
The initialimplementation of the library consisted of an on-the-fly computation of the epsilonclosures combined with determinization.
This was abandoned for two reasons: it couldnot be generalized to the case of general weight sets, and it was not outputting theintermediate psilon-removed machine (which might be of interest in itself).
In thecurrent version, c-moves must be removed before determinization is possible.
Thismechanism thus is comparable to our per graph variant.
Apparently, FSM employsan algorithm equivalent to our per graph s,a.
The resulting determinized machines aregenerally larger than the machines produced by our integrated variants and the vari-ants that incorporate c-moves on the target side of transitions.
The timings below areobtained for the pipefsmrmepsilon I fsmdeterminizeThis is somewhat unfair, since this includes the time to write and read the intermediatemachine.
Even so, it is interesting to note that the FSM library is a constant factor fasterthan our per graphS,a; for larger numbers of jumps the per state and per subset variantsconsistently beat the FSM library.Experiment: Automata Generated by Approximation Algorithms.
The automata used in theprevious experiments were randomly generated.
However, it may well be that inpractice the automata that are to be treated by the algorithm have typical propertiesnot reflected in this test data.
For this reason, results are presented for a number ofautomata that were generated using approximation techniques for context-free gram-mars; in particular, for automata created by Nederhof, using the technique describedin Nederhof (1997), and a small number of automata created using the techniqueof Pereira and Wright (1997) (as implemented by Nederhof).
We have restricted ourattention to automata with at least 1,000 states in the input.The automata typically contain lots of jumps.
Moreover, the number of states ofthe resulting automaton is often smaller than the number of states in the input automa-ton.
Results are given in Tables I and 2.
One of the most striking examples is the ygrimautomaton consisting of 3,382 states and 9,124 jumps.
For this example, the per graphimplementations ran out of memory (after a long time), whereas the implementationof the per subset alorithm produced the determinized automaton (containing only 9states) within a single CPU-second.
The FSM implementation took much longer forthis example (whereas for many of the other examples it is faster than our implemen-tations).
Note that this example has the highest ratio of number of jumps to numberof states.
This confirms the observation that the per subset alorithm performs betteron inputs with a high deterministic jump density.5.
Conc lus ionWe have discussed a number of variants of the subset construction algorithm for deter-minizing finite automata containing c-moves.
The experiments support he followingconclusions:The integrated variants per subset and per state work much better forautomata containing a large number of c-moves.
The per subset varianttends to improve upon the per state algorithm if the number of E-movesincreases even further.73Computational Linguistics Volume 26, Number 1Table 1The automata generated by approximation algorithms.
The table lists the number of states,transitions, and jumps of the input automaton, and the number of states of the determinizedmachine using the erred, efree t, and the efree t; variants, respectively.Input OutputId # of States # of Transitions # of Jumps # of Statesper graph s per graph t per graph t;per graph s~ per subsetFSM per stateg14 1,048 403 1,272 137 137 131ovis4.n 1,424 2,210 517 164 133 107g13 1,441 1,006 1,272 337 337 329rene2 1,800 2,597 96 846 844 844ovis9.p 1,868 2,791 2,688 2,478 2,478 1,386ygrim 3,382 5,422 9,124 9 9 9ygrim.p 48,062 63,704 109,296 702 702 702java19 54,369 28,333 51,018 1,971 1,971 1,855java16 64,210 43,935 41,305 3,186 3,186 3,078zovis3 88,156 78,895 68,093 5,174 5,154 4,182zovis2 89,832 80,400 69,377 6,561 6,541 5,309Table 2Results for automata generated by approximation algorithms.
The dashes in thetable indicate that the corresponding algorithm ran out of memory (after a longperiod of time) for that particular example.CPU-time (sec)graph t graph t'c graph s graph s~ subset state FSMg14 0.4 0.4 0.3 0.3 0.4 0.2 0.1ovis4.n 0.9 1.1 0.8 1.0 0.7 0.6 0.6g13 0.9 0.8 0.6 0.6 1.2 0.7 0.2rene2 0.2 0.3 0.2 0.2 0.2 0.2 0.1ovis9.p 36.6 16.0 16.9 17.0 25.2 20.8 .
21.9ygrim - 0.9 21.0 512.1ygrim.p - 562.1 - 4512.4java19 55.5 67.4 52.6 45.0 25.8 19.0 3.8java16 30.0 45.8 35.0 29.9 11.3 12.1 3.0zovis3 741.1 557.5 407.4 358.4 302.5 325.6zovis2 909.2 627.2 496.0 454.4 369.4 392.1?
We have identified four different variants of the per graph algorithm.
Inour experiments, the per graph t is the algorithm of choice for automatacontaining few c-moves, because it is faster than the other algorithms,and because it produces maller automata than the per graph s and pergraph s,a variants.?
The per graph t,c variant is an interesting alternative in that it produces thesmallest results.
This variant should be used if the input automaton isexpected to contain many non-co-accessible states.74van Noord Epsilon Moves in Subset ConstructionAutomata produced by finite-state approximation techniques tend tocontain many c-moves.
We found that for these automata the differencesin speed between the various algorithms can be enormous.
The per subsetand per state algorithms are good candidates for this application.We have attempted to characterize the expected efficiency of the various algorithmsin terms of the number of jumps and the number of states in the input automaton.
Itis quite conceivable that other simple properties of the input automaton can be usedeven more effectively for this purpose.
One reviewer suggests using the number ofstrongly c-connected components (the strongly connected components of the graph ofall c-moves) for this purpose.
We leave this and other possibilities to a future occasion.AcknowledgmentsI am grateful to Mark-Jan Nederhof forsupport, and for providing me with lots of(often dreadful) automata generated by hisfinite-state approximation tools.
Thecomments of the anonymous FSMNLP andCL reviewers were extremely useful.ReferencesAho, Alfred V., Ravi Sethi, and Jeffrey D.Ullman.
1986.
Compilers.
Principles,Techniques and Tools.
Addison Wesley.Black, Alan W. 1989.
Finite state machinesfrom feature grammars.
In InternationalWorkshop on Parsing Technologies, pages277-285, Pittsburgh, PA.Chomsky, Noam.
1963.
Formal properties ofgrammars.
In R. Duncan Luce, Robert R.Bush, and Eugene Galanter, editors,Handbook of Mathematical Psychology;Volume II.
John Wiley, pages 323-418.Chomsky, Noam.
1964.
On the notion 'ruleof grammar.'
In Jerry E. Fodor andJerrold J. Katz, editors, The Structure ofLanguage; Readings in the Philosophy ofLanguage.
Prentice Hall, pages 119-136.Cormen, Thomas H., Charles E. Leiserson,and Ronald L. Rivest.
1990.
Introduction toAlgorithms.
MIT Press, Cambridge, MA.Gerdemann, Dale and Gertjan van Noord.1999.
Transducers from rewrite rules withbackreferences.
In Ninth Conference oftheEuropean Chapter o/the Association forComputational Linguistics, Bergen, Norway.Grimley-Evans, Edmund.
1997.Approximating context-free grammarswith a finite-state calculus.
In Proceedingsof the 35th Annual Meeting of the Associationfor Computational Linguistics and 8thConference ofthe European Chapter o/theAssociation for Computational Linguistics,pages 452--459, Madrid, Spain.Hopcroft, John E. and Jeffrey D. Ullman.1979.
Introduction to Automata Theory,Languages, and Computation.Addison-Wesley, Reading, MA.Johnson, J. Howard and Derick Wood.
1997.Instruction computation i subsetconstruction.
In Darrell Raymond, DerickWood, and Sheng Yu, editors, AutomataImplementation.
Springer Verlag, pages64-71.
Lecture Notes in Computer Science1260.Johnson, Mark.
1998.
Finite-stateapproximation of constraint-basedgrammars using left-comer grammartransforms.
In COLING-ACL '98: 36thAnnual Meeting of the Association forComputational Linguistics and 17thInternational Conference on ComputationalLinguistics.
Proceedings ofthe Conference,pages 619-623, Montreal, Quebec,Canada.Leslie, Ted.
1995.
Efficient approaches tosubset construction.
Master's thesis,Computer Science, University ofWaterloo.Miller, George and Noam Chomsky.
1963.Finitary models of language users.
InR.
Luce, R. Bush, and E. Galanter, editors,Handbook o/Mathematical Psychology.Volume 2.
John Wiley, pages 419-491.Mohri, Mehryar, Fernando C. N. Pereira,and Michael Riley.
1998.
A rational designfor a weighted finite-state transducerlibrary.
In Derick Wood and Sheng Yu,editors, Automata Implementation.
LectureNotes in Computer Science, Number 1436.Springer Verlag, pages 144-158.Nederhof, Mark-Jan. 1997.
Regularapproximations of CFLs: A grammaticalview.
In Proceedings ofthe InternationalWorkshop on Parsing Technologies,pages 159-170, Massachusetts Institute ofTechnology.Nederhof, Mark-Jan. 1998.
Context-freeparsing through regular approximation.In Proceedings ofthe International Workshopon Finite-state Methods in Natural LanguageProcessing, pages 13-24, Ankara, Turkey.75Computational Linguistics Volume 26, Number 1O'Keefe, Richard A.
1990.
The Craft of Prolog.MIT Press, Cambridge, MA.Pereira, Fernando C. N. and Rebecca N.Wright.
1991.
Finite-state approximationof phrase structure grammars.
InProceedings ofthe 29th Annual Meeting,pages 246-255, Berkeley.
Association forComputational Linguistics.Pereira, Fernando C. N. and Rebecca N.Wright.
1997.
Finite-state approximationof phrase-structure grammars.
InEmmanuel Roche and Yves Schabes,editors, Finite-State Language Processing.MIT Press, Cambridge, MA, pages149-173.Rood, C. M. 1996.
Efficient finite-stateapproximation of context free grammars.In A. Kornai, editor, Extended Finite StateModels of Language, Proceedings of theECAI'96 workshop, pages 58-64,Budapest University of EconomicSciences, Hungary.van Noord, Gertjan.
1997.
FSA Utilities: Atoolbox to manipulate finite-stateautomata.
In Darrell Raymond, DerickWood, and Sheng Yu, editors, AutomataImplementation.
Lecture Notes inComputer Science, Number 1260.Springer Verlag, pages 87-108.van Noord, Gertjan.
1998.
The treatment ofepsilon moves in subset construction.
InProceedings ofthe International Workshop onFinite-state Methods in Natural LanguageProcessing, pages 57-68, Ankara, Turkey.cmp-lg/9804003.van Noord, Gertjan.
1999.
FSA6 referencemanual.
The FSA Utilities toolbox isavailable free of charge under GnuGeneral Public License athttp://www.let.rug.nl/~vannoord/Fsa/.van Noord, Gertjan and Dale Gerdemann.1999.
An extendible regular expressioncompiler for finite-state approaches innatural anguage processing.
In O. Boldt,H.
Juergensen, and L. Robbins, editors,Workshop on Implementing Automata; WIA99Pre-Proceedings, pages XW-1-XIV-15,Potsdam, Germany.76
