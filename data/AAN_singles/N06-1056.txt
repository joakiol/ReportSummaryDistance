Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 439?446,New York, June 2006. c?2006 Association for Computational LinguisticsLearning for Semantic Parsing with Statistical Machine TranslationYuk Wah Wong and Raymond J. MooneyDepartment of Computer SciencesThe University of Texas at Austin1 University Station C0500Austin, TX 78712-0233, USA{ywwong,mooney}@cs.utexas.eduAbstractWe present a novel statistical approach tosemantic parsing, WASP, for construct-ing a complete, formal meaning represen-tation of a sentence.
A semantic parseris learned given a set of sentences anno-tated with their correct meaning represen-tations.
The main innovation of WASPis its use of state-of-the-art statistical ma-chine translation techniques.
A wordalignment model is used for lexical acqui-sition, and the parsing model itself can beseen as a syntax-based translation model.We show that WASP performs favorablyin terms of both accuracy and coveragecompared to existing learning methods re-quiring similar amount of supervision, andshows better robustness to variations intask complexity and word order.1 IntroductionRecent work on natural language understanding hasmainly focused on shallow semantic analysis, suchas semantic role labeling and word-sense disam-biguation.
This paper considers a more ambi-tious task of semantic parsing, which is the con-struction of a complete, formal, symbolic, mean-ing representation (MR) of a sentence.
Seman-tic parsing has found its way in practical applica-tions such as natural-language (NL) interfaces todatabases (Androutsopoulos et al, 1995) and ad-vice taking (Kuhlmann et al, 2004).
Figure 1 showsa sample MR written in a meaning-representationlanguage (MRL) called CLANG, which is used for((bowner our {4})(do our {6} (pos (left (half our)))))If our player 4 has the ball, then our player 6 shouldstay in the left side of our half.Figure 1: A meaning representation in CLANGencoding coach advice given to simulated soccer-playing agents (Kuhlmann et al, 2004).Prior research in semantic parsing has mainly fo-cused on relatively simple domains such as ATIS(Air Travel Information Service) (Miller et al, 1996;Papineni et al, 1997; Macherey et al, 2001), inwhich a typcial MR is only a single semantic frame.Learning methods have been devised that can gen-erate MRs with a complex, nested structure (cf.Figure 1).
However, these methods are mostlybased on deterministic parsing (Zelle and Mooney,1996; Kate et al, 2005), which lack the robustnessthat characterizes recent advances in statistical NLP.Other learning methods involve the use of fully-annotated augmented parse trees (Ge and Mooney,2005) or prior knowledge of the NL syntax (Zettle-moyer and Collins, 2005) in training, and hence re-quire extensive human efforts when porting to a newdomain or language.In this paper, we present a novel statistical ap-proach to semantic parsing which can handle MRswith a nested structure, based on previous work onsemantic parsing using transformation rules (Kate etal., 2005).
The algorithm learns a semantic parsergiven a set of NL sentences annotated with theircorrect MRs.
It requires no prior knowledge ofthe NL syntax, although it assumes that an unam-biguous, context-free grammar (CFG) of the targetMRL is available.
The main innovation of this al-439answer(count(city(loc 2(countryid(usa)))))How many cities are there in the US?Figure 2: A meaning representation in GEOQUERYgorithm is its integration with state-of-the-art statis-tical machine translation techniques.
More specif-ically, a statistical word alignment model (Brownet al, 1993) is used to acquire a bilingual lexi-con consisting of NL substrings coupled with theirtranslations in the target MRL.
Complete MRs arethen formed by combining these NL substrings andtheir translations under a parsing framework calledthe synchronous CFG (Aho and Ullman, 1972),which forms the basis of most existing statisti-cal syntax-based translation models (Yamada andKnight, 2001; Chiang, 2005).
Our algorithm iscalled WASP, short for Word Alignment-based Se-mantic Parsing.
In initial evaluation on severalreal-world data sets, we show that WASP performsfavorably in terms of both accuracy and coveragecompared to existing learning methods requiring thesame amount of supervision, and shows better ro-bustness to variations in task complexity and wordorder.Section 2 provides a brief overview of the do-mains being considered.
In Section 3, we presentthe semantic parsing model of WASP.
Section 4 out-lines the algorithm for acquiring a bilingual lexiconthrough the use of word alignments.
Section 5 de-scribes a probabilistic model for semantic parsing.Finally, we report on experiments that show the ro-bustness of WASP in Section 6, followed by the con-clusion in Section 7.2 Application DomainsIn this paper, we consider two domains.
The first do-main is ROBOCUP.
ROBOCUP (www.robocup.org)is an AI research initiative using robotic soccer as itsprimary domain.
In the ROBOCUP Coach Competi-tion, teams of agents compete on a simulated soccerfield and receive coach advice written in a formallanguage called CLANG (Chen et al, 2003).
Fig-ure 1 shows a sample MR in CLANG.The second domain is GEOQUERY, where a func-tional, variable-free query language is used forquerying a small database on U.S. geography (Zelleand Mooney, 1996; Kate et al, 2005).
Figure 2shows a sample query in this language.
Note thatboth domains involve the use of MRs with a com-plex, nested structure.3 The Semantic Parsing ModelTo describe the semantic parsing model of WASP,it is best to start with an example.
Consider thetask of translating the sentence in Figure 1 into itsMR in CLANG.
To achieve this task, we may firstanalyze the syntactic structure of the sentence us-ing a semantic grammar (Allen, 1995), whose non-terminals are the ones in the CLANG grammar.
Themeaning of the sentence is then obtained by com-bining the meanings of its sub-parts according tothe semantic parse.
Figure 3(a) shows a possiblepartial semantic parse of the sample sentence basedon CLANG non-terminals (UNUM stands for uni-form number).
Figure 3(b) shows the correspondingCLANG parse from which the MR is constructed.This process can be formalized as an instance ofsynchronous parsing (Aho and Ullman, 1972), orig-inally developed as a theory of compilers in whichsyntax analysis and code generation are combinedinto a single phase.
Synchronous parsing has seen asurge of interest recently in the machine translationcommunity as a way of formalizing syntax-basedtranslation models (Melamed, 2004; Chiang, 2005).According to this theory, a semantic parser defines atranslation, a set of pairs of strings in which eachpair is an NL sentence coupled with its MR. Tofinitely specify a potentially infinite translation, weuse a synchronous context-free grammar (SCFG) forgenerating the pairs in a translation.
Analogous toan ordinary CFG, each SCFG rule consists of a sin-gle non-terminal on the left-hand side (LHS).
Theright-hand side (RHS) of an SCFG rule is a pair ofstrings, ?
?, ?
?, where the non-terminals in ?
are apermutation of the non-terminals in ?.
Below aresome SCFG rules that can be used for generating theparse trees in Figure 3:RULE ?
?if CONDITION 1 , DIRECTIVE 2 .
,(CONDITION 1 DIRECTIVE 2 )?CONDITION ?
?TEAM 1 player UNUM 2 has the ball ,(bowner TEAM 1 {UNUM 2 })?TEAM ?
?our , our?UNUM ?
?4 , 4?440RULEIf CONDITIONTEAMourplayer UNUM4has the ball...(a) EnglishRULE( CONDITION(bowner TEAMour{ UNUM4})...)(b) CLANGFigure 3: Partial parse trees for the CLANG statement and its English gloss shown in Figure 1Each SCFG rule X ?
?
?, ??
is a combination of aproduction of the NL semantic grammar, X ?
?,and a production of the MRL grammar, X ?
?.Each rule corresponds to a transformation rule inKate et al (2005).
Following their terminology,we call the string ?
a pattern, and the string ?
atemplate.
Non-terminals are indexed to show theirassociation between a pattern and a template.
Allderivations start with a pair of associated start sym-bols, ?S 1 , S 1 ?.
Each step of a derivation involvesthe rewriting of a pair of associated non-terminalsin both of the NL and MRL streams.
Below is aderivation that would generate the sample sentenceand its MR simultaneously: (Note that RULE is thestart symbol for CLANG)?RULE 1 , RULE 1 ??
?if CONDITION 1 , DIRECTIVE 2 .
,(CONDITION 1 DIRECTIVE 2 )??
?if TEAM 1 player UNUM 2 has the ball, DIR 3 .
,((bowner TEAM 1 {UNUM 2 }) DIR 3 )??
?if our player UNUM 1 has the ball, DIR 2 .
,((bowner our {UNUM 1 }) DIR 2 )??
?if our player 4 has the ball, DIRECTIVE 1 .
,((bowner our {4}) DIRECTIVE 1 )??
...?
?if our player 4 has the ball, then our player 6should stay in the left side of our half.
,((bowner our {4})(do our {6} (pos (left (half our)))))?Here the MR string is said to be a translation of theNL string.
Given an input sentence, e, the task ofsemantic parsing is to find a derivation that yields?e, f?, so that f is a translation of e. Since there maybe multiple derivations that yield e (and thus mul-tiple possible translations of e), a mechanism mustbe devised for discriminating the correct derivationfrom the incorrect ones.The semantic parsing model of WASP thus con-sists of an SCFG, G, and a probabilistic model, pa-rameterized by ?, that takes a possible derivation, d,and returns its likelihood of being correct given aninput sentence, e. The output translation, f?, for asentence, e, is defined as:f?
= m(arg maxd?D(G|e)Pr?
(d|e))(1)where m(d) is the MR string that a derivation dyields, and D(G|e) is the set of all possible deriva-tions of G that yield e. In other words, the outputMR is the yield of the most probable derivation thatyields e in the NL stream.The learning task is to induce a set of SCFG rules,which we call a lexicon, and a probabilistic modelfor derivations.
A lexicon defines the set of deriva-tions that are possible, so the induction of a proba-bilistic model first requires a lexicon.
Therefore, thelearning task can be separated into two sub-tasks:(1) the induction of a lexicon, followed by (2) theinduction of a probabilistic model.
Both sub-tasksrequire a training set, {?ei, fi?
}, where each trainingexample ?ei, fi?
is an NL sentence, ei, paired withits correct MR, fi.
Lexical induction also requiresan unambiguous CFG of the MRL.
Since there is nolexicon to begin with, it is not possible to includecorrect derivations in the training data.
This is un-like most recent work on syntactic parsing based ongold-standard treebanks.
Therefore, the induction ofa probabilistic model for derivations is done in anunsupervised manner.4 Lexical AcquisitionIn this section, we focus on lexical learning, whichis done by finding optimal word alignments between441RULE ?
(CONDITION DIRECTIVE)TEAM ?
ourUNUM ?
4Ifourplayer4hastheballCONDITION ?
(bowner TEAM {UNUM})Figure 4: Partial word alignment for the CLANG statement and its English gloss shown in Figure 1NL sentences and their MRs in the training set.
Bydefining a mapping of words from one language toanother, word alignments define a bilingual lexicon.Using word alignments to induce a lexicon is not anew idea (Och and Ney, 2003).
Indeed, attemptshave been made to directly apply machine transla-tion systems to the problem of semantic parsing (Pa-pineni et al, 1997; Macherey et al, 2001).
However,these systems make no use of the MRL grammar,thus allocating probability mass to MR translationsthat are not even syntactically well-formed.
Here wepresent a lexical induction algorithm that guaranteessyntactic well-formedness of MR translations by us-ing the MRL grammar.The basic idea is to train a statistical word align-ment model on the training set, and then form alexicon by extracting transformation rules from theK = 10 most probable word alignments betweenthe training sentences and their MRs.
While NLwords could be directly aligned with MR tokens,this is a bad approach for two reasons.
First, not allMR tokens carry specific meanings.
For example, inCLANG, parentheses and braces are delimiters thatare semantically vacuous.
Such tokens are not sup-posed to be aligned with any words, and inclusion ofthese tokens in the training data is likely to confusethe word alignment model.
Second, MR tokens mayexhibit polysemy.
For instance, the CLANG pred-icate pt has three meanings based on the types ofarguments it is given: it specifies the xy-coordinates(e.g.
(pt 0 0)), the current position of the ball (i.e.
(pt ball)), or the current position of a player (e.g.
(pt our 4)).
Judging from the pt token alone, theword alignment model would not be able to identifyits exact meaning.A simple, principled way to avoid these diffi-culties is to represent an MR using a sequence ofproductions used to generate it.
Specifically, thesequence corresponds to the top-down, left-mostderivation of an MR.
Figure 4 shows a partial wordalignment between the sample sentence and the lin-earized parse of its MR.
Here the second produc-tion, CONDITION ?
(bowner TEAM {UNUM}), isthe one that rewrites the CONDITION non-terminalin the first production, RULE ?
(CONDITION DI-RECTIVE), and so on.
Note that the structure of aparse tree is preserved through linearization, and foreach MR there is a unique linearized parse, since theMRL grammar is unambiguous.
Such alignmentscan be obtained through the use of any off-the-shelfword alignment model.
In this work, we use theGIZA++ implementation (Och and Ney, 2003) ofIBM Model 5 (Brown et al, 1993).Assuming that each NL word is linked to at mostone MRL production, transformation rules are ex-tracted in a bottom-up manner.
The process startswith productions whose RHS is all terminals, e.g.TEAM ?
our and UNUM ?
4.
For each of theseproductions, X ?
?, a rule X ?
?
?, ??
is ex-tracted such that ?
consists of the words to whichthe production is linked, e.g.
TEAM ?
?our, our?,UNUM ?
?4, 4?.
Then we consider productionswhose RHS contains non-terminals, i.e.
predicateswith arguments.
In this case, an extracted patternconsists of the words to which the production islinked, as well as non-terminals showing where thearguments are realized.
For example, for the bownerpredicate, the extracted rule would be CONDITION?
?TEAM 1 player UNUM 2 has (1) ball, (bownerTEAM 1 {UNUM 2 })?, where (1) denotes a wordgap of size 1, due to the unaligned word the thatcomes between has and ball.
A word gap, (g), canbe seen as a non-terminal that expands to at mostg words in the NL stream, which allows for someflexibility in pattern matching.
Rule extraction thusproceeds backward from the end of a linearized MR442ourleftpenaltyareaREGION ?
(left REGION)REGION ?
(penalty-area TEAM)TEAM ?
ourFigure 5: A word alignment from which no rules can be extracted for the penalty-area predicateparse (so that a predicate is processed only after itsarguments have all been processed), until rules areextracted for all productions.There are two cases where the above algorithmwould not extract any rules for a production r. Firstis when no descendants of r in the MR parse arelinked to any words.
Second is when there is alink from a word w, covered by the pattern for r,to a production r?
outside the sub-parse rooted atr.
Rule extraction is forbidden in this case be-cause it would destroy the link between w and r?.The first case arises when a component of an MRis not realized, e.g.
assumed in context.
The sec-ond case arises when a predicate and its argumentsare not realized close enough.
Figure 5 shows anexample of this, where no rules can be extractedfor the penalty-area predicate.
Both cases can besolved by merging nodes in the MR parse tree, com-bining several productions into one.
For example,since no rules can be extracted for penalty-area,it is combined with its parent to form REGION ?
(left (penalty-area TEAM)), for which the pat-tern TEAM left penalty area is extracted.The above algorithm is effective only when wordslinked to an MR predicate and its arguments stayclose to each other, a property that we call phrasalcoherence.
Any links that destroy this propertywould lead to excessive node merging, a major causeof overfitting.
Since building a model that strictlyobserves phrasal coherence often requires rules thatmodel the reordering of tree nodes, our goal is tobootstrap the learning process by using a simpler,word-based alignment model that produces a gen-erally coherent alignment, and then remove linksthat would cause excessive node merging before ruleextraction takes place.
Given an alignment, a, wecount the number of links that would prevent a rulefrom being extracted for each production in the MRparse.
Then the total sum for all productions is ob-tained, denoted by v(a).
A greedy procedure is em-ployed that repeatedly removes a link a ?
a thatwould maximize v(a) ?
v(a\{a}) > 0, until v(a)cannot be further reduced.
A link w ?
r is neverremoved if the translation probability, Pr(r|w), isgreater than a certain threshold (0.9).
To replenishthe removed links, links from the most probable re-verse alignment, a?
(obtained by treating the sourcelanguage as target, and vice versa), are added to a, aslong as a remains n-to-1, and v(a) is not increased.5 Parameter EstimationOnce a lexicon is acquired, the next task is to learn aprobabilistic model for the semantic parser.
We pro-pose a maximum-entropy model that defines a con-ditional probability distribution over derivations (d)given the observed NL string (e):Pr?
(d|e) =1Z?
(e)exp?i?ifi(d) (2)where fi is a feature function, and Z?
(e) is a nor-malizing factor.
For each rule r in the lexicon thereis a feature function that returns the number of timesr is used in a derivation.
Also for each word w thereis a feature function that returns the number of timesw is generated from word gaps.
Generation of un-seen words is modeled using an extra feature whosevalue is the total number of words generated fromword gaps.
The number of features is quite modest(less than 3,000 in our experiments).
A similar fea-ture set is used by Zettlemoyer and Collins (2005).Decoding of the model can be done in cubic timewith respect to sentence length using the Viterbi al-gorithm.
An Earley chart is used for keeping trackof all derivations that are consistent with the in-put (Stolcke, 1995).
The maximum conditional like-lihood criterion is used for estimating the model pa-rameters, ?i.
A Gaussian prior (?2 = 1) is used forregularizing the model (Chen and Rosenfeld, 1999).Since gold-standard derivations are not available inthe training data, correct derivations must be treatedas hidden variables.
Here we use a version of im-443proved iterative scaling (IIS) coupled with EM (Rie-zler et al, 2000) for finding an optimal set of param-eters.1 Unlike the fully-supervised case, the condi-tional likelihood is not concave with respect to ?,so the estimation algorithm is sensitive to initial pa-rameters.
To assume as little as possible, ?
is initial-ized to 0.
The estimation algorithm requires statis-tics that depend on all possible derivations for a sen-tence or a sentence-MR pair.
While it is not fea-sible to enumerate all derivations, a variant of theInside-Outside algorithm can be used for efficientlycollecting the required statistics (Miyao and Tsujii,2002).
Following Zettlemoyer and Collins (2005),only rules that are used in the best parses for thetraining set are retained in the final lexicon.
Allother rules are discarded.
This heuristic, commonlyknown as Viterbi approximation, is used to improveaccuracy, assuming that rules used in the best parsesare the most accurate.6 ExperimentsWe evaluated WASP in the ROBOCUP and GEO-QUERY domains (see Section 2).
To build a cor-pus for ROBOCUP, 300 pieces of coach advice wererandomly selected from the log files of the 2003ROBOCUP Coach Competition, which were manu-ally translated into English (Kuhlmann et al, 2004).The average sentence length is 22.52.
To build acorpus for GEOQUERY, 880 English questions weregathered from various sources, which were manu-ally translated into the functional GEOQUERY lan-guage (Tang and Mooney, 2001).
The average sen-tence length is 7.48, much shorter than ROBOCUP.250 of the queries were also translated into Spanish,Japanese and Turkish, resulting in a smaller, multi-lingual data set.For each domain, there was a minimal set of ini-tial rules representing knowledge needed for trans-lating basic domain entities.
These rules were al-ways included in a lexicon.
For example, in GEO-QUERY, the initial rules were: NUM ?
?x, x?, forall x ?
R; CITY ?
?c, cityid(?c?, )?, for allcity names c (e.g.
new york); and similar rules forother types of names (e.g.
rivers).
Name transla-tions were provided for the multilingual data set (e.g.1We also implemented limited-memory BFGS (Nocedal,1980).
Preliminary experiments showed that it typically reducestraining time by more than half with similar accuracy.CITY ?
?nyuu yooku, cityid(?new york?, )?
forJapanese).Standard 10-fold cross validation was used in ourexperiments.
A semantic parser was learned fromthe training set.
Then the learned parser was usedto translate the test sentences into MRs. Translationfailed when there were constructs that the parser didnot cover.
We counted the number of sentences thatwere translated into anMR, and the number of trans-lations that were correct.
For ROBOCUP, a trans-lation was correct if it exactly matched the correctMR.
For GEOQUERY, a translation was correct if itretrieved the same answer as the correct query.
Us-ing these counts, we measured the performance ofthe parser in terms of precision (percentage of trans-lations that were correct) and recall (percentage oftest sentences that were correctly translated).
ForROBOCUP, it took 47 minutes to learn a parser us-ing IIS.
For GEOQUERY, it took 83 minutes.Figure 6 shows the performance of WASP com-pared to four other algorithms: SILT (Kate et al,2005), COCKTAIL (Tang and Mooney, 2001), SCIS-SOR (Ge and Mooney, 2005) and Zettlemoyer andCollins (2005).
Experimental results clearly showthe advantage of extra supervision in SCISSOR andZettlemoyer and Collins?s parser (see Section 1).However, WASP performs quite favorably comparedto SILT and COCKTAIL, which use the same train-ing data.
In particular, COCKTAIL, a determinis-tic shift-reduce parser based on inductive logic pro-gramming, fails to scale up to the ROBOCUP do-main where sentences are much longer, and crasheson larger training sets due to memory overflow.WASP also outperforms SILT in terms of recall,where lexical learning is done by a local bottom-upsearch, which is much less effective than the word-alignment-based algorithm in WASP.Figure 7 shows the performance of WASP onthe multilingual GEOQUERY data set.
The lan-guages being considered differ in terms of word or-der: Subject-Verb-Object for English and Spanish,and Subject-Object-Verb for Japanese and Turkish.WASP?s performance is consistent across these lan-guages despite some slight differences, most proba-bly due to factors other than word order (e.g.
lowerrecall for Turkish due to a much larger vocabulary).Details can be found in a longer version of this pa-per (Wong, 2005).4440204060801000  50  100  150  200  250  300Precision(%)Number of training examplesWASPSILTCOCKTAILSCISSOR(a) Precision for ROBOCUP0204060801000  50  100  150  200  250  300Recall (%)Number of training examplesWASPSILTCOCKTAILSCISSOR(b) Recall for ROBOCUP0204060801000  100  200  300  400  500  600  700  800Precision(%)Number of training examplesWASPSILTCOCKTAILSCISSORZettlemoyer et al (2005)(c) Precision for GEOQUERY0204060801000  100  200  300  400  500  600  700  800Recall (%)Number of training examplesWASPSILTCOCKTAILSCISSORZettlemoyer et al (2005)(d) Recall for GEOQUERYFigure 6: Precision and recall learning curves comparing various semantic parsers0204060801000  50  100  150  200  250Precision(%)Number of training examplesEnglishSpanishJapaneseTurkish(a) Precision for GEOQUERY0204060801000  50  100  150  200  250Recall (%)Number of training examplesEnglishSpanishJapaneseTurkish(b) Recall for GEOQUERYFigure 7: Precision and recall learning curves comparing various natural languages7 ConclusionWe have presented a novel statistical approach tosemantic parsing in which a word-based alignmentmodel is used for lexical learning, and the parsingmodel itself can be seen as a syntax-based trans-lation model.
Our method is like many phrase-based translation models, which require a simpler,word-based alignment model for the acquisition of aphrasal lexicon (Och and Ney, 2003).
It is also sim-ilar to the hierarchical phrase-based model of Chi-ang (2005), in which hierarchical phrase pairs, es-sentially SCFG rules, are learned through the use ofa simpler, phrase-based alignment model.
Our workshows that ideas from compiler theory (SCFG) andmachine translation (word alignment models) can besuccessfully applied to semantic parsing, a closely-related task whose goal is to translate a natural lan-guage into a formal language.Lexical learning requires word alignments that arephrasally coherent.
We presented a simple greedyalgorithm for removing links that destroy phrasal co-herence.
Although it is shown to be quite effective inthe current domains, it is preferable to have a moreprincipled way of promoting phrasal coherence.
Theproblem is that, by treating MRL productions asatomic units, current word-based alignment modelshave no knowledge about the tree structure hiddenin a linearized MR parse.
In the future, we wouldlike to develop a word-based alignment model that445is aware of the MRL syntax, so that better lexiconscan be learned.AcknowledgmentsThis research was supported by Defense AdvancedResearch Projects Agency under grant HR0011-04-1-0007.ReferencesA.
V. Aho and J. D. Ullman.
1972.
The Theory of Pars-ing, Translation, and Compiling.
Prentice Hall, Engle-wood Cliffs, NJ.J.
F. Allen.
1995.
Natural Language Understanding (2ndEd.).
Benjamin/Cummings, Menlo Park, CA.I.
Androutsopoulos, G. D. Ritchie, and P. Thanisch.1995.
Natural language interfaces to databases: Anintroduction.
Journal of Natural Language Engineer-ing, 1(1):29?81.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?312, June.S.
Chen and R. Rosenfeld.
1999.
A Gaussian prior forsmoothing maximum entropy models.
Technical re-port, Carnegie Mellon University, Pittsburgh, PA.M.
Chen et al 2003.
Users manual: RoboCup soc-cer server manual for soccer server version 7.07 andlater.
Available at http://sourceforge.net/projects/sserver/.D.
Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
of ACL-05,pages 263?270, Ann Arbor, MI, June.R.
Ge and R. J. Mooney.
2005.
A statistical semanticparser that integrates syntax and semantics.
In Proc.of CoNLL-05, pages 9?16, Ann Arbor, MI, July.R.
J. Kate, Y. W. Wong, and R. J. Mooney.
2005.
Learn-ing to transform natural to formal languages.
In Proc.of AAAI-05, pages 1062?1068, Pittsburgh, PA, July.G.
Kuhlmann, P. Stone, R. J. Mooney, and J. W. Shavlik.2004.
Guiding a reinforcement learner with naturallanguage advice: Initial results in RoboCup soccer.
InProc.
of the AAAI-04 Workshop on Supervisory Con-trol of Learning and Adaptive Systems, San Jose, CA,July.K.
Macherey, F. J. Och, and H. Ney.
2001.
Natural lan-guage understanding using statistical machine transla-tion.
In Proc.
of EuroSpeech-01, pages 2205?2208,Aalborg, Denmark.I.
D. Melamed.
2004.
Statistical machine translationby parsing.
In Proc.
of ACL-04, pages 653?660,Barcelona, Spain.S.
Miller, D. Stallard, R. Bobrow, and R. Schwartz.
1996.A fully statistical approach to natural language inter-faces.
In Proc.
of ACL-96, pages 55?61, Santa Cruz,CA.Y.
Miyao and J. Tsujii.
2002.
Maximum entropy estima-tion for feature forests.
In Proc.
of HLT-02, San Diego,CA, March.J.
Nocedal.
1980.
Updating quasi-Newton matriceswith limited storage.
Mathematics of Computation,35(151):773?782, July.F.
J. Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.K.
A. Papineni, S. Roukos, and R. T. Ward.
1997.Feature-based language understanding.
In Proc.
ofEuroSpeech-97, pages 1435?1438, Rhodes, Greece.S.
Riezler, D. Prescher, J. Kuhn, and M. Johnson.
2000.Lexicalized stochastic modeling of constraint-basedgrammars using log-linear measures and EM training.In Proc.
of ACL-00, pages 480?487, Hong Kong.A.
Stolcke.
1995.
An efficient probabilistic context-freeparsing algorithm that computes prefix probabilities.Computational Linguistics, 21(2):165?201.L.
R. Tang and R. J. Mooney.
2001.
Using multipleclause constructors in inductive logic programming forsemantic parsing.
In Proc.
of ECML-01, pages 466?477, Freiburg, Germany.Y.
W. Wong.
2005.
Learning for semantic parsing us-ing statistical machine translation techniques.
Techni-cal Report UT-AI-05-323, Artificial Intelligence Lab,University of Texas at Austin, Austin, TX, October.K.
Yamada and K. Knight.
2001.
A syntax-based sta-tistical translation model.
In Proc.
of ACL-01, pages523?530, Toulouse, France.J.
M. Zelle and R. J. Mooney.
1996.
Learning to parsedatabase queries using inductive logic programming.In Proc.
of AAAI-96, pages 1050?1055, Portland, OR,August.L.
S. Zettlemoyer and M. Collins.
2005.
Learning tomap sentences to logical form: Structured classifica-tion with probabilistic categorial grammars.
In Proc.of UAI-05, Edinburgh, Scotland, July.446
