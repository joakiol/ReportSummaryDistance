Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 791?802,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsJoint Learning and Inference for Grammatical Error CorrectionAlla Rozovskaya and Dan RothCognitive Computation GroupUniversity of Illinois at Urbana-Champaign201 N. Goodwin AvenueUrbana, IL 61801{rozovska,danr}@illinois.eduAbstractState-of-the-art systems for grammatical er-ror correction are based on a collection ofindependently-trained models for specific er-rors.
Such models ignore linguistic interac-tions at the sentence level and thus do poorlyon mistakes that involve grammatical depen-dencies among several words.
In this paper,we identify linguistic structures with interact-ing grammatical properties and propose to ad-dress such dependencies via joint inferenceand joint learning.We show that it is possible to identify interac-tions well enough to facilitate a joint approachand, consequently, that joint methods correctincoherent predictions that independently-trained classifiers tend to produce.
Further-more, because the joint learning model con-siders interacting phenomena during training,it is able to identify mistakes that require mak-ing multiple changes simultaneously and thatstandard approaches miss.
Overall, our modelsignificantly outperforms the Illinois systemthat placed first in the CoNLL-2013 sharedtask on grammatical error correction.1 IntroductionThere has recently been a lot of work addressing er-rors made by English as a Second Language (ESL)learners.
In the past two years, three competitionsdevoted to grammatical error correction for non-native writers took place: HOO-2011 (Dale and Kil-garriff, 2011), HOO-2012 (Dale et al 2012), andthe CoNLL-2013 shared task (Ng et al 2013).Nowadays *phone/phones *has/have manyfunctionalities, *included/including *?/acamera and *?/a Wi-Fi receiver.Figure 1: Examples of representative ESL errors.Most of the work in the area of ESL error cor-rection has addressed the task by building statisticalmodels that specialize in correcting a specific typeof a mistake.
Figure 1 illustrates several types oferrors common among non-native speakers of En-glish: article, subject-verb agreement, noun num-ber, and verb form.
A significant proportion of re-search has focused on correcting mistakes in articleand preposition usage (Izumi et al 2003; Han etal., 2006; Felice and Pulman, 2008; Gamon et al2008; Tetreault and Chodorow, 2008; Gamon, 2010;Rozovskaya and Roth, 2010b).
Several studies alsoconsider verb-related and noun-related errors (Leeand Seneff, 2008; Gamon et al 2008; Dahlmeierand Ng, 2012).
The predictions made by individualmodels are then applied independently (Rozovskayaet al 2011) or pipelined (Dahlmeier and Ng, 2012).The standard approach of training individual clas-sifiers considers each word independently and thusassumes that there are no interactions between er-rors and between grammatical phenomena.
But anESL writer may make multiple mistakes in a singlesentence and these result in misleading local cuesgiven to individual classifiers.
In the example shownin Figure 1, the agreement error on the verb ?have?interacts with the noun number error: a correctionsystem that takes into account the context may in-fer, because of the word ?phone?, that the verb num-ber is correct.
For this reason, a system that consid-791ers noun and agreement errors separately will fail toidentify and correct the interacting errors shown inFig.
1.
Furthermore, it may also produce inconsis-tent predictions.Even though it is quite clear that grammatical er-rors interact, for various conceptual and technicalreasons, this issue has not been addressed in a sig-nificant way in the literature.
We believe that thereasons for that are three-fold: (1) Data: until veryrecently we did not have data that jointly annotatessufficiently many errors of interacting phenomena(see Sec.
2).
(2) Conceptual: Correcting errors ininteracting linguistic phenomena requires that oneidentifies those phenomena and, more importantly,can recognize reliably the interacting components(e.g., given a verb, identify the subject to enable en-forcing agreement).
The perception has been thatthis cannot be done reliably (Sec.
4).
(3) Technical:The NLP community has started to better understandjoint learning and inference and apply it to variousphenomena (Roth and Yih, 2004; Punyakanok et al2008; Martins et al 2011; Clarke and Lapata, 2007;Sutton and McCallum, 2007) (Sec.
5).In this paper we present, for the first time, a suc-cessful approach to jointly resolving grammatical er-rors.
Specifically:?
We identify two pairs of interacting phenomena,subject-verb and article-NPhead agreements; weshow how to reliably identify these pairs in noisyESL data, thereby facilitating the joint correction ofthese phenomena.
?We propose two joint approaches: (1) a joint infer-ence approach implemented on top of individuallylearned models using an integer linear programmingformulation (ILP, (Roth and Yih, 2004)), and (2) amodel that jointly learns each pair of these phenom-ena.
We show that each of these methods has its ad-vantages, and that both solve the two challenges out-lined above: the joint models exclude inconsistentpredictions that violate linguistic constraints.
Thejoint learning model exhibits superior performance,as it is also able to overcome the problem of thenoisy context encountered by the individual mod-els and to identify errors in contexts, where multiplechanges need to be applied at the same time.We show that our joint models produce state-of-the-art performance and, in particular, significantlyoutperform the University of Illinois system thatplaced first in the CoNLL-2013 shared task, increas-ing the F1 score by 2 and 4 points in different evalu-ation settings.2 Task Description and MotivationTo illustrate the utility of jointly addressing interact-ing grammatical phenomena, we consider the cor-pus of the CoNLL-2013 shared task on grammaticalerror correction (Ng et al 2013), which we foundto be particularly well-suited for addressing interac-tions between grammatical phenomena.
The task fo-cuses on the following five common mistakes madeby ESL writers: article, preposition, noun number,subject-verb agreement, and verb form, and we ad-dress two interactions: article-NPhead and subject-verb.The training data for the task is from the NUCLEcorpus (Dahlmeier et al 2013), an error-tagged col-lection of essays written by non-native learners ofEnglish.
The test data is an additional set of essaysby learners from the same linguistic background.The training and the test data contain 1.2M and 29Kwords, respectively.
Although the corpus containserrors of other types, the task focuses on five typesof errors.
Table 1 shows the number of mistakes1 ofeach type and the error rates, i.e.
the percentage oferroneous words by error type.Error Number of errors and error rateTraining TestArticle 6658 (2.4%) 690 (10.0%)Prep.
2404 (2.0%) 311 (10.7%)Noun 3779 (1.6%) 396 (6.0%)Verb Agr.
1527(2.0%) 124 (5.2%)Verb Form 1453 (0.8%) 122 (2.5%)Table 1: Number of annotated errors in the CoNLL-2013 shared task.
Percentage denotes the error rates, i.e.the number of erroneous instances with respect to the to-tal number of relevant instances in the data.
For example,10.7% of prepositions in the test data are used incorrectly.The numbers in the revised data set are slightly higher.We note that the CoNLL-2013 data set is the firstannotated collection that makes a study like oursfeasible.
The presence of a common test set that1System performance in the shared task is evaluated on datawith and without additional revisions added based on the inputfrom participants.
The number of mistakes in the revised testdata is slightly higher.792contains a good number of interacting errors ?
ar-ticle, noun, and verb agreement mistakes ?
makesthe data set well-suited for studying which approachworks best for addressing interacting phenomena.The HOO-2011 shared task collection (Dale andKilgarriff, 2011) contains a very small number ofnoun and agreement errors (41 and 11 in test, re-spectively), while the HOO-2012 competition (Daleet al 2012) only addresses article and prepositionmistakes.
Indeed, in parallel to the work presentedhere, Wu and Ng (2013) attempted the ILP-basedapproach of Roth and Yih (2004) in this domain.They were not able to show any improvement, fortwo reasons.
First, the HOO-2011 data set whichthey used does not contain a good number of errorsin interacting structures.
Second, and most impor-tantly, they applied constraints in an indiscriminatemanner.
In contrast, we show how to identify theinteracting structures?
components in a reliable way,and this plays a key role in the joint modeling im-provements.Lack of data hindered other earlier efforts forerror correction beyond individual language phe-nomena.
Brockett et al(2006) applied machine-translation techniques to correct noun number errorson mass nouns and article usage but their applicationwas restricted to a small set of constructions.
Parkand Levy (2011) proposed a language-modeling ap-proach to whole sentence error correction but theirmodel is not competitive with individually trainedmodels.
Finally, Dahlmeier and Ng (2012) proposeda decoder model, focusing on four types of errorsin the data set of the HOO-2011 competition (Daleand Kilgarriff, 2011).
The decoder optimized the se-quence in which individual classifiers were to be ap-plied to the sentence.
However, because the decoderstill corrected mistakes in a pipeline fashion, one ata time, it is unlikely that it could deal with cases thatrequire simultaneous changes.3 The University of Illinois SystemBelow, we briefly describe the University of Illinoissystem (henceforth Illinois; in the overview paper ofthe shared task the system is referred to as UI) thatachieved the best result in the CoNLL-2013 sharedtask and which we use as our baseline model.
Fora complete description, we refer the reader to Ro-zovskaya et al(2013).The Illinois system implements five machine-learning independently-trained classifiers that fol-low the popular approach to ESL error correctionborrowed from the context-sensitive spelling correc-tion task (Golding and Roth, 1999; Carlson et al2001).
A confusion set is defined that specifies alist of confusable words.
Each occurrence of a con-fusable word in text is represented as a vector offeatures derived from a context window around thetarget.
The problem is cast as a multi-class classi-fication task and a classifier is trained on native orlearner data.
At prediction time, the model selectsthe most likely candidate from the confusion set.The confusion set for prepositions includes thetop 12 most frequent English prepositions.
The arti-cle confusion set is as follows: {a,the,?}2.
The con-fusion sets for noun, agreement, and form modulesdepend on the target word and include its morpho-logical variants (Table 2).
?Hence, the environmental *factor/factors also*contributes/contribute to various difficulties,*included/including problems in nuclear technol-ogy.
?Error type Confusion setNoun {factor, factors}Verb Agr.
{contribute, contributes}Verb Form {included, including, includes, include}Table 2: Confusion sets for noun number, agreement,and form classifiers.The article classifier is a discriminative modelthat draws on the state-of-the-art approach describedin Rozovskaya et al(2012).
The model makes useof the Averaged Perceptron algorithm (Freund andSchapire, 1996) and is trained on the training data ofthe shared task with rich features.The other models are trained on native Englishdata, the Google Web 1T 5-gram corpus (henceforth,Google, (Brants and Franz, 2006)) with the Na?
?veBayes (NB) algorithm.
All models use word n-gramfeatures derived from the 4-word window around thetarget word.
In the preposition model, priors forpreposition preferences are learned from the sharedtask training data (Rozovskaya and Roth, 2011).2?
denotes noun-phrase-initial contexts where an article islikely to have been omitted.
The variants ?a?
and ?an?
are con-flated and are restored later.793Example Predictions made by the Illinois system?They believe that such situation must be avoided.?
such situation?
such a situations?Nevertheless , electric cars is still regarded as a great trial innovation.?
cars is?
car are?Every students have appointments with the head of the department.?
No changeTable 3: Examples of predictions of the Illinois system that combines independently-trained models.The words that are selected as input to classifiersare called candidates.
Article and preposition can-didates are identified with a closed list of words;noun-phrase-initial contexts for the article classifierare determined using a shallow parser3 (Punyakanokand Roth, 2001).
Candidates for the noun, agree-ment, and form classifiers are identified with a part-of-speech tagger4, e.g.
noun candidates are wordsthat are tagged as NN or NNS.
Table 4 shows thetotal number of candidates for each classifier.ClassifierArt.
P N Agr.
FTrain 254K 103K 240K 75K 175KTest 6K 2.5K 2.6K 2.4K 4.8KTable 4: Number of candidate words by classifier typein training and test data.4 Interacting MistakesThe approach of addressing each type of mistake in-dividually is problematic when multiple phenomenainteract.
Consider the examples in Table 3 and thepredictions made by the Illinois system.
In the firstand second sentences, there are two possible waysto correct the structures ?such situation?
and ?carsis?.
In the former, either the article or the noun num-ber should be changed; in the latter, either the nounnumber or the verb agreement marker5.
In these ex-amples, each of the independently-trained classifiersidentifies the problem because each system makes adecision using the second error as part of its contex-tual cues, and thus the individual systems produceinconsistent predictions.3http://cogcomp.cs.illinois.edu/page/software view/Chunker4http://cogcomp.cs.illinois.edu/page/software view/POS5Both of these solutions will result in grammatical outputand the specific choice between the two depends on the wideressay context.The second type of interaction concerns cases thatrequire correcting more than one word at a time:the last example in Table 3 requires making changesboth to the verb and the subject.
Since each of the in-dependent classifiers (for nouns and for verb agree-ment) takes into account the other word as part ofits features, they both infer that the verb number iscorrect and that the grammatical subject ?student?should be plural.We refer to the words whose grammatical prop-erties interact as structures.
The independently-trained classifiers tend to fail to provide valid cor-rections in contexts where it is important to considerboth words of the structure.4.1 Structures for Joint ModelingWe address two linguistic structures that are relevantfor the grammatical phenomena considered: article-NPhead and subject-verb.
In the article-NPheadstructures, the interaction is between the head ofthe noun phrase (NP) and the article that refers tothe NP (first example in Table 3).
In particular,the model should take into account that the article?a?
cannot be combined with a noun in plural form.For subject-verb agreement, the subject and the verbshould agree in number.We now need to identify all pairs of candidatesthat form the relevant structures.
Article-NPheadstructures are pairs of words, such that the first wordis a candidate of type article, while the second wordis a noun candidate.
Given an article candidate, thehead of its NP is determined using the POS infor-mation (this information is obtained from the articlefeature vector because the NP head is a feature usedby the article system)6.
Subject-verb structures arepairs of noun-agreement candidates.
Given a verb,its subject is identified with a dependency parser(Marneffe et al 2006).To evaluate the accuracy of subject and NP head6Some heads are not identified or belong to a different partof speech.794predictions, a random sample of 500 structures ofeach type from the training data was examined bya human annotator with formal training in Linguis-tics.
The human annotations were then comparedagainst the automatic predictions.
The results ofthe evaluation for subject-verb and article-NPheadstructures are shown in Tables 5 and 6, respectively.Although the overall accuracy is above 90% for bothstructures, the accuracy varies by the distance be-tween the structure components and drops signifi-cantly as the distance increases.
For article-NPheadstructures, distance indicates the position of the NPhead with respect to the article, e.g.
distance of 1means that the head immediately follows the arti-cle.
For subject-verb structures, distance is shownwith respect to the verb: a distance of -1 means thatthe subject immediately precedes the verb.
Althoughin most cases the subject is located to the left ofthe verb, in some constructions, such as existentialclauses and inversions, it occurs after the verb.Based on the accuracy results for identifying thestructure components, we select those structureswhere the components are reliably identified.
Forarticle-NPhead, valid structures are those where thedistance is at most three words.
For subject-verb, weconsider as valid those structures where the identi-fied subject is located within two words to the left orthree words to the right of the verb.The valid structures are selected as input to thejoint model (Sec.
5).
The joint learning model con-siders only those valid structures whose componentsare adjacent.
In adjacent structures the NP head im-mediately follows the article, and the verb immedi-ately follows the subject.
Joint inference is not re-stricted to adjacent structures.The last column of Table 5 shows that validsubject-verb structures account for 67.5% of allverbs whose subjects are common nouns (51.7% arecases where the words are adjacent).
Verbs whosesubjects are common nouns account for 57.8% of allverbs that have subjects (verbs with different typesof subjects, most of which are personal pronouns,are not considered here, since these subjects are notpart of the noun classifier).Valid article-NPhead structures account for98.0% of all articles whose NP heads are commonnouns (47.5% of those are adjacent structures), asshown in the last column of Table 6.
71.0% of arti-cles in the training data belong to an NP whose headis a common noun; NPs whose heads belong to dif-ferent parts of speech are not considered.Note also that because a noun may belong both toan article-NPhead and a subject-verb structure, thestructures contain an overlap.Distance Accuracy % of all subj.
Cumul.predictions-1 97.6% 51.7% 51.7%1,2,3 100.0% 8.9% 60.6%-2 88.2% 6.9% 67.5%Other 80.8% 32.5% 100.0%Table 5: Accuracy of subject identification on a randomsample of subject-verb structures from the training data.The overall accuracy is 91.52%.
For each distance, the follow-ing are shown: accuracy based on comparison with human eval-uation; the percentage of all predictions that have this distance;the cumulative percentage.Distance Accuracy % of all head Cumul.predictions1 94.8% 47.5% 47.5%2 94.4% 44.0% 91.5%3 92.3% 6.5% 98.0%Other 89.1% 2.0% 100%Table 6: Accuracy of NP head identification on a randomsample of article-NPhead structures from training data.
Theoverall accuracy is 94.45%.
For each distance, the following areshown: accuracy based on comparison with human evaluation;the percentage of all predictions that have this distance; the cu-mulative percentage.5 The Joint ModelIn this section, we present the joint inference andthe joint learning approaches.
In the joint inferenceapproach, we use the independently-learned modelsfrom the Illinois system, and the interacting targetwords identified earlier are considered only at infer-ence stage.
In the joint learning method, we jointlylearn a model for the interacting phenomena.The label space in the joint models correspondsto sequences of labels from the confusion sets ofthe individual classifiers: {a ?
sing, a ?
pl, the ?sing, the ?
pl,?
?
sing,?
?
pl} and {sing ?sing, sing?pl, pl?sing, pl?pl} for article-NPheadand subject-verb structures, respectively7.
Invalid7?sing?
and ?pl?
refer to the grammatical number of noun795structures, such as pl-sing are excluded via hard con-straints (when we run joint inference) or via implicitsoft constraints (when we use joint learning).5.1 Joint InferenceIn the individual model approach, decisions aremade for each word independently, ignoring the in-teractions among linguistic phenomena.
The pur-pose of joint inference is to include linguistic (i.e.structural) knowledge, such as ?plural nouns do nottake an indefinite article?, and ?agreement consis-tency between the verb and the subject that controlsit?.
This knowledge should be useful for resolvinginconsistencies produced by individual classifiers.The inference approach we develop in this paperfollows the one proposed by Roth and Yih (2004)of training individual models and combining themat decision time via joint inference.
The advantageof this method is that it allows us to build uponany existing independently-learned models that pro-vide a distribution over their outcome, and producea coherent global output that respects our declarativeconstraints.
We formulate our component inferenceproblems as integer linear program (ILP) instancesas in Roth and Yih (2004).The inference takes as input the individual clas-sifiers?
confidence scores for each prediction, alongwith a list of constraints.
The output is the optimalsolution that maximizes the linear sum of the confi-dence scores, subject to the constraints that encodethe interactions.
The joint model thus selects a hy-pothesis that both obtains the best score accordingto the individual models and satisfies the constraintsthat reflect the interactions among the grammaticalphenomena at the level of linguistic structures, asdefined in Sec.
4.Inference The joint inference is enforced at thelevel of structures, and each structure correspondsto one ILP instance.
All structures consist of two orthree words: when an article-NPhead structure anda subject-verb structure include the same noun, thestructure input to the ILP consists of an article-noun-and verb agreement candidates.
The candidates themselves arethe surface forms of specific words that realize these grammat-ical properties.
Note that a subject in subject-verb structures isalways third person, since all subjects in subject-verb structuresare common nouns; other subjects, including pronouns, are ex-cluded.
Thus the agreement distinction is singular vs. plural.verb triple.
We formulate the inference problem asfollows: Given a structure s that consists of n words,let wi correspond to the ith word in the structure.
Leth denote a hypothesis from the hypothesis space Hfor s, and score(wi, h, li) denote the score assignedby the appropriate error-specific model to wi underh for label l from the confusion set of word wi.
Wedenote by ew,l the Boolean variable that indicateswhether the prediction on word w is assigned thevalue l (ew,l = 1) or not (ew,l = 0).We assume that each independent classifier re-turns a score that corresponds to the likelihood ofword wi under h being labeled li.
The softmax func-tion (Bishop, 1995) is used to convert raw activationscores to conditional probabilities for the discrimi-native article model.
The NB scores are also normal-ized and correspond to probabilities.
Then the infer-ence task is solved by maximizing the overall scoreof a candidate assignment of labels l to words w (thisset of feasible assignments is denoted H here) sub-ject to the constraints C for the structure s:h?
= argmaxh?Hscore(h) == argmaxh?Hn?i=1score(wi, h, li)ewi,lisubject to C(s)Constraints In the {0, 1} linear programming for-mulation described above, we can encode linguis-tic constraints that reflect the interactions among thelinguistic phenomena.
The inference enforces thefollowing structural and linguistic constraints:1.
The indefinite article ?a?
cannot refer to an NP headed bya plural noun.2.
Subject and verb must agree in number.In addition, we encode ?legitimacy?
constraints, thatmake sure that each w is assigned a single label.
Allconstraints are encoded as hard constraints.5.2 Joint LearningWe now describe how we learn the subject-verb andarticle-NPhead structures jointly.
The joint model isimplemented as a NB classifier and is trained in thesame way as the independent models on the Googlecorpus with word n-gram features.
Unlike the inde-pendent models, where the target corresponds to one796System Adjacent structures All distancesF1 (Orig.)
F1 (Revised) F1 (Orig.)
F1 (Revised)Illinois 31.20 42.14 31.20 42.14Na?
?veVerb 31.19 42.20 31.13 42.16Na?
?veNoun 31.03 41.87 30.91 41.70This paper joint systems Joint Inference (adjacent) Joint Inference (all distances)F1 (Orig.)
F1 (Revised) F1 (Orig.)
F1 (Revised)Subject-verb 31.90 42.94 31.97 42.86Article-NPhead 31.63 42.48 31.79 42.59Subject-verb + article-NPhead 32.35 43.16 32.51 43.19Table 7: Joint Inference Results.
All results are on the CoNLL-2013 test data using the original and revised gold annotations.Adjacent denotes a setting, where the joint inference is applied to structures with consecutive components (article-NPhead orsubject-verb).
All distances denotes a setting, where the constraints are applied to all valid structures, as described in Sec.
4.1.Illinois denotes the result obtained by the top CoNLL-2013 shared task system.
In all cases, the candidates that are not part of thestructures are handled by the respective components of the Illinois system.
Na?
?veVerb and Na?
?veNoun denote heuristics, where averb or subject are changed to ensure agreement.
All improvements over the Illinois system are statistically significant (McNemar?stest, p < 0.01).word, here the target corresponds to two words thatare part of the structure and the label space of themodel is modified accordingly.
Since we use fea-tures that can be computed from the small windowsin the Google corpus, the joint learning model han-dles only adjacent structures (Sec.
4.1).
Because thetarget consists of two words and the Google corpuscontains counts for n-grams of length at most five,the features are collected in the three word windowaround the target.8Unlike with the joint inference, here we do notexplicitly encode linguistic constraints.
One reasonfor this is that the NP head and subject predictionsare not 100% accurate, so input structures will havenoise.
However, the joint model learns these con-straints through the evidence seen in training.6 ExperimentsIn this section, we describe our experimental setupand evaluate the performance of the joint approach.In the joint approach, the joint components pre-sented in Sec.
5 handle the interacting structures de-scribed in Sec.
4.
The individual classifiers of theIllinois system make predictions for the remainingwords.
The research question addressed by the ex-periments is the following: Given independently-trained systems for different types of errors, can weimprove the performance by considering the phe-8Also note that when the article is ?, the surface form ofthe structure corresponds to the NP head alone; this does notpresent a problem because in the NB model the context countsare normalized with the prior counts.nomena that interact jointly?
To address this, wereport the results in the following settings:1.
Joint Inference: we compare the Illinois sys-tem that is a collection of individually-trained mod-els that are applied independently with a modelthat uses joint inference encoded as declarative con-straints in the ILP formulation and show that usingjoint inference results in a strong performance gain.2.
Joint Learning: we compare the Illinois systemwith a model that incorporates jointly-trained com-ponents for the two linguistic structures that we de-scribed in Sec.
4.
We show that joint training pro-duces an even stronger gain in performance com-pared to the Illinois model.2.
Joint Learning and Inference: we apply joint in-ference to the output of the joint learning system toaccount for dependencies not covered by the jointlearning model.We report F1 performance scored using the offi-cial scorer from the shared task (Dahlmeier and Ng,2012).
The task reports two types of evaluation: onthe original gold data and on gold data with addi-tional corrections.
We refer to the results as Origi-nal and Revised.6.1 Joint Inference ResultsTable 7 shows the results of applying joint infer-ence to the Illinois system.
Both the article-NPheadand the subject-verb constraints improve the perfor-mance.
The results for the joint inference are shownin two settings, adjacent and all structures, so thatlater we can compare joint inference with the jointlearning model that handles only adjacent structures.797Illinois system Illinois-NBArticleF1 (Orig.)
F1 (Revised) F1 (Orig.)
F1 (Revised)Illinois 31.20 42.14 31.71 41.38This paper joint systems Joint Learning (adjacent) Joint Learning (adjacent)F1 (Orig.)
F1 (Revised) F1 (Orig.)
F1 (Revised)Subject-verb 32.64* 43.37* 33.09* 42.78*Article-NPhead 33.89* 42.57* 33.16* 41.51Subject-verb + article-NPhead 35.12* 43.73* 34.41* 42.76*Table 8: Joint Learning Results.
All results are on the CoNLL-2013 test data using the original and revised gold annotations.Illinois-NBArticle denotes the Illinois system, where the discriminative article model is replaced with a NB classifier.
Adjacentdenotes a setting, where the structure components are consecutive (article-NPhead or subject-verb), as described in Sec.
4.1.Illinois denotes the result obtained by the top CoNLL-2013 shared task system.
In all cases, the candidates that are not part of thestructures are handled by the respective components of the Illinois system.
Statistically significant improvements (McNemar?s test,p < 0.01) over the Illinois system are marked with an asterisk (*).It is also interesting to note that the key improvementcomes from considering structures whose compo-nents are adjacent.
This is not surprising given thatthe accuracy for subject and NP head identificationdrops as the distance increases.For subject-verb constraints, we also implementa na?
?ve approach that looks for contradictions andchanges either the verb or the subject if they do notsatisfy the number agreement.
These two heuris-tics are denoted as Na?
?veVerb and Na??veNoun.
Theheuristics differ from the joint inference in that theyenforce agreement by always changing either thenoun (Na?
?veNoun) or the verb (Na?
?veVerb), whilethe joint inference does this using the scores pro-duced by the independent models.
In other words,the key is the objective function, while the compo-nents of the objective function are the same in theheuristics and the joint inference.
The results in Ta-ble 7 show that simply enforcing agreement does notwork well and that the ILP formulation is indeed ef-fective and improves over the independently-trainedmodels in all cases.Recall that valid structures include only thosewhose components can be identified in a reliableway (Sec.
4.1).
To evaluate the impact of that filter-ing, we perform two experiments with subject-verbstructures (long-distance dependencies are morecommon in those constructions than in the article-NPhead structures): first, we apply joint inferenceto all subject-verb structures.
We obtain F1 scores of31.61 and 42.28, on original and revised gold data,respectively, which is significantly worse than theresults on subject-verb structures in Table 7 (31.97and 42.86, respectively) and only slightly better thanthe baseline performance of the Illinois system.
Fur-thermore, when we apply joint inference to thosestructures which were excluded by filtering in Sec.4.1, we find that the performance degrades com-pared to the Illinois system (30.85 and 41.58).
Theseresults demonstrate that the joint inference improve-ments are due to structures whose components canbe identified with high accuracy and that it is essen-tial to identify these structures; bad structures, on theother hand, hurt performance.6.2 Joint Learning ResultsNow we show experimental results of the joint learn-ing (Table 8).
Note that the joint learning componentconsiders only those structures where the words areadjacent.
Because the Illinois system presented inSec.
3 makes use of a discriminative article model,while the joint model uses NB, we also show results,where the article model is replaced by a NB classi-fier trained on the Google corpus.
In all cases, jointlearning demonstrates a strong performance gain.6.3 Joint Learning and Inference ResultsFinally, we apply joint inference to the output of thejoint learning system in Sec.
6.2.
Table 9 showsthe results of the Illinois model, the model that ap-plies joint inference and joint learning separately,and both.
Even though the joint learning performsbetter than the joint inference, the joint learningcovers only adjacent structures.
Furthermore, jointlearning does not address overlapping structures oftriples that consist of article, subject, and verb (6%of all structures).
Joint inference allows us to ensureconsistent predictions in cases not addressed by the798Example Illinois system JL and JI?Moreover, the increased technologies help people to overcomedifferent natural disasters.No change technology helps?At that time,... there are surveillances in everyone?s heart andcriminals are more difficult to hide.
?there are* surveillance* there is surveillance?In such situation, individuals will lose their basic privacy.?
such a* situations* such a situation?In supermarket monitor is needed because we have to trackthieves.
?No change monitors areTable 10: Examples of mistakes that are corrected by the joint model but not by the Illinois model.
Illinois denotes the resultobtained by the top CoNLL-2013 shared task system from the University of Illinois.
JL and JI stand for joint learning and jointinference, respectively.
Inconsistent predictions are starred.F1 (Orig.)
F1 (Revised)Illinois 31.20 42.14Joint Inference 32.51 43.19Joint Learning 35.12 43.73Joint Learn.
+ Inf.
35.21 43.74Table 9: Joint Learning and Inference.
All results are on theCoNLL-2013 test data using the original and revised gold anno-tations.
Results of the joint models that include the joint infer-ence component are shown for structures of all distances.
Illi-nois denotes the result obtained by the top CoNLL-2013 sharedtask system.
All joint systems demonstrate a statistically sig-nificant improvement over the Illinois system; joint learningimprovements are also statistically significant compared to thejoint inference results (McNemar?s test, p < 0.01).joint learning model.
Indeed, we can get a small im-provement by adding joint inference on top of thejoint learning on original annotations.
Since the re-vised corrections are based on the participants?
inputand are most likely biased towards system predic-tions for corrections missed by the original annota-tors (Ng et al 2013), it is more difficult to showimprovement on revised data.7 Discussion and Error AnalysisIn the previous section, we evaluated the proposedjoint inference and joint learning models that han-dle interacting grammatical phenomena.
We showedthat the joint models produce significant improve-ments over the highest-scoring CoNLL-2013 sharedtask system that consists of independently-trainedclassifiers: the joint approaches increase the F1score by 4 F1 points on the original gold data andalmost 2 points on the revised data (Table 9).These results are interesting from the point ofview of developing a practical error correction sys-tem.
However, recall that the errors in the interact-ing structures are only a subset of mistakes in theCoNLL-2013 data set but the evaluation in Sec.
6 isperformed with respect to all of these errors.
Froma scientific point of view, it is interesting to evalu-ate the impact of the joint models more precisely byconsidering the improvements on the relevant struc-tures only.
Table 11 shows how much the joint learn-ing approach improves on the subset of relevant mis-takes.StructurePerformance (F1)Illinois Joint LearningSubject-verb 39.64 52.25Article-NPhead 30.65 35.90Table 11: Evaluation of the joint learning performance onthe subset of the data containing interacting errors.
All re-sults are on the CoNLL-2013 test data using the original anno-tations.
Illinois denotes the result obtained by the top CoNLL-2013 shared task system.
All improvements are statistically sig-nificant over the Illinois system (McNemar?s test, p < 0.01).Error Analysis To better understand where the jointmodels have an advantage over the independently-trained classifiers, we analyze the output producedby each of the approaches.
In Table 10 we showexamples of mistakes that the model that uses jointlearning and inference is able to identify correctly,along with the original predictions made by the Illi-nois system.Joint Inference vs. Joint Learning We wishto stress that the joint approaches do not simplyperform better but also make coherent decisionsby disallowing illegitimate outputs.
The joint in-ference approach does this by enforcing linguis-tic constraints on the output.
The joint learningmodel, while not explicitly encoding these con-straints, learns them from the distribution of thetraining data.799Joint inference is a less expensive model, since ituses the scores produced by the individual classifiersand thus does not require additional training.
Jointlearning, on the other hand, is superior to joint infer-ence, since it is better at modeling interactions wheremultiple errors occur simultaneously ?
it eliminatesthe noisy context present when learning the inde-pendent classifiers.
Consider the first example fromTable 10, where both the noun and the agreementclassifiers receive noisy input: the verb ?help?
andthe noun ?technologies?
act as part of input featuresfor the noun and agreement classifiers, respectively.The noisy features prevent both modules from iden-tifying the two errors.Finally, an important distinction of the joint learn-ing method is that it considers all possible output se-quences in training, and thus it is able to better iden-tify errors that require multiple changes, such as thelast example in Table 10, where the Illinois systemproposes no changes.7.1 Error Correction: ChallengesWe finalize our discussion with a few comments onthe challenges of the error correction task.Task Difficulty As shown in Table 1 in Sec.
2, onlya small percentage of words have mistakes, whileover 90% (about 98% in training) are used correctly.The low error rates are the key reason the error cor-rection task is so difficult: it is quite challenging fora system to improve over a writer that already per-forms at the level of over 90%.
Indeed, very fewNLP tasks already have systems that perform at thatlevel, even when the data is not as noisy as the ESLdata.Evaluation Metrics In the CoNLL-2013 competi-tion, as well as the competitions alluded to earlier,systems were compared on F1 performance, and,consequently, this is the metric we optimize in thispaper.
Practical error correction systems, however,should be tuned to minimize recall to guarantee thatthe overall quality of the text does not go down.
In-deed, the error sparsity makes it very challenging toidentify mistakes accurately, and no system in theshared task achieves a precision over 50%.
How-ever, once the precision drops below 50%, the sys-tem introduces more mistakes than it identifies.Clearly, optimizing the F1 measure does not en-sure that the quality of the text improves as a re-sult of running the system.
Thus, it can be arguedthat the F1 measure is not the right measure for er-ror correction.
A different evaluation metric basedon the accuracy of the data before and after runningthe system was proposed in Rozovskaya and Roth(2010c).
When optimizing for this metric, the nounmodule, for instance, at recall point 20%, achievesa precision of 63.93%.
This translates into accuracyof 94.46%, while the baseline on noun errors in thetest data (i.e.
the accuracy of the data before runningthe system) is 94.0% (Table 1).
This means that thesystem improves the quality of the data.Annotation Lastly, we believe that it is importantto provide alternative corrections, as the agreementon what constitutes a mistake even among nativeEnglish speakers can be quite low (Madnani et al2011).8 ConclusionThis work presented the first successful study thatjointly corrects grammatical mistakes.
We ad-dressed two pairs of interacting phenomena andshowed that it is possible to reliably identify theircomponents, thereby facilitating the joint approach.We described two joint methods: a joint in-ference approach implemented via ILP and ajoint learning model.
The joint inference en-forces constraints using the scores produced by theindependently-trained models.
The joint learningmodel learns the interacting phenomena as struc-tures.
The joint methods produce a significant im-provement over a state-of-the-art system that com-bines independently-trained models and, impor-tantly, produce linguistically legitimate output.AcknowledgmentsThe authors thank Peter Chew, Jennifer Cole, Mark Sam-mons, and the anonymous reviewers for their helpfulfeedback.
The authors thank Josh Gioja for the codethat performs phonetic disambiguation of the indefinitearticle.
This material is based on research sponsoredby DARPA under agreement number FA8750-13-2-0008.The U.S. Government is authorized to reproduce and dis-tribute reprints for Governmental purposes notwithstand-ing any copyright notation thereon.
The views and con-clusions contained herein are those of the authors andshould not be interpreted as necessarily representing theofficial policies or endorsements, either expressed or im-plied, of DARPA or the U.S. Government.800ReferencesC.
Bishop.
1995.
Neural Networks for Pattern Recogni-tion, chapter 6.4: Modelling conditional distributions.Oxford University Press.T.
Brants and A. Franz.
2006.
Web 1T 5-gram Version 1.Linguistic Data Consortium, Philadelphia, PA.C.
Brockett, D. B. William, and M. Gamon.
2006.Correcting ESL errors using phrasal SMT techniques.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 249?256, Sydney, Australia, July.
Associationfor Computational Linguistics.A.
J. Carlson, J. Rosen, and D. Roth.
2001.
Scaling upcontext sensitive text correction.
In IAAI.J.
Clarke and M. Lapata.
2007.
Modelling compressionwith discourse constraints.
In Proceedings of the 2007Joint Conference of EMNLP-CoNLL.D.
Dahlmeier and H.T Ng.
2012.
A beam-search de-coder for grammatical error correction.
In EMNLP-CoNLL, Jeju Island, Korea, July.
Association for Com-putational Linguistics.D.
Dahlmeier, H.T.
Ng, and S.M.
Wu.
2013.
Buildinga large annotated corpus of learner english: The nuscorpus of learner english.
In Proc.
of the NAACL HLT2013 Eighth Workshop on Innovative Use of NLP forBuilding Educational Applications, Atlanta, Georgia,June.
Association for Computational Linguistics.R.
Dale and A. Kilgarriff.
2011.
Helping Our Own: TheHOO 2011 pilot shared task.
In Proceedings of the13th European Workshop on Natural Language Gen-eration.R.
Dale, I. Anisimoff, and G. Narroway.
2012.
A re-port on the preposition and determiner error correctionshared task.
In Proc.
of the NAACL HLT 2012 SeventhWorkshop on Innovative Use of NLP for Building Edu-cational Applications, Montreal, Canada, June.
Asso-ciation for Computational Linguistics.R.
De Felice and S. Pulman.
2008.
A classifier-based ap-proach to preposition and determiner error correctionin L2 English.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics (Col-ing 2008), pages 169?176, Manchester, UK, August.Y.
Freund and R. E. Schapire.
1996.
Experiments witha new boosting algorithm.
In Proc.
13th InternationalConference on Machine Learning.M.
Gamon, J. Gao, C. Brockett, A. Klementiev,W.
Dolan, D. Belenko, and L. Vanderwende.
2008.Using contextual speller techniques and languagemodeling for ESL error correction.
In Proceedings ofIJCNLP.M.
Gamon.
2010.
Using mostly native data to correcterrors in learners?
writing.
In NAACL, pages 163?171,Los Angeles, California, June.A.
R. Golding and D. Roth.
1999.
A Winnow basedapproach to context-sensitive spelling correction.
Ma-chine Learning.N.
Han, M. Chodorow, and C. Leacock.
2006.
Detectingerrors in English article usage by non-native speakers.Journal of Natural Language Engineering, 12(2):115?129.E.
Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-hara.
2003.
Automatic error detection in the Japaneselearners?
English spoken data.
In The Companion Vol-ume to the Proceedings of 41st Annual Meeting ofthe Association for Computational Linguistics, pages145?148, Sapporo, Japan, July.J.
Lee and S. Seneff.
2008.
Correcting misuse of verbforms.
In ACL, pages 174?182, Columbus, Ohio,June.
Association for Computational Linguistics.N.
Madnani, M. Chodorow, J. Tetreault, and A. Ro-zovskaya.
2011.
They can help: Using crowdsourcingto improve the evaluation of grammatical error detec-tion systems.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 508?513, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.M.
Marneffe, B. MacCartney, and Ch.
Manning.
2006.Generating typed dependency parses from phrasestructure parses.
In LREC.A.
Martins, Noah N. Smith, M. Figueiredo, and P. Aguiar.2011.
Dual decomposition with many overlappingcomponents.
In Proceedings of the 2011 Conferenceon Empirical Methods in Natural Language Process-ing, pages 238?249, Edinburgh, Scotland, UK., July.Association for Computational Linguistics.H.
T. Ng, S. M. Wu, Y. Wu, Ch.
Hadiwinoto, andJ.
Tetreault.
2013.
The conll-2013 shared task ongrammatical error correction.
In Proc.
of the Sev-enteenth Conference on Computational Natural Lan-guage Learning.
Association for Computational Lin-guistics.A.
Park and R. Levy.
2011.
Automated whole sentencegrammar correction using a noisy channel model.
InACL, Portland, Oregon, USA, June.
Association forComputational Linguistics.V.
Punyakanok and D. Roth.
2001.
The use of classifiersin sequential inference.
In NIPS.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The impor-tance of syntactic parsing and inference in semanticrole labeling.
Computational Linguistics, 34(2).D.
Roth and W. Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.
InHwee Tou Ng and Ellen Riloff, editors, CoNLL.A.
Rozovskaya and D. Roth.
2010a.
Annotating ESLerrors: Challenges and rewards.
In Proceedings of the801NAACL Workshop on Innovative Use of NLP for Build-ing Educational Applications.A.
Rozovskaya and D. Roth.
2010b.
Generating con-fusion sets for context-sensitive error correction.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP).A.
Rozovskaya and D. Roth.
2010c.
Training paradigmsfor correcting errors in grammar and usage.
InNAACL.A.
Rozovskaya and D. Roth.
2011.
Algorithm selec-tion and model adaptation for ESL correction tasks.
InACL.A.
Rozovskaya, M. Sammons, J. Gioja, and D. Roth.2011.
University of Illinois system in HOO text cor-rection shared task.A.
Rozovskaya, M. Sammons, and D. Roth.
2012.
TheUI system in the HOO 2012 shared task on error cor-rection.A.
Rozovskaya, K.-W. Chang, M. Sammons, andD.
Roth.
2013.
The University of Illinois system inthe CoNLL-2013 shared task.
In CoNLL Shared Task.C.
Sutton and A. McCallum.
2007.
Piecewise pseudo-likelihood for efficient training of conditional randomfields.
In Zoubin Ghahramani, editor, ICML.J.
Tetreault and M. Chodorow.
2008.
The ups anddowns of preposition error detection in ESL writing.In Proceedings of the 22nd International Conferenceon Computational Linguistics (Coling 2008), pages865?872, Manchester, UK, August.Y.
Wu and H.T.
Ng.
2013.
Grammatical error correctionusing integer linear programming.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers), pages1456?1465, Sofia, Bulgaria, August.
Association forComputational Linguistics.802
