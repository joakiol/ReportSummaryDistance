Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 483?490,Sydney, July 2006. c?2006 Association for Computational LinguisticsAutomatic Identification of Pro and Con Reasons in Online ReviewsSoo-Min Kim and Eduard HovyUSC Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292-6695{skim, hovy}@ISI.EDUAbstractIn this paper, we present a system thatautomatically extracts the pros and consfrom online reviews.
Although many ap-proaches have been developed for ex-tracting opinions from text, our focushere is on extracting the reasons of theopinions, which may themselves be in theform of either fact or opinion.
Leveragingonline review sites with author-generatedpros and cons, we propose a system foraligning the pros and cons to their sen-tences in review texts.
A maximum en-tropy model is then trained on the result-ing labeled set to subsequently extractpros and cons from online review sitesthat do not explicitly provide them.
Ourexperimental results show that our result-ing system identifies pros and cons with66% precision and 76% recall.1 IntroductionMany opinions are being expressed on the Webin such settings as product reviews, personalblogs, and news group message boards.
Peopleincreasingly participate to express their opinionsonline.
This trend has raised many interestingand challenging research topics such as subjec-tivity detection, semantic orientation classifica-tion, and review classification.Subjectivity detection is the task of identifyingsubjective words, expressions, and sentences.
(Wiebe et al, 1999; Hatzivassiloglou and Wiebe,2000; Riloff et al 2003).
Identifying subjectivityhelps separate opinions from fact, which may beuseful in question answering, summarization, etc.Semantic orientation classification is a task ofdetermining positive or negative sentiment ofwords (Hatzivassiloglou and McKeown, 1997;Turney, 2002; Esuli and Sebastiani, 2005).
Sen-timent of phrases and sentences has also beenstudied in (Kim and Hovy, 2004; Wilson et al,2005).
Document level sentiment classification ismostly applied to reviews, where systems assigna positive or negative sentiment for a whole re-view document (Pang et al, 2002; Turney,2002).Building on this work, more sophisticatedproblems in the opinion domain have been stud-ied by many researchers.
(Bethard et al, 2004;Choi et al, 2005; Kim and Hovy, 2006) identi-fied the holder (source) of opinions expressed insentences using various techniques.
(Wilson etal., 2004) focused on the strength of opinionclauses, finding strong and weak opinions.
(Chklovski, 2006) presented a system that aggre-gates and quantifies degree assessment of opin-ions scattered throughout web pages.Beyond document level sentiment classifica-tion in online product reviews, (Hu and Liu,2004; Popescu and Etzioni, 2005) concentratedon mining and summarizing reviews by extract-ing opinion sentences regarding product features.In this paper, we focus on another challengingyet critical problem of opinion analysis, identify-ing reasons for opinions, especially for opinionsin online product reviews.
The opinion reasonidentification problem in online reviews seeks toanswer the question ?What are the reasons thatthe author of this review likes or dislikes theproduct??
For example, in hotel reviews, infor-mation such as ?found 189 positive reviews and65 negative reviews?
may not fully satisfy theinformation needs of different users.
More usefulinformation would be ?This hotel is great forfamilies with young infants?
or ?Elevators aregrouped according to floors, which makes thewait short?.This work differs in important ways fromstudies in (Hu and Liu, 2004) and (Popescu andEtzioni, 2005).
These approaches extract features483of products and identify sentences that containopinions about those features by using opinionwords and phrases.
Here, we focus on extractingpros and cons which include not only sentencesthat contain opinion-bearing expressions aboutproducts and features but also sentences withreasons why an author of a review writes the re-view.
Following are examples identified by oursystem.It creates duplicate files.Video drains battery.It won't play music from allmusic storesEven though finding reasons in opinion-bearing texts is a critical part of in-depth opinionassessment, no study has been done in this par-ticular vein partly because there is no annotateddata.
Labeling each sentence is a time-consuming and costly task.
In this paper, we pro-pose a framework for automatically identifyingreasons in online reviews and introduce a noveltechnique to automatically label training data forthis task.
We assume reasons in an online reviewdocument are closely related to pros and consrepresented in the text.
We leverage the fact thatreviews on some websites such as epinions.comalready contain pros and cons written by thesame author as the reviews.
We use those prosand cons to automatically label sentences in thereviews on which we subsequently train our clas-sification system.
We then apply the resultingsystem to extract pros and cons from reviews inother websites which do not have specified prosand cons.This paper is organized as follows: Section 2describes a definition of reasons in online re-views in terms of pros and cons.
Section 3 pre-sents our approach to identify them and Section 4explains our automatic data labeling process.Section 5 describes experimental and results andfinally, in Section 6, we conclude with futurework.2 Pros and Cons in Online ReviewsThis section describes how we define reasons inonline reviews for our study.
First, we take alook at how researchers in Computational Lin-guistics define an opinion for their studies.
It isdifficult to define what an opinion means in acomputational model because of the difficulty ofdetermining the unit of an opinion.
In general,researchers study opinion at three different lev-els: word level, sentence level, and documentlevel.Word level opinion analysis includes wordsentiment classification, which views single lexi-cal items (such as good or bad) as sentiment car-riers, allowing one to classify words into positiveand negative semantic categories.
Studies in sen-tence level opinion regard the sentence as a mini-mum unit of opinion.
Researchers try to identifyopinion-bearing sentences, classify their senti-ment, and identify opinion holders and topics ofopinion sentences.
Document level opinionanalysis has been mostly applied to review clas-sification, in which a whole document written fora review is judged as carrying either positive ornegative sentiment.
Many researchers, however,consider a whole document as the unit of anopinion to be too coarse.In our study, we take the approach that a re-view text has a main opinion (recommendationor not) about a given product, but also includesvarious reasons for recommendation or non-recommendation, which are valuable to identify.Therefore, we focus on detecting those reasons inonline product review.
We also assume that rea-sons in a review are closely related to pros andcons expressed in the review.
Pros in a productreview are sentences that describe reasons whyan author of the review likes the product.
Consare reasons why the author doesn?t like the prod-uct.
Based on our observation in online reviews,most reviews have both pros and cons even ifsometimes one of them dominates.3 Finding Pros and ConsThis section describes our approach for find-ing pro and con sentences given a review text.We first collect data from epinions.com andautomatically label each sentences in the data set.We then model our system using one of the ma-chine learning techniques that have been success-fully applied to various problems in NaturalLanguage Processing.
This section also describesfeatures we used for our model.3.1 Automatically Labeling Pro and ConSentencesAmong many web sites that have product re-views such as amazon.com and epinions.com,some of them (e.g.
epinions.com) explicitly statepros and cons phrases in their respective catego-ries by each review?s author along with the re-view text.
First, we collected a large set of <re-view text, pros, cons> triplets from epin-484ions.com.
A review document in epinions.comconsists of a topic (a product model, restaurantname, travel destination, etc.
), pros and cons(mostly a few keywords but sometimes completesentences), and the review text.
Our automaticlabeling system first collects phrases in pro andcon fields and then searches the main review textin order to collect sentences corresponding tothose phrases.
Figure 1 illustrates the automaticlabeling process.Figure 1.
The automatic labeling process ofpros and cons sentences in a review.The system first extracts comma-delimitedphrases from each pro and con field, generatingtwo sets of phrases: {P1, P2, ?, Pn} for prosand {C1, C2, ?, Cm} for cons.
In the example inFigure 1, ?beautiful display?
can be Pi and ?notsomething you want to drop?
can be Cj.
Then thesystem compares these phrases to the sentencesin the text in the ?Full Review?.
For each phrasein {P1, P2, ?, Pn} and {C1, C2, ?, Cm}, thesystem checks each sentence to find a sentencethat covers most of the words in the phrase.
Thenthe system annotates this sentence with the ap-propriate ?pro?
or ?con?
label.
All remainingsentences with neither label are marked as ?nei-ther?.
After labeling all the epinion data, we useit to train our pro and con sentence recognitionsystem.3.2 Modeling with Maximum EntropyClassificationWe use Maximum Entropy classification for thetask of finding pro and con sentences in a givenreview.
Maximum Entropy classification hasbeen successfully applied in many tasks in natu-ral language processing, such as Semantic Rolelabeling, Question Answering, and InformationExtraction.Maximum Entropy models implement the in-tuition that the best model is the one that is con-sistent with the set of constraints imposed by theevidence but otherwise is as uniform as possible(Berger et al, 1996).
We modeled the condi-tional probability of a class c  given a featurevector x  as follows:)),(exp(1)|( ?=iiixxcfZxcp ?where xZ  is a normalization factor which can becalculated by the following:?
?=c iiix xcfZ )),(exp( ?In the first equation, ),( xcfi  is a feature func-tion which has a binary value, 0 or 1. i?
is aweight parameter for the feature function),( xcfi  and higher value of the weight indicatesthat ),( xcfi  is an important feature for a classc .
For our system development, we usedMegaM toolkit 1  which implements the aboveintuition.In order to build an efficient model, we sepa-rated the task of finding pro and con sentencesinto two phases, each being a binary classifica-tion.
The first is an identification phase and thesecond is a classification phase.
For this 2-phasemodel, we defined the 3 classes of c  listed inTable 1.
The identification task separates pro andcon candidate sentences (CR and PR in Table 1)from sentences irrelevant to either of them (NR).The classification task then classifies candidatesinto pros (PR) and cons (CR).
Section 5 reportssystem results of both phases.1 http://www.isi.edu/~hdaume/megam/index.htmlTable 1: Classes defined for the classificationtasks.Classsymbol DescriptionPR Sentences related to pros in a reviewCR Sentences related to cons in a reviewNR Sentences related to neither PR nor CR4853.3 FeaturesThe classification uses three types of features:lexical features, positional features, and opinion-bearing word features.For lexical features, we use unigrams, bi-grams, and trigrams collected from the trainingset.
They investigate the intuition that there arecertain words that are frequently used in pro andcon sentences which are likely to represent rea-sons why an author writes a review.
Examples ofsuch words and phrases are: ?because?
and?that?s why?.For positional features, we first find para-graph boundaries in review texts using html tagssuch as <br> and <p>.
After finding paragraphboundaries, we add features indicating the first,the second, the last, and the second last sentencein a paragraph.
These features test the intuitionused in document summarization that importantsentences that contain topics in a text have cer-tain positional patterns in a paragraph (Lin andHovy, 1997), which may apply because reasonslike pros and cons in a review document are mostimportant sentences that summarize the wholepoint of the review.For opinion-bearing word features, we usedpre-selected opinion-bearing words produced bya combination of two methods.
The first methodderived a list of opinion-bearing words from alarge news corpus by separating opinion articlessuch as letters or editorials from news articleswhich simply reported news or events.
The sec-ond method calculated semantic orientations ofwords based on WordNet2 synonyms.
In our pre-vious work (Kim and Hovy, 2005), we demon-strated that the list of words produced by a com-bination of those two methods performed verywell in detecting opinion bearing sentences.
Bothalgorithms are described in that paper.The motivation for including the list of opin-ion-bearing words as one of our features is thatpro and con sentences are quite likely to containopinion-bearing expressions (even though someof them are only facts), such as ?The waitingtime was horrible?
and ?Their portion size offood was extremely generous!?
in restaurant re-views.
We presumed pro and con sentences con-taining only facts, such as ?The battery lasted 3hours, not 5 hours like they advertised?, wouldbe captured by lexical or positional features.In Section 5, we report experimental resultswith different combinations of these features.2 http://wordnet.princeton.edu/Table 2 summarizes the features we used for ourmodel and the symbols we will use in the rest ofthis paper.4 DataWe collected data from two different sources:epinions.com and complaints.com3 (see Section3.1 for details about review data in epinion.com).Data from epinions.com is mostly used to trainthe system whereas data from complaints.com isto test how the trained model performs on newdata.Complaints.com includes a large database ofpublicized consumer complaints about diverseproducts, services, and companies collected forover 6 years.
Interestingly, reviews in com-plaint.com are somewhat different from manyother web sites which are directly or indirectlylinked to Internet shopping malls such as ama-zon.com and epinions.com.
The purpose of re-views in complaints.com is to share consumers?mostly negative experiences and alert businessesto customers feedback.
However, many reviewsin Internet shopping mall related reviews arepositive and sometimes encourage people to buymore products or to use more services.Despite its significance, however, there is nohand-annotated data that we can use to build asystem to identify reasons of complaints.com.
Inorder to solve this problem, we assume that rea-sons in complaints reviews are similar to cons inother reviews and therefore if we are, somehow,able to build a system that can identify cons from3 http://www.complaints.com/Table 2: Feature summary.Featurecategory Description SymbolLexicalFeaturesunigramsbigramstrigramsLexPositionalFeaturesthe first, the second,the last, the secondto last sentence in aparagraphPosOpinion-bearingwordfeaturespre-selected opin-ion-bearing words Op486reviews, we can apply it to identify reasons incomplaints reviews.
Based on this assumption,we learn a system using the data from epin-ions.com, to which we can apply our automaticdata labeling technique, and employ the resultingsystem to identify reasons from reviews in com-plaint.com.
The following sections describe eachdata set.4.1 Dataset 1: Automatically Labeled DataWe collected two different domains of reviewsfrom epinions.com: product reviews and restau-rant reviews.
As for the product reviews, we col-lected 3241 reviews (115029 sentences) aboutmp3 players made by various manufacturers suchas Apple, iRiver, Creative Lab, and Samsung.We also collected 7524 reviews (194393 sen-tences) about various types of restaurants such asfamily restaurants, Mexican restaurants, fast foodchains, steak houses, and Asian restaurants.
Theaverage numbers of sentences in a review docu-ment are 35.49 and 25.89 respectively.The purpose of selecting one of electronicsproducts and restaurants as topics of reviews forour study is to test our approach in two ex-tremely different situations.
Reasons why con-sumers like or dislike a product in electronics?reviews are mostly about specific and tangiblefeatures.
Also, there are somewhat a fixed set offeatures of a specific type of product, for exam-ple, ease of use, durability, battery life, photoquality, and shutter lag for digital cameras.
Con-sequently, we can expect that reasons in electron-ics?
reviews may share those product featurewords and words that describe aspects of featuressuch as short or long for battery life.
This factmight make the reason identification task easy.On the other hand, restaurant reviewers talkabout very diverse aspects and abstract featuresas reasons.
For example, reasons such as ?Youfeel like you are in a train station or a busyamusement park that is ill-staffed to meet de-mand!
?, ?preferential treatment given to largegroups?, and ?they don't offer salads of anykind?
are hard to predict.
Also, they seem rarelyshare common keyword features.We first automatically labeled each sentencein those reviews collected from each domainwith the features described in Section 3.1.
Wedivided the data for training and testing.
We thentrained our model using the training set andtested it to see if the system can successfully la-bel sentences in the test set.4.2 Dataset 2: Complaints.com DataFrom the database 4  in complaints.com, wesearched for the same topics of reviews as Data-set 1: 59 complaints reviews about mp3 playersand 322 reviews about restaurants5.
We testedour system on this dataset and compare the re-sults against human judges?
annotation results.Subsection 5.2 reports the evaluation results.5 Experiments and ResultsWe describe two goals in our experiments in thissection.
The first is to investigate how well ourpro and con detection model with different fea-ture combinations performs on the data we col-lected from epinions.com.
The second is to seehow well the trained model performs on newdata from a different source, complaint.com.For both datasets, we carried out two separatesets of experiments, for the domains of mp3players and restaurant reviews.
We divided datainto 80% for training, 10% for development, and10% for test for our experiments.5.1 Experiments on Dataset 1Identification step: Table 3 and 4 show pros andcons sentences identification results of our sys-tem for mp3 player and restaurant reviews re-spectively.
The first column indicates whichcombination of features was used for our model(see Table 2 for the meaning of Op, Lex, and Posfeature categories).
We measure the performancewith accuracy (Acc), precision (Prec), recall(Recl), and F-score 6.The baseline system assigned all sentences asreason and achieved 57.75% and 54.82% of ac-curacy.
The system performed well when it onlyused lexical features in mp3 player reviews(76.27% of accuracy in Lex), whereas it per-formed well with the combination of lexical andopinion features in restaurant reviews (Lex+Oprow in Table 4).It was very interesting to see that the systemachieved a very low score when it only usedopinion word features.
We can interpret this phe-nomenon as supporting our hypothesis that proand con sentences in reviews are often purely4 At the time (December 2005), there were total 42593complaint reviews available in the database.5 Average numbers of sentences in a complaint is19.57 for mp3 player reviews and 21.38 for restaurantreviews.6 We calculated F-score byRecall PrecisionRecall Precision   2+??487factual.
However, opinion features improvedboth precision and recall when combined withlexical features in restaurant reviews.
It was alsointeresting that experiments on mp3 players re-views achieved mostly higher scores than restau-rants.
Like the observation we described in Sub-section 4.1, frequently mentioned keywords ofproduct features (e.g.
durability) may havehelped performance, especially with lexical fea-tures.
Another interesting observation is that thepositional features that helped in topic sentenceidentification did not help much for our task.Classification step: Tables 5 and 6 show thesystem results of the pro and con classificationtask.
The baseline system marked all sentencesas pros and achieved 53.87% and 50.71% accu-racy for each domain.
All features performedbetter than the baseline but the results are not asgood as in the identification task.
Unlike theidentification task, opinion words by themselvesachieved the best accuracy in both mp3 playerand restaurant domains.
We think opinion wordsplayed more important roles in classifying prosand cons than identifying them.
Position featureshelped recognizing con sentences in mp3 playerreviews.5.2 Experiments on Dataset 2This subsection reports the evaluation results ofour system on Dataset 2.
Since Dataset 2 fromcomplaints.com has no training data, we traineda system on Dataset 1 and applied it to Dataset 2.Table 3: Pros and cons sentences identificationresults on mp3 player reviews.FeaturesusedAcc(%)Prec(%)Recl(%)F-score(%)Op 60.15 65.84 57.31 61.28Lex 76.27 66.18 76.42 70.93Lex+Pos 63.10 71.14 60.72 65.52Lex+Op 62.75 70.64 60.07 64.93Lex+Pos+Op 62.23 70.58 59.35 64.48Baseline 57.75Table 4: Reason sentence identification resultson restaurant reviews.FeaturesusedAcc(%)Prec(%)Recl(%)F-score(%)Op 61.64 60.76 47.48 53.31Lex 63.77 67.10 51.20 58.08Lex+Pos 63.89 67.62 51.70 58.60Lex+Op 61.66 69.13 54.30 60.83Lex+Pos+Op 63.13 66.80 50.41 57.46Baseline 54.82Table 5: Pros and cons sentences classification results for mp3 player reviews.Cons  Pros FeaturesusedAcc(%) Prec(%)Recl(%)F-score(%)Prec(%)Recl(%)F-score(%)Op 57.18 54.43 67.10 60.10 61.18 48.00 53.80Lex 55.88 55.49 67.45 60.89 56.52 43.88 49.40Lex+Pos 55.62 55.26 68.12 61.02 56.24 42.62 48.49Lex+Op 55.60 55.46 64.63 59.70 55.81 46.26 50.59Lex+Pos+Op 56.68 56.70 62.45 59.44 56.65 50.71 53.52baseline 53.87      (mark all as pros)Table 6: Pros and cons sentences classification results for restaurant reviews.Cons Pros FeaturesusedAcc(%) Prec(%)Recl(%)F-score(%)Prec(%)Recl(%)F-score(%)Op 57.32 54.78 51.62 53.15 59.32 62.35 60.80Lex 55.76 55.94 52.52 54.18 55.60 58.97 57.24Lex+Pos 56.07 56.20 53.33 54.73 55.94 58.78 57.33Lex+Op 55.88 56.10 52.39 54.18 55.68 59.34 57.45Lex+Pos+Op 55.79 55.89 53.17 54.50 55.70 58.38 57.01baseline 50.71      (mark all as pros)488A tough question, however, is how to evaluatethe system results.
Since it seemed impossible toevaluate the system without involving a humanjudge, we annotated a small set of data manuallyfor evaluation purposes.Gold Standard Annotation: Four humansannotated 3 sets of test sets: Testset 1 with 5complaints (73 sentences), Testset 2 with 7 com-plaints (105 sentences), and Testset 3 with 6complaints (85 sentences).
Testset 1 and 2 arefrom mp3 player complaints and Testset 3 isfrom restaurant reviews.
Annotators marked sen-tences if they describe specific reasons of thecomplaint.
Each test set was annotated by 2 hu-mans.
The average pair-wise human agreementwas 82.1%7.System Performance: Like the human anno-tators, our system also labeled reason sentences.Since our goal is to identify reason sentences incomplaints, we applied a system modeled as inthe identification phase described in Subsection3.2 instead of the classification phase8.
Table 7reports the accuracy, precision, and recall of thesystem on each test set.
We calculated numbersin each A and B column by assuming each anno-tator?s answers separately as a gold standard.In Table 7, accuracies indicate the agreementbetween the system and human annotators.
Theaverage accuracy 68.0% is comparable with thepair-wise human agreement 82.1% even if thereis still a lot of room for improvement9.
It wasinteresting to see that Testset 3, which was fromrestaurant complaints, achieved higher accuracyand recall than the other test sets from mp3player complaints, suggesting that it would beinteresting to further investigate the performance7 The kappa value was 0.63.8 In complaints reviews, we believe that it is moreimportant to identify reason sentences than to classifybecause most reasons in complaints are likely to becons.9 The baseline system which assigned the majorityclass to each sentence achieved 59.9% of averageaccuracy.of reason identification in various other reviewdomains such as travel and beauty products infuture work.
Also, even though we were some-what able to measure reason sentence identifica-tion in complaint reviews, we agree that we needmore data annotation for more precise evalua-tion.Finally, the followings are examples of sen-tences that our system identified as reasons ofcomplaints.
(1) Unfortunately, I find thatI am no longer comfortable inyour establishment because ofthe unprofessional, rude, ob-noxious, and unsanitary treat-ment from the employees.
(2) They never get my orderright the first time and whatreally disgusts me is how theyhandle the food.
(3) The kids play area atBraum's in The Colony, Texas isvery dirty.
(4) The only complaint that Ihave is that the French friesare usually cold.
(5) The cashier there had shortchanged me on the payment of mybill.As we can see from the examples, our systemwas able to detect con sentences which containedopinion-bearing expressions such as in (1), (2),and (3) as well as reason sentences that mostlydescribed mere facts as in (4) and (5).6 Conclusions and Future workThis paper proposes a framework for identifyingone of the critical elements of online product re-views to answer the question, ?What are reasonsthat the author of a review likes or dislikes theproduct??
We believe that pro and con sentencesin reviews can be answers for this question.
Wepresent a novel technique that automatically la-bels a large set of pro and con sentences in onlinereviews using clue phrases for pros and cons inepinions.com in order to train our system.
Weapplied it to label sentences both on epin-ions.com and complaints.com.
To investigate thereliability of our system, we tested it on two ex-tremely different review domains, mp3 playerreviews and restaurant reviews.
Our system withthe best feature selection performs 71% F-scorein the reason identification task and 61% F-scorein the reason classification task.Table 7: System results on Complaint.comreviews (A, B: The first and the second anno-tator of each set)Testset 1 Testset 2 Testset 3A B A B A BAvgAcc(%) 65.8 63.0 67.6 61.0 77.6 72.9 68.0Prec(%) 50.0 60.7 68.6 62.9 67.9 60.7 61.8Recl(%) 56.0 51.5 51.1 44.0 65.5 58.6 54.5489The experimental results further show that proand con sentences are a mixture of opinions andfacts, making identifying them in online reviewsa distinct problem from opinion sentence identi-fication.
Finally, we also apply the resulting sys-tem to another review data in complaints.com inorder to analyze reasons of consumers?
com-plaints.In the future, we plan to extend our pro andcon identification system on other sorts of opin-ion texts, such as debates about political and so-cial agenda that we can find on blogs or newsgroup discussions, to analyze why people sup-port a specific agenda and why people areagainst it.ReferenceBerger, Adam L., Stephen Della Pietra, and Vin-cent Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing, Computa-tional Linguistics, (22-1).Bethard, Steven, Hong Yu, Ashley Thornton, Va-sileios Hatzivassiloglou, and Dan Jurafsky.2004.
Automatic Extraction of Opinion Proposi-tions and their Holders, AAAI Spring Symposiumon Exploring Attitude and Affect in Text: Theo-ries and Applications.Chklovski, Timothy.
2006.
Deriving QuantitativeOverviews of Free Text Assessments on theWeb.
Proceedings of 2006 International Confer-ence on Intelligent User Interfaces (IUI06).Sydney, Australia.Choi, Y., Cardie, C., Riloff, E., and Patwardhan, S.2005.
Identifying Sources of Opinions withConditional Random Fields and Extraction Pat-terns.
Proceedings of HLT/EMNLP-05.Esuli, Andrea and Fabrizio Sebastiani.
2005.
De-termining the semantic orientation of termsthrough gloss classification.
Proceedings ofCIKM-05, 14th ACM International Conferenceon Information and Knowledge Management,Bremen, DE, pp.
617-624.Hatzivassiloglou, Vasileios and Kathleen McKe-own.
1997.
Predicting the Semantic Orientationof Adjectives.
Proceedings of 35th Annual Meet-ing of the Assoc.
for Computational Linguistics(ACL-97): 174-181Hatzivassiloglou, Vasileios and Janyce Wiebe.2000.
Effects of Adjective Orientation andGradability on Sentence Subjectivity.
Proceed-ings of International Conference on Computa-tional Linguistics (COLING-2000).
Saarbr?cken,Germany.Hu, Minqing and Bing Liu.
2004.
Mining andsummarizing customer reviews".
Proceedings ofthe ACM SIGKDD International Conference onKnowledge Discovery & Data Mining (KDD-2004), Seattle, Washington, USA.Kim, Soo-Min and Eduard Hovy.
2004.
Determin-ing the Sentiment of Opinions.
Proceedings ofCOLING-04.
pp.
1367-1373.
Geneva, Switzer-land.Kim, Soo-Min and Eduard Hovy.
2005.
AutomaticDetection of Opinion Bearing Words and Sen-tences.
In the Companion Volume of the Pro-ceedings of IJCNLP-05, Jeju Island, Republic ofKorea.Kim, Soo-Min and Eduard Hovy.
2006.
Identifyingand Analyzing Judgment Opinions.
Proceedingsof HLT/NAACL-2006, New York City, NY.Lin, Chin-Yew and Eduard Hovy.
1997.Identifying Topics by Position.
Proceedings ofthe 5th Conference on Applied Natural Lan-guage Processing (ANLP97).
Washington, D.C.Pang, Bo, Lillian Lee, and Shivakumar Vaithyana-than.
2002.
Thumbs up?
Sentiment Classifica-tion using Machine Learning Techniques, Pro-ceedings of EMNLP 2002.Popescu, Ana-Maria, and Oren Etzioni.
2005.Extracting Product Features and Opinions fromReviews , Proceedings of HLT-EMNLP 2005.Riloff, Ellen, Janyce Wiebe, and Theresa Wilson.2003.
Learning Subjective Nouns Using Extrac-tion Pattern Bootstrapping.
Proceedings of Sev-enth Conference on Natural Language Learning(CoNLL-03).
ACL SIGNLL.
Pages 25-32.Turney, Peter D. 2002.
Thumbs up or thumbsdown?
Semantic orientation applied to unsuper-vised classification of reviews, Proceedings ofACL-02, Philadelphia, Pennsylvania, 417-424Wiebe, Janyce M., Bruce, Rebecca F., and O'Hara,Thomas P. 1999.
Development and use of a goldstandard data set for subjectivity classifications.Proceedings of ACL-99.
University of Maryland,June, pp.
246-253.Wilson, Theresa, Janyce Wiebe, and Paul Hoff-mann.
2005.
Recognizing Contextual Polarity inPhrase-Level Sentiment Analysis.
Proceedingsof HLT/EMNLP 2005, Vancouver, CanadaWilson, Theresa, Janyce Wiebe, and Rebecca Hwa.2004.
Just how mad are you?
Finding strong andweak opinion clauses.
Proceedings of 19th Na-tional Conference on Artificial Intelligence(AAAI-2004).490
