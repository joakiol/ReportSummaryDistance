Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1?11,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Word-Class Approach to Labeling PSCFG Rules for Machine TranslationAndreas Zollmann and Stephan VogelLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{zollmann,vogel+}@cs.cmu.eduAbstractIn this work we propose methods to labelprobabilistic synchronous context-free gram-mar (PSCFG) rules using only word tags,generated by either part-of-speech analysisor unsupervised word class induction.
Theproposals range from simple tag-combinationschemes to a phrase clustering model that canincorporate an arbitrary number of features.Our models improve translation quality overthe single generic label approach of Chiang(2005) and perform on par with the syntacti-cally motivated approach from Zollmann andVenugopal (2006) on the NIST large Chinese-to-English translation task.
These results per-sist when using automatically learned wordtags, suggesting broad applicability of ourtechnique across diverse language pairs forwhich syntactic resources are not available.1 IntroductionThe Probabilistic Synchronous Context Free Gram-mar (PSCFG) formalism suggests an intuitive ap-proach to model the long-distance and lexically sen-sitive reordering phenomena that often occur acrosslanguage pairs considered for statistical machinetranslation.
As in monolingual parsing, nonterminalsymbols in translation rules are used to generalizebeyond purely lexical operations.
Labels on thesenonterminal symbols are often used to enforce syn-tactic constraints in the generation of bilingual sen-tences and imply conditional independence assump-tions in the translation model.
Several techniqueshave been recently proposed to automatically iden-tify and estimate parameters for PSCFGs (or relatedsynchronous grammars) from parallel corpora (Gal-ley et al, 2004; Chiang, 2005; Zollmann and Venu-gopal, 2006; Liu et al, 2006; Marcu et al, 2006).While all of these techniques rely on word-alignments to suggest lexical relationships, they dif-fer in the way in which they assign labels to non-terminal symbols of PSCFG rules.
Chiang (2005)describes a procedure to extract PSCFG rules fromword-aligned (Brown et al, 1993) corpora, whereall nonterminals share the same generic label X .
InGalley et al (2004) and Marcu et al (2006), tar-get language parse trees are used to identify rulesand label their nonterminal symbols, while Liu et al(2006) use source language parse trees instead.
Zoll-mann and Venugopal (2006) directly extend the ruleextraction procedure from Chiang (2005) to heuristi-cally label any phrase pair based on target languageparse trees.
Label-based approaches have resultedin improvements in translation quality over the sin-gleX label approach (Zollmann et al, 2008; Mi andHuang, 2008); however, all the works cited here relyon stochastic parsers that have been trained on man-ually created syntactic treebanks.
These treebanksare difficult and expensive to produce and exist for alimited set of languages only.In this work, we propose a labeling approach thatis based merely on part-of-speech analysis of thesource or target language (or even both).
To-wards the ultimate goal of building end-to-end ma-chine translation systems without any human anno-tations, we also experiment with automatically in-ferred word classes using distributional clustering(Kneser and Ney, 1993).
Since the number of classesis a parameter of the clustering method and the re-sulting nonterminal size of our grammar is a func-tion of the number of word classes, the PSCFGgrammar complexity can be adjusted to the specifictranslation task at hand.Finally, we introduce a more flexible labeling ap-proach based on K-means clustering, which allows1the incorporation of an arbitrary number of word-class based features, including phrasal contexts, canmake use of multiple tagging schemes, and also al-lows non-class features such as phrase sizes.2 PSCFG-based translationIn this work we experiment with PSCFGs that havebeen automatically learned from word-aligned par-allel corpora.
PSCFGs are defined by a source ter-minal set (source vocabulary) TS , a target terminalset (target vocabulary) TT , a shared nonterminal setN and rules of the form: A?
?
?, ?,w?
where?
A ?
N is a labeled nonterminal referred to as theleft-hand-side of the rule,?
?
?
(N ?
TS)?
is the source side of the rule,?
?
?
(N ?
TT )?
is the target side of the rule,?
w ?
[0,?)
is a non-negative real-valued weightassigned to the rule; in our model,w is the productof features ?i raised to the power of weight ?i.Chiang (2005) learns a single-nonterminal PSCFGfrom a bilingual corpus by first identifying initialphrase pairs using the technique from Koehn et al(2003), and then performing a generalization opera-tion to generate phrase pairs with gaps, which can beviewed as PSCFG rules with generic ?X?
nontermi-nal left-hand-sides and substitution sites.
Bilingualfeatures ?i that judge the quality of each rule are es-timated based on rule extraction frequency counts.3 Hard rule labeling from word classesWe now describe a simple method of inducing amulti-nonterminal PSCFG from a parallel corpuswith word-tagged target side sentences.
The sameprocedure can straightforwardly be applied to a cor-pus with tagged source side sentences.
We use thesimple term ?tag?
to stand for any kind of word-levelanalysis?a syntactic, statistical, or other means ofgrouping word types or tokens into classes, possiblybased on their position and context in the sentence,POS tagging being the most obvious example.As in Chiang?s hierarchical system, we rely onan external phrase-extraction procedure such as theone of Koehn et al (2003) to provide us with a setof phrase pairs for each sentence pair in the train-ing corpus, annotated with their respective start andend positions in the source and target sentences.Let f = f1 ?
?
?
fm be the current source sentence,e = e1 ?
?
?
en the current target sentence, and t =t1 ?
?
?
tn its corresponding target tag sequence.
Weconvert each extracted phrase pair, represented byits source span ?i, j?
and target span ?k, `?, into aninitial ruletk-t` ?
fi ?
?
?
fj | ek ?
?
?
e`by assigning it a nonterminal ?tk-t`?
constructed bycombining the tag of the target phrase?s left-mostword with the tag of its right-most word.The creation of complex rules based on all initialrules obtained from the current sentence now pro-ceeds just as in Chiang?s model.Consider the target-tagged example sentence pair:Ich habe ihn gesehen | I/PRP saw/VBD him/PRPThen (depending on the extracted phrase pairs), theresulting initial rules could be:1: PRP-PRP?
Ich | I2: PRP-PRP?
ihn | him3: VBD-VBD?
gesehen | saw4: VBD-PRP?
habe ihn gesehen | saw him5: PRP-PRP?
Ich habe ihn gesehen | I saw himNow, by abstracting-out initial rule 2 from initialrule 4, we obtain the complex rule:VBD-PRP?
habe PRP-PRP1 gesehen | saw PRP-PRP1Intuitively, the labeling of initial rules with tagsmarking the boundary of their target sides results incomplex rules whose nonterminal occurrences im-pose weak syntactic constraints on the rules eligi-ble for substitution in a PSCFG derivation: The leftand right boundary word tags of the inserted rule?starget side have to match the respective boundaryword tags of the phrase pair that was replaced bya nonterminal when the complex rule was createdfrom a training sentence pair.
Since consecutivewords within a rule stem from consecutive words inthe training corpus and thus are already consistent,the boundary word tags are more informative thantags of words between the boundaries for the taskof combining different rules in a derivation, and aretherefore a more appropriate choice for the creationof grammar labels than tags of inside words.Accounting for phrase size A drawback of thecurrent approach is that a single-word rule such asPRP-PRP?
Ich | I2can have the same left-hand-side nonterminal as along rule with identical left and right boundary tags,such as (when using target-side tags):PRP-PRP?
Ich habe ihn gesehen | I saw himWe therefore introduce a means of distinguishingbetween one-word, two-word, and multiple-wordphrases as follows: Each one-word phrase with tagT simply receives the label T , instead of T -T .
Two-word phrases with tag sequence T1T2 are labeledT1-T2 as before.
Phrases of length greater two withtag sequence T1 ?
?
?Tn are labeled T1..Tn to denotethat tags were omitted from the phrase?s tag se-quence.
The resulting number of grammar nonter-minals based on a tag vocabulary of size t is thusgiven by 2t2 + t.An alternative way of accounting for phrase sizeis presented by Chiang et al (2008), who intro-duce structural distortion features into a hierarchi-cal phrase-based model, aimed at modeling nonter-minal reordering given source span length.
Ourapproach instead uses distinct grammar rules andlabels to discriminate phrase size, with the advan-tage of enabling all translation models to estimatedistinct weights for distinct size classes and avoid-ing the need of additional models in the log-linearframework; however, the increase in the number oflabels and thus grammar rules decreases the relia-bility of estimated models for rare events due to in-creased data sparseness.Extension to a bilingually tagged corpus Whilethe availability of syntactic annotations for bothsource and target language is unlikely in most trans-lation scenarios, some form of word tags, be it part-of-speech tags or learned word clusters (cf.
Sec-tion 3) might be available on both sides.
In this case,our grammar extraction procedure can be easily ex-tended to impose both source and target constraintson the eligible substitutions simultaneously.Let Nf be the nonterminal label that would beassigned to a given initial rule when utilizing thesource-side tag sequence, and Ne the assigned la-bel according to the target-side tag sequence.
Thenour bilingual tag-based model assigns ?Nf + Ne?to the initial rule.
The extraction of complex rulesproceeds as before.
The number of nonterminalsin this model, based on a source tag vocabulary ofsize s and a target tag vocabulary of size t, is thusgiven by s2t2 for the regular labeling method and(2s2 + s)(2t2 + t) when accounting for phrase size.Consider again our example sentence pair (nowalso annotated with source-side part-of-speech tags):Ich/PRP habe/AUX ihn/PRP gesehen/VBNI/PRP saw/VBD him/PRPGiven the same phrase extraction method as before,the resulting initial rules for our bilingual model,when also accounting for phrase size, are as follows:1: PRP+PRP?
Ich | I2: PRP+PRP?
ihn | him3: VBN+VBD?
gesehen | saw4: AUX..VBN+VBD-PRP ?
habe ihngesehen | saw him5: PRP..VBN+PRP..PRP ?
Ich habe ihngesehen | I saw himAbstracting-out rule 2 from rule 4, for instance,leads to the complex rule:AUX..VBN+VBD-PRP ?
habe PRP+PRP1gesehen | saw PRP+PRP1Unsupervised word class assignment by cluster-ing As an alternative to POS tags, we experimentwith unsupervised word clustering methods basedon the exchange algorithm (Kneser and Ney, 1993).Its objective function is maximizing the likelihoodn?i=1P (wi|w1, .
.
.
, wi?1)of the training data w = w1, .
.
.
, wn given a par-tially class-based bigram model of the formP (wi|w1, .
.
.
, wi?1) ?
p(c(wi)|wi?1) ?p(wi|c(wi))where c : V ?
{1, .
.
.
, N} maps a word (type, nottoken) w to its class c(w), V is the vocabulary, andN the fixed number of classes, which has to be cho-sen a priori.
We use the publicly available imple-mentation MKCLS (Och, 1999) to train this model.As training data we use the respective side of theparallel training data for the translation system.We also experiment with the extension of thismodel by Clark (2003), who incorporated morpho-logical information by imposing a Bayesian prioron the class mapping c, based on N individual dis-tributions over strings, one for each word class.Each such distribution is a character-based hiddenMarkov model, thus encouraging the grouping ofmorphologically similar words into the same class.34 Clustering phrase pairs directly usingthe K-means algorithmEven though we have only made use of the first andlast words?
classes in the labeling methods describedso far, the number of resulting grammar nontermi-nals quickly explodes.
Using a scheme based onsource and target phrases with accounting for phrasesize, with 36 word classes (the size of the Penn En-glish POS tag set) for both languages, yields a gram-mar with (36+2?362)2 = 6.9m nonterminal labels.Quite plausibly, phrase labeling should be in-formed by more than just the classes of the first andlast words of the phrase.
Taking phrase context intoaccount, for example, can aid the learning of syn-tactic properties: a phrase beginning with a deter-miner and ending with a noun, with a verb as rightcontext, is more likely to be a noun phrase than thesame phrase with another noun as right context.
Inthe current scheme, there is no way of distinguish-ing between these two cases.
Similarly, it is con-ceivable that using non-boundary words inside thephrase might aid the labeling process.When relying on unsupervised learning of theword classes, we are forced to chose a fixed num-ber of classes.
A smaller number of word clusterswill result in smaller number of grammar nonter-minals, and thus more reliable feature estimation,while a larger number has the potential to discovermore subtle syntactic properties.
Using multipleword clusterings simultaneously, each based on adifferent number of classes, could turn this global,hard trade-off into a local, soft one, informed by thenumber of phrase pair instances available for a givengranularity.Lastly, our method of accounting for phrase sizeis somewhat displeasing: While there is a hard par-titioning of one-word and two-word phrases, no dis-tinction is made between phrases of length greaterthan two.
Marking phrase sizes greater than twoexplicitly by length, however, would create manysparse, low-frequency rules, and one of the strengthsof PSCFG-based translation is the ability to sub-stitute flexible-length spans into nonterminals of aderivation.
A partitioning where phrase size is in-stead merely a feature informing the labeling pro-cess seems more desirable.We thus propose to represent each phrase pair in-stance (including its bilingual one-word contexts) asfeature vectors, i.e., points of a vector space.
Wethen use these data points to partition the space intoclusters, and subsequently assign each phrase pairinstance the cluster of its corresponding feature vec-tor as label.The feature mapping Consider the phrase pair in-stance(f0)f1 ?
?
?
fm(fm+1) | (e0)e1 ?
?
?
en(en+1)(where f0, fm+1, e0, en+1 are the left and right,source and target side contexts, respectively).
Webegin with the case of only a single, target-sideword class scheme (either a tagger or an unsuper-vised word clustering/POS induction method).
LetC = {c1, .
.
.
, cN} be its set of word classes.
Fur-ther, let c0 be a short-hand for the result of lookingup the class of a word that is out of bounds (e.g., theleft context of the first word of a sentence, or the sec-ond word of a one-word phrase).
We now map ourphrase pair instance to the real-valued vector (where1[P ] is the indicator function defined as 1 if propertyP is true, and 0 otherwise):?1[e1=c0], .
.
.
,1[e1=cN ],1[en=c0], .
.
.
,1[en=cN ],?sec1[e2=c0], .
.
.
, ?sec1[e2=cN ],?sec1[en?1=c0], .
.
.
, ?sec1[en?1=cN ],?ins?ni=1 1[ei=c0]n, .
.
.
,?ins?ni=1 1[ei=cN ]n,?cntxt1[e0=c0], .
.
.
, ?cntxt1[e0=cN ],?cntxt1[en+1=c0], .
.
.
, ?cntxt1[en+1=cN ],?phrsize?N + 1 log10(n)?The ?
parameters determine the influence of the dif-ferent types of information.
The elements in the firstline represent the phrase boundary word classes, thenext two lines the classes of the second and penul-timate word, followed by a line representing the ac-cumulated contents of the whole phrase, followed bytwo lines pertaining to the context word classes.
Thefinal element of the vector is proportional to the log-arithm of the phrase length.1 We chose the logarithmassuming that length deviation of syntactic phrasalunits is not constant, but proportional to the averagelength.
Thus, all other features being equal, the dis-tance between a two-word and a four-word phrase is1The?N + 1 factor serves to make the feature?s influence in-dependent of the number of word classes by yielding the samedistance (under L2) as N + 1 identical copies of the feature.4the same as the distance between a four-word and aneight-word phrase.We will mainly use the Euclidean (L2) distance tocompare points for clustering purposes.
Our featurespace is thus the Euclidean vector space R7N+8.To additionally make use of source-side wordclasses, we append elements analogous to the onesabove to the vector, all further multiplied by a pa-rameter ?src that allows trading off the relevanceof source-side and target-side information.
In thesame fashion, we can incorporate multiple taggingschemes (e.g., word clusterings of different gran-ularities) into the same feature vector.
As finer-grained schemes have more elements in the fea-ture vector than coarser-grained ones, and thus ex-ert more influence, we set the ?
parameter for eachscheme to 1/N (where N is the number of wordclasses of the scheme).The K-means algorithm To create the clusters,we chose the K-means algorithm (Steinhaus, 1956;MacQueen, 1967) for both its computational effi-ciency and ease of implementation and paralleliza-tion.
Given an initial mapping from the data pointsto K clusters, the procedure alternates between (i)computing the centroid of each cluster and (ii) re-allocating each data point to the closest cluster cen-troid, until convergence.We implemented two commonly used initializa-tion methods: Forgy and Random Partition.
TheForgy method randomly chooses K observationsfrom the data set and uses these as the initial means.The Random Partition method first randomly as-signs a cluster to each observation and then proceedsstraight to step (ii).
Forgy tends to spread the ini-tial means out, while Random Partition places allof them close to the center of the data set.
As theresulting clusters looked similar, and Random Parti-tion sometimes led to a high rate of empty clusters,we settled for Forgy.5 ExperimentsWe evaluate our approach by comparing translationquality, as evaluated by the IBM-BLEU (Papineniet al, 2002) metric on the NIST Chinese-to-Englishtranslation task using MT04 as development set totrain the model parameters ?, and MT05, MT06 andMT08 as test sets.
Even though a key advantageof our method is its applicability to resource-poorlanguages, we used a language pair for which lin-guistic resources are available in order to determinehow close translation performance can get to a fullysyntax-based system.
Accordingly, we use Chiang?shierarchical phrase based translation model (Chiang,2007) as a base line, and the syntax-augmented MTmodel (Zollmann and Venugopal, 2006) as a ?targetline?, a model that would not be applicable for lan-guage pairs without linguistic resources.We perform PSCFG rule extraction and decodingusing the open-source ?SAMT?
system (Venugopaland Zollmann, 2009), using the provided implemen-tations for the hierarchical and syntax-augmentedgrammars.
Apart from the language model, the lex-ical, phrasal, and (for the syntax grammar) label-conditioned features, and the rule, target word,and glue operation counters, Venugopal and Zoll-mann (2009) also provide both the hierarchical andsyntax-augmented grammars with a rareness penalty1/ cnt(r), where cnt(r) is the occurrence count ofrule r in the training corpus, allowing the system tolearn penalization of low-frequency rules, as well asthree indicator features firing if the rule has one, twounswapped, and two swapped nonterminal pairs, re-spectively.2 Further, to mitigate badly estimatedPSCFG derivations based on low-frequency rules ofthe much sparser syntax model, the syntax grammaralso contains the hierarchical grammar as a back-bone (cf.
Zollmann and Vogel (2010) for details andempirical analysis).We implemented our rule labeling approachwithin the SAMT rule extraction pipeline, resultingin comparable features across all systems.
For allsystems, we use the bottom-up chart parsing decoderimplemented in the SAMT toolkit with a reorder-ing limit of 15 source words, and correspondinglyextract rules from initial phrase pairs of maximumsource length 15.
All rules have at most two non-terminal symbols, which must be non-consecutiveon the source side, and rules must contain at leastone source-side terminal symbol.
The beam set-tings for the hierarchical system are 600 items per?X?
(generic rule) cell, and 600 per ?S?
(glue) cell.3Due to memory limitations, the multi-nonterminalgrammars have to be pruned more harshly: We al-2Penalization or reward of purely-lexical rules can be indirectlylearned by trading off these features with the rule counter fea-ture.3For comparison, Chiang (2007) uses 30 and 15, respectively,and further prunes items that deviate too much in score fromthe best item.
He extracts initial phrases of maximum length10.5low 100 ?S?
items, and a total of 500 non-?S?
items,but maximally 40 items per nonterminal.
For all sys-tems, we further discard non-initial rules occurringonly once.4 For the multi-nonterminal systems, wegenerally further discard all non-generic non-initialrules occurring less than 6 times, but we additionallygive results for a ?slow?
version of the Syntax target-line system and our best word class based systems,where only single-occurrences were removed.For parameter tuning, we use the L0-regularizedminimum-error-rate training tool provided by theSAMT toolkit.
Each system is trained separately toadapt the parameters to its specific properties (sizeof nonterminal set, grammar complexity, featuressparseness, reliance on the language model, etc.
).The parallel training data comprises of 9.6Msentence pairs (206M Chinese and 228M Englishwords).
The source and target language parses forthe syntax-augmented grammar, as well as the POStags for our POS-based grammars were generated bythe Stanford parser (Klein and Manning, 2003).The results are given in Table 1.
Results for theSyntax system are consistent with previous results(Zollmann et al, 2008), indicating improvementsover the hierarchical system.
Our approach, usingtarget POS tags (?POS-tgt (no phr.
s.)?
), outper-forms the hierarchical system on all three tests sets,and gains further improvements when accountingfor phrase size (?POS-tgt?).
The latter approach isroughly on par with the corresponding Syntax sys-tem, slightly outperforming it on average, but notconsistently across all test sets.
The same is true forthe ?slow?
version (?POS-tgt-slow?
).The model based on bilingually tagged traininginstances (?POS-src&tgt?)
does not gain further im-provements over the merely target-based one, butactually performs worse.
We assume this is due tothe huge number of nonterminals of ?POS-src&tgt?
((2 ?
332 + 33)(2 ?
362 + 36) = 5.8M in princi-ple) compared to ?POS-tgt?
(2 ?
362 + 36 = 2628),increasing the sparseness of the grammar and thusleading to less reliable statistical estimates.We also experimented with a source-tag basedmodel (?POS-src?).
In line with previous findingsfor syntax-augmented grammars (Zollmann and Vo-gel, 2010), the source-side-based grammar does notreach the translation quality of its target-based coun-terpart; however, the model still outperforms the hi-4As shown in Zollmann et al (2008), the impact of these ruleson translation quality is negligible.erarchical system on all test sets.
Further, decod-ing is much faster than for ?POS-ext-tgt?
and evenslightly faster than ?Hierarchical?.
This is due tothe fact that for the source-tag based approach, agiven chart cell in the CYK decoder, represented bya start and end position in the source sentence, al-most uniquely determines the nonterminal any hy-pothesis in this cell can have: Disregarding part-of-speech tag ambiguity and phrase size accounting,that nonterminal will be the composition of the tagsof the start and end source words spanned by thatcell.
At the same time, this demonstrates that thereis hence less of a role for the nonterminal labels toresolve translational ambiguity in the source basedmodel than in the target based model.Performance of the word-clustering based mod-els To empirically validate the unsupervised clus-tering approaches, we first need to decide how to de-termine the number of word classes, N .
A straight-forward approach is to run experiments and reporttest set results for many different N .
While thiswould allow us to reliably conclude the optimalnumber N , a comparison of that best-performingclustering method to the hierarchical, syntax, andPOS systems would be tainted by the fact that Nwas effectively tuned on the test sets.
We there-fore chooseN merely based on development set per-formance.
Unfortunately, variance in developmentset BLEU scores tends to be higher than test setscores, despite of SAMT MERT?s inbuilt algorithmsto overcome local optima, such as random restartsand zeroing-out.
We have noticed that using an L0-penalized BLEU score5 as MERT?s objective on themerged n-best lists over all iterations is more stableand will therefore use this score to determine N .Figure 1 (left) shows the performance of thedistributional clustering model (?Clust?)
and itsmorphology-sensitive extension (?Clust-morph?)
ac-cording to this score for varying values of N =1, .
.
.
, 36 (the number Penn treebank POS tags, usedfor the ?POS?
models, is 36).6 For ?Clust?, we see acomfortably wide plateau of nearly-identical scoresfrom N = 7, .
.
.
, 15.
Scores for ?Clust-morph?
arelower throughout, and peak at N = 7.Looking back at Table 1, we now compare theclustering models chosen by the procedure above?5Given by: BLEU??
?
|{i ?
{1, .
.
.
,K}|?i 6= 0}|, where?1, .
.
.
, ?K are the feature weights and the constant ?
(whichwe set to 0.00001) is the regularization penalty.6All these models account for phrase size.6Dev (MT04) MT05 MT06 MT08 TestAvg TimeHierarchical 38.63 36.51 33.26 25.77 31.85 14.3Syntax 39.39 37.09 34.01 26.53 32.54 18.1Syntax-slow 39.69 37.56 34.66 26.93 33.05 34.6POS-tgt (no phr.
s.) 39.31 37.29 33.79 26.13 32.40 27.7POS-tgt 39.14 37.29 33.97 26.77 32.68 19.2POS-src 38.74 36.75 33.85 26.76 32.45 12.2POS-src&tgt 38.78 36.71 33.65 26.52 32.29 18.8POS-tgt-slow 39.86 37.78 34.37 27.14 33.10 44.6Clust-7-tgt 39.24 36.74 34.00 26.93 32.56 24.3Clust-7-morph-tgt 39.08 36.57 33.81 26.40 32.26 23.6Clust-7-src 38.68 36.17 33.23 26.55 31.98 11.1Clust-7-src&tgt 38.71 36.49 33.65 26.33 32.16 15.8Clust-7-tgt-slow 39.48 37.70 34.31 27.24 33.08 45.2kmeans-POS-src&tgt 39.11 37.23 33.92 26.80 32.65 18.5kmeans-POS-src&tgt-L1 39.33 36.92 33.81 26.59 32.44 17.6kmeans-POS-src&tgt-cosine 39.15 37.07 33.98 26.68 32.58 17.7kmeans-POS-src&tgt (?ins = .5) 39.07 36.88 33.71 26.26 32.28 16.5kmeans-Clust-7-src&tgt 39.19 36.96 34.26 26.97 32.73 19.3kmeans-Clust-7..36-src&tgt 39.09 36.93 34.24 26.92 32.70 17.3kmeans-POS-src&tgt-slow 39.28 37.16 34.38 27.11 32.88 36.3kmeans-Clust-7..36-s&t-slow 39.18 37.12 34.13 27.35 32.87 34.3Table 1: Translation quality in % case-insensitive IBM-BLEU (i.e., brevity penalty based on closest reference length)for Chinese-English NIST-large translation tasks, comparing baseline Hierarchical and Syntax systems with POS andclustering based approaches proposed in this work.
?TestAvg?
shows the average score over the three test sets.
?Time?is the average decoding time per sentence in seconds on one CPU.resulting in N = 7 for the morphology-unawaremodel (?Clust-7-tgt?)
as well as the morphology-aware model (?Clust-7-morph-tgt?
)?to the othersystems.
?Clust-7-tgt?
improves over the hierarchi-cal base line on all three test sets and is on parwith the corresponding Syntax and POS target lines.The same holds for the ?Clust-7-tgt-slow?
version.We also experimented with a model variant basedon seven source and seven target language clusters(?Clust-7-src&tgt?)
and a source-only labeled model(?Clust-7-src?
)?both performing worse.Surprisingly, the morphology-sensitive cluster-ing model (?Clust-7-morph-tgt?
), while still improv-ing over the hierarchical system, performs worsethan the morphology-unaware model.
An in-spection of the trained word clusters showed thatthe model, while far superior to the morphology-unaware model in e.g.
mapping all numbers tothe same class, is overzealous in discovering mor-phological regularities (such as the ?-ed?
suffix) topartition functionally only slightly dissimilar words(such present-tense and past-tense verbs) into dif-ferent classes.
While these subtle distinctions makefor good partitionings when the number of clustersis large, they appear to lead to inferior results forour task that relies on coarse-grained partitioningsof the vocabulary.
Note that there are no ?src?
or?src&tgt?
systems for ?Clust-morph?, as Chinese, be-ing a monosyllabic writing system, does not lend it-self to morphology-sensitive clustering.K-means clustering based models To establishsuitable values for the ?
parameters and investigatethe impact of the number of clusters, we looked atthe development performance over various param-eter combinations for a K-means model based onsource and/or target part-of-speech tags.7 As canbe seen from Figure 1 (right), our method reachesits peak performance at around 50 clusters and thenlevels off slightly.
Encouragingly, in contrast tothe hard labeling procedure, K-means actually im-proves when adding source-side information.
Theoptimal ratio of weighting source and target classesis 0.5:1, corresponding to ?src = .5.
Incorporat-ing context information also helps, and does best for?cntxt = 0.25, i.e.
when giving contexts 1/4 the in-fluence of the phrase boundary words.7We set ?sec = .25, ?ins = 0, and ?phrsize = .5 throughout.7Figure 1: Left: Performance of the distributional clustering model ?Clust?
and its morphology-sensitive extension?Clust-morph?
according to L0-penalized development set BLEU score for varying numbers N of word classes.
Foreach data point N , its corresponding n.o.
nonterminals of the induced grammar is stated in parentheses.Right: Dev.
set performance of K-means for various n.o.
labels and values of ?src and ?cntxt.Entry ?kmeans-POS-src&tgt?
in Table 1 showsthe test set results for the development-set best K-means configuration (i.e., ?src = .5, ?cntxt = 0.25,and using 500 clusters).
While beating the hier-archical baseline, it is only minimally better thanthe much simpler target-based hard labeling method?POS-tgt?.
We also tried K-means variants in whichthe Euclidean distance metric is replaced by thecity block distance L1 and the cosine dissimilarity,respectively, with slightly worse outcomes.
Con-figuration ?kmeans-POS-src&tgt (?ins = .5)?
in-vestigates the incorporation of non-boundary wordtags inside the phrase.
Unfortunately, these featuresappear to deteriorate performance, presumably be-cause given a fixed number of clusters, accountingfor contents inside the phrase comes at the cost ofneglect of boundary words, which are more relevantto producing correctly reordered translations.The two completely unsupervised systems?kmeans-Clust-7-src&tgt?
(based on 7-classMKCLS distributional word clustering) and?kmeans-Clust-7..36-src&tgt?
(using six differentword clustering models simultaneously: all theMKCLS models from Figure 1 (left) except for thetwo-, three- and five-class models) have the bestresults, outperforming the other K-means models aswell as ?Syntax?
and ?POS-tgt?
on average, but noton all test sets.Lastly, we give results for ?slow?
K-means config-urations (?kmeans-POS-src&tgt-slow?
and ?kmeans-Clust-7..36-s&t-slow?).
Unfortunately (or fortu-nately, from a pragmatic viewpoint), the models areoutperformed by the much simpler ?POS-tgt-slow?and ?Clust-7-tgt-slow?
models.6 Related workHassan et al (2007) improve the statistical phrase-based MT model by injecting supertags, lexical in-formation such as the POS tag of the word and itssubcategorization information, into the phrase table,resulting in generalized phrases with placeholders inthem.
The supertags are also injected into the lan-guage model.
Our approach also generates phraselabels and placeholders based on word tags (albeitin a different manner and without the use of subcat-egorization information), but produces PSCFG rulesfor use in a parsing-based decoding system.Unsupervised synchronous grammar induction,apart from the contribution of Chiang (2005) dis-cussed earlier, has been proposed by Wu (1997) forinversion transduction grammars, but as Chiang?smodel only uses a single generic nonterminal la-bel.
Blunsom et al (2009) present a nonparamet-ric PSCFG translation model that directly inducesa grammar from parallel sentences without the useof or constraints from a word-alignment model, and8Cohn and Blunsom (2009) achieve the same fortree-to-string grammars, with encouraging resultson small data.
Our more humble approach treatsthe training sentences?
word alignments and phrasepairs, obtained from external modules, as groundtruth and employs a straight-forward generalizationof Chiang?s popular rule extraction approach to la-beled phrase pairs, resulting in a PSCFG with mul-tiple nonterminal labels.Our phrase pair clustering approach is similar inspirit to the work of Lin and Wu (2009), who use K-means to cluster (monolingual) phrases and use theresulting clusters as features in discriminative clas-sifiers for a named-entity-recognition and a queryclassification task.
Phrases are represented in termsof their contexts, which can be more than one wordlong; words within the phrase are not considered.Further, each context contributes one dimension pervocabulary word (not per word class as in our ap-proach) to the feature space, allowing for the dis-covery of subtle semantic similarities in the phrases,but at much greater computational expense.
Anotherdistinction is that Lin and Wu (2009) work withphrase types instead of phrase instances, obtaininga phrase type?s contexts by averaging the contextsof all its phrase instances.Nagata et al (2006) present a reordering modelfor machine translation, and make use of clusteredphrase pairs to cope with data sparseness in themodel.
They achieve the clustering by reducingphrases to their head words and then applying theMKCLS tool to these pseudo-words.Kuhn et al (2010) cluster the phrase pairs ofan SMT phrase table based on their co-occurrencecounts and edit distances in order to arrive at seman-tically similar phrases for the purpose of phrase tablesmoothing.
The clustering proceeds in a bottom-upfashion, gradually merging similar phrases while al-ternating back and forth between the two languages.7 Conclusion and discussionIn this work we proposed methods of labeling phrasepairs to create automatically learned PSCFG rulesfor machine translation.
Crucially, our methods onlyrely on ?shallow?
lexical tags, either generated byPOS taggers or by automatic clustering of words intoclasses.
Evaluated on a Chinese-to-English transla-tion task, our approach improves translation qual-ity over a popular PSCFG baseline?the hierarchi-cal model of Chiang (2005) ?and performs on parwith the model of Zollmann and Venugopal (2006),using heuristically generated labels from parse trees.Using automatically obtained word clusters insteadof POS tags yields essentially the same results, thusmaking our methods applicable to all languagespairs with parallel corpora, whether syntactic re-sources are available for them or not.We also propose a more flexible way of obtainingthe phrase labels from word classes using K-meansclustering.
While currently the simple hard-labelingmethods perform just as well, we hope that the easeof incorporating new features into the K-means la-beling method will spur interesting future research.When considering the constraints and indepen-dence relationships implied by each labeling ap-proach, we can distinguish between approaches thatlabel rules differently within the context of the sen-tence that they were extracted from, and those thatdo not.
The Syntax system from Zollmann andVenugopal (2006) is at one end of this extreme.
Agiven target span might be labeled differently de-pending on the syntactic analysis of the sentencethat it is a part of.
On the other extreme, the clus-tering based approach labels phrases based on thecontained words alone.8 The POS grammar repre-sents an intermediate point on this spectrum, sincePOS tags can change based on surrounding words inthe sentence; and the position of the K-means modeldepends on the influence of the phrase contexts onthe clustering process.
Context insensitive labelinghas the advantage that there are less alternative left-hand-side labels for initial rules, producing gram-mars with less rules, whose weights can be moreaccurately estimated.
This could explain the strongperformance of the word-clustering based labelingapproach.All source code underlying this work is availableunder the GNU Lesser General Public License aspart of the Hadoop-based ?SAMT?
system at:www.cs.cmu.edu/?zollmann/samtAcknowledgmentsWe thank Jakob Uszkoreit and Ashish Venugopal forhelpful comments and suggestions and Yahoo!
forthe access to the M45 supercomputing cluster.8Note, however, that the creation of clusters itself did take thecontext of the clustered words into account.9ReferencesPhil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A Gibbs sampler for phrasal syn-chronous grammar induction.
In Proceedings of ACL,Singapore, August.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Computational Linguistics, 19(2).David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the 2008Conference on Empirical Methods in Natural Lan-guage Processing, Honolulu, Hawaii, October.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe Annual Meeting of the Association for Computa-tional Linguistics (ACL).David Chiang.
2007.
Hierarchical phrase based transla-tion.
Computational Linguistics, 33(2).Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of the European chapter of theAssociation for Computational Linguistics (EACL),pages 59?66.Trevor Cohn and Phil Blunsom.
2009.
A Bayesian modelof syntax-directed tree to string grammar induction.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),Singapore.Michael Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proceedings of the Human Language TechnologyConference of the North American Chapter of the As-sociation for Computational Linguistics Conference(HLT/NAACL).Hany Hassan, Khalil Sima?an, and Andy Way.
2007.
Su-pertagged phrase-based statistical machine translation.In Proceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, Prague, CzechRepublic, June.Dan Klein and Christoper Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the AnnualMeeting of the Association for Computational Linguis-tics (ACL).Reinhard Kneser and Hermann Ney.
1993.
Improvedclustering techniques for class-based statistical lan-guage modelling.
In Proceedings of the 3rd EuropeanConference on Speech Communication and Technol-ogy, pages 973?976, Berlin, Germany.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofthe Human Language Technology Conference of theNorth American Chapter of the Association for Com-putational Linguistics Conference (HLT/NAACL).Roland Kuhn, Boxing Chen, George Foster, and EvanStratford.
2010.
Phrase clustering for smoothingTM probabilities - or, how to extract paraphrases fromphrase tables.
In Proceedings of the 23rd Interna-tional Conference on Computational Linguistics (Col-ing 2010), pages 608?616, Beijing, China, August.Dekang Lin and Xiaoyun Wu.
2009.
Phrase clusteringfor discriminative learning.
In Proceedings of the 47thAnnual Meeting of the Association for ComputationalLinguistics (ACL).Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thannual meeting of the Association for ComputationalLinguistics.J.
B. MacQueen.
1967.
Some methods for classificationand analysis of multivariate observations.
In L. M. LeCam and J. Neyman, editors, Proc.
of the fifth BerkeleySymposium on Mathematical Statistics and Probabil-ity, volume 1, pages 281?297.
University of CaliforniaPress.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target language phrases.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), Syd-ney, Australia.Haitao Mi and Liang Huang.
2008.
Forest-based transla-tion rule extraction.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP).Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,and Kazuteru Ohashi.
2006.
A clustered global phrasereordering model for statistical machine translation.
InProceedings of the 21st International Conference onComputational Linguistics and the 44th annual meet-ing of the Association for Computational Linguistics,ACL-44, pages 713?720.Franz Josef Och.
1999.
An efficient method for de-termining bilingual word classes.
In Proceedings ofthe European chapter of the Association for Computa-tional Linguistics (EACL), pages 71?76.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics (ACL).Hugo Steinhaus.
1956.
Sur la division des corpsmate?riels en parties.
Bull.
Acad.
Polon.
Sci.
Cl.
III.4, pages 801?804.10Ashish Venugopal and Andreas Zollmann.
2009.
Gram-mar based statistical MT on Hadoop: An end-to-endtoolkit for large scale PSCFG based MT.
The PragueBulletin of Mathematical Linguistics, 91:67?78.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3).Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of the Workshop on Statistical MachineTranslation, HLT/NAACL.Andreas Zollmann and Stephan Vogel.
2010.
Newparameterizations and features for PSCFG-based ma-chine translation.
In Proceedings of the 4th Work-shop on Syntax and Structure in Statistical Translation(SSST), Beijing, China.Andreas Zollmann, Ashish Venugopal, Franz J. Och, andJay Ponte.
2008.
A systematic comparison of phrase-based, hierarchical and syntax-augmented statisticalMT.
In Proceedings of the Conference on Computa-tional Linguistics (COLING).11
