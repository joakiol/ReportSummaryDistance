Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 642?647,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsProbabilistic Document Modeling for Syntax Removal in TextSummarizationWilliam M. DarlingSchool of Computer ScienceUniversity of Guelph50 Stone Rd E, Guelph, ONN1G 2W1 Canadawdarling@uoguelph.caFei SongSchool of Computer ScienceUniversity of Guelph50 Stone Rd E, Guelph, ONN1G 2W1 Canadafsong@uoguelph.caAbstractStatistical approaches to automatic text sum-marization based on term frequency continueto perform on par with more complex sum-marization methods.
To compute useful fre-quency statistics, however, the semanticallyimportant words must be separated from thelow-content function words.
The standard ap-proach of using an a priori stopword list tendsto result in both undercoverage, where syn-tactical words are seen as semantically rele-vant, and overcoverage, where words relatedto content are ignored.
We present a genera-tive probabilistic modeling approach to build-ing content distributions for use with statisti-cal multi-document summarization where thesyntax words are learned directly from thedata with a Hidden Markov Model and arethereby deemphasized in the term frequencystatistics.
This approach is compared to both astopword-list and POS-tagging approach andour method demonstrates improved coverageon the DUC 2006 and TAC 2010 datasets us-ing the ROUGE metric.1 IntroductionWhile the dominant problem in Information Re-trieval in the first part of the century was findingrelevant information within a datastream that is ex-ponentially growing, the problem has arguably tran-sitioned from finding what we are looking for to sift-ing through it.
We can now be quite confident thatsearch engines like Google will return several pagesrelevant to our queries, but rarely does one have timeto go through the enormous amount of data that issupplied.
Therefore, automatic text summarization,which aims at providing a shorter representation ofthe salient parts of a large amount of information,has been steadily growing in both importance andpopularity over the last several years.
The summa-rization tracks at the Document Understanding Con-ference (DUC), and its successor the Text AnalysisConference (TAC)1, have helped fuel this interest byhosting yearly competitions to promote the advance-ment of automatic text summarization methods.The tasks at the DUC and TAC involve takinga set of documents as input and outputting a shortsummary (either 100 or 250 words, depending onthe year) containing what the system deems to be themost important information contained in the originaldocuments.
While a system matching human perfor-mance will likely require deep language understand-ing, most existing systems use an extractive, ratherthan abstractive, approach whereby the most salientsentences are extracted from the original documentsand strung together to form an output summary.2In this paper, we present a summarization modelbased on (Griffiths et al, 2005) that integrates top-ics and syntax.
We show that a simple model thatseparates syntax and content words and uses thecontent distribution as a representative model ofthe important words in a document set can achievehigh performance in multi-document summariza-tion, competitive with state-of-the-art summariza-tion systems.1http://www.nist.gov/tac2NLP techniques such as sentence compression are oftenused, but this is far from abstractive summarization.6422 Related Work2.1 SumBasicNenkova et al (2006) describe SumBasic, a simple,yet high-performing summarization system based onterm frequency.
While the methodology underly-ing SumBasic departs very little from the pioneer-ing summarization work performed at IBM in the1950?s (Luhn, 1958), methods based on simple wordstatistics continue to outperform more complicatedapproaches to automatic summarization.3 Nenkovaet al (2006) empirically showed that a word that ap-pears more frequently in the original text will bemore likely to appear in a human generated sum-mary.The SumBasic algorithm uses the empirical uni-gram probability distribution of the non-stop-wordsin the input such that for each word w, p(w) = nwNwhere nw is the number of occurrences of word wandN is the total number of words in the input.
Sen-tences are then scored based on a composition func-tion CF (?)
that composes the score for the sentencebased on its contained words.
The most commonlyused composition function adds the probabilities ofthe words in a sentence together, and then divides bythe number of words in that sentence.
However, toreduce redundancy, once a sentence has been chosenfor summary inclusion, the probability distributionis recalculated such that any word that appears inthe chosen sentence has its probability diminished.Sentences are continually marked for inclusion un-til the summary word-limit is reached.
Despite itssimplicity, SumBasic continues to be one of the topsummarization performers in both manual and auto-matic evaluations (Nenkova et al, 2006).2.2 Modeling Content and SyntaxGriffiths et al (2005) describe a composite gener-ative model that combines syntax and semantics.The semantic portion of the model is similar to La-tent Dirichlet Allocation and models long-range the-matic word dependencies with a set of topics, whileshort-range (sentence-wide) word dependencies aremodeled with syntax classes using a Hidden MarkovModel.
The model has an HMM at its base where3A system based on SumBasic was one of the top performersat the Text Analysis Conference 2010 summarization track.one of its syntax classes is replaced with an LDA-like topic model.
When the model is in the semanticclass state, it chooses a topic from the given docu-ment?s topic distribution, samples a word from thattopic?s word distribution, and generates it.
Other-wise, the model samples a word from the currentsyntax class in the HMM and outputs that word.3 Our Summarization ModelNenkova et al (2006) show that using term fre-quency is a powerful approach to modeling humansummarization.
Nevertheless, for SumBasic to per-form well, stop-words must be removed from thecomposition scoring function.
Because these wordsadd nothing to the content of a summary, if theywere not removed for the scoring calculation, thesentence scores would no longer provide a good fitwith sentences that a human summarizer would findsalient.
However, by simply removing pre-selectedwords from a list, we will inevitably miss wordsthat in different contexts would be considered non-content words.
In contrast, if too many words areremoved, the opposite problem appears and we mayremove important information that would be usefulin determining sentence scores.
These problems arereferred to as undercoverage and overcoverage, re-spectively.To alleviate this problem, we would like to putless probability mass for our document set proba-bility distribution on non-content words and moreon words with strong semantic meaning.
One ap-proach that could achieve this would be to build sep-arate stopword lists for specific domains, and thereare approaches to automatically build such lists (Loet al, 2005).
However, a list-based approach can-not take context into account and therefore, amongother things, will encounter problems with poly-semy and synonymy.
Another approach would be touse a part-of-speech (POS) tagger on each sentenceand ignore all non-noun words because high-contentwords are almost exclusively nouns.
One could alsoinclude verbs, adverbs, adjectives, or any combina-tion thereof, and therefore solve some of the context-based problems associated with using a stopwordlist.
Nevertheless, this approach introduces deepercontext-related problems of its own (a noun, for ex-ample, is not always a content word).
A separate ap-643DMNMc w??C???
?zFigure 1: Graphical model depiction of our content andsyntax summarization method.
There are D documentsets, M documents in each set, NM words in documentM , and C syntax classes.proach would be to model the syntax and semanticwords used in a document collection in an HMM, asin Griffiths et al (2005), and use the semantic classas the content-word distribution for summarization.Our approach to summarization builds on Sum-Basic, and combines it with a similar approachto separating content and syntax distributions asthat described in (Griffiths et al, 2005).
Like(Haghighi and Vanderwende, 2009), (Daume?
andMarcu, 2006), and (Barzilay and Lee, 2004), wemodel words as being generated from latent distribu-tions.
However, instead of background, content, anddocument-specific distributions, we model all wordsin a document set as being there for one of only twopurposes: a semantic (content) purpose, or a syntac-tic (functional) purpose.
We model the syntax classdistributions using an HMM and model the contentwords using a simple language model.
The princi-pal difference between our generative model and theone described in (Griffiths et al, 2005) is that wesimplify the model by assuming that each documentis generated solely from one topic distribution that isshared throughout each document set.
This results ina smoothed language model for each document set?scontent distribution where the counts from contentwords (as determined through inference) are used todetermine their probability, and the syntax words areessentially discarded.Therefore, our model describes the process ofgenerating a document as traversing an HMM andinatofonwithbyelninoweatherpacificoceannormaltemperaturessaidtoldaskedsaysays......Figure 2: Portion of Content and Syntax HMM.
Theleft and right states show the top words for those syntaxclasses while the middle state shows the top words for thegiven document set?s content distribution.emitting either a content word from a single topic?s(document set?s) content word distribution, or a syn-tax word from one of C corpus-wide syntax classeswhere C is a parameter input to the algorithm.
Morespecifically, a document is generated as follows:1.
Choose a topic z corresponding to the givendocument set (z = {z1, ..., zk} where k is thenumber of document sets to summarize.)2.
For each word wi in document d(a) Draw ci from pi(ci?1)(b) If ci = 1, then draw wi from ?
(z), other-wise draw wi from ?
(ci)Each class ci and topic z correspond to multinomialdistributions over words, and transitions betweenclasses follow the transition distribution pi(ci?1).When ci = 1, a content word is emitted fromthe topic word distribution ?
(z) for the given doc-ument set z.
Otherwise, a syntax word is emittedfrom the corpus-wide syntax word distribution ?
(ci).The word distributions and transition vectors are alldrawn from Dirichlet priors.
A graphical model de-piction of this distribution is shown in Figure 1.
Aportion of an example HMM (from the DUC 2006dataset) is shown in Figure 2 with the most proba-ble words in the content class in the middle and twosyntax classes on either side of it.3.1 InferenceBecause the posterior probability of the content(document set) word distributions and syntax classword distributions cannot be solved analytically, aswith many topic modeling approaches, we appeal644to an approximation.
Following Griffiths et al(2005), we use Markov Chain Monte Carlo (see,e.g.
(Gilks et al, 1999)), or more specifically, ?col-lapsed?
Gibbs sampling where the multinomial pa-rameters are integrated out.4 We ran our sampler forbetween 500 and 5,000 iterations (though the dis-tributions would typically converge by 1,000 itera-tions), and chose between 5 and 10 (with negligiblechanges in results) for the cardinality of the classesset C. We leave optimizing the number of syntaxclasses, or determining them directly from the data,for future work.3.2 SummarizationHere we describe how we use the estimated topicand syntax distributions to perform extractive multi-document summarization.
We follow the SumBasicalgorithm, but replace the empirical unigram distri-bution of the document set with the learned topicdistributions for the given documents.
This modelsthe effect of not only ignoring stop-words, but alsoreduces the amount of probability mass in the distri-bution placed on functional words that serve no se-mantic purpose and that would likely be less usefulin a summary.
Because this is a fully probabilisticmodel, we do not entirely ?ignore?
stop-words; in-stead, the model forces the probability mass of thesewords to the syntax classes.For a given document set to be summarized, eachsentence is assigned a score corresponding to theaverage probability of the words contained withinit: Score(S) = 1|S|?w?S p(w).
In SumBasic,p(wi) =niN .
In our model, SyntaxSum, p(wi) =p(wi|?
(z)), where ?
(z) is a multinomial distributionover the corpus?
fixed vocabulary that puts highprobabilities on content words that are used oftenin the given document set and low probabilitieson words that are more important in other syntaxclasses.
The middle node in Figure 2 is a true repre-sentation of the top words in the ?
(z) distribution fordocument set 43 in the DUC 2006 dataset.4 Experiments and ResultsHere we describe our experiments and give quanti-tative results using the ROUGE automatic text sum-4See http://lingpipe.files.wordpress.com/2010/07/lda1.pdf for more information.MethodROUGE ROUGE (-s)R-1 R-2 R-SU4 R-1 R-2 R-SU4SB- 37.0 5.5 11.0 23.3 3.8 6.2SumBasic 38.1 6.7 11.9 29.4 5.3 8.1N 36.8 7.0 12.2 25.5 4.8 7.3N,V 36.9 6.5 12.0 24.4 4.4 6.9N,J 37.4 6.8 12.3 26.5 5.0 7.7N,V,J 37.4 6.8 12.2 25.5 4.9 7.4SBH 38.9 7.3 12.6 30.7 5.9 8.7Table 1: ROUGE Results on the DUC 2006 dataset.
Re-sults statistically significantly higher than SumBasic (asdetermined by a pairwise t-test with 99% confidence) aredisplayed in bold.marization metric for unigram (R-1), bigram (R-2),and skip-4 bigram (R-SU4) recall both with andwithout (-s) stopwords removed (Lin, 2004).
Wetested our models on the popular DUC 2006 datasetwhich aids in model comparison and also on themore recent TAC 2010 dataset.
The DUC 2006dataset consists of 50 sets of 25 news articles each,whereas the TAC 2010 dataset consists of 46 sets of10 news articles each.5 For DUC 2006, summariesare a maximum of 250 words; for TAC 2010, theycan be at most 100.
Our approach is compared tousing an a priori stopword list, and using a POS-tagger to build distributions of words coming fromonly a subset of the parts-of-speech.4.1 SumBasicTo cogently demonstrate the effect of ignoring non-semantic words in term frequency-based summa-rization, we implemented two initial versions ofSumBasic.
The first, SB-, does not ignore stop-words while the second, SumBasic, ignores all stop-words from a list included in the Python NLTK li-brary.6 For SumBasic without stop-word removal(SB-), we obtain 3.8 R-2 and 6.2 R-SU4 (with the -sflag).7 With stop-words removed from the sentencescoring calculation (SumBasic), our results increaseto 5.3 R-2 and 8.1 R-SU4, a significantly large in-crease.
For complete ROUGE results of all of ourtested models on DUC 2006, see Table 1.5We limit our testing to the initial TAC 2010 data as opposedto the update portion.6Available at http://www.nltk.org.7Note that we present our ROUGE scores scaled by 100 toaid in readability.6454.2 POS TaggerBecause the content distributions learned from ourmodel seem to favor almost exclusively nouns (seeFigure 2), another approach to building a seman-tically strong word distribution for determiningsalient sentences in summarization might be to ig-nore all words except nouns.
This would avoidmost stopwords (many of which are modeled as theirown part-of-speech) and would serve as a simplerapproach to finding important content.
Neverthe-less, adjectives and verbs also often carry impor-tant semantic information.
Therefore, we ran a POStagger over the input sentences and tried selectingsentences based on word distributions that includedonly nouns; nouns and verbs; nouns and adjectives;and nouns, verbs, and adjectives.
In each case,this approach performs either worse than or no bet-ter than SumBasic using a priori stopword removal.The nouns and adjectives distribution did the best,whereas the nouns and verbs were the worst.4.3 Content and Syntax ModelFinally, we test our model.
Using the content dis-tributions found by separating the ?content?
wordsfrom the ?syntax?
words in our modified topics andsyntax model, we replaced the unigram probabil-ity distribution p(w) of each document set with thelearned content distribution for that document set?stopic, ?
(z), where z is the topic for the given docu-ment set.
Following this method, which we call SBHfor ?SumBasic with HMM?, our ROUGE scores in-crease considerably and we obtain 5.9 R-2 and 8.7R-SU4 without stop-word removal.
This is the high-est performing model we tested.
Due to space con-straints, we omit full TAC 2010 results but R-2 andR-SU4 results without stopwords improved fromSumBasic?s 7.3 and 8.6 to 8.0 and 9.1, respectively,both of which were statistically significant increases.5 Conclusions and Future WorkThis paper has described using a domain-independent document modeling approach ofavoiding low-content syntax words in an NLP taskwhere high-content semantic words should be theprincipal focus.
Specifically, we have shown thatwe can increase summarization performance bymodeling the document set probability distributionusing a hybrid LDA-HMM content and syntaxmodel.
We model a document set?s creation byseparating content and syntax words throughobserving short-range and long-range word depen-dencies, and then use that information to build aword distribution more representative of contentthan either a simple stopword-removed unigramprobability distribution, or one made up of wordsfrom a particular subset of the parts-of-speech.This is a very flexible approach to finding contentwords and works well for increasing performance ofsimple statistics-based text summarization.
It couldalso, however, prove to be useful in any other NLPtask where stopwords should be removed.
Somefuture work includes applying this model to areassuch as topic tracking and text segmentation, andcoherently adjusting it to fit an n-gram modelingapproach.AcknowledgmentsWilliam Darling is supported by an NSERC Doc-toral Postgraduate Scholarship.
The authors wouldlike to acknowledge the financial support providedfrom Ontario Centres of Excellence (OCE) throughthe OCE/Precarn Alliance Program.
We also thankthe anonymous reviewers for their helpful com-ments.ReferencesRegina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In HLT-NAACL2004: Proceedings of the Main Conference, pages113?120.
Best paper award.Hal Daume?, III and Daniel Marcu.
2006.
Bayesianquery-focused summarization.
In ACL-44: Proceed-ings of the 21st International Conference on Compu-tational Linguistics and the 44th annual meeting ofthe Association for Computational Linguistics, pages305?312, Morristown, NJ, USA.
Association for Com-putational Linguistics.W.
R. Gilks, S. Richardson, and D. J. Spiegelhalter.
1999.Markov ChainMonte Carlo In Practice.
Chapman andHall/CRC.Thomas L. Griffiths, Mark Steyvers, David M. Blei, andJoshua B. Tenenbaum.
2005.
Integrating topics andsyntax.
In In Advances in Neural Information Pro-cessing Systems 17, pages 537?544.
MIT Press.646Aria Haghighi and Lucy Vanderwende.
2009.
Exploringcontent models for multi-document summarization.
InNAACL ?09: Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 362?370, Morristown, NJ,USA.
Association for Computational Linguistics.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization BranchesOut: Proceedings of the ACL-04 Workshop, pages 74?81, Barcelona, Spain, July.
Association for Computa-tional Linguistics.Rachel Tsz-Wai Lo, Ben He, and Iadh Ounis.
2005.
Au-tomatically building a stopword list for an informationretrieval system.
JDIM, pages 3?8.H.
P. Luhn.
1958.
The automatic creation of literatureabstracts.
IBM J. Res.
Dev., 2(2):159?165.Ani Nenkova, Lucy Vanderwende, and Kathleen McKe-own.
2006.
A compositional context sensitive multi-document summarizer: exploring the factors that in-fluence summarization.
In SIGIR ?06: Proceedings ofthe 29th annual international ACM SIGIR conferenceon Research and development in information retrieval,pages 573?580, New York, NY, USA.
ACM.647
