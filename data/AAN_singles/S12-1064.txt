First Joint Conference on Lexical and Computational Semantics (*SEM), pages 467?471,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsHDU: Cross-lingual Textual Entailment with SMT FeaturesKatharina Wa?schle and Sascha FendrichDepartment of Computational LinguisticsHeidelberg UniversityHeidelberg, Germany{waeschle, fendrich}@cl.uni-heidelberg.deAbstractWe describe the Heidelberg University systemfor the Cross-lingual Textual Entailment taskat SemEval-2012.
The system relies on fea-tures extracted with statistical machine trans-lation methods and tools, combining mono-lingual and cross-lingual word alignmentsas well as standard textual entailment dis-tance and bag-of-words features in a statisti-cal learning framework.
We learn separate bi-nary classifiers for each entailment directionand combine them to obtain four entailmentrelations.
Our system yielded the best overallscore for three out of four language pairs.1 IntroductionCross-lingual textual entailment (CLTE) (Mehdad etal., 2010) is an extension of textual entailment (TE)(Dagan and Glickman, 2004).
The task of recog-nizing entailment is to determine whether a hypoth-esis H can be semantically inferred from a text T .The CLTE task adds a cross-lingual dimension to theproblem by considering sentence pairs, where T andH are in different languages.
The SemEval-2012CLTE task (Negri et al, 2012) asks participants tojudge entailment pairs in four language combina-tions1, defining four target entailment relations, for-ward, backward, bidirectional and no entailment.We investigate this problem in a statistical learn-ing framework, which allows us to combine cross-lingual word alignment features as well as common1Spanish-English (es-en), Italian-English (it-en), French-English (fr-en) and German-English (de-en).monolingual entailment metrics, such as bag-of-words overlap, edit distance and monolingual align-ments on translations of T and H , using standardstatistical machine translation (SMT) tools and re-sources.
Our goal is to address this task without deepprocessing components to make it easily portableacross languages.
We argue that the cross-lingualentailment task can benefit from direct alignmentsbetween T and H , since a large amount of bilin-gual parallel data is available, which naturally mod-els synonymy and paraphrasing across languages.2 Related WorkWith the yearly Recognizing Textual Entailment(RTE) challenge (Dagan et al, 2006), there has beena lot of work on monolingual TE.
We therefore in-clude established monolingual features in our ap-proach, such as alignment scores (MacCartney etal., 2008), edit distance and bag-of-words lexicaloverlap measures (Kouylekov and Negri, 2010).
Sofar, the only work on CLTE that we are aware of isMehdad et al (2010), where the problem is reducedto monolingual entailment using machine transla-tion, and Mehdad et al (2011), which exploits par-allel corpora for generating features based on phrasealignments as input to an SVM.
Our approach com-bines ideas from both, mostly resembling Mehdadet al (2011).
There are, however, several differ-ences; we use word translation probabilities insteadof phrase tables and model monolingual and cross-lingual alignment separately.
We also include addi-tional similarity measures derived from the MT eval-uation metric Meteor, which was used in Volokh andNeumann (2011) for the monolingual TE task.
Con-467versely, Pado?
et al (2009) showed that textual entail-ment features can be used for measuring MT quality,indicating a strong relatedness of the two problems.The CLTE task is also related to the problem ofidentifying parallel sentence pairs in a non-parallelcorpus, so we adapt alignment-based features fromMunteanu and Marcu (2005), where a MaximumEntropy classifier was used to judge if two sentencesare sufficiently parallel.Regarding the view on entailment, MacCartneyand Manning (2007) proposed the decompositionof top-level entailment, such as equivalence (whichcorresponds to the CLTE bidirectional class), intoatomic forward and backward entailment predic-tions, which is mirrored in our multi-label approachwith two binary classifiers.3 SMT Features for CLTEThe SemEval-2012 CLTE task emerges from themonolingual RTE task; however the perception ofentailment differs slightly.
In CLTE, the sentencesT1 and T2 are of roughly the same length and theentailment is predicted in both directions.
Negri etal.
(2011) states that the CLTE pairs were createdby paraphrasing an English sentence E and leavingout or adding information to construct a modifiedsentence E?, which was then translated into a dif-ferent language2, yielding sentence F and thus cre-ating a bilingual entailment pair.
For this reason,we believe that our system should be less inference-oriented than some previous RTE systems and rathershould capture?
paraphrases and synonymy to identify semanticequivalence,?
phrases that have no matching correspondent inthe other sentence, indicating missing (respec-tively, additional) information.To this end, we define a number of similaritymetrics based on different views on the data pairs,which we combine as features in a statistical learn-ing framework.
Our features are both cross- andmonolingual.
We obtain monolingual pairs by trans-lating the English sentence E into the foreign lan-2We refer to the non-English language sentence as F .guage, yielding T (E) and vice versa T (F ) from F ,using Google Translate3.3.1 Token ratio featuresA first indicator for additional or missing informa-tion are simple token ratio features, i.e.
the fractionof the number of tokens in T1 and T2.
We definethree token ratio measures:?
English-to-Foreign, |E||F |?
English-to-English-Translation, |E||T (F )|?
Foreign-to-Foreign-Translation, |T (E)||F |3.2 Bag-of-words and distance featuresTypical similarity measures used in monolingualTE are lexical overlap metrics, computed on bag-of-words representations of both sentences.
Weuse the following similarities, computing bothsim(E, T (F )) and sim(F, T (E)).?
Jaccard coefficient, sim(A,B) = |A?B||A?B|?
Overlap coefficient, sim(A,B) = |A?B|min(|A|,|B|)We also compute the lexical overlap on bigramsand trigrams.In addition, we include a simple distance measurebased on string edit distance ed, summing up overall distances between every token a in A and its mostsimilar token b in B, where we assume that the cor-responding token is the one with the smallest editdistance:?
dist(A,B) = log?a?Aminb?Bed(a, b)3.3 Meteor featuresThe Meteor scoring tool (Denkowski and Lavie,2011) for evaluating the output of statistical machinetranslation systems can be used to calculate the simi-larity of two sentences in the same language.
Meteoruses stemming, paraphrase tables and synonym col-lections to align words between the two sentencesand scores the resulting alignment.
We include theoverall weighted Meteor score both for (E, T (F ))3http://translate.google.com/468and (F, T (E))4 as well as separate alignment preci-sion, recall and fragmentation scores for (E, T (F )).3.4 Monolingual alignment featuresWe use the alignments output by the Meteor-1.3scorer for (E, T (F ))5 to calculate the followingmetrics:?
number of unaligned words?
percentage of aligned words?
length of the longest unaligned subsequence3.5 Cross-lingual alignment featuresWe calculate IBM model 1 word alignments (Brownet al, 1993) with GIZA++ (Och and Ney, 2003) ona data set concatenated from Europarl-v66 (Koehn,2005) and a bilingual dictionary obtained fromdict.cc7 for coverage.
We then heuristically aligneach word e in E with the word f in F for which wefind the highest word translation probability p(e|f)and vice versa.
Words for which no translation isfound are considered unaligned.
From this align-ment a, we derive the following features both forE and F (resulting in a total of eight cross-lingualalignment features):?
number of unaligned words?
percentage of aligned words?
alignment score 1|E|?e?Ep(e|a(e))?
length of the longest unaligned subsequence4 ClassificationTo account for the different data ranges, we normal-ized all feature value distributions to the normal dis-tribution N (0, 13), so that 99% of the feature valuesare in [?1, 1].
We employed SVMlight (Joachims,1999) for learning different classifiers to output thefour entailment classes.
We submitted a second4Meteor-1.3 supports English, Spanish, French and German.We used the Spanish version for scoring Italian, since those lan-guages are related.5Since the synonymy module is only available for English,we do not use the alignment of (F, T (E)).6http://www.statmt.org/europarl/7http://www.dict.cc/T1 ?
T2 T2 ?
T1 entailment1 1 bidirectional1 0 forward0 1 backward0 0 no entailmentTable 1: Combination of atomic entailment relations.run to evaluate our recently implemented stochasticlearning toolkit Sol (Fendrich, 2012), which imple-ments binary, multi-class, and multi-label classifica-tion.For development, we split the training set in twoparts, which were alternatingly used as training andtest set.
We first experimented with a multi-classclassifier that learned all four entailment classes atonce.
However, although the task defines four tar-get entailment relations, those can be broken downinto two atomic relations, namely directional entail-ment from T1 to T2 and from T2 to T1 (table 1).
Wetherefore learned a binary classifier for each atomicentailment relation and combined the output to ob-tain the final entailment class.
We found this view tobe a much better fit for the problem, improving theaccuracy score on the development set by more than10 percentage points (table 2).
This two-classifiersapproach can also be seen as a variant of multi-labellearning, with the two atomic entailment relationsas labels.
We therefore also trained a direct imple-mentation of multi-label classification.
Although itsubstantially outperformed the multi-class approach,the system yielded considerably lower scores thanthe version using two binary classifiers.5 ResultsThe accuracy scores of our two runs on theSemEval-2012 CLTE test set are presented in ta-ble 3.
Our system performed best out of ten sys-tems for the language pairs es-en and de-en and tiedin first place for fr-en.
For it-en, our system camein second.
Regarding the choice of the learner, ourtoolkit slightly outperformed SVMlight on three ofthe four language pairs.To determine the contribution of different fea-ture types for each language combination, we per-formed ablation tests on the development set, wherewe switched off groups of features and measured the469es-en it-en fr-en de-enmulti-class 0.47 0.456 0.466 0.458multi-label 0.586 0.526 0.568 0.5222?
binary 0.646 0.614 0.628 0.588Table 2: Different classifiers on development set.es-en it-en fr-en de-enSVMlight 0.630 0.554 0.564 0.558Sol 0.632 0.562 0.570 0.552Table 3: Results on test set.impact on the accuracy score (table 4).
We assessedthe statistical significance of differences in scorewith an approximate randomization test8 (Noreen,1989), indicating a significant impact in bold font.The results show that only in two cases a single fea-ture group significantly impacts the score, namelythe Meteor score features for es-en and the cross-lingual alignment features for de-en.
However, nofeature group hurts the score either, since negativevariations in score are not significant.
To ensurethat the different feature groups actually express di-verse information, we also evaluated our system us-ing only one group of features at a time.
The re-sults confirm the most significant feature type foreach language pair, but even the best-scoring featuregroup for each pair always yielded scores 3-6 per-centage points lower than the system with all featuregroups combined.
We therefore conclude that thecombination of diverse features is one key aspect ofour system.8Using a significance level of 0.05.6 ConclusionsWe have shown that SMT methods can be profitablyapplied for the problem of CLTE and that combiningdifferent feature types improves accuracy.
Key toour approach is furthermore the view of the four-class entailment problem as a bidirectional binary ormulti-label problem.
A possible explanation for thesuperior performance of the multi-label approach isthat the overlap of the bidirectional entailment withforward and backward entailment might confuse themulti-class learner.Regarding future work, we think that our resultscan be improved by building on better alignments,i.e.
using more data for estimating cross-lingualalignments and larger paraphrase tables.
Further-more, we would like to investigate more thoroughlyin what way the representation of the problem interms of machine learning impacts the system per-formance on the task ?
in particular, why the two-classifiers approach substantially outperforms themulti-label implementation.ReferencesPeter F. Brown, Vincent J. Della Pietra, Stephen A.Della Pietra, and Robert L. Mercer.
1993.
The mathe-matics of statistical machine translation: Parameter es-timation.
Computational linguistics, 19(2):263?311.Ido Dagan and Oren Glickman.
2004.
Probabilistic tex-tual entailment: Generic applied modeling of languagevariability.Ido Dagan, Oren Glickman, and Bernado Magnini.
2006.The pascal recognising textual entailment challenge.Machine Learning Challenges.
Evaluating PredictiveUncertainty, Visual Object Classification, and Recog-nising Tectual Entailment, pages 177?190.Michael Denkowski and Alon Lavie.
2011.
Meteor 1.3:es-en it-en fr-en de-enfeature group (#/features) score impact score impact score impact score impactMeteor scores (5) 0.616 -0.03 0.6 -0.014 0.618 0.01 0.59 -0.002distance/bow (10) 0.644 -0.002 0.608 -0.006 0.62 0.008 0.596 -0.008token ratio (3) 0.652 -0.006 0.606 -0.008 0.62 0.008 0.588 -0cross-lingual alignment (8) 0.638 -0.008 0.592 -0.022 0.62 0.008 0.526 -0.062monolingual alignment (3) 0.648 -0.002 0.624 -0.01 0.59 0.038 0.596 -0.008all (29) 0.646 0.614 0.628 0.588Table 4: Ablation tests on development set.470Automatic Metric for Reliable Optimization and Eval-uation of Machine Translation Systems.
In Proceed-ings of the EMNLP 2011 Workshop on Statistical Ma-chine Translation.Sascha Fendrich.
2012.
Sol ?
Stochastic LearningToolkit.
Technical report, Department of Computa-tional Linguistics, Heidelberg University.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
Advances in Kernel Methods Sup-port Vector Learning, pages 169?184.Philipp Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In MT summit, volume 5.Milen Kouylekov and Matteo Negri.
2010.
An open-source package for recognizing textual entailment.
InProceedings of the ACL 2010 System Demonstrations,pages 42?47.Bill MacCartney and Christopher D. Manning.
2007.Natural logic for textual inference.
In Proceedingsof the ACL-PASCAL Workshop on Textual Entailmentand Paraphrasing, pages 193?200.Bill MacCartney, Michel Galley, and Christopher D.Manning.
2008.
A phrase-based alignment model fornatural language inference.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 802?811.Yashar Mehdad, Matteo Negri, and Marcello Federico.2010.
Towards cross-lingual textual entailment.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 321?324.Yashar Mehdad, Matteo Negri, and Marcello Federico.2011.
Using bilingual parallel corpora for cross-lingual textual entailment.
Proceedings of ACL-HLT.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguistics,31(4):477?504.Matteo Negri, Luisa Bentivogli, Yashar Mehdad, DaniloGiampiccolo, and Alessandro Marchetti.
2011.
Di-vide and conquer: crowdsourcing the creation of cross-lingual textual entailment corpora.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing, pages 670?679.Matteo Negri, Alessandro Marchetti, Yashar Mehdad,Luisa Bentivogli, and Danilo Giampiccolo.
2012.Semeval-2012 Task 8: Cross-lingual Textual Entail-ment for Content Synchronization.
In Proceedings ofthe 6th International Workshop on Semantic Evalua-tion (SemEval 2012).Eric W. Noreen.
1989.
Computer Intensive Methodsfor Testing Hypotheses.
An Introduction.
Wiley, NewYork.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational linguistics, 29(1):19?51.Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,and Christopher D. Manning.
2009.
Measuring ma-chine translation quality as semantic equivalence: Ametric based on entailment features.
Machine Trans-lation, 23(2):181?193.Alexander Volokh and Gu?nter Neumann.
2011.
UsingMT-based metrics for RTE.
In The Fourth Text Analy-sis Conference.
NIST.471
