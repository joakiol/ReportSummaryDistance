An Example-based Decoder for Spoken Language Machine Transla-tionZhou-Jun Li Wen-Han ChaoAbstractIn this paper, we propose an example-baseddecoder for a statistical machine translation(SMT) system, which is used for spokenlanguage machine translation.
In this way,it will help to solve the re-ordering problemand other problems for spoken languageMT, such as lots of omissions, idioms etc.Through experiments, we show that thisapproach obtains improvements over thebaseline on a Chinese-English spoken lan-guage translation task.1 IntroductionThe state-of-the-art statistical machine translation(SMT) model is the log-linear model (Och and Ney,2002), which provides a framework to incorporateany useful knowledge for machine translation,such as translation model, language model etc.In a SMT system, one important problem is there-ordering between words and phrases, especiallywhen the source language and target language arevery different in word order, such as Chinese andEnglish.For the spoken language translation, the re-ordering problem will be more crucial, since thespoken language is more flexible in word order.
Inaddition, lots of omissions and idioms make thetranslation more difficult.However, there exists some "useful" features,such as, most of the spoken text is shorter than thewritten text and there are some fixed translationstructures.
For example,  ( ????
/ Would youplease ?
?
), (??
?/May I??
).We can learn these fixed structures and takethem as rules, Chiang (2005) presents a method tolearn these rules, and uses them in the SMT.
Gen-erally, the number of these rules will be very large.In this paper, we propose an example-based de-coder in a SMT model, which will use the transla-tion examples to keep the translation structure, i.e.constraint the reordering, and make the omittedwords having the chance to be translated.The rest of this paper is organized as follows:Since our decoder is based on the inversion trans-duction grammars (ITG) (Wu, 1997), we introducethe ITG in Section 2 and describe the derived SMTmodel.
In Section 3, we design the example-baseddecoder.
In Section 4, we test our model and com-pare it with the baseline system.
Then, we con-clude in Section 5 and Section 6.2 The SMT modelITG is a synchronous context-free grammar, whichgenerates two output streams simultaneously.
Itconsists of the following five types of rules:jijip ececAAAAA /|/|/||][ ??><???
(1)Where A is the non-terminal symbol, [] and <>represent the two operations which generate out-puts in straight and inverted orientation respec-tively.
and  are terminal symbols, which rep-resent the words in both languages,ic je?
is the nullNational Laboratory forParallel and DistributedProcessing, Changsha,ChinaSchool of Computer Sci-ence and Engineering,Beihang University,Chinalizj@buaa.edu.cnNational Laboratory forParallel and DistributedProcessing, Changsha,Chinacwh2k@163.comYue-Xin ChenNational Laboratory forParallel and DistributedProcessing, Changsha,China1Sixth SIGHAN Workshop on Chinese Language Processingwords.
The last three rules are called lexical rules.is the probability of the rule.
pIn this paper, we consider the phrase-based SMT,so the  and  represent phrases in both lan-guages, which are consecutive words.
And a pairof   and  is called a phrase-pair, or a block.ic jeic jeDuring the process of decoding, each phrasein the source sentence is translated into a targetphrase  through lexical rules, and then rules []or <>  are used to merge two adjacent blocks into alarger block in straight or inverted orientation, untilthe whole source sentence is covered.
In this way,we will obtain a binary branching tree, which isdifferent from the traditional syntactical tree, sinceeach constituent in the branching tree is not a syn-tactical constituent.icjeThus, the model achieves a great flexibility tointerpret alost arbitrary reordering during the de-coding, while keeping a weak but effective con-straint.
Figure 1(a) gives an example to illustrate aderivation from the ITG model.??
1 ?
2 ??
3 ?
4 ??
5 ?6where1 ?s2 the3 nearest4 cassino5 ?6(b)  A word alignment(a)  An ITG tree?/ the ?
??
/ where ?s??
?
/ nearest ??
/ cassino ?
/ ?Figure 1.
(a) An ITG tree derived from the ITGwhere the line between the branches means an in-verted orientation, otherwise a straight one, (b) Aword alignment corresponds to the ITG tree in (a).Since we regard the process of the decoding as asequence of applications of rules in (1), i.e., theoutput sentence pair (C,E) will be a derivation D ofthe ITG, where C represents the source sentenceand E is the target sentence.Following Och and Ney (2002), we define theprobability for each rule as:?=iiirulehrule ?
)()Pr(  (2)Where the hi represents the feature and ?i is thecorresponding weight of the feature.We will consider mainly the following featuresfor rules:z Translation Models: , ,and .
The first two mod-els consider the probability of phrase transla-tion; and the latter two consider the lexicaltranslation, i.e., the probability that the wordsin source (or target) phrase translate to theones in the target (or source) phrase.
)|( ceP )|( ecP)|( cePlex )|( ecPlexz Reordering model: , where o isthe output orientation and b),|( 21 bboP1, b2 are the twoblocks in the rule.z Language model: )(Pr elm?
, which considersthe increment of the language model for eachrule.And the probability for the derivation will be:?=?DrrD )Pr()Pr(  (3)So the decoder searches the best E* derivedfrom the best derivation D*, when given a sourcesentence C.)Pr(maxarg*)(DDCDc ==  (4)2.1 Building the modelsIn our SMT model, we use the translation modelsand reordering model.
They will be built from thetraining corpus, which is a word-aligned bilingualcorpus satisfying the ITG constraint.We define the word alignment A for the sen-tence pair (C,E) in the following ways:z A region : )..,..( tsji ji..  represents a sequenceof position index in sentence C, i.e.jii ,...,1, +  and  represents a sequence ofposition index in sentence E, i.e.ts..tss ,...,1, + .We also call the  and ji.. ts..  are regions inmonolingual sentences.
The region corre-sponds to a phrase pair, which we called as ablock.
The length of the block is|)1||,1max(| +?+?
stij .2Sixth SIGHAN Workshop on Chinese Language Processingz A link : And each link representsthe alignment between the consecutive wordsin both of the sentences, which position in-dexes are in  and)..,..( tsjil =ji.. ts.. .
If one of theandji..ts..  is ?, i.e.
an empty region, we call thelink a null-align.z A word alignment A: a set of links.
},...,,{ 21 nlllA =We can merge two links  andto form a larger link, if the twolinks are adjacent in both of the sentences, i.e.is adjacent to  where)..,..( 11111 tsjil =)..,..( 22222 tsjil =11.. ji 22.. ji 112 += ji  or, or  (or ) is ?
, so do theto .
If the region can be formed bymerging two adjacent links gradually, we call theregion is independent, and the corresponding blockis also independent.121 += ji 11.. ji 22.. ji 11..ts22..ts )..,..( tsjiIn our system, the word alignment must satisfythe ITG constraint, i.e.
the word alignment is ableto form a binary branching tree.
Figure 1(b) illus-trates a word alignment example; the number be-low the word is the position index.
In the example,the region (1..3, 3..5) is independent, and the block(   ??
?
??
?the nearest cassino) is also inde-pendent.In order to obtain the word alignment satisfyingthe ITG constraint, Wu(1997) propose a DP algo-rithm, and we (Chao and Li, 2007) have transferredthe constraint to four simple position judgmentprocedures in an explicit way, so that we can in-corporate the ITG constraint as a feature into a log-linear word alignment model (Moore, 2005).After obtaining the word-aligned corpus, inwhich each word alignment satisfy the ITG con-straint, we can extract the blocks in a straight-forward way.
For the word alignment forms a hier-archical binary tree, we choose each constituent asa block.
Each block is formed by combining one ormore links, and must be independent.
Consideringthe data sparseness, we limit the length of eachblock as N (here N=3~5).We can also collect the reordering informationbetween two blocks according to the orientation ofthe branches.Thus, we will build the translation models, ,  and , usingthe frequencies of the blocks, and the re-orderingmodel ,)|( ceP )|( ecP )|( cePlex )|( ecPlex),|( 21 bboP },{ invertstraighto?
in thefollowing way:),( of freq.
)),(( of freq.
),|(212121 bbcooccurobbObbop==  (5)Considering the data sparseness, we transfer there-ordering model in the following way:)*,|(,*)|(),|( 2121 bopbopbbop ?=  (6)where * represents any block, repre-sents the probability when , i.e., whenoccurs, the orientation it merges with any otherblock is o .
So we can estimate the merging orien-tation through the two blocks respectively.,*)|( 1bopobO =,*)( 11b2.2 A Baseline DecoderIn order to evaluate the example-based decoder, wedevelop a CKY style decoder as a baseline (Chaoet al 2007), which will generate a derivation fromthe ITG in a DP way.
And it is similar with thetopical phrase-based SMT system, while maintain-ing the ITG constraint.3 The Example-based DecoderThe SMT obtains the translation models duringtraining, and does not need the training corpuswhen decoding; while the example-based machinetranslation system (EBMT) using the similar ex-amples in the training corpus when decoding.However, both of them use the same corpus; wecan generate a hybrid MT, which is a SMT systemwhile using an example-based decoder, to benefitfrom the advantages within the two systems.Our example-based decoder consists of twocomponents: retrieval of examples and decoding.Figure 2 shows the structure of the decoder.Training CorpusSMT ModelsInput sentenceDecodingMergingRetrieval of examplesMatchingOutputFigure 2.
The structure of the example-based de-coder.3Sixth SIGHAN Workshop on Chinese Language Processing3.1 Retrieval of ExamplesOur training corpus is a sentence-aligned bilingualcorpus.
For each sentence pair (C,E), we obtainedthe word alignment A, satisfying the ITG constaintthrough the methods described in section 2.
Wecall the triple (C,A,E) as an example.So, the problem of retrieval of examples is:given the input source sentence C0 and the trainingcorpus, collecting a set of translation examples{( C1, A1, E1) , ( C2, TA2, E2),....} from the corpus,where each translation example (Ci, Ai, Ei)  issimilar to the input sentence C0.The quality of the retrieval of the similar exam-ples is very import to the hybrid MT.
For the trans-lating may run in a large-scale corpus and in a real-time way, we divide the retrieval of similar exam-ples into two phases:z Fast Retrieval Phase: retrieving the similarexamples from the corpus quickly, and takethem as candidates.
The complexity shouldnot be too high.z Refining Phase: refining the candidates tofind the most similar examples.3.1.1 The Similarity Metric for Fast RetrievalGiven an input sentence  and an ex-ample (C, A, E), we calculate the number of thematched source words between the input sentenceand the source sentence C  in the example firstly.nwwwI ...21=),,()(*2),(EACLenILenMatchExamISim ww +=(7)where  is the number of the matchedwords and  is the number of words inwMatch)(ILen I ,and is the number of the words in thein C .
),,( EACLenGiven an input sentence , we ob-tain the relative blocks in the translation model foreach word .
We use torepresent the blocks, in which for each block ,the source phrase c  use the word as the firstword, and the length of  c   is , i.e.
the.
For each c , there may exists morethan one blocks with c  as the source phrase, so wewill sort them by the probability and keep the bestN (here set N=5) blocks.
Now we represent theinput sentence as:nwwwI ...21=},...2,1{( niwi ?
i gramkB ?
),( eciwk)1..( ?+= kiiwc}1,1,|{)( nkniBbbI i gramk ????
?= ??
(8)For example, in an input sentence ?
?????
?,)},(),,(),,(),,{(11 MinemymeiB gram ???
?=?Note, some  may be empty, e.g., since no blocks with ?
??
???
asthe source phrase.igramkB ?
?=?22 gramBIn the same way, we represent the exampleas:  ),,( EAC*},|{),,( AbBbbEAC i gramk ?
?= ??
(9)where *A  represents the blocks which are links inthe alignment  or can be formed by merging ad-jacent links independently.
In order to acceleratethe retrieval of similar examples, we generate theblock set for the example during the training proc-ess and store them in the corpus.ANow, we can use the number of the matchedblocks to measure the similarity of the input andthe example:ExamgramIgrambbBBMatchExamISim +=*2),((10)where  is the number of the matchedblocks and  is the number of( ) inbMatchIgramBigramkB ???
?i gramkB )(I?
, and is the numberof the blocks inExamgramB),,( EAC?
.Since each block is attached a probability, wecan compute the similarity in the following way:ExamgramIgramMatchbpBBbobExamISim b+?= ?
)(Pr*2),((11)So the final similarity metric for fast retrieval ofthe candidates is:pbwfast SimSimSimExamISim ???
++=),(  (12)where 11,,0 =++??
??????
.
Here we usemean values, i.e.
3/1=== ???
.
During the fastretrieval phase, we first filter out the examples us-ing the , then calculate the  for eachexample left, and retrieve the best N examples.wSim fastSim4Sixth SIGHAN Workshop on Chinese Language Processing3.1.2 The Alignment Structure MetricAfter retrieving the candidate similar examples, werefine the candidates using the word alignmentstructure with the example, to find the best M simi-lar examples (here set M=10).
The word alignmentin the example satisfies the ITG constraint, whichprovides a weak structure constraint.Given the input sentence I  and an example, we first search the matched blocks, atthis moment the order of the source phrases in theblocks must correspond with the order of the wordsin the input.
),,( EACAs Figure 3 shows, the matching divides the in-put and the example respectively into several re-gions, where some regions are matched and someun-matched.
And we take each region as a wholeand align them between the input and the exampleaccording to the order of the matched regions.
Forexample, the region (1..3,3..5) in  is un-matched, which aligns to the region (1..1) in),,( EACI .
Inthis way, we can use a similar edit distance methodto measure the similarity.
We count the number ofthe Deletion / Insertion / Substitution operations,which take the region as the object.??
1 ?
2 ??
3 ?
4 ??
5 ?6where1 ?s2 the3 nearest4 cassino5 ?6(a)  An example???
1 ?
2 ??
3 ?4(b)  An inputFigure 3.
An input and an example.
After matching,there are three regions in both sides, which are in-cluded in the line box, where the region (4..5,1..2)in the example matches the region (2..3) in the in-put, so do (6..6,6..6) to (4..4).
And the region(1..3,3..5) in the example should be substituted to(1..1) in the input.We set the penalty for each deletion and inser-tion operation as 1, while considering the un-matched region in the example may be independ-ent or not, we set the penalty for substitution as 0.5if the region is independent, otherwise as 1.
E.g.,the distance is 0.5 for substituting the region(1..3,3..5) to (1..1).We get the metric for measuring the structuresimilarity of the I  and : ),,( EACexmapleinputalignRRSIDExamISim+++?=1),( (13)where D, I, S are the deletion, insertion and substi-tution distances, respectively.
And the  andare the region numbers in the input andexample.inputRexmapleRIn the end, we obtain the similarity metric,which considers all of the above metrics:alignfastfinal SimSimExamISim ''),( ??
+= (14)where  1''1','0 =+??
????
.
Here we alsouse mean values 2/1'' == ??
.After the two phrases, we obtain the most simi-lar examples with the input sentence.3.2 DecodingAfter retrieving the translation examples, our goalis to use these examples to constrain the order ofthe output words.
During the decoding, we iteratethe following two steps.3.2.1 MatchingFor each translation example (Ck, Ak, Ek) consistsof the constituent structure tree, we can match theinput sentence with the tree as in Section 3.1.2.After matching, we obtain a translation of theinput sentence, in which some input phrases arematched to blocks in the tree, i.e.
they are trans-lated, and some phrases are un-translated.
The or-der of the matched blocks must be the same as theinput phrases.
We call the translation as a transla-tion template for the input.If we take each un-translated phrase as a null-aligned block, the translation template will be ableto form a new constituent tree.
And the matchedblocks in the template will restrict the translationstructure.Figure 4(a-c) illustrates the matching process,and Figure 4(c) is a translation template, in which "?
?"
and "? "
have been translated and "?
??
??
?"
is not translated.
And the translation5Sixth SIGHAN Workshop on Chinese Language Processingtemplate can be derived from the ITG as follows(here we remove the un-matched phrase):couldAyouAAAAAAAA///?][43243121????>?>?>?>><?>?
(15)Since we have M (here M=10) similar examples,we will get more than one translation template forthe input sentence.
So we define the evaluationfunction f for each translation template as :)(log)(log)( untranstrans CHDPtempf +=  (16)Where  is the probability for the newITG tree without the un-translated phrases, whichis a derivation from the ITG, so we can calculate itusing the SMT model in Section 2 ( formula 3).
)( transDPAnd the  is the estimated score forthe un-translated phrases.
In order to ob-tain , we estimate the score for eachun-translated phrase  in the following way:)( untransCH)( untransCHnmc ..)}|*(),()(max{)( ..*...... maxmax nmenkkmknm cepcHcHcH ?= (17)That is, using the best translation to estimate thetranslation score.
Thus we can estimate theas: )( untransCH?=cnmuntrans cHCH )()( ..  (18)We call the un-translated phrases as child inputs,and try to translate them literately, i.e., decodingthem using the examples.
If there are no un-translated phrases in the input, the decoding iscompleted, and the decoder returns the translationtemplate with the best score as the result.3.2.2 MergingIf one child input is translated completely, i.e.
nophrase is un-translated.
Then, it should be mergedinto the parent translation template to form a newtemplate.
When merging, we must satisfy the ITGconstraint, so we use the rules [] and <> to mergethe child input with the adjacent blocks.
Figure4(c-f) illustrates a merging process.
(b) Example A?
?
?
??
?
?could you spell it ?
?
/ spell ?/ could ?
?/ ?
??/?
?/ it?/you?
?
??
??
?
?
?
(a) Input(c) Translation Tempate after match input with Example A?
?
??
??
?
?could you ?
??
??
?
?/ could ?
?/ ?
?/you?
(d) Example B?
??
??
?
?please open your bag ..
??
/ your ?
?/ open ?/ .
?/bag?/please(e) Translation Tempate after match the child input with Example B??
/ your?
?/ open ?/bag??
??
?open your bag(f) Final translation after merged (c) and (e)?
???
?could you??
??
?open your bag ?/ could ?
?/ ?
?/you ??
/ your?
?/ open ?/bagFigure 4.
An example to illustrate the example-based decoding process, in which there are twotranslation examples.When merging, it may modify some rules whichare adjacent to the child inputs.
For example, whenmerging Figure 4(c) and (e), we may add a newrule:]  [ 1'1 childAAA >?
(19)Achild is the root non-terminal for the child input.And we should modify the rule  as: ][ 21AAA >?
][ 2'1AAA >?
(20)The merged template may vary due to the fol-lowing situations:z The orientation may vary.
The orientation be-tween the new block formed from the child6Sixth SIGHAN Workshop on Chinese Language Processingtemplate and the preceding or posteriorblocks may be straight or inverted.z The position to merge may vary.
We maymerge the new block with either the wholepreceding or posterior blocks, or only thechild blocks of them respectively, i.e.
wemay take the preceding or posterior blocksas the whole blocks or not.Thus, we will obtain a collection of the mergedtranslation templates, the decoder will evaluatethem using the formualte (16).
If all the templateshave no un-translated phrases, return the templatewith the best score.3.2.3 Decoding AlgorithmThe decoding algorithm is showed in Figure 5.In line 5~8, we match the input sentence witheach similar example, and generate a collection oftranslation templates, using the formular (16) toevaluate the templates.In line 9~11, we verify whether the set of thetemplates for the input is null: If it is null,decoding the input using the normal CKY decoder,and return the translations.In lin 12~23, we decode the un-matched phrasein each template, and merge it with the parenttemplate, until all of the template are translatedcompletely.In line 24, we return the best N translations.4 ExperimentsWe carried out experiments on an open Chinese-English translation task IWSLT2007, which con-sisting of sentence-aligned spoken language textfor traveling.
There are five development set, andwe take the third development set, i.e.
theIWSLT07_devset3_*, to tune the feature weights.Chinese EnglishstemmedSentences 39,963Words 351,060 377,890Train.cor-pus Vocabu-lary11,302 7,610Sentences 506 Dev.Set Words 3,826Sentences 489 TestSet Words 3,189Table 1.
The statistics of the corpus1: Function Example_Decoder(I,examples)2: Input: Input sentence I?Similar Examples examples3: Output: The best N tranlsations4: Begin5:   For each exampleA in examples Do6:     templates = Match(exampleA,I);7:     AddTemplate(templates,I);8:  End {For}9:  If templates is null then10:    templates = CYK_Decoder(I);11:    return templates;12: For each templateA in templates Do13:   If templateA is complete then14:      AddTemplate_Complete(templateA,I);15:   Else16:      RemoveTemplate(templateA,I);17:      For each untranslated phraseB in templateA do18:        childTemplates = Example_Decoder(phraseB);19:        For each childTemplateC in childTemplates Do20:          templateD=MergeTemplate(templateA,childTemplateC);21:    End{If}22:    AddTemplate(templateD,I);23:  End{For}24:  return BEST_N(complete_templates);28: EndFigure 5.
The decoding algorithm.Considering the size of the training corpus isrelatively small, and the words in Chinese have nomorphological changes, we stemmed the words inthe English sentences.Table 1 shows the statistics for the training cor-pus, development set and test set.In order to compare with the other SMT systems,we choose the Moses1, which is an extension to thestate-of-the-art SMT system Pharaoh (Koehn,2004).
We use the default tool in the Moses to trainthe model and tune the weights, in which the wordalignment tool is Giza++ (Och and Ney 2003) andthe language model tool is SRILM(Stolcke, 2002).The test results are showed in Table2.The first column lists the different MT systems,and the second column lists the Bleu scores (Pap-ineni et.
al, 2002) for the four decoders.The first system is the Moses, and the second isour SMT system described in section 2, whichusing a CKY-style decoder.
We take them as base-line systems.
The third is the hybrid system but1 http://www.statmt.org/moses/.7Sixth SIGHAN Workshop on Chinese Language Processingonly using the fast retrieval module and the fourthis the hybrid system with refined retrieval module.Considering the result from the Moses, wethink that maybe the size of the training corpus istoo small, so that the word alignment obtained byGiza++ is poor.The results show that the example-based de-coder achieves an improvement over the baselinedecoders.Decoder BleuMoses 22.61SMT-CKY 28.33Hybrid MT with fast retrieval 30.03Hybrid MT with refined retrieval 33.05Table 2.
Test results for several systems.5 Related worksThere is some works about the hybrid machinetranslation.
One way is to merge EBMT and SMTresources, such as Groves and Way (2005).Another way is to implement an exmaple-baseddecoder, Watanabe and Sumita (2003) presents anexample-based decoder, which using a informationretrieval framework to retrieve the examples; andwhen decoding, which runs a hill-climbing algo-rithm to modify the translation example ( Ck, Ek,Ak) to obtain an alignment ( C0, E'k, A'k).6 ConclusionsIn this paper, we proposed a SMT system with anexample-based decoder for the spoken languagemachine translation.
This approach will take ad-vantage of the constituent tree within the transla-tion examples to constrain the flexible word re-ordering in the spoken language, and it will alsomake the omitted words have the chance to betranslated.
Combining with the re-ordering modeland the translation models in the SMT, the exam-ple-based decoder obtains an improvement overthe baseline phrase-based SMT system.In the future, we will test our method in thewritten text corpus.
In addition, we will improvethe methods to handle the morphological changesfrom the stemmed English words.AcknowledgementsThis work is supported by the National ScienceFoundation of China under Grants No.
60573057,60473057 and 90604007.ReferencesWen-Han Chao and Zhou-Jun Li.(2007).
IncorporatingConstituent Structure Constraint into DiscriminativeWord Alignment.
MT Summit XI, Copenhagen,Denmark, September 10-14, 2007. pp.97-103.Wen-Han Chao, Zhou-Jun Li, and Yue-Xin Chen.
(2007)An Integrated Reordering Model for Statistical Ma-chine Translation.
In proceedings of MICAI 2007,LNAI 4827, pp.
955?965, 2007.David Chiang.
(2005).
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In Proc.of ACL 2005, pages 263?270.Declan Groves and Andy Way: Hybrid Example-BasedSMT: the Best of Both Worlds?
In Proceedings of theACL Workshop on Building and Using Parallel Texts,pp.
183-190(2005)P.
Koehn.
(2004) Pharaoh: a beam search decoder forphrase-based statistical machine translation models.In: Proceedings of the Sixth Conference of the Asso-ciation for Machine Translation in the Americas, pp.115?124.R.
Moore.
(2005).
A discriminative framework for bilin-gual word alignment.
In Proceedings of HLT-EMNLP, pages 81?88, Vancouver, Canada, October.Franz Joseph Och and Hermann Ney.(2002).
Discrimi-native training and maximum entropy models for sta-tistical machine translation.
In Proceedings of the40th Annual Meeting of the ACL, pp.
295?302.Franz Joseph Och and Hermann Ney.
(2003) A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics 29(1), 19?52Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu.
(2002).
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting of the Association foComputational Linguistics (ACL), Philadelphia, July2002, pp.
311-318.A.
Stolcke.
(2002).
SRILM ?
An extensible languagemodeling toolkit.
In Proceedings of the InternationalConference on Spoken Language Processing, Denver,Colorado, 2002, pp.
901?904.Taro Watanabe and Eiichiro Sumita.
(2003).
Example-based Decoding for Statistical Machine Translation.In Machine Translation Summit IX pp.
410-417.Dekai Wu.
(1997).
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Cor-pora.
Computational Linguistics, 23(3):374.8Sixth SIGHAN Workshop on Chinese Language Processing
