Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 59?68,Honolulu, October 2008. c?2008 Association for Computational LinguisticsDiscriminative Learning of Selectional Preference from Unlabeled TextShane BergsmaDepartment of Computing ScienceUniversity of AlbertaEdmonton, AlbertaCanada, T6G 2E8bergsma@cs.ualberta.caDekang LinGoogle, Inc.1600 Amphitheatre ParkwayMountain ViewCalifornia, 94301lindek@google.comRandy GoebelDepartment of Computing ScienceUniversity of AlbertaEdmonton, AlbertaCanada, T6G 2E8goebel@cs.ualberta.caAbstractWe present a discriminative method for learn-ing selectional preferences from unlabeledtext.
Positive examples are taken from ob-served predicate-argument pairs, while nega-tives are constructed from unobserved combi-nations.
We train a Support Vector Machineclassifier to distinguish the positive from thenegative instances.
We show how to parti-tion the examples for efficient training with57 thousand features and 6.5 million traininginstances.
The model outperforms other re-cent approaches, achieving excellent correla-tion with human plausibility judgments.
Com-pared to Mutual Information, it identifies 66%more verb-object pairs in unseen text, and re-solves 37% more pronouns correctly in a pro-noun resolution experiment.1 IntroductionSelectional preferences (SPs) tell us which argu-ments are plausible for a particular predicate.
Forexample, Table 2 (Section 4.4) lists plausible andimplausible direct objects (arguments) for particu-lar verbs (predicates).
SPs can help resolve syntac-tic, word sense, and reference ambiguity (Clark andWeir, 2002), and so gathering them has received alot of attention in the NLP community.One way to determine SPs is from co-occurrencesof predicates and arguments in text.
Unfortunately,no matter how much text we use, many acceptablepairs will be missing.
Bikel (2004) found that only1.49% of the bilexical dependencies considered byCollins?
parser during decoding were observed dur-ing training.
In our parsed corpus (Section 4.1),for example, we find eat with nachos, burritos, andtacos, but not with the equally tasty quesadillas,chimichangas, or tostadas.
Rather than solely re-lying on co-occurrence counts, we would like to usethem to generalize to unseen pairs.In particular, we would like to exploit a numberof arbitrary and potentially overlapping propertiesof predicates and arguments when we assign SPs.We do this by representing these properties as fea-tures in a linear classifier, and training the weightsusing discriminative learning.
Positive examplesare taken from observed predicate-argument pairs,while pseudo-negatives are constructed from unob-served combinations.
We train a Support Vector Ma-chine (SVM) classifier to distinguish the positivesfrom the negatives.
We refer to our model?s scoresas Discriminative Selectional Preference (DSP).
Bycreating training vectors automatically, DSP enjoysall the advantages of supervised learning, but with-out the need for manual annotation of examples.We evaluate DSP on the task of assigning verb-object selectional preference.
We encode a noun?stextual distribution as feature information.
Thelearned feature weights are linguistically interesting,yielding high-quality similar-word lists as latent in-formation.
Despite its representational power, DSPscales to real-world data sizes: examples are parti-tioned by predicate, and a separate SVM is trainedfor each partition.
This allows us to efficiently learnwith over 57 thousand features and 6.5 million ex-amples.
DSP outperforms recently proposed alterna-tives in a range of experiments, and better correlateswith human plausibility judgments.
It also showsstrong gains over a Mutual Information-based co-59occurrence model on two tasks: identifying objectsof verbs in an unseen corpus and finding pronominalantecedents in coreference data.2 Related WorkMost approaches to SPs generalize from observedpredicate-argument pairs to semantically similarones by modeling the semantic class of the argu-ment, following Resnik (1996).
For example, wemight have a class Mexican Food and learn that theentire class is suitable for eating.
Usually, the classesare from WordNet (Miller et al, 1990), althoughthey can also be inferred from clustering (Rooth etal., 1999).
Brockmann and Lapata (2003) comparea number of WordNet-based approaches, includingResnik (1996), Li and Abe (1998), and Clark andWeir (2002), and found that the more sophisticatedclass-based approaches do not always outperformsimple frequency-based models.Another line of research generalizes using simi-lar words.
Suppose we are calculating the proba-bility of a particular noun, n, occurring as the ob-ject argument of a given verbal predicate, v. LetPr(n|v) be the empirical maximum-likelihood esti-mate from observed text.
Dagan et al (1999) definethe similarity-weighted probability, PrSIM, to be:PrSIM(n|v) =?v?
?SIMS(v)Sim(v?, v)Pr(n|v?)
(1)where Sim(v?, v) returns a real-valued similarity be-tween two verbs v?
and v (normalized over all pairsimilarities in the sum).
In contrast, Erk (2007)generalizes by substituting similar arguments, whileWang et al (2005) use the cross-product of simi-lar pairs.
One key issue is how to define the setof similar words, SIMS(w).
Erk (2007) compared anumber of techniques for creating similar-word setsand found that both the Jaccard coefficient and Lin(1998a)?s information-theoretic metric work best.Similarity-smoothed models are simple to compute,potentially adaptable to new domains, and requireno manually-compiled resources such as WordNet.Selectional Preferences have also been a recentfocus of researchers investigating the learning ofparaphrases and inference rules (Pantel et al, 2007;Roberto et al, 2007).
Inferences such as ?
[X winsY] ?
[X plays Y]?
are only valid for certain argu-ments X and Y.
We follow Pantel et al (2007) in us-ing automatically-extracted semantic classes to helpcharacterize plausible arguments.Discriminative techniques are widely used in NLPand have been applied to the related tasks of wordprediction and language modeling.
Even-Zohar andRoth (2000) use a classifier to predict the most likelyword to fill a position in a sentence (in their ex-periments: a verb) from a set of candidates (setsof verbs), by inspecting the context of the targettoken (e.g., the presence or absence of a particu-lar nearby word in the sentence).
This approachcan therefore learn which specific arguments occurwith a particular predicate.
In comparison, our fea-tures are second-order: we learn what kinds of argu-ments occur with a predicate by encoding featuresof the arguments.
Recent distributed and latent-variable models also represent words with featurevectors (Bengio et al, 2003; Blitzer et al, 2005).Many of these approaches learn both the featureweights and the feature representation.
Vectors mustbe kept low-dimensional for tractability, while learn-ing and inference on larger scales is impractical.
Bypartitioning our examples by predicate, we can effi-ciently use high-dimensional, sparse vectors.Our technique of generating negative examplesis similar to the approach of Okanohara and Tsujii(2007).
They learn a classifier to disambiguate ac-tual sentences from pseudo-negative examples sam-pled from an N-gram language model.
Smith andEisner (2005) also automatically generate negativeexamples.
They perturb their input sequence (e.g.the sentence word order) to create a neighborhood ofimplicit negative evidence.
We create negatives bysubstitution rather than perturbation, and use corpus-wide statistics to choose our negative instances.3 Methodology3.1 Creating ExamplesTo learn a discriminative model of selectional pref-erence, we create positive and negative training ex-amples automatically from raw text.
To create thepositives, we automatically parse a large corpus, andthen extract the predicate-argument pairs that havea statistical association in this data.
We measurethis association using pointwise Mutual Information(MI) (Church and Hanks, 1990).
The MI between a60verb predicate, v, and its object argument, n, is:MI(v, n) = log Pr(v, n)Pr(v)Pr(n) = logPr(n|v)Pr(n) (2)If MI>0, the probability v and n occur together isgreater than if they were independently distributed.We create sets of positive and negative examplesseparately for each predicate, v. First, we extract allpairs where MI(v, n)>?
as positives.
For each pos-itive, we create pseudo-negative examples, (v, n?
),by pairing v with a new argument, n?, that either hasMI below the threshold or did not occur with v in thecorpus.
We require each negative n?
to have a similarfrequency to its corresponding n. This prevents ourlearning algorithm from focusing on any accidentalfrequency-based bias.
We mix in K negatives foreach positive, sampling without replacement to cre-ate all the negatives for a particular predicate.
Foreach v, 1K+1 of its examples will be positive.
Thethreshold ?
represents a trade-off between capturinga large number of positive pairs and ensuring thesepairs have good association.
Similarly, K is a trade-off between the number of examples and the com-putational efficiency.
Ultimately, these parametersshould be optimized for task performance.Of course, some negatives will actually be plau-sible arguments that were unobserved due to sparse-ness.
Fortunately, modern discriminative methodslike soft-margin SVMs can learn in the face of labelerror by allowing slack, subject to a tunable regular-ization penalty (Cortes and Vapnik, 1995).If MI is a sparse and imperfect model of SP, whatcan DSP gain by training on MI?s scores?
We canregard DSP as learning a view of SP that is or-thogonal to MI, in a co-training sense (Blum andMitchell, 1998).
MI labels the data based solelyon co-occurrence; DSP uses these labels to iden-tify other regularities ?
ones that extend beyond co-occurring words.
For example, many instances ofn where MI(eat, n)>?
also have MI(buy, n)>?
andMI(cook, n)>?
.
Also, compared to other nouns,a disproportionate number of eat-nouns are lower-case, single-token words, and they rarely containdigits, hyphens, or begin with a human first namelike Bob.
DSP encodes these interdependent prop-erties as features in a linear classifier.
This classi-fier can score any noun as a plausible argument ofeat if indicative features are present; MI can onlyassign high plausibility to observed (eat,n) pairs.Similarity-smoothed models can make use of theregularities across similar verbs, but not the finer-grained string- and token-based features.Our training examples are similar to the data cre-ated for pseudodisambiguation, the usual evalua-tion task for SP models (Erk, 2007; Keller and La-pata, 2003; Rooth et al, 1999).
This data con-sists of triples (v, n, n?)
where v, n is a predicate-argument pair observed in the corpus and v, n?
hasnot been observed.
The models score correctlyif they rank observed (and thus plausible) argu-ments above corresponding unobserved (and thuslikely implausible) ones.
We refer to this as Pair-wise Disambiguation.
Unlike this task, we classifyeach predicate-argument pair independently as plau-sible/implausible.
We also use MI rather than fre-quency to define the positive pairs, ensuring that thepositive pairs truly have a statistical association, andare not simply the result of parser error or noise.13.2 Partitioning for Efficient TrainingAfter creating our positive and negative trainingpairs, we must select a feature representation for ourexamples.
Let ?
be a mapping from a predicate-argument pair (v, n) to a feature vector, ?
:(v, n) ?
??1...?k?.
Predictions are made basedon a weighted combination of the features, y =?
??
(v, n), where ?
is our learned weight vector.We can make training significantly more efficientby using a special form of attribute-value features.Let every feature ?i be of the form ?i(v, n) = ?v =v??f(n)?.
That is, every feature is an intersection ofthe occurrence of a particular predicate, v?, and somefeature of the argument f(n).
For example, a fea-ture for a verb-object pair might be, ?the verb is eatand the object is lower-case.?
In this representation,features for one predicate will be completely inde-pendent from those for every other predicate.
Thusrather than a single training procedure, we can actu-ally partition the examples by predicate, and train a1For a fixed verb, MI is proportional to Keller and Lapata(2003)?s conditional probability scores for pseudodisambigua-tion of (v, n, n?)
triples: Pr(v|n) = Pr(v, n)/Pr(n), which wasshown to be a better measure of association than co-occurrencefrequency f(v, n).
Normalizing by Pr(v) (yielding MI) allowsus to use a constant threshold across all verbs.
MI was alsorecently used for inference-rule SPs by Pantel et al (2007).61classifier for each predicate independently.
The pre-diction becomes yv = ?v ?
?v(n), where ?v are thelearned weights corresponding to predicate v and allfeatures ?v(n)=f(n) depend on the argument only.Some predicate partitions may have insufficientexamples for training.
Also, a predicate may oc-cur in test data that was unseen during training.
Tohandle these instances, we decided to cluster low-frequency predicates.
In our experiments assigningSP to verb-object pairs, we cluster all verbs that haveless than 250 positive examples, using clusters gen-erated by the CBC algorithm (Pantel and Lin, 2002).For example, the low-frequency verbs incarcerate,parole, and court-martial are all mapped to the samepartition, while more-frequent verbs like arrest andexecute each have their own partition.
About 5.5%of examples are clustered, corresponding to 30% ofthe 7367 total verbs.
40% of verbs (but only 0.6% ofexamples) were not in any CBC cluster; these weremapped to a single backoff partition.The parameters for each partition, ?v, can betrained with any supervised learning technique.
Weuse SVM (Section 4.1) because it is effective in simi-lar high-dimensional, sparse-vector settings, and hasan efficient implementation (Joachims, 1999).
InSVM, the sign of yv gives the classification.
We canalso use the scalar yv as our DSP score (i.e.
the posi-tive distance from the separating SVM hyperplane).3.3 FeaturesThis section details our argument features, f(n), forassigning verb-object selectional preference.
For averb predicate (or partition) v and object argumentn, the form of our classifier is yv = ?i ?vi fi(n).3.3.1 Verb co-occurrenceWe provide features for the empirical probabilityof the noun occurring as the object argument of otherverbs, Pr(n|v?).
If we were to only use these features(indexing the feature weights by each verb v?
), theform of our classifier would be:yv =?v??vv?Pr(n|v?)
(3)Note the similarity between Equation (3) and Equa-tion (1).
Now the feature weights, ?vv?
, take the roleof the similarity function, Sim(v?, v).
Unlike Equa-tion (1), however, these weights are not set by anexternal similarity algorithm, but are optimized todiscriminate the positive and negative training ex-amples.
We need not restrict ourselves to a short listof similar verbs; we include Probj(n|v?)
features forevery verb that occurs more than 10 times in our cor-pus.
?vv?
may be positive or negative, depending onthe relation between v?
and v. We also include fea-tures for the probability of the noun occurring as thesubject of other verbs, Prsubj(n|v?).
For example,nouns that can be the object of eat will also occur asthe subject of taste and contain.
Other contexts, suchas adjectival and nominal predicates, could also aidthe prediction, but have not yet been investigated.The advantage of tuning similarity to the appli-cation of interest has been shown previously byWeeds and Weir (2005).
They optimize a few meta-parameters separately for the tasks of thesaurus gen-eration and pseudodisambiguation.
Our approach,on the other hand, discriminatively sets millions ofindividual similarity values.
Like Weeds and Weir(2005), our similarity values are asymmetric.3.3.2 String-basedWe include several simple character-based fea-tures of the noun string: the number of tokens, thecase, and whether it contains digits, hyphens, anapostrophe, or other punctuation.
We also include afeature for the first and last token, and fire indicatorfeatures if any token in the noun occurs on in-houselists of given names, family names, cities, provinces,countries, corporations, languages, etc.
We also firea feature if a token is a corporate designation (likeinc.
or ltd.) or a human one (like Mr. or Sheik).3.3.3 Semantic classesMotivated by previous SP models that make useof semantic classes, we generated word clusters us-ing CBC (Pantel and Lin, 2002) on a 10 GB corpus,giving 3620 clusters.
If a noun belongs in a cluster,a corresponding feature fires.
If a noun is in none ofthe clusters, a no-class feature fires.As an example, CBC cluster 1891 contains:sidewalk, driveway, roadway, footpath,bridge, highway, road, runway, street, alley,path, Interstate, .
.
.In our training data, we have examples like widenhighway, widen road and widen motorway.
If we62see that we can widen a highway, we learn that wecan also widen a sidewalk, bridge, runway, etc.We also made use of the person-name/instancepairs automatically extracted by Fleischman et al(2003).2 This data provides counts for pairs suchas ?Edwin Moses, hurdler?
and ?William Farley, in-dustrialist.?
We have features for all concepts andtherefore learn their association with each verb.4 Experiments and Results4.1 Set upWe parsed the 3 GB AQUAINT corpus (Voorhees,2002) using Minipar (Lin, 1998b), and collectedverb-object and verb-subject frequencies, buildingan empirical MI model from this data.
Verbs andnouns were converted to their (possibly multi-token)root, and string case was preserved.
Passive sub-jects (the car was bought) were converted to objects(bought car).
We set the MI-threshold, ?
, to be 0,and the negative-to-positive ratio, K, to be 2.Numerous previous pseudodisambiguation evalu-ations only include arguments that occur between 30and 3000 times (Erk, 2007; Keller and Lapata, 2003;Rooth et al, 1999).
Presumably the lower bound isto help ensure the negative argument is unobservedbecause it is unsuitable, not because of data sparse-ness.
We wish to use our model on arguments ofany frequency, including those that never occurredin the training corpus (and therefore have empty co-occurrence features (Section 3.3.1)).
We proceed asfollows: first, we exclude pairs whenever the nounoccurs less than 3 times in our corpus, removingmany misspellings and other noun noise.
Next, weomit verb co-occurrence features for nouns that oc-cur less than 10 times, and instead fire a low-countfeature.
When we move to a new corpus, previously-unseen nouns are treated like these low-count train-ing nouns.This processing results in a set of 6.8 millionpairs, divided into 2318 partitions (192 of whichare verb clusters (Section 3.2)).
For each parti-tion, we take 95% of the examples for training,2.5% for development and 2.5% for a final unseentest set.
We provide full results for two models:DSPcooc which only uses the verb co-occurrence fea-tures, and DSPall which uses all the features men-2Available at http://www.mit.edu/?mbf/instances.txt.gztioned in Section 3.3.
Feature values are normalizedwithin each feature type.
We train our (linear kernel)discriminative models using SVMlight (Joachims,1999) on each partition, but set meta-parameters C(regularization) and j (cost of positive vs. nega-tive misclassifications: max at j=2) on the macro-averaged score across all development partitions.Note that we can not use the development set to op-timize ?
and K because the development examplesare obtained after setting these values.4.2 Feature weightsIt is interesting to inspect the feature weights re-turned by our system.
In particular, the weightson the verb co-occurrence features (Section 3.3.1)provide a high-quality, argument-specific similarity-ranking of other verb contexts.
The DSP parametersfor eat, for example, place high weight on featureslike Pr(n|braise), Pr(n|ration), and Pr(n|garnish).Lin (1998a)?s similar word list for eat misses thesebut includes sleep (ranked 6) and sit (ranked 14), be-cause these have similar subjects to eat.
Discrimina-tive, context-specific training seems to yield a bet-ter set of similar predicates, e.g.
the highest-rankedcontexts for DSPcooc on the verb join,3lead 1.42, rejoin 1.39, form 1.34, belong to1.31, found 1.31, quit 1.29, guide 1.19, induct1.19, launch (subj) 1.18, work at 1.14give a better SIMS(join) for Equation (1) than thetop similarities returned by (Lin, 1998a):participate 0.164, lead 0.150, return to 0.148,say 0.143, rejoin 0.142, sign 0.142, meet0.142, include 0.141, leave 0.140, work 0.137Other features are also weighted intuitively.
Notethat case is a strong indicator for some arguments,for example the weight on being lower-case is highfor become (0.972) and eat (0.505), but highly nega-tive for accuse (-0.675) and embroil (-0.573) whichoften take names of people and organizations.4.3 PseudodisambiguationWe first evaluate DSP on disambiguating posi-tives from pseudo-negatives, comparing to recently-3Which all correspond to nouns occurring in the object po-sition of the verb (e.g.
Probj(n|lead)), except ?launch (subj)?which corresponds to Prsubj(n|launch).63System MacroAvg MicroAvg PairwiseP R F P R F Acc CovDagan et al (1999) 0.36 0.90 0.51 0.68 0.92 0.78 0.58 0.98Erk (2007) 0.49 0.66 0.56 0.70 0.82 0.76 0.72 0.83Keller and Lapata (2003) 0.72 0.34 0.46 0.80 0.50 0.62 0.80 0.57DSPcooc 0.53 0.72 0.61 0.73 0.94 0.82 0.77 1.00DSPall 0.60 0.71 0.65 0.77 0.90 0.83 0.81 1.00Table 1: Pseudodisambiguation results averaged across each example (MacroAvg), weighted by word frequency (Mi-croAvg), plus coverage and accuracy of pairwise competition (Pairwise).proposed systems that also require no manually-compiled resources like WordNet.
We convert Da-gan et al (1999)?s similarity-smoothed probabilityto MI by replacing the empirical Pr(n|v) in Equa-tion (2) with the smoothed PrSIM from Equation (1).We also test an MI model inspired by Erk (2007):MISIM(n, v) = log?n?
?SIMS(n)Sim(n?, n) Pr(v, n?)Pr(v)Pr(n?
)We gather similar words using Lin (1998a), miningsimilar verbs from a comparable-sized parsed cor-pus, and collecting similar nouns from a broader 10GB corpus of English text.4We also use Keller and Lapata (2003)?s approachto obtaining web-counts.
Rather than mining parsetrees, this technique retrieves counts for the pattern?V Det N?
in raw online text, where V is any in-flection of the verb, Det is the, a, or the emptystring, and N is the singular or plural form of thenoun.
We compute a web-based MI by collectingPr(n, v), Pr(n), and Pr(v) using all inflections, ex-cept we only use the root form of the noun.
Ratherthan using a search engine, we obtain counts fromthe Google Web 5-gram Corpus.5All systems are thresholded at zero to make a clas-sification.
Unlike DSP, the comparison systems may4For both the similar-noun and similar-verb smoothing, weonly smooth over similar pairs that occurred in the corpus.While averaging over all similar pairs tends to underestimatethe probability, averaging over only the observed pairs tends tooverestimate it.
We tested both and adopt the latter because itresulted in better performance on our development set.5Available from the LDC as LDC2006T13.
This collectionwas generated from approximately 1 trillion tokens of onlinetext.
Unfortunately, tokens appearing less than 200 times havebeen mapped to the ?UNK?
symbol, and only N-grams appear-ing more than 40 times are included.
Unlike results from searchengines, however, experiments with this corpus are replicable.not be able to provide a score for each example.The similarity-smoothed examples will be undefinedif SIMS(w) is empty.
Also, the Keller and Lapata(2003) approach will be undefined if the pair is un-observed on the web.
As a reasonable default forthese cases, we assign them a negative decision.We evaluate disambiguation using precision (P),recall (R), and their harmonic mean, F-Score (F).Table 1 gives the results of our comparison.
In theMacroAvg results, we weight each example equally.For MicroAvg, we weight each example by the fre-quency of the noun.
To more directly compare withprevious work, we also reproduced Pairwise Disam-biguation by randomly pairing each positive withone of the negatives and then evaluating each systemby the percentage it ranks correctly (Acc).
For thecomparison approaches, if one score is undefined,we choose the other one.
If both are undefined, weabstain from a decision.
Coverage (Cov) is the per-cent of pairs where a decision was made.6Our simple system with only verb co-occurrencefeatures, DSPcooc, outperforms all comparison ap-proaches.
Using the richer feature set in DSPallresults in a statistically significant gain in perfor-mance, up to an F-Score of 0.65 and a pairwisedisambiguation accuracy of 0.81.7 DSPall has bothbroader coverage and better accuracy than all com-peting approaches.
In the remainder of the experi-ments, we use DSPall and refer to it simply as DSP.Some errors are because of plausible but unseenarguments being used as test-set pseudo-negatives.For example, for the verb damage, DSP?s three mosthigh-scoring false positives are the nouns jetliner,carpet, and gear.
While none occur with damage in6I.e.
we use the ?half coverage?
condition from Erk (2007).7The differences between DSPall and all comparison sys-tems are statistically significant (McNemar?s test, p<0.01).6400.10.20.30.40.50.60.70.80.910  100  1000  10000  100000  1e+06F-ScoreNoun FrequencyDSPallErk (2007)Keller and Lapata (2003)Figure 1: Disambiguation results by noun frequency.our corpus, all intuitively satisfy the verb?s SPs.MacroAvg performance is worse than MicroAvgbecause all systems perform better on frequentnouns.
When we plot F-Score by noun frequency(Figure 1), we see that DSP outperforms comparisonapproaches across all frequencies, but achieves itsbiggest gains on the low-frequency nouns.
A richerfeature set alows DSP to make correct inferences onexamples that provide minimal co-occurrence data.These are also the examples for which we would ex-pect co-occurrence models like MI to fail.As a further experiment, we re-trained DSP butwith only the string-based features removed.
Overallmacro-averaged F-score dropped from 0.65 to 0.64(a statistically significant reduction in performance).The system scored nearly identically to DSP on thehigh-frequency nouns, but performed roughly 15%worse on the nouns that occurred less than ten times.This shows that the string-based features are impor-tant for selectional preference, and particularly help-ful for low-frequency nouns.4.4 Human PlausibilityTable 2 compares some of our systems on data usedby Resnik (1996) (also Appendix 2 in Holmes et al(1989)).
The plausibility of these pairs was initiallyjudged based on the experimenters?
intuitions, andlater confirmed in a human experiment.
We includethe scores of Resnik?s system, and note that its errorswere attributed to sense ambiguity and other limi-tations of class-based approaches (Resnik, 1996).88For example, warn-engine scores highly because enginesare in the class entity, and physical entities (e.g.
people) areoften objects of warn.
Unlike DSP, Resnik?s approach cannotlearn that for warn, ?the property of being a person is moreSeen Criteria Unseen Verb-Object Freq.All = 1 = 2 = 3 > 3MI > 0 0.44 0.33 0.57 0.70 0.82Freq.
> 0 0.57 0.45 0.76 0.89 0.96DSP > 0 0.73 0.69 0.80 0.85 0.88Table 3: Recall on identification of Verb-Object pairsfrom an unseen corpus (divided by pair frequency).The other comparison approaches also make a num-ber of mistakes, which can often be traced to a mis-guided choice of similar word to smooth with.We also compare to our empirical MI model,trained on our parsed corpus.
Although Resnik(1996) reported that 10 of the 16 plausible pairs didnot occur in his training corpus, all of them occurredin ours and hence MI gives very reasonable scoreson the plausible objects.
It has no statistics, however,for many of the implausible ones.
DSP can makefiner decisions than MI, recognizing that ?warningan engine?
is more absurd than ?judging a climate.
?4.5 Unseen Verb-Object IdentificationWe next compare MI and DSP on a much larger setof plausible examples, and also test how well themodels generalize across data sets.
We took the MIand DSP systems trained on AQUAINT and askedthem to rate observed (and thus likely plausible)verb-object pairs taken from an unseen corpus.
Weextracted the pairs by parsing the San Jose MercuryNews (SJM) section of the TIPSTER corpus (Har-man, 1992).
Each unique verb-object pair is a singleinstance in this evaluation.Table 3 gives recall across all pairs (All) andgrouped by pair-frequency in the unseen corpus (1,2, 3, >3).
DSP accepts far more pairs than MI(73% vs. 44%), even far more than a system thataccepts any previously observed verb-object combi-nation as plausible (57%).
Recall is higher on morefrequent verb-object pairs, but 70% of the pairs oc-curred only once in the corpus.
Even if we smoothMI by smoothing Pr(n|v) in Equation 2 using modi-fied KN-smoothing (Chen and Goodman, 1998), therecall of MI>0 on SJM only increases from 44.1%to 44.9%, still far below DSP.
Frequency-basedmodels have fundamentally low coverage.
As fur-important than the property of being an entity?
(Resnik, 1996).65Verb Plaus./Implaus.
Resnik Dagan et al Erk MI DSPsee friend/method 5.79/-0.01 0.20/1.40* 0.46/-0.07 1.11/-0.57 0.98/0.02read article/fashion 6.80/-0.20 3.00/0.11 3.80/1.90 4.00/?
2.12/-0.65find label/fever 1.10/0.22 1.50/2.20* 0.59/0.01 0.42/0.07 1.61/0.81hear story/issue 1.89/1.89* 0.66/1.50* 2.00/2.60* 2.99/-1.03 1.66/0.67write letter/market 7.26/0.00 2.50/-0.43 3.60/-0.24 5.06/-4.12 3.08/-1.31urge daughter/contrast 1.14/1.86* 0.14/1.60* 1.10/3.60* -0.95/?
-0.34/-0.62warn driver/engine 4.73/3.61 1.20/0.05 2.30/0.62 2.87/?
2.00/-0.99judge contest/climate 1.30/0.28 1.50/1.90* 1.70/1.70* 3.90/?
1.00/0.51teach language/distance 1.87/1.86 2.50/1.30 3.60/2.70 3.53/?
1.86/0.19show sample/travel 1.44/0.41 1.60/0.14 0.40/-0.82 0.53/-0.49 1.00/-0.83expect visit/mouth 0.59/5.93* 1.40/1.50* 1.40/0.37 1.05/-0.65 1.44/-0.15answer request/tragedy 4.49/3.88 2.70/1.50 3.10/-0.64 2.93/?
1.00/0.01recognize author/pocket 0.50/0.50* 0.03/0.37* 0.77/1.30* 0.48/?
1.00/0.00repeat comment/journal 1.23/1.23* 2.30/1.40 2.90/?
2.59/?
1.00/-0.48understand concept/session 1.52/1.51 2.70/0.25 2.00/-0.28 3.96/?
2.23/-0.46remember reply/smoke 1.31/0.20 2.10/1.20 0.54/2.60* 1.13/-0.06 1.00/-0.42Table 2: Selectional ratings for plausible/implausible direct objects (Holmes et al, 1989).
Mistakes are marked withan asterisk (*), undefined scores are marked with a dash (?).
Only DSP is completely defined and completely correct.00.10.20.30.40.50.60.70.80  0.2  0.4  0.6  0.8  1InterpolatedPrecisionRecallDSP>TMI>TDSP>0MI>0Figure 2: Pronoun resolution precision-recall on MUC.ther evidence, if we build a model of MI on the SJMcorpus and use it in our pseudodisambiguation ex-periment (Section 4.3), MI>0 gets a MacroAvg pre-cision of 86% but a MacroAvg recall of only 12%.94.6 Pronoun ResolutionFinally, we evaluate DSP on a common applicationof selectional preferences: choosing the correct an-tecedent for pronouns in text (Dagan and Itai, 1990;Kehler et al, 2004).
We study the cases where a9Recall that even the Keller and Lapata (2003) system, builton the world?s largest corpus, achieves only 34% recall (Table 1)(with only 48% of positives and 27% of all pairs previouslyobserved, but see Footnote 5).pronoun is the direct object of a verb predicate, v. Apronoun?s antecedent must obey v?s selectional pref-erences.
If we have a better model of SP, we shouldbe able to better select pronoun antecedents.We parsed the MUC-7 (1997) coreference corpusand extracted all pronouns in a direct object rela-tion.
For each pronoun, p, modified by a verb, v, weextracted all preceding nouns within the current orprevious sentence.
Thirty-nine anaphoric pronounshad an antecedent in this window and are used inthe evaluation.
For each p, let N(p)+ by the set ofpreceding nouns coreferent with p, and let N(p)?be the remaining non-coreferent nouns.
We takeall (v, n+) where n+ ?
N(p)+ as positive, and allother pairs (v, n?
), n?
?
N(p)?
as negative.We compare MI and DSP on this set, classifyingevery (v, n) with MI>T (or DSP>T ) as positive.By varying T , we get a precision-recall curve (Fig-ure 2).
Precision is low because, of course, thereare many nouns that satisfy the predicate?s SPs thatare not coreferent.
DSP>0 has both a higher recalland higher precision than accepting every pair pre-viously seen in text (the right-most point on MI>T ).The DSP>T system achieves higher precision thanMI>T for points where recall is greater than 60%(where MI<0).
Interestingly, the recall of MI>0 is66System AccMost-Recent Noun 17.9%Maximum MI 28.2%Maximum DSP 38.5%Table 4: Pronoun resolution accuracy on nouns in currentor previous sentence in MUC.higher here than it is for general verb-objects (Sec-tion 4.5).
On the subset of pairs with strong empir-ical association (MI>0), MI generally outperformsDSP at equivalent recall values.We next compare MI and DSP as stand-alone pro-noun resolution systems (Table 4).
As a standardbaseline, for each pronoun, we choose the mostrecent noun in text as the pronoun?s antecedent,achieving 17.9% resolution accuracy.
This baselineis quite low because many of the most-recent nounsare subjects of the pronoun?s verb phrase, and there-fore resolution violates syntactic coreference con-straints.
If instead we choose the previous noun withthe highest MI as antecedent, we get an accuracy of28.2%, while choosing the previous noun with thehighest DSP achieves 38.5%.
DSP resolves 37%more pronouns correctly than MI.
We leave as fu-ture work a full-scale pronoun resolution system thatincorporates both MI and DSP as backed-off, inter-polated, or separate semantic features.5 Conclusions and Future WorkWe have presented a simple, effective model of se-lectional preference based on discriminative train-ing.
Supervised techniques typically achieve higherperformance than unsupervised models, and we du-plicate these gains with DSP.
Here, however, thesegains come at no additional labeling cost, as train-ing examples are generated automatically from un-labeled text.
DSP allows an arbitrary combination offeatures, including verb co-occurrence features thatyield high-quality similar-word lists as latent output.This work only scratches the surface of possible fea-ture mining; information from WordNet relations,Wikipedia categories, or parallel corpora could alsoprovide valuable clues to SP.
Also, if any other sys-tem were to exceed DSP?s performance, it could alsobe included as one of DSP?s features.It would be interesting to expand our co-occurrence features, including co-occurrence countsacross more grammatical relations and using countsfrom external, unparsed corpora like the world wideweb.
We could also reverse the role of noun and verbin our training, having verb-specific features anddiscriminating separately for each argument noun.The latent information would then be lists of similarnouns.Finally, note that while we focused on word-wordco-occurrences, sense-sense SPs can also be learnedwith our algorithm.
If our training corpus was sense-labeled, we could run our algorithm over the sensesrather than the words.
The resulting model wouldthen require sense-tagged input if it were to be usedwithin an application like parsing or coreference res-olution.
Also, like other models of SP, our techniquecan also be used for sense disambiguations: theweightings on our semantic class features indicate,for a particular noun, which of its senses (classes) ismost compatible with each verb.AcknowledgmentsWe gratefully acknowledge support from the Natu-ral Sciences and Engineering Research Council ofCanada, the Alberta Ingenuity Fund, and the AlbertaInformatics Circle of Research Excellence.ReferencesYoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research,3:1137?1155.Daniel M. Bikel.
2004.
Intricacies of Collins?
parsingmodel.
Computational Linguistics, 30(4):479?511.John Blitzer, Amir Globerson, and Fernando Pereira.2005.
Distributed latent variable models of lexical co-occurrences.
In AISTATS.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of COLT, pages 92?100.Carsten Brockmann and Mirella Lapata.
2003.
Evalu-ating and combining approaches to selectional prefer-ence acquisition.
In EACL, pages 27?34.Stanley F. Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-eling.
TR-10-98, Harvard University.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicogra-phy.
Computational Linguistics, 16(1):22?29.67Stephen Clark and David Weir.
2002.
Class-based prob-ability estimation using a semantic hierarchy.
Compu-tational Linguistics, 28(2):187?206.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine Learning, 20(3):273?297.Ido Dagan and Alan Itai.
1990.
Automatic processing oflarge corpora for the resolution of anaphora references.In COLING, volume 3, pages 330?332.Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.1999.
Similarity-based models of word cooccurrenceprobabilities.
Machine Learning, 34(1-3):43?69.Katrin Erk.
2007.
A simple, similarity-based model forselectional preference.
In ACL, pages 216?223.Yair Even-Zohar and Dan Roth.
2000.
A classificationapproach to word prediction.
In NAACL, pages 124?131.Michael Fleischman, Eduard Hovy, and AbdessamadEchihabi.
2003.
Offline strategies for online questionanswering: answering questions before they are asked.In ACL, pages 1?7.Donna Harman.
1992.
The DARPA TIPSTER project.ACM SIGIR Forum, 26(2):26?28.Virginia M. Holmes, Laurie Stowe, and Linda Cupples.1989.
Lexical expectations in parsing complement-verb sentences.
Journal of Memory and Language,28:668?689.Thorsten Joachims.
1999.
Making large-scale SupportVector Machine learning practical.
In B. Scho?lkopfand C. Burges, editors, Advances in Kernel Methods:Support Vector Machines, pages 169?184.
MIT-Press.Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-sandr Simma.
2004.
The (non)utility of predicate-argument frequencies for pronoun interpretation.
InHLT/NAACL, pages 289?296.Frank Keller and Mirella Lapata.
2003.
Using the web toobtain frequencies for unseen bigrams.
ComputationalLinguistics, 29(3):459?484.Hang Li and Naoki Abe.
1998.
Generalizing case framesusing a thesaurus and the MDL principle.
Computa-tional Linguistics, 24(2):217?244.Dekang Lin.
1998a.
Automatic retrieval and clusteringof similar words.
In COLING-ACL, pages 768?773.Dekang Lin.
1998b.
Dependency-based evaluation ofMINIPAR.
In LREC Workshop on the Evaluation ofParsing Systems.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.
1990.Introduction to WordNet: an on-line lexical database.International Journal of Lexicography, 3(4):235?244.MUC-7.
1997.
Coreference task definition (v3.0, 13 Jul97).
In Proceedings of the Seventh Message Under-standing Conference (MUC-7).Daisuke Okanohara and Jun?ichi Tsujii.
2007.
Adiscriminative language model with pseudo-negativesamples.
In ACL, pages 73?80.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In KDD, pages 613?619.Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,Timothy Chklovski, and Eduard Hovy.
2007.
ISP:Learning inferential selectional preferences.
InNAACL-HLT, pages 564?571.Philip Resnik.
1996.
Selectional constraints: Aninformation-theoretic model and its computational re-alization.
Cognition, 61:127?159.Basili Roberto, Diego De Cao, Paolo Marocco, andMarco Pennacchiotti.
2007.
Learning selectionalpreferences for entailment or paraphrasing rules.
InRANLP.Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-roll, and Franz Beil.
1999.
Inducing a semanticallyannotated lexicon via EM-based clustering.
In ACL,pages 104?111.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: training log-linear models on unlabeled data.In ACL, pages 354?362.Ellen Voorhees.
2002.
Overview of the TREC 2002question answering track.
In Proceedings of theEleventh Text REtrieval Conference (TREC).Qin Iris Wang, Dale Schuurmans, and Dekang Lin.
2005.Strictly lexical dependency parsing.
In InternationalWorkshop on Parsing Technologies, pages 152?159.Julie Weeds and David Weir.
2005.
Co-occurrence re-trieval: a flexible framework for lexical distributionalsimilarity.
Computational Linguistics, 31(4):439?475.68
