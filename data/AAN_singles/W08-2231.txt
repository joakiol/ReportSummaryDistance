A Resource-Poor Approach forLinking Ontology Classes toWikipedia ArticlesNils ReiterMatthias HartungAnette FrankUniversity of Heidelberg (Germany)email: reiter@cl.uni-heidelberg.deAbstractThe applicability of ontologies for natural language processing dependson the ability to link ontological concepts and relations to their realisa-tions in texts.
We present a general, resource-poor account to create sucha linking automatically by extracting Wikipedia articles corresponding toontology classes.
We evaluate our approach in an experiment with theMusic Ontology.
We consider linking as a promising starting point forsubsequent steps of information extraction.381382 Reiter, Hartung, and Frank1 IntroductionOntologies are becoming increasingly popular as a means for formal, machine-read-able modelling of domain knowledge, in terms of concepts and relations.
Linkingontological concepts and relations to their natural language equivalents is of utmostimportance for ontology-based applications in natural language processing.
Providinglarger quantities of text that clearly belongs to a given ontological concept is a pre-requisite for further steps towards ontology population with relations and instances.We thus consider this work as a point of departure for future work on populating andlexicalizing ontologies, and their use in semantic processing.In this paper we present a method that provides relevant textual sources for a do-main ontology by linking ontological classes to the most appropriate Wikipedia arti-cles describing the respective ontological class.
The paper is structured as follows:We discuss related work in Section 2.
Section 3 presents our method for linking on-tology classes to Wikipedia articles.
The method is implemented and tested using themusic ontology (Raimond et al, 2007) and a Wikipedia dump of 2007.
We presentthis experiment in Section 4 and its evaluation in Section 5.
Section 6 concludes andgives an outlook on directions of future work.2 Related WorkOur goal is to detect the most appropriateWikipedia article for a given ontology class.As Wikipedia is a domain-independent resource, it usually contains many more sensesfor one concept name than does a domain-specific ontology.
Thus, one of the chal-lenges we meet is the need for disambiguation between multiple candidate articleswith respect to one specific ontology class.1 Therefore, we compare our approach toprevious work on sense disambiguation.
Since in our approach, we aim at minimiz-ing the degree of language- and resource dependency, our focus is on the amount ofexternal knowledge used.One method towards sense disambiguation that has been studied is to use differentkinds of text overlap: Ruiz-Casado et al (2005) calculate vector similarity between aWikipedia article and WordNet glosses based on term frequencies.
Obviously, suchglosses are not available for all languages, domains and applications.
Wu and Weld(2007) and Cucerzan (2007) calculate the overlap between contexts of named entitiesand candidate articles from Wikipedia, using overlap ratios or similarity scores in avector space model, respectively.
Both approaches disambiguate named entities usingtextual context.
Since our aim is to acquire concept-related text sources, these methodsare not applicable.A general corpus-based approach has been proposed by Reiter and Buitelaar (2008):Using a domain corpus and a domain-independent reference corpus, they select thearticle with the highest domain relevance score among multiple candidates.
This ap-proach works reasonably well but relies on the availability of domain-specific corporaand fails at selecting the appropriate among multiple in-domain senses.
In contrast,our resource-poor approach does not rely on additional textual resources, as ontologiesusually do not contain contexts for classes.1Mihalcea (2007) shows that Wikipedia can indeed be used as a sense inventory for sensedisambiguation.A Resource-Poor Approach for Linking Ontology Classes to Wikipedia 3833 Linking Ontology classes to Wikipedia articlesThis section briefly reviews relevant information about Wikipedia and describes ourmethod for linking ontology classes to Wikipedia articles.
Our algorithm consists oftwo steps: (i) extracting candidate articles from Wikipedia and (ii) selecting the mostappropriate one.
The algorithm is independent of the choice of a specific ontology.23.1 WikipediaThe online encyclopedia Wikipedia currently comprises more than 2,382,000 articlesin about 250 languages.
Wikipedia is interesting for our approach because it is semi-structured and articles usually talk about one specific topic.The structural elements in Wikipedia that we rely on are links between articles,inter-language links, disambiguation and redirect pages.
Inter-language links refer toan article about the same topic in a different language.
Disambiguation pages collectthe different senses of a given term.
Redirect pages point to other pages, allowing forspelling variations, abbreviations and synonyms.3.2 Extracting the candidate articlesThe first step of our algorithm is to extract candidate articles for ontology classes.
Themethod we employ is based on Reiter and Buitelaar (2008).
The algorithm starts withthe English label LC of an ontology class C, and tries to retrieve the article that bearsthe same title.3 Any Wikipedia page P retrieved by this approach falls into one ofthree categories:1.
P is an article: The template {{otheruses}} in the article indicates that adisambiguation page exists which lists further candidate articles for C. Thedisambiguation page is then retrieved and we proceed with step 2.
Otherwise,P is considered to be the only article for C.2.
P is a disambiguation page: The algorithm extracts all links on P and considersevery linked page as a candidate article.43.
P is a redirect page: The redirect is being followed and the algorithm checksthe different cases once again.3.3 Features for the classifierWe now discuss the features we apply to disambiguate candidate articles retrieved byour candidate extraction method with regard to the respective ontology class.
Somefeatures use structural properties of both Wikipedia and the ontology, others are basedon shallow linguistic processing.2It is still dependent on the language used for coding ontological concepts (here English).
In futurework we aim at bridging between languages using Wikipedia?s inter-language links or other multi-lingualresources.3We use common heuristics to cope with CamelCase, underscore whitespace alternation etc.4Note that, apart from pointing to different readings of a term, disambiguation pages sometimes includepages that are clearly not a sense of the given term.
Distinguishing these from true/appropriate readings ofthe term is not trivial.384 Reiter, Hartung, and FrankDomain relevanceWikipedia articles can be classified according to their domain relevance by computingthe proportion of domain terms they contain.
In this paper, we explore several variantsof matching a set of domain terms against the article in question:Class labels.
The labels of all concepts in the ontology are used as a set of domainterms.?
We extract the nouns from the POS-tagged candidate article.
The relative fre-quency of domain terms is then computed for the complete article and for nounsonly, both for types and for tokens.?
We compute the frequency of domain terms in the first paragraph only, assumingit contains domain relevant key terms.?
The redirects pointing to the article in question, i.e., spelling variations andsynonyms, are extracted.
We then compute their relative frequency in the set ofclass labels.Comments.
As most ontologies contain natural language comments for classes, weuse them to retrieve domain terms.
All class comments extracted from the ontology arePOS-tagged.
We use all nouns as domain terms and compute their relative frequenciesin the article.Class vs. InstanceIt is intuitively clear that a class in the ontology needs to be linked to a Wikipediaarticle representing a class rather than an instance.5 We extract the following featuresin order to detect whether an article represents a class or an instance, thus being ableto reject certain articles as inappropriate link targets for a particular class.Translation distance.
Instances inWikipedia are usually named entities (NEs).
Thus,the distinction between concepts and instances can, to a great extent, be rephrased asthe problem of NE detection.
As our intention is to develop a linking algorithm whichis, in principle, language-independent, we decided to rely on the inter-language linksprovided by Wikipedia.
The basic idea is that NEs are very similar across differentlanguages (at least in languages using the same script), while concepts show a greatervariation in their surface forms across different languages.
Thus, for the inter-languagelinks on the article in question that use latin script, we compute the average string sim-ilarity in terms of Levenshtein Distance (Levenshtein, 1966) between the title of thepage and its translations.Templates.
Wikipedia offers a number of structural elements that might be usefulin order to distinguish instances from concepts.
In particular, the infobox templateis used to express structured information about instances of a certain type and someof their properties.
Thus, we consider articles containing an infobox template tocorrespond to an instance.5We are aware of the fact that the distinction between classes and instances is problematic on both sides:Ontologies described in OWL Full or RDF do not distinguish clearly between classes and instances andWikipedia does not provide an explicit distinction either.A Resource-Poor Approach for Linking Ontology Classes to Wikipedia 3854 Experiment4.1 The Music OntologyWe test our approach on the Music Ontology (MO) (Raimond et al, 2007).
The MOhas been developed for the annotation of musical entities on the web and providescapabilities to encode data about artists, their albums, tracks on albums and the processof creating musical items.The ontology defines 53 classes and 129 musical properties (e.g.
melody) in itsnamespace, 78 external classes are referenced.
Most of the classes are annotated withcomments in natural language.
TheMO is connected to several other ontologies (W3Ctime6, timeline7, event8, FOAF9), making it an interesting resource for domain rele-vant IE tasks and generalisation of the presented techniques to further domains.
TheMO is defined in RDF and freely available10.4.2 Experimental SetupThe experiment is divided into two steps: candidate page selection and classification(see Section 3).
For candidate selection we extract Wikipedia pages with titles thatare near-string identical to the 53 class labels.
28 of them are disambiguation pages.From these pages, we extract the links and use them as candidates.
The remaining 25are directly linked to a single Wikipedia article.To test our classification features, we divide the overall set of ontology classes intraining and test sets of 43 and 10 classes, respectively, that need to be associatedwith their most appropriate candidate article.
We restrict the linking to one mostappropriate article.
For the classification step, we extract the features discussed inSection 3.Since the candidate set of pages shows a heavily skewed distribution in favour ofnegative instances, we generate an additional training set by random oversampling(Batista et al, 2004) in order to yield training data with a more uniform distributionof positive and negative instances.5 EvaluationFor evaluation, the ambiguous concepts in the ontology have been manually linkedto Wikipedia articles.
The linking was carried out independently by three annotators,all of them computational linguists.
Each annotator was presented the class label,its comment as provided by the ontology and the super class from which the classinherits.
On the Wikipedia side, all pages found by our candidate extraction methodwere presented to the annotators.The inter-annotator agreement is ?
= 0.68 (Fleiss?
Kappa).
For eight concepts, allthree annotators agreed that none of the candidate articles is appropriate and for ten allthree agreed on the same article.
These figures underline the difficulty of the problem,as the information contained in domain ontologies and Wikipedia varies substantiallywith respect to granularity and structure.6www.w3.org/TR/owl-time/7motools.sourceforge.net/timeline/timeline.html8motools.sourceforge.net/event/event.html9xmlns.com/foaf/spec/10musicontology.com386 Reiter, Hartung, and FrankCandidate article selection.
Candidate selection yields 16 candidate articles perconcept on average.
These articles contain 1567 tokens on average.
The minimaland maximal number of articles per concepts are 3 and 38, respectively.Candidate article classification.
We train a decision tree11 using both the originaland the oversampled training sets as explained above.Table 1: Results after training on original and over-sampled dataPositives Negatives Averageorig.
samp.
orig.
samp.
orig.
samp.P 1 0.63 0.87 0.97 0.94 0.80R 0.17 0.83 1 0.91 0.58 0.87F 0.27 0.71 0.93 0.94 0.75 0.83Table 1 displays precision, recall and f-score results for positive and negative in-stances as well as their average.
As the data shows, oversampling can increase theperformance considerably.
We suspect this to be caused not only by the larger trainingset, but primarily by the more uniform distribution.The table shows further that the negative instances can be classified reliably us-ing the original or oversampled data set.
However, as we intend to select positiveappropriate Wikipedia articles rather than to deselect inappropriate ones, we are par-ticularly interested in good performance for the positive instances.
We observe thatthis approach identifies positive instances (i.e., appropriate Wikipedia articles) with areasonable performancewhen using the oversampled training set.
It is noteworthy thatnot a single feature performs better than with an f-measure of 0.6 when used alone.The figures shown in Table 1 are obtained using the combination of all features.Table 2: Results for combination of best features onlyPositives NegativesP 0.60 1.00R 1.00 0.88F 0.75 0.94In Table 2, we present the results for the best performing features taken together(using oversampling on the training set): nountypes-classlabels (F-measure: 0.6),langlinks (0.5), redirects-classlabels (0.5), nountokens-classlabels (0.44),fulltextclasslabels (0.44).
Recall improves considerably, while there is a smalldecrease in precision.6 ConclusionsWe have presented ongoing research on linking ontology classes to appropriate Wiki-pedia articles.
We consider this task a necessary step towards automatic ontologylexicalization and population from texts.11We used the ADTree implementation in the Weka toolkit www.cs.waikato.ac.nz/ml/weka/.A Resource-Poor Approach for Linking Ontology Classes to Wikipedia 387The crucial challenge in this task is to deal with the high degree of ambiguity thatis introduced by the fact that Wikipedia covers a large amount of fine-grained infor-mation for numerous domains.
This leads to a great number of potential candidatearticles for a given ontology class.Our approach to this problem is independent of the particular ontology that is usedas a starting point.
Moreover, it merely depends on a set of rather shallow but effec-tive features which can be easily extracted from the domain ontology and Wikipedia,respectively.
From the results we derived in our experiments with the Music Ontol-ogy, we conclude that our approach is feasible and yields reasonable results even forsmall domain ontologies, provided we can overcome highly skewed distributions ofthe training examples due to an overwhelming majority of negative instances.
In fu-ture work we will apply the methods described here to different domain ontologies anduse the selectedWikipedia articles as a starting point for extracting instances, relationsand attributes.Acknowledgements.
We kindly thank our annotators for their effort and R?digerWolf for technical support.ReferencesBatista, G., R. Prati, and M. C. Monard (2004).
A Study of the Behavior of SeveralMethods for BalancingMachine Learning Training Data.
SIGKDD Explorations 6,20?29.Cucerzan, S. (2007).
Large-Scale Named Entity Disambiguation Based on WikipediaData.
In Proc.
of EMNLP, Prague.Levenshtein, V. I.
(1966).
Binary codes capable of correcting deletions, insertions,and reversals.
Soviet Physics Doklady 10, 707?710.Mihalcea, R. (2007).
Using Wikipedia for Automatic Word Sense Disambiguation.In Proc.
of NAACL-07, Rochester, New York, pp.
196?203.Raimond, Y., S. Abdallah, M. Sandler, and F. Giasson (2007).
The Music Ontol-ogy.
In Proc.
of the 8th International Conference on Music Information Retrieval,Vienna, Austria.Reiter, N. and P. Buitelaar (2008).
Lexical Enrichment of Biomedical Ontologies.
InInformation Retrieval in Biomedicine: Natural Language Processing for Knowl-edge Integration.
IGI Global, to appear.Ruiz-Casado, M., E. Alfonseca, and P. Castells (2005).
Automatic Assignment ofWikipedia Encyclopedic Entries to WordNet Synsets.
In Proc.
of the 3rd AtlanticWeb Intelligence Conference, Volume 3528, Lodz, Poland, pp.
380?385.Wu, F. and D. S. Weld (2007).
Autonomously Semantifying Wikipedia.
In Proc.
ofthe Conference on Information and Knowledge Management, Lisboa, Portugal.
