Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 10?15,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsDistributed Structures and Distributional MeaningFabio Massimo ZanzottoDISP University of Rome ?Tor Vergata?Via del Politecnico 100133 Roma, Italyzanzotto@info.uniroma2.itLorenzo Dell?ArcipreteUniversity of Rome ?Tor Vergata?Via del Politecnico 100133 Roma, Italylorenzo.dellarciprete@gmail.comAbstractStemming from distributed representation the-ories, we investigate the interaction betweendistributed structure and distributional mean-ing.
We propose a pure distributed tree (DT)and distributional distributed tree (DDT).
DTsand DDTs are exploited for defining dis-tributed tree kernels (DTKs) and distributionaldistributed tree kernels (DDTKs).
We com-pare DTKs and DDTKs in two tasks: approx-imating tree kernels TK (Collins and Duffy,2002); performing textual entailment recog-nition (RTE).
Results show that DTKs corre-late with TKs and perform in RTE better thanDDTKs.
Then, including distributional vec-tors in distributed structures is a very difficulttask.1 IntroductionDemonstrating that distributional semantics is a se-mantic model of natural language is a real researchchallenge in natural language processing.
Frege?sprinciple of compositionality (Frege, 1884), nat-urally taken into account in logic-based semanticmodels of natural language (Montague, 1974), ishardly effectively included in distributional seman-tics models.
These models should composition-ally derive distributional vectors for sentences andphrases from the distributional vectors of the com-posing words.Besides vector averaging (Landauer and Dumais,1997; Foltz et al, 1998), that can model distribu-tional meaning of sentences, recent distributionalcompositional models focus on finding distribu-tional vectors of word pairs (Mitchell and Lapata,2010; Guevara, 2010; Baroni and Zamparelli, 2010;Zanzotto et al, 2010).
Scaling up these 2-word se-quence models to the sentence level is not trivial assyntactic structure of sentences plays a very impor-tant role.
Understanding the relation between thestructure and the meaning is needed for building dis-tributional compositional models for sentences.Research in Distributed Representations (DR)(Hinton et al, 1986) proposed models and methodsfor encoding data structures in vectors, matrices, orhigh-order tensors.
Distributed Representations areoriented to preserve the structural information in thefinal representation.
For this purpose, DR modelsgenerally use random and possibly orthogonal vec-tors for words and structural elements (Plate, 1994).As distributional semantics vectors are unlikely tobe orthogonal, syntactic structure of sentences maybe easily lost in the final vector combination.In this paper, we investigate the interaction be-tween distributed structure and distributional mean-ing by proposing a model to encode syntactic treesin distributed structures and by exploiting this modelin kernel machines (Vapnik, 1995) to determine thesimilarity between syntactic trees.
We propose apure distributed tree (DT) and a distributional dis-tributed tree (DDT).
In line with the distributed rep-resentation theory, DTs use random vectors for rep-resenting words whereas DDTs use distributionalvectors for words.
Our interest is in understand-ing if the introduction of distributional semantic in-formation in an inherently syntactic based model,such as distributed representations, leads to betterperformances in semantic aware tasks.
DTs andDDTs are exploited for defining distributed tree ker-10nels (DTKs) and distributional distributed tree ker-nels (DDTKs).
We study the interaction betweenstructure and meaning in two ways: 1) by compar-ing DTKs and DDTKs with the classical tree sim-ilarity functions, i.e., the tree kernels TK (Collinsand Duffy, 2002); 2) by comparing the accuracy ofDTKs and DDTKs in a semantic task such as recog-nizing textual entailment (RTE).
Results show thatDTKs correlate with TKs and perform in RTE betterthan DDTKs.
This indicates that including distri-butional vectors in distributed structures should beperformed in a more complex fashion.2 Related WorkDistributed Representations (DR) (Hinton et al,1986) are models and methods for encoding datastructures as trees in vectors, matrices, or high-order tensors.
DR are studied in opposition to sym-bolic representations to describe how knowledge istreated in connectionist models (Rumelhart and Mc-clelland, 1986).
Basic symbolic elements, e.g., Johnor car, as well as eventually nested structures, e.g.,buy(John,car,in(1978)), are represented as vectors,matrices, or higher order tensors.
Vectors of ba-sic elements (words, or concepts) can be randomlygenerated (e.g.
(Anderson, 1973; Murdock, 1983))or, instead, they may represent their attributes andcan be manually built (e.g.
(McRae et al, 1997; An-drews et al, 2009)).
Vectors, matrices, or tensors forstructures are compositionally derived using vectorsfor basic elements.Good compositionally obtained vectors for struc-tures are explicit and immediately accessible: infor-mation stored in a distributed representation shouldbe easily accessible with simple operations (Plate,1994).
Circular convolution in Holographic Re-duced Representations (HRRs) (Plate, 1994) is de-signed to satisfy the immediate accessibility prop-erty.
It supports two operations for producing andaccessing the compact representations: the circularconvolution and the correlation.
Given that com-ponent vectors are obtained randomly (as in (An-derson, 1973; Murdock, 1983)), correlation is theinverse of composition.
Yet, distributed represen-tations offer an informative way of encoding struc-tures if basic vectors are nearly orthogonal.3 Distributed Trees and DistributionalDistributed TreesStemming from distributed representations, we pro-pose a way to encode syntactic trees in distributedvectors.
These vectors can be pure distributed treevectors (DT) or distributional distributed tree vectors(DDT).
Once defined, these vectors can be used asa tree similarity function in kernel machines (Vap-nik, 1995).
We can build pure distributed tree ker-nels (DTK) or distributional distributed tree kernels(DDTK) to be used in recognizing textual entailment(RTE).The rest of the section is organized as follows.We firstly present the distributed trees and the dis-tributed tree kernels (Sec.
3.1).
We then describehow to obtain DTs and DDTs (Sec.
3.2).
Finally, wedescribe how the related kernels can be used for therecognizing textual entailment task (Sec.
3.2.1).3.1 Distributed Trees and Distributed TreeKernelsWe define a distributed vector in order to finally pro-duce a similarity function between trees (i.e., a ker-nel function) as the classical tree kernel (Collins andDuffy, 2002).
A distributed vector ~?T is a vector rep-resenting the subtrees of a tree T .
The final functionis:~?T =?n?N(T )s(n) (1)where N(T ) is the set of nodes of the tree T , n isa node, and s(n) is the sum of the distributed vec-tors of the subtrees of T rooted in the node n. Thefunction s(n) is recursively defined as follows:?
s(n) = ~n?~w if n is a pre-terminal node n?
wwhere ~n is the vector representing n and ~w isthe one representing the word w.?
s(n) = ~n?
(~c1 + s(c1))?
.
.
.?
(~cn + s(cn))where n is not a pre-terminal node, n ?c1 .
.
.
cn is the first production of the tree rootedin n, ~n is the vector of the node n, and ~ci arethe vectors of the nodes ci.The distributed vectors of the nodes only depend ontags of the nodes.11The function ?
is defined as the reverse element-wise product ~v = ~a?~b as:vi = ?aibn?i+1 (2)where vi, ai, and bi are the elements of, respectively,the vectors ~v, ~a, and ~b; n is the dimension of thespace; and ?
is a value to ensure that the operation?approximate the property of vector module preserva-tion.
This function is not commutative and this guar-antees that different trees t have different vectors ~t.It is possible to demonstrate that:~?T =?t?S(T )~t (3)where S(T ) is the set of the subtrees of T , t is oneof its subtrees, and ~t is its distributed representation.The distributed kernel T?K function over treesthen easily follows as:T?K(T1, T2) =~?T1 ?~?T2 =?t1?S(T1)?t2?S(T2)~t1 ?~t2 (4)If the different trees are orthogonal, T?K(T1, T2)counts approximately the number of subtrees incommon between the two trees T1 and T2.3.2 Pure Distributed vs. DistributionalDistributed TreesFor producing the distributed trees, we use basic ran-dom vectors representing tree nodes ~n.
These aregenerated by independently drawing their elementsfrom a normal distribution N(0,1) with mean 0 andvariance 1.
The vectors are then normalized so thatthey have unitary Euclidean length.
This generationprocess guarantees that, for a high enough numberof dimensions, the vectors are statistically expectedto be nearly orthogonal, i.e.
the dot product amongpairs of different vectors is expected to be 0.We can obtain the pure distributed trees (DT) andthe distributional distributed trees (DDT) along withtheir kernel functions, DTK and DDTK, by usingdifferent word vectors ~w.
In the DTs, these vectorsare random vectors as the other nodes.
In DDTs,these vectors are distributional vectors obtained ona corpus with an LSA reduction (Deerwester et al,1990).3.2.1 Entailment-specific KernelsRecognizing textual entailment (RTE) is acomplex semantic task often interpreted as a classi-fication task.
Given the text T and the hypothesisH determine whether or not T entails H .
Forapplying the previous kernels to this classificationtask, we need to define a specific class of kernels.As in (Zanzotto and Moschitti, 2006; Wang andNeumann, 2007; Zanzotto et al, 2009), we encodethe text T and the hypothesis H in two separatesyntactic feature spaces.
Then, given two pairs oftext-hypothesis P1 = (T1, H1) and P2 = (T2, H2),the prototypical kernel PK is written as follows:PK(P1, P2) = K(T1, T2) +K(H1, H2) (5)where K(?, ?)
is a generic kernel.
We will then ex-periment with different PK kernels obtained using:the original tree kernel function (TK) (Collins andDuffy, 2002), DTK, and DDTK.Along with the previous task specific kernels, weuse a simpler feature (Lex) that is extremely effec-tive in determining the entailment between T andH .This simple feature is the lexical similarity betweenT andH computed using WordNet-based metrics asin (Corley and Mihalcea, 2005).
This feature, here-after called Lex, encodes the similarity between TandH , i.e., sim(T,H).
This feature is used alone orin combination with the previous kernels and it givesan important boost to their performances.
In thetask experiment, we will then also have: Lex+TK,Lex+DTK, and Lex+DDTK.4 Experimental EvaluationIn this section, we experiment with the distributedtree kernels (DTK) and the distributional distributedtree kernels (DDTK) in order to understand whetheror not the syntactic structure and the distributionalmeaning can be easily encoded in the distributedtrees.
We will experiment in two ways: (1) directcomparison of the distances produced by the origi-nal tree kernel (TK) (Collins and Duffy, 2002) andthe novel kernels DTK and DDTK; (2) task drivenevaluation of DTK and DDTK using the RTE task.The rest of the section is organized as follows.
Wefirstly introduce the experiment set up that is usedfor the two settings (Sec.
4.1).
Secondly, we reporton the experimental results (Sec.
4.2).124.1 Experimental Set-upWe have the double aim of producing a direct com-parison of how the distributed tree kernel (DTK) isapproximating the original tree kernel (TK) and atask based comparison for assessing if the approx-imation is enough effective to similarly solve thetask that is textual entailment recognition.
For bothexperimental settings, we take the recognizing tex-tual entailment sets ranging from the first challenge(RTE-1) to the fifth (RTE-5) (Dagan et al, 2006;Bar-Haim et al, 2006; Giampiccolo et al, 2007;Bentivogli et al, 2009).The distributional vectors used for DDTK havebeen obtained by an LSA reduction of the word-by-word cooccurrence matrix generated on the UKWaCcorpus (Ferraresi et al, 2008), using a context win-dow of size 3.
An appropriate size for the LSA re-duction was deemed to be 250.
Thus, in the exper-iments we used 250 dimensions both for distribu-tional and random vectors, to allow a correct com-parison between DTK and DDTK models.For the direct comparison, we used tree pairs de-rived from the RTE sets.
Each pair is derived from aT-H pair where T and H are syntactically analyzedand each RTE set produces the corresponding set oftree pairs, e.g., the development set of RTE1 pro-duces a set of 567 tree pairs.
To determine whetheror not a distributed kernel, DTK or DDTK, is be-having similarly to the original TK kernel, given aset of tree pairs, we produce two ranked lists of treepairs: the first is ranked according to the original TKapplied to the tree pairs and the second according tothe target distributed kernel.
We evaluate the corre-lation of the two ranked lists according to the spear-man?s correlation.
Higher correlation corresponds toa better approximation of TK.For the task driven comparison, we experimentedwith the datasets in the classical learning setting: thedevelopment set is used as training set and the finalclassifier is tested on the testing set.
We used a sup-port vector machine (Joachims, 1999) with an im-plementation of the original tree kernel (Moschitti,2006).
The classifiers are evaluated according to theaccuracy of the classification decision on the testingset, i.e., the ratio of the correct decisions over all thedecisions to take.Average Spearman?s CorrelationDTK 0.8335DDTK 0.7641Table 1: Average Spearman?s correlations of the tree ker-nel (TK) with the distributed tree kernel (DTK) and thedistributed distributional tree kernel (DDTK) in a vectorspace with 250 dimensionsavg RTE1 RTE2 RTE3 RTE5TK 55.02% 55.50% 53.38% 55.88% 55.33%DTK 55.63% 57.25% 54.88% 54.38% 56.00%DDTK 55.11% 54.00% 53.88% 55.38% 57.17%Lex+TK 62.11% 59.75% 61.25% 66.62% 60.83%Lex+DTK 63.25% 61.12% 62.12% 66.25% 63.50%Lex+DDTK 62.90% 60.62% 61.25% 66.38% 63.33%Table 2: Accuracies of the different methods on the tex-tual entailment recognition task4.2 Experimental resultsIn the first experiment of this set, we want to in-vestigate which one between DTK and DDTK cor-relates better with original TK.
Table 1 reports thespearman?s correlations of tree kernels with DTKand DDTK in a vector space with 250 dimensions.These correlations are obtained averaging the corre-lations over the 9 RTE sets.
According to these re-sults, DTK better correlates with TK with respect toDDTK.
Distributional vectors used for words are notorthogonal as these are used to induce the similaritybetween words.
Yet, this important feature of thesevectors determines a worse encoding of the syntacticstructure.In the task driven experiment, we wanted to in-vestigate whether the difference in correlation hassome effect on the performance of the different sys-tems.
Accuracy results on the RTE task are reportedin Table 2.
The columns RTE1, RTE2, RTE3, andRTE5 represent the accuracies of the different ker-nels using the traditional split of training and test-ing.
The column avg reports the average accuracyof the different methods in the 4 sets.
Rows rep-resent the different kernels used in this comparativeexperiment.
These kernels are used with the taskspecific kernel PK by changing the generic kernelK.
The first 3 rows represent the pure kernels whilethe last 3 rows represent the kernels boosted withthe lexical similarity (Lex), a simple feature com-puted using WordNet-based metrics, as in (Corley13and Mihalcea, 2005).
Looking at the first 3 rows,we derive that there is not a significant difference be-tween TK, DTK, and DDTK.
DTK and DDTK canthen be used instead of the TK.
This is an importantresult, since the computation of DTK (or DDTK) ismuch faster than that of TK, due to TK?s complex-ity being quadratic with respect to the size of thetrees, and DTK requiring a simple dot product overvectors that can be obtained with linear complex-ity with respect to the tree size.
The second fact isthat there is no difference between DTK and DDTK:more semantically informed word vectors have thesame performance of random vectors.5 ConclusionsDistributed structures and distributional meaning arelargely correlated.
In this paper, we analyzed thiscorrelation with respect to the research challenge ofproducing compositional models for distributionalsemantics.
In the studies of distributed represen-tation, compositionality is a big issue that has pro-duced many models and approaches.
Compositionaldistributional semantics poses the same issue.
Weempirically showed that a methodology for includ-ing distributional meaning in distributed represen-tation is possible, but it must be furtherly devel-oped to be an added value.
Distributional semanticshas been positively added in traditional tree kernels(Mehdad et al, 2010).
Yet, the specific requirementof distributed tree kernels (i.e., the orthogonality ofthe vectors) reduces this positive effect.ReferencesJames A. Anderson.
1973.
A theory for the recognitionof items from short memorized lists.
PsychologicalReview, 80(6):417 ?
438.Mark Andrews, Gabriella Vigliocco, and David Vinson.2009.
Integrating experiential and distributional datato learn semantic representations.
Psychological Re-view, 116(3):463 ?
498.Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, DaniloGiampiccolo, Bernardo Magnini, and Idan Szpektor.2006.
The second pascal recognising textual entail-ment challenge.
In Proceedings of the Second PAS-CAL Challenges Workshop on Recognising TextualEntailment.
Venice, Italy.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages1183?1193, Cambridge, MA, October.
Association forComputational Linguistics.Luisa Bentivogli, Ido Dagan, Hoa T. Dang, Danilo Gi-ampiccolo, and Bernardo Magnini.
2009.
The FifthPASCAL Recognizing Textual Entailment Challenge.In Proceedings of TAC?2009.Michael Collins and Nigel Duffy.
2002.
New rankingalgorithms for parsing and tagging: Kernels over dis-crete structures, and the voted perceptron.
In Proceed-ings of ACL02.Courtney Corley and Rada Mihalcea.
2005.
Measur-ing the semantic similarity of texts.
In Proc.
of theACL Workshop on Empirical Modeling of SemanticEquivalence and Entailment, pages 13?18.
Associa-tion for Computational Linguistics, Ann Arbor, Michi-gan, June.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailment chal-lenge.
In Quionero-Candela et al, editor, LNAI 3944:MLCW 2005, pages 177?190.
Springer-Verlag, Milan,Italy.Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society of Information Science,41(6):391?407.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukwac, a very large web-derived corpus of english.
InIn Proceed-ings of the WAC4Workshop at LREC 2008,Marrakesh, Morocco.P.
Foltz, W. Kintsch, and T. Landauer.
1998.
The mea-surement of textual coherence with latent semanticanalysis.
Discourse Processes, 25(2&3):285?307.Gottlob Frege.
1884.
Die Grundlagen der Arith-metik (The Foundations of Arithmetic): eine logisch-mathematische Untersuchung ber den Begriff derZahl.
Breslau.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, andBill Dolan.
2007.
The third pascal recognizing tex-tual entailment challenge.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Para-phrasing, pages 1?9.
Association for ComputationalLinguistics, Prague, June.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of the 2010 Workshop onGEometrical Models of Natural Language Semantics,pages 33?37, Uppsala, Sweden, July.
Association forComputational Linguistics.14G.
E. Hinton, J. L. McClelland, and D. E. Rumelhart.1986.
Distributed representations.
In D. E. Rumel-hart and J. L. McClelland, editors, Parallel DistributedProcessing: Explorations in the Microstructure ofCognition.
Volume 1: Foundations.
MIT Press, Cam-bridge, MA.Thorsten Joachims.
1999.
Making large-scale svmlearning practical.
In B. Schlkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods-Support Vector Learning.
MIT Press.Thomas K. Landauer and Susan T. Dumais.
1997.
A so-lution to plato?s problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge.
Psychological Review, 104(2):211?240,April.K.
McRae, V. R. de Sa, and M. S. Seidenberg.
1997.On the nature and scope of featural representations ofword meaning.
J Exp Psychol Gen, 126(2):99?130,June.Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-simo Zanzotto.
2010.
Syntactic/semantic structuresfor textual entailment recognition.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, HLT ?10, pages 1020?1028, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science.Richard Montague.
1974.
English as a formal language.In Richmond Thomason, editor, Formal Philosophy:Selected Papers of Richard Montague, pages 188?221.Yale University Press, New Haven.Alessandro Moschitti.
2006.
Making tree kernels prac-tical for natural language learning.
In Proceedings ofEACL?06, Trento, Italy.Bennet B. Murdock.
1983.
A distributed memory modelfor serial-order information.
Psychological Review,90(4):316 ?
338.T.
A.
Plate.
1994.
Distributed Representations andNested Compositional Structure.
Ph.D. thesis.David E. Rumelhart and James L. Mcclelland.
1986.Parallel Distributed Processing: Explorations in theMicrostructure of Cognition : Foundations (ParallelDistributed Processing).
MIT Press, August.Vladimir Vapnik.
1995.
The Nature of Statistical Learn-ing Theory.
Springer.Rui Wang and Gu?nter Neumann.
2007.
Recognizing tex-tual entailment using sentence similarity based on de-pendency tree skeletons.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Para-phrasing, pages 36?41, Prague, June.
Association forComputational Linguistics.Fabio Massimo Zanzotto and Alessandro Moschitti.2006.
Automatic learning of textual entailments withcross-pair similarities.
In Proceedings of the 21st Col-ing and 44th ACL, pages 401?408, Sydney, Australia,July.Fabio Massimo Zanzotto, Marco Pennacchiotti, andAlessandro Moschitti.
2009.
A machine learning ap-proach to textual entailment recognition.
NATURALLANGUAGE ENGINEERING, 15-04:551?582.Fabio Massimo Zanzotto, Ioannis Korkontzelos,Francesca Fallucchi, and Suresh Manandhar.
2010.Estimating linear models for compositional dis-tributional semantics.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics (COLING), August,.15
