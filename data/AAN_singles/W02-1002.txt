Conditional Structure versus Conditional Estimation in NLP ModelsDan Klein and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305-9040{klein, manning}@cs.stanford.eduAbstractThis paper separates conditional parameter estima-tion, which consistently raises test set accuracy onstatistical NLP tasks, from conditional model struc-tures, such as the conditional Markov model usedfor maximum-entropy tagging, which tend to loweraccuracy.
Error analysis on part-of-speech taggingshows that the actual tagging errors made by theconditionally structured model derive not only fromlabel bias, but also from other ways in which the in-dependence assumptions of the conditional modelstructure are unsuited to linguistic sequences.
Thepaper presents new word-sense disambiguation andPOS tagging experiments, and integrates apparentlyconflicting reports from other recent work.1 IntroductionThe success and widespread adoption of probabilis-tic models in NLP has led to numerous variant meth-ods for any given task, and it can be difficult to tellwhat aspects of a system have led to its relative suc-cesses or failures.
As an example, maximum en-tropy taggers have achieved very good performance(Ratnaparkhi, 1998; Toutanova and Manning, 2000;Lafferty et al, 2001), but almost identical perfor-mance has also come from finely tuned HMM mod-els (Brants, 2000; Thede and Harper, 1999).
Are anyperformance gains due to the sequence model used,the maximum entropy approach to parameter estima-tion, or the features employed by the system?Recent experiments have given conflicting recom-mendations.
Johnson (2001) finds that a condition-ally trained PCFG marginally outperforms a standardjointly trained PCFG, but that a conditional shift-reduce model performs worse than a joint formu-lation.
Lafferty et al (2001) suggest on abstractgrounds that conditional models will suffer from aphenomenon called label bias (Bottou, 1991) ?
seesection 3 ?
but is this a significant effect for real NLPproblems?We suggest that the results in the literature, alongwith the new results we present in this work, can beexplained by the following generalizations:?
The ability to include better features in a well-founded fashion leads to better performance.?
For fixed features, assumptions implicit in themodel structure have a large impact on errors.?
Maximizing the objective being evaluated has a re-liably positive, but often small, effect.It is especially important to study these issues us-ing NLP data sets: NLP tasks are marked by theircomplexity and sparsity, and, as we show, conclu-sions imported from the machine-learning literaturedo not always hold in these characteristic contexts.In previous work, the structure of a model andthe method of parameter estimation were often bothchanged simultaneously (for reasons of naturalnessor computational ease), but in this paper we seek totease apart the separate effects of these two factors.In section 2, we take the Naive-Bayes model, ap-plied to word-sense disambiguation (WSD), and trainit to maximize various objective functions.
Our ex-periments reaffirm that discriminative objectives likeconditional likelihood improve test-set accuracy.
Insection 3, we examine two different model structuresfor part-of-speech (POS) tagging.
There, we ana-lyze how assumptions latent in conditional structureslower tagging accuracy and produce strange quali-tative behaviors.
Finally, we discuss related recentfindings by other researchers.2 Objective Functions: Naive-BayesFor bag-of-words WSD, we have a corpus D of la-beled examples (s, o).
Each o = ?oi ?
is a list of con-text words, and the corresponding s is the correctsense of a fixed target word occuring in that context.A particular model for this task is the familiar multi-Association for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
9-16.Proceedings of the Conference on Empirical Methods in Naturalnomial Naive-Bayes (NB) model (Gale et al, 1992;McCallum and Nigam, 1998), where we assume con-ditional independence between each of the oi .
ThisNB model gives a joint distribution over the s and ?oi ?variables:P(s, o) = P(s)?iP(oi |s)It also implicitly makes conditional predictions:P(s|o) = P(s, o)/?s?P(s?, o)In NLP, NB models are typically used in this latterway to make conditional decisions, such as chosingthe most likely word sense.1The parameters 2 = ?
?s; ?o|s?
for this model arethe sense priors P(s) and the sense-conditional worddistributions P(o|s).
These are typically set using(smoothed) relative frequency estimators (RFEs):?s = P(s) = count(s)/|D|?o|s = P(o|s) = count(s, o)/?o?count(s, o?
)These intuitive relative frequency estimators are theestimates for 2 which maximize the joint likelihood(JL) of D according to the NB model:J L(2, D) =?
(s,o)?DP(s, o)A NB model which has been trained to maximize JLwill be referred to as NB-JL.
It is worth emphasiz-ing that, in NLP applications, the model is typicallytrained jointly, then used for its P(s|o) predictions.We can set the parameters in other ways, withoutchanging our model.
If we are doing classification,we may not care about JL.
Rather, we will want tominimize whatever kinds of errors we get chargedfor.
The JL objective is the evaluation criterion forlanguage modeling, but a decision process?
evalua-tion is more naturally phrased in terms of P(s|o).
Ifwe want to maximize the probability assigned to thecorrect labeling of the corpus, the appropriate objec-tive is conditional likelihood (CL):C L(2, D) =?
(s,o)?DP(s|o)This focuses on the sense predictions, not the words,which is what we cared about in the first place.Figure 1 shows an example of the trade-offs be-tween JL and CL.
Assume there are two classes (1and 2), two words (a and b), and only 2-word con-texts.
Assume the actual distribution (training andtest) is 3 each of (1, ab) and (1, ba) and one (2, aa)1A possible use for the joint predictions would be a topic-conditional unigram language model.P(s, o) P(s|o) Correct?s o Counts Actual NB-JL NB-CL Actual NB-JL NB-CL NB-JL NB-CL1 aa 0 0 3/14 /4 0 3/5 /41 ab 3 3/7 3/14 /4 1 1 1 + +1 ba 3 3/7 3/14 /4 1 1 1 + +1 bb 0 0 3/14 /4 0 1 12 aa 1 1/7 1/7 1 ?
 1 2/5 1 ?
/4 - +2 ab 0 0 0 0 0 0 02 ba 0 0 0 0 0 0 02 bb 0 0 0 0 0 0 0Limit log prod.
-0.44 -0.69 -?
0.00 -0.05 0.00Accuracy 6/7 7/7Model P(1) P(2) P(a|1) P(b|1) P(a|2) P(b|2)NB-JL 6/7 1/7 1/2 1/2 1 0NB-CL  1- 1/2 1/2 1 0Figure 1: Example of joint vs. conditional estimation.for 7 samples.
Then, as shown in figure 1, the JL-maximizing NB model has priors of 6/7 and 1/7, likethe data.
The actual (joint) distribution is not in thefamily of NB models, and so it cannot be learned per-fectly.
Still, the NB-JL assigns reasonable probabili-ties to all occurring events.
However, its priors causeit to incorrectly predict that aa belongs to class 1.
Onthe other hand, maximizing CL will push the prior forsense 1 arbitrarily close to zero.
As a result, its con-ditional predictions become more accurate at the costof its joint prediction.
NB-CL joint prediction assignsvanishing mass to events other than (2, aa), and so itsjoint likelihood score gets arbitrarily bad.There are other objectives (or loss functions).
Inthe SENSEVAL competition (Kilgarriff, 1998), weguess sense distributions, and our score is the sumof the masses assigned to the correct senses.
Thisobjective is the sum of conditional likelihoods (SCL):SC L(2, D) =?
(s,o)?DP(s|o)SCL is less appropriate that CL when the model isused as a step in a probabilistic process, rather thanin isolation.
CL is more appropriate for filter pro-cesses, because it highly punishes assigning zero ornear-zero probabilities to observed outcomes.If we choose single senses and receive a score ofeither 1 or 0 on an instance, then we have 0/1-loss(Friedman, 1997).
This gives the ?number correct?and so we refer to the corresponding objective as ac-curacy (Acc):Acc(2, D) =?(s,o)?D?
(s = arg maxs?
P(s?|o))In the following experiments, we illustrate that, fora fixed model structure, it is advantageous to max-imize objective functions which are similar to theevaluation criteria.
Although in principle we can op-timize any of the objectives above, in practice someare harder to optimize than others.
As stated above,JL is trivial to maximize with a NB model.
CL andSCL, since they are continuous in 2, can be opti-mized by gradient methods.
Acc is not continuousin 2 and is unsuited to direct optimization (indeed,finding an optimum is NP-complete).When optimizing an arbitrary function of 2, wehave to make sure that our probabilities remain well-formed.
If we want to have a well-formed joint NB in-terpretation, we must have non-negative parametersand the inequalities ?s?o ?o|s ?
1 and?s ?s ?
1.If we want to be guaranteed a non-deficient joint in-terpretation, we can require equality.
However, if werelax the equality then we have a larger feasible spacewhich may give better values of our objective.We performed the following WSD experimentswith Naive-Bayes models.
We took as data the col-lection of SENSEVAL-2 English lexical sample WSDcorpora.2 We set the NB model parameters in severalways.
We optimized JL (using the RFEs).3 We alsooptimized SCL and (the log of) CL, using a conju-gate gradient (CG) method (Press et al, 1988).4 ForCL and SCL, we optimized each objective both overthe space of all distributions and over the subspaceof non-deficient models (giving CL?
and SCL?).
Accwas not directly optimized.Unconstrained CL corresponds exactly to a condi-tional maximum entropy model (Berger et al, 1996;Lafferty et al, 2001).
This particular case, wherethere are multiple explanatory variables and a sin-gle categorical response variable, is also preciselythe well-studied statistical model of (multinomial)logistic regression (Agresti, 1990).
Its optimizationproblem is concave (over log parameters) and there-fore has a unique global maximum.
For CL?, SCL,and SCL?, we are only guaranteed local optima, butin practice we detected no maxima which were not2http://www.sle.sharp.co.uk/senseval2/3Smoothing is an important factor for this task.
So that thevarious estimates would be smoothed as similarly as possible,we smoothed implicitly, by adding smoothing data.
We addedone instance of each class occurring with the bag containingeach vocabulary word once.
This gave the same result as add-one smoothing on the RFEs for NB-JL, and ensured that NB-CL would not assign zero conditional probability to any unseenevent.
The smoothing data did not, however, result in smoothedestimates for SCL; any conditional probability will sum to oneover the smoothing instances.
For this objective, we added apenalty term proportional to?
?2, which ensured that no con-ditional sense probabilities reached 0 or 1.4All optimization was done using conjugate gradient as-cent over log parameters ?i = log ?i , rather than the givenparameters due to sensitivity near zero and improved qualityof quadratic approximations during optimization.
Linear con-straints over ?
are not linear in log space, and were enforcedusing a quadratic Lagrange penalty method (Bertsekas, 1995).TRAINING SETOptimization Acc MacroAcc log J L log C L SCLNB-JL 86.8 86.2 -22969684.7 -243184.1 4505.9NB-CL* 98.5 96.2 -23366291.2 -973.0 5101.2NB-CL 98.5 96.2 -23431010.0 -854.1 5115.1NB-SCL* 94.2 93.7 -23054768.6 -226187.8 4884.4NB-SCL 97.3 95.5 -23146735.3 -220145.0 5055.8TEST SETOptimization Acc MacroAcc log J L log C L SCLNB-JL 73.6 55.0 -1816757.1 -55251.5 3695.4NB-CL* 72.3 53.4 -1954977.1 -19854.1 3566.3NB-CL 76.2 56.5 -1964498.5 -20498.7 3798.8NB-SCL* 74.8 57.2 -1841305.0 -43027.8 3754.1NB-SCL 77.5 59.7 -1872533.0 -33249.7 3890.8Figure 2: Scores for the NB model trained according to vari-ous objectives.
Scores are usually higher on both training andtest sets for the objective maximized, and discriminative criterialead to better test-set accuracy.
The best scores are in bold.global over the feasible region.Figure 2 shows, for each objective maximized, thevalues of all objectives on both the training and testset.
Optimizing for a given objective generally gavethe best score for that objective for both the trainingset and the test set.
The exception is NB-SCL and NB-SCL* which have lower SCL score than NB-CL andNB-CL*.
This is due to the penalty used for smooth-ing the summed models (see fn.
3).Accuracy is higher when optimizing the discrim-inative objectives, CL and SCL, than when optimiz-ing JL (including for macro-averaging, where eachword?s contribution to average accuracy is madeequal).
That these estimates beat NB-JL on accu-racy is unsurprising, since Acc is a discretization ofconditional predictions, not joint ones.
This sup-ports the claim that maximizing conditional likeli-hood, or other discriminative objectives, improvestest set accuracy for realistic NLP tasks.
NB-SCL,though harder to maximize in general, gives bettertest-set accuracy than NB-CL.5 NB-CL* is some-where between JL and CL for all objectives on thetraining data.
Its behavior shows that the changefrom a standard NB approach (NB-JL) to a maximumentropy classifier (NB-CL) can be broken into two as-pects: a change in objective and an abandonment ofa non-deficiency constraint.6 Note that the JL scorefor NB-CL*, is not very much lower than for NB-JL,despite a large change in CL.It would be too strong to state that maximizing CL5This difference seems to be partially due to the differentsmoothing methods used: Chen and Rosenfeld (1999) showthat quadratic penalties are very effective in practice, while thesmoothing-data method is quite crude.6If one is only interested in the model?s conditional predic-tions, there is no reason to disprefer deficient joint models.-500500 50 100 150 200 250  	 		   ff fififlffifi!
"#$&% ')( * ( $&% +-, .
)$/ $&( % *-0&1214325 +4126Figure 3: Conditional NB has higher accuracy than joint NB forWSD on most SENSEVAL-2 word sets.
The relative improve-ment gained by switching to conditional estimation is positivelycorrelated to training set size.
(in particular) and discriminative objectives (in gen-eral) is always better than maximizing JL for improv-ing test-set accuracy.
Even on the present task, CLstrictly beat JL in accuracy for only 15 of 24 words.Figure 3 shows a plot of the relative accuracy for CL:(AccCL ?
AccJL)/AccJL.
The x-axis is the averagenumber of training instances per sense, weighted bythe frequency of that sense in the test data.
Thereis a clear trend that larger training sets saw a largerbenefit from using NB-CL.
The scatter in this trendis partially due to the wide range in data set condi-tions.
The data sets exhibit an unusual amount ofdrift between training and test distributions.
For ex-ample, the test data for amaze consists entirely of70 instances of the less frequent of its two trainingsenses, and represents the highest point on this graph,with NB-CL having a relative accuracy increase of28%.
This drift between the training and test cor-pora generally favors conditional estimates.
On theother hand, many of these data sets are very small,individually, and 6 of the 7 sets where NB-JL winsare among the 8 smallest, 4 of them in fact being the4 smallest.
Ng and Jordan (2002) show that, betweenNB-JL and NB-CL, the discriminative NB-CL should,in principle, have a lower asymptotic error, but thegenerative NB-JL should perform better in low-datasituations.
They argue that unless one has a relativelylarge data set, one is in fact likely to be better offwith the generative estimate.
Their claim seems toostrong here; even smaller data sets often show benefitto accuracy from CL estimation, although all wouldqualify as small on their scale.Since the number of senses and skew towardscommon senses is so varied between SENSEVAL-2words, we turned to larger data sets to test the ef-fective ?break-even?
size for WSD data, using thehard and line data from Leacock et al (1998).
Fig-ure 4 shows the accuracy of NB-CL and NB-JL as theamount of training data increases.
Conditional beats0.450.500.550.600.650.700.750.800.850.900 1000 2000 3000 4000748 9 : ; : ; <	=2> ?
=2: @ >AccuracyA)B C D E F E B C G H I4JK B E C F I4J0.790.800.810.820.830.840.850.860 1000 2000 3000 4000L M N O P O P Q&R S T R2O U SAccuracyV-W P X O T O W P N Y Z4[\ W O P T Z)[(a) (b)Figure 4: Conditional NB is better than Joint NB for WSD givenall but possibly the smallest training sets, and the advantage in-creases with training set size.
(a) ?line?
(b) ?hard?joint for all but the smallest training sizes, and the im-provement is greater with larger training sets.
Onlyfor the line data does the conditional model ever dropbelow the joint model.For this task, then, NB-CL is performing betterthan expected.
This appears to be due to two ways inwhich CL estimation is suited to linguistic data.
First,the Ng and Jordan results do not involve smootheddata.
Their data sets do not require it like linguisticdata does, and smoothing largely prevents the low-data overfitting that can plague conditional models.There is another, more interesting reason why con-ditional estimation for this model might work betterfor an NLP task like WSD than for a general machinelearning task.
One signature difficulty in NLP is thatthe data contains a great many rare observations.
Inthe case of WSD, the issue is in telling the kinds ofrare events apart.
Consider a word w which occursonly once, with a sense s. In the joint model, smooth-ing ensures that w does not signal s too strongly.However, every w which occurs only once with s willreceive the same P(w|s).
Ideally, we would want tobe able to tell the accidental singletons from true in-dicator words.
The conditional model implicitly doesthis to a certain extent.
If w occurs with s in an ex-ample where other good indicator words are present,then those other words?
large weights will explain theoccurrence of s, and without w having to have a largeweight, its expected count with s in that instance willapproach 1.
On the other hand, if no trigger wordsoccur in that instance, there will be no other expla-nation for s other than the presence of w and theother non-indicative words.
Therefore, w?s weight,and the other words?, will grow until s is predictedsufficiently strongly.As a concrete illustration, we isolated two sensesof ?line?
into a two-sense data set.
Sense 1 was ?aqueue?
and sense 2 was ?a phone line.?
In this cor-pus, the words transatlantic and flowers both occuronly once, and only with the ?phone?
sense (plusonce with each in the smoothing data).
However,transatlantic occurs in the instance thanks, anyway,the transatlantic line 2 died.
, while flowers occursin the longer instance .
.
.
phones with more than oneline 2, plush robes, exotic flowers, and complimen-tary wine.
In the first instance, the only non-singletoncontent word is died which occurs once with sense 1and twice with sense 2.
However, in the other case,phone occurs 191 times with sense 2 and only 4 timeswith sense 1.
Additionally, there are more words inthe second instance with which flowers can share theburden of increasing its expectation.
Experimentally,PJL(flowers|2)PJL(flowers|1) =PJL(transatlantic|2)PJL(transatlantic|1)= 2while with conditional estimation,PCL(flowers|2)PCL(flowers|1) = 2.05PCL(transatlantic|2)PCL(transatlantic|1)= 3.74With joint estimation, both words signal sense 2 withequal strength.
With conditional estimation, the pre-sense of words like phone cause flowers to indicatesense 2 less strongly that transatlantic.
Given thatthe conditional estimation is implicitly differentiallyweighting rare events in a plausibly way, it is perhapsunsurprising that a task like WSD would see the ben-efits on smaller corpus sizes than would be expectedon standard machine-learning data sets.7These trends are reliable, but sometimes small.
Inpractice, one must decide if, for example, a 5% errorreduction is worth the added work: CG optimization,especially with constraints, is considerably harder toimplement than simple RFE estimates for JL.
It is alsoconsiderably slower: the total training time for theentire SENSEVAL-2 corpus was less than 3 secondsfor NB-JL, but two hours for NB-CL.3 Model Structure: HMMs and CMMsWe now consider sequence data, with POS tagging asa concrete NLP example.
In the previous section, wehad a single model, but several ways of estimatingparameters.
In this section, we have two differentmodel structures.First is the classic hidden Markov model (HMM),shown in figure 6a.
For an instance (s, o), where7Interestingly, the common approach of discarding low-count events (for both training speed and overfitting reasons)when estimating the conditional models used in maxent taggersrobs the system of the opportunity to exploit this effect of con-ditional estimation.ModelObjective HMM MEMM MEMM?JL 91.23 89.22 90.44CL?
91.41 89.22 90.44CL 91.44 89.22 90.44Figure 5: Tagging accuracy: For a fixed model, conditionalestimation is slightly advantageous.
For a fixed objective, theMEMM is inferior, though it can be improved by unobservingunambiguous words.o = ?oi ?
is a word sequence and s = ?si ?
is a tagsequence, we write the following (joint) model:P(s, o) = P(s)P(o|s) =?iP(si |si?1)P(oi |si)where we use a start state s0 to simplify notation.The parameters of this model are the transition andemission probabilities.
Again, we can set these pa-rameters to maximize JL, as is typical, or we can setthem to maximize other objectives, without chang-ing the model structure.
If we maximize CL, we get(possibly deficient) HMMs which are instances of theconditional random fields of Lafferty et al (2001).8Figure 5 shows the tagging accuracy of an HMMtrained to maximize each objective.
JL is the standardHMM.
CL duplicates the simple CRFs in (Laffertyet al, 2001).
CL?
is again an intermediate, wherewe optimized conditional likelihood but required theHMM to be non-deficient.
This separates out the ben-efit of the conditional objective from the benefit fromthe possibility of deficiency (which relates to labelbias, see below).
In accordance with our observa-tions in the last section, and consistent with the re-sults of (Lafferty et al, 2001), the CL accuracy isslightly higher than JL for this fixed model.Another model often used for sequence data is theupward Conditional Markov Model (CMM), shownas a graphical model in figure 6b.
This is the modelused in maximum entropy tagging.
The graphicalmodel shown gives a joint distribution over (s, o),just like an HMM.
It is a conditionally structuredmodel, in the sense that that distribution can be writ-ten as P(s, o) = P(s|o)P(o).
Since tagging onlyuses P(s|o), we can discard what the model saysabout P(o).
The model as drawn assumes that eachobservation is independent, but we could add any ar-rows we please among the oi without changing theconditional predictions.
Therefore, it is common tothink about this model as if the joint interpretationwere absent, and not to model the observations atall.
For models which are conditional in the sense of8The general class of CRFs is more expressive and reducesto deficient HMMs only when they have just these features.o1 o2 o3 .
.
.
.
ons1 s2 s3 sn(a)o1 o2 o3 .
.
.
.
ons1 s2 s3 sn(b)Figure 6: Graphical models: (a) the downward HMM, and (b)the upward conditional Markov model (CMM).the factorization above, the JL and CL estimates forP(s|o) will always be the same.
It is therefore tempt-ing to believe that since one can find closed-form CLestimates (the RFEs) for these models, one can gainthe benefit of conditional estimation.
We will showthat this is not true, at least not here.Adopting the CMM has effects in and of itself, re-gardless of whether a maximum entropy approach isused to populate the P(s|s?1, o) estimates.
The MLestimate for this model is the RFE for P(s|s?1, o).For tagging, sparsity makes this impossible to reli-ably estimate directly, but even if we could do so, wewould have a graphical model with several defects.Every graphical model embodies conditional inde-pendence assumptions.
The NB model assumes thatobservations are independent given the class.
TheHMM assumes the Markov property that future ob-servations are independent from past ones given theintermediate state.
Both assumptions are obviouslyfalse in the data, but the models do well enough forthe tasks we ask of them.
However, the assumptionsin this upward model are worse, both qualitativelyand quantitatively.
It is a conditional model, in thatthe model can be factored as P(o)P(s|o).
As a re-sult, it makes no useful statement about the distribu-tion of the data, making it useless, for example, forgeneration or language modeling.
But more subtlynote that states are independent of future observa-tions.
As a result, future cues are unable to influ-ence past decisions in certain cases.
For example,imagine tagging an entire sentence where the firstword is an unknown word.
With this model struc-ture, if we ask about the possible tags for the firstword, we will get back the marginal distribution over(sentence-initial) unknown words?
tags, regardless ofthe following words.We constructed two taggers.
One was an HMM,as in figure 6a.
It was trained for JL, CL?, andCL.
The second was a CMM, as in figure 6b.
Weused a maximum entropy model over the (word, tag)and (previous-tag, tag) features to approximate theP(s|s?1, o) conditional probabilities.
This CMM isreferred to as an MEMM.
A 9-1 split of the Penn tree-bank was used as the data corpus.
To smooth thesemodels as equally as possible and to give as unifieda treatment of unseen words as possible, we mappedall words which occurred only once in training to anunknown token.
New words in the test data were alsomapped to this token.9Using these taggers, we examined what kinds oferrors actually occurred.
One kind of error tendencyin CMMs which has been hypothesized in the liter-ature is called label bias (Bottou, 1991; Lafferty etal., 2001).
Label bias is a type of explaining-awayphenomenon (Pearl, 1988) which can be attributedto the local conditional modeling of each state.
Theidea is that states whose following-state distributionshave low entropy will be preferred.
Whatever massarrives at a state must be pushed to successor states;it cannot be dumped on alternate observations as inan HMM.
In theory, this means that the model can getinto a dysfunctional behavior where a trajectory hasno relation to the observations but will still stumbleonward with high conditional probability.
The sensein which this is an explaining-away phenomenon isthat the previous state explains the current state sowell that the observation at the current state is effec-tively ignored.
What we found in the case of POS tag-ging was the opposite.
The state-state distributionsare on average nowhere near as sharply distributedas the state-observation distributions.
This gives riseto the reverse explaining-away effect.
The observa-tions explain the states above them so well that theprevious states are effectively ignored.
We call thisobservation bias.As an example, consider what happens when aword has only a single tag.
The conditional distri-bution for the tag above that word will always as-sign conditional probability one to that single tag, re-gardless of the previous tag.
Figure 7 shows the sen-tence All the indexes dove ., in which All should betagged as a predeterminer (PDT).10 Most occurrencesof All, however, are as a determiner (DT, 106/135 vs26/135), and it is much more common for a sentenceto begin with a determiner than a predeterminer.
The9Doing so lowered our accuracy by approximately 2% forall models, but gave better-controlled experiments.10The treebank predeterminer tag is meant for when wordslike All are followed by a determiner, as in this case.HMM MEMM MEMM?Correct States PDT DT NNS VBD .
-0.0 -1.3 -0.0Incorrect States DT DT NNS VBD .
-5.4 -0.3 -5.7Observations All the indexes dove .Figure 7: The MEMM exhibits observation bias: knowing thatthe is a DT makes the quality of the DT-DT transition irrelevant,and All receives its most common tag (DT).other words occur with only one tag in the tree-bank.11 The HMM tags this sentence correctly, be-cause two determiners in a row is rarer than All be-ing a predeterminer (and a predeterminer beginninga sentence).
However, the MEMM shows exactly theeffect described above, choosing the most commontag (DT) for All, since the choice of tag for All doesnot effect the conditional tagging distribution for the.The MEMM parameters do assign a lower weight tothe DT DT feature than to the PDT DT feature, but thethe ensures a DT tag, regardless.Exploiting the joint interpretation of the CMM,what we can do is to unobserve word nodes, leavingthe graphical model as it is, but changing the obser-vation status of a given node to ?not observed?.
Forexample, we can retain our knowledge that the stateabove the is DT, but ?forget?
that we know that theword at that position is the.
If we do inference in thisexample with the unobserved, taking a weighted sumover all values of that node, then the conditional dis-tribution over tag sequences changes as shown underMEMM?
: the correct tagging has once again becomemost probable.
Unobserving the word itself is not apriori a good idea.
It could easily put too much pres-sure on the last state to explain the fixed state.
Thiseffect is even visible in this small example: the like-lihood of the more typical PDT-DT tag sequence iseven higher for MEMM?
than the HMM.These issues are quite important for NLP, sincestate-of-the-art statistical taggers are all based on oneof these two models.
In order to check which, if ei-ther, of label or observation bias is actually contribut-ing to tagging error, we performed the following ex-periments with our simple HMM and MEMM taggers.First, we measured, on the training data, the entropyof the next-state distribution P(s|s?1) for each states.
For both the HMM and MEMM, we then measuredthe relative overproposal rate for each state: the num-ber of errors where that state was incorrectly pre-dicted in the test set, divided by the overall frequencyof that state in the correct answers.
The label bias hy-pothesis makes a concrete prediction: lower entropy11For the sake of clarity, this example has been slightly doc-tored by the removal of several non-DT occurrences of the in thetreebank ?
all incorrect.-1-0.8-0.6-0.4-0.200.20.40.60.810 1 2 3 4HMMMEMMFigure 8: State transition entropy (x-axis) does not appear to bepositively correlated with the relative over-proposal frequency(y-axis) of the tags for the MEMM model, though it is slightly sowith the HMM model.states should have higher relative overproposal val-ues, especially for the MEMM.
Figure 8 shows thatthe trends, if any, are not clear.
There does appearto be a slight tendency to have higher error on thelow-entropy tags for the HMM, but if there is any su-perficial trend for the MEMM, it is the reverse.On the other hand, if systematically unobservingunambiguous observations in the MEMM led to an in-crease in accuracy, then we would have evidence ofobservation bias.
Figure 5 shows that this is exactlythe case.
The error rate of the MEMM drops whenwe unobserve these single-tag words (from 10.8%to 9.5%), and the error rate in positions before suchwords drops even more sharply (17.1% to 15.0%).The drop in overall error in fact cuts the gap betweenthe HMM and the MEMM by about half.The claim here is not that label bias is impossi-ble for MEMMs, nor that state-of-the-art maxent tag-gers would necessarily benefit from the unobservingof fixed-tag words ?
if there are already (tag, next-word) features in the model, this effect should befar weaker.
The claim is that the independence as-sumptions embodied by the conditionally structuredmodel were the primary root of the lower accuracyfor this model.
Label bias and observation bias areboth explaining-away phenomena, and are both con-sequences of these assumptions.
Explaining-awayeffects will be found quite generally in conditionally-structured models, and should be carefully consid-ered before such models are adopted.
The effect canbe good or bad: In the case of the NB-CL model,there was also an explaining-away effects among thewords.
This is exactly the cause for flowers beinga weaker indicator than transatlantic in our condi-tional estimation example.
In that case, we wantedcertain word occurrences to be explained away by thepresence of more explanatory words.
However, whensome of the competing conditioned features are pre-vious local decisions, ignoring them can be harmful.4 Related ResultsJohnson (2001) describes two parsing experiments.First, he examines a PCFG over the ATIS treebank,trained both using RFEs to maximize JL, and using aCG method to maximize what we have been callingCL?.
He does not give results for the unconstrainedCL, but even in the constrained case, the effects fromsection 2 occur.
CL and parsing accuracy are bothhigher using the CL?
estimates.
He also describesa conditional shift-reduce parsing model, but notesthat it underperforms the simpler joint formulation.We take these two results not as contradictory, but asconfirmation that conditional estimation, though of-ten slow, generally improves accuracy, while condi-tional model structures must be used with caution.The conditional shift-reduce parsing model he de-scribes can be expected to exhibit the same type ofcompeting-variable explaining-away issues that oc-cur in MEMM tagging.
As an extreme example, if allwords have been shifted, the rest of the parser actionswill be reductions with probability one.Goodman (1996) describes algorithms for parseselection where the criterion being maximized inparse selection is the bracket-based accuracy mea-sure that parses are scored by.
He shows a test-setaccuracy benefit from optimizing accuracy directly.Finally, model structure and parameter estimationare not the entirety of factors which determine the be-havior of a model.
Model features are crucial, and theability to incorporate richer features in a relativelysensible way also leads to improved models.
Thisis the main basis of the real world benefit which hasbeen derived from maxent models.5 ConclusionsWe have argued that optimizing an objective that isas close to the task ?accuracy?
as possible is advanta-geous in NLP domains, even in data-poor cases wheremachine-learning results suggest discriminative ap-proaches may not be reliable.
We have also arguedthat the model structure is a far more important issue.For simple POS tagging, the observation bias effect ofthe model?s independence assumptions is more evi-dent than label bias as a source of error, but both areexamples of explaining-away effects which can arisein conditionally structured models.
Our results, com-bined with others in the literature, suggest that con-ditional model structure is, in and of itself, undesir-able, unless that structure enables methods of incor-porating better features, explaining why maximum-entropy taggers and parsers have had such successdespite the inferior performance of their basic skele-tal models.ReferencesAlan Agresti.
1990.
Categorical Data Analysis.
John Wiley &Sons, New York.Adam L. Berger, Stephen A. Della Pietra, and Vincent J. DellaPietra.
1996.
A maximum entropy approach to natural lan-guage processing.
Computational Linguistics, 22:39?71.D.
P. Bertsekas.
1995.
Nonlinear Programming.
Athena Scien-tific, Belmont, MA.Le?on Bottou.
1991.
Une approche theorique de l?apprentissageconnexioniste; applications a la reconnaissance de la pa-role.
Ph.D. thesis, Universite?
de Paris XI.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speech tagger.In ANLP 6, pages 224?231.S.
Chen and R. Rosenfeld.
1999.
A gaussian prior for smooth-ing maximum entropy models.
Technical Report CMU CS-99-108, Carnegie Mellon University.Jerome H. Friedman.
1997.
On bias, variance, 0/1?loss, andthe curse-of-dimensionality.
Data Mining and KnowledgeDiscovery, 1(1):55?77.William A. Gale, Kenneth W. Church, and David Yarowsky.1992.
A method for disambiguating word senses in a largecorpus.
Computers and the Humanities, 26:415?439.Joshua Goodman.
1996.
Parsing algorithms and metrics.
InACL 34, pages 177?183.Mark Johnson.
2001.
Joint and conditional estimation of tag-ging and parsing models.
In ACL 39, pages 314?321.A.
Kilgarriff.
1998.
Senseval: An exercise in evaluating wordsense disambiguation programs.
In LREC, pages 581?588.John Lafferty, Fernando Pereira, and Andrew McCallum.
2001.Conditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In ICML.Claudia Leacock, Martin Chodorow, and George A. Miller.1998.
Using corpus statistics and Wordnet relations for senseidentification.
Computational Linguistics, 24:147?165.Andrew McCallum and Kamal Nigam.
1998.
A comparison ofevent models for naive bayes text classification.
In WorkingNotes of the 1998 AAAI/ICML Workshop on Learning forText Categorization.Andrew Y. Ng and Michael Jordan.
2002.
On discriminative vs.generative classifiers: A comparison of logistic regressionand naive bayes.
In NIPS 14.Judea Pearl.
1988.
Probabilistic Reasoning in Intelligent Sys-tems: Networks of Plausible Inference.
Morgan Kaufmann,San Mateo, CA.W.
H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetter-ling.
1988.
Numerical Recipes in C. Cambridge UniversityPress, Cambridge.Adwait Ratnaparkhi.
1998.
Maximum Entropy Models for Nat-ural Language Ambiguity Resolution.
Ph.D. thesis, Univer-sity of Pennsylvania.Scott M. Thede and Mary P. Harper.
1999.
Second-order hiddenMarkov model for part-of-speech tagging.
In ACL 37, pages175?182.Kristina Toutanova and Christopher D. Manning.
2000.
En-riching the knowledge sources used in a maximum entropypart-of-speech tagger.
In EMNLP/VLC 2000, pages 63?70.
