Relaxation Techniques for ParsingGrammatically Ill-Formed Input inNatural Language Understanding Systems 1Stan C. KwasnyComputer Science DepartmentIndiana UniversityBloomington, Indiana 47401Norman K. SondheimerSperry UnivacBlue Bell, Pennsylvania 19424This paper investigates several language phenomena either considered deviant bylinguistic standards or insufficiently addressed by existing approaches.
These includeco-occurrence violations, some forms of ellipsis and extraneous forms, and conjunction.Relaxation techniques for their treatment in Natural Language Understanding Systems arediscussed.
These techniques, developed within the Augmented Transition Network (ATN)model, are shown to be adequate to handle many of these cases.1.
IntroductionAmong the components included in a Natural Lan-guage Understanding (NLU) system is a grammarwhich specifies much of the linguistic structure of theutterances that can be expected.
However, it is cer-tain that inputs that are ill-formed with respect o thegrammar will be received, both because people regu-larly form ungrammatical utterances and because thereare a variety of forms that cannot be readily includedin current grammatical models and are hence "extra-grammatical".
These might be rejected, but as Wilks(1976) stresses, "... understanding requires, at thevery least, ... some attempt to interpret, rather thanmerely reject, what seem to be ill-formed utterances.
"This paper investigates several anguage phenomenacommonly considered ungrammatical or extra-grammatical and discusses techniques directed at inte-grating them as much as possible into the conventionalframework of grammatical processing performed byNLU systems.
A "normative" grammar is assumedl This paper is a revised and extended version of a paper"Ungrammatical i ty and Extra-Grammatical i ty in Natural LanguageUnderstanding Systems" presented at the 17th Annual  Meeting ofthe Association for Computational  Linguistics, San Diego, Califor-nia, August,  1979.which specifies the structure of well-formed inputs.Rules that are both manually added to the originalgrammar and, by relaxing the constraints of the gram-mar, automatically constructed uring parsing analyzethe ill-formed input.
The ill-formedness is shown atthe completion of a parse by an indication of its devi-ance from a fully grammatical structure.
We havebeen able to accomplish this while preserving thestructural characteristics of the original grammar andits inherent efficiency.The Augmented Transition Network (ATN) modelwas chosen as the tool in which to express our ideas,and a basic understanding of this model is assumedthroughout the paper.
2 However, we believe our ideasare more general in scope than the ATN implementa-tion may suggest.
Similar ideas have recently beenintegrated into the Augmented Phrase Structure Gram-mar model by Miller, Heidorn, and Jensen (1981).Some of the phenomena discussed have been con-sidered previously in particular NLU systems, as, forexample, the ellipsis handling in L IFER described inHendrix, Sacerdoti, Sagalowicz, and Slocum (1978).Similar methods for processing conjunction have been2 For a thorough introduct ion to the ATN formalism, seeBates (1978).Copyright 1981 by the Association for Computational  Linguistics.
Permission to copy without fee all or part of this material is grantedprovided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included onthe first page.
To copy otherwise, or to republish, requires a fee and/or  specific permission.0362-613X/81/020099-10501.00American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 99Stan C. Kwasny and Norman K. Sondheimer Relaxation Techniques for Parsing Ill-Formed Inputused in the LUNAR system as discussed in Woods(1973).
In linguistic studies, Chomsky (1964) andKatz (1964), among others, have considered the treat-ment of ungrammaticality within the TransformationalGenerative model.
Contemporary studies closest toours are those of Weischedel and Black (1980) andHayes and Mouradian (1980).
Similarities will bepointed out as our techniques are presented.
Thepresent study is distinguished by the range of phenom-ena considered, structural and efficiency goals, and theinclusion of techniques in one uniform framework.This paper surveys these problems, discusses mech-anisms aimed at solving them, and discusses how thesemechanisms are used.
At the end, some limitations arediscussed and extensions uggested.
Unless otherwisenoted, all ideas have been tested through implementa-tion.
A more detailed discussion of all points may befound in Kwasny (1980).2.
Language PhenomenaThroughout his paper we will argue that success inhandling ungrammatical  and extra-grammatical  inputdepends on two factors.
The first is the identificationof types of i l l-formedness and the patterns that theyfollow.
The second is the relating of i l l-formed inputto the parsing path of a grammatical input the userintends.
This section introduces the types of ill-formedness we have studied and discusses their rela-tionship to grammatical structures in terms of ATNgrammars.2.1 Co-Occurrence ViolationsOur first class of errors is connected to co-occurrence restrictions within a sentence.
There aremany occasions in a sentence where two or more partsmust agree, as in the following examples (an asterisk"*" indicates an i l l - formed or ungrammatical  sen-tence):*Draw a circles.
*I, along with many other Germans, areconcerned about the Russian threat.
*I will stay from now under midnight.The errors in the above involve coordination betweenthe italicized words.
The first and second examplesillustrate simple agreement problems.
The third in-volves a complicated relation among at least the threeitalicized terms.Such phenomena do occur naturally.
For example,Shores (1977) analyzes fifty-six freshman Englishpapers written by black college students and revealspatterns of nonstandard usage ranging from uninflect-ed plurals, possessives, and third person singulars tooverinflection (use of inappropriate ndings).In ATN terms, the significant blocks that keepinputs with co-occurrence violations from being parsedas t~ae user intended arise from a failure of a test onan arc or the failure to satisfy an arc type restriction,e.g., the failure of a word to be in the correct catego-ry.
The essential block in the first example abovewould likely occur on an agreement test  on an arcaccepting a noun.
The essential blockages in the sec-ond and third examples are likely to come from failureof the arcs testing the verb and final preposition, re-spectively.2.2 Ellipsis and Extraneous TermsIn handling ellipsis, the most relevant distinction tomake is between contextual and telegraphic ellipsis.Contextual  ellipsis occurs when a form makes senseonly in the context of other sentences.
For example,the form*President Reagan has.seems ungrammatical without the preceding questionWho has a wife named Nancy?Telegraphic ellipsis, on the other hand, occurs when aform only makes proper sense in a particular situation.For example, the forms*3 chairs no waiting (sign in barber shop)*Yanks split (headline in sports section)*Profit margins for each product(query submitted to a NLU system)are cases of telegraphic ellipsis with appropriate situa-tions for their use noted in parentheses.
The third oneis from an experimental  study of NLU for manage-ment information by Malhotra (1975) which indicatedthat such forms must be considered.Further evidence to justify a study of ellipsis isgiven by Thompson (1980) who analyzes several mod-es of communication and concludes that an importantsubset of utterances used in communication are frag-ments, not sentences.
Of particular interest in theanalysis is the notion of a terse question, which is atype of telegraphic ellipsis, and a terse reply, which isa type of contextual ellipsis.
In her human-to-computer experiments, he found 7.6% terse questionsand 10.3% terse replies out of 882 parsed messages.Another type of ungrammatical i ty complementaryto ellipsis occurs when the user puts unnecessarywords or phrases in an utterance.
The reason for anextra word may be a change of intention in the middleof an utterance, an oversight, or simply for emphasis.For example,*Did you ... did I erase any files?
*List prices of single unit prices for 72 and 73:The second example again comes from Malhotra(1975).100 American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981Stan C. Kwasny and Norman K. Sondheimer Relaxation Techniques for Parsing Ill-Formed InputThe best way to view il l-formedness in terms of theATN is to think of the user as trying to complete apath through the grammar, but having produced aninput that has too many or too few forms necessary totraverse all arcs.
For contextual ellipsis, grammaticalprocessing can be guided, in part, by expectations ofwhat the form of the input will be.For telegraphic ellipsis, it might be argued that theforms could be placed independently in the grammar,but since they allow so much variation compared togrammatical forms, including them with existing tech-niques would dramatically increase the size of thegrammar.
Furthermore, there is a real distinction interms of completeness and clarity of intent betweengrammatical and ungrammatical forms.
Hence we feeljustified in suggesting special techniques for theirtreatment which will relate their structure to the nor-mative grammar.2.3 Conjunct ionConjunction is an extremely common phenomenon,but it is seldom directly treated in a grammar.
Wehave considered several types of conjunction.
Simpleforms of conjunction occur most frequently, as inJohn loves Mary and hates Sue.Gapping occurs when internal segments of the secondconjunct are missing, as inJohn loves Mary and Mary John.The list form of conjunction occurs when more thantwo elements are joined in a single phrase, as inJohn loves Mary, Sue, Nancy, and Bill.Correlative conjunction occurs in sentences to coordi-nate the joining of constituents, as inJohn both loves and hates Sue.Conjunctions are generally not included in gram-mars because, similarly to ungrammatical forms, theycan appear in so many places that their inclusionwould dramatically increase the size of the grammar.We see conjunction as similar to ellipsis.
For ex-ample, we see some of the .above as resulting fromsentence-joining with elided forms:John loves Mary and John hates Sue.John loves Mary and Mary loves John.John loves Mary, John loves Sue,John loves Nancy, and John loves Bill.This view of conjunction is consistent with views ex-pressed by Halliday and Hasan (1976) and by Quirk,Greenbaum, Leech, and Svartik (1972).3.
The Mechanisms and How They ApplyIn this section, techniques are described for ad-dressing the phenomena introduced in the previoussection.
All of the techniques follow a general para-digm of relaxation wherein a normative grammar isassumed which describes the set of acceptable inputsto the system.
During parsing, whenever a relaxablearc cannot be traversed, a backtrack point is createdwhich includes the relaxed version of the arc.
Thesealternatives are not considered until after all possiblegrammatical paths have been attempted, thereby insur-ing that grammatical inputs will be handled correctly.Relaxation of previously relaxed arcs is also possible.When processing is completed, a "deviance note"which describes how the input deviated from the ex-pected grammatical form is passed along to other proc-essing components of the system.3.1 Feature Relaxation TechniquesThe most straightforward methods of relaxation arethose that operate at the lexical level.
The principleinvolved is to permit a word which is slightly inappro-priate to stand in place of the correct word.
Slightfeature variations are present in the lexical entry forthe word found in the input when compared to theexpected word.
Two methods of feature relaxationhave been investigated.Our first method involves relaxing a test on an arc.Test relaxation occurs when the test portion of an arccontains a relaxable predicate and the test fails.
Twomethods of test relaxation have been devised and im-plemented based on predicate type.
Predicates can bedesignated by the grammar writer as either absolutelyviolable in which case the opposite value of the predi-cate is substituted for the predicate during relaxationor conditionally violable in which case a substitutepredicate is provided.
For example, consider the fol-lowing to be a test that fails(AND(NUMBER-AGREE SUBJ V)(INTRANS V))If the predicate NUMBER-AGREE was declared ab-solutely violable and its use in this test returned thevalue NIL, then the negation of (NUMBER-AGREESUBJ V) would replace it in the test creating a newarc with the test:(ANDT(INTRANS V))If INTRANS were condit ionally violable with the sub-stitute predicate TRANS, then the following testwould appear on the new arc:(AND(NUMBER-AGREE SUBJ V)(TRANS V))American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 101Stan C. Kwasny and Norman K. Sondheimer Relaxation Techniques for Parsing I l l -Formed InputWhenever more than one test in a failing arc is viola-ble, all possible single relaxations are attempted inde-pendently.
This is similar to a method of Weischedeland Black (1980).Chomsky (1964), in developing his ideas on de-grees of grammaticalness, discusses the notion of or-ganizing word categories hierarchically.
We have ap-plied and extended these ideas in our second methodof relaxation called category relaxation.
In this me-thod, the grammar writer produces, along with thegrammar, a hierarchy describing the relationshipamong words, categories, and phrase types which isused by the relaxation mechanism to construct relaxedversions of arcs that have failed.
When an arc failsbecause of an arc type failure, i.e., because a particu-lar word, category, or phrase was not found, a new arc(or arcs) may be created according to the descriptionof the word, category, or phrase in the hierarchy.
Typ-ically, PUSH arcs will relax to PUSH arcs, CAT arcsto CAT or PUSH arcs, and WRD or MEM arcs toCAT arcs.
Consider, for example, the syntactic cate-gory hierarchy for pronouns shown in Figure 1.
Forthis example, the category relaxation mechanism wouldallow the relaxation of a CAT arc identifying PER-SONAL pronouns to include the entire PRONOUNcategory, including the subcategories REFLEXIVEand DEMONSTRATIVE .
As with test relaxation,successive relaxations could occur.For both methods of relaxation, "deviance notes"are generated which describe the nature of the relaxa-tion in each case.
For example, in the first relaxationexample given above, The deviance note produced byour implementation would look like:(RELAXED-TEST(FROM(AND(TO( AND(NUMBER-AGREE SUBJ V)(INTRANS V)))T(INTRANS V) ) ) )Where multiple types or multiple levels of relaxationoccur, a note is generated for each.
The entire list ofdeviance notes accompanies the final structure prod-uced by the parser.
In this way, the final structure ismarked as deviant and the nature of the deviance isavailable for use by other components of the under-standing system.In our implementation, test relaxation has beenfully implemented, while category relaxation has beenimplemented for all cases except those involving PUSHarcs.
In general, the CAT to PUSH category relaxa-tion should involve no special problems in implementa-PRONOUNSDEMONSTRATIVE PERSONAL REFLEXIVEooTHIS/ /  oooHE SHE ME THEMFigure 1.
Category hierarchy. 6QYOURSELFtion, while the PUSH to PUSH relaxation involvesidentifying the failure of a PUSH arc, which involveskeeping track of well-formed substrings.3.2 Co-Occurrence  and Feature Relaxat ionHandling forms that are deviant because of co-occurrence violations involves using the above featurerelaxation methods.
Where simple tests exist within agrammar to filter out unacceptable forms, these testsmay be relaxed to allow the acceptance of these forms.For  co-occurrence violations, the points in thegrammar where parsing becomes blocked are useful indetermining exactly where the test or category viola-tions occur.
Arcs at those points are being attemptedand fail due to the failure of a co-occurrence test orcategorization requirement.
Relaxation is then appliedand alternatives generated which may be explored at alater point via backtracking.
For example, the sen-tence:*John love Maryshows a number disagreement between the subject andthe verb.
Most probably this would show up duringparsing in a test on an arc when the verb of the sen-tence is being considered.
That test could in fact bethe one considered in our example.
The test wouldfail and the traversal would not be allowed.
At thatpoint, an ungrammatical lternative similar to the onewe described would be created for later backtrackingconsideration, and processing would proceed as dis-cussed.Absolutely violable predicates can be permitted incases where the test describes ome superficial consist-ency checking or where the test 's failure or successdoes not have a direct effect on meaning, while condi-tionally violable predicates apply to predicates whichmust be relaxed cautiously to avoid loss of meaning.102 American Journal of Computational Linguistics, Volume 7, Number 2, Apri l - June 1981Stan C. Kwasny and Norman K. Sondheimer Relaxation Techniques for Parsing: Ill-Formed InputAn example of the application of category relaxa-tion can be seen utilizing the category hierarchy inFigure 1 :Draw me a circle.The normative grammar may be written so that aWRD arc is used to specify the processing of the word"me".
Subsequent relaxation of that arc, followingFigure 1, would permit any personal pronoun.
Thiswould specify a method of processing sentences uchas the following:*Draw he a circle.A second level of relaxation would permit forms like:*Draw this a circle.The technique of category relaxation is certainlynot limited merely to syntactic ategories.
The Burton(1976) semantic grammar approach to language proc-essing makes exclusive use of categories which distin-guish semantic entities rather than the traditional syn-tactic ones.
A category hierarchy can easily be con-structed which describes how these categories relate inthe system without extending the mechanism describedhere.3.3 Pat terns  and  the Pat tern  ArcIn this section, relaxation techniques are applied tothe grammar itself through the use of patterns andpattern-matching algorithms.
Other researchers haveused patterns for parsing, such as Parkison, Colby, andFaught (t977),  Wilks (t978),  and Hayes and Moura-dian (1980), but we have devised a method of inte-grating patterns within the ATN formalism for thepurpose of improving the grammar's responsiveness toungrammatical nd extra-grammatical inputs.In our current formulation, a pattern is a linearsequence of ATN arcs which is matched against theinput string.
A pattern arc (PAT) has been added tothe ATN formalism with a form similar to that of oth-er arcs:(PAT <pat spec> <test> <act>* <term>)The pattern specification (<pat spec>) is described inFigure 2 through a modified BNF grammar, where anasterisk indicates zero or more occurrences of thepreceding non-terminal.
The pattern (<pat t>)  is ei-ther a user-assigned pattern name, a ">" ,  or a list ofATN arcs, each of which may be preceded by the sym-bol ">"  to indicate a potentially optional arc, whilethe pattern mode (<mode>)  can be any of the key-words, UNANCHOR,  OPTIONAL,  or SKIP.
Theseare discussed below.An index of patterns is supported in our implemen-tation, to permit naming and referencing of patterns.Also supported is an index of arcs, allowing the nam-ing and referencing of arcs as well.
Further, namedpatterns and named arcs are defined as LISP macros,.which expand into arcs according to how they arereferenced.
This allows the two indexes and the gram-mar to be substantially reduced in size.Consider the following example of a pattern arc.Suppose the sentence:Mary drove the car.was processed by art ATN path through a sentencenetwork of three arcs, namely PERSON, TRANS-ACT,  and OBJ which processed "Mary'", "drove", and"the car" respectively.
Then a pattern of the sentencecould be specified as:(PERSON TRANS-ACT OBJ)This, then, could be included in a pattern arc, wherethe " .
.
. "
is realized by the appropriate tests andactions, as:(PAT ( (PERSON TRANS-ACT OBJ))  T ... )It could further be modified to permit optional constit-uents, as in(PAT ( (>PERSON >TRANS-ACT OBJ  )OPT IONAL)  T ... )Pattern matching proceeds by matching each arc inthe pattern against the input string, but is affected bythe chosen "mode'" of matching.
Since the individualcomponent arcs are, in a sense, complex patterns, theATN interpreter can be considered part of the match-ing algorithm as we\[l. In arcs within patterns, explicittransfer to a new state is ignored arid the next arcattempted on success is the one following ha the pat-tern.
As in the example above, art arc in a patternprefaced by ">"  can be.
considered optional, if theOPTIONAL mode has been selected to activate thisfeature.
When this is done, the matching algorithmstill attempts to match optional arcs, but may ignorethem.
In the example of a pattern arc using the op-<pat spec> :.
:= (<patt> <mode>*)<patt> ::= (<p arc>*)<pat name><mode> ::= UNANCHOROPTIONALSKIP<p arc> ::= <arc>> <arc>< pat name> :: = user-assigned pattern ame>Figure 2.
Description of pattern specification.American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 103Stan C. Kwasny and Norman K. Sondheimer Relaxation Techniques for Parsing Ill-Formed Inputtional feature above, the pattern component would beable to match any of the fragments:*Mary the car*Drove the car*The caras well as the original sentence.
We will return to thisexample in our discussion on conjunction.The manipulation of registers within optional arcsposes some interesting problems.
With the optionalityfeature, not every arc will be executed in a pattern.To complicate matters, registers previously set may beused within a test or they may hold elements of thefinal structure.
Tests involving undefined registers aredefaulted to allow traversal of these arcs, whilestructure-building actions involving undefined registersmark the missing components as undefined.In addition, there are two other modes of matching.A pattern unanchoring capability is activated by speci-fying the mode UNANCHOR.
In this mode, patternsare permitted to skip words prior to matching.
Specify-ing the SKIP mode results in words being ignored be-tween matches of the arcs within a pattern.
This is ageneralization of the UNANCHOR mode.As with all forms of relaxation, pattern matchingresults in deviance notes.
For patterns, these notescontain information necessary to determine howmatching succeeded.
Sufficient information is con-tained in the deviance notes to recover skipped wordsand skipped arcs, if desired.3.4 Patterns,  Ellipsis, and Extraneous FormsThe pattern arc is the primary mechanism for han-dling ellipsis and extraneous forms.
A pattern arc canbe seen as capturing a single path through a network.The matcher gives some freedom in how that pathrelates to a string.
The appropriate parsing paththrough a network relates to an elliptical sentence orone with extra words in the same way.
With contextu-al ellipsis, the relationship will be in having some ofthe arcs on the correct path not satisfied.
In patternarcs, these will be represented by arcs marked as op-tional.
Dialogue context will provide the defaults forthe missing components.
With pattern arcs, the devi-ance notes will show what was left out and the othercomponents in the NLU system will be responsible forsupplying the values.As an example, consider the following question andits possible elliptical responses:Who drove a car?*Mary.
*Mary did.
*Mary her Ford.The form of the basic expected response could beshown by the following pattern, where AUX refers toan arc that identifies auxiliary verbs:(PAT ( (PERSON >AUX >TRANS-ACT >OBJ)OPT IONAL)  T ... )This would serve to identify the three elliptical respon-ses as well as others.
It would also accept some un-likely inputs such as:*Mary could a car.
*Mary the Empire State Building.These would have to be filtered by later processing byother components through the use of the deviancenotes.
A pattern for the similar question:Mary drove what?could be as follows:(PAT ( (>PERSON >AUX >TRANS-ACT OBJ)OPT IONAL)  T ... )The source of patterns for contextual ellipsis isimportant.
In the L IFER system discussed in Hendrix,Sacerdoti, Sagalowicz, and Slocum (1978), the previ-ous user input can be seen as a pattern for ellipticalprocessing of the current input.
The automatic patterngenerator introduced in the next section, along with anexpectation mechanism, will capture this level of proc-essing.
But, with the ability to construct arbitrarypatterns and to add them to the grammar from othercomponents of the NLU system, our approach canaccomplish much more.
For example, a question gen-eration routine could add an expectation of a yes /noanswer in front of a transformed rephrasing of a ques-tion, as inDid Mary drive a car?
*Yes, she did.
*No, she did not.The appropriate patterns might be the following,where YES, NO, and NOT refer to arcs that acceptthese words:(PAT ( (YES >PERSON >AUX >TRANS-ACT>OBJ )  OPT IONAL)  T ... )(PAT ( (NO >PERSON >AUX >NOT >TRANS-ACT>OBJ )  OPT IONAL)  T ... )Patterns for telegraphic ellipsis have to be added tothe grammar manually.
One typical usage would be inaccepting terse questions where the actions default toa standard operation in the system.
For example, in adatabase situation, a reference to a set of objectscould be understood as a command to retrieve them,as with the "profit margin" query given earlier.
Apattern here might be the following, where IMP-ACTis an arc that identifies the typical action, and SET104 American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981Stan C. Kwasny and Norman K. Sondheimer Relaxation Techniques for Parsing Ill-Formed Inputidentifies a set description:(PAT ( (> IMP-ACT SET)OPT IONAL)  T ... )Generally, patterns of usage must be identified, say ina study like that of Malhotra (1975), so that appropri-ate patterns can be constructed.
Patterns for extrane-ous forms must also be added in advance.
Thesewould use either the UNANCHOR option in order toskip false starts, or use dynamically-produced patternsto catch repetitions for emphasis.
In general, only alimited number of these patterns should be required.The value of the pattern mechanism here, especially inthe case of telegraphic ellipsis, is in connecting theungrammatical forms to the grammatical ones.The problem of extraneous words can be attackedby two of the <mode> settings in pattern arcs.
TheUNANCHOR mode applies to restarts, as in the earli-er example, through the use of a simple pattern arc:mar through one of two devices.
They may be con-structed as needed by special macro arcs or they maybe constructed for future use through an expectationmechanism.As the expectat ion-based parsing efforts clearlyshow, syntactic elements, especially words, containimportant clues on processing (Riesbeck and Schank,1976).
Indeed, we also have found it useful to makethe ATN mechanism more "active" by allowing it toproduce new arcs based on such clues.
To achievethis, the CAT, MEM, TST, and WRD arcs have beengeneralized and four new "macro"  arcs, known asCAT*, MEM*, TST*, and WRD*, have been added tothe ATN formalism.
These are similar in every way totheir counterparts,  except that as a final action, in-stead of indicating the state to which the traversalleads, a new arc is constructed ynamically and imme-diately executed.
The difference in the form that thenew arc takes is seen in the following comparison:(PAT ( ( (PUSH S/ ... )) UNANCHOR)  ... )where the S/  network is used for parsing sentences.Extraneous words in the interior of sentences can beaccommodated by the SKIP mode.
These methods aresimilar to those described by Hayes and Mouradian(1980).3.5 Dynamic  Generat ion of PatternsPatterns need to be generated ynamically in orderto make use of the patterns of language occurring inthe sentence.
In this way, previous processing can bemade to contribute more directly to later processing.An automatic pattern generation mechanism has beenimplemented using the trace of the current executionpath to produce a pattern.
This is invoked by usingthe symbol ">"  in place of the pattern name.
Pat-terns produced in this fashion contain only those arcstraversed at the current level of recursion in the net-work, although this could easily be generalized in away which would permit PUSH arcs to be automatical-ly replaced by their subnetwork paths.
Each arc in anautomatic pattern of this type is marked as optional;however, additional control of the optionality featureof the pattern matcher can be exercised through judi-cious use of the test component of the arc.
Patternscan also be constructed ynamically in precisely thesame way grammatical structures are built by usingBUILDQ and other structure-building functions.Pattern arcs enter the grammar in two ways.
Theyare manually written into the grammar in those caseswhere the ungrammaticalit ies are common and theyare added to the grammar automatically in those caseswhere the ungrammaticality is dependent on context.Those that are produced dynamically enter the gram-(CAT <cat> <test> <act>*  <term>)(CAT* <cat> <test> <act>*  <creat act>)In this example, <creat  act> is used to define thedynamic arc through the use of BUILDQ and similarstructure-building functions, while ( te rm> representsany terminal action (usually a TO action).
Arcs com-puted by macro arcs can be of any type permitted bythe ATN, but one of the most useful arcs to computein this manner is the pattern arc discussed above.
Thisallows a prescribed path of arcs to be attempted inprocessing the sentence.An example of much of what was just described isthe following macro arc:(WRD* AND T(BUILDQ(PAT ( > OPT IONAL )T(SETR S (BUILDQ (AND + *) ) )) s))This WRD* macro arc attempts to find the word ANDat the current position of processing the input.
Ifsuccessful, it computes a pattern arc with the patternleft to be automatically generated upon execution.While the macro arc forces immediate xecution ofan arc, arcs may also be computed and temporari lyadded to the grammar for later execution through theexpectation mechanism.
Expectations are performed asactions within arcs (analogous to the HOLD action forparsing structures) or as actions elsewhere in the NLUsystem, e.g., during generation, when particular typesAmerican Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981 105Stan C. Kwasny and Norman K. Sondheimer Relaxation Techniques for Parsing I l l -Formed Inputof responses can be foreseen.
Two forms are allowed: (essentially) produce the arc:(EXPECT <creat act> <state>)(EXPECT <creat act>)In the first case, the arc created is bound to a state asspecified.
When later processing leads to that state,the expected arc will be attempted as one alternativeat that state.
In the second case, where no state isspecified, the effect is to attempt he arc at every statevisited during the parse.The range of an expectation produced during pars-ing is ordinarily limited to a single sentence, with thearc disappearing after it has been used; however, thestart state, S/ ,  is reserved for expectations intended tobe active at the beginning of the next  sentence.
Thesewill disappear in turn at the end of processing for thatsentence.3.6 Conjunction and Macro ArcsBesides their utility in handling cases of ellipsis,pattern arcs are also the primary mechanism for han-dling conjunction.
As mentioned earlier, the rationalefor this is the connection between conjunction andellipsis.
Whenever a conjunction is seen, a pattern isdeveloped from the already identified elements andmatched against the remaining segments of input.All of the forms of conjunction described above aretreated through a globally defined set of "conjunctionarcs", which are active at every state of the grammar.Some restricted cases, such as "and" following"between" ,  may have the conjunction built into thegrammar.
These forms are not subject to the conjunc-tion arcs.
In general, the conjunction arcs are madeup of macro arcs which compute pattern arcs.
Theautomatic pattern mechanism is heavily used.Returning to the earlier example of pattern arcs, itcan  be shown how these conjunction arcs work.
Con-sider what takes place when the macro arc previouslydescribed is activated during the processing of thesentence:Mary drove the car and John drove the truck.Processing proceeds until the conjunction "and" isencountered.
For the purpose of this example, it isassumed that the top level path of arcs followed duringprocessing is simply the pattern discussed earlier:(PERSON TRANS-ACT OBJ)The action of the macro arc is, therefore, to(PAT ((>PERSON >TRANS-ACT >OBJ) OPTIONAL)T(SETR S (BUILDQ (AND + * ) S))) )Processing continues from the state that triggered theexecution of the conjunction arc, and a structure forthe conjoined sentence is produced.
In fact, any ofthe following variations on the sentence can be han-dled in precisely the same way:Mary drove the car and the truck.Mary drove the car and John the truck.Mary drove the car and drove the truck.Of course, more actions could be specified in the arc,as well as more complicated tests.
In particular, a testshould be included to force matching of the OBJ arc.With simple conjunctions, the rightmost elements inthe patterns are matched.
Internal elements in pat-terns are skipped when gapping occurs.
The list formof conjunction which uses punctuat ion in place ofsome of the conjunctions, can also be handled throughthe careful construction of dynamic patterns whichform deferred expectations.
Correlative conjunction istreated similarly, with expectations based on deferringthe dynamic building of patterns as well.The fact that conjunction arcs occur implicitly atevery state of the grammar permits the same conjunc-tion to trigger the same set of conjunction arcs fromseveral different states at potentially several differentlevels in the grammar.
With pattern arcs being com-puted dynamically, different patterns are attempted,depending largely on the subnetwork in which thestate occurs.
In this way the proper joining of constit-uents is realized.3.7 Interaction of TechniquesAs grammatical processing proceeds, ungrammaticalpossibilities are continually being suggested from thevarious mechanisms we have implemented.
To coordi-nate all of these activities, the backtracking mechanismhas been improved to keep track of these alternatives.All paths in the original grammar are attempted first.Only when all of these fail are the conjunction alterna-tives and the manual ly-added and dynamical ly-produced ungrammatical lternatives tried.
All of thealternatives associated with a single state can bethought of as a single possibility.
A selection mecha-nism is used to determine which backtrack pointamong the many potential alternatives hould be ex-plored next.
Currently, we use a method also used byWeischedel and Black (1980) of selecting the alterna-tive with the longest path, i.e., the one with the deep-est penetration in the network.106 American Journal of Computat ional  Linguistics, Vo lume 7, Number  2, Apr i l - June 1981Stan C. Kwasny and Norman K. Sondheimer Relaxation Techniques for Parsing Ill-Formed Input4.
Limitations and ExtensionsWork has only begun on the problems associatedwith handling ill-formed input.
The techniques dis-cussed in this paper, we believe, are a step towardsolving these problems, but work is continuing alongseveral dimensions.As they are presented and implemented, the testand category relaxation methods can be too non-specific in permitting relaxation of these types, sinceall arcs with the named tests and categories becomepotentially relaxable.
But, the same function can beused for different purposes.
For example, the functionGETR can be used just to retrieve a value for a latertest or as a predicate when used as a register contain-ing a true/false flag.
Weischedel and Black (1980)allow for such cases by having each instance of a testindividually predicated.
A compromise that wouldallow more generality to be captured would be tospecify some or all of the arguments to the relaxablefunction when describing the form of relaxable func-tion calls.
The identification of arcs for category re-laxation can be treated similarly.The SKIP mode on patterns is limited in its applica-bility to sentences with extraneous forms.
First, themode setting can only be used on patterns, not on thegeneral grammar.
Second, it is probably too non-selective.
The interpreter does prefer to skip as fewwords as possible; however no preference is given asto where to skip them.
Thus, even if patterns of ex-traneous words are identified precisely, the word skip-ping mechanism would simply not be able to allow forthem.One weakness in the pattern arc is the treatment ofregister settings in optional arcs that are skipped.Rather than ignoring the use of registers that wouldhave been set in the arc, it would be better to devise adefault strategy to force their setting.It is instructive to compare our conjunction techni-que to the SYSCONJ facility of Woods (1973).
Thelatter treats conjunction as showing alternative waysof continuing a sentence.
This allows for sentencessuch asHe drove his car through and brokea plate glass window.which at best we will accept with a misleading devi-ance note.
This is in part an example of what Quirk(1972) calls "cataphoric" ellipsis:John can pass the examination, and Bobcertainly will, pass the examinationHe drove his car through a plate glass windowand he broke a plate glass window.Here, the ellipsis occurs before, not after, the conjunc-tion.
To utilize techniques in our style on the aboveexamples, it would be necessary to extract the properpattern from the entire path of traversed arcs.
Howev-er, unlike SYSCONJ,  our technique can handle theobvious elliptical cases such as gapping, or the tightlyconstrained cases such as correlatives.One of our original hopes for this work was thateach of the relaxations would be applicable to a broadset of individual grammars.
However, it quickly be-came obvious that the relaxations and grammar workbest if they are developed together.
The test relaxa-tions that depend on the use of the predicates in thegrammar are an obvious case in point.
Another exam-ple is the case of conjunction arcs, which work bestwhen constituent boundaries in the grammar reflectthe structures that can be conjoined.Another area for improvement is in the amount oftime spent parsing before any ungrammatical choicesare considered.
Currently, all grammatical paths areconsidered first.
This organization may be temperedby adjusting operational parameters in our implemen-tation so that when a block occurs some or all of theungrammatical ternatives may be investigated.
How-ever, guidelines for adjusting these parameters arelacking at the moment.Another area for more research is the production ofpatterns for interpreting contextual ellipsis.
At themoment,  we inspect question forms and add prede-fined patterns to accept their elliptical answers.
Inorder to produce these patterns automatically, a theoryof question-and-answer dialogue is necessary.Finally, developing heuristics which choose alterna-tives for relaxation among many blockages is a majorproblem.
Our heuristic which picks the alternativewith the longest path can overshoot he actual failureand possibly lead to an incorrectly built structure.
Webelieve, as do Weischedel and Black (1980), that theanswer may lie in being able to examine the semanticplausibility of partial structures.Several other types of i l l-formedness have not beenconsidered in this study, for example, idioms, meta-phors, incorrect word order, run-together sentences,incorrect punctuation, misspelling, and presupposition-al failure.
For each of these phenomena, either littleis known about it or it has been studied elsewhereindependently.
In either case, work remains to bedone to develop processing for them within our frame-work.5.
ConclusionsDespite the limitations and needed extensions dis-cussed above, the results described here are signifi-cant, we believe, because they extend the state of theart in several ways.
Most important are the following:1.
The use of the category hierarchy to han-dle arc type failures.American Journal of Computational Linguistics, Volume 7, Number 2.
April-June 1981 107Stan C. Kwasny and Norman K. Sondheimer Relaxation Techniques for Parsing lU-Formed Input2.
The use of the pattern mechanism to allowfor contextual ellipsis and gapping.3.
More generally, the use of patterns to al-low for many sorts of ellipsis and con-junctions.4.
Finally, the combination of all of the tech-niques in one coherent system, where be-cause all grammatical alternatives aretried first and no modifications are madeto the original grammar, its inherent effi-ciency and structure are preserved.The theme of the final point is further developed inSondheimer and Weischedel (1980).AcknowledgmentsWe wish to acknowledge the comments  of RalphWeischedel  and Marc  Fogel  on previous drafts of thispaper.
The cooperat ion  and comments  of  GeorgeHeidorn,  R ichard Wexelblat ,  and the reviewers of thispaper also contr ibuted greatly.ReferencesBates, M., "The Theory and Practice of Augmented TransitionNetwork Grammars," in Natural Language Communication withComputers, L. Bolc (Ed.
), 191-259, Springer-Verlag, Berlin,191-259, 1978.Burton, Richard R., "Semantic Grammar: An Engineering Techni-que for Constructing Natural Language Understanding Sys-tems," Bolt Beranek and Newman, Report No.
3453, Decem-ber, 1976.Chomsky, N., "Degrees of Grammaticalness," in The Structure ofLanguage: Readings in the Philosophy of Language, Fodor, J.A.and J.J. Katz (Eds.
), 384-389, Prentice-Hall, Englewood Cliffs,New Jersey, 1964.Halliday, M.A.K.
and R. Hasan, Cohesion in English, Longman,London, 1976.Hayes, P., and G. Mouradian, "Flexible Parsing," Proceedings of the18th Annual Meeting of the Association for ComputationalLinguistics, 97-103, Philadelphia, June, 1980.Hendrix, G.G., E.D.
Sacerdoti, D. Sagalowicz, and J. Slocum,"Developing a Natural Language Interface to Complex Data,"ACM Transactions on Database Systems, 3, 2, 105-147, June,1978.Katz, J.J., "Semi-Sentences," in The Structure of Language: Read-ings in the Philosophy of Language, Fodor, J.A.
and J.J.
Katz(Eds.
), 400-416, Prentice-Hall, Englewood Cliffs, New Jersey,1964.Kwasny, S.C., "Treatment of Ungrammatical nd ExtragrammaticalPhenomena in Natural Language Understanding Systems," PhDdissertation, Ohio State University, 1980, (available through theIndiana University Linguistics Club, Bloomington, Indiana).Malhotra, A., "Design Criteria for a Knowledge-Based EnglishLanguage System for Management: An Experimental Analysis,"MAC TR-146, M.I.T., Cambridge, MA, February, 1975.Miller, L.A., G.E.
Heidorn, and K. Jensen, "Text-Critiquing withthe EPISTLE System: An Author's Aid to Better Syntax,"Proceedings of the National Computer Conference, 50, 649-655,AFIPS Press, Arlington, VA, 1981.Parkison, R.C., K.M.
Colby, and W.S.
Faught, "ConversationalLanguage Comprehension Using Integrated Pattern-Matchingand Parsing," Artificial Intelligence 9, 2, 111-134, October,1977.Quirk, R., S. Greenbaum, G. Leech, and J. Svartik, A Grammar ofContemporary English, Seminar Press, New York, 1972.Riesbeck, C.K., and R.C.
Schank, "Comprehension by Computer:Expectation-Based Analysis of Sentences in Context," YaleUniversity, Research Report 78, October, 1976.Shores, D.L., "Black English and Black Attitudes," in Papers inLanguage Variation, D.L Shores and C.P.
Hines (Ed.
), TheUniversity of Alabama Press, University, Alabama, 1977.Sondheimer, N.K., and R.M.
Weischedel, "A Rule-Based Approachto Ill-Formed Input," COLING 80: Proceedings of the EighthInternational Conference on Computational Linguistics, 46-53,Tokyo, October, 1980.Thompson, B.H., "Linguistic Analysis of Natural Language Com-munication with Computers," COLING 80: Proceedings of theEighth International Conference on Computational Linguistics,190-201, Tokyo, October, 1980.Weischedel, R.M., and J.
Black, "Responding Intelligently to Un-parsable Inputs," American Journal of Computational Linguistics,6, 2, 97-109, 1980.Wilks, Y., "Natural Language Understanding Systems Within theA.I.
Paradigm: A Survey," American Journal of ComputationalLinguistics, microfiche 40, 1, 1976.Woods, W.A., "An Experimental Parsing System for TransitionNetwork Grammars," in Natural Language Processing, R.
Rustin(Ed.
), 111-154, Algorithmics Press, New York, 1973.Stan C. Kwasny is an Assistant Professor in theComputer Science Department at lndiana University.He received the Ph.D degree in Computer and Informa-tion Science f rom Ohio State University in 1980.Norman K. Sondheimer is a Senior Computer Scien-tist in the Software Research Department of  Sperry Uni-vac.
He received the Ph.D degree in Computer Sciencef rom the University o f  Wisconsin-Madison i 1975.108 American Journal of Computational Linguistics, Volume 7, Number 2, April-June 1981
