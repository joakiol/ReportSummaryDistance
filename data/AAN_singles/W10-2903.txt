Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 18?27,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsDriving Semantic Parsing from the World?s ResponseJames Clarke Dan Goldwasser Ming-Wei Chang Dan RothDepartment of Computer ScienceUniversity of IllinoisUrbana, IL 61820{clarkeje,goldwas1,mchang21,danr}@illinois.eduAbstractCurrent approaches to semantic parsing,the task of converting text to a formalmeaning representation, rely on annotatedtraining data mapping sentences to logi-cal forms.
Providing this supervision isa major bottleneck in scaling semanticparsers.
This paper presents a new learn-ing paradigm aimed at alleviating the su-pervision burden.
We develop two novellearning algorithms capable of predictingcomplex structures which only rely on abinary feedback signal based on the con-text of an external world.
In addition wereformulate the semantic parsing problemto reduce the dependency of the model onsyntactic patterns, thus allowing our parserto scale better using less supervision.
Ourresults surprisingly show that without us-ing any annotated meaning representationslearning with a weak feedback signal is ca-pable of producing a parser that is compet-itive with fully supervised parsers.1 IntroductionSemantic Parsing, the process of converting textinto a formal meaning representation (MR), is oneof the key challenges in natural language process-ing.
Unlike shallow approaches for semantic in-terpretation (e.g., semantic role labeling and in-formation extraction) which often result in an in-complete or ambiguous interpretation of the natu-ral language (NL) input, the output of a semanticparser is a complete meaning representation thatcan be executed directly by a computer program.Semantic parsing has mainly been studied in thecontext of providing natural language interfacesto computer systems.
In these settings the targetmeaning representation is defined by the seman-tics of the underlying task.
For example, provid-ing access to databases: a question posed in nat-ural language is converted into a formal databasequery that can be executed to retrieve information.Example 1 shows a NL input query and its corre-sponding meaning representation.Example 1 Geoquery input text and output MR?What is the largest state that borders Texas?
?largest(state(next to(const(texas))))Previous works (Zelle and Mooney, 1996; Tangand Mooney, 2001; Zettlemoyer and Collins,2005; Ge and Mooney, 2005; Zettlemoyer andCollins, 2007; Wong and Mooney, 2007) employmachine learning techniques to construct a seman-tic parser.
The learning algorithm is given a set ofinput sentences and their corresponding meaningrepresentations, and learns a statistical semanticparser ?
a set of rules mapping lexical items andsyntactic patterns to their meaning representationand a score associated with each rule.
Given a sen-tence, these rules are applied recursively to derivethe most probable meaning representation.
Sincesemantic interpretation is limited to syntactic pat-terns identified in the training data, the learningalgorithm requires considerable amounts of anno-tated data to account for the syntactic variationsassociated with the meaning representation.
An-notating sentences with their MR is a difficult,time consuming task; minimizing the supervisioneffort required for learning is a major challenge inscaling semantic parsers.This paper proposes a new model and learningparadigm for semantic parsing aimed to alleviatethe supervision bottleneck.
Following the obser-vation that the target meaning representation is tobe executed by a computer program which in turnprovides a response or outcome; we propose a re-sponse driven learning framework capable of ex-ploiting feedback based on the response.
The feed-back can be viewed as a teacher judging whetherthe execution of the meaning representation pro-duced the desired response for the input sentence.18This type of supervision is very natural in manysituations and requires no expertise, thus can besupplied by any user.Continuing with Example 1, the response gen-erated by executing a database query would beused to provide feedback.
The feedback would bewhether the generated response is the correct an-swer for the input question or not, in this case NewMexico is the desired response.In response driven semantic parsing, the learneris provided with a set of natural language sen-tences and a feedback function that encapsulatesthe teacher.
The feedback function informs thelearner whether its interpretation of the input sen-tence produces the desired response.
We considerscenarios where the feedback is provided as a bi-nary signal, correct +1 or incorrect ?1.This weaker form of supervision poses a chal-lenge to conventional learning methods: semanticparsing is in essence a structured prediction prob-lem requiring supervision for a set of interdepen-dent decisions, while the provided supervision isbinary, indicating the correctness of a generatedmeaning representation.
To bridge this differencewe propose two novel learning algorithms suitedto the response driven setting.Furthermore, to account for the many syntac-tic variations associated with the MR, we proposea new model for semantic parsing that allows usto learn effectively and generalize better.
Cur-rent semantic parsing approaches extract parsingrules mapping NL to their MR, restricting pos-sible interpretations to previously seen syntacticpatterns.
We replace the rigid inference processinduced by the learned parsing rules with a flex-ible framework.
We model semantic interpreta-tion as a sequence of interdependent decisions,mapping text spans to predicates and use syntac-tic information to determine how the meaning ofthese logical fragments should be composed.
Weframe this process as an Integer Linear Program-ming (ILP) problem, a powerful and flexible in-ference framework that allows us to inject rele-vant domain knowledge into the inference process,such as specific domain semantics that restrict thespace of possible interpretations.We evaluate our learning approach and modelon the well studied Geoquery domain (Zelle andMooney, 1996; Tang and Mooney, 2001), adatabase consisting of U.S. geographical informa-tion, and natural language questions.
Our experi-mental results show that our model with responsedriven learning can outperform existing modelstrained with annotated logical forms.The key contributions of this paper are:Response driven learning for semantic parsingWe propose a new learning paradigm for learn-ing semantic parsers without any annotated mean-ing representations.
The supervision for learningcomes from a binary feedback signal based a re-sponse generated by executing a meaning repre-sentation.
This type of supervision signal is nat-ural to produce and can be acquired from non-expert users.Novel training algorithms Two novel train-ing algorithms are developed within the responsedriven learning paradigm.
The training algorithmsare applicable beyond semantic parsing and can beused in situations where it is possible to obtain bi-nary feedback for a structured learning problem.Flexible semantic interpretation process Wepropose a novel flexible semantic parsing modelthat can handle previously unseen syntactic varia-tions of the meaning representation.2 Semantic ParsingThe goal of semantic parsing is to produce a func-tion F : X ?
Z that maps from the space naturallanguage input sentences, X , to the space of mean-ing representations, Z .
This type of task is usu-ally cast as a structured output prediction problem,where the goal is to obtain a model that assigns thehighest score to the correct meaning representa-tion given an input sentence.
However, in the taskof semantic parsing, this decision relies on identi-fying a hidden intermediate representation (or analignment) that captures the way in which frag-ments of the text correspond to the meaning repre-sentation.
Therefore, we formulate the predictionfunction as follows:z?
= Fw(x) = argmaxy?Y ,z?ZwT?
(x,y, z) (1)Where ?
is a feature function that describes therelationships between an input sentence x, align-ment y and meaning representation z. w is theweight vector which contains the parameters of themodel.
We refer to the argmax above as the in-ference problem.
The feature function combinedwith the nature of the inference problem definesthe semantic parsing model.
The key to producing19What is the largest Texas?largest( const(texas))))New Mexicox:y:z:r:that bordersstatestate( next_to(Figure 1: Example input sentence, meaning repre-sentation, alignment and answer for the Geoquerydomaina semantic parser involves defining a model and alearning algorithm to obtain w.In order to exemplify these concepts we con-sider the Geoquery domain.
Geoquery contains aquery language for a database of U.S. geograph-ical facts.
Figure 1 illustrates concrete examplesof the terminology introduce.
The input sentencesx are natural language queries about U.S. geog-raphy.
The meaning representations z are logicalforms which can be executed on the database toobtain a response which we denote with r. Thealignment y captures the associations between xand z.Building a semantic parser involves defining themodel (feature function ?
and inference problem)and a learning strategy to obtain weights (w) as-sociated with the model.
We defer discussion ofour model until Section 4 and first focus on ourlearning strategy.3 Structured Learning with BinaryFeedbackPrevious approaches to semantic parsing haveassumed a fully supervised setting wherea training set is available consisting of ei-ther: input sentences and logical forms{(xl, zl)}Nl=1 (e.g., (Zettlemoyer and Collins,2005)) or input sentences, logical formsand a mapping between their constituents{(xl,yl, zl)}Nl=1 (e.g., (Ge and Mooney, 2005)).Given such training examples a weight vector wcan be learned using structured learning methods.Obtaining, through annotation or other means, thisform of training data is an expensive and difficultprocess which presents a major bottleneck forsemantic parsing.To reduce the burden of annotation we focuson a new learning paradigm which uses feedbackfrom a teacher.
The feedback signal is binary(+1,?1) and informs the learner whether a pre-dicted logical form z?
when executed on the targetAlgorithm 1 Direct Approach (Binary Learning)Input: Sentences {xl}Nl=1,Feedback : X ?Z ?
{+1, 1},initial weight vector w1: Bl ?
{} for all l = 1, .
.
.
, N2: repeat3: for l = 1, .
.
.
, N do4: y?, z?
= argmaxy,z wT?
(xl,y, z)5: f = Feedback (xl, z?
)6: add (?
(xl, y?, z?
)/|xl|, f) to Bl7: end for8: w?
BinaryLearn(B) where B = ?lBl9: until no Bl has new unique examples10: return wdomain produces the desired response or outcome.This is a very natural method for providing super-vision in many situations and requires no exper-tise.
For example, a user can observe the responseand provide a judgement.
The general form ofthe teacher?s feedback is provided by a functionFeedback : X ?
Z ?
{+1,?1}.For the Geoquery domain this amounts towhether the logical form produces the correct re-sponse r for the input sentence.
Geoquery has theadded benefit that the teacher can be automatedif we have a dataset consisting of input sentencesand response pairs {(xl, rl)}Nl=1.
Feedback eval-uates whether a logical form produces a responsematching r:Feedback (xl, z) ={+1 if execute(z) = rl?1 otherwiseWe are now ready to present our learningwith feedback algorithms that operate in situationswhere input sentences, {xl}Nl=1, and a teacherfeedback mechanism, Feedback , are available.
Wedo not assume the availability of logical forms.3.1 Direct Approach (Binary Learning)In general, a weight vector can be consideredgood if when used in the inference problem (Equa-tion (1)) it scores the correct logical form andalignment (which may be hidden) higher than allother logical forms and alignments for a given in-put sentence.
The intuition behind the direct ap-proach is that the feedback function can be used tosubsample the space of possible structures (align-ments and logical forms (Y ?
Z)) for a given in-put x.
The feedback mechanism indicates whetherthe structure is good (+1) or bad (?1).
Using this20intuition we can cast the problem of learning aweight vector for Equation (1) as a binary classifi-cation problem where we directly consider struc-tures the feedback assigns +1 as positive examplesand those assigned ?1 as negative.We represent the input to the binary classifieras the feature vector ?
(x,y, z) normalized by thesize of the input sentence1 |x|, and the label as theresult from Feedback (x, z).Algorithm 1 outlines the approach in detail.
Thefirst stage of the algorithm iterates over all thetraining input sentences and computes the bestlogical form z?
and alignment y?
by solving the in-ference problem (line 4).
The feedback functionis queried (line 5) and a training example for thebinary predictor created using the normalized fea-ture vector from the triple containing the sentence,alignment and logical form as input and the feed-back as the label.
This training example is addedto the working set of training examples for this in-put sentence (line 6).
All the feedback training ex-amples are used to train a binary classifier whoseweight vector is used in the next iteration (line 8).The algorithm repeats until no new unique trainingexamples are added to any of the working sets forany input sentence.
Although the number of possi-ble training examples is very large, in practice thealgorithm is efficient and converges quickly.
Notethat this approach is capable of using a wide va-riety of linear classifiers as the base learner (line8).A policy is required to specify the nature ofthe working set of training examples (Bl) used fortraining the base classifier.
This is pertinent in line6 of the algorithm.
Possible policies include: al-lowing duplicates in the working set (i.e., Bl isa multiset), disallowing duplicates (Bl is a set),or only allowing one example per input sentence(?Bl?
= 1).
We adopt the first approach in thispaper.23.2 Aggressive Approach (StructuredLearning)There is important implicit information whichthe direct approach ignores.
It is implicit thatwhen the teacher indicates an input paired withan alignment and logical form is good (+1 feed-1Normalization is required to ensure that each sentencecontributes equally to the binary learning problem regardlessof the sentence?s length.2The working set Bl for each sentence may contain multi-ple positive examples with the same and differing alignments.Algorithm 2 Aggressive Approach (StructuredLearning)Input: Sentences {xl}Nl=1,Feedback : X ?Z ?
{+1, 1},initial weight vector w1: Sl ?
?
for all l = 1, .
.
.
, N2: repeat3: for l = 1, .
.
.
, N do4: y?, z?
= argmaxy,z wT?
(xl,y, z)5: f = Feedback (xl, z?
)6: if f is +1 then7: Sl ?
{(xl, y?, z?
)}8: end if9: end for10: w?
StructLearn(S,?)
where S = ?lSl11: until no Sl has changed12: return wback) that in order to repeat this behavior all othercompeting structures should be made suboptimal(or bad).
To leverage this implicit informationwe adopt a structured learning strategy in whichwe consider the prediction as the optimal structureand all others as suboptimal.
This is in contrast tothe direct approach where only structures that haveexplicitly received negative feedback are consid-ered subopitmal.When a structure is found with positive feed-back it is added to the training pool for a struc-tured learner.
We consider this approach aggres-sive as the structured learner implicitly considersall other structures as being suboptimal.
Negativefeedback indicates that the structure should not beadded to the training pool as it will introduce noiseinto the learning process.Algorithm 2 outlines the learning in more detail.As before, y?
and z?
are predicted using the cur-rent weight vector and feedback received (lines 4and 5).
When positive feedback is received a newtraining instance for a structured learner is createdfrom the input sentence and prediction (line 7) thistraining instance replaces any previous instancefor the input sentence.
When negative feedbackis received the training pool Sl is not updated.
Aweight vector is learned using a structured learnerwhere the training data S contains at most one ex-ample per input sentence.
In the first iteration ofthe outer loop the training data S will contain veryfew examples.
In each subsequent iteration thenewly learned weight vector allows the algorithmto acquire new examples.
This is repeated until no21new examples are added or changed in S.Like the direct approach, this learning frame-work is makes very few assumptions about thetype of structured learner used as a base learner(line 10).34 ModelSemantic parsing is the process of converting anatural language input into a formal logic repre-sentation.
This process is performed by associat-ing lexical items and syntactic patterns with logi-cal fragments and composing them into a completeformula.
Existing approaches rely on extractinga set of parsing rules, mapping text constituentsto a logical representation, from annotated train-ing data and applying them recursively to obtainthe meaning representation.
Adapting to new datais a major limitation of these approaches as theycannot handle inputs containing syntactic patternswhich were not observed in the training data.
Forexample, assume the training data produced thefollowing set of parsing rules:Example 2 Typical parsing rules(1) NP [?x.capital(x)]?
capital(2) PP [ const(texas)]?
of Texas(3) NNP [ const(texas)]?
Texas(4) NP [capital(const(texas))]?NP[?x.capital(x)] PP [ const(texas)]At test time the parser is given the sentences inExample 3.
Despite the lexical similarity in theseexamples, the semantic parser will correctly parsethe first sentence but fail to parse the second be-cause the lexical items belong to different a syn-tactic category (i.e., the word Texas is not part of apreposition phrase in the second sentence).Example 3 Syntactic variations of the same MRTarget logical form: capital(const(texas))Sentence 1: ?What is the capital of Texas?
?Sentence 2: ?What is Texas?
capital?
?The ability to adapt to unseen inputs is oneof the key challenges in semantic parsing.
Sev-eral works (Zettlemoyer and Collins, 2007; Kate,2008) have addressed this issue explicitly by man-ually defining syntactic transformation rules thatcan help the learned parser generalize better.
Un-fortunately these are only partial solutions as a3Mistake driven algorithms that do not enforce marginconstraints may not be able to generalize using this proto-col since they will repeat the same prediction at training timeand therefore will not update the model.manually constructed rule set cannot cover themany syntactic variations.Given the previous example, we observethat it is enough to identify that the functioncapital(?)
and the constant const(texas)appear in the target MR, since there is only a singleway to compose these entities into a single formula?
capital(const(texas)).Motivated by this observation we define ourmeaning derivation process over the rules of theMR language and use syntactic information as away to bias the MR construction process.
Thatis, our inference process considers the entire spaceof meaning representations irrespective of the pat-terns observed in the training data.
This is possi-ble as the MRs are defined by a formal languageand formal grammar.4 The syntactic informationpresent in the natural language is used as soft ev-idence (features) which guides the inference pro-cess to good meaning representations.This formulation is a major shift from existingapproaches that rely on extracting parsing rulesfrom the training data.
In existing approachesthe space of possible meaning representations isconstrained by the patterns in the training dataand syntactic structure of the natural language in-put.
Our formulation considers the entire space ofmeaning representations and allows the model toadapt to previously unseen data and always pro-duce a semantic interpretation by using the pat-terns observed in the input.We frame our semantic interpretation processas a constrained optimization process, maximiz-ing the objective function defined by Equation 1which relies on extracting lexical and syntacticfeatures instead of parsing rules.
In the remain-der of this section we explain the components ofour inference model.4.1 Target Meaning RepresentationFollowing previous work, we capture the se-mantics of the Geoquery domain using a sub-set of first-order logic consisting of typed con-stants and functions.
There are two types: en-tities E in the domain and numeric values N .Functions describe a functional relationship overtypes (e.g., population : E ?
N ).
A com-plete logical form is constructed through func-tional composition; in our formalism this is per-4This is true for all meaning representations designed tobe executed by a computer system.22formed by the substitution operator.
For ex-ample, given the function next to(x) andthe expression const(texas), substitution re-places the occurrence of the free variable x, withthe expression, resulting in a new logical form:next to(const(texas)).
Due to space lim-itations we refer the reader to (Zelle and Mooney,1996) for a detailed description of the Geoquerydomain.4.2 Semantic Parsing as ConstrainedOptimizationRecall that the goal of semantic parsing is to pro-duce the following function (Equation (1)):Fw(x) = argmaxy,zwT?
(x,y, z)However, given that y and z are complex struc-tures it is necessary to decompose the structureinto a set of smaller decisions to facilitate efficientinference.In order to define our decomposition we intro-duce additional notation: c is a constituent (orword span) in the input sentence x and D is theset of all function and constant symbols in the do-main.
The alignment y is defined as a set of map-pings between constituents and symbols in the do-main y = {(c, s)} where s ?
D.We decompose the construction of an alignmentand logical form into two types of decisions:First-order decisions.
A mapping between con-stituents and logical symbols (functions and con-stants).Second-order decisions.
Expressing how logi-cal symbols are composed into a complete logicalinterpretation.
For example, whether next toand state forms next to(state(?))
orstate(next to(?
)).Note that for all possible logical forms andalignments there exists a one-to-one mapping tothese decisions.We frame the inference problem as an IntegerLinear Programming (ILP) problem (Equation (2))in which the first-order decisions are governed by?cs, a binary decision variable indicating that con-stituent c is aligned with logical symbol s. And?cs,dt capture the second-order decisions indicat-ing the symbol t (associated with constituent d)is an argument to function s (associated with con-stituent c).Fw(x) = argmax?,?
?c?x?s?D?cs ?wT?1(x, c, s)+?c,d?x?s,t?D?cs,dt ?wT?2(x, c, s, d, t) (2)It is clear that there are dependencies betweenthe ?-variables and ?-variables.
For example,given that ?cs,dt is active, the corresponding ?-variables ?cs and ?dt must also be active.
In orderto ensure a consistent solution we introduce a setof constraints on Equation (2).
In addition we addconstraints which leverage the typing informationinherent in the domain to eliminate logical formsthat are invalid in the Geoquery domain.
For ex-ample, the function length only accepts rivertypes as input.
The set of constraints are:?
A given constituent can be associated withexactly one logical symbol.?
?cs,dt is active if and only if ?cs and ?dt areactive.?
If ?cs,dt is active, s must be a function andthe types of s and t should be consistent.?
Functional composition is directional andacyclic.The flexibility of ILP has previously been advan-tageous in natural language processing tasks (Rothand Yih, 2007) as it allows us to easily incorporatesuch constraints.4.3 FeaturesThe inference problem defined in Equation (2)uses two feature functions: ?1 and ?2.First-order decision features ?1 Determiningif a logical symbol is aligned with a specific con-stituent depends mostly on lexical information.Following previous work (e.g., (Zettlemoyer andCollins, 2005)) we create a small lexicon, mappinglogical symbols to surface forms.5 This lexicon issmall and only used as a starting point.
Existingapproaches rely on annotated logical forms to ex-tend the lexicon.
However, in our setting we donot have access to annotated logical forms, insteadwe rely on external knowledge to supply further5The lexicon contains on average 1.42 words per func-tion and 1.07 words per constant.
For example the functionnext to has the lexical entries: borders, next, adjacent andthe constant illinois the lexical item illinois.23information.
We add features which measure thelexical similarity between a constituent and a logi-cal symbol?s surface forms (as defined by the lexi-con).
Two metrics are used: stemmed word matchand a similarity metric based on WordNet (Milleret al, 1990) which allows our model to accountfor words not in the lexicon.
The WordNet met-ric measures similarity based on synonymy, hy-ponymy and meronymy (Do et al, 2010).
In thecase where the constituent is a preposition, whichare notorious for being ambiguous, we add a fea-ture that considers the current lexical context (oneword to the left and right) in addition to word sim-ilarity.Second-order decision features ?2 Determin-ing how to compose two logical symbols relies onsyntactic information, in our model we use the de-pendency tree (Klein and Manning, 2003) of theinput sentence.
Given a second-order decision?cs,dt, the dependency feature takes the normal-ized distance between the head words in the con-stituents c and d. A set of features also indicatewhich logical symbols are usually composed to-gether, without considering their alignment to text.5 ExperimentsIn this section we describe our experimental setup,which includes the details of the domain, re-sources and parameters.5.1 Domain and CorpusWe evaluate our system on the Geoquery domainas described previously.
The domain consists ofa database and Prolog query language for U.S.geographical facts.
The corpus contains of 880natural language queries paired with Prolog log-ical form queries ((x, z) pairs).
We follow previ-ous approaches and transform these queries into afunctional representation.
We randomly select 250sentences for training and 250 sentences for test-ing.6 We refer to the training set as Response 250(R250) indicating that each example x in this dataset has a corresponding desired database responser.
We refer the testing set as Query 250 (Q250)where the examples only contain the natural lan-guage queries.6Our inference problem is less constrained than previousapproaches thus we limit the training data to 250 examplesdue to scalability issues.
We also prune the search space bylimiting the number of logical symbol candidates per word(on average 13 logical symbols per word).Precision and recall are typically used as eval-uation metrics in semantic parsing.
However, asour model inherently has the ability to map anyinput sentence into the space of meaning repre-sentations the trade off between precision and re-call does not exist.
Thus, we report accuracy: thepercentage of meaning representations which pro-duce the correct response.
This is equivalent torecall in previous work (Wong and Mooney, 2007;Zettlemoyer and Collins, 2005; Zettlemoyer andCollins, 2007).5.2 Resources and ParametersFeedback Recall that our learning frameworkdoes not require meaning representation annota-tions.
However, we do require a Feedback func-tion that informs the learner whether a predictedmeaning representation when executed producesthe desired response for a given input sentence.We automatically generate a set of natural lan-guage queries and response pairs {(x, r)} by exe-cuting the annotated logical forms on the database.Using this data we construct an automatic feed-back function as described in Section 3.Domain knowledge Our learning approachesrequire an initial weight vector as input.
In or-der to provide an initial starting point, we initializethe weight vector using a similar procedure to theone used in (Zettlemoyer and Collins, 2007) to setweights for three features and a bias term.
Theweights were developed on the training set usingthe feedback function to guide our choices.Underlying Learning Algorithms In the directapproach the base linear classifier we use is a lin-ear kernel Support Vector Machine with squared-hinge loss.
In the aggressive approach we de-fine our base structured learner to be a structuralSupport Vector Machine with squared-hinge lossand use hamming distance as the distance func-tion.
We use a custom implementation to op-timize the objective function using the Cutting-Plane method, this allows us to parrallelize thelearning process by solving the inference problemfor multiple training examples simultaneously.6 ResultsOur experiments are designed to answer threequestions:1.
Is it possible to learn a semantic parser with-out annotated logical forms?24Algorithm R250 Q250NOLEARN 22.2 ?DIRECT 75.2 69.2AGGRESSIVE 82.4 73.2SUPERVISED 87.6 80.4Table 1: Accuracy of learned models on R250 data andQ250 (testing) data.
NOLEARN: using initialized weightvector, DIRECT: using feedback with the direct approach,AGGRESSIVE: using feedback with the aggressive approach,SUPERVISED: using gold 250 logical forms for training.Note that none of the approaches use any annotated logicalforms besides the SUPERVISED approach.Algorithm # LF AccuracyAGGRESSIVE ?
73.2SUPERVISED 250 80.4W&M 2006 ?
310 ?
60.0W&M 2007 ?
310 ?
75.0Z&C 2005 600 79.29Z&C 2007 600 86.07W&M 2007 800 86.59Table 2: Comparison against previously published results.Results show that with a similar number of logical forms(# LF) for training our SUPERVISED approach outperformsexisting systems, while the AGGRESSIVE approach remainscompetitive without using any logical forms.2.
How much performance do we sacrifice bynot restricting our model to parsing rules?3.
What, if any, are the differences in behaviourbetween the two learning with feedback ap-proaches?We first compare how well our model performsunder four different learning regimes.
NOLEARNuses a manually initialized weight vector.
DIRECTand AGGRESSIVE use the two response drivenlearning approaches, where a feedback functionbut no logical forms are provided.
As an up-per bound we train the model using a fully SU-PERVISED approach where the input sentences arepaired with hand annotated logical forms.Table 1 shows the accuracy of each setup.
Themodel without learning (NOLEARN) gives a start-ing point with an accuracy of 22.2%.
The re-sponse driven learning methods perform substan-tially better than the starting point.
The DIRECTapproach which uses a binary learner reaches anaccuracy of 75.2% on the R250 data and 69.2% onthe Q250 (testing) data.
While the AGGRESSIVEapproach which uses a structured learner sees abigger improvement, reaching 82.4% and 73.2%respectively.
This is only 7% below the fully SU-PERVISED upper bound of the model.To answer the second question, we compare asupervised version of our model to existing se-mantic parsers.
The results are in Table 2.
Al-though the numbers are not directly comparabledue to different splits in the data7, we can see thatwith a similar number of logical forms for train-ing our SUPERVISED approach outperforms ex-isting systems (Wong and Mooney, 2006; Wongand Mooney, 2007), while the AGGRESSIVE ap-proach remains competitive without using any log-ical forms.
Our SUPERVISED model is still verycompetitive with other approaches (Zettlemoyerand Collins, 2007; Wong and Mooney, 2007),which used considerably more annotated logicalforms in the training phase.In order to answer the third question, we turnour attention to the differences between the tworesponse driven learning approaches.
The DIRECTand AGGRESSIVE approaches use binary feedbackto learn, however they utilize the signal differently.DIRECT uses the signal directly to learn a bi-nary classifier capable of replicating the feedback,whereas AGGRESSIVE learns a structured predic-tor that can repeatedly obtain the logical formsfor which positive feedback was received.
Thus,although the AGGRESSIVE outperforms the DI-RECT approach the concepts each approach learnsmay be different.
Analysis over the training datashows that in 66.8% examples both approachespredict a logical form that gives the correct an-swer.
While AGGRESSIVE correctly answers anadditional 16% which DIRECT gets incorrect.
Inthe opposite direction, DIRECT correctly answers8.8% that AGGRESSIVE does not.
Leaving only8.4% of the examples that both approaches pre-dict incorrect logical forms.
This suggests that anapproach which combines DIRECT and AGGRES-SIVE may be able to improve even further.Figure 2 shows the accuracy on the entire train-ing data (R250) at each iteration of learning.
Wesee that the AGGRESSIVE approach learns to covermore of the training data and at a faster rate thanDIRECT.
Note that the performance of the DI-RECT approach drops at the first iteration.
We hy-pothesize this is due to imbalances in the binaryfeedback dataset (too many negative examples) inthe first iteration.7It is relatively difficult to compare different approachesin the Geoquery domain given that many existing papers donot use the same data split.2570 1 2 3 4 5 69001020304050607080Learning IterationsAccuracyonResponse250Direct ApproachAggressive ApproachInitializationFigure 2: Accuracy on training set as number of learningiterations increases.7 Related WorkLearning to map sentences to a meaning repre-sentation has been studied extensively in the NLPcommunity.
Early works (Zelle and Mooney,1996; Tang and Mooney, 2000) employed induc-tive logic programming approaches to learn a se-mantic parser.
More recent works apply statisti-cal learning methods to the problem.
In (Ge andMooney, 2005; Nguyen et al, 2006), the input tothe learner consists of complete syntactic deriva-tions for the input sentences annotated with logi-cal expressions.
Other works (Wong and Mooney,2006; Kate and Mooney, 2006; Zettlemoyer andCollins, 2005; Zettlemoyer and Collins, 2007;Zettlemoyer and Collins, 2009) try to alleviate theannotation effort by only taking sentence and log-ical form pairs to train the models.
Learning isthen defined over hidden patterns in the trainingdata that associate logical symbols with lexicaland syntactic elements.In this work we take an additional step to-wards alleviating the difficulty of training seman-tic parsers and present a world response basedtraining protocol.
Several recent works (Chen andMooney, 2008; Liang et al, 2009; Branavan etal., 2009) explore using an external world contextas a supervision signal for semantic interpretation.These works operate in settings different to ours asthey rely on an external world state that is directlyreferenced by the input text.
Although our frame-work can also be applied in these settings we donot assume that the text can be grounded in a worldstate.
In our experiments the input text consists ofgeneralized statements which describe some infor-mation need that does not correspond directly to agrounded world state.Our learning framework closely follows recentwork on learning from indirect supervision.
Thedirect approach resembles learning a binary clas-sifier over a latent structure (Chang et al, 2010a);while the aggressive approach has similarities withwork that uses labeled structures and a binarysignal indicating the existence of good structuresto improve structured prediction (Chang et al,2010b).8 ConclusionsIn this paper we tackle one of the key bottlenecksin semantic parsing ?
providing sufficient super-vision to train a semantic parser.
Our solution istwo fold, first we present a new training paradigmfor semantic parsing that relies on natural, hu-man level supervision.
Second, we suggest a newmodel for semantic interpretation that does notrely on NL syntactic parsing rules, but rather usesthe syntactic information to bias the interpretationprocess.
This approach allows the model to gener-alize better and reduce the required amount of su-pervision.
We demonstrate the effectiveness of ourtraining paradigm and interpretation model overthe Geoquery domain, and show that our modelcan outperform fully supervised systems.Acknowledgements We are grateful to Rohit Kate andRaymond Mooney for their help with the Geoquery dataset.Thanks to Yee Seng Chan, Nick Rizzolo, Shankar Vembuand the three anonymous reviewers for their insightful com-ments.
This material is based upon work supported by theAir Force Research Laboratory (AFRL) under prime contractno.
FA8750-09-C-0181 and by DARPA under the BootstrapLearning Program.
Any opinions, findings, and conclusion orrecommendations expressed in this material are those of theauthors and do not necessarily reflect the views of the AFRLor DARPA.ReferencesS.R.K.
Branavan, H. Chen, L. Zettlemoyer, andR.
Barzilay.
2009.
Reinforcement learning for map-ping instructions to actions.
In Proc.
of the AnnualMeeting of the Association for Computational Lin-guistics (ACL).M.
Chang, D. Goldwasser, D. Roth, and V. Srikumar.2010a.
Discriminative learning over constrained la-tent representations.
In Proc.
of the Annual Meetingof the North American Association of ComputationalLinguistics (NAACL).26M.
Chang, D. Goldwasser, D. Roth, and V. Srikumar.2010b.
Structured output learning with indirect su-pervision.
In Proc.
of the International Conferenceon Machine Learning (ICML).D.
Chen and R. Mooney.
2008.
Learning to sportscast:a test of grounded language acquisition.
In Proc.
ofthe International Conference on Machine Learning(ICML).Q.
Do, D. Roth, M. Sammons, Y. Tu, and V.G.
Vydis-waran.
2010.
Robust, Light-weight Approaches tocompute Lexical Similarity.
Computer Science Re-search and Technical Reports, University of Illinois.http://hdl.handle.net/2142/15462.R.
Ge and R. Mooney.
2005.
A statistical semanticparser that integrates syntax and semantics.
In Proc.of the Annual Conference on Computational NaturalLanguage Learning (CoNLL).R.
Kate and R. Mooney.
2006.
Using string-kernelsfor learning semantic parsers.
In Proc.
of the An-nual Meeting of the Association for ComputationalLinguistics (ACL).R.
Kate.
2008.
Transforming meaning representationgrammars to improve semantic parsing.
In Proc.
ofthe Annual Conference on Computational NaturalLanguage Learning (CoNLL).D.
Klein and C. D. Manning.
2003.
Fast exact in-ference with a factored model for natural languageparsing.
In Proc.
of the Conference on Advances inNeural Information Processing Systems (NIPS).P.
Liang, M. I. Jordan, and D. Klein.
2009.
Learningsemantic correspondences with less supervision.
InProc.
of the Annual Meeting of the Association forComputational Linguistics (ACL).G.
Miller, R. Beckwith, C. Fellbaum, D. Gross, andK.J.
Miller.
1990.
Wordnet: An on-line lexicaldatabase.
International Journal of Lexicography.L.
Nguyen, A. Shimazu, and X. Phan.
2006.
Semanticparsing with structured svm ensemble classificationmodels.
In Proc.
of the Annual Meeting of the Asso-ciation for Computational Linguistics (ACL).D.
Roth and W. Yih.
2007.
Global inference for entityand relation identification via a linear programmingformulation.
In Lise Getoor and Ben Taskar, editors,Introduction to Statistical Relational Learning.L.
Tang and R. Mooney.
2000.
Automated construc-tion of database interfaces: integrating statistical andrelational learning for semantic parsing.
In Proc.
ofthe Conference on Empirical Methods for NaturalLanguage Processing (EMNLP).L.
R. Tang and R. J. Mooney.
2001.
Using multipleclause constructors in inductive logic programmingfor semantic parsing.
In Proc.
of the European Con-ference on Machine Learning (ECML).Y.-W. Wong and R. Mooney.
2006.
Learning forsemantic parsing with statistical machine transla-tion.
In Proc.
of the Annual Meeting of the NorthAmerican Association of Computational Linguistics(NAACL).Y.-W. Wong and R. Mooney.
2007.
Learningsynchronous grammars for semantic parsing withlambda calculus.
In Proc.
of the Annual Meet-ing of the Association for Computational Linguistics(ACL).J.
M. Zelle and R. J. Mooney.
1996.
Learning to parsedatabase queries using inductive logic proramming.In Proc.
of the National Conference on Artificial In-telligence (AAAI).L.
Zettlemoyer and M. Collins.
2005.
Learning to mapsentences to logical form: Structured classificationwith probabilistic categorial grammars.
In Proc.
ofthe Annual Conference in Uncertainty in ArtificialIntelligence (UAI).L.
Zettlemoyer and M. Collins.
2007.
Online learn-ing of relaxed CCG grammars for parsing to logicalform.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Pro-cessing and on Computational Natural LanguageLearning (EMNLP-CoNLL).L.
Zettlemoyer and M. Collins.
2009.
Learningcontext-dependent mappings from sentences to log-ical form.
In Proc.
of the Annual Meeting of theAssociation for Computational Linguistics (ACL).27
