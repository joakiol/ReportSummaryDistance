Summarization-based Query Expansion in Information RetrievalTomel~ S~rzaIl~owsl~i, J n Wang, and Bowden WiseGE Corporate Research and Development1 Research CircleNiskayuna, NY 12309strzalkowski~crd.ge.comAbstractWe discuss a seml-interactive approach to infor-mation retrieval which consists of two tasks per-formed in a sequence.
First, the system assiststhe searcher in building a comprehensive statementof information eed, using automatically generatedtopical summaries of sample documents.
Second,the detailed statement of information eed is auto-matically processed by a series of natural anguageprocessing routines in order to derive an optimalsearch query for a statistical information retrievalsystem.
In this paper, we investigate he role of au-tomated document summarization in building effec-tive search statements.
We also discuss the resultsof latest evaluation of our system at the annual TextRetrieval Conference (TKEC).In fo rmat ion  Ret~r ieva lInformation retrieval (IR) is a task of selecting docu-ments from a database in response to a user's query,and ranking them according to relevance.
This hasbeen usually accomplished using statistical methods(often coupled with manual encoding) that (a) selectterms (words, phrases, and other units) from docu-ments that are deemed to best represent their con-tent, and (b) create an inverted index file (or files)that provide an easy access to documents containingthese terms.
A subsequent search process attemptsto match preprocessed user queries against term-based representations of documents in each case de-termining a degree of relevance between the twowhich depends upon the number and types of match-ing terms.A search is successful if it can return as manyas possible documents which are relevant to thequery, with as few as possible non-relevant docu-ments.
In addition, the relevant documents shouldbe ranked ahead of non-relevant ones.
The quanti-tative tex~ representation methods, predominant intoday's leading information retrieval systems 1 limitII~epresentations anchored on words, word or char-the system's ability to generate a successful searchbecause they rely more on the ,form of a query thanon its content in finding document matches.
Thisproblem is particularly acute in ad-hoc retrieval situ-ations where the user has only a limited knowledge ofdatabase composition and needs to resort to genericor otherwise incomplete search statements.
IrI or-der to overcome this limitation, marIy IR systemsallow varying degrees of user interaction that facil-itates query optimization and calibration to closermatch user's information seeking goals.
A populartechnique here is relevance feedback, where the useror the system judges the relevance of a sample of re-suits returned from an initial search, and the query issubsequently rebuilt to reflect his information.
Au-tomatic relevance feedback techniques can lead toa very close mapping of known relevant documents,however, they also tend to overflt, which in turn re-duces their ability of finding new documents on thesame subject.
Therefore, a serious challenge for in-formation retrieval is to devise methods for buildingbetter queries, or in assisting user to do so.Bu i ld ing  e f fec t ive  search  quer iesWe have been experimenting with manual and auto-matic natural language query (or topic, in TRECparlance) building techniques.
This differs frommost query modification techniques used in IR inthat our method is to reformulate the user's state~ment of information eed rather than the search sys-tem's internal representation f it, as relevance feed-back does.
Our goal is to devise a method of full-text expansion that would allow for creating exhaus-tive search topics such that: (1) the performanceof any system using the expanded topics would besignificantly better than when the system is run us-ing the original topics, and (2) the method of topicacter sequences, or some surrogates of these, along withsignificance weights derived from their distribution i  thedatabase.1258expansion could eventually be automated or semi-automated so as to be useful to a non-expert user.Note that the first of the above requirements effec-tively calls for a free text, unstructured, but highlyprecise and exhaustive description of user's searchstatement.
The preliminary results from TI~ECevaluations how that such an approach is indeedvery effective.One way to view query expansion is to make theuser query resemble more closely the documents it isexpected to retrieve.
This may include both content,as well as some other aspects uch as composition,style, language type, etc.
If the query is indeed madeto resemble a "typical" relevant document, hen sud-denly everything about this query becomes a validsearch criterion: words, collocations, phrases, var-ious relationships, etc.
Unfortunately, an averagesearch query does not look anything like this, mostof the time.
It is more likely to be a statement speci-fying the semantic riteria of relevance.
This meansthat except for the semantic or conceptual resem-blance (which we cannot model very well as yet)much of the appearance of the query (which we canmodel reasonably well) may be, and often is, quitemisleading for search purposes.
Where can we getthe right queries?In today's information retrieval, query expansionusually is typically limited to adding, deleting orre-weighting of terms.
For example, content ermsfrom documents judged relevant are added to thequery while weights of all terms are adjusted in or-der to reflect he relevance information.
Thus, termsoccurring predominantly in relevant documents willhave their weights increased, while those occurringmostly in non-relevant documents will have theirweights decreased.
This process can be performedautomatically using a relevance feedback method,e.g., (Rocchio 1971), with the relevance informa-tion either supplied manually by the user (Har-man 1988), or otherwise guessed, e.g.
by assum-ing top 10 documents relevant, etc.
(Buckley, etal.
1995).
A serious problem with this term-basedexpansion is its limited ability to capture and rep-resent many important aspects of what makes somedocuments relevant o the query, including particu-lar term co-occurrence patterns, and other hard-to-measure text features, such as discourse structure orstylistics.
Additionally, relevance-feedback expan-sion depends on ~he inherently partial relevance in-formation, which is normally unavailable, or unre-liable.
Other types of query expansions, includinggeneral purpose thesauri or lexical databases (e.g.,WordneQ have been found generally unsuccessful ininformation retrieval, (Voorhees 1994).An alternative to term-only expansion is a full-text expansion described in (Strzalkowski et al1997).
In this approach, search topics are expandedby pasting in entire sentences, paragraphs, and othersequences directly from any text document.
Tomake this process efficient, an initial search is per-formed with the unexpanded queries and the topN (10-30) returned documents are used for queryexpansion.
These documents, irrespective of theiroverall relevancy to the search topic, are scannedfor passages containing concepts referred to in thequery.
The resulting expanded queries undergo fur-ther text processing steps, before the search is runagain.
We need to note that the expansion ma-terial was found in both relevant and non-relevantdocuments, benefiting the final query all the same.In fact, the presence of such text in otherwise non-relevant documents underscores the inherent limRa-fions of distribution-based term reweighting used inrelevance feedback.In this paper, we describe a method of full-texttopic expansion where the expansion passages areobtained from an automatic text summarizer.
Apreliminary examination of Tt{EC-6 results indicatethat this mode of expansion is at least as effectiveas the purely manual expansion which requires theusers to read entire documents to select expansionpassages.
This brings us a step closer to a fully au-tomated expansion: the human-decision factor hasbeen reduced to an accept/reject decision for ex-panding the search query with a summary.Summar izat ion -6ased  query  expansionWe used our automatic text summarizer to de-rive query-specific summaries of documents returnedfrom the first round of retrieval.
The summarieswere usually 1 or 2 consecutive paragraphs selectedfrom the original document text.
The initial purposewas to show to the user, by the way of a quick-readabstract, why a document has been retrieved.
If thesummary appeared relevant and moreover capturedsome important aspect of relevant information, thenthe user had an option to paste it into the query,thus increasing the chances of a more successful sub-sequent search.
Note again that it wasn't importantif the summarized ocuments were themselves rele-vant, although they usually were.The query expansion interaction proceeds as fol-lows:1.
The initial natural anguage statement of informa-tion need is submitted to SMART-based NLIK re-trieval engine via a Query Expansion Tool (QET)interface.
The statement is converted into an in-1259ternal search query and run against he TRECdatabase.
22.
NEIR returns top N (=30) documents from thedatabase that match the search query.3.
The user determines a topic for the summarizer.By default, it is the title field of the initial searchstatement (see below).4.
The summarizer is invoked to automatically sum-marize each of the N documents with respect othe selected topic.5.
The user reviews the summaries (spending ap-prox.
5-15 seconds per summary) and de-selectsthese that are not relevant o the search state-ment.6.
All remaining summaries are automatically at-tached to the search statement.7.
The expanded search statement is passed througha series of natural anguage processing steps andthen submitted for the final retrieval.A partially expanded TREC Topic 304 is shownbelow.
The original topic comprises the first fourfields, with the Expanded field added through thequery expansion process.
The initial query, whilesomewhat lengthy by IR standards (though not byTREC standards) is still quite generic in form, thatis, it supplies few specifics to guide the search.
Incontrast, the Expanded section supplies not onlymany concrete xamples of relevant concepts (here,names of endangered mammals) but also the lan-guage and the style used by others to describe them.< ~op ><num > Number: 304< f~le  > Endangered Species (Mammals)< desc > Description:Compile a list of mammals  that  are considered to be endan-gered, identify their  hab i tat  and, if possible, specify whatthreatens them.<narr  > Narrative:Any document identifying a mammal  as endangered is rel-evant.
Statements  of author i t ies d isput ing the endangeredstatus  would also be relevant.
A document containing infor-mat ion on hab i tat  and populat ions of a mammal  identifiedelsewhere as endangered would also be relevant even if thedocument at hand did not identify the species as endan-gered.
Generalized statements  about  endangered specieswithout reference to specific mammals  would not be rele-vant.< expd > Expanded:~TFtEC-6 database consisted of approx.
2 GBytes ofdocuments from Associated Press newswire, Wall StreetJournal, Financial Times, Federal Keglster, FBIS andother sources (Haxman & Voorhees 1998).The Service is responsible \[or eight species ot" marine mam-mals under the jur isdict ion of the Department  of the Inte-rior, as assigned by the Marine Mammal  Protect ion Act of1972.
These species are polar bear, sea and marine otters,walrus, manatees (three species) and dugong.
The reportreviews the Service's marine mammal-re lated act iv i t ies dar-ing the report period.The U.S.
Fish and Wildl i fe Service had classified the pri-mate  as a "threatened" species, but officials said that  moreprotect ion was needed in view of recent studies document-ing a drast ic decline in the populat ions of wild chimps inAFrica.The Endangered Species Act was passed in 1973 and hasbeen used to provide protect ion to the bald eagle and grizzlybear, among other animals.Under the law, a designation ot" a threatened species meansit is l ikely to become ext inct  wi thout  protection, whereasext inct ion is viewed as a certainty for an endangeredspecies.The  bear on Cal i fornia 's  state flag should remind us oF whatwe have done to some or our species, I t  is a grizzly.
Andit is ext inct  in Cal i fornia and in most other states where itonce roamed.< /~op >In the next section we describe the summarizationprocess in detail.Robust  text  summar izat ionPerhaps the most difficult problem in designing anautomatic text summarization is to define what asummary is, and how to tell a summary from a non-summary, or a good summary from a bad one.
Theanswer depends in part upon who the summary isintended for, and in part upon what it is meant oachieve, which in large measure precludes any ob-jective evaluation.
For most of us, a summary is abrief synopsis of the content of a larger document, anabstract recounting the main points while suppress-ing most details.
One purpose of having a summaryis to quickly learn some facts, and decide what youwant to do with the entire story.
Therefore, one im-portant evaluation criterion is the tradeoff betweenthe degree of compression afforded by the summary,which may result in a decreased accuracy of infor-mation, and the time required to review that infor-mation.
This interpretations is particularly useful,though it isn't the only one acceptable, in summariz-ing news and other report-like documents.
It is alsowell suited for evaluating the usefulness of summa-rization in context of an information retrieval sys-tem, where the user needs to rapidly and efficientlyreview the documents returned from search for anindication of relevance and, possibly, to see whichaspect of relevance is present.Our early inspiration, and a benchmark, havebeen the Quick Read Summaries, posted daily offthe front page of New York Times on-line edition(htip://www.nytimes.com).
These summaries, pro-duced manually by NYT staff, are assembled out of1260passages, sentences, and sometimes entence frag-ments taken from the main article with very few,if any, editorial adjustmergs.
The effect is a col-lection of perfectly coherent idbits of news: thewho, the what, and when, but perhaps not why.This kind of summarization, where appropriate pas-sages are extracted from the original text, is veryefficient, and arguably ei~ective, because it doesn'trequire generation of any new text, and thus low-ers the risk of misinterpretation.
It is also relativelyeasier to automate, because we only need to iden-tify the suitable passages among the other text, atask that can be accomplished via shallow NEP andstatistical techniques.
3It has been noted, eg., (Rino & Scott 1994),(Weissberg & Buker 1990), that certain types oftex~s, such as news articles, technical reports, re-search papers, etc., conform to a set of style and or-ganization constraints, called the Discourse MacroStructure (DMS) which help the author to achievea desired communication effect.
News reports, forexample, tend to be built hierarchically out of com-ponents which fall roughly into one of the two cate-gories: the what's-the-news category, and the op-tional background category.
The background, ifpresent, supplies the context necessary to under-stand the central story, or to make a follow up storyself-contained.
This organization is oiSen reflectedin the summary, as illustrated in the example belowfrom NYT 10/15/97, where the highlighted portionprovides the background for the main news:Spies Just Wouldn't Come In From Cold War, Files ShowTerry  Squillaco~e was a Pentagon lawyer who haled herjob.
Kur t  Stand was a union leader wi~h an aging beat-nik's slouch.
J im Clark was a lonely pr ivate investigator.\[A 200-page affidavit filed last week by\] the Federal Bureauof Investigation says the three were out-oF-work spies \[orEast Germany.
And alter that state withered away, it says,they desperately reached out for anyone who might wantthem as secret agents.In this example, the two passages are non-consecutive paragraphs in the original text; thestring in the square brackets at the opening of thesecond passage has been omitted in the summary.Here the human summarizer's actions appear rela-tively straightforward, and it would not be difficultto propose an algorithmic method to do the same.This may go as follows:1.
Choose a DMS template for the summary; e.g.,Background+News.3This approach is contrasted wlth a far more difl~-cult method of summarizing text "in your own words.
"Computational attempts at such discourse-level andknowledge-level summarization i clude (Ono, Sumita &Miike 1994), (McKeown & tIadev 1995), (DeJong 1982),and (I\]ehnert 1981).2.
Select appropriate passages from the original textand fill the DMS template.3.
Assemble the summary in the desired order; deleteextraneous words.We have used this method to build our auto-mated summarizer.
We overcome the shortcom-ings of sentence-based summarization by working onparagraph level instead.
4 The summarizer has beenapplied to a variety of documents, including Asso-ciated Press newswires, articles from the New YorkTimes, Wall Street Journal, Financial Times, SanJose Mercury, as well as documents from the FederalRegister, and Congressional Record.
The programis domain independent, and it can be easily adaptedto most European languages.
It is also very robust:we used it to derive summaries of thousands of doc-uments returned by an information retrieval system.It can work in two modes: generic and topical.
Inthe generic mode, it captures the main topic of adocument; in the topical mode, it takes a user sup-plied statement of interest and derives a summaryrelated to this topic.
The topical summary is usu-ally different han the generic summary of ihe samedocument.Deriving automatic summariesEach component of a summary DMS needs to be in-stantiated by one or more passages extracted fromthe original text.
Initially, all eligible passages (i.e.,explicitly delineated paragraphs) within a documentare potential candidates for the summary.
As wemove through text, paragraphs are scored for theirsummary-worthiness.
The final score for each pas-sage, normalized for its length, is a weighted sumof a number of minor scores, using the followingformula: 51score(paragraph) = -\[ ?
E w~ ?
S~ (1)hwhere Sa is a minor score calculated using metric h;wh is the weight reflecting how effective this metricis in general; l is the length of the segment.The following metrics are used to score passagesconsidered for the main news section of the summaryDMS.
We list here only the criteria which are the4Kefer to (Euhn 1958) (Paice 1990) (l~u, Brandow& Mitze 1994) (Kupiec, Pedersen & Chen 1995) forsentence-based summarization approaches.SThe weights w~ are trainable in a supervised mode,given a corpus of texts and their summaries, or in an un-supervised mode as described in (Strzalkowski & Wang1996).
For the purpose of the experiments describedhere, these weights have been set manually.1261most relevant for generating summaries in contex~of an information retrieval system.1.
Words and phrases frequergly occurring in a tex~are likely to be indicative of its content, espe-cially if such words or phrases do not occur oldenelsewhere in the database.
A weighted frequencyscore, similar to tf~df used in automatic tex~ in-dexing is applicable.
Here, idf stands for the in-verted document frequency of a term.2.
Title of a tex~ is often strongly related to its con-tent.
Therefore, words and phrases from the titlerepeated in text are considered as important in-dicators of content concentration within a docu-men&3.
Noun phrases occurring in the opening sentencesof multiple paragraphs tend to be indicative of thecontent.
These phrases, along with words from thetitle receive premium scores.4.
In addition, all significant terms in a passage (i.e.,other than the common stopwords) are rankedby a passage-level inverted frequency distribution,e.g., N/pf, where pf is the number of passagescontaining the term and N is the total number ofpassages contained in a document.5.
For generic-type summaries, in case of score ties~he passages closer to the beginning of a text arepreferred to those located towards the end.The process of passage selection as described hereresembles query-based ocument retrieval.
The"documents" here are the passages, and the "query"is a set of words and phrases found in the document'stitle and in the openings of some paragraphs.
Notethat the summarizer scores both single- and multi-paragraph passages, which makes it more indepen-dent from any particular physical paragraph struc-ture of a document.Supp ly ing  the lSacl~ground passageThe background section supplies information thatmakes the summary self-contained.
For example, apassage selected from a document may have signif-icant links, both explicit and implicit, to the sur-rounding context, which if severed are likely to ren-der the passage uncomprehensible, or even mislead-ing.
The following passage illustrates the point:"Once again this demonstrates the substantial influenceIran holds over terrorist kidnapers," Redman said, addingthat it is not yet clear what prompted Iran to take the ac-tion it did.Adding a background paragraph makes this a farmore informative summary:Both the French and Iranian governments acknowledged theIranian role in the release ot" the three French hostages,Jean-Paul Kauffmann, Marcel Carton and Marcel Fontaine.
"Once again this demonstrates the substantial influenceIran holds over terrorist kidnapers," Redman said, addingthat it is not yet clear what prompted Iran to take the ac-tion it did.Below are three main criteria we consider to decideif a background passage is required, and if so, howto get one.1.
One indication that a background informationmay be needed is the presence of outgoing refer-ences, such as anaphors.
If an anaphor is detectedwithin the first N (=6) items (words, phrases) ofthe selected passage, the preceding passage is ap-pended to the summary.
Anaphors and other ref-erences are identified by the presence of pronouns,definite noun phrases, and quoted expressions..
Initially the passages are formed from single physi-cal paragraphs, but for some texts the required in-formation may be spread over multiple paragraphsso that no clear "winner" can be selected.
Sub-sequently, multi-paragraph passages are scored,starting with pairs of adjacent paragraphs..
If the selected main summary passage is shorterthan 15 characters, then the passage following it isadded to the to the summary.
The value of E de-pends upon the average length of the documentsbeing summarized, and it was set as 100 charac-ters for AP newswire articles.
This helps avoidingchoppy summaries from texts with a weak para-graph structure.Imp lernen~af ion  and  eva luat ionThe summarizer has been implemented as a demon-stration system, primarily for news summarization.In general we are quite pleased with the system'sperformance.
The summarizer is domain indepen-dent, and can effectively process a range of typesof documents.
The summaries are quite informativewith excellent readability.
They are also quite short,generally only 5 to 10% of the original text and canbe read and understood very quickly.As discussed before, we have included the sum-marizer as a helper application within the user in-terface to the natural anguage information retrievalsystem.
In this application, the summarizer is usedto derive query-related summaries of documents re-turned from database search.
The summarizationmethod used here is the same as for generic sum-maries described thus far, with the following excep-tions:12621.
The passage-search "query" is derived from theuser's document search query rather than fromthe document title.2.
The distance of a passage from the beginningof the document is not considered towards itssummary-worthiness.The topical summaries are read by the users toquickly decide their relevance to the search topicand, if desired, to expand the initial informationsearch statement in order to produce a significantlymore effective query.
The following example showsa topical (query-guided summary) and compares itto the generic summary (we abbreviate SGML forbrevity).INITIAL SEARCH STATEMENT:< ~iHe > Evidence of Iranian support for Lebanese hostagetakers.< desc > Document will give data linking Iran to groupsin Lebanon which seize and hold Western hostages.FIRST RETRIEVED DOCUMENT (TITLE):Arab Hijackers' Demands Similar To Those of Hostage-Takers in LebanonSUMMARIZER TOPIC:Evidence of Iranian support For Lebanese hostage takersTOPICAL  SUMMARY (used for expansion):Mugniyeh, 36, is a key figure in the security apparatus ofHezbollah, or Party of God, an Iranian-backed SMite move-ment believed to be the umbrella For Factions holding mostof the 22 foreign hostages in Lebanon.GENERIC SUMMARY (for comparison):The demand made by hijackers of a Kuwaiti jet is the sameas that made by Moslems holding Americans hostage inLebanon - freedom \['or 17 pro-lranian extremists jailed inKuwait \['or bombing U.S. and French embassies there in1983.PARTIALLY EXPANDED SEARCH STATEMENT:< ~itle > Evidence of Iranian support for Lebanese hostagetakers.< desc > Document will give data linking Iran to groupsin Lebanon which seize and hold Western hostages.< expd > Mugniyeh, 36, is a key figure in the securityapparatus of Hezbollah, or Party of God, an Iranian-backedShiite movement believed to be the umbrella For factionsholding most of the 22 t'oreign hostages in Lebanon.Overv iew o f  t~tie NL IR  SystemThe Natural I~anguage Information 17Letrieval Sys-tem (NISIR) ?
as been designed as a series of par-allel text processing and indexing "s\[reams '~.
Eachstream constitutes an alternative representation ofthe database obtained using differenl combinationof natural language processing steps.
The purposeof NI~ processing is to obtain a more accurate con-tent representation than that based on words alone,which will in turn lead to improved performance.The following term extraction steps correspond tosome of the streams used in our syslem:6For more  details, see (Strzalkowskl 1995), (Strza-Ikowski et al 1997)1.
Elimination of stopwords: Documents are indexedusing original words minus selected "stopwords"that include all closed-class words (determiners,prepositions, etc.)2.
Morphological stemming: Words are normalizedacross morphological variants using a lexicon-based stemmer.3.
Phrase extraction: Shallow text processing tech-niques, including part-of-speech tagging, phraseboundary detection, and word co-occurrence met-rics are used to identify relatively stable groups ofwords, e.g., joint venture.4.
Phrase normalization: Documents are processedwith a syntactic parser, and "Head+Modifier"pairs are extracted in order to normalize acrosssyntactic variants and reduce to a common "con-cept", e.g., weapon+proliferate.5.
Proper name extraction: Names of people, loca-lions, organizations, etc.
are identified.Search queries, after appropriate processing, arerun against each stream, i.e., a phrase query againstthe phrase stream, a name query against he namestream, etc.
The results are obtained by mergingranked lists of documents obtained from searchingall streams.
This allows for an easy combinationof alternative retrieval methods, creating a meta-search strategy which maximizes the contribution ofeach stream.
Different information retrieval systemscan used as indexing and search engines each stream.In the experiments described here we used Cornell'sSMART (version 11) (Buckley, et al 1995).TREC Eva luat l ion  Resu I t l sTable 1 lists selected runs performed with theNLIR system on TREC-6 database using 50 queries(TREC topics) numbered 301 through 350.
Theexpanded query runs are contrasted with runs ob-tained using TI~EC original topics using NLIt{ aswell as Cornell's SMART (version 11) which serveshere as a benchmark.
The first two columns areautomatic runs, which means that there was no hu-man intervention i  the process at any time.
Sincequery expansion requires human decision on sum-mary selection, these runs (columns 3 and 4) areclassified as "manual", although most of the processis automatic.
As can be seen, query expansion pro-duces an impressive improvement in precision at alllevels, l~ecall figures are shown at 1000 retrieveddocuments.Query expansion appears to produce consistentlyhigh gains not only for different sets of queries but1263Table I: Performance improvement for expandedqueriesqueries: original original expanded expandedSYSTEM SMART NLIR SMART NLIRPRECISIONAverage 0.1429 0.1837 0.2672 0.2859%change +28.5 +87.0 +100.0At 10 docs 0.3000 0.3840 0.5060 0.5200%change +28.0 +68.6 +73.3At 30 docs 0.2387 0.2747 0.3887 0.3940%change +15.0 +62.8 +65.0At 100 doc 0.1600 0.1736 0.2480 0.2574%change +8.5 +55.0 +60.8Recall 0.57 0.53 0.61 0.62%change -7.0 +7.0 +8.7also for different systems: we asked other groupsparticipating in TREC to run search using our ex-panded queries, and they reported similarly largeimprovements.Finally, we may note that NLP-based indexing hasalso a positive ffect on overall performance, but theimprovements are relatively modest, particularly onthe expanded queries.
A similar effect of reduced ef-fectiveness of linguistic indexing has been reportedalso in connection with improved term weightingtechniques.Conc lus ionsWe have developed a method to derive quick-readsummaries from news-like texts using a number ofshallow NISP and simple quantitative techniques.The summary isassembled out of passages extractedfrom the original text, based on a pre-determinedDMS template.
This approach as produced a verye~cient and robust summarizer for news-like tex~s.We used the summarizer, via the QET inter-face, to build effective search queries for an informa-tion retrieval system.
This has been demonstratedto produce dramatic performance improvements inTREC evaluations.
We believe that this query ex-pansion approach will also prove useful in searchingvery large databases where obtaining a full indexmay be impractical or impossible, and accurate sam-pling will become critical.Acknowledgements We thank Chris Buckley forhelping us to understand the inner workings ofSMART, and also for providing SMART system re-sults used here.
This paper is based upon work sup-ported in part by the Defense Advanced ResearchProjects Agency under Tipster Phase-3 Contract 97-F157200-000.ReferencesBuckley, Chris, Amit Singhal, Mandar Mitra, Gerard Salton.1995.
"New Retrieval Approaches Using SMART: TREC 4".Proceedings of TREC-4 Cont'erence, NIST Special Publication500-236.DeJong, G.G., 1992.
An overview of the FRUMP system, Lehn-err, W.G.
and M.H.
Ringle (eds), Strategies \]or NLP, LawrenceErlbaum, Hillsdale, NJ.Harman, Donna.
1988.
"Towards interactive query expansion.
"Proceedings of ACM SIGIR-88, pp.
321-331.Harman, Donna, and Ellen Voorhees (eds).
1998.
The Text Re-trieval Conference (TREC-6).
NIST Special Publication (to ap-pear).Kupiec,J., J. Pedersen and F. Chen, 1995.
A trainable documentsummarizer, Proceedings of ACM SIGIR-95, pp.
68-73.Lehnert, W.O., 1981.
Plots Units and Narrative summarization,Cognitive Science, 4, pp 293-331.Luhn, H.P., 1958.
The automatic creation of literature abstracts,IBM Journal, Apt, pp.
159-165.McKeown, K.R.
and D.R.
Radev, 1995.
Generating Summariesof Multiple News Articles, Proceedings of ACM SIGIR-95Proceedings of 5th Message Understanding Conference, SanFrancisco, CA:Morgan Kaufman Publishers.
1993.OnO, K., K. Sumita and S.Miike, 1994.
Abstract Generationbased on Rhetorical Structure Extraction, COLINGg$, vol 1,pp 344-348, Kyoto, Japan.Paice, C.D., 1990.
Constructing literature abstracts by com-puter: techniques and prospects, Information Processing andManagemenf, vol 26 (1), pp 171-186.Rau, L.F., R. Brandow and K. Mitze, 1994.
Domain-independent summarization r news, Summarizing text for in-~elligen~ communication, page 71-75, Dagstuhl, Gemany.RinG, L.H.M.
and D. Scott, 1994.
Content selection in summarygeneration, Third International Con\]erence on the CognitiveScience of NLP, Dublin City University, Ireland.Rocchio, J. J.
1971.
"Relevance Feedback in Informatio Re-trieval."
In Salton, G.
(Ed.
), The SMART Retrieval System,pp.
313-323.
Prentice Hall, Inc., Englewood Cliffs, NJ.Strzalkowski, Tomek, Jin Wang, and Bowden Wise.
1998.
"ARobust Practical Text Summarization."
Proceedings of AAAISpring Symposium on Intelligent Text Summarization (to ap-pear).Strzalkowski, Tomek, Fang Lin, Jose Perez-Carballo, and JinWang.
1997.
"Natural Language Information Retrieval: TRECo6 Report."
Proceedings of TREC-6 conference.Strzalkowski, Tomek, Louise Guthrie, Jussi Karlgren, Jim Leis-tensnider, Fang Lin, Jose Perez-Carballo, Troy Straszheim, JinWang, and Jon Wilding.
1997.
"Natural Language InformationRetrieval: TREC?5 Report."
Proceedings of TREC-5 confer?ence.Strzalkowski, Tomek.
1995.
"Natural Language Information Re-trieval" Information Processing and Management, Vol.
31, No.3, pp.
397-417.
Pergamon/Elsevier.Strzalkowski, Tomek.
and Jin Wang, 1996.
A Serf-Learning Uni-versal Concept Spotter, Proceedings of COLING-96, pp.
931-936.Tipster Tez~ Phase ~: ~ month Conference, Morgan-Kaufmann.
1996Voorhees, Ellen M. 1994.
"Query Expansion Using Lexical-Semantic Relations."
Proceedings of ACM SIGIR'94, pp.
61-70.Wetssberg, R. and S. Buker, 1990.
Writing up Research: Ex-perimental Research Repor~ Writing \]or Student of English,Prentice Hail, Inc.1264
