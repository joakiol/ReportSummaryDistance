Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1007?1017,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsUnderspecifying and Predicting Voice for Surface Realisation RankingSina Zarrie?, Aoife Cahill and Jonas KuhnInstitut fu?r maschinelle SprachverarbeitungUniversita?t Stuttgart, Germany{sina.zarriess,aoife.cahill,jonas.kuhn}@ims.uni-stuttgart.deAbstractThis paper addresses a data-driven surfacerealisation model based on a large-scale re-versible grammar of German.
We investigatethe relationship between the surface realisa-tion performance and the character of the in-put to generation, i.e.
its degree of underspec-ification.
We extend a syntactic surface reali-sation system, which can be trained to chooseamong word order variants, such that the can-didate set includes active and passive variants.This allows us to study the interaction of voiceand word order alternations in realistic Ger-man corpus data.
We show that with an ap-propriately underspecified input, a linguisti-cally informed realisation model trained to re-generate strings from the underlying semanticrepresentation achieves 91.5% accuracy (overa baseline of 82.5%) in the prediction of theoriginal voice.1 IntroductionThis paper1 presents work on modelling the usageof voice and word order alternations in a free wordorder language.
Given a set of meaning-equivalentcandidate sentences, such as in the simplified En-glish Example (1), our model makes predictionsabout which candidate sentence is most appropriateor natural given the context.
(1) Context: The Parliament started the debate about the statebudget in April.a.
It wasn?t until June that the Parliament approved it.b.
It wasn?t until June that it was approved by the Parliament.c.
It wasn?t until June that it was approved.We address the problem of predicting the usage oflinguistic alternations in the framework of a surface1This work has been supported by the Deutsche Forschungs-gemeinschaft (DFG; German Research Foundation) in SFB 732Incremental specification in context, project D2 (PIs: JonasKuhn and Christian Rohrer).realisation ranking system.
Such ranking systemsare practically relevant for the real-world applica-tion of grammar-based generators that usually gen-erate several grammatical surface sentences from agiven abstract input, e.g.
(Velldal and Oepen, 2006).Moreover, this framework allows for detailed exper-imental studies of the interaction of specific linguis-tic features.
Thus it has been demonstrated that forfree word order languages like German, word or-der prediction quality can be improved with care-fully designed, linguistically informed models cap-turing information-structural strategies (Filippovaand Strube, 2007; Cahill and Riester, 2009).This paper is situated in the same framework, us-ing rich linguistic representations over corpus datafor machine learning of realisation ranking.
How-ever, we go beyond the task of finding the correct or-dering for an almost fixed set of word forms.
Quiteobviously, word order is only one of the means ata speaker?s disposal for expressing some content ina contextually appropriate form; we add systematicalternations like the voice alternation (active vs. pas-sive) to the picture.
As an alternative way of pro-moting or demoting the prominence of a syntacticargument, its interaction with word ordering strate-gies in real corpus data is of high theoretical interest(Aissen, 1999; Aissen, 2003; Bresnan et al, 2001).Our main goals are (i) to establish a corpus-basedsurface realisation framework for empirically inves-tigating interactions of voice and word order in Ger-man, (ii) to design an input representation for gen-eration capturing voice alternations in a variety ofcontexts, (iii) to better understand the relationshipbetween the performance of a generation rankingmodel and the type of realisation candidates avail-able in its input.
In working towards these goals,this paper addresses the question of evaluation.
Weconduct a pilot human evaluation on the voice al-1007ternation data and relate our findings to our resultsestablished in the automatic ranking experiments.Addressing interactions among a range of gram-matical and discourse phenomena on realistic corpusdata turns out to be a major methodological chal-lenge for data-driven surface realisation.
The set ofcandidate realisations available for ranking will in-fluence the findings, and here, existing surface re-alisers vary considerably.
Belz et al (2010) pointout the differences across approaches in the type ofsyntactic and semantic information present and ab-sent in the input representation; and it is the type ofunderspecification that determines the number (andcharacter) of available candidate realisations and,hence, the complexity of the realisation task.We study the effect of varying degrees of under-specification explicitly, extending a syntactic gen-eration system by a semantic component capturingvoice alternations.
In regeneration studies involvingunderspecified underlying representations, corpus-oriented work reveals an additional methodologicalchallenge.
When using standard semantic represen-tations, as common in broad-coverage work in se-mantic parsing (i.e., from the point of view of analy-sis), alternative variants for sentence realisation willoften receive slightly different representations: Inthe context of (1), the continuation (1-c) is presum-ably more natural than (1-b), but with a standardsentence-bounded semantic analysis, only (1-a) and(1-b) would receive equivalent representations.Rather than waiting for the availability of robustand reliable techniques for detecting the reference ofimplicit arguments in analysis (or for contextuallyaware reasoning components), we adopt a relativelysimple heuristic approach (see Section 3.1) that ap-proximates the desired equivalences by augmentedrepresentations for examples like (1-c).
This waywe can overcome an extremely skewed distributionin the naturally occurring meaning-equivalent activevs.
passive sentences, a factor which we believe jus-tifies taking the risk of occasional overgeneration.The paper is structured as follows: Section 2 situ-ates our methodology with respect to other work onsurface realisation and briefly summarises the rele-vant theoretical linguistic background.
In Section 3,we present our generation architecture and the de-sign of the input representation.
Section 4 describesthe setup for the experiments in Section 5.
In Section6, we present the results from the human evaluation.2 Related Work2.1 Generation BackgroundThe first widely known data-driven approach tosurface realisation, or tactical generation, (Langk-ilde and Knight, 1998) used language-model n-gram statistics on a word lattice of candidate re-alisations to guide a ranker.
Subsequent work ex-plored ways of exploiting linguistically annotateddata for trainable generation models (Ratnaparkhi,2000; Marciniak and Strube, 2005; Belz, 2005, a.o.
).Work on data-driven approaches has led to insightsinto the importance of linguistic features for sen-tence linearisation decisions (Ringger et al, 2004;Filippova and Strube, 2009).
The availability of dis-criminative learning techniques for the ranking ofcandidate analyses output by broad-coverage gram-mars with rich linguistic representations, originallyin parsing (Riezler et al, 2000; Riezler et al, 2002),has also led to a revival of interest in linguisticallysophisticated reversible grammars as the basis forsurface realisation (Velldal and Oepen, 2006; Cahillet al, 2007).
The grammar generates candidateanalyses for an underlying representation and theranker?s task is to predict the contextually appropri-ate realisation.The work that is most closely related to ours isVelldal (2008).
He uses an MRS representationderived by an HPSG grammar that can be under-specified for information status.
In his case, theunderspecification is encoded in the grammar andnot directly controlled.
In multilingually orientedlinearisation work, Bohnet et al (2010) generatefrom semantic corpus annotations included in theCoNLL?09 shared task data.
However, they note thatthese annotations are not suitable for full generationsince they are often incomplete.
Thus, it is not clearto which degree these annotations are actually un-derspecified for certain paraphrases.2.2 Linguistic BackgroundIn competition-based linguistic theories (Optimal-ity Theory and related frameworks), the use ofargument alternations is construed as an effectof markedness hierarchies (Aissen, 1999; Aissen,2003).
Argument functions (subject, object, .
.
. )
on1008the one hand and the various properties that argu-ment phrases can bear (person, animacy, definite-ness) on the other are organised in markedness hi-erarchies.
Wherever possible, there is a tendency toalign the hierarchies, i.e., use prominent functions torealise prominently marked argument phrases.
Forinstance, Bresnan et al (2001) find that there is a sta-tistical tendency in English to passivise a verb if thepatient is higher on the person scale than the agent,but an active is grammatically possible.Bresnan et al (2007) correlate the use of the En-glish dative alternation to a number of features suchas givenness, pronominalisation, definiteness, con-stituent length, animacy of the involved verb argu-ments.
These features are assumed to reflect the dis-course acessibility of the arguments.Interestingly, the properties that have been usedto model argument alternations in strict word or-der languages like English have been identified asfactors that influence word order in free word or-der languages like German, see Filippova and Strube(2007) for a number of pointers.
Cahill and Riester(2009) implement a model for German word or-der variation that approximates the information sta-tus of constituents through morphological featureslike definiteness, pronominalisation etc.
We are notaware of any corpus-based generation studies inves-tigating how these properties relate to argument al-ternations in free word order languages.3 Generation ArchitectureOur data-driven methodology for investigating fac-tors relevant to surface realisation uses a regen-eration set-up2 with two main components: a) agrammar-based component used to parse a corpussentence and map it to all its meaning-equivalentsurface realisations, b) a statistical ranking compo-nent used to select the correct, i.e.
contextually mostappropriate surface realisation.
Two variants of thisset-up that we use are sketched in Figure 1.We generally use a hand-crafted, broad-coverageLFG for German (Rohrer and Forst, 2006) to parsea corpus sentence into a f(unctional) structure3and generate all surface realisations from a given2Compare the bidirectional competition set-up in someOptimality-Theoretic work, e.g., (Kuhn, 2003).3The choice among alternative f-structures is done with adiscriminative model (Forst, 2007).SntxSVM RankerSnta1 Snta2 ... SntamLFG grammarFSaLFG grammarSntiSntySVM RankerSntb1 Snta1 Snta2 ... SntbnLFG GrammarFSa FSbReverse Sem.
RulesSEMSem.
RulesFS1LFG GrammarSntiFigure 1: Generation pipelinesf-structure, following the generation approach ofCahill et al (2007).
F-structures are attribute-value matrices representing grammatical functionsand morphosyntactic features; their theoretical mo-tivation lies in the abstraction over details of sur-face realisation.
The grammar is implemented in theXLE framework (Crouch et al, 2006), which allowsfor reversible use of the same declarative grammarin the parsing and generation direction.To obtain a more abstract underlying representa-tion (in the pipeline on the right-hand side of Fig-ure 1), the present work uses an additional seman-tic construction component (Crouch and King, 2006;Zarrie?, 2009) to map LFG f-structures to meaningrepresentations.
For the reverse direction, the mean-ing representations are mapped to f-structures whichcan then be mapped to surface strings by the XLEgenerator (Zarrie?
and Kuhn, 2010).For the final realisation ranking step in bothpipelines, we used SVMrank, a Support VectorMachine-based learning tool (Joachims, 1996).
Theranking step is thus technically independent from theLFG-based component.
However, the grammar isused to produce the training data, pairs of corpussentences and the possible alternations.The two pipelines allow us to vary the degree towhich the generation input is underspecified.
An f-structure abstracts away from word order, i.e.
thecandidate set will contain just word order alterna-tions.
In the semantic input, syntactic function andvoice are underspecified, so a larger set of surfacerealisation candidates is generated.
Figure 2 illus-trates the two representation levels for an active and1009a passive sentence.
The subject of the passive andthe object of the active f-structure are mapped to thesame role (patient) in the meaning representation.3.1 Issues with ?naive?
underspecificationIn order to create an underspecified voice represen-tation that does indeed leave open the realisation op-tions available to the speaker/writer, it is often notsufficient to remove just the syntactic function in-formation.
For instance, the subject of the activesentence (2) is an arbitrary reference pronoun man?one?
which cannot be used as an oblique agent ina passive, sentence (2-b) is ungrammatical.
(2) a. ManOnehathasdentheKanzlerchancellorgesehen.seen.b.
*DerTheKanzlerchancellorwurdewasvonbymanonegesehen.seen.So, when combined with the grammar, the mean-ing representation for (2) in Figure 2 contains im-plicit information about the voice of the original cor-pus sentence; the candidate set will not include anypassive realisations.
However, a passive realisationwithout the oblique agent in the by-phrase, as in Ex-ample (3), is a very natural variant.
(3) DerTheKanzlerchancellorwurdewasgesehen.seen.The reverse situation arises frequently too: pas-sive sentences where the agent role is not overtlyrealised.
Given the standard, ?analysis-oriented?meaning representation for Sentence (4) in Figure2, the realiser will not generate an active realisationsince the agent role cannot be instantiated by anyphrase in the grammar.
However, depending on theexact context there are typically options for realis-ing the subject phrase in an active with very littledescriptive content.Ideally, one would like to account for these phe-nomena in a meaning representation that under-specifies the lexicalisation of discourse referents,and also captures the reference of implicit argu-ments.
Especially the latter task has hardly beenaddressed in NLP applications (but see Gerber andChai (2010)).
In order to work around that problem,we implemented some simple heuristics which un-derspecify the realisation of certain verb arguments.These rules define: 1. a set of pronouns (generic andneutral pronouns, universal quantifiers) that corre-spond to ?trivial?
agents in active and implicit agentsActive Passive2-role trans.
71% (82%) 10% (2%)1-role trans.
11% (0%) 8% (16%)Table 1: Distribution of voices in SEMh (SEMn)in passive sentences; 2. a set of prepositional ad-juncts in passive sentences that correspond to sub-jects in active sentence (e.g.
causative and instru-mental prepositions like durch ?by means of?
); 3.certain syntactic contexts where special underspec-ification devices are needed, e.g.
coordinations orembeddings, see Zarrie?
and Kuhn (2010) for ex-amples.
In the following, we will distinguish 1-roletransitives where the agent is ?trivial?
or implicitfrom 2-role transitives with a non-implicit agent.By means of the extended underspecification rulesfor voice, the sentences in (2) and (3) receive anidentical meaning representation.
As a result, oursurface realiser can produce an active alternation for(3) and a passive alternation for (2).
In the follow-ing, we will refer to the extended representations asSEMh (?heuristic semantics?
), and to the originalrepresentations as SEMn (?naive semantics?
).We are aware of the fact that these approximationsintroduce some noise into the data and do not alwaysrepresent the underlying referents correctly.
For in-stance, the implicit agent in a passive need not be?trivial?
but can correspond to an actual discoursereferent.
However, we consider these heuristics asa first step towards capturing an important discoursefunction of the passive alternation, namely the dele-tion of the agent role.
If we did not treat the passiveswith an implicit agent on a par with certain actives,we would have to ignore a major portion of the pas-sives occurring in corpus data.Table 1 summarises the distribution of the voicesfor the heuristic meaning representation SEMh onthe data-set we will introduce in Section 4, withthe distribution for the naive representation SEMnin parentheses.4 Experimental Set-upData To obtain a sizable set of realistic corpus ex-amples for our experiments on voice alternations, wecreated our own dataset of input sentences and rep-resentations, instead of building on treebank exam-ples as Cahill et al (2007) do.
We extracted 19,905sentences, all containing at least one transitive verb,1010f-structureExample (2)266664PRED ?see < (?
SUBJ)(?
OBJ) >?SUBJ?PRED ?one?
?OBJ?PRED ?chancellor??TOPIC??one?
?PASS ?377775f-structureExample (3)2664PRED ?see < NULL (?
SUBJ) >?SUBJ?PRED ?chancellor??TOPIC??chancellor?
?PASS +3775semanticsExample (2)HEAD (see)PAST (see)ROLE (agent,see,one)ROLE (patient,see,chancellor)semanticsExample (3)HEAD (see)PAST (see)ROLE (agent,see,implicit)ROLE (patient,see,chancellor)Figure 2: F-structure pair for passive-active alternationfrom the HGC, a huge German corpus of newspa-per text (204.5 million tokens).
The sentences areautomatically parsed with the German LFG gram-mar.
The resulting f-structure parses are transferredto meaning representations and mapped back to f-structure charts.
For our generation experiments,we only use those f-structure charts that the XLEgenerator can map back to a set of surface realisa-tions.
This results in a total of 1236 test sentencesand 8044 sentences in our training set.
The data lossis mostly due to the fact the XLE generator oftenfails on incomplete parses, and on very long sen-tences.
Nevertheless, the average sentence length(17.28) and number of surface realisations (see Ta-ble 2) are higher than in Cahill et al (2007).Labelling For the training of our ranking model,we have to tell the learner how closely each surfacerealisation candidate resembles the original corpussentence.
We distinguish the rank categories: ?1?identical to the corpus string, ?2?
identical to thecorpus string ignoring punctuation, ?3?
small editdistance (< 4) to the corpus string ignoring punc-tuation, ?4?
different from the corpus sentence.
Inone of our experiments (Section 5.1), we used therank category ?5?
to explicitly label the surface real-isations derived from the alternation f-structure thatdoes not correspond to the parse of the original cor-pus sentence.
The intermediate rank categories ?2?and ?3?
are useful since the grammar does not al-ways regenerate the exact corpus string, see Cahillet al (2007) for explanation.Features The linguistic theories sketched in Sec-tion 2.2 correlate morphological, syntactic and se-mantic properties of constituents (or discourse ref-erents) with their order and argument realisation.
Inour system, this correlation is modelled by a combi-nation of linguistic properties that can be extractedfrom the f-structure or meaning representation andof the surface order that is read off the sentencestring.
Standard n-gram features are also used asfeatures.4 The feature model is built as follows:for every lemma in the f-structure, we extract a setof morphological properties (definiteness, person,pronominal status etc.
), the voice of the verbal head,its syntactic and semantic role, and a set of infor-mations status features following Cahill and Riester(2009).
These properties are combined in two ways:a) Precedence features: relative order of propertiesin the surface string, e.g.
?theme < agent in pas-sive?, ?1st person < 3rd person?
; b) ?scale align-ment?
features (ScalAl.
): combinations of voice androle properties with morphological properties, e.g.
?subject is singular?, ?agent is 3rd person in activevoice?
(these are surface-independent, identical foreach alternation candidate).The model for which we present our results isbased on sentence-internal features only; as Cahilland Riester (2009) showed, these feature carry aconsiderable amount of implicit information aboutthe discourse context (e.g.
in the shape of referringexpressions).
We also implemented a set of explic-itly inter-sentential features, inspired by CenteringTheory (Grosz et al, 1995).
This model did not im-prove over the intra-sentential model.Evaluation Measures In order to assess the gen-eral quality of our generation ranking models, we4The language model is trained on the German data releasefor the 2009 ACL Workshop on Machine Translation sharedtask, 11,991,277 total sentences.1011FS SEMn SEMhAvg.
# strings 36.7 68.2 75.8Random Match 16.98 10.72 7.28LMMatch 15.45 15.04 11.89BLEU 0.68 0.68 0.65NIST 13.01 12.95 12.69Ling.
ModelMatch 27.91 27.66 26.38BLEU 0.764 0.759 0.747NIST 13.18 13.14 13.01Table 2: Evaluation of Experiment 1use several standard measures: a) exact match:how often does the model select the original cor-pus sentence, b) BLEU: n-gram overlap betweentop-ranked and original sentence, c) NIST: modifi-cation of BLEU giving more weight to less frequentn-grams.
Second, we are interested in the model?sperformance wrt.
specific linguistic criteria.
We re-port the following accuracies: d) Voice: how oftendoes the model select a sentence realising the correctvoice, e) Precedence: how often does the model gen-erate the right order of the verb arguments (agent andpatient), and f) Vorfeld: how often does the modelcorrectly predict the verb arguments to appear in thesentence initial position before the finite verb, theso-called Vorfeld.
See Sections 5.3 and 6 for a dis-cussion of these measures.5 Experiments5.1 Exp.
1: Effect of Underspecified InputWe investigate the effect of the input?s underspecifi-cation on a state-of-the-art surface realisation rank-ing model.
This model implements the entire fea-ture set described in Section 4 (it is further analysedin the subsequent experiments).
We built 3 datasetsfrom our alternation data: FS - candidates generatedfrom the f-structure; SEMn - realisations from thenaive meaning representations; SEMh - candidatesfrom the heuristically underspecified meaning rep-resentation.
Thus, we keep the set of original cor-pus sentences (=the target realisations) constant, buttrain and test the model on different candidate sets.In Table 2, we compare the performance of thelinguistically informed model described in Section 4on the candidates sets against a random choice anda language model (LM) baseline.
The differences inBLEU between the candidate sets and models areFS SEMn SEMh SEMn?AllTrans.
Voice Acc.
100 98.06 91.05 97.59Voice Spec.
100 22.8 0 0Majority BL 82.4 98.12-roleTrans.
Voice Acc.
100 97.7 91.8 97.59Voice Spec.
100 8.33 0 0Majority BL 88.5 98.11-roleTrans.
Voice Acc.
100 100 90.0 -Voice Spec.
100 100 0 -Majority BL 53.9 -Table 3: Accuracy of Voice Prediction by Ling.
Model inExperiment 1statistically significant.5 In general, the linguisticmodel largely outperforms the LM and is less sen-sitive to the additional confusion introduced by theSEMh input.
Its BLEU score and match accuracydecrease only slightly (though statistically signifi-cantly).In Table 3, we report the performance of the lin-guistic model on the different candidate sets with re-spect to voice accuracy.
Since the candidate sets dif-fer in the proportion of items that underspecify thevoice (see ?Voice Spec.?
in Table 3), we also reportthe accuracy on the SEMn?
test set, which is a sub-set of SEMn excluding the items where the voice isspecified.
Table 3 shows that the proportion of activerealisations for the SEMn?
input is very high, andthe model does not outperform the majority baseline(which always selects active).
In contrast, the SEMhmodel clearly outperforms the majority baseline.Example (4) is a case from our development setwhere the SEMn model incorrectly predicts an ac-tive (4-a), and the SEMh correctly predicts a passive(4-b).
(4) a.
2626kostspieligeexpensiveStudienstudieserwa?hntenmentioneddietheFinanzierung.funding.b.
DieTheFinanzierungfundingwurdewasvonby2626kostspieligenexpensiveStudienstudieserwa?hnt.mentioned.This prediction is according to the markedness hier-archy: the patient is singular and definite, the agent5According to a bootstrap resampling test, p < 0.051012Features Match BLEU Voice Prec.
VFPrec.
16.3 0.70 88.43 64.1 59.1ScalAl.
10.4 0.64 90.37 58.9 56.3Union 26.4 0.75 91.50 80.2 70.9Table 4: Evaluation of Experiment 2is plural and indefinite.
Counterexamples are possi-ble, but there is a clear statistical preference ?
whichthe model was able to pick up.On the one hand, the rankers can cope surpris-ingly well with the additional realisations obtainedfrom the meaning representations.
According to theglobal sentence overlap measures, their quality isnot seriously impaired.
On the other hand, the de-sign of the representations has a substantial effecton the prediction of the alternations.
The SEMndoes not seem to learn certain preferences becauseof the extremely imbalanced distribution in the in-put data.
This confirms the hypothesis sketched inSection 3.1, according to which the degree of theinput?s underspecification can crucially change thebehaviour of the ranking model.5.2 Exp.
2: Word Order and VoiceWe examine the impact of certain feature types onthe prediction of the variation types in our data.
Weare particularly interested in the interaction of voiceand word order (precedence) since linguistic theo-ries (see Section 2.2) predict similar information-structural factors guiding their use, but usually donot consider them in conjunction.In Table 4, we report the performance of rankingmodels trained on the different feature subsets intro-duced in Section 4.
The union of the features corre-sponds to the model trained on SEMh in Experiment1.
At a very broad level, the results suggest that theprecedence and the scale alignment features interactboth in the prediction of voice and word order.The most pronounced effect on voice accuracycan be seen when comparing the precedence modelto the union model.
Adding the surface-independentscale alignment features to the precedence featuresleads to a big improvement in the prediction of wordorder.
This is not a trivial observation since a) thesurface-independent features do not discriminate be-tween the word orders and b) the precedence fea-tures are built from the same properties (see Sec-tion 4).
Thus, the SVM learner discovers depen-dencies between relative precedence preferences andabstract properties of a verb argument which cannotbe encoded in the precedence alone.It is worth noting that the precedence features im-prove the voice prediction.
This indicates that wher-ever the application context allows it, voice shouldnot be specified at a stage prior to word order.
Ex-ample (5) is taken from our development set, illus-trating a case where the union model predicted thecorrect voice and word order (5-a), and the scalealignment model top-ranked the incorrect voice andword order.
The active verb arguments in (5-b) areboth case-ambigous and placed in the non-canonicalorder (object < subject), so the semantic relation canbe easily misunderstood.
The passive in (5-a) is un-ambiguous since the agent is realised in a PP (andplaced in the Vorfeld).
(5) a. VonBydenthedeutschenGermanMedienmediawurdenweredietheAusla?nderforeignersnuronlyerwa?hnt,mentioned,wennwhenesthereZofftroublegab.was.b.
WennWhenesthereZofftroublegab,was,erwa?hntenmentioneddietheAusla?nderforeignersnuronlydiethedeutschenGermanMedien.media.Moreover, our results confirm Filippova andStrube (2007) who find that it is harder to predictthe correct Vorfeld occupant in a German sentence,than to predict the relative order of the constituents.5.3 Exp.
3: Capturing Flexible VariationThe previous experiment has shown that there is acertain inter-dependence between word order andvoice.
This experiment addresses this interactionby varying the way the training data for the rankeris labelled.
We contrast two ways of labelling thesentences (see Section 4): a) all sentences that arenot (nearly) identical to the reference sentence havethe rank category ?4?, irrespective of their voice (re-ferred to as unlabelled model), b) the sentences thatdo not realise the correct voice are ranked lower thansentences with the correct voice (?4?
vs.
?5?
), re-ferred to as labelled model.
Intuitively, the latterway of labelling tells the ranker that all sentencesin the incorrect voice are worse than all sentencesin the correct voice, independent of the word order.Given the first labelling strategy, the ranker can de-cide in an unsupervised way which combinations ofword order and voice are to be preferred.1013Top 1 Top 1 Top 1 Top 2 Top 3Model Match BLEU NIST Voice Prec.
Prec.+Voice Prec.+Voice Prec.+VoiceLabelled, no LM 21.52 0.73 12.93 91.9 76.25 71.01 78.35 82.31Unlabelled, no LM 26.83 0.75 13.01 91.5 80.19 74.51 84.28 88.59Unlabeled + LM 27.35 0.75 13.08 91.5 79.6 73.92 79.74 82.89Table 5: Evaluation of Experiment 3In Table 5, it can be seen that the unlabelled modelimproves over the labelled on all the sentence over-lap measures.
The improvements are statisticallysignificant.
Moreover, we compare the n-best ac-curacies achieved by the models for the joint pre-diction of voice and argument order.
The unla-belled model is very flexible with respect to the wordorder-voice interaction: the accuracy dramaticallyimproves when looking at the top 3 sentences.
Ta-ble 5 also reports the performance of an unlabelledmodel that additionally integrates LM scores.
Sur-prisingly, these scores have a very small positive ef-fect on the sentence overlap features and no positiveeffect on the voice and precedence accuracy.
Then-best evaluations even suggest that the LM scoresnegatively impact the ranker: the accuracy for thetop 3 sentences increases much less as compared tothe model that does not integrate LM scores.6The n-best performance of a realisation ranker ispractically relevant for re-ranking applications suchas Velldal (2008).
We think that it is also concep-tually interesting.
Previous evaluation studies sug-gest that the original corpus sentence is not alwaysthe only optimal realisation of a given linguistic in-put (Cahill and Forst, 2010; Belz and Kow, 2010).Humans seem to have varying preferences for wordorder contrasts in certain contexts.
The n-best evalu-ation could reflect the behaviour of a ranking modelwith respect to the range of variations encounteredin real discourse.
The pilot human evaluation in thenext Section deals with this question.6 Human EvaluationOur experiment in Section 5.3 has shown that the ac-curacy of our linguistically informed ranking modeldramatically increases when we consider the three6(Nakanishi et al, 2005) also note a negative effect of in-cluding LM scores in their model, pointing out that the LM wasnot trained on enough data.
The corpus used for training ourLM might also have been too small or distinct in genre.best sentences rather than only the top-ranked sen-tence.
This means that the model sometimes predictsalmost equal naturalness for different voice realisa-tions.
Moreover, in the case of word order, we knowfrom previous evaluation studies, that humans some-times prefer different realisations than the originalcorpus sentences.
This Section investigates agree-ment in human judgements of voice realisation.Whereas previous studies in generation mainlyused human evaluation to compare different sys-tems, or to correlate human and automatic evalua-tions, our primary interest is the agreement or cor-relation between human rankings.
In particular, weexplore the hypothesis that this agreement is higherin certain contexts than in others.
In order to selectthese contexts, we use the predictions made by ourranking model.The questionnaire for our experiment comprised24 items falling into 3 classes: a) items where the3 best sentences predicted by the model have thesame voice as the original sentence (?Correct?
), b)items where the 3 top-ranked sentences realise dif-ferent voices (?Mixed?
), c) items where the modelpredicted the incorrect voice in all 3 top sentences(?False?).
Each item is composed of the originalsentence, the 3 top-ranked sentences (if not identicalto the corpus sentence) and 2 further sentences suchthat each item contains different voices.
For eachitem, we presented the previous context sentence.The experiment was completed by 8 participants,all native speakers of German, 5 had a linguisticbackground.
The participants were asked to rankeach sentence on a scale from 1-6 according to itsnaturalness and plausibility in the given context.
Theparticipants were explicitly allowed to use the samerank for sentences they find equally natural.
The par-ticipants made heavy use of this option: out of the192 annotated items, only 8 are ranked such that notwo sentences have the same rank.We compare the human judgements by correlat-1014ing them with Spearman?s ?.
This measure is con-sidered appropriate for graded annotation tasks ingeneral (Erk and McCarthy, 2009), and has alsobeen used for analysing human realisation rankings(Velldal, 2008; Cahill and Forst, 2010).
We nor-malise the ranks according to the procedure in Vell-dal (2008).
In Table 6, we report the correlationsobtained from averaging over all pairwise correla-tions between the participants and the correlationsrestricted to the item and sentence classes.
We usedbootstrap re-sampling on the pairwise correlations totest that the correlations on the different item classessignificantly differ from each other.The correlations in Table 6 suggest that the agree-ment between annotators is highest on the falseitems, and lowest on the mixed items.
Humanstended to give the best rank to the original sentencemore often on the false items (91%) than on the oth-ers.
Moreover, the agreement is generally higher onthe sentences realising the correct voice.These results seem to confirm our hypothesis thatthe general level of agreement between humans dif-fers depending on the context.
However, one has tobe careful in relating the effects in our data solely tovoice preferences.
Since the sentences were chosenautomatically, some examples contain very unnatu-ral word orders that probably guided the annotators?decisions more than the voice.
This is illustratedby Example (6) showing two passive sentences fromour questionnaire which differ only in the position ofthe adverb besser ?better?.
Sentence (6-a) is com-pletely implausible for a native speaker of German,whereas Sentence (6-b) sounds very natural.
(6) a. DurchBydastheneuenewGesetzlawsollenshouldbesserbetterEigenheimbesitzerhouse ownersgeschu?tztprotectedwerden.be.b.
DurchBydastheneuenewGesetzlawsollenshouldEigenheimbesitzerhouse ownersbesserbettergeschu?tztprotectedwerden.be.This observation brings us back to our initial pointthat the surface realisation task is especially chal-lenging due to the interaction of a range of semanticand discourse phenomena.
Obviously, this interac-tion makes it difficult to single out preferences for aspecific alternation type.
Future work will have toestablish how this problem should be dealt with inItemsAll Correct Mixed False?All?
sent.
0.58 0.6 0.54 0.62?Correct?
sent.
0.64 0.63 0.56 0.72?False?
sent.
0.47 0.57 0.48 0.44Top-rankedcorpus sent.84% 78% 83% 91%Table 6: Human Evaluationthe design of human evaluation experiments.7 ConclusionWe have presented a grammar-based generation ar-chitecture which implements the surface realisationof meaning representations abstracting from voiceand word order.
In order to be able to study voicealternations in a variety of contexts, we designedheuristic underspecification rules which establish,for instance, the alternation relation between an ac-tive with a generic agent and a passive that doesnot overtly realise the agent.
This strategy leadsto a better balanced distribution of the alternationsin the training data, such that our linguisticallyinformed generation ranking model achieves highBLEU scores and accurately predicts active and pas-sive.
In future work, we will extend our experimentsto a wider range of alternations and try to captureinter-sentential context more explicitly.
Moreover, itwould be interesting to carry over our methodologyto a purely statistical linearisation system where therelation between an input representation and a set ofcandidate realisations is not so clearly defined as ina grammar-based system.Our study also addressed the interaction of dif-ferent linguistic variation types, i.e.
word orderand voice, by looking at different types of linguis-tic features and exploring different ways of labellingthe training data.
However, our SVM-based learn-ing framework is not well-suited to directly assessthe correlation between a certain feature (or fea-ture combination) and the occurrence of an alterna-tion.
Therefore, it would be interesting to relate ourwork to the techniques used in theoretical papers,e.g.
(Bresnan et al, 2007), where these correlationsare analysed more directly.1015ReferencesJudith Aissen.
1999.
Markedness and subject choice inoptimality theory.
Natural Language and LinguisticTheory, 17(4):673?711.Judith Aissen.
2003.
Differential Object Marking:Iconicity vs. Economy.
Natural Language and Lin-guistic Theory, 21:435?483.Anja Belz and Eric Kow.
2010.
Comparing ratingscales and preference judgements in language evalu-ation.
In Proceedings of the 6th International NaturalLanguage Generation Conference (INLG?10).Anja Belz, Mike White, Josef van Genabith, DeirdreHogan, and Amanda Stent.
2010.
Finding commonground: Towards a surface realisation shared task.In Proceedings of the 6th International Natural Lan-guage Generation Conference (INLG?10).Anja Belz.
2005.
Statistical generation: Three meth-ods compared and evaluated.
In Proceedings of TenthEuropean Workshop on Natural Language Generation(ENLG-05), pages 15?23.Bernd Bohnet, Leo Wanner, Simon Mill, and AliciaBurga.
2010.
Broad coverage multilingual deep sen-tence generation with a stochastic multi-level realizer.In Proceedings of the 23rd International Conferenceon Computational Linguistics (COLING 2010), Bei-jing, China.Joan Bresnan, Shipra Dingare, and Christopher D. Man-ning.
2001.
Soft Constraints Mirror Hard Constraints:Voice and Person in English and Lummi.
In Proceed-ings of the LFG ?01 Conference.Joan Bresnan, Anna Cueni, Tatiana Nikitina, and HaraldBaayen.
2007.
Predicting the Dative Alternation.
InG.
Boume, I. Kraemer, and J. Zwarts, editors, Cogni-tive Foundations of Interpretation.
Amsterdam: RoyalNetherlands Academy of Science.Aoife Cahill and Martin Forst.
2010.
Human Evaluationof a German Surface Realisation Ranker.
In Proceed-ings of the 12th Conference of the European Chapterof the ACL (EACL 2009), pages 112 ?
120, Athens,Greece.
Association for Computational Linguistics.Aoife Cahill and Arndt Riester.
2009.
IncorporatingInformation Status into Generation Ranking.
In Pro-ceedings of the 47th Annual Meeting of the ACL, pages817?825, Suntec, Singapore, August.
Association forComputational Linguistics.Aoife Cahill, Martin Forst, and Christian Rohrer.
2007.Stochastic realisation ranking for a free word orderlanguage.
In Proceedings of the Eleventh EuropeanWorkshop on Natural Language Generation, pages17?24, Saarbru?cken, Germany, June.
DFKI GmbH.Document D-07-01.Dick Crouch and Tracy Holloway King.
2006.
Se-mantics via F-Structure Rewriting.
In Miriam Buttand Tracy Holloway King, editors, Proceedings of theLFG06 Conference.Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,John Maxwell, and Paula Newman.
2006.
XLE Docu-mentation.
Technical report, Palo Alto Research Cen-ter, CA.Katrin Erk and Diana McCarthy.
2009.
Graded WordSense Assignment.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 440 ?
449, Singapore.Katja Filippova and Michael Strube.
2007.
Generat-ing constituent order in German clauses.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics (ACL 07), Prague, CzechRepublic.Katja Filippova and Michael Strube.
2009.
Tree lin-earization in English: Improving language modelbased approaches.
In Companion Volume to the Pro-ceedings of Human Language Technologies Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics (NAACL-HLT 09,short)., Boulder, Colorado.Martin Forst.
2007.
Filling Statistics with Linguistics?
Property Design for the Disambiguation of GermanLFG Parses.
In ACL 2007 Workshop on Deep Linguis-tic Processing, pages 17?24, Prague, Czech Republic,June.
Association for Computational Linguistics.Matthew Gerber and Joyce Chai.
2010.
Beyond nom-bank: A study of implicit argumentation for nominalpredicates.
In Proceedings of the ACM Conference onKnowledge Discovery and Data Mining (KDD).Barbara J. Grosz, Aravind Joshi, and Scott Weinstein.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.Thorsten Joachims.
1996.
Training linear svms in lineartime.
In M. Butt and T. H. King, editors, Proceedingsof the ACM Conference on Knowledge Discovery andData Mining (KDD), CSLI Proceedings Online.Jonas Kuhn.
2003.
Optimality-Theoretic Syntax?ADeclarative Approach.
CSLI Publications, Stanford,CA.Irene Langkilde and Kevin Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InProceedings of the ACL/COLING-98, pages 704?710,Montreal, Quebec.Tomasz Marciniak and Michael Strube.
2005.
Usingan annotated corpus as a knowledge source for lan-guage generation.
In Proceedings of Workshop on Us-ing Corpora for Natural Language Generation, pages19?24, Birmingham, UK.Hiroko Nakanishi, Yusuke Miyao, and Junichi Tsujii.2005.
Probabilistic models for disambiguation of an1016HPSG-based chart generator.
In Proceedings of IWPT2005.Adwait Ratnaparkhi.
2000.
Trainable methods for sur-face natural language generation.
In Proceedings ofNAACL 2000, pages 194?201, Seattle, WA.Stefan Riezler, Detlef Prescher, Jonas Kuhn, and MarkJohnson.
2000.
Lexicalized stochastic modeling ofconstraint-based grammars using log-linear measuresand EM training.
In Proceedings of the 38th AnnualMeeting of the Association for Computational Linguis-tics (ACL?00), Hong Kong, pages 480?487.Stefan Riezler, Dick Crouch, Ron Kaplan, Tracy King,John Maxwell, and Mark Johnson.
2002.
Parsing theWall Street Journal using a Lexical-Functional Gram-mar and discriminative estimation techniques.
In Pro-ceedings of the 40th Annual Meeting of the Associa-tion for Computational Linguistics (ACL?02), Pennsyl-vania, Philadelphia.Eric K. Ringger, Michael Gamon, Robert C. Moore,David Rojas, Martine Smets, and Simon Corston-Oliver.
2004.
Linguistically Informed StatisticalModels of Constituent Structure for Ordering in Sen-tence Realization.
In Proceedings of the 2004 In-ternational Conference on Computational Linguistics,Geneva, Switzerland.Christian Rohrer and Martin Forst.
2006.
Improvingcoverage and parsing quality of a large-scale LFG forGerman.
In Proceedings of LREC-2006.Erik Velldal and Stephan Oepen.
2006.
Statistical rank-ing in tactical generation.
In Proceedings of the 2006Conference on Empirical Methods in Natural Lan-guage Processing, Sydney, Australia.Erik Velldal.
2008.
Empirical Realization Ranking.Ph.D.
thesis, University of Oslo, Department of Infor-matics.Sina Zarrie?
and Jonas Kuhn.
2010.
Reversing F-structure Rewriting for Generation from Meaning Rep-resentations.
In Proceedings of the LFG10 Confer-ence, Ottawa.Sina Zarrie?.
2009.
Developing German Semantics onthe basis of Parallel LFG Grammars.
In Proceed-ings of the 2009 Workshop on Grammar EngineeringAcross Frameworks (GEAF 2009), pages 10?18, Sun-tec, Singapore, August.
Association for ComputationalLinguistics.1017
