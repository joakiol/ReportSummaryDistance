JETR: A ROBUST MACHINE TRANSLATION SYSTEMRika YoshiiDepartment of Information and Computer ScienceUniversity of California, Irvine,Irvine, California, 92717 tABSTRACTThis paper presents an expectation-based Japanese-to-English translation system called JETR which relieson the forward expectation-refinement process tohandle ungrammatical sentences in an elegant andefficient manner without relying on the presence ofparticles and verbs in the source text.
JETR uses achain of result states to perform context analysis forresolving pronoun and object references and fillingellipses.
Unlike other knowledge-based systems, JETRattempts to achieve semantic, pragmatic, structural andlexical invariance.INTRODUCTIONRecently there has been a revitalized interest inmachine translation as both a practical engineeringproblem and a tool to test various ArtificialIntelligence (AI) theories.
As a result of increasedinternational communication, there exists today amassive Japanese effort in machine translation.However, systems ready for commercialization are stillconcentrating on syntactic information and are unableto translate syntactically obscure but meaningfulsentences.
Moreover, many of these systems do notperform context analysis and thus cannot fill ellipsesor resolve pronoun references.
Knowledge-basedsystems, on the other hand, tend to discard the syntaxof the source text and thus are unable to preserve thesyntactic style of the source text.
Moreover, thesesystems concentrate on understanding and thus do notpreserve the semantic ontent of the source text.An expectation-based approach to "Japanese-to-English machine translation is presented.
Theapproach is demonstrated by the JETR system which isdesigned to translate recipes and instruction booklets.Unlike other Japanese-to-English translation systems,which rely on the presence of particles and main verbsin the source text (AAT 1984, Ibuki 1983, Nitta 1982,tThe author is now located at:Rockwell International Corp.Autonetics Strategic Systems DivisionMail Code: GA423370 Miraloma Avenue, P.O.
Box 4192Anaheim, California 92803-4192Saino 1983, Shimazu 1983), JETR is designed totranslate ungrammatical nd abbreviated sentencesusing semantic and contextual information.
Unlikeother knowledge-based translation systems(Cullingford 1976, Ishizaki 1983, Schank 1982, Yang1981), JETR does not view machine translation as aparaphrasing problem.
JETR attempts to achievesemantic, pragmatic, structural and lexical invariancewhich (Carbonell 1981) gives as multiple dimensionsof quality in the translation process.Sends phrases, woodd~sses and phrase roles\[Analyzer\[ (PDA) ,~Sends object framesSends object frammsand action framesSends modified expectations,modified object types andfilled framesGeneratorIResolves Sendsan~hofic objectreferences framesI Context Analyzer IFigure 1.
JETR ComponentsJETR is comprised of three interleaved components:the particle-driven analyzer, the generator, and thecontext analyzer as shown in Figure 1.
The threecomponents interact with one another to preserveinformation contained in grammatical as well asungrammatical texts.
The overview of each componentis presented below.
This paper focuses on the particle-driven analyzer.CIIARACTERISTICS OF TilE JAPANESE LANGUAGEThe difficulty of translation depends on thesimilarity between the languages involved.
Japaneseand English are vastly different languages.
Translationfrom Japanese to English involves restructuring ofsentences, disambiguation of words, and additions and25deletions of certain lexical items.
The followingcharacteristics of the Japanese language haveinfluenced the design of the JETR system:1.
Japanese is a left-branching, post-positional, subject-object-verb language.2.
Particles and not word order are importantin determining the roles of the nounphrases in a Japanese sentence.. Information is usually more explicitlystated in English than in Japanese.
Thereare no articles (i.e.
"a", "an", and "the").There are no singular and plural forms ofnouns.
Grammatical sentences can havetheir subjects and objects missing (i.e.ellipses).PDA: PARTICLE-DRIVEN ANALYZERObserve the following sentences:Verb-deletion:Neji (screw) o (object marker) migi (right) e(direction marker) 3 kurikku (clicks).Particle-deletion:Shin (salt) keiniku (chicken) ni (destinationmarker) furu (sprinkle).The first sentence lacks the main verb, while thesecond sentence lacks the particle after the noun"shin."
The role of "shin" must be determinedwithout relying on the particle and the word order.In addition to the problems of unknown words andunclear or ambiguous interpretation, missing particlesand verbs are often found in recipes, instructionbooklets and other informal texts posing specialproblems for machine translation systems.
TheParticle-Driven Analyzer (PDA) is a robustintrasentence analyzer designed to handleungrammatical sentences in an elegant and efficientmanner.While analyzers of the English language relyheavily on verb-oriented processing, the existence ofparticles in the Japanese language and the subject-object-verb word order have led to the PDA's relianceon forward expectations from words other than verbs.The PDA is unique in that it does not rely on thepresence of particles and verbs in the source text.
Totake care of missing particles and verbs, not onlyverbs but all nouns and adverbs are made to point toaction frames which are structures used to describeactions.
For both grammatical and ungrammaticalsentences, the PDA continuously combines and refinesforward expectations from various phrases to determ/netheir roles and to predict actions.
These expectationsare semantic in nature and disregard the word order ofthe sentence.
Each expectation is an action-role pair ofthe form (<action> <role>).
Actions are names ofaction frames while roles correspond to the slot namesof action frames.
Since the main verb is almost alwaysfound at the end of the sentence, combined forwardexpectations are strong enough to point to the roles ofthe nouns and the meaning of the verb.
For example,consider "neji (screw) migi (right) ?
3 kurikku(clicks)."
By the time, "3 clicks" is read, there arestrong expectations for the act of turning, and thescrew expects to be the object of the act.Input: <muM> o ~ ~ <verb>(al ~e~) J2(a3 ~$Una~)(a4 des~na~on)(al oqect)(al iN;~ument)(a3 destination) 4Intersection:(a2 oqe~ (~ dasdna~on) (a2 desdnaton)Figure 2.
Expectation Refinement in the PDAFigure 2 describes the forward expectation-refinement process.
In order to keep the expectationlist to a manageable size, only ten of the most likelyroles and actions are attached to each word.Input:.Expectations:<noun1> mIntersection:(at ~ .
(al ~j~ a2a3(~e~ \[ (~o . )
4.. nounl ~e~t'~(a4 deshion).
/9ene~ole tier(at oqd(~)  , ,mp~Figure 3.
Expectation Mismatch in the PDAThe PDA is similar to IPP (Lebowitz 1983) in thatwords other than verbs are made to point to structureswhich describe actions.
However, unlike IPP, a genericrole-filling process will be invoked only if an26unexpected verb is encountered or the forwardexpectations do not match.
Figure 3 shows such acase.
The verb will not invoke any role-filling orrole-determining process ff the semantic expectationsfrom the other phrases match the verb.
Therefore, thePDA discourages inefficient verb-initiated backwardsearches for role-fillers even when particles aremissing.Unlike LUTE (Shimazu 1983), the PDA's generic role-filling process does not rely on the presence ofparticles.
To each slot of each action frame, acceptablefiller types are attached.
When particles are missing,the role-filling rule matches the object types of rolefillers against the information attached to actionframes.
The object types in each domain are organizedin a hierarchy, and frame slots are allowed to point toany level in the hierarchy.Verbs with multiple meanings are disambiguated bystarting out with a set of action frames (e.g.
a2 and a3)and discarding a frame if a given phrase cannot fill anyslot of the frame.The PDA's processes can be summarized as follows:1.
Grab a phrase bottom-up using syntacticand semantic word classes.
Build an objectframe if applicable.2.
Recall all expectations (action-role pairs)attached to the phrase.3.4.If a particle follows, use the particle torefine the expectations attached to thephrase.Take the intersection of the old and newexpectations.5.
If the intersection is empty, set a flag.6.7.If this is a verb phrase and the flag is up,invoke the generic role-filling process.Else if this is the end of a simplesentence, build an action frame usingforward expectations.8.
Otherwise go back to Step 1.To achieve extensibility and flexibility, ideas such asthe detachment of control structure from the wordlevel, and the combination of top-down and bottom-upprocessing have been incorporated.SIMULTANEOUS GENERATORCertain syntactic features of the source text canserve as functionally relevant features of the situationbeing described in the source text.
Preservation ofthese features often helps the meaning and the nuanceto be reproduced.
However, knowledge-based systemsdiscard the syntax of the original text.
In other words,the information about the syntactic style of the sourcetext, such as the phrase order and the syntactic lassesof the original words, is not found in the internalrepresentation.
Furthermore, inferred role fillers, causalconnections, and events are generated isregarding thebrevity of the original text.
For example, thegenerator built by the Electrotechnical Laboratory ofJapan (Ishizaki 1983), which produces Japanese textsfrom the conceptual representation based on MOPs(Schank 1982), generates a pronoun whenever thesame noun is seen the second time.
Disregarding theoriginal sentence order, the system determines theorder using causal chains.
Moreover, the subject andobject are often omitted from the target sentence toprevent wordiness.Unl ike other knowledge-based systems, JETR canpreserve the syntax of  the original text, and it does sow i thout  bu i ld ing  the source- language tree.
Thegeneration algorithm is based on the observation thathuman translators do not have to wait until the end ofthe sentence to start translating the sentence.
A humantranslator can start translating phrases as he receivesthem one at a t ime and can apply partial syntax-transfer ules as soon as he notices a phrase sequencewhich is ungrammatical in the target language.Verb Deletion:Shio o Ilikiniku hi.Mizu wa nabe hi.SaJt on ground meat.As for the water, in a poLPar~cle Deletion:Hikiniku, shio o furu.
~ Ground meat, sprinkle sailWord Order Preservation:o-kina fukai nabe ~ big deep potfukai o-kina nabe ~ deep big potLe~cal ~nveriance:200 g no hikiniku oitameru.
Kosho- ohikiniku ni futtesusumeru.Stir-fry 200g of groundmeat.
Sprinkle pepper onthe ground meat;, serve.2009 no hikiniku oitameru.
Kosho- osore ni futte susumeru.Stir-fry 200g of groundmeat.
Sprinkle pepperon it; serve.Figure 4.
Style Preservation In the GeneratorThe generator does not go through the completesemantic representation of each sentence built by theother components of the system.
As soon as a phraseis processed by the PDA, the generator eceives thephrase along with its semantic role and startsgenerating the phrase if it is unambiguous.
Thus thegenerator can easily distinguish between inferredinformation and information explicitly present in the27source text.
The generator and not the PDA calls thecontext analyzer to obtain missing information thatare needed to translate grammatical Japanese sentencesinto grammatical English sentences.
No other inferredinformation is generated.
A preposition is notgenerated for a phrase which is lacking a particle, andan inferred verb is not generated for a verb-lesssentence.
Because the generator has access to theactual words in the source phrase, it is able toreproduce frequent occurrences of particular lexicalitems.
And the original word order is preserved asmuch as possible.
Therefore, the generator is able topreserve idiolects, emphases, lengths, ellipses, syntaxerrors and ambiguities due to missing information.Examples of target sentences for special cases areshown in Figure 4.To achieve structural invariance, phrases are outputas soon as possible without violating the Englishphrase order.
In other words, the generator pretendsthat incoming phrases are English phrases, andwhenever an ungrammatical phrase sequence isdetected, the new phrase is saved in one of threequeues: SAVED-PREPOSITIONAL, SAVED-REFINER,and SAVED-OBJECT, As long as no violation of theEnglish phrase order is detected or expected, thephrases are generated immediately.
Therefore, nosource-language tree needs to be constructed, and nostructural information needs to be stored in thesemantic representation f the complete sentence.To prevent awkwardness, a small knowledge basewhich relates source language idioms to those of thetarget language is being used by JETR; however, oneproblem with the generator is that it concentrates toomuch on information preservation, and the targetsentences are awkward at times.
Currently, the systemcannot decide when to sacrif ice informationpreservation.
Future research should examine theability of human transla~rs to determine the importantaspects of the source text.INSTRA: Tile CONTEXT ANALYZERThe context analyzer component of JETR is calledINSTRA (INSTRuction Analyzer).
The goal of INSTRAis to aid the other components in the following ways:I.
Keep track of the changes in object typesand forward expectations as objects aremodified by various modifiers and actions.. Resolve pronoun references so that correctEnglish pronouns can be generated andexpectations and object types can beassociated with pronouns.. Resolve object references o that correctexpectations and object types can beassociated with objects and consequentlythe article and the number of each nouncan be determined.4.
Choose among the multiple interpretationsof a sentence produced by the PDA..
Fill ellipses when necessary so that well-formed English sentences can begenerated.In knowledge-based systems, the context analyzer isdesigned with the goal of natural-languageunderstanding in mind; therefore, object and pronounreferences are resolved, and ellipses are filled as a byproduct of understanding the input text.
However,some human translators claim that they do not alwaysunderstand the texts they translate (Slocum 1985).Moreover, knowledge-based translation systems areless practical than systems based on direct and transfermethods.
Wilks (1973) states that "...it may bepossible to establish a level of understandingsomewhat short of that required for question-answeringand other intelligent behaviors."
Althoughidentifying the level of understanding required ingeneral by a machine translation system is difficult,the.
level clearly depends on the languages, the texttype and the tasks involved in translation.
INSTRAwas designed with the goal of identifying the level ofunderstanding required in translating instructionbooklets from Japanese to English.A unique characteristic of instruction booklets isthat every action produces a clearly defined resultingstate which is a transformed object or a collection oftransformed objects that arc likely to be referenced bylater actions.
For example, when salt is dissolved intowater, the salty water is the result.
When a screw isturned, the screw is the result.
When an object isplaced into liquid, the object, the liquid, the containerthat contains the liquid, and everthing else in thecontainer are the results.
INSTRA keeps a chain of theresulting states of the actions.
INSTRA's five tasks alldeal with searches or modifications of the results inthe chain.- bgreoients -OBJ RICEV~IT 3 CUPS~ALIAS INGOOBJ WING~DJ CHICKEI~MT 100 TO 120 GRAMS~LIAS ING1OBJ EGGV~MT 4~,LIAS ING2OBJ BAMBOO:SHOOT~DJ BOILEDV~.MT 40 GRAMSU~IAS ING3OBJ ONIONV~.DJ SMALL~AMT I~LIAS ING4OBJ SHIITAKE:MUSHROOMV~DJ FRESH~AMT 2~ALIAS INGSOEJ LAVERV~MT AN APPROPRIATE AMOUNT~,LIAS ING6OBJ MITSUBA'tAM'T ASMALL AMOUntS  ING7- the rk:e is bo\]h~:l -STEP10BJ RICE~,LIAS INGOV~T I~EFPLURAL T- the chicken, onion, bamboo shoots, mushrooms and mitsuba te cut.STEP20BJ CHICKEN'tALIAS INGI~RT '1~REF PLURAL TSTEP20BJ ONION~IAS ING4~ARTSTEP20BJ BAMBOO:SHOOT ~ALIAS ING3IART ~REFPLURAL TSTEP2 08J SHIITAKE:MUSHROOM~ FRESHV~LIAS ING5~RTREFPLURAL TSTEP20BJ MITSUBAV~J.IAS INGT~ARTFigure S. Chain or State= Used by INSTRA28To keep track of the state of each object, the objecttype and expectations of the object are changedwhenever certain modifiers are found.
Similarly, at theend of each sentence, 1) the object frames representingthe result objects are extracted from the frame, 2) eachresult object is given a unique name, and 3) the typeand expectations are changed if necessary and areattached to the unique name.
To identify the result ofeach action, information about what results from theaction is attached to each frame.
The result objects areadded to the end of the chain which may alreadycontain the ingredients or object components.
Anexample of a chain of the resulting states is shown inFigure 5.In instructions, a pronoun always refers to the resultof the previous action.
Therefore, for each pronounreference, the unique name of the object at the end ofthe chain is returned along with the information aboutthe number (plural or singular) of the object.For an object reference, INSTRA receives an objectframe, the chain is searched backwards for a match, andits unique name and information about its number arereturned.
INSTRA uses a set of rules that takes intoaccount he characteristics of modifiers in instructionsto determine whether two objects match.
Objectreference is important also in disambiguating itemparts.
When JETR encounters an item part that needsto be disambiguated, it goes through the chain ofresults to find the item which has the part and retrievesan appropriate translation equivalent.
The system usesadditional specialized rules for step number eferencesand divided objects.Ellipses are filled by searching through the chainbackwards for objects whose types are accepted by thecorresponding frame slots.
To preserve semantic,pragmatic and structural information, ellipses are filledonly when 1) missing information is needed togenerate grammatical target sentences, 2) INSTRA mustchoose among the multiple interpretations of asentence produced by the PDA, or 3) the result of anaction is needed.The domain-specific knowledge is stated solely interms of action frames and object types.
INSTRAaccomplishes the five tasks I) without pre-editing andpost-editing, 2) without relying on the user except inspecial cases involving unknown words, and 3)without fully understanding the text.
INSTRA assumesthat the user is monolingual.
Because the methodrefrains from using inferences in unnecessary cases,the semantic and pragmatic information contained inthe source text can be preserved.CONCLUSIONSThis paper has presented a robust expectation-basedapproach to machine translation which does not viewmachine translation as a testhod for AI.
The paper hasshown the need to consider problems unique tomachine translation such as preservation of syntaciteand semantic information contained in grammatical swell as ungrammatical sentences.The integration of the forward expectation-refinement process, the interleaved generationtechnique and the state-change-based processing hasled to the construction of an extensible, flexible andefficient system.
Although JETR is designed totranslate instruction booklets, the general algorithmused by the analyzer and the generator are applicableto other kinds of text.
JETR is written in UCI LISP ona DEC system 20/20.
The control structure consists ofroughly 5500 lines of code.
On the average it takesonly 1 CPU second to process a simple sentence.JETR has successfully translated published recipestaken from (Ishikawa 1975, Murakami 1978) and aninstruction booklet accompanying the Hybrid-H239watch (Hybrid) in addition to hundreds of test texts.Currently the dictionary and the knowledge base arebeing extended to translate more texts.Sample translations produced by JETR are found inthe appendix at the end of the paper.REFERENCESAAT.
1984.
Fujitsu has 2-way Translation System.AAT Report 66.
Advanced AmericanTechnology, Los Angeles, California.CarboneU, J. G.; Cullingford, R. E. and Gershman, A.G. 1981.
Steps Toward Knowledge-BasedMachine Translation.
IEEE Transaction onPattern Analysis and Machine IntelligencePAMI, 3(4).Cullingford, R. E. 1976.
The Application of Script-Based Knowledge in an Integrated StoryUnderstanding System.
Proceedings ofCOLING-1976.Granger, R.; Meyers, A.; Yoshii, R. and Taylor, G.1983.
An Extensible Natural LanguageUnderstanding System.
Proceedings of theArtificial Intelligence Conference, OaklandUniversity, Rochester, Michigan.Hybrid.
Hybrid--cal.
H239 Watch Instruction Booklet.Seiko, Tokyo, Japan.Ibuki, J; et.
al.
1983.
Japanese-to-English TitleTranslation System, TITRAN - Its Outline andthe Handling of Special Expressions in Titles.Journal of Information Processing, 6(4): 231-238.Ishikawa, K. 1975.
Wakamuki Hyoban Okazu 100 Sen.Shufu no Tomo, Tokyo, Japan.Ishizakl, S. 1983.
Generation of Japanese Sentencesfrom Conceptual Representation.
Proceedingsof IJCAI-1983.Lebowitz, M. 1983.
Memory-Based Parsing.
ArtificialIntelligence, 21: 363-404.Murakami, A.
1978.
Futari no Ryori to Kondate.Shufu no Tomo, Tokyo, Japan.Nitta, H. 1982.
A Heuristic Approach to English-into-Japanese Machine Translation.
Proceedings ofCOLING-1982.29Saino, T. 1983.
Jitsuyoka ?
Ririku Suru ShizengengoShori-Gijutsu.
Nikkei Computer, 39: 55-75.Schank, R. C. and Lytinen, S. 1982.
Representationand Translation.
Research Report 234.
YaleUniversity, New Haven, Connecticut.Shimazu, A; Naito, A. and Nomura, H. 1983.
JapaneseLanguage Semantic Analyzer Based on anExtended Case Frame Model.
Proceedings ofIJCAI-1983.Slocum, J.
1985.
A Survey of Machine Translation: ItsHistory, Current Status and Future Prospects.Computational Linguistics, 11(1): 1-17.Wilks, Y.
1973.
An Artificial Intelligence Approach toMachine Translation.
In: Schank, R. C. andColby, K., Eds., Computer Models of Thoughtand Language.
W. H. Freeman, San Francisco,California: 114-151.Yang, C. J.
1981.
High Level Memory Structures andText Coherence in Translation.
Proceedings ofLICAI-1981.Yoshii, R. 1986.
JETR: A Robust Machine TranslationSystem.
Doctoral dissertation, University ofCalifornia, Irvine, California.APPENDIX  - EXAMPLESNOTE: Comments are surrounded by angle brackets.EXAMPLE 1SOURCE TEXT: (Hybrid)Anarogu bu no jikoku:awase.60 pun shu-sei.Ryu-zu o hikidashite migi ?
subayaku 2 kurikkumawasu to cho-shin ga 1 kaiten shire 60 pun susumu.Mata gyaku hi, hidari e subayaku 2 kurikku mawasu tocho-shin ga I kaiten shim 60 pun modoru.
Ryu-zu o Ikurikku mawasu tabigoto ni pitt to iu kakuninon gadcru.TARGET TEXT:The time setting of the analogue part.The 60 minute adjustmentPull out the crown; when you quickly turn it clockwise2 clicks, the minute hand turns one cycle and advances60 minutes.
Also conversely, when you quickly turn itcounterclockwise 2 clicks, the minute hand turns onecycle and goes back 60 minutes.
Everytime you turnthe crown I click, the confirmation alarm "peep" goesoff.EXAMPLE 2SOURCE TEXT: (Murakami 1978)Tori no karaage.4 ninmac.<<ingredients need not be separated by punctuation>>honetsuki butsugiri no keiniku 500 guramujagaimo 2 kokyabetsu 2 maitamanegi 1/2 koremon 1/2 kopaseri.
(I).Keiniku ni sho-yu o-saji 2 o karamete 1 jikan oku.
(2).Jagaimo wa yatsuwari ni shire kara kawa o muki mizuni I0 pun hodo sarasu.
<<wa is an ambiguousparticle>>(3).Tamanegi wa usugiri ni shire mizu ni sarashi kyabetsuwa katai tokoro o sogitotte hate ni 3 to-bun shite karahosoku kizami mizu ni sarasu.
(4).Chu-ka:nabe ni abura o 6 bunme hodo here chu-bi nikakeru.
(5).Betsu nabe ni yu o wakashi jagaimo no rnizuko o kittc2 fun hodo yude zaru ni agete mizuke o kiru.(6).
(1) no keiniku no shirnke o kitte komugiko o usukumabusu.
(7).Jagaimo ga atsui uchini ko-on no abura ni ire ukiagattekita ra chu-bi ni shi ~tsuneiro ni irozuitc kita ratsuyobi ni shite kararito sasete ageami dotebayaku sukuiage agcdai ni totte abura o kiru.
(8).Keiniku o abura ni ire ukiagatte kita ra yowame nochu-bi ni shite 2 fun hodo kakem naka made hi o to-shi tsuyobi ni shim kitsuneiro ni agcru.
<<hi o to-shiis idiomatic>>(9).
(3) no tamanegi, kyabetsu no mizuke o kiru.
Kyabetsuo utsuwa ni shiite keiniku o mori jagaimo to tamanegio soe lemon to paseri o ashirau.TARGET TEXT:Fried chicken.4 servings.500 grams of chopped chicken2 potatoes2 leaves of cabbage1/2 onionI/2 lemonparsely(1).All over the chicken place 2 tablespoons of soy sauce;let alne 1 hour.30(2).As for the potatoes, after you cut them into eightpieces, remove the skin; place about 10 minutes inwater.
(3).As for the onion, cut into thin slices; place in water.As for the cabbage, remove the hard part; after you cutthem vertically into 3 equal pieces, cut into finepieces; place in water.
(4).In a wok, place oil about 6110 full; put over mediumheat.
(5).In a different pot, boil hot water; remove the moistureof the potatoes; boil about 2 minutes; remove to abamboo basket; remove the moisture.
(6).Remove the moisture of the chicken of (1); sprinkleflour lightly.
(7).While the potatoes are hot, place in the hot oil; whenthey float up, switch to medium heat; when they turngolden brown, switch to strong heat; make themcrispy; with a lifter drainer, scoop up quickly; removeto a basket; remove the oil.
(s).Place the chicken in the oil; when they float up,switch to low medium heat; put over the heat about 2minutes; completely let the heat work through; switchto strong heat; fry golden brown.
(9).Remove the moisture of the onion of (3) and thecabbage of (3); spread the cabbage on a dish; serve thechicken; add the potatoes and the onion; add the lemonand the parsely to garnish the dish.31
