ProAlign: Shared Task System DescriptionDekang Lin and Colin CherryDepartment of Computing ScienceUniversity of AlbertaEdmonton, Alberta, Canada, T6G 2E8{lindek,colinc}@cs.ualberta.caAbstractProAlign combines several different ap-proaches in order to produce high quality wordword alignments.
Like competitive linking,ProAlign uses a constrained search to find highscoring alignments.
Like EM-based methods,a probability model is used to rank possiblealignments.
The goal of this paper is to givea bird?s eye view of the ProAlign system toencourage discussion and comparison.1 Alignment Algorithm at a GlanceWe have submitted the ProAlign alignment system tothe WPT?03 shared task.
It received a 5.71% AER onthe English-French task and 29.36% on the Romanian-English task.
These results are with the no-null data; ouroutput was not formatted to work with explicit nulls.ProAlign works by iteratively improving an align-ment.
The algorithm creates an initial alignment us-ing search, constraints, and summed ?2 correlation-basedscores (Gale and Church, 1991).
This is similar to thecompetitive linking process (Melamed, 2000).
It thenlearns a probability model from the current alignment,and conducts a constrained search again, this time scor-ing alignments according to the probability model.
Theprocess continues until results on a validation set begin toindicate over-fitting.For the purposes of our algorithm, we view an align-ment as a set of links between the words in a sen-tence pair.
Before describing the algorithm, we will de-fine the following notation.
Let E be an English sen-tence e1, e2, .
.
.
, em and let F be a French sentencef1, f2, .
.
.
, fn.
We define a link l(ei, fj) to exist if ei andfj are a translation (or part of a translation) of one an-other.
We define the null link l(ei, f0) to exist if ei doesnot correspond to a translation for any French word in F .The null link l(e0, fj) is defined similarly.
An alignmentA for two sentences E and F is a set of links such that ev-ery word in E and F participates in at least one link, anda word linked to e0 or f0 participates in no other links.
Ife occurs in E x times and f occurs in F y times, we saythat e and f co-occur xy times in this sentence pair.ProAlign conducts a best-first search (with constantbeam and agenda size) to search a constrained space ofpossible alignments.
A state in this space is a partialalignment, and a transition is defined as the addition ofa single link to the current state.
Any link which wouldcreate a state that does not violate any constraint is con-sidered to be a valid transition.
Our start state is the emptyalignment, where all words in E and F are implicitlylinked to null.
A terminal state is a state in which no morelinks can be added without violating a constraint.
Ourgoal is to find the terminal state with the highest proba-bility.To complete this algorithm, one requires a set of con-straints and a method for determining which alignment ismost likely.
These are presented in the next two sections.The algorithm takes as input a set of English-French sen-tence pairs, along with dependency trees for the Englishsentences.
The presence of the English dependency treeallows us to incorporate linguistic features into our modeland linguistic intuitions into our constraints.2 ConstraintsThe model used for scoring alignments has no mecha-nism to prevent certain types of undesirable alignments,such as having all French words align to the same En-glish word.
To guide the search to correct alignments, weemploy two constraints to limit our search for the mostprobable alignment.
The first constraint is the one-to-oneconstraint (Melamed, 2000): every word (except the nullwords e0 and f0) participates in exactly one link.The second constraint, known as the cohesion con-straint (Fox, 2002), uses the dependency tree (Mel?c?uk,1987) of the English sentence to restrict possible linkcombinations.
Given the dependency tree TE and a (par-tial) alignment A, the cohesion constraint requires thatphrasal cohesion is maintained in the French sentence.
Iftwo phrases are disjoint in the English sentence, the align-ment must not map them to overlapping intervals in theFrench sentence.
This notion of phrasal constraints onalignments need not be restricted to phrases determinedfrom a dependency structure.
However, the experimentsconducted in (Fox, 2002) indicate that dependency treesdemonstrate a higher degree of phrasal cohesion duringtranslation than other structures.Consider the partial alignment in Figure 1.
The mostprobable lexical match for the English word to is theFrench word a`.
When the system attempts to link to anda`, the distinct English phrases [the reboot] and [the hostto discover all the devices] will be mapped to intervalsin the French sentence, creating the induced phrasal in-tervals [a` .
.
.
[re?initialisation] .
.
.
pe?riphe?riques].
Re-gardless of what these French phrases will be after thealignment is completed, we know now that their intervalswill overlap.
Therefore, this link will not be added to thepartial alignment.The?reboot?causes?the?host?to?discover?all?the?devices?det?
subj?
det?
subj?aux?pre?det?obj?mod??
la?Suite?
r?initialisation  ,?
l'  h?te?rep?re?tous?les?p?riph?riques?after?to?
the?
reboot?
the?host?
locate?
all?
the?
peripherals?1?
2?
3?
4?
5?6?
7?
8?
9?
10?1?
2?
3?
4?
5?6?7?
8?
9?10?
11?Figure 1: An Example of Cohesion ConstraintTo define this notion more formally, let TE(ei) bethe subtree of TE rooted at ei.
The phrase span ofei, spanP(ei, TE , A), is the image of the English phraseheaded by ei in F given a (partial) alignment A. Moreprecisely, spanP(ei, TE , A) = [k1, k2], wherek1 = min{j|l(u, j) ?
A, eu ?
TE(ei)}k2 = max{j|l(u, j) ?
A, eu ?
TE(ei)}The head span is the image of ei itself.
We definespanH(ei, TE , A) = [k1, k2], wherek1 = min{j|l(i, j) ?
A}k2 = max{j|l(i, j) ?
A}In Figure 1, for the node reboot, the phrase span is[4,4] and the head span is also [4,4]; for the node discover(with the link between to and a` in place), the phrase spanis [2,11] and the head span is the empty set ?.With these definitions of phrase and head spans, we de-fine two notions of overlap, originally introduced in (Fox,2002) as crossings.
Given a head node eh and its modi-fier em, a head-modifier overlap occurs when:spanH(eh, TE , A) ?
spanP(em, TE , A) 6= ?Given two nodes em1 and em2 which both modify thesame head node, a modifier-modifier overlap occurswhen:spanP(em1 , TE , A) ?
spanP(em2 , TE , A) 6= ?Following (Fox, 2002), we say an alignment is co-hesive with respect to TE if it does not introduceany head-modifier or modifier-modifier overlaps.
Forexample, the alignment A in Figure 1 is not cohe-sive because spanP (reboot, TE , A) = [4, 4] intersectsspanP (discover, TE , A) = [2, 11].
Since both rebootand discover modify causes, this creates a modifier-modifier overlap.
One can check for constraint viola-tions inexpensively by incrementally updating the vari-ous spans as new links are added to the partial alignment,and checking for overlap after each modification.
Moredetails on the cohesion constraint can be found in (Linand Cherry, 2003).3 Probability ModelWe define the word alignment problem as finding thealignment A that maximizes P (A|E,F ).
ProAlign mod-els P (A|E,F ) directly, using a different decompositionof terms than the model used by IBM (Brown et al,1993).
In the IBM models of translation, alignments ex-ist as artifacts of a stochastic process, where the wordsin the English sentence generate the words in the Frenchsentence.
Our model does not assume that one sentencegenerates the other.
Instead it takes both sentences asgiven, and uses the sentences to determine an alignment.An alignment A consists of t links {l1, l2, .
.
.
, lt}, whereeach lk = l(eik , fjk) for some ik and jk.
We will refer toconsecutive subsets of A as lji = {li, li+1, .
.
.
, lj}.
Giventhis notation, P (A|E,F ) can be decomposed as follows:P (A|E,F ) = P (lt1|E,F ) =t?k=1P (lk|E,F, lk?11 )At this point, we factor P (lk|E,F, lk?11 ) to make com-putation feasible.
Let Ck = {E,F, lk?11 } represent thecontext of lk.
Note that both the context Ck and the linklk imply the occurrence of eik and fjk .
We can rewriteP (lk|Ck) as:P (lk|Ck) =P (lk, Ck)P (Ck)=P (Ck|lk)P (lk)P (Ck, eik , fjk)= P (lk|eik , fjk)?P (Ck|lk)P (Ck|eik , fjk)Here P (lk|eik , fjk) is link probability given a co-occurrence of the two words, which is similar in spirit toMelamed?s explicit noise model (Melamed, 2000).
Thisterm depends only on the words involved directly in thelink.
The ratio P (Ck|lk)P (Ck|eik ,fjk ) modifies the link probability,providing context-sensitive information.Ck remains too broad to deal with in practical sys-tems.
We will consider only a subset FT k of relevantfeatures of Ck.
We will make the Na?
?ve Bayes-style as-sumption that these features ft ?
FT k are conditionallyindependent given either lk or (eik , fjk).
This produces atractable formulation for P (A|E,F ):t?k=1?
?P (lk|eik , fjk)?
?ft?FTkP (ft |lk)P (ft |eik , fjk)?
?More details on the probability model used by ProAlignare available in (Cherry and Lin, 2003).3.1 Features used in the shared taskFor the purposes of the shared task, we use two featuretypes.
Each type could have any number of instantiationsfor any number of contexts.
Note that each feature typeis described in terms of the context surrounding a wordpair.The first feature type fta concerns surrounding links.It has been observed that words close to each other inthe source language tend to remain close to each other inthe translation (S. Vogel and Tillmann, 1996).
To capturethis notion, for any word pair (ei, fj), if a link l(ei?
, fj?
)exists within a window of two words (where i?2 ?
i?
?i+2 and j?2 ?
j?
?
j+2), then we say that the featurefta(i?
i?, j ?
j?, ei?)
is active for this context.
We referto these as adjacency features.The second feature type ftd uses the English parse treeto capture regularities among grammatical relations be-tween languages.
For example, when dealing with Frenchand English, the location of the determiner with respectto its governor is never swapped during translation, whilethe location of adjectives is swapped frequently.
For anyword pair (ei, fj), let ei?
be the governor of ei, and letrel be the relationship between them.
If a link l(ei?
, fj?
)exists, then we say that the feature ftd(j ?
j?, rel) is ac-tive for this context.
We refer to these as dependencyfeatures.Take for example Figure 2 which shows a partial align-ment with all links completed except for those involvingthe.
Given this sentence pair and English parse tree, wecan extract features of both types to assist in the align-ment of the1.
The word pair (the1, l?)
will have an activeadjacency feature fta(+1,+1, host) as well as a depen-dency feature ftd(?1, det).
These two features will worktogether to increase the probability of this correct link.thehostdiscoversallthedevicesdetsubjpredetobjl'  h?terep?retouslesp?riph?riques123451234566thehostlocatealltheperipheralsFigure 2: Feature Extraction ExampleIn contrast, the incorrect link (the1, les) will have onlyftd(+3, det), which will work to lower the link probabil-ity, since most determiners are located before their gov-ernors.3.2 Training the modelSince we always work from a current alignment, trainingthe model is a simple matter of counting events in thecurrent alignment.
Link probability is the number of timetwo words are linked, divided by the number of timesthey co-occur.
The various feature probabilities can becalculated by also counting the number of times a featureoccurs in the context of a linked pair of words, and thenumber of times the feature is active for co-occurrencesof the same word pair.Considering only a single, potentially noisy alignmentfor a given sentence pair can result in reinforcing errorspresent in the current alignment during training.
To avoidthis problem, we sample from a space of probable align-ments, as is done in IBM models 3 and above (Brownet al, 1993), and weight counts based on the likelihoodof each alignment sampled under the current probabilitymodel.
To further reduce the impact of rare, and poten-tially incorrect events, we also smooth our probabilitiesusing m-estimate smoothing (Mitchell, 1997).4 Multiple AlignmentsThe result of the constrained alignment search is a high-precision, word-to-word alignment.
We then relax theword-to-word constraint, and use statistics regarding col-locations with unaligned words in order to make many-to-one alignments.
We also employ a further relaxed link-ing process to catch some cases where the cohesion con-straint ruled out otherwise good alignments.
These auxil-iary methods are currently not integrated into our searchor our probability model, although that is certainly a di-rection for future work.5 ConclusionsWe have presented a brief overview of the major ideasbehind our entry to the WPT?03 Shared Task.
Primaryamong these ideas are the use of a cohesion constraint insearch, and our novel probability model.AcknowledgmentsThis project is funded by and jointly undertaken with SunMicrosystems, Inc. We wish to thank Finola Brady, BobKuhns and Michael McHugh for their help.
We also wishto thank the WPT?03 reviewers for their helpful com-ments.ReferencesP.
F. Brown, V. S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?312.Colin Cherry and Dekang Lin.
2003.
A probabilitymodel to improve word alignment.
Submitted.Heidi J.
Fox.
2002.
Phrasal cohesion and statisticalmachine translation.
In 2002 Conference on Empiri-cal Methods in Natural Language Processing (EMNLP2002), pages 304?311.W.A.
Gale and K.W.
Church.
1991.
Identifying wordcorrespondences in parallel texts.
In 4th Speechand Natural Language Workshop, pages 152?157.DARPA, Morgan Kaufmann.Dekang Lin and Colin Cherry.
2003.
Word alignmentwith cohesion constraint.
Submitted.I.
Dan Melamed.
2000.
Models of translational equiv-alence among words.
Computational Linguistics,26(2):221?249, June.Igor A. Mel?c?uk.
1987.
Dependency syntax: theory andpractice.
State University of New York Press, Albany.Tom Mitchell.
1997.
Machine Learning.
McGraw Hill.H.
Ney S. Vogel and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In 16th In-ternational Conference on Computational Linguistics,pages 836?841, Copenhagen, Denmark, August.
