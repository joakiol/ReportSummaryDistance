Proceedings of the 10th Conference on Parsing Technologies, pages 121?132,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsOn the Complexity of Non-Projective Data-Driven Dependency ParsingRyan McDonaldGoogle Inc.76 Ninth AvenueNew York, NY 10028ryanmcd@google.comGiorgio SattaUniversity of Paduavia Gradenigo 6/AI-35131 Padova, Italysatta@dei.unipd.itAbstractIn this paper we investigate several non-projective parsing algorithms for depen-dency parsing, providing novel polynomialtime solutions under the assumption thateach dependency decision is independent ofall the others, called here the edge-factoredmodel.
We also investigate algorithms fornon-projective parsing that account for non-local information, and present several hard-ness results.
This suggests that it is unlikelythat exact non-projective dependency pars-ing is tractable for any model richer than theedge-factored model.1 IntroductionDependency representations of natural language area simple yet flexible mechanism for encoding wordsand their syntactic dependencies through directedgraphs.
These representations have been thoroughlystudied in descriptive linguistics (Tesnie`re, 1959;Hudson, 1984; Sgall et al, 1986; Mel?c?uk, 1988) andhave been applied in numerous language process-ing tasks.
Figure 1 gives an example dependencygraph for the sentence Mr. Tomash will remain as adirector emeritus, which has been extracted from thePenn Treebank (Marcus et al, 1993).
Each edge inthis graph represents a single syntactic dependencydirected from a word to its modifier.
In this rep-resentation all edges are labeled with the specificsyntactic function of the dependency, e.g., SBJ forsubject and NMOD for modifier of a noun.
To sim-plify computation and some important definitions,an artificial token is inserted into the sentence as theleft most word and will always represent the root ofthe dependency graph.
We assume all dependencygraphs are directed trees originating out of a singlenode, which is a common constraint (Nivre, 2005).The dependency graph in Figure 1 is an exam-ple of a nested or projective graph.
Under the as-sumption that the root of the graph is the left mostword of the sentence, a projective graph is one wherethe edges can be drawn in the plane above the sen-tence with no two edges crossing.
Conversely, anon-projective dependency graph does not satisfythis property.
Figure 2 gives an example of a non-projective graph for a sentence that has also beenextracted from the Penn Treebank.
Non-projectivityarises due to long distance dependencies or in lan-guages with flexible word order.
For many lan-guages, a significant portion of sentences requirea non-projective dependency analysis (Buchholz etal., 2006).
Thus, the ability to learn and infer non-projective dependency graphs is an important prob-lem in multilingual language processing.Syntactic dependency parsing has seen a num-ber of new learning and inference algorithms whichhave raised state-of-the-art parsing accuracies formany languages.
In this work we focus on data-drivenmodels of dependency parsing.
These modelsare not driven by any underlying grammar, but in-stead learn to predict dependency graphs based ona set of parameters learned solely from a labeledcorpus.
The advantage of these models is that theynegate the need for the development of grammarswhen adapting the model to new languages.One interesting class of data-driven models are121Figure 1: A projective dependency graph.Figure 2: Non-projective dependency graph.those that assume each dependency decision is in-dependent modulo the global structural constraintthat dependency graphs must be trees.
Such mod-els are commonly referred to as edge-factored sincetheir parameters factor relative to individual edgesof the graph (Paskin, 2001; McDonald et al,2005a).
Edge-factored models have many computa-tional benefits, most notably that inference for non-projective dependency graphs can be achieved inpolynomial time (McDonald et al, 2005b).
The pri-mary problem in treating each dependency as in-dependent is that it is not a realistic assumption.Non-local information, such as arity (or valency)and neighbouring dependencies, can be crucial toobtaining high parsing accuracies (Klein and Man-ning, 2002; McDonald and Pereira, 2006).
How-ever, in the data-driven parsing setting this can bepartially adverted by incorporating rich feature rep-resentations over the input (McDonald et al, 2005a).The goal of this work is to further our currentunderstanding of the computational nature of non-projective parsing algorithms for both learning andinference within the data-driven setting.
We start byinvestigating and extending the edge-factored modelof McDonald et al (2005b).
In particular, we ap-peal to the Matrix Tree Theorem for multi-digraphsto design polynomial-time algorithms for calculat-ing both the partition function and edge expecta-tions over all possible dependency graphs for a givensentence.
To motivate these algorithms, we showthat they can be used in many important learningand inference problems including min-risk decod-ing, training globally normalized log-linear mod-els, syntactic language modeling, and unsupervisedlearning via the EM algorithm ?
none of which havepreviously been known to have exact non-projectiveimplementations.We then switch focus to models that account fornon-local information, in particular arity and neigh-bouring parse decisions.
For systems that model ar-ity constraints we give a reduction from the Hamilto-nian graph problem suggesting that the parsing prob-lem is intractable in this case.
For neighbouringparse decisions, we extend the work of McDonaldand Pereira (2006) and show that modeling verticalneighbourhoods makes parsing intractable in addi-tion to modeling horizontal neighbourhoods.
A con-sequence of these results is that it is unlikely thatexact non-projective dependency parsing is tractablefor any model assumptions weaker than those madeby the edge-factored models.1.1 Related WorkThere has been extensive work on data-driven de-pendency parsing for both projective parsing (Eis-ner, 1996; Paskin, 2001; Yamada and Matsumoto,2003; Nivre and Scholz, 2004; McDonald et al,2005a) and non-projective parsing systems (Nivreand Nilsson, 2005; Hall and No?va?k, 2005; McDon-ald et al, 2005b).
These approaches can often beclassified into two broad categories.
In the first cat-egory are those methods that employ approximateinference, typically through the use of linear timeshift-reduce parsing algorithms (Yamada and Mat-sumoto, 2003; Nivre and Scholz, 2004; Nivre andNilsson, 2005).
In the second category are thosethat employ exhaustive inference algorithms, usu-ally by making strong independence assumptions, asis the case for edge-factored models (Paskin, 2001;McDonald et al, 2005a; McDonald et al, 2005b).Recently there have also been proposals for exhaus-tive methods that weaken the edge-factored assump-tion, including both approximate methods (McDon-ald and Pereira, 2006) and exact methods through in-teger linear programming (Riedel and Clarke, 2006)or branch-and-bound algorithms (Hirakawa, 2006).For grammar based models there has been limitedwork on empirical systems for non-projective pars-ing systems, notable exceptions include the workof Wang and Harper (2004).
Theoretical studies ofnote include the work of Neuhaus and Bo?ker (1997)showing that the recognition problem for a mini-122mal dependency grammar is hard.
In addition, thework of Kahane et al (1998) provides a polynomialparsing algorithm for a constrained class of non-projective structures.
Non-projective dependencyparsing can be related to certain parsing problemsdefined for phrase structure representations, as forinstance immediate dominance CFG parsing (Bartonet al, 1987) and shake-and-bake translation (Brew,1992).Independently of this work, Koo et al (2007) andSmith and Smith (2007) showed that the Matrix-Tree Theorem can be used to train edge-factoredlog-linear models of dependency parsing.
Both stud-ies constructed implementations that compare favor-ably with the state-of-the-art.
The work of Meila?and Jaakkola (2000) is also of note.
In that studythey use the Matrix Tree Theorem to develop atractable bayesian learning algorithms for tree beliefnetworks, which in many ways are closely relatedto probabilistic dependency parsing formalisms andthe problems we address here.2 PreliminariesLet L = {l1, .
.
.
, l|L|} be a set of permissible syn-tactic edge labels and x = x0x1 ?
?
?xn be a sen-tence such that x0=root.
From this sentence we con-struct a complete labeled directed graph (digraph)Gx = (Vx, Ex) such that,?
Vx = {0, 1, .
.
.
, n}?
Ex = {(i, j)k | ?
i, j ?
Vx and 1 ?
k ?
|L|}Gx is a graph where each word in the sentence is anode, and there is a directed edge between every pairof nodes for every possible label.
By its definition,Gx is a multi-digraph, which is a digraph that mayhave more than one edge between any two nodes.Let (i, j)k represent the kth edge from i to j. Gx en-codes all possible labeled dependencies between thewords of x.
Thus every possible dependency graphof x must be a subgraph of Gx.Let i ?+ j be a relation that is true if and onlyif there is a non-empty directed path from node i tonode j in some graph under consideration.
A di-rected spanning tree1 of a graph G, that originates1A directed spanning tree is commonly referred to as a ar-borescence in the graph theory literature.out of node 0, is any subgraph T = (VT , ET ) suchthat,?
VT = Vx and ET ?
Ex?
?j ?
VT , 0 ?+ j if and only if j 6= 0?
If (i, j)k ?
ET , then (i?, j)k?/?
ET , ?i?
6= iand/or k?
6= k.Define T (G) as the set of all directed spanning treesfor a graph G. As McDonald et al (2005b) noted,there is a one-to-one correspondence between span-ning trees of Gx and labeled dependency graphsof x, i.e., T (Gx) is exactly the set of all possibleprojective and non-projective dependency graphs forsentence x.
Throughout the rest of this paper, wewill refer to any T ?
T (Gx) as a valid dependencygraph for a sentence x.
Thus, by definition, everyvalid dependency graph must be a tree.3 Edge-factored ModelsIn this section we examine the class of models thatassume each dependency decision is independent.Within this setting, every edge in an induced graphGx for a sentence x will have an associated weightwkij ?
0 that maps the kth directed edge from nodei to node j to a real valued numerical weight.
Theseweights represents the likelihood of a dependencyoccurring from word wi to word wj with label lk.Define the weight of a spanning tree T = (VT , ET )as the product of the edge weightsw(T ) =?
(i,j)k?ETwkijIt is easily shown that this formulation includesthe projective model of Paskin (2001) and the non-projective model of McDonald et al (2005b).The definition of wkij depends on the context inwhich it is being used.
For example, in the work ofMcDonald et al (2005b) it is simply a linear classi-fier that is a function of the words in the dependency,the label of the dependency, and any contextual fea-tures of the words in the sentence.
In a generativeprobabilistic model (such as Paskin (2001)) it couldrepresent the conditional probability of a word wjbeing generated with a label lk given that the wordbeing modified is wi (possibly with some other in-formation such as the orientation of the dependency123or the number of words betweenwi andwj).
We willattempt to make any assumptions about the formwkijclear when necessary.For the remainder of this section we discuss threecrucial problems for learning and inference whileshowing that each can be computed tractably for thenon-projective case.3.1 Finding the ArgmaxThe first problem of interest is finding the highestweighted tree for a given input sentence xT = argmaxT?T (Gx)?
(i,j)k?ETwkijMcDonald et al (2005b) showed that this can besolved in O(n2) for unlabeled parsing using theChu-Liu-Edmonds algorithm for standard digraphs(Chu and Liu, 1965; Edmonds, 1967).
Unlike mostexact projective parsing algorithms, which use effi-cient bottom-up chart parsing algorithms, the Chu-Liu-Edmonds algorithm is greedy in nature.
It be-gins by selecting the single best incoming depen-dency edge for each node j.
It then post-processesthe resulting graph to eliminate cycles and then con-tinues recursively until a spanning tree (or validdependency graph) results (see McDonald et al(2005b) for details).The algorithm is trivially extended to the multi-digraph case for use in labeled dependency parsing.First we note that if the maximum directed spanningtree of a multi-digraph Gx contains any edge (i, j)k,then we must have k = k?
= argmaxk wkij .
Oth-erwise we could simply substitute (i, j)k?in placeof (i, j)k and obtain a higher weighted tree.
There-fore, without effecting the solution to the argmaxproblem, we can delete all edges in Gx that do notsatisfy this property.
The resulting digraph is nolonger a multi-digraph and the Chu-Liu-Edmondsalgorithm can be applied directly.
The new runtimeis O(|L|n2).As a side note, the k-best argmax problem for di-graphs can be solved in O(kn2) (Camerini et al,1980).
This can also be easily extended to the multi-digraph case for labeled parsing.3.2 Partition FunctionA common step in many learning algorithms is tocompute the sum over the weight of all the possi-ble outputs for a given input x.
This value is oftenreferred to as the partition function due to its sim-ilarity with a value by the same name in statisticalmechanics.
We denote this value as Zx,Zx =?T?T (Gx)w(T ) =?T?T (Gx)?
(i,j)k?ETwki,jTo compute this sum it is possible to use the MatrixTree Theorem for multi-digraphs,Matrix Tree Theorem (Tutte, 1984): Let G be amulti-digraph with nodes V = {0, 1, .
.
.
, n} andedges E. Define (Laplacian) matrix Q as a (n +1)?
(n+1) matrix indexed from 0 to n. For all i andj, define:Qjj =?i6=j,(i,j)k?Ewkij & Qij =?i6=j,(i,j)k?E?wkijIf the ith row and column are removed from Q toproduce the matrixQi, then the sum of the weights ofall directed spanning trees rooted at node i is equalto |Qi| (the determinant of Qi).Thus, if we construct Q for a graph Gx, then the de-terminant of the matrix Q0 is equivalent to Zx.
Thedeterminant of an n?n matrix can be calculated innumerous ways, most of which takeO(n3) (Cormenet al, 1990).
The most efficient algorithms for cal-culating the determinant of a matrix use the fact thatthe problem is no harder than matrix multiplication(Cormen et al, 1990).
Matrix multiplication cur-rently has known O(n2.38) implementations and ithas been widely conjectured that it can be solved inO(n2) (Robinson, 2005).
However, most algorithmswith sub-O(n3) running times require constants thatare large enough to negate any asymptotic advantagefor the case of dependency parsing.
As a result, inthis work we use O(n3) as the runtime for comput-ing Zx.Since it takes O(|L|n2) to construct the matrix Q,the entire runtime to compute Zx is O(n3 + |L|n2).3.3 Edge ExpectationsAnother important problem for various learningparadigms is to calculate the expected value of eachedge for an input sentence x,?
(i, j)k?x =?T?T (Gx)w(T )?
I((i, j)k, T )124Input: x = x0x1 ?
?
?xn1.
Construct Q O(|L|n2)2. for j : 1 .. n O(n)3.
Q?jj = Qjj and Q?ij = Qij , 0 ?
?i ?
n O(n)4.
Qjj = 1 and Qij = 0, 0 ?
?i ?
n O(n)5. for i : 0 .. n & i 6= j O(n)6.
Qij = ?1 O(1)7.
Zx = |Q0| O(n3)8.
?
(i, j)k?x = wkijZx, ?1 ?
k ?
|L| O(|L|)9. end for10.
Qjj = Q?jj and Qij = Q?ij , 0 ?
?i ?
n O(n)11. end forFigure 3: Algorithm to calculate ?
(i, j)k?x inO(n5 + |L|n2).where I((i, j)k, T ) is an indicator function that isone when the edge (i, j)k is in the tree T .To calculate the expectation for the edge (i, j)k,we can simply eliminate all edges (i?, j)k?6= (i, j)kfrom Gx and calculate Zx.
Zx will now be equalto the sum of the weights of all trees that con-tain (i, j)k. A naive implementation to computethe expectation of all |L|n2 edges takes O(|L|n5 +|L|2n4), since calculating Zx takes O(n3 + |L|n2)for a single edge.
However, we can reduce this con-siderably by constructing Q a single time and onlymaking modifications to it when necessary.
An al-gorithm is given in Figure 3.3 that has a runtime ofO(n5 + |L|n2).
This algorithm works by first con-structing Q.
It then considers edges from the node ito the node j.
Now, assume that there is only a singleedge from i to j and that that edge has a weight of 1.Furthermore assume that this edge is the only edgedirected into the node j.
In this case Q should bemodified so that Qjj = 1, Qij = ?1, and Qi?j = 0,?i?
6= i, j (by the Matrix Tree Theorem).
The valueof Zx under this new Q will be equivalent to theweight of all trees containing the single edge from ito j with a weight of 1.
For a specific edge (i, j)k itsexpectation is simplywkijZx, since we can factor outthe weight 1 edge from i to j in all the trees that con-tribute to Zx and multiply through the actual weightfor the edge.
The algorithm then reconstructs Q andcontinues.Following the work of Koo et al (2007) and Smithand Smith (2007), it is possible to compute all ex-pectations in O(n3 + |L|n2) through matrix inver-sion.
To make this paper self contained, we reporthere their algorithm adapted to our notation.
First,consider the equivalence,?
logZx?wkij=?
logZx?Zx?Zx?wkij=1Zx?T?T (Gx)w(T )wkij?
I((i, j)k, T )As a result, we can re-write the edge expectations as,?
(i, j)k?
= Zxwkij?
logZx?wkij= Zxwkij?
log |Q0|?wkijUsing the chain rule, we get,?
log |Q0|?wkij=?i?,j??1?
log |Q0|?(Q0)i?j??(Q0)i?j?
?wkijWe assume the rows and columns of Q0 are in-dexed from 1 so that the indexes of Q and Q0 co-incide.
To calculate ?
(i, j)k?
when i, j > 0, we canuse the fact that ?
log |X|/Xij = (X?1)ji and that?
(Q0)i?j?/?wkij is non zero only when i?
= i andj?
= j or i?
= j?
= j to get,?
(i, j)k?
= Zxwkij [((Q0)?1)jj ?
((Q0)?1)ji]When i = 0 and j > 0 the only non zero term ofthis sum is when i?
= j?
= j and so?
(0, j)k?
= Zxwk0j((Q0)?1)jjZx and (Q0)?1 can both be calculated a single time,each taking O(n3).
Using these values, each expec-tation is computed in O(1).
Coupled with with thefact that we need to construct Q and compute theexpectation for all |L|n2 possible edges, in total ittakes O(n3 + |L|n2) time to compute all edge ex-pectations.3.4 Comparison with Projective ParsingProjective dependency parsing algorithms are wellunderstood due to their close connection to phrase-based chart parsing algorithms.
The work of Eis-ner (1996) showed that the argmax problem for di-graphs could be solved in O(n3) using a bottom-up dynamic programming algorithm similar to CKY.Paskin (2001) presented an O(n3) inside-outside al-gorithm for projective dependency parsing using theEisner algorithm as its backbone.
Using this al-gorithm it is trivial to calculate both Zx and each125Projective Non-Projectiveargmax O(n3 + |L|n2) O(|L|n2)Zx O(n3 + |L|n2) O(n3 + |L|n2)?
(i, j)k?x O(n3 + |L|n2) O(n3 + |L|n2)Table 1: Comparison of runtime for non-projectiveand projective algorithms.edge expectation.
Crucially, the nested property ofprojective structures allows edge expectations to becomputed in O(n3) from the inside-outside values.It is straight-forward to extend the algorithms of Eis-ner (1996) and Paskin (2001) to the labeled caseadding only a factor of O(|L|n2).Table 1 gives an overview of the computationalcomplexity for the three problems considered herefor both the projective and non-projective case.
Wesee that the non-projective case compares favorablyfor all three problems.4 ApplicationsTo motivate the algorithms from Section 3, wepresent some important situations where each cal-culation is required.4.1 Inference Based LearningMany learning paradigms can be defined asinference-based learning.
These include the per-ceptron (Collins, 2002) and its large-margin vari-ants (Crammer and Singer, 2003; McDonald et al,2005a).
In these settings, a models parameters areiteratively updated based on the argmax calculationfor a single or set of training instances under thecurrent parameter settings.
The work of McDon-ald et al (2005b) showed that it is possible to learna highly accurate non-projective dependency parserfor multiple languages using the Chu-Liu-Edmondsalgorithm for unlabeled parsing.4.2 Non-Projective Min-Risk DecodingIn min-risk decoding the goal is to find the depen-dency graph for an input sentence x, that on averagehas the lowest expected risk,T = argminT?T (Gx)?T ?
?T (Gx)w(T ?
)R(T, T ?
)where R is a risk function measuring the error be-tween two graphs.
Min-risk decoding has beenstudied for both phrase-structure parsing and depen-dency parsing (Titov and Henderson, 2006).
In thatwork, as is common with many min-risk decodingschemes, T (Gx) is not the entire space of parsestructures.
Instead, this set is usually restricted toa small number of possible trees that have been pre-selected by some baseline system.
In this subsectionwe show that when the risk function is of a specificform, this restriction can be dropped.
The result isan exact min-risk decoding procedure.Let R(T, T ?)
be the Hamming distance betweentwo dependency graphs for an input sentence x =x0x1 ?
?
?xn,R(T, T ?)
= n ??
(i,j)k?ETI((i, j)k, T ?
)This is a common definition of risk between twographs as it corresponds directly to labeled depen-dency parsing accuracy (McDonald et al, 2005a;Buchholz et al, 2006).
Some algebra reveals,T = argminT?T (Gx)XT ?
?T (Gx)w(T ?
)R(T, T ?
)= argminT?T (Gx)XT ?
?T (Gx)w(T ?
)[n ?X(i,j)k?ETI((i, j)k, T ?
)]= argminT?T (Gx)?XT ?
?T (Gx)w(T ?
)X(i,j)k?ETI((i, j)k, T ?
)= argminT?T (Gx)?X(i,j)k?ETXT ?
?T (Gx)w(T ?
)I((i, j)k, T ?
)= argmaxT?T (Gx)X(i,j)k?ETXT ?
?T (Gx)w(T ?
)I((i, j)k, T ?
)= argmaxT?T (Gx)Y(i,j)k?ETePT ?
?T (Gx)w(T ?
)I((i,j)k,T ?
)= argmaxT?T (Gx)Y(i,j)k?ETe?
(i,j)k?xBy setting the edge weights to wkij = e?
(i,j)k?x wecan directly solve this problem using the edge ex-pectation algorithm described in Section 3.3 and theargmax algorithm described in Section 3.1.4.3 Non-Projective Log-Linear ModelsConditional Random Fields (CRFs) (Lafferty et al,2001) are global discriminative learning algorithmsfor problems with structured output spaces, such asdependency parsing.
For dependency parsing, CRFswould define the conditional probability of a depen-dency graph T for a sentence x as a globally nor-126malized log-linear model,p(T |x) =?
(i,j)k?ETew?f(i,j,k)?T ?
?T (Gx)?
(i,j)k?ET ?ew?f(i,j,k)=?
(i,j)k?ETwkij?T ?
?T (Gx)?
(i,j)k?ET ?wkij=w(T )ZxHere, the weights wkij are potential functions overeach edge defined as an exponentiated linear classi-fier with weight vector w ?
RN and feature vectorf(i, j, k) ?
RN , where fu(i, j, k) ?
R represents asingle dimension of the vector f. The denominator,which is exactly the sum over all graph weights, is anormalization constant forcing the conditional prob-ability distribution to sum to one.CRFs set the parameters w to maximize the log-likelihood of the conditional probability over a train-ing set of examples T = {(x?, T?
)}|T |?=1,w = argmaxw?
?log p(T?|x?
)This optimization can be solved through a vari-ety of iterative gradient based techniques.
Manyof these require the calculation of feature expecta-tions over the training set under model parametersfor the previous iteration.
First, we note that thefeature functions factor over edges, i.e., fu(T ) =?
(i,j)k?ETfu(i, j, k).
Because of this, we can useedge expectations to compute the expectation of ev-ery feature fu.
Let ?fu?x?
represent the expectationof feature fu for the training instance x?,?fu?x?
=XT?T (Gx?
)p(T |x?
)fu(T )=XT?T (Gx?
)p(T |x?
)X(i,j)k?ETfu(i, j, k)=XT?T (Gx?
)w(T )ZxX(i,j)k?ETfu(i, j, k)=1ZxX(i,j)k?Ex?XT?T (Gx)w(T )I((i, j)k, T )fu(i, j, k)=1ZxX(i,j)k?Ex??
(i, j)k?x?fu(i, j, k)Thus, we can calculate the feature expectation pertraining instance using the algorithms for comput-ing Zx and edge expectations.
Using this, we cancalculate feature expectations over the entire train-ing set,?fu?T =??p(x?
)?fu?x?where p(x?)
is typically set to 1/|T |.4.4 Non-projective Generative Parsing ModelsA generative probabilistic dependency model oversome alphabet ?
consists of parameters pkx,y asso-ciated with each dependency from word x ?
?
toword y ?
?
with label lk ?
L. In addition, we im-pose 0 ?
pkx,y ?
1 and the normalization conditions?y,k pkx,y = 1 for each x ?
?.
We define a gen-erative probability model p over trees T ?
T (Gx)and a sentence x = x0x1 ?
?
?xn conditioned on thesentence length, which is always known,p(x, T |n) = p(x|T, n)p(T |n)=?
(i,j)k?ETpkxi,xj p(T |n)We assume that p(T |n) = ?
is uniform.
This modelis studied specifically by Paskin (2001).
In thismodel, one can view the sentence as being generatedrecursively in a top-down process.
First, a tree isgenerated from the distribution p(T |n).
Then start-ing at the root of the tree, every word generates all ofits modifiers independently in a recursive breadth-first manner.
Thus, pkx,y represents the probabilityof the word x generating its modifier y with labellk.
This distribution is usually smoothed and is of-ten conditioned on more information including theorientation of x relative to y (i.e., to the left/right)and distance between the two words.
In the super-vised setting this model can be trained with maxi-mum likelihood estimation, which amounts to sim-ple counts over the data.
Learning in the unsuper-vised setting requires EM and is discussed in Sec-tion 4.4.2.Another generative dependency model of interestis that given by Klein and Manning (2004).
In thismodel the sentence and tree are generated jointly,which allows one to drop the assumption that p(T |n)is uniform.
This requires the addition to the modelof parameters px,STOP for each x ?
?, with the nor-malization condition px,STOP +?y,k pkx,y = 1.
It ispossible to extend the model of Klein and Manning127(2004) to the non-projective case.
However, the re-sulting distribution will be over multisets of wordsfrom the alphabet instead of strings.
The discus-sion in this section is stated for the model in Paskin(2001); a similar treatment can be developed for themodel in Klein and Manning (2004).4.4.1 Language ModelingA generative model of dependency structuremight be used to determine the probability of a sen-tence x by marginalizing out all possible depen-dency trees,p(x|n) =?T?T (Gx)p(x, T |n)=?T?T (Gx)p(x|T, n)p(T |n)= ?
?T?T (Gx)?
(i,j)k?ETpkxi,xj = ?ZxThis probability can be used directly as a non-projective syntactic language model (Chelba et al,1997) or possibly interpolated with a separate n-gram model.4.4.2 Unsupervised LearningIn unsupervised learning we train our model ona sample of unannotated sentences X = {x?
}|X |?=1.Let |x?| = n?
and p(T |n?)
= ??.
We choose theparameters that maximize the log-likelihood|X |??=1log(p(x?|n?))
==|X |?
?=1log(?T?T (Gx?
)p(x?|T, n?))
+|X |??=1log(??
),viewed as a function of the parameters and subjectto the normalization conditions, i.e.,?y,k pkx,y = 1and pkx,y ?
0.Let x?i be the ith word of x?.
By solving theabove constrained optimization problem with theusual Lagrange multipliers method one getspkx,y ==?|X |?=11Zx?
?i : x?i = x,j : x?j = y?
(i, j)k?x?
?|X |?=11Zx??y?,k?
?i : x?i = x,j?
: x?j?
= y??
(i, j?)k?
?x?,where for each x?
the expectation ?
(i, j)k?x?
is de-fined as in Section 3, but with the weight w(T ) re-placed by the probability distribution p(x?|T, n?
).The above |L| ?
|?|2 relations represent a non-linear system of equations.
There is no closed formsolution in the general case, and one adopts the ex-pectation maximization (EM) method, which is aspecialization of the standard fixed-point iterationmethod for the solution of non-linear systems.
Westart with some initial assignment of the parametersand at each iteration we use the induced distribu-tion p(x?|T, n?)
to compute a refined value for theparameters themselves.
We are always guaranteedthat the Kullback-Liebler divergence between twoapproximated distributions computed at successiveiterations does not increase, which implies the con-vergence of the method to some local maxima (withthe exception of saddle points).Observe that at each iteration we can computequantities ?
(i, j)k?x?
and Zx?
in polynomial timeusing the algorithms from Section 3 with pkx?i,x?jin place of wki,j .
Furthermore, under some standardconditions the fixed-point iteration method guaran-tees a constant number of bits of precision gain forthe parameters at each iteration, resulting in overallpolynomial time computation in the size of the inputand in the required number of bits for the precision.As far as we know, this is the first EM learning algo-rithm for the model in Paskin (2001) working in thenon-projective case.
The projective case has beeninvestigated in Paskin (2001).5 Beyond Edge-factored ModelsWe have shown that several computational problemsrelated to parsing can be solved in polynomial timefor the class of non-projective dependency modelswith the assumption that dependency relations aremutually independent.
These independence assump-tions are unwarranted, as it has already been estab-lished that modeling non-local information such asarity and nearby parsing decisions improves the ac-curacy of dependency models (Klein and Manning,2002; McDonald and Pereira, 2006).In the spirit of our effort to understand the natureof exact non-projective algorithms, we examine de-pendency models that introduce arity constraints aswell as permit edge decisions to be dependent on a128limited neighbourhood of other edges in the graph.Both kinds of models can no longer be considerededge-factored, since the likelihood of a dependencyoccurring in a particular analysis is now dependenton properties beyond the edge itself.5.1 ArityOne feature of the edge-factored models is that norestriction is imposed on the arity of the nodes in thedependency trees.
As a consequence, these modelscan generate dependency trees of unbounded arity.We show below that this is a crucial feature in thedevelopment of the complexity results we have ob-tained in the previous sections.Let us assume a graph G(?
)x = (Vx, Ex) definedas before, but with the additional condition that eachnode i ?
Vx is associated with an integer value?
(i) ?
0.
T (G(?
)x ) is now defined as the set of alldirected spanning trees for G(?
)x rooted in node 0,such that every node i ?
Vx has arity smaller than orequal to ?(i).
We now introduce a construction thatwill be used to establish several hardness results forthe computational problems discussed in this paper.Recall that a Hamiltonian path in a directed graphG is a directed path that visits all of the nodes of Gexactly once.Let G be some directed graph with set of nodesV = {1, 2, .
.
.
, n}.
We construct a target graphG(?
)x = (Vx, Ex) with Vx = V ?
{0} (0 the rootnode) and |L| = 1.
For each i, j ?
Vx with i 6= j,we add an edge (i, j)1 to Ex.
We set w1i,j = 1 ifthere is an edge from i to j in G, or else if i or jis the root node 0, and w1i,j = 0 otherwise.
Fur-thermore, we set ?
(i) = 1 for each i ?
Vx.
Thisconstruction can be clearly carried out in log-space.Note that each T ?
T (G(?
)x ) must be a monadictree with weight equal to either 0 or 1.
It is not dif-ficult to see that if w(T ) = 1, then when we removethe root node 0 from T we obtain a Hamiltonian pathin G. Conversely, each Hamiltonian path in G canbe extended to a spanning tree T ?
T (G(?
)x ) withw(T ) = 1, by adding the root node 0.Using the above observations, it can be shown thatthe solution of the argmax problem for G(?
)x pro-vides some Hamiltonian directed path in G. The lat-ter search problem is FNP-hard, and is unlikely tobe solved in polynomial time.
Furthermore, quan-tity Zx provides the count of the Hamiltonian di-rected paths in G, and for each i ?
V , the expecta-tion ?
(0, i)1?x provides the count of the Hamiltoniandirected paths in G starting from node i.
Both thesecounting problems are #P-hard, and very unlikely tohave polynomial time solutions.This result helps to relate the hardness of data-driven models to the commonly known hardnessresults in the grammar-driven literature given byNeuhaus and Bo?ker (1997).
In that work, an arityconstraint is included in their minimal grammar.5.2 Vertical and Horizontal MarkovizationIn general, we would like to say that every depen-dency decision is dependent on every other edge ina graph.
However, modeling dependency parsing insuch a manner would be a computational nightmare.Instead, we would like to make a Markov assump-tion over the edges of the tree, in a similar way thata Markov assumption can be made for sequentialclassification problems in order to ensure tractablelearning and inference.Klein and Manning (2003) distinguish betweentwo kinds of Markovization for unlexicalized CFGparsing.
The first is vertical Markovization, whichmakes the generation of a non-terminal dependenton other non-terminals that have been generated atdifferent levels in the phrase-structure tree.
Thesecond is horizontal Markovization, which makesthe generation of a non-terminal dependent on othernon-terminals that have been generated at the samelevel in the tree.For dependency parsing there are analogous no-tions of vertical and horizontal Markovization for agiven edge (i, j)k. First, let us define the vertical andhorizontal neighbourhoods of (i, j)k. The verticalneighbourhood includes all edges in any path fromthe root to a leaf that passes through (i, j)k. The hor-izontal neighbourhood contains all edges (i, j?
)k?.Figure 4 graphically displays the vertical and hor-izontal neighbourhoods for an edge in the depen-dency graph from Figure 1.Vertical and horizontal Markovization essentiallyallow the score of the graph to factor over a largerscope of edges, provided those edges are in the samevertical or horizontal neighbourhood.
A dth orderfactorization is one in which the score factors onlyover the d nearest edges in the neighbourhoods.
In129Figure 4: Vertical and Horizontal neighbourhood forthe edge from will to remain.McDonald and Pereira (2006), it was shown thatnon-projective dependency parsing with horizontalMarkovization is FNP-hard.
In this study we com-plete the picture and show that vertical Markoviza-tion is also FNP-hard.Consider a first-order vertical Markovization inwhich the score for a dependency graph factors overpairs of vertically adjacent edges2,w(T ) =?(h,i)k,(i,j)k?
?ETkhiwk?ijwhere khiwk?ij is the weight of including both edges(h, i)k and (i, j)k?in the dependency graph.
Notethat this formulation does not include any contribu-tions from dependencies that have no vertically adja-cent neighbours, i.e., any edge (0, i)k such that thereis no edge (i, j)k?in the graph.
We can easily rec-tify this by inserting a second root node, say 0?, andincluding the weights k0?0wk?0i .
To ensure that onlyvalid dependency graphs get a weight greater thanzero, we can set khiwk?ij = 0 if i = 0?
and k0?iwk?ij = 0if i 6= 0.Now, consider the NP-complete 3D-matchingproblem (3DM).
As input we are given three setsof size m, call them A, B and C, and a set S ?A?B ?C.
The 3DM problem asks if there is a setS?
?
S such that |S?| = m and for any two tuples(a, b, c), (a?, b?, c?)
?
S?
it is the case that a 6= a?,b 6= b?, and c 6= c?.2McDonald and Pereira (2006) define this as a second-orderMarkov assumption.
This is simply a difference in terminologyand does not represent any meaningful distinction.We can reduce the 3D-matching problem to thefirst-order vertical Markov parsing problem by con-structing a graph G = (V,E), such that L =A ?
B ?
C, V = {0?, 0} ?
A ?
B ?
C and E ={(i, j)k | i, j ?
V, k ?
L}.
The set E contains mul-tiple edges between ever pair of nodes, each edgetaking on a label representing a single element ofthe set A ?
B ?
C. Now, define k0?0wk?0a = 1, for alla ?
A and k, k?
?
A ?
B ?
C, and b0awcab = 1, forall a ?
A and b ?
B and c ?
C, and cabwcbc = 1, forall (a, b, c) ?
S. All other weights are set to zero.We show below that there exists a bijection be-tween the set of valid 3DMs for S and the set of non-zero weighted dependency graphs in T (G).
First, itis easy to show that for any 3DM S?, there is a rep-resentative dependency graph that has a weight of1.
This graph simply consists of the edges (0, a)b,(a, b)c, and (b, c)c, for all (a, b, c) ?
S?, plus an ar-bitrarily labeled edge from 0?
to 0.To prove the reverse, consider a graph with weight1.
This graph must have a weight 1 edge into thenode a of the form (0, a)b since the graph must bespanning.
By the definition of the weight function,in any non-zero weighted tree, a must have a sin-gle outgoing edge, and that edge must be directedinto the node b.
Let?s say that this edge is (a, b)c.Then again by the weighting function, in any non-zero weighted graph, b must have a single outgoingedge that is directed into c, in particular the edge(b, c)c. Thus, for any node a, there is a single pathdirected out of it to a single leaf c ?
C. We canthen state that the only non-zero weighted depen-dency graph is one where each a ?
A, b ?
B andc ?
C occurs in exactly one ofm disjoint paths fromthe root of the form 0 ?
a ?
b ?
c. This is be-cause the label of the single edge going into node awill determine exactly the node b that the one outgo-ing edge from a must go into.
The label of that edgedetermines exactly the single outgoing edge from binto some node c. Now, since the weighting func-tion ensures that the only non-zero weighted pathsinto any leaf node c correspond directly to elementsof S, each of the m disjoint paths represent a singletuple in a 3DM.
Thus, if there is a non-zero weightedgraph in T (G), then it must directly correspond to avalid 3DM, which concludes the proof.Note that any dth order Markovization can be em-bedded into a d + 1th Markovization.
Thus, this re-130sult also holds for any arbitrary Markovization.6 DiscussionIn this paper we have shown that many importantlearning and inference problems can be solved effi-ciently for non-projective edge-factored dependencymodels by appealing to the Matrix Tree Theoremfor multi-digraphs.
These results extend the workof McDonald et al (2005b) and help to further ourunderstanding of when exact non-projective algo-rithms can be employed.
When this analysis is cou-pled with the projective parsing algorithms of Eisner(1996) and Paskin (2001) we begin to get a clear pic-ture of the complexity for data-driven dependencyparsing within an edge-factored framework.
To fur-ther justify the algorithms presented here, we out-lined a few novel learning and inference settings inwhich they are required.However, for the non-projective case, movingbeyond edge-factored models will almost certainlylead to intractable parsing problems.
We have pro-vided further evidence for this by proving the hard-ness of incorporating arity constraints and hori-zontal/vertical edge Markovization, both of whichincorporate information unavailable to an edge-factored model.
The hardness results providedhere are also of interest since both arity constraintsand Markovization can be incorporated efficientlyin the projective case through the straight-forwardaugmentation of the underlying chart parsing algo-rithms used in the projective edge-factored models.This highlights a fundamental difference betweenthe nature of projective parsing algorithms and non-projective parsing algorithms.
On the projectiveside, all algorithms use a bottom-up chart parsingframework to search the space of nested construc-tions.
On the non-projective side, algorithms areeither greedy-recursive in nature (i.e., the Chu-Liu-Edmonds algorithm) or based on the calculation ofthe determinant of a matrix (i.e., the partition func-tion and edge expectations).Thus, the existence of bottom-up chart parsingalgorithms for projective dependency parsing pro-vides many advantages.
As mentioned above, itpermits simple augmentation techniques to incorpo-rate non-local information such as arity constraintsand Markovization.
It also ensures the compatibilityof projective parsing algorithms with many impor-tant natural language processing methods that workwithin a bottom-up chart parsing framework, includ-ing information extraction (Miller et al, 2000) andsyntax-based machine translation (Wu, 1996).The complexity results given here suggest thatpolynomial chart-parsing algorithms do not existfor the non-projective case.
Otherwise we shouldbe able to augment them and move beyond edge-factored models without encountering intractability?
just like the projective case.
An interesting lineof research is to investigate classes of non-projectivestructures that can be parsed with chart-parsing algo-rithms and how these classes relate to the languagesparsable by other syntactic formalisms.AcknowledgmentsThanks to Ben Taskar for pointing out the work ofMeila?
and Jaakkola (2000).
Thanks to David Smith,Noah Smith and Michael Collins for making draftsof their EMNLP papers available.ReferencesG.
E. Barton, R. C. Berwick, and E. S. Ristad.
1987.Computational Complexity and Natural Language.MIT Press, Cambridge, MA.C.
Brew.
1992.
Letting the cat out of the bag: Generationfor Shake-and-Bake MT.
In Proc.
COLING.S.
Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.2006.
CoNLL-X shared task on multilingual depen-dency parsing.
In Proc.
CoNLL.P.
M. Camerini, L. Fratta, and F. Maffioli.
1980.
The kbest spanning arborescences of a network.
Networks,10(2):91?110.C.
Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khudan-pur, L. Mangu, H. Printz, E.S.
Ristad, R. Rosenfeld,A.
Stolcke, and D. Wu.
1997.
Structure and per-formance of a dependency language model.
In Eu-rospeech.Y.J.
Chu and T.H.
Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14:1396?1400.M.
Collins.
2002.
Discriminative training methods forhidden Markov models: Theory and experiments withperceptron algorithms.
In Proc.
EMNLP.T.H.
Cormen, C.E.
Leiserson, and R.L.
Rivest.
1990.
In-troduction to Algorithms.
MIT Press/McGraw-Hill.131K.
Crammer and Y.
Singer.
2003.
Ultraconservative on-line algorithms for multiclass problems.
JMLR.J.
Edmonds.
1967.
Optimum branchings.
Journal of Re-search of the National Bureau of Standards, 71B:233?240.J.
Eisner.
1996.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proc.
COLING.K.
Hall and V. No?va?k.
2005.
Corrective modeling fornon-projective dependency parsing.
In Proc.
IWPT.H.
Hirakawa.
2006.
Graph branch algorithm: An opti-mum tree search method for scored dependency graphwith arc co-occurrence constraints.
In Proc.
ACL.R.
Hudson.
1984.
Word Grammar.
Blackwell.S.
Kahane, A. Nasr, and O Rambow.
1998.
Pseudo-projectivity: A polynomially parsable non-projectivedependency grammar.
In Proc.
ACL.D.
Klein and C.D.
Manning.
2002.
Fast exact natu-ral language parsing with a factored model.
In Proc.NIPS.D.
Klein and C. Manning.
2003.
Accurate unlexicalizedparsing.
In Proc.
ACL.D.
Klein and C. Manning.
2004.
Corpus-based induc-tion of syntactic structure: Models of dependency andconstituency.
In Proc.
ACL.T.
Koo, A. Globerson, X. Carreras, and M. Collins.
2007.Structured prediction models via the matrix-tree theo-rem.
In Proc.
EMNLP.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
ICML.M.
Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In ProcEACL.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
On-line large-margin training of dependency parsers.
InProc.
ACL.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005b.Non-projective dependency parsing using spanningtree algorithms.
In Proc.
HLT/EMNLP.M.
Meila?
and T. Jaakkola.
2000.
Tractable Bayesianlearning of tree belief networks.
In Proc.
UAI.I.A.
Mel?c?uk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press.S.
Miller, H. Fox, L.A. Ramshaw, and R.M.
Weischedel.2000.
A novel use of statistical parsing to extract in-formation from text.
In Proc NAACL, pages 226?233.P.
Neuhaus and N. Bo?ker.
1997.
The complexityof recognition of linguistically adequate dependencygrammars.
In Proc.
ACL.J.
Nivre and J. Nilsson.
2005.
Pseudo-projective depen-dency parsing.
In Proc.
ACL.J.
Nivre and M. Scholz.
2004.
Deterministic dependencyparsing of english text.
In Proc.
COLING.J.
Nivre.
2005.
Dependency grammar and dependencyparsing.
Technical Report MSI report 05133, Va?xjo?University: School of Mathematics and Systems Engi-neering.M.A.
Paskin.
2001.
Cubic-time parsing and learning al-gorithms for grammatical bigram models.
TechnicalReport UCB/CSD-01-1148, Computer Science Divi-sion, University of California Berkeley.S.
Riedel and J. Clarke.
2006.
Incremental integer linearprogramming for non-projective dependency parsing.In Proc.
EMNLP.S.
Robinson.
2005.
Toward an optimal algorithm formatrix multiplication.
News Journal of the Society forIndustrial and Applied Mathematics, 38(9).P.
Sgall, E.
Hajic?ova?, and J. Panevova?.
1986.
The Mean-ing of the Sentence in Its Pragmatic Aspects.
Reidel.D.A.
Smith and N.A.
Smith.
2007.
Probabilistic modelsof nonprojective dependency trees.
In Proc.
EMNLP.L.
Tesnie`re.
1959.
E?le?ments de syntaxe structurale.
Edi-tions Klincksieck.I.
Titov and J. Henderson.
2006.
Bayes risk minimiza-tion in natural language parsing.
University of Genevatechnical report.W.T.
Tutte.
1984.
Graph Theory.
Cambridge UniversityPress.W.
Wang and M. P. Harper.
2004.
A statistical constraintdependency grammar (CDG) parser.
In Workshop onIncremental Parsing: Bringing Engineering and Cog-nition Together (ACL).D.
Wu.
1996.
A polynomial-time algorithm for statisti-cal machine translation.
In Proc.
ACL.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Proc.IWPT.132
