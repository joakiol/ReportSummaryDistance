Poly-co : an unsupervised co-reference detection systemE?ric Charton, Michel Gagnon, Benoit OzellE?cole Polytechnique de Montre?al2900 boulevard Edouard-Montpetit, Montreal, QC H3T 1J4, Canada.
{eric.charton, michel.gagnon, benoit.ozell}@polymtl.caAbstractWe describe our contribution to the Gen-eration Challenge 2010 for the tasksof Named Entity Recognition and co-reference detection (GREC-NER).
To ex-tract the NE and the referring expressions,we employ a combination of a Part ofSpeech Tagger and the Conditional Ran-dom Fields (CRF) learning technique.
Wefinally experiment an original algorithmto detect co-references.
We concludewith discussion about our system perfor-mances.1 IntroductionThree submission tracks are proposed in Genera-tion Challenges 2010.
GREC-NEG, where partic-ipating systems select a referring expression (RE)from a given list.
GREC-NER where partic-ipating systems must recognize all mentions ofpeople in a text and identify which mentions co-refer.
And GREC-Full, end-to-end RE regener-ation task; participating systems must identify allmentions of people and then aim to generate im-proved REs for the mentions.
In this paper wepresent an unsupervised CRF based Named EntityRecognition (NER) system applied to the GREC-NER Task.2 System descriptionThe proposed system follows a pipelined architec-ture (each module processes the information pro-vided by the previous one).
First, a Part of Speech(POS) tagger is applied to the corpus.
Then, thecombination of words and POS tags are used bya CRF classifier to detect Named Entities (NE).Next, logical rules based on combination of POStags, words and NE labels are used to detect pro-nouns related to persons.
Finally, an algorithm1This work is granted by Unima Inc and Prompt Que?becidentifies, among the person entities that havebeen detected, the ones that co-refer and clusterthem.
At the end, all collected information is ag-gregated in a XML file conform to GREC-NERspecifications.2.1 Part of speechThe part of speech labeling is done with the En-glish version of Treetagger1.
It is completed bya step where every NAM tag associated to a firstnname is replaced by a FNAME tag, using a lex-ical resource of first names (see table 2, columnPOS Tag).
The first name tag improves the NEdetection model while it improves the estimationof conditional probabilities for words describing aperson, encountered by a NER system.Word from Corpus POS Tag NE TagAdrianne FNAM PERSCalvo NAM PERSenrolled VVD UNKat IN UNKJohnson NAM ORGWales NAM ORGCollege NAM ORGTable 2: Sample of word list with POS Taggingand NE tagging2.2 Named entity and pronoun labelingThe Named Entity Recognition (NER) systemis an implementation of the CRF based system(Be?chet and Charton, 2010) that has been usedin the French NER evaluation campaign ESTER2 (Galliano et al, 2009)2.
For the present task,training of the NER tool is fully unsupervised asit does not use the GREC training corpus.
It istrained in English with an automatically NE an-notated version of the Wikipedia Corpus (the fullsystem configuration is described in (Charton and1The Tree-tagger is a tool for annotating text with part-of-speech and lemma information.
http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/2Referenced in this paper as LIAPoly-co Score B3 CEAF MUCSet Precision Recall FScore Precision Recall FScore Precision Recall FScoreFull set 91.48 85.89 88,60 85.40 85.40 85.40 92.15 86.95 89.47Chef 91.12 87.84 89.45 86.53 86.53 86.53 91.86 88.55 90.18Composers 92.01 87.14 89.51 86.87 86.87 86.87 92.11 87.02 89.49Inventors 91.27 82.63 86.74 82.73 82.73 82.73 92.48 85.29 88.74Table 1: System results obtained on dev-setTorres-Moreno, 2010)).
It is able to label PERS3,ORG, LOC, TIME, DATE.
We measured a spe-cific precision of 0,93 on PERS NE detection ap-plied to the English ACE4 evaluation set.Following the NE detection process, detectionrules are used to label each personal pronoun withthe PERS tag.
Boolean AND rules are appliedto triples {word, POS tag, NE tag}, where word= {he, him, she, her ...}, POS tag=NN, and NEtag=UNK .
This rule structure is adopted to avoidthe PERS labeling of pronouns included in an ex-pression or in a previously tagged NE (i.e a musicalbum or a movie title, using word She, and pre-viously labeled with PROD NE tag).
Finally, eachPERS labeled entity is numbered by order of ap-parition and is associated with the sentences refer-ence number where it appears (consecutive PERSlabeled words, not separated by punctuation mark,receive the same index number).2.3 Entities clustering by unstackingIn the final stage, our system determines whichentities co-refer.
First, a clustering process isachieved.
The principle of the algorithm is asfollows: entities characteristics (words, POS tags,sentence position) are indexed in a stack, orderedaccording to their chronological apparition in thetext (the entity at the top of the stack is the first onethat has been detected in the document).
At thebeginning of the process, the entity that is at thetop of the stack is removed and constitutes the firstitem of a cluster.
This entity is compared sequen-tially, by using similarity rules, with every otherentities contained in the stack.
When there is amatch, entity is transfered to the currently instan-tiated cluster and removed from the stack.
Whenthe end of the stack is reached, remaining entitiesare reordered and the process iterates form the be-ginning.
This operation is repeated until the stackis empty.Comparison of entities in the stack is done in3PERS tag is commonly used in NER Task to describelabels applied to people, ORG describe organisations, LOCis for places.4ACE is the former NIST NER evaluation campaign.two ways according to the nature of the entity.We consider a candidate entity Ec from stackS.
According to iteration k, the current clus-ter is Ck.
Each element of the sequence Ec (i.eChester FNAME Carton NAM) is compared to thesequences previously transfered in Ck during theexploration process of the stack.
If Ec ?
?Ck, itis included in cluster Ck and removed from S. Fi-nally inclusion of pronouns from S in Ec is doneby resolving the anaphora, according to the Hobbsalgorithm, as described in (Jurafsky et al, 2000)5.3 Results and conclusionsTable 1 shows our results on dev-set.
We ob-tain good precision on the 3 subsets.
Our systemslightly underperforms the recall.
This can be ex-plained by a good performance in the NE detectionprocess, but a difficulty in some cases for the clus-tering algorithm to group entities.
We have ob-served in the Inventors dev-set some difficulties,due to strong variation of surface forms for spe-cific entities.
We plan to experiment the use ofan external resource of surface forms for personnames extracted from Wikipedia to improve oursystem in such specific case.ReferencesFre?de?ric Be?chet and Eric Charton.
2010.
Unsuper-vised knowledge acquisition for extracting namedentities from speech.
In ICASSP 2010, Dallas.ICASSP.Eric Charton and J.M.
Torres-Moreno.
2010.
NL-GbAse: a free linguistic resource for Natural Lan-guage Processing systems.
In LREC 2010, editor,English, number 1, Matla.
Proceedings of LREC2010.S.
Galliano, G. Gravier, and L. Chaubard.
2009.
TheESTER 2 Evaluation Campaign for the Rich Tran-scription of French Radio Broadcasts.
In Interna-tional Speech Communication Association confer-ence 2009, pages 2583?2586.
Interspeech 2010.D.
Jurafsky, J.H.
Martin, A. Kehler, K. Vander Linden,and N. Ward.
2000.
Speech and language process-ing.
Prentice Hall New York.5p704, 21.6
