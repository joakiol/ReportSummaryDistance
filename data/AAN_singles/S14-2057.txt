Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 341?345,Dublin, Ireland, August 23-24, 2014.Indian Institute of Technology-Patna: Sentiment Analysis in TwitterVikram Singh, Arif Md.
Khan and Asif EkbalIndian Institute of Technology PatnaPatna, India(vikram.mtcs13,arif.mtmc13,asif)@iitp.ac.inAbstractThis paper is an overview of the systemsubmitted to the SemEval-2014 sharedtask on sentiment analysis in twitter.
Forthe very first time we participated in boththe tasks, viz contextual polarity disam-biguation and message polarity classifi-cation.
Our approach is supervised innature and we use sequential minimaloptimization classifier.
We implementthe features for sentiment analysis with-out using deep domain-specific resourcesand/or tools.
Experiments within thebenchmark setup of SemEval-14 showsthe F-scores of 77.99%, 75.99%, 76.54%, 76.43% and 71.43% for LiveJour-nal2014, SMS2013, Twitter2013, Twit-ter2014 and Twitter2014Sarcasm, respec-tively for Subtask A.
For Subtask B weobtain the F-scores of 60.39%, 51.96%,52.58%, 57.25%, 41.33% for five differenttest sets, respectively.1 IntroductionIn current era microblogging is an efficient wayof communication where people can communicatewithout physical presence of receiver(s).
Twitteris the medium where people post real time mes-sages to discuss on the different topics, and ex-press their sentiments.
The texts used in twit-ter are generally informal and unstructured in na-ture.
Tweets and SMS messages are very shortin length, usually a sentence or a headline ratherthan a document.
These texts are very informalin nature and contains creative spellings and punc-tuation symbols.
Text also contains lots of mis-spellings, slang, out-of-vocabulary words, URLs,This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/and genre-specific terminology and abbreviations,e.g., RT for re-Tweet and #hashtags.
Such kinds ofstructures introduce difficulties in building variouslexical and syntactic resources and/or tools, whichare required for efficient processing of texts.
Find-ing relevant information from these posts posesbig challenges to the researchers compared to thetraditional text genres such as newswire.In recent times, there has been a huge inter-est to mine and understand the opinions and sen-timents that people are communicating in socialmedia (Barbosa and Feng, 2010; Bifet et al.,; Pak and Paroubek, 2010; Kouloumpis et al.,2011).
There is a tremendous interest in sentimentanalysis of Tweets across a variety of domainssuch as commerce (Jansen et al., 2009), health(Chew and Eysenbach, 2010; Salathe and Khan-delwal, 2011) and disaster management (Vermaet al., 2011; Mandel et al., 2012).
Agarwal etal.
(Agarwal et al., 2011) used tree kernel decisiontree that made use of the features such as Part-of-Speech (PoS) information, lexicon-based fea-tures and several other features.
They acquired11,875 manually annotated Twitter data (Tweets)from a commercial source, and reported an accu-racy of 75.39%.
Semantics has also been used asthe feature to improve the performance of senti-ment analysis (Saif et al., 2012).
For each ex-tracted entity (e.g.
iPhone) from Tweets, theyadded its semantic concept (e.g.
Apple product)as an additional feature.
Thereafter they deviseda method to measure the correlation of the rep-resentative concept with negative/positive senti-ment, and applied this approach to predict sen-timent for three different Twitter datasets.
Theyshowed that semantic features produce better re-call and F-score when classifying negative senti-ment, and better precision with lower recall andF-score in positive sentiment classification.
Thebenchmark corpus were made available with theSemEval-2013 shared task (Nakov et al., 2013) on341sentiment analysis in twitter.
The datasets used arefrom the domains of Tweets and SMS messages.The datasets were labelled with contextual phrase-level polarity and overall message-level polarity.Among the 44 submissions, the support vector ma-chine based system proposed in (Mohammad etal., 2013) achieved the highest F-scores of 69.02%for Task A, i.e.
the message-level polarity and and88.93% for Task B, i.e.
term-level polarity.The issues addressed in SemEval-13 are furtherextended in SemEval-14 shared task1.
The sametwo tasks, viz.
Subtask A and Subtask B denot-ing contextual polarity disambiguation and mes-sage polarity classification.
The goal of SubtaskA is to determine, for a given message containinga marked instance of a word or phrase, whetherthat instance is positive, negative or neutral in thatcontext.
Given a message, the task is to classifyit with its entirety whether it is positive, negative,or neutral sentiment.
For messages that conveyboth positive and negative sentiments, the strongerone should be chosen.
In this paper we reporton our submitted systems for both the tasks.
Ourevaluation for the first task shows the F-scores of77.99%, 75.99%, 76.54%, 76.43% and 71.43% forLiveJournal2014, SMS2013, Twitter2013, Twit-ter2014 and Twitter2014Sarcasm, respectively forSubtask A.
For Subtask B we obtain the F-scoresof 60.39%, 51.96%, 52.58%, 57.25%, 41.33% forfive different test sets, respectively.2 MethodsIn this section we describe preprocessing steps,features and our methods for sentiment classifica-tion2.1 Preprocessing of DataThe data has to be pre-processed before being usedfor actual machine learning training.
Each Tweetis processed to extract only those relevant partsthat are useful for sentiment classification.
Forexample, stop words are removed; symbols andpunctuation markers are filtered out; URLs arereplaced by the word URL etc.
Each Tweet isthen passed through the ARK tagger developed byCMU2for tokenization and Part-of-Speech (PoS)tagging.1http://alt.qcri.org/semeval2014/task9/2http://www.ark.cs.cmu.edu/TweetNLP/2.2 ApproachOur approach is based on supervised machinelearning.
We explored different models such asnaive Bayes, decision tree and support vector ma-chine.
Based on the results obtained on the de-velopment sets we finally select SVM for boththe tasks.
We also carried out a number of ex-periments with the various feature combinations.Once the model is fixed with certain feature com-binations, these are finally used for blind evalua-tion on the test sets for both the tasks.
We sub-mit two runs, one for each task.
Both of our sub-missions were constrained in nature, i.e.
we didnot make use of any additional resources and/ortools to build our systems.
We adapt a supervisedmachine learning algorithm, namely Support Vec-tor Machine (Joachims, 1999; Vapnik, 1995).
Weuse its sequential minimal optimization version forfaster training3.
We use the same set of featuresfor both the tasks.
Development sets are used toidentify the best feature combinations for both thetasks.
Default parameters as implemented inWekaare used for the SVM experiments.2.3 FeaturesLike any other classification algorithm, featuresplay an important role for sentiment classifica-tion.
For the very first time we participated inthis kind of task, and therefore had to spend quitelong time in conceptualization and implementa-tion of the features.
We focused on implementingthe features without using any domain-dependentresources and/or tools.
Brief descriptions of thefeatures that we use are presented below:?
Bag-of-words: Bag-of-words in the expres-sion or in the entire Tweet is used as the fea-ture(s).?
SentiWordNet feature: This feature is de-fined based on the scores assigned to eachword of a Tweet using the SentiWordNet4.
Afeature vector of length three is defined.
Thescores of all words of the phrase or Tweet issummed over and normalized in the scale of3.
We define the following three thresholds:if the score is less than 0.5 then it is treated tobe a negative polarity; for the score above 0.8,it is assumed to contain positive sentiment;3http://research.microsoft.com/en-us/um/people/jplatt/smo-book.pdf4sentiwordnet.isti.cnr.it/342and the polarity is considered to be neutralfor all the other words.
Depending upon thescore the corresponding bit of the feature vec-tor is set.?
Stop word: If a Tweet/phrase is having morenumber of stop words then it most likely con-tains neutral sentiment.
We obtain the stopwords from the Wikipedia5.
We assume thata particular Tweet or phrase most likely bearsa neutral sentiment if 20% of its words be-long to the category of stop words.?
All Cap Words: This feature is defined tocount the number of capitalized words in anentire Tweet/phrase.
More the number ofcapitalized words, more the chances of beingpositive or negative sentiment bearing units.While counting, the words preceded by # arenot considered.
We include this with the as-sumption that the texts written in capitalizedletters express the sentiment strongly.?
Init Cap: The words starting with capital-ized letter contribute more towards classify-ing it.?
Percent Cap: This feature is based on thepercentage of capitalized characters in aTweet/phrase.
If this is more than 75%, thenmost likely it is not of neutral type.?
Psmiley (+ve Smiley): Generally people usesmileys to represent their emotions.
A smileypresent in a Tweet/phrase directly representsits sentiment.
A feature is defined that takesthe value equal to the number of positive smi-leys.
We make use of the list available at thispage6.?
Nsmiley (-ve Smiley): The value of this fea-ture is set to the number of negative smileyspresent in the Tweet.
This list was also ob-tained from the web7.?
NumberPostive words: This feature takesthe value equal to the number of positivewords present in the Tweet/phrase.
We searchthe adjective words present in the Tweet inthe SentiWordNet to determine whether itbears positive sentiment.5http://en.wikipedia.org/wiki/Stop words6http://en.wikipedia.org/wiki/List of emoticons7http://en.wikipedia.org/wiki/List of emoticons?
NumberNegative words: This feature takesthe value equal to the number of negativewords present in the Tweet/phrase.
Thewords are again looked at the SentiWordNetto determine its polarity.?
NumberNeutral words: This feature deter-mines the number of neutral words present inthe Tweet or phrase.
This information is ob-tained by looking the adjective words in theSentiWordNet.?
Repeating char: It has been seen that peo-ple express strong emotion by typing a char-acter many times in a Tweet.
For exam-ple, happppppppy, hurrrrrey etc.
This featurechecks whether the word(s) have at least threeconsecutive repeated characters.?
LenTweet: Length of the Tweet is used asthe feature.
The value of this feature is setequal to the number of words present in theTweet/phrase.?
Numhash: The value of this feature is setequal to the number of hashtags present in theTweet.3 Experiments and AnalysisSemEval-2014 shared task is a continuation of theSemEval-2013 shared task.
In 2014 shared task,datasets from different domains were incorporatedwith a wide range of topics, including a mixture ofentities, products and events.
Messages relevant tothe topics are selected based on the keywords andtwitter hashtags.The training set of Task-A has 4,914 positive,2,592 negative and 384 neutral class instances.The Task-B training set contains 3,057 positive,1,200 negative and 3,941 neutral sentiments.
De-velopments sets contain 555, 45 and 365 positive,negative and neutral sentiments, respectively forthe first task; and 493, 288 and 632 positive,negative and neutral sentiments, respectively forthe second task.
The selected test sets were takenmainly from the following domains:LiveJournal2014: 2000 sentences from Live-Journal blogs;SMS2013: SMS test from last year-used as aprogress test for comparison;Twitter2013: Twitter test data from last year-usedas a progress test for comparison;Twitter2014: A new Twitter test data of 2000343Model Avg.
F-scoreModel-1 75.75Model-2 72.69Model-3 75.45Model-4 75.77Table 1: Results for Task-A on development set(in%).Tweets;Twitter2014Sarcasm: 100 Tweets that are knownto contain sarcasm.We build different models by varying the fea-tures as follows:1.
Model-1: This model is constructed byconsidering the features, ?Repeating char?,?Numhash?, ?LenTweet?, ?Percent Cap?,?Init Cap?, ?All Cap?, ?Bag-of-words?,?Nsmiley?, ?Psmiley?, ?SentiWordNet?
and?Stop Words?.2.
Model-2: This model is constructed by thefeatures ?Repeating char?, ?Percent Cap?,?Numhash?, ?LenTweet?, ?Init Cap?,?All Cap?, ?Bag-of-words?, ?SentiWord-Net?
and ?Stop Words?.3.
Model-3: This model is built by consid-ering the features ?Repeating char?, ?Bag-of-words?, ?SentiWordNet?, ?Nsmiley?
and?Psmiley?.4.
Model-4: The model incorporates the fea-tures ?Repeating char?, ?Bag-of-words?,?SentiWordNet?, ?Nsmiley?, ?Psmiley?,?Stop Words?, ?Numhash?, ?LenTweet?,?Init Cap?
and ?All Cap?.Results on the development set for Task-A arereported in Table 1 that shows the highest perfor-mance in Model-4 with the average F-score valueof 75.77%.
Thereafter we use this particular fea-ture combination for training SVM, and to reportthe results.
Detailed results are reported in Table2 for both the tasks.
It shows 77.99%, 75.99%,76.54 %, 76.43% and 71.43% F-scores for theLiveJournal2014, SMS2013, Twitter2013, Twit-ter2014 and Twitter2014Sarcasm, respectively forSubtask A.
For Subtask B we obtain the F-scoresof 60.39%, 51.96%, 52.58%, 57.25% and 41.33%for the five different test sets, respectively.
Acloser investigation to the evaluation results re-veals that most of the errors are due to the con-fusions between positive vs. neutral and negativevs.
neutral classes.Comparisons with the best system(s) submittedin this shared task show that we are behind ap-proximately in the range of 6-14% F-score mea-sures for all the domains for Task-A.
Results thatwe obtain in Task-B need more attention as thesefall much shorter compared to the best one (in thethe range of 14-18%).Features used Classifier Result(Task A) Result(Task B)SWN +ve LiveJournal2014 LiveJournal2014SWN -ve 77.99 60.39SWN neutral SMS2013 SMS2013#Stop Words 75.99 51.96#All Cap Words Twitter2013 Twitter2013#Numhash 76.54 52.58Len Tweet Twitter2014 Twitter2014#Init Cap Words 76.43 57.25% Init Cap SVM T2014S T2014S#+ve Smiley 71.43 41.33#-ve Smiley#+ve Words#-ve Words#Neutral Words#Bag of wordsRep characterTable 2: Result on test sets for Task-A and Task-B.4 ConclusionIn this paper we report our works as part of ourparticipation to the SemEval-14 shared task onsentiment analysis for twitter data.
Our systemswere based on supervised classification, where wefixed SVM to report the test results after conduct-ing several experiments with different classifierson the development data.
We implement a set offeatures that are applied for both the tasks.
Ourruns are constrained in nature, i.e.
we did not makeuse of any external resources and/or tools.
Our re-sults are quite promising that need further inves-tigation.
A closer analysis to the results suggestthat most of the errors are due to the confusionsbetween positive vs. neutral and negative vs. neu-tral classes.This is our first participation, and within theshort period of time we developed the systemswith reasonable accuracies.
There are still manyways to improve the performance.
Possible im-mediate future extension will be to investigate andimplement more features, specific to the task.344ReferencesAgarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, andRebecca Passonneau.
2011.
Sentiment Analysis ofTwitter Data.
ACL Workshop on Languages in So-cial Media LSM-2011, pages 30?38.Luciano Barbosa and Junlan Feng.
2010.
Robust Sen-timent Detection on Twitter from Biased and NoisyData.
In Proceedings of the 23rd International Con-ference on Computational Linguistics (COLING),Beijing, China.Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,and Ricard Gavald?a.
Detecting Sentiment Changein Twitter Streaming Data.
Journal of MachineLearning Research - Proceedings Track, 17.Cynthia Chew and Gunther Eysenbach.
2010.
Pan-demics in the Age of Twitter: Content Analysisof Tweets during the 2009 H1N1 Outbreak.
PLoSONE, 5(11):e14118+.Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Ab-dur Chowdury.
2009.
Twitter Power: Tweets asElectronic Word of Mouth.
Journal of the Ameri-can Society for Information Science and Technology,60(11):2169?2188.Thorsten Joachims, 1999.
Making Large Scale SVMLearning Practical, pages 169?184.
MIT Press,Cambridge, MA, USA.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter Sentiment Analysis: TheGood the Bad and the OMG!
In Proceedings ofthe Fifth International Conference on Weblogs andSocial Media, ICWSM, pages 538?541, Barcelona,Spain.Benjamin Mandel, Aron Culotta, John Boulahanis,Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.2012.
A Demographic Analysis of Online Senti-ment during Hurricane Irene.
In Proceedings ofthe Second Workshop on Language in Social Media,LSM 12, Stroudsburg.Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-dan Zhu.
2013.
NRC-Canada: Building the State-of-the-art in Sentiment Analysis of Tweets.
In Pro-ceedings of Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Sev-enth International Workshop on Semantic Evalua-tion (SemEval 2013), pages 321?327, Atlanta, Geor-gia.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
Semeval-2013 Task 2: Sentiment Analysis inTwitter.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), Volume 2: Pro-ceedings of the Seventh International Workshop onSemantic Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, USA.Alexander Pak and Patrick Paroubek.
2010.
Twit-ter based System: Using Twitter for Disambiguat-ing Sentiment Ambiguous Adjectives.
In Proceed-ings of the 5th International Workshop on SemanticEvaluation, SemEval 10, pages 436?439, Los Ange-les,USA.Hassan Saif, Yulan He, and Harith Alani.
2012.
Se-mantic Sentiment Analysis of Twitter.
In ISWC?12Proceedings of the 11th International Conference onthe Semantic Web - Volume Part I, pages 508?524.Marcel Salathe and Shashank Khandelwal.
2011.
As-sessing Vaccination Sentiments with Online SocialMedia: Implications for Infectious Disease Dynam-ics and Control.
PLoS Computational Biology,7(10):e14118+.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag New York, Inc.,New York, NY, USA.Sudha Verma, Sarah Vieweg, William Corvey, LeysiaPalen, JamesMartin, Martha Palmer, Aaron Schram,and Kenneth Anderson.
2011.
Natural LanguageProcessing to the Rescue?
Extracting SituationalAwareness Tweets during Mass Emergency.
In Pro-ceedings of the AAAI Conference on Weblogs andSocial Media, Velingrad.345
