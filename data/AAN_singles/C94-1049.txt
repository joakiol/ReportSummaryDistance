CO-OCCURRENCE VECTORS FROM CORPORA VS.D ISTANCE VECTORS FROM DICT IONARIESYoshiki Niwa ~tnd Yoshihiko NittaAdvanced  Research  l~aboratory,  l l i tach i ,  l ,td.Hatoyama,  Saitam~t 350-03 ,  .
)ap;m{n iwa2,  nitta)~harl.hitachi.co.jpAbstractA comparison W~LS made of vectors derived by usingordinary co-occurrence statistics from large text cor-pora and of vectors derived by measuring the inter-word distances in dictionary definitions.
The precisionof word sense disambiguation by using co-occurrencevectors frorn the 1987 Wall Street Journal (20M totalwords) was higher than that by using distance vectorsfrom the Collins English l)ictionary (60K head words+ 1.6M definition words), llowever, other experimen--tal results suggest hat distance vectors contain somedifferent semantic information from co-occurrence vec-tors.1 In t roduct ionWord vectors reflecting word meanings are expected toenable numerical approaches to semantics.
Some earlyattempts at vector representation i  I)sycholinguisticswere the semantic d(O'erential approach (Osgood etal.
1957) and the associative distribution apl)roach(Deese 1962).
llowever, they were derived manuallythrough psychological experiments.
An early attemptat automation was made I)y Wilks el aL (t990) us-.ing co-occurrence statistics.
Since then, there haw"been some promising results from using co-occurrencevectors, such as word sense disambiguation (Schiitze\[993), and word clustering (Pereira eL al.
1993).llowever, using the co-occurrence statistics re-quires a huge corpus that covers even most rare words.We recently developed word vectors that are derivedfrom an ordinary dictionary by measuring the inter-word distances in the word definitions (Niwa and Nitta1993).
'this method, by its nature, h~s no prol)lomhandling rare words.
In this paper we examine thensefldness of these distance vectors as semantic re Wresentations by comparing them with co-occur,'encevectors.2 Dis tance  VectorsA reference network of the words in a dictionary (Fig.1) is used to measure the distance between words, q'henetwork is a graph that shows which words are used inthe.
definition of each word (Nitta 1988).
The networkshown in Fig.
1 is for a w~ry small portion of the refer-ence network for the Collins English 1)ictionary (1979edition) in the CI)-I{OM I (Liberman 1991), with 60Khead words -b 1.6M definition words.writing unit (Or)\ /wordcomnmnieation / ~ alphMmtlcal\ L_ \  /l anguag , - - - - - -  dictionaryo, / \ ( : ) p,.~ople ~,ook (O~)Fig.
1.
Portion of a reference network.For example, tile delinition for diclionarg is %book ill which the words of a language are listed al-phabetically .
.
.
.  "
The word dicliona~d is thus linkedto the words book, word, language, and alphabelical.A word w~etor is defined its the list of distancesfrom a word to a certain sew of selected words, whichwe call origins.
The words in Fig.
1 marked withOi (unit, book, and people) m'e assumed to be originwords.
In principle, origin words can be freoly chosen.In our exl~eriments we used mi(Idle fi'equency words:the 51st to 1050th most frequent words in the refer-ence Collins English I)ictiotmry (CI';D),The distance w~ctor fl)r diclionary is deriwM it'* foblOWS:~) ... disti~uc,, ((ticl., 01)dictionary ~ 1 .
.
.
distance (dict., 0'2)2 .
.
.
distance (dicL, Oa)The i-4,h element is the distance (the length of theshortest path) between diclionary and the i-th origin,Oi.
To begin, we assume every link has a constantlength o\[' 1.
The actual definition for link length willbe given later.If word A is used in the definition of word B, t.he,mwords are expected to be strongly related.
This is thebasis of our hypothesis that the distances in the refi~r-ence network reflect the associative distances betweenwords (Nitta 1933).304Use (if Refe.renee Networks  l{efi,rence net-works have been successfully used its neural networks(by Vdronis and Ide (1990) for word sense disainl)igua-tion) and as fields for artificial association, such itsspreading activation (by Kojiina and l:urugori (1993)for context-coherence measurement).
The distancevector of a word can be considered to be a list, of theactivation strengths at the origin nodes when the wordnode is activated.
Therefore, distance w~ctors can beexpected to convey almost the santo information asthe entire network, and clearly they are Ili~icli easierto handle.Dependence  on  Diet io ln l r les  As a seinant{crepresentation of  words, distltllCe w~ctors are expectedto depend very weakly  on the particular source dic-tionary.
We eolilpared two sets of distance vectors,one from I,I)OCE (Procter 1978) and the other fromCOBUILD (Sinclair 1987), and verified that their dif-ference is at least snlaller than the difDrence of theword definitions themselves (Niwa and Nitta 1993).We will now describe some technical details al)Olltthe derivation of distance vectors.Lh lk  Length  Distance measurenient in a refer-ence network depends on the detinition of link length.Previously, we assumed for siinplicity that every linkhas a construct length.
Ilowever, this shnph; definitionseerns tlnnatllral because it does not relh'.ct word fre-quency.
Because tt path through low-fi'equency words(rare words) implies a strong relation, it should beineasnred ms a shorter path.
Therefore, we use the fol-lowing definition of link length, which takes accotltltof word frequency.length (Wi,  W2) d,'I:-- - log (7N-77'~.,)n'This shows the length of the links between wordsWi(i = 1,2) ill Fig, 2, where Ni denotes the total mini-bet of links front and to }Vi and n denotes the uulnlmrof direct links bt.
'tween these two words.Fig.
2 Links between two words.Normal i za t ion  l)istance vectors ;ire norrrial-ized by first changing each coordinal,e into its devi-ation in the coordin;tLe:v --: ( 'v i )  -~+ v'  = vi  - -  aiwhere a i and o i are the average and the standaM devi-ation of the distances fi'om the i-th origin.
Next, eachcoordinal.e is changed hire its deviation in thc ~ vector:where t?
and cd are tile average .~_llld i,he standard de-viation of v} (i = I .... ).3 Co-occur ro .
r i c ( ;  VectorsWe use ordinary co-o(:Clll'rl;llCe statistics ;tlld i l lellSllrethe co-occurrei/ce likelihood betweeii two words, Xand Y, hy the Inutua\] hiforlnaLioii estilnate.
((\]hurchand ll~uiks 1989)'.l(X,V) = i<,g i P(x IV)P(X) 'where P(X) is the oCcilrreilce, density of word X hiwhole corllus, and the conditional probabil i ty l'(x Iv)is the density of X in a neight>orhood of word Y, llerethe neighl)orhood is defined as 50 words lie.fore or afters.iiy appearance of word Y.
(There is a variety of neigh-borhood definitions Sllch as "100 sllrrollllding words"(Yarowsky 1992) and "within a distance of no morethall 3 words igllorh/g filnction words" (I)agarl el, al.l~)n:/).
)The logarithm with '-t-' is dellned to be () for an ar-g;ument less than 1.
Negative stimates were neglectedbecause they are mostly accidental except when X andY are frequent enough (Chnrch and lIanl,:s 1989).A co-occurence vector of a word is defined as thelist of co-occtlrrellce likelihood of the word with a cer-tahi set o\['orighi words.
We tlsed the salne set oforightwords ;is for the distance vectors.I(w, ?30l(w,%)CV\[w} =I(w, 0,,,)C(~-oeel l l ' l 'e l le (  ~, V(~t'tol ' .When the frequency of X or Y is zero, we can notmeasure their co-c, ccurence likelihood, and such crusesare not exceptional.
This sparseness problem is well-known and serious in the co-occurrence sLatisC\[cs.
Weused as ~ corpus the 1!
)87 Wall Street; JournM in theCI)-I~.OM i (1991), which has a total of 20M words.
'\]'he nUll iber o f  words which appeared al, least OIlCe,was about 50% of the total 62I( head words of CEI),and tile.
percentage Of" tile word-origin pairs which ap-peared tit least once was about 16% of total 62K ?1K (=62M) pairs.
When the co-occurrence likelihoodCall liOt Im ineasurc~d> I,he vahle I(X, Y) was set to 0.3054 Exper imenta l  R, esu l t sWe compared the two vector representations by usingthem for the following two semantic tmsks.
The first isword sense disambiguation (WSD) based on the simi-larity of context vectors; the second is the learning ofpositive or negative meanings from example words.With WSD, the precision by using co-occurrencevectors from a 20M words corpus was higher than byusing distance vectors from the CEIL4 .1  Word  Sense  D isambiguat ionWord sense disambiguation is a serious semantic prob-lena.
A variety of approaches have been proposed forsolving it.
For example, V(!ronis and Ide (1990) usedreference networks as neural networks, llearst (1991)used (shallow) syntactic similarity between contexts,Cowie el al.
(1992) used simulated annealing for quickparallel disambignation, and Yarowsky (1992) usedco-occurrence statistics between words and thesauruscategories.Our disambiguation method is based on the shn-ilarity of context vectors, which was originated byWilks el al.
(1990).
In this method, a context vec-tor is the sum of its constituent word vectors (exceptthe target word itself).
That is, tile context vector forcontext,C:  .
.
.W_N .
.
.W_ l  WWl  ...WN, ... ~is N tv(c) = ~ V(w~).i=  -NThe similarity of contexts is measured by the angleof their vectors (or actually the inner product of theirnormalized vectors).V(C I )  V(C2)sim(C~,C.~) = lv(C~)l IV(C2)l'Let word w have senses l, s2, ..., sll I } a | ' ld  each sells(;have the following context examples.Sense Context Examplessl C l l ,  C12, ,.. Cln,s2 C~l, C22 .
.
.
.
C~,~:Sm Cml ,  Cm2, ... Cmn,,,We infer that the sense of word w in an arhitrarycontext C is si if for some j the similarity, sire(C, Cij),is maximum among all tile context examples.Another possible way to infer the sense is to choosesense si such that the average of sim(C, Cij) overj = 1,2,...,hi is maximum.
We selected the firstmethod because a peculiarly similar example is moreimportant han the average similarity.Figure 3 (next page) shows the disamhiguationprecision for 9 words.
For each word, we selected twosenses hown over each graph.
These senses were cho-sen because they are clearly different and we couldcollect sufficient nmnber (more than 20) of contextexamples.
The names of senses were chosen from thecategory names in Roger's International Thesaurus,except organ's.The results using distance vectors are shown byclots (.
?
.
), and using co-occurrence vectors from the1987 vsa (20M words) by cir,.tes (o o o).A context size (x-axis) of, for example, 10 means10 words before tile target word and 10 words aftertile target word.
Wc used 20 examples per sense;they were taken from tlle 1988 WSJ.
Tile test contextswere from the 1987 WSJ: The nmnber of test contextsvaries from word to word (100 to 1000).
The precisionis the simple average of the respective precisions forthe two senses.The results of Fig.
3 show that the precision byusing co-occurrence vectors are higher than that byusing distance vectors except two cases, interest andcustoms.
And we have not yet found a case where thedistance vectors give higher precision.
Therefore weconclude that co-occurrence vectors are advantageousover distance vectors to WSD based on the contextsimilarity.The sl)arseness problem for co-occurrence vectorsis not serious in this case because ach context consistsof plural words.4 .2  Learn ing  o f  pos i t ivc-or -ne#al iveAnother experiment using the same two vector repre-sentations was done to measure tile learning of positiveor negative meanings.
1,'igure 4 shows tile changes inthe precision (the percentage of agreement with theauthors' combined judgement).
The x-axis indicatestile nunll)er of example words for each positive or ~teg-alive pair.
Judgement w~s again done by using thenearest example.
The example and test words areshown in Tables 1 and 2, respectively.In this case, the distance vectors were advanta-geous.
The precision by using distance vectors in-creased to about 80% and then leveled off, while theprecision by using co-occurrence vectors tayed arouud60%.
We can therefore conclude that the propertyof positive-or-negative is reflected in distance vectorsmore strongly than ill co-occurrence vectors.
Tilesparseness l)roblem is supposed to be a major factorin this case.306%10050su i t  (CLDTI I ING / LAWSUIT)o oo^0~???
o o o o 100e$ 4)4) o ?
??
4)50o come.
vector?
( l i s tance vectorj iA ,  , , l l J ,  J l , ,5 10 20 30 40 50context  s i zeorgan  (BODY / MUSIC)OOoooo o o o a *oo oo ?oo0 oe  e e, ,  , i t  , J I L J  i ,5 10 20 311 4'(} 50i s s l l ( :  (EMERGENCE / TOPIC)1 O(l500 ?%ooo0o ~ 8 g~ 0000eQOO0I l l ,  , I.)
;0 a; ;0 50%lOO50?
l ) .
I l k  (CONTAINER / VE I I ICLE)  o rd ( : r  (C ( )MMAND / DEMAND)  a ( | ( l r i ; ss  ( I IAB ITAT  / SPEECH)tOO?o  o ?????~..
~ ~~0 eO00 ?O o5(I0 CO-(R;.
V?.C \[,O ra distance vector5 iCl 20 30 40 50context  s i zeo o~o e~ oo  o ooou  o O o000 O eO ?
?
?x_t_~t.~ ~ 'l'O ' 3t() ~- -a  5 20 'I0 50I00500 0 0 @ ?
0 Ooo e$*e.  8?
O o ?
?
?.
.
.
.
.
.
.  '
; ,\['0 ' 5 l0 20 3 50%I0050race  (CLASS / OPPOSIT ION)o ~?o  oo o o ?
g o o oe,U,~ ee  o ?
?
(, ?0 ?o cO-oc, vector?
d i s tance  vector, , , , , , , ,  , , jlo io :;o ;0 5ocontext  s ize( ' .
l l S | ;O I I IS  ( I IA I I I T (p l . )
/ SE l l .V ICE)  h l~ .eres ( ;  (CURIOS ITY  / DEBT)100 I 0(1,5\[)o Oo".?.o.'?o?'.
.
?
oo?
?
0 ?
?5 I0 2() 30 40 505(I~ooeoo o ~ O?
?ooo  ~ ?o5 10 20 30 40 50F ig .
3 D isambiguat ion  o f  9 words  hy  us ing  ro -o t : rm' rence  vectors (ooo)  m, l  hyus ing  d i s tance  w.*ctors ( - - , ) .
(The number  of examples  is 10 \['or each sense.)307100%50%.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
t~ J J J J J  J~2 ??
?
?
?
?
o?
?
?
?
?
o  ?o ?
?
?
?00000000000 0 0 0000000o .
.
.
.
.
~_990 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ oo co-oc, vector (20M)?
distance vectori i  t l i i t  l I t t l l I t l  t r i l l  I t t  t I l l~010 20number  of example pairsFig.
4 Learning of posiffve-or-negative.Table 1 Example pairs.positive negative positive negative1 t rue  fa l se  16  l ) roper ly  c r ime2 new wrong  17 succeed  ( l ie3 bet te r  d i sease  18 wor th  v io lent,l c lear  angry  19 f r iend ly  hur t5 pleasure noise 20 use fu l  punishment,6 correct pain 21 success poor7 pleasant lose 22 intcrestlng badly8 snltable destroy 23 active fail9 clean dangerous 2,1 polite suffering10 advantage harm 25 win enemy11 love kill 26 improve rude12 best fear 27 favour danger131,115snccessfld war 28 development angerattractive ill 29 happy wastepowerful foolish 30 praise doubtTable 2 Test words.positive (20 words)balanced elaborate elation eligible enjoyfluent honorary Imnourable hopeful hopefullyinfluential interested legible lustre normalrecreation replete resilient restorative sincerenegative (30 words)conflmion cuckold dally daumation dullferocious flaw hesitate hostage huddleinattentive liverlsh lowly mock neglectqueer rape ridiculous avage scantysceptical schizophrenia scoff scrnffy shipwrecksuperstition sycophant trouble wicked worthless4.3  Supplementary DataIn the experiments discussed above, the corpus size forco-occurrence v ctors was set to 20M words ('87 WSJ)and the vector dimension for both co-occurrence anddistance vectors wins set to 1000. llere we show somesupplementary data that support hese parameter set-tings.a.
Corpus  size (for co -occur rence  vectors )Figure 5 shows the change in disambiguation pre-eision as the corpus size for co-occurrence statisticsincreases from 200 words to 20M words.
(The wordsare suit, issue and race, the context size is 10, andthe number of examples per sense is 10.)
These threegraphs level off after around IM words.
Therefore, acorpus size of 20M words is not too small.\]00%50%* o ooO o*0  0 0* 0 00 ?
0 0, *  * * * * *o ?o  o g~?o 0 O0ooo* suit0 lSS l le0 facelfl 3 104 10 ~ 1M 10Meorptls size (wor(I)Fig.
5 Dependence of the disambiguation precisionon the corpus size for c.o-occurrence v ctors.context size: 10,number of examples: 10/sense,vector dimension: 1000.l).
Vector  D imens ionFigure 6 (next page) shows the dependence of dis-ambiguation precision on the vector dimension for (i)co-occurrence and (ii) distance vectors.
As for co-occurrence vectors, the precision levels off near a di-mension of 100.
Therefore, a dimension size of 1000 issuflicient or cvcn redumlant.
IIowever, in the distancevector's case, it is not clear whether the precision isleveling or still increasing around 1000 dimension.5 Conclus ion?
A comparison was nlade of co-occnrrence v ctorsfrom large text corpora and of distance vectorsfrom dictionary delinitions.?
For tile word sense disambiguation based on thecontext simihtrity, co-occurrence vectors fl'omtile 1987 Wall Street Journal (20M total words)was advantageous over distance vectors from theCollins l,;nglish Dictionary (60K head words +1.6M definition words).?
For learning positive or negalive meanings fromexample words, distance vectors gave remark-ably higher precision than co-occurrence v ctors.This suggests, though further investigation is re-quired, that distance w:ctors contain some dif-ferent semantic information from co-occurrencevectors.308lOO%s0%100%5o%Fig.
6(i) by co-oe, vectors* * * * ** 0 0 00 0 000 0 0 0 0 00 00 0 0* sult0 18S l le0 l ' J l ee= _ _ u _ _ L _ a ~ t ~10 100 1000vector dllllension(ii) by distance vectors~ 0 0 08 ooo# *## o?
Q OO o O O8 ?, suito issue.O race10 100 lOOOvector diln(msionI)ependence on vector dimension for (i) co-occurrence veetors and (ii) distance vectors.context size: 10, examples: 10/sense,corpus size for co-oe, vectors: 20M word.ReferencesKenneth W. Church and Patrick llanks.
1989.
Wordassociation orms, mutual information, and lexi-cography.
In Proceedings of lhe 27th Annual Meet-ing of the Association for Computalional Ling,is-tics, pages 76-83, Vancouver, Canada.Jim Cowie, Joe Guthrie, and Louise Guthrie.
1992.Lexieal disambiguation using simulated .:mtwal-ing.
In Proceedings of COI, ING-92, pages 1/59-365,Nantes, France.Ido Dagan, Shaul Marcus, and Shaul Markovitch.1993.
Contextual word similarity and estimationfrom sparse data.
In Proceedings of Ihe 31st An-nual Meeting of the Association for CompulationalLinguist&s, pages 164-171, Columbus, Ohio.James Deese.
1962.
On the structure of associativemeaning.
Psychological Review, 69(3):16F 175.Marti A. IIearst.
1991.
Noun homograph disambigna-tion using local context in large text eorl)ora.
InProceedings of lhe 71h Annum Confercncc of IheUniversily of Walerloo Center for lhc New OEI)and Text Research, pages 1-22, Oxford.llideki Kozima and Teiji Furugori.
1993.
Similaritybetween words computed by spreading actiw~tionon an english dictionary.
In Proceedings of I'7A CL-93, pages 232--239, Utrecht, the Netherlands.Mark Liberman, editor.
1991.
CD-ROM L Associa-rio,, for Comlmtational I,inguistics Data CollectionInitiative, University of Pennsylvania.Yoshihiko Nitta.
1988.
The referential structure, of theword definitions in ordinary dictionaries, h, Pro-ceedings of lhe Workshop on rite Aspects of Lex-icon for Natural Language Processing, LNL88-8,JSSST', pages I-21, Fukuoka University, Japan.
( i "  ,1 apanese) .Yoshihiko Nitta.
1993.
Refi.
'rential structure.
- anmchanism for giving word-delinition i  ordinarylexicons.
In C. Lee and II.
Kant, editors, Lan-guage, Information and Computation, pages 99-1 t0.
Thaehaksa, Seoul.Yoshiki Niwa and Yoshihiko Nitta.
1993.
Distancevector representation \[' words, derived from refe.r-ence networks i,t ordinary dictionaries.
MCCS 93-253, (;Oml)l,ting ll.esearch I,aboratory, New Mex-ico State University, l,as Cruces.C.
1';.
Osgood, (l. F. Such, and P. II.
Tantmnl)anln.1957.
7'he Measurement of Meaning.
Universityof Illinois Press, Urlmna.Fernando Pereira, Naftali Tishby, and IAIlian Lee.1993.
l)istributional clustering of english words.lit Proceedings of the 31st Annval Meeting of theAssociation for Computational Lin:luislics, pagesI 8;I 190, Colmnlms, Ohio.I'aul Procter, e<lit.or.
1978.
Longman Dictionary ofContemporary lCnglish (LI)OCE).
Long\]nan, liar-low, Essex, tirst edition.llinrich Sch/itze.
1993.
Word space.
% J. D. Cowan,q.
J. llanson an(I C. L. C, iles, editors, Advancesin Neural Information lb'ocessing ?
'ystems, pages8!
)5 902.
Morgan Kaufinann, San Mateo, Califof, l ia.John Sinclair, editor.
1987.
Collins COBUILD En-glish Language l)iclionary.
Collins and t.he Uni-w~rslty of llirmingham, London.Jean Ve'ro,fis and Nancy M. \[de.
1990.
Wordsense disambiguation with very large neural net-works extracted from machine readable dictionar-ies.
In Proceedings ofCOLING-90, pages 389-394,llelsinki.Yorick Wilks, I)a,, Fass, Cheng mint Guo, James 1".MeDolmhl, Tony Plate, and Ilrian M. Slator.
1990.Providing machine tractable dictionary tools.
Ma-chine Translation, 5(2):99 154.l)avid Yarowsky.
1992.
Word-sense disambigua-lion using statistieal models of roget's categoriestrained on large corpora.
In Proceedings ofCOLING-92, pages 454-460, Nantes, France.309
