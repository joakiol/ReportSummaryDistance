Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 384?388,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsClause Restructuring For SMT Not Absolutely HelpfulSusan Howlett and Mark DrasCentre for Language TechnologyMacquarie UniversitySydney, Australiasusan.howlett@students.mq.edu.au, mark.dras@mq.edu.auAbstractThere are a number of systems that use asyntax-based reordering step prior to phrase-based statistical MT.
An early work proposingthis idea showed improved translation perfor-mance, but subsequent work has had mixed re-sults.
Speculations as to cause have suggestedthe parser, the data, or other factors.
We sys-tematically investigate possible factors to givean initial answer to the question: Under whatconditions does this use of syntax help PSMT?1 IntroductionPhrase-based statistical machine translation (PSMT)translates documents from one human language toanother by dividing text into contiguous sequencesof words (phrases), translating each, and finally re-ordering them according to a distortion model.The PSMT distortion model typically does notconsider linguistic information, and as such encoun-ters difficulty in language pairs that require specificlong-distance reorderings, such as German?English.Collins et al (2005) address this problem by re-ordering German sentences to more closely paral-lel English word order, prior to translation by aPSMT system.
They find that this reordering-as-preprocessing approach results in a significant im-provement in translation performance over the base-line.
However, there have been several other systemsusing the reordering-as-preprocessing approach, andthey have met with mixed success.We systematically explore possible explanationsfor these contradictory results, and conclude that,while reordering is helpful for some sentences, po-tential improvement can be eroded by many aspectsof the PSMT system, independent of the reordering.2 Prior WorkReordering-as-preprocessing systems typically in-volve three steps.
First, the input sentence is parsed.Second, the parse is used to permute the words ac-cording to some reordering rules, which may beautomatically or manually determined.
Finally, aphrase-based SMT system is trained and tested us-ing the reordered sentences as input, in place of theoriginal sentences.
Many such systems exist, withresults being mixed; we review several here.Xia and McCord (2004) (English-to-French trans-lation, using automatically-extracted reorderingrules) train on the Canadian Hansard.
On a Hansardtest set, an improvement over the baseline was onlyseen if the translation system?s phrase table was re-stricted to phrases of length at most four.
On anews test set, the reordered system performed sig-nificantly better than the baseline regardless of themaximum length of phrases.
However, this improve-ment was only apparent with monotonic decoding;when using a distortion model, the difference dis-appeared.
Xia and McCord attribute the drop-offin performance on the Hansard set to similarity oftraining and test data.Collins et al (2005) (German-to-English) use sixhand-crafted reordering rules targeting the place-ment of verbs, subjects, particles and negation.
Theytrain and evaluate their system on Europarl text andobtain a BLEU score (Papineni et al, 2002) of 26.8,with the baseline PSMT system achieving 25.2.
Ahuman evaluation confirms that reordered transla-tions are generally (but not universally) better.On Web text, Xu et al (2009) report significantimprovements applying one set of hand-crafted rulesto translation from English to each of five SOV lan-384guages: Korean, Japanese, Hindi, Urdu and Turkish.Training on news text, Wang et al (2007)(Chinese-to-English, hand-crafted rules) report asignificant improvement over the baseline system onthe NIST 2006 test set, using a distance-based dis-tortion model.
Similar results are mentioned in pass-ing for a lexicalised distortion model.Also on news text, Habash (2007) (automatically-extracted rules, Arabic-to-English) reports a verylarge improvement when phrases are limited tolength 1 and translation is monotonic.
However,allowing phrases up to 7 words in length or usinga distance-based distortion model causes the differ-ence in performance to disappear.
Habash attributesthis to parser and alignment performance.
He alsoincludes oracle experiments, in which each systemoutperforms the other on 40?50% of sentences, sug-gesting that reordering is useful for many sentences.Zwarts and Dras (2007) implement six rules forDutch-to-English translation, analogous to those ofCollins et al (2005), as part of an exploration ofdependency distance in syntax-augmented PSMT.Considering only their baseline and reordered sys-tems, the improvement is from 20.7 to only 20.8;they attribute their poor result to the parser used.Howlett and Dras (2010) reimplement the Collinset al (2005) system for use in lattice-based transla-tion.
In addition to their main system, they give re-sults for the baseline and reordered systems, trainingand testing on Europarl and news text.
In strong con-trast to the results of Collins et al (2005), Howlettand Dras (2010) report 20.04 for the reordered sys-tem, below the baseline at 20.77.
They explain theirlower absolute scores as a consequence of the differ-ent test set, but do not explore the reversal in conclu-sion.
Like Habash (2007), Howlett and Dras (2010)include oracle experiments which demonstrate thatthe reordering is useful for some sentences.In this paper, we focus on the Collins et al (2005)and Howlett and Dras (2010) systems (hereafterCKK and HD), as they are the most similar but haveperhaps the most divergent results.
Possible expla-nations for the difference are differences in the re-ordering process, from either parser performance orimplementation of the rules, and differences in thetranslation process, including PSMT system setupand data used.
We examine parser performance in?3 and the remaining possibilities in ?4?5.Precision RecallDubey and Keller (2003) 65.49 70.45Petrov and Klein (2008) 69.23 70.41Howlett and Dras (2010) 72.78 73.15This paper, lowercased 71.09 73.16This paper, 50% data 68.65 70.86This paper, 50% data, lowerc.
67.59 70.23This paper, 25% data 65.24 67.13This paper, 10% data 61.56 63.01Table 1: Precision and recall for the parsers mentioned in?3.
The numbers are collated for reference only and arenot directly comparable; see the text for details.3 Parser PerformanceWe first compare the performance of the two parsersused.
CKK uses the Dubey and Keller (2003) parser,which is trained on the Negra corpus (Skut et al,1997).
HD instead uses the Berkeley parser (Petrovet al, 2006), trained on Negra?s successor, the largerTiger corpus (Brants et al, 2002).Refer to Table 1 for precision and recall for eachmodel.
Note that the CKK reordering requires notjust category labels (e.g.
NP) but also function labels(e.g.
SB for subject); parser performance typicallygoes down when these are learnt, due to sparsity.
Allmodels in Table 1 include function labels.Dubey and Keller (2003) train and test on theNegra corpus, with 18,602 sentences for training,1,000 development and 1,000 test, removing sen-tences longer than 40 words.Petrov and Klein (2008) train and test the Berke-ley parser on part of the Tiger corpus, with 20,894sentences for training and 2,611 sentences for eachof development and test, all at most 40 words long.The parsing model used by HD is trained onthe full Tiger corpus, unrestricted for length, with38,020 sentences for training and 2,000 sentencesfor development.
The figures reported in Table 1are the model?s performance on this developmentset.
With twice as much data, the increase in per-formance is unsurprising.From these figures, we conclude that sheer parsergrunt is unlikely to be responsible for the discrep-ancy between CKK and HD.
It is possible that parseroutput differs qualitatively in some important way;parser figures alone do not reveal this.Here, we reuse the HD parsing model, plus five385Data Set name SizeCKK Train 751,088Test 2,000WMT Train europarl-v4 1,418,115Tuning test2007 2,000news-test2008 2,051Test test2008 2,000newstest2009 2,525Table 2: Corpora used, and # of sentence pairs in each.additional models trained by the same method.
Thefirst is trained on the same data, lowercased; thenext two use only 19,000 training sentences (for onemodel, lowercased); the fourth uses 9,500 sentences;the fifth only 3,800 sentences.
The 50% data modelsare closer to the amount of data available to CKK,and the 25% and 10% models are to investigate theeffects of further reduced parser quality.4 ExperimentsWe conduct a number of experiments with the HDsystem to attempt to replicate the CKK and HD find-ings.
All parts of the system are available online.1Each experiment is paired: the reordered systemreuses the recasing and language models of its cor-responding baseline system, to eliminate one sourceof possible variation.
Training the parser with lessdata affects only the reordered systems; for experi-ments using these models, the corresponding base-lines (and thus the shared models) are not retrained.For each system pair, we also run the HD oracle.4.1 System VariationsCKK uses the PSMT system Pharaoh (Koehn et al,2003), whereas HD uses its successor Moses (Koehnet al, 2007).
In itself, this should not cause a dra-matic difference in performance, as the two systemsperform similarly (Hoang and Koehn, 2008).However, there are a number of other differencesbetween the two systems.
Koehn et al (2003) (andthus presumably CKK) use an unlexicalised distor-tion model, whereas HD uses a lexicalised model.CKK does not include a tuning (minimum error ratetraining) phase, unlike HD.
Finally, HD uses a 5-gram language model.
The CKK language model isunspecified; we assume a 3-gram model would be1http://www.showlett.id.au/LM DM T Base.
Reord.
Diff.
Oracle3 dist ?
25.58 26.73 +1.15 28.1126.63 +1.05 28.03Table 3: Replicating CKK.
Top row: full parsing model;second row: 50% parsing model.
Columns as for Table 4.more likely for the time.
We explore combinationsof all these choices.4.2 DataA likely cause of the results difference between HDand CKK is the data used.
CKK used Europarl fortraining and test, while HD used Europarl and newsfor training, with news for tuning and test.Our first experiment attempts to replicate CKK asclosely as possible, using the CKK training and testdata.
This data came already tokenized and lower-cased; we thus skip tokenisation in preprocessing,use the lowercased parsing models, and skip tokeni-sation and casing steps in the PSMT system.
We tryboth the full data and 50% data parsing models.Our next experiments use untokenised and casedtext from the Workshop on Statistical MachineTranslation.
To remain close to CKK, we use datafrom the 2009 Workshop,2 which provided Europarlsets for both training and development.
We useeuroparl-v4 for training, test2007 for tun-ing, and test2008 for testing.We also run the 3-gram systems of this set witheach of the reduced parser models.Our final experiments start to bridge the gap toHD.
We still train on europarl-v4 (divergingfrom HD), but substitute one or both of the tuningand test sets with those of HD: news-test2008and newstest2009 from the 2010 Workshop.3For the language model, HD uses both Europarland news text.
To remain close to CKK, we train ourlanguage models only on the Europarl training data,and thus use considerably less data than HD here.4.3 EvaluationAll systems are evaluated using case-insensitiveBLEU (Papineni et al, 2002).
HD used the NISTBLEU scorer, which requires SGML format.
TheCKK data is plain text, so instead we report scores2http://www.statmt.org/wmt09/translation-task.html3http://www.statmt.org/wmt10/translation-task.html386LM DM T Base.
Reord.
Diff.
Oracle3 dist ?
26.53 27.34 +0.81 28.93E 27.58 28.65 +1.07 30.31N 26.99 27.16 +0.17 29.37lex ?
27.35 27.88 +0.53 29.55E 28.34 28.76 +0.42 30.79N 27.77 28.27 +0.50 30.105 dist ?
27.23 28.12 +0.89 29.69E 28.28 28.94 +0.66 30.81N 27.42 28.38 +0.96 30.08lex ?
28.24 28.70 +0.46 30.47E 28.81 29.14 +0.33 31.24N 28.32 28.59 +0.27 30.69Table 4: BLEU scores for each experiment on Europarltest set.
Columns give: language model order, distortionmodel (distance, lexicalised), tuning data (none (?
), Eu-roparl, News), baseline BLEU score, reordered systemBLEU score, performance increase, oracle BLEU score.from the Moses multi-reference BLEU script (multi-bleu), using one reference translation.
Comparingthe scripts, we found that the NIST scores are alwayslower than multi-bleu?s on test2008, but higheron newstest2009, with differences at most 0.23.This partially indicates the noise level in the scores.5 ResultsResults for the first experiments, closely replicat-ing CKK, are given in Table 3.
The results are verysimilar to the those CKK reported (baseline 25.2, re-ordered 26.8).
Thus the HD reimplementation is in-deed close to the original CKK system.
Any qualita-tive differences in parser output not revealed by ?3,in the implementation of the rules, or in the PSMTsystem, are thus producing only a small effect.Results for the remaining experiments are given inTables 4 and 5, which give results on the test2008and newstest2009 test sets respectively, and Ta-ble 6, which gives results on the test2008 test setusing the reduced parsing models.We see that the choice of data can have a profoundeffect, nullifying or even reversing the overall result,even when the reordering system remains the same.Genre differences are an obvious possibility, but wehave demonstrated only a dependence on data set.The other factors tested?language model order,lexicalisation of the distortion model, and use of atuning phase?can all affect the overall performanceLM DM T Base.
Reord.
Diff.
Oracle3 dist ?
16.28 15.96 -0.32 17.12E 16.43 16.39 -0.04 17.92N 17.25 16.51 -0.74 18.40lex ?
16.81 16.34 -0.47 17.82E 16.75 16.35 -0.40 18.19N 17.75 17.02 -0.73 18.735 dist ?
16.44 15.97 -0.47 17.28E 16.21 15.89 -0.32 17.55N 17.27 16.96 -0.31 18.21lex ?
17.10 16.58 -0.52 18.16E 17.03 17.04 +0.01 18.76N 17.73 17.11 -0.62 19.01Table 5: Results on news test set.
Columns as for Table 4.DM T % Base.
Reord.
Diff.
Oracledist ?
50 26.53 27.26 +0.73 28.8525 27.03 +0.50 28.6610 27.01 +0.48 28.75E 50 27.58 28.50 +0.92 30.1925 28.27 +0.69 30.2110 28.17 +0.59 30.18lex ?
50 27.35 27.90 +0.55 29.5225 27.62 +0.27 29.4610 27.54 +0.19 29.42E 50 28.34 28.56 +0.22 30.5525 28.44 +0.10 30.4610 28.42 +0.08 30.42Table 6: Results using the smaller parsing models.Columns are as for Table 4 except LM removed (all are3-gram), and parser data percentage (%) added.gain of the reordered system, but less distinctly.
Re-ducing the quality of the parsing model (by trainingon less data) also has a negative effect, but the dropmust be substantial before it outweighs other factors.In all cases, the oracle outperforms both baselineand reordered systems by a large margin.
Its selec-tions show that, in changing test sets, the balanceshifts from one system to the other, but both stillcontribute strongly.
This shows that improvementsare possible across the board if it is possible to cor-rectly choose which sentences will benefit from re-ordering.6 ConclusionCollins et al (2005) reported that a reordering-as-preprocessing approach improved overall perfor-mance in German-to-English translation.
The reim-387plementation of this system by Howlett and Dras(2010) came to the opposite conclusion.We have systematically varied several aspects ofthe Howlett and Dras (2010) system and reproducedresults close to both papers, plus a full range in be-tween.
Our results show that choices in the PSMTsystem can completely erode potential gains of thereordering preprocessing step, with the largest effectdue to simple choice of data.
We have shown thata lack of overall improvement using reordering-as-preprocessing need not be due to the usual suspects,language pair and reordering process.Significantly, our oracle experiments show that inall cases the reordering system does produce bettertranslations for some sentences.
We conclude thateffort is best directed at determining for which sen-tences the improvement will appear.AcknowledgementsOur thanks to Michael Collins for providing the dataused in Collins et al (2005), and to members ofthe Centre for Language Technology and the anony-mous reviewers for their helpful comments.ReferencesSabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The TIGER tree-bank.
In Proceedings of the Workshop on Treebanksand Linguistic Theories, pages 24?41.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics,pages 531?540.Amit Dubey and Frank Keller.
2003.
Probabilistic pars-ing for German using sister-head dependencies.
InProceedings of the 41st Annual Meeting of the Asso-ciation for Computational Linguistics, pages 96?103.Nizar Habash.
2007.
Syntactic preprocessing for statis-tical machine translation.
In Proceedings of the MTSummit XI, pages 215?222.Hieu Hoang and Philipp Koehn.
2008.
Design of theMoses decoder for statistical machine translation.
InSoftware Engineering, Testing, and Quality Assurancefor Natural Language Processing, pages 58?65.Susan Howlett and Mark Dras.
2010.
Dual-path phrase-based statistical machine translation.
In Proceedingsof the Australasian Language Technology AssociationWorkshop, pages 32?40.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the Human Language Technology Confer-ence and the North American Association for Compu-tational Linguistics, pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics Companion Volume Pro-ceedings of the Demo and Poster Sessions, pages 177?180.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics, pages 311?318.Slav Petrov and Dan Klein.
2008.
Parsing German withlatent variable grammars.
In Proceedings of the ACL-08: HLT Workshop on Parsing German, pages 33?39.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 433?440.Wojciech Skut, Brigitte Krenn, Thorsten Brants, andHans Uszkoreit.
1997.
An annotation scheme forfree word order languages.
In Proceedings of the FifthConference on Applied Natural Language Processing,pages 88?95.Chao Wang, Michael Collins, and Philipp Koehn.
2007.Chinese syntactic reordering for statistical machinetranslation.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, pages 737?745.Fei Xia and Michael McCord.
2004.
Improving a statisti-cal MT system with automatically learned rewrite pat-terns.
In Proceedings of the 20th International Confer-ence on Computational Linguistics, pages 508?514.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a dependency parser to improveSMT for subject-object-verb languages.
In HumanLanguage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 245?253.Simon Zwarts and Mark Dras.
2007.
Syntax-based wordreordering in phrase-based statistical machine transla-tion: Why does it work?
In Proceedings of the MTSummit XI, pages 559?566.388
