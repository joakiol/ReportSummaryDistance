Determining Word Sense Dominance Using a ThesaurusSaif Mohammad and Graeme HirstDepartment of Computer ScienceUniversity of TorontoToronto, ON M5S 3G4, Canadafsmm,ghg@cs.toronto.eduAbstractThe degree of dominance of a sense of aword is the proportion of occurrences ofthat sense in text.
We propose four newmethods to accurately determine wordsense dominance using raw text and a pub-lished thesaurus.
Unlike the McCarthyet al (2004) system, these methods canbe used on relatively small target texts,without the need for a similarly-sense-distributed auxiliary text.
We perform anextensive evaluation using artificially gen-erated thesaurus-sense-tagged data.
In theprocess, we create a word?category co-occurrence matrix, which can be used forunsupervised word sense disambiguationand estimating distributional similarity ofword senses, as well.1 IntroductionThe occurrences of the senses of a word usuallyhave skewed distribution in text.
Further, the dis-tribution varies in accordance with the domain ortopic of discussion.
For example, the ?assertionof illegality?
sense of charge is more frequent inthe judicial domain, while in the domain of eco-nomics, the ?expense/cost?
sense occurs more of-ten.
Formally, the degree of dominance of a par-ticular sense of a word (target word) in a giventext (target text) may be defined as the ratio of theoccurrences of the sense to the total occurrences ofthe target word.
The sense with the highest domi-nance in the target text is called the predominantsense of the target word.Determination of word sense dominance hasmany uses.
An unsupervised system will benefitby backing off to the predominant sense in caseof insufficient evidence.
The dominance valuesmay be used as prior probabilities for the differ-ent senses, obviating the need for labeled train-ing data in a sense disambiguation task.
Naturallanguage systems can choose to ignore infrequentsenses of words or consider only the most domi-nant senses (McCarthy et al, 2004).
An unsuper-vised algorithm that discriminates instances intodifferent usages can use word sense dominance toassign senses to the different clusters generated.Sense dominance may be determined by sim-ple counting in sense-tagged data.
However, dom-inance varies with domain, and existing sense-tagged data is largely insufficient.
McCarthyet al (2004) automatically determine domain-specific predominant senses of words, where thedomain may be specified in the form of an un-tagged target text or simply by name (for exam-ple, financial domain).
The system (Figure 1) au-tomatically generates a thesaurus (Lin, 1998) us-ing a measure of distributional similarity and anuntagged corpus.
The target text is used for thispurpose, provided it is large enough to learn a the-saurus from.
Otherwise a large corpus with sensedistribution similar to the target text (text pertain-ing to the specified domain) must be used.The thesaurus has an entry for each word type,which lists a limited number of words (neigh-bors) that are distributionally most similar to it.Since Lin?s distributional measure overestimatesthe distributional similarity of more-frequent wordpairs (Mohammad and Hirst, Submitted), theneighbors of a word corresponding to the predom-inant sense are distributionally closer to it thanthose corresponding to any other sense.
For eachsense of a word, the distributional similarity scoresof all its neighbors are summed using the semanticsimilarity of the word with the closest sense of the121TARGETAUXLARYIISIMILARLY SENSE DISTRIBUTEDDOMINANCE VALUESTHESAURUSLIN?SDCRPUSOWORDNETTEXTFigure 1: The McCarthy et al system.TARGETAUXLARYIIDOMINANCE VALUESDCRPUSOWCCMTEXTPUBLISHED THESAURUSFigure 2: Our system.neighbor as weight.
The sense that gets the highestscore is chosen as the predominant sense.The McCarthy et al system needs to re-train(create a new thesaurus) every time it is to de-termine predominant senses in data from a differ-ent domain.
This requires large amounts of part-of-speech-tagged and chunked data from that do-main.
Further, the target text must be large enoughto learn a thesaurus from (Lin (1998) used a 64-million-word corpus), or a large auxiliary text witha sense distribution similar to the target text mustbe provided (McCarthy et al (2004) separatelyused 90-, 32.5-, and 9.1-million-word corpora).By contrast, in this paper we present a methodthat accurately determines sense dominance evenin relatively small amounts of target text (a fewhundred sentences); although it does use a corpus,it does not require a similarly-sense-distributedcorpus.
Nor does our system (Figure 2) needany part-of-speech-tagged data (although that mayimprove results further), and it does not need togenerate a thesaurus or execute any such time-intensive operation at run time.
Our method standson the hypothesis that words surrounding the tar-get word are indicative of its intended sense, andthat the dominance of a particular sense is pro-portional to the relative strength of association be-tween it and co-occurring words in the target text.We therefore rely on first-order co-occurrences,which we believe are better indicators of a word?scharacteristics than second-order co-occurrences(distributionally similar words).2 ThesauriPublished thesauri, such as Roget?s and Mac-quarie, divide the English vocabulary into arounda thousand categories.
Each category has a listof semantically related words, which we will callcategory terms or c-terms for short.
Words withmultiple meanings may be listed in more than onecategory.
For every word type in the vocabularyof the thesaurus, the index lists the categories thatinclude it as a c-term.
Categories roughly cor-respond to coarse senses of a word (Yarowsky,1992), and the two terms will be used interchange-ably.
For example, in the Macquarie Thesaurus,bark is a c-term in the categories ?animal noises?and ?membrane?.
These categories represent thecoarse senses of bark.
Note that published the-sauri are structurally quite different from the ?the-saurus?
automatically generated by Lin (1998),wherein a word has exactly one entry, and itsneighbors may be semantically related to it in anyof its senses.
All future mentions of thesaurus willrefer to a published thesaurus.While other sense inventories such as WordNetexist, use of a published thesaurus has three dis-tinct advantages: (i) coarse senses?it is widelybelieved that the sense distinctions of WordNet arefar too fine-grained (Agirre and Lopez de LacalleLekuona (2003) and citations therein); (ii) compu-tational ease?with just around a thousand cate-gories, the word?category matrix has a manage-able size; (iii) widespread availability?thesauriare available (or can be created with relativelyless effort) in numerous languages, while Word-Net is available only for English and a few ro-mance languages.
We use the Macquarie The-saurus (Bernard, 1986) for our experiments.
Itconsists of 812 categories with around 176,000c-terms and 98,000 word types.
Note, however,that using a sense inventory other than WordNetwill mean that we cannot directly compare perfor-mance with McCarthy et al (2004), as that wouldrequire knowing exactly how thesaurus sensesmap to WordNet.
Further, it has been argued thatsuch a mapping across sense inventories is at bestdifficult and maybe impossible (Kilgarriff and Yal-lop (2001) and citations therein).1223 Co-occurrence Information3.1 Word?Category Co-occurrence MatrixThe strength of association between a particularcategory of the target word and its co-occurringwords can be very useful?calculating word sensedominance being just one application.
To thisend we create the word?category co-occurrencematrix (WCCM) in which one dimension is thelist of all words (w1;w2; : : :) in the vocabulary,and the other dimension is a list of all categories(c1;c2; : : :).c1 c2 : : : c j : : :w1 m11 m12 : : : m1 j : : :w2 m21 m22 : : : m2 j : : :............: : : : : :wi mi1 mi2 : : : mi j : : :..................A particular cell, mi j, pertaining to word wi andcategory c j, is the number of times wi occurs ina predetermined window around any c-term of c jin a text corpus.
We will refer to this particularWCCM created after the first pass over the textas the base WCCM.
A contingency table for anyparticular word w and category c (see below) canbe easily generated from the WCCM by collaps-ing cells for all other words and categories intoone and summing up their frequencies.
The ap-plication of a suitable statistic will then yield thestrength of association between the word and thecategory.c :cw nwc nw::w n:c n::Even though the base WCCM is created fromunannotated text, and so is expected to be noisy,we argue that it captures strong associations rea-sonably accurately.
This is because the errorsin determining the true category that a word co-occurs with will be distributed thinly across anumber of other categories (details in Section 3.2).Therefore, we can take a second pass over the cor-pus and determine the intended sense of each wordusing the word?category co-occurrence frequency(from the base WCCM) as evidence.
We canthus create a newer, more accurate, bootstrappedWCCM by populating it just as mentioned ear-lier, except that this time counts of only the co-occurring word and the disambiguated categoryare incremented.
The steps of word sense disam-biguation and creating new bootstrapped WCCMscan be repeated until the bootstrapping fails to im-prove accuracy significantly.The cells of the WCCM are populated using alarge untagged corpus (usually different from thetarget text) which we will call the auxiliary cor-pus.
In our experiments we use a subset (all exceptevery twelfth sentence) of the British NationalCorpus World Edition (BNC) (Burnard, 2000) asthe auxiliary corpus and a window size of 5words.
The remaining one twelfth of the BNC isused for evaluation purposes.
Note that if the tar-get text belongs to a particular domain, then thecreation of the WCCM from an auxiliary text ofthe same domain is expected to give better resultsthan the use of a domain-free text.3.2 Analysis of the Base WCCMThe use of untagged data for the creation of thebase WCCM means that words that do not re-ally co-occur with a certain category but ratherdo so with a homographic word used in a differ-ent sense will (erroneously) increment the countscorresponding to the category.
Nevertheless, thestrength of association, calculated from the baseWCCM, of words that truly and strongly co-occurwith a certain category will be reasonably accuratedespite this noise.We demonstrate this through an example.
As-sume that category c has 100 c-terms and each c-term has 4 senses, only one of which correspondsto c while the rest are randomly distributed amongother categories.
Further, let there be 5 sentenceseach in the auxiliary text corresponding to everyc-term?sense pair.
If the window size is the com-plete sentence, then words in 2,000 sentences willincrement co-occurrence counts for c. Observethat 500 of these sentences truly correspond to cat-egory c, while the other 1500 pertain to about 300other categories.
Thus on average 5 sentences cor-respond to each category other than c. Thereforein the 2000 sentences, words that truly co-occurwith c will likely occur a large number of times,while the rest will be spread out thinly over 300 orso other categories.We therefore claim that the application of asuitable statistic, such as odds ratio, will resultin significantly large association values for word?category pairs where the word truly and stronglyco-occurs with the category, and the effect of noise123will be insignificant.
The word?category pairshaving low strength of association will likely beadversely affected by the noise, since the amountof noise may be comparable to the actual strengthof association.
In most natural language applica-tions, the strength of association is evidence for aparticular proposition.
In that case, even if associ-ation values from all pairs are used, evidence fromless-reliable, low-strength pairs will contribute lit-tle to the final cumulative evidence, as comparedto more-reliable, high-strength pairs.
Thus even ifthe base WCCM is less accurate when generatedfrom untagged text, it can still be used to provideassociation values suitable for most natural lan-guage applications.
Experiments to be describedin section 6 below substantiate this.3.3 Measures of AssociationThe strength of association between a sense orcategory of the target word and its co-occurringwords may be determined by applying a suitablestatistic on the corresponding contingency table.Association values are calculated from observedfrequencies (nwc;n:c;nw:; and n::), marginal fre-quencies (nw = nwc+nw:; n: = n:c+n::; nc =nwc + n:c; and n: = nw: + n::), and the samplesize (N = nwc +n:c+nw:+n::).
We provide ex-perimental results using Dice coefficient (Dice),cosine (cos), pointwise mutual information (pmi),odds ratio (odds), Yule?s coefficient of colligation(Yule), and phi coefficient (?
)1.4 Word Sense DominanceWe examine each occurrence of the target wordin a given untagged target text to determine dom-inance of any of its senses.
For each occurrencet 0 of a target word t, let T 0 be the set of words(tokens) co-occurring within a predetermined win-dow around t 0; let T be the union of all such T 0and let Xt be the set of all such T 0.
(Thus jXt j isequal to the number of occurrences of t, and jT j isequal to the total number of words (tokens) in thewindows around occurrences of t.) We describe1Measures of association (Sheskin, 2003):cos(w;c) =nwcpnwpnc; pmi(w;c) = log nwcNnwnc;odds(w;c) = nwcn::nw:n:c; Yule(w;c) =podds(w;c) 1podds(w;c)+1;Dice(w;c) =2nwcnw+nc; ?
(w;c) = (nwcn::)  (nw:n:c)pnwn:ncn:UnweightedWeighteddisambiguationImplicit senseExplicit sensedisambiguationvotingvotingDI,WDE,WDI,UE,UDFigure 3: The four dominance methods.four methods (Figure 3) to determine dominance(DI;W ;DI;U ;DE ;W ; and DE ;U ) and the underlyingassumptions of each.DI;W is based on the assumption that the moredominant a particular sense is, the greater thestrength of its association with words that co-occurwith it.
For example, if most occurrences of bankin the target text correspond to ?river bank?, thenthe strength of association of ?river bank?
with allof bank?s co-occurring words will be larger thanthe sum for any other sense.
Dominance DI;W of asense or category (c) of the target word (t) is:DI;W (t;c) =?w2T A(w;c)?c02senses(t) ?w2T A(w;c0)(1)where A is any one of the measures of associationfrom section 3.3.
Metaphorically, words that co-occur with the target word give a weighted vote toeach of its senses.
The weight is proportional tothe strength of association between the sense andthe co-occurring word.
The dominance of a senseis the ratio of the total votes it gets to the sum ofvotes received by all the senses.A slightly different assumption is that the moredominant a particular sense is, the greater the num-ber of co-occurring words having highest strengthof association with that sense (as opposed to anyother).
This leads to the following methodol-ogy.
Each co-occurring word casts an equal, un-weighted vote.
It votes for that sense (and noother) of the target word with which it has thehighest strength of association.
The dominanceDI;U of the sense is the ratio of the votes it getsto the total votes cast for the word (number of co-occurring words).DI;U(t;c) =jfw 2 T : Sns1(w; t) = cgjjT j(2)Sns1(w; t) = argmaxc02senses(t)A(w;c0) (3)Observe that in order to determine DI;W orDI;U , we do not need to explicitly disambiguate124the senses of the target word?s occurrences.
Wenow describe alternative approaches that may beused for explicit sense disambiguation of the targetword?s occurrences and thereby determine sensedominance (the proportion of occurrences of thatsense).
DE ;W relies on the hypothesis that the in-tended sense of any occurrence of the target wordhas highest strength of association with its co-occurring words.DE ;W (t;c) =jfT 0 2Xt : Sns2(T 0; t) = cgjjXt j(4)Sns2(T 0; t) = argmaxc02senses(t)?w2T 0A(w;c0) (5)Metaphorically, words that co-occur with the tar-get word give a weighted vote to each of its sensesjust as in DI;W .
However, votes from co-occurringwords in an occurrence are summed to determinethe intended sense (sense with the most votes) ofthe target word.
The process is repeated for alloccurrences that have the target word.
If eachword that co-occurs with the target word votes asdescribed for DI;U , then the following hypothesisforms the basis of DE ;U : in a particular occurrence,the sense that gets the maximum votes from itsneighbors is the intended sense.DE ;U(t;c) =jfT 0 2Xt : Sns3(T 0; t) = cgjjXt j(6)Sns3(T 0; t) = argmaxc02senses(t)jfw 2 T 0 : Sns1(w; t) = c0gj(7)In methods DE ;W and DE ;U , the dominance ofa sense is the proportion of occurrences of thatsense.The degree of dominance provided by all fourmethods has the following properties: (i) Thedominance values are in the range 0 to 1?a scoreof 0 implies lowest possible dominance, while ascore of 1 means that the dominance is highest.
(ii) The dominance values for all the senses of aword sum to 1.5 Pseudo-Thesaurus-Sense-Tagged DataTo evaluate the four dominance methods we wouldideally like sentences with target words annotatedwith senses from the thesaurus.
Since human an-notation is both expensive and time intensive, wepresent an alternative approach of artificially gen-erating thesaurus-sense-tagged data following theideas of Leacock et al (1998).
Around 63,700of the 98,000 word types in the Macquarie The-saurus are monosemous?listed under just oneof the 812 categories.
This means that on aver-age around 77 c-terms per category are monose-mous.
Pseudo-thesaurus-sense-tagged (PTST)data for a non-monosemous target word t (forexample, brilliant) used in a particular sense orcategory c of the thesaurus (for example, ?intel-ligence?)
may be generated as follows.
Identifymonosemous c-terms (for example, clever) be-longing to the same category as c. Pick sentencescontaining the monosemous c-terms from an un-tagged auxiliary text corpus.Hermione had a clever plan.In each such sentence, replace the monosemousword with the target word t. In theory the c-terms in a thesaurus are near-synonyms or at leaststrongly related words, making the replacement ofone by another acceptable.
For the sentence above,we replace clever with brilliant.
This results in(artificial) sentences with the target word usedin a sense corresponding to the desired category.Clearly, many of these sentences will not be lin-guistically well formed, but the non-monosemousc-term used in a particular sense is likely to havesimilar co-occurring words as the monosemous c-term of the same category.2 This justifies the useof these pseudo-thesaurus-sense-tagged data forthe purpose of evaluation.We generated PTST test data for the head wordsin SENSEVAL-1 English lexical sample space3 us-ing the Macquarie Thesaurus and the held out sub-set of the BNC (every twelfth sentence).6 ExperimentsWe evaluate the four dominance methods, likeMcCarthy et al (2004), through the accuracy ofa naive sense disambiguation system that alwaysgives out the predominant sense of the target word.In our experiments, the predominant sense is de-termined by each of the four dominance methods,individually.
We used the following setup to studythe effect of sense distribution on performance.2Strong collocations are an exception to this, and their ef-fect must be countered by considering larger window sizes.Therefore, we do not use a window size of just one or twowords on either side of the target word, but rather windowsof 5 words in our experiments.3SENSEVAL-1 head words have a wide range of possiblesenses, and availability of alternative sense-tagged data maybe exploited in the future.125(phi, pmi, odds, Yule): .11I,UD0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.10.20.30.40.50.60.70.80.91baselinebaselineAccuracyDistribution (alpha)Mean distance below upper boundDE,W (pmi, odds, Yule)(pmi)(phi, pmi,DDI,UI,WE,UI,W(phi, pmi, odds, Yule): .16(pmi): .03DDDE,W(pmi, odds, Yule): .02(phi, pmi,DE,Uupper boundupper boundodds, Yule)odds, Yule)lower bound lower boundFigure 4: Best results: four dominance methods6.1 SetupFor each target word for which we have PTSTdata, the two most dominant senses are identified,say s1 and s2.
If the number of sentences annotatedwith s1 and s2 is x and y, respectively, where x > y,then all y sentences of s2 and the first y sentencesof s1 are placed in a data bin.
Eventually the bincontains an equal number of PTST sentences forthe two most dominant senses of each target word.Our data bin contained 17,446 sentences for 27nouns, verbs, and adjectives.
We then generate dif-ferent test data sets d?
from the bin, where ?
takesvalues 0; :1; :2; : : : ;1, such that the fraction of sen-tences annotated with s1 is ?
and those with s2 is1 ?.
Thus the data sets have different dominancevalues even though they have the same number ofsentences?half as many in the bin.Each data set d?
is given as input to the naivesense disambiguation system.
If the predominantsense is correctly identified for all target words,then the system will achieve highest accuracy,whereas if it is falsely determined for all targetwords, then the system achieves the lowest ac-curacy.
The value of ?
determines this upperbound and lower bound.
If ?
is close to 0:5, theneven if the system correctly identifies the predom-inant sense, the naive disambiguation system can-not achieve accuracies much higher than 50%.
Onthe other hand, if ?
is close to 0 or 1, then thesystem may achieve accuracies close to 100%.
Adisambiguation system that randomly chooses oneof the two possible senses for each occurrence ofthe target word will act as the baseline.
Note thatno matter what the distribution of the two senses(?
), this system will get an accuracy of 50%.DI,W (odds), base: .08E,W(odds), bootstrapped: .02DMean distance below upper bound0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.10.20.30.40.50.60.70.80.91 upper bound upper boundbaselinebaselineAccuracyDistribution (alpha)DE,W (odds), bootstrapped(odds), baseDI,Wlower bound lower boundFigure 5: Best results: base vs. bootstrapped6.2 ResultsHighest accuracies achieved using the four dom-inance methods and the measures of associationthat worked best with each are shown in Figure 4.The table below the figure shows mean distancebelow upper bound (MDUB) for all ?
valuesconsidered.
Measures that perform almost iden-tically are grouped together and the MDUB val-ues listed are averages.
The window size used was5 words around the target word.
Each datasetd?, which corresponds to a different target text inFigure 2, was processed in less than 1 second ona 1.3GHz machine with 16GB memory.
Weightedvoting methods, DE ;W and DI;W , perform best withMDUBs of just .02 and .03, respectively.
Yule?scoefficient, odds ratio, and pmi give near-identical,maximal accuracies for all four methods with aslightly greater divergence in DI;W , where pmidoes best.
The ?
coefficient performs best forunweighted methods.
Dice and cosine do onlyslightly better than the baseline.
In general, re-sults from the method?measure combinations aresymmetric across ?
= 0:5, as they should be.Marked improvements in accuracy wereachieved as a result of bootstrapping the WCCM(Figure 5).
Most of the gain was provided bythe first iteration itself, whereas further iterationsresulted in just marginal improvements.
Allbootstrapped results reported in this paper pertainto just one iteration.
Also, the bootstrappedWCCM is 72% smaller, and 5 times faster atprocessing the data sets, than the base WCCM,which has many non-zero cells even though thecorresponding word and category never actuallyco-occurred (as mentioned in Section 3.2 earlier).1266.3 DiscussionConsidering that this is a completely unsupervisedapproach, not only are the accuracies achieved us-ing the weighted methods well above the baseline,but also remarkably close to the upper bound.
Thisis especially true for ?
values close to 0 and 1.
Thelower accuracies for ?
near 0.5 are understandableas the amount of evidence towards both senses ofthe target word are nearly equal.Odds, pmi, and Yule perform almost equallywell for all methods.
Since the number of timestwo words co-occur is usually much less thanthe number of times they occur individually, pmitends to approximate the logarithm of odds ra-tio.
Also, Yule is a derivative of odds.
Thus allthree measures will perform similarly in case theco-occurring words give an unweighted vote forthe most appropriate sense of the target as in DI;Uand DE ;U .
For the weighted voting schemes, DI;Wand DE ;W , the effect of scale change is slightlyhigher in DI;W as the weighted votes are summedover the complete text to determine dominance.
InDE ;W the small number of weighted votes summedto determine the sense of the target word may bethe reason why performances using pmi, Yule, andodds do not differ markedly.
Dice coefficient andcosine gave below-baseline accuracies for a num-ber of sense distributions.
This suggests that thenormalization4 to take into account the frequencyof individual events inherent in the Dice and co-sine measures may not be suitable for this task.The accuracies of the dominance methods re-main the same if the target text is partitioned as perthe target word, and each of the pieces is given in-dividually to the disambiguation system.
The av-erage number of sentences per target word in eachdataset d?
is 323.
Thus the results shown abovecorrespond to an average target text size of only323 sentences.We repeated the experiments on the baseWCCM after filtering out (setting to 0) cells withfrequency less than 5 to investigate the effect onaccuracies and gain in computation time (propor-tional to size of WCCM).
There were no markedchanges in accuracy but a 75% reduction in sizeof the WCCM.
Using a window equal to the com-plete sentence as opposed to 5 words on eitherside of the target resulted in a drop of accuracies.4If two events occur individually a large number of times,then they must occur together much more often to get sub-stantial association scores through pmi or odds, as comparedto cosine or the Dice coefficient.7 Related WorkThe WCCM has similarities with latent semanticanalysis, or LSA, and specifically with work bySchu?tze and Pedersen (1997), wherein the dimen-sionality of a word?word co-occurrence matrix isreduced to create a word?concept matrix.
How-ever, there is no non-heuristic way to determinewhen the dimension reduction should stop.
Fur-ther, the generic concepts represented by the re-duced dimensions are not interpretable, i.e., onecannot determine which concepts they representin a given sense inventory.
This means that LSAcannot be used directly for tasks such as unsuper-vised sense disambiguation or determining seman-tic similarity of known concepts.
Our approachdoes not have these limitations.Yarowsky (1992) uses the product of a mutualinformation?like measure and frequency to iden-tify words that best represent each category in theRoget?s Thesaurus and uses these words for sensedisambiguation with a Bayesian model.
We im-proved the accuracy of the WCCM using sim-ple bootstrapping techniques, used all the wordsthat co-occur with a category, and proposed fournew methods to determine sense dominance?two of which do explicit sense disambiguation.Ve?ronis (2005) presents a graph theory?based ap-proach to identify the various senses of a word in atext corpus without the use of a dictionary.
Highlyinterconnected components of the graph representthe different senses of the target word.
The node(word) with the most connections in a componentis representative of that sense and its associationswith words that occur in a test instance are used asevidence for that sense.
However, these associa-tions are at best only rough estimates of the associ-ations between the sense and co-occurring words,since a sense in his system is represented by asingle (possibly ambiguous) word.
Pantel (2005)proposes a framework for ontologizing lexical re-sources.
For example, co-occurrence vectors forthe nodes in WordNet can be created using the co-occurrence vectors for words (or lexicals).
How-ever, if a leaf node has a single lexical, then oncethe appropriate co-occurring words for this nodeare identified (coup phase), they are assigned thesame co-occurrence counts as that of the lexical.55A word may have different, stronger-than-chancestrengths of association with multiple senses of a lexical.These are different from the association of the word with thelexical.1278 Conclusions and Future DirectionsWe proposed a new method for creating a word?category co-occurrence matrix (WCCM) using apublished thesaurus and raw text, and applyingsimple sense disambiguation and bootstrappingtechniques.
We presented four methods to deter-mine degree of dominance of a sense of a word us-ing the WCCM.
We automatically generated sen-tences with a target word annotated with sensesfrom the published thesaurus, which we used toperform an extensive evaluation of the dominancemethods.
We achieved near-upper-bound resultsusing all combinations of the the weighted meth-ods (DI;W and DE ;W ) and three measures of asso-ciation (odds, pmi, and Yule).We cannot compare accuracies with McCarthyet al (2004) because use of a thesaurus insteadof WordNet means that knowledge of exactly howthe thesaurus senses map to WordNet is required.We used a thesaurus as such a resource, unlikeWordNet, is available in more languages, pro-vides us with coarse senses, and leads to a smallerWCCM (making computationally intensive oper-ations viable).
Further, unlike the McCarthy etal.
system, we showed that our system gives accu-rate results without the need for a large similarly-sense-distributed text or retraining.
The targettexts used were much smaller (few hundred sen-tences) than those needed for automatic creationof a thesaurus (few million words).The WCCM has a number of other applications,as well.
The strength of association between aword and a word sense can be used to determinethe (more intuitive) distributional similarity ofword senses (as opposed to words).
Conditionalprobabilities of lexical features can be calculatedfrom the WCCM, which in turn can be used in un-supervised sense disambiguation.
In conclusion,we provided a framework for capturing distribu-tional properties of word senses from raw text anddemonstrated one of its uses?determining wordsense dominance.AcknowledgmentsWe thank Diana McCarthy, Afsaneh Fazly, andSuzanne Stevenson for their valuable feedback.This research is financially supported by the Natu-ral Sciences and Engineering Research Council ofCanada and the University of Toronto.ReferencesEneko Agirre and O. Lopez de Lacalle Lekuona.
2003.Clustering WordNet word senses.
In Proceedingsof the Conference on Recent Advances on NaturalLanguage Processing (RANLP?03), Bulgaria.J.R.L.
Bernard, editor.
1986.
The Macquarie The-saurus.
Macquarie Library, Sydney, Australia.Lou Burnard.
2000.
Reference Guide for the BritishNational Corpus (World Edition).
Oxford Univer-sity Computing Services.Adam Kilgarriff and Colin Yallop.
2001.
What?s ina thesaurus.
In Proceedings of the Second Interna-tional Conference on Language Resources and Eval-uation (LREC), pages 1371?1379, Athens, Greece.Claudia Leacock, Martin Chodrow, and George A.Miller.
1998.
Using corpus statistics and WordNetrelations for sense identification.
ComputationalLinguistics, 24(1):147?165.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 17th Inter-national Conference on Computational Linguistics(COLING-98), pages 768?773, Montreal, Canada.Diana McCarthy, Rob Koeling, Julie Weeds, and JohnCarroll.
2004.
Finding predominant senses inuntagged text.
In Proceedings of the 42nd An-nual Meeting of the Association for ComputationalLinguistics (ACL-04), pages 280?267, Barcelona,Spain.Saif Mohammad and Graeme Hirst.
Submitted.
Dis-tributional measures as proxies for semantic related-ness.Patrick Pantel.
2005.
Inducing ontological co-occurrence vectors.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics (ACL-05), pages 125?132, Ann Arbor,Michigan.Hinrich Schu?tze and Jan O. Pedersen.
1997.
Acooccurrence-based thesaurus and two applicationsto information retreival.
Information Processingand Management, 33(3):307?318.David Sheskin.
2003.
The handbook of paramet-ric and nonparametric statistical procedures.
CRCPress, Boca Raton, Florida.Jean Ve?ronis.
2005.
Hyperlex: Lexical cartographyfor information retrieval.
To appear in ComputerSpeech and Language.
Special Issue on Word SenseDisambiguation.David Yarowsky.
1992.
Word-sense disambiguationusing statistical models of Roget?s categories trainedon large corpora.
In Proceedings of the 14th Inter-national Conference on Computational Linguistics(COLING-92), pages 454?460, Nantes, France.128
