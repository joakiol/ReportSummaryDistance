Mining Transliterations from Web Query Results:An Incremental ApproachJin-Shea Kuo Haizhou Li Chih-Lung LinChung-Hwa Telecomm.Laboratories, Taiwand8807302 @gmail.comInstitute for InfocommResearch, Singapore 119613hzli@ieee.comChung Yuan ChristianUniversity, Taiwanlinclr@gmail.comAbstractWe study an adaptive learning frameworkfor phonetic similarity modeling (PSM) thatsupports the automatic acquisition of trans-literations by exploiting minimum priorknowledge about machine transliteration tomine transliterations incrementally from thelive Web.
We formulate an incrementallearning strategy for the framework basedon Bayesian theory for PSM adaptation.The idea of incremental learning is to bene-fit from the continuously developing his-tory to update a static model towards the in-tended reality.
In this way, the learningprocess refines the PSM incrementallywhile constructing a transliteration lexiconat the same time on a development corpus.We further demonstrate that the proposedlearning framework is reliably effective inmining live transliterations from Web queryresults.1 IntroductionTransliteration is a process of rewriting a wordfrom one language into another by preserving itspronunciation in its original language, also knownas translation-by-sound.
It usually takes place be-tween languages with different scripts, for example,from English to Chinese, and words, such as propernouns, that do not have ?easy?
or semantic transla-tions.The increasing size of multilingual content onthe Web has made it a live information source richin transliterations.
Research on automatic acquisi-tion of transliteration pairs in batch mode hasshown promising results (Kuo et al, 2006).
Indealing with the dynamic growth of the Web, it isalmost impossible to collect and store all its con-tents in local storage.
Therefore, there is a need todevelop an incremental learning algorithm to minetransliterations in an on-line manner.
In general, anincremental learning technique is designed foradapting a model towards a changing environment.We are interested in deducing the incrementallearning method for automatically constructing anEnglish-Chinese (E-C) transliteration lexicon fromWeb query results.In the deduction, we start with a phonetic simi-larity model (PSM), which measures the phoneticsimilarity between words in two different scripts,and study the learning mechanism of PSM in bothbatch and incremental modes.
The contributions ofthis paper include: (i) the formulation of a batchlearning framework and an incremental learningframework for PSM learning; (ii) a comparativestudy of the batch and incremental unsupervisedlearning strategies.In this paper, Section 2 briefly introduces priorwork related to machine transliteration.
In Section3, we formulate the PSM and its batch and incre-mental learning algorithms while in Section 4, wediscuss the practical issues in implementation.
Sec-tion 5 provides a report on the experiments con-ducted and finally, we conclude in Section 6.2 Related WorkMuch of research on extraction of transliterationshas been motivated by information retrieval tech-niques, where attempts to extracting transliterationpairs from large bodies of corpora have been made.Some have proposed extracting translations fromparallel or comparable bitexts using co-occurrenceanalysis or a context-vector approach (Fung andYee, 1998; Nie et al, 1999).
These methods com-pare the semantic similarities between source andtarget words without taking their phonetic similari-ties into account.Another direction of research is focused on es-16Sixth SIGHAN Workshop on Chinese Language Processingtablishing the phonetic relationship between trans-literation pairs.
This typically involves the encod-ing of phoneme- or grapheme-based mapping rulesusing a generative model trained from a large bi-lingual lexicon.
Suppose that EW and CW form anE-C transliteration pair.
The phoneme-based ap-proach (Knight & Graehl, 1998) first converts EWinto an intermediate phonemic representation andthen converts the phonemic representation into itsChinese counterpart CW.
The grapheme-based ap-proach, also known as direct orthographical map-ping (Li et al, 2004), which treats transliteration asa statistical machine translation problem undermonotonic constraints, has also achieved promisingresults.Many efforts have also been channeled to tap-ping the wealth of the Web for harvesting translit-eration/translation pairs.
These include studying thequery logs (Brill et al, 2001), unrelated corpora(Rapp, 1999), and comparable corpora (Sproat et al2006).
To establish cross-lingual correspondence inthe harvest, these algorithms usually rely on one ormore statistical clues (Lam et al, 2004), such asthe correlation between word frequencies, and cog-nates of similar spelling or pronunciations.
In doingso, two things are needed: first, a robust mecha-nism that establishes statistical relationships be-tween bilingual words, such as a phonetic similar-ity model which is motivated by transliterationmodeling research; and second, an effective learn-ing framework that is able to adaptively discovernew events from the Web.In Chinese/Japanese/Korean (CJK) Web pages,translated terms are frequently accompanied bytheir original Latin words, with the Latin wordsserving as the appositives of the CJK words.
Inother words, the E-C pairs are always closely col-located.
Inspired by this observation in CJK texts,some algorithms were proposed (Kuo et al, 2006)to search over the close context of an English wordin a Chinese predominant bilingual snippet fortransliteration.Unfortunately, many of the reported works havenot taken the dynamic nature of the Web into ac-count.
In this paper, we study the learning frame-work of the phonetic similarity model, whichadopts a transliteration modeling approach fortransliteration extraction from the Web in an in-cremental manner.3 Phonetic Similarity ModelPhonetic similarity model (PSM) is a probabilisticmodel that encodes the syllable mapping betweenE-C pairs.
Let 1{ ,... ,... }m MES e e e= be a sequence ofEnglish syllables derived from EW and1{ ,... ,... }n NCS s s s=  be the sequence of Chinese syl-lables derived from CW, represented by a Chinesecharacter string 1,... ,...,n NCW w w w?
.
If each of theEnglish syllables is drawn from a vocabulary of Xentries, 1{ ,..., }m Ie x x?
, and each of the Chinesesyllable from a vocabulary of Y entries,1{ ,..., }n Js y y?
, then the E-C transliteration can beconsidered as a generative process formulated bythe noisy channel model, which recovers the inputCW from the observed output EW.
ApplyingBayesian rule, we have Eq.
(1), where ( | )P EW CWis estimated to characterize the noisy channel,known as the transliteration probability and( )P CW  is a language model to characterize thesource language.
( | ) ( | ) ( ) / ( )P CW EW P EW CW P CW P EW= .
(1)Following the translation-by-sound principle,( | )P EW CW can be approximated by the phoneticprobability ( | )P ES CS , which is given by Eq.
(2).
( | ) max ( , | ),P ES CS P ES CS??
?= ?
(2)where ?
is the set of all possible alignment pathsbetween ES and CS.
To find the best alignmentpath ?
, one can resort to a dynamic warping algo-rithm (Myers and Rabiner, 1981).
Assuming condi-tional independence of syllables in ES and CS, wehave1( | ) ( | )k kKm nkP ES CS P e s==?
where k is theindex of alignment.
We rewrite Eq.
(1) as,( | ) ( | ) ( ) / ( )P CW EW P ES CS P CW P EW?
.
(3)The language model ( )P CW in Eq.
(3) can be repre-sented by the n-gram statistics of the Chinese char-acters derived from a monolingual corpus.
Usingbigram to approximate the n-gram model, we have1 12( ) ( ) ( | )Nn nnP CW P w P w w ?=?
?
.
(4)Removing ( )P EW  from Eq.
(3) which is not a func-tion of CW, a PSM ?
now consists of both( | )P ES CS and ( )P CW  parameters (Kuo et al,2007).
We now look into the mathematic formula-tion for the learning of ( | )P ES CS  parameters froma bilingual transliteration lexicon.3.1 Batch Learning of PSM17Sixth SIGHAN Workshop on Chinese Language ProcessingA collection of manually selected or automaticallyextracted E-C pairs can form a transliteration lexi-con.
Given such a lexicon for training, the PSMparameters can be estimated in a batch mode.
Aninitial PSM is bootstrapped using limited priorknowledge such as a small amount of translitera-tions, which may be obtained by exploiting co-occurrence information (Sproat et al, 2006).
Thenwe align the E-C pairs using the PSM ?
and derivesyllable mapping statistics.Suppose that we have the event counts ,i jc =( , )m i n jcount e x s y= = , and ( )j n jc count s y= =  for agiven transliteration lexicon D with alignments ?
.We would like to find the parameters|i jP = ( | )m i n jP e x s y= = , ,m ne s< >??
, that maxi-mize the probability,,|( , | ) ( | ) i jcm n j i i jP D P e s P??
?
= =?
?
?
,       (5)where |{ , 1,..., , 1,..., }i jP i I j J?
= = = , with maximumlikelihood estimation (MLE) criteria, subject to theconstraints of | 1,i jiP j= ??
.
Rewriting Eq.
(5) inlog-likelihood ( LL ), |( , | )log ( | ) logm n i j i jj iLL DP e s c P??
?= =?
??
(6)It is described as the cross-entropy of the true datadistribution ,i jc with regard to the PSM model.Given an alignment ???
, the MLE estimate ofPSM is:| , /i j i j jP c c= .
(7)With a new PSM, one is able to arrive at a newalignment.
This is formulated as an expectation-maximization (EM) process (Dempster, 1977),which assumes that there exists a mappingD??
,where ?
is introduced as the latent information,also known as missing data in the EM literature.The EM algorithm maximizes the likelihood prob-ability ( | )P D ?
over ?
by exploiting( | ) ( , | )P D P D??
= ?
??
.The EM process guarantees non-decreasing like-lihood probability ( | )P D ?
through multiple EMsteps until it converges.
In the E-step, we derive theevent counts ,i jc  and jc  by force-aligning all theE-C pairs in the training lexicon D  using a PSM.In the M-step, we estimate the PSM parameters ?by Eq.(7).
The EM process also serves as a refiningprocess to obtain the best alignment between the E-C syllables.
In each EM cycle, the model is updatedafter observing the whole corpus D .
An EM cycleis also called an iteration in batch learning.
Thebatch learning process is described as follows anddepicted in Figure 1.Figure 1.
Batch learning of PSMBatch Learning Algorithm:Start: Bootstrap PSM parameters |i jP using priorphonetic mapping knowledge;E-Step: Force-align corpus D  using |i jP  to obtain?
and hence the counts of ,i jc  and jc ;M-Step: Re-estimate | , /i j i j jP c c=  using the countsfrom E-Step;Iterate: Repeat E-Step and M-Step until ( | )P D ?converges;3.2 Incremental Learning of PSMIn batch learning all the training samples have to becollected in advance.
In a dynamically changingenvironment, such as the Web, new samples alwaysappear and it is impossible to collect all of them.Incremental learning (Zavaliagkos, 1995) is de-vised to achieve rapid adaptation towards the work-ing environment by updating the model as learningsamples arrive in sequence.
It is believed that if thestatistics for the E-step are incrementally collectedand the parameters are frequently estimated, incre-mental learning converges quicker because the in-formation from the new data contributes to the pa-rameter estimation more effectively than the batchalgorithm does (Gotoh et al, 1998).
In incrementallearning, the model is typically updated progres-sively as the training samples become available andthe number of incremental samples may vary fromas few as one to as many as they are available.
Inthe extreme case where all the learning samples areIterateInitialPSME-StepTrainingCorpusM-Step Final PSM18Sixth SIGHAN Workshop on Chinese Language Processingavailable and the updating is done after observingall of them, the incremental learning becomes batchlearning.
Therefore, the batch learning can be con-sidered as a special case of the incremental learning.The incremental learning can be formulatedthrough maximum a posteriori (MAP) framework,also known as Bayesian learning, where we assumethat the parameters ?
are random variables subjectto a prior distribution.
A possible candidate for theprior distribution of |i jP  is the Dirichlet densityover each of the parameters |i jP (Bacchiani et al,2006).
Let |{ , 1,..., }j i jP i I?
= = , we introduce,| 1|( ) ,i jhj i jiP P j?
??
?
??
,   (8)where | 1i ji h =?
, and ?
, which can be empiricallyset, is a positive scalar.
Assuming H is the set ofhyperparameters, we have as many hyperparame-ters |i jh H?
as the parameters |i jP .
The probabilityof generating the aligned transliteration lexicon isobtained by integrating over the parameter space,( ) ( | ) ( )P D P D P d= ?
?
??
.This integration can be easily written down in aclosed form due to the conjugacy between Dirichletdistribution | 1| i jhi jiP ?
??
and the multinomial dis-tribution ,|i jci i jP?
.
Instead of finding ?
thatmaximizes ( | )P D ?
with MLE, we maximize aposteriori (MAP) probability as follows:argmax ( | ) argmax ( | ) ( ) / ( )argmax ( | ) ( ) (9)MAP P D P D P P DP D P?
???
= ?
= ?
?= ?
?The MAP solution uses a distribution to model theuncertainty of the parameters ?
, while the MLEgives a point estimation (Jelinek, 1990; MacKay,1994).
We rewrite Eq.
(9) as Eq.
(10) using Eq.
(5)and Eq.
(8)., | 1|argmax i j i jjc hmapj i jiP ?+ ???
?
?
(10)Eq.
(10) can be seen as a Dirichlet function of ?given H , or a multinomial function of H given ?
.With given prior H , the MAP estimation is there-fore similar to the MLE problem which is to findthe mode of the kernel density in Eq.
(10).| | |(1 )i j i j i jP h f?
?= + ?
,             (11)where | , /i j i j jf c c= , ,/( )i ji c?
?
?= +?
.One can find that ?
serves as a weighting factorbetween the prior and the current observations.
Thedifference between MLE and MAP strategy lies inthe fact that MAP introduces prior knowledge intothe parameter updating formula.
Eq.
(11) assumesthat the prior parameters H  are known and staticwhile the training samples are available all at once.The idea of incremental learning is to benefitfrom the continuously developing history to updatethe static model towards the intended reality.
As isoften the case, the Web query results in an on-lineapplication arrive in sequence.
It is of practical useto devise such an incremental mechanism thatadapts both parameters and the prior knowledgeover time.
The quasi-Bayesian (QB) learningmethod offers a solution to it (Bai and Li, 2006).Let?s break up a training corpus D into a se-quence of sample subsets 1 2{ , ,..., }TD D D D=  anddenote an accumulated sample subset ( )tD =1 2{ , ,..., },1tD D D t T?
?
as an incremental corpus.Therefore, we have ( )TD D= .
The QB method ap-proximates the posterior probability ( 1)( | )tP D ?
?by the closest tractable prior density ( 1)( | )tP H ?
?with ( 1)tH ?
evolved from historical corpus ( 1)tD ?
,( 1),( ) ( )( 1)1|1argmax ( | )argmax ( | ) ( | )argmax , .ti j it tQBttI c hi jiP DP D P DP j????
?+ ?=??
= ??
?
?= ??
(12)QB estimation offers a recursive learningmechanism.
Starting with a hyperparameter set(0)H  and a corpus subset 1D , we estimate (1)H  and(1)QB?
, then (2)H  and (2)QB?
and so on until ( )tH  and( )tQB?
as observed samples arrive in sequence.
Theupdating of parameters can be iterated between thereproducible prior and posterior estimates as in Eq.
(13) and Eq.
(14).
Assuming T ??
, we have thefollowing:Incremental Learning Algorithm:Start: Bootstrap (0)QB?
and (0)H using prior phoneticmapping knowledge and set 1t = ;E-Step: Force-align corpus subset tD  using ( 1)tQB??
,compute the event counts ( ),ti jc  and reproduce priorparameters ( 1) ( )t tH H?
?
.
( ) ( 1) ( ),| | /t t ti ji j i jh h c?= + ?
(13)19Sixth SIGHAN Workshop on Chinese Language ProcessingM-Step: Re-estimate parameters ( )( ) tt QBH ??
and|i jP  using the counts from E-Step.
( ) ( ) ( )| | |/t t ti j i j i jiP h h= ?
(14)EM cycle: Repeat E-Step and M-Step until( )( | )tP D?
converges.Iterate: Repeat T EM cycles covering the entiredata set D in an iteration.The algorithm updates the PSM as training sam-ples become available.
The scalar factor ?
can beseen as a forgetting factor.
When ?
is big, the up-date of hyperparameters favors the prior.
Otherwise,current observation is given more attention.
As forthe sample subset size | |tD , if we set | | 100tD = ,each EM cycle updates ?
after observing every100 samples.
To be comparable with batch learning,we define an iteration here to be a sequence of EMcycles that covers the whole corpus D. If corpus Dhas a fixed size ( )| |TD , an iteration means T EMcycles in incremental learning.4 Mining Transliterations from the WebSince the Web is dynamically changing and newtransliterations come out all the time, it is better tomine transliterations from the Web in an incre-mental way.
Words transliterated by closely ob-serving common guidelines are referred to as regu-lar transliterations.
However, in Web publishing,translators in different regions may not observe thesame guidelines.
Sometimes they skew the translit-erations in different ways to introduce semanticimplications, also known as wordplay, resulting incasual transliterations.
Casual transliteration leadsto multiple Chinese transliteration variants for thesame English word.
For example, ?Disney?
may betransliterated into ???
?/Di-Shi-Ni/1?, ????/Di-Si-Nai/?
and ???
?/Di-Si-Nai/?.Suppose that a sufficiently large, manually vali-dated transliteration lexicon is available, a PSMcan be built in a supervised manner.
However, thismethod hinges on the availability of such a lexicon.Even if a lexicon is available, the derived modelcan only be as good as what the training lexiconoffers.
New transliterations, such as casual ones,may not be well handled.
It is desirable to adapt thePSM as new transliterations become available, also1 The Chinese words are romanized in Hanyu Pinyin.referred to as the learning-at-work mechanism.Some solutions have been proposed recently alongthis direction (Kuo et al, 2006).
However, the ef-fort was mainly devoted to mitigating the need ofmanual labeling.
A dynamic learning-at-workmechanism for mining transliterations has not beenwell studied.Here we are interested in an unsupervised learn-ing process, in which we adapt the PSM as we ex-tract transliterations.
The learning-at-work frame-work is illustrated in Figure 2.
As opposed to amanually labeled training corpus in Figure 1, weinsert into the EM process an automatic translitera-tion extraction mechanism, search and rank, asshown in the left panel of Figure 2.
The search andrank shortlists a set of transliterations from theWeb query results or bilingual snippets.Figure 2.
Diagram of unsupervised transliterationextraction ?
learning-at-work.4.1 Search and RankWe obtain bilingual snippets from the Web byiteratively submitting queries to the Web searchengines (Brin and Page, 1998).
Qualified sentencesare extracted from the results of each query.
Eachqualified sentence has at least one English word.Given a qualified sentence, first, the competingChinese transliteration candidates are denoted as aset ?
, from which we would like to pick the mostlikely one.
Second, we would like to know if thereis indeed a Chinese transliteration CW in the closecontext of the English word EW.We propose ranking the candidates using thePSM model to find the most likely CW for a givenEW.
The CW candidate that gives the highest poste-rior probability is considered the most probableFinalPSMInitial PSME-StepM-StepThe WebTransliterationsSearch andRankFinalLexiconIterate20Sixth SIGHAN Workshop on Chinese Language Processingcandidate CW ?
.argmax ( | )argmax ( | ) ( )CWCWCW P CW EWP ES CS P CW?????
=?
(15)The next step is to examine if CW ?
and EW indeedform a genuine E-C pair.
We define the confidenceof the E-C pair as the posterior odds similar to thatin a hypothesis test under the Bayesian interpreta-tion.
We have 0H , which hypothesizes thatCW ?
and EW  form an E-C pair, and 1H , whichhypothesizes otherwise, and use posterior odd ?
(Kuo et al, 2006) for hypothesis tests.Our search and rank formulation can be seen asan extension to a prior work (Brill et al, 2001).The posterior odd ?
is used as the confidencescore so that E-C pairs extracted from differentcontexts can be directly compared.
In practice, weset a threshold for ?
to decide on a cutoff point forE-C pairs short-listing.
In this way, the search andrank is able to retrieve a collection of translitera-tions from the Web given a PSM.4.2 Unsupervised Learning StrategyNow we can carry out PSM learning as formulatedin Section 3 using the transliterations as if theywere manually validated.
By unsupervised batchlearning, we mean to re-estimate the PSM aftersearch and rank over the whole database, i.e., ineach iteration.
Just as in supervised learning, onecan expect the PSM performance to improve overmultiple iterations.
We report the F-measure ateach iteration.
The extracted transliterations alsoform a new training corpus in next iteration.In contrast to the batch learning, incrementallearning updates the PSM parameters as the train-ing samples arrive in sequence.
This is especiallyuseful in Web mining.
With the QB incrementaloptimization, one can think of an EM process thatcontinuously re-estimates PSM parameters as theWeb crawler discovers new ?territories?.
In thisway, the search and rank process gathers qualifiedtraining samples tD after crawling a portion of theWeb.
Note that the incremental EM process up-dates parameters more often than batch learningdoes.
To evaluate performance of both learning, wedefine an iteration to be T EM cycles in incre-mental learning on a training corpus ( )TD D=  asdiscussed in Section 3.2.5 ExperimentsTo obtain the ground truth for performance evalua-tion, each possible transliteration pair is manuallychecked based on the following criteria: (i) only thephonetic transliteration is extracted to form a trans-literation pair; (ii) multiple E-C pairs may appear inone sentence; (iii) an EW can have multiple validChinese transliterations and vice versa.
The valida-tion process results in a collection of qualified E-Cpairs, also referred to as distinct qualified translit-eration pairs (DQTPs), which form a translitera-tion lexicon.To simulate the dynamic Web, we collected aWeb corpus, which consists of about 500 MB ofWeb pages, referred to as SET1.
From SET1,80,094 qualified sentences were automatically ex-tracted and 8,898 DQTPs were further selectedwith manual validation.To establish a reference for performance bench-marking, we first initialize a PSM, referred to asseed PSM hereafter, using randomly selected 100seed DQTPs.
By exploiting the seed PSM on all8,898 DQTPs, we train a PSM in a supervisedbatch mode and improve the PSM on SET1 aftereach iteration.
The performance defined below inprecision, recall and F-measure in the 6th iterationis reported in Table 1 and the F-measure is alsoshown in Figure 3.# _ /# _ ,# _ /# _ ,2 /( )precision extracted DQTPs extracted pairsrecall extracted DQTPs total DQTPsF measure recall precision recall precision==?
= ?
?
+Precision Recall F-measureClosed-test 0.834 0.663 0.739Table 1.
The performance achieved by supervisedbatch learning on SET1.We use this closed test (supervised batch learning)as the reference point for unsupervised experiments.Next we further implement two PSM learningstrategies, namely unsupervised batch and unsu-pervised incremental learning.5.1 Unsupervised Batch LearningWe begin with the same seed PSM.
However, weuse transliterations that are extracted automaticallyinstead of manually validated DQTPs for training.Note that the transliterations are extracted and col-lected at the end of each iteration.
It may differfrom one iteration to another.
After re-estimating21Sixth SIGHAN Workshop on Chinese Language Processingthe PSM in each iteration, we evaluate performanceon SET1.Comparing the two batch mode learning strate-gies in Figure 3, it is observed that learning sub-stantially improves the seed PSM after the first it-eration.
Without surprise, the supervised learningconsistently outperforms the unsupervised one,which reaches a plateau at 0.679 F-measure.
Thisperformance is considered as the baseline for com-parison in this paper.
The unsupervised batch learn-ing presented here is similar to that in (Kuo et al,2006).0.450.550.650.751 2 3 4 5 6#IterationF-measureSupervised BatchUnsupervised BatchU-Incremental (100)U-Incremental (5,000)Figure 3.
Comparison of F-measure over iterations(U-Incremental: Unsupervised Incremental).5.2 Unsupervised Incremental LearningWe now formulate an on-line2 unsupervised incre-mental learning algorithm:(i) Start with the seed PSM, set 1t = ;(ii) Extract | |tD  quasi-transliterations pairs fol-lowed by E-Step in incremental learning algo-rithm;(iii) Re-estimate PSM using | |tD  (M-Step), 1t t= + ;(iv) Repeat (ii) and (iii) to crawl over a corpus.To simulate the on-line incremental learning justdescribed, we train and test on SET1 because of theavailability of gold standard and comparison withperformance by batch mode.
We empirically set0.5?
=  and study different | |tD settings.
An itera-tion is defined as multiple cycles of steps (ii)-(iii)that screen through the whole SET1 once.
We runmultiple iterations.The performance of incremental learning with| | 100tD = and | | 5,000tD = are reported in Figure 3.It is observed that incremental learning benefitsfrom more frequent PSM updating.
With | | 100tD = ,it not only attains good F-measure in the first itera-2 In an actual on-line environment, we are not supposed tostore documents, thus no iteration can take place.tion, but also outperforms that of unsupervisedbatch learning along the EM process.
The PSMupdating becomes less frequent for larger | |tD .When | |tD  is set to be the whole corpus, then in-cremental learning becomes a batch mode learning,which is evidenced by | | 5,000tD =  and it performsclose to the batch mode learning.
The experimentsin Figure 3 are considered closed tests.
Next wemove on to an actual on-line experiment.5.3 Learning from the Live WebIn practice, it is possible to extract bilingual snip-pets of interest by repeatedly submitting queries tothe Web.
With the learning-at-work mechanism,we can mine the query results for up-to-date trans-literations in an on-line environment.
For example,by submitting ?Amy?
to search engines, we mayget ?Amy-??/Ai-Mi/?
and, as a by-product, ?Jes-sica-???/Jie-Xi-Ka/?
as well.
In this way, newqueries can be generated iteratively, thus new pairsare discovered.Following the unsupervised incremental learningalgorithm, we start the crawling with the same seedPSM as in Section 5.2.
We adapt the PSM as every100 quasi-transliterations are extracted, i.e.| | 100tD = .
The crawling stops after accumulating67,944 Web pages, where there are 100 snippets atmost in a page, with 2,122,026 qualified sentences.We obtain 123,215 distinct E-C pairs when thecrawling stops.
For comparison, we also carry outunsupervised batch learning over the same2,122,026 qualified sentences in a single iterationunder such an on-line environment.
As the goldstandard for this live corpus is not available, werandomly select 500 quasi-transliteration pairs formanual checking of precision (see Table 2).
It isfound that incremental learning is more productivethan batch learning in discovering transliterationpairs.
This finding is consistent with the test resultson SET1.Unsupervised BatchUnsupervisedIncremental#distinct E-C pairs 67,708 123,215Estimated Precision 0.758 0.768Table 2.
Comparison between the unsupervisedbatch and incremental learning from live Web.The live Web corpus was used in transliterationextraction using active learning (Kuo et al, 2006).22Sixth SIGHAN Workshop on Chinese Language ProcessingKuo et al reported slightly better performance byannotating some samples manually and adaptingthe learning process in a batch manner.
However, itis apparent that, in an on-line environment, the un-supervised learning is more suitable for discoveringknowledge without resorting to human annotation;incremental learning is desirable as it does not re-quire storing all documents in advance.6 ConclusionsWe have proposed a learning framework for min-ing E-C transliterations using bilingual snippetsfrom a live Web corpus.
In this learning-at-workframework, we formulate the PSM learning methodand study strategies for PSM learning in both batchand incremental manners.
The batch mode learningbenefits from multiple iterations for improving per-formance, while the unsupervised incremental one,which does not require all the training data to beavailable in advance, adapts to the dynamicallychanging environment easily without compromis-ing the performance.
Unsupervised incrementallearning provides a practical and effective solutionto transliteration extraction from query results,which can be easily extended to other Web miningapplications.ReferencesS.
Bai and H. Li.
2006.
Bayesian Learning of N-gramStatistical Language Modeling, In Proc.
of ICASSP,pp.
1045-1048.M.
Bacchiani, B. Roark, M. Riley and R. Sproat.
2006.MAP Adaptation of Stochastic Grammars, ComputerSpeech and Language, 20(1), pp.
41-68.E.
Brill, G. Kacmarcik, C. Brockett.
2001.
Automati-cally Harvesting Katakana-English Term Pairs fromSearch Engine Query Logs, In Proc.
of NLPPRS, pp.393-399.S.
Brin and L. Page.
1998.
The Anatomy of a Large-scale Hypertextual Web Search Engine, In Proc.
of 7thWWW, pp.
107-117.A.
P. Dempster, N. M. Laird and D. B. Rubin.
1977.Maximum Likelihood from Incomplete Data via theEM Algorithm, Journal of the Royal Statistical Soci-ety, Ser.
B. Vol.
39, pp.
1-38.P.
Fung and L.-Y.
Yee.
1998.
An IR Approach forTranslating New Words from Nonparallel, Compara-ble Texts, In Proc.
of 17th COLING and 36th ACL, pp.414-420.Y.
Gotoh, M. M. Hochberg and H. F. Silverman.
1998.Efficient Training Algorithms for HMM?s Using In-cremental Estimation, IEEE Trans.
on Speech andAudio Processing, 6(6), pp.
539-547.F.
Jelinek.
1999.
Self-organized Language Modeling forSpeech Recognition, Readings in speech recognition,Morgan Kaufmann, pp.
450-506.D.
Jurafsky and J. H. Martin.
2000.
Speech and Lan-guage Processing, pp.
102-120, Prentice-Hall, NewJersey.K.
Knight and J. Graehl.
1998.
Machine Transliteration,Computational Linguistics, 24(4), pp.
599-612.J.-S. Kuo, H. Li and Y.-K. Yang.
2006.
Learning Trans-literation Lexicons from the Web, In Proc.
of 44thACL, pp.
1129-1136.J.-S. Kuo, H. Li and Y.-K. Yang.
2007.
A PhoneticSimilarity Model for Automatic Extraction of Trans-literation Pairs, ACM TALIP, 6(2), pp.
1-24.H.
Li, M. Zhang and J. Su.
2004.
A Joint Source Chan-nel Model for Machine Transliteration, In Proc.
of42nd ACL, pp.
159-166.W.
Lam, R.-Z.
Huang and P.-S. Cheung.
2004.
LearningPhonetic Similarity for Matching Named EntityTranslations and Mining New Translations, In Proc.of 27th ACM SIGIR, pp.
289-296.D.
MacKay and L. Peto.
1994.
A Hierarchical DirichletLanguage Model, Natural Language Engineering,1(3), pp.1-19.C.
S. Myers and L. R. Rabiner.
1981.
A ComparativeStudy of Several Dynamic Time-warping Algorithmsfor Connected word Recognition, The Bell SystemTechnical Journal, 60(7), pp.
1389-1409.J.-Y.
Nie, P. Isabelle, M. Simard and R. Durand.
1999.Cross-language Information Retrieval based on Paral-lel Texts and Automatic Mining of Parallel Text fromthe Web, In Proc.
of 22nd ACM SIGIR, pp.
74-81.R.
Rapp.
1999.
Automatic Identification of Word Trans-lations from Unrelated English and German Corpora,In Proc.
of 37th ACL, pp.
519-526.R.
Sproat, T. Tao and C. Zhai.
2006.
Named EntityTransliteration with Comparable Corpora, In Proc.
of44th ACL, pp.
73-80.G.
Zavaliagkos, R. Schwartz, and J. Makhoul.
1995.Batch, Incremental and Instantaneous AdaptationTechniques for Speech Recognition, In Proc.
ofICASSP, pp.
676-679.23Sixth SIGHAN Workshop on Chinese Language Processing
