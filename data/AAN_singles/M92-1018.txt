SRA SOLOMON :MUC-4 TEST RESULTS AND ANALYSI SChinatsu Aone, Doug McKee, Sandy Shinn, Hatte Bleje rSystems Research and Applications (SRA )2000 15th Street NorthArlington, VA 2220 1aonec@sra.comINTRODUCTIONIn this paper, we report SRA's results on the MUC-4 task and describe how we trained our natural languag eprocessing system for MUC-4 .
We also report on what worked, what didn't work, and lessons learned .Our MUC-4 system embeds the SOLOMON knowledge-based NLP shell which is designed for both domain -independence and language-independence.
We are currently using SOLOMON for a Spanish and Japanes etext understanding project in a different domain .
Although this was our first year participating in MUC, w ehave built and are currently building other data extraction systems .RESULTSOur TST3 and TST4 results are shown in Figures 1 and 2 .
The similarity of these scores as well as thei rsimilarity to SRA-internal testing results reflects the portability of SRA's MUC-4 system .
In fact, our scor eon the TST4 texts was better than that of TST3, even though those texts covered a different time perio dthan that of the training texts or TST3 .Our matched-only precision and recall for both test sets were very high (TST3 : 68/47, TST4: 73/49) .When SOLOMON recognized a MUC event, it did a very accurate and complete job at filling the requisit etemplates .SOLOMON performance was tuned so that the all-templates recall and precision were as close as possibl eto maximize the F-Measure .
As shown in Figure 3, our F-Measure steadily increased over time.
The factthat this slope has not yet leveled off shows SOLOMON's potential for improvement .EFFORT SPENTWe spent a total of 9 staff months starting January 1, 1992 through May 31, 1992 on MUC-4 .
A task-specific breakdown of effort is shown in Figure 4 .
The bulk of the work was spent porting SOLOMON t oa new domain with new vocabulary, concepts, template-output format, and fill rules .
Approximately 72%of the effort was domain-dependent .
However, about 63% of the total effort was language-independent, i .e .it would be directly applicable to understanding texts about terrorism in any language.
We expect thatour English MUC-4 system could be ported to a new language in about 3 months, given a basic grammar ,lexicon and preprocessing data similar to the ones which existed for English .
We partially demonstrated thi s137REC PRE OVG FALMATCHED/MISSING 27 68 8MATCHED/SPURIOUS 47 32 5 7MATCHED ONLY 47 68 8ALL TEMPLATES 27 32 5 7TEXT FILTERING 71 85 15 2 3F-MEASURESP&R29 .292P&R30 .86P&2R27 .87Figure 1 : TST3 ResultsREC PRE OVG FALMATCHED/MISSING 38 73 4MATCHED/SPURIOUS 49 31 5 9MATCHED ONLY 49 73 4ALL TEMPLATES 38 31 5 9TEXT FILTERING 91 75 25 3 5F-MEASURESP&R34 .142P&R32 .19P&2R36 .3 6Figure 2 : TST4 Resultsclaim by showing our MUC-4 system processing English, Japanese and Spanish newspaper articles aboutthe murder of Jesuit priests at the demonstration session of MUC-4 .
We spent less than 2 weeks after thefinal test adding MUC-specific words to Spanish and Japanese lexicons, and extending the grammars of thetwo languages .Data40% of the total effort building MUC-data was spent on lexicon and KB entry acquisition .
Much of this datawas acquired automatically.
We used the supplied geographical data to automatically build location lexiconsand KBs .
Using the development templates, we acquired lexical and KB entries for classes of domain term ssuch as human and physical targets and terrorist organizations .
We automatically derived subcategorizationinformation for the domain verbs from the development texts (cf.
[1]) .
These automatically acquired lexiconsand KBs did require some manual cleanup and correction .Certain multi-word phenomena which occur frequently in texts but are unsuitable for general parsing wer ehandled by pattern matching during Preprocessing .
For example, we created patterns for Spanish phrases ,complex location phrases, relative times, and names of political, military and terrorist organizations .Modifications to SOLOMON's broad-coverage English grammar included adding more semantic restric-tions, extending some phrase-structure rules, and improving general robustness .Based on our knowledge engineering effort, we built a set of commonsense reasoning rules that aredescribed in detail in our system description.
Our EXTRACT module recognizes MUC-relevant events i nthe output of SOLOMON and translates them into MUC-4 filled templates .
We implemented all the domain-specific information as mapping rules or simple conversion functions (e .g .
numeric values like "at least 5 "means "5-" ) .
This data is stored in the knowledge base, and is completely language independent .13 8soT4T20'30 ?
13{M?TT2 0M?s20 ?
T2 ,iT2 s10sIss0 ?
isIIII I I I III .
11200130010100200300 400 500 600 700500000 1000 1100 1400mooJAN 1 MAR 25 MAY 1Hours of EffortMAY17 MAY 31Imo 113125 517 5124 5125 5127 5/3 1Noun 0300 1240 1380 1400 1440 1500TST2 011 .43 19.48 2625 27.43 2525TST3 2020T8T4 34 .14Figure 3 : Tracking SOLOMON Performanc eTask Category ~ % of Total EffortDATA 7 1Knowledge Engineering 1 3Data Acquisition 3 0Grammar 7Pragmatic Inference Rules 1 1Extract Data 1 0PROCESSING - 29Message Zoning 3Extract Extensions 7Testing 1 0Misc .
Bug Fixing 10Figure 4 : Breakdown of Effort Spent for MUC- 413 9ProcessingWe spent 1 week porting our existing Message Zoner to deal with message headers in MUC messages .
TheMessage Zoner could already recognize more general message structures such as paragraphs and sentences .We extended EXTRACT while maintaining domain and language independence of the module .
Featuresadded included event merging and handling of flat MUC templates instead of the more object-oriente ddatabase records that SOLOMON is accustomed to .
Our time spent on fixing bugs was distributed through-out the system, but problems in Debris Parsing and Debris Semantics received the most attention .SYSTEM TRAININ GWe used TST2 texts for blind testing and the entire 1300 development texts for both testing and trainin gmaterial .
The development set was crucial to both our automated data acquisition and our knowledgeengineering task .
We performed frequent testing to track and direct our progress .
To raise recall, w efocussed on data acquisition ; to raise precision, we focussed on stricter definitions of "legal" MUC events .To improve overall performance, we focussed on more robust syntactic and semantic analysis and mor ereliable event merging .LIMITING FACTOR SThe two main limiting factors were the number of development texts and templates and the amount of tim eallotted for the MUC-4 effort .
With more texts, we could have applied other more data-intensive automate dacquisition techniques and had more examples of phenomena to draw upon .
With more time, we would addmore domain-dependent lexical knowledge and additional pragmatic inference rules .
We also need to tuneour EXTRACT mapping rules more finely and improve our discourse module for both NP reference an devent reference resolution .
Integration of existing on-line resources such as machine-readable dictionaries ,the World Factbook, or WordNet would also improve system performance.
A more extensive testing andevaluation strategy at both the blackbox and glassbox levels would help direct progress, but was not feasibl ein the amount of time we had .WHAT WAS OR WAS NOT SUCCESSFU LThere were several areas where hybrid solutions worked very well .
Totally automated knowledge acquisitionwas quite successful when supplemented by manual checking and editing of domain-crucial information .
Sim-ilarly, augmenting a pure bottom-up parser with "simulated top-down parsing" (See SRA's MUC-4 SystemDescription) worked well .
Improved Debris Semantics and significantly extended Pragmatic Inferencing wer ealso important contributors to the system's performance .REUSABILITYSRA's SOLOMON NLP system has been designed for portability and proven to be highly reusable .
Thisincludes portability to other domains, other languages, and other applications .
As shown in Figure 5, a larg e140ssnwM kWprpr?wonPSOLOMONPopmaimiuMiW YlwrMee fume .U luwm Wad 11malyWmd4mn MMydpMN+eV [wwqAIMPTVRr' MrplrndWmsnal, gsnoopfdsediiNnMMMIarr1ripO+Wrw+dtl1mIiMIMIr11?PMT?PT,rwIMIRSupwDwain-.
: NMMU?
.~JIiWHapadk?HM~Menomrtl~OBlbw 41..dolSmomldSINS,MUCOssis?Mo leMU CEmmaFigure 5 : MUC NLP System Reusabilitypart of SOLOMON 's data and almost all of the processing modules are completely reusable for NLP in othe rdomains or languages .Currently, our Spanish and Japanese data extraction project MURASAKI is using, without modification ,the same processing modules and the core knowledge base as those used for MUC-4 .
The MURASAKIsystem processes Spanish and Japanese language newspaper and journal articles as well as TV transcripts .This project's domain is the AIDS disease.
Thus, the only difference between our MUC-4 system an dMURASAKI system is that the latter uses Spanish and Japanese lexicons, patterns and grammars, an dMURASAKI domain-dependent knowledge bases .
SOLOMON has also been embedded in several Englis hmessage understanding systems : ALEXIS (operational) and WARBUCKS .LESSONS LEARNED AND REAFFIRMED BY MUC- 4We have learned and reaffirmed the following points as the most crucial aspects of successful text under -standing for data extraction .Overcoming the Knowledge Acquisition Bottleneck : We must develop techniques and tools for ac-quiring timely, complete, and proven system data .Solving the Parsing Problem : We need more robust, semantically constrained syntactic analysis .
Gram-mars must be broad-coverage and highly accurate on complex input .Developing Sophisticated Discourse Analysis : We must handle real world discourse phenomena foun din actual texts .
The discourse architecture must be flexible enough to accommodate particular discours ephenomena which are crucial in particular domains or languages .MUC-4 has reaffirmed our knowledge of what is involved in porting an NLP system to a new domain .9 staff months is a bare minimum for such an effort .
Improved knowledge acquisition tools as well a s141on-line resources are desirable .
To ensure good results, it is necessary to have sufficient time for knowledg eengineering, testing and evaluation .
Our experience underscores the fact that natural language understandin gis a highly data-driven problem .
The system's performance is often proportional to the level of understandin gof the input and output .
The MUC-4 development texts and templates were extremely helpful in this regard .References[1] Doug McKee and John Maloney.
Using Statistics Gained from Corpora in a Knowledge-Based NLPSystem.
In Proceedings of The AAAI Workshop on Statistically-Based NLP Techniques, 1992 .142
