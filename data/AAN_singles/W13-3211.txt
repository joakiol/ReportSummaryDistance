Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 91?99,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsA Generative Model of Vector Space SemanticsJacob AndreasComputer LaboratoryUniversity of Cambridgejda33@cam.ac.ukZoubin GhahramaniDepartment of EngineeringUniversity of Cambridgezoubin@eng.cam.ac.ukAbstractWe present a novel compositional, gener-ative model for vector space representa-tions of meaning.
This model reformulatesearlier tensor-based approaches to vectorspace semantics as a top-down process,and provides efficient algorithms for trans-formation from natural language to vectorsand from vectors to natural language.
Wedescribe procedures for estimating the pa-rameters of the model from positive exam-ples of similar phrases, and from distribu-tional representations, then use these pro-cedures to obtain similarity judgments fora set of adjective-noun pairs.
The model?sestimation of the similarity of these pairscorrelates well with human annotations,demonstrating a substantial improvementover several existing compositional ap-proaches in both settings.1 IntroductionVector-based word representations have gainedenormous popularity in recent years as a basic toolfor natural language processing.
Various modelsof linguistic phenomena benefit from the ability torepresent words as vectors, and vector space wordrepresentations allow many problems in NLP to bereformulated as standard machine learning tasks(Blei et al 2003; Deerwester et al 1990).Most research to date has focused on only onemeans of obtaining vectorial representations ofwords: namely, by representing them distribution-ally.
The meaning of a word is assumed to befully specified by ?the company it keeps?
(Firth,1957), and word co-occurrence (or occasionallyterm-document) matrices are taken to encode thiscontext adequately.
Distributional representationshave been shown to work well for a variety of dif-ferent tasks (Schu?tze and Pedersen, 1993; Bakerand McCallum, 1998).The problem becomes more complicated whenwe attempt represent larger linguistic structures?multiword constituents or entire sentences?within the same vector space model.
The most ba-sic issue is one of sparsity: the larger a phrase, theless frequently we expect it to occur in a corpus,and the less data we will have from which to es-timate a distributional representation.
To resolvethis problem, recent work has focused on compo-sitional vector space models of semantics.
Basedon the Fregean observation that the meaning of asentence is composed from the individual mean-ings of its parts (Frege, 1892), research in com-positional distributional semantics focuses on de-scribing procedures for combining vectors for in-dividual words in order to obtain an appropriaterepresentation of larger syntactic constituents.But various aspects of this account remain un-satisfying.
We have a continuous semantic spacein which finitely many vectors are associated withwords, but no way (other than crude approxima-tions like nearest-neighbor) to interpret the ?mean-ing?
of all the other points in the space.
More gen-erally, it?s not clear that it even makes sense to talkabout the meaning of sentences or large phrases indistributional terms, when there is no natural con-text to represent.We can begin to address these concerns by turn-ing the conventional account of composition invector space semantics on its head, and describ-ing a model for generating language from vectorsin semantic space.
Our approach is still composi-tional, in the sense that a sentence?s meaning canbe inferred from the meanings of its parts, but werelax the requirement that lexical items correspondto single vectors by allowing any vector.
In theprocess, we acquire algorithms for both meaninginference and natural language generation.Our contributions in this paper are as follows:?
A new generative, compositional model of91phrase meaning in vector space.?
A convex optimization procedure for map-ping words onto their vector representations.?
A training algorithm which requires onlypositive examples of phrases with the samemeaning.?
Another training algorithm which requiresonly distributional representations of phrases.?
A set of preliminary experimental results in-dicating that the model performs well on real-world data in both training settings.2 Model overview2.1 MotivationsThe most basic requirement for a vector spacemodel of meaning is that whatever distance metricit is equipped with accurately model human judg-ments of semantic similarity.
That is: sequencesof words which are judged to ?mean the samething?
should cluster close together in the seman-tic space, and totally unrelated sequences of wordsshould be spread far apart.Beyond this, of course, inference of vectorspace representations should be tractable: we re-quire efficient algorithms for analyzing naturallanguage strings into their corresponding vectors,and for estimating the parameters of the model thatdoes the mapping.
For some tasks, it is also usefulto have an algorithm for the opposite problem?given a vector in the semantic space, it should bepossible to produce a natural-language string en-coding the meaning of that vector; and, in keep-ing with our earlier requirements, if we choosea vector close to the vector corresponding to aknown string, the resulting interpretation shouldbe judged by a human to mean the same thing,and perhaps with some probability be exactly thesame.It is these three requirements?the use of humansimilarity judgments as the measure of the seman-tic space?s quality, and the existence of efficient al-gorithms for both generation and inference?thatmotivate the remainder of this work.We take as our starting point the general pro-gram of Coecke et al(2010) which suggests thatthe task of analyzing into a vector space shouldbe driven by syntax.
In this framework, the com-positional process consists of repeatedly combin-ing vector space word representations according tolinguistic rules, in a bottom-up process for trans-lating a natural language string to a vector inspace.But our requirement that all vectors be trans-latable into meanings?that we have both analy-sis and generation algorithms?suggests that weshould take the opposite approach, working with atop down model of vector space semantics.For simplicity, our initial presentation of thismodel, and the accompanying experiments, willbe restricted to the case of adjective-noun pairs.Section 5 will then describe how this frameworkcan be extended to full sentences.2.2 PreliminariesWe want to specify a procedure for mapping anatural language noun-adjective pair (a, n) intoa vector space which we will take to be Rp.We assume that our input sentence has alreadybeen assigned a single CCG parse (Steedman andBaldridge, 2011), which for noun-adjective pairshas the formblue orangutansN/N N >N(1)Here, the parser has assigned each token a cate-gory of the form N, N/N, etc.
Categories are ei-ther simple, drawn from a set of base types (herejust N for ?noun?
), or complex, formed by com-bining simple categories.
A category of the formX/Y ?looks right?
for a category of the form Y,and can combine with other constituents by appli-cation (we write X/Y Y ?
X) or composition(X/Y Y/Z ?
X/Z) to form higher-level con-stituents.To this model we add a vector space seman-tics.
We begin with a brief review the work ofCoecke et al(2010).
Having assigned simple cat-egories to vector spaces (in this case, N to Rp),complex categories correspond to spaces of ten-sors.
A category of the form X/Y is recursivelyassociated with SX ?
SY, where SX and SY arethe tensor spaces associated with the categories Xand Y respectively.
So the space of adjectives (oftype N/N) is just Rq?Rq, understood as the set ofq?
q matrices.
To find the meaning of a adjective-noun pair, we simply multiply the adjective matrixand noun vector as specified by the CCG deriva-tion.
The result is another vector in the same se-mantic space as the noun, as desired.92To turn this into a top-down process, we needto describe a procedure for splitting meanings andtheir associated categories.2.3 GenerationOur goal in this subsection is to describe a proba-bilistic generative process by which a vector in asemantic space is realized in natural language.Given a constituent of category X, and a corre-sponding vector x residing in some SX , we can ei-ther generate a lexical item of the appropriate typeor probabilistically draw a CCG derivation rootedin X, then independently generate the leaves.
Fornoun-adjective pairs, this can only be done in oneway, namely as in (1) (for a detailed account ofgenerative models for CCG see Hockenmaier andSteedman (2002)).
We will assume that this CCGderivation tree is observed, and concern ourselveswith filling in the appropriate vectors and lexicalitems.
This is a strong independence assumption!It effectively says ?the grammatical realization ofa concept is independent of its meaning?.
We willreturn to it in Section 6.The adjective-noun model has four groups ofparameters: (1) a collection ?N/N of weight vec-tors ?a for adjectives a, (2) a collection ?N ofweight vectors ?n for nouns n, (3) a collectionEN/N of adjective matrices Ea for adjectives a, andfinally (4) a noise parameter ?2.
For compactnessof notation we will denote this complete set of pa-rameters ?.Now we can describe how to generate anadjective-noun pair from a vector x.
The CCGderivation tells us to produce a noun and an ad-jective, and the type information further informsus that the adjective acts as a functor (here a ma-trix) and the noun as an argument.
We begin bychoosing an adjective a conditional on x. Havingmade our lexical choice, we deterministically se-lect the corresponding matrix Ea from EN/N.
Nextwe noisily generate a new vector y = Eax + ?,a vector in the same space as x, correspondingto the meaning of x without the semantic contentof a.
Finally, we select a noun n conditional ony, and output the noun-adjective pair (a, n).
Touse the previous example, suppose x means blueorangutans.
First we choose an adjective a =?blue?
(or with some probability ?azure?
or ?ultra-marine?
), and select a corresponding adjectiveEa.Then the vector y = Eax should mean orangutan,and when we generate a noun conditional on y weshould have n = ?orangutan?
(or perhaps ?mon-key?, ?primate?, etc.
).This process can be summarized with the graph-ical model in Figure 1.
In particular, we draw a?N/NaxEa y nEN/N ?2 ?NFigure 1: Graphical model of the generative pro-cess.from a log-linear distribution over all words of theappropriate category, and use the correspondingEa with Gaussian noise to map x onto y:p(a|x; ?N/N) =exp(?>a x)?????N/Nexp(?
?>x)(2)p(y|x,Ea;?2) = N (Eax, ?2)(y) (3)Last we choose n asp(n|y; ?N) =exp(?>n y)?????Nexp(?
?>z)(4)Some high-level intuition about this model: inthe bottom-up account, operators (drawn from ten-sor spaces associated with complex categories)can be thought of as acting on simple objects and?adding?
information to them.
(Suppose, for ex-ample, that the dimensions of the vector space cor-respond to actual perceptual dimensions; in thebottom-up account the matrix corresponding to theadjective ?red?
should increase the component ofan input vector that lies in dimension correspond-ing to redness.)
In our account, by contrast, ma-trices remove information, and the ?red?
matrixshould act to reduce the vector component corre-sponding to redness.2.4 AnalysisNow we must solve the opposite problem: givenan input pair (a, n), we wish to map it to an ap-propriate vector in Rp.
We assume, as before, thatwe already have a CCG parse of the input.
Then,analysis corresponds to solving the following op-timization problem:arg minx?
log p(x|a, n;?
)93By Bayes?
rule,p(x|a, n;?)
?
p(a, n|x;?
)p(x)so it suffices to minimize?
log p(x)?
log p(a, n|x;?
)To find the single best complete derivation of aninput pair (equivalent to the Viterbi parse tree insyntactic parsing), we can rewrite this asarg minx,y?
log p(x)?
log p(a, b, y|x;?)
(5)where, as before, y corresponds to the vector spacesemantics representation of the noun alone.
Wetake our prior log p(x) to be a standard normal.We have:?
log p(a, n, y|x)= ?
log p(a|x;?)?
log p(y|a, x;?)?
log p(n|y;?)?
?
?>a x+ log????
?N/Nexp ??>x+1?2||Eax?
y||2?
?>n y + log????
?Nexp ?
?>yObserve that this probability is convex: it con-sists of a sum of linear terms, Euclidean norms,and log-normalizers, all convex functions.
Conse-quently, Equation 5 can be solved exactly and ef-ficiently using standard convex optimization tools(Boyd and Vandenberghe, 2004).3 Relation to existing workThe approach perhaps most closely related to thepresent work is the bottom-up account given byCoecke et al(2010), which has already been dis-cussed in some detail in the preceding section.
Aregression-based training procedure for a similarmodel is given by Grefenstette et al(2013).
Otherwork which takes as its starting point the decisionto endow some (or all) lexical items with matrix-like operator semantics include that of Socher etal.
(2012) and Baroni and Zamparelli (2010).
In-deed, it is possible to think of the model in Ba-roni and Zamparelli?s paper as corresponding toa training procedure for a special case of thismodel, in which the positions of both nouns andnoun-adjective vectors are fixed in advance, and inwhich no lexical generation step takes place.
Theadjective matrices learned in that paper correspondto the inverses of the E matrices used above.Also relevant here is the work of Mitchell andLapata (2008) and Zanzotto et al(2010), whichprovide several alternative procedures for compos-ing distributional representations of words, andWu et al(2011), which describes a compositionalvector space semantics with an integrated syntac-tic model.
Our work differs from these approachesin requiring only positive examples for training,and in providing a mechanism for generation aswell as parsing.
Other generative work on vec-tor space semantics includes that of Hermann etal.
(2012), which models the distribution of noun-noun compounds.
This work differs from themodel that paper in attempting to generate com-plete natural language strings, rather than simplyrecover distributional representations.In training settings where we allow all posi-tional vectors to be free parameters, it?s possibleto view this work as a kind of linear relational em-bedding (Paccanaro and Hinton, 2002).
It differsfrom that work, obviously, in that we are interestedin modeling natural language syntax and seman-tics rather than arbitrary hierarchical models, andprovide a mechanism for realization of the embed-ded structures as natural language sentences.4 ExperimentsSince our goal is to ensure that the distance be-tween natural language expressions in the vectorspace correlates with human judgments of theirrelatedness, it makes sense to validate this modelby measuring precisely that correlation.
In the re-mainder of this section, we provide evidence of theusefulness of our approach by focusing on mea-surements of the similarity of adjective-noun pairs(ANs).
We describe two different parameter esti-mation procedures for different kinds of trainingdata.4.1 Learning from matching pairsWe begin by training the model on matchingpairs.
In this setting, we start with a collec-tion N sets of up to M adjective-noun pairs(ai1, ni1), (ai2, ni2), .
.
.
which mean the samething.
We fix the vector space representation yi ofeach noun ni distributionally, as described below,and find optimal settings for the lexical choice pa-rameters ?N/N and ?N, matrices (here all q ?
q)94EN/N, and, for each group of adjective-noun pairsin the training set, a latent representation xi.
Thefact that the vectors yi are tied to their distribu-tional vectors does not mean we have committedto the distributional representation of the corre-sponding nouns!
The final model represents lexi-cal choice only with the weight vectors ?
?fixingthe vectors just reduces the dimensionality of theparameter estimation problem and helps steer thetraining algorithm toward a good solution.
Thenoise parameter then acts as a kind of slack vari-able, modeling the fact that there may be no pa-rameter setting which reproduces these fixed dis-tributional representations through exact linear op-erations alone.We find a maximum-likelihood estimate forthese parameters by minimizingL(?, x) = ?N?i=1M?i=1log p(aij , nij |xi;?)
(6)The latent vectors xi are initialized to one of theircorresponding nouns, adjective matricesE are ini-tialized to the identity.
The components of ?N areinitialized identically to the nouns they select, andthe components of ?N/N initialized randomly.
Weadditionally place an L2 regularization penalty onthe scoring vectors in both ?
(to prevent weightsfrom going to infinity) and E (to encourage adjec-tives to behave roughly like the identity).
Thesepenalties, as well as the noise parameter, are ini-tially set to 0.1.Note that the training objective, unlike theanalysis objective, is non-convex.
We use L-BFGS (Liu and Nocedal, 1989) on the likeli-hood function described above with ten such ran-dom restarts, and choose the parameter settingwhich assigns the best score to a held-out cross-validation set.
Computation of the objective andits gradient at each step is linear in the number oftraining examples and quadratic in the dimension-ality of the vector space.Final evaluation is performed by taking a set ofpairs of ANs which have been assigned a similar-ity score from 1?6 by human annotators.
For eachpair, we map it into the vector space as describedin Section 2.4 above.
and finally compute the co-sine similarity of the two pair vectors.
Perfor-mance is measured in the correlation (Spearman?s?)
between these cosine similarity scores and thehuman similarity judgments.4.1.1 Setup detailsNoun vectors yi are estimated distributionallyfrom a corpus of approximately 10 million tokensof English-language Wikipedia data (WikimediaFoundation, 2013).
A training set of adjective-noun pairs are collected automatically from a col-lection of reference translations originally pre-pared for a machine translation task.
For eachforeign sentence we have four reference transla-tions produced by different translators.
We as-sign POS tags to each reference (Loper and Bird,2002) then add to the training data any adjec-tive that appears exactly once in multiple refer-ence translations, with all the nouns that follow it(e.g.
?great success?, ?great victory?, ?great ac-complishment?).
We then do the same for repeatednouns and the adjectives that precede them (e.g.
?great success?, ?huge success?, ?tremendous suc-cess?).
This approach is crude, and the data col-lected are noisy, featuring such ?synonym pairs?as (?incomplete myths?, ?incomplete autumns?
)and (?similar training?, ?province-level training?
),as well as occasional pairs which are not adjective-noun pairs at all (e.g.
?first parliamentary?).
Nev-ertheless, as results below suggest, they appear tobe good enough for purposes of learning an appro-priate representation.For the experiments described in this section,we use 500 sets of such adjective-noun pairs,corresponding to 1104 total training examples.Testing data consists of the subset of entries inthe dataset from (Mitchell and Lapata, 2010) forwhich both the adjective and noun appear at leastonce (not necessarily together) in the training set,a total of 396 pairs.
None of the pairs in this testset appears in training.
We additionally withholdfrom this set the ten pairs assigned a score of 6(indicating exact similarity), setting these aside forcross-validation.In addition to the model discussed in the firstsection of this paper (referred to here as ?GEN?
),we consider a model in which there is only oneadjective matrix E used regardless of the lexicalitem (referred to as ?GEN-1?
).The NP space is taken to be R20, and we re-duce distributional vectors to 20 dimensions usinga singular value decomposition.954.2 Learning from distributionalrepresentationsWhile the model does not require distributionalrepresentations of latent vectors, it?s useful to con-sider whether it can also provide a generative ana-log to recent models aimed explicitly at produc-ing vectorial representations of phrases given onlydistributional representations of their constituentwords.
To do this, we take as our training data aset of N single ANs, paired with a distributionalrepresentation of each AN.
In the new model, themeaning vectors x are no longer free parameters,but fully determined by these distributional repre-sentations.
We must still obtain estimates for each?
and EN/N, which we do by minimizingL(?)
= ?N?i=1log p(ai,j , ni,j |xi;?)
(7)4.2.1 Experimental setupExperimental setup is similar to the previous sec-tion; however, instead of same-meaning pairs col-lected from a reference corpus, our training datais a set of distributional vectors.
We use the samenoun vectors, and obtain these new latent pair vec-tors by estimating them in the same fashion fromthe same corpus.In order to facilitate comparison with the otherexperiment, we collect all pairs (ai, ni) such thatboth ai and ni appear in the training set used inSection 4.1 (although, once again, not necessar-ily together).
Initialization of ?
and E , regular-ization and noise parameters, as well as the cross-validation procedure, all proceed as in the previ-ous section.
We also use the same restricted eval-uation set, again to allow the results of the twoexperiments to be compared.
We evaluate by mea-suring the correlation of cosine similarities in thelearned model with human similarity judgments,and as before consider a variant of the model inwhich a single adjective matrix is shared.4.3 ResultsExperimental results are displayed in Table 1.
Forcomparison, we also provide results for a base-line which uses a distributional representation ofthe noun only, the Adjective-Specific Linear Map(ALM) model of Baroni and Zamparelli (2010) andtwo vector-based compositional models discussedin (Mitchell and Lapata, 2008): , which takesthe Hadamard (elementwise) product of the distri-butional representations of the adjective and noun,and +, which adds the distributions.
As before,we use SVD to project these distributional repre-sentations onto a 20-dimensional subspace.We observe that in both matrix-based learn-ing settings, the GEN model or its parameter-tied variant achieves the highest score (thoughthe distributionally-trained GEN-1 doesn?t per-form as well as the summing approach).
The pair-trained model performs best overall.
All corre-lations except  and the distributionally-trainedGEN are statistically significant (p < 0.05), asare the differences in correlation between thematching-pairs-trained GEN and all other mod-els, and between the distributionally-trained GEN-1 and ALM.
Readers familiar with other papersemploying the similarity-judgment evaluation willnote that scores here are uniformly lower than re-ported elsewhere; we attribute this to the compar-atively small training set (with hundreds, insteadof thousands or tens of thousands of examples).This is particularly notable in the case of the ALMmodel, which Baroni and Zamparelli report out-performs the noun baseline when given a trainingset of sufficient size.Training data Model ?Word distributions Noun .185+ .239.000Matching pairs GEN-1 .130GEN .365Word and phrase ALM .136distributions GEN-1 .201GEN .097Table 1: Results for the similarity judgment exper-iment.We also give a brief demonstration of the gen-eration capability of this model as shown in Fig-ure 2.
We demonstrate generation from three dif-ferent vectors: one inferred as the latent represen-tation of ?basic principles?
during training, oneobtained by computing a vectorial representationof ?economic development?
as described in Sec-tion 2.4 and one selected randomly from withinvector space.
We observe that the model cor-rectly identifies the adjectives ?fundamental?
and?main?
as synonymous with ?basic?
(at least whenapplied to ?principles?).
It is also able to cor-rectly map the vector associated with ?economic96Input RealizationTraining vector tyrannical principles(?basic principles?)
fundamental principlesmain principlesTest vector economic development(?economic development?)
economic developmenteconomic developmentRandom vector vital turningfurther obligationsbad negotiationsFigure 2: Generation examples using the GENmodel trained with matching pairs.development?
back onto the correct lexical real-ization.
Words generated from the random vectorappear completely unrelated; this suggests that weare sampling a portion of the space which does notcorrespond to any well-defined concept.4.4 DiscussionThese experimental results demonstrate, first andforemost, the usefulness of a model that is not tiedto distributional representations of meaning vec-tors: as the comparatively poor performance ofthe distribution-trained models shows, with onlya small number of training examples it is better tolet the model invent its own latent representationsof the adjective-noun pairs.It is somewhat surprising, in the experi-ments with distributional training data, that thesingle-adjective model outperforms the multiple-adjective model by so much.
We hypothesize thatthis is due to a search error?the significantly ex-panded parameter space of the multiple-adjectivemodel makes it considerably harder to estimateparameters; in the case of the distribution-onlymodel it is evidently so hard the model is unableto identify an adequate solution even over multipletraining runs.5 Extending the modelHaving described and demonstrated the usefulnessof this model for capturing noun-adjective similar-ity, we now describe how to extend it to capturearbitrary syntax.
While appropriate experimentalevaluation is reserved for future work, we outlinethe formal properties of the model here.
We?ll takeas our example the following CCG derivation:sister Cecilia has blue orangutansN/N N (S\N)/N N/N N> >N N>S\N<SObserve that ?blue orangutans?
is generated ac-cording to the noun-adjective model already de-scribed.5.1 GenerationTo handle general syntax, we must first extend theset EN/N of adjective matrices to sets EX for allfunctor categories X, and create an additional setof weight vectors ?X for every category X.When describing how to generate one split inthe CCG derivation (e.g.
a constituent of type Sinto constituents of type NP and S\NP), we canidentify three cases.
The first, ?fully-lexicalized?case is the one already described, and is the gen-erative process by which the a vector meaningblue orangutans is transformed into ?blue?
and?orangutans?, or sister Cecilia into ?sister?
and?Cecilia?.
But how do we get from the top-levelsentence meaning to a pair of vectors meaningsister Cecilia and has blue orangutans (an ?un-lexicalized?
split), and from has blue orangutansto the word ?has?
and a vector meaning blueorangutans (a ?half-lexicalized?
split)?Unlexicalized split We have a vector xwith cat-egory X, from which we wish to obtain a vector ywith category Y, and z with category Z.
For this wefurther augment the sets E with matrices indexedby category rather than lexical item.
Then we pro-duce y = EYx + ?, z = EZ + ?
where, as in theprevious case, ?
is Gaussian noise with variance?2.
We then recursively generate subtrees from yand z.Half-lexicalized split This proceeds much as inthe fully lexicalized case.
We have a vector x fromwhich we wish to obtain a vector y with categoryY, and a lexical item w with category Z.We choose w according to Equation 2, selecta matrix Ew and produce y = Ewx + ?
as be-fore, and then recursively generate a subtree fromy without immediately generating another lexicalitem for y.5.2 AnalysisAs before, it suffices to minimize?
log p(x) ?
log p(W,P |x) for a sentence97W = (w1, w2, ?
?
?
, wn) and a set of internalvectors P .
We select our prior p(x) exactly asbefore, and can define p(W,P |x) recursively.
Thefully-lexicalized case is exactly as above.
For theremaining cases, we have:Unlexicalized split Given a subsequenceWi:j = (wi, ?
?
?
, wj), if the CCG parse splits Wi:jinto constituents Wi:k and Wk:j , with categoriesY and Z, we have:?
log p(Wi:j |x) =?
log p(Wi:k, P |EY x)?
log p(Wk:j , P |EZx)Half-lexicalized split If the parse splits Wi:jinto wi and Wi+1:j with categories Y and Z, andy ?
P is the intermediate vector used at this stepof the derivation, we have:?
log p(Wi:j , y|x)= ?
log p(wi|x)?
log p(y|x,wi)?
log p(Wi+1:j |y)?
?
?Twix+ log?w?
?LYexp ?Tw?x+1?2||Ewix?
y||2?
log p(Wi+1:j , P |y)Finally, observe that the complete expressionof the log probability of any derivation is, as be-fore, a sum of linear and convex terms, so theoptimization problem remains convex for generalparse trees.6 Future workVarious extensions to the model proposed in thispaper are possible.
The fact that relaxing thedistributional requirement for phrases led to per-formance gains suggests that something similarmight be gained from nouns.
If a reliable train-ing procedure could be devised with noun vectorsas free parameters, it might learn an even bettermodel of phrase similarity?and, in the process,simultaneously perform unsupervised word sensedisambiguation on the training corpus.Unlike the work of Coecke et al(2010), thestructure of the types appearing in the CCG deriva-tions used here are neither necessary nor sufficientto specify the form of the matrices used in thispaper.
Instead, the function of the CCG deriva-tion is simply to determine which words shouldbe assigned matrices, and which nouns.
WhileCCG provides a very natural way to do this, itis by no means the only way, and future workmight focus on providing an analog using a differ-ent grammar?all we need is a binary-branchinggrammar with a natural functor-argument distinc-tion.Finally, as mentioned in Section 2.3, we havemade a significant independence assumption in re-quiring that the entire CCG derivation be gener-ated in advance.
This assumption was necessaryto ensure that the probability of a vector in mean-ing space given its natural language representationwould be a convex program.
We suspect, however,that it is possible to express a similar probabil-ity for an entire packed forest of derivations, andoptimize it globally by means of a CKY-like dy-namic programming approach.
This would makeit possible to optimize simultaneously over all pos-sible derivations of a sentence, and allow positionsin meaning space to influence the form of thosederivations.7 ConclusionWe have introduced a new model for vectorspace representations of word and phrase mean-ing, by providing an explicit probabilistic processby which natural language expressions are gener-ated from vectors in a continuous space of mean-ings.
We?ve given efficient algorithms for bothanalysis into and generation out of this meaningspace, and described two different training proce-dures for estimating the parameters of the model.Experimental results demonstrate that these al-gorithms are capable of modeling graded humanjudgments of phrase similarity given only positiveexamples of matching pairs, or distributional rep-resentations of pairs as training data; when trainedin this fashion, the model outperforms severalother compositional approaches to vector spacesemantics.
We have concluded by suggesting howsyntactic information might be more closely inte-grated into this model.
While the results presentedhere are preliminary, we believe they present com-pelling evidence of representational power, andmotivate further study of related models for thisproblem.AcknowledgmentsWe would like to thank Stephen Clark and An-dreas Vlachos for feedback on a draft of this pa-per.98ReferencesL Douglas Baker and Andrew Kachites McCallum.1998.
Distributional clustering of words for textclassification.
In Proceedings of the 21st annual in-ternational ACM SIGIR conference on Research anddevelopment in information retrieval, pages 96?103.ACM.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages1183?1193.
Association for Computational Linguis-tics.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alcation.
the Journal of Ma-chine Learning Research, 3:993?1022.Stephen Boyd and Lieven Vandenberghe.
2004.
Con-vex optimization.
Cambridge university press.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2010.
Mathematical foundations for a com-positional distributional model of meaning.
arXivpreprint arXiv:1003.4394.Scott Deerwester, Susan T. Dumais, George W Fur-nas, Thomas K Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American society for information science,41(6):391?407.John Rupert Firth.
1957.
A synopsis of linguistic the-ory, 1930-1955.Gottlob Frege.
1892.
Uber Sinn und Bedeutung.Zeitschrift fur Philosophie und philosophische Kri-tik, pages 25?50.
English Translation: em On Senseand Meaning, in Brian McGuinness (ed), em Frege:collected works, pp.
157?177, Basil Blackwell, Ox-ford.Edward Grefenstette, Georgiana Dinu, Yao-ZhongZhang, Mehrnoosh Sadrzadeh, and Marco Baroni.2013.
Multi-step regression learning for compo-sitional distributional semantics.
Proceedings ofthe 10th International Conference on ComputationalSemantics (IWCS 2013).Karl Moritz Hermann, Phil Blunsom, and Stephen Pul-man.
2012.
An unsupervised ranking model fornoun-noun compositionality.
In Proceedings of theFirst Joint Conference on Lexical and Computa-tional Semantics-Volume 1: Proceedings of the mainconference and the shared task, and Volume 2: Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation, pages 132?141.
Association forComputational Linguistics.Julia Hockenmaier and Mark Steedman.
2002.
Gen-erative models for statistical parsing with combina-tory categorial grammar.
In Proceedings of the 40thAnnual Meeting on Association for ComputationalLinguistics, pages 335?342.
Association for Com-putational Linguistics.Dong C Liu and Jorge Nocedal.
1989.
On the limitedmemory bfgs method for large scale optimization.Mathematical programming, 45(1-3):503?528.Edward Loper and Steven Bird.
2002.
Nltk: the nat-ural language toolkit.
In Proceedings of the ACL-02 Workshop on Effective tools and methodologiesfor teaching natural language processing and com-putational linguistics - Volume 1, ETMTNLP ?02,pages 63?70, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
proceedings ofACL-08: HLT, pages 236?244.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Alberto Paccanaro and Jefferey Hinton.
2002.
Learn-ing hierarchical structures with linear relational em-bedding.
In Advances in Neural Information Pro-cessing Systems 14: Proceedings of the 2001 NeuralInformation Processing Systems (NIPS) Conference,volume 14, page 857.
MIT Press.Hinrich Schu?tze and Jan Pedersen.
1993.
A vectormodel for syntagmatic and paradigmatic relatedness.Making sense of words, pages 104?113.Richard Socher, Brody Huval, Christopher D Manning,and Andrew Y Ng.
2012.
Semantic compositional-ity through recursive matrix-vector spaces.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211.
Association for Computational Linguis-tics.Mark Steedman and Jason Baldridge.
2011.
Combi-natory categorial grammar.
Non-TransformationalSyntax Oxford: Blackwell, pages 181?224.Wikimedia Foundation.
2013.
Wikipedia.http://dumps.wikimedia.org/enwiki/.
Accessed:2013-04-20.Stephen Wu, William Schuler, et al2011.
Struc-tured composition of semantic vectors.
In Proceed-ings of the Ninth International Conference on Com-putational Semantics (IWCS 2011), pages 295?304.Citeseer.Fabio Massimo Zanzotto, Ioannis Korkontzelos,Francesca Fallucchi, and Suresh Manandhar.
2010.Estimating linear models for compositional distri-butional semantics.
In Proceedings of the 23rd In-ternational Conference on Computational Linguis-tics, pages 1263?1271.
Association for Computa-tional Linguistics.99
