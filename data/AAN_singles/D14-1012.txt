Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 110?120,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsRevisiting Embedding Features for Simple Semi-supervised LearningJiang Guo?, Wanxiang Che?, Haifeng Wang?, Ting Liu??
?Research Center for Social Computing and Information RetrievalHarbin Institute of Technology, China?Baidu Inc., Beijing, China{jguo, car, tliu}@ir.hit.edu.cnwanghaifeng@baidu.comAbstractRecent work has shown success in us-ing continuous word embeddings learnedfrom unlabeled data as features to improvesupervised NLP systems, which is re-garded as a simple semi-supervised learn-ing mechanism.
However, fundamen-tal problems on effectively incorporatingthe word embedding features within theframework of linear models remain.
Inthis study, we investigate and analyze threedifferent approaches, including a new pro-posed distributional prototype approach,for utilizing the embedding features.
Thepresented approaches can be integratedinto most of the classical linear models inNLP.
Experiments on the task of namedentity recognition show that each of theproposed approaches can better utilize theword embedding features, among whichthe distributional prototype approach per-forms the best.
Moreover, the combinationof the approaches provides additive im-provements, outperforming the dense andcontinuous embedding features by nearly2 points of F1 score.1 IntroductionLearning generalized representation of words isan effective way of handling data sparsity causedby high-dimensional lexical features in NLP sys-tems, such as named entity recognition (NER)and dependency parsing.
As a typical low-dimensional and generalized word representa-tion, Brown clustering of words has been stud-ied for a long time.
For example, Liang (2005)and Koo et al.
(2008) used the Brown clusterfeatures for semi-supervised learning of variousNLP tasks and achieved significant improvements.
?Email correspondence.Recent research has focused on a special fam-ily of word representations, named ?word embed-dings?.
Word embeddings are conventionally de-fined as dense, continuous, and low-dimensionalvector representations of words.
Word embed-dings can be learned from large-scale unlabeledtexts through context-predicting models (e.g., neu-ral network language models) or spectral methods(e.g., canonical correlation analysis) in an unsu-pervised setting.Compared with the so-called one-hot represen-tation where each word is represented as a sparsevector of the same size of the vocabulary and onlyone dimension is on, word embedding preservesrich linguistic regularities of words with each di-mension hopefully representing a latent feature.Similar words are expected to be distributed closeto one another in the embedding space.
Conse-quently, word embeddings can be beneficial fora variety of NLP applications in different ways,among which the most simple and general way isto be fed as features to enhance existing supervisedNLP systems.Previous work has demonstrated effectivenessof the continuous word embedding features in sev-eral tasks such as chunking and NER using gener-alized linear models (Turian et al., 2010).1How-ever, there still remain two fundamental problemsthat should be addressed:?
Are the continuous embedding features fit forthe generalized linear models that are mostwidely adopted in NLP??
How can the generalized linear models betterutilize the embedding features?According to the results provided by Turian et1Generalized linear models refer to the models that de-scribe the data as a combination of linear basis functions,either directly in the input variables space or through sometransformation of the probability distributions (e.g., log-linear models).110al.
(2010), the embedding features brought signif-icantly less improvement than Brown clusteringfeatures.
This result is actually not reasonable be-cause the expressing power of word embeddingsis theoretically stronger than clustering-based rep-resentations which can be regarded as a kind ofone-hot representation but over a low-dimensionalvocabulary (Bengio et al., 2013).Wang and Manning (2013) showed that lineararchitectures perform better in high-dimensionaldiscrete feature space than non-linear ones,whereas non-linear architectures are more effec-tive in low-dimensional and continuous featurespace.
Hence, the previous method that directlyuses the continuous word embeddings as featuresin linear models (CRF) is inappropriate.
Wordembeddings may be better utilized in the linearmodeling framework by smartly transforming theembeddings to some relatively higher dimensionaland discrete representations.Driven by this motivation, we present threedifferent approaches: binarization (Section 3.2),clustering (Section 3.3) and a new proposed distri-butional prototype method (Section 3.4) for betterincorporating the embeddings features.
In the bi-narization approach, we directly binarize the con-tinuous word embeddings by dimension.
In theclustering approach, we cluster words based ontheir embeddings and use the resulting word clus-ter features instead.
In the distributional prototypeapproach, we derive task-specific features fromword embeddings by utilizing a set of automati-cally extracted prototypes for each target label.We carefully compare and analyze these ap-proaches in the task of NER.
Experimental resultsare promising.
With each of the three approaches,we achieve higher performance than directly usingthe continuous embedding features, among whichthe distributional prototype approach performs thebest.
Furthermore, by putting the most effectivetwo of these features together, we finally outper-form the continuous embedding features by nearly2 points of F1 Score (86.21% vs. 88.11%).The major contribution of this paper is twofold.
(1) We investigate various approaches that can bet-ter utilize word embeddings for semi-supervisedlearning.
(2) We propose a novel distributionalprototype approach that shows the great potentialof word embedding features.
All the presented ap-proaches can be easily integrated into most of theclassical linear NLP models.2 Semi-supervised Learning with WordEmbeddingsStatistical modeling has achieved great successin most NLP tasks.
However, there still remainsome major unsolved problems and challenges,among which the most widely concerned is thedata sparsity problem.
Data sparsity in NLP ismainly caused by two factors, namely, the lackof labeled training data and the Zipf distributionof words.
On the one hand, large-scale labeledtraining data are typically difficult to obtain, espe-cially for structure prediction tasks, such as syn-tactic parsing.
Therefore, the supervised mod-els can only see limited examples and thus makebiased estimation.
On the other hand, the nat-ural language words are Zipf distributed, whichmeans that most of the words appear a few timesor are completely absent in our texts.
For theselow-frequency words, the corresponding parame-ters usually cannot be fully trained.More foundationally, the reason for the abovefactors lies in the high-dimensional and sparse lex-ical feature representation, which completely ig-nores the similarity between features, especiallyword features.
To overcome this weakness, an ef-fective way is to learn more generalized represen-tations of words by exploiting the numerous un-labeled data, in a semi-supervised manner.
Afterwhich, the generalized word representations canbe used as extra features to facilitate the super-vised systems.Liang (2005) learned Brown clusters ofwords (Brown et al., 1992) from unlabeled dataand use them as features to promote the supervisedNER and Chinese word segmentation.
Brownclusters of words can be seen as a generalizedword representation distributed in a discrete andlow-dimensional vocabulary space.
Contextuallysimilar words are grouped in the same cluster.
TheBrown clustering of words was also adopted in de-pendency parsing (Koo et al., 2008) and POS tag-ging for online conversational text (Owoputi et al.,2013), demonstrating significant improvements.Recently, another kind of word representationnamed ?word embeddings?
has been widely stud-ied (Bengio et al., 2003; Mnih and Hinton, 2008).Using word embeddings, we can evaluate the sim-ilarity of two words straightforward by comput-ing the dot-product of two numerical vectors in theHilbert space.
Two similar words are expected to111be distributed close to each other.2Word embeddings can be useful as input to anNLP model (mostly non-linear) or as additionalfeatures to enhance existing systems.
Collobertet al.
(2011) used word embeddings as input to adeep neural network for multi-task learning.
De-spite of the effectiveness, such non-linear modelsare hard to build and optimize.
Besides, these ar-chitectures are often specialized for a certain taskand not scalable to general tasks.
A simple andmore general way is to feed word embeddings asaugmented features to an existing supervised sys-tem, which is similar to the semi-supervised learn-ing with Brown clusters.As discussed in Section 1, Turian et al.
(2010)is the pioneering work on using word embeddingfeatures for semi-supervised learning.
However,their approach cannot fully exploit the potentialof word embeddings.
We revisit this problemin this study and investigate three different ap-proaches for better utilizing word embeddings insemi-supervised learning.3 Approaches for Utilizing EmbeddingFeatures3.1 Word Embedding TrainingIn this paper, we will consider a context-predicting model, more specifically, the Skip-grammodel (Mikolov et al., 2013a; Mikolov et al.,2013b) for learning word embeddings, since it ismuch more efficient as well as memory-savingthan other approaches.Let?s denote the embedding matrix to be learnedby Cd?N, where N is the vocabulary size and d isthe dimension of word embeddings.
Each columnof C represents the embedding of a word.
TheSkip-gram model takes the current word w as in-put, and predicts the probability distribution of itscontext words within a fixed window size.
Con-cretely, w is first mapped to its embedding vwbyselecting the corresponding column vector of C(or multiplying C with the one-hot vector of w).The probability of its context word c is then com-puted using a log-linear function:P (c|w; ?)
=exp(v>cvw)?c?
?Vexp(vc?>vw)(1)where V is the vocabulary.
The parameters ?
arevwi, vcifor w, c ?
V and i = 1, ..., d. Then, the2The term similar should be viewed depending on the spe-cific task.log-likelihood over the entire training dataset Dcan be computed as:J(?)
=?
(w,c)?Dlog p(c|w; ?)
(2)The model can be trained by maximizing J(?
).Here, we suppose that the word embeddingshave already been trained from large-scale unla-beled texts.
We will introduce various approachesfor utilizing the word embeddings as features forsemi-supervised learning.
The main idea, as in-troduced in Section 1, is to transform the continu-ous word embeddings to some relatively higher di-mensional and discrete representations.
The directuse of continuous embeddings as features (Turianet al., 2010) will serve as our baseline setting.3.2 Binarization of EmbeddingsOne fairly natural approach for converting thecontinuous-valued word embeddings to discretevalues is binarization by dimension.Formally, we aim to convert the continuous-valued embedding matrixCd?N, to another matrixMd?Nwhich is discrete-valued.
There are variousconversion functions.
Here, we consider a sim-ple one.
For the ithdimension of the word em-beddings, we divide the corresponding row vectorCiinto two halves for positive (Ci+) and nega-tive (Ci?
), respectively.
The conversion functionis then defined as follows:Mij= ?
(Cij) =????
?U+, if Cij?
mean(Ci+)B?, if Cij?
mean(Ci?
)0, otherwisewhere mean(v) is the mean value of vector v, U+is a string feature which turns on when the value(Cij) falls into the upper part of the positive list.Similarly, B?refers to the bottom part of the neg-ative list.
The insight behind ?
is that we only con-sider the features with strong opinions (i.e., posi-tive or negative) on each dimension and omit thevalues close to zero.3.3 Clustering of EmbeddingsYu et al.
(2013) introduced clustering embeddingsto overcome the disadvantage that word embed-dings are not suitable for linear models.
They sug-gested that the high-dimensional cluster featuresmake samples from different classes better sepa-rated by linear models.112In this study, we again investigate this ap-proach.
Concretely, each word is treated as a sin-gle sample.
The batch k-means clustering algo-rithm (Sculley, 2010) is used,3and each clusteris represented as the mean of the embeddings ofwords assigned to it.
Similarities between wordsand clusters are measured by Euclidean distance.Moreover, different number of clusters n con-tain information of different granularities.
There-fore, we combine the cluster features of differentns to better utilize the embeddings.3.4 Distributional Prototype FeaturesWe propose a novel kind of embedding features,named distributional prototype features for su-pervised models.
This is mainly inspired byprototype-driven learning (Haghighi and Klein,2006) which was originally introduced as a pri-marily unsupervised approach for sequence mod-eling.
In prototype-driven learning, a few pro-totypical examples are specified for each targetlabel, which can be treated as an injection ofprior knowledge.
This sparse prototype informa-tion is then propagated across an unlabeled corpusthrough distributional similarities.The basic motivation of the distributional pro-totype features is that similar words are supposedto be tagged with the same label.
This hypothesismakes great sense in tasks such as NER and POStagging.
For example, suppose Michael is a pro-totype of the named entity (NE) type PER.
Usingthe distributional similarity, we could link similarwords to the same prototypes, so the word Davidcan be linked to Michael because the two wordshave high similarity (exceeds a threshold).
Usingthis link feature, the model will push David closerto PER.To derive the distributional prototype features,first, we need to construct a few canonical exam-ples (prototypes) for each target annotation label.We use the normalized pointwise mutual informa-tion (NPMI) (Bouma, 2009) between the label andword, which is a smoothing version of the standardPMI, to decide the prototypes of each label.
Giventhe annotated training corpus, the NPMI betweena label and word is computed as follows:?n(label, word) =?
(label, word)?
ln p(label, word)(3)3code.google.com/p/sofia-mlNE Type PrototypesB-PER Mark, Michael, David, PaulI-PER Akram, Ahmed, Khan, YounisB-ORG Reuters, U.N., Ajax, PSVI-ORG Newsroom, Inc, Corp, PartyB-LOC U.S., Germany, Britain, AustraliaI-LOC States, Republic, Africa, LankaB-MISC Russian, German, French, BritishI-MISC Cup, Open, League, OPENO ., ,, the, toTable 1: Prototypes extracted from the CoNLL-2003 NER training data using NPMI.where,?
(label, word) = lnp(label, word)p(label)p(word)(4)is the standard PMI.For each target label l (e.g., PER, ORG, LOC),we compute the NPMI of l and all words in thevocabulary, and the top m words are chosen as theprototypes of l. We should note that the proto-types are extracted fully automatically, without in-troducing additional human prior knowledge.Table 1 shows the top four prototypes extractedfrom the NER training corpus of CoNLL-2003shared task (Tjong Kim Sang and De Meul-der, 2003), which contains four NE types, namely,PER, ORG, LOC, and MISC.
Non-NEs are denotedby O.
We convert the original annotation to thestandard BIO-style.
Thus, the final corpus con-tains nine labels in total.Next, we introduce the prototypes as features toour supervised model.
We denote the set of pro-totypes for all target labels by Sp.
For each proto-type z ?
Sp, we add a predicate proto = z, whichbecomes active at each w if the distributional sim-ilarity between z and w (DistSim(z, w)) is abovesome threshold.
DistSim(z, w) can be efficientlycalculated through the cosine similarity of the em-beddings of z and w. Figure 1 gives an illustra-tion of the distributional prototype features.
Un-like previous embedding features or Brown clus-ters, the distributional prototype features are task-specific because the prototypes of each label areextracted from the training data.Moreover, each prototype word is also its ownprototype (since a word has maximum similarityto itself).
Thus, if the prototype is closely relatedto a label, all the words that are distributionally113i -1x ix1?iy iyO B-LOCin/INHague/NNPO B-LOC1( , )?
?
?i if y y( , )word = Haguepos = NNPproto = Britain  B-LOCproto = England...??
??
??
??
???
??
??
??
??
?i if x yFigure 1: An example of distributional prototypefeatures for NER.similar to that prototype are pushed towards thatlabel.4 Supervised Evaluation TaskVarious tasks can be considered to compare andanalyze the effectiveness of the above three ap-proaches.
In this study, we partly follow Turianet al.
(2010) and Yu et al.
(2013), and take NER asthe supervised evaluation task.NER identifies and classifies the named entitiessuch as the names of persons, locations, and orga-nizations in text.
The state-of-the-art systems typ-ically treat NER as a sequence labeling problem,where each word is tagged either as a BIO-styleNE or a non-NE category.Here, we use the linear chain CRF model, whichis most widely used for sequence modeling in thefield of NLP.
The CoNLL-2003 shared task datasetfrom the Reuters, which was used by Turian etal.
(2010) and Yu et al.
(2013), was chosen asour evaluation dataset.
The training set contains14,987 sentences, the development set contains3,466 sentences and is used for parameter tuning,and the test set contains 3,684 sentences.The baseline features are shown in Table 2.4.1 Embedding Feature TemplatesIn this section, we introduce the embedding fea-tures to the baseline NER system, turning the su-pervised approach into a semi-supervised one.Dense embedding features.
The dense con-tinuous embedding features can be fed directly tothe CRF model.
These embedding features canbe seen as heterogeneous features from the exist-ing baseline features, which are discrete.
There isno effective way for dense embedding features tobe combined internally or with other discrete fea-tures.
So we only use the unigram embedding fea-tures following Turian et al.
(2010).
Concretely,the embedding feature template is:Baseline NER Feature Templates00: wi+k,?2 ?
k ?
201: wi+k?
wi+k+1,?2 ?
k ?
102: ti+k,?2 ?
k ?
203: ti+k?
ti+k+1,?2 ?
k ?
104: chki+k,?2 ?
k ?
205: chki+k?
chki+k+1,?2 ?
k ?
106: Prefix (wi+k, l),?2 ?
k ?
2, 1 ?
l ?
407: Suffix (wi+k, l),?2 ?
k ?
2, 1 ?
l ?
408: Type(wi+k),?2 ?
k ?
2Unigram Featuresyi?
00?
08Bigram Featuresyi?1?
yiTable 2: Features used in the NER system.
t isthe POS tag.
chk is the chunking tag.
Prefixand Suffix are the first and last l characters of aword.
Type indicates if the word is all-capitalized,is-capitalized, all-digits, etc.?
dei+k[d], ?2 ?
k ?
2, d ranges over thedimensions of the dense word embedding de.Binarized embedding features.
The binarizedembedding feature template is similar to the denseone.
The only difference is that the feature val-ues are discrete and we omit dimensions with zerovalue.
Therefore, the feature template becomes:?
bii+k[d], ?2 ?
k ?
2, where bii+k[d] 6= 0,d ranges over the dimensions of the binarizedvector bi of word embedding.In this way, the dimension of the binarized em-bedding feature space becomes 2 ?
d comparedwith the originally d of the dense embeddings.Compound cluster features.
The advantage ofthe cluster features is that they can be combinedinternally or with other features to form compoundfeatures, which can be more discriminative.
Fur-thermore, the number of resulting clusters n canbe tuned, and different ns indicate different granu-larities.
Concretely, the compound cluster featuretemplate for each specific n is:?
ci+k, ?2 ?
k ?
2.?
ci+k?
ci+k+1,?2 ?
k ?
1.?
ci?1?
ci+1.Distributional prototype features.
The set ofprototypes is again denoted by Sp, which is de-114cided by selecting the topm (NPMI) words as pro-totypes of each label, where m is tuned on the de-velopment set.
For each word wiin a sequence,we compute the distributional similarity betweenwiand each prototype in Spand select the proto-types zs that DistSim(z, w) ?
?.
We set ?
= 0.5without manual tuning.
The distributional proto-type feature template is then:?
{protoi+k=z | DistSim(wi+k, z) ?
?
& z ?Sp}, ?2 ?
k ?
2 .We only use the unigram features, since thenumber of active distributional prototype featuresvaries for different words (positions).
Hence,these features cannot be combined effectively.4.2 Brown ClusteringBrown clustering has achieved great success invarious NLP applications.
At most time, itprovides a strong baseline that is difficult tobeat (Turian et al., 2010).
Consequently, in ourstudy, we conduct comparisons among the embed-ding features and the Brown clustering features,along with further investigations of their combina-tion.The Brown algorithm is a hierarchical cluster-ing algorithm which optimizes a class-based bi-gram language model defined on the word clus-ters (Brown et al., 1992).
The output of the Brownalgorithm is a binary tree, where each word isuniquely identified by its path from the root.
Thuseach word can be represented as a bit-string witha specific length.Following the setting of Owoputi et al.
(2013),we will use the prefix features of hierarchical clus-ters to take advantage of the word similarity in dif-ferent granularities.
Concretely, the Brown clusterfeature template is:?
bci+k, ?2 ?
k ?
2.?
prefix (bci+k, p), p ?
{2,4,6,...,16}, ?2 ?k ?
2. prefix takes the p-length prefix ofthe Brown cluster coding bci+k.5 Experiments5.1 Experimental SettingWe take the English Wikipedia until August 2012as our unlabeled data to train the word embed-dings.4Little pre-processing is conducted for the4download.wikimedia.org.training of word embeddings.
We remove para-graphs that contain non-roman characters and allMediaWiki markups.
The resulting text is tok-enized using the Stanford tokenizer,5and everyword is converted to lowercase.
The final datasetcontains about 30 million sentences and 1.52 bil-lion words.
We use a dictionary that contains212,779 most common words (frequency ?
80) inthe dataset.
An efficient open-source implementa-tion of the Skip-gram model is adopted.6We ap-ply the negative sampling7method for optimiza-tion, and the asynchronous stochastic gradient de-scent algorithm (Asynchronous SGD) for parallelweight updating.
In this study, we set the dimen-sion of the word embeddings to 50.
Higher di-mension is supposed to bring more improvementsin semi-supervised learning, but its comparison isbeyond the scope of this paper.For the cluster features, we tune the numberof clusters n from 500 to 3000 on the develop-ment set, and finally use the combination of n =500, 1000, 1500, 2000, 3000, which achieves thebest results.
For the distributional prototype fea-tures, we use a fixed number of prototype words(m) for each target label.
m is tuned on the devel-opment set and is finally set to 40.We induce 1,000 brown clusters of words, thesetting in prior work (Koo et al., 2008; Turian etal., 2010).
The training data of brown clustering isthe same with that of training word embeddings.5.2 ResultsTable 3 shows the performances of NER on thetest dataset.
Our baseline is slightly lower thanthat of Turian et al.
(2010), because they usethe BILOU encoding of NE types which outper-forms BIO encoding (Ratinov and Roth, 2009).8Nonetheless, our conclusions hold.
As we can see,all of the three approaches we investigate in thisstudy achieve better performance than the directuse of the dense continuous embedding features.To our surprise, even the binarized embeddingfeatures (BinarizedEmb) outperform the continu-ous version (DenseEmb).
This provides clear evi-dence that directly using the dense continuous em-beddings as features in CRF indeed cannot fully5nlp.stanford.edu/software/tokenizer.shtml.6code.google.com/p/word2vec/.7More details are analyzed in (Goldberg and Levy, 2014).8We use BIO encoding here in order to compare with mostof the reported benchmarks.115Setting F1Baseline 83.43+DenseEmb?
86.21+BinarizedEmb 86.75+ClusterEmb 86.90+DistPrototype 87.44+BinarizedEmb+ClusterEmb 87.56+BinarizedEmb+DistPrototype 87.46+ClusterEmb+DistPrototype 88.11+Brown 87.49+Brown+ClusterEmb 88.17+Brown+DistPrototype 88.04+Brown+ClusterEmb+DistPrototype 88.58Finkel et al.
(2005) 86.86Krishnan and Manning (2006) 87.24Ando and Zhang (2005) 89.31Collobert et al.
(2011) 88.67Table 3: The performance of semi-supervisedNER on the CoNLL-2003 test data, using vari-ous embedding features.
?
DenseEmb refers to themethod used by Turian et al.
(2010), i.e., the directuse of the dense and continuous embeddings.exploit the potential of word embeddings.
Thecompound cluster features (ClusterEmb) also out-perform the DenseEmb.
The same result is alsoshown in (Yu et al., 2013).
Further, the distribu-tional prototype features (DistPrototype) achievethe best performance among the three approaches(1.23% higher than DenseEmb).We should note that the feature templates usedfor BinarizedEmb and DistPrototype are merelyunigram features.
However, for ClusterEmb, weform more complex features by combining theclusters of the context words.
We also considerdifferent number of clusters n, to take advantageof the different granularities.
Consequently, thedimension of the cluster features is much higherthan that of BinarizedEmb and DistPrototype.We further combine the proposed features to seeif they are complementary to each other.
As shownin Table 3, the cluster and distributional prototypefeatures are the most complementary, whereas thebinarized embedding features seem to have largeoverlap with the distributional prototype features.By combining the cluster and distributional pro-totype features, we further push the performanceto 88.11%, which is nearly two points higher thanthe performance of the dense embedding features(86.21%).9We also compare the proposed features withthe Brown cluster features.
As shown in Table 3,the distributional prototype features alone achievecomparable performance with the Brown clusters.When the cluster and distributional prototype fea-tures are used together, we outperform the Brownclusters.
This result is inspiring because we showthat the embedding features indeed have strongerexpressing power than the Brown clusters, as de-sired.
Finally, by combining the Brown clusterfeatures and the proposed embedding features, theperformance can be improved further (88.58%).The binarized embedding features are not includedin the final compound features because they are al-most overlapped with the distributional prototypefeatures in performance.We also summarize some of the reportedbenchmarks that utilize unlabeled data (with nogazetteers used), including the Stanford NER tag-ger (Finkel et al.
(2005) and Krishnan and Man-ning (2006)) with distributional similarity fea-tures.
Ando and Zhang (2005) use unlabeled datafor constructing auxiliary problems that are ex-pected to capture a good feature representation ofthe target problem.
Collobert et al.
(2011) adjustthe feature embeddings according to the specifictask in a deep neural network architecture.
Wecan see that both Ando and Zhang (2005) and Col-lobert et al.
(2011) learn task-specific lexical fea-tures, which is similar to the proposed distribu-tional prototype method in our study.
We suggestthis to be the main reason for the superiority ofthese methods.Another advantage of the proposed discrete fea-tures over the dense continuous features is tag-ging efficiency.
Table 4 shows the running timeusing different kinds of embedding features.
Weachieve a significant reduction of the tagging timeper sentence when using the discrete features.
Thisis mainly due to the dense/sparse battle.
Al-though the dense embedding features are low-dimensional, the feature vector for each word ismuch denser than in the sparse and discrete featurespace.
Therefore, we actually need much morecomputation during decoding.
Similar results canbe observed in the comparison of the DistProto-type and ClusterEmb features, since the density ofthe DistPrototype features is higher.
It is possible9Statistical significant with p-value < 0.001 by two-tailedt-test.116Setting Time (ms) / sentBaseline 1.04+DenseEmb 4.75+BinarizedEmb 1.25+ClusterEmb 1.16+DistPrototype 2.31Table 4: Running time of different features on aIntel(R) Xeon(R) E5620 2.40GHz machine.to accelerate the DistPrototype, by increasing thethreshold of DistSim(z, w).
However, this is in-deed an issue of trade-off between efficiency andaccuracy.5.3 AnalysisIn this section, we conduct analyses to show thereasons for the improvements.5.3.1 Rare wordsAs discussed by Turian et al.
(2010), much of theNER F1 is derived from decisions regarding rarewords.
Therefore, in order to show that the threeproposed embedding features have stronger abil-ity for handling rare words, we first conduct anal-ysis for the tagging errors of words with differ-ent frequency in the unlabeled data.
We assign theword frequencies to several buckets, and evaluatethe per-token errors that occurred in each bucket.Results are shown in Figure 2.
In most cases, allthree embedding features result in fewer errors onrare words than the direct use of dense continuousembedding features.Interestingly, we find that for words that areextremely rare (0?256), the binarized embeddingfeatures incur significantly fewer errors than otherapproaches.
As we know, the embeddings for therare words are close to their initial value, becausethey received few updates during training.
Hence,these words are not fully trained.
In this case,we would like to omit these features because theirembeddings are not even trustable.
However, allembedding features that we proposed except Bi-narizedEmb are unable to handle this.In order to see how much we have utilizedthe embedding features in BinarizedEmb, we cal-culate the sparsity of the binarized embeddingvectors, i.e., the ratio of zero values in eachvector (Section 3.2).
As demonstrated in Fig-ure 3, the sparsity-frequency curve has good prop-erties: higher sparsity for very rare words andvery frequent words, while lower sparsity for mid-frequent words.
It indicates that for words that arevery rare or very frequent, BinarizedEmb just omitmost of the features.
This is reasonable also forthe very frequent words, since they usually haverich and diverse context distributions and theirembeddings cannot be well learned by our mod-els (Huang et al., 2012).llll lllllllFrequency of word in unlabeled dataSparsity256 1k 4k 16k 64k0.500.550.600.650.70Figure 3: Sparsity (with confidence interval) of thebinarized embedding vector w.r.t.
word frequencyin the unlabeled data.Figure 2(b) further supports our analysis.
Bina-rizedEmb also reduce much of the errors for thehighly frequent words (32k-64k).As expected, the distributional prototype fea-tures produce fewest errors in most cases.
Themain reason is that the prototype features are task-specific.
The prototypes are extracted from thetraining data and contained indicative informationof the target labels.
By contrast, the other em-bedding features are simply derived from generalword representations and are not specialized forcertain tasks, such as NER.5.3.2 Linear SeparabilityAnother reason for the superiority of the proposedembedding features is that the high-dimensionaldiscrete features are more linear separable thanthe low-dimensional continuous embeddings.
Toverify the hypothesis, we further carry out experi-ments to analyze the linear separability of the pro-posed discrete embedding features against densecontinuous embeddings.We formalize this problem as a binary classi-fication task, to determine whether a word is anNE or not (NE identification).
The linear supportvector machine (SVM) is used to build the clas-sifiers, using different embedding features respec-1170?256 256?512 512?1k 1k?2kFrequency of word in unlabeled datanumber of per?tokenerrors050100150200250 DenseEmbBinarizedEmbClusterEmbDistPrototype(a)4k?8k 8k?16k 16k?32k 32k?64kFrequency of word in unlabeled datanumber of per?tokenerrors406080100120 DenseEmbBinarizedEmbClusterEmbDistPrototype(b)Figure 2: The number of per-token errors w.r.t.
word frequency in the unlabeled data.
(a) For rare words(frequency ?
2k).
(b) For frequent words (frequency ?
4k).Setting Acc.
#featuresDenseEmb 95.46 250BinarizedEmb 94.10 500ClusterEmb 97.57 482,635DistPrototype 96.09 1,700DistPrototype-binary 96.82 4,530Table 5: Performance of the NE/non-NE classi-fication on the CoNLL-2003 development datasetusing different embedding features.tively.
We use the LIBLINEAR tool (Fan et al.,2008) as our SVM implementation.
The penaltyparameter C is tuned from 0.1 to 1.0 on the devel-opment dataset.
The results are shown in Table 5.As we can see, NEs and non-NEs can be betterseparated using ClusterEmb or DistPrototype fea-tures.
However, the BinarizedEmb features per-form worse than the direct use of word embeddingfeatures.
The reason might be inferred from thethird column of Table 5.
As demonstrated in Wangand Manning (2013), linear models are more ef-fective in high-dimensional and discrete featurespace.
The dimension of the BinarizedEmb fea-tures remains small (500), which is merely twicethe DenseEmb.
By contrast, feature dimensionsare much higher for ClusterEmb and DistProto-type, leading to better linear separability and thuscan be better utilized by linear models.We notice that the DistPrototype features per-form significantly worse than ClusterEmb in NEidentification.
As described in Section 3.4, inprevious experiments, we automatically extractedprototypes for each label, and propagated the in-formation via distributional similarities.
Intu-itively, the prototypes we used should be more ef-fective in determining fine-grained NE types thanidentifying whether a word is an NE.
To verifythis, we extract new prototypes considering onlytwo labels, namely, NE and non-NE, using thesame metric in Section 3.4.
As shown in the lastrow of Table 5, higher performance is achieved.6 Related StudiesSemi-supervised learning with generalized wordrepresentations is a simple and general way of im-proving supervised NLP systems.
One commonapproach for inducing generalized word represen-tations is to use clustering (e.g., Brown clustering)(Miller et al., 2004; Liang, 2005; Koo et al., 2008;Huang and Yates, 2009).Aside from word clustering, word embeddingshave been widely studied.
Bengio et al.
(2003)propose a feed-forward neural network based lan-guage model (NNLM), which uses an embeddinglayer to map each word to a dense continuous-valued and low-dimensional vector (parameters),and then use these vectors as the input to predictthe probability distribution of the next word.
TheNNLM can be seen as a joint learning frameworkfor language modeling and word representations.Alternative models for learning word embed-dings are mostly inspired by the feed-forwardNNLM, including the Hierarchical Log-BilinearModel (Mnih and Hinton, 2008), the recurrentneural network language model (Mikolov, 2012),the C&W model (Collobert et al., 2011), the log-linear models such as the CBOW and the Skip-118gram model (Mikolov et al., 2013a; Mikolov etal., 2013b).Aside from the NNLMs, word embeddings canalso be induced using spectral methods, such aslatent semantic analysis and canonical correlationanalysis (Dhillon et al., 2011).
The spectral meth-ods are generally faster but much more memory-consuming than NNLMs.There has been a plenty of work that exploitsword embeddings as features for semi-supervisedlearning, most of which take the continuous fea-tures directly in linear models (Turian et al., 2010;Guo et al., 2014).
Yu et al.
(2013) propose com-pound k-means cluster features based on word em-beddings.
They show that the high-dimensionaldiscrete cluster features can be better utilized bylinear models such as CRF.
Wu et al.
(2013) fur-ther apply the cluster features to transition-baseddependency parsing.7 Conclusion and Future WorkThis paper revisits the problem of semi-supervisedlearning with word embeddings.
We present threedifferent approaches for a careful comparison andanalysis.
Using any of the three embedding fea-tures, we obtain higher performance than the di-rect use of continuous embeddings, among whichthe distributional prototype features perform thebest, showing the great potential of word embed-dings.
Moreover, the combination of the proposedembedding features provides significant additiveimprovements.We give detailed analysis about the experimen-tal results.
Analysis on rare words and linear sep-arability provides convincing explanations for theperformance of the embedding features.For future work, we are exploring a novel and atheoretically more sounding approach of introduc-ing embedding kernel into the linear models.AcknowledgmentsWe are grateful to Mo Yu for the fruitful discus-sion on the implementation of the cluster-basedembedding features.
We also thank Ruiji Fu,Meishan Zhang, Sendong Zhao and the anony-mous reviewers for their insightful comments andsuggestions.
This work was supported by theNational Key Basic Research Program of Chinavia grant 2014CB340503 and the National Natu-ral Science Foundation of China (NSFC) via grant61133012 and 61370164.ReferencesRie Kubota Ando and Tong Zhang.
2005.
A high-performance semi-supervised learning method fortext chunking.
In Proceedings of the 43rd annualmeeting on association for computational linguis-tics, pages 1?9.
Association for Computational Lin-guistics.Yoshua Bengio, R. E. Jean Ducharme, Pascal Vincent,and Christian Janvin.
2003.
A neural probabilisticlanguage model.
The Journal of Machine LearningResearch, 3(Feb):1137?1155.Yoshua Bengio, Aaron Courville, and Pascal Vincent.2013.
Representation learning: A review and newperspectives.
Pattern Analysis and Machine Intelli-gence, IEEE Transactions on, 35(8):1798?1828.Gerlof Bouma.
2009.
Normalized (pointwise) mutualinformation in collocation extraction.
Proceedingsof GSCL, pages 31?40.Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n-gram models of natural language.Computational linguistics, 18(4):467?479.Ronan Collobert, Jason Weston, L. E. On Bottou,Michael Karlen, Koray Kavukcuoglu, and PavelKuksa.
2011.
Natural language processing (almost)from scratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Un-gar.
2011.
Multi-view learning of word embeddingsvia cca.
In NIPS, volume 24 of NIPS, pages 199?207.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
The Journal ofMachine Learning Research, 9:1871?1874.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 363?370.
Association for Computational Lin-guistics.Yoav Goldberg and Omer Levy.
2014. word2vec ex-plained: deriving mikolov et al.
?s negative-samplingword-embedding method.
CoRR, abs/1402.3722.Jiang Guo, Wanxiang Che, Haifeng Wang, and TingLiu.
2014.
Learning sense-specific word embed-dings by exploiting bilingual resources.
In Pro-ceedings of COLING 2014, the 25th InternationalConference on Computational Linguistics: Techni-cal Papers, pages 497?507, Dublin, Ireland, August.Dublin City University and Association for Compu-tational Linguistics.119Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe main conference on Human Language Technol-ogy Conference of the North American Chapter ofthe Association of Computational Linguistics, pages320?327.
Association for Computational Linguis-tics.Fei Huang and Alexander Yates.
2009.
Distribu-tional representations for handling sparsity in super-vised sequence-labeling.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP: Volume 1-Volume 1, Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the AFNLP: Volume 1-Volume 1, pages495?503.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proc.
of the Annual Meeting of theAssociation for Computational Linguistics (ACL),Proc.
of the Annual Meeting of the Association forComputational Linguistics (ACL), pages 873?882,Jeju Island, Korea.
ACL.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency pars-ing.
In Kathleen McKeown, Johanna D. Moore, Si-mone Teufel, James Allan, and Sadaoki Furui, edi-tors, Proc.
of ACL-08: HLT, Proc.
of ACL-08: HLT,pages 595?603, Columbus, Ohio.
ACL.Vijay Krishnan and Christopher D Manning.
2006.An effective two-stage model for exploiting non-local dependencies in named entity recognition.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pages 1121?1128.
Association for Compu-tational Linguistics.Percy Liang.
2005.
Semi-supervised learning for natu-ral language.
Master thesis, Massachusetts Instituteof Technology.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word repre-sentations in vector space.
In Proc.
of Workshop atICLR, Proc.
of Workshop at ICLR, Arizona.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In Proc.
of the NIPS, Proc.
of the NIPS, pages3111?3119, Nevada.
MIT Press.Tomas Mikolov.
2012.
Statistical Language ModelsBased on Neural Networks.
Ph.
d. thesis, Brno Uni-versity of Technology.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrim-inative training.
In HLT-NAACL, volume 4, pages337?342.Andriy Mnih and Geoffrey E. Hinton.
2008.
A scal-able hierarchical distributed language model.
InProc.
of the NIPS, Proc.
of the NIPS, pages 1081?1088, Vancouver.
MIT Press.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL-HLT, pages 380?390.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProceedings of the Thirteenth Conference on Com-putational Natural Language Learning, CoNLL ?09,pages 147?155, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.D Sculley.
2010.
Combined regression and ranking.In Proceedings of the 16th ACM SIGKDD interna-tional conference on Knowledge discovery and datamining, pages 979?988.
ACM.Erik F Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the conll-2003 shared task:Language-independent named entity recognition.
InProceedings of the seventh conference on Naturallanguage learning at HLT-NAACL 2003-Volume 4,pages 142?147.
Association for Computational Lin-guistics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Jan Hajic, San-dra Carberry, and Stephen Clark, editors, Proc.
ofthe Annual Meeting of the Association for Computa-tional Linguistics (ACL), Proc.
of the Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 384?394, Uppsala, Sweden.
ACL.Mengqiu Wang and Christopher D. Manning.
2013.Effect of non-linear deep architecture in sequence la-beling.
In Proc.
of the Sixth International Joint Con-ference on Natural Language Processing, Proc.
ofthe Sixth International Joint Conference on NaturalLanguage Processing, pages 1285?1291, Nagoya,Japan.
Asian Federation of Natural Language Pro-cessing.Xianchao Wu, Jie Zhou, Yu Sun, Zhanyi Liu, Dian-hai Yu, Hua Wu, and Haifeng Wang.
2013.
Gener-alization of words for chinese dependency parsing.IWPT-2013, page 73.Mo Yu, Tiejun Zhao, Daxiang Dong, Hao Tian, and Di-anhai Yu.
2013.
Compound embedding features forsemi-supervised learning.
In Proc.
of the NAACL-HLT, Proc.
of the NAACL-HLT, pages 563?568, At-lanta.
NAACL.120
