Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1793?1802, Dublin, Ireland, August 23-29 2014.Interpolated Dirichlet Class Language Model for Speech RecognitionIncorporating Long-distance N-gramsMd.
Akmal Haidar and Douglas O?ShaughnessyINRS-EMT, University of Quebec6900-800 De la Gauchetier Ouest, H5A 1K6, Montreal (Quebec), Canadahaidar@emt.inrs.ca, dougo@emt.inrs.caAbstractWe propose a language modeling (LM) approach incorporating interpolated distanced n-grams ina Dirichlet class language model (DCLM) (Chien and Chueh, 2011) for speech recognition.
TheDCLM relaxes the bag-of-words assumption and documents topic extraction of latent Dirichletallocation (LDA).
The latent variable of DCLM reflects the class information of an n-gram eventrather than the topic in LDA.
The DCLM model uses default background n-grams where classinformation is extracted from the (n-1) history words through Dirichlet distribution in calculat-ing n-gram probabilities.
The model does not capture the long-range information from outsideof the n-gram window that can improve the language modeling performance.
In this paper, wepresent an interpolated DCLM (IDCLM) by using different distanced n-grams.
Here, the classinformation is exploited from (n-1) history words through the Dirichlet distribution using in-terpolated distanced n-grams.
A variational Bayesian procedure is introduced to estimate theIDCLM parameters.
We carried out experiments on a continuous speech recognition (CSR) taskusing the Wall Street Journal (WSJ) corpus.
The proposed approach shows significant perplexityand word error rate (WER) reductions over the other approach.1 IntroductionStatistical n-gram LMs have been successfully used for speech recognition and many other applications.They suffer from insufficiencies of training data and long-distance information, which limit the modelgeneralization (Chien, 2006).
The data sparseness problem is usually solved by backoff smoothing usinglower-order language models (Katz, 1987; Kneser and Ney, 1995).
The class-based language modelwas investigated where the class n-grams were calculated by considering the generation of concatenatedclasses rather than words (Brown et al., 1992).
By incorporating the multidimensional word classesand considering the classes from various positions of left and right contextual information (Bai et al.,1998), the class n-gram can be improved (Yamamoto et al., 2003).
A neural network language model(NNLM) was trained by linearly projecting the history words of an n-gram event into a continuousspace (Bengio et al., 2003; Schwenk, 2007).
Later, a recurrent neural network-based LM was investigatedthat shows better results than NNLM (Mikolov et al., 2010; Mikolov et al., 2011).
Unsupervised class-based language models such as Random Forest LM (Xu and Jelinek, 2007), Model M (Chen, 2008) havebeen investigated that outperform a word-based LM.
However, the long-distance information is capturedby using a cache-based LM that takes advantage of the fact that a word observed earlier in a documentcould occur again.
This helps to increase the probability of the seen words when predicting the nextword (Kuhn and Mori, 1990).To compensate for the weakness of the n-gram models, latent topic analysis has been used broadly.Several techniques such as Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Bellegarda, 2000),probabilistic LSA (PLSA) (Hofmann, 1999; Gildea and Hofmann, 1999), and Latent Dirichlet Alloca-tion (LDA) (Blei et al., 2003) have been studied to extract the latent semantic information from a trainingThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1793corpus.
The LSA, PLSA and LDA models have been used successfully in recent research work for LMadaptation (Bellegarda, 2000; Gildea and Hofmann, 1999; Mrva and Woodland, 2004; Tam and Schultz,2005; Tam and Schultz, 2006; Haidar and O?Shaughnessy, 2011; Haidar and O?Shaughnessy, 2012b;Haidar and O?Shaughnessy, 2012a).
Even so, the extracted topic information is not directly useful forspeech recognition, where the latent topic of n-gram events should be of concern.
In (chien and Chueh,2008), a latent Dirichlet language model (LDLM) was proposed where the latent topic information wasexploited from (n-1) history words through the Dirichlet distribution in calculating the n-gram proba-bilities.
A topic cache language model was proposed where the topic information was obtained fromlong-distance history through multinomial distributions (Chueh and Chien, 2010).
Topic-dependent-class-based n-gram LM was proposed where the LSA method was used to reveal latent topic informationfrom noun-noun relations (Naptali et al., 2012).
In (Bassiou and Kotropoulos, 2010), a PLSA techniqueenhanced with long-distance bigrams was used to incorporate the long-term word dependencies in de-termining word clusters.
This technique was used in (Haidar and O?Shaughnessy, 2013b) and (Haidarand O?Shaughnessy, 2013a) for the PLSA and LDLM models respectively where the long-distance in-formation was captured by using interpolated distanced n-grams and their parameters were estimatedby using an expectation maximization (EM) procedure (Dempster et al., 1977).
In (Chien and Chueh,2011), the DCLM model was proposed to tackle the data sparseness and to extract the large-span infor-mation for the n-gram model.
In this model, the topic structure in LDA is assumed to derive the hiddenclasses of histories in calculating the language model.
A Bayesian class-based language model was pre-sented where a variational Bayes-EM procedure was used to compute the model parameters.
Also, acache DCLM model was proposed to capture the long-distance information beyond the n-gram window.However, in the DCLM model (Chien and Chueh, 2011), the class information of the history words wasobtained from the n-gram events of the corpus.
Here, the long-range information outside the n-gramwindow is not captured.
In this paper, we present an IDCLM model to capture the long-range informa-tion in the DCLM using the interpolated distanced n-grams.
The n-gram probabilities of the proposedIDCLM model are computed by mixing the component distanced word probabilities for classes and theinterpolated class information for histories.
Similar to the DCLM model, the parameters of the IDCLMmodel are computed by using the variational Bayesian-EM procedure.The rest of this paper is organized as follows.
Section 2 is used for reviewing the DCLM model.
Theproposed IDCLM model is described in section 3.
The comparison of the IDCLM and the DCLM modelsis described in section 4.
The experimental details are described in section 5.
Finally, the conclusionsand future work are described in section 6.2 DCLMLDA is used to compute the document probability by using the topic structure at the document level,which is inconsistent with the language model for speech recognition where the n-gram regularities arecharacterized (Chien and Chueh, 2011).
The DCLM was developed to model the n-gram events of thecorpus for speech recognition.
In the DCLM, the class structure is described by Dirichlet densities andestimated from n-gram events.
The graphical model of the DCLM for a text corpus that comprises n-gram events {wi?1i?n+1, wi} is described in Figure 1.
Here, H and Nhrepresent the number of historyevents wi?1i?n+1and the number of collected words that occur following the history wi?1i?n+1, respectively.The (n-1) history words wi?1i?n+1are represented by a (n-1)V ?
1 vector h, consisting of n-1 blocksubvectors, with the entries of the seen words assigned to ones and those of unseen words assignedto zeros (Chien and Chueh, 2011).
Here, V represents the size of the vocabulary.
The vector h isthen projected into a C-dimensional continuous class space using a class-dependent linear discriminantfunction:gc(h) = aTch (1)where aTcis the cthrow vector of matrix A = [a1, ?
?
?
,aC] (Chien and Chueh, 2011).
The functiongc(h) describes the class posterior probability p(c|h), which is used in predicting the class informationfor an unseen history (Chien and Chueh, 2011).
The model can be described as:1794Hh?A?wi N hciFigure 1: The graphical model of the DCLM.
Shaded circles represent observed variables.?
For each history vector h, the class information c is drawn from a history-dependent Dirichlet prior?, which is related to a global projection matrix A:p(?|h,A) ?C?c=1?gc(h)?1c, (2)?
For each predicted word wiof the n-gram events from a multinomial distribution with parameter?, the associated class ciis chosen by using a multinomial distribution with parameter ?.
The jointprobability of the variable ?, ci, and wiconditioned on h can be computed as:p(?, ci, wi|h,A,?)
= p(?|h,A)p(ci|?)p(wi|ci,?)
(3)?
The conditional probability in the n-gram language model can thus be obtained as:p(wi|h,A,?)
=?p(?|h,A)C?ci=1p(ci|?)p(wi|ci,?
)d?, (4)where the integral is computed as:p(ci|h,A) =?p(?|h,A)p(ci|?)d?
=gci(h)?Cj=1gj(h).
(5)which is an expectation of a Dirichlet distribution of latent class ci(Chien and Chueh, 2011).Therefore, the probability of an n-gram event using the DCLM (Equation 4 and 5) can be writtenas (Chien and Chueh, 2011):p(wi|h,A,?)
=C?c=1p(wi|c,?
)gc(h)?Cj=1gj(h)(6)The parameters (A,?)
of the model are computed by using the variational bayesian EM (VB-EM) pro-cedure (Chien and Chueh, 2011).1795?I ?1wi N h1ciwici N h2N hLH IhI=?d=1Lhdci wi?2?L..AIFigure 2: The graphical model of the IDCLM.
Shaded circles represent observed variables.3 Proposed IDCLMThe DCLM does not capture the long-range information from outside of the n-gram window (Chienand Chueh, 2011).
To incorporate the long-range information into the DCLM, we propose an IDCLMwhere the class information is extracted from interpolated distance n-gram histories through a Dirichletdistribution in calculating the language model probability.
In this model, we interpolate the distancedn-gram events into the original n-gram events of the DCLM.
The graphical model of the IDCLM isdescribed in Figure 2.
In Figure 2, HIcontains the histories of all the distanced d n-grams, d representsthe distance between words in the n-gram events, and L describes the maximum length of distance d.When d = 1, the n-grams are the default background n-grams.
For example, the distanced tri-gramsof the phrase ?Interpolated Dirichlet Class Language Model for Speech Recognition?
are described inTable 1 for the distance d = 1, 2, 3.
Here, the (n-1)V dimensional discrete history vector hIis projectedd Trigrams1 Interpolated Dirichlet Class, Dirichlet Class Language, Class Language Model,Language Model for, Model for Speech, for Speech Recognition2 Interpolated Class Model, Dirichlet Language for, Class Model Speech, Language for Recognition3 Interpolated Language Speech, Dirichlet Model RecognitionTable 1: Distanced tri-grams for the phrase ?Interpolated Dirichlet Class Language Model for SpeechRecognition?into a C-dimensional continuous class space using a class-dependent linear discriminant function:gc(hI) = aTc,IhI(7)1796where hIis the combined histories of all the distanced histories hdand is defined as hI=?Ld=1hd.Here,?represents the logical OR operator.
aTc,Iis the cthrow vector of the matrix AIand gc(hI)describes the class posterior probability p(c|hI).The n-gram probability of the IDCLM model is computed as:pI(wi|hI,AI,?d) =C?ci=1{[?d?dpd(wi|ci,?d)]?
?p(?I|hI,AI)p(ci|?I)d?I}=C?c=1[?d?d?d,ic]gc(hI)?Cj=1gj(hI)(8)where ?dare the weights for each component probability estimated on the held-out data using the EMalgorithm (Bassiou and Kotropoulos, 2010; Dempster et al., 1977).The parameters of the IDCLM model are computed using the variational Bayes EM (VB-EM) proce-dure by maximizing the marginal distribution of the training data that contains a set of n-gram eventsD = {wi?1i?n+1, wi}:log p(D|AI,?d) =?
(wi,hI)?Dlog pI(wi|hI,AI,?d)=?hIlog{?p(?I|hI,AI)?
[?dNhd?j=1C?cj=1?dpd(wj|cj,?d)p(cj|?I)]d?I}(9)where D contains all the distanced n-gram events, Nhdrepresents the number of collected words thatoccur following the history hdin d-distanced n-grams.
In Equation 9, the summation is over all possiblehistories in training samples D. However, directly optimizing the Equation 9 is intractable (Chien andChueh, 2011).
A variational IDCLM is introduced where the marginal likelihood is approximated bymaximizing the lower bound of Equation 9.
The VB-EM procedure is required since the parameterestimation involves the latent variables of {?I, chd= {ci}Nhdi=1}.The lower bound L(AI,?d;??I,?
?d) is given by:?hI{log ?
(C?c=1gc(hI))?C?c=1log ?
(gc(hI)) +C?c=1(gc(hI)?
1)?(?(?hI,c)??(C?j=1?hI,j))}+?d?hdNhd?i=1C?c=1?d?hd,ic(?(?hI,c)??(C?j=1?hI,j))+?d?hdNhd?i=1C?c=1V?v=1?d?hd,ic?
(wv, wi) log ?d,vc?
?hI{log ?
(C?c=1?hI,c)?C?c=1log ?(?hI,c)+C?c=1(?hI,c?
1)(?(?hI,c)??(C?j=1?hI,j))}?
?d?hdNhd?i=1C?c=1?d?hd,iclog ?hd,icwhere ?(.)
is the derivative of the log gamma function, and is known as a digamma function (Chienand Chueh, 2011).
The history-dependent variational parameters {?
?hI= ??hI,c,??hd=?
?hd,vc}, corre-sponding to the latent variables ?I, ch,d, are then estimated in the VB-E step by setting the differentials(?L(?))/(?
?hI,c) and (?L(?))/(?
?hd,ic) to zero respectively (Chien and Chueh, 2011):?
?hI,c= gc(hI) +?dNhd?i=1?d?hd,ic(10)1797??hd,ic=?d,icexp[?(?hI,c)??(?Cj=1?hI,j)]?Cl=1?d,ilexp[?(?hI,l)??
(?Cj=1?hI,j)](11)In computing?
?hd,icthe corresponding ?hd,cis used in Equation 11.
With the updated??hI,?
?hdin theVB-E step, the IDCLM parameters {AI,?d} are estimated in the VB-M step as (Chien and Chueh,2011):??d,vc=?hd?Nhdi=1?d??hd,ic?
(wv, wi)?Vm=1?hd?Nhdi=1?d??hd,ic?
(wm, wi)(12)where?Vv=1?d,vc=1 and ?
(wv, wi) is the Kronecker delta function that equals one when vocabularyword wvis identical to the predicted word wiand equals zero otherwise.
The gradient ascent algorithmis used to calculate the parameters?AI= [a?1,I, ?
?
?
, a?C,I] by updating the gradient 5ac,Ias (Chien andChueh, 2011):5ac,I?5ac,I+?hI[?(C?j=1gj(hI))??
(gc(hI)) + ?(??hI,c)??(C?j=1?
?hI,j)].hI(13)The n-gram probabilities pt(wi,ht,AI,?d) of the test document t are then computed using Equa-tion 8.
To capture the local lexical regularities, the model pt(wi|ht,AI,?d) is then interpolated with thebackground trigram model as:pInterpolated(wi|h) = ?pBackground(wi|h) + (1?
?
)pt(wi|ht,AI,?d) (14)4 Comparison of DCLM and IDCLM ModelsIn the DCLM model, the class information for the (n?
1) history words is obtained by using the n-gramcounts in the corpus.
The current word is predicted from the history-dependent Dirichlet parameter,which is controlled by a matrix A and corpus-based histories h (Chien and Chueh, 2011).
In contrast,the IDCLM model captures long-range information by incorporating distanced n-grams.
Here, the classinformation is exploited for the interpolated (n ?
1) history words hIthat are obtained from all thedistanced n-gram events.
Both the DCLM and IDCLM exploit the word distribution given the historywords.
They perform the history clustering of the corpus.
For the DCLM model, the number of parame-ters {A,?}
increases linearly with the number of history words and is given by (n?
1)CV + CV .
Forthe IDCLM model, the number of parameters {AI,?d} increases linearly with the number of historywords and distance d and is given by ((n?
1)CV +CV d).
The time complexity of DCLM and IDCLMare O(HV C) and O(HIV Cd) with H corpus-based histories, HIcorpus-based interpolated histories,V vocabulary words, d distances and C classes.5 Experiments5.1 Data and experimental setupThe LM approaches are evaluated using the Wall Street Journal (WSJ) corpus (Paul and Baker, 1992).The SRILM toolkit (Stolcke, 2002) and the HTK toolkit (Young et al., 2013) are used for generating theLMs and computing the WER respectively.
The ?87-89 WSJ corpus is used to train language models.The background trigrams are trained using the back-off version of the Witten-Bell smoothing; the 5Knon-verbalized punctuation closed vocabulary.
We train the trigram IDCLM model using L = 2 andL = 3.
Ten EM iterations in the VB-EM procedure were used.
The initial values of the entries in thematrix ?,?dwere set to be 1/V and those in A,AIwere randomly set in the range [0,1].
To updatethe variational parameters in the VB-E step, one iteration was used.
The VB-M step was executed toupdate the parameters A,AIby three iterations (Chien and Chueh, 2011).
To capture the local lexi-cal regularity, trigrams of various methods are interpolated with the background trigrams.
The acousticmodel from (Vertanen, 2013) is used in our experiments.
The acoustic model is trained by using allWSJ and TIMIT (Garofolo et al., 1993) training data, the 40-phone set of the CMU dictionary (-, 2013),1798approximately 10000 tied-states, 32 Gaussians per state and 64 Gaussians per silence state.
The acous-tic waveforms are parameterized into a 39-dimensional feature vector consisting of 12 cepstral coeffi-cients plus the 0thcepstral, delta and delta delta coefficients, normalized using cepstral mean subtraction(MFCC0?D?A?Z).
We evaluated the cross-word models.
The values of the word insertion penalty,beam width, and the language model scale factor are -4.0, 350.0, and 15.0 respectively (Vertanen, 2013).The interpolation weights ?dand ?
are computed by optimizing on the held-out data according to themetric of perplexity.
The experiments are evaluated on the evaluation test, which is a total of 330 testutterances from the November 1992 ARPA CSR benchmark test data for vocabularies of 5K words (Pauland Baker, 1992; Woodland et al., 1994).5.2 Experimental ResultsDue to the higher memory and training time requirements for the IDCLM model, we trained the DCLMand IDCLM models for class sizes of 10 and 20.
The perplexity and WER results are described in Table 2and Figure 3 respectively.Language Model 10 Classes 20 ClassesBackground (B) 109.41 109.41B+Class 106.65 106.97B+DCLM 100.20 100.45B+IDCLM (L=2) 98.01 97.94B+IDCLM (L=3) 95.63 95.43Table 2: Perplexity results of the models?
??????????????????
??
??
????
?
??
???
?
??
????
??
??
????
??
??
???
??
?
???????
??
??
?????
??
?
????
?
??
?????
??
?
??????
???????
?
??????
???
????
?????
??
?
??????
???
????
????
?Figure 3: WER results for different class sizesFrom Table 2, we can note the proposed IDCLM model outperforms the other models for all classsizes.
The performance of IDCLM improves with more distances (L = 3).We evaluated the WER experiments using lattice rescoring.
In the first pass decoding, we used thebackground trigram for lattice generation.
In the second pass, we applied the interpolated model forlattice rescoring.
The WER results are described in Figure 3.
From Figure 3, we can note that theproposed IDCLM (L = 3) model yields a WER reduction of about 34.54% (5.79% to 3.79%), 33.5%(5.7% to 3.79%), and 9.76% (4.2% to 3.79%) for 10 classes and about 33.85% (5.79% to 3.83%), 32.8%1799(5.7% to 3.83%), and 11.34% (4.32% to 3.83%) over the background trigram, class trigram (Brown et al.,1992), and the DCLM (Chien and Chueh, 2011) approaches respectively.
The significance improvementin WER is done by using a match-pair-test where the misrecognized words in each test utterance arecounted.
The p-values are described in Table 3.
From Table 3, we can note that the IDCLM (L = 2)Language Model 10 Classes 20 ClassesB+Class & B+IDCLM (L=2) 3.8E-10 4.3E-10B+Class & B+IDCLM (L=3) 4.7E-12 4.7E-12B+DCLM & B+IDCLM (L=2) 0.04 0.01B+DCLM & B+IDCLM (L=3) 0.004 0.006Table 3: p-values obtained from the match-pair test on the WER resultsis statistically significant to the class-based LM (Brown et al., 1992) and DCLM (Chien and Chueh,2011) at a significance level of 0.01 and 0.05 respectively.
However, the IDCLM (L = 3) model isstatistically significant to the above models at a significance level of 0.01.
We have also seen that thecache DCLM model also gives the same results as DCLM (Chien and Chueh, 2011) for smaller numberof classes (Chien and Chueh, 2011).6 Conclusions and Future WorkIn this paper, we proposed an integration of distanced n-grams into the original DCLM model (Chienand Chueh, 2011).
The DCLM model (Chien and Chueh, 2011) extracted the class information from the(n-1) history words through a Dirichlet distribution in calculating the n-gram probabilities.
However, itdoes not capture the long-range semantic information from outside of the n-gram events.
The proposedIDCLM overcomes the shortcomings of DCLM by incorporating the interpolated long-distance n-gramsthat capture the long-term word dependencies.
Using the IDCLM, the class information for the historiesis trained using the interpolated distanced n-grams.
The IDCLM yields better results with includingmore distances (L = 3).
The model probabilities are computed by weighting the component wordprobabilities for classes and the interpolated class information for histories.
A variational Bayesian EM(VB-EM) procedure is presented to estimate the model parameters.For future work, we will evaluate the proposed approach with neural network-based language mod-els and exponential class-based language models.
Furthermore, we will find out a way to perform theexperiments for higher numbers of classes.References-.
2013.
The Carnegie Mellon University (CMU) Pronounciation Dictionary.
http://www.speech.cs.cmu.edu/cgi-bin/cmudict.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.
Maximum Likelihood from Incomplete Data via the EMAlgorithm.
Journal of the Royal Statistical Society, Series B 39(1):1 ?
38.Andreas Stolcke.
2002.
SRILM-an Extensible Language Modeling Toolkit.
In Proceedings of ICSLP, pages901?904.Chuang-H. Chueh and Jen-T. Chien.
2010.
Topic Cache Language Model for Speech Recognition.
In Proc.
ofICASSP, pages 5194?5197.Daniel Gildea and Thomas Hofmann.
1999.
Topic-based Language Models using EM.
In Proceedings of EU-ROSPEECH, pages 2167?2170.David Mrva and Philip C. Woodland.
2004.
A PLSA-based Language Model for Conversational TelephoneSpeech.
In Proc.
of ICSLP, pages 2257?2260.David M. Blei, Andrew Y.
Ng., and Michael I. Jordan.
2003.
Latent Dirichlet Allocation.
Journal of MachineLearning Research, 3:993?1022.1800Dougls B. Paul and Janet M. Baker.
1992.
The Design for the Wall Street Journal-based CSR Corpus.
In Proc.
ofICSLP, pages 899?902.Hirofumi Yamamoto, Shuntaro Isogai, and Yoshinori Sagisaka.
2003.
Multi-class Composite n-gram LanguageModel.
Speech Communication, 41:369 ?
379.Holger Schwenk.
2007.
Continuous Space Language Models.
Computer Speech and Language, 21:492 ?
518.Jen-T. Chien and Chuang-H. Chueh.
2008.
Latent Dirichlet Language Model for Speech Recognition.
In Proc.
ofIEEE SLT Workshop, pages 201?204.Jen-T. Chien and Chuang-H. Chueh.
2011.
Dirichlet Class Language Models for Speech Recognition.
IEEETrans.
on Audio, Speech and Language Processing, 19(3):482 ?
495.Jen-T. Chien.
2006.
Association Pattern Language Modeling.
IEEE Trans.
on Audio, Speech and LanguageProcessing, 14(5):1719 ?
1728.Jerome R. Bellegarda.
2000.
Exploiting Latent Semantic Information in Statistical Language modeling.
IEEETransactions on Speech and Audio Processing, 88 (8):1279?1296.John S. Garofolo, Lori F. Lamel, William M. Fisher, Jonathan G. Fiscus, David S. Pallett, Nancy L. Dahlgren, andVictor Zue.
1993.
TIMIT Acoustic-phonetic Continuous Speech Corpus.
Linguistic Data Consortium.Keith Vertanen.
2013.
HTK Wall Street Journal Training Recipe.
http://www.keithv.com/software/htk/us/.Md.
A. Haidar and Douglas O?Shaughnessy.
2011.
Unsupervised Language Model Adaptation using N-gramWeighting.
In Proceedings of CCECE, pages 857?860.Md.
A. Haidar and Douglas O?Shaughnessy.
2012a.
LDA-based LM Adaptation using Latent Semantic Marginalsand Minimum Discrimination Information.
In Proceedings of EUSIPCO, pages 2040?2044.Md.
A. Haidar and Douglas O?Shaughnessy.
2012b.
Topic N-gram Count Language Model for Speech Recogni-tion.
In Proceedings of IEEE Spoken Language Technology (SLT) Workshop, pages 165?169.Md.
A. Haidar and Douglas O?Shaughnessy.
2013a.
Fitting Long-range Information using Interpolated Distancedn-grams and Cache Models into a Latent Dirichlet Language Model for Speech Recognition.
In Proc.
of IN-TERSPEECH, pages 2678?2682.Md.
A. Haidar and Douglas O?Shaughnessy.
2013b.
PLSA Enhance with a Long-distance Bigram LanguageModel for Speech Recognition.
In Proc.
of EUSIPCO.Nikoletta Bassiou and Constantine Kotropoulos.
2010.
Word Clustering PLSA Enhanced with Long DistanceBigrams.
In Proc.
of International Conferance on Pattern Recognition, pages 4226?4229.P.C.
Woodland, J. J. Odell, V. Valtchev, and S. J.
Young.
1994.
Large Vocabulary Continuous Speech Recognitionusing HTK.
In Proceedings of ICASSP, pages 125?128.Peng Xu and Frederick Jelinek.
2007.
Random Forests and the Data Sparseness Problem in Language Modeling.Computer Speech and Language, 21 (1):105 ?
152.Peter F. Brown, Vincent Della Pietra, Peter De Souza, Jenifer Lai, and Robert L. Mercer.
1992.
Classbased n-gramModels of Natural Language.
Computational Linguist., 18 (4):467 ?
479.Reinhard Kneser and Hermann Ney.
1995.
Improved Backing-off for m-gram Language Modeling.
In Proc.
IEEEInt Conf.
Acoust., Speech, Signal Process., pages 181?184.Roland Kuhn and Renato D. Mori.
1990.
A Cache-based Natural Language Model for Speech Recognition.
IEEETransactions of Pattern Analysis and Machine Intelligence, 12 (6):570?583.Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman.
1990.Indexing by Latent Semantic Analysis.
Journal of the American Society for Information Science, 41(6):391 ?407.Shuanghu Bai, Haizhou Li, Zhiwei Lin, and Baosheng Yuan.
1998.
Building Class-based Language Models withContextual Statistics.
In Proc.
IEEE Int Conf.
Acoust., Speech, Signal Process, pages 173?176.1801Slava M. Katz.
1987.
Estimation of Probabilities from Sparse Data for the Language Model Component of aSpeech Recognizer.
EEE Trans.
Acoust., Speech, Signal Process., 35(3):400 ?
401.Stanley Chen, 2008.
Performance Prediction for Exponential Language Models.
Tech.
Rep. RC 24671, IBMResearch, Tech.
Rep.Steve Young, Phil Woodland, Gunnar Evermann, and Mark Gales.
2013.
The HTK Toolkit 3.4.1. http://htk.eng.cam.ac.uk/.Thomas Hofmann.
1999.
Probabilistic Latent Semantic Analysis.
In Proceedings of the Fifteenth Annual Confer-ence on Uncertainty in Artificial Intelligence (UAI-99), pages 289?296, San Francisco, CA.
Morgan Kaufmann.Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan H. Cernocky, and Sanjeev Khudanpur.
2010.
RecurrentNeural Network Based Language Model.
In Proc.
of INTERSPEECH, pages 1045?1048.Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan H. Cernocky, and Sanjeev Khudanpur.
2011.
ExtensionsRecurrent Neural Network Language Model.
In Proc.
of ICASSP, pages 5528?5531.Welly Naptali, Masatoshi Tsuchiya, and Seiichi Nakagawa.
2012.
Topic Dependent Class-based n-gram LanguageModel.
IEEE Trans.
on Audio, Speech and Language Processing, 20:1513 ?
1525.Yik-Cheung Tam and Tanja Schultz.
2005.
Dynamic Language Model Adaptation using Variational Bayes Infer-ence.
In Proceedings of INTERSPEECH, pages 5?8.Yik-Cheung Tam and Tanja Schultz.
2006.
Unsupervised Language Model Adaptation using Latent SemanticMarginals.
In Proceedings of INTERSPEECH, pages 2206?2209.Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin.
2003.
A Neural Probabilistic LanguageModel.
Journal of Machine Learning Research, 3:1137 ?
1155.1802
