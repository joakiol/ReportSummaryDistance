Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 923?932,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsPEM: A Paraphrase Evaluation Metric Exploiting Parallel TextsChang Liu1 and Daniel Dahlmeier2 and Hwee Tou Ng1,21Department of Computer Science, National University of Singapore2NUS Graduate School for Integrative Sciences and Engineering{liuchan1,danielhe,nght}@comp.nus.edu.sgAbstractWe present PEM, the first fully automatic met-ric to evaluate the quality of paraphrases, andconsequently, that of paraphrase generationsystems.
Our metric is based on three crite-ria: adequacy, fluency, and lexical dissimilar-ity.
The key component in our metric is a ro-bust and shallow semantic similarity measurebased on pivot language N-grams that allowsus to approximate adequacy independently oflexical similarity.
Human evaluation showsthat PEM achieves high correlation with hu-man judgments.1 IntroductionIn recent years, there has been an increasing inter-est in the task of paraphrase generation (PG) (Barzi-lay and Lee, 2003; Pang et al, 2003; Quirk et al,2004; Bannard and Callison-Burch, 2005; Kauchakand Barzilay, 2006; Zhao et al, 2008; Zhao et al,2009).
At the same time, the task has seen appli-cations such as machine translation (MT) (Callison-Burch et al, 2006; Madnani et al, 2007; Madnaniet al, 2008), MT evaluation (Kauchak and Barzilay,2006; Zhou et al, 2006a; Owczarzak et al, 2006),summary evaluation (Zhou et al, 2006b), and ques-tion answering (Duboue and Chu-Carroll, 2006).Despite the research activities, we see two majorproblems in the field.
First, there is currently no con-sensus on what attributes characterize a good para-phrase.
As a result, works on the application of para-phrases tend to build their own PG system in viewof the immediate needs instead of using an existingsystem.Second, and as a consequence, no automatic eval-uation metric exists for paraphrases.
Most works inthis area resort to ad hoc manual evaluations, such asthe percentage of ?yes?
judgments to the question of?is the meaning preserved?.
This type of evaluationis incomprehensive, expensive, and non-comparablebetween different studies, making progress hard tojudge.In this work we address both problems.
We pro-pose a set of three criteria for good paraphrases: ad-equacy, fluency, and lexical dissimilarity.
Consid-ering that paraphrase evaluation is a very subjec-tive task with no rigid definition, we conduct ex-periments with human judges to show that humansgenerally have a consistent intuition for good para-phrases, and that the three criteria are good indica-tors.Based on these criteria, we construct PEM (Para-phrase Evaluation Metric), a fully automatic evalua-tion metric for PG systems.
PEM takes as input theoriginal sentence R and its paraphrase candidate P ,and outputs a single numeric score b estimating thequality of P as a paraphrase of R. PG systems canbe compared based on the average scores of theiroutput paraphrases.
To the best of our knowledge,this is the first automatic metric that gives an objec-tive and unambiguous ranking of different PG sys-tems, which serves as a benchmark of progress inthe field of PG.The main difficulty of deriving PEM is to measuresemantic closeness without relying on lexical levelsimilarity.
To this end, we propose bag of pivot lan-guage N-grams (BPNG) as a robust, broad-coverage,and knowledge-lean semantic representation for nat-ural language sentences.
Most importantly, BPNGdoes not depend on lexical or syntactic similarity,allowing us to address the conflicting requirementsof paraphrase evaluation.
The only linguistic re-923source required to evaluate BPNG is a parallel textof the target language and an arbitrary other lan-guage, known as the pivot language.We highlight that paraphrase evaluation and para-phrase recognition (Heilman and Smith, 2010; Dasand Smith, 2009; Wan et al, 2006; Qiu et al, 2006)are related yet distinct tasks.
Consider two sentencesS1 and S2 that are the same except for the substitu-tion of a single synonym.
A paraphrase recognitionsystem should assign them a very high score, but aparaphrase evaluation system would assign a rela-tively low one.
Indeed, the latter is often a betterindicator of how useful a PG system potentially isfor the applications of PG described earlier.The rest of the paper is organized as follows.
Wesurvey other automatic evaluation metrics in naturallanguage processing (NLP) in Section 2.
We definethe task of paraphrase evaluation in Section 3 anddevelop our metric in Section 4.
We conduct a hu-man evaluation and analyze the results in Section 5.The correlation of PEM with human judgments isstudied in Section 6.
Finally, we discuss our find-ings and future work in Section 7 and conclude inSection 8.2 Related workThemost well-known automatic evaluation metric inNLP is BLEU (Papineni et al, 2002) for MT, basedon N-gram matching precisions.
The simplicity ofBLEU lends well to MT techniques that directly op-timize the evaluation metric.The weakness of BLEU is that it operates purelyat the lexical surface level.
Later works attempt totake more syntactic and semantic features into con-sideration (see (Callison-Burch et al, 2009) for anoverview).
The whole spectrum of NLP resourceshas found application in machine translation eval-uation, including POS tags, constituent and depen-dency parses, WordNet (Fellbaum, 1998), semanticroles, textual entailment features, and more.
Manyof these metrics have been shown to correlate bet-ter with human judges than BLEU (Chan and Ng,2008; Liu et al, 2010).
Interestingly, few MT eval-uation metrics exploit parallel texts as a source ofinformation, when statistical MT is centered almostentirely around mining parallel texts.Compared to these MT evaluation metrics, ourmethod focuses on addressing the unique require-ment of paraphrase evaluation: that lexical closenessdoes not necessarily entail goodness, contrary to thebasis of MT evaluation.Inspired by the success of automatic MT evalua-tion, Lin (2004) and Hovy et al (2006) propose au-tomatic metrics for summary evaluation.
The for-mer is entirely lexical based, whereas the latter alsoexploits constituent and dependency parses, and se-mantic features derived from WordNet.The only prior attempt to devise an automaticevaluation metric for paraphrases that we are awareof is ParaMetric (Callison-Burch et al, 2008), whichcompares the collection of paraphrases discoveredby automatic paraphrasing algorithms against amanual gold standard collected over the same sen-tences.
The recall and precision of several currentparaphrase generation systems are evaluated.
Para-Metric does not attempt to propose a single metricto correlate well with human judgments.
Rather, itconsists of a few indirect and partial measures of thequality of PG systems.3 Task definitionThe first step in defining a paraphrase evaluationmetric is to define a good paraphrase.
Merriam-Webster dictionary gives the following definition:a restatement of a text, passage, or work givingthe meaning in another form.
We identify two keypoints in this definition: (1) that the meaning is pre-served, and (2) that the lexical form is different.
Towhich we add a third, that the paraphrase must befluent.The first and last point are similar to MT evalua-tion, where adequacy and fluency have been estab-lished as the standard criteria.
In paraphrase evalu-ation, we have one more: lexical dissimilarity.
Al-though lexical dissimilarity is seemingly the easiestto judge automatically among the three, it poses aninteresting challenge to automatic evaluation met-rics, as overlap with the reference has been the basisof almost all evaluation metrics.
That is, while MTevaluation and paraphrase evaluation are conceptu-ally closely related, the latter actually highlights thedeficiencies of the former, namely that in most au-tomatic evaluations, semantic equivalence is under-represented and substituted by lexical and syntactic924equivalence.The task of paraphrase evaluation is then definedas follows: Given an original sentence R and a para-phrase candidate P , output a numeric score b esti-mating the quality of P as a paraphrase ofR by con-sidering adequacy, fluency, and lexical dissimilarity.In this study, we use a scale of 1 to 5 (inclusive) forb, although that can be transformed linearly into anyrange desired.We observe here that the overall assessment b isnot a linear combination of the three measures.
Inparticular, a high dissimilarity score is meaninglessby itself.
It could simply be that the paraphrase isunrelated to the source sentence, or is incoherent.However, when accompanied by high adequacy andfluency scores, it differentiates the mediocre para-phrases from the good ones.4 Paraphrase Evaluation Metric (PEM)In this section we devise our metric according to thethree proposed evaluation criteria, namely adequacy,fluency, and dissimilarity.
The main challenge is tomeasure the adequacy, or semantic similarity, com-pletely independent of any lexical similarity.
We ad-dress this problem in Sections 4.1 to 4.3.
The re-maining two criteria are addressed in Section 4.4,and we describe the final combined metric PEM inSection 4.5.4.1 Phrase-level semantic representationWithout loss of generality, suppose we are to eval-uate English paraphrases, and have been suppliedmany sentence-aligned parallel texts of French andEnglish as an additional resource.
We can then alignthe parallel texts at word level automatically usingwell-known algorithms such as GIZA++ (Och andNey, 2003) or the Berkeley aligner (Liang et al,2006; Haghighi et al, 2009).To measure adequacy without relying on lexicalsimilarity, we make the key observation that thealigned French texts can act as a proxy of the se-mantics to a fragment of an English text.
If two En-glish phrases are often mapped to the same Frenchphrase, they can be considered similar in mean-ing.
Similar observations have been made by previ-ous researchers (Wu and Zhou, 2003; Bannard andCallison-Burch, 2005; Callison-Burch et al, 2006;Snover et al, 2009).
We can treat the distributionof aligned French phrases as a semantic representa-tion of the English phrase.
The semantic distancebetween two English phrases can then be measuredby their degree of overlap in this representation.In this work, we use the widely-used phrase ex-traction heuristic in (Koehn et al, 2003) to extractphrase pairs from parallel texts into a phrase table1.The phrases extracted do not necessarily correspondto the speakers?
intuition.
Rather, they are unitswhose boundaries are preserved during translation.However, the distinction does not affect our work.4.2 Segmenting a sentence into phrasesHaving established a way to measure the similarityof two English phrases, we now extend the conceptto sentences.
Here we discuss how to segment anEnglish sentence (the original or the paraphrase) intophrases.From the phrase table, we know the frequencies ofall the phrases and we approximate the probabilityof a phrase p by:Pr(p) =N(p)?p?
N(p?)(1)N(?)
is the count of a phrase in the phrase table, andthe denominator is a constant for all p. We definethe likelihood of segmenting a sentence S into a se-quence of phrases (p1, p2, .
.
.
, pn) by:Pr(p1, p2, .
.
.
, pn|S) =1Z(S)n?i=1Pr(pi) (2)where Z(S) is a normalizing constant.
The best seg-mentation of S according to Equation 2 can be cal-culated efficiently using a dynamic programming al-gorithm.
Note that Z(S) does not need to be calcu-lated, as it is the same for all different segmentationsof S. The formula has a strong preference for longerphrases, since every Pr(pi) has a large denominator.Many sentences are impossible to segment intoknown phrases, including all those containing out-of-vocabulary words.
We therefore allow any sin-gle word w to be considered as a phrase, and ifN(w) = 0, we use N(w) = 0.5 instead.1The same heuristic is used in the popular MT packageMoses.925Bonjour , / 0.9Salut , / 0.1Querrien / 1.0 .
/ 1.0Figure 1: A confusion network in the pivot languageBonjour , /on0.9ur S a lStFigure 2: A degenerated confusion network in the pivotlanguage4.3 Sentence-level semantic representationSimply merging the phrase-level semantic represen-tations is insufficient to produce a sensible sentence-level semantic representation.
For example, assumethe English sentence Morning , sir .
is segmented asa single phrase, because the following phrase pair isfound in the phrase table:En: Morning , sir .Fr: Bonjour , monsieur .However, another English sentence Hello , Quer-rien .
has an out-of-vocabulary word Querrienand consequently the most probable segmentation isfound to be ?Hello , ||| Querrien ||| .?
:En: Hello ,Fr: Bonjour , (Pr(Bonjour ,|Hello ,) = 0.9)Fr: Salut , (Pr(Salut ,|Hello ,) = 0.1)En: QuerrienFr: QuerrienEn: .Fr: .A naive comparison of the bags of French phrasesaligned to Morning , sir .
and Hello , Querrien .
de-picted above would conclude that the two sentencesare completely unrelated, as their bags of alignedFrench phrases are completely disjoint.
We tacklethis problem by constructing a confusion networkrepresentation of the French phrases, as shown inFigures 1 and 2.
The confusion network is formedby first joining the different French translations ofevery English phrase in parallel, and then joiningthese segments in series.The confusion network is a compact representa-tion of an exponentially large number of (likely mal-formed) weighted French sentences.
We can easilyenumerate the N-grams from the confusion networkrepresentation and collect the statistics for this en-semble of French sentences efficiently.
In this work,we consider N up to 4.
The N-grams for Hello ,Querrien .
are:1-grams: Bonjour (0.9), Salut (0.1), comma(1.0), Querrien (1.0), period (1.0).2-grams: Bonjour comma (0.9), Salut comma(0.1), comma Querrien (1.0), Querrien period (1.0).3-grams: Bonjour comma Querrien (0.9), Salutcomma Querrien (0.1), comma Querrien period(1.0).4-grams: Bonjour comma Querrien period (0.9),Salut comma Querrien period (0.1).We call this representation of an English sentencea bag of pivot language N-grams (BPNG), whereFrench is the pivot language in our illustrating ex-ample.
We can extract the BPNG of Morning , sir .analogously:1-grams: Bonjour (1.0), comma (1.0), monsieur(1.0), period (1.0).2-grams: Bonjour comma (1.0), comma mon-sieur (1.0), monsieur period (1.0).3-grams: Bonjour comma monsieur (1.0),comma monsieur period (1.0).4-grams: Bonjour comma monsieur period (1.0).The BPNG of Hello , Querrien.
can now be com-pared sensibly with that of the sentence Morning ,sir .
We use the F1 agreement between the two BP-NGs as a measure of the semantic similarity.
The F1agreement is defined asF1 =2 ?
Precision ?
RecallPrecision + RecallThe precision and the recall for an original sen-tence R and a paraphrase P is defined as follows.Let French N-gram g ?
BPNG(R)?BPNG(P ), andWR(g) and WP (g) be the weights of g in the BPNGof R and P respectively, thenPrecision =?g min(WR(g),WP (g))?g WP (g)Recall =?g min(WR(g),WP (g))?g WR(g)In our example, the numerators for both the preci-sion and the recall are 0.9 + 1 + 1 + 0.9, for the N-grams Bonjour, comma, period, and Bonjour comma926respectively.
The denominators for both terms are10.0.
Consequently, F1 = Precision = Recall =0.38, and we conclude that the two sentences are38% similar.
We call the resulting metric the pivotlanguage F1.
Note that since F1 is symmetric withrespect to the precision and the recall, our metric isunaffected whether we consider Morning, sir.
as theparaphrase of Hello, Querrien .
or the other wayround.An actual example from our corpus is:Reference sihanouk ||| put forth ||| this proposal |||in ||| a statement ||| made ||| yesterday ||| .Paraphrase shihanuk ||| put forward ||| this pro-posal ||| in his ||| yesterday ||| ?s statement ||| .The ||| sign denotes phrase segmentation as de-scribed earlier.
Our semantic representation suc-cessfully recognizes that put forth and put forwardare paraphrases of each other, based on their similarChinese translation statistics (ti2 chu1 in Chinese).4.4 Fluency and dissimilarityWe measure the fluency of a paraphrase by a nor-malized language model score Pn, defined byPn =logPr(S)length(S)where Pr(S) is the sentence probability predictedby a standard 4-gram language model.We measure dissimilarity between two Englishsentences using the target language F1, where wecollect the bag of all N-grams up to 4-grams fromeach English (referred to as the target language) sen-tence.
The target language F1 is then defined as theF1 agreement of the two bags of N-grams, analogousto the definition of the pivot language F1.
The targetlanguage F1 correlates positively with the similar-ity of the two sentences, or equivalently, negativelywith the dissimilarity of the two sentences.4.5 The metricTo produce the final PEM metric, we combine thethree component automatic metrics, pivot languageF1, normalized language model, and target languageF1, which measure adequacy, fluency, and dissimi-larity respectively.As discussed previously, a linear combination ofthe three component metrics is insufficient.
We turnto support vector machine (SVM) regression withthe radial basis function (RBF) kernel.
The RBF isa simple and expressive function, commonly used tointroduce non-linearity into large margin classifica-tions and regressions.RBF(xi, xj) = e??
?xi?xj?2We use the implementation in SVM light(Joachims, 1999).
The SVM is to be trained on a setof human-judged paraphrase pairs, where the threecomponent automatic metrics are fit to the humanoverall assessment.
After training, the model canthen be used to evaluate new paraphrase pairs in afully automatic fashion.5 Human evaluationTo validate our definition of paraphrase evaluationand the PEM method, we conduct an experimentto evaluate paraphrase qualities manually, which al-lows us to judge whether paraphrase evaluation ac-cording to our definition is an inherently coherentand well-defined problem.
The evaluation also al-lows us to establish an upper bound for the para-phrase evaluation task, and to validate the contribu-tion of the three proposed criteria to the overall para-phrase score.5.1 Evaluation setupWe use the Multiple-Translation Chinese Corpus(MTC)2 as a source of paraphrases.
The MTCcorpus consists of Chinese news articles (993 sen-tences in total) and multiple sentence-aligned En-glish translations.
We select one human transla-tion as the original text.
Two other human transla-tions and two automatic machine translations serveas paraphrases of the original sentences.
We refer tothe two human translations and the two MT systemsas paraphrase systems human1, human2, machine1,and machine2.We employ three human judges to manually as-sess the quality of 300 original sentences pairedwith each of the four paraphrases.
Therefore, eachjudge assesses 1,200 paraphrase pairs in total.
The2LDC Catalog No.
: LDC2002T01927judgment for each paraphrase pair consists of fourscores, each given on a five-point scale:?
Adequacy (Is the meaning preserved ade-quately?)?
Fluency (Is the paraphrase fluent English?)?
Lexical Dissimilarity (How much has the para-phrase changed the original sentence?)?
Overall scoreThe instructions given to the judges for the overallscore were as follows.A good paraphrase should convey thesame meaning as the original sentence,while being as different as possible on thesurface form and being fluent and gram-matical English.
With respect to this defi-nition, give an overall score from 5 (per-fect) to 1 (unacceptable) for this para-phrase.The paraphrases are presented to the judges in a ran-dom order and without any information as to whichparaphrase system produced the paraphrase.In addition to the four paraphrase systems men-tioned above, for each original English sentence, weadd three more artificially constructed paraphraseswith pre-determined ?human?
judgment scores: (1)the original sentence itself, with adequacy 5, fluency5, dissimilarity 1, and overall score 2; (2) a randomsentence drawn from the same domain, with ade-quacy 1, fluency 5, dissimilarity 5, and overall score1; and (3) a random sentence generated by a uni-gram language model, with adequacy 1, fluency 1,dissimilarity 5, and overall score 1.
These artificialparaphrases serve as controls in our evaluation.
Ourfinal data set therefore consists of 2,100 paraphrasepairs with judgments on 4 different criteria.5.2 Inter-judge correlationThe first step in our evaluation is to investigate thecorrelation between the human judges.
We use Pear-son?s correlation coefficient, a common measure ofthe linear dependence between two random vari-ables.We investigate inter-judge correlation at the sen-tence and at the system level.
At the sentencelevel, we construct three vectors, each containingthe 1,200 sentence level judgments from one judgeSentence Level System LevelJudge A Judge B Judge A Judge BJudge B 0.6406 - 0.9962 -Judge C 0.6717 0.5993 0.9995 0.9943Table 1: Inter-judge correlation for overall paraphrasescoreSentence Level System LevelAdequacy 0.7635 0.7616Fluency 0.3736 0.3351Dissimilarity -0.3737 -0.3937Dissimilarity (A,F?4) 0.8881 0.9956Table 2: Correlation of paraphrase criteria with overallscorefor the overall score.
The pair-wise correlations be-tween these three vectors are then taken.
Note thatwe exclude the three artificial control paraphrasesystems from consideration, as that would inflate thecorrelation.
At the system level, we construct threevectors each of size four, containing the averagescores given by one judge to each of the four para-phrase systems human1, human2, machine1, andmachine2.
The correlations are then taken in thesame fashion.The results are listed in Table 1.
The inter-judgecorrelation is between 0.60 and 0.67 at the sentencelevel and above 0.99 at the system level.
These cor-relation scores can be considered very high whencompared to similar results reported in MT evalu-ations, e.g., Blatz et al (2003).
The high correlationconfirms that our evaluation task is well defined.Having confirmed that human judgments corre-late strongly, we combine the scores of the threejudges by taking their arithmetic mean.
Togetherwith the three artificial control paraphrase systems,they form the human reference evaluation which weuse for the remainder of the experiments.5.3 Adequacy, fluency, and dissimilarityIn this section, we empirically validate the impor-tance of our three proposed criteria: adequacy, flu-ency, and lexical dissimilarity.
This can be done bymeasuring the correlation of each criterion with theoverall score.
The system and sentence level corre-lations are shown in Table 2.We can see a positive correlation of adequacy and928Figure 3: Scatter plot of dissimilarity vs. overall scorefor paraphrases with high adequacy and fluency.fluency with the overall score, and the correlationwith adequacy is particularly strong.
Thus, higheradequacy and to a lesser degree higher fluency indi-cate higher paraphrase quality to the human judges.On the other hand, dissimilarity is found to have anegative correlation with the overall score.
This canbe explained by the fact that the two human trans-lations usually have much higher similarity with thereference translation, and at the same time are scoredas better paraphrases.
This effect dominates a sim-ple linear fitting of the paraphrase score vs. the dis-similarity, resulting in the counter intuitive negativecorrelation.
We note that a high dissimilarity alonetells us little about the quality of the paraphrase.Rather, we expect dissimilarity to be a differentia-tor between the mediocre and good paraphrases.To test this hypothesis, we select the subset ofparaphrase pairs that receive adequacy and fluencyscores of at least four and again measure the cor-relation of the dissimilarity and the overall score.The result is tabulated in the last row of Table 2 andshows a strong correlation.
Figure 3 shows a scatterplot of the same result3.The empirical results presented so far confirm thatparaphrase evaluation is a well-defined task permit-ting consistent subjective judgments, and that ade-quacy, fluency, and dissimilarity are suitable criteriafor paraphrase quality.3We automatically add jitter (small amounts of noise) forease of presentation.6 PEM vs. human evaluationIn the last section, we have shown that the threeproposed criteria are good indicators of paraphrasequality.
In this section, we investigate how wellPEM can predict the overall paraphrase quality fromthe three automatic metrics (pivot language F1, nor-malized language model, and target language F1),designed to match the three evaluation criteria.
Wedescribe the experimental setup in Section 6.1, be-fore we show the results in Section 6.2.6.1 Experimental setupWe build the phrase table used to evaluate the pivotlanguage F1 from the FBIS Chinese-English corpus,consisting of about 250,000 Chinese sentences, eachwith a single English translation.
The paraphrasesare taken from the MTC corpus in the same wayas the human experiment described in Section 5.1.Both FBIS and MTC are in the Chinese newswiredomain.We stem all English words in both data sets withthe Porter stemmer (Porter, 1980).
We use the maxi-mum entropy segmenter of (Low et al, 2005) to seg-ment the Chinese part of the FBIS corpus.
Subse-quently, word level Chinese-English alignments aregenerated using the Berkeley aligner (Liang et al,2006; Haghighi et al, 2009) with five iterations oftraining.
Phrases are then extracted with the widely-used heuristic in Koehn et al (2003).
We extractphrases of up to four words in length.Bags of Chinese pivot language N-grams are ex-tracted for all paraphrase pairs as described in Sec-tion 4.3.
For computational efficiency, we consideronly edges of the confusion network with probabil-ities higher than 0.1, and only N-grams with proba-bilities higher than 0.01 in the bag of N-grams.
Wecollect N-grams up to length four.The language model used to judge fluency istrained on the English side of the FBIS parallel text.We use SRILM (Stolcke, 2002) to build a 4-grammodel with the default parameters.The PEM SVM regression is trained on the para-phrase pairs for the first 200 original English sen-tences and tested on the paraphrase pairs of the re-maining 100 original English sentences.
Thus, thereare 1,400 instances for training and 700 instances fortesting.
For each instance, we calculate the values929Figure 4: Scatter plot of PEM vs. human judgment (over-all score) at the sentence levelFigure 5: Scatter plot of PEM vs. human judgment (over-all score) at the system levelof pivot language F1, normalized language modelscore, and target language F1.
These values serveas the input features to the SVM regression and thetarget value is the human assessment of the overallscore, on a scale of 1 to 5.6.2 ResultsAs in the human evaluation, we investigate the cor-relation of the PEM scores with the human judg-ments at the sentence and at the system level.
Fig-ure 4 shows the sentence level PEM scores plottedagainst the human overall scores, where each humanoverall score is the arithmetic mean of the scoresgiven by the three judges.
The Pearson correlationbetween the automatic PEM scores and the humanjudgments is 0.8073.
This is substantially higherthan the sentence level correlation of MT metricsSentence Level System LevelPEM vs. Human Avg.
0.8073 0.9867PEM vs. Judge A 0.5777 0.9757PEM vs. Judge B 0.5281 0.9892PEM vs. Judge C 0.5231 0.9718Table 3: Correlation of PEMwith human judgment (over-all score)like BLEU.
For example, the highest sentence levelPearson correlation by any metric in the Metrics-MATR 2008 competition (Przybocki et al, 2009)was 0.6855 by METEOR-v0.6; BLEU achieved acorrelation of 0.4513.Figure 5 shows the system level PEM scores plot-ted against the human scores.
The Pearson correla-tion between PEM scores and the human scores atthe system level is 0.9867.We also calculate the Pearson correlation betweenPEM and each individual human judge.
Here, weexclude the three artificial control paraphrase sys-tems from the data, to make the results compara-ble to the inter-judge correlation presented in Sec-tion 5.2.
The correlation is between 0.52 and 0.57at the sentence level and between 0.97 and 0.98 atthe system level.
As we would expect, the correla-tion between PEM and a human judge is not as highas the correlation between two human judges, butPEM still shows a strong and consistent correlationwith all three judges.
The results are summarized inTable 3.7 Discussion and future workThe paraphrases that we use in this study are notactual machine generated paraphrases.
Instead, theEnglish paraphrases are multiple translations of thesame Chinese source sentence.
Our seven ?para-phrase systems?
are two human translators, two ma-chine translation systems, and three artificially cre-ated extreme scenarios.
The reason for using multi-ple translations is that we could not find any PG sys-tem that can paraphrase a whole input sentence andis publicly available.
We intend to obtain and evalu-ate paraphrases generated from real PG systems andcompare their performances in a follow-up study.Our method models paraphrasing up to the phraselevel.
Unfortunately, it makes no provisions for syn-930tactic paraphrasing at the sentence level, which isprobably a much greater challenge, and the literatureoffers few successes to draw inspirations from.
Wehope to be able to partially address this deficiency infuture work.The only external linguistic resource required byPEM is a parallel text of the target language andanother arbitrary language.
While we only useChinese-English parallel text in this study, other lan-guage pairs need to be explored too.
Another alter-native is to collect parallel texts against multiple for-eign languages, e.g., using Europarl (Koehn, 2005).We leave this for future work.Our evaluation method does not require human-generated references like in MT evaluation.
There-fore, we can easily formulate a paraphrase genera-tor by directly optimizing the PEM metric, althoughsolving it is not trivial:paraphrase(R) = argmaxPPEM(P,R)where R is the original sentence and P is the para-phrase.Finally, the PEM metric, in particular the seman-tic representation BPNG, can be useful in manyother contexts, such as MT evaluation, summaryevaluation, and paraphrase recognition.
To facil-itate future research, we will package and releasePEM under an open source license at http://nlp.comp.nus.edu.sg/software.8 ConclusionWe proposed PEM, a novel automatic metric forparaphrase evaluation based on adequacy, fluency,and lexical dissimilarity.
The key component in ourmetric is a novel technique to measure the seman-tic similarity of two sentences through their N-gramoverlap in an aligned foreign language text.
Weconducted an extensive human evaluation of para-phrase quality which shows that our proposed met-ric achieves high correlation with human judgments.To the best of our knowledge, PEM is the first auto-matic metric for paraphrase evaluation.AcknowledgmentsThis research was done for CSIDM Project No.CSIDM-200804 partially funded by a grant fromthe National Research Foundation (NRF) adminis-tered by the Media Development Authority (MDA)of Singapore.ReferencesC.
Bannard and C. Callison-Burch.
2005.
Paraphrasingwith bilingual parallel corpora.
In Proc.
of ACL.R.
Barzilay and L. Lee.
2003.
Learning to paraphrase:An unsupervised approach using multiple-sequencealignment.
In Proc.
of HLT-NAACL.J.
Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,A.
Kulesza, A. Sanchis, and N. Ueffing.
2003.
Con-fidence estimation for machine translation.
Technicalreport, CLSP Workshop Johns Hopkins University.C.
Callison-Burch, P. Koehn, and M. Osborne.
2006.Improved statistical machine translation using para-phrases.
In Proc.
of HLT-NAACL.C.
Callison-Burch, T. Cohn, and M. Lapata.
2008.
Para-Metric: An automatic evaluation metric for paraphras-ing.
In Proc.
of COLING.C.
Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.2009.
Findings of the 2009 Workshop on StatisticalMachine Translation.
In Proceedings of WMT.Y.S.
Chan and H.T.
Ng.
2008.
MAXSIM: A maximumsimilarity metric for machine translation evaluation.In Proc.
of ACL-08: HLT.D.
Das and N.A.
Smith.
2009.
Paraphrase identifica-tion as probabilistic quasi-synchronous recognition.
InProc.
of ACL-IJCNLP.P.
Duboue and J. Chu-Carroll.
2006.
Answering thequestion you wish they had asked: The impact of para-phrasing for question answering.
In Proc.
of HLT-NAACL Companion Volume: Short Papers.C.
Fellbaum, editor.
1998.
WordNet: An electronic lexi-cal database.
MIT Press, Cambridge, MA.A.
Haghighi, J. Blitzer, J. DeNero, and D. Klein.
2009.Better word alignments with supervised ITG models.In Proc.
of ACL-IJCNLP.M.
Heilman and N.A.
Smith.
2010.
Tree edit modelsfor recognizing textual entailments, paraphrases, andanswers to questions.
In Proc.
of NAACL.E.
Hovy, C.Y.
Lin, L. Zhou, and J. Fukumoto.
2006.Automated summarization evaluation with basic ele-ments.
In Proc.
of LREC.T.
Joachims.
1999.
Making large-scale SVM learningpractical.
In B. Sch?lkopf, C. Burges, and A. Smola,editors, Advances in Kernel Methods - Support VectorLearning.
MIT Press.D.
Kauchak and R. Barzilay.
2006.
Paraphrasing forautomatic evaluation.
In Proc.
of HLT-NAACL.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of HLT-NAACL.931P.
Koehn.
2005.
Europarl: A parallel corpus for statisti-cal machine translation.
In MT Summit, volume 5.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In Proc.
of HLT-NAACL.C.Y.
Lin.
2004.
ROUGE: A package for automatic eval-uation of summaries.
In Proc.
of the ACL-04 Work-shop on Text Summarization Branches Out.C.
Liu, D. Dahlmeier, and H.T.
Ng.
2010.
TESLA:translation evaluation of sentences with linear-programming-based analysis.
In Proc.
of WMT.J.K.
Low, H.T.
Ng, and W. Guo.
2005.
A maximumentropy approach to Chinese word segmentation.
InProc.
of the 4th SIGHAN Workshop.N.
Madnani, N.F.
Ayan, P. Resnik, and B.J.
Dorr.
2007.Using paraphrases for parameter tuning in statisticalmachine translation.
In Proc.
of WMT.N.
Madnani, P. Resnik, B.J.
Dorr, and R. Schwartz.
2008.Are multiple reference translations necessary?
Investi-gating the value of paraphrased reference translationsin parameter optimization.
In Proc.
of AMTA.F.J.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1).K.
Owczarzak, D. Groves, J.
Van Genabith, and A. Way.2006.
Contextual bitext-derived paraphrases in auto-matic MT evaluation.
In Proc.
of WMT.B.
Pang, K. Knight, and D. Marcu.
2003.
Syntax-basedalignment of multiple translations: Extracting para-phrases and generating new sentences.
In Proc.
ofHLT-NAACL.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proc.
of ACL.M.
Porter.
1980.
An algorithm for suffix stripping.
Pro-gram, 40(3).M.
Przybocki, K. Peterson, S. Bronsart, and G Sanders.2009.
Evaluating machine translation with LFG de-pendencies.
Machine Translation, 23(2).L.
Qiu, M.Y.
Kan, and T.S.
Chua.
2006.
Paraphraserecognition via dissimilarity significance classifica-tion.
In Proc.
of EMNLP.C.
Quirk, C. Brockett, and W. Dolan.
2004.
Monolin-gual machine translation for paraphrase generation.
InProc.
of EMNLP.M.
Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009.Fluency, adequacy, or HTER?
Exploring different hu-man judgments with a tunable MT metric.
In Proc.
ofWMT.A.
Stolcke.
2002.
SRILM - an extensible language mod-eling toolkit.
In Proc.
of ICSLP.S.
Wan, M. Dras, R. Dale, and C Paris.
2006.
Usingdependency-based features to take the ?para-farce?
outof paraphrase.
In Proc.
of ALTW 2006.H.
Wu and M. Zhou.
2003.
Synonymous collocationextraction using translation information.
In Proc.
ofACL.S.Q.
Zhao, C. Niu, M. Zhou, T. Liu, and S. Li.
2008.Combining multiple resources to improve SMT-basedparaphrasing model.
In Proc.
of ACL-08: HLT.S.Q.
Zhao, X. Lan, T. Liu, and S. Li.
2009.
Application-driven statistical paraphrase generation.
In Proc.
ofACL-IJCNLP.L.
Zhou, C.Y.
Lin, and E. Hovy.
2006a.
Re-evaluatingmachine translation results with paraphrase support.In Proc.
of EMNLP.L.
Zhou, C.Y.
Lin, D.S.
Munteanu, and E. Hovy.
2006b.ParaEval: Using paraphrases to evaluate summariesautomatically.
In Proc.
of HLT-NAACL.932
