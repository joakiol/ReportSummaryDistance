Large linguistically-processed Web corpora for multiple languagesMarco BaroniSSLMITUniversity of BolognaItalybaroni@sslmit.unibo.itAdam KilgarriffLexical Computing Ltd. andUniversity of SussexBrighton, UKadam@lexmasterclass.comAbstractThe Web contains vast amounts of linguis-tic data.
One key issue for linguists andlanguage technologists is how to accessit.
Commercial search engines give highlycompromised access.
An alternative is tocrawl the Web ourselves, which also al-lows us to remove duplicates and near-duplicates, navigational material, and arange of other kinds of non-linguistic mat-ter.
We can also tokenize, lemmatise andpart-of-speech tag the corpus, and load thedata into a corpus query tool which sup-ports sophisticated linguistic queries.
Wehave now done this for German and Ital-ian, with corpus sizes of over 1 billionwords in each case.
We provide Web ac-cess to the corpora in our query tool, theSketch Engine.1 IntroductionThe Web contains vast amounts of linguistic datafor many languages (Kilgarriff and Grefenstette,2003).
One key issue for linguists and languagetechnologists is how to access it.
The drawbacksof using commercial search engines are presentedin Kilgarriff (2003).
An alternative is to crawl theWeb ourselves.1 We have done this for two lan-guages, German and Italian, and here we report onthe pipeline of processes which give us reasonablywell-behaved, ?clean?
corpora for each language.1Another Web access option is Alexa (http://pages.alexa.com/company/index.html), who allow theuser (for a modest fee) to access their cached Web directly.Using Alexa would mean one did not need to crawl; howeverin our experience, crawling, given free software like Heritrix,is not the bottleneck.
The point at which input is required isthe filtering out of non-linguistic material.We use the German corpus (which was developedfirst) as our example throughout.
The procedurewas carried on a server running RH Fedora Core 3with 4 GB RAM, Dual Xeon 4.3 GHz CPUs andabout 2.5 TB hard disk space.
We are making thetools we develop as part of the project freely avail-able,2 in the hope of stimulating public sharing ofresources and know-how.2 Crawl seeding and crawlingWe would like a ?balanced?
resource, containinga range of types of text corresponding, to somedegree, to the mix of texts we find in designed lin-guistic corpora (Atkins et al, 1992), though alsoincluding text types found on the Web which werenot anticipated in linguists?
corpus design discus-sions.
We do not want a ?blind?
sample dominatedby product listings, catalogues and computer sci-entists?
bulletin boards.
Our pragmatic solution isto query Google through its API service for ran-dom pairs of randomly selected content words inthe target language.
In preliminary experimenta-tion, we found that single word queries yieldedmany inappropriate pages (dictionary definitionsof the word, top pages of companies with the wordin their name), whereas combining more than twowords retrieved pages with lists of words, ratherthan collected text.Ueyama (2006) showed how queries for wordssampled from traditional written sources such asnewspaper text and published essays tend to yield?public sphere?
pages (online newspaper, govern-ment and academic sites), whereas basic vocabu-lary/everyday life words tend to yield ?personal?pages (blogs, bulletin boards).
Since we wantedboth types, we obtained seed URLs with queries2http://sslmitdev-online.sslmit.unibo.it/wac/wac.php87for words from both kinds of sources.
For Ger-man, we sampled 2000 mid-frequency words froma corpus of the Su?ddeutsche Zeitung newspaperand paired them randomly.
Then, we found a ba-sic vocabulary list for German learners,3 removedfunction words and particles and built 653 randompairs.
We queried Google via its API retrievingmaximally 10 pages for each pair.
We then col-lapsed the URL list, insuring maximal sparsenessby keeping only one (randomly selected) URL foreach domain, leaving a list of 8626 seed URLs.They were fed to the crawler.The crawls are performed using the Her-itrix crawler,4 with a multi-threaded breadth-firstcrawling strategy.
The crawl is limited to pageswhose URL does not end in one of several suffixesthat cue non-html data (.pdf, .jpeg, etc.
)5 ForGerman, the crawl is limited to sites from the .deand .at domains.
Heritrix default crawling op-tions are not modified in any other respect.
Welet the German crawl run for ten days, retrievinggzipped archives (the Heritrix output format) ofabout 85GB.3 FilteringWe undertake some post-processing on the ba-sis of the Heritrix logs.
We identify documentsof mime type text/html and size between 5and 200KB.
As observed by Fletcher (2004) verysmall documents tend to contain little genuine text(5KB counts as ?very small?
because of the htmlcode overhead) and very large documents tend tobe lists of various sorts, such as library indices,store catalogues, etc.
The logs also contain sha-1 fingerprints, allowing us to identify perfect du-plicates.
After inspecting some of the duplicateddocuments (about 50 pairs), we decided for a dras-tic policy: if a document has at least one dupli-cate, we discard not only the duplicate(s) but alsothe document itself.
We observed that, typically,such documents came from the same site and werewarning messages, copyright statements and sim-ilar, of limited or no linguistic interest.
While thestrategy may lose some content, one of our gen-eral principles is that, given how vast the Web is,we can afford to privilege precision over recall.All the documents that passed the pre-filtering3http://mypage.bluewin.ch/a-z/cusipage/4http://crawler.archive.org5Further work should evaluate pros and cons of retrievingdocuments in other formats, e.g., Adobe pdf.stage are run through a perl program that performs1) boilerplate stripping 2) function word filtering3) porn filtering.Boilerplate strippingBy ?boilerplate?
we mean all those componentsof Web pages which are the same across manypages.
We include stripping out HTML markup,javascript and other non-linguistic material in thisphase.
We aimed to identify and remove sectionsof a document that contain link lists, navigationalinformation, fixed notices, and other sections poorin human-produced connected text.
For purposesof corpus construction, boilerplate removal is crit-ical as it will distort statistics collected from thecorpus.6 We adopted the heuristic used in the Hyp-pia project BTE tool,7: content-rich sections of apage will have a low html tag density, whereasboilerplate is accompanied by a wealth of html(because of special formatting, newlines, links,etc.)
The method is based on general propertiesof Web documents, so is relatively independent oflanguage and crawling strategy.Function word and pornography filteringConnected text in sentences reliably contains ahigh proportion of function words (Baroni, to ap-pear), so, if a page does not meet this criterionwe reject it.
The German function word list con-tains 124 terms.
We require that a minimum of 10types and 30 tokens appear in a page, with a ra-tio of function words to total words of at least onequarter.
The filter also works as a simple languageidentifier.8Finally, we use a stop list of words likely to oc-cur in pornographic Web pages, not out of prudery,but because they tend to contain randomly gener-ated text, long keyword lists and other linguisti-cally problematic elements.
We filter out docu-ments that have at least three types or ten tokensfrom a list of words highly used in pornography.The list was derived from the analysis of porno-graphic pages harvested in a previous crawl.
Thisis not entirely satisfactory, since some of the words6We note that this phase currently removes the links fromthe text, so we can no longer explore the graph structure ofthe dataset.
In future we may retain link structure, to supportresearch into the relation between it and linguistic character-istics.7http://www.smi.ucd.ie/hyppia/8Of course, these simple methods will not filter out allmachine-generated text (typically produced as part of searchengine ranking scams or for other shady purposes); some-times this appears to have been generated with a bigram lan-guage model, and thus identifying it with automated tech-niques is far from trivial.88in the list, taken in isolation, are wholly innocent(fat, girls, tongue, etc.)
We shall revisit the strat-egy in due course.This filtering took 5 days and resulted in a ver-sion of the corpus containing 4.86M documentsfor a total of 20GB of uncompressed data.4 Near-duplicate detectionWe use a simplified version of the ?shingling?
al-gorithm (Broder et al, 1997).
For each document,after removing all function words, we take finger-prints of a fixed number s of randomly selected n-grams; then, for each pair of documents, we countthe number of shared n-grams, which can be seenas an unbiased estimate of the overlap between thetwo documents (Broder et al, 1997; Chakrabarti,2002).
We look for pairs of documents sharingmore than t n-grams, and we discard one of thetwo.After preliminary experimentation, we chose toextract 25 5-grams from each document, and totreat as near-duplicates documents that shared atleast two of these 5-grams.
Near-duplicate spot-ting on the German corpus took about 4 days.2,466,271 near-duplicates were removed.
The cor-pus size decreased to 13GB.
Most of the process-ing time was spent in extracting the n-grams andadding the corresponding fingerprints to the data-base (which could be parallelized).5 Part-of-speech tagging/lemmatizationand post-annotation cleaningWe performed German part-of-speech tagging andlemmatization with TreeTagger.9 Annotation took5 days.
The resulting corpus contains 2.13Bwords, or 34GB of data including annotation.After inspecting various documents from theannotated corpus, we decided to perform a furtherround of cleaning.
There are two reasons for this:first, we can exploit the annotation to find otheranomalous documents, through observing wherethe distribution of parts-of-speech tags is very un-usual and thus not likely to contain connected text.Second, the TreeTagger was not trained on Webdata, and thus its performance on texts that areheavy on Web-like usage (e.g., texts all in lower-case, colloquial forms of inflected verbs, etc.)
isdismal.
While a better solution to this secondproblem would be to re-train the tagger on Web9http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTaggerdata (ultimately, the documents displaying the sec-ond problem might be among the most interest-ing ones to have in the corpus!
), for now we try toidentify the most problematic documents throughautomated criteria and discard them.
The cues weused included the number of words not recognisedby the lemmatizer; the proportion of words withupper-case initial letters; proportion of nouns, andproportion of sentence markers.After this further processing step, the corpuscontains 1,870,259 documents from 10818 differ-ent domains, and its final size is 1.71 billion to-kens (26GB of data, with annotation).
The finalsize of the Italian corpus is 1,875,337 documentsand about 1.9 billion tokens.6 Indexing and Web user interfaceWe believe that matters of efficient indexing anduser friendly interfacing will be crucial to the suc-cess of our initiative, both because many linguistswill lack the relevant technical skills to write theirown corpus-access routines, and because we shallnot publicly distribute the corpora for copyrightreasons; an advanced interface that allows lin-guists to do actual research on the corpus (includ-ing the possibility of saving settings and resultsacross sessions) will allow us to make the corpuswidely available while keeping it on our servers.10We are using the Sketch Engine,11 a corpus querytool which has been widely used in lexicographyand which supports queries combining regular ex-pressions and boolean operators over words, lem-mas and part-of-speech tags.7 Comparison with other corporaWe would like to compare the German Web cor-pus to an existing ?balanced?
corpus of Germanattempting to represent a broad range of genresand topics.
Unfortunately, as far as we know noresource of this sort is publicly available (whichis one of the reasons why we are interested in de-veloping the German Web corpus in the first in-stance.)
Instead, we use a corpus of newswirearticles from the Austria Presse Agentur (APA,kindly provided to us by ?OFAI) as our reference10The legal situation is of course complex.
We considerthat our case is equivalent to that of other search engines,and that offering linguistically-encoded snippets of pages toresearchers does not go beyond the ?fair use?
terms routinelyinvoked by search engine companies in relation to Web pagecaching.11http://www.sketchengine.co.uk/89WEB APAich hier APA NATOdass wir Schlu?
EUund man Prozent Fortssie nicht Mill AFPist das MRD Dollaroder sind Wien Reuterskann so Kosovo Dienstagdu mir DPA Mittwochwenn ein US Donnerstagwas da am seiTable 1: Typical Web and APA wordspoint.
This corpus contains 28M tokens, and,despite its uniformity in terms of genre and re-stricted thematic range, it has been successfullyemployed as a general-purpose German corpus inmany projects.
After basic regular-expression-based normalization and filtering, the APA con-tains about 500K word types, the Web corpusabout 7.4M.
There is a large overlap among the 30most frequent words in both corpora: 24 out of 30words are shared.
The non-overlapping words oc-curring in the Web top 30 only are function words:sie ?she?, ich ?I?, werden ?become/be?, oder ?or?,sind ?are?, er ?he?.
The words only in the APAlist show a bias towards newswire-specific vocab-ulary (APA, Prozent ?percent?, Schlu?
?closure?
)and temporal expressions that are also typical ofnewswires (am ?at?, um ?on the?, nach ?after?
).Of the 232,322 hapaxes (words occurring onlyonce) in the APA corpus, 170,328 (73%) occur inthe Web corpus as well.12 89% of these APA ha-paxes occur more than once in the Web corpus,suggesting how the Web data will help addressdata sparseness issues.Adopting the methodology of Sharoff (2006),we then extracted the 20 words most characteris-tics of the Web corpus vs. APA and vice versa,based on the log-likelihood ratio association mea-sure.
Results are presented in Table 1.
The APAcorpus has a strong bias towards newswire par-lance (acronyms and named entities, temporal ex-pressions, financial terms, toponyms), whereas theterms that come out as most typical of the Webcorpus are function words that are not stronglyconnected with any particular topic or genre.
Sev-eral of these top-ranked function words mark firstand second person forms (ich, du, wir, mir).This preliminary comparison both functioned asa ?sanity check?, showing that there is consider-12Less than 1% of the Web corpus hapaxes are attested inthe APA corpus.able overlap between our corpus and a smaller cor-pus used in previous research, and suggested thatthe Web corpus has more a higher proportion ofinterpersonal material.8 ConclusionWe have developed very large corpora from theWeb for German and Italian (with other languagesto follow).
We have filtered and cleaned the text sothat the obvious problems with using the Web as acorpus for linguistic research do not hold.
Prelim-inary evidence suggests the ?balance?
of our Ger-man corpus compares favourably with that of anewswire corpus (though of course any such claimbegs a number of open research questions aboutcorpus comparability).
We have lemmatised andpart-of-speech-tagged the data and loaded it intoa corpus query tool supporting sophisticated lin-guistic queries, and made it available to all.ReferencesB.
Atkins, J.
Clear, and N. Ostler.
1992.
Corpus designcriteria.
Literary and Linguistic Computing, 7:1?16.M.
Baroni.
to appear.
Distributions in text.
InA.
Lu?deling and M.
Kyto?, editors, Corpus lin-guistics: An international handbook.
Mouton deGruyter, Berlin.A.
Broder, S. Glassman, M. Manasse, and G. Zweig.1997.
Syntactic clustering of the Web.
In Proc.Sixth International World-Wide Web Conference.S.
Chakrabarti.
2002.
Mining the Web: Discoveringknowledge from hypertext data.
Morgan Kaufmann,San Francisco.W.
Fletcher.
2004.
Making the web more useful asa source for linguistic corpora.
In U. Connor andT.
Upton, editors, Corpus Linguistics in North Amer-ica 2002.A.
Kilgarriff and G. Grefenstette.
2003.
Introductionto the special issue on the Web as corpus.
Compu-tational Linguistics, 29(3):333?347.A.
Kilgarriff.
2003.
Linguistic search engine.
InK.
Simov, editor, Proc.
SPROLAC Workshop, Lan-caster.S.
Sharoff.
2006.
Creating general-purpose corporausing automated search engine queries.
In M. Ba-roni and S. Bernardini, editors, WaCky!
Working pa-pers on the Web as Corpus.
Gedit, Bologna.M.
Ueyama.
2006.
Creation of general-purposeJapanese Web corpora with different search enginequery strategies.
In M. Baroni and S. Bernardini,editors, WaCky!
Working papers on the Web as Cor-pus.
Gedit, Bologna.90
