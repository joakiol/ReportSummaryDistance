Improving Word Alignment by Adjusting Chinese Word SegmentationMing-Hong Bai1,2 Keh-Jiann Chen1 Jason S. Chang21 Institute of Information Science, Academia Sinica2 Department of Computer Science, National Tsing-Hua Universitymhbai@sinica.edu.tw kchen@iis.sinica.edu.tw jschang@cs.nthu.edu.twAbstractMost of the current Chinese wordalignment tasks often adopt wordsegmentation systems firstly to identifywords.
However, word-mismatchingproblems exist between languages and willdegrade the performance of wordalignment.
In this paper, we propose twounsupervised methods to adjust wordsegmentation to make the tokens 1-to-1mapping as many as possible between thecorresponding sentences.
The first methodis learning affix rules from a bilingualterminology bank.
The second method isusing the concept of impurity measuremotivated by the decision tree.
Ourexperiments showed that both of theadjusting methods improve theperformance of word alignmentsignificantly.1 IntroductionWord alignment is an important preprocessing taskfor statistical machine translation.
There have beenmany statistical word alignment methods proposedsince the IBM models have been introduced.
Mostexisting methods treat word tokens as basicalignment units (Brown et al, 1993; Vogel et al,1996; Deng and Byrne, 2005), however, manylanguages have no explicit word boundary markers,such as Chinese and Japanese.
In these languages,word segmentation (Chen and Liu, 1992; Chen andBai, 1998; Chen and Ma, 2002; Ma and Chen,2003; Gao et al, 2005) is often carried out firstlyto identify words before word alignment (Wu andXia, 1994).
However, the differences inlexicalization may degrade word alignmentperformance, for different languages may realizethe same concept using different numbers of words(Ma et al, 2007; Wu, 1997).
For instance, Chinesemulti-syllabic words composed of more than onemeaningful morpheme which may be translated toseveral English words.
For example, the Chineseword ???
is composed of two meaning units,??
and ?, and is translated to Department ofEducation in English.
The morphemes ??
and ?have their own meanings and are translated toEducation and Department respectively.
Thephenomenon of lexicalization mismatch willdegrade the performance of word alignment forseveral reasons.
The first reason is that it willreduce the cooccurrence counts of Chinese andEnglish tokens.
Consider the previous example.Since ???
is treated as a single unit, it does notcontribute to the occurrence counts of Education/??
and Department/?
token pairs.
Secondly, therarely occurring compound word may cause thegarbage collectors effect (Moore, 2004; Liang etal., 2006), aligning a rare word in source languageto too many words in the target language, due tothe frequency imbalance with the correspondingtranslation words in English (Lee, 2004).
Finally,the IBM models (Moore, 2004) impose thelimitation that each word in the target sentence canbe generated by at most one word in the sourcesentence.
In this case, a many-to-one alignment,links a phrase in the source sentence to a singletoken in the target sentence, is not allowed, forcingmost links of a phrase in the source sentence to beabolished.
As in the previous example, whenaligning from English to Chinese, ???
can onlybe linked to one of the English words, sayEducation, because of the limitation of the IBMmodel.
However for remedy, many of the currentword alignment methods combine the results ofboth alignment directions, via intersection or249grow-diag-final heuristic, to improve the alignmentreliability (Koehn et al, 2003; Liang et al, 2006;Ayan et al, 2006; DeNero et al, 2007).
Howeverthe many-to-one link limitation will undermine thereliability due to the fact that some links are notallowed in one of the directions.In this paper, we propose two novel methods toadjust word segmentation so as to decrease theeffect of lexicalization differences to improve wordalignment performance.
The main idea of ourmethods is to adjust Chinese word segmentationaccording to their translation derived from parallelsentences in order to make the tokens compatibleto 1-to-1 mapping between the correspondingsentences.
The first method is based on learning aset of affix rules from bilingual terminology bank,and adjusting the segmentation according to theseaffix rules when preprocessing the Chinese part ofthe parallel corpus.
The second method is based onthe so-called impurity measure, which wasmotivated by the decision tree (Duda et al, 2001).2 Related WorksOur methods are motivated by the translation-driven segmentation method proposed by Wu(1997) to segment words in a way to improve wordalignment.
However, Wu's method needs atranslation lexicon to filter out the links whichwere not in the lexicon and the result was onlyevaluated on the sentence pairs which werecovered by the lexicon.A word packing method has been proposed byMa et al (2007) to improve the word alignmenttask.
Before carrying out word alignment, thismethod packs several consecutive words togetherwhen those words believed to correspond to asingle word in the other language.
Our basic idea issimilar to this, but on the contrary, we try tounpack words which are translations of severalwords in the other language.
Since the wordpacking method treats the packed consecutivewords as a single token, as we mentioned in theprevious section, it weakens the associationstrength of translation pairs of their morphemeswhile applying the IBM word alignment model.A lot of morphological analysis methods havebeen proposed to improve the performance of wordalignment for inflectional language (Lee et al,2003; Lee, 2004; Goldwater, 2005).
They proposedto split a word into a morpheme sequence of thepattern prefix*-stem-suffix* (* denotes zero ormore occurrences of a morpheme).
Theirexperiments showed that morphological analysiscan improve the quality of machine translation byreducing data sparseness and by making the tokensin two languages correspond more 1-to-1.However, these segmentation methods weredeveloped from the monolingual perspective.3 Adjusting Word SegmentationThe goal of word segmentation adjustment is toadjust the segmentation of Chinese words such thatwe have as many 1-to-1 links to the English wordsas possible.
In this task, we will face the problemof finding the proper morpheme boundaries forChinese words.
The challenge is that almost allcharacters of Chinese are morphemes and thereforealmost every character boundary in a word couldbe the boundary of a morpheme, there is no simplerules to find the suitable boundaries of morphemes.Furthermore, not all meaningful morphemes needto be segmented to meet the requirement of 1-to-1mapping.
For example, washing machine/??
?can be segmented into ??
and ?
correspondingto washing and machine while heater/???
doesnot need, it depends on their translations.In this paper, we have proposed two differentmethods to solve this problem: 1. learning affixrules from terminology bank to segmentmorphemes and 2. using impurity measure tofinding the morpheme boundaries.
The detail ofthese methods will be described in the followingsections.4 Affix Rule MethodThe main idea of this method is to segment aChinese word according to some properly designedconditional dependent affix rules.
As shown inFigure 1, each rule is composed of threeconditional constraints, a) affix condition, b)English word condition and c) exception condition.In the affix condition, we place a underscore on theleft of a morpheme, such as _?, to denote a suffixand on the right, such as ?_, to denote a prefix.The affix rules are applied to each word bychecking the following three conditions:1.
The target word has the affix.2502.
The English word which is the target oftranslation exists in the parallel sentence.3.
The target word does not contain themorphemes in the exception list (Themorpheme in the exception list shows analternative segmentation.
).If the target word satisfies all of the aboveconditions of any rule, then the morpheme shouldbe separated from the word.
The remainingproblem will be how to derive the set of affix rules.affix English word exception_?
machine_?
engine?_ vice?_ deputy ??_?
industry ?
?Figure 1.
Samples of affix rules.4.1 Training DataWe use an unsupervised method to extract affixrules from a Chinese-English terminology bank1.The bilingual terminology bank a total of1,046,058 English terms with Chinese transla-tions in 63 categories.
Among them, 60% or629,352 terms are compounds.
We take theadvantage of the terminology bank, that allterminologies are 1-to-1 well translated, to find thebest morpheme segmentation from ambiguoussegmentations of a Chinese word according to itsEnglish counterpart.
Then we extracted affix rulesfrom the word-to-morpheme alignment results ofterms and translation.4.2 Word-to-Morpheme AlignmentThe training phase of word-to-morphemealignment is based loosely on word-to-wordalignment of the IBM model 1.
Instead of usingChinese words, we considered all the possiblemorphemes.
For example, consider the task ofaligning Department of Education and ???
as1 The bilingual terminology bank was compiled by the Na-tional Institute for Compilation and Translation.
It is freelydownload at http://terms.nict.gov.tw by registering your in-formation.shown as Figure 2.
We use the EM algorithm totrain the translation probabilities of word-morpheme pairs based on IBM model 1.Figure 2.
Example of word-to-morphemealignment.In the aligning phase, the original IBM model 1does not work properly as we expected.
Becausethe English words prefer to link to single characterand it results that some correct Chinese translationswill not be linked.
The reason is that theprobability of a morpheme, say p(?
?|education),is always less than its substring, p(?|education),since whatever ??
occurs ?
and ?
alwaysoccur but not vice versa.
So the aligning result willbe ?
/Education and ?
/Department, ?
isabandoned.
To overcome this problem, a constraintof alignment is imposed to the model to ensure thatthe aligning result covers every Chinese charactersof a target word and no overlapped characters inthe result morpheme sequence.
For instances, both?
/Education    ?
/Department and ?
?/Education    ?
?/Department are not allowedalignment sequences.
The constraint is applied toeach possible aligning result.
If the alignmentviolates the constraint, it will be rejected.Since the new alignment algorithm mustenumerate all of the possible alignments, theprocess is very time consuming.
Therefore, it isadvantageous to use a bilingual terminology bankrather than a parallel corpus.
The average length ofterminologies is short and much shorter than atypical sentence in a parallel corpus.
This makeswords to morphemes alignment computationallyfeasible and the results highly accurate (Chang etal., 2001; Bai et al, 2006).
This makes it possibleto use the result as pseudo gold standards toevaluate affix rules as described in section 4.3.251air|??
refrigeration|??
machine|?building|??
industry|?compound|??
steam|??
engine|?electronics|??
industry|?vice|?
chancellor|?
?Figure 3.
Sample of word-to-morpheme alignment.4.3 Rule ExtractionAfter the alignment task, we will get a word-to-morpheme aligned terminology bank as shown inFigure 3.
We can subsequently extract affix rulesfrom the aligned terminology bank by thefollowing steps:1) Generate candidates of affix rule:For each alignment, we produce all alignmentlinks as affix rules.
For instance, with(electronics|??
industry|?
), we wouldproduce two rules:(a) ?
?_, electronics(b) _?, industry2) Evaluate the rules:The precision of each candidate rule isestimated by applying the rule to segment theChinese terms.
If a Chinese term contains theaffix shown in the rule, the affix will besegmented.
The results of segmentation arethen to compare with the segmentation resultsof the alignments done by the algorithm of thesection 4.2 as pseudo gold standards.
Someexample results of rule evaluations are shownin Figure 4.affix English wordRuleAppliedCorrectsegments precision?_ master 458 378 0.825?
?_ periodic 130 100 0.769?
?_ video 46 40 0.870_?
chain 147 107 0.728_?
box 716 545 0.761Figure 4.
Sample evaluations of candidate rules.3) Adding exception condition:In the third step, we sort the rules according totheir precision rates in descending order,resulting in rules R1..Rn .
And then for each Ri ,we scan R1 to Ri-1, if there is a rule, Rj, havethe same English word condition and the affixcondition of Ri subsume that of Rj, then weadd affix condition of Rj as exceptioncondition of Ri.
For example, _?
, industryand _?
?, industry are rule candidates in thesorted table and have the same English wordcondition.
Furthermore, the condition _?subsumes that of ?
?, we add ??
to theexception condition of the rule with a shorteraffix.4) Reevaluate the rules with exceptioncondition:After adding the exception conditions, therules are reevaluated with considering the ex-ception condition to get new evaluation scores.5) Select rules by scores:Finally, filter out the rules with scores lowerthan a threshold2.The reason of using exception condition is thatan affix is usually an abbreviation of a word, suchas _?
is an abbreviation of ??.
In general, a fullmorpheme is preferred to be segmented than itsabbreviation while both occurred in a target word.For example, when applying rules to ???
?/electronic industry, _??
,industry is preferredthan _?,industry.
However, in the evaluation step,precision rate of _?,industry will be reduced whenapplying to full morphemes, such as ???
?/electronic industry, and then could be filtered outif the precision is lower than the threshold.5 Impurity Measure MethodThe impurity measure was used by decision tree(Duda et al, 2001) to split the training examplesinto smaller and smaller subsets progressivelyaccording to features and hope that all the samplesin each subset is as pure as possible.
Forconvenient, they define the impurity functionrather than the purity function of a subset asfollows:?
?=jjj wPwPSimpurity )(log)()( 22 We set the threshold as 0.7.252(a) impurity value of ????.
(b) impurity values of ??
and ?
?.Figure 5.
Examples of impurity values.Where P(wj) is the fraction of examples at set Sthat are in category wj.
By the well-knownproperties of entropy if all the examples are of thesame category the impurity is 0; otherwise it ispositive, with the greatest value occurring whenthe different classes are equal likely.5.1 Impurity Measure of TranslationIn our experiment, the impurity measure is usedto split a Chinese word into two substrings andhope that all the characters in a substring aregenerated by the parallel English words as pure aspossible.
Here, we treat a Chinese word as a set ofcharacters, the parallel English words as categoriesand the fraction of examples is redefined by theexpected fraction number of characters that aregenerated by each English word.
So we redefinethe entropy impurity as follows:);|(log);|();( 2 fe,fe,fe,eefcefcfIeE ???
?=In which f denotes the target Chinese word, e and fdenote the parallel English and Chinese sentencethat f belongs to and   is the expectedfraction number of characters in f that aregenerated by word e. The expected fractionnumber can be defined as follows:);|( fe,efc?????
???
?=efe,e fcfcecpecpefc)|()|();|(Where p(c | e) denotes the translation probabilityof Chinese character c given English word e.For example, as shown in Figure 5, the impurityvalue of ???
?, Figure 5.
(a), is much higherthan values of ??
and ?
?, Figure 5.(b).
Whichmeans that the generating relations from English toChinese tokens are purified by breaking ???
?into ??
and ?
?.The translation probabilities between Chinesecharacters and English word can be trained usingIBM model 1 by treating Chinese characters astokens.5.2 Target Word SelectionIn this experiment, we treat the Chinese wordswhich can be segmented into morphemes andlinked to different English words as target words.In order to speedup our impurity method only tar-get words will be segmented during the process.Therefore we investigate the actual distribution oftarget words first, we have tagged 1,573 Chinesewords manually with target and non-target.
It turnsout that only 6.87% of the Chinese words aretagged as target and 94.4% of target words arenouns.
The results show that most of the Chinesewords do not need to be re-segmented and theirPOS distribution is very unbalanced.
The resultsshow that we can filter out the non-target words bysimple clues.
In our experiment, we use three fea-tures to filter out non-target words:1) POS: Since 94.4% of the target words arenouns, we focus our experiment on nounsand filter out words with other POS.2) One-to-many alignment in GIZA++:  OnlyChinese words which are linked to multipleEnglish words in the result of GIZA++ areconsidered to be target words.3) Impurity measure: the target words are ex-pected to have high impurity values.
So thewords with a impurity values larger than athreshold are selected as target words3.3 In our experiment, we use 0.3 as our threshold.2535.3 Best Breaking Point and we used these annotated data as our goldstandard in testing.
The goal of segmentation adjustment usingimpurity is to find the best breaking point of aChinese word according to parallel English words.When a word is broken into two substrings, thenew substrings can be compared to original wordby the information gain which is defined in termsof impurity as follows:Because of the modification of Chinese tokenscaused by the word segmentation adjustment, aproblem has been created when we wanted tocompare the results to the copy which did notundergo adjustment.
Therefore, after the alignmentwas done, we merged the alignment links related totokens that were split up during adjustment.
Forexample, the two links of foreign/??
minister/??
were merged as foreign minister/????.
),;(21),;(21),;(),,(1111fefefe niEiEEniifIfIfIfffIG++?
?=The evaluation of word alignment results areshown in Table 1, including precision-recall andAER evaluation methods.
In which the baseline isalignment result of the unadjusted data.
The tableshows that after the adjustment of wordsegmentation, both methods obtain significantimprovement over the baseline, especially for theEnglish-Chinese direction and the intersectionresults of both directions.
The impurity method inparticular improves alignment in both English-Chinese and Chinese-English directions.Where i denotes a break point in f,  denotesfirst i characters of f, and  denotes last n-icharacters of f. If the information gain of abreaking point is positive, the result substrings areconsidered to be better, i.e.
more pure than originalword.if1nif 1+The goal of finding the best breaking point canbe achieved by finding the point which maximizesthe information gain as the following formula:The improvement of intersection of bothdirections is important for machine translation.Because the intersection result has higher precision,a lot of machine translation method relies onintersecting the alignment results.
The phrase-based machine translation (Koehn et al, 2003)uses the grow-diag-final heuristic to extend theword alignment to phrase alignment by using theintersection result.
Liang (Liang et al, 2006) hasproposed a symmetric word alignment model thatmerges two simple asymmetric models into asymmetric model by maximizing a combination oflikelihood and agreement between the models.This method uses the intersection as the agreementof both models in the training time.
The methodhas reduced the alignment error significantly overthe traditional asymmetric models.
),,(maxarg 111niinifffIG +<?Note that a word can be separated into twosubstrings each time.
If we want to segment acomplex word composed of many morphemes, justsplit the word again and again like the constructionof decision tree, until the information gain isnegative or less than a threshold4.6 ExperimentsIn order to evaluate the effect of our methods onthe word alignment task, we preprocessed parallelcorpus in three ways: First we use a state-of-the-artword segmenter to tokenize the Chinese part of thecorpus.
Then, we used the affix rules to adjustword segmentation.
Finally, we do the same but byusing the impurity measure method.
We used theGIZA++ package (Och and Ney, 2003) as the wordalignment tool to align tokens on the three copiesof preprocessed parallel corpora.In order to analyze the adjustment results, wealso manually segment and link the words ofChinese sentences to make the alignments 1-to-1mapping as many as possible according to theirtranslations for the 112 gold standard sentences.Table 2 shows the results of our analysis, theperformance of impurity measure method is alsoslightly better than the affix rules in both recall andprecision measure.We used the first 100,000 sentences of HongKong News parallel corpus from LDC as ourtraining data.
And 112 randomly selected parallelsentences were aligned manually with sure andpossible tags, as described in (Och and Ney, 2000),4 In our experiment, we set 0 as the threshold.254direction Recall precision F-score AEREnglish-Chinese 68.3 61.2 64.6 35.7Chinese-English 79.6 67.0 72.8 27.8 baselineintersection 59.9 92.0 72.6 26.6English-Chinese 78.2 64.6 70.8 29.8Chinese-English 80.2 68.0 73.6 27.0 affix rulesintersection 69.1 92.3 79.0 20.2English-Chinese 78.1 64.9 70.9 29.7Chinese-English 81.4 70.4 75.5 25.0 impurityintersection 70.2 91.9 79.6 19.8Table 1.
Alignment results based on the standard word segmentation data.recall precisionaffix rules 82.35 66.66impurity 84.31 67.72Table 2.
Alignment results based on the manualword segmentation data.7 ConclusionIn this paper, we have proposed two Chinese wordsegmentation adjustment methods to improve wordalignment.
The first method uses the affix ruleslearned from a bilingual terminology bank andthen applies the rules to the parallel corpus to splitthe compound Chinese words into morphemes ac-cording to its counterpart parallel sentence.
Thesecond method uses the impurity method, whichwas motivated by the method of decision tree.
Theexperimental results show that both methods leadto significant improvement in word alignment per-formance.Acknowledgements: This research was supportedin part by the National Science Council of Taiwanunder NSC Grants: NSC95-2422-H-001-031.ReferencesNecip Fazil Ayan and Bonnie J. Dorr.
2006.
GoingBeyond AER: An Extensive Analysis of WordAlignments and Their Impact on MT.
In Proceedingsof ACL 2006, pages 9-16, Sydney, Australia.Ming-Hong Bai, Keh-Jiann Chen and Jason S. Chang.2006.
Sense Extraction and Disambiguation forChinese Words from Bilingual Terminology Bank.Computational Linguistics and Chinese LanguageProcessing, 11(3):223-244.Petter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, Robert L. Mercer.
1993.
TheMathematics of Machine Translation: ParameterEstimation.
Computational Linguistics, 19(2):263-311.Jason S Chang, David Yu, Chun-Jun Lee.
2001.
Statisti-cal Translation Model for Phrases(in Chinese).
Com-putational Linguistics and Chinese Language Proc-essing, 6(2):43-64.Keh-Jiann Chen, Ming-Hong Bai.
1998.
UnknownWord Detection for Chinese by a Corpus-basedLearning Method.
International Journal ofComputational linguistics and Chinese LanguageProcessing, 1998, Vol.3, #1, pages 27-44.Keh-Jiann Chen, Wei-Yun Ma.
2002.
Unknown WordExtraction for Chinese Documents.
In Proceedings ofCOLING 2002, pages 169-175, Taipei, Taiwan.Keh-Jiann Chen, Shing-Huan Liu.
1992.
WordIdentification for Mandarin Chinese Sentences.
InProceedings of 14th COLING, pages 101-107.John DeNero, Dan Klein.
2007.
Tailoring WordAlignments to Syntactic Machine Translation.
InProceedings of ACL 2007, pages 17-24, Prague,Czech Republic.Yonggang Deng, William Byrne.
2005.
HMM word andphrase alignment for statistical machine translation.In Proceedings of HLT-EMNLP 2005, pages 169-176,Vancouver, Canada.Richard O. Duda, Peter E. Hart, David G. Stork.
2001.Pattern Classification.
John Wiley & Sons, Inc.Jianfeng Gao, Mu Li, Andi Wu and Chang-NingHuang.
2005.
Chinese word segmentation andnamed entity recognition: a pragmatic approach.Computational Linguistics, 31(4)Sharon Goldwater, David McClosky.
2005.
ImprovingStatistical MT through Morphological Analysis.
In255Proceedings of HLT/EMNLP 2005, pages 676-683,Vancouver, Canada.Philipp Koehn, Franz J. Och, Daniel Marcu.
2003.
Sta-tistical Phrase-Based Translation.
In Proceedings ofHLT/NAACL 2003, pages 48-54, Edmonton, Canada.Young-Suk Lee.
2004.
Morphological Analysis forStatistical Machine Translation.
In Proceedings ofHLT-NAACL 2004, pages 57-60, Boston, USA.Young-Suk Lee, Kishore Papineni, Salim Roukos.
2003.Language Model Based Arabic Word Segmentation.In Proceedings of ACL 2003, pages 399-406,Sapporo, Japan.Percy Liang, Ben Taskar, Dan Klein.
2006.
Alignmentby Agreement.
In Proceedings of HLT-NAACL 2006,pages 104-111, New York, USA.Wei-Yun Ma, Keh-Jiann Chen.
2003.
A Bottom-upMerging Algorithm for Chinese Unknown WordExtraction.
In Proceedings of ACL 2003, SecondSIGHAN Workshop on Chinese Language Processing,pp31-38, Sapporo, Japan.Yanjun Ma, Nicolas Stroppa, Andy Way.
2007.Bootstrapping Word Alignment via Word Packing.
InProceedings of ACL 2007, pages 304-311, Prague,Czech Republic.Robert C. Moore.
2004.
Improving IBM Word-Alignment Model 1.
In Proceedings of ACL 2004,pages 519-526, Barcelona, Spain.Franz Josef Och, Hermann Ney.
A SystematicComparison of Various Statistical Alignment Models,Computational Linguistics, volume 29, number 1, pp.19-51 March 2003.Franz J. Och, Hermann Ney., Improved StatisticalAlignment Models, In Proceedings of the 38th An-nual Meeting of the Association for ComputationalLinguistics, 2000, Hong Kong, pp.
440-447.Stefan Vogel, Hermann Ney, Christoph Tillmann.
1996.HMM-based word alignment in statistical translation.In Proceedings of COLING 1996, pages 836-841,Copenhagen, Denmark.Dekai Wu, Xuanyin Xia.
1994.
Learning an English-Chinese Lexicon from a Parallel Corpus.
InProceedings of AMTA 1994, pages 206-213,Columbia, MD.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Corpora.Computational Linguistics, 23(3):377-403.256
