Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 906?911,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsExtracting Subevents via an Effective Two-phase ApproachAllison Badgett and Ruihong HuangTexas A&M University{allisonbadgett, huangrh}@cse.tamu.eduAbstractWe present our pilot research on automaticallyextracting subevents from a domain-specificcorpus, focusing on the type of subevents thatdescribe physical actions composing an event.We decompose the challenging problem andpropose a two-phase approach that effectivelycaptures sentential and local cues that describesubevents.
We extracted a rich set of over 600novel subevent phrases.
Evaluation shows theautomatically learned subevents help to dis-cover 10% additional main events (of whichthe learned subevents are a part) and improveevent detection performance.1 IntroductionGeneral and abstract event expressions and eventkeywords are often used for detecting events.
Todetect civil unrest events for example, commonevent expressions ?took to the streets?
and ?stageda protest?, and event keywords ?rally?
and ?strike?are usually considered as the first option.
Subevents,events that occur as a part of the main event andtherefore are useful to instantiate the main event,widely exist in event descriptions but they are rarelyused for detecting the main event.In this paper, we focus on learning subeventphrases that describe physical actions composing anevent.
Such subevents are the evidence that an eventis occurring.
For example, if a person were to ex-plain how he knew that the crowd gathered in thestreet was rioting, he might point to the shouting ofpolitical slogans or tires lit on fire.
In this instance,the riot would be the event.
The slogan-shoutingand tire-burning would be the subevents.
Becausesubevent detection requires an understanding of lev-els of abstraction, this task can even be difficult forhumans to perform.
Furthermore, subevent phrasesand general event phrases often have the same gram-matical structure and may share common words,which makes automatically differentiating betweenevent phrases and subevent phrases within a docu-ment even more difficult.Additionally, events and subevents are not disjointclasses.
There are some subevents that are unam-biguous.
For example, ?burning tires?
is a concretephrase that would not fall into the category of moreabstract events.
However, ?gathered at site?
is cer-tainly more ambiguous.
Even human analysts wouldnot necessarily agree on the appropriate class for thisphrase.
In many cases, the categorization would becontext-dependent.
Because of this, our research fo-cused on identifying the less ambiguous, and there-fore more concrete, cases.Instead of separating subevent phrases from gen-eral event phrases, we explicitly acquire subeventphrases by leveraging both sentential and local cuesin describing subevents.
We observed that subeventsof the type in which we are interested, as impor-tant facts presented in news stories, are commonlymentioned in sentences that refer to the source of in-formation or simply in quotation sentences.
Thesesentences usually start or end with characteristicphrases such as ?media reports?
and ?witness said?.Furthermore, we observed that subevent phrases of-ten occur in conjunction constructions as a sequenceof subevent phrases, as shown in the following ex-amples:906(1) State television broadcast the event live, offeringsweeping aerial views that showed the sea of peoplewaving banners, blew whistles, and shoutedslogans.
(2) They also set fires, stoned civilian vehicles,taunted the police and hurled stones at them,witnesses said.where the subevents are shown in bold, and sen-tential cues are underlined.Inspired by these observations, we propose anovel two-phase approach to automatically extractsubevents, which consists of a sentence classifierthat incrementally identifies sentences mentioningsubevents and a subevent extractor which looks for asequence of subevent phrases in a conjunction struc-ture.
Our sentence classifier is trained in a weaklysupervised manner and only requires a small set ofsubevent phrases as a guide.
The classifier was ini-tially trained with sentences containing eight pro-vided subevent seeds, then it proceeded to labelmore sentences that mention new subevent phrases.This two-phase subevent extraction approach cansuccessfully identify 610 diverse subevent phrasesfrom a domain-specific corpus.
We evaluate our au-tomatically learned subevent phrases by using themto detect events.
Experimental results show that thelearned subevent phrases can recover an additional10% of event articles and improve event detectionF-1 score by 3%.2 Related WorkWhile it is generally agreed that subevents are animportant type of information in event descriptions,they are seldom considered in decades of event ex-traction research (Appelt et al, 1993; Riloff, 1993;Soderland et al, 1995; Sudo et al, 2003; Li etal., 2005; Yu et al, 2005; Gu and Cercone, 2006;Maslennikov and Chua, 2007; S. and E., 2009;Liao and Grishman, 2010; Huang and Riloff, 2011;Chambers and Jurafsky, 2011; Huang and Riloff,2012; Huang et al, 2016).
Subevents as a themehas been discussed in the past three Event work-shops (Eve, 2013), (Eve, 2014), (Eve, 2015).
How-ever, despite the great potential of using subeventsto improve event detection and extraction (Hakeemand Shah, 2005), and event coreference resolution(Araki et al, 2014), there is little existing researchon automatically learning subevent phrases, par-tially because researchers have not agreed upon thedefinition of subevents.
Much recent research inevent timeline generation (Huang and Huang, 2013)suggests the usefulness of subevents in improvingquality and completeness of automatically generatedevent summaries.
However, they often focus ona different notion of subevents that broadly coverspre-condition events and consequence events and istemporally-based.Subevents have been studied for event trackingapplications (Shen et al, 2013; Meladianos et al,2015).
However, most current research is specifi-cally related to social media applications, like Twit-ter, in terms of both its definition of subevents andmethodologies.
For example, in previous researchby (Shen et al, 2013), a subevent is defined asa topic that is discussed intensively in the Twitterstream for a short period of time before fading away.Accordingly, the subevent detection method relieson modeling the ?burstiness?
and ?cohesiveness?properties of tweets in the stream.
We instead aimto provide a more general definition of subevents aswell as present a method for identifying subevent atthe article level.3 A Two-phase Approach for SubeventExtractionAs illustrated in Figure 1, We use a two-phase algo-rithm to identify subevent phrases from our domain-specific corpus.
For the first stage, we implementeda bootstrapped artificial neural network in orderto identify sentences that are likely to contain asubevent phrase.
In the second stage, we identifyphrases fitting a predetermined conjunction patternwithin the sentences classified by the first-stage neu-ral network.3.1 Phase 1: Identifying Subevent Sentences3.1.1 Domain-specific CorpusThanks to previous research on multi-facetedevent recognition by (Huang and Riloff, 2013), wecompiled our own domain-specific corpus that de-scribes civil unrest events.
Using civil unrest eventsas an example, (Huang and Riloff, 2013) demon-907Figure 1: The Two-step Subevent Learning Paradigmstrated that we can use automatically-learned eventfacet phrases (event agents and purposes) and mainevent expressions to find event articles with a highaccuracy.
We first obtained their learned event facetphrases and event expressions, most of which referto general events.
Then we followed their paper andidentified two types of news articles that are likelyto describe a civil unrest event by matching the ob-tained phrases to the English Gigaword fifth edition(Parker et al, 2011)Specifically, we first consider news articles thatcontain a civil unrest keyword such as ?strike?
and?protest?1, then we identify an article as relevant if itcontains a sentence where either two types of facetphrases or one facet phrase together with an eventexpression are found.
In addition, we consider newsarticles that do not contain a civil unrest keyword;we require an article to contain a sentence wherethree types of event information are matched.
Over-all, we get a civil unrest corpus containing 232,710news articles.1We used the same set of keywords as used by (Huang andRiloff, 2013)3.1.2 Context Feature SelectionWe hypothesized that the first and last noun/verbphrases and their positions in the sentence werelikely to be good indicators that the sentence mightcontain a subevent phrase.
Because our documentcorpus was composed of news articles, we deter-mined that concrete subevents would require a levelof substantiation that abstract, non-specific eventswould not.
For example, a reporter would not usu-ally cite a source in a sentence informing the readerthat a riot occurred but would likely choose to quotea source when reporting that rioters burned tires inthe streets.
Because of this, sentences containingsubevent phrases often begin or end with phrasessuch as ?he witnessed?
or ?she told the press.?
Torepresent the nouns and verbs, we used the 50-dimention Stanford GloVe (Pennington et al, 2014)word embeddings pre-trained on Wikipedia 2014and Gigaword5 (Parker et al, 2011).3.1.3 Seeds and Training Sentence GenerationTo form a training set for the neural network, weused eight seed subevent phrases (as shown in Table1) to identify a set of positive sentences that con-908waved bannersshouted sloganschanted slogansburned flagburned flagsblocked roadclashed with policeclashed with governmentTable 1: First Stage Classifier Seed Subevent Phrasestain one of the seed phrases.
In total, we obtainedaround 5000 positive sentences and bounded this to3500 for use with the classifier.
Finding a sufficientnumber of negative sentences was a more challeng-ing task.
After reviewing the corpus, we determinedthat the first and last sentences of an article are un-likely to contain subevent phrases.
These sentencesoften function as general introductions and conclu-sions that refer to the main event of the article.
Weselected 7000 of these sentences to form the nega-tive set.
The rest of the sentences not classified aspositive or negative remain unknown, amounting toalmost 1 million.3.1.4 Artificial Neural Networks for SentenceClassificationWe implemented an artificial neural network witha single hidden layer composed of 500 nodes.
In or-der to facilitate faster training, we used tanh as theactivation function of the hidden layer.
Softmax wasused as the output layer activation function.
In or-der to train the network, we provided an initial set ofpositive and negative vectors representing sentencedata from the corpus as described in Section 3.1.3.These input vectors were then divided into a train-ing set, validation set and testing set.
The trainingset was comprised of 70% of the full dataset, thevalidation set of 20% and the testing set of 10%.The neural network was trained for 1000 epochs andused the validation set and test set to measure per-formance in order to avoid overfitting.Because we began with a limited number of seedphrases to create the positive set, we chose to usea bootstrapping approach to expand our data setand improve the classifier.
After training, the en-tire unknown data set would be classified, and sen-tences determined to be positive with 0.90 certaintyIteration Positives Negatives1 13223 264462 12611 252223 9411 188224 6076 121525 2842 5684Table 2: Number of Sentences Added after Each Iterationor greater would be added to the positive set.
Sen-tences classified as negative with 0.90 certainty orgreater would be added to the negative set.
How-ever, in order to maintain the 2:1 ratio of negativeto positive vectors, the number of negative vectorsthat could be added was capped at twice the num-ber of positive additions for each iteration.
After thenew sentences were added to the positive and nega-tive sets, the neural network was retrained with thisdata and classified additional previously unknownsentences.
The process repeated for five iterations,then bootstrapping ended because not enough newlyidentified positive sentences were found (<3000 inthe last iteration).
Table 2 shows the number of sen-tences that were added after each bootstrapping iter-ation.3.2 Phase 2: Subevent ExtractionAfter accumulating a large set of sentences likelycontaining subevents from the first phase of the sys-tem, the second step identifies the subevent phraseswithin these sentences.
We observe that subeventphrases often occur in lists and we focus on lever-aging such local cues to extract subevents.
Specif-ically, we identify conjunction constructions thatcontain three or more verb phrases, each verb phraseobeys one of the following two forms: verb + directobject or verb + prepositional phrase.
We extractthe sequence of verb phrases, each as a subeventcandidate.
We only included subevents with fre-quency greater than or equal to two in the final eval-uation.
Through the two-stage extraction procedure,we identified 610 unique subevents.
Table 3.2 showssome of the learned subevents.Clearly, this second phase suffers from low re-call.
However, because subevents are identified atthe corpus level as opposed to the document level,per-sentence recall is not a significant concern aslong as a sufficient number of subevents are identi-909threw stones, hurled rocks, pounded in airsmashed through roadblocks, detained peopleforced way, fired in air, threw at policesmashed windows, set fire, burned tiresthrew bombs, opened fire, blocked roadpelted with stones, appeared on balconyarrested for vandalism, threw bombsburned cars, carried banners, lit candlesdetained people, planted flag, wore masksstoned police, converged on highwaychanted against authorities, chanted in citybroke through barricade, blocked trafficbroke windows, screamed outside palacetorched cars, ransacked office, smashed shopshouted in unison, sang songs, planted flagsruns alongside shrines, chanted for democracyTable 3: Subset of learned subeventsRecall Precision F1-score(Huang and Riloff, 2013) 71 88 79+Subevents 81 83 82Table 4: Event Recognition Performance Before/After Incor-porating Subeventsfied across the whole corpus.
As we demonstrate inthe evaluation section, corpus-level recall was highenough to produce noticeable results.4 EvaluationWe show that our acquired subevent phrases are use-ful to discover articles that describe the main eventand therefore improve event detection performance.For direct comparisons, we tested our subeventsusing the same test data and the same evaluation set-ting as the previous multi-faceted event recognitionresearch by (Huang and Riloff, 2013).
Specifically,they have annotated 300 new articles that each con-tains a civil unrest keyword and only 101 of them areactually civil unrest stories.
They have shown thatthe multi-faceted event recognition approach can ac-curately identify civil unrest documents, by identify-ing a sentence in the documents where two types offacet phrases or one facet phrase and a main eventexpression were matched.
The first row of Table 4shows their multi-faceted event recognition perfor-mance.We compared our learned subevent phrases withthe event phrases learned by (Huang and Riloff,2013) and found that 559 out of our 610 uniquephrases are not in their list.
We augmented theirprovided event phrase list with our newly acquiredsubevent phrases and then used the exactly sameevaluation procedure.
Essentially, we used a longerevent phrase dictionary which is a combination ofmain event expressions resulted from the previ-ous research by (Huang and Riloff, 2013) and ourlearned subevent phrases.
Row 2 shows the eventrecognition performance using the extended eventphrase list.
We can see that after incorporatingsubevent phrases, additional 10% of civil unrest sto-ries were discovered, with a small precision loss, theF1-score on event detection was improved by 3%.5 ConclusionWe have presented a two-phase approach for iden-tifying a specific type of ?subevents?, referring tophysical actions composing an event.
While our ap-proach is certainly tailored to the civil unrest do-main, we believe that this method is applicable tomany other domains within the scope of news re-ports, including health, economics and even poli-tics, where reporters overwhelmingly rely on outsideopinion to present the facts of the story and providethe summary themselves.
However in more casualdomains where this is not necessarily the case, thisapproach will suffer.
For instance, in sports writing,a reporter giving a play-by-play of a basketball gamewill not need to call upon witnesses or field expertsto present concrete subevents.Furthermore, we have shown the great potential ofusing subevents to improve event detection perfor-mance.
In addition, distinguishing between eventsand subevents develops an event hierarchy and canbenefit multiple applications such as text summa-rization and event timeline generation.AcknowledgmentsWe want to thank our anonymous reviewers for pro-viding useful comments.ReferencesD.
Appelt, J. Hobbs, J.
Bear, D. Israel, and M. Tyson.1993.
FASTUS: a Finite-state Processor for Informa-tion Extraction from Real-world Text.
In Proceedings910of the Thirteenth International Joint Conference on Ar-tificial Intelligence (IJCAI).Jun Araki, Zhengzhong Liu, Eduard H Hovy, and TerukoMitamura.
2014.
Detecting subevent structure forevent coreference resolution.
In LREC, pages 4553?4558.Nathanael Chambers and Dan Jurafsky.
2011.
Template-Based Information Extraction without the Templates.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies (ACL-11).2013.
The 1st Workshop on EVENTS: Definition,Detection, Coreference, and Representation.
Inhttps://sites.google.com/site/cfpwsevents/home.2014.
The 2nd Workshop on EVENTS: Definition,Detection, Coreference, and Representation.
Inhttps://sites.google.com/site/wsevents2014/home.2015.
The 3rd Workshop on EVENTS: Definition,Detection, Coreference, and Representation.
Inhttps://sites.google.com/site/wsevents2015/home.Z.
Gu and N. Cercone.
2006.
Segment-Based HiddenMarkov Models for Information Extraction.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics, pages481?488, Sydney, Australia, July.Asaad Hakeem and Mubarak Shah.
2005.
Multiple agentevent detection and representation in videos.
In AAAI,pages 89?94.Lifu Huang and Lian?en Huang.
2013.
Optimizedevent storyline generation based on mixture-event-aspect model.
In EMNLP, pages 726?735.Ruihong Huang and Ellen Riloff.
2011.
Peeling Backthe Layers: Detecting Event Role Fillers in SecondaryContexts.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies (ACL-11).Ruihong Huang and Ellen Riloff.
2012.
Modeling Tex-tual Cohesion for Event Extraction.
In Proceedings ofthe 26th Conference on Artificial Intelligence (AAAI-12).Ruihong Huang and Ellen Riloff.
2013.
Multi-facetedEvent Recognition with Bootstrapped Dictionaries.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-13).L.
Huang, T. Cassidy, X. Feng, H. Ji, C. Voss, J. Han,and A. Sil.
2016.
Liberal event extraction and eventschema induction.
In Proceedings of the 54th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies (ACL-16).Y.
Li, K. Bontcheva, and H. Cunningham.
2005.
Us-ing Uneven Margins SVM and Perceptron for Infor-mation Extraction.
In Proceedings of Ninth Confer-ence on Computational Natural Language Learning,pages 72?79, Ann Arbor, MI, June.Shasha Liao and Ralph Grishman.
2010.
Using Doc-ument Level Cross-Event Inference to Improve EventExtraction.
In Proceedings of the 48st Annual Meetingon Association for Computational Linguistics (ACL-10).M.
Maslennikov and T. Chua.
2007.
A Multi-ResolutionFramework for Information Extraction from Free Text.In Proceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics.P.
Meladianos, G. Nikolentzos, F. Rousseau,Y.
Stavrakas, and M. Vazirgiannis.
2015.
Degeneracy-based real-time sub-event detection in twitter stream.In Proceedings of the 9th AAAI international con-ference on web and social media (ICWSM), pages248?257.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2011.
English Gigaword.
In Lin-guistic Data Consortium.Jeffrey Pennington, Richard Socher, and Christopher D.Manning.
2014.
Glove: Global vectors for word rep-resentation.
In Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 1532?1543.E.
Riloff.
1993.
Automatically Constructing a Dictio-nary for Information Extraction Tasks.
In Proceedingsof the 11th National Conference on Artificial Intelli-gence.Patwardhan S. and Riloff E. 2009.
A Unified Model ofPhrasal and Sentential Evidence for Information Ex-traction.
In Proceedings of 2009 the Conference onEmpirical Methods in Natural Language Processing(EMNLP-2009).Chao Shen, Fei Liu, Fuliang Weng, and Tao Li.
2013.A participant-based approach for event summarizationusing twitter streams.
In HLT-NAACL, pages 1152?1162.S.
Soderland, D. Fisher, J. Aseltine, and W. Lehnert.1995.
CRYSTAL: Inducing a Conceptual Dictionary.In Proc.
of the Fourteenth International Joint Confer-ence on Artificial Intelligence, pages 1314?1319.K.
Sudo, S. Sekine, and R. Grishman.
2003.
An Im-proved Extraction Pattern Representation Model forAutomatic IE Pattern Acquisition.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics (ACL-03).K.
Yu, G. Guan, and M. Zhou.
2005.
Resume?
Infor-mation Extraction with Cascaded Hybrid Model.
InProceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 499?506,Ann Arbor, MI, June.911
