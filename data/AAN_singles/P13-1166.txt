Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1691?1701,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsOffspring from Reproduction Problems:What Replication Failure Teaches UsAntske Fokkens and Marieke van ErpThe Network InstituteVU University AmsterdamAmsterdam, The Netherlands{a.s.fokkens,m.g.j.van.erp}@vu.nlMarten PostmaUtrecht UniversityUtrecht, The Netherlandsmartenp@gmail.comTed PedersenDept.
of Computer ScienceUniversity of MinnesotaDuluth, MN 55812 USAtpederse@d.umn.eduPiek VossenThe Network InstituteVU University AmsterdamAmsterdam, The Netherlandspiek.vossen@vu.nlNuno FreireThe European LibraryThe Hague, The Netherlandsnfreire@gmail.comAbstractRepeating experiments is an important in-strument in the scientific toolbox to vali-date previous work and build upon exist-ing work.
We present two concrete usecases involving key techniques in the NLPdomain for which we show that reproduc-ing results is still difficult.
We show thatthe deviation that can be found in repro-duction efforts leads to questions abouthow our results should be interpreted.Moreover, investigating these deviationsprovides new insights and a deeper under-standing of the examined techniques.
Weidentify five aspects that can influence theoutcomes of experiments that are typicallynot addressed in research papers.
Our usecases show that these aspects may changethe answer to research questions leadingus to conclude that more care should betaken in interpreting our results and moreresearch involving systematic testing ofmethods is required in our field.1 IntroductionResearch is a collaborative effort to increaseknowledge.
While it includes validating previousapproaches, our experience is that most researchoutput in our field focuses on presenting new ap-proaches, and to a somewhat lesser extent buildingupon existing work.In this paper, we argue that the value of researchthat attempts to replicate previous approaches goesbeyond simply validating what is already known.It is also an essential aspect for building uponexisting approaches.
Especially when validationfails or variations in results are found, systematictesting helps to obtain a clearer picture of both theapproach itself and of the meaning of state-of-the-art results leading to a better insight into the qual-ity of new approaches in relation to previous work.We support our claims by presenting two usecases that aim to reproduce results of previouswork in two key NLP technologies: measuringWordNet similarity and Named Entity Recogni-tion (NER).
Besides highlighting the difficulty ofrepeating other researchers?
work, new insightsabout the approaches emerged that were not pre-sented in the original papers.
This last point showsthat reproducing results is not merely part of goodpractice in science, but also an essential part ingaining a better understanding of the methods weuse.
Likewise, the problems we face in reproduc-ing previous results are not merely frustrating in-conveniences, but also pointers to research ques-tions that deserve deeper investigation.We investigated five aspects that cause exper-imental variation that are not typically describedin publications: preprocessing (e.g.
tokenisa-tion), experimental setup (e.g.
splitting data forcross-validation), versioning (e.g.
which versionof WordNet), system output (e.g.
the exact fea-tures used for individual tokens in NER), and sys-tem variation (e.g.
treatment of ties).As such, reproduction provides a platform forsystematically testing individual aspects of an ap-proach that contribute to a given result.
What isthe influence of the size of the dataset, for exam-ple?
How does using a different dataset affect theresults?
What is a reasonable divergence betweendifferent runs of the same experiment?
Findinganswers to these questions enables us to better in-terpret our state-of-the-art results.1691Moreover, the experiments in this paper showthat even while strictly trying to replicate a pre-vious experiment, results may vary up to a pointwhere they lead to different answers to the mainquestion addressed by the experiment.
The Word-Net similarity experiment use case compares theperformance of different similarity measures.
Wewill show that the answer as to which measureworks best changes depending on factors such asthe gold standard used, the strategy towards part-of-speech or the ranking coefficient, all aspectsthat are typically not addressed in the literature.The main contributions of this paper are thefollowing:1) An in-depth analysis of two reproduction usecases in NLP2) New insights into the state-of-the-art resultsfor WordNet similarities and NER, found becauseof problems in reproducing prior research3) A categorisation of aspects influencingreproduction of experiments and suggestions ontesting their influence systematicallyThe code, data and experimental setupfor the WordNet experiments are avail-able at http://github.com/antske/WordNetSimilarity, and for the NER exper-iments at http://github.com/Mvanerp/NER.
The experiments presented in this paperhave been repeated by colleagues not involved inthe development of the software using the codeincluded in these repositories.
The remainder ofthis paper is structured as follows.
In Section 2,previous work is discussed.
Sections 3 and 4describe our real-world use cases.
In Section 5,we present our observations, followed by a moregeneral discussion in Section 6.
In Section 7, wepresent our conclusions.2 BackgroundThis section provides a brief overview of recentwork addressing reproduction and benchmark re-sults in computer science related studies and dis-cusses how our research fits in the overall picture.Most researchers agree that validating resultsentails that a method should lead to the same over-all conclusions rather than producing the exactsame numbers (Drummond, 2009; Dalle, 2012;Buchert and Nussbaum, 2012, etc.).
In otherwords, we should strive to reproduce the same an-swer to a research question by different means,perhaps by re-implementing an algorithm or eval-uating it on a new (in domain) data set.
Replica-tion has a somewhat more limited aim, and simplyinvolves running the exact same system under thesame conditions in order to get the exact same re-sults as output.According to Drummond (2009) replication isnot interesting, since it does not lead to new in-sights.
On this point we disagree with Drum-mond (2009) as replication allows us to: 1) vali-date prior research, 2) improve on prior researchwithout having to rebuild software from scratch,and 3) compare results of reimplementations andobtain the necessary insights to perform reproduc-tion experiments.
The outcome of our use casesconfirms the statement that deeper insights into anapproach can be obtained when all resources areavailable, an observation also made by Ince et al(2012).Even if exact replication is not a goal manystrive for, Ince et al (2012) argue that insightfulreproduction can be an (almost) impossible un-dertaking without the source code being available.Moreover, it is not always clear where replicationstops and reproduction begins.
Dalle (2012) dis-tinguishes levels of reproducing results related tohow close they are to the original work and howeach contributes to research.
In general, an in-creasing awareness of the importance of reproduc-tion research and open code and data can be ob-served based on publications in high-profile jour-nals (e.g.
Nature (Ince et al, 2012)) and initiativessuch as myExperiment.1Howison and Herbsleb (2013) point out that,even though this is important, often not enough(academic) credit is gained from making resourcesavailable.
What is worse, the same holds for re-search that investigates existing methods ratherthan introducing new ones, as illustrated by thequestion that is found on many review forms ?hownovel is the presented approach??.
On the otherhand, initiatives for journals addressing exactlythis issue (Neylon et al, 2012) and tracks focus-ing on results verification at conferences such asVLDB2 show that this opinion is not universal.A handful of use cases on reproducing or repli-cating results have been published.
Louridas andGousios (2012) present a use case revealing thatsource code alone is not enough for reproducing1http://www.myexperiment.org2http://www.vldb.org/2013/1692results, a point that is also made by Mende (2010)who provides an overview of all information re-quired to replicate results.The experiments in this paper provide use casesthat confirm the points brought out in the litera-ture mentioned above.
This includes both obser-vations that a detailed level of information is re-quired for truly insightful reproduction research aswell as the claim that such research leads to betterunderstanding of our techniques.
Furthermore, thework in this paper relates to Bikel (2004)?s work.He provides all information needed in addition toCollins (1999) to replicate Collins?
benchmark re-sults.
Our work is similar in that we also aim to fillin the blanks needed to replicate results.
It mustbe noted, however, that the use cases in this paperhave a significantly smaller scale than Bikel?s.Our research distinguishes itself from previouswork, because it links the challenges of reproduc-tion to what they mean for reported results be-yond validation.
Ruml (2010) mentions variationsin outcome as a reason not to emphasise compar-isons to benchmarks.
Vanschoren et al (2012)propose to use experimental databases to system-atically test variations for machine learning, butneither links the two issues together.
Raeder et al(2010) come closest to our work in a critical studyon the evaluation of machine learning.
They showthat choices in the methodology, such as data sets,evaluation metrics and type of cross-validation caninfluence the conclusions of an experiment, as wealso find in our second use case.
However, theyfocus on the problem of evaluation and recom-mendations on how to achieve consistent repro-ducible results.
Our contribution is to investigatehow much results vary.
We cannot control howfellow researchers carry out their evaluation, butif we have an idea of the variations that typicallyoccur within a system, we can better compare ap-proaches for which not all details are known.3 WordNet Similarity MeasuresPatwardhan and Pedersen (2006) and Pedersen(2010) present studies where the output of a va-riety of WordNet similarity and relatedness mea-sures are compared.
They rank Miller and Charles(1991)?s set (henceforth ?mc-set?)
of 30 wordpairs according to their semantic relatedness withseveral WordNet similarity measures.Each measure ranks the mc-set of word pairsand these outputs are compared to Miller andCharles (1991)?s gold standard based on humanrankings using the Spearman?s Correlation Coeffi-cient (Spearman, 1904, ?).
Pedersen (2010) alsoranks the original set of 65 word pairs rankedby humans in an experiment by Rubenstein andGoodenough (1965) (rg-set) which is a superset ofMiller and Charles?s set.3.1 Replication AttemptsThis research emerged from a project run-ning a similar experiment for Dutch on Cor-netto (Vossen et al, 2013).
First, an attemptwas made to reproduce the results reported inPatwardhan and Pedersen (2006) and Peder-sen (2010) on the English WordNet using theirWordNet::Similarity web-interface.3 Results dif-fered from those reported in the aforementionedworks, even when using the same versions asthe original, WordNet::Similarity-1.02 and Word-Net 2.1 (Patwardhan and Pedersen, 2006) andWordNet::Similarity-2.05 and WordNet 3.0 (Ped-ersen, 2010), respectively.4The fact that results of similarity measures onWordNet can differ even while the same softwareand same versions are used indicates that proper-ties which are not addressed in the literature mayinfluence the output of similarity measures.
Wetherefore conducted a range of experiments that,in addition to searching for the right settings toreplicate results of previous research, address thefollowing questions:1) Which properties have an impact on the per-formance of WordNet similarity measures?2) How much does the performance of individ-ual measures vary?3) How do commonly used measures comparewhen the variation of their performance are takeninto account?3.2 Methodology and first observationsThe questions above were addressed in two stages.In the first stage, Fokkens, who was not involvedin the first replication attempt implemented ascript to calculate similarity measures using Word-Net::Similarity.
This included similarity mea-sures introduced by Wu and Palmer (1994) (wup),3Obtained from http://talisker.d.umn.edu/cgi-bin/similarity/similarity.cgi, Word-Net::Similarity version 2.05.
This web interface has nowmoved to http://maraca.d.umn.edu4WordNet::Similarity were obtained http://search.cpan.org/dist/WordNet-Similarity/.1693Leacock and Chodorow (1998) (lch), Resnik(1995) (res), Jiang and Conrath (1997) (jcn),Lin (1998) (lin), Banerjee and Pedersen (2003)(lesk), Hirst and St-Onge (1998) (hso) andPatwardhan and Pedersen (2006) (vector andvpairs) respectively.Consequently, settings and properties werechanged systematically and shared with Pedersenwho attempted to produce the new results with hisown implementations.
First, we made sure thatthe script implemented by Fokkens could producethe same WordNet similarity scores for each in-dividual word pair as those used to calculate theranking on the mc-set by Pedersen (2010).
Finally,the gold standard and exact implementation of theSpearman ranking coefficient were compared.Differences in results turned out to be relatedto variations in the experimental setup.
First,we made different assumptions on the restrictionof part-of-speech tags (henceforth ?PoS-tag?)
con-sidered in the comparison.
Miller and Charles(1991) do not discuss how they deal with wordswith more than one PoS-tag in their study.
Ped-ersen therefore included all senses with any PoS-tag in his study.
The first replication attempt hadrestricted PoS-tags to nouns based on the ideathat most items are nouns and subjects would beprimed to primarily think of the noun senses.
Bothassumptions are reasonable.
Pos-tags were not re-stricted in the second replication attempt, but be-cause of a bug in the code only the first identifiedPoS-tag (?noun?
in all cases) was considered.
Wetherefore mistakenly assumed that PoS-tag restric-tions did not matter until we compared individualscores between Pedersen and the replication at-tempts.Second, there are two gold standards for theMiller and Charles (1991) set: one has the scoresassigned during the original experiment run byRubenstein and Goodenough (1965), the otherhas the scores assigned during Miller and Charles(1991)?s own experiment.
The ranking correlationbetween the two sets is high, but they are not iden-tical.
Again, there is no reason why one gold stan-dard would be a better choice than the other, but inorder to replicate results, it must be known whichof the two was used.
Third, results changed be-cause of differences in the treatment of ties whilecalculating Spearman ?.
The influence of the ex-act gold standard and calculation of Spearman ?could only be found because Pedersen could pro-measure Spearman ?
Kendall ?
rankingmin max min max variationpath based similaritypath 0.70 0.78 0.55 0.62 1-8wup 0.70 0.79 0.53 0.61 1-6lch 0.70 0.78 0.55 0.62 1-7path based information contentres 0.65 0.75 0.26 0.57 4-11lin 0.49 0.73 0.36 0.53 6-10jcn 0.46 0.73 0.32 0.55 5, 7-11path based relatednesshso 0.73 0.80 0.36 0.41 1-3,5-10dictionary and corpus based relatednessvpairs 0.40 0.70 0.26 0.50 7-11vector 0.48 0.92 0.33 0.76 1,2,4,6-11lesk 0.66 0.83 -0.02 0.61 1-8,11,12Table 1: Variation WordNet measures?
resultsvide the output of the similarity measures he usedto calculate the coefficient.
It is unlikely we wouldhave been able to replicate his results at all with-out the output of this intermediate step.
Finally,results for lch, lesk and wup changed accord-ing to measure specific configuration settings suchas including a PoS-tag specific root node or turn-ing on normalisation.In the second stage of this research, we ran ex-periments that systematically manipulate the influ-ential factors described above.
In this experiment,we included both the mc-set and the complete rg-set.
The implementation of Spearman ?
used inPedersen (2010) assigned the lowest number inranking to ties rather than the mean, resulting inan unjustified drop in results for scores that leadto many ties.
We therefore experimented with adifferent correlation measure, Kendall tau coeffi-cient (Kendall, 1938, ? )
rather than two versionsof Spearman ?.3.3 Variation per measureAll measures varied in their performance.The complete outcome of our experiments(both the similarity measures assigned toeach pair as well as the output of the rankingcoefficients) are included in the data set pro-vided at http://github.com/antske/WordNetSimilarity.
Table 1 presents anoverview of the main point we wish to makethrough this experiment: the minimal and maxi-mal results according to both ranking coefficients.Results for similarity measures varied from 0.06-0.42 points for Spearman ?
and from 0.05-0.60points for Kendall ?
.
The last column indi-cates the variation of performance of a measure1694compared to the other measures, where 1 is thebest performing measure and 12 is the worst.5For instance, path has been best performingmeasure, second best, eighth best and all positionsin between, vector has ranked first, second andfourth, but also occupied all positions from six toeleven.In principle, it is to be expected that num-bers are not exactly the same while evaluatingagainst a different data set (the mc-set versus therg-set), taking a different set of synsets to evalu-ate on (changing PoS-tag restrictions) or changingconfiguration settings that influence the similarityscore.
However, a variation of up to 0.44 pointsin Spearman ?
and 0.60 in Kendall ?
6 leads tothe question of how indicative these results reallyare.
A more serious problem is the fact that thecomparative performance of individual measurechanges.
Which measure performs best dependson the evaluation set, ranking coefficient, PoS-tagrestrictions and configuration settings.
This meansthat the answer to the question of which similaritymeasure is best to mimic human similarity scoresdepends on aspects that are often not even men-tioned, let alne systematically compared.3.4 Variation per categoryFor each influential category of experimental vari-ation, we compared the variation in Spearman ?and Kendall ?
, while similarity measure and otherinfluential categories were kept stable.
The cat-egories we varied include WordNet and Word-Net::Similarity version, the gold standard used toevaluate, restrictions on PoS-tags, and measurespecific configurations.
Table 2 presents the maxi-mum variation found across measures for each cat-egory.
The last column indicates how often theranking of a specific measure changed as the cat-egory changed, e.g.
did the measure ranking thirdusing specific configurations, PoS-tag restrictionsand a specific gold standard using WordNet 2.1still rank third when WordNet 3.0 was used in-stead?
The number in parentheses next to the ?dif-ferent ranks?
in the table presents the total num-ber of scores investigated.
Note that this num-ber changes for each category, because we com-5Some measures ranked differently as their individualconfiguration settings changed.
In these cases, the measurewas included in the overall ranking multiple times, which iswhy there are more ranking positions than measures.6Section 3.4 explains why the variation in Kendall is thisextreme and ?
is more appropriate for this task.Variation Maximum difference DifferentSpearman ?
Kendall ?
rank (tot)WN version 0.44 0.42 223 (252)gold standard 0.24 0.21 359 (504)PoS-tag 0.09 0.08 208 (504)configuration 0.08 0.60 37 (90)Table 2: Variations per categorypared two WordNet versions (WN version), threegold standard and PoS-tag restriction variationsand configuration only for the subset of scoreswhere configuration matters.There are no definite statements to make as towhich version (Patwardhan and Pedersen (2006)vs Pedersen (2010)), PoS-tag restriction or con-figuration gives the best results.
Likewise, whilemost measures do better on the smaller data set,some achieve their highest results on the full set.This is partially due to the fact that ranking coef-ficients are sensitive to outliers.
In several caseswhere PoS-tag restrictions led to different results,only one pair received a different score.
For in-stance, path assigns a relatively high score tothe pair chord-smile when verbs are included, be-cause the hierarchy of verbs in WordNet is rela-tively flat.
This effect is not observed in wup andlch which correct for the depth of the hierarchy.On the other hand, res, lin and jcn score bet-ter on the same set when verbs are considered, be-cause they cannot detect any relatedness for thepair crane-implement when restricted to nouns.On top of the variations presented above, we no-tice a discrepancy between the two coefficients.Kendall ?
generally leads to lower coefficiencyscores than Spearman ?.
Moreover, they eachgive different relative indications: where leskachieves its highest Spearman ?, it has an ex-tremely low Kendall ?
of 0.01.
Spearman ?
usesthe difference in rank as its basis to calculate a cor-relation, where Kendall ?
uses the number of itemswith the correct rank.
The low Kendall ?
for leskis the result of three pairs receiving a score that istoo high.
Other pairs that get a relatively accuratescore are pushed one place down in rank.
Becauseonly items that receive the exact same rank help toincrease ?
, such a shift can result in a drastic dropin the coefficient.
In our opinion, Spearman ?
istherefore preferable over Kendall ?
.
We included?
, because many authors do not mention the rank-ing coefficient they use (cf.
Budanitsky and Hirst(2006), Resnik (1995)) and both ?
and ?
are com-1695monly used coefficients.Except for WordNet, which Budanitsky andHirst (2006) hold accountable for minor variationsin a footnote, the influential categories we investi-gated in this paper, to our knowledge, have not yetbeen addressed in the literature.
Cramer (2008)points out that results from WordNet-Human sim-ilarity correlations lead to scattered results report-ing variations similar to ours, but she comparesstudies using different measures, data and exper-imental setup.
This study shows that even ifthe main properties are kept stable, results varyenough to change the identity of the measure thatyields the best performance.
Table 1 reveals awide variation in ranking relative to alternative ap-proaches.
Results in Table 2 show that it is com-mon for the ranking of a score to change due tovariations that are not at the core of the method.This study shows that it is far from clear howdifferent WordNet similarity measures relate toeach other.
In fact, we do not know how we canobtain the best results.
This is particularly chal-lenging, because the ?best results?
may depend onthe intended use of the similarity scores (Menget al, 2013).
This is also the reason why wepresented the maximum variation observed, ratherthan the average or typical variation (mostly be-low 0.10 points).
The experiments presented inthis paper resulted in a vast amount of data.
Anelaborate analysis of this data is needed to get abetter understanding of how measures work andwhy results vary to such an extent.
We leave thisinvestigation to future work.
If there is one take-home message from this experiment, it is that oneshould experiment with parameters such as restric-tions on PoS-tags or configurations and determinewhich score to use depending on what it is usedfor, rather than picking something that did best ina study using different data for a different task andmay have used a different version of WordNet.4 Reproducing a NER methodFreire et al (2012) describe an approach to clas-sifying named entities in the cultural heritage do-main.
The approach is based on the assumptionthat domain knowledge, encoded in complex fea-tures, can aid a machine learning algorithm inNER tasks when only little training data is avail-able.
These features include information aboutperson and organisation names, locations, as wellas PoS-tags.
Additionally, some general featuresare used such as a window of three preceding andtwo following tokens, token length and capitalisa-tion information.
Experiments are run in a 10-foldcross-validation setup using an open source ma-chine learning toolkit (McCallum, 2002).4.1 Reproducing NER ExperimentsThis experiment can be seen as a real-world caseof the sad tale of the Zigglebottom tagger (Peder-sen, 2008).
The (fictional) Zigglebottom tagger isa tagger with spectacular results that looks like itwill solve some major problems in your system.However, the code is not available and a new im-plementation does not yield the same results.
Theoriginal authors cannot provide the necessary de-tails to reproduce their results, because most of thework has been done by a PhD student who has fin-ished and moved on to something else.
In the end,the newly implemented Zigglebottom tagger is notused, because it does not lead to the promised bet-ter results and all effort went to waste.Van Erp was interested in the NER approachpresented in Freire et al (2012).
Unfortunately,the code could not be made available, so she de-cided to reimplement the approach.
Despite feed-back from Freire about particular details of thesystem, results remained 20 points below thosereported in Freire et al (2012) in overall F-score(Van Erp and Van der Meij, 2013).The reimplementation process involved choicesabout seemingly small details such as roundingto how many decimals, how to tokenise or howmuch data cleanup to perform (normalisation ofnon-alphanumeric characters for example).
Try-ing different parameter combinations for featuregeneration and the algorithm never yielded the ex-act same results as Freire et al (2012).
The resultsof the best run in our first reproduction attempt,together with the original results from Freire et al(2012) are presented in Table 3.
Van Erp and Vander Meij (2013) provide an overview of the imple-mentation efforts.4.2 Following up from reproductionSince the experiments in Van Erp and Van der Meij(2013) introduce several new research questionsregarding the influence of data cleaning and thelimitations of the dataset, we performed some ad-ditional experiments.First, we varied the tokenisation, removing non-alphanumeric characters from the data set.
Thisyielded a significantly smaller data set (10,4421696(Freire et al, 2012) results Van Erp and Van der Meij?s replication resultsPrecision Recall F?=1 Precision Recall F?=1LOC (388) 92% 55% 69 77.80% 39.18% 52.05ORG (157) 90% 57% 70 65.75% 30.57% 41.74PER (614) 91% 56% 69 73.33% 37.62% 49.73Overall (1,159) 91% 55% 69 73.33% 37.19% 49.45Table 3: Precision, recall and F?=1 scores for the original experiments from Freire et al 2012 and ourreplication of their approach as presented in Van Erp and Van der Meij (2013)tokens vs 12,510), and a 15 point drop in over-all F-score.
Then, we investigated whether vari-ation in the cross-validation splits made any dif-ference as we noticed that some NEs were onlypresent in particular fields in the data, which canhave a significant impact on a small dataset.
Weinspected the difference between different cross-validation folds by computing the standard devi-ations of the scores and found deviations of upto 25 points in F-score between the 10 splits.
Inthe general setup, database records were randomlydistributed over the folds and cut off to balance thefold sizes.
In a different approach to dividing thedata by distributing individual sentences from therecords over the folds, performance increases by8.57 points in overall F-score to 58.02.
This is notwhat was done in the original Freire et al (2012)paper, but shows that the results obtained with thisdataset are quite fragile.As we worried about the complexity of the fea-ture set relative to the size of the data set, we de-viated somewhat from Freire et al (2012)?s exper-iments in that we switched some features on andoff.
Removal of complex features pertaining to thewindow around the focus token improved our re-sults by 3.84 points in overall F-score to 53.39.The complex features based on VIAF,7 GeoN-ames8 and WordNet do contribute to the classifica-tion in the Mallet setup as removing them and onlyusing the focus token, window and generic fea-tures causes a slight drop in overall F-score from49.45 to 47.25.When training the Stanford NER system (Finkelet al, 2005) on just the tokens from theFreire data set and the parameters from en-glish.all.3class.distsim.prop (included in the Stan-ford NER release, see also Van Erp and Van derMeij (2013)), our F-scores come very close tothose reported by Freire et al (2012), but mostlywith a higher recall and lower precision.
It is puz-zling that the Stanford system obtains such high7http://www.viaf.org8http://www.geonames.orgresults with only very simple features, whereasfor Mallet the complex features show improve-ment over simpler features.
This leads to ques-tions about the differences between the CRF im-plementations and the influence of their parame-ters, which we hope to investigate in future work.4.3 Reproduction difficulties explainedSeveral reasons may be the cause of why we fail toreproduce results.
As mentioned, not all resourcesand data were available for this experiment, thuscausing us to navigate in the dark as we could notreverse-engineer intermediate steps, but only com-pare to the final precision, recall and F-scores.The experiments follow a general machinelearning setup consisting roughly of four steps:preprocess data, generate features, train model andtest model.
The novelty and replication problemslie in the first three steps.
How the data was pre-processed is a major factor here.
The data set con-sisted of XML files marked up with inline namedentity tags.
In order to generate machine learn-ing features, this data has to be tokenised, possi-bly cleaned up and the named entity markup hadto be converted to a token-based scheme.
Each ofthese steps can be carried out in several ways, andchoices made here can have great influence on therest of the pipeline.Similar choices have to be made for prepro-cessing external resources.
From the descriptionsin the original paper, it is unclear how recordsin VIAF and GeoNames were preprocessed, oreven which versions of these resources were used.Preprocessing and calculating occurrence statis-tics over VIAF takes 30 hours for each run.
Itis thus not feasible to identify the main potentialvariations without the original data to verify thisprepatory step.Numbers had to be rounded when generatingthe features, leading to the question of how manydecimals are required to be discriminative with-out creating an overly sparse dataset.
Freire recallsthat encoding features as multi-value discrete fea-1697tures versus several boolean features can have sig-nificant impact.
These settings are not mentionedin the paper, making reproduction very difficult.As the project in which the original researchwas performed has ended, and there is no cen-tral repository where such information can be re-trieved, we are left to wonder how to reuse thisapproach in order to further domain-specific NER.5 ObservationsIn this section, we generalise the observationsfrom our use cases to the main categories that caninfluence reproduction.Despite our efforts to describe our systems asclearly as possible, details that can make a tremen-dous difference are often omitted in papers.
It willbe no surprise to researchers in the field that pre-processing of data can make or break an experi-ment.The choice of which steps we perform, and howeach of these steps is carried out exactly are partof our experimental setup.
A major difference inthe results for the NER experiments was caused byvariations in the way in which we split the data forcross-validation.As we fine-tune our techniques, software getsupdated, data sets are extended or annotation bugsare fixed.
In the WordNet experiment, we foundthat there were two different gold standard datasets.
There are also different versions of Word-Net, and the WordNet::Similarity packages.
Sim-ilarly for the NER experiment, GeoNames, VIAFand Mallet are updated regularly.
It is thereforecritical to pay attention to versioning.Our experiments often consist of several differ-ent steps whose outputs may be difficult to retrace.In order to check the output of a reproduction ex-periment at every step of the way, system out-put of experiments, including intermediate steps,is vital.
The WordNet replication was only pos-sible, because Pedersen could provide the similar-ity scores of each word pair.
This enabled us tocompare the intermediate output and identify thesource of differences in output.Lastly, there may be inherent system variationsin the techniques used.
Machine learning algo-rithms may for instance use coin flips in case ofa tie.
This was not observed in our experiments,but such variations may be determined by runningan experiment several times and taking the averageover the different runs (cf.
Raeder et al (2010)).All together, these observations show that shar-ing data and software play a key role in gaining in-sight into how our methods work.
Vanschoren etal.
(2012) propose a setup that allows researchersto provide their full experimental setup, whichshould include exact steps followed in preprocess-ing the data, documentation of the experimen-tal setup, exact versions of the software and re-sources used and experimental output.
Havingaccess to such a setup allows other researchersto validate research, but also tweak the approachto investigate system variation, systematically testthe approach in order to learn its limitations andstrengths and ultimately improve on it.6 DiscussionMany of the aspects addressed in the previous sec-tion such as preprocessing are typically only men-tioned in passing, or not at all.
There is often notenough space to capture all details, and they aregenerally not the core of the research described.Still, our use cases have shown that they can have atremendous impact on reproduction, and can evenlead to different conclusions.
This leads to seriousquestions on how we can interpret our results andhow we can compare the performance of differentmethods.
Is an improvement of a few per cent re-ally due to the novelty of the approach if largervariations are found when the data is split differ-ently?
Is a method that does not quite achieve thehighest reported state-of-the-art result truly lessgood?
What does a state-of-the-art result mean ifit is only tested on one data set?If one really wants to know whether a resultis better or worse than the state-of-the-art, therange of variation within the state-of-the-art mustbe known.
Systematic experiments such as theones we carried out for WordNet similarity andNER, can help determine this range.
For resultsthat fall within the range, it holds that they canonly be judged by evaluations going beyond com-paring performance numbers, i.e.
an evaluation ofhow the approach achieves a given result and howthat relates to alternative approaches.Naturally, our use cases do not represent the en-tire gamut of research methodologies and prob-lems in the NLP community.
However, they dorepresent two core technologies and our observa-tions align with previous literature on replicationand reproduction.Despite the systematic variation we employed1698in our experiments, they do not answer all ques-tions that the problems in reproduction evoked.For the WordNet experiments, deeper analysis isrequired to gain full understanding of how indi-vidual influential aspects interact with each mea-surement.
For the NER experiments, we are yet toidentify the cause of our failure to reproduce.The considerable time investment required forsuch experiments forms a challenge.
Due to pres-sure to publish or other time limitations, they can-not be carried out for each evaluation.
There-fore, it is important to share our experiments, sothat other researchers (or students) can take thisup.
This could be stimulated by instituting repro-duction tracks in conferences, thus rewarding sys-tematic investigation of research approaches.
Itcan also be aided by adopting initiatives that en-able authors to easily include data, code and/orworkflows with their publications such as thePLOS/figshare collaboration.9 We already do asimilar thing for our research problems by organ-ising challenges or shared tasks, why not extendthis to systematic testing of our approaches?7 ConclusionWe have presented two reproduction use cases forthe NLP domain.
We show that repeating otherresearchers?
experiments can lead to new researchquestions and provide new insights into and betterunderstanding of the investigated techniques.Our WordNet experiments show that the perfor-mance of similarity measures can be influenced bythe PoS-tags considered, measure specific varia-tions, the rank coefficient and the gold standardused for comparison.
We not only find that suchvariations lead to different numbers, but also dif-ferent rankings of the individual measures, i.e.these aspects lead to a different answer to thequestion as to which measure performs best.
Wedid not succeed in reproducing the NER resultsof Freire et al (2012), showing the complexityof what seems a straightforward reproduction casebased on a system description and training dataonly.
Our analyses show that it is still an openquestion whether additional complex features im-prove domain specific NER and that this may par-tially depend on the CRF implementation.Some observations go beyond our use cases.
Inparticular, the fact that results vary significantly9http://blogs.plos.org/plos/2013/01/easier-access-to-plos-data/because of details that are not made explicit inour publications.
Systematic testing can providean indication of this variation.
We have classi-fied relevant aspects in five categories occurringacross subdisciplines of NLP: preprocessing, ex-perimental setup, versioning, system output,and system variation.We believe that knowing the influence of differ-ent aspects in our experimental workflow can helpincrease our understanding of the robustness ofthe approach at hand and will help understand themeaning of the state-of-the-art better.
Some tech-niques are reused so often (the papers introducingWordNet similarity measures have around 1,000-2,000 citations each as of February 2013, for ex-ample) that knowing their strengths and weak-nesses is essential for optimising their use.As mentioned many times before, sharing is keyto facilitating reuse, even if the code is imper-fect and contains hacks and possibly bugs.
In theend, the same holds for software as for documen-tation: it is like sex: if it is good, it is very goodand if it is bad, it is better than nothing!10 Butmost of all: when reproduction fails, regardless ofwhether original code or a reimplementation wasused, valuable insights can emerge from investi-gating the cause of this failure.
So don?t let yourfailing reimplementations of the Zigglebottom tag-ger collect dusk on a shelf while others reimple-ment their own failing Zigglebottoms.
As a com-munity, we need to know where our approachesfail, as much ?if not more?
as where they succeed.AcknowledgmentsWe would like to thank the anonymous review-ers for their eye to detail and useful commentsto make this a better paper.
We furthermorethank Ruben Izquierdo, Lourens van der Meij,Christoph Zwirello, Rebecca Dridan and the Se-mantic Web Group at VU University for theirhelp and useful feedback.
The research leading tothis paper was supported by the European Union?s7th Framework Programme via the NewsReaderProject (ICT-316404), the Agora project, by NWOCATCH programme, grant 640.004.801, and theBiographyNed project, a joint project with Huy-gens/ING Institute of the Dutch Academy of Sci-ences funded by the Netherlands eScience Center(http://esciencecenter.nl/).10The documentation variant of this quote is attributed toDick Brandon.1699ReferencesStanjeev Banerjee and Ted Pedersen.
2003.
Extendedgloss overlaps as a measure of semantic relatedness.In Proceedings of the Eighteenth International JointConference on Artificial Intelligence, pages 805?810, Acapulco, August.Daniel M. Bikel.
2004.
Intricacies of Collins?
parsingmodel.
Computational Linguistics, 30(4):479?511.Tomasz Buchert and Lucas Nussbaum.
2012.
Lever-aging business workflows in distributed systems re-search for the orchestration of reproducible and scal-able experiments.
In Anne Etien, editor, 9e`mee?dition de la confe?rence MAnifestation des JE-unes Chercheurs en Sciences et Technologies del?Information et de la Communication - MajecSTIC2012 (2012).Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating WordNet-based measures of lexical semanticrelatedness.
Computational Linguistics, 32(1):13?47.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Phd dissertation,University of Pennsylvania.Irene Cramer.
2008.
How well do semantic related-ness measures perform?
a meta-study.
In Semanticsin Text Processing.
STEP 2008 Conference Proceed-ings, volume 1, pages 59?70.Olivier Dalle.
2012.
On reproducibility and trace-ability of simulations.
In WSC-Winter SimulationConference-2012.Chris Drummond.
2009.
Replicability is not repro-ducibility: nor is it good science.
In Proceedings ofthe Twenty-Sixth International Conference on Ma-chine Learning: Workshop on Evaluation Methodsfor Machine Learning IV.Jenny Finkel, Trond Grenager, and Christopher D.Manning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbssampling.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics, pages 363?370, Ann Arbor, USA.Nuno Freire, Jose?
Borbinha, and Pa?vel Calado.
2012.An approach for named entity recognition in poorlystructured data.
In Proceedings of ESWC 2012.Graeme Hirst and David St-Onge.
1998.
Lexicalchains as representations of context for the detectionand correction of malapropisms.
In C. Fellbaum, ed-itor, WordNet: An electronic lexical database, pages305?332.
MIT Press.James Howison and James D. Herbsleb.
2013.
Shar-ing the spoils: incentives and collaboration in sci-entific software development.
In Proceedings of the2013 conference on Computer Supported Coopera-tive Work, pages 459?470.Darrel C. Ince, Leslie Hatton, and John Graham-Cumming.
2012.
The case for open computer pro-grams.
Nature, 482(7386):485?488.Jay J. Jiang and David W. Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexical tax-onomy.
In Proceedings of the International Confer-ence on Research in Computational Linguistics (RO-CLING X), pages 19?33, Taiwan.Maurice Kendall.
1938.
A new measure of rank corre-lation.
Biometrika, 30(1-2):81?93.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and WordNet similarity forword sense identification.
In C. Fellbaum, edi-tor, WordNet: An electronic lexical database, pages265?283.
MIT Press.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the 15th In-ternational Conference on Machine Learning, pages296?304, Madison, USA.Panos Louridas and Georgios Gousios.
2012.
A noteon rigour and replicability.
SIGSOFT Softw.
Eng.Notes, 37(5):1?4.Andrew K. McCallum.
2002.
MALLET: A machinelearning for language toolkit.
http://mallet.cs.umass.edu.Thilo Mende.
2010.
Replication of defect predictionstudies: problems, pitfalls and recommendations.
InProceedings of the 6th International Conference onPredictive Models in Software Engineering.
ACM.Lingling Meng, Runqing Huang, and Junzhong Gu.2013.
A review of semantic similarity measures inwordnet.
International Journal of Hybrid Informa-tion Technology, 6(1):1?12.George A. Miller and Walter G. Charles.
1991.
Con-textual correlates of semantic similarity.
Languageand Cognitive Processes, 6(1):1?28.Cameron Neylon, Jan Aerts, C Titus Brown, Si-mon J Coles, Les Hatton, Daniel Lemire, K Jar-rod Millman, Peter Murray-Rust, Fernando Perez,Neil Saunders, Nigam Shah, Arfon Smith, Gae?lVaroquaux, and Egon Willighagen.
2012.
Chang-ing computational research.
the challenges ahead.Source Code for Biology and Medicine, 7(2).Siddharth Patwardhan and Ted Pedersen.
2006.
Us-ing wordnet based context vectors to estimate thesemantic relatedness of concepts.
In Proceedings ofthe EACL 2006 Workshop Making Sense of Sense -Bringing Computational Linguistics and Psycholin-guistics Together, pages 1?8, Trento, Italy.Ted Pedersen.
2008.
Empiricism is not a matter offaith.
Computational Linguistics, 34(3):465?470.1700Ted Pedersen.
2010.
Information content measuresof semantic similarity perform better without sense-tagged text.
In Proceedings of the 11th Annual Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics (NAACL-HLT2010), pages 329?332, Los Angeles, USA.Troy Raeder, T. Ryan Hoens, and Nitesh V. Chawla.2010.
Consequences of variability in classifier per-formance estimates.
In Proceedings of ICDM?2010.Philip Resnik.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
In Pro-ceedings of the 14th International Joint Conferenceon Artificial Intelligence (IJCAI), pages 448?453,Montreal, Canada.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Communica-tions of the ACM, 8(10):627?633.Wheeler Ruml.
2010.
The logic of benchmarking: Acase against state-of-the-art performance.
In Pro-ceedings of the Third Annual Symposium on Combi-natorial Search (SOCS-10).Charles Spearman.
1904.
Proof and measurement ofassociation between two things.
American Journalof Psychology, 15:72?101.Marieke Van Erp and Lourens Van der Meij.
2013.Reusable research?
a case study in named entityrecognition.
CLTL 2013-01, Computational Lexi-cology & Terminology Lab, VU University Amster-dam.Joaquin Vanschoren, Hendrik Blockeel, BernhardPfahringer, and Geoffrey Holmes.
2012.
Experi-ment databases.
Machine Learning, 87(2):127?158.Piek Vossen, Isa Maks, Roxane Segers, Hennie van derVliet, Marie-Francine Moens, Katja Hofmann, ErikTjong Kim Sang, and Maarten de Rijke.
2013.
Cor-netto: a Combinatorial Lexical Semantic Databasefor Dutch.
In Peter Spyns and Jan Odijk, editors, Es-sential Speech and Language Technology for DutchResults by the STEVIN-programme, number XVII inTheory and Applications of Natural Language Pro-cessing, chapter 10.
Springer.Zhibiao Wu and Martha Palmer.
1994.
Verb seman-tics and lexical selection.
In Proceedings of the32nd Annual Meeting of the Association for Com-putational Linguistics, pages 133?138, Las Cruces,USA.1701
