Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1464?1472,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsDetecting Experiences from WeblogsKeun Chan Park, Yoonjae Jeong and Sung Hyon MyaengDepartment of Computer ScienceKorea Advanced Institute of Science and Technology{keunchan, hybris, myaeng}@kaist.ac.krAbstractWeblogs are a source of human activity know-ledge comprising valuable information such asfacts, opinions and personal experiences.
Inthis paper, we propose a method for miningpersonal experiences from a large set of web-logs.
We define experience as knowledge em-bedded in a collection of activities or eventswhich an individual or group has actually un-dergone.
Based on an observation that expe-rience-revealing sentences have a certain lin-guistic style, we formulate the problem of de-tecting experience as a classification task us-ing various features including tense, mood, as-pect, modality, experiencer, and verb classes.We also present an activity verb lexicon con-struction method based on theories of lexicalsemantics.
Our results demonstrate that the ac-tivity verb lexicon plays a pivotal role amongselected features in the classification perfor-mance and shows that our proposed methodoutperforms the baseline significantly.1 IntroductionIn traditional philosophy, human beings areknown to acquire knowledge mainly by reason-ing and experience.
Reasoning allows us to drawa conclusion based on evidence, but people tendto believe it firmly when they experience or ob-serve it in the physical world.
Despite the factthat direct experiences play a crucial role in mak-ing a firm decision and solving a problem,people often resort to indirect experiences byreading written materials or asking around.Among many sources people resort to, the Webhas become the largest one for human expe-riences, especially with the proliferation of web-logs.While Web documents contain various typesof information including facts, encyclopedicknowledge, opinions, and experiences in general,personal experiences tend to be found in weblogsmore often than other web documents like newsarticles, home pages, and scientific papers.
Assuch, we have begun to see some research effortsin mining experience-related attributes such astime, location, topic, and experiencer, and theirrelations from weblogs (Inui et al, 2008; Kura-shima et al, 2009).Mined experiences can be of practical use inwide application areas.
For example, a collectionof experiences from the people who visited aresort area would help planning what to do andhow to do things correctly without having tospend time sifting through a variety of resourcesor rely on commercially-oriented sources.Another example would be a public service de-partment gleaning information about how a parkis being used at a specific location and time.Experiences can be recorded around a framelike ?who did what, when, where, and why?
al-though opinions and emotions can be also linked.Therefore attributes such as location, time, andactivity and their relations must be extracted bydevising a method for selecting experience-containing sentences based on verbs that have aparticular linguistics case frame or belong to a?do?
class (Kurashima et al, 2009).
However,this kind of method may extract the followingsentences as containing an experience:[1] If Jason arrives on time, I?ll buy him a drink.
[2] Probably, she will laugh and dance in his funeral.
[3] Can anyone explain what is going on here?
[4] Don?t play soccer on the roads!None of the sentences contain actual experiencesbecause hypotheses, questions, and orders havenot actually happened in the real world.
For ex-perience mining, it is important to ensure a sen-tence mentions an event or passes a factualitytest to contain experience (Inui et al, 2008).In this paper, we focus on the problem of de-tecting experiences from weblogs.
We formulate1464Class ExamplesState like, know, believeActivity run, swim, walkAchievement recognize, realizeAccomplishmentpaint (a picture),build (a house)Table 1.
Vendler class examplesthe problem as a classification task using variouslinguistic features including tense, mood, aspect,modality, experiencer, and verb classes.Based on our observation that experience-revealing sentences tend to have a certain lin-guistic style (Jijkoun et al, 2010), we investigateon the roles of various features.
The ability todetect experience-revealing sentences should bea precursor for ensuring the quality of extractingvarious elements of actual experiences.Another issue addressed in this paper is au-tomatic construction of a lexicon for verbs re-lated to activities and events.
While there havebeen well-known studies about classifying verbsbased on aspectual features (Vendler, 1967),thematic roles and selectional restrictions (Fill-more, 1968; Somers, 1987; Kipper et al, 2008),valence alternations and intuitions (Levin, 1993)and conceptual structures (Fillmore and Baker,2001), we found that none of the existing lexicalresources such as Framenet (Baker et al, 2003)and Verbnet (Kipper et al, 2008) are sufficientfor identifying experience-revealing verbs.
Weintroduce a method for constructing an activi-ty/event verb lexicon based on Vendler?s theoryand statistics obtained by utilizing a web searchengine.We define experience as knowledge embed-ded in a collection of activities or events whichan individual or group has actually undergone1.
Itcan be subjective as in opinions as well as objec-tive, but our focus in this article lies in objectiveknowledge.
The following sentences contain ob-jective experiences:[5] I ran with my wife 3 times a week until wemoved to Washington, D.C.[6] Jane and I hopped on a bus into the city centre.
[7] We went to a restaurant near the central park.Whereas sentences like the following containsubjective knowledge:[8] I like your new style.
You?re beautiful!
[9] The food was great, the interior too.Subject knowledge has been studied extensivelyfor various functions such as identification, po-1 http://en.wikipedia.org/wiki/Experience_(disambiguation)larity detection, and holder extraction under thenames of opinion mining and sentiment analysis(Pang and Lee, 2008).In summary, our contribution lies in three as-pects: 1) conception of experience detection,which is a precursor for experience mining, andspecific related tasks that can be tackled with ahigh performance machine learning based solu-tion; 2) examination and identification of salientlinguistic features for experience detection; 3) anovel lexicon construction method with identifi-cation of key features to be used for verb classi-fication.The remainder of the paper is organized as fol-lows.
Section 2 presents our lexicon constructionmethod with experiments.
Section 3 describesthe experience detection method, including expe-rimental setup, evaluation, and results.
In Section4, we discuss related work, before we close withconclusion and future work in Section 5.2 Lexicon ConstructionSince our definition of experience is based onactivities and events, it is critical to determinewhether a sentence contains a predicate describ-ing an activity or an event.
To this end, it is quiteconceivable that a lexicon containing activity /event verbs would play a key role.
Given thatour ultimate goal is to extract experiences from alarge amount of weblogs, we opt for increasedcoverage by automatically constructing a lexiconrather than high precision obtainable by manual-ly crafted lexicon.Based on the theory of Vendler (1967), weclassify a given verb or a verb phrase into one ofthe two categories: activity and state.
We consid-er all the verbs and verb phrases in WordNet(Fellbaum, 1998) which is the largest electroniclexical database.
In addition to the linguisticschemata features based on Vendler?s theory, weused thematic role features and an externalknowledge feature.2.1 BackgroundVendler (1967) proposes that verb meanings canbe categorized into four basic classes, states, ac-tivities, achievements, and accomplishments, de-pending on interactions between the verbs andtheir aspectual and temporal modifiers.
Table 1shows some examples for the classes.Vendler (1967) and Dowty (1979) introducelinguistic schemata that serve as evidence for theclasses.1465LinguisticSchematabs prs prp pts ptpNo schema  ?
?
?
?
?Progressive   ?Force ?Persuade ?Stop   ?For ?
?
?
?
?Carefully ?
?
?
?
?Table 2.
Query matrix.
The ???
indicates that thequery is applied.
No Schema indicates that noschema is applied when the word itself is a query.bs, prs, prp, pts, ptp correspond to base form,present simple (3rd person singular), present par-ticiple, past simple and past participle, respect-fully.Below are the six schemata we chose becausethey can be tested automatically: progressive,force, persuade, stop, for, and carefully (An aste-risk denotes that the statement is awkward).?
States cannot occur in progressive tense:John is running.John is liking.*?
States cannot occur as complements offorce and persuade:John forced harry to run.John forced harry to know.
*John persuaded harry to know.*?
Achievements cannot occur as comple-ments of stop:John stopped running.John stopped realizing.*?
Achievements cannot occur with time ad-verbial for:John ran for an hour.John realized for an hour.*?
State and achievement cannot occur withadverb carefully:John runs carefully.John knows carefully.
*The schemata are not perfect because verbs canshift classes due to various contextual factorssuch as arguments and senses.
However, a verbcertainly has its fundamental class that is its mostnatural category at least in its dominant use.The four classes can further be grouped intotwo genuses: a genus of processes going on intime and the other that refers to non-processes.Activity and accomplishment belong to the for-mer whereas state and achievement belong to thelatter.
As can be seen in table 1, states are ratherimmanent operations and achievements are thoseoccur in a single moment or operations related toperception level.
On the other hand, activity andaccomplishment are processes (transeunt opera-tions) in traditional philosophy.
We henceforthcall the first genus activity and the latter state.Our aim is to classify verbs into the two genuses.2.2 Features based on Linguistic SchemataWe developed a relatively simple computationaltesting method for the schemata.
Assuming thatan awkward expression like, ?John is likingsomething?
won?t occur frequently, for example,we generated a co-occurrence based test for thefirst linguistic schema using the Web as a corpus.By issuing a search query, ((be OR am OR is ORwas OR were OR been) and ?
ing) where ??
?represents the verb at hand, to a search engine,we can get an estimate about how the verb islikely to belong to state.
A test can be generatedfor each of the schemata in a similar way.For completeness, we considered all the verbforms (i.e., 3rd person singular present, presentparticiple, simple past, past participle) available.However, some of the patterns cannot be appliedto some forms.
For example, other forms exceptthe base form cannot come as a complement offorce (e.g., force to runs.*).
Therefore, wecreated a query matrix which represents all querypatterns we have applied, in table 2.Based on the query matrix in table 2, we is-sued queries for all the verbs and verb phrasesfrom WordNet to a search engine.
We used theGoogle news archive search for two reasons.First, since news articles are written rather for-mally compared to weblogs and other web pages,the statistics obtained for a test would be morereliable.
Second, Google provides an advancedoption to retrieve snippets containing the queryword.
Normally, a snippet is composed of 3~5sentences.The basic statistics we consider are hit count,candidate sentence count and correct sentencecount which we use the notations Hij(w), Sij(w),and Cij(w), respectfully, where w is a word, i thelinguistic schema and j the verb form from thequery matrix in table 2.
Hij(w) was directly ga-thered from the Google search engine.
Sij(w) isthe number of sentences containing the word win the search result snippets.
Cij(w) is the numberof correct sentences matching the query patternamong the candidate sentences.
For example, theprogressive schema for a verb ?build?
can re-trieve the following sentences.
[10]   ?, New-York, is building one of the largest ?
[11]   Is building an artifact?1466?Building?
in the first example is a progressiveverb, but the one in second is a noun, which doesnot satisfy the linguistic schema.
For a POS andgrammatical check of a candidate sentence, weused the Stanford POS tagger (Toutanova et al,2003) and Stanford dependency parser (Kleinand Manning, 2003).For each linguistic schema, we derived threefeatures: Absolute hit ratio, Relative hit ratio andValid ratio for which we use the notations Ai(w),Ri(w) and Vi(w), respectfully, where w is a wordand i a linguistic schema.
The index j for summa-tions represents the j-th verb form.
They arecomputed as follows.
( )( )( )( )( )( )( )( )( )*ijjiiijjiNo SchemejijjiijjH wA wHH wR wH wC wV wS w===?????
(1)Absolute hit ratio is computes the extent towhich the target word w occurs with the i-thschema over all occurrences of the schema.
Thedenominator is the hit count of wild card ?
*?matching any single word with the schema pat-tern from Google (e.g., H1(*), the progressivetest hit count is 3.82 ?
108).
Relative hit ratiocomputes the extent to which the target word woccurs with the i-th schema over all occurrencesof the word.
The denominator is the sum of allverb forms.
Valid ratio means the fraction of cor-rect sentences among candidate sentences.
Theweight of a linguistic schema increases as thevalid ratio gets high.
With the three differentratios, Ai(w), Ri(w) and Vi(w), for each test, wecan generate a total of 18 features.2.3 Features based on case framesSince the hit count via Google API sometimesreturns unreliable results (e.g., when the querybecomes too long in case of long verb phrases),we also consider additional features.
While ourinitial observation indicated that the existing lex-ical resources would not be sufficient for ourgoal, it occurred to us that the linguistic theorybehind them would be worth exploring as gene-rating additional features for categorizing verbsfor the two classes.
Consider the following ex-amples:[12]   John(D) believed(V) the story(O).
[13]   John(A) hit(V) him(O) with a bat(I).The subject of a state verb is dative (D) as in [12]whereas the subject for an action verb takes theagent (A) role.
In addition, a verb with the in-strument (I) role tends to be an action verb.
Fromthese observations, we can use the distribution ofcases (thematic roles) for a verb in a corpus.
Ac-tivity verbs are expected to have high frequencyof agent and instrument roles than state verbs.Although a verb may have more than one caseframe, it is possible to determine which thematicroles used more dominantly.We utilize two major resources of lexical se-mantics, Verbnet (Kipper et al, 2008) based onthe theory of Levin (1993), and Framenet (Bakeret al, 2003), which is based on Fillmore (1968).Levin (1993) demonstrated that syntactic alterna-tions can be the basis for groupings of verbs se-mantically and accord reasonably well with lin-guistic intuitions.
Verbnet provides 274 verbclasses with 23 thematic roles covering 3,769verbs based on their alternation behaviors withthematic roles annotated.
Framenet defines 978semantic frames with 7,124 unique semanticroles, covering 11,583 words including verbs,nouns, adverbs, etc.Using Verbnet alne does not suit our needsbecause it has a relatively small number of ex-ample sentences.
Framenet contains a much larg-er number of examples but the vast number ofsemantic roles presents a problem.
In order to getmeaningful distributions for a manageable num-ber of thematic roles, we used Semlink (Loper etal., 2007) that provides a mapping between Fra-menet and Verbnet and uses a total of 23 themat-ic roles of Verbnet for the annotated corpora ofthe two resources.
By the mapping, we obtaineddistributions of the thematic roles for 2,868unique verbs that exist in both of the resources.For example, the verb ?construct?
has high fre-quencies with agent, material and product roles.2.4 Features based on how-to instructionsRyu et al (2010) presented a method for extract-ing action steps for how-to goals from eHow2 awebsite containing a large number of how-to in-structions.
The authors attempted to extract ac-tions comprising a verb and some ingredientslike an object entity from the documents basedon syntactic patterns and a CRF based model.Since each extracted action has its probability,we can use the value as a feature for state / activ-ity verb classification.
However, a verb may ap-pear in different contexts and can have multiple2 http://www.ehow.com1467FeatureME SVMPrec.
Recall Prec.
RecallAll 43 68% 50% 83% 75%Top 30 72% 52% 83% 75%Top 20 83% 76% 85% 77%Top 10 89% 88% 91% 78%Table 3.
Classification PerformanceClass ExamplesActivityact, battle, build, carry, chase,drive, hike, jump, kick, skydive, tap dance, walk, ?Stateadmire, believe, know, like,love, ?Table 4.
Classified Examplesprobability values.
To generate a single value fora verb, we combine multiple probability valuesusing the following sigmoid function:1( )1( )wtdd DE wet P w??=+=?
(2)Evidence of a word w being an action in eHow isdenoted as E(w) where variable t is the sum ofindividual action probability values in Dw the setof documents from which the word w has beenextracted as an action.
The higher probability aword gets and the more frequent the word hasbeen extracted as an action, the more evidencewe get.2.5 ClassificationFor training, we selected 80 seed verbs fromDowty?s list (1979) which are representativeverbs for each Vendler (1967) class.
The selec-tion was based on the lack of word sense ambi-guity.One of our classifiers is based on MaximumEntropy (ME) models that implement the intui-tion that the best model will be the one that isconsistent with the set of constraints imposed bythe evidence, but otherwise is as uniform aspossible (Berger et al, 1996).
ME models arewidely used in natural language processing tasksfor its flexibility to incorporate a diverse range offeatures.
The other one is based on Support Vec-tor Machine (Chang and Lin, 2001) which is thestate-of-the-art algorithm for many classificationtasks.
We used RBF kernel with the default set-tings (Hsu et al, 2009) because it is been knownto show moderate performance using multiplefeature compositions.The features we considered are a total of 42real values: 18 from linguistic schemata, 23 the-matic role distributions, and one from eHow.
Inorder to examine which features are discrimina-tive for the classification, we used two wellknown feature selection methods, Chi-square andinformation gain.2.6 ResultsTable 3 shows the classification performancevalues for different feature selection methods.The evaluation was done on the training datawith 10-fold cross validation.Note that the precision and recall are macro-averaged values across the two classes, activityand state.
The most discriminative features wereabsolute ratio and relative ratio in conjunctionwith the force, stop, progressive, and persuadeschemata, the role distribution of experiencer,and the eHow evidence.It is noteworthy that eHow evidence and thedistribution of experiencer got into the top 10.Other thematic roles did not perform well be-cause of the data sparseness.
Only a few roles(e.g., experience, agent, topic, location) amongthe 23 had frequency values other than 0 formany verbs.
Data sparseness affected the linguis-tic schemata as well.
Many of the verbs had zerohit counts for the for and carefully schemata.
It isalso interesting that the validity ratio Vi(w) wasnot shown to be a good feature-generating statis-tic.We finally trained our model with the top 10features and classified all WordNet verbs andverb phrases.
For actual construction of the lex-icon, 11,416 verbs and verb phrases were classi-fied into the two classes roughly equally.
Werandomly sampled 200 items and examined howaccurately the classification was done.
A total of164 items were correctly classified, resulting in82% accuracy.
Some examples from the classifi-cation are shown in table 4.A further analysis of the results show thatmost of the errors occurred with domain-specificverbs (e.g., ablactate, alkalify, and transaminatein chemistry) and multi-word verb phrases (e.g.,turn a nice dime; keep one?s shoulder to thewheel).
Since many features are computed basedon Web resources, rare verbs cannot be classifiedcorrectly when their hit rations are very low.
Thedomain-specific words rarely appear in Framenetor e-how, either.3 Experience DetectionAs mentioned earlier, experience-revealing sen-tences tend to have a certain linguistic style.1468Having converted the problem of experience de-tection for sentences to a classification task, wefocus on the extent to which various linguisticfeatures contribute to the performance of the bi-nary classifier for sentences.
We also explain theexperimental setting for evaluation, including theclassifier and the test corpus.3.1 Linguistic featuresIn addition to the verb class feature available inthe verb lexicon constructed automatically, weused tense, mood, aspect, modality, and expe-riencer features.Verb class: The feature comes directly fromthe lexicon since a verb has been classified into astate or activity verb.
The predicate part of thesentence to be classified for experience is lookedup in the lexicon without sense disambiguation.Tense: The tense of a sentence is importantsince an experience-revealing sentence tends touse past and present tense.
Future tenses are notexperiences in most cases.
We use POS tagging(Toutanova et al, 2003) for tense determination,but since the Penn tagset provides no futuretenses, they are determined by exploiting modalverbs such as ?will?
and future expressions such?going to?.Mood: It is one of distinctive forms that areused to signal the modal status of a sentence.
Weconsider three mood categories: indicative, im-perative and subjunctive.
We determine themood of a sentence by a small set of heuristicrules using the order of POS occurrences andpunctuation marks.Aspect: It defines the temporal flow of a verbin the activity or state.
Two categories are used:progressive and perfective.
This feature is deter-mined by the POS of the predicate in a sentence.Modality: In linguistics, modals are expres-sions broadly associated with notions of possibil-ity.
While modality can be classified at a finelevel (e.g., epistemic and deontic), we simplydetermine whether or not a sentence includes amodal marker that is involved in the main predi-cate of the sentence.
In other words, this binaryfeature is determined based on the existence of amodel verb like ?can?, ?shall?, ?must?, and ?may?or a phrase like ?have to?
or ?need to?.
The de-pendency parser is used to ensure a modal mark-er is indeed associated with the main predicate.Experiencer: A sentence can or cannot betreated as containing an experience depending onthe subject or experiencer of the verb (note thatthis is different from the experiencer role in acase frame).
Consider the following sentences:[14]   The stranger messed up the entire garden.
[15]   His presence messed up the whole situation.The first sentence is considered an experiencesince the subject is a person.
However, thesecond sentence with the same verb is not, be-cause the subject is a non-animate abstract con-cept.
That is, a non-animate noun can hardlyconstitute an experience.
In order to make a dis-tinction, we use the dependency parser and anamed-entity recognizer (Finkel et al, 2005) thatcan recognize person pronouns and person names.3.2 ClassificationTo train our classifier, we first crawled weblogsfrom Wordpress3, one of the most popular blogsites in use today.
Worpress provides an interfaceto search blog posts with queries.
In selectingexperience-containing blog pots, we used loca-tion names such as Central Park, SOHO, Seouland general place names such as airport, subwaystation, and restaurant because blog posts withsome places are expected to describe experiencesrather than facts or thoughts.We crawled 6,000 blog posts.
After deletingnon-English and multi-media blog posts forwhich we could not obtain any meaningful textdata, the number became 5,326.
We randomlysampled 1,000 sentences4 and asked three anno-tators to judge whether or not individual sen-tences are considered containing an experiencebased on our definition.
For maximum accuracy,we decided to use only those sentences all thethree annotators agreed, resulting in a total of568 sentences.While we tested several classifiers, we choseto use two different classifiers based on SVMand Logistic Regression for the final experimen-tal results because they showed the best perfor-mance.3.3 ResultsFor comparison purposes, we take the method ofKurashima et al (2005) as our baseline becausethe method was used in subsequent studies (Ku-rashima et al, 2006; Kurashima et al, 2009)where experience attributes are extracted.
Webriefly describe the method and present how weimplemented it.The method first extracts all verbs and theirdependent phrasal unit from candidate sentences.3 http://wordpress.com4 It was due to the limited human resources, but when weincreased the number at a later stage, the performance in-crease was almost negligible.1469FeatureLogisticRegressionSVMPrec.
Recall Prec.
RecallBaseline 32.0% 55.1% 25.3% 44.4%Lexicon 77.5% 76.0% 77.5% 76.0%Tense 75.1% 75.1% 75.1% 75.1%Mood 75.8% 60.3% 75.8% 60.3%Aspect 26.7% 51.7% 26.7% 51.7%Modality 79.8% 70.5% 79.8% 70.5%Experiencer 54.3% 53.5% 54.3% 53.5%All included 91.9% 91.7% 91.7% 91.4%Table 5.
Experience Detection PerformanceThe candidate goes through three filters before itis treated as experience-containing sentence.First, the candidates that do not have an objectivecase (Fillmore, 1968) are eliminated becausetheir definition of experience as ?action + object?.This was done by identifying the object-indicating particle (case marker) in Japanese.Next, the candidates belonging to ?become?
and?be?
statements based on Japanese verb types arefiltered out.
Finally, the candidate sentences in-cluding a verb that indicates a movement areeliminated because the main interest was to iden-tify an activity in a place.Although their definition of experience issomewhat different from ours (i.e., ?action + ob-ject?
), they used the method to generate candi-date sentences from which various experienceattributes are extracted.
From this perspective,the method functioned like our experience detec-tion.
Put differently, the definition and the me-thod by which it is determined were much cruderthan the one we are using, which seems close toour general understanding.5The three filtering steps were implemented asfollows.
We used the dependency parser for ex-tracting objective cases using the direct objectrelation.
The second step, however, could not beapplied because there is no grammatical distinc-tion among ?do, be, become?
statements in Eng-lish.
We had to alter this step by adopting theapproach of Inui et al (2008).
The authors pro-pose a lexicon of experience expression by col-lecting hyponyms from a hierarchically struc-tured dictionary.
We collected all hyponyms ofwords ?do?
and ?act?, from WordNet (Fellbaum,1998).
Lastly, we removed all the verbs that areunder the hierarchy of ?move?
from WordNet.We not only compared our results with thebaseline in terms of precision and recall but also5 This is based on our observation that the three annotatorsfound their task of identifying experience sentences notdifficulty, resulting in a high degree of agreements.FeatureLogisticRegressionSVMPrec.
Recall Prec.
RecallBaseline 32.0% 55.1% 25.3% 44.4%-Lexicon 84.6% 84.6% 83.1% 81.2%-Tense 87.3% 87.1% 86.8% 86.5%-Mood 89.5% 89.5% 89.3% 89.2%-Aspect 90.8% 90.5% 89.0% 88.6%-Modality 89.5% 89.5% 82.8% 82.8%-Experiencer 91.5% 91.4% 91.1% 90.8%All included 91.9% 91.7% 91.7% 91.4%Table 6.
Experience Detection Performancewithout Individual Featuresevaluated individual features for their importancein experience detection (classification).
Theevaluation was conducted with 10-fold cross va-lidation.
The results are shown in table 5.The performance, especially precision, of thebaseline is much lower than those of the others.The method devised for Japanese doesn?t seemsuitable for English.
It seems that the linguisticstyles shown in experience expressions are dif-ferent from each other.
In addition, the lexiconwe constructed for the baseline (i.e., using theWordNet) contains more errors than our activitylexicon for activity verbs.
Some hyponyms of anactivity verb may not be activity verbs.
(e.g.,?appear?
is a hyponym of ?do?
).There is almost no difference between the Lo-gistic Regression and SVM classifiers for ourmethods although SVM was inferior for thebaseline.
The performance for the best case withall the features included is very promising,closed to   92% precision and recall.
Among thefeatures, the lexicon, i.e., verb classes, gave thebest result when each is used alone, followed bymodality, tense, and mood.
Aspect was the worstbut close to the baseline.
This result is very en-couraging for the automatic lexicon constructionwork because the lexicon plays a pivotal role inthe overall performance.In order to see the effect of including individ-ual features in the feature set, precision and re-call were measured after eliminating a particularfeature from the full set.
The results are shown intable 6.
Although the absence of the lexicon fea-ture hurt the performance most badly, still theperformance was reasonably high (roughly 84 %in precision and recall for the Logistic Regres-sion case).
Similar to table 5, the aspect and ex-perience features were the least contributors asthe performance drops are almost negligible.14704 Related WorkExperience mining in its entirety is a relativelynew area where various natural languageprocessing and text mining techniques can play asignificant role.
While opinion mining or senti-ment analysis, which can be considered an im-portant part of experience mining, has been stu-died quite extensively (see Pang and Lee?s excel-lent survey (2008)), another sub-area, factualityanalysis, begins to gain some popularity (Inui etal., 2008; Saur?, 2008).
Very few studies havefocused explicitly on extracting various entitiesthat constitute experiences (Kurashima et al,2009) or detecting experience-containing parts oftext although many NLP research areas such asnamed entity recognition and verb classificationare strongly related.
The previous work on expe-rience detection relies on a handcrafted lexicon.There have been a number of studies for verbclassification (Fillmore, 1968; Vendler, 1967;Somers, 1982; Levin, 1993; Fillmore and Baker,2001; Kipper et al, 2008) that are essential forconstruction of an activity verb lexicon, which inturn is important for experience detection.
Mostsimilar to our work was done by Siegel andMcKeown (2000), who attempted to categorizeverbs into state or event classes based on 14 testssimilar to those of Vendler?s.
They attempted tocompute co-occurrence statistics from a corpus.The event class, however, includes activity, ac-complishment, and achievement.
Similarly, Za-crone and Lenci (2008) attempted to categorizeverbs in Italian into the four Vendler classes us-ing the Vendler tests by using a tagged corpus.They focused on existence of arguments such assubject and object that should co-occur with thelinguistic features in the tests.The main difference between the previouswork and ours lies in the goal and scope of thework.
Since our work is specifically geared to-ward domain-independent experience detection,we attempted to maximize the coverage by usingall the verbs in WordNet, as opposed to the verbsappearing in a particular domain-specific corpus(e.g., medicine domain) as done in the previouswork.
Another difference is that while we are notlimited to a particular domain, we did not useextensive human-annotated corpus other thanusing the 80 seed verbs and existing lexical re-sources.5 Conclusion and Future WorkWe defined experience detection as an essentialtask for experience mining, which is restated asdetermining whether individual sentences con-tain experience or not.
Viewing the task as aclassification problem, we focused on identifica-tion and examination of various linguistic fea-tures such as verb class, tense, aspect, mood,modality, and experience, all of which werecomputed automatically.
For verb classes, in par-ticular, we devised a method for classifying allthe verbs and verb phrases in WordNet into theactivity and state classes.
The experimental re-sults show that verb and verb phrase classifica-tion method is reasonably accurate with 91%precision and 78% recall with manually con-structed gold standard consisting of 80 verbs and82% accuracy for a random sample of all theWordNet entries.
For experience detection, theperformance was very promising, closed to 92%in precision and recall when all the features wereused.
Among the features, the verb classes, or thelexicon we constructed, contributed the most.In order to increase the coverage even furtherand reduce the errors in lexicon construction, i.e.,verb classification, caused by data sparseness, weneed to devise a different method, perhaps usingdomain specific resources.Given that experience mining is a relativelynew research area, there are many areas to ex-plore.
In addition to refinements of our work, ournext step is to develop a method for representingand extracting actual experiences from expe-rience-revealing sentences.
Furthermore, consi-dering that only 13% of the blog data weprocessed contain experiences, an interestingextension is to apply the methodology to extractother types of knowledge such as facts, whichare not necessarily experiences.AcknowledgmentsThis research was supported by the IT R&D pro-gram of MKE/KEIT under grant KI001877 [Lo-cational/Societal Relation-Aware Social MediaService Technology], and by the MKE (TheMinistry of Knowledge Economy), Korea, underthe ITRC (Information Technology ResearchCenter) support program supervised by the NIPA(National IT Industry Promotion Agency) [NI-PA-2010-C1090-1011-0008].ReferenceEiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhi-ko Ohe.
2009.
TEXT2TABLE: Medical TextSummarization System based on Named Entity1471Recognition and Modality Identification.
In Pro-ceedings of the Workshop on BioNLP.Collin F. Baker, Charles J. Fillmore, and Beau Cronin.2003.
The Structure of the Framenet Database.
In-ternational Journal of Lexicography.Adam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A Mximum EntropyApproach to Natural Language Processing.
Com-putational Linguistics.Chih-Chung Chang and Chih-Jen Lin.
2001.LIBSVM : a Library for Support Vector Machines.http://www.csie.ntu.edu.tw/~cjlin/libsvm.David R. Dowty.
1979.
Word meaning and MontagueGrammar.
Reidel, Dordrecht.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Charles J. Fillmore.
1968.
The Case for Case.
In Bachand Harms (Ed.
): Universals in Linguistic Theory.Charles J. Fillmore and Collin F. Baker.
2001.
FrameSemantics for Text Understanding.
In Proceedingsof WordNet and Other Lexical Resources Work-shop, NAACL.Jenny R. Finkel, Trond Grenager, and Christopher D.Manning.
2005.
Incorporating Non-local Informa-tion into Information Extraction Systems by GibbsSampling.
In Proceedings of ACL.Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.2009.
A Practical Guide to Support Vector Classi-fication.
http://www.csie.ntu.edu.tw/~cjlin/libsvm.Kentaro Inui, Shuya Abe, Kazuo Hara, Hiraku Morita,Chitose Sao, Megumi Eguchi, Asuka Sumida, KojiMurakami, and Suguru Matsuyoshi.
2008.
Expe-rience Mining: Building a Large-Scale Database ofPersonal Experiences and Opinions from WebDocuments.
In Proceedings of the InternationalConference on Web Intelligence.Valentin Jijkoun, Maarten de Rijke, Wouter Weer-kamp, Paul Ackermans and Gijs Geleijnse.
2010.Mining User Experiences from Online Forums: AnExploration.
In Proceedings of NAACL HLT Work-shop on Computational Linguistics in a World ofSocial Media.Karin Kipper, Anna Korhonen, Neville Ryant, andMartha Palmer.
2008.
A Large-scale Classificationof English Verbs.
Language Resources and Evalu-ation Journal.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of ACL.Takeshi Kurashima, Ko Fujimura, and Hidenori Oku-da.
2009.
Discovering Association Rules on Expe-riences from Large-Scale Blog Entries.
In Proceed-ings of ECIR.Takeshi Kurashima, Taro Tezuka, and Katsumi Tana-ka.
2005.
Blog Map of Experiences: Extracting andGeographically Mapping Visitor Experiences fromUrban Blogs.
In Proceedings of WISE.Takeshi Kurashima, Taro Tezuka, and Katsumi Tana-ka.
2006.
Mining and Visualizing Local Expe-riences from Blog Entries.
In Proceedings ofDEXA.John Lafferty, Andew McCallum, and Fernando Pe-reira.
2001.
Conditional Random Fields: Probabil-istic Models for Segmenting and Labeling Se-quence Data.
In Proceedings of ICML.Beth Levin.
1993.
English verb classes and alterna-tions: A Preliminary investigation.
University ofChicago press.Edward Loper, Szu-ting Yi, and Martha Palmer.
2007.Combining Lexical Resources: Mapping BetweenPropBank and Verbnet.
In Proceedings of the In-ternational Workshop on Computational Linguis-tics.Bo Pang and Lillian Lee.
2008.
Opinion Mining andSentiment Analysis, Foundations and Trends in In-formation Retrieval.Jihee Ryu, Yuchul Jung, Kyung-min Kim and Sung H.Myaeng.
2010.
Automatic Extraction of HumanActivity Knowledge from Method-Describing WebArticles.
In Proceedings of the 1st Workshop on Au-tomated Knowledge Base Construction.Roser Saur?.
2008.
A Factuality Profiler for Eventuali-ties in Text.
PhD thesis, Brandeis University.Eric V. Siegel and Kathleen R. McKeown.
2000.Learing Methods to Combine Linguistic Indicators:Improving Aspectual Classification and RevealingLinguistic Insights.
In Computational Linguistics.Harold L. Somers.
1987.
Valency and Case in Com-putational Linguistics.
Edinburgh University Press.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic DependencyNetwork.
In Proceedings of HLT-NAACL.Zeno Vendler.
1967.
Linguistics in Philosophy.
Cor-nell University Press.Alessandra Zarcone and Alessandro Lenci.
2008.Computational Models of Event Type Classifica-tion in Context.
In Proceedings of LREC.1472
